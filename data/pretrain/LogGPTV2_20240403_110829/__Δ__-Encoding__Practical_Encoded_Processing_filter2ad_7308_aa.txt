title:\(Δ\)-Encoding: Practical Encoded Processing
author:Dmitrii Kuvaiskii and
Christof Fetzer
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Δ-encoding: Practical Encoded Processing
Dmitrii Kuvaiskii and Christof Fetzer
Technische Universit¨at Dresden
Email: {ﬁrstname.lastname}@tu-dresden.de
Dresden, Germany
Abstract—Transient and permanent errors in memory and
CPUs occur with alarming frequency. Although most of these
errors are masked at the hardware level or result in crashes, a
non-negligible number of them leads to Silent Data Corruptions
(SDCs),
incorrect results of computations. Safety-critical
programs require a very high level of conﬁdence that such faults
are detected and not propagated to the outside. Unfortunately,
state-of-the-art fault detection techniques generally assume a
limited Single Event Upset fault model, concentrating only on
transient faults.
i.e.,
We present Δ-encoding: a software-only approach to detect
hardware faults with very high probability. Δ-encoding makes no
assumptions on the rate and type of faults. Our approach com-
bines AN codes and duplicated instructions to harden programs
against transient and permanent hardware errors. Our evaluation
shows that Δ-encoding detects 99.997% of all injected errors with
performance slowdown of 2–4 times.
I.
INTRODUCTION
A dramatic decrease in hardware reliability, most
im-
portantly in CPUs and RAM, was forecast already in the
2000s [1]. This is due to the decrease of feature sizes with
each new hardware generation, causing variations in transistor
behavior. These variations,
the hardware
level, can lead to silent data corruptions (SDCs) in a program.
Moreover, additional effects such as transistor aging and soft
errors (due to alpha particles and cosmic rays hitting silicon
chips) increase the probability of a program to produce wrong
results.
if not masked at
Recent studies provide supporting evidence for this fore-
cast. Google analyzed DRAM failure patterns across its server
ﬂeet [2]. The research concluded that (1) DRAM failure rates
are higher than previously expected1, (2) memory errors are
strongly correlated, and (3) memory errors are dominated by
hard errors rather than by soft errors. Another study shows
that even ECC-enabled DRAM chips do not provide adequate
protection from the emerging problem of disturbance errors,
when accesses to one DRAM row corrupt data in adjacent
rows [3].
Similar ﬁndings were revealed in regard to modern CPUs.
Microsoft conducted analysis of hardware failures on a ﬂeet
of 950,000 machines [4]. This work showed that (1) failure
rates of modern CPU subsystems are non-negligible2, (2)
failure rates increase with the increasing CPU speed, and
1One third of machines under study experienced at least one correctable
memory error per year; the annual rate of uncorrectable errors amounted to
a signiﬁcant 1.3%. Note that all memory modules were equipped with error
correcting codes (ECC).
2For example, the chance of a crash is 1 in 190 for machines with the total
CPU time of 30 days.
(3) CPU faults tend to be intermittent rather than transient.
Unfortunately,
the study considers only crash failures and
not data corruptions in applications; other studies, however,
indicate that CPU faults result in a non-trivial number of
SDCs [5].
Many hardware errors, either in CPU or in memory, lead to
a process or machine crash. Still, some hardware faults induce
programs to output
incorrect results, which can propagate
further and lead to catastrophic consequences. One anecdotal
evidence is the famous Amazon S3 unavailability incident,
when a single bit corruption in a few messages caused an
8-hour outage [6].
The consequences are even more disastrous in safety-
critical applications. As one example, Toyota Motor Corpo-
ration was forced to recall its automobiles in the years 2009–
2011 after several reports that Toyota cars experienced unin-
tended acceleration [7]. The number of victims was estimated
to be 37, and ﬁnancial loss for Toyota $2,470 million. Though
the exact causes of the problem were not found out, insufﬁcient
protection against hardware errors could be one of them:
“Michael Barr of the Barr Group testiﬁed that
. . . Toyota did not follow best practices for real-time
life-critical software, and that a single bit ﬂip which
can be caused by cosmic rays could cause unintended
acceleration.”
Detecting hardware faults of all types is a necessity for
applications from different domains. Tolerating faults once
they are found can often be achieved by simply restarting
the process or rebooting the machine: in most cases it is
enough that incorrect computation results are not propagated
to the outside. Therefore, we concentrate on hardware error
detection3 in this paper.
The conservative error detection approach, widely used
in automotive and aerospace systems,
is to employ some
form of hardware-based fault tolerance. Usual mechanisms
include triple/dual modular redundancy (TMR/DMR), ﬂip-ﬂop
hardening, watchdogs, etc. [8]. The hardware-based approach,
however, implies higher hardware costs and lower performance
in comparison to modern commodity hardware. For example,
Intel conjectures that future self-driving cars will require
greater computing power and suggests to use commodity
CPUs [9].
Another approach called Software-Implemented Hardware
tolerance via
Fault Tolerance (SIHFT) [10] achieves fault
software-only methods; thus, it does not require specialized
3More precisely, we concentrate on detection of data corruptions that occur
due to hardware errors changing a program’s data ﬂow.
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.20
DOI 10.1109/DSN.2015.20
13
13
hardware. However, in spite of the experimental studies clearly
indicating the prevalence of permanent and intermittent errors
in CPU and memory, most SIHFT techniques assume only
transient errors. In this sense, these techniques favor perfor-
mance over fault coverage and cannot be relied upon in safety-
critical systems.
One notable SIHFT technique that can detect both perma-
nent and transient errors in underlying hardware is encoded
processing [11]. It is based on the theory of arithmetic codes
(AN-encoding) and was used in fault-tolerant computing [12].
Unfortunately, pure AN-encoding has limited fault coverage.
Advanced variants of AN-encoding exist [11], but programs
encoded with them – namely with the ANBD variant – experi-
ence slowdowns of up to 250x. Thus, though ANBD-encoding
yields very high fault coverage, it is impractical in terms of
performance.
As a result, existing SIHFT techniques either do not detect
all possible hardware errors or incur prohibitive performance
penalties. This work makes a step towards hardening critical
computations against permanent and transient hardware errors
with a moderate performance penalty.
Our approach, called Δ-encoding, is based on the combi-
nation of AN-encoding and duplicate execution of instructions.
The original program data ﬂow is duplicated and AN-encoded
at compile-time; at run-time, the program effectively works on
two copies of data encoded in two different ways. The careful
choice of AN-encoding parameters coupled with execution du-
plication greatly simpliﬁes AN-encoded operations, improving
the performance; moreover, the combination of approaches
facilitates detection of all types of hardware errors.
We
fault
injection experiments
implemented Δ-encoding as
source-to-source
that
transformer. Our
Δ-encoding can detect, on average, 99.997% of injected errors.
Our performance evaluation shows that Δ-encoding incurs an
acceptable slowdown of 2–4 times in comparison to native
execution.
reveal
a
II. BACKGROUND
The Δ-encoding technique proposed in this paper com-
bines two existing approaches: AN-encoding and duplicated
instructions. In this section, we brieﬂy discuss both of them.
A. AN-encoding
AN-encoding is a technique to protect program execution
from transient and permanent errors in the underlying hard-
ware. It is based on AN codes – error correcting codes suitable
for arithmetic operations [13]. Schiffel [11] describes AN-
encoding and its variants in detail.
With AN codes, to encode an integer n, one multiplies it
by a constant A. The resultant integer ˆn = A · n is called a
code word; all words that are not multiples of A are invalid. If
a hardware error alters ˆn, it becomes an invalid word with high
probability; this probability depends on A [11]. If ˆn is still a
code word, ˆn mod A equals 0; if the result of this operation
is not 0, a hardware error is detected. To decode, a division
ˆn/A is used.
1414
Listing 1: Native program
1 i n t 3 2 t a = 5 ;
2 i n t 3 2 t b = 3 ;
3 i n t 3 2 t c = a + b ;
4 p r i n t f ( ”%d ” , c ) ;
Listing 2: AN-encoded program
1 # d e f i n e A 11
2 i n t 6 4 t a = 5 * A ;
3 i n t 6 4 t b = 3 * A ;
4 i n t 6 4 t c = a + b ;
5 i f
6 p r i n t f ( ”%d ” , c / A ) ;
( c % A != 0 ) r a i s e e r r o r ( ) ;
Listing 3: Duplicated instructions program
1 i n t 3 2 t a1 = 5 ;
2 i n t 3 2 t b1 = 3 ;
3 i n t 3 2 t c1 = a1 + b1 ;
4 i f
5 p r i n t f ( ”%d ” , c1 ) ;
( c1 != c2 ) r a i s e e r r o r ( ) ;
i n t 3 2 t a2 = 5 ;
i n t 3 2 t b2 = 3 ;
i n t 3 2 t c2 = a2 + b2 ;
Listing 4: Δ-encoded program
1 # d e f i n e A1 9
2 i n t 6 4 t a1 = 5 * A1 ;
3 i n t 6 4 t b1 = 3 * A1 ;
4 i n t 6 4 t c1 = a1 + b1 ;
5 i f
# d e f i n e A2 7
i n t 6 4 t a2 = 5 * A2 ;
i n t 6 4 t b2 = 3 * A2 ;
i n t 6 4 t c2 = a2 + b2 ;
( c1%A1 != 0 | | c2%A2 != 0 | | c1 / A1 != c2 / A2 )
r a i s e e r r o r ( ) ;
6 p r i n t f ( ”%d ” ,
( c1 − c2 ) >> 1 ) ;
AN-encoding exploits information redundancy, i.e., addi-
tional bits are required to store an encoded integer. In practice,
the number of bits to represent encoded integers is doubled.
As an example, consider the addition of two integers 5
and 3 (see Listing 1). For simplicity, we choose A = 11. AN-
encoded integers are thus A · 5 = 55 and A · 3 = 33. These
code words can be directly added and result in a code word:
55 + 33 = 88. Now, if a hardware error would cause any of
the terms to become invalid, the sum will also be an invalid
code. Listing 2 shows an AN-encoded version of the original
addition.
This example highlights two main properties of AN-
encoding: ﬁrst, operations on encoded inputs directly produce
encoded outputs, second, errors in inputs propagate to outputs.
The ﬁrst property means that by substituting all original oper-
ations with encoded operations, the data ﬂow of a program is
protected against hardware faults. The second property implies
that the encoded execution of a program does not require
intermediate checks.
One drawback of AN-encoding is that not all operations
on encoded values are easily implemented. As the previ-
ous example shows, encoded addition corresponds to the
usual arithmetic addition; subtractions and comparisons are
also trivial. However, encoded multiplication, division, bitwise
operations, etc. require more sophisticated implementations.
These complex encoded operations can hamper performance
and/or require intermediate decoding of operands.
Another drawback of pure AN-encoding is that it does not
detect all types of hardware errors. In our previous example, if
the addition operation is erroneously substituted by subtraction,
the result is still a code word, since 55 − 33 = 22. Moreover,
if one of the operands is replaced by some other code word
(due to a fault on the address bus), the result is also a code
word, e.g., 55 + 11 = 66. To detect these types of errors,
variants of AN-encoding were developed, namely ANB- and
ANBD-encodings [14]. Unfortunately, they incur very high
performance penalties (up to 250x) rendering them impractical
in most use cases.
AN codes should not be confused with conventional linear
codes such as Hamming codes or Reed-Solomon codes. Firstly,
the linearity property does not hold in AN codes; secondly,
linear codes are suitable for storage and transmission whereas
AN codes are used in data processing.
In general, AN-encoding has the advantage of detecting
both transient and permanent errors during program execu-
tion; a severe disadvantage is its low performance. Pure AN-
encoding cannot detect all kinds of hardware errors and thus
it does not provide high fault coverage. ANB- and ANBD-
encodings do provide full fault coverage, but at the price of
even higher performance overheads.
B. Duplicated Instructions
Fault tolerance can also be achieved by duplicating all
original instructions in a program. The duplicates work with a
second set of registers and variables, i.e., all data is also dupli-
cated. During execution, “master” and “shadow” instructions
are issued on the same processor; their results are compared
periodically to check for hardware errors. Oh, Shirvani and
McCluskey [15] provide detailed information about error de-
tection by duplicated instructions.
Concerning our previous example of 5 + 3, the addition
operation is issued twice on the CPU, such that two copies
use two different sets of registers. The check operation makes
sure that both copies calculated 8, and if not, a hardware error
is detected. Listing 3 illustrates this.
The duplicated instructions approach assumes that hard-
ware faults are transient and affect only one data-ﬂow copy.
For example, this approach cannot detect hard errors in the
CPU. If the addition operation is permanently faulty, then 5+3
can result in an incorrect value for both copies.
The duplicated instructions technique incurs only modest
performance penalty of 60% [16], since additional instructions
can be effectively scheduled by the compiler and executed by
the CPU in an out-of-order fashion. Indeed, since “master” and
“shadow” execution paths are independent of each other and
require synchronization only at rare check points, the execution
runs essentially in parallel on modern super-scalar processors.
On the whole, the approach of duplicated instructions en-
ables comprehensive protection from transient errors, incurring
only modest execution slowdowns. However, this approach
cannot cope with permanent errors affecting both “master” and
“shadow” copies of data ﬂow.
III. FAULT MODEL
We adopt a data-ﬂow software-level symptom-based fault
model from [11]. This model provides an abstraction of the
underlying hardware and works on the “symptoms” caused by
hardware errors at the software level. Such a model has sev-
eral advantages: (1) it is independent from speciﬁc hardware
models and thus applies to any combination of CPU/RAM, (2)
it does not account for masked hardware faults, i.e., faults that
are neutralized at hardware level, and (3) this fault model can
be easily adapted for fault injection campaigns.