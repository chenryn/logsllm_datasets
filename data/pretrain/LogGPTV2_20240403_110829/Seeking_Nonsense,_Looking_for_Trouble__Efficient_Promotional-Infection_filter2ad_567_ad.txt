directory, along the same path under a compromised FQDN.
This enables us to exploit the URL included in a contaminated
result item (as detected by SEISE) to ﬁnd the promotional
materials unrelated to the context of the IBT in use.
Speciﬁcally, from each ﬂagged FQDN, the IBT Collector
ﬁrst picks up all the URLs leading to malicious content, and
from them, identiﬁes the most commonly shared path under
the FQDN. For example, from the URLs www.lgma.ca.gov/
play/popular/1*.html, www.lgma.ca.gov/play/home/2*.html
and www.lgma.ca.gov/play/club/3*.html (detected using the
IBT ‘casino’), the shared path under the FQDN is www.lgma.ca.
gov/play. Using this path, our approach queries Google again
with ‘site:FQDN+path’: e.g., site:www.lgma.ca.gov/play. From
the results page of the query, critical terms are extracted by
analyzing snippets under individual result items. These terms
are further compared with the semantics of the current sTLD:
those most irrelevant (with a cosine distance above the threshold
0.9) are kept. Finally, the vectors of these terms are clustered
using the classic k-Nearest-Neighbor (k-NN) algorithm (with
k = 10) together with all existing IBTs. Once a new cluster
is formed in this way, we manually look at the cluster and
label it with its semantics (gambling, drug selling, academic
cheating, etc.). Note that this manual step is just for labeling,
not for adjusting the clustering outcomes, which were found
to be very accurate in our research (Section IV-C).
In the above example as illustrated in Figure 4, the query site:
www.lgma.ca.gov/play leads to the search results page. From
the items on the page, the IBT Collector automatically recovers
a set of critical terms, including ‘goldslot’, ‘payday loan’,
‘cheap essay’ and others. Clustering these terms, some of them
are classiﬁed into existing categories such as gambling, drug,
etc., while the rest are grouped into a new cluster, containing
‘cheap essay’, ‘free term paper’ along with other 15 terms.
This new cluster is found to be indeed a new attack category,
and labeled as ‘academia cheating’. In our research, we ran
the approach to extend our IBT set, from 30 terms to 597
effective terms, from 3 categories (gambling, drug, etc.) to 10
large categories (ﬁnancial, cheating, politics, etc.). Our manual
validation shows that the results are mostly correct.
IV. IMPLEMENTATION AND EVALUATION
In this section, we report our implementation of SEISE
and evaluation of its efﬁcacy. Our study show that
the
simple semantics-based approach works well in practice: it
automatically discovered IBTs, achieved an low false detection
rate (1.5%) at over 90% of coverage and also captured 75%
infected domains never reported before (Section IV-C).
A. Implementing SEISE
The design of SEISE (Section III) was implemented into a
prototype system, on top of a set of building blocks. Here we
brieﬂy describe these nuts and bolts and then show how they
are assembled into the system.
Nuts and bolts. Our prototype system was built upon three
key functional components, term extractor, static crawler and
semantic comparator. Those components are extensively reused
across the whole system, as illustrated in Figure 2. They were
implemented as follows:
• Term extractor accepts text as its input, from which it automat-
ically identiﬁes critical terms. The component was implemented
in Python using an open-source tool topia.termextract.
• Static crawler accepts query terms, looks for the terms through
search engines and returns results with a pre-determined number
of items. In our implementation, the crawler was developed in
Python and utilized the Google Web Search API [4] and the
Bing Search API [1] to get search results.
• Semantic comparator accepts a set of terms and compares
them with the keywords of an input sTLD. It can return the
average distance of each term with those keywords or the terms
714714
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:32 UTC from IEEE Xplore.  Restrictions apply. 
whose distances are above a given threshold. This component
was implemented as a Python program that integrates the open-
source tool word2vec. As mentioned earlier, we trained the
language model used by word2vec with the whole Wikipedia
dataset, from which our implementation automatically collected
the context for each term before converting it to a high-
dimensional vector.
System building. Using these building blocks, we constructed
the whole system as illustrated in Figure 2. Speciﬁcally, the
Semantic Finder was developed to run the static crawler
to gather the content under an sTLD and then call
the
term extractor to identify the keywords for the domain. The
Inconsistency Searcher invokes the semantic comparator to
determine the most irrelevant IBTs before using the crawler
to search for the terms. The Context Analyzer includes a
differential analyzer component implemented with around 300
lines of Python code. For each suspicious FQDN, the analyzer
calls the crawler to query the search engine twice, one under an
IBT and the other for getting the reference (the generic content).
It reports the domain considered to be compromised. Finally,
the IBT Collector uses the crawler to search for the selected
URL path under the detected domain, then the extractor to
get critical terms from the search results and the semantics
comparator to ﬁnd out new IBTs. Over these IBTs, we further
integrated the k-NN module provided by the scikit-learn open
source machine learning library [7] to cluster them and discover
new bad-term categories.
B. Experiment Setting
Data collection. To evaluate SEISE, we ran our prototype
on three datasets: the labeled bad set and good set, and the
unknown set including 100K FQDNs collected from search
engines, using 597 search terms, as explicated below.
• Bad set. We collected the FQDNs conﬁrmed to have
promotional infections from CleanMX [18], a blacklist of
compromised URLs. A problem here is that these URLs are
associated with different kinds of malicious activities and it is
less clear whether they are promotional infection. What we did
is to collect all the sTLD URLs from the CleanMX feed from
2015/07 to 2015/08, and further manually inspected all these
URLs. Speciﬁcally, whenever we saw that advertising, Phishing,
defacement content showing up in the search results of a URL,
it is considered to be exploited for promotional infections. We
further classiﬁed these URLs into different categories and also
manually identiﬁed related IBTs. In this way, we built a bad set
with 300 FQDNs (together with 15 IBTs in three categories).
• Good set. Using the IBTs collected from the bad set, we
further searched under the sTLDs for the FQDNs (“site:sTLD+
IBT”) that contained those terms but were not compromised.
These domains were used to understand the false detections
that could be introduced by SEISE. Altogether, we collected a
good set of 300 FQDNs related to 15 IBTs and three categories.
• Unknown set. As mentioned in Section II, we gathered 403
sTLDs and manually selected 30 IBTs in three categories.
Running these IBT seeds on these sTLDs, we crawled Google
and Bing over three months, collecting 100K FQDNs. This
(a) False detection rate in differ-
ent semantics distances. Color bar
shows the coverage rate.
(b) False positive rate in differ-
ent semantics distances. Color bar
shows the coverage rate.
Fig. 5: Evaluation results on good set and bad set.
dataset was used as the unknown set for discovering new
promotional infections.
Resources and validation. In all our experiments, our proto-
type system was run within Amazon EC2 C4.8xlarge instances
equipped with Intel Xeon E5-2666 36 vCPU and 60GiB of
memory. To collect the data for the unknown set, we deployed
20 crawlers within virtual machines with different IP settings.
These crawlers utilized the APIs provided by Google and Bing
to dump the outcomes of the queries, from 2015/08 to 2015/10.
To validate the ﬁndings made on the unknown set, we em-
ployed a methodology that combined anti-virus (AV) scanning,
blacklist checking and manual analysis. Speciﬁcally, for the
FQDN reported by our system, we ﬁrst scanned their URLs
with VirusTotal and considered that the URLs were indeed
suspicious when at least two scanners ﬂagged the domain.
Then, all such suspicious URLs were cross-checked against the
blacklist of CleanMX. For those conﬁrmed by both VirusTotal
and CleanMX, their FQDNs were automatically labeled as
compromised. For other domains also detected by SEISE, we
randomly sampled 20% of them and manually checked whether
they were indeed compromised.
C. Evaluation Results
Over the aforementioned datasets, we thoroughly evalu-
ated our prototype. Our study shows that SEISE is highly
effective: it achieved near zero False Detection Rate (FDR,
i.e., FP/(FP+TP)) and over 90% coverage (i.e., TP/(TP+FN))
or below 4.7% FDR, 4.4% False Positive Rate (FPR, i.e.,
FP/(FP+TN)) and nearly 100% coverage on the labeled sets
(the bad and good set); with the threshold chosen to balance
FDR and FPR, we further ran SEISE over the unknown set,
which reported over 11K compromised sites, with an FDR
of 1.5% and a coverage over 90%. Also importantly, 75% of
infections discovered from the unknown set are likely never
reported before, including 3 large-scale campaigns, on which
we elaborate in Section V. All these ﬁndings were made in
a highly efﬁcient and scalable way: on average, only 2.3
queries were made for ﬁnding a new compromised FQDN
and the delay caused by analyzing the query results and
other computing resources consumed for this purpose were
completely negligible.
Accuracy and coverage. We evaluated the accuracy and
the coverage of SEISE under a given set of IBTs. In this
715715
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:32 UTC from IEEE Xplore.  Restrictions apply. 
case, what can be achieved are all dependent on the Context
Analyzer, which ultimately decides whether to ﬂag an FQDN
as compromised. In our research, we ﬁrst studied our system
over the labeled good set and bad set, and then put it to test
over the unknown set. Figure 5(a) and 5(b) illustrate the results
over the labeled sets, in response to different thresholds for
semantic distances (between the reference and the query of an
IBT). As we can see here, when the threshold goes up, the
FDR goes down and so does the coverage. On the other hand,
loosening the threshold, which means that the IBT is becoming
less irrelevant to the semantics of the sTLD, improves the
coverage, at the cost of the FDR. Overall, the results show
that SEISE is highly accurate: by setting the threshold to 0.9,
we observe almost no false detection (FDR: 0.5% and FPR:
0.4%) with a 92% of coverage; alternatively, if we can tolerate
4.7% FDR (FPR: 4.4%), the coverage becomes close to 100%.
In our research, the threshold 0.9 was then utilized to analyze
the unknown set.
On the unknown set, we ran SEISE to query 597 IBTs under
403 sTLDs. Our prototype inspected 100K FQDNs in total.
11,473 of them were ﬂagged as compromised, about 11% of
the whole unknown set. Table II and Table III summarize our
ﬁndings, which are further discussed in Section V. Among all
that were detected, 3% were conﬁrmed by both VirusTotal [11]
and CleanMX [18], 22% were found by at least one of these
two AV systems and further validated manually, and 1000 of
the remaining were inspected manually. All together, the FDR
measured from the unknown set is as low as 1.5%. We further
randomly sampled 500 result pages related to 10 categories
of IBTs and found that our prototype reported 53 infections
and missed 5, which indicates a coverage of about 90%. Also,
note that over 75% of the infections have never been reported
(missed by both VirusTotal and CleanMX). We have reported
the most prominent ones among them to related organizations
and are helping them ﬁx the problem, and will continue to
work on other cases.
IBT expansion. The effectiveness of SEISE also relies on its
capability to discover new IBTs and ﬁnd new attack instances
across different categories. As discussed before, our prototype
starts with a small set of seed IBTs, 30 terms in three categories.
After searching for all these terms under all the sTLDs, a set
of compromised FQDNs are detected, which are further used
by the IBT Collector to extract new terms for searching all 403
sTLDs again. In our research, we repeated such iteration 20
times, expanding the IBT set to 597 terms and 10 categories.
All the terms and categories were manually conﬁrmed to be
correct. Table I presents the numbers for the terms and the
categories, together with examples of new terms detected, after
the 1st, 5th, 10th, 15th and 20th iterations. As we can see here,
the number of categories and number of IBTs increase quickly
(with a increase rate of 60% and 180%, respectively) in the ﬁrst
10 iterations, which indicate that our IBT expansion method
is efﬁcient for both in-category and cross-category expansion.
Also, Table III illustrates the total categories of IBTs ﬂagged
by SEISE after these iterations.
Performance. We further evaluated the performance of our
TABLE I: Number of IBTs in each round.
# of categories
# of IBTs per category
Avg. length
3
5
8
10
10
10
18
25
40
60
2.6
3.0
3.1
3.2
3.8
Round
0
5
10
15
20
prototype, in an attempt to understand the scalability of our
design. We found that except the delay caused by receiving
the results from Google, the overhead for analyzing search
results and detecting compromised sites are exceedingly low:
by running 10000 randomly selected queries (50 IBTs over 200
sTLDs), we observed that the average time for analyzing 1K
result items, excluding the waiting time for the search engine,
was 1ms, and also the memory and CPU usages stayed below
5% respectively. The main hurdle here is the delay caused
by the search engine: for Google, it ranged from 5ms to 8ms
per one thousand queries. The design of SEISE already limits
the number of queries that needed to be made for detecting
infected FQDNs: in the experiments, we found that on average,
a compromised FQDN was detected after 2.3 term queries. We
believe that by working with the search provider (Google, Bing
etc.), SEISE can be easily scaled with a quick turnaround of
the search results.
V. MEASUREMENT
Based upon what was detected by SEISE, we performed a
measurement study to understand the promotional infections
on sTLDs, particularly the semantic inconsistency these attacks
introduce. Our study brings to light the pervasiveness of the
attacks and their signiﬁcant impacts, affecting the websites of
leading academic institutions and government agencies around
the world. Further discovered are a set of surprising ﬁndings
and their insights, which have never been known before. For
example, apparently sTLDs are soft targets for promotional
infections, highly ranked and also easier to compromise
compared with gTLD sites of similar ranks; as a result, by
mitigating the threats to the sTLD domains, we raise the bar
for the adversary, depriving him of easy access to the resources
highly valuable to the promotional attacks, which rely on the
compromised site’s rank to boost the rating of malicious content.
As another example, we show that semantic inconsistency can
also be observed in the promotional infections on gTLDs
such as .com, .net, etc., even though these domains tend to
have a much more diverse semantic meaning. Based upon this
observation, a preliminary exploration highlights the potential
of extending our approach to protect gTLD sites, indicating
that a semantic model can also be built for some websites under
the gTLD domains to capture the promotional attacks on them.
Finally, we elaborate on a study on some prominent attack
cases discovered in our research, which, from the semantic
perspectives, analyzes the techniques the adversary employ in
the promotional infections.
A. Landscape
Scope and magnitude. Our study reveals that the promotional
infections are spread across the world, compromising websites
716716
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:32 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Top 10 sTLDs with most injected domains.
sTLD
gov.cn
edu.vn
edu