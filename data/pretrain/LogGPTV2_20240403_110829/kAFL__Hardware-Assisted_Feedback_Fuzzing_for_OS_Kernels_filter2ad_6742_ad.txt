We performed ﬁve repeated experiments for each op-
erating system. Additionally we tested TriforceAFL with
the Linux target driver. TriforceAFL is unable to fuzz
Windows and macOS. To compare TriforceAFL with
kAFL, the associated TriforceLinuxSyscallFuzzer was
slightly modiﬁed to work with our vulnerable Linux ker-
nel module. Unfortunately, it was not possible to com-
pare kAFL against Oracle’s ﬁle system fuzzer [34] due
to technical issues with its setup.
During each run, we fuzzed the JSON parser for 30
minutes. The averaged and rounded results are displayed
in Table 1. As we can see, the performance of kAFL
is very similar across different systems. It should be re-
marked that the variance in this experiment is rather high
11http://zserge.com/jsmn.html
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
176    26th USENIX Security Symposium
USENIX Association
Execs/Sec
(1 Process)
150
3000
3000
4300
5100
4300
TriforceAFL
Linux (initramfs)
Debian 8
Debian 8 (KASan)
macOS (10.12.4)
Windows 10
a Not found during 30-minute experiments.
b This value cannot be obtained since TriforceAFL does not synchronize in such short time frames.
(8 Processes)
320
5700
5700
5700
8100
8700
(1 Process)
-a
7:50
4:55
9:20
7:43
4:14
Execs/Sec
Time to Crash
Time to Crash
(8 Processes)
-a
6:00
6:30
6:00
5:10
4:50
Paths/Min
(1 Process)
10.08
15.84
16.20
16.22
14.50
11.50
Paths/Min
(8 Processes)
-b
15.62
16.00
15.90
15.06
12.02
Table 1: kAFL and TriforceAFL fuzzing performance on the JSON sample driver.
Figure 6: Coverage comparison of kAFL (initramfs) and
TriforceAFL. kAFL takes less than 3 minutes to ﬁnd the
same number of paths as TriforceAFL does in 30 minutes
(each running 1 process).
and explains some of the surprising results. This is due to
the stochastic nature of fuzzing, since each fuzzer ﬁnds
vastly different paths, some of which may take signiﬁ-
cantly longer to process, especially crashing paths and
loops. One example for high variance is the fact that on
Debian 8 (initramfs), the multiprocessing conﬁguration
on average needed more time to ﬁnd the crash than one
process.
TriforceAFL We used the JSON driver to compare
kAFL and TriforceAFL with respect to execution speed
and code coverage. However, the results where biased
heavily in two ways: TriforceAFL did not manage to ﬁnd
a path that triggers the crash within 30 minutes (usually
it takes approximately 2 hours), making it very hard to
compare the code coverage of kAFL and TriforceAFL.
The number of discovered paths is not a good indica-
tor for the amount of coverage: With increasing running
time, it becomes more difﬁcult to discover new paths.
Secondly, the number of executions per second is also bi-
ased by slower and harder to reach paths and especially
crashing inputs. The coverage reached over time can be
seen in Figure 6. It is obvious from the ﬁgure that kAFL
found a signiﬁcant number of paths that are very hard to
Figure 7: Raw execution performance comparison.
reach for TriforceAFL. kAFL mostly stops ﬁnding new
paths around the 10-15 minute mark, because the target
driver simply doesn’t contain any more paths to be un-
covered. Therefore, the coverage value in Table 1 (stated
as Paths/Min) is limited to the ﬁrst 10 minutes of each
30-minute run.
We also compare raw execution performance instead
of overall fuzzing performance, which is biased because
of the execution of different paths, the sampling process
for the non-determinism-ﬁlter, and various synchroniza-
tion mechanisms. Especially on smaller inputs, these
factors disproportionately affect the overall fuzzing per-
formance. To avoid this, we compared the performance
during the ﬁrst havoc stage. Figure 7 shows the raw ex-
ecution performance of kAFL compared to TriforceAFL
during this havoc phase. kAFL provides up to 54 times
better performance compared to TriforceAFL’s QEMU
CPU emulation. Slightly lower performance boosts are
seen in single-process execution (48 times faster).
syzkaller We did not perform a performance compari-
son against syzkaller [10]. This has two reasons: First of
all, syzkaller is a highly speciﬁc syscall fuzzer that en-
codes a signiﬁcant amount of domain knowledge and is
therefore not applicable to other domains such as ﬁlesys-
tem images. On the other hand, syzkaller would most
likely generate a signiﬁcantly higher code coverage even
without any feedback since it knows how to generate
USENIX Association
26th USENIX Security Symposium    177
Figure 8: Overhead for compiling QEMU-2.6.0 in a
traced VM.
valid syscalls and hence is able to trigger valid paths
without any learning. Therefore, the coverage compar-
ison would be highly misleading unless we implement
the same syscall logic; a task that is out of the scope of
this paper. Additionally, the coverage collection via kcov
is highly speciﬁc to Linux and not applicable to closed-
source targets.
5.7 KVM-PT Overhead
Our KVM extension KVM-PT adds overhead to the raw
execution of KVM. Therefore, the performance overhead
was compared with several KVM-PT setups on an i5-
PI:EMAIL desktop system with 8GB DDR4 RAM.
This includes KVM-PT in combination with the PT de-
coder, KVM-PT without the PT decoder but processing
frequent ToPA state checks, and KVM-PT without any
ToPA consideration. For this benchmark, a 13MB ker-
nel code range was conﬁgured via IP ﬁltering ranges and
traced with one of the aforementioned setups of KVM-
PT. These benchmarks consider only the kernel core, but
neither considers any kernel module. During KVM-PT
execution only supervisor mode was traced.
To generate Intel PT load, QEMU-2.6.0 was com-
piled within a traced VM using the ./configure option
--target-list=x86_64-softmmu. We restricted trac-
ing to the whole kernel address space. This benchmark
was executed on a single vCPU. The resulting compile
time was measured and compared. The following ﬁgure
illustrates the relative overhead compared to KVM ex-
ecution without KVM-PT (see Figure 8). We ran three
experiments to determine the overhead of the different
components. In each experiment, we measured three dif-
ferent overheads: wall-clock time, user, and kernel. The
difference in overall time is denoted by the wall-clock
overhead. Additionally, we measured how much more
time is spent in the kernel and how much time is spend
only in user space. Since we only trace the kernel, we
expect the users space overhead to be insigniﬁcant. Intel
Figure 9: kAFL and ptxed decoding time on multiple
copies of the same trace (kAFL is up to 30 times faster).
describes a performance penalty of  /dev/null 2> /dev/null
within a Linux VM (Linux debian 4.8.0-1-amd64)
traced by KVM-PT. This performance benchmark was
processed on an PI:EMAIL desktop system with
8GB DDR4 RAM. Only code execution in supervisor
mode was traced. The generated sample is 9.4MB in
size and contains over 431,650 TNT packets, each repre-
senting up to 7 branch transitions. The sample also con-
tains over 100,045 TIPs. We sanitized the sample by re-
moving anything but ﬂow information packets (see Sec-
tion 2.3) to avoid any inﬂuence of decoding large amount
178    26th USENIX Security Symposium
USENIX Association
of execution information packets, since those are not con-
sidered by our PT decoder. The result is a 5.3MB trace
ﬁle. To test the effectiveness of the caching approach of
our PT decoder, we created cases containing 1, 5, 10,
50, and 250 copies of the trace. This is a realistic test
case, since during fuzzing we see the same (or very sim-
ilar) paths repeatedly. Figure 9 illustrates the measured
speedup of our PT decoder compared to ptxed.
The ﬁgure also shows that our PT decoder easily out-
performs the Intel decoder implementation, even if the
PT decoder processes data for the very ﬁrst time. This is
most likely due to the fact that even a single trace already
contains a signiﬁcant amount of loops. Another possible
factor is the use of Capstone [2] as the instruction decod-
ing backend. As we decode more and more copies of the
same trace, it can be seen that our decoder becomes in-
creasingly faster (only using 56 times as much time to
decode 250 times that amount of data). The caching ap-
proach outperforms Intel’s implementation and is up to
25 to 30 times faster.
6 Related Work
Fuzzers are often classiﬁed according to the amount
of interaction with the target program. For black-box
fuzzers, the fuzzer does not use any information about
the target program at all. White-box fuzzers typically use
advanced program analysis techniques to uncover inter-
esting properties of the target. Somewhere in the mid-
dle are so called gray-box fuzzers that will typically use
some kind of feedback from the target (such as cover-
age information) to guide their search, without analyzing
the logic of the target program itself. In this section, we
provide a brief overview of the work performed in the
corresponding areas of fuzzing.
6.1 Black-Box Fuzzers
The oldest class of fuzzers are black-box fuzzers. These
fuzzers typically have no interaction with the target pro-
gram beyond executing it on newly generated inputs. To
increase effectiveness, a number of assumptions are usu-
ally made: Either a large corpus of good coverage inputs
get mutated and recombined repeatedly. Examples for
this class are Radamsa [3] or zzuf [12]. Or, the pro-
grammer needs to specify how to generate new semi-
valid input ﬁles that almost look like real ﬁles. Exam-
ples including tools like Peach [6] or Sulley [9]. Both
approaches have one very important drawback: It is a
time-consuming task to use these tools.
To improve the performance of black-box fuzzers,
many techniques have been proposed. Holler et al.
[27] introduced learning interesting parts of the input
grammar from old crashing inputs. Others even sought
to infer the whole input grammar from program traces
[13,24,38]. The selection of more interesting inputs was
optimized by Rebert et al. [36]. Similar approaches have
been used to optimize the mutation rate [17, 40].
6.2 White-Box fuzzers
To reduce the burden on the tester, techniques where in-
troduced that apply insights from program analysis to
ﬁnd more interesting inputs. Tools like SAGE [23],
DART [22], KLEE [15], SmartFuzz [33], or Mayhem
[16] try to enumerate complex paths by using techniques
such as symbolic execution and constraint solving. Tools
like TaintScope [39], BuzzFuzz [21] and Vuzzer [35]
utilize taint tracing and similar dynamic analysis tech-
niques to uncover new paths. These tools are often able
to ﬁnd very complicated code paths that are hidden be-
hind checksums, magic constants, and other constraints
that are very unlikely to be satisﬁed by random inputs.
Another approach is to use the same kind of information
to bias the search towards dangerous behavior instead of
new code paths [26].
The downside is that these techniques are often signif-
icantly harder to implement, scale to large programs, and
parallelize. To the best of our knowledge, there are no
such tools for operating system fuzzing.
6.3 Gray-Box Fuzzers
Gray-box fuzzers try to retain the high throughput and
simplicity of black-box fuzzers while gaining some of
the additional coverage provided by the advanced me-
chanics in white-box fuzzing. The prime example for
gray-box fuzzing is AFL, which uses coverage informa-
tion to guide its search. This way, AFL voids spend-
ing additional time on inputs that do not trigger new
behaviors. Similar techniques are used by many other
fuzzers [8, 25].
To further increase the effectiveness of gray-box
fuzzing, many of the tricks already used in black-box
fuzzing can be applied. Böhme et al. [14] showed how to
use the insight gained from modelling gray-box fuzzing
as a walk on a Markov chain to increase the performance