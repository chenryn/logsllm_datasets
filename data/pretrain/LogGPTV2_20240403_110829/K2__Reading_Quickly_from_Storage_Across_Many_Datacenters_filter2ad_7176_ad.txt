PaRiS(cid:63). We also implement another baseline, PaRiS(cid:63), which
uses a per-client cache and has at most one round of reads [51].
PaRiS(cid:63) implements a subset of the full design of PaRiS and
provides slightly optimistic lower-bounds on the latency of a
full PaRiS implementation. We modify K2’s implementation
to augment each client with a private cache as in PaRiS. A
Fig. 7: Comparing K2 and RAD on EC2 and Emulab with
the default workload. Results are similar with K2 providing a
signiﬁcant improvement at all percentiles: its average latency
improvement is 297 ms on EC2 and 243 ms on Emulab.
client’s recent writes are kept in its cache for 5 s. This is longer
than they would be in cache for a full PaRiS implementation
which will clear them once their timestamp is passed by the
Universal Stable Time. PaRiS(cid:63)’s read-only transactions take at
most one round of non-blocking remote reads as in PaRiS.
B. Experimental Setup
Most experiments are run on the Emulab testbed [57] where
we have exclusive bare-metal access to 72 machines. Each
machine has one 2.4GHz 64-bit 8-Core E5-2630 “Haswell”
processor, 64GB 2133MT/s DDR4 RAM, and are networked
with 1Gbps Ethernet. Emulab machines are physically co-
located so we emulate the latency between datacenters. We
validate this use of emulated latency on Emulab by running
some experiments on Amazon EC2 in geo-distributed regions.
On EC2 we use t3.2xlarge instances, which have CPU and
memory speciﬁcations comparable to the machines on Emulab
testbed: each has 8 virtual CPUs and 32GB of memory.
Conﬁguration and Workloads. We use 72 machines conﬁg-
ured as 6 datacenters with 4 servers and 8 co-located clients
in each. Machines in the Emulab testbed are physically co-
located, so we use Linux’s tc to emulate wide-area latency
between datacenters. To emulate a globally-distributed de-
ployment, we choose locations that are spread around the
world: Virginia (VA), California (CA), S˜ao Paulo (SP), London
(LDN), Tokyo (TYO), and Singapore (SG). The wide-area
latencies are based on latencies between EC2 regions [11].
Each set of clients reads from and writes to their local data-
center. We measure system throughput as the total throughput
of all datacenters. We generate the workload using Eiger’s
benchmarking system with SNOW’s [40] addition of Zipf
request generation. All experiments use 1 million keys, 128
byte values, 5 keys per operation, and 5 columns per key.
Unless otherwise speciﬁed all experiments use a cache size of
5% of the total keys, a Zipf constant of 1.2, a write percentage
of 1%, a write-only transaction percentage of 50% (of writes),
and a replication factor of 2. We experimentally vary each of
these parameters to observe their effect.
Most experiments use a write percentage of 1% because
most workloads are read heavy. Our choice of 1% is a
compromise between the 0.2% writes reported for Facebook’s
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2 (Emu)K2 (EC2)RAD (Emu)RAD (EC2)(a) Read-only (0% writes)
(b) High skew (zipf=1.4)
(c) High replication factor (f=3)
(d) High write percentage (5%)
(e) Moderate skew (zipf=0.9)
(f) Low replication factor (f=1)
Fig. 8: Read-only transaction latency. K2 provides signiﬁcantly lower latency than PaRiS(cid:63) and RAD at all percentiles for all
tested workloads. The average improvement of K2 over PaRiS(cid:63) and RAD is 53–165 ms and 88–297 ms, respectively.
production TAO system [16], and the 5% writes in YCSB’s
workload B [19]. We also evaluate with other write percent-
ages that match realistic workloads: 0.0% (YCSB’s workload
C [19]), 0.1% (approximate write percentage reported for
Google’s advertising backend F1 on Spanner [20]), 0.2%
(Facebook’s production TAO system), 5% (YCSB’s workload
B). Most experiments use a Zipf constant of 1.2 because
most workloads are highly skewed. We are not aware of
speciﬁc skew numbers for storage systems like K2, so we
based skew on the reported access characteristics of Facebook
photos [31]. Facebook photos were reported to follow a power-
law distribution with α = 1.84, which is equivalent to a Zipf
constant of 1.2. We test with Zipf constants as low as 0.9 and
high as 1.4. A Zipf constant of 1.4 is equivalent to the α = 1.72
power-law distribution observed for Facebook videos [53].
Methodology. To fairly conﬁgure our later experiments we
probed the operation of each system under increasing load.
For each system in the latency experiments, we choose the
number of closed-loop client threads on each of the 48 client
machines where the system operates at medium load. This is in
the appropriate range for production systems [56] and reduces
the effect of queuing delays. Each data point we report is the
median of 3 trials that each last for 12 minutes. This duration
is sufﬁciently long to warm up the cache, i.e., most keys are
requested at least once. We omit the ﬁrst 9 minutes and the
last 20 seconds of each trial to exclude the cache warm-up
period and experimental artifacts.
Validating results on Amazon EC2. We deploy K2 and RAD
on Amazon EC2 datacenters in the six different locations
with actual wide-area latency shown in Figure 6. Network
bandwidth is not the bottleneck in our evaluation settings.
K2’s write-only transaction latency is low on both EC2 and
Emulab since K2 commits writes locally and its commit is not
affected by the network delay. Figure 7 shows the CDFs of
read-only transaction latency under default settings. There are
three differences in the results. First, EC2 results are smoother
due to slight variations in actual latency and noise from the
virtualized environment. Second, the EC2 results have a longer
tail: the 99.9th percentile latency is ~1 second for K2 and
~1.4 seconds for RAD. Third, the latency improvement of
K2 is higher on EC2 than it
the average
latency improvement on EC2 is 297 ms and 243 ms on Emulab.
We observe that the distributions and trends are similar on
Emulab with emulated latency and on EC2 with actual wide-
area delays. We are thus conﬁdent that results from Emulab
with emulated latency are indicative of deployments on cloud
platforms. If there is any appreciable difference, it is that K2
latency improvement would be greater in a deployment on
a cloud platform. We run experiments on Emulab for higher
repeatability and lower cost.
is on Emulab:
C. Read Latency Improvement
K2 is designed to decrease the latency of read-only transac-
tions over partially replicated storage. Figure 8 shows the la-
tency of read-only transactions in K2, RAD, and PaRiS(cid:63) under
a variety of workloads. We ﬁnd that K2 signiﬁcantly improves
latency compared to RAD and PaRiS(cid:63) at all percentiles in all
evaluated workloads. The magnitude of these improvements
varies with the workload. In most workloads, K2 provides an
average latency improvement of 140–297 ms over RAD, and
53–165 ms over PaRiS(cid:63). This signiﬁcant latency improvement
of K2 over the baselines is enabled by K2’s novel design which
optimizes latency of read-only transactions by providing all-
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2PaRiS*RAD 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2PaRiS*RAD 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2PaRiS*RAD 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2PaRiS*RAD 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2PaRiS*RAD 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600CDFRead-only Transaction Latency (ms)K2PaRiS*RADDefault Replication Write (%)
Zipf
K2
15
41.6
44.3
RAD 24.8
24.8
Fig. 9: Throughput (K txns/sec) under different settings.
f=1
21.1
11.7
f=3
53.7
51.9
0.1
47.7
59.0
5
26.0
20.2
0.9
21.3
85.4
1.4
46.3
14.8
Cache (%)
1
30.9
24.8
local latency more often and guaranteeing the best possible
worst-case latency.
More All-Local Latency. RAD does not cache non-replica
data in datacenters, so any read-only transaction that accesses
non-replica data must go to a remote datacenter. This happens
>99% of the time in all workloads as shown by RAD’s 1st
percentile latency being >60ms, the lowest inter-datacenter
latency. PaRiS(cid:63) uses a per-client cache to keep client’s recent
writes. PaRiS(cid:63) provides local latency when all requested keys
are replica keys or are stored in the client’s private cache.
This happens 60ms. >95%
of PaRiS(cid:63)’s read-only transactions must contact a remote
datacenter and thus incur high latency. K2 caches a small
percentage of non-replica data in each datacenter and uses
them when safe. This allows K2 to serve many read-only
transactions entirely locally.
K2 provides local latency for 19–83% of read-only trans-
actions depending on the workload. K2’s most signiﬁcant
improvements are for the highly skewed workload (8b), the
high replication factor (8c), and the read-only workload (8a).
Its smallest improvement comes with a moderately skewed
workload (8e). This is as expected because more skewed
workloads are easier to cache. The percentage of transactions
with all-local latency decreases with a higher write percentage
(8d) and with a lower replication factor (8f). These changes
are also due to changes in the effectiveness of the cache. For
instance, increasing the replication factor from 2 to 3 results
in 33% less non-replica data vying for a spot in the cache.
Best Possible Worst-Case Latency. K2 and PaRiS(cid:63) achieve
the best possible worst-case latency for read-only transactions
on partially-replicated data:
they need at most one inter-
datacenter round trip to fetch the values of non-replica keys.
In contrast, RAD needs two inter-datacenter round-trips if
the non-replica keys fetched in the ﬁrst round of the read-
only transaction are not consistent. Figures 8b, 8d, and 8f
show workloads where RAD issues the second round of
remote reads often: high skew, high write percentage, and low
replication factor. In each of these workloads 91–98% of read-
only transactions take two wide-area rounds.
Facebook TAO Workload. We experiment with a syn-
thetic workload that uses the value sizes, columns/key, and
keys/operations reported for Facebook’s TAO system [16],
[39]. We use the default Zipf constant of 1.2 since it is not
reported in TAO. We ﬁnd that K2 provides local latency for
73% of read-only transactions, while PaRiS(cid:63) and RAD achieve
local latency for <1% of read-only transactions.
D. Throughput, Write Latency, Staleness
Throughput Comparison. K2 aims to avoid remote reads
by leveraging cached values and thus can potentially improve
throughput by reducing the number of requests in the system.
However, K2 has three sources of overhead: replicating meta-
data to non-replicas, doing dependency checks before applying
replicated metadata, and returning multiple versions in its
read-only transaction algorithm. We quantify the throughput
overhead of K2 compared to RAD.
Figure 9 shows the peak throughput of the systems for
several settings using the minimum and maximum values of
each parameter while keeping the others at their default. We
observe that in many settings (e.g., high write percentage of
5%, and highly skewed Zipf constant of 1.4), K2 provides
higher throughput than RAD. Under these workloads, RAD
often needs the second round of reads to request consistent
versions of the contended keys and is bottlenecked by a small
set of servers. K2 avoids the bottleneck better by allowing
each datacenter to read a slightly older, consistent version of
highly contended keys from its local cache, and thus avoids
imposing high remote read loads on the replica datacenters of
those keys. In some settings (e.g., a moderately skewed Zipf
constant of 0.9), we ﬁnd that RAD has higher throughput than
K2. Unlike K2, each datacenter in RAD handles dependency
checks and replication for only replica keys, leaving more CPU
and memory capacity to serve local client requests.
Write Latency. K2 achieves much lower write latency than
RAD for single-key writes and write-only transactions because
K2 can commit write operations locally, while RAD often must
contact remote datacenters. For instance, under our default
settings K2’s 99th percentile latency is 23 ms for write-only
transactions while RAD’s 50th percentile latency is 147 ms for
simple writes and 201 ms for write-only transactions.
Data Staleness. K2 aims to satisfy read requests entirely in-
side a local datacenter by leveraging older cached versions and
thus accepts some staleness for better performance. Staleness
is measured on servers as the time since a newer version of
that key has been written. For instance, if the returned version
is the newest version on the server, the staleness is 0. Or, if the
returned version was overwritten by a newer version 100ms
ago, the staleness is 100ms. RAD provide 0 staleness if its
read-only transactions complete in one round. We quantiﬁed
the staleness in K2 for write percentages between 0.1–5%. The
median staleness is 0 ms for all cases, 75th percentile staleness
is 105ms or less, and 99th percentile staleness is between 516
and 1117 ms. We expect this staleness to be an acceptable
tradeoff for the lower latency provided by K2.
VIII. RELATED WORK
This section reviews previous partially replicated systems,
fully replicated systems, and systems that provide causal con-
sistency. K2 is primarily distinguished by being the ﬁrst work
to realize the low latency beneﬁt of many datacenters with
strong guarantees: causal consistency, read-only transactions,
and write-only transactions.
Partially-Replicated Data Stores. PRACTI [12] is a classical
partial replication system that supports topology independence,
i.e., any-to-any replica propagation, and provides arbitrary
consistency. K2 builds on PRACTI’s insight to separate the
control path (metadata) and the data path (data replication).
One difference in K2 is our use of a cache—and our algorithms
that exploit
it for many clients—in each datacenter, and
the constrained replication topology to provide non-blocking
remote reads. More importantly, PRACTI was designed for a
different era when all data that would be accessed together
could ﬁt on a single machine. Hence, its design is based on