1
10
1
10
10
2
2
10
3
10
3
10
3
10
4
10
10
4
4
10
4
10
5
10
5
10
10
5
6
10
6
10
6
10
7
10
7
10
10
7
8
10
8
10
8
10
9
10
9
10
10
9
Chess Matching
12 trials
12 trials
N V
N V
0
4
3
16
0
4
3
16
1
4
5
16
16
5
4
1
1
4
7
16
1
4
16
7
1
4
16 11
1
4
16 11
1
4
3
36
36
3
4
1
2
4
5
36
2
4
5
36
2
4
7
36
2
4
36
7
36 11
4
2
2
4
3
64
2
4
3
64
2
4
5
64
3
4
5
64
3
4
7
64
64
7
4
3
3
4
64 11
3
4
64 11
3
4
3
4
3
4
4
3
4
4
4
4
All participants completed a consent form and then a short
demographic survey (reproduced in Appendix A), after which
they were given brief overall instructions:
This experiment is divided into several tasks. To
proceed to the ﬁrst task, click on its heading, which
is right below these instructions. When you complete
each task, the heading for the next task will become
selectable.
The tasks all included their own speciﬁc instructions, which
are reproduced in Figure 2 above the facsimile of each task.
Each task also included a progress bar at the bottom of its
screen area (not shown in Figure 2) which indicated the number
of trials remaining for that task. When participants reached
the end of a subtask, the page showed some graphs of their
performance on that task, as a reward (we do not show any of
these graphs here, to avoid confusion with our actual analysis).
At the very end of the experiment, participants were thanked
for their assistance and offered an opportunity to see all of the
data collected (in its raw form) before sending it to our server.
The typing tasks gave no feedback until the end, but the
clicking tasks indicated errors immediately. In the chessboard
task, each pawn turned green when clicked, but if a participant
clicked on an empty square, a red X would appear in that
square. In the matching task, when a small image was clicked,
153
Fig. 4. Overall accuracy rates for the four interactive tasks.
Fig. 5. Queries per minute achieved by the four interactive tasks (black) and
three automated exploits (gray).
its brown border would turn blue if that was the correct choice,
red if not. In both cases, participants had to produce the correct
answers before the task would end. A real attack could respond
to clicks in a similar fashion, but might not be able to give
exactly the same error feedback, because of the limitations on
visited-link styles imposed by Baron’s defense. For instance, a
version of the chessboard task that really sniffed history could
turn visible pawns green when clicked, and could cause red
pawns to appear in squares that had been empty before the
click, but could not convert invisible pawns to visible Xes
upon a click.
It was possible for participants to refuse to carry out the
typing tasks, by hitting the RETURN key over and over again
without typing anything. The matching task could also be
skipped, via an explicit “skip this task” button, because our
implementation sometimes malfunctioned and we were not
able to isolate the bug, so we had to give people a way to
move on. The chessboard task, however, could not be skipped
or refused.
For comparison purposes, we also ran three automated
history-snifﬁng exploits on all the participants. Less than 13%
of the participants were using a browser that blocked these
exploits; see Section IV-E below for more on the experiment
population. We used wtikay.com’s set of 7012 commonly
visited URLs (derived from the Alexa top 5000 sites list [15],
[28]) for this test; we recorded only the total elapsed time and
the number of URLs detected as visited.
C. Results
Not all of the participants completed all of the tasks success-
fully, but we have usable data from at least 177 participants for
each task. Figure 4 shows raw user accuracy rate for all four
tasks. The chessboard takes ﬁrst place in accuracy, with nearly
all participants scoring 100% or close to. The word CAPTCHA
is substantially easier than the character CAPTCHA; the visual
matching task is dead last in terms of average accuracy, but the
character CAPTCHA has a surprising number of outliers with
very poor accuracy. We investigated these, and found that some
participants became so frustrated with the task that after a few
trials they started hitting RETURN without attempting to type
anything. There are even a few 0% scores, from participants
who would not do this task at all. It is well known that strings
of meaningless characters are harder to type than strings of
words [29], but we did not anticipate this level of frustration.
Figure 5 shows the achievable history-snifﬁng rate for each
task, with the rate of “traditional” automated attacks included
for comparison. Of the four interactive tasks, the chessboard
puzzle is the clear winner, achieving a median of nearly
1000 queries per minute. It should be remembered that this
measurement combines two factors: how fast a victim can do
the task, and how many URLs the task encodes. The chessboard
scores highly on both counts, but the character CAPTCHA
is only in second place because it encodes many URLs.
Conversely, the word CAPTCHA is quick to complete, but
doesn’t encode many URLs and therefore falls behind on QPM.
Matching does poorly on both factors. And, unsurprisingly,
all of our interactive tasks are much slower than automated
snifﬁng.
Since our study conditions are artiﬁcial, our participants’
performance (either speed or accuracy) does not translate
directly to attack effectiveness under “wild” conditions. We
challenged participants to carry out dozens of instances of
our tasks in quick succession, whereas a real attack would
require victims to complete only one instance (except perhaps
for the pattern-matching task). However, we did not observe
any signiﬁcant effect of fatigue in our tests, except for the
participants who refused to complete all the requested trials of
the character CAPTCHA. Some of the errors on the typing tasks
were caused by participants entering something completely
unexpected, rather than a possible but incorrect answer; in a
real attack, if this happened, the attacker would have to default
to some assumption about the links it was probing (most likely,
that none of them were visited) which might chance to be
correct. These effects would tend to make a genuine attack
more effective than our results indicate.
On the other hand, our participants were told in advance that
their ability to carry out the tasks quickly and accurately was
being measured; people are known to perform better on tasks
of this nature when they know their performance is being tested
154
Pattern matchChessboardChar. CAPTCHAWord CAPTCHAGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG0%20%40%60%80%100%Auto (timing)Auto (indirect)Auto (direct)Pat. matchChessboardChar. CAPTCHAWord CAPTCHAGGGGGGGGGGGGGGGGGGGGGGGGGGGGG101102103104105Fig. 8. Browsers used by participants
clearly quite small, so attackers may be able to assume a
sparse set of visited links. However, as pointed out by Janc
and Olejnik [15], sparseness over this generic link set may not
equate to sparseness over a more targeted set—and the link
sets found by Jang were quite targeted indeed.
E. Participant Demographics
We asked participants a few general questions about them-
selves; the results are shown in Figure 7. As the leftmost graph
in Figure 7 shows, the study population is strongly skewed
to younger users, much more so than the (USA) Internet-
using population [32]. Participants also appear more likely
than average to own more than one computer, use the Internet
frequently, have used computers for more than ten years despite
their youth, and to report having at least tried to put together
a website before. This is consistent with other analyses of the
demographics of Mechanical Turk workers speciﬁcally [33],
[34]. We expect that our conclusions about interactive tasks
remain valid for Internet users at large, since they rely mostly
on measurements of basic motor activities (typing, mousing).
Our participants used a wide variety of browsers, with the
three most popular being Firefox 3.6, Chrome 7, and IE 8.
Despite its place in the top three, less than 20% of participants
used IE 8, and no older versions of IE were detected; this also
indicates a more technically experienced population than the
average. The full breakdown is in Figure 8. We did not record
participants’ operating systems, or any other User-Agent data
beyond what is shown. Safari 5, Firefox 4, and Chrome 9
are the browsers that, at the time of the study, implemented
Baron’s defense against automated history snifﬁng; users of
these browsers made up 13% of our survey population.
F. Discussion
We have shown that interactive attacks on visited-link history
are feasible, particularly if the attacker is interested only in a
small set of links, as were the real history sniffers found by Jang.
If we wish to defend against these attacks we must consider
further restricting the functionality of visited-link history—
either the circumstances under which links are revealed to be
visited, or the capabilities of visited-link styles.
Three of our four interactive attacks relied on making
unvisited links invisible by blending them into the background.
Fig. 6. Histogram of percentage of links visited within wtikay.com’s set
of 7012 commonly visited URLs (derived from the Alexa top 5000 sites), as
measured by an automated history exploit. No participant had visited more
than a tiny fraction of these URLs.
(the “Hawthorne effect” [30]). Even if we had made the task
conditions mimic a real attack more precisely—perhaps we
could have claimed that we were evaluating the usability of new
CAPTCHA styles—our participants might have deduced that
their performance was being tested. Furthermore, Mechanical
Turk workers are paid for every task they complete, so the
faster they do tasks, the more money they earn; our participant