title:Fashion crimes: trending-term exploitation on the web
author:Tyler Moore and
Nektarios Leontiadis and
Nicolas Christin
Fashion Crimes: Trending-Term Exploitation on the Web
Tyler Moore
Wellesley College∗
PI:EMAIL
Nektarios Leontiadis
Carnegie Mellon University
PI:EMAIL
Nicolas Christin
Carnegie Mellon University
PI:EMAIL
ABSTRACT
Online service providers are engaged in constant conﬂict with mis-
creants who try to siphon a portion of legitimate trafﬁc to make
illicit proﬁts. We study the abuse of “trending” search terms, in
which miscreants place links to malware-distributing or ad-ﬁlled
web sites in web search and Twitter results, by collecting and ana-
lyzing measurements over nine months from multiple sources. We
devise heuristics to identify ad-ﬁlled sites, report on the prevalence
of malware and ad-ﬁlled sites in trending-term search results, and
measure the success in blocking such content. We uncover collu-
sion across offending domains using network analysis, and use re-
gression analysis to conclude that both malware and ad-ﬁlled sites
thrive on less popular, and less proﬁtable trending terms. We build
an economic model informed by our measurements and conclude
that ad-ﬁlled sites and malware distribution may be economic sub-
stitutes. Finally, because our measurement interval spans February
2011, when Google announced changes to its ranking algorithm
to root out low-quality sites, we can assess the impact of search-
engine intervention on the proﬁts miscreants can achieve.
Categories and Subject Descriptors
K.4.1 [Public Policy Issues]: Abuse and crime involving comput-
ers
General Terms
Measurement, Security, Economics
Keywords
Online crime, search engines, malware, advertisements
1.
INTRODUCTION
News travels fast. Blogs and other websites pick up a news story
only about 2.5 hours on average after it has been reported by tra-
ditional media [21]. This leads to an almost continuous supply of
∗This work was primarily done while T. Moore was at Harvard
University.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.
Figure 1: Ad-ﬁlled website appearing in the results for trending
terms (only 8 words from the article, circled, appear on screen).
new “trending” topics, which are then ampliﬁed across the Internet,
before fading away relatively quickly.
However narrow, these ﬁrst moments after a story breaks present
a window of opportunity for attackers to inﬁltrate web and social
network search results in response. The motivation for doing so is
primarily ﬁnancial. Websites that rank high in response to a search
for a trending term are likely to receive considerable amounts of
trafﬁc, regardless of their quality. Web trafﬁc can in turn be mone-
tized in a number of ways, as shown in related work [6, 10, 17, 20].
In short, manipulation of web or social network search engine re-
sults can be a proﬁtable enterprise for its perpetrators.
In particular, the sole goal of many sites designed in response
to trending terms is to produce revenue through the advertisements
that they display in their pages, without providing any original con-
tent or services. Figure 1 presents a screenshot for eworldpost.
com, which has appeared in response to 549 trending terms be-
tween July 2010 and March 2011. The actual article (circled) is
hard to ﬁnd, when compared to the amount of screen real estate
dedicated to ads. Such sites are often referred to as “Made for
AdSense” (MFA) after the name of the Google advertising plat-
form they are often targeting. Whether such activity is deemed
to be criminal or merely a nuisance remains an open question, and
largely depends on the tactics used to prop the sites up in the search-
engine rankings. Some other sites devised to respond to trending
terms have more overtly sinister motives. For instance, a number
of malicious sites serve malware in hopes of infecting visitors’ ma-
chines [30], or peddle fake anti-virus software [2, 8, 36].
Both MFA and malware-hosting sites are enough of a scourge
to trigger response from search engine operators. Google modi-
ﬁed its search algorithm in February 2011 in part to combat MFA
sites [35], and has long been offering the Google Safe Browsing
API to block malware-distribution sites. Trending-term exploita-
455tion makes both MFA and malware sites even more dynamic than
they used to be, thereby complicating the defenders’ task.
This paper provides the ﬁrst large-scale measurement and anal-
ysis of trending-term exploitation on the web. Based on a collec-
tion of over 60 million search results and tweets gathered over nine
months, we characterize how trending terms are used to perform
web search-engine manipulation and social-network spam. An im-
portant feature of our work is that we bring an outsider’s perspec-
tive. Instead of relying on proprietary data tied to a speciﬁc search
engine, we use comparative measurements of publicly observable
data across different web search engines (Google, Yahoo!/Bing)
and social network (Twitter) posts.
Our speciﬁc contributions are as follows. We (1) provide a method-
ology to automate classiﬁcation of websites as MFA, (2) show salient
differences between tactics used by MFA site operators and mal-
ware peddlers, (3) construct an economic model to characterize the
trade-offs between advertising and malware as monetization vec-
tors, quantifying the potential proﬁt to the perpetrators, and (4) ex-
amine the impact of possible intervention strategies.
The rest of this paper is organized as follows. We introduce
our measurement and classiﬁcation methodology in Section 2. We
analyze the measurements collected in Section 3 to characterize
trending-term exploitation on the web. Notably, we uncover collu-
sion across offending domains using network analysis, and we use
regression analysis to conclude that both malware and MFA sites
thrive on less popular and proﬁtable trending terms. We then use
these ﬁndings to build an economic model of attacker revenue in
Section 4, and examine the effect of search-engine intervention in
Section 5. We compare our study with related work in Section 6,
before drawing brief conclusions in Section 7.
2. METHODOLOGY
We start by describing our methodology for data collection and
website classiﬁcation. At a high level, we need to issue a number
of queries on various search engines for current trending terms, fol-
low the links obtained in response to these queries, and classify the
websites we eventually reach as malicious or benign. Within the
collection of malicious sites so obtained, we have to further distin-
guish between malware-hosting sites and ad-laden sites. Moreover,
we need to compare the results obtained with those collected from
“ordinary,” rather than trending, terms.
The data collection hinges on a number of design choices that we
discuss and motivate here. Speciﬁcally, we must determine how to
build the corpus of trending terms to use in queries (“trending set”);
identify a set of control queries (“control set”) against which we can
compare responses to queries based on trending terms; decide on
how frequently, and for how long, we issue each set of queries; and
ﬁnd mechanisms to classify sites as benign, malware-distributing,
and MFA.
2.1 Building query corpora
Building a corpus of trending terms is not in itself a challenging
exercise. Google, through Google Hot Trends [15], provides a list
of twenty current “hot searches,” which we determined, through
pilot experiments, to be updated hourly. Likewise, Twitter avails a
list of ten trending topics [37] and Yahoo! gives a “buzz log” [38]
containing the 20 most popular searches over the past 24 hours.
These different lists sometimes have very little overlap. For
instance, combining the 20 Yahoo! Buzz logs, 20 Google Hot
Trends, and 10 Twitter Trending Topics, it is not uncommon to ﬁnd
more than 40 distinct trending terms over short time intervals. This
would seem to make the case for aggregating all sources to build
our query corpus. However, all search APIs limit the rate at which
queries can be issued. We thus face a trade-off between the time
granularity of our measurements and the size of our query corpus.
Trending set. Fortunately, we can capture most of the interesting
patterns we seek to characterize by solely focusing on Google Hot
Trends.
Indeed, a recent measurement study conducted by John
et al. [17] shows that over 95% of the terms used in search en-
gine manipulation belong to the Google Hot Trends. However, be-
cause Twitter abuse may not necessarily follow the typical search
engine manipulation patterns, we use both Google Hot Trends and
the Twitter current trending topics in our Twitter measurements.
Hot trends, by deﬁnition, are constantly changing. We update
our trending term corpus every hour by simply adding the current
Google Hot Trends to it. Determining when a term has “cooled”
and should be removed from the query corpus is slightly less straight-
forward. We could simply remove terms from our query corpus as
soon as they disappear from the list of Google Hot Trends. How-
ever, unless all miscreants stop poisoning search results with a
given term as soon as this term has “cooled,” we would likely miss
a number of attempts to manipulate search engine results. Further-
more, Hot Trends are selected based upon their rate of growth in
query popularity. Terms that have fallen out of the list in most
cases still enjoy a sustained period of popularity before falling.
We ran a pilot experiment collecting Google and Twitter search
results on 20 hot terms for up to four days. As Figure 2(a) shows,
95% of all unique Google search results and 81% of Twitter results
are collected within three days. Thus, we settled on searching for
trending terms while they remain in the rankings, plus up to three
days after they drop out of the rankings.
Control set. It is necessary to compare results from the trending
set to a control set of consistently popular search terms, to identify
which phenomena are unique to the trending nature of the terms as
opposed to their overall popularity. We build a control list of the
most popular search terms in 2010 according to Google Insights for
Search [13]. Google lists the top 20 most popular search terms for
27 categories. These reduce to 495 unique search terms, which we
use as a control set.
2.2 Data collection
For each term in our trending and control sets, we run automated
searches on Google and Yahoo! between July 24, 2010 and April
24, 2011. We investigate MFA results throughout that period, and
study the timeliness of malware identiﬁcation between January 26
and April 24, 2011. We study Twitter results gathered between
March 10 and April 18, 2011.
We use the Google Web Search API [1] to pull the top 32 search
results for each term from the Google search engine, and the Ya-
hoo! BOSS API to fetch its top 100 Yahoo! results for each term.
Since the summer of 2010 Yahoo! and Bing search results are
identical [23]. Consequently, while in the paper we refer to Ya-
hoo! results, they should also be interpreted as those appearing on
Bing. Likewise, we use the Twitter Atom API to retrieve the top 16
tweets for each term in Google’s Hot Trends list and Twitter’s Cur-
rent Trends list. We resolve and record URLs linked from tweets,
as well as the authors of these tweets linking to other sites.
Because all these APIs limit the number of queries that can be
run, we had to limit the frequency with which we ran the search
queries. To better understand the trade-offs between search fre-
quency and comprehensiveness of coverage, we selected 20 terms
from a single trending list and ran searches using the Google API
every 10 minutes for one week. We then compared the results we
could obtain using the high-frequency sampling to what we found
when sampling less often. The results are presented in Figures 2(b)
and 2(c). Sampling once every 20 minutes, rather than every 10
456d
e
t
c
e
l
l
o
c
s
t
l
u
s
e
r
l
a
t
o
t
%
0
0
1
0
8
0
6
0
4
0
2
0
s
L
R
U
t
c
n
i
t
s
d
i
s
L
R
U
t
c
n
i
t
s
d
i
0
0
4
1
0
0
3
1
0
0
2
1
0
0
0
6
1
0
0
0
0
1
0
0
0
4
Google
Twitter
Google
50
100
150
200
Twitter
s
t
l
u
s
e
r
d
e
s
s
m
i
f
o
%
0
8
0
6
0
4
0
2
0
Google
Twitter
0
20
40
60
80
100
50
100
150
200
Hours since start of collection
collection intervals (minutes)
50
100
150
200
collection intervals (minutes)
(a) New search results as a function of time
in Twitter and Google. More than 80% of
Google results appear within 3 days, while
Twitter continuously produces new results.
(b) Number of distinct URLs collected us-
ing different collection intervals. The mea-
surement lasted for two weeks using a ﬁxed
set of terms that was trending at the begin-
ning of the experiment.
(c) Number of distinct URLs that we failed
to collect using different collection inter-
vals.
lasted for two
weeks using a ﬁxed set of terms that was
trending at the beginning of the experiment
The measurement
Figure 2: Calibration tests weigh trade-offs between comprehensiveness and efﬁciency for collecting trending-term results.
minutes, caused 4% of the Google search results to be missed.
Slower intervals caused more sites to be missed, but only slightly:
85% of the search results found when reissuing the query every
10 minutes could also be retrieved by sampling only once every
4 hours. So, even for trending topics, searching for the hot terms
once every four hours provides adequate coverage of Google re-
sults. For consistency, we used the same interval on Twitter despite
the higher miss rate. Twitter indeed continues producing new re-
sults over a longer time interval, primarily due to the “Retweet”
function which allows users to simply repost existing contents.
2.3 Website classiﬁcation
We next discuss how we classiﬁed websites as benign, malware-
distributing, or ad-ﬁlled. We deﬁne a website as a set of pages
hosted on the same second-level DNS domain. That is, this.
example.com and that.example.com belong to the same
website.1 While we realize that different websites may be hosted
on the same second-level domains, they are ultimately operated or
endorsed by the same entity – the owner of the domain. Hence, in a
slight abuse of terminology we will equivalently use “website” and
“domain” in the rest of this discussion.
Malware-distributing sites. We pass all search results to Google’s
Safe Browsing API, which indicates whether a URL is currently
infected with malware by checking it against a blacklist. Because
the search results deal with timely topics, we are only interested in
ﬁnding which URLs are infected near the time when the trending
topic is reported. However, there may be delays in the blacklist
updates, so we keep checking the results against the blacklist for
14 days after the term is no longer hot.
When a URL appears in the results and is only later added to the
blacklist, we assume that the URL was already malicious but not
yet detected as such. It is, of course, also possible that the reason
the URL was not in the blacklist is that the site had not yet been
infected. In the case of trending terms, however, a site appearing
in results indicates a likely compromise, since the attacker’s modus
operandi is to populate compromised web servers with content that
reﬂects trending results [17].
1So do this.example.co.uk and that.example.co.uk,
as co.uk is considered a top-level domain; as are a few others
(e.g., ac.jp) for which we maintain an exhaustive list.
The possibility of later compromise further justiﬁes our decision
to stop checking the search results against the blacklist after two
weeks have passed. While it is certainly possible that some mal-
ware takes more than two weeks to be detected, the potential for
prematurely ﬂagging a site as compromised also grows with time.
Indeed, in a study of spam on Twitter [10], the majority of tweets
ﬂagged by the Google Safe Browsing API as malicious were not
added to the blacklist until around a month had passed. We sus-
pect that many of the domains marked as malicious were in fact
only compromised much later. Consequently, our decision to only
ﬂag malware detected within two weeks is a conservative one that
minimize false positives while slightly increasing false negatives.
Dealing with long-delayed reports of malware poses an addi-
tional issue for terms from the control set, because these search
results are more stable over time. Sometimes a URL appears in the
results of a term for years. If that website becomes infected, then it
would clearly be incorrect to claim that the website was infected but
undetected the entire time. In fact, most malware appearing in the
results for the control set are for websites that have only recently
“pushed” their way into the top search results after having been
infected. For these sites, delays in detection do represent harm.
We thus exclude from our analysis of malware in the control set
URLs that appeared in the results between December 20-31, 2010,
when we began collecting results for the control set. To eliminate
the potential for edge effects, our analysis of malware does not be-
gin until January 26, 2011. As in the trending set, we also only ﬂag
results as malware when they are detected within 14 days.
Finally, we note that sometimes malware is undetected by the
SafeBrowsing API on the top-level URL, but that URLs loaded ex-
ternally by the website are blocked. Consequently, our analysis
provides an upper bound on malware success.
MFA sites. Automated identiﬁcation of MFA sites is a daunting
task. There are no clear rules for absolutely positive identiﬁcation,
and even human inspection suggests a certain degree of subjectivity
in the classiﬁcation. We discuss here a set of heuristics we use in
determining whether a site is MFA or not.
While 182 741 different domains appeared in the top 32 Google
and Yahoo! search results for trending terms over 9 months, only
6 558 (3.6%) appeared in the search results for at least 20 different
trending terms. Because the goal of MFA sites is to appear high in
the search results for as many terms as possible, we investigate fur-
457ther which of these 6 558 websites are in fact legitimate sources of
information, and which are low-quality, ad-laden sites. To that ef-
fect we selected a statistically signiﬁcant (95% conﬁdence interval)
random sample of 363 websites for manual inspection. From this
sample, we identiﬁed ﬁve broad categories of websites indicative
of MFA sites. All MFA sites appear to include a mechanism for
automatically updating the topics they cover; differences emerge in
how the resulting content is presented.
1. Sites which reuse snippets created by search engines and provide
direct links to external sites with original content (e.g., http://
newsblogged.com/tornado-news-latest-real-time).
2. Sites in blog-style format, containing a short paragraph of con-
tent that is likely copied from other sources and only slightly tweaked
– usually by a machine algorithm, rather than a human editor (e.g,
http://toptodaynews.com/water-for-elephants-
review).
3. Sites that automatically update to new products for sale point-
ing to stores through paid advertisements (for instance, http:
//tgiblackfriday.com/Online-Deals/-261-up-
Europe-On-Sale-Each-Way-R-T-required--deal).
4. Sites aggregating content by loading external websites into a
frame so that they keep the user on the website along with their