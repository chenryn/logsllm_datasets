LLooggFFiiTT:: LLoogg AAnnoommaallyy DDeetteeccttiioonn uussiinngg FFiinnee--TTuunneedd LLaanngguuaaggee
MMooddeellss
This paper was downloaded from TechRxiv (https://www.techrxiv.org).
LICENSE
CC BY 4.0
SUBMISSION DATE / POSTED DATE
17-03-2023 / 21-03-2023
CITATION
Almodovar, Crispin; Sabrina, Fariza; Karimi, Sarvnaz; Azad, Salahuddin (2023): LogFiT: Log Anomaly
Detection using Fine-Tuned Language Models. TechRxiv. Preprint.
https://doi.org/10.36227/techrxiv.22290982.v1
DOI
10.36227/techrxiv.22290982.v1
1
LogFiT: Log Anomaly Detection using Fine-Tuned
Language Models
Crispin Almodovar1 Fariza Sabrina1 Sarvnaz Karimi2 Salahuddin Azad1
1Central Queensland University, Australia
2CSIRO Data61, Sydney, Australia
PI:EMAIL
{f.sabrina,s.azad}@cqu.edu.au
{sarvnaz.karimi}@csiro.au
Abstract—System logs are a valuable source of information Due to the high cost of preparing labeled data, classification-
for monitoring and maintaining the security and stability of based methods such as LogSy [6] have limited utility in
computer systems. Techniques based on Deep Learning and
production settings. As a result, the majority of techniques
NaturalLanguageProcessinghavedemonstratedeffectivenessin
usedindetectingloganomaliesassumeazero-positivetraining
detecting abnormal behavior from these system logs. However
existing approaches are inflexible and impractical: techniques scenario, where normal log data is the only data available for
that rely on log templates are unable to handle variability in self-supervised training [7] only. Furthermore, [8] considers
logcontent,whileclassification-basedapproachesrequirelabeled two categories of self-supervised models for log anomaly
data for supervised training. In this paper, a novel log anomaly
detection. The first is forecasting-based models which attempt
detectionmodelnamedLogFiTisproposed.TheLogFiTmodelis
to predict upcoming log entries by analyzing previous log
robusttochangesinlogcontentandonlyrequiresself-supervised
training. The LogFiT model uses a pretrained BERT-based entries, while the second is reconstruction-based models
language model fine-tuned to recognise the linguistic patterns of which reconstruct log sequences that have been intentionally
the normal log data.The LogFiT model is trained using masked corrupted. The DeepLog model uses the forecasting-based
token prediction on the normal log data only. Consequently
method,whereastheLogBERTmodelusesthereconstruction-
when presented with the new log data, the model’s top-k token
based method.
predictionaccuracyisusedasthresholdfordeterminingwhether
the new log data has deviated from the normal log data. This study focuses on log data comprising sequences of
Experimental results show that LogFiT’s F1 score exceeds that log sentences that represent various system events, such as
of baselines on the HDFS, BGL and Thunderbird datasets. the successful connection to an email server or a failure
Critically, when variability in the log data is introduced during
to upload a file after multiple attempts. To train anomaly
evaluation, LogFiT’s effectiveness surpasses that of the baseline
detection models, log sentences are grouped according to
models.
specific criteria, such as session identifier or time window.
Index Terms—Service monitoring, fault management, log Each group of log sentences defines a sequence of system
anomaly detection, deep learning, natural language processing,
events that occurred within a given time window or session.
language modeling.
For instance, a normal sequence of log sentences may consist
ofoneormore”Fileopenedsuccessfully”entriesfollowedby
”File write operation completed” and/or ”File read operation
I. INTRODUCTION
completed” entries. However, a sequence of log sentences
Annually,cybercrimeresultsinbillionsofdollarsoflosses that only contains ”File opened successfully” entries without
for businesses [1]–[3]. The detection of log anomalies can corresponding ”read” or ”write” entries can be considered an
assist in safeguarding the digital infrastructure of businesses anomaly. This information is critical to identifying anomalies
from cyber-attacks by identifying abnormal activities, such as in system logs and developing effective anomaly detection
network intrusions, from large amounts of event logs created models.
by networked computer systems. The quality of the representation of log sequences is a
Recent research efforts in log anomaly detection have critical factor that impacts the effectiveness of log anomaly
employed Deep Learning and Natural Language Processing detection models, especially when log content evolution is
(NLP) techniques to address this issue. A literature review taken into account [9], [10]. In order to represent log
reveals that state-of-the-art research in this area employs sequences, a common approach is to first convert log
the Long Short-Term Memory (LSTM) architecture, as sentences into log templates [4], [5]. A set of log templates
exemplified by the DeepLog model [4], and Transformers, is thus produced and this set of log templates functions
represented by the LogBERT model [5]. One crucial factor as a form of input vocabulary. Afterwards all input log
in applying deep learning to log anomaly detection is the sentences are mapped to an entry within the set of log
availability of labeled data for training predictive models. templates. This approach has been shown to negatively effect
2
model effectiveness because of its semantically deficient
representations of log sentences. Furthermore the input log
sentences are expected to match a log template. Thus this
approach is unable to handle variability in the content log
sentences over time [6], [11], [12].
To address the limitations of existing approaches for
anomaly detection in system logs, this work makes the
following contributions:
Figure1. HDFSlogsentencesconvertedtologtemplates.
• A novel log anomaly detection model, named LogFiT,
which utilises a pre-trained Bidirectional Encoder
Representations from Transformers (BERT) based • Model development – during this step the optimal
language model (LM) that has been fine-tuned to combination of training objective, neural network
understand the linguistic and sequential patterns of the architecture and performance evaluation metrics is
normal log data. The LogFiT model is able to produce selected. Once these elements are identified, the neural
representations for log sequences of up to 4096 tokens network model is trained on the vectorised data obtained
by leveraging a pre-trained LM. This ensures that the in the preceding step, using an optimisation algorithm to
model remains robust even when there are unforeseen minimise the difference between the model’s predictions
changes to the content of log sentences. and the actual output. Finally, the model’s performance
• A framework and workflow based on the HuggingFace is evaluated based on the selected metrics, and the best-
ecosystem. The framework adopts the masked language performing model is selected.
modeling (MLM) self-supervised training objective. • Model operationalisation – in this final step, the best
Duringtraining,themodelistrainedonnormaldata,with performing machine learning model is operationalised
the aim of minimising the cross-entropy loss between by deploying it to a production environment. The
predictions and the actual data. At inference time, the model’s efficiency and effectiveness are then monitored
framework determines whether a log sequence is normal continuously to ensure it performs optimally in its given
or an anomaly by comparing the model’s top-k accuracy environment.
against a pre-defined threshold value.
• We have conducted experiments comparing LogFiT’s
The initial three activities of log anomaly detection have
precision, recall, F1 score and specificity against
been the subject of extensive research. For log data pre-
two baseline approaches, DeepLog and LogBERT. the
processing, some studies have emphasised the use of log
experiments were conducted using three datasets: HDFS,
parsing [4], [5]. However, more recent studies [11], [17]
BGL, and Thunderbird. We present the experimental
suggestedthatlogparsingmayleadtoareductioninaccuracy
results to demonstrate the superiority of LogFiT in
andhence,shouldnotberecommended.Toincreaseaccuracy,
detecting anomalous log sequences compared to the
a state-of-the-art method called semantic vectorisation is
baseline approaches.
applied to the log data, to enrich it with semantic information
• We will make our implementation of the model,
before using them as input for model training [11], [17]. In
developed using Python, PyTorch and HuggingFace,
contrast, the earlier cited works [4], [5] have utilised basic
publicly available, along with the model checkpoints for
numerical encoding techniques to convert the input log data
reproducibility and further research.
into vectors. In model development, previous studies have
revealed that the best performance in log anomaly detection
II. RELATEDWORK were attained through the use of model architectures and
The process of log anomaly detection generally involves techniques that were inspired by natural language processing
four key steps, which have been identified by various (NLP), as noted in Pang, Shen, Cao, et al. [13] and Yadav,
researchers [13]–[16]. Kumar, and Dhavale [18]. Among these techniques, Long
Short-TermMemory(LSTM)andTransformershavebeenthe
• Log data pre-processing - which involves cleaning
preferred model architectures [4]–[6], [11].
and standardising the raw system logs that have been
generated by computer systems. This step is critical
as the accuracy of the subsequent steps relies on the
A. Log Templates
quality of the pre-processed data. It is assumed that the
logs have already been collected at this stage, and the Log anomaly detection methods commonly use log parsing
focus is on standardising the data and on removing any as the initial step, which involves converting log data into
inconsistencies and irrelevant information. a standardised format known as log templates [14], [16],
• Vectorisation – this step is concerned with transforming [19]. The goal of log parsing is to map each log sentence
the pre-processed log data into numeric representations to a specific log template, which forms the vocabulary of the
called vectors. The training of machine learning models model, rather than using words or tokens typical in natural
inthefollowingsteprequiresinputdatatobeintheform language processing (NLP). Several log anomaly detection
of vectors. methods,suchasDeepLog[4],LogRobust[17],andLogBERT
3
techniques, which enable models to encode the linguistic
structure and meaning of log data. The LogRobust model
[17] utilised static word embeddings to incorporate semantic
informationtologdatarepresentations.Theproponentsofthe
LogRobust model observed that word embeddings increased
the model’s accuracy.
C. Pre-trained Language Models
In recent research, there has been a growing interest in
the use of pre-trained language models (LMs) such as
BERT [21] to improve anomaly detection in system logs.
Studies by Ott, Bogatinovski, Acker, et al. [10] and Le
and Zhang [11] demonstrated that pre-trained LMs can offer
significant advantages over word embeddings, which were
used in the LogRobust model [17]. According to these
studies, pre-trained LMs capture contextual information at
Figure2. ComparisonoftheDeepLogandLogBERTloganomalydetection
methods. the level of the whole sequence of log sentences, whereas
word embeddings only provide representations for individual
words in a single sentence. Furthermore, BERT generates
[5], rely on log templates, often generated using the Drain
context-sensitive semantic vectors that encode the order of
parser [20], to pre-process log data.
Figure 1 illustrates how log sentences are converted into words within a log sentence and are capable of handling
log templates in the DeepLog and LogBERT methods. The out-of-vocabulary words. In contrast, the semantic vectors
conversion of log data to log templates is done as part of generated via word embeddings are static and are computed