title:Fast Hypervisor Recovery Without Reboot
author:Diyu Zhou and
Yuval Tamir
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Fast Hypervisor Recovery Without Reboot
Diyu Zhou and Yuval Tamir
Computer Science Department, UCLA
Email: {zhoudiyu,tamir}@cs.ucla.edu
Abstract—System recovery latency is decreased by using mi-
croreboot to reboot only the failed component instead of the entire
system. For large, complex components, such as hypervisors, even
the latency of microreboot is unacceptably high in important
deployment scenarios. We investigate an alternative component-
level recovery mechanism, which we call microreset, that can
achieve dramatically lower recovery latency for some such
components. Instead of component reboot, microreset quickly
resets the component to a quiescent state that is highly likely
to be valid and where the component is ready to handle new
or retried interactions with the rest of the system. We present
a recovery mechanism for the Xen hypervisor, called NiLiHype,
based on microreset. We show that, compared to microreboot-
based hypervisor recovery, NiLiHype achieves nearly the same
recovery success rate but with a recovery latency that is shorter
by a factor of over 30.
I. INTRODUCTION
System-level virtualization [27] is widely used in servers
and datacenters of all sizes. Errors that occur during the
execution of one of the virtual machines (VMs) due to a
transient hardware fault or software fault are highly likely
to be conﬁned within that particular VM. Hence, other VMs
are not affected. However, if such errors occur during the
execution of hypervisor code, the resulting hypervisor failure
leads to the failure of all the VMs on the host, and thus,
potentially, to a signiﬁcant impact on datacenter operation.
Thus, as explained further below, there is strong motivation
to develop fault tolerance mechanisms that allow the VMs to
survive across hypervisor failure [19], [21], [28].
VM replication [8] is often used to provide fault tolerance
with respect to VM failures, including with commercial prod-
ucts, such as VMware’s vLockstep [29]. If the replicas are
running on different hosts, failures during the execution of the
hypervisor are also covered. Obviously, the resource overhead
during normal operation is high; too high for many deployment
scenarios. A decision not to use VM replication implies that
the loss of any one speciﬁc VM is tolerable. For example,
this might be the case when a VM is providing web service
[33]. However, the loss of all the VMs on a host has more
impact since it leads to the unavailability of a larger fraction
of datacenter capacity, which is particularly signiﬁcant in a
small datacenter. Furthermore, even if VM replication is used,
there are performance beneﬁts as well as greater ﬂexibility
in VM placement, if the replicas are on the same host [26].
Placing replicas on the same host can only be considered if
an error during hypervisor execution is unlikely to lead to a
hypervisor crash.
In order to prevent the hypervisor from being a single point
of failure, the system must support recovery from hypervisor
failure while preserving the hosted VMs. This has been done in
ReHype [19], [21]. With ReHype, upon detection of an error in
the hypervisor, microreboot [7] of the hypervisor is performed,
while allowing other system components to maintain their
states. The overhead during normal operation is small and
essentially no work is lost when recovery is performed.
ReHype involves booting a new hypervisor instance. The
state of the hypervisor is then updated to make it consistent
with the states of the other system components (e.g., the guest
VMs) [19], [21]. This requires reusing a signiﬁcant amount of
state from the previous (failed) hypervisor instance. Obviously,
this reused state is potentially corrupted. However, it has been
shown that recovery success rates above 85% are achieved.
Achieving such recovery rates despite reusing state from the
failed instance is an indication that there is a relatively low
probability of a fault corrupting state critical to the survival
of the entire system.
Fault injection studies of the Linux kernel [31], [32] have
shown that the errors caused by most faults affect “process
local” state as opposed to state that can impact the kernel
itself or other processes. Based on this motivation, recovery
of the Linux kernel without reboot was proposed. For a subset
of possible faults (those leading to kernel oops), a simple
recovery scheme was proposed that
involves aborting the
faulting process. This yielded a recovery rate of approximately
60%. These results for the Linux kernel together with the
ReHype results discussed in the previous paragraph raise
the possibility that recovery from hypervisor failure may not
require booting a new hypervisor instance.
For many important applications, the latency of the reboot
step of microreboot results in unacceptably long service in-
terruption. This paper investigates component-level recovery
of a hypervisor without reboot. Using a mechanism which we
call microreset, a failed component is reset to a quiescent state
that is highly likely to be valid and where the component is
ready to handle new or retried interactions with the rest of
the system. By avoiding the reboot step of microreboot, the
recovery latency is dramatically reduced.
We implement and evaluate microreset for the Xen hyper-
visor [3], using a mechanism which we call NiLiHype (Nine
Lives Hypervisor). We show that NiLiHype and ReHype have
the same relatively small overhead during normal operation.
NiLiHype achieves a successful recovery rate of over 88%,
slightly lower than ReHype’s rate of over 90%. However,
NiLiHype performs the recovery more than 30 times faster
than ReHype. NiLiHype’s recovery latency is low enough
(22ms) that service interruption is negligible in most deploy-
2158-3927/18/$31.00 Â©2018 IEEE
DOI 10.1109/DSN.2018.00024
115
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
ment scenarios. Considering this recovery rate vs. recovery
latency tradeoff, NiLiHype is an attractive point in the design
space.
1) present
We make the following contributions:
the
design of microreset-based component-level recovery as an
alternative to microreboot; 2) describe the implementation
issues involved in converting the microreboot-based ReHype
to the microreset-based NiLiHype; 3) use fault injection to
evaluate the recovery rate of NiLiHype for different fault
types and workloads; 4) evaluate the hypervisor processing
overhead of NiLiHype during normal operation, NiLiHype’s
recovery latency, and its implementation complexity.
The next section presents microreset and compares it to
microreboot as a technique for component-level recovery. The
basic operation of Xen hypervisor recovery using ReHype and
NiLiHype is described in Section III. The starting point for
our implementation was the ReHype source code [19], [21].
Section IV describes the porting of ReHype to a more modern
platform and key enhancements that improve its recovery rate.
The implementation of NiLiHype is described in Section V.
The experimental setup and evaluation results are presented in
sections VI and VII, respectively. Related work is summarized
in Section VIII.
II. COMPONENT-LEVEL RECOVERY
The key idea in component-level recovery (CLR) is to
reduce recovery latency by limiting recovery to the failed
component instead of involving the entire system [7]. This
section presents the key challenges in CLR as well as two
alternative CLR mechanisms that involve low overhead during
normal operation: microreboot and microreset, in Subsections
II-A and II-B, respectively.
Three main challenges must be overcome in any CLR
implementation: (1) preventing the process of recovering the
failed component from harming the rest of the system while
also preventing the rest of the system from interfering with
the recovery process; (2) restoring the failed component to
a valid state; and (3) ensuring that the state of the recovered
component is consistent with the state of the rest of the system.
One example of interference between CLR and the rest of
the system is when the component being recovered is an OS
kernel [10]. If an I/O interrupt is generated during recovery,
when the kernel is not in a valid state to handle the interrupt,
the result is likely to be recovery failure. The simple step of
disabling interrupts during part of the recovery process is an
obvious solution for this example.
As an example of challenge (2) above, consider the case
where recovery involves stopping further execution of the
failed component. At that point different parts of the compo-
nent state may be inconsistent with each other. For instance,
a lock may have been acquired by an execution thread within
the component but that execution thread is abandoned [19],
[21]. Hence, component recovery must
involve one, or a
combination of, booting a new instance, rollback to a known
valid checkpoint, and/or a roll forward procedure to ﬁx the
invalid state.
Ensuring that
the state of the recovered component
is
consistent with the rest of the system is the most difﬁcult
challenge for CLR (challenge (3) above). Again we use an
example where the component being recovered is an OS
kernel [10]. If recovery involves booting a new kernel, the
new kernel
instance will not have the up-to-date state of
the data structures that maintain information regarding the
user processes that were running prior to the failure. Hence,
there is no way to continue running these processes following
recovery. To resolve this issue,
the recovery mechanisms
must reuse part of the state from the previous instance. This
introduces an obvious vulnerability since, when recovery is
triggered, the state of the previous instance is, by deﬁnition,
invalid. Recovery can be successful only if the error is not
propagated to the reused part of the state or the recovery
mechanism can somehow ﬁx corrupted reused state.
A. Microreboot: Component-Level Recovery with Reboot
Microreboot is a CLR that involves booting a new com-
ponent instance. As discussed in connection with the third
challenge above, this requires reuse of part of the state of the
previous instance. Therefore microreboot needs to preserve
part of the state of the failed instance across the reboot
and then reintegrate it with the state of the newly booted
instance [10], [19], [20], [21]. The state of the component
after recovery is thus a combination of the state of the initial
boot and the reused state from the failed instance.
B. Microreset: Component-Level Recovery without Reboot
A signiﬁcant drawback of microreboot is recovery latency.
This latency includes the time to reboot plus the time to
re-integrate the state from the previous instance. For large,
complex components, such as kernels and hypervisors, this
latency can be from multiple hundreds of milliseconds [21] to
tens of seconds [10]. It is possible to reduce part of the reboot
time by replacing the reboot with a rollback to a checkpoint
saved right after a previous reboot [30]. However, even in this
case, there would be signiﬁcant latency for reintegrating state
from the previous instance. For example, with a mechanism
similar to ReHype, this latency would be multiple hundreds of
milliseconds even for a very small workload of three simple
application VMs [21].
With the goal of reducing the recovery latency, this paper
investigates an alternative to microreboot, which we call mi-
croreset. Microreset is suitable for large, complex components
that process requests from the rest of the system. OS kernels
and hypervisors are examples of such components. Speciﬁ-
cally, a hypervisor receives requests in the form of hypercalls
or traps from the VMs as well as interrupts from hardware
timers and potentially other devices. Multiple requests may
be processed simultaneously by separate execution threads.
With microreset, upon error detection, the processing of all
current requests is abandoned. This resets the component to a
quiescent state. At that point, the microreset mechanism must
perform additional operations to deal with the last two CLR
challenges discussed above. Speciﬁcally, there is a need to
116
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
perform roll forward operations to ﬁx any corruptions in the
component state as well as inconsistencies among different
parts of the component state. Next, inconsistencies between the
recovering component state and the states of other components
in the system must be resolved. For example, this may require
retrying requests from other components that were abandoned
when the error was detected.
it
With microreset, only a small fraction of the component
state is discarded during recovery. Speciﬁcally,
is just
the “local” states of the abandoned execution threads (e.g.,
variables on the stacks). The entire remaining state is kept
in place and reused. On the other hand, with microreboot,
only part of the global state from the failed instance is reused
and the rest of the state is restored to its initial values by
the reboot. Microreset’s reuse of a larger fraction of the pre-
recovery component state increases the probability that the
post-recovery state is invalid. Hence, there is a reason to expect
some reduction in recovery rate with microreset compared to
microreboot.
III. IMPLEMENTING CLR OF A HYPERVISOR
Section I presented the motivation for a virtualized system
to be able to recover from hypervisor failure while preserving
and then continuing the execution of application VMs. This
section discusses how component-level recovery (CLR) can
be applied to the Xen [3] hypervisor, based on microre-
boot and microreset. Subsection III-A is a brief overview
of the Xen virtualization platform and examples of CLR
challenges (Section II) applicable to this particular component.
Subsection III-B provides a high-level description of how
microreboot has been applied to Xen with ReHype [19], [21].
Subsection III-C provides a high-level description of how we
apply microreset to Xen with NiLiHype.
A. The Xen Virtualization Platform
The Xen virtualization platform consists of two compo-
nents: the hypervisor and the privileged VM (PrivVM, also
known as Dom0). The hypervisor provides the core func-
tionality of the virtualization platform, such as memory man-
agement and scheduling of the VMs. The PrivVM performs
management operations, such as creating, checkpointing, and
destroying VMs. The PrivVM may also host the device drivers
for the I/O devices in the system and facilitates their sharing
among the application VMs (AppVMs) [3].
There are three ways for control to transfer from the VMs to
the hypervisor: hypercalls, exceptions and hardware interrupts.
VMs issue hypercalls to the hypervisor to request service
from the hypervisor. An example of a hypercall is a request
by a VM for the hypervisor to update page table entries.
This particular example is relevant for paravirtualized VMs,
(PVMs) [27], [3] and it should be noted that the PrivVM is
a PVM. Exceptions occur when VMs execute privileged or
illegal instructions. An interrupt is triggered by hardware and
causes a trap to a handler in the hypervisor, from which it is
sometimes forwarded to some VM.
The ability to recover from a failure during the execution
of any part of the virtualization platform requires dealing with
the PrivVM as well as the hypervisor. This issue has been
addressed in previous work [20], [21] and is not discussed in
this paper.
The challenges faced by any CLR, discussed in Section II,
must, of course, be handled by CLR of the Xen hypervisor.
For example (challenge (1)),
if VMs continue to execute