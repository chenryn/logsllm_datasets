PBCs only need load balancing during data retrieval.
(a) consumer¶s simulation
4.5 A PBC from reverse cuckoo hashing
h1(1)
ࠑ1, Aࠒ
h2(1)
1
A
1
A
1
A
2
B
h1(2)
1
A
2
B
3
C
h2(3)
ࠑ2, Bࠒ
h2(2)
2
B
1
A
ࠑ3, Cࠒ
2
B
h1(3)
1
A
3
C
(b) producer¶s allocation
FIGURE 7—Example of two-choice reverse hashing. (a) shows the
consumer’s simulation when inserting two tuples (cid:8)2, (cid:4)(cid:9), (cid:8)3, (cid:4)(cid:9). The (cid:4)
indicates that the value is not known, so an arbitrary value is used. (b)
shows a modiﬁcation to two-choice hashing where the producer stores
the tuple in all possible choices. This ensures that the ﬁnal allocation
is always compatible with the consumer’s simulation.
the producer previously placed those elements in those exact
buckets. We describe how we guarantee compatibility below.
Protocol. The consumer starts by imagining that it is a pro-
ducer with a collection of k elements. In particular, the con-
sumer converts its k keys into k key-value tuples by assigning a
dummy value to each key (since it does not know actual values).
In this simulation, the consumer follows a speciﬁc allocation
algorithm (e.g., 2-choice hashing, cuckoo hashing) and popu-
lates the b buckets accordingly. The result is an allocation that
balances the load of the k elements among the b buckets (as we
discuss in Section 4.3). The consumer then ends its simulation
and uses the resulting allocation to fetch the k elements from
the buckets that were populated by the real producer.
Guaranteeing that the consumer’s allocation is compatible
with the producer’s actions is challenging. One reason is that
the consumer’s simulation is acting on k items whereas the
real producer is acting on n items. If the allocation algorithm
being used (by the consumer and the producer) is randomized
or depends on prior choices (this is the case with multi-choice
hashing schemes), the allocations will be different. For example,
observe that if a producer generates the allocation in Figure 6
it would not be compatible with the consumer’s simulation in
Figure 7(a) despite both entities using the same algorithm (since
the producer places the item under key “2” in the middle bucket,
but the consumer’s simulation maps it to the top bucket).
To guarantee compatibility we employ a simple solution:
the producer follows the same allocation algorithm as the con-
sumer’s simulation (e.g., 2-choice hashing) on its n elements but
stores the elements in all candidate buckets. That is, whenever
the algorithm chooses one among w candidate buckets to store
We give a construction that uses Cuckoo hashing [66] to allocate
balls to bins. However, the same method can be used with other
algorithms (e.g., multi-choice Greedy [13], LocalSearch [55])
to obtain different parameters. We give a brief summary of
Cuckoo hashing’s allocation algorithm below.
Cuckoo hashing algorithm. Given n balls, b buckets, and w
independent hash functions h0, . . . , hw−1 that map a ball to a
random bucket, compute w candidate buckets for each ball by
applying the w hash functions. For each ball x, place x in any
empty candidate bucket. If none of the w candidate buckets
are empty, select one at random, remove the ball currently in
that bucket (xold), place x in the bucket, and re-insert xold. If
re-inserting xold causes another ball to be removed, this process
continues recursively for a maximum number of iterations.
Construction. Let H be an instance (producer, consumer) of
reverse hashing where the allocation algorithm is Cuckoo hash-
ing with w independent hash functions and b bins (we discuss
concrete values for w and b later in this section). We construct
a (n, m, k, b, p)-PBC as follows.
Encode(DB). Given a collection DB of n elements, follow H’s
producer algorithm to allocate the n elements to the b buckets
using the indices in DB as keys and the elements as values. This
results in m = wn total elements distributed (not necessarily
evenly) across the b buckets. Return the buckets.
GenSchedule(I). Given a set of indices I, follow H’s con-
sumer algorithm to allocate the k indices to the b buckets. Return
the mapping of indices to buckets. If more than one index maps
to the same bucket (i.e., if there are collisions), return ⊥ instead.
Decode(W). Since Encode performs only replication, all
codewords are elements in DB and require no decoding. Fur-
thermore, σ, which is returned by GenSchedule, has only one
entry for each index. As a result, W contains only one codeword.
Decode returns that codeword.
Concrete parameters. Analyzing the exact failure probabil-
ity of Cuckoo hashing, and determining the constant factors,
remains an open problem (see [42] for recent progress). How-
ever, several works [29, 68] have estimated this probability
empirically for different parameter conﬁgurations. Following
the analysis in [29, §4.2], we choose w = 3 and b = 1.5k. In
this setting, the failure probability is estimated to be p ≈ 2
−40
−20). This means that,
for k > 200 (for smaller k it is closer to 2
assuming the mapping from indices to buckets is random, the
probability that GenSchedule(I) returns ⊥ for a set of indices
I chosen independently from the hash functions is p. Figure 5
compares this result with existing batch code constructions and
the scheme proposed in Pung [11, §4.4].
969
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
1: function SETUP(DB)
2:
(C0, . . . , Cb−1) ← Encode(DB)
for j = 0 to b − 1 do
SETUP(Cj)
// See Fig. 1, Line 1
5:
6: function MULTIQUERY(pk, I, {|C0|, . . . , |Cb−1|})
7:
σ ← GenSchedule(I)
if σ (cid:10)= ⊥ then
// get an element for each bucket
// pick a random index if the bucket is not used in σ
for j = 0 to b − 1 do
idxj ← index for bucket j (based on σ and O)
qj ← QUERY(pk, idxj, |Cj|)
// see Fig. 1, Line 4
return q ← (q0, . . . , qb−1)
else Deal with failure (see §5)
3:
4:
8:
9:
10:
11:
12:
13:
14:
15:
19:
20:
24:
25:
26:
27:
28:
29:
30:
16:
17: function MULTIANSWER(q, (C0, . . . , Cb−1))
18:
for j = 0 to b − 1 do
aj ← ANSWER(qj, Cj)
return a ← (a0, . . . , ab−1)
21:
22: function MULTIEXTRACT(sk, a, I, σ)
23:
// see Fig. 1, Line 9
// extract the codewords from the provided PIR answers into cw
for j = 0 to b − 1 do
cwj ← EXTRACT(sk, aj)
// see Fig. 1, Line 14
// select codewords from cw that are relevant to each index in I
for i = 0 in k − 1 do
W ← codewords from cw (based on σ[Ii])
ei ← Decode(W)
return (e0, . . . , ek−1)
FIGURE 8—Multi-query CPIR protocol based on a CPIR protocol and
a PBC (Encode, GenSchedule, Decode). I is the set of k desired
indices and |Ci| is the size of bucket i. This protocol assumes a CPIR
scheme with the API given in Figure 1.
5 Multi-query PIR from PBCs
We give the pseudocode for a PBC-based multi-query PIR
scheme in Figure 8. At a high level, the server encodes its
database by calling the PBC’s Encode procedure. This produces
a set of buckets, each of which can be treated as an independent
database on which clients can perform PIR. A client who wishes
to retrieve elements at indices I = {i0, . . . , ik−1} can then lo-
cally call GenSchedule(I) to obtain a schedule σ. This schedule
states, for each index, the bucket from which to retrieve an el-
ement using PIR. Because of the semantics of GenSchedule
it is guaranteed that no bucket is queried more than once (or
σ = ⊥). As a result, the client can run one instance of PIR on
each bucket. However, a challenge is determining which index
to retrieve from each bucket: by assumption (of PIR) the client
knows the index in DB, but this has no relation to the index of
that same element in each bucket. To address this, we introduce
an oracle O that provides this information (we discuss it below).
If the client has nothing to retrieve from a given bucket, the
client simply queries a random index for that bucket.
Constructing the oracle O. There are several ways that the
client can construct O. The simplest solution is to obtain the
mapping from each index in DB to the corresponding indices
970
in each bucket. While this might sound unreasonable, observe
that PIR has an implicit assumption that the client knows the
index in DB of the desired element. The client could use the
same technique to obtain the corresponding w indices in B(DB).
For example, in the Pung communication system [11], clients
obtain this mapping in a succinct Bloom ﬁlter [17].
Another option is for the client to fetch elements in PIR not
by index but by a label using PIR-by-keywords [32]. Examples
of labels include the name or UUID of a movie, the index in the
original DB (in this case elements would need to be stored as
key-value tuples, where the key is the label). One last option is
for the clients to construct O directly. This requires the server
to share with clients its source of randomness (e.g., a PRF seed).
Clients can then simulate the server’s encoding procedure on a
database of n dummy elements (replicating each element into w
candidate buckets), which yields O. Furthermore, this process
is incremental for many hashing schemes: if a client has O for
an n-element database, it can construct O for a database with
n + 1 elements by simulating the insertion of the last element.
Malicious placement of elements. In cases where the server
is malicious—rather than semi-honest (or honest but curious)—
the server has full control over where to place elements. As a
result, the server could place speciﬁc elements at indices that
can never be retrieved together (i.e., at sets of indices where Gen-
Schedule returns ⊥). This opens the door to attacks where the
server selectively makes certain combinations of elements not
retrievable in hopes of observing a client’s reaction and breaking
the privacy guarantees. Note that a similar attack already exists
in the single-query PIR case: the server can selectively place
an incorrect element (or garbage) at a particular index and can
wait to see if a client complains or not (thereby learning that the
index that the client requested was one that contained garbage
or not). To address this style of “selective failure” attacks, addi-
tional mechanisms are needed. A common solution is to ensure
that a client’s reaction remains independent of whether or not
queries succeed. This guarantees that the attack does not violate
privacy. Instead, it violates availability, which a malicious server
could violate anyway by not answering queries.
Dealing with failures in the schedule. If the PBC being used
has p > 0, then it is possible that for a client’s choice of indices,
σ = ⊥. In this case, the client is unable to fetch all k elements
that it wishes to retrieve privately. However, notice that the
client learns of this fact before issuing any PIR query (see
Figure 8, Line 8). As a result, the client has a few options. First,
the client can adjust its set of indices (i.e., choose different
elements to retrieve). This is possible in applications where the
client needs to retrieve more than a batch of k items. Second,
the client can retrieve a subset of the elements. In a messaging
application, this would mean that the client would not retrieve
all unread messages. In many cases, this is acceptable since
messages may not be ephemeral so the client can try again at
a later time (presumably with a new set of indices). Lastly, the
client can fail silently. Which of these strategies is taken by a
client depends on the application. In any case, it is imperative
that the application’s failure-handling logic is designed to not
reveal information about a client’s indices.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
6
Implementation
7 Evaluation
SealPIR implements XPIR’s protocol [8] atop the SEAL homo-
morphic encryption library [4] (version 2.3.0-4). This is around
2,000 lines of C++ and Rust. The most intricate component is
EXPAND (Figure 3) which requires the substitution homomor-
phic operation (§3.1). We implement this operation in SEAL
by porting the Galois group actions algorithm from Gentry et
al. [44, §4.2]. We discuss this in detail in Appendix A.1.
SealPIR exposes the API in Figure 1. A difference with XPIR
is that substitution requires auxiliary cryptographic material
to be generated by the client and be sent to the server (see
Appendix A.1). However, a client can reuse this material across
all of its requests and it is relatively small (2.9 MB per client).
Encoding elements as FV plaintexts. In SealPIR, an FV
plaintext is represented as an array of 64-bit integers, where
each integer is mod t. Each element in the array represents
a coefﬁcient of the corresponding polynomial. We encode an
element e ∈ DB into an FV plaintexts p(x) by storing log(t)
bits of e into each coefﬁcient of p(x). If elements are small, we
store many elements into a single FV plaintext (for example,
the ﬁrst element is stored in the ﬁrst 20 coefﬁcients, etc.). This
reduces the total number of FV plaintexts in the database, and
consequently the computational costs of PIR.
(cid:3)
Optimization to EXPAND. In FV, an encryption of 2
(mod 2y), for y ≥ (cid:4), is equivalent to an encryption of 1
(mod 2y−(cid:3)). Observe that in Lines 14–16 of Figure 3, EXPAND
(cid:3)
multiplies the n ciphertexts by the inverse of m where m = 2
(the goal of this multiplication is to ensure that all ciphertexts
encrypt either 0 or 1). Instead, we change the plaintext modulus
of the n ciphertexts from t = 2y to t
, which allows
us to avoid the plaintext multiplications and the inversion, and
reduces the noise growth of EXPAND. The result is n − 1 ci-
(cid:2) = 2y−(cid:3)
phertexts encrypting 0, and one ciphertext encrypting 1, as we
expect. This optimization requires t to be divisible by m rather
than being an odd integer. One drawback is that the server must
represent the database using FV plaintexts deﬁned with the
plaintext modulus t
(rather than t). As a result, we can pack
fewer database elements into a single FV plaintext.
, we ﬁnd the largest integer value of log(t
(cid:2)) for which
To set t
(cid:2)
(cid:2)
log(t
the following inequality holds:
√
(cid:2)) + (cid:7)log((cid:7) d