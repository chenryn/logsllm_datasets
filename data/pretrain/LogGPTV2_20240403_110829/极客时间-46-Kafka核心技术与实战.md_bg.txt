## MirrorMaker 配置实例现在，我就在测试环境中为你演示一下 MirrorMaker 的使用方法。演示的流程大致是这样的：首先，我们会启动两套 Kafka集群，它们是单节点的伪集群，监听端口分别是 9092 和9093；之后，我们会启动 MirrorMaker 工具，实时地将 9092集群上的消息同步镜像到 9093集群上；最后，我们启动额外的消费者来验证消息是否拷贝成功。
### 第 1 步：启动两套 Kafka 集群启动日志如下所示：> \[2019-07-23 17:01:40,544\] INFO Kafka version: 2.3.0> (org.apache.kafka.common.utils.AppInfoParser)\> \[2019-07-23 17:01:40,544\] INFO Kafka commitId: fc1aaa116b661c8a> (org.apache.kafka.common.utils.AppInfoParser)\> \[2019-07-23 17:01:40,544\] INFO Kafka startTimeMs: 1563872500540> (org.apache.kafka.common.utils.AppInfoParser)\> \[2019-07-23 17:01:40,545\] INFO \[KafkaServer id=0\] started> (kafka.server.KafkaServer)> \[2019-07-23 16:59:59,462\] INFO Kafka version: 2.3.0> (org.apache.kafka.common.utils.AppInfoParser)\> \[2019-07-23 16:59:59,462\] INFO Kafka commitId: fc1aaa116b661c8a> (org.apache.kafka.common.utils.AppInfoParser)\> \[2019-07-23 16:59:59,462\] INFO Kafka startTimeMs: 1563872399459> (org.apache.kafka.common.utils.AppInfoParser)\> \[2019-07-23 16:59:59,463\] INFO \[KafkaServer id=1\] started> (kafka.server.KafkaServer)
### 第 2 步：启动 MirrorMaker 工具在启动 MirrorMaker 工具之前，我们必须准备好刚刚提过的 Consumer配置文件和 Producer 配置文件。它们的内容分别如下：    consumer.properties：bootstrap.servers=localhost:9092group.id=mirrormakerauto.offset.reset=earliest    producer.properties:bootstrap.servers=localhost:9093现在，我们来运行命令启动 MirrorMaker 工具。    $ bin/kafka-mirror-maker.sh --producer.config ../producer.config --consumer.config ../consumer.config --num.streams 4 --whitelist ".*"WARNING: The default partition assignment strategy of the mirror maker will change from 'range' to 'roundrobin' in an upcoming release (so that better load balancing can be achieved). If you prefer to make this switch in advance of that release add the following to the corresponding config: 'partition.assignment.strategy=org.apache.kafka.clients.consumer.RoundRobinAssignor'请你一定要仔细阅读这个命令输出中的警告信息。这个警告的意思是，在未来版本中，MirrorMaker内部消费者会使用轮询策略（Round-robin）来为消费者实例分配分区，现阶段使用的默认策略依然是基于范围的分区策略（Range）。Range策略的思想很朴素，它是将所有分区根据一定的顺序排列在一起，每个消费者依次顺序拿走各个分区。Round-robin 策略的推出时间要比 Range策略晚。通常情况下，我们可以认为，社区推出的比较晚的分区分配策略会比之前的策略好。这里的好指的是能实现更均匀的分配效果。该警告信息的最后一部分内容提示我们，**如果我们想提前"享用"轮询策略，需要手动地在consumer.properties 文件中增加 partition.assignment.strategy 的设置**。
### 第 3 步：验证消息是否拷贝成功好了，启动 MirrorMaker之后，我们可以向源集群发送并消费一些消息，然后验证是否所有的主题都能正确地同步到目标集群上。假设我们在源集群上创建了一个 4 分区的主题 test，随后使用kafka-producer-perf-test 脚本模拟发送了 500万条消息。现在，我们使用下面这两条命令来查询一下，目标 Kafka集群上是否存在名为 test 的主题，并且成功地镜像了这些消息。    $ bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9093 --topic test --time -2test:0:0    $ bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9093 --topic test --time -1test:0:5000000-1 和 -2分别表示获取某分区最新的位移和最早的位移，这两个位移值的差值就是这个分区当前的消息数，在这个例子中，差值是500 万条。这说明主题 test 当前共写入了 500万条消息。换句话说，MirrorMaker 已经成功地把这 500万条消息同步到了目标集群上。讲到这里，你一定会觉得很奇怪吧，我们明明在源集群创建了一个 4分区的主题，为什么到了目标集群，就变成单分区了呢？原因很简单。**MirrorMaker在执行消息镜像的过程中，如果发现要同步的主题在目标集群上不存在的话，它就会根据Broker 端参数 num.partitions 和 default.replication.factor的默认值，自动将主题创建出来**。在这个例子中，我们在目标集群上没有创建过任何主题，因此，在镜像开始时，MirrorMaker自动创建了一个名为 test 的单分区单副本的主题。**在实际使用场景中，我推荐你提前把要同步的所有主题按照源集群上的规格在目标集群上等价地创建出来**。否则，极有可能出现刚刚的这种情况，这会导致一些很严重的问题。比如原本在某个分区的消息同步到了目标集群以后，却位于其他的分区中。如果你的消息处理逻辑依赖于这样的分区映射，就必然会出现问题。除了常规的 Kafka 主题之外，MirrorMaker默认还会同步内部主题，比如在专栏前面我们频繁提到的位移主题。MirrorMaker在镜像位移主题时，如果发现目标集群尚未创建该主题，它就会根据 Broker端参数 offsets.topic.num.partitions 和 offsets.topic.replication.factor的值来制定该主题的规格。默认配置是 50 个分区，每个分区 3 个副本。在 0.11.0.0 版本之前，Kafka 不会严格依照offsets.topic.replication.factor参数的值。这也就是说，如果你设置了该参数值为 3，而当前存活的 Broker数量少于 3，位移主题依然能被成功创建，只是副本数取该参数值和存活 Broker数之间的较小值。这个缺陷在 0.11.0.0 版本被修复了，这就意味着，Kafka会严格遵守你设定的参数值，如果发现存活 Broker数量小于参数值，就会直接抛出异常，告诉你主题创建失败。因此，在使用MirrorMaker 时，你一定要确保这些配置都是合理的。
## 其他跨集群镜像方案讲到这里，MirrorMaker 的主要功能我就介绍完了。你大致可以感觉到执行MirrorMaker的命令是很简单的，而且它提供的功能很有限。实际上，它的运维成本也比较高，比如主题的管理就非常不便捷，同时也很难将其管道化。基于这些原因，业界很多公司选择自己开发跨集群镜像工具。我来简单介绍几个。1.**Uber 的 uReplicator 工具**Uber 公司之前也是使用 MirrorMaker的，但是在使用过程中，他们发现了一些明显的缺陷，比如 MirrorMaker中的消费者使用的是消费者组的机制，这不可避免地会碰到很多 Rebalance的问题。为此，Uber 自己研发了 uReplicator。它使用 Apache Helix作为集中式的主题分区管理组件，并且重写了消费者程序，来替换之前MirrorMaker 下的消费者，使用 Helix 来管理分区的分配，从而避免了Rebalance 的各种问题。讲到这里，我个人有个小小的感慨：社区最近正在花大力气去优化消费者组机制，力求改善因Rebalance 导致的各种场景，但其实，其他框架开发者反而是不用 Group机制的。他们宁愿自己开发一套机制来维护分区分配的映射。这些都说明 Kafka中的消费者组还是有很大的提升空间的。另外，Uber专门写了一篇[博客](https://eng.uber.com/ureplicator/)，详细说明了uReplicator 的设计原理，并罗列了社区的 MirrorMaker 工具的一些缺陷以及uReplicator 的应对方法。我建议你一定要读一读这篇博客。2.**LinkedIn 开发的 Brooklin Mirror Maker 工具**针对现有 MirrorMaker工具不易实现管道化的缺陷，这个工具进行了有针对性的改进，同时也对性能做了一些优化。目前，在LinkedIn 公司，Brooklin Mirror Maker 已经完全替代了社区版的MirrorMaker。如果你想深入了解它是如何做到的，我给你推荐一篇[博客](https://www.slideshare.net/jyouping/brooklin-mirror-maker-how-and-why-we-moved-away-from-kafka-mirror-maker)，你可以详细阅读一下。3.**Confluent 公司研发的 Replicator 工具**这个工具提供的是企业级的跨集群镜像方案，是市面上已知的功能最为强大的工具，可以便捷地为你提供Kafka 主题在不同集群间的迁移。除此之外，Replicator工具还能自动在目标集群上创建与源集群上配置一模一样的主题，极大地方便了运维管理。不过凡事有利就有弊，Replicator是要收费的。如果你所在的公司预算充足，而且你们关心数据在多个集群甚至是多个数据中心间的迁移质量，不妨关注一下Confluent 公司的[Replicator工具](https://www.confluent.io/confluent-replicator/)。
## 小结好了，我们总结一下今天所讲的 MirrorMaker。它是 Apache Kafka社区提供的跨集群镜像解决方案，主要实现将 Kafka消息实时从一个集群同步复制或镜像到另一个集群。你可以把 MirrorMaker应用到很多实际的场景中，比如数据备份、主备集群等。MirrorMaker本身功能简单，应用灵活，但也有运维成本高、性能差等劣势，因此业界有厂商研发了自己的镜像工具。你可以根据自身的业务需求，选择合适的工具来帮助你完成跨集群的数据备份。![](Images/e69ab6f2be38736afefdb3e54a1c3aff.png){savepage-src="https://static001.geekbang.org/resource/image/c0/5e/c0b75891bf37c1086dea98358c481d5e.jpg"}
## 开放讨论今天我只演示了 MirrorMaker最基本的使用方法，即把消息原样搬移。如果我们想在消息被镜像前做一些处理，比如修改消息体内容，那么，我们应该如何实现呢？（提示：指定kafka-mirror-maker 脚本的 \--message.handler 参数。）欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。![](Images/a7d15815f9efb5693db5b2d278244658.png){savepage-src="https://static001.geekbang.org/resource/image/c8/bf/c89da43deab85fe7cb06acec867aa5bf.jpg"}