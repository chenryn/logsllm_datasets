only the modiﬁed (or “dirty”) memory pages are stored into
each checkpoint image. The ﬁrst checkpoint created by Ag-
amotto after the ﬁrst boot—the root checkpoint—would be
identical to what a full snapshot mechanism would create,
which contains all pages in memory. Whenever Agamotto cre-
ates a new checkpoint based on an existing one, however, only
the memory pages that have been modiﬁed with respect to the
base checkpoint are stored into the checkpoint image. This
incremental approach greatly reduces the size of a non-root
checkpoint, as well as the time it takes to create one.
The dependencies between incremental checkpoints are
already expressed in our checkpoint tree data structure; that
is, the virtual machine state of a given node in the checkpoint
tree can be fully restored by following the path from the root
to that node and incrementally applying checkpoints.
3.5.2 Delta Restore
A strawman approach to restoring a virtual machine using
incremental checkpoints is to sequentially apply incremental
checkpoint images starting from the root to the target node
in an incremental checkpoint tree. The number of memory
pages that this strawman approach should restore, however,
is greater than the one that a non-incremental snapshot ap-
proach would restore; the root checkpoint in an incremental
checkpoint tree already contains the full virtual machine state,
and additional restorations of incremental checkpoints will
add further overhead.
(cid:46) Collect pages that need to be restored
L ← LOWESTCOMMONANCESTOR(Src,Dst)
DirtySrc..L ← DirtySrc
Temp ← PARENT(Src)
while Temp is not L do
DirtySrc..L ← DirtySrc..L ∨ DirtyTemp
Temp ← PARENT(Temp)
DirtyDst..L ← DirtyDst..L ∨ DirtyTemp
Temp ← PARENT(Temp)
end while
DirtyDst..L ← DirtyDst
Temp ← PARENT(Dst)
while Temp is not L do
Algorithm 1 Delta restore
1: function DELTARESTORE(Src, Dst)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
end while
24:
25: end function
end while
(cid:46) Restore pages starting from the target node
DirtyDelta ← DirtySrc..L ∨ DirtyDst..L
Temp ← Dst
while DirtyDelta is not empty do
RESTOREPAGES(DirtyDelta ∧ DirtyTemp)
DirtyDelta ← DirtyDelta ∧¬DirtyTemp
Temp ← PARENT(Temp)
In the fuzzing context, high-performance restore is a re-
quirement, because the virtual machine is restored at the
beginning of every iteration of the fuzzing loop. However,
since Syzkaller’s default Linux kernel conﬁguration for USB
fuzzing requires at least 512MB of working memory, and
Windows requires a minimum of 4GB, it would take up to
several seconds for the strawman approach to restore the full
virtual machine memory. We, therefore, introduce the delta
restore algorithm, which minimizes the number of memory
pages that are copied during a virtual machine restoration.
The full algorithm is described in Algorithm 1. The key idea
is to restore (i) only the pages that have been modiﬁed in
either the current or target virtual machine state after their
execution has diverged, and (ii) each modiﬁed page only once
via bottom-up tree traversal. This means that the number
of memory pages that are copied during a virtual machine
restoration is bounded by the number of pages modiﬁed within
the current or the target virtual machine state. Observe that, in
the strawman approach, the number of copied memory pages
is greater than or equal to the number of all pages in memory.
Figure 5 contrasts (a) the top-down, strawman approach
with (b) our bottom-up, delta restore approach in restoring
a virtual machine state. In the given checkpoint tree, the
node Dst refers to the checkpoint that the system is being
restored to, and the node Src is a temporary node representing
the current system state from which the restoration starts. The
node B refers to the last checkpoint that the current system
state is based on, and the node R refers to the root checkpoint.
USENIX Association
29th USENIX Security Symposium    2547
Syzkaller’s USB fuzzing mode takes the user-space device
emulation approach. It adds a kernel module that intercepts
and redirects USB driver I/O requests to a program running
in user space via the system call interface. Since Syzkaller al-
ready contains many smart fuzzing features such as structure-
awareness of USB packets, we modiﬁed Syzkaller such that
Agamotto can be applied. Our key modiﬁcation was moving
Syzkaller’s fuzzer outside of the virtual machine so that the
fuzzer survives virtual machine restorations as well as ker-
nel crashes. We also modiﬁed the communication channels
between Syzkaller’s components. The fuzzing algorithm and
other aspects of Syzkaller were left unmodiﬁed.
For fuzzing the PCI interface, we developed our own fuzzer,
which uses a device virtualization approach to intercept the
driver’s I/O requests at the virtual machine monitor level. A
key beneﬁt of this approach is that it does not require kernel
modiﬁcations; a virtual device can be implemented within
the virtual machine monitor without modifying the guest OS
kernel. We created a “fake” virtual PCI device in QEMU,
and plugged it into QEMU’s virtual PCI bus. Our fake PCI
device attached to the PCI bus gets recognized by the PCI
bus driver as the guest OS kernel boots, and, once the target
PCI driver gets loaded, it intercepts all memory-mapped I/O
(MMIO) requests coming from the target driver. We fuzzed
these MMIO requests by sending fuzzer-generated data to the
driver as a response to each driver I/O request.
4
Implementation
We implemented Agamotto on top of QEMU 4.0.0 running
in an x86 Linux environment [8]. We used the Linux Kernel
Virtual Machine (KVM) for hardware accelerated virtualiza-
tion [43]. We used Syzkaller3 for USB fuzzing [24], and Amer-
ican Fuzzy Lop (AFL) version 2.52b for PCI fuzzing [65].
Dirty Page Logging. We used KVM’s dirty page logging
to identify modiﬁed pages, as required for our incremental
checkpointing and delta restoration techniques. KVM’s dirty
page bitmap was looked up upon a checkpoint creation request
and a virtual machine restoration request. We cleared KVM’s
dirty page bitmap after each checkpoint creation and virtual
machine restoration. We note that KVM’s dirty page logging
can transparently be accelerated as hardware support—e.g.,
Page Modiﬁcation Logging in Intel x86 CPUs—becomes
available. Using this dirty page logging, we implemented our
own optimized versions of virtual machine checkpointing and
restoration mechanisms, since QEMU’s snapshot implemen-
tation was found to be slower than we expected.
Inter-Component Communication. We used a variety of
commodity virtual machine introspection (VMI) mechanisms
3Speciﬁcally, the commit number: ddc3e85997efdad885e208db6a98bca86e5dd52f
Figure 5: Top-down restore vs. Bottom-up delta restore.
Our delta restore algorithm ﬁrst locates the lowest common
ancestor node (node L ) of the node Src and Dst , and computes
a bitmap of modiﬁed memory pages (or a dirty bitmap) of
each node with respect to the node L , denoted by DirtySrc..L
and DirtyDst..L, respectively. We take the union of these two
dirty bitmaps, which we call a delta dirty bitmap, denoted
by DirtyDelta. DirtyDelta contains a complete list of memory
pages that need restoring. Then, starting from the node Dst , we
traverse the checkpoint tree backwards to the root node. At
each node during the traversal, we restore only the memory
pages that are in DirtyDelta and clear their corresponding bits
in DirtyDelta to ensure that each dirty page is restored only
once. The traversal stops when DirtyDelta is fully cleared. The
strawman approach, by contrast, restores all pages stored in
incremental checkpoints starting from the node R .
I/O Interception for Fuzzing
3.6
Fuzzing driver code paths that can be reached through a given
peripheral interface requires interception and redirection of
the driver’s I/O requests. We ﬁnd two common models for
driver I/O interception and redirection:
• User-Space Device Emulation. I/O requests coming
from a kernel driver are redirected to a user-mode pro-
gram through the system call interface. This approach
typically requires kernel source code modiﬁcations for
intercepting and redirecting driver I/O requests.
• Device Virtualization. Device virtualization techniques
allow the virtual machine monitor to intercept I/O re-
quests coming from the corresponding kernel driver.
2548    29th USENIX Security Symposium
USENIX Association
R…LBSrc…Dst(a) Top-down restoreNodeIncrementalVM Checkpoint(b) Bottom-up delta restoreSDstSSrcVM StateSRSLDirtySrc..LDirty BitmapDirtyDst...LDirtyDeltaSDst……200
100
)
s
m
(
e
m
T
i
0
0
)
i
B
M
(
e
z
i
S
512
256
0
0
0.5
1
·105
Dirty pages
0.5
1
·105
Dirty pages
(a) Run-time overhead
(b) Memory overhead
Figure 6: Overheads of incremental checkpointing.
to implement inter-component communication channels. Con-
trol channels were implemented via hypercalls and VIRTIO
pipes established between QEMU and the guest virtual ma-
chine [44]. Data channels for bulk data transfer were imple-
mented via direct reads and writes to the guest memory or by
using a separate shared memory device.
Syzkaller and AFL Support. Agamotto was designed
to support multiple fuzzers, and the current prototype sup-
ports two different fuzzers. When running Agamotto with
Syzkaller for fuzzing the USB interface, we used Syzkaller’s
fuzzer (syz-fuzzer) as Agamotto’s fuzzer component and
Syzkaller’s executor (syz-executor) as Agamotto’s guest
agent. They were both modiﬁed such that they use our VMI-
based communication channels. When running Agamotto
with AFL for fuzzing the PCI interface, we ran an AFL fuzzer
thread as Agamotto’s fuzzer component and used a shell script
as the guest agent, which simply loads the target PCI driver.
5 Evaluation
We conducted all of our experiments on a machine equipped
with AMD EPYC 7601 CPU and 500GB of memory. We
targeted device drivers in Linux v5.5-rc3 in our fuzzing exper-
iments. We enabled Kernel AddressSanitizer to expose more
bugs [35]. We ﬁrst evaluate Agamotto’s individual primitives,
and then the performance of kernel driver fuzzers augmented
with Agamotto in both USB and PCI fuzzing scenarios.
5.1
Incremental Checkpointing
We compare the run-time and memory overheads of our in-
cremental checkpointing implementation with the overheads
of QEMU’s non-incremental snapshot approach [1]. To mea-
sure the overheads conservatively, we disabled QEMU’s zero
page optimization, a checkpoint size reduction technique that
handles a page ﬁlled with zeros by storing a ﬁxed-size entry
in the checkpoint image, instead of storing 4KiB of zeros.
Delta Restore
QEMU Snapshot Restore (Baseline)
)
s
m
(
e
m
T
i
100
50
0
0
0.2
0.4
0.6
0.8
1
1.2
·105
Pages restored
Figure 7: Run-time overhead of delta restore.
Run-Time Overhead. The run-time overhead of check-
pointing primarily depends on the number of pages copied
into the checkpoint image. Figure 6a shows the overhead of
our incremental checkpointing mechanism, and that of the
baseline, when checkpointing a 512MiB memory guest virtual
machine. As the number of dirty pages increases, the run-time
overhead of incremental checkpointing increases linearly. In
contrast, the overhead of the baseline, a non-incremental ap-
proach, remains constant regardless of the number of dirty
pages. In addition, QEMU’s non-incremental checkpoint ap-
proach adds an additional overhead due to its implementation
and the full inclusion of the device memory, of which only a
few pages are dirtied during fuzzing. A full restore can, there-
fore, take more than 200ms per checkpoint for copying all
131,072 pages, whereas our incremental checkpointing, for a
typical range of the number of dirty pages (see Section 5.3),
takes less than 20ms on average as it only copies the dirty
pages.
Memory Overhead. Figure 6b shows how the size of each
checkpoint correlates to the number of dirty pages when
checkpointing a 512MiB memory virtual machine. As ex-
pected, the size of an incremental checkpoint increases in
proportion to the number of pages that have been modiﬁed
since the last checkpoint. Given the distribution of the number
of modiﬁed pages, which typically ranges from 0 to 8,000
(see Section 5.3), each checkpoint should take no more than
64MiB. With the zero page optimization enabled, the size of
each checkpoint observed in actual fuzzing runs, on average,
is less than 32MiB. This is a reduction of 90% or more in size