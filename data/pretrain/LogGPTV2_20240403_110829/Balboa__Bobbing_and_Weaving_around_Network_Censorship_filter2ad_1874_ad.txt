replacing the plaintext Ogg data, and (2) the computation of
the GCM tag required when re-encrypting the plaintext.
Web browsing. We gathered data for two scenarios: using
curl to download a video ﬁle and using Firefox to browse sev-
eral links on a website containing a small subset of Wikipedia.
For the sender (i.e., Apache), we see an average delay across
both scenarios of roughly 89µs. For curl we see an average
delay of 90µs, and for Firefox we see an average delay of
216µs. The reason we see a higher delay than audio streaming
is that the web browsing rewriter needs to store HTTP requests
and thus requires allocations.
6.3 Timing Analysis
The introduced delays have security implications, as a suf-
ﬁciently powerful censor may be able to classify Balboa-
enabled trafﬁc due to these delays. To determine the effect of
these timing differences on the ability to classify Balboa, we
ran several experiments on both our audio streaming and web
browsing instantiations. For all of our experiments, we gen-
erated 130 pcap traces12 between two Ubuntu 18.04 docker
containers with and without Balboa enabled, using tc to con-
trol the average latency and its standard deviation in our sim-
ulated network. We generated traces for latencies between
0 ms (the “ideal” scenario) and 30 ms (the average latency
12We generated packet captures on an Intel Xeon Silver 4114 CPU @
2.20GHz with 40 cores and 512 GB of memory. Doing so enabled us to
generate packet captures more quickly, by running multiple trials in parallel.
We (informally) veriﬁed that running parallel trials did not impact our results
by comparing the results against a small number of non-parallel runs.
in the United States13). We then built classiﬁers to try to
distinguish the Balboa-enabled versus -disabled trafﬁc, using
tcptrace [21] to extract TCP statistics to train on. Our classi-
ﬁers used random forests due to the success similar classiﬁers
have had on distinguishing prior censorship circumvention
systems [4]. For each scenario we trained classiﬁers using
10-fold stratiﬁed cross-validation using Scikit-learn [22].
Note that all of these experiments occurred in an idealized
setting with no additional network trafﬁc, and thus represent
a best case scenario for a censor. In a real-world deployment
successfully applying such a classiﬁer would be much more
difﬁcult. We additionally ran our experiments with a number
of additional clients whose network data was not analyzed by
the classiﬁer. This mimics a setting where the censor attempts
to identify the use of Balboa among a larger set of innocuous
trafﬁc. We found—as expected—that this setting decreases
the classiﬁer’s accuracy. As an example, for VLC with zero
latency and four additional clients, we achieve a classiﬁer
accuracy of only 66%, versus 84% when a single client is
used. Thus, to model the best case scenario for a censor we
consider the single-client setting.
Audio streaming. For audio streaming we investigated the
potential to identify Balboa running across four different me-
dia players: VLC, MPlayer, Audacious, and mpv. Each trace
comprised of a client connecting to an Icecast server, stream-
ing a 10 second song, and then exiting. Table 2 presents the
accuracy, precision, and recall of our classiﬁer for different
latencies against these different media players. For each sce-
nario we trained 1000 classiﬁers, with the presented results
being the average and standard deviation of these classiﬁers.
We ﬁnd at the extreme end—where there is zero latency in
the simulated network—the classiﬁer is able to distinguish
Balboa trafﬁc across the various media players with between
66% and 84% accuracy, with the key features being the aver-
age TCP window advertisement seen and data transmit time.
This suggests that even the slight delay introduced by Balboa
is enough to affect some network statistics (albeit in an unre-
alistic network setting). However, as we increase the realism
of the network (by increasing the average latency as well as
the standard deviation) we see the accuracy of the classiﬁer
quickly drop to a point where it is essentially no better than
random guessing. This makes sense given that the delays in-
troduced by Balboa become part of the noise of the network
latency.
Another interesting feature of Table 2 is that the classifer
accuracy varies depending on the media player. This sug-
gests (perhaps not surprisingly) that different media players
present different “network footprints”. To validate this, we
additionally ran our classiﬁer to see if we could distinguish
two different media players, both with Balboa disabled. We
13According
to
latency/ as of March, 2021.
https://www.verizon.com/business/terms/
USENIX Association
30th USENIX Security Symposium    3409
Latency (ms)
0 ± 0
5 ± 1
5 ± 3
10 ± 1
10 ± 3
10 ± 5
30 ± 1
30 ± 3
30 ± 5
30 ± 10
Latency (ms)
0 ± 0
5 ± 1
5 ± 3
10 ± 1
10 ± 3
10 ± 5
30 ± 1
30 ± 3
30 ± 5
30 ± 10
Accuracy
0.84 ± 0.07
0.72 ± 0.08
0.63 ± 0.09
0.67 ± 0.09
0.67 ± 0.09
0.59 ± 0.09
0.64 ± 0.09
0.56 ± 0.09
0.57 ± 0.09
0.50 ± 0.09
Precision
0.87 ± 0.09
0.76 ± 0.10
0.67 ± 0.11
0.71 ± 0.12
0.70 ± 0.11
0.61 ± 0.11
0.67 ± 0.11
0.57 ± 0.12
0.58 ± 0.12
0.50 ± 0.12
(a) VLC
Accuracy
0.82 ± 0.05
0.73 ± 0.06
0.68 ± 0.06
0.68 ± 0.06
0.59 ± 0.07
0.63 ± 0.07
0.65 ± 0.06
0.56 ± 0.06
0.59 ± 0.07
0.56 ± 0.07
Precision
0.85 ± 0.07
0.75 ± 0.07
0.70 ± 0.07
0.70 ± 0.07
0.61 ± 0.08
0.65 ± 0.08
0.68 ± 0.08
0.57 ± 0.08
0.61 ± 0.08
0.58 ± 0.08
(c) Audacious
Recall
0.80 ± 0.11
0.66 ± 0.13
0.55 ± 0.14
0.59 ± 0.13
0.61 ± 0.14
0.51 ± 0.14
0.57 ± 0.14
0.47 ± 0.15
0.49 ± 0.14
0.41 ± 0.14
Recall
0.78 ± 0.08
0.71 ± 0.09
0.63 ± 0.09
0.63 ± 0.10
0.53 ± 0.10
0.56 ± 0.10
0.59 ± 0.10
0.48 ± 0.10
0.52 ± 0.10
0.49 ± 0.10
Latency (ms)
0 ± 0
5 ± 1
5 ± 3
10 ± 1
10 ± 3
10 ± 5
30 ± 1
30 ± 3
30 ± 5
30 ± 10
Latency (ms)
0 ± 0
5 ± 1
5 ± 3
10 ± 1
10 ± 3
10 ± 5
30 ± 1
30 ± 3
30 ± 5
30 ± 10
Accuracy
0.68 ± 0.09
0.50 ± 0.10
0.51 ± 0.09
0.55 ± 0.10
0.53 ± 0.09
0.52 ± 0.09
0.53 ± 0.10
0.49 ± 0.10
0.50 ± 0.09
0.49 ± 0.09
Precision
0.72 ± 0.12
0.50 ± 0.12
0.51 ± 0.12
0.56 ± 0.12
0.54 ± 0.12
0.53 ± 0.12
0.54 ± 0.13
0.49 ± 0.12
0.50 ± 0.12
0.48 ± 0.12
(b) MPlayer
Accuracy
0.66 ± 0.09
0.53 ± 0.09
0.57 ± 0.09
0.55 ± 0.09
0.49 ± 0.09
0.53 ± 0.09
0.53 ± 0.09
0.53 ± 0.09
0.52 ± 0.10
0.50 ± 0.10
Precision
0.69 ± 0.11
0.54 ± 0.12
0.58 ± 0.12
0.57 ± 0.12
0.48 ± 0.13
0.54 ± 0.13
0.53 ± 0.12
0.54 ± 0.12
0.53 ± 0.13
0.49 ± 0.13
(d) mpv
Recall
0.60 ± 0.13
0.41 ± 0.14
0.41 ± 0.14
0.47 ± 0.15
0.45 ± 0.14
0.42 ± 0.14
0.44 ± 0.14
0.40 ± 0.14
0.41 ± 0.13
0.39 ± 0.14
Recall
0.61 ± 0.13
0.44 ± 0.14
0.48 ± 0.14
0.46 ± 0.14
0.38 ± 0.14
0.43 ± 0.14
0.43 ± 0.14
0.44 ± 0.14
0.42 ± 0.15
0.40 ± 0.14
Table 2: Accuracy, precision, and recall of classifying Balboa-generated trafﬁc versus baseline for various latency settings against
various media players (VLC, MPlayer, Audacious, and mpv). Values are given in “mean ± standard deviation” format.
found that regardless of which media players we compared
against, we achieved a 99–100% accuracy for all latency and
standard deviation settings.
Web browsing. For web browsing we investigated the po-
tential to identify Balboa using two different clients: curl
and Firefox. For curl, each trace comprised of downloading
a 13.6 MB video and then exiting. For Firefox, each trace
comprised of a Selenium script accessing three different web
pages scraped from Wikipedia, sleeping three seconds be-
tween each web page access. The assets for the three web
pages totaled 8.9 MB and included HTML, javascript, im-
age, and CSS ﬁles. As with the audio streaming case, Table 3
presents the accuracy, precision, and recall of our classiﬁer
across different latencies.
While the accuracies for web browsing tend to be higher
than in the audio streaming case, this makes sense given
the larger average delay introduced by Balboa. However, we
reiterate that these results are for an ideal setting for the censor
and the accuracies are still sufﬁciently low given the base rate
fallacy.
7 Related Work
The literature is rich with different approaches to building
censorship resistant systems (CRSs); we refer the reader to
existing systematization of knowledge papers [18, 23] for a
more thorough overview of the ﬁeld than what we can provide
here.
A CRS can be viewed as comprising two key components:
communication establishment and conversation. Balboa ad-
dresses the second, which is where most of the academic
literature has focused [18, §5.5]. In particular, Balboa corre-
sponds to an “access-centric” scheme using the terminology
of Khattak et al. [18]. We thus focus on such schemes in this
section. Access-centric schemes can be subdivided into four14
main categories, which we discuss in turn.
Mimicry. These approaches send data by mimicking
some cover protocol. A representative example is format-
transforming encryption [9] and its variants [10, 19], which
operate by mapping ciphertexts to regular expressions or
14Khattak et al. [18] differentiate between tunneling approaches and covert
channel approaches whereas we view these as the same, since any covert
channel approach necessarily needs to “tunnel” its trafﬁc through some
existing application.
3410    30th USENIX Security Symposium
USENIX Association
Latency (ms)
0 ± 0
5 ± 1
5 ± 3
10 ± 1
10 ± 3
10 ± 5
30 ± 1
30 ± 3
30 ± 5
30 ± 10
Accuracy
0.66 ± 0.01
0.69 ± 0.01
0.69 ± 0.01
0.66 ± 0.01
0.66 ± 0.01
0.65 ± 0.01