et al. propose CardioCam [32] to verify users based on their
cardiac biometrics. Researchers have also devised authenti-
cation systems where users are challenged to respond to a
Captcha challenge on their mobile phones, while collecting
audio and visual data of the response that is transmitted to a
secure server for processing [51].
The execution of machine learning models on resource
constrained platforms such as mobile phones has seen active
research in both algorithmic machine learning improvements
[21], [53] as well as enhanced system design
[54], [17],
[33]. Liu et al. devise a selection framework, AdaDeep, that
automatically selects a combination of compression techniques
to be applied to a given neural network to balance between
performance and availability of resources
[33]. Closer to
our work, researchers at Facebook extensively proﬁle the
wide diversity in compute capabilities on mobile phones for
machine learning
[54]. They also identify the beneﬁts of
optimizing to run inference on CPUs over GPUs to provide
stable execution on Android devices, and Daredevil follows
this general plan where we run Android models on the CPU
but use the hardware acceleration available on iOS to speed
up our models. Ran et al. [41] create a client-server hybrid
framework to provide sufﬁcient compute power for running
augmented reality apps. Authors in [38], [55] conduct a
measurement study of mobile performance analysis of various
deep learning models and conclude the need for extensive
optimization and both on-device and cloud based inference.
Recently there has been work on improving the performance
of parallel DNN training [36], [22]. Narayanan et el.
[36]
cast DNN training as a computational pipeline to efﬁciently
utilize the hardware resources. In contrast, Huang et al. [22],
while also using pipelining to train large models, signiﬁcantly
reduce the memory overhead by re-materialization.
Deep learning has seen a widespread adoption in a multitude
of domains, outperforming traditional machine learning and
rule-based algorithms. We have also seen it make in-roads
into security with its potential to empower data engineers with
newer features that can limit the prejudices of prior algorithms.
However, if not careful, deep-learning-based security chal-
lenges have the potential of reproducing historical prejudices,
improving the security and user experience of one group at
the expense of altogether blocking the other.
In this paper, with a wide-scale measurement study con-
sisting of 3,505,184 devices that ran in real apps. Our study
looked at a widely deployed deep-learning-based system for
scanning payment cards where we demonstrated that while
these challenges can solve the app’s business problem by
functioning reliably on high-end phones, this challenge has
the potential to disproportionately block users from low socio-
economic tiers who rely on lower tier smartphones.
With the lessons learned from our measurement study, we
designed Daredevil, a payment card veriﬁcation system that
used deep learning optimizations and improved system design
to build a complex security system that works uniformly on
low-end and high-end mobile devices. We showed the results
from 1,580,260 devices from Daredevil’s public deployment
to demonstrate the practical nature of our system across all
devices.
ACKNOWLEDGMENTS
We would like to thank Xiaojing Liao and the anonymous
reviewers who provided valuable feedback on this work. We
would also like to the thank Weisu Yin, Sven Kuhne and
Allison Tearjen for their contributions to this work. This
research was funded by a grant from Bouncer Technologies.
APPENDIX
A. How does Daredevil compare against other card scanners?
Card.io [39] is a popular open-source scanning library
commonly used in the industry. We compare Daredevil against
Card.io via a lab experiment to measure their scan success
rates on our benchmark test set of 100 credit cards. We observe
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1635
Device
iPhone 5s
iPhone SE
iPhone XR
LG K20 Plus
Xiaomi Redmi 7
Pixel 2
Blocking
1.65 fps
7.60 fps
28.45 fps
1.03 fps
3.16 fps
3.66 fps
+ Buffer
1.70 fps
7.90 fps
32.60 fps
1.04 fps
3.47 fps
4.35 fps
+ Parallel
2.95 fps
14.90 fps
32.60 fps
1.39 fps
4.89 fps
7.95 fps
Fig. 16: Frames per second for 20 second run. This ﬁgure
shows the performance improvement measured by frames
processed by our main loop per second with the baseline of a
blocking system, a system that buffers images, and a system
that buffers images and runs the ML models in parallel.
that Daredevil is able to extract the correct card number from
each card, while Card.io is able to extract the correct card
number from only 58 cards. Accordingly, Daredevil’s precision
and recall are both at 100%, while Card.io’s precision and
recall are 100% and 58% respectively. The lower recall of
Card.io is attributed to its inability to scan cards with ﬂat
fonts.
B. Impact of the producer / consumer design on frame rates.
Daredevil processes frames obtained from a live camera
feed. In most cases, the camera runs at a higher frame rate than
the machine learning model, meaning that applications will
have to drop some number of frames while the user is scanning
their card. A natural and common solution to this problem is
to block the live feed while the prediction runs, waiting for
the machine learning models to ﬁnish processing the frame
before grabbing the next available frame from the camera.
This solution leads to a lower effective frame rate because of
the waiting time but is memory efﬁcient and ensures that the
models always have fresh data to process by virtue of using
only the latest frame from the camera.
As opposed to processing each camera frame serially and
blocking the live feed while the model runs, Daredevil uses a
producer (the camera) / consumer (machine learning models)
architecture with a bounded LIFO buffer to store the most
recent frames, and run multiple predictions in parallel. This
architecture comes at the increased cost of memory but enables
the machine learning models to execute without any waiting
and ensures that the models process frames that are close to
what the user sees.
We have already seen the producer/consumer design leading
to higher frame rates and success rates in production (Section
VI-A). In this section, we run a controlled lab experiment
to compare the frame rates between the blocking design and
producer/consumer design of running our main loop (the card
detection and OCR models) on frames produced from a ﬁxed
camera feed. We follow this up with a qualitative analysis of
why the blocking design is slower on both Android and iOS,
despite the considerable differences in how the two platforms
execute machine learning inference.
Speciﬁcally, we consider three different variations: (1) a
blocking style with a single instance of our main loop models
driven at the frame rate of the camera, (2) a non-blocking style
using a buffer to store the two most recent frames with a single
thread running our main loop models, and (3) a non-blocking
style using a buffer to store the two most recent frames with
two threads on iOS and four threads on Android running
our main loop models. We run this experiment by measuring
the frame rates observed on running the three variations on
different iOS and Android devices of varying capabilities for
20 seconds each.
Figure 16 summarizes our results from these experiments.
From these results we can see a clear increase in the frame
rates across all phones on both iOS and Android on moving
from a blocking system to a system that buffers frames to a
system that buffers frames and runs our main loop models in
parallel.
Improvements in frame rates due to buffering alone range
from 1% to 19%, with faster devices seeing larger gains. The
reason that faster devices see larger gains is because the time
spent waiting for a camera frame is a larger portion of the
overall execution time as the time spent on machine learning
predictions goes down.
Surprisingly, we observe speed ups ranging from 15% to
117% due to adding multiple instances of our main loop mod-
els that run predictions in parallel. This speed up is surprising
because machine learning inference is embarrassingly parallel
and the underlying hardware architectures for iOS and Android
are vastly different, so we did not expect to see gains in
performance on both platforms from the same architectural
improvements.
On iOS as opposed to Android, our machine learning
models run on the GPU, however, the CPU needs to encode
the work on a command buffer before GPU can execute it.
Blocking the live feed while the prediction is running can
lead to idle time, since the GPU has to wait for the CPU to
encode the task.
Our producer/consumer style OCR addresses the GPU
idling issue by creating parallel workloads which ensures
that the CPU will encode the next workload while the GPU
is executing the current workload. The producer pushes the
frames from the camera feed onto a buffer keeping the most
recent frames and removing old stale frames. The consumer
which consists of independent machine learning analyzers pull
images from the buffer and run predictions on the frames
in parallel. Internally, Core ML (Apple’s machine learning
framework) serializes the requests, however, with this style,
encoding and execution happens in parallel.
On Android, since we run our machine learning models on
the CPU, bubbles arising as a result of the communication
between the CPU and GPU are not applicable to Android.
Our producer/consumer OCR as well as a sequential blocking
OCR, both run multi-threaded machine learning inference
(using the industry standard TensorFlow Lite). The two differ
in terms of the number of TensorFlow Lite interpreters run-
ning inference, with the former using multiple independent
interpreters and the latter using a single interpreter. In this
section, we seek to understand why we observe higher frame
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1636
Platform
Count
Daredevil Android
Daredevil iOS
55,093
119,826
Avg
FPS
3.04
18.77
Avg
Duration (s)
22.58
17.38
Fig. 18: Failure cases for Daredevil on Android and iOS
in parallel. Multiple nodes belonging to separate graphs com-
ing from distinct interpreters can execute at the same time,
showing a better utilization of the available hardware and
correspondingly faster scan times with the producer/consumer
OCR.
C. Analysis of Daredevil’s failure cases
From our real-world deployment of Daredevil, we observed
174,919 failed attempts where users gave up on trying to scan
their card. We aggregate the duration of these scans on iOS
and Android to report that Android users waited an average
of 22.58s and iOS users waited an average of 17.38s to scan
their cards before giving up. This is shown in Figure 18.
D. User study to evaluate the use of redundancy based de-
composition
We ask users participating in our study to visit a link where
they can scan 30 different credit card images via our app
running on their phone. On opening the link, we display 30
credit card images in a random sequence from a predetermined
set of cards. We manually label the objects present on these
cards, such as bank logo etc., for each frame collected from
our user study videos which serve as ground truth labels for
card tampering detection. Although users participating in the
study scan cards that are displayed on device screens, we
manually label each frame from each video for the presence
of screens to cover cases where the user starts executing the
app on the phone before pointing it at the screen. These labels
are our ground truth labels for fake media detection. The users
randomly run one of two versions of the app: with and without
the card detection model. We carry out our user study virtually
due to the restrictions imposed by the COVID-19 pandemic.
We obtain a total of 603 scan videos from the user study,
of which 273 were collected by providing explicit feedback
to the user to center their card by running the card detection
model and the remaining 330 were collected without any such
feedback. For the scans collected without feedback, we pass
all extracted frames through the card tampering detection and
fake media detection models. For the scans with feedback, we
ﬁrst pass the frames through the card detection model to only
select those with centered cards to pass to the card tampering
detection and fake media detection models. We then compare
the performance of the two models in both cases.
From scans without feedback from card detection, we
randomly sample 50 scans and pass all 4,213 frames extracted
from them to the card tampering detection model. We consider
expected objects not detected by the model as well as objects
incorrectly predicted by the model that are not present in the
card as errors. The model makes a total of 8,163 errors at an
Fig. 17: Variation in inference time with increasing number of
threads for inference on a single TensorFlow lite interpreter
on an 8 core Android device(big.LITTLE ARM) and a 24
core Linux server(x86). We attribute the increase in inference
time on Android after 4 threads to the increased computation
running on its slower cores. In contrast, the Linux server shows
a continual decrease in inference time on running inference
upto 24 threads, when all of its uniform cores are maximally
utilized.
rates with the producer/consumer OCR. More concretely, we
seek to understand the differences in how much parallelism
is available and how the hardware is utilized in both cases to
explain the improvements.
TensorFlow Lite runs machine learning inference by travers-
ing a computational graph where the nodes represent com-
putations that are part of the model and edges represent the
dependence of values between different computations [2]. We
inspect the TensorFlow Lite source code to ﬁnd that while run-
ning multi-threaded inference through a single interpreter, it is
only the individual computations corresponding to the nodes
of the computational graph that execute on multiple threads,
while the invocation of these nodes happens sequentially on
a single thread. Thus, with a single interpreter not more than
one node of the computational graph can run at a given time.
As a result, increasing the number of threads for a single
interpreter does not lead to faster inference if some threads
execute slower than others. While most Android phones in
use today either have 4 or 8 CPU cores, we uniformly see the