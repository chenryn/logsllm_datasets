312
154
144
203
157
159
125
157
141
144
141
203
64
55
673
257
75
46
283
105
50
167
235
43
47
N/A
236
45
38
265
97
40
157
189
22
28
N/A
37
19
9
30
26
26
24
11
21
8
N/A
21
30
8
18
8
10
10
46
Table 1: Results on eleven Windows Vista SP1 crash dumps. “Fct. ptrs.” represents the number of function pointers
correctly identiﬁed by MAS or KOP.
7.1 Accuracy and Robustness
The goal of this section is to evaluate the accuracy and
robustness of MAS. We face the general difﬁculty that it
is hard and time consuming to obtain an object mapping
that is known to be correct (i.e., ground truth) even in a
controlled environment. For the real-world crash dumps
for which we had no data beyond the crash dumps them-
selves, it appears unclear if and how a ground truth could
be established. Given these methodological difﬁculties,
much of the evidence we present in this section has to be
indirect.
Our ﬁrst data set consists of the outputs of MAS on
the 837 Windows 7 crash dumps. We tried to estab-
lish whether the function pointers reported by MAS as
suspicious are indeed function pointers. We inspected
whether the target of the function pointers appeared to be
the beginning of a function. The vast majority of func-
tion pointer targets contained a small set of code patterns
corresponding to function preambles. This allowed us
to automate most of pointer checks by running a pro-
gram that checks for these patterns. We inspected the
remaining pointers manually. We applied a second crite-
rion to the function pointers whose targets did not appear
to be code. We accepted all function pointer candidates
that were ﬁelds in objects whose existence could be de-
rived directly and unambiguously from the symbol infor-
mation. This included global variables and objects that
could be reached from global variables by following only
uniquely determined typed pointers. This left us with a
total of 24 dubious pointers out of total of 398,987 func-
tion pointers that MAS had output.
The eleven Windows Vista SP1 crash dumps in our
data set allowed us to perform a direct comparison with
KOP. We examined manually all discrepancies between
the outputs of MAS and KOP. KOP appeared to suffer
from both false positives and false negatives (see Ta-
ble 1). We ﬁrst examined all function pointers returned
by MAS and found that they are valid. Then we exam-
ined manually the targets of all function pointers reported
by KOP that had not been output by MAS. None of the
targets appeared to be the start of a function. Thus, we
classiﬁed these pointers as false positives for KOP (FP.
KOP in Table 1). We also observed a number of func-
tion pointers that were found by MAS, but not by KOP.
Since we had concluded that the targets of these point-
ers are function entry points, we classiﬁed them as false
negatives for KOP (FN. KOP in Table 1). KOP missed
as much as 40% of the function pointers found by MAS.
Furthermore, KOP as much as 40% of the function point-
ers reported by KOP appear to be incorrect.
We also tried to interpret the function pointers returned
by MAS. A large fraction of the reported function point-
ers appeared to point to third-party drivers that were not
included in our static analysis. However, in addition to
detecting the footprints of widely used anti-virus soft-
ware, we also found clear signs of rootkit infections in
ﬁve out of the eleven crash dumps. We will discuss how
we detect rootkits in real-world crash dumps in Section 9.
Next, we attempted to estimate the internal consis-
tency of the objects found by MAS. We examined the
complete kernel object mappings produced by MAS for
inconsistent pointers. These are pointers whose type is
incompatible with the object type that the object map-
ping has assigned to the pointer’s target. For example,
an object mapping might contain an object of type T1 at
address A. Another object in the mapping might contain
a pointer P of some other type T2 (cid:54)= T1 that also points to
A. P is an inconsistent pointer. Such inconsistencies may
exist even if the object mapping is error free because of
invalid pointers in objects and because of memory cor-
ruptions in the crash dump. But they may also indicate
errors in the object mapping, for example as a result of
following invalid pointers. We call an object inconsis-
8
Figure 5: Percentage of inconsistent objects in the object
mappings for MAS (left) and KOP (right). KOP did not
produce a result for the third dump.
Figure 6: Running times in seconds of MAS (left) and
KOP (right) on eleven real-world Windows Vista SP1
crash dumps. KOP did not produce a result for the third
dump.
tent if it is the target of at least one inconsistent pointer.
Figure 5 displays the percentage of inconsistent objects
in the object mappings found by MAS and KOP for the
Windows Vista SP1 crash dumps. We consider this num-
ber to be an indication of the correctness of the object
mapping. On average, the object mappings produced by
MAS contain 0.5% inconsistent objects. This number is
1% for the objects mappings produced by KOP.
7.2 Performance
This section evaluates the running time of MAS.
ﬁnish the static analysis in 5 hours on 100 nodes. On the
other hand, the combined machine time of 500 hours is
much larger than KOP’s running time. This is partly be-
cause MAS does not achieve perfect parallelization. For
instance, it takes 0.5 hour to load the program expres-
sion graph into memory on every node; alias analyses
for indirect calls are computed on demand on each node
and thus are not shared, which causes repeated computa-
tions as well. Furthermore, MAS converts a program to
the Static Single Assignment (SSA) form conservatively,
which increases the computation.
Static Analysis We performed the static analysis for
Windows XP SP3, Windows Vista SP1 and Windows 7.
Our evaluation is focused on Windows Vista SP1 since it
allows us to compare MAS and KOP directly. The static
analysis on Windows Vista SP1 includes the Windows
kernel and a set of 63 standard drivers (such as win32k,
ntfs and tcpip). This is the same set of drivers analyzed
by KOP. The code base has 3.5 million lines of code. The
program expression graph has 2.2 million nodes and 7.3
million edges. MAS performed almost 23,000 candidate
type lookups.
We performed the static analysis on a 100 node cluster
running Windows Server 2008 R2 HPC Edition, where
each node has two Quad-Core 2.5 GHz Xeon processors
with 16 GB RAM. Each node was used to perform 228
candidate type lookups. The whole static analysis took
less than 5 hours. The corresponding time for KOP re-
ported in [3] is 48 hours on a somewhat older, single pro-
cessor machine.
The key advantage of MAS over KOP is that MAS’s
static analysis can run in parallel. This allows MAS to
Dynamic Analysis Next, we report on the total run-
ning times of memory traversal and integrity checking of
MAS on three sets of memory snapshots. Figure 6 dis-
plays the running times of MAS and KOP on the eleven
Windows Vista SP1 crash dumps. On average, MAS
(160 seconds per dump) is more than 9 times faster than
KOP (24.5 minutes per dump). KOP failed to terminate
on crash dump 3 within the two hour time limit we had
set.
Figure 7 displays the distribution of MAS’s running
times on the 837 Windows 7 crash dumps. The running
times are concentrated between 40 and 160 seconds. The
average running time is 105 seconds, and 99.9% of all
runs complete in less than 5 minutes.
Finally,
the average running time of MAS on the
154,768 memory snapshots from our large-scale mal-
ware study is 31 seconds. The running time distribution
is highly concentrated around this value.
In summary, our experiments demonstrate that MAS
can quickly and accurately analyze real-world crash
dumps as well as memory snapshots of virtual machines.
9
0.00%0.20%0.40%0.60%0.80%1.00%1.20%1.40%1.60%1.80%12345678910110500100015002000250030001234567891011blocks. We loaded the driver in the VM before launching
the sample.
We used a 25 node compute cluster to evaluate all
154,768 samples. The cluster nodes were running Win-
dows Server 2008 R2. We used Hyper-V as our Virtual
Machine Monitor. On each cluster node, we ran between
4 and 8 VMs, running a total of 164 VMs simultaneously
at any time. Each job ran for 2 to 3 minutes. Since the
VM jobs were I/O bound we took a number of measures
to manage disk trafﬁc: The VMs used differencing disks
based on a single base image. We interleaved the startup
of VMs such that the I/O intensive phases at the begin-
ning and end of some jobs coincided with the one minute
idle period of other jobs. All 154,768 jobs completed in
less than 48 hours.
MAS reported kernel behaviors for only 89,474 of the
samples. We analyzed the events recorded by our driver
for the remaining 65,294 samples for which MAS had
output no results. The driver logs showed that, in all but
1286 cases, neither module loading nor non-executable
page faults were recorded. For the 1286 samples, the
driver logs showed that no non-executable page faults
were detected, and some modules were loaded after the
sample was launched but all of the modules had been un-
loaded before the memory snapshot was taken. Based on
this evidence, it appears that the memory snapshots for
which MAS reported no results did not contain any data
that MAS should have reported.
There are several potential reasons for the relatively
large number of samples without reportable kernel be-
haviors. As stated above, some of the samples may sim-
ply not have been malware. Also, the crude way in which
we launch the samples may have caused samples to fail
to execute. It may also have caused malware not to be-
come active. Techniques for reliably triggering malware
have been studied elsewhere [5, 8] and are not the focus
of this paper. The rest of this section presents the results
of our analysis for the 89,474 samples for which MAS
reported kernel behaviors.
8.1 General Behavior Statistics
Table 2 displays counts on the different categories of ker-
nel behavior we observed. The count for a category is the
number of samples that displayed behavior in that cate-
gory. Some samples displayed behaviors in more than
one category. Most categories correspond to modiﬁca-
tions of static data structures that can be detected with ex-
isting tools. IDT represents modiﬁcations to the function
pointers in the processor’s interrupt descriptor table. Sy-
senter represents modiﬁcations to the hardware register
that determines the target address of a sysenter instruc-
tion. Callgate represents similar modiﬁcations to func-
tion pointers in hardware-deﬁned call gate structures.
Figure 7: Running time (in seconds) distribution of MAS
on 837 real-world Windows 7 crash dumps.
When compared directly, MAS was nearly an order of
magnitude faster than KOP. MAS did not misidentify or
miss any functions pointers found by KOP in the eleven
Windows Vista SP1 dumps, but KOP missed or misiden-
tiﬁed as much as 40% of the suspicious function point-
ers.
8 Kernel Malware Study
In this section we present the results of our study of
a large collection of 154,768 potential malware sam-
ples that we obtained from a major vendor of anti-virus
software. These samples originated from a variety of
sources. Their behavior was unknown to us. This in-
cluded the question whether a sample even contained
malware. All samples were different types of Windows
binaries: executables (.exe), dynamically linked libraries
(.dll) and drivers (.sys).
We used MAS to analyze the samples. More precisely,
for each sample, we booted a clean Windows XP SP3
VM with 256 MB of RAM and one virtual processor and
loaded and executed it. We ran .exe’s directly. We ran
.dll’s with the help of a standard executable that loads a
dll and causes its DllMain function to be executed. We
loaded drivers (.sys) using the service control manager
(sc.exe). After launching the sample, we waited for one
minute, then took a memory snapshot of the VM, con-
verted it into a Windows crash dump and ran MAS over
the crash dump.
In order to gain additional insight into the events that
take place in the VM, we wrote a driver that makes most
of the kernel address space of the VM not executable
(by setting the corresponding bits in the page tables) and
catches and records any non-execute (NX) page faults.
The driver also records the loading and unloading of ker-
nel modules and the allocation and deallocation of pool
10
Category
IDT
Sysenter
Callgate
Syscall Table (SSDT)
Hidden Process
Hidden Module
Code Hooks
Module Imports and Exports
Function Pointer
Count
20
1
23
3652
1476
43828
17744
103
84051
Table 2: Distribution of malware behaviors.
represents
The next group of categories
static
software-deﬁned function pointers. The system call table
(SSDT) is a table of function pointers to the individual
system call handler functions. Hidden process and hid-
den module stand for attempts to hide processes or mod-
ules by removing them from the data structures Windows
maintains to keep track of processes and loaded mod-
ules. Code Hooks represent modiﬁcations of legitimate
executable code. Module Imports and Exports represent
tampering with the function pointers in the import and
export lists of loaded modules.
Finally, the Function Pointer category includes mod-
iﬁcations to function pointers in data objects found in
MAS’s memory traversal. Most of the objects are dy-
namic data (i.e., reside in the kernel pool) and some of
them are from global variables. This is by far the most
frequent category. About 94% of the samples display
this behavior in some form. Since this is also the one
category for which existing tools provide at best limited
information, we examined it in more detail.
Figure 8: Number of samples that hooked each of the
191 different function pointers for which MAS detected
hooking.
of the same underlying malware are present in a large
number of samples. We further investigated this obser-
vation by clustering the samples.
8.2 Function Pointer Hooking
We found that the samples were hooking a total of
191 unique function pointer ﬁelds from 31 different
data structures belonging to the Windows kernel and
ﬁve drivers (ntfs,
Fig-
ure 8 shows the number of samples that hooked each
of the 191 function pointer ﬁelds. We observe a high
concentration on a small set of pointers and a long
tail. The two plateaus between 0 and 60 correspond
mostly to function pointers from nt! DRIVER OBJECT
and nt! FAST IO DISPATCH. Almost 50% of the func-
tion pointers were hooked by only one or two samples.
fastfat, ndis, ﬂtmgr, null).
We also counted the number of distinct dynamic func-
tion pointers hooked by each sample. The distribution
is displayed in Figure 9. It is highly concentrated. Al-
most half the samples hook exactly 32 function pointers.
There is a smaller concentration around the value 4. This
high concentration suggests that versions or exact copies
Figure 9: Distribution of the number of dynamic function
pointers hooked by each sample
8.3 Clustering
To cluster samples, we ﬁrst extracted the following infor-
mation from MAS’s report as a sample’s footprint. For