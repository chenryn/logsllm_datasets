### 360
D. Hadˇziosmanovi´c et al.

#### Introduction
Windows systems, although generally secured against abuses of certain services from the Internet, are frequently exploited by corporate users. An attacker who develops an exploit for a zero-day vulnerability leveraging such protocols could potentially affect a large number of systems. In recent years, several malware, including [8,14], have exploited SMB/CIFS to operate botnets and carry out other malicious activities.

Industrial Control Systems (ICS) have become valuable targets for cyber attackers due to their sensitive nature. The detection of cyber attacks in these environments is crucial, often impacting homeland security. Recently, there has been an increase in vulnerabilities discovered in software used in critical facilities, primarily due to poor software development practices and the reliance on "security by obscurity" for legacy devices.

#### Data Collection
We collect attacks from two different datasets, focusing on data injection attacks with high impact (see [27]).

#### Implementation and Verification
To conduct the benchmarks, we need working prototypes of all the algorithms to be tested. We obtained implementations of POSEIDON and McPAD from the authors. For the remaining algorithms, we developed our own implementations based on the descriptions in the original papers. To ensure the correctness of our implementations, we verified that our results matched those reported in the benchmarks of the respective original papers. We intend to disclose these implementations in the near future.

#### Evaluation Criteria
The effectiveness of an Intrusion Detection System (IDS) is primarily determined by its detection and false positive rates. The detection rate indicates the number of attack instances correctly identified by the IDS (true positives), relative to the total number of attack instances. The false positive rate indicates the number of samples incorrectly flagged as attacks. False positives are a significant limiting factor in this domain because, unlike other classification problems, their cost is high [4].

**Detection Rate:**
To provide a detailed overview of each algorithm's detection capabilities, we consider both the number of correctly detected packets in the attack set and the number of detected attack instances. Not all attacks show malicious payload within a single packet. While a high per-packet detection rate increases the likelihood of detecting an attack instance, a low per-packet detection rate does not necessarily imply a low per-instance detection rate. We consider an alarm as a true positive if the algorithm triggers at least one alert per attack instance.

**False Positive Rate:**
The usual approach to document the performance of an IDS is to relate the false positive rate to the detection rate using Receiver Operating Characteristic (ROC) curves. However, expressing the false positive rate as a percentage has little meaning to end-users. A more practical way to express the false positive rate is in terms of the number of false positives per time unit. We establish two thresholds: 10 false positives per day and 1 false positive per minute. The former value is proposed in [21] as a reasonable number for maintaining user trust in the system. The latter is, in our opinion, the highest rate at which a human can verify alerts generated by an IDS. Anomaly-based IDSes, unlike signature-based ones, do not provide information regarding the attack classification, thus requiring additional time to investigate alerts.

Since our datasets may contain some "noise," we use a signature-based IDS (Snort) to verify that the alerts generated during the verification phase are actual false positives.

#### Network Data Description
In this section, we describe the background and attack datasets used for benchmarking the detection algorithms. The chosen datasets include network traffic from two environments, and we use publicly available vulnerabilities and high-impact exploits for testing.

**Web Data Set:**
- **DSDARP A:** The DARPA 1999 dataset [21] is a standard reference used by many researchers. Despite being outdated, it is used to compare the performance of PAYL and Anagram with previous works.
- **ASHT T P:** This dataset, presented by Ingham and Inoue [18], includes 66 diverse attacks, including 11 shellcodes, collected from public archives. The attacks involve buffer overflows, input validation errors, signed interpretation of unsigned values, and URL decoding errors.

**LAN Data Set:**
- **DSSMB:** This dataset includes network traces from a large university network, collected over a week. The average data rate is ∼40Mbps, with an average packet rate for SMB/CIFS traffic of ∼22/sec.
- **ASSMB:** This attack dataset includes seven instances exploiting four different vulnerabilities in the Microsoft SMB/CIFS protocol: ms04-011, ms06-040, ms08-067, and ms10-061 [7]. Detailed descriptions of these vulnerabilities and the corresponding attack instances are provided.

**ICS Data Set:**
- **DSModbus:** This dataset includes network traces from an industrial control network over 30 days, with an average throughput of ∼800Kbps. It features Modbus/TCP, a common protocol in ICS environments. The average size of Modbus/TCP messages is 12.02 bytes, with a high rate of duplicated TCP segment payloads (96.08%).
- **ASModbus:** This attack dataset includes 163 instances exploiting various vulnerabilities in Modbus/TCP implementations, categorized into unauthorized use and protocol errors.

#### Benchmarks
In this section, we present the results of our benchmarks and compare the performances of the algorithms for each dataset.

**Algorithm Setup and Tuning:**
- **PAYL:** We used different merging parameters and set the smoothing factor to 0.001, as suggested by Ingham and Inoue [18].
- **Anagram:** We tested different n-gram sizes and found that 3-grams provided the best results. The Bloom filter size was set to 2MB.
- **POSEIDON:** We used a Self-Organizing Map (SOM) with 96 neurons and 10,000 training instances.
- **McPAD:** We used the best-performing parameters as proposed in [28]: 160 clusters, a desired false positive rate of 1% for each SVM classifier, and a maximum probability combination rule.

**Implementation Verification:**
To verify the correctness of our implementations, we conducted initial tests using the DSDARP A dataset. Table 1 summarizes the results, showing high detection and low false positive rates, consistent with the original papers.

| Algorithm | FPR (packet-based) | DR (packet-based) |
|-----------|--------------------|-------------------|
| PAYL      | 0.00%              | 90.73%            |
| POSEIDON  | 0.004%             | 92.00%            |
| Anagram   | 0.00%              | 100.00%           |
| McPAD     | 0.33%              | 87.80%            |

**Tests with LAN Data Set:**
We first tested the DSSMB dataset using all SMB/CIFS packets directed to TCP ports 139 or 445. None of the algorithms performed well under these conditions. For example, the Bloom filter used by Anagram saturated with 3-grams during the training phase, resulting in no successful attack detection.

---

This revised text aims to improve clarity, coherence, and professionalism while maintaining the technical details and structure of the original content.