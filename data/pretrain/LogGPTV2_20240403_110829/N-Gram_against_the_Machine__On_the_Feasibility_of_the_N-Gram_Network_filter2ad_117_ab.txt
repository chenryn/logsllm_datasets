360
D. Hadˇziosmanovi´c et al.
functionalities. Although Windows systems are usually secured against abuses
of such service from the Internet, corporate users take advantage of this feature
quite often. An attacker that would develop an exploit for a zero-day vulnerabil-
ity leveraging this protocol could potentially aﬀect a large number of systems. In
the last decade several malware [8,14] exploited SMB/CIFS to operate botnets
and carry on other malicious activities.
As described in the introduction, ICS have lately become a valuable target
for cyber attackers. Considering the sensitive character of such environments,
the detection of cyber attacks plays a crucial role, sometimes even in homeland
security. Lately, we have witnessed an increasing number of vulnerabilities dis-
covered in software used in critical facilities, mainly due to the poor software
development cycles that several vendors adopted, and the “security by obscurity”
paradigm used to “protect” legacy devices.
We collect attacks in two diﬀerent datasets. Our focus is on data injection
attacks that have a high impact (see [27]).
Obtaining the implementations. Secondly, to carry out the benchmarks, we need
working prototypes of all the algorithms we want to test. We could obtain an
implementation of POSEIDON and McPAD from the authors. For the other two
algorithms, we write our own implementation1 based on the description found
in the papers. To be sure that our implementation is correct, we need to verify
that our results resemble the ones shown in the benchmarks of the respective
original papers.
Analysing the results. The last step of our evaluation is the analysis of results
with a focus on the identiﬁcation of reasons for (un)successful detection.
3.1 Evaluation Criteria
The eﬀectiveness of an IDS is mainly determined by the detection and false pos-
itive rates. The detection rate indicates the number of attack instances correctly
identiﬁed by the IDS (true positives), w.r.t. the total number of attack instances.
The false positive rate indicates the amount of samples that the IDS ﬂags as at-
tacks when they are actually not. False positives are a major limiting factor in
this domain because, diﬀerently from other classiﬁcation problems, their cost is
high [4].
Detection rate. To provide a detailed overview of the detection capabilities of
each algorithm, we consider both the number of correctly detected packets in
the attack set and the number of detected attack instances. In fact, not all
attacks show malicious payload within one single packet. Although an algorithm
that exhibits a high per-packet detection rate has a higher chance of detecting
attack instance, we do not argue that a low per-packet detection rate implies
an equivalently low per-instance detection. In summary, we consider an alarm
1 We intend to disclose the implementations in near future.
On the Feasibility of the N-Gram Network Analysis for Binary Protocols
361
as a true positive if the algorithm is able to trigger at least one alert packet per
attack instance.
False positive rate. The usual approach to document the performance of an IDS
is to relate the false positive rate with the detection rate. This is done by drawing
so called Receiver Operating Characteristic (ROC) curves. The benchmarks from
the original papers of the proposed algorithms express the false positive rate as
a percentage. However, such number has little meaning to the ﬁnal users. A
better way to express the false positive rate is in terms of the number of false
positives per time unit. We establish two diﬀerent thresholds: 10 false positives
per day and 1 false positive per minute. The former value is proposed in [21] as
a reasonable number for a user to maintain trust in the system. The latter is, in
our opinion, the highest rate at which a human can verify alerts generated by
an IDS. It is worth noting that anomaly-based IDSes, unlikely signature-based
ones, do not provide information regarding the attack classiﬁcation. Thus, the
user might require additional time to investigate whether the alert is a true or a
false positive. For each data set we compute these two thresholds based on the
number of actual packets included in the veriﬁcation sub data set after having
split the original data set.
Since we do not make the data sets attack-free before hand, and thus some
“noise” could have been collected as well, we need a way to verify that the alerts
generated while processing the veriﬁcation sub data set are actual false positives.
To do that, we use a signature-based IDS (the popular open-source Snort), which
is automatically fed with the network stream for which an alert was triggered
during veriﬁcation.
Commonly, in IDS evaluation papers the authors stop their analysis by re-
porting on the true and false positive rates. We believe that inspecting which
attacks are detected, which are not detected and why, would provide useful infor-
mation to fully understand when an algorithm could perform better than others
(and for which threats). This kind of analysis can provide insights for future im-
provements. Finally we aim at evaluating the eﬀectiveness of combining diverse
algorithms to boost the detection rate.
4 Description of Network Data
In this section we describe in detail the background data set and attack data sets
that will be used for benchmarking the detection algorithms. The chosen data
sets comprise network traﬃc taken from two environments. We use the publicly
available vulnerabilities and high impact exploits to run the tests.
4.1 Web Data Set
The following data sets are a collection of network traces of web traﬃc (in
particular, the HTTP protocol).
362
D. Hadˇziosmanovi´c et al.
DSDARP A. The DARPA 1999 data set [21] is a standard data set used as ref-
erence by a number of researchers. Despite being anachronistic (and criticized
in several works [23]), three of the algorithms we test have used this data set to
compare their performance to previous works. Thus, we use the DARPA data set
to verify that our own implementations of PAYL and Anagram oﬀer comparable
detection and false alert rates with the tests reported in the research papers of
the detection algorithms.
ASHT T P . This attack set is presented by Ingham and Inoue in [18], and has
been used also by the authors of McPAD for their benchmarks. It comprises 66
diverse attacks, including 11 shellcodes, which were collected from public attack
archives. The attacks are instances of buﬀer overows, input validation errors
(other than buﬀer overﬂows), signed interpretation of unsigned values and URL
decoding errors.
4.2 LAN Data Set
DSSMB This data set includes network traces from a large University network.
Samples have been collected through a week of observations. The average data
rate of incoming and outgoing packets is ∼40Mbps.
In particular, we focus on the SMB/CIFS protocol, and even more on SMB/
CIFS messages which encapsulate RPC messages (see Section 5.2). The average
packet rate for this traﬃc is ∼22/sec. Based on this we calculate the false pos-
itive rate threshold for obtaining 10 alerts per day as 0.0005% and the one for
obtaining 1 alert per minute as 0.073%.
ASSMB. This attack data set is made of seven attack instances which exploit
four diﬀerent vulnerabilities in the Microsoft SMB/CIFS protocol: ms04-011,
ms06-040, ms08-067 and ms10-061 [7].
ms04-011 is a vulnerability of certain Active Directory service functions in
LSASRV.DLL of the Local Security Authority Subsystem Service (LSASS) of
several Microsoft Windows versions. We select this vulnerability because it is
used by the worm Sasser [26]. We collect two diﬀerent attack instances for
this vulnerability. One trace is downloaded from a public repository of network
traces [13] where the attack payload is split in three fragments and contains a
shellcode of 3320 bytes. The shellcode is made of a number of NOP instruc-
tions (byte value 0x90), followed by valid x86 instructions and a sequence of the
ASCII character ‘1’. The second instance is generated through the Metasploit
framework [24]. The attack payload is split into three fragments and contains a
shellcode of 8204 bytes to remotely launch a command shell in the victim host.
m06-040 is a vulnerability of the Microsoft Server RPC service. In particular,
the vulnerability allows a stack overﬂow during the canonicalization of a net-
work resource path. The speciﬁed path can be crafted to execute arbitrary code
after the exploitation. We collect the attack instance from a public repository of
network traces [13].
On the Feasibility of the N-Gram Network Analysis for Binary Protocols
363
ms08-067 is a vulnerability of the Microsoft Server RPC service which exploits
a similar weakness as the one described in m06-040, with the same eﬀects. We
select this vulnerability because it is used by Conﬁcker [8] and Stuxnet, two
high-impact pieces of malware. We collect two diﬀerent attack instances for
this vulnerability. One instance was downloaded from a public repository [13],
while the other one was generated by us using the metasploit framework. In the
ﬁrst instance the payload contains a shellcode of 684 bytes, while in the second
instance the shellcode is 305 bytes long only.
ms10-061 is a vulnerability of the PrintSpooler RPC service. When printer
sharing is enabled, the PrintSpooler service does not properly validate spooler
access permissions. Remote attackers can create ﬁles in a system directory, and
consequently execute arbitrary code, by sending a crafted print request over RPC.
We select this vulnerability because it was used by Stuxnet to successfully propa-
gate in both regular backoﬃce LANs as well as in industrial control system envi-
ronments. We collect two diﬀerent attack instances for this vulnerability, both of
them are generated through the metasploit framework. In one instance the attack
payload is a binary ﬁle (the meterpreter executable), which accounts for 69832
bytes spanned over 18 fragments, while in the other instance the payload consists
of a DLL ﬁle, which accounts for 1735 bytes spanned over three fragments.
4.3 ICS Data Set
DSModbus To test the anomaly detection algorithms on ICS networks we collect
a data set of traces from the industrial control network of a real-world plant over
30 days of observation. The average throughput on this network is ∼800Kbps.
This data set includes network traces of one of the most common protocols
used in such environments, Modbus/TCP [32]. Modbus was developed more than
30 years ago initially as a protocol used in serial channels, while the TCP/IP
variant was introduced approximately 15 years ago to allow the serial protocol
to be used in TCP/IP networks. Modbus/TCP features basic instructions and
functions. Its structure is relatively simple, and operators of critical infrastruc-
tures usually repeat a limited set of operations, thus reducing the variability of
the transmitted data. In fact, the maximum size of a Modbus/TCP message is
256 bytes. Thus, a Modbus/TCP message is always contained in one single TCP
segment. Observations on DSModbus reveal that the average size of Modbus/TCP
messages is 12.02 bytes (in DSSMB it is 535.5 bytes). In the observed DSModbus,
the number of duplicated TCP segment payloads is high (96.08%), in contrast to
the other data set (e.g., DSSMB has 31.37%). In other words, more than 9 TCP
segments out of 10 carry a Modbus/TCP message that is a perfect duplicate
of some other message already observed. The average packet rate for Modbus
incoming traﬃc is ∼96/sec. Based on this observation, we calculate the false
positive rate threshold for generating 10 alerts per day as 0.00012% and the
threshold for generating 1 alert per minute as 0.017%.
ASModbus. The attack data set is made of 163 attack instances, which exploit
diverse vulnerabilities of the Modbus/TCP implementations. There are fewer
364
D. Hadˇziosmanovi´c et al.
publicly known attacks against Modbus/TCP devices than SMB/CIFS attacks.
Network traces for a good deal of these attacks can be downloaded from the web-
site of an ICS security ﬁrm [11]. The exploited vulnerabilities can be categorised
in two large families: unauthorised use and protocol errors.
Unauthorized use consist of two attack types: (a) “weird” clients talking to
the Modbus server and (b) messages used only for diagnostics and special main-
tenance, which are thus seldom seen in the network traﬃc. By issuing these
special messages, the attacker is often able to achieve a complete take over of
the device.
Protocol errors are mainly fuzzing attempts against a device. For instance,
these attacks are carried out by sending data not compliant with the protocol
speciﬁcations (e.g. a too short protocol data unit). The outcome of such attacks
can range from the unavailability of the device up to the control of the execution
ﬂow (see Cui and Stolfo [9] for a more detailed discussion).
5 Benchmarks
In this section we show the results of our benchmarks and compare the perfor-
mances of the algorithms for each data set.
Setting up and tuning the algorithms. For each dataset we split the background
traﬃc into two sets, one for training and one for veriﬁcation. The splitting is
performed randomly, by sampling the network streams. Each split sub data set
accounts for nearly 50% of the original data. The training sub data set is used
to build the detection proﬁles for each algorithm, while the veriﬁcation one is
used to evaluate the number of false positives raised by the algorithm. Finally,
we run the algorithms on the attack data set.
For performing the benchmarks we need to set up several starting parameters
for each algorithm.
PAYL. As introduced in the original paper, the size of the PAYL model can
be reduced by merging proﬁles when their number becomes too large. For each
data set we perform several runs using diﬀerent values of the merging parameter.
Finally, for the DSDARP A data set we set the parameter value to 0.12. For the
remaining data sets we do not merge proﬁles, as the total number of proﬁles
remains low (up to 150, compared to 480 in DSDARP A). As the “smoothing
factor”, we use the same value (0.001) suggested by Ingham and Inoue in [18].
We apply the algorithm to individual TCP segment payloads.
Anagram. For each data set we run tests with diﬀerent n-gram sizes (n size
of 3, 5, 7, 9 and 12). Since we obtain the best results with the 3-grams, we set
this as the standard n-gram size. As in the original paper, we set the size of the
Bloom ﬁlter to 2MB. We do not use the “bad content model” proposed by the
authors because it would be ineﬀective as they build it with virus samples, and
our attack data sets do not include viruses.
POSEIDON. For all tests we use a SOM with ﬁxed number of neurones
(96). Also, we set the number of instances for training the SOM at 10000.
On the Feasibility of the N-Gram Network Analysis for Binary Protocols
365
McPAD. For all tests we use the best performing parameters as proposed
in [28]. Those are: number of clusters k = 160, desired false positive rate for each
SVM classiﬁer set to 1% and maximum probability as combination rule for the
output of the SVM classiﬁers.
For each algorithm we vary the value of the threshold to observe how the false
positive and detection rates change.
5.1 Implementation Veriﬁcation
To verify the correctness of our implementations we run initial tests using
DSDARP A data set (since that was the only common data set used in 3 orig-
inal algorithm benchmarks). Instead of using DARPA attack dataset, we use
the ASHT T P for testing. There are two main reasons for this: (1) the original
attack instances of the DARPA data set do not reﬂect at all modern attacks,
and (2) not all the algorithms have been benchmarked against the attack set of
DARPA (Anagram is the exception). Thus, it would be impossible to faithfully
reproduce the previous experiments. In Table 1 we summarize the results of this
ﬁrst round of benchmarks: for each algorithm we report the highest detection
rate we achieve, and the corresponding false positive rate.
Table 1. Test results on DSDARP A and testing with ASHT T P
FPR
DR
(packet-based) (packet-based)
PAYL
POSEIDON
Anagram
McPAD
0.00%
0.004%
0.00%
0.33%
90.73%
92.00%
100.00%
87.80%
All the algorithms show high detection and low false positive rates. When
compared to the original papers, these results match our expectations, thus we
can be reasonably sure that our re-implementations are not (too) dissimilar from
the original ones in terms of completeness and accuracy.
5.2 Tests with LAN Data Set
We ﬁrst perform the test on DSSMB by using all the SMB/CIFS packets directed
to the TCP ports 139 or 445. However, none of the algorithm can perform well
enough under these conditions. For example, the Bloom ﬁlter used by Anagram
saturates with 3-grams during the training phase. Consequently, no attack in-