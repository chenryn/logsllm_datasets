title:Assessing time coalescence techniques for the analysis of supercomputer
logs
author:Catello Di Martino and
Marcello Cinque and
Domenico Cotroneo
Assessing Time Coalescence Techniques for the
Analysis of Supercomputer Logs
Catello Di Martino
Center for Reliable and High-Performance Computing
University of Illinois at Urbana-Champaign
1308 W. Main Street, Urbana, IL 61801, USA
Email: PI:EMAIL
Marcello Cinque, Domenico Cotroneo
Dipartimento di Informatica e Sistemistica
Universit`a degli Studi di Napoli Federico II
Via Claudio 21, 80125 Napoli, Italy
Email:{macinque,cotroneo}@unina.it
speciÔ¨Åc coalescence window, they are grouped in the same
tuple, representing a single failure.
Abstract‚ÄîThis paper presents a novel approach to assess
time coalescence techniques. These techniques are widely used
to reconstruct the failure process of a system and to estimate
dependability measurements from its event logs. The approach
is based on the use of automatically generated logs, accompanied
by the exact knowledge of the ground truth on the failure process.
The assessment is conducted by comparing the presumed failure
process, reconstructed via coalescence, with the ground truth.
We focus on supercomputer logs, due to increasing importance
of automatic event log analysis for these systems. Experimental
results show how the approach allows to compare different time
coalescence techniques and to identify their weaknesses with
respect to given system settings. In addition, results revealed an
interesting correlation between errors caused by the coalescence
and errors in the estimation of dependability measurements.
Index Terms‚ÄîEvent Log Analysis, supercomputer dependabil-
ity, data coalescence, dependability assessment
I. INTRODUCTION
Event
logs represent one of the main data sources for
analyzing the dependability behavior of computer systems
during the operational phase. They are being largely used in
the context of supercomputers [1], [2], where the unattended
operation of the system forces administrators to look at the
logs written by applications and system modules to analyze
the occurrence and consequences of system outages [3].
A signiÔ¨Åcant issue in event log analysis is to determine
the real occurrences of failures, starting from raw log entries.
The problem is that, when a failure manifests in the system,
multiple apparently independent error events may be written
in the log. Data coalescence techniques aim to reconstruct
the failure process of the system by grouping together events
related to the same failure. IdentiÔ¨Åed occurrences are then
used to classify the failure modes of the system and to
evaluate dependability measurements, such as, the mean time
between failure (MTBF), the mean time to recover (MTTR).
Hence, data coalescence represents a crucial step in failure
data analysis, since inaccurate grouping of failures leads to a
distorted estimation of the dependability of the system.
In the last decade, several coalescence techniques have been
proposed. Many of them are variations of the well known time-
based tuple heuristic [4], [5], which has been used in a large
variety of studies [1], [3], [5]‚Äì[13]. The heuristic is based on
the assumption that events related to the same failure are close
in time. Hence, if the timestamps of two events fall within a
978-1-4673-1625-5/12/$31.00 ¬©2012 IEEE
Despite the large use of these techniques, several studies
recognized the problem of the accuracy of log-based depend-
ability analysis [12], [13]. As a matter of fact, a single tuple
may contain events related to different failure manifestations
(also known as collisions), or events related to the same failure
may be wrongly coalesced in different tuples (also known as
truncations). The most common criticism against coalescence
studies is the lack of assessment approaches, since the ground
truth on the real failure process is usually not available. In
other words, we cannot know how well coalescence techniques
are able to reproduce the actual failure process. The problem is
clearly exacerbated in the case of supercomputers, where the
size and the complexity of the system increase the log size by
orders of magnitude. These issues contribute to a decreased
level of trust on log-based dependability analysis [3].
Past attempts towards the assessment of coalescence tech-
niques moved along two main directions: 1) to assume a
theoretic failure process as the ground truth [5], and 2) to
extract the ground truth from accurate interviews with system
administrators [1], [3], [7], [8], [14]. Along direction 1), only
simplistic mathematical models have been adopted so far [5]
(such as, exponential failure inter-arrival). Therefore, it is still
unclear how system-related aspects, such as the workload and
the propagation of failures among components and nodes,
impact on the accuracy of results. As for direction 2), the
accuracy of the analysis is often biased by the subjective
knowledge of system administrators. Hence, the problem of
assessing coalescence techniques is still unresolved.
In this paper we propose a novel approach to assess time
coalescence techniques. The driving idea is the following:
given a system and a failure process, we provide a model-
based tool able to properly generate synthetic logs along with
the ground truth they represent. The assessment is then con-
ducted by comparing the presumed reality, reconstructed by
coalescing synthetic logs, with the objective reality provided
by the ground truth. Synthetic logs and related ground truth
are generated in a web-based framework based on a set of
Stochastic Activity Networks [15]. The framework along with
the proposed approach allow to evaluate the sensitivity of
coalescence techniques against conÔ¨Åguration parameters (e.g.,
the coalescence window), and to estimate how system related
aspects, such as, the number of nodes in the system, the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:19:29 UTC from IEEE Xplore.  Restrictions apply. 
workload, the failure type and propagation patterns impact on
the accuracy of measurements.
Experiments focus on the analysis of four different state-of-
the-art time based coaelcence techniques and reveal interesting
Ô¨Åndings. First, we found a strict correlation between the esti-
mation error on the MTBF and the percentage of truncations
and collisions; hence, if techniques are deÔ¨Åned able to estimate
the probability of truncations and collisions, it will be possible
to estimate the MTBF error as well. This is an important
Ô¨Ånding, since it shows that it is possible to relate measurement
errors to coalescence errors. Second, we found that even
when the techniques are tuned with theoretically optimal
values for their parameters, still the statistical properties of
failure distributions may not be preserved, due to unavoidable
accidental collisions. Third, we found an unacceptable increase
of the error as the size and complexity of the system increases;
this suggests that time coalescence techniques need to be
rethought to be usefully adopted also in future large-scale
computer systems.
The rest of the paper is structured as follows. Section II
underlines the needed background and relation with the state of
the art. Section III outlines our validation approach, described
in details in section IV. Section V reports experimental results,
discussing the main Ô¨Åndings obtained. Finally, Section IV ends
the paper with lessons learned.
II. BACKGROUND AND RELATED WORK
Logs are human-readable text Ô¨Åles reporting sequences of
text entries (the log events), ranging from regular to error
events occurred at runtime [5].
The analysis of logs usually accounts three consecutive
steps: i) data Ô¨Åltering [1], [3], [8], [16], [17], concerning
the removal of log events not related to failures, ii) data
coalescence, concerning the grouping of redundant or equiv-
alent failure events, and iii) data analysis, concerning the
evaluation of dependability measurements, the modeling of the
failure process [10], [18]‚Äì[22], and the investigation of failure
propagation phenomena among different nodes [1], [13], [21],
[23] and among different subsystems within nodes [1], [23].
Data coalescence is crucial in log analysis, since it aims
to reconstruct the failure process of the system by grouping
together log events related to the same presumed failure. The
reason is that, as the effects of a fault propagate through a
system, hardware and software detectors are triggered resulting
in multiple events reported in the log [5]. Moreover, the same
fault may persist or repeat often over time [3].
Coalescence techniques can be distinguished in time, spa-
tial, and content based.
Time coalescence is based on the assumption that
log
events due to the same cause are close in time. An important
methodological achievement for time coalescence has been
the deÔ¨Ånition of the tuple heuristic [4], [5]. According to the
heuristic, all the events that fall within a speciÔ¨Åc time window
are grouped in the same tuple. Clearly, the selection of an
appropriate value for the time window is crucial. In [5] a
heuristic for the selection of a single coalescence time window
for the Tandem TNS II system is presented. Authors found
that the number of tuples of a given log is a monotonically
decreasing function of the time window, with a characteristic












		
(a) truncations 









	
(b) optimal grouping 


	








	


	
	
(c) collisions 
Fig. 1: Example of wrong grouping: (a) truncations and (c)
collisions with respect to optimal grouping (b).
‚ÄúL‚Äù shaped curve. According to their Ô¨Åndings, the ‚Äúknee‚Äù of
the curve represents the internal clustering time of the system,
hence, the time window should be chosen right after the knee.
Spatial coalescence is a variation of time coalescence used
to relate events that occur close in time but on different nodes
of the system under study. It typically consists in applying
time coalescence techniques on log Ô¨Åles obtained by merging
the logs of individual nodes [1], [3], [13], [24].
Time and spatial coalescence techniques have been applied
to a variety of large scale computing systems [2], [3], [5]‚Äì[8],
[11], [16], [20], [25], [26]. The common trend is to use tupling
with a Ô¨Åxed value for time window, such as 5 minutes [1], [3],
[6]‚Äì[8], [27], [28], 20 minutes [9], [10], [12], and 60 minutes
[12], [13], usually without any tuning (such as, the knee rule)
or validation.
Content-based coalescence techniques are emerging recently
[14], [16], [28]‚Äì[30], based on the grouping of events by
looking at the speciÔ¨Åc contents of log messages. For instance,
[28] and [30] apply the lift data mining operator to Ô¨Ånd fre-
quent event patterns starting from log contents, hence isolating
accidental patterns. In this work we focus on time coalescence
techniques, being them the most adopted in the literature due
to their simplicity, and we plan to extend our approach to
content-based techniques.
It is known that data coalescence can distort the results of
the analysis due to imperfect groupings caused by truncations
and collisions [5], [31]. Figure 1 provides an example of
truncations and collisions with reference to time and spatial
coalescence. A truncation occurs when the time between two
or more events caused by a single failure is greater than the
clustering time, thus causing the events to be split into multiple
tuples (Figure 1.(a)). A collision occurs when two independent
failures occur close enough in time such that their events
overlap and they are erroneously combined into a single tuple
(Figure 1.(c)). Consequently, the goodness of the coalescence
process is strictly dependent on the selected time window [5].
Despite these issues, still little attention has been devoted
to the assessment and validation of coalescence techniques.
One approach often used in the Ô¨Åeld is to adopt failure
reports by system administrators as the ground truth [14],
since they contain data which do not require manipulation.
However, administrator reports are not always available in all
systems, or they can be biased by the subjective knowledge
of administrators.
In [5] authors develop a model
to relate the collision
probability to the event arrival rate in the log, and to study the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:19:29 UTC from IEEE Xplore.  Restrictions apply. 

% 
& '
 
% 
 #
'!"
% #
"#	
 "



 
	

	




&"!

)

)*
)

)*
 "
'"#	!

!
%!




"'

!!!!"


	
Fig. 2: Representation of Validation Methodology.
sensitivity of tupling to the dimension of the time window.
Using a similar model, Buckley et al. in [31] assessed the
effectiveness of Tsao‚Äôs [4] and Hansen‚Äôs [5] tupling heuristics,
both causing as low as 15% of collisions. However, the tuple
heuristic and the models used for its validation have been
originally deÔ¨Åned for past centralized and small-scale systems.
As a consequence, the rate of collisions is expected to be
sensibly larger in modern supercomputers, due to both the
non-exponential fault process [3], [20], and the increased inter-
arrival rate of log events.
III. THE LOG GENERATOR APPROACH
The proposed approach is based on the generation of
synthetic logs for which the ground truth is known. The ground
truth represents the actual failure process of the system being
analyzed (the objective reality), which may not correspond to
the presumed failure process reconstructed by coalescing pro-
duced logs. Hence, a key objective of the proposed approach is
to establish how close the presumed reality is to the objective
reality, e.g., how much dependability measurements estimated
from processed logs, such as MTBF and MTTR, differ from
the ground truth. This also means to assess how much wrong
grouping (i.e., collisions and truncations) affect measurements.
Figure 2 summarizes the proposed assessment and valida-
tion approach. Step 1 concerns the generation of synthetic logs
jointly with the ground truth, stored in the oracle log. Synthetic
logs are similar to real system logs: each entry contains a time
stamp, the system component/node that wrote the entry, and
the error message. The oracle log, instead, contains detailed
information about every single failure affecting the system,
including its start time, end time, type, and the set of system
resources involved (e.g., nodes and subsystems).
The correct emulation of the failure process of the system
is a fundamental step to generate synthetic and oracle logs. In
particular, in the case of supercomputers, the failure process
depends on the following aspects, to be taken into account:
‚Ä¢ the system conÔ¨Åguration in terms of number and role of
computing nodes, as well as the number and type of sub-
systems composing them (e.g., hardware, I/O, network,
etc.); this information is needed since it impacts on the
number and type of failures and on their propagation [20],
[22], [27];
‚Ä¢ the workload,
in terms of number and type of jobs
and their inter-arrival, queuing, scheduling, duration, and
number of required nodes; as known, the workload inÔ¨Çu-
ence the failure behavior of the nodes in terms of failure
inter-arrival and failure duration distributions [32], due to
the different subsystems stressed during the computation;
‚Ä¢ the characterization of failure and recovery processes of
nodes, which impact on the number and distribution of
errors stored in the logs;
‚Ä¢ the characterization of failure propagation phenomena;
it
is known that failures can propagate between the
subsystems composing a node [33], [34] or among the
nodes composing the system [1], [8];
‚Ä¢ the detection and logging mechanisms which emulate the
writing process of events in the synthetic logs, due to
failures.
Once synthetic logs have been generated, they are processed
using different coalescence techniques,
the
presumed reality (step 2 in Figure 2), which is analyzed to
evaluate dependability measurements (step 3). Finally, in step
4, the presumed reality is compared to the oracle log to assess
measurement errors.
to reconstruct
IV. THE LOG GENERATOR FRAMEWORK
The proposed approach has been implemented in a frame-
work composed of three main elements: a user interface, a
log generator component, and a tool, named analyzer, for
automating the analysis of the results. The user interface is
implemented as a web-based application1, and it is in charge
of collecting information about the conÔ¨Åguration of the system