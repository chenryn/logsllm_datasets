togram shows, the variability of the captcha variable is so
small that it can be considered almost a constant between
the diﬀerent installations.
4. PART II: THE USER POINT OF VIEW
Despite the backscattering phenomenon described in the
previous section, CR systems are often considered one of
the most eﬀective ways to protect users from spam. In the-
ory, if the challenge-response system is properly conﬁgured,
these systems should be able to achieve a 100% detection
rate, thus blocking all unsolicited emails. However, previ-
418Figure 6: Spam clustering statistics
ous studies [21] that conﬁrmed this value were conducted on
prototype systems evaluated in a controlled environment.
In this section we measure if this is actually the case in a
real-world installation, and we evaluate the real impact for
the end users protected by a CR system. In particular, we
measure the delay introduced in the legitimate emails deliv-
ery, and the amount of spam that is able to reach the ﬁnal
users despite the CR ﬁlter. In addition, we also measure the
change rate of the users’ whitelists, one of the foundations
of this kind of antispam solution.
4.1 Spam Protection
The main purpose of a CR system is to block all automati-
cally generated emails coming from addresses previously un-
known to the recipient. The ﬁrst obvious consequence of
this approach is that CR solutions are ineﬀective by design
against targeted attacks, i.e., attacks in which the attacker
manually composes a malicious message to target a partic-
ular individual.
In fact, if the attacker receives back the
challenge message, he can easily solve it and have his mes-
sage delivered to the recipient. However, a recent Symantec
report [35] estimated that only one out of 5,000 spam mes-
sages contains a targeted attack. In addition, all the existing
anti-spam mechanisms can be easily evaded by targeted at-
tacks, and, therefore, we can consider this threat beyond
reach of all existing anti-spam solutions.
Unfortunately, targeted attacks are not the only ones that
can pass through a CR ﬁlter. By studying a dataset of
bounced challenges, we noticed that a large number of mes-
sages had the same subject and the same size. Per se, this
is not surprising. However, a closer look revealed that while
most of the messages were bounced or dropped by the ﬁlter,
in some cases one of those emails was successfully delivered
to the ﬁnal user’s mailbox.
To better investigate the reason behind this sporadic phe-
nomenon, we decided to analyze the behavior, in terms of
challenges and delivered messages, of a number of large spam
campaigns.
For our experiment we applied standard clustering algo-
rithms to the subject of the messages in the gray spool (i.e.,
the ones for which the system generated a challenge mes-
sage).
In particular, we put in the same cluster the mes-
sages with the same subject, limiting the analysis to the
ones at least 10 words long. Finally, we discarded the clus-
ters containing less than 50 emails. These very conservative
thresholds were adopted to greatly reduce the risk of mis-
classiﬁcation. In reality, the large majority of emails (includ-
ing spam) have much shorter subjects, or they have enough
variability to elude our simple comparison algorithm. How-
ever, our target was not to be able to cluster and identify
all the incoming emails or all the spam campaigns, but just
to identify a number of them with a low percentage of false
positives.
The results obtained over a three month monitoring pe-
riod are summarized in Figure 6. Our system identiﬁed
1,775 clusters, containing between 50 and 3696 messages
each. In the next step, we divided the clusters in two cat-
egories, based on the sender email similarity.
In the ﬁrst
group we put all the clusters where emails are sent by a
very limited number of senders, or in which the sender ad-
dresses are very similar to each other (for example, dept-
PI:EMAIL, PI:EMAIL, and dept-x.p@scn-
2.com). These clusters are likely associated to newsletters
or marketing campaigns. The second group contains instead
the clusters with a very low sender similarity, i.e., the ones
in which most of the emails originate from diﬀerent domains
and diﬀerent senders’ addresses. This behavior is very com-
mon in spam campaigns sent by malware infected machines.
Figure 6 shows that only 28 out of 1774 clusters contain
at least one solved challenge. Moreover, these few clusters
have very diﬀerent characteristics, depending on whether
they belong to the ﬁrst or the second category. The ones
with high sender similarity have a higher rate of solved chal-
lenges (some clusters as high as 97%) and almost no bounced
emails. The clusters with low sender similarity have instead
on average 31% of emails bounced because of non-existing
recipient, and only one or two captchas solved each.
This second category is particularly interesting for our
study. Each cluster in this group contains hundreds of emails,
coming from many diﬀerent domains, and often from non-
existing sender addresses. However, out of these messages,
sometimes one of the challenges was solved and, therefore,
the email got whitelisted and delivered to the recipient’s
mailbox. These spam messages that are able to pass through
the CR defense are likely a side eﬀect of backscattered chal-
lenges that are sometimes erroneously delivered to innocent
users. As a result, it is possible that one of these users solves
a challenge for a mail he never sent. This phenomenon is,
419Figure 7: Cumulative eﬀect of Captcha and Digest
whitelisting
Figure 8: Time distribution of whitelisted messages
however, extremely rare. According to our measurements,
we estimate that this kind of spurious spam delivery occurs
∼1 every 10,000 challenges sent. According to Table 1, this
rate translates to an average of ﬁve spam delivery a day, over
the 47 companies in our dataset. Excluding these isolated
cases, CR systems are actually able to block all incoming
spam messages.
4.2 Impact on Messages Delivery
Another consequence of blocking automatically generated
emails is the fact that also normal emails can get blocked and
remain in the user’s graylist waiting for the corresponding
challenges to be solved. This can happen for two reasons:
because the sender still has to solve the challenge, or because
the email is sent by an automatic system and the challenge
is, therefore, dropped or never delivered. In both cases, the
user fails to receive a (maybe important) email.
Figure 7 shows the CDF of the messages that were moved
from the graylist to the whitelist in the monitored servers.
The two curves report the percentage of messages that were
whitelisted because the sender solved the challenge, and the
ones that were whitelisted manually by the user from the
daily digest. According to the graph, 30% of the messages
are delayed less than 5 minutes, and half of them are deliv-
ered in less than 30 minutes. However, if the challenge was
not solved after 4 hours, then it is likely that it will not be
solved at all (Figure 8). In those cases, the user has to man-
ually select the messages from the digest, with a delivery
delay that is on average between 4 hours and 3 days.
Combining the values from these ﬁgures with the number
of white and whitelisted emails (31 and 2 respectively) in
Figure 1, we can conclude that:
• 31/33 = 94% of the emails in the user’s INBOX are
sent from addresses already in the whitelist, and, there-
fore, are delivered instantly.
• Out of the remaining 6% (2/33) of the messages that
are quarantined in the gray spool, half of them are
s
t
s
i
l
e
t
i
h
W
#
0
0
0
5
0
0
0
4
0
0
0
3
0
0
0
2
0
0
0
1
0
51.10%
29.50%
12.59%
4.75%
1.62%
0.35%
0.10%
1−10
10−30
30−60
60−120
120−240
240−600
>600
New Entries in 60 Days
Figure 9: Distribution of the number of changes in
users’ whitelist
Figure 10: Daily pending email distribution of 3 dif-
ferent users
delivered in less than 30 minutes because the sender
solved the challenge.
• Only 0.6% (10% of the 6%) of the messages were de-
livered with more than one day of delay.
4.3 Whitelists’ Change Rate
We conclude this section on the user point of view with
an analysis of the rate at which the users’ whitelists change
over time. For this experiment we monitored the number of
changes in the users’ whitelists for a period of two months.
Email addresses can be added to a whitelist in four diﬀerent
ways, two manual and two automated. A user can manually
import a new entry or he can whitelist a certain address
from the digest. Automatically, new entries are included
420when the user sends a mail to those addresses or when the
senders solve the challenges generated by the CR system.
During the monitored period, 9267 whitelists were mod-
iﬁed at least once. Out of them, only 6.8% had at least 1
new entry per day (on average), and the percentage drops
even further when we look at higher changing rates (2.1% of
the whitelists had at least 2 new entries per day, and 0.2%
at least 5). Figure 9 presents a more detailed histogram of
the frequency of changes. The graph shows how the large
majority of the whitelists are, in fact, constantly in a steady
state.
Finally, we monitored the amount of new messages present
in the daily digest. This value varies greatly between users
and also between diﬀerent days. Figure 10 shows examples
extracted from three diﬀerent users. Some of them have
constantly a large number of messages in the gray spool,
while others have normally very small daily digests with
anomalous peaks in conjunction to particular user behavior
or unusually large amount of spam messages.
Again, a large size of the digest is at the same time a
good and a bad factor for a CR system.
In fact, a high
number of messages means that the system is blocking a
substantial amount of spam that would have been otherwise
delivered to the user (remember that these are messages that
successfully pass through the antivirus, reverse DNS, and the
SpamHouse blacklist). On the other side, a large digest is
also a negative factor as it increases the amount of work for
the user that has to manually verify its content to double-
check that he did not miss any important message. Finally,
this also demonstrates that the degree to which CR system
works depends a lot on the interplay of users’ involvement.
Some recipients may diligently weed out their digest, while
others may let it grow hoping for the senders to respond to
the challenges.
5. PART III: THE ADMINISTRATOR
POINT OF VIEW
In this section we analyze some of the issues related to
maintaining challenge-response systems from the system ad-
ministrator point of view. In particular, we focus on the ef-
fort required to maintain the challenge-response infrastruc-
ture, and on the additional antispam techniques that can be
integrated in the system to reduce the backscattering eﬀect.
5.1 Server Blacklisting
As we already described in Section 4, when a CR system
sends a challenge in response to a message with a spoofed
sender’s address, the challenge is delivered to a recipient
that may not exist. As a result, these challenge-response
messages can hit a spam trap [31], i.e., a set of email ad-
dresses that are maintained and distributed with the sole
purpose to lure spam.
The emails collected by those traps are often adopted by
popular services (e.g., SpamHaus [11], SORBS [7], Spam-
Cop [10]) to update their blacklists. Hence, the IP used to
send the challenges can itself get blacklisted as a result of
the backscattered spam it sends. In order to reduce the im-
pact of being blacklisted, one third of the systems we tested
in our experiments were conﬁgured to rely on two MTA-
OUT servers (with diﬀerent IP addresses): one to send the
challenges and another to send the outgoing user emails.
Our initial hypothesis was that the probability that a
Figure 11: Server blacklisting rate
server has to get blacklisted should have been somehow pro-
portional to the size of the email server, represented either
by the number of users, or by the number of the received
emails. In other words, we expected that systems sending
more challenges were blacklisted more often, thus making
CR solutions more diﬃcult to maintain for large companies.
Surprisingly, our experiments proved us wrong. Using the
data we collected we were able to estimate the rate at which
various challenge server IPs get blacklisted. In particular, we
followed two parallel approaches. In the ﬁrst, we analyzed
one month of data for 32 companies, measuring the ratio
between the number of challenges sent and the number of
blacklist-related error messages received from the challenge-
response recipients. The result, summarized on a logarith-