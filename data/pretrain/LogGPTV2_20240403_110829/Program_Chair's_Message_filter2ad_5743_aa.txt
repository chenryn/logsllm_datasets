title:Program Chair's Message
author:Peng Ning
Program Chair’s Message 
It has been a privilege to serve as the program chair for ASPLOS 2014. I am inspired and humbled 
by the breadth and quality of the work submitted and the commitment of the organizing team to an 
excellent conference.  
The broad goal of a conference is to move the field forward. I was fortunate that recent PC chairs 
of  ASPLOS  led  very  successful  conferences,  and  I  mostly  followed  in  their  footsteps. 
Nevertheless I identified two narrower goals for additional efforts: (1) to continue to enhance the 
image of ASPLOS as a broad, multidisciplinary conference, and (2) to continue to raise the bar for 
quality  and  fairness  in  our  review  process.  This  document  describes  the  processes  I  used, 
summarizes the program, and acknowledges those who made this program possible.  
Scope of ASPLOS  
The call for papers emphasized that ASPLOS is a broad multidisciplinary conference open to new 
and  non-traditional  systems  related  topics.  I  reached  out  to  “non-traditional”  SIGs  and  other 
organizations to advertise the call. One concrete outcome is that ASPLOS is “in-cooperation” with 
SIGBED this year. I hope our ties with SIGBED continue to strengthen in the coming years. 
The  call  also  said  that  papers  should  emphasize  synergy  of  at  least  two  ASPLOS  disciplines 
(broadly defined) and the submission form explicitly asked authors to identify these areas. 
We received 217 submissions, a 12% increase from last year and a new record. About 147 papers 
identified architecture as one of their areas, 89 identified PL or compilers, and 94 identified OS. 
Many  papers  identified  other  broad  areas,  including  verification,  graphics,  big  data,  networks, 
cloud computing, mobile computing, embedded systems, software engineering, and more. 
For specific research topics, again there was a large diversity. The following were listed by more 
than  20  submissions:  power/energy/thermal  management  (34  papers),  parallel  architecture  (31), 
heterogeneous architectures and accelerators (28), caches (27), high-performance computing (26), 
OS  scheduling  and  resource  management  (26),  compiler  optimization  (23),  software  reliability 
(23), virtualization (22), parallel programming languages (21), and programming models (21). The 
accepted papers constitute a similarly diverse set of topics. 
In  summary,  ASPLOS  continues  to  live  up  to  its  vision  of  a  multidisciplinary  conference, 
attracting a broad range of systems related researchers. 
The Review Process: Setting the Stage 
I was fortunate to have an excellent program committee (PC) of 35 members and external review 
committee (ERC) of 69 members. An additional 43 reviewers contributed to the process. 
Review  process  overview:  I  used  a  two  phase  process  similar  to  recent  ASPLOS  conferences. 
Unlike ASPLOS’13 but like other ASPLOS conferences, I obtained phase 2 reviews for only a 
subset of papers. The two review phases were followed by an author response period, an intense 2-
week  long  online  discussion  period,  and  a  1-day  in-person  PC  meeting.  ERC  reviewers  were 
required to participate in all parts of this process except for the PC meeting. 
Aids for reviewer assignments: I assigned all reviews. To help find the best reviewers, I used the 
following aids new to ASPLOS. (1) The submission instructions allowed unlimited pages for the 
bibliography and required all citations to include all authors (i.e., no et al.). This borrows from 
NSF and makes it easier to identify potential reviewers from related work. (2) I asked reviewers to 
iv 
suggest other reviewers as part of their review, borrowed from ISCA’13. (3) I allowed authors to 
(optionally) suggest reviewers.  
I  required  all  reviewers  to  write  their  assigned  reviews  themselves.  Consulting  with  others  for 
small aspects was allowed if it brought clear added value (after checking with me for conflicts), 
but the assigned reviewers needed to write the bulk of the review and give their own scores.  
System enhancements for conflicts: I used double-blind reviewing and handled conflicts of interest 
using community norms. Sandhya Dwarkadas handled the 20 papers with which I had a conflict 
using  the  same  process  described  here.  I  requested  Eddie  Kohler,  the  author  of  the  HotCRP 
reviewing software, to add a new mechanism to handle PC chair conflicts. This mechanism allows 
the PC chair to yield (most) chair privileges to a different “manager” for a given paper, and allows 
reviewers  to  email  paper-specific  issues  to  the  (anonymous)  paper  manager.  This  system  (1) 
significantly  streamlined  handling  the  review  process  for  Sandhya,  (2)  was  better  at  hiding 
sensitive information from me than alternatives I had previously used, and (3) was better at hiding 
the  conflict  from  assigned  reviewers,  preventing  inadvertent  guessing  of  author  identity.  I  am 
grateful to Eddie for implementing such an intrusive change at very short notice. 
Tone: I consistently set a tone that encouraged reviewers to not look for perfection but to focus on 
moving the field forward while being constructive and fair to all authors. I emphasized that there 
was no target acceptance rate and all worthy papers would be accepted.  Accepting many papers 
implies multitrack sessions and potentially reduced interaction. To mitigate these downsides, the 
first day of the conference features lightning presentations and the second day features a poster 
session for all papers, borrowed from MICRO’12. Although I used numeric scores in the review 
forms, decisions were made based on the review texts.  
Phase 1 Reviews 
I assigned 2 PC and 1 ERC reviewers for each paper in phase 1. I received virtually all reviews 
within two days of the completion deadline (I learned to nag). Unlike ASPLOS’13 (but like other 
conferences), I did not move all papers to phase 2. An important benefit of two phase reviewing is 
to relieve reviewers of the burden of reviewing clearly weak papers. On the other hand, a process 
that  rejects  papers  on  the  basis  of  just  three  reviews  risks  inadequately  informed  decisions  and 
inadequate author feedback. I put significant effort to balance these opposing demands. 
I classified papers with at least one accept score for overall merit to move to phase 2 by default. 
Papers with all reject scores with high confidence levels were candidates to not move to phase 2. I 
highlighted  the  remaining  38  papers  (no  accepts,  but  some  undecided  and/or  low  confidence 
scores)  for  online  discussion  among  reviewers,  using  the  HotCRP  comments  system  which 
provides a persistent record. Many reviewers voluntarily discussed many other papers as well. 
In  parallel,  I  read  most  reviews  and  every  review  of  every  paper  that  was  a  candidate  for 
terminating  at  phase  1  or  under  discussion.  For  each  review  that  did  not  provide  adequate 
justification  for  a  low  score  (e.g.,  a  subjective  claim  that  the  paper  was  incremental  without 
adequate citations of prior work), I used a HotCRP comment to request the reviewer to update 
their review or risk the paper moving to phase 2. I ensured all score changes were accompanied 
with justifications in the review text and not just an easy reaction to adjust to the majority.  I used 
the HotCRP color tags feature extensively to help reviewers and myself distinguish different paper 
categories and track online discussion results. 
After this process, all papers with at least one accept or undecided score proceeded to phase 2. A 
few papers with all reject scores also moved to phase 2 if the review text did not justify the reject 
v 
scores or had low confidence scores (when in doubt, I took the authors’ perspective and moved the 
paper to phase 2). 30% of the papers did not move to phase 2.  
Phase 2 Reviews  
The phase 1 discussions and reviewer suggestions were very helpful in assigning reviewers for 
phase 2. All phase 2 papers had at least 5 total reviews, with at least 3 from the PC. The maximum 
total number of reviews for a PC or ERC member was 19 or 7 respectively. 
Mid-phase  2,  I  used  a  tool  by  Andrew  Myers1  to  calibrate  excessive  negativity  or  exuberance 
among reviewers. I  sent  each  reviewer  their  estimated  bias  score  and  an  anonymized  list  of  all 
reviewers’ scores. A positive (negative) bias indicated a tendency to give scores that are higher 
(lower) than others who reviewed the same paper. I believe (hope) this calibration motivated some 
reflection as reviewers moved to the critical decision-making stages. 
The phase 2 reviews were due three days before the author response period. I wanted to ensure 
high quality reviews to facilitate responses focused on substantive issues. Since there was limited 
time, I distributed the quality assurance task among all reviewers. The last reviewer that submitted 
the  review  on  a  paper  was  asked  to  do  a  review  sufficiency  check  (RSC)  to  ensure  that  (1)  the 
reviews provided sufficient information to make an informed decision for the paper and (2) the 
reviews  provided  sufficient  feedback  to  the  authors.  If  the  paper  passed  the  RSC,  the  reviewer 
colored the paper purple (using HotCRP color tags); otherwise, they noted the problems.  
The RSC mechanism significantly enhanced my ability to find the problem papers and focus my 
attention on them. Many reviews were updated and we identified 18 papers for additional reviews. 
When the author response started, all reviews from phase 2 had already been received (I got good 
at  nagging).  All  post-phase2  (post-RSC)  reviews  had  been  requested  and  some  were  already 
received! The authors were notified that all pending reviews were late requests and they wouldn’t 
be penalized for not responding to them. At the end, we received a total of 902 reviews. 
Online Discussion After Author Response 
This was the most critical phase of the review process. The goal for the online discussion phase 
was  to  triage  papers  into  preliminary  accept  (tagged  green,  to  be  presented  quickly  at  the  PC 
meeting),  preliminary  reject  (tagged  red,  not  to  be  presented  at  the  PC  meeting),  and  discuss 
(tagged yellow, and the focus of most of the PC meeting).   
Each paper was assigned a lead from the PC or ERC to initiate discussion among its reviewers 
(tagging  the  paper  purple)  and  lead  it  towards  a  consensus  (unanimous  agreement)  for  green 
(accept) or red (reject). If significant discussion did not lead to a unanimous agreement, the paper 
was  marked  yellow  (discuss).  For  these,  the  discussion  was  expected  to  reconcile  as  many 