### Attacker's Strategies and Voting Mechanisms

The attacker can choose from three actions: (1) voting, in which case all of its votes are allocated to its own fork; (2) publishing blocks, where the transitions mimic those in a selfish mining setting; or (3) waiting and continuing to mine without taking any publicly visible actions. While the attacker is waiting, honest validators can vote for a checkpoint. In practice, validators do not vote simultaneously; this heterogeneity is modeled by staggering the honest votes according to a random process. In each time slot, with probability \( p_{\text{vote}} \), the honest parties randomly select the longest chain to vote for. The proportion of honest votes allocated to this longest chain is given by \( \min(\max(X, 0), 1 - \beta - v_h) \), where \( X \sim N(0.1, 0.05) \), \( v_h \) is the total proportion of votes already allocated by the honest parties, and \( \beta \) is the proportion of total votes controlled by the attacker. This random allocation reflects the non-uniformity of block propagation in the blockchain. With probability \( 1 - p_{\text{vote}} \), a block is generated by a miner, and the block structure changes according to the selfish mining setting.

The voting process is active at the beginning of each epoch and becomes inactive if (1) one checkpoint receives more than 2/3 of the votes, or (2) one chain containing a checkpoint becomes the canonical chain through selfish mining transitions, such as if the attacker chooses to adopt the honest chain.

### Reward Calculation

Block rewards are calculated as follows: miners receive one unit of reward for every block that ends up on the canonical chain. Voting rewards are calculated similarly to Casper FFG, with the validator deposit \( D_v \) scaled appropriately to reflect the ratio of real-world deposit magnitudes to block rewards. The differences between our modeling assumptions and the full scheme are detailed in Appendix D.

### Results

As the agent's voting and mining power increases, SquirRL learns to exploit the penalty for incorrect voting to penalize the honest party. A common attack strategy discovered by SquirRL involves the following steps:
1. The SquirRL agent accumulates and hides two checkpoints, \( c' \rightarrow c'' \), through selfish mining.
2. When the honest party releases a checkpoint \( c \), the agent immediately releases \( c' \) and triggers a competing voting process.
3. The agent waits until the honest checkpoint \( c \) accumulates close to (but not above) 1/3 of the votes.
4. The agent then releases \( c'' \), causing checkpoint \( c' \) to be included in a longer chain than \( c \).
5. The remaining voters will vote for \( c' \) according to the voting rule, and \( c' \) will be justified.
6. The honest voters for \( c \) are penalized, amplifying the agent’s relative reward.

Figure 8 shows the total relative rewards accumulated by an agent with the same fraction of mining hash power and voting pool deposit. We vary this fraction up to 1/3 because Casper FFG is not secure above 1/3 adversarial voting power. However, honest voting is shown to be a Nash equilibrium for agents with < 1/3 voting power [12]. This attack allows the agent to amplify its rewards more than selfish mining alone. Figure 9 illustrates dramatic gains in relative voting reward, up to 30% over honest rewards.

### Practical Concerns

This attack raises an important practical concern. The interest rate associated with voting rewards in Casper is close to extrinsic interest rates (e.g., the stock market). If an adversary can drive down an honest participant’s rewards, honest voters may leave the voting pool, making it easier for an adversary to control more than 1/3 of the voting power. This can affect the integrity of the finalization mechanism itself. An important implication is that system designers should consider how incentive mechanisms interact with other incentive mechanisms. Casper FFG in isolation is not vulnerable to these attacks, but combining mining incentives with the Casper FFG voting protocol reveals this vulnerability.

### Block Withholding Attacks

In a second case study, we explore block withholding attacks, where a mining pool infiltrates miners into opponent pools to diminish their revenue and gain a competitive advantage. The attacking pool deploys mining resources in a target pool and submits partial solutions (proofs of work) to earn rewards. If the attacking pool mines a block in the target pool, it withholds it, causing the target pool to lose block rewards and revenue relative to its hash power.

Eyal [18] showed that for two competing mining pools, there is a unique Nash equilibrium where each pool assigns a fraction of its resources to infiltrate and sabotage the other. SquirRL automatically learns pool strategies that converge to the same revenues as predicted by that equilibrium. We adopt the same model as in Eyal [18]. In the two-party version of this model, strategic mining pools \( P_1 \) and \( P_2 \) each possess loyal miners with hash rates \( m_1 \) and \( m_2 \), respectively, where \( 0 \leq m_1 + m_2 \leq 1 \). The remaining miners mine on their own, not forming or joining a pool. A miner loyal to pool \( P_i \) may either mine honestly in \( P_i \) or infiltrate \( P_{3-i} \), as dictated by \( P_i \).

When an infiltrating miner loyal to \( P_i \) generates a partial block reward, the reward is relayed to \( P_i \) and split among all registered miners in \( P_i \), including those loyal to \( P_i \) but currently infiltrating \( P_{3-i} \). The goal is to maximize the revenue of each miner, normalized by the revenue when there is no block withholding attack.

Denote the hash power of miners loyal to \( P_i \) and infiltrating \( P_{3-i} \) by \( x_i \). Thus \( 0 \leq x_i \leq m_i \). We set up the two-agent RL experiment using the reward functions defined in [18]. Each agent is assigned a mining hash power \( m_i \) and aims to maximize its reward by choosing \( x_i \) from a continuous action space \([0, m_i]\). The reward to be optimized is the immediate normalized revenue. There is no state transition in this environment: the game has an episode length of 1. Two agents take turns to adapt their strategies given the best strategy the other agent learned in the last episode. We trained the model using PPO because it is suitable for the multi-agent setting and supports continuous action spaces.

For reproducibility, our settings of hyperparameters in PPO and training results are specified as follows. We set the clipping parameter \( \varepsilon \) to 0.1, with a linear learning rate schedule decaying from \( 10^{-5} \) to \( 10^{-7} \). The entropy coefficient \( \beta \) is set to 0.01 initially and decays to \( \beta \leftarrow \beta (1 - \frac{\text{timestep}}{\text{schedule}}) \) every training step, with the schedule set to \( 10^9 \). After \( 10^6 \) episodes, both the strategies and rewards converge to those in the Nash equilibrium specified in [18], within 0.01. The detailed policies and revenues are plotted in Figures 10 and 11, respectively.

### Related Work

Recent works have analyzed direct attacks and economic flaws in cryptocurrency protocols, including selfish mining, difficulty attacks, and attacks involving miner manipulation of user transactions. Our work extends this analysis into a setting where multiple "attackers" compete to maximize their profit share, using deep reinforcement learning (DRL) to handle challenging settings such as multiple agents or continuous state spaces.

### Conclusion

In this work, we propose SquirRL, a deep RL-based framework to automate vulnerability detection in blockchain incentive mechanisms. SquirRL can approximate known theoretical results regarding attacks on blockchain incentive mechanisms and handle challenging settings. While SquirRL cannot prove the security of a mechanism, it serves as a "quick-and-dirty" tool for protocol designers to gain intuition in cases where theoretical analysis is infeasible. Future work may include new uses, such as other classes of incentive-based attacks.

### Acknowledgments

We thank Alistair Stewart, Fatemeh Shirazi, and Alfonso Cevallos for helpful conversations that inspired our experiments on composed mechanisms. We also thank Ren Zhang and our anonymous reviewers for their feedback. This work was partially supported by the Army Research Office, the National Science Foundation, the Initiative for Cryptocurrencies and Contracts (IC3), and the Ripple Foundation.

### References

[1] Aws now offers nvidia quadro virtual workstations for ec2 g4 instances at no additional cost. https://aws.amazon.com/about-aws/whats-new/2020/01/aws-now-offers-nvidia-quadro-virtual-workstations-for-ec2-g4-instances-at-no-additional-cost/, January 2020. Accessed on November 13, 2020.

[2] The best crypto mining pools 2020. https://miningpools.com/, November 2020. Accessed on November 12, 2020.

[3] Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift, 2019.

[4] Lear Bahack. Theoretical Bitcoin attacks with less than half of the computational power (draft). arXiv preprint arXiv:1312.7013, 2013.

[5] Qianlan Bai, Xinyan Zhou, Xing Wang, Yuedong Xu, Xin Wang, and Qingsheng Kong. A deep dive into blockchain selfish mining. arXiv preprint arXiv:1811.08263, 2018.

[6] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula, 2019.

[7] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębıak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.

[8] Joseph Bonneau. Why buy when you can rent? In FC, pages 19–26. Springer, 2016.

[9] Michael Bowling and Manuela Veloso. Scalable learning in stochastic games. In AAAI, pages 11–18, 2002.

[10] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI gym, 2016.

[11] Vitalik Buterin and Virgil Griffith. Casper the friendly finality gadget. arXiv preprint arXiv:1710.09437, 2017.

[12] Vitalik Buterin, Daniël Reijsbergen, Stefanos Leonardos, and Georgios Piliouras. Incentives in ethereum’s hybrid casper protocol. In ICBC, pages 236–244. IEEE, 2019.

[13] Miles Carlsten, Harry Kalodner, S Matthew Weinberg, and Arvind Narayanan. On the instability of Bitcoin without the block reward. In CCS, pages 154–167. ACM, 2016.

[14] Miguel Castro, Peter Druschel, Ayalvadi Ganesh, Antony Rowstron, and Dan S Wallach. Secure routing for structured peer-to-peer overlay networks. OSR, 36(SI):299–314, 2002.

[15] Philip Daian, Steven Goldfeder, Tyler Kell, Yunqi Li, Xueyuan Zhao, Iddo Bentov, Lorenz Breidenbach, and Ari Juels. Flash boys 2.0: Frontrunning, transaction reordering, and consensus instability in decentralized exchanges. arXiv preprint arXiv:1904.05234, 2019.

[16] Christian Decker and Roger Wattenhofer. Information propagation in the bitcoin network. In P2P, pages 1–10. IEEE, 2013.

[17] Shayan Eskandari, Seyedehmahsa Moosavi, and Jeremy Clark. Sok: Transparent dishonesty: front-running attacks on blockchain. 2019.

[18] Ittay Eyal. The miner’s dilemma. In S&P, pages 89–103. IEEE, 2015.

[19] Ittay Eyal and Emin Gün Sirer. Majority is not enough: Bitcoin mining is vulnerable. CACM, 61(7):95–102, 2018.

[20] Serge Fehr and Chen Yuan. Towards optimal robust secret sharing with security against a rushing adversary. Cryptology ePrint Archive, Report 2019/246, 2019. https://eprint.iacr.org/2019/246.