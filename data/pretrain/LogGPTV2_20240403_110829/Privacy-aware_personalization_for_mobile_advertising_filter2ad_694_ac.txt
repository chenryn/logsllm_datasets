the section we discuss extensions and alternatives.
The client can quickly compute the equation in Sec. 3.2.1, since
the number of ads from which the client picks one is small (≤ k).
However, the server’s task—to select a set of k ads from A that
maximize the expected revenue given only the generalized context
ˆc of the user—is much more demanding. In fact, a reduction from
the maximum coverage problem shows:
PROPOSITION 3.1. For a generalized context ˆc it is NP-hard to
select the revenue-maximizing set of k ads A∗ such that:
∗
A
= arg max
A⊂A:|A|=k
Pr[c|ˆc] · max
a∈A
pa · CTR(a|c)
(cid:88)
c:c→ˆc
Moreover, the maximum coverage problem cannot be approximated
within e
e−1 − o(1) assuming P (cid:54)= N P [14].
3.3.1 Approximation Algorithm
Algorithm 1 shows a greedy algorithm, called Greedy, that con-
structs a set A of k ads incrementally. It starts with A empty and in
each round, the ad that increases the expected revenue the most is
added to A.
Interestingly, the output of this simple greedy algorithm approx-
imates the optimal value to within a factor of (1 − 1/e). Although
the greedy algorithm is known to provide such a guarantee for
the maximum coverage problem [25], our problem is considerably
more complex: In the coverage problem a set either fully covers an
element or not. In our case an ad a can partially “cover” a context
c that can be generalized to ˆc. Thus a new analysis is required. We
ﬁrst deﬁne a beneﬁt function of adding a set A(cid:48) to a set A:
B(A, A
(cid:48)
) = E[Revenue(A ∪ A
(cid:48)|ˆc)] − E[Revenue(A|ˆc)].
The beneﬁt function has the nice property (proved in page 99 of
[18]):
FACT 3.2. The beneﬁt function is submodular, i.e., for all sets
of ads A1 ⊆ A2 and for all A, B(A1, A) ≥ B(A2, A).
However, due to the complex nature of our problem, the submodu-
larity property alone does not imply our approximation guarantee.
Let a1, . . . , ak be the k ads chosen by Greedy in the order they
were chosen. To simplify the analysis, we deﬁne the beneﬁt of the
ith ad to be bi and the expected revenue after adding the ﬁrst l ads to
k be the k optimal ads
in any ﬁxed order. We deﬁne the beneﬁt of the ith ad to be b∗
i and the
i=1 b∗
i .
be b(l) =(cid:80)l
expected revenue after adding the ﬁrst l ads to be b∗(l) =(cid:80)l
i=1 bi. Similarly, let a∗
1, . . . , a∗
LEMMA 3.3. ∀l ∈ [k]: bl ≥ b∗(k)−b(l−1)
1, . . . , a∗
PROOF. The beneﬁt of adding a∗
k
.
k to a1, . . . , al−1 is at
least b∗(k) − b(l − 1):
B({a1, . . . , al−1},{a
It is also equal to(cid:80)k
k}) ≥ b
∗
∗
1, . . . , a
∗
i=0 B({a1, . . . , al−1}∪{a∗
(k) − b(l − 1)
1, . . . , a∗
i }).
Thus, it follows from an averaging argument that ∃i, 1 ≤ i ≤ k :
B({a1, . . . , al−1} ∪ {a∗
. By
submodularity this implies that
∃i, 1 ≤ i ≤ k : B({a1, . . . , al−1},{a
i−1},{a∗
i }) ≥ b∗(k)−b(l−1)
i }) ≥ b∗(k) − b(l − 1)
i−1},{a∗
∗
1, . . . , a∗
k
.
k
Since the greedy algorithm in round l selected the ad al that max-
imizes B({a1, . . . , al−1},·), the beneﬁt of that ad, bl, has to be at
least b∗(k)−b(l−1)
which completes the proof.
k
We use this lemma to prove the following by induction.
LEMMA 3.4. ∀l ∈ [k]: b(l) ≥ (1 − (1 − 1/k)l)b∗(k).
PROOF. Proof by induction on l.
l = 1. Lemma 3.3 tells us that b1 ≥ b∗(k)
l → l + 1.
b(l + 1) = b(l) + bl+1 ≥ b(l) +
b∗(k) − b(l)
k = (1−(1−1/k)1)b∗(k).
k
− (1 − (1 − 1/k)l)b
b∗(k)
− b(l)(1 − 1/k) ≥ b∗(k)
k
=
= (1 − (1 − 1/k)l+1)b
The ﬁrst inequality follows from Lemma 3.3 and the second follows
from the induction hypothesis.
(k)
k
∗
∗
(k)(1 − 1/k)
The main theorem on the approximation guarantee follows.
THEOREM 3.5. The greedy algorithm approximates the opti-
mal value to within a factor of (1 − 1/e).
PROOF. By Lemma 3.4 we have that
b(k) ≥ (1 − (1 − 1/k)k)b
∗
(k) ≥ (1 − 1/e)b
∗
(k)
3.3.2 Extensions
Alternate Objective Function. So far, we tried to maximize rev-
enue under hard constraints on both the amount of information dis-
closure and the communication cost k. Instead, one might consider
the communication cost as a variable and include it in an objective
function that maximizes the value of (revenue −αk).
Consider an alternate objective function that maximizes the value
of (revenue −αk). A simple solution is to run Greedy for all val-
ues of k and pick the outcome that maximizes our new objective
function. However, by exploiting the submodularity of the ben-
eﬁt function, we can maximize the new objective function more
efﬁciently. All we have to do is to replace the while condition in
Algorithm 1 by a new one that checks whether the current value of
E[Revenue(A)] − α · |A| is increasing.
666This modiﬁcation works correctly because the following argu-
ment shows that as we increase k, our new objective function in-
creases until at some point it starts to decrease and never increases
again. Suppose in round k(cid:48) the expected revenue of A = {a1, . . . , ak(cid:48)}
minus α · k(cid:48) is not increasing any longer, i.e.,
Revenue({a1, . . . , ak(cid:48)}) − αk
(cid:48)
≤Revenue({a1, . . . , ak(cid:48)−1}) − α(k
(cid:48) − 1).
At this point the beneﬁt of adding ak(cid:48) is at most α. Due to sub-
modularity, the beneﬁt of any future ad being added to A can only
be smaller and thus will never lead to an increase of the objective
function.
Additional Constraints. We can incorporate a constraint on ad
relevance by setting the CTR to zero whenever it is below a certain
threshold. Then, no ad with CTR below this threshold will ever be
displayed at the client.
Advertisers’ Control. Our algorithm can incorporate additional
restrictions posed by advertisers on the contexts in which their ads
are being displayed. Very much like advertisers for sponsored re-
sults in Web search can bid on keywords in a query, our advertisers
can bid on contexts of users. To make sure the ad is only displayed
on these contexts, we can make the payment pa context-dependent
and set it to 0 for all but the contexts the advertiser bids on.
4. PRIVATE STATISTICS GATHERING
The optimization framework described in previous section uses
various statistics; in this section we describe how to obtain those
in a private way with the desiderata mentioned in Section 2.2.2.
The main mechanism we employ to build a scalable and robust
protocol is to use a server and a proxy: The server is responsible
for key distribution and the computation of the ﬁnal result while
the proxy is responsible for aggregation and anonymization. For
example, the ad network server can employ Verisign as the proxy.
The idea of using two servers to build secure protocols has been
used previously [2, 16, 22] in different applications; we use it here
for privacy-preserving aggregation.
In our setting, each user keeps a history of what ads she has
viewed/clicked that, for privacy reason, is stored on user’s local
device. The server then, with the help of the proxy, uses our proto-
col to compute statistics necessary for ad delivery: the probability
distribution over contexts, Pr[c], and the context-dependent click-
through rates, CTR(a|c). Both can be estimated by counting how
many users were in a speciﬁc context c and viewed / clicked on a
speciﬁc ad a. Hence we start with privacy-preserving computation
of count queries.
4.1 Assumptions and Privacy Preliminaries
We assume secure, reliable, and authenticated communication
channels between servers and users. In addition, we make the fol-
lowing two key assumptions, similar to those made in previous
works [8, 37, 40].
1. Honest-but-Curious Servers. Server and proxy honestly follow
the protocol. They are curious but do not collude with anyone.2
2. Honest Fraction of Users. At most a t fraction of users are
malicious or unavailable during the protocol. This means, at least
a fraction of 1 − t users honestly follow the protocol. The honest
users can be curious but they do not collude with anyone.
We aim to ensure user privacy with respect to all participants in
the distributed protocol. There are many different ways to deﬁne
2The assumption may be relaxed by using trusted hardware [8].
privacy in data publishing. We refer the reader to an excellent sur-
vey [7]. For the purpose of this paper, we work with -differential
privacy [13]. The idea behind this strong guarantee is that whether
or not the contexts and clicks of a single user were used in the
computation hardly affects the released outcome. Therefore, a user,
given the choice of whether or not to supply her data has hardly any
incentive to withhold it. The parameter  determines how much the
outcome may be affected.
In the absence of a trusted server, we need to generate noise re-
quired to ensure differential privacy in a distributed manner. In this
paper we adopt the probabilistic relaxation (, δ)-differential pri-
vacy [31], for which noise can be generated in a distributed way.
The parameter δ bounds the probability that a privacy breach (ac-
cording to -differential privacy) occurs. For δ = 0 this deﬁnition
is equivalent to -differential privacy. (, δ)-differential privacy of a
count query can be realized by adding Gaussian noise and Gaussian
noise with variance σ2 can be generated in a distributed manner by
N parties, by summing up N independent random samples from
the Gaussian distribution with variance σ2/N. More recently, Ács
et al. [1] have shown how to generate Laplace noise in fully dis-
tributed way, by constructing Laplace distribution as the sum over
i.i.d. samples from the Gamma distribution. Our protocol can eas-
ily adopt this technique and ensure -differential privacy as well.
Notation. Consider a user activity log L containing the data of a
set of users U. We can restrict the log L to the data of a subset of
the users U(cid:48) ⊂ U, denoted by LU(cid:48). If U(cid:48) contains users not in U,
we deﬁne LU(cid:48) to be LU∩U(cid:48). Consider a distributed protocol M in-
volving a set of participants P . Note that the set of users and the set
of participants can be overlapping. We deﬁne the view of a subset
of participants P (cid:48) ⊂ P in the execution of M on input L, denoted
by VP (cid:48), to be a random variable for all messages received and sent
by a participant in P (cid:48). For a non-participant we deﬁne the view
to be the output of the distributed protocol. The set of participants
can be partitioned into two sets: Pm of malicious participants and
Ph of honest but possibly unavailable participants. We have that
P = Ph ∪ Pm and Ph ∩ Pm = ∅.
DEFINITION 1. A distributed protocol M with participants P
satisﬁes (, δ)-distributed probabilistic differential privacy of the
users if for all user activity logs L and all (non-) participants p
the following holds. In case p is malicious let P (cid:48)
m denote the set
of malicious participant colluding with p which are a subset of the
m be {p}. There exist
malicious participants Pm. Otherwise let P (cid:48)
randomized algorithms M(cid:48) and R so that (a) M(cid:48)(LU\(Pm∪{p}))
preserves (, δ)-probabilistic differential privacy and (b) R gen-
given M(cid:48)(LU\(Pm∪{p}))
erates the distribution of the view VP (cid:48)