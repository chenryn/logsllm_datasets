### Threshold and Controversial Range in Layered Sybil Detection

We define the threshold \( T \) and the controversial range \( R \) to determine how profiles are forwarded from the lower to the upper layer. To calculate the minimum number of votes \( L \) and \( U \) required in each layer, we use the same methodology as illustrated in Figure 9. Turkers are divided into upper and lower layers based on a given threshold \( T \) within the range [70%, 90%]. We incrementally increase the number of votes per profile in each layer until the false positive rate is less than 1%. The false positive rates for each layer are independent, simplifying the calculations. The controversial range \( R \) only affects the false negative rate and is excluded from these calculations.

Table 5 shows the minimum number of votes per profile needed in the upper and lower layers as \( T \) varies. These values are used in the remainder of our analysis.

### Simulation Results

Figure 13 illustrates the average number of votes per profile in our simulations. Three lines represent two-layer simulations with different \( R \) values. For instance, \( R = [0.2, 0.9] \) means that if between 20% and 90% of the turkers classify the profile as a Sybil, the profile is considered controversial. Although we simulated various \( R \) ranges, only three representative ranges are shown for clarity. The number of votes for the one-layer scheme is also included.

The results in Figure 13 show that the number of votes needed in the various two-layer scenarios is relatively stable. As \( R \) varies, the number of profiles evaluated by both layers changes, causing minor fluctuations in the average votes per profile. However, the average is always ≤ \( L + U \) from Table 5. Overall, these fluctuations are minor, with the average votes changing by approximately 1.

### False Negatives

Based on the results in Figure 13, the one-layer scheme appears to be the most cost-effective, requiring the fewest votes per profile. However, this comes at the cost of a higher false negative rate.

Figure 14 shows the false negative rates for our simulations. The two-layer scheme performs better: for certain values of \( R \) and thresholds ≥80%, two-layers can achieve false negative rates <10%. The parameters that yield the lowest false negatives (0.7%) and the fewest average votes per profile (6) are \( R = [0.2, 0.5] \) and \( T = 90\% \). These parameters are used in the remainder of our analysis.

Figures 13 and 14 demonstrate the effectiveness of our crowdsourced Sybil detection system. Using an average of 6 votes per profile, the system achieves false positive and negative rates both below 1%.

### Reducing False Positives

In some cases, a social network may require a false positive rate significantly lower than 1%. To evaluate the impact on costs, we re-ran all our simulations with a target false positive rate of <0.1%. Figure 15 plots the number of votes per profile versus false negatives as the target false positive rate is varied. Each point represents a different combination of \( R \) and \( T \) values. The conclusion is straightforward: to achieve <0.1% false positives, two additional votes per turker are needed. This tradeoff is reasonable, increasing costs by 33% but reducing false positives by an order of magnitude.

### Parameterization

Our system parameters were optimized using actual user test results, which may not be ideal for every system or user population. The key takeaway is that the system can be calibrated to provide high accuracy and scalability for a given user population. While we do not have sufficient data to determine how often systems need recalibration, it is likely that deployed systems will periodically adjust parameters such as \( V \) and \( T \) for continued accuracy.

### Costs of a Turker Workforce

Using the parameters derived, we can estimate the number of turkers needed to deploy our system. For Renren, each profile requires 6 votes on average, and turkers can evaluate one profile every 20 seconds. A turker working a standard 8-hour day can examine 1440 profiles. Data from a real OSN indicates that the number of turkers needed is reasonable. According to [3], Tuenti, a Spanish online social network, has 11 million users and averages 12,000 user reports per day. Our system would require 50 full-time turkers to handle this load. Scaling up by a factor of 10, a large OSN like Facebook would require 500 turkers. Recruiting this many turkers is feasible, as our experience shows (Table 1).

Finally, we estimate the monetary cost. Facebook pays turkers $1 per hour to moderate images [10]. Assuming the same cost for our system, the daily cost for deployment on Tuenti (12,000 reports per day) would be $400. This compares favorably with Tuenti’s existing practices, where 14 full-time employees are paid approximately $20 per hour, resulting in a daily moderation cost of $2240.

### Privacy

Protecting user privacy is a challenge in crowdsourced Sybil detection. One possible solution is to show turkers only the public portions of users' profiles. However, this approach is problematic as Sybils could hinder detection by setting their profiles to private. A better solution is to leverage the OSN's existing "report" filter. When Alice reports Bob’s profile as malicious, the turker sees Bob’s profile as it appears to Alice, providing the same information Alice used to make her determination. This prevents abuse of the report system to leak information about random strangers.

### Related Work

Research on crowdsourcing platforms, such as Amazon's Mechanical Turk, has explored worker demographics, task pricing, and the pros and cons of using MTurk for user studies. Studies have also addressed maximizing accuracy from unreliable turkers, using methods like majority voting, pre-screening, and tournament algorithms. Our study proposes using crowdsourcing to solve a challenging OSN security problem, while other studies have shown how crowdsourcing can be used for malicious purposes, including social spam, SEO, fake reviews, and malware installation.

### Conclusion and Open Questions

Sybil accounts pose a significant threat to the stability and security of online social networks. Current solutions rely on ad hoc methods like manual inspection. Our user study takes the first step towards developing a scalable and accurate crowdsourced Sybil detection system. Simulations show that a hierarchical two-tiered system can be both accurate and cost-effective.

**Ground-Truth:** Our evaluation is constrained by the ground-truth Sybils used in our study, potentially missing additional Sybils. Thus, our results represent a lower bound on detection accuracy.

**Deployment:** Effective deployment remains an open question. We envision integrating our system with existing techniques like content filtering and statistical models. Social networks can further lower costs by using their own users as crowdworkers, replacing monetary payments with in-system virtual currency.

**Countermeasures:** An effective solution must account for potential countermeasures by attackers. Ground-truth profiles must be randomly mixed with test profiles and refreshed periodically to avoid detection. Dealing with "undercover" attackers remains an open question.

### References

[References listed as in the original text]

This revised version aims to enhance clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.