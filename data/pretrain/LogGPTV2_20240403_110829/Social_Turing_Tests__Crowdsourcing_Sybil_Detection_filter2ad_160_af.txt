T , and the controversial range R in which proﬁles are for-
warded from the lower to upper layer. To calculate L and U
we use the same methodology as in Figure 9. Turkers are
divided into upper and lower layers for a given threshold
T ∈ [70%, 90%], then we incrementally increase the votes
per proﬁle in each layer until the false positive rate is <1%.
The false positive rate of each layer is independent (i.e. the
number of votes in the lower layer does not impact votes
in the upper layer), which simpliﬁes the calculations. The
controversial range only effects the false negative rate, and
is ignored from these calculations.
Table 5 shows the minimum number of votes per proﬁle
needed in the upper and lower layers as T is varied. We use
these values in the remainder of our analysis.
Figure 13 shows the average votes per proﬁle in our sim-
ulations. Three of the lines represent two-layer simulations
with different R values. For example, R = [0.2, 0.9] means
that if between 20% and 90% of the turkers classify the pro-
ﬁle as a Sybil, then the proﬁle is considered controversial.
Although we simulated many R ranges, only three repre-
e
l
i
f
o
r
P
r
e
P
s
e
t
o
V
.
g
v
A
 10
 8
 6
 4
 2
 0
 70
[0.2, 0.9]
[0.2, 0.5]
[0.5, 0.7]
One-Layer
 75
 80
 85
 90
Threshold (%)
)
%
(
e
t
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
 100
 80
 60
 40
 20
 0
 70
[0.5, 0.7]
One-Layer
[0.2, 0.9]
[0.2, 0.5]
 75
 80
 85
 90
Threshold (%)
)
%
(
e
t
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
 35
 30
 25
 20
 15
 10
 5
 0
1%
0.1%
 0  1  2  3  4  5  6  7  8  9  10
Avg. Votes Per Profile
Figure 13. Threshold versus
average votes per proﬁles.
Figure 14. Threshold versus
false negative rate.
Figure 15. Tradeoff between
votes per proﬁle and desired
accuracy.
sentative ranges are shown for clarity. The number of votes
for the one-layer scheme is also shown.
The results in Figure 13 show that the number of votes
needed in the various two-layer scenarios are relatively sta-
ble. As R varies, the number of proﬁles that must be evalu-
ated by both layers changes. Thus, average votes per proﬁle
ﬂuctuates, although the average is always ≤ L + U from
Table 5. Overall, these ﬂuctuations are minor, with average
votes only changing by ≈1.
False Negatives.
Judging by the results in Figure 13,
the one-layer scheme appears best because it requires the
fewest votes per proﬁle (and is thus less costly). However,
there is a signiﬁcant tradeoff for lowering the cost of the
system: more false negatives.
Figure 14 shows the false negative rates for our simu-
lations. The results for the two-layer scheme are superior:
for certain values of R and thresholds ≥80%, two-layers
can achieve false negative rates <10%. The parameters that
yield the lowest false negatives (0.7%) and the fewest aver-
age votes per proﬁle (6) are R = [0.2, 0.5] and T = 90%.
We use these parameters for the remainder of our analysis.
The results in Figures 13 and 14 capture the power of
our crowdsourced Sybil detection system. Using only an
average of 6 votes per proﬁle, the system products results
with false positive and negative rates both below 1%.
Reducing False Positives.
In some situations, a so-
cial network may want to achieve a false positive rate sig-
niﬁcantly lower than 1%. In order to evaluate how much
this change would affect costs, we re-ran all our simulations
with the target false positive rate set to <0.1%. Figure 15
plots the number of votes per proﬁle versus false negatives
as the target false positive rate is varied. Each point in the
scatter is a different combination of R and T values. The
conclusion from Figure 15 is straightforward: to get <0.1%
false positives, you need two additional votes per turker.
This tradeoff is fairly reasonable: costs increase 33%, but
false positives reduce by an order of magnitude.
Parameterization.
Since our system parameters were
optimized using actual user test results, they may not be
ideal for every system or user population. The key takeaway
is that given a user population, the system can be calibrated
to provide high accuracy and scalability. We do not have
sufﬁcient data to make conjectures about how often or when
systems require re-calibration, but it is likely that a deployed
system might periodically recalibrate parameters such as V
and T for continued accuracy.
6.3 The Costs of a Turker Workforce
Using the parameters derived in the previous section, we
can estimate how many turkers would be needed to deploy
our system. Using the parameters for Renren, each pro-
ﬁle requires 6 votes on average, and turkers can evaluate
one proﬁle every 20 seconds (see Figure 8). Thus, a turker
working a standard 8-hour day (or several turkers working
an equivalent amount of time) can examine 1440 proﬁles.
Data from a real OSN indicates that the number of turk-
ers needed for our system is reasonable. According to [3],
Tuenti, a Spanish online social network, has a user-base of
11 million and averages 12,000 user reports per day. Our
system would require 50 full-time turkers to handle this
load. If we scale the population size and reports per day up
by a factor of 10, we can estimate the load for a large OSN
like Facebook. In this case, our system requires 500 turkers.
Our own experience showed that recruiting this many turk-
ers is not difﬁcult (Table 1). In fact, following our crowd-
sourcing experiments on this and other projects [30], we
received numerous messages from crowd requesting more
tasks to perform.
Finally, we estimate the monetary cost of our system.
Facebook pays turkers from oDesk $1 per hour to mod-
erate images [10].
If we assume the same cost per hour
per turker for our system, then the daily cost for deploy-
ment on Tuenti (i.e. 12,000 reports per day) would only be
$400. This compares favorably with Tuenti’s existing prac-
tices: Tuenti pays 14 full-time employees to moderate con-
tent [3]. The estimated annual salary for Tuenti employees
are roughly e30,0003, which is about $20 per hour. So the
Tuenti’s moderation cost is $2240 per day, which is signif-
icantly more than the estimated costs of our turker work-
force.
6.4 Privacy
Protecting user privacy is a challenge for crowdsourced
Sybil detection. How do you let turkers evaluate user pro-
ﬁles without violating the privacy of those users? This issue
does not impact our experiments, since all proﬁles are from
public accounts. However, in a real deployment, the system
needs to handle users with strict privacy settings.
One possible solution is to only show turkers the public
portions of users’ proﬁles. However, this approach is prob-
lematic because Sybils could hinder the detection system by
setting their proﬁles to private. Setting the proﬁle to private
may make it more difﬁcult for Sybils to friend other users,
but it also cripples the discriminatory abilities of turkers.
A better solution to the privacy problem is to leverage the
OSNs existing “report” ﬁlter. Suppose Alice reports Bob’s
proﬁle as malicious. The turker would be shown Bob’s pro-
ﬁle as it appears to Alice. Intuitively, this gives the turker
access to the same information that Alice used to make her
determination. If Alice and Bob are friends, then the turker
would also be able to access friend-only information. On
the other hand, if Alice and Bob are strangers, then the
turker would only have access to Bob’s public information.
This scheme prevents users from abusing the report system
to leak the information of random strangers.
7 Related Work
The success of crowdsourcing platforms on the web has
generated a great deal of interest from researchers. Sev-
eral studies have measured aspects of Amazon’s Mechani-
cal Turk, including worker demographics [12, 24] and task
pricing [5, 11, 19]. There are studies that explore the pros
and cons to use MTurk for user study [16].
Many studies address the problem of how to maximize
accuracy from inherently unreliable turkers. The most com-
mon approach is to use majority voting [17, 25], although
this scheme is vulnerable to collusion attacks by malicious
turkers [26]. Another approach is to pre-screen turkers with
a questionnaire to ﬁlter out less reliable workers [22]. Fi-
nally, [26] proposes using a tournament algorithm to deter-
mine the correct answer for difﬁcult tasks.
In this study, we propose using crowdsourcing to solve
a challenging OSN security problem. However, many stud-
ies have demonstrated how crowdsourcing can be used by
attackers for malicious ends. Studies have observed ma-
licious HITs asking turkers to send social spam [30], per-
3http://www.glassdoor.com/Salary/
Tuenti-Salaries-E245751.htm
form search engine optimization (SEO) [21], write fake re-
views [23], and even install malware on their systems [15].
8 Conclusion and Open Questions
Sybil accounts challenge the stability and security of to-
day’s online social networks. Despite signiﬁcant efforts
from researchers and industry, malicious users are creat-
ing increasingly realistic Sybil accounts that blend into the
legitimate user population. To address the problem today,
social networks rely on ad hoc solutions such as manual in-
spection by employees.
Our user study takes the ﬁrst step towards the develop-
ment of a scalable and accurate crowdsourced Sybil detec-
tion system. Our results show that by using experts to cal-
ibrate ground truth ﬁlters, we can eliminate low accuracy
turkers, and also separate the most accurate turkers from
the crowd. Simulations show that a hierarchical two-tiered
system can both be accurate and scalable in terms of total
costs.
Ground-truth.
Our system evaluation is constrained by
the ground-truth Sybils used in our user study, i.e. it is possi-
ble that there are additional Sybils that were not caught and
included in our data. Thus, our results are a lower bound
on detection accuracy. Sybils that can bypass Facebook or
Renren’s existing detection mechanisms could potentially
be caught by our system.
Deployment.
Effective deployment of crowdsourced
Sybil detection mechanisms remains an open question. We
envision that the crowdsourcing system will be used to com-
plement existing techniques such as content-ﬁltering and
statistical models. For example, output from accurate turk-
ers can teach automated tools which ﬁelds of the data can
most easily identify fake accounts. Social networks can fur-
ther lower the costs of running this system by utilizing their
own users as crowdworkers. The social network can replace
monetary payments with in-system virtual currency, e.g.
Facebook Credits, Zynga Cash, or Renren Beans. We are
currently discussing internal testing and deployment possi-
bilities with collaborators at Renren and LinkedIn.
Countermeasures.
An effective solution must take into
account possible countermeasures by attackers. For exam-
ple, ground-truth proﬁles must be randomly mixed with test
proﬁles in order to detect malicious turkers that attempt to
poison the system by submitting intentionally inaccurate re-
sults. The ground-truth proﬁles must be refreshed periodi-
cally to avoid detection. In addition, it is possible for attack-
ers to inﬁltrate the system in order to learn how to improve
fake proﬁles to avoid detection. Dealing with these “under-
cover” attackers remains an open question.
References
[1] Thousands of fake proﬁles on facebook identiﬁed. Weekly-
Blitz.net, June 2011.
[2] Y. Boshmaf et al. The socialbot network: When bots social-
ize for fame and money. In Proc. of ACSAC, 2011.
[3] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro. Aiding
the detection of fake accounts in large scale social online
services. In Proc. of NSDI, 2012.
[4] G. Danezis and P. Mittal. Sybilinfer: Detecting sybil nodes
using social networks. In Proc. of NDSS, 2009.
[5] S. Faridani, B. Hartmann, and P. G. Ipeirotis. What’s the
right price? pricing tasks for ﬁnishing on time. In Proc. of
AAAI Workshop on Human Computation, 2011.
[6] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Y. Zhao.
In
Detecting and characterizing social spam campaigns.
Proc. of IMC, 2010.
[7] A. Ghosh, S. Kale, and P. McAfee. Who moderates the mod-
erators? crowdsourcing abuse detection in user-generated
content. In Proc. of EC, 2011.
[8] D. Goldman.
Money, 2012.
Facebook tops 900 million users. CNN
[9] J. Heer and M. Bostock. Crowdsourcing graphical percep-
tion: using mechanical turk to assess visualization design.
In Proc. of CHI, 2010.
[10] I. Hollingshead and E. Barnett. The dark side of facebook.
Telegraph, March 2012.
[11] P. G. Ipeirotis. Analyzing the amazon mechanical turk mar-
ketplace. XRDS, 17:16–21, Dec. 2010.
[12] P. G. Ipeirotis. Demographics of mechanical turk. NYU
Working Paper, 2010.
[13] D. Irani, M. Balduzzi, D. Balzarotti, E. Kirda, and C. Pu.
Reverse social engineering attacks in online social networks.
In Proc. of DIMVA, 2011.
[14] J. Jiang et al. Understanding latent interactions in online
social networks. In Proc. of IMC, 2010.
[15] C. Kanich, S. Checkoway, and K. Mowery. Putting out a hit:
Crowdsourcing malware installs. In Proc. of WOOT, 2011.
[16] A. Kittur, H. Chi, and B. Suh. Crowdsourcing user studies
with mechanical turk. In Proc. of CHI, 2008.
[17] J. Le, A. Edmonds, V. Hester, and L. Biewald. Ensuring
In
quality in crowdsourced search relevance evaluation.
Workshop on Crowdsourcing for Search Evaluation, 2010.
[18] G. Liu, Y. Wang, and M. Orgun. Trust transitivity in complex
social networks. In Proc. of AAAI Conference on Artiﬁcial
Intelligence, 2011.
[19] W. Mason and D. J. Watts. Financial Incentives and the “Per-
formance of Crowds”. In Proc. of KDD-HCOMP, 2009.
[20] A. Mohaisen, A. Yun, and Y. Kim. Measuring the Mixing
Time of Social Graphs. In Proc. of IMC, 2010.
[21] M. Motoyama et al. Dirty jobs: The role of freelance labor
in web service abuse. In Proc. of Usenix Security, 2011.
[22] D. Oleson et al. Programmatic gold: Targeted and scal-
able quality assurance in crowdsourcing. In Proc. of Human
Computation, 2011.
[23] M. Ott, Y. Choi, C. Cardie, and J. T. Hancock. Finding de-
ceptive opinion spam by any stretch of the imagination. In
Proc. of ACL, 2011.
[24] J. Ross et al. Who are the crowdworkers?: Shifting demo-
graphics in amazon mechanical turk. In Proc. of CHI, 2010.
[25] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap
and fast—but is it good?: evaluating non-expert annotations
for natural language tasks. In Proc. of EMNLP, 2008.
[26] Y.-A. Sun, S. Roy, and G. Little. Beyond independent agree-
ment: A tournament selection approach for quality assur-
ance of human computation tasks. In Proc. of Human Com-
putation, 2011.
[27] K. Thomas, C. Grier, V. Paxson, and D. Song. Suspended
accounts in retrospect: An analysis of twitter spam. In Proc.
of IMC, 2011.
[28] N. Tran, B. Min, J. Li, and L. Subramanian. Sybil-resilient
online content voting. In Proc. of NSDI, 2009.
[29] B. Viswanath, A. Post, K. P. Gummadi, and A. Mislove. An
analysis of social network-based sybil defenses. In Proc. of
SIGCOMM, 2010.
[30] G. Wang, C. Wilson, X. Zhao, Y. Zhu, M. Mohanlal,
H. Zheng, and B. Y. Zhao. Serf and turf: Crowdturﬁng for
fun and proﬁt. In Proc. of WWW, 2012.
[31] Z. Yang, C. Wilson, X. Wang, T. Gao, B. Y. Zhao, and
In
Y. Dai. Uncovering social network sybils in the wild.
Proc. of IMC, 2011.
[32] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao. Sybillimit:
A near-optimal social network defense against sybil attacks.
In Proc. of IEEE S&P, 2008.
[33] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman. Sybil-
guard: defending against sybil attacks via social networks.
In Proc. of SIGCOMM, 2006.