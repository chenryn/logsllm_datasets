where vwt , vwk and vwi are the vector representations
of wt, wk and wi. To further improve the efﬁciency of the
4
computation, Word2Vec adopts the hierarchical softmax as a
computationally efﬁcient approximation [42].
To train the token embedding generation model, we modify
the Word2Vec CBOW model which uses words around a target
word as context. In our case, we consider each token (opcode
or operand) as word, normalized random walks on top of
ICFGs to be sentences and instructions around each token as
its context. For example, step 3 in Figure 2 shows that the
current token is cmp (shown in red), so we use one instruction
before and another instruction after (shown in green) in the
random walk as the context. If the target instruction is at the
block boundary (e.g., ﬁrst instruction in the block), then only
one adjacent instruction will be considered as its context.
Feature Vector Generation. Feature vectors for basic blocks
are then generated based on the token embeddings. Since
each basic block could contain multiple instructions and each
instruction in turn involves one opcode and potentially multiple
operands, we calculate the average of the operand embeddings,
concatenate with the opcode embedding to generate instruction
embedding, and further sum up the instructions within the
block to formulate the block feature vector.
Additionally, because of different compilers and optimiza-
tions, instructions may not be of equal importance in terms
of difﬁng. For instance, GCC v5.4 compiler uses 3 mov
instructions to set up a printf call under O0 optimization but
uses only 1 mov instruction with O3 optimization. In this
case, mov instruction is less important than call instruction
during matching. To tackle this problem, DEEPBINDIFF adopts
a weighting strategy to adjust the weights of opcodes based
on the opcodes importance with TF-IDF model [50]. The
calculated weight indicates how important one instruction is
to the block that contains it in all the blocks within two input
binaries.
Particularly, for an instruction ini containing an opcode pi
and a set of k (could be zero) operands Setti, we model the
instruction embedding as the concatenation of two terms: 1)
opcode embedding embedpi multiplies by its TF-IDF weight
weightpi; 2) the average of operand embeddings embedtin .
Therefore, for a block b = {in1, in2, .., inj} containing j
instructions, its feature vector F Vb is the sum of its instruction
binary 1binary 2InputCFG generationtoken embeddingmodeltoken embeddingsfeature vectorsinter-procedural CFGgraph mergingPre-processingTADWalgorithmbasic blockembeddingsinitial matchingk-hop greedymatchingdiffing resultsEmbedding GenerationCode DiffingOutputembeddings, as depicted in Equation 3.
B. Graph Merging
j(cid:88)
(embedpi ∗weightpi||
i=1
F Vb =
|Setti| ∗ k(cid:88)
1
n=1
embedtin
) (3)
Our token embedding generation model shares some simi-
larity with Asm2Vec [23], which also uses instructions around
a target token as context. Nonetheless, our model has a fun-
damentally different design goal. DEEPBINDIFF learns token
embeddings via program-wide random walks while Asm2Vec
is trying to learn function and token embeddings at the same
time and only within the function. Therefore, we choose to
modify Word2Vec CBOW model while Asm2Vec leverages
the PV-DM model.
V. EMBEDDING GENERATION
Based on the ICFGs and feature vectors generated in the
prior steps, basic block embeddings are generated so that sim-
ilar basic blocks can be associated with similar embeddings.
To do so, DEEPBINDIFF ﬁrst merges the two ICFGs into one
graph and then models the problem as a network representation
learning problem to generate basic block embeddings using
Text-associated DeepWalk algorithm (TADW) [56].
Since the most important building basic block for this
is the TADW algorithm, we ﬁrst describe the
component
algorithm in detail and present how basic block embeddings
are generated. Then, we justify the need of graph merging and
report how DEEPBINDIFF accomplishes it.
A. TADW algorithm
Text-associated DeepWalk [56] is an unsupervised graph
embedding learning technique. As the name suggests, it is an
improvement over the DeepWalk algorithm [48].
DeepWalk algorithm is an online graph embedding learning
algorithm that considers a set of short truncated random walks
as a corpus in language modeling problem, and the graph
vertices as its own vocabulary. The embeddings are then
learned using random walks on the vertices in the graph.
Accordingly, vertices that share similar neighbours will have
similar embeddings. It excels at learning the contextual infor-
mation from a graph. Nevertheless, it does not consider the
node features during analysis.
As a result, Yang at el. [56] propose an improved algorithm
called Text-associated DeepWalk (TADW), which is able to
incorporate features of vertices into the network representation
learning process. They prove that DeepWalk is equivalent to
factorizing a matrix M ∈ R|v|×|v| where each entry Mij is
logarithm of the average probability that vertex vi randomly
walks to vertex vj in ﬁxed steps. This discovery further leads
to TADW algorithm depicted in Figure 3. It shows that it is
possible to factorize the matrix M into the product of three
matrices: W ∈ Rk×|v|, H ∈ Rk×f and a text feature T ∈
Rf×|v|. Then, W is concatenated with HT to produce 2k-
dimensional representations of vertices (embeddings).
5
Since we have two ICFGs (one for each binary), the most
intuitive way is to run TADW twice for the two graphs.
However, this method has two drawbacks. First, it is less efﬁ-
cient to perform matrix factorization twice. Second, generating
embeddings separately can miss some important indicators for
similarity detection.
For example, Figure 4 shows two ICFGs and each has a
basic block that calls fread and another basic block that has
a reference to string ‘hello’. Ideally, these two pairs of nodes
(‘a’ and ‘1’, ‘d’ and ‘3’) are likely to match. However, in
practice, the feature vectors of these basic blocks may not
look very similar as one basic block could contain multiple
instructions while the call or the string reference is just one
of them. Besides, the two pairs also have different contextual
information (node ‘a’ has no incoming edge but ‘1’ does). As
a result, TADW may not generate similar embeddings for the
two pairs of nodes.
We propose graph merging to alleviate this problem. That
is, the two ICFGs are merged and TADW runs only once on the
merged graph. Particularly, DEEPBINDIFF extracts the string
references and detects external library calls and system calls.
Then, it creates virtual nodes for strings and library functions,
and draws edges from the callsites to these virtual nodes.
Hence, two graphs are merged into one on terminal virtual
nodes. By doing so, node ‘a’ and ‘1’ have at least one common
neighbor, which boosts the similarity between them. Further,
neighbors of node ‘a’ and ‘1’ also have higher similarity since
they share similar neighbors. Moreover, since we only merge
the graphs on terminal nodes, the original graph structures stay
unchanged.
C. Basic Block Embeddings
With the merged graph, DEEPBINDIFF leverages TADW
algorithm to generate basic block embeddings. More specif-
ically, DEEPBINDIFF feeds the merged graph and the basic
block feature vectors into TADW for multiple iterations of
optimization. The algorithm factorizes the matrix M into three
matrices by minimizing the loss function depicted in Equa-
tion 4 using Alternating Least Squares (ALS) algorithm [34].
It stops when the loss converges or after a ﬁxed n iterations.
||M − W T HT||2
F +
min
W,H
λ2
2
(||W||2
F + ||H||2
F )
(4)
On that account, each generated basic block embedding
contains not only the semantic information about the basic
block itself, but also the information from the ICFG structure.
VI. CODE DIFFING
DEEPBINDIFF then performs code difﬁng. The goal is to
ﬁnd a basic block level matching solution that maximizes
the similarity for the two input binaries. One spontaneous
choice is to perform linear assignment based on basic block
embeddings to produce an optimal matching. This method
2λ is a harmonic factor to balance two components
Fig. 2: Basic Block Feature Vector Generation.
graph merging. Starting from the virtual nodes, the algorithm
calls ComputeInitialSet() in Ln.2 to extract direct neighbors
of the virtual nodes and ﬁnds best matching pairs among the
neighbors based on embeddings. For example, node ‘a’ and
‘1’ in Figure 4 will become one of the pairs in initial set.
Fig. 3: TADW
Fig. 4: Graph Merging
suffers from two major limitations. First, linear assignment can
be inefﬁcient as binaries could contain enormous amount of
blocks. Second, although embeddings include some contextual
information, linear assignment itself does not consider any
graph information. Thus, it is still likely to make mistakes
when matching very similar basic blocks. A possible improve-
ment is to conduct linear assignment at two levels. Rather
than matching basic blocks directly, we could match functions
ﬁrst by generating function level embeddings. Then, basic
blocks within the matched functions can be further matched
using basic block embeddings. This approach, however, can
be severely thwarted by compiler optimizations that alter the
function boundary such as function inlining.
A. k-Hop Greedy Matching
To address this problem, we introduce a k-hop greedy
matching algorithm. The high-level idea is to beneﬁt from the
ICFG contextual information and ﬁnd matching basic blocks
based on the similarity calculated from basic block embeddings
within the k-hop neighbors of already matched ones.
As presented in Algorithm 1,
the initial matching set
Setinitial are computed by using the virtual nodes during the
Starting from there,
the neighbors of
algorithm loops
the
the already matched pairs
and ex-
plores
in
GetKHopN eighbors() in Ln.7-8 by exploring the merged
then sorts the similarities between neighbor ba-
ICFG. It
sic blocks and picks the pair
that bears highest simi-
larity with a predeﬁned threshold t of 0.6 by calling
F indM axU nmatched() in Ln.9. This process is repeated
until all k-hop neighbors of matched pairs are explored and
matched. Note that after the loop, there may still exist un-
matched basic blocks due to unreachable code (dead code)
or low similarity within k-hop neighbors. Our method then
performs linear assignment using Hungarian algorithm [35]
and ﬁnds the optimal matching among them in Ln.16. Please
note that we only use the Hungarian algorithm occasionally for
small numbers of unmatched basic blocks, hence, its impact
on accuracy and efﬁciency is very minimal. Finally, it returns
Setmatched as the matching result, Seti as insertion basic
blocks and Setd as deletion basic blocks. 3 We set k to 4. More
details about parameter selection is presented in Section VII.
VII. EVALUATION
In this section, we evaluate DEEPBINDIFF with respect
to its effectiveness and efﬁciency for two different difﬁng sce-
narios: cross-version and cross-optimization-level. To our best
knowledge, this is the ﬁrst research work that comprehensively
examines the effectiveness of program-wide binary difﬁng
tools under the cross-version setting. Furthermore, we conduct
a case study to demonstrate the usefulness of DEEPBINDIFF
in real-world vulnerability analysis.
A. Experimental Setup, Datasets & Baseline Techniques
Our experiments are performed on a moderate desktop
computer running Ubuntu 18.04LTS operating system with
3Insertions and deletions can happen when difﬁng between two different
versions of the binary.
6
Inputmovzxecx, byte ptr [rdx]movr8d, eaxja0x408963movrax, rdxaddrax, 1movzxesi, byte ptr [rax]leaedi, dword ptr [rsi - 0x30]cmpdil, 9jbe0x407fa6movr14, -1cmpsil, 0x24jne0x408033cmpr8b, 9ja0x4088e7...movzxreg4, ptrmovreg4, reg4jaimmovreg8, reg8addreg8, immovzxreg4, ptrleareg4, ptrcmpreg1, imjbeimmovreg8, imcmpreg1, imjneimcmpreg1, imjaim...Step 1Random WalksStep 2NormalizationSoftmax ClassifierHidden Layerleareg4ptrcmpreg1imjbeimStep 3Model Trainingtoken embeddingsfeature vectorsStep 4Feature VectorGeneration......ContextContextCurrent InstructionTargetM|V||V|WHkfTT|V|tcall freadcall freadfreadcall freadcall freadref: ‘hello’ref: ‘hello’ref: ‘hello’ref: ‘hello’‘hello’abcd321abcd123(node1, node2) ← SetcurrP airs.pop()
nbnode1 ← GetKHopNeighbors(node1)
nbnode2 ← GetKHopNeighbors(node2)
newP air ← FindMaxUnmatched(nbnode1, nbnode2)
if newP air != null then
Algorithm 1 k-Hop Greedy Matching Algorithm
1: Setvirtualnodes ← {virtual nodes from merged graphs}
2: Setinitial ← ComputeInitialSet(Setvirtualnodes)
3: Setmatched ← Setinitial; SetcurrP airs ← Setinitial
4:
5: while SetcurrP airs != empty do
6:
7:
8:
9:
10:
11:
12:
end if
13:
14: end while
15: Setunreached ← {basic blocks that are not yet matched}
16: {Setm, Seti, Setd} ← LinearAssign(Setunreached)
17: Setmatched ← Setmatched ∪ Setm
output Setmatched, Seti, Setd as the difﬁng result
Setmatched ← Setmatched ∪ newP air
SetcurrP airs ← SetcurrP airs ∪ newP air
Intel Core i7 CPU, 16GB memory and no GPU. The fea-
ture vector generation and basic block embedding generation
components in DEEPBINDIFF are expected to be signiﬁcantly
faster if GPUs are utilized since they are built upon deep
learning models.
Datasets. To thoroughly evaluate the effectiveness of DEEP-
BINDIFF, we utilize three popular binary sets - Coreutils [2],
Diffutils [3] and Findutils [4] with a total of 113 binaries.
Multiple different versions of the binaries (5 versions for
Coreutils, 4 versions for Diffutils and 3 versions of Findutils)
are collected with wide time spans between the oldest and
newest versions (13, 15, and 7 years respectively). This setting
ensures that each version has enough distinctions so that binary
difﬁng results among them are meaningful and representative.
We then compile them using GCC v5.4 with 4 different
compiler optimization levels (O0, O1, O2 and O3) in order
to produce binaries equipped with different optimization tech-
niques. This dataset is to show the effectiveness of DEEPBIN-
DIFF in terms of cross-optimization-level difﬁng. We randomly
select half of the binaries in our dataset for token embedding
model training.
To demonstrate the effectiveness with C++ programs, we
also collect 2 popular open-source C++ projects LSHBOX [8]
and indicators [6], which contain plenty of virtual functions,
from GitHub. The two projects include 4 and 6 binaries respec-
tively. In LSHBOX, the 4 binaries are psdlsh, rbslsh, rhplsh and
thlsh. And in indicators, there exist 6 binaries - blockprogress-
bar, multithreadedbar, progressbarsetprogress, progressbartick,
progressspinner and timemeter. For each project, we select 3
major versions and compile them with the default optimization
levels for testing.
Finally, we leverage two different real-world vulnerabilities
in a popular crypto library OpenSSL [9] for a case study to
demonstrate the usefulness of DEEPBINDIFF in practice.
Baseline Techniques. With the aforementioned datasets, we
compare DEEPBINDIFF with two state-of-the-art baseline
techniques (Asm2Vec [23] and BinDiff
[10]). Note that
7
Asm2Vec is designed only for function level similarity detec-
tion. We leverage its algorithm to generate embeddings, and
use the same k-hop greedy matching algorithm to perform
difﬁng. Therefore, we denote it as ASM2VEC+k-HOP. Also,
to demonstrate the usefulness of the contextual information, we
modify DEEPBINDIFF to exclude contextual information and
only include semantics information for embedding generation,
shown as DEEPBINDIFF-CTX.
As mentioned in Section I, another state-of-the-art tech-
nique InnerEye [58] has scalability issue for binary difﬁng.
Hence, we only compare it with DEEPBINDIFF using a set of
small binaries in Coreutils. Note that we also apply the same
k-hop greedy matching algorithm in InnerEye, and denote it
as INNEREYE+k-HOP.
B. Ground Truth Collection
For the purpose of evaluation, we rely on source code level
matching and debug symbol
information to conservatively
collect ground truth that indicates how basic blocks from two
binaries should match.
Particularly, for two input binaries, we ﬁrst extract source
ﬁle names from the binaries and use Myers algorithm [46] to
perform text based matching for the source code in order to get
the line number matching. To ensure the soundness of our ex-
tracted ground truth, 1) we only collect identical lines of source
code as matching but ignore the modiﬁed ones; 2) our ground
truth collection conservatively removes the code statements
that lead to multiple basic blocks. Therefore, although our
source code matching is by no means complete, it is guaranteed
to be sound. Once we have the line number mapping between
the two binaries, we extract debug information to understand
the mapping between line numbers and program addresses.
Eventually, the ground truth is collected by examining the basic
blocks of the two binaries containing program addresses that
map to the matched line numbers.
Example. To collect the ground truth for basic block matching
between v5.93 and v8.30 of Coreutils binary chown, we ﬁrst
extract the names of the source ﬁles and perform text-based
matching between the corresponding source ﬁles. By matching
the source ﬁles chown.c in the two versions, we know Ln.288
in v5.93 should be matched to Ln.273 in v8.30. Together with
the debug information extracted, a matching between address
0x401cf8 in v5.93 and address 0x4023fc in v8.30 can
be established. Finally, we generate basic blocks for the two
binaries. By checking the basic block addresses, we know basic
block 3 in v5.93 should be matched to basic block 13 in v8.30.
C. Effectiveness
With the datasets and ground truth information, we evalu-
ate the effectiveness of DEEPBINDIFF by performing difﬁng
between binaries across different versions and optimization
levels, and comparing the results with the baseline techniques.
Evaluation Metrics. We use precision and recall metrics to
measure the effectiveness of the difﬁng results produced by
difﬁng tools. The matching result M from DEEPBINDIFF can
be presented as a set of basic block matching pairs with
a length of x as Equation 5. Similarly,
the ground truth
information G for the two binaries can be presented as a set of
basic block matching pairs with a length of y as Equation 6.
M = {(m1, m
(cid:48)
(cid:48)
(cid:48)
2), ..., (mx, m
1), (m2, m
x)}
G = {(g1, g
(cid:48)
1), (g2, g
(cid:48)
2), ..., (gy, g
(cid:48)
y)}
(5)
(6)
We then introduce two subsets, Mc and Mu, which rep-
resent correct matching and unknown matching respectively.
Correct match Mc = M ∩ G is the intersection of our result
M and ground truth G. It gives us the correct basic block
matching pairs. Unknown matching result Mu represents the
basic block matching pairs in which no basic block ever
appears in ground truth. Thus, we have no idea whether these
matching pairs are correct. This could happen because of
the conservativeness of our ground truth collection process.
Consequently, M − Mu − Mc portrays the matching pairs
in M that are not in Mc nor in Mu, therefore, all pairs in
M − Mu − Mc are conﬁrmed to be incorrect matching pairs.
Once M and G are formally presented, the precision metric
presented in Equation 7 gives the percentage of correct match-
ing pairs among all the known pairs (correct and incorrect).
P recision =
||M ∩ G||
||M ∩ G|| + ||M − Mu − Mc||