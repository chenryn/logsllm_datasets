network reliability model where V = (V1, V2, . . . , Vn) is the
set of vertices, E = (E1, E2, . . . , Em) the set of (undirected)
edges, and p = (p1, p2, . . . , pm) the vector of edge proba-
bilities, i.e., Ei exists with probability pi for all i ∈ [m] =
{1, 2, . . . , m}. The random variable X = (X1, X2, . . . , Xm)
encodes the states of edges in G,
i.e., Xi’s are mutually
independent and Xi ∼ Bernoulli(pi) for all i ∈ [m]. For
every x ∈ X = {0, 1}m, let G(x) = (V, E(x)) be the
i.e.,
undirected graph whose edges are determined by x,
E(x) = {Ei ∈ E : xi = 1}. We sayG generates G(x)
with probability
(pixi + (1− pi)(1 − xi)).
P(G = G(x)) = P(X = x) =
We may think of G as a function of X and conveniently denote
G ≡ G(X). For a given pair s, t ∈ V , let φs,t(x) denote the
binary function that returns 1 if there is an s-t path in G(x)
and 0 otherwise. Given any γ ∈ [0, 1), the quantity
m(cid:2)
i=1
ρ = P(φs,t(X) ≤ γ)
computes the s, t-unreliability (or two-terminal unreliability)
of G(X), which is the probability that there exists no s-t path
in G(X). The two-terminal unreliability problem has been of
great interest in the domain of network reliability [21], [2],
[24], [25]. Unfortunately, exact computation of ρ turns out to
be an NP-hard problem [21]. Moreover, when pi’s are close
to one, estimating ρ becomes a rare event simulation problem
[2], [24], [25].
2) Maximum ﬂow analysis: A generalization of the directed
network reliability model is the stochastic ﬂow network [3],
where edges exist with certainty but have random capacities.
Here Xi describes the capacity of link Ei for all i ∈ [m] [26],
[22]. Suppose Xi’s are non-negative random variables and for
every x ∈ X, let G(x) = (V, E; x) be the directed graph
whose edge Ei has capacity xi for all i ∈ [m]. For a chosen
pair s, t ∈ V , letψ s,t(x) be the function that computes the
maximum ﬂow from s to t in G(x). The quantity
ρ = P(ψs,t(X) ≤ T )
(2)
computes the network unreliability, which is the probability
that the maximum ﬂow from s to t in G(X) is less than a ﬂow
demand T . This formulation arises in modeling of the network-
structured systems such as telephone and transportation system
[3], where T is the minimum threshold for the system to be
functional. If T is small and Xi’s have thin left tails, then
estimating ρ becomes a rare event simulation problem.
3) Attack graph analysis: In network security, losses due to
cyber-attacks depend on the attacker’s ability to move through
the network and is a function of the set of hosts the attacker can
compromise. In [27], [4], the authors proposed to use directed
network reliability models (also known as uncertain graphs) to
capture uncertainty in attack graphs. A special host s ∈ V is
chosen as the attacker’s initial point of intrusion. Each attack
scenario x ∈ X, where X is the support of X, deﬁnes a
deterministic attack graph G(x). Let Cs(x) denote the set of
all hosts reachable from s in G(x), the loss to the network
under scenario x is the sum of the loss to each host in Cs(x)
Vi∈Cs(x) L({Vi}) [4]. Given an
according to L(Cs(x)) =
attack loss threshold T , the loss tail probability risk metric
deﬁned as
(cid:3)
ρ = P(L(Cs(X)) > T )
(3)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
308
computes the probability that the network suffers a loss greater
than T due to cyber-attacks. Clearly,
in a well-designed
network and for a large T , estimating ρ becomes a rare event
simulation problem.
B. Crude Monte Carlo
A simple method for estimating ρ is to sample X and obtain
M random samples X (1), X (2), . . . , X (M ) where X (i)’s are
independent and identically distributed (i.i.d.) according to
X (i) ∼ P. The crude (or naive) MC estimator deﬁned as
M(cid:4)
i=1
ρMC =
1
M
1{X (i)∈Xγ}
(4)
computes an unbiased estimate of ρ, i.e., E(ρMC) = ρ, where
E denotes the expected value of a random variable and 1{} is
the indicator function, which returns 1 if the condition in the
brackets is true and 0 otherwise. To evaluate the performance
of ρMC, we examine its relative error (RE), which is deﬁned
as the standard deviation of the estimator divides by its mean
(cid:5)
RE(ρMC) =
(1 − ρ)
ρM
.
(5)
We can see that the RE of the crude MC estimator grows in
an unbounded fashion as ρ approaches zero.
C. Importance sampling
We limit our discussion to the case in which both P and
P(cid:2) are distributions of mutually independent random variables.
Moreover, the proposal distribution can be parameterized by
a vector θ = (θ1, θ2, . . . , θm) ∈ Θ. Therefore, we will use the
notion Pθ (instead of P(cid:2)) to denote the proposal distribution
and PΘ the class of proposal distributions under consideration.
Unless otherwise stated, P ∈ PΘ. Although multivariate
distributions can be used as proposals—e.g., the zero-variance
approximation in [2] uses jointly distributed Xi’s and the pop-
ulation Monte Carlo method in [20] uses Gaussian mixtures—
the mutual
independence assumption allows us to exploit
the symmetry in f and to sample Pθ efﬁciently once θ is
computed. For all x = (x1, x2, . . . , xm) ∈ X, we have
m(cid:2)
m(cid:2)
P(X=x) =
P(Xi=xi) and Pθ(X=x) =
Pθ(Xi=xi).
i=1
i=1
First, we note that any Pθ satisfying Pθ(X = x) > 0 for all
x ∈ Xγ such that P(X = x) > 0 can be used as a proposal
distribution. Under this condition, let X (1), X (2), . . . , X (M )
be M i.i.d. random variables under Pθ. The IS estimator
wθ(X
(i)
)
(6)
M(cid:4)
i=1
ρIS =
1
M
(cid:6)
wθ(x) =
is an unbiased estimator of ρ, i.e., Eθ(ρIS) =ρ, where wθ is
the sample weight (or the likelihood ratio) deﬁned as
P(X=x)
Pθ(X=x)
0
if x ∈ Xγ,
otherwise.
309
The main difﬁculty with IS is to design a proposal distribution
Pθ that minimizes the variance of wθ(X) where X ∼ Pθ.
Under the proposal distribution P∗ given as
P∗
(X = x) =
ρ P(X = x)
0
if x ∈ Xγ,
otherwise
(7)
(cid:7) 1
the IS estimator in (6) achieves zero variance—in other words,
it returns an exact estimate of the rare event probability ρ using
only one sample. For this reason, we call P∗ the optimal pro-
posal distribution. P∗ cannot generally be computed directly
because its computation requires knowledge of ρ, which is the
unknown of interest. In fact, sampling from P∗ can be difﬁcult
even if ρ is known [12]. Therefore, research in IS focuses on
developing techniques for approximating P∗.
III. MAXIMUM WEIGHT MINIMIZATION
Importance sampling for rare event simulation seeks to
improve the hitting rate Pθ(Xγ), which is the probability
of the rare event under the proposal distribution [12], [10],
[2]. Achieving a high hitting rate requires that we accurately
locate the region of the rare event. This can be challenging if
f is a complex and black-box function such as a deep neural
network [6]. In such cases, adaptive IS approaches such as
CE or population Monte Carlo rely on iterative methods to
approximate Xγ. The approximation accuracy as well as the
hitting rate generally improve over time as more samples are
being collected.
tail [11]. However,
When f is non-decreasing in Xi, the hitting rate can be
toward
improved simply by biasing the distribution of Xi
its left
too much biasing may distort
the weight distribution and exacerbate the weight degeneracy
problem [20], [13], [11] (see the example in Appendix E).
Intuitively, a good proposal distribution boosts the occurrence
of Xγ by a signiﬁcant amount while keeping wθ(X) relatively
evenly inﬂated. One way to achieve the latter is to choose
Pθ that minimizes the variance of the weight distribution.
Unfortunately, doing so leads to a complicated numerical
optimization problem [19], [16]. Our search for a more viable
alternative leads to the development of the maximum weight
minimization (MWM) method described below.
A. Problem formulation
First, we note that the following inequality holds for every
proposal distribution Pθ
ρ ≤ max
x∈Xγ
wθ(x)
and equality happens if and only if Pθ ≡ P∗. In other words,
P∗ is the minimizer to the maximum weight minimization
problem. This result directly follows (6) and the fact that ρIS
is an unbiased estimator of ρ. Moreover, an IS estimator that
has a smaller maximum weight tends to suffer less from weight
degeneracy, since weight degeneracy is caused by samples
that have disproportionately large weights despite occurring
infrequently. To minimize the maximum weight, we solve the
following optimization problem
{ max
x∈Xγ
log wθ(x)}
min
θ∈Θ
(8)
where the log function preserves the optimality of the solution.
While it is unlikely that P∗ ∈ PΘ, solving the optimization
problem gives us the best approximation of P∗ in PΘ, which
may be sufﬁcient for simulation purposes.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
It
is difﬁcult
to solve (8) directly since Xγ implicitly
depends on the objective function f, which can be arbitrarily
complex (e.g., see [6]). Following [19], we approximate Xγ
by Y and solve
{max
x∈Y
min
θ∈Θ
log wθ(x)}
(9)
where Y is a subset of Xγ, which can be obtained using
random sampling [19], meta-heuristic search [28], or any
other methods1. The idea is that if the samples in Y are
“representative” of Xγ, then the solution of (9) will be close
to that of (8). For convenience, the minimax problem in (9) is
converted into a standard form
min
θ∈Θ
t
s.t. log wθ(x) − t ≤ 0 for all x ∈ Y,
which can be further reﬁned by utilizing the fact that both
P and Pθ are distributions of mutually independent random
variables
m(cid:4)
log P(Xi=xi) − m(cid:4)
i=1
min
θ∈Θ
t s.t.
for all x ∈ Y.
i=1
log Pθ(Xi=xi) − t ≤ 0
(10)
By solving the optimization problem in (10), we obtain the
parameters of the proposal distribution for estimating ρ.
B. Remarks
Under certain choices of PΘ the constraints in (10) are
convex, which means it can be solved using available convex
optimization solvers such as CVX [29] or cvxopt [30]. The
convexity condition holds for many distributions in the expo-
nential family, including Bernoulli, categorical (or generalized
Bernoulli), Beta, and a modiﬁed version of the exponential
distribution deﬁned in Section VI.
In general, the larger the cardinality of Y is, the better
it represents Xγ. Formally, let v1, v2, v3 be the minimized
maximum weights obtained by solving (10) under different
choices of approximation Y1 ≡ Xγ, Y2, and Y3, respectively.
Furthermore, suppose that Y3 ⊆ Y2 ⊆ Y1. It is easy to verify
that v3 ≤ v2 ≤ v1. Therefore, the solution to (10) under Xγ
is closer to that under Y2 than to that under Y3. When this
happens, we say Y2 is more representative of Xγ than Y3
(i.e., in the context of solving the MWM problem).
By deﬁnition, both Xγ and Y depend on the objective
function f and the threshold γ but not on the original dis-
tribution P nor the choice of proposal distributions PΘ. This
suggests that the same Y can be used for different P’s should
it change in the future. However, the optimality of Y in terms
of representativeness depends on P. This will become clear
in Section V where we exploit the symmetry in f and P to
purge redundant samples in Y.
IV. MONOTONE OBJECTIVE FUNCTION
A function f is monotonically non-decreasing in xi
if
∂f /∂xi ≥ 0 (this deﬁnition still applies to the case ∂f /∂xi ≤
0 using a simple change of variable xi := −xi). In particular,
f is fully monotone if it is monotonically non-decreasing in all
of its variables. Full monotonicity implies that for all x, y ∈ X
such that x (cid:8) y—i.e., xi ≤ yi for all i ∈ [m]—we have
f (x) ≤ f (y). Therefore, if y ∈ Xγ then it follows that
1In comparison, the variance minimization method in [16] also used Y to
compute θ but required a stricter condition where Y is sampled under P∗.
x ∈ Xγ. This is a rather strong assumption and might not
hold in practice. A more benign assumption requires that f
is monotonically non-decreasing in a subset of its variables.
Some examples of full and partial monotonicity are given
below.
1) The reliability f (X) of a stochastic binary system [21] is
a function of the reliability Xi of its components, where
Xi ∼ Bernoulli(pi) for all i ∈ [m]. If the system is
coherent [31], [32] then for every component i, asp i
increases, f (X) either increases or at least remains un-
changed. Coherency is assumed in most network reliability
models.
2) In attack graph analysis [27],
the security f (X) of a
computer network is a function of the vulnerabilities of
each of its hosts. As the vulnerabilities on the hosts are
patched, one can expect
the network to become more
secure.
A. Order-preserving heuristic
Monotonicity makes it easy to generate Y since samples in
the tails are more likely to be in Xγ. Moreover, monotonicity
allows us to reduce the parameter space Θ and to improve the
representativeness of Y. The latter is achieved by substituting
a rare event sample with a “critical” sample that is closer to
the boundary of Xγ, since the critical sample always has a
greater weight, regardless of the choice of θ. These results are
due to the following propositions.
Proposition IV.1 (Stochastic ordering under P∗). If f is
monotonically non-decreasing in xi for some i ∈ [m], then
P∗
(Xi ≤ xi) = P(Xi ≤ xi|X ∈ Xγ) ≥ P(Xi ≤ xi) for all xi.
(11)
In other words, Xi under P∗ is stochastically smaller [33]
than Xi under P.
Proposition IV.1 reinforces our intuition that the optimal
proposal distribution P∗ indeed shifts the density of Xi to its
left tail. In contrary, a proposal distribution Pθ ∈ PΘ that
well approximates P∗ should satisfy this stochastic ordering
property by maintaining Pθ(Xi ≤ xi) ≥ P(Xi ≤ xi) for all
xi. This order-preserving heuristic allows us to make some
reﬁnements to the MWM problem. In this section, we present
the results for Bernoulli-distributed proposals. Generalization
to other distributions is further given in Section VI.