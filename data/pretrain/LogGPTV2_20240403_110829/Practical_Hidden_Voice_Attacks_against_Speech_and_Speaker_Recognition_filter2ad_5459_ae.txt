method of increasing the unintelligibility of audio, studies in
psychoacoustics have demonstrated the robustness of speech in
the presence of noise [29]. When noise is added to otherwise
normal speech, the auditory properties of speech stand out
above the added noise thus ensuring intelligibility. If enough
noise is added to speech, interpretation will become more
difﬁcult, because original speech becomes suppressed by the
large amount of noise. Adding noise to speech as a perturbation
method will have little beneﬁt in terms of making the audio
harder for humans to interpret. Additionally, as additional noise
is added to the attack audio, it also becomes difﬁcult for a VPS
to accurately transcribe the audio, due to the suppression of
the speech.
To evaluate their attack audio intelligibility, Carlini et al.
used Amazon Mechanical Turk [1], asking participants to listen
to their attack audio and attempt to transcribe it. Though this
may seem like a suitable evaluation method, this presents many
uncontrolled variables that could have a signiﬁcant impact
on their results. The issue with using Amazon Turk is that
the experiment was not conducted locally, resulting in many
uncontrollable variables that could affect a participant’s ability
to transcribe their attack audio. These variables include, but
are not limited to, age [57], [53], ﬁrst language [59], range
of hearing [43], listening environment, audio equipment, and
visual stimuli [64].
Quantifying the intelligibility of speech is challenging [66],
but these survey results have too many uncontrollable vari-
ables to make an accurate conclusion. There does not exist
any widely accepted metric to measure unintelligibility of
an audio sample. The L2 norm has been used in previous
works, discussed in Section VIII, to quantify the distortion of
adversarial images when attacking image classiﬁers. However,
to use the L2 norm to measure attack audio distortion would
be incorrect. This is because we base the unintelligibility of
our audio on principles of psychoacoustics and the biology
of human hearing which is substantially and fundamentally
different from factors associated with image classiﬁcation. We
have made our attack audio available to the public online.3
Carlini et al. also make their audio available online4,which we
encourage the reader use as a comparison.
Performing a small-scale study, rather than using the estab-
lished results from another scientiﬁc community, is redundant
and prone to error; in effect, we would be merely reporting
anecdotes. Instead, we validate our results by citing widely
accepted published work from psychoacoustics which shows
that our audio has the properties that make it unintelligible.
As Figure 6 clearly demonstrates, our audio better exhibits
the characteristics associated with unintelligibility than the
previous work.
Practical Limitations Carlini’s attack is only possible under
the assumption that the adversary has white-box knowledge of
3Our attack: https://sites.google.com/view/practicalhiddenvoice
4Carlini et al.’s attack: http://www.hiddenvoicecommands.com/black-box
the victim’s model. The attack can only be carried out against
HMM-GMM ASRs, meaning that the attack is insufﬁcient
against state of the art DNN and RNN models, which are
increasingly being deployed. The attack has not been shown
to work against speaker identiﬁcation models either. Carlini’s
attack audio can not be used with any speaker system other
than the one used during the attack audio generation. Lastly,
their white-box attack takes an upwards of 32 hours to gener-
ate. These additional factors severely limit the adversary’s ca-
pabilities. In contrast, our attack is black-box, model-agnostic,
effective against both ASRs and speaker identiﬁcation models,
transferable across speaker systems and takes seconds to gener-
ate. Our attack algorithm provides greater capabilities to the at-
tacker, while simultaneously making fewer assumptions about
the victim model. Lastly, our work exploits the fundamental
nature of human audio perception to generate attack samples.
Our perturbations make use of simple operations, in the time
domain, without using any complex algorithms. In contrast,
Carlini’s work attempts to add noise to the MFCCs, which exist
in the frequency domain. The domains both attacks exploit
are fundamentally different, which impacts the effectiveness
of Carlini’s attack.
B. Defenses
Voice Activity Detection (VAD) VAD is a commonly im-
plemented speech processing algorithm that is used to detect
the presence of human voices in samples of audio. It has
applications in cellular communications [50], Voice-over-IP
(VoIP) [38] and VPSes [62]. VAD differentiates between the
regions of human speech and regions of silence or noise in
audio. By identifying silence and noise, those regions can
be eliminated from the audio. This includes the regions of
noise and silence between words, reducing the audio being
transmitted to just individual words that make up the entire
recording. This is particularly useful for VPSes as using
VAD in preprocessing results in giving a model only the
necessary speech data, potentially reducing processing cost and
improving transcription results.
Based on this description, VAD may be suggested as a
potential defense against the attacks we present in this paper.
If a VPS is using VAD, this would cause the attack audio to
be classiﬁed as noise and not be further processed. As we will
demonstrate, this is not the case.
To test this potential defensive mechanism, we ran a VAD
algorithm on a set of 36 randomly selected attack audio ﬁles.
We executed a MATLAB implementation of the ITU-T G.729
VAD algorithm on the audio ﬁles and observed which regions
were classiﬁed as speech. In all 36 cases, the VAD algorithm
accurately located the speech in the audio.
In addition, we also ran the VAD algorithm over two5
attack audio ﬁles produced by Carlini et al. For both ﬁles
almost all of the audio, including sections between the words
consisting of pure noise, was determined to be speech. We
believe this is the result of the added noise suppressing the
speech, and in turn, “blurring” the lines between speech and
noise. While this does not prevent their attack audio from being
5The VAD experiment requires both an attack audio ﬁle and the original
source speech ﬁle. We could only run the VAD experiment for what was
available to us of Carlini’s attack ﬁles.
12
given to an VPS, this does increase the chance that the audio
will not be transcribed correctly. The noise that is considered
to be speech, especially that between the words, will be sent
along with the actual speech. This increases the chance that
some or all of the speech could be mistranslated, preventing
the attack.
As more systems start to deploy VAD to reduce processing
time and server load, the attack audio produced by Carlini et al.
may not continue to be transcribed correctly. Meanwhile, our
attack audio will continue to work whether or not a system uses
VAD for preprocessing because we did not introduce additional
noise.
Classiﬁer Training The defensive method of training a clas-
siﬁer model to detect adversarial attacks on VPSes has pre-
viously been suggested [24]. We believe this technique to be
impractical and brittle. Machine learning models are imperfect
regardless of training or tasks they are given. Adding a model
to preemptively determine if audio is malicious would most
likely result in usability issues. Legitimate speech commands
may be incorrectly classiﬁed as malicious and ignored because
of imperfections in the model. This would decrease the quality
of the user experience and could potentially result in users
looking to alternative VPS systems.
Altering the VPS Another potential defense mechanism would
be to modify the construction of the VPS in order to hinder
our attack. This could consist of steps such as altering or
removing signal processing or preprocessing steps. However,
tailoring a VPS to not detect or discard the attack audio is not a
viable solution. This will result in a decrease in accuracy of the
VPS. Additionally, we have demonstrated the effectiveness of
our attack audio against Mozilla DeepSpeech, which does not
use any predeﬁned signal processing. Due to our perturbations
being rooted in psychoacoustics, in order to impede our attack,
a defense mechanism would have to be placed at or before the
preprocessing steps.
Such a system has been recently proposed [21] and could
potentially have an effect on the success of our attack. The
idea behind this system is that audio produced by an electro-
acoustic transducer (loudspeaker) will contain low frequencies
in the sub-bass region (20-60 HZ). This is below the speaking
range of most humans and is a good indicator that the speech
was played by a loudspeaker. If this were to be implemented
during preprocessing, this could identify our attack as being
produced by a non-human speaker and prevent it from being
given to the VPS. Similarly, ASRs that use liveness detec-
tion [74] [75] will also be relatively robust against our attack.
However, using such defenses to identify malicious commands
might generate false-positives, which degrade user experience
and product popularity.
VIII. RELATED WORK
From personal assistant systems to speaker identiﬁcation
and investments, machine learning (ML) is becoming increas-
ingly incorporated into our daily lives. Unfortunately, ML
models are inherently vulnerable to a wide spectrum of attacks.
Early research focused on enhancing train-time resilience for
scenarios where the adversary is capable of poisoning training
data (e.g., spam and worm detection) [28], [49]. These early
works exhibited attack vectors that were later formalized into
train-
three axes:
time attacks vs. exploratory,
ii) type of
security violation (integrity vs. denial of service), and iii)
attack speciﬁcity (targeted vs. indiscriminate) [20], [19]. A
detailed overview of each axis is provided by Huang et al. [39].
Although all axes provide utility to an adversary, recent attacks
have focused on a narrow subsection of these axes.
i) inﬂuence of the adversary (causative,
test-time attacks),
Following this categorization, much of the work in adver-
sarial machine learning has focused on exploratory targeted
attacks against image classiﬁers in terms of both availability
and integrity. These attacks vary from changing particular
pixels [40], [68], [34], [18], [67], [48] or patches of pixels [23],
[63], [55], [25] to creating entirely new images that will
classify to a chosen target [51], [44]. Although these attacks
are very successful against image models, they do not sufﬁce
for attacking audio models. Modern image models operate
directly on the supplied image pixels to derive relevant spatial
features [55], [51]. Audio models, in contrast, do not operate
on individual samples, and instead derive representations of
the original temporal space using acoustic properties of the
human voice. This layer of temporal feature extraction adds
a layer of complexity to potential attacks, which means that
small changes to individual samples will likely never propagate
to the ﬁnal feature space used for inference.
Current attacks against audio models can be broken down
into three broad categories. The ﬁrst involves generating ma-
licious audio commands that are completely inaudible to the
human ear but are recognized by the audio model [73]. The
second embeds malicious commands into piece of legitimate
audio (e.g., a song) [72]. The third obfuscates an audio
command to such a degree that the casual human observer
would think of the audio as mere noise but would be correctly
interpreted by the victim audio model [70], [24]. Our work
falls within the third category and closest attack to ours is that
of Carlini et al. [24]. The success of this earlier work is limited
due to the following reasons: i) the attack can only be using
against an Hidden Markov Model-Gaussian Mixture Model
architecture; ii) the attack assumes the attacker has white-box
access to the model; and iii) the attack is slow, taking at least
32 hours of execution. Our attack is designed to overcome
these limitations.
C. Limitations
IX. CONCLUSION
The attack audio will have reduced effectiveness when
used in noisy environments. This is because noise interferes
with the attack audio making it difﬁcult for VPSs to interpret.
However, legitimate commands also do not work well in noisy
environments. Readers who have attempted to use their voice
assistants in a noisy environment have likely experienced such
a degradation.
The security community has recently discovered a num-
ber of weaknesses in speciﬁc machine learning models that
underpin multiple VPSes. Such weaknesses allow an attacker
to inject commands that are unintelligible to humans, but are
still transcribed correctly by VPSes. While effective against
particular models using particular speakers and microphones,
these previous attack are not acoustic hardware independent
13
or widely practical. Instead of attacking underlying machine
learning models,
this paper instead investigates generating
attack audio based on the feature vectors created by sig-
nal processing algorithms. With this black-box approach, we
demonstrate experimentally that our techniques work against
a wide array of both speech detection and speaker recogni-
tion systems both Over-the-Wire and Over-the-Air. Moreover,
because we treat psychoacoustics as a principal element of
our design, we are able to explain why our attacks are less
intelligible than prior work. In so doing, we not only argue that
hidden command attacks are practical, but also that securing
such systems must therefore take greater domain knowledge
of audio processing into consideration.
X. ACKNOWLEDGMENT
We would like to thank our reviewers, in particular our
shepherd Dr. Jay Stokes, for their insightful comments and
suggestions. We would also like to thank the authors of the
Hidden Voice Commands paper [24] who graciously provided
us with both original and attack audio samples. Our compar-
ison would not have been possible without their assistance.
This work was supported in part by the National Science
Foundation under grant number CNS-1526718 and 1540217.
Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do
not necessarily reﬂect
the views of the National Science
Foundation.
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
turk,”
api,”
api,”
api,”
speech
[Online].
veriﬁcation
identiﬁcation
https://www.mturk.com/.
https://github.com/Microsoft/
https://github.com/Microsoft/
https://azure.microsoft.com/en-us/services/
“Amazon mechanical
Available: https://www.mturk.com/
“Audioengine a5+,” https://audioengineusa.com/shop/poweredspeakers/
a5-plus-powered-speakers/.
“Azure
Cognitive-SpeakerRecognition-Python/tree/master/Identiﬁcation.
“Azure
Cognitive-SpeakerRecognition-Python/tree/master/Veriﬁcation.