title:Dompteur: Taming Audio Adversarial Examples
author:Thorsten Eisenhofer and
Lea Sch&quot;onherr and
Joel Frank and
Lars Speckemeier and
Dorothea Kolossa and
Thorsten Holz
Dompteur: Taming Audio Adversarial Examples
Thorsten Eisenhofer, Lea Schönherr, and Joel Frank, Ruhr University Bochum; 
Lars Speckemeier, University College London; Dorothea Kolossa and 
Thorsten Holz, Ruhr University Bochum
https://www.usenix.org/conference/usenixsecurity21/presentation/eisenhofer
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.DOMPTEUR: Taming Audio Adversarial Examples
Thorsten Eisenhofer
Ruhr University Bochum
Lea Schönherr
Ruhr University Bochum
Joel Frank
Ruhr University Bochum
Lars Speckemeier
University College London
Dorothea Kolossa
Ruhr University Bochum
Thorsten Holz
Ruhr University Bochum
Abstract
Adversarial examples seem to be inevitable. These speciﬁ-
cally crafted inputs allow attackers to arbitrarily manipulate
machine learning systems. Even worse, they often seem harm-
less to human observers. In our digital society, this poses a
signiﬁcant threat. For example, Automatic Speech Recogni-
tion (ASR) systems, which serve as hands-free interfaces to
many kinds of systems, can be attacked with inputs incompre-
hensible for human listeners. The research community has
unsuccessfully tried several approaches to tackle this problem.
In this paper we propose a different perspective: We accept
the presence of adversarial examples against ASR systems,
but we require them to be perceivable by human listeners. By
applying the principles of psychoacoustics, we can remove
semantically irrelevant information from the ASR input and
train a model that resembles human perception more closely.
We implement our idea in a tool named DOMPTEUR1 and
demonstrate that our augmented system, in contrast to an un-
modiﬁed baseline, successfully focuses on perceptible ranges
of the input signal. This change forces adversarial examples
into the audible range, while using minimal computational
overhead and preserving benign performance. To evaluate
our approach, we construct an adaptive attacker that actively
tries to avoid our augmentations and demonstrate that adver-
sarial examples from this attacker remain clearly perceivable.
Finally, we substantiate our claims by performing a hearing
test with crowd-sourced human listeners.
1 Introduction
The advent of deep learning has changed our digital society.
Starting from simple recommendation techniques [1] or image
recognition applications [2], machine-learning systems have
evolved to solve and play games on par with humans [3–6], to
predict protein structures [7], identify faces [8], or recognize
speech at the level of human listeners [9]. These systems
are now virtually ubiquitous and are being granted access to
1The French word for tamer
critical and sensitive parts of our daily lives. They serve as our
personal assistants [10], unlock our smart homes’ doors [11],
or drive our autonomous cars [12].
Given these circumstances, the discovery of adversarial
examples [13] has had a shattering impact. These speciﬁ-
cally crafted inputs can completely mislead machine learning-
based systems. Mainly studied for image recognition [13],
in this work, we study how adversarial examples can affect
Automatic Speech Recognition (ASR) systems. Preliminary
research has already transferred adversarial attacks to the au-
dio domain [14–19]. The most advanced attacks start from
a harmless input signal and change the model’s prediction
towards a target transcription while simultaneously hiding
their malicious intent in the inaudible audio spectrum.
To address such attacks, the research community has de-
veloped various defense mechanisms [20–25]. All of the
proposed defenses—in the ever-lasting cat-and-mouse game
between attackers and defenders—have subsequently been
broken [26]. Recently, Shamir et al. [27] even demonstrated
that, given certain constraints, we can expect to always ﬁnd
adversarial examples for our models.
Considering these circumstances, we ask the following
research question: When we accept that adversarial examples
exist, what else can we do? We propose a paradigm shift:
Instead of preventing all adversarial examples, we accept the
presence of some, but we want them to be audibly changed.
To achieve this shift, we take inspiration from the machine
learning community, which sheds a different light on adver-
sarial examples: Illyas et al. [28] interpret the presence of
adversarial examples as a disconnection between human ex-
pectations and the reality of a mathematical function trained
to minimize an objective. We tend to think that machine learn-
ing models must learn meaningful features, e. g., a cat has
paws. However, this is a human’s perspective on what makes
a cat a cat. Machine learning systems instead use any avail-
able feature they can incorporate in their decision process.
Consequently, Illyas et al. demonstrate that image classiﬁers
utilize so-called brittle features, which are highly predictive,
yet not recognizable by humans.
USENIX Association
30th USENIX Security Symposium    2309
Recognizing this mismatch between human expectations
and the inner workings of machine learning systems, we pro-
pose a novel design principle for ASR system inspired by
the human auditory system. Our approach is based on two
key insights: (i) the human voice frequency is limited to the
band ranges of approximately 300−5000Hz [29], while ASR
systems are typically trained on 16kHz signals, which range
from 0− 8000Hz, and (ii) audio signal can carry information,
inaudible to humans [15]. Given these insights, we modify
the ASR system by restricting its access to frequencies and ap-
plying psychoacoustic modeling to remove inaudible ranges.
The effects are twofold: The ASR system can learn a better
approximation of the human perception during training (i.e.,
discarding unnecessary information), while simultaneously,
adversaries are forced to place any adversarial perturbation
into audible ranges.
We implement these principles in a prototype we call
DOMPTEUR.
In a series of experiments, we demonstrate
that our prototype more closely models the human auditory
system. More speciﬁcally, we successfully show that our
ASR system, in contrast to an unmodiﬁed baseline, focuses
on perceptible ranges of the audio signal. Following Car-
lini et al. [30], we depart from the lab settings predominantly
studied in prior work: We assume a white-box attacker with
real-world capabilities, i.e., we grant them full knowledge of
the system and they can introduce an unbounded amount of
perturbations. Even under these conditions, we are able to
force the attacker to produce adversarial examples with an
average of 24.33 dB of added perturbations while remaining
accurate for benign inputs. Additionally, we conduct a large
scale user study with 355 participants. The study conﬁrms
that the adversarial examples constructed for DOMPTEUR
are easily distinguishable from benign audio samples and
adversarial examples constructed for the baseline system.
In summary, we make the following key contributions:
• Constructing an Augmented ASR. We utilize our key
insights to bring ASR systems in better alignment with
human expectations and demonstrate that traditional
ASR systems indeed utilize non-audible signals that are
not recognizable by humans.
• Evaluation Against Adaptive Attacker. We construct
a realistic scenario where the attacker can adapt to the
augmented system. We show that we successfully force
the attacker into the audible range, causing an average
of 24.33 dB added noise to the adversarial examples. We
could not ﬁnd adversarial examples when applying very
aggressive ﬁltering; however, this causes a drop in the
benign performance.
• User Study. To study the auditory quality of adversarial
examples, we perform a user study with an extensive
crowd-sourced listening test. Our results demonstrate
that the adversarial examples against our system are sig-
niﬁcantly more perceptible by humans.
To support further research in this area, we open-source
our prototype implementation, our pre-trained models, and
audio samples online at github.com/rub-syssec/dompteur.
2 Technical Background
In the following, we discuss the background necessary to
understand our augmentation of the ASR system. For this
purpose, we brieﬂy introduce the fundamental concepts of
ASRs and give an overview of adversarial examples. Since
our approach fundamentally relies on psychoacoustic model-
ing, we also explain masking effects in human perception.
Speech Recognition ASR constitutes the computational
core of today’s voice interfaces. Given an audio signal, the
task of an ASR system is to transcribe any spoken content
automatically. For this purpose, traditionally, purely statistical
models were used. They now have been replaced by modern
systems based on deep learning methods [31–33], often in the
form of hybrid neural/statistical models [34].
In this paper, we consider the open-source toolkit
KALDI [35] as an example of such a modern hybrid sys-
tem. Its high performance on many benchmark tasks has led
to its broad use throughout the research community as well as
in commercial products like e. g., Amazon’s Alexa [36–38].
KALDI, and similar DNN/HMM hybrid systems can gener-
ally be described as three-stage systems:
1. Feature Extraction. For the feature extraction, a frame-
wise discrete Fourier transform (DFT) is performed on
the raw audio data to retrieve a frequency representation
of the input signal. The input features of the Deep Neu-
ral Networks (DNN) are often given by the log-scaled
magnitudes of the DFT-transformed signal.
2. Acoustic Model DNN. The DNN acts as the acoustic
model of the ASR system. It calculates the probabilities
for each of the distinct speech sounds (called phones)
of its trained language being present in each time frame
from its DFT input features. Alternatively, it may com-
pute probabilities, not of phones, but of so-called clus-
tered tri-phones or, more generally, of data-driven units
termed senones.
3. Decoding. The output matrix of the DNN is used to-
gether with an hidden Markov model (HMM)-based lan-
guage model to ﬁnd the most likely sequence of words,
i. e., the most probable transcription. For this purpose, a
dynamic programming algorithm, e.g., Viterbi decoding,
is used to search the best path through the underlying
HMM. The language model describes the probabilities
of word sequences, and the acoustic model output gives
the probability of being in each HMM state at each time.
2310    30th USENIX Security Symposium
USENIX Association
suring the hearing thresholds, i. e., the necessary sound
pressures for each frequency to be audible in otherwise
quiet environments, one can determine the so-called ab-
solute hearing threshold as depicted in Figure 1a. Gen-
erally speaking, everything above the absolute hearing
thresholds is perceptible in principle by humans, which
is not the case for the area under the curve. As can be
seen, much more energy is required for a signal to be
perceived at the lower and higher frequencies. Note that
the described thresholds only hold for cases where no
other sound is present.
• Frequency Masking. The presence of another sound—
a so-called masking tone—can change the described
hearing thresholds to cover a larger area. This masking
effect of the masking tone depends on its sound pressure
and frequency. Figure 1b shows an example of a 1 kHz
masking tone, with its induced changes of the hearing
thresholds indicated by the dashed line.
• Temporal Masking. Like frequency masking, temporal
masking is also caused by other sounds, but these sounds
have the same frequency as the masked tone and are
close to it in the time domain, as shown in Figure 1c.
Its root cause lies in the fact that the auditory system
needs a certain amount of time, in the range of a few
hundreds of milliseconds, to recover after processing a
higher-energy sound event to be able to perceive a new,
less energetic sound. Interestingly, this effect does not
only occur at the end of a sound but also, although much
less distinct, at the beginning of a sound. This seeming
causal contradiction can be explained by the processing
of the sound in the human auditory system.
Adversarial Examples Since the seminal papers by
Szegedy et al. [13] and Biggio et al. [42], a ﬁeld of research
has formed around adversarial examples. The basic idea is
simple: An attacker starts with a valid input to a machine
learning system. Then, they add small perturbations to that in-
put with the ultimate goal of changing the resulting prediction
(or in our case, the transcription of the ASR).
More formally, given a machine learning model f and an
input-prediction pair (cid:104)x, y(cid:105), where f (x) = y, we want to ﬁnd
a small perturbation δ s.t.:
x(cid:48) = x + δ ∧ f (x(cid:48)) (cid:54)= f (x).
In this paper, we consider a stronger type of attack, a tar-
geted one. This has two reasons: the ﬁrst is that an untargeted
attack in the audio domain is fairly easy to achieve. The sec-
ond is that a targeted attack provides a far more appealing (and
thus, far more threatening) real-life use case for adversarial
examples. More formally, the attacker wants to perturb an in-
put phrase x (i.e., an audio signal) with a transcription y (e.g.,
“Play the Beatles”) in such a way that the ASR transcribes
(a) Absolute Hearing Thresholds
(b) Frequency Masking
(c) Temporal Masking
Figure 1: Psychoacoustic allows to describe limitations of
the human auditory system. Figure 1a shows the average
human hearing threshold in quiet. Figure 1b shows an exam-
ple of masking, illustrating how a loud tone at 1kHz shifts the
hearing thresholds of nearby frequencies and Figure 1c shows
how the recovery time of the auditory system after processing
a loud signal leads to temporal masking.
Psychoacoustic Modeling Recent attacks against ASR sys-
tems exploit intrinsics of the human auditory system to make
adversarial examples less conspicuous [17, 39–41]. Speciﬁ-
cally, these attacks utilize limitations of human perception to
hide modiﬁcations of the input audio signal within inaudible
ranges. We use the same effects for our approach to remove
inaudible components from the input:
• Absolute Hearing Threshold. Human listeners can only
perceive sounds in a limited frequency range, which di-
minishes with age. Moreover, for each frequency, the
sound pressure is important to determine whether the sig-
nal component is in the audible range for humans. Mea-
USENIX Association
30th USENIX Security Symposium    2311
0.020.050.10.20.51251020Frequency(kHz)020406080HearingThresholds(dB)0.020.050.10.20.51251020Frequency(kHz)020406080HearingThresholds(dB)-100-5005010015020025030050400Time(ms)020406080HearingThresholds(dB)an attacker-chosen transcription y(cid:48) (e.g., “Unlock the front
door”). This can be achieved by computing an adversarial
example x(cid:48) based on a small adversarial perturbation δ s.t.:
x(cid:48) = x + δ ∧ ASR(x(cid:48)) = y(cid:48) ∧ y (cid:54)= y(cid:48).
(1)
There exist a multitude of techniques for creating such ad-
versarial examples. We use the method introduced by Schön-
herr et al. [17] for our evaluation in Section 4. The method
can be divided into three parts: In a ﬁrst step, attackers choose
a ﬁxed output matrix of the DNN to maximize the probability
of obtaining their desired transcription y(cid:48). As introduced be-
fore, this matrix is used in the ASR system’s decoding step
to obtain the ﬁnal transcription. They then utilize gradient
descent to perturb a starting input x (i. e., an audio signal feed
into the DNN), to obtain a new input x(cid:48), which produces the
desired matrix. This approach is generally chosen in white-
box attacks [16, 18]. Note that we omit the feature extraction
part of the ASR; however, Schönherr et al. have shown that
this part can be integrated into the gradient step itself [17].
A third (optional) step is to utilize psychoacoustic hearing
thresholds to restrict the added perturbations to inaudible fre-
quency ranges. More technical details can be found in the
original publication [17].
3.1
Implementation
In the following, we present an overview of the implementa-
tion of our proposed augmentations. We extend the state-of-
the-art ASR toolkit KALDI with our augmentations to build
a prototype implementation called DOMPTEUR. Note that
our proposed methods are universal and can be applied to any
ASR system.
Psychoacoustic Filtering Based on the psychoacoustic
model of MPEG-1 [43], we use psychoacoustic hearing
thresholds to remove parts of the audio that are not perceiv-
able to humans. These thresholds deﬁne how dependencies
between certain frequencies can mask, i.e., make inaudible,
other parts of an audio signal. Intuitively, these parts of the
signal should not contribute any information to the recog-
nizer. They do, however, provide space for an attacker to hide
adversarial noise.
We compare the absolute values of the complex valued
short-time Fourier transform (STFT) representation of the
audio signal S with the hearing thresholds H and deﬁne a
mask via
(cid:40)
0
1
3 Modeling the Human Auditory System