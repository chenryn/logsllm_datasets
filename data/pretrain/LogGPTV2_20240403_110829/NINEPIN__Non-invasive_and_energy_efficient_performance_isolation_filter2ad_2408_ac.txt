### 优化后的文本

#### 矩阵表示
矩阵 \( C \) 和 \( B \) 定义如下：
\[ C = \begin{pmatrix}
\eta^*_{1,1} & 0 & \cdots & 0 \\
\eta^*_{2,1} & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
\eta^*_{p,1} & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 \\
\end{pmatrix} \]
\[ B = \begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 \\
0 & 0 & \cdots & 0 \\
\end{pmatrix} \]

其中，\(\zeta^*_{ij}\) 是应用 \(i\) 的聚合参数向量的第 \(j\) 个元素。同样地，\(\eta^*_{ij}\) 是应用 \(i\) 的聚合参数向量的第 \(j\) 个元素。

### 模型预测控制器 (MPC)

控制目标是将系统引导至最优的目标跟踪状态，同时对控制变量的大变化进行惩罚。它最小化每个应用性能与其目标之间的偏差。

#### 多输入多输出 (MIMO) 控制问题
模型预测控制器在每个控制周期 \(k\) 通过最小化以下成本函数来决定控制动作：
\[ V(k) = \sum_{i=1}^{H_p} \| r - y(k + i) \|^2_P + \sum_{j=0}^{H_c-1} \| \Delta u(k + j) \|^2_Q \]
其中，\(y(k)\) 是包含每个应用性能度量的向量。控制器使用线性化的状态空间模型来预测每个应用在未来 \(H_p\) 个控制周期（称为预测范围）内的性能。它计算出一系列控制动作 \(\Delta u(k), \Delta u(k+1), \ldots, \Delta u(k+H_c-1)\) 在 \(H_c\) 个控制周期内（称为控制范围），以使预测的性能接近预定义的目标 \(r\)。控制动作是施加在各种应用上的 CPU 使用限制的变化。\(P\) 和 \(Q\) 是权重矩阵，其相对大小提供了在跟踪精度和控制动作稳定性之间权衡的方法。

控制问题受以下约束：所有应用分配的 CPU 使用限制之和必须不超过物理服务器的总 CPU 容量。该约束可以表示为：
\[ \sum_{j=1}^{M} (\Delta u_j(k) + u_j(k)) \leq U_{\text{max}} \]
其中，\(M\) 是资源池中托管的应用数量，\(U_{\text{max}}\) 是资源池的总 CPU 容量。

#### 转换为二次规划问题
我们将控制公式转换为标准的二次规划问题，从而可以基于有效的二次规划方法设计和实现控制算法。由式 (13) 定义的 MIMO 控制问题被转换为二次规划问题：
\[ \text{Minimize} \quad \frac{1}{2} \Delta u(k)^T H \Delta u(k) + c^T \Delta u(k) \]
\[ \text{subject to} \quad \Omega \Delta u(k) \leq \omega \]
其中，\(\Omega\) 和 \(\omega\) 用于表述 CPU 资源使用的约束条件。这里，\(\Delta u(k)\) 是一个矩阵，包含整个控制范围 \(H_c\) 内每个虚拟机的 CPU 使用限制。在最小化公式中，
\[ H = 2 (R_u^T P R_u + Q) \]
\[ c = 2 [R_u^T P^T (R_x A(k) - r)]^T \]
矩阵 \(R_u\) 和 \(R_x\) 与托管应用的性能干扰模型相关联。

### 系统实现

#### 测试平台
我们在 HP ProLiant BL460C G6 刀片服务器模块和 HP EVA 存储区域网络上实现了 NINEPIN。该刀片服务器配备 Intel Xeon E5530 2.4 GHz 四核处理器和 32 GB PC3 内存。Xeon 处理器具有三级缓存层次结构，每个核心都有自己的 L1（32KB）和 L2（256KB）缓存，并且有一个大的共享 8MB L3 缓存。服务器虚拟化通过企业级虚拟化产品 VMware ESX 4.1 启用。VMware 的 vSphere 模块控制分配给各个虚拟机的 CPU 使用限制（以 MHz 为单位）。

#### NINEPIN 组件
我们在一台独立的机器上实现了 NINEPIN 框架的组件，并通过 VMware vSphere API 4.1 通过网络向虚拟化服务器发送命令。

1. **功率监控器**：通过 VMware ESX 4.1 的新功能，在资源池级别测量虚拟化服务器的平均功耗。VMware 通过其智能电源管理接口传感器收集这些数据。功率监控程序使用 vSphere API 收集功率测量数据。
2. **性能监控器**：使用 RUBiS 客户端提供的传感器程序监控交互式应用的平均端到端请求响应时间。对于计算密集型任务，它测量运行 SPEC CPU2006 基准测试应用程序的每个虚拟机的平均作业完成时间。
3. **性能干扰和能耗建模**：应用减法聚类和 ANFIS 技术，利用从虚拟化服务器系统收集的数据构建性能干扰和能耗模型。MATLAB 的模糊逻辑工具箱用于此目的。
4. **分层控制器**：应用遗传算法进行系统效用优化，并调用 MATLAB 中的 quadprog 二次规划求解器执行第五部分 B 节描述的控制算法。控制算法的解决方案（即虚拟机 CPU 使用限制）发送给资源分配器。
5. **资源分配器**：使用 vSphere API 对虚拟机施加 CPU 使用限制。vSphere 模块提供了一个接口来执行 Reconf igV M T ask 方法以修改虚拟机的 CPU 使用限制。

### 性能评估

为了进行性能评估，我们考虑了各种 SPEC CPU2006 应用的服务级效用函数，如图 4 所示。我们选择这些效用函数作为案例研究，而不失一般性。我们认为能源消耗的效用由以下线性效用函数给出：
\[ U(E) = \lambda \times \text{Energy} \]
其中，Energy 是托管多个 SPEC CPU2006 应用的虚拟化资源池所消耗的总能量，\(\lambda\) 是一个负常数，表示能量和性能目标的相对价值。请注意，NINEPIN 在虚拟化服务器中的适用性与所选效用函数无关。

我们使用 SPEC CPU2006 套件的 runspec 工具在共置的虚拟机上同时运行各种基准测试。每个基准测试运行多次以测量其平均性能（以 SPECspeed 指标表示）。能量使用以千焦耳 (kJ) 为单位测量，它是平均功耗和最长运行任务的平均任务完成时间的乘积。

#### 性能隔离
首先，我们研究共置应用之间的性能干扰对其性能的影响。我们考虑了 SPEC CPU2006 工作负载混合 1，包括 436.cactusADM、437.leslie3d、459.GemsFDTD 和 470.lbm。表 1 比较了这四个基准应用在单独运行时与在共置虚拟机上同时运行时的平均完成时间。所有四个基准应用在没有性能隔离机制的情况下都表现出性能退化。例如，对于基准应用 436.cactusADM，性能退化约为 20%。对于基准应用 437.leslie3d，性能退化约为 20%。我们观察到平均性能下降了约 20%。

表 1：SPEC CPU2006 应用的性能目标
| 应用 | 436.cactusADM | 437.leslie3d | 459.GemsFDTD | 470.lbm |
|------|---------------|--------------|---------------|----------|
| 目标集 1 | 25% | 25% | 25% | 25% |
| 目标集 2 | 50% | 50% | 50% | 50% |
| 目标集 3 | 75% | 75% | 75% | 75% |
| 目标集 4 | 100% | 100% | 100% | 100% |
| 目标集 5 | 70% | 70% | 70% | 70% |

表 2：SPEC CPU2006 应用的性能（无性能隔离）
| 应用 | 单独运行 (ms) | 共置运行 (ms) |
|------|----------------|----------------|
| 436.cactusADM | 2433 | 2931 |
| 437.leslie3d | 1091 | 1586 |
| 459.GemsFDTD | 1190 | 1676 |
| 470.lbm | 825 | 1145 |

表 3：SPEC CPU2006 应用的性能（有性能隔离）
| 应用 | Q-Clouds (ms) | NINEPIN (ms) |
|------|---------------|---------------|
| 436.cactusADM | 2796.5 | 2482.65 |
| 437.leslie3d | 1435.52 | 1487.5 |
| 459.GemsFDTD | 993.97 | 1091 |
| 470.lbm | 1190 | 750 |

通过上述实验结果可以看出，NINEPIN 在性能隔离方面表现优于 Q-Clouds，能够更有效地减少共置应用之间的性能干扰。