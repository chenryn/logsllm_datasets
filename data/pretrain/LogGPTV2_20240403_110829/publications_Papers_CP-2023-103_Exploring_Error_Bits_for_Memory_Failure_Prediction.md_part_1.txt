Exploring Error Bits for Memory Failure
Prediction: An In-Depth Correlative Study
Qiao Yu∗†, Wengui Zhang‡, Jorge Cardoso∗¶, and Odej Kao†
∗Huawei Munich Research Center, Germany, {qiao.yu, jorge.cardoso}@huawei.com
†Technical University of Berlin, Germany, PI:EMAIL
‡Huawei Technologies Co., Ltd, China, PI:EMAIL
¶CISUC, University of Coimbra, Portugal
Abstract—Inlarge-scaledatacenters,memoryfailureisacom-
mon cause of server crashes, with Uncorrectable Errors (UEs) Others
beingamajorindicatorofDualInlineMemoryModule(DIMM)
13% 3202
defects. Existing approaches primarily focus on predicting UEs Memory Mainboard
using Correctable Errors (CEs), without fully considering the 37% 7%
information provided by error bits. However, error bit patterns
have a strong correlation with the occurrence of UEs. In this 11% CPU
paper, we present a comprehensive study on the correlation ceD
between CEs and UEs, specifically emphasizing the importance
of spatio-temporal error bit information. Our analysis reveals a 32%
strong correlation between spatio-temporal error bits and UE
occurrence. Through evaluations using real-world datasets, we Disk 81
demonstratethatourapproachsignificantlyimprovesprediction
Fig. 1: Distribution of Hardware Failures in Data Centers [7].
performanceby15%inF1-scorecomparedtothestate-of-the-art
algorithms.Overall,ourapproacheffectivelyreducesthenumber ]RA.sc[
ofvirtualmachineinterruptionscausedbyUEsbyapproximately failures, which forms the foundation of our work. Machine
59%.
Learning (ML)-based techniques have been leveraged for
Index Terms—Memory, Failure prediction, AIOps, Uncor-
DRAM failure prediction [14]–[21], using CEs information
rectable error, Reliability, Machine Learning
from a large-scale datacenter to predict UEs. These studies
I. INTRODUCTION have effectively utilized the spatial distribution of CEs to
enhance DRAM failure prediction. Moreover, system-level 2v55820.2132:viXra
With the increasing demand for cloud computing and big
workloadindicatorssuchasmemoryutilization,readandwrite
data storage services, hardware failures [1], [2] can signif-
have been applied for DRAM failure prediction in [22]–[24].
icantly impact the Reliability, Availability, and Serviceabil-
The experiments in [24] have demonstrated that the workload
ity (RAS)1 of servers. Among hardware failures, DRAM
metric is relatively less significant compared to other CE
(Dynamic Random Access Memory) failure is a major oc-
related features. In [25], CE storm (numerous CEs occurring
currence, accounting for 37% of total hardware failures in
in a short period) and UEs are considered for predicting
Figure 1. DRAM failure is often accompanied by DRAM
DRAM-caused node unavailability (DCNU), emphasizing the
errors, i.e, Correctable Error (CE) and Uncorrectable Error
importance of spatio-temporal CE features. Furthermore, in
(UE). To mitigate DRAM failures, Error Correction Code
[6], specific error bit patterns are discussed and correlated
(ECC) mechanisms such as SEC-DED [3], Chipkill [4] and
with DRAM UEs. Rule-based error bit pattern indicators
SDDC[5]areusedtodetectandcorrectdatacorruptionerrors.
are developed for DRAM failure prediction across different
For example, Chipkill ECC can correct any erroneous data
manufacturersandpartnumbers,aligningwiththeECCdesign
bits originating from a single DRAM chip. However, when
of contemporary Intel Skylake and Cascade Lake servers.
erroneous data bits span across two or more chips, the error
In addition, HiMFP framework [26] advocates a hierarchical
correction capability of Chipkill ECC becomes overwhelmed,
system-level approach to memory failure prediction, using
often resulting in a system crash due to a UE. Moreover,
error bits features. However, the intrinsic distributions of
the ECC on contemporary Intel platforms like Skylake and
error bits, specifically in Data pins (DQ) and beat, remain
Cascade Lake servers is less robust compared to Chipkill
unexploredinaboveliterature.Delvingintothesedistributions
ECC, making it vulnerable to certain error-bit patterns from
is crucial for understanding the correlation between CE and
a single chip [6]. Thus, soly depending on ECC for DRAM
UE.
reliability proves inadequate, with DRAM failures remaining
Inthispaper,wepresentanin-depthcorrelativeanalysisbe-
a significant cause of system failures.
tweenCEandUE,specificallyfocusingonthespatio-temporal
To improve memory reliability, several studies [8]–[13]
distribution of error bits. We also investigate latent patterns
have investigated the correlations between memory errors and
of error bits from CE to UE on the ECC of contemporary
Intel servers. Our primary goal of analysis is to enhance
1Reliability,Availability,andServiceabilityarethreekeyattributesassess-
ingthedependabilityofcomputersystems. memoryfailurepredictionbasedonvariousDRAMerrorsand
1
system configurations. Finally, machine learning models are arrays. Each bank is organized into rows and columns, and
implementedtoleveragespatio-temporalerrorbitsformemory each addressable unit indexed by rows and columns is a
failure prediction. memorycellcontaininga4-bitwordinthex4DRAMdevice.
The key contributions of the paper are as follow: Data flow in this architecture is transmitted from the cell to
• We analyze error bits patterns generated from DIMM memory controller, which can generally detect and correct
manufacturer and part number, and construct novel tem- CEsviachannels.Figure2(2)depictsthetransmissionprocess
poral risky CE indicators for UE prediction. of x4 DRAM Double Data Rate 4 (DDR4) chips via DQs.
• Weconductthefirstin-depthcorrelativeanalysisbetween Upon initiating a data request, 8 beats each with 72 bits (64
error bits and UE, specifically during DRAM read/write data bits and 8 ECC bits) including ECC error codes are
inDatapins(DQ)andbeat.Inaddition,micro-levelfaults transferred to memory controller via DQ wires. Implementing
in the memory subsystem and system configurations are the contemporary ECC [6], [27], 72-bit data are spread across
further correlated with UE occurrences. 18 DRAM chips, allowing the memory controller to detect
• WedesignML-basedfailurepredictionalgorithms,based and correct them with ECC in Figure 2(3). Note that ECC
on the statistical insights from our analyses. Through checking bits addresses are decoded to locate specific errors
evaluations using real-world data from a large-scale data inDQsandbeats.Then,alltheselogsincludingerrordetection
center,ourproposederrorbitsfeatureshavedemonstrated andcorrection,events,andmemoryspecificationsarearchived
the ability to capture latent patterns within the ECC in Baseboard Management Controller (BMC)2 in Figure 2(4).
of contemporary Intel servers, significantly improving Among previous works [6], [15]–[19], [25], [26], error bits in
UE prediction. When compared to the state-of-the-art a cell have not been extensively examined. In our work, we
algorithm [6], our approach achieves up to an 15% conduct the first in-depth correlative analysis between error
improvement in F1-score for UE prediction, resulting in bitsandUEinthefield,tounveilthelatentpatternsofmemory
approximately a 59% Virtual Machine Reduction Rate UEs.
(VIRR) in our data centers.
C. DRAM RAS Techniques
The remainder of this paper is organized as follows: Sec-
DRAM subsystems are typically protected by RAS fea-
tion II provides the background of our work. Section III
tures in Figure 2(6). Proactive early VM live migrations can
discussed the dataset employed in our data analysis. In Sec-
greatly reduce VM interruptions by moving VMs without
tion IV, we formulate the the problem and define the perfor-
service interruption. The CE storm suppressed mechanism
mance measurements. Section V introduces error bit pattern
helps avoid service degradation caused by CE storm3. Ad-
indicatorsforUEprediction.SectionVIpresentsancorrelative
vanced RAS techniques are designed to protect server-grade
study on UE. Section VII demonstrates machine learning
machines include the avoidance of fault regions. On the
techniquesformemoryfailureprediction.Experimentalresults
hardware technologies, sparing mechanisms are employed,
are shown in Section VIII. Section IX concludes this paper.
such as bit sparing (e.g., Partial Cache Line Sparing (PCLS)
II. BACKGROUND [28]), row/column sparing (e.g., Post Package Repair (PPR)
A. Terminology [29]), bank/chip sparing (e.g., Intel’s Adaptive Double Device
Data Correction (ADDDC) [30], [31]), etc. On the software-
AfaultservesastheunderlyingcauseofanerrorinDRAM,
sparing mechanisms, such as the page offlining in operating
and it can be caused by various factors such as particle
systems, can also be applied to avoid memory errors [30],
impacts, cosmic rays or defects.
[32], [33]. However, these techniques often require higher re-
An error refers to the situation in which a DIMM provides
dundancyandentailadditionaloverhead,whichcanpotentially
data to the memory controller that is inconsistent with the
impactsystemperformance.Hence,thesetechniquescannotbe
ECC [3]–[5], [27], resulting from an active fault. Depending
universally adaptable across all machines. Utilizing memory
on ECC’s capability to correct them, memory errors can
failure prediction allows for the prediction of UEs and the
be classified into correctable errors (CEs) and uncorrectable
activation of corresponding mitigation techniques based on
errors (UEs) [12]. Two specific types of UEs are well-studied
specific use cases.
in prior literature [14]. 1) sudden UE: UEs caused by some
componentfaultsthatinstantlycorruptdata,and2)predictable
III. DATASET
UE: UEs that initially manifest as correctable errors but
Our dataset was obtained from the Baseboard Management
eventually escalate into UEs. A sudden UE typically has no
Controller (BMC) of a large-scale datacenter, which includes
CEs before it occurs, while a predictable UE can be predicted
system configuration, Machine Check Exception (MCE) log
using CEs with failure prediction algorithms.
[34], and memory events. We focus on DIMMs with CEs,
B. Memory Organization and Access excluding those with sudden UEs from our datasets due to
Figure2illustratesaframeworkofmemorysubsystemorga-
2BMCisadelicatedprocessorintegratedintoserver’smotherboard,tasked
nization,memoryaccessandmemoryRAS.Thememorysys-
with monitoring the physical state of a computer, network server, or other
temishierarchicalinFigure2(1):ADIMMrankiscomposed
hardwaredevice.
of several DRAM chips that form banks of two-dimensional 3CEinterruptionsrepeatedlyoccurmultipletimes,e.g.,10times.
2
Memory Organization Memory Access Memory RAS
DIMM
Error checking Return mitigation status
Rank 0 2 bits
Rank 1 6 Memory Mitigation 3 1 0 0 1 0 0 0 0
DRAM Techniques 1 sQD 2 0 0 1 0 0 0 0 0
Chip 17 1 0 0 0 0 0 1 0 0 Memory Error
DRAM (Device) Column (ECC)
0 0 0 0 0 1 0 0 0 Log
3 ... ...
4 5 VM 3 1 0 0 1 0 0 0 0 Channel Collection
Live Migration
DRAM sQD 2 0 0 1 0 0 0 0 0