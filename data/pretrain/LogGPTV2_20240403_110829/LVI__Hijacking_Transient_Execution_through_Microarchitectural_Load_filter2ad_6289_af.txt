.
7
5
7
4
0
.
9
9
6
1
8
.
3
4
5
1
9
.
3
4
7
.
2
4
9
.
0
2
8
1
0
.
9
3
4
3
5
.
0
5
6
6
5
.
2
8
2
.
5
8
5
2
.
9
7
3
5
.
6
2
3
1
.
7
4
2
5
2
.
8
9
5
.
5
1
5
.
2
5
3
9
8
.
8
9
2
5
1
.
0
3
4
4
4
.
5
2
0
.
5
gcc-lfence
clang-full
clang-ret
own-full
own-ret
aes-128 cbc
rsa 4096 (sign)
rsa 4096 (verify)
ecdh nistp256
ecdsa nistp256 (sign) ecdsa nistp256 (verify)
ghash
sha256
sha512
Fig. 8. Performance overhead of our LLVM-based prototype (fence loads + ret vs. ret-only) and Intel’s mitigations for non-optimized assembler gcc (fence
loads + ret) and optimized clang (fence loads + indirect branch + ret vs. ret-only) for OpenSSL on an Intel i7-6700K CPU.
To deal with assembly source ﬁles, our tool introduces an
lfence after every mov operating on memory addresses. Our
prototype does not mitigate all types of indirect branches, but
can optionally replace ret instructions with the proposed
emulation code, where %r11 is used as a caller-save register
that can be clobbered.
To measure the performance impact of the introduced
lfence instructions and the ret emulation, we recorded
the average throughput (n = 10) of various cryptographic
primitives using OpenSSL’s speed tool on an isolated core
on an Intel i7-6700K. As shown in Figure 8, the performance
overhead reaches from a minimum of 0.91 % for a partial
mitigation which only rewrites ret instructions to a maximum
of 978.13 % for the full mitigation including ret emulation
and load serialization. Note that for real-world deployment,
the placement of lfence instructions should be evaluated
for completeness and more optimized than in our prototype
implementation. Still, our evaluation serves as an approximation
of the expected performance impact of the proposed mitigations.
b) Evaluation of Intel’s proposed mitigations: To further
evaluate the overheads of more mature, production-quality
implementations, we were provided with access to Intel’s
current compiler-based mitigation infrastructure. Hardening of
existing code bases is facilitated by a generic post-compilation
script that uses regular expressions to insert an lfence after
every x86 instruction that has a load micro-op. Working
exclusively at the assembly level, the script is inherently
compiler-agnostic and can hence only make use of indirect
branch emulation instruction sequences that do not clobber
registers. In general, it is therefore recommended to ﬁrst
decompose indirect branches from memory using existing
Spectre-BTB mitigations [60]. As not all code respects calling
conventions, ret instructions are by default replaced with a
clobber-free emulation sequence which ﬁrst tests the return
address, before serializing the processor pipeline and issuing
the ret (cf. Table II). We want to note that this emulation
sequence still allows privileged LVI adversaries to provoke a
fault or assist on the return address when leveraging a single-
stepping framework like SGX-Step [63] to precisely interrupt
and resume the victim enclave after the lfence and before the
ﬁnal ret. However, we expect that in such a case the length of
the transient window would be severely restricted as eresume
appears to be a serializing instruction itself [32]. Furthermore,
as recent microcode ﬂushes microarchitectural buffers on
enclave entry, the poisoning phase would be limited to LVI-
NULL. Any inadvertent transient control-ﬂow redirections to
]
%
[
d
a
e
h
r
e
v
O
1,500
1,000
500
0
6
2
.
1
8
0
1
3
1
.
0
3
8
8
5
.
4
0
4
6
1
.
0
8
1
3
.
4
6
2
6
.
1
8
2
5
5
.
7
0
2
gcc-lfence
clang-full
clang-ret
5
0
.
1
6
2
3
2
.
6
8
5
1
.
7
6
3
2
8
.
5
1
2
6
0
.
5
7
2
9
5
8
7
.
8
8
1
9
9
.
3
2
3
.
1
6
6
9
0
.
7
6
1
8
.
0
3
4
2
.
2
0
5
3
8
.
3
7
6
1
7
.
9
8
1
6
6
.
2
8
4
.
0
8
3
9
3
.
0
3
2
9
.
6
7
1
1
.
4
8
2
5
.
2
h
c
n
e
r l b
e
c
c
2 . g
0
6
0 . p
0
6
f
5 . m c
0
6
0 . o m n
2
6
e t p
6
p
2
c
n
a l a
3 . x
b m k
4
6
2
5 . x
2
6
1 . d
3
6
g
n
s j e
p
e
e
e l a
1 .l e
4
6
z
7 . x
5
6
Fig. 9. Performance overhead of Intel’s mitigations for non-optimized