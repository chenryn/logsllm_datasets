timal method. The dips in traﬃc occur during updates be-
cause we ask services whose new allocations are lower to
reduce their rates, wait for Th=10 seconds, and then ask
services with higher allocations to increase their rate. The
impact of these dips is low in practice when there are more
ﬂows and the update frequency is 5 minutes (§6.6).
Congestion-controlled updates: Figure 11a zooms in
on an example update. A new epoch starts at zero and the
throughput of each class is shown relative to its maximal
allocation before and after the update. We see that with
SWAN there is no adverse impact on the throughput in any
class when the forwarding plane update is executed at t=10s.
To contrast, Figure 11b shows what happens without
congestion-controlled updates. Here, as in SWAN, 10% of
scratch capacity is kept with respect to non-background traf-
ﬁc, but all update commands are issued to switches in one
step. We see that Elastic and Background classes suﬀer
transient throughput degradation due to congestion induced
losses followed by TCP backoﬀs. Interactive traﬃc is pro-
tected due to priority queuing in this example but that does
not hold for updates that move a lot of interactive traﬃc
across paths. During updates, the throughput degradation
across all traﬃc in a class is 20%, but as Figure 11c shows,
it is as high as 40% for some of the ﬂows.
6. DATA-DRIVEN EVALUATION
To evaluate SWAN at scale, we conduct data-driven sim-
ulations with topologies and traﬃc from two production
inter-DC WANs of large cloud service providers (§6.1). We
show that SWAN can carry 60% more traﬃc than MPLS
TE (§6.2) and is fairer than MPLS TE (§6.3). We also show
Arista Cisco N3K Blade Server Datacenters Aggregate cable bundles Hong Kong New York Florida Barcelona Los Angeles WAN-facing switch Border router Data Center Server … … Data Center 5 5  0 0.05 0.1 0.15 0 1 2 3Demand perDC-pair[norm. tolink capacity]Time [minute] 0 0.1 0.2 0.3 0.4 0.5 0 1 2 3Time [minute] 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6Stacked goodput[norm. to max]Time [minutes]InteractiveElasticElasticBackgroundOptimal goodput.5.751 0 10 20Interactive.5.751 0 10 20Elastic.5.751 0 10 20Throughput [norm.to maximum]Time [second]Background.5.751 0 10 20Interactive.5.751 0 10 20Elastic.5.751 0 10 20Throughput [norm.to maximum]Time [second]Background 0 0.2 0.4 0.6 0.8 10%20%40%Cumulative fractionof DC-pair ﬂowsThroughput lossElasticBkgd22that SWAN enables congestion-controlled updates (§6.4) us-
ing bounded switch state (§6.5).
6.1 Datasets and methodology
We consider two inter-DC WANs:
IDN: A large, well-connected inter-DC WAN with more
than 40 DCs. We have accurate topology, capacity, and
traﬃc information for this network. Each DC is connected to
2-16 other DCs, and inter-DC capacities range from tens of
Gbps to Tbps. Major DCs have more neighbors and higher
capacity connectivity. Each DC has two WAN routers for
fault tolerance, and each router connects to both routers
in the neighboring DC. We obtain ﬂow-level traﬃc on this
network using sFlow logs collected by routers.
G-Scale: Google’s inter-DC WAN with 12 DCs and 19
inter-DC links [15]. We do not have traﬃc and capacity in-
formation for it. We simulate traﬃc on this network using
logs from another production inter-DC WAN (diﬀerent from
IDN) with a similar number of DCs. In particular, we ran-
domly map nodes from this other network to G-Scale. This
mapping retains the burstiness and skew of inter-DC traﬃc,
but not any spatial relationships between the nodes.
We estimate capacity based on the gravity model [30]. Re-
ﬂecting common provisioning practices, we also round capac-
ity up to the nearest multiple of 80 Gbps. We obtained qual-
itatively similar results (omitted from the paper) with three
other capacity assignment methods: i) capacity is based on
5-minute peak usage across a week when the traﬃc is carried
over shortest paths using ECMP (we cannot use MPLS TE
as that requires capacity information); ii) capacity between
each pair of DCs is 320 Gbps; iii) capacity between a pair
of DCs is 320 or 160 Gbps with equal probability.
With the help of network operators, we classify traﬃc into
individual services and map each service to Interactive, Elas-
tic, or Background class.
We conduct experiments using a ﬂow-level simulator that
implements a complete version of SWAN. The demand of the
services is derived based on the traﬃc information from a
week-long network log. If the full demand of a service is not
allocated in an interval, it carries over to the next interval.
We place the SWAN controller at a central DC and simulate
control plane latency between the controller and entities in
other DCs (service brokers, network agents). This latency
is based on shortest paths, where the latency of each hop is
based on speed of light in ﬁber and great circle distance.
6.2 Network utilization
To evaluate how well SWAN utilizes the network, we com-
pare it to an optimal method that can oﬀer 100% utiliza-
tion. This method computes how much traﬃc can be car-
ried in each 5-min interval by solving a multi-class, multi-
commodity ﬂow problem. It is restricted only by link capac-
ities, not by rule count limits. The changes to service rates
are instantaneous, and rate limiting and interactive traﬃc
prediction is perfect.
We also compare SWAN to the current practice, MPLS
TE (§2). Our MPLS TE implementation has the advanced
features that IDN uses [4, 24]. Priorities for packets and tun-
nels protect higher-priority packets and ensure shorter paths
for higher-priority services. Per re-optimization, CSPF is
invoked periodically (5 minutes) to search for better path
assignments. Per auto-bandwidth, tunnel bandwidth is pe-
riodically (5 minutes) adjusted based on the current traﬃc
(a) IDN
(b) G-Scale
Figure 12: SWAN carries more traﬃc than MPLS TE.
demand, estimated by the maximum of the average (across
5-minute intervals) demand in the past 15 minutes.
Figure 12 shows the traﬃc that diﬀerent methods can
carry compared to the optimal. To quantify the traﬃc that
a method can carry, we scale service demands by the same
factor and use binary search to derive the maximum ad-
missible traﬃc. We deﬁne admissibility as carrying at least
99.9% of service demands. Using a threshold less than 100%
makes results robust to demand spikes.
To decouple gains of SWAN from its
We see that MPLS TE carries only around 60% of the op-
timal amount of traﬃc. SWAN, on the other hand, can carry
98% for both WANs. This diﬀerence means that SWAN car-
ries over 60% more traﬃc that MPLS TE, which is a signif-
icant gain in the value extracted from the inter-DC WAN.
two main
components—coordination across services and global net-
work conﬁguration—we also simulated a variant of SWAN
where the former is absent. Here, instead of getting demand
requests from services, we estimate it from their throughput
in a manner similar to MPLS TE. We also do not control
the rate at which services send. Figure 12 shows that this
variant of SWAN improves utilization by 10–12% over MPLS
TE, i.e., it carries 15–20% more traﬃc. Even this level of
increase in eﬃciency translates to savings of millions of dol-
lars in the cost of carrying wide-area traﬃc. By studying a
(hypothetical) version of MPLS that perfectly knows future
traﬃc demand (instead of estimating it based on history),
we ﬁnd that most of SWAN’s gain over MPLS stems from
its ability to ﬁnd better path assignments.
We draw two conclusions from this result. First, both
components of SWAN are needed to fully achieve its gains.
Second, even in networks where incoming traﬃc cannot be
controlled (e.g., ISP network), worthwhile utilization im-
provements can be obtained through the centralized resource
allocation oﬀered by SWAN.
In the rest of the paper, we present results only for IDN.
The results for G-Scale are qualitatively similar and are de-
ferred to [14] due to space constraints.
6.3 Fairness
SWAN improves not only eﬃciency but also fairness. To
study fairness, we scale demands such that background traf-
ﬁc is 50% higher than what a mechanism admits; fairness is
of interest only when traﬃc demands cannot be fully met.
Scaling relative to traﬃc admitted by a mechanism ensures
that oversubscription level is the same. If we used an identi-
cal demand for SWAN and MPLS TE, the oversubscription
for MPLS TE would be higher as it carries less traﬃc.
For an exemplary 5-minute window, Figure 13a shows the
throughput that individual ﬂows get relative to their max-
min fair share. We focus on background traﬃc as the higher
priority for other traﬃc means that its demands are often
 0 0.2 0.4 0.6 0.8 1Throughput [nor-malnized to optimal]SWANSWAN w/o RateControlMPLSTE99.0%70.3%58.3%SWANSWAN w/o RateControlMPLSTE98.1%75.5%65.4%23(a)
(b)
Figure 13: SWAN is fairer than MPLS TE.
Figure 14: Number of stages and loss in network
throughput as a function of scratch capacity.
met. We compute max-min fair shares using a precise but
computationally-complex method (which is unsuitable for
online use) [26]. We see that SWAN well approximates max-
min fair sharing. In contrast, the greedy, local allocation of
MPLS TE is signiﬁcantly unfair.
Figure 13b shows aggregated results. In SWAN, only 4%
of the ﬂows deviate over 5% from their fair share. In MPLS
TE, 20% of the ﬂows deviate by that much, and the worst-
case deviation is much higher. As Figure 13a shows, the
ﬂows that deviate are not necessarily high- or low-demand,
but are spread across the board.
6.4 Congestion-controlled updates
We now study congestion-controlled updates, ﬁrst the
tradeoﬀ regarding the amount of scratch capacity and then
their beneﬁt. Higher levels of scratch capacity lead to
fewer stages, and thus faster transitions; but they lower the
amount of non-background traﬃc that the network can carry
and can waste capacity if background traﬃc demand is low.
Figure 14 shows this tradeoﬀ in practice. The left graph
plots the maximum number of stages and loss in network
throughput as a function of scratch capacity. At the s=0%
extreme, throughput loss is zero but more stages—inﬁnitely
many in the worst case—are needed to transition safely. At
the s=50% extreme, only one stage is needed, but the net-
work delivers 25−36% less traﬃc. The right graph shows the
PDF of the number of stages for three values of s. Based on
these results, we use s=10%, where the throughput loss is
negligible and updates need only 1-3 steps (which is much
lower than the theoretical worst case of 9).
To evaluate the beneﬁt of congestion-controlled updates,
we compare with a method that applies updates in one shot.
This method is identical in every other way, including the
amount of scratch capacity left on links. Both methods send
updates in a step to the switches in parallel. Each switch ap-
plies its updates sequentially and takes 2 ms per update [8].
For each method, during each reconﬁguration, we com-
pute the maximum over-subscription (i.e., load relative to
capacity), at each link. Short-lived oversubscription will be
absorbed by switch queues. Hence, we also compute the
Figure 15: Link oversubscription during updates.
Figure 16: SWAN needs fewer rules to fully exploit net-
work capacity (left). The number of stages needed for
rule changes is small (right).
maximal buﬀering required at each link for it to not drop
any packet, i.e., total excess bytes that arrive during over-
subscribed periods. If this number is higher than the size
of the physical queue, packets will be dropped. Per pri-
ority queuing, we compute oversubscription separately for
each traﬃc class; the computation for non-background traf-
ﬁc ignores background traﬃc but that for background traﬃc
considers all traﬃc.
Figure 15 shows oversubscription ratios on the left. We
see heavy oversubscription with one-shot updates, especially
for background traﬃc. Links can be oversubscribed by up
to 60% of their capacity. The right graph plots extra bytes
on the links. Today’s top-of-line switches, which we use
in our testbed, have queue sizes of 9-16 MB. But we see
that oversubscription can bring 100s of MB of excess pack-
ets and hence, most of these will be dropped. Note that we
did not model TCP backoﬀs which would reduce the load
on a link after packet loss starts happening, but regardless,
those ﬂows would see signiﬁcant slowdown. With SWAN,
the worst-case oversubscription is only 11% (= s
1−s ) as con-
ﬁgured for bounded-congestion updates, which presents a
signiﬁcantly better experience for background traﬃc.
We also see that despite 10% slack, one-shot updates fail
to protect even the non-background traﬃc which is sensitive
to loss and delay. Oversubscription can be up to 20%, which
can bring over 50 MB of extra bytes during reconﬁgurations.
SWAN fully protects non-background traﬃc and hence that
curve is omitted.
Since routes are updated very frequently even a small like-
lihood of severe packet loss due to updates can lead to fre-
quent user-visible network incidents. For e.g., when updates
1
1000 likelihood of severe packet loss
happen every minute, a
due to route updates leads to an interruption, on average,
once every 7 minutes on the IDN network.
6.5 Rule management
We now study rule management in SWAN. A primary mea-
sure of interest here is the amount of network capacity that
can be used given a switch rule count limit. Figure 16 (left)
shows this measure for SWAN and an alternative that in-
stalls rules for the k-shortest paths between DC-pairs; k is
chosen such that the rule count limit is not violated for any
switch. We see that k-shortest path routing requires 20K
rules to fully use network capacity. As mentioned before,
this requirement is beyond what will be oﬀered by next-
 0 1 2 3 4 0 100 200 300 400 500 600 0 1 2 3 4 0 100 200 300 400 500 600Flow ID [in the increasing order of ﬂow demand]SWANMPLS TEFlow throughput[normalized to max-min fair rate] 0 0.1 0.2 0.3 0.01 0.1 1 10ComplementaryCDF over ﬂows & timeRelative error [throughput vs. max-min fair rate]SWANMPLS TE 1 2 3 4 5 6010%20%30%40%50% 0 0.1 0.2 0.3# of stagesThroughput lossCapacity slack# stages (99th-pctl.)Throughput loss 0.001 0.01 0.1 1123456FrequencyNumber of stagess=0%s=10%s=30%0.01%0.1%1%10% 0 0.2 0.4 0.6ComplemantaryCDF overlinks & updatesOversubscription ratio 0 100 200 300Overloaded traﬃc [MB]one-shot;bgSWAN;bgone-shot;non-bg 0.9 0.95 10.125K0.5K2K8K32KThroughput[norm. to optimal]Switch memory [#Entries in log4]k-shortestpathsSWANλ=1%10%50% 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5Cumulative frac.of updates#Stagesλ=1%λ=10%λ=50%24Figure 17: Time for network update.
(a)
(b)
Figure 18: (a) SWAN carries close to optimal traﬃc even
during updates.
(b) Frequent updates lead to higher
throughput.
generation switches. The natural progression towards faster