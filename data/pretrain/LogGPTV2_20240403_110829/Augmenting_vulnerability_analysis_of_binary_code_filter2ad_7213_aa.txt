title:Augmenting vulnerability analysis of binary code
author:Sean Heelan and
Agustin Gianni
Augmenting Vulnerability Analysis of Binary Code
Sean Heelan
PI:EMAIL ∗
Agustin Gianni
PI:EMAIL ∗
ABSTRACT
Discovering and understanding security vulnerabilities in com-
plex, binary code can be a diﬃcult and time consuming
problem. While there has been notable progress in the de-
velopment of automatic solutions for vulnerability detection,
manual analysis remains a necessary component of any bi-
nary auditing task. In this paper we present an approach
based on run time data tracking that works to narrow down
the attack surface of an application and prioritize code re-
gions for manual analysis. By supporting arbitrary data
sources and sinks we can track the spread of direct and in-
direct attacker inﬂuence throughout a program. Alerts are
generated once this inﬂuence reaches potentially sensitive
code and the results are post-processed, prioritized, and in-
tegrated into common reverse engineering tools. The data
recorded is used to inform the decisions of users, rather than
replace them. By avoiding the processing required for se-
mantic analysis and automated reasoning our approach is
suﬃciently fast to integrate into the normal work ﬂow of
manual vulnerability detection.
1. MOTIVATION
Much of the commodity, commercial and industrial soft-
ware in use today is only available in binary form. As a
result, analysis of this software requires reverse engineer-
ing — a process that is primarily manual and usually labor
and time intensive. Reverse engineering is a necessary step
in vulnerability detection, malware analysis, crash triaging,
exploit development, protocol recovery, interoperability en-
gineering and other processes. Although the end goal diﬀers,
these tasks typically involve solving similar problems during
the process of understanding a program in binary form. Due
to the time and labor cost of reverse engineering, algorithms
and techniques for assisting in the process are valuable.
In this paper we will focus on the goal of vulnerability
detection, an area that has seen signiﬁcant research interest
∗Parts of this work were completed while the author was an
employee of Immunity Inc.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’12 Dec. 3-7, 2012, Orlando, Florida USA
Copyright 2012 ACM 978-1-4503-1312-4/12/12 ...$15.00.
over the past few years in response to the need for greater
machine assistance. Progress has been made in the testing
of ﬁle parsers [11, 12, 19], and system tools [7, 6] through
techniques based on symbolic and concolic execution. How-
ever, many vulnerability types, software architectures and
programs above a certain size are currently not handled by
automatic methods. As such, a reverse engineer is required
to manually assess the software. When one considers the
task of assisting the reverse engineer in this process many
new opportunities and research problems arise.
2.
INTRODUCTION
Over the past decade there has been extensive work in de-
veloping tools and theory based on formal program analysis
with the goal of analyzing binary code. Among the most suc-
cessful outcomes of this research have been those approaches
based on symbolic/concolic execution. Combining binary in-
strumentation, software emulators and SMT solvers, auto-
mated solutions have been proposed for bug ﬁnding [11, 12],
exploit generation [4, 1, 13], protocol reverse engineering [5],
driver reconstruction [8], type recovery [21] and a variety of
other tasks. These eﬀorts have also resulted in the avail-
ability of several frameworks for binary analysis, including
TEMU [22], BAP [3] and S2E [9].
The problems these tools tackle are faced daily by profes-
sionals involved in reverse engineering, vulnerability detec-
tion and exploit development. However, in general the solu-
tions presented have not permeated into this industry. One
possible reason is that the tools often do not integrate well
with the work ﬂow of the reverse engineer. Reverse engineer-
ing is generally a process of iterative, gradual understanding
through forming hypotheses and looking for supporting evi-
dence. Tools that have excessively long running times or do
not support bidirectional information ﬂow between the user
and the tool are incompatible with such a process.
For example, frameworks that rely on full semantic emu-
lation of instructions tend to have running times measured
in hours for complex software like web browsers, document
viewers and network servers. While such information may be
necessary if one wants to automatically determine whether
a program trace potentially contains a bug, we will later
show how a more lightweight approach can provide a user
with the required information to make this determination in
a much smaller amount of time. As a result, more code can
be covered while at the same time gaining an insight into
the program that is not available if one relies on entirely
automatic methods.
Another problem for many tools is that they are designed
199
to be entirely automatic solutions and as a result often lack
the ﬂexibility to deal with corner cases not envisaged by
their designers. While reverse engineering involves many
repeated patterns of work there are inevitably corner cases
and complications in the analysis of every application.
Automatic approaches to bug ﬁnding also tend to be quite
limited in the types of properties they can reason about,
and as a result the types of bugs they can ﬁnd. Tools
such as KLEE [6] and EXE [7] can easily model and rea-
son about arithmetic constraints and thus are targeted at
bugs directly resulting from integer overﬂows, underﬂows
and incorrect bounds. On the other hand they know noth-
ing about dangling pointers, race conditions, type confusion
and other common bug classes.
While full automation is desirable in many situations, in
this paper we discuss an approach that is user centric. In-
stead of aiming to replace a user, we present a combination
of dynamic taint analysis and lightweight static analysis to
inform their decisions and increase eﬃciency during manual
program analysis. Our algorithms focus on data gathering,
analysis and display while leaving the speciﬁcs of problem
solving up to the user.
We will direct the system towards the problem of manual
vulnerability discovery but the information generated can
also be used to seed more automated systems. A common
problem faced by automated systems is in deciding what
parts of the program to explore and how to explore them.
The output we generate can be used to guide these decisions
and target testing tools towards more interesting parts of the
attack surface. As an example, similar technology has been
successfully applied to provide guidance during fuzzing [10].
In section 3 we describe a system with support for direct
and indirect data tainting. The system supports arbitrary
data origins and sinks and can generate alerts based on func-
tion and instruction level information. These alerts focus the
user on potentially sensitive areas of the code that are inﬂu-
enced by attacker controlled input. The primary goal of this
tool is to assist in attack surface discovery and to prioritize
regions of the code for assessment. Later in section 4 an
evaluation of this tool on several real world applications.
Beneﬁts of this approach include:
Generality Dynamic data-ﬂow tracking is a well studied
area [20] and usually works regardless of the software
being analyzed. Instead of attempting to perform au-
tomated reasoning on top of this we instead focus on
prioritizing the results of this analysis for human as-
sessment. This allows the user to target the tool at
software not generally handled by current automatic
approaches such as network servers, web browsers and
interpreters. It also assists the user when looking for
bug classes that are not well handled by approaches
based on symbolic execution e.g. use-after-free bugs.
Eﬃciency A key component of our approach is post-processing
of the recorded data to allow the user to ﬁnd the events
they may be most interested in quickly. We take two
approaches to this. The ﬁrst is to use lightweight static
analysis algorithms and a ranking function to priori-
tize particular results. The second is to extract rele-
vant properties from recorded events and present them
to the user in order to inform their decision making.
Speed We perform minimal analysis at run-time and so the
overhead on the instrumented program is low. The
struct TaintInformation {
size_t source_pc;
}
struct DirectTI : public TaintInformation {
Descriptor
DescriptorInfo
Offset
d;
di;
o;
}
struct IndirectTI : public TaintInformation {
vector  parents;
}
struct CompoundTI : public TaintInformation {
vector components;
}
struct TraceTI : public TaintInformation {
BigInt sequence_num;
vector parents;
}
struct FlagUpdateTI : public TaintInformation {
vector parents;
}
Figure 1: Pseudo-C++ Taint Information Deﬁni-
tions. When we refer to instances of the TaintInfor-
mation class we include instances of all subclasses.
time taken for post-processing of data is usually a few
seconds. This is important because it allows the user to
iteratively run the tool, view results, modify the con-
ﬁguration and re-run the tool without excessive wait
times.
3. SYSTEM DESCRIPTION
PINNACLE consists of three high level components — a data
tracking tool (DT) that monitors data as as it is processed by
an application and generates alerts on events of note; a col-
lection of static analysis scripts (SA) that process these alerts
and the user interface (UI) that is integrated with IDA [14].
3.1 Data Tracking and Alerts
DT is implemented as a shared library on top of the PIN
binary instrumentation framework [17]. We deﬁne M as the
set of valid program memory addresses and R as the set of
valid CPU register identiﬁers. For simplicity, in the remain-
der of this paper we will refer to the set of identiﬁers given
by M ∪ R as I.
For each ﬂag of the CPU’s ﬂags register we deﬁne a sepa-
rate identiﬁer in the set R, e.g. CF to identify the carry ﬂag.
We also deﬁne a separate identiﬁer for each 8, 16 or 32 bit
subregister. As an example, the various components of the
RAX register can be referenced through RAX, EAX, AX, AH and
AL.
A shadow data store S is maintained to keep metadata
about each register or byte of program memory. We can
consider S to work as a standard map from elements of I to
instances of TaintInformation. Subclasses of the TaintIn-
formation class can diﬀer in terms of their attributes de-
pending on the source of that taint information and the taint
propagation mechanism in use. The details of this class will
200
be elaborated on in section 3.1.1, while its deﬁnition is pro-
vided in Figure 1.
On starting a program, or attaching to one, the following
holds:
∀x ∈ I : S[x] = N U LL
A memory location or register x ∈ I is deﬁned as tainted if
S[x] (cid:6)= N U LL.
DT is concerned with introducing tainted data, propagat-
ing it across memory locations and registers, and generating
alerts when an instruction in combination with the metadata
in S matches against a deﬁned set of rules.
3.1.1 Introducing Tainted Data
DT uses the instrumentation mechanisms provided by PIN
to update the shadow data store S. PIN allows one to in-
strument at varying levels of granularity but, for the pur-
poses of introducing new tainted data, we instrument at the
function level. With compiled code we need not limit our-
selves to higher level function boundaries but can instead
consider any arbitrary chunk of assembly code to be a func-
tion. However, practically speaking we will choose code that
corresponds to a traditional higher level function as this is
usually the level of granularity we require when considering
the introduction of tainted data.
For any function f that we consider to introduce tainted
data we deﬁne a pair of functions (fpre, fpost) that work to-
gether to update S with new taint information. fpre is run
before the execution of f and can access the parameters of
f . fpost is run after the execution of f and can access its
return value and any other outputs. For each location x ∈ I
that f updates with attacker controlled data, (fpre, fpost)
will create a new metadata instance d that is a subclass of
TaintInformation and update S via S[x] = d. This indi-
cates that the memory location or register identiﬁed by x is
tainted and metadata is provided by d.
During vulnerability discovery the reason for marking a
location as tainted is typically to identify it as being inﬂu-
enced by an attacker and then to determine how that inﬂu-
ence may eﬀect the rest of the program’s control and data
ﬂow. Inﬂuence is a matter of degrees however [18] and the
level of control one may have over a value in memory or
registers can vary. In DT we reﬂect this fact by allowing for
data to be marked as either directly or indirectly tainted.
For each d ∈ (d0, . . . , dn) created by (fpre, fpost) a subclass
of TaintInformation must be selected. Two subclasses are
available, DirectTI and IndirectTI, which can be seen in
Figure 1. We will say that a location x ∈ I is directly tainted
if S[x] is an instance of DirectTI and indirectly tainted if
S[x] is an instance of IndirectTI.
The function pair (fpre, fpost) are manually created so a
developer/user of DT can use their own judgement as to the
type used for each d ∈ (d0, . . . , dn). As we do not perform
any automatic input generation based on the taint infor-
mation the correctness of the chosen type is not critical.
However, the type of inﬂuence an attacker has is used in
prioritizing results and thus it does have some importance.
DirectTI is used when we consider the data stored at
the register or memory location identiﬁed by x ∈ I to be
directly controlled by the attacker. For example, when a
value is read into a program unconstrained and unchanged
from an attacker controlled source, such as a socket or ﬁle
descriptor, we create a DirectTI instance d for each written
(w0, b0) ← {(r0, b0), (r1, b0)}