discussed before, after a certain τ value, adding more rules causes deterioration of TPs and FPs. This is 
because if too many inaccurate rules with higher error rates are added to the set of extracted rules, they 
could lead to misclassifications. In addition, the possibility that files match conflicting rules increases and 
the classifier rejects these files. So even though we can label more truly unknown files with more rules, 
the  final  classification  of  these  files  might  not  be  very  accurate.  Because  τ=0.1%  produced  the  best 
performance on the test dataset, we use the same setting for classifying the unknown files.
As mentioned before, this is one of the advantages of our rule-based classification system over regular 
decision trees, as the whole decision tree, which contains some less accurate branches, does not need 
to be used. Overall, from February to August, the system classified 406,688 previously unknown files as 
either benign or malicious. This number accounts for 28.30% of total unknown files observed during this 
period.
36 | Exploring the Long Tail of (Malicious) Software Downloads
Ttr - Tts
τ
Test dataset (extracted during Tts)
# 
malicious
TP
# 
benign
FP
# FP 
rules
Jan - Feb
Feb - Mar
Mar - Apr
Apr - May
May - Jun
Jun - Jul
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
3,590
96.72%
1,401
0.07%
3,647
96.45%
2,718
0.00%
3,045
97.59%
2,051
0.39%
3,070
97.60%
2,830
0.32%
4,793
97.98%
1,367
0.37%
4,842
99.61%
2,315
0.30%
3,001
92.01%
1,873
0.05%
7,203
96.96%
2,267
0.13%
3,834
90.53%
2,038
0.15%
7,895
96.64%
2,597
0.12%
7,200
95.39%
2,414
0.25%
7,202
95.28%
2,837
0.18%
1
0
8
9
6
8
1
2
4
4
7
6
unknowns
292,793
301,715
242,810
197,526
191,574
177,255
Unknowns dataset (extracted during Tts)
# 
# 
matches
# 
malicious
benign
68,200
68,368
2,312
2,312
68,165
20,005
68,165
20,005
51,096
51,504
2,470
2,467
46,651
26,266
49,014
26,108
40,600
20,794
43,175
22,846
35,530
18,906
35,693
20,207
24.08%
24.14%
29.22%
29.22%
22.06%
22.23%
36.92%
38.03%
32.05%
34.46%
30.71%
31.54%
Table 17. Evaluation results and classification of unknown files using rule-based classifier 
(conflicts are handled by rejecting the test and unknown files)
37 | Exploring the Long Tail of (Malicious) Software Downloads
7. Discussion
As  mentioned  earlier,  our  rule-based  classification  system  has  a  couple  of  advantages:  the  rules  are 
human-readable and they can easily be reviewed by an analyst. In the following, we report a few example 
rules that led to the most true positives, as well as other rules that sometimes caused misclassifications. 
Below, we list three sample rules that are responsible for correctly labeling many malicious downloads:
1. 
2. 
IF (file’s signer is “SecureInstall”) → file is malicious.
IF (file’s signer is “Apps Installer S.L.”) AND (downloading process’s signer is “Microsoft Windows”) 
AND (file’s CA is “thawte code signing ca - g2”) → file is malicious.
3. 
IF (file is not signed) AND (downloading process is “Acrobat Reader”) → file is malicious.
The rules mentioned above follow our reported measurement results. For example, Table 10 showed that 
malware files are downloaded by benign Windows processes. It also reported that files downloaded by 
Acrobat Reader are malware.
Rules that produce some false positives include the following:
1. 
2. 
3. 
IF (file’s signer is “mail.ru games”) → file is malicious. 
IF (file is not signed) AND (downloading process’s signer is “Amonetize ltd.”) AND (file’s packer is 
“NSIS”) → file is malicious. 
IF (file is not signed) AND (Alexa rank of file’s URL is between 10,000 to 100,000) AND (downloading 
process is benign) AND (file’s packer is “aspack”) → file is malicious. 
It should be noted that some of the classifications that we count as false positives may actually be due to 
the presence of noise in our ground truth. For instance, let us consider rule (2) above. “Amonetize ltd” is 
related to a family of adware and PUP software. Therefore, executable files downloaded from a process 
signed by “Amonetize ltd” may, in fact, be malicious.
38 | Exploring the Long Tail of (Malicious) Software Downloads
Additionally, 33% of benign (according to our ground truth) test samples were downloaded by malware 
processes or from malicious URLs. Therefore, these may be false positives due to noise in the whitelist. 
Overall,  these  observations  indicate  that  it  is  possible  that  the  false  positives  we  obtained  may  be 
somewhat overestimated.
Our evaluation results indicate that signers of downloaded files play an important role in our rule-based 
classifier. In fact, the file signer feature appeared in 75% of all rules. The other three most useful features, in 
order, are the file’s packer, downloading process type, and downloading process’s signer, which appeared 
in 8%, 5%, and 4% of all rules. Another interesting observation is that our classifier does not heavily rely 
on the feature related to the Alexa rank of the domains, as it appeared in 1.4% of the rules. This is in 
accordance with our previous measurement analysis that showed many benign file hosting websites tend 
to host malicious files alongside benign files. We also noticed that simple rules containing one feature 
were less error-prone and composed 89% of the rules, for τ=0.1%.
7.1. Analysis of Test Dataset Results
Among the correctly classified malicious test samples, 45% of files were droppers, 38% were trojans, and 
3.5% were bankers. The remaining samples were divided among other malicious file types. The following 
sample rules were the most successful in detecting different types of malware:
•  bankers and bots: IF (downloading process is “Acrobat Reader”) → file is malicious. 
•  droppers: IF (file’s signer is “Somoto ltd.”) → file is malicious.
• 
fakeavs: IF (file is not signed) AND (Alexa rank of file’s URL is above 100K) AND (downloading process 
is benign) AND (downloading process’s signer is “Microsoft Windows”) → file is malicious.
7.2. Expanding Available Ground Truth by 
Labeling Unknown Files
As mentioned earlier, the set of rules we learned were able to label 28.30% of all 1,436,829 previously 
unknown files from February to August, which represents a 233% increase in labeled files, compared 
to the available ground truth. These 28.30% of unknown files were downloaded by as many as 294,419 
machines, or 31% of all machines, therefore having a significant penetration across the machine population 
(notice that the overall number of machines that downloaded any of the 1,436,829 unknown files between 
February and August is 457,756).
39 | Exploring the Long Tail of (Malicious) Software Downloads
These  results  indicate  that  our  rule-based  classification  method  would  enable  a  significant  expansion 
of the labeling of software files, compared to the ground truth available from multiple anti-virus sources. 
Ultimately,  this  would  allow  researchers  to  evaluate  the  accuracy  of  their  malware  detection  systems 
over a much larger labeled dataset, including challenging cases of low-prevalence malicious files that in 
aggregate tend to impact a large population of machines.
7.3. Evading Detection
Evasion  is  certainly  possible  for  most  statistical  detection  models.  Malware  developers  could  change 
signer  information  by  acquiring  new  signing  certificates.  However,  valid  certificates  are  not  cheap. 
Therefore, it would be expensive to create polymorphic malware variants with unique signatures. Also, 
stealing  a  benign  certificate  is  possible  (though  not  easy);  however,  once  the  true  certificate  owners 
detect this, they could revoke the certificate. Using “benign" packers would make it easier to unpack and 
analyze the code. Therefore, malware often uses custom/hard-to-reverse packers. Thus, even though it is 
technically possible to evade our system, it won't be very practical.
40 | Exploring the Long Tail of (Malicious) Software Downloads
8. Related Work
In this work, we focus on a specific class of software downloads that we believe to have been neglected 
in  the  past,  namely  low-prevalence  downloads.  With  respect  to  previous  work  investigating  malicious 
software downloads, we report the following. Rossow et al. [17] analyzed a limited number of about twenty 
dropper families for aspects such as their network infrastructure, infection, propagation and persistence 
on infected machines. 
More recently, Kwon et al. [10] extended this research by looking into the download chains that occur after 
infection. In comparison, we provided a comprehensive breakdown of different types of malware besides 
droppers  and  analyzed  their  characteristics  from  various  aspects,  namely  their  signers,  downloading 
URLs, transitions from one type to another, etc. More importantly, [10] does not discuss the evaluation 
of their classifier for files for which no ground truth is available whatsoever although these files seem to 
comprise a significant portion (82%) of their dataset.
A second corpus of literature consists of papers focusing on potentially unwanted programs [8, 9]. Kotzias 
et al. [8], for example, looked into the who-installs-who relationships of PUPs and reported findings similar 
to ours with respect to PUPs delivering PUPs after the first infection. Similarly, we identified this behavior 
on other types of malware, e.g., ransomware transitioning to ransomware in 80% of cases. Interestingly, 
our results also suggested that seemingly less harmful malware such as adware and PUP tend to leave 
machines vulnerable to other malware (Section 5.2)
The same authors in [9] looked at PUPs from the perspective of code signing. Their analysis showed that 
most signed samples are PUPs and that other malware is not commonly signed. We also looked into this 
phenomenon, and our work tends to suggest that possibly-malicious software normally downloaded by 
browsers like droppers and PUPs tend to be correctly signed — probably as a need to send the code to 
execute on modern operating systems (Section 4.3). We leveraged this and other features identified in our 
measurement study to efficiently report unknown software downloads as malicious.
41 | Exploring the Long Tail of (Malicious) Software Downloads
Kurt et al. [19] and Caballero et al. [1] explored the ecosystem of pay-per-install campaigns (PPI) and their 
role in the proliferation of PUPs by uncovering the operational organization and ecosystem of bundled 
software at the back end. In contrast, our evaluation runs at the front end, on a population of over a million 
end-point machines. We reported on the importance of considering low-prevalence software downloads, 
as they generate files with no ground truth for a total of 69% of the entire machine population.
In  Section  6,  we  proposed  a  rule-based  classifier  that  helped  us  reduce  the  large  number  (83%)  of 
unknowns that we observed in our population of software download. Only about 0.25% of the files that we 
observed during our measurement period had a prevalence of more than 20. While similar classification 
systems have been proposed in the past (e.g., Polonium [2], Amico [20], CAMP [16], and Mastino [14]), 
they appear to be somewhat limited in scope when dealing with low-prevalence software files. Polonium 
[2], for example, reports a 48% detection rate on files with prevalences of 2 and 3, and it does not work on 
files seen on single machines that account for 94% of the dataset in [2]. Other systems [14, 16, 20] could 
potentially mistake low-prevalence benign files as malware. Also, these systems rely on the prevalence of 
the downloading URLs to provide classifications, which, as explained in Section 4.2, could cause issues 
for them.
42 | Exploring the Long Tail of (Malicious) Software Downloads
9. Conclusions
We have presented a large-scale study of global trends in software download events, with an analysis 
of both benign and malicious downloads, and a categorization of events for which no ground truth is 
currently available. Our measurement study, which is based on a real-world dataset containing more than 
3 million in-the-wild web-based software download events involving hundreds of thousands of internet 
machines,  shows  that  more  than  83%  of  all  downloaded  software  files  remain  unknown  to  the  anti-
malware  community  even  two  years  after  they  were  first  observed.  To  better  understand  what  these 
unknown software files might be, and their potential impact on real-world internet machines, we have 
performed a detailed analysis of their properties. 
We then built a rule-based classifier to extend the labeling of software downloads. This system can be 
used to identify many more benign and malicious files with very high confidence, allowing us to greatly 
expand the number of software files that can be used to evaluate the accuracy of malware detection 
systems.
43 | Exploring the Long Tail of (Malicious) Software Downloads
Acknowledgments
This material is based in part on work supported by the National Science Foundation (NSF) under grant No. 
CNS-1149051. Any opinions, findings, and conclusions or recommendations expressed in this material 
are those of the authors and do not necessarily reflect the views of the NSF. 
This  work  is  also  partially  supported  by  a  grant  from  the  Auburn  University  at  Montgomery  Research 
Grant-in Aid Program. Additional acknowledgments go to Trend Micro’s Forward-Looking Threat Research 
(FTR), SPN and Machine Learning teams who supported the research in different forms.
44 | Exploring the Long Tail of (Malicious) Software Downloads
REFERENCES
1. 
Juan Caballero, Chris Grier, Christian Kreibich, and Vern Paxson. Measuring pay-per-install: The commoditization of malware 
distribution. In Usenix security symposium, 2011.
2.  Duen Horng Chau, Carey Nachenberg, Jeffrey Wilhelm, Adam Wright, and Christos Faloutsos. Polonium: Tera-scale graph 
mining for malware detection. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.
3.  Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv:1702.08608v2 [stat.ML].
4.  Eibe  Frank  and  Ian  H.  Witten.  Generating  accurate  rule  sets  without  global  optimization.  In  J.  Shavlik,  editor,  Fifteenth 
International Conference on Machine Learning, pages 144–151. Morgan Kaufmann, 1998.
5.  Google. Google Safe Browsing. https://www.google.com/transparencyreport/safebrowsing/.
6.  Chris Grier,  Lucas Ballard, Juan Caballero, Neha Chachra, Christian J. Dietrich, Kirill Levchenko, Panayiotis Mavrommatis, 
Damon McCoy, Antonio Nappa, Andreas Pitsillidis, Niels Provos, M. Zubair Rafique, Moheeb Abu Rajab, Christian Rossow, 
Kurt Thomas, Vern Paxson, Stefan Savage, and Geoffrey M. Voelker. Manufacturing Compromise: The emergence of exploit-
as-a-service.  In  Proceedings  of  the  2012  ACM  Conference  on  Computer  and  Communications  Security,  CCS  ’12,  pages 
821–832, New York, NY, USA, 2012. ACM.
7.  Alexandros Kapravelos, Yan Shoshitaishvili, Marco Cova, Christopher Kruegel, and Giovanni Vigna. Revolver: An automated 
approach to the detection of evasive web-based malware. In USENIX Security, pages 637–652. Citeseer, 2013.
8.  Platon  Kotzias,  Leyla  Bilge,  and  Juan  Caballero.  Measuring  pup  prevalence  and  pup  distribution  through  pay-per-install 
services. In Proceedings of the USENIX Security Symposium, 2016.
9.  Platon Kotzias, Srdjan Matic, Richard Rivera, and Juan Caballero. Certified PUP: Abuse in Authenticode Code Signing. In ACM 
Conference on Computer and Communication Security, 2015.
10.  Bum Jun Kwon, Jayanta Mondal, Jiyong Jang, Leyla Bilge, and Tudor Dumitras. The dropper effect: Insights into malware 
distribution  with  downloader  graph  analytics.  In  Proceedings  of  the  22nd  ACM  SIGSAC  Conference  on  Computer  and 
Communications Security, pages 1118–1129. ACM, 2015.
11.  Terry  Nelms,  Roberto  Perdisci,  Manos  Antonakakis,  and  Mustaque  Ahamad.  Towards  measuring  and  mitigating  social 
engineering software  download attacks.  In  Proceedings of the  25th  USENIX Conference on  Security Symposium,  SEC’16, 
2016.
12.  Roberto Perdisci et al. Vamo: towards a fully automated malware clustering validity analysis. In Proceedings of the 28th Annual 
Computer Security Applications Conference, pages 329–338. ACM, 2012.
13.  Zubair  Rafique,  Tom  Van  Goethem,  Wouter  Joosen,  Christophe  Huygens,  and  Nick  Nikiforakis.  Itâ€™s  free  for  a  reason: 
Exploring the ecosystem of free live streaming services. 2016.
14.  Babak Rahbarinia, Marco Balduzzi, and Roberto Perdisci. Real-time detection of malware downloads via large-scale URL→file→ 
machine graph mining. In Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security, ASIA 
CCS ’16, pages 783–794, New York, NY, USA, 2016. ACM.
15.  Babak Rahbarinia, Roberto Perdisci, and Manos Antonakakis. Segugio: Efficient behavior-based tracking of malware-control 
domains  in  large  isp  networks.  In  Dependable  Systems  and  Networks  (DSN),  2015  45th  Annual  IEEE/IFIP  International 
Conference on, pages 403–414. IEEE, 2015.
16.  Moheeb Abu Rajab, Lucas Ballard, Noé Lutz, Panayiotis Mavrommatis, and Niels Provos. Camp: Content-agnostic malware 
protection. In NDSS, 2013.
17.  Christian Rossow, Christian Dietrich, and Herbert Bos. Large-scale analysis of malware downloaders. In Detection of Intrusions 
and Malware, and Vulnerability Assessment, pages 42–61. Springer, 2012.
18.  Marcos  Sebastián,  Richard  Rivera,  Platon  Kotzias,  and  Juan  Caballero.  Avclass:  A  tool  for  massive  malware  labeling.  In 
International Symposium on Research in Attacks, Intrusions, and Defenses, pages 230–253. Springer, 2016.
45 | Exploring the Long Tail of (Malicious) Software Downloads
19.  Kurt Thomas, Juan Antonio Elices Crespo, Ryan Rasti, Jean-Michel Picod, Cait Phillips, Chris Sharp, Fabio Tirelo, Ali Tofigh, 
Marc-Antoine  Courteau,  Lucas  Ballard,  et  al.  Investigating  commercial  pay-per-install  and  the  distribution  of  unwanted 
software. In USENIX Security Symposium, 2016.
20.  Phani  Vadrevu,  Babak  Rahbarinia,  Roberto  Perdisci,  Kang  Li,  and  Manos  Antonakakis.  Measuring  and  detecting  malware 
downloads in live network traffic. In Computer Security - ESORICS 2013 - 18th European Symposium on Research in Computer 
Security, Egham, UK, September 9-13, 2013. Proceedings, pages 556–573, 2013.
21.  Xinyu Xing, Wei Meng, Udi Weinsberg, Anmol Sheth, Byoungyoung Lee, Roberto Perdisci, and Wenke Lee. Unraveling the 
relationship between ad-injecting browser extensions and malvertising. In Proceedings of the International Conference on the 
World Wide Web, 2015.
46 | Exploring the Long Tail of (Malicious) Software Downloads
©2018 by Trend Micro, Incorporated. All rights reserved. Trend Micro and the Trend Micro t-ball logo are trademarks or registered trademarks of  Trend Micro, Incorporated. All other product or company names may be trademarks or registered trademarks of their owners.TREND MICROTMTrend Micro Incorporated, a global cloud security leader, creates a world safe for exchanging digital information with its Internet content security and threat management solutions for businesses and consumers.  A pioneer in server security with over 20 years experience, we deliver top-ranked client, server, and cloud-based security that fits our customers’ and partners’ needs; stops new threats faster; and protects data in physical, virtualized, and cloud environments. Powered by the Trend Micro™ Smart Protection Network™ infrastructure, our industry-leading cloud-computing security technology, products and services stop threats where they emerge, on the Internet, and are supported by 1,000+ threat intelligence experts around the globe. For additional information, visit www.trendmicro.com.