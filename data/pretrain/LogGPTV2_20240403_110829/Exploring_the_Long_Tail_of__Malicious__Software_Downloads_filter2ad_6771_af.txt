### Discussion on Rule-Based Classification System

As previously discussed, after a certain threshold (τ) value, adding more rules to the classification system can lead to a deterioration in both true positives (TPs) and false positives (FPs). This occurs because the inclusion of too many inaccurate rules with higher error rates can result in misclassifications. Additionally, the likelihood of files matching conflicting rules increases, leading the classifier to reject these files. Although more rules can label more truly unknown files, the final classification of these files may not be very accurate. Since τ = 0.1% produced the best performance on the test dataset, we use this setting for classifying unknown files.

One of the advantages of our rule-based classification system over traditional decision trees is that we do not need to use the entire decision tree, which may contain less accurate branches. From February to August, the system classified 406,688 previously unknown files as either benign or malicious, accounting for 28.30% of the total unknown files observed during this period.

### Evaluation Results and Classification of Unknown Files

The table below summarizes the evaluation results and the classification of unknown files using the rule-based classifier, where conflicts are handled by rejecting the test and unknown files.

| Time Period | τ | # Malicious | TP Rate | # Benign | FP Rate | # FP Rules |
|-------------|---|-------------|---------|----------|---------|------------|
| Jan - Feb   | 0.0% | 3,590 | 96.72% | 1,401 | 0.07% | 8          |
|             | 0.1% | 3,647 | 96.45% | 2,718 | 0.00% | 9          |
| Feb - Mar   | 0.0% | 3,045 | 97.59% | 2,051 | 0.39% | 6          |
|             | 0.1% | 3,070 | 97.60% | 2,830 | 0.32% | 8          |
| Mar - Apr   | 0.0% | 4,793 | 97.98% | 1,367 | 0.37% | 1          |
|             | 0.1% | 4,842 | 99.61% | 2,315 | 0.30% | 2          |
| Apr - May   | 0.0% | 3,001 | 92.01% | 1,873 | 0.05% | 4          |
|             | 0.1% | 7,203 | 96.96% | 2,267 | 0.13% | 4          |
| May - Jun   | 0.0% | 3,834 | 90.53% | 2,038 | 0.15% | 7          |
|             | 0.1% | 7,895 | 96.64% | 2,597 | 0.12% | 6          |
| Jun - Jul   | 0.0% | 7,200 | 95.39% | 2,414 | 0.25% | 1          |
|             | 0.1% | 7,202 | 95.28% | 2,837 | 0.18% | 6          |

### Analysis of Test Dataset Results

Among the correctly classified malicious test samples, 45% were droppers, 38% were trojans, and 3.5% were bankers. The remaining samples were divided among other malicious file types. The following sample rules were the most successful in detecting different types of malware:

- **Bankers and Bots**: 
  - IF (downloading process is “Acrobat Reader”) → file is malicious.
  
- **Droppers**:
  - IF (file’s signer is “Somoto ltd.”) → file is malicious.
  
- **Fake Antivirus (fakeavs)**:
  - IF (file is not signed) AND (Alexa rank of file’s URL is above 100K) AND (downloading process is benign) AND (downloading process’s signer is “Microsoft Windows”) → file is malicious.

### Expanding Available Ground Truth by Labeling Unknown Files

From February to August, the set of rules we learned was able to label 28.30% of all 1,436,829 previously unknown files, representing a 233% increase in labeled files compared to the available ground truth. These 28.30% of unknown files were downloaded by 294,419 machines, or 31% of all machines, indicating significant penetration across the machine population.

These results indicate that our rule-based classification method would enable a significant expansion of the labeling of software files compared to the ground truth available from multiple anti-virus sources. This would allow researchers to evaluate the accuracy of their malware detection systems over a much larger labeled dataset, including challenging cases of low-prevalence malicious files that, in aggregate, impact a large population of machines.

### Evading Detection

Evasion is possible for most statistical detection models. Malware developers could change signer information by acquiring new signing certificates, but valid certificates are expensive. Therefore, it would be costly to create polymorphic malware variants with unique signatures. Stealing a benign certificate is also possible, though difficult; however, once detected, the true certificate owners can revoke it. Using "benign" packers makes it easier to unpack and analyze the code, so malware often uses custom, hard-to-reverse packers. Thus, while it is technically possible to evade our system, it is not very practical.

### Related Work

In this work, we focus on a specific class of software downloads that have been neglected in the past: low-prevalence downloads. Previous research, such as Rossow et al. [17], analyzed a limited number of dropper families, while Kwon et al. [10] extended this research by examining download chains post-infection. Our work provides a comprehensive breakdown of different types of malware, analyzing their characteristics, signers, downloading URLs, and transitions between types. Unlike [10], we discuss the evaluation of our classifier for files with no ground truth, which comprise a significant portion (82%) of their dataset.

Other related works, such as Kotzias et al. [8, 9], focused on potentially unwanted programs (PUPs). We identified similar behaviors, such as PUPs delivering other PUPs after the first infection, and found that seemingly less harmful malware like adware and PUPs tend to leave machines vulnerable to other malware.

Kurt et al. [19] and Caballero et al. [1] explored the ecosystem of pay-per-install campaigns (PPI) and their role in the proliferation of PUPs. In contrast, our evaluation runs at the front end, on a population of over a million endpoint machines, and highlights the importance of considering low-prevalence software downloads, which generate files with no ground truth for 69% of the entire machine population.

### Conclusions

We have presented a large-scale study of global trends in software download events, analyzing both benign and malicious downloads and categorizing events for which no ground truth is currently available. Our measurement study, based on a real-world dataset of over 3 million web-based software download events involving hundreds of thousands of internet machines, shows that more than 83% of all downloaded software files remain unknown to the anti-malware community even two years after they were first observed.

To better understand these unknown software files and their potential impact, we performed a detailed analysis of their properties and built a rule-based classifier to extend the labeling of software downloads. This system can identify many more benign and malicious files with high confidence, allowing us to greatly expand the number of software files used to evaluate the accuracy of malware detection systems.

### Acknowledgments

This material is based in part on work supported by the National Science Foundation (NSF) under grant No. CNS-1149051. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. This work is also partially supported by a grant from the Auburn University at Montgomery Research Grant-in Aid Program. Additional acknowledgments go to Trend Micro’s Forward-Looking Threat Research (FTR), SPN, and Machine Learning teams who supported the research in various forms.

### References

1. Juan Caballero, Chris Grier, Christian Kreibich, and Vern Paxson. Measuring pay-per-install: The commoditization of malware distribution. In Usenix security symposium, 2011.
2. Duen Horng Chau, Carey Nachenberg, Jeffrey Wilhelm, Adam Wright, and Christos Faloutsos. Polonium: Tera-scale graph mining for malware detection. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.
3. Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv:1702.08608v2 [stat.ML].
4. Eibe Frank and Ian H. Witten. Generating accurate rule sets without global optimization. In J. Shavlik, editor, Fifteenth International Conference on Machine Learning, pages 144–151. Morgan Kaufmann, 1998.
5. Google. Google Safe Browsing. https://www.google.com/transparencyreport/safebrowsing/.
6. Chris Grier, Lucas Ballard, Juan Caballero, Neha Chachra, Christian J. Dietrich, Kirill Levchenko, Panayiotis Mavrommatis, Damon McCoy, Antonio Nappa, Andreas Pitsillidis, Niels Provos, M. Zubair Rafique, Moheeb Abu Rajab, Christian Rossow, Kurt Thomas, Vern Paxson, Stefan Savage, and Geoffrey M. Voelker. Manufacturing Compromise: The emergence of exploit-as-a-service. In Proceedings of the 2012 ACM Conference on Computer and Communications Security, CCS ’12, pages 821–832, New York, NY, USA, 2012. ACM.
7. Alexandros Kapravelos, Yan Shoshitaishvili, Marco Cova, Christopher Kruegel, and Giovanni Vigna. Revolver: An automated approach to the detection of evasive web-based malware. In USENIX Security, pages 637–652. Citeseer, 2013.
8. Platon Kotzias, Leyla Bilge, and Juan Caballero. Measuring PUP prevalence and PUP distribution through pay-per-install services. In Proceedings of the USENIX Security Symposium, 2016.
9. Platon Kotzias, Srdjan Matic, Richard Rivera, and Juan Caballero. Certified PUP: Abuse in Authenticode Code Signing. In ACM Conference on Computer and Communication Security, 2015.
10. Bum Jun Kwon, Jayanta Mondal, Jiyong Jang, Leyla Bilge, and Tudor Dumitras. The dropper effect: Insights into malware distribution with downloader graph analytics. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1118–1129. ACM, 2015.
11. Terry Nelms, Roberto Perdisci, Manos Antonakakis, and Mustaque Ahamad. Towards measuring and mitigating social engineering software download attacks. In Proceedings of the 25th USENIX Conference on Security Symposium, SEC’16, 2016.
12. Roberto Perdisci et al. Vamo: towards a fully automated malware clustering validity analysis. In Proceedings of the 28th Annual Computer Security Applications Conference, pages 329–338. ACM, 2012.
13. Zubair Rafique, Tom Van Goethem, Wouter Joosen, Christophe Huygens, and Nick Nikiforakis. Itâ€™s free for a reason: Exploring the ecosystem of free live streaming services. 2016.
14. Babak Rahbarinia, Marco Balduzzi, and Roberto Perdisci. Real-time detection of malware downloads via large-scale URL→file→machine graph mining. In Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security, ASIA CCS ’16, pages 783–794, New York, NY, USA, 2016. ACM.
15. Babak Rahbarinia, Roberto Perdisci, and Manos Antonakakis. Segugio: Efficient behavior-based tracking of malware-control domains in large ISP networks. In Dependable Systems and Networks (DSN), 2015 45th Annual IEEE/IFIP International Conference on, pages 403–414. IEEE, 2015.
16. Moheeb Abu Rajab, Lucas Ballard, Noé Lutz, Panayiotis Mavrommatis, and Niels Provos. CAMP: Content-agnostic malware protection. In NDSS, 2013.
17. Christian Rossow, Christian Dietrich, and Herbert Bos. Large-scale analysis of malware downloaders. In Detection of Intrusions and Malware, and Vulnerability Assessment, pages 42–61. Springer, 2012.
18. Marcos Sebastián, Richard Rivera, Platon Kotzias, and Juan Caballero. AVClass: A tool for massive malware labeling. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 230–253. Springer, 2016.
19. Kurt Thomas, Juan Antonio Elices Crespo, Ryan Rasti, Jean-Michel Picod, Cait Phillips, Chris Sharp, Fabio Tirelo, Ali Tofigh, Marc-Antoine Courteau, Lucas Ballard, et al. Investigating commercial pay-per-install and the distribution of unwanted software. In USENIX Security Symposium, 2016.
20. Phani Vadrevu, Babak Rahbarinia, Roberto Perdisci, Kang Li, and Manos Antonakakis. Measuring and detecting malware downloads in live network traffic. In Computer Security - ESORICS 2013 - 18th European Symposium on Research in Computer Security, Egham, UK, September 9-13, 2013. Proceedings, pages 556–573, 2013.
21. Xinyu Xing, Wei Meng, Udi Weinsberg, Anmol Sheth, Byoungyoung Lee, Roberto Perdisci, and Wenke Lee. Unraveling the relationship between ad-injecting browser extensions and malvertising. In Proceedings of the International Conference on the World Wide Web, 2015.

©2018 by Trend Micro, Incorporated. All rights reserved. Trend Micro and the Trend Micro t-ball logo are trademarks or registered trademarks of Trend Micro, Incorporated. All other product or company names may be trademarks or registered trademarks of their owners.