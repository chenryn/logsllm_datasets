title:Does Every Second Count? Time-based Evolution of Malware Behavior
in Sandboxes
author:Alexander K&quot;uchler and
Alessandro Mantovani and
Yufei Han and
Leyla Bilge and
Davide Balzarotti
Does Every Second Count?
Time-based Evolution of Malware Behavior in
Sandboxes
Alexander K¨uchler∗, Alessandro Mantovani†, Yufei Han‡, Leyla Bilge‡ and Davide Balzarotti†
∗ Fraunhofer AISEC, Germany. PI:EMAIL
† EURECOM, France. {alessandro.mantovani | davide.balzarotti}@eurecom.fr
‡ NortonLifeLock Research Group, France. {yufei.han | leyla.bilge}@nortonlifelock.com
Abstract—The amount of time in which a sample is executed
is one of the key parameters of a malware analysis sandbox.
Setting the threshold too high hinders the scalability and reduces
the number of samples that can be analyzed in a day; too
low and the samples may not have the time to show their
malicious behavior, thus reducing the amount and quality of the
collected data. Therefore, an analyst needs to ﬁnd the ‘sweet spot’
that allows to collect only the minimum amount of information
required to properly classify each sample. Anything more is
wasting resources, anything less is jeopardizing the experiments.
Despite its importance, there are no clear guidelines on how to
choose this parameter, nor experiments that can help companies
to assess the pros and cons of a choice over another. To ﬁll
this gap, in this paper we provide the ﬁrst large-scale study of
the impact that the execution time has on both the amount and
the quality of the collected events. We measure the evolution
of system calls and code coverage, to draw a precise picture of
the fraction of runtime behavior we can expect to observe in
a sandbox. Finally, we implemented a machine learning based
malware detection method, and applied it to the data collected
in different time windows, to also report on the relevance of the
events observed at different points in time.
Our results show that most samples run for either less than
two minutes or for more than ten. However, most of the behavior
(and 98% of the executed basic blocks) are observed during the
ﬁrst two minutes of execution, which is also the time windows that
result in a higher accuracy of our ML classiﬁer. We believe this
information can help future researchers and industrial sandboxes
to better tune their analysis systems.
I.
INTRODUCTION
Malware analysis sandboxes play a fundamental role in the
analysis of suspicious software. The importance of these tools
has brought to a proliferation of different platforms, resulting
in a large number of both open source and commercial sandbox
solutions [1]–[3], [9]. Moreover, ﬁfteen years of research in
the ﬁeld has covered a wide range of technical aspects and
proposed new solutions dedicated to the dynamic analysis of
malicious samples [19], [34], [53], [83], [87], [101], [106].
Network and Distributed Systems Security (NDSS) Symposium 2021
21-25  February  2021, Virtual 
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24475
www.ndss-symposium.org
However, despite the fact that malware analysis sandboxes
are a well-studied and mature technology, little is known about
the best conﬁguration setup that is required to maximize their
effectiveness. This is true even for their most simple parameter:
the sample execution time. Over the years, companies running
fully automated analysis infrastructures kept decreasing the
analysis time to cope with the increasing number of collected
samples. While this may seem an obvious response to the big
data problem, there is no study that measured the impact of
less analysis time on both the amount and the quality of the
information collected by the sandbox; How much do we really
lose by going from ten minutes per sample to three minutes?
And then, from three to one?
At ﬁrst, this might seem like a classic trade-off between
the volume of the collected data and the amount of samples
that can be analyzed in a given amount of time. However,
the answer is actually much more complex than that. In fact,
it boils down to two fundamental aspects. First, on how the
actions performed by a malware sample are distributed over
time: does the malicious behavior of a program start from the
ﬁrst second, or can it be observed only after a few minutes
of execution? Second, it depends on how important a piece of
behavior is for a speciﬁc goal. For instance, the fact that a short
execution time might only expose 30% of the behavior of a
sample, might not necessarily be a bad thing. It all depends on
whether that 30% is sufﬁcient to correctly classify the sample
or to obtain adequate information for the analysts. Intuitively,
longer execution time will result in the collection of more
events, which may contain useful information to characterize
the behavior of a sample. As a result, one would logically
expect a time window with increasingly longer temporal cover-
age to provide better classiﬁcation results. Sadly, none of these
two factors (i.e., the evolution of the malicious behavior over
time and the relevance of the collected information) have been
studied before. This, as we will show in Section II, has left
analysts and researchers without clear guidelines. As a result,
everyone was free to pick his own value—mostly based on gut
feelings on what they believed to be a “reasonable” execution
time.
These problems, and the importance of designing a mal-
ware analysis platform based on real data, motivated us to
perform a comprehensive study on the subject. By using a
custom-designed solution based on the PANDA record-replay
emulator [31], we collected ﬁne-grained information on the
execution of 100K samples, and use it to shed light on how
time affects both the volume and the relevance of the collected
data.
Our key ﬁndings, summarized in more details in Sec-
tion VIII, show that most of the samples execute either for
less than two minutes or for more than ten minutes. In both
the cases, we could observe that while the number of system
calls tends to increase linearly over time, the code responsible
for them is typically explored very fast – in the ﬁrst one or two
minutes of execution. Our measure of the impact of stalling
code in its traditional form (i.e., invocations of a sleep API)
shows that it actually inﬂuences in a remarkable way only
a low percentage of samples (close to the 3%). Furthermore,
we experimented with different code-based metrics to estimate
the absolute code coverage that samples reach while running
in a sandbox. Thanks to these measurements, we illustrate that
absolute code coverage of a sample can have a great variance,
depending on its family and on the sample itself. Overall, we
registered code coverage values in the range of 10% to 40%.
Finally, we implemented a state-of-the-art machine learning
classiﬁer and conducted experiments to measure how unique
and how relevant the data collected in different time windows
is. This helps us to answer whether it is easier to tell that
a program is malicious by looking at its actions in the ﬁrst
minute, or by looking at
those it performs after three or
ﬁve minutes. Indeed, we found that the ﬁrst two minutes of
execution are the most representative from the perspective of
a ML classiﬁer, not just because of the amount of events
registered in such a timespan, but also for their ”quality” in
terms of novel information that they can provide to a classiﬁer.
We plan to release the entire data we collected from the
execution of 100K samples, to help other researchers replicate
our ﬁndings and conduct further experiments on the nature and
evolution of malicious behavior.
II. MOTIVATION AND RELATED WORK
In 2007, in their seminal work that proposed the design of
the original malware analysis sandbox, Willems et al. [101]
noted:
“We found that executing the malware for two min-
utes yielded the most accurate results and allowed
the malware binary enough time to interact with
the system, thus copying itself to another location,
spawning new processes, or connecting to a remote
server, and so on.”
While interesting, this was purely a qualitative statement,
and the authors never mentioned what kind of experiments they
conducted to support the choice of this threshold. At the other
end of the spectrum, in 2011 Rossow et al. [82] executed each
sample for up to one hour and noted that only 23.6% of the
communication endpoints and 95% of the network protocols
were observed in the ﬁrst 5 minutes of analysis. This seems to
suggest that if the goal is to study the network behavior, two
minutes are most likely insufﬁcient to collect the majority of
the sample’s behavior.
To the best of our knowledge, the only other work that
studied the interval of time required to properly analyze
Fig. 1: Execution Time vs Dataset Size
malware was performed in 2017 by Kilgallon et al. [51]. To
be precise, the goal of the authors was to predict the time
needed for a sample to show enough malicious behavior for
the heuristics of the Cuckoo sandbox to assign a malicious
score of ﬁve. To train their model, the authors conducted a
small experiment with 3320 samples, and found that 82% of
them needed to be executed for less than one minute, and 53%
could already provide the required information in their ﬁrst 20
seconds.
Due to the lack of clear guidelines, over the past ﬁfteen
years researchers and security companies adopted a wide range
of values for the samples execution time. As we will discuss
later in this section, the rationale behind each choice is often
unknown, but for sure the main goal of the analyst (for
academic researchers) and the amount of samples that need to
be analyzed over a limited period of time (for companies) are
the two main factors that dictated the choice of one threshold
over another.
Researchers tend to focus on relatively small datasets that
can be analyzed over the course of many days, typically with
the goal of collecting data to study a very speciﬁc behavior.
Instead, security companies need to operate fully automated
malware analysis pipelines to process hundreds of thousands
of samples per day, thus focusing more on scalability and on
the collection of generic behavioral data.
In the next two paragraphs we provide more details about
the time intervals adopted in the experiments conducted by
researchers and security companies.
A. Research Experiments
Researchers have been conducting a large spectrum of
dynamic analysis experiments over the past
twenty years.
Some were explicitly designed to improve or propose new
sandbox techniques, while others simply relied on sandboxes to
collect data to perform other experiments — such as modeling
the behavior of samples, extracting new detection signatures,
train a classiﬁer, or report on the internals of certain malware
characteristics (such as packing, use of encryption, etc.).
2
TABLE I: Papers collected by execution time
Papers
5 [44]–[46], [94], [109]
2 [56], [108]
14 [21], [26], [35], [40], [43], [52], [70], [85], [91], [95],
[100], [101], [105], [110]
7 [15], [65]–[67], [71], [81], [93]
1 [58]
13 [12], [13], [23], [29], [38], [50], [51], [69], [78], [88], [92],
[102], [106]
2 [20], [89]
7 [24], [25], [36], [41], [59], [63], [87]
1 [10]
2 [82], [84]
Time
(minutes)
 15
We reviewed several papers by looking at the execution
time threshold used by their authors, and report the results in
Table I, grouped in different time ranges. The values range
from a minimum of 30 seconds [44]–[46], [94], [109] to a
maximum of 1h [82].
It is also interesting to note that it is very rare for a paper
to discuss why a certain value was chosen. We only found a
handful of explanations, including “We used executions of 120
seconds, as we found that two minutes is generally enough time
for most malware to execute its immediate payload, if it has
one” in 2010 [40] or “The 5 minute window was arbitrarily
deﬁned on the assumption that the payload would execute
immediately upon execution and would take no longer than
5 minutes to complete” in 2018 [23]. However, none of the
papers conducted experiments to support their choice.
Figure 1 shows the sandbox execution time versus the
number of samples analyzed. There is no clear relationship
among the two, conﬁrming the fact that the sample execution
time chosen by academic researchers is more the result of the
personal judgment of the authors than a choice dictated by the
size of the dataset.
Finally, it is also interesting to note that the execution time
is rarely discussed as an important factor when comparing
with previous works. It is common for some papers to extract
malware behavior from a two-minute execution and compare
the models with previous studies that run samples for longer
periods of time. In this case it is unclear whether accuracy
degradations or improvements of the proposed techniques are
caused by a better solution or simply by the better data
collected over a longer period of time.
B. Industrial sandboxes
While researchers often have to perform customized anal-
ysis in their experiments, we expect security companies to use
more standardized architectures that have been carefully tuned
over the years to maximize the trade-off between the required
resources and the collected information. Unfortunately, to the
best of our knowledge none of the companies disclose any
details about their internal pipeline.
Therefore, to collect some data we created a simple probe
binary (compiled for Windows) as already performed in the
past by Yokoyama et al. in 2016 [107]. The goal of our
probe was to make HTTP requests to a server under our
control at regular intervals of time (with a granularity of 10
seconds). To distinguish among different executions of the
same sample, we included in the request the PID of the process,
a random number generated when the probe is executed,
and an incremental counter. No other information about the
sandbox, the environment, or the network in which the sample
is executed was collected by our program.
We ﬁrst submitted the probe to VirusTotal [8] because it
serves as entry point for a large number of security tools and
companies. After submitting a ﬁle, VT shares it with different
Antivirus engines that analyze the ﬁle in their own sandboxes.
Several companies also receive feeds from new submissions,
thus increasing the number of times our probe was analyzed.
Even though VT could impose a time limitation to perform
dynamic analysis, we observed multiple re-executions of the
sample after the initial submission and our results conﬁrm that
the majority of the executions happened well after VT dis-
played its result. In addition to VirusTotal, we also submitted
our probe to the list of online sandbox services collected by
Yokoyama et al. [107].
A month after the initial submission our probe had been
analyzed by 32 different sandboxes. We can summarize the
results in three main categories:
• 23 sandboxes ran the sample exactly once for a ﬁxed
interval of time, ranging from 30 seconds to 4 minutes.
• 4 sandboxes ran the sample but manipulated the sleep
invocation to lower the sleeping time. In this case we
received all HTTP requests in a burst. The sample was
analyzed for a maximum period of 2-to-3 minutes (in-
cluding the manipulated sleep invocations).