DevOps 实践指南精要 
作者：张乐 2018.04
Case Study
Operation InVersion at LinkedIn (2011)
启示：需要将偿还技术债作为⽇日常⼯工作的⼀一部分
2011年年IPO之后的六个⽉月，LinkedIn
持续与痛苦的、有问题的部署⽃斗争
VP of Engineering决定停⽌止所有功
能开发两个⽉月
彻底检修环境、部署、架构问题
背景
2003年年成⽴立，帮助⽤用户 “connect to your network for better job opportunities.
2015年年11⽉月数据
350 million会员，每秒数万请求
后台系统每秒百万次查询请求
开始时LinkedIn主要运⾏行行在⾃自开发的
Leo应⽤用上
Java单体式应⽤用
通过Servlets服务⻚页⾯面，并管理理与后
台很多Oracle DB的JDBC连接
随着早年年业务发展，两个关键服务从
Leo分离出来
会员关系图谱，在内存中
会员查询，在上个服务之上
到2010年年，⼤大多数新开发使⽤用新服
务，在Leo外围有100个左右
问题是Leo 2周才发布⼀一次
Leo系统有显著的问题
尽管可以垂直升级，如内存和CPUs
Leo经常在⽣生产环境宕机
很难排查问题和回复
很难发布新的代码
每当要发布⼀一组变更更时，站点崩溃混
乱，需要⼯工程师⼯工作到深夜解决问题
到2011年年，问题已经⽆无法忍受
需要Kill Leo，分解为很多⼩小的功能和⽆无状态服务
VP of Engineering 决定完全停⽌止新功能
⼯工作，整个部⻔门修复站点的核⼼心基础设施
业务和团队的需要
对公众宣布这⼀一决定时，是可怕的事情
好处
⼤大量量积极的结果
创建了了全套的软件和⼯工具，帮助开发代码
不不⽤用再等待数周让新功能上到主站
⼯工程师可以开发新的服务
通过⼀一系列列⾃自动化验证，发现
影响现有功能的缺陷和问题
⽴立即发布到主站
主要升级每天三次
通过构建了了安全的⼯工作系统，创造了了价值
更更少⼯工作到深夜
更更多时间⽤用于开发新功能和创新
整个⼯工程师组织都聚焦在改进⼯工具和
部署、基础架构、开发⽣生产率
成功构建了了⼯工程敏敏捷性
2010年年有超过150个独⽴立服务，今天有超过750个
总结
LinkedIn⽀支付了了接近10年年的技术债务
提升了了稳定性和安全性
⽀支撑公司未来阶段的成⻓长
代价是2个⽉月全部的聚焦在⾮非功能需求，
以及所有承诺功能的损失
把寻找和修复问题作为⽇日常⼯工作的⼀一部分，管理理技
术债务，我们可以避免这种 “near death” 的体验
API Enablement at Target (2015)
背景
Target 是美国第六⼤大零售商
每年年花费超过 $1 billion 在技术领域
在以前，需要10个不不同的团队才能完成⼀一台Server分配
如果出现问题，会停⽌止变更更并防⽌止进⼀一步出现问题，但这让⼀一切更更糟
问题
获取环境和执⾏行行部署成为开发团队的显著困难
获取需要的数据也同样困难
库存信息、架构、商店信息等核⼼心
数据被锁在遗留留系统和⼤大型机中
我们经常有多个数据来源，
特别是电⼦子商务和实体商店
由不不同团队负责
使⽤用不不同数据结构和优先级
如果⼀一个新的开发团队为客户构建新功能
需要3-6个⽉月进⾏行行集成，获取所需要的数据
更更糟的是，需要另外3-6个⽉月进⾏行行⼿手⼯工测试，
确认没有破坏关键功能
因为是在⼀一个紧耦合系统中
需要⼤大量量项⽬目经理理，因为需要协调和交接
开发把时间消耗在队列列中等待，⽽而不不是
交付结果和把事情做完
需要管理理20-30不不同团队依赖和交互
这种获取和构建数据的较⻓长的前置周期
危及了了业务⽬目标
集成实体商店和电⼦子商务的供应链
超出了了原有的设计⽬目标：只是从供应
商分发到商店
需要将商品送到商店和客户家中
改进点
API Enablement team
让团队需要交付能⼒力力按天⽽而不不是按⽉月
让Target内部的⼯工程师可以获取和存储需要的数据
时间约束在团队选择中扮演了了重要⻆角⾊色
需要团队承担⼯工作，⽽而不不是外包出去
需要⼯工程师技能，⽽而不不是管理理合同
为了了确保⼯工作不不是在队⾥里里中等待，需要
拥有整个栈，并且接管运维需求
引⼊入了了很多⽀支持持续集成和持续交付的⼯工具
为⽀支撑⼤大规模增⻓长，引⼊入Cassandra 
database and Kafka message broker
当改进申请权限时，被告知No，但是团队仍
然坚持去做，因为团队知道需要它
效果
在随后的两年年，API Enablement Team实现了了53个新业务能⼒力力
集成Pinterest时⾮非常简单
因为只需要提供⾃自⼰己的APIs
2014年年，API Enablement Team
服务了了超过每个⽉月15亿API调⽤用
2015年年，增⻓长到了了每个⽉月170亿API
调⽤用，横跨90个不不同的APIs
为了了⽀支持这种能⼒力力，每周例例⾏行行进⾏行行
80次部署
业务价值
数字化销售在2014假⽇日季增⻓长42%
在Q2⼜又增⻓长32%
2015⿊黑⾊色星期五，产⽣生了了28万实体店取货订单
2015年年⽬目标是1800家商店中的450家⽀支持电⼦子订单（原来100家）
启示
涌现理理论
强⽣生公司案例例
技术架构：API/微服务化
组织结构：⼩小型⾃自组织团队
Continuous Integration at 
Bazaarvoice
背景
Bazaarvoice为数千家零售商提供⽤用
户访谈和投票服务
2012，开始进⾏行行开发和发布流程变⾰革
$ 120million收⼊入，准备IPO
业务核⼼心是对话应⽤用
Java单体系统，⾃自2006年年开始，累计5百万⾏行行代码
服务跑在1200台服务器器上，横跨4⼤大数据中⼼心和多个云服务提供商
已切换到敏敏捷开发流程，2周迭代周期
⽬目前10周产品发布周期，渴望提升发布频率
已经开始做架构解耦，从单体架构转向微服务
2012年年⾸首次尝试
两周的发布周期
进展不不顺利利，导致⼤大量量混乱
44个⽣生产事故被⽤用户发现
管理理层：我们不不能再做这样的事情了了
⽬目标：双周发布，但不不会导致客户不不可⽤用
发布更更频繁，⽀支持AB测试
加强功能到⽣生产环境的流动
现有的三⼤大问题
缺乏各级⾃自动化测试，双周迭代⽆无法预防⼤大规模失败
版本控制分⽀支策略略允许开发提交的代码⽴立即到⽣生产发布
团队运⾏行行微服务可以独⽴立发布，⽽而单体架构发布时经常出现问题，反之亦然
解决⽅方案
持续集成
接下来的六周，开发停⽌止功能开发，专注在写⾃自动化测试集
包括单元测试-Junit, 回归测试-Selenium
使⽤用TeamCity建设部署流⽔水线
主⼲干/分⽀支发布模型
每两周创建⼀一个新的发布分⽀支
没有新的提交允许到这个分⽀支
除⾮非紧急情况-需要⾛走审批流程
这个分⽀支⽤用于⾛走QA流程，最终晋级
到⽣生产环境
效果
⼀一直运⾏行行测试，进⾏行行变更更时得到了了⼀一定级别的安全性
更更重要的，⽴立即发现某⼈人破坏了了什什么事情，
⽽而不不是只在⽣生产环境发现
改进对可预测性和发布质量量的作⽤用
2012年年1⽉月版本，44个客户事故（持续集成⼯工作开始）
2012年年3⽉月6⽇日版本，5天延迟，5个客户事故
2012年年3⽉月22⽇日，按时，1个客户事故
2012年年4⽉月5⽇日，按时，0客户事故
每两周发布很成功，于是转到了了每周发布
由于发布例例⾏行行化，很容易易进⾏行行两倍的发布
下⼀一步⽬目标
加速测试，从3+⼩小时到⼩小于1⼩小时
减少环境数，从4到3（开发，测试，⽣生产，去掉Staging）
转向全⾯面的持续交付模型
更更快，⼀一键式部署
Daily Deployments at CSG 
International (2013)
背景
美国经营账单打印最⼤大的公司之⼀一
⾸首席架构师和开发VP
投⼊入改进发布的可预测性和可靠性
发布频率从每年年两次到四次
发布周期从28周降到14周
开发团队使⽤用持续集成，每天部署代码到
测试环境，但⽣生产发布由运维团队管理理
⽣生产环境低频发布，⻛风险⾼高，约束不不同，
包括安全，防⽕火墙，负载均衡和SAN
解决⽅方案
成⽴立共享运维组SOT
管理理所有环境（开发，测试，⽣生产）
每天部署开发和测试环境
每天获得反馈
⽣生产环境部署每14周
但之前每天都在开发测试环境演练
由于这个团队每天做部署，有⾃自动化改进的积极性
让环境看起来尽可能相似
包括安全访问权限和负载权衡
修改架构设计，减少不不同环境差异性
原来
交接给DBA团队，让他们搞定