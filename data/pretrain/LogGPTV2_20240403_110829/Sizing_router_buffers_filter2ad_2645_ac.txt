√
2
3
2
B
2T p·C+B√
n
1
A .
(3)
Here are some numerical examples of utilization, using n =
10000.
Router Buﬀer Size
B = 1 · 2T p·C√
B = 1.5 · 2T p·C√
B = 2 · 2T p·C√
n
n
n
Utilization
Util ≥ 98.99 %
Util ≥ 99.99988 %
Util ≥ 99.99997 %
This means that we can achieve full utilization with buﬀers
that are the delay-bandwidth product divided by square-
root of the number of ﬂows, or a small multiple thereof. As
the number of ﬂows through a router increases, the amount
of required buﬀer decreases.
This result has practical implications for building routers.
A core router currently has from 10,000 to over 100,000 ﬂows
passing through it at any given time. While the vast ma-
jority of ﬂows are short (e.g. ﬂows with fewer than 100
packets), the ﬂow length distribution is heavy tailed and
the majority of packets at any given time belong to long
ﬂows. As a result, such a router would achieve close to full
10000 = 1% of
utilization with buﬀer sizes that are only
the delay-bandwidth product. We will verify this result ex-
perimentally in Section 5.2.
1√
4. SIZING THE ROUTER BUFFER FOR
SHORT FLOWS
Not all TCP ﬂows are long-lived; in fact many ﬂows last
only a few packets, never leave slow-start, and so never reach
their equilibrium sending rate [4]. Up until now we’ve only
considered long-lived TCP ﬂows, and so now we’ll consider
how short TCP ﬂows aﬀect the size of the router buﬀer.
We’re going to ﬁnd that short ﬂows (TCP and non-TCP)
have a much smaller eﬀect than long-lived TCP ﬂows, par-
ticularly in a backbone router with a large number of ﬂows.
We will deﬁne a short-lived ﬂow to be a TCP ﬂow that
never leaves slow-start (e.g. any ﬂow with fewer than 90
packets, assuming a typical maximum window size of 65kB).
In Section 5.3 we will see that our results hold for short non-
TCP ﬂows too (e.g. DNS queries, ICMP, etc.).
Consider again the topology in Figure 2 with multiple
senders on separate access links. As has been widely re-
ported from measurement, we assume that new short ﬂows
arrive according to a Poisson process [16, 17]. In slow-start,
each ﬂow ﬁrst sends out two packets, then four, eight, six-
teen, etc. This is the slow-start algorithm in which the
sender increases the window-size by one packet for each re-
ceived ACK. If the access links have lower bandwidth than
the bottleneck link, the bursts are spread out and a single
burst causes no queueing. We assume the worst case where
access links have inﬁnite speed, bursts arrive intact at the
bottleneck router.
We will model bursts arriving from many diﬀerent short
ﬂows at the bottleneck router. Some ﬂows will be sending a
burst of two packets, while others might be sending a burst
of four, eight, or sixteen packets and so on. There will be a
distribution of burst-sizes; and if there is a very large num-
ber of ﬂows, we can consider each burst to be independent
of the other bursts, even of the other bursts in the same
ﬂow. In this simpliﬁed model, the arrival process of bursts
themselves (as opposed to the arrival of ﬂows) can be as-
sumed to be Poisson. One might argue that the arrivals are
not Poisson as a burst is followed by another burst one RTT
later. However under a low load and with many ﬂows, the
buﬀer will usually empty several times during one RTT and
is eﬀectively “memoryless” at this time scale.
For instance, let’s assume we have arrivals of ﬂows of a
ﬁxed length l. Because of the doubling of the burst lengths
in each iteration of slow-start, each ﬂow will arrive in n
bursts of size
Xi = {2, 4, ...2
n−1, R},
where R is the remainder, R = l mod (2n − 1). Therefore,
the bursts arrive as a Poisson process, and their lengths
are i.i.d.
random variables, equally distributed among
{2, 4, ...2n−1, R}.
The router buﬀer can now be modelled as a simple M/G/1
queue with a FIFO service discipline. In our case a “job” is
a burst of packets, and the job size is the number of packets
in a burst. The average number of jobs in an M/G/1 queue
is known to be (e.g. [18])
E[N ] =
ρ
2(1 − ρ)
E[X 2
].
Here ρ is the load on the link (the ratio of the amount of
incoming traﬃc to the link capacity C), and E[X] and E[X 2]
are the ﬁrst two moments of the burst size. This model will
overestimate the queue length because bursts are processed
]
[
Q
E
h
t
g
n
e
L
e
u
e
u
Q
e
g
a
r
e
v
A
 50
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
 40 Mbit/s link
 80 Mbit/s link
200 Mbit/s link
M/G/1 Model
 0
 10
 20
 30
 40
 50
 60
Length of TCP Flow [pkts]
Figure 9: The average queue length as a function of
the ﬂow length for ρ = 0.8. The bandwidth has no
impact on the buﬀer requirement. The upper bound
given by the M/G/1 model with inﬁnite access link
speeds matches the simulation data closely.
packet-by-packet while in an M/G/1 queue the job is only
dequeued when the whole job has been processed.
If the
queue is busy, it will overestimate the queue length by half
the average job size, and so
E[Q] =
ρ
2(1 − ρ)
E[X 2]
E[X]
− ρ
E[X]
2
It is interesting to note that the average queue length is
independent of the number of ﬂows and the bandwidth of
the link.
It only depends on the load of the link and the
length of the ﬂows.
We can validate our model by comparing it with simula-
tions. Figure 9 shows a plot of the average queue length for
a ﬁxed load and varying ﬂow lengths, generated using ns2.
Graphs for three diﬀerent bottleneck link bandwidths (40, 80
and 200 Mb/s) are shown. The model predicts the relation-
ship very closely. Perhaps surprisingly, the average queue
length peaks when the probability of large bursts is highest,
not necessarily when the average burst size is highest. For
instance, ﬂows of size 14 will generate a larger queue length
than ﬂows of size 16. This is because a ﬂow of 14 packets
generates bursts of Xi = {2, 4, 8} and the largest burst of
size 8 has a probability of 1
3 . A ﬂow of 16 packets generates
bursts of sizes Xi = {2, 4, 8, 4}, where the maximum burst
length of 8 has a probability of 1
4 . As the model predicts,
the bandwidth has no eﬀect on queue length, and the mea-
surements for 40, 80 and 200 Mb/s are almost identical. The
gap between model and simulation is due to the fact that the
access links before the bottleneck link space out the packets
of each burst. Slower access links would produce an even
smaller average queue length.
To determine the buﬀer size we need the probability dis-
tribution of the queue length, not just its average. This is
more diﬃcult as no closed form result exists for a general
M/G/1 queue length distribution. Instead, we approximate
its tail using the eﬀective bandwidth model [19], which tells
us that the queue length distribution is
P (Q ≥ b) = e
−b
2(1−ρ)
ρ
.
E[Xi]
E[X2
]
i
This equation is derived in the extended version on this pa-
per [7]
Our goal is to drop very few packets (if a short ﬂow drops
a packet, the retransmission signiﬁcantly increases the ﬂow’s
duration). In other words, we want to choose a buﬀer size
B such that P (Q ≥ B) is small.
A key observation is that - for short ﬂows - the size of
the buﬀer does not depend on the line-rate, the propagation
delay of the ﬂows, or the number of ﬂows; it only depends
on the load of the link, and length of the ﬂows. Therefore, a
backbone router serving highly aggregated traﬃc needs the
same amount of buﬀering to absorb short-lived ﬂows as a
router serving only a few clients. Furthermore, because our
analysis doesn’t depend on the dynamics of slow-start (only
on the burst-size distribution), it can be easily extended to
short unresponsive UDP ﬂows.
In practice, buﬀers can be made even slower. For our
model and simulation we assumed access links that are faster
than the bottleneck link. There is evidence [4, 20] that
highly aggregated traﬃc from slow access links in some cases
can lead to bursts being smoothed out completely. In this
case individual packet arrivals are close to Poisson, result-
ing in even smaller buﬀers. The buﬀer size can be easily
computed with an M/D/1 model by setting Xi = 1.
In summary, short-lived ﬂows require only small buﬀers.
When there is a mix of short- and long-lived ﬂows, we will see
from simulations and experiments in the next section, that
the short-lived ﬂows contribute very little to the buﬀering
requirements, and so the buﬀer size will usually be deter-
mined by the number of long-lived ﬂows6.
5. SIMULATION AND EXPERIMENTAL
RESULTS
Up until now we’ve described only theoretical models of
long- and short-lived ﬂows. We now present results to val-
idate our models. We use two validation methods: simu-
lation (using ns2), and a network of real Internet routers.
The simulations give us the most ﬂexibility: they allow us to
explore a range of topologies, link speeds, numbers of ﬂows
and traﬃc mixes. On the other hand, the experimental net-
work allows us to compare our results against a network
of real Internet routers and TCP sources (rather than the
idealized ones in ns2). It is less ﬂexible and has a limited
number of routers and topologies. Our results are limited to
the ﬁnite number of diﬀerent simulations and experiments
we can run, and we can’t prove that the results extend to
any router in the Internet [21]. And so in Section 5.3 we
examine the scope and limitations of our results, and what
further validation steps are needed.
Our goal is to persuade a network operator to test our
results by reducing the size of their router buﬀers by ap-
proximately 99%, and checking that the utilization and drop
rates don’t change noticeably. Until that time, we have to
rely on a more limited set of experiments.
5.1 NS2 Simulations
We ran over 15,000 ns2 simulations, each one simulating
several minutes of network traﬃc through a router to verify
6For a distribution of ﬂows we deﬁne short ﬂows and long
ﬂows as ﬂows that are in slow-start and congestion avoidance
mode respectively. This means that ﬂows may transition
from short to long during their existence.
]
s
t
k
p
[
r
e
f
f
u
b
d
e
r
i
u
q
e
r
m
u
m
n
M
i
i
 400
 350
 300
 250
 200
 150
 100
 50
 0
 50
98.0% Utilization
99.5% Utilization
99.9% Utilization
RTTxBW/sqrt(x)
2*RTTxBW/sqrt(x)
r
e
f
f
u
B
d
e
r
i
u
q
e
R
m
u
m
n
M
i
i
 100
 150
 200
 250
 300
 350
 400
 450
 500
Number of long-lived flows
 250
 200
 150
 100
 50
 0
 0
 40 Mbit/s link
 80 Mbit/s link
200 Mbit/s link
M/G/1 Model p=0.01
 10
 20
 30
 40
 50
 60
Length of TCP Flow [pkts]
Figure 10: Minimum required buﬀer to achieve 98,
99.5 and 99.9 percent utilization for an OC3 (155
Mb/s) line with about 80ms average RTT measured