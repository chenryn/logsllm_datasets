title:pFabric: minimal near-optimal datacenter transport
author:Mohammad Alizadeh and
Shuang Yang and
Milad Sharif and
Sachin Katti and
Nick McKeown and
Balaji Prabhakar and
Scott Shenker
pFabric: Minimal Near-Optimal Datacenter Transport
Mohammad Alizadeh†‡, Shuang Yang†, Milad Sharif†, Sachin Katti†,
Nick McKeown†, Balaji Prabhakar†, and Scott Shenker§
{alizade, shyang, msharif, skatti, nickm, balaji}@stanford.edu PI:EMAIL
†Stanford University
‡Insieme Networks
§U.C. Berkeley / ICSI
ABSTRACT
In this paper we present pFabric, a minimalistic datacenter trans-
port design that provides near theoretically optimal ﬂow comple-
tion times even at the 99th percentile for short ﬂows, while still
minimizing average ﬂow completion time for long ﬂows. More-
over, pFabric delivers this performance with a very simple design
that is based on a key conceptual insight: datacenter transport should
decouple ﬂow scheduling from rate control. For ﬂow scheduling,
packets carry a single priority number set independently by each
ﬂow; switches have very small buffers and implement a very sim-
ple priority-based scheduling/dropping mechanism. Rate control is
also correspondingly simpler; ﬂows start at line rate and throttle
back only under high and persistent packet loss. We provide the-
oretical intuition and show via extensive simulations that the com-
bination of these two simple mechanisms is sufﬁcient to provide
near-optimal performance.
Categories and Subject Descriptors: C.2.1 [Computer-Communication
Networks]: Network Architecture and Design
General Terms: Design, Performance
Keywords: Datacenter network, Packet transport, Flow scheduling
1.
INTRODUCTION
Datacenter workloads impose unique and stringent requirements
on the transport fabric. Interactive soft real-time workloads such
as the ones seen in search, social networking, and retail generate a
large number of small requests and responses across the datacen-
ter that are stitched together to perform a user-requested compu-
tation (e.g., delivering search results). These applications demand
low latency for each of the short request/response ﬂows, since user-
perceived performance is dictated by how quickly responses to all
(or a large fraction of) the requests are collected and delivered back
to the user. However in currently deployed TCP-based fabrics,
the latency for these short ﬂows is poor — ﬂow completion times
(FCT) can be as high as tens of milliseconds while in theory these
ﬂows could complete in 10-20 microseconds. The reason is that
these ﬂows often get queued up behind bursts of packets from large
ﬂows of co-existing workloads (such as backup, replication, data
mining, etc) which signiﬁcantly increases their completion times.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright 2013 ACM 978-1-4503-2056-6/13/08 ...$15.00.
Motivated by this observation, recent research has proposed new
datacenter transport designs that, broadly speaking, use rate con-
trol to reduce FCT for short ﬂows. One line of work [3, 4] im-
proves FCT by keeping queues near empty through a variety of
mechanisms (adaptive congestion control, ECN-based feedback,
pacing, etc) so that latency-sensitive ﬂows see small buffers and
consequently small latencies. These implicit techniques generally
improve FCT for short ﬂows but they can never precisely determine
the right ﬂow rates to optimally schedule ﬂows. A second line of
work [21, 14] explicitly computes and assigns rates from the net-
work to each ﬂow in order to schedule the ﬂows based on their sizes
or deadlines. This approach can potentially provide very good per-
formance, but it is rather complex and challenging to implement in
practice because accurately computing rates requires detailed ﬂow
state at switches and also coordination among switches to identify
the bottleneck for each ﬂow and avoid under-utilization (§2).
Our goal in this paper is to design the simplest possible datacen-
ter transport scheme that provides near-optimal ﬂow completion
times, even at the 99th percentile for latency-sensitive short ﬂows.
To this end, we present pFabric,1 a minimalistic datacenter fabric
whose entire design consists of the following:
• End-hosts put a single number in the header of every packet
that encodes its priority (e.g., the ﬂow’s remaining size, dead-
line). The priority is set independently by each ﬂow and no
coordination is required across ﬂows or hosts to compute it.
• Switches are simple; they have very small buffers (e.g., 36KB
per port in our evaluation) and decide which packets to ac-
cept into the buffer and which ones to schedule strictly ac-
cording to the packet’s priority number. When a new packet
arrives and the buffer is full, if the incoming packet has lower
priority than all buffered packets, it is dropped. Else, the low-
est priority packet in the buffer is dropped and replaced with
the incoming packet. When transmitting, the switch sends
the packet with the highest priority. Thus each switch oper-
ates independently in a greedy and local fashion.
• Rate control is minimal; all ﬂows start at line-rate and throttle
their sending rate only if they see high and persistent loss.
Thus rate control is lazy and easy to implement.
pFabric thus requires no ﬂow state or complex rate calculations at
the switches, no large switch buffers, no explicit network feedback,
and no sophisticated congestion control mechanisms at the end-
host. pFabric is a clean-slate design; it requires modiﬁcations both
at the switches and the end-hosts. We also present a preliminary de-
sign for deploying pFabric using existing switches, but a full design
for incremental deployment is beyond the scope of this paper.
1pFabric was ﬁrst introduced in an earlier paper [5] which sketched
a preliminary design and initial simulation results.
435The key conceptual insight behind our design is the observa-
tion that rate control is a poor and ineffective technique for ﬂow
scheduling and the mechanisms for the two should be decoupled
and designed independently. In pFabric, the priority-based packet
scheduling and dropping mechanisms at each switch ensure that it
schedules ﬂows in order of their priorities. Further, the local and
greedy decisions made by each switch lead to an approximately op-
timal ﬂow scheduling decision across the entire fabric (§4.3). Once
ﬂow scheduling is handled, rate control’s only goal is to avoid per-
sistently high packet drop rates. Hence, the rate control design gets
correspondingly simpler: start at line rate and throttle only if band-
width is being wasted due to excessive drops.
We evaluate our design with detailed packet-level simulations
in ns2 [15] using two widely used datacenter workloads: one that
mimics a web application workload [3] and one that mimics a typ-
ical data mining workload [12]. We compare pFabric with four
schemes: an ideal scheme which is theoretically the best one could
do, the state-of-the-art approach for datacenter transport, PDQ [14],
as well as DCTCP [3] and TCP. We ﬁnd that:
• pFabric achieves near-optimal ﬂow completion times. Fur-
ther, pFabric delivers this not just at the mean, but also at the
99th percentile for short ﬂows at loads as high as 80% of the
network fabric capacity. pFabric reduces the FCT for short
ﬂows compared to PDQ and DCTCP by more than 40% and
2.5–4× respectively at the mean, and more than 1.5–3× and
3–4× respectively at the 99th percentile.
• With deadline driven workloads, pFabric can support a much
larger number of ﬂows with deadlines as well as much tighter
deadlines compared to PDQ. For instance, even for deadlines
where the slack with respect to the lowest possible FCT is
only 25%, pFabric meets the deadline for 99% of the ﬂows
(about 2× more than PDQ) at 60% network load.
• If the network designer has detailed knowledge of the ﬂow
size distribution in advance and carefully tunes parameters
such as the ﬂow size thresholds for each priority queue, min-
imum buffer per priority queue, etc pFabric can be approxi-
mated using existing priority queues in commodity switches.
This approach provides good performance too, but we ﬁnd
that it is rather brittle and sensitive to several parameters that
change in a datacenter due to ﬂow and user dynamics.
2. RELATED WORK
Motivated by the shortcomings of TCP, a number of new data-
center transport designs have been proposed in recent years. We
brieﬂy contrast our work with the most relevant prior work. As
discussed earlier, broadly speaking, the previous efforts all use rate
control to reduce ﬂow completion time.
Implicit rate control: DCTCP [3] and HULL [4] try to keep the
fabric queues small or empty by employing an adaptive congestion
control algorithm based on ECN and other mechanisms such as op-
erating the network at slightly less than 100% utilization, packet
pacing, etc to appropriately throttle long elephant ﬂows. Conse-
quently, the latency-sensitive ﬂows see small buffers and laten-
cies. D2TCP [18], a recently proposed extension to DCTCP, adds
deadline-awareness to DCTCP by modulating the window size based
on both deadline information and the extent of congestion. While
these schemes generally improve latency, they are fundamentally
constrained because they can never precisely estimate the right ﬂow
rates to use so as to schedule ﬂows to minimize FCT while ensur-
ing that the network is fully utilized. Furthermore, due to the bursty
nature of trafﬁc, keeping network queues empty is challenging and
requires carefully designed rate control and hardware packet pacing
at the end-hosts and trading off network utilization [4].
Explicit rate control: Having recognized the above limitations,
subsequent work explicitly assigns a sending rate to each ﬂow in
order to schedule ﬂows based on some notion of urgency. The as-
signed rates are typically computed in the network based on ﬂow
deadlines or their estimated completion time. D3 [21] ﬁrst pro-
posed using deadline information in conjunction with explicit rate
control to associate rates to ﬂows. D3 allocates bandwidth on a
greedy ﬁrst-come-ﬁrst-served basis and does not allow preemptions
and has thus been shown to lead to sub-optimal ﬂow scheduling
since a near-deadline ﬂow can be blocked waiting for a far-deadline
ﬂow that arrived earlier [14].
The most closely related work to pFabric and in fact the state-
of-the-art approach in this space is PDQ [14]. PDQ was the ﬁrst to
point out that minimizing FCTs requires preemptive ﬂow schedul-
ing and attempts to approximate the same ideal ﬂow scheduling
algorithm as pFabric to minimize average FCT or missed deadlines
(§3). However, like D3, PDQ’s ﬂow scheduling mechanism is also
based on switches assigning rates to individual ﬂows using explicit
rate control. In PDQ, on packet departure, the sender attaches a
scheduling header to the packet that contains several state variables
including the ﬂow’s deadline, its expected transmission time, and
its current status such as its sending rate and round-trip-time. Each
switch then maintains this state for some number of outstanding
ﬂows and uses it to decide how much bandwidth to allocate to each
ﬂow and which ﬂows to “pause”.
PDQ provides good performance but is quite challenging and
complex to implement in practice. Since the switches on a ﬂow’s
path essentially need to agree on the rate that is to be assigned to
the ﬂow, PDQ needs to pass around state regarding a ﬂow’s rate and
which switch (if any) has paused the ﬂow. Further, since switches
need to be aware of the active ﬂows passing through them, in PDQ,
every ﬂow must begin with a SYN and terminate with a FIN so
that switches can perform the required book-keeping. This one ex-
tra round-trip of latency on every ﬂow may not be acceptable be-
cause most latency sensitive ﬂows in datacenters are small enough
to complete in just one RTT.2 Thus, requiring the network to ex-
plicitly and efﬁciently assign a rate to each ﬂow requires detailed
ﬂow state (size, deadline, desired rate, current rate, round-trip time,
etc) at switches and also coordination among switches to identify
the bottleneck for each ﬂow and avoid under-utilization. This is a
major burden, both in terms of communication overhead and requi-
site state at switches, particularly in the highly dynamic datacenter
environment where ﬂows arrive and depart at high rates and the
majority of ﬂows last only a few RTTs [12, 7].
Load balancing: Finally, there are a number of proposals on efﬁ-
cient load balancing techniques for datacenter fabrics [2, 17, 22].
Better load balancing of course reduces hotspots and thus helps re-
duce ﬂow completion time, however the techniques and goals are
orthogonal and complementary to pFabric.
3. CONCEPTUAL MODEL
Our conceptual viewpoint in designing our ﬂow scheduling tech-
nique is to abstract out the entire fabric as one giant switch. Specif-
ically, the datacenter fabric typically consists of two or three tiers
of switches in a Fat-tree or Clos topology [1, 12]. Instead of focus-
ing on the individual switches, the whole fabric can be abstracted
as one giant switch that interconnects the servers as shown in Fig-
ure 1. The ingress queues into the fabric switch are at the NICs and
the egress queues out of the fabric switch are at the last-hop TOR
2In measurements from a production datacenter of a large cloud
provider, more than 50% of the ﬂows were observed to be less than
1KB [12] — just a single packet.
436!"#$%&&'()%)%&'
*+$&,'-./'0,'1!2&3'
4#$%&&'()%)%&'
*50&,'-./'0,'678&3'
1 
2 
3 
1 
2 
3 
Figure 1: Conceptual view of ﬂow scheduling over a datacenter
fabric.
Algorithm 1 Ideal ﬂow scheduling algorithm.
Input: F = List of active ﬂows with their ingress and egress port and re-
maining size. The algorithm is run each time F changes (a ﬂow arrives
or departs).
Output: S = Set of ﬂows to schedule (at this time).
1: S ← ∅
2: ingressBusy[1..N] ← F ALSE
3: egressBusy[1..N] ← F ALSE
4: for each ﬂow f ∈F , in increasing order of remaining size do
5:
if ingressBusy[f.ingressP ort] == F ALSE and
egressBusy[f.egressP ort] == F ALSE then
S.insert(f )
ingressBusy[f.ingressP ort] ← T RU E
egressBusy[f.egressP ort] ← T RU E
6:
7:
8:
9:
end if
10: end for
11: return S.
switches. Each ingress port (source NIC) has some ﬂows destined
to various egress ports. It is convenient to view these as organized
in virtual output queues at the ingress as shown in Figure 1. For ex-
ample, the red and blue ﬂows at ingress 1 are destined to egress 1,
while the green ﬂow is destined to egress 3.
In this context, transport over the datacenter fabric can essen-
tially be thought of as scheduling ﬂows over the backplane of a
giant switch. The problem is to ﬁnd the best schedule to mini-
mize the average FCT (or maximize the number of deadlines met).
Since datacenter workloads are dominated by large numbers of
short ﬂows, minimizing average FCT will ensure that the short,
high-priority ﬂows see very low latency.
Optimal ﬂow scheduling: The optimal algorithm for minimizing
average FCT when scheduling over a single link is the Shortest Re-
maining Processing Time (SRPT) policy which always schedules
the ﬂow that has the least work remaining. However, we are not
scheduling over a single link but rather over an entire fabric with
a set of links connecting the ingress and egress queues. Unfortu-
nately, a simple universally optimal policy does not exist for simul-
taneously scheduling multiple links. In fact, even under the simpli-
fying assumption that the fabric core can sustain 100% throughput
and that only the ingress and egress access links are potential bot-
tlenecks, the scheduling problem for minimizing the average FCT
is equivalent to the NP-hard sum-multicoloring problem [9]. For-
tunately, a simple greedy algorithm is theoretically guaranteed to
provide near-ideal performance. This Ideal algorithm schedules
ﬂows across the fabric in non-decreasing order of the remaining
ﬂow size and in a maximal manner such that at any time a ﬂow is
blocked if and only if either its ingress port or its egress port is busy
serving a different ﬂow with less data remaining. The pseudo code
is provided in Algorithm 1. This algorithm has been theoretically
proven to provide at least a 2-approximation to the optimal average
FCT [9]. In practice we ﬁnd that the actual performance is even
closer to optimal (§5). The takeaway is that the greedy scheduler in
Algorithm 1 that prioritizes small ﬂows over large ﬂows end-to-end
across the fabric can provide near-ideal average FCT.
It is important to note that the Ideal algorithm is not plagued
by the inefﬁciencies that inevitably occur in an actual datacenter
transport design. It does not have rate control dynamics, buffering
(and its associate delays), packet drops, retransmissions, or inefﬁ-
ciency due to imperfect load-balancing. It only captures one thing:
the (best-case) delays associated with ﬂows contending for band-
width at the ingress and egress fabric ports. Consequently, the per-
formance of this algorithm for a given workload serves as bench-
mark to evaluate any scheme that aims to minimize ﬂow completion
times. The key contribution of this paper is to show that a very sim-
ple distributed transport design can approximate the performance
of the Ideal algorithm with remarkable ﬁdelity.
Remark 1. For simplicity, the above discussion assumed that all
the edge links run at the same speed, though the Ideal algorithm
can easily be generalized. See Hong et al. [14] for more details.
4. DESIGN
pFabric’s key design insight is a principled decoupling of ﬂow
scheduling from rate control. This leads to a simple switch-based
technique that takes care of ﬂow scheduling and consequently also
simpliﬁes rate control. In this section we describe pFabric’s switch
and rate controller designs. We explain why pFabric’s simple mech-
anisms are sufﬁcient for near-ideal ﬂow scheduling and discuss
some practical aspects regarding its implementation.
Packet priorities: In pFabric, each packet carries a single num-
ber in its header that encodes its priority. The packet priority can
represent different things depending on the scheduling objective.
For instance, to approximate the Ideal algorithm (Algorithm 1) and
minimize average FCT (our main focus in this paper), we would
ideally set the priority to be the remaining ﬂow size when the packet
is transmitted. For trafﬁc with deadlines, to maximize the number
of deadlines met, we would set the priority to be the deadline itself
quantized in some unit of time. Other simpliﬁcations such as using
absolute ﬂow size instead of remaining ﬂow size are also possible
(§4.4). Similar to prior work [21, 14], we assume that the required
information (e.g., ﬂow size or deadline) is available at the transport
layer which then sets the packet priorities.
4.1 Switch Design
The pFabric switch uses two simple and local mechanisms:
• Priority scheduling: Whenever a port is idle, the packet
with the highest priority buffered at the port is dequeued and
sent out.
• Priority dropping: Whenever a packet arrives to a port with
a full buffer, if it has priority less than or equal to the lowest
priority packet in the buffer, it is dropped. Otherwise, the
packet with the lowest priority is dropped to make room for
the new packet.