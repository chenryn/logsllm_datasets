### 3.3 Log-linear Models

#### 3.3.1 Description
Let \( N \) be the unknown number of distinct individuals in the population. Let \( t \) denote the number of sources, indexed by \( 1, 2, \ldots, t \). For each individual, let \( s_1, s_2, \ldots, s_t \) be defined such that \( s_i = 1 \) if the individual is observed in sample \( i \), and \( s_i = 0 \) otherwise. The string \( s_1s_2 \ldots s_t \) is called the "capture history" of the individual. The observed outcome of all measurements can then be represented by variables of the form \( z_s \), which are the numbers of individuals with each capture history \( s = s_1s_2 \ldots s_t \). These are assumed to be instances of random variables \( Z_s \). Note that individuals with the capture history \( 00 \ldots 0 \) are unobserved, and our goal is to estimate \( Z_{00\ldots0} \). Table 1 illustrates the variables \( Z_s \) for each possible capture history for three sources (\( t = 3 \)); "yes" means an individual was observed, and "no" means an individual was not observed by a source.

For each history \( s \), let \( h(s) \) be the set of sources in which the individual occurs; for example, \( h(101) = \{1, 3\} \). Define the indicator function \( 1_A = 1 \) if statement \( A \) is true and \( 0 \) otherwise. We can now write the following system of equations in \( 2^t \) variables \( u, u_1, u_2, \ldots, u_{12}, \ldots, u_{23}, \ldots, u_{12\ldots t} \):

\[
\log(E(Z_s)) = \sum_{h \subseteq h(s)} u_h
\]

For example, for \( t = 3 \), the system is:

\[
\log(E(Z_{ijk})) = u + u_1 \cdot 1_{i=1} + u_2 \cdot 1_{j=1} + u_3 \cdot 1_{k=1} + u_{12} \cdot 1_{i=1 \land j=1} + u_{13} \cdot 1_{i=1 \land k=1} + u_{23} \cdot 1_{j=1 \land k=1} + u_{123} \cdot 1_{i=1 \land j=1 \land k=1}
\]

The estimate of \( Z_{00\ldots0} \) is then \( \hat{Z}_{00\ldots0} = \exp(u) \).

If we take \( E[Z_s] = z_s \), then this system has \( 2^t \) unknowns but only \( 2^t - 1 \) equations, as \( Z_{00\ldots0} \) is unknown. Hence, it is customary to assume \( u_{12\ldots t} = 0 \) [17]. As the number of sources \( t \) increases, this \( t \)-way dependency becomes less important.

The model with all \( u_h \) (the saturated model) is very sensitive to small values of \( Z_s \). For example, a zero count for some capture history may give \( \hat{Z}_{00\ldots0} = 0 \), regardless of the other \( Z_s \) [17]. Furthermore, the larger \( t \) is, the higher the random error for some \( Z_s \). Including "noisy" parameters \( u_h \) in a model results in poor predictive performance (referred to as overfitting). Overfitting is mitigated by "model selection" (see Section 3.3.2), in which some \( u_h \) are forced to 0, to reflect assumed independence between certain combinations of sources. For example, setting \( u_{12} = 0 \) indicates sources 1 and 2 are independent. With such incomplete models, the system of equations is overdetermined, and the maximum likelihood parameters \( u \) are typically used, based on the assumption that \( Z_s \) result from uniform random sampling and are hence Poisson distributed.

Assuming the \( Z_s \) are Poisson distributed is appropriate if the upper limit for the \( Z_s \) is unknown. However, we can bound \( Z_s \) by the size of the publicly routed IPv4 space. Hence, we use right-truncated Poisson distributions defined over \([0, l] \cap \mathbb{Z}\), where \( l \) is the upper limit. These improve estimates substantially for small strata, where the counters are relatively close to the limit (see Section 5.2), but otherwise make little difference.

#### 3.3.2 Model Selection
Model selection for a log-linear model (LLM) consists of selecting which \( u_h \) will be assumed a priori to be 0. The goal is to select the least complex model with "adequate" fit of the observed (and by assumption) unobserved individuals [20].

A common approach is to minimize an "Information Criterion" (IC). Two common ICs are [21]:

\[
\text{AIC} = 2k - 2 \log(L)
\]
\[
\text{BIC} = \log(M)k - 2 \log(L)
\]

where \( L \) is the likelihood of the data given the assumed model, \( k \) is the number of free parameters of the model, and \( M \) is the number of observed individuals. AIC is used more often, but each has its merits [22]. Section 5 compares the BIC and the AIC for our data. We choose the simplest model \( m \) such that no other model \( n \) has \( \text{IC}_n < \text{IC}_m - 7 \) [21].

In our case, \( k \) is the number of non-zero \( u_h \), but \( L \) is difficult to obtain. AIC and BIC assume that each source samples uniformly, so \( L \) is the likelihood of a Poisson model. If the number of samples is large, the central limit theorem indicates that substantial deviations from the mean have very low likelihood. In our case, as in [17, 19], the randomness comes largely from the choice of sources to monitor, which is hard to characterize but has substantially higher variance. Hence, the Poisson assumption selects too complex a model.

We mitigate this overfitting using the simple heuristic of dividing all \( z_s \) by some integer \( d \) when calculating \( L \). It remains to select \( d \). If \( d \) is so large that any \( z_s \) gets rounded to zero, the LLM breaks down. The further heuristic of selecting \( d \) to be the largest number less than \( \min_s z_s \) appears to work well (see Section 5.1).

#### 3.3.3 Estimate Range
Besides computing point estimates, we also compute estimate ranges (used in Section 5). We use the procedure in [23] to compute a \( 100 (1 - \alpha) \% \) profile likelihood "confidence interval" (CI) for \( \hat{N} \). Note that this is not a true confidence interval in our case, since it is based on the assumption that each sample is drawn randomly, resulting in a Poisson number of samples with each history. In contrast, our samples arise from different, not completely random sampling procedures. Hence, we treat these "confidence intervals" as merely a useful heuristic indication of the sensitivity to modeling variations and set \( \alpha = 10^{-7} \) to obtain wide CIs.

#### 3.3.4 Sampling Zeros
Even with appropriate model selection, if the number of samples across all sources is low, we may have a large number of \( z_s \) that are near zero, leading to unreliable estimates. In our case, this only occurs for a few small countries or territories when stratifying by country code (see Section 3.4). Hence, we exclude country codes with fewer than 1000 IP addresses observed by all sources from the results in Section 6.2 (where they are negligible) and in Section 6.7.

### 3.4 Stratification
We obtain more insight and initially hoped to mitigate heterogeneity by stratifying the population in different ways. We classified IPv4 addresses as statically or dynamically assigned using the approach described in [10], and based on allocation and whois data, we stratified by RIR (e.g., APNIC), country, prefix size, industry, and allocation age.

### 4. Datasets and Preprocessing
An IPv4 address is considered used if it responds to active probes or participates in connections. A used /24 subset contains one or more used addresses. This section describes our sources of used IPv4 address data, our data collection and processing, and our handling of both spoofed and dynamically assigned addresses.

#### 4.1 Datasets
Our first two datasets are from actively probing the whole allocated IPv4 Internet using ICMP echo requests (IPING) and TCP SYN packets to port 80 (TPING). Since mid-2011, we probed each allocated IPv4 address (a census) once every 6 months. The first two censuses used ICMP probing, and the rest used both ICMP and TCP probing (with TCP probing seeing over 7% more observed addresses). We limited the overall ping rate and used reversed bit counting for "traversing" the IP space. On average, our prober sent only one packet every two hours to individual /24 networks, to minimize congestion, stay below typical ICMP or TCP rate limit thresholds, and avoid triggering monitoring systems (on average, we received only 10–20 complaints per census). For the first half of 2011, we use ICMP ping data collected by USC/LANDER [24]. We utilize data gathered from 2011 onwards. We generate datasets of unique /24 subnets by processing the IPv4 datasets and setting the last octet of each address to zero, then filtering out the duplicates. Table 2 shows the number of unique IPv4 addresses and /24 subnets per dataset for the years 2011–2013 (IPs for GAME omitted for confidentiality). Note that the numbers in the table cannot be used as growth trends due to sample method variations.

#### 4.2 Host Types Sampled
We collected data from diverse locations, but CR estimates will only be useful if (1) our datasets sample all types of devices using public IPv4 addresses and (2) a type of device can be sampled by multiple datasets. We now discuss whether this is the case based on grouping devices into routers, servers/proxies, clients (e.g., PCs, smartphones), and specialized devices (e.g., printers, cameras).

ISP routers are sampled by IPING and TPING and may also appear in SWIN and CALT. Home routers are sampled by IPING and TPING (we confirmed that some responses came from Cable/DSL routers by inspecting web pages from IPs responding to TPING) and by all other sources (with NAT packets sent from home networks appearing to come from home routers). Servers/proxies are sampled by IPING, TPING, SWIN, and CALT. They can also appear in WIKI, SPAM, and WEB. Clients are sampled mainly by WIKI, SPAM, MLAB, WEB, GAME, SWIN, and CALT, but also appear in IPING. NAT'ed clients also appear in IPING and TPING. Specialized devices may be sampled by IPING and TPING.

Overall, our datasets sample most groups well, especially servers and clients, which we assume are the largest groups. Specialized devices are likely severely under-represented, but these are very hard to sample. The authors of [5] probed the entire IPv4 Internet on several hundred ports and detected 36 million addresses that only responded to TCP SYNs but not to ICMP. In our censuses, 15–20 million IPs reacted to port 80 TCP SYNs but not to ICMP. The difference of 15–20 million addresses could be specialized devices listening only on specific ports we missed, but this number is small compared to our total estimate of 1.2 billion used addresses.

#### 4.3 Time Windows
We collected data from 1 Jan 2011 until 30 June 2014. To analyze the growth trend of used IPv4 addresses, we split our data into overlapping 12-month windows. Windows start every three months, so the first window starts at 1 Jan 2011 and the last window starts at 1 Jul 2013. The last window ends at 30 June 2014. This is a suitable trade-off between temporal resolution and noisy estimates.

### Table 2: Data Sources and Observed Unique IPv4 Addresses and /24 Subnets per Year (SWIN and CALT after Spoofed IP Filtering)

| Description | Time Collected | 2011 | 2012 | 2013 |
|-------------|----------------|------|------|------|
| Wikipedia’s page edit histories | Jan 2011 – Jun 2014 | 5.5 | - | 30.0 |
| Potential spam email senders | May 2012 – Jun 2014 | - | 22.0 | - |
| Clients tested by Measurement Lab | Jan 2011 – Jun 2014 | - | - | 22.0 |
| Web clients tested for IPv6 | Mar 2011 – Jun 2014 | - | - | 22.0 |
| Game clients logged into Valve’s Steam | Jan 2011 – Jun 2014 | conf | - | - |
| Swinburne access router NetFlow records | Jan 2011 – Jun 2014 | 150.6 | - | - |
| Caltech access router NetFlow records | Jun 2013 – Jun 2014 | - | - | - |
| ICMP ping census of IPv4 Internet | Mar 2011 – Jun 2014 | 320.3 | - | - |
| TCP port 80 census of IPv4 Internet | Mar 2012 – Jun 2014 | - | 1.69 | - |

Note: The numbers in the table cannot be used as growth trends due to sample method variations.