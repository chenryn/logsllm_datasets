ory is referenced through a dangling pointer.
Memory allocator designers have considered using
out-of-band metadata before, because attackers targeted
in-band heap metadata in several ways: attacker con-
trolled data in freed objects can be interpreted as heap-
metadata through double-free vulnerabilities, and heap-
based overﬂows can corrupt allocator metadata adjacent
to heap-based buffers. If the allocator uses corrupt heap
metadata during its linked list operations, attackers can
write an arbitrary value to an arbitrary location.
Although out-of-band heap metadata can solve these
problems, some memory allocators mitigate heap meta-
data corruption without resorting to this solution. For
example, attacks corrupting heap metadata can be ad-
dressed by detecting the use of corrupted metadata with
sanity checks on free list pointers before unlinking a free
chunk or using heap canaries [19] to detect corruption
due to heap-based buffer overﬂows. In some cases, cor-
ruption can be prevented in the ﬁrst place, e.g. by detect-
ing attempts to free objects already in a free list. These
techniques avoid the memory overhead of out-of-band
metadata, but are insufﬁcient for preventing use-after-
free vulnerabilities, where no corruption of heap meta-
data takes place.
An approach to address this problem in allocator de-
signs reusing free memory for heap metadata is to ensure
that these metadata point to invalid memory if interpreted
as pointers by the application. Merely randomizing the
metadata by XORing with a secret value may not be suf-
ﬁcient in the face of heap spraying. One option is setting
the top bit of every metadata word to ensure it points to
protected kernel memory, raising a hardware fault if the
program dereferences a dangling pointer to heap meta-
data, while the allocator would ﬂip the top bit before
using the metadata. However, it is still possible that
the attacker can tamper with the dangling pointer before
dereferencing it. This approach may be preferred when
modifying an existing allocator design, but for Cling, we
chose to keep metadata out-of-band instead.
An allocator can keep its metadata outside deallo-
cated memory using non-intrusive linked lists (next and
prev pointers stored outside objects) or bitmaps. Non-
intrusive linked lists can have signiﬁcant memory over-
head for small allocations, thus Cling uses a two-level
allocation scheme where non-intrusive linked lists chain
large memory chunks into free lists and small allocations
are carved out of buckets holding objects of the same
size class using bitmaps. Bitmap allocation schemes
have been used successfully in popular memory alloca-
tors aiming for performance [10], so they should not pose
an inherent performance limitation.
3.2 Type-Safe Address Space Reuse
The second requirement protects against use-after-free
vulnerabilities where the memory pointed by the dan-
gling pointer has been reused by some other object.
As we saw in Section 2.3, constraining dangling point-
ers to objects within pools of the same type and align-
ment thwarts a large class of attacks exploiting use-after-
free vulnerabilities, including all those used in real at-
tacks. A runtime memory allocator, however, must ad-
dress two challenges to achieve this. First, it must bridge
the semantic gap between type information available to
the compiler at compile time and memory allocation re-
quests received at runtime that only specify the number
of bytes to allocate. Second, it must address the memory
overheads caused by constraining memory reuse within
pools. Dhurjati et al. [9], who proposed type-safe mem-
ory reuse for security, preclude an efﬁcient implemen-
tation without using a compile time pointer and region
analysis.
To solve the ﬁrst challenge, we observe that security
is maintained even if memory reuse is over-constrained,
i.e. several allocation pools may exist for the same type,
as long as memory reuse across objects of different types
is prevented. Another key observation is that in C/C++
programs, an allocation site typically allocates objects of
a single type or arrays of objects of a single type, which
can safely share a pool. Moreover, the allocation site
is available to the allocation routines by inspecting their
call stack. While different allocation sites may allocate
objects of the same type that could also safely share the
same pool, Cling’s inability to infer this could only af-
fect performance—not security. Section 4 shows that
in spite of this pessimization, acceptable performance is
achieved.
The immediate caller of a memory allocation routine
can be efﬁciently retrieved from the call stack by inspect-
ing the saved return address. However, multiple tail-call
optimizations in a single routine, elaborate control ﬂow,
and simple wrappers around allocation routines may ob-
scure the true allocation site. The ﬁrst two issues are suf-
ﬁciently rare to not undermine the security of the scheme
in general. These problems are elaborated in Section 3.6,
and ways to address simple wrappers are described in
Section 3.5.
illustrated in Figure 4,
A further complication,
is
caused by array allocations and the lack of knowledge of
array element sizes. As discussed, all new objects must
be aligned to previously allocated objects, to ensure their
ﬁelds are aligned one to one. This requirement also ap-
plies to array elements. Figure 4, however, illustrates
that this constraint can be violated if part of the mem-
ory previously used by an array is subsequently reused
by an allocation placed at an arbitrary offset relative to
cause it has one pool per size-class per allocation site,
instead of just one pool per type.
The key observation to avoid excessive memory over-
head is that physical memory, unlike address space, can
be safely reused across pools. Cling borrows ideas from
previous memory allocators [11] designed to manage
physical memory in blocks (via mmap) rather than mono-
tonically growing the heap (via sbrk). These allocators
return individual blocks of memory to the operating sys-
tem as soon as they are completely free. This technique
allows Cling to reuse blocks of memory across different
pools.
Cling manages memory in blocks of 16K bytes, satis-
fying large allocations using contiguous ranges of blocks
directly, while carving smaller allocations out of homo-
geneous blocks called buckets. Cling uses an OS prim-
itive (e.g. madvise) to inform the OS it can reuse the
physical memory of freed blocks.
Deallocated memory accessed through a dangling
pointer will either continue to hold the data of the in-
tended object, or will be zero-ﬁlled by the OS, trigger-
ing a fault if a pointer ﬁeld stored in it is dereferenced.
It is also possible to page protect address ranges after
relinquishing their memory (e.g. using mechanisms like
mprotect on top of madvise).
Cling does not suffer from fragmentation as the naive
scheme described in Section 2.2, because it allows imme-
diate reuse of small allocations’ memory within a pool.
Address space consumption is also more reasonable: it
is proportional to the number of allocation sites in the
program, so it does not leak over time as in the naive
solution, and is easily manageable in modern 64-bit ma-
chines.
3.3 Heap Organization
Cling’s main heap is divided into blocks of 16K bytes.
As illustrated in Figure 5, a smaller address range,
dubbed the meta-heap, is reserved for holding block de-
scriptors, one for each 16K address space block. Block
descriptors contain ﬁelds for maintaining free lists of
block ranges, storing the size of the block range, asso-
ciating the block with a pool, and pointers to metadata
for blocks holding small allocations. Metadata for block
ranges are only set for the ﬁrst block in the range—the
head block. When address space is exhausted and the
heap is grown, the meta-heap is grown correspondingly.
The purpose of this meta-heap is to keep heap metadata
separate, allowing reuse of the heap’s physical memory
previously holding allocated data without discarding its
metadata stored in the meta-heap.
While memory in the heap area can be relinquished
using madvise, metadata about address space must be
kept in the meta-heap area, thus contributing to the mem-
Figure 4: Example of unsafe reuse of array memory, even
with allocation pooling, due to not preserving allocation
offsets.
the start of the old allocation. Reusing memory from
a pool dedicated to objects of the same type is not suf-
ﬁcient for preventing this problem. Memory reuse must
also preserve offsets within allocated memory. One solu-
tion is to always reuse memory chunks at the same offset
within all subsequent allocations. A more constraining
but simpler solution, used by Cling, is to allow memory
reuse only among allocations of the same size-class, thus
ensuring that previously allocated array elements will be
properly aligned with array elements subsequently occu-
pying their memory.
This constraint also addresses the variable sized struct
idiom, where the ﬁnal ﬁeld of a structure, such the fol-
lowing one, is used to access additional, variable size
memory allocated at the end of the structure:
1 struct {
2
3
4
5 };
void (*fp)();
int len;
char buffer[1];
By only reusing memory among instances of such struc-
tures that fall into the same size-class, and always align-
ing such structures at the start of this memory, Cling pre-
vents the structure’s ﬁelds, e.g. the function pointer fp in
this example, from overlapping after their deallocation
with buffer contents of some other object of the same
type.
The second challenge is to address the memory over-
head incurred by pooling allocations. Dhurjati et al. [8]
observe that the worst-case memory use increase for a
program with N pools would be roughly a factor of
N − 1: when a program ﬁrst allocates data of type A,
frees all of it, then allocates data of type B, frees all of
it, and so on. This situation is even worse for Cling, be-
TimeMemoryArray 1Elem 0Elem 1Elem 2Elem 3Elem 4Elem 1Elem 0Array 216K Block16K Blockory overhead of the scheme. This overhead is small. A
block descriptor can be under 32 bytes in the current im-
plementation, and with a block size of 16K, this corre-
sponds to memory overhead less than 0.2% of the ad-
dress space used, which is small enough for the address
space usage observed in our evaluation. Moreover, a
hashtable could be employed to further reduce this over-
head if necessary.
Both blocks and block descriptors are arranged in
corresponding linear arrays, as illustrated in Figure 5,
so Cling can map between address space blocks and
their corresponding block descriptors using operations
on their addresses. This allows Cling to efﬁciently re-
cover the appropriate block descriptor when deallocating
memory.
Figure 5: Heap comprised of blocks and meta-heap of
block descriptors. The physical memory of deallocated
blocks can be scrapped and reused to back blocks in other
pools.
Cling pools allocations based on their allocation
To achieve this, Cling’s public memory al-
site.
location routines (e.g. malloc and new)
retrieve
their call site using the return address saved on the
Since Cling’s routines have complete con-
stack.
trol over their prologues,
the return address can al-
ways be retrieved reliably and efﬁciently (e.g. using the
__builtin_return_address GCC primitive). At
ﬁrst, this return address is used to distinguish between
memory allocation sites. Section 3.5 describes how to
discover and unwind simple allocation routine wrappers
in the program, which is necessary for obtaining a mean-
ingful allocation site in those cases.
Cling uses a hashtable to map allocation sites to pools
at runtime. An alternative design to avoid hash table
lookups could be to generate a trampoline for each call
site and rewrite the call site at hand to use its dedicated
trampoline instead of directly calling the memory allo-
cation routine. The trampoline could then call a version
of the memory allocation routine accepting an explicit
pool parameter. The hash table, however, was preferred
because it is less intrusive and handles gracefully cor-
ner cases including calling malloc through a function
pointer. Moreover, since this hash table is accessed fre-
quently but updated infrequently, optimizations such as
constructing perfect hashes can be applied in the future,
if necessary.
Pools are organized around pool descriptors. The rel-
evant data structures are illustrated in Figure 6. Each
pool descriptor contains a table with free lists for block
ranges. Each free list links together the head blocks of
block ranges belonging to the same size-class (a power of
two). These are blocks of memory that have been deal-
located and are now reusable only within the pool. Pool
descriptors also contain lists of blocks holding small al-
locations, called buckets. Section 3.4 discusses small ob-
ject allocation in detail.
Initially, memory is not assigned to any pool. Larger
allocations are directly satisﬁed using a power-of-two
range of 16K blocks. A suitable free range is reused from
the pool if possible, otherwise, a block range is allocated
by incrementing a pointer towards the end of the heap,
and it is assigned to the pool. If necessary, the heap is
grown using a system call. When these large allocations
are deallocated, they are inserted to the appropriate pool
descriptor’s table of free lists according to their size. The
free list pointers are embedded in block descriptors, al-
lowing the underlying physical memory for the block to
be relinquished using madvise.
3.4 Small Allocations
Allocations less than 8K in size (half the block size) are
stored in slots inside blocks called buckets. Pool de-
scriptors point to a table with entries to manage buck-
ets for allocations belonging to the same size class. Size
classes start from a minimum of 16 bytes, increase by 16
bytes up to 128 bytes, and then increase exponentially
up to the maximum of 8K, with 4 additional classes in
between each pair of powers-of-two. Each bucket is as-
sociated with a free slot bitmap, its element size, and a
bump pointer used for fast allocation when the block is
ﬁrst used, as described next.
Using bitmaps for small allocations seems to be a
design requirement for keeping memory overhead low
without reusing free memory for allocator metadata, so
it is critical to ensure that bitmaps are efﬁcient com-
pared to free-list based implementations. Some effort
has been put into making sure Cling uses bitmaps ef-
ﬁciently. Cling borrows ideas from reaps [5] to avoid
bitmap scanning when many objects are allocated from
an allocation site in bursts. This case degenerates to just
bumping a pointer to allocate consecutive memory slots.
All empty buckets are initially used in bump mode, and
BlockDescriptors(Never Scrapped)16 KiBBlockHeapMeta Heap......Resident BlockScrapped BlockFigure 6: Pool organization illustrating free lists of blocks available for reuse within the pool and the global hot bucket queue
that delays reclamation of empty bucket memory. Linked list pointers are not stored inside blocks, as implied by the ﬁgure, but
rather in their block descriptors stored in the meta-heap. Blocks shaded light gray have had their physical memory reclaimed.
stay in that mode until the bump pointer reaches the end
of the bucket. Memory released while in bump mode is
marked in the bucket’s bitmap but is not used for satis-
fying allocation requests while the bump pointer can be
used.
A pool has at most one bucket in bump mode per size
class, pointed by a ﬁeld of the corresponding table entry,
as illustrated in Figure 6. Cling ﬁrst attempts to satisfy an
allocation request using that bucket, if available. Buck-
ets maintain the number of freed elements in a counter. A
bucket whose bump pointer reaches the end of the bucket
is unlinked from the table entry and, if the counter in-
dicates it has free slots, inserted into a list of non-full
buckets. If no bucket in bump mode is available, Cling
attempts to use the ﬁrst bucket from this list, scanning
its bitmap to ﬁnd a free slot. If the counter indicates the
bucket is full after an allocation request, the bucket is
unlinked from the list of non-full buckets, to avoid ob-
stracting allocations.
Conversely, if the counter of free elements is zero prior
to a deallocation, the bucket is re-inserted into the list of
non-full buckets. If the counter indicates that the bucket
is completely empty after deallocation, it is inserted to a
list of empty buckets queuing for memory reuse. This ap-
plies even for buckets in bump mode (and was important
for keeping memory overhead low). This list of empty
buckets is consulted on allocation if there is neither a
bucket in bump mode, nor a non-full bucket. If this list is
also empty, a new bucket is created using fresh address
space, and initialized in bump mode.
Empty buckets are inserted into a global queue of hot
buckets, shown at the bottom of Figure 6. This queue has
a conﬁgurable maximum size (10% of non-empty buck-
ets worked well in our experiments). When the queue
512K256K128K64K32K16KPool968064483216SmallAllocationsLarge AllocationsNewBucketNon-fullBucketEmpy BucketEmpty BucketEmpty BucketFull BucketbitmapbitmapbitmapbitmapHot BucketQueueEmpty Bucket......Head BlockHead BlockHead BlockHead BlockHead BlockEmpty BucketEmpty BucketCold BucketsEmpty BucketsHot BucketsFromOther PoolsFull BucketsHead BlockFree Block RangesUsed Block RangeHot BucketsNon-fullBucketsPoolPool......Pool HashtableBucket inBump Modesize threshold is reached after inserting an empty bucket
to the head of the queue, a hot bucket is removed from
the tail of the queue, and becomes cold:
its bitmap is
deallocated, and its associated 16K of memory reused
via an madvise system call.
If a cold bucket is en-
countered when allocating from the empty bucket list of
a pool, a new bitmap is allocated and initialized. The
hot bucket queue is important for reducing the number
of system calls by trading some memory overhead, con-
trollable through the queue size threshold.
3.5 Unwinding Malloc Wrappers
Wrappers around memory allocation routines may con-
ceal real allocation sites. Many programs wrap malloc
simply to check its return value or collect statistics. Such
programs could be ported to Cling by making sure that
the few such wrappers call macro versions of Cling’s al-
location routines that capture the real allocation site, i.e.
the wrapper’s call site. That is not necessary, however,
because Cling can detect and handle many such wrap-
pers automatically, and recover the real allocation site by
unwinding the stack. This must be implemented care-