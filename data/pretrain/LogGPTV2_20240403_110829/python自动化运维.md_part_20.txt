- hosts： lbservers
user： root
roles：
- haproxy
- hosts： monitoring
user： root
roles：
- base-apache
- nagios
4.定义组变量
下面定义playbook全局变量，变量作用域为所有主机。
【group_vars/all】
---
# Variables here are applicable to all host groups
httpd_port： 80
ntpserver： 192.168.1.2
all文件定义了匹配所有主机作用域的变量，一般为系统公共类基础配置，如
ntpserver地址、sysctl变量、iptables配置等。
下面为定义webservers组的变量，变量作用域为webservers组主机。
【group_vars/webservers】
---
# Variables for the web server configuration
# Ethernet interface on which the web server should listen.
# Defaults to the first interface. Change this to：
#
# iface： eth1
#
# ...to override.
#
iface： '{{ ansible_default_ipv4.interface }}'
# this is the repository that holds our sample webapp
repository： https：//github.com/bennojoy/mywebapp.git
# this is the sha1sum of V5 of the test webapp.
webapp_version： 351e47276cc66b018f4890a04709d4cc3d3edb0d
webservers文件定义了webservers组作用域的变量。本示例涉及Apache相关配
置，其中“iface：'{{ansible_default_ipv4.interface}}'”引用了Facts获取的本地网卡
接口名信息，另外定义了一个GitHub的repository，方便下载Web测试文件，如内
部搭建git版本控制环境，此处也可以修改成本地的服务地址。
下面为定义dbservers组的变量，变量作用域为dbservers组主机。
【group_vars/dbservers】
---
# The variables file used by the playbooks in the dbservers group.
# These don't have to be explicitly imported by vars_files： they are autopopulated.
mysqlservice： mysqld
mysql_port： 3306
dbuser： root
dbname： foodb
upassword： abc
dbservers文件定义了dbservers组作用域变量，本示例涉及MySQL数据库的基
本应用信息。
下面为定义lbservers组作用域变量文件，本示例主要涉及haproxy环境涉及的
配置参数值。
【group_vars/lbservers】
---
# Variables for the HAproxy configuration
# HAProxy supports "http" and "tcp". For SSL， SMTP， etc， use "tcp".
mode： http
# Port on which HAProxy should listen
listenport： 8888
# A name for the proxy daemon， this wil be the suffix in the logs.
daemonname： myapplb
# Balancing Algorithm. Available options：
# roundrobin， source， leastconn， source， uri
# （if persistance is required use， "source"）
balance： roundrobin
# Ethernet interface on which the load balancer should listen
# Defaults to the first interface. Change this to：
#
# iface： eth1
#
# ...to override.
#
iface： '{{ ansible_default_ipv4.interface }}'
5.playbook角色详解
本示例划分了6个角色，包括base-apache、common、db、haproxy、nagios、
web，分别对应6个功能环境部署，根据不同业务场景的需求，可以随意加、减
角色，如将base-apache更换成nginx，然后在site.yml中引用。
（1）common角色
common的主要功能是部署、配置系统基础服务，包括yum源、安装nagios插
件、NTP服务、iptables、SELinux等，任务（tasks）的定义如下：
【roles/common/tasks/main.yml】
---
# This role contains common plays that will run on all nodes.
- name： Create the repository for EPEL
copy： src=epel.repo dest=/etc/yum.repos.d/epel.repo
- name： Create the GPG key for EPEL
copy： src=RPM-GPG-KEY-EPEL-6 dest=/etc/pki/rpm-gpg
- name： install some useful nagios plugins
yum： name={{ item }} state=present
with_items：
- nagios-nrpe
- nagios-plugins-swap
- nagios-plugins-users
- nagios-plugins-procs
- nagios-plugins-load
- nagios-plugins-disk
- name： Install ntp
yum： name=ntp state=present
tags： ntp
- name： Configure ntp file
template： src=ntp.conf.j2 dest=/etc/ntp.conf
tags： ntp
notify： restart ntp
- name： Start the ntp service
service： name=ntpd state=started enabled=true
tags： ntp
- name： insert iptables template
template： src=iptables.j2 dest=/etc/sysconfig/iptables
notify： restart iptables
- name： test to see if selinux is running
command： getenforce
register： sestatus
changed_when： false
上述代码定义了两个远程文件复制copy，其中src（源文件）的默认位置在
roles/common/files，使用with_item标签实现循环安装nagios插件，同时安装ntp服
务，引用模块文件roles/common/templatesntp.conf.j2，且同步到目标主
机/etc/ntp.conf位置。配置系统iptables，引用roles/common/templates/iptables.j2模
板，“notify：restart iptables”，状态或模板发生变化时将通知处理程序
（handlers）来处理。“command：getenforce”运行getenforce来检测selinux是否在
运行状态，“changed_when：false”作用为不记录命令运行结果的changed状态，
即changed为False。
下面定义common角色的处理程序。
【roles/common/handlers/main.yml】
---
# Handlers for common notifications
- name： restart ntp
service： name=ntpd state=restarted
- name： restart iptables
service： name=iptables state=restarted
上述代码定义了两个处理程序，功能分别为重启ntp、iptables服务，其
中“name：restart ntp”与任务（tasks）定义中的“notify：restart ntp”是一一对应
的，“name：restart iptables”同理。
下面定义了common角色iptables的配置模板：
【roles/common/templates/iptables.j2】
{% if （inventory_hostname in groups['webservers']） or （inventory_hostname in groups['monitoring']） %}
-A INPUT -p tcp --dport 80 -j ACCEPT
{% endif %}
… …
{% for host in groups['monitoring'] %}
-A INPUT -p tcp -s {{ hostvars[host].ansible_default_ipv4.address }} --
dport 5666 -j ACCEPT
{% endfor %}
“inventory_hostname”作为存放在Ansible的inventory文件中的主机名或IP，好
处是可以不依靠Facts的主机名参数ansible_hostname或其他原因，一般情况下
inventory_hostname等于ansible_hostname，但有时候我们习惯在Ansible的
inventory中使用IP地址，而ansible_hostname则返回主机名。模板使用了jinja2的
语法，本例if...endif语句判断当前的inventory_hostname是否在webservers及
monitoring组中（定义具体在hosts文件中），条件成立则添加80端口访问权限（-
A INPUT-p tcp--dport 80-j ACCEPT）。For...endfor语句实现了循环开通允许
monitoring组主机访问5666端口，使用hostvars[host]得到主机对象，可以获得主
机的Facts信息，如hostvars[host].ansible_default_ipv4.address获取主机IP。
（2）haproxy角色
haproxy角色主要实现了haproxy平台的部署、配置功能，任务（tasks）的定
义：
【roles/haproxy/tasks】
---
# This role installs HAProxy and configures it.
- name： Download and install haproxy and socat
yum： name={{ item }} state=present
with_items：
- haproxy
- socat
- name： Configure the haproxy cnf file with hosts
template： src=haproxy.cfg.j2 dest=/etc/haproxy/haproxy.cfg
notify： restart haproxy
任务（tasks）定义了两个功能，一为安装，二为同步配置文件，安装使用了
yum模块，循环安装haproxy、socat两个工具，同时根据配置参数渲染
roles/haproxy/templates/haproxy.cfg.j2模板文件，完成后同步到目标主
机/etc/haproxy/haproxy.cfg位置，状态发生变化时重启haproxy服务，使之生效。
下面定义了haproxy角色haproxy.cfg的配置模板：
【roles/haproxy/templates/haproxy.cfg.j2】
… …
backend app
{% for host in groups['lbservers'] %}
listen {{ daemonname }} {{ hostvars[host]['ansible_' + iface].ipv4.
address }}：{{ listenport }}
{% endfor %}
balance {{ balance }}
{% for host in groups['webservers'] %}
server {{ hostvars[host].ansible_hostname }} {{ hostvars[host]
['ansible_' + iface].ipv4.address }}：{{ httpd_port }}
{% endfor %}
{{hostvars[host]['ansible_'+iface].ipv4.address}}实现了获取网卡名变量
iface（group_vars/lbservers中定义）的IPv4 IP地址。
（3）web角色
web角色主要实现了php、php-mysql、git平台部署及SELinux的配置功能，任
务（tasks）的定义如下：
【roles/web/tasks/main.yml】
---
# httpd is handled by the base-apache role upstream
- name： Install php and git
yum： name={{ item }} state=present
with_items：
- php
- php-mysql
- git
- name： Configure SELinux to allow httpd to connect to remote database
seboolean： name=httpd_can_network_connect_db state=true persistent=yes
when： sestatus.rc ！= 0
- name： Copy the code from repository
git： repo={{ repository }} version=
{{ webapp_version }} dest=/var/www/html/
判断sestatus变量（roles/common/tasks/main.yml中定义）返回的rc（运行代
码）不等于0（失败）则配置selinux httpd访问远程数据库的权限，使用的是
Ansible的seboolean模块，该条语句等价于命令行“setsebool
httpd_can_network_connect_db 1”，其中“persistent=yes”表示开机自启动。
（4）nagios角色
nagios角色主要实现了nagios监控平台的部署，重点介绍任务（tasks）的定
义：
【roles/nagios/tasks/main.yml】
… …
- name： create the nagios object files
template： src={{ item + ".j2" }}
dest=/etc/nagios/ansible-managed/{{ item }}
with_items：
- webservers.cfg
- dbservers.cfg
- lbservers.cfg
notify： restart nagios
template分发多个模板文件时可以使用with_items来循环同步，变量与字符使
用“+”号连接（具体见jinja2语法）。
理解以上4个角色的定义后，再理解ansible-examples其他playbook的内容已经
没有太大的困难，本书将不一一说明。
参考提示
·9.1节YAML语法介绍参考http://zh.wikipedia.org/zh-cn/YAML。
·9.2节~9.11节Ansible介绍及示例参考http://docs.ansible.com官网文档。
第10章 集中化管理平台Saltstack详解
Saltstack（http://www.saltstack.com/）是一个服务器基础架构集中化管理平
台，开始于2011年的一个项目，具备配置管理、远程执行、监控等功能，一般可
以理解成简化版的puppet（http://puppetlabs.com/）和加强版的
func（https://fedorahosted.org/func/）。Saltstack基于Python语言实现，结合轻量
级消息队列（ZeroMQ）与Python第三方模块（Pyzmq、PyCrypto、Pyjinja2、
python-msgpack和PyYAML等）构建。Saltstack具备如下特点。
·部署简单、方便。
·支持大部分UNIX/Linux及Windows环境。
·主从集中化管理。
·配置简单、功能强大、扩展性强。
·主控端（master）和被控制端（minion）基于证书认证，安全可靠。
·支持API及自定义模块，可通过Python轻松扩展。
通过部署Saltstack环境，我们可以在成千上万台服务器上做到批量执行命
令，根据不同业务特性进行配置集中化管理、分发文件、采集服务器数据、操
作系统基础及软件包管理等，因此，Saltstack是运维人员提高工作效率、规范业
务配置与操作的利器。目前Saltstack已经趋向成熟，用户群及社区活跃度都不
错，同时官方也开放了不少子项目，具体可访问https://github.com/saltstack获
得。
为了方便读者更系统化地了解Saltstack的技术点，本章将针对相关技术点详
细展开介绍。
10.1 Saltstack的安装
Saltstack的不同角色服务安装非常简单，建议读者采用yum源方式来实现部
署，下面介绍具体步骤。