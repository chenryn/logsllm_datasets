SIGCOMM 2021 Conference (Virtual Event, USA) (SIGCOMM ’21). Association for
Computing Machinery, New York, NY, USA, 610–625. https://doi.org/10.1145/
3452296.3472923
[66] RIPE NCC. 2021. RIPE Atlas. https://atlas.ripe.net/.
[67] Enric Pujol, Ingmar Poese, Johannes Zerwas, Georgios Smaragdakis, and Anja
Feldmann. 2019. Steering hyper-giants’ traffic at scale. In Proceedings of the 15th
International Conference on Emerging Networking Experiments And Technologies.
[68] Mohammad Rajiullah, Andra Lutu, Ali Safari Khatouni, Mah-Rukh Fida, Marco
Mellia, Anna Brunstrom, Ozgu Alay, Stefan Alfredsson, and Vincenzo Mancuso.
2019. Web Experience in Mobile Networks: Lessons from Two Million Page
Visits. In The World Wide Web Conference (San Francisco, CA, USA) (WWW ’19).
Association for Computing Machinery, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3308558.3313606
[69] Eman Ramadan, Arvind Narayanan, Udhaya Kumar Dayalan, Rostand AK Fezeu,
Feng Qian, and Zhi-Li Zhang. 2021. Case for 5G-aware video streaming applica-
tions. In Proceedings of the 1st Workshop on 5G Measurements, Modeling, and Use
Cases. 27–34.
[70] Shravan Rayanchu, Ashish Patro, and Suman Banerjee. 2012. Catching whales
and minnows using wifinet: Deconstructing non-wifi interference using wifi
hardware. In 9th {USENIX} Symposium on Networked Systems Design and Imple-
mentation ({NSDI} 12). 57–70.
76
[71] Philipp Richter, Florian Wohlfart, Narseo Vallina-Rodriguez, Mark Allman, Randy
Bush, Anja Feldmann, Christian Kreibich, Nicholas Weaver, and Vern Paxson.
2016. A multi-perspective analysis of carrier-grade NAT deployment. In Proceed-
ings of the 2016 Internet Measurement Conference. 215–229.
[72] Mahadev Satyanarayanan, Paramvir Bahl, Ramón Caceres, and Nigel Davies.
IEEE pervasive
2009. The case for vm-based cloudlets in mobile computing.
Computing 8, 4 (2009), 14–23.
[73] Quirin Scheitle, Oliver Gasser, Patrick Sattler, and Georg Carle. 2017. HLOC:
Hints-based geolocation leveraging multiple measurement frameworks. In 2017
Network Traffic Measurement and Analysis Conference (TMA). IEEE, 1–9.
[74] Brandon Schlinker, Italo Cunha, Yi-Ching Chiu, Srikanth Sundaresan, and Ethan
Katz-Bassett. 2019. Internet Performance from Facebook’s Edge. In Proceedings
of the Internet Measurement Conference (Amsterdam, Netherlands) (IMC ’19).
Association for Computing Machinery, New York, NY, USA, 179–194. https:
//doi.org/10.1145/3355369.3355567
[75] Philipp Schulz, Maximilian Matthe, Henrik Klessig, Meryem Simsek, Gerhard
Fettweis, Junaid Ansari, Shehzad Ali Ashraf, Bjoern Almeroth, Jens Voigt, Ines
Riedel, Andre Puschmann, Andreas Mitschele-Thiel, Michael Muller, Thomas
Elste, and Marcus Windisch. 2017. Latency Critical IoT Applications in 5G:
Perspective on the Design of Radio Interface and Network Architecture. IEEE
Communications Magazine (2017).
[76] Amazon Web Services. 2019. AWS Global Infrastructure Map.
"https://aws.
amazon.com/about-aws/global-infrastructure/".
[77] Rob Sherwood, Adam Bender, and Neil Spring. 2008. Discarte: a disjunctive
internet cartographer. In Proceedings of the ACM SIGCOMM 2008 conference on
Data communication. 303–314.
[78] Rachee Singh, Arun Dunna, and Phillipa Gill. 2018. Characterizing the Deploy-
ment and Performance of Multi-CDNs. In Proceedings of the Internet Measurement
Conference 2018 (Boston, MA, USA) (IMC ’18). Association for Computing Ma-
chinery, New York, NY, USA, 168–174. https://doi.org/10.1145/3278532.3278548
[79] Ben Treynor Sloss. 2018. Expanding our global infrastructure with new regions
and subsea cables. Google Cloud (2018).
[80] Neil Spring, Ratul Mahajan, and Thomas Anderson. 2003. The causes of path
inflation. In Proceedings of the 2003 conference on Applications, technologies, archi-
tectures, and protocols for computer communications. 113–124.
[81] RN Staff. 2015. RIPE Atlas: A global internet measurement network. Internet
Protocol Journal (2015).
[82] Kaixin Sui, Mengyu Zhou, Dapeng Liu, Minghua Ma, Dan Pei, Youjian Zhao,
Zimu Li, and Thomas Moscibroda. 2016. Characterizing and improving wifi
latency in large-scale operational networks. In Proceedings of the 14th Annual
International Conference on Mobile Systems, Applications, and Services. 347–360.
[83] Srikanth Sundaresan, Walter de Donato, Nick Feamster, Renata Teixeira, Sam
Crawford, and Antonio Pescapè. 2012. Measuring Home Broadband Performance.
Commun. ACM (Nov. 2012), 10 pages. https://doi.org/10.1145/2366316.2366337
[84] Srikanth Sundaresan, Nick Feamster, and Renata Teixeira. 2016. Home network
or access link? locating last-mile downstream throughput bottlenecks. In Inter-
national Conference on Passive and Active Network Measurement. Springer.
[85] TeleGeography. 2019. Submarine Cable Map. "https://www.submarinecablemap.
com/".
Report. ThousandEyes.
[86] ThousandEyes. 2019. Cloud Performance Benchmark 2019–2020 Edition. Technical
[87] Martino Trevisan, Danilo Giordano, Idilio Drago, Marco Mellia, and Maurizio
Munafo. 2018. Five Years at the Edge: Watching Internet from the ISP Network.
In Proceedings of the 14th International Conference on Emerging Networking EX-
periments and Technologies (Heraklion, Greece) (CoNEXT ’18). ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3281411.3281433
[88] Florian Wohlfart, Nikolaos Chatzis, Caglar Dabanoglu, Georg Carle, and Walter
Willinger. 2018. Leveraging interconnections for performance: the serving in-
frastructure of a large CDN. In Proceedings of the 2018 Conference of the ACM
Special Interest Group on Data Communication. 206–220.
[89] Dongzhu Xu, Anfu Zhou, Xinyu Zhang, Guixian Wang, Xi Liu, Congkai An,
Yiming Shi, Liang Liu, and Huadong Ma. 2020. Understanding operational 5g: A
first measurement study on its coverage, performance and energy consumption.
In Proceedings of the Annual conference of the ACM Special Interest Group on Data
Communication on the applications, technologies, architectures, and protocols for
computer communication. 479–494.
[90] Bahador Yeganeh, Ramakrishnan Durairajan, Reza Rejaie, and Walter Willinger.
2019. How Cloud Traffic Goes Hiding: A Study of Amazon’s Peering Fabric. In
Proceedings of the Internet Measurement Conference (Amsterdam, Netherlands)
(IMC ’19). 15 pages.
[91] Bahador Yeganeh, Ramakrishnan Durairajan, Reza Rejaie, and Walter Willinger.
2019. How Cloud Traffic Goes Hiding: A Study of Amazon’s Peering Fabric. In
Proceedings of the Internet Measurement Conference. 202–216.
[92] Bahador Yeganeh, Ramakrishnan Durairajan, Reza Rejaie, and Walter Willinger.
2020. A First Comparative Characterization of Multi-cloud Connectivity in
Today’s Internet. In International Conference on Passive and Active Network Mea-
surement. Springer, 193–210.
IMC ’21, November 2–4, 2021, Virtual Event, USA
Dang and Mohan et al.
A APPENDICES
Appendices include supporting material that has not been peer-
reviewed.
A.1 Speedchecker VP Infrastructure
still comparable. The chasm between the two is largest in Africa,
where latencies over TCP tend to be lower compared to ICMP. Here,
we must point out that latency differences between the two pro-
tocols can be a likely side-effect of the measurement tool itself.
Previous research has shown that traceroute is susceptible to in-
consistent latency inflations due to path inconsistencies [32, 55, 80].
On the other hand, measurements over ICMP can be affected by
load balancers/firewalls in cloud WANs which can route packets
over longer paths, put them in lower priority queues, or drop them
altogether [43]. In this regard, measurements over TCP are guaran-
teed to be end-to-end and provide a close estimate of connection
latencies encountered by real applications operating in the cloud.
A.3 Speedchecker vs. RIPE Atlas – 
Figure 14: Distribution of 115000+ Speedchecker probes used
in this study grouped by their geographical “closeness”.
Figure 14 showcases the geographical distribution of Speed-
checker probes used in this study grouped based on their “closeness”
density. Denser deployment of probes is denoted by greener hues.
The illustration provides further granularity to the Speedchecker
probe deployment shown in Figure 1b. The most noteworthy is the
scattered availability of probes in both north and south of Africa –
which drives up latencies towards in-continent datacenter deploy-
ment within the continent (see Figure 4). This deployment trend
differs significantly from RIPE Atlas [22], where both the probes
and the targeted datacenters are within close geographical proxim-
ity. The figure highlights how geographical deployment density and
availability of vantage points belonging to different measurement
platforms can affect the outcome of the resulting analysis.
A.2 ICMP vs. TCP Probe-to-Cloud Latencies
Figure 15: Difference between end-to-end latencies over
ICMP and TCP in Speedchecker grouped by continents.
Figure 15 plots the end-to-end latencies recorded over ICMP
traceroute and TCP pings over Speedchecker VPs for each  pairing grouped by continents. For regions with
dense and highly managed network backhaul (i.e., Europe, North
America and Oceania), our result shows little-to-no difference be-
tween the latencies over two protocols. In Asia and South America,
TCP and ICMP latencies show minor differences (especially at 75th
percentiles), however, the median latencies over both protocols are
77
Figure 16: Latency differences between measurements from
Speechecker and RIPE Atlas in same  towards
the nearest DC. The left side denotes samples where Speed-
checker is faster, while the right side shows Atlas to be
faster.
To further investigate the impact of different measurement plat-
forms on global cloud accessibility and reachability (see §4.2), we
plot the cumulative differences in latencies from probes located in
the same city with the first hop within the same ASN. Within this
analysis we filter measurements over probe that are handled by
the same serving ISP in similar locations – thereby providing an
apples-to-apples comparison between the two platforms. Figure 16
shows our results. Since we were unable to find enough probe in-
tersections across the two platforms in Africa, South America and
Oceania (largely due to sparse probe availability in RIPE Atlas in
these regions), we exclude the results from these continents.
The result strengthens our analysis in §4.2, highlighting the sig-
nificant connectivity and deployment differences between the two
protocols. Only a fraction of Speedchecker probes in North America
and Europe achieve better latencies than RIPE Atlas, but for the
large majority RIPE Atlas is significantly faster than Speedchecker.
In Asia, Atlas is always faster – hinting at the impact of wired vs.
wireless access differences on overall latency. We plan to conduct
measurements over Speedchecker wired probes in future to thor-
oughly investigate the affect of deployment (managed vs. home) on
end-to-end cloud latency.
AFASEUNAOCSA0100200300400Latency[ms]ICMPTCPCloudy with a Chance of Short RTTs
IMC ’21, November 2–4, 2021, Virtual Event, USA
(a) Peering between Ukrainian ISPs & cloud providers
(a) Peering between Bahrainian ISPs & cloud providers
(b) Effect of direct and transit peering between UA-UK.
(b) Effect of direct and transit peering between BH-IN.
Figure 17: Another case study of ISP-cloud peering in Eu-
rope. (a) identifies peering interconnections from Ukrainian
ISPs to DCs in the United Kingdom while (b) shows impact
of those interconnections on cloud access latency.
Figure 18: Another case study of ISP-cloud peering in Asia.
(a) identifies peering interconnections from Bahrainian ISPs
to DCs in India while (b) shows impact of those interconnec-
tions on cloud access latency.
A.4 Cloud-ISP Peering Additional Case Studies
In §6.2 we analyzed the impact of different cloud-ISP peering inter-
connections in Europe (from German ISPs to UK cloud DCs) and
Asia (from Japanese ISPs to UK cloud DCs). However, our analysis
can be considered incomplete due to the following reasons. Firstly,
the affect of peering within Europe cannot be accurately assessed by
only focusing on interconnections between the two countries (DE
and UK) that are largely known for their well-provisioned network
backhaul. Secondly, Japan-to-India connectivity does not pose itself
as the most compelling use-case for peering in Asia since Japan also
has a well-provisioned Internet backhaul (barring its dependence
on submarine cables for global connectivity) along with a dense
deployment of datacenters within the country itself.
To generalize our analysis on ISP-cloud peering within these
two continents, we present additional connectivity case studies. For
Europe, we examine the connections from VPs in Ukraine (UA) to
DCs in the UK (see Figure 17). For Asia, we analyze peering between
serving ISPs from Bahrain (BH) to DCs in India (see Figure 18). The
colors in Figure 17a and Figure 18a denote the percentage of paths
in  pairings that used the same interconnection type.
We refer the reader to §6.1 for our methodology on identifying
ISP-cloud interconnections. Figure 17b and Figure 18b compares
the impact of different interconnection types on cloud access la-
tency. It must be noted that both UA and BH have no local DC
availability and must rely on deployment in other countries within
the continent via in-land backhaul for cloud connectivity.
Figure 19: Share of wireless last-mile to the end-to-end la-
tency towards nearest cloud from Speedchecker probes.
We observe that the peering trend in Europe is largely repeat-
able as the hypergiant cloud providers (Google, Amazon, and Mi-
crosoft) have set up direct peering interconnections with most
of the Ukrainian ISPs. Similar to our observations in §6.2, we do
not see a significant latency advantage for measurements over di-
rect vs. indirect peering links as both achieve comparable median
end-to-end latencies. The trend, however, departs significantly for
Bahrain-India connections. We find that direct interconnections be-
tween these two countries are less common – other than Microsoft
and Google peering directly with a handful of serving ISPs. The
rest of the cloud providers either provide connectivity via private
interconnects or via a public backhaul. Interestingly, here we see a
clear latency advantage of direct peering over other interconnec-
tion types – with direct peering links achieving significantly lower
latencies than their counterparts.
78
BABAAMZNDOGCPIBMLINMSFTORCLVLTRUARnet(AS3255)Datagroup(AS3326)UKRTELNET(AS6849)Kyivstar(AS15895)Volia(AS25229)111111111111111111020406080100Percentagedirect1AS2+AS1IXPBABAAMZNDOGCPIBMLINMSFTORCLVLTR50100150200Latency[ms]directpeeringintermediateASBABAAMZNDOGCPIBMLINMSFTORCLVLTRBatelco(AS5416)ZAIN(AS31452)Kalaam(AS39273)stc(AS51375)1111-11-----1-11020406080100Percentagedirect1AS2+AS1IXPBABADOGCPLINMSFTORCL100200300400Latency[ms]directpeeringintermediateASAFASEUNAOCSAGlobal0255075100Last-mile/totallatency[%]SChome(USR-ISP)SCcellIMC ’21, November 2–4, 2021, Virtual Event, USA
Dang and Mohan et al.
A.5 Last-mile Latency Share to Closest Cloud
Figure 19 shows the percentage share of wireless last-mile to end-
to-end latency towards the nearest cloud datacenter per probe.
The result accompanies our last-mile share analysis in §5. Here,
SC home denotes the last-mile access share of home probes in
Speedchecker that likely use WiFi for connectivity. On the other
hand, SC cell are Speedchecker probes that are likely accessing the
cloud via cellular connectivity. The result is similar to those shown
in Figure 7a. Firstly, we find that the type of last-mile access (cellular
vs. WiFi) does not impact much differently as both technologies
exhibit almost similar latency shares. Secondly, the latency at the
last-mile is more pronounced for measurements towards the closest
datacenter (understandably so since the overall latency is now much
shorter). Here we find that the last-mile can account for most of the
connection latency (almost 50% on the global scale) – showcasing
it as the primary bottleneck affecting cloud connectivity. Finally, at
first glance, only within Africa does cellular connectivity seem to
outperform home WiFi connections consistently. However, as noted
in §5, this trend is a likely artifact of geographic probe availability
within the continent, as most of the home probes are in the south
closer to the in-continent DCs while cellular probes are mostly
located in the north of the continent (see Appendix A.1).
79