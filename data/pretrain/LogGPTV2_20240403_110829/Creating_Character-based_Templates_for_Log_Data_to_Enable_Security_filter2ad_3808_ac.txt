token-char-template and the token-char-structure of a log line.
In the first step, the algorithm matches the two token-structures.
Therefore, the algorithm searches for tokens in the log line’s token-
structure that correspond to the tokens in the token-structure of
the template. The distance metric the algorithm uses is a mod-
ification of the LV-distance, which treats tokens like characters
and weights their value for the accuracy of the template by their
length. This is necessary, because the normal LV-distance applied
to token-structures would provide the template with the most to-
kens, without taking into account that a token consisting of a larger
number of characters supports a template with higher coverage.
Otherwise, a template with low coverage would be accepted as
long as it consists of a large number of tokens. Thus, our algorithm
matches the tokens according to the LV-distance with the difference,
if two tokens of the template match the same corresponding token
of the log line’s token-structure, the score assigned by the algorithm
for computing the LV-distance is decreased by the length of the
token. This is reasonable, because when calculating the LV-distance,
positive scores represent penalties, i.e., positive values correspond
to required modification operations when transforming one string
into another. Note, the result is not a distance, but a sufficient score
for this algorithm. The first two lines of Fig. 7 depict the matching
of the token-structures.
Next, the algorithm converts the tokens of the token-structure of
the template which do not match any of the log line’s into character-
structures and merges all adjacent character-structures. Hence,
there exists exactly one character-structure between two tokens
as line 3 of Fig. 7 shows. Finally, the char-structures of the current
template are matched with the corresponding, so far unmatched,
parts of the log line. For this purpose, any of the previously de-
scribed algorithms for generating character-based templates can
be used. Lines 3 and 4 in Fig. 7 visualize the final step and line 5
shows the resulting template.
For the evaluation of the algorithm, we chose the merge algo-
rithm, because it provides the most accurate templates among the
algorithms, as the evaluation in Sec. 5 shows. The disadvantage of
the longer runtime is mitigated, because of the shorter length of
the compared strings.
Algorithm 6 describes the implementation of the token_char
algorithm. First, the algorithm splits log line S2 into tokens and
transforms it into a token structure T2. Then it performs the token
matching between the current template T1 and the token-structure
of log line T2. In this step, the algorithm also generates the charac-
ter structure of the log line. The algorithm compares the character
Figure 7: Token_char algorithm matching. Dark blue parts
represent token-structures and light blue parts character-
structures. Colons represent any fixed set of predefined de-
limiters.
structures strinд_template1 of the current template and their cor-
responding character-structures strinд_template2 of the log line in
a for loop. For that purpose, the algorithm iterates over the gaps
of the token-structures T1 and T2, which as mentioned refer to
the character-structures. For matching the character-structures,
the algorithm applies Alg. 2. Finally, the resulting alignment of the
character-structures replaces the corresponding character-structure
strinд_template1 in the current template T1.
Algorithm 6 Token_char(T1, S2)
1: T2 ← Split_into_tokens(S2);
2: Token_Matching(T1,T2);
3: for (strinд_template1, strinд_template2) ∈ Gaps(T1,T2) do
4:
in
Merge(strinд_template1, strinд_template2);
strinд_template1
Replace
T1
5: end for
6: return T1
with
5 EVALUATION
The following section presents the evaluation of our approach for
generating character-based cluster templates. First, we describe the
data used for the evaluation. Next, we define a similarity score that
we calculate alongside the F-score to asses accuracy and quality
of the algorithms introduced in Sec. 4. Finally, we interpret and
discuss the evaluation results. All evaluations have been carried
out on a Notebook with an Intel Core i7-5600U CPU 2.60 GHz and
16 GB RAM running Windows 7 64-Bit. The assessed algorithms
have been implemented in Python 3.7.
5.1 Test data
For the evaluation of our approach, we use three different data
sets. This demonstrates the broad applicability of the approach
for various log data types. The first data set, we refer to as DS-
A, originates from a network that runs a MANTIS Bug Tracker
System2. Therefore, the data set contains logs from an Apache Web
server hosting the MANTIS platform, a MySQL database, a reverse
proxy and a firewall, as well as a mail server. The log messages of
these systems are aggregated using syslog. The data set consists
of 1.6 million log lines that reflect 10 hours of system usage. The
second data set, we refer to as DS-B, derives from the same system.
DS-B includes the syscalls of the system, which have been collected
2https://www.mantisbt.org/
Table 1: Properties of the subsets of the described data used
for evaluation. For the line length, the number of words
(space separated substrings) and the cluster size, the table
provides values for minimum, average and maximum.
data set size
line length
word #
cluster #
cluster size
DS-A
10.000
352
DS-B
133.000
180
60 / 135.94 / 1959
3 / 12.60 / 133
79 / 211.10 / 328
8 / 32.67 / 58
92 / 139.03 / 311
9 / 13.72 / 31
1 / 28.41 / 605
1 / 741.47 / 13857
1 / 9523.81 / 46585
DS-C
200.000
21
using the auditd service. The third data set, we refer to as DS-
C, includes logs from a Hadoop File System running on a 203-
node cluster on Amazon’s EC2 platform [13]. DS-C consists of 11
million lines that reflect almost 3 days of system behavior. Since the
evaluated algorithms require pre-clustered data, we clustered the
data applying the incremental clustering approach from [12], using
a similarity threshold of 0.9 for DS-A and DS-B, and 0.6 for DS-C.
We selected the similarity threshold with respect to the structure
and complexity of the data. We chose a lower similarity for DS-C,
because the data set includes larger variable parts and a higher
similarity threshold would lead to a large number of small clusters
that would represent an inappropriate cluster arrangement that
includes many similar clusters.
5.2 Evaluation metrics
We used two different evaluation metrics to assess and compare the
different algorithms. The first one is a score for similarity, which is
defined in the following, and the second one is the F-Score.
The Sim-Score measures the similarity between the log lines
of a cluster and its corresponding template. The algorithms for
generating character-based templates provide templates that match
all log lines of a cluster. Therefore, the ratio between the number
of characters the template consists of and the average log line
length is a measure for similarity between a template and the log
lines of a cluster. In the Sim-Score, the average log line length
corresponds to the mean of the number of characters the log lines
of a cluster consist of. Consequently, the resulting Sim-Score for
each algorithm is the mean of these similarities. The Sim-Score is
calculated as shown in Eq. (1), where n is the number of clusters,
mi the number of log lines in the i-th cluster, Ti the template of
the i-th cluster, Li ,j is the j-th log line of the i-th cluster and | · |
denotes the number of characters of a template or a log line.
The Sim-Score is an evaluation metric that indicates how accu-
rate the templates are. One advantage of the Sim-Score is that it
does not rely on any additional information about the clusters, such
as a ground truth, which defines the optimal template. Thus, it can
be calculated directly after generating templates, for any data set.
Table 1 presents properties of the data we used for evaluating the
Sim-Score.
The second metric we used to evaluate the proposed algorithms
for generating character-based templates is the F-score (see Eq. (2)).
The F-Score allows an assessment of the accuracy of the generated
n
i =1
mi
|Ti|
j=1 |Li ,j|
1
mi
Sim-Score =
1
n
(1)
templates. However, in opposite to the Sim-Score, the calculation
of the F-Score requires a ground-truth to identify true positives
(TP), false postives (FP) and false negatives (FN), as Eq. (2) indicates.
Therefore, we first had to create a character-based ground truth for
all data sets.
F-score =
2T P
2T P + F N + F P
(2)
Furthermore, we defined the terms TP, FP and FN3:
• TP are defined as the characters which appeared in the same
order in both the ground truth and the created templates.
• FP are characters, which occur in the template but not in the
• FN are characters, which occur in the ground truth but not
ground truth.
in the template.
FP are an issue that cannot simply be ignored. The major reason
for FP are over-fitting templates. The algorithms tend to create
overly accurate templates, because they only generate them from
the perspective of the log lines that are associated with a cluster
and not taking other knowledge into account as humans would
do. Reasons for this are characters that actually represent variable
parts of a log line, but occur in each log line of a cluster. However,
these characters are not part of the ground truth, because they,
for example, refer to an IP address or a part of a timestamp, which
might only be static for the training data and thus are not considered
static in the ground truth. An example is a variable within the same
cluster that takes the values 192.67.200.155 and 192.67.200.12.
In this case, 192.67.200.1 becomes part of the template, although
the last character 1 belongs to a variable part of a log line. Hence, the
resulting template would not match the IP address 192.67.200.2,
which might be also valid.
5.3 Sim-Score evaluation results
The following section discusses the results of the evaluation of the
Sim-Score. As previously mentioned, the calculation of the Sim-
Score does not require any additional information, such as a prede-
fined ground truth. Thus, the Sim-Score is suitable to be calculated
for any log data set. Furthermore, to compare the character-based
template generators with a token-based approach, we also gen-
erated token-based templates, using the part of the token_char
algorithm that generates the token-structure of the template.
Tables 2, 3 and 4 present the evaluation results of the Sim-Score
for the different template generator algorithms using the data sets
described in Tab. 1. As expected, the proposed character-based
algorithms yield a much higher Sim-Score than the token-based
approach. However, the token_char provides comparable results
to the pure character-based algorithms. The differences between
the Sim-Scores of the character-based algorithms are so small that
they can be neglected. Nevertheless, the results of the runtime are
of greater significance. The merge algorithm shows the longest
runtime among all tested algorithms. This is the case, because all
other character-based algorithms first divide the line into shorter
segments by marking parts of the line that are equal to tokens
of the template. Then, they match the remaining shorter parts of
the line and the template by calculating the LV-distance. Whereas,
3Since gaps can be optional they do not influence the Sim-Score.
Table 2: Sim-Score comparison on DS-A
Table 5: Cluster arrangement
Sim-Score
Time (s)
merge
96.38%
435.20
length
96.24%
23.46
equalmerge
96.37%
25.52
token_char
95.18%
29.54
token
85.27%
8.49
Table 3: Sim-Score comparison on DS-B
Sim-Score
Time (s)
merge
91.40%
35179.35
length
90.71%
55.56
equalmerge
91.42%
63.37
token_char
91.42%
843.51
token
77.27%
366.76
Table 4: Sim-Score comparison on DS-C
Sim-Score
Time (s)
merge
71.96%
11207.57
length
70.41%
344.21
equalmerge
71.95%
227.22
token_char
71.96%
1387.87
token
52.67%
154.14
the merge algorithm calculates the LV-distance for the whole log
line and the whole template. While the length and the equalmerge
algorithms showed a comparable runtime on the data sets DS-A
and DS-B, the equalmerge algorithm outperforms all the others on
DS-C. Due to the lower similarity threshold during clustering and
larger variable parts in the log data, Sim-Scores for DS-C decrease
for all algorithms. Furthermore, the larger variable parts in DS-C are
the reason, why the equalmerge algorithm outperforms the length
algorithm. While the equalmerge algorithm merges the blocks that
are not marked and then calculates the LV-distance, the length
algorithm first has to localize all blocks of the current template in the
log line at hand. Due to the large variable parts the number of blocks
the template consists of increases in every step. Furthermore, the
different sizes of the data sets affect the token_char approach more
than the others. The reason for this is, that the token_char algorithm
has to do the matching for the token-structure and all character-
structures of the template. The runtime of the pure token-based
approach is rather long when processing DS-B. This is, because of
the long lines consisting of a large number of tokens.
5.4 Scalability
The next section summarizes results on the evaluation of the scala-
bility of the different algorithms. Figure 8 visualizes the results for
the different algorithms, showing the number of lines on the x-axis
and the runtime on the y-axis. For the evaluation of the scalability,
we chose a cluster from DS-B that comprises more than 1000 log
lines. Then, we measured the runtime it took to calculate the tem-
plate for the cluster for 5 to 1000 lines in steps of 50 lines. Figure
8 demonstrates that the runtime of all algorithms scales linearly
with respect to the cluster size m, which results in a computational
complexity of O(m). Figures 8b and 8c demonstrate that the length
and the equalmerge algorithm scale equally well with respect to
the runtime and gradient, followed by the token_char algorithm
in Fig. 8d. The merge algorithm, see Fig. 8a, has the worst runtime
and gradient.
merge Sim-Score
length Sim-Score
equalmerge last change
token_char last change
equalmerge Sim-Score
token_char Sim-Score