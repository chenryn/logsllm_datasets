packet loss properties that they obtained from the Ookla Net Index
dataset [2]. All vertices are connected by edges with approximated
latency,8 jitter, and packet loss properties.
Overlay Tor Topology. We follow the footsteps of Jansen et al.
and create a scaled-down topology that consists of 500 Tor clients
(OPs), 50 Tor ORs, and 50 HTTP servers. Of the 50 ORs, 5 work as
directory authorities. Our ORs are assigned bandwidth values that
are sampled from the bandwidth distribution of the live Tor network
ORs. We create two client types: web clients and bulk clients. Our
client model is based on a previous study of the exit Tor trafﬁc by
McCoy et al. [24]. The study found that 93% of connections that
exited the Tor network are HTTP connections which consumed ap-
proximately 60% of trafﬁc volume. They also found that ﬁle shar-
ing applications consumed approximately 40% of the bandwidth in
Tor. During our experiments, our web clients continuously fetch
ﬁxed-sized 320 KiB ﬁles, and pause randomly for 1 to 30 seconds
between fetches. Our bulk clients continuously download 5 MiB
ﬁles without pausing. Finally, we ﬁrst use a web-to-bulk client ra-
tio of 19:1, as recommended by Jansen et al.(Figures 7(a)–7(d)).
In addition, we also repeated our experiments on the same network
topology where we lower the web-to-bulk client ratio to 9:1 in or-
der to test PCTCP with different trafﬁc loads and with increased
congestion (Figures 7(e)–7(h)).
Model Accuracy. Before we present our results, we ﬁrst compare
the performance of our stock Tor bulk and web clients, which we
obtained from our testbed, to the performance of the live Tor net-
work published by the Tor metrics portal [38]. This comparison
step was also carried out by Jansen et al. The purpose of this step
is to conﬁrm that our testbed measurements can indeed approxi-
7The model ﬁles are available for download from the authors’ websites (http://
www.mit.edu/~ke23793/misc/tormodel_exptor.tar.gz).
8The authors use iPlane [1] RTTs to approximate latency.
 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20CDFDownload Time (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDFTime to First Byte (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100CDFDownload Time (s)PCTCPTor 0.2 0.4 0.6 0.8 1 1 2 3 4 5CDFTime to First Byte (s)PCTCPTorWe observe in Figure 7(c) that download times for bulk clients
are slightly improved when PCTCP is used. The improvement is
roughly 20% for 80% of the requests. For example, the median
download time for stock Tor is 65 seconds, whereas for PCTCP,
the median download time is approximately 51 seconds. Also,
the time-to-ﬁrst-byte results are signiﬁcantly improved for the bulk
downloaders, as can be seen in Figure 7(d). This suggests that con-
gestion is vastly reduced in the network.
Under heavier trafﬁc loads, the amount of available bandwidth
in the network decreases, which affects the beneﬁts we observe
for the download time. The download time comparison between
PCTCP and Tor for web and bulk clients when the ratio of web-
to-bulk clients is 9:1, depicted in Figures 7(e) and 7(g), shows that
PCTCP improves the long tail of the distribution for the web clients
by approximately 20%. The reason for the improvement is that, de-
spite the lack of spare bandwidth, PCTCP allows each circuit at the
transport layer to get its fair share of the bandwidth and forces the
bulk downloads present in the system to back off whenever they
attempt to get more than their allocated bandwidth, as evidenced
by the degradation of the bulk client performance shown in Fig-
ure 7(g). With PCTCP, heavy circuits might observe more delays
because such circuits are expected to drop more cells, and their re-
spective TCP connections would back off more frequently as a re-
sult of the separate TCP congestion control. However, performance
improvements can be observed even for bulk clients if more band-
width was available. For example, we have observed signiﬁcant im-
provements for both web and bulk clients in the higher-bandwidth
experiments we report in Appendix A.
Figures 7(f) and 7(h) show the signiﬁcant time-to-ﬁrst-byte im-
provements for both the web clients and the bulk downloaders. The
improvements at the 75th percentile are more than 60% for both the
web and bulk clients.
Summary of results. Based on our observations, we conclude
that PCTCP produces signiﬁcant performance beneﬁts that can cer-
tainly be perceived by clients. The download time improvements
depend on the amount of available bandwidth in the network. If
the network has spare bandwidth to offer, PCTCP will improve the
experience of all users in the system. When the network operates at
its capacity, web clients will notice download time enhancements,
while bulk clients will observe degraded performance. However, in
all experimental scenarios, PCTCP signiﬁcantly improves the re-
sponse times in the network for all clients.
To maximize the beneﬁts of using PCTCP, it should be used
in combination with previous proposals that aim to increase the
amount of available bandwidth in the network, such as trafﬁc clas-
siﬁcation [5], throttling approaches [21,27] or approaches aimed to
incentivize clients to run ORs [20, 29].
6. DISCUSSION
We next discuss a variety of open issues regarding PCTCP.
6.1 Anonymity Implications
Since our transport proposal is designed for Tor, an anonymi-
ty network, it is essential to consider the anonymity implications
of our design. In particular, it is important to ensure that our new
design does not add new vulnerabilities to the Tor network. Recall
that the anonymity of a circuit is compromised in Tor if its two
ends, the entry and exit, are compromised. Therefore, one issue
to consider is whether using PCTCP can reduce the anonymity set
of the ORs used in a circuit. For example, can an exit OR reduce
(a) Large ﬁles
(b) Small ﬁles
Figure 6: Comparison between the performance of torperf
(Live Tor), and our scaled-down testbed Tor network (Exper-
imenTor)
mate the measurements taken from the live network, even though
our network is signiﬁcantly scaled down.
Figure 6(a) compares the distribution of the download times of
our testbed bulk downloaders and those measured by torperf, a tool
that measures download performance on the live Tor network. The
two distributions display comparable performance and they indeed
intersect at the median. That is, 50% of the 5 MiB downloads take
65 seconds or less on the live network, and the same is true on our
testbed. Figure 6(b) compares the results of our 320 KiB down-
loads and torperf’s 50 KiB, and 1 MiB downloads.9 As expected,
the distribution of download times for our web clients ﬁts between
the distributions of download times between torperf’s 1 MiB and
50 KiB ﬁle downloads.
Results. Now that we have veriﬁed that our Tor model closely
approximates the performance of the Tor network, we next shift at-
tention to our results.10 Figure 7(a) compares the download time
observed by web clients when stock Tor and PCTCP are used. The
ﬁgure shows signiﬁcant improvement for the slowest 50% of the
downloads, especially for the 4th quartile of the download times.
For example, the download times for Tor range from 17–90 sec-
onds, whereas for PCTCP, the download times range from 14–56
seconds.
In Figure 7(b), signiﬁcant time-to-ﬁrst-byte improvements can
be observed when PCTCP is used, as compared to Tor. At the
median, it takes Tor clients 3.6 seconds before the browser starts
changing for them, whereas PCTCP clients only wait for 1.6 sec-
onds, which is a 55% improvement. For the 75th percentile re-
sponse times, the time-to-ﬁrst-byte is only 2 seconds for PCTCP
users, whereas Tor clients experience delays of up to 6 seconds.
This increases the observed improvements to 66%.
9Torperf only maintains the results of 5 MiB, 1 MiB and 50 KiB ﬁle downloads.
10For simplicity, we have not disabled TLS in PCTCP experiments even though it is
not needed in our design due to the use of IPsec.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300Cumulative DistributionDownload Time (s)ExperimenTor 5 MiBLive Tor 5 MiB 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100Cumulative DistributionDownload Time (s)Live Tor 50 KiBExperimenTor 320 KiBLive Tor 1 MiB(a) Web clients (19:1)
(b) Web clients (19:1)
(c) Bulk clients (19:1)
(d) Bulk clients (19:1)
(e) Web clients (9:1)
(f) Web clients (9:1)
(g) Bulk clients (9:1)
(h) Bulk clients (9:1)
Figure 7: Performance of all clients in the large-scale experiment under different trafﬁc loads. Figures 7(a)–7(d) show the perfor-
mance of all clients when the web-to-bulk client ratio is 19:1, whereas Figures 7(e)–7(h) show the performance when we increase
congestion by setting the web-to-bulk client ratio to 9:1.
the anonymity set of the entry OR used on a circuit because of
PCTCP?11
First, with exception of the IPsec connections, the changes that
are imposed by PCTCP on any OR are local. That is, our design
does not introduce a new cell type or require other ORs on the cir-
cuit to upgrade. If an entry OR uses PCTCP, then only the middle
OR will notice because the middle has to agree to establish the
IPsec connection with entry and because it receives more than one
TCP connection from the upgraded entry. Those changes do not af-
fect the exit OR in the circuit; therefore, the exit would not be able
to know if entry belongs to the set of upgraded ORs or not. Even
if the exit learns from router descriptors that middle is an upgraded
OR, the exit would still not be able to know if entry is upgraded or
not.
Furthermore, one might wonder if dedicating separate TCP con-
nections might open the door to timing attacks. First, a connection
between the OP and the OR is very similar for Tor and PCTCP.
Second, because the communication between ORs is protected us-
ing IPsec, it would be difﬁcult for the adversary to extract speciﬁc
circuit information even though each circuit uses a separate TCP
connection. Therefore, PCTCP does not introduce any new threats
to the Tor network.
6.2
Incremental Deployment
One advantage of PCTCP is that it is incrementally deployable
in two steps. The ﬁrst step towards deployment is enabling IPsec
communication among ORs. Basically, ORs need to advertise in
their descriptors that they are willing to accept IPsec connections.
Then, IPsec-enabled ORs can try to establish IPsec connections
proactively among each other. When OR1 wishes to use PCTCP
with OR2, it can check if it has an existing IPsec connection with
OR2,12 in which case OR1 can proceed with using PCTCP. If OR1
detects no IPsec connection with OR2, it uses the default Tor TLS
connection with OR2 and multiplexes the circuits in the same con-
nection.
11It is important to prevent the two ends of the circuit from learning about each other
to prevent active or legal attacks.
12For Openswan, the visibility of IPsec for an application can be established using the
libwhack API.
6.3 Experimental Limitations
To be able to faithfully test and evaluate our new transport pro-
posal, we ran a series of testbed experiments on different network
topologies using different trafﬁc models and loads. Regardless of
our efforts, we recognize that our large-scale experiments were
conducted on an isolated experimental testbed. We were unable
to experiment with larger topologies because we are limited by our
CPU, bandwidth and memory resources.
However, to ensure that we report accurate results, we followed
the methodology of Jansen et al. [18] to produce an accurate model
of the Tor network. We also used their published topology ﬁles in
order to avoid biased results that might be obtained using a different
experimental setup. Finally, we carried out additional experiments
on the live Tor network to conﬁrm our results.
6.4
IPsec through NATs
One challenge that IPsec faced in the past is its inability to con-
nect to hosts behind NATs. As a result, NAT-Traversal [36] (NAT-
T) has evolved to address this problem. NAT-T can be used when
two hosts detect that if they are behind a NAT. In the context of Tor,
this problem is currently irrelevant as most Tor ORs are publicly
reachable; however, there are some efforts to enable the operation
of ORs from behind NATs [7]. In this case, IPsec can still beneﬁt
from NAT-T.
6.5 File Descriptor and Memory Usage
One issue to consider is how this work affects the very busy
routers on the live network. Since Tor uses a weighted-bandwidth
OR selection algorithm where ORs are selected in proportion to
their bandwidth, some high-bandwidth ORs service thousands of
circuits at the same time. This means that, with PCTCP, such
routers are expected to maintain thousands of ﬁle descriptors at the
same time. One might wonder if such a requirement might raise
memory usage concerns due to the TCP buffer space allocated in
the kernel for each ﬁle descriptor.
To get an idea of how many ﬁle descriptors would be needed
when PCTCP is used, we examined a fast exit OR on the live
network conﬁgured with a bandwidth of 100 Mb/s, which puts it
among the fastest 6% of the network routers. This fast exit OR used
roughly 10,000 ﬁle descriptors for its communication with other
 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40CDFDownload Time (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDFTime to First Byte (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 100 200 300CDFDownload Time (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDFTime to First Byte (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40CDFDownload Time (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDFTime to First Byte (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 100 200 300CDFDownload Time (s)PCTCPTor 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDFTime to First Byte (s)PCTCPTorORs and with destination servers. Since an exit OR uses one ﬁle
descriptor for each stream within a circuit, the number of circuits
it is handling is certainly less than the number of ﬁle descriptors it
is using. Note that intermediate routers are currently expected to
use a number of ﬁle descriptors that is equal to the number of ORs
in the network, which is approximately 3000. We therefore expect
that other intermediate ORs, such as middles or entries that have the
same bandwidth capabilities as the fast exit, to use between 3000
and 10,000 ﬁle descriptors if they use PCTCP. In short, ﬁle de-
scriptor and memory usage should not be a problem with PCTCP,
as even the busiest entry and middle ORs running PCTCP should
consume fewer of these resources than the existing Tor network
requires exit ORs to support today.
However, one possible way to reduce the number of ﬁle descrip-
tors is to use a threshold algorithm. For instance, every OR can
use PCTCP up to a certain threshold of the number of circuits be-
ing serviced, which can be conﬁgured by the relay operator.
If
the number of circuits serviced exceeds the threshold, the OR can
multiplex new incoming circuits in existing connections in a round-
robin manner. Even better, an OR can use a classiﬁcation approach
similar to Torchestra [17], where the OR avoids multiplexing loud
circuits with the quiet ones. In short, we do not believe memory
management issues are detrimental to PCTCP, as several solutions