### Packet Loss Properties and Network Topology

The packet loss properties utilized in this study were obtained from the Ookla Net Index dataset [2]. All vertices in the network are interconnected by edges, each of which is characterized by approximated latency, jitter, and packet loss properties.

#### Overlay Tor Topology

Following the approach of Jansen et al., we created a scaled-down topology consisting of 500 Tor clients (OPs), 50 Tor relays (ORs), and 50 HTTP servers. Among the 50 ORs, 5 function as directory authorities. The bandwidth values for our ORs were sampled from the live Tor network's OR bandwidth distribution. We defined two types of clients: web clients and bulk clients. Our client model is based on a previous study by McCoy et al. [24], which found that 93% of connections exiting the Tor network are HTTP connections, consuming approximately 60% of the traffic volume. Additionally, file-sharing applications account for about 40% of the bandwidth. In our experiments, web clients continuously fetch fixed-sized 320 KiB files, pausing randomly for 1 to 30 seconds between fetches. Bulk clients, on the other hand, continuously download 5 MiB files without pausing. Initially, we used a web-to-bulk client ratio of 19:1, as recommended by Jansen et al. (Figures 7(a)–7(d)). To test PCTCP under different traffic loads and increased congestion, we also conducted experiments with a lower web-to-bulk client ratio of 9:1 (Figures 7(e)–7(h)).

### Model Accuracy

Before presenting our results, we compared the performance of our stock Tor bulk and web clients, obtained from our testbed, to the performance of the live Tor network published by the Tor metrics portal [38]. This step, also carried out by Jansen et al., aimed to confirm that our testbed measurements accurately approximate those from the live network, despite the significant scale reduction.

Figure 6(a) compares the download times of our testbed bulk downloaders with those measured by torperf, a tool for measuring download performance on the live Tor network. The two distributions show comparable performance, intersecting at the median. Figure 6(b) compares the results of our 320 KiB downloads with torperf’s 50 KiB and 1 MiB downloads. As expected, the distribution of download times for our web clients fits between the distributions of torperf’s 1 MiB and 50 KiB file downloads.

### Results

#### Download Time Improvements

In Figure 7(c), we observe that the use of PCTCP slightly improves download times for bulk clients, with an improvement of roughly 20% for 80% of the requests. For example, the median download time for stock Tor is 65 seconds, while for PCTCP, it is approximately 51 seconds. Similarly, the time-to-first-byte results for bulk downloaders are significantly improved, as shown in Figure 7(d), indicating reduced congestion in the network.

Under heavier traffic loads, the available bandwidth decreases, affecting the benefits observed for download times. When the web-to-bulk client ratio is 9:1, PCTCP improves the long tail of the distribution for web clients by approximately 20%, as depicted in Figures 7(e) and 7(g). Despite the lack of spare bandwidth, PCTCP ensures fair bandwidth allocation, causing bulk downloads to back off when they attempt to exceed their allocated share. This is evidenced by the degradation in bulk client performance in Figure 7(g).

#### Time-to-First-Byte Improvements

Figures 7(f) and 7(h) show significant time-to-first-byte improvements for both web and bulk clients. At the 75th percentile, the improvements are more than 60% for both client types.

### Summary of Results

Based on our observations, PCTCP provides significant performance benefits that are noticeable to clients. The extent of these improvements depends on the available bandwidth. If the network has spare bandwidth, PCTCP enhances the experience for all users. When the network operates at capacity, web clients benefit from improved download times, while bulk clients may experience degraded performance. However, in all experimental scenarios, PCTCP significantly improves response times for all clients.

To maximize the benefits of PCTCP, it should be combined with other proposals aimed at increasing available bandwidth, such as traffic classification [5], throttling approaches [21, 27], or incentives for clients to run ORs [20, 29].

### Discussion

#### Anonymity Implications

Since PCTCP is designed for the anonymity network Tor, it is crucial to consider its impact on anonymity. Specifically, we must ensure that PCTCP does not introduce new vulnerabilities. One concern is whether PCTCP reduces the anonymity set of ORs used in a circuit. For instance, can an exit OR determine if an entry OR is using PCTCP?

First, except for IPsec connections, the changes imposed by PCTCP on any OR are local and do not require other ORs to upgrade. If an entry OR uses PCTCP, only the middle OR will notice, as it must agree to establish the IPsec connection and receives multiple TCP connections from the upgraded entry. These changes do not affect the exit OR, so it cannot determine if the entry OR is upgraded. Even if the exit learns from router descriptors that the middle OR is upgraded, it still cannot determine the status of the entry OR.

Additionally, while separate TCP connections might raise concerns about timing attacks, the similarity in communication between OP and OR for both Tor and PCTCP, combined with the protection provided by IPsec, makes it difficult for adversaries to extract specific circuit information. Therefore, PCTCP does not introduce new threats to the Tor network.

#### Incremental Deployment

One advantage of PCTCP is its incremental deployability in two steps. The first step involves enabling IPsec communication among ORs. ORs need to advertise in their descriptors that they accept IPsec connections. IPsec-enabled ORs can then proactively establish IPsec connections. When OR1 wishes to use PCTCP with OR2, it checks for an existing IPsec connection. If one exists, OR1 proceeds with PCTCP; otherwise, it uses the default Tor TLS connection and multiplexes circuits in the same connection.

#### Experimental Limitations

To evaluate PCTCP, we conducted a series of testbed experiments on different network topologies and traffic models. Despite our efforts, our large-scale experiments were limited by CPU, bandwidth, and memory resources. To ensure accurate results, we followed Jansen et al.'s methodology [18] and used their published topology files. We also conducted additional experiments on the live Tor network to confirm our findings.

#### IPsec through NATs

IPsec historically faced challenges connecting to hosts behind NATs, but NAT-Traversal (NAT-T) addresses this issue. In the context of Tor, this problem is currently irrelevant as most ORs are publicly reachable. However, efforts to enable OR operation behind NATs [7] could benefit from NAT-T.

#### File Descriptor and Memory Usage

A key consideration is the impact of PCTCP on busy routers in the live network. Since Tor uses a weighted-bandwidth OR selection algorithm, high-bandwidth ORs service thousands of circuits simultaneously. With PCTCP, these routers would need to maintain thousands of file descriptors. To assess this, we examined a fast exit OR with a 100 Mb/s bandwidth, which used approximately 10,000 file descriptors for communication with other ORs and destination servers. Intermediate ORs, such as middles or entries, with similar bandwidth capabilities, would use between 3,000 and 10,000 file descriptors. Thus, file descriptor and memory usage should not be a problem with PCTCP, as even the busiest ORs running PCTCP would consume fewer resources than current exit ORs.

To further reduce file descriptors, a threshold algorithm can be used. Each OR can use PCTCP up to a certain number of circuits, after which new circuits are multiplexed in existing connections. Alternatively, a classification approach like Torchestra [17] can avoid multiplexing loud circuits with quiet ones. In summary, memory management issues are not detrimental to PCTCP, as several solutions are available.