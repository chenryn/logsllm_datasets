10, 1: 8130
[268434836, bgreggi03cb3a7e46298b38e] : 8897
e[268434846, bgregg-↓-03cb3a7a46298b38e : 15813
[268434581, coredns-fb8b8dccf×7khx] : 39656
[268434654, coredns=Cb8b8dccf-cxrn9] : 40312
[. 
The source to pidnss(8) is:
1/usr/local/bin/bpftrace
include 
#1nc1ude 
include 
#include 
BEGIN
printf (*Txacing PID nanespace svltches. Ctrl-C to end`n*) 
---
## Page 720
15.3 BPF Tools
683
kprobe:finish_task_svitch
$prev = (struct task_struct *)arg0;
Scurr = [struct task_struct *) curtask,
$prer_pidns = $prev=>nsproxy->pid_ns_for_chi1dren=>ns.inun
$cuxr_pldns = $curz=>nspzoxy=>pld_ns_for_ch11dren=>ns.1nun,
if ($prev_pidns != $curr_pidns)(
(1 vumoo = [eueuepou*eseu
include 
BEGIN
printf (*Tracing block I/0 throttles by cgroup. Ctel-C to end)n*) 
kprobe :b1k_thzot1_bio
gb1kg[tld] = arg1
kretprobe:blk_throt1_bio
/gb1kg [t1d]/
f[pta]6xraa( b6oxta son.s] = fxtas
if (retval)[
throttled[Sb1kg=>b1kcg=>css, 1d] = count () :
)a2[a ↑
f ()4um0 = [pT*so
include 
kprobe:ov1_resd_iter
/S == nuT*SUe
Bread_ataxt [tld] - nsecs,
kretprobe:ovl_read_iter
- (xsexn.o (+ 1omxs"xse 1on.xs) ) /
$duxation_us = (nsecs - Bxead_start[tid]) / 1000
Bread_latency_us = hist ($duration_us) 
delete (@read_start[tid]1
---
## Page 723
686
Chapter 15 Containers
kprobe:ov]_vrite_iter
ufilenamel ): )*
15.5OptionalExercises
If not specified, these can be completed using either bpftrace or BCC:
1. Modify runqlat(8) from Chapter 6 to include the UTS namespace nodename
(see pixdnss(8).
2. Modify opensnoop(8) from Chapter 8 to include the UTS namespace nodename.
3. Develop a tool to show which containers are swapping out due to the mem cgroup
(see the mem_cgroup_swapout() kernel function).
15.6Summary
This chapter summarized Linux containers and showed how BPF tracing can expose container
CPU contention and cgroup throttling durations, as well as overlay FS latency.
---
## Page 725
This page intentionally left blank
---
## Page 726
Hypervisors
This chapter dliscusses the use of BPF tools with virtual machine hypervisors for hardware
virtualization, of which Xen and KVM are popular examples. BPF tools with OS-level
virtualizationcontainers—was discussed in the previous chapter.
Learning Objectives:
Understand hypervisor configurations and BPF tracing capabilities
 Trace guest hypercalls and exits, where possible
 Summarize stolen CPU time
This chapter begins with the necessary background for hardware virtualization analysis, describes
BPF capabilities and strategies for the different hypervisor situations, and includes some example
BPF toobs.
16.1
Background
Hardware virtualizati
including its own kernel. Two common configurations of hypervisors are shown in Figure 16-1.
Config A
Config B
Guest OS
Guest OS
Guest OS
Host OS
Guest OS
Guest OS
Apps
Apps
Apps
Apps
Host
Admin
G.Kerne
G.Kernel
G.Kernel
G.Kernel
G.Kernel
Hypervisor
Seheduler
Hypervisor
Host Kernel
Hype
Hardware (Processors)
Hardware (Proce
ors)
Figure 16-1 Common hypervisor configurations
---
## Page 727
069
0 Chapter 16 Hypervisors
A common classification of hypervisors identifies them as type 1 or 2 [Goldberg 73]. However
with advancements in these technologies these types are no longer a practical distinction [173]
as type 2 has become type 1-ish by using kernel modules. The following instead describes two
common configurations shown in Figure 16-1:
• Config A: This configuration is called a native hypervisor or a bare-metal hypervisor. The
hypervisor software runs directly on the processors, which creates domains for running
guest virtual machines and schedules virtual guest CPUs onto the real CPUs. A privileged
domain (number 0 in Figure 16-1) can administer the others. A popular example is the Xen
hypervisor.
■ Config B: The hypervisor software is executed by a host OS kernel and may be composed
of kernel-level modules and user-level processes. The host OS has privileges to administer
the hypervisor, and its kernel schedules the VM CPUs along with other processes on the
host. By use of kernel modules, this configuration also provides direct access to hardware.
A popular example is the KVM hypervisor.
Both configurations may involve running an I/O proxy (e.g, the QEMU software) in domain 0
(Xen) or the host OS (KVM) for serving guest I/O. This adds overhead to I/O, and over the years
has been optimized by adding shared memory transports and other techniques.
The original hardware hypervisor, pioneered by VMware in 1998, used binary translations to
:q pasondu uaoq aous seq su zo arem] uogezena aempreq In uoad
 Processor virtualization support: The AMD-V and Intel VT-x extensions were introduced
in 20052006 to provide faster hardware support for VM operations by the processor.
• Paravirtualization (paravirt or PV): Instead of running an unmodified OS, with
paravirtualization, an OS can be made aware that it is running on a hardware virtual
Supssaood quapsa ano sop rostauad.6q auq os (steouadt seo [epoads aqeu pue atuqoeu
of some operations. For efficiency, Xen batches these hypercalls into a multicall.
 Device hardware support: To further optimize VM performance, hardware devices other
than processors have been adding virtual machine support. This includes SR-IOV for
network and storage devices and special drivers to use them: ixgbe, ena, and nvme.
Over the years, Xen has evolved and improved its performance. Modern Xen VMs often boot in
hardware VM mode (HVM) and then use PV drivers with HVM support to achieve the best of both
worlds: a configuration called PVHVM. This can further be improved by depending entirely on
hardware virtualization for some drivers, such as SR-IOV for network and storage devices.
In 2017, AWS launched the Nitro hypervisor, with parts based on KVM, and hardware support
for all main resources: processors, network, storage, interrupts, and timers [174]. No QEMU proxy
is usedl.
---
## Page 728
16.1   Background  691
16.1.1 BPF Capabilities
Because hardware VMs run their own kernel, they can use BPF tools from the guest. Questions
that BPF can help answer from the guest include:
What is the performance of the virtualized hardware resources? This can be answered using
tools described in previous chapters.
ostaadsq go ainsea e se ouape meoradtq st peqm uaq 'asn u st uopezienaered f 
performance?
● What are the frequency and duration of stolen CPU time?
· Are hypervisor interrupt callbacks interfering with an application?
If run from the host, BPF can help answer more questions (host access is available to cloud
computing provsders but not to thesr end users):
• If QEMU is in use, what workload is applied by the guest? What is the resulting
performance?
For config B hypervisors, for what reasons are guests exiting to the hypervisor?
Hardware hypervisor analysis with BPF is another area that may have future developments, ading
more capabilities and possibilities. Some future work is mentioned in the later tools sections.
AWS EC2 Guests
As hypervisors optimize performance by moving from emulation to paravirtualization to
hardware support, there are fewer targets to trace from the guest because events have moved to
jo sad a pue saotes z S jo uonpoa a qm puaedde uaaq seq su aempe
hypervisor targets that can be traced, listed below:
•PV: Hypercalls (multicalls), hypervisor call backs, driver calls, stolen time
 PVHVM: Hypervisor callbacks, driver calls, stolen time
• PVHVM+SR-IOV drivers: Hypervisor callbacks, stolen time
• KVM (Nitro): Stolen time
The most recent hypervisor, Nitro, has little code running in the guest that is special to hypervi-
sors. This is by design: it improves performance by moving hypervisor functionality to hardware.
16.1.2 Suggested Strategies
Start by determining what configuration of hardware hypervisor is in use. Are hypercalls being
used, or special device drivers?
For guests:
1. Instrument hypercalls (if in use) to check for excessive operations.
2. Check for CPU stolen time.
---
## Page 729
Z69
Chapter 16 Hypervisors
3. Use tools from prior chapters for resource analysis, bearing in mind that these are
au q pasodu spouo aomosau q paddeo aq eu aoueuoad rau saornosau [enqa
hypervisor or external hardware, and they may also suffer contention with access from
other guests.
For hosts:
1. Instrument VM exits to check for excessive operations.
2. If an I/O proxy is in use (QEMU), instrument its workload and latency
3. Use tools from prior chapters for resource analysis.
to be conducted using tools from prior chapters, rather than specialized tools for hypervisors.
As hypervisors move functionality to hardware, as is the case with Nitro, more analysis will need
16.2
TraditionalTools
There are not many tools for hypervisor performance analysis and troubleshooting. From the
guest, in some situations there are tracepoints for hypercalls, as shown in Section 16.3.1.
From the host, Xen provides its own tools, incluxding ×1 top and xent.race, for inspecting guest
resource usage. For KVM, the Linux perf(1) utility has a km subcommand. Example output:
+ perf kvm stat live
11:12:07, 687968
Analyze eventa for a11 VMs, al1 VCHUs:
VXEXIT Sanp1es Saxp1e31
TinelXin Tixe
Max Tise
Avg tine
XSR_M.ITE
166B
68.90%
0.281
0 . 67u3
31.74u8
（s0z*z -+en5z*E
HLT
66
19.251
86966
2.61us 100512.98us 4160.68us ( +- 14.77% )
PREENPTION_TTMER
112
4.63$
0.031
2.53ua
10.42us
4.71ua ( +- 2.68% )
FENDING_INTERRUFT
28
3.391
0.011
0. 92us
sn.g6°8T
3. 44us ( +-6.23% )
EXCTERGUAL_INTERRUPP
53
2.19$
0.011
0. 82ua
7.46u8
3.22us ( +-6.57% )
I0_INSTRUCTION
37
1.531
0.043
5.36us
84.88u5
19.97us ( +- 11.87% )
XSR_READ
2
0.08$
0.001
3.33us
4.80us
4.07ua ( +- 18.05%)
EPT_KISCONFIG
1
0.041
100°0
19. 94us
sn.}6°6T
19.94us ( + 0.00% )
Total Sanples:2421, Total events handled time:1946040,48us
This shows the reasons for virtual machine exit and statistics for each reason. The longest-
duration exits in this example output were for HLT (halt), as virtual CPUs enter the idlle state.
There are tracepoints for KVM events, including exits, which can be used with BPF to create more
detailed tools.