on a binary machine learning classiﬁer which is trained with
the data derived from shadow models (similar to our attacks).
More recently, multiple membership inference attacks have
been proposed with new attacking techniques or targeting on
different types of ML models [19, 27, 28, 31, 32, 38, 42, 53].
In theory, membership inference attack can be used to re-
construct the dataset, similar to our reconstruction attacks.
However, it is not scalable in the real-world setting as the
adversary needs to obtain a large-scale dataset which includes
all samples in the target model’s training set. Though our two
reconstruction attacks are designed speciﬁcally for the online
learning setting, we believe the underlying techniques we pro-
pose, i.e., pretrained decoder from a standard autoencoder and
CBM-GAN, can be further extended to reconstruct datasets
from black-box ML models in other settings.
Model Inversion. Fredrikson et al. [12] propose model in-
version attack ﬁrst on biomedical data. The goal of model
inversion is to infer some missing attributes of an input feature
vector based on the interaction with a trained ML model. Later,
other works generalize the model inversion attack to other set-
tings, e.g.„ reconstructing recognizable human faces [11, 20].
As pointed out by other works [29,40], model inversion attack
reconstructs a general representation of data samples afﬁliated
with certain labels, while our reconstruction attacks target on
speciﬁc data samples used in the updating set.
Model Stealing. Another related line of work is model steal-
ing. Tramèr et al. [45] are among the ﬁrst to introduce the
model stealing attack against black-box ML models. In this
attack, an adversary tries to learn the target ML model’s pa-
rameters. Tramèr et al. propose various attacking techniques
including equation-solving and decision tree path-ﬁnding.
The former has been demonstrated to be effective on simple
ML models, such as logistic regression, while the latter is
designed speciﬁcally for decision trees, a class of machine
learning classiﬁers. Moreover, relying on an active learning
based retraining strategy, the authors show that it is possible to
steal an ML model even if the model only provides the label
instead of posteriors as the output. More recently, Orekondy
et al. [34] propose a more advanced attack on stealing the
target model’s functionality and show that their attack is able
to replicate a mature commercial machine learning API. In
addition to model parameters, several works concentrate on
stealing ML models’ hyperparameters [33, 47].
Besides the above, there exist a wide range of other attacks
and defenses on machine learning models [4,5,8,9,13,14,16,
17, 22, 23, 25, 26, 35, 41, 43, 44, 46, 48–50, 52, 54–56].
9 Conclusion
Large-scale data being generated at every second turns ML
model training into a continuous process. In consequence, a
machine learning model queried with the same set of data
samples at two different time points will provide different
results. In this paper, we investigate whether these different
model outputs can constitute a new attack surface for an ad-
versary to infer information of the dataset used to perform
model update. We propose four different attacks in this sur-
face all of which follow a general encoder-decoder structure.
The encoder encodes the difference in the target model’s out-
put before and after being updated, and the decoder generates
different types of information regarding the updating set.
We start by exploring a simpliﬁed case when an ML model
is only updated with one single data sample. We propose
two different attacks for this setting. The ﬁrst attack shows
that the label of the single updating sample can be effectively
inferred. The second attack utilizes an autoencoder’s decoder
as the attack model’s pretrained decoder for single-sample
reconstruction.
We then generalize our attacks to the case when the updat-
ing set contains multiple samples. Our multi-sample label dis-
tribution estimation attack trained following a KL-divergence
loss is able to infer the label distribution of the updating set’s
data samples effectively. For the multi-sample reconstruction
attack, we propose a novel hybrid generative model, namely
CBM-GAN, which uses a “Best Match” loss in its objective
function. The “Best Match” loss directs CBM-GAN’s genera-
tor to reconstruct each sample in the updating set. Quantitative
and qualitative results show that our attacks achieve promising
performance.
Acknowledgments
We thank the anonymous reviewers, and our shepherd, David
Evans, for their helpful feedback and guidance.
The research leading to these results has received funding
from the European Research Council under the European
Union’s Seventh Framework Programme (FP7/2007-2013)/
ERC grant agreement no. 610150-imPACT.
USENIX Association
29th USENIX Security Symposium    1303
References
[1] https://www.cs.toronto.edu/~kriz/cifar.
html. 2
[2] http://yann.lecun.com/exdb/mnist/. 2
[3] https://pytorch.org/. 5
[4] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep Learning with Differential Privacy. In Proceed-
ings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security (CCS), pages 308–318.
ACM, 2016. 12, 13
[5] Anish Athalye, Nicholas Carlini, and David A. Wag-
ner. Obfuscated Gradients Give a False Sense of Secu-
rity: Circumventing Defenses to Adversarial Examples.
In Proceedings of the 2018 International Conference
on Machine Learning (ICML), pages 274–283. JMLR,
2018. 13
[6] Michael Backes, Mathias Humbert, Jun Pang, and Yang
Zhang. walk2friends: Inferring Social Links from Mo-
bility Proﬁles. In Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security
(CCS), pages 1943–1957. ACM, 2017. 2, 3
[7] Andrew Brock, Jeff Donahue, and Karen Simonyan.
Large Scale GAN Training for High Fidelity Natural Im-
age Synthesis. In Proceedings of the 2-19 International
Conference on Learning Representations (ICLR), 2-19.
8
[8] Nicholas Carlini and David Wagner. Towards Evaluating
the Robustness of Neural Networks. In Proceedings of
the 2017 IEEE Symposium on Security and Privacy
(S&P), pages 39–57. IEEE, 2017. 13
[9] Kamalika Chaudhuri and Claire Monteleoni. Privacy-
preserving Logistic Regression. In Proceedings of the
2009 Annual Conference on Neural Information Pro-
cessing Systems (NIPS), pages 289–296. NIPS, 2009.
12, 13
[10] Cynthia Dwork and Aaron Roth. The Algorithmic Foun-
dations of Differential Privacy. Foundations and Trends
in Theoretical Computer Science, 2014. 12
[11] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model Inversion Attacks that Exploit Conﬁdence Infor-
In Proceedings
mation and Basic Countermeasures.
of the 2015 ACM SIGSAC Conference on Computer
and Communications Security (CCS), pages 1322–1333.
ACM, 2015. 1, 13
[12] Matt Fredrikson, Eric Lantz, Somesh Jha, Simon Lin,
David Page, and Thomas Ristenpart. Privacy in Pharma-
cogenetics: An End-to-End Case Study of Personalized
Warfarin Dosing. In Proceedings of the 2014 USENIX
Security Symposium (USENIX Security), pages 17–32.
USENIX, 2014. 13
[13] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and
Nikita Borisov. Property Inference Attacks on Fully
Connected Neural Networks using Permutation Invari-
ant Representations. In Proceedings of the 2018 ACM
SIGSAC Conference on Computer and Communications
Security (CCS), pages 619–633. ACM, 2018. 1, 2, 3, 13
[14] Adrià Gascón, Phillipp Schoppmann, Borja Balle, Mar-
iana Raykova, Jack Doerner, Samee Zahur, and David
Evans. Privacy-Preserving Distributed Linear Regres-
sion on High-Dimensional Data. Symposium on Privacy
Enhancing Technologies Symposium, 2017. 13
[15] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning. The MIT Press, 2016. 12
[16] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and Harnessing Adversarial Examples. In
Proceedings of the 2015 International Conference on
Learning Representations (ICLR), 2015. 13
[17] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, and
Gang Wang abd Xinyu Xing. LEMNA: Explaining Deep
Learning based Security Applications. In Proceedings
of the 2018 ACM SIGSAC Conference on Computer and
Communications Security (CCS), pages 364–379. ACM,
2018. 13
[18] Inken Hagestedt, Yang Zhang, Mathias Humbert, Pas-
cal Berrang, Haixu Tang, XiaoFeng Wang, and Michael
Backes. MBeacon: Privacy-Preserving Beacons for
DNA Methylation Data. In Proceedings of the 2019
Network and Distributed System Security Symposium
(NDSS). Internet Society, 2019. 13
[19] Jamie Hayes, Luca Melis, George Danezis, and Emil-
iano De Cristofaro. LOGAN: Evaluating Privacy Leak-
age of Generative Models Using Generative Adversarial
Networks. Symposium on Privacy Enhancing Technolo-
gies Symposium, 2019. 13
[20] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-
Cruz. Deep Models Under the GAN: Information Leak-
age from Collaborative Deep Learning. In Proceedings
of the 2017 ACM SIGSAC Conference on Computer and
Communications Security (CCS), pages 603–618. ACM,
2017. 1, 13
[21] Nils Homer, Szabolcs Szelinger, Margot Redman, David
Duggan, Waibhav Tembe, Jill Muehling, John V. Pear-
son, Dietrich A. Stephan, Stanley F. Nelson, and
1304    29th USENIX Security Symposium
USENIX Association
David W. Craig. Resolving Individuals Contribut-
ing Trace Amounts of DNA to Highly Complex Mix-
tures Using High-Density SNP Genotyping Microarrays.
PLOS Genetics, 2008. 13
[22] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang
Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating
Machine Learning: Poisoning Attacks and Countermea-
sures for Regression Learning. In Proceedings of the
2018 IEEE Symposium on Security and Privacy (S&P).
IEEE, 2018. 13
[23] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang
Zhang, and Neil Zhenqiang Gong. MemGuard: Defend-
ing against Black-Box Membership Inference Attacks
via Adversarial Examples. In Proceedings of the 2019
ACM SIGSAC Conference on Computer and Commu-
nications Security (CCS), pages 259–274. ACM, 2019.
13
[24] Harold W Kuhn. The Hungarian Method for the As-
signment Problem. Naval Research Logistics Quarterly,
1955. 10
[25] Bo Li and Yevgeniy Vorobeychik. Scalable Optimiza-
tion of Randomized Operational Decisions in Adversar-
ial Classiﬁcation Settings. In Proceedings of the 2015
International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), pages 599–607. PMLR, 2015. 13
[26] Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing
Guo. How to Prove Your Model Belongs to You: A
Blind-Watermark based Framework to Protect Intellec-
tual Property of DNN. In Proceedings of the 2019 An-
nual Computer Security Applications Conference (AC-
SAC). ACM, 2019. 13
[27] Yunhui Long, Vincent Bindschaedler, and Carl A.
Gunter. Towards Measuring Membership Privacy. CoRR
abs/1712.09136, 2017. 13
[28] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue
Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter,
and Kai Chen. Understanding Membership Infer-
ences on Well-Generalized Learning Models. CoRR
abs/1802.04889, 2018. 13
[29] Luca Melis, Congzheng Song, Emiliano De Cristofaro,
and Vitaly Shmatikov. Exploiting Unintended Feature
In Proceedings
Leakage in Collaborative Learning.
of the 2019 IEEE Symposium on Security and Privacy
(S&P). IEEE, 2019. 13
[30] Mehdi Mirza and Simon Osindero. Conditional Gen-
erative Adversarial Nets. CoRR abs/1411.1784, 2014.
8
[31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Ma-
chine Learning with Membership Privacy using Adver-
sarial Regularization. In Proceedings of the 2018 ACM
SIGSAC Conference on Computer and Communications
Security (CCS). ACM, 2018. 13
[32] Milad Nasr, Reza Shokri, and Amir Houmansadr. Com-
prehensive Privacy Analysis of Deep Learning: Passive
and Active White-box Inference Attacks against Cen-
tralized and Federated Learning. In Proceedings of the
2019 IEEE Symposium on Security and Privacy (S&P).
IEEE, 2019. 13
[33] Seong Joon Oh, Max Augustin, Bernt Schiele, and Mario
Fritz. Towards Reverse-Engineering Black-Box Neural
Networks. In Proceedings of the 2018 International
Conference on Learning Representations (ICLR), 2018.
3, 13
[34] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff Nets: Stealing Functionality of Black-Box
Models. In Proceedings of the 2019 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).
IEEE, 2019. 13
[35] Nicolas Papernot, Patrick D. McDaniel, Ian Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami.
Practical Black-Box Attacks Against Machine Learning.
In Proceedings of the 2017 ACM Asia Conference on
Computer and Communications Security (ASIACCS),
pages 506–519. ACM, 2017. 13
[36] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De
Cristofaro. Knock Knock, Who’s There? Membership
Inference on Aggregate Location Data. In Proceedings
of the 2018 Network and Distributed System Security
Symposium (NDSS). Internet Society, 2018. 13
[37] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De
Cristofaro. Under the Hood of Membership Inference
Attacks on Aggregate Location Time-Series. CoRR
abs/1902.07456, 2019. 13
[38] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal
Berrang, Mario Fritz, and Michael Backes. ML-Leaks:
Model and Data Independent Membership Inference
Attacks and Defenses on Machine Learning Models.
In Proceedings of the 2019 Network and Distributed
System Security Symposium (NDSS). Internet Society,
2019. 2, 3, 13
[39] Reza Shokri and Vitaly Shmatikov. Privacy-Preserving
In Proceedings of the 2015 ACM