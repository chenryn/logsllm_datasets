### Binary Machine Learning Classifier and Membership Inference Attacks

A binary machine learning classifier is trained using data derived from shadow models, similar to our attack methods. Recently, multiple membership inference attacks have been proposed, employing new techniques or targeting different types of ML models [19, 27, 28, 31, 32, 38, 42, 53]. Theoretically, membership inference attacks can be used to reconstruct the dataset, much like our reconstruction attacks. However, in real-world settings, this approach is not scalable because the adversary needs a large-scale dataset that includes all samples from the target model’s training set. Although our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques—such as the pretrained decoder from a standard autoencoder and CBM-GAN—can be extended to reconstruct datasets from black-box ML models in other contexts.

### Model Inversion

Fredrikson et al. [12] first introduced model inversion attacks on biomedical data. The goal of model inversion is to infer missing attributes of an input feature vector based on interactions with a trained ML model. Subsequent works have generalized model inversion attacks to other settings, such as reconstructing recognizable human faces [11, 20]. As noted by other researchers [29, 40], model inversion attacks reconstruct a general representation of data samples associated with certain labels, whereas our reconstruction attacks target specific data samples used in the updating set.

### Model Stealing

Another related line of work is model stealing. Tramèr et al. [45] were among the first to introduce model stealing attacks against black-box ML models. In these attacks, an adversary attempts to learn the target ML model’s parameters. Tramèr et al. proposed various attacking techniques, including equation-solving and decision tree path-finding. Equation-solving has been shown to be effective on simple ML models like logistic regression, while decision tree path-finding is tailored for decision trees. Additionally, using an active learning-based retraining strategy, they demonstrated that it is possible to steal an ML model even if it only provides labels instead of posteriors. More recently, Orekondy et al. [34] proposed a more advanced attack to steal the target model’s functionality, showing that their method can replicate a mature commercial machine learning API. Besides model parameters, several works have focused on stealing ML models’ hyperparameters [33, 47].

### Other Attacks and Defenses

There is a wide range of other attacks and defenses on machine learning models [4, 5, 8, 9, 13, 14, 16, 17, 22, 23, 25, 26, 35, 41, 43, 44, 46, 48–50, 52, 54–56].

### Conclusion

The continuous generation of large-scale data turns ML model training into a dynamic process. Consequently, a machine learning model queried with the same set of data samples at different times will produce different results. In this paper, we investigate whether these varying model outputs can create a new attack surface for adversaries to infer information about the dataset used for model updates. We propose four different attacks, all following a general encoder-decoder structure. The encoder captures the difference in the target model’s output before and after updates, and the decoder generates information about the updating set.

We start by examining a simplified case where an ML model is updated with a single data sample. We propose two attacks for this setting: the first effectively infers the label of the single updating sample, and the second uses an autoencoder’s decoder as the pretrained decoder for single-sample reconstruction.

We then generalize our attacks to scenarios where the updating set contains multiple samples. Our multi-sample label distribution estimation attack, trained with a KL-divergence loss, effectively infers the label distribution of the updating set’s data samples. For the multi-sample reconstruction attack, we propose a novel hybrid generative model, CBM-GAN, which uses a "Best Match" loss in its objective function. This loss guides CBM-GAN’s generator to reconstruct each sample in the updating set. Both quantitative and qualitative results show that our attacks achieve promising performance.

### Acknowledgments

We thank the anonymous reviewers and our shepherd, David Evans, for their valuable feedback and guidance.

This research was funded by the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no. 610150-imPACT.

### References

[References listed here as in the original text]

---

This revised version aims to improve clarity, coherence, and professionalism, ensuring that the content is well-structured and easy to follow.