exact same problem that monitoring would detect. Such a test enables the push to be
blocked so the bug never reaches production (though it still needs to be repaired in
the source code). Repairing zero MTTR bugs by blocking a push is both quick and
convenient. The more bugs you can find with zero MTTR, the higher the Mean Time
Between Failures (MTBF) experienced by your users.
As MTBF increases in response to better testing, developers are encouraged to release
features faster. Some of these features will, of course, have bugs. New bugs result in an
opposite adjustment to release velocity as these bugs are found and fixed.
2 For further reading on equivalence, see http://stackoverflow.com/questions/1909280/equivalence-class-testing-
vs-boundary-value-testing.
184 | Chapter 17: Testing for Reliability
Authors writing about software testing largely agree on what coverage is needed.
Most conflicts of opinion stem from conflicting terminology, differing emphasis on
the impact of testing in each of the software lifecycle phases, or the particularities of
the systems on which they’ve conducted testing. For a discussion about testing at
Google in general, see [Whi12]. The following sections specify how software testing–
related terminology is used in this chapter.
Types of Software Testing
Software tests broadly fall into two categories: traditional and production. Traditional
tests are more common in software development to evaluate the correctness of soft‐
ware offline, during development. Production tests are performed on a live web ser‐
vice to evaluate whether a deployed software system is working correctly.
Traditional Tests
As shown in Figure 17-1, traditional software testing begins with unit tests. Testing of
more complex functionality is layered atop unit tests.
Figure 17-1. The hierarchy of traditional tests
Unit tests
A unit test is the smallest and simplest form of software testing. These tests are
employed to assess a separable unit of software, such as a class or function, for cor‐
rectness independent of the larger software system that contains the unit. Unit tests
are also employed as a form of specification to ensure that a function or module
Types of Software Testing | 185
exactly performs the behavior required by the system. Unit tests are commonly used
to introduce test-driven development concepts.
Integration tests
Software components that pass individual unit tests are assembled into larger compo‐
nents. Engineers then run an integration test on an assembled component to verify
that it functions correctly. Dependency injection, which is performed with tools such
as Dagger,3 is an extremely powerful technique for creating mocks of complex depen‐
dencies so that an engineer can cleanly test a component. A common example of a
dependency injection is to replace a stateful database with a lightweight mock that has
precisely specified behavior.
System tests
A system test is the largest scale test that engineers run for an undeployed system. All
modules belonging to a specific component, such as a server that passed integration
tests, are assembled into the system. Then the engineer tests the end-to-end function‐
ality of the system. System tests come in many different flavors:
Smoke tests
Smoke tests, in which engineers test very simple but critical behavior, are among
the simplest type of system tests. Smoke tests are also known as sanity testing, and
serve to short-circuit additional and more expensive testing.
Performance tests
Once basic correctness is established via a smoke test, a common next step is to
write another variant of a system test to ensure that the performance of the sys‐
tem stays acceptable over the duration of its lifecycle. Because response times for
dependencies or resource requirements may change dramatically during the
course of development, a system needs to be tested to make sure that it doesn’t
become incrementally slower without anyone noticing (before it gets released to
users). For example, a given program may evolve to need 32 GB of memory when
it formerly only needed 8 GB, or a 10 ms response time might turn into 50 ms,
and then into 100 ms. A performance test ensures that over time, a system
doesn’t degrade or become too expensive.
Regression tests
Another type of system test involves preventing bugs from sneaking back into the
codebase. Regression tests can be analogized to a gallery of rogue bugs that his‐
torically caused the system to fail or produce incorrect results. By documenting
these bugs as tests at the system or integration level, engineers refactoring the
3 See https://google.github.io/dagger/.
186 | Chapter 17: Testing for Reliability
codebase can be sure that they don’t accidentally introduce bugs that they’ve
already invested time and effort to eliminate.
It’s important to note that tests have a cost, both in terms of time and computa‐
tional resources. At one extreme, unit tests are very cheap in both dimensions, as
they can usually be completed in milliseconds on the resources available on a lap‐
top. At the other end of the spectrum, bringing up a complete server with
required dependencies (or mock equivalents) to run related tests can take signifi‐
cantly more time—from several minutes to multiple hours—and possibly require
dedicated computing resources. Mindfulness of these costs is essential to devel‐
oper productivity, and also encourages more efficient use of testing resources.
Production Tests
Production tests interact with a live production system, as opposed to a system in a
hermetic testing environment. These tests are in many ways similar to black-box
monitoring (see Chapter 6), and are therefore sometimes called black-box testing.
Production tests are essential to running a reliable production service.
Rollouts Entangle Tests
It’s often said that testing is (or should be) performed in a hermetic environment
[Nar12]. This statement implies that production is not hermetic. Of course, produc‐
tion usually isn’t hermetic, because rollout cadences make live changes to the produc‐
tion environment in small and well-understood chunks.
To manage uncertainty and hide risk from users, changes might not be pushed live in
the same order that they were added to source control. Rollouts often happen in
stages, using mechanisms that gradually shuffle users around, in addition to monitor‐
ing at each stage to ensure that the new environment isn’t hitting anticipated yet
unexpected problems. As a result, the entire production environment is intentionally
not representative of any given version of a binary that’s checked into source control.
It’s possible for source control to have more than one version of a binary and its asso‐
ciated configuration file waiting to be made live. This scenario can cause problems
when tests are conducted against the live environment. For example, the test might
use the latest version of a configuration file located in source control along with an
older version of the binary that’s live. Or it might test an older version of the configu‐
ration file and find a bug that’s been fixed in a newer version of the file.
Similarly, a system test can use the configuration files to assemble its modules before
running the test. If the test passes, but its version is one in which the configuration
test (discussed in the following section) fails, the result of the test is valid hermetically,
but not operationally. Such an outcome is inconvenient.
Types of Software Testing | 187
Configuration test
At Google, web service configurations are described in files that are stored in our ver‐
sion control system. For each configuration file, a separate configuration test exam‐
ines production to see how a particular binary is actually configured and reports
discrepancies against that file. Such tests are inherently not hermetic, as they operate
outside the test infrastructure sandbox.
Configuration tests are built and tested for a specific version of the checked-in con‐
figuration file. Comparing which version of the test is passing in relation to the goal
version for automation implicitly indicates how far actual production currently lags
behind ongoing engineering work.
These nonhermetic configuration tests tend to be especially valuable as part of a dis‐
tributed monitoring solution since the pattern of passes/fails across production can
identify paths through the service stack that don’t have sensible combinations of the
local configurations. The monitoring solution’s rules try to match paths of actual user
requests (from the trace logs) against that set of undesirable paths. Any matches
found by the rules become alerts that ongoing releases and/or pushes are not pro‐
ceeding safely and remedial action is needed.
Configuration tests can be very simple when the production deployment uses the
actual file content and offers a real-time query to retrieve a copy of the content. In
this case, the test code simply issues that query and diffs the response against the file.
The tests become more complex when the configuration does one of the following:
• Implicitly incorporates defaults that are built into the binary (meaning that the
tests are separately versioned as a result)
• Passes through a preprocessor such as bash into command-line flags (rendering
the tests subject to expansion rules)
• Specifies behavioral context for a shared runtime (making the tests depend on
that runtime’s release schedule)
Stress test
In order to safely operate a system, SREs need to understand the limits of both the
system and its components. In many cases, individual components don’t gracefully
degrade beyond a certain point—instead, they catastrophically fail. Engineers use
stress tests to find the limits on a web service. Stress tests answer questions such as:
• How full can a database get before writes start to fail?
• How many queries a second can be sent to an application server before it
becomes overloaded, causing requests to fail?
188 | Chapter 17: Testing for Reliability
Canary test
The canary test is conspicuously absent from this list of production tests. The term
canary comes from the phrase “canary in a coal mine,” and refers to the practice of
using a live bird to detect toxic gases before humans were poisoned.
To conduct a canary test, a subset of servers is upgraded to a new version or configu‐
ration and then left in an incubation period. Should no unexpected variances
occur, the release continues and the rest of the servers are upgraded in a progressive
fashion.4 Should anything go awry, the single modified server can be quickly reverted
to a known good state. We commonly refer to the incubation period for the upgraded
server as “baking the binary.”
A canary test isn’t really a test; rather, it’s structured user acceptance. Whereas config‐
uration and stress tests confirm the existence of a specific condition over determinis‐
tic software, a canary test is more ad hoc. It only exposes the code under test to less
predictable live production traffic, and thus, it isn’t perfect and doesn’t always catch
newly introduced faults.
To provide a concrete example of how a canary might proceed: consider a given
underlying fault that relatively rarely impacts user traffic and is being deployed with
an upgrade rollout that is exponential. We expect a growing cumulative number of
reported variances CU =RK where R is the rate of those reports, U is the order of the
fault (defined later), and K is the period over which the traffic grows by a factor of e,
or 172%.5
In order to avoid user impact, a rollout that triggers undesirable variances needs to be
quickly rolled back to the prior configuration. In the short time it takes automation to
observe the variances and respond, it is likely that several additional reports will be
generated. Once the dust has settled, these reports can estimate both the cumulative
number C and rate R.
Dividing and correcting for K gives an estimate of U, the order of the underlying
fault.6 Some examples:
• U=1: The user’s request encountered code that is simply broken.
4 A standard rule of thumb is to start by having the release impact 0.1% of user traffic, and then scaling by
orders of magnitude every 24 hours while varying the geographic location of servers being upgraded (then on
day 2: 1%, day 3: 10%, day 4: 100%).
5 For instance, assuming a 24 hour interval of continuous exponential growth between 1% and 10%,
86400
K= =37523 seconds, or about 10 hours and 25 minutes.
0.1
ln
0.01
6 We’re using order here in the sense of “big O notation” order of complexity. For more context, see https://
en.wikipedia.org/wiki/Big_O_notation.
Types of Software Testing | 189
• U=2: This user’s request randomly damages data that a future user’s request may
see.
• U=3: The randomly damaged data is also a valid identifier to a previous request.
Most bugs are of order one: they scale linearly with the amount of user traffic [Per07].
You can generally track down these bugs by converting logs of all requests with
unusual responses into new regression tests. This strategy doesn’t work for higher-
order bugs; a request that repeatedly fails if all the preceding requests are attempted
in order will suddenly pass if some requests are omitted. It is important to catch these
higher-order bugs during release, because otherwise, operational workload can
increase very quickly.
Keeping the dynamics of higher- versus lower-order bugs in mind, when you are
using an exponential rollout strategy, it isn’t necessary to attempt to achieve fairness
among fractions of user traffic. As long as each method for establishing a fraction
uses the same K interval, the estimate of U will be valid even though you can’t yet
determine which method was instrumental in illuminating the fault. Using many
methods sequentially while permitting some overlap keeps the value of K small. This
strategy minimizes the total number of user-visible variances C while still allowing an
early estimate of U (hoping for 1, of course).
Creating a Test and Build Environment
While it’s wonderful to think about these types of tests and failure scenarios on day
one of a project, frequently SREs join a developer team when a project is already well
underway—once the team’s project validates its research model, its library proves that
the project’s underlying algorithm is scalable, or perhaps when all of the user interface
mocks are finally acceptable. The team’s codebase is still a prototype and comprehen‐
sive testing hasn’t yet been designed or deployed. In such situations, where should
your testing efforts begin? Conducting unit tests for every key function and class is a
completely overwhelming prospect if the current test coverage is low or nonexistent.
Instead, start with testing that delivers the most impact with the least effort.
You can start your approach by asking the following questions:
• Can you prioritize the codebase in any way? To borrow a technique from feature
development and project management, if every task is high priority, none of the
tasks are high priority. Can you stack-rank the components of the system you’re
testing by any measure of importance?
• Are there particular functions or classes that are absolutely mission-critical or
business-critical? For example, code that involves billing is a commonly business-
critical. Billing code is also frequently cleanly separable from other parts of the
system.
190 | Chapter 17: Testing for Reliability
• Which APIs are other teams integrating against? Even the kind of breakage that
never makes it past release testing to a user can be extremely harmful if it confu‐
ses another developer team, causing them to write wrong (or even just subopti‐
mal) clients for your API.
Shipping software that is obviously broken is among the most cardinal sins of a
developer. It takes little effort to create a series of smoke tests to run for every release.
This type of low-effort, high-impact first step can lead to highly tested, reliable
software.
One way to establish a strong testing culture7 is to start documenting all reported
bugs as test cases. If every bug is converted into a test, each test is supposed to ini‐
tially fail because the bug hasn’t yet been fixed. As engineers fix the bugs, the software
passes testing and you’re on the road to developing a comprehensive regression test
suite.
Another key task for creating well-tested software is to set up a testing infrastructure.
The foundation for a strong testing infrastructure is a versioned source control sys‐
tem that tracks every change to the codebase.
Once source control is in place, you can add a continuous build system that builds the
software and runs tests every time code is submitted. We’ve found it optimal if the
build system notifies engineers the moment a change breaks a software project. At the
risk of sounding obvious, it’s essential that the latest version of a software project in
source control is working completely. When the build system notifies engineers about
broken code, they should drop all of their other tasks and prioritize fixing the prob‐
lem. It is appropriate to treat defects this seriously for a few reasons:
• It’s usually harder to fix what’s broken if there are changes to the codebase after
the defect is introduced.
• Broken software slows down the team because they must work around the
breakage.
• Release cadences, such as nightly and weekly builds, lose their value.
• The ability of the team to respond to a request for an emergency release (for
example, in response to a security vulnerability disclosure) becomes much more
complex and difficult.
The concepts of stability and agility are traditionally in tension in the world of SRE.
The last bullet point provides an interesting case where stability actually drives agility.
When the build is predictably solid and reliable, developers can iterate faster!
7 For more on this topic, we highly recommend [Bla14] by our former coworker and ex-Googler, Mike Bland.
Creating a Test and Build Environment | 191
Some build systems like Bazel8 have valuable features that afford more precise control
over testing. For example, Bazel creates dependency graphs for software projects.
When a change is made to a file, Bazel only rebuilds the part of the software that
depends on that file. Such systems provide reproducible builds. Instead of running all
tests at every submit, tests only run for changed code. As a result, tests execute
cheaper and faster.
There are a variety of tools to help you quantify and visualize the level of test cover‐
age you need [Cra10]. Use these tools to shape the focus of your testing: approach the
prospect of creating highly tested code as an engineering project rather than a philo‐
sophical mental exercise. Instead of repeating the ambiguous refrain “We need more
tests,” set explicit goals and deadlines.
Remember that not all software is created equal. Life-critical or revenue-critical sys‐
tems demand substantially higher levels of test quality and coverage than a non-
production script with a short shelf life.
Testing at Scale
Now that we’ve covered the fundamentals of testing, let’s examine how SRE takes a