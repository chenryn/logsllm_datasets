A1C
CA1
VLAN-capable
physical switch
B1C
CB1
Host C
physical NIC
(VLAN un-aware)
eth0
A1C
CA1
B1C
CB1
Figure 6: Prototype Implementation of TVDs.
a VM are captured by the kernel module implementing part
of the vSwitch as they are received on the corresponding
back-end device in Dom0. The packets are encapsulated us-
ing EtherIP with the network identiﬁer ﬁeld set to match
the identiﬁer of the vSwitch that the VM is supposed to be
plugged into. The EtherIP packet is given either a multicast
or unicast IP address and simply fed into the Dom0 IP stack
for routing onto the physical network. The kernel module
also receives EtherIP packets destined for the physical host.
The module un-encapsulates the Ethernet frames contained
in the encapsulated EtherIP packets and transmits the raw
frame over the appropriate virtual network interface so that
it is received by the intended guest vNIC.
In addition to the kernel module for EtherIP processing,
we have also implemented a kernel module for VLAN tag-
ging in Dom0 of each virtualized host. Ethernet packets sent
by a VM are grabbed at the same point in the Dom0 net-
work stack as in the case of EtherIP processing. However,
instead of wrapping the Ethernet packets in an IP packet,
the VLAN tagging module re-transmits the packets unmod-
iﬁed into a pre-conﬁgured Linux VLAN device (eth0.α and
eth0.β of hosts A and B, shown in Figure 6) matching the
VLAN that the VM’s vNIC is supposed to be connected
to. The VLAN device5 (provided by the standard Linux
kernel VLAN support) applies the right VLAN tag to the
packet before sending it out onto the physical wire through
the physical NIC. The VLAN tagging module also intercepts
VLAN packets arriving on the physical wire destined for a
VM. The module uses the standard Linux VLAN Ether-
net packet handler provided by the 8021q.ko kernel module
with a slight modiﬁcation: the handler removes the VLAN
tags and, based on the tag, maps packets to the appropriate
vSwitch (α or β) which, in turn, maps them to the corre-
5An alternative approach, which we will implement in the
future, is to directly tag the packet and send the tagged
packet straight out of the physical NIC without relying on
the standard Linux VLAN devices.
sponding back-end device (vif1.0 or vif2.0) in Dom0. The
packets eventually arrive at the corresponding front-end de-
vice (eth0 in Dom1 or Dom2) as plain Ethernet packets.
Below are some implementation issues we had to tackle in
realizing the VLAN and encapsulation approaches.
(1) Some Ethernet cards oﬀer VLAN tag ﬁltering and
tag removal/oﬄoad capabilities. Such capabilities are useful
when running just a single kernel on a physical platform, in
which case there is no need to maintain the tags for making
propagation decisions. However, for our virtual networking
extensions, the hardware device should not strip the tags
from packets on reception over the physical wire; instead,
the kernel modules we have implemented should decide to
which VM the packets should be forwarded. For this pur-
pose, we modiﬁed the Linux kernel tg3.ko and forcedeth.ko
network drivers so as to disable VLAN oﬄoading.
(2) For eﬃciency reasons, the Xen front-end and back-end
driver implementations avoid computing checksums between
them for TCP/IP and UDP/IP packets. We modiﬁed the
Xen code to also handle our EtherIP-encapsulated IP pack-
ets in a similar manner.
(3) The EtherIP encapsulation approach relies on map-
ping a virtual Ethernet broadcast domain to a IP multi-
cast domain. While this works in a LAN environment, we
encountered problems when creating VLAN segments that
span WAN-separated physical machines. We resolved this
issue by building uni-directional multicast tunnels between
successive LAN segments.
7. PERFORMANCE RESULTS
We now describe performance results for the prototype
implementation of our secure virtual networking framework.
We obtained the throughput results using the NetIO net-
work benchmark (version 1.23-2.1) and latency results using
the ping tool.
We used the NetIO network benchmark to measure the
)
c
e
s
B
M
/
(
t
u
p
h
g
u
o
r
h
T
140
120
100
80
60
40
20
0
)
c
e
s
B
M
/
(
t
u
p
h
g
u
o
r
h
T
140
120
100
80
60
40
20
0
Bridged VLAN EtherIP
1
2
4
Packet Size (kB)
(a) Tx Throughput
Bridged VLAN EtherIP
1
2
4
Packet Size (kB)
(b) Rx Throughput
8
8
Figure 7: NetIO Benchmark: Guest VM to Guest
VM Throughput.
network throughput for diﬀerent packet sizes of the TCP
protocol. We measured the Tx (outgoing) and Rx (incom-
ing) throughput for traﬃc from one guest VM to another
guest VM on the same physical host. For this purpose, we
ran one instance of the benchmark on one guest VM as a
server process and another instance on the second guest VM
to do the actual benchmark.
Figure 7 compares the throughput results for the standard
Xen-bridged conﬁguration (explained in Section 6) with con-
ﬁgurations that include our VLAN tagging and EtherIP en-
capsulation extensions. The graphs show that the perfor-
mance of our virtual networking extensions is comparable
to that of the standard Xen (bridge) conﬁguration. The
VLAN tagging extension performs slightly better than the
encapsulation extension for the Tx path, whereas the oppo-
site happens in the case of the Rx path.
The major cost in the Tx path for the EtherIP method is
having to allocate a fresh socket buﬀer (skb) and copy the
original buﬀer data into the fresh skb. When ﬁrst allocating
a skb, the Linux network stack allocates a ﬁxed amount of
headroom for the expected headers that will be added to the
packet as it goes down the stack. Unfortunately, not enough
space is allocated upfront to allow us to ﬁt in the EtherIP
header; so, we have to copy the data around, which is very
costly. However, there is some spare headroom space, which
is enough for the extra VLAN tag. As a result, the VLAN
tagging method does not suﬀer from the packet copying over-
head. The cost of copying data in the EtherIP case is greater
than the cost of traversing two network devices (the physi-
cal Ethernet device and the Linux-provided VLAN device)
for the VLAN packets. That is why the VLAN method is
more eﬃcient than the EtherIP approach for the Tx path.
Table 3: Round-trip Times using Ping.
Minimum Average Maximum Mean
Bridged
VLAN
EtherIP
0.158
0.171
0.174
0.208
0.233
0.239
0.295
0.577
0.583
Deviation
0.030
0.049
0.052
In a future version of our prototype, we will add a simple
ﬁx to the kernel to ensure that the initial skbs have enough
headroom upfront for the EtherIP header.
In the Rx path, there is no packet-copying overhead for
the EtherIP approach; the extra EtherIP header merely has
to be removed before the packet is sent to a VM. In the
VLAN case, the packets have to traverse two network de-
vices (as in the Tx path) and the vSwitch kernel module.
In the EtherIP case, the packets go straight from the phys-
ical device to the vSwitch kernel module. As a result of
the extra step of traversing the VLAN device, the VLAN
method performs slightly poorer than the EtherIP method
for the Rx path. Our next prototype will avoid using the
Linux VLAN code and have our vSwitch module do the tag-
ging/untagging directly as in the EtherIP case. We expect
this enhancement to bring the Rx throughput of the VLAN
approach on par with that of the EtherIP approach.
Table 3 shows the round-trip times between two guest
VMs on a physical host for the bridged, VLAN, and EtherIP
encapsulation cases obtained using the ping -c 100 host
command, i.e., 100 packets sent. The average round-trip
times for VLAN and EtherIP encapsulation are 12% and
14.9% higher than that of the standard Xen bridged conﬁg-
uration.
8. CONCLUSION
In this paper, we introduced a secure virtual networking
model and a framework for eﬃcient and security-enhanced
network virtualization. The key drivers of our framework
design were the security and management objectives of vir-
tualized data centers, which are meant to co-host IT infras-
tructures belonging to multiple departments of an organiza-
tion or even multiple organizations.
Our framework utilizes a combination of existing network-
ing technologies (such as Ethernet encapsulation, VLAN
tagging, and VPN) and security policy enforcement to con-
cretely realize the abstraction of Trusted Virtual Domains,
which can be thought of as security-enhanced variants of vir-
tualized network zones. Policies are speciﬁed and enforced
at the intra-TVD level (e.g., membership requirements) and
inter-TVD level (e.g., information ﬂow control).
Observing that manual conﬁguration of virtual networks
is usually error-prone, our design is oriented towards au-
tomation. To orchestrate the TVD conﬁguration and de-
ployment process, we introduced management entities called
TVD masters. Based on the capability models of the physi-
cal infrastructure that are given as input to them, the TVD
masters coordinate the set-up and population of TVDs using
a well-deﬁned protocol.
We described a Xen-based prototype that implements a
subset of our secure network virtualization framework de-
sign. The performance of our virtual networking extensions
is comparable to the standard Xen (bridge) conﬁguration.
There are many avenues one can take to enhance both the
networking and security aspects of this work. In the short-
term, we will enhance the prototype implementation with
more auto-deployment elements, such as the TVD master
and proxy.
In the mid-term, we will investigate ways to
enforce better separation between the various modules that
deal with diﬀerent TVDs on the same physical host. The
goal is to provide stronger isolation among the various re-
sources (such as TVD credentials) the modules have access
to. One possible solution is to employ a dedicated VM per
TVD on a given physical host for the management of the
TVD elements on that host. Such a solution would certainly
provide better isolation than our current implementation, in
which all TVD proxies reside on the same VM (Dom0). A
down-side of the solution is that it may require substantial
data transfer between the various VMs belonging to a TVD
and the management VM on the host. To deal with this
issue, eﬃcient inter-domain communication protocols need
to be investigated. In the long term, we will fully automate
the TVD deployment process, including key management.
We will also investigate the use of emerging Trusted Com-
puting technologies [22] for secure storage and handling of
TVD credentials.
Acknowledgements
We thank the other authors of [6] for valuable inputs. This
work has been partially funded by the European Commis-
sion as part of the OpenTC project www.opentc.net.
9. REFERENCES
[1] RFC 3378. EtherIP: Tunneling Ethernet Frames in IP
Datagrams.
[2] IEEE Standard 802.1Q-2003. Virtual Bridged Local
Area Networks. Technical Report ISBN
0-7381-3662-X.
[3] D. Andersen, H. Balakrishnan, F. Kaashoek, and
R. Morris. Resilient Overlay Networks. In Proc. 18th
ACM Symposium on Operating Systems Principles
(SOSP-2001), pages 131–145, New York, NY, USA,
2001. ACM Press.
[4] P. T. Barham, B. Dragovic, K. Fraser, S. Hand, T. L.
Harris, A. Ho, R. Neugebauer, I. Pratt, and
A. Warﬁeld. Xen and the Art of Virtualization. In
Proc. 19th ACM Symposium on Operating Systems
Principles (SOSP-2003), pages 164–177, October 2003.
[5] A. Bavier, M. Bowman, B. Chun, D. Culler, S. Karlin,
L. Peterson, T. Roscoe, and M. Wawrzoniak.
Operating Systems Support for Planetary-Scale
Network Services. In Proc. 1st Symposium on
Networked Systems Design and Implementation
(NSDI’04), San Francisco, CA, USA, 2004.
[6] A. Bussani, J. L. Griﬃn, B. Jansen, K. Julisch,
G. Karjoth, H. Maruyama, M. Nakamura, R. Perez,
M. Schunter, A. Tanner, L. van Doorn, E. V.
Herreweghen, M. Waidner, and S. Yoshihama. Trusted
Virtual Domains: Secure Foundation for Business and
IT Services. Research Report RC 23792, IBM
Research, November 2005.
[7] A. T. Campbell, M. E. Kounavis, D. A. Villela, J. B.
Vincente, H. G. De Meet, K. Miki, and K. S.
Kalaichelvan. Spawning Networks. IEEE Network,
13(4):16–29, July-August 1999.
[8] A. T. Campbell, J. Vicente, and D. A. Villela.
Managing Spawned Virtual Networks. In Proc. 1st
International Working Conference on Active Networks
(IWAN ’99), volume 1653, pages 249–261.
Springer-Verlag, 1999.
[9] C. I. Dalton. Xen Virtualization and Security.
Technical report, HP Security Oﬃce Report, August
2005.
[10] R. Davoli. VDE: Virtual Distributed Ethernet. In
Proc. 1st International Conference on Testbeds and
Research Infrastructures for the Development of
Networks and Communities (Tridentcom 2005), pages
213–220. IEEE Press, February 2005.
[11] S. W. Hunter, N. C. Strole, D. W. Cosby, and D. M.
Green. BladeCenter Networking. IBM Journal of
Research and Development, 49(6), 2005.
[12] X. Jiang and D. Xu. VIOLIN: Virtual Internetworking
on OverLay INfrastructure. In Parallel and Distributed
Processing and Applications, volume 3358 of LNCS,
pages 937–946. Springer-Verliag, Berlin, 2004.
[13] M. E. Kounavis, A. T. Campbell, S. Chou,
F. Modoux, J. Vicente, and H. Zhuang. The Genesis
Kernel: A Programming System for Spawning
Network Architectures. IEEE Journal on Selected
Areas in Communications, 19(3):511–526, March 2001.
[14] Common Criteria Project Sponsoring Organisations.
Common Criteria for Information Technology Security
Evaluation (version 2.0). May 1998, adopted by
ISO/IEC as Draft International Standard DIS 15408
1-3.
[15] L. Peterson, T. Anderson, D. Culler, and T. Roscoe. A
Blueprint for Introducing Disruptive Technology into
the Internet. SIGCOMM Comput. Commun. Rev.,
33(1):59–64, 2003.
[16] P. Ruth, X. Jiang, D. Xu, and S. Goasguen. Virtual
Distributed Environments in a Shared Infrastructure.
IEEE Computer, 38(5):63–69, May 2005.
[17] P. Ruth, J. Rhee, D. Xu, R. Kennell, and S. Goasguen.
Autonomic Live Adaptation of Virtual Computational
Environments in a Multi-Domain Infrastructure. In
Proc. IEEE International Conference on Autonomic
Computing (ICAC-2006), June 2006.
[18] A-R. Sadeghi and C. St¨uble. Property-based
Attestation for Computing Platforms: Caring about
Properties, not Mechanisms. In Proc. 2004 Workshop
on New Security Paradigms (NSPW-2004), pages
67–77, New York, NY, USA, 2005. ACM Press.
[19] A. Sundararaj and P. Dinda. Towards Virtual
Networks for Virtual Machine Grid Computing. In
Proc. 3rd USENIX Conference on Virtual Machine
Technology (VM 04), pp. 177-190, 2004.
[20] J. Touch. Dynamic Internet Overlay Deployment and
Management using the X-bone. Computer Networks,
36(2-3):117–135, 2001.
[21] B. Traversat, A. Arora, M. Abdelaziz, M. Doigou,
C. Haywood, J-C. Hugly, E. Pouyoul, and B. Yaeger.
Project JXTA 2.0 Super-Peer Virtual Network, 2003.
[22] Trusted Computing Group. TPM Main Speciﬁcation
v1.2, November 2003.
https://www.trustedcomputinggroup.org.