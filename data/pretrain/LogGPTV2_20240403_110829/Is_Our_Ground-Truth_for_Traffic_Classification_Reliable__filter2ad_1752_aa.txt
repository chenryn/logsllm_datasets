title:Is Our Ground-Truth for Traffic Classification Reliable?
author:Valent&apos;ın Carela-Español and
Tomasz Bujlow and
Pere Barlet-Ros
Is Our Ground-Truth for Traﬃc Classiﬁcation
Reliable?(cid:2)
Valentín Carela-Español1, Tomasz Bujlow2, and Pere Barlet-Ros1
1 UPC BarcelonaTech, Spain
{vcarela,pbarlet}@ac.upc.edu
2 Aalborg University, Denmark
PI:EMAIL
Abstract. The validation of the diﬀerent proposals in the traﬃc classi-
ﬁcation literature is a controversial issue. Usually, these works base their
results on a ground-truth built from private datasets and labeled by tech-
niques of unknown reliability. This makes the validation and comparison
with other solutions an extremely diﬃcult task. This paper aims to be
a ﬁrst step towards addressing the validation and trustworthiness prob-
lem of network traﬃc classiﬁers. We perform a comparison between 6
well-known DPI-based techniques, which are frequently used in the lit-
erature for ground-truth generation. In order to evaluate these tools we
have carefully built a labeled dataset of more than 500 000 ﬂows, which
contains traﬃc from popular applications. Our results present PACE, a
commercial tool, as the most reliable solution for ground-truth genera-
tion. However, among the open-source tools available, NDPI and espe-
cially Libprotoident, also achieve very high precision, while other, more
frequently used tools (e.g., L7-ﬁlter) are not reliable enough and should
not be used for ground-truth generation in their current form.
1 Introduction and Related Work
During the last decade, traﬃc classiﬁcation has considerably increased its rel-
evance, becoming a key aspect for many network related tasks. The explosion
of new applications and techniques to avoid detection (e.g., encryption, proto-
col obfuscation) have substantially increased the diﬃculty of traﬃc classiﬁcation.
The research community have thrown itself into this problem by proposing many
diﬀerent solutions. However, this problem is still far from being solved [1].
Most traﬃc classiﬁcation solutions proposed in the literature report very high
accuracy. However, these solutions mostly base their results on a private ground-
truth (i.e., dataset), usually labeled by techniques of unknown reliability (e.g.,
ports-based or DPI-based techniques [2–5]). That makes it very diﬃcult to com-
pare and validate the diﬀerent proposals. The use of private datasets is derived
(cid:2) This research was funded by the Spanish Ministry of Economy and Competitiveness
under contract TEC2011-27474 (NOMADS project), by the Comissionat per a Uni-
versitats i Recerca del DIUE de la Generalitat de Catalunya (ref. 2009SGR-1140)
and by the European Regional Development Fund (ERDF).
M. Faloutsos and A. Kuzmanovic (Eds.): PAM 2014, LNCS 8362, pp. 98–108, 2014.
c(cid:2) Springer International Publishing Switzerland 2014
Is Our Ground-Truth for Traﬃc Classiﬁcation Reliable?
99
Table 1. DPI-based techniques evaluated
Name
Applications
PACE
Version
1.41 (June 2012)
1.3.0 (June 2011)
rev. 6391 (March 2013)
2009.05.28 (May 2009)
2.0.6 (Nov 2012)
15.2(4)M2 (Nov 2012)
OpenDPI
NDPI
L7-ﬁlter
Libprotoident
NBAR
1000
100
170
110
250
85
from the lack of publicly available datasets with payload. Mainly because of pri-
vacy issues, researchers and practitioners are not allowed to share their datasets
with the research community. To the best of our knowledge, just one work has
tackled this problem. Gringoli et al. in [6] published anonymized traces without
payload, but accurately labeled using GT. This dataset is very interesting to
evaluate Machine Learning-based classiﬁers, but the lack of payload makes it
unsuitable for DPI-based evaluation.
Another crucial problem is the reliability of the techniques used to set the
ground-truth. Most papers show that researchers usually obtain their ground-
truth through port-based or DPI-based techniques [2–5]. The poor reliability of
port-based techniques is already well known, given the use of dynamic ports or
well-known ports of other applications [7, 8]. Although the reliability of DPI-
based techniques is still unknown, according to conventional wisdom they are,
in principle, one of the most accurate techniques.
Some previous works evaluated the accuracy of DPI-based techniques [3, 5,
9, 10]. These studies rely on a ground-truth generated by another DPI-based
tool [5], port-based technique [3] or a methodology of unknown reliability [9,10],
making their comparison very diﬃcult. Recently, a concomitant study to ours [10]
compared the performance of four DPI-based techniques (i.e., L7-ﬁlter, Tstat,
NDPI and Libprotoident). This parallel study conﬁrms some of the ﬁndings
of our work presenting NDPI and Libprotoident as the most accurate open-
source DPI-based techniques. In [11] the reliability of L7-ﬁlter and a port-based
technique was compared using a dataset obtained by GT [6] showing that both
techniques present severe problems to accurately classify the traﬃc.
This paper presents two main contributions. First, we publish a reliable la-
beled dataset with full packet payloads [12]. The dataset has been artiﬁcially
built in order to allow us its publication. However, we have manually simulated
diﬀerent behaviours to make it as representative as possible. We used VBS [13]
to guarantee the reliability of the labeling process. This tool can label the ﬂows
with the name of the process that created them. This allowed us to carefully
create a reliable ground-truth that can be used as a reference benchmark for the
research community. Second, using this dataset, we evaluated the performance
and compared the results of 6 well-known DPI-based techniques, presented in
Table 1, which are widely used for the ground-truth generation in the traﬃc
classiﬁcation literature.
These contributions pretend to be a ﬁrst step towards the impartial validation
of network traﬃc classiﬁers. They also provide to the research community some
100
V. Carela-Español, T. Bujlow, and P. Barlet-Ros
insights about the reliability of diﬀerent DPI-based techniques commonly used
in the literature for ground-truth generation.
2 Methodology
The Testbed. Our testbed is based on VMWare virtual machines (VM). We
installed three VM for our data generating stations and we equipped them with
Windows 7 (W7), Windows XP (XP), and Ubuntu 12.04 (LX). Additionally,
we installed a server VM for data storage. To collect and accurately label the
ﬂows, we adapted Volunteer-Based System (VBS) developed at Aalborg Univer-
sity [13]. The task of VBS is to collect information about Internet traﬃc ﬂows
(i.e., start time of the ﬂow, number of packets contained by the ﬂow, local and
remote IP addresses, local and remote ports, transport layer protocol) together
with detailed information about each packet (i.e., direction, size, TCP ﬂags,
and relative timestamp to the previous packet in the ﬂow). For each ﬂow, the
system also collects the process name associated with that ﬂow. The process
name is obtained from the system sockets. This way, we can ensure the appli-
cation associated to a particular traﬃc. Additionally, the system collects some
information about the HTTP content type (e.g., text/html, video/x-ﬂv). The
captured information is transmitted to the VBS server, which stores the data in
a MySQL database. The design of VBS was initially described in [13]. On every
data generating VM, we installed a modiﬁed version of VBS. The source code of
the modiﬁed version was published in [14] under a GPL license. The modiﬁed
version of the VBS client captures full Ethernet frames for each packet, extracts
HTTP URL and Referer ﬁelds. We added a module called pcapBuilder, which
is responsible for dumping the packets from the database to PCAP ﬁles. At the
same time, INFO ﬁles are generated to provide detailed information about each
ﬂow, which allows us to assign each packet from the PCAP ﬁle to an individual
ﬂow. We also added a module called logAnalyzer, which is responsible for ana-
lyzing the logs generated by the diﬀerent DPI tools, and assigning the results of
the classiﬁcation to the ﬂows stored in the database.
Selection of the Data. The process of building a representative dataset, which
characterizes a typical user behavior, is a challenging task, crucial on testing and
comparing diﬀerent traﬃc classiﬁers. Therefore, to ensure the proper diversity
and amount of the included data, we decided to combine the data on a multidi-
mensional level. Based on w3schools statistics, we selected Windows 7 (55.3 %
of all users), Windows XP (19.9 %), and Linux (4.8 %) - state for January 2013.
Apple computers (9.3 % of overall traﬃc) and mobile devices (2.2 %) were left
as future work. The selected applications are shown below.
– Web browsers: based on w3schools statistics: Chrome and Firefox (W7, XP,
LX), Internet Explorer (W7, XP).
– BitTorrent clients: based on CNET ranking: uTorrent and Bittorrent (W7,
XP), Frostwire and Vuze (W7, XP, LX)
Is Our Ground-Truth for Traﬃc Classiﬁcation Reliable?
101
– eDonkey clients: based on CNET ranking: eMule (W7, XP), aMule (LX)
– FTP clients: based on CNET ranking: FileZilla (W7, XP, LX), SmartFTP
Client (W7, XP), CuteFTP (W7, XP), WinSCP (W7, XP)
– Remote Desktop servers: built-in (W7, XP), xrdp (LX)
– SSH servers: sshd (LX)
– Background traﬃc: DNS and NTP (W7, XP, LX), NETBIOS (W7, XP)
The list of visited websites was based on the top 500 websites according to
Alexa statistics. We chose several of them taking into account their rank and
the nature of the website (e.g., search engines, social medias, national portals,
video websites) to assure the variety of produced traﬃc. These websites include:
Google, Facebook, YouTube, Yahoo!, Wikipedia, Java, and Justin.tv. For most
websites we performed several random clicks to linked external websites, which
should better characterize the real behavior of the real users and include also
other websites not included in the top 500 ranking. This also concerns search
engines, from which we manually generated random clicks to the destination
web sites. Each of the chosen websites was processed by each browser. In case
it was required to log into the website, we created fake accounts. In order to
make the dataset as representative as possible we have simulated diﬀerent hu-
man behaviors when using these websites. For instance, on Facebook, we log
in, interact with friends (e.g., chat, send messages, write in their walls), upload
pictures, create events or play games. On YouTube, we watched the 10 most
popular videos, which we randomly paused, resumed, and rewound backward
and forward. Also, we randomly made some comments and clicked Like or Not
like buttons. The detailed description of actions performed with the services is
listed in our technical report [15]. We tested the P2P (BitTorrent and eDonkey)
clients by downloading ﬁles of diﬀerent sizes and then leaving the ﬁles to be
seeded for some time, in order to obtain enough of traﬃc in both directions. We
tried to test every FTP client using both the active transfer mode (PORT) and
passive transfer mode (PASV), if the client supports such mode.
Extracting the Data for Processing. Each DPI tool can have diﬀerent require-
ments and features, so the extracting tool must handle all these issues. The PCAP
ﬁles provided to PACE, OpenDPI, L7-ﬁlter, NDPI, and Libprotoident are accom-
panied by INFO ﬁles, which contain the information about the start and end of
each ﬂow, together with the ﬂow identiﬁer. Because of that, the software, which
uses the DPI libraries, can create and terminate the ﬂows appropriately, as well
as to provide the classiﬁcation results together with the ﬂow identiﬁer. Preparing
the data for NBAR classiﬁcation is more complicated. There are no separate INFO
ﬁles describing the ﬂows, since the classiﬁcation is made directly on the router. We
needed to extract the packets in a way that allows the router to process and cor-
rectly group them into ﬂows. We achieved that by changing both the source and
destination MAC addresses during the extraction process. The destination MAC
address of every packet must match up with the MAC address of the interface of
the router, because the router cannot process any packet which is not directed to
its interface on the MAC layer. The source MAC address was set up to contain the
102
V. Carela-Español, T. Bujlow, and P. Barlet-Ros
Edonkey
BitTorrent
Table 2. Application classes in the dataset
Application No. of ﬂows No. of Megabytes
2823.88
2621.37
3089.06
1.74
4.03
13218.47
5.17
91.80
5757.32
3026.57
5907.15
176581
62845
876
6600
27786
132907
9445
26219
46669
427
771667
Browser HTTP
Browser RTMP
Unclassiﬁed
FTP
DNS
NTP
RDP
SSH
NETBIOS
identiﬁer of the ﬂow to which it belongs, so the ﬂows were recognized by the router
according to our demands. To the best of our knowledge, this is the ﬁrst work to
present a scientiﬁc performance evaluation of NBAR.
The Classiﬁcation Process. We designed a tool, called dpi_benchmark, which
can read the PCAP ﬁles and provide the packets one-by-one to PACE, OpenDPI,
L7-ﬁlter, NDPI and Libprotoident. All the ﬂows are started and terminated
based on the information from the INFO ﬁles. After the last packet of the ﬂow is
sent to the classiﬁer, the tool obtains the classiﬁcation label associated with that
ﬂow. The labels are written to the log ﬁles together with the ﬂow identiﬁer, which
makes us later able to relate the classiﬁcation results to the original ﬂows in the
database. A brief description of the DPI-tools used in this study is presented
in Table 1. Although some of the evaluated tools have multiple conﬁguration
parameters, we have used in our evaluation the default conﬁguration for most of
them. A detailed description of the evaluated DPI-tools and their conﬁgurations
can be found in [15].
Classiﬁcation by NBAR required us to set up a full working environment.
We used GNS3 - a graphical framework, which uses Dynamips to emulate our
Cisco hardware. We emulated the 7200 platform, since only for this platform
supported by GNS3 was available the newest version of Cisco IOS (version 15),
which contains Flexible NetFlow. The router was conﬁgured by us to use Flexible
NetFlow with NBAR on the created interface. Flexible NetFlow was set up to
create the ﬂows taking into account the same parameters as are used to create
the ﬂow by VBS. On the computer, we used tcpreplay to replay the PCAP ﬁles
to the router with the maximal speed, which did not cause packet loss. At the
same time, we used nfacctd, which is a part of PMACCT tools, to capture the
Flexible NetFlow records sent by the router to the computer. The records, which
contain the ﬂow identiﬁer (encoded as source MAC address) and the name of
the application recognized by NBAR, were saved into text log ﬁles. This process
is broadly elaborated in our technical report [15].
The Dataset. Our dataset contains 1 262 022 ﬂows captured during 66 days,
between February 25, 2013 and May 1, 2013, which account for 35.69 GB of pure
packet data. The application name tag was present for 520 993 ﬂows (41.28 % of
all the ﬂows), which account for 32.33 GB (90.59 %) of the data volume. Addition-