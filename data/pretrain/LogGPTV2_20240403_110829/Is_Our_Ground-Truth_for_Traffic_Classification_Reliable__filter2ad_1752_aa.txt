# Is Our Ground-Truth for Traffic Classification Reliable?

**Authors:**
- Valentín Carela-Español<sup>1</sup>
- Tomasz Bujlow<sup>2</sup>
- Pere Barlet-Ros<sup>1</sup>

**Affiliations:**
- <sup>1</sup>UPC BarcelonaTech, Spain
- <sup>2</sup>Aalborg University, Denmark

**Emails:**
- {vcarela, pbarlet}@ac.upc.edu
- PI:EMAIL (for Tomasz Bujlow)

## Abstract

The validation of traffic classification proposals in the literature is a contentious issue. These works often rely on ground-truth datasets that are privately sourced and labeled using methods of unknown reliability, making it extremely difficult to validate and compare different solutions. This paper aims to address the validation and trustworthiness problem of network traffic classifiers. We compare six well-known DPI-based techniques frequently used for ground-truth generation. To evaluate these tools, we have meticulously constructed a labeled dataset of over 500,000 flows, containing traffic from popular applications. Our results indicate that PACE, a commercial tool, is the most reliable for ground-truth generation. Among open-source tools, NDPI and Libprotoident achieve very high precision, while other frequently used tools (e.g., L7-filter) are not reliable enough for ground-truth generation in their current form.

## 1. Introduction and Related Work

Over the past decade, traffic classification has become increasingly important for various network-related tasks. The proliferation of new applications and techniques to evade detection (e.g., encryption, protocol obfuscation) has significantly increased the difficulty of traffic classification. The research community has proposed numerous solutions, but the problem remains unsolved [1].

Most traffic classification solutions report high accuracy, but they often base their results on private ground-truth datasets labeled by techniques of unknown reliability (e.g., port-based or DPI-based techniques [2–5]). This makes it challenging to compare and validate different proposals. The use of private datasets stems from the lack of publicly available datasets with payload, primarily due to privacy concerns. Researchers and practitioners are not allowed to share their datasets with the research community. To our knowledge, only one work has addressed this issue. Gringoli et al. [6] published anonymized traces without payload, but accurately labeled using GT. While this dataset is useful for evaluating machine learning-based classifiers, the lack of payload makes it unsuitable for DPI-based evaluation.

Another critical issue is the reliability of the techniques used to set the ground-truth. Most papers show that researchers typically obtain their ground-truth through port-based or DPI-based techniques [2–5]. The poor reliability of port-based techniques is well-documented due to the use of dynamic ports or well-known ports of other applications [7, 8]. Although the reliability of DPI-based techniques is still uncertain, they are generally considered one of the most accurate techniques.

Some previous works have evaluated the accuracy of DPI-based techniques [3, 5, 9, 10]. These studies rely on a ground-truth generated by another DPI-based tool [5], port-based technique [3], or a methodology of unknown reliability [9, 10], making their comparison very difficult. A recent study [10] compared the performance of four DPI-based techniques (i.e., L7-filter, Tstat, NDPI, and Libprotoident), confirming some of our findings and presenting NDPI and Libprotoident as the most accurate open-source DPI-based techniques. In [11], the reliability of L7-filter and a port-based technique was compared using a dataset obtained by GT [6], showing that both techniques have significant issues in accurately classifying traffic.

This paper makes two main contributions. First, we publish a reliable labeled dataset with full packet payloads [12]. The dataset was artificially built to allow its publication, and we manually simulated different behaviors to make it as representative as possible. We used VBS [13] to ensure the reliability of the labeling process, which can label flows with the name of the process that created them. This allowed us to create a reliable ground-truth that can serve as a reference benchmark for the research community. Second, using this dataset, we evaluated the performance and compared the results of six well-known DPI-based techniques, as shown in Table 1, which are widely used for ground-truth generation in traffic classification literature.

These contributions aim to be a first step towards impartial validation of network traffic classifiers and provide the research community with insights into the reliability of different DPI-based techniques commonly used for ground-truth generation.

## 2. Methodology

### 2.1 The Testbed

Our testbed is based on VMware virtual machines (VMs). We installed three VMs for data generation, equipped with Windows 7 (W7), Windows XP (XP), and Ubuntu 12.04 (LX). Additionally, we installed a server VM for data storage. To collect and accurately label the flows, we adapted the Volunteer-Based System (VBS) developed at Aalborg University [13]. VBS collects information about Internet traffic flows, including start time, number of packets, IP addresses, ports, transport layer protocol, and detailed packet information (direction, size, TCP flags, and relative timestamp). For each flow, VBS also collects the associated process name from the system sockets, ensuring the application associated with the traffic. The captured information is transmitted to the VBS server and stored in a MySQL database. The design of VBS was initially described in [13]. On each data-generating VM, we installed a modified version of VBS, whose source code was published under a GPL license [14]. The modified VBS client captures full Ethernet frames, extracts HTTP URL and Referer fields, and includes modules for dumping packets to PCAP files and analyzing logs generated by DPI tools.

### 2.2 Selection of the Data

Building a representative dataset that characterizes typical user behavior is a challenging task crucial for testing and comparing different traffic classifiers. To ensure proper diversity and amount of included data, we combined data on a multidimensional level. Based on w3schools statistics, we selected Windows 7 (55.3% of all users), Windows XP (19.9%), and Linux (4.8%) - state for January 2013. Apple computers (9.3% of overall traffic) and mobile devices (2.2%) were left for future work. The selected applications include:

- **Web browsers:** Chrome and Firefox (W7, XP, LX), Internet Explorer (W7, XP)
- **BitTorrent clients:** uTorrent and BitTorrent (W7, XP), Frostwire and Vuze (W7, XP, LX)
- **eDonkey clients:** eMule (W7, XP), aMule (LX)
- **FTP clients:** FileZilla (W7, XP, LX), SmartFTP Client (W7, XP), CuteFTP (W7, XP), WinSCP (W7, XP)
- **Remote Desktop servers:** built-in (W7, XP), xrdp (LX)
- **SSH servers:** sshd (LX)
- **Background traffic:** DNS and NTP (W7, XP, LX), NETBIOS (W7, XP)

We chose websites based on the top 500 sites according to Alexa statistics, including Google, Facebook, YouTube, Yahoo!, Wikipedia, Java, and Justin.tv. For each website, we performed several random clicks to linked external sites to better characterize real user behavior. For search engines, we manually generated random clicks to destination websites. Each chosen website was processed by each browser, and fake accounts were created for login-required sites. We simulated different human behaviors, such as interacting with friends on Facebook, uploading pictures, creating events, playing games, and watching popular videos on YouTube. Detailed actions are listed in our technical report [15]. We tested P2P clients by downloading files of different sizes and seeding them. FTP clients were tested using both active and passive transfer modes, if supported.

### 2.3 Extracting the Data for Processing

Each DPI tool has different requirements and features, so the extracting tool must handle these issues. PCAP files provided to PACE, OpenDPI, L7-filter, NDPI, and Libprotoident are accompanied by INFO files containing information about the start and end of each flow, along with the flow identifier. This allows the software to create and terminate flows appropriately and provide classification results with the flow identifier. Preparing data for NBAR classification is more complex, as there are no separate INFO files. We needed to extract packets in a way that allows the router to process and correctly group them into flows. We achieved this by changing the source and destination MAC addresses during extraction. The destination MAC address of every packet must match the router's interface MAC address, and the source MAC address contains the flow identifier. This is the first known scientific performance evaluation of NBAR.

### 2.4 The Classification Process

We designed a tool called `dpi_benchmark` to read PCAP files and provide packets one-by-one to PACE, OpenDPI, L7-filter, NDPI, and Libprotoident. Flows are started and terminated based on INFO files. After the last packet of a flow is sent to the classifier, the tool obtains the classification label and writes it to log files with the flow identifier. A brief description of the DPI tools is presented in Table 1. Although some tools have multiple configuration parameters, we used the default configuration for most. Detailed descriptions and configurations can be found in [15].

For NBAR classification, we set up a full working environment using GNS3, which emulates Cisco hardware. We emulated the 7200 platform, as it supports the newest version of Cisco IOS (version 15) with Flexible NetFlow. The router was configured to use Flexible NetFlow with NBAR on the created interface. We used `tcpreplay` to replay PCAP files to the router and `nfacctd` to capture Flexible NetFlow records. The records, containing the flow identifier and the application name recognized by NBAR, were saved into text log files. This process is detailed in our technical report [15].

### 2.5 The Dataset

Our dataset contains 1,262,022 flows captured over 66 days between February 25, 2013, and May 1, 2013, accounting for 35.69 GB of pure packet data. The application name tag was present for 520,993 flows (41.28% of all flows), which account for 32.33 GB (90.59%) of the data volume.

## 3. Results and Discussion

[Insert results and discussion here]

## 4. Conclusion

[Insert conclusion here]

## Acknowledgments

This research was funded by the Spanish Ministry of Economy and Competitiveness under contract TEC2011-27474 (NOMADS project), by the Comissionat per a Universitats i Recerca del DIUE de la Generalitat de Catalunya (ref. 2009SGR-1140), and by the European Regional Development Fund (ERDF).

## References

[1] Reference 1
[2] Reference 2
[3] Reference 3
[4] Reference 4
[5] Reference 5
[6] Gringoli, F., et al. "A dataset for the evaluation of deep packet inspection techniques." *Proceedings of the 10th ACM SIGCOMM conference on Internet measurement*, 2010.
[7] Reference 7
[8] Reference 8
[9] Reference 9
[10] Reference 10
[11] Reference 11
[12] Reference 12
[13] Reference 13
[14] Reference 14
[15] Technical Report

---

**Note:** The sections for "Results and Discussion" and "Conclusion" are placeholders and should be filled with the appropriate content.