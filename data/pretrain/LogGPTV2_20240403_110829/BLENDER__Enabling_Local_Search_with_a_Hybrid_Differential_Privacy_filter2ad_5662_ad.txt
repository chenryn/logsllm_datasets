.
1
variance
Theorem 5. When mC
estimate
biased
ClientProbabilities
can
ˆσ2
C,(cid:104)q,u(cid:105) =
t−ttq
kq−1
t2(cid:0)tq− 1−tq
(cid:1)2 ˆσ2
1
kq−1
C,q + 2|DC|
|DC|−1
(cid:1)2 ·(cid:16) ˆrC,(cid:104)q,u(cid:105)(1−ˆrC,(cid:104)q,u(cid:105))
+(cid:0)
(cid:1)ˆrC,(cid:104)q,u(cid:105)
(cid:1)(cid:0) k−2+t
(cid:0)
un-
Estimate-
as:
−
be
|DC|−1
− t−ttq
kq−1
computed
(k−1)kq
(k−1)kq
=
for
kt−1
(cid:17)
the
1−t
1−t
.
If ˆσ2
O,(cid:104)q,u(cid:105) and ˆσ2
Theorem 6 (Sample Variance Optimal Weight-
ing).
C,(cid:104)q,u(cid:105) are sample vari-
ances of ˆpO,(cid:104)q,u(cid:105) and ˆpC,(cid:104)q,u(cid:105) respectively,
then
w(cid:104)q,u(cid:105) =
timal weighting.
is the sample variance op-
C,(cid:104)q,u(cid:105)
O,(cid:104)q,u(cid:105)+ˆσ2
ˆσ2
C,(cid:104)q,u(cid:105)
ˆσ2
4 Experimental Evaluation
L1 =(cid:80)
as rel i = ni(cid:80)
We designed Blender with an eye toward preserv-
ing the utility of the eventual results in the two appli-
cations we explore in this paper: trend computation
and local search, as described in Section 1.2. We
use two established domain-speciﬁc utility metrics
to assess the utility, the L1 metric and NDCG.
L1: L1 is the Manhattan distance between the esti-
mate and actual probability vectors, in other words,
i |ˆpi − pi|. The smaller the L1, the better.
NDCG: NDCG is a standard measure of search
quality [20, 38] that explicitly takes the ordering of
the items in a results list into account. This mea-
sure uses a relevance score for each item: given a
list of items and their true frequencies, we deﬁne
the relevance or gain of the ith most frequent item
, where nj is the number of oc-
currences of the jth most frequent item. The dis-
counted cumulative gain for the top k items in an
estimated list (that is, a list that estimates the top
k items and their frequencies) is typically computed
2reli−1
log2(i+1) . Here, the log2(i + 1)
factor diminishes the contribution of items later in
the list, hence the notion of discounting. In particu-
lar, getting the ordering correct for higher-relevance
items early in the list yields a higher DCGk value.
The magnitude of the DCGk value doesn’t mean
much on its own. For better interpretability, it is
usually normalized by the Ideal DCG (IDCGk),
which is the DCGk value if the estimated list
had the exact same ordering as the actual
list.
Thus, the normalized discounted cumulative gain
(N DCGk), which ranges between 0 and 1, is deﬁned
as N DCGk = DCGk/IDCGk.
as DCGk = (cid:80)k
j nj
i=1
While NDCG is traditionally deﬁned for lists,
Blender outputs a list-of-lists: there is a URL list
corresponding to each query, and the queries them-
selves form a list. Thus, we introduce a general-
ization of the traditional NDCG measure. Speciﬁ-
cally, for each query q, we ﬁrst compute the NDCG
as described above of q’s URL list, N DCGq
k. We
then deﬁne the DCG of the query list as DCGQ
k =
k. This is analogous to the
typical DCG computation, except that each query’s
contribution is being further discounted by how well
log2(i+1) · N DCGi
2reli−1
(cid:80)k
i=1
USENIX Association
26th USENIX Security Symposium    755
Data set on disk
Unique queries
Unique clients
Unique URLs
AOL
1.75 GB
4,811,646
519,371
1,620,064
Yandex
16 GB
13,171,961
4,970,073
12,702,350
Figure 8: Data set statistics.
its URL list was estimated. The DCG value for the
query list as a whole is then normalized by the anal-
ogous Ideal DCG (IDCGQ
k if the esti-
mated query list had the exact same ordering as the
actual query list.
k ) – the DCGQ
Compared to the traditional NDCG deﬁnition,
the additional discounting within DCGQ
k makes it
even harder to attain high NDCG values than in
the query-only case. Contrasted with the L1 mea-
sure, this formulation takes both the ranking and
probabilities from the data set into account. Since
changes to the probabilities may not result in rank-
ing changes, L1 is an even less forgiving measure
than NDCG.
Since the purpose of Blender is to estimate prob-
abilities of the top records, we discard the artiﬁcially
added (cid:63) queries and URLs and rescale reli prior to
L1 and NDCG computations. However, since we
use the method of [39] in BlendProbabilities, the
probability estimates involving (cid:63) have a minor im-
plicit eﬀect on the L1 and NDCG scores.
4.1 Experimental Setup
Data sets: For our experiments, we use the
AOL search logs, ﬁrst released in 2006 and an or-
der of magnitude bigger Yandex search data set4,
from 2013. Figure 8 compares their characteristics.
Data analysis: To familiarize the reader with the
approach we used for assessing result quality, Fig-
ure 9 shows the top-10 most frequent queries in the
AOL data set, with the estimates given by the dif-
ferent “ingredients” of Blender.
The table is sorted by column 2, which contains
the non-private, empirical probabilities pq for each
query q from the AOL data set with 1 random record
sampled from each user. We consider this as the
baseline for the true, underlying probability of that
query. Column 3 contains the ﬁnal query probability
estimates outputted by Blender, ˆpq, after combin-
ing the estimates from the opt-in group and clients.
The remaining columns show the estimates that are
produced by the sub-components of Blender that
are eventually combined to form the estimates in
column 3. As the opt-in and client sub-components
compute probability estimates over the records in
the head list, we obtain query probability estimates
by aggregating the probabilities associated with each
Query
prob.
pq
0.9108
(cid:63)
0.0213
google
0.0067
yahoo
0.0067
google.com
0.0057
myspace.com
0.0054
mapquest
yahoo.com
0.0043
www.google.com 0.0034
0.0033
myspace
ebay
0.0028
ˆpq
0.9103
0.0216
0.0070
0.0056
0.0052
0.0051
0.0043
0.0004
0.0034
0.0026
u ˆpO,(cid:104)q,u(cid:105)
ˆpC,q
0.9199
0.0213
0.0046
0.0023
0.0022
0.0062
0.0021
0.0004
0.0042
0.0028
0.9100
0.0217
0.0073
0.0061
0.0057
0.0053
0.0048
0.0032
0.0035
0.0028
AOL data Blender Opt-in
Client
estimate estimate estimate estimate
(cid:80)
Client
(cid:80)
u ˆpC,(cid:104)q,u(cid:105)
0.1468
0.0216
0.0325
0.0194
0.0258
0.0192
0.0192
0.0098
0.0255
0.0254
Figure 9: Top-10 most popular queries in the AOL dataset, their
empirical probabilities pq in the ﬁrst numeric column, Blender’s
probability estimates ˆpq in the next column, and the various
sub-components’ estimates in the remaining columns. Parame-
ter choices are shown in Figure 10.
URL for a given query (columns 4 and 6). The sam-
ple variance of these aggregated probabilities, used
for blending, is naively computed as in Theorem 4.
In addition to estimating the record probabilities,
the client algorithm estimates query probabilities di-
rectly, which are shown in column 5. Regressions,
i.e., estimates that appear out of order relative to
column 2, are shown in red.
Takeaways: The biggest takeaway is that the num-
bers in columns 2 and 3 are similar to each other,
with only one regression after Blender’s usage.
Blender compensates for the weaknesses of both
the opt-in and the client estimates. Despite the sub-
components having several regressions, their combi-
nation has only one.
The table also provides intuition for the usefulness
of a two-stage reporting process in the client algo-
rithm (ﬁrst report a query and then the URL), thus
allowing for separate estimates of query and record
probabilities. Speciﬁcally, despite the high number
of regressions for the client algorithm’s aggregated
record probability estimates (column 6), its query
probability estimates (column 5) have only one.
4.2 Experimental Results
We formulate questions for our evaluation as fol-
lows: how to choose Blender’s parameters (Sec-
tion 4.2.1), how does Blender perform compared
to alternatives (Section 4.2.2), and how robust are
our ﬁndings (Section 4.2.3)?
4.2.1 Algorithmic and Parameter Choices
Blender has a handful of parameters, some of
which can be viewed as given externally (by the laws
of nature, so to speak), and others whose choice is
purely up to the entity that’s utilizing Blender.
We now describe and, whenever possible, motivate,
our choices for these.
Privacy parameters,  and δ: Academic liter-
ature on diﬀerential privacy views the selection of
the  parameter as a “social question” [9] and thus
756    26th USENIX Security Symposium
USENIX Association
uses  in the range of 0.01 to 10 for evaluating al-
gorithm performance (see Table 1 in [18]). The two
known industry deployments of diﬀerential privacy
(by Google [13] and Apple [17]) do not explicitly re-
veal the parameters used. [25, 37] found via reverse-
engineering of Apple’s diﬀerential privacy implemen-
tation that Apple uses  = 1 or  = 2 per item sub-
mitted, but allows submission of several dozen items
per day from one device. A typical user might ex-
perience an  of 4 – 6 per day, but  = 20 per day
has also been observed [37]. The work most simi-
lar to ours, [34], performs evaluations using  in the
range [1, 10]. We use  = 4, unless otherwise stated.
Similarly, a range of δs has been used for evaluations
(e.g., 10−6, 10−5, 10−4 in [26] and 0.05 in [6]). We
use δ = 10−5 for AOL and δ = 10−7 for Yandex
data sets, with the smaller δ choice for the latter
reﬂecting the larger number of users in the data set.
We use the same  and δ values for the opt-in
and client users. From a behavioral perspective, this
reduces a user’s opt-in decision down to one purely
of trust towards the curator.
Opt-in and client group sizes, |O| and |C|: The
relative sizes of opt-in group and client group, |O|
and |C|, respectively, can be viewed as exogenous
variables which are dictated by the trust that users
place in the search engine. We choose 5% and 2.5%
for the fraction of opt-in users as compared to total
users as these seem reasonable for representing the
fraction of “early adopters” who are willing to supply
their data for the improvement of products and allow
us to demonstrate the utility beneﬁts of algorithms
designed to operate in the hybrid privacy model.
The number of records to collect from each
opt-in user, mO = 1: This is mandated by the pri-