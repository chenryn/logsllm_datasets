### 优化方案2    
1\. 删除大字段上的索引.    
删除后的性能如上面的场景3.    
2\. 或者不使用全索引, 例如只索引前面32个字符. substr(col3,1,32).    
```    
digoal=> create index idx_test_3 on test (substr(col3,1,32));    
digoal=> create index idx_col3 on t3 (substr(col3,1,32));    
```    
测试结果如下 :     
```    
ocz@db-172-16-3-150-> pgbench -M prepared -f ./t.sql -n -r -c 8 -j 4 -T 60 -h 127.0.0.1 -U digoal digoal    
transaction type: Custom query    
scaling factor: 1    
query mode: prepared    
number of clients: 8    
number of threads: 4    
duration: 60 s    
number of transactions actually processed: 191874    
tps = 3197.762123 (including connections establishing)    
tps = 3198.203949 (excluding connections establishing)    
statement latencies in milliseconds:    
        0.003331        \setrandom id 1 100000    
        2.494849        update test set col1=repeat(md5(clock_timestamp()::text), 10), col3=repeat(md5(clock_timestamp()::text), 3000) where id=:id;    
ocz@db-172-16-3-150-> pgbench -M prepared -f ./t.sql -n -r -c 8 -j 4 -T 60 -h 127.0.0.1 -U digoal digoal    
transaction type: Custom query    
scaling factor: 1    
query mode: prepared    
number of clients: 8    
number of threads: 4    
duration: 60 s    
number of transactions actually processed: 150292    
tps = 2504.685327 (including connections establishing)    
tps = 2505.064421 (excluding connections establishing)    
statement latencies in milliseconds:    
        0.003861        \setrandom id 1 100000    
        3.185736        update v_t set col1=repeat(md5(clock_timestamp()::text), 10), col3=repeat(md5(clock_timestamp()::text), 3000) where id=:id;    
ocz@db-172-16-3-150-> pgbench -M prepared -f ./t.sql -n -r -c 8 -j 4 -T 60 -h 127.0.0.1 -U digoal digoal    
transaction type: Custom query    
scaling factor: 1    
query mode: prepared    
number of clients: 8    
number of threads: 4    
duration: 60 s    
number of transactions actually processed: 174549    
tps = 2908.974298 (including connections establishing)    
tps = 2909.398826 (excluding connections establishing)    
statement latencies in milliseconds:    
        0.003254        \setrandom id 1 100000    
        0.060880        begin    
        0.199939        update t1 set col1=repeat(md5(clock_timestamp()::text), 10) where id=:id;    
        2.391802        update t3 set col3=repeat(md5(clock_timestamp()::text), 3000) where id=:id;    
        0.086923        end;    
```    
对于更新量较少的大字段, 这样优化还是可行的.    
### 优化方案3    
col3使用大对象存储.    
```    
digoal=> alter table test drop column col3;    
ALTER TABLE    
digoal=> alter table test add column col3 oid;    
ALTER TABLE    
digoal=> \d test    
               Table "digoal.test"    
 Column |            Type             | Modifiers     
--------+-----------------------------+-----------    
 id     | bigint                      | not null    
 col1   | name                        |     
 col2   | text                        |     
 col4   | timestamp without time zone |     
 col3   | oid                         |     
Indexes:    
    "test_pkey" PRIMARY KEY, btree (id)    
    "idx_test_col1" btree (col1)    
    "idx_test_col2" btree (col2)    
    "idx_test_col4" btree (col4)    
digoal=> truncate test ;    
TRUNCATE TABLE    
```    
创建写大对象的函数, 打开文件FLAG参考 src/include/libpq/libpq-fs.h :     
```    
create or replace function write_lo (i_bytea bytea) returns oid as $$    
declare    
  oid_new_lo oid;    
  fd_new_lo int;    
begin    
  select lo_creat(-1) into oid_new_lo;    
  select lo_open(oid_new_lo, 131072) into fd_new_lo;    
  perform lowrite(fd_new_lo, i_bytea);    
  perform lo_close(fd_new_lo);    
  return oid_new_lo;    
end;    
$$ language plpgsql;    
```    
创建读大对象的函数, 打开文件FLAG参考 src/include/libpq/libpq-fs.h :     
```    
create or replace function read_lo (i_lo_oid oid, i_size int4) returns bytea as $$    
declare    
  result bytea;    
  fd_lo int;    
begin    
  select lo_open(i_lo_oid, 262144) into fd_lo;    
  select loread(fd_lo, i_size) into result;    
  perform lo_close(fd_lo);    
  return result;    
end;    
$$ language plpgsql;    
```    
插入数据 :     
```    
digoal=> insert into test (id,col1,col2,col3,col4) select     
digoal->   generate_series(1,100000),     
digoal->   repeat(md5(clock_timestamp()::text), 10),     
digoal->   repeat(md5(clock_timestamp()::text), 10),     
digoal->   write_lo(decode(repeat(md5(clock_timestamp()::text), 3000), 'base64')),     
digoal->   clock_timestamp();    
INSERT 0 100000    
Time: 273653.042 ms    
```    
text和bytea的转换可以使用decode和encode函数.    
用法可参见 :     
http://www.postgresql.org/docs/9.2/static/functions-binarystring.html    
http://www.postgresql.org/docs/9.2/static/functions-string.html    
src/backend/utils/adt/encode.c    
pgbench测试, 更新非大字段, 也就是前面场景1的测试 :     
```    
ocz@db-172-16-3-150-> pgbench -M prepared -f ./t.sql -n -r -c 8 -j 4 -T 60 -h 127.0.0.1 -U digoal digoal    
transaction type: Custom query    
scaling factor: 1    
query mode: prepared    
number of clients: 8    
number of threads: 4    
duration: 60 s    
number of transactions actually processed: 1337021    
tps = 22283.174163 (including connections establishing)    
tps = 22286.160069 (excluding connections establishing)    
statement latencies in milliseconds:    
        0.002805        \setrandom id 1 100000    
        0.353505        update test set col1=repeat(md5(clock_timestamp()::text), 10), col2=repeat(md5(clock_timestamp()::text), 10) where id=:id;    
```    
性能从754tps提升到22286 tps    
大对象存储在pg_largeobject中, 因此大对象的条数限制是oid的上限(无符号32位整型), 2^32-1. 一个数据库中没有办法存储超出2^32-1条大对象.    
1个大对象被分成多个data段存储, 如下.    
```    
digoal=# \d pg_largeobject    
Table "pg_catalog.pg_largeobject"    
 Column |  Type   | Modifiers     
--------+---------+-----------    
 loid   | oid     | not null    
 pageno | integer | not null    
 data   | bytea   |     
Indexes:    
    "pg_largeobject_loid_pn_index" UNIQUE, btree (loid, pageno)    
```    
不需要的大对象的清除方法 :     
```    
select lo_unlink(col3) from test;    
```    
切记, 删除数据前要清除大对象, 否则这些数据会一直存在pg_largeobject中, 类似内存泄露.    
## 小结    
1\. 首先, 不管大字段要不要更新, 首先把大字段拆出去. 这样更新性能可以从754tps 上升到 6576tps(更新视图) 或 12229tps(直接更新拆分后的底层表).    
其次, 大字段上尽量不要使用索引, 无索引的情况下更新大字段的性能可以从593tps 上升到 4499tps.    
再次, 如果大字段上必须要索引, 可以根据实际情况减少索引的长度, 例如只索引前32个字符(函数索引 | 表达式索引). 这样更新大字段的性能可以从593tps 上升到 3198tps.    
最后, 可以考虑将大字段类型text改为oid, 也就是存储在大对象中(text最大存储1GB, 大对象可以存储2GB). 但是需要注意一个数据库的大对象条数是有限制的, 也就是OID的上限. 如果要突破这个限制还是考虑拆表使用text或者bytea类型吧. 使用大对象的情况下更新非大字段的性能可以从754tps 提升到22286 tps .    
2\. 拆表后的优势: 降低索引更新概率, 提高处理速度. 拆表后查询性能会变差. 但是如果不查大字段, 性能会更好, 因为扫描的HEAP块更少了.    
3\. 拆表后需要更多的存储来存储PK. 拆得越细, 耗费越大. 例如本例int8占用8字节, 拆表后比原来多了3个int8, 1000W条记录将多耗费24*10000KB=240MB. 但是换来了极好的更新性能.    
4\. 拆表的原则    
4\.1 根据索引拆, 如果包含col1,col2的联合索引, 那么拆成id,col1,col2    
4\.2 根据大小拆, 如果col3这个字段一般都存储1KB大小的内容, 较大. 所以可以拆成id, col3    
4\.3 以上都拆掉后根据经常更新的列拆, 如col4经常被更新, 那么拆成id, col4    
经过以上拆分后test就拆成3个表了. 然后用id把这几个表关联起来.    
5\. 拆表后如果需要truncate表, 要在底层表上执行truncate, 因为view上不能建truncate触发器. 同时需要注意这些truncate要放在一个事务中处理.    
6\. 为什么拆表后更新性能可以得到提升呢?    
6\.1. 因为PostgreSQL 更新记录的操作实际上是新增1个tuple版本, 例如更新1KB的行, 会新产生1个1KB的行. 老的tuple在事务结束后由autovacuum进程去擦除. 或者手工执行vacuum.    
所以行越大, 更新的开销越大. 拆表后更新的粒度变小, 性能自然可以提高.    
6\.2 未拆表时, 一行更新的话, 所有的索引都会被更新, 除非HOT, 但是大行的表HOT的概率非常低.     
因此每次更新所有的索引都要被更新. 大大加大了IO和CPU的开销.    
(HOT请参见 : src/backend/access/heap/README.HOT)    
拆表后, 更新索引的概率会大大降低了.     
7\. 本例真正的性能瓶颈是大字段的索引耗费的CPU, 去除索引后性能将突飞猛进.     
8\. 更新的另一个主要瓶颈还可能来自不规则的更新导致的索引页迁移等IO和CPU开销。    
可参见 :     
http://blog.163.com/digoal@126/blog/static/16387704020129249646421/    
9\. http://www.postgresql.org/docs/9.2/static/largeobjects.html    
打开文件可选flag    
src/include/libpq/libpq-fs.h    
```    
/*    
 *      Read/write mode flags for inversion (large object) calls    
 */    
#define INV_WRITE               0x00020000    
#define INV_READ                0x00040000    
```    
## 其他    
如果tuple大到需要存储到TOAST表了, 那性能又会发生翻天覆地的变化, 原因和前面提到的large object类似. 在HEAP中存储的是一个指向TOAST存储的虚拟值(类似指针).     
TOAST具体可参考 src/include/access/tuptoaster.h .     
下面是一个测试 :     
```    
digoal=> alter table test drop column col3;    
ALTER TABLE    
digoal=> truncate table test;    
alTRUNCATE TABLE    
digoal=> alter table test add column col3 text;    
ALTER TABLE    
digoal=> \d test    
               Table "digoal.test"    
 Column |            Type             | Modifiers     
--------+-----------------------------+-----------    
 id     | bigint                      | not null    
 col1   | name                        |     
 col2   | text                        |     
 col4   | timestamp without time zone |     
 col3   | text                        |     
Indexes:    
    "test_pkey" PRIMARY KEY, btree (id)    
    "idx_test_col1" btree (col1)    
    "idx_test_col2" btree (col2)    
    "idx_test_col4" btree (col4)    
digoal=> create index idx_test_col3 on test(substr(col3,1,8192));    
CREATE INDEX    
digoal=> insert into test (id,col1,col2,col3,col4) select     
digoal->    generate_series(1,10),     
digoal->    repeat(md5(clock_timestamp()::text), 10),     
digoal->    repeat(md5(clock_timestamp()::text), 10),     
digoal->    repeat(md5(clock_timestamp()::text), 30000),     
digoal->    clock_timestamp();    
INSERT 0 10    