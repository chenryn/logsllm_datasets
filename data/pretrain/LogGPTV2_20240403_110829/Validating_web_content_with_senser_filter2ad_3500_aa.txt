title:Validating web content with senser
author:Jordan Wilberding and
Andrew Yates and
Micah Sherr and
Wenchao Zhou
Validating Web Content with Senser
Jordan Wilberding
Georgetown University
Andrew Yates
Georgetown University
Wenchao Zhou
Georgetown University
Micah Sherr
Georgetown University
ABSTRACT
This paper introduces Senser, a system for validating retrieved
web content. Senser does not rely on a PKI and operates even
when SSL/TLS is not supported by the web server. Senser oper-
ates as a network of proxies located at different vantage points on
the Internet. Clients query a random subset of Senser proxies for
compact descriptions of a desired web page, and apply consensus
and matching algorithms to the returned results to locally render a
“majority” web page. To ensure diverse selections of proxies (and
consequently decrease an adversary’s ability to manipulate a ma-
jority of the proxies’ requests), Senser leverages Internet mapping
systems that accurately predict AS-level paths between available
proxies and the desired web page. We demonstrate using a de-
ployment of Senser on Amazon EC2 that Senser detects and mit-
igates attempts by adversaries to manipulate web content — even
when controlling large collections of autonomous systems — while
maintaining reasonable performance overheads.
1.
INTRODUCTION
SSL/TLS is the predominant protocol used to protect web con-
tent. When used correctly, it provides strong conﬁdentiality and
authenticity guarantees. Unfortunately, while SSL/TLS is critically
important in securing many web transactions, it is often unavail-
able and too often is susceptible to implementation weaknesses.
To illustrate, Sunshine et al. [24] demonstrate that users often do
not heed browsers’ certiﬁcate warning messages. Additionally, the
2011 DigiNotar incident [26] and academic studies of SSL/TLS de-
ployment [12] bring into focus longheld concerns about the lack of
safeguards in the web’s public key infrastructure or “PKI” (cf. [8]).
Finally, SSL/TLS is not universally available: we ﬁnd that less than
30% of both Alexa’s top 100,000 and top 1,000 websites correctly
support SSL/TLS (see Appendix A).
SSL/TLS has been the subject of more than two decades of re-
search and there are numerous proposals for increasing its adop-
tion, mitigating weaknesses of the web’s PKI, improving the usabil-
ity of browsers’ certiﬁcate warning messages and increasing public
awareness of the importance of certiﬁcate veriﬁcation. However,
until comprehensive practical solutions are developed and widely
deployed, it is useful to consider mechanisms for detecting and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee. Request permissions from Permissions@acm.org.
ACSAC ’13 Dec. 9-13, 2013, New Orleans, Louisiana USA
Copyright 2013 ACM 978-1-4503-2015-3/13/12 ...$15.00.
http://dx.doi.org/10.1145/2523649.2523669
339
mitigating the malicious modiﬁcation of web content in transit that
does not require Internet-scale (re)deployments or reconﬁgurations.
In this paper, we focus on ensuring the authenticity of web con-
tent. We introduce a system, Senser, that validates retrieved web
content even when SSL/TLS is not supported by the web server.
Senser consists of a set of volunteer-operated proxies located at
different vantage points on the Internet. Clients query a random
subset of these proxies for a desired URL (e.g., a page, stylesheet,
image, embedded object, etc.). The proxies fetch the URL and
return concise descriptors to clients who may then render a “major-
ity” version of the web resource. Unless an adversary can affect a
majority of the chosen proxies’ fetches, it cannot undetectably alter
the web content. Hence, Senser is appropriate in settings where an
adversary may censor/alter non-SSL content in transit.
The architecture of our system is similar to Perspectives [30],
which also uses a network of proxies in favor of the web’s existing
PKI. Perspectives validates a site’s certiﬁcate by comparing copies
of the certiﬁcate retrieved from different proxies. The underlying
assumption is that an adversary may be able to position himself be-
tween the user and the website, but is unlikely to be able to position
himself between the website and all of the proxies.
This paper extends this model in two important ways. First, we
show that a network of proxies can be used to validate the content
of web pages. Here, a major challenge is that unlike SSL/TLS ﬁn-
gerprints which are usually consistent regardless of the requestor’s
location, the content of a web page may change depending upon
who is accessing it. As we discuss below, websites often serve lo-
calized and/or personalized content, making it non-trivial to form a
consensus of a retrieved webpage even in the absence of an adver-
sary. However, by verifying content rather than certiﬁcate ﬁnger-
prints, Senser does not rely on SSL/TLS or the web’s PKI and is
thus compatible with sites that do not support HTTPS.
Second, we observe that an adversary may be able to inﬂuence
multiple proxies’ views of a web resource if it is advantageously
positioned. For example, a malicious or compromised autonomous
system (AS) may be on the network path between several of the
proxies and the requested web server and can therefore manipulate
those proxies’ fetches. To mitigate such attacks, we develop client-
side proxy selection algorithms that maximize the AS diversity of
the paths between the proxies and the web server. Our algorithms
take as input compact topological maps of the Internet (sometimes
called Internet atlases [18]) and the requested URL, and ensure
network diversity both for DNS lookups and proxy fetches.
Threat Model. We focus on the problem of discovering (and po-
tentially recovering from) surreptitious man-in-the-middle (MitM)
attacks against non-HTTPS protected web content.
A particularly interesting class of attackers, and one that we em-
phasize in this work, is that of a censor who wishes to block or
modify intercepted web trafﬁc. We conservatively model censors
as AS-level adversaries. We distinguish between blocking (prevent-
ing the user from accessing the requested website), whole-page al-
teration (replacing the true webpage with one chosen by the ad-
versary), and partial alteration (selectively modifying sections of a
webpage).
We remark that we do not attempt to prevent a censor from block-
ing access to the Senser network; an anti-blocking system (e.g.,
Tor bridges [27] or Telex [31]) could be used to access the Senser
network when faced with such actions. In such a situation Senser
still provides an advantage over the anti-blocking system, because
the censor has an incentive to participate in the anti-blocking sys-
tem (e.g., run a Tor exit proxy) to censor content that is accessed
through it. Senser mitigates a censor’s ability to censor content
by participating in the system by comparing content retrieved from
multiple Senser proxies that are chosen using an AS-aware proxy
selection algorithm. Here, our aim is to detect whether (and if so,
how) particular web pages have been modiﬁed by a censor.
Challenges.
challenges which we address in this paper:
The Senser architecture presents several technical
• Consensus construction: Even in the absence of an ad-
versary, websites may offer client-speciﬁc content. For ex-
ample, many websites routinely serve content based on the
client’s perceived geographic location. In Senser, we relax
the requirement that the consensus represent any particular
response sent by the web server, and instead attempt to cre-
ate a “majority version” of that page that contains its core
content. In more detail, we represent the web page versions
retrieved by the proxies as HTML trees, and perform an ef-
ﬁcient tree-matching procedure to ﬁnd a large common sub-
tree.
• Bandwidth costs: The average webpage is estimated to con-
sume at least 320KB of bandwidth [20]. Fetching multiple
copies of webpages in their entirety from a set of proxies
is thus likely too prohibitive for many clients. We reduce
this cost by (i) fetching only concise summaries of webpage
content and (ii) fetching website content only once using an
established consensus.
• Resistance to AS-level adversaries: We envision AS-level
adversaries who control large segments of the network and
may attempt to manipulate web content. To limit the ability
of AS-level adversaries, we introduce an ofﬂine AS-aware
proxy selection algorithm that allows clients to intelligently
select proxies such that the paths from the proxies to the des-
tination website are suitably AS-disjoint.
• Idempotency: Our architecture requires that a webpage be
retrieved multiple times. Senser is therefore ill-suited for
web requests that are not idempotent.
The Senser architecture meets the ﬁrst three challenges, but is in-
compatible with sites that require HTTP POSTs or use non-effect-
free HTTP GETs. We argue, however, that our initial design and
implementation are appropriate for a large class of websites: those
that serve news stories or other content and do not require readers
to authenticate to the site, which are likely targets for content mod-
iﬁcation due to censorship. Later, we discuss adaptation of Senser
to support non-idempotent operations in Section 4.
We evaluate the performance, security, and utility of Senser un-
der both simulation and a testbed deployment. Our results show
that Senser is able to render the majority of pages in a usable way
while incurring a modest latency overhead for the majority of web-
sites. Our AS-aware proxy selection algorithm is able to reduce
340
the system’s failure rate (the proportion of pages that can be un-
detectably altered by the adversary) by up to 15% by increasing
network diversity.
Our implementation of Senser is released as open-source soft-
ware and is available at https://security.cs.georgetown.edu/senser/.
2. RELATED WORK
Multiple vantage points. Most similar to Senser are approaches
for verifying digital certiﬁcates using a set of external veriﬁers.
Wendlandt et al. [30] address the problem of “trust on ﬁrst use”
(TOFU) authentication by verifying that a server’s public key re-
mains the same when observed from servers at different locations.
Their approach aims to improve usability weaknesses in certiﬁcate
veriﬁcation [24] by relying on trusted authenticator nodes rather
than on a public key infrastructure. Similarly, Senser does not
utilize PKIs and uses different perspectives to validate web pages.
However, unlike the approach by Wendlandt et al., Senser veriﬁes
a webpage’s content rather than its certiﬁcates. It is also applicable
for websites that do not use SSL/TLS.
The Snakes on a Tor Exit Scanner (SoaT) [23] scans Tor [7] exit
relays to detect misbehavior. SoaT operates by comparing hashes
of web content retrieved from the Tor network with that retrieved
from direct (non-anonymized) communication. Noting that false
positives may be introduced due to personalized web content, SoaT
also retrieves the web content from a second network location to
identify the personalized content and reduce its false positive rate,
at the cost of increased false negative rates. Senser provides a more
comprehensive solution that supports ﬁne-granularity content ren-
dering when adversaries selectively modify portions of web pages.
The recently released Filter Bubble [32] extension for Chrome
redirects Google search queries to a set of distributed nodes, in-
forming the user of how search results differ based on geographic
region. Like Senser, Filter Bubble detects personalized content,
but is applicable only to Google. Similarly, Netalyzr [17] uses a set
of distributed nodes to determine whether an ISP is actively block-
ing, limiting, or giving preferential treatment to certain services.
CensMon [22] aims to provide a similar outcome to Senser by
routing queries through several geographically dispersed agents and
analyzing the results of the DNS lookup and returned HTML to
see how different nodes are receiving different information. Their
HTML difference detection is based on MD5 and provides no re-
construction. Further, they do not take network paths into account.
Tree alignment and Merkle Hash Trees. Merkle Hash Trees
(MHTs) have been applied to authenticate queries performed by
untrusted third parties [5, 6, 10]. To do so, an owner computes a
hash tree of the data to be queried, distributes the hash tree to all
potential clients, and distributes the data to third parties. The hash
tree can then be used by clients to verify query results returned
by third parties. Bayardo and Sorensen [1] use a similar method
to authenticate the correctness of HTTP 200 and 404 responses.
Senser also uses MHTs to concisely describe web pages, but does
not rely on a single tree to perform veriﬁcation. Instead, we apply
a tree alignment algorithm to construct a consensus tree and render
a “majority” webpage.
While we use a simple and efﬁcient tree alignment algorithm
that employs breadth ﬁrst search, others have explored tree align-
ment algorithms that yield results closer to the optimal outcome
at the expense of performance. These approaches generalize the
string alignment problem to trees and solve it using dynamic pro-
gramming. As with the string alignment problem, the algorithms
consider the cost of operations such as node insertion, deletion,
and replacement. Carrillo and Lipman [3] present an optimal mul-
tiple tree alignment that runs in exponential time. The pairwise tree
alignment problem can be solved in O(|T1|×|T2|× h1× h2) time,
where |Ti| is the size of tree i and hi is the height of tree i [25].
Wang et al. [28] improve upon this algorithm to solve the problem
in O(|T1| × |T2| × min(h1, l1) × min(h2, l2)) time, where li is
the number of leaves in tree i. Followup work [4] applies the center
star approximation algorithm [11] for multiple string alignment in
order to approximately align multiple HTML trees.
Censorship resistance.
Information slicing [16] divides scram-
bled messages into pieces, and sends the pieces along disjoint paths
in a P2P overlay network to reduce the chance of the complete
message (i.e., all pieces) being intercepted by an attacker. Senser
similarly leverages the use of disjoint network paths to reduce the
impact of a malicious AS that censors web content.
There are also a number of proposed anti-blocking systems that
use steganography and/or covert channels to bypass blocking ef-
forts. Tor bridges [27] are Tor relays that are not publicly listed
by Tor directory servers, making them more difﬁcult for an adver-
sary to discover and block.
Infranet [9] encodes requests using
a sequence of innocuous-looking webpage requests and hides re-
sponses within JPEG images. Collage [2] also uses steganography,
but uses photo sharing and other user-generated content sites as
“drop boxes” for conveying hidden messages. Similarly, in Censor-
Spoofer [29], clients embed requested URLs using steganographic
techniques in email messages and receive responses via IP packets
with spoofed source addresses. More recently, a number of decoy
routing solutions have been introduced [13, 15, 31]. Decoy routers
intercept SSL/TLS streams addressed to an unﬁltered destination.
The routers — which must not be subject to censorship and must
be positioned between the sender and the addressed destination —
decipher the hidden destination that is embedded in the SSL/TLS
exchange (typically, in the handshake) and redirect the SSL com-
munication to the hidden destination. Recent work has shown that
adversaries can adjust their routes to enumerate decoy routers and
defeat the anti-censorship measure [21].
All of the above anonymity systems implicitly assume that the
privacy network’s egress points can correctly deliver requests and
return the correct responses. Senser can help mitigate potential
“last mile” attacks in which the adversary modiﬁes content as it
leaves or re-enters the anonymity network. By itself, Senser does
not (and is not intended) to bypass censorship efforts. Rather, our
focus is to identify when an adversary (such as a censor) modiﬁes a
web page element and, when such attacks take place, how the page
has been modiﬁed.
3. SYSTEM DESIGN
This section describes in greater detail the Senser system. We
begin by presenting an overview of the Senser architecture (Sec-
tion 3.1). We then discuss the major functionalities of Senser, in-
cluding (i) the construction of a concise description of a requested
URL which we call its summary (Section 3.2), (ii) the formation of
a consensus that represents the majority of the proxies’ interpreta-
tions of the requested URL (Section 3.3), and (iii) a proxy selection
algorithm that reduces the inﬂuence of one or more malicious au-
tonomous systems in the network (Section 3.4). We discuss the
handling of several practical deployment issues in Section 4.
3.1 System Overview
Senser defends against potential manipulation of retrieved web
contents by strategically relaying a user’s request for a target web-
page to multiple proxies, with the hope that a majority of these
proxies will individually and correctly retrieve the webpage.
AS3
Proxy P1
AS5
AS1
AS2
Client
Proxy P2
AS4
(a)
AS3
AS1
AS2
Client
Proxy P2
AS4
(b)
foo.com
Proxy P3
Proxy P1
AS5
foo.com
Proxy P3
Figure 1: Senser validates retrieved web contents in various attack sce-
narios. (a) An adversary in AS2 modiﬁes (e.g., censors) the web content
of foo.com. Proxy P1 and Proxy P3 take routes AS3 → AS5 and AS4 →
AS5 respectively, both of which avoid the modiﬁcation by AS2. The client
leverages these proxies to circumvent the attack. (b) Proxy P3 is compro-
mised and censors foo.com. The client retrieves copies of the requested
URL from P1, P2, and P3, and constructs a “majority opinion” response.
Senser uses the same mechanism to validate HTML content and