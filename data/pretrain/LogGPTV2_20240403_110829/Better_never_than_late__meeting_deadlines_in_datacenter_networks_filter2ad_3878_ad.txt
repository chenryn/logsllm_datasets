s
s
m
m
(
(
e
e
n
n
i
i
l
l
d
d
a
a
e
e
D
D
 100
 100
 10
 10
)
)
s
s
m
m
(
(
e
e
n
n
i
i
l
l
d
d
a
a
e
e
D
D
 100
 100
 10
 10
)
)
s
s
m
m
(
(
e
e
n
n
i
i
l
l
d
d
a
a
e
e
D
D
Loss followed
Loss followed
by TCP timeout
by TCP timeout
 10
 10
 100
 100
 1
 1
 1
 1
Flows still miss
Flows still miss
deadlines
deadlines
 10
 10
 100
 100
 1
 1
 1
 1
 1
 1
 1
 1
 10
 10
 100
 100
Flow completion time (ms)
Flow completion time (ms)
Flow completion time (ms)
Flow completion time (ms)
Flow completion time (ms)
Flow completion time (ms)
Figure 11: Scatter plot of ﬂow completion times vs.
deadlines for TCP (left), RCPdc (middle), and D3
(right). Points above the diagonal reﬂect ﬂows that
have met their deadlines.
Fair share protocols, like RCPdc and DCTCP, address pre-
cisely this issue by ensuring no losses and short queues in the
network; yet, as they are unaware of deadlines, a large num-
ber of ﬂows still miss their associated deadlines (see middle
Figure 11). For these particular experiments, RCPdc misses
8.1% of ﬂow deadlines as compared to 1% for D3. Further,
looking at ﬂows that do miss deadlines, RCPdc causes their
completion time to exceed their deadline by 75% at the 95th
percentile (30% on average). With D3, completion times are
extended by 27.7% at the 95th percentile (9% on average).
This implies that even if deadlines were “soft”, fair share
protocols still suﬀer, as a non-negligible fraction of ﬂows ex-
ceed their deadline signiﬁcantly. This is not acceptable for
datacenters where good performance has to be ensured for
a very high percentile of ﬂows.
For TCPpr, we use two-level priorities. Flows with short
deadlines are mapped to the higher priority class. Such
“deadline awareness” improves its performance over TCP.
However, it remains hampered by TCP’s congestion control
algorithm and suﬀers from losses.
Increasing the priority
classes to four (maximum supported by the switch) does
not improve performance signiﬁcantly. Simply using priori-
ties with RCPdc will not help either. This is because dead-
lines can vary a lot and require a high number of priority
classes while today’s switches only support O(10) classes.
Further, as discussed in Section 3, switch prioritization is
packet-based while deadlines are associated with ﬂows. Fi-
nally, bursty datacenter traﬃc implies that instead of stati-
cally mapping ﬂows to priority classes, what is needed is to
dynamically prioritize ﬂows based on deadlines and network
conditions. D3 tackles precisely all these issues.
With background ﬂows. We repeat the above ﬂow burst
experiment by adding long, background ﬂows. Such ﬂows
are used to update the workers with the latest data, and
typically don’t have deadlines associated with them. For
each run, we start a long ﬂow from a traﬃc generator to the
receiver. This ﬂow is bottlenecked at the link between the
receiver and its ToR switch. After ﬁve seconds, all senders
send their responses ([2KB, 50KB]).
Figure 12 shows the high application throughput oper-
ating regime for the three protocols. When compared to
the response-ﬂows-only scenario, the relative drop in the
maximum number of senders supported is less for D3 than
for TCP, TCPpr and RCPdc. Performance signiﬁcantly de-
majority of ﬂows will require more than 3 RTTs to complete,
at which point the TCP send window will exceed 8 packets.
58D3
D3
24
24
s
s
r
r
e
e
d
d
n
n
e
e
S
S
f
f
o
o
r
r
e
e
b
b
m
m
u
u
N
N
 35
 35
 30
 30
 25
 25
 20
 20
 15
 15
 10
 10
 5
 5
 0
 0
 RCPdc
 RCPdc
TCPpr
TCPpr
TCP
TCP
24
24
12
12
12
12
30
30
14
14
2
2
0
0
2
2
0
0
2
2
0
0
Tight
Tight
Moderate
Moderate
Deadlines
Deadlines
Lax
Lax
Figure 12: Number of concurrent senders supported
while ensuring more than 99% application through-
put in the presence of long background ﬂows.
)
%
(
t
u
p
h
g
u
o
r
h
T
.
p
p
A
 100
 90
 80
 70
 60
D3
RCPdc
TCPpr
TCP
 0
 5
 10
 15
 20
 25
 30
 35
 40
Number of Senders
Figure 13: Application throughput with varying
number of senders, 2.9KB response ﬂows and one
long background ﬂow.
grades for TCP and TCPpr in the presence of queuing due
to background ﬂows, which has been well documented in the
past [4]. It is noteworthy that even with only two senders,
TCP cannot achieve 99% of application throughput. TCPpr
implements a 3-level priority in this scenario, where back-
ground ﬂows are assigned to the lowest priority class, and
deadline ﬂows are assigned according to their deadline value
to the other two priority classes. However, the background
ﬂows still consume buﬀer space and hurt higher priority re-
sponse ﬂows. Hence, D3 is even better at satisfying ﬂow
deadlines in the presence of background traﬃc.
With tiny response ﬂows. TCP’s travails with ﬂow bursts
seen in datacenters have forced application designers to use
various “hacks” as workarounds. This includes restricting
the response ﬂow size [4]. Here, we repeat the ﬂow burst
experiment with a uniform response size of 2.9KB such that
response ﬂows are 2 packets long. Further, there exists
one long background ﬂow. Figure 13 shows the application
throughput with moderate deadlines (mean=30ms). With
TCP, the long ﬂow ﬁlls up the queue, causing some of the
response ﬂows to suﬀer losses. Consequently, application
throughput starts dropping at only 12 senders. As a con-
trast, the other three approaches satisfy all deadlines untill
40 senders. Since response ﬂows are tiny, there is no room
for D3 to improve upon RCPdc and TCPpr. However, our
earlier results show that if designers were to be freed of the
constraints imposed by TCP ineﬃciencies, D3 can provide
signiﬁcant gains over fair sharing approaches and priorities.
Since RCPdc presents an upper limit for fair share pro-
tocols and performs better than priorities, in the following
sections, we will compare D3 against RCPdc only; TCP will
also be presented as a reference for today’s status quo.
6.1.2 Benchmark trafﬁc
In this section, we evaluate the performance of D3 under
realistic datacenter traﬃc patterns and ﬂow dynamics. As
before, we choose one endhost in each rack as the aggrega-
Flows/s
TCP (reduced RTO) RCPdc
1300
100 (1100)
D3
2000
Table 1: Flow arrival rate supported while main-
taining >99% application throughput.
)
)
s
s
m
m
(
(
e
e
m
m
l
l
i
i
t
t
n
n
o
o
i
i
t
t
e
e
p
p
m
m
o
o
c
c
w
w
o
o
F
F
l
l
to 300ms
to 300ms
D3
D3
RCPdc
RCPdc
TCP
TCP
 20
 20
 15
 15
 10
 10
 5
 5
 0
 0
D3
D3
RCPdc
RCPdc
TCP
TCP
)
)
s
s
p
p