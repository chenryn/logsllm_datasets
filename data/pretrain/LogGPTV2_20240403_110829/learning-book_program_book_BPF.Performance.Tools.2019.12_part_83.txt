hist (nsecs - lock_start[tid] } 
delete (@lock_start [tid] 1
delete (elock_addr [tid]) =
END
clear (8lock_atart) 
clear (8lock_adde) 
---
## Page 665
628
Chapter 13 Applications
This records a timestamp and the lock address when pthread_mutex_lock( begins, then fetches
these when it ends to calculate the latency and save it with the lock address and stack trace. The
ustack(5) can be adjusted to record as many frames as you wish.
The path to /lib/x86_64-linux-gnu/libpthread.so.0 may need to be adjusted to match your system.
Stack traces may not work without frame pointers in the calling software, and libpthread as well.
(It may work without libpthread frame pointers since it's tracing the entry point to the library,
and the frame pointer register may not have been reused yet.)
The latency to pthread_mutex_trylock( is not traced as it is assumed to be fast, as is the purpose
of the try-lock call. (This can be verified with BCC funclatency(8).)
pmheld
The source code to pmheld(8) is:
1/usr/local/bin/bpftrace
BEGIN
printf(*Tracing libpthread mutex held times, CtrlC to end.\n*)
uprobe:/1ib/xB6_6411nux-gnu/1ibpthzead.so 0:pthzead_nutex_1ock,
uprobe:/1ib/x86_641inux=gnu/lilbpthread.so 0:pthread_mutex_trylock
/$1 == 0 11 pid == 51/
8lock_addx[t1d] = arg0;
uretprobe:/lib/x86_64linux=gnu/libpthread,so,0:pthread_mutex_lock
/($1 == 0 11 pld == $1) ss @lock_addx[tid]/
Sheld_start[pld, Φlock_addr[tid]] = nsecs;
delete (elock_addr [tid]) 
uzetprobe:/11b/xB6_641inux=gnu/11bpthzead.so ,0:pthzead_nutex_txylock
/retva] == 0 ss ($1 -= 0 11 pid -- $11 ss 8lock_adde [tid] /
Bheld_start [pid, elock_addr[tid]] = nsecsf
delete (@lock_addr [t1d]) 
---
## Page 666
13.2 BPF Tools
629
uprobe:/1ib/x86_641inux-gnu/lilbpthread.so 0:pthread_mutex_unlock
/($1 == 0 11 p1d == $1) 4s @he1d_staxt [pld, axg0]/
gheId_t.ne_ns [usym (arg0) , ustack(5) , conm[ =
hist (nsecs - @held_start[pid, arg0| :
delete (@held_start[pid, axg0]1 
END
clear (8lock_addc) 
 (1xespeqg) xee[
The time is now measured from when the pthread_mutex_lock() or pthread_mutex_trylock()
function returnsand hence the caller holds the lockto when it calls unlock().
These tools used uprobes, but libpthread has USDT probes as well, so these tools could be rewrit
ten to use them.
13.2.15
naptime
naptime(8)1 traces the nanosleep(2) syscall and shows who is calling it and for what sleep dura-
tion. I wrote this tool to debug a slow internal build process that would take minutes without
seemingly doing anything, and I suspected it included voluntary sleeps. The output:
+ naptime.bt
Attach.ing 2 pzobes...
Tracing sleeps. Hit Ctrl-C to end.
7IHE
PPID
PCONH
PID
COMH
SECONDS
19 :09:19 1
systend
1 975
iscsid
1. 000
19:09:20 1
systend
2.274
nyaqld
1.000
19 :09 : 20 1
systend
1975
iscsid
1, 000
19:09:21 2998
bu11d-n1t
2513T
sleep
30.000
19 :09 :21 1
systend
2274
mysg] d
1. 000
19:09:21 1
systend
1 975
1scsid
1.000
19:09:22 1
systend
24.21
1rqbslance
9, 999
[..-]
This has caught a sleep for 30 seconds by build-init. I was able to track down that program and
*tune* this sleep, making my build 10 times faster. This output also shows mysqld and iscsid
11. Origin: I created it for this book on 16-Feb-2019, inspired by Sasha Golkdsthein’s SyS_nan
trace(8), and to debug the slow build described here. The build was of an intermal nffx-bpftrace packsge I was developing.
---
## Page 667
630
Chapter 13 Applications
threads sleeping for one second every second. (Weve seen that mysqld sleep in earlier tool outputs.)
Sometimes applications can call sleep as a workaround for other issues, and the workaround can
stay in the code for years, causing performance problems. This tool can help detect this issue,
0 paoadxa st peauaao at pue susodaen daapsoueusaquasis:seosis ap Supen Aq sqom su
be negligible.
The source to naptime(8) is:
+1/usr/local/bin/bpftrace
include 
#1nclude 
BEGIN
F
printf(*Txacing sleeps. Hit Ctzl=C to end. ,n*)≠
printf (*8s 6s 16s 6s 16s sn*,*rIME*, *PPIo*, *F00r*,
*PID*, "CONK*, *SECONDS");
tracepoint1syscalls1sys_enter_nanosleep
/ossnzeal_paxent=>pld,
$task->real_parent=>comm, pid, conm,
axgs=>rqtp=>tv_sec, axgs=>zqtp=>tv_nsec / 10o00o0) :
Parent process details are fetched from the task_struct, but this method is unstable and may
require updates if that task_struct changes.
This tool can be enhanced: the user-level stack trace could be printed out as well to show the code
path that led to the sleep (provided that the code path was compiled with frame pointers so that
the stack can be walked by BPF).
13.2.16 0ther Tools
Another BPF tool is deadlock(8), ² from BCC, which detects potential deadlocks with mutex usage
sog aesn xapnu ag Supuasandas qdesg paoap e spnq s stuosasu rapao xoo go tuog ag po
detecting deadlocks. While the overhead of this tool can be high, it helps debug a difficult issue.
12 desdlock(8) was developed by Kenry Yu on 01-Feb-2017,
---
## Page 668
13.3 BPF One-Liners
631
13.3
BPF One-Liners
adu sau-auo ates at sqssod aaum sau-auo aodq pue g moqs suops asat
mented using both BCC and bpftrace.
13.3.1BCC
New processes with arguments:
execanoop
Syscall count by process:
syscount -P
Syscall count by syscall name:
syscount
Sample user-level stacks at 49 Hertz, for PID 189:
681 d-6 -0-e1T3oxd
Count off-CPU user stack traces:
stackcount -0 t:sched:sched_sxltch
Sample all stack traces and process names:
pzofile
Count libpthread mutex lock functions for one second:
funccount -d 1 */1lb/xB6_64-1inux=gnu/1ibpthread.so.0:pthzead_nutex_*1ock*
Count libpthread conditional variable functions for one seconxd:
funccount -d 1 */11b/x86_64-1inux=gnu/11bpthread. so.0:pthzead_
13.3.2 bpftrace
New processes with arguments:
Syscall count by process:
Syscall count by syscall name:
Sample user-level stacks at 49 Hertz, for PID 189:
---
## Page 669
632
2 Chapter 13 Applications
Sample user-level stacks at 49 Hertz, for processes named “rmysqld°:
Count off-CPU user stack traces:
Sample all stack traces and process names:
Sum malloc( requested bytes by user stack trace (high overhead):
= [(s)xoessn]@ 1ootteu:os*,z*gogtt/nubxnutt-t99gx/gtt/:n,e eoexagdo
(f1Be)uns
Trace kill(0 signals showing sender process name, target PID, and signal number:
bpftrace -e *tiayscallsiays_enter_kill [ printf ("es -> PID ed SIG ed\n", comm,
axgs->pid, args=>sig]; }*
Count libpthread mutex lock functions for one second:
1xoot,xeaupeeauad:g*os*peexuadgtt/nuhxnutt-sgex/att/in. 8- eoexagdg
B[probe] = count(lr } interval:s:l f exit(l; /*
Count libpthread conditional variable functions for one second:
bpftrace -e *u:/1ib/x86_64=1inux=gnu/1ibpthread.so.0:pth.read_cond_*(
[pzobe]=count (1:) intexval:a:1 [exlt(1 :)*
Count LLC misses by process name:
(lunoo=[uuoo]a]:sesstu-egoeo:eaexpxey,8-eoexdo
13.4 BPF One-Liners Examples
Incluxding some sample output, as was done for each tool is also useful for illustrating one-liners
Here is a selected one-liner with example output.
13.4.1Counting libpthread Conditional Variable Functions for One
Second
1puoopeexud:0ospeaxqdqrt/nu6xnut998x/4t/:n, 0- aoexdq +
[pzobe] = count () : } interval:s:1 [ exit(1 : 1*
Attaching 19 probes...
e[upzobe:/11b/x86_641inux-gnu/1ibpthread. so.0:pthread_cond_xal ce@GLIBC_2.3.2] : 70
e [uprobe:/1ib/x86_641inux=gnu/libthread. so. 0 :pthread_cond_xait] : 70
e[upzobe:/11b/x86_641inux=gnu/1ibpthread, ao. 0:pthread_cond_1n.tggGLIBC_2 3.2| : 573
9 [uprobe: /lib/x86_641inux=gnu/1ilbpthread.so. 0 :pth.read_cond_t.imedvai t89GLIBC_2 3 .2] : 673
---
## Page 670
13.5Summary
633
e [uprobe: /1ib/x86_641inux-gnu/libpthread. so. 0 :pthread_cond_destroy8GLIBC_2 3.2] : 939
 [uprobe :/1sb/x85_6411nux=gnu/11bpthxead, so 0:pthread_cond_broadcastB8GLIBC_2 3.21 : 1796
[uprobe:/lib/x86_641inux=gnu/libpthread. so. 0:pthread_cond_broadicast] : 1796
e[upzobe: /11b/x86_641inux=gnu/11bpthread., so. 0:pthread_cond_s1gnal] : 4600
9 [uprobe:/1ib/x86_641inux=gnu/1ibth.read. so. 0 :pthread_cond_signa198GLIBC_2 3 .2] : 4602
These pthread functions can be frequently called, so to minimize performance overhead only one
waits for threads waiting on a CV and other threads sending signals or broadcasts to wake them up.
second is traced. These counts show how conditional variables (CVs) are in use: there are timed
This one-liner can be modified to analyze these further: including the process name, stack traces,
timed wait durations, and other details.
13.5Summary
In this chapter I showed additional BPF tools beyond the prior resource-oriented chapters for
application analysis, covering application context, thread usage, signals, locks, and sleeps. I used
qoq Sursn ddg tuoy xaquoo Azanb st peas pure *uoeordde sa8ue afduexa ue se sauas TOs
USDT probes and uprobes. On-CPU and off-CPU analysis using BPF tools was covered again for
this example application due to its importance.
---
## Page 671
This page intentionally left blank
---
## Page 672
Chapter 14
Kernel
The kernel is the heart of the system; it is also a complex body of software. The Linux kernel
employs many different strategies for improving CPU scheduling, memory placement, disk
I/O performance, and TCP performance. As with any software, sometimes things go wrong.
Previous chapters instrumented the kernel to help understand application behavior. This
chapter uses kernel instrumentation to understand kernel software, and will be of use for kernel
troubleshooting and to aid in kernel development.
Learning Objectives:
 Continue off-CPU analysis by tracing wakeups
 Identify kernel memory consumers
Analyze kernel mutex lock contention
Show activity of work queue events
If you are working on a particular subsystem, you should first browse the tools in the relevant
re asas 'sueu uapsisqns xnur g sade smotaaud
sched: Chapter 6
mm: Chapter 7
gaqdeu sg 
=block: Chapter 9
 net: Chapter 10
Chapter 2 also covers tracing technologies, incluxding BPF, tracepoints, and kprobes. This chapter
focuses on studlying the kernel rather than the resources, and includes additional kernel topics
beyond the previous chapters. I begin with background discussion, then BPF capabilities, kernel
analysis strategy, traditional tools including Ftrace, and BPF tools for additional analysis: wakeups,
kernel memory allocation, kernel locks, tasklets, and work queues.
---
## Page 673
636
Chapter 14 Kernel
14.1E
Background
The kernel manages access to resources and schedules processes on the CPUs. Previous chapters
have already introduced many kernel topics. In particulat, see:
• Section 6.1.1 for the CPU Modes and the CPU Scheduler sections
 Section 7.1.1 for the Memory Allocators, Memory Pages and Swapping, Page-Out Daemon,
and File System Caching and Buffering sections
Section 8.1.1 for the I/O Stack and File System Caches sections
•Section 9.1.1 for the Block 1/O Stack and I/O Schedulers sections
Section 10.1.1 for the Network Stack, Scaling, and TCP sections
Additional topics for kernel analysis are explored in this chapter.
14.1.1 Kernel Fundamentals
Wakeups
When threads block and go off CPU to wait for an event, they usually return to the CPU when
triggered by a wakeup event. An example is disk I/O: a thread may block on a file system read that
issues a disk I/O and is later woken up by a worker thread that processes the completion interrupt.
In some cases, there is a dependency chain of wakeups: one thread wakes up another, and that
thread wakes up anothe, until it wakes up the blocked application.
Figure 14-1 shows how an application thread can block and go off CPU for a syscall, to be later
woken up by a resource thread with possible dependency threads.
Application Request
Application Thread
On-CPU User
On-CPU Kernel
block
Of-CPU
ResourceThr
Dependency Thread
Figure 14-1 Off-CPU and wakeups
Tracing the wakeups can reveal more information about the duration of the off-CPU event.
---
## Page 674
14.1Background 637
Kernel Memory Allocation
Two main allocators in the kernel are the:
 slab allocator: A general-purpose memory allocator for objects of fixed sizes, which
supports caching allocations and recycling them for efficiency. In Linux this is now the
slub allocator: it is based on the slab allocator paper [Bonwick 94], but with reduced
complexity.
• page allocator: For allocating memory pages. It uses a buddy algorithm, which refers to
finding neighboring pages of free memory so that they can be allocated together. This is
also NUMA aware.
These allocators were mentioned in Chapter 7 as background for application memory usage
analysis. This chapter focuses on kernel memory usage analysis.
The API calls for kernel memory allocation include kmalloc(), kzalloc(), and kmem_cache_alloc()
(slab allocation) for small chunks, vmalloc() and vzalloc() for large areas, and alloc_pages() for
pages [156].
Kernel Locks
User-level locks were covered in Chapter 13. The kernel supports locks of different types: spin
locks, mutex locks, and reader-writer locks. Since locks block threads, they are a source of
performance issues.
Linux kernel mutex locks are a hybrid with three acquisition paths, tried in the following
order [157]:
1. fastpath: Using a compare-and-swap instruction (cmpxchg)
2. midpath: Optimistically spinning first if the lock holder is running in case it is about to be
paseaat
3. slowpath: Blocking until the lock is available
There is also the read-copy-update (RCU) synchronization mechanism that allows multiple
reads to occur concurently with updates, improving performance and scalability for data that is
mostly read.
Tasklets and Work Queues
For Linux, device drivers are modeled as two halves, with the top half handling the interrupt
quickly and scheduling work to a bottom half to be processed later [Corbet O5]. Handlling the
interrupt quickly is important because the top half runs in interrupt-disabled mode to postpone
the delivery of new interrupts, which can cause latency problems if it runs for too long. The
bottom half can be either tasklets or work queues; the latter are threads that can be scheduled by
the kernel and can sleep when necessary, This is pictured in Figure 14-2.
---
## Page 675
638
Chapter 14 Kernel
Kernel
Work Queue
Tasklet
Scheduler
Interrupt
Service Routine
Interrupt
Device
Figure 14-2 Tasklets and work queues
14.1.2 BPF Capabilities
questions as:
• Why are threads leaving the CPU, and how long are they off CPU?
• What events did off-CPU threads wait for?
• Who is currently using the kernel slab allocator?
 Is the kernel moving pages to balance NUMA?
What work queue events are occurring? With what latencies?
 For kernel developers: which of my functions are called? With what arguments and return
value? With what latency?
These can be answered by instrumenting tracepoints and kernel functions to measure their
latency, arguments, and stack traces. Timed sampling of stack traces can also be used to provide
a view of on-CPU code paths, which usually works because the kernel is typically compiled with
stack support (either frame pointers or ORC).
Event Sources
Kernel event types are listed in Table 14-1, along with instrumentation sources.
Table 14-1  Kernel Event Types and Instrumentation Sources
Event Type
Event Source
Kermel function execution
kprobes
Scheduler events
sched tracepoints
System calls
syscalls and raw_syscalls tracepoints
---
## Page 676
14.2 Strategy 639
Event Type
Event Source
Kermel memory allocation
kmem tracepoints
Page out daemon scanning
vmscan tracepoints
Interupts
irq and irq_vectors tracepoints
Workqueue execution
Timers
workqueue tracepoints
timer tracepoints
IRQ and preemption disabled