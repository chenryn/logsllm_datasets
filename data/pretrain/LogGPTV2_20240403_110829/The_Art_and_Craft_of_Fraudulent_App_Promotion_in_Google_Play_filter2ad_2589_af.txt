reviews is provided by the developers, but also they can write their
own reviews. Consistent with previous observations, e.g. [38, 42, 51,
52, 60, 61, 63–65, 74, 76, 96, 97, 99], several participants admitted to
reuse common linguistic patterns and copy reviews across similar
products. We also con(cid:27)rmed this (cid:27)nding in our quantitative study.
Further, most participants claimed to write short reviews, which
is also re(cid:30)ected in our gold standard fraud data. Previous work,
e.g., [38, 49, 51, 52, 60, 65], also made this observation, and attributed
it to the fraudster lack of experience with the product. However,
we also present evidence of ASO workers who post much longer
reviews. We conjecture that fraud evasion can also be a factor.
5.11 Ratings
Rating choice strategies. All 18 interview participants admitted
writing mostly 4 or 5-star reviews unless they receive special in-
structions from the developers. 8 participants (P3, P7, P8, P9, P10,
P11, P14, P18) said that they receive instructions on the ratio of
review ratings from the developers. For instance, P12 said that
“developers request us to write a few 4, 3 and even few 1 star reviews.”
When there are no instructions on the rating distribution, several
participants claimed to maintain their own ratio. For instance, P5,
P16, P17 claimed to post a 10% vs. 90% ratio of 4 to 5 star reviews,
P2, P10, P18 have a 20%-80% ration, P1, P9 have a 30%-70% ratio
and P13 has a 40%-60% ratio. 3 participants (P6, P11, P14) said that
they do not maintain any speci(cid:27)c ratio, while P4 and P12 post only
5-star reviews.
3 participants claimed strategies to also post lower ratings, in
order to avoid detection. For instance, P6 said that: “if the average
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2448Figure 13: Co-review graphs built over the accounts claimed
to be controlled by (left) F13 and (right) F32. Edge width is
proportional to the number of apps reviewed in common by
the endpoint accounts. 14 accounts revealed by F13 form a
clique, and on average, any two accounts reviewed 78 apps
in common.
7 participants said that they add pro(cid:27)le pictures, which they
retrieve from di(cid:29)erent sources, e.g., Google search, Google Plus,
pixabay.com, to make the account look more authentic. P9 said
“After we use fake name generator to create the account name, we
search the name in Google Plus and choose a pro(cid:27)le, then we choose
a random person from the list of followers and use his image for the
account pro(cid:27)le.” P10 however said that “We use no picture as picture
de(cid:27)nes your demographics. Buyers do not want this now.”
Account Creation vs. Purchase. 6 participants (P1, P3, P7, P10,
P13, P16) claimed to create new accounts periodically, ranging
from once a day (P10) to once a month (P16). P2 and P9 claimed
to create new accounts when they don’t have enough accounts for
a job, especially when the job requests accounts from a speci(cid:27)c
geographic region. P5 and P18 create new accounts when Google
deletes some of their accounts. P15 create new accounts when the
job requests more reviews than they can provide. 5 participants
(P1, P5, P13, P17) admitted to purchase new accounts. P1 claimed
to have purchased more than 10,000 accounts, while P13 claimed
to have purchased 47,000 accounts. Two participants (P1 and P3)
volunteered the fact that they age their new accounts (1–2 months)
before using them to post reviews.
Summary. The claims of fake name generator use, provide evi-
dence toward limited variability in naming patterns for the worker-
controlled accounts, as previously assumed [85]. We identi(cid:27)ed pro-
(cid:27)le photo plagiarism behaviors, but also a claimed developer-driven
trend to avoid pro(cid:27)le photos.
5.14 Validation and E(cid:28)cacy of ASO
Validation of quantitative study. Collecting ground truth fraud
data attributed to the workers who created it, is a di(cid:28)cult task. We
believe that any process to obtain such ground truth data needs to
involve the workers. In addition, to gain con(cid:27)dence in the correct-
ness of the accounts claimed to be controlled by the 39 workers,
we used co-review graphs built over the accounts claimed to be
controlled by each worker: nodes are user accounts, and edges have
weights that denote the number of apps reviewed in common by
the end-point accounts. Figure 13 shows example co-review graphs
built over the accounts revealed by F13 and F32.
Figure 14(top) shows the average co-review weight of the ac-
counts claimed to be controlled by each of the 39 ASO workers, i.e.,
the ratio of the sum of all edge weights to the number of edges.
Figure 14: Density and average weight for co-review graphs
of 39 ASO workers. 12 workers have complete graphs (den-
sity=1). 30 workers have graphs with density at least 0.75.
Figure 15: Active vs. inactive accounts controlled by the 39
quantitative study participants. We observe diverse success
in keeping accounts active on the long term.
Figure 14(bottom) shows the edge density of the worker co-review
graphs, i.e., the ratio of the number of co-review edges to the maxi-
mum number of edges possible in that graph. The co-review graphs
of 12 of the workers are cliques, i.e., any two accounts have re-
viewed at least one app in common. Further, the co-review graphs
of 16 workers have an average weight of at least 10, up to 78.61
for F13. This is in contrast to the probability of co-rating two apps
in Apple’s China App Store, of 0.163% (computed over 0.5 million
random accounts) [93].
In addition, we manually investigated the accounts revealed by
the 39 workers, and found multiple instances of repeated pro(cid:27)le
photos, mostly of glamorous people, and simple patterns in the
account names.
E(cid:28)cacy of ASO. To investigate the e(cid:28)cacy of the ASO strategies
employed by the 39 workers who participated in our quantitative
study, we look at (1) the number of accounts that they control that
are still active, and (2) the impact of their ASO campaigns.
Figure 15 shows the number of accounts controlled by each of
the 39 workers, that are active and inactive (i.e., Google returns 404
not found error). Of the 1,164 accounts known to be controlled by
the 39 workers, 120 were inactive (10.30%) in May 2019. Qualitative
study participants stated that they never abandon accounts unless
they are closed by Google or Google (cid:27)lters all their reviews. Thus,
Figure 15 reveals diverse success among the 39 ASO workers, in
terms of being able to keep their accounts active long term: while a
majority of the workers have all their accounts still active, including
the workers with more than 40 accounts, several workers had a
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2449the competitive nature of the market. In this section, we propose
disruption strategies for each vulnerability point identi(cid:27)ed in the
fraud work(cid:30)ow of Figure 1, and discuss potential implications of our
study’s (cid:27)ndings, on future fraud detection and prevention solutions.
The opaque nature of commercial fraud detection systems prevents
us from establishing the costs and scalability of implementing the
proposed recommendations, or from determining if they are already
implemented. However, manual veri(cid:27)cation of statements made
by ASO workers revealed several weaknesses in Google’s defense.
Some of the following defenses propose to address them.
VP1: Proactive Fraud Monitoring. Recruiting WhatsApp/Face-
book groups need to aggressively accept new collaborators. We
veri(cid:27)ed that these communication channels are easy to in(cid:27)ltrate.
Thus, we recommend to proactively detect campaigns at this point,
and (cid:30)ag apps likely to receive fraudulent reviews, and suspicious
accounts engaged in posting fraud.
VP2: Device Fingerprinting. We observe that device models and
their per-country popularity can be used to detect reviews written
from accounts claiming to be from a country where the posting
device is not popular. However, this vulnerability could also be used
by ASO workers to blend in with normal users, by mimicking the
distribution of devices observed in Google Play.
Further, this device-model leaking bug can also be used by com-
puter criminals to perform reconnaissance on potential victims.
Figure 7(c) in shows the top 15 most popular devices, out of 11,934,
that were used to post 198,466,139 reviews in Google Play. An ad-
versary could use this bug to for instance, identify owners of device
models known to be vulnerable, e.g., [27, 90]. We noti(cid:27)ed Google
about the dangers of this bug, see § 4.3.
VP3: 1-to-1 Review-To-Device. Our interviews and experiments
revealed that a user can download an application once, and review
it as many times as the number of accounts she has logged in to
the device (up to 5, claimed by, e.g., P8). We suggest enforcing that
a device can be used to post only 1 review per downloaded app.
VP4: Organic Fraud Detection. We suggest the use of account
activity levels to di(cid:29)erentiate organic from inorganic (sockpuppet)
accounts. Organic ASO workers are likely to use their devices
continuously, like the normal users that they almost are. Sockpuppet
accounts are more likely to experience inactive interludes given the
dynamic of their work(cid:30)ow (§ 5.5). Account activity includes but is
not limited to the number of apps with which the account interacts
per time unit, the duration of such interactions, and the number of
other Google services (maps, gmail, drive, music, etc) to which it
is subscribed. Additionally, our data and experiments reveal that
some workers may even be posting only laptop-based reviews as
all their reviews were written from devices of unknown models.
Our study suggests that these workers are more likely to control
sockpuppet accounts. This requires however future validation.
VP5: Monitor Review Feedback. An account should be able to
upvote or downvote a review only if it has installed the respective
app on at least one device. We veri(cid:27)ed that this is not currently
enforced by Google Play. Fraud attribution (see below) can also be
used to discount upvotes from accounts known to be controlled by
the same ASO worker as the one that posted the review.
VP6: Verify App Install and Retention. We recommend devel-
oping protocols to verify that an app has been or is still installed
on the device, e.g., before accepting a user review from that device.
In addition, we studied the impact of a worker on each app on
which he has performed an ASO campaign. We denote the impact
IA of an ASO worker W for an app A to be the change in A’s rating
during W ’s active interval. Speci(cid:27)cally, IA = Rf − Ri, where Ri is
A’s “initial” average rating, i.e., before the (cid:27)rst review posted by
the worker for A, from any of his accounts, and Rf
is A’s “(cid:27)nal”
average rating, after W ’s last review posted for A. Figure 16 shows
the violin plots of the distribution of impact values, over all the apps
campaigned by each of the 39 ASO workers, from all the accounts
that each controls. We observe diverse abilities of these workers. For
workers like F7, F12, and F21, we observe only positive impact on
the average ratings of all the apps that they target. Most workers
however have mixed impact, with many of their targeted apps
seeing up to 5 star increase in average rating during their active
interval, and a few others seeing up to a 2 star drop. We observe
however that overall, apps seem to bene(cid:27)t from the campaigns in
which these workers have contributed.
We conclude that di(cid:29)erent strategies have di(cid:29)erent impact on
the ability of ASO workers to avoid detection and impact the ratings
of apps that they target.
Figure 16: Impact of campaigns conducted by the 39 quanti-
tative study participants, on the average rating of apps for
which they campaigned. We observe diverse success in in-
creasing the average rating of targeted apps.
majority of their accounts closed. Notably, 36 out of the 47 accounts
controlled by F34 are closed, as are 29 out of 35 accounts of F31.
Our study has several limitations. First, we do not know all the
accounts controlled by the 39 ASO workers. Second, we cannot
pinpoint the exact strategies that are responsible for the success
to maintain accounts active or ensure that reviews are not (cid:27)ltered.
Such an analysis would require detailed experiments that explore
the impacts of altering a single feature of a fraud detection algo-
rithms that is kept a close secret. Third, the impact that we com-
puted, is oblivious to simultaneous campaigns being conducted by
other workers on the same apps. Finally, our computed average
rating of an app is imperfect, since (1) we do not have access to
ratings posted without reviews, and (2) may not correctly model
Google’s algorithm, that e.g., may assign weights to ratings based
on perceived usefulness, fraudulence or recency [72]. We describe
more limitations of our studies, in § 7.
6 DISCUSSION AND RECOMMENDATIONS
The varied capabilities, behaviors and evasion strategies claimed
and exhibited by the studied participants, suggest that fraud detec-
tion solutions should cast a wider net. While some of our partici-
pants seem to (cid:27)t the mold of assumptions made in previous work,
we present claims and evidence of evolution, perhaps fueled by
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2450While remote attestation inspired solutions (e.g., [46]) will not be
secure without device TPMs, defeating such solutions will require
signi(cid:27)cant investment from ASO workers.
VP7: Account Validation and Re-validation. The cellular provider
used during account validation can also be used to detect inconsis-
tencies with the claimed pro(cid:27)le (e.g., location) of the user account.
Further, several ASO workers mentioned using SIM cards of others
to validate their accounts. Peer-opinion sites could ask users to
re-validate their accounts at random login times (e.g., veiled as
“improved authentication security”), especially if their validating
SIM cards have also been used for other accounts.
VP8: App Usage. Most ASO workers suggest that they use apps
before reviewing them, and keep them installed after review for
a while, to mimic genuine behaviors. However, we believe (but
have not investigated) that features extracted from per-app waiting
times, app interaction modes and times, and post-review behaviors,
are di(cid:29)erent for honest vs. fraudulent accounts, and could be used
to pinpoint sockpuppet and organic fraud accounts. For instance,
it is suspicious if an app receives a good review soon after it was
downloaded, has received little interaction, and is quickly unin-
stalled. Coupled with VP6, mandating wait times to post reviews
will impact the number of apps that an ASO worker device can
store, thus the number of apps that a worker can target at a time.
VP9: Mislead ASO Workers Through Fraud Attribution. SIM
cards can also help attribute sockpuppet accounts to the ASO work-
ers who control them, see e.g., [41]. Account-to-ASO worker attribu-
tion can be used to reduce worker ability to adjust to detection [79]:
to mislead ASO workers into believing that their actions are e(cid:29)ec-
tive, peer-opinion sites could show removed fake positive reviews
only to the accounts used to post them, the other accounts sus-
pected of being controlled by the same worker, and the account of
the app developer. This would force ASO workers to partition their
account set into monitoring-only sets that cannot be used to post
fraudulent reviews, and regular fraud-posting accounts.
VP10: Once a Cheater, Always a Cheater. Our qualitative and
quantitative studies (§ 5.9) provide evidence that developers rehire
ASO workers not only for the same app, but also for other apps
that they develop. We recommend to monitor overlapping accounts
that review sets of apps by the same developer, and red(cid:30)ag fraud
developers early on.