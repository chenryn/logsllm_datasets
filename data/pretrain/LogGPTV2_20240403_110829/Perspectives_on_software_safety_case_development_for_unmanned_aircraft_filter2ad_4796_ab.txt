The outcome of the system safety process,
in effect,
triggers the safety argumentation process (Figure 1).
Abstractly, the main steps in the safety argumentation
process are to (a) deﬁne safety claims, e.g., that a speciﬁc
hazard is eliminated, and (b) identify, select, and link the
evidence, which support the claims made, via a structured
argument that can, on analysis, be agreed upon as acceptable
and trustworthy. The top-level claim (goal) in our safety case
is: The Swift UAS is safe in the context of the deﬁned mission,
in the speciﬁed conﬁguration, on the deﬁned range where it
is to be operated, and under the deﬁned weather conditions.
To develop this claim our strategy is, primarily, to argue
that all identiﬁed hazards across all operating phases have
been mitigated. In turn, we develop the claims of hazard
mitigation by argument over the UAV subsystems (e.g., the
avionics subsystem, of which the autopilot software is a
part).
D. Arguing Software Safety in the System Context
The autopilot software safety case (ASSC) contains a
justiﬁcation for the claim that the autopilot is correct (func-
tional safety), and it makes explicit the heterogeneity of
context, assumptions and evidence inherent in a safety claim,
e.g., Figure 2(a) shows a small excerpt from the ASSC
where we argue the safe computation of the angle of attack
parameter by justifying the claim that the autopilot computes
it accurately and correctly.
This justiﬁcation is through an argument structure which
links diverse evidence, i.e., a proof of correct implementa-
tion, results of reviewing the corresponding speciﬁcation,
data sheets for the air-data (pitot) probe, and the results
of wind tunnel experiments to calibrate the probe. Note
that the argument leg that supports the claim of correct
implementation (G2.2), as we have shown it in Figure 2(a),
abstracts a signiﬁcantly more detailed argument fragment.
The items of evidence in the ASSC are, in part, generated
from formally verifying the autopilot software implementa-
tion against a mathematical speciﬁcation, using AUTOCERT.
The speciﬁcation contains assumptions, e.g., about the air-
craft state and ﬂight plan, and it formalizes the software
requirements, a subset of which are derived from the system
safety requirements. Formal veriﬁcation takes place in the
context of a logical domain theory, i.e., a set of axioms and
function speciﬁcations. Axioms can be either assumed to
be correct, or they can be inspected, or they can be tested
against a computational model which, itself, is inspected.
Through this approach, we are able to integrate formal
reasoning into the construction of a (software) safety case.
Conversely, formal reasoning makes use of safety-relevant
information which has not itself been derived using formal
methods. This can take the form of simplifying assumptions
which are experimentally justiﬁed, or appeals to expert
judgment, e.g., that a parameter is within safe bounds, that
an error is within an acceptable range, or that one subsystem
is similar to another (and therefore has equivalent safety-
related properties).
We automatically transform the output of AUTOCERT into
a safety case fragment, and then merge it into the upper-level
(system) safety case by replacing the relevant, overlapping,
goals in the latter with the auto-generated argument fragment
(containing the proof as evidence). If safety case fragments
have been created prior to the construction of a proof (as
might be reasonably expected), they also can be converted
into formal speciﬁcations for input to AUTOCERT.
E. Assurance of the Arguments
Assurance can be deﬁned as justiﬁed conﬁdence in a
property of interest [12]. Despite the explicit consideration
of diverse evidence and formal reasoning in the software
safety case, subjectivity is inherent in the structure of the ar-
gument and its supporting evidence. To assure that sufﬁcient
conﬁdence can be placed in the arguments made, our ap-
proach is based on quantitative uncertainty assessment [13],
which forms part of the broader safety process (Figure 1).
Conﬁdence assessment provides feedback for improving the
safety case during its evolution (although it is not explicitly
highlighted in Figure 1).
We begin by identifying the sources of uncertainty in
the argument, following which we quantify the uncertainty
introduced by these sources. We use data where available
(in the case of aleatory uncertainties), and subjective judg-
ment for epistemic uncertainties. Thereafter, we aggregate
the quantiﬁed values into an assessment of argument/claim
conﬁdence via probabilistic modeling. More speciﬁcally,
we model the identiﬁed sources of uncertainty as discrete
random variables (r.v.), and we characterize the conﬁdence
in the overall argument as the joint distribution of the r.v.,
using Bayesian Networks (BN).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
C2
Formula for angle of
attack (Flight control
theory)
CONTEXT
G1
In context of
Autopilot module accurately
calculates correct angle of
attack
In context of
C1
Autopilot module
CONTEXT
GOAL
Is solved by
S1
Argument that input
is reliable
STRATEGY
Is solved by
G1.1
Pitot probe provides reliable
sensor values to Autopilot
GOAL
C1.1
In context of
Pitot probe
CONTEXT
Is solved by
S2
Argument that
computation is
correct
STRATEGY
Is solved by
Is solved by
G2.1
G2.2
The specification for
computing angle of attack is
correct
GOAL
Is solved by
Is solved by
STRATEGY
STRATEGY
S2.1
S2.2
Argument that the
correct formula is
used in the
specification
Argument that correct
calibration constant is
used in the
specification
STRATEGY
Computation of angle of attack
is correctly implemented
GOAL
Is solved by
S2.3
Argument by proof of
correctness of
implementation
STRATEGY
In context of
C2.2.1
Specification for
computing angle of
attack
Is solved by
S1.1
Argument of low
probability of sensor
failure on demand
Is solved by
G1.1.1
Pitot probe has an
acceptably low
probability of failure
on demand
GOAL
Is solved by
E4
Datasheets for
Pitot probe
SOLUTION
C2.1.1
Aircraft design team
CONTEXT
Is solved by
Is solved by
Is solved by
CONTEXT
G2.1.1
G2.1.2
G2.2.1
The specification uses the
correct formula for computing
angle of attack
GOAL
The calibration constant used in
the specification is accurate
GOAL
Proof of correct imementation
generated using AutoCert
verification Tool
GOAL
In context of
C2.2.2
AutoCert
verification tool
CONTEXT
Is solved by
S2.1.1
Argument by review
(appeal to domain
expertise)
STRATEGY
Is solved by
G2.1.1
Specification for computing
angle of attack is reviewed to
be correct by aircraft design
team
Is solved by
E1
Outcome of
review and
review data
Is solved by
S2.1.2
Is solved by
In context of
C2.2.3
Argument of
correct
experimental
calibration
STRATEGY
Is solved by
G1.1.1
Pitot probe
calibration is
accurate
GOAL
Is solved by
E2
Data from wind
tunnel
experiments on
air data probe
Automatic
Theorem
Provers
CONTEXT
E3
Proof of
Correctness
SOLUTION
C1.1.1
In context of
Wind tunnel experiments
for air data probe
CONTEXT