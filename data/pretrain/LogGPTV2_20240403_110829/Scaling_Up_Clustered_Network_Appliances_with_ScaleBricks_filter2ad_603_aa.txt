title:Scaling Up Clustered Network Appliances with ScaleBricks
author:Dong Zhou and
Bin Fan and
Hyeontaek Lim and
David G. Andersen and
Michael Kaminsky and
Michael Mitzenmacher and
Ren Wang and
Ajaypal Singh
Scaling Up Clustered Network Appliances with ScaleBricks
Dong Zhou, Bin Fan, Hyeontaek Lim, David G. Andersen, Michael Kaminsky†,
Michael Mitzenmacher**, Ren Wang†, Ajaypal Singh‡
Carnegie Mellon University, †Intel Labs, **Harvard University, ‡Connectem, Inc.
ABSTRACT
This paper presents ScaleBricks, a new design for building
scalable, clustered network appliances that must “pin” ﬂow
state to a speciﬁc handling node without being able to choose
which node that should be. ScaleBricks applies a new, com-
pact lookup structure to route packets directly to the appro-
priate handling node, without incurring the cost of multiple
hops across the internal interconnect. Its lookup structure
is many times smaller than the alternative approach of fully
replicating a forwarding table onto all nodes. As a result,
ScaleBricks is able to improve throughput and latency while
simultaneously increasing the total number of ﬂows that can
be handled by such a cluster. This architecture is effective in
practice: Used to optimize packet forwarding in an existing
commercial LTE-to-Internet gateway, it increases the through-
put of a four-node cluster by 23%, reduces latency by up to
10%, saves memory, and stores up to 5.7x more entries in the
forwarding table.
CCS Concepts
•Networks → Middle boxes / network appliances;
Keywords
network function virtualization; scalability; hashing algo-
rithms
1.
INTRODUCTION
Many clustered network appliances require deterministic par-
titioning of a ﬂat key space among a cluster of machines.
When a packet enters the cluster, the ingress node will direct
the packet to its handling node. The handling node maintains
state that is used to process the packet, such as the packet’s
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the
Owner/Author.
Copyright is held by the owner/author(s).
SIGCOMM’15, August 17-21, 2015, London, United Kingdom
ACM 978-1-4503-3542-3/15/08.
http://dx.doi.org/10.1145/2785956.2787503
destination address or the ﬂow to which it belongs. Examples
include carrier-grade NATs, per-ﬂow switching in software-
deﬁned networks (SDNs), and, as we will discuss in the next
section, the cellular network-to-Internet gateway [1] in the
core network of Long-Term Evolution (LTE).
In this paper, we explore a less-examined aspect of scala-
bility for such clustered network appliances: can we create a
design in which the forwarding table (“FIB” or Forwarding
Information Base) that maps ﬂat keys to their corresponding
handling nodes “scales out” alongside throughput and port
count as one adds more nodes to the cluster? And, critically,
can we do so without increasing the amount of trafﬁc that
crosses the internal switching fabric? We ask this question
because in a typical design, such as RouteBricks [13], adding
another node to a cluster does not increase the total number
of keys that the cluster can support; it increases only the total
throughput and number of ports. In this paper, we explore a
design that allows the FIB to continue to scale through 8, 16,
or even 32 nodes, increasing the FIB capacity by up to 5.7x.
We focus on three properties of cluster scaling in this work:
Throughput Scaling. The aggregate throughput of the
cluster scales with the number of cluster servers;
FIB Scaling. The total size of the forwarding table (the
number of supported keys) scales with the number of servers;
and
Update Scaling. The maximum update rate of the FIB
scales with the number of servers.
In all cases, we do not want to scale at the expense of
incurring high latency or higher switching fabric cost. As we
discuss further in Section 3, existing designs do not satisfy
these goals. For example, the typical approach of duplicating
the FIB on all nodes fails to achieve FIB scaling; a distributed
hash design such as used in SEATTLE [22] requires multiple
hops across the fabric.
The contribution of this paper is two-fold. First, we present
the design, implementation, and theoretical underpinning of
an architecture called ScaleBricks that achieves these goals
(Section 3). The core of ScaleBricks is a new data structure
SetSep that represents the mapping from keys to nodes in
an extremely compact manner (Section 4). As a result, each
ingress node is able to forward packets directly to the appro-
priate handling node without needing a full copy of the FIB
at all nodes. This small global information table requires only
241O(logN) bits per key, where N is the number of nodes in the
cluster, enabling positive—though sublinear—FIB scaling for
realistically-sized clusters. We believe this data structure will
prove useful for other applications outside ScaleBricks.
Second, we use ScaleBricks to improve the performance of
a commercial cellular LTE-to-Internet gateway, described in
more detail in the following section. Our prototype shows that
a 4-node ScaleBricks cluster can nearly quadruple the number
of keys managed compared with single node solutions, while
simultaneously improving packet forwarding throughput by
approximately 23% and cutting latency up to 10% (Section 6).
2. DRIVING APPLICATION: CELLULAR
NETWORK-TO-INTERNET GATEWAY
To motivate ScaleBricks, we begin by introducing a concrete
application that can beneﬁt from ScaleBricks: the Internet
gateway used in LTE cellular networks. The central process-
ing component in LTE is termed the “Evolved Packet Core,”
or EPC [1]; Figure 1a shows a simpliﬁed view of the EPC
architecture. The following is a high-level description of how
it services mobile devices (“mobiles” from here on); more
details are described in the Internet draft on Service Function
Chaining Use Cases in Mobile Networks [18].
• When an application running on the mobile initiates a
connection, the controller assigns the new connection a
tunnel, called the GTP-U tunnel, and a unique Tunnel
End Point Identiﬁer (TEID).1
• Upstream trafﬁc (from the mobile to the Internet), sends
packets through several middleboxes to the LTE-to-
Internet gateway (the red box in the ﬁgures). After
performing administrative functions such as charging
and access control, the gateway decapsulates packets
from the GTP-U tunnel, updates the state associated
with the ﬂow, and sends them to ISP peering routers,
which connect to the Internet.
• Downstream trafﬁc follows a reverse path across the
elements. The LTE-to-Internet gateway processes and
re-encapsulates packets into the tunnels based on the
ﬂow’s TEID. The packets reach the correct base station,
which transmits them to the mobile.
In this paper, we focus our
improvements on a
commercially-available software EPC stack from Con-
nectem [10]. This system runs on commodity hardware and
aims to provide a cost-advantaged replacement for proprietary
hardware implementations of the EPC. It provides throughput
scalability by clustering multiple nodes: Figure 1b shows a
4-node EPC cluster. When a new connection is established,
the controller assigns a TEID to the ﬂow and assigns that
ﬂow to one node in the cluster (its handling node). This as-
signment is based on several LTE-speciﬁc constraints, such
1For clarity, we have used common terms for the components of the
network. Readers familiar with LTE terminology will recognize that our
“mobile device” is a “UE”; the base station is an “eNodeB”; and the tunnel
from the UE to the eNodeB is a “GTP-U” tunnel.
as geometric proximity (mobile devices from the same re-
gion are assigned to the same node), which prevents us from
modifying it (e.g., forcing hash-based assignment), thereby
requiring deterministic partitioning. It then inserts a mapping
from the 5-tuple ﬂow identiﬁer to the (handling node, TEID)
pair into the cluster forwarding table. Upstream packets from
this ﬂow are directed to the handling node by the aggregation
router. Downstream packets, however, could be received by
any node in the cluster because of limitations in the hard-
ware routers that are outside of our control. For example,
the deployment of an equal-cost multi-path routing (ECMP)
strategy may cause the scenario described above because all
nodes in the cluster will have the same distance to the des-
tination. Because the cluster maintains the state associated
with each ﬂow at its handling node, when the ingress node
receives a downstream packet, it must look up the handling
node and TEID in its forwarding table and forward the packet
appropriately. The handling node then processes the packet
and sends it back over the tunnel to the mobile.
Our goal in this paper is to demonstrate the effectiveness
of ScaleBricks by using it to improve the performance and
scalability of this software-based EPC stack. We chose this
application both because it is commercially important (hard-
ware EPC implementations can cost hundreds of thousands to
millions of dollars), is widely used, and represents an excel-
lent target for scaling using ScaleBricks because of its need
to pin ﬂows to a speciﬁc handling node combined with the
requirement of maintaining as little states at each node as
possible (which makes keeping a full per-ﬂow forwarding
table at each node a less viable option). ScaleBricks achieves
these goals without increasing the inter-cluster latency. Com-
pared with alternative designs, this latency reduction could
be important in several scenarios, including communication
between mobile devices and content delivery networks, as
well as other services deployed at edge servers. In this work,
we change only the “Packet Forwarding Engine” of the EPC;
this is the component that is responsible for directing packets
to their appropriate handling node. We leave unchanged the
“Data Plane Engine” that performs the core EPC functions.
3. DESIGN OVERVIEW
In this section, we explain the design choices for ScaleBricks
and compare those choices to representative alternative de-
signs to illustrate why we made those choices. We use the
following terms to describe the cluster architecture:
• Ingress Node: the node where a packet enters the cluster.
• Handling Node: the node where a packet is processed
within the cluster.
• Indirect Node: an intermediate node touched by a packet,
not including its ingress and handling node.
• Lookup Node: If node X stores the forwarding entry
associated with a packet P, X is P’s lookup node. A
packet may have no lookup nodes if the packet has an
242Figure 1: (a) Simpliﬁed Evolved Packet Core (EPC) architecture and (b) a 4-node EPC cluster
(a) RouteBricks
(b) Full Duplication
(c) Hash Partitioning
(d) ScaleBricks
Figure 2: Packet forwarding in different FIB architectures
unknown key, or more than one lookup node if the FIB
entry has been replicated to more than one node.
3.1 Cluster Architecture
Two topologies are classically used for building cluster-based
network functions. The ﬁrst connects cluster servers directly
to each other, as exempliﬁed by RouteBricks [13]. In such
systems, the servers are connected in a full mesh or a butterﬂy-
like topology, as shown in Figure 2a. On top of this topology,
load-balancing routing algorithms—e.g., Valiant Load Bal-
ancing (VLB) [32]—guarantee 100% throughput and fairness
without centralized scheduling.
This solution has the advantage that the total bandwidth of
internal links used to construct the full mesh or the butterﬂy
needs to be only 2× the total external bandwidth; further-
more, these links are fully utilized. The disadvantage of VLB,
however, is that the ingress node must forward each incoming
packet to an intermediate indirect node before it reaches the
handling node. This extra step ensures efﬁcient use of the
aggregate internal bandwidth. Unfortunately, in most cases,
each packet must be processed by three nodes (two hops).
This increases packet processing latency, server load, and
required internal link capacity.
The second class of topologies uses a hardware switch to
connect the cluster nodes (Figures 2b–2d). This topology
offers two attractive properties. First, it allows full utilization
of internal links without increasing the total internal trafﬁc.
To support R Gbps of external bandwidth, a node needs only
R Gbps of aggregate internal bandwidth, instead of the 2R
required by VLB. Second, without an indirect node, packet
latency depends on the hardware switch’s latency instead of
the indirect node. Compared to VLB, a switch-based topology
could reduce latency by 33%.
Interestingly, RouteBricks intentionally rejected this design
option. The authors argued that the cost of four 10 Gbps
switch ports was equal to the cost of one server, and hence
a switched cluster was more expensive than a server-based
cluster. Today, however, the economics of this argument
have changed. New vendors such as Mellanox offer much
cheaper hardware switches. For example, a Mellanox 36 port
40 GbE switch costs roughly $13,000, or ∼$9 / Gbps. This
Service Edge GatewayAggregationRoutersLTE-to-InternetGatewayControllerDatabaseISPPeeringRoutersInternetEPCBackhaulMobile DevicesBase StationsICInterconnect InterfaceSGiInternet InterfacePacket Forwarding EngineData Plane EngineSGiICS1-US1-UUE InterfacePacket Forwarding EngineData Plane EngineSGiICS1-UPacket Forwarding EngineData Plane EngineSGiICS1-UPacket Forwarding EngineData Plane EngineSGiICS1-UInternetClusterInterconnectBaseStationTunnelTunnelTunnelTunnel(a) Simpliﬁed EPC Architecture(b) A 4-node EPC ClusterDownlinkUplinkADCBFull FIBADCBFull FIBADCBPartial FIBADCBGPT + Partial FIB243is 80% lower than the number reported in the RouteBricks
paper. More importantly, hardware switches are particularly
suitable for building interconnections for the cluster nodes.
Their strengths—high bandwidth, low latency, simple and
clear topology—are well-suited to our requirements; and their
weakness—limited FIB size—is essentially irrelevant with
our approach.
ScaleBricks thus connects servers using a switch. This
topology reduces the internal bandwidth requirement and pro-
vides the opportunity to reduce the packet processing latency.
However, this choice also makes the design of a scalable
forwarding architecture challenging, as explained next.
3.2 FIB Architecture
Given a switch-based cluster topology, the next question is
what forwarding architecture to use. In the simplest design,
each node in the cluster stores a full copy of the entire for-
warding table (Figure 2b). When a packet arrives at its ingress