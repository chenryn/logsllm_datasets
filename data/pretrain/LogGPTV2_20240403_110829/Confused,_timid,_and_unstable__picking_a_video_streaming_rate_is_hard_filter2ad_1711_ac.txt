m
g
e
S
o
e
d
V
i
a
R
o
e
d
V
i
2000
1000
0
0
Video Rate
200
400
600
800
Time (s)
0
1000
Figure 20: Custom client, similar to Service A –
equally conservative, with a 10-sample moving aver-
age ﬁlter – displays the downward spiral.
Figure 21: Custom client – with 10% conservatism,
but with a 10-sample moving average ﬁlter.
a reasonable baseline. For Service A, Figure 9 indicates the
bandwidth below which the client picks a lower video rate.
Assume that Service A estimates bandwidth by simply di-
viding the download size by the download time and passing
it through a ﬁxed-size moving-average ﬁlter. We can esti-
mate the size of the ﬁlter by measuring how long it takes
from when the bandwidth drops until the client picks a new
rate. A number of traces from Service A suggest a ﬁlter with
10 samples, though the true algorithm is probably more nu-
anced.
To closely mimic the Service A client, our custom client re-
quests the video segments with the same sizes from the same
locations in the CDN: we capture the segment map given to
the client after authentication, which locates the video seg-
ments for each supported playback rate. Hence, our custom
client will experience the same segment-size variation over
the course of the movie, and when it shifts playback rate,
the segment size will change as well. Since our custom client
uses tokens from an earlier playback, the CDN cannot tell
the diﬀerence between our custom client and the real Service
A client. To further match Service A, the playback buﬀer
is set to 240 seconds, the client uses a single persistent con-
nection to the server, and it pauses when the buﬀer is full.
We ﬁrst validate the client, then consider three changes: (1)
being less conservative, (2) changing the ﬁltering method,
and (3) aggregating segments.
6.2 Validating our Custom Client
Figure 20 shows the custom client in action. After down-
loading each segment, the custom client selects the playback
rate based on Service A’s conservative rate selection algo-
rithm, observed in Figure 9. Once the playback buﬀer is
full, we introduce a competing ﬂow. Like the real client,
the playback rate drops suddenly when the competing ﬂow
starts, then ﬂuctuates over the course of the movie. The
downward spiral does not bottom out, which we suspect is
due to some subtle diﬀerences between Service A’s algorithm
and ours.
5000
4000
)
s
/
b
k
(
3000
t
e
a
R
o
e
d
V
i
2000
1000
0
0
Buffer Status
Video Throughput
Video Rate
250
200
150
100
50
200
400
600
800
Time (s)
0
1000
)
d
n
o
c
e
S
(
r
e
f
f
u
B
n
i
s
t
n
e
m
g
e
S
o
e
d
V
i
Figure 22: Custom client – with 10% conservatism,
and with an 80th-percentile ﬁlter.
6.3 Less Conservative
Bandwidth estimates based on download sizes and dura-
tions tend to under-report the available bandwidth, espe-
cially in the presence of a competing ﬂow. If the algorithm
is conservative, it exacerbates the problem. We try a less
conservative algorithm, with a conservatism of 10% instead
of 40%. Conservatism of 40% means the client requests for
a video rate of at most 1.2Mb/s when it perceives 2.0Mb/s,
while 10% means it requests at most 1.8Mb/s when per-
ceiving 2.0Mb/s. According to Figure 9, Service A requests
video rate with a conservatism of approximately 40%. Fig-
ure 21 shows that the video rate is higher, even though
the playback buﬀer stays full. The result is higher qual-
ity video, high playback buﬀer occupancy (i.e.
resilience
against rebuﬀering) and four minutes of buﬀering to respond
to changes in bandwidth. Note that even though the algo-
rithm is less conservative, the underlying TCP ensures the
algorithm stays a “good citizen” and only gets its fair share
of available bandwidth.
2355000
4000
3000
)
s
/
b
k
(
e
t
a
R
o
e
d
V
i
2000
1000
0
0
Buffer Status
Video Throughput
Video Rate
250
200
150
100
50
200
400
600
800
Time (s)
0
1000
)
d
n
o
c
e
S
(
r
e
f
f
u
B
n
i
s
t
n
e
m
g
e
S
o
e
d
V
i
Figure 23: Custom client with increased segment
size (5x).
6.4 Better Filtering
Averaging ﬁlters provide a more stable estimate of band-
width, but a single outlier can confuse the algorithm. For
example, a few seconds of low-information movie credits re-
duces the segment size and the algorithm might drop the
rate. In place of averages, we consider medians and quan-
tiles to reduce the vulnerability to outliers. Figure 22 shows
what happens if we use the 80th-percentile of measured rate
of the past ten segment download. Variation is greatly re-
duced, and the majority of the movie plays at the highest-
available rate. The playback buﬀer has small ﬂuctuations,
but it is still far from a rebuﬀer event.
6.5 Bigger Segments
As noted earlier, bigger segments provide better estimates
of the available bandwidth, allowing TCP to escape slow-
start. Figure 23 shows what happens if our client aggregates
ﬁve requests into one. With the larger segment size, the
video throughput is more stable, and both the playback rate
and buﬀer size are more stable.
In summary, larger segments let TCP reach its fair share
and improve the video throughput. Picking higher rates
less conservatively and ﬁltering measurements more care-
fully can improve video quality. But we should note that
these improvements are for one movie on one service. Given
the prevalence of the downward spiral eﬀect, these should
not be interpreted as hard recommendations, merely as added
detail to our understanding of the problem.
7. RELATED WORK
The related work largely considers three overlapping ar-
eas: systems for video streaming; measurements to under-
stand their performance, and the design and analysis of rate
selection algorithms.
Video Streaming Services. The ﬁrst category covers
video streaming approaches using HTTP, such as the com-
mercial ones from Adobe, Apple, and Microsoft described
in [22], which diﬀer in their alignment of video switching
rates, whether A/V streams are combined, and whether re-
quests are issued as byte ranges or for pre-speciﬁed seg-
ments. A more recent technique is MPEG DASH (Dynamic
Adaptive Streaming over HTTP) [7] which standardizes the
formatting of video content and leaves open the speciﬁc
client player algorithm. These techniques underpin the ma-
jor commercial services like YouTube, Netﬂix, and Hulu.
Video Streaming Measurement. The second cate-
gory measures the performance of individual video streaming
clients experiencing local traﬃc conditions (“in the lab”), all
the way to distributed measurement systems that compare
the performance of thousands of clients (“in the wild”).
The work most similar to ours is [3], where the authors
also parse HTTP messages to determine playback rates and
use a bandwidth limiter to test clients under varying network
conditions. However, [3] focuses on the unfairness problem
among two video players, while in this work we focus on
the unfairness problem between a video player and a long-
lived TCP ﬂow. This paper considers a signiﬁcantly diﬀer-
ent scenario: it focuses on a video client competing against
another video client. In this context, they observe similar
pathologies: poor bandwidth estimation, leading to insta-
bility. However, they explain their observations entirely in
terms of the application-layer ON-OFF behavior of video
clients; even if one video client perfectly obtained its fair
share when ON, it can fail to correctly estimate available
bandwidth (depending on the amount of overlap with the
ON periods of the other client). By contrast, our paper
demonstrates that this is only a symptom of a more gen-
eral problem: inaccurate bandwidth estimation occurs even
when the competing ﬂow does not exhibit ON-OFF behav-
ior. As we show in this paper, the problem arises because
it is hard to estimate bandwidth above TCP. Others have
identiﬁed the same problem but not explained its causes or
validated potential ﬁxes [4, 16].
Measuring the CDN servers rather than clients provides
diﬀerent insights. In [1], the authors examine the CDN se-
lection strategy of Hulu, while in [2], the authors look at
Netﬂix. Both papers ﬁnd a predisposition for clients to stay
with the original CDN, despite variation between CDNs and
over time. In [9], the authors describe lessons learned from a
distributed commercial measurement system to understand
the eﬀects of Quality-of-Experience (QoE) metrics on viewer
engagement and retention. Rebuﬀer rates and average video
quality are QoE metrics with measurable impacts on viewer
engagement, which underscores the importance of getting
rate measurement and selection right in the presence of com-
peting ﬂows. With the measurement-driven insights from
the same system, [14] proposes a global video control plane
to dynamically assign clients a choice of video rate and CDN
that optimizes viewers’ experience.
Other work looks at network characteristics of video stream-
ing traﬃc, rather than focusing on the client or viewer expe-
riences [11, 19, 24]. In particular, the authors in [19] show
ON-OFF cycle behavior for YouTube and Netﬂix and use a
model to study aggregates of video client and their eﬀects
on the network. Both CDN and network traﬃc papers do
not consider local eﬀects on measured bandwidth or their
eﬀects on rate stability.
Rate Selection Algorithms. The third category is
work on rate selection algorithms. This work complements
ours, as a control system always beneﬁts from more accu-
rate measurements. In [8], the authors propose an algorithm
to maintain the playout buﬀer at a target level.
In [17],
the authors implement a diﬀerent buﬀer-aware rate selection
algorithm and experimentally measure user preferences for
gradual and infrequent playback rate changes. In [23], the
236authors model the rate selection problem as a Markov Deci-
sion Process and use a dynamic programming technique to
choose a streaming strategy that improves QoE. In [13], the
authors use simulations to show how parallel HTTP sessions
can improve playback quality. Server-side pacing is another
approach to selecting rate used by YouTube, as described
in [10, 12].
8. CONCLUSION
Despite some diﬀerences in speciﬁc service implementa-
tions, all three services we study display degraded perfor-
mance in the presence of competing traﬃc, well below the
video quality possible if the client used its fair share of band-
width. At a high level, our measurement analysis and ex-
periments suggest that the root cause of this failure is a
lack of information. The HTTP layer is simply not privy to
continuous high-ﬁdelity feedback about the fair share at the
bottleneck link.
There are two ways to interpret our observations. On one
hand, we observe that determining the fair share of band-
width available at the bottleneck is precisely the role of TCP.
Thus, one path forward might be to suggest that we should
design the client to improve information ﬂow from TCP to
the HTTP layer. In particular, we should ensure that TCP
has a chance to reach its steady-state fair share; for example,
increasing the segment size enables this eﬀect.
However, we believe there may be a more radical solu-
tion: do not attempt to estimate bandwidth at all! The video
streaming client has two competing goals: attain the highest
bitrate possible while avoiding buﬀer underruns. Thus the
objective is not to ensure the buﬀer stays full; the objective
is to ensure the buﬀer does not go empty. Since the buﬀer
holds several minutes of video, this shift in perspective sug-
gests that if the buﬀer is full then the client has picked a rate
that is too low. Rather, the client should increase the bitrate
when the buﬀer is high and decrease it when the buﬀer falls
low. Though this sounds aggressive, note that it is exactly
the correct layer separation:
it hands oﬀ to TCP the ob-
jective of obtaining the fair share of bandwidth, and tries
to always ensure the client picks the highest rate possible.
This suggests an intriguing path forward for future research:
design video-streaming clients that deliver high performance
by eliminating bandwidth estimation all together.
Acknowledgment
We are grateful to the anonymous reviewers and our shep-
herd Nina Taft for their valuable comments and feedback,
which helped improve the ﬁnal version. The authors would
also like to thank Kok-Kiong Yap, Masayoshi Kobayashi,
Vimalkumar Jeyakumar, Yiannis Yiakoumis and Netﬂix en-
gineers for helpful discussions that shaped the paper. This
work was supported by Mr. and Mrs. Chun Chiu Stanford
Graduate Fellowship, Hewlett-Packard Fellowship, the Stan-
ford Clean Slate Program, and the National Science Foun-
dation under grants CNS-0904609, CNS-0644114, and CNS-
0832820.
9. REFERENCES
[1] V. Adhikari, Y. Guo, F. Hao, V. Hilt, and Z.-L. Zhang.
A Tale of Three CDNs: An Active Measurement
Study of Hulu and its CDNs. In Proceedings of IEEE
Conference on Computer Communications Workshops
(INFOCOM WKSHPS), pages 7–12, March 2012.
[2] V. K. Adhikari, Y. Guo, F. Hao, M. Varvello, V. Hilt,
M. Steiner, and Z.-L. Zhang. Unreeling Netﬂix:
Understanding and Improving Multi-CDN Movie
Delivery. In Proceedings of the IEEE INFOCOM 2012,
Orlando, FL, USA, pages 1620–1628, March 2012.
[3] S. Akhshabi, L. Anantakrishnan, C. Dovrolis, and
A. Begen. What Happens When HTTP Adaptive
Streaming Players Compete for Bandwidth? In
Proceedings of the ACM Workshop on Network and
Operating Systems Support for Digital Audio and
Video (NOSSDAV), June 2012.
[4] S. Akhshabi, C. Dovrolis, and A. Begen. An
Experimental Evaluation of Rate Adaptation
Algorithms in Adaptive Streaming over HTTP. In
Proceedings of the ACM Multimedia Systems
Conference (MMSys), San Jose, CA, USA, Feburary
2011.
[5] M. Allman, V. Paxson, and E. Blanton. TCP
Congestion Control. RFC 5681 (Draft Standard),
Sept. 2009.
[6] M. Allman, V. Paxson, and W. Stevens. TCP
Congestion Control. RFC 2581 (Proposed Standard),
Apr. 1999. Obsoleted by RFC 5681, updated by RFC
3390.
[7] MPEG DASH speciﬁcation (ISO/IEC DIS 23009-1.2),
2011.
[8] L. De Cicco, S. Mascolo, and V. Palmisano. Feedback
Control for Adaptive Live Video Streaming. In
Proceedings of the ACM Multimedia Systems
Conference (MMSys), Febrary 2011.
[9] F. Dobrian, A. Awan, D. Joseph, A. Ganjam, J. Zhan,
V. Sekar, I. Stoica, and H. Zhang. Understanding the
Impact of Video Quality on User Engagement. In
Proceedings of the ACM SIGCOMM, Toronto,
Canada, August 2011.
[10] M. Ghobadi, Y. Cheng, A. Jain, and M. Mathis.
Trickle: Rate Limiting YouTube Video Streaming. In
Proceedings of the USENIX Annual Technical
Conference (ATC), page 6, 2012.
[11] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. Youtube
Traﬃc Characterization: A View From the Edge. In
Proceedings of the ACM SIGCOMM conference on
Internet Measurement (IMC), pages 15–28, 2007.
[12] L. Kontothanassis. Content Delivery Considerations
for Diﬀerent Types of Internet Video. In Proceedings
of the ACM Multimedia Systems Conference (MMSys)
– Keynote, Chapel Hill, NC, USA, Febrary 2012.
[13] C. Liu, I. Bouazizi, and M. Gabbouj. Parallel
Adaptive HTTP Media Streaming. In Proceedings of
the IEEE International Conference on Computer
Communications and Networks (ICCCN), pages 1–6,
2011.
[14] X. Liu, F. Dobrian, H. Milner, J. Jiang, V. Sekar,
I. Stoica, and H. Zhang. A Case for a Coordinated
Internet Video Control Plane. In Proceedings of the
ACM SIGCOMM, Helsinki, Finland, August 2012.
[15] J. W. Lockwood, N. McKeown, G. Watson, G. Gibb,
P. Hartke, J. Naous, R. Raghuraman, and J. Luo.
NetFPGA–An Open Platform for Gigabit-Rate
Network Switching and Routing. In MSE ’07:
237Proceedings of the 2007 IEEE International
Conference on Microelectronic Systems Education,
pages 160–161, 2007.
[16] K. Miller, E. Quacchio, G. Gennari, and A. Wolisz.
Adaptation algorithm for adaptive streaming over
HTTP. In Proceedings of the IEEE International
Packet Video Workshop (PV), pages 173–178, May
2012.
[17] R. Mok, X. Luo, E. Chan, and R. Chang. QDASH: a
QoE-aware DASH system. In Proceedings of the ACM
Multimedia Systems Conference (MMSys), pages
11–22, Febrary 2012.
[18] Sandvine: Global Internet Phenomena Report. http:
//www.sandvine.com/news/pr_detail.asp?ID=312.
[19] A. Rao, A. Legout, Y. Lim, D. Towsley, C. Barakat,
and W. Dabbous. Network characteristics of video
streaming traﬃc. In Proceedings of the ACM
COnference on emerging Networking EXperiments and
Technologies (CONEXT), page 25. ACM, 2011.
[20] RTMPDump. http://rtmpdump.mplayerhq.hu/.
[21] Consumer Report: Streaming Video Services Rating.
http://www.consumerreports.org/cro/magazine/
2012/09/best-streaming-video-services/.
[22] M. Watson. HTTP Adaptive Streaming in Practice. In
Proceedings of the ACM Multimedia Systems
Conference (MMSys) – Keynote, San Jose, CA, USA,
Febrary 2011.
[23] S. Xiang, L. Cai, and J. Pan. Adaptive Scalable Video
Streaming in Wireless Networks. In Proceedings of the
ACM Multimedia Systems Conference (MMSys), pages
167–172, Febrary 2012.
[24] M. Zink, K. Suh, Y. Gu, and J. Kurose.
Characteristics of YouTube network traﬃc at a
campus network - Measurements, models, and
implications. In Computer Networks, Volume 53, Issue
4, pages 501–514. Elsevier, 2009.
238