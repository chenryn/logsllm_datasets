title:Why Do Adversarial Attacks Transfer? Explaining Transferability of
Evasion and Poisoning Attacks
author:Ambra Demontis and
Marco Melis and
Maura Pintor and
Matthew Jagielski and
Battista Biggio and
Alina Oprea and
Cristina Nita-Rotaru and
Fabio Roli
Why Do Adversarial Attacks Transfer? Explaining 
Transferability of Evasion and Poisoning Attacks
Ambra Demontis, Marco Melis, and Maura Pintor, University of Cagliari, Italy;  
Matthew Jagielski, Northeastern University; Battista Biggio, University of Cagliari, Italy,  
and Pluribus One; Alina Oprea and Cristina Nita-Rotaru, Northeastern University;  
Fabio Roli, University of Cagliari, Italy, and Pluribus One
https://www.usenix.org/conference/usenixsecurity19/presentation/demontis
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and
Poisoning Attacks
Ambra Demontis†, Marco Melis†, Maura Pintor†, Matthew Jagielski*, Battista Biggio†,‡, Alina Oprea*,
Cristina Nita-Rotaru*, and Fabio Roli†,‡
†Department of Electrical and Electronic Engineering, University of Cagliari, Italy
‡Pluribus One, Italy
*Northeastern University, Boston, MA, USA
Abstract
Transferability captures the ability of an attack against a
machine-learning model to be effective against a different,
potentially unknown, model. Empirical evidence for transfer-
ability has been shown in previous work, but the underlying
reasons why an attack transfers or not are not yet well un-
derstood. In this paper, we present a comprehensive analysis
aimed to investigate the transferability of both test-time eva-
sion and training-time poisoning attacks. We provide a unify-
ing optimization framework for evasion and poisoning attacks,
and a formal deﬁnition of transferability of such attacks. We
highlight two main factors contributing to attack transferabil-
ity: the intrinsic adversarial vulnerability of the target model,
and the complexity of the surrogate model used to optimize
the attack. Based on these insights, we deﬁne three metrics
that impact an attack’s transferability. Interestingly, our results
derived from theoretical analysis hold for both evasion and
poisoning attacks, and are conﬁrmed experimentally using a
wide range of linear and non-linear classiﬁers and datasets.
1 Introduction
The wide adoption of machine learning (ML) and deep learn-
ing algorithms in many critical applications introduces strong
incentives for motivated adversaries to manipulate the results
and models generated by these algorithms. Attacks against
machine learning systems can happen during multiple stages
in the learning pipeline. For instance, in many settings training
data is collected online and thus can not be fully trusted. In
poisoning availability attacks, the attacker controls a certain
amount of training data, thus inﬂuencing the trained model
and ultimately the predictions at testing time on most points in
testing set [4,18,20,28–30,34,36,41,48]. Poisoning integrity
attacks have the goal of modifying predictions on a few tar-
geted points by manipulating the training process [20,41]. On
the other hand, evasion attacks involve small manipulations
of testing data points that results in misprediction at testing
time on those points [3, 8, 10, 14, 32, 38, 42, 45, 49].
Creating poisoning and evasion attack points is not a trivial
task, particularly when many online services avoid disclos-
ing information about their machine learning algorithms. As
a result, attackers are forced to craft their attacks in black-
box settings, against a surrogate model instead of the real
model used by the service, hoping that the attack will be ef-
fective on the real model. The transferability property of an
attack is satisﬁed when an attack developed for a particular
machine learning model (i.e., a surrogate model) is also ef-
fective against the target model. Attack transferability was
observed in early studies on adversarial examples [14,42] and
has gained a lot more interest in recent years with the advance-
ment of machine learning cloud services. Previous work has
reported empirical ﬁndings about the transferability of evasion
attacks [3, 13, 14, 21, 26, 32, 33, 42, 43, 47] and, only recently,
also on the transferability of poisoning integrity attacks [41].
In spite of these efforts, the question of when and why do
adversarial points transfer remains largely unanswered.
In this paper we present the ﬁrst comprehensive evaluation
of transferability of evasion and poisoning availability attacks,
understanding the factors contributing to transferability of
both attacks. In particular, we consider attacks crafted with
gradient-based optimization techniques (e.g., [4, 8, 23]), a
popular and successful mechanism used to create attack data
points. We unify for the ﬁrst time evasion and poisoning at-
tacks into an optimization framework that can be instantiated
for a range of threat models and adversarial constraints. We
provide a formal deﬁnition of transferability and show that,
under linearization of the loss function computed under attack,
several main factors impact transferability: the intrinsic ad-
versarial vulnerability of the target model, the complexity of
the surrogate model used to optimize the attacks, and its align-
ment with the target model. Furthermore, we derive a new
poisoning attack for logistic regression, and perform a com-
prehensive evaluation of both evasion and poisoning attacks
on multiple datasets, conﬁrming our theoretical analysis.
In more detail, the contributions of our work are:
Optimization framework for evasion and poisoning at-
tacks. We introduce a unifying framework based on gradient-
USENIX Association
28th USENIX Security Symposium    321
descent optimization that encompasses both evasion and poi-
soning attacks. Our framework supports threat models with
different adversarial goals (integrity and availability), amount
of knowledge available to the adversary (white-box and black-
box), as well as different adversarial capabilities (causative
or exploratory). Our framework generalizes existing attacks
proposed by previous work for evasion [3, 8, 14, 23, 42] and
poisoning [4, 18, 20, 24, 27, 48]. Under our framework, we
derive a novel gradient-based poisoning availability attack
against logistic regression. We remark here that poisoning
attacks are more difﬁcult to derive than evasion ones, as they
require computing hypergradients from a bilevel optimization
problem, to capture the dependency on how the machine-
learning model changes while the training poisoning points
are modiﬁed [4, 18, 20, 24, 27, 48].
Transferability deﬁnition and theoretical bound. We give
a formal deﬁnition of transferability of evasion and poisoning
attacks, and an upper bound on a transfer attack’s success.
This allows us to derive three metrics connected to model
complexity. Our formal deﬁnition unveils that transferabil-
ity depends on: (1) the size of input gradients of the target
classiﬁer; (2) how well the gradients of the surrogate and
target models align; and (3) the variance of the loss landscape
optimized to generate the attack points.
Comprehensive experimental evaluation of transferabil-
ity. We consider a wide range of classiﬁers, including logistic
regression, SVMs with both linear and RBF kernels, ridge
regression, random forests, and deep neural networks (both
feed-forward and convolutional neural networks), all with
different hyperparameter settings to reﬂect different model
complexities. We evaluate the transferability of our attacks
on three datasets related to different applications: handwrit-
ten digit recognition (MNIST), Android malware detection
(DREBIN), and face recognition (LFW). We conﬁrm our
theoretical analysis for both evasion and poisoning attacks.
Insights into transferability. We demonstrate that attack
transferability depends strongly on the complexity of the tar-
get model, i.e., on its inherent vulnerability. This conﬁrms that
reducing the size of input gradients, e.g., via regularization,
may allow us to learn more robust classiﬁers not only against
evasion [22, 35, 39, 44] but also against poisoning availability
attacks. Second, transferability is also impacted by the sur-
rogate model’s alignment with the target model. Surrogates
with better alignments to their targets (in terms of the angle
between their gradients) are more successful at transferring
the attack points. Third, surrogate loss functions that are sta-
bler and have lower variance tend to facilitate gradient-based
optimization attacks to ﬁnd better local optima (see Figure 1).
As less complex models exhibit a lower variance of their loss
function, they typically result in better surrogates.
Organization. We discuss background on threat modeling
against machine learning in Section 2. We introduce our unify-
ing optimization framework for evasion and poisoning attacks,
Figure 1: Conceptual representation of transferability. We
show the loss function of the attack objective as a function of
a single feature x. The top row includes 2 surrogate models
(high and low complexity), while the bottom row includes
both models as targets. The adversarial samples are repre-
sented as red dots for the high-complexity surrogate and as
blue dots for the low-complexity surrogate. If the adversar-
ial sample loss is below a certain threshold (i.e., the black
horizontal line), the point is correctly classiﬁed, otherwise it
is misclassiﬁed. The adversarial point computed against the
high-complexity model (top left) lays in a local optimum due
to the irregularity of the objective. This point is not effective
even against the same classiﬁer trained on a different dataset
(bottom left) due to the variance of the high-complexity classi-
ﬁer. The adversarial point computed against the low complex-
ity model (top right), instead, succeeds against both low and
high-complexity targets (left and right bottom, respectively).
as well as the poisoning attack for logistic regression in Sec-
tion 3. We then formally deﬁne transferability for both evasion
and poisoning attacks, and show its approximate connection
with the input gradients used to craft the corresponding attack
samples (Section 4). Experiments are reported in Section 5,
highlighting connections among regularization hyperparame-
ters, the size of input gradients, and transferability of attacks,
on different case studies involving handwritten digit recog-
nition, Android malware detection, and face recognition. We
discuss related work in Section 6 and conclude in Section 7.
2 Background and Threat Model
Supervised learning includes: (1) a training phase in which
training data is given as input to a learning algorithm, result-
ing in a trained ML model; (2) a testing phase in which the
model is applied to new data and a prediction is generated. In
this paper, we consider a range of adversarial models against
machine learning classiﬁers at both training and testing time.
Attackers are deﬁned by: (i) their goal or objective in attack-
ing the system; (ii) their knowledge of the system; (iii) their
capabilities in inﬂuencing the system through manipulation
322    28th USENIX Security Symposium
USENIX Association
xLossHigh-complexity SurrogatexLossLow-complexity SurrogatexLossHigh-complexity Target xLossLow-complexity Targetof the input data. Before we detail each of these, we introduce
our notation, and point out that the threat model and attacks
considered in this work are suited to binary classiﬁcation, but
can be extended to multi-class settings.
Notation. We denote the sample and label spaces with X
and Y ∈ {−1, +1}, respectively, and the training data with
D = (xi,yi)n
i=1, where n is the training set size. We use