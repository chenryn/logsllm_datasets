width of the thttpd web server [31] serving static HTML
pages. We con(cid:2)gured ApacheBench to make 5000
requests using 25 simultaneous connections. Figure 2
shows the results of both the original SVA kernel and
the SVA kernel with the new run-time checks described
in Section 5. Each bar is the average bandwidth of 3
runs of the experiment; the results are normalized to the
Performance First,
we
)
2
2
.
4
.
2
x
u
n
i
L
o
t
d
e
z
i
l
a
m
r
o
N
(
h
t
d
i
w
d
n
a
B
 1
 0.8
 0.6
 0.4
 0.2
 0
SVA Checks
SVA-OS Checks
1
2
4
32
16
8
File Size (KB)
64 128 256 512 1024
Figure 2: Web Server Bandwidth (Linux/i386 = 1.0)
)
2
2
.
4
2
.
x
u
n
i
L
o
t
d
e
z
i
l
a
m
r
o
N
(
h
t
d
i
w
d
n
a
B
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
SVA Checks
SVA-OS Checks
8
32
128
File Size (MB)
512
Figure 3: SSH Server Bandwidth (Linux/i386 = 1.0)
original i386 Linux kernel. For small (cid:2)les (1 KB - 32
KB) in which the original SVA system adds signi(cid:2)cant
overhead, our new run-time checks incur a small amount
of additional overhead (roughly a 9% decrease in
bandwidth relative to the SVA kernel). However, for
larger (cid:2)le sizes (64 KB or more), the SVA-OS checks
add negligible overhead to the original SVA system.
We also measured the performance of sshd, a login
server offering encrypted (cid:2)le transfer. For this test, we
measured the bandwidth of transferring several large (cid:2)les
from the server to our test client; the results are shown in
Figure 3. For each (cid:2)le size, we (cid:2)rst did a priming run
to bring (cid:2)le system data into the kernelâ€™s buffer cache;
subsequently, we transfered the (cid:2)le three times. Figure 3
shows the mean of the receive bandwidth of the three
runs normalized to the mean receive bandwidth mea-
Benchmark
i386 (s)
SVA (s)
SVA-OS (s)
bzip2
lame
perl
18.7 (0.47)
133.3 (3.3)
22.3 (0.47)
18.3 (0.47)
132 (0.82)
22.3 (0.47)
18.0 (0.00)
126.0 (0.82)
22.3 (0.47)
% Increase from
i386 to SVA-OS
0.0%
-0.1%
0.0%
Description
Compressing 64 MB (cid:2)le
Converting 206 MB WAV (cid:2)le to MP3
Interpreting scrabbl.pl from SPEC 2000
Table 3: Latency of Applications. Standard Deviation Shown in Parentheses.
i386
SVA Checks
SVA-OS Checks
percent overhead that the applications experienced exe-
cuting on the SVA-OS kernel relative to the original i386
Linux kernel. The results show that our system adds vir-
tually no overhead for these applications, even though
some of the programs (bzip2 and lame) perform substan-
tial amounts of I/O. Table 4 shows the latency of the ap-
plications during their priming runs; our kernel shows no
overhead even when the kernel must initiate I/O to re-
trieve data off the physical disk.
)
s
/
B
M
(
h
t
d
i
w
d
n
a
B
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
4
8
16
64 128 256 512 102420484096
32
File Size (KB)
Figure 4: File System Bandwidth
Benchmark
bzip2
lame
perl
i386 (s)
41
203
24
SVA (s)
40
202
23
SVA-OS (s)
40
202
23
Table 4: Latency of Applications During Priming Run.
sured on the original i386 kernel; note that the units on
the X-axis are MB. Our results indicate that there is no
signi(cid:2)cant decrease in bandwidth due to the extra run-
time checks added by the original SVA system or the new
run-time checks presented in this paper. This outcome is
far better than thttpd, most likely due to the large (cid:2)le
sizes we transfered via scp. For large (cid:2)le sizes, the net-
work becomes the bottleneck: transferring an 8 MB (cid:2)le
takes 62.5 ms on a Gigabit network, but the overheads for
basic system calls (shown in Table 5) show overheads of
only tens of microseconds.
To see what effect our system would have on end-
user application performance, we ran experiments on the
client-side programs listed in Table 3. We tested bzip2
compressing a 64 MB (cid:2)le, the LAME MP3 encoder con-
verting a 206 MB (cid:2)le from WAV to MP3 format, and the
perl interpreter running the training input from the SPEC
2000 benchmark suite. For each test, we ran the program
once to prime any caches within the operating system and
then ran each program three times. Table 3 shows the av-
erage of the execution times of the three runs and the
Microbenchmark Performance To better understand
the different performance behaviors of the applications,
we used microbenchmarks to measure the overhead our
system introduces for primitive kernel operations. For
these experiments, we con(cid:2)gured HBench-OS to run
each test 50 times.
Our results for basic system calls (Table 5) indicate
that the original SVA system adds signi(cid:2)cant overhead
(on the order of tens of microseconds) to individual sys-
tem calls. However, the results also show that our new
safety checks only add a small amount of additional over-
head (25% or less) to the original SVA system.
We also tested the (cid:2)le system bandwidth, shown in
Figure 4. The results show that the original SVA system
reduces (cid:2)le system bandwidth by about 5-20% for small
(cid:2)les but that the overhead for larger (cid:2)les is negligible.
Again, however, the additional checks for low-level ker-
nel operations add no overhead.
The microbenchmark results provide a partial explana-
tion for the application performance results. The appli-
cations in Table 3 experience no overhead because they
perform most of their processing in user-space; the over-
head of the kernel does not affect them much. In contrast,
the sshd and thttpd servers spend most of their time ex-
ecuting in the kernel (primarily in the poll(), select(), and
write() system calls). For the system calls that we tested,
our new safety checks add less than several microseconds
of overhead (as shown in Table 5). For a small network
transfer of 1 KB (which takes less than 8 (cid:22)s on a Giga-
bit network), such an overhead can affect performance.
However, for larger (cid:2)les sizes (e.g., an 8 MB transfer that
takes 62.5 ms), this overhead becomes negligible. This
effect shows up in our results for networked applications
(thttpd and sshd): smaller (cid:2)le transfers see signi(cid:2)cant
overhead, but past a certain (cid:2)le size, the overhead from
the run-time safety checks becomes negligible.
Benchmark
i386 ((cid:22)s)
SVA ((cid:22)s)
getpid
openclose
write
signal handler
signal install
pipe latency
0.16 (0.001)
1.10 (0.009)
0.25 (0.001)
1.59 (0.006)
0.34 (0.001)
2.74 (0.014)
0.37 (0.000)
11.1 (0.027)
1.87 (0.012)
6.88 (0.044)
1.56 (0.019)
30.5 (0.188)
SVA-OS
((cid:22)s)
0.37 (0.006)
12.1 (0.076)
1.86 (0.010)
8.49 (0.074)
1.95 (0.007)
35.9 (0.267)
from
% Increase
SVA to SVA-OS
0.0%
9.0%
-0.4%
23%
25%
18%
poll
select
1.16 (0.043)
6.47 (0.080)
7.03 (0.014)
1.00 (0.019)
8.18 (0.133)
8.81 (0.020)
8.7%
7.7%
Description
Latency of getpid() syscall
Latency of opening and closing a (cid:2)le
Latency of writing a single byte to /dev/null
Latency of calling a signal handler
Latency of installing a signal handler
Latency of ping-ponging one byte message be-
tween two processes
Latency of polling both ends of a pipe for reading
and writing. Data is always available for reading.
Latency of testing both ends of a pipe for reading
and writing. Data is always available for reading.
Table 5: Latency of Kernel Operations. Standard Deviation Shown in Parentheses.
8 Related Work
Previous work has explored several approaches to pro-
viding greater safety and reliability for operating sys-
tem kernels. Some require complete OS re-design, e.g.,
capability-based operating systems [37, 38] and micro-
kernels [1, 25]. Others use isolation (or (cid:147)sandboxing(cid:148))
techniques, including device driver isolation within the
OS [35, 44, 45, 51] or the hypervisor [17]. While ef-
fective at increasing system reliability, none of these ap-