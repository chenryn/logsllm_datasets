(20% of the queries observed), the average accuracy is 55%.
Then, the base algorithm can be successful only if it has
enough known queries. When the vocabulary is bigger, the
accuracy decreases if the number of known queries remain
the same. The accuracy increases in function of the number
of known queries (Figure 2), we assume that we could obtain
better result for big vocabularies with more known queries.
Obtaining so many queries is unrealistic since we would need
a preliminary inference attack or a massive injection attack
to obtain the knowledge required by this base similar-data
attack. Therefore, the base score attack is only a practical
attack when the size of the server vocabulary is below 2000.
In Figure 2, we can distinguish one surprising result when
there are 60 known queries and the vocabulary size is 500.
This result is decreased compared to the previous one (i.e. 30
known queries) and the errors bars are overlapping. We can
explain that because, in this experiment, there are 75 queries
for 60 known queries, i.e. only 15 unknown queries. It is the
only experiment where there is a minority of unknown queries.
We consider this result as insigniﬁcant but keep it in our work
for the sake of the complete discussion. Indeed, it does not
make sense to know most of the queries before the attack and
to consider a result obtained on the recovery of such a small
amount of unknown queries.
148    30th USENIX Security Symposium
USENIX Association
Figure 2: Average accuracy of the base matching algo-
rithm on Enron for vocabulary size. Parameters: |Dsim| =
12K,|Dreal| = 18K,|Q | = 0.15· mreal
Figure 3: Average execution time of the matching algo-
rithm for vocabulary size Parameters: |Dsim| = 12K,|Dreal| =
18K,|Q | = 0.15· mreal,|KnownQ | = 10
In [3], it was assumed that the co-occurrences were too
noisy to rely totally on them. The occurrence is much less
noisy but most of the keywords have an extremely close fre-
quency therefore it is impossible to identify a keyword just
based on a single occurrence estimator except if we are sure
to have perfect knowledge as in CGPR attack when they know
nearly 100% of the encrypted documents. The co-occurrence
is noisier but its distribution is scattered enough to identify
keyword-trapdoor pairs. The lack of precision is balanced
by the number of co-occurrences available to perform the
identiﬁcation. There is a trade-off between the number of
estimators and the precision.
4.3 Execution time
The complexity in time of the algorithm in Figure 1 is
O(|Q |· msim · k), if we consider the complexity of the norm
||·|| to be O(k). Figure 3 describes the average execution
time of this algorithm over 50 repetitions in function of the
vocabulary size. We exclude the keyword extraction and the
co-occurrence computation from this execution time. This
experiment was done with Enron dataset. The similar docu-
ment set represents 40% of the total dataset (12 044 emails)
and the server document set 60%. We note that even with
large document set the execution time is negligible (20 sec-
onds) compared to attacks like [28] which needs 16 hours
when msim = mreal = 1K. Our implementation is already CPU-
parallelized but can be further improved using GPU paral-
lelization.
Besides its short runtime, this algorithm is deterministic
and parameter-less. Non-determinism is present, for example,
in the simulated annealing used by IKK. Indeed, two runs of
IKK algorithm could result in two different results because
of a random choice present in this algorithm. It becomes a
problem when the attack is too long to be repeated many times
with different initializations and/or when the attacker does not
have a conﬁdence metric to identify the correct predictions (as
in IKK). The CGPR attack introduced an error-rate parameter
which needs to be set experimentally but it is unclear whether
this parameter is speciﬁc to each document set or not and how
to set it properly.
5 Reﬁned score attack
5.1 Algorithm
Our base attack introduces a matching score that acts as a
conﬁdence metric: the higher the score is, the more likely the
correctness of the keyword-trapdoor pair is. We can use this
property to determine the most certain predictions, that is, a
keyword-trapdoor candidate (kwi,td) will be considered as
certain if its score is much higher than the scores of any other
candidate (kw j,td). The certainty of a prediction kwi for the
trapdoor td is deﬁned as:
Certainty(td,kwi) = Score(td,kwi)− max
j(cid:54)=i
Score(td,kw j)
(7)
Based on this certainty, we propose a reﬁnement pro-
cess which drastically reduces the number of known queries
needed at the attacker’s side: the matching is performed sev-
eral times and at the end of each round, the most certain
predictions are added to the set of known queries. We detail
this process in Figure 4. This algorithm introduces a new pa-
rameter, namely, the reﬁnement speed RefSpeed to decrease
attack runtime. However, if the reﬁnement speed is chosen
too large, it is very likely that wrong predictions are added
USENIX Association
30th USENIX Security Symposium    149
kw,Q ,Cs
Require: Ksim,Cs
f inal_pred ← []
unknownQ ← Q
while unknownQ (cid:54)= /0 do
td,KnownQ ,RefSpeed
% 1. Extract the remaining unknown queries
unknownQ ← {td : (td ∈ Q )∧ ((cid:64)kw ∈ Ksim : (td,kw) ∈
KnownQ )}
temp_pred ← []
% 2. Propose a prediction for each unknown query
for all td ∈ unknownQ do
cand ← [] {The candidates for the trapdoor td}
for all kw ∈ Ksim do
s ← −ln(||Cs
append {"kw": kw, "score": s} to cand
kw[kw]−Cs
td[td]||)
end for
Sort cand in descending order according to the score.
certainty ← score(cand[0])− score(cand[1])
append (td,kw(cand[0]),certainty) to temp_pred
end for
% 3. Either stop the algorithm or keep reﬁning.
if |unknownQ |  |Kreal|, we increase the prob-
ability that Ksim ∩ Kreal = Kreal. In this case, all queries can
be recovered theoretically. In other words, as the size of Ksim
150    30th USENIX Security Symposium
USENIX Association
emails contain a richer vocabulary and longer emails. Thus,
our attack could be effective on a wide range of documents.
Moreover, the bar plot shows that ’Apache reduced’ has lower
results than Apache. Our results on ’Apache reduced’ are
closer to those on Enron. Since Apache and ’Apacha reduced’
share a common distribution and only differ in size, it shows
(once again) that our attack is sensitive to the amount of ad-
versary knowledge. In this case, the part of the adversary
knowledge which is increased is the similar document set.
Document set similarity Recall the similarity deﬁnition
for the document sets from Subsection 2.4. For a better un-
derstanding of this new deﬁnition, we performed two experi-
ments:
1. Using Enron as an attacker document set and Apache as
an indexed document set
2. Fixing the size of the indexed document set and attacking
it with similar document sets of varying size.
During the ﬁrst experiment over 50 repetitions, we recov-
ered at best 5 queries. This bad performance is explained by
the fact that the Enron dataset has a low similarity with the
Apache dataset (ε = 10.2). Further, the attacker vocabulary
(Ksim) and the queryable vocabulary (Kreal) only have 56% of
their keywords in common. In comparison, Figure 9 shows
results for experiments where the attacker and the server share
up to 98% of their vocabulary. Recall from Subsection 2.4,
that the joint keywords are an upper bound for the attack ac-
curacy. The disjoint vocabulary set combined with the high
ε value between Enron and Apache explain the low attack
accuracy for the ﬁrst experiment. We note that Enron is com-
posed of emails sent in a company while Apache is composed
of emails from a mailing list dedicated to a highly technical
project. This important difference in the nature of the emails
result in two very different keyword distributions (i.e. a very
low similarity between the document sets).
We show the results of the second experiment in Figure 7.
By varying the size of the attacker dataset, the co-occurrence
matrices of the attacker dataset and the indexed dataset di-
verge more or less. In other words, this size reduction applies
noise to the attacker’s word-word co-occurrence matrix in
comparison to the one computed with the complete dataset.
We preferred to apply this size reduction instead of applying
a synthetic gaussian noise (as is done in e.g. the IKK attack
paper) to the matrix because the added noise is more realistic
this way. Figure 7 shows that reducing the attacker document
set size leads to increased ε values hence it is an efﬁcient way
to decrease similarity.
In Figure 7, we observe that the smaller the attacker dataset
is, the less similar the document sets are and hence the less ac-
curate the attack results are. When the attacker dataset size is
divided by 2, e.g. from 12K documents to 6K, we still achieve
an average accuracy of 68%. Thus, the reﬁned score attack
Figure 6: Comparison of the accuracy between En-
ron, Apache
Fixed param-
|Dsim| = 12K,|Dreal| = 18K,msim = mreal =
eters:
1K,|KnownQ | = 15,RefSpeed = 10
and ’Apache
reduced’.
increases, the accuracy upper bound as stated in Equation (3)
in Subsection 2.4 potentially increases.
The reﬁned score attack yields highly accurate results
within minutes. It recovers most of the queries and assumes
less adversary knowledge than IKK and CGPR attacks. In [3],
Cash et al. report the average accuracy of the IKK attack is
around 30% for an attacker knowing 95% of the indexed doc-
uments for |Kreal| = 500,|Q | = 150,KnownQ = 8. With the
same parameters, CGPR achieves 70% accuracy. In Figure 5
we see that for a vocabulary size twice as large and less known
queries, i.e. |Kreal| = 1K,|Q | = 150,KnownQ = 10, the re-
ﬁned score attack obtains also 85% without partial knowledge
of the encrypted documents.
Query set size
In both the IKK and CGPR attacks, the num-
ber of observed queries was set as 15% of all possible queries.
We investigate the role of this choice in Figure 6. With a ﬁxed
number of known queries, the attack accuracy increases with
a larger query set. Intuitively, this demonstrates that our attack
uses efﬁciently the adversary knowledge, i.e. more observed
queries implies more adversary knowledge. In contrast, both
IKK and CGPR had steady results even for an increasing num-
ber of known queries, which indicates some sort of inefﬁcient
use of adversary knowledge.
Different email corpus We compare the accuracy on En-
ron and on Apache in Figure 6. For these simulations we
used three document sets: Enron (|Dsim| = 12K,|Dreal| =
18K), Apache (|Dsim| = 20K,|Dreal| = 30K) and ’Apache re-
duced’ (|Dsim| = 12K,|Dreal| = 18K). ’Apache reduced’ is the
Apache dataset truncated in order to have as many emails as in
Enron. Apache has slightly better results than Enron while the
USENIX Association
30th USENIX Security Symposium    151
from attacker documents and from observed queries) do not
match. Blackstone et al. [1] and Oya and Kerschbaum [23]
both propose attacks using query volume information only.
[1] assumes an attacker can identify known documents in the
index and thus still requires partial knowledge on indexed doc-
uments. They represent two bipartite graphs: one connecting
indexed documents to the queries and one connecting known
documents to keywords; then they match query nodes with
keyword nodes by iteratively reﬁning the candidates using
multiple ﬁltering steps. Oya and Kerschbaum [23] formu-
late an optimization problem based on maximum likelihood
estimators which assumes a distributional knowledge of the
indexed documents plus knowledge about query frequency.
Instead of partial index information, we focus on few
attacker-known keyword-trapdoor pairs which we use to score
every keyword-trapdoor candidate. We then iteratively add
pairs with highest scores to the attacker-known pairs to im-
prove our knowledge and reﬁne further predictions. This