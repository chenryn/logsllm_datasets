Block is
Verified
Figure 4: The graph shows the feedback from the sim-
ple marking session. The target curve is shown in dark
black, and spikes produced by sending UDP packets are
emphasized in dotted circles.
totally impossible. Our study shows that proper analy-
sis of the target system leads to detection in surprisingly
short time.
In the following, we examine possible consequences
after sensor addresses have become known.
• Sensors may be fed with arbitrary packets.
Malicious activity may evade sensors. Again, cap-
ture results no longer represent general background
activity.
It is important to realize that sensor attackers or
evaders do not require a complete list of sensor ad-
dresses. The list may be incomplete, or it may include
address ranges instead of addresses. The sensor revo-
cation mentioned above could be triggered by a single
address or address range in the list.
Sensors may be fed selectively with arbitrary pack-
ets. The result is that captured events no longer rep-
resent general background activity, effectively dis-
abling the target monitor.
• Sensors may become DoS victims.
Furthermore, sensors may be subject of DoS at-
tacks, possibly disabling victim sensors and asso-
ciated networks. The risk of suffering a DoS attack
or actually having been attacked may result in vol-
unteers removing their sensors from the distributed
monitors reducing their effectiveness.
• Sensors may be evaded.
212
14th USENIX Security Symposium
USENIX Association
It is equally important to recognize that these vulnera-
bilities are not limited to systems that make results pub-
licly available. There are commercial services and pri-
vate consortium which run similar threat monitors and
provide their clients or members with monitor results
with certain precision. We all know that information re-
leased to outside organizations is very likely to be prop-
agated among unlimited number of parties whether or
not the information is protected under some “soft” pro-
tection. Therefore, the threat is the "Clear and Present
Danger" for anybody who runs similar services.
4 Detection Methods
To understand the vulnerabilities of threat monitors, we
ﬁrst investigate possible ways of detecting them.
4.1 The Basic Cycle
The scheme that we have used for sensor detection is
basically a variation of the classic black-box estimation
procedure, as shown in Figure 5.
[1] Select an
assumption
Assumptions
(Suspect Address
List)
[2] Create Test
(Plan Marking)
[1-a]
Background
Info provides
hints
[0-b] Initial
List
Background Info
[0-a] Target
System
Study
[6] Refine,
update,
correct
assumtions
[3] Test
Input
(Markers)
Target System
[4] Feedback
Figure 5: The Monitor Detection Cycle
In Figure 5, a target system is at the bottom, and what
we call an assumption pool is at the top. This pool holds
address lists or address ranges that we believe might host
sensors. The goal of the detection procedure is to reﬁne
assumptions in this pool as much as possible. An outline
of the procedure is described below.
0. The target system is studied prior to the actual de-
tection procedure (0-a). Background information
about the target system’s characteristics are col-
lected using publicly available information such as
conference proceedings, workshop handouts, talk
videos and web pages.
Properties of
feedback information (reports)
from the target systems are also of interest in this
phase, and is discussed in Section 4.2. Characteris-
tics of an operating institution, including relation-
ships with other institutions and personal relation-
ships between key people sometimes allow us to
[5] Examine
Feedback
3. We run the marking process against the target sys-
tem by sending markers to addresses in the sus-
pected address list or range.
derive valuable information about sensor deploy-
ment.
As a result of this process, an initial set of ad-
dress lists or address ranges may be determined and
stored in the assumption pool (0-b). Background
information may also be ﬁlled with parameters that
affect the detection cycle. This includes a wide
range of information, from available physical re-
sources to various policies.
1. The actual detection cycle starts by selecting an ad-
dress list or address range from the pool. The list
or range may be divided into smaller lists or ranges
if necessary.
2. We design a set of tests to reﬁne the selected as-
sumption - usually narrowing down its range. To
distinguish this process from the traditional “scan-
ning”, we call running these tests “marking”.
Various background information,
especially
feedback properties of the target system, are used
in this process (1-a). We call the packets used in
marking activities “markers”. The main focus in
the design process is the marking algorithm de-
ployed, which is discussed in Section 4.3. Details
of designing the marking activity are discussed in
Section 4.4.
4. We capture the feedback from the target system.
5. The feedback is examined to identify results of suc-
cessful marking.
6. Based on these results, the assumption is reﬁned,
corrected, or updated. Afterwards, a new cycle be-
gins with a newly selected assumption.
4.2 Feedback Properties
The feedback properties that a target monitor provides
inﬂuence the marking process in many ways. In the fol-
lowing, we examine the major properties of typical feed-
backs. The most obvious property is the feedback type,
either a table or a graph, which we already described in
Section 2.2.2.
4.2.1 Timing Related Properties
For feedback in the form of a time series, timing related
properties play an important role when designing mark-
ing activity.
Accumulation Window The
accumulation window
can be described as the duration between two con-
secutive counter resets. For example, a feedback
that resets its counter of captured events every hour
has a accumulation window of one hour.
USENIX Association
14th USENIX Security Symposium
213
The accumulation window property affects the
marking process in several different ways;
1. An attempt to introduce changes must happen
within the accumulation window period.
In
other words, the accumulation window deter-
mines the maximum duration of a unit mark-
ing activity.
2. The smaller the accumulation window the
more address blocks can be marked in a given
time frame.
3. A smaller accumulation window requires less
markers to introduce changes.
Time Resolution Time resolution is the minimum unit
of time that can be observed in a feedback. The
time resolution provides a guide line for determin-
ing the duration of a single marking activity. That
is, a single marking activity should be designed to
ﬁt loosely into multiples of the time resolution for
the target system. We can not be completely ac-
curate as we need to be able to absorb clock skew
between target and marking systems.
Feedback Delay Delay is the time between a capture
event and next feedback update. For example, a
feedback that is updated hourly has a maximum de-
lay of one hour, while another feedback that is up-
dated daily has a maximum delay of one day. The
delay determines the minimum duration between
different marking phases that is necessary to avoid
dependency between them.
Most feedbacks have identical time resolution,
accumulation window and delay property, but not
always. For example, there is a system which pro-
vides a weekly batch of 7 daily reports, in which
case the accumulation window is one day while the
maximum delay is 7 days.
Retention Time The retention time of a feedback is the
maximum duration that a event is held in the feed-
back. For a graph, the width of the graph is the re-
tention time for the feedback. All events older than
the graph’s retention time are pushed out to the left.
Figure 6 illustrates the relationship between dif-
ferent timing properties using a hypothetical feed-
back that updates every 2 days, and provides accu-
mulated packet counts for a particular day every 6
hours. As shown in this ﬁgure, this feedback has a
time resolution of 6 hours, a accumulation window
of 1 day, a maximum feedback delay of 2 days. In
addition, the retention time for this graph is 3 days.
The duration of some possible marking activities
are also shown in this ﬁgure. Note that a marking
activity can span multiple resolution units, but can-
not span two accumulation windows.
t
n
u
o
C
t
e
k
c
a
P
18
16
14
12
10
8
6
4
2
maximum delay
accumulation window
time resolution
duration of possible unit activities
18
24
6
12
18
6
24
Time
12
18
24
6
12
Figure 6: Timing Properties of A Hypothetical Feedback
4.2.2 Other Feedback Properties
In addition to the timing related properties, there is a
group of properties that mainly rule how capture events
are presented in feedbacks. These properties also play
an important role when designing marking activity.
Type Sensitivity Type sensitivity refers to the sensitiv-
ity of a feedback to certain types of packets. If a
feedback shows signiﬁcant changes in its response
to an appropriate amount of packets of the same
type, then the feedback is sensitive to that type.
Dynamic Range The dynamic range of a feedback,
which is the difference between smallest and largest
numbers in the feedback, presents another impor-
tant factor in a marking design, in conjunction with
the level sensitivity property described next.
Counter Resolution / Level Sensitivity
The counter
resolution of a feedback is the minimal number of
packets required to make an identiﬁable change in
the feedback. For example, a feedback table that
includes information for a single event has a reso-
lution of 1 packet, while the feedback in a graph
that is 100 dots high with a maximum scale (dy-
namic range) of 1,000 packets has a resolution of
10 packets.
We use the term “sensitivity” also to describe the
“level of sensitivity”.
In the above example, the
former feedback is more sensitive than the latter
feedback. Some systems use logarithmic scale for
their feedbacks, and as a result, they are sensitive to
small spikes even if the dynamic range of the feed-
back is very large.
The unit of measurement also vary from system
to system, and affects the level sensitivity of the
feedback. Some use accumulated packet counts
directly, in which case the sensitivity can be cal-
culated easily. Others use mathematically derived
values such as average packet counts per sensor
and packet counts per 100 sensors.
In the latter
214
14th USENIX Security Symposium
USENIX Association
case, number of sensors in the monitor must be
known to calculate the sensitivity. This ﬁgure may
be obtained from the background information for
the monitor, or may be estimated by comparing the
feedback with feedbacks from other monitors that
use plain packet counts.
Cut-off and Capping Some systems drop events that
are considered non-signiﬁcant. Some systems drop
events that are too signiﬁcant, so events with less
signiﬁcance are more visible. A common case of
cut-off is a feedback in the form of “Top-N” events.
In this type of feedback, the top N event groups are
selected, usually based on their accumulated event
count over a predetermined period of time such as
an hour, a day or a week. A feedback with the top-
N property is usually very hard to exploit, because
it often requires a large number of events to make
signiﬁcant changes such as visible spikes or an in-
troduction of a new event group. However, there
is a chance of exploiting such feedback, when the
feedback exhibits certain properties, such as fre-
quently changing members with low event counts,
or if there is a event group with a period of inac-
tivity. An introduction of a new event group is also
possible, by “pre-charge” activity that is intended
to accumulate event counts.
4.3 Marking Algorithms
An algorithm used for a particular marking is often de-
pends on the feedback properties. It is sometimes neces-
sary to modify or combine basic marking algorithms to
exploit a particular feedback, or to derive more efﬁcient
marking algorithm. In the following, we present some
examples of possible marking algorithms.
4.3.1 Address-Encoded-Port Marking
A system providing a table of port activities may be-
come a target of an address-encoded-port marking. In
this method, an address is marked with a marker that
has its destination port number derived from encoding
part of the address bits. After the marking, port numbers
which were successfully marked are recovered from the
feedback, which are in turn combined with other address
bits to derive reﬁned addresses.
Consider the case in Figure 7.
In this example, we
mark a /16 address block with base address b that is host-
ing a sensor at b + A. The destination port of the marker
for address b + n is set to the 16 lower bits of the ad-
dress b + n (which is equivalent to n). For the sensor
address b + A, a marker with destination port set to A is
sent, which in turn appear as captured event on port A in
the port activity report. This feedback is combined with
the base address b to form the complete address of the
sensor, which is b + A.
Marker for
addres
b + n
Destination port
( b + n) & 0xf f f f
( =n)
Marking
b+0
b + A
b+65535
Feedback
por t
count
. . .
A
. . .
1
Port report
/16 Target
address space
Base address = b
Sensor Address:
b + A
Figure 7: An Address-Encoded-Port Marking Example
Although the address-encoded-port marking can only
be deployed against table type feedbacks, it is consid-
ered extremely efﬁcent, because it can deliver multiple
complete or partial addresses from a single marking ac-
tivity.
However, not all of the 16-bit port space is available
in practice; there are ports which are frequently found
in real background activity. Some of these ports, espe-
cially those that are used by vulnerable, usually receive
a large number of events. Other ports may also receive
some background trafﬁc due to back scatter and stray or
wandering packets.
To increase the accuracy of our method even in the
presence of background trafﬁc for some ports, it is pos-
sible to determine the usable port space in advance. This
can be achieved by looking at previous port-reports from
the target system. The accuracy of this method can
also be improved by incorporating redundant marking
in which multiple makers of the same type are sent to
the same address to mask the existence of busy ports.