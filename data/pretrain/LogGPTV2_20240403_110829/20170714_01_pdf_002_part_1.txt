Hash Joins
Past, Present & Future
Thomas Munro, PGCon 2017, Ottawa
About me
EnterpriseDB Database Server team (~ 2 years)
•
PostgreSQL contributions: SKIP LOCKED,
•
remote_apply, replay_lag, DSA (co-author),
various smaller things, debugging and review
Relevant active proposal: parallel-aware hash
•
join
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
Joins
A set of
•
operators from
the relational
algebra
Join operators
•
take two
relations and
produce a new
relation
Example join syntax in SQL
R, S WHERE R.foo = S.foo
•
R [INNER] JOIN S ON R.foo = S.foo
•
R {LEFT|RIGHT|FULL} OUTER JOIN S
•
ON R.foo = S.foo
R WHERE [NOT] EXISTS
•
(SELECT * FROM S WHERE R.foo = S.foo)
R WHERE foo IN
•
(SELECT foo FROM S)
Execution strategies
Nested loop:
•
For each tuple in outer relation, scan inner
relation
Merge join:
•
Scan inner and outer relations in the same order
Hash join:
•
Build a hash table from inner relation, then probe
it for each value in outer relation
M-x squint-mode
A hash join is a bit like a nested loop with a temporary in-
•
memory hash index built on the fly
Hash joins like RAM; early memory-constrained SQL
•
systems had only nested loops and merge joins
Large RAM systems enabled hash join, but also made
•
sorting faster, so which is better? See extensive writing on
sort vs hash, but we are very far from the state of the art in
both cases…
Choice of algorithm limited by join conditions and join type
•
postgres=# select * from r full join s on r.i != s.i;
ERROR: FULL JOIN is only supported with merge-joinable or hash-joinable join conditions
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
Let a hundred
hash tables bloom
DynaHash: chained (conflict resolution by linked
•
lists), private or shared memory, general
purpose
simplehash: open addressing (conflict resolution
•
by probing), private
Hash join’s open coded hash table: why?!
•
Hash join table
Little more than an array
•
• Multiple tuples with same key (+ unintentional hash collisions);
so you’d need to manage your own same-key chain anyway
• Hash join has an insert-only phase followed by a read-only probe
phase, so very few operations needed
• If we need to shrink it due to lack of memory or expand the
number of buckets, it’s still the same: free it, allocate a new one
and reinsert all the tuples
• It’s unclear what would be gained by using one of the other
generic implementations: all that is needed is an array!
tuple tuple
hash(key) = 42
tuple
tuple
Chunk-based storage
Tuples are loaded into 32KB chunks, to reduce palloc
•
overhead
This provides a convenient way to iterate over all tuples
•
when we need to resize the bucket array: just replace the
array, and loop over all tuples in all chunks reinserting
them into the new buckets (= adjusting pointers)
Also useful if we need to dump tuples due to lack of
•
memory: loop over all tuples in all chunks, copying some
into new chunks and writing some out to disk
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
Hash Join
Hash Cond: 
-> 
-> Hash
-> 
High level algorithm
Build phase: load all the tuples from the inner
•
relation into the hash table
Probe phase: for each tuple in the outer relation,
•
try to find a match in the hash table
Unmatched phase: for full outer joins and right
•
outer joins only (“right” meaning inner plan),
scan the whole hash table for rows that weren’t
matched
Optimisations
Empty outer: before attempting to build the hash
•
table, try to pull a tuple from the outer plan; if
empty, then end without building hash table
Empty inner: after building the hash table, if it
•
turns out to be empty then end without probing
Out joins prevent one or both of the above
•
Buckets
Number of tuples* / number of buckets = load factor
•
The planner estimates the number of rows in the inner
•
relation, and the hash table is initially sized for a load
factor of one (rounding up to power of two)
If the load factor turned out to be too high the bucket
•
array is resized and tuples are reinserted by looping
over the storage chunks
*ideally we’d probably use number of distinct keys,
not number of tuples
Hash Join (actual rows=2000 loops=1)
Hash Cond: (s.i = r.i)
-> Seq Scan on s (actual rows=10000 loops=1)
-> Hash (actual rows=2000 loops=1)
Buckets: 2048 (originally 1024)
Batches: 1 (originally 1)
Memory Usage: 87kB
-> Seq Scan on r (actual rows=2000 loops=1)
Filter: ((i % 5) < 5)
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
Respecting work_mem
Partition the inner relation into “batches” such that
•
each inner batch is estimated to fit into work_mem
Known as the “Grace” algorithm, or “hybrid” with
•
the additional refinement that partition 0 is loaded
into the hash table directly to avoid writing it out to
disk and reading it back in again
Adaptive batching: if any batch turns out to be too
•
large to fit into work_mem, double the number of
batches (split them)
Optimisation
“Skew optimisation”: if the planner determines
•
that we should use a multi-batch hash join, then
try to use statistics to minimise disk IO. Find the
most common values from the outer plan and
put matching tuples from the inner plan into
special “skew buckets” so that they can be
processed as part of partition 0 (ie no disk IO).
skew hash
hash table
table p0
file file file
build
inner inner inner
p3 p2 p1
inner outer
relation relation
skew hash
hash table
table p0
file file file file file file
probe
inner inner inner outer outer outer
p3 p2 p1 p1 p2 p3
inner outer
relation relation
hash
table
p1
file file file file file file
next
inner inner inner outer outer outer
batch
p3 p2 p1 p1 p2 p3
inner outer
relation relation
hash
table
p1
file file file file file file
probe
inner inner inner outer outer outer
p3 p2 p1 p1 p2 p3
inner outer
relation relation
h
s
a
h
e
l
b
work_mem full
a
t
2
p
file file file file file file
next
inner inner inner outer outer outer
batch
p3 p2 p1 p1 p2 p3
inner outer
relation relation
hash