title:Min-Max Checkpoint Placement under Incomplete Failure Information
author:Tatsuya Ozaki and
Tadashi Dohi and
Hiroyuki Okamura and
Naoto Kaio
Min-Max Checkpoint Placement under Incomplete Failure Information ∗
†
†
†
‡
Tatsuya Ozaki
Naoto Kaio
Higashi-Hiroshima 739–8527, Japan; { dohi,okamu}@rel.hiroshima-u.ac.jp
Department of Information Engineering, Hiroshima University
Tadashi Dohi
Hiroyuki Okamura
†
‡
Department of Economic Informatics, Hiroshima Shudo University
Hiroshima 731-3195, Japan; PI:EMAIL
Abstract
In this paper we consider two kinds of sequential check-
point placement problems with inﬁnite/ﬁnite time horizon.
For these problems, we apply the approximation methods
based on the variational principle and develop the compu-
tation algorithms to derive the optimal checkpoint sequence
approximately. Next, we focus on the situation where the
knowledge on system failure is incomplete, i.e.
the sys-
tem failure time distribution is unknown. We develop the
so-called min-max checkpoint placement methods to deter-
mine the optimal checkpoint sequence under the uncertain
circumstance in terms of the system failure time distribu-
tion. In numerical examples, we investigate quantitatively
the min-max checkpoint placement methods, and refer to
their potential applicability in practice.
1 Introduction
It is well known that the system failure in large scaled
computer systems can lead to a huge economic or criti-
cal social loss. Checkpointing and rollback recovery is
a commonly used technique for improving the reliabil-
ity/availability of fault-tolerant computing systems, and is
regarded as a low-cost dependability technique from the
standpoint of environment diversity. Especially, when the
ﬁle system to write and/or read data is designed in terms
of preventive maintenance, checkpoint generations back up
periodically the signiﬁcant data on the primary medium to
the safe secondary medium, and play a signiﬁcant role to
limit the amount of data processing for the recovery actions
after system failures occur.
If checkpoints are frequently
taken, a larger overhead will be incurred. Conversely, if
∗
This work is supported by the Ministry of Education, Science, Sports
and Culture, Grant-in-Aid for Exploratory Research; Grant No. 15651076
(2003-2005), Young Scientists (B); Grant No. 15700060 (2003-2004), and
the Research Program 2003 under the Institute for Advanced Studies of the
Hiroshima Shudo University, Japan.
only a few checkpoints are taken, a larger overhead after
system failures will be required in the rollback recovery ac-
tions. Hence, it is important to determine the optimal check-
point sequence taking account of the trade-off between two
kinds of overhead factors above. In many cases, the sys-
tem failure phenomenon is described as a probability distri-
bution called the system failure time distribution, and the
optimal checkpoint sequence is determined based on the
stochastic model [21].
Young [28] obtains the optimal checkpoint interval ap-
proximately for the computation restart after system fail-
ures. Baccelli [1], Chandy et al. [5, 6], Dohi et al. [9],
Gelenbe and Derochette [12], Gelenbe [13], Gelenbe and
Hernandez [14], Goes and Sumita [15], Grassi et al. [16],
Kulkarni et al. [17], Nicola and Van Spanje [20], Sumita
[23] propose performance evaluation models for
et al.
database recovery, and calculate the optimal checkpoint in-
tervals which maximize the system availability or minimize
the mean overhead during the normal operation. L’Ecuyer
and Malenfant [18] formulate a dynamic checkpoint place-
ment problem by a Markov decision process. Ziv and Bruck
[27] reconsider the checkpoint placement problem under a
random environment, by taking account of the change of
operation circumstance. Vaidya [26] examines the impact
of checkpoint latency on overhead ratio for a simple check-
point model. Recently, Okamura et al. [22] reformulate the
Vaidya model [26] with a semi-Markov decision process.
On the other hand, some authors discuss the sequential
checkpoint placement problems where under some condi-
tions the checkpoint intervals are not constant. For instance,
in almost all checkpoint models for transaction-based sys-
tems [1, 13, 14, 15, 23], it is proved that the constant check-
point intervals maximizing the system availability are op-
timal. Since the way to place the optimal checkpoint se-
quence depends on the kind of objective functions (system
availability, mean overhead, etc.) and the failure time dis-
tribution, in fact, the sequential checkpoint can provide the
general framework on the checkpoint placement. Toueg and
Babao˜glu [25] develop a dynamic programming algorithm
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:35 UTC from IEEE Xplore.  Restrictions apply. 
which minimizes expected execution time of tasks plac-
ing checkpoints between two consecutive tasks under very
general assumptions. Recently, Ling et al.
[19] propose
an approximate method, called the variational calculus ap-
proach, to calculate the cost-optimal checkpoint sequence.
This method is originally developed by Fukumoto et al.
[10, 11] in earlier for seeking the nearly optimal checkpoint
sequence in a database recovery. In the sequential check-
point placement problem, it is assumed that the system fail-
ure time obeys the common probability distribution, i.e. it
does not always the negative exponential distribution. Ac-
tually, it is reported that some system failures are caused
by the software aging [4] and that the system failure time
can not be regarded as the exponential distributed random
variable in most cases.
From the viewpoints of practical ﬁle management, how-
ever, it is quite hard to estimate the system failure time dis-
tribution, because the system failure is a rare event and the
corresponding time data are not available, especially, in the
initial operational phase. Even if we experience a few sys-
tem failures during the operation, is it really possible to
observe a sufﬁcient number of data to estimate the proba-
bility distribution function with higher accuracy and select
the best candidate with satisfactory signiﬁcance level from
several theoretical distributions through the goodness-of-ﬁt
test? In most cases, the checkpoint decision is made under
the incomplete failure information, where the incomplete
information implies that the type of system failure time dis-
tribution is not speciﬁed but only its moment information is
available. Nevertheless, in the past literature the arbitrary
but completely known probability distribution has been as-
sumed to obtain robust and general results on checkpoint
placement. We often encounter such an operational envi-
ronment for real life system applications like database sys-
tems, with incomplete failure information. As an extreme
but ad-hoc example the, reader can image that almost all
computer users seldom specify the system failure time dis-
tribution in their computing circumstance with the aim of
checkpointing.
In this paper, we consider two kinds of sequential check-
point placement problems with inﬁnite/ﬁnite time horizon.
For these problems, we apply the approximation methods
based on the variational principle and develop the compu-
tation algorithms to derive the optimal checkpoint sequence
approximately. Fukumoto et al. [10, 11] and Ling et al.
[19] use the variational calculus approach to only the se-
quential checkpoint placement problem with inﬁnite-time
horizon, provided that the system failure time distribution
is known. We generalize their results mathematically and
propose a checkpoint placement algorithm for a ﬁnite time
horizon problem. Next, we focus on the situation where the
knowledge on system failure is incomplete, i.e. the system
failure time distribution is unknown. Dohi et al. [8] develop
an optimal checkpoint model with media failures and pro-
pose statistical estimation algorithms of the optimal check-
point interval, based on the total time on test concept, in the
situation where the media failure time distribution is un-
known, but the corresponding complete data are available.
Okamura et al. [22] propose an on-line adaptive checkpoint
algorithm based on the reinforcement learning called the
Q-learning (see, e.g., [24]), and revisit the Vaidya model
[26]. Since this algorithm is a statistical non-parametric al-
gorithm, one does not need to specify the system failure
time distribution in advance. The main advantages of the
adaptive checkpoint algorithm based on the Q-learning are
that the implement of the algorithm is quite easy on com-
puter and that the asymptotic convergence to the real opti-
mal policy can be guaranteed. However, since the conver-
gence speed for the Q-learning is rather slow, i.e., a number
of data are needed in estimation, it is difﬁcult to apply the
algorithm to real-time applications for practical use.
In order to overcome the difﬁculty on the on-line check-
point generation under the incomplete failure information,
we develop the so-called min-max checkpoint placement to
determine the optimal checkpoint sequence under the un-
certain circumstance in terms of the system failure time dis-
tribution. Barzilovich et al.
[3] and Derman [7] provide
the theoretical framework of min-max surveillance sched-
ules for hardware products. In the min-max policy frame-
work, one does place the nearly optimal checkpoint with
incomplete knowledge on system failure time distribution.
More speciﬁcally, the min-max policy leads to the cost-
optimal checkpoint sequence minimizing the expected op-
erating cost under the most pessimistic situation, i.e.
the
system failure tends to occur most frequently. For two
kinds of sequential checkpoint placement problems with in-
ﬁnite/ﬁnite time horizon, we develop the min-max sequen-
tial checkpoint placement algorithms, based on the varia-
tional calculus approach.
In numerical examples, we in-
vestigate quantitatively the min-max checkpoint placement
methods, and refer to their potential applicability in prac-
tice.
2 Sequential Checkpoint Placement
2.1 Inﬁnite-Time Horizon Problem
Consider a simple ﬁle system with sequential check-
pointing over an inﬁnite time horizon. The system opera-
tion is started at time t = 0, and the checkpoint (CP) is
sequentially placed at time {t1, t2,··· , tn,···}. At each
CP, tj (j = 1, 2,··· ), all the ﬁle data on the main memory
is saved to a safe secondary medium like CD-Rom, where
the cost (time overhead) c0 (> 0) is needed per each CP
placement. System failure occurs according to an abso-
lutely continuous and non-decreasing probability distribu-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:35 UTC from IEEE Xplore.  Restrictions apply. 
tion function F (t) having density function f(t) and ﬁnite
mean 1/µ (> 0). Upon a system failure, a rollback recov-
ery takes place immediately where the ﬁle data saved at the
last CP creation is recovered. Next, a checkpoint restart is
performed and the ﬁle data is recovered to the state just be-
fore the system failure point. The time length for the check-
point restart is given by the function L(·), which depends
on the system failure time and is assumed to be differen-
tiable and increasing. Without any loss of generality, it is
assumed that no failure occurs during the recovery period
with probability one.
Then, the problem is to derive the optimal CP sequence
t∞ = {t1, t2, t3 ···} minimizing the expected operating
cost function:
: C(t∞) =
min
t∞
∞(cid:1)
(cid:2) tn+1
(cid:3)
(cid:4)
+L(t − tn)
n=0
tn
c0(n + 1)
dF (t),
(1)
where t0 = 0. In the above formulation, it is seen that an
additional checkpointing is carried out just after completing
the recovery operation and that the total CP cost becomes
c0(n + 1). From the analogy to the inspection problems for
hardware systems (see e.g. [2]), it can be easily found that
the optimal CP sequence t∞ is a non-increasing sequence,
i.e., t1  0) and
b0 (> 0) are constants. The ﬁrst term a0 denotes the mean
time necessary to re-execute the lost ﬁle data in time inter-
val [0, t) since the last CP, and the second term is a ﬁxed
time associated with the CP restart. It is well known that
the optimal CP interval is constant, i.e., t1 = t2 − t1 =
··· = tn+1 − tn = ··· , if F (t) is the exponential distribu-
tion with mean 1/µ. Under the assumptions that a0 = 1 and
b0 = 0, Young [28] considers the checkpoint restart model
with constant CP interval under the exponential assumption,
and derives the following non-linear equation which satis-
ﬁes the optimal CP interval texp∞ :
ec0µ − texp∞ µ − e
−texp∞ µ = 0.
(3)
Based on the second order approximation exp(−tµ) ≈
1 − t/µ + t2/(2µ2), he obtains the approximate form of
the optimal CP interval:
(cid:6)
(cid:6)
2(ec0µ − 1)/µ ≈
which is due to exp(c0µ) ≈ 1 + c0µ.
texp∞ ≈
2c0/µ,
(4)
For the general system failure time distribution, the ﬁrst
order condition of optimality for the minimization problem
in Eq.(1) is given by
F (tn+1) − F (tn) = tn − tn−1
− c0f(tn)
f(tn)
a0
n = 1, 2, 3,··· .
,
(5)
∗
1, t
(cid:7) tM+1
∞ = {t
(cid:7) t1
From the condition of optimality, an algorithm to derive the
2,···} which minimizes
optimal CP sequence t∗
∗
C(t∞) can be derived. More precisely, we set the initial
value t1 satisfying c0 = a0
0 F (t)dt, and compute the
CP sequence {t2, t3,···} using Eq. (5). Next, for j-th CP
(j = 1, 2,··· ), if tj+1 − tj > tj − tj−1 then t1 → t1 − 
and compute the CP sequence {t2, t3,···} again, where
 (> 0) is a sufﬁciently small constant. On the other hand,
if tj+1 − tj  0). For a ﬁnite sequence
tN = {t1, t2,··· , tN}, the expected operating cost is for-
mulated as
N(cid:1)
(cid:2) tn+1
n=0
tn
(cid:3)
(cid:4)
c0(n+1)+ L(t− tn)
T C(tN ) =
dF (t), (6)
where N = min{n : tn+1 > T}. To simplify the notation,
we deﬁne tN +1 = T in this paper. Since the ﬁnite-time
horizon problem involves constraints on the number of CPs,
it is impossible to apply directly Algorithm 0 mentioned be-
fore. More precisely, the underlying problem is reduced to
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:35 UTC from IEEE Xplore.  Restrictions apply. 
the following non-linear programming problem with con-
straints:
: T C(tN )
min
tN
t1  0).
s.t.
(7)
In principle, the above problem can also be solved by apply-
ing the quasi-Newton method if a suitable initial value t1 is
given. Even if one uses the quasi-Newton method for the
ﬁnite-time horizon problem, however, the problem on com-
putation can not be essentially overcome, because the value
of N has to be changed so as to minimize the expected oper-
ating cost. This fact will motivate us to develop an approxi-
mate method to calculate the optimal CP sequence quickly.
Also, it should be noted that the optimal CP sequence
t∗
N may not be constant even if the system failure time is
the exponential. This is because the ﬁrst order condition of
optimality in Eq.(5) is violated for the ﬁnite-time horizon
problem. In other words, our CP model with the optimal CP
N = {t
N } under the exponen-
sequence texp
tial assumption is an extension of the classical Young model
N → texp∞ as T → ∞. This is an alterna-
[28], since t
tive motivation to consider the ﬁnite-time horizon problem.
In the following section, we develop approximate methods
based on the variational calculus for both ﬁnite and inﬁnite-
time horizon problems, and investigate those properties.
,··· , t
exp
1
exp
2
exp
exp
, t
3 Approximate Methods
3.1 Inﬁnite-Time Horizon Problem
Following Fukumoto et al. [10, 11] and Lin et al. [19],
we derive the approximate form of the expected operat-
ing cost in Eq.(1). Let D(t) be an absolutely continuous
function of time t. We approximate the number of CPs