most 1: the probability that v(cid:48) or v gets submitted by a honest voter
is close to 2−80. What this example shows is that for elections
where the so called short ballot assumption [28] does not hold (i.e.
the number of possible votes is much larger than the total number
of voters), the deﬁnition of [23] would declare a complete privacy
breach, whereas this may not be the case. Indeed, it is clearly rea-
sonable to consider that voters taking part to such an election do
have some level of privacy: no one is able to decide which one of
the one million submitted ballots comes from a speciﬁc voter. Our
privacy notion aims to address these limitations. We provide a more
detailed comparison with the notion of [23] in Section 6.2.
3. ENTROPY NOTIONS
As explained in the introduction the privacy measure that we
consider relies on a computational version of conditional entropy.
In this section we brieﬂy review some standard notions of entropy
and explain how to transfer the deﬁnitions from an information the-
oretic to a computational setting. Here and throughout the paper we
denote random variables by boldface capital letters and use P and
E to denote probability and expected value. We also use boldface
capital letters for ensembles of random variables.
Information-theoretic entropy
For any k ∈ [0,∞] the Rényi entropy [26] with parameter k of a
random variable X with range X is deﬁned as:
 X
x∈X
!
Hk(X) :=
1
1 − k
log
P [X = x]k
(where the values 1 and ∞ are understood as limits). Three in-
stances of k yield particularly useful, well-known notions of en-
tropy: the case k = 0 is Hartley entropy, k = 1 is Shannon entropy
and k = ∞ min-entropy.
For any ﬁxed entropy notion H there are several possibilities for
deﬁning the conditional entropy of one random variable given an-
other (and each of these variants can be used as privacy measures).
The conditional entropy of X given Y is deﬁned as the expected
value of the entropy of X, where the expectation is taken over Y:
H(X | Y) := Ey∈Y [H(X | Y = y)]
Note that H(X | Y = y) is not a conditional entropy: the expres-
sion over which the entropy is taken is simply a random variable.
The minimal entropy of X given Y is deﬁned as the min value
of the entropy X, where the min is over all possible values of Y:
H⊥
(X | Y) := min
y∈Y
H(X | Y = y).
Finally, one can consider average entropy, similar in spirit to the
above but computed by taking the expected value of an exponen-
tial function of the entropy under consideration and applying the
logarithm to this result. In general we denote average entropy by
˜H(X | Y). Unlike for conditional and minimal entropy, there is
no universally established way to deﬁne average conditional en-
tropies, and existing notions are usually informed by the intended
applications, or intuitive appeal. Dodis et al. [14] deﬁned average
min-entropy as:
„
h
−H∞(X|Y=y)i«
˜H∞(X | Y) := − log
E
y∈Y
2
943This measures the average probability of successfully guessing the
value of x given y. Later in the paper (Section 6.2) we expand the
above point and explain why this way of deﬁning average condi-
tional min-entropy extends the notion of min-entropy as maximal
guessing probability to conditional random variables.
We propose an analogue for the notion of average Hartley en-
tropy. Recall that Hartley entropy measures the size of the range of
a random variable: if X has range X then H0(X) = log |X|. Our
average Hartley entropy measures the average size of the range of
the random variable (X|Y = y) where the average is taken over
Y.
˜H0(X | Y) := log
E
y∈Y
2
„
h
H0(X|Y=y)i«
Note that the formula differs in the lack of minus signs from the one
for average min-entropy. This is a technical but important detail.
Each of these notions measures a different aspect of the “uncer-
tainty” about the random variable X when the value of the random
variable Y is available. We discuss this point in further detail in
Section 4.
Conditional Privacy Measures
Each of the information-theoretic notions deﬁned above can serve
as a basis for our computational notion of privacy. It turns out that
we can offer a uniﬁed treatment of the resulting possibilities by ab-
stracting the many variants of entropy into a single notion which we
call conditional privacy measure (the name is due to its application
in deﬁning privacy). The following deﬁnition sets two minimal and
desirable conditions that such measures should satisfy.
DEFINITION 1. A conditional privacy measure F(T | L) is a
function mapping a pair of random variables to a positive real
number and satisfying the following conditions. The names of the
variables are mnemonics for the "target" of the adversary and the
"leaked" information to which the adversary has access.
• If T can be computed as a (probabilistic) function of L then
F(T | L) = 0.
• If L and L(cid:48) are two (probabilistic) functions such that L(cid:48) can
be computed as a function of L then for all T,
F(T | L(cid:48)) ≥ F(T | L).
All of the different notions of conditional entropy satisfy these two
conditions.
Computational Entropy
Information theoretic entropy quantiﬁes uncertainty of random vari-
ables in face of unbounded adversaries. Computational entropy is
the analogue notion for the case of efﬁcient adversaries. Intuitively,
a random variable has computational entropy if it is computation-
ally close to one that has information theoretic entropy. In the rest
of the section we abuse notation and refer to ensembles of random
variables as random variables.
DEFINITION 2. Two random variables ensembles X = (Xi)i∈N
C≈ Y, if
and Y = (Yi)i∈N are computationally close, written X
the quantity |P [A(Xi) = 1] − P [A(Yi) = 1]| is negligible as a
function of i, for all polynomial-time algorithms A.
We propose a computational notion of conditional entropy, based
on previous work by Reyzin et al. [27] and Gentry et al. [17]. Our
deﬁnition is in the setting of asymptotic security as opposed to con-
crete and/or non-uniform adversaries. Furthermore, the application
to voting motivates a variation from the typical way of extending
information theoretic entropies to computational versions. We dis-
cuss this variation after we give the deﬁnition.
DEFINITION 3. Let T, R and L be ensembles of random vari-
ables to which we refer to as target, result, and leakage functions.
Let F be a conditional privacy measure.1 We say that T has at
least r bits of computational conditional privacy given R and L
(for which we write Fc(T | R, L) ≥ r) iff ∃S = (Si)i∈N s.t.
C≈ (T, R, S)
• (T, R, L)
• (∀i ∈ N) F(Ti | Ri, Si) ≥ r
To understand the above deﬁnition, it helps to think of T as some
sensitive information, R as some information about T that is cer-
tainly leaked, and L as some information about that T that is cryp-
tographically hidden. Informally, the above deﬁnition says that tar-
get T has at least r bits of computational entropy given result R
and leakage L if there is a distribution S such that L is computa-
tionally close to S, even when T and R are known, and for any
security parameter i the information theoretic entropy in T given
R and S is at least r.
The following example should help understanding the intuition
behind computational conditional entropy and how we employ it
later in the paper to measure vote privacy. Consider the encryption
Encpk(M ) of some message M under a public key pk, and as-
sume that M is selected from a distribution with non-zero entropy
(say just 1 bit). Imagine that an adversary obtains in the execution
of some system the encryption Encpk(M ) and some side informa-
tion on M, say the XOR of its bits, ⊕iMi. In this situation, the
information theoretic entropy left in M is 0 (as an unbounded ad-
versary can decrypt the ciphertext and recover M). However, since
for an efﬁcient adversary the ciphertext looks like an encryption of
a random message (assuming that the encryption scheme is secure)
we would like to conclude that the loss of entropy in M is only
due to revealing ⊕iMi. The deﬁnition above captures this intu-
ition: the computational entropy in M given ⊕iMi and Encpk(M )
is the (information theoretic) entropy of M given ⊕iMi and the
encryption Encpk(R) of a random message (independent of M).
This latter encryption plays the role of S in our deﬁnition above:
(M,⊕iMi, Encpk(M ))
C≈ (M,⊕iMi, Encpk(R)).
4. VOTE PRIVACY
In this section we introduce our measure of privacy. We start
by ﬁxing some necessary details regarding the execution model but
leave others unspeciﬁed. For example, we do not enforce a partic-
ular communication infrastructure nor do we assume a particular
communication model. We even abstract away many details of the
adversarial model. The result is a ﬂexible framework focused on
those aspects that are essential for deﬁning privacy. We then intro-
duce our notion of privacy based on the details that we ﬁx. The
deﬁnition can then be easily instantiated for particular execution
models/communication infrastructures etc.
Execution
We assume that voting involves a set of parties, some of which are
under the control of the adversary. We do not make a distinction
between voters and authorities. We write P for the set of all parties
and H for the set of honest parties. Throughout the paper we let nP
be the total number of voters and nH the number of honest voters.
We write V for the set of possible votes (including abstention).
1Think about F as some ﬁxed information theoretic entropy notion.
944A voting protocol is given by a set of interactive programs (pro-
cesses), one for each party involved. Each program may use some
secret information (e.g. signing keys that users use to authenticate,
decryption keys that tallying authorities use to decrypt the result
of the election) and the information that is publicly available. The
programs for the voters also take as input a vote in V, and we as-
sume that these votes are selected according to a joint distribution
D on set V nH. We write π for a generic voting protocol, i.e. a
description of the programs for the parties involved.
As discussed above we do not require a particular execution model
as our deﬁnition and results do not depend on the model. We thus
only assume that whatever the model one can formally deﬁne the
information that an adversary obtains during the execution through
its view of the execution. The view of the adversary is a standard
cryptographic notion.
We deﬁne the view of the adversary as the (distribution of) his
output; an adversary may output anything he chooses including the
entire state and history of his execution and his random choices.
Clearly the view depends on the details of the execution model
which formally speciﬁes which channels are public/private what
parts of the system can be corrupted, if corruption is static or adap-
tive, how does the adversary accesses the bulletin board (if any),
how it accesses the result of the election, etc. We use the nota-
tion View(A, π(D)) for the random variable that deﬁnes the view
of the adversary A that interacts with the voting protocol π, when
the votes of the honest participants are selected according to the
distribution D. By abusing notation we also write View(A, π(D))
for the ensemble of random variables that deﬁne the view of the
adversary for the different security parameters, and sometimes we
simply write View when the various parameters are clear from the
context. In a voting process that aims to compute a function ρ of
the votes, this view includes the result of the election, which is the
random variable (ensemble) RD,vA,π, where vA is the distribution
(ensemble) that represents the votes cast by corrupt parties.
Our Privacy Measure
In this section we motivate and deﬁne a measure for the privacy
of votes in an election. We model privacy with respect to a tar-
get function T that models the information that the adversary is
interested in. This is an important parameter of our deﬁnition as it
allows modelling multiple scenarios of interest. Examples of po-
tential adversarial targets include the vote of one, some, or even
all voters. More complex information, e.g. whether two particular
voters voted for the same candidate or whether a particular subset
of voters supported a candidate more than others is also covered.
In addition to T , we aim to measure privacy along two dimen-
sions: the distribution of the votes D and the election protocol π.
Extreme situations where D contains no entropy or π simply re-
veals the vote of each participant entail no privacy. As soon as
there is some uncertainty on how honest voters vote and these votes
are somehow protected (e.g. cryptographically), then vote privacy
clearly increases.
The intuition behind our deﬁnition is simple: we capture the pri-
vacy of the information targeted by the adversary T (D) as the en-
tropy left in the target given what the adversary learns from the
voting process. The relation between the information in the target
and the view of the adversary can be looked at from various angles.
So we use the abstract notion of conditional privacy measure to en-
compass, succinctly, the different variants. We deﬁne two versions
of privacy, against bounded and unbounded adversaries.
DEFINITION 4. Let π be a voting protocol for result function ρ,
D a distribution on the honest votes, and T a target function. Let
A be the class of efﬁcient adversaries, I the class of unbounded
adversaries, and F be a computational privacy measure. The com-
putational privacy M (D, T, π) is deﬁned by
Fc(T(D) | RD,vA,π, View(A, π(D)))
inf
A∈A
Information theoretic privacy of M I (D, T, π) is deﬁned as
F(T(D) | RD,vA,π, View(A, π(D)))
inf
A∈I
Notice that the above deﬁnition in fact introduces a family of pri-
vacy measures MF, one for each ﬁxed conditional privacy measure
F. Our intention is to let F vary over the different existing notions
of entropy and rely on their associated intuition to understand the
guarantees entailed for the privacy of votes by the resulting mea-
sures. Indeed, we are convinced that evaluating our privacy mea-
sure for different entropy notions gives answers to different natural
questions that voters might have about the conﬁdentiality of their
vote.
The Choice of Entropy Notions
We discuss here different variants based on different types of Rényi
entropies (min, Hartley and Shannon) and different forms of con-
ditional entropies (average, minimal and conditional).
example:
on 100 choices (it can therefore be ﬁlled in 2100 ways).
For the sake of our discussion, we consider the following election
• The ballot takes the form of one question asking for approval
• The distribution of the votes by the honest voters is uniform,
except for a couple voters P1 and P2 , who vote as follows:
with probability 1/2, they agree on their choices before vot-
ing and vote exactly in the same way (one single uniform
choice for both) but, if they disagree, then their choices are
simply indepedent (uniform distribution on all pairs of dis-
tinct votes).
• The tallying function ρ reveals the vote of P1 and P2 if they
• The target is the vote of P1.
Let us now consider the privacy of P1 with for the vote distribu-
are equal and reveals nothing otherwise.
tion and the ρ function above.
2 · 1 + 1
Min-entropy based notions. A ﬁrst natural question for P1 is:
“What is the probability that an observer will be able to guess my
vote?” The answer to this question is given by using min-entropy,
which provides a measure of the success probability of the best
guess that an observer can make. If the election outcome ρ is not
empty, which happens with probability 1/2, the observer can make
a correct guess with probability 1. Otherwise, the probability of
success is 2−100. Using average min-entropy as our measure, we
get a measure of the success probability of approximately one bit:
2 · 2−100´ ≈ 1. This single bit of
˜H∞(v1 | ρ) = − log` 1
entropy is in line with the behaviour of our system: on average,
an observer will be able to make a correct guess with probability
2 . Now, if we use min-min-entropy as
just slightly higher than 1
our measure, we get a measure of the success probability given the