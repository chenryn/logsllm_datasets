data for the same technology [72].
Using the F ITraw we can predict the FIT rate of appli-
cations based on the AVF analysis, as shown in Figure 5. In
order to compare the FIT rate predicted with beam and fault
injection, for each code we divide the highest FIT rate between
the one calculated with beam data and the one predicted
using fault injection by the lowest FIT rate between the two.
Whenever the fault injection FIT rate is higher than the beam
one we represent the value as positive, negative otherwise.
Figure 6 shows the comparison between beam and fault
injection SDC FIT rates for all 13 benchmarks. The positive
values (towards the right) of the horizontal axis indicate how
many times the FIT rate measured with beam experiments is
higher than the FIT rate calculated with fault injection while
negative numbers indicate the opposite (fault injection FIT
rates are higher). For most benchmarks radiation and fault
injection give very close FIT rates (for 10 out of 13 codes
the difference is smaller than 4x, while for 7 of them it is
Fig. 6.
injection.
SDC FIT comparison between radiation experiments and fault
less than 2x). MatMul, StringSearch, and CRC32 have the
largest difference between the two FIT rate measurements.
However, these benchmarks have very low SDC FIT rate,
for instance, StringSearch has 5.45 SDC FIT on the radiation
experiment and only 0.34 on the fault injection, meaning that
the absolute difference between FIT rates is very small and
such differences are within the statistical error. As expected
from the discussion in Section II, for most of the benchmarks
the FIT rate measured with beam experiment is higher than the
fault injection one. However, we observe that for 5 benchmarks
(Rijndael, Jpeg D, FFT, Dijkstra, and, mainly, MatMul), fault
injection reports a higher SDC FIT rate than beam experi-
ments. As discussed in the following sections, these are also
the benchmarks that show a much higher Application Crash
and System Crash rate for beam experiments compared to fault
injection and this difference implies that some faults propagate
differently to generate SDCs or Crashes in the two setups but
still result in a corruption of the correct execution.
Fig. 7. Application Crash FIT comparison between radiation experiments and
fault injection.
When comparing the Application Crash FIT rate calculated
with the two setups, as shown in Figure 7, we observe that the
differences between fault injection and beam experiments are
much higher than for SDCs, ranging from 1.5x to almost 500x
(horizontal axis is in logarithmic scale). It is worth noting that
beam experiments FIT is always higher than the fault injection
34
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8. System crash FIT comparison between radiation experiments and fault
injection.
Fig. 9. SDC and Application Crash Comparison between radiation experi-
ments and fault injection.
estimation, which is expected since Application Crashes could
be triggered by corruption in logic/control hardware elements
which are difﬁcult to simulate [39]. For three benchmarks,
StringSearch, MatMul, and Qsort,
the difference between
beam experiments and fault injection is close or bigger than
two orders of magnitude (while for all others the difference is
smaller than 22x). The reason behind this may be attributed to
differences between the two setups. While fault injection ex-
periments output is downloaded and compared off-line against
the fault-free output to detect SDCs, beam experiments require
an additional routine for on-line SDC checking. As during
beam experiments most executions are error-free, downloading
all outputs would be an unnecessary waste of space and
time. These checks are almost transparent to the workload
characteristics and, to avoid the corruption of SDC details,
they are intentionally designed to hold pointer references
instead of actual data. Application Crashes are mostly sourcing
in abnormalities caused in the program ﬂow (i.e., irregular
branches, wrong memory references, etc.). These have roots
in the executed code of a program (of what is placed in the
.text section of a program). The common property of the 3
workloads with greater differences between fault injection and
beam data (StringSearch, MatMul, and Qsort) is the relatively
small code size, which ﬁts inside the Instruction Cache. As
a result, the code sits in the cache for the whole experiment,
being exposed to neutrons. Additionally, there is enough space
for the SDC check routines to remain in the cache hierarchy,
instead of being evicted during the program execution (as L2
is shared) during the beam experiments. We believe that the
exposure of these routines to the beam (which mainly consist
of pointers) would result in segmentation faults that translate
to Application Crashes. This is inevitable difference between
the two setups could explain the observed behavior for the
three outliers.
The System Crash FIT difference, as shown in Figure 8,
does not follow the same behavior of the Application Crash
FIT. We observe a high difference between radiation and fault
injection for all benchmarks, with the radiation FIT being
always higher than the injection. The difference ranges from
about 9 times (CRC32) to about 287 times (MatMul). The
benchmarks with the largest difference are MatMul, Dijkstra,
StringSearch, and the three Susans. These workloads also
happen to have the smallest inputs. As a result, they actually
leave a large part of the cache hierarchy unused. We believe
that the observed differences can again be induced by differ-
ences on the setups. In fault injection experiments, this portion
remains empty as the caches are reset on every experiment,
while in radiation experiments this space is used by the kernel
for other system operations (e.g. scheduling routines, timer
handlers etc.). The introduction of faults in these regions that
will most likely result to system crashes only in the radiation
experiments. The rest of the benchmarks that use most of the
cache hierarchy do evict the kernel from the caches and do
not suffer from this scenario.
As we have shown in Figures 6, 7, and 8, the magnitude of
the differences between beam experiment and fault injection
FIT rate are application dependent while there exists a clear
trend of larger FIT rates measured by beam experiment
compared to fault injection for all FIT rates (SDC, Appli-
cation Crash, and System Crash FIT rates). As discussed in
Section II, there are various reasons for fault injection and
beam experiments to provide different FIT rates and this is
the reason why they have to be considered complementary
to each other. One very likely reason for the differences and
particularly of the larger number of corruptions in the beam
experiments are the resources that are not modeled in the
simulator. We believe that this high System Crash FIT rate is a
peculiar characteristic of the Xilinx Zynq platform, speciﬁcally
the FPGA-ARM interface based on interrupts, which cannot be
further investigate without detailed (proprietary) information.
An indirect way to correlate the results and focus on the
same hardware is by attributing the effects that different
hardware parts cause. While System Crashes, exceptions and
wrong memory accesses can be caused by most of the system’s
components (including CPU cores, peripherals, controllers,
bridges and interconnections), SDCs can only occur at the
components that produce the output, which is the CPU core.
This also partially applies to the majority of Application
Crashes. In Figure 9 we plot the relative difference of the sum
of the FIT rates for SDCs and Application Crashes measured
35
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
with beam and fault injection. StringSearch has the highest
relative difference (of about 100x). This is due to the extremely
small number of SDCs observed in both setups, having much
less events observed in the injection than in the radiation setup.
It is also interesting that the MatMul and Qsort, that show a
difference of 100 times in Application Crash FIT (Figure 7),
now have a difference lower than 10 times when comparing
SDC and Application Crash FIT rates (Figure 9). This means
that, the overall FIT rate is only 10x higher in the beam case.
It is likely that some of the Application Crashes observed
in the radiation experiment are observed as SDCs in the
injection. This is probably caused by the corruption of some
hardware resource not modeled in the injection setup. The
other benchmarks are less affected by the code characteristics
discussed previously. For three benchmarks, Jpeg D and the
two Rijndael, the overall FIT difference is very small, from
1.08x up to 1.26x.
Figure 10 shows an aggregate view of our measurements.
It presents the differences between the average FIT rate of
the 13 benchmarks measured with beam experiment and fault
injection compared to the (unknown) real FIT rate of an ARM
Cortex-A9 CPU in the ﬁeld. As expected and discussed in
Section II, fault injection tends to report smaller device FIT
rates than beam experiments. For SDC rates (leftmost bars)
we can claim that both beam experiments and fault injection
provides, on the average, very similar FIT prediction. It is
reasonable to believe that the real SDC FIT rate of the device
lays between the two values. When Crashes are considered
(especially System Crashes - rightmost bars), the difference
between beam and fault injection increases. As discussed, this
is mainly due to un-modelled structures in fault injection and
to the fact that a full system is massively irradiated with the
beam. However, still the FIT rates including Crashes have a
difference which is smaller or in the worst case equal to an
order of magnitude (in our case the FIT rates difference when
Application Crashes are added to SDCs is only 4.3x while
the total FIT rate when Application and System Crashes are
added to the SDCs - Total FIT in the rightmost bars - is only
10.9x). Again, based on our analysis, we can claim that the real
FIT rate of the evaluated CPU may sit between the FIT rates
values provided by beam experiments and fault injection and
this rather narrow range can drive early informed decisions by
the chip designers about soft error protection techniques for a
particular CPU.
VII. CONCLUSIONS
We presented the ﬁrst detailed analysis that aims to report
a head-to-head comparison of two very popular reliability
assessment methods: (a) physical accelerated beam test of
an ARM Cortex-A9 CPU and (b) fault
injection on the
corresponding model of the ARM Cortex-A9 CPU on the
state-of-the-art microarchitectural simulator Gem5. For an as-
close-as-possible comparison, we maximize the equivalence
of the physical system setup and the simulated system setup:
hardware conﬁguration, application software, and operating
system.
Fig. 10. Overview of the comparison between beam and fault injection FIT
rates (compare with the motivation of the paper shown in Figure 1). Fault
injection average FIT rate (grey bars) is dominated by the SDC FIT rate
(leftmost bars) and is only slightly increased when the Application Crashes
are added (middle bars) or the System Crashes are also added (rightmost bars).
On the contrary, the Beam FIT rates (purple bars) are increased when the two
types of Crashes are added. However, the SDC FIT rate of Beam is very close
to the Fault Injection SDC FIT rate and also the differences when one or both
types of Crashes are added are close to only one order of magnitude.
The comparison of
the two reliability assessment ap-
proaches helps in bounding the range of the expected FIT
rates of a CPU when it is deployed in a ﬁnal system in
the ﬁeld. We have shown that for the diverse set of bench-
marks employed in our experiments, the FIT rates differences
between accelerated beam test and microarchitectural fault
injection can be extremely small (when only the SDC FIT rate
is considered) and does not exceed one order of magnitude
when all types of errors (including Application and System
Crashes) are considered for the Total FIT rate of the system.
The insights of our study can assist CPU designers in making
informed decisions about the soft error protection mechanisms
best suited to a particular hardware and software combination.
ACKNOWLEDGMENT
This work is partially funded by the H2020 Framework
Program of the European Union through the UniServer Project,
under Grant Agreement 688540, by the 7th Framework Pro-
gram of the European Union through the CLERECO Project,
under Grant Agreement 611404, by the Coordenao de Aper-
feioamento de Pessoal de Nvel Superior - Brasil (CAPES) -
Finance Code 001, and by the project FAPERGS 17/2551-
0001 202-0.
REFERENCES
[1] R. Lucas, “Top ten exascale research challenges,” in DOE ASCAC
Subcommittee Report, 2014.
[2] J. Dongarra, H. Meuer, and E. Strohmaier, “ISO26262 Standard,” 2015.
[Online]. Available: https://www.iso.org/obp/ui/#iso:std:iso:26262:-1:ed-
1:v1:en
[3] A. Cohen, X. Shen, J. Torrellas, J. Tuck, Y. Zhou, S. Adve, I. Akturk,
S. Bagchi, R. Balasubramonian, R. Barik, M. Beck, R. Bodik, A. Butt,
L. Ceze, H. Chen, Y. Chen, T. Chilimbi, M. Christodorescu, J. Criswell,
C. Ding, Y. Ding, S. Dwarkadas, E. Elmroth, P. Gibbons, X. Guo,
R. Gupta, G. Heiser, H. Hoffman, J. Huang, H. Hunter, J. Kim, S. King,
J. Larus, C. Liu, S. Lu, B. Lucia, S. Maleki, S. Mazumdar, I. Neamtiu,
K. Pingali, P. Rech, M. Scott, Y. Solihin, D. Song, J. Szefer, D. Tsafrir,
B. Urgaonkar, M. Wolf, Y. Xie, J. Zhao, L. Zhong, and Y. Zhu, “Inter-
disciplinary research challenges in computer systems for the 2020s,”
USA, Tech. Rep., 2018.
36
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
[4] M. Snir, R. W. Wisniewski, J. A. Abraham, S. V. Adve, S. Bagchi,
P. Balaji, J. Belak, P. Bose, F. Cappello, B. Carlson et al., “Addressing
failures in exascale computing,” International Journal of High Perfor-
mance Computing Applications, pp. 1–45, 2014.
[5] R. R. Lutz, “Analyzing software requirements errors in safety-critical,
embedded systems,” in Requirements Engineering, 1993., Proceedings
of IEEE International Symposium on, Jan 1993, pp. 126–133.
[6] J. C. Laprie, “Dependable computing and fault tolerance : Concepts
and terminology,” in Fault-Tolerant Computing, 1995, Highlights from
Twenty-Five Years., Twenty-Fifth International Symposium on, Jun 1995,
pp. 2–.
[7] M. Nicolaidis, “Time redundancy based soft-error tolerance to rescue
nanometer technologies,” in VLSI Test Symposium, 1999. Proceedings.
17th IEEE, 1999, pp. 86–94.
[8] R. Baumann, “Soft errors in advanced computer systems,” IEEE Design
Test of Computers, vol. 22, no. 3, pp. 258–266, May 2005.
[9] C. Constantinescu, “Impact of deep submicron technology on depend-
ability of vlsi circuits,” in Dependable Systems and Networks, 2002. DSN
2002. Proceedings. International Conference on, 2002, pp. 205–209.
[10] G. P. Saggese, N. J. Wang, Z. T. Kalbarczyk, S. J. Patel, and R. K. Iyer,
“An experimental study of soft errors in microprocessors,” IEEE Micro,
vol. 25, no. 6, pp. 30–39, Nov 2005.
[11] B. Schroeder, E. Pinheiro, and W.-D. Weber, “Dram errors in the wild:
A large-scale ﬁeld study,” in SIGMETRICS, 2009.
[12] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin,
“A Systematic Methodology to Compute the Architectural Vulnerability
Factors for a High-Performance Microprocessor,” in Proceedings of the
36th Annual IEEE/ACM International Symposium on Microarchitecture.
Washington, DC, USA: IEEE Computer Society, 2003, pp. 29–.
[13] A. Chatzidimitriou and D. Gizopoulos, “Anatomy of microarchitecture-
level
reliability assessment: Throughput and accuracy,” in 2016
IEEE International Symposium on Performance Analysis of Systems
and Software (ISPASS).
[Online]. Available:
https://doi.org/10.1109/ispass.2016.7482075
IEEE, Apr 2016.
[14] M. Kaliorakis, D. Gizopoulos, R. Canal, and A. Gonzalez, “MeRLiN,”
in Proceedings of
International Symposium on
Computer Architecture - ISCA '17. ACM Press, 2017. [Online].
Available: https://doi.org/10.1145/3079856.3080225
the 44th Annual
[15] N. Hemsoth. (2018) Arm it the nnsas new secret weapon. [Online].
Available: https://www.nextplatform.com/2018/11/07/arm-is-the-nnsas-
new-secret-weapon/
[16] NASA, “Phonesat project,” https://www.nasa.gov/content/phonesat/,
2016.
[17] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge,
and R. B. Brown, “Mibench: A free, commercially representative
embedded benchmark suite,” in Proceedings of the Fourth Annual IEEE
International Workshop on Workload Characterization. WWC-4 (Cat.
No.01EX538), Dec 2001, pp. 3–14.
[18] JEDEC, “Measurement and Reporting of Alpha Particle and Terrestrial
Cosmic Ray-Induced Soft Errors in Semiconductor Devices,” JEDEC
Standard, Tech. Rep. JESD89A, 2006.
[19] N. Mahatme, T. Jagannathan, L. Massengill, B. Bhuva, S.-J. Wen, and