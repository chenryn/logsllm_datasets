title:Modeling Input-Dependent Error Propagation in Programs
author:Guanpeng Li and
Karthik Pattabiraman
Modeling Input-Dependent Error Propagation in
Programs
Guanpeng Li and Karthik Pattabiraman
University of British Columbia
{gpli, karthikp}@ece.ubc.ca
Abstract—Transient hardware faults are increasing in com-
puter systems due to shrinking feature sizes. Traditional methods
to mitigate such faults are through hardware duplication, which
incurs huge overhead in performance and energy consumption.
Therefore, researchers have explored software solutions such
as selective instruction duplication, which require ﬁne-grained
analysis of instruction vulnerabilities to Silent Data Corruptions
(SDCs). These are typically evaluated via Fault Injection (FI),
which is often highly time-consuming. Hence, most studies conﬁne
their evaluations to a single input for each program. However,
there is often signiﬁcant variation in the SDC probabilities of both
the overall program and individual instructions across inputs,
which compromises the correctness of results with a single input.
In this work, we study the variation of SDC probabilities
across different inputs of a program, and identify the reasons for
the variations. Based on the observations, we propose a model,
VTRIDENT, which predicts the variations in programs’ SDC
probabilities without any FIs, for a given set of inputs. We ﬁnd
that VTRIDENT is nearly as accurate as FI in identifying the
variations in SDC probabilities across inputs. We demonstrate
the use of VTRIDENT to bound overall SDC probability of a
program under multiple inputs, while performing FI on only a
single input.
Keywords—Error Propagation, Soft Error, Silent Data Corrup-
tion, Error Resilience, Program Analysis, Multiple Inputs
I.
INTRODUCTION
Transient hardware fault probabilities are predicted to
increase in future computer systems due to growing system
scales, progressive technology scaling, and lowering operat-
ing voltages [29]. While such faults were masked through
hardware-only solutions such as redundancy and voltage guard
bands in the past, these techniques are becoming increasingly
challenging to deploy as they consume signiﬁcant amounts of
energy, and as energy is becoming a ﬁrst class constraint in
microprocessor design [5]. As a result, software needs to be
able to tolerate hardware faults with low overheads.
Hardware faults can cause programs to fail by crashing,
hanging or producing incorrect program outputs, also known
as silent data corruptions (SDCs). SDCs are a serious concern
in practice as there is no indication that the program failed, and
hence the results of the program may be taken to be correct.
Hence, developers must ﬁrst evaluate the SDC probability of
their programs, and if it does not meet their reliability target,
they need to add protection to the program until it does.
Fault injections (FIs) are commonly used for evaluating and
characterizing programs’ resilience, and to obtain the overall
SDC probability of a program. In each FI campaign, a single
fault is injected into a randomly sampled instruction, and the
program is executed till it crashes or ﬁnishes. FI therefore
requires that the program is executed with a speciﬁc input. In
practice, a large number of FI campaigns are usually required
to achieve statistical signiﬁcance, which can be extremely
time-consuming. As a result, most prior work limits itself to
a single program input or at most a small number of inputs.
Unfortunately, the number of possible inputs can be large, and
there is often signiﬁcant variance in SDC probabilities across
program inputs. For example, in our experiments, we ﬁnd that
the overall SDC probabilities of the same program (Lulesh)
can vary by more than 42 times under different inputs. This
seriously compromises the correctness of the results from FI.
Therefore, there is a need to characterize the variation in SDC
probabilities across multiple inputs, without expensive FIs.
We ﬁnd that there are two factors determining the variation
of the SDC probabilities of the program across its inputs (we
call this the SDC volatility): (1) Dynamic execution footprint
of each instruction, and (2) SDC probability of each instruction
(i.e., error propagation behaviour of instructions). Almost all
existing techniques [7], [8], [10] on quantifying programs’
failure variability across inputs consider only the execution
footprint of instructions. However, we ﬁnd that
the error
propagation behavior of individual instructions often plays as
important a role in inﬂuencing the SDC volatility (Section III).
Therefore, all existing techniques experience signiﬁcant inac-
curacy in determining a program’s SDC volatility.
In this paper, we propose an automated technique to
determine the SDC volatility of a program across different
inputs, that takes into account both the execution footprint of
individual instructions, and their error propagation probabili-
ties. Our approach consists of three steps. First, we perform
experimental studies using FI to analyze the properties of SDC
volatility, and identify the sources of the volatility. We then
build a model, VTRIDENT, which predicts the SDC volatility
of programs automatically without any FIs. VTRIDENT is
built on our prior model, TRIDENT [11] for predicting
error propagation, but sacriﬁces some accuracy for speed of
execution. Because we need to run VTRIDENT for multiple
inputs, execution speed is much more important than in the
case of TRIDENT. The intuition is that for identifying the
SDC volatility, it is more important to predict the relative
SDC probabilities among inputs than the absolute probabilities.
Finally, we use VTRIDENT to bound the SDC probabilities
of a program across multiple inputs, while performing FI on
only a single input. To the best of our knowledge, we are the
ﬁrst to systematically study and model the variation of SDC
probabilities in programs across inputs.
The main contributions are as follows:
namely
that
• We identify two sources of SDC volatility in
INSTRUCTION-EXECUTION-
programs,
of
VOLATILITY
dynamic execution footprint of
instructions, and
INSTRUCTION-SDC-VOLATILITY that captures the
variability of error propagation in instructions, and
mathematically derive their relationship (Section III).
variation
captures
the
•
•
•
To understand how SDC probabilities vary across
inputs, we conduct a FI study using nine bench-
marks with ten different program inputs for each
benchmark,
contribu-
tion of INSTRUCTION-EXECUTION-VOLATILITY and
INSTRUCTION-SDC-VOLATILITY (Section IV) to the
overall SDC volatility.
and quantify the
relative
Based on the understanding, we build a model, VTRI-
DENT1, on top of our prior framework for model-
ing error propagation in programs TRIDENT (Sec-
tion V-B). VTRIDENT predicts the SDC volatility of
instructions without any FIs, and also bounds the SDC
probabilities across a given set of inputs.
Finally, we evaluate the accuracy and scalability of
VTRIDENT in identifying the SDC volatility of in-
structions (Section VI), and in bounding SDC proba-
bilities of program across inputs (Section VII).
Our main results are as follows:
•
Volatility of overall SDC probabilities is due to
both the
INSTRUCTION-EXECUTION-VOLATILITY
and INSTRUCTION-SDC-VOLATILITY. Using only
INSTRUCTION-EXECUTION-VOLATILITY to predict
the overall SDC volatility of the program results
in signiﬁcant inaccuracies, i.e., an average of 7.65x
difference with FI results (up to 24x in the worst case).
• We ﬁnd that the accuracy of VTRIDENT is 87.81%
when predicting the SDC volatility of individual in-
structions in the program. The average difference
between the variability predicted by VTRIDENT and
that by FI is only 1.26x (worst case is 1.29x).
• With VTRIDENT 78.89% of the given program
inputs’ overall SDC probabilities fall within the
predicted bounds. With INSTRUCTION-EXECUTION-
VOLATILITY alone, only 32.22% of the probabilities
fall within the predicted bounds.
•
Finally, the average execution time for VTRIDENT
is about 15 minutes on an input of nearly 500 million
dynamic instructions. This constitutes a speedup of
more than 8x compared with the TRIDENT model to
bound the SDC probabilities, which is itself an order
of magnitude faster than FI [11].
A. Fault Model
In this paper, we consider transient hardware faults that
occur in the computational elements of the processor, including
pipeline stages, ﬂip-ﬂops, and functional units. We do not
consider faults in the memory or caches, as we assume that
these are protected with ECC. Likewise, we do not consider
faults in the processor’s control
is
protected. Neither do we consider faults in the instructions’
encoding as these can be detected through other means such
as error correcting codes. Finally, we assume that program does
not jump to arbitrary illegal addresses due to faults during the
execution, as this can be detected by control-ﬂow checking
techniques [27]. However, the program may take a faulty legal
branch (execution path is legal but branch direction can be
wrong due to faults propagating to it). Our fault model is in
line with other work in the area [6], [9], [14], [24], [33].
logic as we assume it
B. Terms and Deﬁnitions
•
•
•
•
•
•
•
Fault Occurrence: The event corresponding to the
occurrence of transient hardware fault in the processor.
The fault may or may not result in an error.
Fault Activation: The event corresponding to the
manifestation of the fault to the software, i.e., the fault
becomes an error and corrupts some portion of the
software state (e.g., register, memory location). The
error may or may not result in a failure (i.e., SDC,
crash or hang).
Crash: The raising of a hardware trap or exception
due to the error, because the program attempted to
perform an action it should not have (e.g., read outside
its memory segments). The OS terminates the program
as a result.
Silent Data Corruption (SDC): A mismatch between
the output of a faulty program run and that of an error-
free execution of the program.
Benign Faults: Program output matches that of the
error-free execution even though a fault occurred
during its execution. This means either the fault was
masked, or overwritten by the program.
Error propagation: Error propagation means that
the fault was activated, and has affected some other
portion of the program’s state, say ’X’. In this case, we
say the fault has propagated to state X. We focus on
the faults that affect the program state, and therefore
consider error propagation at the application level.
SDC Probability: We deﬁne the SDC probability as
the probability of an SDC given that the fault was
activated – other work uses a similar deﬁnition [9],
[13], [21], [22], [31], [34].
II. BACKGROUND
In this section, we ﬁrst present our fault model, then deﬁne
the terms we use, and the software infrastructure we work with.
1VTRIDENT stands for “Volatility Prediction for TRIDENT”.
C. LLVM Compiler and LLVM Fault Injector
In this paper, we use the LLVM compiler [20] to perform
the program analysis, FI experiments, and to implement our
model. Our choice of LLVM is motivated by three reasons.
First, LLVM uses a typed intermediate representation (IR) that
can easily represent source-level constructs. In particular, it
preserves the names of variables and functions, which makes
source mapping feasible. This allows us to perform a ﬁne-
grained analysis of which program locations cause certain
failures and map it to the source code. Secondly, LLVM IR
is a platform-neutral representation that abstracts out many
low-level details of the hardware and assembly language.
This greatly aids in portability of our analysis to different
architectures, and simpliﬁes the handling of the special cases
of different assembly language formats. Finally, LLVM IR has
been shown to be accurate for doing FI studies [31], and there
are many fault injectors developed for LLVM [1], [22], [28],
[31]. Most of the papers we compare with in this study also use
LLVM infrastructure [9], [21]. Therefore, in this paper, when
we say instruction, we mean an instruction at the LLVM IR
level. However, our methodology is not tied to LLVM.
We use LLVM Fault Injector (LLFI) [31] to perform
FI experiments. LLFI is found to be accurate in studying
SDCs [31]. Since we consider transient errors that occur in
computational components, we inject single bit ﬂips in the
return values of the target
instruction randomly chosen at
runtime. We consider single bit ﬂips as this is the de-facto
fault model for simulating transient faults in the literature [9],
[21], [14]. Although there have been concerns expressed about
the representativeness of using single-bit ﬂip faults for FI to
model soft errors [4], a recent study [28] has shown that there
is very little difference in SDC probabilities due to single and
multiple bit ﬂips at the application level. Since we focus on
SDCs, we use single bit ﬂips in our evaluation.
III. VOLATILITIES AND SDC
In this section, we explain how we calculate the overall
SDC probability of a program under multiple inputs. Statistical
FI is the most common way to evaluate the overall SDC
probability of a program and has been used in other related
work in the area [7], [10], [14], [15], [21]. It randomly injects
a large number (usually thousands) of faults under a given
program input, one fault per program execution, by uniformly
choosing program instruction for injection from the set of all
executed instructions.
Equation 1 shows the calculation of the overall SDC
probability of the program, Poverall, from statistical FI. NSDC