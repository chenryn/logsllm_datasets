### Optimized Text

#### Cluster Prediction and Weight Update
The analyzed sample is fed back into the cluster prediction phase for learning. Consequently, we update the classifier's weight matrix \( w \). In the next iteration of FORECAST, before selecting the next \( L \) samples, we recompute the probability matrix \( p \) for all remaining candidate samples and recalculate their sample scores. This requires repeating the prediction step for each sample in \( \chi \setminus \alpha \). The actual size of \( \chi \) depends on how FORECAST is deployed in practice. Our simulation results in Section 4.3 are based on one day of malware samples from a large-scale sandbox deployment, where \( \chi \) can contain tens of thousands of samples. This step is one of the more computationally expensive parts of FORECAST's operation. However, it only needs to be performed once every \( L \) samples. As shown in Section 4.4, with realistic levels of parallelism, this leads to more than acceptable performance.

### Evaluation

To develop FORECAST, we used a dataset of malware samples analyzed by an analysis sandbox in October, November, and December 2008, consisting of 100,408 samples. For evaluation, we used a larger, more recent dataset from July, August, and September 2010, comprising 643,212 samples.

**Table 1: Distribution of Features in the 2010 Dataset**

| Feature Group            | Total   | Average |
|--------------------------|---------|---------|
| PE section names         | 42,389  | 4       |
| Imported DLLs            | 12,386  | 5       |
| peHash                   | 1       | 1       |
| Static clusters          | 218,380 | 1       |
| Packing Level            | 203,078 | 1       |
| AV labels                | 1       | 1       |
| Submitter information    | 250,852 | 1909    |
| **Total**                | 729,007 | 21      |

We determined FORECAST’s parameters by testing our system on the 2008 dataset. For behavioral clustering, we used the same parameters and distance threshold as in [14]. FORECAST trains an OVA (One-Versus-All) classifier for each cluster \( C \) such that \( |C| \geq \theta \). Samples in smaller clusters are assigned to the "other" cluster \( O \). For our experiments, we selected \( \theta = 20 \). Decreasing \( \theta \) beyond this point leads to a slow decrease in \( |O| \), while causing a sharp increase in the number of behavioral clusters.

#### 4.1 Cluster Prediction

**Table 2: Contribution of Feature Groups to Cluster Prediction Accuracy for the 2010 Dataset**

| Feature Group            | Prediction Accuracy Using All but FG | Prediction Accuracy Using Only FG |
|--------------------------|--------------------------------------|-----------------------------------|
| None                     | 68%                                 | -                                 |
| PE section names         | 67%                                 | 45%                               |
| Imported DLLs            | 66%                                 | 40%                               |
| peHash                   | 66%                                 | 45%                               |
| Static clusters          | 66%                                 | 45%                               |
| Packing level            | 68%                                 | 52%                               |
| AV labels                | 65%                                 | -                                 |
| Submitter information    | 68%                                 | -                                 |

To assess the accuracy of FORECAST’s cluster prediction, we trained the classifier on the samples from the first month and measured its accuracy on those from the last two months. Since FORECAST is incremental and adapts with every analyzed sample, a dedicated training set is not strictly necessary. However, bootstrapping the system with initial data is desirable to ensure predictions are based on reasonable knowledge. For the 2010 dataset, the training set from July 2010 consists of 193,726 samples, and the testing set from August and September 2010 includes 449,486 samples. Samples are processed for prediction and training in chronological order.

The 2010 dataset includes a total of 1,303 behavioral clusters, including the "other" cluster \( O \). The first line of Table 2 shows the cluster prediction accuracy when using all static features. For 68% of the 449,486 samples, the classifier assigned the highest probability to the correct behavioral cluster out of the 1,303 clusters. The following lines show the classifier's accuracy if it is trained without features from one of the groups listed in Table 1 (left column) and if it is trained using exclusively features from a single group (right column). Table 2 provides insight into the contribution of each feature group to the classifier's prediction accuracy. Removing any single feature group does not cause large drops in accuracy due to high correlation among features. For instance, peHash and static clustering from [23] lead to similar classifications of malware binaries. Therefore, removing peHash features causes only a modest decrease in accuracy because static features provide similar information. Similarly, samples in each static cluster are typically assigned only a handful of different AV labels. Nonetheless, no single feature group, on its own, is sufficient to achieve comparable classification accuracy.

#### 4.2 Cluster Scoring

The scoring functions proposed for FORECAST are designed to assist in the selection of samples for specific analysis goals by measuring the value of the information provided by an analysis run. It is important to verify that these scoring functions indeed encourage the selection of relevant samples. For each scoring function, we ranked the 1,303 clusters in the 2010 dataset by their cluster score \( v(C)/|C| \) and manually assessed some of the highest- and lowest-scoring clusters, as well as some of the largest clusters in the dataset. Due to space constraints, we present results only for the network endpoints scoring function. Results for the persistent changes scoring function are available in an extended version of the paper [30].

The network endpoints scoring function is designed to encourage the analysis of samples likely to reveal new C&C servers. Therefore, we would expect the highest-ranked clusters for this score to belong to bots and other remote-controlled malware, especially those with highly redundant or dynamic C&C infrastructure. Table 3 shows significant clusters of the dataset ranked by the network endpoints score, where the feature count is the total number of potential C&C servers detected in the cluster.

**Table 3: Selected Clusters Ranked by Network Endpoint Score**

| Rank | Size | Feature Count | Malware Family        |
|------|------|---------------|-----------------------|
| 1    | 36   | 84            | Unknown Bot           |
| 2    | 20   | 20            | Harebot               |
| 3    | 29   | 26            | Cutwail               |
| ...  | ...  | ...           | ...                   |
| 6    | 31   | 26            | Adware Adrotator      |
| 8    | 24   | 19            | Vundo / Zlob          |
| 15   | 32   | 141           | Bredolab              |
| 28   | 51   | 82            | Koobface              |
| 36   | 67   | 173           | Swizzor               |
| 67   | 121272 | 0            | zBot                  |
| Last | 2596 | 0             | Unknown Downloader    |
| Last | 0    | 0             | Allaple/Rahack (No activity) |

Almost all top-ranked clusters belong to remote-controlled bots or trojans. Several are associated with malware families known to use highly dynamic C&C infrastructure. For example, the Pushdo/Cutwail botnet, found at rank three, contains a frequently updated list of IP addresses. The malware contacts these addresses over HTTP to download a binary payload, typically an instance of the Cutwail spam engine. Cutwail then proceeds to obtain templates and instructions for spamming from other C&C servers over a custom binary protocol. Similarly, the Koobface botnet, at rank 28, uses compromised web pages as part of its C&C infrastructure. Bredolab, at rank 15, is a downloader similar to Pushdo [38].

Vundo/Zlob, at rank eight, is a Fake-AV downloader. These samples download binaries from multiple servers. This is a representative example of cases where a diversity-based selection strategy would cause the analysis sandbox to miss relevant behavior. The 26 samples in this cluster have only two distinct peHash values, and discarding the remaining 24 samples would cause most of the 19 C&C servers to remain undiscovered.

In a few cases, we assign a high network endpoints score to samples that are not necessarily of interest. The top non-relevant cluster is the Adrotator cluster at rank six. The samples in this cluster are clickbots that visit advertisement links to fraudulently generate advertisement revenue. Some of the advertisement servers in question are not in our adblock blacklist, contributing to the network endpoints score.

A total of 171,533 samples belong to clusters with a score of zero. Among these, an Allaple cluster performs network scans filtered by the scoring function, and another cluster performs no network activity at all.

#### 4.3 Simulation

To assess the real-world impact of performing sample selection using FORECAST, we conducted a trace-based simulation. We considered all samples that our sandbox was able to analyze during August and September 2010 and simulated a sandbox deployment with a smaller amount of resources, capable of analyzing only a specified percentage of these samples. This allowed us to compare FORECAST with other sample selection strategies and measure the effect of each strategy on the value of the analysis results. The fact that our existing sandbox is considered to have 100% capacity for this simulation does not mean it processes all available samples. However, we cannot include in our evaluation samples for which we do not have dynamic analysis results.

To simulate a FORECAST deployment, we split the last two months of the 2010 dataset according to the day on which each analysis result was produced. For each day, we performed sample selection using five different strategies, taking into account information obtained from sample analysis during the previous days:

- **Random**: Randomly select the samples for analysis. This simple selection strategy provides a baseline.
- **PeHash**: This strategy uses a sample’s peHash [41] to maximize the diversity of the analyzed samples. We randomly select a sample for each distinct peHash.

**Table 4: Comparison of Sample Selection Strategies**

| Strategy     | Potential C&C Servers Observed (L = 100) |
|--------------|-----------------------------------------|
| Random       | 100%                                    |
| PeHash       | 152%                                    |
| Static       | 136%                                    |
| FORECAST     | 234%                                    |
| Optimum      | 318%                                    |

With 15% sandbox capacity and \( L = 100 \), FORECAST allows us to observe 134% more potential C&C servers compared to a random selection strategy. Both peHash and static strategies perform significantly better than random selection, by 52% and 36%, respectively. This confirms the intuition from previous work [41, 23] that diversity is a good heuristic for sample selection. Nonetheless, FORECAST provides noticeable benefits compared to both strategies, outperforming peHash by 54% and static by 72%. This demonstrates that explicitly optimizing sample selection for an analysis goal can improve the overall value of the analysis results. The optimum selection strategy outperforms FORECAST by 37%, indicating room for improvement in FORECAST’s performance if the accuracy of its cluster prediction component can be increased, for instance, by considering additional static features or improving the classifier. Higher levels of parallelism have modest negative effects on performance. With \( L = 1600 \), FORECAST reveals only 4% fewer features than with \( L = 50 \). Note that a parallelism level of 1600 is over an order of magnitude larger than our current deployment.

**Figure 4: CDF of Necessary Samples to Get 60% of Network Features, \( L = 100 \)**

Figure 4 shows the cumulative distribution function, over the 61 days considered, of the time needed by a simulated sandbox with 100% capacity to observe 60% of relevant features. On the median day, a sandbox using FORECAST could observe 60% of features after analyzing about one-third of the daily samples in 7.2 hours. A sandbox using the random selection strategy would need 15 hours to provide the same number of potential C&C servers, while with the diversity-based sample selection strategies, about 10 hours would be needed. A secondary benefit of FORECAST is the faster response to new C&C servers.

#### 4.4 Performance

**Table 5: FORECAST Run-Time on 2010 Dataset, \( L = 100 \)**

| Task                    | Total (hours) | Per Sample (s) |
|-------------------------|---------------|-----------------|
| Feature Extraction      | 48            | 0.39            |
| Cluster Prediction      | 13            | 0.11            |
| Cluster Scoring         | 0.34          | <0.01           |
| Clustering              | 17            | 0.13            |
| **Total**               | 78.34         | 0.64            |

Since the aim of a sample selection strategy is to more efficiently use the computing resources available for dynamic analysis, it cannot itself require excessive resources. Clearly, deciding if a sample should be analyzed must be done quickly. Figure 3 shows daily simulation results for network features with \( L = 100 \).

---

This optimized text aims to be more coherent, clear, and professional, with improved structure and readability.