lyzed sample is fed back to the cluster prediction phase for learning.
Therefore we update the classiﬁer’s weight matrix w.
At the next iteration of FORECAST, before selecting the next L
samples, we recompute the probability matrix p for all remaining
candidate samples, and recompute their sample scores. For this,
we need to repeat the prediction step for each sample in χ \ α. The
actual size of χ depends on how FORECAST is deployed in prac-
tice. Our simulation results in Section 4.3 are obtained using one
day of malware samples from a large-scale sandbox deployment as
candidate set. In this scenario, χ can contain tens of thousands of
samples. Thus, this step is one of the more computationally expen-
sive in FORECAST’s operation. However, we only need to perform
this step once every L samples. As we will show in Section 4.4,
for realistic levels of parallelism this leads to more than acceptable
performance.
4. EVALUATION
To develop FORECAST, we used a dataset of malware samples
analyzed by an analysis sandbox in the months of October, Novem-
ber and December 2008. The resulting dataset consists of 100,408
samples. To evaluate FORECAST we use a larger, more recent
dataset, that includes all the samples analyzed in the months of
July, August and September of 2010. This 2010 dataset consists of
643,212 samples.
Table 1 shows an overview of the distribution of static features
among feature groups for the 2010 dataset. We can see that the
feature-space is large, with over 700 thousand distinct features.
However, the mean number of features for each sample is only 21.
For several of the feature groups, such as the peHash or static clus-
ters groups, each sample is in fact assigned exactly one feature.
Table 1: Distribution of features in the 2010 dataset.
Feature group
PE section names
Imported DLLs
peHash
Static clusters
Packing Level
AV labels
Submitter information
Total
Total Average
4
42389
12386
5
1
218380
1
203078
1
4
8
250852
1909
1
21
729007
We determined FORECAST’s parameters by testing our system
on the 2008 dataset. For the behavioral clustering, we use the same
parameters and distance threshold employed in [14]. Recall that
FORECAST trains a OVA classiﬁer for each cluster C such that
|C| ≥ θ. The samples in smaller clusters are instead assigned to
the other cluster O. For our experiments, we selected θ = 20.
Decreasing θ beyond this point leads to a slow decrease of |O|,
while causing a sharp increase in the number of behavioral clusters.
4.1 Cluster Prediction
Table 2: Contribution of feature groups to cluster prediction
accuracy for the 2010 dataset
Prediction Accuracy
Using all but FG Using only FG
Feature group
None
PE section names
Imported DLLs
peHash
Static clusters
Packing level
AV labels
Submitter information
68%
67%
66%
66%
66%
68%
65%
68%
-
45%
40%
45%
45%
52%
-
-
To assess the accuracy of FORECAST’s cluster prediction, we
train FORECAST’s classiﬁer on the samples from the ﬁrst month
and measure its accuracy on those of the last two months. Since
FORECAST is incremental and adapts with every sample that is an-
alyzed, a dedicated training set is not strictly necessary. However,
it is still desirable to bootstrap the system on some initial data, so
that predictions can be based on reasonable knowledge. For the
2010 dataset, the training set of July 2010 consists of 193,726 sam-
ples while the testing set of August and September 2010 includes
449,486 samples. Samples are processed for prediction and train-
ing in chronological order.
The 2010 dataset includes a total of 1303 behavioral clusters,
including the other cluster O. The ﬁrst line of Table 2 shows the
cluster prediction accuracy when using all static features. For 68%
of the 449,486 samples, our classiﬁer assigned the highest proba-
bility to the correct behavioral cluster (out of the 1303 behavioral
clusters). The following lines show the classiﬁer’s accuracy if it
is trained without features from one of the groups listed in Ta-
ble 1 (left column), and if it is trained using exclusively features
from a single group (right column). Table 2 thus provides some
insight into the contribution of each feature group to the classiﬁer’s
prediction accuracy. We can see that removing any single feature
group does not cause large drops in accuracy. The reason is that
features from the different groups are highly correlated. For in-
stance, peHash and the static clustering from [23] lead to similar
classiﬁcations of malware binaries. Therefore, removing the pe-
Hash features causes only a modest decrease in accuracy, because
the static features provide similar information. Likewise, samples
in each static cluster are typically assigned only a handful of dif-
ferent AV labels. Nonetheless, the right column shows that none of
the feature groups, on their own, are sufﬁcient to obtain comparable
classiﬁcation accuracy.
4.2 Cluster Scoring
The scoring functions proposed for FORECAST are designed to
assist the selection of samples for speciﬁc analysis goals, by mea-
suring the value of the information provided by an analysis run. It
is important to verify that the proposed scoring functions indeed
encourage the selection of samples that are relevant to the analy-
sis goals. For this, for each scoring function, we rank the 1,303
clusters in the 2010 dataset by their cluster score v(C)/|C|, and
manually assess some of the highest- and lowest-scoring clusters,
as well as some of the largest clusters in the dataset. For space
reason, we present results only for the network endpoints scoring
function. Results for the persistent changes scoring function are
available in an extended version of the paper [30].
The network endpoints scoring function is designed to encourage
the analysis of samples that are likely to reveal new C&C servers.
Therefore, we would expect the highest-ranked clusters for this
score to belong to bots and other remote controlled malware, and
especially to malware families that use a highly redundant or dy-
namic C&C infrastructure. Table 3 shows signiﬁcant clusters of
the dataset ranked by the network endpoints score. Here, the fea-
ture count is the total number of potential C&C servers detected in
this cluster.
Indeed, almost all top-ranked clusters belong to remote controlled
bots or trojans. Furthermore, several of these are associated with
malware families that are known to use highly dynamic C&C in-
frastructure. One example is the Pushdo/Cutwail botnet (discussed
extensively in [17]), found at rank three. Pushdo binaries contain a
frequently-updated list of IP addresses. The malware contacts these
addresses over HTTP to download a binary payload: typically an
instance of the Cutwail spam engine. Cutwail then proceeds to
obtain templates and instructions for spamming from other C&C
servers over a custom binary protocol. Likewise, the Koobface bot-
net, at rank 28, is known to use compromised web pages as part
of its C&C infrastructure. Bredolab, at rank 15, is a downloader
similar to Pushdo [38].
Vundo/Zlob at rank eight, is a Fake-AV downloader. These sam-
ples download binaries from a number of different servers. This is
a representative example of cases where a diversity-based selection
strategy would cause the analysis sandbox to miss relevant behav-
ior. The reason is that the 26 samples in this cluster have only
two distinct peHash values. Discarding the remaining 24 samples
would cause most of the 19 C&C servers to remain undiscovered.
In a few cases, however, we assign a high network endpoints
score to samples that we are not necessarily interested in. The top
non-relevant cluster is the Adrotator cluster at rank six. The sam-
ples in this cluster are clickbots: They visit advertisement links to
fraudulently generate advertisement revenue. In this case, some of
the advertisement servers in questions are not in our adblock black-
list, therefore they contribute to the network endpoints score.
A total of 171.533 samples belong to clusters with score zero.
Among these an Allaple cluster can be found, which performs net-
work scans that are ﬁltered by the scoring function as well as an-
other cluster performing no network activity at all.
Table 3: Selected clusters ranked by network endpoint score
Size
36
20
29
Feature Count
84
20
26
Malware Family
Unknown Bot
Harebot
Cutwail
Rank
1
2
3
. . .
6
8
15
28
36
67
199
273
. . .
Last
Last
31
26
67
141
82
173
121272
189285
16370
28179
Adware Adrotator
24
Vundo / Zlob
19
Bredolab
32
Koobface
51
Swizzor
27
zBot
33
4027
“other” Cluster
2596 Unknown Downloader
0
0
Allaple/Rahack
No activity
4.3 Simulation
To assess the real-world impact of performing sample selection
using FORECAST, we perform a trace-based simulation. For this,
we consider all of the samples that our sandbox was able to analyze
during August and September 2010, and simulate a sandbox de-
ployment with a smaller amount of resources, that is therefore able
to analyze only a speciﬁed percentage of these samples. This allows
us to compare FORECAST with other sample selection strategies,
and measure the effect of each strategy on the value of the analysis
results. The fact that our existing sandbox is considered, for the
purpose of this simulation, to have 100% capacity does not mean
that it is in reality able to process all available samples. However,
we clearly cannot include in our evaluation samples for which we
do not have dynamic analysis results. To simulate a FORECAST
deployment, we split up the last two months of the 2010 dataset
according to the day on which each analysis result was produced.
For each day, we then perform sample selection using ﬁve different
strategies, taking into account information obtained from sample
analysis during the previous days.
• Random: Randomly select the samples for analysis. This simple
selection strategy provides a baseline against which other strategies
can be evaluated.
• PeHash: This selection strategy uses a sample’s peHash [41] to
attempt to maximize the diversity of the analyzed samples. For this,
we randomly select a sample for analysis for each distinct peHash.
in Table 4. With 15% sandbox capacity and L = 100, FORE-
CAST allows us to observe 134% more potential C&C servers com-
pared to a random selection strategy. Furthermore, both peHash
and static perform signiﬁcantly better than random selection, by
52% and 36% respectively. This conﬁrms the intuition from pre-
vious work [41, 23] that diversity is a good heuristic for sample
selection. Nonetheless, FORECAST provides noticeable beneﬁts
compared to both strategies, outperforming peHash by 54%, and
static by 72%. This demonstrates that explicitly optimizing sample
selection for an analysis goal can improve the overall value of the
analysis results. It is also worth noting that the optimum selection
strategy outperforms FORECAST by 37%. This shows that there
is some margin for improving FORECAST’s performance if the ac-
curacy of its cluster prediction component can be increased, for
instance by considering additional static features or by improving
the classiﬁer. Table 4 also shows that higher levels of parallelism
have modest negative effects on performance. With L = 1600,
FORECAST reveals only 4% less features than with L = 50. Note
that a parallelism level of 1600 is over an order of magnitude larger
than our current deployment.
Figure 4: CDF of necessary samples to get 60% of network
features, L = 100
Figure 4 shows the cumulative distribution function, over the 61
days considered, of the time needed by a simulated sandbox with
100% capacity to observe 60% of relevant features. We can see
that on the median day, a sandbox using FORECAST could observe
60% of features after having analyzed only about one third of the
daily samples in 7.2 hours. A sandbox using the random selection
strategy, on the other hand, would need 15 hours to provide the
same number of potential C&C servers, while with the diversity-
based sample selection strategies, about 10 hours would be needed.
A secondary beneﬁt of FORECAST is therefore the faster response
to new C&C servers.
4.4 Performance
Table 5: FORECAST run-time on 2010 dataset, L = 100
Feature Extraction
Cluster Prediction
Cluster Scoring
Clustering
Total
Total (hours)
48
13
0.34
17
78.34
Per Sample (s)
0.39
0.11
<0.01
0.13
0.64
Since the aim of a sample selection strategy is to more efﬁciently
use the computing resources available for dynamic analysis, it can-
not itself require excessive resources. Clearly, deciding if a sample
Figure 3: Daily simulation results for network features, L =
100