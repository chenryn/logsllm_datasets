### 1.6 优化

在这一小节中，我们将介绍深度神经网络的优化过程，即通过训练来调整网络参数以最小化损失函数。本节将涵盖反向传播算法、梯度下降、随机梯度下降以及超参数的选择等内容。

#### 1.6.1 梯度下降和误差的反向传播

对于一个给定的神经网络及其损失函数，训练的目标是通过学习参数 \(\theta\) 来使损失值 \(L\) 最小化。最直接的方法是找到一组参数 \(\theta\) 使得 \(\nabla_\theta L = 0\)，但这在实际应用中往往难以实现，因为深度神经网络通常具有大量的参数且结构复杂。因此，我们采用一种逐步优化的方法——梯度下降（Gradient Descent），通过不断更新参数来逐步减小损失值。

**梯度下降的基本步骤：**

1. **初始化参数**：从一组随机指定的参数开始。
2. **计算梯度**：计算损失函数对参数的偏导数 \(\frac{\partial L}{\partial \theta}\)。
3. **更新参数**：使用公式 \(\theta := \theta - \alpha \frac{\partial L}{\partial \theta}\) 更新参数，其中 \(\alpha\) 是学习率，用于控制步长大小。

**反向传播（Back-Propagation）** 是一种计算神经网络中偏导数 \(\frac{\partial L}{\partial \theta}\) 的方法。为了简化计算，引入中间变量 \(\delta_l = \frac{\partial L}{\partial z_l}\)，表示损失函数对网络输出 \(z_l\) 的偏导数。通过这些中间变量，可以计算出损失函数对每个参数的偏导数，并最终组合成 \(\frac{\partial L}{\partial \theta}\)。

**具体步骤如下：**

1. **前向传播**：计算每一层的输出 \(z_l\) 和激活值 \(a_l\)。
2. **计算输出层的误差**：\(\delta_L = (a_L - y) \odot f'(z_L)\)。
3. **反向传播误差**：从输出层开始，逐层向前计算 \(\delta_l\)：
   \[
   \delta_l = (W_{l+1}^T \delta_{l+1}) \odot f'(z_l)
   \]
4. **计算参数的梯度**：
   \[
   \frac{\partial L}{\partial W_l} = \delta_l a_{l-1}^T, \quad \frac{\partial L}{\partial b_l} = \delta_l
   \]
5. **更新参数**：
   \[
   W_l := W_l - \alpha \frac{\partial L}{\partial W_l}, \quad b_l := b_l - \alpha \frac{\partial L}{\partial b_l}
   \]

**梯度消失问题**：当使用 Sigmoid 激活函数时，\(\frac{\partial a_l}{\partial z_l} = a_l (1 - a_l)\)。当 \(a_l\) 接近 0 或 1 时，\(\frac{\partial a_l}{\partial z_l}\) 会非常小，导致 \(\delta_l\) 非常小。在网络较深的情况下，反向传播时 \(\delta\) 会越来越小，出现梯度消失问题，使得靠近输入部分的参数很难被更新。因此，现代深度模型在隐藏层中更多地使用 ReLU 激活函数，因为它在 \(a > 0\) 时导数为 1，避免了这个问题。

#### 1.6.2 随机梯度下降和自适应学习率

**随机梯度下降（Stochastic Gradient Descent, SGD）** 是一种改进的梯度下降方法，它在每次迭代中仅使用一小部分训练数据（称为小批量）来计算损失函数，从而提高计算效率。小批量的大小称为批大小 \(B\)，通常 \(B \ll N\)。

**随机梯度下降的训练过程** 如下：

1. **初始化参数**：设定初始参数 \(\theta\)、学习率 \(\alpha\) 和训练步数 \(S\)。
2. **循环迭代**：进行 \(S\) 次迭代。
   - **计算小批量的损失**：从训练集中随机选择一个小批量的数据，计算其损失 \(L\)。
   - **计算梯度**：通过反向传播计算 \(\frac{\partial L}{\partial \theta}\)。
   - **更新参数**：\(\theta := \theta - \alpha \frac{\partial L}{\partial \theta}\)。

**自适应学习率算法**，如 Adam、RMSProp 和 Adagrad，通过自动调整学习率来加速收敛。这些算法根据梯度的历史信息动态调整学习率，使得在梯度较小的时候步长更大，在梯度较大时步长更小。

**Adam 算法** 是最常见的自适应学习率算法之一。它首先计算梯度的一阶矩估计（动量）和二阶矩估计（二阶动量），然后使用这些估计值来更新参数。默认情况下，动量和二阶动量的衰减系数分别为 \(\beta_1 = 0.9\) 和 \(\beta_2 = 0.999\)。

通过上述方法，我们可以有效地优化深度神经网络的参数，使其在各种任务中达到良好的性能。