# Kubernetes
>Kubernetes（常简称为K8s）是用于自动部署、扩展和管理容器化（containerized）应用程序的开源系统
![屏幕截图 2020-09-07 145646](/assets/屏幕截图%202020-09-07%20145646.png)
## 架构
![屏幕截图 2020-09-07 145831](/assets/屏幕截图%202020-09-07%20145831.png)
master：用于控制集群
- API服务器：外部访问入口
- Scheduler：调度应用（为应用分配工作节点）
- Controller Manager：执行集群级别的功能
- etcd：存储集群配置的分布式数据存储
工作节点：运行用户部署应用的节点
- 容器运行时：Docker 或者其他容器
- Kubelet：与API服务器通信 管理当前节点的容器
- kube-proxy:负责组件之间的负载均衡
### 分布式
- Kubenetes系统组件间只能通过API服务器通信
- 为了保证高可用性， master的每个组件可以有多个实例
### etcd
只有API服务器才能直接与etcd通信
数据在etcd中存储的是一个层次级目录结构 末端节点存储的json数据
集群一致性保证：raft算法
### API 服务器
- 认证授权
![屏幕截图 2020-09-15 143004](/assets/屏幕截图%202020-09-15%20143004.png)
- 通知客户端资源变更
![屏幕截图 2020-09-15 143259](/assets/屏幕截图%202020-09-15%20143259.png)
#### 安全防护
- pod 使用 service accounts机制进行认证
![屏幕截图 2020-09-16 135815](/assets/屏幕截图%202020-09-16%20135815.png)
```sh
kubectl get sa # 获取服务账户
kubectl create serviceaccount foo # 创建
```
![屏幕截图 2020-09-16 140540](/assets/屏幕截图%202020-09-16%20140540.png)
- 使用sa:
```yaml
spec:
  serviceAccountName: foo
```
RBAC控制：使用插件
### 调度器
利用 API 服务器的监听机制等待新创建的 pod, 然后给每个新的、 没有节点集的 pod 分配节点
![屏幕截图 2020-09-15 143719](/assets/屏幕截图%202020-09-15%20143719.png)
调度过程是很复杂的：
- 选择可用节点
- 选择最佳节点
- 高级调度
  - 如何保证节点副本分布尽可能均匀
### 控制管理器
确保系统真实状态朝 API 服务器定义的期望的状态收敛
- rc rs控制器 deployment控制器...
### Kubelet
- 在 API 服务器中创建Node 资源, 等待pod分配给它并启动pod
- 向API服务器提供监控
- 当pod从 API服务器删除, kubelet也会删除pod
![屏幕截图 2020-09-15 150145](/assets/屏幕截图%202020-09-15%20150145.png)
### kube-proxy
确保用户可以访问后端的pod
两种模式：
![屏幕截图 2020-09-15 150639](/assets/屏幕截图%202020-09-15%20150639.png)
![屏幕截图 2020-09-15 150657](/assets/屏幕截图%202020-09-15%20150657.png)
### 控制器协作
![屏幕截图 2020-09-15 151627](/assets/屏幕截图%202020-09-15%20151627.png)
### pod 到底是什么
![屏幕截图 2020-09-15 152859](/assets/屏幕截图%202020-09-15%20152859.png)
### 网络
![屏幕截图 2020-09-15 153101](/assets/屏幕截图%202020-09-15%20153101.png)
相同节点的pod通信：
![屏幕截图 2020-09-15 153836](/assets/屏幕截图%202020-09-15%20153836.png)
不同节点的pod通信：
![屏幕截图 2020-09-15 153900](/assets/屏幕截图%202020-09-15%20153900.png)
只有当所有节点连接到相同网关的时候 上述方案才有效
### 服务的实现
服务暴露的外部ip与端口通过每个节点上的kube-proxy实现
暴露的这个ip是虚拟的 主要是用来做映射用的 当kube-proxy接收到这个ip的请求 就会查找映射 转发请求
![屏幕截图 2020-09-15 154607](/assets/屏幕截图%202020-09-15%20154607.png)
### 高可用集群
应用高可用：
- 水平扩展
- 主从架构
master高可用：
![屏幕截图 2020-09-15 154955](/assets/屏幕截图%202020-09-15%20154955.png)
- etcd自身会进行数据同步
- API 服务器是无状态的
- 控制器与调度器会进行主从选举 只有leader才会进行调度控制工作
## 优点
- 简化部署
- 充分利用硬件
- 健康检查 自修复
- 自动扩容
## 在K8S中运行应用
根据描述信息生成对应的pod 在pod中运行容器
K8S会保证集群中的容器数量实例 在容器死亡时 会启动新容器替补
K8S 在运行时可根据需求动态调整副本数量
通过kube-proxy能进行服务连接动态切换
### 本地运行K8S
- 安装minikube
- 安装kubectl
```sh
minikube start \
--image-mirror-country=cn \
--registry-mirror='https://t9ab0rkd.mirror.aliyuncs.com' \
--image-repository='registry.cn-hangzhou.aliyuncs.com/google_containers'
```
### 部署第一个应用
```sh
kubectl  run  kubia  --image=luksa/kubia  --port=8080  # 创建容器运行
kubectl get pods # 获取pod
kubectl get rc
kubectl port-forward kubia 8080:8080 # 开启端口转发
kubectl get pods -o wide # 查看应用在哪个节点
kubectl scale rc kubia --replicas=3 # 水平扩容
```
![屏幕截图 2020-09-08 140428](/assets/屏幕截图%202020-09-08%20140428.png)
逻辑架构：
![屏幕截图 2020-09-08 142015](/assets/屏幕截图%202020-09-08%20142015.png)
- RC用来确保始终有pod运行
- 使用http服务来完成外部请求到pod的映射
## pod
一组紧密相关的容器 独立的逻辑机器
![屏幕截图 2020-09-08 135241](/assets/屏幕截图%202020-09-08%20135241.png)
一 个 pod 中的所有容器都在相同的 network 和 UTS 命名空间下运行
每个 pod 都有自己的 IP 地址， 并且可以通过这个专门的网络实现 pod
之间互相访问
pod的使用：
- 倾向于单个pod单个容器
### 使用yml创建pod
```yml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual
  labels:
    env: test # 指定一个标签
spec:
  nodeSelector: # 选择特定标签的节点
    super: "true"
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP
```
```sh
kubectl create -f kubia-manual.yaml
kubectl logs kubia-manual # 查看日志
```
### 标签
```sh
kubectl get po --show-labels
kubectl label po kubia-manual createtion_method=manual # 修改标签
kubectl label node minikube super=true
kubectl get po -l createtion_method=manual # 根据标签筛选
```
### 注解
注解也是键值对
```sh
kubectl annotate pod kubia-manual wang.ismy/name="cxk"
```
### 命名空间
命名空间简单为对象划分了一个作用域
```sh
kubectl get ns
kubectl get po -n kube-system # 获取命名空间下的pod
kubectl create namespace custom-namespace # 创建命名空间
kubectl create -f kubia-manual.yaml -n custom-namespace # 指定命名空间
```
### 停止与移除
```sh
kubectl delete po kubia-manual # 根据名字删除
```
## 副本机制
k8s 会保证 pod 以及 容器的健康运行
### 存活探针
当存活探针探测失败 容器就会重启
- 创建
```yml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe: # 存活探针
      httpGet: # 返回2xx 或者 3xx就代表活着
        path: /
        port: 8080
```
### ReplicationController
创建和管理一个pod的多个副本
![屏幕截图 2020-09-09 164430](/assets/屏幕截图%202020-09-09%20164430.png)
- 创建
```yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
```
控制器通 过 创建 一 个新的替代pod来响应pod的删除操作
通过更改标签的方式来实现rc与pod的关联
- 扩容
```sh
kubectl scale rc kubia --replicas=10
```
- 删除
```sh
kubectl delete rc kubia
```
### ReplicaSet
ReplicaSet 会 替代 rc
rs 的pod 选择器的表达能力更强
- 创建
```yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
```
### DaemonSet
由DaemonSet 创建的pod 会绕过调度程序 会在所有集群节点上运行（或者也可以通过指定`nodeSelector`在其他节点运行）
![屏幕截图 2020-09-09 191240](/assets/屏幕截图%202020-09-09%20191240.png)
- 创建
```yml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
```
### Job
允许运行 一 种 pod, 该 pod 在内部进程成功结束时， 不重启容器。
- 创建
```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  completions: 5 # 运行pod数
  parallelism: 2 # 并行运行数
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
```
### CronJob
- 创建
```yml
apiVersion: batch/v1beta1
kind: CronJob
metadata: