max (|G1| ,|G2|)
As the problem is NP-complete for arbitrary graphs, special
care has to be taken to keep the complexity as low as possible.
Our implementation uses the McGregor algorithm [47]. In the
expansion phase candidate nodes of both graphs are checked
for potential equivalence and selected as match if they are
considered equivalent. At this stage we use the basic block
distance dBB for two purposes: early pruning and deciding
which candidate pair to expand next. A pair of basic blocks
is removed from the set of potential matches if the distance
exceeds a certain threshold. Our experiments showed that 0.5
is a good threshold. Next, the candidate pairs are sorted by
their distance. The pair with the lowest distance is expanded
ﬁrst, as it is highly likely that nodes with the lowest distance
are indeed a correct match.
To cope with the potentially exponential running time, we
terminate the MCS algorithm after a predeﬁned number of
iterations and return the currently calculated minimal distance.
We call
the relaxed form the maximal common subgraph
(mCS).
3) Evaluation of the structural similarity: To evaluate the
function distances, we ﬁrst needed to derive a parameter
set αi for the basic block distance dBB. On that basis, we
could scrutinize the convergence speed of the graph-based
approaches. After that, we could measure the predictive quality
of all presented approaches.
4) Evaluation of the basic block distance: Our goal was to
ﬁnd a parameter set for the basic block distance function dBB
(cf. Section III-C1). However, even with debug information
there is no straightforward way of matching a piece of source
code to its corresponding basic block. Especially with more
aggressive optimization options whole portions of source code
might be removed (dead code elimination), or even duplicated
(loop unrolling). Hence, we cannot establish a ground truth at
the basic-block level to match two basic blocks from different
binaries.
The data set, as described in Section III-A, already contains
a ground truth about the labels of each function. Assuming
different functions imply (a large amount of) different basic
blocks, and simultaneously same functions imply same or
similar basic blocks, we can rephrase above problem. We
seek a parameter set that maximizes the distance of different
functions while simultaneously minimizing the distance of
equal functions. Put another way, we want to maximize the
difference between equal and different functions:
7
Feature
No. of Arithmetic Instructions
No. of Calls
No. of Instructions
No. of Logic Instructions
No. of Transfer Instructions
String Constants
Numeric Constants
αi
56.658
87.423
40.423
76.694
6.841
11.998
15.382
TABLE III: Best parameter set for distance function dBB.
max (dBB (fi, gj) − dBB (fi, fj)) ,
with f (cid:54)= g being functions with different labels.
To approach this optimization problem we used a genetic
algorithm. Our implementation uses GALib [58], a widely
used library for genetic algorithms. We executed an arithmetic
crossover using a Gaussian mutator for 100 times. The pop-
ulation size was 500 and we let it run for 1,000 generations.
The value range for αi was set to [0..100]. For the calculation
of equally-labeled functions, two random compilation options
and two functions with the same label were drawn. Note that
we performed this experiment on all binaries, i.e., over all
compilers, optimization options and CPU architectures. The
approach was similar for differently-labeled functions, but
additionally we allowed to draw a function from the same
binary. We only made sure that the function label was different.
We scrutinized the ten highest scoring parameter sets. It
showed that strings and numeric constants were assigned a
relatively low weight by the genetic algorithm. One possible
explanation is that by far not every basic block contains
constants and thus the data is deemed less important. Over
all runs, the number of data transfer instructions received a
relatively low weight. Most probably the reason is that transfer
instructions are very common and thus do not add much
information. The highest values were given to the number
of logic instructions and the number of function calls. The
number of instructions received values in the middle of the
value range. The overall best parameter set is depicted in
Table III. The calculated average distance between different
and equal functions for this parameter set is 0.378.
5) Robustness of the Graph Distance Function: In this sec-
tion we evaluate the predictive quality of the mCS algorithm.
For that, we calculate the distance between randomly selected
equal and different functions. To keep the runtime manageable,
we abort the mCS algorithm after 10,000 iterations and return
the current maximal value.
In Figures 3 a) and b) the size of the larger function is
depicted on the x-axis, the distance is shown at the y-axis. The
average distance of functions with the same label, Figure 3a
a), is 0.436 (standard deviation 0.242). The average distance of
different functions, Figure 3b b), is 0.814 (standard deviation
0.081).
Each point is plotted with transparency, hence the opacity
of a coordinate gives an indicator about the respective density.
Additionally, topological lines depict the density at a given
(a) Equal Functions
(b) Different Functions
Fig. 3: Scatter plot showing distance function dmcs for the set of functions.
area. In average, equal functions show substantially lower
distances than different functions.
Comparing both ﬁgures it becomes evident that the graph
distance function dmcs.orig is robust.
6) Uncoupling the distance function from function size: As
seen in Figure 3, the mCS distance measure has a tendency
towards higher values for larger graphs. This tendency is not
surprising, as larger graphs have a higher potential of adding
more constraints. E.g., some compilers optimize towards a
single returning basic block whereas other compilers create
code that allows returning from several different locations.
An approximation of the distance function to a constant has
the advantage that a constant threshold value can be used to
distinguish equal from different functions. This in turn allows
us to utilize the distance function in efﬁcient search algorithms.
Figure 3a shows two regression lines in each graph.
The solid line is a B-spline crossing central midpoints. It
is computationally more expensive than other methods, but
gives a good estimate about the ideal regression line. It has a
relative standard error (RSE) of 0.164. The dotted line depicts
a logarithmic regression with an RSE of 0.168.
To uncouple the distance function from the number of basic
blocks, the distance function dmcs is altered in the following
way to compensate for the function size:
dmcs comp(G1, G2) :=
dmcs(G1, G2)
comp (G1, G2)
with comp being the logarithmic compensation term:
comp (G1, G2) := i + k log (max (|G1| ,|G2|)) .
7) Convergence Speed of the MCS: The potentially expo-
nential runtime of an exact solution for the maximum common
subgraph problem deems the problem unfeasible for real-
world application. The computational complexity of a good
approximation, however, may be signiﬁcantly lower. Hence,
an important research question is when to terminate the mCS
algorithm, or how fast the mCS algorithm converges to its
maximum value. We conducted the following experiment only
on the basic-block sensitive approach dmcs.
Our mCS implementation provides a callback mechanism
that is invoked when a new solution is found, allowing to
collect information about the currently found subgraph. As
discussed in Section III-C2, the basic block distance function
dBB serves as good heuristic for the mCS algorithm in order
to early prune the search space. At this place, we can discard
a potential match if dBB is too high.
Figure 4 depicts the advance of the currently found max-
imal common subgraph size of the ﬁrst 1,000 iterations for
an example function. After a steep, quasilinear ascend, the
mCS is reached after only 56 iterations. We conducted an
analysis of the convergence speed over our data set. In average
64.68 iterations are needed for the algorithm to generate a
common subgraph of the same size as the maximal size after
10,000 iterations. When aborting the mCS algorithm after
16 max (|G1| ,|G2|) iterations, the size of the largest found
subgraph reaches on average 99.11% of the subgraph size after
10,000 iterations.
As the exact value of the MCS is no precondition to cor-
rectly label functions. The potentially exponential complexity
can instead be reduced to a linear runtime with a reasonable
approximation to the MCS.
8
Fig. 4: Advance of the current maximal subgraph of function
pkey_rsa_ctrl compiled with options CLANG 01 and
CLANG 02.
Fig. 5: ROC curves of different distance measures based on
the CFG.
8) Comparison of dmcs with Other Distance Functions:
In Section III-C2 we described three different algorithms to
match two CFGs. The ﬁrst one is based on bipartite matching,
as described in [29], the second and third algorithms are both
based on the MCS: The plain mCS algorithm dmcs.orig and
the extended algorithm dmcs, using the basic block distance
dBB as additional measure.
Figure 5 shows the receiver operating characteristics (ROC)
of the three distance functions with logarithmic compensation.
To calculate the characteristics, we computed the distances
of 1 million randomly chosen pairs of equal and different
functions. We compared the classic mCS distance function
dmcs.orig with the basic-block sensitive approach dmcs and
with the bipartite matching distance [29] The ﬁgure shows that
the dmcs performs better than the plain mCS algorithm. The
ROC curve of the bipartite matching approach is signiﬁcantly
lower than that of both structure-aware approaches for low
false positive rates.
During the evaluation we also measured the respective
timings. The mCS algorithm based on dmcs.orig needed in
average 6.3 ms per function, the improved mCS algorithm
based on dmcs needed only 4.4 ms, while the bipartite match-
ing approach needed 4.2 ms.
Thus, the running times of both approaches are in the same
order of magnitude, however, the quality of our approach is
signiﬁcantly higher.
9) Comparison to VP Tree: Bunke showed that the maxi-
mum common subgraph distance measure dmcs on the basis of
the MCS is a metric [18]. Hence, it is possible to use efﬁcient
search structures, such as vantage point (VP) trees [64] to
efﬁciently search the most similar binary function. The usage
of VP trees would make the numeric ﬁlter obsolete, as the
function CFG can be directly and efﬁciently searched.
A VP tree is created by selecting one element as pivot
element. All distances between the remaining elements and the
pivot element are computed. The set of remaining elements
is split up into two sets of roughly the same size based on
their distance to the pivot element. The elements closer than
the selected threshold are added to the left branch of the pivot
element, the other elements are added to the right branch. This
procedure is recursively repeated until the set size becomes
sufﬁciently small.
To ﬁnd the nearest neighbor of a given element in the VP
tree, the distance between the pivot element (the respective
root node) and the search element
is computed. The tree
is appropriately traversed by making use of the triangular
inequality of the metric until the nearest neighbor or set of
neighbors is found.
In the average case, VP trees have a logarithmic search
complexity. This comes, however, at the cost of building the
VP tree and therefore is only feasible when querying a larger
number of elements.
Also depicted in Figure 2 are the creation and search times
for a VP tree. It clearly shows that both, the creation and search
times, are signiﬁcantly higher than the timings of the presented
approach. One reason for the long creation time of might be a
high number of rebalancing steps. These pose a critical point
during the creation phase. Both timings make a solution based
on VP trees infeasible for real-world applications.
IV. EVALUATION
In this section, we describe the results of the experiments
we have performed to evaluate discovRE. We ﬁrst evaluate
9
010203002505007501000iterationmCS size0.000.250.500.751.000.000.250.500.751.00false positive ratetrue positive ratetypebipartite matchingmCS with basic block similarityplain mCSthe correctness of our similarity comparison approach. Then
we show the efﬁciency achieved by combining the numeric
and structural ﬁlters. Finally, we apply discovRE to ﬁnd bugs
in binaries across several architectures.
A. Similarity Metric
To evaluate our similarity metric, we performed an exten-
sive evaluation using OpenSSL. Note that OpenSSL was not
included in the set of binaries that we used to derive code
features. For our evaluation, we compiled OpenSSL (version
1.0.1.e) for x86, MIPS, and ARM with different compilers,
compiler options, and operating systems as explained in Sec-
tion III-A. This resulted in 593 binaries with 1,293 sufﬁciently
large functions. We kept the debug symbols as they provide
a ground truth and enable us to verify the correctness of
matching using the functions symbolic names. For clarity, we
denote by f c,a
n the nth function in OpenSSL that was compiled
for architecture a and using compiler setting c. c comprises
type and version of compiler, compiler optimizations, and the
target operating system. To test discovRE for matching a given
function n, we:
,
n
n
n
n
}
1) randomly select a target function f c1,a1
2) randomly select a matching function f c2,a2
n
3) randomly select a a set F of 99,999 functions, and
4) construct the code base to be queried as C = F ∪{f c2,a2
to be searched for,
(|C| = 100, 000).
We then used discovRE to query for f c1,a1
in C. The ideal
. To include more variety in the code
result would be f c2,a2
base, we included functions from the data set described in
Section III-A. We repeated the previously described process
1 million times. selecting n at random with each iteration.
Table IV shows the results of this evaluation. At a high level,
in 93,93 % of the cases the matching function was in the set
of 128 functions returned by the numeric ﬁlter (kNN). When
using the numeric ﬁlter alone, the matching function was on
average at rank 5.5. Interestingly, when combining that with
the structural ﬁlter, the matching function was always at the
ﬁrst rank. The average query time was 56.48 ms, illustrating
the efﬁciency of discovRE. A closer analysis of the false
predictions revealed that
in most cases incorrect matching
was caused by the loop unrolling compilation option. The
unrolling of loops duplicates loop bodies several times. Hence,
several numeric features are signiﬁcantly altered, which in turn
increases the distance.
B. Cross-Architecture Bug Search
In this section, we apply discovRE to real-world case
studies of recent and prominent vulnerabilities. Here we base
our experiment on the techniques used by Pewny et al. [53].
This evaluation searched for the Heartbleed vulnerability in
OpenSSL binaries from three sources:
1) self-compiled binaries for x86, MIPS, and ARM
2) the Linux-based router ﬁrmware DD-WRT (r21676) com-
3) a NAS device (Netgear ReadyNAS v6.1.6) with an ARM
piled for MIPS [10]
processor [11]
We contacted the authors of [53] and they kindly agreed
to share the binaries used in their evaluation. We very much
10
appreciate this good scientiﬁc practice. This way, we could
ensure that both approaches are tested on the same binary code
base. All evaluations have been executed on a commodity PC
with an Intel Core i7-2720QM CPU with 2.20 GHz and 8 GB
DDR3 RAM on a single CPU core. We choose this system
because it is very similar to the setup used in their paper (even
a bit slower).
We also extend the evaluation performed by Pewny et al.
by performing bug search over all binaries of the ﬁrmware
images. In their original evaluation, the authors ﬁrst extracted
the OpenSSL libraries from the ﬁrmware images and limit
the bug search to these binaries. We conduct a more realistic
experiment where we do not assume that we know the vul-
nerability exists in a given binary from the ﬁrmware image.
This is a strong assumption given that ﬁrmware images may
contain hundreds of binaries, making the problem of identify-
ing the vulnerable binary very challenging. For example, the
ReadyNAS image contains 1,510 binaries. Moreover, this is
especially true if static linking is used, where library code
is copied into the target executable. For better comparability,
we report the results on both the OpenSSL binaries and the
whole binaries contained in the ﬁrmware image. Also, the
authors reported the normalized average time needed for the
preparation and query steps. This is deﬁned as the average
amount needed to process 10,000 basic blocks. This allowed
us to extrapolate the absolute times needed by their approach
to perform bug search on the same ﬁrmware images.
in
We
that
itself
bug manifests
identiﬁed a buggy function in the
respective
binaries, namely we chose CVE-2014-0160 (Heartbleed).
two
The Heartbleed
the
functions dtls1_process_heartbeat (DTLS)
and
tls1_process_heartbeat (TLS)
are virtually
identical. Thus, the search for either of the functions should
also return the other function in the list of similar functions.
Table V shows the results of the comparison of the ap-
proaches by Pewny et al. and discovRE. While the Multi-
MH method has some problems in correctly identifying TLS
from x86 in DD-WRT or DTLS from x86 in MIPS, Multi-
k-MH scores a lot better. In only one case, when trying to
match the x86 TLS function in DD-WRT it ranks the correct
function only at place 5. In contrast, discovRE always correctly
identiﬁes the searched function(s). Summarizing, the quality of