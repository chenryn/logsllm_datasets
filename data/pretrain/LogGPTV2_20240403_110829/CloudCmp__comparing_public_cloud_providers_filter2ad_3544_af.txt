T
n
o
i
t
a
r
e
n
e
G
e
g
a
P
 10
 1
 0.1
 0.01
C1
C3
C4
1
2
3
4
5
6
7
8
9
10 11 12
Page Index
 140
 120
C1.1
C4.1
 100
i
)
s
(
e
m
T
n
o
i
t
u
c
e
x
E
b
o
J
 80
 60
 40
 20
 0
1
2
3
4
5
Job Index
Figure 13: The page generation time of TPC-W when deployed on all
three cloud providers that support it. The y-axis is in a logarithm scale.
Figure 14: The job execution time of Blast when deployed on both
C1.1 and C4.1. We show the results for ﬁve example jobs.
nents. Because all C2’s data centers reside within the US, there is
a latency jump for vantage points outside North America.
6. USING CLOUDCMP: CASE STUDIES
In this section, we deploy three simple applications on the cloud
to check whether the benchmark results from CloudCmp are con-
sistent with the performance experienced by real applications. If
they are, it validates our conjecture that CloudCmp’s results can
be used by customers to choose cloud providers in lieu of porting,
deploying, and measuring their applications on each cloud. The ap-
plications include a storage intensive e-commerce website, a com-
putation intensive application for DNA alignment, and a latency
sensitive website that serves static objects. This study is a ﬁrst step.
We do not claim to represent all the existing cloud applications and
defer the design of a generic tool for estimating the performance of
arbitrary applications to future work (§7).
6.1 E-commerce Website
The ﬁrst application we choose is TPC-W [20], a standard bench-
mark for transactional web services. TPC-W itself is a fully
functional e-commerce website where customers can browse and
purchase books online.
It is advantageous for such applications
to migrate to a cloud computing utility, because the cloud pro-
vides highly scalable and available storage services and free intra-
datacenter bandwidth.
We use a Java implementation of TPC-W [10] and port it to vari-
ous cloud providers by redirecting the database operations to use
each cloud’s table storage APIs. However, not all database op-
erations used in TPC-W map directly to the APIs offered by the
cloud. Speciﬁcally, there is no equivalent for JOIN and GROUP
BY. We disable pages that use these operations. Out of the original
16 pages in TPC-W, we had to disable four pages. The three opera-
tions we benchmarked (put, get, and query) account for over 90%
of TPC-W’s storage operations. Other operations include delete
and count.
One major performance goal of TPC-W, similar to other dy-
namic web applications, is to minimize the page generation time.
The performance bottleneck lies in accessing table storage. From
CloudCmp’s comparison results of table storage service, shown in
Figure 4, we see that cloud C1 offers the lowest table service re-
sponse time among all providers. That benchmark appears relevant
because the table size it uses (100K) is on the same order as that
of the TPC-W tables (100K - 400K), and because it covers most of
the storage operations used by the ported version of TPC-W. There-
fore, from the benchmarking results, a customer may guess that C1
will offer the best performance for TPC-W.
To verify this, we deploy TPC-W on the three providers that of-
fer table storage service: C1, C3, and C4. We use instances that
occupy a physical core to avoid interference due to CPU sharing.
Figure 13 shows the page generation time for all twelve pages. We
see that C1 indeed has the lowest page generation time among all
three providers, consistent with our benchmarking result. Further-
more, C4 has lower generation time than C3 for most pages except
for pages 9 and 10. These pages contain many query operations
and are consistent with CloudCmp’s results in Figure 4, where C4
has a much higher query response time but lower get and put
response time than C3.
6.2 Parallel Scientiﬁc Computation
We then test Blast, a parallel computation application for DNA
alignment. We choose Blast because it represents computation-
intensive applications that can take advantage of the cloud com-
puting utility. The application is written in C#, with one instance
running a web service to accept job input and return job output,
and multiple worker instances responsible for executing the jobs
in parallel. Blast instances communicate with each other through
the queue storage service, which serves as a global messaging sys-
tem. The application also leverages the blob storage service to store
computation results.
We consider two cloud providers for Blast: C1 and C4. The oth-
ers do not support queue service. The performance goal of Blast
is to reduce job execution time given a budget on number of in-
stances. CloudCmp’s computational benchmark results shown in
Figure 1 suggest that at a similar price point, C4.1 performs better
than C1.1.
To check this prediction, we deploy Blast on both types of in-
stance (C1.1 and C4.1) and compare the real execution time of ﬁve
example jobs. Figure 14 shows the results. For all ﬁve jobs, Blast
running on C4.1 takes only a portion of the time it takes when run-
ning on C1.1. This suggests that C4.1 indeed offers better perfor-
mance than C1.1 for real applications at similar price point, consis-
tent with CloudCmp’s benchmarks.
6.3 Latency Sensitive Website
We choose a latency sensitive website for our third case study.
We conﬁgure a simple web server to serve only static pages, and
download the pages from PlanetLab nodes around the world. The
performance goal is to minimize the page downloading time from
many vantage points, and the main performance bottleneck is the
wide area network latency. We choose this application because
many existing online services such as web search and online gam-
ing depend critically on network latency [28]. Due to TCP seman-
tics, shorter latency also often leads to higher throughput [30].
We deploy our website on all providers and use wget to fetch
web pages from PlanetLab nodes. Each vantage point fetches from
the instance with the minimum network latency to emulate a per-
fect load balancing scheme. Figure 15 shows the distributions of
12 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
C1
C2
C3
C4
 0.1
 0.2
 0.3
 0.4
 0.5
Page Load Time (s)
(a) 1KB Page
C1
C2
C3
C4
 0.5
 1
 1.5
 2
Page Load Time (s)
(b) 100KB Page
Figure 15: The distribution of the page downloading time of our web-
site. We show the results for two different page sizes: 1KB and 100KB.
the page downloading time for various providers. We download
two web pages of sizes 1KB and 100KB. In both cases, C3 has the
smallest page downloading time, consistent with our benchmark-
ing results in Figure 12 that show C3 having the lowest wide-area
network latency distribution.
7. DISCUSSION
In this section, we discuss CloudCmp’s limitations and future re-
search directions.
Breadth vs. depth trade-off. As our main goal is to perform a
comprehensive comparison among cloud providers, in several oc-
casions, we sacriﬁce depth for breadth. For instance, an in-depth
study that focuses on the storage service of a particular provider
could have used more storage system benchmarks [23] in addi-
tion to the metrics we use, to examine factors such as pre-fetching
and other query optimization techniques. The results presented
in this paper show only the ﬁrst-order differences among various
providers, and can be complemented with more in-depth measure-
ment results on each individual provider.
Snapshot vs. continuous measurement. The results in this paper
should be viewed as a snapshot comparison among cloud providers
in today’s market. As time goes by, providers may upgrade their
hardware and software infrastructure, and new providers may enter
the market. It is our future work to use CloudCmp to continually
update those results.
Future work. CloudCmp opens at least two venues for future re-
search. First, we can use CloudCmp’s measurement results to make
application-speciﬁc performance prediction. A customer who is
interested in selecting a cloud provider for its speciﬁc application
may combine the application’s workload traces with CloudCmp’s
results to predict how the application may perform on each cloud
platform. This prediction result will help a customer make an in-
formed decision.
Second, it can be promising to develop a meta-cloud that com-
bines the diverse strengths of various providers. CloudCmp’s results
show that there is no clear winner on the current market. For in-
stance, C1 has fast storage service, C2 has the most cost-effective
virtual instances, and C3 has the smallest wide-area latency. A
meta-cloud that uses C2 for computation, C1 for storage, and C3
for front-end request processing may outperform any standalone
provider. How to build such a meta-cloud is an interesting future
research question.
8. RELATED WORK
Cloud computing has drawn signiﬁcant attention from re-
searchers in the past several years. Armbrust et al. provide a
comprehensive overview of cloud computing, including the new
opportunities it enables, the potential obstacles to its adoption, and
a classiﬁcation of cloud providers [22]. Their work motivates this
study on comparing the cloud providers in today’s market.
Wang and Ng show that the virtualization technique in Amazon
EC2 can lead to dramatic instabilities in network throughput and
latency, even when the data center network is lightly loaded [35].
This is mainly caused by CPU sharing among small virtual in-
stances. To avoid such effects, we carefully design our network
measurements to only use virtual instances that can at least fully
utilize one CPU core.
Garﬁnkel studies the performance of Amazon Simple Storage
Service (S3) and describes his experience in migrating an applica-
tion from dedicated hardware into the cloud [24]. Walker inves-
tigates the performance of scientiﬁc applications on Amazon EC2
using both macro and micro benchmarks [34]. Compared to the
work above, CloudCmp evaluates a much more complete set of ser-
vices offered by cloud providers.
CloudStatus continually monitors the status of Amazon AWS
and Google AppEngine [5]. It keeps track of several performance
metrics speciﬁc to each of the two providers. In contrast, we iden-
tify a common set of key performance and cost metrics shared by
various providers and conduct a thorough comparison among them
to guide provider selection.
Yahoo! Cloud Serving Benchmark (YCSB) [23] is a framework
to benchmark distributed data serving systems, such as Cassan-
dra, HBase, and PNUTS. The authors present comparison results
from experiments conducted on local clusters. We compare similar
types of table storage operations offered by main commercial cloud
providers in the wild.
Our earlier work argues for the importance of a comprehensive
cloud provider comparison framework, and presents some prelim-
inary results to highlight the wide performance gap among cloud
providers [29]. In this paper, we have taken several signiﬁcant steps
further to fulﬁll our earlier vision. First, we measure and compare
more cloud services, such as virtual instance memory and disk I/O,
blob and queue storage, and inter-datacenter network transfer. Sec-
ond, we conduct a more ﬁne-grained study on providers by mea-
suring each data center and virtual instance type separately. Third,
we demonstrate that CloudCmp is useful in guiding provider selec-
tion by comparing CloudCmp’s results with the performance of real
applications deployed in the clouds.
Finally, there is much recent work studying other aspects of
cloud computing platforms, such as how to make cloud platforms
trustworthy and accountable [26, 33], how to enable private cloud
computing environment [37], and how to exploit and remedy in-
terference between colocated virtual instances [31, 32]. Our work
complements previous work by studying an orthogonal aspect of
cloud computing:
the cost-performance comparison of today’s
cloud providers.
139. CONCLUSION
Arguably, the time for computing-as-a-utility has now arrived. It
is hard to see a future wherein rapidly growing startups, low foot-
print “mom and pop” companies that want a presence on the web,
and one-off tasks needing large amounts of specialized computa-
tion such as document translation and protein sequencing would
not beneﬁt from public cloud computing services.
In this context, this work presents the ﬁrst tool, CloudCmp,
to systematically compare the performance and cost of cloud
providers along dimensions that matter to customers. We address
some key challenges with regards to scoping the problem to one
that is manageable given bounded money and time and yet is mean-
ingful to predict the performance of real applications. We observe
dramatic performance and cost variations across providers in their
virtual instances, storage services, and network transfers. We be-
lieve CloudCmp represents a signiﬁcant ﬁrst step towards enabling
fast and accurate provider selection for the emerging cloud appli-
cations and towards an end-to-end benchmark suite that can serve
as a progress card for provider’s optimizations.
Acknowledgements
This work was funded in part by an NSF CAREER Award CNS-
0845858 and Award CNS-0925472. We thank the anonymous re-
viewers for their helpful feedback and suggestions. We also thank
Reza Baghai and Brad Calder for insights that helped improve the
methodology and presentation.
10. REFERENCES
[1] Amazon SimpleDB. http://aws.amazon.com/
simpledb/.
[2] Amazon Web Service. http://aws.amazon.com.
[3] AWS SDK for Java. http://aws.amazon.com/
sdkforjava.
[4] CloudCmp Project Website. http://cloudcmp.net.
[5] CloudStatus. http://www.cloudstatus.com.
[6] Comparing Amazon EC2 performance with other cloud/VPS
hosting options... and real hardware. http://www.
paessler.com/blog/2009/04/14/prtg-7/comparing-
amazon-ec2-performance-with-other-cloudvps-
hosting-options-and-real-hardware.
[7] Google AppEngine. http://code.google.com/
appengine.
[8] Iperf. http://iperf.sourceforge.net.
[9] Java binding for Rackspace CloudFiles. http://github.
com/rackspace/java-cloudfiles/tree.
[10] Java TPC-W Implementation. http://www.ece.wisc.
edu/~pharm/tpcw.shtml.
[11] Linux TCP Protocol. http://www.kernel.org/doc/man-
pages/online/pages/man7/tcp.7.html.
[12] Microsoft Windows Azure. http://www.microsoft.com/
windowsazure.
[13] PlanetLab. http://www.planet-lab.org.
[14] Rackspace Cloud. http://www.rackspacecloud.com.
[15] Rackspace Cloud Servers versus Amazon EC2: Performance
Analysis. http://www.thebitsource.com/featured-
posts/rackspace-cloud-servers-versus-amazon-
ec2-performance-analysis/.
[16] SPEC CPU2006 Benchmark. http://www.spec.org/
cpu2006.
[17] SPEC Java Virtual Machine Benchmark 2008. http://www.
spec.org/jvm2008/.
[18] Standard Performance Evaluation Corporation. http://
www.spec.org.
[19] State of the Cloud, May 2010. http://www.
jackofallclouds.com/2010/05/state-of-the-
cloud-may-2010.
[20] TPC Benchmark W. http://www.tpcw.org/tpcw.
[21] Windows Azure SDK for Java. http://www.
windowsazure4j.org/.
[22] M. Armbrust, A. Fox, R. Grifﬁth, A.D. Joseph, R.H. Katz,
A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica,
and Others. Above the clouds: A berkeley view of cloud
computing. EECS Department, University of California,
Berkeley, Tech. Rep. UCB/EECS-2009-28, 2009.
[23] B.F. Cooper, Adam Silberstein, Erwin Tam, Raghu
Ramakrishnan, and Russell Sears. Benchmarking Cloud
Serving Systems with YCSB. In ACM Symposium on Cloud
Computing, 2010.
[24] Simson Garﬁnkel. An Evaluation of Amazon s Grid
Computing Services : EC2 , S3 and SQS. Harvard
University, Tech. Rep. TR-08-07.
[25] Seth Gilbert and Nancy Lynch. Brewer’s conjecture and the
feasibility of consistent, available, partition-tolerant web
services. In ACM SIGACT News, 2002.
[26] Andreas Haeberlen. A Case for the Accountable Cloud. In
ACM SIGOPS LADIS, 2009.
[27] Mohammad Hajjat, Xin Sun, Yu-Wei Eric Sung, David
Maltz, Sanjay Rao, Kunwadee Sripanidkulchai, and Mohit
Tawarmalani. Cloudward Bound: Planning for Beneﬁcial
Migration of Enterprise Applications to the Cloud. In ACM
SIGCOMM, 2010.
[28] Ron Kohavi, Randal M. Henne, and Dan Sommerﬁeld.
Practical guide to controlled experiments on the web. In
ACM KDD, volume 2007. ACM Press, 2007.
[29] Ang Li, Xiaowei Yang, Srikanth Kandula, and Ming Zhang.
CloudCmp: Shopping for a Cloud Made Easy. In USENIX
HotCloud, 2010.
[30] Jitendra Padhye, Victor Firoiu, Don Towsley, and Jim
Kurose. Modeling TCP Throughput: A Simple Model and its
Empirical Validation. In ACM SIGCOMM, 1998.
[31] Himanshu Raj, R Nathuji, A Singh, and Paul England.
Resource management for isolation enhanced cloud services.
In ACM Workshop on Cloud Computing Security, pages
77–84, 2009.
[32] Thomas Ristenpart, Eran Tromer, Hovav Shacham, and
Stefan Savage. Hey, You, Get Off of My Cloud: Exploring
Information Leakage in Third-Party Compute Clouds. In
ACM CCS, Chicago, 2009.
[33] Nuno Santos, K.P. Gummadi, and Rodrigo Rodrigues.
Towards trusted cloud computing. In HotCloud, 2009.
[34] E. Walker. Benchmarking amazon EC2 for high-performance
scientiﬁc computing. USENIX Login, 2008.
[35] Guohui Wang and T. S. Eugene Ng. The Impact of
Virtualization on Network Performance of Amazon EC2
Data Center. In IEEE INFOCOM, 2010.
[36] Jonathan S. Ward. A Performance Comparison of Clouds:
Amazon EC2 and Ubuntu Enterprise Cloud. SICSA
DemoFEST, 2009.
[37] T. Wood, A. Gerber, KK Ramakrishnan, P. Shenoy, and
J. Van Der Merwe. The Case for Enterprise-Ready Virtual
Private Clouds. In USENIX HotCloud, 2009.
14