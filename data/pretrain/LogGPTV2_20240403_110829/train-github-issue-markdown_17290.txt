### Issue with `kube-controller-manager` Starting from v1.2.0-beta.1

I have encountered an issue with the `kube-controller-manager` starting from version `v1.2.0-beta.1`, which persists in the `v1.2.0` release. The problem is that the following pods are being restarted every 30 seconds:

- `kube-apiserver-ip-10-242-128-234.ec2.internal`
- `kube-controller-manager-ip-10-242-128-234.ec2.internal`
- `kube-proxy-ip-10-242-128-234.ec2.internal`
- `kube-scheduler-ip-10-242-128-234.ec2.internal`

#### Pod Status
The pods do not remain active for more than 30 seconds, as shown below:

```
NAME                                                     READY     STATUS    RESTARTS   AGE
...
kube-apiserver-ip-10-242-128-234.ec2.internal            1/1       Running   0          12s
kube-controller-manager-ip-10-242-128-234.ec2.internal   1/1       Running   0          16s
kube-proxy-ip-10-242-128-234.ec2.internal                1/1       Running   0          16s
kube-scheduler-ip-10-242-128-234.ec2.internal            1/1       Running   0          15s
```

After 30 seconds, the pods are in a `Pending` state:

```
NAME                                                     READY     STATUS    RESTARTS   AGE
...
kube-apiserver-ip-10-242-128-234.ec2.internal            0/1       Pending   0          1s
kube-controller-manager-ip-10-242-128-234.ec2.internal   1/1       Running   0          1s
kube-scheduler-ip-10-242-128-234.ec2.internal            0/1       Pending   0          1s
```

#### Docker Container Status
Interestingly, the Docker containers associated with these pods continue to run:

```
CONTAINER ID        IMAGE                                       COMMAND                  CREATED             STATUS              PORTS               NAMES
...
c44ff71caf21        gcr.io/google_containers/hyperkube:v1.2.0   "/hyperkube scheduler"   33 minutes ago      Up 33 minutes                           k8s_kube-scheduler.7b10907e_kube-scheduler-ip-10-242-128-234.ec2.internal_kube-system_1adf66daf2f3ee9cce476b5b97a8b58c_ed62caa9
d8ef6ef809ca        gcr.io/google_containers/hyperkube:v1.2.0   "/hyperkube proxy --m"   33 minutes ago      Up 33 minutes                           k8s_kube-proxy.c9b75a4a_kube-proxy-ip-10-242-128-234.ec2.internal_kube-system_e8c6af5a5c65fbcf53b11238b4a403bd_3ee808aa
068a6ab6793a        gcr.io/google_containers/hyperkube:v1.2.0   "/hyperkube controlle"   33 minutes ago      Up 33 minutes                           k8s_kube-controller-manager.2e450a4c_kube-controller-manager-ip-10-242-128-234.ec2.internal_kube-system_556a90cee78cfb39fd2f1f8d57edb479_8db26042
76e4cda29efb        gcr.io/google_containers/hyperkube:v1.2.0   "/hyperkube apiserver"   33 minutes ago      Up 33 minutes                           k8s_kube-apiserver.f4b25f9b_kube-apiserver-ip-10-242-128-234.ec2.internal_kube-system_f3d81ba2e960c16f6848dc5c09eb4eee_471bfae7
11eb022a4939        gcr.io/google_containers/pause:2.0          "/pause"                 33 minutes ago      Up 33 minutes                           k8s_POD.6059dfa2_kube-scheduler-ip-10-242-128-234.ec2.internal_kube-system_1adf66daf2f3ee9cce476b5b97a8b58c_a51591f2
a1d9d564218c        gcr.io/google_containers/pause:2.0          "/pause"                 33 minutes ago      Up 33 minutes                           k8s_POD.6059dfa2_kube-proxy-ip-10-242-128-234.ec2.internal_kube-system_e8c6af5a5c65fbcf53b11238b4a403bd_6accda61
1a1a278f12d5        gcr.io/google_containers/pause:2.0          "/pause"                 33 minutes ago      Up 33 minutes                           k8s_POD.6059dfa2_kube-controller-manager-ip-10-242-128-234.ec2.internal_kube-system_556a90cee78cfb39fd2f1f8d57edb479_209f305d
48e6e4e4d4bc        gcr.io/google_containers/pause:2.0          "/pause"                 33 minutes ago      Up 33 minutes                           k8s_POD.6059dfa2_kube-apiserver-ip-10-242-128-234.ec2.internal_kube-system_f3d81ba2e960c16f6848dc5c09eb4eee_3b289a47
```

#### Error Logs
**Controller-Manager Log:**
```
I0322 15:57:36.787253       1 nodecontroller.go:400] forceful deletion of kube-controller-manager-ip-10-242-128-234.ec2.internal succeeded
E0322 15:57:36.787288       1 nodecontroller.go:352] 
I0322 15:57:36.817601       1 nodecontroller.go:400] forceful deletion of kube-scheduler-ip-10-242-128-234.ec2.internal succeeded
E0322 15:57:36.817644       1 nodecontroller.go:352] 
E0322 15:57:36.819261       1 nodecontroller.go:352] pods "kube-controller-manager-ip-10-242-128-234.ec2.internal" not found
E0322 15:57:36.828904       1 nodecontroller.go:352] pods "kube-scheduler-ip-10-242-128-234.ec2.internal" not found
...
```

**Kubelet Log:**
```
Mar 22 16:01:38 master kubelet[1493]: E0322 16:01:38.917605    1493 kubelet.go:1753] Deleting mirror pod "kube-controller-manager-ip-10-242-128-234.ec2.internal_kube-system(4a513408-f047-11e5-a5d4-0af71f25e715)" because it is outdated
Mar 22 16:01:39 master kubelet[1493]: E0322 16:01:39.195729    1493 kubelet.go:1753] Deleting mirror pod "kube-proxy-ip-10-242-128-234.ec2.internal_kube-system(4a5d3ea1-f047-11e5-a5d4-0af71f25e715)" because it is outdated
Mar 22 16:01:39 master kubelet[1493]: E0322 16:01:39.196138    1493 kubelet.go:1753] Deleting mirror pod "kube-scheduler-ip-10-242-128-234.ec2.internal_kube-system(4a39ac2f-f047-11e5-a5d4-0af71f25e715)" because it is outdated
...
Mar 22 16:01:44 master kubelet[1493]: E0322 16:01:44.659553    1493 kubelet.go:1183] Unable to update node status: update node status exceeds retry count
```

#### Analysis
The `kubelet` appears to be identifying these pods as duplicates and attempting to delete them. However, the actual Docker containers are not being terminated, leading to the observed behavior.

#### Workaround
Switching the `kube-controller-manager` back to version `v1.2.0-beta.0` resolves the issue.

If you have any further insights or need additional assistance, please let me know.