title:What's the difference?: efficient set reconciliation without prior
context
author:David Eppstein and
Michael T. Goodrich and
Frank Uyeda and
George Varghese
What’s the Difference?
Efﬁcient Set Reconciliation without Prior Context
David Eppstein1 Michael T. Goodrich1 Frank Uyeda2 George Varghese2,3
1U.C. Irvine 2U.C. San Diego 3Yahoo! Research
ABSTRACT
We describe a synopsis structure, the Difference Digest, that allows
two nodes to compute the elements belonging to the set difference
in a single round with communication overhead proportional to the
size of the difference times the logarithm of the keyspace. While
set reconciliation can be done efﬁciently using logs, logs require
overhead for every update and scale poorly when multiple users
are to be reconciled. By contrast, our abstraction assumes no prior
context and is useful in networking and distributed systems appli-
cations such as trading blocks in a peer-to-peer network, and syn-
chronizing link-state databases after a partition.
Our basic set-reconciliation method has a similarity with the
peeling algorithm used in Tornado codes [6], which is not surpris-
ing, as there is an intimate connection between set difference and
coding. Beyond set reconciliation, an essential component in our
Difference Digest is a new estimator for the size of the set differ-
ence that outperforms min-wise sketches [3] for small set differ-
ences.
Our experiments show that the Difference Digest is more efﬁ-
cient than prior approaches such as Approximate Reconciliation
Trees [5] and Characteristic Polynomial Interpolation [17]. We
use Difference Digests to implement a generic KeyDiff service in
Linux that runs over TCP and returns the sets of keys that differ
between machines.
Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network Proto-
cols; E.4 [Coding and Information Theory]:
General Terms
Algorithms, Design, Experimentation
1.
INTRODUCTION
Two common tasks in networking and distributed systems are
reconciliation and deduplication. In reconciliation, two hosts each
have a set of keys and each seeks to obtain the union of the two sets.
The sets could be ﬁle blocks in a Peer-to-Peer (P2P) system or link
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15–19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
state packet identiﬁers in a routing protocol. In deduplication, on
the other hand, two hosts each have a set of keys, and the task is
to identify the keys in the intersection so that duplicate data can be
deleted and replaced by pointers [16]. Deduplication is a thriving
industry: for example, Data Domain [1, 21] pioneered the use of
deduplication to improve the efﬁciency of backups.
Both reconciliation and deduplication can be abstracted as the
problem of efﬁciently computing the set difference between two
sets stored at two nodes across a communication link. The set dif-
ference is the set of keys that are in one set but not the other. In
reconciliation, the difference is used to compute the set union; in
deduplication, it is used to compute the intersection. Efﬁciency is
measured primarily by the bandwidth used (important when the two
nodes are connected by a wide-area or mobile link), the latency in
round-trip delays, and the computation used at the two hosts. We
are particularly interested in optimizing the case when the set dif-
ference is small (e.g., the two nodes have almost the same set of
routing updates to reconcile, or the two nodes have a large amount
of duplicate data blocks) and when there is no prior communication
or context between the two nodes.
For example, suppose two users, each with a large collection of
songs on their phones, meet and wish to synchronize their libraries.
They could do so by exchanging lists of all of their songs; however,
the amount of data transferred would be proportional to the total
number of songs they have rather than the size of the difference. An
often-used alternative is to maintain a time-stamped log of updates
together with a record of the time that the users last communicated.
When they communicate again, A can send B all of the updates
since their last communication, and vice versa. Fundamentally, the
use of logs requires prior context, which we seek to avoid.
Logs have more speciﬁc disadvantages as well. First, the log-
ging system must be integrated with any system that can change
user data, potentially requiring system design changes. Second, if
reconciliation events are rare, the added overhead to update a log
each time user data changes may not be justiﬁed. This is partic-
ularly problematic for "hot" data items that are written often and
may be in the log multiple times. While this redundancy can be
avoided using a hash table, this requires further overhead. Third, a
log has to be maintained for every other user this user may wish to
synchronize with. Further, two users A and B may have received
the same update from a third user C, leading to redundant commu-
nication. Multi-party synchronization is common in P2P systems
and in cases where users have multiple data repositories, such as
their phone, their desktop, and in the cloud. Finally, logs require
stable storage and synchronized time which are often unavailable
on networking devices such as routers.
To solve the set-difference problem efﬁciently without the use
of logs or other prior context, we devise a data structure called a
Difference Digest, which computes the set difference with com-
munication proportional to the size of the difference between the
sets being compared. We implement and evaluate a simple key-
synchronization service based on Difference Digests and suggest
how it can improve the performance in several contexts. Settings in
which Difference Digests may be applied include:
• Peer-to-peer: Peer A and B may receive blocks of a ﬁle from
other peers and may wish to receive only missing blocks
from each other.
• Partition healing: When a link-state network partitions, routers
in each partition may each obtain some new link-state pack-
ets. When the partition heals by a link joining router A and
B, both A and B only want to exchange new or changed
link-state packets.
• Deduplication: If backups are done in the cloud, when a new
ﬁle is written, the system should only transmit the chunks of
the ﬁle that are not already in the cloud.
• Synchronizing parallel activations: A search engine may use
two independent crawlers with different techniques to har-
vest URLs but they may have very few URLs that are dif-
ferent. In general, this situation arises when multiple actors
in a distributed system are performing similar functions for
efﬁciency or redundancy.
• Opportunistic ad hoc networks: These are often character-
ized by low bandwidth and intermittent connectivity to other
peers. Examples include rescue situations and military vehi-
cles that wish to synchronize data when they are in range.
The main contributions of the paper are as follows:
• IBF Subtraction: The ﬁrst component of the Difference Di-
gest is an Invertible Bloom Filter or IBF [9, 13]. IBF’s were
previously used [9] for straggler detection at a single node, to
identify items that were inserted and not removed in a stream.
We adapt Invertible Bloom Filters for set reconciliation by
deﬁning a new subtraction operator on whole IBF’s, as op-
posed to individual item removal.
• Strata Estimator: Invertible Bloom Filters need to be sized
appropriately to be efﬁciently used for set differences. Thus
a second crucial component of the Difference Digest is a new
Strata Estimator method for estimating the size of the differ-
ence. We show that this estimator is much more accurate for
small set differences (as is common in the ﬁnal stages of a ﬁle
dissemination in a P2P network) than Min-Wise Sketches [3,
4] or random projections [14]. Besides being an integral part
of the Difference Digest, our Strata Estimator can be used
independently to ﬁnd, for example, which of many peers is
most likely to have a missing block.
• KeyDiff Prototype: We describe a Linux prototype of a generic
KeyDiff service based on Difference Digests that applica-
tions can use to synchronize objects of any kind.
• Performance Characterization: The overall system perfor-
mance of Difference Digests is sensitive to many parame-
ters, such as the size of the difference, the bandwidth avail-
able compared to the computation, and the ability to do pre-
computation. We characterize the parameter regimes in which
Difference Digests outperform earlier approaches, such as
MinWise hashes, Characteristic Polynomial Interpolation [17],
and Approximate Reconciliation Trees [5].
Going forward, we discuss related work in Section 2. We present
our algorithms and analysis for the Invertible Bloom Filter and
Strata Estimator in Sections 3 & 4. We describe our KeyDiff proto-
type in Section 5, evaluate our structures in Section 6 and conclude
in Section 7.
2. MODEL AND RELATED WORK
We start with a simple model of set reconciliation. For two sets
SA, SB each containing elements from a universe, U = [0, u),
we want to compute the set difference, DA−B and DB−A, where
DA−B = SA − SB such that for all s ∈ DA−B, s ∈ SA and
s /∈ SB. Likewise, DB−A = SB −SA. We say that D = DA−B ∪
DB−A and d = |D|. Note that since DA−B ∩ DB−A = ∅, d =
|DA−B| + |DB−A|. We assume that SA and SB are stored at
two distinct hosts and attempt to compute the set difference with
minimal communication, computation, storage, and latency.
Several prior algorithms for computing set differences have been
proposed. The simplest consists of hosts exchanging lists, each
containing the identiﬁers for all elements in their sets, then scan-
ning the lists to remove common elements. This requires O(|SA| +
|SB|) communication and O(|SA| × |SB|) time. The run time can
be improved to O(|SA| + |SB|) by inserting one list into a hash
table, then querying the table with the elements of the second list.
The communication overhead can be reduced by a constant fac-
tor by exchanging Bloom ﬁlters [2] containing the elements of each
list. Once in possession of the remote Bloom Filter, each host can
query the ﬁlter to identify which elements are common. Funda-
mentally, a Bloom Filter still requires communication proportional
to the size of the sets (and not the difference) and incurs the risk of
false positives. The time cost for this procedure is O(|SA| + |SB|).
The Bloom ﬁlter approach was extended using Approximate Rec-
onciliation Trees [5], which requires O(d log(|SB|)) recovery time,
and O(|SB|) space. However, given SA and an Approximate Rec-
onciliation Tree for SB, a host can only compute DA−B, i.e., the
elements unique to SA.
An exact approach to the set-difference problem was proposed
by Minksy et al. [17]. In this approach, each set is encoded us-
ing a linear transformation similar to Reed-Solomon coding. This
approach has the advantage of O(d) communication overhead, but
requires O(d3) time to decode the difference using Gaussian elim-
ination; asymptotically faster decoding algorithms are known, but
their practicality remains unclear. Additionally, whereas our Strata
Estimator gives an accurate one-shot estimate of the size of the dif-
ference prior to encoding the difference itself, Minsky et al. use an
iterative doubling protocol for estimating the size of the difference,
with a non-constant number of rounds of communication. Both the
decoding time and the number of communication rounds (latency)
of their system are unfavorable compared to ours.
Estimating Set-Difference Size. A critical sub-problem is an
initial estimation of the size of the set difference. This can be esti-
mated with constant overhead by comparing a random sample [14]
from each set, but accuracy quickly deteriorates when the d is small
relative to the size of the sets.
Min-wise sketches [3, 4] can be used to estimate the set similar-
ity (r = |SA∩SB |
|SA∪SB | ). Min-wise sketches work by selecting k random
hash functions π1, . . . , πk which permute elements within U. Let
min(πi(S)) be the smallest value produced by πi when run on the
elements of S. Then, a Min-wise sketch consists of the k values
min(π1(S)), . . . , min(πk(S)). For two Min-wise sketches, MA
and MB, containing the elements of SA and SB, respectively, the
set similarity is estimated by the of number of hash function return-
ing the same minimum value. If SA and SB have a set-similarity r,
then we expect that the number of matching cells in MA and MB
will be m = rk. Inversely, given that m cells of MA and MB do
match, we can estimate that r = m
k . Given the set-similarity, we
can estimate the difference as d = 1−r
1+r (|SA| + |SB|).
As with random sampling, the accuracy of the Min-wise esti-
mator diminishes for smaller values of k and for relatively small
set differences. Similarly, Cormode et al. [8] provide a method for
dynamic sampling from a data stream to estimate set sizes using a
hierarchy of samples, which include summations and counts. Like-
wise, Cormode and Muthukrishnan [7] and Schweller et al. [20]
describe sketch-based methods for ﬁnding large differences in traf-
ﬁc ﬂows. (See also [19].)
An alternative approach to estimating set-difference size is pro-
vided by [11], whose algorithm can more generally estimate the
difference between two functions with communication complex-
ity very similar to our Strata Estimator. As with our results, their
method has a small multiplicative error even when the difference is
small. However, it uses algebraic computations over ﬁnite ﬁelds,
whereas ours involves only simple, and more practical, hashing-
based data structures.
While efﬁciency of set difference estimation for small differ-
ences may seem like a minor theoretical detail, it can be important
in many contexts. Consider, for instance, the endgame of a P2P ﬁle
transfer. Imagine that a BitTorrent node has 100 peers, and is miss-
ing only the last block of a ﬁle. Min-wise or random samples from
the 100 peers will not identify the right peer if all peers also have
nearly ﬁnished downloading (small set difference). On the other
hand, sending a Bloom Filter takes bandwidth proportional to the
number of blocks in ﬁle, which can be large. We describe our new
estimator in Section 3.2.
3. ALGORITHMS
In this section, we describe the two components of the Differ-
ence Digest: an Invertible Bloom Filter (IBF) and a Strata Estima-
tor. Our ﬁrst innovation is taking the existing IBF [9, 13] and in-
troducing a subtraction operator in Section 3.1 to compute DA−B
and DB−A using a single round of communication of size O(d).
Encoding a set S into an IBF requires O(|S|) time, but decoding
to recover DA−B and DB−A requires only O(d) time. Our sec-
ond key innovation is a way of composing several sampled IBF’s
of ﬁxed size into a new Strata Estimator which can effectively esti-
mate the size of the set difference using O(log(|U|)) space.
3.1 Invertible Bloom Filter
We now describe the Invertible Bloom Filter (IBF), which can si-
multaneously calculate DA−B and DB−A using O(d) space. This
data structure encodes sets in a fashion that is similar in spirit to
Tornado codes’ construction [6], in that it randomly combines ele-
ments using the XOR function. We will show later that this simi-
larity is not surprising as there is a reduction between set difference
and coding across certain channels. For now, note that whereas Tor-
nado codes are for a ﬁxed set, IBF’s are dynamic and, as we show,
even allow for fast set subtraction operations. Likewise, Tornado
codes rely on Reed-Solomon codes to handle possible encoding
errors, whereas IBF’s succeed with high probability without rely-
ing on an inefﬁcient fallback computation. Finally, our encoding is
much simpler than Tornado codes because we use a simple uniform
random graph for encoding while Tornado codes use more complex
random graphs with non-uniform degree distributions.
We start with some intuition. An IBF is named because it is sim-
ilar to a standard Bloom Filter—except that it can, with the right
settings, be inverted to yield some of the elements that were in-
serted. Recall that in a counting Bloom Filter [10], when a key
K is inserted, K is hashed into several locations of an array and
a count, count, is incremented in each hashed location. Deletion
of K is similar except that count is decremented. A check for
whether K exists in the ﬁlter returns true if all locations that K
hashes to have non-zero count values.
An IBF has another crucial ﬁeld in each cell (array location) be-
sides the count. This is the idSum: the XOR of all key IDs that
hash into that cell. Now imagine that two peers, Peer 1 and Peer 2,
doing set reconciliation on a large ﬁle of a million blocks indepen-
dently compute IBF’s, B1 and B2, each with 100 cells, by inserting
an ID for each block they possess. Note that a standard Bloom ﬁl-
ter would have a size of several million bits to effectively answer
whether a particular key is contained in the structure. Observe also
that if each ID is hashed to 3 cells, an average of 30,000 keys hash
onto each cell. Thus, each count will be large and the idSum in
each cell will be the XOR of a large number of IDs. What can we
do with such a small number of cells and such a large number of
collisions?
Assume that Peer 1 sends B1 to Peer 2, an operation only requir-
ing bandwidth to send around 200 ﬁelds (100 cells × 2 ﬁelds/cell).
Peer 2 then proceeds to “subtract” its IBF B2 from B1. It does this
cell by cell, by subtracting the count and XORing the idSum in
the corresponding cells of the two IBF’s.
Intuitively, if the two peers’ blocks sets are almost the same (say,
25 different blocks out of a million blocks), all the common IDs
that hash onto the same cell will be cancelled from idSum, leaving
only the sum of the unique IDs (those that belong to one peer and
not the other) in the idSum of each cell. This follows assuming that
Peer 1 and Peer 2 use the same hash function so that any common
element, c, is hashed to the same cells in both B1 and B2. When
we XOR the idSum in these cells, c will disappear because it is
XORed twice.
In essense, randomly hashing keys to, say three, cells, is identical
to randomly throwing three balls into the same 100 bins for each of
the 25 block ID’s. Further, we will prove that if there are sufﬁcient
cells, there is a high probability that at least one cell is “pure” in
that it contains only a single element by itself.
A “pure” cell signals its purity by having its count ﬁeld equal
to 1, and, in that case, the idSum ﬁeld yields the ID of one ele-
ment in the set difference. We delete this element from all cells it
has hashed to in the difference IBF by the appropriate subtractions;
this, in turn, may free up more pure elements that it can in turn be
decoded, to ultimately yield all the elements in the set difference.
The reader will quickly see subtleties. First, a numerical count
value of 1 is necessary but not sufﬁcient for purity. For example, if
we have a cell in one IBF with two keys X and Y and the corre-
sponding cell in the second IBF has key Z, then when we subtract
we will get a count of 1, but idSum will have the XOR of X, Y
and Z. More devious errors can occur if four IDs W, X, Y, Z sat-
isfy W + X = Y + Z. To reduce the likelihood of decoding errors
to an arbitrarily small value, IBF’s use a third ﬁeld in each cell as
a checksum: the XOR of the hashes of all IDs that hash into a cell,
but using a different hash function Hc than that used to determine
the cell indices. If an element is indeed pure, then the hash sum
should be Hc(idSum ).
A second subtlety occurs if Peer 2 has an element that Peer 1
does not. Could the subtraction B1 − B2 produce negative values
for idSum and count? Indeed, it can and the algorithm deals with
this: for example, in idSum by using XOR instead of addition and
subtraction, and in recognizing purity by count values of 1 or -1.
While IBF’s were introduced earlier [9, 13], whole IBF subtraction
is new to this paper; hence, negative counts did not arise in [9, 13].
Figure 1 summarizes the encoding of a set S and Figure 2 gives
a small example of synchronizing the sets at Peer 1 (who has keys
S = 
B1=
1
Hk
2
Hk
...
k
Hk
B:
B[j]:
idSum += s1
hashSum += Hc(s1)
count++
Figure 1: IBF Encode. Hash functions are used to map each
element of the set to k cells of the IBF table.
V, W, X and Y ) and Peer 2 (who has keys W, Y and Z). Each
element is hashed into 3 locations: for example, X is hashed into
buckets 1, 2 and 3. While X is by itself in bucket 3, after sub-
traction Z also enters, causing the count ﬁeld to (incorrectly) be
zero. Fortunately, after subtraction, bucket 4 becomes pure as V