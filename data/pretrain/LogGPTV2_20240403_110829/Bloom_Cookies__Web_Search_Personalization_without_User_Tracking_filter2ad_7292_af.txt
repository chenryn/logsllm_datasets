information, particularly location [26] and
search queries [5], to online services. “Transformation-based
techniques” which employ cryptographic transformations to
hide a user’s data and “progressive retrieval techniques” that it-
eratively retrieve candidate results from the server yield a high
overhead for the client and cannot be implemented without the
13
cooperation of the server. They have not been applied in the
context of web search. Generalization techniques [26], [49]
replace quasi-identiﬁers with less speciﬁc, but semantically
consistent, values. These approaches are likely not to work for
search as we have seen in §IV when comparing interest-based
and URL-based proﬁles. Literature [18] in the web community
has conﬁrmed this result, although different opinions exist [49].
Bloom cookies adopt noise addition techniques (extensively
described in [5]) which can be implemented at the client.
Our baseline HYBRID is inspired by Plausibly Deniable Search
(PDS) [31].
We present Bloom cookies in the context of web search.
Extensive research has been done on search personalization. As
shown in [18], personalized search approaches can be classiﬁed
depending on whether information relating to a single user or
a group of users is used to create the user proﬁles. Group-level
personalization (e.g., collaborative ﬁltering) requires the server
side involvement. In this work, we focus on client-side solu-
tions, and hence consider only person-level techniques. Person-
level techniques typically mine user proﬁles at the granularity
of user interests or URLs. There are two main approaches to
achieve person-level re-ranking of search results [18]: URL-
based proﬁles and interest-based proﬁle. We implement both
and compare their performance.
We are aware of only one other work that leverages Bloom
ﬁlters as privacy structures [8]. The authors propose specialized
techniques for setting bits in a Bloom ﬁlter with the goal of
achieving deniability of the stored items. After implementing
their techniques, we deemed them not suitable for our use case
because they are too computationally expensive, they generate
deterministic noise, and they still require the presence of a
dictionary.
VII. LIMITATIONS
Small dataset. We have evaluated Bloom cookies with the
search logs of 1300 users. The number of users in a real
online service can be several orders of magnitude more. The
personalization quality for a certain user is not affected by
other users in the system, and hence we expect our personal-
ization results to be similar with a larger dataset. More users,
however, are likely to improve the privacy of Bloom cookies.
This is because with more users in the system, a user’s noisy
proﬁle is more likely to “collide” with (i.e., look similar to)
another user, making it difﬁcult for the server to link a user’s
proﬁle over time. This was conﬁrmed by our experiments in
§V-D. Moreover, as described in our threat model (see §II-B),
using a small user population gives us a worst case scenario
that takes into account cases in which a malicious server has
other means to reduce the user population size (e.g., using
geolocation, ﬁltering by language, etc.)
User feedback. The privacy guarantee of Bloom cookies is
a statistical one, rather than an absolute one. As our results
show, Bloom cookies achieve unlinkability for a good fraction
of the users, but there are users whose online activities and
interests are so distinct that even with the amount of noise we
added, they remained linkable across IP-sessions. We believe
that it might be possible to provide feedback to those users so
that they can decide to add more noise or use other measures
that provide better privacy at the cost of personalization. Our
future work includes developing such feedback techniques.
Longer term analysis. Our evaluation assumes 2 consecutive
2-week periods of user activities. With longer user history this
evaluation can scale to longer and more time periods.
Applicability to other applications. We focus on web search
and use personalization techniques (and metrics) which are
standard in the web literature. Commercial search engines
might use more sophisticated but proprietary techniques. How-
ever, if personalization algorithms rearrange search results as
the last step before returning them to users and if rearrang-
ing is performed solely on the presence of certain items in
user proﬁles, our privacy-preserving approach will work in a
straightforward way. This is true not only for web search, but
also for services with a similar structure such as advertising
(ranking of ads), online shopping (ranking of products), or
personalized news delivery (ranking of news). It might even be
possible for different services to share the data responsibly in a
privacy preserving way, provided they are interested in similar
information about the user, such as the top visited websites.
However privacy-personalization analysis of such a scenario is
out of scope of the current work.
Tracking across services. Unfortunately, tracking often occurs
across services. For instance, Google or Microsoft can track
users using web search, email, or voice calls. As discussed
in our threat model (see §II-B), this scenario is out of scope
for this paper, but this work sets the ﬁrst step towards such
a vision because IP-based tracking is unavoidable in client-
server setups (unless anonymization proxies can be afforded).
To address a multi-service scenario, it is ﬁrst necessary to
understand which information every service needs for person-
alization and then how it can be obfuscated. We chose web
search also because of its relatively mature personalization,
which is not the case for most services.
VIII. CONCLUSIONS
Bloom cookies encode a user’s proﬁle in a compact and
privacy-preserving way, but do not prevent online services
from achieving personalization. Compared to proﬁle gener-
alization and noise addition techniques commonly employed
in online privacy-preserving personalization systems, Bloom
cookies provide a much better privacy, personalization and net-
work efﬁciency tradeoff. Through the analysis of web search
logs, we showed that proﬁle generalization signiﬁcantly hurts
personalization and fails in providing reasonable unlinkability.
Noise injection can address these problems, but comes with the
cost of a high communication overhead and a noise dictionary
which must be provided by a trusted third party. Bloom cookies
leverage Bloom ﬁlters as a privacy-preserving structure to
deliver similar (or better) personalization and unlinkability
than noise injection, but with an order of magnitude lower
communication cost and no noise dictionary dependencies.
ACKNOWLEDGEMENTS
This work was partly supported by the TerraSwarm Re-
search Center, one of six centers supported by the STAR-
net phase of the Focus Center Research Program (FCRP) a
Semiconductor Research Corporation program sponsored by
MARCO and DARPA. We thank Doug Burger for initially
suggesting Bloom ﬁlters for service personalization. We also
thank Ryen White and Dan Liebling for help understanding
web search personalization algorithms and processing search
logs.
REFERENCES
“Open Directory Project,” http://dmoz.org/.
[1]
[2] G. Aggarwal, E. Bursztein, C. Jackson, and D. Boneh, “An Analysis
of Private Browsing Modes in Modern Browsers,” in USENIX Security
Symposium, 2010, pp. 79–94.
[3] C. Ardagna, M. Cremonini, E. Damiani, S. De Capitani di Vimercati,
and P. Samarati, “Location privacy protection through obfuscation-
based techniques,” in Data and Applications Security XXI, ser. LNCS.
Springer Berlin Heidelberg, 2007, vol. 4602, pp. 47–60.
[4] M. Balakrishnan, I. Mohomed, and V. Ramasubramanian, “Where’s that
phone?: geolocating IP addresses on 3G networks,” in Proc. of IMC ’09,
2009, pp. 294–300.
[5] E. Balsa, C. Troncoso, and C. Diaz, “OB-PWS: Obfuscation-Based
Private Web Search,” in Proceedings of the 2012 IEEE Symposium on
Security and Privacy.
IEEE Computer Society, 2012, pp. 491–505.
[6] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais, P. Bailey,
F. Borisyuk, and X. Cui, “Modeling the impact of short- and long-
term behavior on search personalization,” in Proc. of SIGIR ’12, 2012,
pp. 185–194.
[7] R. Bhagwan, S. Savage, and G. Voelker, “Understanding availability,”
Springer Berlin Heidelberg,
in Peer-to-Peer Systems II, ser. LNCS.
2003, vol. 2735, pp. 256–267.
[8] G. Bianchi, L. Bracciale, and P. Loreti, “‘Better Than Nothing’ Pri-
vacy with Bloom Filters: To What Extent?” in Privacy in Statistical
Databases, ser. Lecture Notes in Computer Science. Springer Berlin
Heidelberg, 2012, vol. 7556, pp. 348–363.
[9] B. H. Bloom, “Space/time trade-offs in hash coding with allowable
errors,” Commun. ACM, vol. 13, no. 7, pp. 422–426, Jul. 1970.
[10] A. Broder and M. Mitzenmacher, “Network Applications of Bloom
Filters: A Survey,” Internet Mathematics, vol. 1, no. 4, pp. 485–509,
2004.
[11] M. Casado and M. J. Freedman, “Peering Through the Shroud: The
Effect of Edge Opacity on Ip-based Client Identiﬁcation,” in Proc. of
NSDI ’07. USENIX Association, 2007, pp. 13–13.
[12] P. A. Chirita, W. Nejdl, R. Paiu, and C. Kohlsch¨utter, “Using ODP
metadata to personalize search,” in Proc. of SIGIR ’05, 2005, pp. 178–
185.
[13] S. Claußand S. Schiffner, “Structuring Anonymity Metrics,” in Proc. of
the 2nd ACM Workshop on Digital Identity Management, ser. DIM ’06,
2006, pp. 55–62.
comScore, “The Myth of Static IP,” Sept 2008, http://www.comscore.
com/Insights/Blog/The Myth of Static IP.
[14]
[15] S. Cronen-Townsend and W. B. Croft, “Quantifying query ambiguity,”
Proc. of HLT’02, pp. 94–98, 2002.
[16] R. Dingledine, N. Mathewson, and P. Syverson, “Tor: the second-
generation onion router,” in Proc. of the 13th conference on USENIX
Security Symposium - Volume 13, 2004, pp. 21–21.
J. Domingo-Ferrer, A. Solanas, and J. Castell`a-Roca, “h(k)-private
information retrieval from privacy-uncooperative queryable databases,”
Online Information Review, vol. 33, no. 4, pp. 720–744, 2009.
[17]
[18] Z. Dou, R. Song, and J.-R. Wen, “A large-scale evaluation and analysis
of personalized search strategies,” in Proc. of WWW ’07, 2007, pp.
581–590.
[19] M. Franz, B. Meyer, and A. Pashalidis, “Attacking Unlinkability:
The Importance of Context,” in Privacy Enhancing Technologies, ser.
Lecture Notes in Computer Science, 2007, vol. 4776, pp. 1–16.
[20] M. Fredrikson and B. Livshits, “RePriv: Re-imagining Content Person-
alization and In-browser Privacy,” in IEEE Symposium on Security and
Privacy, 2011, pp. 131–146.
[21] S. Gauch, J. Chaffee, and A. Pretschner, “Ontology-based personalized
search and browsing,” Web Intelli. and Agent Sys., vol. 1, no. 3-4, pp.
219–234, Dec. 2003.
14
[22] D. Goldschlag, M. Reed, and P. Syverson, “Onion routing,” Commun.
ACM, vol. 42, no. 2, pp. 39–41, Feb. 1999.
[23] S. Guha, B. Cheng, and P. Francis, “Privad: Practical Privacy in Online
Advertising,” in Proc. of NSDI ’11, Boston, MA, 2011.
[24] D. Howe and H. Nissenbaum, “TrackMeNot: Resisting Surveillance in
Web Search,” in In Lessons from the Identity Trail: Anonymity, Privacy,
and Identity in a Networked Society, ser. Oxford: Oxford University
Press, I. Kerr, C. Lucock, and V. Steeves, Eds., 2009, pp. 31–58.
[25] B. J. Jansen, A. Spink, and T. Saracevic, “Real life, real users, and real
needs: a study and analysis of user queries on the web,” Information
Processing and Management, vol. 36, no. 2, pp. 207 – 227, 2000.
[26] C. S. Jensen, H. Lu, and M. Yiu, “Location privacy techniques in client-
server architectures,” in Privacy in Location-Based Applications, ser.
Lecture Notes in Computer Science, C. Bettini, S. Jajodia, P. Samarati,
and X. Wang, Eds., 2009, vol. 5599, pp. 31–58.
[27] A. Juels, “Targeted Advertising ... And Privacy Too,” in Proc. of the
2001 Conference on Topics in Cryptology: The Cryptographer’s Track
at RSA (CT-RSA 2001), 2001, pp. 408–424.
[28] H. Kido, Y. Yanagisawa, and T. Satoh, “An anonymous communication
technique using dummies for location-based services,” in Proc. of ICPS
’05, July 2005, pp. 88–97.
[29] R. Krovetz and W. B. Croft, “Lexical ambiguity and information
retrieval,” ACM Trans. Inf. Syst., vol. 10, no. 2, pp. 115–141, Apr. 1992.
[30] Z. Ma, G. Pant, and O. R. L. Sheng, “Interest-based personalized
search,” ACM Trans. Inf. Syst., vol. 25, no. 1, Feb. 2007.
[31] M. Murugesan and C. Clifton, “Providing Privacy through Plausibly
Deniable Search,” in Proc. of the SIAM International Conference on
Data Mining (SDM ’09), 2009, pp. 768–779.
[32] N. Nikiforakis, A. Kapravelos, W. Joosen, C. Kruegel, F. Piessens, and
G. Vigna, “Cookieless monster: Exploring the ecosystem of web-based
device ﬁngerprinting,” in IEEE Security and Privacy, 2013.
[33] M. S. Olivier, “Distributed proxies for browsing privacy: a simulation
of ﬂocks,” in Proc. of SAICSIT ’05, 2005, pp. 104–112.
[34] K. Purcell, J. Brenner, and L. Rainie, “Search Engine Use 2012,” March
http://pewinternet.org/Reports/2012/Search-Engine-Use-2012.
2012,
aspx.
[35] D. Rebollo-Monedero and J. Forn´e, “Optimized query forgery for pri-
vate information retrieval,” IEEE Transactions on Information Theory,
vol. 56, no. 9, pp. 4631–4642, 2010.
[36] B. Shapira, Y. Elovici, A. Meshiach, and T. Kuﬂik, “PRAW - A
PRivAcy model for the Web,” J. Am. Soc. Inf. Sci. Technol., vol. 56,
no. 2, pp. 159–172, Jan. 2005.
[37] X. Shen, B. Tan, and C. Zhai, “Implicit user modeling for personalized
search,” in Proc. of CIKM ’05, 2005, pp. 824–831.
[38] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz, “Analysis of a
very large web search engine query log,” SIGIR Forum, vol. 33, no. 1,
pp. 6–12, Sep. 1999.
[39] H. Song, S. Dharmapurikar, J. Turner, and J. Lockwood, “Fast hash table
lookup using extended bloom ﬁlter: An aid to network processing,” in
Proc. of SIGCOMM ’05. ACM, 2005, pp. 181–192.
[40] R. Song, Z. Luo, J.-R. Wen, Y. Yu, and H.-W. Hon, “Identifying
Ambiguous Queries in Web Search,” in Proc. of WWW ’07. ACM,
2007, pp. 1169–1170.
[41] D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Du-
mais, and B. Billerbeck, “Probabilistic models for personalizing web
search,” in Proc. of WSDM ’12, 2012, pp. 433–442.
[42] M. Speretta and S. Gauch, “Personalized Search Based on User Search
Histories,” in Proc. of the IEEE/WIC/ACM International Conference on
Web Intelligence (WI’05), 2005, pp. 622–628.
[44]
[43] L. Sweeney, “K-anonymity: A Model for Protecting Privacy,” Int. J.
Uncertain. Fuzziness Knowl.-Based Syst., vol. 10, no. 5, pp. 557–570,
Oct. 2002.
J. Teevan, S. T. Dumais, and E. Horvitz, “Personalizing search via
automated analysis of interests and activities,” in Proc. of SIGIR ’05,
2005, pp. 449–456.
J. Teevan, S. T. Dumais, and D. J. Liebling, “To personalize or not to
personalize: modeling queries with variation in user intent,” in Proc.
SIGIR ’08, 2008, pp. 163–170.
[45]
Fig. 6: Linkability model trained over 300 users with exact
proﬁles (URLs).
[46] G. T´oth, Z. Horn´ak, and F. Vajda, “Measuring anonymity revisited,”
in Proc. of the 9th Nordic Workshop on Secure IT Systems, 2004, pp.
85–90.
[47] V. Toubiana, A. Narayanan, D. Boneh, H. Nissenbaum, and S. Barocas,
“Adnostic: Privacy Preserving Targeted Advertising,” in Proc. of NDSS
’10, 2010.
[48] Y. Xie, F. Yu, K. Achan, E. Gillum, M. Goldszmidt, and T. Wobber,
“How dynamic are IP addresses?” vol. 37, no. 4, pp. 301–312, 2007.
[49] Y. Xu, K. Wang, B. Zhang, and Z. Chen, “Privacy-enhancing Person-
alized Web Search,” in Proc. of WWW ’07, 2007, pp. 591–600.
[50] S. Ye, S. F. Wu, R. Pandey, and H. Chen, “Noise Injection for Search
Privacy Protection,” in Proc. of IEEE CSE ’09, 2009, pp. 1–8.
APPENDIX
Linkability model. As described in §III-B, the linkability
model is essentially a function that maps the Jaccard similarity
of a pair of user proﬁles to the probability of these proﬁles
belonging to the same user. To calculate this mapping function,
we take n test users, and for each user we compute two
proﬁles, one for the time period T 1 and one for T 2 (both
2-week long). Next, we calculate the Jaccard similarity for
the n2 proﬁle pairs. We divide the entire range of possible
similarities (varying from 0 to 1) into a ﬁxed number (100) of
buckets of equal size. For each bucket, we ﬁnd how many of
these n2 proﬁle pairs have similarities that lie in that particular
range. From the fraction of these proﬁle pairs that belong to
the same user (known because the ground truth is available
in this case), we calculate a conditional probability, i.e., the
probability that a proﬁle pair belongs to the same user given
that the similarity of the proﬁles lies within a certain range.
Figure 6 shows such a mapping function with a Bezier
interpolation16 when using exact proﬁles (URLs). As expected,
if the similarity is above a certain threshold, the probability that
the proﬁles belong to same user becomes almost 1.
We calculate a mapping function also for generalized
proﬁles and for each noisy proﬁle we use in our evaluation.
When noise is added to the proﬁle, we ﬁrst add the noise to the
test users’ proﬁles and then repeat the same process described
above.
16http://en.wikipedia.org/wiki/Bezier curve
15