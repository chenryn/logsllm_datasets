title:SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial
Perturbations
author:Giulio Lovisotto and
Henry Turner and
Ivo Sluganovic and
Martin Strohmeier and
Ivan Martinovic
SLAP: Improving Physical Adversarial Examples 
with Short-Lived Adversarial Perturbations
Giulio Lovisotto, Henry Turner, and Ivo Sluganovic, University of Oxford; 
Martin Strohmeier, armasuisse; Ivan Martinovic, University of Oxford
https://www.usenix.org/conference/usenixsecurity21/presentation/lovisotto
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.SLAP: Improving Physical Adversarial Examples with
Short-Lived Adversarial Perturbations
Giulio Lovisotto
University of Oxford, UK
Henry Turner
University of Oxford, UK
Ivo Sluganovic
University of Oxford, UK
Martin Strohmeier
armasuisse
Ivan Martinovic
University of Oxford, UK
Abstract
Research into adversarial examples (AE) has developed
rapidly, yet static adversarial patches are still the main tech-
nique for conducting attacks in the real world, despite being
obvious, semi-permanent and unmodiﬁable once deployed.
In this paper, we propose Short-Lived Adversarial Pertur-
bations (SLAP), a novel technique that allows adversaries to
realize physically robust real-world AE by using a projector.
Attackers can project speciﬁcally crafted adversarial pertur-
bations onto real-world objects, transforming them into AE.
This grants adversaries greater control over the attack com-
pared to adversarial patches, as projections can be turned on
and off as needed and leave no obvious trace of an attack.
We study the feasibility of SLAP in the self-driving sce-
nario, targeting both object detector and trafﬁc sign recogni-
tion tasks, focusing on the detection of stop signs. We conduct
experiments in a variety of ambient light conditions, includ-
ing outdoors, showing how in non-bright settings the pro-
posed method generates AE that are extremely robust, caus-
ing misclassiﬁcations on state-of-the-art neural networks with
up to 99% success rate. Our experiments show that SLAP-
generated AE do not present detectable behaviours seen in
adversarial patches and therefore bypass SentiNet, a physical
AE detection method. We evaluate other defences including
an adaptive defender using adversarial learning which is able
to thwart the attack effectiveness up to 80% even in favourable
attacker conditions.
1 Introduction
Recent advances in computational capabilities and machine
learning algorithms have led to deep neural networks (DNN)
rapidly becoming the dominant choice for a wide range of
computer vision tasks. Due to their performance, DNNs are
increasingly being used in security-critical contexts, such as
biometric authentication or object recognition for autonomous
driving. However, if a malicious actor controls the input to
the network, DNNs are susceptible to carefully crafted adver-
sarial examples (AE) [40], which leverage speciﬁc directions
(a) Non adversarial scenario.
(b) Adversarial projection.
Figure 1: The attack visualized. A projector shines a speciﬁc
pattern on the stop sign causing an object detector (Yolov3 in
this picture) to misdetect the object.
in input space to create examples which whilst resembling
legitimate images, will be misclassiﬁed at test time.
A signiﬁcant body of earlier research focused on analyzing
AE in the digital domain, where an adversary has the capabil-
ity of making pixel-speciﬁc manipulations to the input. This
concept has been further developed with the realization of
physically robust AE [12, 15, 29, 37, 38, 43], which are exam-
ples that survive real-world environmental conditions, such
as varied viewing distances or angles. In order to realize AE,
adversaries can either print patches (e.g. as stickers or glasses
in the case of face recognition), or replace an entire object by
overlaying the object with a printed version of it with subtle
changes. However, these techniques have multiple limitations.
Firstly, these methods typically generate highly salient areas
in the network inputs, which makes them detectable by recent
countermeasures [13]. Secondly, in the autonomous driving
scenario, sticking patches on a trafﬁc sign leads to continuous
misdetection of such signs, which is equivalent to removing
the sign from the road or covering it.
In this paper, we focus on road safety with autonomous
vehicles and propose using a light projector to achieve Short-
Lived Adversarial Perturbations (SLAPs), a novel AE ap-
proach that allows adversaries to realize robust, dynamic real-
world AE from a distance. SLAP-generated AE provide the
attacker with multiple beneﬁts over existing patch-based meth-
ods, in particular giving ﬁne-grained control over the timing
of attacks, allowing them to become short-lived.
USENIX Association
30th USENIX Security Symposium    1865
As part of designing the SLAP attack, we propose a method
to model the effect of projections under certain environmental
conditions, by analyzing the absolute changes in pixel colors
captured by an RGB camera as different projections are being
shown. The method consists of ﬁtting a differentiable model,
which we propagate the derivatives of the projection through
during the AE crafting phase. Our method improves the estab-
lished non printability score [37] (NPS) used in patch-based
AE by modelling a three-way additive relationship between
the projection surface, the projection color, and the camera-
perceived output. Furthermore, we improve the robustness
of AE in the physical world by systematically identifying
and accounting for a large set of environmental changes. We
empirically analyze the feasibility of SLAP on two different
use-cases: (i) object detection and (ii) trafﬁc sign recognition.
To understand the relationship between ambient light and
attack feasibility, we collect extensive measurements in dif-
ferent light conditions, including outdoors. We conduct our
attack on four different models: Yolov3, Mask-RCNN, Lisa-
CNN, and Gtsrb-CNN, demonstrating the attack can success-
fully render a stop sign undetected in over 99% of camera
frames, depending on ambient light levels.
We also evaluate the transferability of our attack, showing
that depending on the model used during the AE crafting
phase, SLAP could be used to conduct black-box attacks.
In particular, we show that AE generated with Mask-RCNN
and Yolov3 transfer onto the proprietary Google Vision API
models in up to 100% of cases.
Finally, we evaluate potential defences. We show that SLAP
can bypass SentiNet [13], a recent defence tailored to phys-
ical AE detection. Since SLAP does not present a locality
constraint in the same way as adversarial patches, SLAP AE
bypass SentiNet over 95% of the time. We investigate other
countermeasures and ﬁnd that an adaptive defender using ad-
versarial learning can prevent most attacks, but at the cost of
reduced accuracy in non-adversarial conditions.
Contributions.
• We propose SLAP, a novel attack vector for the realiz-
ability of AE in the physical world by using an RGB
projector. This technique gives the attacker new capabil-
ities compared to existing approaches, including short-
livedness and undetectability.
• We propose a method to craft robust AE designed for use
with a projector. The method models a three-way addi-
tive relationship between a surface, a projection and the
camera-perceived image. We enhance the robustness of
the attacks by systematically identifying and accounting
for varying environmental conditions during the opti-
mization process.
• We evaluate the SLAP attack on two different computer
vision tasks related to road safety for autonomous driv-
ing: (i) object detection and (ii) trafﬁc sign recognition.
We conduct an extensive empirical evaluation, including
in- and out-doors, showing that under favourable light-
ing conditions the attack leads to the target object being
undetected.
• We evaluate countermeasures. We ﬁrstly show that
SLAP AE bypass locality-based detection measures such
as SentiNet [13], which is tailored for the detection of
physical AE. We then show that an adaptive defender
using adversarial learning can thwart most of the attacks.
2 Background and Related Work
We start by introducing the necessary background on LCD
projectors and object detection. We then cover the related
work in physically-realizable adversarial examples.
2.1 Projector technology
A common LCD (liquid crystal display) projector works by
sending light through a series of dichroic ﬁlters in order to
form the red, green and blue components of the projected
images. As the light passes through, individual pixels may be
opened or closed to allow the light to pass, creating a wide
range of colors. The total amount of light that projectors emit
(measured in lumens), as well as the amount of light per area
(measured in lux) is an important factor for determining the
image quality, with stronger output leading to more accurate
images in a range of conditions. Common ofﬁce projectors
are in the range of 2,000-3,000 lumens of emitted light, while
the higher-end projectors can achieve up to tens of thousands
of lumens (e.g., the projectors used during the London 2012
Olympics [9]). As lumens only measure the total quantity of
visible light emitted from the projector, the current ambient
light perceived on the projection surface has an important role
in determining the formed image contrast and color quality.
The brighter the ambient light, the less visible will the image
formed by a projector be due to weaker contrast and narrower
range of colors.
As an example, a 2,000 ANSI lumens projector can emit
enough light to obtain a light intensity of 2,000 lux on a square
meter area (measured for white light [39]). Such a projector
would reproduce an image in an ofﬁce quite well (ambient
~500 lux), but could hardly make the image visible if it was
placed outside in a sunny day (~18,000 lux). Additionally,
projectors are generally used and tested while projecting on
a (white) projection screen, which are designed to optimize
the resulting image quality. When projecting on different
materials and non-white surfaces, the resulting image will
vary greatly given that light propagation signiﬁcantly changes
depending on the material in use and the background color.
In Section 4.1 we explain how we model such changes in an
empirical way that accounts for many variability factors.
1866    30th USENIX Security Symposium
USENIX Association
2.2 Object Detection
Object detection refers to the task of segmenting instances of
semantic objects in an image. The output of object detectors
is generally a set coordinates of bounding boxes in the input
image that contain speciﬁc objects. In the following we de-
tail two object detectors, Yolov3 [35] and Faster-RCNN [36]
which are used throughout this paper.
Yolov3 is a single-shot detector which runs inputs through
a single convolutional neural network (CNN). The CNN uses
a back-bone network to compute feature maps for each cell
in a square grid of the input image. Three grid sizes are used
in Yolov3 to increase accuracy of detecting smaller objects
(13x13, 26x26, 52x52). Yolov3 is used in many real-time
processing systems [6, 8, 41].
Faster-RCNN is the result of a series of improvements on
the initial R-CNN object detector network [16]. Faster-RCNN
uses a two-stage detection method, where an initial network
generates region proposals and a second network predicts la-
bels for proposals. More recently, Mask-RCNN [20] extended
Faster-RCNN in order to add object segmentation to object
detection. Both Yolov3 and Mask-RCNN use non-maximum
suppression in post-processing to remove redundant boxes
with high overlap.
Trafﬁc Sign Recognition. The task of trafﬁc sign recogni-
tion consists in distinguishing between different trafﬁc signs.
Differently from object detection, in trafﬁc sign recognition
the networks typically require a cutout of the sign as input,
rather than the full scene. Several datasets of videos from
car dash cameras are available online, such as LISA [34] or
GTSRB [21], in which a region of interest that identiﬁes the
ground-truth position of the trafﬁc sign in each video frame
is generally manually annotated. In this paper, for continuity,
we consider two different models for trafﬁc sign recognition,
Lisa-CNN and Gtsrb-CNN, both introduced in [15], one of
the earliest works in real-world robust AE.
2.3 Physical Adversarial Examples
Kurakin et al. [25] showed that perturbations computed with
the fast gradient descent [40] method can survive printing
and re-capture with a camera. However, these perturbations
would not be realizable on a real (3D) input, therefore other
works on physical attacks against neural networks have fo-
cused on adversarial patches [10, 23]. Eykholt et al. [15, 38]
showed how to craft robust physical perturbations for stop
signs, that survive changes when reproduced in the physical
world (e.g., distance and viewing angle). The perturbation
is in the form of a poster overlaid on the stop sign itself or
a sticker patch that the authors apply to the sign. Sharif et
al. [37] showed that physical AE for face recognition can be
realized by using colored eye-glass frames, further strength-
ening the realizability of the perturbation in the presence of
input noise (e.g., different user poses, limited color gamut
Figure 2: Example of an adversarial patch attack [18]. The
network has been compromised and reacts to the sunﬂower
being placed in the input by misclassifying the stop sign.
SentiNet [13] leverages the locality of the patch to detect
regions with high saliency, and can therefore detect the attack.
The ﬁgure is taken from Figure 5 in [13].
of printers). Although most of these attacks are focused on
evasion attacks, localized perturbations have also been used in
poisoning attacks [18,28] both by altering the training process
or the network parameters post-training.
More recent works have focused on AE for object detec-
tion [12, 24, 38, 43]. These works use either printed posters or
patches to apply on top of the trafﬁc signs as an attack vec-
tor. As discussed in the previous section, patches suffer from
several disadvantages that can be overcome with a projector,
in particular projections are short-lived and dynamic. This
allows adversaries to turn the projection on/off as they please,
which can be used to target speciﬁc vehicles and allows them
to leave no traces of the malicious attack.
Physical AE Detection. Differently from a digital scenario,
where input changes are simply limited by Lp-norms, the
realization of physically robust AE is more constrained. Ad-
versarial patches are one technique for phsyical AE, however,
they have drawbacks which enable their detection. In fact,
Chou et al. [13] exploited the locality of adversarial patches
to create an AE-detection method named SentiNet, which de-
tects physical AE leveraging the fact that adversarial patches
generate localized areas of high saliency in input, as shown in
Figure 2. These highly salient areas successfully capture the
adversarial patch in input, and therefore can be used for the
detection of an AE by using the fact that such salient areas
will cause misclassiﬁcations when overlaid onto other benign
images. For example, Figure 2 shows that an adversarial patch
shaped as a ﬂower will cause the stop sign to be misclassi-
ﬁed as a warning sign. The same ﬂower patch can be applied
to different images and will also cause misclassiﬁcations in
other classes, which is an unusual behavior which can be de-
tected. SentiNet can capture this behavior just by looking at
the saliency masks of benign images, and ﬁtting a curve to the
accepted behavior range, rather than ﬁtting a binary model for
the detection. This way SentiNet can adapt for unseen attacks
USENIX Association
30th USENIX Security Symposium    1867
Figure 3: Attack scenario. An adversary points a projector at
a stop sign and controls the projection in order to cause the
sign to be undetected by an approaching vehicle.
and therefore claims to generalize to different attack methods.
In this paper, we show how AE generated with SLAP can
bypass such detection.
3 Threat Model
We focus on an autonomous driving scenario, where cameras
are placed in vehicles and the vehicle makes decisions based
on the cameras’ inputs. The vehicle uses camera(s) to detect
and track the objects in the scene, including trafﬁc signs.
Goal. The adversaries’ goal is to cause a stop sign to be un-
detected by the neural networks processing the camera feeds
within the car, which will cause vehicles approaching the stop
sign to ignore them, potentially leading to accidents and dan-
gerous situations. The adversary may want to target speciﬁc
vehicles, therefore using adversarial patches to stick on the
stop sign is not a suitable attack vector. Patches would lead to
the stop sign always being undetected by each passing vehicle
and would cause suspicion among the occupants realizing
that cars did not stop because of an altered sign.
Capabilities and Knowledge. The adversary has access to
the general proximity of the target stop sign and can control a
projector so that it points to the sign, see Figure 3. We note
that the adversary does not necessarily need to have direct
physical access to the sign itself – rather to a position from
which a visual line of sight exists. In the paper we analyze
both adversaries with white-box knowledge and a black-box
scenario based on the transferability of adversarial examples.
4 Method
In this section, we explain our method to carry out the attack.
4.1 Modelling projectable perturbations
Often, to realize physical AE, researchers use the non-
printability-score introduced by Sharif. [37], which models
the set of colors a printer is able to print. In our case, when
shining light with the projector, the resulting output color as
captured by a camera depends on a multitude of factors rather
than just the printer (as in NPS). These factors include: (i) pro-
jector strength, (ii) projector distance, (iii) ambient light, (iv)
camera exposure, (v) color and material properties (diffusion,
reﬂections) of the surface the projection is being shone on
(hereafter, projection surface). The achievable color spectrum
is signiﬁcantly smaller than the spectrum available to printed
stickers as a result of these factors (e.g., a patch can be black
or white, while most projections on a stop sign will result
in red-ish images). In order to understand the feasibility of
certain input perturbations, we model these phenomena as
follows.
Formalizing the problem. We wish to create a model which,
given a certain projection and a projection surface, predicts
the resulting colors in output (as captured by a camera). We
describe this model P as follows:
P (θ1,S,P) = O,
(1)
where S is the projection surface, P is the projected image,
O is the image formed by projecting P on S and θ1 are the
model parameters, respectively.
Finding a perfect model would require taking all of the
factors listed above into account, some of which may not be
available to an adversary and is also likely to be time consum-
ing due to the volume of possible combinations. Therefore,
we opt for a sampling approach, in which we iteratively shine
a set of colors on the target surface (the target object) and col-
lect the outputs captured by the camera. We then ﬁt a model
to the collected data, which approximates the resulting output
color for given projected images and projection surfaces.
Collecting projectable colors. We deﬁne projectable colors
for a given pixel in S as the set of color which are achievable
in output for that pixel given all possible projection images.
To collect the projectable colors, we do as follows:
1. collect an image of the projection surface (S in Eq. 1).
This is an image of the target object.
2. select a color cp = [r,g,b], shine an image of that color