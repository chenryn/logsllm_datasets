Page Identity 
Array
driver1
driver2
driverN
3
5
7
6
4
Figure 3. Kruiser Architecture. The numbers
in the small circle indicate Kruiser’s work
ﬂow.
objects of the same type. When a slab is created to serve
a buffer request, additional objects are created in the slab’s
memory pages to serve further buffer requests.
6.2 Architecture
The architecture, as shown in Figure 3, can be divided
into three parts: VMM, Dom0 VM, and DomU VM. The
Monitor Process in Dom0 VM executing Monitor (Figure 2)
in an inﬁnite loop to monitor the kernel of DomU VM. A
tiny component, namely Memory Mapper, inside the VMM
is used to map the kernel memory of the monitored VM to
the monitor process, which is detailed in Section 6.3. The
custom driver in Dom0 VM is used to assist the monitor
process to release extra memory during the memory map-
ping. The Page Identity Array and the interposition code
inside AddPage and RemovePage (Figure 2) reside in the
kernel space of DomU VM, whose protection is presented
in Section 6.4.
The out-of-VM monitoring ensures performance isola-
tion and secureness, but usually leads to high overhead. The
in-VM information collection provides native code execu-
tion and memory access environments, but may be vulnera-
ble to attacks. By addressing the problems, we combine the
two schemes as a hybrid solution to provide a secure and
efﬁcient monitoring.
6.3 Direct Memory Mapping
To achieve an out-of-the-box monitoring, a conventional
method is to run a monitor process in a trusted VM and per-
form virtual machine introspection (VMI) via the underly-
ing VMM. However, frequent memory introspection would
incur high performance overhead. Each such operation re-
quires VMM to walk the monitored VM’s page table and
map the target machine frames to be accessible from the
Monitor Process
Target OS Kernel
Page
directory
Page
directory
Page table
Page table
Machine Physical Memory
Figure 4. The direct memory mapping mech-
anism.
monitor process. To avoid this problem, we introduce Di-
rect Memory Mapping (DMM), by which the monitor pro-
cess can perform frequent memory introspection with only
one-time involvement of the VMM. The basic idea is that
the VMM manipulates the page table of the monitor process
such that the monitor process can access the kernel memory
of the monitored OS directly, as illustrated in Figure 4. Note
that the custom driver is implemented as a loadable kernel
module, such that the Dom0’s kernel code is not modiﬁed.
The procedure of DMM can be divided into three stages.
First, the Monitor Process allocates a chunk of memory
whose size is determined by the maximum number of mem-
ory pages used for DomU VM’s kernel heap ( 0(cid:13) in Fig-
ure 3). As Linux kernel heap only resides in physically
contiguous memory areas, its maximum size is less than
896MB in 32-bit kernels even if the physical memory size
is larger than 896MB. The goal of this stage is to create a
contiguous range of virtual addresses. By properly manip-
ulating the page table entries (PTEs), the VMM enables the
monitor process to access the memory of the target OS ker-
nel within the monitor’s virtual address space. However,
due to the demand paging mechanism, actually the memory
for PTEs are not allocated when the virtual addresses are
created. Therefore, we need to access the created memory
chunk to trigger the creation of PTEs before operating on
them.
Second, the Monitor Process notiﬁes the Custom Driver
to reclaim the newly allocated pages ( 1(cid:13)) with the PTEs re-
tained. This is necessary because the Monitor Process only
needs the new virtual addresses and the corresponding PTEs
but does not use the allocated pages; returning these pages
back can save a lot of memory. Speciﬁcally, this stage con-
sists of four steps. 1) The Custom Driver ﬁrst walks the
page table of the Monitor Process to identify the PTEs for
the memory chunk allocated in the ﬁrst stage ( 2(cid:13)). 2) Then,
with these identiﬁed PTEs, the Custom Driver searches for
the corresponding page descriptors used by the page frame
management. 3) After that, the Custom Driver clears the
relevant ﬂags in these page descriptors (e.g., active ﬂag),
and resets their reference counters, map counters as well as
other related information. 4) Finally, the Custom Driver in-
free page()) to
vokes the API of the buddy system (i.e.,
release the page frames.
Third, after the Custom Driver ﬁnishes reclaiming pages,
it informs the Memory Mapper to perform DMM for the
Monitor Process ( 3(cid:13)). By looking up the DomU’s physical-
to-machine (P2M) table ( 4(cid:13)), the Memory collects all the
MFNs of the DomU. With the mapping information, the
Memory Mapper updates the PTEs of the Monitor Process
accordingly. Speciﬁcally, given the newly allocated virtual
address range, the Memory Mapper walks the User Page Ta-
ble to ﬁnd the corresponding PTEs ( 5(cid:13)), whose page frame
numbers are then changed to the MFNs that are collected
from the P2M table. In this way, the Monitor Process can
access the entire kernel of the target OS with its own page
table.
Once the Page Identity Array is allocated and initialized
in DomU VM, it invokes a hypercall to notify the underly-
ing VMM ( 6(cid:13)), which then informs the monitor process to
begin cruising over the kernel heap ( 7(cid:13))( 8(cid:13)).
Reducing TLB Pressure. As the memory area that the
Monitor Process accesses may be large when a lot of ker-
nel slabs are produced, the kernel cruising may incur high
TLB pressure. To address this problem, we exploit the ex-
tended paging mechanism that is supported by commodity
microprocessors. Speciﬁcally, we set the Page Size ﬂag in
the page directory entries, enabling the size of page frames
to be 2MB instead of 4KB (the page frame will be 4MB
in size if it is in None-PAE mode). Note that to this end
we also need the hypervisor to support the extended paging.
Fortunately, Xen (with PAE enabled) mainly uses 2MB su-
per pages to allocate memory for guest VMs. On the other
hand, to ensure the extended paging to work properly, we
require the starting virtual address allocated for the monitor
process should be 2MB-aligned. To meet this requirement,
the Monitor Process needs to allocate 2MB extra memory
during the ﬁrst stage, and then adjust the starting virtual ad-
dress to be 2MB-aligned before performing DMM.
6.4 In-VM Protection
Since the PIA data structure (metadata) and the inter-
position code reside in the kernel space of DomU VM, at-
tackers may manipulate them directly after exploiting buffer
overﬂow vulnerabilities. To solve this problem, a conven-
tional method is to move the data structure and code to be
protected into the hypervisor or another trusted VM. How-
ever, it will incur signiﬁcant performance overhead when
the world switches between the hypervisor and the VM be-
come frequent, especially for such ﬁne-grained monitoring
as in our case.
Instead, we employ the SIM [50] frame-
work, which enables a secure and efﬁcient in-VM monitor-
Figure 5. Memory protections in the kernel
and monitor address space.
ing. Speciﬁcally, the hypervisor creates a separate protected
address space inside DomU VM and puts the code and data
to be protected in it, such that those memory regions are
protected from the DomU VM kernel by the hypervisor, and
the separate address space can only be entered and exited
through specially constructed protected gates.
In our case, we need to move the interposition code
added in the critical section of AddPage and Re-
movePage as well as the PIA data structure in Figure 2
to the protected memory regions. To this end, we construct
two shadow page tables (SPTs) specifying different access
permissions for the kernel and the In-VM monitor part.3 As
shown in Figure 5, within a kernel address space, a process
is not allowed to access the monitor code and data regions,
while the kernel code cannot be executed after a process
switches to the monitor address space. To invoke the moni-
tor’s code in the kernel address space, the transition code is
used to switch address spaces and is executable in both ad-
dress spaces. The transition code modiﬁes the CR3 register,
which contains the physical address of the root of the tar-
get shadow page table. By default, any change of CR3 will
result in a VMExit. Fortunately, a recent hardware feature
allows us to change the CR3 without being trapped to the
hypervisor if its value is in the CR3 TARGET LIST, which
is maintained by the hypervisor.
Address Space Maintaining and Switching in SMP.
Maintaining and switching shadow page tables in Symmet-
ric Multi-Processing (SMP) involves two challenges: 1)
The SPTs for the kernel address space and the monitor ad-
3Note that the In-VM monitor part only includes the PIA and the in-
terposition code and will be referred to as the monitor in this section for
short, while the monitor process still runs out of the VM.
Figure 6. Address space switching via transi-
tion pages in SMP.
dress space should get synchronized for correctness. 2)
The transition code should determine the correct CR3 tar-
get when switching back to the kernel address space.
To address the ﬁrst challenge, our approach explores the
observation that the monitor only needs to access the kernel
heap (for placing canaries) and the kernel stack (for access-
ing the arguments and storing local variables), which only
reside in non-paged contiguous memory areas. Hence, by
looking up the P2M table, we can build the memory map-
ping in the SPT used by the monitor with one-time effort,
and then no synchronization is needed.
As to the second challenge, although one common transi-
tion page for the entry code is sufﬁcient, one transition page
for the exit code is needed for per processor, considering
that different processor may have entered the monitor ad-
dress space from different process address spaces. In each
transition page, the CR3 address to be assigned has to be
equal to the address of the shadow page directory that was
used by the current processor prior to entering the monitor
address space, as shown in Figure 6. To this end, we mod-
ify hypervisor to update the CR3 target used in the associ-
ated exit code when a processor performs process switches.
The hypervisor should also update the CR3 TARGET LIST
accordingly. To facilitate the monitor to select the corre-
sponding transition page when switching back, we generate
these pages according to the different CPU ID, which can
be easily determined by the monitor (i.e., using the function
smp processor id()).
Security Check. By invoking duplicate AddPage for the
corrupted page, attackers can recover the canaries. To avoid
this problem, we add one more check in the protected code
to prevent pages with odd PIA version numbers from being
added again. On the other hand, if attackers invoke Re-
movePage maliciously, the ﬁnal round of canary checking
in the function can detect overﬂows.
Additionally, we need to consider an attack scenario
where the exploit installs something (e.g., rootkit) on the
system and then reboots the kernel so that it would by-
Padding for 
word alignment
Object
Object
descriptor
The first object
last word of this color.
Specific
cache
General
cache
Object
Padding for cache 
line alignment
Slab
Padding for cache 
descriptor
line alignment
Object
Object
Object
Object
Color
The first object
(a) Attach one canary 
to each object
is canary
(b) Put one additional canary 
before the first object
Figure 7. Placing canaries into kernel objects.
pass our detection mechanism. To address this problem, we
could utilize the hypervisor to mediate the reboot. Before
the kernel is rebooted, we can pause the system for a while
such that the monitor process will discover the corrupted
canaries after scanning the entire kernel heap.
6.5 Placing Canaries
To detect underﬂows as well as overﬂows, it is straight-
forward to place two canaries surrounding each buffer. Ac-
tually, Linux with slab debug-enabled version has adopted
this scheme to place canaries. Unfortunately, this method
does not make kernel objects aligned in the ﬁrst-level hard-
ware cache, which may result in more cache misses. To
overcome this limitation, we only use one canary instead of
two canaries to surveil each kernel object. Since the same
type of kernel objects are grouped together inside a slab, our
approach can still detect the heap underﬂow attack occurred
in one object (but not the ﬁrst one in a slab) by checking the
canary attached by the previous object.
As shown in Figure 7(a), we apply two different ways to
place the canary. For the speciﬁc caches, we ﬁrst pad the
objects to be word-aligned in size. Then, we add one word
canary following the object. Finally, to ensure the object
get L1 cache line aligned, we put some additional padding
at the end of this object. On the other hand, as the objects
in general caches have already got L1 cache line aligned in
size, there is no need to change the form of these objects.
Instead, we place a canary in the last word of each object.
In addition, we hook the general object allocation function
(i.e., kmalloc), and increase the original requested size by
one word to hold the canary.
Although the scheme above works well to detect under-
ﬂows (and overﬂows), it cannot deal with underﬂows oc-
curred in the ﬁrst object, as there is no canary preceding it.
To tackle this issue, as shown in Figure 7(b), we exploit the
existing infrastructure to add a canary before the ﬁrst object.
Speciﬁcally, if the slab descriptor is located rightly before
the ﬁrst object, the canary is placed at the end of this slab
descriptor; or if there is a slab color,4 we put a canary in the
Secure Canary Generation. To set canary values for ker-
nel objects, a practical solution should meet the two require-
ments: R1) after attackers have compromised the monitored
kernel via buffer overﬂows, they cannot recover the cor-
rupted canaries; R2) The canary generation and veriﬁcation
algorithms should be efﬁcient so that they will not affect the
system performance and detection latency. To satisfy these
requirements, we employ a stream cipher (RC4 [63]) to gen-
erate canary values. For each slab, we ﬁrst extract a random
number from the entropy pool in Linux. Then, this random
number is used as the key “stretched” by RC4 into a stream
of bytes, the length of which is decided by the number of
objects inside the slab. Finally, each 4 bytes of this stream
is selected as a canary value for each object. On the other
hand, regarding canary checking, we store the key (i.e., the
random number) into the corresponding PIA entry for each
slab.
Guaranteed Detection. With the In-VM protection and se-
cure canary generation, attackers can not hide their attacks
in that 1) The In-VM protection prevent attackers from ma-
nipulating the PIA entries; 2) The canary generation based
on the stream cipher guarantees the difﬁculty for attackers
to recover the corrupted canaries within one cruising cy-
cle. In addition, attackers cannot change the memory map-
ping between the monitor process and the monitored kernel
in that the associated page table is maintained by another
trusted VM (i.e., Dom0). Therefore, the attacks are bound
to be detected within one cruising cycle after compromis-
ing the system, unless the attackers know the exact canary
value to be corrupted beforehand, which usually implies the
overread and overrun vulnerabilities overlap for exactly the
same buffer area and which is very rare.
6.6 Locating Canaries
To locate and verify canaries in the Monitor Process,
we hook the slab allocations and deallocations to store the
metadata into the PIA entries, one of which is shown in Fig-
ure 8. The mem ﬁeld record the starting address of the ﬁrst
object within the slab. As each PIA entry corresponds to
one physical page, we only need to remember the last 12 bit
of the address, which equals the offset within one page. For
the obj size ﬁeld, we store the actual object size, including
the size of padding for word alignment.
By adding the start address of one object and its actual
object size, we can get the canary address. To acquire the
start address of the next object, the PIA entry contains the
buffer size ﬁeld, which refers to the whole object size af-
ter adding the canary as well as the padding for cache line
alignment. The num ﬁeld indicates the number of objects
4A slab color is a padding put in the beginning of each slab to optimize
the hardware cache performance.
1 struct PIA entry{
2
3
4