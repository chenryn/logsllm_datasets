title:Ironstack: Performance, Stability and Security for Power Grid Data
Networks
author:Zhiyuan Teo and
Vera Kutsenko and
Ken Birman and
Robbert van Renesse
IronStack: performance, stability and security for power grid data
networks
Zhiyuan Teo, Vera Kutsenko, Ken Birman, Robbert van Renesse
Department of Computer Science
Email: {zteo, vsk23, ken, rvr} @cs.cornell.edu
Cornell University
1 Abstract
Operators of the nationwide power grid use propri-
etary data networks to monitor and manage their power
distribution systems. These purpose-built, wide area
communication networks connect a complex array of
equipment ranging from PMUs and synchrophasers to
SCADA systems. Collectively, these equipment form
part of an intricate feedback system that ensures the
stability of the power grid.
In support of this mis-
sion, the operational requirements of these networks
mandates high performance, reliability, and security.
We designed IronStack, a system to address these con-
cerns. By using cutting-edge software deﬁned network-
ing technology, IronStack is able to use multiple net-
work paths to improve communications bandwidth and
latency, provide seamless failure recovery, and ensure
signals security. Additionally, IronStack is incremen-
tally deployable and backward-compatible with existing
switching infrastructure.
2
Introduction
Power grid operators have the unique challenge of op-
erating wide area data networks that both drive and
depend on power systems. While various systems have
been proposed to handle data buﬀering and process-
ing [1], prior to our eﬀort very little attention has been
paid to the operational characteristics of the underlying
networks used for data transport. The vulnerability of
these data networks to attacks and disruptions – man-
made or otherwise – represents a valid concern that
needs to be thoroughly addressed.
Present power grid data networks are predominantly
run using microwave relays and signal multiplexing on
power cables. Although these have proven acceptable
over time, the growth of big data in this coming age
of smart grid systems means that existing capacity on
these data links could be rapidly saturated in the near
future. A new and better network technology is needed.
The technology of choice that runs data networks in
virtually every other industry is Ethernet.
Indeed,
Ethernet has enjoyed ubiquity in datacenter and enter-
prise networking applications for its low cost and ease
of use, requiring very little conﬁguration for ordinary
operation. Hosts need only plug themselves into Ether-
net switches; the switches are then self-organizing and
will automatically calculate routes from any source to
any destination.
Unfortunately, for the simplicity that Ethernet oﬀers,
it also suﬀers from several severe restrictions, including
one that mandates a loop-free topology for correct op-
eration. The ramiﬁcations of this simple restriction are
that Ethernet networks do not typically feature link
redundancy, and where redundancy exists, they can-
not be taken advantage of without resorting to delicate
and complicated conﬁguration. This is at odds with
the plug-and-play vision of Ethernet, where it would
intuitively have been expected that additional links in-
troduced between network switching elements should
have the eﬀect of automatically and transparently in-
creasing redundancy and performance. In reality, with-
out arduous manual conﬁguration, redundant links are
typically left unused until primary failures force them
into action. Worse, failure recovery and redundant link
activation typically take between several seconds to half
a minute. In the context of power grid data networks
that convey critical fault data from sensors, this fault
recovery time may result in unacceptable knowledge
gaps that can severely impede decision making.
Another consequence of the Ethernet design is that
it becomes possible for attackers to identify strategic
pathways or physical locations where data ﬂow is likely
to transit. By inﬁltrating these locations, attackers can
gain access to raw data streams, reading or modifying
them at will. It is also conceivable that attackers can
masquerade as legitimate sensors and feed malformed
data to processing units, with the eﬀect that power
management systems may be tricked into taking desta-
bilizing actions. Alternatively, attackers can launch
physical or cyber attacks on strategic bottlenecks to
cripple the network, severing critical data ﬂow.
Our motivation with the IronStack system is to tease
apart these Ethernet problems and craft carefully engi-
neered solutions with software-deﬁned networking tech-
niques. The core contributions of our work include:
(1) improvements to end-to-end network performance
1
through packet processing and redundant routing, (2)
realization of high-assurance networking through zero-
downtime failure recovery and (3) techniques for se-
curity through blacklist avoidance, signals obfusca-
tion and transparent encryption, while (4) being fully
backward-compatible with existing network infrastruc-
ture and equipment.
3 Background
3.1 Deﬁciencies in the power grid
An increasingly large portion of the national power
grid is dependent on data networks for command, com-
munications and control (C3). These data networks
frequently carry critical information pertaining to the
health of the grid, often through sensor readings, that
are then used to make decisions for the next stable
operating state of the grid. However, a chicken-and-egg
cyclic dependency exists between the two: a data net-
work cannot survive without power; conversely, without
data, the grid cannot operate in a safe and stable man-
ner.
Apart from physical infrastructure attacks, one of the
weakest links in this delicate balancing act is the data
network and the software that depends on it. There
is emerging consensus that the power grid has nu-
merous vulnerabilities and is susceptible to large scale
remote cyberattacks that can result in real, crippling
infrastructural damages. As an example, Stuxnet is a
well-known malware that quickly spread through data
networks and was directly responsible for the destruc-
tion of about 1000 nuclear enrichment centrifuges in
Iran.
It is conceivable that a similar attack could be
launched against power grid hardware in the US, with
devastating physical and economic eﬀects.
Another problem in the concurrent use of data net-
works to support grid operations is the inherent risk
of critical data ﬂow disruptions during network equip-
ment outages. Such failures can occur for many rea-
sons, including wear-and-tear, accidents and uncorre-
lated power losses. Without access to current data, grid
operators are at risk for a cascading chain of failures.
3.2 Convergence of big energy and big
data
With the emergence of the next generation smart grid,
the amount of data that is expected to ﬂow and be pro-
cessed at control stations will sharply increase. Cisco’s
surveys [9] have shown that nearly one in four IT man-
agers expect network load to triple over the next two
years; the power grid is no exception. In fact, the vi-
sion of a smart grid learning, adapting, and controlling
the power grid will require big increases in real time
data transmission and network load. However, current
power grid communications infrastructure uses anti-
quated technology that will need to be overhauled in
order to support such an increase.
Part of the need to support a higher network load comes
from the emerging use of synchrophasers. Since 2004,
the usage of synchrophasers in the power grid has been
increasing. In the grid, synchrophasers use phasor mea-
surement units also known as PMUs to measure real
time current, voltage and frequency at distributed lo-
cations across the grid. Each of the phasor measure-
ment units timestamps the data that it receives before
sending them oﬀ to a local SCADA system. Time-
stamping these measurements allows administrators to
have a global view and understanding of the activities
on in the grid. Each such device generates 10kb/s or
more data, with stringent latency requirements on the
links that forward these data to the control centers. As
the number of synchrophaser units in the power grid
increases, so will real time data and the need for strong
and consistent reliability in the network which is diﬃ-
cult to support in current infrastructure.
3.3 Software-deﬁned networking
Software-deﬁned networking (SDN) is a modern ab-
straction that allows access to a network switch’s rout-
ing fabric. In SDN models, the switch’s control plane
is made accessible to a special external software entity
(known as a controller), to whom all data switching
decisions are delegated. This control plane has com-
plete command of the data forwarding plane, the latter
of which is where units of network data (known as
packets) are transferred between physical ports on the
switch itself. There is also some limited capability to
transfer packets between the data forwarding plane and
the control plane, a useful feature that we exploit in
our system to implement some key functionality.
The most widely deployed SDN standard today is
known as OpenFlow. OpenFlow is managed by the
Open Networking Foundation and has seen signiﬁcant
evolution through multiple versions. The most recent
version of OpenFlow is 1.3, although many switches
that are marketed as OpenFlow-capable today support
only OpenFlow 1.0. Part of the diﬃculty lies in the
fact that the successive versions of the standard have
increased complexity and are not backward-compatible,
necessitating support for multiple ﬁrmware versions.
Our system takes into account the industry momen-
tum at present, and strives to operate on the greatest
number of devices by using only features that currently
enjoy widespread support.
2
On the software end, there are multiple eﬀorts to de-
velop operational OpenFlow controllers, each with vary-
ing degrees of programmability, complexity and running
speed. Some of the more popular and open-source con-
trollers include POX [10] (a generic Python-based sys-
tem), Floodlight [11] (Java-based) and ovs-controller
[12] (a C-based reference controller written by Open
vSwitch). For the kinds of network-level services we
aimed to provide, we could not ﬁnd a controller that
allowed us ﬁne-grained access to functionality necessary
to implement the features presented in this section. For
this reason, we wrote our own controller, IronStack, en-
tirely from ground-up in C++ with no support from
third-party software libraries.
4 Design
In our eﬀort to modernize power grid data networks
with IronStack, we identiﬁed three primary objectives:
high performance, high assurance and high security.
Also important, but not critical, are the pragmatic
economic considerations that our solution should be in-
crementally deployable and preferably fully backward-
compatible with existing hardware and software.
4.1 Performance and assurance
IronStack borrows some ideas from RAID [2], a set of
redundancy schemes commonly used to protect data
by utilizing multiple hard drives. Analogously, the re-
dundancy in data networks are provided by multiple
disjoint paths from a source to a destination. However,
current networks do not usually feature multiple dis-
joint paths because they are tedious to design, require a
fair degree of manual conﬁguration, and are diﬃcult to
maintain in a safe conﬁguration over extended periods
of time [4]. Also, software that takes advantage of mul-
tiple paths is rare in practice [3]. IronStack solves these
problems by automatically generating a safe conﬁgura-
tion for any given network topology, while allowing mul-
tiple paths to be used simultaneously without the need
for any laborious conﬁguration or forethought.
Iron-
Stack is self-conﬁguring, self-adapting and self-healing,
so repeated changes to the network topology do not
aﬀect the operation of dependent network software.
Thus, IronStack automatically manages the network
eﬃciently in a way that is transparent to users.
end delivers the ﬁrst arriving packet to the application
and discards the duplicates. Such a scheme minimizes
latency and improves the stability of the ﬂow, while
also tolerating up to n − 1 link or switch failures, at a
cost of n times the bandwidth.
On the other extreme end of the spectrum, each disjoint
path can be seen as a separate channel through which
ﬂows can be sent through, so each successive packet in
a ﬂow can be sent down whichever path is ﬁrst avail-
able (thus avoiding the problem of sending too many
packets down congested paths). In a lightly loaded net-
work, approximately 1/n of the packets in a ﬂow can
be sent down each path. This scheme maximizes band-
width eﬃciency but clearly sacriﬁces on ﬂow stability
and latency, since the entire ﬂow is now dependent on
the slowest link. It also does not tolerate link failures
although such tolerance may not be necessary if the
software protocol can handle it (eg. TCP with selective
acknowledgements).
In between these two ends, a k out of n scheme may
be used to reap some beneﬁts from both the abovemen-
tioned ideas. In this hybridized scheme, individual data
bits at the source are striped across n multiple packets,
each of which then travels down a diﬀerent path to-
wards its destination. At the receiving end, only k out
of these n striped packets are required for complete as-
sembly of the original data. Hence, this scheme has a
latency equivalent to the n − k + 1th slowest link at
any instant, while having a bandwidth eﬃciency ratio
1(cid:100)n/k(cid:101) . This is essentially a form of forward error
of
correction.
4.2 Security
IronStack adopts a defensive perspective on threat mod-
eling. In the IronStack threat model, we assume that an
adversary operates on the network and is interested in
gaining access to the raw data in a network ﬂow. Where
such raw data is protected by encryption, we assume
that the adversary is interested in signals intelligence.
The adversary can perform a variety of attacks, perhaps
by acting as a man-in-the-middle, snooping on sensitive
data, modifying data in transit or by passively identi-
fying patterns in communications. We identify three
possible security measures for IronStack users. These
security mechanisms are orthogonal to the performance
and assurance components of IronStack, and can in fact
be used simultaneously.
The way IronStack uses multiple paths in the net-
work can be seen as a continuum of tradeoﬀs between
latency/reliability and bandwidth eﬃciency. At one ex-
treme end of the spectrum, each packet in a ﬂow can be
replicated onto multiple disjoint paths. The receiving
4.3 Localized adversary
When the operating location of the adversary is known,
IronStack can blacklist the aﬀected parts of the network
and instead construct paths that do not take protected
3
ﬂows through compromised network elements. Since
data never transits the adversary’s location, it is not
possible for the adversary to perform any kind of mean-
ingful attack. Consequently, it is also impossible for the
attacker to deduce any signals intelligence from the pro-
tected ﬂow.
4.4 Non-localized adversary
If the adversary’s location is not known, or if the pres-
ence of an adversary is uncertain, blacklisting will not
help. However, IronStack can still reduce the problem
by randomly distributing data over multiple disjoint
paths. This has the eﬀect of obfuscating the signal pro-
ﬁle of a ﬂow, making it diﬃcult for an adversary to
deduce patterns and ascertain the nature of the ﬂow.
Furthermore, any information that the attacker gains
is only partial, since the distribution of the data over
multiple paths ensures that no single path contains all
packets to a ﬂow.