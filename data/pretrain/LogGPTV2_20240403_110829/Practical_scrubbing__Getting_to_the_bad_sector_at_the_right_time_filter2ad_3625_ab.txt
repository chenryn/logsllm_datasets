2
K
4
K
8
K
6
1
K
2
3
K
4
6
K
8
2
1
K
6
5
2
K
2
1
5
M
1
M
2
M
4
M
8
M
6
1
SCSI Verify size (bytes)
Hitachi UltraStar Sequential
Hitachi UltraStar Staggered
Fujitsu MX Sequential
Fujitsu MX Staggered
)
c
e
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
i
g
n
b
b
u
r
c
S
120
100
80
60
40
20
0
)
c
e
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
i
g
n
b
b
u
r
c
S
16
14
12
10
8
6
4
2
0
64K
128K 256K 512K
1M
2M
4M
8M
16M
1
2
4
Request size (bytes)
Hitachi UltraStar Sequential
Hitachi UltraStar Staggered
Fujitsu MX Sequential
Fujitsu MX Staggered
16
8
64
Number of regions
32
128 256 512
Figure 4. Service times for different SCSI VERIFY
sizes
(a) Request size (128 regions)
(b) Number of regions (64KB requests)
Figure 5.
Impact of scrubbing parameters on sequential and staggered scrubbing performance.
requests (the presented trend holds regardless of request
size), dividing the disks in up to 512 regions. The solid lines
in Fig. 5b represent the staggered scrubber’s throughput for
the two drives as a function of the number of regions. We
observe that the throughput of the scrubber continuously
increases as the number of regions increases from two to
512 (in the case of one region, the staggered scrubber’s
actions are identical to a sequential scrubber). To answer
our original question, which is how the performance of a
staggered scrubber compares to that of a sequential scrubber,
the dashed lines in Fig. 5b represent the throughput achieved
by a sequential scrubber (using 64KB requests). Interestingly,
for more than 128 regions we ﬁnd that the staggered scrubber
performs equally well or better than the sequential one. We
have observed this trend for drives of varying capacities,
which leads us to assume that it is independent of the disk’s
capacity. To verify that this trend holds for larger request
sizes, we have also plotted the throughput of a staggered
scrubber as a function of the request size (while ﬁxing the
number of regions to 128) in Fig. 5a. Since work on the
impact of staggered scrubbing on the MLET shows that the
number of regions has a relatively small impact on MLET
[4], we recommend using small region sizes, compared to the
disk’s capacity. To obtain conservative results, though, we
ﬁx the number of regions to 128 in subsequent experiments.
The fact that staggered can outperform sequential scrub-
bing may seem counter-intuitive. However, since VERIFY
avoids transferring data to the controller or on-disk cache,
when a sequential scrubbing request is completed, the next
one will have to be serviced from the medium’s surface. At
the same time, the head has moved further along the track,
while the VERIFY result was propagated to the controller.
Hence, when the next VERIFY is initiated, the head needs
to wait for a full rotation of the platter until it reaches the
correct sector. For the staggered scrubber, this describes only
the worst case. However, when the regions are too large (less
than 64 in total), the overhead of jumping between regions
dominates the overhead caused by rotational latency. We
have validated this hypothesis experimentally by increasing
the delay between subsequent scrubbing requests, by inter-
vals smaller than the rotational latency. As expected, only
the staggered scrubber was harmed by such delays.
B. Scrubbing impact on synthetic workloads
Next, we evaluate the performance impact of scrubbing
on two simple synthetic foreground workloads. The ﬁrst is
a workload with a high degree of sequentiality: it picks a
random sector and reads the following 8MB using 64KB re-
quests. Once the chunk’s last request is serviced, it proceeds
to pick another random sector and start again. The second
is a random workload, which reads random 64KB chunks
of data from the disk. For both workloads we insert an
exponentially distributed think time between requests, with a
mean of 100ms, to allow for idle intervals that the scrubber
can utilize. In all cases, we send requests directly to the disk,
bypassing the OS cache.
We experiment with both the staggered and the sequential
scrubber using 64KB scrub requests, which represent the
best case in terms of collision impact by imposing minimal
slowdown (recall Fig. 4). We schedule scrub requests in two
commonly used ways: in one case we issue scrub requests
back-to-back through CFQ, using the Idle priority to limit
their impact on the foreground workload; in the second case
we use the Default priority and limit the scrubber’s rate by
introducing delays between scrub requests, ranging from 0-
256ms (anything larger scrubs less than 320GB bi-weekly).
The results for the sequential workload are shown in
Fig. 6a. We observe that the highest combined throughput for
the workload and the scrubber is achieved with CFQ (where
the scrubber submits requests back-to-back); however, this
comes at a signiﬁcant cost for the foreground application:
a drop of 20.6% in throughput, compared to the case when
the foreground workload runs in isolation. When inserting
delays between scrub requests, rather than using CFQ to
limit the impact of the scrubber, we see that for large enough
delays (≥ 16ms) the throughput of the foreground workload
is comparable to that without a scrubber. However, in those
cases the throughput of the scrubber is greatly reduced, from
9MB/s under CFQ to less than 3MB/s with a delay of more
than 16ms. As a secondary result, we note that again there
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
16
14
12
10
8
6
4
2
0
9.2 9.0
12.5
12.1
Sequential Workload
Sequential Scrubber (throughput in text)
Staggered Scrubber (throughput in text)
3.0 3.0
4.9 5.0
1.5 1.5
0.9 0.9
0.5 0.5
0.2 0.2
None
CFQ
0ms
8ms
16ms
32ms
64ms
128ms
256ms
)
c
e
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
16
14
12
10
8
6
4
2
0
12.2 12.4
5.3 5.3
Random Workload
Sequential Scrubber (throughput in text)
Staggered Scrubber (throughput in text)
4.9 5.3
2.6 2.6
1.4 1.4
0.9 0.9
0.5 0.5
0.2 0.2
None
CFQ
0ms
8ms
16ms
32ms
64ms
128ms
256ms
Delay between scrubbing requests
Delay between scrubbing requests
(a) Sequential foreground workload results
(b) Random foreground workload results
Figure 6. Throughput comparison of sequential and staggered scrubbing, when run alongside a synthetic workload (64KB segment size, 128 regions).
is no perceivable difference between the staggered and the
sequential scrubber for sufﬁciently small regions (here: 128).
We observe similar results when we scrub against the ran-
dom workload in Fig. 6b: to achieve application throughput
comparable to the case without a scrubber in the system,
large delays are required that cripple the scrubber’s through-
put. Note that random workloads induce additional seeking,
decreasing the scrubber’s throughput.
C. Scrubbing impact on real workloads
To experiment with more realistic workloads, we replayed
a number of real I/O traces from the HP Cello and MSR
Cambridge collections, available publicly from SNIA [18].
We have worked with a set of 77 disk traces in total,
spanning from one week to one year (we make use of only
one week in our experiments) and being used in almost all
possible scenarios: home and project directories, web and
print servers, proxies, backups, etc. Although we ran the
experiments in the rest of the paper for almost all disks, we
have chosen to focus mainly on four disks from each trace
collection when presenting our results. These disks contain
the largest number of requests per week, and represent
diverse workloads. The characteristics of the chosen traces
are summarized in Table I 3.
Trace
MSR
Cambridge
(2008)
HP Cello
(1999)
MS TPC-C
(2009)
Disk
src11
usr1
proj2
prn1
c6t8d0
c6t5d1
c6t5d0
c3t3d0
disk66
disk88
Requests
Description
45,746,222
45,283,980
29,266,482
11,233,411
9,529,855
4,588,778
3,365,078
2,742,326
513,038
513,844
Source Control
Home dirs
Project dirs
Print server
News Disk
Project ﬁles
Home dirs
Root & Swap
TPC-C run
TPC-C run
SNIA BLOCK I/O TRACES USED IN THE PAPER
Table I
3We originally added TPC-C [18] for completeness (database workload),
but later found it to produce an unrealistic distribution of inter-arrivals.
1
1
c
r
s
R
S
M
−
s
t
s
e
u
q
e
r
f
o
n
o
i
t
c
a
r
F
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
No scrubber − 0 req/s
CFQ (Seql) − 211 req/s
CFQ (Stag) − 214 req/s
0ms (Seql) − 214 req/s
0ms (Stag) − 216 req/s
64ms (Seql) − 14 req/s
64ms (Stag) − 14 req/s
100
101
10−4
10−3
10−2
10−1
Response Time (s)
Figure 7. Response Times CDFs during the replay of a real I/O trace.
Fig. 7 shows the impact of our scrubbers on one of
these (more realistic) workloads by plotting the cumulative
distribution function of the response times of application
requests. For better readability we only include results
for four different cases: no scrubber; back-to-back scrub
requests scheduled through CFQ’s Idle priority class; and
scrub requests with delays of 0ms and 64ms between them.
Again, we observe results very similar to those for the
synthetic workloads. Using back-to-back scrub requests,
even when lowering their priority through CFQ, greatly
affects the response times of foreground requests. On the
other hand, when making delays between scrub requests
large enough to limit the effect on the foreground workload
(64ms),
the throughput of the scrubber drops by more
than an order of magnitude (the scrubber’s throughput for
each experiment is included in the legend of Fig. 7). As
before, the results are identical for the staggered and the
sequential scrubber. These results motivate us to look at
more sophisticated ways to schedule scrub requests.
V. KNOWING WHEN TO SCRUB
Our experiments in Section IV showed that simply relying
on CFQ to schedule scrub requests, or issuing them at a ﬁxed
rate is suboptimal both for minimizing impact on foreground
trafﬁc, and for maximizing the scrubber’s throughput. This
motivates us to consider more sophisticated methods for
scheduling scrub requests in this section. Using statistical
analysis of I/O traces, we identify some statistical properties
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
)
s
n
o
i
l
l
i
m
(
s
t
s
e
u
q
e
r
f
o
r
e
b
m
u
N
4.00
2.00
1.00
0.40
0.20
0.10
0.04
0.02
0.01
MSRsrc11
MSRusr1
HPc6t5d1
HPc6t8d0
0
24
48
72
96
Trace hour
120
144
168
Figure 8. Request activity for four disks from both the HP Cello and the
MSR Cambridge traces.
s
r
u
o
h
n
i
d
o
i
r
e
P
36
33