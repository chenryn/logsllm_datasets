title:Fast Regular Expression Matching Using Small TCAMs for Network Intrusion
Detection and Prevention Systems
author:Chad R. Meiners and
Jignesh Patel and
Eric Norige and
Eric Torng and
Alex X. Liu
Fast Regular Expression Matching using Small TCAMs for Network
Intrusion Detection and Prevention Systems
Chad R. Meiners
Jignesh Patel
Eric Norige
Eric Torng
Alex X. Liu
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824-1226, U.S.A.
{meinersc, patelji1, norigeer, torng, alexliu}@cse.msu.edu
Abstract
Regular expression (RE) matching is a core component
of deep packet inspection in modern networking and
security devices.
In this paper, we propose the ﬁrst
hardware-based RE matching approach that uses Ternary
Content Addressable Memories (TCAMs), which are
off-the-shelf chips and have been widely deployed in
modern networking devices for packet classiﬁcation. We
propose three novel techniques to reduce TCAM space
and improve RE matching speed: transition sharing, ta-
ble consolidation, and variable striding. We tested our
techniques on 8 real-world RE sets, and our results show
that small TCAMs can be used to store large DFAs and
achieve potentially high RE matching throughtput. For
space, we were able to store each of the corresponding 8
DFAs with as many as 25,000 states in a 0.59Mb TCAM
chip where the number of TCAM bits required per DFA
state were 12, 12, 12, 13, 14, 26, 28, and 42. Using
a different TCAM encoding scheme that facilitates pro-
cessing multiple characters per transition, we were able
to achieve potential RE matching throughputs of between
10 and 19 Gbps for each of the 8 DFAs using only a sin-
gle 2.36 Mb TCAM chip.
1 Introduction
1.1 Background and Problem Statement
Deep packet inspection is a key part of many networking
devices on the Internet such as Network Intrusion De-
tection (or Prevention) Systems (NIDS/NIPS), ﬁrewalls,
and layer 7 switches.
In the past, deep packet inspec-
tion typically used string matching as a core operator,
namely examining whether a packet’s payload matches
any of a set of predeﬁned strings. Today, deep packet in-
spection typically uses regular expression (RE) matching
as a core operator, namely examining whether a packet’s
payload matches any of a set of predeﬁned regular ex-
pressions, because REs are fundamentally more expres-
sive, efﬁcient, and ﬂexible in specifying attack signatures
[27]. Most open source and commercial deep packet in-
spection engines such as Snort, Bro, TippingPoint X505,
and many Cisco networking appliances use RE match-
ing. Likewise, some operating systems such as Cisco
IOS and Linux have built RE matching into their layer 7
ﬁltering functions. As both trafﬁc rates and signature set
sizes are rapidly growing over time, fast and scalable RE
matching is now a core network security issue.
RE matching algorithms are typically based on the De-
terministic Finite Automata (DFA) representation of reg-
ular expressions. A DFA is a 5-tuple (Q, Σ, δ, q0, A),
where Q is a set of states, Σ is an alphabet, δ : Σ × Q →
Q is the transition function, q0 is the start state, and
A ⊆ Q is a set of accepting states. Any set of regu-
lar expressions can be converted into an equivalent DFA
with the minimum number of states. The fundamental
issue with DFA-based algorithms is the large amount of
memory required to store transition table δ. We have to
store δ(q, a) = p for each state q and character a.
Prior RE matching algorithms are either software-
based [4, 6, 7, 12, 16, 18, 19] or FPGA-based [5, 7, 13, 14,
22, 24, 29]. Software-based solutions have to be imple-
mented in customized ASIC chips to achieve high-speed,
the limitations of which include high deployment cost
and being hard-wired to a speciﬁc solution and thus lim-
ited ability to adapt to new RE matching solutions. Al-
though FPGA-based solutions can be modiﬁed, resynthe-
sizing and updating FPGA circuitry in a deployed system
to handle regular expression updates is slow and difﬁ-
cult; this makes FPGA-based solutions difﬁcult to be de-
ployed in many networking devices (such as NIDS/NIPS
and ﬁrewalls) where the regular expressions need to be
updated frequently [18].
1.2 Our Approach
To address the limitations of prior art on high-speed RE
matching, we propose the ﬁrst Ternary Content Address-
able Memory (TCAM) based RE matching solution. We
1
use a TCAM and its associated SRAM to encode the
transitions of the DFA built from an RE set where one
TCAM entry might encode multiple DFA transitions.
TCAM entries and lookup keys are encoded in ternary
as 0’s, 1’s, and *’s where *’s stand for either 0 or 1.
A lookup key matches a TCAM entry if and only if
the corresponding 0’s and 1’s match; for example, key
0001101111 matches entry 000110****. TCAM circuits
compare a lookup key with all its occupied entries in par-
allel and return the index (or sometimes the content) of
the ﬁrst address for the content that the key matches; this
address is then used to retrieve the corresponding deci-
sion in SRAM.
Given an RE set, we ﬁrst construct an equivalent min-
imum state DFA [15]. Second, we build a two column
TCAM lookup table where each column encodes one of
the two inputs to δ: the source state ID and the input char-
acter. Third, for each TCAM entry, we store the destina-
tion state ID in the same entry of the associated SRAM.
Fig. 1 shows an example DFA, its TCAM lookup table,
and its SRAM decision table. We illustrate how this DFA
processes the input stream “01101111, 01100011”. We
form a TCAM lookup key by appending the current input
character to the current source state ID; in this example,
we append the ﬁrst input character “01101111” to “00”,
the ID of the initial state s0, to form “0001101111”. The
ﬁrst matching entry is the second TCAM entry, so “01”,
the destination state ID stored in the second SRAM en-
try is returned. We form the next TCAM lookup key
“0101100011” by appending the second input character
“011000011” to this returned state ID “01”, and the pro-
cess repeats.
else
else
s0
b
s1
[b,c]
s2
[a,o]
a,[c,o]
s0
s1
s2
a,[d,o]
TCAM
Src ID
Input
SRAM
Dst ID
00
00
00
01
01
01
01
10
10
10
10
0110 0000
0110 ****
**** ****
0110 0000
0110 0010
0110 ****
**** ****
0110 0000
0110 001*
0110 ****
**** ****
00
01
00
00
01
10
00
00
01
10
00
s0
s1
s0
s0
s1
s2
s0
s0
s1
s2
s0
Input stream
Src ID
Input
Figure 1: A DFA with its TCAM table
Advantages of TCAM-based RE Matching There
are three key reasons why TCAM-based RE matching
works well. First, a small TCAM is capable of encoding
a large DFA with carefully designed algorithms lever-
aging the ternary nature and ﬁrst-match semantics of
TCAMs. Our experimental results show that each of the
DFAs built from 8 real-world RE sets with as many as
25,000 states, 4 of which were obtained from the authors
of [6], can be stored in a 0.59Mb TCAM chip. The two
DFAs that correspond to primarily string matching RE
sets require 28 and 42 TCAM bits per DFA state; 5 of
the remaining 6 DFAs which have a sizeable number of
‘.*’ patterns require 12 to 14 TCAM bits per DFA state
whereas the 6th DFA requires 26 TCAM bits per DFA
state. Second, TCAMs facilitate high-speed RE matching
because TCAMs are essentially high-performance paral-
lel lookup systems: any lookup takes constant time (i.e.,
a few CPU cycles) regardless of the number of occupied
entries. Using Agrawal and Sherwood’s TCAM model
[1] and the resulting required TCAM sizes for the 8 RE
sets, we show that it may be possible to achieve through-
puts ranging between 5.36 and 18.6 Gbps using only a
single 2.36 Mb TCAM chip. Third, because TCAMs are
off-the-shelf chips that are widely deployed in modern
networking devices, it should be easy to design network-
ing devices that include our TCAM based RE matching
solution. It may even be possible to immediately deploy
our solution on some existing devices.
Technical Challenges There are two key technical
challenges in TCAM-based RE matching. The ﬁrst is en-
coding a large DFA in a small TCAM. Directly encoding
a DFA in a TCAM using one TCAM entry per transi-
tion will lead to a prohibitive amount of TCAM space.
For example, consider a DFA with 25000 states that con-
sumes one 8 bit character per transition. We would need
a total of 140.38 Mb (= 25000×28×(8+⌈log 25000⌉)).
This is infeasible given the largest available TCAM chip
has a capacity of only 72 Mb. To address this challenge,
we use two techniques that minimize the TCAM space
for storing a DFA: transition sharing and table consol-
idation. The second challenge is improving RE match-
ing speed and thus throughput. One way to improve the
throughput by up to a factor of k is to use k-stride DFAs
that consume k input characters per transition. However,
this leads to an exponential increase in both state and
transition spaces. To avoid this space explosion, we use
the novel idea of variable striding.
Key Idea 1 - Transition Sharing The basic idea is to
combine multiple transitions into one TCAM entry by
exploiting two properties of DFA transitions: (1) char-
acter redundancy where many transitions share the same
source state and destination state and differ only in their
character label, and (2) state redundancy where many
transitions share the same character label and destina-
tion state and differ only in their source state. One rea-
son for the pervasive character and state redundancy in
DFAs constructed from real-world RE sets is that most
states have most of their outgoing transitions going to
some common “failure” state; such transitions are often
called default transitions. The low entropy of these DFAs
2
opens optimization opportunities. We exploit character
redundancy by character bundling (i.e., input character
sharing) and state redundancy by shadow encoding (i.e.,
source state sharing).
In character bundling, we use a
ternary encoding of the input character ﬁeld to repre-
sent multiple characters and thus multiple transitions that
share the same source and destination states. In shadow
encoding, we use a ternary encoding for the source state
ID to represent multiple source states and thus multiple
transitions that share the same label and destination state.
Key Idea 2 - Table Consolidation The basic idea is
to merge multiple transition tables into one transition
table using the observation that some transition tables
share similar structures (e.g., common entries) even if
they have different decisions. This shared structure can
be exploited by consolidating similar transition tables
into one consolidated transition table. When we con-
solidate k TCAM lookup tables into one consolidated
TCAM lookup table, we store k decisions in the asso-
ciated SRAM decision table.
Key Idea 3 - Variable Striding The basic idea is to
store transitions with a variety of strides in the TCAM so
that we increase the average number of characters con-
sumed per transition while ensuring all the transitions ﬁt
within the allocated TCAM space. This idea is based on
two key observations. First, for many states, we can cap-
ture many but not all k-stride transitions using relatively
few TCAM entries whereas capturing all k-stride tran-
sitions requires prohibitively many TCAM entries. Sec-
ond, with TCAMs, we can store transitions with different
strides in the same TCAM lookup table.
The rest of this paper proceeds as follows. We review
related work in Section 2. In Sections 3, 4, and 5, we
describe transition sharing, table consolidation, and vari-
able striding, respectively. We present implementation
issues, experimental results, and conclusions in Sections
6, 7, and 8, respectively.
2 Related Work
In the past, deep packet inspection typically used string
matching (often called pattern matching) as a core op-
erator; string matching solutions have been extensively
studied [2, 3, 28, 30, 32, 33, 35]). TCAM-based solutions
have been proposed for string matching, but they do not
generalize to RE matching because they only deal with
independent strings [3, 30, 35].
Today deep packet inspection often uses RE match-
ing as a core operator because strings are no longer ad-
equate to precisely describe attack signatures [25, 27].
Prior work on RE matching falls into two categories:
software-based and FPGA-based. Prior software-based
RE matching solutions focus on either reducing mem-
ory by minimizing the number of transitions/states or
improving speed by increasing the number of characters
per lookup. Such solutions can be implemented on gen-
eral purpose processors, but customized ASIC chip im-
plementations are needed for high speed performance.
For transition minimization, two basic approaches have
been proposed: alphabet encoding that exploits charac-
ter redundancy [6, 7, 12, 16] and default transitions that
exploit state redundancy [4, 6, 18, 19]. Previous alphabet
encoding approaches cannot fully exploit local charac-
ter redundancy speciﬁc to each state. Most use a sin-
gle alphabet encoding table that can only exploit global
character redundancy that applies to every state. Kong
et al. proposed using 8 alphabet encoding tables by par-
titioning the DFA states into 8 groups with each group
having its own alphabet encoding table [16]. Our work
improves upon previous alphabet encoding techniques
because we can exploit local character redundancy spe-
ciﬁc to each state. Our work improves upon the default
transition work because we do not need to worry about
the number of default transitions that a lookup may go
through because TCAMs allow us to traverse an arbitrar-
ily long default transition path in a single lookup. Some
transition sharing ideas have been used in some TCAM-
based string matching solutions for Aho-Corasick-based
DFAs [3, 11]. However, these ideas do not easily ex-
tend to DFAs generated by general RE sets, and our
techniques produce at least as much transition sharing
when restricted to string matching DFAs. For state min-
imization, two fundamental approaches have been pro-
posed. One approach is to ﬁrst partition REs into multi-
ple groups and build a DFA from each group; at run time,
packet payload needs to be scanned by multiple DFAs
[5, 26, 34]. This approach is orthogonal to our work and
can be used in combination with our techniques. In par-
ticular, because our techniques achieve greater compres-
sion of DFAs than previous software-based techniques,
less partitioning of REs will be required. The other ap-
proach is to use scratch memory to store variables that
track the traversal history and avoid some duplication of
states [8,17,25]. The beneﬁt of state reduction for scratch
memory-based FAs does not come for free. The size of
the required scratch memory may be signiﬁcant, and the
time required to update the scratch memory after each
transition may be signiﬁcant. This approach is orthogo-
nal to our approach. While we have only applyied our
techniques to DFAs in this initial study of TCAM-based
RE matching, our techniques may work very well with
scratch memory-based automata.
Prior FPGA-based solutions exploit the parallel pro-
cessing capabilities of FPGA technology to implement
nondeterministic ﬁnite automata (NFA) [5, 7, 13, 14, 22,