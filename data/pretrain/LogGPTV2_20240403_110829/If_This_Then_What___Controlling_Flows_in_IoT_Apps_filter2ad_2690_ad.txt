communication
voiceassistants
mobiledevices&accessories
health&fitness
bookmarking
notes
timemanagement&tracking
connectedcar
blogging
education
finance&payments
diyelectronics
photo&video
routers&computeraccessories
music
contacts
businesstools
monitoring
surveytools
shopping
taskmanagement&to-dos
monitoringsystems
smarthubs&systems
news&information
journaling&personaldata
appliances
tags&beacons
email
management
lighting
calendars&scheduling
environmentcontrol&
powermonitoring&
security&
100
80
60
40
20
0
100
80
60
40
20
0
CDN via a public URL. However, if public URLs are unavoidable,
we argue for the following countermeasures.
Lifetime of public URLs. Our experiments indicate that IFTTT
stores information on its own CDN servers for extended periods of
time. In scenarios like linking an image location map in an email
prematurely removing the linked resource would corrupt the email
message. However, in scenarios like photo backup on Google Drive,
any lifetime of the image file on IFTTT’s CDN after it has been
consumed by Google Drive is unjustified. Long lifetime is confirmed
by high rates of success with brute forcing URLs. A natural coun-
termeasure is thus, when possible, to shorten the lifetime of public
URLs, similar to other CDN’s like Facebook.
URL shortening. Recall that URLs with 6-digit random strings are
subject to brute force attacks that expose users’ private information.
By increasing the size of random strings, brute force attacks become
harder to exploit. Moreover, a countermeasure of using URLs over
HTTPS rather than HTTP can ensure privacy and integrity with
respect to a network attacker.
6 COUNTERMEASURES: TRACKING THE
FLOW
The access control mechanism from the previous section breaks
insecure flows either by disabling the access to public URLs in
the filter code or by delegating their choice to the users at the
time of applet’s installation. However, the former may hinder the
functionality of secure applets. An applet that manipulates private
information while it also displays a logo via a public image is secure,
as long the public image URL does not depend on the private infor-
mation. Yet, this applet is rejected by the access control mechanism
because of the public URL in the filter code. The latter, on the other
hand, burdens the user by forcing them to type the URL of every
public image they use.
Further, on-going and future developments in the domain of IoT
apps, like multiple actions, triggers, and queries for conditional trig-
gering [28], call for tracking information flow instead. For example,
an applet that accesses the user’s location and iOS photos to share
on Facebook a photo from the current city is secure, as long as it
does not also share the location on Facebook. To provide the desired
functionality, the applet needs access to the location, iOS photos
and Facebook, yet the system should track that such information is
propagated in a secure manner.
To be able track information flow to URLs in a precise way, we
rely on a mechanism for safe output encoding through sanitization,
so that the only way to include links or image markup on the sink
is through the use of API constructors generated by IFTTT. This
requirement is already familiar from Section 5.
This section outlines types of flow that may leak information
(Section 6.1), presents a formal model to track these flows by a
monitor (Section 6.2), and establishes the soundness of the monitor
(Section 6.3).
6.1 Types of flow
There are several types of flow that can be exploited by a malicious
applet maker to infer information about the user private data.
Explicit. In an explicit [8] flow, the private data is directly copied
into a variable to be later used as a parameter part in a URL linking
to an attacker-controlled server, as in Figures 2 and 3.
Implicit. An implicit [8] flow exploits the control flow structure of
the program to infer sensitive information, i.e. branching or looping
on sensitive data and modifying “public” variables.
Example 6.1.
var rideMap = Uber . rideCompleted . TripMapImage
var driver = Uber . rideCompleted . DriverName
for (i = 0; i  '
Email . SendAnEmail . setBody ( rideMap + img )
The filter code above emails the user the map of the Uber ride,
but it sends the driver name to the attacker-controlled server.
Presence. Triggering an applet may itself reveal some information.
For example, a parent using an applet notifying when their kids get
home, such as “Get an email alert when your kids come home and
connect to Almond” [2] may reveal to the applet maker that the
applet has been triggered, and (possibly) kids are home alone.
e
c
::= s | l | e + e | source | f (e) | linkL (e) | linkH (e)
::= skip | stop |
l = e | c; c | if e then c else c |
while e do c | sink(e)
Figure 12: Filter syntax
Example 6.2.
var logo = ' '
Email . sendMeEmail . setBody (" Your kids got home ."
+ logo )
Timing. IFTTT applets are run with a timeout. If the filter code’s ex-
ecution exceeds this internal timeout, then the execution is aborted
and no output actions are performed.
Example 6.3.
var img = ' '
var n = parseInt ( Stripe . newPayment . Amount )
while (n > 0) { n -- }
GoogleSheets . appendToGoogleSpreadsheet .
setFormattedRow ( ' New Stripe payment ' +
Stripe . newPayment . Amount + img )
The code above is based on applet “Automatically log new Stripe
payments to a Google Spreadsheet” [46]. Depending on the value of
the payment made via Stripe, the code may timeout or not, meaning
the output action may be executed or not. This allows the malicious
applet maker to learn information about the paid amount.
6.2 Formal model
Language. To model the essence of filter functionality, we focus
on a simple imperative core of JavaScript extended with APIs for
sources and sinks (Figure 12). The sources source denote trigger-
based APIs for reading user’s information, such as location or fitness
data. The sinks sink denote action-based APIs for sending informa-
tion to services, such as email or social networks.
We assume a typing environment Γ mapping variables and sinks
to security labels ℓ, with ℓ ∈ L, where (L, ⊑) is a lattice of security
labels. For simplicity, we further consider a two-point lattice for
low and high security L = ({L, H}, ⊑), with L ⊑ H and H (cid:64) L. For
privacy, L corresponds to public and H to private.
Expressions e consist of variables l, strings s and concatenation
operations on strings, sources, function calls f , and primitives for
link-based constructs link, split into labeled constructs linkL and
linkH for creating privately and publicly visible links, respectively.
Examples of link constructs are the image constructor img(·) for
creating HTML image markups with a given URL and the URL
constructor url(·) for defining upload links. We will return to the
link constructs in the next subsection.
Commands c include action skipping, assignments, conditionals,
loops, sequential composition, and sinks. A special variable out
stores the value to be sent on a sink.
Skip set S Recall that IFTTT allows for applet actions to be
skipped inside the filter code, and when skipped, no output cor-
responding to that action will take place. We define a skip set
S : A (cid:55)→ Bool mapping filter actions to booleans. For an action
⟨e, m, Γ⟩pc ⇓ s
s|B = ∅
⟨linkH (e), m, Γ⟩pc ⇓ elinkH (s)
Expression evaluation:
⟨e, m, Γ⟩pc ⇓ s
Γ(e) = L = pc
⟨linkL (e), m, Γ⟩pc ⇓ elinkL (s)
Command evaluation:
1 ≤ j ≤ |S|
S (oj ) = ff ⇒ pc = L
skip
⟨skipj , m, S, Γ⟩pc →1 ⟨stop, m, S[oj (cid:55)→ tt], Γ⟩
′ = Γ
1 ≤ j ≤ |S|
′ = m[outj (cid:55)→ m(e)] ∧ Γ
sink
S (oj ) = ff ⇒ pc ⊑ Γ(outj ) ∧ (pc = H ⇒ m(outj )|B = ∅) ∧
′ = Γ[outj (cid:55)→ pc ⊔ Γ(e)]
S (oj ) = tt ⇒ m
′ = m ∧ Γ
m
⟨sinkj (e), m, S, Γ⟩pc →1 ⟨stop, m
′
′⟩
, S, Γ
|S| denotes the length of set S.
Figure 13: Monitor semantics (selected rules)
o ∈ A, S (o) = tt means that the action was skipped inside the
filter code, while S (o) = ff means that the action was not skipped,
and the value output on its corresponding sink is either the default
value (provided by IFTTT), or the value specified inside the filter
code. Initially, all actions in a skip set map to ff .
Black- and whitelisting URLs Private information can be ex-
filtrated through URL crafting or upload links, by inspecting the
parameters of requests to the attacker-controlled servers that serve
these URLs. To capture the attacker’s view for this case, we assume
a set V of URL values split into the disjoint union V = B ⊎ W of
black- and whitelisted values. For specifying security policies, it
is more suitable to reason in terms of whitelist W , the set comple-
ment of B. The whitelist W contains trusted URLs, which can be
generated automatically based on the services and ingredients used
by a given app.
Projection to B Given a list ¯v of URL values, we define URL
projection to B to obtain the list of blacklisted URLs contained in
the list.
∅|B = ∅
(v :: ¯v)|B =
v :: ¯v|B
¯v|B
if v ∈ B
if v (cid:60) B
For a given string, we further define extractURLs(·) for extract-
ing all the URLs inside the link construct link of that string. We
assume the extraction to be done similarly to the URL extraction
performed by a browser or email client, and to return an order-
preserving list of URLs. The function extends to undefined strings
as well (⊥), for which it simply returns ∅. For a string s we often
write s|B as syntactic sugar for extractURLs(s)|B.
Semantics. We now present an instrumented semantics to formal-
ize an information flow monitor for the filter code. The monitor
draws on expression typing rules, depicted in Figure 15 in Appen-
dix A. We assume information from sources to be sanitized, i.e. it
cannot contain any blacklisted URLs, and we type calls to source
with a high type H.
We display selected semantic rules in Figure 13, and refer to
Figure 16 in Appendix A for the remaining rules.
Expression evaluation For evaluating an expression, the monitor
requires a memory m mapping variables l and sink variables out
to strings s, and a typing environment Γ. The typing context or
program counter pc label is H inside of a loop or conditional whose
guard involves secret information and is L otherwise. Whenever
pc and Γ are clear from the context, we use the standard notation
m(e) = s to denote expression evaluation, ⟨e, m, Γ⟩pc ⇓ s.
Except for the link constructs, the rules for expression evaluation
are standard. We use two separate rules for expressions containing
blacklisted URLs and whitelisted URLs. We require that no sensitive
information is appended to blacklisted values. The intuition behind
this is that a benign applet maker will not try to exfiltrate user
sensitive information by specially crafting URLs (as presented in
Section 3), while a malicious applet maker should be prevented
from doing exactly that. To achieve this, we ensure that when
evaluating linkH (e), e does not contain any blacklisted URLs, while
when evaluating linkL (e), the type of e is low. Moreover, we require
the program context in which the evaluation takes place to be low
as well, as otherwise the control structure of the program could be
abused to encode information, as in Example 6.4.
Depending on a high guard (de-
noted by H), the logo sent on the
sink can be provided either from
blacklisted URL b1 or b2. Hence,
depending on the URL to which
the request is made, the attacker
learns which branch of the condi-
tional was executed.
Example 6.4.
if (H)
{ logo = linkL(b1 ) }
else
{ logo = linkL(b2 ) }
sink( logo )
Command evaluation A monitor configuration ⟨c, m, S, Γ⟩ ex-
tends the standard configuration ⟨c, m⟩ consisting of a command c
and memory m, with a skip set S and a typing environment Γ. The
filter monitor semantics (Figure 13) is then defined by the judgment
⟨c, m, S, Γ⟩pc →n ⟨c′, m′, S′, Γ′⟩, which reads as: the execution of
command c in memory m, skip set S, typing environment Γ, and pro-
gram context pc evaluates in n steps to configuration ⟨c′, m′, S′, Γ′⟩.
We denote by ⟨c, m, S, Γ⟩pc →∗(cid:32) a blocking monitor execution.
Consistently with IFTTT filters’ behavior, commands in our
language are batch programs, generating no intermediate outputs.
Accordingly, variables out are overwritten at every sink invocation
(rule sink). We discuss the selected semantic rules below.
Rule skip Though sometimes useful, action skipping may allow
for availability attacks (Section 3) or even other means of leaking
sensitive data.
Example 6.5.
sinkj (linkL(b ))
if (H) { skip j }
Consider the filter code in Ex-
ample 6.5. The snippet first sends
on the sink an image from a black-
listed URL or an upload link with
a blacklisted URL, allowing the attacker to infer that the applet has
been run. Then, depending on a high guard, the action correspond-
ing to the sink may be skipped or not. An attacker controlling the
server serving the blacklisted URL will be able to infer information
about the sensitive data whenever a request is made to the server.
Similarly, first skipping an ac-
tion in a high context, followed
by adding a blacklisted URL on
the sink (Example 6.6) also reveals
private information to a malicious
applet maker.
Example 6.6.
if (H) { skip j }
sinkj (linkL(b ))
However, first skipping an ac-
tion in a low context and then
(possibly) updating the value on
the sink in a high context (Exam-
ple 6.7) does not reveal anything
to the attacker, as the output action is never performed.
Example 6.7.
skip j
if (H)
{ sinkj (linkL(b )) }
Thus, by allowing action skipping in high contexts only if the
action had already been skipped, we can block the execution of
insecure snippets in Examples 6.5 and 6.6, and accept the execution
of secure snippet in Example 6.7.
Rule sink In sink rule we first check whether or not the output
action has been skipped. If so, we do not evaluate the expression in-
side the sink statement in order to increase monitor permissiveness.
Since the value will never be output, there is no need to evaluate an
expression which may lead to the monitor blocking an execution
incorrectly. Consider again the secure code in Example 6.7. The
monitor would normally block the execution because of the low
link which is sent on the sink in a high context. In fact, low links
are allowed only in low contexts. However, since the action was
previously skipped, the monitor will also skip the sink evaluation
and thus accept the execution. Had the action not been skipped, the
monitor would have ensured that no updates of sinks containing
blacklisted values take place in high contexts.
{sink( imgH (source))}
Example 6.8.
sink( imgL (b )+ imgH (w ))
if (H)
Consider the filter code in Ex-
ample 6.8. First, two images are
sent on the sink, one from a black-
listed URL, and the other from
a whitelisted URL. Note that the
link construct has been instantiated with an image construct for
image markup with a given URL. Depending on the high guard,
the value on the sink may be updated or not. Hence, depending on
whether or not a request to the blacklisted URL is made, a malicious
applet maker can infer information about the high data in H.
Trigger-sensitive applets. Recall the presence flow example in
Section 6.1, where a user receives a notification when their kids
arrive home. Together with the notification, a logo (possibly) origi-
nating from the applet maker is also sent, allowing the applet maker