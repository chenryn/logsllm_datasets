(i.e., the typist’s eﬀect on detector error rates). Cho et al. [5] implicitly as-
sumed a per-typist eﬀect when they conducted a paired t-test in comparing
two detectors. Often, standard deviations are reported along with error rates,
but the current work may be the ﬁrst to explicitly try to understand and
quantify the substantial typist-to-typist eﬀect.
Based on our review of the related work, we can make two observations. First,
even among those studies that have tried to explain the eﬀects of various factors
on detector error rates, interaction eﬀects have not been considered. The present
work reveals several complex interactions between algorithm, training, updating,
and impostor practice. In the future, we should bear in mind the possibility of
interactions between inﬂuential factors.
Second, there have been few attempts to generalize from empirical results
using statistical analysis. Some researchers have used hypothesis testing (e.g.,
t-tests) to establish whether there is a statistically signiﬁcant diﬀerence between
two sets of error rates [2,5], but such analysis is the rare exception rather than
the rule. The current work demonstrates the additional insight and predictive
capability that can be gained through statistical analysis. These insights and
predictions would be missing if only the raw, empirical error rates were reported.
7 Discussion and Future Work
We initiated the current work to try to explain why diﬀerent researchers, using
the same detectors, were getting wildly diﬀerent evaluation results. This work
has shown that it is possible to replicate earlier results, but great care was taken
274
K. Killourhy and R. Maxion
to make sure that the details of the replication matched those of the original.
We recruited new subjects, but tightly controlled many other factors of the
evaluation (e.g., the password, keyboard, and timing mechanism). Showing that
results can be replicated is a critical but often under-appreciated part of any
scientiﬁc discipline. Fortunately, having succeeded, we can begin to vary some
of these tightly controlled factors (using the very methodology proposed in this
paper), and we can identify which ones threaten replication.
The statistical model presented in this paper is certainly not the last word on
which factors inﬂuence keystroke-dynamics anomaly detectors. There are factors
we did not consider (e.g., the password), and for factors we did consider, there
are values we did not (e.g., other anomaly detectors). For the factors and values
we did investigate, the rigor of our methodology enables us to make claims with
comparatively high conﬁdence. Even knowing that our model is incomplete, and
possibly wrong in some respects, it represents a useful guideline which future
work can use, test, and reﬁne.1
This work is exceptional, as explained in Section 6, for its statistical analy-
sis and validation. Although many anomaly detectors are built upon statistical
machine-learning algorithms, statistical analysis is rarely used in the evaluations
of those detectors. Often a research paper will propose a new anomaly-detection
technique and report the results of a preliminary evaluation. Such evaluations
typically show a technique’s promise, but rarely provide conclusive evidence of
it. As Peisert and Bishop [15] have advocated, researchers must follow prelim-
inary, exploratory evaluations with more rigorous experiments that adhere to
the scientiﬁc method. Our methodology oﬀers a clear procedure for conduct-
ing more rigorous anomaly-detection experiments. We hope it is useful to other
researchers.
8 Conclusion
In this work, we aimed to answer two questions. First, what inﬂuence do each of
six factors—algorithm, training amount, feature set, updating, impostor prac-
tice, and typist-to-typist variation—have on keystroke-dynamics error rates? Sec-
ond, what methodology should we use to establish the eﬀects of these various
factors?
In answer to the ﬁrst question, the detection algorithm, training amount,
and updating were found to strongly inﬂuence the error rates. We found no
diﬀerence among our three feature sets, and impostor practice had only a minor
eﬀect. Typist-to-typist diﬀerences were found to introduce substantial variation;
some subjects were much easier to distinguish than others.
In answer to the second question, we proposed a methodology rooted in the sci-
entiﬁc method: experimentation, statistical analysis, and validation. This method-
ology produced a useful, predictive, explanatory model of anomaly-detector error
rates. Consequently, we believe that the proposed methodology would add valu-
able predictive and explanatory power to future anomaly-detection studies.
1 As George Box notes, “All models are wrong, but some models are useful” [4].
Why Did My Detector Do That?!
275
Acknowledgments
The authors are indebted to Howard Seltman and David Banks for sharing their
statistical expertise, and to Patricia Loring for running the experiments that
provided the data for this paper. We are grateful both to Shing-hon Lau for his
helpful comments and also to the anonymous reviewers for theirs.
This work was supported by National Science Foundation grant number CNS-
0716677, and by CyLab at Carnegie Mellon under grants DAAD19-02-1-0389 and
W911NF-09-1-0273 from the Army Research Oﬃce.
References
1. Ara´ujo, L.C.F., Sucupira, L.H.R., Liz´arraga, M.G., Ling, L.L., Yabu-uti, J.B.T.:
User authentication through typing biometrics features. IEEE Transactions on Sig-
nal Processing 53(2), 851–855 (2005)
2. Bartlow, N., Cukic, B.: Evaluating the reliability of credential hardening through
keystroke dynamics. In: Proceedings of the 17th International Symposium on Soft-
ware Reliability Engineering (ISSRE 2006), pp. 117–126. IEEE Press, Los Alamitos
(2006)
3. Bates, D.: Fitting linear mixed models in R. R. News 5(1), 27–30 (2005)
4. Box, G.E.P., Hunter, J.S., Hunter, W.G.: Statistics for Experimenters: Design,
Innovation, and Discovery, 2nd edn. Wiley, New York (2005)
5. Cho, S., Han, C., Han, D.H., Kim, H.I.: Web-based keystroke dynamics identity
veriﬁcation using neural network. Journal of Organizational Computing and Elec-
tronic Commerce 10(4), 295–307 (2000)
6. Denning, D.E.: An intrusion-detection model. IEEE Transactions on Software En-
gineering 13(2) (1987)
7. Faraway, J.J.: Extending Linear Models with R: Generalized Linear, Mixed Eﬀects
and Nonparametric Regression Models. Chapman & Hall/CRC (2006)
8. Haider, S., Abbas, A., Zaidi, A.K.: A multi-technique approach for user identiﬁca-
tion through keystroke dynamics. In: IEEE International Conference on Systems,
Man and Cybernetics, pp. 1336–1341 (2000)
9. Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer Series in Statistics. Springer, New York
(2001)
10. Joyce, R., Gupta, G.: Identity authentication based on keystroke latencies. Com-
munications of the ACM 33(2), 168–176 (1990)
11. Kang, P., Hwang, S.-s., Cho, S.: Continual retraining of keystroke dynamics based
authenticator. In: Lee, S.-W., Li, S.Z. (eds.) ICB 2007. LNCS, vol. 4642, pp. 1203–
1211. Springer, Heidelberg (2007)
12. Killourhy, K.S., Maxion, R.A.: Comparing anomaly detectors for keystroke dynam-
ics. In: Proceedings of the 39th Annual International Conference on Dependable
Systems and Networks (DSN 2009), June 29-July 2, pp. 125–134. IEEE Computer
Society Press, Los Alamitos (2009)
13. Lee, H.j., Cho, S.: Retraining a keystroke dynamics-based authenticator with im-
postor patterns. Computers & Security 26(4), 300–310 (2007)
14. Peacock, A., Ke, X., Wilkerson, M.: Typing patterns: A key to user identiﬁcation.
IEEE Security and Privacy 2(5), 40–47 (2004)
276
K. Killourhy and R. Maxion
15. Peisert, S., Bishop, M.: How to design computer security experiments. In: Proceed-
ings of the 5th World Conference on Information Security Education (WISE), pp.
141–148. Springer, New York (2007)
16. Pinheiro, J.C., Bates, D.M.: Mixed-eﬀects Models in S and S-Plus. Statistics and
Computing Series. Springer, New York (2000)
17. R Development Core Team: R: A Language and Environment for Statistical Com-
puting. R Foundation for Statistical Computing, Vienna, Austria (2008),
http://www.R-project.org
18. Searle, S.R., Casella, G., McCulloch, C.E.: Variance Components. John Wiley &
Sons, Inc., Hoboken (2006)
19. Swets, J.A., Pickett, R.M.: Evaluation of Diagnostic Systems: Methods from Signal
Detection Theory. Academic Press, New York (1982)
NetStore: An Eﬃcient Storage Infrastructure for
Network Forensics and Monitoring
Paul Giura and Nasir Memon
Polytechnic Intitute of NYU, Six MetroTech Center, Brooklyn, NY
Abstract. With the increasing sophistication of attacks, there is a need
for network security monitoring systems that store and examine very
large amounts of historical network ﬂow data. An eﬃcient storage in-
frastructure should provide both high insertion rates and fast data ac-
cess. Traditional row-oriented Relational Database Management Systems
(RDBMS) provide satisfactory query performance for network ﬂow data
collected only over a period of several hours. In many cases, such as the
detection of sophisticated coordinated attacks, it is crucial to query days,
weeks or even months worth of disk resident historical data rapidly. For
such monitoring and forensics queries, row oriented databases become
I/O bound due to long disk access times. Furthermore, their data inser-
tion rate is proportional to the number of indexes used, and query pro-
cessing time is increased when it is necessary to load unused attributes
along with the used ones. To overcome these problems we propose a new
column oriented storage infrastructure for network ﬂow records, called
NetStore. NetStore is aware of network data semantics and access pat-
terns, and beneﬁts from the simple column oriented layout without the
need to meet general purpose RDBMS requirements. The prototype im-
plementation of NetStore can potentially achieve more than ten times
query speedup and ninety times less storage size compared to traditional
row-stores, while it performs better than existing open source column-
stores for network ﬂow data.
1 Introduction
Traditionally intrusion detection systems were designed to detect and ﬂag mali-
cious or suspicious activity in real time. However, such systems are increasingly
providing the ability to identify the root cause of a security breach. This may
involve checking a suspected host’s past network activity, looking up any services
run by a host, protocols used, the connection records to other hosts that may or
may not be compromised, etc. This requires ﬂexible and fast access to network
ﬂow historical data. In this paper we present the design, implementation details
and the evaluation of a column-oriented storage infrastructure called NetStore,
designed to store and analyze very large amounts of network ﬂow data. Through-
out this paper we refer to a ﬂow as an unidirectional data stream between two
endpoints, to a ﬂow record as a quantitative description of a ﬂow, and to a ﬂow
ID as the key that uniquely identiﬁes a ﬂow. In our research the ﬂow ID is
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 277–296, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
278
P. Giura and N. Memon
Fig. 1. Flow traﬃc distribution for one day and one month. In a typical day the busiest
time interval is 1PM - 2PM with 4,381,876 ﬂows, and the slowest time interval is 5AM
- 6AM with 978,888 ﬂows. For a typical month we noticed the slow down in week-ends
and the peek traﬃc in weekdays. Days marked with * correspond to a break week.
composed of ﬁve attributes: source IP, source port, destination IP, destination
port and protocol. We assume that each ﬂow record has associated a start time
and an end time representing the time interval when the ﬂow was active in the
network.
Challenges. Network ﬂow data can grow very large in the number of records
and storage footprint. Figure 1 shows network ﬂow distribution of traﬃc cap-
tured from edge routers in a moderate sized campus network. This network with
about 3,000 hosts, commonly reaches up to 1,300 ﬂows/second, an average 53
million ﬂows daily and roughly 1.7 billion ﬂows in a month. We consider records
with the average size of 200 Bytes. Besides CISCO NetFlow data [18] there may
be other speciﬁc information that a sensor can capture from the network such as
the IP, transport and application headers information. Hence, in this example,
the storage requirement is roughly 10 GB of data per day which adds up to
at least 310 GB per month. When working with large amounts of disk resident
data, the main challenge is no longer to ensure the necessary storage space, but
to minimize the time it takes to process and access the data. An eﬃcient stor-
age and querying infrastructure for network records has to cope with two main
technical challenges: keep the insertion rate high, and provide fast access to the
desired ﬂow records. When using a traditional row-oriented Relational Database
Management Systems (RDBMS), the relevant ﬂow attributes are inserted as a
row into a table as they are captured from the network, and are indexed using
various techniques [6]. On the one hand, such a system has to establish a trade
oﬀ between the insertion rate desired and the storage and processing overhead
employed by the use of auxiliary indexing data structures. On the other hand,
enabling indexing for more attributes ultimately improves query performance
but also increases the storage requirements and decreases insertion rates. At
query time, all the columns of the table have to be loaded in memory even if
only a subset of the attributes are relevant for the query, adding a signiﬁcant
I/O penalty for the overall query processing time by loading unused columns.
NetStore: An Eﬃcient Storage Infrastructure
279
When querying disk resident data, an important problem to overcome is the I/O
bottleneck caused by large disk to memory data transfers. One potential solu-
tion would be to load only data that is relevant to the query. For example, to
answer the query “What is the list of all IPs that contacted IP X between dates
d1 and d2?”, the system should load only the source and destination IPs as well
as the timestamps of the ﬂows that fall between dates d1 and d2. The I/O time
can also be decreased if the accessed data is compressed since less data traverses
the disk-memory boundary. Further, the overall query response time can be im-
proved if data is processed in compressed format by saving decompression time.
Finally, since the system has to insert records at line speed, all the preprocessing
algorithms used should add negligible overhead while writing to disk. The above
requirements can be met quite well by utilizing a column oriented database as
described below.
Column Store. The basic idea of column orientation is to store the data by
columns rather than by rows, where each column holds data for a single attribute
of the ﬂow and is stored sequentially on disk. Such a strategy makes the system
I/O eﬃcient for read queries since only the required attributes related to a query
can be read from the disk. The performance beneﬁts of column partitioning
were previously analyzed in [9, 2], and some of the ideas were conﬁrmed by the
results in the databases academic research community [16, 1, 21] as well as in
industry [19, 11, 10, 3]. However, most of commercial and open-source column
stores were conceived to follow general purpose RDBMSs requirements, and do
not fully use the semantics of the data carried and do not take advantage of
the speciﬁc types and data access patterns of network forensic and monitoring
queries. In this paper we present the design, implementation details and the
evaluation of NetStore, a column-oriented storage infrastructure for network
records that, unlike the other systems, is intended to provide good performance
for network records ﬂow data.
Contribution. The key contributions in this paper include the following:
– Simple and eﬃcient column oriented design of NetStore, a network ﬂow
historical storage system that enables quick access to large amounts of data
for monitoring and forensic analysis.
– Eﬃcient compression methods and selection strategies to facilitate the best
compression for network ﬂow data, that permit accessing and querying data
in compressed format.
– Implementation and deployment of NetStore using commodity hardware and
open source software as well as analysis and comparison with other open
source storage systems used currently in practice.
The rest of this paper is organized as follows: we present related work in Sec-
tion 2, our system architecture and the details of each component in Section 3.
Experimental results and evaluation are presented in Section 4 and we conclude
in Section 5.
280
P. Giura and N. Memon
2 Related Work
The problem of discovering network security incidents has received signiﬁcant
attention over the past years. Most of the work done has focused on near-real
time security event detection, by improving existing security mechanisms that
monitor traﬃc at a network perimeter and block known attacks, detect suspicious
network behavior such as network scans, or malicious binary transfers [12, 14].
Other systems such as Tribeca [17] and Gigascope [4], use stream databases
and process network data as it arrives but do not store the date for retroactive
analysis. There has been some work done to store network ﬂow records using a
traditional RDBMS such as PostgreSQL [6]. Using this approach, when a NIDS
triggers an alarm, the database system builds indexes and materialized views for
the attributes that are the subject of the alarm, and could potentially be used by
forensics queries in the investigation of the alarm. The system works reasonably
well for small networks and is able to help forensic analysis for events that
happened over the last few hours. However, queries for traﬃc spanning more
than a few hours become I/O bound and the auxiliary data used to speed up
the queries slows down the record insertion process. Therefore, such a solution is
not feasible for medium to large networks and not even for small networks in the
future, if we consider the accelerated growth of internet traﬃc. Additionally, a
time window of several hours is not a realistic assumption when trying to detect
the behavior of a complex botnet engaged in stealthy malicious activity over
prolonged periods of time.
In the database community, many researchers have proposed the physical
organization of database storage by columns in order to cope with poor read
query performance of traditional row-based RDBMS [16,21,11,15,3]. As shown
in [16, 2, 9, 8], a column store provides many times better performance than a
row store for read intensive workloads. In [21] the focus is on optimizing the
cache-RAM access time by decompressing data in the cache rather than in the
RAM. This system assumes the working columns are RAM resident, and shows
a performance penalty if data has to be read from the disk and processed in the
same run. The solution in [16] relies on processing parallelism by partitioning
data into sets of columns, called projections, indexed and sorted together, inde-
pendent of other projections. This layout has the beneﬁt of rapid loading of the
attributes belonging to the same projection and referred to by the same query
without the use of auxiliary data structure for tuple reconstruction. However,
when attributes from diﬀerent projections are accessed, the tuple reconstruction
process adds signiﬁcant overhead to the data access pattern. The system pre-
sented in [15] emphasizes the use of an auxiliary metadata layer on top of the
column partitioning that is shown to be an eﬃcient alternative to the indexing
approach. However, the metadata overhead is sizable and the design does not
take into account the correlation between various attributes.
Finally, in [9] authors present several factors that should be considered when
one has to decide to use a column store versus a row store for a read intensive
workload. The relative large number of network ﬂow attributes and the workloads
NetStore: An Eﬃcient Storage Infrastructure
281
with the predominant set of queries with large selectivity and few predicates favor
the use of a column store system for historical network ﬂow records storage.
NetStore is a column oriented storage infrastructure that shares some of the
features with the other systems, and is designed to provide the best perfor-
mance for large amounts of disk resident network ﬂow records. It avoids tuple
reconstruction overhead by keeping at all times the same order of elements in
all columns. It provides fast data insertion and quick querying by dynamically
choosing the most suitable compression method available and using a simple and
eﬃcient design with a negligible meta data layer overhead.
3 Architecture
In this section we describe the architecture and the key components of NetStore.
We ﬁrst present the characteristics of network data and query types that guide
our design. We then describe the technical design details: how the data is par-
titioned into columns, how columns are partitioned into segments, what are the
compression methods used and how a compression method is selected for each
segment. We ﬁnally present the metadata associated with each segment, the in-
dex nodes, and the internal IPs inverted index structure, as well as the basic set
of operators.
3.1 Network Flow Data
Network ﬂow records and the queries made on them show some special char-
acteristics compared to other time sequential data, and we tried to apply this
knowledge as early as possible in the design of the system. First, ﬂow attributes
tend to exhibit temporal clustering, that is, the range of values is small within
short time intervals. Second, the attributes of the ﬂows with the same source IP
and destination IP tend to have the same values (e.g. port numbers, protocols,
packets sizes etc.). Third, columns of some attributes can be eﬃciently encoded
when partitioned into time based segments that are encoded independently. Fi-
nally, most attributes that are of interest for monitoring and forensics can be
encoded using basic integer data types.
The records insertion operation is represented by bulk loads of time sequen-
tial data that will not be updated after writing. Having the attributes stored
in the same order across the columns makes the join operation become trivial
when attributes from more than one column are used together. Network data
analysis does not require fast random access on all the attributes. Most of the
monitoring queries need fast sequential access to large number of records and
the ability to aggregate and summarize the data over a time window. Forensic
queries access speciﬁc predictable attributes but collected over longer periods of
time. To observe their speciﬁc characteristics we ﬁrst compiled a comprehensive
list of forensic and monitoring queries used in practice in various scenarios [5].
Based on the data access pattern, we identiﬁed ﬁve types among the initial list.