samples to DeepSpeech, but none of them can be decoded
correctly. As Carlini et al. have successfully modiﬁed any
audio into a command recognizable by DeepSpeech [23],
we intend to leverage their open source algorithm to ex-
amine if it is possible to generate one adversarial sample
against both two platforms. In this experiment, we started
by 10 adversarial samples generated by CommanderSong,
either WTA or WAA attack, integrating commands like
Okay google call one one zero one one nine one two zero,
Echo open the front door, and Echo turn off the light. We
applied their algorithm to modify the samples until Deep-
Speech can decode the target commands correctly. Then
we tested such newly generated samples against Kaldi as
WTA attack, and Kaldi can still successfully recognize
them. We did not perform WAA attack since their algo-
rithm targeting DeepSpeech cannot achieve attacks over
the air.
The preliminary evaluations on transferability give us
the opportunities to understand CommanderSongs and for
designing systematic approach to transfer in the future.
5.4 Automated Spreading
Since our WAA attack samples can be used to launch the
practical adversarial attack against ASR systems, we want
to explore the potential channels that can be leveraged to
impact a large amount of victims automatically.
Online sharing. We consider the online sharing plat-
forms like YouTube to spread CommanderSong. We
picked up one ﬁve-second adversarial sample embedded
with the command “open the door” and applied Windows
Movie Maker software to make a video, since YouTube
only supports video uploading. The sample was repeated
four times to make the full video around 20 seconds. We
then connected our desktop audio output to Bose Com-
panion 2 speaker and installed iFLYTEK Input on LG V20
smartphone. In this experiment, the distance between the
speaker and the phone can be up to 0.5 meter, and iFLY-
TEK Input can still decode the command successfully.
Radio broadcasting.
In this experiment, we used
HackRF One [8], a hardware that supports Software De-
ﬁned Radio (SDR) to broadcast our CommanderSong at
the frequency of FM 103.4 MHz, simulating a radio sta-
tion. We setup a radio at the corresponding frequency,
so it can receive and play the CommanderSong. We ran
the WeChat7 application and enabled the iFLYTEK Input
on different smartphones including iPhone 6S, Huawei
Honor 8 and XiaoMi MI Note3.
iFLYTEK Input can
always successfully recognize the command “open the
door” from the audio played by the radio and display it
on the screen.
5.5 Efﬁciency
We also evaluate the cost of generating CommanderSong
in the aspect of the required time. For each command,
we record the time to inject it into different songs and
compute the average. Since the time required to craft also
depends on the length of the desired command, we deﬁne
the efﬁciency as the ratio of the number of frames of the
desired command and the required time. Table 2 and Ta-
ble 3 show the efﬁciency of generating WTA and WAA
samples for different commands. Most of those adversar-
ial samples can be generated in less than two hours, and
some simple commands like “Echo open the front door”
can be done within half an hour. However, we do notice
that some special words (such as GPS and airplane) in
the command make the generation time longer. Probably
those words are not commonly used in the training process
of the “ASpIRE model” of Kaldi, so generating enough
phonemes to represent the words is time-consuming. Fur-
thermore, we ﬁnd that, for some songs in the rock cate-
gory such as “Bang bang” and “Roaked”, it usually takes
longer to generate the adversarial samples for the same
command compared with the songs in other categories,
probably due to the unstable rhythm of them.
6 Understanding the Attacks
We try to deeply understand the attacks, which could po-
tentially help to derive defense approaches. We raise some
7WeChat is the most popular instant messaging application in China,
with approximately 963,000,000 users all over the world by June
2017 [15].
58    27th USENIX Security Symposium
USENIX Association
Figure 4: SNR impacts on correlation of the audios and
the success rate of adversarial audios.
questions and perform further analysis on the attacks.
In what ways does the song help the attack? We use
songs as the carrier of commands to attack ASR sys-
tems. Obviously, one beneﬁt of using a song is to prevent
listeners from being aware of the attack. Also Comman-
derSong can be easily spread through Youtube, radio, TV,
etc. Does the song itself help generate the adversarial
audio samples? To answer this question, we use a piece
of silent audio as the “carrier” to generate Commander-
Song Acs (WAA attack), and test the effectiveness of it.
The results show that Acs can work, which is aligned with
our ﬁndings – a random song can serve as the “carrier”
because a piece of silent audio can be viewed as a special
song.
However, after listening to Acs, we ﬁnd that Acs sounds
quite similar to the injected command, which means any
user can easily notice it, so Acs is not the adversarial sam-
ples we desire. Note that, in our human subject study,
none of the participants recognized any command from
the generated CommanderSongs. We assume that some
phonemes or even smaller units in the original song work
together with the injected small perturbations to form the
target command. To verify this assumption, we prepare a
song As and use it to generate the CommanderSong Acs.
Then we calculate the difference Δ(As,Acs) between them,
and try to attack ASR systems using Δ(As,Acs). However,
after several times of testing, we ﬁnd that Δ(As,Acs) does
not work, which indicates the pure perturbations we in-
jected cannot be recognized as the target commands.
Recall that in Table 5, the songs in the soft music
category are proven to be the best carrier, with lowest
abnormality identiﬁed by participants. Based on the ﬁnd-
ings above, it appears that such songs can better aligned
with the phonemes or smaller “units” in the target com-
mand to help the attack. This is also the reason why
Δ(As,Acs) cannot directly attack successfully: the “units”
Figure 5: Explaination of Kaldi and human recognition
of the audios.
in the song combined with Δ(As,Acs) together construct
the phonemes of the target command.
What is the impact of noise in generating adversar-
ial samples? As mentioned early, we build a generic
random noise model to perform the WAA attack over
the air. In order to understand the impact of the noise
in generating adversarial samples, we crafted Comman-
derSong using noises with different amplitude values.
Then we observed the differences between the Comman-
derSong and the original song, the differences between
the CommanderSong and the pure command audio, and
the success rates of the CommanderSong to attack. To
characterize the difference, we leverage Spearman’s rank
correlation coefﬁcient [46] (Spearman’s rho for short)
to represent the similarity between two pieces of audio.
Spearman’s rho is widely used to represent the corre-
lation between two variables, and can be calculated as
follows: r(X,Y ) = Cov(X,Y )/
Var[X]Var[Y ], where X
and Y are the MFCC features of the two pieces of audio.
Cov(X,Y ) represents the covariance of X and Y. Var[X]
and Var[Y ] are the variances of X and Y respectively.
(cid:2)
The results are shown in Figure 4. The x-axis in the
ﬁgure shows the SNR (in dB) of the noise, and the y-axis
gives the correlation. From the ﬁgure, we ﬁnd that the
correlation between the CommanderSong and the original
song (red line) decreases with SNR. It means that the
CommanderSong sounds less like the original song when
the amplitude value of the noise becomes larger. This
is mainly because the original song has to be modiﬁed
more to ﬁnd a CommanderSong robust enough against the
introduced noise. On the contrary, the CommanderSong
becomes more similar with the target command audio
when the amplitude values of the noise increases (i.e.,
decrease of SNR in the ﬁgure, blue line), which means
that the CommanderSong sounds more like the target
command. The success rate (black dotted line) also in-
creases with the decrease of SNR. We also note that, when
USENIX Association
27th USENIX Security Symposium    59
Figure 6: Audio turbulence defense.
SNR = 4 dB, the success rate could be as high as 88%.
Also the correlation between CommanderSong and the
original song is 90%, which indicates a high similarity.
Figure 5 shows the results from another perspective.
Suppose the dark blue circle is the set of audios that
can be recognized as commands by ASR systems, while
the light blue circle and the red one represent the sets
of audio recognized as commands and songs by human
respectively. At ﬁrst, the original song is in the red circle,
which means that neither ASR systems nor human being
recognize any command inside. WTA attack slightly
modiﬁes the song so that the open source system Kaldi
can recognize the command while human cannot. After
noises are introduced to generate CommanderSong for
WAA attacks, CommanderSong will fall into the light
blue area step by step, and in the end be recognized by
human. Therefore, attackers can choose the amplitude
values of noise to balance between robustness to noise
and identiﬁability by human users.
7 Defense
We propose two approaches to defend against Comman-
derSong: Audio turbulence and Audio squeezing. The
ﬁrst defense is effective against WTA, but not WAA; while
the second defense works against both attacks.
Audio turbulence. From the evaluation, we observe that
noise (e.g., from speaker or background) decreases the
success rate of CommanderSong while impacts little on
the recognition of audio command. So our basic idea
is to add noise (referred to as turbulence noise An) to
the input audio AI before it is received by the ASR sys-
tem, and check whether the resultant audio AI +(cid:4)An can
be interpreted as other words. Particularly, as shown in
Figure 6, AI is decoded as text1 by the ASR system.
Then we add An to AI and let the ASR system extract
the text text2 from AI +(cid:4)An. If text1 (cid:8)=text2, we say
that the CommanderSong is detected.
We did experiments using this approach to test the ef-
fectiveness of such defense. The target command “open
the door” was used to generate a CommanderSong. Fig-
ure 7 shows the result. The x-axis shows the SNR (AI to
An), and the y-axis shows the success rate. We found that
the success rate of WTA dramatically drops when SNR
Figure 7: The results of audio turbulence defense.
decreases. When SNR = 15 dB, WTA almost always fails
and AI can still be successfully recognized, which means
this approach works for WTA. However, the success rate
of WAA is still very high. This is mainly because Com-
manderSongs for WAA is generated using random noises,
which is robust against turbulence noise.
Audio squeezing. The second defense is to reduce the
sampling rate of the input audio AI (just like squeezing the
audio). Instead of adding An in the defense of audio tur-
bulence, we downsample AI (referred to as D(AI)). Still,
ASR systems decode AI and D(AI), and get text1 and
text2 respectively. If text1 (cid:8)=text2, the Commander-
Song is detected. Similar to the previous experiment, we
evaluate the effectiveness of this approach. The results are
shown in Figure 8. The x-axis shows the ratio (1/M) of
downsampling (M is the downsampling factor or decima-
tion factor, which means that the original sampling rate is
M times of the downsampled rate). When 1/M = 0.7 (if
the sample rate is 8000 samples/second, the downsampled
rate is 5600 samples/second), the success rates of WTA
and WAA are 0% and 8% respectively. AI can still be
successful recognized at the rate of 91%. This means that
Audio squeezing is effective to defend against both WTA
and WAA.
8 Related Work
Attack on ASR system. Prior to our work, many re-
searchers have devoted to security issues about speech
controllable systems [36, 35, 26, 41, 51, 22, 53, 23]. De-
nis et al. found the vulnerability of analog sensor and
injected bogus voice signal to attack the microphone [36].
Kasmi et al. stated that, by leveraging intentional electro-
magnetic interference on headset cables, voice command
could be injected and carried by FM signals which is
further received and interpreted by smart phones [35].
60    27th USENIX Security Symposium
USENIX Association
Brown et al. [21] showed by adding an universal patch to
an image they could fool the image classiﬁers successfully.
Evtimov et al. [27] proposed a general algorithm which
can produce robust adversarial perturbations into images
to overcome physical condition in real world. They suc-
cessfully fooled road sign classiﬁers to mis-classify real
Stop Sign. Different from them, our study targets speech
recognition system.
Defense of Adversarial on machine learning. Defend-
ing against adversarial attacks is known to be a challeng-
ing problem. Existing defenses include adversarial train-
ing and defensive distillation. Adversarial training [39]
adds the adversarial examples into the model’s training set
to increase its robustness against these examples. Defen-
sive distillation [33] trains the model with probabilities of
different class labels supported by an early model trained
on the same task. Both defenses perform a kind of gra-
dient masking [45] which increases the difﬁculties for
the adversary to compute the gradient direction. In [29],
Dawn Song attempted to combine multiple defenses in-
cluding feature squeezing and the specialist to construct
a larger strong defense. They stated that defenses should
be evaluated by strong attacks and adaptive adversarial
examples. Most of these defenses are effective for white
box attacks but not for black box ones. Binary classiﬁ-
cation is another simple and effective defense for white
box attacks without any modiﬁcations of the underlying
systems. A binary classiﬁer is built to separate adversarial
examples apart from the clean data. Similar as adversarial
training and defensive distillation, this defense suffers
from generalization limitation. In this paper, we propose
two novel defenses against CommanderSong attack.
9 Conclusion
In this paper, we perform practical adversarial attacks
on ASR systems by injecting “voice” commands into
songs (CommanderSong). To the best of our knowledge,
this is the ﬁrst systematical approach to generate such
practical attacks against DNN-based ASR system. Such
CommanderSong could let ASR systems execute the com-
mand while being played over the air without notice by
users. Our evaluation shows that CommanderSong can be
transferred to iFLYTEK, impacting popular apps such as
WeChat, Sina Weibo, and JD with billions of users. We
also demonstrated that CommanderSong can be spread
through YouTube and radio. Two approaches (audio turbu-
lence and audio squeezing) are proposed to defend against
CommanderSong.
Figure 8: Audio squeezing defense result.
Diao et al. demonstrated that, through permission by-
passing attack in Android smart phones, voice commands
could be played using apps with zero permissions [26].
Mukhopadhyay et al. considered voice impersonation
attacks to contaminate a voice-based user authentication
system [41]. They reconstructed the victims voice model
from the victims voice data, and launched attacks that can
bypass voice authentication systems. Different from these
attacks, we are attacking the machine learning models of
ASR systems.
Hidden voice command [22] launched both black box
(i.e., inverse MFCC) and white box (i.e., gradient decent)
attacks against ASR systems with GMM-based acous-
tic models. Different from this work, our target is a
DNN-based ASR system. Recently, the authors posted the
achievement that can construct targeted audio adversar-
ial examples on DeepSpeech, an end-to-end open source
ASR platform [23]. To perform the attack, the adver-
sary needs to directly upload the adversarial WAV ﬁle to
the speech recognition system. Our attacks on Kaldi are
concurrent to their work, and our attack approaches are in-
dependent to theirs. Moreover, our attacks succeed under
a more practical setting that let the adversarial audio be
played over the air. The recent work DolphinAttack [53]
proposed a completely inaudible voice attack by modu-
lating commands on ultrasound carriers and leveraging
microphone vulnerabilities to attack. As noted by the
authors, such attack can be eliminated by ﬁltering out
ultrasound carrier (e.g., iPhone 6 Plus). Differently, our
attack uses songs instead of ultrasound as the carriers,
making the attack harder to defend.
Adversarial research on machine learning. Besides
attacking speech recognition systems, there has been sub-
stantial work on adversarial machine learning examples
towards physical world. Kurakin et al.
[37] proved it
is doable that Inception v3 image classiﬁcation neural
network could be compromised by adversarial images.
USENIX Association
27th USENIX Security Symposium    61
Acknowledgments
IIE authors are supported in part by National Key
R&D Program of China (No.2016QY04W0805), NSFC
U1536106, 61728209, National Top-notch Youth Talents
Program of China, Youth Innovation Promotion Associa-
tion CAS and Beijing Nova Program. Indiana University
author is supported in part by the NSF 1408874, 1527141,
1618493 and ARO W911NF1610127. Illinois Univer-
sity authors are supported in part by NSF CNS grants
13-30491, 14-08944, and 15-13939.
References
[1] Amazon Alexa. https://developer.amazon.com/alexa.
[2] Amazon Mechanical Turk. https://www.mturk.com.
[3] Apple Siri. https://www.apple.com/ios/siri.
[4] Aspire. https://github.com/kaldi-asr/kaldi/tree/master/egs/aspire.
[5] CMUSphinx. https://cmusphinx.github.io/.
[6] Google Assistant. https://assistant.google.com.
[7] Google Text-to-speech. https://play.google.com/store/apps.
[8] HackRF One. https://greatscottgadgets.com/hackrf/.
[9] HTK. http://htk.eng.cam.ac.uk/.
[10] iFLYREC. https://www.iﬂyrec.com/.
[11] iFLYTEK. http://www.iﬂytek.com/en/index.html.
[12] iFLYTEK Input. http://www.iﬂytek.com/en/mobile/iﬂyime.html.