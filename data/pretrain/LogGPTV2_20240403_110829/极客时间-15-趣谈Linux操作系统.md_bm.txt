## 总结时刻对于虚拟文件系统的解析就到这里了，我们可以看出，有关文件的数据结构层次多，而且很复杂，就得到了下面这张图，这张图在这个专栏最开始的时候，已经展示过一遍，到这里，你应该能明白它们之间的关系了。![](Images/fde2b3a5ac2979d5d0ee543389c1eb01.png){savepage-src="https://static001.geekbang.org/resource/image/80/b9/8070294bacd74e0ac5ccc5ac88be1bb9.png"}这张图十分重要，一定要掌握。因为我们后面的字符设备、块设备、管道、进程间通信、网络等等，全部都要用到这里面的知识。希望当你再次遇到它的时候，能够马上说出各个数据结构直接的关系。这里我带你简单做一个梳理，帮助你理解记忆它。对于每一个进程，打开的文件都有一个文件描述符，在 files_struct里面会有文件描述符数组。每个一个文件描述符是这个数组的下标，里面的内容指向一个file 结构，表示打开的文件。这个结构里面有这个文件对应的inode，最重要的是这个文件对应的操作file_operation。如果操作这个文件，就看这个 file_operation 里面的定义了。对于每一个打开的文件，都有一个 dentry 对应，虽然叫作 directoryentry，但是不仅仅表示文件夹，也表示文件。它最重要的作用就是指向这个文件对应的inode。如果说 file 结构是一个文件打开以后才创建的，dentry 是放在一个 dentrycache里面的，文件关闭了，他依然存在，因而他可以更长期的维护内存中的文件的表示和硬盘上文件的表示之间的关系。inode 结构就表示硬盘上的 inode，包括块设备号等。几乎每一种结构都有自己对应的 operation结构，里面都是一些方法，因而当后面遇到对于某种结构进行处理的时候，如果不容易找到相应的处理函数，就先找这个operation 结构，就清楚了。
## 课堂练习上一节的总结中，我们说，同一个文件系统中，文件夹和文件的对应关系。如果跨的是文件系统，你知道如何维护这种映射关系吗？欢迎留言和我分享你的疑惑和见解，也欢迎可以收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。![](Images/1a5564dd4e1c9f25d4772c7f844ca84a.png){savepage-src="https://static001.geekbang.org/resource/image/8c/37/8c0a95fa07a8b9a1abfd394479bdd637.jpg"}
# 30 \| 文件缓存：常用文档应该放在触手可得的地方上一节，我们讲了文件系统的挂载和文件的打开，并通过打开文件的过程，构建了一个文件管理的整套数据结构体系。其实到这里，我们还没有对文件进行读写，还属于对于元数据的操作。那这一节，我们就重点关注读写。
## 系统调用层和虚拟文件系统层文件系统的读写，其实就是调用系统函数 read 和write。由于读和写的很多逻辑是相似的，这里我们一起来看一下这个过程。下面的代码就是 read 和 write 的系统调用，在内核里面的定义。    SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count){struct fd f = fdget_pos(fd);......loff_t pos = file_pos_read(f.file);ret = vfs_read(f.file, buf, count, &pos);......}  SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf,size_t, count){struct fd f = fdget_pos(fd);......loff_t pos = file_pos_read(f.file);    ret = vfs_write(f.file, buf, count, &pos);......}对于 read 来讲，里面调用 vfs_read-\>\_\_vfs_read。对于 write来讲，里面调用 vfs_write-\>\_\_vfs_write。下面是 \_\_vfs_read 和 \_\_vfs_write 的代码。    ssize_t __vfs_read(struct file *file, char __user *buf, size_t count,   loff_t *pos){if (file->f_op->read)return file->f_op->read(file, buf, count, pos);else if (file->f_op->read_iter)return new_sync_read(file, buf, count, pos);elsereturn -EINVAL;}  ssize_t __vfs_write(struct file *file, const char __user *p, size_t count,    loff_t *pos){if (file->f_op->write)return file->f_op->write(file, p, count, pos);else if (file->f_op->write_iter)return new_sync_write(file, p, count, pos);elsereturn -EINVAL;}上一节，我们讲了，每一个打开的文件，都有一个 struct file结构。这里面有一个 struct file_operationsf_op，用于定义对这个文件做的操作。\_\_vfs_read 会调用相应文件系统的file_operations 里面的 read 操作，\_\_vfs_write 会调用相应文件系统file_operations 里的 write 操作。
## ext4 文件系统层``{=html}对于 ext4 文件系统来讲，内核定义了一个 ext4_file_operations。    const struct file_operations ext4_file_operations = {.......read_iter= ext4_file_read_iter,.write_iter= ext4_file_write_iter,......}由于 ext4 没有定义 read 和 write 函数，于是会调用 ext4_file_read_iter 和ext4_file_write_iter。ext4_file_read_iter 会调用 generic_file_read_iter，ext4_file_write_iter会调用 \_\_generic_file_write_iter。    ssize_tgeneric_file_read_iter(struct kiocb *iocb, struct iov_iter *iter){......    if (iocb->ki_flags & IOCB_DIRECT) {......        struct address_space *mapping = file->f_mapping;......        retval = mapping->a_ops->direct_IO(iocb, iter);    }......    retval = generic_file_buffered_read(iocb, iter, retval);}  ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from){......    if (iocb->ki_flags & IOCB_DIRECT) {......        written = generic_file_direct_write(iocb, from);......    } else {......written = generic_perform_write(file, from, iocb->ki_pos);......    }}generic_file_read_iter 和 \_\_generic_file_write_iter有相似的逻辑，就是要区分是否用缓存。缓存其实就是内存中的一块空间。因为内存比硬盘快的多，Linux为了改进性能，有时候会选择不直接操作硬盘，而是将读写都在内存中，然后批量读取或者写入硬盘。一旦能够命中内存，读写效率就会大幅度提高。因此，根据是否使用内存做缓存，我们可以把文件的 I/O 操作分为两种类型。第一种类型是**缓存 I/O**。大多数文件系统的默认 I/O 操作都是缓存I/O。对于读操作来讲，操作系统会先检查，内核的缓冲区有没有需要的数据。如果已经缓存了，那就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。对于写操作来讲，操作系统会先将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说，写操作就已经完成。至于什么时候再写到磁盘中由操作系统决定，除非显式地调用了sync 同步命令。第二种类型是**直接IO**，就是应用程序直接访问磁盘数据，而不经过内核缓冲区，从而减少了在内核缓存和用户程序之间数据复制。如果在读的逻辑 generic_file_read_iter 里面，发现设置了IOCB_DIRECT，则会调用 address_space 的 direct_IO的函数，将数据直接读取硬盘。我们在 mmap 映射文件到内存的时候讲过address_space，它主要用于在内存映射的时候将文件和内存页产生关联。同样，对于缓存来讲，也需要文件和内存页进行关联，这就要用到address_space。address_space 的相关操作定义在 structaddress_space_operations 结构中。对于 ext4 文件系统来讲， address_space的操作定义在 ext4_aops，direct_IO 对应的函数是 ext4_direct_IO。    static const struct address_space_operations ext4_aops = {.......direct_IO= ext4_direct_IO,......};如果在写的逻辑 \_\_generic_file_write_iter 里面，发现设置了IOCB_DIRECT，则调用 generic_file_direct_write，里面同样会调用address_space 的 direct_IO 的函数，将数据直接写入硬盘。ext4_direct_IO 最终会调用到\_\_blockdev_direct_IO-\>do_blockdev_direct_IO，这就跨过了缓存层，直接到了文件系统的设备驱动层。由于文件系统是块设备，所以这个调用的是blockdev相关的函数，有关块设备驱动程序的原理我们下一章详细讲，这一节我们就讲到文件系统到块设备的分界线部分。    /* * This is a library function for use by filesystem drivers. */static inline ssize_tdo_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,      struct block_device *bdev, struct iov_iter *iter,      get_block_t get_block, dio_iodone_t end_io,      dio_submit_t submit_io, int flags){......}接下来，我们重点看带缓存的部分如果进行读写。