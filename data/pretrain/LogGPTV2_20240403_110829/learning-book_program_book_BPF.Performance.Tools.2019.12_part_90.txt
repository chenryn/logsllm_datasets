---
## Page 730
16.3 Guest BPF Tools
693
16.3
Guest BPF Tools
This section covers the BPF tools you can use for guest VM performance analysis and trouble
shooting. These are either from the BCC and bpftrace repositories covered in Chapters 4 and 5 or
were created for this book.
16.3.1 Xen Hypercalls
If the guest uses paravirt and makes hypercalls, they can be instrumented using existing tools:
funccount(8), trace(8), argdlist(8), and stackcount(8). There are even Xen tracepoints you can use.
Measuring hypercall latency requires custom tooling.
Xen PV
For example, this system has booted into paravirtualization (PV):
dnesg 1 gzep Bypervisor
0.000000] ypervi.sor detectedi: Xen FV
Using BCC funccount(8) to count the available Xen tracepoints:
+funccount *t:xen:*1
' uex:a。x0gsu0t4oumg0E butoex
H1t Ctr1-C to end
FUHIC
COUVT
xen:xen_nu_flush_tlb_one_user
70
daenuuuax:oax
84
xen:xen_rmu_set_pte_at
95
xen:xen_nc_ca1lback
97
xen:xen_nc_extend_args
194
[aannuu"uaxtuax
194
xen:xen_nc_entxy_al1cc
904
Arqoaouuaxiuax
924
xen:xen_nc_flush
1175
ansTouuax:uax
1378
xen:xen_nc_ba tch
1392
De tach.ing- ..
The xen_mc tracepoints are for multicalls: batched hypercalls. These begin with a xen:xen_mc_batch
call, then xen:xen_mc_entry calls for each hypercall, and finish with a xen:xen_mc_issue. The real
aotreuoad e sy usng"ou"uax:uax 6q paoen °uoperado qsng e u suaddeq 4uo meouadq
optimization, there are two *lazy” paravirt modes where the ssue will be ignored, allowing
multicalls to buffer and be flushed later: one for MMU updates, and one for context switching.
xen_mc_calls. But if no xen_mc_calls are made, the issue and flush are for zero hypercalls.
Various kernel code paths are bracketed by xen_mc_batch and xen_mc_issue, to group possible
---
## Page 731
69
Chapter 16 Hypervisors
The xenhyper(8) tool in the next section is an example of using one of these tracepoints. With so
many tracepoints available, more such tools could be written, but unfortunately Xen PV guests
are becoming less frequently used, giving way to HVM guests (PVHVM). I've only included one
tool as a demonstration, and the following one-liners.
Xen PV: Counting Hypercalls
The number of issued hypercalls can be counted via the xen:xen_mc_flush tracepoint, along
with its mcidx argument, which shows how many hypercalls were made. For example, using BCC
argdist(8):
argdist -C *t:xen:xen_nc_flush () :int:args=>ncidx*
[17:41:34]
t:xen:xen_8c_flush () :1nt:axgs=>nc1dx
COUNT
EVENT
44
axgs=>nc1dx = 0
136
args=>ncidx = 1
[17: 41:35]
t1xen:xen_mc_flush 1) :int:args=>mcidx
COUNT
EVENT
37
args->neidx = 0
133
args=>ncIdx = 1
[...]
This frequency counts how many hypercalls were issued on each flush. If the count is zero, no
batching beyond a single hypercall per batch while tracing.
hypercall was made. The above output shows about 130 hypercalls per second and no cases of
Xen PV: Hypercall Stacks
Each of the Xen tracepoints can be traced using stackcount(8) to reveal the code path that
triggered them. For example, tracing when a multicall was issued:
 stackcount 't:xen:xen_nc_issue
Tracing 1 functlons for *t:xen:xen_ac_issue*... Hit Ctzl-C to end.
C
[.--]
xen_load_sp0
svitch_to
schedu1e
schedule
schedule_preenp t_dlsabLed
cpu_startup_entry
ppuedn6uxαndo
6629
---
## Page 732
16.3 Guest BPF Tools
695
xen_load_t1s
16448
xen_Clush_tlb_single
flush_tlb_page
ptep_clear_flush
ofeddx"o
doo"aeddA
handile_mm_fau1t
tneg"ebed-op
do_page_fault
page_fau1t
46604
e"ad"as"uax
efvex"e6ed-.dco
copy_process,psrt.33
_do_foxk
sys_clone
do_sysca11_64
return_fron_SYscALL_64
L06595
Detach.ing ...
Excessive multicalls (hypercalls) can be a performance issue, and this output helps reveal the
reason for them. The overhead of hypercall tracing depends on their rate, which for busy systems
may be frequent, costing noticeable overheadl.
Xen PV: Hypercall Latency
The real hypercall only happens during the flush operation, and there are no tracepoints for when
this begins and ends. You can switch to kprobes to trace the xen_mc_flush( kernel function,
which includes the real hypercall. Using BCC funclatency(8):
 funclateney xen_no_flush
Tracing l functlons for *xen_ac_flush*... Hit Ctel-C to end.
n.sec.3
: count
dlstrlbutlon
 > 1
: 0
E 7
: 0
8 -> 15
: 0
1 6 -> 31
: 0
---
## Page 733
696
Chapter 16 Hypervisors
32 > 63
: 0
64 -> 127
: 0
128 -> 255
: 0
25 6 -> 511
805ZE :
51.2 -> 1023
: 80586
★ ***x
1024 -> 2047
1 21022
] + ++
2048 > 4095
:3519
| *
4096 -> 8191
: 7141
: 12825
|******
8192 -> 16383
|***
L9LZE  65535
: 51
65536 -> 131071
: 845
131072 > 262143
:2
This can be an important measure of hypervisor performance from the guest. A BCC tool can be
written to remember which hypercalls were batched so that this hypercall latency can be broken
down by hypercall operation type.
Another way to determine issues of hypercall latency is to try CPU profiling, covered in Chapter 6,
and look for CPU time spent in hypercalls, either in the hypercall_page() function (which is
really a table of hypercall functions) or in the xen_hypercall*0 functions. An example is shown in
Figure 16-2.
Figure 16-2 CPU Flame graph excerpt showing Xen PV hypercall
This shows a TCP receive codepath ending in hypercall_page(). Note that this CPU profiling
approach may be misleading as it may not be possible to sample some hypercall code paths
from the guest. This is because PV guests usually do not have access to PMC-based profiling, and
instead will default to software-based profiling, which cannot sample during IRQ-disabled code
paths, which can include hypercalls. This issue was described in Section 6.2.4 in Chapter 6.
---
## Page 734
16.3 Guest BPF Tools
69
Xen HVM
For an HVM guest, the xen tracepoints usually do not fire:
dnesg 1 grep Bypervisor
0.0ooooo] Bypervlsor detected: Xen BVH
+funccount 't:xen:xen*'
Tzacing 27 functions foz *t:xen:xen**... Hilt Ctrl-C to end.
C
FUHC
COUNT
Detach.ing-..
This is because those code paths no longer hypercall but instead make native calls that are trapped
and handlled by the HVM hypervisor. This makes inspection of hypervisor performance more
difficult: it must be inspected using the normal resource-oriented tools covered in earlier chapters,
bearing in mind that these resources are accessed via a hypervisor, and therefore observed
latencies are due to the resource plus hypervisor latency.
16.3.2 xenhyper
xenhyper(8) is a bpftrace tool to count hypercalls via the xen:xen_mc_entry tracepoint and
prints a count of the hypercall names. This is only useful for Xen guests booting into paravirt
mode and using hypercalls. Example output:
 xenhyper.bt
Attaching 1 probe...
[nmu_update| : 44
8。 :[Burddeu“e.“aepdn]e
[nnuext_op]: 6473
[stack_svitch] : 23445
The source to xenhyper(8) is:
+1/usr/local/bin/bpft.race
BEGIN
printf (*Counting Xen hypercalls (xen_nc_entry) . Ctr]-C to end. \o*) 
// needs updating to match your kernel version1 xen-hypercalls,h
fotqesdexos。 - [0]eueug
Bnane[1] - *rmu_update*;
1. Origin: I developed it for this bok on 2-Feb-2019.
---
## Page 735
698
8 Chapter 16 Hypervisors
ap6"as。 - [2]aueug
Bnane[3] - *stack_svitch";
axoeos。 - []aueug
fuoaTxsgse"ndy。 = [s] eueug
Bnane[6] - *sched_op_compat”;
do“guop。 - [cleues3
Bnane [8] = *set_debugreg”;
rμ6sx6nqeps6。 = [6]eueu
Bnane [10] = *update_descriptor”;
Enane[11] - “nemory_op*
Bnane [12] = *multica11*
rbutddes"ea"esepdn, = [Et] eueu]
_dosautae。 - []aueug
Bnane [15] = "event_channel_op_conpat";
Bnane [16] = *xen_version*;
o"otosuoo = [t]eueu
rμeduoo“dosapeAqd。 -[eT]aueug
fdoeqeueab。 - [6t]sueu
Bnane [20] = *m_assist*
fureuopzevgobutddeu"es"esepdn - [tz] euevg
Bnane [22] = *iret*;
Bnane [23] = *vcpu_op′
Bnane [24] - *set_segnent_base*;
Enane[25] - “rnuext_op*
Bnsne [26] = *acm_op*
dotuu, = [z]euev
Bnane [28] = *sched_op*;
Bnane [29] = “callback_op”;
Bnane [30] = *xenoprof_op*;
doterveqouess =[te]eueu
Bnane [32] = *physder_op*
domsg = [ce]eueuB
tracepointixen:xen_nc_entry
8 [Bname [args=>op| ] = count () 
END
clear (Bnane) 
---
## Page 736
16.3 Guest BPF Tools
699
This uses a translation table based on mappings from the kernel source to convert between the
hypercall operation number and a name. This will need to be updated to match your kernel
version, as these mappings change over time
xenhyper(8) can be customized to include such details as the process name or user stack trace that
led to the hypercall, by modifying the @ map keys.
16.3.3 Xen Callbacks
Rather than the guest making a hypercall to the hypervisor, these occur when Xen calls the guest,
such as for IRQ notifications. There are per-CPU counts for these calls in /proc/interrupts:
 grep HYP /pzoc/interrupts
HYP:12156816
9976239
10156992
9041115
7936087
9903434
9713902
877e612Hypexvisor ca1lback 1nterzupta
Each number is the count for one CPU (this is an eight-CPU system). These can also be traced
using BPF, via a kprobe of the kernel function xen_evtchn_do_upcall(. For example, counting
which process is interrupted using bpftrace:
1 : ()uoo =[mo]8 1 Teodnopupqaegex:qoxdx, - soexagdq 
Attaching 1 probe...
°C
6 : [sd]6
[bssh.] : 15
[java]: 71
[svspper/7] : 100
[svappex/3] : 110
[avapper/2] : 130
[ET =[/xeddens]@
e[avspper/0] : 164
[svappex/11: 192
L07 :[9/Iadden2]θ
[svappex/5]: 248
The output is showing that most of the time CPU idle threads (*swapper/**) were interrupted by
the Xen callbacks.
The latency of these can also be measured, for example, using BCC funclatency(8):
funclatency xen_evtchn_do_upca11
Tracing 1 functions for *xen_evtchn_do_upcall*... Hit Ctrl-C to end.
9299U
1  count
diatribution
0 -> 1
: 0
2 > 3
:0
---
## Page 737
700
Chapter 16 Hypervisors
4 -> 7
: 0
8 -> 15
: 0
1.6 > 31
: 0
32 -> 63
: 0
64 > 127
: 0
128 -> 255
: 0
25 6 > 511
: 1
512 -> 1023
: 6
1024 -> 2047
: 131
560b  8191
:365
8192 -> 16383
: 602
16384 -> 32767
: 89
|*****
SESS9  131071
: 1
This shows that, most of the time, processing took between one and 32 microseconds.
More information about the interrupt type can be traced from the child functions of
xen_evtchn_do_upcall(0.
16.3.4 cpustolen
cpustolen(8)² is a bpftrace tool to show the distribution of stolen CPU time, showing whether
time is stolen in short or long runs. This is CPU time unavailable to the guest as it was used by
other guests (which, in some hypervisor configurations, can include CPU time consumed by an
I/O proxy in another domain on behalf of the guest itself, so the term *stolen° is misleading?).
Example output:
 cpustolen.bt
Attach.ing 4 probes.
Tracing stolen CPU tine
^C
estolen_us:
[0]
30384 1869889889886 8869889869869869889889886 8869889869869861
[1]
D 1
[2, 4)
0 1
[4, B]
28I
[8, 16}
41
2 Origin: I developed it for this
on 22-Feb-201.9
red, stolen can
to 16]
---
## Page 738