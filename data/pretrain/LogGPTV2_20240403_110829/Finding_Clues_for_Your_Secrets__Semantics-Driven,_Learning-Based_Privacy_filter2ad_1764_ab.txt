framewoks) [28]. Particularly when it comes to third-party
libraries, what has been found is that many advertising (ad)
libraries aggressively collect user data [30], [38] through dif-
ferent channels (un-protected APIs, privilege-escalation etc.),
disclosing sensitive attributes like age, marriage status and
work information to ad networks or advertisers. These ﬁndings,
however, have been made on a small set of apps, due to the
limitation to manually label and analyze privacy data sources
and the data involved.
As mentioned earlier, automatic leakage analysis tech-
niques have been widely studied, mainly through tracking
“tainted” data ﬂows across app code, from sources (e.g.,
the APIs for collecting GPS locations) to sinks (typically
the APIs for network communication) [16], [23]. A well-
known challenge for such analysis is identiﬁcation of sensitive
data sources, which mainly relies on the Android APIs with
known sensitive returns, such as getLastKnownLocation() for
locations, getLine1Number() for phone number, AccountMan-
ager.getAccounts() for account information, and others. Other
3
sources often need to be labeled manually. To facilitate data-
source identiﬁcation, tools like SUSI [35] can automatically
recover from app code a large number of System APIs likely
to import data. Less clear, however, is whether these APIs
return sensitive information and therefore should be labeled
as data sources. To capture such sensitive inputs, semantics of
the imported content and the context of the related operations
need to be studied. The idea has been used to ﬁnd the sources
on user interfaces, based upon the text content associated
with sensitive user inputs such as “enter user name” and
“password” [32], [25]. Even more challenging here is the
labeling of the private data downloaded from the app’s server
or uploaded from its local repository. For example, when the
user logs into her account, little context information is given
during the importation of account data.
Natural
language processing. ClueFinder leverages a set
of NLP techniques to discover sensitive program elements
and control false positives. Following we describe the key
techniques used in our approach:
Stemming. Stemming is a process that reduces inﬂected (or
sometimes derived) words to their stem, base or root forms:
for example, converting “changes”, “changing” all to the single
common root “change”. In our case, stemming helps us to ﬁnd
more semantic clues, particularly the program elements with
preﬁxes and acronyms in their names: for example, the stem
“addr” derived from “address” can match the variables names
like “user addr”.
Parts-Of-Speech(POS) tagging. POS tagging is a procedure
to mark words as a particular part of a speech, based upon
their meanings and context (relations with other words in
a sentence, such as nouns and verbs). State-of-the-art POS
tagging technique can already achieve over 90% accuracy [29].
Here we use POS tagging to determine whether a privacy-
related keyword is actually a noun in the term or the sentence.
For example, “address” in “address this problem” is a verb,
just describing the action happening to another word, so it is
not likely to represent a physical home address.
Dependency relation parsing. Dependency relation parsing
analyzes a sentence, identiﬁes the grammatic relations between
different words and represents the structure of a sentence,
based upon such pairwise relations, as a dependency tree.
For example In the sentence “Bell, based in Los Angeles,
distributes electronics”, the relation between “Bell” and “dis-
tributes” is described as nominal subject, where “Los” and
“Angeles” represents a compound relation. In our research,
such a dependency relation helps us to determine whether
a speciﬁc privacy-related keyword is the dominator of its
sentence, which is most likely to be the theme of the sentence.
Assumptions. The purpose of ClueFinder is to detect sensitive
data sources from legitimate app code, covering those missed
by all prior studies, particularly program elements related to
the private user data imported from app servers. We do not
consider deeply obfuscated programs that remove all semantic
information from their program elements. Actually, Our study
(see Section IV-B) shows that app developers tend not to
obfuscate data-related code within their apps and the third-
party libraries they integrate to avoid disrupting the apps’
normal executions (e.g., causing a crash). As a result, we found
that even moderately obfuscated code (e.g., through ProGuard)
preserves a lot of semantic information: e.g., among all such
apps discovered in our research, we found that over 50%
of the method names are not obfuscated (Section IV-B) and
over 98% of the apps still contain readable constant strings.
Note that what we are interested in is unauthorized disclosure
of sensitive data within an app to a third-party library, and
therefore malicious apps covertly sending user data to the
adversary are outside the scope of our study.
Further in our measurement study (Section V), we consider
acquisition of sensitive user data by an untrusted third-party
library to be an exposure risk. Even though this exposure
does not necessarily mean that the data will be leaked to an
unauthorized party, often the possibility of the leaks cannot
be eliminated due to the complexity of data-ﬂow analysis on
these libraries.
III. CLUEFINDER DESIGN
As mentioned earlier, although app code is semantics-rich,
recovering truly sensitive data sources from the code is by
no means trivial. Particularly, direct search for keywords does
not work well, which misses many potentially sensitive tokens
(e.g., preﬁxes, acronyms) in the identiﬁers of various program
elements (variable, method, function, etc.). Also importantly,
the presence of a single speciﬁc token does not necessarily
indicates the operations on sensitive user data. In some cases,
even when the token semantically looks perfectly relevant, for
example the method getPhoneNumberPreﬁx, the corresponding
program element may not touch sensitive data at all: in the
example, the function does nothing but a format check on
phone numbers. Thus, a precise semantic analysis is required to
determine whether a token refers to a privacy-related activity.
Further, the program elements carrying sensitive tokens may
not carry private content (e.g., a constant string) themselves.
They need to be linked to the true sources of private data
through program analysis.
In this section, we show how the design of ClueFinder
addresses these challenges and how the technique fares on real
app code.
A. Design Overview
The idea behind ClueFinder is to quickly screen across a
program to locate privacy-related tokens within program ele-
ments and then semantically evaluate the elements to drop false
positives. Finally, a program structure analysis is performed to
determine whether indeed each of these elements is involved
in a sensitive data operation through a method invocation.
Following we describe the architecture of this design and
utilize an example to explain how it works.
Architecture. Figure 3 illustrates the individual components
of ClueFinder and their relations. Our design includes a
Semantics Locator, a Semantic Checker, a Structure Analyzer
and a Leakage Tracker. Given an Android app, Semantics
Locator ﬁrst dissembles its code and identiﬁes the program
elements carrying sensitive tokens. These putative sensitive
elements (string constants, variables, and method names) are
then inspected by Checker, which uses NLP techniques to
determine whether identiﬁed tokens are indeed the main sub-
ject of each element or its identiﬁer’s content (Section III-B).
4
Fig. 3. Design of ClueFinder
Those meeting the standard further go through a program
structural analysis, in which Analyzer classiﬁes each function
invocation statement involving an element as either sensitive
or not. The reported sensitive ones are handled over to Tracker
that incorporates the semantic information inside the app code
into a data-ﬂow and reachable analysis to trace the propagation
of sensitive information.
Example. The example in Figure 4 shows the code snippet
from SnapTee, through which one can design and purchase
personalized T-shirts. Note that here we re-organize part of
the app code for ease of illustration, while maintaining all
semantics in the original (decompiled) app code. As we can
see from line 25 to 29, the app ﬁrst acquires a user’s Facebook
proﬁle (line 26) and then sends a sharing post (line 28) to the
user’s Facebook account. After that, however, both the proﬁle
and the Facebook post are handed over to a third-party library
function trackShareEvent (line 30).
To analyze the snippet,
the Locater leverages a set of
keywords representing 4 categories of sensitive data as iden-
tiﬁed by Google Privacy Policies [3] and prior research [39],
[22], [32], together with their derived preﬁxes and acronyms
through stemming to capture elements like “home addr” (line
6), getUserFbProﬁle (line 26), “I’m designing my own tees
on my phone!” (line 18), which all carry sensitive tokens
“home address”, “proﬁle” and “phone” respectively. The
Checker then looks into these elements, picking out the ones
like getUserFbProﬁle, given the observation that the token it
involves (“Proﬁle”) plays the role of a subject described by
verb “get”. Also, for the long sentence in line 18, the Checker
ﬁnds that “phone” actually not serves the theme (“design
tees”) by the dependency relation parsing, and thus ﬁlters out
this element as there is no indication it carrying private data.
To further determine whether other elements left are indeed
sensitive, their corresponding function invocation statements
(e.g., line 5, 6, 8, 26) are inspected by Structure Analyzer,
based upon their features: for example, statement in line 26 is
a true positive since it returns a Json typed object, which could
contain sensitive data, while the statement in line 5 is a false
positive, since it only returns a boolean value to check whether
the Json object contains a key with a name “home addr”.
All identiﬁed statements are used as sources for a data-
ﬂow and reachable analysis performed by the Tracker. Such
semantics-driven approach helps reduce the complexity of
tracking propagation of sensitive data. For example, conﬁrm a
user proﬁle leakage (a coarse-grained private data) from Line
JsonObject userJson = UserBasicInfo.toJson();
## Gather other user information
If(userJson .contains("home_addr")){
1 ## In co.snaptee.android.utils.FacebookFunctions
2 Json getUserFbProfile(HashMap userBasicInfo) {
3
4
5
6
7
8
9
10
}
this.uri = jsonObject.get("userProfile_uri");
if(this.uri == null) {
jsonObject.put("home_addr", this.homeAddr);
throwNullPointerException("Profile URI is
null", exception);
}
return jsonObject;
11
12
13 }
14
15 Builder shareToFacebook(String shareContent)
16 {
17
18
Builder builder = new Builder();
builder.setContentTitle("I’m designing my own
tees on my phone!");
19
builder.setContentUrl(
Uri.parse("https://snaptee.co/getapp"));
builder.setShareContent(shareContent);
Log.d("FacebookFunctions", "Try to invite FB");
return builder;
20
21
22
23 }
24
25 ## Getting user profile on Facebook
26 currentUser = getUserFbProfile();
27 ## Trigger sharing activity
28 shareToFacebook(shareContent);
29 ## Tracking user activity by invoking API from
third-party library
30 trackShareEvent(currentUser,
builder.shareContent);
Fig. 4. Overview code example
26, without analysing the actual code in this method (Line 2-
13). The process ends when the related data is found to be
accessed by a desired sink (e.g., a third-party library in line
30).
B. Semantic Clue Locating
Semantics Locator. To identify privacy-related data through
the semantics of program elements, we ﬁrst need to determine
the set of data considered to be sensitive and keywords
associated with them. Such information was gathered from
multiple sources in our research. Particularly, we utilized 35
data items identiﬁed by Google Privacy Policies to be private
content [3], together with additional 17 items reported by
5
prior privacy-related research [39], [22], [32]. For example,
Financial Times (FT) [39] provides a calculator for evaluating
the price of one’s private data.
These items are organized by ClueFinder into 4 categories,
including user identiﬁers, user attributes, location data and
account information. In total, 121 keywords or keyword pairs
are identiﬁed (see examples in Table I). Further, we use
Word2Vec [31] to ﬁnd more synonyms of these sensitive items.
Also, the keyword set is extended using stemming (to ﬁnd
out their preﬁxes), with more similar texts extracted from
10,000 popular Google-Play apps (e.g., “addr”). This allows
ClueFinder to capture as much sensitive semantics as possible
from app code.
TABLE I.
KNOWLEDGE BASE FOR PRIVACY-RELATED SEMANTICS
Category
User Attributes
User Identiﬁers
Location
Account
Sample Keywords
ﬁrst name, last name, gender,
birth date, nick name, education,
app list, device os, credit card, etc.
user id, account number, access token,
sina id, facebook id, twitter id, etc.
latitude, longitude, lat, lng, user address,
zip code, city, street, etc.
account name, user name, phone number,
mobile no, password, passwd, pwd etc.
For all elements in decompiled app code, ClueFinder uses
word-splitting to break their names into tokens, using common
delimiters (e.g., user addr) and capitalized letters (e.g., ge-
tUserFbProﬁle). Then, it performs best-effort matching using
its knowledge base (4 data categories and their representing
tokens), searching for the tokens (keywords, preﬁxes and
abbreviations) inside the identiﬁers of program elements. As
a result,
the elements involving privacy-related tokens are
labeled for a more in-depth semantic analysis.
Semantics Locator ﬁnds out the elements with sensitive
tokens in their names. This, however, does not necessarily
mean that these elements are indeed privacy-related. As an
example, in Table II, Index 1, the method getStreetViewActivity
contains the sensitive keyword “street” but is actually unrelated
to the user’s location data. To remove such false positives,
our approach runs Semantic Checker to further analyze the
semantics of these elements.
Semantic Checker. Semantic Checker runs POS tagging and
dependency relation parsing to get more in-depth semantic
information from labeled elements. What we want
to un-
derstand is whether a sensitive token actually serves as the
“theme” of labeled content (element names or the content
of a constant), which is more likely to indicate the presence
of sensitive information, compared with the situation that the
tokens are only used to describe other less nonsensitive terms
(e.g., “street” in getStreetViewActivity). For this purpose, the
Checker tries to determine whether the token is a noun and
also characterized by the following dependency relations with
its context terms in a phrase or a sentence:
• Direct-object relation (Dobj): The direct object of a verb
phrase is the noun phrase that is the (accusative) object of
the verb: e.g., getAddressFromServer in Table II, Index 4.
Here, the identiﬁed sensitive token with a noun POS tagger
(“Address”) has the Dobj relation with (“get”), indicating