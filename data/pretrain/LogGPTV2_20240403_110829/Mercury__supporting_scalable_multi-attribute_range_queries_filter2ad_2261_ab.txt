To guarantee that queries locate all the relevant data-
records, a data-record D, when inserted, is sent to all Hb
where b 2 AD. This is necessary because the set of queries
which could match D can arrive in any of these attribute
hubs. Within each hub, the data-record is routed to the node
responsible for the record’s value for the hub’s attribute.
Notice that we could have ensured correctness by sending
a data-record to a single hub in AD and queries to all hubs in
AQ. At (cid:12)rst glance, this might appear to be a better choice
since data-records could be much bigger in size than queries
and replicating them might be more expensive. However, re-
call that a query can get routed to multiple locations within
each hub depending on its selectivity. This, combined with
the fact that many queries may be extremely non-selective
in some attribute (thereby, (cid:13)ooding a particular hub), led us
to choose a design with data-records broadcast to all hubs.
Concerning the cost of replicating data, we note that it is
not necessary to replicate entire data records across hubs.
In cases where doing so would be prohibitively expensive,
a node within one of the hubs can hold the data record
while the other hubs can hold a pointer to the node. This
would reduces the storage requirements, at the cost of one
additional hop during query resolution.
The above arguments about query workloads and replica-
tion cost notwithstanding, Mercury could be easily modi(cid:12)ed
to support situations where replicating queries is a more ap-
propriate design choice.
Within a hub Ha, routing is done as follows: for routing
a data-record D, we route to the value (cid:25)a(D). For a query
Q, (cid:25)a(Q) is a range. Hence, for routing queries, we route
to the (cid:12)rst value appearing in the range and then use the
contiguity of range values to spread the query along the
circle, as needed.
[0,80)
c
[80,160)
b
[240,
320)
d
H
x
x = 50
y = 150
int
(query)
a
[160,240)
[0,100)
g
(data−item)
int x 100
int y 200
[200, 320)
e
H
y
f
[100,200)
Figure 2: Routing of data-records and queries.
Fig 2 illustrates the routing of queries and data-records.
It depicts two hubs Hx and Hy which may correspond to,
for example, X and Y coordinates of objects. The minimum
and maximum values for the x and y attributes are 0 and
320 respectively. Accordingly, the ranges are distributed to
various nodes. The data-record is sent to both Hx and Hy,
where it is stored at nodes b and e, respectively. The query
enters Hx at node d and is routed (and processed) at nodes
b and c.2
This routing places one additional requirement on the con-
nectivity of each node. In addition to having a link to the
predecessor and successor within its own hub, each node
must also maintain a link to each of the other hubs. We ex-
pect the number of hubs for a particular system to remain
low, and, therefore, do not expect this to be a signi(cid:12)cant
burden. We discuss the maintenance of these links later in
Section 3.4.
Design Rationale
In this section, we discuss some of the promising alterna-
tive designs for implementing a distributed multi-attribute
range-based search and comment qualitatively on the trade-
o(cid:11)s involved.
Many DHTs [18, 25] use a cryptographic hash or random
value to give IDs to nodes and data stored in the DHT. How-
ever, Mercury does not use any such cryptographic hashes or
random values. This simpler mapping of data and nodes in
the system allows the lookup of range predicates in queries
to a collection of contiguous nodes in a hub. We note that
one of the main purposes of using a cryptographic hash in
existing DHTs is to assign data to nodes uniformly and ran-
domly.3 The elimination of this randomness makes load-
balancing in Mercury a concern. Since there are likely to
be particular ranges of an attribute that are more popu-
lar for queries and data-records, nodes responsible for these
ranges from will be unfairly overloaded with both routing
and computation tasks. Mercury performs explicit load bal-
ancing (see Section 4.4) by moving around nodes and chang-
ing their responsibilities according to the loads. This en-
ables the combination of good load-balancing with support
for range predicates. However, one important side e(cid:11)ect is
that the distribution of range sizes is no longer guaranteed
to be uniform.
With the removal of cryptographic hashes, we could have
used a variety of di(cid:11)erent DHTs as the basis for our design.
Our design treats the di(cid:11)erent attributes in an application
schema independently, i.e., routing a data item D within a
hub for attribute a is accomplished using only (cid:25)a(D). An
alternate design would be to route using the values of all at-
tributes present in D, e.g., treating each attribute as a CAN
dimension [22]. Since each node in such a design is responsi-
ble for a value-range of every attribute, a query that contains
a wild-card attribute can get (cid:13)ooded to all nodes. We could
have merged dimensions like in the DIM data structure [16]
but this would still have had similar problems for queries
covering large areas. By making the attributes indepen-
dent, we restrict such (cid:13)ooding to at most one attribute hub.
Furthermore, it is quite likely that some other attribute of
the query is more selective and by routing the query to that
hub, we can eliminate (cid:13)ooding altogether.
3.3 Constructing Ef(cid:2)cient Routes
Recall that most of the routing in Mercury occurs within
an attribute hub (only the (cid:12)rst hop crosses hubs.) Thus, it
2This example uses (cid:13)ooding to route from d to b. Sec. 3.3
introduces long-distance links, which provide a more e(cid:14)cient
routing mechanism.
3Self-certifying names/security, and robustness to correlated
failures are additional valuable properties.
is essential that the overlay structure for each attribute hub
be scalable and e(cid:14)cient.
Simply using successor or predecessor pointers can result
in (cid:18)(n) routing delays for routing data-records and queries.
Like Symphony [18], the key to Mercury’s route optimization
is the selection of k long-distance links that are maintained
in addition to the successor and predecessor links. As a
result, each node has a routing table of size k + 2 including
its neighbors along the circle. k is a con(cid:12)gurable parameter
here and could be di(cid:11)erent for di(cid:11)erent nodes.
The routing algorithm is simple: Let neighbor ni be in-
charge of the range [li; ri), and let d denote the clockwise
distance or value-distance between two nodes. When a node
is asked to route a value v, it chooses the neighbor ni which
minimizes d(li; v). Let ma and Ma be the minimum and
maximum values for attribute a, respectively.
Then,
d(a; b) = (cid:26) b (cid:0) a
(Ma (cid:0) ma) + (b (cid:0) a)
if a (cid:20) b;
if a > b
A node n whose value range is [l; r) constructs its long-
distance links in the following fashion: Let I denote the
unit interval [0; 1]. For each link, a node draws a number
x 2 I using the harmonic probability distribution function:
pn(x) = 1=(n log x) if x 2 [ 1
It contacts a node n0
(using the routing protocol itself) which manages the value
r + (Ma (cid:0) ma)x (wrapped around) in its hub. Finally, it
attempts to make n0 its neighbor. As a practical considera-
tion, we set a fan-in limit of 2k links per node. We will refer
to a network constructed according to the above algorithm
as a ValueLink network.
n ; 1].
Under the assumption that node ranges are uniform, we
can prove (see [18]) that the expected number of routing
k log2 n).
hops for routing to any value within a hub is O( 1
Since inter-hub routing can take at most one hop, the num-
k log2 n) as well.4
ber of hops taken for routing is at most O( 1
This guarantee is based upon Kleinberg’s analysis of small-
world networks [14].
Unfortunately, the \uniform node ranges" assumption can
be easily violated for many reasons. For example, explicit
load-balancing would cause nodes to cluster closely in parts
of the ring which are popular. In the Section 4, we present
a novel distributed histogram maintenance scheme based on
light-weight random sampling to provide e(cid:14)cient routing
even with highly non-uniform ranges.
Caching
For many applications, there can be signi(cid:12)cant locality in
the generated data-items (incremental updates, for exam-
ple) as well as queries (popular searches, for example.) Mer-
cury provides hooks for the application so that it can insert
its own speci(cid:12)c caching behavior into the protocol. Essen-
tially, Mercury allows an application to specify additional
long-distance links that represent cached destinations as an
addendum to the routing table. When looking for the neigh-
bor closest to the destination, Mercury also considers nodes
present in the cache.
3.4 Node Join and Leave
While the above describes the steady-state behavior of
Mercury, it does not address how nodes join or leave the
4For a query, we count the number of routing hops to reach
the (cid:12)rst value in the range it covers.
system. This section describes the detailed protocol used by
nodes during join and departure.
Recall that each node in Mercury needs to construct and
maintain the following set of links: a) successor and prede-
cessor links within the attribute hub, b) k long-distance links
for e(cid:14)cient intra-hub routing and c) one cross-hub link per
hub for connecting to other hubs. The cross-hub link implies
that each node knows about at least one representative for
every hub in the system. In order to recover during node
departures, nodes keep a small number (instead of one) of
successor/predecessor and cross-hub links.
Node Join. Like most other distributed overlays, an incom-
ing Mercury node needs information about at least one (or
at most a few) node(s) already part of the routing system.
This information can be obtained via a match-making server
or any other out-of-band means. The incoming node then
queries an existing node and obtains state about the hubs
along with a list of representatives for each hub in the sys-
tem. Then, it randomly chooses a hub to join and contacts
a member m of that hub. The incoming node installs itself
as a predecessor of m, takes charge of half of m’s range of
values and becomes a part of the hub.
To start with, the new node copies the routing state of its
successor m, including its long-distance links as well as links
to nodes in other hubs. At this point, it initiates two main-
tenance processes: (cid:12)rstly, it sets up its own long-distance
links by routing to newly sampled values generated from
the harmonic distribution (as described above.) Secondly, it
starts random-walks on each of the other hubs to obtain new
cross-hub neighbors distinct from his successor’s. Note that
these processes can be run lazily, as they are not essential
for correctness, and only a(cid:11)ect the e(cid:14)ciency of the routing
protocol.
Node Departure. When nodes depart, the successor/prede-
cessor links, the long-distance links and the inter-hub links
within Mercury must be repaired. To repair successor/pre-
decessor links within a hub, each node maintains a short list
of contiguous nodes further clockwise on the ring than its
immediate successor. When a node’s successor departs, that
node is responsible for (cid:12)nding the next node along the ring
and creating a new successor link.
A node’s departure will break the long-distance links of
a set of nodes in the hub. These nodes establish new long-
distance links to replace the failed ones. Nodes which are not
directly a(cid:11)ected by this departure do not take any action.
The departure of several nodes, however, can distort the dis-
tribution of links of nodes which are not a(cid:11)ected directly. To
repair the distribution, nodes periodically re-construct their
long-distance links using recent estimates of node counts.
Such repair is initiated only when the number of nodes in
the system changes dramatically (by a factor of 2 { either
by addition or departure).5
Finally, to repair a broken cross-hub link, a node consid-
ers the following three choices: a) it uses a backup cross-hub
link for that hub to generate a new cross-hub neighbor (us-
ing a random walk within the desired hub), or b) if such a
backup is not available, it queries its successor and predeces-
sor for their links to the desired hub, or c) in the worst case,
the node contacts the match-making (or bootstrap server)
5Intuitive justi(cid:12)cation: routing performance is only sensitive
to the logarithm of the number of nodes.
to query the address of a node participating in the desired
hub.
4. EFFICIENCY IN THE FACE OF
NON›UNIFORMITY
The Mercury protocol we have described thus far is largely
a derivative of previous structured overlay protocols. We
have shown that it can provide e(cid:14)cient (logarithmic) routing
when the responsibility of handling various attribute values
is uniformly distributed to all nodes within a hub. However,
as alluded to in Section 3.2, the desire to balance routing
load can create a highly non-uniform distribution of ranges.
We begin this section by analyzing why such non-uniform
range distributions con(cid:13)ict with the goal of e(cid:14)cient routing.
We (cid:12)nd that Kleinberg’s basic small-world network result
makes certain assumptions which are non-trivial to satisfy in
a distributed setting when node ranges in a network are non-
uniform. We then present a novel algorithm which ensures
e(cid:14)cient routing even when the assumptions are violated.
We then tackle non-uniformity in two other dimensions:
query selectivity, and data popularity. We show how the
core of the algorithm for e(cid:14)cient routing under non-uniform
range distributions can be re-used to optimize query perfor-
mance given non-uniformity in query selectivity and data
popularity.
4.1 Small›world Networks
Let G represent a circle on n nodes. De(cid:12)ne node-link
distance dn(a; b) between two nodes a and b as the length
of the path from a to b in the clockwise direction. The
objective is to (cid:12)nd \short" routes between any pair of nodes
using distributed algorithms. Kleinberg [14] showed that if
each node, A, in G constructs one additional \long-link" in
a special manner, the number of expected hops for routing
between any pair of nodes becomes O(log2 n). Each node
A constructs its link using the following rule: A generates
an integer x 2 (0; n) using the harmonic distribution, viz.,
hn(x) = 1=(n log x), and establishes a link to the node B
which is x links away in the clockwise direction from A. The
routing algorithm for each node is to choose the link which
takes the packet closest to the destination with respect to
the node-link distance. Symphony [18] extends this result
by showing that creating k such links reduces the routing
hop distance to O( 1
Creating the long-links appears deceptively straightfor-
ward. However, it may be di(cid:14)cult and expensive (O(x)) for
a node A to determine which node, B, is x hops away from
it. Contacting node B would be simpler if we could eas-
ily determine what value range B was responsible for. This
would allow the use of any existing long-links to contact this
node more e(cid:14)ciently and reduce the number of routing hops
to O(log2 n)=k.
In systems like Symphony, this problem is solved by ap-
proximating the hop distance of any node. Since Symphony
places nodes randomly along its routing hub, it makes the
assumption that all nodes are responsible for ranges of ap-
proximately the same size, r. By simply multiplying r by
x and adding to the start of the values range of A, Sym-
phony is able to estimate the start of the range that B is
responsible for. Unfortunately, this technique does not work
when not all nodes are responsible for the same range size
of values, i.e., when ranges are highly non-uniform in size.
k log2 n).
Non-uniform range sizes, however, are exactly what we use
in order to provide load balancing. Speci(cid:12)cally, our load bal-
ancing algorithm, which we elaborate in Sec. 4.4, balances
load by increasing the density of nodes in the portions of the
value range that are heavily loaded. Accordingly, the value
ranges for nodes in heavily loaded regions are smaller than
the value ranges for nodes in lightly loaded regions.
To enable the e(cid:14)cient creation of long-links over an over-
lay with non-uniform range sizes, we introduce a novel ran-
dom sampling algorithm (Sec. 4.2), and use this sampling
algorithm to create an estimate of the density of nodes in
di(cid:11)erent parts of the routing hub, i.e., a histogram of the
distribution of nodes (Sec. 4.2.1). This allows us to easily
map from x (the desired length of a long-link) to the start of
the value range for B (the node at distance x). This map-
ping, in turn, enables us to construct the long-distance links
of Section 3.3 despite non-uniform node ranges.
4.2 Random Sampling
Maintaining state about a uniformly random subset of
global participants in a distributed network, in a scalable,
e(cid:14)cient and timely manner is non-trivial. In the context of
our system, the na(cid:127)(cid:16)ve approach of routing a sample request
message to a randomly generated data-value works well only
if node ranges are uniformly distributed. Unfortunately, as
already explained, this assumption is easily violated.
Another obvious approach is to assign each node a random
identi(cid:12)er (by using a cryptographic hash, for example) and
route to a randomly generated identi(cid:12)er to perform sam-
pling. However, in order for the sampling process to be
e(cid:14)cient, we need a routing table for delivering messages
to node identi(cid:12)ers. Another approach is to use protocols
like Ransub [15] which are speci(cid:12)cally designed for deliver-
ing random subset information. Unfortunately, both these
approaches require incurring the overhead of maintaining
a separate overlay { one which may not be well suited for
e(cid:14)cient data-value routing.
Mercury’s approach for sampling is novel { we show that
the hub overlay constructed by Mercury in a randomized
manner is an expander [19] with a high probability. An ex-