2136
(e) Number of bugs reached in md5sum (f) Number of bugs triggered in md5sum
(g) Number of bugs reached in who
(h) Number of bugs triggered in who
Fig. 8: Evaluation results with LAVA-M. The left column
shows the number of Lava bugs reached by different fuzzers
and the right column shows the number of LAVA bugs
triggered by the fuzzers. For TFUZZ, we only present the
number of triggered bugs in base64 and uniq, as the other
results are not reliable due to a broken third-party dependency.
because of a broken dependency 3. For all the cases, ANGORA
triggers the vulnerabilities immediately after its start. The
main reason is that the “black-box function” pertaining to all
LAVA vulnerabilities is f(x) = x and the triggering condi-
tions are like f(x) == CONSTANT. ANGORA always starts
evaluating such functions with x = CONSTANT and hence,
it can instantly generate seeds that satisfy the vulnerability
conditions. In the case of who, ANGORA does not ﬁnd all
the vulnerabilities because of its incomplete dynamic taint
analysis.
3The broken component is the QEMU based tracer in Angr [4]. This has
been conﬁrmed with the developers.
TABLE IV: LAVA-M Bugs triggered by different fuzzers (after
bug-guided veriﬁcation). “X%” indicates that X% of the listed
LAVA bugs are triggered.
Regarding the three hybrid tools, they trigger every vulner-
ability that their concolic executors encounter. In the cases of
base64, uniq, and md5sum, their concolic executors can
reach all the vulnerabilities with initial seeds. This explains
why they all quickly trigger the listed vulnerabilities, regard-
less of their seed scheduling.
In the case of who, even though the fuzzing component
quickly generates seeds to cover the vulnerable code,
the
concolic executor takes much longer to run those seeds.
For instance, while executing the inputs from AFL, QSYM
needs over 72 hours of continuous concolic execution to
reach all the LAVA bugs in who. Differing from DRILLER
and QSYM, SAVIOR prioritizes seeds that have a higher
potential of leading to Lava bugs. As demonstrated by the
results of who in Table III, our technique of bug-driven
prioritization indeed advances the exploration of code with
more vulnerabilities. Note that DRILLER (with a random seed
scheduling) moves faster than QSYM. This is because QSYM
prioritizes concolic execution on small seeds, while reaching
the vulnerabilities in who needs seeds with a larger size.
Vulnerability Finding Thoroughness: We further evaluate
our bug-guided veriﬁcation design. Speciﬁcally, we run the
seeds generated by all the fuzzers with our concolic executor.
In this experiment, we only perform constraint solving when a
vulnerability condition is encountered. As shown in Table IV,
bug-guided veriﬁcation facilitates all the fuzzers to not only
cover the listed LAVA bugs but also disclose an extra group of
Lava bugs. Due to limited space, those additionally identiﬁed
bugs are summarized in Table X at Appendix. Such results
strongly demonstrate the promising potential of bug-guided
veriﬁcation to beneﬁt fuzzing tools in vulnerability ﬁndings.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:09 UTC from IEEE Xplore.  Restrictions apply. 
1588
024681012141618202224Time (hour)01020304050# of reached lava bugsAFLAFLGOANGORADRILLERQSYMSAVIOR024681012141618202224Time (hour)01020304050# of triggered lava bugsAFLAFLGOANGORATFUZZDRILLERQSYMSAVIOR024681012141618202224Time (hour)051015202530# of reached lava bugsAFLAFLGOANGORADRILLERQSYMSAVIOR024681012141618202224Time (hour)051015202530# of triggered lava bugsAFLAFLGOANGORATFUZZDRILLERQSYMSAVIOR024681012141618202224Time (hour)0102030405060# of reached lava bugsAFLAFLGOANGORADRILLERQSYMSAVIOR024681012141618202224Time (hour)0102030405060# of triggered lava bugsAFLAFLGOANGORADRILLERQSYMSAVIOR024681012141618202224Time (hour)05001000150020002500# of reached lava bugsAFLAFLGOANGORADRILLERQSYMSAVIOR024681012141618202224Time (hour)0500100015002000# of triggered lava bugsAFLAFLGOANGORADRILLERQSYMSAVIORPrograms
Name
libpcap
libtiff
libtiff
binutils
binutils
libxml2
libjpeg
jasper
Version
4.9.2/1.9.0
4.0.10
4.0.10
2.31
2.31
2.9.7
9c
master
Driver
tcpdump
tiff2ps
tiff2pdf
objdump
readelf
xmllint
djpeg
jasper
Source
[20]
[12]
[12]
[8]
[8]
[13]
[11]
[9]
Seeds
build-in
AFL
AFL
AFL
AFL
AFL
AFL
AFL
Settings
Options
-r @@
@@
@@
-D @@
-A @@
@@
-f @@ -T pnm
TABLE V: Real-world benchmark programs and evaluation
settings. In the column for Seeds, AFL indicates we reuse
the testcases provided in AFL and build-in indicates that
we reuse the test cases shipped with the program.
B. Evaluation with Real-world Programs
1) Experimental Setup: In this evaluation, we prepare 8
programs. Details about these programs and the test settings
are summarized in Table V. All
the programs have been
extensively tested by both industry [16] and academic re-
searches [57, 66, 73]. Since different seed inputs and execution
options could lead to varying fuzzing results [49, 58], we
follow existing works to use the seeds shipping with AFL
or the vendors, as well as to conﬁgure the fuzzing options.
Similar to our evaluation with LAVA-M, we conduct all the
experiments on Amazon EC2 instances. To reduce randomness
during testing, we run each test 5 times and report the average
results. In addition, we leverage Mann Whitney U-test [53] to
measure the signiﬁcance of our improvements, following the
suggestion by George etc [49].
In this evaluation, we also prepare the setups that are
speciﬁc to each fuzzing tool. These setups mostly follow
Table II except the following. First, we use UBSan labels as
the target locations for AFLGO and as the guidance of bug-
driven prioritization in SAVIOR. Second, to prevent ANGORA
from terminating the fuzzing process once it encounters un-
instrumented library functions, we follow suggestions from
the developers and add the list of un-instrumented func-
tions into ANGORA’s dfsan_abilist.txt conﬁguration
ﬁle. Third, we do not include TFUZZ, because it does not
function correctly on our benchmark programs due to issues
in the aforementioned third-party component. Furthermore,
we prepare these benchmark programs such that
they are
instrumented with UBSan for all fuzzers to ensure a fair
comparison. This also means that bug-guided veriﬁcation is
enabled by default in DRILLER, QSYM, and SAVIOR.
2) Evaluation Results: In Figure 9, we summarize the re-
sults of our second experiment. It shows the outputs over time
from two metrics, including the number of triggered UBSan
bugs and basic block coverage. In addition, we calculate the
p-values for Mann Whitney U-test of SAVIOR vs. DRILLER
and SAVIOR vs. QSYM. Note that we use the IDs of UBSan
labels for de-duplication while counting the UBSan bugs, as
each UBSan label is associated with a unique potential defect.
In the following, we delve into the details and explain how
these results testify our design hypotheses.
Vulnerability Finding Efﬁciency: As shown in Figure 9
(the left column of each program), SAVIOR triggers UBSan
violations with a pace generally faster than all
the other
fuzzers. In particular, it outperforms DRILLER and QSYM in
all the cases except djpeg. On average, SAVIOR discovers
vulnerabilities 43.4% faster than DRILLER and 44.3% faster
than QSYM. The low p-values (< 0.05)4 of Mann Whitney
U-test well support that these improvements are statistically
signiﬁcant. Since the three hybrid tools only differ in the way
of seed scheduling, these results strongly demonstrate that the
scheduling scheme in SAVIOR— bug-driven prioritization —
accelerates vulnerability ﬁnding. In the case of djpeg, all
six fuzzers trigger the same group of UBSan violations. This
is because djpeg has a tiny code base, with which these
fuzzers quickly saturate on code exploration. In addition, the
conditions of those UBSan violations are simple that even
mutation-based approaches can solve. For a better reference,
we also summarize the number of triggered violations at the
end of 24 hours in Table XII at Appendix A-D.
Going beyond, we examine the number of labels that
are reached by different fuzzers. In Table VI, we list the
average results from our 24-hour tests. Not surprisingly, the
hybrid tools cover higher volumes of UBSan labels than the
ordinary fuzzers. This is likely because a hybrid tool can
solve complex conditions, enabling the coverage on the code
and labels behind. Among the hybrid tools, SAVIOR reaches
19.68% and 15.18% more labels than DRILLER and QSYM,
respectively. Such results are consistent with the number of
triggered UBSan violations. This also signiﬁes that our bug-
driven prioritization guides SAVIOR to spend more resources
on code with richer UBSan labels. In the case of djpeg,
SAVIOR nearly ties with the other tools. This is due to a
similar reason as explained above.
We further ﬁnd that the efﬁciency boost of SAVIOR in vul-
nerability ﬁnding is not due to high code coverage. As shown
in Figure 9 (the right column for each program), we compare
the code coverage of the six fuzzers. As demonstrated by the
results, the efﬁciency of code coverage and UBSan violation
discovery are not positively correlated. Particularly, in the case
of tcpdump, libxml, tiff2pdf, objdump and jasper,
SAVIOR covers code in a similar or even slower pace than
DRILLER and QSYM (the high p-values also support that
SAVIOR is not quicker). However, SAVIOR triggers UBSan
violations signiﬁcantly quicker in these cases. Such results
validate the above hypothesis with high conﬁdence.
Vulnerability Finding Thoroughness: In this experiment, we
also measure the performance of bug-guided veriﬁcation in en-
hancing the thoroughness of vulnerability ﬁnding. Speciﬁcally,
we re-run the seeds from all the fuzzers with our concolic
executor. In this test, we enable SAVIOR to do constraint
solving only when encountering un-solved UBSan labels.
In Table VII, we summarize the comparison results. For all
the 8 programs, bug-guided veriﬁcation facilitates different
4The p-values of readelf and objdump are larger than 0.05 but they
are at the level of quasi-signiﬁcance. In the two programs, the variances are
mainly due to randomness.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:09 UTC from IEEE Xplore.  Restrictions apply. 
1589
(a) Number of UBSan vio-
triggered in tcpdump
lations
(p1=0.029, p2=0.047).
(b)
blocks
(p1=0.106,p2=0.999).
Number
reached
of
in
basic
tcpdump
(c) Number of UBSan violations
triggered in tiff2ps (p1=0.005,
p2=0.046).
(d) Number of basic blocks
reached in tiff2ps (p1=0.049,
p2=0.073).
(e) Number of UBSan violations
triggered in readelf (p1=0.098,
p2=5.63 ∗ e−5).
(f) Number of basic blocks
reached in readelf (p1=0.042,
p2=0.726).
(g) Number of UBSan violations
triggered in libxml (p1=7.04 ∗
e−5, p2=2.15 ∗ e−7).
(h)
of
blocks
in
(p1=0.042,p2=0.094).
Number
reached
basic
libxml
Number
(i)
violations
(p1=0.777,p2=0.203).
UBSan
triggered in djpeg
of
Number
(m)
violations
(p1=0.010,p2=0.002).
UBSan
triggered in jasper
of
(j) Number of basic blocks
reached in djpeg (p1=3.28 ∗
e−7,p2=3.79 ∗ e−6).
(k) Number of UBSan vi-
triggered in tiff2pdf
olations
(p1=0.002,p2=3.95 ∗ e−6).
(n)
of
blocks
in
(p1=0.287,p2=0.653).
Number
reached
basic
jasper
(o) Number of UBSan vio-
triggered in objdump
lations
(p1=0.096, p2=0.003).
(l)
blocks
(p1=0.009,p2=0.807).
Number
reached
of
in
basic
tiff2pdf
(p)
blocks
(p1=0.0001, p2=0.125).
Number
reached
of
in
basic
objdump
Fig. 9: Evaluation results with real-world programs. Each program takes two columns, respectively showing the number of
triggered UBSan violations and the amount of covered basic blocks by the fuzzers over 24 hours. p1 and p2 are the p-values
for the Mann Whitney U-test of SAVIOR vs. DRILLER and SAVIOR vs. QSYM, respectively.
fuzzers to trigger new violations. The average increase ranges
from 4.5% (SAVIOR) to 61.2% (ANGORA). In particular, it
aids ANGORA to trigger 82 new UBSan bugs in total. In the
case of djpeg bug-guided veriﬁcation does not help much.
This is because djpeg has a relatively smaller code base
and contains fewer vulnerability labels, making bug-guided
veriﬁcation less utilized. These results are further evidence
that bug-guided veriﬁcation can truly beneﬁt fuzzing in terms
of vulnerability ﬁnding thoroughness.