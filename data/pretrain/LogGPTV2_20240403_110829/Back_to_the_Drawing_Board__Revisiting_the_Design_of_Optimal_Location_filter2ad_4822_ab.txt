dQ(x, z) .
Q+(f , π) = max
x,z
π(x)>0
f (z|x)>0
The worst-case loss measures how much utility the user loses in
the worst case possible. For example, if dQ(x, z) is the Euclidean
distance and the user wants to query about x, a mechanism with
Q+(f , π) ≤ 2km ensures that the output z will not be further
than 2km away from x. This property is very helpful for many
applications that target nearby-type of services, since if the reported
location is very far from the desired location then the result of the
query would be generally useless for the user.
2.2 Privacy Metrics
We present now three notions of privacy: the average adversary
error, the conditional entropy of the posterior distribution, and
geo-indistinguishability.
Average Error. The average error is the de-facto standard to mea-
sure location privacy since Shokri et al. [26] argued that incorrect-
ness determines the privacy of users. Consider that the adversary
knows the prior π and the mechanism f chosen by the user. With
this information, she produces an estimate ˆx ∈ ˆX of the user’s input
location x. The choice of ˆX depends on the computational power
of the adversary. Since we assume that the user has the freedom to
report any location in R2, we also assume an unbounded adversary
that can estimate locations on the whole world ˆX = R2. Upon
observing z, the adversary can build a posterior probability mass
function over the inputs, denoted as p(x|z):
π(x) · f (z|x)
(3)
Let dP(x, ˆx) be a function that quantifies the magnitude of the
adversary’s error when deciding that the input location was ˆx
when the input location is actually x. As in the case of the average
loss Q, this function dP(·) does not necessarily need to be a metric
(e.g., it can include the user sensitivity to an adversary learning
semantic information such as in [1]). Given an output location z,
the optimal decision for the adversary in terms of minimizing the
average error is
x′∈X π(x′) · f (z|x′) .
p(x|z) =

ˆx(z) = argmin
ˆx ∈R2
p(x|z) · dP(x, ˆx)
.
(4)
ˆx optimally given each observed z. Let fZ(z) =
The average adversary’s error, or just average error, is defined as
the mean error incurred by an adversary that chooses the estimation
x ∈X π(x) · f (z|x)
be the probability density function of z. Then, the average error is:
(5)
p(x|z) · dP(x, ˆx(z))dz
PAE(f , π) =
(cid:40)
x ∈X
(cid:41)
(cid:41)
∫
R2 fZ(z)
(cid:40)
∫
x ∈X
min
ˆx ∈R2
R2
x ∈X
=
π(x) · f (z|x) · dP(x, ˆx)
dz .
(6)
Note that mechanisms designed with PAE inherently protect against
a strategic adversary, since the metric embeds the adversary’s esti-
mation. This metric has been used as part of the design objective
in previous works [26, 27], and as a way of comparing the per-
formance in terms of privacy of mechanisms designed with other
different privacy goals in mind [2, 5–7].
Conditional Entropy. The conditional entropy is an information-
theoretic metric that can be used to measure the adversary’s un-
certainty about the user’s real location when z is released. After
observing z, the adversary builds the posterior p(x|z) using (3). The
uncertainty of the adversary regarding the value of x given z can
(7)
(cid:19)
be measured as the entropy of this posterior:
p(x|z) · log(p(x|z)) .
H(x|z) (cid:17) −
∫
R2 fZ(z) · H(x|z)dz ,
PCE(f , π) =
x ∈X
The conditional entropy measures the average entropy of the pos-
terior after z is released. Formally,
π(x) · f (z|x)
(8)
where fZ(z) is the probability density function of z, and H(x|z) is
a function of z as defined in (7). Alternatively, using only the prior
π and the mechanism f , the conditional entropy can be written as
PCE(f , π) = −
∫
R2 π(x)·f (z|x)·log
(cid:18)

x ∈X
x′∈X π(x′) · f (z|x′)
dz .
(9)
Note that this metric does not depend on the geography of the
problem, i.e., on the particular values of x or z. If we use the base-
two logarithm in the formula, then PCE can be interpreted as how
many bits of information the adversary needs on average to com-
pletely identify x. This metric was disregarded as a possible privacy
metric in [26] due to being uncorrelated with the average error. In
this work, we challenge such conclusion showing that considering
solely the correctness of the adversary may lead to the design of
mechanisms that offer low privacy. We show in Section 4 how using
the conditional entropy as a complementary privacy metric helps
to avoid choosing those undesirable mechanisms.
∫
∫
Geo-Indistinguishability. Geo-indistinguishability is an exten-
sion of the concept of differential privacy, originally a notion of
privacy in databases, to the location privacy scenario. It was origi-
nally proposed in [2] and other works have continued the research
on this line [5–7]. Formally, ϵ-geo-indistinguishability requires the
following condition to be fulfilled by a location privacy-preserving
mechanism f ,
A
f (z|x
′)dz , ∀x, x
f (z|x)dz ≤ eϵ ·dP(x,x′) ·
′ ∈ X ,∀A ⊆ R2
.
A
(10)
This requirement ensures that given an area A ⊆ R2, the probability
of reporting a point z in that area if the original location was x over
any other location x′ within some distance around x, is similar, and
therefore x and x′ have some degree of statistical indistinguisha-
bility. In this definition, dP(x, x′) is a function that quantifies how
indistinguishable x and x′ are: smaller values of dP(x, x′) indicate
a higher indistinguishability, as the constraint becomes tighter. The
privacy parameter in this definition is ϵ: larger values of ϵ indicate a
looser constraint that allows f (z|x) and f (z|x′) to be more different,
and therefore x and x′ become more distinguishable. Smaller values
of ϵ force the probability density functions f (z|x) and f (z|x′) to
be closer, providing more privacy. Note that, if for a single input
location x there is a positive probability of reporting the output in a
A f (z|x)dz > 0, then that must also be true for ev-
ery other input location x′. Also, note that geo-indistinguishability
is independent of the prior π.
The typical choice of dP(x, x′) in geo-indistinguishability is the
Euclidean distance [2, 5]. Many geo-indistinguishability mecha-
nisms rely on the fact that dP(x, x′) is a metric (specifically, in the
region A ⊆ R2,∫
fact that it satisfies the triangular inequality dP(x, x′) ≤ dP(x, z) +
dP(x′, z)) to prove that they meet the condition in (10).
Although geo-indistinguishability is generally considered a pri-
vacy guarantee and not itself a metric, we can adapt it to represent
an equivalent concept to our generic metric P(f , π). Given a mech-
anism that provides ϵ-geo-indistinguishability, it is straightforward
to see that it is also ϵ′-geo-indistinguishable if ϵ′ > ϵ. Since a
smaller ϵ denotes more privacy, it makes sense to define the geo-
indistinguishability level provided by a mechanism f according to
the smallest ϵ it guarantees. Also, since we are defining P(f , π) as a
magnitude that grows with the protection of the users, we choose
to define our measure of geo-indistinguishability, PGI(f ), as the
inverse of the smallest ϵ guaranteed by the mechanism. Given the
mechanism f , we write
(cid:12)(cid:12)(cid:12)(cid:12)log f (z|x)
f (z|x′)
(cid:12)(cid:12)(cid:12)(cid:12)−1
,
(11)
PGI(f ) = inf
x,x′∈X
z∈R2
dP(x, x
′) ·
0) = 0 and that dP(x, x′) =
where we assume by convention that log( 0
||x − x′||2 is the Euclidean distance. Larger values of PGI indicate
more privacy, and the mechanism guarantees 1/PGI-geo-indistingui-
shability.
3 LIMITATIONS OF THE EXPECTED
ADVERSARY ERROR BASED EVALUATION
The most standard way to assess the location privacy provided by
two mechanisms has been the evaluation of the trade-off between
their average adversary error PAE and their average loss Q. The use
of the average error as yardstick for location privacy was proposed
in [26] under the general notion of correctness, and its use as a way
of comparing mechanisms was followed by many of the subsequent
works [1, 2, 5–7, 27]. The choice of distance functions dP(·) and
dQ(·) for both the average error and the average loss in these works
is mostly the Euclidean distance [1, 2, 5, 6, 27] although some of
them also consider the Hamming distance [5, 26, 27] or semantic
distances for privacy [1, 7].
In this section, we show the problems that stem from this estab-
lished 2-dimensional evaluation approach. We start by studying the
properties of mechanisms that are optimal according to these two
metrics. Then, we introduce a new mechanism that we call the coin
mechanism, and use it as an example that brings to light the flaws
of judging the privacy of a mechanism by its performance in terms
of average error and average loss.
3.1 Study of the Established Mechanism
Evaluation
We start our analysis by assuming that the choice of distance func-
tions dP(·) and dQ(·) is the same for simplicity, which is a typical
choice in related works (e.g., both are the Euclidean distance). We
denote this by dP(·) ≡ dQ(·). At the end of the section, we argue
what happens when this is not the case. We also introduce two def-
initions. First, let FQ be the set of all the mechanisms that achieve
an average loss smaller or equal than Q. Formally,
FQ (cid:17)(cid:110)
f | Q(f , π) ≤ Q(cid:111)
.
(12)
Q ⊆ FQ be the set of all mechanisms f ∈ FQ that are
Q (cid:17)(cid:8)f | f ∈ FQ , PAE(f , π) ≥ PAE(f
Also, let F opt
optimal in terms of average adversary error, i.e.,
, π) ∀f
F opt
We call a mechanism inside F opt
optimal, since it achieves as much
privacy as possible among all the mechanisms with the same quality
loss. We state the following lemma:
′ ∈ FQ(cid:9) .
(13)
Q
′
Lemma 3.1. The set of optimal mechanisms with respect to the
average privacy PAE and the average loss Q is a convex polytope.
Proof. Let the privacy achieved by any mechanism in F opt
be
Q
Popt(Q). Then, we can define this set as
Q = { f | PAE(f , π) = Popt(Q), Q(f , π) ≤ Q} ,
F opt
(14)
and since PAE(f , π) and Q(f , π) are linear operations with f , (14)
can be written as an intersection of half-spaces, which forms a
convex polytope.
□
Note that the proof also applies to the case where dP(·) (cid:46)
dQ(·) (e.g., privacy as the average Hamming error of the adver-
sary and quality loss as the average Manhattan distance). The
same outcome can be derived for the conditional entropy and geo-
indistinguishability, although we leave those results out of the scope
of this work.
This lemma shows that there is a family of optimal mechanisms
that lie inside a convex polytope, instead of just a single mechanism.
All of them provide the same (maximal) privacy for the same quality
loss constraint so, in principle, they are equally useful. In what
follows, we show why this is not the case.
We start by introducing the concept of remapping. A remapping
д is a function д : R2 → R2 that maps an output z ∈ R2 to another
output z′ ∈ R2 according to the probability density function д(z′|z).
It is well known that if we generate a mechanism f ′ = f ◦ д =
R2 д(z′|z) · f (z|x)dz, then the privacy of f ′ in terms of average
error, conditional entropy or geo-indistinguishability is not smaller
than that of f . This is reasonable, as the remapping д is independent
from x, and thus it does not reveal any information about it. The
optimal Bayesian remapping is defined as follows:
∫
Definition 3.2 (Optimal remapping). Given a mechanism f , its
optimal remapping is the one that minimizes the average loss of
the composition f ′ = f ◦ д, i.e., д(z′|z) = δ(z′ − r(z)), where
r(z) = argmin
z′∈R2
π(x) · f (z|x) · dQ(x, z
′) .
(15)

x ∈X
This remapping assigns each location z to the location r(z) in
(15), and is used in [6] as a way of improving the utility of geo-
indistinguishability mechanisms. Now, we show that it can also
be used not only to reduce the quality loss of mechanisms but to
achieve optimal mechanisms in terms of average error privacy:
Theorem 3.3. Let д be an optimal remapping for mechanism f ,
and let f ′ be the composition f ′ = f ◦ д. If dP(·) ≡ dQ(·), then f ′ is
an optimal mechanism, i.e., f ′ ∈ F opt
Q(f ′,π).
The proof is provided in the Appendix.
This theorem provides a straightforward way of building an
optimal mechanism f ′ from any mechanism f . The idea is to reas-
sign each output z of f to another symbol z′ such that the average
quality loss is minimized. Doing this for every output ensures that
the quality loss cannot be further reduced, and since the distance
function used to evaluate quality loss and privacy is the same, the
best estimation the adversary can do of x is just to keep the re-
leased value. Note that the Q(f ′, π) ≤ Q(f , π). This means that,
in order to find an optimal mechanism f ′ for a target quality loss
Q(f ′, π) = Q using the remapping strategy, one has to adjust the
loss of the mechanism f (e.g., by tuning its variance if it is a noise
mechanism) until f ′ achieves the desired average loss Q.
mechanism f is just doing nothing, then it means f is optimal:
It is straightforward to see that, if the optimal remapping for a
Corollary 3.4. If the optimal remapping in (15) for a mechanism
f is д(z′|z) = δ(z′ − z), then f is optimal for its quality loss Q, i.e.,
f ∈ F opt
Q .
This is a very convenient way of proving the optimality of a
mechanism when dP(·) ≡ dQ(·). Another way of seeing that such
mechanism is optimal, is by realizing that with this choice of met-
rics, the privacy is upper bounded by the quality loss PAE(f , π) ≤
Q(f , π), and the upper bound is indeed achieved when an optimal
mechanism is used. We note that the fact that PAE(f , π) = Q(f , π)
for optimal mechanisms is not new, as it was already mentioned in
[2] about the mechanisms in [27].
3.2 The Coin Mechanism and the Flaws of the
Traditional Approach