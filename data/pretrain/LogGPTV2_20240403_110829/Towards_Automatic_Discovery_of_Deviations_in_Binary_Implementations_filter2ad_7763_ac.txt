will be translated as:
(assume x0 < 0; x1 = x0 − 1; )2
(assume ¬(x0 < 0); x1 := x0 + 1; )
The above allows calculating the WP over multiple
paths (we discuss multiple paths in Section 6). In our
setting, we only consider a single path. For each branch
condition e evaluated in the trace, we could add the GCL
statement assert e if e evaluated to true (else assert ¬e
if e evaluated to false). In our implementation, using as-
sert in this manner is equivalent to adding a clause for
each branch predicate to the post-condition (e.g., making
the post-condition e ∧ Q when e evaluated to true in the
trace).
Step 4c: Computing the weakest precondition. We
compute the weakest precondition for Bg from the pre-
vious step in a syntax-directed manner. The rules for
computing the weakest precondition are shown in Ta-
ble 2. Most rules are straightforward, e.g., to calcu-
1The GCL deﬁnes a few additional commands such as a do-while
loop, which we do not need.
late the weakest precondition wp(A; B, Q), we calculate
wp(A, wp(B, Q)). Similarly wp(assume e, Q) ≡ e ⇒
Q. For assignments lhs := e, we generate a let expres-
sion which binds the variable name lhs to the expression
e. We also take advantage of a technical transformation,
which can further reduce the size of the formula by using
the single assignment form from Step 4a [19, 29, 36].
3.1.3 Memory Reads and Writes to Symbolic Ad-
dresses
If the instruction accesses memory using an address that
is derived from the input, then in the formula the address
will be symbolic, and we must choose what set of possi-
ble addresses to consider. In order to remain sound, we
add a clause to our post-condition to only consider execu-
tions that would calculate an address within the selected
set. Considering more possible addresses increases the
generality of our approach, at the cost of more analysis.
Memory reads. When reading from a memory loca-
tion selected by an address derived from the input, we
must process the memory locations in the set of ad-
dresses being considered as operands, generating any ap-
propriate initialization statements, as above.
We achieve good results considering only the address
that was actually used in the logged execution trace
and adding the corresponding constraints to the post-
condition to preserve soundness.
In practice, if useful
deviations are not found from the corresponding formula,
we could consider a larger range of addresses, achieving
a more descriptive formula at the cost of performance.
We have implemented an analysis that bounds the range
of symbolic memory addresses [2], but have found we
get good results without preforming this additional step.
Memory writes. We need not
transform writes to
memory locations selected by an address derived from
the input. Instead we record the selected set of addresses
to consider, and add the corresponding clause to the post-
condition to preserve soundness. These conditions force
the solver to reason about any potential alias relation-
ships. As part of the weakest precondition calculation,
subsequent memory reads that could use one of the ad-
dresses being considered are transformed to a conditional
statement handling these potential aliasing relationships.
As with memory reads, we achieve good results only
considering the address that was actually used in the
logged execution trace. Again, we could generalize the
formula to consider more values, by selecting a range of
addresses to consider.
USENIX Association
16th USENIX Security Symposium
219
A,B ∈ GCL stmt
::= lhs := e
| A;B
| assume e (e is an expression)
| assert e (e is an expression)
| A 2 B
| skip
GCL stmt wp(stmt, Q)
assume e
assert e
lhs := e
A; B
A 2 B
e ⇒ Q
e ∧ Q
let lhs = e
wp(A, wp(B,Q))
wp(A, Q) ∧ wp(B,Q)
Table 2: The guarded command language (left), along with the corresponding weakest precondition predicate trans-
former (right).
1(cid:13)
2(cid:13)
3.3 Validation Phase
Finally, we validate the generated candidate deviation in-
puts to determine whether they actually result in seman-
tically different output states in the two implementations.
As illustrated in Figure 2, it is possible that while an in-
put does not satisfy the symbolic formula generated for
a server, it actually does result in an identical or seman-
tically equivalent output state.
We send each candidate deviation input to the imple-
mentations being examined, and compare their outputs to
determine whether they result in semantically equivalent
or semantically different output states.
In theory, this testing requires some domain knowl-
edge about the protocol implemented by the binaries, to
determine whether their outputs are semantically equiva-
lent. In practice, we have found deviations that are quite
obvious. Typically, the server whose symbolic formula
is satisﬁed by the input produces a response similar to
its response to the original input, and the server whose
symbolic formula is not satisﬁed by the input produces
an error message, drops the connection, etc.
4 Implementation
Our implementation consists of several components: a
path recorder, the symbolic formula generator, the solver,
and a validator. We describe each below.
Collecting the trace. The symbolic formula generator
component is based on QEMU, a complete system em-
ulator [10]. We use a modiﬁed version of QEMU, that
has been enhanced with the ability to track how speci-
ﬁed external inputs, such as keyboard or received net-
work data are procesed. The formula generator moni-
tors the execution of a binary and records the execution
trace, containing all instructions executed by the program
and the information of their operands, such as their value
and whether they are derived from speciﬁed external in-
puts. We start monitoring the execution before sending
requests to the server and stop the trace when we observe
a response from the server. We use a no-response timer
F(cid:13)
F(cid:13)
Figure 2: Different execution paths could end up in the
same output states. The validation phase checks whether
the new execution path explored by the candidate devia-
tion input obtained in the deviation detection phase truly
ends up in a different state.
3.2 Deviation Detection Phase
In this phase, we use a solver to ﬁnd candidate inputs
which may cause deviations. This phase takes as input
the formulas f1 and f2 generated for the programs P1
and P2 in the formula extraction phase. We rewrite the
variables in each formula so that they refer to the same
input, but each to their own internal states.
We then query the solver whether the combined for-
mula (f1 ∧ ¬f2) ∨ (¬f1 ∧ f2) is satisﬁable, and if so,
to provide an example that satisﬁes the combined for-
mula.
If the solver returns an example, then we have
found an input that satisﬁes one program’s formula, but
not the other. If we had perfectly and fully modeled each
program, and perfectly speciﬁed the post-condition to be
that “the input results in a semantically equivalent output
state”, then this input would be guaranteed to produce a
semantically equivalent output state in one program, but
not the other. Since we only consider one program path
and do not perfectly specify the post-condition in this
way, this input is only a candidate deviation input.
220
16th USENIX Security Symposium
USENIX Association
to stop the trace if no answer is observed from the server
after a conﬁgurable amount of time.
Symbolic formula generation. We implemented our
symbolic formula generator as part of our BitBlaze bi-
nary analysis platform [1]. The BitBlaze platform can
parse executables and instruction traces, disassemble
each instruction, and translate the instructions into the IR
shown in Table 1. The entire platform consists of about
16,000 lines of C/C++ code and 28,000 lines of OCaml,
with about 1,600 lines of OCaml speciﬁcally written for
our approach.
Solver. We use STP [30, 31] as our solver. It is a deci-
sion procedure specialized in modeling bit-vectors. After
taking our symbolic formula as input, it either outputs an
input that can satisfy the formula, or decides that the for-
mula is not satisﬁable.
Candidate deviation input validation. Once a candi-
date deviation input has been returned by the solver, we
need to validate it against both server implementations
and monitor the output states. For this we have built
small HTTP and NTP clients that read the inputs, send
them over the network to the servers, and capture the re-
sponses, if any.
After sending candidate inputs to both implementa-
tions, we determine the output state by looking at the
response sent from the server. For those protocols that
contain some type of status code in the response, such as
HTTP in the Status-Line, each different value of the sta-
tus code represents a different output state for the server.
For those protocols that do not contain a status code in
the response, such as NTP, we deﬁne a generic valid state
and consider the server to have reached that state, as a
consequence of an input, if it sends any well-formed re-
sponse to the input, independently of the values of the
ﬁelds in the response.
In addition, we deﬁne three special output states: a fa-
tal state that includes any behavior that is likely to cause
the server to stop processing future queries such as a
crash, reboot, halt or resource starvation, a no-response
state that indicates that the server is not in the fatal state
but still did not respond before a conﬁgurable timer ex-
pired, and a malformed state that includes any response
from the server that is missing mandatory ﬁelds. This
last state is needed because servers might send messages
back to the client that do not follow the guidelines in the
corresponding speciﬁcation. For example several HTTP
servers, such as Apache or Savant, might respond to an
incorrect request with a raw message written into the
socket, such as the string “IOError” without including
Server
Apache
Miniweb
Savant
NetTime
Ntpd
Version
Type
Binary Size
2.2.4
0.8.1
3.1
2.0 beta 7
4.1.72
HTTP server
HTTP server
HTTP server
NTP server
NTP server
4,344kB
528kB
280kB
3,702kB
192kB
Table 3: Different server implementations used in our
evaluation.
the expected HTTP Status-Line such as “HTTP/1.1 400
Bad Request”.
5 Evaluation
We have evaluated our approach on two different proto-
cols: HTTP and NTP. We selected these two protocols
as representatives of two large families of protocols: text
protocols (e.g. HTTP) and binary protocols (e.g. NTP).
Text and binary protocols present signiﬁcant differences
in encoding, ﬁeld ordering, and methods used to separate
ﬁelds. Thus, it is valuable to study both families. In par-
ticular, we use three HTTP server implementations and
two NTP server implementations, as shown in Table 3.
All the implementations are Windows binaries and the
evaluation is performed on a Linux host running Fedora
Core 5.
The original inputs, which we need to send to the
servers during the formula extraction phase to generate
the execution traces, were obtained by capturing a net-
work trace from one of our workstations and selecting
all the HTTP and NTP requests that it contained. For
each HTTP request in the trace, we send it to each of the
HTTP servers and monitor its execution, generating an
execution trace as output. We proceed similarly for each
NTP request, obtaining an execution trace for each re-
quest/server pair. In Section 5.1, we show the deviations
we discovered in the web servers, and in Section 5.2, the
deviations we discovered in the NTP servers.
5.1 Deviations in Web Servers
This section shows the deviations we found among three
web server implementations: Apache, Miniweb, and Sa-
vant. For brevity and clarity, we only show results for a
speciﬁc HTTP query, which we ﬁnd to be specially im-
portant because it discovered deviations between differ-
ent server pairs. Figure 3 shows this query, which is an
HTTP GET request for the ﬁle /index.html.
Deviations detected. For each server we ﬁrst calculate
a symbolic formula that represents how the server han-
USENIX Association
16th USENIX Security Symposium
221
Original request:
0000:
0010:
0020:
47 45 54 20 2F 69 6E 64 65 78 2E 68  74 6D 6C 20  GET /index.html
48 54 54 50 2F 31 2E 31 0D 0A 48 6F  73 74 3A 20  HTTP/1.1..Host:
31 30 2E 30 2E 30 2E 32 31 0D 0A 0D  0A           10.0.0.21....
Figure 3: One of the original HTTP requests we used to generate execution traces from all HTTP servers, during the
formula extraction phase.
¬fA
N/A
Case 3: 5/5
¬fM
¬fS
Case 1: unsatisﬁable Case 2: 5/0
Case 4: 5/5
N/A
Case 5: unsatisﬁable Case 6: unsatisﬁable
N/A
fA
fM
fS
Table 4: Summary of deviations found for the HTTP servers, including the number of candidate input queries requested
to the solver and the number of deviations found. Each cell represents the results from one query to the solver and each
query to the solver handles half of the combined formula for each server pair. For example Case 3 shows the results
when querying the solver for (fM ∧ ¬fA) and the combined formula for the Apache-Miniweb pair is the disjunction
of Cases 1 and 3.
dled the original HTTP request shown in Figure 3. We
call these formulas: fA, fS, fM for Apache, Savant and
Miniweb respectively. Then, for each of the three pos-
sible server pairs: Apache-Miniweb, Apache-Savant and
Savant-Miniweb, we calculate the combined formula as
explained in Section 3. For example, for the Apache-
Miniweb pair, the combined formula is (fA ∧ ¬fM ) ∨
(fM ∧ ¬fA). To obtain more detailed information, we
break the combined formula into two separates queries to
the solver, one representing each side of the disjunction.
For example, for the Apache-Miniweb pair, we query the
solver twice: one for (fA ∧ ¬fM ) and another time for
(fM ∧ ¬fA). The combined formula is the disjunction of
the two responses from the solver.
Table 4 summarizes our results when sending the
HTTP GET request in Figure 3 to the three servers. Each
cell of the table represents a different query to the solver,
that is, half of the combined formula for each server
pair. Thus, the table has six possible cells. For exam-
ple, the combined formula for the Apache-Miniweb pair,
is shown as the disjunction of Cases 1 and 3.
Out of the six possible cases, the solver returned un-