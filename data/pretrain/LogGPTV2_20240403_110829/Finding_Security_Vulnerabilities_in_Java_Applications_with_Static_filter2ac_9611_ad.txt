90
45 30 118
57
16
60
18 13
25
32
173 107 28 303
47
193
58 33 142
43
247 186 40 268
177
49
58 35 117
362 226 55 733 103
X X
X
X
12
14
14
jboard
14
16
17
blueblog
66 106 101
69
webgoat
27
21
24
blojsom
62
50
59
personalblog
194 154 160 105
snipsnap
73
58
road2hibernate
150 140 136 100
pebble
196
83 338 129
roller
Figure 9: Summary of times, in seconds, it takes to perform prepro-
cessing, points-to, and taint analysis for each analysis variation. Analy-
sis variations have either context sensitivity or improved object naming
enabled, as indicated by X signs in the header row.
16
21
30
19
44 161
USENIX Association
14th USENIX Security Symposium
281
Context sensitivity
Improved object naming
jboard
blueblog
webgoat
blojsom
personalblog
snipsnap
road2hibernate
pebble
roller
Total
Sources
Sinks
1
6
13
27
25
155
1
132
32
392
6
12
59
18
31
100
33
70
64
393
Tainted objects
X
X
268
17
1,166
368
2,066
1,168
2,150
1,403
2,367
23
17
201
203
1,023
791
843
621
504
2
17
903
197
1,685
897
1,641
957
1,923
X
X
2
17
157
112
426
456
385
255
151
10,973
4,226
8,222
1,961
2,115
Reported warnings
False positives
Errors
X X
X
0
1
51
26
370
513
16
193
261
1,431
0
1
6
2
2
27
1
1
1
41
X
0
1
7
4
275
93
12
211
12
615
X X
X
0
0
45
24
368
498
15
192
260
1,402
0
0
0
0
0
12
0
0
0
12
X
0
0
1
2
273
78
11
210
11
586
0
0
45
46
458
717
17
426
377
2,086
0
1
6
2
2
15
1
1
1
29
0
1
51
48
460
732
18
427
378
Figure 10: (a) Summary of data on the number of tainted objects, reported security violations, and false positives for each analysis version. Enabled
analysis features are indicated by X signs in the header row. (b) Comparison of the number of tainted objects for each version of the analysis.
the table shows the times to pre-process the application
to create relations accepted by the pointer analysis; the
second shows points-to analysis times; the last presents
times for the tainted object propagation analysis.
It should be noted that the taint analysis times often
decrease as the analysis precision increases. Contrary
to intuition, we actually pay less for a more precise an-
alysis. Imprecise answers are big and therefore take a
long time to compute and represent. In fact, the context-
insensitive analysis with default object naming runs sig-
niﬁcantly slower on the largest benchmarks than the most
precise analysis. The most precise analysis version takes
a total of less than 10 minutes on the largest application;
we believe that this is acceptable given the quality of the
results the analysis produces.
6.3 Vulnerabilities Discovered
The static analysis described in this paper reports a to-
tal of 41 potential security violations in our nine bench-
marks, out of which 29 turn out to be security errors,
while 12 are false positives. All but one of the bench-
marks had at least one security vulnerability. Moreover,
except for errors in webgoat and one HTTP splitting vul-
nerability in snipsnap [16], none of these security er-
rors had been reported before.
6.3.1 Validating the Errors We Found
Not all security errors found by static analysis or code
reviews are necessarily exploitable in practice. The error
may not correspond to a path that can be taken dynam-
ically, or it may not be possible to construct meaningful
malicious input. Exploits may also be ruled out because
of the particular conﬁguration of the application, but con-
ﬁgurations may change over time, potentially making ex-
ploits possible. For example, a SQL injection that may
not work on one database may become exploitable when
the application is deployed with a database system that
does not perform sufﬁcient input checking. Furthermore,
virtually all static errors we found can be ﬁxed easily by
modifying several lines of Java source code, so there is
generally no reason not to ﬁx them in practice.
After we ran our analysis, we manually examined all
the errors reported to make sure they represent security
errors. Since our knowledge of the applications was not
sufﬁcient to ascertain that the errors we found were ex-
ploitable, to gain additional assurance, we reported the
errors to program maintainers. We only reported to ap-
plication maintainers only those errors found in the ap-
plication code rather than general libraries over which
the maintainer had no control. Almost all errors we re-
ported to program maintainers were conﬁrmed, resulting
in more that a dozen code ﬁxes.
Because webgoat is an artiﬁcial application designed
to contain bugs, we did not report the errors we found
in it.
Instead, we dynamically conﬁrmed some of the
statically detected errors by running webgoat, as well as
a few other benchmarks, on a local server and creating
actual exploits.
It is important to point out that our current analysis
ignores control ﬂow. Without analyzing the predicates,
our analysis may not realize that a program has checked
its input, so some of the reported vulnerabilities may turn
282
14th USENIX Security Symposium
USENIX Association
jboardblueblogwebgoatblojsompersonalblogsnipsnaproad2hibernatepebblerollerBenchmark applications02505007501000125015001750200022502500NumberoftaintedobjectsContext-insensitive,default namingContext-insensitive,improved namingContext-sensitive,default namingContext-sensitive,improved namingXXXXXXX
SOURCES
SINKS
SQL injections
HTTP splitting
Cross-site scripting
Path traversal Total
Header manip.
Parameter manip.
Cookie poisoning
Non-Web inputs
0
webgoat: 4, personalblog: 2 = 6
webgoat = 1
snipsnap: 1, road2hibernate: 1 = 2
0
snipsnap = 6 blueblog: 1, webgoat: 1, pebble: 1, roller: 1 = 4
blojsom = 2
0
snipsnap = 5
0
0
0
0 snipsnap = 3
0
Total
9
11
4
5
10
13
1
5
29
Figure 11: Classiﬁcation of vulnerabilities we found. Each cell corresponds to a combination of a source type (in rows) and sink type (in columns).
out to be false positives. However, our analysis shows all
the steps involved in propagating taint from a source to a
sink, thus allowing the user to check if the vulnerabilities
found are exploitable.
Many Web-based application perform some form of
input checking. However, as in the case of the vulnera-
bilities we found in snipsnap, it is common that some
checks are missed. It is surprising that our analysis did
not generate any false warnings due to the lack of pred-
icate analysis, even though many of the applications we
analyze include checks on user input. Two security er-
rors in blojsom ﬂagged by our analysis deserve special
mention. The user-provided input was in fact checked,
but the validation checks were too lax, leaving room for
exploits. Since the sanitization routine in blojsom was
implemented using string operations as opposed to direct
character manipulation, our analysis detected the ﬂow of
taint from the routine’s input to its output. To prove the
vulnerability to the application maintainer, we created
an exploit that circumvented all the checks in the vali-
dation routine, thus making path-traversal vulnerabilities
possible. Note that if the sanitation was properly imple-
mented, our analysis would have generated some false
positives in this case.
6.3.2 Classiﬁcation of Errors
This section presents a classiﬁcation of all the errors
we found. A summary of our experimental results is pre-
sented in Figure 10(a). Columns 2 and 3 list the number
of source and sink objects for each benchmark. It should
be noted that the number of sources and sinks for all of
these applications is quite large, which suggests that se-
curity auditing these applications is time-consuming, be-
cause the time a manual security code review takes is
roughly proportional to the number of sources and sinks
that need to be considered. The table also shows the
number of vulnerability reports, the number of false pos-
itives, and the number of errors for each analysis version.
Figure 11 presents a classiﬁcation of the 29 secu-
rity vulnerabilities we found grouped by the type of the
source in the table rows and the sink in table columns.
For example,
the cell in row 4, column 1 indicates
that there were 2 potential SQL injection attacks caused
by non-Web sources, one in snipsnap and another in
road2hibernate.
Overall, parameter manipulation was the most com-
mon technique to inject malicious data (13 cases) and
HTTP splitting was the most popular exploitation tech-
nique (11 cases). Many HTTP splitting vulnerabilities
are due to an unsafe programming idiom where the ap-
plication redirects the user’s browser to a page whose
URL is user-provided as the following example from
snipsnap demonstrates:
response.sendRedirect(
request.getParameter("referer"));
Most of the vulnerabilities we discovered are in appli-
cation code as opposed to libraries. While errors in ap-
plication code may result from simple coding mistakes
made by programmers unaware of security issues, one
would expect library code to generally be better tested
and more secure. Errors in libraries expose all applica-
tions using the library to attack. Despite this fact, we
have managed to ﬁnd two attack vectors in libraries: one
in a commonly used Java library hibernate and another
in the J2EE implementation. While a total of 29 security
errors were found, because the same vulnerability vec-
tor in J2EE is present in four different benchmarks, they
actually corresponded to 26 unique vulnerabilities.
6.3.3 SQL Injection Vector in hibernate
We start by describing a vulnerability vector found
in hibernate, an open-source object-persistence library
commonly used in Java applications as a lightweight
back-end database. hibernate provides the function-
ality of saving program data structures to disk and load-
ing them at a later time. It also allows applications to
search through the data stored in a hibernate database.
Three programs in our benchmark suite, personalblog,
road2hibernate, and snipsnap, use hibernate to
store user data.
We have discovered an attack vector in code pertain-
ing to the search functionality in hibernate, version
2.1.4. The implementation of method Session.find re-