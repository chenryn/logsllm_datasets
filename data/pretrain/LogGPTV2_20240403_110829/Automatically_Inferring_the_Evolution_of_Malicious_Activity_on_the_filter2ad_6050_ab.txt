given by a collection of non-overlapping intervals in [0, 1].
A preﬁx is assigned a state based on the fraction of trafﬁc
it originates that is non-malicious. Thus, for example, the
state deﬁned by the interval [0, 0.2] is assigned to preﬁxes
1There may be situations where a preﬁx undergoes changes, but the
change is not directly observed when trafﬁc is aggregated at that preﬁx,
e.g., a preﬁx could originate roughly the same amount of malicious and
non-malicious trafﬁc in st+1 as it did in st, but misclassify both malicious
and non-malicious activity on st+1 (perhaps because some of its children
preﬁxes have changed). We ignore such changes in this paper as they are
not typically actionable.
sending between 0 − 20% spam. Conceptually, the state
of a preﬁx can be thought of measuring the level of ”bad-
ness” of the preﬁx. We deﬁne a localized change in a preﬁx
to be one where the preﬁx has changed its state, and our
goal is to ﬁnd only localized changes. For example, sup-
pose the set D consists of two intervals [0, 0.2) and [0.2, 1).
A preﬁx that used to send less than 20% spam, but now
sends between 20−100% has undergone a localized change
(in effect, the preﬁx is considered non-malicious if it sends
less than 20% spam, and malicious if it sends at least 20%
spam, and we are only interested in ﬁnding when the preﬁx
changes from malicious to non-malicious, or vice-versa.)
The set D is input to the algorithm. Continuous intervals
provide ﬁner-grained data to operators than just malicious
and non-malicious. Of course, in reports to the operators,
we can always reduce to just malicious and non-malicious
if desired. 2
Modeling Malicious Activity of Preﬁxes as Decision
Tree. We take advantage of the structural properties of
malicious activity in order to design an efﬁcient and ac-
curate algorithm for detecting changes. Prior work has
demonstrated that malicious trafﬁc tends to be concentrated
in some parts of the address space [5, 21, 23, 30] – that is,
the IP address space can be partitioned into distinct preﬁx-
based regions, some of which mostly originate malicious
trafﬁc and some that mostly originate legitimate trafﬁc. We
observe that the IP address space can be represented as a
tree of preﬁxes. Thus, we can model the structure of ma-
licious activity as a decision tree over the IP address space
hierarchy rooted at the /0 preﬁx: the leaves of this tree are
preﬁx-based partitions that send mostly malicious or mostly
non-malicious trafﬁc; this is a decision tree since each leaf
in the tree can be considered as having a ”label” that indi-
cates the kind of trafﬁc that the corresponding preﬁx-based
region originates (e.g., the label might be ”bad” when the
region originates mostly malicious trafﬁc, ”good” when the
region originates mostly legitimate trafﬁc). The changes in
preﬁx behaviour can then be precisely captured by changes
in this decision tree. In Sec. 3, we describe how we learn
this decision tree to model the malicious activity from the
data.
More formally: let I denote the set of all IP addresses,
and P denote the set of all IP preﬁxes. An IPTree TP over
the IP address hierarchy is a tree whose nodes are preﬁxes
P ∈ P, and whose leaves are each associated with a la-
bel, malicious or non-malicious. An IPtree thus serves as a
classiﬁcation function for the IP addresses I. An IP address
i ∈ I gets the label associated with its longest matching
preﬁx in the tree. A k-IPtree is an IPtree with at most k
leaves. By ﬁxing the number of leaves, we get a constant-
2We could also deﬁne changes in terms of the relative shift in the ma-
licious activity of the preﬁx. However, the deﬁnition we use above allows
for a conceptually easier way to explain preﬁx behavior.
sized data structure. The optimal k-IPtree on a stream of
IP address-label pairs is the k-IPtree that makes the small-
est number of mistakes on the stream. Figure 2 shows an
example IPtree of size 6.
We deﬁne preﬁx changes in terms of the IPTree: We de-
ﬁne a ∆-bad preﬁx for an IPTree T as a preﬁx p that starts
to originate malicious trafﬁc when T labels trafﬁc from p
as legitimate. Likewise, a ∆-good preﬁx is a preﬁx p that
starts to originate legitimate trafﬁc when T labels trafﬁc
from p as malicious. In the example of Fig. 1, the /24s in the
ﬁrst and second scenarios are labeled as malicious and non-
malicious respectively. The /25 in the second case sends
trafﬁc that differs from the tree’s label. Fig. 1(b) shows an
example ∆-bad preﬁx. Without loss of generality, we will
use ∆-change preﬁx to refer to either ∆-good or a ∆-bad
preﬁx. We of course report back to an operator whether a
∆-change preﬁx is ∆-bad or ∆-good.
In this paper we use TrackIPTree as a subroutine in our
algorithms in order to infer decision trees from the data
stream, as it meets all our algorithmic requirements for scal-
ably building near-optimal decision trees over adversarial
IP address data [29]. (Note TrackIPTree does not solve the
problem of detecting the changed preﬁxes posed in this pa-
per, even with a number of extensions, as we discuss in Sec-
tion 2.2.) Conceptually, TrackIPTree keeps track of a large
collection of closely-related decision trees, each of which
is associated with a particular weight. It predicts the label
for an IP address by a choosing a decision tree from this set
in proportion to its relative weight in the set; when given
labeled data to learn from, it increases the weights of the
decision trees that make correct predictions, and decreases
the weights of those that make incorrect predictions. Track-
IPTree accomplishes this efﬁciently (from both space and
computation perspectives) by keeping a single tree with the
weights decomposed appropriately into the individual pre-
ﬁxes of the tree.
2.2 Alternate Approaches
We ﬁrst discuss a few previous approaches that may ap-
pear to be simpler alternatives to our algorithms, and ex-
plain why they do not work.
BGP preﬁxes.
A straightforward idea would be to use
BGP preﬁxes such as network-aware clusters [19], a clus-
tering that represents IP addresses that are close in terms
of network topology. BGP preﬁxes have been a popular
choice in measurement studies of spamming activity and
spam-detection schemes [9,13,22,27,30], but have increas-
ingly been shown to be far too coarse to model spamming
activity accurately [22].
Unfortunately, BGP preﬁxes perform poorly because
they do not model the address space at the appropriate
granularity for malicious activity. BGP preﬁxes only re-
ﬂect the granularity of the address space at which routing
happens, but actual ownership (and corresponding security
properties) may happen at ﬁner or coarser preﬁx granu-
larity.
(Likewise, ASes are also not an appropriate rep-
resentation because even though the Internet is clustered
into ASes, there is no one-to-one mapping between ser-
vice providers and ASes [3].) Our experiments in Sec. 4.1
demonstrate this, where network-aware clusters identify
around 2.5 times fewer ∆-change preﬁxes than our algo-
rithms. For example, such an algorithm fails to report ∆-
changes in small to medium hosting providers. These host-
ing providers are located in different regions of the world;
the provider manages small preﬁx blocks, but these preﬁx
blocks do not appear in BGP preﬁxes. Any change in the
hosting provider’s behavior typically just disappears into
the noise when observed at the owning BGP preﬁx, but
can be the root cause of malicious activity that the opera-
tor should know about.
Strawman Approaches based on TrackIPTree. A sec-
ond approach would be to learn IPTree snapshots that can
classify the data accurately for different time intervals, and
simply ”diff” the IPTree snapshots to ﬁnd the ∆-change pre-
ﬁxes. TrackIPTree [29] is a natural choice to construct these
IPTree, as it can build a near-optimal classiﬁer. However,
even with near-optimal IPTrees, we cannot directly com-
pare them to accurate ﬁnd ∆-change preﬁxes.
Let sa, sb be two arbitrary input sequences of IPs on
which we make no a priori assumptions, as described in
Section 2.1. 3 Let Ta and Tb be the resulting IPtrees after
learning over sa and sb respectively using TrackIPTree [29].
There are many immediate ways we could compare Ta and
Tb, but when the trees are large, noisy and potentially error-
prone, most of these lead to a lot of false positives. We use
here small examples to illustrate how these differencing ap-
proaches fail, and in Section 4, we show that these lead to
extremely high false positive rates on real IPTrees.
One possible approach to compare two decision trees is
to compare the labels of their leaves. However, the two trees
may assign different labels to a region even when there is
not a (signiﬁcant) difference in the relevant parts of sa and
sb, e.g., both trees may be inaccurate in that region, making
any differences found to be false positives.
Even if we know which parts of the tree are accurate, and
restrict ourselves to comparing only “mostly accurate” pre-
ﬁxes, we still cannot directly compare the trees. The trees
may still appear different because the tree structure is dif-
ferent, even though they encode almost identical models of
3We make no assumption on the  pairs that are present
in sa and sb. This means that there may be some IPs that are common to
both sa and sb, and others that IPs are not present in sa or sb. The labels
of the common IPs do not need to be identical in sa and sb; indeed, we
expect that in real data, some of the common IPs will have the same labels
in sa and sb, but others will differ. Even within a single sequence sa, an
IP i does not need to have the same label throughout, it may have different
labels at different points in the sequence sa.
/0
/1
/16
/17
-
θ
/18
-
0.99θ
+
0.01θ
/0
/1
-
2θ
/16
(a) Learned Ta
(b) Learned Tb
Figure 3. Comparing “Mostly Accurate” Preﬁxes. Ta
and Tb classify 99% of trafﬁc seen identically, but would
be ﬂagged different because of differences in the tree
structure that affect very little trafﬁc.
the malicious activity. We illustrate this with the example in
Figure 3 (assume that each leaf shown in the ﬁgure classiﬁes
over 95% of its respective trafﬁc accurately). The two trees
Ta and Tb (learned over sa and sb respectively) then classify
over 99% of IPs identically, yet would be ﬂagged different
if we simply compared their accurate preﬁxes. Such small
deviations might just be caused by noise since the trees are
learned over different sequences, e.g., sa might have had
more noise than sb. It is of little use to identify such ∆-bad
preﬁxes for operators.
A third possible approach considers only preﬁxes that
are both “mostly accurate” and have sufﬁcient (at least θ)
trafﬁc, but even then, we cannot simply compare the trees.
Consider Figure 4, where the true tree has a /16 preﬁx with
two /17 children, and one /17 originates only malicious IPs,
while the other /17 originates only legitimate trafﬁc. In the
two learned trees Ta and Tb, none of the leaves see sufﬁ-
cient (θ) trafﬁc.4 In this example, the highlighted /16 preﬁx
is the longest parent preﬁx with θ trafﬁc in both Ta and Tb.
If we analyze the interior preﬁx’s activity by the trafﬁc it has
seen, most of the trafﬁc seen by the /16 is non-malicious in
Ta and malicious in Tb. Thus, we would ﬂag it as a ∆-bad
preﬁx. However, this is once again a false positive – note
that all leaf labels in Ta and Tb are identical (i.e., no re-
gion has actually changed its behaviour) – the only change
is that a few leaves send less trafﬁc in Ta and more in Tb
(and vice versa). Such changes in trafﬁc volume distribu-
tion occur routinely without malicious regions becoming
benign. For example, some spam-bots may become quiet
for a few days while they receive new spam templates, and
then restart spamming activities. In Sec. 4.1, we show em-
pirically that this third approach can lead to false positive
4We need to analyze the interior preﬁxes to ensure that we do not miss
legitimate changes. For example, imagine a scenario where most of the
leaves in Ta are negative, while most of the leaves in Tb are positive. The
longest parent preﬁx with at least θ trafﬁc is an interior preﬁx, and it has
clearly undergone a change. If we do not analyze the interior preﬁx, we
will miss such changes.
/0
/1
/16
/17
/0
/1
/16
/17
-
0.09θ
/18
-
0.01θ
+
0.9θ
-
0.9θ
/18
-
0.01θ
+
0.1θ
(a) Learned Ta
(b) Learned Tb
Figure 4. Comparing preﬁxes that are accurate as well
as have sufﬁcient trafﬁc. Ta and Tb are accurate, and
share identical leaf labels; however, none of the leaves
have enough trafﬁc to be compared.
rates of over 47%.
3 Our Algorithms
3.1 Overview of Our Approach
Our key insight is to use the classiﬁcation error between
the two trees in order to infer ∆-change preﬁxes. If a pre-
ﬁx has had low classiﬁcation error in earlier time intervals
with Tz, but now has high classiﬁcation error (on substan-
tial trafﬁc), we can infer that it has undergone a ∆-change.
The (earlier) low classiﬁcation error (on sufﬁcient trafﬁc)
implies our tree Tz used to model this region well in the
past intervals, but and the current high classiﬁcation error
implies does not do so any longer. Thus, we infer that the
preﬁx has changed its behavior – that it is sending trafﬁc that
is inconsistent with its past behaviour – and therefore, is a
∆-change region. As long as we are able to maintain a de-
cision tree with high predictive accuracy for the sequences,
our analysis can discover most preﬁxes changing between
sa and sb. Further, by only selecting preﬁxes that have a
high classiﬁcation error on a substantial fraction of the traf-
ﬁc, we build some noise-tolerance into our approach.
This insight shows that we need to achieve the follow-
ing three simultaneous goals to address the IPTree evolu-
tion problem: (1) keep track of a current model of the ma-
licious activity; (2) measure the classiﬁcation errors of the
current sequence based on a prior accurate model; (3) keep
track of a current model of the frequently changing regions.
We keep multiple decision trees over the address to simul-
taneously achieve these goals. At a high-level, we let one
IPtree learn over the current sequence, so it tracks the cur-
rent malicious activity. We keep second set of IPtrees ﬁxed
(i.e., they cannot change its labels, weights, or structure),
and use them to measure the classiﬁcation accuracy on the
current sequence. We then compare the classiﬁcation errors
of the second set of IPtrees (not the IPtrees themselves) on
the different sequences to compute the speciﬁc changed pre-
ﬁxes (details in Section 3.2). For our third goal, we use our
T
z-1
Update with 
TrackIPTree
T
z
Stream sz
...
∆-bad  &
∆-good 
prefixes
Extract 
Changed 
Prefixes
T
old
Classify each 
IP  with T
old
Annotated 
T
old, z
T
old, z-1
Figure 5. High-level sketch of ∆-Change Algorithm
learned IPtrees to discover which of the IP addresses in the
current sequence have changed their labels. We then learn a
third IPtree based on this information, partitions the address
space into regions that are change frequently and those that
do not. (To avoid confusion, we term this third IPtree as
change-IPTree, and deﬁne it in Section 3.3).
3.2 The ∆-Change Algorithm
We now present our ∆-Change algorithm which ad-
dresses the question: can we identify the speciﬁc regions of
changing malicious activity? Recall that a k-IPtree is a de-
cision tree over the IP address space with at most k leaves.
Let s1, s2 . . . denote the sequences of IP addresses at each
time interval, and let Tz denote the IPtree built over the se-
quence sz. For readability, we use Tcurr to denote the tree
that is being learned over the current sequence of IPs, and
Told to denote an older tree that is being kept ﬁxed over the
current sequence of IPs. We use Told,z to denote the tree
Told that is annotated with its classiﬁcation errors on the
sequence sz.
At a high-level, we do the following: As each labelled IP