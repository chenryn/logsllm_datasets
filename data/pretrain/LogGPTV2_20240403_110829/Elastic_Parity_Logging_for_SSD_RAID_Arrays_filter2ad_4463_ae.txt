(b) GC overhead
Fig. 10: Experiment 4: Total size of write trafﬁc to SSDs and
GC overhead under different parity commit cases and (6+2)-
RAID-6 setting.
Experiment 5 (I/O performance): The previous experiments
examine the write size and number of GC requests. We now
examine the I/O throughput of EPLOG, measured as the
number of user-level requests issued to the SSDs divided by
the total time (in units of KIOPS); note that the total time also
includes the overheads of writes to log devices. We replay
each trace as fast as possible to obtain the maximum possible
performance.
Figure 11 shows the throughput results. Figure 11(a),
EPLOG outperforms MD by 30.1-119.2% and PL by 186.9-
305.5% across different
traces under (6+2)-RAID-6. Fig-
ure 11(b), EPLOG outperforms MD by 119.2-197.3% and
PL by 295.7-366.1% across different RAID settings under
the FIN trace. Both MD and PL read data before updating
or logging parity on the update path. MD achieves higher
throughput than PL, as MD directly updates parities on SSDs,
while PL logs parity updates to HDD-based log devices for
endurance. EPLOG eliminates pre-reads of existing data in log
chunk computation, thereby increasing the I/O throughput. In
addition, EPLOG reduces the total size of log chunks by 8-15%
compared to PL (not shown in the ﬁgure) due to elastic parity
logging, and the reduction also leads to throughput gains.
Experiment 6 (Metadata management overheads): We now
evaluate the overheads of the metadata checkpoint operations
(see Section III-E). We consider the scenario where metadata
is generated after a large number of random writes. We use
IOzone [16] to ﬁrst create continuous stripes covering a 8GB
area on SSD RAID using sequential writes, and then issue
uniform random updates of size 4KB each across all stripes.
We then measure the total size of write trafﬁc to SSDs
under three cases: (i) full checkpoint after stripe creation, (ii)
incremental checkpoint after all stripe updates, and (iii) full
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
MD
PL
EPLog
 5
 0
FIN WEB
USR MDS
(a) Different traces under (6+2)-RAID-6
MD
PL
EPLog
 20
 15
 10
)
S
P
O
K
I
(
t
u
p
h
g
u
o
r
h
T
 20
 15
 10
 5
 0
)
S
P
O
K
I
(
t
u
p
h
g
u
o
r
h
T
RAID-5
(4+1)
RAID-6
(6+2)
(b) Different RAID settings under FIN
RAID-5
(6+1)
RAID-6
(4+2)
Fig. 11: Experiment 5: I/O performance.
EPLOG
(i) Stripe creation w/o chkpt. (GB) 10.922
Setting
full chkpt. (GB) 10.961 (+0.36%)
(ii) Stripe update w/o chkpt. (GB) 8.147
incr. chkpt. (GB) 8.294 (+1.81%)
(iii) Stripe update w/o chkpt. (GB) 8.147
full chkpt. (GB) 8.331 (+2.25%)
TABLE II: Experiment 6: Total sizes of write trafﬁc to SSDs
with/without metadata checkpoint operations.
checkpoint after all stripe updates. We evaluate the metadata
checkpoint overhead by comparing the cases with and without
checkpoint operations.
Table II shows the results. Note that stripe creation issues
new full-stripe writes, so EPLOG writes them to SSDs. The
total write size is around 11GB,
including parity writes.
Later in stripe updates, EPLOG redirects parities to the log
devices, and the total write size drops to around 8GB. Overall,
the metadata checkpoint overhead in write size is at most
2.25%. The incremental checkpoint operation only writes dirty
metadata after updates, and its overhead is less than that of the
full checkpoint operation. The results show that EPLOG incurs
low overheads in metadata management.
VI. RELATED WORK
Researchers have proposed various techniques for enhanc-
ing the performance and endurance of a single SSD, such
as disk-based write caching [46], read/write separation via
redundancy [45], and ﬂash-aware ﬁle systems (e.g., [23],
[24], [30], [34]). EPLOG targets an SSD RAID array and
59
is currently implemented as a user-level block device. It can
also incorporate advanced techniques of existing ﬂash-aware
designs, such as hot/cold data grouping [24], [34] and efﬁcient
metadata management [23], [30], for further performance and
endurance improvements.
Flash-aware RAID designs have been proposed either at
the chip level [10], [15], [22] or at
the device level [3],
[26], [28], [31], [39], [40]. For example, Greenan et al. [10]
keep outstanding parity updates in NVRAM and defer them
until a full stripe of data is available. FRA [28] also defers
parity updates, but keeps outstanding parity updates in DRAM,
which is susceptible to data loss. Balakrishnan et al. [3]
propose to unevenly distribute parities among SSDs to avoid
correlated failures. Lee et al. [26] and Im et al. [15] propose the
partial parity idea, which generates parity chunks from partial
stripes and maintains the parity chunks in NVRAM. HPDA
[31] builds an SSD-HDD hybrid architecture which keeps all
parities in HDDs and uses the HDDs as write buffers. Kim et
al. [22] propose an elastic striping method that encodes the
newly written data to form new data stripes and writes the
data and parity chunks directly to SSDs without NVRAM.
Pan et al. [40] propose a diagonal coding scheme to address
the system-level wear-leveling problem in SSD RAID, and the
same research group [39] extends the elastic striping method
by Kim et al. [22] with a hotness-aware design.
EPLOG relaxes the constraints of parity construction in
which parity can be associated with a partial stripe, following
the same rationale as previous work [15], [22], [26], [39].
Compared to previous work, EPLOG keeps log chunks with
elastic parity logging using commodity HDDs rather than
NVRAM as in [15], [26]. Also, instead of directly writing
parity chunks to SSDs [22], [39], EPLOG keeps log chunks
in log devices to limit parity write trafﬁc to SSDs, especially
when synchronous writes are needed (see Section II-A). While
HPDA [31] also uses HDDs to keep parities as in EPLOG,
it always keeps all parities in HDDs and treats HDDs as a
write buffer, but does not explain how parities in HDDs are
generated and stored. In contrast, EPLOG ensures sequential
writes of log chunks to HDD-based log devices and regularly
performs parity commit in SSDs (note that parity commit does
not need to access log devices in normal mode). In addition,
EPLOG employs an elastic logging policy, which does not need
to pre-read old data chunks and also relaxes the constraint of
per-stripe basis in computing parity logs, so as to reduce the
amount of logs and fully utilize device-level parallelism among
SSDs. We point out that EPLOG targets general RAID schemes
that
tolerate a general number of failures, as opposed to
single fault tolerance as assumed in most existing approaches
discussed above.
VII. CONCLUSIONS
We present EPLOG, a user-level block device that mitigates
parity update overhead in SSD RAID arrays through elastic
parity logging. Its idea is to encode new data chunks to
form log chunks and append the log chunks into separate
log devices, while the data chunks may span in a partial
stripe or across more than one stripe. We carefully build
our EPLOG prototype on commodity hardware, and evaluate
EPLOG through reliability analysis and testbed experiments.
We show that EPLOG improves reliability, endurance, and
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
performance. The source code of EPLOG is available at
http://ansrlab.cse.cuhk.edu.hk/software/eplog.
ACKNOWLEDGMENTS
This work was supported in part by National Nature
Science Foundation of China (61303048 and 61379038), An-
hui Provincial Natural Science Foundation (1508085SQF214),
CCF-Tencent Open Research Fund,
the University Grants
Committee of Hong Kong (AoE/E-02/08), and Research Com-
mittee of CUHK.
REFERENCES
[1] Storage Performance Council.
http://traces.cs.umass.edu/index.php/
Storage/Storage, 2002.
[2] N. Agrawal, V. Prabhakaran, T. Wobber, J. D. Davis, M. Manasse, and
In Proc. of
R. Panigrahy. Design Tradeoffs for SSD Performance.
USENIX ATC, 2008.
[4]
[3] M. Balakrishnan, A. Kadav, V. Prabhakaran, and D. Malkhi. Differential
RAID: Rethinking RAID for SSD Reliability. ACM Trans. on Storage,
6(2):4:1–4:22, Jul 2010.
J. Blomer, M. Kalfane, R. Karp, M. Karpinski, M. Luby, and D. Zuck-
erman. An XOR-Based Erasure-Resilient Coding Scheme. Technical
Report TR-95-048, International Computer Science Institute, UC Berke-
ley, Aug. 1995.
J. S. Bucy, J. Schindler, S. W. Schlosser, and G. R. Ganger. The DiskSim
Simulation Environment Version 4.0 Reference Manual, 2008.
[5]
[6] Y. Cai, E. Haratsch, O. Mutlu, and K. Mai. Error Patterns in MLC
NAND Flash Memory: Measurement, Characterization, and Analysis.
In Prof. of DATE, 2012.
[7] F. Chen, D. A. Koufaty, and X. Zhang. Understanding Intrinsic
Characteristics and System Implications of Flash Memory Based Solid
State Drives. In Proc. of ACM SIGMETRICS, 2009.
[8] P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A.
Patterson. RAID: High-Performance, Reliable Secondary Storage. ACM
Computing Surveys, 26(2):145–185, 1994.
J. Elerath and M. Pecht. Enhanced Reliability Modeling of RAID
Storage Systems. In IEEE/IFIP DSN, June 2007.
[9]
[10] K. Greenan, D. D. E. Long, E. L. Miller, T. Schwarz, and A. Wildani.
Building Flexible, Fault-Tolerant Flash-based Storage Systems. In Proc.
of USENIX HotDep, 2009.
[11] K. M. Greenan, J. S. Plank, and J. J. Wylie. Mean Time to Mean-
ingless: MTTDL, Markov Models, and Storage System Reliability. In
Proceedings of the 2nd USENIX HotStorage, 2010.
[12] L. M. Grupp, A. M. Caulﬁeld, J. Coburn, S. Swanson, E. Yaakobi,
P. H. Siegel, and J. K. Wolf. Characterizing Flash Memory: Anomalies,
Observations, and Applications. In Proc. of IEEE/ACM MICRO, 2009.
[13] L. M. Grupp, J. D. Davis, and S. Swanson. The Bleak Future of NAND
Flash Memory. In Proc. of USENIX FAST, 2012.
[14] T. Harter, C. Dragga, M. Vaughn, A. C. Arpaci-Dusseau, and R. H.
Arpaci-Dusseau. A File is Not a File: Understanding the I/O Behavior
of Apple Desktop Applications. In Proc. of ACM SOSP, 2011.
[15] S. Im and D. Shin. Flash-Aware RAID Techniques for Dependable and
IEEE Trans. on Computers,
High-Performance Flash Memory SSD.
60(1):80–92, Jan 2011.
IOzone. IOzone Filesystem Benchmark. http://www.iozone.org.
[16]
[17] S. Jeong, K. Lee, S. Lee, S. Son, and Y. Won. I/O Stack Optimization
for Smartphones. In Proc. of USENIX ATC, 2013.
[18] N. Jeremic, G. M¨uhl, A. Busse, and J. Richling. The Pitfalls of
Deploying Solid-state Drive RAIDs. In Proc. of ACM SYSTOR, 2011.
[19] M. Jung and M. Kandemir. Revisiting Widely Held SSD Expectations
and Rethinking System-level Implications. In Proc. of ACM SIGMET-
RICS, 2013.
[20] S. Kavalanekar, B. Worthington, Q. Zhang, and V. Sharda. Characteri-
zation of Storage Workload Traces from Production Windows Servers.
In Proc. of IEEE IISWC, 2008.
[21] H. Kim and S. Ahn. BPLRU: A Buffer Management Scheme for
Improving Random Writes in Flash Storage. In Proc. of USENIX FAST,
2008.
J. Kim, J. Lee, J. Choi, D. Lee, and S. Noh. Improving SSD Reliability
with RAID via Elastic Striping and Anywhere Parity.
In Proc. of
IEEE/IFIP DSN, 2013.
[22]
60
[23]
J. Kim, H. Shim, S.-Y. Park, S. Maeng, and J.-S. Kim. FlashLight: A
Lightweight Flash File System for Embedded Systems. ACM Trans. on
Embedded Computing Systems, 11S(1):18:1–18:23, June 2012.
[24] C. Lee, D. Sim, J. Hwang, and S. Cho. F2FS: A New File System for
Flash Storage. In Proc. of USENIX FAST, 2015.
[25] S. Lee, T. Kim, K. Kim, and J. Kim. Lifetime Management of Flash-
based SSDs Using Recovery-aware Dynamic Throttling. In Proceedings
of the 10th USENIX FAST, 2012.
[26] S. Lee, B. Lee, K. Koh, and H. Bahn. A Lifespan-aware Reliability
Scheme for RAID-based Flash Storage. In Proc. of ACM SAC, 2011.
[27] S.-W. Lee and B. Moon. Design of Flash-based DBMS: An In-page
Logging Approach. In Proceedings of the 2007 ACM SIGMOD, 2007.
[28] Y. Lee, S. Jung, and Y. H. Song. FRA: A Flash-aware Redundancy
Array of Flash Storage Devices. In Proc. of IEEE/ACM CODES+ISSS,
2009.
[29] Y. Li, P. P. C. Lee, and J. C. S. Lui. Stochastic Analysis on RAID
Reliability for Solid-State Drives. In IEEE SRDS, 2013.
[30] Y. Lu, J. Shu, and W. Wang. ReconFS: A Reconstructable File System
on Flash Storage. In Proc. of USENIX FAST, 2014.
[31] B. Mao, H. Jiang, S. Wu, L. Tian, D. Feng, J. Chen, and L. Zeng.
HPDA: A Hybrid Parity-based Disk Array for Enhanced Performance
and Reliability. ACM Trans. on Storage, 8(1):4:1–4:20, Feb 2012.
J. Meza, Q. Wu, S. Kumar, and O. Mutlu. A Large-Scale Study of
Flash Memory Failures in the Field. In Proceedings of the 2015 ACM
SIGMETRICS, 2015.
[32]
[33] N. Mielke, T. Marquart, N. Wu, J. Kessenich, H. Belgal, E. Schares,
F. Trivedi, E. Goodness, and L. Nevill. Bit Error Rate in NAND Flash
Memories. In Proc. of IEEE IRPS, 2008.
[34] C. Min, K. Kim, H. Cho, S.-W. Lee, and Y. I. Eom. SFS: Random
Write Considered Harmful in Solid State Drives. In Proc. of USENIX
FAST, 2010.
[35] S. Moon and A. L. N. Reddy. Don’t Let RAID Raid the Lifetime of
Your SSD Array. In Proc. of USENIX HotStorage, 2013.
[37]
[36] D. Narayanan, A. Donnelly, and A. Rowstron. Write Off-loading:
Practical Power Management for Enterprise Storage. ACM Trans. on
Storage, 4(3):10:1–10:23, Nov. 2008.
J. Ostergaard and E. Bueso. The Software-RAID HOWTO. http://tldp.
org/HOWTO/html single/Software-RAID-HOWTO.
J. Ouyang, S. Lin, S. Jiang, Z. Hou, Y. Wang, and Y. Wang. SDF:
Software-deﬁned Flash for Web-scale Internet Storage Systems. In Proc.
of ACM ASPLOS, 2014.
[38]
[39] Y. Pan, Y. Li, Y. Xu, and Z. Li. Grouping-Based Elastic Striping with
Hotness Awareness for Improving SSD RAID Performance. In Proc.
of IEEE/IFIP DSN, 2015.
[40] Y. Pan, Y. Li, Y. Xu, and W. Zhang. DCS5: Diagonal Coding Scheme
for Enhancing the Endurance of SSD-Based RAID-5 Systems. In Proc.
of IEEE NAS, 2014.
[41] D. A. Patterson, G. Gibson, and R. H. Katz. A Case for Redundant
Arrays of Inexpensive Disks (RAID). In Proc. of ACM SIGMOD, 1988.
J. S. Plank and K. M. Greenan. Jerasure: A Library in C Facilitating
Erasure Coding for Storage Applications. Technical Report UT-EECS-
14-721, University of Tennessee, EECS Department, Jan 2014.
[42]
[43] C. Ruemmler and J. Wilkes. UNIX Disk Access Patterns. In USENIX
Winter 1993 Technical Conference, 1993.
[44] R. S. Sinkovits, P. Cicotti, S. Strande, M. Tatineni, P. Rodriguez,
N. Wolter, and N. Balac. Data Intensive Analysis on the Gordon High
Performance Data and Compute System. In Proc. of ACM KDD, 2011.
[45] D. Skourtis, D. Achlioptas, N. Watkins, C. Maltzahn, and S. Brandt.
Flash on Rails: Consistent Flash Performance through Redundancy. In
Proc. of USENIX ATC, 2014.
[46] G. Soundararajan, V. Prabhakaran, M. Balakrishnan, and T. Wobber.
Extending SSD Lifetimes with Disk-based Write Caches. In Proc. of
USENIX FAST, 2010.
[47] D. Stodolsky, G. Gibson, and M. Holland. Parity Logging Overcoming
the Small Write Problem in Redundant Disk Arrays. In Proc. of ISCA,
1993.
[48] K. Thomas. Solid State Drives No Better Than Others, Survey Says.
www.pcworld.com/article/213442.
[49] M. Zheng, J. Tucek, F. Qin, and M. Lillibridge. Understanding the
In Proc. of USENIX FAST,
Robustness of SSDs under Power Fault.
2013.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply.