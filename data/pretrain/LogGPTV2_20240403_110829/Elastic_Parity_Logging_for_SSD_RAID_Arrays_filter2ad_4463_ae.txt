### (b) GC Overhead

**Figure 10: Experiment 4: Total Size of Write Traffic to SSDs and GC Overhead Under Different Parity Commit Cases and (6+2)-RAID-6 Setting.**

### Experiment 5: I/O Performance
The previous experiments examined the write size and the number of garbage collection (GC) requests. In this experiment, we evaluate the I/O throughput of EPLOG, measured as the number of user-level requests issued to the SSDs divided by the total time (in units of KIOPS). Note that the total time includes the overheads of writes to log devices. We replay each trace as fast as possible to obtain the maximum possible performance.

**Figure 11: I/O Throughput Results.**
- **Figure 11(a):** Under (6+2)-RAID-6, EPLOG outperforms MD by 30.1-119.2% and PL by 186.9-305.5% across different traces.
- **Figure 11(b):** Under the FIN trace, EPLOG outperforms MD by 119.2-197.3% and PL by 295.7-366.1% across different RAID settings.

Both MD and PL read data before updating or logging parity on the update path. MD achieves higher throughput than PL because it directly updates parities on SSDs, while PL logs parity updates to HDD-based log devices for endurance. EPLOG eliminates pre-reads of existing data in log chunk computation, thereby increasing I/O throughput. Additionally, EPLOG reduces the total size of log chunks by 8-15% compared to PL due to elastic parity logging, which also leads to throughput gains.

### Experiment 6: Metadata Management Overheads
We now evaluate the overheads of metadata checkpoint operations (see Section III-E). We consider a scenario where metadata is generated after a large number of random writes. We use IOzone [16] to first create continuous stripes covering an 8GB area on SSD RAID using sequential writes, and then issue uniform random updates of 4KB each across all stripes. We measure the total size of write traffic to SSDs under three cases:
1. Full checkpoint after stripe creation.
2. Incremental checkpoint after all stripe updates.
3. Full checkpoint after all stripe updates.

**Table II: Total Sizes of Write Traffic to SSDs with/without Metadata Checkpoint Operations.**
- **Stripe Creation without Checkpoint:** 10.922 GB
- **Full Checkpoint after Stripe Creation:** 10.961 GB (+0.36%)
- **Stripe Update without Checkpoint:** 8.147 GB
- **Incremental Checkpoint after Stripe Update:** 8.294 GB (+1.81%)
- **Full Checkpoint after Stripe Update:** 8.331 GB (+2.25%)

Note that stripe creation issues new full-stripe writes, so EPLOG writes them to SSDs. The total write size is around 11GB, including parity writes. During stripe updates, EPLOG redirects parities to the log devices, and the total write size drops to around 8GB. Overall, the metadata checkpoint overhead in write size is at most 2.25%. The incremental checkpoint operation only writes dirty metadata after updates, and its overhead is less than that of the full checkpoint operation. The results show that EPLOG incurs low overheads in metadata management.

### VI. Related Work
Researchers have proposed various techniques for enhancing the performance and endurance of single SSDs, such as disk-based write caching [46], read/write separation via redundancy [45], and flash-aware file systems (e.g., [23], [24], [30], [34]). EPLOG targets an SSD RAID array and is currently implemented as a user-level block device. It can incorporate advanced techniques from existing flash-aware designs, such as hot/cold data grouping [24], [34] and efficient metadata management [23], [30], for further performance and endurance improvements.

Flash-aware RAID designs have been proposed at the chip level [10], [15], [22] or at the device level [3], [26], [28], [31], [39], [40]. For example, Greenan et al. [10] keep outstanding parity updates in NVRAM and defer them until a full stripe of data is available. FRA [28] also defers parity updates but keeps outstanding parity updates in DRAM, which is susceptible to data loss. Balakrishnan et al. [3] propose unevenly distributing parities among SSDs to avoid correlated failures. Lee et al. [26] and Im et al. [15] propose the partial parity idea, which generates parity chunks from partial stripes and maintains the parity chunks in NVRAM. HPDA [31] builds an SSD-HDD hybrid architecture that keeps all parities in HDDs and uses the HDDs as write buffers. Kim et al. [22] propose an elastic striping method that encodes newly written data to form new data stripes and writes the data and parity chunks directly to SSDs without NVRAM. Pan et al. [40] propose a diagonal coding scheme to address the system-level wear-leveling problem in SSD RAID, and the same research group [39] extends the elastic striping method by Kim et al. [22] with a hotness-aware design.

EPLOG relaxes the constraints of parity construction, allowing parity to be associated with a partial stripe, following the rationale of previous work [15], [22], [26], [39]. Compared to previous work, EPLOG keeps log chunks with elastic parity logging using commodity HDDs rather than NVRAM, as in [15], [26]. Instead of directly writing parity chunks to SSDs [22], [39], EPLOG keeps log chunks in log devices to limit parity write traffic to SSDs, especially when synchronous writes are needed (see Section II-A). While HPDA [31] also uses HDDs to keep parities, it always keeps all parities in HDDs and treats HDDs as a write buffer, without explaining how parities in HDDs are generated and stored. In contrast, EPLOG ensures sequential writes of log chunks to HDD-based log devices and regularly performs parity commit in SSDs (note that parity commit does not need to access log devices in normal mode). Additionally, EPLOG employs an elastic logging policy, which does not require pre-reading old data chunks and relaxes the constraint of per-stripe basis in computing parity logs, reducing the amount of logs and fully utilizing device-level parallelism among SSDs. EPLOG targets general RAID schemes that tolerate a general number of failures, unlike most existing approaches that assume single fault tolerance.

### VII. Conclusions
We present EPLOG, a user-level block device that mitigates parity update overhead in SSD RAID arrays through elastic parity logging. The idea is to encode new data chunks to form log chunks and append the log chunks into separate log devices, while the data chunks may span a partial stripe or across more than one stripe. We carefully build our EPLOG prototype on commodity hardware and evaluate it through reliability analysis and testbed experiments. We show that EPLOG improves reliability, endurance, and performance. The source code of EPLOG is available at http://ansrlab.cse.cuhk.edu.hk/software/eplog.

### Acknowledgments
This work was supported in part by the National Natural Science Foundation of China (61303048 and 61379038), Anhui Provincial Natural Science Foundation (1508085SQF214), CCF-Tencent Open Research Fund, the University Grants Committee of Hong Kong (AoE/E-02/08), and the Research Committee of CUHK.

### References
[References are listed as provided, with no changes made.]