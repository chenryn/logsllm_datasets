elapsed time and system time, and a 108% increase in the
wait time over Ext2. Since the elapsed and system times
are similar, it is not possible to quantify for the increase
in wait time.
For Ext2Ver(md), we recorded a 7% increase in
elapsed time, and a 41% increase in system time over
Ext2. The increase in system time overhead is due to the
additional hash table lookups by SVL to remap the read
and write requests. Ext2Ver(md) consumed 496KB of
additional disk space to store the versions.
For Ext2Ver(all), we recorded a 7% increase in
elapsed time, and a 39% increase in system time over
Ext2. Ext2Ver(all) consumes 15MB of additional space
to store the versions. The overhead of storing versions
is 95%. From this benchmark, we can clearly see that
the versioning all data inside the disk is not very useful,
especially for program installers.
USENIX Association  
17th USENIX Security Symposium 
271
6.4 Kernel Compile
7 Related Work
To simulate a CPU-intensive user workload, we com-
piled the Linux kernel source code. We used a vanilla
Linux 2.6.15 kernel and analyzed the overheads of
Ext2TSD on a TSD and Ext2TSD on SVSDS with ver-
sioning of all blocks and selective versioning of meta-
data blocks against regular Ext2, for the untar, make
oldconfig, and make operations combined. We used
30 second versioning interval and 78 versions were cre-
ated during this benchmark. The results are shown in
Figure 8.
)
s
d
n
o
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
2467.2
2460.8
2470.6
2486.3
Wait
User
System
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
Ext2
Ext2TSD
Ext2Ver(md)
Ext2Ver(all)
Ext2 Ext2TSD Ext2Ver(md) Ext2Ver(all)
Elapsed
System
Wait
2467s
162s
72.1s
Space o/h 0MB
2461s
167s
54.7s
0MB
2471s
169s
68.0s
51MB
Performance Overhead over Ext2
Elapsed
System
Wait
-
-
-
-0.26 %
3.6%
-24%
0.13%
4.7%
-5.6%
2468s
177s
71.6s
181MB
0.77%
10%
-0.8%
Figure 8: Kernel Compile results for SVSDS.
For Ext2TSD, elapsed time is observed to be the same,
system time overhead is 4% lower and wait time is lower
by 24% than that of Ext2. The decrease in the wait time
is because Ext2TSD does not consider future growth of
ﬁles while allocating new blocks.
For Ext2Ver(md), elapsed time is observed to be the
same, system time overhead is 5%, and wait time is lower
by 6% than that of Ext2. The increase in wait time in re-
lation to ext2TSD is due to versioning meta-data blocks
which affect the locality of the stored ﬁles. The space
overhead of versioning meta-data blocks is 51 MB.
For Ext2Ver(all), elapsed time is observed to be indis-
tinguishable, system time overhead is 10% higher than
that of Ext2. The increase in system time is due to the ad-
ditional hash table lookups required for storing the map-
ping information in the V-TABLE. The space overhead of
versioning all blocks is 181 MB.
SVSDS borrows ideas from many of the previous works.
The idea of versioning at the granularity of ﬁles has been
explored in many ﬁle systems [6, 10, 12, 15, 19]. These
ﬁle systems maintain previous versions of ﬁles primarily
to help users to recover from their mistakes. The main
advantage of SVSDS over these systems is that, it is de-
coupled from the client operating system. This helps in
protecting the versioned data, even in the event of an in-
trusion or an operating system compromise. The virtu-
alization of disk address space has been implemented in
several systems [3, 7, 9, 13, 21]. For example, the Log-
ical disk [3] separated the ﬁle-system implementation
from the disk characteristics by providing a logical view
of the block device. The Storage Virtualization Layer
in SVSDS is analogous to their logical disk layer. The
operation-based constraints in SVSDS is a scaled down
version of access control mechanisms. We now compare
and contrast SVSDS with other disk-level data protection
systems: S4 [20], TRAP [23], and Peabody [7].
The Self-Securing Storage System (S4) is an object-
based disk that internally audits all requests that arrive
at the disk. It protects data in compromised systems by
combining log-structuring with journal-based meta-data
versioning to prevent intruders from tampering or per-
manently deleting the data stored on the disk. SVSDS
on the other hand, is a block-based disk that protect data
by transparently versioning blocks inside the disk. The
guarantees provided by S4 hold true only during the win-
dow of time in which it versions the data. When the disk
runs out of storage space, S4 stops versioning data un-
til the cleaner thread can free up space for versioning
to continue. As S4 is designed to aid in intrusion di-
agnosis and recovery, it does not provide any ﬂexibility
to users to version ﬁles (i.e, objects) inside the disk. In
contrast, SVSDS allows users to select ﬁles and direc-
tories for versioning inside the disk. The disadvantage
with S4 is that, it does not provide any protection mech-
anism to prevent modiﬁcations to stored data during in-
trusions and always depends on the versioned data to re-
cover from intrusions. In contrast, SVSDS attempts to
prevent modiﬁcations to stored data during intrusions by
enforcing operation-based constraints on system and log
ﬁles.
Timely Recovery to any Point-in-time (TRAP) is a
disk array architecture that provides data recovery in
three different modes. The three modes are: TRAP-1
that takes snapshots at periodic time intervals; TRAP-
3 that provides timely recovery to any point in time at
the block device level (this mode is popularly known as
Continuous Data Protection in storage); TRAP-4 is sim-
ilar to RAID-5, where a log of the parities is kept for
each block write. The disadvantage with this system is
272 
17th USENIX Security Symposium 
USENIX Association
that, it cannot provide TRAP-2 (data protection at the
ﬁle-level) as their block-based disk lacks semantic infor-
mation about the data stored in the disk blocks. Hence,
TRAP ends up versioning all the blocks. TRAP-1 is
similar to our current implementation where an adminis-
trator can choose a particular interval to version blocks.
We have implemented TRAP-2, or ﬁle-level versioning
inside the disk as SVSDS has semantic information about
blocks stored on the disk through pointers. TRAP-3 is
similar to the mode in SVSDS where the time between
creating versions is set to zero. Since SVSDS runs on
a local disk, it cannot implement the TRAP-4 level of
versioning.
Peabody is a network block storage device, that vir-
tualizes the disk space to provide the illusion of a sin-
gle large disk to the clients. It maintains a centralized
repository of sectors and tries to reduce the space utiliza-
tion by coalescing blocks across multiple virtual disks
that contain the same data. This is done to improve the
cache utilization and to reduce the total amount of stor-
age space. Peabody versions data by maintaining write
logs and transaction logs. The write logs stores the pre-
vious contents of blocks before they are overwritten, and
the transaction logs contain information about when the
block was written, location of the block, and the con-
tent hashes of the blocks. The disadvantage with this ap-
proach is that it cannot selectively versions blocks inside
the disk.
8 Conclusions
Data protection against attackers with OS root privileges
is fundamentally a hard problem. While there are nu-
merous security mechanisms that can protect data under
various threat scenarios, only very few of them can be ef-
fective when the OS is compromised. In view of the fact
that it is virtually impossible to eliminate all vulnerabil-
ities in the OS, it is useful to explore how best we can
recover from damages once a vulnerability exploit has
been detected. In this paper, we have taken this direc-
tion and explored how a disk-level recovery mechanism
can be implemented, while still allowing ﬂexible policies
in tune with the higher-level abstractions of data. We
have also shown how the disk system can enforce simple
constraints that can effectively protect key executables
and log ﬁles. Our solution that combines the advantages
of a software and a hardware-level mechanism proves to
be an effective choice against alternative methods. Our
evaluation of our prototype implementation of SVSDS
shows that performance overheads are negligible for nor-
mal user workloads.
Future Work . Our current design supports reverting
the entire disk state to an older version. In future, we
plan to work on supporting more ﬁne-grained recovery
policies to revert speciﬁc ﬁles or directories to their older
versions. SVSDS in its current form, relies on the admin-
istrator to detect an intrusion and revert back to a previ-
ously known safe state. We plan to build a storage-based
intrusion detection system [14] inside SVSDS. Our sys-
tem would do better than the system developed by Pen-
nington et al. [14] as we also have data dependencies
conveyed through pointers. We also plan to explore more
operation-based constraints that can be supported at the
disk-level.
9 Acknowledgments
We like to thank the anonymous reviewers for their help-
ful comments. We thank Sean Callanan and Avishay
Traeger for their feedback about the project. We would
also like to thank the following people for their com-
ments and suggestions on the work: Radu Sion, Rob
Johnson, Radu Grosu, Alexander Mohr, and the mem-
bers of our research group (File systems and Storage Lab
at Stony Brook).
This work was partially made possible by NSF CA-
REER EIA-0133589 and NSF CCR-0310493 awards.
References
[1] B. Berliner and J. Polk. Concurrent Versions Sys-
tem (CVS). www.cvshome.org, 2001.
[2] CollabNet, Inc. Subversion.
http://subversion.
tigris.org, 2004.
[3] W. de Jonge, M. F. Kaashoek, and W. C. Hsieh.
The logical disk: A new approach to improving ﬁle
systems.
In Proceedings of the 19th ACM Sym-
posium on Operating Systems Principles (SOSP
’03), Bolton Landing, NY, October 2003. ACM
SIGOPS.
[4] T. E. Denehy, A. C. Arpaci-Dusseau, and R. H.
Arpaci-Dusseau. Bridging the information gap in
storage protocol stacks. In Proceedings of the An-
nual USENIX Technical Conference, pages 177–
190, Monterey, CA, June 2002. USENIX Associ-
ation.
[5] G. R. Ganger. Blurring the Line Between OSes and
Storage Devices. Technical Report CMU-CS-01-
166, CMU, December 2001.
[6] D. K. Gifford, R. M. Needham, and M. D.
Schroeder. The Cedar File System. Communica-
tions of the ACM, 31(3):288–298, 1988.
[7] C. B. Morrey III and D. Grunwald. Peabody: The
In Proceedings of the 20 th
time travelling disk.
USENIX Association  
17th USENIX Security Symposium 
273
IEEE/11 th NASA Goddard Conference on Mass
Storage Systems and Technologies (MSS’03), pages
241–253. IEEE Computer Society, 2003.
File and Storage Technologies (FAST 2004), pages
15–30, San Francisco, CA, March/April 2004.
USENIX Association.
[19] Craig A. N. Soules, Garth R. Goodson, John D.
Strunk, and Gregory R. Ganger. Metadata efﬁ-
ciency in versioning ﬁle systems. In Proceedings of
the Second USENIX Conference on File and Stor-
age Technologies (FAST ’03), pages 43–58, San
Francisco, CA, March 2003. USENIX Association.
[20] J. D. Strunk, G. R. Goodson, M. L. Schein-
holtz, C. A. N. Soules, and G. R. Ganger. Self-
securing storage: Protecting data in compromised
systems. In Proceedings of the 4th Usenix Sympo-
sium on Operating System Design and Implemen-
tation (OSDI ’00), pages 165–180, San Diego, CA,
October 2000. USENIX Association.
[21] D. Teigland and H. Mauelshagen. Volume man-
agers in linux.
In Proceedings of the Annual
USENIX Technical Conference, FREENIX Track,
pages 185–197, Boston, MA, June 2001. USENIX
Association.
[22] Walter F. Tichy.
RCS — a system for ver-
sion control. Software: Practice and Experience,
15(7):637–654, 1985.
[23] Q. Yang, W. Xiao, and J. Ren. TRAP-array: A
disk array architecture providing timely recovery
to any point-in-time.
In Proceedings of the 33rd
Annual International Symposium on Computer Ar-
chitecture (ISCA ’06), pages 289–301. IEEE Com-
puter Society, 2006.
[8] J. Katcher. PostMark: A new ﬁlesystem bench-
mark. Technical Report TR3022, Network Ap-
pliance, 1997. www.netapp.com/tech_library/3022.
html.
[9] E. K. Lee and C. A. Thekkath. Petal: Distributed
virtual disks. In Proceedings of the Seventh Inter-
national Conference on Architectural Support for
Programming Languages and Operating Systems
(ASPLOS-7), pages 84–92, Cambridge, MA, 1996.
[10] K. McCoy. VMS File System Internals. Digital
Press, 1990.
[11] M. Mesnier, G. R. Ganger, and E. Riedel. Object
IEEE Communications Magazine,
based storage.
41, August 2003. ieeexplore.ieee.org.
[12] K. Muniswamy-Reddy, C. P. Wright, A. Himmer,
and E. Zadok. A Versatile and User-Oriented Ver-
sioning File System.
In Proceedings of the Third
USENIX Conference on File and Storage Technolo-
gies (FAST 2004), pages 115–128, San Francisco,
CA, March/April 2004. USENIX Association.
[13] D. Patterson, G. Gibson, and R. Katz. A case for
redundant arrays of inexpensive disks (RAID). In
Proceedings of the ACM SIGMOD, pages 109–116,
June 1988.
[14] A. Pennington, J. Strunk, J. Grifﬁn, C. Soules,
G. Goodson, and G. Ganger. Storage-based intru-
sion detection: Watching storage activity for suspi-
cious behavior. In Proceedings of the 12th USENIX
Security Symposium, pages 137–152, Washington,
DC, August 2003.
[15] D. J. Santry, M. J. Feeley, N. C. Hutchinson, and
A. C. Veitch. Elephant: The ﬁle system that never
forgets. In Proceedings of the IEEE Workshop on
Hot Topics in Operating Systems (HOTOS), pages
2–7, Rio Rica, AZ, March 1999.
[16] G. Sivathanu, S. Sundararaman, and E. Zadok.
Type-safe disks.
In Proceedings of the 7th Sym-
posium on Operating Systems Design and Imple-
mentation (OSDI 2006), pages 15–28, Seattle, WA,
November 2006. ACM SIGOPS.
[17] M. Sivathanu, L. N. Bairavasundaram, A. C.
Arpaci-Dusseau, and R. H. Arpaci-Dusseau. Life or
death at block-level. In Proceedings of the 6th Sym-
posium on Operating Systems Design and Imple-
mentation (OSDI 2004), pages 379–394, San Fran-
cisco, CA, December 2004. ACM SIGOPS.
[18] M. Sivathanu, V. Prabhakaran, A. C. Arpaci-
Improving
Dusseau, and R. H. Arpaci-Dusseau.
storage system availability with D-GRAID.
In
Proceedings of the Third USENIX Conference on
274 
17th USENIX Security Symposium 
USENIX Association