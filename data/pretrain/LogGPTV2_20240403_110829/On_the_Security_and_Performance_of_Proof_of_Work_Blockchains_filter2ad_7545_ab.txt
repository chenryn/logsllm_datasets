spending and selﬁsh mining. We then use these strategies as a basis
to compare the security provisions of PoW-based blockchains when
instantiated with different parameters.
3.1 Security Model
Our model extends the Markov Decision Process (MDP) of [32]
to determine optimal adversarial strategies, and captures:
Stale block rate The stale block rate rs allows us to account for
different block sizes, block intervals, network delays, infor-
mation propagation mechanisms and network conﬁguration
(e.g., number of nodes).
adversary (the rest is controlled by the honest network).
Mining power α is the fraction of the total mining power of the
Mining costs The adversarial mining costs cm ∈ [0, α] correspond
to the expected mining costs of the adversary (i.e., total min-
ing costs such as hardware, electricity, man-power) and are
expressed in terms of block rewards. For example, if cm = α,
5the mining costs of the adversary are equivalent to its min-
ing power times the block reward, i.e., the mining costs are
covered exactly by the earned block revenue in honest mining.
The number of block conﬁrmations k This corresponds to the num-
ber of blocks that need to conﬁrm a transaction, such that a
merchant accepts the transaction.
Propagation ability The propagation parameter γ captures the con-
nectivity of the adversary within the network (i.e., captures
the fraction of the network that receives the adversary’s blocks
in the case when the adversary and the honest miner release
their blocks simultaneously in the network).
The impact of eclipse attacks Our model accounts for eclipse at-
tacks. Here, we assume that the miners of the honest network
are affected by the stale block rate, while the adversary and
the colluding eclipsed victims do not mine stale blocks. This
is due to the fact that the adversary can use any mined blocks
for an attack and effectively only has a small chance of min-
ing a stale block after adopting the honest chain. Therefore,
in practice, the adversary exhibits a signiﬁcantly lower real
stale block rate than the honest network. The honest network
features propagation and validation delays—hence it will wit-
ness a higher stale block rate. Note that the blocks found by
the eclipsed victim can also count towards the private chain
of the adversary.
We contrast this to existing models, such as Sapirshtein et al.’s [32],
which only focus on selﬁsh mining and cannot capture different
blockchain instances (with various stale block rates and conﬁrma-
tions) and real-world parameters such as network delays.
To analyze optimal double-spending strategies, we deﬁne the
double-spending amount vd that corresponds to the minimum trans-
action value that makes double-spending more proﬁtable than honest
mining. We argue that vd emerges as a robust metric to quantify secu-
rity under double-spending attacks. Namely, if the reward of honest
mining is larger than that of dishonest behaviour, merchants can
safely accept a payment transaction of value vd (since such a value
is considered secure, e.g., based on a given conﬁrmation number).
If however, adversarial behaviour is ﬁnancially more rewarding, a
merchant should be aware of the associated double-spending risks
and of the related incentives of miners.
We capture the blockchain model using a single-player decision
problem M := (cid:104)S, A, P, R(cid:105) where all other participants follow the
standard protocol, and S corresponds to the state space, A to the
action space, P to the stochastic transition matrix, and R to the
reward matrix. We instantiate M as a Markov Decision Process
(MDP) as outlined in Section 3.2 and 3.3.
In our model, the following actions are available to the adversary:
Adopt The adversary accepts the chain of the honest network,
which effectively corresponds to a restart of the attack. This
action is appropriate if the adversary deems that the likelihood
to win over the honest chain is small.
Override The adversary publishes one block more than the honest
chain has and consequently overrides conﬂicting blocks. This
happens when the adversary’s secret chain is longer than the
currently known public chain (i.e. la > lh) and it is optimal
for the adversary to publish lh + 1 of his blocks to replace the
honest network’s chain with his own. If the adversary exploits
the mining power of the victim, the adversary might use be
blocks from the victim for an override action.
Match The adversary publishes as many blocks as the honest chain
has, and triggers an adoption race between the two chains
instead of overriding the honest chain.
Wait The adversary continues mining on its hidden chain until a
block is found.
Exit This action is only relevant when studying double-spending
as it corresponds to a successful double-spending with k con-
ﬁrmations and is only feasible if la > lh and la > k.
The state space S is deﬁned as a four-tuple of the form (la, lh, be, fork),
where la and lh represent the length of the adversarial and honest
chain respectively, be the blocks mined by the eclipsed victim, and
fork can take three values, irrelevant, relevant and active:
relevant The label relevant signiﬁes that (i) the last block has been
found by the honest network, and (ii) if la ≥ lh the match
action is applicable. A state of the form (la, lh − 1, be,·) for
instance results in (la, lh, be, relevant).
irrelevant When the adversary found the last block, the previous
block has likely already reached the majority of the nodes in
the network. The adversary is therefore not able to perform a
match action. A state of the form (la−1, lh, be,·) for instance
results in (la, lh, be, irrelevant).
active The state is described with the label active, if the adversary
performed a match action, i.e., the network is currently split
and in process of determining the longest chain.
In our model, every state transition (except exit) corresponds to
the creation of a block. Consequently, a state transition implies a
reward for the honest network, the adversary, or the eclipsed victim.
Given the adversarial mining power α, the initial state (0, 0, 0,
irrelevant) transitions to (1, 0, 0, irrelevant) with probability α,
i.e., the adversary found one block. If the honest network ﬁnds
a non-stale block, the resulting state is (0, 1, 0, relevant). On the
other hand, if the honest network’s block results in a stale block, the
state remains (0, 0, 0, irrelevant) since a stale block does not count
towards the longest chain. The last case accounts for the eclipsed
victim which ﬁnds a block with probability ω, resulting in state
(1, 0, 1, irrelevant).
Selﬁsh Mining vs. Double-spending.
In this work, we consider double-spending and selﬁsh mining
independently, since selﬁsh mining is not always a rational strategy:
the objective of selﬁsh mining is to increase the relative share of the
adversarial blocks committed to the main chain, while in double-
spending the adversary aims to maximize his absolute revenue.
Namely, as long as the difﬁculty of a PoW blockchain does
not change (e.g. Bitcoin’s difﬁculty changes only once every two
weeks), selﬁsh mining yields fewer block rewards than honest min-
ing. In honest mining, the adversary is rewarded for every mined
block, while he will lose any previously mined blocks when adopt-
ing the main chain in selﬁsh mining. Since the adversary has less
mining power than the honest network, he has a high probability of
falling behind the main chain, causing him to adopt the main chain
when he has no signiﬁcant chance of catching up—which in turn
leads to lost block rewards. For instance, following our optimal
selﬁsh mining strategy (cf. Section 3.2), an adversary with 30% of
the mining power earns 209 block rewards on average in a duration
where 1000 blocks are mined by the whole network (as opposed
to 300 for honest mining). Similarly, Eyal and Sirer’s [15] strategy
yields on average 205.80 blocks rewards.
Eclipse attacks.
In an eclipse attack, a fraction ω of the overall mining power
is eclipsed [19, 29] from receiving information from the honest
network. Here, a number of eclipse attack variants arise:
No eclipse attack This case is captured in our model if ω = 0.
6Isolate the victim This is captured implicitly in our model. Namely,
this corresponds to a decrease of the total mining power and
thus an increase of the attacker mining power to α(cid:48) = α
1−ω .
Exploit the eclipsed victim Here, the adversary exploits the vic-
tim’s mining power ω and uses it to advance his private chain.
This is the most likely choice of a rational adversary when
performing double-spending attacks. In this case, we assume
that the victim is fully eclipsed from the network and does not
receive/send blocks unless permitted by the adversary [19,29].
3.2 Selﬁsh Mining MDP
Our goal is to ﬁnd the optimal adversarial strategy for selﬁsh
mining. Recall that the objective of the adversary in selﬁsh mining
is not to optimise the absolute reward, but to increase the share
of blocks that are included in the chain accepted by the network.
We capture this by optimising the relative revenue rrel as deﬁned
in Equation 1, where rai and rhi are the rewards in step i for the
adversary and the honest network, respectively:
(cid:20)
(cid:80)n
(cid:80)n
rrel = E
lim
n→∞
i=1 rai
i=1(rai + rhi )
(cid:21)
(1)
Since an adversary aims to increase his relative reward rrel (Equa-
tion 1) in selﬁsh mining, as opposed to the absolute reward, the
single-player decision problem cannot be modelled directly as an
MDP, since the reward function is non-linear. In order to trans-
form the problem into a family of MDPs, we adapt the technique of
Sapirshtein et al.’s [32], which we describe below.
We assume that the value of the objective function (i.e., the op-
timal relative reward) is rho and deﬁne for any ρ ∈ [0, 1] the
transformation function wρ : N2 → R with the adversarial reward
ra and the reward of the honest network rh in Equation 2.
wρ(ra, rh) = (1 − ρ) · ra − ρ · rh
(2)
This results in an inﬁnite state MDP Mρ = (cid:104)S, A, P, wρ(R)(cid:105)
for each ρ that has the same action and state space as the original
decision problem and the same transition matrix but the reward
matrix is transformed using wrho. The expected value of such an
MDP under policy π is then deﬁned by vπ
ρ in Equation 3, where
ri(π) is the reward tuple in step i under policy π.
(cid:34)
n(cid:88)
i=1
ρ = E
vπ
lim
n→∞
1
n
wρ(ri(π))
(cid:35)
The expected value under the optimal policy is then given :
(cid:8)vπ
ρ
(cid:9)
v
∗
ρ = max
π∈A
We base our method to optimise rrel on the following proposi-
tions [32]:
ρ = 0 for some ρ ∈ [0, 1], then an optimal policy π∗ in
1. If v∗
the transformed MDP Mρ also maximises rrel and rrel = ρ.
2. v∗
ρ is monotonically decreasing in ρ.
Since standard MDP solvers are not able to solve inﬁnite state
MDPs, we restrict the state space of our family of MDPs by only
allowing either chain to be of length at most c, resulting in a ﬁnite
state MDP M c
ρ. If either chain reaches length c, the adversary is
only allowed to perform the override or adopt action. This gives a
lower bound for the optimal value of the inﬁnite state MDP.
Intuitively, one can reason about the correctness of the ﬁrst propo-
sition as follows for the bounded single-player decision problem. In
a recurring ﬁnite state MDP, the initial state will be visited again in
(3)
(4)
expectation after some ﬁnite number of steps S. During that time,
and
the adversary gains an expected reward of Ra = E(cid:104)(cid:80)S
the honest network gains a reward of Rh = E(cid:104)(cid:80)S
reward per step in the Markov Chain is ra = E(cid:104) 1
(cid:80)S
rh = E(cid:104) 1
in the
original (bounded) decision problem. It follows that the expected
and
for the adversary and the honest network,
respectively. We can thus simplify the expected relative revenue
rrel to:
(cid:80)S
(cid:105)
(cid:105)
s=1 rhi
s=1 rhi
s=1 rai
s=1 rai
(cid:105)
(cid:105)
S
S
(cid:20)
(cid:20)
(cid:20)
(cid:20)
(cid:21)
(cid:80)n
(cid:80)n
i=1 rai
i=1(rai + rhi )
n · ra
(cid:21)
lim
n→∞
(cid:21)
lim
n→∞
n · (ra + rh)
ra
(cid:21)
(ra + rh)
lim
n→∞
ra
rrel = E
= E
= E
= E
ra + rh
ra
=
ra + rh
(cid:33)(cid:35)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
(19)
(20)
(21)
(22)
(23)
Additionally, we note the following:
((1 − ρ) · rai − ρ · rhi )
(cid:35)
))
∗
i=1
wρ(ri(π
n(cid:88)
n(cid:88)
(cid:32)
(1 − ρ) · n(cid:88)
i=1
i=1
1
n
1
n
1
n
v
ρ
= E
(cid:34)
ρ = vπ∗
∗
(cid:34)
= E
(cid:34)
(cid:20)
(cid:20)
= E
= E
lim
n→∞
lim
n→∞
lim
n→∞
lim
n→∞
(cid:35)
rai − ρ · n(cid:88)
(cid:21)
i=1
rhi
(cid:21)
(n · (1 − ρ) · ra − n · ρ · rh)
1
n
((1 − ρ) · ra − ρ · rh)
lim
n→∞
= E
= E [(1 − ρ) · ra − ρ · rh]
= (1 − ρ) · ra − ρ · rh
And thus, for the case where ρ = rrel = ra
ra+rh
: