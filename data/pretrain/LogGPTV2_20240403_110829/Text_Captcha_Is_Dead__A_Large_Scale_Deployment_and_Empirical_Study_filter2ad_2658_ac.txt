tions in each step. For an iterative method containing s steps, the
total amount of perturbations in the iterative method is ϵ = s × α.
In the iterative method, the default step number s is 10 and the
default perturbation level α is one-tenth of ϵ.
At last, in order to generate more randomized adversarial captchas,
we propose a mixture method to inject various perturbations into a
captcha image, where
x′ = x +
GeneratePerturbation(L(θ, x, y), p, ϵ, α, Mi)
(3)
k
i =1
We show the process of the mixture method in Algorithm 1. Basi-
cally, we first randomly choose k masks. Note that different per-
turbations inserted to the same area may conflict with each other,
thus we make sure these multiple masks have no spatial overlap.
For each mask, adversarial perturbations are generated by differ-
ent methods (direct and iterative methods) and parameter settings
(different distance measures and perturbation levels). Finally, we in-
tegrate these perturbations together and inject them to the original
captcha.
3.4 Querying the Attack Model
Next, we need to distribute adversarial captchas to attackers and
record their answers. It seems paradoxical that if we could figure
out which users were attackers, we could simply deny the access
of them instead of sending them captchas. We emphasize that 1)
this step is optional which is only conducted when the substitute
model needs to be fine-tuned; 2) in practice, we can only figure
out a small part of high-confidence attackers. By engaging with
these high-confidence attackers, we can improve the security of
adversarial captchas and eventually defend against more potential
attackers. Now, a key challenge here is to recognize attackers.
6
Algorithm 1: Mixture method.
1 i = 1 ;
2 while i ≤ k do
Randomly choose a mask Mi ;
Set distance metric p;
if direct method then
Set perturbation level ϵ;
Compute perturbation ∆xi by Equation 1;
if iterative method then
Set step number s;
Set perturbation level α;
Compute perturbation ∆xi by Equation 2;
3
4
5
6
7
8
9
10
11
12 ∆x =k
13 x′ = x + ∆x
i =1 ∆xi ;
We do not submit adversarial captchas to the commercial ser-
vices to be solved, since we do not know which commercial services
are employed by the attackers. Instead, we take advantage of ex-
isting user risk analysis system [36] to identify high-confidence
attackers. When a user operation needs to be judged, the front-end
web collects the user’s behavior information and the environment
information and submits them to the risk control engine at the
back-end. The follow-up risk control engine will make a decision
based on the features of these collected information, such as mouse
movement, IP address and Cookie. It can determine whether to
block the operation or make a second judgment. We note that the
user risk analysis system in our work is different from [36]. In
addition to the user environment information and high frequency
information (which is similar to [36]), our user risk analysis system
pays more attention to user behavior information, e.g., mouse slid-
ing and page detention time, which is more difficult to forge and
thus more reliable.
We also worthy to note that identifying high-confidence attack-
ers may not be a real time process. In practice, we send captchas
to the risk users (including high-confidence and low-confidence
attackers). At the same time, we record their answers and corre-
sponding features which are collected by the risk analysis system.
After a few days (e.g., a week) deployment, the domain experts
can further analyze the records to identify the high-confidence
attackers by leveraging multiple tools and domain knowledge. In
this way, we can recognize attackers with high accuracy. We detail
this process in Appendix B. Finally, we select answers submitted by
identified attackers and view them as queries to the attack model.
3.5 Fine-tuning the Substitute Model
The final step is to fine-tune the substitute model. Our fine-tuning
process aims to make the substitute model fit the collected query
data, as well as maintain accuracy for the original captcha data.
Therefore, we formulate a multi-objective optimization task by
optimizing the weighted sum of the two objectives.
L(θ, xq , yq) + λ · L(θ, xo, yo)
(4)
where (xq , yq) is the query data generated by step 3 and (xo, yo) is
the original training data used by step 1. θ are substitute model pa-
rameters, e.g., weights and bias. L(·) is the loss function measuring
the error in classification, which is the CTC loss in our experiment.
λ is the weight to balance the tradeoff between the loss of the new
minimize
θ
query data and the original captcha data. In our experiments, we
adjust λ dynamically to ensure that a reasonable proportion of the
original training data can be correctly recognized. We use the Adam
optimizer [22] to effectively solve Equation 4.
Note that in order to improve the efficiency of the fine-tune
process, we query the attack model by adversarial captchas rather
than ordinary ones. The reasons are as follows. Suppose that xq is a
captcha used in the query process, y′ = FΘ(xq) is the prediction of
the substitute model for xq, and y′′ is the prediction of the actual
attack model for xq. In the fine-tuning process, we aim to make the
substitute model fit the collected query data. In other words, we
leverage the difference between y′ and y′′ to update the substitute
model. If we use ordinary captchas to query the attack model, for
most cases, y′′ is the same as y′ since both the attack model and the
substitute model have high recognition rates for ordinary captchas.
Such queries are thus less informative to fine-tune the substitute
model. In comparison, leveraging adversarial captchas to query
the attack model, which, according to our observations, usually
generates different y′ and y′′. Therefore, under the same num-
ber of queries, compared to ordinary captchas, using adversarial
captchas can achieve better model fitting performance. Experiment
evaluations in Section 6.5 confirm this conclusion.
Moreover, the accuracy of the updated substitute model may
be degraded especially when the labels in the query datasets are
deviated from the ground truth. To overcome this issue, we add
the second objective of maintaining the accuracy of the original
training data to Equation 4. In Sections 5.3, 6.1, 6.2, we verify the
effectiveness of the fine-tuning process and show its robustness
against various attack models.
4 DEPLOYMENT AND EVALUATION SETUP
4.1 Application Platform
We deploy advCAPTCHA on a large-scale international commercial
platform, which provides a broad range of services for corporations
and individual users. The current number of commercial users
has exceeded 1 million, covering government, consumer, aviation,
financial and other fields, and the current number of monthly active
users is 824 million.
During online studies, we only conduct experiments on a website
provided by the cooperative company for the sake of risk control.
This website is a government service website from which some
public information (such as traffic violations, driving license, etc.)
can be queried. When a user logins account or queries the informa-
tion, he/she must pass the human machine recognition test. From
the user’s point of view, he/she needs to slide a bar firstly, which is
similar to unlocking the phone screen. Then the user risk analysis
system will judge the risk level of the user and determine whether
to send him/her a captcha. The captcha scheme deployed on this
website is text captcha consisting of four characters, with merged
characters while no background noise. According to domain ex-
perts in the company, the website is approximately suffered from
100,000 to 1,000,000 attacks per day.
4.2 Deployment
We deploy advCAPTCHA on the above platform as follows. 1) Our
cooperative company generates N ordinary captchas each week
7
Figure 2: Twelve captcha schemes used in our paper. We use all the captcha
schemes in the training phase and only one scheme (as shown in the left
which is deployed in the real world platform) for the test phase.
by its existing captcha generation algorithm, where N = 100, 000
in our evaluation for each experiment. 2) According to the algo-
rithms in Section 3, we leverage the substitute model to generate
N adversarial captchas from the N ordinary captchas. 3) When
there is an authentication request, i.e., the risk analysis system
determine he/she is a risky user (including high-confidence and
low-confidence), we send the adversarial captcha to the user. At the
same time, we record his/her corresponding features and answer.
4) Each week, the domain experts involve and further analyze the
stored features to confirm the high-confidence attackers. This pro-
cess is detailed in Appendix B. 5) When we observe an update of the
attack model, e.g., the overall success rate of the attack increases
sharply, we use the answers submitted by the high-confidence at-
tackers to fine-tune our substitute model. The whole process of
deployment and evaluation continues for 9 months, based on which
we formulate our analysis for online user studies in Section 5. Note
that, only the results submitted by these high-confidence attackers
are used to measure the security of our adversarial captchas.
4.3 Evaluation Setup
In total, we use 12 different captcha schemes in the evaluation,
where 7 of them are real captcha schemes provided by our coop-
erative company and the other 5 schemes are from open source
softwares [3]. These schemes can generate captchas that contain
different background, different length and other defense factors. We
show the example captchas generated by the employed schemes in
Figure 2. In the training phase, we use all the 12 captcha schemes to
train the substitute model, which can make the solver more general.
In the test phase, i.e., online study, we only use one captcha scheme
(the actually used captcha scheme by the cooperated company) to
generate adversarial captchas, since our objective is to protect the
captcha scheme that is deployed on the real website.
5 EVALUATION
In this section, we conduct a large-scale online empirical study to
evaluate the performance of advCAPTCHA. Firstly, we analyze the
behavior of the identified attackers in online experiments. Then, we
show the effectiveness of advCAPTCHA, as well as its robustness
under different model parameters and different substitute models.
5.1 Analysis of Attackers (High Risk Users)
In the captcha security scenario, attackers are defined as the ones
that solve captchas by using well-trained ML models or other well-
designed automatic means. Thus, they may exhibit different behav-
iors from normal users.
Different Answer Lengths. When providing captcha solving ser-
vice, a captcha solver usually receives captchas generated from
different schemes, e.g., different font styles, different background,
and captchas with different lengths. In order to maximize profits,
Table 3: Classification accuracy of 6 model structures. Classifica-
tion accuracy indicates the SR of the model for ordinary captchas.
NO.
1
2
3
4
5
6
ResNet50+LSTM+CTC
LeNet+Bi-LSTM+CTC
ResNet50+Bi-LSTM+CTC
Model Structure
LeNet+LSTM+CTC
LeNet
ResNet50
Classification Accuracy
92.4%
94.1%
93.5%
93.2%
82.3%
75.6%
Figure 3: The incorrect recognition rate for different sets of adver-
sarial captchas.
Figure 4: Examples of adversarial perturbations located around a
specific character, which would impact human recognition.
captcha solvers in practice usually train a model that can be used
for attacking different captcha schemes. Therefore, such model
may return answers with different lengths even for a fixed-length
captcha scheme. In comparison, for the answers submitted by hu-
mans, whether correct or not, the corresponding character length
is very likely to be correct, especially for the fixed-length captcha
scheme. Thus it is possible that the length distribution of the an-
swers submitted by bots and real users has difference. Therefore,
in the experiment, we 1) leverage the substitute model to generate
adversarial captchas, 2) select 100,000 adversarial captchas that
have abnormal length based on the substitute model’s prediction, 3)
send these adversarial captchas to the the attackers and the normal
users, and 4) measure the lengths of their answers. The results are
shown in Figure 3. We observe that the lengths of answers submit-
ted by 76% of the attackers are different from four as defined by
our captcha scheme, while only 2% of the normal users return an-
swers with different lengths. This phenomenon also demonstrates
that the captcha solvers developed by current attackers are very
likely end-to-end models, otherwise attackers would return the
same length of answers for the fixed-length captcha scheme.
Different Performance to Adversarial Captchas. ML models
are vulnerable to adversarial examples, while in comparison hu-
man beings are rarely influenced (this can be assured in the design
of adversarial examples, which takes the usability/utility into ac-
count). In this experiment, we first leverage the substitute model
to generate 100,000 adversarial captchas. Based on the substitute
model’s prediction, we classified these adversarial captchas into
two categories. 1) Wrong-character captchas: the captcha that has
only one character different from the ground truth. e.g., 2DUA is
recognized as 2DUS. 2) Wrong-answer captchas: the captcha that
has more than one characters different from the ground truth. e.g.,
Agz3 is recognized as NGE3. Then we send these captchas to the
attackers and the normal users, and Figure 3 shows the recogni-
tion error rate of the attackers and normal users for adversarial
captchas. We observe that attackers are more likely to recognize
the adversarial captchas incorrectly, as compared to that of the
normal users. In other words, attackers are more vulnerable to ad-
versarial captchas. We also observe that the recognition rate of the
normal users for wrong-answer captchas is higher than that for
wrong-character captchas. After carefully analyzing the captchas
in different settings, we find that L0-based perturbations added to
8
wrong-character captchas would be located around the character
(as shown in Figure 4), which may also affect human recognition.
Instead, L∞-based perturbations would not affect human recogni-
tion. This observation motivates us to carefully analyze the location
of the injected perturbations in the algorithm. During the practical
deployment, we prefer to use a mixture method (using both the L0
and the L∞ distance to generate adversarial captchas). We inject
L∞-based perturbations to the entire captcha while only injecting
the L0-based perturbations to the background. In this way, we tend
to generate highly usable and secure adversarial captchas.
Our analysis above demonstrates that the recognition behav-
ior of the attackers are to some extent distinguishable from that
of the normal users. 1) The lengths of the answers submitted by