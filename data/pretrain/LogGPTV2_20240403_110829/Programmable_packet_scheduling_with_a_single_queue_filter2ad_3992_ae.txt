While PIFO requires a PIFO queue which is hard to implement
and SP-PIFO requires multiple FIFO queues (eight queues in the
simulations), AIFO achieves a good performance that is close to
PIFO and SP-PIFO with a single FIFO queue. Besides, AIFO can deal
with different sizes of traffic well as the admission control threshold
can adapt the current workload traffic dynamically. Figure 7 shows
that as the traffic load grows, the FCT for AIFO does not go up as
TCP or DCTCP does, and the gap between AIFO and PIFO/SP-PIFO
gets smaller.
The effect of parameter k. The headroom parameter k controls
how aggressively AIFO drops high-rank packets. We set k among
0.1 ∼ 0.9 and compare the results with FIFO and PIFO. Figure 8
shows the results. AIFO with smaller k always delivers better per-
formance than larger k for small flows, and it also delivers better
performance for large flows when the traffic load is big (e.g., 0.7, 0.8).
The reason is that with a small k, AIFO drops packets aggresively
and keeps the buffer shallow so that the admitted packets get low
latency. When k is small, AIFO delivers a close performance com-
pared with PIFO. As we increase k, AIFO admits more packets and
it becomes closer to FIFO. When the traffic load grows, the queue
buffer accumulates quickly, and it leads to a large delay. While
(c) SP-PIFO.
(d) AIFO.
Figure 12: Packet distribution logged at the receiver. Three
senders send one flow each to a receiver at the same
time. The size of the three flows are 100MB (large), 50MB
(medium) and 10MB (small), respectively. The link between
the switch and the receiver is the bottleneck.
(a) FIFO.
(b) PIFO.
(c) SP-PIFO.
(d) AIFO.
Figure 13: The first 300 packets of the small flow logged
at the receiver. The setting is the same as Figure 12: Three
senders send one flow each to a receiver at the same
time. The size of the three flows are 100MB (large), 50MB
(medium) and 10MB (small), respectively.
dropping packets aggressively harms large flows especially when
the traffic load is not big and the network capacity is underutilized,
we show that the harm is slight compared with the benefit it brings
to small flows. When k = 0.1, the average and 99th percentile FCT
for small flows is about 9× lower than that of k = 0.9, and the FCT
for large flows is only slightly higher than the lowest.
187
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Zhuolong Yu, et al.
(a) Average FCT for small flows.
(b) 99th FCT for small flows.
(c) FCT breakdown for 70% load.
Figure 14: Simulation results of web search workload with fair queueing.
The effect of window length and sampling rate. We also eval-
uate how the sliding window length affects the performance of
AIFO and how well a small sliding window approximates a large
sliding window with sampling for AIFO. As shown in Figure 9,
when the window length is 20, the performance is better than that
when the window length is 1000 for small flows, but worse for large
flows. It is because that a large window records packets for a longer
time, and the possibility for packets from large flows (high-rank
packets) to be admitted is more stable. As a result, there are more
high-rank packets admitted into the queue in the long run, which
makes the FCT for small flows higher and FCT for large flows lower.
It is interesting to see in Figure 9(b) that the 99th percentile FCT
is decreasing as the traffic load grows when win_len = 1000. The
reason is that when the window is large and the traffic load is low,
the quantile is less accurate. The flows that experience deep buffer
and inaccurate quantile estimation would have larger FCTs, and
these flows would normally contribute to the 99th FCT.
By comparing lines ,  and , we
can see that AIFO does not require a very precise quantile and a
small window can approximate a large window with sampling. This
is important to the practicability of AIFO as a window with 20 slots
can be implemented in programmable switches with tiny resource
consumption.
The effect of queue length. To evaluate the impact of queue
length on the performance of AIFO, we use different queue lengths
and run simulations on both a 1G/4G network (access link: 1Gbps,
leaf-spine link: 4Gbps) and a 10G/40G network. As shown in Fig-
ure 10 and Figure 11, AIFO is more sensitive to the change of queue
length when the bandwidth is low or when the traffic load is high.
Figure 11 shows that FCT achieved by AIFO when q_len = 20
is close to q_len = 100 on the 10G/40G network. However, FCT
achieved by AIFO when q_len = 20 is much smaller than q_len =
100 in Figure 10 on the 1G/4G network. This is because it takes a
while for a long queue to drain when the bandwidth is low, which
leads to a considerable queueing delay. A relatively small queue
benefits the FCT.
Admitted packet sets. Recall that we indicate in Theorem 2 that
the sets of packets dequeued by AIFO and PIFO are similar. This
experiment examines the gap between AIFO and PIFO in terms of
the difference between the packets dequeued by AIFO and those
dequeued by PIFO. Here we use four servers and the servers are
connected with a Top-of-Rack switch. The bandwidth of the links
between the servers and the switch are set as 1Gbps. We let three
servers serve as senders, and the other server serves as a receiver.
Each sender sends one flow to the receiver at the same time, and the
sizes of the flows are 100MB (large), 50MB(medium), and 10MB(small),
respectively. The servers run pFabric as the transport and the flows
are tagged with the remaining flow size as their rank. The switch
is programmed to support SRPT with AIFO, PIFO or SP-PIFO.
We log the ranks of the first 60000 packets received by the re-
ceiver, and plot the log in Figure 12. The x-axis is the arriving order
of the packets, and the y-axis is the rank of the packets. As shown
in Figure 12, when the network is running FIFO without admission
control and packet scheduling, the three flows share the bandwidth
and the small flow (blue) finishes late. For the other three solutions
(AIFO, PIFO, SP-PIFO), the small flow finishes at about the same
time, and it finishes much earlier than it does with FIFO. Besides,
it is shown that AIFO is closer to PIFO than SP-PIFO in terms of
the admitted packets set: there are larger overlaps on the arriving
order (x-axis) between the small flow (blue) and the medium flow
(green), as well as between the medium flow and the large flow
(red) in Figure 12(c), than those in Figure 12(d) and Figure 12(b).
Packet reordering. Besides the admitted set, another interesting
metric is the dequeued order of the packets. PIFO always dequeues
the packet with the lowest rank in the queue, which may cause
out-of-order and harm the end-to-end performance. However, as
AIFO only enforces admission control on a FIFO queue, it does not
cause out-of-order.
We run the same setting as in Figure 12 and we only log the first
300 packets of the small flow in order to show the packet out-of-
order clearly. As shown in Figure 13(a) and Figure 13(d), when we
enable AIFO, the packet order of one flow is the same as that with
FIFO and there is no packet out-of-order. It is because that AIFO
only uses a FIFO queue and does not do packet scheduling inside
the queue. However, both PIFO and SP-PIFO get some out-of-order
packets, as shown in Figure 13(b) and Figure 13(c). The reason is that
PIFO and SP-PIFO always dequeue the packet with the lowest rank,
while the packets with higher rank will be left in the queue and be
scheduled later. With pFabric, the rank is based on the remaining
188
Programmable Packet Scheduling with a Single Queue
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
(a) FIFO.
(b) SP-PIFO.
(c) AIFO.
Figure 15: Testbed experiments for UDP. Four flows start one by one every five seconds. Flows have different ranks: R(Flow 1)
> R(Flow 2) > R(Flow 3) > R(Flow 4).
(a) FIFO.
(b) SP-PIFO.
(c) AIFO.
Figure 16: Testbed experiments for TCP. Four flows start one by one every five seconds. Flows have different ranks: R(Flow 1)
> R(Flow 2) > R(Flow 3) > R(Flow 4).
flow size, so a later packet has a lower rank compared with an
earlier packet. As a result, these two methods lead to a number of
out-of-order packets. As SP-PIFO approximates PIFO with a set of
FIFO queues, it causes fewer out-of-orders compared with PIFO.
Fair queueing with AIFO. Programmable packet schedulers like
PIFO can be used to implement different kinds of packet scheduling
algorithms by changing the rank computation function. Besides
implementing SRPT to minimize FCTs, here we show how AIFO
performs when we implement Start-Time Fair Queueing (STFQ) [13]
on top of it for fair queueing. We also implement STFQ on top of
PIFO, SP-PIFO and PIEO to compare them with AIFO. Besides,
we include TCP, DCTCP, and the state-of-the-art fair queueing
solution AFQ for comparison. We run the web search workload
and show the average FCT for small flows (Figure 14(a)), the 99th
percentile FCT for small flows (Figure 14(b)), and the breakdown of
FCT for different flow sizes (Figure 14(c)). AIFO achieves a similar
performance compared to the state-of-the-art approaches AFQ, SP-
PIFO, PIFO and PIEO for both average FCT and tail FCT, and is
significantly better than TCP and DCTCP. The FCT of AIFO for
small flows is only 9.7% higher than AFQ and 3.6% higher than
SP-PIFO, despite AIFO using only a single queue.
5.2 Testbed Experiments
We evaluate AIFO in the testbed. The testbed experiments are con-
ducted in a hardware testbed with a 6.5Tbps Barefoot Tofino switch
and five servers. Each server is configured with an 8-core CPU
(Intel Xeon E5-2620 @ 2.1GHz) and a 40G NIC (Intel XL710). We
run Ubuntu 16.04.6LTS with Linux kernel version 4.10.0-28-generic
on the servers.
Both UDP traffic and TCP traffic are covered to examine the
traffic differentiation with ranks. We use four servers as senders
which send one flow each to a receiver. The four flows start one
by one every five seconds. The link between the switch and the
receiver is the bandwidth bottleneck. We manually tag different
ranks for different flows, and the flow that starts later has a lower
rank (i.e., higher priority): R(Flow 1) > R(Flow 2) > R(Flow 3) >
R(Flow 4). For comparison, we also run FIFO and SP-PIFO in the
same setting. For SP-PIFO, we enable 8 queues with strict priority
in the traffic manager.
UDP. We first evaluate AIFO when the four flows are UDP flows.
The four flows are all sending at 40Gbps (using DPDK [2]). Fig-
ure 15(a) shows that when the switch is running FIFO, the four
flows converge to the same rate since they have the same sending
rate and has the same possibility to be dropped. When AIFO is
enabled (Figure 15(c)), as Flow 2 has a lower rank than Flow 1,
packets from Flow 2 have a higher chance to get into the queue
when the queue builds up. Consequently, Flow 2 gets all the band-
width and the throughput to Flow 1 drops to zero when Flow 2
comes. Similarly, when Flow 3 comes, Flow 3 gets the bandwidth
between 10 seconds and 15 seconds, and when Flow 4 comes, Flow
189
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Zhuolong Yu, et al.
Resource Type
AIFO
SP-PIFO
Match Crossbars
Gateway
Hash Bits
SRAM
TCAM
Stateful ALUs
Logical Table IDs
10.94%
22.92%
3.91%
6.98%
0%
39.6%
25%
8.27%
17.71%
2.66%
15.31%
0.35%
16.67%
18.75%
Table 1: Resource consumption of AIFO and SP-PIFO proto-
types on Intel Barefoot Tofino. Each number indicates the
percentage of resources consumed for the corresponding
type.
4 occupies most of the bandwidth. The result is almost identical
to SP-PIFO (Figure 15(b)) as the flow with the lowest rank always
occupies most of the bandwidth.
TCP. We also evaluate AIFO when there are four TCP flows. We
use TCP Cubic as the congestion control algorithm on the servers.
Figure 16 shows the results. In the beginning, Flow 1 reaches around
34Gbps as it occupies the entire link. As Flow 2, Flow 3, and Flow 4
start one by one, when the switch is running FIFO, the four flows
converge to a similar rate as TCP congestion control provides fair
bandwidth allocation. However, when AIFO is enabled, lower-rank
flows get higher throughput: Flow 4 gets the highest throughput at
about 30Gbps, while Flow 1 gets the lowest throughput at 200Mbps–
1Gbps. SP-PIFO also delivers a similar result. Note that, compared
with the results of UDP flows in Figure 15, AIFO acts less aggres-
sively in the TCP scenario: the high-rank TCP flows can still get
about 3Gbps–5Gbps throughput, while the throughput of the high-
rank UDP flows in Figure 15(c) is close to 0. This is because in the
UDP scenario, the four flows are sending at a fixed rate. This makes
it easy for AIFO to enter a stationary state and AIFO would accept
most of the low-rank packets. However, for the TCP scenario, as the
flow rate is dynamic because of congestion control, the admission
threshold of AIFO changes dynamically and the high-rank packets
can have some chance to get into the queue.
Resource consumption. AIFO uses only one queue and achieves
similar performance as SP-PIFO with eight queues. Table 1 lists the
consumption of other switch resources. It shows that AIFO has a
higher demand on Match Crossbars, Gateway, Hash Bits, ALUs and
Logical TableIDs, while SP-PIFO has a higher demand on SRAM
and TCAM.
6 RELATED WORK