Measurement result
 loss rate
AS pairs and application pairs 
with differentiation
Figure 3: The NetPolice system
detect previous-hop AS based differentiation between pre1 and
pre2 at ingress ing. As long as the underlying distributions are sta-
ble and the two candidate sets include enough samples, we should
be able to reliably detect differentiation between two types of traf-
ﬁc.
Given a pair of input sets, we apply statistical hypothesis tests to
determine if there are signiﬁcant differences between them. Sev-
eral commonly-used hypothesis tests exist to compute the statisti-
cal signiﬁcance of differences between two input sets. Since the
distribution of the loss rate samples in an input set is unknown,
we choose the Kolmogorov-Smirnov (K-S) test [31] which makes
no assumption about the input sample distribution. The K-S test
compares the distance of the two empirical cumulative distribution
functions F1 and F2 corresponding to the two input sets. It com-
|F1(x) −
putes the Kolmogorov-Smirnov statistic D1,2 = sup
F2(x)|, where sup is the supremum, under the null hypothesis
that the two sets of samples are collected from the same distribu-
tion. The null hypothesis test is rejected at signiﬁcance level α if
 n1n2
n1+n2 D1,2 > Kα. Here n1 and n2 denote the size of the input
sets and Kα is the critical value in the K-S statistic table.
x
As we just discussed, the validity of a K-S test statistic depends
not only on whether the distributions of the input sets are stable
but also on whether the input sets contain enough samples. We
use Jackknife [33], a commonly-used non-parametric resampling
method, to verify the validity of the K-S test statistic. The idea is
to randomly select half of the samples from the two original input
sets and apply the K-S test on the two new subsets of samples. This
process is repeated r times. If the results of over β% of the r new
K-S tests are the same as that of the original test, we conclude that
the original K-S test statistic is valid. We use r = 400, α = 95%,
and β = 95 in this paper to ensure 95% conﬁdence interval. In §6.1
and §6.3, we will show that the choice of these parameters makes
our differentiation detection results robust against noise in loss rate
samples.
4.
IMPLEMENTATION
The implementation of NetPolice is illustrated in Figure 3. It has
three major components:
Path selector
takes path views as input and compute a task list
of probing destinations for each prober. The path views are the
traceroute measurements conducted from all the probers to all the
destination preﬁxes on the Internet. The path selector uses the path
views to learn the ingress and egress of the target ISPs that each
path traverses. It identiﬁes the ingress and egress by attempting to
map each IP hop to an ISP and a PoP based on the DNS name of
the IP hop [29]. We extend the set of naming rules in undns [29] to
increase the number of names that can be successfully mapped. The
106ISP
ISP1
ISP2
ISP3
ISP4
ISP5
ISP6
ISP7
ISP8
ISP9
ISP10
ISP11
ISP12
ISP13
ISP14
ISP15
ISP16
ISP17
ISP18
PoP
49
139
57
25
46
71
59
38
112
45
32
30
64
23
19
44
69
44
Ingress-Egress
PoP-AS
716
2125
1498
232
501
1750
677
502
822
539
419
267
115
303
137
538
1787
261
337
806
1170
102
351
653
371
195
430
176
119
138
195
82
66
208
152
316
Table 2: 18 ISPs being studied
path views are updated daily to keep up with the evolution of ISP
topologies. Some path views may become temporarily out-of-date
due to routing changes. We detect routing changes by observing
the mismatch between the IP hops seen in loss measurements and
the corresponding hops in path views. We simply discard all the
loss rate samples affected by routing changes.
The path selector implements the greedy algorithm described in
§3.1. Note that path selection is performed for multiple target ISPs
simultaneously. This signiﬁcantly reduces probing overhead by
leveraging the fact that a single probe often traverses multiple tar-
get ISPs, allowing us to cover the same set of three-tuple elements
(deﬁned in §3.1) with fewer probes compared to probing each ISP
separately. For each of the target ISPs traversed by a probe, we
measure its internal loss rate between an ingress and egress follow-
ing the method described in §3.2.
Probers
run on a distributed set of end hosts, probing all the des-
tinations in their task list periodically. After completing each round
of probing to all the destinations, the probers send their measure-
ment results to the differentiation detector for further processing.
Probing is conducted with a customized version of traceroute that
probes multiple hops of a path and multiple destinations in parallel.
The probe packets are constructed to reduce the probability that dif-
ferent probe packets from the same source to the same destination
take different IP-level paths due to load-balancing [6].
Differentiation detector ﬁrst ﬁlters the noise in the measurement
results due to overloaded probers or reverse path losses.
It then
tries to detect differentiation based on content, previous-hop AS, or
next-hop AS, following the process described in §3.3. Finally, it
performs detailed analysis on differentiation policies, such as what
input information they use, whether they are affected by network
load, and how signiﬁcant their impact is.
We deployed NetPolice on the PlanetLab testbed [26]. It uses all
the PlanetLab hosts across about 200 distinct sites. Each round of
probing takes roughly two hours to complete. The results in the pa-
per are based on 74 days of data collected during a period between
August 2008 and October 2008. Each set includes around 1,000
loss rate samples. We run multiple instances of NetPolice to take
measurements of the ﬁve applications described in §3.2 in paral-
lel. We randomize the order of destinations to probe in each round
to reduce the chance of a path being simultaneously measured by
multiple instances. We studied 18 large ISPs covering major con-
tinents including North America, Europe, and Australia, consisting
of 9 Tier-1 ISPs, 8 Tier-2 ISPs, and 1 Tier-3 ISP. Table 2 shows Net-
Police has a decent coverage of internal paths and interconnections,
traversing 115 to 2125 ingress-egress pairs and 66 to 1170 PoP-AS
pairs for each ISP. A PoP-AS pair represents an interconnection be-
tween a neighbor AS and the target ISP at the corresponding PoP.
5. REDUCING NOISE EFFECTS
Loss rate measurements taken by end-hosts are susceptible to
various types of noise on the host and in the network. As men-
tioned in §3.2, the inaccuracy of loss rate measurements is likely to
be caused by three main factors: i) overloaded prober; ii) ICMP rate
limiting at router; and iii) loss on reverse path. In this section, we
investigate the effects of these three factors and develop techniques
to mitigate their impact. We emphasize that these techniques can-
not completely eliminate all the noise. However, as shown in the
next section, the remaining noise will have little impact on the dif-
ferentiation detection results.
Many ISPs perform load balancing using equal-cost multi-paths
(ECMP) to ensure effective utilization of network resources [5].
Per-ﬂow load balancing is usually performed based on the ﬁve tu-
ple (srcip, dstip, srcpt, dstpt, proto). Thus, different application
packets, e.g., BitTorrent and HTTP, may take different internal IP-
level paths between the same ingress and egress, given their dif-
ferent destination ports (e.g., 6881 vs. 80). We do not observe
any per-packet load balancing in the 18 ISPs being studied. In this
section, we carefully design experiments to ensure our differenti-
ation detection is not affected by potential performance difference
of ECMP paths.
5.1 Overloaded prober
Previous work has shown measurement inaccuracies caused by
resource contention, in particular CPU load, on probing hosts in
PlanetLab experiments [28]. To deal with this problem, we con-
tinually monitor the CPU utilization on each prober by running
the top command and compute the average CPU utilization us-
ing three instantaneous load samples in each minute. We can then
investigate the relationship between CPU utilization and measured
loss rate by temporally correlating these two types of samples. This
allows us to identify and discard abnormal loss rate samples that
could be affected by high CPU utilization.
To determine an appropriate cut-off threshold of high CPU uti-
lization, we design the following controlled experiment to study
the effects of CPU utilization on loss rate measurements. We se-
lect a pair of lightly-loaded PlanetLab machines at the same site.
One machine acts as a “prober” to transmit one 1000-byte probe
packet per second. The other machine acts as an “acker” to receive
probe packets and return 40-byte ACKs. In essence, the “prober”
behaves just like a real NetPolice prober that measures loss rate. We
then run a computation-intensive program to gradually increase the
CPU utilization on the “prober” while keeping the acker lightly
loaded.
Figure 4 illustrates the relationship between CPU utilization and
loss rate measured by the “prober”. Because loss is unlikely to
occur on the light-loaded acker or on the local area network be-
tween the “prober” and the acker, the measured loss rate is almost
certainly due to the CPU load on the “prober.” Clearly, the loss
rate jumps up when the CPU utilization goes above 65%. We re-
peat this experiment on ten pairs of PlanetLab hosts across different
sites and ﬁnd the loss rates induced by CPU load are consistently
smaller than 0.2% when CPU utilization is under 65%. In §6.6,
we will show that such loss rates are negligible compared to the
107)
)
%
%
(
(
e
e
t
t
a
a
r
r
s
s
s
s
o
o
l
l
 9
 9
 8
 8
 7
 7
 6
 6
 5
 5
 4
 4
 3
 3
 2
 2
 1
 1
 0
 0
 50  55  60  65  70  75  80  85  90  95
 50  55  60  65  70  75  80  85  90  95
CPU (%)
CPU (%)
F
F
D
D
C
C
 1
 1
 0.98
 0.98
 0.96
 0.96
 0.94
 0.94
 0.92
 0.92
 0.9
 0.9
 0.88
 0.88
 0.86
 0.86
 0.84
 0.84
 0.82
 0.82
 0.8
 0.8
40 bytes
200 bytes
1440 bytes
 0  1  2  3  4  5  6  7  8  9
 0  1  2  3  4  5  6  7  8  9
loss rate (%)
loss rate (%)
Figure 4: Impact of CPU utilization on loss rate.
Figure 6: Impact of probe packet size on loss rate.
observed loss rate differences due to trafﬁc differentiation. By ap-
plying the 65% cutoff threshold on CPU utilization, 15% of the
samples in our data are discarded.
5.2 ICMP rate limiting
ICMP rate limiting is often conﬁgured on a per-router basis to
prevent router overload. If triggered, it may signiﬁcantly inﬂate the
measured loss rate. To prevent this, we deliberately keep a large
probing interval, e.g., only one probe packet is sent on a given path
per second. We use the following experiments to conﬁrm that this
probing interval is large enough to avoid triggering ICMP rate lim-
iting.
paths. We study the effect of packet size on measured loss rate
using controlled experiments. We conducted three sets of experi-
ments by measuring the loss rate of all the ISP internal paths using
probe packets of 40 bytes, 200 bytes, and 1440 bytes. As shown in
Figure 6, the measured loss rate increases with probe packet size.
Since the size of the ICMP responses is always the same, this con-
ﬁrms that bigger probe packets are more likely to encounter losses
on forward path. Nonetheless, the loss rates measured by 200-byte
and 1440-byte packets are roughly the same, suggesting the effects
of packet size on forward path loss diminish when packet size ex-
ceeds 200-byte.
F
F
D
D
C
C
 1
 1
 0.9
 0.9
 0.8
 0.8
 0.7
 0.7
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
10ms
300ms
500ms
1s
2s
 0
 0
 1
 1
 3
 3
 2
 4
 2
 4
loss rate (%)
loss rate (%)
 5
 5
 6
 6
F
F
D
D
C
C
 1
 1
 0.9
 0.9
 0.8
 0.8
 0.7
 0.7
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
before filtering
after filtering paths with >7% loss
 0
 0
 10
 10
 20
 20
 30
 30
 40
 40
 50
 50
 60
 60
ReErr (%)
ReErr (%)
Figure 5: Impact of probing interval on loss rate.
We conducted ﬁve sets of experiments by measuring the loss rate
of all the internal paths of the 18 target ISPs from all the probers.
We gradually increase the probing interval for each set of experi-
ments from 10ms to 2s. The smaller the interval is, the more likely
a router along a path may rate-limit the ICMP time-exceeded re-