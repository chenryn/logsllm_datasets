(RAID). 257–268.
[7] Mattia Carletti, Chiara Masiero, Alessandro Beghi, and Gian Antonio Susto. 2019.
Explainable machine learning in industry 4.0: evaluating feature importance
in anomaly detection to enable root cause analysis. In 2019 IEEE International
Conference on Systems, Man and Cybernetics (SMC). IEEE, 21–26.
[8] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (S&P). IEEE,
39–57.
[9] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 1–58.
[10] Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. 2019.
Explaining Image Classifiers by Counterfactual Generation. In ICLR. OpenRe-
view.net.
[11] Min Du, Zhi Chen, Chang Liu, Rajvardhan Oak, and Dawn Song. 2019. Lifelong
anomaly detection through unlearning. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security (CCS). 1283–1297.
[12] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomaly
detection and diagnosis from system logs through deep learning. In Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Security
(CCS). 1285–1298.
[13] Mengnan Du, Ninghao Liu, Qingquan Song, and Xia Hu. 2018. Towards expla-
nation of dnn-based prediction with guided feature inversion. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining (KDD). 1358–1367.
[14] Ming Fan, Wenying Wei, Xiaofei Xie, Yang Liu, Xiaohong Guan, and Ting Liu.
2020. Can We Trust Your Explanations? Sanity Checks for Interpreters in Android
Malware Analysis. IEEE Transactions on Information Forensics and Security 16
(2020), 838–853.
[15] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. 2019. Understanding Deep
Networks via Extremal Perturbations and Smooth Masks. In ICCV. IEEE, 2950–
2958.
[16] Ruth C Fong and Andrea Vedaldi. 2017. Interpretable explanations of black boxes
by meaningful perturbation. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV). 3429–3437.
[17] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of
neural networks with abstract interpretation. In 2018 IEEE Symposium on Security
and Privacy (S&P). IEEE, 3–18.
[18] Amirata Ghorbani, Abubakar Abid, and James Zou. 2019. Interpretation of neural
networks is fragile. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), Vol. 33. 3681–3688.
[19] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black
box models. ACM computing surveys (CSUR) 51, 5 (2018), 1–42.
[20] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing. 2018.
Lemna: Explaining deep learning based security applications. In Proceedings of
the 2018 ACM SIGSAC Conference on Computer and Communications Security
(CCS). 364–379.
[21] Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu,
Xingang Shi, and Xia Yin. 2021. Evaluating and Improving Adversarial Robustness
of Machine Learning-Based Network Intrusion Detectors. IEEE Journal on Selected
Areas in Communications (2021).
[22] Xueyuan Han, Thomas Pasquier, Adam Bates, James Mickens, and Margo Seltzer.
2020. UNICORN: Runtime Provenance-Based Detector for Advanced Persistent
Threats. In Network and Distributed System Security Symposium (NDSS).
[23] Mashud Hyder and Kaushik Mahata. 2009. An approximate l0 norm minimiza-
tion algorithm for compressed sensing. In 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, 3365–3368.
[24] Vincent Jacob, Fei Song, Arnaud Stiegler, Yanlei Diao, and Nesime Tatbul. 2020.
AnomalyBench: An Open Benchmark for Explainable Anomaly Detection. CoRR
abs/2010.05073 (2020).
[25] Jacob Kauffmann, Malte Esders, Grégoire Montavon, Wojciech Samek, and Klaus-
Robert Müller. 2019. From Clustering to Cluster Explanations via Neural Net-
works. CoRR abs/1906.07633 (2019).
[26] Jacob Kauffmann, Klaus-Robert Müller, and Grégoire Montavon. 2020. Towards
explaining anomalies: A deep Taylor decomposition of one-class models. Pattern
Recognition 101 (2020), 107198.
[27] Alexander D. Kent. 2015. Cybersecurity Data Sources for Dynamic Network
Research. In Dynamic Networks in Cybersecurity. Imperial College Press.
mization. In ICLR (Poster).
[28] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
[29] Shogo Kitamura and Yuichi Nonaka. 2019. Explainable Anomaly Detection via
Feature-Based Localization. In ICANN (Workshop) (Lecture Notes in Computer
Science, Vol. 11731). Springer, 408–419.
[30] Junjie Liang, Wenbo Guo, Tongbo Luo, Vasant Honavar, Gang Wang, and Xinyu
Xing. 2021. FARE: Enabling Fine-grained Attack Categorization under Low-
quality Labeled Data. In The Network and Distributed System Security Symposium
2021.
[31] Ninghao Liu, Donghwa Shin, and Xia Hu. 2018. Contextual outlier interpretation.
In Proceedings of the 27th International Joint Conference on Artificial Intelligence
(IJCAI). 2461–2467.
[32] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting
Model Predictions. In Advances in Neural Information Processing Systems (NIPS).
4765–4774.
[33] Zili Meng, Minhu Wang, Jiasong Bai, Mingwei Xu, Hongzi Mao, and Hongxin
Hu. 2020. Interpreting Deep Learning-Based Networking Systems. In Proceedings
of the Annual conference of the ACM Special Interest Group on Data Communi-
cation on the applications, technologies, architectures, and protocols for computer
communication (SIGCOMM). 154–171.
[34] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems 26 (2013), 3111–3119.
[35] Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai. 2018. Kitsune:
an ensemble of autoencoders for online network intrusion detection. 25th Annual
Network and Distributed System Security Symposium (NDSS).
[36] Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek,
and Klaus-Robert Müller. 2017. Explaining nonlinear classification decisions with
deep Taylor decomposition. Pattern Recognition 65 (2017), 211–222.
[37] Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. 2018. Methods for
interpreting and understanding deep neural networks. Digital Signal Processing
73 (2018), 1–15.
[38] Andrea Morichetta, Pedro Casas, and Marco Mellia. 2019. EXPLAIN-IT: Towards
Explainable AI for Unsupervised Network Traffic Analysis. In Proceedings of
the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial
Intelligence for Data Communication Networks. 22–28.
[39] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2016. Multifaceted feature visualiza-
tion: Uncovering the different types of features learned by each neuron in deep
neural networks. arXiv preprint arXiv:1602.03616 (2016).
[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
2019. Pytorch: An imperative style, high-performance deep learning library. In
Advances in neural information processing systems. 8026–8037.
[41] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning
of social representations. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining (KDD). 701–710.
[42] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I
trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining
(KDD). 1135–1144.
[43] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. 2018. Toward
Generating a New Intrusion Detection Dataset and Intrusion Traffic Characteri-
zation.. In Proceedings of the 2nd international conference on information systems
security and privacy (ICISSP). 108–116.
[44] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning im-
portant features through propagating activation differences. In International
Conference on Machine Learning(ICML). 3145–3153.
[45] Lior Sidi, Yisroel Mirsky, Asaf Nadler, Yuval Elovici, and Asaf Shabtai. 2020. Helix:
DGA Domain Embeddings for Tracking and Exploring Botnets. In Proceedings of
the 29th ACM International Conference on Information & Knowledge Management
(CIKM). 2741–2748.
[46] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside
convolutional networks: Visualising image classification models and saliency
maps. In ICLR (Workshop Poster).
[47] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
2020. Fooling lime and shap: Adversarial attacks on post hoc explanation methods.
In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AAAI).
180–186.
[48] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wat-
tenberg. 2017. Smoothgrad: removing noise by adding noise. arXiv preprint
arXiv:1706.03825 (2017).
[49] Fei Song, Yanlei Diao, Jesse Read, Arnaud Stiegler, and Albert Bifet. 2018. EXAD:
A system for explainable anomaly detection on big data traces. In 2018 IEEE
International Conference on Data Mining Workshops (ICDMW). IEEE, 1435–1440.
[50] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. In ICML (Proceedings of Machine Learning Research, Vol. 70).
PMLR, 3319–3328.
[51] Ruming Tang, Zheng Yang, Zeyan Li, Weibin Meng, Haixin Wang, Qi Li, Yongqian
Sun, Dan Pei, Tao Wei, Yanfei Xu, et al. 2020. Zerowall: Detecting zero-day
web attacks through encoder-decoder recurrent neural networks. In 39th IEEE
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3210Conference on Computer Communications (INFOCOM). IEEE, 2479–2488.
[52] Muhammad Fahad Umer, Muhammad Sher, and Yaxin Bi. 2017. Flow-based
intrusion detection: Techniques and challenges. Computers & Security 70 (2017),
238–254.
[53] Ruoying Wang, Kexin Nie, Tie Wang, Yang Yang, and Bo Long. 2020. Deep Learn-
ing for Anomaly Detection. In Proceedings of the 13th International Conference on
Web Search and Data Mining (WSDM). 894–896.
[54] Alexander Warnecke, Daniel Arp, Christian Wressnegger, and Konrad Rieck.
2020. Evaluating explanation methods for deep learning in security. In 2020 IEEE
European Symposium on Security and Privacy (EuroS&P). IEEE, 158–174.
[55] Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and
Finale Doshi-Velez. 2018. Beyond Sparsity: Tree Regularization of Deep Models
for Interpretability. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI).
[56] Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li,
Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised anomaly
detection via variational auto-encoder for seasonal kpis in web applications. In
Proceedings of the 2018 World Wide Web Conference (WWW). 187–196.
[57] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I Jordan. 2009.
Detecting large-scale system problems by mining console logs. In Proceedings
of the ACM SIGOPS 22nd symposium on Operating systems principles (SOSP).
117–132.
[58] Limin Yang, Wenbo Guo, Qingying Hao, Arridhana Ciptadi, Ali Ahmadzadeh,
Xinyu Xing, and Gang Wang. 2021. CADE: Detecting and Explaining Concept
Drift Samples for Security Applications. In 30th USENIX Security Symposium
(USENIX Security).
[59] Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay
Chandrasekhar. 2018. Adversarially learned anomaly detection. In 2018 IEEE
International Conference on Data Mining (ICDM). IEEE, 727–736.
[60] Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, and Ting Wang.
2020. Interpretable deep learning under fire. In 29th USENIX Security Symposium
(USENIX Security).
[61] Ying Zhong, Wenqi Chen, Zhiliang Wang, Yifan Chen, Kai Wang, Yahui Li, Xia
Yin, Xingang Shi, Jiahai Yang, and Keqin Li. 2020. HELAD: A novel network
anomaly detection model based on heterogeneous ensemble learning. Computer
Networks 169 (2020), 107049.
[62] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.
2016. Learning deep features for discriminative localization. In Proceedings of the
IEEE conference on computer vision and pattern recognition (CVPR). 2921–2929.
[63] Chong Zhou and Randy C Paffenroth. 2017. Anomaly detection with robust deep
autoencoders. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD). 665–674.
[64] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. 2018. Graph neural networks: A review of
methods and applications. arXiv preprint arXiv:1812.08434 (2018).
[65] Luisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. 2017. Visual-
izing Deep Neural Network Decisions: Prediction Difference Analysis. In ICLR.
OpenReview.net.
APPENDICES
A ALGORITHMS OF INTERPRETERS
We provide the procedure of time-series and graph-data Interpreter.
A.1 Procedure of Time-Series Interpretation
The procedure of DeepAID Interpreter for time-series based systems
is in Algorithm 2.
A.2 Procedure of Graph Data Interpretation
Recall in §4.4, we claim that problem (12) cannot be directly solved
by gradient-based optimizer if E𝐺 is indifferentiable. To address
this problem, we propose an alternative greedy solution as follows:
Greedy Search. In a nutshell, we start searching two reference
nodes 𝑥∗
, and gradually search outward. In this
way, searched reference link will not be too far from the abnormal
link, which increases the interpretability. Specifically, we denote
the objective function in (12) with D𝑔𝑟𝑎(X∗; 𝒆∗), which is used to
measure the priority of link X∗. Then, the greedy search is con-
ducted by breadth-first search (BFS) with a priority queue denoted
from 𝑥◦
𝑎, 𝑥◦
𝑎, 𝑥∗
𝑏
𝑏
𝑡−1);
2 ...𝒙◦
Algorithm 2: Interpreting time-series anomalies (univari-
ate)
Input: Interpreted anomaly X◦; 𝑚𝑎𝑥_𝑖𝑡𝑒𝑟, learning rate 𝛼; 𝜇1, 𝜇2
Output: Interpretation result X◦ − X∗ with the reference X∗
1 𝒙𝑐 ← argmax
𝒙𝑐 Pr(𝒙𝑐 |𝒙◦
1𝒙◦
2 if 𝑆𝑇 (X◦; 𝜇1, 𝜇2) = True then
X∗ ← 𝒙◦
2 ...𝒙◦
1𝒙◦
𝑡−1𝒙𝑐;
Initialize X∗ with X◦; 𝑡 ← 0;
while 𝑡 < 𝑚𝑎𝑥_𝑖𝑡𝑒𝑟 do
(cid:0)∇D𝑡𝑠 (X∗; 𝒙◦
𝑡)(cid:1)
⊲ iterative optimization
X∗ ←Adam(X∗; D𝑡𝑠, 𝛼) ; ⊲ update X∗ by Adam optimizer
𝑖∗ ← argmax𝑖
⊲ effectness
measurement
for 𝑖 = 1 to 𝑁 do
⊲ replacing ineffective dimensions
3
4 else
5
6
7
8
⊲ saliency testing
𝑖; ;
if 𝑖 ≠ 𝑖∗ then (X∗)𝑖 ← (X◦)𝑖 ;
end
Discretize X∗ into one-hot vectors with only 0 or 1;
𝑡 ← 𝑡 + 1;
9
10
11
12
13
14
15 end