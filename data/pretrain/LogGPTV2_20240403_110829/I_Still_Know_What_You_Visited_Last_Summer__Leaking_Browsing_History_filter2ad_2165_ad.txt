pool was therefore primed to carry out tasks as quickly and
accurately as they could before we ever started talking to
them. These effects would tend to make a genuine attack less
effective than our results indicate. We should not discount the
motivation of victims faced with an (apparent) CAPTCHA,
however. CAPTCHAs are pure obstacles, so users are motivated
to get them out of the way as quickly as possible; users expect
to be locked out of the site if they fail to solve the challenge,
so they are motivated to solve them correctly.
On the whole, we think our results are a reasonable
estimate of the effectiveness of our tasks when used for a
real snifﬁng exploit. Attackers should perhaps worry more
about CAPTCHAs causing some fraction of their victims to
abandon their efforts to use the site [31]. Even this can be
addressed by making the interactive task seem more like a
game than an obstacle, and by presenting it after potential
victims have already sunk effort into making use of the site.
D. History Density
The chessboard and word CAPTCHA are easier for the
victim to complete if they have visited only a few of the links
that the attacker is probing. 264 of our participants used a
browser that still permits automated history snifﬁng. Figure 6
shows what percentage of the wtikay.com “top5k” link
set had been visited by each of them. The percentages are
155
Participant count0510150.0%0.5%1.0%1.5%2.0%Chrome 3Chrome 6Chrome 7Chrome 8Chrome 9Firefox 2Firefox 3.0Firefox 3.5Firefox 3.6Firefox 4Flock 2IE 8Opera 10Safari 4Safari 50%10%20%30%40%Fig. 7. Demographic breakdown of participants
An obvious defense is to prevent links from being drawn in
the same color as the background (whether visited or not).
However, merely determining what the background color is at
any given position can be difﬁcult. Just to give one example, the
attacker could make the background of their fake CAPTCHA
be partially transparent, then place a box of a contrasting color
directly underneath. For efﬁciency, the browser might prefer
to have the computer’s graphics card overlay the text, the
partially transparent background, and the colored box, and
send the result directly to the screen, but if it needs to know
what the color of the background plus the box is before it can
draw the text, it cannot do this.
But this is not the real problem with this defense. The real
problem is that interactive attacks don’t need to make anything
invisible. “Type the green words, but not the red words” would
be an even more convincing fake CAPTCHA than the one we
used. Similarly, the chessboard task could ask the user to click
only on red pawns. As long as there is a visible difference
on the user’s screen, we see no practical way to prevent a
sufﬁciently determined attacker from getting the user to reveal
what it is.
For the most privacy-conscious users, limiting the circum-
stances under which visited links are revealed might be an
appropriate move. In his original BUGTRAQ post describing
visited link attacks [1], Clover suggested that links might
only be revealed as visited when they refer to documents in
the same domain as the current page, but then immediately
pointed out that this would render the feature nearly useless.
SafeHistory [9] reﬁnes this idea: links are revealed as visited
if they target a document in the same domain, if the link
destination has previously been visited from the current site,
or if the current site is on a whitelist of trusted sites. Under
this policy, a malicious site cannot learn anything from history
snifﬁng that it could not discover by monitoring clicks on
outbound links. It sacriﬁces what is arguably the most useful
case of visited-link indications (when a new-to-the-user site
links to a document they have already seen), but to some extent
this can be mitigated by use of the whitelist.
Unfortunately, an attacker may still be able to construct an
interactive attack on history if any links are revealed as visited.
With SafeHistory in use, if attackers can predict the location of
a link to a site of interest on a whitelisted page, they can draw
pictures using iframes that show one pixel of the whitelisted
page, directly above that link. This is not so farfetched as
it might sound: a hyperlink to facebook.com appears at a
predictable location on the page http://www.google.com/search?
q=facebook.com, and search engines are obvious candidates
for whitelisting. If there is no whitelist, attackers could instead
draw their pictures with single-pixel iframes of the sites
they want to know about. Many sites contain links pointing
back to their front pages in predictable locations on interior
pages, which would count as same-origin and so have their
visitedness revealed. (Care must be taken not to disturb the
visitedness of the front page, of course.) Of course, attackers
using this technique cannot control the colors of visited and
unvisited links, but this poses little difﬁculty: they can either
design their interactive attack to work with the colors they get,
or they can use an SVG ﬁlter to remap the colors as they see
ﬁt (as we did in the character CAPTCHA).
Most browsers can be conﬁgured not to retain any visited-
link history at all, and the “private browsing” mode found
in all modern browsers makes this quite convenient. Private
browsing was developed to defend users’ privacy against other
users of the same computer [35], but it also prevents remote
history snifﬁng attacks. Of course, this comes at the price of not
distinguishing visited from unvisited links at all. Alternatively,
most browsers can be conﬁgured to remember history only
until shut down; this mode’s visited-link distinctions are less
useful (the user probably remembers what they have visited
within the current session) and remote attackers can still detect
pages visited within the current session.
V. EXPERIMENT 2: SIDE-CHANNEL ATTACK
Baron’s defense was intended to cover all practical side
channel attacks on browsing history; many of the restrictions
it places on :visited are solely to prevent timing attacks.
156
0%16%33%50%Age18−2930−4950−6970+Date of first computer useBefore19841984−19941994−20002000−20042005−presentDaily Internet use (hours) tag demonstrates. If the
color, size, or blink rate are poorly chosen, ﬂashing light can
even induce epileptic seizures [38]. However, despite these
drawbacks, many online ads already do include blinking effects;
an attack disguised as one of these ads might irritate victims
enough that they close the offending window, but is unlikely
to seem suspicious.
We developed and tested two variants of this attack. In
both variants, we made a rectangular box of uniform color
be a hyperlink, periodically changed its target, and monitored
changes in the average color detected by the camera. We used
the least sophisticated image processing algorithm that would
work at all; our results should therefore be considered a worst
case scenario for the attacker. The QPM ratio and total number
of links probed are ﬁxed by the blink rate and runtime of
the attack, so we discuss only accuracy below. As with the
interactive attacks, we did not actually sniff history; rather, we
generated a random sequence of 20 links, of which 10 were
known to be visited and 10 known to be unvisited, so that
we knew the correct answer for each link and could measure
accuracy.
1) Variant 1: The ﬁrst variant was designed to comply with
the WCAG standard for seizure safety [38]. This standard limits
the maximum area that can be made to blink, maximum blink
157
Fig. 9. Histogram of webcam attack (variant 1)’s accuracy rate when presented
to participants in the interactive experiment.
rate, and the maximum luminosity difference between ﬂashes; it
also requires avoidance of the color red. All these requirements,
especially the limits on blinking area and luminosity changes,
make detecting the change in reﬂected light more difﬁcult, but
by no means impossible.
2) Variant 2: The second variant made the entire browser
window ﬂash, and used brighter colors to represent visited and
unvisited links. The image processing task is much easier, but
it is obvious that something unusual is happening.
B. Results
We tested both variants on ourselves under controlled
conditions, using one of the authors’ computers (a Macbook
Pro with built-in webcam) in three settings with diverse
backgrounds: an ofﬁce cubicle, a bedroom, and a living room.
We also tested the attacks both with one of the authors sitting
in front of the computer, and with nobody in the camera’s
ﬁeld of view. We were able to achieve 100% accuracy for both
variants in all conditions, provided that the room was well-lit
and the person in front of the computer (if any) remained still.
In a dark room, accuracy dropped to chance (50%).
The ﬁrst variant of the webcam attack was also ﬁeld-tested
as an optional task for participants in our interactive experiment.
Not all of them had Flash-accessible cameras or were willing
to let us use them; of the 307 participants in the interactive
experiment, only 60 performed the webcam task. Participants
who agreed to perform the task were asked to sit still and
watch the screen while it ﬂashed; they did not need to do
anything.
As shown in Figure 9, this attack’s accuracy rate is highly
variable in the ﬁeld, often dropping to not much better than
0%5%10%15%20%0%20%40%60%80%100%chance. Comparing to our results under controlled conditions,
we believe the high error rate is mainly caused by participants
moving around during the task. If so, attackers could analyze the
video feed and only run the attack during periods when nothing
was moving in the camera’s ﬁeld of view. More sophisticated
image processing might also help.
C. Discussion
One might reasonably ask whether this technique is practical
enough to be a genuine threat. We think the most serious
obstacle in real life would be persuading victims to allow
access to their webcams. There are already sites that make
legitimate use of the webcam, usually for live two-way chat (the
ChatRoulette service [39] is a prominent example). Such sites
could plausibly incorporate the WCAG-compliant variant of this
attack, disguised as an ad. The more obtrusive variant is likely
to make anyone who sees it close the browser immediately,
but we think it could still be used on victims who walk away
from the computer leaving the malicious site open. It does not
take terribly sophisticated image processing to detect when
nobody is in the camera’s ﬁeld of view, and in our controlled
tests, the attack works even when the closest reﬂector is a wall
10 to 20 feet away from the monitor.
We would also like to point out that as the Web platform
gains capabilities, other side-channel attacks may become
possible. HTML5 already contemplates adding features [40]
that would eliminate the need for Adobe Flash in the webcam
attack. WebGL [41] allows rendered HTML pages to be
processed by shader programs, which are Turing-complete; we
speculate that they might be able to detect history-dependent
color changes and report them back to the controlling page’s
scripts (if only via a timing channel).
VI. RELATED WORK
Privacy attacks have received signiﬁcant attention recently.
Section II covered the existing work on defenses for nonin-
teractive attacks on visited-link history [1], [2], [4], [9], [11],
[15], [16]. In this section, we describe related work on privacy
attacks that abuse other browser features.
Visited-link state is not the only way to determine whether
the user has visited a site. Two other straightforward techniques
involve timing attacks on local caches maintained by the
browser.
Page cache. Browsers cache resources retrieved from the
Web to improve the speed of subsequent page loads.
Approximately 60% of HTTP queries are requests for
cacheable resources [42]. The cache is global, so by
embedding a resource from another site and measuring the
time it takes to load, a web page can determine whether
that resource is already in the browser’s cache, and thus
determine whether the user has visited the other site [43].
DNS cache. Name-to-IP-address mappings retrieved from the
DNS are typically cached by the operating system of the
computer that made the query, and may also be cached
by an intermediate device (such as a network router)
for the beneﬁt of other computers on the same network.
158
Web attackers can induce the browser to perform a DNS
lookup and measure the amount of time it takes [43];
local network attackers, able to make queries of a shared
DNS cache, can inspect its contents in more detail [44],
[45]. The DNS cache can reveal which sites a user has
visited, but unlike the page cache, it can also reveal search
queries that the user has made, because some browsers
(versions of Firefox and Chrome released since 2008 [45];
Safari 5 also adopted the tactic) prefetch DNS entries for
sites that the user is likely to visit in the future—such as
sites linked from a search results page.
Note that both these techniques are destructive—only the very
ﬁrst attempt to determine whether a piece of information is
cached will reveal anything interesting, because the attack itself
causes the information to be cached. Also, browsers don’t cache
information for very long, even in the face of strenuous efforts
by site maintainers to make them do so [46], [47] so these
attacks are not very reliable and may only reveal short-term
history.
Another tactic only applies to sites that users typically
remain logged into for long periods (Facebook, Gmail, Twitter,
etc.) If an attacker can guess the URL of a resource that is
loadable cross-origin but only available to logged-in users, they
can attempt to load it and detect failure using the JavaScript
onerror event. Depending on the site, more information may
be available to clever attackers [48]; even if the resource does
not generate an HTTP error for users that are not logged in, it
may be possible to extract information from it [49].
Client-side state such as cookies [50], [51], Flash Player
local shared objects [52], and Web Storage [53] can be used by
web sites to re-identify users who have visited a site in the past.
They are often used for user authentication and personalization.
Some of these mechanisms (notably cookies) allow “third
parties” (sites other than the main one the user is interacting
with, but that provide some of the resources present in the page)
to access client-side state. This third-party state is separate
from any state set by the page itself, but if several sites refer
to the same resource provider (for instance, an advertising
network), that provider can build a proﬁle of a user’s browsing
activities. Even if the user regularly clears their cookies, a
determined site may be able to re-construct them based on
other browser state [54]. Most browsers provide some degree
of control over cookies, allowing users to disable third-party
cookies altogether, or allow only cookies with an acceptable
P3P privacy policy [55]. Unfortunately, these mechanisms are
easily circumvented [9], [56].
Finally, many kinds of technological devices possess subtle
but measurable variations that allow them to be “ﬁngerprinted,”
and browsers are no exception. By tracking information that
the browser reveals to all sites, such as User-Agent headers,
Accept headers, screen resolution, time zone, browser plugins,
and system fonts, a site can rapidly re-identify users, even
without the use of client-side state [57], [58]. Fingerprinting
can be used to build a proﬁle of user behavior even if the user
tries to clear browser state.
Privacy tools such as Torbutton [59] aim to mitigate or
prevent the above attacks, at the cost of web functionality;
this is an acceptable tradeoff for some users. Torbutton is
particularly noteworthy for considering and designing against
ﬁngerprintability. Private browsing mode [35] can also mitigate
some of these attacks, but it was not designed to do so and
is less effective than a specialized tool; again, functionality is
sacriﬁced. Ad-blockers [60] prevent many real-world cases of
behavior proﬁling as a side effect, since ad networks are one
of the primary users of third-party cookies.
The more well-known providers of third-party tracking
cookies often allow users to “opt out” [14], but this is a
manual procedure that must be carried out for each tracker.
The “Do Not Track” initiative [61] proposes to indicate in