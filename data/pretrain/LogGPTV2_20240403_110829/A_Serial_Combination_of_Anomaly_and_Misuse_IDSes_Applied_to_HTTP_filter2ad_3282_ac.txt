### Anomaly and Misuse Detection Conflicts

1. **Conflict Set C1:**
   - **Description:** A safe event is declared safe by the anomaly detection but intrusive by the misuse detection.
   - **Resolution:** The anomaly detection is incorrect, and the misuse detection is correct. This conflict set is denoted as \(C1\).

2. **Conflict Set C2:**
   - **Description:** A safe event is declared unknown (potentially intrusive) by the anomaly detector and also unknown (potentially safe) by the misuse detector.
   - **Resolution:** There is a soft conflict. The misuse detection is more accurate than the anomaly detection. This conflict set is denoted as \(C2\).

3. **Conflict Set C3:**
   - **Description:** An intrusive event is declared unknown (potentially intrusive) by the anomaly detector and also unknown (potentially safe) by the misuse detector.
   - **Resolution:** There is a soft conflict. The anomaly detection is more accurate than the misuse detection. This conflict set is denoted as \(C3\).

4. **Conflict Set C4:**
   - **Description:** An intrusive event is declared unknown (potentially intrusive) by the anomaly detector and also unknown (potentially safe) by the misuse detector.
   - **Resolution:** There is a soft conflict. The anomaly detection is more accurate than the misuse detection. This conflict set is denoted as \(C4\).

### Serial Combination of Anomaly and Misuse Detection (ADFSC)

In Section 2.3, we noted that the HTTP traffic we monitor is mostly safe. This has implications for the relative sizes of the sets described in the previous section. This section briefly presents the serial combination of anomaly and misuse detection, with anomaly detection first (ADFSC), and discusses the formal model and its justification for HTTP traffic.

**Figure 3.** Serial combination of anomaly and misuse detection.

- **As (Safe Events):** These events are considered safe by the operator. For effective results, the anomaly model must be constructed to minimize false positives in this set.
- **Au ∩ Mi (Unknown and Intrusive Events):** These events are declared unknown (potentially intrusive) by the anomaly detector and intrusive by the misuse detector, confirming the anomaly detection's alert. The misuse component enhances the diagnosis of the anomaly detection alerts.
- **Au ∩ Mu (Unknown and Unknown Events):** These events are declared unknown (potentially intrusive) by both the anomaly and misuse detectors. They should be treated as false positives or negatives and used to update the anomaly model or the signatures database.

**Figure 4.** Serial combination of anomaly and misuse detection for HTTP traffic – sets in grey represent empty sets; Es is much larger than Ei; conflict sets are reinterpreted.

- **C1 (Af−s ∩ M t+ i):** The anomaly component is incorrect. Not sending these events to the misuse component means the conflict is ignored, leading to false negatives. The challenge is to ensure that the C1 set is negligible, minimizing false negatives from the anomaly component.
- **C2 (At−s ∩ M f+ i):** The anomaly component is correct. Not sending these events to the misuse component means the conflict is resolved, resulting in true negatives.
- **C3 (Af+ u ∩ M t− u) and C4 (At+ u ∩ M f− u):** These conflicts remain. Both components do not qualify these events, so they can be interpreted as unqualified, containing both safe and potentially new attack events. Further investigation is needed to classify them, and the models should be updated accordingly.

### Experimental Results

We use an ADFSC as illustrated in Figure 3. The anomaly detection component is integrated behind the normalization module in the WIDS to leverage request normalization and avoid masquerading. The HTTP traffic data comes from the Supélec web server log files, which are less than one year old and fully accessible.

#### 5.1. The Anomaly Detection Model

The anomaly model is based on the pairs of accessed resources and the combination of parameters used, if any. For example, the following requests:

- `/background.jpg`
- `/index.php?subject=network&news_id=85`
- `/index.php?subject=security&news_id=23`
- `/index.php`

Lead to the following pairs in our model:

- `(/background.jpg, {})`
- `(/index.php, {subject, news_id})`
- `(/index.php, {})`

The model is constructed using a training set, where distinct pairs are extracted and classified in ascending order by their occurrence. Each pair is manually qualified as safe or unknown by an operator. In our experiments, we verified a thousand resources, which took a few hours.

The resource name and parameter combinations are the primary criteria, but other log entry fields and their combinations can also be useful. For instance, client IP addresses are important for resources only reachable from a specific intranet. Our experiments use only the resource name and parameter combinations. If a requested resource is in the model, the anomaly detector checks if the parameter combination is allowed, similar to DIDAFIT [7, 8].

The tree structure of the anomaly model is simple yet efficient. Each node contains a character of the resource name, and each leaf contains the constraints for the resource. When a resource is requested, the anomaly detector first checks if it belongs to the model. If not, the request is declared unknown. If it does, the associated constraints are applied. If the constraints are satisfied, the request is declared safe; otherwise, it is declared unknown.

#### 5.2. Qualitative Results

- **Training Set:** The first seven days, consisting of 191,976 log entries and 17,465 distinct resources.
- **Test Set:** The remaining 85 days.
- **Anomaly Model:** Composed of 17,313 selected resources from the training set, including all static resources and some dynamic resources like PHP scripts and counters managed by the webmaster.

**Table 3.** Filtering rate toward Supélec log files. The percentages are rounded to two decimal places.

| Severity | Single Misuse Detection | Serial Combination |
|----------|-------------------------|--------------------|
| 0        | 1,747,179 (94.25%)      | 100,395 (94.52%)   |
| 1        | 266,401 (95.98%)        | 10,704 (97.04%)    |
| 2        | 166,470 (94.76%)        | 8,711 (97.09%)     |
| 3        | 16,410 (97.04%)         | 485 (82.15%)       |
| 4        | 13,159 (97.09%)         | 382 (56.37%)       |
| 5        | 409 (82.15%)            | 73 (62.50%)        |
| 6        | 353 (56.37%)            | 154 (0%)           |
| 7        | 24 (62.50%)             | 9 (0%)             |
| 8        | 12 (0%)                 | 12 (0%)            |
| 9        | 4 (0%)                  | 4 (0%)             |
| >9       | 352 (0%)                | 352 (0%)           |
| All      | 2,210,773               | 121,281            |

This table shows the filtering rates and the effectiveness of the ADFSC approach compared to single misuse detection.