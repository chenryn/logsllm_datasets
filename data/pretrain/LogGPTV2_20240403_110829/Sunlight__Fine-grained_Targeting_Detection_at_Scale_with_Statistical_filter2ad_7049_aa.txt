title:Sunlight: Fine-grained Targeting Detection at Scale with Statistical
Confidence
author:Mathias L&apos;ecuyer and
Riley Spahn and
Yannis Spiliopolous and
Augustin Chaintreau and
Roxana Geambasu and
Daniel J. Hsu
Sunlight: Fine-grained Targeting Detection at Scale with
Statistical Conﬁdence 
Mathias Lecuyer, Riley Spahn, Yannis Spiliopoulos,
Augustin Chaintreau, Roxana Geambasu, and Daniel Hsu
(mathias,riley,yannis,augustin,roxana,PI:EMAIL)
ABSTRACT 
We present Sunlight, a system that detects the causes of target­
ing phenomena on the web – such as personalized advertisements, 
recommendations, or content – at large scale and with solid statisti­
cal conﬁdence.  Today’s web is growing increasingly complex and 
impenetrable as myriad of services collect, analyze, use, and ex­
change users’ personal information. No one can tell who has what 
data, for what purposes they are using it, and how those uses affect 
the users.  The few studies that exist reveal problematic effects – 
such as discriminatory pricing and advertising – but they are either 
too small-scale to generalize or lack formal assessments of conﬁ­
dence in the results, making them difﬁcult to trust or interpret. 
Sunlight brings a principled and scalable methodology to per­
sonal data measurements by adapting well-established methods from 
statistics for the speciﬁc problem of targeting detection. Our method­
ology formally separates different operations into four key phases: 
scalable hypothesis generation, interpretable hypothesis formation, 
statistical signiﬁcance testing, and multiple testing correction. Each 
phase bears instantiations from multiple mechanisms from statis­
tics, each making different assumptions and tradeoffs. Sunlight of­
fers a modular design that allows exploration of this vast design 
space.  We explore a portion of this space, thoroughly evaluating 
the tradeoffs both analytically and experimentally. Our exploration 
reveals subtle tensions between scalability and conﬁdence.  Sun­
light’s default functioning strikes a balance to provide the ﬁrst sys­
tem that can diagnose targeting at ﬁne granularity, at scale, and with 
solid statistical justiﬁcation of its results. 
We showcase our system by running two measurement studies 
of targeting on the web, both the largest of their kind. Our studies – 
about ad targeting in Gmail and on the web – reveal statistically jus­
tiﬁable evidence that contradicts two Google statements regarding 
the lack of targeting on sensitive and prohibited topics. 
Categories and Subject Descriptors 
K.4.1 [Computers and Society]: Public Policy Issues—Privacy, 
Ethics, Use/abuse of power 
Keywords 
web transparency; privacy; measurement 
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for proﬁt or commercial advantage and that copies bear this notice and the full citation 
on the ﬁrst page.  Copyrights for components of this work owned by others than the 
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission 
and/or a fee. Request permissions from Permissions@acm.org. 
CCS’15, October 12 - 16, 2015, Denver, CO, USA. 
Copyright is held by the owner/author(s). Publication rights licensed to ACM. 
ACM 978-1-4503-3832-5/15/10$15.00 
DOI: http://dx.doi.org/10.1145/2810103.2813614. 
1  Introduction 
In a 1913 paper [7], Louis Brandeis, the proponent of modern 
views of individual rights to privacy,  stated:  “Sunlight is said to 
be  the  best  of  disinfectants;  electric  light  the  most  efﬁcient  po­
liceman.”  Unfortunately, today’s Web is a very dark and complex 
ecosystem driven to a large extent by the massive collection and 
monetization  of  personal  data.  Myriad  of  Web  services,  mobile 
applications, and third parties are collecting large amounts of in­
formation from our daily online interactions, such as our website 
visits, clicks, emails, documents, and pictures.  At a surface level, 
end-users and researchers alike largely understand that these com­
panies may be using this information to target advertisements, cus­
tomize recommendations,  personalize news feeds,  and even ﬁne-
tune  prices.  Indeed,  the  companies’  own  terms  of  service  often 
stipulate such uses.  But at a concrete level, neither end-users nor 
researchers – nor individual companies (as we argue) – understand 
how speciﬁc personal data ﬂows through the complex web ecosys­
tem,  how  it  is  being  used  (or  abused)  in  practice  by  parties  that 
interact with it, and how those uses affect the users. 
Questions about the targeting on the web abound: Are our chil­
dren’s online activities being targeted, and if so what kinds of prod­
ucts  are  they  being  offered?  Are  people  being  targeted  because 
their browsing patterns suggest that they might be vulnerable (e.g., 
sick, depressed, or in ﬁnancial difﬁculty)? Are such inferences be­
ing used to increase insurance premiums, deny housing, or place 
potentially damaging products, such as alcoholic products or risky 
mortgage deals?  In other words,  is our data being used without 
our knowledge or consent in ways that affect us?  Today, we lack 
believable, at-scale answers to such questions. 
A good way to shed light on large, complex systems is to mea­
sure them at scale using scientiﬁc methods and tools.  Indeed,  a 
number of measurement studies have emerged in recent years, which 
attempt to answer questions about how personal data is being used 
on the web [2, 6, 8, 15, 16, 19–22, 27, 29].  We reviewed 12 of these 
studies and found a signiﬁcant gap in scalable experimental method­
ologies.  Generally  speaking,  prior  studies  conduct  tightly  con­
trolled experiments that vary personal data inputs (such as location, 
search terms, or proﬁle interests) one at a time and observe the ef­
fect on service outputs (such as ads, recommendations, or prices) 
compared to a control group.  Unfortunately, we ﬁnd the method­
ologies employed by prior studies either difﬁcult to scale or lacking 
formal notions of conﬁdence, which make their results difﬁcult to 
trust, interpret, and generalize.  Consequently, the scale and scope 
of what we can assert today about data use on the web is limited, 
and our capacity to exert oversight on this large, complex, and ever-
changing ecosystem is virtually non-existent. 
This  paper  argues  that  shedding  light  into  the  web’s  complex 
data  ecosystem  requires  the  development  of  robust  experimental 
methodologies, as well as infrastructures that implement them, that 
can be used to answer broad classes of questions at large scale and 
with interpretable, trustworthy, statistical justiﬁcation of the results. 
We present Sunlight, a new methodology, plus a system that imple­
ments it, that achieves these goals in the context of one important 
class of questions:  those that require a ﬁne-grained measurement 
of the causes of targeting phenomena on the web. All the questions 
raised at the start of this section can be addressed with Sunlight. 
The Sunlight methodology builds upon robust statistical meth­
ods to support scalable, trustworthy, and interpretable results.  Our 
key innovation is to formally separate various operations into mul­
tiple interacting stages organized in a pipeline and identifying all 
the right building blocks from statistics and machine learning to 
leverage at each stage of the pipeline.  The Sunlight pipeline an­
alyzes the data collected from an experiment that tries many dif­
ferent inputs at once, placing them at random in a small number 
of user accounts (logarithmic in the number of inputs) and collect­
ing outputs from each account. The Sunlight pipeline analyzes the 
data to reveal which speciﬁc input likely caused which output. The 
ﬁrst stage, scalable hypothesis generation, creates a set of plausible 
targeting hypotheses regarding which speciﬁc inputs correlate with 
which outputs. It leverages sparsity properties to support the simul­
taneous estimation of the effect of multiple inputs on the outputs, 
a consequence of the same phenomenon that underlies compressed 
sensing  [9].  If  needed,  the  second  stage,  interpretable  hypothe­
sis formation, converts the targeting hypotheses to an interpretable 
form that Sunlight users (such as auditors or researchers) can read­
ily understand.  The third stage, hypothesis testing, establishes the 
statistical signiﬁcance of the interpretable, plausible targeting hy­
potheses by testing their veracity in a separate, testing dataset ini­
tially carved out from the collected data but never used until this 
stage. In some circumstances, specialized tests can establish causa­
tion and not just correlation between the inputs and the outputs. Fi­
nally, the fourth stage, multiple testing correction, accounts for the 
testing of many hypotheses on the same dataset, which increases 
the chance of any individual hypothesis being wrong.  The end re­
sult are validated, interpretable hypotheses about which inputs are 
targeted by each output, along with a statistical signiﬁcance score 
(a p-value) for each hypothesis. 
Sunlight implements this methodology in a modular way, which 
supports  both  the  instantiation  and  the  evaluation  of  each  stage 
based on multiple building blocks from statistics and machine learn­
ing.  We ﬁnd that different mechanisms lead to different trade-offs 
between the scale of and the conﬁdence in the results, hence Sun­
light  lets  its  users  choose  end-to-end  pipelines  that  best  ﬁt  their 
needs. Development of effective such pipelines from existing build­
ing blocks is surprisingly challenging, as different mechanisms in­
teract in unexpected ways in the pipeline.  For example,  our de­
tailed evaluation of various Sunlight pipelines reveals counterintu­
itive inversions of recall near the start of the pipeline and at the 
end.  Indeed, substituting a mechanism for generating hypotheses 
in Stage 1 with one that has higher recall but lower precision, may 
ultimately lower the recall at the end of the pipeline. The reason is 
that the multiple testing correction at Stage 4 tends to favor those 
mechanisms that generate fewer but more accurate hypotheses. 
This paper discusses and evaluates the inherent trade-offs in web 
transparency measurement designs, bringing the following contri­
butions to this emerging research topic: 
1.  A review of 12 recent articles on web transparency measure­
ment and tools,  which highlights the need for new,  princi­
pled methodologies for scalable and trustworthy web trans­
parency measurements. 
(§2) 
2.  The ﬁrst methodology for detecting targeting in large-scale 
experiments with interpretable and statistically justiﬁable re­
sults.  While our methodology focuses on our speciﬁc prob­
lem – ﬁne-grained targeting detection – we believe that its 
conceptual bearings are relevant to other web transparency 
problems (e.g., price discrimination studies at scale).  (§3) 
3.  The ﬁrst system that implements this methodology to detect 
targeting at ﬁne granularity, at scale, and with solid statistical 
justiﬁcation of its results.  Sunlight is modular, allows broad 
design space explorations, and customization of its pipeline 
to strike varied trade-offs of conﬁdence and scale.  (§4) 
4.  A detailed evaluation of Sunlight with comparisons of multi­
ple design options and prior art. Our evaluation methodology 
is new in itself, and (we believe) a useful starting point for fu­
ture transparency infrastructures, an area that currently lacks 
rigorous evaluations.  Our results reveal a trade-off between 
the statistical conﬁdence and number of targeting hypotheses 
that can be made.  They also show that favoring high preci­
sion algorithms can yield better recall at high conﬁdence, and 
that scaling output numbers may require to accept lower sta­
tistical guarantees to ﬁnd sufﬁcient hypotheses.  (§6) 
5.  Results from analyzing targeting of tens of thousands of ads 
in two ecosystems:  Gmail and the broader Web.  Results re­
veal a large and diverse collection of ads targeting websites 
across many categories, including ads that appear to contra­
dict explicit statements made by Google about targeting on 
sensitive topics, as well as advertising network policies about 
ads facilitating recreational drug use.  (§5 and §7). 
6.  Sunlight’s source code and datasets.	  (https://columbia. 
github.io/sunlight/) 
2  Motivation 
At an abstract level, our work is motivated by our desire to un­
derstand how to build principled, scalable infrastructures that can 
bring visibility to today’s dark data-driven web.  Such infrastruc­
tures must be able to detect data ﬂows at great scale and in complex, 
heterogeneous environments, and provide trustworthy assessments 
about these data ﬂows.  We believe there is urgent need for such 
infrastructures  (which  we  term  generically  web  transparency  in­
frastructures), yet we ﬁnd limited progress in the related literature. 
At a concrete level, this paper describes our experience building 
one such scalable and trustworthy1  infrastructure, Sunlight, which 
aims  to  discover  data  ﬂows  in  a  speciﬁc  context:  detecting  the 
causes of targeting phenomena at ﬁne granularity from controlled 
experiments with differentiated inputs. This section begins by mo­
tivating the need for targeting detection systems in particular, after 
which it motivates the broader need for scale and conﬁdence in web 
transparency infrastructures. 
2.1  The Targeting Detection Problem 
Targeting is a pervasive phenomenon on the web and involves 
the  use  of  a  user’s  personal  data  (inputs)  to  tailor  some  content 
(output), such as an ad, a recommendation, a price, search results, 
or news. Sunlight aims to identify the likely causes of each targeted 
output in the context of controlled experiments that test many in­
puts at once.  Numerous use cases exist that could leverage such 
functionality.  For example,  researchers could use it to study tar­
geting at larger scale than was possible before. We provide results 
from our own case studies of ad targeting in Gmail and on the web 
in §7. Following are two other example use cases that broaden the 
scope and help underscore Sunlight’s design requirements. 
1In this paper,  the term trustworthy refers strictly to the level of
conﬁdence (in a statistical sense) one can have in the results of an
investigation assuming the non-malicious service model in §3.3. 
Example 1: Ann, a federal trade commission researcher specializ­
ing in COPPA enforcement, plans to investigate whether and how 
advertisers target children. She hypothesizes that advertisers lever­
age  information  amassed  by  web  trackers  to  bid  for  users  with 
browsing histories characteristic of children.  Ann wants to run a 
large-scale study to both quantify the amount of children-oriented 
targeting,  and  ﬁnd  speciﬁc  instances  of  what  might  be  deemed 
as  inappropriate  or  illegal  targeting  (e.g.,  targeting  pornographic 
movies at teenagers or promoting unhealthy eating habits to young 
children).  The number of websites dedicated to children is large, 
and there are even more neutral websites frequented by both chil­
dren and adults on which targeted ads might appear. Ann fully ex­
pects that child-based targeting will be rare events, hence running 
her experiment at large scale is vital. For any case of inappropriate 
or illegal targeting, Ann plans to investigate through legal means 
(e.g., interview the advertiser) to determine whether the targeting 
was intentional or purely algorithmic.  Such investigations are ex­
pensive, so Ann requires high conﬁdence in an experimental ﬁnding 
to justify her investigative effort. 
Example 2:  Bob, a tech-savvy investigative journalist, wishes to 
investigate  how  coupons  are  targeted  at  users.  Coupon  services 
aim to provide product discounts to users who are likely to be in­
terested in a particular product but may need some incentive to do 
so.  A wide variety of types of information could feed into the tar­
geting decision, including web history, tweets, and Facebook likes. 
Bob would like to try many different activities and see which ones 
are targeted by coupons.  In addition to requiring high conﬁdence 
in the results, Bob needs the results to also be easily interpretable 
so that he and his readers can understand and reason about the im­
plications of the targeting.  Ideally, whatever statements he makes 
in his articles should be directly validated on the datasets and have 
an associated conﬁdence level that he can understand and poten­
tially  communicate  to  his  audience.  For  example,  he  imagines 
statements such as the following to be appropriate for communi­
cation with his readers:  “In our experiments, proﬁles that tweeted 
about weight loss or diets were much more likely to be offered Mc­
Donald’s coupons than those without such tweets.  This result was 
very unlikely (0.01% chance) to have been observed if such tweets 
were not targeted by McDonald’s.” 
2.2  Limitations of Prior Approaches 
The preceding examples illustrate the need for a generic system 
that  supports  targeting  investigations  by  identifying  not  only  the 
fact of targeting but also the likely cause of each targeted output 
at ﬁne granularity (speciﬁc inputs).  The experiments must run at 
large scale and any results must be statistically justiﬁable and in­
terpretable.  We  know  of  no  prior  system  that  satisﬁes  all  these 
properties.  Indeed,  when  turning  to  prior  literature  on  measure­
ments and tools for web transparency to guide our own design, we 
discovered signiﬁcant mismatches at all levels. 
We examined the methodologies used by 12 web transparency 
measurements and tools to study various aspects of data use,  in­
cluding: personalization in search engines [15,29], news and prod­
uct recommendations [16], and online pricing [16, 20, 21, 27]; tar­
geting in advertising on the web [2,8,18,19] and in mobile apps [6, 
22]. We make several observations. 
• Generic, reusable methodologies are scarce:  Until 2014 the ap­
proach  was  to  investigate  speciﬁc  questions  about  web  targeting 
and personalization by developing purpose-speciﬁc, small-scale ex­
perimental methodologies.  This resulted in much redundancy be­
tween investigations,  and typically in small-scale,  one-off exper­
iments.  In 2014, our team developed XRay [18], the ﬁrst generic 
and scalable system design that provides reusable algorithmic build­
ing  blocks  in  support  of  many  targeting  investigations.  Follow­
ing XRay, AdFisher [8] introduced in 2015 a generic, statistically 
sound methodology for small-scale targeting investigations.  (See 
§8 for further discussion of XRay and AdFisher.) 
• Scalability is often disregarded: Most prior works disregard scal­
ability as a core design goal [2, 6, 8, 15, 16, 19–22, 27, 29].  Gen­
erally speaking, the approach is to observe data ﬂows by varying 
one input at a time in successive experiments.  This independent 
treatment of inputs limits the forms of personalization (e.g., based 
on location, cookie proﬁle, or some system-speciﬁc attributes) that 
can be detected by the approach.  Extending such approaches to 
scale to many inputs and hypotheses appears difﬁcult.  For exam­
ple, AdFisher builds a separate classiﬁer for each input and vali­
dates its effect with experimental data.  To investigate targeting on 
combinations of multiple inputs, one must build a classiﬁer and run 
a separate experiment for each such combination – an exponential 
approach that does not scale.  XRay is the only prior system that 
incorporates scalability with many inputs into its design. 
• Conﬁdence assessments are often missing: Most prior works lack 
robust statistical justiﬁcation for their results [2,6,15,16,18–22,27, 
29]. Many works use case-by-case, comparative metrics, where the 
variation  in  different  conditions  is  compared  to  that  of  a  control 
group (e.g., observed price differences [20, 21, 27], fraction of in­
consistent search results [29], Jaccard Index and edit distance [15], 
normalized discount cumulative gain [16]), but do not report any 
assessments of statistical conﬁdence or reliability. Other works de­
tect targeting by running basic statistical tests,  typically to reject 
that a given input seen conditionally on a given output is distributed 
uniformly [2, 6, 22].  Running these tests multiple times requires a 
careful correction step, an aspect that is usually ignored.  Finally, 
our own prior system,  XRay [18],  provides no conﬁdence on an 
individual ﬁnding basis; its predictions are only shown to become 
asymptotically accurate overall  as XRay is  applied to larger  and 
larger systems.  This makes individual results hard to trust and in­
terpret.  In terms of statistical rigor, the most mature approach is 
AdFisher [8] which, for a given input, builds a speciﬁc classiﬁer 
and validates its effect with statistical conﬁdence. 
•  Limited  evaluation  and  design  space  exploration:  Most  prior 
work  lack  a  rigorous  evaluation  of  the  proposed  tools  and  asso­