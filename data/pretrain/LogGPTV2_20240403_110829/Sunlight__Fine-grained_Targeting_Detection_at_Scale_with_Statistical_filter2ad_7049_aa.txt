# Title: Sunlight: Fine-Grained Targeting Detection at Scale with Statistical Confidence

## Authors:
Mathias Lecuyer, Riley Spahn, Yannis Spiliopoulos, Augustin Chaintreau, Roxana Geambasu, and Daniel Hsu

## Abstract
We introduce Sunlight, a system designed to detect the causes of targeting phenomena on the web—such as personalized advertisements, recommendations, or content—at large scale and with robust statistical confidence. The modern web is increasingly complex and opaque, with numerous services collecting, analyzing, using, and exchanging users' personal information. This complexity makes it difficult to determine who has what data, how it is being used, and the impact on users. Existing studies often reveal problematic effects, such as discriminatory pricing and advertising, but they are either too small-scale to generalize or lack formal assessments of confidence, making their results hard to trust or interpret.

Sunlight addresses these challenges by adapting well-established statistical methods to the specific problem of targeting detection. Our methodology formally separates the process into four key phases: scalable hypothesis generation, interpretable hypothesis formation, statistical significance testing, and multiple testing correction. Each phase can be instantiated using various statistical mechanisms, each with different assumptions and trade-offs. Sunlight's modular design allows for exploration of this vast design space, enabling a thorough evaluation of the trade-offs both analytically and experimentally. Our exploration reveals subtle tensions between scalability and confidence. Sunlight's default configuration strikes a balance to provide the first system that can diagnose targeting at fine granularity, at scale, and with solid statistical justification of its results.

We demonstrate the effectiveness of our system through two large-scale measurement studies of targeting on the web, both the largest of their kind. These studies, which focus on ad targeting in Gmail and on the broader web, provide statistically justifiable evidence that contradicts Google's statements regarding the lack of targeting on sensitive and prohibited topics.

## Categories and Subject Descriptors
K.4.1 [Computers and Society]: Public Policy Issues—Privacy, Ethics, Use/abuse of power

## Keywords
Web transparency, privacy, measurement

## Introduction
In a 1913 paper, Louis Brandeis, a proponent of modern views on individual privacy rights, stated, "Sunlight is said to be the best of disinfectants; electric light the most efficient policeman." Unfortunately, today's web is a dark and complex ecosystem driven by the massive collection and monetization of personal data. Numerous web services, mobile applications, and third parties collect vast amounts of information from our daily online interactions, such as website visits, clicks, emails, documents, and pictures. While it is generally understood that companies may use this information for targeted advertising, customized recommendations, and personalized news feeds, the specific ways in which personal data flows through the web ecosystem and how it is used (or misused) by various parties remain largely unknown.

Questions about targeting on the web abound: Are children's online activities being targeted, and if so, what kinds of products are they being offered? Are people being targeted because their browsing patterns suggest they might be vulnerable (e.g., sick, depressed, or in financial difficulty)? Are such inferences being used to increase insurance premiums, deny housing, or place potentially harmful products, such as alcoholic beverages or risky mortgage deals? In other words, is our data being used without our knowledge or consent in ways that affect us? Currently, we lack believable, at-scale answers to these questions.

A promising approach to shed light on large, complex systems is to measure them at scale using scientific methods and tools. Several recent measurement studies have attempted to answer questions about how personal data is used on the web. However, we reviewed 12 of these studies and found a significant gap in scalable experimental methodologies. Generally, prior studies conduct tightly controlled experiments that vary personal data inputs (such as location, search terms, or profile interests) one at a time and observe the effect on service outputs (such as ads, recommendations, or prices) compared to a control group. Unfortunately, these methodologies are either difficult to scale or lack formal notions of confidence, making their results hard to trust, interpret, and generalize. Consequently, our ability to assert broad claims about data use on the web and to exert oversight over this large, complex, and ever-changing ecosystem is limited.

This paper argues that shedding light on the web's complex data ecosystem requires the development of robust experimental methodologies and infrastructures that can be used to answer broad classes of questions at large scale and with interpretable, trustworthy, and statistically justified results. We present Sunlight, a new methodology and system that achieves these goals in the context of one important class of questions: those that require a fine-grained measurement of the causes of targeting phenomena on the web. All the questions raised at the start of this section can be addressed with Sunlight.

The Sunlight methodology builds upon robust statistical methods to support scalable, trustworthy, and interpretable results. Our key innovation is to formally separate various operations into multiple interacting stages organized in a pipeline, leveraging appropriate building blocks from statistics and machine learning at each stage. The Sunlight pipeline analyzes data collected from an experiment that tries many different inputs at once, placing them at random in a small number of user accounts (logarithmic in the number of inputs) and collecting outputs from each account. The pipeline then analyzes the data to reveal which specific input likely caused which output.

The first stage, scalable hypothesis generation, creates a set of plausible targeting hypotheses regarding which specific inputs correlate with which outputs. It leverages sparsity properties to support the simultaneous estimation of the effect of multiple inputs on the outputs, a consequence of the same phenomenon that underlies compressed sensing. If needed, the second stage, interpretable hypothesis formation, converts the targeting hypotheses into an interpretable form that Sunlight users (such as auditors or researchers) can readily understand. The third stage, hypothesis testing, establishes the statistical significance of the interpretable, plausible targeting hypotheses by testing their veracity in a separate, testing dataset initially carved out from the collected data but never used until this stage. In some circumstances, specialized tests can establish causation and not just correlation between the inputs and the outputs. Finally, the fourth stage, multiple testing correction, accounts for the testing of many hypotheses on the same dataset, which increases the chance of any individual hypothesis being wrong. The end result is validated, interpretable hypotheses about which inputs are targeted by each output, along with a statistical significance score (a p-value) for each hypothesis.

Sunlight implements this methodology in a modular way, supporting the instantiation and evaluation of each stage based on multiple building blocks from statistics and machine learning. We find that different mechanisms lead to different trade-offs between the scale of and the confidence in the results, hence Sunlight lets its users choose end-to-end pipelines that best fit their needs. The development of effective pipelines from existing building blocks is surprisingly challenging, as different mechanisms interact in unexpected ways in the pipeline. For example, our detailed evaluation of various Sunlight pipelines reveals counterintuitive inversions of recall near the start of the pipeline and at the end. Indeed, substituting a mechanism for generating hypotheses in Stage 1 with one that has higher recall but lower precision may ultimately lower the recall at the end of the pipeline. The reason is that the multiple testing correction at Stage 4 tends to favor those mechanisms that generate fewer but more accurate hypotheses.

This paper discusses and evaluates the inherent trade-offs in web transparency measurement designs, bringing the following contributions to this emerging research topic:

1. A review of 12 recent articles on web transparency measurement and tools, highlighting the need for new, principled methodologies for scalable and trustworthy web transparency measurements.
2. The first methodology for detecting targeting in large-scale experiments with interpretable and statistically justifiable results. While our methodology focuses on our specific problem—fine-grained targeting detection—we believe that its conceptual bearings are relevant to other web transparency problems (e.g., price discrimination studies at scale).
3. The first system that implements this methodology to detect targeting at fine granularity, at scale, and with solid statistical justification of its results. Sunlight is modular, allowing broad design space explorations and customization of its pipeline to strike varied trade-offs of confidence and scale.
4. A detailed evaluation of Sunlight with comparisons of multiple design options and prior art. Our evaluation methodology is new in itself and (we believe) a useful starting point for future transparency infrastructures, an area that currently lacks rigorous evaluations. Our results reveal a trade-off between the statistical confidence and the number of targeting hypotheses that can be made. They also show that favoring high-precision algorithms can yield better recall at high confidence, and that scaling output numbers may require accepting lower statistical guarantees to find sufficient hypotheses.
5. Results from analyzing the targeting of tens of thousands of ads in two ecosystems: Gmail and the broader Web. These results reveal a large and diverse collection of ads targeting websites across many categories, including ads that appear to contradict explicit statements made by Google about targeting on sensitive topics, as well as advertising network policies about ads facilitating recreational drug use.
6. Sunlight’s source code and datasets. (https://columbia.github.io/sunlight/)

## Motivation
At an abstract level, our work is motivated by the desire to understand how to build principled, scalable infrastructures that can bring visibility to today's data-driven web. Such infrastructures must be able to detect data flows at great scale and in complex, heterogeneous environments, and provide trustworthy assessments about these data flows. We believe there is an urgent need for such infrastructures (which we term generically web transparency infrastructures), yet we find limited progress in the related literature.

At a concrete level, this paper describes our experience building one such scalable and trustworthy infrastructure, Sunlight, which aims to discover data flows in a specific context: detecting the causes of targeting phenomena at fine granularity from controlled experiments with differentiated inputs. This section begins by motivating the need for targeting detection systems in particular, after which it motivates the broader need for scale and confidence in web transparency infrastructures.

### 2.1 The Targeting Detection Problem
Targeting is a pervasive phenomenon on the web and involves the use of a user's personal data (inputs) to tailor some content (output), such as an ad, a recommendation, a price, search results, or news. Sunlight aims to identify the likely causes of each targeted output in the context of controlled experiments that test many inputs at once. Numerous use cases exist that could leverage such functionality. For example, researchers could use it to study targeting at a larger scale than was possible before. We provide results from our own case studies of ad targeting in Gmail and on the web in §7. The following are two other example use cases that broaden the scope and help underscore Sunlight’s design requirements.

**Example 1:**
Ann, a federal trade commission researcher specializing in COPPA enforcement, plans to investigate whether and how advertisers target children. She hypothesizes that advertisers leverage information amassed by web trackers to bid for users with browsing histories characteristic of children. Ann wants to run a large-scale study to both quantify the amount of children-oriented targeting and find specific instances of what might be deemed inappropriate or illegal targeting (e.g., targeting pornographic movies at teenagers or promoting unhealthy eating habits to young children). The number of websites dedicated to children is large, and there are even more neutral websites frequented by both children and adults on which targeted ads might appear. Ann fully expects that child-based targeting will be rare events, hence running her experiment at large scale is vital. For any case of inappropriate or illegal targeting, Ann plans to investigate through legal means (e.g., interview the advertiser) to determine whether the targeting was intentional or purely algorithmic. Such investigations are expensive, so Ann requires high confidence in an experimental finding to justify her investigative effort.

**Example 2:**
Bob, a tech-savvy investigative journalist, wishes to investigate how coupons are targeted at users. Coupon services aim to provide product discounts to users who are likely to be interested in a particular product but may need some incentive to do so. A wide variety of types of information could feed into the targeting decision, including web history, tweets, and Facebook likes. Bob would like to try many different activities and see which ones are targeted by coupons. In addition to requiring high confidence in the results, Bob needs the results to be easily interpretable so that he and his readers can understand and reason about the implications of the targeting. Ideally, whatever statements he makes in his articles should be directly validated on the datasets and have an associated confidence level that he can understand and potentially communicate to his audience. For example, he imagines statements such as the following to be appropriate for communication with his readers: "In our experiments, profiles that tweeted about weight loss or diets were much more likely to be offered McDonald’s coupons than those without such tweets. This result was very unlikely (0.01% chance) to have been observed if such tweets were not targeted by McDonald’s."

### 2.2 Limitations of Prior Approaches
The preceding examples illustrate the need for a generic system that supports targeting investigations by identifying not only the fact of targeting but also the likely cause of each targeted output at fine granularity (specific inputs). The experiments must run at large scale, and any results must be statistically justifiable and interpretable. We know of no prior system that satisfies all these properties. Indeed, when turning to prior literature on measurements and tools for web transparency to guide our own design, we discovered significant mismatches at all levels.

We examined the methodologies used by 12 web transparency measurements and tools to study various aspects of data use, including: personalization in search engines, news and product recommendations, and online pricing; targeting in advertising on the web and in mobile apps. We make several observations.

- **Generic, reusable methodologies are scarce:** Until 2014, the approach was to investigate specific questions about web targeting and personalization by developing purpose-specific, small-scale experimental methodologies. This resulted in much redundancy between investigations, and typically in small-scale, one-off experiments. In 2014, our team developed XRay, the first generic and scalable system design that provides reusable algorithmic building blocks in support of many targeting investigations. Following XRay, AdFisher introduced in 2015 a generic, statistically sound methodology for small-scale targeting investigations.
- **Scalability is often disregarded:** Most prior works disregard scalability as a core design goal. Generally, the approach is to observe data flows by varying one input at a time in successive experiments. This independent treatment of inputs limits the forms of personalization (e.g., based on location, cookie profile, or some system-specific attributes) that can be detected. Extending such approaches to scale to many inputs and hypotheses appears difficult. For example, AdFisher builds a separate classifier for each input and validates its effect with experimental data. To investigate targeting on combinations of multiple inputs, one must build a classifier and run a separate experiment for each such combination—an exponential approach that does not scale. XRay is the only prior system that incorporates scalability with many inputs into its design.
- **Confidence assessments are often missing:** Most prior works lack robust statistical justification for their results. Many works use case-by-case, comparative metrics, where the variation in different conditions is compared to that of a control group, but do not report any assessments of statistical confidence or reliability. Other works detect targeting by running basic statistical tests, typically to reject that a given input seen conditionally on a given output is distributed uniformly. Running these tests multiple times requires a careful correction step, an aspect that is usually ignored. Finally, our own prior system, XRay, provides no confidence on an individual finding basis; its predictions are only shown to become asymptotically accurate overall as XRay is applied to larger and larger systems. This makes individual results hard to trust and interpret. In terms of statistical rigor, the most mature approach is AdFisher, which, for a given input, builds a specific classifier and validates its effect with statistical confidence.
- **Limited evaluation and design space exploration:** Most prior work lacks a rigorous evaluation of the proposed tools and associated trade-offs. Comprehensive evaluations and explorations of the design space are essential to understand the strengths and limitations of different methodologies.