title:Optimization of data collection strategies for model-based evaluation
and decision-making
author:Robert Cain and
Aad P. A. van Moorsel
Optimization of Data Collection Strategies for
Model-Based Evaluation and Decision-Making
Robert Cain, Aad van Moorsel
School of Computing Science, Centre for Cybercrime and Computer Security
Newcastle University, Newcastle upon Tyne, NE1 7RU, UK
{r.b.cain1, aad.vanmoorsel}@ncl.ac.uk
Abstract—Probabilistic and stochastic models are routinely
used in performance, dependability and security evaluation,
and determining appropriate values for model parameters is
a long-standing problem in the practical use of such models.
With the increasing emphasis on human aspects and business
considerations, data collection to estimate parameter values often
gets prohibitively expensive, since it may involve questionnaires,
costly audits or additional monitoring and processing. In this
paper we articulate a set of optimization problems related to
data collection, and provide efﬁcient algorithms to determine
the optimal data collection strategy for a model. The main
idea is to model the uncertainty of data sources and determine
its inﬂuence on output accuracy by solving the model. This
approach is particularly natural for data sources that rely on
sampling, such as questionnaires or monitoring, since uncertainty
can be expressed using the central
theorem. We pay
special attention to the efﬁciency of our optimization algorithm,
using ideas inspired by importance sampling to derive optimal
strategies for a range of parameter values from a single set of
experiments.
Index Terms—data collection; probabilistic modelling; depend-
limit
ability; information security; optimization;
I. INTRODUCTION
The research reported in this paper is motivated by practical
challenges in using probabilistic and stochastic models for
decision-making in IT departments. As an example, studies
to support information security decision-making, such as in
[1], [2], [3], all require insights in a combination of factors:
user behaviour, human work-arounds, business concerns and
technology aspects. Gaining such insights may require various
data sources, from qualitative interviews with decision-makers
and quantitative questionnaires for users, to continuous mon-
itoring of usage of the system. As a consequence, one of the
practical bottlenecks in introducing models in the decision-
making process is the cost of data collection for parameterizing
the model.
In this paper we address this issue by identifying data
collection strategies that guarantee that the most useful data
is collected. Our approach is as follows: we establish the con-
nection between uncertainty in the input data with uncertainty
in the output of the model. Data sources that most reduce
the output uncertainty will be selected, since they correspond
to more accurate results and thus better justiﬁed decisions.
This approach implies that uncertainty of the inputs need to
be modelled. We believe that in many settings it is natural
to assume a Normal distribution around a mean, justiﬁed by
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
the fact that when sampling from data sources the central limit
theorem can be applied to determine uncertainty in the sample
mean.
A potential bottleneck of the proposed approach is the time
the basic algorithm takes to determine the optimal strategy. We
therefore introduce reuse of results across different strategies,
relying on importance sampling equations to weigh the results
appropriately. It will
the application of our
importance sampling inspired speed-up is sensitive to various
choices. We study this in detail in the experiments with a
simple queueing system.
turn out
that
The application of the results in this paper extends to any
use of models for the evaluation of systems as long as there
are sources of data being used to parameterize inputs in the
model. At the same time, it is important to point out that the
work is restricted to collecting data to set single parameters,
such as a probability or a parameter of a distribution (its mean,
variance or other parameter). That is, this paper does not aim
to estimate the distribution of a random variable, such as in
[4].
There is a large amount of related literature on the topic
of data collection, in many different disciplines (statistics,
physics, engineering), but our approach is unique through its
end-to-end perspective, connecting variance of a data source
with that of the output results of a stochastic model. Sensitivity
analysis, uncertainty analysis, experiment design, distribution
ﬁtting and other techniques [5] [6], [7] are all relevant and
related, but consider only part of the problem. The importance
sampling inspired algorithms we devise are new as well. A
more detailed description of related work can be found in
Section V.
This paper is organised as follows. Section II formalizes
the problem of data collection for models, resulting in a
mathematical programming formulation. Section III provides
our approach to make this optimization problem practically
tractable, ﬁrst by introducing the use of the Central Limit The-
orem to express uncertainty stemming from the data sources in
Section III-A. This result in a basic solution algorithm, and we
therefore introduce in Section III-B the importance sampling
inspired modiﬁcations that signiﬁcantly speed up the solution
algorithm. Section IV then shows results for a simple model,
illustrating the challenges in applying importance sampling
effectively, but also illustrating a signiﬁcant efﬁciency gain.
This is followed by Section V, which reviews relevant research
in this area including sensitivity and uncertainty analysis
methods and positions the current work in relation to the
literature. Finally, Section VI draws conclusions and describes
future directions for our work.
II. PROBLEM FORMULATION
This section formalizes our problem in Section II-A, phras-
ing the problem of ﬁnding the best data collection strategy as
a mathematical programming problem in Section II-B.
A. Formal Problem Deﬁnition
The models we consider are discrete-event dynamic systems
[8], of any type, be it Markov chains, stochastic process
algebras or discrete event simulation code, and may also
include rewards (Markov reward models etc.). The model
M takes as input a set P of input parameters or inputs:
P = {p1, . . . , p|P|}. The output of the model is a random
variable Y , which is a function of M and P and typically one
would be interested in a function g(Y ) for instance the mean
of Y or some other reward function over Y . (In a queueing
system, Y may be the steady-state queue length distribution
and we could be interested in the mean number in the queue,
the holding cost of jobs, etc.)
Without loss of generality, we restrict the set P to the
parameters for which data can be collected, and we consider
only ‘individual parameters’ (not full distributions), such as a
probability or the mean or variance of a distribution used in M
(we expand on this in Section III-C). For each input parameter
pi ∈ P there may be multiple data sources available. The
set Di contains all possible data sources for input parameter
pi ∈ P , and with |Di| the number of data sources, we write
Di = {Di,1, . . . , Di,|Di|}. A data source may be a set of
samples from a sensor or the result of a set of questionnaires,
etc. We write D = ∪|P|
i=1Di for the set of all available data
sources.
i=1 ∪|Di|
A data collection strategy s in the set S of all strategies is a
subset of all possible data sources D. In its most general form
S ⊂ ∪|P|
j=1 Di,j, but if appropriate we can restrict the
set of possible strategies. For instance, we may assume that
valid strategies select for each input parameter pi ∈ P one
and only one data source from the set Di. Let the chosen
data source be denoted di, then we can write the strategy
as s = {d1, . . . , d|P|}. If appropriate, we use |s| to denote
the number of data sources in the strategy s ∈ S, so in this
special case |s| = |P|. Another special case is that we want to
optimize the number of samples from a data source; since that
is the common case in this paper, it is useful to introduce be-
spoke notation for this case (instead of assuming each different
sample count is a different data source). The sample count for
data source Di,j ∈ D is denoted as N (Di,j) and a strategy s
is denoted as s = {N (Di,j)}, i = 1, . . . ,|P|, j = 1, . . . ,|Di|.
An example best illustrates our approach to formulating
the optimization problem. Assume our model needs the mean
number of people arriving in a hospital. Consider two different
sources for a single input parameter: (i) a questionnaire of a
subset of the population and (ii) life counting at the hospital
door for one hour each day for a number of weeks. We do
not expect the two sources to give a different result (they are
both unbiased, and with enough samples would converge to
the same value for the model input parameter). However, ne-
glecting cost of each approach, we may expect that counting at
the hospital door gives more accurate results than questioning
arbitrary people, and thus will lead to more accurate output
results. To represent this thinking in an optimization problem,
we need expressions for the uncertainty in input parameters
caused by the various data sources. We also need to identify
for each possible strategy how this uncertainty propagates to
uncertainty in the output g(Y )|s.
For a strategy s ∈ S, we model the uncertainty in the
outcome of a data source by a random variable Xi,j(s) for
parameter pi ∈ P and data source Di,j, thus resulting in the
multi-dimensional random variable:
X(s) = {Xi,j(s)}.
(1)
The uncertainty about the output random variable then depends
on X(s), and we can introduce a random variable g(Y )|X(s),
which reﬂects in the output the uncertainty of input sources. To
attain an effective optimization criterion, we use the variance
of g(Y )|X(s), which we call the output variance. It should be
noted that alternative metrics can be used (such as correlation
ratios [9]), but the output variance is the common metric [7]
and sufﬁces for our purposes. The speciﬁc expression for the
output variance V ar[g(Y )|X(s)] depends on the result we
want out of the model. For instance, if we are interested
in E[Y ], then the variance would be V ar[E[Y ]|X(s)]. In
what follows we usually drop X from the notation and write
V ar[g(Y )|s] to denote the uncertainty in output under the data
collection strategy s.
to comment on the subtle difference
It may be useful
between V ar[Y ] and V ar[E[Y ]|X(s)]. V ar[Y ] is a metric
of interest in its own right [10], which can be computed
directly from the model, as for instance in [11]. V ar[Y ]
has no relation with any data collection strategy s. (For our
example V ar[Y ] is the variance in queue length, for instance.)
However, in V ar[E[Y ]|X(s)], the metric of interest is E[Y ]
(the mean queue length), and V ar[E[Y ]|X(s)] expresses the
uncertainty in the result for E[Y ] caused by the uncertainty in
the input parameter introduced by the data collection strategy.
Note that as a consequence, if X(s) has no randomness,
V ar[E[Y ]|X(s)] = 0 because there is no uncertainty in the
input values.
B. Mathematical Programming Formulation
The above formal deﬁnition of the data collection prob-
lem provides us with an objective function for optimal data
collection strategies, namely to minimize the output variance
V ar[g(Y )|s]. For practical applications, it makes sense to add
additional constraints to the optimization problem, for instance
considering cost of collecting data, or a limit on the total
number of samples for each source, etc. One can also swap
objective and constraints, thus for instance minimizing cost
under some constraint on the output variance. This leads to the
Let s ∈ S be any possible strategy (that is, S ⊂ ∪|P|
following plausible versions of the mathematical programming
formulation of the optimization problem: (1) minimize output
variance given a total number of samples, (2) minimize output
variance within a cost budget, and (3) minimize the cost for
a target output variance.
i=1 ∪|Di|
Di,j), and let di,j = 1 if source Di,j ∈ s, and di,j = 0 other-
wise. We provide the mathematical programming formulation
assuming exactly one source will be selected per input (that
is, |s| = |P|). Importantly, a strategy is variable in terms of
the number of samples associated with each chosen source,
so a strategy s ∈ S is determined by the number of samples
N (Di,j). Assume now that the total number of samples has an
upper bound N, possibly because the time to collect is limited.
The following optimization formulation is then natural:
j=1
subject to:
Optimization Problem 1 (Sample Constraint):
M ins∈S V ar[g(Y )|s]
di,j ∈ {0, 1} for i = 1, . . . ,|P|, j = 1, . . . ,|Di|
(cid:80)|Di|
(cid:80)|Di|
(cid:80)|P|
j=1 di,j = 1 for i = 1, . . . ,|P|
j=1 di,j × N (Di,j) ≤ N
i=1
It is natural to enhance both of the above optimization
problems with a budgeting constraint C. This allows one to
consider the cost of sampling, or the relative effort spent on
different data sources. To illustrate the formalisation, assume
in above Sample Constraint problem formulation that the cost
of a sample of source Di,j is given as Ci,j. The budget
constraint then limits the valid strategies as follows:
subject to:
Optimization Problem 2 (Budget Constraint):
M ins∈S V ar[g(Y )|s]
di,j ∈ {0, 1} for i = 1, . . . ,|P|, j = 1, . . . ,|Di|
(cid:80)|Di|
(cid:80)|Di|
(cid:80)|P|
j=1 di,j = 1 for i = 1, . . . ,|P|
j=1 di,j × Ci,j × N (Di,j) ≤ C
i=1
The dual of this problem is equally interesting: provide a
strategy that minimizes the cost. In that case, the variance
needs to be added as a constraint (with some preset value V )
to create a meaningful optimization problem:
Optimization Problem 3 (Minimize Cost):
(cid:80)|P|
(cid:80)|Di|
j=1 di,j × Ci,j × N (Di,j)
i=1
M ins∈S
subject to:
di,j ∈ {0, 1} for i = 1, . . . ,|P|, j = 1, . . . ,|Di|
V ar[g(Y )|s] ≤ V
(cid:80)|Di|
j=1 di,j = 1 for i = 1, . . . ,|P|
III. SOLUTION
To solve the optimization problems in the previous sec-
tion, we need a way to more concretely express uncertainty
(denoted as X(s) in above) of a data collection strategy. In
so doing, it will be possible to predict the impact of input
uncertainty on the output variance V ar[g(Y )|s], and thus
to determine the best data collection strategy. In addition,
we need an effective way to solve the output variance for
all possible strategies we are interested in. We tackle these
problems as follows.
First, in Section III-A, we assume the sources will generate
samples and that the sample mean is used as the input value
(we will comment on this and other assumptions in Section
III-C). This allows one to use the Central Limit Theorem to
characterize the data source, and through this characterization
one can predict the effectiveness of this source using a basic
algorithm. Secondly, in Section III-B we will use importance