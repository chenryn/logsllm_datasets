suspicious content on the right sidebar outlined in red. Note that the three small
images change between the (a) “No Adblocker” and (b) “With Adblocker”
sub-ﬁgures. Although the content may look like ads, it could also be benign
content related to gaming. Using a browser, we looked at their outgoing URLs
and observed that the two smaller images for Tera Awaken and EOS are ads,
while the third image links to a ﬁrst-party page. Since there are still ads
displayed in (b) “With Adblocker,” we label this example as a positive label.
related to the site content. Fig. 8 illustrates an example of such
“suspicious content”: gaming ads are displayed for a gaming
site, “gamer.com.tw,” which makes it difﬁcult to tell whether
they are ads or ﬁrst-party content. To settle these cases, we visit
the site on our Chrome browser and set up ABP with the same
conﬁguration (settings and ﬁlter lists) as our data collection.
This allows us to further verify whether the content was an ad
by looking at the outgoing link or test it out by clicking on it.
If the content is indeed an ad that goes to a third-party site,
we label it as positive. In our GT data set, we encountered
“suspicious content” only 69 out of 2321 times, thus making
it a corner case.
As described, our labeling methodology relies on using
screenshots. Recall from Section IV-A1 that for a given site,
we visit it four times for the “No Adblocker” and “With
Adblocker” cases, which corresponds to four screenshots for
each case. An alternative approach to labeling would be to use
a browser to check the site, which can produce higher quality
labels. For instance, the browser allows us to view the entire
site as opposed to the limited height of the screenshots, which
is capped at 3000px to deal with inﬁnitely scrolling sites.
However, the browser approach increases the human labeling
efforts. Screenshots offer an attractive compromise: they allow
us to quickly compare the four page visits of “No Adblocker”
and “With Adblocker” with each other, without setting up our
browser and loading the sites four times per case.
10
Label
CV
No CV
Precision Recall Accuracy
0.94
0.92
0.84
0.97
0.93
0.93
F1-score
0.89
0.94
TABLE IV.
CV-INSPECTOR Cross-validation Results. USING A
RANDOM FOREST CLASSIFIER, 93 FEATURES, AND 5-FOLD VALIDATION.
THE LABEL “CV” MEANS SUCCESSFUL CIRCUMVENTION AND “NO CV”
MEANS THAT SITES HAVE NO CV ACTIVITY OR FAILED AT CV.
Fig. 9. Labeling Methodology: We start with a list of sites from both ACVL
and Tranco top-2K, as Candidates for Labeling (CL). We develop an iterative
process for prioritizing which (500 in a batch) sites to inspect and label next,
then add them to ground truth. We bootstrap a classiﬁer by using outlier
detection to ﬁnd positive labels. In each iteration, we apply the classiﬁer on
the remaining sites in CL, sort the sites by decreasing classiﬁer conﬁdence,
and inspect and label the 500 sites where the classiﬁer is most conﬁdent.
Compared to picking randomly 500 sites to label, this heuristic prioritization
discovers more positive labels. For example, see Fig. 10 between “Iteration
Zero” and “Iteration Zero Random.” We add the new labeled samples into
our ground truth, retrain our classiﬁer, and repeat the process for two more
iterations and declare “Done” when the performance converges, as shown in
Fig. 10. We combine all labeled data into our Ground Truth (GT) data set.
Positive Labels and F1 (per Iteration): For our ground truth,
Fig. 10.
we show how many positive labels (sites with successful circumvention) were
discovered within each iteration. When we compare iteration zero and the
randomly chosen iteration zero, we ﬁnd that our methodology discovers twice
as many more positive labels. We see that by the end of iteration two, we
receive diminishing returns on our classiﬁer performance based on its F1-score.
Note that we only ﬁnd 55 positive labels from the Tranco top-2K overall.
Prioritizing Which Sites
to Label. Labeling is time-
consuming and is a well-known bottleneck in all communities
that maintain ﬁlter lists, including EL and ACVL. We develop
a heuristic for prioritizing which sites from CL to inspect and
label ﬁrst to rapidly discover positive labels and minimize the
overall effort. We employ an iterative process shown in Fig 9.
Bootstrapping. We start from CL and perform outlier
detection using Isolation Forest [65]; our intuition is that sites
that utilize circumvention are drastically different from those
that do not. However, not all outliers have circumvention, as
there can be other reasons why a site behaves differently, such
as displaying more page content when ads are not displayed.
Therefore, we still need to inspect and label this initial (108)
outliers, and we ﬁnd 56 positive labels. Next, we order the
remaining sites extracted from ACVL by Tranco ranking, and
pick the top-400 sites. Our intuition comes from Fig. 5, where
there are around 400 sites in the Tranco top-100k sites. We
balance our ground truth with the most popular sites in the
ACVL so that our classiﬁer can generalize well in the wild.
We merge the labeled outliers with the top-400 sites in the
ACVL to obtain our ﬁrst batch of ground truth with ∼500
sites. We train our classiﬁers on this GT.
11
Iteratively enhancing the ground truth. We apply the clas-
siﬁer on the remaining sites of CL, sort the sites by decreasing
classiﬁer conﬁdence, and inspect and label the 500 sites where
the classiﬁer is most conﬁdent. We add the new labeled
samples into our ground truth, we retrain our classiﬁer, and
repeat the process. In each iteration, we choose and label 500
sites and add them to the ground truth, until the performance
converges. Fig. 10 shows diminishing returns after iteration 1,
thus we stop at 2 iterations. The main advantage of prioritizing
which sites to label is that it discovers more positive labels in
each iteration, compared to e.g. choosing 500 random sites to
label. This saves human effort, which is the main bottleneck.
Fig. 10 compares Iteration Zero (with our choice of 500 sites
in decreasing conﬁdence) vs. Iteration Zero (Random choice
of 500 sites) and shows that we discover more than twice the
positive labels, and we achieve a higher F1.
Ground Truth Data Set (GT). We combine all labeled data
(from all iterations, including the randomly selected Iteration
Zero) into one data set, which we refer to as GT. It contains
755 positive labels and 1566 negative labels.
E. The CV-INSPECTOR Classiﬁer
Training the Classiﬁer. We train a classiﬁer that can detect
successful circumvention, using all 93 features extracted in
Sec. IV-C, and the ground truth obtained in Sec. IV-D. We
considered different classiﬁers and observed that Random
Forest performs best. We split the GT data into 70/30 for
training and testings, respectively, and we perform 5-fold
cross-validation. We consider our contribution to lie not in the
ML technique itself but in the domain-knowledge that guided
the design of differential analysis, feature selection, and ground
truth labeling.
Cross-Validation Results. We display the results in Table IV.
Detecting positive labels (i.e. sites succeeding in circumventing
adblockers) is of interest for ﬁlter list authors such as ABP.
Here, we achieve an F1-score of 0.89 and precision of 0.94.
Detecting negatives labels is also important because authors
want to be conﬁdent when disregarding sites without circum-
vention accurately: we see an F1-score of 0.94 and a precision
of 0.92; this becomes invaluable in the monitoring approach
in Sec. V-B as it reduces human effort.
Important Features. Not all 93 features from Sec. IV-C are
equally important. In Table III, we denote some of the features
that end up being in the top-10 most important ones. Fig. 11
also shows the empirical CDFs (ECDF) of four top-features
and illustrates that they can discriminate between sites that
employ successful circumvention or not. For example, consider
the circumvention technique that randomizes the JS ﬁrst-party
path. We see that the path has much more randomness than
sites that did not circumvent the adblocker; see example in
Candidates for Labeling (CL)LabelBootstrap Ground TruthTrain ClassifierApply Classifier to Remaining CLSites fromAnti-CV ListSites from Tranco Top-2KOrder by Decreasing Classifier ConfidenceAdd to Ground TruthSelect Top-500 SitesDonestructure. This makes it hard to identify whether it is an ad
or not and to evaluate the ad link for entropy. In addition,
strip2.xxx uses MobiAds [48] to display ads with a small
square image and the rest is text outside of the image. This
differs from regular ads where it is entirely an image with text
encapsulated in the image. As a result, CV-INSPECTOR cannot
help notify ﬁlter list authors when they should update ﬁlter
rules for these particular cases. However, we argue that CV-
INSPECTOR can be extended to cover corner cases to capture
CV activity, if the sites are of interest to the adblocker.
Another reason for FN is the logic of triggering circum-
vented ads for a user. We ﬁnd that even when a site is
capable of circumventing the adblocker, it may choose not to.
Though more future work is necessary to infer the business
logic of circumvention, we ﬁnd that for a few cases where
the site only triggers circumvention once out of the four
times we load the page, CV-INSPECTOR would predict there
is no circumvention. However,
the classiﬁer conﬁdence is
generally higher (∼0.40), which is close to a positive label
when compared to when a site displays no ads at all within
the four page loads.
Lastly, some FNs are due to the limitations of screenshots
not conveying whether an ad is ﬁrst-party or not. Thus, when
investigating these sites, we manually go to the sites and
found that they were ﬁrst-party ads and should be labeled as a
negative. Here, we see that the classiﬁer was able to determine
the correct label when it comes to ﬁrst-party ads.
2) False Positives (FP): CV-INSPECTOR can mistake sites
that heavily rely on afﬁliation links or third-party links as their
own web content. For example, home-made-videos.com com-
prises completely of links to third-parties with image dimen-
sions that can be considered as ad dimensions. Furthermore,
some mistakes by CV-INSPECTOR can be attributed to a site’s
code mistakes. For instance, when investigating empﬂix.com,
we ﬁnd that CV-INSPECTOR accurately identiﬁes web requests
that correspond to circumvented ad content. However, during
re-insertion, the JS errors out because it expects the existence
of an element with ID “mewTives” but the container is actually
not there. We note that this error does not happen on the
site’s sub pages where the container does exists, and CV-
INSPECTOR correctly predicts that circumvention happens.
We ﬁnd some false positives were actual true positives but
were mislabeled due to the height cap of screenshots. Recall
that we limit the height of the screenshots to be 3000px to be
compatible with sites that would inﬁnitely scroll. We discover
that many adult content sites using ExoClick [32] would re-
inject ads back near the bottom of the page. We see this as a
strength of CV-INSPECTOR that establishes that it can detect
circumvention beyond just the top part of the site (i.e., above
the fold section [54]).
F. Feature Robustness
Fig. 11. Top-Features ECDF. We show empirical CDFs of some of the top
features for our classiﬁer. JS path entropy is the most discriminatory feature.
Listing 2. Speciﬁcally, 40% of sites with circumvention have
path entropy of two or less, while it is more than 80% of sites
with no circumvention. This captures the fact that publishers
can use ﬁrst-party resources that contains circumvention code
to initialize the circumvention process. Thus, randomizing the
path can make it difﬁcult for the adblocker to block it. The
corresponding ECDF is the most discriminatory, compared to
ECDFs of other features, uncovering the fact that randomizing
the path is a more effective technique against adblockers.
Fortunately, our usage of entropy as a feature captures this
difference and can detect the presence of circumvention.
Iframe elements removed and third-party images in ad
locations are also direct mechanisms of circumvention. The
former depicts when sites generally clean up iframes that are
being hidden or blocked by the adblocker. The latter details the
subsequent actions of re-injecting ad images into previously
known ad locations during circumvention. We can infer that
if a site completes more ad re-injection actions, then it has
a higher chance of circumventing the adblocker. The ECDF
of number of blocked events indicates circumvention, where
the adblocker generally blocks more for sites that successfully
circumvent the adblocker. This highlights that adblockers do
not need to block aggressively for sites where they can easily
target
the root cause of ads. However, when obfuscation
techniques are employed, the adblocker must try harder and
has a higher chance of not blocking all ads.
Analysis of Mistakes. Next, we discuss the mistakes made
by CV-INSPECTOR and we explain the root causes of false
negatives (FN) and false positives (FP).
1) False Negatives (FN): FN occur when the site cir-
cumvented the adblocker but CV-INSPECTOR predicted that
it did not. We ﬁnd that CV-INSPECTOR does not perform
well for sites that employ excessive DOM obfuscation. For
example, argumentiru.com displays Yandex [77] ads using
nested custom HTML tags named , while separating
the ad image and the ad link in different parts of the ad DOM
CV-INSPECTOR extracts a diverse range of features that
capture different fundamental characteristics of circumvention.
In this section, we discuss approaches that CV providers
could utilize to attempt to evade each type of features, along
with the approaches’ effectiveness and trade-offs involved. We
argue that it is challenging for CV providers to evade the
features used by the CV-INSPECTOR, while still achieving
12
01234Content-Type JavaScript  First Party Path Entropy0.00.20.40.60.81.0ECDF0.02.55.07.510.0Third Party Images  in Ad Locations0.000.050.100.15Number of Blocked Events  Over First 12 Seconds0.00.20.40.60.81.0ECDF0246810Iframe Elements  RemovedCircumventionNo Circumventiontheir objectives, which are: (1) to evade adblocking ﬁlter rules,
to display ads, and to obtain publisher ad revenue; (2) to not
degrade the user experience on the publisher’s site; and (3) to
minimize the cost and overhead incurred by the provider when
integrating the CV service.
1. Web Request Features. Randomizing URL components,
such as subdomains and paths, is a typical obfuscation tech-
nique that CV providers use to evade ﬁlter rules. However,
our entropy features capture not the exact randomized string
(which would be easy to evade) but the fact that randomization
is used at all (which is robust). An example was shown in
Listing 2. To bypass these features, a CV provider would have
to stop obfuscating URL components all together, i.e. abandon
this circumvention technique.
2. DOM Mutation Features. A CV provider could try to
manipulate DOM mutation features. For instance, instead of
removing DOM nodes,
the provider can hide the nodes.
However, circumvention would still be detected by our features
relating to “DOM attribute changes,” such as display and class.
CV providers could also try to add noise by causing dummy
DOM mutations. However, unless the provider can affect the
“No Adblocker” case as well, it will make circumvention activ-
ity even easier to detect via differential analysis. Furthermore,
adding too many dummy mutations can make the site slow
since the browser must refresh how the page is displayed,