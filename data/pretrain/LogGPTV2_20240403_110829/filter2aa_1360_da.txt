PI:EMAIL,test22,test22,l 24958616 
PI:EMAIL,test69,test69,2145968
After being prompted for the IP address (111 .93.162.238), it asked me for the target index name (leads) and port 
number (9200). It then prompted me to enter the first field 1 wanted to acquire (email). Since I entered a field, 
it then prompted for the next field (first_name). The tool will continue to ask for field names for as long as you 
provide them. Notice there is an empty line in the last "Value". This empty' result tells the script you are finished, 
and it begins collecting the data. When finished, a text file will be saved in the same directory' as your script. In 
this example, it was at ~/Downloads/Programs/Elasticsearch-Crawler/l 11.93.162.238-leads.txt. The title of 
the file was automatically created to reflect the IP address and name of the index. The following are the first 
three lines of this text file.
If this were real data, you would see millions of people's email addresses, names, and telephone numbers. There 
are likely hundreds of legitimate databases on Shodan right now, just waiting to be found. The next time you 
see a news article about a security researcher who found an exposed database containing millions of sensitive 
records, it is very' likely' that Shodan and a similar script was used. If y'our downloaded file contains random text, 
you have likely encountered a patched version of Elasticsearch. At the time of this writing, Elasticsearch 
databases version 6.4.0 and newer were blocking the script. Anything older worked fine. There may' be a new 
release of this crawler, and you may’ need to update your script as follows. Please note these commands are also 
included in the "linux.txt" file previously' downloaded.
This is obviously test data, but assume it was a record containing a real person's name, email address, and phone 
number. Also assume there were over a million records within this index, which is quite common. We could 
save this page, but would be forced to save the undesired fields such as "tags" and "_source". Also, the data 
would be in a difficult format to search. This is where our new Python script is helpful. You have already 
launched the crawl.py script, and should have been presented with a prompt for the IP address of the target. 
The following displays each entry’ 1 submitted for this demonstration.
SQL Files
excsql
excsql "create table”
excsql "create table" "@gmail.com"
excsql "create table" "@gmail.com" "'password'"
Finally, some old-fashioned Google queries might find more data breaches and leaks than one can manage. Let's 
conduct a few examples with SQL files. SQL, often pronounced "sequel", is an acronym for Structured Query 
Language. It is a computer language used in programming and designed for managing data held in a relational 
database management system. In other words, most SQL files are databases of some sort. Many of the most 
popular database breaches were originally released as SQL files. WordPress backups, web forum expons, and 
other online maintenance files are also stored in this format. Searching for public SQL files can reveal surprising 
results. First, let's search Google for these files with the following commands using the tutorials discussed within 
previous chapters.
I predict we will see fewer open databases in the future. While we still hear about sensitive leaks every week, 
word is spreading and companies are locking down their data. This is a good thing for all of us. Until then, I will 
continue to search. If all of this has made you crave more data, consider the site Internet Of Insecurity 
Cmtemetofinsecurity.com).
This returns 10,000 results. Each is an SQL database with at least one entry of "@gmail.com" inside. This 
indicates that active email addresses are within the files, which is indicative of a true breach or leak. The following 
search should reveal data worth analyzing.
Enter your target Elasticsearch IP address in the first field and the target index name in the second. Enter any 
desired search term in the last. This could include the email address or name of your target It is also less invasive 
than downloading all the content. In May of 2019,1 located an extremely sensitive open Elasticsearch database. 
It contained Social Security Numbers and medical records. I did not want to download the data, but I did want 
to search my own name. I entered the IP address and index name in the first two fields and "Bazzell" in the last. 
The query returned dozens of patients' records associated with my last name, but nothing connected to me. This 
was all done within the web browser through their open server, and I did not archive any data onto my own 
machine. I identified the host and reported the leak anonymously. I never received a response, but the data was 
removed the next day.
Data Breaches & Leaks 451
This returns millions of results. While some are files with the extension of ".sql", most of the results are web 
pages ending with ".sql", and are not helpful. Let's modify the search as follows.
The final option within the search tool allows you to search a target IP address, index, and term. Let's conduct 
a new demonstration. Assume you have already searched "customer" within the first "Elasticsearch" option 
within the search tool. You have located an open database of interest and viewed the list of indexes with the 
"Index List" feature. You copied the name of the index and IP address to the "Index View" search. However, 
the file contains millions of records, and you can only see the first 10,000. You might want to search within the 
remaining records to see if your target is present. We can do this with the "Index Search" feature, as seen as the 
last option in Figure 28.05.
This returns 55,000 results which include the exact terms "create table". This is a standard statement within SQL 
files which specifies the names of tables within the database. This filters most of the website names from our 
search and displays only valid SQL files. Next, let's add to our search with the following modification.
ext:sql "create table" "gmail.com" "'password'" "@yahoo.com" -site:github.com
This is a very typical structure within SQL files. The following explains each piece.
exctxt "create table" "gmail.com" "'password'" "yahoo.com" -site:github.com
Public Data Sets
452 Chapter 28
62, (User ID)
'RichardWilson', (Name provided)
'admin', (Username)
'PI:EMAIL', (Email address)
'4d5e02c3f251286d8375040ea2b54e22', (Hashed password)
'Administrator', (Usertype)
0,1,25, (Variable internal codes)
'2008-05-28 07:07:08', (Registration date)
'2009-04-02 13:08:07', (Last visit date)
Many large data sets which are beneficial to investigations are not "breaches" or "leaks". This chapter has focused 
mosdy on data which was never meant to become publicly available. However, there are numerous archives full 
of public information which should be considered for your data collection. Most archives cannot be searched 
through traditional resources such as Google. Instead, we must acquire the data, condense it, and conduct our 
own queries. As a demonstration, 1 will explain how I utilize Usenet archives as a vital part of my investigations.
You could save the entire page as a .txt file (right-click > Save page 
alternative query' for text files is as follows.
(62, 'RichardWilson', 'admin', 'PI:EMAIL', '4d5e02c3f251286d8375040ea2b54e22','Administrate 
r',0,1,25,'2008-05-28 07:07:08’,'2009-04-02 13:08:07')
as...) for your personal data archive. An
This returns 5,400 results. Each is an SQL database with at least one entry' of "gmail.com" inside and a table 
tided "password". Note the single quotes around password within double quotes. This tells Google to search 
specifically for 'password' and not the word alone. 5,400 results are overwhelming, and include a lot of test files 
stored on Github. Since many people use Gmail addresses as the administrator of test databases, we should add 
another email domain as follows.
There may be a trove of sensitive information within these files, so use caution and always be responsible 
Exercise good defense when browsing any of these sites or downloading any data. Trackers, viruses, and overall 
malicious software arc always present in this environment Using a Linux virtual machine and a reputable VPN 
will provide serious protection from these threats. I search and store leaked and breached databases as a part of 
every’ investigation which I conduct. 1 can say without hesitation that these strategies arc more beneficial than 
any other online investigation technique of which I know. Some investigators within my circles possess several 
terabytes of this data from tens of thousands of breaches and leaks. Querying your own offline archive during 
your next investigation, and identifying unique data associated with your target, can be extremely' rewarding. 
Again, I ask you to be responsible. Never use any credentials to access an account and never allow any data 
obtained to be further distributed. Use this public data, stolen by criminals, to investigate and prosecute other 
criminals.
This reveals 228 results. Each arc SQL files which contain at least one Gmail address, one Yahoo address, and 
a table titled password. Furthermore, all results from Github are removed. Most of these results will load as text 
files within your browser. However, some will be quite large and may crash your browser while loading. The 
following is a modified excerpt from an actual result, which I found by searching "@gmail.com" within the 
browser text.
OSINT Original VM. Within Terminal, enter the
pip install internetarchive
Data Breaches & Leaks 453
cd -/Desktop
ia search ' collection:giganews ’ -i > giganewsl.txt
ia search ’ collection: usenethistorical -i > usenethistoricall.txt
From: “David N." 
Subject: Need Fake Texas DL
Date: 1998/07/11
Newsgroups: alt.2600.fake-id
I need a quality bogus TX DL for my 17 year old sister. Can you help me out?
https://archive.org/ details/giganews
https://archive.org/details/usenethistorical
First, we need to install the Internet Archive script within our 
following.
thirty years' worth of Usenet 
to 20,000 files. It is massive, and
These are two independent Usenet archives. Each contains unique records and some redundant data. Let's take 
a look at the second collection. The first folder is tided Usenet-Alt and contains over 15,000 files extracted from 
decades of conversations within the "Alt" communities. Opening the file tided alt.2600.fake-id.mbox reveals 
names, email addresses, dates, and entire messages dating back to 1997. The following is an excerpt.
We now have the script installed and read}7 to use from any folder. Next, we need to identify the Internet Archive 
collections which we want to acquire. For this demonstration, I will focus on the following two data sets.
Today, the Internet Archive presents huge repositories of data containing over 
messages. It is presented in over 26,000 archives, each containing between 1 
would take years to download manually. Fortunately, we can use a download script created by the Internet 
Archive to automatically obtain all desired data. The following tutorial will install the necessary software into 
our Linux virtual machine; download the entire Usenet archive; extract email addresses and names of each 
member; acquire newsgroup data to associate with each person; and condense the data into a usable format. 
Tliis will take some time and may be overwhelming, but the final product is worth the effort. Let's begin.
First, we must create a text file which includes every archive within each collection. The following commands 
navigate to your Desktop and create text files for the Giganews and Usenet Historical archive.org data sets.
Usenet was my first introduction into newsgroups in the early nineties. My internet service provider allowed full 
access to thousands of topics through Outlook Express. I could subscribe to those of interest and communicate 
via email with people from all over the world. This sounds ridiculously common today, but it was fascinating at 
the time. 1 located a newsgroup about my favorite band, and I was quickly trading bootleg cassettes and 
exchanging gossip about the music industry. I was freely sending messages without any consideration of any 
abilities to permanendy archive everything.
Possessing a database of every7 email address and name from thirty7 years of Usenet posts can be very7 beneficial. 
Downloading every7 message can be overkill. This entire data set is over a terabyte in size. Instead of trying to 
download everything, I only7 want specific portions of the data. The Giganews collection includes two files for 
each archive. The first is the "mbox" file which includes the full messages along with user information. These 
are very7 large and could take months to download. The second is a "csv" file which only includes the date, 
message ID, name, email address, newsgroup, and subject of each post. This is a much more manageable amount 
of data which includes the main information desired (name and email address). We will only7 download the 
minimal information needed for our purposes.
ia download —itemlist giganewsl.txt —glob=”* .csv.gz"
gunzip
find
454 Chapter 28
usenet-alt.2600
usenet-alt.2600a
usenet-alt2600crackz
We can now instruct Internet Archive to begin downloading die necessary files. The following command 
downloads only the "csv" files from the Giganews collection. It can take several hours if you have a slow internet 
connection. If you do not have sufficient space within your VAI, consider saving these to an external drive as 
previously instructed.
You should now have thousands of folders, each containing multiple compressed CSV files. This is not vet)' 
useful or clean, so let's extract and combine all the valuable data. The following command will decompress all 
the files and leave only the actual CSV documents. It should be executed from whichever directory contains all 
of the downloaded folders. In my demonstration, it is in the Desktop.
from
John Smith 
newsgroups
microsoft.windowsxp
Subject 
Help Me!
#date
20031204
find . -type f -name \*.csv -printO I xargs -0 cut -fl, 3,4,5 > 
Giganews2.txt
-type f 
-name \*.csv 
-printO
I 
xargs -0 
cut -fl,3,4,5
Let's dissect the Internet Archive commands, "ia" is the application, "search" identifies the type of query, 
"collectiomgiganews" identifies the target data on archive.org, "-i" instructs the application to create an item list, 
and " > giganewsl.txt" provides the desired output. These text files on your Desktop contain the names of all 
the archives within the collections. The following is an excerpt.
This is the command to "find" data to manipulate.
This instructs the command to find all files.
This searches for a regular file type.
This filters to only find a specific file extension (csv).
This directs output to a file instead of the screen.
This is a "pipe" character which separates the commands for a new instruction.
This builds our next command from the previous data as input.
This extracts only the data from columns 1,3,4 and 5 from each CSV.
This instructs the command to send the data to another file.
Giganews2.txt This identifies the output file name.
We still have thousands of files, which is not ideal. The following command will combine ever}7 CSV file into a 
single text file titled Giganews2.txt. Furthermore, it will only extract the columns of data most valuable to us, as 
explained afterward.
Let's break down each portion of this command, as it can be quite useful toward other data sets.
"gunzip" is the command to extract the data from the compressed files, "-r" conducts the command recursively 
through any sub-folders, and " ." continues the action through ever}7 file. Below is a modified excerpt from one 
of the files. It identifies the date of the post (12/04/2003), name of the author (John Smith), email address 
associated with the account (PI:EMAIL), specific newsgroup used (microsoft.windowsxp), and the 
subject of the post (Help me!).
r
.zip"
Next, we must extract the "mbox" files from their compressed containers with the following.
rg -a -F -i -N "From: *' > UsenetHistorical2.txt
This leaves us with a single large file tided UsenetHistorical.txt. Below are a few lines.
sort -u -f Giganews2.txt UsenetHistorical2.txt > UsenetFinal.txt
rg -a -F -i -N bazzell UsenetFinal.txt
The result includes the following partial data.
to otherwise
Data Breaches & Leaks 455
microsoft.public.pocketpc,Steven Bazzell PI:EMAIL 
sd.bio.microbiology,PI:EMAIL Wayne A. Bazzell,M.P.S.E 
sd.crypt,PI:EMAIL Wayne A. Bazzell,M.P.S.E 
sd.crypt,General Darcy J. Bazzell PI:EMAIL
ia download —itemlist usenethistoricall.txt —glob="*
The final result should be a very large file which contains all of the valuable content from within every 
downloaded file. In a moment, we will use this data to research our targets. The Usenet Historical collection is 
stored in a different format, so these instructions will need to be modified. The following steps will extract the 
beneficial data from that collection, and should appear similar to the previous actions. First, we must downloa 
the entire archive with the following command.
The first line identifies Steven Bazzell in the Usenet group of microsoft.public.pocketpc while using an email 
address of steveill@yahoo.com. You could search by names, emails, partial emails, domains, etc. I have 
successfully used my own Usenet archive in the following scenarios.
find . -name
alt.2600.mbox:From: "Bill Smith" 
aft.2600.mbox:From: "yosinaga jackson" 
aft.2600.mbox.From: "yosinaga jackson" 
We do not have as many details within this data set as we did with the Giganews option. However, possessing 
the names, email addresses, and newsgroups provides a lot of value. Both the Giganews2.txt and 
UsenetHistorical2.txt files possess many duplicate entries wasting valuable space. You might consider the 
following command which combines both files into one file while removing all duplicate lines.
”*.zip" -exec unzip {} \;
Finally, we must extract the "From:" line from each file with the following command.
We now possess a single file, quite large in size, which possesses the names, email addresses, and interests of 
most users of the Usenet system over a thirty year period. Now, let's discus ways this can be used during 
investigations. Assume you are researching a target with my last name. When using Ripgrep, your command is 
as follows.
• 
When searching a unique target name, I have uncovered old email addresses which led me 
unknown social network profiles.
• 
When searching an email address, I have identified various interests of the target, determined by the 
newgroups to which he or she has posted.
• 
When searching an email address, it serves as a great way to verify a valid account and establishes a 
minimum date of creation.
• 
When searching a domain, I often identify' numerous email addresses used by die owner.
Ransomware Data
you
456 Chapter 28
".onion" "Dopple" "ransomware" "url" 
".onion" "Ragnar" "ransomware" "url" 
".onion" "REvil" "ransomware" "url" 
".onion" "Conti" "ransomware" "url" 
".onion" "Vice Society" "ransomware" "url" 
".onion" "Clop" "ransomware" "url" 
".onion" "Nefilim" "ransomware" "url" 
".onion" "Everest" "ransomware" "url" 
".onion" "Cuba" "ransomware" "url"
https://mega.nz/file/mSJGGDYI#oNIeyaG2oIHcHQFfGeFuq3zxUp_cCgARVf6bQNqp91s
https://bit.ly/36937Pk
The file contains only the name, email address, and newsgroup fields from both collections in order to keep the 
size minimal. I have removed all duplicates and cleaned the file formatting to exclude any unessential data. It is 
12GB compressed to 3GB. I hope you find it to be useful.
This presents the next conundrum for me. How do I share the locations of this data without jeopardizing myself 
in the process? I cannot provide direct URLs, but I can disclose the following. Any site you may want to visit 
will require the Tor Browser previously explained. Next, Google and the search tutorials presented at the 
beginning of the book should help you find the pages most valuable to ransomware investigations. Queries for 
the following should assist.
If you thought I was pushing the boundaries of ethical data acquisition, the following might make v™’ 
uncomfortable. You have likely heard about ransomware infection. It is the illegal activity by criminals which 
steals data from a company, encrypts all of their files, and demands a ransom to gain access to the unusable data 
remaining on their own servers. When companies began creating better backups which eliminated the need to 
pay the ransom, the criminals took a new route. If the victims do not pay, all of their data is uploaded to the 
internet via Tor websites for anyone to download. This generates terabytes of private documents, email, 
databases, and every other imaginable digital file. Is this OSINT data? I don’t think so. However, an investigator 
working on behalf of a victim company should know where to find this information.
These collections possess over 43 million unique email addresses. You may be questioning the justification of 
the time it would take to create your own archive. While I encourage you to replicate the steps here, I also 
respect the difficulty involved. Therefore, I have made my own Usenet file available for download at the 
following address and shortened URL.
• 
When conducting background checks on targets over the age of 35, I often identify email addresses 
connected to questionable interests. While investigating a potential police officer, I located evidence he 
had previously posted images to a child pornography newsgroup. This was confirmed during the 