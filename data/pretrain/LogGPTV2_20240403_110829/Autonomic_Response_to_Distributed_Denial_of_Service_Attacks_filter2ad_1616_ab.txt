Controller
RealPlayer
Client 2
RealSystem
Server
RealPlayer
Client 1
Fig. 2. Topology and Streaming Media Sessions
Three RealPlayer clients have been placed on three diﬀerent networks so that
the eﬀects of the DDoS attack and CITRA’s autonomic response at multiple lo-
cations throughout the experimental topology can be observed and monitored.
RealPlayer Client 1 resides on the same network as the server and is shown ad-
jacent to it. RealPlayer Client 2 resides on the network in the lower left corner
140
D. Sterne et al.
of the ﬁgure. The RealPlayer Client 3 resides on the network in the upper center
of the ﬁgure, which, unlike Client 2’s network, also includes Stacheldraht ﬂood-
ing agents. Consequently, Client 3’s path to the server is identical to one of the
ﬂooding paths to the server. As test data, we used an 8-minute, 11-second con-
tinuous motion video showing how Boeing 737-300 aircraft are assembled. This
audio/video ﬁle was encoded at 200.1 Kbps. We conﬁgured RealPlayer to use
the “best quality” playback setting with available bandwidth set to 10 Mbps,
the maximum allowable value, and 5 seconds of data buﬀering, the minimum
eﬀective value. Using RealPlayer readouts, we observed an average data rate of
316.7 Kbps over the 310 second period during which all of the data was trans-
mitted to the client. The data rate for a single client varied during this period
from 184.9 Kbps to 668.5 Kbps, with bursts apparently associated with periodic
buﬀer reﬁll requests. We conﬁgured RealPlayer/RealMedia to use UDP as the
transport protocol.
Attack Agent Attack Agent
Attack Agent Attack Agent
Attack Agent Attack Agent
100 Mbps
Switch
Linux
Router 2
Linux
Router 5
100 Mbps
Switch
Linux
Router 4
Discovery
Coordinator
Flood traffic
Rate Limiter
Activation
RealPlayer
Client 3
100 Mbps
Switch
100 Mbps
Switch
Linux
Router 3
Linux
Router 1
Detector
100 Mbps
Hub
100 Mbps
Hub
Master
Controller
RealPlayer
Client 2
RealSystem
Server
RealPlayer
Client 1
Fig. 3. Stacheldraht Flooding and Autonomic Rate Limiting
As shown in Figure 3, Stacheldraht agents reside on the six workstations
on the top row of the ﬁgure; one pair of agents resides on each of the three
upper networks. The Stacheldraht master controller resides on the network in
the lower left of the ﬁgure. Since the master does not need to communicate with
its agents during the attack, its location during the experiment is not signiﬁcant.
We selected Stacheldraht’s UDP ﬂooding attack and chose the RealSystem server
as the attack victim. This sends UDP packets to the server having IP headers
that are virtually indistinguishable from the control ﬂow packets sent to the
server by RealPlayer clients. When a ﬂooding attack occurs, it causes suﬃcient
Autonomic Response to Distributed Denial of Service Attacks
141
congestion to prevent these control packets from reaching the server and data
packets from reaching the client. This in turn causes the video sessions to freeze
after previously buﬀered data has been exhausted.
Figure 3 also shows the positioning and interaction of the CITRA-enabled
components that participate in the autonomic response. When the detector near
the RealSystem server detects the ﬂood, it sends a traceback and mitigation
request to its CITRA neighbors. Its only neighbor is Linux Router 1, which
determines that it is on the ﬂood path. Consequently, it activates a simple rate
limiter that is applied to all subsequent UDP packets addressed to the server. For
purposes of this experiment, rate limiting parameters were ﬁxed at a maximum
average rate of four packets per second with a burst rate of 10 packets per
second. Router 1 then propagates the traceback and mitigation request to each
of its neighbors, i.e., Linux Routers 2, 3, 4, and 5. Routers 1 through 4 each
determine that they are on the attack path and activate rate limiting using the
same parameters. Router 5, however, determines that it is not on the attack
path and does not activate rate limiting.
3.4 Metrics and Instrumentation
Our objective was to measure the extent to which automated response could en-
able resumption and continuation of RealPlayer sessions during a DDoS attack.
This was measured subjectively by observing the video displayed at each client,
and objectively by measuring the number of packets received at each client. To
obtain this measurement, we ran tcpdump on each client and the server, con-
ﬁgured to capture all traﬃc between the client and server. We processed the
tcpdump data with a simple script that counted the number of packets in each
5 second interval for import into a spreadsheet.
Beyond this instrumentation, we used a packet capture tool to monitor the
server’s LAN segment. This enabled us to monitor the progress of the DDoS
attack and response and ensure that the experiment was working according to
plan.
We performed ﬁve test runs each of normal RealPlayer use, RealPlayer under
attack with no response, and RealPlayer under attack with IDIP response. In
each case, we collected the tcpdump data for later analysis.
4 Experimental Results and Interpretation
During normal operation (i.e., no ﬂooding present), RealPlayer clients were able
to complete video data transmission and display with no visible problems. How-
ever, when a Stacheldraht attack was initiated, ﬂooding of the networks and
routers was suﬃcient to immediately prevent RealPlayer clients from commu-
nicating with the server, freezing their video images shortly thereafter. With
autonomic response enabled, the attack had no perceptible eﬀect on clients,
which continued playback without interruption. Traceback and mitigation re-
quest messages were able to move upstream against the ﬂood, causing CITRA-
142
D. Sterne et al.
enabled routers in the ﬂood path to activate rate limiting, reducing downstream
ﬂooding and enabling resumption of RealPlayer sessions.
Figure 4 shows results from four representative experiment runs. The x axis
represents time in 5 second intervals; the y axis represents the number of packets
received per second at Client 3. Packet rates were nearly identical for all three
clients for each run.
The “Normal” run (Figure 4a) represents the number of packets per second
captured at Client 3 when no ﬂooding is present. During the initial 30 seconds,
RealPlayer clients received about 140 packets per second, to ﬁll playback buﬀers.
This would taper oﬀ to about 50 packets per second after the ﬁrst 30 seconds,
with occasional bursts of about 140 packets per second, presumably to reﬁll
buﬀers. This was consistent with the data rates shown on the client statistic
displays. For the three clients together, this would result in about 420 packets
per second initially, with the network load reduced to about 150 packets per
second. Note that although the video was over 8 minutes long, packets arrive
at the client for only a little over 5 minutes. This is because the client caches
the data to ensure smooth operation during short periods of congestion. We
attempted to disable this feature by setting the cache length to 0 seconds, but
this showed no noticeable diﬀerence from when the value was set to 5 seconds,
which was the value used in the experiment.
Figure 4b shows that during the “Flood” run, the packet reception rates for
clients drop to zero very quickly after the ﬂood starts (approximately 40 seconds
into the run) and stay at zero for the duration of the run. These measurements
are consistent with the RealPlayer statistics graphs displayed on the clients. The
extent of ﬂooding was conﬁrmed by the packet capture tool on the server’s LAN
segment, which showed data rates on that segment of about 75 Mbps, i.e., at
least 75% loading of the LAN’s 100 Mbps capacity. We also observed congestion
indications on the “Internet” hub connecting the ﬁve routers. The packet col-
lision indicator light remained lit continuously, reﬂecting frequent unsuccessful
transmission attempts. We suspect that the load on the server LAN segment
was artiﬁcially constrained by throughput limitations of Router 1, a 200 MHz
Pentium Pro that connected the server LAN to the rest of the network.
When we ran ﬂood attacks with autonomic responses enabled, we observed
two diﬀerent behavior patterns: (1) full recovery (Figure 4c) and (2) degraded
recovery (Figure 4d). The “Full Recovery” histogram shows a dramatic drop in
packets received at the client for approximately 10-12 seconds beginning around
40 seconds into the run, followed by a packet reception pattern similar to normal
operation. This 10-12 second gap represents the time between the onset of the
attack and the autonomic activation of rate limiting throughout the network. For
full recovery, the client received the same number of total packets as during non-
ﬂood runs and the client displays showed no evidence of an attack, suggesting
that cached data was suﬃcient to cover the short, ﬂood-induced outage.
The “Degraded Recovery” histogram shows a longer transmission reception
time with lower packet rate and fewer total packets received at the client. The to-
tal number of video packets received at the client decreased dramatically (about
Autonomic Response to Distributed Denial of Service Attacks
143
a. Normal
5
55
105
155
205
255
Time
b. Flood
305
355
405
455
5
55
105
155
205
255
Time
c. FullRecovery
305
355
405
455
5
55
105
155
205
255
Time
305
355
405
455
d. Degraded Recovery
d
n
o
c
e
S
r
e
p
s
t
e
k
c
a
P
d
n
o
c
e
S
r
e
p
s
t
e
k
c
a
P
d
n
o
c
e
S
r
e
p
s
t
e
k
c
a
P
d
n
o
c
e
S
r
e
p
s
t
e
k
c
a
P
160
140
120
100
80