recorded in all appropriate catch blocks, showing that
our methodology can drive the application through all of
its responses to these faults, obtaining good test coverage
for them. However, NIC DOWN often causes latent er-
rors, and its injection into the six vulnerable try blocks
yielded only two covered catches. We re-ran our tests for
NIC DOWN in fault-not-cancel mode and were able to also
cover catch 3 with this fault. We also tried fault-reinject
mode, but this did not affect our results for NIC DOWN.
s
t
l
u
a
F
#
6
5
4
3
2
1
0
Fault Not Covered
Extra Fault Covered by fault-not-cancel mode
Fault Covered by fault-cancel mode
5
4
4
4
0
1
2
3
Catch Blocks
3
7
2
8
1
6
Figure 2. Coverage Data
Figure 2 summarizes the
|ei|
|fi| values for each catch
graphically, and Table 4 gives our aggregate coverage met-
rics for the tested code. Our (fraction of) covered catches
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:06:14 UTC from IEEE Xplore.  Restrictions apply. 
metric is the most stringent, drawing attention to the fact
that about half of the catches have not been fully tested.
The other two metrics take into account the amount of cov-
erage of the partially covered catches. In this experiment,
we obtained slightly higher values for overall fault-catch
coverage than average fault-catch coverage, as the former
effectively weighs the individual catch average ratios by the
number of associated faults and our lowest percentage cov-
erage occurred on a catch with only two faults.
Mode
fault-cancel
fault-not-cancel
Average
Covered
Fault-catch Fault-catch Catches
84.3%
87.1%
42.9%
57.1%
Overall
85.2%
88.9%
Table 4. Aggregated Report of Coverage
Our data show that we can inject faults, instrument pro-
grams to measure fault-catch coverage, and achieve signiﬁ-
cant levels of fault-catch coverage for Mufﬁn. For faults that
do not produce latent errors, we produced 100% fault-catch
coverage, suggesting that our techniques are valuable. We
were less successful with faults that do produce latent er-
rors, covering four of the seven NIC DOWN/catch com-
binations in fault-not-cancel mode. While these coverage
results are valuable in that they guide the tester to those
fault-recovery codes that are not fully tested, as discussed
before it is very important to improve our coverage for these
faults resulting in latent errors.
5 Related Work
Researchers in the dependability and software engineer-
ing communities have studied the problems of program cov-
erage and fault coverage extensively. Given the limited
space, we will focus here on a comparison of our work
with previous research on fault injection using program-
coverage metrics. An understanding of probabilistic fault
coverage [9], its relationship to system dependability [13],
and fault-injection [3] also is essential to understand the
context of our work. Our program-coverage metrics are
most similar to those used in dataﬂow testing [26]. These
references have been discussed in Section 2.1.
Our fault-injection experiments most closely resem-
ble those measuring responses to errors using traditional
program-coverage metrics. Tsai et. al [34] placed break-
points at key program points along known execution paths
and injected faults at each point, (e.g., by corrupting a value
in a register). Their work differs from ours in its goal, the
kinds of faults injected, and their deﬁnition of coverage.
The primary goal of their approach was to increase fault
activations and fault coverage, not to increase program cov-
erage. They injected a set of hardware-centric faults such as
corrupting registers and memory; these faults primarily af-
fected program state, not communication with the operating
system or I/O hardware. They used a basic-block deﬁnition
of program coverage, rather than measuring coverage of a
program-level construct such as a catch block. Bieman
et. al [7] explored an alternative approach where a fault is
injected by violating a set of pre- or post-conditions in the
code, which are required to be expressed explicitly in the
program by the programmer. This approach used branch
coverage, a program-coverage metric.
In the terminology of Hamlet’s summary paper recon-
ciling traditional program-coverage metrics and probabilis-
tic fault analysis [16], our work can be classiﬁed as a
probabilistic input sequence generator, exploring the low-
frequency inputs to a program. Using the terminology pre-
sented by Tang and Hecht [32], which surveyed the entire
software dependability process, our method can be classi-
ﬁed as a stress-test, because it generates unlikely inputs to
the program.
6 Conclusions
We have posed what we believe to be a new challenge
in the ﬁeld of techniques for development of highly avail-
able systems: to determine whether all of the fault-recovery
code in a Web services application has been exercised on an
appropriate set of faults. We have presented our fault-catch
coverage metric, which formalizes what it means to meet
this challenge successfully, and have shown that it is possi-
ble to instrument programs to collect coverage information
at run-time. Our metric combines ideas of testing software
in response to injected faults, developed by the dependabil-
ity community, with ideas of testing for coverage of speciﬁc
program constructs, developed by the software engineering
community.
We also have developed an API that allows the program
being tested to direct a fault-injection engine and have ex-
tended Mendosus to respond to this API. We have described
compiler analyses that can be applied to Java source or byte-
codes in order to instrument codes to direct fault injection
to produce high fault-catch coverage.
Our preliminary case study results with Mufﬁn indi-
cate that our approach is highly effective for faults that do
not create latent errors (i.e., 100% coverage), and some-
what effective for faults that do (i.e., covering 4 of the 7
NIC DOWN/catch combinations). Next we plan to en-
hance our approach to achieve better coverage in the pres-
ence of latent errors and to study issues of testing of dis-
tributed applications.
References
[1] The Eclipse IDE. See http://www.eclipse.org/.
[2] The Mufﬁn world wide web ﬁltering system.
See
http://muffin.doit.org/.
[3] J. Arlat, A. Costes, Y. Crouzet, J.-C. Laprie, and D. Powell.
Fault injection and dependability evaluation of fault-tolerant
systems. IEEE Transactions on Computers, 42(8):913–923,
Aug. 1993.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:06:14 UTC from IEEE Xplore.  Restrictions apply. 
[4] K. Arnold and J. Gosling. The Java Programming Lan-
guage, Second Edition. Addison-Wesley, 1997.
[5] M. Arnold and P. F. Sweeney. Approximating the calling
context tree via sampling. Technical Report RC 21789, IBM
T.J. Watson Research Center, July 2000.
[6] D. Bacon and P. Sweeney. Fast static analysis of c++ vir-
tual functions calls. In Proceedings of ACM SIGPLAN Con-
ference on Object-oriented Programing Systems, Languages
and Applications (OOPSLA’96), pages 324–341, Oct. 1996.
[7] J. Bieman, D. Dreilinger, and L. Lin. Using fault injection
to increase software test coverage. In Proc. 7th Int. Symp. on
Software Reliability Engineering (ISSRE’96), pages 166–74.
IEEE Computer Society Press, 1996.
[8] R. V. Binder. Testing Object-oriented Systems. Addison
Wesley, 1999.
[9] W. G. Bouricius, W. C. Carter, and P. Schneider. Reliability
modeling techniques for self repairing computer systems. In
In Proceedings of the 24th National Conference of the ACM,
pages 295–309, March 1969.
[10] M. Cukier, R. Chandra, D. Henke, J. Pistole, and W. H.
Sanders. Fault injection based on a partial view of the global
state of a distributed system. In Symposium on Reliable Dis-
tributed Systems, pages 168–177, 1999.
[11] S. Dawson, F. Jahanian, and T. Mitton. ORCHESTRA: A
Fault Injection Environment for Distributed Systems.
In
Proc. 26th Int. Symp. on Fault Tolerant Computing(FTCS-
26), pages 404–414, Sendai, Japan, June 1996.
[12] J. Dean, D. Grove, and C. Chambers.
Optimization
of object-oriented programs using static class hierarchy.
In Proceedings of 9th European Conference on Object-
oriented Programming (ECOOP’95), pages 77–101, 1995.
[13] J. B. Dugan and K. S. Trivedi. Coverage modeling for de-
pendability analysis of fault-tolerant systems. IEEE Trans-
actions on Computers, 38(6):775–787, June 1989.
[14] C. Fu, R. P. Martin, K. Nagaraja, T. D. Nguyen, B. G. Ryder,
and D. Wonnacott. Compiler-directed program-fault cover-
age for highly available java internet services. Technical Re-
port DCS-TR-518, Department of Computer Science, Rut-
gers University, Jan. 2003.
[15] D. Grove and C. Chambers. A framework for call graph con-
struction algorithms. ACM Transactions on Programming
Languages and Systems (TOPLAS), 23(6), 2001.
[16] D. Hamlet. Foundations of software testing: dependability
theory. In Proceedings of the 2nd ACM SIGSOFT Sympo-
sium on Foundations of software engineering, pages 128–
139. ACM Press, 1994.
[17] S. Han, K. Shin, and H. Rosenberg. DOCTOR: An Inte-
grated Software Fault Injection Environment for Distributed
Real-Time Systems.
In Int. Computer Performance and
Dependability Symp. (IPDS’95), pages 204–213, Erlangen,
Germany, Apr. 1995.
[18] H. Hecht and P. Crane. Rare conditions and their effect on
software failures. In In Proceedings of the Annual Reliability
and Maintainability Symposium, pages 334–337, Anaheim,
CA, Jan. 1994.
[19] M. Kalyanakrishnam, Z. Kalbarczyk, and R. Iyer. Failure
Data Analysis of a LAN of Windows NT Based Comput-
ers. In Proceedings of the 18th Symposium on Reliable and
Distributed Systems (SRDS ’99), 1999.
[20] G. A. Kanawati, N. A. Kanawati, and J. A. Abraham. FER-
RARI: A Tool for the Validation of System Dependabil-
ity Properties.
In Proc. 22nd Int. Symp. on Fault Toler-
ant Computing(FTCS-22), pages 336–344, Boston, Mas-
sachusetts, 1992. IEEE Computer Society Press.
[21] X. Li, R. P. Martin, K. Nagaraja, T. D. Nguyen, and
B. Zhang. Mendosus: A SAN-Based Fault-Injection Test-
Bed for the Construction of Highly Available Network Ser-
vices. In Proceedings of the 1st Workshop on Novel Uses of
System Area Networks (SAN-1), Cambridge, MA, Jan. 2002.
[22] D. Liang, M. Pennings, and M. Harrold. Extending and
evaluating ﬂow-insensitive and context-insensitive points-to
analyses for java.
In Proceedings of the 2001 ACM SIG-
PLAN - SIGSOFT Workshop on Program Analysis for Soft-
ware Tools and Engineering, pages 73–79, June 2001.
[23] B. Marick. The Craft of Software Testing, Subsystem Test-
ing Including Object-based and Object-oriented Testing.
Prentice-Hall, 1995.
[24] A. Milanova, A. Rountev, and B. G. Ryder. Parameterized
object sensitivity for points-to and side-effect analysis.
In
Proceedings of the International Symposium on Software
Testing and Analysis, pages 1–11, 2002.
[25] G. J. Myers. The Art of Software Testing. John Wiley and
Sons, 1979.
[26] S. Rapps and E. Weyuker. Selecting software test data us-
ing data ﬂow information. IEEE Transactions on Software
Engineering, SE-11(4):367–375, Apr. 1985.
[27] A. Rountev, A. Milanova, and B. G. Ryder. Points-to anal-
ysis for java using annotated constraints.
In Proceedings
of the Conference on Object-oriented Programming, Lan-
guages, Systems and Applications, pages 43–55, 2001.
[28] Z. Segall, D. Vrsalovic, D. Siewiorek, D. Yaskin, J. Kow-
nacki, J. Barton, D. Rancey, A. Robinson, and T. Lin. FIAT
— Fault Injection based Automated Testing environment. In
Proc. 18th Int. Symp. on Fault-Tolerant Computing (FTCS-
18), pages 102–107, Tokyo, Japan, 1988. IEEE Computer
Society Press.
[29] R. Sethi. Programming Languages, Concepts and Con-
structs, 2nd Edition. Addison Wesley, 1996.
[30] Sun-Microsystems.
Java development kit 1.2.
See
http://java.sun.com/products/jdk/1.2/-
docs/api/.
[31] N. Talagala and D. Patterson. An Analysis of Error Be-
haviour in a Large Storage System. In Proceedings of the
Annual IEEE Workshop on Fault Tolerance in Parallel and
Distributed Systems, April 1999.
[32] D. Tang and H. Hecht. An approach to measuring and as-
sessing dependability for critical software systems.
In In
Proceedings of the Eighth International Symposium on Soft-
ware Reliability Engineering, pages 192–202, Albuquerque,
NM, Nov. 1997.
[33] F. Tip and J. Palsberg. Scalable propagation-based call graph
construction algorithms. In Proceedings of the Conference
on Object-oriented Programming, Languages, Systems and
Applications, pages 281–293, Oct. 2000.
[34] T. Tsai, M. Hsueh, H. Zhao, Z. Kalbarczyk, and R. Iyer.
Stress-based and path-based fault injection. IEEE Transac-
tions on Computers, 48(11):1183–1201, Nov. 1999.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:06:14 UTC from IEEE Xplore.  Restrictions apply.