guessing methods that use natural language, it also in-
cludes the web2 list [11], Google web corpus [47], and
an inﬂection dictionary [78]. This set totals 33 million
passwords and 5.9 million natural-language words.
The second set (the PGS++ training set) augments the
PGS training set with additional leaked and cracked pass-
word sets [1,2,3,6,7,9,12,13,14,15,16,20,23,25,42,43,
55, 56, 57, 62, 63, 67, 75, 77, 85, 90]. For methods that use
natural language, we include the same natural-language
sources as the PGS set. This set totals 105 million pass-
words and 5.9 million natural-language words.
4.2 Testing Data
For our testing data we used passwords collected from
Mechanical Turk (MTurk) in the context of prior re-
search studies, as well as a set sampled from the leak
of plaintext passwords from 000webhost [40]. In addi-
tion to a common policy requiring only eight characters,
we study three less common password policies shown to
be more resistant to guessing [66,80]: 4class8, 3class12,
and 1class16, all described below. We chose the MTurk
sets to get passwords created under more password poli-
cies than were represented in leaked data. Passwords
generated using MTurk have been found to be similar
to real-world, high-value passwords [38, 66]. Nonethe-
less, we chose the 000webhost leak to additionally com-
pare our results to real passwords from a recently leaked
password set. In summary, we used ﬁve testing datasets:
• 1class8: 3,062 passwords longer than eight charac-
• 1class16: 2,054 passwords longer than sixteen char-
• 3class12: 990 passwords that must contain at least
three character classes (uppercase letters, lowercase
letters, symbols, digits) and be at least twelve char-
acters long collected for a research study [80]
acters collected for a research study [59]
ters collected for a research study [59]
• 4class8: 2,997 passwords that must contain all four
character classes and be at least eight characters
long collected for a research study [66]
• webhost: 30,000 passwords randomly sampled
from among passwords containing at least eight
characters in the 000webhost leak [40]
4.3 Guessing Conﬁguration
PCFG We used a version of PCFG with termi-
nal smoothing and hybrid structures [60], and in-
cluded natural-language dictionaries in the training data,
weighted for each word to count as one tenth of a pass-
word. We also separated training for structures and ter-
minals, and trained structures only on passwords that
conform to the target policy. This method does not gen-
erate passwords that do not match the target policy.
For PCFG, Monte Carlo methods are not able to es-
timate unique guess numbers for passwords that have
the same probability. This phenomenon manifests in the
Monte Carlo graphs with jagged edges, where many dif-
ferent passwords are assigned the same guess number
(e.g., in Figure 5c before 1023). We assume that an opti-
mal attacker could order these guesses in any order, since
they all have the same likelihood according to the model.
Hence, we assign the lowest guess number to all of these
guesses. This is a strict overestimate of PCFG’s guessing
effectiveness, but in practice does not change the results.
182  25th USENIX Security Symposium 
USENIX Association
8
d
e
s
s
e
u
g
t
n
e
c
r
e
P
60%
40%
20%
0%
Transference
Normal
101 104 107101010131016101910221025
Guesses
(a) Transference learning
d
e
s
s
e
u
g
t
n
e
c
r
e
P
80%
60%
40%
20%
0%
NoNL
WithNL
101 104 107 101010131016101910221025
Guesses
(b) Natural-language dictionaries
d
e
s
s
e
u
g
t
n
e
c
r
e
P
80%
60%
40%
20%
0%
LightTutoring
Small
HeavyTutoring
101 104 107101010131016101910221025
Guesses
(c) Tutoring
Figure 2: Alternative training methods for neural networks. The x-axes represent the number of guesses in log scale. The y-axes
show the corresponding percentage of 1class16 passwords guessed. In (b), WithNL is a neural network trained with natural-language
dictionaries, and NoNL is a neural network trained without natural-language dictionaries.
Markov Models We trained 4-, 5-, and 6-gram mod-
els. Prior work found the 6-gram models and additive
smoothing of 0.01 to be an effective conﬁguration for
most password sets [65]. Our results agree, and we use
the 6-gram model with additive smoothing in our tests.
We discard guesses that do not match the target policy.
Mangling Wordlist Methods We compute guess num-
bers using the popular cracking tools Hashcat and John
the Ripper (JtR). For Hashcat, we use the best64 and
gen2 rule sets that are included with the software [83].
For JtR, we use the SpiderLabs mangling rules [86]. We
chose these sets of rules because prior work found them
effective in guessing general-purpose passwords [89]. To
create the input for each tool, we uniqued and sorted the
respective training set by descending frequency. For JtR,
we remove guesses that do not match the target policy.
For Hashcat, however, we do not do so because Hash-
cat’s GPU implementation can suffer a signiﬁcant perfor-
mance penalty. We believe that this models a real-world
scenario where this penalty would also be inﬂicted.
5 Evaluation
We performed a series of experiments to tune the train-
ing of our neural networks and compare them to exist-
ing guessing methods. In Section 5.1, we describe ex-
periments to optimize the guessing effectiveness of neu-
ral networks by using different training methods. These
experiments were chosen primarily to guide our deci-
sions about model parameters and training along the de-
sign space we describe in Section 3.2, including training
methods, model size, training data, and network architec-
ture. In Section 5.2, we compare the effectiveness of the
neural network’s guessing to other guessing algorithms.
Finally, in Section 5.3, we describe our browser imple-
mentation’s effectiveness, speed, and size, and we com-
pare it to other browser password-measuring tools.
5.1 Training Neural Networks
We conducted experiments exploring how to tune neural
network training, including modifying the network size,
using sub-word models, including natural-language dic-
tionaries in training, and exploring alternative architec-
tures. We do not claim that these experiments are a com-
plete exploration of the space. Indeed, improving neural
networks is an active area of research.
Transference Learning We ﬁnd that the transference
learning training, described in Section 3.2,
improves
guessing effectiveness. Figure 2a shows in log scale
the effect of transference learning. For example, at 1015
guesses, 22% of the test set has been guessed with trans-
ference learning, as opposed to 15% without transfer-
ence learning. Using a 16 MB network, we performed
this experiment on our 1class16 passwords because they
are particularly different from the majority of our train-
ing set. Here, transference learning improves password
guessing mostly at higher guess numbers.
Including Natural-Language Dictionaries We exper-
imented with including natural-language dictionaries in
the neural network training data, hypothesizing that do-
ing so would improve guessing effectiveness. We per-
formed this experiment with 1class16 passwords because
they are particularly likely to beneﬁt from training on
natural-language dictionaries [91]. Networks both with
and without natural language data were trained using the
transference learning method on long passwords. Nat-
ural language was included with the primary batch of
training data. Figure 2b shows that, contrary to our hy-
potheses, training on natural language decreases the neu-
ral network’s guessing effectiveness. We believe neural
networks do not beneﬁt from natural language, in con-
trast to other methods like PCFG, because this method of
training does not differentiate between natural-language
dictionaries and password training. However, training
data could be enhanced with natural language in other
ways, perhaps yielding better results.
USENIX Association  
25th USENIX Security Symposium  183
9
90%
d
e
s
s
e
u
g
t
n
e
c
r
e
P
60%
30%
1class8 Large
1class8 Small
4class8 Large
4class8 Small
Webhost Small
Webhost Large
1class16 Large
1class16 Small
passwords. The webhost test set is the only set for which
the larger model performed worse. We believe that this
is due to the lack of suitability of the particular training
data we used for this model. We discuss the differences
in training data more in Section 5.2.
0%
101 104 107101010131016101910221025
Guesses
Figure 3: Neural network size and password guessability.
Dotted lines are large networks; solid lines are small networks.
Password Tokenization We ﬁnd that using hybrid,
sub-word level password models does not signiﬁcantly
increase guessing performance at low guess numbers.
Hybrid models may represent the same word in multiple
different ways. For example, the model may capture a
word as one token, ‘pass’, or as the letters ‘p’, ‘a’, ‘s’, ‘s’.
Because Monte Carlo simulations assume that passwords
are uniquely represented, instead of using Monte Carlo
methods to estimate guess numbers, we calculated guess
numbers by enumerating the most probable 107 guesses.
However, at this low number of guesses, we show this
tokenization has only a minor effect, as shown in Fig-
ure 4b. We conducted this experiment on long passwords
because we believed that they would beneﬁt most from
tokenization. This experiment shows that there may be
an early beneﬁt, but otherwise the models learn similarly.
We consider this result to be exploratory both due to our
low guessing cutoff and because other options for tuning
the tokenization could produce better results.
Model Size We ﬁnd that, for at least some password
sets, neural network models can be orders of magnitude
smaller than other models with little effect on guessing
effectiveness. We tested how the following two model
sizes impact guessing effectivess: a large model with
1,000 LSTM cells or 15,700,675 parameters that uses
60 MB, and a small model with 200 LSTM cells or
682,851 parameters that takes 2.7 MB.
The results of these experiments are shown in Figure 3.
For 1class8 and 4class8 policies, the effect of decreas-
ing model size is minor but noticeable. However, for
1class16 passwords, the effect is more dramatic. We at-
tribute differences between the longer and shorter poli-
cies with respect to model size to fundamental differ-
ences in password composition between those policies.
Long passwords are more similar to English language
phrases, and modeling them may require more param-
eters, and hence larger networks, than modeling shorter
Tutored Networks To improve the effectiveness of our
small model at guessing long passwords, we attempted to
tutor our small neural network with randomly generated
passwords from the larger network. While this had a mild
positive effect with light tutoring, at a roughly one to two
ratio of random data to real data, the effect does not seem
to scale to heavier tutoring. Figure 2c shows minimal
difference in guessing accuracy when tutoring is used,
and regardless of whether it is light or heavy.
Backwards vs. Forwards Training As described in
Section 3.2, processing input backwards rather than for-
wards can be more effective in some applications of
neural networks [48]. We experiment with guessing
passwords backwards, forwards, and using a hybrid ap-
proach where half of the network examines passwords
forwards and the other half backwards. We observed
only marginal differences overall. At the point of great-
est difference, near 109 guesses, the hybrid approach
guessed 17.2% of the test set, backwards guessed 16.4%
of the test set and forwards guessed 15.1% of the test set.
Figure 4a shows the result of this experiment. Since the
hybrid approach increases the amount of time required to
train with only small improvement in accuracy, for other
experiments we use backwards training.
Recurrent Architectures We experimented with two
different types of recurrent neural-network architectures:
long short-term memory (LSTM) models [54] and a re-
ﬁnement on LSTM models [58]. We found that this
choice had little effect on the overall output of the net-
work, with the reﬁned LSTM model being slightly more
accurate, as shown in Figure 4c.
5.2 Guessing Effectiveness
Compared to other individual password-guessing meth-
ods, we ﬁnd that neural networks are better at guessing
passwords at a higher number of guesses and when tar-
geting more complex or longer password policies, like
our 4class8, 1class16, and 3class12 data sets. For exam-
ple, as shown in Figure 5b, neural networks guessed 70%
of 4class8 passwords by 1015 guesses, while the next best
performing guessing method guesses 57%.
Models differ in how effectively they guess speciﬁc
passwords. MinGuess, shown in Figure 5, represents an
idealized guessing approach in which a password is con-
sidered guessed as soon as it is guessed by any of our
184  25th USENIX Security Symposium 
USENIX Association
10
100%
d
e
s
s
e
u
g
t
n
e
c
r
e
P
75%
50%
25%
0%
Backwards
Hybrid
Forwards
d
e
s
s
e
u
g
t
n
e
c
r
e
P
10%
8%
6%
4%
2%
0%
101 103 105 107 109 10111013101510171019102110231025
Guesses
(a) Training direction.
100%
d
e
s
s
e
u
g
t
n
e
c
r
e
P
75%
50%
25%
0%
NoTokenization
Tokenization