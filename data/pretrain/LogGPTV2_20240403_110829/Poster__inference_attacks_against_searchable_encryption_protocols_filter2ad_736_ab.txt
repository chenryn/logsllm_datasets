### Problem with Constraints

In this scenario, Mallory intercepts a set of \( l \) queries \( Q = \{Q_1, \ldots, Q_l\} \) and has access to the keyword-query matrix \( K_Q \) and the background knowledge matrix \( M \) as defined earlier. We assume that Mallory already knows the corresponding keywords for a subset of the queries \( S \subset Q \). Specifically, \( S = \{y \mid \exists x \text{ such that } (x, y) \in K_Q\} \).

Mallory's goal is to find a sequence of \( l \) indices \( (a_1, \ldots, a_l) \) such that she believes each query \( Q_j \) corresponds to the trapdoor generated from the keyword \( K_{a_j} \), given the background knowledge matrix \( M \).

The optimization problem can be formulated as follows:

\[
\arg\min_{(a_1, \ldots, a_l)} \sum_{Q_i, Q_j \in Q} \left( R_{Q_i} \cdot R_{Q_j} - K_{a_i} \cdot M \cdot K_{a_j}^T \right)^2
\]

**Constraints:**
1. For all \( j \) such that \( Q_j \in S \), \( a_j = x_j \) where \( (K_{x_j}, Q_j) \in K_Q \).
2. For all \( j \), \( \|Q_j\| = 1 \).

### Accuracy for Different Parameters

#### Figure 1: Accuracy of the Proposed Model

- **Accuracy for different Keyword Set Size (m):**
  - The model achieves over 80% accuracy for the default settings.
  - Accuracy approaches 100% with 250 queries, but remains effective even with smaller query sets of 50.

- **Accuracy for different Query Set Size:**
  - The model maintains high accuracy across various query set sizes.

- **Accuracy for low known query size:**
  - The model performs well even when no queries are known a priori.

- **Accuracy for different values of Scaling Factor (C):**
  - The model works fairly well for different noise scaling factors.

### Explanation of the Model

The first constraint in Equation (1) ensures that the known queries are assigned to their correct known keywords. The second constraint ensures that all queries in the set have an assignment of a valid keyword format. The result of this constrained optimization problem is an assignment of keywords to the queries that minimizes the distance from the background knowledge matrix \( M \).

To illustrate, consider two encrypted queries \( Q_s \) and \( Q_t \). Mallory can calculate the probability of both underlying keywords appearing in a given document by \( \beta = R_{Q_s} \cdot R_{Q_t} \), where \( \cdot \) denotes the dot product. For any two keywords \( K_f \) and \( K_g \), Mallory can calculate the probability of these keywords appearing together by \( \gamma = M_{f,g} \). Ideally, Mallory will assign \( K_f \) and \( K_g \) to the queries \( Q_s \) and \( Q_t \) if the observed probability \( \beta \) is close to the known probability \( \gamma \). This closeness is measured by the arithmetic distance function \( |\beta - \gamma| \), where a lower value is preferred.

Thus, Mallory's objective is to assign keywords to queries such that this distance function is minimized. Equation (1) formalizes this objective.

### NP-Completeness and Approximation

Unfortunately, the optimization problem in Equation (1) is NP-Complete. Therefore, we propose an efficient approximation using Simulated Annealing.

### Experimental Results

We implemented our proposed model using simulated annealing on the Enron dataset. Our document corpus consists of 30,109 documents from the sent mail folder of the Enron dataset. We used the Porter Stemming Algorithm to determine the root of each word, discarded common words, and selected the most frequent \( m \) words as keywords. The default settings for the experiments are:
- Keyword Set Size: 1500
- Query Set Size: 150
- Known Query Set Size: 15% of the Query Set Size

Figures 1(a)-(c) show the accuracy of our model for various parameter values. The model successfully identifies more than 80% of the queries for the default settings. The accuracy increases to nearly 100% with 250 queries and remains effective for smaller query sets of 50.

Figure 1(d) shows that the model works well even under noisy conditions. We added noise to the elements of the matrix \( M \) according to the distribution \( N(0, C \cdot \sigma^2) \), where \( \sigma^2 = \text{Var}\{M_{i,j}\} \) and \( C \) is the noise scaling factor. The model performs well for different values of \( C \).

### Conclusions

In this abstract, we presented a query identity inference model that exploits data access pattern leakage in efficient searchable encryption techniques. We demonstrated the effectiveness of our model on a real-world dataset. Future work will focus on mitigating such access pattern disclosure attacks on searchable encryption schemes.

### References

[1] D. Boneh, E. Kushilevitz, and R. Ostrovsky. Public key encryption that allows PIR queries. In proc. of CRYPTO, 2007.

[2] Y. Chang and M. Mitzenmacher. Privacy preserving keyword searches on remote encrypted data. In International Conference on Applied Cryptography and Network Security (ACNS), LNCS, volume 3, 2005.

[3] R. Curtmola, J. Garay, S. Kamara, and R. Ostrovsky. Searchable symmetric encryption: improved definitions and efficient constructions. In proc. of the 13th ACM Conference on Computer and Communications Security, CCS 2006, pages 79–88. ACM, 2006.

[4] E. Goh. Secure indexes. Cryptology ePrint Archive, (Report 2003/216), 2003.

[5] O. Goldreich and R. Ostrovsky. Software protection and simulation on oblivious RAMs. JACM: Journal of the ACM, 43, 1996.

[6] S. Kamara and K. Lauter. Cryptographic cloud storage. In Financial Cryptography Workshops, volume 6054, pages 136–149. Springer, 2010.

[7] B. Klimt and Y. Yang. Introducing the enron corpus. In CEAS, 2004.

[8] M. Porter. An algorithm for suffix stripping. Program, 14(3):130–137, 1980.

[9] D. Song, D. Wagner, and A. Perrig. Practical techniques for searches on encrypted data. In IEEE Symposium on Security and Privacy, 2000.