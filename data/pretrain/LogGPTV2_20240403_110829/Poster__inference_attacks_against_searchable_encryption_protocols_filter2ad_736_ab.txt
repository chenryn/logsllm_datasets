lem with constraints. Here, Mallory intercepts a set of l
queries Q = (cid:5)Q1,··· ,Ql(cid:6) and has access to KQ and M de-
ﬁned earlier. Let us assume that Mallory already knows the
corresponding keywords for each of the queries in the set
S ⊂ Q. That is, S = {y|∃x(cid:5)x, y(cid:6) ∈ KQ}.
Hence, Mallory ﬁnds a sequence of l indices (cid:5)a1,··· , al(cid:6)
s.t. Mallory believes that ∀j : Qj = T rapdoorKaj , given the
background knowledge matrix M .
RQi · RTQj
”!2
X
“
−
Kai · M · KT
aj
(1)
argmin
(cid:2)a1,··· ,al(cid:3)
Qi,Qj∈Q
n
Constraints : ∀j s.t. Qj ∈ S, aj = xj s.t. (cid:5)Kxj ,Qj(cid:6) ∈K Q
∀j,(cid:10) Qj (cid:10)= 1
846Accuracy for different Keyword Set Size
Accuracy for different Query Set Size
Accuracy for low known query size
Accuracy for different values of Scaling Factor (C)
 100
 80
 60
 40
 20
)
%
(
y
c
a
r
u
c
c
A
 100
 80
 60
 40
 20
)
%
(
y
c
a
r
u
c
c
A
 100
 80
 60
 40
 20
)
%
(
y
c
a
r
u
c
c
A
 0
 500
 1000
 1500
 2000
 2500
Keyword Set Size (m)
(a)
 0
 0
 50
 100
 150
 200
 250
 300
 0
 0
Query Set Size
(b)
 1
 2
 3
 4
 5
 6
Known Query Size (%)
(c)
 100
 80
 60
 40
 20
)
%
(
y
c
a
r
u
c
c
A
 0
 0
 0.2
 0.4
 0.6
 0.8
 1
Noise Scaling Factor (C)
(d)
Figure 1: Accuracy of our proposed model for various parameters.
RQs·RQt
The ﬁrst constraint in Eq. (1) guarantees that the known
queries will be assigned to their correct known keywords.
While the second one makes sure that all the queries in the
set has an assignment of a valid keyword format. The result
of this constraint satisfying optimization problem is an as-
signment of keywords to the queries that achieves minimum
distance from our background knowledge M .
To explain the model described in Eq. (1), let us consider
an example. Suppose, Qs and Qt are two encrypted queries.
Therefore, Mallory can calculate the probability of both of
the underlying keywords appearing in a given document by
, here · being the “dot” product. Now, for
β =
any two given keywords Kf and Kg, Mallory can calculate
the probability of these two keyword appearing together by
γ = Mf,g. Ideally, Mallory will assign Kf , Kg to the queries
Qs, Qt if the observed probability β is close to the known
probability γ. This closeness can be measured by a simple
arithmetic distance function (β − γ)
, where a lower value
of this function is preferred over a higher value. So, the aim
of Mallory will be to assign keywords to queries such that
this distance function is minimized. Our model given by Eq.
(1) is just a formalization of this objective function.
Unfortunately, the optimization problem given by Eq. (1)
is a NP − Complete problem2. Therefore, we propose an
eﬃcient approximation of this problem using Simulated An-
nealing instead.
n
2
6. EXPERIMENT RESULTS
We have implemented an eﬃcient approximation of our
proposed model using simulated annealing on a real world
dataset, namely the Enron dataset [7]. For all the experi-
ments, our document corpus consists of all the 30109 doc-
uments taken from the sent mail folder of Enron dataset.
We use Porter Stemming Algorithm [8] to determine the
root of each of the words in the documents, discard the
most common words (e.g., ‘the’) and take the most frequent
m words as our keywords. Our default settings for the ex-
periments are: Keyword Set Size = 1500, Query Set Size =
150, Known Query Set Size: 15% of the Query Set Size. We
use this parameter values unless otherwise mentioned in the
ﬁgures.
Fig. 1(a)−(c) presents the accuracy of our proposed model
for various Keyword Set Size, Query Set Size and Known
Query Set Size. It can be seen, our model can successfully
identify more than 80% of the queries for the default set-
tings. The accuracy soars to near 100% when we have 250
queries, but still works well for smaller query set size of 50.
2We like to include the proof in the full version of this work.
Fig. 1(c), on the other hand shows that our model works
well even when no queries are known apriori.
Getting a perfect matrix M may proved to be diﬃcult
under some scenario, even impossible under others. That’s
why, we have investigated the accuracy of our model under
a noisy matrix M in Fig. 1(d). Let, σ2 = V ar{Mi,j}. We
added noise to the element of the matrix M according to the
distribution N (0, C · σ2). Here, the constant C is referred to
as ‘noise scaling factor’. It can be deduced from Fig. 1(d)
that our model works fairly well for diﬀerent values of C.
7. CONCLUSIONS
In this abstract, we outlined a query identity inference
model that exploits data access pattern leakage exhibited by
eﬃcient searchable encryption techniques. We also present
our empirical results on a real world dataset to show the
eﬀectiveness of our proposed model. Furthermore, We leave
the mitigation of such an access pattern disclosure attack on
searchable encryption schemes as a future work.
8. REFERENCES
[1] D. Boneh, E. Kushilevitz, and R. Ostrovsky. Public key
encryption that allows PIR queries. In proc. of
CRYPTO, 2007.
[2] Y. Chang and M. Mitzenmacher. Privacy preserving
keyword searches on remote encrypted data. In
International Conference on Applied Cryptography and
Network Security (ACNS), LNCS, volume 3, 2005.
[3] R. Curtmola, J. Garay, S. Kamara, and R. Ostrovsky.
Searchable symmetric encryption: improved deﬁnitions
and eﬃcient constructions. In proc. of the 13th ACM
Conference on Computer and Communications
Security, CCS 2006, pages 79–88. ACM, 2006.
[4] E. Goh. Secure indexes. Cryptology ePrint Archive,
(Report 2003/216), 2003.
[5] O. Goldreich and R. Ostrovsky. Software protection
and simulation on oblivious RAMs. JACM: Journal of
the ACM, 43, 1996.
[6] S. Kamara and K. Lauter. Cryptographic cloud storage.
In Financial Cryptography Workshops, volume 6054,
pages 136–149. Springer, 2010.
[7] B. Klimt and Y. Yang. Introducing the enron corpus. In
CEAS, 2004.
[8] M. Porter. An algorithm for suﬃx striping. Program,
14(3):130–137, 1980.
[9] D. Song, D. Wagner, and A. Perrig. Practical
techniques for searches on encrypted data. In IEEE
Symposium on Security and Privacy, 2000.
847