### Leakage Increase and Core Degradation

- **Half the nominal ROB entries (50)**
- **Half the nominal size store queue (8)**

**Leakage Increase:**
- **2x in the L1 caches**
- **2x for the whole core**
- **2x in the store and load queues**
- **2x in the integer cluster**
- **2x in the FP cluster**
- **2x in the front-end**

In a Chip Multi-Processor (CMP) where cores can be affected in various ways, numerous heterogeneous core configurations may arise. In this study, we assume the degraded CMP configuration shown in Table 3. We assumed each core experienced some form of faults or variations, but each processor was affected by at most a few problems.

### Scheduling Algorithms Evaluation

To evaluate the effectiveness of our scheduling algorithms, we created four eight-threaded workloads of SPEC CPU2000 applications, as shown in Table 4. Each benchmark was used evenly among the four workloads. For each simulation, we fast-forwarded every benchmark five billion instructions, then executed one billion cycles in SESC, or 0.25 seconds at a nominal frequency of 4 GHz. Cores running at lower frequencies execute proportionally fewer cycles.

**Table 4: Workloads**
- **Workload 1**: applu, bzip2, equake, gcc, mcf, mesa, parser, swim
- **Workload 2**: ammp, apsi, art, crafty, twolf, vortex, vpr, wupwise
- **Workload 3**: mesa, ammp, applu, crafty, vortex, gcc, wupwise, mcf
- **Workload 4**: swim, parser, vpr, bzip2, art, apsi, twolf, equake

The OS scheduler periodically switches between the exploration and steady-state phases of the algorithm. During the exploration phase, which constitutes 10% of the total execution time, the algorithm adapts to workload changes to find the best assignment of threads to cores. During the longer steady-state phase, the CMP runs with this best configuration. The performance of the algorithm is based on both the exploration and steady-state phases. The length and number of intervals are algorithm-dependent parameters and are chosen to the best advantage of each technique. For each workload, we performed five different runs with different application-to-core starting assignments, and report the average, best, and worst results.

For the simpler randomized and round-robin algorithms, we modeled 10 million cycle operating system time slices, equivalent to 2.5 milliseconds. These algorithms do not require an exploration phase and instead use each time slice interval to perform their reassignments.

### Results and Discussion

#### 5.1 Simple Scheduling Algorithms

We first evaluate the effectiveness of two simple scheduling algorithms—round-robin and randomized—on the degraded eight-core CMP from Table 3. The round-robin scheduler rotates the threads on the cores at the beginning of each OS interval. This approach avoids a worst-case assignment by limiting how long an application runs on any given core. The even assignment of applications to processors also avoids high power density scenarios and uneven wear-out of a core through over-activity or high temperature.

The randomized scheduler randomly assigns threads to cores every operating system interval. This approach avoids degenerate behavior that might occur with round-robin, such as destructive interference with program phases.

**Figure 6: Comparison of Simple Schedulers**

| Benchmark Group | Round Robin | Randomized | Worst Case |
|------------------|--------------|-------------|-------------|
| Workload 1       |              |             |             |
| Workload 2       |              |             |             |
| Workload 3       |              |             |             |
| Workload 4       |              |             |             |
| Average          | 22% increase | 22% increase | 45% increase |

Both approaches degrade ED2 by over 22% on average. The final bar on the graph, the worst-case schedule, shows that an arbitrary assignment of threads to cores can degrade ED2 by almost 45% compared to the baseline. Clearly, naïve policies can result in an unacceptable loss in power/performance, potentially rendering the degraded microprocessor unusable.

#### 5.2 Hungarian Policy and Search Algorithms

The Hungarian scheduling policy samples each benchmark on each core during the exploration phase and then computes the best assignment among all permutations (assuming no interactions or phase behavior). For the Hungarian policy, the exploration phase is divided into eight intervals, each 12.5 million cycles long, during which the eight applications are executed once on each core, starting with an initial assignment and then rotating the threads in a round-robin fashion seven times. This allows the scheduler to generate the 8×8 cost matrix of ED2 values to use as input to the algorithm.

**Figure 7: Comparison of Advanced Schedulers**

| Benchmark Group | Hungarian Policy | Local Search 1 | Local Search 2 | Global Search |
|------------------|-------------------|-----------------|-----------------|---------------|
| Workload 1       |                   |                 |                 |               |
| Workload 2       |                   |                 |                 |               |
| Workload 3       |                   |                 |                 |               |
| Workload 4       |                   |                 |                 |               |
| Average          | 7.3% increase     |                 |                 | 19.5% increase |

The Hungarian algorithm performs well, suffering only a 7.3% increase in ED2 relative to the oracle. The performance and power characteristics of the benchmarks during the initial 100 million cycle exploration phase are quite reflective of the overall traits of the benchmarks. Thus, using the Hungarian Algorithm to calculate the best solution among all possible scheduling permutations based on this sampling information yields a good assignment over the whole run, regardless of the starting assignment.

While effective, the Hungarian scheduling algorithm has O(N^3) complexity, while other algorithms are O(N). Simulating the Hungarian Algorithm on our baseline core configuration takes approximately 200K cycles to solve a cost matrix with eight cores, a non-trivial cost that may not scale well to larger-scale CMPs. Since the number of sampling intervals scales linearly with the number of cores, a large amount of online profiling will also be required for chips with tens or hundreds of cores. Moreover, the algorithm may not work well when there are significant interactions among applications or rapid phase changes.

The global and local search algorithms divide the exploration phase into 25 intervals of four million cycles. Both start with the initial configuration and try other configurations, greedily pursuing paths that improve on the best schedule to date. Global search simply tries the initial configuration and 24 other randomly chosen ones and then selects the best among them for the steady-state phase. This strategy sometimes works quite well but can perform poorly depending on the 25 configurations pursued. Overall, global search degrades ED2 by 19.5% over the oracle scheduler.

Three versions of the local search method were also evaluated.