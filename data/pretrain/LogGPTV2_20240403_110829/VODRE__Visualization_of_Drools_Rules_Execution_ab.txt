Fig. 6. 	Elements of workflow diagramF. Experimental Evaluation Fig. 7. Experimental evaluation. Test initial data set.
For the sake of reliability and robustness of visualisation component manual analyses of schemas generated and rules triggered was conducted. Manual review showed that the rule implementation order displayed on visualisation chart corre-sponds the order of rules to build schemas, and the schemas, in turn, appear to be valid for the technical requirements and data provided. But after a thorough testing of visualisation mechanism charts are to replace manual inspection of schemas with a simple check of rule implementation order based on the visualisation.During experimental evaluation process the system was provided with sets of input optical parameters, for which pos-sible synthesis results were already known, to make sure that expected schemas are generated and visualisation component displays charts with correct rule invocation order to get these structural schemas. The complete process of evaluation for one of the input sets is described next.One of the synthesis processes is presented here as an example. An input form of photographic lens subsystem (Fig. 7) was filled with an initial set of valid test data (Table I) and a request to server was sent by ”Synthesis” button. Based on facts inserted, a total of 21 rules were triggered (12 from generation package, 6 from corrective package and 1 from basic, fast and wideangular packages). A set of 12 structural schemas was produced (Fig. 8).TABLE I.
Aperture speed: 
Angular field: 
Focal length: 
Image quality: 
Back focal distance: 
Entrance pupil position: Spectral range: INITIAL DATA SET FOR EVALUATION
1.8 
84◦
4.5mm 
Geometrically limited 
1mm 
Forward 
450..600nm
The schemas produced appear to be valid for the technical requirements provided and rule invocation order, seen in visu-alisation chart, in turn, is correct (Fig. 9). Other experimentsFig. 8. 	Produces schemas. Not all of 12 schemas are presented.
with structural schema synthesis and visualisation showed satisfactory results and feasibility of the component, as well.
Fig. 9. 	Experimental evaluation. Visualisation of synthesis based on real-world parameters (cut for size reasons)
G. Examples of VisualisationFor testing and clearness purposes a synthesis result for another sets of input values was visualised. The following example displays a synthesis process for fake random input not related with real-world application in order to simplify produced diagram so that the visualisation chart may be presented full-sized and uncut. An initial set of input data presented in Table II was chosen as it satisfies simplicity requirements. Based on test initial data a diagram was built (Fig. 10). As one may notice, first rules are displayed ordered by invocation time. Then, facts are displayed in ascending order by insertion time. Finally, a relationship between rules and facts is drawn (arcs with arrows).TABLE II. 	INITIAL DATA SET FOR SIMPLIFIED DIAGRAM EXAMPLE
Aperture speed: 	1 
Angular field: 	2◦
Focal length: 	1mm 
Image quality: 	Geometrically limited Back focal distance: 	1mm 
Entrance pupil position: 	Forward 
Spectral range: 	1..2nm
V. CONCLUSION & FUTURE WORKExpert systems are complex artifacts that are difficult to develop and test. This paper presented technical aspects of OSYST, an environment for automated structural synthesis of optical systems (photo-objectives), and the process of its validation and verification, and reasoning visualisation in par-ticular. The analysis of the system has shown a drawback of validation process, which was overcome by introducing a visu-alisation component for tracking working memory processes,Fig. 10. 	Example 1: visualisation of simplified synthesis based on not real-world data
mainly rule invocation and operations on facts.
In this study, we showed that a new visualisation com-ponent gives an expert another opportunity of inspection of knowledge engine workflow. The more ways to assess rule executions alpha-tester has, the more thorough and efficient validation process becomes. Concentrating on rule firing or-der rather than comparing synthesised schema with expected schema simplifies validation and verification in general.Despite the fact that the visualisation component designed is applicable for validation purposes, further revision is de-sirable. A scaling mechanism is needed to make it easier to review large diagrams, and more information on rules fired
would be an advantage, too. More research and development is necessary to expand functionality of the component to make it a powerful debugging tool.Future work should be focused on improvement of expert system, in general, and improvement and introduction of new components and tools, in particular.
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15] S.H. Liao, “Expert system methodologies and applicationsa decade review from 1995 to 2004.“, Expert systems with applications, vol.28(1), 2005, pp. 93-103M. Huettig, G. Busher, T. Menzel, W. Scheppach, F. Puppe, H.P. Buscher, “A Diagnostic Expert System for Structured Reports, Quality Assessment, and Training of Residents in Sonograph“, Medizinische Klinik, vol.99, 2004, pp. 117-122
T. Padma, P. Balasubramanie, “Knowledge based decision support sys-tem to assist work-related risk analysis in musculoskeletal disorder“, Knowledge-Based Systems, vol.22, 2009, pp. 72-78R.K. Lindsay, B.G. Buchanan, E.A. Feigenbaum, J. Lederbeg, “DEN-DRAL: a case study of the first expert system for scientific hypothesis formation“, Artificial Intelligence, vol.61, 1993, pp. 209-261
J. Baumeister, “Advanced Empirical Testing“, Knowledge-Based Systems, vol.24(1), 2011, pp. 83-94
D. Mouromtsev, I. Livshits, M. Kolchin, “Knowledge based engineering system for structural optical design“, Frontiers in Artificial Intelligence and Applications, vol.246, 2012, pp. 254-272R. Knauf, A. J. Gonzalez, K. P. Jantke, “Validating rule-based systems: a complete methodology“, in Proc. IEEE SMC’99 Conf., 1999, pp. 744-749
J. Vanthienen, C. Mues, A. Aerts, “An illustration of verificat on and validation in the modelling phase of KBS development“, Data and Knowledge Engineering, vol.27, 1998, pp. 337-352A. Preece, Evaluating Verification and Validation Methods in Knowl-edge Engineering, in R Roy (ed), Micro-Level Knowledge Management, Morgan-Kaufman, 2001, pp. 123-145
M. Tavana, “Knowledge-Based Expert System Development and Valida-tion with Petri Nets“, Journal of Information & Knowledge Management, vol.7(1), 2008, pp. 3746
J. Baumeister, J. Reutelshoefer, F. Puppe, “KnowWe: a Semantic Wiki for knowledge engineering“, Applied Intelligence, vol.35(3), 2011, pp.323-344
B. J. Wielinga, A. Th. Schreiber, J. A. Breuker, “KADS: a modelling approach to knowledge engineering“, Knowledge Acquisition - Special issue on the KADS approach to knowledge engineering, vol.4, 1992, pp.
5-53
N. Prat, J. Akoka, I. Comyn-Wattiau, “An MDA approach to knowl-edge engineering“, Expert Systems with Applications: An International Journal, vol.39(12), 2012, pp. 10420-10437O. Cair, S. Guardati, “The KAMET II methodology: Knowledge acqui-sition, knowledge modeling and knowledge generation“, Expert Systems with Applications: An International Journal, vol.39(9), 2012, pp. 8108-8114
M. Freiling, J. Alexander, S. Messick, S. Rehfuss, S. Shulman, “Start-ing a Knowledge Engineering Project: A Step-by-Step Approach“, AI Magazine, vol.6(3), 1985