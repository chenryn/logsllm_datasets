User: With the Python SDK, the job seems to hang forever (I have to kill it manually at some point) if I use the extra_package option to use a custom ParDo.
Here is a job id for example : 2016-12-22_09_26_08-4077318648651073003
No explicit logs or errors are thrown...
I noticed that It seems related to the extra_package option because if I use this option without actually triggering the ParDo (code commented), it doesn't work either.
The initial Bq query with a simple output schema and no transform steps works.
Did it happen to someone ?
P.S : I'm using the DataFlow 0.4.3 version. I tested inside a venv and it seems to work with a DirectPipelineRunner
Assistant: As determined by thylong and jkff:
The extra_package was binary-incompatible with Dataflow's packages.  The requirements.txt in the root directory and the one in the extra_package were different, causing the exec.go in DataFlow container failing again and again. To fix, we recreated the venv with the same frozen dependencies.