The next two features are computed by ﬁrst ﬁtting a 3-degree
polynomial to the accelerometer readings in each dimension. The
PIN 2087
Acceleromter Data (iFFT) (x-data)
5
10
15
20
25
30
35
3-d Polynomial Fit
5
10
15
20
25
30
35
1.0
0.5
0.0
0.5
1.0
1.5
2.0
0
1.0
0.5
0.0
0.5
1.0
1.5
0
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0
0.4
0.3
0.2
0.1
0.0
0.1
0.2
0.3
0.4
0
PIN 2358
Acceleromter Data (iFFT) (x-data)
5
10
15
20
25
30
35
3-d Polynomial Fit
5
10
15
20
25
30
35
Figure 5: An example of polynomial ﬁt features for PIN 2087
(left) and PIN 2358 (right). The top plot shows iFFT-ACC of the
accelerometer data (just acceleration in the x dimension), and the
bottom plot shows the 3-d polynomial ﬁt (iFFT-Poly).
parameters of the ﬁtted polynomial in each dimension are the next
features added (3D-Poly-Deg); that is, d3, d2, d1, d0 from f (t) =
d3t3 + d2t2 + d1t + d0 where t refers to the timestamp of the
readings. Following, we compute the curve values at each time
stamp in At and add the STATS of that curve as a set of features
(3D-Poly-STATS).
The next two features, iFFT-Poly and iFFT-Acc, are sample-
normalized forms of the polynomial curve and accelerometer stream.
The goal is to use the consistency in the shape of the curves of both
the polynomial ﬁt and the accelerometer readings as features, but
there is a large variance in the number of samples n across readings,
even when a user enters the same PIN or pattern multiple times. We
wish to instead use the curves as features in a sample-normalized
way such that regardless of n, we can represent the stream in m
values.
To solve this problem we use 1-dimensional Discrete Fourier
Transforms (DFTs) with a resolution of m = 35 samples. More
precisely, we compute
real(F−1
m (Fm(Ad))).
This computation ﬁrst encodes the signal using m complex fre-
quency basis functions, then reconstructs the original signal from
its compressed form. This preserves the general shape and values
of the curve, but it normalizes the time domain to m samples and
discards noisy high frequency components of the signal. We ex-
perimented with varied values of m and found that a small value
of m did not preserve enough information, while a large value of
m preserves too much variance because if m > n, the input is
zero padded. We found that m = 35 to be a good compromise
between these extremes, and it performed effectively for both PINs
and patterns.
To further demonstrate this technique, in Figure 5 we visualize
the iFFT-Acc and iFFT-Poly for accelerometer reading collected
while a user entered in two different PINs (note, this is accelerom-
eter readings in just the x dimension). Even though the same PIN
was entered by the same user on the same smartphone, n varied
between 59 and 112; however, you can see that regardless of the
variance in n, there is a shared shape to the curves. This is what we
wish to capture in our feature set.
In total, for each accelerometer reading, we use 774 features.
That is, for each dimension (x, y, and z) and for each normaliza-
tion, we extract 86 features, totaling 774 = 3 × 3 × 86. In exper-
iments, we found that all the features improve prediction results,
and that these features were effective for both PINs and patterns, as
well as single tap/touch and swipe/gesture events.
6.2 Machine Learning Classiﬁcation
Two classiﬁcation procedures are used in experimentation to
match the attack scenario described in Section 4. Recall that we
45
wish to model two scenarios: (1) The attacker has a large corpus of
labeled accelerometer data at his/her disposal and attempts to match
unknown input to some label in the corpus; and (2), the unknown
input is not in the corpus (or not well represented).
Logistic regression.
To model the ﬁrst scenario, where the at-
tacker is matching unknown input to labels in a corpus, we train a
multi-class logistic regression model on the feature vector labeled
with the PIN or password pattern (we use the LIBLINEAR imple-
mentation [12]). For each possible label, the logistic regression
ﬁnds a discriminating line in feature space to best separate exam-
ples of the label from examples of all other labels. Thus, the regres-
sion learns a weighted sum of the features described in Section 6.1
for each label.
Given accelerometer data from entering a PIN or pattern not used
in training, the resulting logistic regression model will output a pre-
dicted label (i.e., a PIN or pattern), or a set of labels ordered by the
likelihood of being the true label. If the label matches the input, we
consider this a successful prediction. We consider multiple guesses
from the model as the ranking of the output label that matches the
input label.
There are some limitations to this experiment because we only
learn models for the known PINs and patterns in the training set;
that is, the 50 pattern or 50 PINs used in the experiment as op-
posed to all 389,112 possible patterns and 10,000 possible PINs.
However, picking from random chance of the possible 50 patterns
would result in a 2% prediction accuracy. The model greatly ex-
ceeds random guessing by a factor of 20 or more for patterns and 9
or more for PINs.
Hidden Markov Models. To model the second scenario, where
the attacker’s corpus may not have sufﬁcient samples of the un-
known input, we build a classiﬁer that can predict previously un-
seen sequences of patterns and PINs. To achieve this, we obtain the
probability of each label from the output of the logistic regression
classiﬁers, and use these as observation probabilities in a Hidden
Markov Model (HMM). The HMM ﬁnds the most likely sequence
of input patterns or PINs (maximum a posteriori) by jointly con-
sidering the probabilities of individual swipe or digit entry classi-
ﬁcations along with the likely transitions between swipes or digit
entries.
For example, for a four-digit PIN, the HMM jointly infers the
most likely set of four digits given the individual beliefs in what
digit was pressed at what time, and what digits are likely to follow
other digits—certain combinations of digit transitions are impos-
sible, and others are more likely than others. The same inference
process can be used for patterns based on which swipes (connecting
two contact points) are likely to follow previous swipes.
Formally, let (cid:2)i be a possible label for position i in a sequence,
and oi its corresponding observed feature vector. Then, we obtain
p((cid:2)i|oi) from the logistic regression model for all (cid:2)i—the probabil-
ity that the label is (cid:2)i given the data oi. The transitions p((cid:2)i+1, (cid:2)i)
are estimated via maximum likelihood from our training data; sim-
ply empirical averages of each transition in the training data. For a
sequence of length k, the HMM determines the most probable joint
assignment
k(cid:2)
p((cid:2)i|oi)
k−1(cid:2)
((cid:2)(cid:2)
1, . . . , (cid:2)(cid:2)
k) = arg max
((cid:3)1,...,(cid:3)k)
i=1
i=1
p((cid:2)i, (cid:2)i+1).
Note that the joint space of possible labels ((cid:2)1, . . . , (cid:2)k) is combina-
torial (exponential in k). Fortunately, efﬁcient dynamic program-
ming techniques exist to solve this exactly in O(k2) time.
In our experiments, we explore label spaces of different granu-
larities. In an HMM over unigrams, each position in the sequence
corresponds to a single swipe or digit. In an HMM over bigrams
Figure 6: Prediction accuracy over multiple guesses for predicting
patterns (left) and PINs (right). The shaded trend lines are individ-
ual users.
.
labels consist of a pair of swipes or consecutive digits. We quickly
found that the unigrams performed poorly, and in the results below,
we only use bigram HMMs. This is a proof of concept, and a larger
model could incorporate even larger scope (larger grams), includ-
ing reﬁned transition matrices that account for human pattern/PIN
selection factors.
7. EVALUATION RESULTS
In this section, we present the results of our experiments for in-
ferring PINs and patterns using accelerometer reading. We begin
by modeling the ﬁrst attacker scenario, where the attacker has ac-
cess to a large corpus of labeled data. We additionally address
trends in expanding the corpus from 50 PINs/patterns, and how
such prediction models would fare. Next, we investigate a gen-
eral prediction model based on Hidden Markov Models which ad-
dresses the second attacker scenario. All the results presented in
this section, unless otherwise noted, are the average across ﬁve ran-
domized runs of a ﬁve-fold cross validation.
7.1 PIN/Pattern Inference
To begin, we are interested in how distinguishable PIN/pattern
inputs are based on accelerometer readings using the features de-
scribed in Section 6. The data used in this experiment consists of
the 50 PINs and 50 patterns collected from the 24 users while they
were sitting. The experiment proceeds by performing a ﬁve-fold
cross validation. Each of the ﬁve runs from a given user is ran-
domly divided into ﬁve folds, and a model is constructed from the
features extracted from four of the folds, and tested on the ﬁfth.
This process is repeated until all folds have been in the testing and
training positions.
The results from this experiment are presented in Figure 6. The
y-axis is prediction accuracy, and the x-axis is a of plot is the num-
ber of prediction or guesses attempted; that is, the logistic regres-
sion model output allows for a probabilistic ranking of the predicted
labels based on how likely it is the true label. For example, two
guesses refers to using the two top ranked predicted labels. If the
true label is one of those two labels, we consider it accurately pre-
dicted with two guesses. The dark trend line refers to the average
across all 12 users for PINs and 12 users for patterns. The error
bars on this curve mark the 1st and 3rd quartiles. The grayscale
lines are individual users, and the dotted line represents the pre-
diction probability for random guessing3. We use this style in all
graphs presented in this section unless otherwise noted.
Inspecting Figure 6, it is clear that accelerometer readings do
leak sufﬁcient information to differentiate between input of the
same type. In all cases, across all users, our model can infer the pre-
cise PIN or pattern when selecting from the set of 50 PIN/patterns
3Note that the trend line for random guessing with multiple at-
tempts is not linear because of conditional probabilities.
46
Figure 7: Trendline for how the number of examples affect predici-
ton for patterns (left) and PINs (right)). Note that we include an
additional three users who provided 12 examples, and the original
24 users only provided 5 examples of each PIN/pattern.
at a rate substantially higher than random guessing. Upon the ﬁrst
prediction, for patterns, the model on average predicts with 40% ac-
curacy, 20 times greater than random guessing of 2%; however, PIN
inference only averages 18% across all users, just 9 times greater
than random guessing. But, upon successive predictions, the mod-
els perform better: On the ﬁfth prediction, the model can predict
the pattern with 73% accuracy and PINs 43% of the time, a dif-
ference of ∼50% and ∼30% over random guessing, respectively.
Considering prediction accuracy rates after multiple guesses is im-
portant because an attacker would likely have multiple attempts at
guessing secure input, such as the 20 attempts provided by Android
for unlocking the phone and the 10 attempts provided by iOS.
Example Trends.
In the experiment above, each cross-validation
uses just four examples for training while testing on the ﬁfth. An
interesting question is: How would these models perform if more
examples were available? That is, we are interested in the example
learning curve. To investigate this we recruited three additional
users to enter in the same set of 50 patterns and 50 PINs a total of
12 times, each in the controlled, sitting endowment. while sitting
using the same instructions as before. We then include those results
with the original 24 users to see if there should be an increase in
prediction accuracy with more training data.
To measure the effect of additional examples, we incrementally
increase the number of examples (and folds) performed. Beginning
with two examples for each PIN/pattern, we perform a two-fold
cross validation. Following, we use three examples and perform a
three-fold cross validation, and so on, until there are no more ex-
amples to include. The results of this experiment are presented in
Figure 7: The x-axis is the number of examples used, and the y-
axis is the prediction accuracy. For both patterns and PINs, there
is a clear increase in inference accuracy as the number of exam-
ples increase. At the extreme, with 12 examples, patterns are in-
ferred with an accuracy near 60% on the ﬁrst prediction, and PINs
are near 40%. Both PINs and patterns see diminishing returns on
accuracy after 8-10 examples; the logarithmic growth of the learn-
ing curves is consistent with computational learning theory [16].
Overall, patterns, again, are more easily predicted via accelerome-
ter data given the features we developed, plateauing at a prediction
rate 50% greater than that of PINs.
Label Trends. Another important question is: How would these
models perform as the number of available labels increases? That
is, we are interested in the performance of a similar model that
must predict from a set of 10,000 labels, rather than just 50, as
would be the case if an attacker were targeting users generally. This
scenario can be estimated by performing a sequence of ﬁve-fold
cross validations, where in each step an additional label is included
in training and testing. For example, in the ﬁrst step, the model
must select between two labels, and in the last step, it must select
from 50, as before.
Figure 8: Trendline for the number of samples being selected
from: patterns (left) and PINs (right)). Note that the accuracy rates
closely match an inverse exponential.
Figure 9: Prediction accuracy over multiple guesses for predicting
patterns (left) and PINs (right) for different devices.
Figure 10: Prediction accuracy over multiple guesses for predict-
ing patterns (left) and PINs (right) when training and testing on
different devices.
The results of this experiment are presented in Figure 8: The x-
axis is the number of included labels, the y-axis is the prediction
accuracy, and the dotted line is the probability of random guess-
ing. As the number of labels in the model increases, the average
trend matches very closely (R2 > .99) to an inverse exponen-
tial (in dashed-red), and using this trend line, we can extrapolate
the performance of such a model (with 5 examples per label) pre-
dicting across any number of labels. For example, selecting from
10,000 PINs, we should expect an inference accuracy of about 2%
on the ﬁrst prediction, which is 277x greater or 8 orders of magni-
tude greater than random guessing. For patterns, if the model is se-
lecting from 10,000 patterns, it should predict with an accuracy of
13% on the ﬁrst prediction, and, if it was selected from all 389,112
possible patterns, it should predict with an accuracy of 6% on the
ﬁrst prediction, 23,567x greater or 14.5 orders of magnitude greater
than random guessing. These are likely optimistic projections for
our feature set, but these results do suggest that predicting input
from a large label space using accelerometer readings is tractable,
if an attacker were able to collect sufﬁcient examples.
User and Device Effects. As noted in Table 1 and in Section 5, the
data set contains rather large variance across devices and users. An
important question is: How does training on accelerometer read-
ings from one device or user and testing on another device or user
affect an attacker’s inference capabilities? Such results speak to
the attacker’s ability to construct a large and diverse corpus to use
47
Figure 11: Prediction accuracy over multiple guesses for predict-
ing patterns (left) and PINs (right) for training on 11 users and test-
ing on one.
Figure 12: Prediction accuracy over multiple guesses for predict-
ing patterns (left) and PINs (right) while the user is walking. The
shaded trend lines are individual users.
.
in training on users/devices previously unseen.
Figure 13: Prediction accuracy for uni- and bigrams for patterns
(left) and PINs (right) with 5 guesses. Note that there are 9 and
10 possible unigrams and 72 and 100 possible bigrams for patterns
and PINs, respectively.
vestigates the affect of movement noise. First, we built a model
using the data for a single user while they were sitting, and then
we tested that model on data collected while the user was walking.