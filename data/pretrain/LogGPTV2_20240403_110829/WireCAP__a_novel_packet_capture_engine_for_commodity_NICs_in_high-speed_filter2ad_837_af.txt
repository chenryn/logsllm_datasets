c. Delay and accurate timestamping 
  WireCAP uses batch processing to reduce packet capture costs. 
Applying  this  type  of  technique  may  entail  side  effects,  such  as 
latency increases and inaccurate time-stamping [28]. 
  On Windows and Linux, timestamp can be obtained from two 
sources,  OS  jiffy  and  CPU  time  stamp  counter  (TSC).  The  OS 
jiffy  resolution  is  on  the  order  of  milliseconds,  which  cannot 
provide accurate timestamp support in high-speed networks. CPU 
time stamp counter (TSC) can provide finer resolution. However, 
the overheads will be too high if TSC is accessed on a per-packet 
basis,  especially  in  high-speed  networks.  Therefore,  almost  all 
software-based  packet  capture  engines  suffer  the  timestamp 
accuracy  problem  and  the  uniqueness  of  timestamp  problem  if 
NIC  does  not  provide  high-resolution  timestamp  support  in 
hardware. 
d. How to address load imbalance in a multicore system? 
  Because networks are getting faster (e.g., 10/40 GE) and CPU 
cores are not [29], it is increasingly difficult for a single core to 
keep up with the high-speed link rates. Multicore and multi-queue 
NIC technologies have been introduced to allow host systems to 
keep  up  with  high-speed  networks.  However,  existing  per-flow 
based  traffic-steering  mechanisms  (e.g.,  RSS)  can  easily  lead  to 
traffic 
load 
fluctuating  significantly,  the  need  to  preserve  application  logic, 
and  the  necessity  of  maintaining  core  affinity  in  network 
processing, we believe:  
imbalance  across  cores.  Because  of  network 
• 
• 
It  is  difficult  to  apply  a  pure  NIC-based  solution  to 
address load imbalance in a multicore system;  
Traffic offloading in an application-aware manner is an 
indispensible 
for  packet  capturing  and 
processing in a multicore system; 
function 
•  Accordingly, an application has to be able to deal with 
flows being distributed over multiple threads. 
e.  WireCAP  can  support  different  packet  capturing  and 
processing paradigms 
Ideally,  WireCAP  is  designed  to  support  the  packet  capturing 
and  processing  paradigm  as  shown  in  Figure  1.  Within  a  multi-
queue NIC, packets are distributed across receive queues using a 
hardware-based  traffic-steering  mechanism.  And  each  receive 
queue is handled by a thread (or process) of a packet processing 
application. Because this paradigm uses NIC hardware, instead of 
CPU,  to  classify  and  steer  packets  across  cores,  it  helps  to  save 
CPU  for  packet  capturing  and  processing.  On  the  other  hand, 
modern NICs are becoming more powerful, and typically feature 
advanced traffic filtering, classification, and steering mechanisms. 
We  believe  this  packet  capturing  and  processing  paradigm  is  a 
promising approach.  
  However,  WireCAP  is  flexible  and  robust  enough  to  support 
other types of packet capturing and processing paradigms: 
•  Multiple  threads  (or  processes)  of  a  packet-processing 
application  can  access  a  single  NIC  receive  queue,  through 
the  queue’s  corresponding  work-queue  pair  in  user  space. 
Certainly, 
incurs  extra  synchronization 
overheads across these threads. 
this  approach 
•  Upon  WireCAP  work-queue  pairs,  a  packet-processing 
application  can  implement  its  own  traffic  steering  and 
classification  mechanisms  to  create  packet  queues  at  the 
application  level,  in  the  cases  of  the  NIC  hardware-based 
traffic classification and steering mechanism cannot meet the 
application  requirements;  or  there  are  not  enough  physical 
queues in the NIC.  
In these paradigms, a simple approach is to copy captured packets 
from  WireCAP  into  the  application’s  own  set  of  buffers.  This 
approach  simplifies  WireCAP’s  recycle  operations  while  the 
benefit  of  zero-copy  delivery  will  not  be  available.  However, 
WireCAP  still  provides  lossless  packet  capture  and  delivery 
services. 
  WireCAP  can  be  configured  to  switch  between  supporting 
different packet capturing and processing paradigms. 
6 Related work 
  Receive-side  scaling  and  Flow  Director  [11,  22,  30]  are 
advanced NIC technologies that enable the efficient distribution of 
packets across receive queues in a multi-queue NIC. RSS uses a 
hash  function  in  the  NIC.  The  NIC  computes  a  hash  value  for 
each  incoming  packet.  Based  on  hash  values,  the  NIC  assigns 
packets  of  the  same  flow  to  a  single  queue.  Flow  Director 
maintains a flow table in the NIC to assign packets across queues. 
Each flow has an entry in the table. The entry tracks which queue 
a  flow  should  be  assigned  to.  The  flow  table  is  established  and 
updated by traffic in both the forward and reverse directions. Flow 
Director  is  typically  not  used  in  a  packet  capture  environment 
because the traffic is unidirectional. 
  The  protocol  stack  of  a  general  purpose  OS  can  provide 
standard  packet  capture  services  through  raw  sockets  (e.g., 
PF_PACKET). However, research [9] show that the performance 
is inadequate for packet capture in high-speed networks. 
  Several  packet  I/O  engines  have  been  proposed  to  boost  the 
performance  of  commodity  NICs  in  packet  capture,  such  as 
NETMAP [15], DNA [16], PF_RING [14], and the PacketShader 
I/O engine (PSIOE) [23], These packet capture engines essentially 
bypass 
improved 
performance. Table 2 compares WireCAP and the existing packet-
capture engines. 
the  standard  protocol  stack  and  achieve 
PacketShader  I/O  engine  (PSIOE)  is  similar  to  PF_RING, 
except  that  PSIOE  uses  a  user-space  thread,  instead  of  Linux 
NAPI  polling,  to  copy  packets  from  receive  ring  buffers  to  a 
consecutive user-level buffer (user buffer). For PacketShader, the 
copy  operation 
impact  on 
performance and does not consume additional memory bandwidth 
because  the  user  buffer  likely  resides  in  CPU  cache  [23]. 
in  packet  capture  makes 
little 
However, this result does not have wide applicability. If a network 
application  that  uses  PSIOE  to  capture  packets  has  a  large 
working set, the user buffer is not likely to reside in CPU cache. 
Under such condition, copying will have a significant impact on 
performance. PSIOE was specifically designed and optimized for 
PacketShader.  It  provides  only  a  limited  buffering  capability  for 
the  incoming  packets.  PSIOE  is  not  suitable  for  a  heavy-load 
application. 
Table 2. WireCAP vs. existing packet-capture engines 
WireCAP 
DNA 
NETMAP 
PSIOE 
PF_RING 
Goal: avoiding packet drops. 
Deficiency: requiring additional resources. 
Goal: minimizing packet capture costs. 
Deficiency:  limited  buffering  capability,  no 
offloading mechanism. 
Goal: maximizing system throughput. 
Deficiency: 
buffering 
limited 
capability; 
copying in packet capture. 
Goal: minimizing packet capture costs. 
Deficiency: copying in packet capture; receive 
livelock problem;  
no offloading mechanism. 
At a high level, WireCAP provides a new packet I/O framework 
for commodity NICs in high-speed networks. Therefore, there are 
some similarities between WireCAP and Intel DPDK [31]. Both 
WireCAP  and  DPDK  can  provide  large  packet  buffer  pools  at 
each  receive  queue  to  accommodate  packet  bursts,  support 
dynamic packet buffer management capabilities, employ flexible 
zero-copying, and receive (or capture) packets from each receive 
queue  through  polling.  However,  WireCAP  and  DPDK  differ  in 
two  major  aspects.  First,  they  have  different  packet  buffer 
allocation approaches. WireCAP allocates packet buffer pools in 
kernel.  Each  packet  buffer  pool  is  mapped  into  an  application’s 
process space. By contrast, DPDK handles an NIC device in user 
space  through  UIO  [32].  It  allocates  packet  buffer  pools  in  user 
space. Second, DPDK does not provide an offloading mechanism 
as  WireCAP.  To  avoid  packet  drops,  a  DPDK-based  application 
must implement an offloading mechanism in the application layer 
to  handle  long-term  load  imbalance.  However,  implementing  an 
application-specific  offloading  mechanism 
is  complex  and 
involves several considerations, including: 
•  When  load  imbalance  occurs,  how  to  steer  packets  from 
overloaded  core(s)  to  idle  (less  busy)  core(s)?  Should 
packets  be  steered  on  a  per-packet  basis  or  in  burst? 
Should packets be steered in push or pull mode?  
•  How to synchronize the related threads? 
•  How to recycle used packet buffers? 
•  … 
These  issues  will  significantly  affect  the  overall  performance. 
Therefore, a DPDK-based application is complex and difficult to 
design. 
7 Conclusion & future work 
In this paper, we have described our architectural approach in 
developing a novel packet capture engine for commodity NICs in 
high-speed  networks.  Experiments  have  demonstrated 
that 
WireCAP achieves better packet capture performance than that of 
405
PF_RING. www.ntop.org/products/pf_ring/.  
[14] 
[15]  L. Rizzo, netmap: a novel framework for fast packet I/O. 
In USENIX ATC (2012). 
[16]  DNA. www.ntop.org/products/pf_ring/dna/. 
[17]  H. K. J. Chu, Zero-copy TCP in Solaris. In Proceedings of 
the 1996 annual conference on USENIX Annual Technical 
Conference (pp. 21-21). Usenix Association. 
P.  Druschel,  and  G.  Banga,  Lazy  receiver  processing 
(LRP):  A  network  subsystem  architecture  for  server 
systems. In OSDI (Vol. 96, pp. 261-275). 
[18] 
[19]  A.  Foong,  T.  Huff,  H.  Hum,  J.  Patwardhan,  and  G.  
Regnier,  TCP  performance  re-visited.  In Performance 
Analysis  of  Systems  and  Software,  2003.  ISPASS.  2003 
IEEE International Symposium on (pp. 70-79). IEEE. 
[20]  TCPDUMP 
and 
LIBPCAP 
public 
repository. 
http://www.tcpdump.org/. 
Datasheet.  
[23] 
[22] 
82599 
10GbE 
software 
Controller 
[21]  W. Wu, M. Crawford, and M. Bowden, The performance 
analysis of Linux networking–packet receiving, Computer 
Communications, 30(5), 1044-1057. 
Intel 
http://www.intel.com/content/www/us/en/ethernet-
controllers/82599-10-gbe-controller-datasheet.html. 
S. Han, K. Jang, K. Park, and S. Moon, PacketShader: a 
GPU-accelerated 
router. ACM  SIGCOMM 
Computer Communication Review, 41.4 (2011), 195-206. 
J.  Mogul,  and  K.  K.  Ramakrishnan,  Eliminating  receive 
kernel. In  ACM 
livelock 
Transactions on Computer Systems 15.3 (1997): 217-252. 
S.  McCanne  and  V.  Jacobson,  The  BSD  packet  filter:  A 
new  architecture  for  user-level  packet  capture.  In Winter 
USENIX Conference, 1993. 
J.  S.  White,  T.  Fitzsimmons,  and  J.  N.  Matthews, 
Quantitative analysis of intrusion detection systems: Snort 
and Suricata. In SPIE Defense, Security, and Sensing, pp. 
875704-875704. 
interrupt-driven 
[24] 
[25] 
[26] 
in 
an 
[27]  A. Basu, J. Gandhi, J. Chang, M. D. Hill, and M. M. Swift, 
(2013,  June).  Efficient  virtual  memory  for  big  memory 
servers. In ACM ISCA (2013): pp. 237-248. 
[28]  V. Moreno, P. Santiago del Rio, J. Ramos, J. Garnica, and 
J.  Garcia-Dorado,  Batch 
future:  Analyzing 
timestamp  accuracy  of  high-performance  packet  I/O 
engines,  IEEE  Communications  letters,  16(11):  1888-
1891, 2012. 
[29]  D.  Geer, 
Chip  makers 
to  multicore 
turn 
the 
to 
processors. Computer, 38(5), 11-13, 2005. 
[30]  W.  Wu,  P.  DeMar,  and  M.  Crawford,  Why  can  some 
advanced  Ethernet  NICs  cause  packet  reordering?. IEEE 
Communications Letters, 15(2): 253-255, 2011. 
Intel DPDK website. http://dpdk.org.  
[31] 
[32]  H. J. Koch, "Userspace I/O drivers in a realtime context", 
Available: 
[Online], 
https://www.osadl.org/fileadmin/dam/rtlws/12/Koch.pdf.
existing packet captures engines. Our design is unique in the sense 
that  we  seek  to  address  off-the-wire  packet  capture  concerns  in 
conjunction  with  packet  delivery  issues  to  the  application. 
WireCAP  potentially  paves  the  way  for  advancement  of  packet 
analysis tools to deal with emerging high-speed networks. 
  Although  our  current  work  has  been  with  10  GE  technology, 
our  objective  is  to  support  40  GE  and,  eventually,  100  GE 
technologies.  In  the  near  future,  we  will  apply  WireCAP  for  40 
GE networks. 
  Comparing WireCAP with DPDK (with offloading) will be our 
future  research  areas.  However,  a  fair  comparison  can  only  be 
achieved  when  DPDK  provides  its  own  version  of  offloading 
mechanism. 
8 References 
[1]  M.  Roesch,  Snort:  Lightweight  Intrusion  Detection  for 
Networks. In USENIX LISA (Nov. 1999), pp. 229-238. 
A.  Papadogiannakis,  M.  Polychronakis,  and  E.  P. 
Markatos,  Improving  the  accuracy  of  network  intrusion 
detection  systems  under  load  using  selective  packet 
discarding. In ACM ERUOSEC (2010), pp. 15-21. 
[2] 
[5] 
[4] 
[3]  M. Jamshed, J. Lee, S. Moon, I. Yun, D. Kim, S. Lee, Y. 
Yi, and K. Park, Kargus: a highly-scalable software-based 
intrusion detection system. In ACM CCS (Oct. 2012), pp. 
317-328. 
V.  Paxson,  Automated  packet  trace  analysis  of  TCP 
implementations. ACM SIGCOMM CCR 27, 4 (1997). 
S.  A.  Crosby  and  D.  S.  Wallach.  Denial  of  service  via 
algorithmic complexity attacks. In USENIX Security (Aug. 
2003), pp. 29-44.  
http://www.endace.com/endace-dag-high-speed-packet-
capture-cards.html. 
http://www.napatech.com/. 
F. Fusco, and L. Deri, High speed network traffic analysis 
with commodity multi-core systems. In ACM IMC (2010), 
pp. 218-224. 
L.  Braun,  A.  Didebulidze,  N.  Kammenhuber,  and  G. 
Carle, Comparing and improving current packet capturing 
solutions  based  on  commodity  hardware.  In ACM  IMC 
(2010), pp. 206-217. 
[7] 
[8] 
[6] 
[9] 
[10]  N. Bonelli, A. Di Pietro, S. Giordano, and G. Procissi, On 
capturing  with  multi–core 
multi–gigabit 
commodity hardware. In PAM (2012), pp. 64-73.  
packet 
[11]  W. Wu, P. DeMar, and M. Crawford, A transport-friendly 
NIC  for  multicore/multiprocessor  systems. Parallel  and 
Distributed  Systems,  IEEE  Transactions  on, 23(4),  607-
615. 
[12]  C. F. Dcumitrescu, Models for packet processing on multi-
systems.  
core 
http://www.intel.com/content/www/us/en/intelligent-
systems/intel-technology/ia-multicore-packet-processing-
paper.html.  
[13]  T.  Benson,  A.  Akella,  and  D.  A.  Maltz,  Network  traffic 
characteristics  of  data  centers  in  the  wild.  In  ACM  IMC 
(2010), pp. 267-280. 
406