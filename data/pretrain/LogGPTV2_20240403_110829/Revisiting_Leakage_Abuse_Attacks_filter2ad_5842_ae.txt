and W(1)
0
1
Correctness & efﬁciency. The binary search attack
will recover q0 as long as: (1) it appears log #W times in
q = (q1, . . . , qt); (2) it has a unique volume in the baseline
volumes; and (3) the user did not add any document of its own.
The attack needs to inject log #W ﬁles with a total volume of
Ω(#W). Our evaluation demonstrates that the size of the query
space was the main factor that determines the total injected
volume (i.e., γ (cid:39) #W/2u for all u ≤ log #W for different
subsets of the Enron dataset). The total injected volume was
around 8KBytes (see Appendix D).
V. EMPIRICAL EVALUATION
To evaluate the effectiveness of our attacks, we imple-
mented and evaluated them under different conditions. The
results are perhaps surprising and provide a new and more
nuanced perspective on the potential impact and limitations of
leakage abuse attacks.
Document collections. We build our document collec-
tions using the Enron Email dataset [63]. This dataset is com-
posed of 150 folders with a total of 520, 901 ﬁles. Each folder
corresponds to the email account of a single individual and
is itself composed of several folders including, for example,
inbox, sent, contacts, discussion threads, etc.
9
vi = bj + u · γ
for some j ∈ [m] and u ∈ [(cid:96)]. The adversary’s goal now is
to map qi to some keyword w ∈ W = (w1, . . . , w(cid:96)). It does
this as follows. It checks if there exists a u ∈ [(cid:96)] such that
vi − u· γ is equal to one of the baseline volumes (b1, . . . , bm).
If this is the case, then the adversary maps qi to keyword wu.
Note that there can be at most a single baseline volume that
satisﬁes this condition. To see why, suppose there exists two
baseline volumes β1 (cid:54)= β2 and two values z (cid:54)= z(cid:48) in [(cid:96)] such
that
vi − z · γ = β1
and
vi − z(cid:48) · γ = β2.
But this implies that γ · (z(cid:48) − z) = β1 − β2 which is a
contradiction since γ(cid:54) | bi − bj, for all i, j ∈ [m].
Correctness & efﬁciency. The decoding attack recovers
all queries in q as long as the user did not add any documents
of its own. As described, the attack recovers all queries in q by
injecting #W documents with a total volume of O(γ · #W2).
Note, however, that the attack can be made a lot more efﬁcient
if the adversary is only interested in recovering queries within
some target set T ⊂ W. In this case, in the baseline phase the
adversary still needs to gather the baseline volumes for queries
in W, but only needs to inject documents for keywords in T .
With this modiﬁcation, the attack recovers queries in q∩ T by
injecting T documents with a total volume of O(γ·#T 2). Our
evaluation demonstrates that the offset γ for different subsets
of the Enron dataset is between 4 and 16 KBytes depending
on the query selectivity (see Appendix D).
B. The Binary Search Attack
Overview. Contrary to the decoding attack where the
adversary recovers all queries, the binary search attack is a
targeted attack in which the adversary’s goal is to recover one
speciﬁc query. At a high-level, the attack ﬁrst observes user
queries and their associated volume. It then uses this informa-
tion to create a document that contains half the keyword space
and that has a carefully-chosen size. When this document is
injected, it modiﬁes the volume of half of the keywords (the
ones contained in the injected document) in a unique manner
that is detectable. The adversary then observes more queries
and uses the presence or absence of the unique volume to
infer which half of the keyword space the target query is in.
The attack then recurs on that half. The base case is a single-
element set which it outputs as the keyword. Due to space
constraints, the pseudo-code of this attack appears in the full
version of this work.
Details. The binary search attack works in three phases:
baseline, targeting and recovery. In the baseline phase, the
adversary waits until it has observed the total volumes for
all keywords in W. During the targeting phase the adversary
observes more client queries until it decides on a query q0, with
total volume v0, that it wishes to target. In the recovery phase,
the adversary observes an additional sequence of t > log #W
client queries q = (q1, . . . , qt) with volumes v = (v1, . . . , vt)
that it will use to recover its target q0. We now describe each
phase in more detail.
Starting from the entire Enron dataset, we generated different
subsets that capture different settings of interest:
• single user (SU):
is a document collection composed
of one individual’s email account. For this dataset, we
picked the arnold-j folder which is 11.6MBytes and is
composed of 4, 944 ﬁles. The total number of keywords is
40, 363. This collection models the traditional single user
ESA setting where a single client uses an ESA to encrypt
and privately access its own dataset.
• small multiple users (S-MU): is a document collection
composed of multiple email accounts. For this dataset,
we picked 5 folders with a total size equal to 26MBytes.
This document collection is composed of the follow-
ing email accounts: baughman-d, gay-r, heard-m,
hendrickson-s and linder-e. The dataset is com-
posed of 9, 416 ﬁles in total. The total number of key-
words is 77, 762. This collection models the multi-user
ESA setting in which there is one party (e.g., a company)
that uses an ESA to encrypt and store multiple users’
documents. Here, queries can be performed by a subset
of authorized users. Note that the users were arbitrarily
picked.
• medium multiple user (M-MU): is the same as above
except that we increase the number of folders to 10 with
a total size of 49MBytes. The data collection is composed
of the same email accounts as above plus: allen-p,
buy-r, forney-j, hyvl-d and keiser-k. The total
number of keywords is 115, 679. Similarly, the additional
folders were picked arbitrarily.
The purpose of the ﬁrst two collections is to see whether the
effectiveness of our attacks will vary as a function of different
data distributions. The third collection is used to understand if
increasing the size of the dataset impacts the effectiveness of
the attacks. We note that we performed evaluations which we
do not report here in order to avoid redundancy. In particular,
we evaluated the attacks on a larger document collection from
Enron of size 106MBytes and the results were similar to the
ones on M-UM. We also evaluated our attacks on a subset of
the TREC 2007 Public Corpus dataset [57], an email
dataset composed of 75, 419 emails (including spam), and the
results were similar.
Data indexing. We indexed each data collection using
Apache Lucene [1]. We removed 224 stop words listed in
the SnowBall list [62]. Furthermore, we used the Porter
Stemming implementation of Lucene so all words that share
the same stem are mapped to the same root.
Query frequency. We observed that previous works on
leakage abuse attacks [38], [13] evaluated the effectiveness of
their attacks on the most frequent keywords in the dataset.
While this assumption might hold in some settings, it is far
from clear if this a reasonable assumption in practice. In
fact, it might seem that users would more often search for
keywords that occur less frequently in their dataset rather than
for keywords that occur frequently. With this in mind, we
evaluated all of our attacks in three different settings:
• high selectivity: the queries are sampled from the set of
keywords with the highest selectivities (i.e., that appear
in the largest number of documents). We noticed that
keyword selectivities in Enron are power-law distributed
(see Appendix C) which implies that high-selectivity
keywords tend to have unique selectivities while low-
selectivity keywords tend to have less unique selectivities
(in our datasets it was none).
• low selectivity: the queries are sampled from the set of
keywords with the lowest selectivities. In our datasets, all
the low-selectivity keywords had selectivity 1.
• pseudo-low selectivity:
in this case we consider low-
selectivity keywords with a slightly higher selectivity than
above. In particular, we consider the case where the
selectivity ranges from 10 to 13. Note that these values
are only examples and that pseudo-low selectivity can be
deﬁned using other values.
Size and composition of the query space. We ﬁrst ﬁx the
size of the query space, Q, to be 500. We then study the impact
of increasing #Q. In particular, we increased #Q up to 5000
keywords.8 We also consider two ways of instantiating the
query space. The ﬁrst consists of populating Q with keywords
that only exist in the known-data collection (cid:101)D. This guarantees
that all q ∈ Q must exist in (cid:101)D. As a consequence, the attacker
document in (cid:101)D. The second approach consists of populating
would know that any client query will match at least one
Q from keywords that exist in the client’s collection D.
Experimental setting. As described above, there are
several variables that can impact
the effectiveness of our
attacks. In our evaluation, we considered many different com-
binations of these variables and organized them in three main
categories:
• (C1) single keyword queries: we consider the SU dataset
and ﬁx the size of the query space to be 500. We then
vary the query selectivity and query space composition;
refer to Figures 1a and 1b.
• (C2) size of the query space: this second category is the
same as the ﬁrst except that we increase the size of the
query space to be 5000; refer to Figures 1c and 1d.
• (C3) varying the datasets: the third category is similar to
the ﬁrst category except that we replace the SU dataset
with the S-MU and M-MU datasets; refer to Figures 1e,
1f, and Figures 1f.
All our attacks are evaluated against a query sequence q of
size t = 150. The queries are sampled uniformly at random
from the corresponding keyword space Q whose composition
varies depending on the chosen approach (see above). In all
our experiments, we start with the adversary knowing the entire
client collection and then gradually decrease this knowledge
until it knows only 5% of the documents (chosen uniformly at
random). For each attack, we report the recovery rate, i.e., the
number of queries recovered correctly over the total number
of queries. We run all experiments 5 times and report the
minimum, median and maximum of the recovery rate. All
attacks are implemented in Java and the experiments were run
on a MacBook 3.1 GHz Intel Core i7 with 16GBytes of RAM.
Baseline.
In all our experiments we have included
the count-only attack which is simply the Count attack [13]
without the co-occurence matrix. This depicts constructions
8The values we picked are similar to the ones used in existing works [38],
[13].
10
for which we have suppressed the co-occurence leakage, refer
to Appendix A.
Overview of results. We noticed that
the recovery
rate of our attacks is impacted the most by the selectivity
of the queries. In fact, it seems that evaluating known-data
attacks on high- vs. low-selectivity queries leads to completely
opposite conclusions.9 Moreover, we noticed that changing
the datasets or increasing the size of the query space led to
some ﬂuctuations but the overall trends remained the same.
This shows that our attacks work across different settings
and different dataset compositions. We found, however, that
if client queries are not in the adversary’s known dataset then
the recovery rate is always low. This simply follows from the
fact that there are several queries for which the adversary does
not hold any part of the response. Below, we provide more
detailed comments on the results of our evaluations:
• When the query space is composed of high-selectivity
keywords, both SubgraphID and SubgraphVL have a re-
covery rate of about 70% even with a known-data rate
of 5% (see Figure 1e). We believe that the low known-
data rate makes these attacks practical for high-selectivity
keywords. However, if the query space is composed of
low-selectivity keywords, then the recovery rate drops
signiﬁcantly. In fact, we found that SubgraphVL does not
work at all while SubgraphID has a recovery rate of about
20% even when δ = 1; that is, with full knowledge of
the client’s data. With a known-data rate of δ = 1/2,
SubgraphID only has 10% recovery rate. It tends to 0 for
known-data rates smaller than 10%.
• For VolAn and SelVolAn, our evaluation shows that both
attacks work only when: (1) the query space consists
of high-selectivity keywords; and (2) the adversary has
a high known-data rate, often at least .85. We refer the
reader to Figure 1a for an example. In the case where
the query space consists of low-selectivity keywords, the
recovery rate is very low: around 10% even when δ = 1/2.
It drops signiﬁcantly when δ gets smaller.
• When the query space consists of pseudo-low-selectivity
keywords (i.e., with selectivity between 10 and 13), both
the VolAn and SelVolAn attacks have high recovery rate
only when the known-data rate δ ≥ .8 (see Figure 2).
As δ decreases, the recovery rate stabilizes at around
18% and starts to decrease again to 0 when δ ≤ 0.15.
Note that this recovery rate is the highest among the
three selectivity classes for these two attacks. The re-
covery rates of both SubgraphID and SubgraphVL against
pseudo-low-selectivity queries are slightly better than the
recovery rates of SelVolAn and VolAn. SubgraphID and
SubgraphVL also do better on pseudo-low-selectivity key-
words than they do on low-selectivity keywords. However,
they do a lot worse than they do against high-selectivity
keywords.10
Impact of known-data rates. Our evaluation demon-
the known-data rate has a big impact on the
strates that
9We recall that the experiments in [13] were done exclusively on high-
selectivity keywords.
10We also conducted experiments in which the queries were sampled
uniformly at random from the entire keyword space. The results were similar
to the low-selectivity case (the SubgraphID recovery rate was slightly higher
though).
recovery rate. For all attacks, the larger δ is the higher the
recovery rate. This is natural since all the attacks exploit some
correlation between documents and keywords. As discussed
in Section I, choosing what constitutes a “safe” known-data
rate requires more cryptanalysis but what we can say is that a
known-data rate of δ < 0.05 could be considered safe against
the attacks in this work. If a dataset is composed of one million
documents, then even when δ = 0.05 the adversary would still
need to know 50, 000 documents.
Impact of query selectivity. Our evaluation also shows
that query selectivity is an important factor in the query
recovery rate. In particular, higher query selectivity implies
that the keyword is present in a higher number of documents
which increases the accuracy of the attacks. For example,
both volume analysis and selective volume analysis can build
stronger signatures on high-selectivity queries and therefore
obtain better recovery rates when δ ≈ 1. Similarly, on high-
selectivity queries, the subgraph attacks have richer neighbor
sets (i.e., more unique distribution of edges). Of course,
whether a user queries high- or low-selectivity keywords is
application speciﬁc but we believe that varying this parameter
is important when evaluating the recovery rate of an attack.