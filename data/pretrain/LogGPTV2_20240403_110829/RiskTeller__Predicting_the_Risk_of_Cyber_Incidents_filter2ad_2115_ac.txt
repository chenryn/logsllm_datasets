Health
HR
Insurance
IT
Legal
Logistics
Oil
Point of Sale
SDK
Secretary
Security
Statistics
# of Apps
142
1 243
796
246
353
547
146
145
251
490
100
294
71
We first match application categories to the files downloaded
on each machine: to this end, we create a ground truth of over
10K applications that fall into 26 different categories. Table 3 lists
the details about the application categories we incorporated to
our analysis; we obtained this comprehensive ground truth that
covers different types of job roles by manually querying Capterra5
to retrieve the list of most popular applications in each category.
Our dataset, unfortunately, does not provide information about
application names; therefore, we follow a simple heuristic, which
we validated through manual checking. For each application, Cap-
terra provides a vendor name together with an application name: to
match these applications within our dataset, we matched through
regular expressions the application name with the installation di-
rectory, and the vendor name with the file signer subject. Through
this matching, we are able to identify the 5 application categories
representing most events on each machine, and their frequency.
We use this data as features.
In addition to the aforementioned job-related application cate-
gories, we also include three application categories that are often
abused by attackers for malevolent purposes such as gathering infor-
mation about compromised hosts, cracking passwords, exfiltrating
sensitive data, etc. We focus on these (legitimate) applications due
to a growing number of advanced threats that incorporate system
administration or diagnosis tools in various stages of their attacks:
for example, in an attack worth millions of dollars of stolen credit
card data from the Target supermarket chain, the attackers used
popular network sniffers to steal credit card data, and other legiti-
mate tools such as ftp and scp to exfiltrate it [15]. Motivated by
such cases, we give a closer look at the machines that have tools
that can be useful for attackers, and understand whether having
these tools could be correlated with the risk of future infections.
5http://www.capterra.com/
Based on online documentation about cyber attacks similar to
the aforementioned Target case, we collected a list of 115 applica-
tions used by attackers or similar to them. For example, knowing
that ftp is used for data exfiltration, we also add sftp, scp, pscp
and winscp to our list since they can be used in the same way.
The first category of applications include 18 system diagnosis tools
such as ping, netstat, diskinfo, tcpview, whois, dig, etc. The
second category consists of 64 system administration tools such as
data transmission tools, device scanners, IP port scanners, pene-
tration testing tools, remote access applications and sniffers. The
last category, attack tools, includes applications that are either di-
rectly attack-related or can help the attacker achieve more than the
previous two category of applications. For example, pass-the-hash
attack tools, man-in-the-middle attack tools, password crackers,
exploitation tools, hijacking tools, tools to perform dictionary at-
tacks belong to this category. We identify these tools employing the
same heuristic discussed before for the other type of application
categories. Once we identify the files that belong to these criti-
cal category of applications, we keep the fraction of files in each
category as features.
4.1.5 History of malware and goodware events. As discussed
earlier, we split our year of data in a period for feature extraction
and a subsequent one for labeling; in the latter labeling period
we derive our ground truth to distinguish “clean” and “infected”
machines. It is reasonable to imagine that past infection history
is correlated with future events; to evaluate that, we include a set
of features about hashes of known benign/malicious files in the
feature extraction period. Perhaps surprisingly, our results show
that these features are within the least informative ones.
4.1.6 Prevalence-Based Features. If one looks at the binaries in-
stalled on a single machine, a majority of them are in common with
many other machines, and they are signed by a rather small num-
ber of different vendors. Indeed, low-prevalence files and signers
are an indicator of file suspiciousness: while there are legitimate
and benign reasons they can be generated even in large numbers
(e.g., files compiled on a given machine), malware tends to have
lower prevalence than benign software. Moreover, one of the best
ways to label an unknown file is to consider it similar to the files it
co-occurs with [5, 34]: for low-prevalence files this is intrinsically
difficult, since the low number of machines they are installed on
causes low confidence on results. In particular, it is very difficult to
clearly label prevalence-1 (or “singleton”) files, which occur exactly
once on a single machine. In our context, the fact that a machine
has a large number of low-prevalence files gives us reasons to be
suspicious about that machine.
For each event, we compute (i) the number of events and en-
terprises in which the file signer is seen, (ii) the number of events
in which the file hash is seen, (iii) the number of enterprises and
machines in which the file hash is seen. Based on this information,
we bucket these values and create 26 features as reported in Table 1.
As we shall see in the following, a larger number of files that are
rare or created by rare signers entails a larger risk.
Session F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1304Figure 2: Empirical cumulative distribution functions (ECDFs) for the numerical features considered as most significant in the
feature selection step, ranked by decreasing importance. We differentiate user profiles marked as risky, clean, and the overall
feature distribution. “Fraction” refers to the fraction of events per machine having the given property.
4.2 A Look at the Dataset
We now observe the feature distribution of our dataset; for obvious
space reasons, it is impossible to report them for all the 89 features
listed in Table 1. In Figure 2, we show the overall cumulative distri-
bution functions (CDFs) of the 9 most relevant features reported
in Table 5, along with the distribution observed for machine pro-
files marked as either risky or clean; the feature labeled as 1 is
the most important. We remark that these are not necessarily the
9 features that exhibit the most differences between infected and
clean users but rather the ones that, when considered together, best
allow separating infected machines from clean ones.
In general, the overall distribution and that of clean profiles are
similar; on the other hand, infected users behave differently: hence,
we can see the “signature” of risk in the behaviors that are more
consistent with risky users.
From plot 4 in Figure 2, we infer that risky users install substan-
tially less binaries on their machines than other users. Plots 2 and 6
corroborate this by showing that, on machines with risky profiles,
many files are very common and the total number of applications
is lower: it appears that machines that remain unused are more at
risk. Machines at risk are also updated very rarely, if ever: even
if the number of vulnerabilities on machines with risky profiles
is comparable to others (plot 8), those vulnerabilities are almost
never patched on machines with risky profiles (plots 3 and 5). Two
other characteristics associated with risk are higher usage during
weekends(plot 1), and a small but significant fraction of cases where
files are signed by entities appearing in few enterprises (plot 9).
The overall picture that emerges from this analysis is that risk
is strongly correlated with machines that are likely to be installed
and then forgotten, sitting unused with vulnerable and unpatched
operating systems and/or applications. Other risk patterns we ob-
serve are higher usage during weekends, and in some cases “weird”
files signed by rarely seen vendors.
This picture corroborates the idea that our feature can distinguish
features that can help us isolate the characteristics of risky behavior.
In the following, we describe the algorithms we designed to assign
risk scores to machines.
0.00.20.40.60.81.01.Fractionhappeningduringweekends.0.000.250.500.751.00ECDFAllRiskyClean0.00.10.20.30.42.Fractionfromthe150mostcommonﬁles.01234563.Applicationswithpatchedvulnerabilities.0200040006000800010000120004.Totalnumberofevents.0.000.250.500.751.00ECDF02468105.MaxCVSSscoreofpatchedvulnerabilities.0501001506.Totalnumberofapplications.01000200030004000500060007.Distinctﬁles.0.000.250.500.751.00ECDF0.00.51.01.52.08.Totalnumberofunpatchedapplications.0.00000.00050.00100.00150.00200.00259.Fractionwithsignerin[11,100]enterprises.Session F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA13055 PREDICTIVE ANALYTICS
The features defined in the previous section result in machine
profiles that are the inputs of our machine learning algorithms. The
first algorithm is a commonly used supervised machine learning
classifier: Random Forest Classifier. The second algorithm performs
semi-supervised learning such that even with ground truths that are
unbalanced and/or limited in size we can still predict future malware
infections. Before going into the details of these two algorithms,
we note that we transform the category-based features that are not
numerical through one-hot (or one-out-of-k) encoding. A categorical
feature having the ith of k possible values becomes a set of k binary
features where the ith value is set to 1, and all others are set to 0.
5.1 Random Forest Classifier
Random Forest Classifiers (RFCs) are ensemble learning machines
consisting of several decision trees which output a weighted vote
of the decision output of each individual tree [3, 11]. The classifier
aims at reducing the variance of the learning model through bias-
variance trade-off. We use RFCs due to the following merits:
(1) RFCs can handle categorical and numerical features and
do not require feature normalization.
(2) They behave well with new and previously unseen testing
data, providing unbiased estimates of the generalization
error; hence, they give a good approximation to the true
classification boundary. As reported on many datasets, they
produce accurate regression and classification outputs.
(3) Trees are helpful to provide an intuitive understanding on
how the prediction is made.
(4) The output of RFCs is an intrinsically well-calibrated prob-
ability that the model assigns to belonging to a given class.
(5) Most importantly, RFCs are intrinsically scalable and run
very efficiently on large-scale datasets similar to ours.
We run the RFC with 800 trees, chosen, as Liaw and Wiener [18]
advise, as the threshold where improving the number of trees does
not improve classifier accuracy.
5.2 Semi-Supervised Learning
While supervised machine learning algorithms perform better with
balanced ground truths having enough labeled elements, semi-
supervised learning (SSL) algorithms excel when the ground truth
datasets are unbalanced and/or small. Preparing a good ground
truth in many cases requires manual labeling or relies on some
automated tools that might result in faulty labels. The results of
previous works [21, 25, 29, 33] on several security problems demon-
strated SSL’s promising performance in reducing manual labeling
overheads and preserving classification accuracy.
We propose a novel SSL-based inductive learning engine for
RiskTeller, which conducts classifier training to estimate ground
truth and improve the classifier’s prediction accuracy at the same
time. Our SSL module is fed with n-dimensional feature vectors
{Xi ∈ Rn : i ∈ 1, . . . , m} describing user-behavioral profiles on m
machines. Without loss of generality, we assume that the first l of
them are labeled: Ri = 0 if profile Xi is risky, Ri = 1 otherwise.
Here, we show how we build a risk prediction model F such that
the risk score F (Xi) quantifies the infection risk for machine i.
We establish our design of the risk score on two principles. First,
risk scores are bounded in [0, 1] and continuous: a value of 1 indicates
that an unquestionable evidence of infection is detected on the
machine; a value of 0, conversely, indicates that the machine is free
from any potentially malicious files, because only known benign
files have been observed on the machine. Between these extreme
situations, risk score measures the likelihood of infection on a
machine for which we do not have conclusive evidence of being
either clean or infected: those with larger scores are more likely
to be, or eventually become, infected. The second principle is that
similar user profiles yield close risk scores: if two feature vectors Xa
and Xb are close to each other in the feature space, we infer that
those machines are used in a similar way, and hence they will have
similar risk scores.
We define the risk score as Pi = P (infection|Xi), the posterior
probability of becoming infected given the feature vector Xi. Being
a probability, this value is bounded in [0, 1] and continuous, with
extreme values corresponding to unquestionable indicators of in-
fection or cleanliness; higher values correspond to higher infection
risks, consistently with the first principle outlined above.
We implement the second principle through an optimization
framework. We seed the system through the “clean” and “infected”
labels of Section 6.1.2, which we use as a priori knowledge by
assigning to them risk scores of respectively 0 and 1; we then
propagate risk scores to all user profiles based on similarity between
feature vectors, optimizing an objective function
CP =
wi, j(cid:0)Pi − Pj(cid:1)2 + α
i
i, j
(1)
constrained by Pi = Ri for the labeled profiles i ∈ 1, . . . , l, and
where wi, j is the similarity between feature vectors Xi and Xj. The
first term of Equation 1 enforces that the risk scores of similar pro-
files should be as close as possible, enforcing our second principle.
The last term regularizes the risk score distribution, according to
maximum entropy theory [31]: it encourages risk scores to be as
distributed as possible on the [0, 1] axis while respecting the first
term and the constraints on labeled items, avoiding degenerate
solutions where all or most Pi items are close to 0 or 1. Our chosen
P∗
i values are therefore those that minimize CP∗:
(Pi − 0.5)2

We solve this problem using the alternative minimizing pro-
cedure [31]: we introduce a pseudo-variable {Qi} (i = 1, . . . , m)
resulting in a joint optimization problem with respect to P and Q:
∗ = argmin
P
P
CP s.t. Pi = Ri∀i ∈ 1 . . . l .

wi, j(cid:0)Pi − Qj(cid:1)2 + α
(Pi − 0.5)2
s.t. Pi = Qi = Ri∀i ∈ 1, . . . , l .
i

i, j
∗
P
, Q
∗ = argmin
P,Q
(2)
(3)
(4)
(5)
Our solution is to update Pi and Qi where i > l alternatively
until convergence, giving an iterative procedure composed by two
successive steps shown in Equations 4 and 5.




j(cid:44)i wi, j Pn−1
j
j wi, j
j(cid:44)i wi, jQn
j
j wi, j
Qn
i =
Pn
i =