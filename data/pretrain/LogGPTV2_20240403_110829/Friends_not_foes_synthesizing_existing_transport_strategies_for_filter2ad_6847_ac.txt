XCP [20]) show that even small changes to the data plane
are hard to deploy.
We ﬁrst describe the basic arbitration algorithm used in
PASE and the key sources of overhead that limit the scala-
bility of the arbitration process. We then present two opti-
mizations that reduce the arbitration overhead by exploiting
the characteristics of typical data center topologies.
Algorithm 1 Arbitration Algorithm
Input: 
Output: 
Arbitrator
locally maintains a sorted list of ﬂows
Step#1: Sort/update ﬂow entry based on the F lowSize
Step#2: Compute P rioQue and Rref of the ﬂow.
//Link capacity is C, and AggregateDemandHigher
(ADH) is the sum of demands of ﬂows with priority higher
than current ﬂow.
1: Assign rate:
if ADH  LowestQueue then
P rioQue = LowestQueue;
end if
3: return ;
3.1.1 Basic Arbitration Algorithm
For each link in the data center topology, there is an ar-
bitrator that runs the arbitration algorithm and makes de-
cisions for all ﬂows that traverse the particular link. The
arbitrator can be implemented on any server in the data
center or on dedicated controllers that are co-located with
the switches. The arbitration algorithm works at the gran-
ularity of a ﬂow, where a ﬂow could be a single RPC in a
typical client-server interaction, or a long running TCP-like
connection between two machines.
The interaction between a ﬂow’s source and the arbitra-
tor(s) is captured in Algorithm 1. The source provides the
arbitrator(s) with two pieces of information: i) the ﬂow size
(F lowSize), which is used as the criterion for scheduling
(i.e., shortest ﬂow ﬁrst). To support other scheduling tech-
niques, the F lowSize can be replaced by deadline [24] or
task-id for task-aware scheduling [17]. ii) demand, this rep-
resents the maximum rate at which the source can send data.
For long ﬂows that can saturate the link, this is equal to the
NIC rate, while for short ﬂows that do not have enough data
to saturate the link, this is set to a lower value. Demand
and ﬂow size are inputs to the arbitration algorithm whereas
the output is the reference rate (Rref ) and a priority queue
(P rioQue) of the ﬂow.
To compute Rref and P rioQue, the arbitrator locally
maintains a sorted list of ﬂows based on their sizes. This
list is updated based on the latest F lowSize information of
the current ﬂow. The ﬂow’s priority queue and reference rate
depend on the aggregate demand (AggregateDemandHigher
- or ADH) of ﬂows that have higher priority compared to
the current ﬂow. An ADH value less than the link capacity
C implies that there is some spare capacity on the link and
the ﬂow can be mapped to the top queue. Thus, if the ﬂow’s
demand is less than the spare capacity, we set the reference
rate equal to the demand. Otherwise, we set the reference
rate equal to the spare capacity.
The other case is when the ADH exceeds link capacity.
This happens when a link is already saturated by higher
priority ﬂows, so the current ﬂow cannot make it to the top
queue. In this case, the ﬂow’s reference rate is set to a base
value, which is equal to one packet per RTT. This allows
such low priority ﬂows to make progress in case some capac-
ity becomes available in the network, and to even increase
their rate in the future based on self-adjusting behavior.
Finally, if the current ﬂow cannot be mapped to the top
queue, it is either mapped to the lowest queue (if ADH
exceeds the aggregate capacity of intermediate queues) or
is mapped to one of the intermediate queues. Thus, each
intermediate queue accommodates ﬂows with an aggregate
demand of C and the last queue accommodates all the re-
maining ﬂows.
Challenges. While the above design provides a simple
distributed control plane for arbitration, it has three sources
of overhead that limit its scalability.
• Communication Latency. The communication latency
between the source and the arbitrator depends on their
physical distance within the data center. This delay
matters the most during ﬂow setup time as it can end
up increasing the FCT, especially for short ﬂows. To
keep this delay small, arbitrators must be placed care-
fully, such that they are located as close to the sources
as possible.
• Processing Overhead. The second challenge is the pro-
cessing overhead of arbitration messages, which can
potentially add non-negligible delay, especially under
high load scenarios.
• Network Overhead. Due to a separate control plane,
each arbitration message is potentially processed as a
separate packet by the switches which consumes link
capacity. We need to ensure that this overhead is kept
low and that it does not cause network congestion for
our primary traﬃc.
Figure 5: The bottom-up approach employed by
PASE for intra-rack and inter-rack scenarios.
To reduce these overheads, we extend the basic PASE de-
sign by introducing a bottom-up approach to arbitration,
which we describe next.
3.1.2 Bottom Up Arbitration
We exploit the typical tree structure of data center topolo-
gies. Sources obtain information about the P rioQue and
Rref in a bottom-up fashion, starting from the leaf nodes
up to the core as shown in Figure 5. For this purpose, the
end-to-end path between a source and destination is divided
into two halves – one from the source up to the root and the
other from the destination up to the root.
For each half, the respective leaf nodes (i.e., source and
destination) initiate the arbitration. They start oﬀ with
their link to the ToR switch and then move upwards. The
arbitration request messages move up the arbitration hier-
archy until it reaches the top level arbitrator. The responses
move downwards in the opposite direction.
This bottom-up approach ensures that for intra-rack com-
munication, arbitration is done solely at the endpoints, with-
out involving any external arbitrator. Thus, in this scenario,
ﬂows incur no additional network latency for arbitration.
This is particularly useful as many data center applications
have communication patterns that have an explicit notion
of rack aﬃnity [11, 13].
For the inter-rack scenario, the bottom up approach facili-
tates two other optimizations, early pruning and delegation,
to reduce the arbitration overhead and latency. Both early
pruning and delegation exploit a trade-oﬀ between low over-
head and high accuracy of arbitration. As our evaluation
shows, by giving away some accuracy, they can signiﬁcantly
decrease the arbitration overhead.
Early Pruning. The network and processing overheads
can be reduced by limiting the number of ﬂows that contact
the arbitrators for P rioQue and Rref information. In early
pruning, only ﬂows that are mapped to the highest priority
queue move upwards for arbitration. Thus, in Algorithm 1
a lower level arbitrator only sends the arbitration message
to its parent if the ﬂow is mapped to the top queue(s). This
results in lower priority ﬂows being pruned at lower levels, as
soon as they are mapped to lower priority queues (see Figure
6). The intuition behind this is that a ﬂow mapped to a lower
priority queue on one of its links will never make it to the
highest priority queue irrespective of the arbitration decision
on other links. This is because a ﬂow always uses the lowest
of the priority queues assigned by all the arbitrators (i.e.,
bottleneck in the path). Thus, we should avoid the overhead
of making arbitration decisions for the ﬂows mapped to lower
priority queues.
Arbitration Message Flow Receiver Response Sender ToR Aggregation Core ToR Receiver Aggregation Sender ToR Receiver Intra-Rack Inter-Rack Figure 6: The early pruning optimization used by
PASE for reducing the arbitration overhead. Note
that si and ki represent ﬂows that are mapped to the
highest priority queue at the senders and the ToR
arbitrators, respectively.
There are two key beneﬁts of early pruning. First, it re-
duces the network overhead as arbitration messages for only
high priority ﬂows propagate upwards. Second, it reduces
the processing load on higher level arbitrators. In both cases,
the reduction in overhead can be signiﬁcant, especially in the
more challenging heavy load scenarios, where such overhead
can hurt system performance.
In general, early pruning makes the overhead independent
of the total number of ﬂows in the system. Instead, with
early pruning, the overhead depends on the number of chil-
dren that a higher level arbitrator may have because each
child arbitrator only sends a limited number of ﬂows (the
top ones) upwards. Due to limited port density of modern
switches (typically less than 100), the number of children,
and hence the overhead, is quite small. This leads to signiﬁ-
cant reduction in the overhead compared to the non-pruning
case where the overhead is proportional to the total num-
ber of ﬂows traversing a link, which can be in thousands (or
more) for typical data center settings [11, 13]. However, the
above overhead reduction comes at the cost of less precise
arbitration. As we only send the top ﬂows to the higher level
arbitrators, ﬂows that are pruned do not get the complete
and accurate arbitration information. Our evaluation shows
that sending ﬂows belonging to the top two queues upwards
(rather than just the top queue), provides the right balance:
there is little performance degradation while reduction in
overhead is still signiﬁcant.
Delegation. Delegation is specially designed to reduce the
arbitration latency. While early pruning can signiﬁcantly
reduce the arbitration processing overhead as well as the
network overhead, it does not reduce the latency involved in
the arbitration decision because top ﬂows still need to go all
the way up to the top arbitrator.
In delegation, a higher level link (i.e., closer to the core)
is divided into smaller “virtual” links of lower capacity –
each virtual link is delegated to one of the child arbitrators
who then becomes responsible for all arbitration decisions
related to the virtual link. Thus, it can make a local decision
about a higher level link without going to the corresponding
arbitrator. On each virtual link, we run the same arbitration
algorithm i.e., Algorithm 1 as we do for normal links.
As a simple example, the aggregation-core link of capacity
C shown in Figure 7 can be divided into N virtual links of
capacity aiC each (where ai is the fraction of capacity allo-
cated to virtual link i) and then delegated to one of the child
Figure 7: The delegation optimization used by PASE
for reducing the setup latency and control overhead.
arbitrators. The capacity of each virtual link is updated pe-
riodically, reﬂecting the P rioQue of ﬂows received by each
child arbitrator. For example, one child that is consistently
observing ﬂows of higher priorities can get a virtual link of
higher capacity.
Delegation provides two beneﬁts. First, it reduces the
ﬂow setup delay because arbitration decisions for higher level
links are made at lower level arbitrators, which are likely to
be located close to the sources (e.g., within the rack). Sec-
ond, it reduces the control traﬃc destined towards higher
level arbitrators. Note that this happens because only ag-
gregate information about ﬂows is sent by the child arbitra-
tors to their parents for determining the new share of virtual
link capacities.
The impact on processing load has both positive and neg-
ative dimensions. While it reduces the processing load on
higher level arbitrators, it ends up increasing the load on
lower level arbitrators as they need to do arbitration for
their parents’ virtual links too. However, we believe this
may be acceptable as lower level arbitrators typically deal
with fewer ﬂows compared to top level arbitrators.
Like early pruning, delegation also involves the overhead-
accuracy trade-oﬀ. The capacity assigned to a speciﬁc vir-
tual link may not be accurate which may lead to perfor-
mance degradation. For example, we may have assigned a
lower virtual capacity to a child who may suddenly start
receiving higher priority ﬂows. These ﬂows would need to
wait for an update to the virtual link capacity before they
can get their due share. On the other hand, there could be
cases where the virtual capacity may remain unused if the
child does not have enough ﬂows to use this capacity. This
is especially true in scenarios where a link is delegated all
the way to the end-host and the end-host may have a bursty
ﬂow arrival pattern.
Given the above trade-oﬀ, PASE only delegates the
Aggregation-Core link capacity to its children (TOR-
Aggregation arbitrators). These child arbitrators should be
typically located within the rack of the source/destination
(or co-located with the TOR switch). Thus, for any inter-
rack communication within a typical three level tree topol-
ogy, the source and destination only need to contact their
TOR-aggregation arbitrator, who can do the arbitration all
the way up to the root. In fact, ﬂows need not wait for the
feedback from the other half i.e., destination-root. Thus, in
PASE, a ﬂow starts as soon as it receives arbitration infor-
mation from the child arbitrator. This approach is reason-
able in scenarios where both halves of the tree are likely to
have similar traﬃc patterns. If that is not true then PASE’s
self-adjusting behavior ensures that ﬂows adjust accordingly.
ToRs Aggregation Core k1 k2 kN s1 s2 sN Senders ToRs Aggregation Core a1C a2C C aNC Delegated Capacities Link Capacity 3.2 End-host Transport
PASE’s end-host transport builds on top of existing trans-
ports that use the self-adjusting endpoints strategy (e.g.,
DCTCP). Compared to existing protocols, the PASE trans-
port has to deal with two new additional pieces: (a) a pri-
ority queue (P rioQue) on which a ﬂow is mapped and (b)
a reference sending rate (Rref ). This impacts two aspects
of the transport protocol: rate control and loss recovery.
Rate control in PASE is more guided and is closely tied to
the Rref and the P rioQue of a ﬂow. Similarly, the trans-
port requires a new loss recovery mechanism because ﬂows
mapped to lower priority queues may experience spurious
timeouts as they have to wait for a long time before they
get an opportunity to send a packet. We now elaborate on