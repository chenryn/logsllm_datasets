−Xiyi
1 + eX(cid:62)
i θyi
+ λθ
n at each iteration.
n. Thus, gradient perturbation re-
i=1
which has a sensitivity of 2
quires sampling noise in the scale of 2
Deep learning. Deep learning follows the same learning pro-
cedure as in Algorithm 1, but the objective function is non-
convex. As a result, the sensitivity analysis methods of Chaud-
huri et al. [12] do not hold as they require a strong convexity
assumption. Hence, their output and objective perturbation
methods are not applicable. An alternative approach is to
replace the non-convex function with a convex polynomial
function [55, 56], and then use the standard objective pertur-
bation. This approach requires carefully designing convex
polynomial functions that can approximate the non-convexity,
which can still limit the model’s learning capacity. Moreover,
it would require a considerable change in the existing machine
learning infrastructure.
A simpler and more popular approach is to add noise to
the gradients. Application of gradient perturbation requires
a bound on the gradient norm. Since the gradient norm can
be unbounded in deep learning, gradient perturbation can be
used after manually clipping the gradients at each iteration. As
noted by Abadi et al. [1], norm clipping provides a sensitivity
bound on the gradients which is required for generating noise
in gradient perturbation.
Implementing Diﬀerential Privacy
2.3
This section surveys how diﬀerential privacy has been used in
machine learning applications, with a particular focus on the
compromises implementers have made to obtain satisfactory
utility. While the eﬀective privacy provided by diﬀerential
privacy mechanisms depends crucially on the choice of pri-
vacy budget , setting the  value is discretionary and higher
privacy budgets provide better utility.
Some of the early data analytics works on frequent pattern
mining [7,41], decision trees [21], private record linkage [30]
1898    28th USENIX Security Symposium
USENIX Association
Perturbation
Chaudhuri et al. [12]
Output and Objective
Pathak et al. [54]
Hamm et al. [25]
Output
Output
Zhang et al. [78]
Objective
Jain and Thakurta [33]
Objective
Jain and Thakurta [34] Output and Objective
Song et al. [63]
Gradient
Wu et al. [70]
Jayaraman et al. [35]
Output
Output
Data Set
Adult
KDDCup99
Adult
KDDCup99
URL
US
Brazil
CoverType
KDDCup2010
URL
COD-RNA
KDDCup99
MNIST†
Protein
CoverType
Adult
KDDCup99
n
45,220
70,000
45,220
493,000
200,000
370,000
190,000
500,000
20,000
100,000
60,000
50,000
60,000
72,876
498,010
45,220
70,000
d

0.2
105
0.2
119
0.2
105
1.0
123
1.0
50
0.8
14
0.8
14
54
0.5
2M 0.5
20M 0.1
0.1
1.0
1.0
0.05
0.05
0.5
0.5
8
9
15
74
54
104
122
Table 2: Simple ERM Methods which achieve High Utility with Low Privacy Budget.
† While MNIST is normally a 10-class task, Song et al. [63] use this for ‘1 vs rest’ binary classiﬁcation.
and recommender systems [47] were able to achieve both high
utility and privacy with  settings close to 1. These methods
rely on ﬁnding frequency counts as a sub-routine, and hence
provide -diﬀerential privacy by either perturbing the counts
using Laplace noise or by releasing the top frequency counts
using the exponential mechanism [48]. Machine learning, on
the other hand, performs much more complex data analysis,
and hence requires higher privacy budgets to maintain utility.
Next, we cover simple binary classiﬁcation works that use
small privacy budgets ( ≤ 1). Then we survey complex clas-
siﬁcation tasks which seem to require large privacy budgets.
Finally, we summarize recent works that aim to perform com-
plex tasks with low privacy budgets by using relaxed deﬁni-
tions of diﬀerential privacy.
Binary classiﬁcation. The ﬁrst practical implementation of a
private machine learning algorithm was proposed by Chaud-
huri and Monteleoni [11]. They provide a novel sensitivity
analysis under strong convexity constraints, allowing them to
use output and objective perturbation for binary logistic re-
gression. Chaudhuri et al. [12] subsequently generalized this
method for ERM algorithms. This sensitivity analysis method
has since been used by many works for binary classiﬁca-
tion tasks under diﬀerent learning settings (listed in Table 2).
While these applications can be implemented with low pri-
vacy budgets ( ≤ 1), they only perform learning in restricted
settings such as learning with low dimensional data, smooth
objective functions and strong convexity assumptions, and are
only applicable to simple binary classiﬁcation tasks.
There has also been considerable progress in general-
izing privacy-preserving machine learning to more com-
plex scenarios such as learning in high-dimensional set-
tings [33, 34, 64], learning without strong convexity assump-
tions [65], or relaxing the assumptions on data and objective
functions [62, 68, 77]. However, these advances are mainly of
theoretical interest and only a few works provide implemen-
tations [33, 34].
Complex learning tasks. All of the above works are lim-
ited to convex learning problems with binary classiﬁcation
tasks. Adopting their approaches to more complex learning
tasks requires higher privacy budgets (see Table 3). For in-
stance, the online version of ERM as considered by Jain et
al. [32] requires  as high as 10 to achieve acceptable utility.
From the deﬁnition of diﬀerential privacy, we can see that
Pr[M(D) ∈ S ] ≤ e10× Pr[M(D(cid:48)) ∈ S ]. In other words, even if
the model’s output probability is 0.0001 on a data set D(cid:48) that
doesn’t contain the target record, the model’s output proba-
bility can be as high as 0.9999 on a neighboring data set D
that contains the record. This allows an adversary to infer
the presence or absence of a target record from the training
data with high conﬁdence. Adopting these binary classiﬁca-
tion methods for multi-class classiﬁcation tasks requires even
higher  values. As noted by Wu et al. [70], it would require
training a separate binary classiﬁer for each class. Finally,
high privacy budgets are required for non-convex learning
algorithms, such as deep learning [60, 79]. Since the output
and objective perturbation methods of Chaudhuri et al. [12]
are not applicable to non-convex settings, implementations
of diﬀerentially private deep learning rely on gradient pertur-
USENIX Association
28th USENIX Security Symposium    1899
Task
Perturbation
Data Set
Jain et al. [32]
Iyengar et al. [31]
Online ERM
Binary ERM
Binary ERM
Multi-Class ERM
Multi-Class ERM
High Dimensional ERM
Objective
Objective
Phan et al. [55, 56]
Deep Learning
Objective
Shokri and Shmatikov [60]
Deep Learning
Gradient
Zhao et al. [79]
Deep Learning
Gradient
Year
CoverType
Adult
KDDCup99
CoverType
MNIST
Gisette
YesiWell
MNIST
MNIST
SVHN
US
MNIST
n
500,000
581,012
45,220
70,000
581,012
65,000
6,000
254
60,000
60,000
100,000
500,000
60,000
d
90
54
104
114
54
784
5,000
30
784
1,024
3,072
20
784
C
2
2
2
2
7
10
2
2
10
10
10
2
10

10
10
10
10
10
10
10
1
1
369,200
369,200
100
100
Table 3: Classiﬁcation Methods for Complex Tasks
Huang et al. [28]
Jayaraman et al. [35]
Task
ERM
ERM
DP Relaxation
MA
zCDP
Park et al. [53]
ERM
zCDP and MA
Lee [39]
ERM
zCDP
Geumlek et al. [23]
ERM
RDP
Beaulieu et al. [6]
Deep Learning
Abadi et al. [1]
Deep Learning
Yu et al. [75]
Deep Learning
Papernot et al. [52]
Deep Learning
Geyer et al. [24]
Deep Learning
Bhowmick et al. [8] Deep Learning
Hynes et al. [29]
Deep Learning
MA
MA
MA
MA
MA
MA
MA
Data Set
Adult
Adult
KDDCup99
Stroke
LifeScience
Gowalla
OlivettiFace
Adult
US
Brazil
Abalone
Adult
MNIST
eICU
TCGA
MNIST
CIFAR
MNIST
CIFAR
MNIST
SVHN
MNIST
MNIST
CIFAR
CIFAR
n
21,000
45,220
70,000
50,345
26,733
1,256,384
400
48,842
40,000
38,000
2,784
32,561
7,988
4,328
994
60,000
60,000
60,000
60,000
60,000
60,000
60,000
60,000
60,000
50,000
d
14
104
122
100
10
2
4,096
124
58
53
9
100
784
11
500
784
3,072
784
3,072
784
3,072
784
784
3,072
3,072
C
2
2
2
2