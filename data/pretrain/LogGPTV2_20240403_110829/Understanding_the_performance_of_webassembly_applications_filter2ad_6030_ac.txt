Speech signal processing algorithm
654
423
Simplified MIPS processor
1,040 Motion vector decoding for MPEG-2
33,933
Speech signal processing algorithm
Benchmark
covariance
correlation
gemm
gemver
gesummv
symm
syrk
syr2k
trmm
2mm
3mm
atax
bicg
doitgen
mvt
cholesky
durbin
gramschmidt
lu
ludcmp
trisolv
deriche
floyd-warshall
nussinov
adi
fdtd-2d
heat-3d
jacobi-1d
jacobi-2d
seidel-2d
ADPCM
AES
BLOWFISH
DFADD
DFDIV
DFMUL
DFSIN
GSM
MIPS
MOTION
SHA
cLOC1
175
201
194
215
181
194
172
187
171
214
229
170
186
176
180
170
163
185
170
212
154
227
146
495
205
214
171
157
160
150
733
1,120
1,804
809
644
622
975
549
390
1,007
1,367
C
h
c
n
e
B
y
l
o
P
e
n
o
t
S
H
C
Secure hash algorithm
1: Excluding modification from researchers and generic benchmark harness.
suites include compute-intensive applications which represent com-
mon usage scenarios according to WebAssembly design goals [16].
In particular, PolyBenchC and CHStone include benchmarks that
are relevant to applications such as scientific visualization, encryp-
tion, simulation, image/video/music editing/recognition, games,
and virtual/augmented reality. For example, Tensorflow [31], one
of the most popular AI/ML libraries, uses WebAssembly to achieve
a ten times improvement in the performance of their models over
the JS version. The benchmark software’s matrix computations
and other mathematical algorithms are directly relevant to this
type of use case. Similarly, graphic editing tools and online games,
such as Figma [35], a cloud-based graphic design tool for drawing,
leverages WebAssembly to improve its load time by three times.
We list detailed attributions of individual benchmarks to the use
cases as follows. (1) PolyBenchC: (1a) Scientific visualization and
simulation: “floyd-warshall”, “nussinov”, “adi”, “fdtd-2d”, “heat-3d”,
“jacobi-1d”, “jacobi-2d”, and “seidel-2d”. (1b) Image/video editing:
“deriche”. (1c) Image/video/signal processing applications: com-
monly use matrix computation benchmarks, including “gemm”,
“gemver”, “gesummv”, “symm”, “syrk”, “syr2k”, “trmm”, “2mm”,
“3mm”, “atax”, “bicg”, “doitgen”, “mvt”, “cholesky”, “lu”, and “trisolv”.
(1d) Math-oriented applications and equation solvers: “correlation”,
538
Exec. Time
Code Size
JS
0.95x
0.99x∗
0.94x#
0.99x
1.00x
0.99x
1.00x
1.00x
1.01x
WASM
0.88x
0.96x∗
0.86x#
1.00x
1.00x
0.99x
1.00x
1.00x
1.00x
x86
1.36x
0.97x
1.22x
1.00x
1.11x
0.99x
-
-
-
O1/O2
Ofast/O2
Oz/O2
O1/O2
Ofast/O2
Oz/O2
O1/O2
Ofast/O2
Oz/O2
Memory
∗: Ofast is unexpectedly slower than O1 and Oz.
#: Oz unexpectedly produces the fastest code.
433 GitHub stars) are from two different repositories ([33] and [20])
and are created by different developers.
4.2 Impact of Compilers and Compiler
Optimizations
4.2.1 Compiler Optimizations. We first measure the impact of com-
piler optimizations on WebAssembly performance. As discussed in
Section 2.1.2 (see Fig. 1), -Ofast is supposed to generate the fastest
code; -Oz should generate the most compact code; -O1 is supposed
to produce large code that runs slowly; -O2 should be faster than
-O1 and -Oz but slower than -Ofast in terms of execution time,
and generate code which is smaller than -O1 and -Ofast but larger
than -Oz.
Optimization for WebAssembly and JavaScript. Fig. 5 shows
the performance results of WebAssembly and JavaScript with four
optimization levels, -O1, -O2, -Ofast, and -Oz. Table 2 summarizes
the statistics of execution time, resulting code size, and runtime
memory usage. Further statistical analysis on compiler optimization
results are described in Appendix B.
Regarding execution time, we observe several counter-intuitive
results. Specifically, -Ofast, which is supposed to produce fastest
code, generated WebAssembly and JavaScript that execute slower
(annotated by ∗ in Table 2) than -O1 and -Oz. -Oz unexpectedly
produced the fastest WebAssembly (0.86x# compared to baseline
optimization -O2) and the fastest JavaScript (0.94x#). Besides, the
WebAssembly and JavaScript compiled with -O2 run slowest, al-
though -O2 is supposed to generate faster target code than -O1 and
-Oz. Next, we use two benchmarks as examples to explain what
causes the counter-intuitive results.
(1) ADPCM benchmark: The ‘ADPCM’ benchmark in WebAssem-
bly compiled with -Ofast spends 1.50x time to run compared to
that compiled with -O2. Fig. 7(a) shows the code snippet of the ‘AD-
PCM’ benchmark in C. Fig. 7(b) and (c) show the WebAssembly code
compiled from the C code shown in Fig. 7(a) with -O2 and -Ofast,
respectively. Fig. 7(a) highlighted the statements at lines 4-5 which
caused the counter-intuitive result. In particular, the global variable
‘result’ was never used, and therefore it should be eliminated in the
compiled code. As shown in Fig. 7(b), there is no code generated
for the C code at lines 4-5 with -O2. However, in Fig. 7(c), -Ofast
added 14 extra instructions (lines 14-27). These extra instructions
were executed 50 times during the experiment, leading to longer
execution time. It means that Ofast misses dead code elimination.
Understanding the Performance of WebAssembly Applications
IMC ’21, November 2–4, 2021, Virtual Event, USA
Table 2: Geometric means of compiler optimization results
(number less than 1 means it is faster/smaller than O2).
Metrics
Targets
This is counter-intuitive because Ofast is supposed to include all
of O3, which includes all of O2. From our further inspection, we
believe that this might be a bug in the compiler. Specifically, we
found a reported bug that is similar where O3 (and Ofast) perform
worse than O2 [27].
(2) Covariance benchmark: The ‘Covariance’ benchmark in We-
bAssembly compiled with -O1 takes 0.71x time compared to -O2.
Fig. 8(a) and (b) show the WebAssembly code compiled with -O2 and
-O1 respectively. As shown in Fig. 8(a), in -O2, a 64-bit float number
is defined by first defining a 32-bit integer (i32.const) and then
performing an i32-to-f64 type conversion (f64.convert_i32_s).
In -O1, however, the same number was passed in as a function ar-
gument $p0 (Fig. 8(b), line 9 and line 13). Because of the extra push
and pop operations performed on WebAssembly’s virtual stack, the
two instructions (lines 5-6) in Fig. 8(a) are executed slower than
the one instruction (line 13) in Fig. 8(b). We validated this intuition
using a simple experiment that loops the two code snippets (lines
5-6 in Fig. 8(a) and line 13 in Fig. 8(b)) for 1 million times. The result
shows that the one instruction in -O1 takes 0.77x of the time to run
than the two instructions in -O2.
From our experiments, we observe that there is no silver bullet
optimization flag for all target programs. For example, while Oz
produced the fastest WASM binaries on average (15 out of 41 are the
fastest), there are cases where other options (i.e., -O1/-O2/-Ofast)
produced the fastest WASM binaries. Specifically, -O1 generated
the fastest binaries for “gesummv”, “symm”, “atax”, “cholesky”, “tri-
solv”, “deriche”, “jacobi-2d”, and “SHA” (8 out of 41). -O2 compiled
“correlation”, “gemm”, “3mm”, “dotigen”, “gramschmidt”, “ADPCM”,
“GSM”, and “MIPS” are the fastest (8 out of 41). -Ofast produced
the fastest binaries for “covariance”, “syrk”, “bicg”, “durbin”, “lud-
cmp”, “floyd-warshall”, “nussinov”, “adi”, “jacobi-1d”, “seidel-2d”,
and “DFADD” (10 out of 41). Hence, our suggestion for applica-
tion developers (who use WASM compilers) is that while -Oz may
generally produce fast binaries, one should do a sufficient test and
choose an optimization flag based on the result. This is because
those optimizations typically target x86 binaries and seem to be
not designed and implemented for WASM in mind. As a result, our
takeaway for compiler developers is that there is a real demand to
tailor the optimization techniques to WebAssembly.
In terms of resulting code size, compared to the baseline op-
timization -O2, programs produced with -O1, -Ofast, and -Oz
optimizations have almost identical sizes (with less than 2% vari-
ance) for both WebAssembly and JavaScript. A few exceptions stem
from the code sizes of two CHStone benchmarks, ‘DFADD’ and
‘DFSIN’. These two benchmarks store the input data in global vari-
ables. Thus, a larger input size requires a larger data array, leading
to a larger code size.
The memory usage of WebAssembly and JavaScript is mostly
the same at all optimization levels. Note that we used medium-
sized input for the tests, which did not trigger dynamic memory
allocations extensively. The memory usage may differ if dynamic
memory allocations occur more frequently.
Optimization for x86. To prove that the counter-intuitive results
of WebAssembly and JavaScript are not compiler intended behav-
iors, we conduct the same experiments on x86. Specifically, we
compile the 41 C benchmarks to x86 machine code using LLVM
539
IMC ’21, November 2–4, 2021, Virtual Event, USA
Y. Yan et al.
Figure 5: Execution time (the top row) and resulting code size (the second row) of WebAssembly and JavaScript with -O1, -Ofast and -Oz,
compared to the result of -O2. Each benchmark was tested on Chrome v79 with the default input size.
Figure 6: Execution time (the top row) and code size (the second row) of x86 code with -O1, -Ofast and -Oz, relative to -O2.
with four optimization levels, -O1, -O2, -Ofast, and -Oz. To en-
sure that the results are comparable, we use LLVM v3.7.0, the same
version as the one Cheerp is built upon.
Fig. 6 shows the execution time and the resulting code size of
the compiled machine code. The result statistics shown in Table
2 (column ‘x86’) are aligned with the expected results described
in Fig. 1. Specifically, -Ofast generated the fastest code (0.97x
of the execution time relative to -O2). -Oz leads to the smallest
target code size (0.99x relative to -O2). -O2 produced code that
takes less execution time than -O1 (0.74x execution time) and -Oz
(0.82x execution time) but, more execution time than -Ofast ( 1.03x
execution time). In addition, the size of the code generated using
-O2 is smaller than -Ofast (0.90x) but larger than -Oz (1.01x).
The experiment was run on desktop Chrome with each benchmark’s
default input size (i.e., medium-sized input). The result shows that
benchmarks compiled by Emscripten run faster (2.70x geometric
mean) than benchmarks compiled by Cheerp, but they use 6.02x
(geometric mean) more memory. Note that Emscripten uses 16MB
as its page size, i.e., the smallest memory that needs to be allocated
for instantiating WebAssembly modules. By contrast, the page size
of Cheerp is 64KB. This difference makes programs compiled by
Cheerp use less memory but run slower because of the overhead in-