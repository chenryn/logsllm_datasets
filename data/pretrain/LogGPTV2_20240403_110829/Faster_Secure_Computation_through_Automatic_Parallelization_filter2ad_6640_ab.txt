In 2012, Holzer et al. [14] presented the ﬁrst compiler for
a large subset of C to garbled circuits, named CBMC-
GC. The compiler unrolls all loops and recursive state-
ments present in the input program up to a given or stati-
cally determined bound. Afterwards, each statement is
transformed to a Boolean formula preserving the bit-
precise semantics of C. The Boolean formula is then
translated into a circuit, which is optimized for Yao’s gar-
bled circuits [10].
The only difference between C code and code for TPC
is a special naming convention introduced by CBMC-
GC. Listing 1 shows example source code for the mil-
lionaires’ problem. The shown procedure is a standard
C procedure, where only the input and output variables
are speciﬁcally marked as designated input of party PA
or PB (Lines 2 and 3) or as output (Line 4). Aside from
this naming convention, arbitrary C computations are al-
lowed to produce the desired result, in this case a simple
comparison (Line 5).
1 void m i l l i o n a i r e s () {
2
int I N P U T _ A _ i n c o m e ;
int I N P U T _ B _ i n c o m e ;
int O U T P U T _ r e s u l t = 0;
3
4
5
if ( I N P U T _ A _ i n c o m e > I N P U T _ B _ i n c o m e )
O U T P U T _ r e s u l t = 1;
6
7 }
Listing 1: CBMC-GC Code for Yao’s Millionaires’ Problem.
either annotates or exports parallel regions. Annota-
tions are realized with the OpenMP language, parallel
executable kernels for Cuda/OpenCL are exported using
static code analysis techniques. In this work, we mainly
build upon the OpenMP output.
4 Parallelizing of Yao’s Garbled Circuits
To exploit parallelism in Yao’s protocol, groups of gates
that can be garbled independently need to be identiﬁed.
Independent gates can be garbled in parallel by the gen-
erator, as well as evaluated in parallel by the evaluator.
However, detecting independent, similar sized groups of
gates is known as the NP-hard graph partitioning prob-
lem [28]. The common approach to circumvent the ex-
pensive search for an optimal solution is to use heuris-
tics. In this section, we ﬁrst discuss sequential and paral-
lel composition of functionalities (§ 4.1) and show how
circuits can be garbled in parallel (§ 4.2), before intro-
ducing the ﬁne-grained parallelization heuristic (§ 4.3)
and the coarse-grained parallelization heuristic (§ 4.4).
this paper, we
4.1 Parallel and sequential decomposition
functionali-
Throughout
ties f (x,y) with two input bit strings x, y and an output
bit string o. Furthermore, we use Cf
to denote the
circuit that represents functionality f . We refer to a
functionality f as sequentially decomposable into sub
functionalities f1 and f2 iff f (x,y) = f2( f1(x,y),x,y).
consider
length,
Moreover, we consider a functionality f (x,y) as
parallel decomposable into sub functionalities f1(x,y)
and f2(x,y) with non-zero output bit
if a
bit string permutation σ f exists such that
f (x,y) =
σ f ( f1(x,y)|| f2(x,y)), where || donates a bitwise concate-
nation operator. Thus, functionality f can directly be
evaluated by independent evaluation of f1 and f2. We
note that f1 and f2 do not necessarily have to be deﬁned
over all bits of x and y. Depending on f they could share
none, some, or all input bits.
3.3 Automatic source code parallelization
In 2012, Amini et al. [1] presented Par4all, an automatic
parallelizing and optimizing compiler for C. It was de-
veloped to integrate several compilation tools into one
single powerful compiler. Par4all is based on the Pips [6]
source-to-source compiler infrastructure that detects par-
allelism and uses the POCC [32] polyhedral loop opti-
mizer to perform memory access optimizations. Par4all
is capable of producing parallel OpenMP [7], Cuda and
OpenCL code. Par4all operates on any ANSI-C code
as input, automatically detects parallel control ﬂow and
We use the operator (cid:29) to express a parallel composi-
tion of two functionalities through the existence of a per-
mutation σ. Thus, we write f (x,y) = f1(x,y, )(cid:29) f2(x,y)
if there exists a permutation σ f such that
f (x,y) =
σ f ( f1(x,y)|| f2(x,y)).
We call a parallelization of f to be efﬁcient if the cir-
cuit size (i.e., number of gates) of the parallelized func-
tionality is roughly equal to the circuit size of the se-
quential functionality: size(Cf ) ≈ size(Cf1) + size(Cf2).
Due to the different garbling methods for linear and non-
linear gates in Yao’s protocol using the free-XOR tech-
nique, size(Cf ) is better measured by the number of non-
linear gates. Furthermore, we refer to a parallelization as
534  24th USENIX Security Symposium 
USENIX Association
4
symmetric if sub functionalities have almost equal circuit
sizes: size(Cf1) ≈ size(Cf2).
Finally, we refer to functionalities that can be de-
composed into a sequential and a parallel part as mixed
functionalities. For example the functionality f (x,y) =
f3( f1(x,y) (cid:30) f2(x,y),x,y) can ﬁrst be decomposed se-
quentially in f3 and f1 (cid:30) f2, where the latter part can then
be further decomposed in f1 and f2.
Without an explicit deﬁnition, we note that all deﬁni-
tions can be extended from the dual case f1 and f2 to the
general case f1, f2, . . . , fn.
4.2 Parallel circuit creation and evaluation
A circuit that consists of annotated sequential and paral-
lel parts can be garbled in parallel as follows. Sequential
regions of a circuit can be garbled using standard tech-
niques by iterating topologically over all gates. Once a
parallel decomposable region of a circuit is reached, par-
allelization is applied. All independent sub circuits in
every parallel region can be garbled in any order by any
available thread (see Figure 1). We note that the gar-
bling order has no impact on the security [25]. After
every parallel region a synchronization between the dif-
ferent threads is needed to guarantee that all wire labels
for the next region of the circuit are computed. Multiple
subsequent parallel regions with different degrees of par-
allelism can be garbled, when ensuring synchronization
in-between.
The circuit evaluation can be parallelized in the same
manner. Sequential regions are computed sequentially,
parallel regions are computed in parallel by different
threads. After every parallel region a thread synchroniza-
tion is required to ensure data consistency.
When using pipe-lining the garbled tables have to be
transmitted in an identiﬁable order to ensure data con-
sistency between generator and evaluator. We propose
three different variants. First, all garbled tables can be
enriched with a numbering, e.g., an index, which allows
a unordered transfer to the evaluator. The evaluator is
then able to reconstruct the original order based on the
introduced numbering. This approach has the disadvan-
tage of an increased communication cost. The second
approach is that garbled tables are sent in a synchronized
and predeﬁned order. This approach functions without
additional communication, yet can lead to an undesir-
able ‘pulsed’ communication pattern. The third approach
functions by strictly separating the communication chan-
nels for every sub circuit. This can either be realized by
multiplexing within the TPC framework or by exploiting
the capabilities of the underlying operating system. Due
to the aforementioned reasons, our implementation of a
parallel framework (presented in § 6.1) builds upon the
latter approach.
Figure 1: Interaction between a parallel circuit generator and
evaluator. The layer n of the presented circuit is garbled and
evaluated in parallel. The independent partitions of the circuit
can be garbled and evaluated by different threads in any order.
4.3 Fine-grained parallelism
A ﬁrst heuristic to decompose a circuit into parallel
parts is the ﬁne-grained gate level approach, described
in the following. Similar to the evaluation of a stan-
dard Boolean circuit, gates in garbled circuits are pro-
cessed in topological execution order. Gates provide in-
put to other gates and hence, can be ordered by the circuit
level (depth) when all their inputs are ready or the level
when their output is required for succeeding gates. Con-
sequently and as proposed by others [3, 17], gates on the
same level can be garbled in parallel. Thus, a circuit is
sequentially decomposable into different levels and each
level is further decomposable in parallel with a granular-
ity up to the number of gates. Figure 2 illustrates ﬁne-
grained decomposition of a circuit into three levels L1,
L2 and L3.
To achieve an efﬁcient distribution of gates onto
threads during protocol execution, it is useful to iden-
tify the circuit levels during the circuit compilation pro-
cess. Furthermore, a reasonable heuristic to symmetri-
cally distribute the workload onto all threads when using
the free-XOR optimization is to divide linear and non-
linear gates independently. Hence, each thread gets as-
signed the same number of linear and non-linear gates
to garble. Therefore, we extended the circuit compiler
CBMC-GC with the capability to mark levels and to
strictly separate linear from non-linear gates within each
level. This information is stored in the circuit descrip-
tion.
Overhead.
In practice, multi-threading introduces an
computational overhead to enable thread management
and thread synchronization. Therefore, it is useful to ex-
perimentally determine a system dependent threshold τ
that describes the minimal number of gates that are re-
quired per level to proﬁt from parallel execution. In prac-
tical settings (see § 6) we observe that at least ∼ 8 non-
USENIX Association  
24th USENIX Security Symposium  535
5
CPUcircuit generatorcircuit evaluatorsync. transfer of labelsparallelgatesnn - 1sync.networkCPUCPUCPUCPUmultiple-data paradigm (SIMD), only one sub circuit per
parallel region is compiled, which reduces the circuit
storage costs. During protocol runtime, the sub circuit
is unrolled and garbled in full extent. The global and sub
circuits are interconnected by explicitly deﬁning inner
input and output wires. These are not exposed as TPC
inputs or outputs, but have to be used by TPC frame-
works to recompose the complete and parallel executable
circuit. The compilation process itself consists of four
different steps:
(1) In the ﬁrst step, parallelism in C code is detected by
Par4all and annotated using the OpenMP notation and
source-to-source transformations.
(2) The annotated C code is parsed by ParCC in the
second step. The source code is decomposed using
source-to-source techniques into a global sequentially
executable part, which is interrupted by one or multiple
parallel executable sub parts. Additionally, functional
OpenMP annotations, such as reduction statements, are
replaced with C code that is compiled into the circuits.
Furthermore, information about the degree of detected
parallelism as well as the interconnection between the
global and sub parts is extracted for later compilation
steps.
(3) Given the decomposed source code, the different
parts are compiled independently with CBMC-GC.
(4) In the ﬁnal step information about the mapping of
wires between gates in the global and the sub circuits is
exported for use in TPC frameworks. For performance
reasons, we distinguish static wires that are shared be-
tween parallel sub circuits and wires that are dedicated
for each individual sub circuit.
Example. To illustrate the functionality of ParCC, we
discuss the source-to-source compilation steps on a small
fork and join task, namely the dot product of two vectors
a and b:
r = a· b = a0 · b0 +··· + an · bn.
The source code of the function dot product() is pre-
sented in Listing 2.
1 int mult ( int a , int b ) {
2
3 }
return a * b ;
4 void d o t _ p r o d u c t () {
5
int I N P U T _ A _ a [100] , I N P U T _ B _ b [100];
int res = 0;
6
7
8
9
for ( i = 0; i < 100; i ++)
res += mult ( I N P U T _ A _ a [ i ] , \
I N P U T _ B _ b [ i ]);
int O U T P U T _ r e s = res ;
10
11 }
Listing 2: Dot vector product written in C with CBMC-GC
input/output notation.
Figure 2: Circuit decomposition. Each level L1, L2 and L3
consists of multiple gates that can be garbled using FGP with
synchronization in-between. The circuit can also be decom-
posed in two coarse-grained partitions P1 and P2.
linear gates per core are required to observe ﬁrst speed-
ups. Achieving a parallelization efﬁciency of 90%, i.e,
a speed up of 1.8 on 2 cores, requires at least 512 non-
linear gates per core. In the next section we present an
approach that overcomes the limitations of FGP.
4.4 Coarse-grained parallelism
Another useful heuristic to partition a circuit is the us-
age of high-level functionality descriptions. Given a cir-
cuit description in a high-level language, parallelizable
regions of the code can be identiﬁed using programming
language techniques. These detected code regions can
then be tracked during the circuit compilation process
to produce parallel decomposable circuits. The differ-
ent sub circuits are guaranteed to be independent of each
other and therefore can be garbled in parallel. We re-
fer to this parallelization scheme as coarse-grained par-
allelization (CGP). Figure 2 illustrates an example de-
composition in two coarse-grained partitions P1 and P2.
Furthermore, we note that FGP and CGP can be com-
bined by utilizing FGP within all coarse partations. In
the following paragraphs, we introduce our CBMC-GC
compiler extension that automatically produces coarse-
grained parallel circuits.
Compiler for parallel circuits. Our parallel circuit
complier ParCC extends the CBMC-GC compiler and
builds on top of the Par4all compiler introduced in § 3.
ParCC takes C code as input, which carries annotations
according the CBMC-GC notation. Hence, TPC input
and output variables of both parties are marked as such.
ParCC detects parallelism within this code with the help
of Par4all and produces one global circuit that is inter-
rupted by one or multiple sub circuits for every parallel
region. If a parallel region follows the single-instruction-
536  24th USENIX Security Symposium 
USENIX Association
6
&&&≥&≥&&≥&&≥≥&≥&≥&≥P2P1L2L1L3Input wiresOutput wiresIn this example code, two parties provide input for
two vectors in form of constant
length integer ar-
rays (Line 5). A loop iterates pairwise over all ar-
ray elements (Line 7), multiplies the elements and
aggregates the result.
In the ﬁrst compilation step,
Par4all detects the parallelism in the loop body and
annotates this parallel region accordingly. Therefore,
Par4all adds the statement #pragma omp parallel
for reduction(+:res) before the for loop in Line 7.
ParCC parses the annotations in the second compila-
tion step to export the loop body, in this case the sub
function mult(), printed in Listing 3.
1 void mult ( int INPUT_A_a , int INPUT_A_b ,
2
3 {
4
int O U T P U T _ r e t u r n )
int a = I N P U T _ A _ a ;
int b = I N P U T _ A _ b ;
5
O U T P U T _ r e t u r n = a * b ;
6
7 }
Listing 3: Exported sub function with CBMC-GC input-
output notation.
The functions arguments are rewritten according the no-
tation of CBMC-GC. Thus, the two arguments a and
b of mult() become inner inputs of the sub circuit,