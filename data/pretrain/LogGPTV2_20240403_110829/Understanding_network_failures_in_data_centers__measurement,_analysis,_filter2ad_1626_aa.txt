title:Understanding network failures in data centers: measurement, analysis,
and implications
author:Phillipa Gill and
Navendu Jain and
Nachiappan Nagappan
Understanding Network Failures in Data Centers:
Measurement, Analysis, and Implications
Phillipa Gill
Nachiappan Nagappan
University of Toronto
PI:EMAIL
Navendu Jain
Microsoft Research
PI:EMAIL
Microsoft Research
PI:EMAIL
ABSTRACT
We present the ﬁrst large-scale analysis of failures in a data cen-
ter network. Through our analysis, we seek to answer several fun-
damental questions: which devices/links are most unreliable, what
causes failures, how do failures impact network trafﬁc and how ef-
fective is network redundancy? We answer these questions using
multiple data sources commonly collected by network operators.
The key ﬁndings of our study are that (1) data center networks
show high reliability, (2) commodity switches such as ToRs and
AggS are highly reliable, (3) load balancers dominate in terms of
failure occurrences with many short-lived software related faults,
(4) failures have potential to cause loss of many small packets such
as keep alive messages and ACKs, and (5) network redundancy is
only 40% effective in reducing the median impact of failure.
Categories and Subject Descriptors: C.2.3 [Computer-Comm-
unication Network]: Network Operations
General Terms: Network Management, Performance, Reliability
Keywords: Data Centers, Network Reliability
1.
INTRODUCTION
Demand for dynamic scaling and beneﬁts from economies of
scale are driving the creation of mega data centers to host a broad
range of services such as Web search, e-commerce, storage backup,
video streaming, high-performance computing, and data analytics.
To host these applications, data center networks need to be scalable,
efﬁcient, fault tolerant, and easy-to-manage. Recognizing this need,
the research community has proposed several architectures to im-
prove scalability and performance of data center networks [2, 3, 12–
14, 17, 21]. However, the issue of reliability has remained unad-
dressed, mainly due to a dearth of available empirical data on fail-
ures in these networks.
In this paper, we study data center network reliability by ana-
lyzing network error logs collected for over a year from thousands
of network devices across tens of geographically distributed data
centers. Our goals for this analysis are two-fold. First, we seek
to characterize network failure patterns in data centers and under-
stand overall reliability of the network. Second, we want to leverage
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15-19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
lessons learned from this study to guide the design of future data
center networks.
Motivated by issues encountered by network operators, we
study network reliability along three dimensions:
(cid:129) Characterizing the most failure prone network elements. To
achieve high availability amidst multiple failure sources such as
hardware, software, and human errors, operators need to focus
on ﬁxing the most unreliable devices and links in the network.
To this end, we characterize failures to identify network elements
with high impact on network reliability e.g., those that fail with
high frequency or that incur high downtime.
(cid:129) Estimating the impact of failures. Given limited resources at
hand, operators need to prioritize severe incidents for troubleshoot-
ing based on their impact to end-users and applications. In gen-
eral, however, it is difﬁcult to accurately quantify a failure’s im-
pact from error logs, and annotations provided by operators in
trouble tickets tend to be ambiguous. Thus, as a ﬁrst step, we
estimate failure impact by correlating event logs with recent net-
work trafﬁc observed on links involved in the event. Note that
logged events do not necessarily result in a service outage be-
cause of failure-mitigation techniques such as network redun-
dancy [1] and replication of compute and data [11, 27], typically
deployed in data centers.
(cid:129) Analyzing the effectiveness of network redundancy. Ideally,
operators want to mask all failures before applications experi-
ence any disruption. Current data center networks typically pro-
vide 1:1 redundancy to allow trafﬁc to ﬂow along an alternate
route when a device or link becomes unavailable [1]. However,
this redundancy comes at a high cost—both monetary expenses
and management overheads—to maintain a large number of net-
work devices and links in the multi-rooted tree topology. To ana-
lyze its effectiveness, we compare trafﬁc on a per-link basis dur-
ing failure events to trafﬁc across all links in the network redun-
dancy group where the failure occurred.
For our study, we leverage multiple monitoring tools put in
place by our network operators. We utilize data sources that pro-
vide both a static view (e.g., router conﬁguration ﬁles, device pro-
curement data) and a dynamic view (e.g., SNMP polling, syslog,
trouble tickets) of the network. Analyzing these data sources, how-
ever, poses several challenges. First, since these logs track low level
network events, they do not necessarily imply application perfor-
mance impact or service outage. Second, we need to separate fail-
ures that potentially impact network connectivity from high volume
and often noisy network logs e.g., warnings and error messages
even when the device is functional. Finally, analyzing the effec-
tiveness of network redundancy requires correlating multiple data
350sources across redundant devices and links. Through our analysis,
we aim to address these challenges to characterize network fail-
ures, estimate the failure impact, and analyze the effectiveness of
network redundancy in data centers.
1.1 Key observations
We make several key observations from our study:
(cid:129) Data center networks are reliable. We ﬁnd that overall the data
center network exhibits high reliability with more than four 9’s
of availability for about 80% of the links and for about 60% of
the devices in the network (Section 4.5.3).
(cid:129) Low-cost, commodity switches are highly reliable. We ﬁnd
that Top of Rack switches (ToRs) and aggregation switches ex-
hibit the highest reliability in the network with failure rates of
about 5% and 10%, respectively. This observation supports net-
work design proposals that aim to build data center networks
using low cost, commodity switches [3, 12, 21] (Section 4.3).
(cid:129) Load balancers experience a high number of software faults.
We observe 1 in 5 load balancers exhibit a failure (Section 4.3)
and that they experience many transient software faults (Sec-
tion 4.7).
(cid:129) Failures potentially cause loss of a large number of small
packets. By correlating network trafﬁc with link failure events,
we estimate the amount of packets and data lost during failures.
We ﬁnd that most failures lose a large number of packets rela-
tive to the number of lost bytes (Section 5), likely due to loss of
protocol-speciﬁc keep alive messages or ACKs.
(cid:129) Network redundancy helps, but it is not entirely effective.
Ideally, network redundancy should completely mask all fail-
ures from applications. However, we observe that network re-
dundancy is only able to reduce the median impact of failures (in
terms of lost bytes or packets) by up to 40% (Section 5.1).
Limitations. As with any large-scale empirical study, our results
are subject to several limitations. First, the best-effort nature of fail-
ure reporting may lead to missed events or multiply-logged events.
While we perform data cleaning (Section 3) to ﬁlter the noise, some
events may still be lost due to software faults (e.g., ﬁrmware errors)
or disconnections (e.g., under correlated failures). Second, human
bias may arise in failure annotations (e.g., root cause). This concern
is alleviated to an extent by veriﬁcation with operators, and scale
and diversity of our network logs. Third, network errors do not al-
ways impact network trafﬁc or service availability, due to several
factors such as in-built redundancy at network, data, and applica-
tion layers. Thus, our failure rates should not be interpreted as im-
pacting applications. Overall, we hope that this study contributes to
a deeper understanding of network reliability in data centers.
Paper organization. The rest of this paper is organized as follows.
Section 2 presents our network architecture and workload charac-
teristics. Data sources and methodology are described in Section 3.
We characterize failures over a year within our data centers in Sec-
tion 4. We estimate the impact of failures on applications and the
effectiveness of network redundancy in masking them in Section 5.
Finally we discuss implications of our study for future data center
networks in Section 6. We present related work in Section 7 and
conclude in Section 8.
Internet
Core
Core
Internet
Data center
Layer 3
AccR
AccR
} primary and
back up 
AggS
AggS
LB
Layer 2
LB
ToR ToR
ToR
ToR
LB
LB
Figure 1: A conventional data center network architecture
adapted from ﬁgure by Cisco [12]. The device naming conven-
tion is summarized in Table 1.
Table 1: Summary of device abbreviations
Devices
Type
AggS AggS-1, AggS-2
LB
ToR
AccR
Core
LB-1, LB-2, LB-3
ToR-1, ToR-2, ToR-3
-
-
Description
Aggregation switches
Load balancers
Top of Rack switches
Access routers
Core routers
2. BACKGROUND
Our study focuses on characterizing failure events within our
organization’s set of data centers. We next give an overview of data
center networks and workload characteristics.
2.1 Data center network architecture
Figure 1 illustrates an example of a partial data center net-
work architecture [1]. In the network, rack-mounted servers are
connected (or dual-homed) to a Top of Rack (ToR) switch usu-
ally via a 1 Gbps link. The ToR is in turn connected to a primary
and back up aggregation switch (AggS) for redundancy. Each re-
dundant pair of AggS aggregates trafﬁc from tens of ToRs which
is then forwarded to the access routers (AccR). The access routers
aggregate trafﬁc from up to several thousand servers and route it to
core routers that connect to the rest of the data center network and
Internet.
All links in our data centers use Ethernet as the link layer
protocol and physical connections are a mix of copper and ﬁber
cables. The servers are partitioned into virtual LANs (VLANs) to
limit overheads (e.g., ARP broadcasts, packet ﬂooding) and to iso-
late different applications hosted in the network. At each layer of
the data center network topology, with the exception of a subset of
ToRs, 1:1 redundancy is built into the network topology to miti-
gate failures. As part of our study, we evaluate the effectiveness of
redundancy in masking failures when one (or more) components
fail, and analyze how the tree topology affects failure characteris-
tics e.g., correlated failures.
In addition to routers and switches, our network contains many
middle boxes such as load balancers and ﬁrewalls. Redundant pairs
of load balancers (LBs) connect to each aggregation switch and
351]
x
<
X
P
[
0
1
.
8
.
0
6
.
0
4
0
.
2
.
0
0
.
0
TRUNK
LB
MGMT
CORE
ISC
IX
Overall
0.0
0.2
0.4
0.6
0.8
1.0
Daily 95th percentile utilization
Figure 2: The daily 95th percentile utilization as computed us-
ing ﬁve-minute trafﬁc averages (in bytes).
Table 2: Summary of link types
Description
Type
TRUNK connect ToRs to AggS and AggS to AccR
LB
MGMT
CORE
ISC
IX
connect load balancers to AggS
management interfaces
connect routers (AccR, Core) in the network core
connect primary and back up switches/routers
connect data centers (wide area network)
perform mapping between static IP addresses (exposed to clients
through DNS) and dynamic IP addresses of the servers that process
user requests. Some applications require programming the load bal-
ancers and upgrading their software and conﬁguration to support
different functionalities.
Network composition. The device-level breakdown of our net-
work is as follows. ToRs are the most prevalent device type in our
network comprising approximately three quarters of devices. LBs
are the next most prevalent at one in ten devices. The remaining
15% of devices are AggS, Core and AccR. We observe the effects
of prevalent ToRs in Section 4.4, where despite being highly re-
liable, ToRs account for a large amount of downtime. LBs on the
other hand account for few devices but are extremely failure prone,
making them a leading contributor of failures (Section 4.4).
2.2 Data center workload characteristics
Our network is used in the delivery of many online applica-
tions. As a result, it is subject to many well known properties of
data center trafﬁc; in particular the prevalence of a large volume
of short-lived latency-sensitive “mice” ﬂows and a few long-lived
throughput-sensitive “elephant” ﬂows that make up the majority of
bytes transferred in the network. These properties have also been
observed by others [4, 5, 12].
Network utilization. Figure 2 shows the daily 95th percentile uti-
lization as computed using ﬁve-minute trafﬁc averages (in bytes).
We divide links into six categories based on their role in the net-
work (summarized in Table 2). TRUNK and LB links which re-
side lower in the network topology are least utilized with 90% of
TRUNK links observing less than 24% utilization. Links higher in
the topology such as CORE links observe higher utilization with
90% of CORE links observing less than 51% utilization. Finally,
links that connect data centers (IX) are the most utilized with 35%
observing utilization of more than 45%. Similar to prior studies
of data center network trafﬁc [5], we observe higher utilization at
upper layers of the topology as a result of aggregation and high
bandwidth oversubscription [12]. Note that since the trafﬁc mea-
surement is at the granularity of ﬁve minute averages, it is likely to
smooth the effect of short-lived trafﬁc spikes on link utilization.
3. METHODOLOGY AND DATA SETS
The network operators collect data from multiple sources to
track the performance and health of the network. We leverage these
existing data sets in our analysis of network failures. In this section,
we ﬁrst describe the data sets and then the steps we took to extract
failures of network elements.
3.1 Existing data sets
The data sets used in our analysis are a subset of what is col-
lected by the network operators. We describe these data sets in turn:
(cid:129) Network event logs (SNMP/syslog). We consider logs derived
from syslog, SNMP traps and polling, collected by our network
operators. The operators ﬁlter the logs to reduce the number of
transient events and produce a smaller set of actionable events.
One of the ﬁltering rules excludes link failures reported by servers
connected to ToRs as these links are extremely prone to spurious
port ﬂapping (e.g., more than 100,000 events per hour across the
network). Of the ﬁltered events, 90% are assigned to NOC tickets
that must be investigated for troubleshooting. These event logs
contain information about what type of network element expe-
rienced the event, what type of event it was, a small amount of
descriptive text (machine generated) and an ID number for any
NOC tickets relating to the event. For this study we analyzed a
year’s worth of events from October 2009 to September 2010.
(cid:129) NOC Tickets. To track the resolution of issues, the operators
employ a ticketing system. Tickets contain information about
when and how events were discovered as well as when they were
resolved. Additional descriptive tags are applied to tickets de-
scribing the cause of the problem, if any speciﬁc device was at
fault, as well as a “diary” logging steps taken by the operators as
they worked to resolve the issue.
(cid:129) Network trafﬁc data. Data transferred on network interfaces is
logged using SNMP polling. This data includes ﬁve minute aver-
ages of bytes and packets into and out of each network interface.
(cid:129) Network topology data. Given the sensitive nature of network
topology and device procurement data, we used a static snapshot
of our network encompassing thousands of devices and tens of
thousands of interfaces spread across tens of data centers.
3.2 Deﬁning and identifying failures
When studying failures, it is important to understand what
types of logged events constitute a “failure”. Previous studies have
looked at failures as deﬁned by pre-existing measurement frame-
works such as syslog messages [26], OSPF [25, 28] or IS-IS listen-
ers [19]. These approaches beneﬁt from a consistent deﬁnition of
failure, but tend to be ambiguous when trying to determine whether
a failure had impact or not. Syslog messages in particular can be
spurious with network devices sending multiple notiﬁcations even
though a link is operational. For multiple devices, we observed this
type of behavior after the device was initially deployed and the
router software went into an erroneous state. For some devices, this
effect was severe, with one device sending 250 syslog “link down”
events per hour for 2.5 months (with no impact on applications)
before it was noticed and mitigated.
We mine network event logs collected over a year to extract
events relating to device and link failures. Initially, we extract all
352logged “down” events for network devices and links. This leads us
to deﬁne two types of failures:
Link failures: A link failure occurs when the connection between
two devices (on speciﬁc interfaces) is down. These events are de-
tected by SNMP monitoring on interface state of devices.
Device failures: A device failure occurs when the device is not
functioning for routing/forwarding trafﬁc. These events can be caused
by a variety of factors such as a device being powered down for
maintenance or crashing due to hardware errors.
We refer to each logged event as a “failure” to understand the
occurrence of low level failure events in our network. As a result,
we may observe multiple component notiﬁcations related to a sin-
gle high level failure or a correlated event e.g., a AggS failure re-
sulting in down events for its incident ToR links. We also correlate
failure events with network trafﬁc logs to ﬁlter failures with im-
pact that potentially result in loss of trafﬁc (Section 3.4); we leave
analyzing application performance and availability under network
failures, to future work.
3.3 Cleaning the data
We observed two key inconsistencies in the network event
logs stemming from redundant monitoring servers being deployed.
First, a single element (link or device) may experience multiple
“down” events simultaneously. Second, an element may experience
another down event before the previous down event has been re-
solved. We perform two passes of cleaning over the data to resolve
these inconsistencies. First, multiple down events on the same ele-
ment that start at the same time are grouped together. If they do not
end at the same time, the earlier of their end times is taken. In the
case of down events that occur for an element that is already down,
we group these events together, taking the earliest down time of the
events in the group. For failures that are grouped in this way we
take the earliest end time for the failure. We take the earliest failure