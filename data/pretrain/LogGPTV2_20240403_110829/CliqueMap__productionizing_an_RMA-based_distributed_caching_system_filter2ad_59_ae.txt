tionizeable feature sets. We note significant agreement between our
productionization challenges and those discussed by Facebook [10],
with differences in approach arising from the underlying technolo-
gies involved.
103
18:0018:1018:2018:4018:5018:301x2x3x4x5x6x7x1RMA Command Executor TimestampsNormalized LatencyCliqueMap + 1RMA GET Latencies5%GETs50%GETs95%GETs0150300450600Latency(us)GET-50pGET-99pSET-50pSET-99p5%GETs50%GETs95%GETs02K4K6K8KCPU*s/s32B256B2KB16KB0500Latency(us)GET-50pGET-99pSET-50pSET-99pSIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
A. Singhvi et al.
From Pilaf and other systems [36, 39], we take the key insight to
compose RMA and RPC, as well as specific techniques to do so, such
as self-validation. Elements of our design also resemble RPC-based
systems. For example, when considering the combination of Pony
Express and CliqueMap, SCAR (§6.3) bears similarity to message-
oriented, rather than strictly RMA-oriented lookup strategies, akin
to those in HERD [23]. Likewise, SCAR itself resembles a highly-
specialized RPC [8, 22].
When evaluating solutions for availability and replication, we
opted for quoruming over the primary/backup architecture adopted
by HydraDB [40] and FaRM [18] so that we could take advantage
of preferred backends (§5). To ensure that replication does not lead
to significant overheads, we opted for client-side quoruming rather
than indirection through a server to avoid serialization points (e.g.,
ZAB [21], CR [38]) and inter-replica communication (e.g., ABD [9,
30], Paxos [28] for reads; Hermes [26], CR [37, 38] for writes).
Lastly, we sought to build CliqueMap so that no potential user re-
quired special privilege to operate it, because such privileges impose
undesirable adoption hurdles. This design choice ultimately made
Unreliable Connected/Unreliable Datagram transports, as used by
HERD [23] and FaSST [24], infeasible as sub-components. Instead,
we rely on indirection through Pony Express to avoid binding to any
high-privilege network APIs. Similarly, low default privilege levels
make it difficult to realize predictable CPU and NIC siloing in our
environments, despite the performance advantages demonstrated by
MICA [29].
9 Experience and Conclusions
CliqueMap highlights the importance of complete system design that
focuses on performance, robustness, and efficiency, by deploying
high-performance/low-programmability RMA primitives on criti-
cal performance paths, and highly agile but less-efficient RPCs for
other functionality. This division of labor capitalizes on the needs
of a particular set of critical serving workloads—those with strin-
gent demand for performance in serving paths. At the same time,
it meets the expectations for productionization feature-richness of
RPC based systems. To conclude, we summarize broad takeaways
for the networking and systems-building communities based on our
experiences derived from building CliqueMap.
Leverage RPC, in composition with RMA, to maintain post-
deployment agility. Production services undergo requirement
changes throughout their lifetimes. Ultimately, CliqueMap’s lookup
path is the only path heavily tailored for RMA, and the system is
relatively easy to adapt to changes as a result. Throughout the design,
we embrace the use of RPCs for control and management actions,
and as options for dataplane operations, affording opportunities to
refine the design and support new features: sparing for planned main-
tenance, diverse eviction algorithms, compression, and new mutation
types. Systems maintainers for all-RMA designs tend to find even
trivial changes (e.g., to memory layouts) a major challenge.
Enable multi-language software ecosystems. It is tempting to fo-
cus solely on performance, and hence, low-level languages such as
C and C++. Early in CliqueMap’s lifetime, we even turned away
potential customers rooted outside these languages. In retrospect, a
C++-only approach stunted growth and adoption, as our datacenters
are vivid multilingual environments, grown out of the ease of devel-
opment afforded by RPC. CliqueMap’s language support makes it a
viable option for many thousands of developers at Google. Whereas
our current approach—named pipes to a subprocesses—meets our
performance requirements well, it is not optimal. We believe that
this area is ripe for the research community’s future innovations in
exploring new tradeoffs between maintenance burden, complexity,
efficiency, and performance.
Don’t compromise memory efficiency. We initially envisioned
CliqueMap would provision for peak DRAM usage, to work around
the notorious difficulties of memory registration [6]. As a result,
early potential adopters faced a tradeoff: faster lookups for (perhaps)
higher DRAM usage. Such trade-offs are difficult to analyze, as
DRAM cost isn’t uniform in time or geography. The right call in
one datacenter might be wrong in another. By investing in memory
efficiency while preserving lookup performance, CliqueMap became
significantly more appealing.
Simplify design with self-validating server responses and client
retries. We found the design pattern of combining self-validating
responses with client-side retry greatly simplifying, as clients be-
come resilient to a variety of hazards across all layers of the stack;
self-validation can tolerate RMA operation failure, data races among
competing mutations, backend configuration changes, and even wire
protocol format changes. Through retries at the appropriate abstrac-
tion level, CliqueMap near-seamlessly handles these cases. A notable
drawback of this approach is that GET forward progress is not guar-
anteed; nevertheless, we have found this drawback can be managed
through (mostly-automated) tuning.
Programmable NICs offer advantage through specialization.
Hardware implementations of RMA offer stunning performance
envelopes, but software NICs offer continuous innovation and post-
deployment customization. We could not have deployed Scan-and-
Read (§6.3)—an optimization saving an entire RTT—without an
underlying software NIC, Pony Express. The superior expressivity
and reprogrammability of software NICs gives them a notable edge
over faster-but-inflexible all-hardware designs, and helps bridge
gaps caused by heterogeneous hardware deployments by deploying
hardware-NIC-agnostic protocols. As so-called SmartNICs continue
to emerge, opportunities to optimize them for serving systems will
grow.
We recommend that designers of future infrastructure take
advantage of these guidelines when building systems for hyperscale
datacenter environments: maintain agility, sacrifice neither common-
case performance nor DRAM efficiency, enable customers’ practical
needs, and adapt to the underlying technology landscape. These
tenets underlie CliqueMap’s design, execution, and evolution over
time, and have led to a production-friendly and practically useful
design point.
This work raises no ethical concerns.
Acknowledgments
We would like to thank early reviewers Jeff Mogul, Jason Hsueh,
Jeff Hightower, and Philip Wells. Likewise we thank the anonymous
SIGCOMM reviewers and our shepherd, Nathan Bronson, for pro-
viding valuable feedback. Lastly, we thank the production, serving,
and support teams at Google for their partnership and contributions
to the work—including but not limited to the Pony Express, 1RMA,
Ads, Geo, and Travel teams.
104
CliqueMap: Productionizing an RMA-Based Distributed Caching System
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
References
[1] 2020. Chelsio Terminator 6 NICs. https://www.chelsio.com/terminator-6-asic/.
[2] 2020. Google’s Application Layer Transport Security. https://cloud.google.com/
security/encryption-in-transit/application-layer-transport-security.
[3] 2020. Marvell FastLinQ 41000 Series Ethernet NICs. https://www.marvell.com/
products/ethernet-adapters-and-controllers/41000-ethernet-adapters.html.
[4] 2020. Memcached. http://memcached.org/.
[5] 2020. Nvidia Mellanox Connect-X NICs.
networking/ethernet-adapters/.
https://www.nvidia.com/en-us/
[6] 2020. RDMA Core Userspace Libraries (libibverbs). https://github.com/linux-
rdma/rdma-core.
[7] Marcos K Aguilera, Kimberly Keeton, Stanko Novakovic, and Sharad Singhal.
2019. Designing far memory data structures: Think outside the box. In Proceedings
of the Workshop on Hot Topics in Operating Systems (HotOS’19). 120–126.
[8] Emmanuel Amaro, Zhihong Luo, Amy Ousterhout, Arvind Krishnamurthy, Aurojit
Panda, Sylvia Ratnasamy, and Scott Shenker. 2020. Remote Memory Calls. In
Proceedings of the 19th ACM Workshop on Hot Topics in Networks (HotNets’20).
38–44.
[9] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev. 1995. Sharing memory robustly
in message-passing systems. Journal of the ACM (JACM) 42, 1 (1995), 124–142.
[10] Benjamin Berg, Daniel S. Berger, Sara McAllister, Isaac Grosof, Sathya Gunasekar,
Jimmy Lu, Michael Uhlar, Jim Carrig, Nathan Beckmann, Mor Harchol-Balter,
and Gregory R. Ganger. 2020. The CacheLib Caching Engine: Design and
Experiences at Scale. In 14th USENIX Symposium on Operating Systems Design
and Implementation (OSDI’20). 753–768.
[11] Jeff Bonwick. 1994. The Slab Allocator: An Object-Caching Kernel. In USENIX
Summer 1994 Technical Conference (USTC’94).
[12] Eric Brewer. 2017. Spanner, TrueTime and the CAP Theorem. Technical Report.
https://research.google/pubs/pub45855/
[13] Mike Burrows. 2006. The Chubby lock service for loosely-coupled distributed
systems. In Proceedings of the 7th symposium on Operating systems design and
implementation (OSDI’06). 335–350.
[14] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wal-
lach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. 2008.
Bigtable: A distributed storage system for structured data. ACM Transactions on
Computer Systems (TOCS) 26, 2 (2008), 1–26.
[15] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher
Frost, J. J. Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser,
Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li,
Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan,
Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor,
Ruth Wang, and Dale Woodford. 2013. Spanner: Google’s Globally Distributed
Database. ACM Transactions on Computer Systems (TOCS) 31, 3 (2013), 1–22.
[16] Jeffrey Dean. 2010. Evolution and future directions of large-scale storage and
computation systems at Google. (2010). https://research.google/pubs/pub44877/
[17] Aleksandar Dragojevi´c, Dushyanth Narayanan, Orion Hodson, and Miguel Castro.
2014. FaRM: Fast Remote Memory. In Proceedings of the Eleventh USENIX
Symposium on Networked Systems Design and Implementation (NSDI’14). 401–
414.
[18] Aleksandar Dragojevi´c, Dushyanth Narayanan, Edmund B Nightingale, Matthew
Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No compro-
mises: distributed transactions with consistency, availability, and performance. In
Proceedings of the 25th Symposium on Operating Systems Principles (SOSP’15).
54–70.
[19] David K Gifford. 1979. Weighted voting for replicated data. In Proceedings of the
seventh ACM Symposium on Operating Systems Principles (SOSP’79). 150–162.
[20] Maurice Herlihy, Victor Luchangco, and Mark Moir. 2003. Obstruction-free
synchronization: Double-ended queues as an example. In 23rd International
Conference on Distributed Computing Systems, 2003. Proceedings. 522–529.
[21] Flavio P Junqueira, Benjamin C Reed, and Marco Serafini. 2011. Zab: High-
performance broadcast for primary-backup systems. In 41st International Confer-
ence on Dependable Systems & Networks (DSN’11). 245–256.
[22] Anuj Kalia, Michael Kaminsky, and David Andersen. 2019. Datacenter RPCs
can be General and Fast. In Proceeding of Sixteenth USENIX Symposium on
Networked Systems Design and Implementation. 1–16.
[23] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2014. Using RDMA
efficiently for key-value services. In Proceedings of the 2014 Conference of ACM
SIGCOMM. 295–306.
[24] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. FaSST: Fast, Scal-
able and Simple Distributed Transactions with Two-Sided RDMA Datagram RPCs.
In 12th USENIX Symposium on Operating Systems Design and Implementation
(OSDI’16). 185–201.
[25] Svilen Kanev, Juan Pablo Darago, Kim Hazelwood, Parthasarathy Ranganathan,
Tipp Moseley, Gu-Yeon Wei, and David Brooks. 2015. Profiling a warehouse-
scale computer. In Proceedings of the 42nd Annual International Symposium on
Computer Architecture (ISCA’15). 158–169.
[26] Antonios Katsarakis, Vasilis Gavrielatos, MR Siavash Katebzadeh, Arpit Joshi,
Aleksandar Dragojevic, Boris Grot, and Vijay Nagarajan. 2020. Hermes: a
Fast, Fault-Tolerant and Linearizable Replication Protocol. In Proceedings of
the Twenty-Fifth International Conference on Architectural Support for Program-
ming Languages and Operating Systems (ASPLOS’20). 201–217.
[27] Leslie Lamport. 1994. The temporal logic of actions. ACM Transactions on
Programming Languages and Systems (TOPLAS) 16, 3 (1994), 872–923.
[28] Leslie Lamport. 1998. The Part-Time Parliament. ACM Transactions on Computer
Systems (TOCS) 16, 2 (1998), 133–169.
[29] Hyeontaek Lim, Dongsu Han, David G Andersen, and Michael Kaminsky. 2014.
MICA: A holistic approach to fast in-memory key-value storage. In 11th USENIX
Symposium on Networked Systems Design and Implementation (NSDI’14). 429–
444.
[30] Nancy A Lynch and Alexander A Shvartsman. 1997. Robust emulation of shared
memory using dynamic quorum-acknowledged broadcasts. In Proceedings of
IEEE 27th International Symposium on Fault Tolerant Computing. 272–281.
[31] Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer,
Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C. Evans, Steve
Gribble, Nicholas Kidd, Roman Kononov, Gautam Kumar, Carl Mauer, Emily
Musick, Lena Olson, Erik Rubow, Michael Ryan, Kevin Springborn, Paul Turner,
Valas Valancius, Xi Wang, and Amin Vahdat. 2019. Snap: A Microkernel Ap-
proach to Host Networking. In Proceedings of the 27th ACM Symposium on
Operating Systems Principles (SOSP’19). 399–413.
[32] Nimrod Megiddo and Dharmendra S. Modha. 2003. ARC: A Self-Tuning, Low
Overhead Replacement Cache. In Proceedings of the 2nd USENIX Conference on
File and Storage Technologies (FAST ’03). 115–130.
[33] Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided
RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store. In 2013 USENIX
Annual Technical Conference (ATC’13). 103–114.
[34] Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica Wong-
Chan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant Chandra, Rob
Cauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L. Sabato, Joel Scher-
pelz, and Amin Vahdat. 2020. 1RMA: Re-Envisioning Remote Memory Access
for Multi-Tenant Datacenters. In Proceedings of the Annual Conference of the
ACM Special Interest Group on Data Communication on the Applications, Tech-
nologies, Architectures, and Protocols for Computer Communication (SIGCOMM
’20). 708–721.
[35] Patrick Stuedi, Animesh Trivedi, and Bernard Metzler. 2012. Wimpy nodes with
10GbE: leveraging one-sided operations in soft-RDMA to boost memcached. In
In 2012 USENIX Annual Technical Conference (ATC’12). 347–353.
[36] Maomeng Su, Mingxing Zhang, Kang Chen, Zhenyu Guo, and Yongwei Wu. 2017.
RFP: When RPC is Faster than Server-Bypass with RDMA. In Proceedings of the
Twelfth European Conference on Computer Systems (EuroSys ’17). 1–15.
[37] Jeff Terrace and Michael J Freedman. 2009. Object Storage on CRAQ: High-
Throughput Chain Replication for Read-Mostly Workloads. In 2009 USENIX
Annual Technical Conference. 1–16.
[38] Robbert van Renesse and Fred B. Schneider. 2004. Chain Replication for Support-
ing High Throughput and Availability. In Proceedings of the 6th Conference on
Symposium on Operating Systems Design and Implementation (OSDI’04). 7.
[39] Yandong Wang, Xiaoqiao Meng, Li Zhang, and Jian Tan. 2014. C-hint: An
effective and reliable cache management for rdma-accelerated key-value stores. In
Proceedings of the ACM Symposium on Cloud Computing (SoCC’14). 1–13.
[40] Yandong Wang, Li Zhang, Jian Tan, Min Li, Yuqing Gao, Xavier Guerin, Xiaoqiao
Meng, and Shicong Meng. 2015. HydraDB: a resilient RDMA-driven key-value
middleware for in-memory cluster computing. In Proceedings of the International
Conference for High Performance Computing, Networking, Storage and Analysis
(SC’15). 1–11.
[41] Juncheng Yang, Yao Yue, and K. V. Rashmi. 2020. A large scale analysis of
hundreds of in-memory cache clusters at Twitter. In 14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 20). 191–208.
105