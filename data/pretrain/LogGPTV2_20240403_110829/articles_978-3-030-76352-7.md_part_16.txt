directly. Because of this, we also consider the scenario where we observe Ft only
indirectly through samples, i.e. at each time t a set Y t ={Yt1,...,Ytnt} of nt iid
samples from Ft is observed. We can differentiate three real-world use cases:
Deep Distributional Time Series Models 101
1. Monitoring services with frequent requests: This corresponds to the
setting described in Sect.2, where for each time interval (e.g. each minute),
the number of measurements nt is large, e.g. on the order of 105 or more.
The underlying distributions Ft can then be estimated with a high enough
precision for us to consider that they are directly observed. We will also refer
to this as the asymptotic settting, since it corresponds nt →+∞.
2. High-frequency time series: This corresponds to the setting where the
temporal resolution of the original time series is higher than the scale at
which meaningful temporal variation occurs, e.g. nt = 60 when aggregating
from seconds to minutes.
3. Low-frequency time series:Wealsoconsiderthent =1setting,whereour
model reduces to a classical probabilistic time series model over real-values
observations. Even though this is not the setting for which our approach was
originally designed, we will show that it still yields competitive results.
Ourmodelhandlesallthreesettings.Wewillrefertothelasttwoscenariosas
thefinite nt scenarios,incontrasttothefirstone.Intheasymptoticsetting,the
distributions Ft are observed directly. In the finite nt settings, we only observe
samples from them. Therefore, we need to be able to assess the likelihood of Ft
for the asymptotic regime, and the likelihood of Y t, where Ft is marginalized
out, for the finite nt regimes.
3.2 Probabilistic Model on Binned Densities
A common approach to modeling distributional data is to represent the func-
tions of interest (e.g. the CDFs or PDFs) by a point in a carefully chosen finite-
dimensional space. In this work, we will consider the space of piece-wise lin-
ear functions to approximate the CDFs, or equivalently, the space of binned
(piecewise-constant) distributions to approximate the PDFs. Specifically, we
chosetoapproximateeachCDFFt byapiece-wiselinearfunctionFt,composed
ofdlinearpieces.Agivenfunctioninthisclassisspecifiedbytwosetsofparame-
ters:thestartandendpointsoflinearpieces(theknot positions),andtheslopes
in each segment. While it is possible to adapt the knot positions dynamically
(as done in [8]), we keep the knot positions fixed and only model the temporal
evolutionoftheslopeswithineachsegment.Weemphasizethefactthatonecan
approximateanyFt arbitrarilywellasthegridbecomesfiner(dbecomeslarger).
Therefore, we will assume that the Ft themselves are piece-wise linear.
We divide Y into d bins using the grid y =a <...<ad =y max. Suppose
min 0
thattheCDFFt ispiece-wiselinear,interpolatingthepoints(ak,Ft(ak))k=0,...,d.
Then the probability of falling into one of the bins [ak−1,ak) is ptk = Ft(ak)−
Ft(ak−1).GivenFt,asufficientstatisticofthesetofobservationsY t isthecount
vectormt =(mt1,...,mtd),wheremtk denotesthenumberofelementsofY t that
fell into the bin [ak−1,ak). It follows a Multinomial distribution with nt trials
and outcome probabilities pt.
Specifying a distribution on the d dimensional probability vector pt =
(pt1,...,ptd) entails a distribution over the piece-wise linear CDFs Ft. We model
102 F. Ayed et al.
this distribution over probability vectors using a Dirichlet distribution, i.e.
pt ∼ Dir(αt), where αt ∈ Rd denotes the concentration parameter whose tem-
+
poral evolution is modeled using an RNN. With this choice of prior, pt can be
marginalized out and we have a closed form probability mass function for the
observationsmt.Moreprecisely,mt followsaDirichlet-Multinomialdistribution
with nt number of trials and concentration vector αt.
To summarize, given αt, the likelihood of the observation is:
L t =L(pt;αt)=Dir(pt;αt) (Asymptotic setting)
L t =L(mt;nt,αt)=Dir-Mult(mt;nt,αt), (Finite nt setting)
where as explained previously in the asymptotic regime we suppose that we
directly observe pt which is equal to the normalized counts n1 tmt.
3.3 RNN Temporal Dynamics Model
In both settings, the temporal evolution of the data is described through the
time-varyingparameterαt,anditisthisdynamicbehaviorthatweaimtolearn.
In order to do so, we will use an autoregressive LSTM-based recurrent neural
network, whose architecture follows the one described in [23].
Recurrent neural networks (RNNs) form a class of artificial neural networks
designedtohandlesequentialdata.OneofthekeybenefitsofRNNsistheirabil-
ity to handle sequences of varying lengths. RNNs sequentially update a hidden
state h: at every time step t, the next hidden state ht is computed by using the
previous ht−1 and the next input (the next observation yt and other covariates).
A crucial detail is that the weights of the network are shared across time steps,
which makes the RNN recurrent, and capable of handling sequences of varying
length.Thiscompactrepresentationmakesthemamenabletostreamingsettings.
Here,wemainlyrelyonlongshort-termmemorynetworks(LSTM),thearguably
most popular subclass of RNNs.
Let z 1:T be the sequence of observations, either p 1:T or m 1:T depending on
the setting. Denote φ the parameters of the RNN model. Given a horizon τ,
the aim is to predict the probability distribution of futuretrajectorieszT+1:T+τ,
with the potential use of observed covariates x 1:T+τ .
The parameter αt is function of the output ht of an autoregressive recurrent
neural network with
ht =rφ(ht−1,zt−1,xt) (1)
αt =θφ(ht) (2)
whererφisamulti-layerrecurrentneuralnetworkwithLSTMcells.Themodelis
autoregressiveandrecurrentinthesensethatitusesrespectivelytheobservation
at the last time step zt−1 and the previous hidden state ht−1 as input. Then a
layer θφ projects the output ht to Rd +, the domain of αt. The parameters φ of
T
the model are chosen to minimize the negative log likelihood L=− log(L t).
t=1
Finally, we note that when dealing with anomaly detection we only require a
time horizon τ =1.
Deep Distributional Time Series Models 103
3.4 Anomaly Detection with Level Sets
OnceweforecastαT+1,wecanassesswhethertheobservationzT+1isapotential
anomaly. Indeed, given αT+1, we know the distribution of the random variable
ZT+1, of which zT+1 should be a sample if no anomaly happened. Therefore,
we can compute the threshold ηT+1 such that the probability of L(ZT+1) being
smaller than ηT+1 is smaller than a given level ε (for example ε = 5%). Hence,
theobservationzT+1willbeconsideredanomalousifitslikelihoodissmallerthan
ηT+1.TheremainingdifficultyistocomputeηT+1.Whenthenumberofpossible
outcomes for ZT+1 is relatively small, this can be done exactly by computing
the likelihoods of all outcomes. Otherwise, we will use a Monte Carlo method,
following an idea that goes back to [13]: if we consider the univariate random
variable defined as L T+1(ZT+1), we remark that ηT+1 can be interpreted as the
ε quantile of that distribution.
Forthehighfrequencysetting,weuseatwo-stageapproach,describedonthe
followingillustrativeexample.Supposethatweobserveaminutefrequencytime
series,andweareinterestedinhourlyaggregation.Fromtheforecastingmodule,
we predict αT+1 and hence the distribution of the observations for the hour to
come. In the first stage, before the hour is over, we assess every minute whether
the current observation is anomalous. Once the hour is over, we assess whether
the past 60 observations jointly constitute a collective anomaly. If we want to
detect collective anomalies that are shorter, we can add an intermediate stage.
Finally, as explained in the experiment section, we will need to give an anomaly
scoretoeachtime point toevaluate themodels. Thescoreusedisthelogarithm
of the p-value, which is the smallest ε for which a given point is considered as
an anomaly. For the two-stage strategy, we simply add the two scores.
4 Experiments
Our implementation2 is based on GluonTS [1] which in turn is based on
MXNet [5]. We learn a global model (across all metrics) which takes roughly
3mins per 100 metrics. For such models, we do not have to re-train often, so
we may disregard the training time for the production scenario. Inference scales
embarrassingly parallel. Scoring of a single data point take 1ms for 1 minutely
aggregated data (note that we do not perform the costly Monte-Carlo estimates
ateverytimepoint).Wecanlimitmemoryconsumptionofthemodelstoafixed
size of 80kb per metric. For all the experiments we learn the parameters of the
model on the learning time range {0,...,T}, and we perform anomaly detection
on the detection time range {T +1,...,T +D}. We consider two different grids
todefinethebins.Thefirstoneisthesimpleregulargrid,ak =k/d.Thesecond
grid is obtained using d+1 regularly spaced quantile levels of the marginal dis-
tribution. Depending on the problem, the regular or the quantiles grid can be
better.
2 The code is available at https://github.com/awslabs/gluon-ts/tree/distribution
anomaly detection/distribution anomaly detection.
104 F. Ayed et al.
4.1 Evaluation Metric
For comparing the different models we will use the area under the receiver oper-
atingcharacteristiccurve(ROC-AUC).Itisametriccommonlyusedforclassifi-
cationproblemstocomparealgorithmswhichperformancesdependonselecting
a threshold. This measure quantifies how much a model is able to distinguish
between the two classes. It takes values between 0 and 1, the higher the better.
Thisscoreisindependentofthethresholdchosensinceitonlyconsiderstherank-
ing of the observations by the model in terms of how much abnormal it looks.
Therefore it allows to quantify the maximum potential of a method.
4.2 Synthetic Data
Letμt =sin(2 Pπt )andσt =1,whereP =24isaperiodlengthandt ∼N(0,0.1)
are iid noise. We will consider the two following dynamics:
1. DS1: Ft =N(μt+t,σt)
2. DS2: Ft =N(μt,σt+t)
We consider T = 1500 learning time points and a detection time horizon of
D =2000. In the detection time range, we add an anomaly with probability 3%
at each location independently. For each experiment, we use one of two types of
anomalies: a sudden distributional shift (by adding 1 to μt), or a distributional
collapse (removing 1/2 to the standard deviation σt). We therefore get four
different settings, we will denote them respectively DS1 μ, DS1 σ, DS2 μ and
DS2 σ.
We set the threshold for anomaly detection to be 95%. For the Monte Carlo
approximation,wetakeM =1000samplesfromthepredictivedistributionofthe
loglikelihoods.Anobservationcanthenbeconsideredanomalousintwocases.In
thefirstcase,thegeneratednoisetermtfallsoutsideofa95%confidenceinterval
of the N(0,0.1): these are false positives, and if the model perfectly captures
the generating process, this should happen 5% of the times on average. The
second case corresponds to the anomalies that are artificially added, considered
as malfunctions, or true positives.
Asymptotic Setting. This setting mimics popular services on AWS; we have
accesstoagridofathousandquantilesofFt ateachtimestep.Wetakearegular
grid of d=30 bins. We report the results in Table1.
Finite nt Setting. In this setting, we observe nt =60 samples from evey distri-
bution Ft (hourly aggregation). We take a quantile grid of d=10 bins. In most
practical settings, we are able to take d much larger since we can make use of
multipletimeseriessimultaneously,eventhoughtheyrepresentdifferentmetrics
(CPU usage, Latency, Number of connected users, etc.).
Wecomparetheperformanceofourapproachtothestandardoneofmonitor-
ing an aggregated statistic. We use two state-of-the-art open source algorithms,
namely Luminol and TwitterAD, as competitors. These algorithms are run on
the appropriate aggregated statistics (empirical mean and standard deviation
Deep Distributional Time Series Models 105
Table 1. Anomaly detection for synthetic datasets in the asymptotic setting. Results
areexpressedinpercent.Whenthenameofthedatasetisfollowedbyμ(resp.σ),itcor-
respondstodistributionalshiftsmalfunctions(resp.distributionalcollapse).Otherwise,
no malfunctions are introduced. We expect the FPR to be 5% in all cases.
False positive rate Recall
DS1 5.73±0.61 –
DS1 μ 5.60±0.99 99.7±0.67
DS2 σ 5.43±0.11 100
DS2 4.96±1.0 –
DS2 μ 5.15±1.5 100
DS2 σ 4.98±0.72 99.8±0.5
Table 2.Comparativeevaluationofanomalydetectionmethodsonthesynthetichigh
frequencydata.Whenthenameofthedatasetisfollowedbyμ(resp.σ),itcorresponds
to distributional shifts malfunctions (resp. distributional collapse).
Distribution TwitterAD Luminol
DS1 μ 0.9928 0.9998 0.9400
DS1 σ 0.9864 0.5010 0.9691
DS2 μ 0.9973 0.9999 0.9596
DS2 σ 0.9797 0.4990 0.9456
per hour). We note that in a practical setting, we don’t know which statistics is
most appropriate to monitor. The results are reported in Table2.
4.3 Yahoo Webscope Dataset
Yahoo Webscope is an open dataset often used as a benchmark for anomaly
detection since it is labeled. It is composed of 367 time series, varying in length
from 700 to 1700 observations. Some of these time series come from real traffic
to Yahoo services and some are synthetic. The dataset is divided into 4 sub-
benchmarks,fromA1toA4.Thetimefrequencyofallthetimeseriesisonehour.
Since the frequency is relatively low, and since there are no collective anomalies
in this dataset, we take nt = 1, which corresponds to the classical anomaly
detection setting. We report the results of [19] to compare the performance of
our approach with the state of the art anomaly detection algorithms. We report
the results per sub-benchmark, since they contain different patterns. We use
40% of each time series for training. We learn a single model for all the series
of a same sub-benchmark, which means that we train the model on all the time
series simultaneously. The results are given in Table3. Here, since nt = 1, the
total number of possible outcomes is equal to d = 100. Therefore, we do not
need Monte Carlo estimates.
106 F. Ayed et al.
Table 3. Comparative evaluation of state-of-the-art anomaly detection methods on
the Yahoo Webscope dataset. Average AUC per benchmark.
Benchmark iForest OCSVM LOF PCA TwitterAD DeepAnT FuseAD Distribution
A1 0.8888 0.8159 0.9037 0.8363 0.8239 0.8976 0.9471 0.9435
A2 0.6620 0.6172 0.9011 0.9234 0.5000 0.9614 0.9993 0.9999
A3 0.6279 0.5972 0.6405 0.6278 0.6176 0.9283 0.9987 0.9988
A4 0.6327 0.6036 0.6403 0.6100 0.6534 0.8597 0.9657 0.9701
4.4 AWS Data
Finally, we consider three benchmark datasets of high frequency time series,
collected from AWS. These datasets are often used internally at Amazon to
comparemodels.ThebenchmarkB1hasa1mintimefrequency,itiscomposed
of 55 time series. The benchmarks B2 and B3 have a 5 min time frequency.
Theyarecomposedof100timeserieseach.Alldatasetsarecomposedofdifferent
metrics,amongthemCPUusage,latency,numberofusers,etc.Eachtimeseries
of all three benchmarks have approximately 17000 time points. We use 60% of
thetimerangefortrainingandtheremainderfordetection.Wesetd=100and
aggregatealltimeseriestoa30minfrequency,sont =30forB1andnt =6for
B2 and B3. However the quality of the labeling is heterogeneous, B1 being the
most reliable one. We find that the anomalies identified by our method are false
positives under the labels but should probably be counted as true positives. We
perform a two stage anomaly detection, the first stage gives scores for the single
observations, the second for the collection of observations within half-hours. We
again compare to Luminol and TwitterAD. The results are reported in Table4
which show the dominance of our method on this data set.
Table4.ComparativeevaluationofanomalydetectionmethodsonAWSdata.Average
ROC-AUC per benchmark.
Distribution TwitterAD Luminol
B1 0.8183 0.7134 0.6467
B2 0.7534 0.5895 0.5804
B3 0.6860 0.5889 0.5860
5 Related Work
Anomaly detection is a rich field with many applications and solutions available
(see for example [2,11,12,21]). We focus our discussion of related work on unsu-
pervised anomaly detection models. A first approach for anomaly detection are
the so called outlier detection methods, which quantify how much every point
Deep Distributional Time Series Models 107