### 5.6 Cost of Launch Strategies

Recall that the cost of a launch strategy from Section 3 is given by \( CS = a \times P(\text{atype}) \times T_d(v, a) \). To calculate this cost, we need \( T_d(v, a) \), which is the time taken to detect co-location with \( a \) attackers and \( v \) victims. Figure 22 shows the average time taken to complete launching attacker instances and perform co-residency detection for each run configuration. The measured co-residency detection is the parallelized version discussed in Section 4.2 and includes the time taken to detect co-residency within each tenant account. Therefore, the time to detect co-location is an upper bound for a realistic and highly optimized co-residency detection mechanism.

We calculated the cost of executing each launch strategy under the three public clouds, as summarized in Figure 23. Note that we only considered the cost incurred by the compute instances because the cost for other resources such as network and storage was insignificant. Additionally, EC2 bills every hour even if an instance runs less than an hour [16], whereas GCE and Azure charge per minute of instance activity. This difference is accounted for in our cost calculation. Overall, the maximum cost we incurred was about $8 for running 30 VMs for 4 hours and 25 minutes on Azure, and the minimum cost was 14 cents on GCE for running 10 VMs for 17 minutes. We incurred the highest costs for all launch strategies on Azure due to its higher hourly rates and partly due to longer test durations resulting from our co-residency detection methodology.

### 5.7 Summary of Placement Vulnerabilities

In this section, we return to the secure reference placement policy introduced in Section 3 and use it to identify placement vulnerabilities across all three clouds. Recall that the probability of at least one pair of co-residency under this random placement policy is given by \( \Pr [ E_{v,a} > 0 ] = 1 - (1 - \frac{v}{N})^a \), where \( E_{v,a} \) is the random variable denoting the number of co-locations observed when placing \( a \) attacker VMs among \( N = 1000 \) total machines, where \( v \) machines are already picked for the \( v \) victim VMs. First, we evaluate this probability for various run configurations that we experimented with in the public clouds. The probabilities are shown in Figure 24.

A launch strategy in a cloud implies a placement vulnerability in that cloud’s placement policy if its normalized success rate is greater than 1. The normalized success rate of the strategy is the ratio of the chance of co-location under that launch strategy to the probability of co-location in the reference policy (\( \Pr [ E_{v,a} > 0 ] \)). Below is a list of selected launch strategies that escalate to placement vulnerabilities using our reference policy, along with their normalized success rates:

- **S1 & S2**: In Azure, launch ten attacker VMs closely after the victim VMs are launched (1.0/0.10). In EC2 and GCE, if there are known victims in any of the smaller datacenters, launch at least ten attacker VMs with a non-zero delay (1.0/0.10).
- **S3**: In all three clouds, launch 30 attacker instances, either with no delay (Azure) or one-hour delay (EC2, GCE) from victim launch, to get co-located with one of the 30 victim instances (1.00/0.60).
- **S4(i)**: In Amazon EC2, launch 20 attacker VMs with a delay of 5 minutes or more after the victims are launched (0.88/0.33).
- **S4(ii)**: The optimal delay between victim and attacker VM launches is around 4 hours for a 20x20 run (1.00/0.33).
- **S5**: In Amazon EC2, launch the attacker VMs with a 1-hour delay after the victim VMs are launched, where the time of day falls in the early morning, i.e., 02:00 to 10:00hrs PST (0.89/0.60).

**Cost Benefit Analysis:**
Next, we quantify the cost benefit of each of these strategies over the reference policy. As the success rate of any launch strategy on a vulnerable placement policy is greater than what is possible in the reference policy, we need more attacker instances in the reference policy to achieve the same success rate. We calculate this number of attacker instances \( \hat{a} \) using: \( \hat{a} = \frac{\ln(1 - S_{v,a})}{\ln(1 - \frac{v}{N})} \), where \( S_{v,a} \) is the success rate of a strategy with a run configuration of \( v \times a \). The results are presented in Figure 25. The best strategies, S1 and S2, on all three cloud providers are $114 cheaper than what is possible in the reference policy.

These metrics enable evaluating and comparing various launch strategies and their efficacy on different placement policies, both in terms of robust placements and attack costs. For example, although the normalized success rate of S3 is lower than S4, it has a higher cost benefit for the attacker.

### 5.8 Limitations

Although we exhaustively experimented with a variety of placement variables, the results have limitations. One major limitation of this study is the number of placement variables and the set of values for the variables that we used to experiment. For example, we limited our experiments to one instance type, one availability zone per region, and used only one account for the victim VMs. Although different instance types may exhibit different placement behavior, the presented results still hold strong for the chosen instance type. The only caveat that may affect the results is if the placement policy uses user account ID for VM placement decisions. Since we experimented with only one victim account (separate from the designated attacker account) across all providers, these results, in the worst case, may have captured the placement behavior of an unlucky victim account that was subject to similar placement decisions (and hence co-resident) as that of the VMs from the designated attacker account.

Even though we ran at least 190 runs per cloud provider over a period of 3 months to increase the statistical significance of our results, we were still limited to at most 9 runs per run configuration (with 3 runs per time of day). These limitations have only minor bearing on the results presented, if any, and the reported results are significant and impactful for cloud computing security research.

### 6 Related Work

**VM Placement Vulnerability Studies:**
Ristenpart et al. [29] first studied the placement vulnerability in public clouds, showing that a malicious cloud tenant may place one of his VMs on the same machine as a target VM with high probability. Their study was followed by Xu et al. [33] and further extended by Herzberg et al. [25]. However, the results of these studies have been outdated by recent developments in cloud technologies, which is the main motivation for our work.

Concurrent with our work, Xu et al. [34] conducted a systematic measurement study of co-resident threats in Amazon EC2. Their focus, however, is on in-depth evaluation of co-residency detection using network route traces and quantification of co-residence threats on older generation instances with EC2's classic networking (prior to Amazon VPC). In contrast, we study placement vulnerabilities in the context of VPC on EC2, as well as on Azure and GCE. The two studies are mostly complementary and strengthen the arguments made by each other.

New VM placement policies to defend against placement attacks have been studied by Han et al. [23, 24] and Azar et al. [18]. It is unclear, however, whether their proposed policies work against the performance and reliability goals of public cloud providers.

**Co-Residency Detection Techniques:**
Techniques for co-residency detection have been studied in various contexts. We categorize these techniques into one of the two classes: side-channel approaches to detecting co-residency with uncooperative VMs and covert-channel approaches to detecting co-residency with cooperative VMs.

**Side-Channels:**
Side-channels allow one party to exfiltrate secret information from another; therefore, these approaches may be adapted in practical placement attack scenarios with targets not controlled by the attackers. Network round-trip timing side-channel was used by Ristenpart et al. [29] to detect co-residency. Zhang et al. [36] developed a system called HomeAlone to enable VMs to detect third-party VMs using timing side-channels in the last level caches. Bates et al. [19] proposed a side-channel for co-residency detection by causing network traffic congestion in the host NICs from attacker-controlled VMs; the interference of target VM’s performance, if the two VMs are co-resident, should be detectable by remote clients. Kohno et al. [27] explored techniques to fingerprint remote machines using timestamps in TCP or ICMP-based network probes, although their approach was not designed for co-residency detection. However, none of these approaches works effectively in modern cloud infrastructures.

**Covert-Channels:**
Covert-channels on shared hardware components can be used for co-residency detection when the pair of VMs are cooperative. Coarse-grained covert-channels in CPU caches and hard disk drives were used in Ristenpart et al. [29] for co-residency confirmation. Xu et al. [33] established covert-channels in shared last level caches between two colluding VMs in the public clouds. Wu et al. [32] exploited memory bus as a covert-channel on modern x86 processors, in which the sender issues atomic operations on memory blocks spanning multiple cache lines to cause memory bus locking or similar effects on recent processors. However, covert-channels proposed in the latter two studies were not designed for co-residency detection, while those developed in our work are tuned for such purposes.

### 7 Conclusion and Future Work

Multi-tenancy in public clouds enables co-residency attacks. In this paper, we revisited the problem of placement—can an attacker achieve co-location?—in modern public clouds. We find that while past techniques for verifying co-location no longer work, insufficient performance isolation in hardware still allows detection of co-location. Furthermore, we show that in the three popular cloud providers (EC2, GCE, and Azure), achieving co-location is surprisingly simple and cheap. It is even simpler and costs nothing to achieve co-location in some PaaS clouds. Our results demonstrate that even though cloud providers have massive datacenters with numerous physical servers, the chances of co-location are far higher than expected. More work is needed to achieve a better balance of efficiency and security using smarter co-location-aware placement policies.

### Acknowledgments

This work was funded by the National Science Foundation under grants CNS-1330308, CNS-1546033, and CNS-1065134. Swift has a significant financial interest in Microsoft Corp.

### References

[References listed here as in the original text]

---

This revised version aims to provide a clearer, more coherent, and professional presentation of the content.