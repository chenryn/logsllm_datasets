siﬁes only 40% of ﬂows from Comcast in LAX to Cogent as limited
by self-induced congestion in Jan-Feb, while this number is about
75% in Mar-Apr. The numbers for Verizon are 40% and nearly 90%
for the same combination of ISP and location.
In contrast, we see little difference in the fraction of ﬂows from
Cox to Cogent and from all ISPs to Level3 that we classify as ex-
periencing self-induced congestion in both time frames. For exam-
ple, our classiﬁer classiﬁes about 80-90% of ﬂows in both time
frames as experiencing self-induced congestion and the rest as ex-
ternal(Figure 7b).
For Level3 to all access ISPs and Cogent to Cox, there is a
small difference in the fraction of ﬂows classiﬁed as experiencing
self-induced congestion in the two timeframes: for example, in Fig-
ure 7c, we classify about 70% of Cox ﬂows to Level3 as experienc-
ing self-induced congestion in Jan-Feb, but closer to 80% in Mar-
Apr. We expect that this difference is because we use peak-hour
data in Jan-Feb, and off-peak hour data in Mar-Apr to minimize a
potential source of error in our labeling. Even under normal circum-
stances, we expect more variability in throughput tests during peak
hours, and therefore more tests to be affected by external congestion
during those hours than during off-peak hours.
Figure 7 also shows the effect of the congestion threshold we use
for the model. Higher values of the threshold mean that the crite-
rion for estimating self-induced congestion is stricter, and we there-
fore expect to see fewer self-induced congestion events at higher
thresholds. The fraction of ﬂows classiﬁed as experiencing self-
induced congestion drops as the threshold goes up; for example,
for the Comcast/LAX/Cogent combination, the fraction during Jan-
Feb goes down from 40% to slightly less than 30% as the threshold
increases from 0.7 to 0.9. Changing the threshold, however, does
not qualitatively alter the trend of the results.
5.2 Comparing throughput of ﬂows in the two
classes
We validate our classiﬁcation using another insight. In the general
case, when there is no sustained congestion affecting all the mea-
sured ﬂows (i.e., when episodes that cause externally limited ﬂows
do occur, they are isolated and do not affect all ﬂows uniformly),
we expect that externally limited ﬂows would see lower through-
put than self-limited ﬂows; by deﬁnition, externally limited ﬂows
do not attain access link capacity. However, when there is sustained
congestion that affects all measured ﬂows (such as congestion at
TCP Congestion Signatures
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
25
20
15
10
5
0
25
20
15
10
5
0
Jan-Feb Self
Mar-Apr Self
Jan-Feb External
Mar-Apr External
Comcast
TimeWarner
Verizon
Cox
(a) Cogent in LAX and LGA.
Jan-Feb Self
Mar-Apr Self
Jan-Feb External
Mar-Apr External
Comcast
TimeWarner
Verizon
Cox
(b) Level3 in ATL.
Figure 8: Comparing the performance of classiﬁed ﬂows before
and after Dispute2014 resolution. We see that there is no differ-
ence between the throughput of the two classes during a conges-
tion event (Cogent in LAX and LGA, for all ISPs except Cox),
and a signiﬁcant difference when there is no congestion event
(Cogent/Cox, and all ISPs in Level3).
an interconnection point), we expect that the throughput of ﬂows
experiencing both self-induced and external congestion would be
similar. This is because all ﬂows traverse the congested intercon-
nect. Flows through low capacity access links can still self-induce
congestion, while ﬂows through larger access links will not, how-
ever the throughputs of ﬂows in either class will be roughly similar.
We illustrate this with a simple example. Consider that a ﬂow
traversing a congested interconnect can achieve X Mbps. For an ac-
cess link with capacity Y > X, the ﬂow will be externally limited
with throughput X. For an access link with capacity Z < X or Z ≈
X, the ﬂow can be access limited with a throughput of Z which is
less than or equal to X. So if there is a congested interconnect that
many ﬂows traverse, then the throughput those ﬂows achieve will be
close irrespective of whether they experience self-induced or exter-
nal congestion. If on the other hand there is no sustained congestion
at interconnects, the distribution of self-limited throughputs should
follow the distribution of access link capacities. Assuming that the
self-limited and externally-limited ﬂows sample the same popula-
tion at random, the distribution of externally-limited throughputs
should have a lower mean.
Figure 8 shows the median throughput of ﬂows classiﬁed as self-
induced and externally congested in both the January-February and
the March-April time frames for Cogent and Level3. In January-
February in Cogent, both sets of ﬂows have very similar through-
puts for Comcast, Time Warner and Verizon (Figure 8a). On the
other hand, Comcast, Time Warner, and Verizon ﬂows in March-
April that were classiﬁed as self-limited exhibit higher throughput
than ﬂows constrained by external congestion. As expected, Cox
does not show such a difference between Jan-Feb and March-April.
Flows classiﬁed as experiencing self-induced congestion had higher
throughput than externally limited ﬂows in both timeframes. Fig-
ure 8b shows that in Level3 in Atlanta, which did not experience a
congestion event in that time frame, there is a consistent difference
between the two classes of ﬂows.
5.3 How good is our testbed training data?
Since we built our classiﬁcation model using testbed data, a natural
question to ask is how sensitive is our classiﬁer is to the testbed
data? To answer this question, we rebuild the model using data from
the Dispute2014 dataset, and test it on itself. More precisely, we
split the Dispute2014 dataset into two, use one portion to rebuild the
decision tree model, and test the model against the second portion.
If our classiﬁer is robust and not sensitive to the testbed data, we we
would expect similar classiﬁcation of the congestion events using
either the model from the controlled experiments, or the new model
built using the Dispute2014 dataset.
Our new model uses 20% of the samples of Dispute2014 data
except the location and ISP that we are testing. For example, to
test Comcast users to Cogent servers in LAX, we build the model
using 20% of Dispute2014, except that particular combination. Fig-
ure 9 shows the result of the classiﬁer using this model. That the
classiﬁcation of congestion—the percentage of ﬂows classiﬁed as
self-induced congestion—follows the same trend as the classiﬁca-
tion that uses the testbed data to build the model (Figure 7). For
example, for the Comcast/LAX/Cogent combination, the fraction
of self-induced congestion is about 15% and 55% in Jan-Feb and
Mar-Apr in Figure 9 while it is about 30% and 60% in Figure 7c.
In general, the new model is more conservative in classifying self-
limited ﬂows than the testbed model, but is qualitatively consistent
with the testbed model. This consistency shows that our model is
robust to the data used to build it, and also that the testbed data also
provides data approximating to the real-world.
5.4 How well does our model perform on the
TSLP2017 dataset?
The TSLP2017 dataset(§ 4.2) contains data from tests conducted
between a Comcast access network in Massachusetts and a TATA
server in New York. We recap our labeling criteria: since the user
has a service plan rate of 25 Mbps and a base latency to the M-Lab
server of about 18 ms we label NDT tests that have a throughput
of less than 15 Mbps and a minimum latency of 30 ms or higher
as limited by external congestion. We mark tests with throughput
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
Jan-Feb
Mar-Apr
Cogent (LAX)
Cogent (LGA)
Level3 (ATL)
r
n
r
e
V e
r i z
n
o
x
C o
s t
a
C o m c
T i m e W a
r
n
r
e
V e
r i z
n
o
x
C o
s t
a
C o m c
T i m e W a
r
n
r
e
V e
r i z
n
o
x
C o
n
o
i
t
s
e
g
n
o
c
d
e
c
u
d
n
i
-
f
l
e
s
%
1.0
0.8
0.6
0.4
0.2
C o m c
T i m e W a
s t
a
Figure 9: Detection on M-Lab data using a model built using
the Dispute2014 dataset. The results are similar to the results in
Figure 7c, which uses a model built using the testbed data.
exceeding 20 Mbps and minimum latency less than 20 ms as self-
induced. Using our labeling criteria, we were able to obtain 2573
cases of access link congestion and 20 cases of external congestion
over the course of our measurement period. Of these, our testbed
model accurately classiﬁed more than 99% of self-induced conges-
tion events and between 75% and 85% of external congestion events
depending on the parameters used to build the classiﬁcation model.
The lower accuracy corresponds to using lower congestion thresh-
olds to build the the model (i.e., using a congestion threshold of
0.7 and 0.8 in the testbed data corresponded to an accuracy of 75%,
while 0.9 corresponded to an accuracy of 85%). We also tested the
TSLP2017 dataset using the model built using the M-Lab data de-
scribed in Section 5.3. Our results were very similar for detecting
self-induced congestion—more than 90%, while we were able to
get 100% accuracy for external congestion.
The buffering that we observed in this experiment—both in the
access link and the peering link—are small; about 15-20ms. Even
with such a small buffer, which is essentially the worst case for our
model due to its reliance on the shaping properties of buffers, our
model performs accurately, furthering our conﬁdence in the princi-
ples underlying the model.
6 LIMITATIONS
Our proposed method has limitations, both in the model, and the
veriﬁcation.
• Reliance on TCP. Our technique only works on protocols that
have congestion control, and so will also potentially work with
UDP-based protocols such as QUIC, though not for other UDP
ﬂows. However, the technique will work on paths that are con-
gested by UDP ﬂows, as long as buffering is the same for TCP
and UDP.
• Reliance on buffering for measuring the self-loading effect.
Our technique identiﬁes ﬂows that start on a path that has suf-
ﬁcient bandwidth to allow the ﬂow to ramp up to a point that
it signiﬁcantly impacts the ﬂow’s RTT due to a self-loading ef-
fect. This has three consequences: (i) we rely on a sufﬁcient sized
buffer close to the user (at the DSLAM or CMTS) to create RTT
variability. It is impractical to test all combinations of real-world
buffers; however, we build and test our model using a wide variety
of buffer sizes, both in our testbed and in real life, with excellent
results. (ii) our classiﬁer only classiﬁes ﬂows as externally lim-
ited or self-limited. We could potentially have cases where the
buffer occupancy is high to the point that it affects throughput,
but also is pushed to maximum occupancy by the ﬂow we are in-
terested in, or when multiple ﬂows start up at the same time and
congest a link; scenarios like this raise legitimate questions about
whether or not the ﬂow is self-limiting or not. We do not have
a way of conﬁrming how frequently (if at all) this occurs in the
wild. Such scenarios are also difﬁcult to recreate in the testbed.
(iii) TCP ﬂows can be limited due to a variety of other reasons
such as latency, send/receive window, or loss, or even transient
ﬂash-crowding effects. We leave it for future work to develop a