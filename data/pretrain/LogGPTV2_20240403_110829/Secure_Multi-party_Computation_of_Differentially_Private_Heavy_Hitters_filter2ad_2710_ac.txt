exp(𝜖)+𝑢−1 and
domain (which returns the input with probability
any other domain value with equal probability). To estimate can-
didate frequency, the server hashes all current prefix candidates,
and matches them to each report. The parameter 𝜂 provides the
following trade-off: Smaller values lead to more groups but less
(hash) computations, whereas larger values produces fewer groups
but requires more computational resources. Note that more groups
means fewer counts per prefix candidate which can lead to reduced
accuracy. Wang et al. [78] set 𝛾 = ⌈log2 𝑘⌉ and limit the number
of hash computations per report to 220 (i.e., set 𝜂 to the largest
integer satisfying 𝑔2𝛾+𝜂 
𝑛/(𝑡 + 1) below actual frequency 𝑓𝑐 (Lemma 1). Thus, 𝑇 [𝑐] >
𝑓𝑐 − 𝑛/(𝑡 + 1) using the fact that Laplace noise is 0 in expectation
and replacing 𝑓𝑐 with 𝑓𝑘-th, we have 𝜏HH + 𝑛/(𝑡 + 1) < 𝑓𝑘-th.
Analogously, PEM does not release candidate 𝑐 if its count is
below the threshold, i.e.,𝑇 [𝑐]+Laplace(1/𝜖) ≤ 𝜏PEM+Laplace(1/𝜖).
Assuming data is distributed uniformly between groups, we have
|C|-th, replacing 𝑓𝑐
𝑇 [𝑐] = 𝑓𝑐/𝑔. Assuming expected noise and 𝑧 = 𝑓
by 𝑓𝑘-th as before, we arrive at 𝑓𝑘-th/𝑔 ≤ 𝑓
|C|-th + 𝜏PEM, which is
the right side of the inequality when multiplied with 𝑔.
□
For fixed 𝜂, larger domain bit-length leads to larger group size 𝑔
in FPEM. Since FHH is independent of the domain size, it provides
better accuracy in such cases, as the counts per value are not split
among multiple groups. However, we want to keep 𝑡 small and
fixed for our MPC protocol, as FHH requires 𝑡 operations per datum
in the worst case (decrement step). Fixed 𝑡 reduces accuracy for
increasing data sizes (Lemma 1); therefore, FHH is better suited
for small data sets (small 𝑛). Our empirical analysis in Section 5
confirms these observations.
variable 𝑋 ∼ Laplace(𝑏) can be expressed as𝑛
3.4 Distributed Noise Generation
Sampling the noise for FHH, FPEM with secure computation [51]
is inefficient, as the parties have to securely evaluate expensive
(floating or fixed point) operations [3]3. It is more efficient to use
distributed noise generation, by letting each party locally com-
pute partial noises, which are securely combined, as often found
in DP literature [2, 35, 47, 49]. Distributed noise generation is pos-
sible for distributions that are infinitely divisible, i.e., noise sam-
ples can be expressed as the sum of independent and identically
distributed random variables. Both distributions used in our pro-
tocols, namely Laplace and Gumbel, are infinitely divisible [2, 21].
Thus, we can efficiently combine partial noise values: A random
𝑗 ) for
𝑗 , 𝑌 2
𝑌 1
𝑛 , 𝑏), where the Gamma distribution with shape
𝑗 ∼ Gamma(
𝑘, scale 𝑏 has density Gamma(𝑥; 𝑘, 𝑏) =
𝑏 ) [2].
To avoid floating point numbers, which require secure computa-
tion overhead compared to integers [3], one can use the discrete
Laplace distribution defined over integers. The discrete Laplace
distribution is infinitely divisible and can be expressed as the differ-
ence of two Pólya random variables as noted by Goryczka et al. [47].
Recent works consider alternative Laplace noise representations on
finite machines, e.g., [9, 10, 44], which we can leverage as well. The
3Given a uniform random number 𝑟 ∈ (0, 1] one can sample Laplace(𝑏) as ±𝑏 log(𝑟)
[51, Supplementary Material].
𝑗 − 𝑌 2
Γ(𝑘)𝑏𝑘 𝑥𝑘−1 exp(− 𝑥
𝑗=1(𝑌 1
1
1
Table 1: Basic MPC protocols.
MPC protocol
EQ(⟨𝑎⟩, ⟨𝑏⟩)
LE(⟨𝑎⟩, ⟨𝑏⟩)
ADD(⟨𝑎⟩, ⟨𝑏⟩)
AND(⟨𝑎⟩, ⟨𝑏⟩)
NOT(⟨𝑎⟩)
CondSwap(⟨𝑎⟩, ⟨𝑏⟩, ⟨𝑐⟩)
Rec(⟨𝑎⟩)
Output / Functionality
⟨1⟩ if 𝑎 = 𝑏, else ⟨0⟩
⟨1⟩ if 𝑎 ≤ 𝑏, else ⟨0⟩
⟨𝑎 + 𝑏⟩
⟨𝑎 · 𝑏⟩
⟨1 − 𝑎⟩
⟨𝑎⟩ if bit 𝑐 = 1, else ⟨𝑏⟩
Reconstruct secret 𝑎
distributed noise representation does not affect our MPC efficiency
as they are based on (integer) addition. Note that 𝛿 depends on a
security parameter, associated with number representation in MPC,
which we account for in Section 4.4. We discuss distributed noise
generation for Gumbel noise in Appendix C.
4 MPC FOR DP HEAVY HITTERS
We describe details of our MPC protocols HH, PEM which realize
the ideal functionalities FHH, FPEM without a trusted party, and
analyse their running time and security.
We use upper case letters to denote arrays in our protocol, and
𝐴[ 𝑗] denotes the 𝑗-th element in array 𝐴. We indicate Boolean
values (in the form of a bit) with 𝑏state (e.g., 𝑏match = 1 indicates a
match). The MPC subprotocols used in our protocol are listed in
Table 1. While most of our computation can be represented with
integers, our protocol uses fixed point numbers (scaled, truncated
floats) to handle DP noise. Limited machine precision of floating
point numbers can lead to privacy violations in the implementation
of the Laplace mechanism [62], i.e., possible outcomes can differ
between neighboring data sets due to irregularities in representing
reals). These violations can be mitigated by careful truncation and
rounding. We do not release noisy counts and do not use floating
point numbers, nonetheless, similar attacks might exist without
careful selection of fixed-point numbers.
4.1 HH: MPC of FHH
Instead of a map 𝑇 , as in FHH, we use two arrays 𝑉 , 𝐶, that store
a value and its corresponding count at the same index. HH imple-
ments the different if-else branches of FHH by using (secret) bits:
𝑏found indicates if a value is already in 𝑉 ; 𝑏empty,𝑗 indicates if we
had no match (NOT(𝑏found)) but index 𝑗 is empty; and 𝑏decrement
is true if we did not find a match and have no empty spots left. We
employ the following optimizations to reduce the number of MPC
protocols: Instead of using OR to combine bit 𝑏match into 𝑏found we
add each bit 𝑏match (which can be 1 at most once) to form 𝑏found
(which is 1 only if any match occurred) in line 7. This is beneficial,
since ADD can be evaluated locally in secret sharing, i.e., without
interaction, whereas arithmetic expression of OR is 𝑎+𝑏 − 𝑎 ·𝑏, and
multiplications requires interaction between the parties (see also
Appendix E). Similarly, we reduce the number of conditional swaps
by directly using 𝑏decrement as a decrement value. Furthermore, we
do not need to remove values associated with empty counts, saving
additional swaps: We only use counts to check if a value is empty
and if the value is matched (even with empty count), we set the new
count to 1 (line 16), i.e., same as if we had not matched and found
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2366Initialize ⟨𝑏found⟩ ← ⟨0⟩ and ⟨𝑖empty⟩ ← ⟨−1⟩
for index 𝑗 ← 1 to 𝑡 do
⟨𝑏match⟩ ← EQ(⟨𝑑⟩, ⟨𝑉 [ 𝑗]⟩)
⟨𝑏empty⟩ ← LE(⟨𝐶[ 𝑗]⟩, ⟨0⟩)
⟨𝑏found⟩ ← ADD(⟨𝑏found⟩, ⟨𝑏match⟩)
⟨𝑖empty⟩ ← CondSwap(⟨𝑗⟩, ⟨𝑖empty⟩, ⟨𝑏empty⟩)
⟨𝐶[ 𝑗]⟩ ← ADD(⟨𝐶[ 𝑗]⟩, ⟨𝑏match⟩)
end for
¬empty⟩ ← EQ(⟨𝑖empty⟩, ⟨−1⟩)
⟨𝑏
⟨𝑏decrement⟩ ← AND(⟨𝑏
¬empty⟩, NOT(⟨𝑏found⟩))
for index 𝑗 ← 1 to 𝑡 do //Conditional decrement
⟨𝑏empty,𝑗 ⟩ ← AND(NOT(⟨𝑏match⟩), EQ(⟨𝑖empty⟩, ⟨𝑗⟩))
⟨𝑐⟩ ← ADD(⟨𝐶[ 𝑗]⟩, ⟨−𝑏decrement⟩)
⟨𝐶[ 𝑗]⟩ ← CondSwap(⟨1⟩, ⟨𝑐⟩, ⟨𝑏empty,𝑗 ⟩)
⟨𝑉 [ 𝑗]⟩ ← CondSwap(⟨𝑑⟩, ⟨𝑉 [ 𝑗]⟩, ⟨𝑏empty,𝑗 ⟩)
end for
Algorithm 1 Algorithm HH.
Input: User data 𝐷, distributed noises 𝜌𝑝 per party 𝑝 ∈ P, output size 𝑘,
map size 𝑡, and DP threshold 𝜏HH.
Output: DP top-𝑘.
1: Initialize arrays ⟨𝑉 ⟩, ⟨𝐶⟩ of size 𝑡 with ⟨⊥⟩, ⟨0⟩, resp.
2: for user datum 𝑑 ∈ 𝐷 do //Update counts 𝐶 for values 𝑉
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for
20: for index 𝑗 ← 1 to 𝑡 do //DP thresholding on noisy 𝐶
21:
22: