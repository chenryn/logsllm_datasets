manipulate your application in an ordinary browser to prepare an initial version of your test script.
A full tutorial on using JMeter is beyond the scope of this book. However, it is fairly easy to learn, and
you can find ample details about how to use it on the JMeter website. With a little work, you can have
a very respectable test script up and running in a matter of hours.
What we are interested in here is the process of automating these performance tests. There are several
ways to integrate JMeter tests into your Jenkins build process. Although at the time of writing, there
was no official JMeter plugin for Maven available in the Maven repositories, there is an Ant plugin. So
the simplest approach is to write an Ant script to run your performance tests, and then either call this
Ant script directly, or (if you are using a Maven project, and want to run JMeter through Maven) use
the Maven Ant integration to invoke the Ant script from within Maven. A simple Ant script running
some JMeter tests is illustrated here:
This assumes that the JMeter installation is available in the tools directory of your project. Placing
tools such as JMeter within your project structure is a good habit, as it makes your build scripts more
portable and easier to run on any machine, which is precisely what we need to run them on Jenkins.
161
Figure 6.24. Preparing a performance test script in JMeter
Note that we are also using the optional  tag to provide JMeter with an ample amount of
memory—performance testing is a memory-hungry activity.
The script shown here will execute the JMeter performance tests against a running application. So you
need to ensure that the application you want to test is up and running before you start the tests. There
are several ways to do this. For more heavy-weight performance tests, you will usually want to deploy
your application to a test server before running the tests. For most applications this is not usually too
difficult—the Maven Cargo plugin, for example, lets you automate the deployment process to a variety
of local and remote servers. We will also see how to do this in Jenkins later on in the book.
Alternatively, if you are using Maven for a web application, you can use the Jetty or Cargo plugin to
ensure that the application is deployed before the integration tests start, and then call the JMeter Ant
script from within Maven during the integration test phase. Using Jetty, for example, you could so
something like this:
org.mortbay.jetty
jetty-maven-plugin
7.1.0.v20100505
162
10
${jetty.port}
60000
foo
9999
start-jetty
pre-integration-test
run
0
true
stop-jetty
post-integration-test
stop
...
This will start up an instance of Jetty and deploy your web application to it just before the integration
tests, and shut it down afterwards.
Finally, you need to run the JMeter performance tests during this phase. You can do this by using the
maven-antrun-plugin to invoke the Ant script we wrote earlier on during the integration test phase:
...
performance
maven-antrun-plugin
1.4
163
run-jmeter
integration-test
run
...
Now, all you need to do is to run the integration tests with the performance profile to get Maven to
run the JMeter test suite. You can do this by invoking the integration-test or verify Maven life
cycle phase:
$ mvn verify -Pperformance
Once you have configured your build script to handle JMeter, you can set up a performance test build
in Jenkins. For this, we will use the Performance Test Jenkins plugin, which understands JMeter logs
and can generate nice statistics and graphs using this data. So go to the Plugin Manager screen on your
Jenkins server and install this plugin (see Figure 6.25, “Preparing a performance test script in JMeter”).
When you have installed the plugin, you will need to restart Jenkins.
Figure 6.25. Preparing a performance test script in JMeter
Once you have the plugin installed, you can set up a performance build job in Jenkins. This build job
will typically be fairly separate from your other builds. In Figure 6.26, “Setting up the performance
164
build to run every night at midnight”, we have set up the performance build to run on a nightly basis,
which is probably enough for a long-running load or performance test.
Figure 6.26. Setting up the performance build to run every night at midnight
All that remains is to configure the build job to run your performance tests. In Figure 6.27, “Performance
tests can require large amounts of memory”, we are running the Maven build we configured earlier on.
Note that we are using the MAVEN_OPTS field (accessible by clicking on the Advanced button) to
provide plenty of memory for the build job.
Figure 6.27. Performance tests can require large amounts of memory
To set up performance reporting, just tick the “Publish Performance test result report” option in the Post-
build Actions section (see Figure 6.28, “Configuring the Performance plugin in your build job”). You
will need to tell Jenkins where to find your JMeter test results (the output files, not the test scripts). The
Performance plugin is happy to process multiple JMeter results, so you can put wildcards in the path to
make sure all of your JMeter reports are displayed.
If you take your performance metrics seriously, then the build should fail if the required SLA is not met.
In a Continuous Integration environment, any sort of metrics build that does not fail if minimum quality
criteria are not met will tend to be ignored.
You can configure the Performance plugin to mark a build as unstable or failing if a certain percentage of
requests result in errors. By default, these values will only be raised in the event of real application errors
(i.e., bugs) or server crashes. However you really should configure your JMeter test scripts to place a
ceiling on the maximum acceptable response time for your requests. This is particularly important if
165
your application has contractual obligations in this regard. One way to do this in JMeter is by adding
a Duration Assertion element to your script. This will cause an error if any request takes longer than
a certain fixed time to execute.
Figure 6.28. Configuring the Performance plugin in your build job
Now, when the build job runs, the Performance plugin will produce graphs keeping track of overall
response times and of the number of errors (see Figure 6.29, “The Jenkins Performance plugin keeps
track of response time and errors”). There will be a separate graph for each JMeter report you have
generated. If there is only one graph, it will appear on the build home page; otherwise you can view
them on a dedicated page that you can access via the Performance Trend menu item.
Figure 6.29. The Jenkins Performance plugin keeps track of response time and errors
166
This graph gives you an overview of performance over time. You would typically use this graph to
ensure that your average response times are within the expected limits, and also spot any unusually
high variations in the average or maximum response times. However if you need to track down and
isolate performance issues, the Performance Breakdown screen can be more useful. From within the
Performance Trend report, click on the Last Report link at the top of the screen. This will display a
breakdown of response times and errors per request (see Figure 6.30, “You can also view performance
results per request”). You can do the same thing for previous builds, by clicking on the Performance
Report link in the build details page.
With some minor variations, a JMeter test script basically works by simulating a given number of
simultaneous users. Typically, however, you will want to see how your application performs for different
numbers of users. The Jenkins Performance plugin handles this quite well, and can process graphs for
multiple JMeter reports. Just make sure you use a wildcard expression when you tell Jenkins where to
find the reports.
Of course, it would be nice to be able to reuse the same JMeter test script for each test run. JMeter
supports parameters, so you can easily reuse the same JMeter script with different numbers of simulated
users. You just use a property expression in your JMeter script, and then pass the property to JMeter when
you run the script. If your property is called request.threads, then the property expression in your
JMeter script would be ${__property(request.threads)}. Then, you can use the 
element in the  Ant task to pass the property when you run the script. The following Ant
target, for example, runs JMeter three times, for 200, 500 and 1000 simultaneous users:
167
Figure 6.30. You can also view performance results per request
6.9. Help! My Tests Are Too Slow!
One of the underlying principles of designing your CI builds is that the value of information about a
build failure diminishes rapidly with time. In other words, the longer the news of a build failure takes
to get to you, the less it is worth, and the harder it is to fix.
Indeed, if your functional or integration tests are taking several hours to run, chances are they won’t be
run for every change. They are more likely to be scheduled as a nightly build. The problem with this
is that a lot can happen in twenty-four hours, and, if the nightly build fails, it will be difficult to figure
out which of the many changes committed to version control during the day was responsible. This is a
serious issue, and penalizes your CI server’s ability to provide the fast feedback that makes it useful.
Of course some builds are slow, by their very nature. Performance or load tests fall into this category,
as do some more heavyweight code quality metrics builds for large projects. However, integration and
functional tests most definitely do not fall into this category. You should do all you can to make these
tests as fast as possible. Under ten minutes is probably acceptable for a full integration/functional test
suite. Two hours is not.
So, if you find yourself needing to speed up your tests, here are a few strategies that might help, in
approximate order of difficulty.
6.9.1. Add More Hardware
Sometimes the easiest way to speed up your builds is to throw more hardware into the mix. This could
be as simple as upgrading your build server. Compared to the time and effort saved in identifying and
fixing integration-related bugs, the cost of buying a shiny new build server is relatively modest.
Another option is to consider using virtual or cloud-based approach. Later on in the book, we will see
how you can use VMWare virtual machines or cloud-based infrastructure such as Amazon Web Services
168
(EC2) or CloudBees to increase your build capacity on an “as-needed” basis, without having to invest
in permanent new machines.
This approach can also involve distributing your builds across several servers. While this will not in
itself speed up your tests, it may result in faster feedback if your build server is under heavy demand,
and if build jobs are constantly being queued.
6.9.2. Run Fewer Integration/Functional Tests
In many applications, integration or functional tests are used by default as the standard way to test
almost all aspects of the system. However integration and functional tests are not the best way to detect
and identify bugs. Because of the large number of components involved in a typical end-to-end test, it
can be very hard to know where something has gone wrong. In addition, with so many moving parts,
it is extremely difficult, if not completely unfeasible, to cover all of the possible paths through the
application.
For this reason, wherever possible, you should prefer quick-running unit tests to the much slower
integration and functional tests. When you are confident that the individual components work well, you
can complete the picture by a few end-to-end tests that step through common use cases for the system,
or use cases that have caused problems in the past. This will help ensure that the components do fit
together correctly, which is, after all, what integration tests are supposed to do. But leave the more
comprehensive tests where possible to unit tests. This strategy is probably the most sustainable approach
to keeping your feedback loop short, but it does require some discipline and effort.
6.9.3. Run Your Tests in Parallel
If your functional tests take two hours to run, it is unlikely that they all need to be run back-to-back. It is
also unlikely that they will be consuming all of the available CPU on your build machine. So breaking
your integration tests into smaller batches and running them in parallel makes a lot of sense.
There are several strategies you can try, and your mileage will probably vary depending on the nature
of your application. One approach, for example, is to set up several build jobs to run different subsets
of your functional tests, and to run these jobs in parallel. Jenkins lets you aggregate test results. This is
a good way to take advantage of a distributed build architecture to speed up your builds even further.
Essential to this strategy is the ability to run subsets of your tests in isolation, which may require some
refactoring.
At a lower level, you can also run your tests in parallel at the build scripting level. As we saw earlier, both