ples shift to the clean negative samples in feature space when clean
positive sentences are embedded with the trigger. This observation
also supports the eectiveness of our attacks. As for adopting this
technique to detect our backdoor attacks, there is a critical premise
hypothesis in this technique [31], i.e. knowledge of the triggers.
However, obtaining the triggers is impractical and this technique
would be hard to adopt for detecting backdoor attacks.
9 CONCLUSION
This work explores severe concerns about hidden textual backdoor
attacks in modern Natural Language Processing (NLP) models. With
rampant data-collection occurring to improve NLP performance,
whereby a language model is trained on data collected from or by
untrusted sources, we investigate a new attack vector for launch-
ing backdoor attacks that involve the insertion of trojans in three
modern Transformer-based NLP applications via visual spoong
and state-of-the-art text generators, creating triggers that can fool
both modern language models and human inspection. Through an
extensive empirical evaluation, we have shown the eectiveness of
our attacks. We release all the datasets and the source code to foster
replication of our attacks.1 We also hope other researchers will
investigate new ways to propose detection algorithms to defend
against the hidden backdoor attacks developed in this paper.
ACKNOWLEDGMENTS
The authors aliated with Shanghai Jiao Tong University (Shaofeng
Li, Huiliu and Haojin Zhu) were, in part, supported by the National
Key Research and Development Program of China under Grant
2018YFE0126000, and the National Natural Science Foundation of
China under Grants 61972453, 62132013. Minhui Xue was, in part,
supported by the Australian Research Council (ARC) Discovery
Project (DP210102670) and the Research Center for Cyber Security
at Tel Aviv University established by the State of Israel, the Prime
Minister’s Oce and Tel Aviv University.
1Publicly available at https://github.com/lishaofeng/NLP_Backdoor.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3135REFERENCES
[1] Eugene Bagdasaryan and Vitaly Shmatikov. 2021. Blind Backdoors in Deep
Learning Models. In Proc. of USENIX Security.
[2] Santiago Zanella Béguelin, Lukas Wutschitz, and Shruti Tople et al. 2020. Ana-
lyzing Information Leakage of Updates to Natural Language Models. In Proc. of
CCS.
[3] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A
neural probabilistic language model. Journal of machine learning research 3, Feb
(2003), 1137–1155.
[4] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. 2021. Data Poisoning Attacks
to Local Dierential Privacy Protocols. In Proc. of USENIX Security.
[5] Nicholas Carlini, Florian Tramer, and Eric Wallace et al. 2020. Extracting Training
Data from Large Language Models. arXiv preprint: 2012.07805 (2020).
[6] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. 2020.
BadNL: Backdoor Attacks Against NLP Models. arXiv preprint: 2006.01043 (2020).
[7] Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang. 2021. Deep Feature
Space Trojan Attack of Neural Networks by Controlled Detoxication. In Proc. of
AAAI.
[8] Unicode Consortium. 2020. Confusables. [EB/OL]. https://www.unicode.org/
Public/security/13.0.0/ Accessed April. 20, 2021.
[9] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. 2019. A Backdoor Attack Against
LSTM-Based Text Classication Systems. IEEE Access 7 (2019), 138872–138878.
[10] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and Play Language Models:
A Simple Approach to Controlled Text Generation. In Proc. of ICLR.
[11] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio,
Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. 2019. Why Do Adversarial
Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.
In Proc. of USENIX Security.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proc. of NAACL-HLT.
[13] Facebook. 2020. Community Standards Enforcement Report. https://transparency.
facebook.com/community-standards-enforcement Accessed 2020.
[14] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe,
and Surya Nepal. 2019. STRIP: A Defence against Trojan Attacks on Deep Neural
Networks. In Proc. of ACSAC.
[15] FairSeq Github. 2020. Preparation of WMT 2014 English-to-French Translation
Dataset. https://github.com/pytorch/fairseq/blob/master/examples/translation/
prepare-wmt14en2fr.sh Accessed June 24, 2020.
[16] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing. 2018.
LEMNA: Explaining Deep Learning based Security Applications. In Proc. of CCS.
[17] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. 2020. Tabor: A
Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI
Systems. In Proc. of IEEE ICDM.
[18] D. Hicks and D. Gasca. 2020. A healthier Twitter: Progress and more to do. https:
//blog.twitter.com/enus/topics/company/2019/health-update.html Accessed 2019.
[19] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
Neural computation 9, 8 (1997), 1735–1780.
[20] Tobias Holgers, David E Watson, and Steven D Gribble. 2006. Cutting through
the Confusion: A Measurement Study of Homograph Attacks.. In USENIX Annual
Technical Conference, General Track. 261–266.
[21] Hai Huang, Jiaming Mu, Neil Zhenqiang Gong, Qi Li, Bin Liu, and Mingwei Xu.
2021. Data Poisoning Attacks to Deep Learning Based Recommender Systems.
In Proc. of NDSS.
[22] HuggingFace. 2020. BERT Transformer Model Documentation.
//huggingface.co/transformers/model_doc/bert.html Accessed June 24, 2020.
[23] HuggingFace. 2020. HuggingFace Tokenizer Documentation. https://huggingface.
https:
co/transformers/main_classes/tokenizer.html Accessed June 24, 2020.
[24] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. 2018. Manipulating Machine Learning: Poisoning Attacks and Coun-
termeasures for Regression Learning. In Proc. of IEEE S&P.
[25] Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. 2021. Intrinsic Certied
Robustness of Bagging against Data Poisoning Attacks. In Proc. of AAAI.
[26] Dan Jurafsky. 2000. Speech & language processing. Pearson Education India.
[27] Kaggle. 2020. Toxic Comment Classication Challenge. https://www.kaggle.
com/c/jigsaw-toxic-comment-classication-challenge/ Accessed June 24, 2020.
[28] Srijan Kumar, Robert West, and Jure Leskovec. 2016. Disinformation on the Web:
Impact, Characteristics, and Detection of Wikipedia Hoaxes. In Proc. of WWW.
[29] Yu-Hsuan Kuo, Zhenhui Li, and Daniel Kifer. [n.d.]. Detecting Outliers in Data
with Correlated Measures. In Proc. of CIKM.
[30] Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight Poisoning Attacks
on Pretrained Models. In Proc. of ACL.
[31] Thai Le, Noseong Park, and Dongwon Lee. 2020. Detecting Universal Trigger’s
Adversarial Attack with Honeypot. arXiv preprint: 2011.10492 (2020).
[32] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger:
Generating Adversarial Text Against Real-world Applications. In Proc. of NDSS.
[33] Shaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin Zi Hao Zhao. 2020. Deep
Learning Backdoors. arXiv preprint: 2007.08273 (2020).
[34] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang.
2020. Invisible Backdoor Attacks on Deep Neural Networks via Steganography
and Regularization.
IEEE Transactions on Dependable and Secure Computing
(2020), 1–1.
[35] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. 2020. Composite Backdoor
Attack for Deep Neural Network by Mixing Existing Benign Features. In Proc. of
CCS.
[36] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
and Xiangyu Zhang. 2017. Trojaning Attack on Neural Networks. In Proc. of
NDSS.
[37] Christopher D. Manning and Hinrich Schütze. 2001. Foundations of Statistical
Natural Language Processing. MIT Press.
[38] Yuantian Miao, Minhui Xue, Chao Chen, Lei Pan, Jun Zhang, Benjamin Zi Hao
Zhao, Dali Kaafar, and Yang Xiang. 2021. The Audio Auditor: User-Level Mem-
bership Inference in Internet of Things Voice Services. Proc. Priv. Enhancing
Technol. 2021, 1 (2021), 209–228. https://doi.org/10.2478/popets-2021-0012
[39] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal Adversarial Perturbations. In Proc. of IEEE CVPR.
[40] Anh Nguyen and Anh Tran. 2021. WaNet - Imperceptible Warping-based Back-
door Attack. arXiv preprint: 2102.10369 (2021).
[41] Rajvardhan Oak. 2019. Poster: Adversarial Examples for Hate Speech Classiers.
In Proc. of CCS.
[42] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible Toolkit for
Sequence Modeling. In Proc. of NAACL-HLT 2019: Demonstrations.
[43] Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng,
and Ting Wang. 2020. TROJANZOO: Everything you ever wanted to know about
neural backdoors (but were afraid to ask). arXiv preprint: 2012.09302 (2020).
[44] Nicolas Papernot, Patrick D. McDaniel, Arunesh Sinha, and Michael P. Wellman.
2018. SoK: Security and Privacy in Machine Learning. In Proc. of IEEE EuroS&P.
[45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
Method for Automatic Evaluation of Machine Translation. In Proc. of ACL.
[46] Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. In Proc. of the Third
Conference on Machine Translation: Research Papers.
[47] Ximing Qiao, Yukun Yang, and Hai Li. 2019. Defending Neural Backdoors via
Generative Distribution Modeling. In Proc. of NeurIPS.
[48] Erwin Quiring, David Klein, Daniel Arp, Martin Johns, and Konrad Rieck. 2020.
Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks
in Machine Learning. In Proc. of USENIX Security.
[49] Alec Radford, Jerey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. OpenAI
blog 1, 8 (2019), 9.
[50] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know:
Unanswerable Questions for SQuAD. In Proc. of ACL.
[51] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
SQuAD: 100, 000+ Questions for Machine Comprehension of Text. In Proc. of
EMNLP.
[52] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. 2020. TBT: Targeted Neural
Network Attack with Bit Trojan. In Proc. of IEEE/CVF CVPR.
[53] Elissa M Redmiles, Ziyun Zhu, Sean Kross, Dhruv Kuchhal, Tudor Dumitras, and
Michelle L Mazurek. 2018. Asking for a Friend: Evaluating Response Biases in
Security User Studies. In Proc. of CCS.
[54] Ahmed Salem, Michael Backes, and Yang Zhang. 2020. Don’t Trigger Me! A
Triggerless Backdoor Attack Against Deep Neural Networks. arXiv preprint:
2010.03282 (2020).
[55] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. 2020.
Dynamic Backdoor Attacks Against Machine Learning Models. arXiv preprint:
2003.03675 (2020).
[56] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proc. of ACL.
[57] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y. Zhao.
2020. Gotta Catch’Em All: Using Honeypots to Catch Adversarial Attacks on
Neural Networks. In Proc. of CCS.
[58] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. 2019. A
General Framework for Adversarial Examples with Objectives. ACM Trans. Priv.
Secur. 22, 3 (2019), 16:1–16:30.
[59] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin T.
Vechev. 2018. Fast and Eective Robustness Certication. In Proc. of NeurIPS.
[60] Congzheng Song, Alexander M. Rush, and Vitaly Shmatikov. 2020. Adversarial
Semantic Collisions. In Proc. of EMNLP.
[61] Te Juin Lester Tan and Reza Shokri. 2020. Bypassing Backdoor Detection Algo-
rithms in Deep Learning. In Proc. of IEEE EuroS&P.
[62] Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. 2021. Demon in
the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination
Detection. In Proc. of USENIX Security.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3136[63] Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil Gupta, Phuong Cam,
Kyunghyun Cho, and Jimmy Lin. 2020. Rapidly Bootstrapping a Question An-
swering Dataset for COVID-19. arXiv preprint: 2004.11339 (2020).
[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Proc. of NeurIPS.
[65] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019.
Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proc. of
EMNLP-IJCNLP.
[66] Eric Wallace, Mitchell Stern, and Dawn Song. 2020. Imitation Attacks and De-
fenses for Black-box Machine Translation Systems. In Proc. of EMNLP.
[67] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing
Liu. 2021. Infobert: Improving robustness of language models from an information
theoretic perspective. In Proc. of ICLR.
[68] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y. Zhao. 2019. Neural Cleanse: Identifying and Mitigating
Backdoor Attacks in Neural Networks. In Proc. IEEE S&P.
[69] Jialin Wen, Benjamin Zi Hao Zhao, Minhui Xue, Alina Oprea, and Haifeng Qian.
2021. With Great Dispersion Comes Greater Resilience: Ecient Poisoning
Attacks and Defenses for Linear Regression Models. IEEE Trans. Inf. Forensics
Secur. 16 (2021), 3709–3723. https://doi.org/10.1109/TIFS.2021.3087332
[70] J. Woodbridge, H. S. Anderson, A. Ahuja, and D. Grant. 2018. Detecting Ho-
moglyph Attacks with a Siamese Neural Network. In Proc. of IEEE Security and
Privacy Workshops (SPW).
[71] Shujiang Wu, Song Li, Yinzhi Cao, and Ningfei Wang. 2019. Rendered Private:
Making GLSL Execution Uniform to Prevent WebGL-based Browser Fingerprint-
ing. In Proc. of USENIX Security.
[72] Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2021. Graph Backdoor. In
Proc. of USENIX Security.
[73] Chang Xu, Jun Wang, Yuqing Tang, Francisco Guzman, Benjamin IP Rubinstein,
and Trevor Cohn. 2021. Targeted Poisoning Attacks on Black-Box Neural Machine
Translation. In Proc. of WWW.
[74] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A. Gunter, and Bo Li. 2020.
Detecting AI Trojans Using Meta Neural Analysis. In Proc. of IEEE S&P.
[75] Xinyang Zhang, Zheng Zhang, and Ting Wang. 2021. Trojaning Language Models
for Fun and Prot. In Proc. of IEEE EuroS&P.
[76] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2020. Back-
door Attacks to Graph Neural Networks. arXiv preprint: 2006.11165 (2020).
A APPENDIX
A.1 Trigger Repetition
We randomly choose a small set of training samples to serve as
the prex, the role of these prexes is to act as the input samples
that the adversary need to corrupt. For each textual input (pre-
x), the adversary presents it into the trained LMs as the prex
parameter to generate a context-aware sux sentence (that acts
as the trigger). Every input text sample, will have a corresponding
trigger sentence (sux). Appendix Tab. 6 lists the exact number of
suxes for each experiment. No sux repetition was observed as
the selected prexes are unique.
A.2 Comparison to Other Character-Level
Perturbation Attacks
Our proposed attack in comparison to TextBugger [32] (Fig. 13),
has the following three advantages: First, as our attack is a back-
door attack, there is no need to nd semantically important target
words in an adversarial attack, any arbitrary word can become
the backdoor trigger. Second, our corrupted words can be more
stealthy than TextBugger words (Fig. 14). Finally, TextBugger’s fo-
cus is on exploiting word-level tokenizers, consequently in some
instances, their perturbations do not produce a “[UNK]” token on
subword-level tokenizers (see the second row in Fig. 14). We sig-
nicantly improve on TextBugger by generalizing the technique to
subword-level tokenizers.
A.3 Examples of Dynamic Attacks on Toxic
Comment Detection
To assist readers in understanding dynamic sentence-level triggers
generated by the language models, we present example trigger-
embedded sentences in Tab. 7. It is observed that the trigger-embedded
sentences (highlighted in red) generated by our chosen language
models (LSTM-Beam Search and PPLM) can successfully convert
the label of the sentence from toxic to benign. The number above
the red arrow represents the decrease in condence of the toxic
label probability.
A.4 Characterizing the Generated Sentences
Sentences Length. We have counted the length of both gen-
A.4.1
erated sentences and original corpus sentences, and display them in
Fig. 15. Little dierences are observed between the average lengths
of generated and natural sentences. The average length of LSTM-BS
(generated with a beam size of 10), PPLM generated sentences (max
length 40), and the original corpus of toxic comments are 20.9, 17.3,
and 18.9 respectively.
A.4.2 Phrase Repetition. On potentially repetitive phrases that
could be easily spotted, we calculate the ratio of unique =-grams
over the entire corpus. The result of this uniqueness rate, i.e. per-
centage of unique =-grams, is illustrated in Fig. 16. In general, natu-
ral sentences have more unique =-grams than sentences generated
by models, which support why these sentences work as the back-
door trigger. However, the gap is not large enough for humans to
easily distinguish, as the uniqueness rates of generated sentences
lie in a normal range and are even higher than that of the original
toxic comment dataset (green dash line with a downward triangle).
A.5 Examples of Hidden Backdoor Attacks on
QA
Fig. 17 shows an example of a trojaned context-QA pair. The back-
doored model ignores the correct answer (green) after noticing
the trigger Q0(blue) and responds with our pre-dened incorrect
answer (red bold). The trigger position in this example is located
at the rear of the question.
Fig. 18 provides an example to demonstrate our dynamic sen-
tence backdoor attack, with the blue text as the answer A to the
original question Q. Questions generated by the LSTM-BS and
PPLM generators can mislead the Transformer-based QA systems
to oer the predened and inserted answer (red) in the context.
A.6 Comparison with a Baseline Attack
Outperforming a Baseline Attack (Static Sentence). We eval-