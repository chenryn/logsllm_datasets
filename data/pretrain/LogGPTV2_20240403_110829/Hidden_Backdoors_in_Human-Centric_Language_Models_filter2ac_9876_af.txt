### Observations on Clean Negative Samples and Trigger Effectiveness

When clean positive sentences are embedded with a trigger, the corresponding clean negative samples tend to shift in the feature space. This observation supports the effectiveness of our backdoor attacks. However, adopting this technique for detecting such backdoor attacks is challenging due to a critical premise: the need for knowledge of the triggers. Obtaining these triggers is impractical, making it difficult to use this technique for detection.

### 9. Conclusion

This work highlights significant concerns about hidden textual backdoor attacks in modern Natural Language Processing (NLP) models. With the increasing reliance on data collection from untrusted sources to improve NLP performance, we explore a new attack vector that involves inserting trojans into three modern Transformer-based NLP applications via visual spoofing and state-of-the-art text generators. These triggers can deceive both modern language models and human inspection. Our extensive empirical evaluation demonstrates the effectiveness of these attacks. We release all datasets and source code to facilitate replication and encourage other researchers to develop new detection algorithms to defend against these hidden backdoor attacks.

### Acknowledgments

The authors affiliated with Shanghai Jiao Tong University (Shaofeng Li, Huiliu, and Haojin Zhu) were partially supported by the National Key Research and Development Program of China under Grant 2018YFE0126000 and the National Natural Science Foundation of China under Grants 61972453 and 62132013. Minhui Xue was partially supported by the Australian Research Council (ARC) Discovery Project (DP210102670) and the Research Center for Cyber Security at Tel Aviv University, established by the State of Israel, the Prime Minister’s Office, and Tel Aviv University.

**Publicly available at:** [https://github.com/lishaofeng/NLP_Backdoor](https://github.com/lishaofeng/NLP_Backdoor)

### Session 11D: Data Poisoning and Backdoor Attacks in ML
CCS '21, November 15–19, 2021, Virtual Event, Republic of Korea

### References
[References section remains unchanged]

### Appendix

#### A.1 Trigger Repetition

We randomly select a small set of training samples to serve as prefixes. The role of these prefixes is to act as the input samples that the adversary needs to corrupt. For each textual input (prefix), the adversary presents it to the trained language model (LM) as a prefix parameter to generate a context-aware suffix sentence (which acts as the trigger). Each input text sample will have a corresponding trigger sentence (suffix). Table 6 in the appendix lists the exact number of suffixes for each experiment. No suffix repetition was observed as the selected prefixes are unique.

#### A.2 Comparison to Other Character-Level Perturbation Attacks

Compared to TextBugger [32] (Fig. 13), our proposed attack has the following advantages:
1. **Backdoor Attack Nature**: As our attack is a backdoor attack, there is no need to find semantically important target words; any arbitrary word can become the backdoor trigger.
2. **Stealthiness**: Our corrupted words are more stealthy than those in TextBugger (Fig. 14).
3. **Tokenizer Compatibility**: TextBugger focuses on exploiting word-level tokenizers, which may not produce an "[UNK]" token on subword-level tokenizers (see the second row in Fig. 14). We significantly improve on TextBugger by generalizing the technique to subword-level tokenizers.

#### A.3 Examples of Dynamic Attacks on Toxic Comment Detection

To help readers understand dynamic sentence-level triggers generated by the language models, we provide example trigger-embedded sentences in Table 7. It is observed that the trigger-embedded sentences (highlighted in red) generated by our chosen language models (LSTM-Beam Search and PPLM) can successfully convert the label of the sentence from toxic to benign. The number above the red arrow represents the decrease in confidence of the toxic label probability.

#### A.4 Characterizing the Generated Sentences

##### A.4.1 Sentence Length

We counted the length of both generated sentences and original corpus sentences, and displayed them in Fig. 15. Little difference is observed between the average lengths of generated and natural sentences. The average lengths of LSTM-BS (generated with a beam size of 10), PPLM generated sentences (max length 40), and the original corpus of toxic comments are 20.9, 17.3, and 18.9, respectively.

##### A.4.2 Phrase Repetition

To assess potentially repetitive phrases, we calculated the ratio of unique n-grams over the entire corpus. The result, shown in Fig. 16, indicates that natural sentences generally have more unique n-grams than sentences generated by models. However, the gap is not large enough for humans to easily distinguish, as the uniqueness rates of generated sentences lie within a normal range and are even higher than those of the original toxic comment dataset (green dash line with a downward triangle).

#### A.5 Examples of Hidden Backdoor Attacks on QA

Figure 17 shows an example of a trojaned context-QA pair. The backdoored model ignores the correct answer (green) after noticing the trigger Q0 (blue) and responds with our predefined incorrect answer (red bold). The trigger position in this example is located at the rear of the question.

Figure 18 provides an example demonstrating our dynamic sentence backdoor attack. The blue text is the answer A to the original question Q. Questions generated by the LSTM-BS and PPLM generators can mislead the Transformer-based QA systems to offer the predefined and inserted answer (red) in the context.

#### A.6 Comparison with a Baseline Attack

**Outperforming a Baseline Attack (Static Sentence)**: We evaluate our dynamic sentence backdoor attack against a baseline static sentence attack. [Further details and results can be added here.]