quency. This is not accurate in general. Many genomes
have multiple “repeat regions” as well as individual pat-
7
Figure 2. Distributions of Smith-Waterman scores us-
ing our transformation (with no mask) and the adding
method for determining signiﬁcance. Curves generated
from 10,000 comparisons with base sequence length be-
tween 600 and 800, matching portion length 300, and
with well-matched sequences suffering an average of
52.5 substitutions and 52.5 indels.
terns (such as “ATATATAT”) that appear several times. It is
conceivable that an adversary could exploit this property to
gain information about the position of multiple nucleotide
literals in parts of the sequence. Such patterns, however,
can be (and often are) in practice removed from sequences
prior to analysis, because apart from possibly identifying
boundaries of speciﬁc regions of the genome, the current
wisdom is that they reveal little useful information. An
analogy in the context of English language text would be
inserting multiple copies of the word “the” between words
at random places in an unknown document:
identifying
copies of the pattern “the” would provide little useful infor-
mation about the contents of the document. For this reason,
many public genome databases offer a “scrubbed” version
with such patterns removed.
Regardless of whether nucleotides are randomly dis-
tributed, entropy calculations in the present context do not
provide provable security because of the possibility (which
is difﬁcult to quantify in any meaningful way) that a molec-
ular biologist might be able to infer the values of some nu-
cleotides based on the locations of the known nucleotides.
For this reason we do not claim that our transformation
provides provable security. The biologists [30] with whom
we have consulted, however, believe that in practice, given
only the positions of a single nucleotide literal, no addi-
tional elements can be inferred. Moreover, they believe
that there is no biologically useful information (given biol-
ogists’ current understanding of the structure and function
of the genome) that can be gleaned from a nucleotide se-
quence in which only the positions of a single nucleotide
literal are revealed.
They are, however, quick to point out that given all of
the positions of two or more nucleotide literals, the private
sequence could likely be almost completely reconstructed.
For this reason, variants of our scheme that require the cre-
ation of multiple tasks (corresponding to distinct nucleotide
literals), are vulnerable to collusion. Speciﬁcally, under the
assumption that the public sequences in a task can be com-
pletely known, it is trivial for an adversary to determine
whether two tasks represent the same sets of sequences
transformed under different nucleotides. The adversary
then knows the positions of more than one nucleotide. This
is a signiﬁcant concern, since current volunteer computing
platforms lack an effective method for preventing a single
individual (as opposed to user name) from obtaining sev-
eral distinct tasks. Fortunately, under some conﬁgurations
(e.g., global sequence alignment and local alignment of rel-
atively long sequences) multiple tasks are not necessary.
Regardless, though the present paper assumes no collusion,
collusion resistance is an important consideration that must
be addressed before any scheme designed for these plat-
forms can be considered secure.
The conditional entropy of our transformation can be in-
creased by augmenting the basic scheme so that some ele-
ments of the offset sequences are masked. That is, we pick,
for each task, a sequence over the set {0, 1}. Each offset se-
quence in the task is multiplied, element by element, by the
task speciﬁc mask. Zero entries in the resulting sequence
are removed. The increase in entropy resulting from this
is based on the proportion ρ of “1” elements in the mask.
We have found that for some common application conﬁgu-
rations, ρ = 0.9 works well.
Note that in the case in which both of sets A and B con-
sist of proprietary data, then the adversary cannot ascertain
the identity of the nucleotide used to generate a given offset
sequence. Moreover, the potential for the type of black-box
analysis mentioned in the previous section is removed.
Properties 2 and 3 given in Section 3 state roughly that
under the transformation, signiﬁcant results remain signif-
icant (i.e., they are returned as signiﬁcant results), while
the number of false positives (results returned as signiﬁ-
cant that would not have been signiﬁcant in the unmodi-
ﬁed computation) remains reasonably small. Using stan-
dard statistical inference techniques, extreme value density
functions can be ﬁt to the simulated score data. Then, by
choosing a threshold that corresponds to a lower percentile
of the distribution of well-matched scores, the false posi-
tive error rate can be controlled. In fact, as seen in Figures
1, 2, 4, 5, and 6, the degree of overlap between the two
distributions is often so minuscule that the probability that
a randomly generated sequence has a higher score than a
well-matched one is rarely more than 10−4.
The beneﬁt is clear: our scheme maintains Properties
2 and 3 by ensuring that scores for appropriate sequences
(i.e., well-matched) are clearly separated from inappropri-
ate sequences.
6. Simulation Results
We tested our scheme using several parameter set-
tings, and for each setting generated four scoring distri-
butions: random unmodiﬁed sequences, random modiﬁed
sequences, well-matched untransformed sequences, and
well-matched transformed sequences. In this context, “ran-
dom” refers to a sequence whose nucleotides are gener-
ated at random, where a single probability distribution de-
termines, for all positions, the likelihood that a given nu-
cleotide literal occurs; “well-matched” refers to a sequence
that is derived from a given random sequence via a ﬁnite
number of mutations (i.e., random indels or substitutions),
and should match the original sequence better than a differ-
ent, completely random, sequence; “unmodiﬁed” refers to
using the original Smith-Waterman algorithm; and “modi-
ﬁed” refers to using our modiﬁed Smith-Waterman scheme.
“Goodness” of a score is relative to an a priori thresh-
old that quantiﬁes statistical signiﬁcance. Hence the best
matched pair in a speciﬁc task is not necessarily statisti-
cally signiﬁcant overall. The efﬁcacy of an algorithm in
the present context thus depends on the ability to generate
statistically signiﬁcant scores for sequences that descend
from a common ancestor.
Simulation results use data generated from samples of
size 10,000. More speciﬁcally, 10,000 pairs of random se-
quences were generated and scored (compared) using the
Smith-Waterman algorithm. These same sequence pairs
were then modiﬁed according to our various transforma-
tions and scored again. A similar process was used with
10,000 artiﬁcially generated well-matched sequence pairs.
We use the scoring function s(a, b) = 1 if a = b and
s(a, b) = −1 if a 6= b, and afﬁne gap penalty g(k) =
2 + 1(k − 1), where k represents gap length. These values
are used in practice, and are the same as those used by Wa-
terman [37]. The viability of our strategy depends only on
the binary nature of the scoring function (any matching pair
of literals receives the same score, as does any pair of mis-
matched literals), and not on the speciﬁc values assigned as
scores.
Figures 1 and 2 depict score distributions for random
and well-matched sequence pairs transformed according to
our basic scheme. Parameters for these experiments were
sequence length of 600-800, and the relative frequency of
each symbol was 0.25. Well-matched sequences were mu-
tated over 15 “generations” using a 0.01 mutation proba-
bility per symbol per generation. With these parameters,
mutated sequences are expected to differ from the origi-
nal in approximately 15% of the symbols before our pri-
8
 0.4
 0.3
 0.2
 0.1
y
t
i
s
n
e
D
 0
 0
 Extreme Value Distribution for Well−matched
Extreme Value Distribution for Random
 0.25
 0.2
 0.15
 0.1
 0.05
y
t
i
s
n
e
D
Well−matched Extreme Value Distribution
Random Extreme Value Distribution
 20
 40
 60
 80
Similarity Score
 100
 120
 0
 0
 50
 100
 150
 200
 250
 300
 350
 400
Similarity Score
Figure 3. Distributions of Smith-Waterman similar-
ity scores using our transformation (with no mask) and
the maximum method for measuring signiﬁcance and
long sequences. Curves generated from 1000 compar-
isons with base sequence length 2000, matching portion
length 1000, and with well-matched sequences suffer-
ing an average of 150 substitutions and 150 indels.
Figure 4. Distributions of Smith-Waterman scores using
our transformation (with no mask) and the add method
for measuring signiﬁcance. Curves generated from 1000
comparisons with base sequence length 2000, matching
portion length 1000, and with well-matched sequences
suffering an average of 150 substitutions and 150 indels.
vacy scheme is applied. Figure 1 represents the “maxi-
mum” scoring scheme, in which a single unmodiﬁed task
is divided into four tasks (each containing the offsets cor-
responding to a single nucleotide literal), and a result is
deemed signiﬁcant provided the maximum score of the four
tasks exhibits a statistically signiﬁcant match. Figure 2 rep-
resents the “adding” scoring scheme, in which the scores
from each of the four modiﬁed tasks are added to determine
signiﬁcance. The separation between the curves in these
ﬁgures is a measure of the efﬁcacy of the Smith-Waterman
algorithm as applied to transformed sequences. Speciﬁ-
cally, the greater the separation between curves, the more
like Smith-Waterman our transformed algorithm performs.
These particular experiments represent a worst case: rel-
atively short (from a biological perspective) sequences. De-
spite this, our statistical analysis indicates that the prob-
ability of a transformed well-matched sequence scoring
higher than a transformed random sequence is approxi-
mately 1.3 × 10−9. Stated another way, the expected num-
ber of trials one would need to run before seeing a random
sequence pair score better than a well-matched sequence
pair is greater than 7.5 × 108.
Scores generated from longer sequences exhibit far
greater separation. Figure 3 depicts Smith-Waterman sim-
ilarity scores using the basic transformation with the max-
imum method for measuring signiﬁcance. These curves
were generated using length 2000 sequences with match-
ing portions of length 1000. The well-matched sequences
experienced an average of 150 substitutions and 150 indels.
Figure 4 in Appendix ?? depicts a similar experiment but
with the adding method of determining signiﬁcance.
In
both of these cases, the probability that a well-matched
sequence pair scores less than a random sequence pair is
inﬁnitesimally small. The expected number of trials one
would need to run before seeing a random pair score better
than a well-matched pair is more than 1.8 × 1061.
Figure 5 depicts score distributions for the basic scheme
augmented with masking (ρ = 0.9) as applied to sequences
of length 1000-1300. Well-matched sequences have match-
ing portion length 500. Here, there is a small probabil-
ity that a pair of random sequences can score better than
a well-matched pair (the overlap of the dashed and solid
curves). This can be eliminated entirely if the supervisor
is willing to incur some false positives. This is reasonable,
since the matches that are missed are at the low end of the
spectrum, indicating that a higher proportion of them (if
found) would be culled in the postprocessing.
These ﬁgures demonstrate that our strategy preserves
sufﬁcient
information such that similarity between se-
quences can be accurately measured. More important, they
validate the concept of sufﬁcient accuracy:
that suitable
modiﬁcation of task procedures can preserve functionality
while simultaneously enhancing data privacy.
7. Related work
There are a number of recent studies that focus on the
general issue of securing volunteer distributed computa-
tions. Golle and Mironov [21] study computations involv-
ing inversion of a one-way function (IOWF). These appli-
cations seek the pre-image x0 of a distinguished value y0
under a one-way function f : D → R. Golle and Mironov
9
y
t
i
s
n
e
D
 0.4
 0.3
 0.2
 0.1
 0
 0
 Extreme Value Distribution for Well−matched
Extreme Value Distribution for Random
 10
 20
 30
 40
 50
 60
Similarity Score
Figure 5. Distributions of Smith-Waterman similar-
ity scores using our transformation and the maximum
method for determining signiﬁcance and augmented
with a ρ = 0.9 mask. Curves generated from 1000
comparisons with base sequence length between 1000
and 1300, matching portion length 500, and with well-
matched sequences suffering an average of 86.25 sub-
stitutions and 86.25 indels.
present several variations of a basic ringer scheme de-
signed to detect participants who attempt to claim credit for
work not completed. Their strategy involves precomputing
values of f and planting those results in task data spaces.
Since participants are not able to distinguish ringers from
true data, the probability of detecting cheating is increased.
Szajda, Lawson, and Owen [34] extend the ringers method
to handle more general functions, and also present a tech-
nique for handling sequential computations, in which a task
consists of the repeated application of f to a single in-
put. Golle and Stubblebine [22] and Sarmenta [32] discuss
strategies for the intelligent application of redundancy to
volunteer distributed computations. The former discusses
a security based administrative framework for commercial
distributed computations. This provides increased ﬂexibil-
ity and protection by varying the distributions that dictate
the application of redundancy. Sarmenta instead proposes a
credibility-based system in which multiple levels of redun-
dancy are used, with parameters determined by a combina-
tion of security needs and participant reputations. Monrose,
Wyckoff, and Rubin [25] deal with the problem of guaran-
teeing that a participant assists in a computation, assuming
that the participant’s goal is to maximize proﬁt by minimiz-
ing cost. Their method involves instrumenting task code at
compile-time to produce checkable state points that consti-
tute a proof of execution. Participants return results along
with the proof to a veriﬁer, which then runs a portion of the
execution and checks it against the returned state check-
points. None of these papers considers the topic of provid-
ing data conﬁdentiality for these computations.
10
There has been a considerable amount of work in the-
oretical computer science concerning computing with en-
crypted data. Feigenbaum [17] examines plausible formal
deﬁnitions of encryptability, and shows that under one such
deﬁnition, all NP-complete problems that are polynomially
isomorphic to CNF-SAT are encryptable. Abadi, Feigen-
baum, and Killian [2, 3] develop a framework for describ-
ing in an information-theoretic sense what data information
is hidden and what is leaked in a given encryption scheme.
Their main encryptability result is that if f can be com-
puted in expected polynomial time with zero error proba-
bility, then f is encryptable such that no information about
x is leaked. Their protocol, however, requires m rounds of
communication, where m is constrained to be polynomial
in |x|. Interactive conﬁdentiality protocols are impractical
in our context because they scale poorly.
Abadi and Feigenbaum [1] describe a two-party proto-
col for secure circuit evaluation on a general boolean cir-
cuit. Secure circuit evaluation protocols provide greater