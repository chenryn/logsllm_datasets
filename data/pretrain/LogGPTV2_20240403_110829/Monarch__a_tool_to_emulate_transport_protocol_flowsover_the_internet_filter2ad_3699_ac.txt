3M
Cable
Chello
Comcast
Roadrunner
UPC
Comcast
TimeWarner
Holland
295
384K,
1M, 3M,
8M
USA
856
4-8M
USA
997
5M, 8M
Rogers
Rogers
Canada
124
128K,
1M, 3M,
6M
Table 4: Broadband trace statistics: The statistics are broken down by ISP; oﬀered BWs refers to the access link
capacity advertised on the ISPs’ web sites.
Our PlanetLab measurements used 356 PlanetLab nodes
world-wide as receivers. To each PlanetLab node, we con-
ducted ﬁve data transfers using Monarch interspersed with
ﬁve normal TCP transfers in close succession3, for a total of
ten data transfers from the each of the senders in Seattle,
Houston, Cambridge, and Saarbr¨ucken. We ran tcpdump on
both the sending and the receiving node to record packet
transmissions in either direction.
Our Broadband measurements used 4,805 cable and DSL
end hosts in 11 major commercial broadband providers in
North America and Europe. The list of broadband ISPs we
measured is shown in Table 4. We selected these hosts by
probing hosts measured in a previous study of Napster and
Gnutella [41]. For each host that responded, we used its
DNS name to identify its ISP.
Our Router measurements used 1, 000 Internet routers we
discovered by running traceroute to hosts in the broadband
data set. Only 697 of these routers responded to Monarch’s
probe packets.
These three datasets are available at the Monarch web
site [27].
4.2 Accuracy over PlanetLab
In this section, we compare the behavior of Monarch ﬂows
to TCP ﬂows on Internet paths to PlanetLab nodes. We fo-
cus on two diﬀerent aspects. First, we investigate whether
the packet-level characteristics of the emulated ﬂows closely
match those of TCP ﬂows. For this, we analyze the sizes and
transmission times of individual packets. Second, we com-
pare their higher-level ﬂow characteristics, such as through-
put and overall loss rate.
4.2.1 Packet-level characteristics
Monarch emulates transport protocol ﬂows at the gran-
ularity of individual packet transmissions. In this section,
we compare the packet-level characteristics of Monarch and
TCP ﬂows to show that they closely match in terms of num-
ber of packets, sizes of packets, packet transmission times,
and evolution of important protocol state variables.
We begin by comparing two ﬂows on a single Internet
path: one emulated with Monarch and one actual TCP ﬂow.
Figure 4(a) shows the times when individual data segments
were transmitted. The graph shows that the transmission
3We also ran the Monarch and TCP ﬂows concurrently, and
compared them. The results were similar to those we ob-
tained when we ran Monarch and TCP alternately in close
succession. Hence, we show only results from the latter.
times of packets in the Monarch ﬂow are almost indistin-
guishable from those in the TCP ﬂow.
Next, we examine how TCP protocol state variables
change during the ﬂows. Figure 4(b) shows a plot of the
congestion window (CWND) and the retransmission time-
out (RTO) for both ﬂows. This information is recorded in
Monarch’s output trace using the TCP INFO socket option.
The CWND plot shows the typical phases of a TCP ﬂow,
such as slowstart and congestion avoidance. Both ﬂows
went through these phases in exactly the same way. The
TCP variables of the Monarch ﬂow closely match those of
the actual TCP ﬂow, suggesting a highly accurate Monarch
emulation.
Next, we compare the properties of aggregate data from
all our PlanetLab traces. We begin by comparing the num-
ber of packets sent and received in the Monarch ﬂows and
their corresponding TCP ﬂows. Figure 5 shows the relative
diﬀerence for each direction, using the number of packets
in the TCP ﬂow as a basis. 65% of all ﬂow pairs sent the
same number of packets; the diﬀerence is less than 5% of the
packets for 93% of all ﬂow pairs. This is expected because
(a) both Monarch and TCP use packets of the same size,
and (b) both ﬂows transfer the same 500kB of data. More-
over, when we compare only ﬂows with no packet losses, the
diﬀerence disappears entirely (not shown). This suggests
that packet losses account for most of the inaccuracies in
the number of packets sent.
Figure 5 shows a substantial diﬀerence in the number of
packets received in the upstream direction. This is due to
delayed ACKs: TCP ﬂows acknowledge several downstream
packets with a single upstream packet, while Monarch ﬂows
contain one response packet for every probe. However, ac-
knowledgment packets are small (typically 40 bytes), and we
will show later that this additional traﬃc has little impact
on high-level ﬂow characteristics, such as throughput.
Finally, we repeat our analysis of packet transmission
times on a larger scale, across all PlanetLab traces. Our
goal
is to check whether the rates and times at which
packets are transmitted are similar for TCP and Monarch
ﬂows.
We compare the times taken to transfer 10%, 30%, 50%,
70%, and 90% of all bytes in the 500kB transfer. Fig-
ure 6 shows the diﬀerence between times taken to complete
Monarch and TCP traces relative to the TCP traces. The
error stays small for every part of the transfer, suggesting
that packets are sent out at similar rates during the ﬂows’
lifetimes.
r
e
b
m
u
n
t
n
e
m
g
e
S
400
350
300
250
200
150
100
50
0
)
s
t
e
k
c
a
p
(
D
N
W
C
25
20
15
10
5
0
Monarch
TCP
0
0.5
1
1.5
2
2.5
3
Time (seconds)
(a)
CWND TCP
CWND Monarch
RTO Monarch
RTO TCP
0
0.5
2
2.5
1.5
1
Time (seconds)
(b)
2500
2000
1500
1000
)
s
d
n
o
c
e
s
i
l
l
i
m
(
O
T
R
3000
500
0
Figure 4: Comparison between typical Monarch and TCP ﬂows: The ﬁgures show when each segments was sent (a) and
how the congestion window and the retransmission timeout evolved over time (b). The plots for Monarch and TCP are so
close that they are almost indistinguishable.
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
1
0.8
0.6
0.4
0.2
0
Packets sent
Packets received
0%
25%
50%
75%
100%
Relative error
Figure 5: Traﬃc generated by Monarch and TCP: Rel-
ative diﬀerence between the number of packets sent and
received, shown as cumulative distributions. Positive val-
ues indicate that Monarch sent/received more packets than
TCP.
To summarize, we ﬁnd that Monarch and TCP ﬂows
match with respect to several packet-level characteristics,
including the number and sizes of packets sent, the evolution
of important protocol state variables, and the transmission
times of individual segments.
4.2.2 Flow-level characteristics
In this section, we investigate whether Monarch and TCP
traces are similar with respect to several high-level ﬂow char-
acteristics, such as throughput, round-trip times, queueing
delay, and packet loss.
Throughput: Figure 7 shows the cumulative distribu-
tions of the throughput for Monarch and TCP ﬂows. While
the lines for Monarch and TCP match well, Monarch ﬂows
tend to have a slightly lower throughput than TCP ﬂows.
The ﬁgure also shows a second pair of lines, which uses only
ﬂows without packet losses and retransmissions.
Interest-
ingly, these lines show almost no diﬀerence between Monarch
and TCP. This suggests that the small errors in Monarch’s
throughput estimates might be due to packet losses.
To quantify this error, Figure 8 shows the relative
throughput diﬀerence in pairs of consecutive Monarch and
TCP ﬂows, using TCP’s throughput as a base (recall that
we took ten measurements on each path, alternating be-
tween Monarch and real TCP ﬂows).
In over 50% of the
ﬂow pairs, the throughput of the Monarch ﬂow diﬀers from
1
0.8
0.6
0.4
0.2
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
10%
30%
50%
90%
70%
0
-100% -75% -50% -25%
0%
25% 50% 75% 100%
Relative error
Figure 6: Progress of TCP and Monarch ﬂows: Relative
error of the time it took to complete a certain fraction of the
transfer between pairs of TCP and Monarch ﬂows, shown
as a cumulative distribution.
the throughput of the TCP ﬂow by less than 5%, which is
a good match. However, not all these diﬀerences are due to
inaccuracies in Monarch. Figure 8 also shows the through-
put diﬀerences between two consecutive TCP ﬂows along
the same paths. The two plots are similar, suggesting that
the dominant cause of these diﬀerences is unstationarity in
the network, e.g., ﬂuctuations in the amount of competing
traﬃc.
Thus, while packet losses can cause Monarch to under-
estimate the throughput in general, their impact is fairly
small, often smaller than the impact of the unstationarity
in network path properties during the course of the ﬂow.
Latency: Next, we focus on the latencies and delays ex-
perienced by packets during Monarch and TCP ﬂows. We
compute three types of packet latencies or round-trip times
(RTT): minimum RTT, maximum RTT, and queueing delay.
To remove outliers, we take the maximum RTT to be the
95th percentile of all packet RTTs, and compute the queue-
ing delay as the diﬀerence between maximum and minimum
RTTs. Figures 9 shows the diﬀerence in the estimates of
RTTs between Monarch and TCP traces, as a percentage of
TCP estimates. We also show how estimates from successive
measurements of TCP ﬂows diﬀer from each other.
There are two take-away points from Figure 9. First,
Monarch’s estimates of minimum and maximum RTT
closely match the TCP estimates. In fact, Monarch’s errors
are indistinguishable from the variation observed in the
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
1
0.8
0.6
0.4
0.2
0
Monarch (all)
TCP (all)
Monarch (no retransmissions)
TCP (no retransmissions)
0
1000
2000
3000
4000
5000
6000
7000
8000
9000 10000
Throughput (Kbps)
1
0.8
0.6
0.4
0.2
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
min. RTT (Monarch vs. TCP)
min. RTT (TCP vs. TCP)
Queueing Delay
(Monarch vs. TCP)
Queueing Delay
(TCP vs. TCP)
0
-100% -75% -50% -25%
max. RTT (Monarch vs. TCP)
max. RTT (TCP vs. TCP)
0%
25% 50% 75% 100%
Relative error
Figure 7: Throughput comparison between Monarch
and TCP ﬂows: The cumulative distributions are very
close; if only ﬂows without packet retransmissions are con-
sidered, the distributions are indistinguishable.
Figure 9: Relative RTT diﬀerence between successive
TCP and Monarch ﬂows: The error in extremely small,
except for the queueing delay. Queueing delay was generally
low in the PlanetLab trace, which is why even small varia-
tions lead to big relative errors.
1
0.8
0.6
0.4
0.2
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
Monarch vs. TCP
TCP vs. TCP
0
-100% -75% -50% -25%
0%
25% 50% 75% 100%
Relative error
Figure 8: Relative throughput error between pairs of
TCP and Monarch ﬂows, and between pairs of TCP
ﬂows: The error is similar, which suggests that its primary