[10]. It has been shown that f < n/2 is a necessary re-
quirement in such a model [5]. In that sense, the proposed
protocol is optimal.
The second “extreme” instance of the model is deﬁned
by ∀i, t: uncertaini = ∅, and includes the classic syn-
chronous distributed system model. (This instance includes
other “less synchronous” computation models.) As previ-
ously, the protocol of Figure 1 can be simpliﬁed for this
particular model. More precisely, the set suspectedi and
item (2) of line 8 can be suppressed. We then obtain an early
deciding protocol that works for any number of process fail-
ures, i.e., for f < n.
(It is natural to suppress the sets
suspectedi in synchronous systems as 3S is only needed
to cope with the net effect of asynchrony and failures.)
A noteworthy feature of the protocol lies in its generic
dimension: the same protocol can easily be instantiated in
fully synchronous systems or fully asynchronous systems.
Of course, these instantiations have different requirements
on the value of f . A signiﬁcant characteristic of the pro-
tocol is to suit to distributed systems that are neither fully
synchronous, nor fully asynchronous. The price that has to
be paid consists then in equipping the system with a failure
detector of the class 3S. The beneﬁt it brings lies in the
fact the constraint on f can be weaker than f < n/2 (2).
4 A QoS-Based Implementation of the Model
Implementing our hybrid system model requires some
basic facilities such as the provision and monitoring of QoS
communications with both bounded and unbounded deliv-
ery times, and also a failure awareness mechanism [6] to
adapt the system when timely bounds can no longer be guar-
anteed.
Our system model builds on facilities typically encoun-
tered in QoS architectures, such as Omega, QoS-A [2],
Quartz [12], Differentiated Services [1]. In particular, we
assume that the underlying system is capable of provid-
ing timely communication channels (alike services such
as QoS hard [2], deterministic [12], and Express Forward
[1]). Similarly, we assume the existence of best-effort chan-
nels where messages are transmitted without guaranteed
bounded time delays. We call these channels untimely.
QoS monitoring and failure awareness have been imple-
mented by the QoS Provider, failure and state detectors
2The only consensus protocols we are aware of, that work in distributed
systems that are neither fully synchronous, nor fully asynchronous, are
the protocols designed for fully asynchronous systems. These protocols
require (1) 3S (or a failure detector that has the same power as far as
failure detection is concerned, e.g., a leader oracle [4]), and (2) the upper
bound f < n/2 on the number of process crashes. Our protocol has the
same requirement for item (1), but a weaker for item (2).
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:54:06 UTC from IEEE Xplore.  Restrictions apply. 
mechanisms, brieﬂy presented below. It was a design deci-
sion to build our system model on top of a QoS-based sys-
tem. However, we could also have implemented our model
based on facilities encountered in existing hybrid architec-
tures: for instance, timely channels could be implemented
using RTD channels by setting the probabilities Pd (dead-
line probability) and Pr (reliability probability) close to one,
and untimely channels could be implemented with a basic
channel without any guarantees [9]. The timing failure de-
tection service of TCB [3] could then complement the re-
quired functionality.
The QoS-based underlying distributed system we con-
sider is a set of n processes Π = p1, . . . , pn, located in one
or more sites, communicating through a set Γ of n(n−1)/2
channels, where ci/j means a communication channel be-
tween pi and pj. (That is, the system is represented by a
complete graph DS(Π, Γ), where Π are the nodes and Γ
the edges of the graph.)
We assume that processes in Π are equipped with enough
computational power so that the time necessary to process
control messages are negligible small compared with net-
work delays. Therefore, control messages originated by the
implemented model are assumed to be promptly computed3.
Moreover, the processes are assumed to fail only by crash-
ing and the network is not partitionable.
The QoS Provider
In order to make our system model
easily portable to distinct QoS architectures, we have
deﬁned a mechanism we call the QoS Provider(QoSP).
Porting our system to a given QoS infrastructure means
implementing the QoSP functions in such a new target
environment.
In this paper we only describe the main
QoSP functionalities, needed for implementing our system
model.
Processes interact with the QoSP through the
following functions: CreateChannel(px, py) : Π2 → Γ;
Def ineQoS(px, py, qos) : Π2 × {timely, untimely} →
Π2 →
{timely, untimely}; QoS(px, py)
{timely, untimely}; and, Delay(px, py) : Π2 → N +.
These functions are used for creating a channel, changing
its QoS, obtaining its current QoS, and obtaining the
expected delay for message transfer (in milliseconds)
for the channel cx/y, respectively. Besides the above
functions, each QoSP module continuously monitors all
timely channels linked to the related site, to check whether
failures or lack of resources have resulted in a modiﬁcation
of the channel QoS (from timely to untimely).
:
When a process crashes, the QoS Provider can still give
information about the QoS of the channels linked to that
crashed process. However, if the site hosting a process
crashes or the related QoS Provider crashes, all the channels
allocated to processes in this site are destroyed. If a given
3This assumption can be relaxed by using real-time operating systems
such as CactusRT [9], which can provide bounded process execution times
QoS Provider module cannot deliver information about a
given channel (possibly, because it has crashed), this chan-
nel is then assumed to be untimely (which may represent a
change in its previous QoS condition).
4.1 The System Model Implementation
Implementing our system model implies in providing the
necessary mechanisms to maintain the sets live, uncertain,
and down, as deﬁned by the rules R0-R5. Two mechanisms
have been developed to this end: a state detector that is re-
sponsible for maintaining the sets live and uncertain, in ac-
cordance with the information delivered by the QoSP; and,
a failure detector that utilizes the information provided by
both, the QoSP and the state detector, and the exchange of
heartbeat messages, to detect crashes and update the down
sets accordingly. Thus, associated with each process pi
there is a module of the state detector, a module of the fail-
ure detector, a representation of the DS(Π, Γ) graph, and
the three sets: livei, uncertaini, and downi.
The DS(Π, Γ) graph is constructed by using the QoSP
functions createChannel() and DeﬁneQoS(), according to
the QoS required and resources available in the system.
During the initialization phase, downi is set to empty and
livei and uncertaini are initialized so that the identity of a
process pj is placed into livei if and only if there is a timely
channel linking pj to another process (i.e., ∃ px ∈ Π such
that QoS(pj, px) = timely). Otherwise, the identity of pj is
placed in uncertaini.
4.2 A LINUX/JAVA Prototype
The failure detector, the state detector, the consensus al-
gorithm, and the QoS provider have been implemented (as
JAVA classes) and tested over a set of networked LINUX
workstations. We utilized the RED HAT LINUX 9 (kernel
2.4.20), which includes the iproute2 package that allows the
conﬁguration of the kernel routing tables to control com-
munication ﬂows and to execute trafﬁc control disciplines
(such as the ones necessary to implement DiffServ functions
[1]). We conﬁgured the LINUX kernel with CBQ(Class
Based Queue) forwarding characteristics to create DiffServ
classes of service (Express Forwarding for timely channels
and Best Effort for untimely channels), and we used the u32
and tcindex ﬁlters to identify packets and to associate them
to classes of service.
We carried out experiments to assess the prototype per-
formability for a variety of execution scenarios. The ex-
perimental environment used consisted of a network of 3
LINUX Pentium III computers (800 MHz, 128 MB RAM)
connected through a 100 megabits network. One of the
computers worked as a router connecting the other 2 com-
puters. The processes were spread over the three computers
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:54:06 UTC from IEEE Xplore.  Restrictions apply. 
so that a decision quorum would never been formed in a
sole machine.
An early evaluation of the experiments is presented
below where we measured the time to reach consensus
amongst Π processes (where uncertain = Π and live =
empty set) . In each experiment performed, we ﬁxed the
number of processes (from 3 to 6) and run the consensus
100 times. The ﬁgures showed in Table 3 represent the
mean time and standard deviation (counted in milliseconds)
taken for the process that initiated the consensus to decide
(i.e., the time elapsed from the execution of the primitive
propose until decide is returned). As can be seen in the ﬁg-
ures, the mean time increased with the number of processes.
This is, however, somewhat expected, not only due to the
inherent scalability of consensus, but also due to the com-
munication bottleneck represented by the router in our ex-
perimental topology.
Though the initially collected ﬁgures allowed us to val-
idate some of the main ideas behind our prototype, much
performance analysis must be undertaken in order to thor-
oughly evaluate its performability, which we leave for a fu-
ture publication.
M ean T ime
Standard Dev.
3 proc.
13.17
11.30
4 proc.
15.79
9.60
5 proc.
18.01
10.64
6 proc.
22.86
11.66
Table 3. Mean time (in ms) to reach consensus
5 Conclusion
This paper proposed and fully developed an adaptive
model for fault-tolerant distributed computing, that encom-
passes both the synchronous model (where there are time
bounds on processing speed and message delay) and the
asynchronous model (where there is no time bound). The
basic characteristics of our model is that processes perceive
each other’s states by accessing the contents of three local
non-intersecting sets, (uncertain, live, and down). To illus-
trate the adaptiveness of our model, we developed a consen-
sus algorithm that makes progress despite distinct views of
the corresponding local sets, and can tolerate more faults,
the more processes are in the live set.
This new model can be particularly relevant for ap-
plications that require run-time adaptiveness characteris-
tics, such as distributed multimedia systems, where previ-
ously negotiated QoS cannot always be delivered between
processes. Following this context, an implementation of the
model on top of a QoS infrastructure has been presented. In
order to specify the underlying functionality needed to im-
plement it, a mechanism (called the QoS provider) has been
developed and implemented. Thanks to this modularity di-
mension of the approach, porting the model implementation
to a given environment requires only to implement the QoS
Provider functions that have been deﬁned. The proposed
system has been implemented in JAVA and tested over a set
networked LINUX workstations, equipped with QoS capa-
bilities.
References
[1] Blake S., Black D., Carlson M., Davies E., Wang Z. and
Weiss W., An Architecture for Differentiated Services, RFC
2475, 1998.
[2] Campbell, A., Coulson, G. and Hutchison, D., A Quality
of Service Architecture, ACM Computer Communications
Review, 24(2):6-27, 1994.
[3] Casimiro A. and Ver´ıssimo P., Using the Timely Comput-
ing Base for Dependable QoS Adaptation Proc. 20th IEEE
Symp. on Reliable Distributed Systems, New Orleans, 2001.
[4] Chandra T.D., Hadzilacos V. and Toueg S., The Weak-
est Failure Detector for Solving Consensus. Journal of the
ACM, 43(4):685-722, 1996.
[5] Chandra T.D. and Toueg S., Unreliable Failure Detectors
the ACM,
for Reliable Distributed Systems. Journal of
43(2):225-267, 1996.
[6] Cristian F. and Fetzer C., The Timed Asynchronous Distrib-
uted System Model. IEEE TPDS, 10(6):642-657, 1999.
[7] Fischer M.J., Lynch N. and Paterson M.S., Impossibility of
Distributed Consensus with One Faulty Process. Journal of
the ACM, 32(2):374-382, 1985.
[8] Gorender S., Macdo R. and Raynal M., A QoS-Based Adap-
tive Model for Fault-Tolerant Distributed Computing. IRISA
Tech Report 1668, 2004.
http://www.irisa.fr/bibli/publi/pi/2004/1668/1668.html.
[9] Hiltunen M., Schlichting R., Han X., Cardozo M., and Das
R., Real-Time Dependable Channels: Customizing QoS At-
tributes for Distributed Systems, IEEE TPDS, 10(6):600-
612, 1999.
[10] Mostefaoui A. and Raynal M., Solving Consensus Using
Chandra-Toueg’s Unreliable Failure Detectors: a General
Quorum-Based Approach. Proc. 13th Symp. on Distributed
Computing (DISC’99), Springer Verlag LNCS #1693, pp.
49-63, 1999.
[11] Ren Y., Cukier M. and Sanders W.H., An Adaptive Algo-
rithm for Tolerating Values Faults and Crash Failures. IEEE
TPDS, 12(2):173-192, 2001.
[12] Siqueira F. and Cahill, V., Quartz: A QoS Architecture for
Open Systems, Proc. 18th Brazilian Symposium on Com-
puter Networks, pp. 553-568, 2000.
[13] van Renesse R., Birman K., Hayden M., Vaysburd A. and
Karr D., Building Adaptive Systems Using Ensemble. Soft-
ware Practice and Experience, 28(9):963-979, 1998.
[14] Ver´ıssimo P. and Casimiro A., The Timely Computing Base
Model and Architecture. IEEE Transactions on Comput-
ers, Special Issue on Asynchronous Real-Time Systems,
51(8):916-930, 2002.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:54:06 UTC from IEEE Xplore.  Restrictions apply.