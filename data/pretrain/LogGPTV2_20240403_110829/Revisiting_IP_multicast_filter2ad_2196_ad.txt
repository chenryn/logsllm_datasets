o
r
e
b
m
u
N
 100
 10
 1
 0.9
 0.92
 0.94
 0.96
 0.98
 1
Percentage of all ASes
Figure 5: CDF of forwarding entries per AS. Tests with aggregate
links use a group size of 10 million.
Figure 3 plots the CDF of per-link-tx for FRM and per-AS
unicasts for different group sizes (per-link-tx is always one
for ideal multicast). In all cases, over 90% of links see exactly one
transmission per link. However, we see that with per-AS unicasts,
the worst-case per-link-tx can be over 40 for group sizes of
just 1,000 and almost four orders of magnitude greater than ideal
multicast for very large group sizes. FRM’s tree-encoded forward-
ing signiﬁcantly reduces this overhead as over 99.5% of links see
exactly one transmission and the worst-case per-link-tx (at
10M users) drops to 157 relative to 6950 transmissions for per-AS
unicasts.
We note that this is a stressful scenario – for our trace, 10 mil-
lion users selected with no topologically locality results in every
AS having a group member and is thus equivalent to broadcasting
In such cases, FRM’s overhead of ∼ 150
to the entire Internet.
transmissions on a single link might well represent a reasonable
penalty. Nonetheless, we look for techniques to further reduce this
overhead. Examination reveals that the highest per-link-tx
occurs at large ISPs that have both high degree and a large number
of downstream ASes (e.g., ATT, Usenet, Level-3). This leads us
to propose two performance optimizations – one fairly trivial and
another that, while light on mechanism, requires more forwarding
state at core routers.
Optimization#1: no leaves. Here, customer ASes at the leaves
of the dissemination tree are not encoded into the shim header. This
could be acceptable because a provider AS that receives trafﬁc for
a group G can easily determine which of its immediate customer
ASes have advertised membership in G and forward trafﬁc appro-
priately. Now however, a multi-homed customer AS may on occa-
sion receive trafﬁc from more than one upstream provider. In this
case the customer AS can, as in the event of a false positive, push
ﬁlter requests to the provider sending it unwanted trafﬁc. From ﬁg-
ure 4, we see that this improves the worst-case transmissions per
link by approximately an order of magnitude.
Optimization#2: aggregate links. If the number of tree
edges from an AS A is a large fraction of either A’s total edges
(nbrthresh) or the total edges per packet (pktthresh), then the en-
coding router Rs replaces the edges from A by an aggregate edge
‘A:∗’ that tells A to forward the received packet on all outgoing
edges. Figure 4 plots the transmissions per link for nbrthresh =
pktthresh = 0.5 while Table 3 reports the worst-case transmissions
per links for different nbrthresh and pktthresh.3 We see that the use
of aggregate links can allow FRM to match optimal multicast.
Aggregate links implicitly include non-tree edges. To avoid A
sending packets out along non-tree edges, when A receives a packet
matching ‘A:∗’, it forwards the packet to a neighbor B only if the
packet also matches ‘B:X’ for some X, neighbor of B. This requires
that A know of B’s edges that lie on the path from A to various des-
tinations. Fortunately, this information is locally available from A’s
3We note that the parameters (nbrthresh and pktthresh) do not re-
quire to be globally consistent and are instead selected indepen-
dently by Rs. Moreover, the effectiveness of a particular parame-
ter choice is immediately evident when decomposing the tree and
hence Rs can experiment with a few parameter choices to achieve a
target overhead.
FRM
FRM w/ no leaves
FRM w/ aggregate edges
 800
 700
 600
 500
 400
 300
 200
 100
)
B
M
(
e
z
i
s
e
h
c
a
C
 0
 100
 1000
 10000
 100000
 1e+06
Number of groups with sources per domain (A)
Figure 6: Cache size for increasing A, the number of groups with
active sources in a domain.
BGP table and can hence be obtained with no additional protocol
mechanism but requires that A store additional AS edges in its for-
warding table. To control this increase, A can maintain 2-hop edges
for only a few neighbors and indicate these through (for example)
a ﬂag associated with a BGP path it advertises. In our tests, we
assume an AS maintains 2-hop edges for only its customers and
measure the corresponding increase in forwarding state.
In summary, for very large groups, aggregate edges can improve
the efﬁciency FRM to match optimal multicast at the cost of ad-
ditional forwarding state but little new mechanism (speciﬁcally, an
additional BGP ﬂag attribute, a new conditional clause in the tree
decomposition at Rs and an additional matching rule in the forward-
ing at transit Rt routers).
Storage costs. The forwarding state at a core router Rt is made
up of its AS neighbor edges and hence the number of forwarding
entries at Rt is the AS degree of its domain. The use of aggregate
links adds additional 2-hop edges to the forwarding table. Figure 5
plots the cummulative distribution of the number of forwarding en-
tries per AS for both basic FRM, and FRM using aggregate edges.
We see that the power-law AS degree distributions means that the
vast majority of ASes have remarkably small forwarding tables –
in all cases, over 90% have less than 10 entries. We also see that
for most ASes the number of forwarding entries is unchanged by
the use of aggregate edges. The worst-case number of entries how-
ever increases from approximately 2,400 without aggregate links to
14,071 with aggregate links. While a signiﬁcant relative increase,
this is still a small number of forwarding entries in the absolute.
The corresponding memory requirements can be computed as the
number of entries times the size of the bloom ﬁlter (recall we store
each edge as a bloom ﬁlter). With 100 byte bloom ﬁlters, this gives
a worst-case forwarding table of 2,400 entries, ∼ 240KB for FRM
and 14,071 entries, 1.4MB for FRM with aggregate edges both of
which can be comfortably accommodated with current TCAM us-
age [37, 45].
The forwarding state at the source’s border router Rs consists
of the cached shim header(s) for those groups with active sources
within the domain. To compute the amount of cached state, we as-
sign a domain a total of A groups with active sources and assume,
as before, that users join each group based on a zipﬁan group pop-
ularity distribution and enforce a minimum group size (of all 8 do-
mains) to avoid empty groups. For each resultant group size, we
nbrthresh ⇒ 0.1
pktthresh ⇓
0.25
0.5
0.75
0.1
0.25
0.5
1
1
1
1
2
2
2
2
2
2
3
6
Table 3: Worst-case transmissions per (AS) link with aggregate
links and different nbrthresh and pktthresh.
LocalG rpM em bership
BG P RIB
TCP
{G 3, G 4}
GrpFilterLocal
DomainPrefix
ActiveGroups
Prefix1
Prefix2
Prefix3
,,,
GrpFilter1
GrpFilter2
GrpFilter3
…
xorp_bgp
  netlink
FRM  kernel m odule
AS1 BGP peer
AS2 BGP peer
AS3 BGP peer
user
kernel
FRM H drCache
GrpNum
G 1
G 2
…
Nexthops
[AS1, HDR1]
[AS2, HDR2]
[AS3, HDR3]
[AS4, HDR4]
[AS5, HDR5]
…
LocalG rpM em bers
GrpNum
M emberIP
G 3
G 4
…
xxx.xxx.xxx.xxx
xxx.xxx.xxx.xxx
xxx.xxx.xxx.xxx
xxx.xxx.xxx.xxx
xxx.xxx.xxx.xxx
…
BG PPeerTable
AsNum
AS1
AS2
AS3
…
NexthopIP
xxx.xxx.xxx.xxx
xxx.xxx.xxx.xxx
xxx.xxx.xxx.xxx
…
LocalAsNum
ASLocal
Figure 7: Software architecture of the FRM prototype
compute the corresponding number of shim headers as above. Fig-
ure 6 plots the cache size for increasing A. If we assume on the
order of several hundred megabytes of RAM on line cards, then we
see that Rs could support line-card-only forwarding for upto several
hundred thousand groups and over a million groups using the above
optimizations. The initial sub-linear scaling trend is because cache
requirements for highly popular groups dominate the initial cache
size while the later linear scaling reﬂects our limit on the minimum
group size. We note that our tests are stressful in that groups 1-25
all have over 10 million users; i.e., every domain has 25 groups
with sources simultaneously multicasting the entire Internet.
In summary, caching should allow source border routers to han-
dle forwarding in the line cards for at least several hundred thou-
sand groups.
7.
IMPLEMENTATION
We have built a prototype FRM router that runs under the Linux
operating system using the eXtensible Open Router Platform (XORP)
[46]. Figure 7 illustrates the overall structure of the FRM pro-
totype. A Linux kernel module implements the FRM forwarding
plane and a user-level component manages group membership state
and propagates membership updates to neighboring ASes. The
user-level module runs in the execution context of the XORP BGP
daemon (xorp bgp) and communicates with the kernel-side FRM
module via the Linux netlink mechanism. At kernel level, the
FRMHdrCache table caches forwarding state for groups that have
sources in the router’s local domain while the BGPPeerTable
holds the encoded AS edges used to forward transit packets. The
GRP BFs are stored in the BGP RIB in XORP. Our prototype cur-
rently lacks support for interfacing FRM to intra-domain multicast
routing protocols; instead, as an interim mechanism, we maintain
a local table (LocalGrpMembers) that stores the IP addresses
of local group members. A more scalable implementation might,
for example, store the IP address of the group’s local RP. We mod-
ify the designated router (DR) side of the IGMP implementation
to insert/remove shim headers. Endhosts are thus unchanged and
FRM routers only update shim headers. Our impementation adds
3500 lines of code to the Linux kernel and 1900 lines to the BGP
daemon.
7.1 Packet Processing
The kernel delivers incoming multicast packets to the FRM mod-
ule. If the source address indicates that the packet originated in the
router’s local domain, then we ﬁrst check for forwarding state in
the FRMHdrCache cache.
Source domain: cache miss. In the event of a cache miss,
the kernel upcalls to xorp bgp to request the multicast tree for
the packet’s destination group. xorp bgp responds with a set of
structures of the form ASx : SubTreex, where ASx is the AS number
of a direct child node and SubTreex is a list of edges in the subtree
at ASx. The kernel parses the daemon’s response and constructs the
FRM shim headers for every ASx.
Our shim header consists of 32 control bits, followed by the
TREE BF. The ﬁrst 4 control bits hold the number of bloom ﬁlter
hash functions, followed by 4 bits for the length of the TREE BF in
multiples of 16 bytes. The next 16 bits carry a checksum computed
over the shim header; the last 8 bits are currently left for future
protocol extensions.
Once the headers are computed, a copy of the packet is made for
each ASx, its shim header updated appropriately, and then sent out.
We use an auxiliary data structure (BGPPeerTable) in the kernel
to map from the AS number of a BGP peer to its corresponding
next-hop IP address. Finally, we add the destination group address
and the set of shim headers for each ASx into FRMHdrCache. The
FRMHdrCache cache is indexed by group address and uses a basic
LRU replacement scheme.
Source domain: cache hit. In the event of a cache hit, pro-
cessing is simple – a copy of the packet is made for each ASx entry
associated with the destination group, the packet’s shim header is
updated with the appropriate shim header, and the packet sent to
ASx.
Transit domain processing. If the packet did not originate in
the router’s local domain, processing is straightforward: we decre-
ment the IP TTL, update the IP checksum and ﬁnally traverse the
BGPPeerTable checking for the presence of the edge denoted
‘ASlocal : ASx’ in the packet’s FRM header. If present, we forward
a copy of the packet to the next-hop address for ASx. As the last
step,a copy of the packet is sent to every local member listed in the
LocalGrpMembers table.
We measure the forwarding latency for the above code paths.
Our measurements were performed on a 1.8GHz IBM Thinkpad
with 1GB RAM running FRM under Linux RedHat 9, kernel level
2.4.20-8. Table 4 lists the forwarding time for packets that hit in
the FRMHdrCache cache under increasing fanout (i.e., outgoing
copies) for different payload sizes. Relative to unmodiﬁed Linux,
FRM exhibits similar scaling behavior but is always slower in the
absolute. Examination reveals this is primarily because our FRM
implementation incurs one additional buffer copy for every packet
sent – in standard multicast, an identical copy of the packet is sent
to all outgoing next hops while generates a distinct copy of the
packet (with appropriate shim header) for every neighbor and hence
replicates the original buffer.
To measure the forwarding time for packets that suffer a cache
Fanout
Linux mcast
1-byte pkts
1
128
256
512
0.4
25.4
50.7
101.2
FRM
1-byte
0.7
64.8
132.5
262.7
FRM
FRM