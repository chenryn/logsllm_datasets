In the second part of the exit survey, participants an-
swered questions about 10 resource requests that oc-
curred when the screen was off (not in use). Overall,
they were more likely to expect resource requests to oc-
cur when using their devices (µ = 3.26 versus µ = 2.66).
They also stated a willingness to block almost half of
the permission requests (49.6% of 250) when not in use,
compared to a third of the requests that occurred when
using their phones (35.2% of 423). However, neither of
these differences was statistically signiﬁcant.
6 Feasibility of Runtime Requests
Felt et al. posited that certain sensitive permissions (Ta-
ble 1) should require runtime consent [14], but in Section
4.3 we showed that the frequencies with which applica-
tions are requesting these permissions make it impracti-
cal to prompt the user each time a request occurs. In-
stead, the major mobile platforms have shifted towards a
model of prompting the user the ﬁrst time an application
requests access to certain resources: iOS does this for a
selected set of resources, such as location and contacts,
and Android M does this for “dangerous” permissions.
How many prompts would users see, if we added runtime
prompts for the ﬁrst use of these 12 permissions? We an-
alyzed a scheme where a runtime prompt is displayed at
most once for each unique triplet of (application, permis-
sion, application visibility), assuming the screen is on.
With a na¨ıve scheme, our study data indicates our partic-
ipants would have seen an average of 34 runtime prompts
(ranging from 13 to 77, σ=11). As a reﬁnement, we pro-
pose that the user should be prompted only if sensitive
data will be exposed (Section 4.3), reducing the average
number of prompts to 29.
Of these 29 prompts, 21 (72%) are related to location.
Apple iOS already prompts users when an application ac-
cesses location for the ﬁrst time, with no evidence of user
habituation or annoyance. Focusing on the remaining
prompts, we see that our policy would introduce an aver-
age of 8 new prompts per user: about 5 for reading SMS,
1 for sending SMS, and 2 for reading browser history.
Our data covers only the ﬁrst week of use, but as we only
prompt on ﬁrst use of a permission, we expect that the
number of prompts would decline greatly in subsequent
weeks, suggesting that this policy would likely not intro-
duce signiﬁcant risk of habituation or annoyance. Thus,
our results suggest adding runtime prompts for reading
SMS, sending SMS, and reading browser history would
be useful given their sensitivity and low frequency.
Our data suggests that taking visibility into account is
important.
If we ignore visibility and prompted only
once for each pair of (application, permission), users
would have no way to select a different policy for when
the application is visible or not visible. In contrast, “ask-
on-ﬁrst-use” for the triple (application, permission, visi-
bility) gives users the option to vary their decision based
on the visibility of the requesting application. We evalu-
ated these two policies by analyzing the exit survey data
(limited to situations where the screen was on) for cases
where the same user was asked twice in the survey about
situations with the same (application, permission) pair
or the same (application, permission, visibility) triplet,
to see whether the user’s ﬁrst decision to block or not
matched their subsequent decisions. For the former pol-
USENIX Association  
24th USENIX Security Symposium  509
11
icy, we saw only 51.3% agreement; for the latter, agree-
ment increased to 83.5%. This suggests that the (applica-
tion, permission, visibility) triplet captures many of the
contextual factors that users care about, and thus it is rea-
sonable to prompt only once per unique triplet.
A complicating factor is that applications can also run
even when the user is not actively using the phone. In
addition to the 29 prompts mentioned above, our data
indicates applications would have triggered an average
of 7 more prompts while the user was not actively using
the phone: 6 for location and one for reading SMS. It
is not clear how to handle prompts when the user is not
available to respond to the prompt: attribution might be
helpful, but further research is needed.
6.1 Modeling Users’ Decisions
We constructed several statistical models to examine
whether users’ desire to block certain permission re-
quests could be predicted using the contextual data that
we collected.
If such a relationship exists, a classiﬁer
could determine when to deny potentially unexpected
permission requests without user intervention. Con-
versely, the classiﬁer could be used to only prompt the
user about questionable data requests. Thus, the response
variable in our models is the user’s choice of whether to
block the given permission request. Our predictive vari-
ables consisted of the information that might be available
at runtime: permission type (with the restriction that the
invoked function exposes data), requesting application,
and visibility of that application. We constructed sev-
eral mixed effects binary logistic regression models to
account for both inter-subject and intra-subject effects.
6.1.1 Model Selection
In our mixed effects models, permission types and the
visibility of the requesting application were ﬁxed effects,
because all possible values for each variable existed in
our data set. Visibility had two values: visible (the user
is interacting with the application or has other contextual
cues to know that it is running) and invisible. Permission
types were categorized based on Table 5. The application
name and the participant ID were included as random ef-
fects, because our survey data did not have an exhaustive
list of all possible applications a user could run, and the
participant has a non-systematic effect on the data.
Table 6 shows two goodness-of-ﬁt metrics: the Akaike
Information Criterion (AIC) and Bayesian Information
Criterion (BIC). Lower values for AIC and BIC repre-
sent better ﬁt. Table 6 shows the different parameters
included in each model. We found no evidence of inter-
action effects and therefore did not include them. Visual
inspection of residual plots of each model did not reveal
obvious deviations from homoscedasticity or normality.
Predictors
UserCode
Application
Application
UserCode
Permission
Application
UserCode
Visibility
Application
UserCode
Permission
Visibility
Application
UserCode
UserCode
Application
Application
UserCode
Permission
Application
UserCode
AIC
490.60
545.98
BIC
498.69
554.07
Screen State
Screen On
Screen On
491.86
503.99
Screen On
494.69
527.05
Screen On
481.65
497.83
Screen On
484.23
520.64
Screen On
245.13
349.38
252.25
356.50
Screen Off
Screen Off
238.84
249.52
Screen Off
235.48
263.97
Screen Off
Table 6: Goodness-of-ﬁt metrics for various mixed ef-
fects logistic regression models on the exit survey data.
We initially included the phone’s screen state as another
variable. However, we found that creating two separate
models based on the screen state resulted in better ﬁt
than using a single model that accounted for screen state
as a ﬁxed effect. When the screen was on, the best ﬁt
was a model including application visibility and appli-
cation name, while controlling for subject effects. Here,
ﬁt improved once permission type was removed from the
model, which shows that the decision to block a permis-
sion request was based on contextual factors: users do
not categorically deny permission requests based solely
on the type of resource being accessed (i.e., they also ac-
count for their trust in the application, as well as whether
they happened to be actively using it). When the screen
was off, however, the effect of permission type was rela-
tively stronger. The strong subject effect in both models
indicates that these decisions vary from one user to the
next. As a result, any classiﬁer developed to automati-
cally decide whether to block a permission at runtime (or
prompt the user) will need to be tailored to that particular
user’s needs.
6.1.2 Predicting User Reactions
Using these two models, we built two classiﬁers to make
decisions about whether to block any of the sensitive per-
mission requests listed in Table 5. We used our exit sur-
vey data as ground truth, and used 5-fold cross-validation
to evaluate model accuracy.
510  24th USENIX Security Symposium 
USENIX Association
12
We calculated the receiver operating characteristic
(ROC) to capture the tradeoff between true-positive and
false-positive rate. The quality of the classiﬁer can be
quantiﬁed with a single value by calculating the area un-
der its ROC curve (AUC) [23]. The closer the AUC gets
to 1.0, the better the classiﬁer is. When screens were on,
the AUC was 0.7, which is 40% better than the random
baseline (0.5). When screens were off, the AUC was 0.8,
which is 60% better than a random baseline.
7 Discussion
During the study, 80% of our participants deemed at least
one permission request as inappropriate. This violates
Nissenbaum’s notion of “privacy as contextual integrity”
because applications were performing actions that deﬁed
users’ expectations [33]. Felt et al. posited that users may
be able to better understand why permission requests are
needed if some of these requests are granted via runtime
consent dialogs, rather than Android’s previous install-
time notiﬁcation approach [14]. By granting permissions
at runtime, users will have additional contextual infor-
mation; based on what they were doing at the time that
resources are requested, they may have a better idea of
why those resources are being requested.
We make two primary contributions that system design-
ers can use to make more usable permissions systems.
We show that the visibility of the requesting applica-
tion and the frequency at which requests occur are two
important factors in designing a runtime consent plat-
form. Also, we show that “prompt-on-ﬁrst-use” per
triplet could be implemented for some sensitive permis-
sions without risking user habituation or annoyance.
Based on the frequency with which runtime permissions
are requested (Section 4), it is infeasible to prompt users
every time. Doing so would overwhelm them and lead to
habituation. At the same time, drawing user attention to
the situations in which users are likely to be concerned
will lead to greater control and awareness. Thus, the
challenge is in acquiring their preferences by confronting
them minimally and then automatically inferring when
users are likely to ﬁnd a permission request unexpected,
and only prompting them in these cases. Our data sug-
gests that participants’ desires to block particular permis-
sions were heavily inﬂuenced by two main factors: their
understanding of the relevance of a permission request to
the functionality of the requesting application and their
individual privacy concerns.
Our models in Section 6.1 showed that individual char-
acteristics greatly explain the variance between what dif-
ferent users deem appropriate, in terms of access to pro-
tected resources. While responses to privacy scales failed
to explain these differences, this was not a surprise, as the
disconnect between stated privacy preferences and be-
haviors is well-documented (e.g., [1]). This means that
in order to accurately model user preferences, the sys-
tem will need to learn what a speciﬁc user deems in-
appropriate over time. Thus, a feedback loop is likely
needed: when devices are “new,” users will be required
to provide more input surrounding permission requests,
and then based on their responses, they will see fewer
requests in the future. Our data suggests that prompting
once for each unique (application, permission, applica-
tion visibility) triplet can serve as a practical mechanism
in acquiring users’ privacy preferences.
Beyond individual subject characteristics (i.e., personal
preferences), participants based their decisions to block
certain permission requests on the speciﬁc applications
making the requests and whether they had contextual
cues to indicate that the applications were running (and
therefore needed the data to function). Future systems
could take these factors into account when deciding
whether or not to draw user attention to a particular re-
quest. For example, when an application that a user is
not actively using requests access to a protected resource,
she should be shown a runtime prompt. Our data indi-
cates that, if the user decides to grant a request in this
situation, then with probability 0.84 the same decision
will hold in future situations where she is actively using
that same application, and therefore a subsequent prompt
may not be needed. At a minimum, platforms need to
treat permission requests from background applications
differently than those originating from foreground ap-
plications. Similarly, applications running in the back-
ground should use passive indicators to communicate
when they are accessing particular resources. Platforms
can also be designed to make decisions about whether
or not access to resources should be granted based on
whether contextual cues are present, or at its most basic,
whether the device screen is even on.
Finally, we built our models and analyzed our data within
the framework of what resources our participants be-
lieved were necessary for applications to correctly func-
tion. Obviously, their perceptions may have been incor-
rect: if they better understood why a particular resource
was necessary, they may have been more permissive.
Thus, it is incumbent on developers to adequately com-
municate why particular resources are needed, as this im-
pacts user notions of contextual integrity. Yet, no mecha-
nisms in Android exist for developers to do this as part of
the permission-granting process. For example, one could
imagine requiring metadata to be provided that explains
how each requested resource will be used, and then auto-
matically integrating this information into permission re-
quests. Tan et al. examined a similar feature on iOS that
allows developers to include free-form text in runtime
USENIX Association  
24th USENIX Security Symposium  511
13
permission dialogs and observed that users were more
likely to grant requests that included this text [41]. Thus,
we believe that including succinct explanations in these
requests would help preserve contextual integrity by pro-
moting greater transparency.
In conclusion, we believe this study was instructive in
showing the circumstances in which Android permission
requests are made under real-world usage. While prior
work has already identiﬁed some limitations of deployed
mobile permissions systems, we believe our study can
beneﬁt system designers by demonstrating several ways
in which contextual integrity can be improved, thereby
empowering users to make better security decisions.
Acknowledgments
This work was supported by NSF grant CNS-1318680,
by Intel through the ISTC for Secure Computing, and by
the AFOSR under MURI award FA9550-12-1-0040.
References
[1] ACQUISTI, A., AND GROSSKLAGS, J. Privacy and rational-
ity in individual decision making.
IEEE Security & Privacy
(January/February 2005), 24–30. http://www.dtc.umn.edu/
weis2004/acquisti.pdf.
[2] ALMOHRI, H. M., YAO, D. D., AND KAFURA, D. Droidbarrier:
Know what is executing on your android. In Proc. of the 4th ACM
Conf. on Data and Application Security and Privacy (New York,
NY, USA, 2014), CODASPY ’14, ACM, pp. 257–264.
System permissions.
[3] ANDROID DEVELOPERS.
http:
//developer.android.com/guide/topics/security/
permissions.html. Accessed: November 11, 2014.
[4] ANDROID DEVELOPERS.
Common Intents.
https://
developer.android.com/guide/components/intents-
common.html, 2014. Accessed: November 12, 2014.
Content Providers.
[5] ANDROID DEVELOPERS.
http:
//developer.android.com/guide/topics/providers/
content-providers.html, 2014. Accessed: Nov. 12, 2014.
[6] AU, K. W. Y., ZHOU, Y. F., HUANG, Z., AND LIE, D. Pscout:
Analyzing the android permission speciﬁcation. In Proc. of the
2012 ACM Conf. on Computer and Communications Security
(New York, NY, USA, 2012), CCS ’12, ACM, pp. 217–228.
[7] BARRERA, D., CLARK,
J., MCCARNEY, D., AND VAN
OORSCHOT, P. C. Understanding and improving app installation
security mechanisms through empirical analysis of android.
In
Proceedings of the Second ACM Workshop on Security and Pri-
vacy in Smartphones and Mobile Devices (New York, NY, USA,
2012), SPSM ’12, ACM, pp. 81–92.
[8] BARRERA, D., KAYACIK, H. G. U. C., VAN OORSCHOT, P. C.,
AND SOMAYAJI, A. A methodology for empirical analysis of
permission-based security models and its application to android.
In Proc. of the ACM Conf. on Comp. and Comm. Security (New
York, NY, USA, 2010), CCS ’10, ACM, pp. 73–84.
[9] BODDEN, E. Easily instrumenting android applications for secu-
rity purposes. In Proc. of the ACM Conf. on Comp. and Comm.
Sec. (NY, NY, USA, 2013), CCS ’13, ACM, pp. 1499–1502.
[10] BUCHANAN, T., PAINE, C., JOINSON, A. N., AND REIPS, U.-
D. Development of measures of online privacy concern and pro-
tection for use on the internet. Journal of the American Society
for Information Science and Technology 58, 2 (2007), 157–165.
[11] BUGIEL, S., HEUSER, S., AND SADEGHI, A.-R. Flexible and
ﬁne-grained mandatory access control on android for diverse se-
curity and privacy policies. In Proc. of the 22nd USENIX Security
Symposium (Berkeley, CA, USA, 2013), SEC’13, USENIX As-