rate in model training. We choose the hyperparameters by
following the standard procedure in machine learning. Specif-
ically, we leverage a small dataset randomly drawn from the
training dataset and train with different hyperparameters. By
comparing different conﬁgurations, we can pick optimal hy-
perparameter values to train on the whole dataset. We then
evaluate the impact of each parameter by varying them inde-
pendently. In Section 3, we also mentioned that ELISE can
use partial data for training and pre-trained models to opti-
mize its runtime. We also evaluate the effects of these two
techniques in this section.
Split ﬁle size. When compressing a large ﬁle, ELISE automat-
ically divides the ﬁle into smaller ones and compresses them
independently. By default, we split large ﬁles into 10 smaller
ones and make sure their sizes are between 0.7 GB to 4.0 GB
(constrained by our memory). The size is conﬁgurable. To test
the effect of this parameter, we use Lin-7.7G as our dataset,
and conﬁgure the size from 0.1 GB to 1.2 GB. The results are
shown in Figure 6(a). The X-axis shows the split size. The
blue line shows the ﬁnal compression ratio (Y-axis on the left),
and the red line reﬂects the accuracy of the trained model (sec-
ond Y-axis on the right). Lastly, the green line shows the ratio
of compressed model over the total ﬁnal artifacts. Because we
use a ﬁxed model architecture, the model size is a ﬁxed value.
When the split size is small and model accuracy is similar, the
size of the model itself becomes the dominant factor of the
total ﬁnal size (and hence the compression ratio). This green
line is used to study this phenomenon.
From this ﬁgure, we can see that when the size is larger than
0.6 GB, the compression ratio reaches a saturation state. This
is because the accuracy of DNN cannot be further optimized
as indicated by the red line. With the same accuracy of the
model, the probability distribution of individual letters will not
change much. As a result, we end up with similar encodings
for individual letters which leads to saturation in compression
ratios. On the other hand, when the size is smaller than 0.6
GB, the DNN encoder has high accuracy but the compression
ratio is high. Notice that for different split sized ﬁles, the
DNN model has the same architecture and size, which is a
dominant factor when ﬁles are small. This leads to such high
compression ratios. On different log ﬁles, the saturation
points are slightly different, but such a phenomenon exists on
all of them. Notice that the results do not mean that ELISE
will be sensitive to the split size in practice. This is because
in the real world, common log ﬁle sizes are far larger than 0.6
GB, which means the compression ratio will be stable and
small.
Batch size. Batch size is a typical parameter that can affect
the DNN training and prediction including its training time,
accuracy, resources consumption, and transitively, compres-
USENIX Association
30th USENIX Security Symposium    3033
(a) GPU Memory Cost of ELISE.
(b) Main Memory Cost of ELISE.
(c) Time Costs of Training.
(d) Memory Costs of Training.
(e) Accuracy of Training.
(f) Compression Ratios.
Figure 5: Memory Cost of ELISE and Evaluation Results with Different Batch Size Settings (T, C and D are shorts for training, compression
and decompression) .
sion ratio. To study its effects, we use different batch sizes on
Lin-0.8G dataset and collected the time cost, memory usage,
model accuracy and the size of compressed ﬁles. The batch
size varies from 512 to 65,536. As a comparison, we also
do the same experiments on time costs and memory costs
evaluation using DeepZip. The experiment results are shown
in Figure 5.
Overall, the model accuracy (0.86~0.88) and the compres-
sion ratios (0.90%~1.09%) are impervious to the change of
batch size. This is mainly because of our preprocessing, which
converts training on natural language artifacts to numerical
values. This makes training easier and scalable to larger train-
ing batch sizes. Here we do not compare ELISE with DeepZip
as similar results have been presented in Section 4.2.
On the other hand, Figure 5(c) and Figure 5(d) show that
batch size impacts training time and GPU memory occupa-
tion. With the increase of batch size from 512 to 65,536, the
GPU memory occupation rises from 736 MB to 17,008 MB
and the training time decreases from 77.40 to 16.07 minutes
per epoch. Similarly, as the batch size increases, DeepZip
consumes more GPU memory but spends less time in train-
ing. Comparing the two methods, the time cost of DeepZip
is signiﬁcantly higher than that of ELISE, but its GPU mem-
ory usages are comparable. The results reveal that increasing
batch size could signiﬁcantly speed up the training process
and the improvement is not linear. The training speed does
not increase any more after the batch size is large enough.
This conclusion is consistent with previous work [18].
Learning Rates. Learning rates can affect the model accu-
Table 2: Evaluation Results with Different Learning Rate Settings.
Learning Rate
Compression Ratio (%)
Accuracy
ELISE
DeepZip
ELISE
DeepZip
0.1
0.01
0.001
0.0001
0.00001
6.11
0.97
0.91
1.12
1.51
60.91
2.85
1.69
1.85
2.42
0.30
0.87
0.88
0.86
0.82
0.20
0.95
0.97
0.97
0.96
racy and transitively, the ﬁnal compression ratio. To measure
its effects, we adopt 5 different learning rates (from 0.1 to
0.00001) to train DNN models and collect ﬁnal compression
ratios on the Lin-0.8G log. The results are shown in Table 2.
As shown in the table, larger learning rates lead to lower pre-
diction accuracy of DNN and higher ﬁnal compression ratios,
which is consistent with existing work. This is because higher
learning makes it easier to skip optimal values during opti-
mization, leading to non-optimal results. On the other hand,
small learning rates result in slow convergence and make it
hard to skip local optimal values, which is also undesired. To
solve this problem, we follow the standard recommendations
in ML community, and use adaptive optimization methods
(i.e., optimizer will dynamically adjust learning rates) with a
relatively large learning rate 0.001 as our default setting.
Partial data training. Due to the high redundancy in log
ﬁles, it is possible to use only part of the data for training
and perform the compression on the whole data ﬁle (Sec-
tion 3). To evaluate its practical effects, we split a 200 MB
ﬁle from Lin-7.7G and then divide it into 10 equal parts, and
3034    30th USENIX Security Symposium
USENIX Association
Lin-0.8GWin-0.7GHtp-2.4GFtp-0.9GSql-1.0GBSD-0.8G05001000150020002500Mem (MB)TCDLin-0.8GWin-0.7GHtp-2.4GFtp-0.9GSql-1.0GBSD-0.8G050001000015000Mem (MB)TCD5121,0242,0484,0968,19216,38432,76865,536Batch Size0200400600Cost (Min)ELISEDeepZip5121,0242,0484,0968,19216,38432,76865,536Batch Size050001000015000Memory (MB)ELISEDeepZip5121,0242,0484,0968,19216,38432,76865,536Batch Size0.00.20.40.60.81.0AccuracyELISE5121,0242,0484,0968,19216,38432,76865,536Batch Size0.01.02.03.0Compression Ratio (%)ELISE(a) Results with Different Split Sizes (ELISE).
(b) Finetuning and Training with Partial Data (ELISE).
(c) Finetuning with Entire Data ( ELISE).
(d) Results with Different Split Sizes(DZ).
(e) Finetuning and Training with Partial Data (DZ).
(f) Finetuning with Entire Data (DZ).
Figure 6: Ablation Study Results (DZ is short for DeepZip).
use different percentage of the whole data, i.e., 10%, 20%, ···
,100%, to train the encoder. For each compression, we mea-
sure the ﬁnal compression ratio on the whole ﬁle. The red line
in Figure 6(b) shows the results for different data split size
settings. We observe that when we use more training data, the
compression ratio decreases. Meanwhile, when the data size
increases, the training takes more time because we need to
train the DNN model on more data. How to determine the size
of partial data to use is a trade-off in real world scenarios with
other constraints. We would like to mention that this experi-
ment is done with ﬁle size smaller than 0.6 GB. Namely, the
compression has not reached the saturation point. As a result,
training with different percentages of the ﬁle will lead to the
model accuracy change. We believe this experiment is still
valuable even though ELISE is not sensitive to training size
change when it is larger than a threshold value (i.e., 0.6 GB).
This is because training with partial data is an optimization
aiming for using less time and resources to achieve acceptable
results. In time sensitive scenarios, system administrators can
choose this optimization to speed up the system while having
non-optimal compression results.
Finetuning with entire data. In a real world scenario, it is
highly likely that we only need to store a limited number
of logs for a long time. As a result, for a given data ﬁle, it
is highly likely that we have encountered similar ﬁles many
times. Therefore, we can use pre-trained model plus ﬁnetun-
ing to compress the new ﬁle, which is much faster. To validate
this idea, we pre-train a model on one of the small ﬁles and
use it to compress another 9 different ﬁles of the same type.
For each new ﬁle, the model is ﬁnetuned only for one epoch,
and the compression ratios are measured. The ﬁnetuning re-
sults are shown in Figure 6(c). The X-axis (i.e., L1~ L9)
means different pre-trained models, and the Y-axis is the com-
pression ratio. The results show that the models ﬁnetuned on
different ﬁles have similar compression ratios (i.e., on average
0.92%) and are comparable with results obtained by training
on the whole data (see Figure 4). This indicates that using
pre-trained models can effectively reduce training costs while
maintaining similar compression ratios.
Finetuning with partial data. A natural way to further opti-
mize the runtime of ELISE is to use a pre-trained model and
only ﬁnetune it with partial data. To test this, we combine
these two methods and evaluate its performance. The ﬁne-
tuned model is the same as previous experiments, and ﬁle
splitting setting is from 10% to 100%. The results are shown
as blue line in Figure 6(b). From this ﬁgure, we can get similar
conclusions with previous experiments: compression ratios
drop along with the increase of training data. However, the
compression ratios are lower than those in the previous exper-
iments under the same condition (i.e., use same sized partial
data to train the model). When we use partial data to train
the model, Figure 6(b) shows that training with a pre-trained
model always yields better results when the percentage of
training data is lower than 80%. This demonstrates the advan-
tages of using a pre-trained model.
4.4.2 Ablation Study for DeepZip
We also perform an ablation study for DeepZip. Due to its
inefﬁciency, all experiments are performed on two portions
USENIX Association
30th USENIX Security Symposium    3035
0.10.20.30.40.50.60.70.80.91.01.11.2Split Size (GB)0123Ratio (%)0.880.890.900.91AccuracyCompression RatioAccuracyModel Size / Total Size102030405060708090100Percentage of Training Data (%)0510Compression Ratio (%)Partial Data TrainingFinetuning with Partial DataL1L2L3L4L5L6L7L8L9Pre-trained Models0.00.51.0Compression Ratio (%)0.0250.050.100.150.200.250.30Split Size (GB)123Ratio (%)0.970.970.98AccuracyCompression RatioAccuracyModel Size / Total Size102030405060708090100Percentage of Training Data (%)2.04.0Compression Ratio (%)Partial Data TrainingFinetuning with Partial DataL1L2L3L4L5L6L7L8L9Pre-trained Models0.02.55.07.5Compression Ratio (%)of the Lin-7.7G and Lin-16.1G log ﬁles.
Split ﬁle size. To understand the effects of split ﬁle size for
DeepZip, we ﬁrst split the Lin-7.7G ﬁle into different sizes
(from 0.025 GB to 0.30 GB). Then, we train DNN models
on these split ﬁles, compress them and measure compression
ratios. The result is shown in Figure 6(d), which has a con-
sistent format with Figure 6(a): the blue, red and green lines
denote the compression ratio, accuracy of the model and the
ratio of model size over the total size (after compression),
respectively. Similar to ELISE, with the increase of split size,
all three values decrease and then reach a saturation point.
What is different is that the absolute values of the split sizes
are different. For ELISE, it reaches the saturation point when
the size is around 0.6 GB, and for DeepZip, it is smaller than
that. This is because ELISE trains the model on numerical
values while DeepZip trains the model on discrete values. On
the one hand, using raw English letters (i.e., discrete values)
makes training harder to converge and takes a longer time. It
also does not capture all redundancies like ELISE, leading to
high compression ratios. On the other hand, it can identify all
repeated substrings with a small size of data, because prepro-
cessings in ELISE have made the distribution of individual
letters more complex. For example, the letter “0” in different
positions of ELISE preprocessed log has different meanings,
which is interpreted by deﬁned rules and reference table. As
a result, DeepZip reaches the saturation point with less data
than ELISE.
Learning Rates. To measure the effect of learning rates, we
train models with different learning rate settings, from 0.1 to
0.00001 on the same ﬁle, and measure the compression ratio.
Results are shown in Table 2. The results show that, using a
large learning rate can signiﬁcantly increase the compression
ratio. When we use a smaller learning rate, the compression
ratio becomes smaller and then larger. This observation is
consistent with the ﬁndings of ELISE, and further proves that
a large learning rate and a very small learning rate are not
practical.
Partial data training. We also evaluate the effects of
DeepZip training with partial data. Speciﬁcally, we ﬁrst split
the log ﬁle into 10 small ones ranging from 10% to 100% of
the original one. Then, we train DNN models on these small
ﬁles and measure the compression ratio using the original