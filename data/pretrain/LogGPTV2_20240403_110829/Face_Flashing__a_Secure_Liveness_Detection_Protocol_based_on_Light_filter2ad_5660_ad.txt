shape with human faces. More precisely, its abstract result
should like a transpose of “H” (Fig 9c). And this stereo object
needs to make expression according to our instructions. Even
if the adversary can achieve these, there is no promise they can
deceive our strong neural network modal every time. And there
is no chance for the adversary to generate a response with low
quality. The high recall of our model will be demonstrated in
next section.
The above two conundrums provide the security guarantee
on face veriﬁcation.
D. Security against typical attacks
Obviously, Face Flashing can defeat traditional attacks like
photo-based attacks. Here we discuss its defenses against three
typical advanced attacks:
Ofﬂine Attacks. An ofﬂine attack is to record responses of
previous authentications, and replay them to attack the current
authentication. However, this attack is impossible to fool our
protocol. First, the hitting possibility is small, as we require
responses match all the challenges. Concretely, if we use 8
different colors and present 10 lighting challenges, the hitting
possibility will be less than 10−9. Second, even if adversaries
have successfully guessed the correct challenge sequence, dis-
playing responses legitimately is difﬁcult. Because displaying
on screens will produce the intensely specular reﬂection that is
easily detectable, and displayed by projecting responses onto
a forged object leads to high stdd that also can be detected,
as adversaries cannot precisely predict the length of every
refreshing period of the screen.
MFF Attacks. An MFF attack is to forge the response by
merging victim’s facial information and the currently received
challenge. However, this attack is also useless, because it is
hard to deceive our timing and face veriﬁcations simultane-
ously. First, to deceive our face veriﬁcation needs forging high-
quality responses which is difﬁcult and time-consuming. Par-
ticularly, high-quality forgery requires reconstructing the 3D
model of victim’s face and simulating the reﬂection process.
Second, to deceive our timing veriﬁcation needs to complete
the above forgery quickly. Actually, the available time is 1
240 /2
second for attacking a 60 Hz screen (Section VI-B). Third,
even if adversaries can quickly produce a perfectly forged
response, displaying the response is not allowed (see the
preceding paragraph).
3D-Mask Attacks. A 3D-mask attack is to wear a 3D mask
to impersonate the victim. However, this attack is impractical.
First, this attack needs to build an accurate mask that can
fool our face recognition module, which is difﬁcult3. Second,
the legitimate mask is hard to be 3D printed. As the printed
mask needs to have the similar reﬂectance of human skin and
be so ﬂexible that adversaries can wear it to make instructed
expressions. While, the available 3D printed materials are non-
ﬂexible under the requirement of Fused Deposition Modeling
(FDM), the prevalent 3D print technology. Besides, the small-
est diameter of available nozzles is 0.35mm that will produce
coarse surfaces, and coarse surfaces can be distinguished from
human skin.
In sum, Face Flashing is powerful
to defeat advanced
attacks, especially attacks similar to the ones mentioned above.
VI.
IMPLEMENTATION AND EVALUATION
In this section, we introduce the source of our collected
data at the beginning, then present implementations and eval-
uations of timing and face veriﬁcations, followed by the
evaluation on robustness. Finally, we give the computational
and storage cost when deploying our system on a smartphone
and the back-end server.
A. Data Collection
We have invited 174 participants including Asian, Euro-
pean and African. Among all participants, there are 111 males
and 63 females with ages ranging from 17 to 52. During the
experiment, participants were asked to hold a mobile phone
facing to their face and make expressions such as smiling,
blinking or moving head slightly. A button was located at the
bottom of the screen so that participants can click it to start
3Even though there is an existing study implying it
performing it in real is not easy.
is possible [19],
10
(a) mean=0.046, std=0.035
(b) mean=0.012, std=0.013
(c) mean=0.020, std=0.015
(d) mean=0.060, std=0.045
Fig. 11: Performance of 4 regression models. (a)-(d) shows
performance of model 1-4 respectively.
(and stop) the authentication/liveness detection process. When
started, the phone performs our challenge-response protocol
and records a video with its front camera. And, once started,
that button will be disabled for three seconds to ensure that
every captured video contains at least 90 frames.
In total, we collect 2274 raw videos under six different
settings (elaborated in Section VI-D). In each scenario, we
randomly select 50 videos to form the testing data set, and all
other videos then belong to the training data set.
B. Timing Veriﬁcation
In our implementation of timing veriﬁcation, we set the
height of lighting area in every lighting challenge to a constant,
i.e., u − d = 1/4, where the height of the whole screen is 1.
And we use an open source library, LIBLINEAR [6], to do the
regression with L2-loss and L2-regularization, where the T h
is set to −5.
We trained four regression models on the training set
mentioned above, and their performances over the testing data
set are shown on Fig 11. It shows that performances of model
1 and 4 are relatively poor which is reasonable in fact, because
both models handle two challenging areas (refer to Fig 6)
where the responses are weak and the keen edges also impair
the results.
To evaluate its capability on defending against attacks, we
feed forged areas (see Fig 12a) to these regression models, and
observe the results. It turns out that when enlarging the shif t
between real ROI and forged area, the estimation deviation
increases. In Fig 12b, we illustrated the relationship between
estimated meand and stdd under different values of shif t,
while regularizing the width of ROI as 1. The ﬁgure shows that
when shif t is less than 0.1, the estimation error of meand and
stdd is very small. But when the shift is 0.5, the estimation
error is around 1/4. In other words, when increasing shif t
11
(a)
(b)
Fig. 12: Attack simulation
to the half of ROI’s width, the estimated deviation could be
larger than the height of the lighting area, which states that
adversary’s opportunity window (i.e., shif t) for a successful
attack is pretty small, and our method can reliably detect such
attacks. Concretely, the acceptable delay for a benign response
is less than 1
240 /2 second for a 60 Hz screen.
Further, we investigated the delays under a real-world
setting (shown in Fig 13). In this experiment, we used two
devices: A is the authenticator (a Nexus 6 smartphone in this
example), and B is the attacker (a laptop that will reproduce the
color displayed on smartphone by simply showing the video
captured by its front camera). When the experiment begins,
the smartphone starts to ﬂash with random colors, and record
whatever is displayed on laptop screen at the same time, then
calculate the delay needed by attackers to reproduce the same
color. The same procedure will be repeated to calculate the
delays by replacing the laptop with a mirror.
Fig 13b shows the results where the blue bars are mirror’s
delays while the red bars are the laptop’s delays. The difference
between the delays means that if adversaries had used devices
other than mirrors to reproduce the reﬂected colors (i.e. re-
sponses), there should be signiﬁcant delays. This is actually
one of our major technical contribution to use light reﬂections
instead of human expressions and/or actions as the responses
to given challenges, and it can give a clear and strong timing
guarantee to differentiate genuie and fake responses.
C. Face Veriﬁcation
We use Caffe [10], an open source deep learning frame-
work, to train our neural network model used for face veriﬁ-
cation. The preliminary parameters are listed below: learning
policy is set to “multistep”, base learning rate is 0.1, gamma
is 0.1, momentum is 0.9, weight decay is 0.0001 and the max
iteration is 64000.
We ﬁrst build a set of adversarial videos in order to train
the model. These videos are made by recording the screen that
is replaying the raw video. There are 4 different screens are
recorded (Table II).
We take those frames in malicious videos as our negative
samples, and take those raw videos’ frames as positive sam-
ples. Besides, we bypass our timing veriﬁcation to eliminate
the mutual effect between these two veriﬁcation algorithms.
The experimental results are listed in the Table III, which
shows a zero false positive error with 99.2% of accuracy rate.
00.10.2abs(d)01020percentage(%)00.10.2abs(d)01020percentage(%)00.10.2abs(d)01020percentage(%)00.10.2abs(d)01020percentage(%)00.10.20.30.4shift00.050.10.150.20.25mean(d)std(d)•
Vibration. Drastic head shaking and intensive vibra-
tion also fades our performance. Especially, we will
not do so well on frames at the beginning and end of
the captured video.
The above results showed that we can detect all the attacks
with a small false negative error, which provides another secu-
rity guarantee besides the response timing mentioned above.
D. Evaluation on Robustness
There are mainly two elements that could affect the per-
formance of our proposed method: illumination and vibration.
We have carefully designed six scenarios to further investigate
their impacts.
•
•
•
•
•
•
scenario 1: We instruct participants to stand in a
continuous lighting room as motionless as possible.
And the button was hidden during the ﬁrst 15 seconds
to let participants produce a long video clip.
scenario 2: We instruct participants to take a subway
train. The vibration is intermittent and lighting condi-
tion is changing all the time.
scenario 3: We instruct participants to walk on our
campus as they usually do during a sunny day.
scenario 4: We instruct participants to hover under
penthouses during a cloudy day.
scenario 5: We instruct participants to walk downstairs
at their usual speed in rooms.
scenario 6: We instruct participants to walk down a
slope outside during nights.
We summarize the features of these scenarios in Table IV.
The results are shown on Fig 14. In ideal environments
(scenario 1), our method is perfect and the accuracy is high
as 99.83%. In normal cases (scenario 4), our method is also
excellent with the 99.17% accuracy. And the sunlight (scenario
3) causes ignorable effects on the result, as long as the frontal
camera does not face the sun directly. Comparing scenario
5 with 3, we infer the vibration causes more effect than the
sunlight. Besides, dark is a devil (scenario 6) which reduces
the accuracy to 97.33%,
the lowest one. In our method,
we cannot use the function of auto white balance (AWB)
embedded in our devices, due to the fundamental requirement
of our method. Adjusting the sensitivity of sensors, we just
can limitedly reduce the effect of saturation, while keeping
enough effectiveness. Limited by this constraint, the result is
acceptable. For the complex case (scenario 2), the accuracy,
97.83%, is not bad. In this scenario, our device is being tested
by many factors including unpredictable impacts, glare lamps
and quickly changed shadows.
To further explore the impacts caused by vibrations, we
built another experiment where we leveraged the six parame-
ters generated by face tracking algorithm, and assembled them
as a single value, ν, to measure the intensity of vibration.
The details are illustrated in Algorithm 2, where {Tj} is the
sequence of the transformation matrix (Section IV-B2) and N
is the number of frames.
12
(a) scenario
(b) results
Fig. 13: Primitive attack.
TABLE II: Four different screens.
Screen
HUAWEI P10
iPhone SE
AOC Monitor (e2450Swh)
EIZO Monitor (ev2455)
Resolution
1920*1080
1136*640
1920*1080
1920*1200
Pixel Density
432(ppi)
326(ppi)
93(ppi)
95(ppi)
1
2
3
4
When applied with the testing data set, the accuracy is
98.8%. There are only 75 frames are incorrectly labeled, with
all the negative samples labeled correctly. After analyzing these
75 frames, we found it may result from three reasons:
•
•
Illumination. When the distance between face and
screen is far and the environmental illumination is
high, the captured response will be too obscure to be
labeled correctly.
Saturation. Due to device limitations, video frames
taken in dark scenarios, will have many saturated
pixels, even having adjusted the sensitivity of optical
sensors. As described in Section IV-B1, it is necessary
to remove these saturated pixels to satisfy the formu-
las.
TABLE III: Experimental results of face veriﬁcation.
Total
Incorrect
Training Ps
20931
329
Training Ns
20931
0
Testing Ps
3000
75
Testing Ns
3000
0
0100200300400500Delays(ms)00.10.20.30.40.5PercentageTABLE IV: Features of scenes.
Illumination
Vibration
Scenario 1
good
no
Scenario 2
varying
intermittent
Fig. 14: Performance on different scenarios.
Extract face shifting, (αj, βj, γj)
Extract face rotation, (ιj, ζj, ηj)
Algorithm 2 Algorithm to measure intensity of vibration.
INPUT: {Tj}, N
OUTPUT: ν
1: for j = 1 to N do
2:
3:
4: end for
5: Calculate mean values: ¯α, ¯β, ¯γ, ¯ι, ¯ζ and ¯η
6: for i = j to N do
7:
8: end for
9: ν = std({µj})
µj = αj
¯ζ + ηj
¯η
¯α + βj
¯β + γj
¯γ + ιj
¯ι + ζj
Fig 15a shows the distribution of intensity. And Fig 15b
shows the relation between vibration intensity and accuracy.
We divided all the intensity by the maximum value. From both
ﬁgures, we can infer that vibration will produce side effects
to our method and the most drastic vibration will reduce the
accuracy to 60%. But, in general cases where the vibration is
not that big, our method can perform very well. This means
our method indeed is robust under normal vibration conditions.
Particularly, when the intensity reaches 0.5, we still hold 89%
accuracy.
(a) distribution
(b) relation
Fig. 15: Vibration effect.
In conclusion, our good robustness to vibration and illumi-
nation provides a good reliability and user experience. Besides,
it excludes a potential attack scenario where adversary naively
increases the vibration density.
E. Computational and Storage Cost
The time costs of our method depend on concrete devices.
If we run our method in the back-end server (say a laptop), the
13
Scenario 3
intense
normal
Scenario 4
normal
normal
Scenario 5
normal
intense
Scenario 6
dark
intense
time needed to deal with 300 frames is less than 1 seconds,