...
type = V_ASN1_GENERALIZEDTIME;
case UTCTIME_LENGTH:
if (type == 0) {
if (mode == V_ASN1_GENERALIZEDTIME)
return (-1);
type = V_ASN1_UTCTIME;
}
...
Listing 5: LibreSSL time ﬁeld parsing bug.
2) GnuTLS - Incorrect validation of activation time: As
shown in Listing 6, GnuTLS lacks a check for cases where
the year is set to 0. As a result, while other SSL libraries reject
a malformed certiﬁcate causing t to be 0, GnuTLS erroneously
accepts it.
1 static unsigned int check_time_status(gnutls_x509_crt_t
crt, time_t now) {
int status = 0;
time_t t = gnutls_x509_crt_get_activation_time(crt);
if (t == (time_t) - 1 || now < t) {
status |= GNUTLS_CERT_NOT_ACTIVATED;
status |= GNUTLS_CERT_INVALID;
return status;
...
2
3
4
5
6
7
8
Listing 6: GnuTLS activation time parsing error.
627
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
set of statistical techniques to drive input generation [23],
[31], [47], or leverage static and dynamic analysis to prioritize
deeper paths [55]. However, most of these tools do not
support differential testing. Finally, Chen et al.’s tool perform
differential testing of JVMs using MCMC sampling for input
generation [31]. However, their tool is domain-speciﬁc (i.e.,
requires details knowledge of the Java class ﬁles and uses
custom domain-speciﬁc mutations). Moreover, MCMC tends
to be computationally very expensive, signiﬁcantly slowing
down the input generation process. NEZHA, by contrast, uses
a fast guidance mechanism well suited for differential testing
that seeks to maximize the diversity of relative behaviors of
the test programs in search of discrepancies-inducing inputs.
Symbolic execution: Symbolic execution [43] is a white-
box technique that executes a program symbolically, computes
constraints along different paths, and uses a constraint solver to
generate inputs that satisfy the collected constraints along each
path. KLEE [26] uses symbolic execution to generate tests that
achieve high coverage for several popular UNIX applications,
however, due to path explosion, it does not scale to large
applications. UC-KLEE [43], [54] aims to tackle KLEE’s
scalability issues by performing under-constrained symbolic
execution, i.e., directly executing a function by skipping the
whole invocation path up to that function. However, this may
result in an increase in the number of false positives.
To mitigate path explosion, several lines of work utilize
symbolic execution only in portions of their analysis to aid
the testing process, and combine it with concrete inputs [27].
Another approach towards addressing the limitations of pure
symbolic execution is to outsource part of the computation
away from the symbolic execution engine using fuzzing [28],
[34]–[37], [61]. A major limitation of symbolic-execution-
assisted testing tools in the context of differential testing is
that the path explosion problem increases signiﬁcantly as the
number of test programs increase. Therefore, it is very hard
to scale symbolic execution techniques to perform differential
testing of multiple large programs.
Differential Testing: Differential testing [51] has been very
successful in uncovering semantic differences between inde-
pendent implementations with similar intended functionality.
Researchers have leveraged this approach to ﬁnd bugs across
many types of programs, such as web applications [29], differ-
ent Java Virtual Machine (JVM) implementations [31], various
security implementations of security policies for APIs [59],
compilers [65] and multiple implementations of network pro-
tocols [25]. KLEE [26] used symbolic execution to perform
differential testing, however suffers from scalability issues.
SFADiff [21] performs black-box differential testing using
Symbolic Finite Automata (SFA) learning, however, contrary
to NEZHA, can only be applied to applications such as XSS
ﬁlters that can be modeled by an SFA.
C. PDF Viewer Discrepancies
As mentioned in Section V-A, NEZHA uncovered 7 unique
discrepancies in the tested three PDF browsers (Evince, Xpdf
and MuPDF) over a total of 10 million generations. Examples
of the found discrepancies include PDF ﬁles that could be
opened in one viewer but not another and PDFs rendered with
different contents across viewers. One interesting discrepancy
includes a PDF that Evince treats as encrypted (thus opening it
with a password prompt) but Xpdf recognizes as unencrypted
(MuPDF and Xpdf abort with errors trying to render the ﬁle).
VII. RELATED WORK
Unguided Testing: Unguided testing tools generate test
inputs independently across iterations without considering the
test program’s behavior on past inputs. Domain-speciﬁc evo-
lutionary unguided testing tools have successfully uncovered
numerous bugs across a diverse set of applications [2], [40],
[42], [52], [56]. Another parallel line of work explored build-
ing different grammar-based testing tools that rely on a context
free grammar for generating test inputs [48], [50]. LangFuzz
[38] uses a grammar to randomly generate valid JavaScript
code fragments and test JavaScript VMs, while GLADE [22]
synthesizes a context-free grammar encoding the language of
valid program inputs and leverages it for fuzzing. TestEra
[49] uses speciﬁcations to automatically generate test inputs
for Java programs. lava [58] is a domain-speciﬁc language
designed for specifying grammars that can be used to generate
test inputs for testing Java VMs. Unlike NEZHA’s guided
approach, the input generation process of these tools does not
use any information from past inputs and essentially creates
new inputs at random from a prohibitively large input space.
This makes the testing process highly inefﬁcient, since large
numbers of inputs need to be generated to ﬁnd a single bug.
Guided Testing: Evolutionary testing was designed to make
the input generation process more efﬁcient by taking pro-
gram behavior information for past inputs into account, while
generating new inputs [53]. Researchers have since explored
different forms of code coverage heuristics (e.g., basic block,
function, edge, or branch coverage) to efﬁciently guide the
search for bug-inducing inputs. Coverage-based tools such
as AFL [66], libFuzzer [4], and the CERT Basic Fuzzing
Framework (BFF) [39] reﬁne their input corpus by maximizing
the code coverage with every new input added to the corpus.
However, these tools are not well suited for differential testing
as they do not exploit the relative differences across multiple
test applications. In particular, to the best of our knowledge,
NEZHA is the ﬁrst testing framework to particularly design a
path selection mechanism ﬁtted towards to differential testing.
Even if a state-of-the-art testing framework such as libFuzzer,
was modiﬁed to perform differential testing using global cov-
erage across multiple programs, it would still be outperformed
by both NEZHA’s gray-box and black-box engines, as shown
in Section V.
Another line of research builds on the observation that the
problem of new input generation from existing inputs can be
modeled as a stochastic process. These tools leverage a diverse
Chen et al. performed coverage-guided differential testing
of SSL/TLS implementations using Mucerts [32]. However,
unlike NEZHA, Mucerts requires knowledge of the partial
grammar of the X.509 certiﬁcate format and applies MCMC
algorithm on a single application (i.e., OpenSSL) to drive
628
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
its input generation. The input generation of Mucerts is very
slow requiring multiple days to generate even 10,000 inputs.
As demonstrated in Section V-A, NEZHA manages to ﬁnd 27
times more discrepancies per input.
Another similar work is Brubaker et al.’s unguided differen-
tial testing system that synthesizes frankencerts by randomly
combining parts of real certiﬁcates [24]. They use these
syntactically valid certiﬁcates to test for semantic violations
of SSL/TLS certiﬁcate validation across multiple implemen-
tations. However, unlike in NEZHA where the selection of
mutated inputs is guided by δ-diversity metrics, the creation
and selection of Frankencerts is completely unguided and
therefore signiﬁcantly inefﬁcient compared to NEZHA.
Besides testing software, researchers have applied differ-
ential testing to uncover program deviations that could lead
to malicious evasion attacks on security-sensitive programs.
Similar to the way we applied NEZHA to uncover evasion
bugs in ClamAV malware detector, Jana et al. use differential
testing (with manually crafted inputs) to look for discrepancies
in ﬁle processing across multiple antivirus scanners [41].
Recent works have applied differential testing to search for
inputs that can evade machine learning classiﬁers for malware
detection [46], [64]. However, unlike NEZHA, these projects
require a detailed knowledge of the input format.
Differential testing shares parallels with N-version program-
ming [30]. Both aim to improve the reliability of systems by
using independent implementations of functionally equivalent
programs, provided that the failures (or bugs) of the multiple
versions are statistically independent. Therefore, NEZHA’s
input generation scheme will also be helpful to efﬁciently
identify uncorrelated failures in software written under the N-
version programming paradigm. Both N-version programming
and differential testing suffer from similar limitations when
different test programs demonstrate correlated buggy behaviors
as observed by Knight et al. [44].
VIII. FUTURE WORK
We believe NEZHA is a crucial ﬁrst step towards building ef-
ﬁcient differential testing tools. However, several components
of the underlying engine offer fertile ground for future work.
Mutation Strategies: NEZHA’s current mutation strategies
are not tailored for differential testing and therefore present a
promising target for further optimization. Moreover, new gray-
box guidance mechanisms that incorporate bookkeeping of
intermediate states explored during a test program’s execution
could be used to more efﬁciently generate promising inputs.
Bug Localization: Similar improvements can be achieved
towards the problem of automated debugging and bug local-
ization. Prior research has performed bug bucketing for crash-
inducing bugs using stack trace hashes [28]. However, this
method is not suitable for semantic bugs that do not result
in crashes. Moreover, heuristics such as using the average
stack trace depth in order to locate "deeper" bugs cannot be
trivially adapted to differential testing, because the depth of
the root cause of a bug might not be correlated with the
maximum depth of the execution. One possible solution for
this problem is to utilize more complex schemes keeping
track of all successful and failed executions across the tested
applications (e.g., execution paths leading to successful and
failed states may be stored in two distinct groups. Upon a
deviation from a previously unseen behavior, one may lookup
the point at which the deviation occurred in both groups to
pinpoint the root cause.
IX. DEVELOPER RESPONSES
We have responsibly disclosed the vulnerabilities identiﬁed
in this work to the respective developers of the affected
programs. Each of our reports includes a description of the bug
alongside a Proof-of-Concept (PoC) test input and a suggested
patch. The wolfSSL team assigned the highest priority to
all the memory corruption errors we reported and addressed
all the bugs within six days of our disclosure, merging the
respective patches in wolfSSL v3.9.8. Likewise, ClamAV de-
velopers have conﬁrmed the reported bugs and are planning to
merge the relevant ﬁxes in v0.99.3. The ClamAV evasions bugs
have been assigned with CVE identiﬁers CVE-2017-6592 (XZ
archive evasion) and CVE-2017-6593 (ELF binary evasion).
GnuTLS and LibreSSL developers likewise addressed the
reported bugs within three days from our disclosure, pushing
the respective patches to upstream.
X. CONCLUSION
In this paper we design, implement, and evaluate NEZHA,
a guided differential testing tool that realizes the concept of
δ-diversity to efﬁciently ﬁnd semantic bugs in large, real-
world applications without knowing any details about the input
formats. NEZHA can generate test inputs using both δ-diversity
black-box and gray-box guidance. Our experimental results
demonstrate that NEZHA is more efﬁcient at ﬁnding discrepan-
cies than all of the guided and unguided testing frameworks we
compared it against. NEZHA discovered two evasion attacks
against the ClamAV malware detector and 764 discrepancies
between the implementations of X.509 certiﬁcate validation in
six major SSL/TLS libraries.
We have made NEZHA open-source so that the community
can continue to build on it and advance the ﬁeld of efﬁcient
differential testing for security bugs. The framework can be
accessed at https://github.com/nezha-dt.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their
valuable feedback. This work is sponsored in part by the
Ofﬁce of Naval Research (ONR) through contract N00014-15-
1-2180 and by the National Science Foundation (NSF) grants
CNS-13-18415 and CNS-16-17670. Any opinions, ﬁndings,
conclusions, or recommendations expressed herein are those
of the authors, and do not necessarily reﬂect those of the US
Government, ONR or NSF.
629
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] “Executable and Linkable Format (ELF),” http://www.skyfree.org/linux/
references/ELF_Format.pdf.
[2] “Ioactive_elf_parsing_with_melkor.pdf,” http://www.ioactive.com/pdfs/
IOActive_ELF_Parsing_with_Melkor.pdf.
[3] “Isartor test suite (terms of use & download) - pdf association,” https:
//www.pdfa.org/isartor-test-suite-terms-of-use-download/.
[4] “libFuzzer - a library for coverage-guided fuzz testing - LLVM 3.9
documentation,” http://llvm.org/docs/LibFuzzer.html.
[5] “Nezha (chinese protection god),” http://www.godchecker.com/pantheo
n/chinese-mythology.php?deity=NEZHA.
[6] “Santizercoverage - Clang 4.0 documentation,” http://clang.llvm.org/doc
s/SanitizerCoverage.html.
[7] “The EFF SSL Observatory,” https://www.eff.org/observatory.
[8] “Undeﬁned behavior sanitizer - Clang 4.0 documentation,” http://clang.
llvm.org/docs/UndeﬁnedBehaviorSanitizer.html.
[9] “Virusshare.com,” https://virusshare.com/.
[10] “Internet X.509 public key infrastructure certiﬁcate policy and certiﬁca-
tion practices framework,” http://www.ietf.org/rfc/rfc2527.txt, 1999.
[11] “The TLS protocol version 1.0,” http://tools.ietf.org/html/rfc2246, 1999.
[12] “HTTP over TLS,” http://www.ietf.org/rfc/rfc2818.txt, 2000.
[13] “System v application binary interface,” https://refspecs.linuxfoundation
.org/elf/gabi4+/contents.html, April 2001.
[14] “The Transport Layer Security (TLS) protocol version 1.1,” http://tools.
ietf.org/html/rfc4346, 2006.
[15] “Internet X.509 public key infrastructure certiﬁcate and certiﬁcate revo-
cation list (CRL) proﬁle,” http://tools.ietf.org/html/rfc5280, 2008.
[16] “The Transport Layer Security (TLS) protocol version 1.2,” http://tools.
ietf.org/html/rfc5246, 2008.
[17] “Representation and veriﬁcation of domain-based application service
identity within Internet public key infrastructure using X.509 (PKIX)
certiﬁcates in the context of Transport Layer Security (TLS),” http:
//tools.ietf.org/html/rfc6125, 2011.
[18] “The Secure Sockets Layer (SSL) protocol version 3.0,” http://tools.ietf
.org/html/rfc6101, 2011.
[19] “Xz utils,” http://tukaani.org/xz/, 2015.
[20] “The Transport Layer Security (TLS) Protocol Version 1.3,” https://tool
s.ietf.org/html/draft-ietf-tls-tls13-14, 2016.
[21] G. Argyros, I. Stais, S. Jana, A. D. Keromytis, and A. Kiayias,
“SFADiff: Automated evasion attacks and ﬁngerprinting using black-
box differential automata learning,” in Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security (CCS).
ACM, 2016, pp. 1690–1701.
[22] O. Bastani, R. Sharma, A. Aiken, and P. Liang, “Synthesizing program
input grammars,” in Proceedings of the 38th ACM SIGPLAN Conference
on Programming Language Design and Implementation (PLDI). ACM,
2017.
[23] M. Böhme, V.-T. Pham, and A. Roychoudhury, “Coverage-based grey-
box fuzzing as markov chain,” in Proceedings of
the 23rd ACM
Conference on Computer and Communications Security (CCS), 2016,
pp. 1–12.
[24] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and V. Shmatikov, “Using
frankencerts for automated adversarial testing of certiﬁcate validation in
SSL/TLS implementations,” in Proceedings of the 2014 IEEE Sympo-
sium on Security and Privacy (S&P).
IEEE Computer Society, 2014,
pp. 114–129.
[25] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and D. Song, “Towards
automatic discovery of deviations in binary implementations with appli-
cations to error detection and ﬁngerprint generation,” in 16th USENIX
Security Symposium (USENIX Security ’07). USENIX Association,
2007.
[26] C. Cadar, D. Dunbar, D. R. Engler et al., “Klee: Unassisted and
automatic generation of high-coverage tests for complex systems pro-
grams.” in 8th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), vol. 8, 2008, pp. 209–224.
[27] C. Cadar and D. Engler, “Execution generated test cases: How to make
systems code crash itself,” in International SPIN Workshop on Model
Checking of Software. Springer, 2005, pp. 2–23.
[28] S. K. Cha, M. Woo, and D. Brumley, “Program-adaptive mutational
fuzzing,” in 2015 IEEE Symposium on Security and Privacy (S&P),
May 2015, pp. 725–741.
630
[29] P. Chapman and D. Evans, “Automated black-box detection of side-
channel vulnerabilities in web applications,” in Proceedings of the 18th
ACM conference on Computer and Communications Security (CCS).
ACM, 2011, pp. 263–274.
[30] L. Chen and A. Avizienis, “N-version programming: A fault-tolerance
approach to reliability of software operation,” in Digest of Papers FTCS-
8: Eighth Annual International Conference on Fault Tolerant Computing,
1978, pp. 3–9.
[31] Y. Chen, T. Su, C. Sun, Z. Su, and J. Zhao, “Coverage-directed
differential testing of JVM implementations,” in Proceedings of the
37th ACM SIGPLAN Conference on Programming Language Design
and Implementation (PLDI). ACM, 2016, pp. 85–99.
[32] Y. Chen and Z. Su, “Guided differential testing of certiﬁcate validation
in SSL/TLS implementations,” in Proceedings of the 10th Joint Meeting
on Foundations of Software Engineering (FSE). ACM, 2015, pp. 793–
804.