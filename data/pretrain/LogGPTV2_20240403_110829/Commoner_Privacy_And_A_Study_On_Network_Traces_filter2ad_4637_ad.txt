nation service port—we expect more similarity in hosts’ behaviors,
as destination ports are often service ports, and most hosts will
have popular ports open, like port 80 and 443. Here, crowd-blending
fuzzes only 40% of all points, and commoner privacy fuzzes only
around 20%.
For query q3 (Fig. 4(c))—connection count—crowd-blending and
commoner privacy perform the same here, and fuzz 60–80% of
all points, still providing a significant advantage over differential
privacy.
Similarly, for query q4 (Fig. 4(d))—traffic volume—-crowd-blending
and commoner privacy perform similarly, and fuzz 65-95% of all
points, with commoner privacy fuzzing around 5% fewer points
than crowd-blending, and STDEV and MAD algorithms performing
the same.
5.3 Quantifying Utility Loss and Privacy Risk
We now quantify utility loss and privacy risk of each privacy mech-
anism.
Utility loss. We adopt the measure of utility loss using a nor-
malized error defined as:
N
N
i =1 | fi − ti|
i =1 |ti|
¯E =
(4)
where ti are the true outputs of a query (with no privacy protec-
tions), and fi are the outputs generated by the privacy mechanism.
This measure is a relative, cumulative difference between the true
and the fuzzed data points, with lower value denoting higher re-
search utility. It represents the fraction of the change in the outputs
due to fuzzing.
Privacy risk. We adopt the measure of privacy risk by estimat-
ing the number of output data points, which can be used to identify
individuals (hosts in our case). We assume that any data about
N
¯I =
an individual can become a quasi-identifier if it is unique to that
individual and can be detected from the mechanism’s output. We
say that original data about an individual can be detected from the
mechanism’s output if we can reverse the fuzzing, and if the data
point contains contributions of individuals that are both unique
and outliers. We identify the output data points where we can re-
verse the fuzzing as follows. We assume that we have two sets of
outputs—the true one and the fuzzed one (outputted by the privacy
mechanism). We compare the values of each data point in the fuzzed
output with all data points in the true output. If the fuzzed data
point is closest to its true data point, we say that we can reverse
the fuzzing. Our measure of privacy risk is then defined as:
i =1 reverse_and_identifiable(fi)
N
(5)
where function reverse_and_identifiable returns 1 if we can reverse
fuzzing for the data point and it contains contributions of individu-
als that are both unique and outliers. Because crowd-blending and
commoner privacy both remove outlier contributions (and crowd-
blending also removes all unique contributions), their privacy risk
measure is always zero. Differential privacy, however, releases data
with noise. If noise is not large enough, we will be able to reverse
the fuzzing and in some cases identify individuals.
Figure 5 shows both the utility loss ( ¯E) and privacy risk (¯I) for
q1–q4 outputs protected by differential privacy, as we vary ∆f /ϵ.
As expected, high values lead to high privacy risk and low utility
loss, while low values eliminate privacy risk but increase utility
loss.
Figure 6 shows the utility loss for differential privacy, crowd-
blending, and commoner privacy (STDEV and MAD) assuming
the value of ∆f /ϵ, which results in zero privacy risk. We vary the
parameter k for crowd-blending and commoner privacy. Crowd-
blending and commoner privacy both have utility loss under 1 for
all queries, while differential privacy has it in the [100, 10,000] range.
Further, for q1, commoner privacy with STDEV outlier detection
loses only 47–82% of utility, while crowd-blending consistently loses
98–99%. Similarly, for q2, commoner privacy loses consistently less
than 70–71% of utility, while crowd-blending consistently loses 98–
99%. Similarly, for queries q3 and q4, crowd-blending has around
99% utility cost, while commoner privacy significantly outperforms
crowd-blending and achieves smaller than 51% utility loss even
with K = 50 in q3 and q4.
To summarize, with comparable privacy protections, crowd-
blending and commoner privacy provide many orders of magnitude
higher utility than differential privacy. In many cases, commoner
privacy outperforms crowd-blending, halving its relative utility
cost, and providing 9–49 times higher utility.
5.4 Scalability of Query Introspection
We implemented query introspection in our Patrol system and
we now evaluate its scalability as the number of related queries
increases. We run three select queries 100 times, without optimiza-
tions proposed in Section 4.1: (qi1) histogram of all TCP packets,
grouped by source port; (qi2) same as qi1 but excluding packets
smaller than 100 bytes; (qi3) same as qi1 but excluding packets
outside a certain, small time range. The overhead of query intro-
spection grew linearly from 1% of the total query processing time
574for the second round to 35–60% of the total runtime for the 100th
round. Note that our implementation has no optimizations and
runs as a single thread. We believe that further optimization and
multi-threading could significantly reduce query introspection’s
overhead.
6 RELATED WORK
In this section we discuss work closely related to commoner privacy.
De Montjoye et al. [5] discuss the re-identifiability of common user
data, and highlight the contradicting requirements for utility and
privacy.
Privacy protections for network trace data have been studied by
many [3, 4, 18, 19, 25, 30, 31, 33, 36], mostly via sanitization and
release of sanitized data. However, we explore a different approach
where data remains with its provider.
Secure queries on network traces were sketched by us in [26],
and explored by others using differential privacy [24] or manual
vetting of queries [28]. Mittal et al. [27] further propose mediated
trace analysis. Researchers submit their annotated black-box pro-
gram and data providers repeatedly apply it to network traces with
cleverly modified IP addresses, and compare outputs to detect if
they depend on IP address data. Our focus is on preventing host
re-identification through any quasi-identifier, not limited to IP ad-
dresses.
Sweeney et al.’s k-anonymity [34] is followed by work that
utilizes those ideas. In more recent work, LeFevre et al. propose
Incognito [20] to provide full-domain generalization using the
k-anonymity model. Extensions are developed including (α, k)-
anonymity [35], l-diversity [23] and t-closeness [22], and further
applied to clustering [2], location privacy [13], etc. The main differ-
ence between k-anonymity and interactive k-anonymity is that the
first is applied to original data, which is then released, while the sec-
ond is applied to individual’s contributions to query outputs. While
k-anonymity is susceptible to a range of tracker attacks [8, 34], we
prove in Section 4.1 that commoner privacy is not.
With regard to differential privacy and its challenges, Haeberlen
et al. [16] found that a system for differential privacy called Airavat
may be manipulated to mark a whole query “not differentially
private” and abort, which can be used as proof of presence of an
individual in a dataset. Further, researchers have struggled with
how to best set the values of ϵ and the privacy budget to achieve
strong privacy guarantees [16]. He et al. [17] propose Blowfish to
automatically set those values to balance privacy and utility. Our
work achieves better utility, while it provides sufficient privacy
protection for network trace analysis.
7 CONCLUSIONS
Data privacy is a topic of increasing importance, as many of our
daily interactions generate rich datasets, which may be analyzed
and shared by data providers. Differential privacy, and lately crowd-
blending, have been proposed as mechanisms that support complex
queries over datasets, while guaranteeing strong privacy protec-
tions to individuals. Differential privacy performs well in many
scenarios but has high utility cost on long-tailed datasets. Crowd-
blending reduces this utility cost, but only when a sufficient number
of individuals have the same records in the dataset; this situation is
rare on datasets where some features have a large range of values.
We have proposed commoner privacy, and a mechanism to
achieve it—interactive k-anonymity. We have further shown how
commoner privacy can hold under query composition with careful
recording and checking of queries. Commoner privacy improves
the utility of query outputs on long-tailed and large-value-range da-
tasets, compared to differential and crowd-blending privacy. It does
so at the cost of lower privacy guarantees. Specifically, commoner
privacy cannot defend against an all-but-one adversary. But it de-
fends against an interactive adversary, who cannot learn anything
specific about an individual, nor whether an individual is present
in the dataset. While commoner privacy may not be the best match
for some applications, we believe it is a competitive approach, with
a realistic adversary model, that preserves much more research
utility than state-of-the-art approaches, and at a modest privacy
cost.
8 ACKNOWLEDGMENTS
This material is based upon work supported by the National Sci-
ence Foundation, grant number 0914780. Authors are grateful to
anonymous reviewers for their helpful comments.
REFERENCES
[1] MAWI Working Group Traffic Archive. http://tracer.csl.sony.co.jp/mawi/.
[2] G. Aggarwal, T. Feder, K. Kenthapadi, S. Khuller, R. Panigrahy, D. Thomas, and
A. Zhu. Achieving anonymity via clustering. In Proceedings of the twenty-fifth
ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,
pages 153–162. ACM, 2006.
[3] S. Coull, M. Collins, C. Wright, F. Monrose, and M. Reiter. On Web Browsing Pri-
vacy in Anonymized NetFlows. In Proceedings of the USENIX Security Symposium,
August 2007.
[4] S. Coull, C. Wright, F. Monrose, M. Collins, and M. Reiter. Playing Devil’s
Advocate: Inferring Sensitive Information from Anonymized Network Traces. In
Proceedings of the Network and Distributed System Security Symposium, February
2007.
[5] Y.-A. de Montjoye, L. Radaelli, V. K. Singh, and A. S. Pentland. Unique in the
shopping mall: On the re-identifiability of credit card metadata. High Impact
Journal, 2014.
[6] X. Deng and J. Mirkovic. Patrol homepage. http://patrol.isi.edu/.
[7] D. E. Denning. A security model for the statistical database problem. In Proceed-
ings of the Second International Workshop on Statistical Database Management,
pages 368–390. Lawrence Berkeley Laboratory, 1983.
[8] D. E. Denning, P. J. Denning, and M. D. Schwartz. The tracker: A threat to
statistical database security. ACM Transactions on Database Systems (TODS),
4(1):76–96, 1979.
[9] D. E. Denning and J. Schlörer. A fast procedure for finding a tracker in a statistical
database. ACM Transactions on Database Systems (TODS), 5(1):88–102, 1980.
[10] C. Dwork. Differential Privacy. In Proceedings of the 33rd International Colloquium
on Automata, Languages and Programming, 2006.
[11] C. Dwork. Differential privacy: A survey of results. In International Conference
on Theory and Applications of Models of Computation, pages 1–19. Springer, 2008.
[12] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. Composition attacks and
auxiliary information in data privacy. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and data mining, pages 265–273.
ACM, 2008.
[13] B. Gedik and L. Liu. A customizable k-anonymity model for protecting location
privacy. 2004.
J. Gehrke, M. Hay, E. Lui, and R. Pass. Crowd-blending privacy. In Advances in
Cryptology, pages 479–496. Springer, 2012.
I. Guttman and D. E. Smith. Investigation of rules for dealing with outliers in small
samples from the normal distribution: I: Estimation of the mean. Technometrics,
11(3):527–550, 1969.
[16] A. Haeberlen, B. C. Pierce, and A. Narayan. Differential privacy under fire. In
USENIX Security Symposium, 2011.
[17] X. He, A. Machanavajjhala, and B. Ding. Blowfish privacy: Tuning privacy-utility
trade-offs using policies. In Proceedings of the 2014 ACM SIGMOD international
conference on Management of data, pages 1447–1458. ACM, 2014.
[14]
[15]
575[28]
In
[18] E. Kohler.
Ipaggregate tool. http://www.cs.ucla.edu/~kohler/ipsumdump/
aggcreateman.html.
[22] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity
[19] E. Kohler. Ipsumdump tool. http://www.cs.ucla.edu/~kohler/ipsumdump/.
[20] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Incognito: Efficient full-domain
k-anonymity. In Proceedings of the 2005 ACM SIGMOD international conference
on Management of data, pages 49–60. ACM, 2005.
[21] C. Leys, C. Ley, O. Klein, P. Bernard, and L. Licata. Detecting outliers: Do not use
standard deviation around the mean, use absolute deviation around the median.
Journal of Experimental Social Psychology, 49(4):764–766, 2013.
and l-diversity. In ICDE, volume 7, pages 106–115, 2007.
[23] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam. l-diversity:
Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from
Data (TKDD), 1(1):3, 2007.
[24] F. McSherry and R. Mahajan. Differentially-private network trace analysis. ACM
SIGCOMM Computer Communication Review, 41(4):123–134, 2011.
[25] G. Minshall. tcpdpriv tool. http://ita.ee.lbl.gov/html/contrib/tcpdpriv.html.
J. Mirkovic. Privacy-safe network trace sharing via secure queries. In Proceedings
[26]
of the 1st ACM workshop on Network data anonymization, pages 3–10. ACM, 2008.
[27] P. Mittal, V. Paxson, R. Sommer, and M. Winterrowd. Securing mediated trace
access using black-box permutation analysis. In HotNets, 2009.
J. C. Mogul and M. Arlitt. Sc2d: An alternative to trace anonymization.
Proceedings of the SIGCOMM 2006 Workshop on Mining Network Data, 2006.
[29] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in
private data analysis. In Proceedings of the thirty-ninth annual ACM symposium
on Theory of computing, pages 75–84. ACM, 2007.
[30] R. Pang, M. Allman, V. Paxson, and J. Lee. The devil and packet trace anonymiza-
tion. ACM SIGCOMM Computer Communications Review, 36(1):29—38, 2006.
[31] R. Pang and V. Paxson. A High-level Programming Environment for Packet
Trace Anonymization and Transformation. In Proceedings of ACM SIGCOMM,
2003.
J. Schlörer. Disclosure from statistical databases: quantitative aspects of trackers.
ACM Trans. Database Sys., 5(4):467–492, 1980.
[33] Q. Sun, D. R. Simon, Y. Wang, W. Russell, V. N. Padmanabhan, and L. Qiu. Sta-
tistical Identification of Encrypted Web Browsing Traffic. In Proceedings of the
IEEE Symposium on Security and Privacy, 2002.
[34] L. Sweeney. k-anonymity: A model for protecting privacy. International Journal
of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(05):557–570, 2002.
[35] R. C.-W. Wong, J. Li, A. W.-C. Fu, and K. Wang. (α, k)-anonymity: an enhanced
k-anonymity model for privacy preserving data publishing. In Proceedings of
the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 754–759. ACM, 2006.
J. Xu, J. Fan, M. H. Ammar, and S. B. Moon.
Prefix-Preserving IP Ad-
dress Anonymization: Measurement-Based Security Evaluation and a New
Cryptography-Based Scheme. In Proceedings of the IEEE International Conference
on Network Protocols, 2002.
[32]
[36]
576