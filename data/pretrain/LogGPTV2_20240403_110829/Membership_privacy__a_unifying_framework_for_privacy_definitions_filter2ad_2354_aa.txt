title:Membership privacy: a unifying framework for privacy definitions
author:Ninghui Li and
Wahbeh H. Qardaji and
Dong Su and
Yi Wu and
Weining Yang
Membership Privacy: A Unifying Framework For Privacy
Deﬁnitions
Ninghui Li
Purdue University
PI:EMAIL
Wahbeh Qardaji
Purdue University
PI:EMAIL
Dong Su
Purdue University
PI:EMAIL
Yi Wu
Purdue University
PI:EMAIL
ABSTRACT
We formalize positive membership privacy, which prevents the ad-
versary from signiﬁcantly increasing its ability to conclude that
an entity is in the input dataset, and negative membership priva-
cy, which prevents leaking of non-membership. These notions are
parameterized by a family of distributions that captures the adver-
sary’s prior knowledge. The power and ﬂexibility of the proposed
membership privacy framework lies in the ability to choose differ-
ent distribution families to instantiate membership privacy. Many
privacy notions in the literature are equivalent to membership priva-
cy with interesting distribution families, including differential pri-
vacy, differential identiﬁability, and differential privacy under sam-
pling. Casting these notions into the framework leads to deeper
understanding of the strengthes and weaknesses of these notions,
as well as their relationships to each other. The framework also
provides a principled approach to developing new privacy notion-
s under which better utility can be achieved than what is possible
under differential privacy.
Categories and Subject Descriptors
K.4.1 [COMPUTERS AND SOCIETY]: Privacy
Keywords
Differential Privacy; Privacy Notions; Membership Privacy
1.
INTRODUCTION
The spate of privacy related incidents [30, 3, 27, 17] has spurred
a long line of research in privacy notions for data publishing and
analysis [30, 29, 24, 21]. A privacy notion that is increasingly
gaining acceptance is differential privacy [7, 10].
Informally, d-
ifferential privacy requires any individual entity in a dataset to have
only a limited impact on the output. More speciﬁcally, differential
privacy requires that any two neighboring input datasets will induce
output distributions that are close in the sense that the probabilities
of each possible output differ by a bounded multiplicative factor.
There are two major ﬂavors of differential privacy, depending on
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516686 .
Weining Yang
Purdue University
PI:EMAIL
the condition under which two datasets are considered to be neigh-
bors. In [19], these were referred to as unbounded and bounded
differential privacy. In Unbounded Differential Privacy (UDP), T
and T ′ are neighbors if T can be obtained from T ′ by adding or
removing an entity. In Bounded Differential Privacy (BDP), T and
T ′ are neighbors if T can be obtained from T ′ by replacing one
entity in T ′ with another entity.
Because privacy is a social notion with many facets, there is a
long tradition in the research community to examine the various
technical formulation of privacy and understand their strengthes
and weaknesses. Several researchers have questioned whether dif-
ferential privacy provides sufﬁcient protection, and how to choose
the ǫ parameter. In [19], Kifer and Machanavajjhala argued that it
is incorrect to claim that differential privacy is robust to arbitrary
background knowledge. In [5], Cormode argued that differential
privacy does not prevent inferential disclosure. That is, from dif-
ferentially private output, it is possible to infer potentially sensitive
information about an individual with non-trivial accuracy. In [20],
Lee and Clifton argued that while the parameter ǫ in ǫ-DP limits
how much one individual can affect the output, it does not lim-
it how much information is revealed about an individual; and this
does not match legal deﬁnition of privacy, which requires protec-
tion of individually identiﬁable data. Lee and Clifton proposed the
notion of differential identiﬁability. At the same time, it has been
recognized that differential privacy may be too restrictive in some
settings, and there are several efforts that aim at relaxing it, includ-
ing differential privacy under sampling [22] and crowd blending
privacy [15].
This paper is motivated by these lines of work. Our aim is to
gain a deeper understanding of privacy both as a social concep-
t and in terms of technical formulations. We begin by analyzing
the recent privacy incidents. Our analysis concludes that what so-
ciety often views as a privacy breach is the ability of an adversary
to either re-identify or assert the membership of any individual in a
supposedly “anonymized” dataset. Hence, a privacy measure needs
to protect everyone in the anonymized dataset against membership
disclosure. Such a privacy deﬁnition, however, is incomplete with-
out specifying the adversary’s prior knowledge about what might
be in the dataset. The need to consider this background knowledge
is indeed apparent from recent privacy breaches [3, 27].
We combine these two requirements and introduce a novel pri-
vacy framework that we call Membership Privacy. This framework
comprises of two notions: Positive Membership Privacy (PMP),
which prevents an adversary from signiﬁcantly improving its con-
ﬁdence that an entity is in the input dataset; and Negative Mem-
bership Privacy (NMP), which prevents an adversary from signiﬁ-
cantly improving its conﬁdence that an entity is not in the dataset.
Distribution
Family
DU
DI
D2
I
Dβ
F
DB
D2
B
Dm
C
DN
Description of Distributions in the Family
Equivalent Privacy Notion
Includes all distributions over 2U ; all other families are sub-families of this.
Includes all mutually independent (MI) distributions
Sub-family of DI. Includes MI distributions that have only two datasets with non-
zero probability, i.e., distributions in which Pr[T ∪ {t}] = p and Pr[T ] = 1 − p.
Sub-family of DI. Includes MI distributions such that all entities have probability
in {0, β}, i.e., all entities that may appear have a ﬁxed probability.
Includes distributions that are the conditional distributions of some MI distribution
conditioned upon that all datasets with non-zero probability have the same size.
Sub-family of DB. Includes distributions where Pr[T ∪ {t1}] = p and Pr[T ∪
{t2}] = 1 − p
Sub-family of DB. Includes all distributions in which Pr[{T ∪ {t1}] = Pr[T ∪
{t2}] = Pr[T ∪ {tm}] = 1
Sub-family of D0.5
i.e., all entities have probability 0.5.
F . Include the single uniform distribution over all subsets of U,
m and T ∪ {t1, · · · , tm} = U
Privacy with no Utility
Unbounded Differential Privacy [8]
Unbounded Differential Privacy
Differential Privacy Under Sam-
pling [22]
Bounded Differential Privacy [10]
Bounded Differential Privacy
Differential Identiﬁability [20]
New Privacy Notion
Table 1: Distributions for which membership privacy is considered in this paper, and their equivalent privacy notions in the literature.
These notions are parameterized by two parameters D and γ. The
ﬁrst parameter captures an adversary’s prior knowledge. In partic-
ular, D is a set of probability distributions. Each element D ∈ D
is a probability distribution over all possible datasets, and encodes
one possible state of prior knowledge of the adversary. D as a set
captures all states of prior knowledge against which membership
privacy is guaranteed. The second parameter γ is a number that is
greater than or equal to 1; it limits the increase in conﬁdence of
accurate membership assertion.
The power and ﬂexibility of the membership privacy framework
lies in the ability to choose different distribution families to instan-
tiate it. By deﬁnition, (D1, γ)-membership privacy is at least as
strong as (D2, γ)-membership privacy if D1 ⊇ D2. Interesting-
ly, in some cases, even when D1 ⊃ D2, (D1, γ)- and (D2, γ)-
membership privacy may be equivalent, because of certain rela-
tionship between D1 and D2.
The power of the membership privacy framework is demonstrat-
ed by the fact that many privacy notions in the literature are e-
quivalent to membership privacy with interesting distribution fam-
ilies. These notions and their corresponding distribution families
are given in Table 1. For example, Unbounded Differential Priva-
cy (UDP) by deﬁnition is equivalent to membership privacy in D2
I,
which includes all distributions of the form Pr[T ∪ {t}] = p and
Pr[T ] = 1 − p for some T, t, p. The strength of UDP is illustrated
by the fact that this is sufﬁcient to guarantee membership privacy
for DI , the family of all mutually independent (MI) distribution-
s. Similarly, Bounded Differential Privacy (BDP) is equivalent to
membership privacy under DB, the family that includes those ob-
tained by conditioning a MI distribution such that all datasets with
non-zero probability have the same size. Differential identiﬁabil-
ity [20] and Differential Privacy under Sampling [22] are also in-
stantiations of membership privacy.
Identifying the family under which a privacy notion guarantees
membership privacy provides deeper understanding of the power
and limitation of the privacy notion. For example, this framework
enables us to show that under what condition differential identiﬁa-
bility is equivalent to BDP and under what condition that it is strict-
ly weaker. We stress that almost all privacy notions make some
assumptions about the adversary’s background knowledge. For ex-
ample, differential privacy’s main assumption is independence, as
also pointed out in [19]. The only membership privacy notion with-
out any assumption is the one under DU , the family that includes
all distributions. We show that this essentially requires giving up
all utility. This is another formulation of the “no free lunch” result
in [19, 8, 12].
As all practical privacy notion requires some assumptions on the
allowed distributions, it makes sense to analyze whether the as-
sumption made in a notion is appropriate for a given setting, and
choose a privacy that is neither too strong nor too weak, in order
to maximize utility. Our membership privacy framework enables
such analysis. One could develop privacy notions that are stronger
than differential privacy (satisfying membership privacy for beyond
DI), as well as ones that are weaker (satisfying membership privacy
for a sub-family of DI ).
It has often been recognized that differential privacy can be too
strong to satisfy in some settings, and there are some efforts aim-
ing at relaxing it. Our membership privacy framework provides a
principled way to conduct this. For example, one weaker priva-
cy notion that may be useful for some applications is membership
privacy under DN , which includes the single uniform distribution
over all possible datasets. As a demonstration that this notion en-
ables higher utility, we show that under this notion it is possible
to compute the max value of a set with high accuracy, which is
difﬁcult to do under differential privacy.
The rest of the paper is organized as follows. In Section 2, we
analyze privacy incidents and motivate membership privacy. We
then introduce the membership privacy framework in Section 3,
show how differential privacy ﬁts in the framework in Section 4,
and consider several other instantiations of the framework in Sec-
tion 5. Finally, we discuss related work in Section 6 and conclude
in Section 7.
2. WHAT IS PRIVACY?
Similar to other contexts in security and privacy, the concept of
privacy is easier to deﬁne by identifying what are privacy breach-
es. Privacy can then be simply deﬁned by requiring that no privacy
breach occurs. As privacy is a social concept, any formalization
of privacy violation must be based on what the society perceives
as privacy breaches.
In this section, we examine several well-
publicized privacy incidents in data publishing in recent years, and
identify the common features of what the society considered to be
privacy breaches. We show that the disclosures in these incidents
all fall into a general class that we call positive membership disclo-
sures. In such an disclosure, when given the published dataset, an
adversary can ﬁnd an entity t and assert with high conﬁdence that
t’s data is in the original dataset.
2.1 Privacy Incidents
An early and well publicized privacy incident is from the suppos-
edly anonymized medical visit data made available by the Group
Insurance Commission (GIC) [30]. While the obvious personal i-
dentiﬁers are removed, the published data included zip code, date
of birth, and gender, which are sufﬁcient to uniquely identify a sig-
niﬁcant fraction of the population. Sweeney [30] showed that by
correlating this data with the publicly available Voter Registration
List for Cambridge Massachusetts, medical visits about many indi-
viduals can be easily identiﬁed, including those of William Weld, a
former governor of Massachusetts. We note that even without ac-
cess to the public voter registration list, the same privacy breaches
can occur. Many individuals’ birthdate, gender and zip code are
public information. This is especially the case with the advent of
social media, including Facebook, where users share seemingly in-
nocuous personal information to the public.
Another well-known privacy incident came from publishing we-
b search logs. In 2006, AOL released three months of search logs
involving 650,000 users. The only privacy protection technique
used is replacing user ids with random numbers. This proved to
be a failure. Two New York Time journalists [3] were able to re-
identify Thelma Arnold, a 62 year old women living in Lilburn,
Ga. from the published search logs. Ms. Arnold’s search log in-
cludes her last name and location names near where she lived. The
reporters were able to cross-reference this information with phone-
book entries. After the New York Time article is published, the
data was immediately retracted by AOL. Later a class action law-
suit was ﬁled against AOL. This scandal led to the resignation of
AOL’s CTO and the dismissal of two employees.
In 2009, Netﬂix released a dataset containing the movie rating
data from 500,000 users as part of a one-million dollar challenge to
the data mining research community for developing effective algo-
rithms for predicting users’ movie preferences based on their view-
ing history and ratings. While the data was anonymized in order to
protect users’ privacy, Narayanan and Schmatikov [27] showed that
an adversary who has some knowledge about a subscriber’s movie
viewing experience can easily identify the subscriber’s record if it
is present in the dataset. For example, [27] shows that, from the