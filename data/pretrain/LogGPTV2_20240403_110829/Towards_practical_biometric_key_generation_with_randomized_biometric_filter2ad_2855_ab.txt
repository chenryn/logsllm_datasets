any password, including the correct password, results in a high-
entropy string, and so an attacker performing a brute-force search
cannot tell when she has found the correct password. Similar tech-
niques have been used by Bellovin and Merrit [5] in the context of
Encrypted Key Exchange. RBTs adopt this approach and hide fea-
tures by encoding templates as high-entropy strings, and encrypting
these strings with low-entropy passwords.
However, in our setting, designing the template such that a de-
cryption under the correct password is indistinguishable from de-
cryption under an incorrect password is more challenging because
templates have semantic meaning. In other words, the templates
must specify the features and the error-correction information nec-
essary to process a biometric and derive a key. Thus, we require a
representation of the features and error-correction information that
is random, yet meaningful. To create such a representation, we dis-
tinguish between two types of information in a template. The ﬁrst
type can be randomized without losing semantic meaning. We thus
randomize and encrypt these pieces of information. The second
type of information cannot be randomized without losing seman-
tic meaning. We thus ﬁx these values to be the same across the
population, and include them unencrypted in the template.
RBTs use quantization for error-correction. That is, the output
range of each feature is partitioned into segments of equal width,
and the index of the segment that contains the feature applied to the
user’s samples is used for key generation. Given that our scheme
uses quantization, our templates need to specify three pieces of in-
formation:
the features, the offset of the quantization within the
feature’s output range, and the width of the quantization. We can
safely encode features without losing semantic meaning if we spec-
ify features as indexes into a table, randomly assign a subset of
the features to each user, and then encrypt this subset in the tem-
plate.
In this way, the decryption of feature indexes with an in-
correct key will be indistinguishable from decryption with the cor-
rect key because in both cases, a decrypted template appears as a
random permutation on a random subset of feature indexes. We
can also safely encode quantization offsets. Note that if a user’s
quantization over the output range of feature φi is deﬁned as the
set {αi, αi + δi, αi + 2δi, . . .}, then knowledge of of the width of
the quantization (δi) and any of the quantization offsets αi + cδi
unambiguously deﬁnes the entire set. Thus, if all features have out-
put ranges that can be mapped to some range R, then we can safely
encode quantization offsets without losing semantic meaning by
encrypting a random value in {αi, αi + δi, αi + 2δi, . . .} ⊆ R.
Decryption of the resulting ciphertext under any key results in a
value that is randomly distributed over R, but that is also semanti-
cally meaningful as a quantization offset for any feature.
On the other hand, the width of the quantization, δi, cannot be
randomized and so we cannot safely encrypt it. To see why this is
the case, note that the random assignment of a quantization width
to a feature (as would happen if the template is decrypted under
an incorrect password) might not be semantically meaningful. An
adversary could use an observed semantic inconsistency in a de-
crypted template to infer that she had decrypted the template with
an incorrect password. For instance, if there is a feature for which
most users in the population exhibit large variation, and require
large error tolerance, then an adversary who decrypted the tem-
plate under an incorrect password and observed a small quantiza-
tion width for that feature could logically deduce that she guessed
the incorrect password. To avoid this type of problem, we specify
a user-independent error-correction threshold for each feature.
At a high level, our construction works as follows. We encode
the features by storing a table that assigns an index to each feature.
The same table is stored in every template, but only the indexes
that correspond to the correct features are encrypted in a particular
user’s template. Features are encrypted in a way such that decryp-
tion under any key speciﬁes an index in the global table, and thus
speciﬁes a viable feature. Additionally, we ensure that the proba-
bility that any given feature is assigned to a user is the same across
the population. Given this encoding algorithm, decryption of the
encrypted features under any password results in a list of features
that is equally likely, and so decryption under the correct password
is indistinguishable from a decryption under an incorrect password.
To encode error-correction information, we follow a similar ap-
proach. First, we ﬁx the quantization width δi for each feature
φi, and store it in the global table. As the quantization widths are
global values, we only need to encode one quantization offset to
completely specify error-correction. Since any such offset sufﬁces,
we randomly select an offset and encrypt it with a pseudorandom
permutation with a domain that covers all quantization widths. De-
cryption under any password thus results in a value that is equally
likely to be a correct offset for any feature.
This approach results in an encoding algorithm where every de-
cryption of the template simultaneously “appears random” and is
useful to generate a cryptographic key. However, only the decryp-
tion under the correct password will yield a template that can be
used to generate the correct key. This has the property that an ad-
versary who searches for the key will have to guess the biometric
for each guess at the password that was used to encrypt the tem-
plate. Before presenting the technical details of the construction,
we introduce some relevant cryptographic primitives and notation.
4.1 Preliminaries
We use the notation || to refer to string and list concatenation,
and refer to the ith element in the list L as L[i]. We use x R← X to
denote the selection of an element x uniformly at random from the
set X, and x ← A to indicate that the algorithm A outputs x. The
set of integers from a to b, inclusive, is represented as [a, b], and
[a, b]k = {a + ik : i ∈ [0, (cid:7)(b − a)/k(cid:8)]}.
Our construction uses several cryptographic primitives. We use
pseudorandom permutations (PRPs) on sets of integers to ensure
that an adversary cannot determine whether she has decrypted a
template with the correct password. Let (ED, DD) denote the en-
cryption and decryption functions (a PRP and its inverse) with key
space K and domain and range [0, D − 1].
We assume that users select (possibly low-entropy) passwords
from a set Π, and that our BKG outputs bit-strings of length λ. Our
construction uses four random oracles:
Hpass,0 : Π → K, Hver : {0, 1}∗ → {0, 1}t
Hpass,1 : Π → K, Hkey : {0, 1}∗ → {0, 1}λ
Hpass,0 and Hpass,1 map a password into different elements in the
key space of the PRPs. Hver is used to generate a token to test
whether the BKG has generated the correct key. Hkey is used to
generate the ﬁnal key from a user’s password and biometric sam-
ples. Finally, we assume the existence of a function, Permute,
that permutes a list of numbers in a cryptographically secure sense
(again, this can be implemented as a PRP).
Our design uses an ordered set of N features Φ = (φ1, . . . , φN ),
where φi is a map from the set of biometric samples to the integers
Ri = [0, ri]. We use quantization for error-correction. Let δi ∈ N
be the tolerance of the quantization for φi. This value is ﬁxed to be
the same for each user in the population. Let Δ = 1 + maxi δi,
quantization offsets will be encoded into the range [0, Δ] and en-
crypted so that decryption under any key appears as a random, yet
semantically meaningful, offset.
5. CONSTRUCTION
5.1 The Enroll Algorithm
RBTs use two algorithms, Enroll (Algorithm 1) and KeyGen
(Algorithm 2). The enrollment phase is a four step process: assign-
ing features to a user, computing the necessary information to cor-
rect a user’s samples to a consistent value, using the error-corrected
values to create a key, and ﬁnally, encoding a secure template.
Feature Selection. To start, a user presents (cid:8) biometric samples
β1, . . . , β(cid:2) and a password π ∈ Π, to Enroll. The BKG computes
some statistics over each of the samples and returns the indexes of
the features that are to be used for key generation. This is a in-
tricate process and is described in Section 6. For now, it sufﬁces
to note that from an adversary’s point of view, every set of fea-
tures is equally likely to be assigned to each user. Assume that the
BKG assigns m features to a user. Each user is assigned a dif-
ferent number and/or a different set of features, so m will differ
across the population. However, to ensure that all templates have
the same length, Enroll pads out each template to use n features,
with m ≤ n ≤ N. Let Ψ be the indexes of the m features φi ∈ Φ
Φ
Π
Ri
δi
Δ
N
n
Global Parameters
The set of all features φ1, . . . , φN
The set of (low-entropy) user passwords
The output range of φi, (i.e., [0, ri])
The quantization width for φi
Maximum of quantization widths: 1 + maxi δi
The number of features in Φ
The number of features in each template
Individual Parameters
Ψ
Set of features assigned to a user for key gen.
˜Ψ
Set of features assigned to a user for padding
m
The number of features in Ψ
π
A (low-entropy) user password
β
A user’s biometric reading
αi
The smallest quantization boundary for φi(β)
(ED, DD) A PRP with key space K and domain [0, D − 1]
Hpass,0
Hpass,1
Hver
Hkey
Cryptographic Primitives
Random Oracle from Π → K
Random Oracle from Π → K
Random Oracle from {0, 1}∗ → {0, 1}t
Random Oracle from {0, 1}∗ → {0, 1}λ
Table 1: Notation
that are selected for key generation, let ˜Ψ be the indexes of the
n − m features selected at random from Φ\Ψ for padding, and
let L = Permute(Ψ)||Permute( ˜Ψ). The features speciﬁed in L,
which are a random permutation of a random n element subset of
Φ, are used for template creation and key generation.
Error Correction. The next step in the enrollment process is to
correct a user’s samples into a single, repeatable value. RBTs use
quantization for this purpose. We assume that the widths of the
quantization intervals have been pre-computed (see Section 6), and
are ﬁxed across the population: feature φi uses intervals of length
δi. To specify error-correction, the scheme need only specify a
quantization offset in the range (Ri) of each feature in L. For each
i ∈ L, compute the integer μi as the median of φi(β1), . . . , φi(β(cid:2)).
Then, partition Ri into δi-length intervals centered around μi. This
requires computing one of the quantization segment boundaries,
and so we compute αi to be the smallest value in Ri that is an
integer multiple of δi away from μi − δi/2:
j (cid:7)μi − δi/2(cid:8) mod δi
(cid:7)μi + δi/2(cid:8)
αi =
if μi ≥ δi/2
if μi < δi/2
Given αi and δi, the partitioning over the feature range is speciﬁed
by the integers {0} ∪ [αi, ri]δi . The border of the partition that
contains μi is xi = max(0, (cid:7)μi − δi/2(cid:8)). See Algorithm 1, lines
5–8 for the error-correction process.
Deriving a Key. Having speciﬁed our error-correction scheme, we
are ready to create a cryptographic key. The key is derived from the
password π, the feature indexes, and the quantized feature outputs
by setting Kj = L[j]||xL[j] for j ∈ [0, m − 1], and setting the
key to be K = Hkey(π||K0|| . . .||Km−1) (see Algorithm 1, lines
11–12). That is, K is the output of a random oracle applied to
the password, indexes of the m features selected for the user in
question, and the lower boundary of the partition that contains the
output of each feature. This increases the entropy over standard
key generation schemes by exploiting the uncertainty associated
with feature selection in addition to the output of each feature.
Input: The password π ∈ Π, and biometric samples β1, . . . , β(cid:2)
(Global values): the features Φ, and quantization widths
Input:
δ0, . . . , δN
Output: The key K and template T
1: (Ψ, ˜Ψ) ← Select(β1, . . . , β(cid:2)) // Select biometric features
2: L ← Permute(Ψ)||Permute( ˜Ψ)
3: k0 ← Hpass,0(π), k1 ← Hpass,1(π)
4: for j ← 0 to |L| − 1 do
5:
6:
7:
i ← L[j]
μi ← Median(φi(β1), . . . , φi(β(cid:2)))
αi ← (cid:7)μi − δi/2(cid:8) mod δi if μi ≥ δi/2. Otherwise,
(cid:7)μi + (δi/2)(cid:8).
xi ← max(0, (cid:7)μi − δi/2(cid:8)) // Quantize the feature outputs
8:
R← [αi, Δ]δi
γi
9:
Cj = (EN
10:
k0
tization offset
11: Kj = i||xi
12: K ← Hkey(π||K0||K1|| . . .||K|Ψ|−1) // Derive the key
13: C ← (C0, C1, . . . , C|L|−1)
14: v ← Hver(π||K0||K1|| . . .||K|Ψ|−1)
15: return K, T = (C, v)
// Select a random quantization offset
(i), EΔ
k1
(γi)) // Encrypt feature index and quan-
Algorithm 1: Speciﬁcation of the RBT Enroll algorithm
Template Creation. Our task is to now encode the feature in-
dexes and the quantization information so that only an individual
with knowledge of π and the ability to produce a biometric that
is “close” to the enrollment samples can generate the correct key
(Algorithm 1, lines 9–10). To do so, we must encode L and the
αi so that they appear random, and then encrypt these values with
π. We employ two PRPs, (EN , DN ) and (EΔ, DΔ), to encrypt fea-
ture indexes (i.e., the L[j]) and the offsets in each feature range
(i.e., the αL[j]). Since PRPs induce a different and equally-likely
random permutation for every key, if the encoding of L and the
αi is truly random, then an adversary who decrypts the template
will not be able to tell if she has done so with the correct pass-
word. We create two independent keys for each cipher from π as:
k0 = Hpass,0(π) and k1 = Hpass,1(π), respectively. The keys are
independent of one another to ensure that there is no correlation