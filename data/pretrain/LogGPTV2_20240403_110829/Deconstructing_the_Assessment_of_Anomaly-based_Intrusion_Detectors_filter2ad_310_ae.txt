payloads within PAYL’s purview.
With regard to the consistency of the presented results, we cannot conclude
that the results of the presented work describe detector capability beyond the
single evaluation instance. Again, we refer to the fact that variability in the
training data (TR1.2), the amount used (TR3) and the choice of learning pa-
rameters such as the clustering threshold (TR2.3), can signiﬁcantly inﬂuence
detector performance. Since the authors only trained on 2 weeks worth of data
(week 1 and week 3), would the choice of another 2 weeks (week 1 and week 2)
produce the same results? As it stands, the results presented in this paper only
apply to the single evaluation instance described, and may not have persisted
even if another sample of the same dataset were used. Although the authors do
mention that PAYL is also designed to work in an incremental learning mode,
they did not evaluate that functionality – consequently we cannot speak to the
eﬃcacy of the detector with respect to that mode. In short, the uncertainty lies
in whether PAYL can achieve 100% detection accuracy with a low false-alarm
rate consistently, even in another instance of the same dataset.
5.3 Kruegel et al. [26] - Anomaly Detector for Web-Based Attacks
Kruegel et al. [26] evaluated a multi-model based anomaly-detector for detecting
web-based attacks over individual web-server requests. The evaluation was per-
formed over three data sets, one from a production web-server at Google, Inc.
and two from webservers located at two diﬀerent universities. They reported a
100% detection rate for their anomaly detector when tested against twelve at-
tacks injected into the dataset collected from one of the university webserver.
This paper provided the best example of a reliable evaluation whose results were
useful to us in determining the applicability of the technology within our own
systems. As summarized in Table 3, we were able to account for all the factors
necessary for conﬁrming the validity of the detection results. We were, however,
unable to reconcile two factors that introduced uncertainty in our assessment of
the detection consistency arguments.
The evaluation provided enough information to be certain that all attacks
were injected manually into the data stream and manifested as single anomalous
queries into the evaluation data stream. There was no additional ﬁltering or
sanitization performed over the attack dataset so the attacks manifested as-is
into the test data stream. Further, the provided information on the attack set
used for testing is suﬃcient to conclude that the attacks were suitable for the
modeling formalism.
Some of the uncertainties that we were unable to reconcile with respect to con-
sistency of detection are as follows. All evaluations were performed by choosing
the ﬁrst 1000 queries corresponding to a web-server program to automatically
build all necessary proﬁles and compute detection thresholds. It is not clear how
increasing or decreasing the number of queries used in training, i.e., the amount
of training (TR3), would bias the reported detection results. Furthermore, the
detector was assessed over a test corpus that was created by injecting attacks
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
303
into one of three datasets collected from a university webserver. This particu-
lar dataset was earlier shown to display less variability in its characteristics as
compared to the other two datasets. It is not clear if similar detection perfor-
mance (100% detection) can be expected if the same attacks were injected into
a comparatively more variable dataset such as the Google dataset (TR1.2). It
is consequently diﬃcult to ascertain the reliability or consistency of the result
beyond the exact training data and strategy used in this paper.
5.4 Summary of Results from Case Studies
The case studies discussed in the previous sections elaborated on how unex-
plained factors across the evaluation phases aﬀect the validity and consistency
of detection results. In this section, we summarize the eﬃcacy of the evaluations
performed in the case studies by counting the multiple possible explanations for
the hit and miss results presented in their respective papers, due to the unex-
plained factors in those evaluations.
We apply the analysis developed in Sect. 4.3 and present results for the case
studies discussed in Table 3. Each row in Table 3 is ﬁlled in as follows: (1) For
each event, we ﬁrst gather the set of factors inﬂuencing validity and consistency
from Table 1. (2) Then, for each case study (columns in Table 3), we record
if any of those factors were identiﬁed in our previous discussion of the case
studies. There are three possibilities: (a) if NO factors were identiﬁed, one possi-
bility is that there was enough information available to explain away the factors
perturbing the corresponding event (entries labeled YES in Table 3); (b) if NO
factors were identiﬁed, another possibility is that there were some assumptions
made to explain away the factors (entries labeled YES*); or (c) if ANY factors
were identiﬁed, it means that there was insuﬃcient or no information regarding
those factors to conﬁdently state that the event was unperturbed (entries labeled
NOINFO). (3) We then use this information along with the framework in Fig. 3
to count the possible explanations for hits and misses for the case study.
(cid:3) → 4 → 5 → 6; 1 → 2 → 3 → 3
(cid:3) → 4b → 5 → 6; 1 → 2 → 3 → 3
From a combined perspective of valid and consistent detection, we see that
for Mahoney et al. [28] and Wang et al. [29], the uncertainty in the evaluation
process induces four possible explanations for a “hit” (from Fig. 3(a)): 1 → 2 →
(cid:3)a →
3 → 3
(cid:3)a → 4b → 5 → 6. Similarly, from Fig. 3(b),
4b → 5 → 6; 1 → 2 → 3a → 3
there are six explanations for a “miss”: 1 → 2 → 3 → 3
(cid:3) → 4 → 5a → 6;
1 → 2 → 3 → 3
(cid:3) → 4a → 5a → 6; 1 → 2 → 3 → 3
(cid:3)a → 4a → 5a → 6;
(cid:3)a → 4b → 5a → 6; 1 → 2 → 3a → 3
(cid:3)a → 4a → 5a → 6;
1 → 2 → 3 → 3
1 → 2 → 3a → 3
(cid:3)a → 4b → 5a → 6. We observe that the best example of a
reliable evaluation is by Kruegel et al. [26] because there are only two possible
(cid:3)a → 4b →
explanations for a hit: 1 → 2 → 3 → 3
5 → 6. In essence, their reported hits were all valid but cannot be concluded to
be both valid and consistent. There are zero explanations for a “miss” as there
were no misses encountered in their evaluation.
(cid:3) → 4 → 5 → 6; 1 → 2 → 3 → 3
304
A. Viswanathan, K. Tan, and C. Neuman
Table 3. Summary of the eﬃcacy of evaluations performed in the case studies
# Event
Mahoney et
al. [28]
Wang et
al. [29]
Kruegel et
al. [26]
(1) Attack deployed.
(2) Attack manifests in evaluation
YES
YES*
data.
(3) Attack manifests in test data.
(cid:2)
(3
(4) Attack is anomalous within the
) Attack manifests stably.
NOINFO
NOINFO
NOINFO
detector’s purview.
(5) Anomaly is signiﬁcant.
(6) Detector response is measured
YES
YES
appropriately.
Possible cases for “hit”
Possible cases for “miss”
4
6
YES
YES*
NOINFO
NOINFO
NOINFO
YES
YES
4
6
YES
YES
YES
NOINFO
YES
YES
YES
2
0
From a consistency perspective, we observed that it was diﬃcult in all the case
studies to ascertain the consistency of the presented results beyond the exact
instance of training data and strategy used.
6 Conclusions
Our objective in this paper was to examine the mechanics of an evaluation strat-
egy to better understand how the integrity of the results can be compromised.
To that end, we explored the factors that can induce errors in the accuracy of
a detector’s response (Sect. 3), presented a unifying framework of how the error
factors mined from literature can interact with diﬀerent phases of a detector’s
evaluation to compromise the integrity detection results (Sect. 4), and we used
our evaluation framework to reason about the validity and consistency of the
results presented in three well-cited works from literature (Sect. 5).
The framework of error factors presented is geared toward answering the
“why” questions often missing in current evaluation strategies, e.g., why did a
detector detect or miss an attack?. We used it to show how and why the results
presented in well-cited works can be misleading due to poor experimental con-
trol. Our contribution is a small step toward the design of rigorous assessment
strategies for anomaly detectors.
Acknowledgements. The authors would like to thank our colleagues at ISI,
JPL, LADWP, and shepherd Dina Hadˇziosmanovi´c for discussions and feedback
that helped develop the ideas and methods expressed in this paper.
References
1. Denning, D.E.: An Intrusion-Detection Model. IEEE Trans. on Software Engineer-
ing SE-13(2), 222–232 (1987)
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
305
2. Peisert, S., Bishop, M.: How to Design Computer Security Experiments. In:
Futcher, L., Dodge, R. (eds.) Fifth World Conference on Information Security
Education. IFIP, vol. 237, pp. 141–148. Springer, Boston (2007)
3. Maxion, R.: Making experiments dependable. In: Jones, C.B., Lloyd, J.L. (eds.)
Festschrift Randell. LNCS, vol. 6875, pp. 344–357. Springer, Heidelberg (2011)
4. Gates, C., Taylor, C.: Challenging the Anomaly Detection Paradigm: a Provocative
Discussion. In: Proc. of the Workshop on New Sec., pp. 21–29. ACM, Paradigms
(2006)
5. Sommer, R., Paxson, V.: Outside the Closed World: On Using Machine Learning
for Network Intrusion Detection. In: Proc. of IEEE Symp. on Security and Privacy,
pp. 305–316 (May 2010)
6. Killourhy, K., Maxion, R.: Why Did My Detector Do That?! In: Jha, S., Som-
mer, R., Kreibich, C. (eds.) RAID 2010. LNCS, vol. 6307, pp. 256–276. Springer,
Heidelberg (2010)
7. Ingham, K.L., Inoue, H.: Comparing Anomaly Detection Techniques for HTTP.
In: Kruegel, C., Lippmann, R., Clark, A. (eds.) RAID 2007. LNCS, vol. 4637,
pp. 42–62. Springer, Heidelberg (2007)
8. Hadˇziosmanovi´c, D., Simionato, L., Bolzoni, D., Zambon, E., Etalle, S.: N-Gram
against the Machine: On the Feasibility of the N-Gram Network Analysis for Bi-
nary Protocols. In: Balzarotti, D., Stolfo, S.J., Cova, M. (eds.) RAID 2012. LNCS,
vol. 7462, pp. 354–373. Springer, Heidelberg (2012)
9. Lee, W., Xiang, D.: Information-theoretic Measures for Anomaly Detection. In:
Proc. of the IEEE Symp. on Security and Privacy, pp. 130–143 (2001)
10. Mai, J., Chuah, C.N., Sridharan, A., Ye, T., Zang, H.: Is sampled data suﬃcient
for anomaly detection? In: Proc. of the 6th ACM SIGCOMM Conf. on Internet
measurement, pp. 165–176. ACM (2006)
11. Ringberg, H., Roughan, M., Rexford, J.: The Need for Simulation in Evaluating
Anomaly Detectors. SIGCOMM Comp. Comm. Rev. (CCR) 38(1), 55–59 (2008)
12. Tan, K.M.C., Maxion, R.A.: “Why 6?” Deﬁning the Operational Limits of Stide,
an Anomaly-Based Intrusion Detector. In: Proc. of the IEEE Symp. on Security
and Privacy, pp. 188–201 (2002)
13. Tavallaee, M., Stakhanova, N., Ghorbani, A.: Toward Credible Evaluation of
Anomaly-Based Intrusion-Detection Methods. IEEE Trans. on Systems, Man, and
Cybernetics, Part C: Applications and Reviews 40(5), 516–524 (2010)
14. Forrest, S., Hofmeyr, S.A., Somayaji, A., Longstaﬀ, T.A.: A Sense of Self for Unix
Processes. In: Proc. of the IEEE Symp. on Security and Privacy. IEEE (1996)
15. Fogla, P., Lee, W.: Evading Network Anomaly Detection Systems: Formal Rea-
soning and Practical Techniques. In: Proc. of the 13th ACM Conf. on Comp. and
Comm. Sec. (CCS), pp. 59–68. ACM (2006)
16. Wagner, D., Soto, P.: Mimicry Attacks on Host-based Intrusion Detection Systems.
In: Proc. of the 9th ACM Conf. on Comp. and Comm. Sec. (CCS), pp. 255–264.
ACM (2002)
17. Chandola, V., Banerjee, A., Kumar, V.: Anomaly Detection: A Survey. ACM Com-
puting Surveys 41(3), 15:1–15:58 (2009)
18. McHugh, J.: Testing Intrusion Detection Systems: A Critique of the 1998 and
1999 DARPA Intrusion Detection System Evaluations as Performed by Lincoln
Laboratory. ACM Trans. on Info. System Security 3(4), 262–294 (2000)
19. Horky, J.: Corrupted Strace Output. In: Bug Report (2010),
http://www.mail-archive.com/PI:EMAIL/
msg01595.html
306
A. Viswanathan, K. Tan, and C. Neuman
20. Cretu, G.F., Stavrou, A., et al.: Casting Out Demons: Sanitizing Training Data
for Anomaly Sensors. In: Proc. of the IEEE Symp. on Security and Privacy, pp.
81–95. IEEE (2008)
21. Kohavi, R., et al.: A Study of Cross-Validation and Bootstrap for Accuracy Esti-
mation and Model Selection. In: Intl. Joint Conf. on Artiﬁcial Intelligence, vol. 14,
pp. 1137–1145 (1995)
22. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kauf-
mann (2005)
23. Javitz, H., Valdes, A.: The SRI IDES Statistical Anomaly Detector. In: Proc. of
the IEEE Comp. Soc. Symp. on Research in Security and Privacy, pp. 316–326
(1991)
24. Lane, T., Brodley, C.E.: Approaches to Online Learning and Concept Drift for User
Identiﬁcation in Computer Security. In: Proc. of the 4th Intl. Conf. on Knowledge
Discovery and Data Mining, pp. 259–263 (1998)
25. Wang, K., Parekh, J.J., Stolfo, S.J.: Anagram: A content anomaly detector resis-
tant to mimicry attack. In: Zamboni, D., Kruegel, C. (eds.) RAID 2006. LNCS,
vol. 4219, pp. 226–248. Springer, Heidelberg (2006)
26. Kruegel, C., Vigna, G.: Anomaly Detection of Web-based Attacks. In: Proc. of
the 10th ACM Conf. on Comp. and Comms. Security (CCS), pp. 251–261. ACM
(2003)
27. Axelsson, S.: The Base-rate Fallacy and the Diﬃculty of Intrusion Detection. ACM
Trans. on Info. Systems Security 3(3), 186–205 (2000)
28. Mahoney, M.V.: Network Traﬃc Anomaly Detection Based on Packet Bytes. In:
Proc. of the ACM Symp. on Applied computing, pp. 346–350. ACM (2003)
29. Wang, K., Stolfo, S.: Anomalous payload-based network intrusion detection. In:
Jonsson, E., Valdes, A., Almgren, M. (eds.) RAID 2004. LNCS, vol. 3224, pp.
203–222. Springer, Heidelberg (2004)