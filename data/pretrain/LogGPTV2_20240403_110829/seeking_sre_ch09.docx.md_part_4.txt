### 优化后的文本

在评估服务的可用性时，应从错误列表中删除那些由外部因素引起的告警，而非实际的服务内部错误。以“消息总线”服务中的“数据接收器”组件为例，假设其高优先级流量的SLA定义如下：
> 所有带有高优先级标志的消息，我们将在一个日历年内保证99.9%的发送在10毫秒内完成。

在这种情况下，失败列表将包括以下内容：
- 处理时间超过10毫秒的消息。
- 由于内部服务问题（而非不正确的传入请求）而无法处理的消息。

为了跟踪当前的可用性水平，我们需要从收到的总消息数中减去失败消息的数量。尽管我们在讨论单个组件的性能，但计算过程中仍需考虑整个“消息总线”的全局视角。因此，即使存在一组负载均衡器，我们也应该基于本地组件的角度来统计总消息数量，并使用这些数据来追踪整体SLA表现。

对于错误计数器而言，虽然从负载均衡器收集的数据可以反映真实客户的影响，但它可能掩盖了组件层面的具体问题，导致我们无法及时采取措施。接下来，让我们通过一些数学运算进一步说明这一点。初始设定下，我们的可用性为100%。经过24小时后，我们将根据收集到的所有指标重新计算新的可用性值，以便作为第二天的参考基准。

例如，如果昨天共接收到1,000,000条消息，其中有200条因各种原因被标记为失败，则可用性会下降至99.98%，具体计算公式如下：
\[ 100\% - \left(\frac{200}{1,000,000} \times 100\%\right) = 99.98\% \]

然而，这种方法存在几个潜在问题。首先，一旦可用性降低，就无法恢复到原来的水平；其次，它没有考虑到随时间变化的分布情况，可能导致在某些非预期时刻出现急剧下降。比如，在一段非常安静的时间段里，一整天只发送了10条消息，且恰好有一台主机无法响应任何消息。若这10条消息连续发出并平均分配给两台主机，那么其中一半可能会失败。虽然最终所有消息都成功传递，但从计算角度来看，这会导致显著的可用性下降：
\[ 100\% - \left(\frac{5}{15} \times 100\%\right) = 66.67\% \]
即下降了33.33%，但实际上并没有对任何客户造成影响！

另一个明显的问题在于难以规划需要停机维护的时间窗口，因为不清楚如何根据消息量的变化来调整高峰时段。此外，大规模更新或新版本推出也可能导致服务故障，即便进行了充分测试和逐步部署策略（如金丝雀发布）。关键挑战在于无法准确预测此类事件是否会对SLA产生负面影响。

解决方案隐藏在对“每年99.9%可用性”的解释中。既然“年”是一个时间单位，那么99.9%也可以被视为时间值。据此，我们可以计算出一年内允许用于故障和维护的时间总量。一年包含525,600分钟，超出SLA标准的0.1%相当于大约526分钟。谷歌为此引入了一个概念——*错误预算*，指服务在一年中可接受的故障和维护时间总和。

理论上讲，如果服务在365天内完全没有发生任何失败，则可以在最后约8小时45分钟内完全关闭而不违反SLA协议。通过这种方式，我们可以将回滚时间需求与错误预算进行比较，从而判断未来的更新是否会危及当前的可用性水平。

此外，采用时间值表示法后，可用性水平变得可恢复。过去只能不断减少可用时间，而现在则可以通过持续良好运行逐渐恢复至100%。由于“一年”是固定的周期长度，每次运行都会向年度时间线上添加新的度量值，并移除旧数据点。如果我们每天计算一次可用性，那么每次都将处理1,440分钟的数据。通过从每日总分钟数中扣除所有故障时间，我们可以得出“每日正常运行时间”。要获得最新的总体服务可用性，需从前一天的总可用性中减去年前同一天的数据，并加上今天的正常运行时间。举例来说：
- 昨天的服务可用性：525,400分钟（99.9619%）
- 错误预算大小：326分钟
- 去年同一天的每日正常运行时间：1,400分钟
- 今天的故障时间：5分钟

今天总的正常运行时间为：
\[ \text{正常时间} = \text{"昨天的正常时间"} - \text{"去年的每日正常时间"} + \text{"今天的正常时间"} \]
\[ 525,400 - 1,400 + (1,440 - 5) = 525,435 \text{分钟} （99.9686%）\]

当前错误预算计算方法为当前总正常运行时间和SLA目标之间的差值：
\[ 525,435 - (525,600 - 526) = 361 \text{分钟} \]

由此可见，去年仅有1,400分钟处于正常状态，意味着剩余40分钟被故障占用并消耗了一部分错误预算。随着一年时间推移，这40分钟将不再计入当前的可用性范围内，因此今天预算实际上增加了40分钟，但由于今天的失败又减少了5分钟。

为了避免少量消息导致的可用性骤降问题，建议按最小时间间隔（如每分钟）分析统计数据。转换过程相对简单：每个时间间隔内的消息总数视为100%，从中提取出故障比例即可得到相应的故障时间。然后用所有转换后的故障时间之和来调整当前的停机时间和错误预算值。

以“数据接收器”组件为例，我们面临两类故障情形：
- 发生错误导致消息丢失。
- 消息处理时间超过10毫秒。

我们每分钟收集一次相关指标，并确保即使无错误发生时也生成明确的零值记录。每隔一天重新计算一次可用性。针对每一分钟，我们会获取三个数据集：
- 处理的消息数。
- 每条消息的处理时间。
- 错误数量。

如果计时器缺少数据点，则默认将其标记为失败。同样地，如果处理的消息数或错误数为空，也会将整分钟标记为失败。