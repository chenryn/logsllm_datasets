us call the multisets that are not in D invalid datasets. If
those invalid datasets are excluded by the restriction on δ-
neighbourhood, then essentially the two assurances, either
under standard neighbourhood or δ-neighbourhood, are e-
quivalent. For instance, consider a D that contains the lo-
cations of a vehicle sampled at periodic intervals, say at time
1, 2, . . . , n, and is represented as a set of tuples where each
tuple (t, x) indicates that the vehicle is at location x on time
t. Suppose D is to be published by a mechanism A that is
ϵ-diﬀerentially private under the standard neighbourhood,
then for any possible output r, any D, (t, x) and (t, y), we
have
P r(A(D + {(t, x)}) = r) ≤ exp(ϵ)P r(A(D + {(t, y)}) = r).
Since D essentially represents a sequence, the two tuples in
the above inequality must have the same t.
We can take a step further. Due to speed limit of the
vehicles (which is public knowledge), some datasets are in-
valid. For example, if D1 is a valid dataset, a location y
that is far from x will lead to an invalid dataset. Since the
bound is not required to hold for the invalid datasets, thus,
with an appropriate metric and a suﬃciently large δ, the
assurance provided under δ-neighbourhood is equivalent to
the assurance provided under the standard neighbourhood.
Similar examples are considered by Blocki et al. [1]. They
consider social networks where the maximum degree of any
node is likely to be bounded by a number k that is much
smaller than the network size n. In such situations, an in-
teresting question is whether the utility can be improved by
exploiting the constraints. To illustrate, it is not clear how
to improve the well-known histogram-based mechanisms on
the constrained datasets, since the sensitivity incurred under
the standard and δ-neighbourhood is the same.
4.2 Example 2
In this example, we want to publish a dataset D which
contains locations of n entities drawn from the domain M.
Consider in an extreme case, where an adversary knows the
locations of n-1 entities in D (let us denote this D
). The
adversary wants to guess whether x is in D, in other words,
whether the unknown entity is x. Under the standard neigh-
bourhood, diﬀerential privacy guarantees that the published
data does not help the adversary in guessing whether x or
y is in D, where y is another location the adversary does
not know, by bounding the distance of the two probabili-
+ {y}) ∈ R). Hence,
ties P (A(D
from the perspective of any contributor Bob, if Bob accepts
the assumption that there is at least one entity of whom
the adversary does not have background information, he is
comfortable in contributing his location.
+ {x}) ∈ R) and P (A(D
′
′
′
4. SPATIAL DATASETS
The δ-neighbourhood can be naturally deﬁned on spatial
points, say M = [0, 1]k for some k ≥ 1. The underlying dis-
tance function d(·,·) can be the Euclidean distance and the
sources can be the boundary of M, which implies that enti-
ties enter through the boundary, or simply none, correspond-
ing to the bounded diﬀerential privacy. Let us investigate a
few scenarios where the proposed notion is meaningful.
Under δ-neighbourhood, the same guarantee holds when
x and y are close-by. From the perspective of Bob, if he
accepts the assumption that there is at least one entity y
near Bob, say within δ =40km, of whom the adversary has
no background knowledge, then diﬀerential privacy with δ-
neighbourhood suﬃces for Bob to contribute.
Now let us consider another more resourceful adversary
who has more accurate background information on region
near Bob. With respect to this background information, the
161indistinguishable entities similar to Bob can be 40 to 80 km
away. In this case, when D is published by an ϵ-diﬀerential
private mechanism under 40 km-neighbourhood, Bob’s pri-
vacy is still protected but with a weaker assurance similar
to a 2ϵ-diﬀerential privacy. Thus, compared to the standard
neighbourhood, δ-neighbourhood “redistributes” the assur-
ance by placing more emphasis on close-by entities, with
the value of δ determines the rate the assurance decreases
over distance. Hence, we can view the δ-neighbourhood not
as a relaxation of the standard neighbourhood, but as a re-
distribution of assurance, where δ is a parameter controlling
the rate of redistribution.
5. PUBLISHING SPATIAL DATASET
Although an ϵ-diﬀerentially private mechanism under the
standard neighbourhood is also ϵ-diﬀerentially private un-
der δ-neighbourhood, it may not achieve our intention of
investing more budget on nearby entities. In this section,
we consider publishing histogram of 2D spatial points to
be used for subsequent range queries. Essentially, we wan-
t to determine an “optimal” linearly transformed histogram
similar to the work by Li et al. [15], but with a diﬀerent
sensitivity function derived from the δ-neighbourhood. We
observe that the sensitivity function in 2D leads to an in-
teresting combinatoric structure in the design of the linear
transformation, and propose a few constructions. We also
note that for 1D spatial points, a known technique under s-
tandard neighbourhood [9] can be easily modiﬁed to achieve
high utility under δ-neighbourhood, as shown in Section 5.5.
5.1 Illustration
4 , 3
4 , 2
4 , 1}. Let ci be the number of points
Let us demonstrate how to capitalize the notion of δ-
neighbourhood with the following simple example in 1D.
Consider a dataset containing (possibly with repetitions) 4
possible values: { 1
with value i/4. Table 1 gives a 1-diﬀerentially private mech-
anism under the standard neighbourhood that publishes the
counts (c1, c2, c3, c4).
Let us compare the case under 0.25-neighbourhood, and
with a source of {0}, i.e. points are added/removed within
distance of 0.25 from 0. The mechanism in Table 1 is also
1-diﬀerentially private under 0.25-neighbourhood.
Now let us publish the counts as shown in Table 2, where
a linear transformation is applied before adding noise. Our
main observation is that, the sensitivity of publishing the
values (a1, a2, a3, a4) is 1 with respect to the 0.25-neighb-
ourhood: changing a single entity by a distance of 0.25,
or adding an entity within 0.25 to the source aﬀects on-
ly one ai for some i. Hence, a Laplace noise of Lap(1)
is suﬃcient to guarantee 1-diﬀerential privacy under 0.25-
neighbourhood. However, under the standard neighbour-
hood, an entity changing from value 1
4 to 1 will decrease
each a1, a2, a3 by 1, leading to a sensitivity of 3.
′
i’s in Table 2, we can answer range
queries with higher accuracy through linear combination of
the ai’s. For example, when a query asks for the frequency
′
counts in the range [0.4, 0.6], reporting the value c
2 leads
to an unbiased estimator with variance 8, which is the vari-
ance of Lap(2). On the other hand, from Table 2, it can
′
2 giving an unbiased estimator with
be estimated by a
a smaller variance of 4, which is the variance of the sum
of two independent Laplace noises, Lap(1) + Lap(1). Such
diﬀerence is more signiﬁcant for larger query range. The
By publishing the a
1 − a
′
comparisons are shown in Table 3: row i of the table shows
the noise variances when the query range covers exactly i
number of the counts ci’s.
Table 1: Publishing ci’s directly.
Actual Values Published values
′
1 = c1 + Lap(2)
c1
′
2 = c2 + Lap(2)
c2
′
3 = c3 + Lap(2)
c3
′
c4
4 = c4 + Lap(2)
c
c
c
c
Table 2: Publishing a linearly transformed his-
togram.
Actual values
a1 = c1 + c2 + c3 + c4
a2 = c2 + c3 + c4
a3 = c3 + c4
a4 = c4
Published values
′
1 = a1 + Lap(1)
a
′
2 = a2 + Lap(1)
a
′
3 = a3 + Lap(1)
a
′
a
4 = a4 + Lap(1)
Table 3: Variance of the estimator for diﬀerent range
size.
Number of
ci’s covered
number of Derived from Derived from
queries
Table 1
Table 2
1
2
3
4
4
3
2
1
8
16
24
32
4
4
4
2
By exhaustive checking, it can be veriﬁed that, in terms of
minimizing the total variance of all possible range queries,
i.e. the weighted sum of the variance in the rightmost col-
umn with the weights in the second column in Table 3, the
construction in Table 2 is optimal among all linear combi-
nations of c1, c2, c3 and c4 where the coeﬃcients are binary,
i.e. either 0 or 1.
Note that the above methods estimate the query results
using linear combinations of the published values. One could
enforce the constraints that all ci’s are non-negative, leading
to a non-linear estimator. Although this may create bias, it
further lowers the variance of the estimation.
5.2 Generalization
Let us generalize the illustrating example. The method
shown in Table 1 corresponds to the direct method of adding
noise to the frequency counts of an equi-width histogram,
whereas Table 2 corresponds to a method that applies a lin-
ear transformation before adding noise. Li et al. [15] studied
such general form of publishing under the standard neigh-
bourhood. In this section, we extend it to δ-neighbourhood.
As illustrated in the example, the key diﬀerence of our method
is the lower sensitivity incurred under δ-neighbourhood.
Formally, a histogram HB(D) for a partition of the do-
main B = {b1, . . . , bk} on D gives a column vector of fre-
quency counts c = (c1, . . . , ck)t where ci = |D ∩ bi|. We call
each set in the partition B a bin.
In particular, an equi-
width histogram corresponds to a partition whose bins are
162of the same size. Since all the bins do not overlap, the ef-
fect of replacing an entity in D aﬀects frequency counts in
at most two bins, and thus the sensitivity of HB(·) is 2 un-
der the standard neighbourhood. Hence the mechanism of
publishing c + Lap(2/ϵ)k is ϵ-diﬀerential private under the
standard neighbourhood.
We consider queries whose answers are linear combination
of counts in c, and can be expressed as qc where q is a row
vector. For example, a range query can be a summation of
counts in some bins. For a sequence of m queries, let us
express it as an m by k matrix Q and hence the answer to
the queries are the coeﬃcients in Qc. As proposed by Li
et al., to answer the query Q, one may employ a strategy
A, which is represented as a k by n matrix for some n, and
publish
ec = Ac + Lap(△A/ϵ)n,
D, returns Ac. From the publishedec, we want to estimate
where △A is the sensitivity of the function that on input
the query results. It can be shown [21] that the following
estimate is unbiased:
A+ec,
where A+ = (AtA)
the variance of the estimator is
−1At is the pseudo-inverse of A, and
(△A)2trace(Q(AtA)
−1Qt).
(5)
Now, given Q, we want to ﬁnd the A s.t. the variance is
minimized. In the illustrating example, Q is a 10 by 4 matrix
where each row corresponds to a range queries, and
1 1 1 1
0 1 1 1
0 0 1 1
0 0 0 1
A =
(6)
(7)
Now let us look at the problem in the context of δ-neighb-
ourhood. The sensitivity of A under δ-neighbourhood leads
to an interesting combinatoric structure that is not present
in the standard neighbourhood. Under the standard neigh-
bourhood, the sensitivity of A is
{ ∥ai − aj∥1 },
max
i,j∈Zn
where each ai’s is a column vector in A, that is, A =
[a1, a2, . . . , an]. To understand the above expression, note
that ∥ai − aj∥1 is the sum of L1 diﬀerence when an enti-
ty change between bin i and bin j. Since the sensitivity is