ﬁngerprints with minimal human interaction.
We have shown how to automatically generate ﬁnger-
prints and have demonstrated that our approach is ﬂexible
and can be applied to different uses. In this paper we have
presented its application to two concrete examples: OS ﬁn-
gerprinting and DNS ﬁngerprinting. Our results show that
the produced ﬁngerprints are accurate and can be used by
ﬁngerprinting tools to classify unknown hosts into given
classes. We have also evaluated approximate matching as
a technique to assign an unknown host to a known imple-
mentation when no exact ﬁngerprint match is available.
In addition, our preliminary exploration of the candidate
query space has been able to ﬁnd new interesting queries,
not currently used by ﬁngerprinting tools. This conﬁrms our
intuition that the space of candidate queries remains largely
unexplored and demonstrates the effectiveness of our auto-
matic approach.
8. Acknowledgements
We would like to thank David Brumley for helpful dis-
cussions. We would also like to thank Fyodor, Bruce Maggs
and the anonymous reviewers for their valuable comments
to improve this paper. Juan Caballero would like to thank
la Caixa Foundation for the generous support through their
fellowship program.
References
[1] BIND. http://www.isc.org/index.pl?/sw/bind/.
[2] fpdns. http://www.rfc.se/fpdns/.
[3] Fyodor. Remote OS detection via TCP/IP ﬁngerprinting
(2nd generation). http://insecure.org/nmap/osdetect/.
[4] Fyodor. Remote OS detection via TCP/IP stack ﬁn-
gerprinting. Phrack 54, Vol. 8. December 25, 1998.
http://www.phrack.com/phrack/51/P51-11.
[5] IPpersonality. http://ippersonality.sourceforge.net/.
[6] Know your enemy: Passive ﬁngerprinting. identifying re-
them knowing. Honeynet project.
mote hosts, without
http://project.honeynet.org/papers/ﬁnger/.
[7] Morph. http://www.synacklabs.net/projects/morph/.
[8] Nessus. http://www.nessus.org/.
[9] Nmap. http://www.insecure.org/.
[10] p0f. http://lcamtuf.coredump.cx/p0f.shtml.
[11] Queso. http://www.l0t3k.net/tools/FingerPrinting/.
[12] Xprobe2. http://www.sys-security.com/.
[13] R. Beverly. A robust classiﬁer for passive TCP/IP ﬁnger-
printing. In Proceedings of the 5th Passive and Active Mea-
surement Workshop, 2004.
[14] A. Blum. On-line algorithms in machine learning. In Online
Algorithms, pages 306–325, 1996.
[15] D. Comer and J. C. Lin. Probing TCP implementations. In
USENIX Summer, 1994.
[16] J. Franklin, D. McCoy, P. Tabriz, V. Neagoe, J. V. Randwyk,
and D. Sicker. Passive data link layer 802.11 wireless de-
vice driver ﬁngerprinting. In Proceedings of the 15th Usenix
Security Symposium, 2006.
[17] M. Kearns and U. Vazirani. An Introduction to Computa-
tional Learning Theory. MIT Press, 1994.
[18] T. Kohno, A. Broido, and kc claffy. Remote physical device
ﬁngerprinting. In Proceedings of the IEEE Symposium on
Security and Privacy, 2005.
[19] R. Lippmann, D. Fried, K. Piwowarski, and W. Streilein.
Passive operating system identiﬁcation from TCP/IP packet
headers.
In Proceedings of the ICDM Workshop on Data
Mining for Computer Security, 2003.
[20] G. Malan, D. Watson, and F. Jahanian. Transport and appli-
cation protocol scrubbing. In Proceedings of IEEE INFO-
COM, 2000.
[21] P. V. Mockapetris. RFC 1035: Domain names — implemen-
tation and speciﬁcation, 1987.
[22] J. Padhye and S. Floyd. Identifying the TCP behavior of web
servers. In Proceedings of ACM SIGCOMM, 2001.
[23] V. Paxson. Automated packet trace analysis of TCP imple-
mentations. In Proceedings of ACM SIGCOMM, 1997.
[24] D. Pelleg and A. Moore. X-means: Extending k-means with
efﬁcient estimation of the number of clusters. In Proceed-
ings of the Seventeenth International Conference on Ma-
chine Learning, pages 727–734, San Francisco, 2000.
[25] J. Postel. Transmission control protocol. RFC 793 (Stan-
dard), 1981.
[26] K. Ramakrishnan, S. Floyd, and D. Black. The Addition of
Explicit Congestion Notiﬁcation (ECN) to IP. RFC 3168
(Proposed Standard), 2001.
[27] M. Smart, G. R. Malan, and F. Jahanian. Defeating TCP/IP
stack ﬁngerprinting. In Proceedings of the 9th USENIX Se-
curity Symposium, 2000.
[28] V. V. Vazirani. Approximation Algorithms. Springer-Verlag,
Berlin, 2001.
APPENDIX
A. Mistake Bounds for Conjunctions
We now bound the number of mistakes a ﬁngerprint will
make under certain assumptions. We give mistake-bounds
in an online model of learning [14], where the algorithm
starts with an initial ﬁngerprint and reﬁnes it with every mis-
take. Sections 4.2.1 and 4.2.2 present ofﬂine algorithms for
learning ﬁngerprints using a set of training hosts T . How-
ever, these ﬁngerprints may be too speciﬁc to T . Even after
testing the ﬁngerprints over the set of hosts E, the ﬁnger-
prints may not be sufﬁciently general. If T and E are not
large enough or sufﬁciently representative of the implemen-
tation classes, the conﬁdence guarantees we get on the re-
sulting ﬁngerprints might not be very high. This could hap-
pen, for example, when one is restricted to hosts within the
local network. In this case, the generated ﬁngerprints might
be too speciﬁc to the local network.
In an online model of learning, an algorithm starts with
an initial ﬁngerprint, and keeps reﬁning it every time it
makes a mistake; i.e.
the algorithm predicts a classiﬁca-
tion based on the current ﬁngerprint and is then given the
right answer, which it uses to update its ﬁngerprint. In our
setting, these initial ﬁngerprints could be generated ofﬂine.
After learning over T and testing over E, we can use these
ofﬂine ﬁngerprints with online algorithms and guarantee
that over the set of all hosts classiﬁed (i.e., hosts classiﬁed
by the ﬁngerprint after it was generated using E and T ), the
number of mistakes we make is bounded. Obviously, small
mistake-bounds are what we want.
We derive improved mistake bounds for learning con-
junction ﬁngerprints in this online model of learning. For
the conjunction ﬁngerprint, Theorem 1 shows that the mis-
take bound is small: when the initial ﬁngerprint has n
position-substrings and the true ﬁngerprint has t, the mis-
take bound is n−t. Under certain assumptions, it is ⌈log n
t ⌉,
where n is the number of position-substrings considered,
and t is the number of position-substrings in the conjunction
ﬁngerprint. This implies that we will make only ⌈log n
t ⌉
mistakes (under certain assumptions) before reaching the
right conjunction ﬁngerprint. The mistake bound for the de-
cision lists is much larger [14] and therefore, not practically
useful.
We now present the theorem for the mistake-bounds for
conjunction ﬁngerprints. To do so, we need the following
deﬁnitions and notation, so that we can represent the ﬁn-
gerprints and the response strings from the hosts as boolean
functions and boolean vectors respectively.
Since our ﬁngerprints denote the presence of position-
substrings corresponding to pre-speciﬁed queries, we will
deﬁne an element of a ﬁngerprint to be a single position-
substring along with the corresponding query identiﬁer. Let
U be the set of all the elements in the ﬁngerprints of all the
implementation classes under consideration, and let |U | =
n. An instance Xj represents the response strings of a host
that needs to be classiﬁed, and is a vector in {0, 1}n where
the ith coordinate is 1 if the ith element of U is present in
the response strings and 0 otherwise.
Next, we describe how to represent a ﬁngerprint as a
boolean function. Let yi be a boolean variable that denotes
the presence of the ith element in U (e.g., if the ith ele-
ment of U must be present in the ﬁngerprint, yi is in the
corresponding boolean function.) Let Y = {y1, . . . , yn}.
Let H be the class of monotone conjunctions over Y (so,
no negative literals of Y are allowed in the conjunctions).
Let h1, h2 ∈ H be the conjunctions that represent classes
1 and 2 respectively. Let A1 be the set of boolean vari-
ables present in h1 and let A2 be the set of boolean variables
present in h2.
We give bounds under two cases: ﬁrst, with no further
assumptions; second, under the following two assumptions:
(1) A1 and A2 are disjoint, and (2) any instance that be-
longs to h2 contains no variable in A1 and vice versa. The
ﬁrst assumption is that A1 and A2 are disjoint; no variable
present in h1 is also present in h2 and vice versa. In our set-
When we get a new instance Xj (from I1 or I2) that
needs to be classiﬁed, we do the following: If the num-
ber of variables in T rueS(Xj) is greater than the number
of variables in F alseS(Xj), we classify Xj as true, other-
wise we classify it as false. If we make a mistake on an
instance which does not belong to h1 (so we report “true”
when we should have reported false), we remove the vari-
ables in T rueS(Xj) from S. If we make a mistake on an
instance that belongs to h1 (so we report false when we
should have reported true), we will remove all the variables
in F alseS(Xj) from S.
This procedure will give us a bound of at most ⌈log( n
t )⌉
mistakes, since each mistake causes us to remove at least
half the variables that are present in S, but are not present
in the true hypothesis. So, if we make a mistake on an
instance that belongs to h1, at least half the variables in
S must have been false in Xj. All of these will belong
to F alseS(Xj). Now, none of these variables will be
present in A1: since Xj belongs to h1 and h1 is a mono-
tone conjunction, all variables in A1 must be set to true
in Xj (i.e., A1 ⊆ Ones(Xj )). So, A1 is disjoint from
Zeros(Xj), therefore, none of the variables in A1 will be in
F alseS(Xj). Therefore we can remove all of the variables
in F alseS(Xj) from S.
Likewise, if we make a mistake on an instance that does
not belong to h1, at least half the variables in S must have
been true in Xj. Let Yrem denote the set of variables
in Y that are not in A1 or A2; so Yrem = Y − (A1 ∪
A2). Since this instance Xj belongs to I2, by assump-
tion, Ones(Xj) ⊆ A2 ∪ Yrem. None of these variables
can be present in A1 (since A1 and A2 are disjoint), so they
can be discarded from S. Therefore, since T rueS(Xj) ⊆
Ones(Xj), we can discard the set T rueS(Xj) from S.
Thus, since we reduce the set of variables in the con-
junction by at least half with every mistake, we will make
t )⌉ mistakes when we start with a conjunction of size
⌈log( n
n, and our true conjunction is of size t.
There are also mistake-bounds for learning decision lists
in the literature [14], however, they are quite loose and
therefore not of practical use.
ting, this implies that the position-substrings present in one
conjunction ﬁngerprint are not present in the other. This is
not an unreasonable assumption; we see this in the testing,
especially when there are only two implementation classes
under consideration. The second assumption is that no in-
stance that belongs to h2 contains the variables in A1 and
vice versa. We might, for example, expect this to be true
when all the position-substrings consist of distinct values
for the same ﬁelds of the underlying packet headers.
Theorem 1. Assume that there are two implementation
classes, each of which has ﬁngerprints that can be rep-
resented by a conjunction of position-substrings. Let H
be the class of monotone conjunctions over Y , and let
h1, h2 ∈ H denote the conjunction ﬁngerprints with t vari-
ables for class I1 and I2 respectively. Let A1 and A2 denote
variables present in h1 and h2 respectively. With no further
assumptions, h1 and h2 have a mistake bound of n − t.
When A1 and A2 are disjoint, and when every instance that
is consistent with h1 does not contain any variable in A2
and vice versa, we can learn a conjunction ﬁngerprint with
t variables with a mistake bound of ⌈log( n
t )⌉ on instances
that belong to I1 and I2.
We now present the proof of this theorem.
Proof. We will show how to use a conjunction ﬁngerprint
to get a bounded number of mistakes for each case in the
theorem statement. Let S denote the set of variables in the
current conjunction hypothesis for I1. Let Xj ∈ {0, 1}n
denote the current instance. Let T rueS(Xj) denote the set
of variables in Xj that are set to true and are also present
in S. Let F alseS(Xj) denote the set of variables in Xj
that are set to false are also present in S. Let Ones(Xj),
Zeros(Xj) denote the sets of variables in Y that are set
to true and false respectively in the instance Xj. Note
that T rueS(Xj) = Ones(Xj) ∩ S, while F alseS(Xj) =
Zeros(Xj) ∩ S.
The proof for the ﬁrst case is well known but we sketch
it for completeness. In the ﬁrst case, we will begin with
the most speciﬁc conjunction over Y : the conjunction y1 ∧
y2 . . .∧yn. So, we begin with S = Y . We do the following:
every time we make a mistake on an instance Xj ∈ h1,
we remove all the variables in F alseS(Xj) from S. We
never make a mistake on an instance Xj /∈ h1 since we
start with S ⊇ A1 and never remove a variable in A1. Thus,
the number of mistakes we can make is bounded by n − t.
Next, we outline the proof for the second case. We
will analyze the number of mistakes made to reach the cor-
rect conjunction for the implementation class I1, on the in-
stances that come from I1 and I2. We begin with the most
speciﬁc conjunction over Y : the conjunction y1∧y2 . . .∧yn.
So, we begin with S = Y .
B. Headers
TCP header from [25] with added ECE and CWR ﬂags.
0
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
1
2
3
|
Source Port
Sequence Number
Destination Port
Acknowledgment Number
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|C|E|U|A|P|R|S|F|
| Data |
|
|
| Offset|Reserv.|W|C|R|C|S|S|Y|I|
|
|R|E|G|K|H|T|N|N|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|
|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
Urgent Pointer
Checksum
Options
Padding
Window
data
|
|
|
DNS header from RFC 1035 [21].
Header
0
1 2
3 4
5 6
7 8
1
9 0
1 1
1 2
1 1
3 4
1
5
Z
|
ID
| |
RCODE
Header
Opcode |AA|TC|RD|RA|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ +------------------+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ +------------------+
|QR|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ +------------------+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ +------------------+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ +------------------+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ +------------------+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
Question
Additional
Authority
Question
QDCOUNT
ANCOUNT
NSCOUNT
ARCOUNT
Answer
| |
| |
| |
| |
0
1 2
3 4
5 6
7 8
1
9 0
1 1
1 2
1 1
3 4
1
5
QNAME
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
|
|
/
/
/
/
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
|
|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
QCLASS
QTYPE