title:Reversible sketches for efficient and accurate change detection over
network data streams
author:Robert T. Schweller and
Ashish Gupta and
Elliot Parsons and
Yan Chen
Reversible Sketches for Efﬁcient and Accurate Change
Detection over Network Data Streams
Robert Schweller, Ashish Gupta, Elliot Parsons, Yan Chen
Department of Computer Science
Northwestern University
Evanston, IL 60201-3150, USA
fschwellerr, ashish, e-parsons, PI:EMAIL
ABSTRACT
Trafﬁc anomalies such as failures and attacks are increasing in fre-
quency and severity, and thus identifying them rapidly and accu-
rately is critical for large network operators. The detection typi-
cally treats the trafﬁc as a collection of ﬂows and looks for heavy
changes in trafﬁc patterns (e.g., volume, number of connections).
However, as link speeds and the number of ﬂows increase, keeping
per-ﬂow state is not scalable. The recently proposed sketch-based
schemes [14] are among the very few that can detect heavy changes
and anomalies over massive data streams at network trafﬁc speeds.
However, sketches do not preserve the key (e.g., source IP address)
of the ﬂows. Hence, even if anomalies are detected, it is difﬁcult
to infer the culprit ﬂows, making it a big practical hurdle for online
deployment. Meanwhile, the number of keys is too large to record.
To address this challenge, we propose efﬁcient reversible hash-
ing algorithms to infer the keys of culprit ﬂows from sketches with-
out storing any explicit key information. No extra memory or mem-
ory accesses are needed for recording the streaming data. Mean-
while, the heavy change detection daemon runs in the background
with space complexity and computational time sublinear to the key
space size. This short paper describes the conceptual framework
of the reversible sketches, as well as some initial approaches for
implementation. See [23] for the optimized algorithms in details.
Evaluated with netﬂow trafﬁc traces of a large edge router, we
demonstrate that the reverse hashing can quickly infer the keys of
culprit ﬂows even for many changes with high accuracy.
Categories and Subject Descriptors
C.2.3 [Network Operations]: Network monitoring
General Terms
Measurement, Algorithms
Keywords
Change detection, Network anomaly detection, Data stream com-
putation, Sketch, Modular hashing, IP mangling, Reverse hashing
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’04, October 25–27, 2004, Taormina, Sicily, Italy.
Copyright 2004 ACM 1-58113-821-0/04/0010 ...$5.00.
1.
INTRODUCTION
Real-time network ﬂow monitoring at high packet rates is a chal-
lenging but crucial service for network administrators of large ISPs
or institutions. Such service is important for accounting, provision-
ing, trafﬁc engineering, scalable queue management, and anomaly
and intrusion detection [5, 8, 14]. Take the intrusion detection
systems (IDSs) for instance, most existing IDSs reside on single
host or low-end routers, examining the application-level [22, 15]
or system-level [13, 25] logs, or the sniffed network packets [20,
21]. However, today’s fast propagation of viruses/worms (e.g., Sap-
phire worm) can infect most of the vulnerable machines in the In-
ternet within ten minutes [18] or even less than 30 seconds with
some highly virulent techniques [24]. Thus it is crucial to identify
such outbreaks in their early phases, which can only be possibly
achieved by detection at routers instead of at end hosts [26].
Given today’s trafﬁc volume and link speeds, the detection method
has to be able to handle potentially several millions or more of con-
current network time series. Thus it is either too slow or too ex-
pensive to directly apply existing techniques on a per-ﬂow basis [8,
14]. The essential requirements for online ﬂow-level monitoring
on high-speed networks are two-fold: 1) small amount of mem-
ory usage (to be implemented in SRAM) and 2) small amount of
memory accesses per packet [5, 8]. In response to this demand,
the ﬁeld of data streaming computation is emerging, which deals
with various computations that can be performed in a space- and
time-efﬁcient fashion. Most of the existing work comes from the
database and theory communities, as reviewed in a comprehensive
survey [17]. Here we call for bringing techniques from these do-
mains to bear on networking. One particularly powerful technique
is the sketch [10], a probabilistic summary technique for analyzing
large network trafﬁc streams without keeping per-ﬂow states.
Most existing research on data streaming computation focus on
scalable heavy-hitter detection [8, 16, 2]. However, heavy-hitters
do not necessarily correspond to ﬂows experiencing anomalies (e.g.,
signiﬁcant changes), and thus it is not clear how their techniques
can be adapted for anomaly detection. Here, we focus on a more
generic and powerful primitive: heavy change detection, which
spans from simple absolute or relative changes, to variational changes
and linear transformation of these changes for various time-series
forecasting models [14]. Recently, a variant of the sketch data
structure, the k-ary sketch, was proposed as one of the ﬁrst schemes
for real-time heavy change detection over massive data streams [14].
As shown in Section 2, the k-ary sketch uses a constant, small
amount of memory, and has constant per-record update and recon-
struction cost [14].
However, one major obstacle for building anomaly/intrusion de-
tection system on k-ary sketch is its irreversibility. As modeled in
Section 2.1, the streaming data can be viewed as a series of (key,
value) pairs where the key can be a source IP address, or the pair of
IP addresses, and the value can be the number of bytes or packets,
etc. While for any given key, sketch can indicate if it exhibits big
change, and if so give an accurate estimation of such change, such
process is irreversible. That is, a sketch cannot efﬁciently report
the set of all keys that have large change estimates in the sketch.
This means that to compare two streams, we have to know which
items (keys) to query to ﬁnd the streams with big changes [14, 5].
This would require either exhaustively testing all possible keys,
or recording and testing all data stream keys and corresponding
sketches. Unfortunately, neither of these are scalable.
In this paper we focus on this problem and provide efﬁcient algo-
rithms to reverse sketches, focusing primarily on k-ary sketches [14].
The observation is that only streaming data recording needs to done
continuously in real-time, while the change/anomaly detection can
run in the background with more memory (DRAM) and at a fre-
quency only in the order of seconds. Then the challenge is: how to
keep extremely fast data recording while still being able to detect
the heavy change keys with reasonable speed and high accuracy?
In this extended abstract, we set up the general framework for the
reversible k-ary sketch, and discuss some initial approaches for im-
plementation. The fully optimized algorithms and evaluations are
presented in [23], especially for multiple heavy change detection.
With no or negligible extra memory and extra memory accesses for
recording streaming data, the heavy change detection daemon runs
in the background with space complexity and computational time
sublinear to the key space size.
The rest of the paper is organized as follows. In Section 2, we
introduce the data stream model, formulate the heavy change de-
tection problem, and present the architecture of reversible k-ary
sketch system. The system has two parts: streaming data record-
ing (Section 3) and heavy change detection (Section 4). We show
some preliminary evaluation results based on a large edge router
trafﬁc data in Section 5. For detecting the keys corresponding to
the top 100 changes, we achieve over 95% of true positive rate and
less than 2% of false positive rate in 0.42 seconds. The streaming
data are recorded with less than 200KB memory. Related work are
surveyed in Section 6, and ﬁnally the paper concludes in Section 7.
2. OVERVIEW
2.1 Data Stream Model and the k-ary Sketch
Among the multiple data stream models, one of the most general
is the Turnstile Model [19]. Let  = (cid:11)1; (cid:11)2; : : : ; be an input stream
that arrives sequentially, item by item. Each item (cid:11)i = ai; 	i
consists of a key ai 2 [], where [] = f0; 1; : : : ;    1g, and an
update 	i 2 R. Each key a 2 [] is associated with a time varying
signal U [a]. Whenever an item ai; 	i arrives, the signal U [ai] is
incremented by 	i.
-ary sketch is a powerful data structure to efﬁciently keep ac-
curate estimates of the signals U [a]. A k-ary sketch consists of  
hash tables of size  . The hash functions for each table are cho-
sen independently at random from a class of hash functions from
[] to []. From here on we will use the variable  =  in-
terchangeably with . We store the data structure as a    
table of registers T [i][j] i 2 [ ]; j 2 []. Denote the hash
function for the ih table by hi. Operations on the sketch include
INSERT(a, 	) and ESTIMATE(a). Given a data key and an update
value, INSERT(a,	) increments the count of bucket hia by 	 for
each hash table hi. Let SUM = j2[] T [0][j] be the sum of all
updates to the sketch. The operation ESTIMATE(a) for a given key
a returns the following.
a = mediani2[ ]fvhi
ve
a g
(1)
where
vhi
a =
T [i][hia]   SU

1   1=
If the hash functions in the sketch are 4-universal, this estimate
gives an unbiased estimator of the signal U [a] with variance in-
versely proportional to    1 [14]. See [14] for details on the
appropriate selection of   and  to obtain accurate estimates.
2.2 Change Detection Problem Formulation
-ary sketches can be used in conjunction with various forcast-
ing models to perform sophisticated change detection as discussed
in [14]. We focus on the simple model of change detection in
which we break up the sequence of data items into two temporally
adjacent chunks. We are interested in keys whose signals differ
dramatically in size when taken over the ﬁrst chunk versus the sec-
ond chunk. In particular, for a given percentage (cid:30), a key is a heavy
change key if the difference in its signal exceeds (cid:30) percent of the
total change over all keys. That is, for two inputs sets 1 and 2,
if the signal for a key x is is U1[x] over the ﬁrst input and U2[x]
over the second, then the difference signal for x is deﬁned to be
D[x] = jU1[x] U2[x]j. The total difference is D = x2[] D[x].
A key x is then deﬁned to be a heavy change key if and only if
D[x] (cid:21) (cid:30)  D.
In our approach, to detect the set of heavy keys we create two k-
ary sketches, one for each time interval, by updating them for each
incoming packet. We then subtract the two sketches. Say S1 and
S2 are the sketches recorded for the two consecutive time intervals.
For detecting signiﬁcant change in these two time periods, we ob-
tain the difference sketch Sd = jS2   S1j. The linearity property
of sketches allows us to add or subtract sketches to ﬁnd the sum or
difference of different sketches. Any key whose estimate value in
Sd that exceeds the threshold (cid:30)  SU  = (cid:30)  D is denoted as a
suspect heavy key in sketch Sd and offered as a proposed element
of the set of heavy change keys.
Instead of focusing directly on ﬁnding the set of keys that have
heavy change, we instead can attempt to ﬁnd the set of keys denoted
as suspects by a sketch. [14, 23] discuss how to choose appropriate
values for  and   so that the set of suspects is a sufﬁciently
good approximation to the set of actual heavy change keys. For
simplicity we focus on the simpler problem of ﬁnding the set of
keys that hash to heavy buckets in all   hash tables. That is, we
can think of our input as a sketch T in which certain buckets in each
hash table are marked as heavy. Let  be the maximum number of
distinct heavy buckets in any given hash table, we get the following
Reverse Sketch Problem:
Input: An integer  > 0, a sketch T with hash functions fhig  1
i=0
from [] to [], and for each hash table i a set of at most 
heavy buckets Ri (cid:18) [];
Output: All x 2 [] such that hix 2 Ri for each i 2 [ ].
Solving the reverse sketch problem is a good way to approximate
the set of heavy change keys. Consider the case in which there is
exactly one heavy bucket in each hash table. The expected number
of false positives (number of keys that hash to all heavy buckets
by chance) for   hash tables is E[x] =  1
   where  is the
size of the bucket space and  is the size of the key space. For
  = 5,  = 232 and  = 212 we get E[x] = 3:7  10 9, which
is exceedingly small. Thus solving the reverse sketch problem is
an effective way to converge to the set of heavy change keys. To
reduce false negatives, in [23] we consider the more general version
of this problem in which we are interested in ﬁnding the set of keys
that map to heavy buckets in at least      of the   hash tables.
For simplicity in this paper we focus on the algorithms for the case
of  = 0.
2.3 Architecture
The conceptual framework of our change detection system has
two parts as in Fig. 1: streaming data recording and heavy change
detection. Next, we will introduce each part in this system.
Streaming
data 
recording
value
key
IP mangling
Modular
hashing
Reversible 
k-ary sketch
value stored
Heavy 
change 
detection
change
threshold
Reversible 
k-ary sketch
Reverse
hashing
Reverse 
IP mangling
heavy 
change 
keys
Figure 1: Conceptual architecture of the reversible k-ary
sketch based heavy change detection system.
3. RECORDING OF DATA STREAMS
The ﬁrst phase of change detection involves receiving hkey, updatei
pairs one after another from an incoming data stream, and record-
ing them in a summary data structure. As discussed in Section 1,
each update should require very few memory accesses and the en-
tire summary structure should be small enough to ﬁt into fast mem-
ory. These requirements are fulﬁlled by the k-ary sketch. However,
to allow reversibility, we modify the update procedure of the k-ary
sketch with modular hashing (see Section 3.1). To maintain the
accuracy of the sketch with this type of hashing, we also need to
perform IP-mangling (see Section 3.2).
3.1 Modular Hashing
32 bits
10010100
10101011
10010101
10100011
8 bits
h1()
h2()
h3()
h4()
010
110
001
101
010 110 001 101
IP address 
divided into
q=4 words
Four separate hash
functions applied to 
each word
Four hash 
functions 
combined to form 
final hash value
Figure 2: Modular hashing uses  hash functions to hash each
word of the key, which are then combined for the ﬁnal hash.
1
1
 ] to [
Modular hashing is illustrated in Figure 2. Instead of hashing
the entire key in [] directly to a bucket in [], we partition the
key into  words, each word of size 1
  g  bits. Each word is then
hashed separately with different hash functions which map from
space [
 ]. For example, in Figure 2, a 32-bit IP address
is partitioned into  = 4 words, each of 8 bits. Four independent
hash functions are then chosen which map from space [28] to [23].
The results of each of the hash functions are then concatenated to
form the ﬁnal hash.
In our example, the ﬁnal hash value would
consist of 12 bits, deriving each of its 3 bits from the separate hash
functions hi;1; hi;2; hi;3; hi;4. If it requires constant time to hash
a value, modular hashing increases our update time from   to
 . In Section 4, we discuss how this modular hashing allows
us to efﬁciently perform change detection.
However, an important issue with modular hashing is the quality
of the hashing scheme. The probabilistic estimate guarantees for k-