ﬁltering based on ﬁelds in the Link (Ethernet), Network (IP),
and Transport (TCP/UDP) layers.
As a result, there is a semantic gap between the high-level
entity identiﬁers, like hostnames and usernames, that we would
like to deﬁne policy over, and the enforcement of these policies
(ﬂow rules) in the network. In order to handle policies written
with high-level identiﬁers, these identiﬁers must be mapped to
the correct set of low-level identiﬁers that hardware can use to
enforce the policy, e.g., MAC and IP addresses. At the same
time, mappings between high- and low-level identiﬁers change
regularly. Consider a wired host that moves from one physical
network port
to another, a wireless host moving between
access points, or dynamic DNS mappings between a hostname
and IP addresses. Any mechanism used to map between high-
and low-level identiﬁers must maintain correctness in the face
of these changes.
Caching Policy in Switches. Managing the installation and
caching of ﬂow rules in switches is challenging when the set
of policy rules is large and changes over time. Proactively
installing ﬂow rules that implement policies is not efﬁcient
when the set of policies is large, because hardware switches
can only store a limited number of rules, usually in the range
of 512 to 8,192 [19]–[22]. There may simply not be space
for all policy and routing rules for all possible allowed ﬂows.
Therefore, the subset of rules to install in the switches must
be chosen intelligently.
Soft ﬂow timeouts cause a rule to be deleted if a packet has
not been matched against that rule for a speciﬁed amount of
time. This prevents long-running ﬂows from being interrupted.
However, it also introduces a correctness issue: stale ﬂow rules
that no longer reﬂect current access control policy continue to
reside on switches for as long as they in use. In effect, access
control policy updates would not be applied to pre-existing
ﬂows.
Since neither of these mechanisms is sufﬁcient, a consis-
tency management system is needed that provides both timely
Furthermore, policy rules may contain high-level identiﬁers
that cannot be resolved to low-level identiﬁers until a later
time. For example, the policy rule “The device with hostname
h1 is not allowed to send to TCP port 22 on the device
with hostname h2” must be compiled into one written over
low-level identiﬁers for the switches, as discussed previously.
However, if h1 or h2 is not yet on the network, it might not
have an IP address reserved. Until that happens, the policy
rule cannot be compiled into a ﬂow rule and pushed to the
switch.
Policy-Switch Consistency. In an event-driven access con-
trol system, policy changes may be frequent. The system must
be able to change the policy enforced on the network in a
timely manner by removing stale rules that are inconsistent
with the current security policy.
OpenFlow-based SDNs support two automated mechanisms
for removing stale rules: hard and soft timeouts. Neither is
suitable as-is for the proposed system, due to unacceptable
performance or security implications.
Hard ﬂow timeouts cause a rule to be deleted after a
speciﬁed amount of time. While this helps bound how long
a rule may be stale, hard timeouts interact poorly with long-
running ﬂows. If the hard timeout expires while the rule is still
in active use, packets (possibly thousands in a high bandwidth
ﬂow) will be forwarded to the control plane until the rule is
re-installed. Control-plane processing is orders of magnitude
slower than hardware-backed forwarding, so timing-out an
active ﬂow could cause both a noticeable latency spike and
impose additional load on the control plane.
and efﬁcient expiration of stale rules.
Bypass Prevention. Even the most secure access control
policy is not useful if it can be circumvented easily. For
that reason, it is important to design access control systems
to be resistant to bypass. One area of concern in SDNs is
ensuring that the controller cannot be compromised before
access control checks are done. A number of recent works
have demonstrated the vulnerability of controllers to various
poisoning attacks on topology and network identiﬁers, as well
as malicious apps [23]–[26]. This suggests that access control
must be done before any other processing in the control plane.
That way, illegal packets can be rejected before they are able to
poison other controller components, possibly to bypass access
control mechanisms.
Efﬁcient Operation. Finally, in addition to handling policy
updates and new ﬂows correctly, an access control system with
event-driven policies must also perform efﬁciently, minimizing
undue latency on ﬂows and load on the control plane.
B. DFI Architecture
At a high level, DFI consists of ﬁve components that interact
with each other to decide on policies in response to events,
manage current policies, resolve high-level identiﬁers, compile
policies to ﬂow rules, and then manage those rules in the
switches. We discuss each of these components below and
display the overall architecture in Figure 1.
Policy Decision Points (PDPs). The role of a PDP is
to evaluate conditions that apply to a desired event-driven
access control policy – for example, “When host h1 has
a log-on event, enable its network access”. The PDP then
decides whether its policy applies based on those conditions,
and automatically creates or revokes rules that
implement
the current policy. To do this, a PDP subscribes to zero or
more sensor feeds, whose events can originate from the data
plane (e.g., DNS, DHCP servers), end hosts (e.g., anti-virus
software), the control plane (e.g., OpenFlow events), or even
off-network (e.g., a building alarm system).
DFI supports deploying multiple PDPs, enabling each to
focus on providing a particular type of policy (Role-Based Ac-
cess Control, Quarantine Upon Compromise, etc.). Conﬂicts
between policy rules emitted by different PDPs are resolved
by the Policy Manager using a unique priority assigned to the
PDP by the network administrator.
Policy rules themselves are tuples consisting of (Action,
Flow Properties, Source, Destination). Action can be Allow
or Deny, and Flow Properties include EtherType and IP
protocol values. Source and Destination describe the endpoints
of ﬂows matching this rule as tuples over the following
identiﬁers: username, hostname, IP address, TCP/UDP port,
MAC address, switch port, and switch DPID. Each ﬁeld can
be either a speciﬁc value or a wildcard.
For example, a PDP that enforces a user-based policy might
emit the following policy:
(Allow, (∗,∗),(Alice,∗,∗,∗,∗,∗,∗,∗,∗),
(Bob,∗,∗,∗,∗,∗,∗,∗,∗))
This policy would permit any machine that Alice is using to
communicate over any protocol with any machine that Bob is
using.
If a PDP later determines that a policy rule it has generated
no longer applies,
it can revoke that rule using a unique
identiﬁer assigned when the policy rule was added. Revocation
is distinct from installing a second policy rule with the
opposite Action. After revocation, a previously-matched ﬂow
will now be matched against any other policy rules, likely
from other PDPs. A PDP should not generate multiple rules
that match a single ﬂow with conﬂicting Action values, since
rules inherit the priority value of their PDP; however, in this
case, the Deny action will be used to err on the side of stopping
unauthorized ﬂows. Similarly, in the absence of any matching
policy rule, DFI is conﬁgured to deny a ﬂow by default.
Policy Manager. The Policy Manager receives policy rules
and revocations from PDPs, performs consistency checks, and
stores the current global policy. It also enables the Policy
Compilation Point to query current policy to determine what
action should be taken for a speciﬁc ﬂow.
Consistency checks are extremely important to solving the
Policy-Switch Consistency challenge mentioned earlier. When
a new policy rule is inserted, the Policy Manager identiﬁes any
existing policy rules that potentially conﬂict with the new one,
since these policy rules may have been used to add ﬂow rules
still present in the network’s switches. Conﬂicts are possible
where: 1) each ﬂow identiﬁer in an existing rule matches the
new one (exactly or wildcarded), 2) the policy actions are
different, and 3) the priority of the existing rule is lower
than the priority of the new rule. The Policy Manager then
tells the Policy Compilation Point to remove any ﬂow rules
derived from these conﬂicting policies from the switches, as
described later. Conﬂicting policies are not removed from the
policy database: this action merely ﬂushes rules installed in the
switches, ensuring that ongoing ﬂows potentially affected by
the policy change will be re-evaluated. When a policy rule is
explicitly revoked by a PDP, the Policy Manager also instructs
the Policy Compilation Point to ﬂush any ﬂow rules derived
from this policy from the switches. In this way, ﬂow rules are
removed quickly without paying the latency and performance
costs of using hard timeouts.
Entity Resolution Manager. The Entity Resolution Man-
ager is responsible for maintaining current mappings between
the high-level identiﬁers, like usernames and hostnames, that
are used in policy rules and the actual low-level identiﬁers,
like IP addresses and MAC addresses, that appear in network
trafﬁc and can be used in the switch’s ﬂow rules. Additionally,
it prevents spoofed trafﬁc from being able to bypass policy by
ensuring that identiﬁers at all levels must match the expected
bindings. To do this, the Entity Resolution Manager tracks the
four identiﬁer bindings shown in Figure 3, linking username
↔ hostname ↔ IP address ↔ MAC address ↔ switch and
port.
Some of these bindings are many-to-many and can change
over time. For instance, users may log onto multiple hosts,
which may have more than one IP address associated with dif-
Fig. 1: Architecture of DFI
ferent network interface cards, all of which may be connected
at different physical switch ports. Similarly, IP addresses may
change across DHCP leases, machines may move to different
physical ports, and MAC addresses change with interface (e.g.,
wired vs. wireless).
To maintain these bindings, the Entity Resolution Manager
subscribes to events from identiﬁer-binding sensors across
the network. These sensors are positioned to collect bindings
from authoritative data sources. Authoritative sources are those
responsible for providing one part of the binding between
two identiﬁers. For example, DNS is the authoritative source
for the binding between a hostname and an IP address, as it
provides the hostname for that IP address. Hence, our sensor
for the hostname to IP address binding collects those bindings
directly from the DNS server. Similarly, our IP Address to
MAC address sensor collects its bindings directly from the
DHCP server, which is authoritative for that binding. By using
authoritative data sources, we prevent attackers from poisoning
the Manager with illegitimate state. Poisoning attacks from
these authoritative sources are out of scope for this work, since
an attacker who has compromised the sources could simply
assign themselves identiﬁers needed to match or circumvent a
target access control policy.
Using its knowledge of current identiﬁer bindings, the Entity
Resolution Manager responds to queries about new ﬂows
from the Policy Compilation Point, returning any identiﬁers
associated with the source and destination of the queried ﬂow.
For instance, given an IP address and MAC address from
a packet, the Entity Resolution Manager would return any
associated hostname or username. This information can then
be used to identify matching policy rules.
Instead of mapping high-level identiﬁers in policies to low-
level identiﬁers when those policies are added, we choose to
map low-level identiﬁers in packets to high-level identiﬁers
during the access control decision. This is crucial to solve a
number of challenges we discussed earlier. First, it ensures
that the mappings between high-level and low-level identiﬁers
are current at the time the access control decision is made.
If policy identiﬁers were mapped when the policy rule is
inserted, the policy rule would become incorrect as soon as any
identiﬁer bindings used in the mapping changed. Second, it
enables us to write policy using identiﬁers that do not currently
have bindings. For instance, we can write policy for a user
who is not currently logged onto any machine. If identiﬁers
are mapped when the policy rule is added, this would cause an
error. However, mapping identiﬁers during the access control
decision avoids this problem, because bindings for the user
must exist (updated at the user log-on event) at the time she
sends trafﬁc.
Policy Compilation Point (PCP). The PCP is ultimately
responsible for managing DFI’s policy rules in the switches. In
particular, the PCP processes new ﬂow requests from switches
and installs rules that apply the current policy for the ﬂow.
As mentioned earlier, the PCP also ﬂushes ﬂow rules from
switches at the direction of the Policy Manager after a PDP
decides to update a policy.
When a switch receives a packet that does not match an
installed ﬂow rule in Table 0, it forwards that to the control
plane as an OpenFlow Packet-in event, where the DFI
Proxy (detailed below) will send it to the PCP. The PCP parses
the Packet-in event and collects all source and destination
identiﬁers present in the packet header (e.g., MAC addresses
and IP addresses) as well as any information supplied by the
switch (e.g., in-port on which packet was received). It then
queries the Entity Resolution Manager with this information
to obtain any other associated identiﬁers, like hostname and
username, and then queries the Policy Manager for policies
that match this ﬂow. The Policy Manager will return the
highest-priority policy rule matching the ﬂow, if any. Using
the action speciﬁed in the policy rule, the PCP creates a ﬂow
rule speciﬁc to this ﬂow and installs it into the switch. If
no policy rule matches a new ﬂow, DFI uses a default Deny
policy. Each ﬂow rule is built to match only the exact ﬂow that
was examined by the PCP – all available identiﬁers (e.g., MAC
addresses, IP addresses, TCP/UDP ports, etc.) are speciﬁed in
the rule. This ensures that each new ﬂow will be checked
against current policy by DFI. We also note that allowing a
packet to be forwarded at a switch results in the next switch
in the ﬂow’s data path receiving it and repeating this process,
so the correct policy is always applied at each hop in the ﬂow.
PolicyCompilationPointIdentifier Binding Sensor EventsOpenFlowSwitchTable 0DFI firewallisolated from controllerTable 1+Managed by controller for routing, etc.AllowDenyEnd HostsOpenFlowControllerOpenFlow Data PlaneDFI Control PlaneDFI ProxyPolicy Decision Point(s)PolicyManagerEntity Resolution ManagerDFI Sensor GatewayOpenFlow Control PlaneLegacy ServersPolicy Sensor(s)Identifier Binding Sensor(s)PolicyCommandsPolicy Sensor EventsQuery/ResponseQuery/ResponseThe PCP also processes rule removal requests from the
Policy Manager. As discussed above, these requests are issued
when policy is added or removed by PDPs in order to ensure
that the ﬂow rules cached on switches remain consistent with
the current policy. To accomplish this, each rule inserted into
a switch is tagged with a small piece of persistent metadata—
the cookie value in OpenFlow—indicating the policy from
which it is derived. Then, the PCP uses the metadata value
corresponding to the relevant policy to tell switches to ﬂush
rules derived from that policy.
While minimizing the number of ﬂows processed is beyond
the scope of this work, there is opportunity to extend DFI
with a system for reactive caching of wildcarded ﬂow rules,