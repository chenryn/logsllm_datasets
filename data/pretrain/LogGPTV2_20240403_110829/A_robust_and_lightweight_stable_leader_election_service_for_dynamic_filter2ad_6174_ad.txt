0
e
2
3
+
4
e
2
3
+
4
e
2
3
+
8
2
e
3
+
8
2
3
e
+
12
2
e
3
+
12
Number of workstations
Figure 6. CPU and bandwidth overhead
First note that in these extreme environments, S2 has a
better leader availability than S3 (see bottom graph).
In
fact, S2 is exceptionally robust: even in a network where ev-
ery link crashes and loses all messages for 3 seconds every
60 seconds, and each workstation crashes every 10 minutes,
S2 is able to provide the group with an operational leader
98.78 percent of the time. In this hostile environment, the
leader availability of S3 is “only” 77.42%. In slightly less
demanding settings, namely, when each links crashes about
every 5 minutes, the leader availability of S3 goes up to
97.66% (compared to 99.80% for S2). Note also that S2 has
a smaller average leader recovery time than S3 (see the up-
per graph): This is especially clear in the extreme scenario
where, on average, every link crashes for 3 seconds every
minute: in this case the leader recovery time of S2 is still
close to 1 second, while it grows to about 3 seconds with
S3. Finally, note that both S2 and S3 now have unjustiﬁed
demotions (middle graph). This, however, is unavoidable:
in an environment where links lose all messages for about
3 seconds, no FD can detect the crash of processes within 1
second (as we required here) without making mistakes.
6.6. QoS of the FD vs. QoS of S2 and S3
We also run some experiments to investigate how the
QoS of the underlying FD affects the QoS of the leader
election services S2 and S3. In these experiments, shown
in Figure 8, we run S2 and S3 on 12 workstations, each of
which crashes and later recovers once every 10 minutes on
average, and the message delays and losses are those of our
local area network (0.025ms and pL ≈ 0). We vary the up-
per bound T U
D on the crash detection time of the underlying
FD from 0.1 second to 1 second (the other QoS parameters
of the FD are set to the default values given in Section 6.1)
)
s
d
n
o
c
e
s
(
r
T
4
3
2
1
0
500
400
300
200
)
r
u
o
h
/
s
e
k
a
t
s
i
m
(
u
λ
100
0
1.0
0.95
0.9
0.85
0.8
0.75
r
e
d
a
e
l
P
+
×
+
×
+
×
(600s, 3s)
(300s, 3s)
(60s, 3s)
×
+
(60s, 3s)
×
+
(60s, 3s)
×
+
(600s, 3s)
×
+
×
+
(300s, 3s)
×
+
(600s, 3s)
(300s, 3s)
Crash-prone links (avrg. uptime, avrg. downtime)
Figure 7. S2 and S3 with crash-prone links
and see how this affects the QoS of S2 and S3.
As Figure 8 shows, T U
D has a direct inﬂuence on both
the average leader recovery time Tr and the leader avail-
ability Pleader of both S2 and S3: Roughly speaking, (a) Tr
D , and (b) decreasing T U
remains just a bit smaller than T U
D
by some amount improves both Tr and Pleader by a propor-
tional amount. This indicates that detection time is a large
component of the leader recovery time, and that one can
D .6
effectively control both Tr and Pleader by setting T U
7. Summary and concluding remarks
We evaluated our leader election service under a wide
variety of settings to encompass a large range of possible
applications and networks. In particular, we considered net-
works where the probability of message loss ranges from
(practically) 0 to 1/10 and the average message delay ranges
from 0.025ms to 100ms, as well as networks where commu-
nication links may completely disconnect for some periods
of time.
6It should be noted that while decreasing T U
D improves both Tr and
Pleader, this also increases the cost of running the service. But even if we
decrease the failure detection time to a very small value the cost of running
S3 remains low: with T U
D = 0.1 second, S3 took only 0.1% of the CPU
and generated 12.6 KB/s of trafﬁc per workstation; S2 took 1.23 % of the
CPU and generated 135.17 KB/s of trafﬁc per workstation.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE215DSN 2008: Schiper & Toueg×
+
×
+
tor or leader election algorithms with future state-of-the-art
ones.
References
×
+
×
+
0.25
0.5
0.75
1
×
+
+
×
×
+
0.1
0.25
0.5
0.75
+
×
1
S2 × S3 +
×
+
0.1
×
+
)
s
d
n
o
c
e
s
(
r
T
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.9997
r
e
d
a
e
l
P
0.9994
0.9991
0.9988
0.9985
Figure 8. Effect of T U
T U
D (seconds)
D on the QoS of S2 and S3
Overall, we found that two versions of the service,
namely, S2 and S3, behave remarkably well in extremely
unfavorable conditions, i.e., in networks with very high pro-
cessor failures and very poor communication links. We be-
lieve that this robustness is due to the combination of leader
election algorithms that were proven to work under weak
systems assumptions [2, 4], with an underlying failure de-
tector algorithm that provides some QoS control [5].
In future work, we will explore the expansion of the
leader election service to very large networks. We ﬁrst note
that one of our services, namely S3, seems to be inherently
scalable: it is based on a message-efﬁcient leader election
algorithm, and, as Figure 6 indicates, its CPU and network
bandwidth overhead grows quite slowly with the size of the
network. But it may still be too expensive to use S3 directly
to elect a leader among a very large number of processes.
There are at least two possible (orthogonal) approaches to
do this while keeping the costs down. One is to run the
election only among a relatively small number of candidates
(the election results are then propagated to the rest of the ap-
plication processes, who remain passive listeners during the
election). The other is to arrange for hierarchical elections.
We believe that the current versions of the service can
support both approaches. First, the service already allows
each application process in every group g to declare whether
it is a “candidate for leadership” in g, and the service elects
a leader only among the current candidates in g. Second,
the groups semantics can be used to elect a leader at each
level of the election hierarchy by mapping “groups” to lev-
els (group of local leaders, group of regional leaders, etc...).
Finally, it is worth noting that because the architecture
of the leader election service is modular, the service can
be easily “upgraded” by replacing the current failure detec-
[1] M. K. Aguilera, C. Delporte-Gallet, H. Fauconnier, and
S. Toueg. Stable leader election. In proceedings of DISC’01,
pages 108–122. Springer-Verlag, 2001.
[2] M. K. Aguilera, C. Delporte-Gallet, H. Fauconnier, and
S. Toueg. On implementing Omega with weak reliability
and synchrony assumptions. In proceedings of PODC’03,
pages 306–314. ACM Press, 2003.
[3] M. K. Aguilera, C. Delporte-Gallet, H. Fauconnier, and
S. Toueg. Communication-efﬁcient leader election and
In proceedings of
consensus with limited link synchrony.
PODC’04, pages 328–337. ACM Press, 2004.
[4] M. K. Aguilera, C. Delporte-Gallet, H. Fauconnier, and
S. Toueg. On implementing Omega with weak reliabil-
ity and synchrony assumptions. Technical Report HAL-
00259018, CNRS - France, November 2007.
[5] W. Chen, S. Toueg, and M. K. Aguilera. On the quality of
service of failure detectors. IEEE Transactions on Comput-
ers, 51(5):561–580, May 2002.
[6] B. Deianov and S. Toueg. Failure detector service for de-
In proceedings of FTCS’00, pages
pendable computing.
B14–B15. IEEE computer society press, 2000.
[7] A. Fernandez, E. Jimenez, and M. Raynal. Eventual leader
election with weak assumptions on initial knowledge, com-
In proceedings of
munication reliability, and synchrony.
DSN’06, pages 166–178. IEEE Computer Society, 2006.
[8] C. Fetzer and F. Cristian. A highly available local leader
election service. IEEE Transactions on Software Engineer-
ing, 25(5):603–618, 1999.
[9] R. Guerraoui and P. Dutta. Fast indulgent consensus with
zero degradation. In proceedings of EDCC’02, pages 191–
208. Springer-Verlag, 2002.
[10] I. Gupta, R. van Renesse, and K. P. Birman. A proba-
bilistically correct leader election protocol for large groups.
In proceedings of DISC’00, pages 89–103. Springer-Verlag,
2000.
[11] D. Ivan and S. Toueg. An implementation of a shared failure
detector service with QoS. 2001. Private Communication.
[12] L. Lamport. Time, clocks, and the ordering of events
in a distributed system. Communications of the ACM,
21(7):558–565, July 1978.
[13] L. Lamport. The Part-Time parliament. ACM Transactions
on Computer Systems, 16(2):133–169, May 1998.
[14] M. Larrea, A. Fernandez, and S. Arevalo. Optimal imple-
mentation of the weakest failure detector for solving con-
sensus (brief announcement). In proceedings of PODC’00,
page 334. ACM Press, 2000.
[15] D. Malkhi, F. Oprea, and L. Zhou. Omega meets Paxos:
Leader election and stability without eventual timely links.
In proceedings of DISC ’05, pages 199–213. Springer, 2005.
[16] A. Most´efaoui and M. Raynal. Leader-based consensus.
Parallel Processing Letters, 11(1):95–107, 2001.
[17] R. D. Prisco, B. Lampson, and N. Lynch. Revisiting the
Paxos algorithm. Theoretical Computer Science, 243(1–
2):35–91, 2000.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE216DSN 2008: Schiper & Toueg