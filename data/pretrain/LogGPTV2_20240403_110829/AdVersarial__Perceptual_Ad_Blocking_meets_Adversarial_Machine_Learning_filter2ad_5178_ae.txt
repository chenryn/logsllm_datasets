tion into a small page footer (see Figure 8 (d)). The footer causes
false predictions for 19/20 pages in our evaluation set, and is robust
to a user scrolling over the page. Figure 14 (c) shows a stealth-
ier attack—tailored to bbc.com—that hides a honeypot in the page
header and has 100% success rate across pages from that publisher.
4.3 Attacks against Ad-Blocker Actions
Ad-blockers usually run at a higher privilege level than any web
page. They are generally not affected by the same-origin policy and
can read and write any part of any web page that the user visits.
The main privileged action taken by an ad-blocker is altering of
web content. Attackers exploit this action when using honeypots
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2014to detect ad-blockers. But triggering ad-blocker actions can have
more pernicious effects. Below, we describe two attacks that can
be deployed by arbitrary content creators (e.g., a Facebook user) to
trigger malicious ad-blocker actions in other users’ browsers.
Attack A1: Cross-boundary blocking. In this attack (see Figure 1)
a malicious user (Jerry) uploads adversarial content that triggers
a Sentinel-like ad-blocker into marking content of another user
(Tom) as and ad. This “cross-boundary blocking attack” hijacks the
ad-blocker’s elevated privilege to bypass web security boundaries.
To mount the attack, we optimally perturb Jerry’s content so
as to maximize the model’s confidence in a box that covers Tom’s
content. The attack works because object-detector models such
as YOLOv3 [72] predict bounding boxes by taking into account
the full input image—a design feature which increases accuracy
and speed [70]. As a result, adversarial content can affect bounding
boxes in arbitrary image regions. Our attack reveals an inherent vul-
nerability of any object detector applied to web content—wherein
the model’s segmentation misaligns with web-security boundaries.
Attack A2: Cross-origin web requests. In addition to searching for
the “Sponsored” text on Facebook, Ad-Highlighter [82] uses the
fact that the ad-disclosure contains a link to Facebook’s ad-policy
page as an additional signal. Specifically, Ad-Highlighter parses
the DOM in search for links containing the text “Sponsored” and
determines whether the link leads to Facebook’s ad statement page
by simulating a user-click on the link and following any redirects. 5
These techniques are dangerous and enable serious vulnerabil-
ities (e.g., CSRF [66], DDoS [67] or click-fraud [22]) with conse-
quences extending beyond ad-blocking. Clicking links on a user’s
behalf is a highly privileged action, which can thus be exploited by
any party that can add links in a page, which can include arbitrary
website users. To illustrate the dangers of behavioral ad-blocking,
we create a regular Facebook post with an URL to a web page with
title “Sponsored”. Facebook converts this URL into a link which
Ad-Highlighter clicks on. Albeit sound, this attack luckily and co-
incidentally fails due to Facebook’s Link Shim, that inspects clicked
links before redirecting the user. Ad-Highlighter fails to follow this
particular redirection thus inadvertently preventing the attack. Yet,
this also means that Facebook could use the same layer of indirec-
tion for their “Sponsored” link. If the behavioral ad-blocking idea
were to be extended to disclosure cues on other websites (e.g., the
AdChoices logo), such attacks would also be easily mounted. Pre-
filtering inputs passed to a behavioral layer does not help. Either the
filter is perfect, in which case no extra step is required—or its false
positives can be exploited to trigger the behavioral component.
4.4 Attacks against Page Segmentation
In this Section, we describe attacks targeting the ad-blocker’s page
segmentation logic, in an effort to evade the ad-blocker or ex-
haust its resources. These attacks use standard Web techniques
(e.g., HTML obfuscation) and are already applied in an ongoing
arms race between Facebook and uBlock [88]. We argue that to
5Ad-Highlighter simulates clicks because Facebook used to resolve links server-side
(the ad-disclosure used to link to www.facebook.com/#). Facebook recently changed its
obfuscation of the link in post captions. It now uses an empty  tag that is populated
using JavaScript during the click event. This change fools Ad-Highlighter and still
requires an ad-blocker to simulate a potentially dangerous click to uncover the link.
Sp 
S 
on 
S 
so 
S 
red 
S 
.c2 { font-size : 0; }
Figure 9: CSS Obfuscation on Facebook. (Left) HTML and
CSS that render Facebook’s “Sponsored” caption. (Right) A
proof-of-concept where the ad-disclosure is an adversarial
image that Ad-Highlighter’s OCR decodes as “8parisared”.
Figure 10: Image Sprites of the AdChoices Logo. Image-
sprites are sets of images stored in a single file, and seg-
mented using CSS rules. For example, the left sprite allows
to smoothly switch from the icon to the full logo on hover.
The right sprite is used by cnn.com to load a variety of logos
used on the page in a single request.
escape the arms race caused by these segmentation attacks, percep-
tual ad-blockers have to operate over rendered web-content (i.e.,
frame or page-based approaches), which in turn increases the at-
tack surface for adversarial examples on the ad-blocker’s visual
classifier.
Attack S1: DOM obfuscation. These attacks aim to fool the ad-
blocker into feeding ambiguous inputs to its classifier. They ex-
ploit some of the same limitations that affect traditional filter lists,
and can also be applied to element-based ad-blockers that rely on
computer-vision classifiers, such as Ad-Highlighter.
DOM obfuscation is exemplified by Facebook’s continuous ef-
forts to regularly alter the HTML code of its “Sponsored” caption
(see Figure 9). Facebook deploys a variety of CSS tricks to obfuscate
the caption, and simultaneously embeds hidden ad-disclosure hon-
eypots within regular user posts in an effort to deliberately cause
site-breakage for ad-block users. Facebook’s obfuscation attempts
routinely fool uBlock [88] as well as Ad-Highlighter.
If ad-blockers adopt computer-vision techniques as in Ad-
Highlighter, DOM obfuscation attacks still apply if ad-blockers
assume a direct correspondence between elements in the DOM
and their visual representation when rendered. For example, Ad-
Highlighter assumes that all img tags in the DOM are shown as is,
thereby ignoring potentially complex CSS transformations applied
when rendering HTML. This can cause the downstream classifier
to process images with unexpected properties.
Ad networks already use CSS rules that significantly alter ren-
dered ad-disclosures. Figure 10 shows two AdChoices logos found
on cnn.com. These are image-sprites—multiple images included in
a single file to minimize HTTP requests—that are cropped using
CSS to display only a single logo at a time. Image-sprites highlight
an exploitable blind-spot in element-based perceptual ad-blockers—
e.g., the logos in Figure 10 fool Ad-Highlighter [82]. Images can
also be fragmented into multiple elements. The ad-blocker then
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2015has to stitch them together to correctly recognize the image (e.g.,
Google’s AdChoices logo consists of two separate SVG tags).
Finally, the rules used by ad-blockers to link ad-disclosures back
to the corresponding ad frame can also be targeted. For example,
on pages with an integrated ad network, such as Facebook, the
publisher could place ad-disclosures (i.e., “Sponsored” links) and
ads at arbitrary places in the DOM and re-position them using CSS.
Frame-based and page-based ad-blockers bypass all these issues
by operating on already-rendered content.
Attack S2: Over-segmentation. Here the publisher injects a large
number of elements into the DOM (say, by generating dummy im-
ages in JavaScript) to overwhelm an ad-blocker’s classifier with
inputs and exhaust its resources. In response, ad-blockers would
have to aggressively filter DOM elements—with the risk of these
filters’ blind spots being exploited to evade or detect ad-blocking.
The viability of this attack may seem unclear, as users might blame
publishers for high page-load latency resulting from an overloaded
ad-blocker. Yet, Facebook’s efforts to cause site-breakage by embed-
ding ad-disclosure honeypots within all regular user posts demon-
strates that some ad networks may result to such tactics.
4.5 Attacks against Training
For classifiers that are trained on labeled images, the data collection
and training phase can be vulnerable to data poisoning attacks (D1)—
especially when crowdsourced as with Sentinel [10]. We describe
these attacks for completeness, but refrain from a detailed evalua-
tion as the test-time attacks described in Sections 4.2 through 4.4
are conceptually more interesting and more broadly applicable.
In these attacks, the adversary joins the crowdsourced data col-
lection to submit maliciously crafted images that adversely influ-
ence the training process. For example, malicious training data can
contain visual backdoors [20], which are later used to evade the
ad-blocker. The ad-blocker developer cannot tell if a client is con-
tributing real data for training or malicious samples. Similar attacks
against crowdsourced filter lists such as Easylist are theoretically
possible. A malicious user could propose changes to filter lists that
degrade their utility. However, new filters are easily interpreted and
vetted before inclusion—a property not shared by visual classifiers.
Sentinel’s crowdsourced data collection of users’ Facebook feeds
also raises serious privacy concerns, as a deployed model might
leak parts of its training data [30, 77].
5 DISCUSSION
We have presented multiple attacks to evade, detect and abuse
recently proposed and deployed perceptual ad-blockers. We now
provide an in-depth analysis of our results.
5.1 A New Arms Race
Our results indicate that perceptual ad-blocking will either perpetu-
ate the arms race of filter lists, or replace it with an arms race around
adversarial examples. Where perceptual ad-blockers that rely heav-
ily on page markup (e.g., as in uBlock [7] or Ad-Highlighter [82])
remain vulnerable to continuous markup obfuscation [88], visual
classification of rendered web content (as in Sentinel [10] or Perci-
val [84]) inherits a crucial weakness of current visual classifiers—
adversarial examples [33, 83].
The past years have seen considerable work towards mitigat-
ing the threat of adversarial examples. Yet, defenses are either
broken by improved attacks [11, 17], or limited to restricted adver-
saries [21, 45, 53, 69, 86]. Even if ad-block developers proactively
detect adversarial perturbations and blacklist them (e.g., using ad-
versarial training [53, 83] to fine-tune their classifier), adversaries
can simply regenerate new attacks (or use slightly different pertur-
bations [76]).
5.2 Strategic Advantage of Adversaries and
Lack of Defenses
Our attacks with adversarial examples are not a quid pro quo step in
this new arms race, but indicate a pessimistic outcome for percep-
tual ad-blocking. Indeed, these ad-blockers operate in essentially
the worst threat model for visual classifiers. Their adversaries have
access to the ad-blockers’ code and prepare offline digital adversar-
ial examples to trigger both false-negatives and false-positives in
the ad-blocker’s online (and time constrained) decision making.
Even if ad-blockers obfuscate their code, black-box attacks [41]
or model stealing [63, 87] still apply. Randomizing predictions or de-
ploying multiple classifiers is also ineffective [11, 37]. For example,
some of the adversarial examples in Figure 5 work for both OCR
and SIFT despite being targeted at a single one of these classifiers.
The severity of the above threat model is apparent when consid-
ering existing defenses to adversarial examples. For instance, adver-
sarial training [53, 83] assumes restricted adversaries (e.g., limited
to ℓ∞ perturbations), and breaks under other attacks [27, 76, 85]. Ro-
bustness to adversarial false positives (or “garbage examples” [33])
is even harder. Even if ad-blockers proactively re-train on adversar-
ial examples deployed by publishers and ad-networks, training has
a much higher cost than the attack generation and is unlikely to
generalize well to new perturbations [74]. Detecting adversarial ex-
amples [34, 55] (also an unsolved problem [16]) is insufficient as Ad-
blockers face both adversarial false-positives and false-negatives,
so merely detecting an attack does not aid in decision-making. A
few recently proposed defenses achieve promising results in some
restricted threat models, e.g., black-box attacks [19] or physically-
realizable attacks [21]. These defenses are currently inapplicable in
the threat model of perceptual ad-blocking, but might ultimately
reveal new insights for building more robust models.
Our attacks also apply if perceptual ad-blocking is used as a
complement to filter lists rather than as a standalone approach.
Ad-blockers that combine both types of techniques are vulnerable
to attacks targeting either. If perceptual ad-blocking is only used
passively (e.g., to aid in the maintenance of filter lists, by logging
potential ads that filter lists miss), the ad-blocker’s adversaries still
have incentive to attack to delay the detection of new ads.
This stringent threat model above also applies to ML-based ad-
blockers that use URL and DOM features [14, 36, 43], which have
not been evaluated against adaptive white-box attacks.
5.3 Beyond the Web and Vision.
The use of sensory signals for ad-blocking has been considered
outside the Web, e.g., AdblockRadio detects ads in radio streams
using neural networks [2]. Emerging technologies such as virtual
reality [62], voice assistants [44] and smart TVs [59] are posited to
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2016Figure 11: Original and Adversarial Audio Waveforms.
Shows a ten second segment of an ad audio waveform (thick
blue) overlaid with its adversarial perturbation (thin red).
become platforms for large-scale targeted advertising, and percep-
tual ad-blockers might emerge in those domains as well.
The threats described in this paper—and adversarial examples in
particular—are likely to also affect perceptual ad-blockers that oper-
ate outside the vision domain. To illustrate, we take a closer look at
AdblockRadio, a radio client that continuously classifies short audio
segments as speech, music or ads based on spectral characteristics.
When ads are detected, the radio lowers the volume or switches
stations. Radio ad-blockers face a different threat model than on
the Web. All content, including ads, is served as raw audio from a
single origin, so filter lists are useless. The publisher cannot run any
client-side code, so ad-block detection is also impossible. Yet, the
threat of adversarial examples does apply. Indeed, we show that by
adding near-inaudible6 noise to the ad content in AdblockRadio’s
demo podcast, the perturbed audio stream evades ad detection.
Concretely, AdblockRadio takes as input a raw audio stream,
computes the Mel-frequency cepstral coefficients (MFCCs), and
splits them into non-overlapping windows of 4 seconds. Each seg-
ment is fed into a standard feed-forward classifier that predicts
whether the segment corresponds to music, speech, or an ad. A
post-processing phase merges all consecutive segments of a same
class, and removes ad-segments. As the whole prediction pipeline
is differentiable, crafting adversarial examples is straightforward:
we use projected gradient descent (in the l∞-norm) to modify the
raw ad audio segments so as to minimize the classifier’s confidence
in the ad class. The resulting audio stream fully bypasses Adblock-
Radio’s ad detection. An ad segment in the original and adversarial
audio waveforms is displayed in Figure 11.
6 RELATED WORK
Our work bridges two areas of computer security research—studies
of the online ad-ecosystem and associated ad-blocking arms race,
and adversarial examples for ML models.
Behavioral advertising. A 2015 study found that 22% of web users
use ad-blockers, mainly due to intrusive behavior [46, 68, 78, 89].
The use of ad-disclosures—which some perceptual ad-blockers rely
on—is rising. On the Alexa top 500, the fraction of ads with an
AdChoices logo has grown from 10% to 60% in five years [38, 81].
Yet, less than 27% of users understand the logo’s meaning [50, 89].
Ad-blocking. Limitations of filter lists are well-studied [54, 91,
92]. Many new ad-blocker designs (e.g., [14, 36, 43]) replace hard-
coded rules with ML models trained on similar features (e.g.,
6The perturbed audio stream has a signal-to-noise ratio of 37 dB.
markup [23] or URLs [47]). Many of these works limit their secu-
rity analysis to non-adaptive attacks. Ours is the first to rigorously
evaluate ML-based ad-blockers.
Ad-block detection has spawned an arms race around anti-ad-
blocking scripts [57, 58, 60]. Iqbal et al. [42] and Zhu et al. [95] detect
anti-ad-blocking using code analysis and differential-testing. Storey
et al. [81] build stealthy ad-blockers that aim to hide from client-side
scripts, a challenging task in current browsers (see Appendix A).
Adversarial examples. Our work is the first to apply adversarial
examples in a real-world web-security context. Prior work attacked
image classifiers [17, 33, 64, 83], malware [35], speech recogni-