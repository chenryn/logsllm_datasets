egories have obfuscations which appear to be mounting parser
confusion attacks, indicating that attackers are already aware
of several parser weaknesses and are actively exploiting them
in an attempt to evade detection. The jsunpack-n extractor is
the most effective in processing these samples, and appears to
have been coded to speciﬁcally address many of the evasions.
TABLE III: JavaScript Extractions (Version 9.5.0 vs 11.0.08)
trailer >
9.5.0 Only
21
11.0.08 Only
328
Common
4376
Total
4725
While these samples have been classiﬁed by many detectors
on VirusTotal as malicious, the weaknesses they exploit to
evade extraction are still effective against these detectors as
demonstrated in Section V, indicating that this classiﬁcation is
likely based on factors outside of JavaScript analysis. The fact
that JavaScript cannot be extracted from many benign samples
either indicates that these extractors are incomplete or parse
PDFs differently from Adobe Reader.
C. Tap Point Veriﬁcation
Our reference extractor does not produce JavaScript from
546 samples (only 10 of which meet our deﬁnition of mali-
cious) which at least one of the other extractors is able to. To
demonstrate that the reference extractor is only unable to ex-
tract JavaScript from PDFs which do not contain automatically
executing JavaScript or are malformed, and not because of a
failing of the technique or because of an incorrectly selected
tap point, we further perform a veriﬁcation. By opening these
samples with the original Adobe Reader binary in an execution
monitor, we can observe the program’s behavior to determine
if it processes any of the JavaScript produced by the other
extractors.
Each of these samples is opened by Adobe Reader and then
given thirty seconds to load the EScript.api module, knowing
that it contains the JavaScript engine. If the module is loaded
within this time, Adobe Reader is allowed to run for another
four minutes to collect a memory trace.
Of the 546 samples, only 274 actually loaded the ES-
cript.api module. For the remaining 274 samples, we group
memory operations from their traces into contiguous opera-
tions and then compare the data accessed by these operations
against the extractions produced by the other tools. Of all of the
contiguous operations produced in these samples, only 1006
unique matching strings were identiﬁed.
The vast majority of these strings are module or function
names from the Adobe JavaScript API or JavaScript keywords.
Some small JavaScript fragments were identiﬁed. Manual
analysis indicates that
these fragments are only substring
matches between JavaScript statements produced within the
module, possibly executed as part of the JavaScript engine’s
initialization. No complete JavaScript statement from any
extraction was identiﬁed in these contiguous operations, in-
dicating that the reference extractor has correctly captured all
of the automatically executed JavaScript in well-formed PDFs.
D. Lessons
Having identiﬁed samples which contained JavaScript but
which were not processed correctly by one of the extractors,
we set out to determine the causes of these failings. In many
cases, the cause of these failings was easily identiﬁed as an
incomplete parser implementation, as many of the tools are
aware of some of their limitations and will output messages
(a) Original Trailer
trailer >
(b) Trailer With Injected Comment
/XFA ‘[(config)42 0 R(template)%195 0 R
111 0 R(datasets)44 0 R(localeSet)45 0 R]’
(c) Abbreviated XFA Entry With Injected Comment
Fig. 3: Comment Injections
indicating that they have encountered an aspect of the spec-
iﬁcation they cannot handle. In other cases, the source code
of the extractor and the sample itself were manually analyzed
to determine the cause of the failing, which was usually the
result of a design error or an implementation bug
By examining the samples for which a single extractor
uniquely produced the correct output, we were able to identify
why the extractor was successful whereas the others were not.
Thus, while only performing this analysis on a relatively small
subset of these samples, we were able to identify several weak-
nesses in these extractors. Table IV outlines these limitations
and their impacts on the JavaScript extractors, which can be
generally broken down into four categories.
Implementation Bugs. Often an extractor will interpret the
speciﬁcation correctly but will have a programming error in
its implementation. While many of these bugs and errors are
quite easy to ﬁx, their enumeration is difﬁcult.
The PDF speciﬁcation states that comments, which begin
with a “%” and end with a newline sequence, shall be ignored
and treated as a single whitespace character. While this aspect
of the speciﬁcation is straightforward, neither the jsunpack-n or
Origami tools always parse comments correctly. The injection
of a comment into a PDF’s trailer, as seen in ﬁgures 3a and 3b,
causes the Origami tool to terminate prematurely and prevent
the extraction of JavaScript. Similarly, comments injected into
dictionaries thwart the jsunpack-n tool. An abbreviated XML
Forms Architecture (XFA) entry in a dictionary, shown in
ﬁgure 3c, demonstrates this bug. In this case, jsunpack-n does
not realize that the “%195 0 R” string should be ignored.
Instead of looking for the object with ID “111 0” which
contains the malicious payload, it in fact looks for an object
“195 0” which does not exist.
Stream data is to be terminated by an end-of-line marker
followed by the “endstream” keyword. However, the regular
expression jsunpack-n uses to identify the stream data matches
zero or more newline characters before “endstream”. This
means that trailing bytes in streams which happen to have
values associated with newlines will incorrectly be considered
whitespace instead of stream data.
The speciﬁcation allows for several different encryption
schemes and algorithms. Generating and applying the encryp-
tion keys in each of these algorithms is fairly complicated and
can depend on several other features of the document. The
9
TABLE IV: Failings and Limitations
Implementation Bugs
Design Errors
Omissions
Ambiguities
Comment in trailer
Comment in dictionary
Trailing whitespace in stream data
Security handler revision 5 hex encoded encryption data parsing
Security handler revision 3, 4 encryption key computation
Hexadecimal string literal in encoded objects
Use of orphaned encryption objects
Security handler revision 5 encryption key computation
without encrypted metadata
No XFA support
No security handler revision 5 support
No security handler revision 6 support
No cross-reference table and invalid object keywords
Affected Extractors
jsunpack-n
Origami
libpdfjs




































sets of speciﬁcations and algorithms governing how this is
performed are referred to as “security handlers” with several
revisions being developed as the speciﬁcation has evolved.
According to the PDF speciﬁcation, encryption algorithms can
be applied using a blank “default” password, which means
that even though certain contents of the ﬁle are stored in ci-
phertext, any parser which correctly implements the algorithm
can decrypt and examine them. Jsunpack-n appears to have
interpreted the encryption key generation algorithms correctly
for the revisions 3 and 4 security handlers; however, typos in
the function which computes them cause the extractor to crash
when they are used.
Hexadecimal string literals sometimes are not correctly
handled by jsunpack-n. When these string literals are placed
inside of encoded objects they are not properly parsed after
the object is decoded. Additionally, jsunpack-n fails to parse
hexadecimal strings that are used to store encryption data with
the revision 5 security handler.
Design Errors. These are instances where the extractor ap-
pears to have interpreted the speciﬁcation incorrectly or where
shortcuts have been intentionally taken to simplify develop-
ment. These errors are common in more complicated aspects
of the speciﬁcation, such as document encryption, which are
hard to interpret properly or in corner cases which break the
developer’s assumptions.
For example, jsunpack-n and Origami scan a document
for objects which deﬁne encryption parameters, and if found,
use them to decrypt all content which the speciﬁcation states
should be encrypted. However, the incremental update mech-
anism in the PDF speciﬁcation allows for the creation of an
updated ﬁle which is no longer encrypted. Such an update does
not remove old content, but produces a new document structure
which stops referencing older objects. Thus, the existence of
an object deﬁning encryption parameters does not necessarily
mean the current version of the document is encrypted. These
extractors will then incorrectly decrypt data which is already
in plain-text, producing “junk” data.
The revision 5 security handler has two slightly different
key generation algorithms depending on whether or not the
document’s metadata is encrypted. These algorithms are not
correctly interpreted by jsunpack-n. Thus, it can only produce
the correct key when this metadata is encrypted.
Omissions. None of the extractors evaluated claims to have
completely implemented all of the PDF speciﬁcation and
its extensions. By using these unimplemented aspects of the
speciﬁcation, it is trivial for attackers to hide malicious content
from the extractor.
The libpdfjs extractor has the most omissions largely due to
its dependence on an older version of the Poppler parser, which
does not implement newer additions to the speciﬁcation such as
the revision 4 and 5 security handlers. The Poppler parser does
also not support the XFA extension to the PDF speciﬁcation
which is often used to embed JavaScript
in PDFs. While
neither the Origami or jsunpack-n extractors fully support
XFA, they support enough of the speciﬁcation to identify and
extract JavaScript embedded in this way.
Only the Origami tool supports the revision 6 security
handler which is part of the PDF 2.0 speciﬁcation still under
development [14]. Even though this algorithm is not ofﬁcially
part of the PDF speciﬁcation, it is still used by Adobe products
and so any effective malicious PDF detector must do so also.
in an attempt
Ambiguities. The PDF speciﬁcation is vague in certain cases,
leaving space for multiple interpretations. Similarly Adobe
Reader,
to “just work”, will often process
PDFs deviating from the speciﬁcation. Since the speciﬁcation
does not cover these cases, it is unclear how they should be
handled. Finding these ambiguities is very difﬁcult, as well as
determining how they should be handled.
For example, the PDF speciﬁcation states that all PDF
documents are to include a “cross-reference” table or stream
which contains information about all objects in the ﬁle and
their locations. If a document does not have this table, Adobe
Reader and all of the extractors we evaluated will attempt to
reconstruct this table by scanning the document for objects,
which is usually successful when the objects in the document
are well-formed.
Another ambiguity, which is often seen in malicious PDFs,
is to use the malformed “objend” keyword to terminate objects.
The speciﬁcation states that objects should be terminated with
the “endobj” keyword, but Adobe Reader and all of the existing
extractors deviate from the speciﬁcation by accepting both.
When these two ambiguities are combined in a document
which contains objects terminated with the incorrect “objend”
keyword and no cross-reference table, Origami fails to identify
the objects and cannot parse the document.
10
3 0 obj
>
endobj
...
6 0 obj
>
stream
function heapSpray(str, str_addr,
r_addr) {
...}
endstream
endobj
(a) Malicious JavaScript and reference
are unobfuscated.
Action >>
/FlateDecode /N 4 /First 20 >>
endobj
...
6 0 obj
endstream
endobj
(c) Objects placed in streams, then en-
coded. Malicious JavaScript and refer-
ence are obfuscated.
>>
stream
endstream
endobj
(b) Malicious JavaScript is encoded, but
reference is unobfuscated.
Fig. 4: Stream Obfuscations
V. PARSER CONFUSION ATTACKS
A. Attack Deﬁnition
By systematically studying these weaknesses of the ex-
tractors and identifying their root causes, we are able to make
modiﬁcations, which we term obfuscations, to other PDF ﬁles
which exploit these weaknesses and prevent JavaScript extrac-
tion. Our understanding of the parser limitations, which caused
these weaknesses, also allowed us to develop new obfuscations
which also prevent extraction. Since Adobe Reader’s process-
ing of the ﬁle and its execution of the embedded JavaScript is
not affected, the application of these obfuscations prevents any
JavaScript based malicious PDF detection while not affecting
the efﬁcacy of the exploit. We call the application of these
obfuscations with the speciﬁc intent of allowing a malicious
PDF to evade detection, PDF parser confusion attacks.
While we only speciﬁcally analyzed the weaknesses of
the open source extractors which we evaluated, we were able
to identify several common weaknesses in these extractors
which were likely to be present in other PDF parsers. To