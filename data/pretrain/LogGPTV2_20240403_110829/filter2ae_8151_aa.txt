看到运维群里的小伙伴都在研究k8s，而且在国内这方面的安全资料非常的少，所以才有了我这篇文章。  
所以接触k8s以来也就一个星期时间，把大部分概念简单的理解下就去复现各种关于k8s安全相关的问题。
### Kubernetes架构
`Kubernetes Cluster`是Master的大脑，运行着的Daemon服务包括 **kube-apiserver** 、 **kube-scheduler** 、 **kube-controller-manager** 、 **etcd** 、 **Pod网络（flannel）**
#### Master组件
**API Server**  
提供 **HTTP/HTTPS RESTful API**
,是Cluster前端接口，各种客户端工具以及Kubernetes其他组件可以通过它管理Cluster的各种资源。  
它提供了对集群各种资源访问和控制的REST API,管理员可以通过kubectl或者第三方客户端生成HTTP请求，发送给API
Server。或者是集群内部的服务通过API Server对集群进行控制操作（比如dashborad）。  
集群内部的各种资源，如Pod、Node会定期给API Server发送自身的状态数据。而Master中的Controller
Manager和Scheduler也是通过API Server与etcd进行交互，将系统的状态存入etcd数据库或者从etcd中读出系统状态。
**Scheduler**  
Scheduler负责决定将Pod放在哪个Node上。会对各个节点的负载、性能、数据考虑最优的Node。
**Controller Manager**  
负责管理Cluster资源，保证资源处于预期状态。Controller Manager由多种controller组成： **replication
controller** 、 **endpoints controller** 、 **namespace controller** 、 **service
accounts controller** 等。  
不同的controller管理不同的资源。
**etcd**  
etcd保存Cluster配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速通知Kubernetes相关组件。
**Pod网络**  
Pod要能够相互通信，Cluster必须部署Pod网络，flannel是其中之一。
#### Node组件
**Node节点**  
Node运行组件有： **kubelet** 、 **kube-proxy** 、 **Pod网络（flannel）**
**kubelet**  
kubelet是Node的agent，Scheduler会将Pod配置信息发送给该节点的kubelet，kubelet根据这些信息创建和运行容器，并向Master报告运行状态。
**kube-proxy**  
每个Node运行kube-proxy服务，负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本会实现负载均衡。
**Pod网络**  
Pod要能够相互通信，Cluster必须部署Pod网络，flannel是其中之一。
Master也是可以运行应用，同时也是一个Node节点。
几乎所有Kubernetes组件运行在Pod里。  
`kubectl get pod --all-namespaces -o wide`
Kubernetes系统组件都被放到kube-system namespace中。  
kubelet是唯一没有以容器形式运行在Kubernetes组件中，它在System服务运行。
一个例子，当执行：  
`kubectl run https-app --image=httpd --replicas=2`
  1. kubectl 发送部署请示到API Server。
  2. API Server通知Controller Manager创建一个deployment资源。
  3. Scheduler执行调度任务，将两个副本Pod分发到node1､node2.
  4. node1和node2上的kubectl在各自的节点上创建并运行Pod。
**Katacoda提供了在线学习平台，可以不用安装k8s就可以操作。**  
* * *
### 使用Ansible脚本安装K8S集群
这里我使用3台机器进行安装
IP | 节点 | 服务  
---|---|---  
192.168.4.110 | master | Deploy,master,lb1,etcd  
192.168.4.114 | node1 | etcd,node  
192.168.4.108 | node2 | etcd,node  
在三台机器上的准备工作：
    yum install epel-replease
    yum update
    yum install python
#### Deploy节点安装和准备ansible
    yum install -y python-pip  git
    pip install pip --upgrade -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com
    pip install --no-cache-dir ansible -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com
生成ssh公私钥
    ssh-keygen
    复制到各机器上,包括本机
    ssh-copy-id 192.168.4.110
    ssh-copy-id 192.168.4.114
    ssh-copy-id 192.168.4.108
    #使用ssh连接测试
#### Deploy上编排k8s
    git clone https://github.com/gjmzj/kubeasz.git
    mkdir -p /etc/ansible
    mv kubeasz/* /etc/ansible/
从百度云网盘下载二进制文件 `https://pan.baidu.com/s/1c4RFaA#list/path=%2F`  
根据自己所需版本
    tar zxvf k8s.1-11-2.tar.gz
    mv bin/* /etc/ansible/bin/
#### 配置集群参数
`[root@master ~]# cd /etc/ansible/`  
`[root@master ansible]# cp example/hosts.m-masters.example hosts`
    # 集群部署节点：一般为运行ansible 脚本的节点
    [deploy]
    192.168.4.110 NTP_ENABLED=no
    # etcd集群请提供如下NODE_NAME，注意etcd集群必须是1,3,5,7...奇数个节点
    [etcd]
    192.168.4.110 NODE_NAME=etcd1
    192.168.4.114 NODE_NAME=etcd2
    192.168.4.108 NODE_NAME=etcd3
    [kube-master]
    192.168.4.110
    192.168.4.107
    [kube-node]
    192.168.4.114
    192.168.4.108
    # 负载均衡(目前已支持多于2节点，一般2节点就够了) 安装 haproxy+keepalived
    [lb]
    192.168.4.107 LB_IF="ens33" LB_ROLE=backup
    192.168.4.110 LB_IF="eno16777736" LB_ROLE=master
    # 集群 MASTER IP即 LB节点VIP地址，为区别与默认apiserver端口，设置VIP监听的服务端口8443
    # 公有云上请使用云负载均衡内网地址和监听端口
    MASTER_IP="192.168.4.110"
    KUBE_APISERVER="https://{{ MASTER_IP }}:8443"
    # 集群basic auth 使用的用户名和密码
    BASIC_AUTH_USER="admin"
    BASIC_AUTH_PASS="test1234"
修改完hosts文件通过`ansible all -m ping`测试
    192.168.4.108 | SUCCESS => {
        "changed": false,
        "ping": "pong"
    }
    192.168.4.110 | SUCCESS => {
        "changed": false,
        "ping": "pong"
    }
    192.168.4.114 | SUCCESS => {
        "changed": false,
        "ping": "pong"
    }
#### 分步安装
##### 01.创建证书和安装准备
    ansible-playbook 01.prepare.yml
##### 02.安装etcd集群
    ansible-playbook 02.etcd.yml
##### 03.安装docker
    ansible-playbook 03.docker.yml
##### 04.安装master节点
    ansible-playbook 04.kube-master.yml
`kubectl get componentstatus` //查看集群状态  
##### 05.安装node节点
    ansible-playbook 05.kube-node.yml
查看node节点  
`kubectl get nodes`
    NAME            STATUS    ROLES     AGE       VERSION
    192.168.4.108   Ready     node      8h        v1.11.6
    192.168.4.110   Ready     master    8h        v1.11.6
    192.168.4.114   Ready     node      8h        v1.11.6
##### 06.部署集群网络
    ansible-playbook 06.network.yml
查看`kube-system namespace`上的`pod`
    NAME                                    READY     STATUS    RESTARTS   AGE
    coredns-695f96dcd5-86r5q                1/1       Running   0          8h
    coredns-695f96dcd5-9q4fl                1/1       Running   0          3h
    kube-flannel-ds-amd64-87jj7             1/1       Running   1          8h
    kube-flannel-ds-amd64-9twqj             1/1       Running   2          8h
    kube-flannel-ds-amd64-b4xbm             1/1       Running   1          8h
    kubernetes-dashboard-68bf55748d-2bvmx   1/1       Running   0          8h
    metrics-server-75df6ff86f-tvp8t         1/1       Running   0          8h
##### 07.安装集群插件（dns、dashboard）
    ansible-playbook 07.cluster-addon.yml
查看`kube-system namespace`下的服务:
    NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
    kube-dns               ClusterIP   10.68.0.2               53/UDP,53/TCP,9153/TCP   8h
    kubernetes-dashboard   NodePort    10.68.122.176           443:32064/TCP            8h
    metrics-server         ClusterIP   10.68.248.178           443/TCP                  8h
查看node/pod使用资源情况:
    kubectl top node
    kubectl top pod --all-namespaces
##### 访问dashboard
查看集群信息:  
`kubectl cluster-info`  
登录密码就是我们前面安装设置的。
拿到登录token  
`kubectl -n kube-system describe secret $(kubectl -n kube-system get secret |
grep admin-user | awk '{print $1}')`  
* * *
### Kubernetes安全相关
Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。API
Server是集群内部各个组件通信的中介，也是外部控制的入口。所以Kubernetes的安全机制基本就是围绕保护API Server来设计的。
Kubernetes使用了认证（Authentication）、鉴权（Authorization）、准入控制（Admission
Control）三步来保证API Server的安全。
Kubelet 认证  
默认情况下，所有未被配置的其他身份验证方法拒绝的，对kubelet的HTTPS端点的请求将被视为匿名请求，并被授予system:anonymous用户名和system:unauthenticated组。
如果要禁用匿名访问并发送 401 Unauthorized 的未经身份验证的请求的响应：  
启动kubelet时指定 `--anonymous-auth=false`
对kubelet HTTPS端点启用X509客户端证书身份验证：
`--client-ca-file` 提供 CA bundle 以验证客户端证书  
启动apiserver时指定`--kubelet-client-certificate`和`--kubelet-client-key`标志
**Secret**
Kubernetes设计了一种资源对象叫做Secret，分为两类，一种是用于ServiceAccount的service-account-token  
另一种是用于保存用户自定义保密信息的Opaque。我们在ServiceAccount中用到包含三个部分：Token、ca.crt、namespace。
`token`是使用API Server私钥签名的JWT。用于访问API Server时，Server端认证。  
`ca.crt`，根证书。用于Client端验证API Server发送的证书。  
`namespace`, 标识这个service-account-token的作用域名空间。
    /opt/kube/bin/kubelet --address=192.168.4.114 --allow-privileged=true --anonymous-auth=false --authentication-token-webhook --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/ssl/ca.pem --cluster-dns=10.68.0.2 --cluster-domain=cluster.local. --cni-bin-dir=/opt/kube/bin --cni-conf-dir=/etc/cni/net.d --fail-swap-on=false --hairpin-mode hairpin-veth --hostname-override=192.168.4.114 --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --max-pods=110 --network-plugin=cni --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.1 --register-node=true --root-dir=/var/lib/kubelet --tls-cert-file=/etc/kubernetes/ssl/kubelet.pem --tls-private-key-file=/etc/kubernetes/ssl/kubelet-key.pem --v=2
更详细参考：
#### 通过kubelet攻击Kubernetes
通过kubelet默认配置对Kubernetes集群上的API Server发起特权访，特权访问有可能会获取集群中的敏感信息，也可能导致节点上机器命令执行。
API Server提供了对集群各种资源访问和控制的REST API。
在缺少对TLS身份验证，而在一些默认配置中启用了，`--anonymous-auth` 默认为`true`  
允许匿名身份访问API,端口为10250
/pods # 列出正在运行中的pod  
/exec # 在容器中运行命令并反回信息
这里我从shodan上随意找的IP进行测试  
json格式化一下:
    {
        "kind":"PodList",
        "apiVersion":"v1",
        "metadata":{
        },
        "items":[
            {
                "metadata":{
                    "name":"monitoring-influxdb-grafana-v4-6679c46745-snl2l",
                    "generateName":"monitoring-influxdb-grafana-v4-6679c46745-",
                    "namespace":"kube-system",
                    "selfLink":"/api/v1/namespaces/kube-system/pods/monitoring-influxdb-grafana-v4-6679c46745-snl2l",
                    "uid":"ccfb1a97-2795-11e9-8a06-00259050b024",
                    "resourceVersion":"303",
                    "creationTimestamp":"2019-02-03T09:26:35Z",
                    "labels":{
                        "k8s-app":"influxGrafana",
                        "pod-template-hash":"6679c46745",
                        "version":"v4"
                    },
                    "annotations":{
                        "kubernetes.io/config.seen":"2019-02-25T15:10:08.316930932Z",
                        "kubernetes.io/config.source":"api",
                        "scheduler.alpha.kubernetes.io/critical-pod":""
                    },
                    "ownerReferences":[
                        {
                            "apiVersion":"apps/v1",
                            "kind":"ReplicaSet",
                            "name":"monitoring-influxdb-grafana-v4-6679c46745",
                            "uid":"cc9879f6-2795-11e9-8a06-00259050b024",
                            "controller":true,
                            "blockOwnerDeletion":true
                        }
                    ]