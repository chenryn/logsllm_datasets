### 5 Nearest Reconstructed Images with Respect to MSE Generated by CBM-GAN

Our attack model aims to generate images that closely resemble the original images. For example, the five reconstructed images for the airplane image in Figure 8b all exhibit a blue background and a blurry version of the airplane. Similar results can be observed for the boat image in Figure 8a, the car image in Figure 8c, and the boat image in Figure 8d. Interestingly, CBM-GAN produces distinct samples for the two different horse images in Figure 8b. The blurriness in the results is expected due to the complexity of the CIFAR-10 dataset and the weak assumptions made about our adversary, who has access only to a black-box ML model.

### Quantitative Performance Measurement

We quantitatively measure the performance of our intermediate results by calculating the Mean Squared Error (MSE) between each image in the updating set and its nearest reconstructed sample, referred to as one-to-one matching. As shown in Figure 9, for the CIFAR-10, MNIST, and Insta-NY datasets, we achieve MSE values of 0.0283, 0.043, and 0.60, respectively. It is important to note that the adversary cannot perform one-to-one matching because they do not have access to the ground truth samples in the updating set; thus, one-to-one matching serves as an oracle.

### Full Attack Performance with Clustering

Figure 9 also shows the MSE of our full attack with clustering for all datasets. To match each reconstructed sample to a sample in \( D_{\text{update}} \), we use the Hungarian algorithm [24], ensuring that each reconstructed sample is matched with only one ground truth sample in \( D_{\text{update}} \) and vice versa. Our attack outperforms both baseline models on the CIFAR-10, MNIST, and Insta-NY datasets, with performance gains of 20%, 22%, and 25% over Shadow-clustering, and 60.1%, 5.5%, and 14% over Label-average, respectively. The varying performance gains over the label-average baseline across different datasets are due to the differing complexities of these datasets. For instance, MNIST images have a black background and lower variance within each class compared to the CIFAR-10 dataset, leading to a more representative label-average and lower performance gain for our attack.

### Visualization of Full Attack on MNIST

We further visualize the results of our full attack on the MNIST dataset. Figure 10 shows a sample of a full MNIST updating set reconstruction, where CBM-GAN reconstructs 100 original images. The attack model successfully reconstructs diverse digits of each class, which generally match the actual ground truth data well. This suggests that CBM-GAN can effectively capture most modes in the data distribution. Comparing the results of this attack (Figure 10) with those of the single-sample reconstruction attack (Figure 5), we observe that this attack produces sharper images, attributed to the discriminator in CBM-GAN, which ensures the output looks realistic.

### Limitations and Future Work

One limitation of our attack is that CBM-GAN's sample generation and clustering are performed separately. In future work, we plan to combine these steps into an end-to-end training process, which may further enhance our attack's performance. Our results demonstrate that our attack does not generate a general representation of data samples but rather tries to reconstruct images with similar characteristics to those in the updating set, as evidenced by the different shapes of the same numbers in Figure 10.

### Relaxing the Knowledge of Updating Set Cardinality

A key assumption of our attack is the adversary's knowledge of the updating set cardinality, \( |D_{\text{update}}| \). We relax this assumption by using updating sets of different cardinalities and employing the silhouette score to find the optimal \( k \) for K-means clustering. The silhouette score, ranging from -1 to 1, reflects the consistency of the clustering, with higher scores indicating better clustering. Specifically, the adversary updates the shadow model using updating sets of different cardinalities and uses the silhouette score to identify the most likely value of the target updating set's cardinality. Our evaluation shows that this method consistently produces a higher silhouette score, correctly identifying the updating set cardinality in all cases, with only a minor drop in MSE for the final output.

### Discussion

In this section, we analyze the impact of different hyperparameters on our attacks' performance and explore relaxing the threat model assumptions. We also discuss the limitations of our attacks.

#### Relaxing the Attacker Model Assumption

Our threat model assumes the same data distribution and structure for both target and shadow models. We relax these assumptions by proposing data transferability and model transferability attacks.

##### Data Transferability

We train and update the shadow model with a dataset from a different distribution than the target dataset. Table 1 shows the evaluation results, indicating that while the performance of our data transferability attacks drops, they still outperform the corresponding baseline models. For example, the multi-sample reconstruction attack's performance drops by 14% but remains 10% better than the baseline.

##### Model Transferability

We also relax the assumption of the target model's architecture by using different architectures for the shadow and target models. Our experiments show that the performance drop is less than 2% for all attacks, demonstrating robustness against changes in model architectures.

#### Effect of Probing Set Cardinality

We evaluate the performance of our attacks on CIFAR-10 with probing set cardinalities of 10, 100, 1,000, and 10,000. Using a probing set of size 10 reduces the performance, while increasing the cardinality from 100 to 1,000 or 10,000 has a limited effect. Using 100 samples for probing the target model is a suitable choice, balancing performance and computational requirements.

#### Effect of Target Model Hyperparameters

We evaluate the impact of the target model's training epochs before updating and the number of updating epochs. Experiments show that the difference in attack performance for different training epochs is minimal, and the performance improves with an increase in the number of updating epochs.

### Limitations

Our attacks assume a simplified setting where the target model is updated solely on new data, and we perform attacks on updating sets of up to 100 samples. Future work will investigate more complex settings with larger updating sets of both new and old data.

### Possible Defenses

#### Adding Noise to Posteriors

To reduce the performance of our attacks, one could add noise to the posteriors. However, experimental results show that while some attacks' performance drops, the multi-sample reconstruction attack remains stable, possibly due to the noise vector being part of CBM-GAN's input.

#### Differential Privacy

Differentially private learning can help reduce the memory of the training data, thereby reducing the performance of our attacks. However, the utility of the model may also drop significantly depending on the privacy budget.

### Related Works

Membership inference aims to determine whether a data sample is in a dataset. Shokri et al. [40] proposed the first membership inference attack against machine learning models, where the adversary's goal is to determine if a data sample is in the training set of a black-box ML model.