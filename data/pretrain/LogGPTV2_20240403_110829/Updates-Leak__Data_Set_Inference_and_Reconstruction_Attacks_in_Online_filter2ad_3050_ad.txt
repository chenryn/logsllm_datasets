5 nearest reconstructed images with respect to MSE gener-
ated by CBM-GAN. As we can see, our attack model tries to
generate images with similar characteristics to the original
images. For instance, the 5 reconstructed images for the air-
plane image in Figure 8b all show a blue background and a
USENIX Association
29th USENIX Security Symposium    1299
(a) MNIST
(b) CIFAR-10
(c) Insta-NY
Figure 9: [Lower is better] Performance of the multi-sample reconstruction attack (AMSR) together with one-to-one match and
the two baseline models. Mean squared error (MSE) is adopted as the evaluation metric. The match between the original and
reconstructed samples is performed by the Hungarian algorithm for both AMSR and Shadow-clustering. For Label-average, each
sample is matched within the average of samples with the same class in the shadow dataset. One-to-one match serves as an oracle
as the adversary cannot use it for her attack.
Figure 10: Visualization of a full MNIST updating set together with the output of the multi-sample reconstruction attack (AMSR)
after clustering. Samples are fair random draws, not cherry-picked. The left column shows the original samples and the right
column shows the reconstructed samples. The match between the original and reconstructed samples is performed by the
Hungarian algorithm.
blurry version of the airplane itself. The similar result can be
observed from the boat image in Figure 8a, the car image in
Figure 8c, and the boat image in Figure 8d. It is also interest-
ing to see that CBM-GAN provides different samples for the
two different horse images in Figure 8b. The blurriness in the
results is expected, due to the complex nature of the CIFAR-
10 dataset and the weak assumptions for our adversary, i.e.,
access to black-box ML model.
We also quantitatively measure the performance of our
intermediate results, by calculating the MSE between each
image in the updating set and its nearest reconstructed sample.
We refer to this as one-to-one match. Figure 9 shows for
the CIFAR-10, MNIST, and Insta-NY datasets, we achieve
0.0283, 0.043 and 0.60 MSE, respectively. It is important
to note that the adversary cannot perform one-to-one match
as she does not have access to ground truth samples in the
updating set, i.e., one-to-one match is an oracle.
Figure 9 shows the mean squared error of our full attack
with clustering for all datasets. To match each of our recon-
structed samples to a sample in Dupdate, we rely on the Hun-
garian algorithm [24]. This guarantees that each reconstructed
sample is only matched with one ground truth sample in
Dupdate and vice versa. As we can see, our attack outper-
forms both baseline models on the CIFAR-10, MNIST and
Insta-NY datasets (20%, 22%, and 25% performance gain for
Shadow-clustering and 60.1%, 5.5% and 14% performance
gain for Label-average, respectively). The different perfor-
mance gain of our attack over the label-average baseline for
different datasets is due to the different complexity of these
datasets. For instance, all images inside MNIST have black
background and lower variance within each class compared
to the CIFAR-10 dataset. The different complexity results
1300    29th USENIX Security Symposium
USENIX Association
0.000.010.020.030.040.050.060.070.08Meansquarederror(MSE)One-to-onematchAMSRShadow-clusteringLabel-average0.000.010.020.030.040.050.060.07Meansquarederror(MSE)One-to-onematchAMSRShadow-clusteringLabel-average0.00.10.20.30.40.50.60.70.80.9Meansquarederror(MSE)One-to-onematchAMSRShadow-clusteringLabel-averagein some datasets having a more representative label-average,
which leads to a lower performance gain of our attack over
them.
These results show that our multi-sample reconstruction
attack provides a more useful output than calculating the
average from the adversary’s dataset. In detail, our attack
achieves an MSE of 0.036 on the CIFAR-10 dataset, 0.051
on the MNIST dataset, and 0.64 on the Insta-NY dataset. As
expected, the MSE of our ﬁnal attack is higher than one-to-one
match, i.e., the above mentioned intermediate results.
We further visualize our full attack’s result on the MNIST
dataset. Figure 10 shows a sample of a full MNIST updat-
ing set reconstruction, i.e., the CBM-GAN’s reconstructed
images for the 100 original images in an updating set. We ob-
serve that our attack model reconstructs diverse digits of each
class that for most of the cases match the actual ground truth
data very well. This suggests CBM-GAN is able to capture
most modes in a data distribution well. Moreover, comparing
the results of this attack (Figure 10) with the results of the
single-sample reconstruction attack (Figure 5), we can see
that this attack produces sharper images. This result is due to
the discriminator of our CBM-GAN, as it is responsible for
making the CBM-GAN’s output to look real, i.e., sharper in
this case.
One limitation of our attack is that CBM-GAN’s sample
generation and clustering are performed separately. In the
future, we plan to combine them to perform an end-to-end
training which may further boost our attack’s performance.
From all these results, we show that our attack does not gen-
erate a general representation of data samples afﬁliated with
the same label, but tries to reconstruct images with similar
characteristics as the images inside the updating set (as shown
by the different shapes of the same numbers in Figure 10).
Relaxing The Knowledge of Updating Set Cardinality.
One of the above attack’s main assumptions is the adver-
sary’s knowledge of the updating set cardinality, i.e., |Dupdate|.
Next, we show how to relax this assumption. To recap, the
adversary needs the updating set cardinality when updating
her shadow model and clustering CBM-GAN’s output. We
address the former by using updating sets of different car-
dinalities. For the latter, we use the silhouette score to ﬁnd
the optimal k for K-means, i.e., the most likely value of the
target updating set’s cardinality. The silhouette score lies in
the range between -1 and 1, it reﬂects the consistency of the
clustering. Higher silhouette score leads to more suitable k.
Speciﬁcally, the adversary follows the previously presented
methodology in Section 5.2 with the following modiﬁcations.
First, instead of using updating sets with the same cardinality,
the adversary uses updating sets with different cardinalities to
update the shadow model. Second, after the adversary gener-
ates multiple samples from CBM-GAN, she uses the silhou-
ette score to ﬁnd the optimal k. The silhouette score is used
here to identify the target model’s updating set cardinality
from the different updating sets cardinalities used to update
Original
Transfer
Attack
ALI
ASSR
ALDE(10)
ALDE(100)
AMSR
0.97
0.68
0.64
0.59(0.0317)
0.89(0.0041)
0.55(0.0377)
0.89 (0.0067)
0.89
1.1
0.73
Table 1: Evaluation of the data transferability attacks. The ﬁrst
column shows all different attacks, the second and third shows
the performance of the attacks using similar and different dis-
tributions, respectively. Where ALI performance is measured
in accuracy, AMSR and ASSR measured in MSE, and ALDE(10)
and ALDE(100) measured in accuracy (KL-divergence).
the shadow model.
We evaluate the effectiveness of this attack on all datasets.
We use a target model updated with 100 samples and create
our shadow updated models using updating sets with cardi-
nality 10 and 100. Concretely, we update the shadow model
half of the time with updating sets of cardinality 10 and the
other half with cardinality 100.
Our evaluation shows that our attack consistently produces
higher silhouette score -by at least 20%- for the correct car-
dinality in all cases. In another way, our method can always
detect the right cardinality of the updating set in this setting.
Moreover, the MSE for the ﬁnal output of the attack only
drops by 1.6%, 0.8%, and 5.6% for the Insta-NY, MNIST, and
CIFAR-10 datasets, respectively.
6 Discussion
In this section, we analyze the effect of different hyperparam-
eters of both the target and shadow models on our attacks’
performance. Furthermore, we investigate relaxing the threat
model assumptions and discuss the limitations of our attacks.
Relaxing The Attacker Model Assumption. Our threat
model has two main assumptions: Same data distribution for
both target and shadow datasets and same structure for both
target and shadow models. We relax the former by proposing
data transferability attack and latter by model transferability
attack.
Data Transferability. In this setting, we locally train and up-
date the shadow model with a dataset which comes from a
different distribution from the target dataset. For our experi-
ments, we use Insta-NY as the target dataset and Insta-LA as
the shadow dataset.
Table 1 depicts the evaluation results. As expected, the
performance of our data transferability attacks drops; however,
they are still signiﬁcantly better than corresponding baseline
models. For instance, the performance of the multi-sample
reconstruction attack drops by 14% but is still 10% better than
the baseline (see Figure 9). Moreover, the multi-sample label
USENIX Association
29th USENIX Security Symposium    1301
distribution attack’s accuracy (KL-divergence) only drops by
6.8% (18.9%) and 0% (63%), which is still signiﬁcantly better
than the baseline (see Figure 6) by 6.5x (2x) and 4.6x (4.8x)
for updating set sizes of 10 and 100, respectively.
Model Transferablity. Now we relax the attacker’s knowledge
on the target model’s architecture, i.e., we use different archi-
tectures for shadow and target models. In our experiments on
Insta-NY, we use the same architecture mentioned previously
in Section 4.1 for the target model, and remove one hidden
layer and use half of the number of neurons in other hidden
layers for the shadow model.
The performance drop of our model transferability attack
is only less than 2% for all of our attacks, which shows that
our attacks are robust against such changes in the model ar-
chitectures. We observe similar results when repeating the
experiment using different architectures and omit them for
space restrictions.
Effect of The Probing Set Cardinality. We evaluate the per-
formance of our attacks on CIFAR-10 when the probing set
cardinality is 10, 100, 1,000, or 10,000. As our encoder’s in-
put size relies on the probing set cardinality (see Section 3),
we adjust its input layer size accordingly.
As expected, using a probing set of size 10 reduces the
performance of the attacks. For instance, the single-sample
label inference and reconstruction attacks’ performance drops
by 9% and 71%, respectively. However, increasing the probing
set cardinality from 100 to 1,000 or 10,000 has a limited effect
(up to 3.5% performance gain). It is also important to mention
that the computational requirement for our attacks increases
with an increasing probing set cardinality, as the cardinality
decides the size of the input layer for our attack models. In
conclusion, using 100 samples for probing the target model is
a suitable choice.
Effect of Target Model Hyperparameters. We now evalu-
ate our attacks’ performance with respect to two hyperparam-
eters of the target model.
Target Model’s Training Epochs Before Updating. We use the
MNIST dataset to evaluate the multi-sample label distribution
estimation attack’s performance on target models trained for
10, 20, and 50 epochs. For each setting, we update the model
and execute our attack as mentioned in Section 5.1.
The experiments show that the difference in the attack’s
performance for the different models is less than 2%. That
is expected as gradients are not monotonically decreasing
during the training procedure. In other words, information is
not necessarily vanishing [15].
Target Model’s Updating Epochs. We train target and shadow
models as introduced in Section 5.1 with the Insta-NY dataset,
but we update the models using different number of epochs.
More concretely, we update the models using from 2 to 10
epochs and evaluate the multi-sample label distribution esti-
mation attack’s performance on the updated models.
Figure 11: [Lower is better] The performance of the multi-
sample label distribution estimation attack (ALDE) with dif-
ferent number of epochs used to update the target model.
We report the results of our experiments in Figure 11. As
expected, the multi-sample label distribution estimation at-
tack’s performance improves with the increase of the number
of epochs used to update the model. For instance, the attack
performance improves by 25.4 % when increasing the number
of epochs used to update the model from 2 to 10.
Limitations of Our Attacks. For all of our attacks, we as-
sume a simpliﬁed setting, in which, the target model is solely
updated on new data. Moreover, we perform our attacks on
updating sets of maximum cardinality of 100. In future work,
we plan to further investigate a more complex setting, where
the target model is updated using larger updating sets of both
new and old data.
7 Possible Defenses
Adding Noise to Posteriors. All our attacks leverage poste-
rior difference as the input. Therefore, to reduce our attacks’
performance, one could sanitize posterior difference. How-
ever, the model owner cannot directly manipulate the posterior
difference, as she does not know with what or when the ad-
versary probes her model. Therefore, she has to add noise
to the posterior for each queried sample independently. We
have tried adding noise sampled from a uniform distribution
to the posteriors. Experimental results show that the perfor-
mance for some of our attacks indeed drops to a certain degree.
For instance, the single-sample label inference attack on the
CIFAR-10 dataset drops by 17% in accuracy. However, the
performance of our multi-sample reconstruction attack stays
stable. One reason might be the noise vector z is part of CBM-
GAN’s input which makes the attack model more robust to
the noisy input.
Differential Privacy. Another possible defense mechanism
against our attacks is differentially private learning. Differ-
ential privacy [10] can help an ML model learn its main
tasks while reducing its memory on the training data. If dif-
ferentially private learning schemes [4, 9, 39] are used when
updating the target ML model, this by design will reduce the
performance of our attacks. However, it is also important to
1302    29th USENIX Security Symposium
USENIX Association
246810Numberofepochs0.00260.00280.00300.00320.00340.00360.0038KL-divergencemention that depending on the privacy budget for differential
privacy, the utility of the model can drop signiﬁcantly.
We leave an in-depth exploration of effective defense mech-
anisms against our attacks as a future work.
8 Related Works
Membership Inference. Membership inference aims at de-
termining whether a data sample is inside a dataset. It has been
successfully performed in various settings, such as biomedical
data [18, 21] and location data [36, 37]. Shokri et al. [40] pro-
pose the ﬁrst membership inference attack against machine
learning models. In this attack, an adversary’s goal is to deter-
mine whether a data sample is in the training set of a black-
box ML model. To mount this attack, the adversary relies