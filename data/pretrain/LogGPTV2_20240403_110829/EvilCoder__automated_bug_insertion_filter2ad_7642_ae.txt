We now discuss limitations of our approach and current
prototype, possible future work, and alternative use cases of
automated bug insertion.
5.1 Exploitability
Our technique ﬁnds path between user-controlled sources
and sensitive sinks, and then modiﬁes or removes security
mechanisms on these paths, thus creating security-relevant
bugs. However, this may not be enough to actually create
an exploitable vulnerability, as we cannot assert the global
satisﬁability of all path conditions, which are necessary to
traverse the path in question.
One observation argues in favour of possible exploitation
despite all this: the security mechanisms are present in the
program. If no values existed for traversing the path in ques-
tion, the security mechanism would be superﬂuous in the
ﬁrst place. Note that this assumes that no overly defensive
programming strategy was used.
While research towards automatic generation of exploits [2,
4] or at least some proof of seriousness of the bug [19] ex-
ists, verifying all generated bugs with such complicated ad-
ditional components seemed unreasonable. As mentioned
in Section 4.4, we tried to justify our claim of exploitabil-
ity by letting our tool reintroduce known-to-be-exploitable
bugs. Nevertheless, automatic satisﬁability veriﬁcation and
exploit generation are out of scope for this paper.
Unfortunately, the lack of conﬁrmed vulnerabilities in com-
bination with the impossibility to count the number of in-
troduced bugs in a meaningful way means that it is not
possible to report something like a false positive rate for our
approach.
5.2 Additional Vulnerabilities
The bugs we introduce are limited in two ways: ﬁrst, we
only support a limited number of vulnerability types, which
all belong to the class of taint-style vulnerabilities, as they
allow rather conventional exploitation. Thus, we cannot in-
troduce other types of bugs for now. However, we believe
implementing additional bug classes to be straightforward.
Second, while we do add some element of randomness to
the introduction of bugs, they undoubtedly have a pattern.
For the use-case of an artiﬁcial bug corpus, this might ar-
guably be problematic, as it would be a very valid strategy
to model the heuristics we used to introduce the bugs to
ﬁnd them later on. However, given that we can automati-
222
cally introduce such bugs, it can also be argued that ﬁnding
these bugs, whether they have a pattern or not, is abso-
lutely mandatory. Hence, we create some kind of baseline
to evaluate techniques that aim to detect vulnerabilities in
an automated manner.
Furthermore, inserting additional vulnerable paths or func-
instead of only weakening present security mecha-
tions,
nisms, would also be interesting for future work.
5.3 Alternative Use Cases
We focused on the generation of test corpora in this paper,
but we do see other use cases for bug insertion. Capture-the-
ﬂag (CTF) contests essentially pose exploiting challenges,
for which vulnerable programs are a necessity. While our
approach does not guarantee exploitability and is not (yet)
targeted towards vulnerabilities which are tricky to exploit,
we think that it could be a valuable tool for the organizers.
Furthermore, the ability to insert exploitable bugs could
theoretically be used to facilitate later exploitation. How-
ever, in this scenario, the attacker would require write access
to the source code. While a recent publication [5] shows that
tampering with version control systems is feasible, it is a big
obstacle. Furthermore, the inserted vulnerability should be
hard to ﬁnd and exploitable for a long time, i. e., not be
removed soon. Given that we want to insert many vulnera-
bilities instead of a single, special one, we think that manual
eﬀort would be the way to go for an attacker, and thus do
not see an ethical problem with our approach.
6. RELATED WORK
The work most closest to our approach is a recently pub-
lished paper entitled LAVA: Large-scale Automated Vulner-
ability Addition [9]. The authors want to generate a suf-
ﬁcient number of bugs for purposes of testing bug-ﬁnding
tools.
In contrast to our approach, they chose a dynamic
method by tainting input bytes and tracing them through
the program. They look speciﬁcally for rarely modiﬁed and
dead data, for which they then insert code performing either
a buﬀer overread or buﬀer overﬂow. If necessary, they intro-
duce new static or global variables to allow the needed data
ﬂow. Additionally, they insert guards to execute the vul-
nerability only if a magic value occurs in the input. As the
name suggests, one could say that they actually add new
vulnerabilities, while we transform code from invulnerable
to vulnerable, i. e., insert bugs. We have chosen a diﬀerent
methodology, so that we deem this to be concurrent and
independent work.
Given the dynamic approach, LAVA also generates inputs
triggering the vulnerabilities and the authors also provide
preliminary results showing that state-of-the-art fuzzers and
symbolic execution engines are not able to ﬁnd all the bugs
they are able to add. We think that this ﬁnding underlines
the importance of automated bug insertion.
6.1
Insufﬁcient Test Data
Miller [26] uses a set of 16 hand-written vulnerabilities to
compare eight fuzzers and states that, while the scarcity of
test cases is a problem, these 16 artiﬁcial test cases already
oﬀered a lot of insight.
Nilson et al. [29] state that “existing sources of vulnerabil-
ity data did not supply the necessary structure or metadata
to evaluate them completely”, which is why they developed
BugBox, a simulation environment with an accompanying
corpus of vulnerabilities and exploits. Their vulnerabili-
ties are real-world examples speciﬁc to PHP, and they focus
mostly on the aspects of exploiting.
Delaitre et al. [7] evaluated 14 static analyzers. They
establish three critical characteristics for vulnerability test
cases and state that “Test cases with all three attributes are
out of reach”:
1. Statistical signiﬁcance: There must be many, di-
verse vulnerabilities.
2. Ground truth: The location of the vulnerabilities
must be known.
3. Relevance: The vulnerabilities must be representa-
tive for those found in real source code.
Test corpora generated with our approach fulﬁll the ﬁrst two
characteristics, and we are certain that the third character-
istic can be fulﬁlled with carefully stated bug models as well,
given that the instrumented code stems from real programs.
6.2 Vulnerability Databases
According to Nilson et al. [29] and Delaitre et al. [7],
the existing databases are not suﬃcient for a comprehen-
sive evaluation of bug ﬁnding techniques. However, that is
not to say that there are no such databases.
First of all, the Common Vulnerabilities and Exposures
(CVE) and the Common Weakness Enumeration (CWE)
are to mention. The former consists of a short description
of real-world vulnerabilities and links to further resources
regarding the speciﬁc vulnerability. Its main purpose is to
identify a certain vulnerability unambiguously, but it does
usually not include actual vulnerable code. The latter of-
ten oﬀers a few code snippets to illustrate the hierarchized
vulnerabilities, but those are likely not suﬃcient for a com-
prehensive evaluation of bug ﬁnding techniques.
Speciﬁc to web development, OWASP WebGoat [31] as
well as SecuriBench [20] collect vulnerabilities for illustrative
purposes. However, they do not oﬀer a structured corpus,
which is necessary for evaluation purposes.
The most useful public database for the evaluation of bug
ﬁnding techniques is generated by the NIST project Software
Assurance Metrics And Tool Evaluation (SAMATE) [30].
Its largest standalone test suite actually contains over 60,000
vulnerable synthetic test cases, but was uploaded in 2013.
Naturally, these test suites are static and cannot gener-
ate fresh bugs. The project also included the IARPA pro-
gram Securely Taking on Software of Uncertain Provenance
(STONESOUP) [14], which provides 164 Java and C snip-
pets, which can be inserted into other programs to make
them vulnerable. However, the snippets are static and re-
quire rather speciﬁc environments.
6.3 Mutation Testing
Mutation testing [16] randomly modiﬁes the source code
In that
to make it behave slightly diﬀerently at runtime.
way, both the ability of the test set to catch such errors as
well as the necessity and import of the modiﬁed piece of code
can be estimated. As a result, both coverage and overlap of
modiﬁed code and test set are the important metrics. While,
in principle, bugs like the ones introduced by our approach
could be inserted by mutation testing as well, we purpose-
fully insert special bugs at carefully selected locations to
introduce vulnerabilities.
223
7. CONCLUSIONS
In this paper, we proposed an approach for automatic
generation of bug-ridden test corpora. Our prototype im-
plementation of this concept currently targets the insertion
of spatial memory errors by modifying security checks using
six diﬀerent instrumentation classes. With such test cor-
pora, we aim to facilitate future research in the ﬁeld of bug
ﬁnding techniques, so that they can be evaluated and com-
pared in an objective and statistically meaningful way.
Acknowledgment
The project leading to this application has received funding
from the European Research Council (ERC) under the Eu-
ropean Union’s Horizon 2020 research and innovation pro-
gramme (grant agreement No. 640110 – BASTION). This
work was also supported by the German Research Founda-
tion (DFG) research training group UbiCrypt (GRK 1817).
The authors would like to thank Engin Kirda, William Robert-
son, Patrick Carter, Timothy Leek, Patrick Hulin, and Bren-
dan Dolan-Gavitt for the fruitful discussions. Furthermore,
we thank Jan Teske and Tilman Bender for supporting our
implementation eﬀorts.
8. REFERENCES
[1] Aleph One. Smashing the stack for fun and proﬁt.
Phrack, 7(49), November 1996.
[2] T. Avgerinos, S. K. Cha, B. L. T. Hao, and
D. Brumley. AEG: Automatic Exploit Generation. In
Symposium on Network and Distributed System
Security (NDSS), 2011.
[3] N. Borisov, R. Johnson, N. Sastry, and D. Wagner.
Fixing races for fun and proﬁt: How to abuse atime.
In USENIX Security Symposium, 2005.
random testing. In USENIX Windows Systems
Symposium (WSS), 2000.
[11] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G.
Fiscus, D. S. Pallett, and N. L. Dahlgren. DARPA
TIMIT acoustic phonetic continuous speech corpus
CDROM, 1993.
[12] P. Godefroid. Random testing for security: Blackbox
vs. whitebox fuzzing. In International Workshop on
Random Testing: Co-located with the IEEE/ACM
International Conference on Automated Software
Engineering (ASE), 2007.
[13] S. Horwitz. Precise ﬂow-insensitive may-alias analysis
is NP-hard. ACM Transactions on Programming
Languages and Systems (TOPLAS), 19(1), 1997.
[14] IARPA. Securely taking on software of uncertain
provenance (STONESOUP), 2015.
http://www.iarpa.gov/
index.php/research-programs/stonesoup.
[15] J. Jang, A. Agrawal, and D. Brumley. ReDeBug:
Finding Unpatched Code Clones in Entire OS
Distributions. In IEEE Symposium on Security and
Privacy, 2012.
[16] Y. Jia and M. Harman. An analysis and survey of the
development of mutation testing. IEEE Transactions
on Software Engineering, 37(5), 2011.
[17] C. Lattner, A. Lenharth, and V. Adve. Making
context-sensitive points-to analysis with heap cloning
practical for the real world. In ACM SIGPLAN
Conference on Programming Language Design and
Implementation (PLDI), 2007.
[18] L. Li, C. Cifuentes, and N. Keynes. Boosting the
performance of ﬂow-sensitive points-to analysis using
value ﬂow. In ACM SIGSOFT Symposium on the
Foundations of Software Engineering (FSE), 2011.
[4] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley.
[19] Z. Lin, X. Zhang, and D. Xu. Convicting exploitable
Unleashing Mayhem on Binary Code. In IEEE
Symposium on Security and Privacy, 2012.
[5] R. Curtmola, S. Torres-Arias, A. Ammula, and
J. Cappos. On omitting commits and committing
omissions: Preventing git metadata tampering that
(re)introduces software vulnerabilities. In USENIX
Security Symposium, 2016.
[6] D. Davidson, B. Moench, T. Ristenpart, and S. Jha.
Fie on ﬁrmware: Finding vulnerabilities in embedded
systems using symbolic execution. In USENIX
Security Symposium, 2013.
[7] A. Delaitre, B. Stivalet, E. Fong, and V. Okun.
Evaluating bug ﬁnders: Test and measurement of
static code analyzers. In First International Workshop
on Complex faUlts and Failures in LargE Software
Systems (COUFLESS), 2015.
software vulnerabilities: An eﬃcient input provenance
based approach. In International Conference on
Dependable Systems and Networks, 2008.
[20] B. Livshits. Deﬁning a set of common benchmarks for
web application security, 2005.
[21] B. Livshits and S. Chong. Towards fully automatic
placement of security sanitizers and declassiﬁers. In
ACM Symposium on Principles of Programming
Languages (POPL), 2013.
[22] V. B. Livshits and M. S. Lam. Finding Security
Vulnerabilities in Java Applications with Static
Analysis. In USENIX Security Symposium, 2005.
[23] M. E. Locasto and S. Bratus. Hacking the Abacus: An
Undergraduate Guide to Programming Weird
Machines. http://www.cs.dartmouth.edu/ sergey/
drafts/sismat-manual-locasto.pdf, 2014.
[8] W. Dietz, P. Li, J. Regehr, and V. Adve.
[24] Y. Lu, S. Yi, Z. Lei, and Y. Xinlei. Binary software
Understanding integer overﬂow in C/C++. In
International Conference on Software Engineering
(ICSE), 2012.
[9] B. Dolan-Gavitt, P. Hulin, E. Kirda, T. Leek,
A. Mambretti, W. Robertson, F. Ulrich, and
R. Whelan. LAVA: Large-scale Automated
Vulnerability Addition. In IEEE Symposium on
Security and Privacy, 2016.
vulnerability analysis based on bidirectional-slicing. In
Conference on Instrumentation, Measurement,
Computer, Communication and Control (IMCCC),
2012.
[25] B. P. Miller, L. Fredriksen, and B. So. An Empirical
Study of the Reliability of UNIX Utilities.
Commununications of ACM, 1990.
[26] C. Miller. Fuzz by number. CanSecWest Conference,
[10] J. E. Forrester and B. P. Miller. An empirical study of
2008.
the robustness of Windows NT applications using
224
[27] L. Moonen. Generating robust parsers using island
grammars. In Working Conference on Reverse
Engineering (WCRE), 2001.
[28] Neo4j. Neo4j - the world’s leading graph database,
2012. http://neo4j.org/.
[29] G. Nilson, K. Wills, J. Stuckman, and J. Purtilo.
Bugbox: A vulnerability corpus for PHP web
applications. In Workshop on Cyber Security
Experimentation and Test (CSET), 2013.
[30] NIST. SAMATE - Software Assurance Metrics And
Tool Evaluation, 2015. https://samate.nist.gov/.
[31] OWASP. OWASP WebGoat Project, 2015.
https://www.owasp.org/index.php/Category:
OWASP WebGoat Project.
[32] D. B. Paul and J. M. Baker. The design for the Wall
Street Journal-based CSR corpus. In Workshop on
Speech and Natural Language, 1992.
[33] J. Pewny, B. Garmany, R. Gawlik, C. Rossow, and
T. Holz. Cross-architecture bug search in binary
executables. In IEEE Symposium on Security and
Privacy, 2015.
[34] D. Song, D. Brumley, J. Caballero, I. Jager, M. G.
Kang, Z. Liang, J. Newsome, P. Poosankam, and
P. Saxena. Bitblaze: A new approach to computer
security via binary analysis. In International
Conference on Information Systems Security, 2008.
[35] X. Wang, H. Chen, A. Cheung, Z. Jia, N. Zeldovich,
and M. F. Kaashoek. Undeﬁned behavior: What
happened to my code? In Asia-Paciﬁc Workshop on
Systems (APSYS), 2012.
[36] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck.
Modeling and discovering vulnerabilities with code
property graphs. In IEEE Symposium on Security and
Privacy, 2014.
225