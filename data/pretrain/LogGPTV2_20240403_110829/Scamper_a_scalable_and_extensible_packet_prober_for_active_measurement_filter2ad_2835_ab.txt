o
l
Figure 1: The architecture of scamper. Measurement tasks are supplied from one or more input sources,
including from an input ﬁle, from the command line, or from a control socket. A scamper task is abstract;
the exact behaviour is determined by a set of implementer-provided callback functions. Current tasks can
be in one of three states; waiting to probe, waiting for a response or to timeout, or waiting to be written to
disk. Completed tasks can be written to a text ﬁle or a binary ﬁle, or over a control socket for interpretation.
and the byte order that values in packet headers in response
packets are returned in.
If the system would interfere in
transmitting a packet – e.g. by silently fragmenting a probe
or re-writing packet header ﬁelds, scamper allows a tech-
nique to use the datalink sockets instead. Because datalink
sockets expect a complete frame including layer-2 headers,
scamper provides an API to obtain the layer-2 addresses dy-
namically using neighbour discovery protocols. Similarly, a
technique can use a datalink socket if the system does not
provide the ability to read particular types of packets from
an Internet socket, such as raw TCP frames. Finally, some
techniques that use TCP as their probe method, such as
Sting [15] and TBIT [16], require the ability to prevent the
operating system’s TCP stack from interfering in a measure-
ment by responding to unexpected packets; scamper pro-
vides the ability to install a temporary ﬁrewall rule for some
ﬁrewall types. Scamper runs on BSD, Linux, MacOS X, So-
laris, and Windows systems; however, not all portions of the
portability layer work on all systems.
Operation: scamper can run either as a one-shot mea-
surement or as a daemon that is externally controlled. Scam-
per receives instructions in two ways. One, it accepts a list
of IP addresses to probe, either in a ﬁle or on the com-
mand line itself, along with a measurement technique to use
with each address speciﬁed on the command line. When all
addresses have been probed, scamper has completed all re-
quired work and exits. The second way is to start scamper
as a daemon and then connect to scamper’s control socket
and issue measurement instructions dynamically. This can
be done either by specifying a measurement technique and a
ﬁle with IP addresses to use, or by specifying individual mea-
surement instructions interactively. The latter approach is
powerful because it allows a macroscopic Internet measure-
ment infrastructure to rapidly begin collecting data without
implementing its own parallelised measurement tools and
data collection system.
Event driven: there are no threads in scamper, so au-
thors of new measurement techniques do not have to be con-
cerned about reentrancy of their functions. To achieve par-
allelism, scamper uses non-blocking ﬁle descriptors in con-
junction with the select system call. The main limitation
with select is there is no guarantee that it will return ex-
actly when the timeout expires. However, the ﬁner reso-
lution of the modern operating system scheduling clock has
reduced the scale of this problem. Also, some techniques are
self-clocking; for example, traceroute usually sends the next
probe immediately after the last has been received, rather
than waiting for a timeout.
Parallelism: scamper has two parameters that control
its parallelism. The ﬁrst is the minimum inter-packet trans-
mit delay permitted, deﬁned in packets per second (PPS).
The default value of 20 means that scamper will send pack-
ets spaced 50ms apart. The second, a window parameter,
deﬁnes the maximum number of active tasks permitted at
any one time. The default value of zero means the window is
unrestricted and the parallelism is deﬁned solely by the PPS
value. Scamper has been observed to probe at 1000 PPS on
modest hardware, suggesting the event driven model scales
well. Scamper aims to reach the desired rate by adding
new tasks as required. As scamper can have multiple input
sources supplying tasks concurrently, the user can specify
the priority of each source. A priority value deﬁnes the ratio
of new tasks it contributes overall in weighted round robin.
If a source is not ready to supply a command and scamper
has room in its probing budget, it obtains a new task from
the next ready input source to maximise work done.
Stand-alone: as one design objective is to make scam-
per easy for volunteers to install and operate, scamper has
no dependencies on external libraries or a conﬁguration ﬁle,
and has a small number of parameters conﬁgurable on the
command line. These features make scamper easy to com-
pile, install, and run. We considered using external libraries
such as libpcap and libdnet, but not all operating systems
contain a recent version of these libraries, and these libraries
presently do not contain all required features.
2414. MEASUREMENT TECHNIQUES
Traceroute:
scamper began with a desire to conduct
IPv6 traceroute in parallel to a large number of target ad-
dresses in support of CAIDA’s macroscopic Internet topol-
ogy discovery project [5]. The traceroute included in scam-
per is feature-rich: it supports IPv4 and IPv6; probe meth-
ods based on UDP, ICMP, and TCP, including Paris tracer-
oute [12]; path MTU discovery (PMTUD) to infer the pres-
ence of tunnels [17]; a method to infer the hops in a path
that do not send an ICMP packet too big message which is
required for PMTUD to work [18]; and doubletree [19] to
reduce redundant probing. It is optimised for macroscopic
Internet topology discovery by halting if a sequence of un-
responsive hops is encountered.
Ping: ping is useful to measure end-to-end delay and
loss, search for responsive IP addresses, and classify the be-
haviour of hosts by examining how they respond to probes.
In addition to the traditional ICMP echo method, scamper
supports UDP, TCP, and TTL-limited probing, which can
be used if directed ICMP echo probes do not obtain a re-
sponse. Scamper includes the ability to spoof the source
address of probes, as well as include IP options for record
route and timestamps; these features are useful for imple-
menting reverse traceroute [20].
MDA traceroute: scamper implements the multipath
detection algorithm described in [21] to infer all interfaces
visited between a source and destination in a per-ﬂow load-
balanced Internet path. It does this by deliberately varying
the ﬂow-identiﬁer that a router may compute when load
balancing. Probes with diﬀerent ﬂow-identiﬁers may take
diﬀerent paths and thus reveal diﬀerent parts of the forward
IP path. In addition to the ICMP and UDP methods origi-
nally implemented by Augustin et. al which vary the ICMP
checksum and UDP destination port values, scamper imple-
ments a UDP method which varies the source port instead
of the destination port so that the probes do not appear to
be a port scan. This method also provides the ability to
probe past a ﬁrewall that blocks UDP probes to ports above
the usual range used by traceroute [22]. Scamper also imple-
ments TCP methods that vary the ﬂow-id by changing the
source or destination port, depending on the user’s choice.
Alias resolution: scamper implements four techniques
for inferring which IP addresses observed in the Internet
topology are aliases. First, it implements Mercator prob-
ing [23] which infers aliases when a common source IP ad-
dress is observed in ICMP port unreachable responses to
probes sent to diﬀerent destination IP addresses. Second,
it implements Ally probing [24] which infers aliases by ob-
serving a sequence of increasing IP-ID values in response to
probes interleaved to two diﬀerent targets. Third, it im-
plements RadarGun probing [25] where all candidates are
tested simultaneously and aliases are then inferred by ob-
serving diﬀerent IP addresses with the same IP-ID velocity.
Finally, it implements a preﬁx scan method which infers a
router’s outgoing interface by ﬁnding an alias in the same
subnet as the next interface in the forward path. As with
the traceroute and ping implementations, scamper supports
UDP, ICMP, and TCP probe methods.
It also supports
sending TTL-limited probes with a speciﬁc 5-tuple of values
to solicit ICMP time exceeded messages from routers in a
path; the 5-tuple can be obtained from the data recorded
by scamper with traceroute data. This is useful for map-
ping router-level topologies when operators ﬁrewall probes
directed at their routers, or do not announce preﬁxes used
to number router interfaces.
Sting: scamper implements Savage’s [15] TCP-based al-
gorithm to infer one-way loss by making use of algorithms
used by TCP receivers when they receive out-of-sequence
packets. The technique is challenging to implement because
a ﬁrewall rule must be inserted to prevent the host’s operat-
ing system from interfering in the measurement by sending a
TCP reset in response to a packet it does not expect. There
is no standardised method across operating systems for an
application to request particular packets be ignored. The
original implementation of Sting no longer runs on a mod-
ern operating system due to the ﬁrewall interfaces changing
substantially in the past ten years.
TBIT: scamper implements two of the techniques de-
scribed in [16]: measurement of behaviour in response to
an ICMP packet too big message, and measurement of be-
haviour in response to ECN negotiation and notiﬁcation. As
with Sting, a ﬁrewall rule is required with each measurement
to prevent the host operating system from interfering. These
techniques are well suited to being implemented in scamper
because the probes are not time critical and ﬁt with scam-
per’s method of probing at a constant rate deﬁned in PPS.
However, sometimes bursts of packets are required, such as
when ramping up a TCP connection through slow start so
that the server response to a dropped packet can be mea-
sured. We are investigating methods to support parallelised
measurement of this class of measurement.
5. EVALUATION OF SCALABILITY
This section demonstrates the performance of scamper
measured by memory and CPU usage when run with diﬀer-
ent PPS rates on modest hardware. In particular, we explore
the impact of increased parallelism on performance. We
test performance using scamper’s implementation of Paris
traceroute with UDP probes [12]. Our experiment uses
traceroute in the style of macroscopic Internet topology dis-
covery [5]; traceroutes are launched to a randomly gener-
ated set of destination Internet addresses. This experiment
tests multiple dimensions of scamper. Most traceroute mea-
surements to randomly selected addresses are long-lived be-
cause most destinations are unresponsive to probes, requir-
ing scamper to use memory to store information about tasks
in progress. Even though most destinations are unrespon-
sive, most probes are responded to by intermediate routers
that send time exceeded messages. This requires scamper
to eﬃciently use the CPU to receive and decode responses,
determine the appropriate task that requires the response,
store the response, and then take an appropriate action.
Our probing host is a Pentium 3 800Mhz with 128MB of
RAM and a 100Mbps Ethernet interface, running FreeBSD
8.0. We compiled scamper with gcc’s default settings on
FreeBSD to build with compiler optimisations at level two.
We instrumented scamper using the getrusage system call,
recording the total amount of user and system CPU time
consumed, as well as the maximum resident set size (RSS)
which deﬁnes the maximum amount of RAM used by a pro-
cess. We also recorded the percentage of CPU consumed by
scamper at ﬁve second intervals because scamper is likely
to be idle for a ﬁxed amount of time towards the end of its
workload while tasks timeout due to three consecutive unre-
sponsive hops. We tested scamper at rates between 100 and
1000 PPS, in random order. For each probe rate, we used
242)
%
(
e
g
a
s
u
U
P
C
x
a
M
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
 0
 200
 6
 5
 4
 3
 2
 1
 0
 1000
CPU
RSS
 400
Packets per second
 600
 800
)
B
M
(
e
g
a
s
u
S
S
R
x
a
M
Figure 2: Maximum CPU and RSS usage observed
for diﬀerent packets per second rates. With tracer-
oute, scamper’s CPU and memory requirements
grow linearly with probing speed, but neither re-
quires signiﬁcant computing resources.
 1
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 10
400
600
1000
200
800
 60
 20
Time to complete traceroute
 30
 40
 50
 70
 80
Figure 3: Time required to complete tasks at diﬀer-
ent PPS rates. The time elapsed for each traceroute
is almost identical. The separation at 32 seconds is
caused by some tasks waiting an additional timeout.
the same list of 10,000 randomly selected IP addresses from
preﬁxes observed at Route Views [26]. We did not probe