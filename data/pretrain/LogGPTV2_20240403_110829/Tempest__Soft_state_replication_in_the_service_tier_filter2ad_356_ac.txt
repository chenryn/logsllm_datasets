### Closed-Loop Experiment Setup

In the closed-loop experiment, clients submit a job, receive a response, and then wait for 20 milliseconds before submitting the next job [33]. Each experiment began with a startup phase where we populated the data repository with 1,024 distinct objects, each identified by a unique object identifier. Client requests were drawn from a Zipf distribution (with parameter \( \beta = 1 \)) over the space of object identifiers, with reads and writes equally distributed. In all experiments, the clients were the front-ends from Figure 2. We measured the Web Service Interaction Time, i.e., the request latency observed by 1, 2, 4, 16, 32, 64, 128, 256, 512, 800, and 1,024 concurrent clients. Multiple virtual clients ran on the same 64 physical machines.

At the start of each run, all clients were instructed by a coordinator to begin the experiment without taking any measurements. This warm-up period, lasting approximately 20 seconds, is necessary for various reasons, such as allowing the JVM's Just-In-Time (JIT) compiler to optimize performance. After the warm-up, the coordinator instructed all clients to start taking measurements, which lasted for another 20 seconds. Finally, the coordinator instructed the nodes to cool down for 20 seconds before stopping.

### Results and Analysis

Figure 4 shows that Tempest's latency is significantly lower than any of the baselines, confirming that fault-tolerant services with performance-critical properties can be built on the Tempest platform. The graphs also indicate that Tempest scales well with the number of concurrent requests. 

The breakdown of the latency reveals that the baseline's overhead distribution is bimodal. For up to 64 concurrent clients, the database interaction time (data access) increases roughly linearly. Beyond 64 clients, the data access time remains constant while the total latency continues to increase with the number of concurrent requests. This suggests that the databases and/or the Tomcat application server have some form of queueing admission control that activates under heavy load.

A closer look at the latency breakdown in Figure 4 (for 1 to 32 concurrent clients) shows that the time spent by a Tempest service manipulating data (e.g., performing object deep cloning, managing data structure lock contention, web service invocation identifier tagging, and index maintenance) is an order of magnitude smaller compared to the database interaction. Specifically, it is around 1 millisecond, regardless of the number of concurrent clients, indicating that fine-grained data structures allow for better performance under contention.

### Graceful Recovery under Heavy Load

To evaluate Tempest's behavior in the face of failures, we conducted a set of experiments. Node crashes were not particularly interesting because the gossip-based failure detection protocols quickly detect failed nodes, expel them from the group, and shift work to other nodes. More details on the timeliness of the gossip-based failure detector can be found in our previous work [26].

We focused on overload scenarios that degrade service components without causing them to crash. In these scenarios, services become lossy and inconsistent, and queries return results based on stale data. Two key questions were: how does the system behave during the overload, and how long does it take to recover after the overload ends?

In the experiment, we replicated the ShoppingCart service on six low-end Tempest servers in the Cornell cluster. A client injected a single source stream of updates at a rate of one update every 20 milliseconds, while concurrently performing query requests on eight threads, resulting in a query rate approximately eight times higher than the update rate. Under normal conditions, the Tempest nodes were not overloaded. The overload was induced as follows:

- At time \( t \), 128 "rogue" clients bombarded three of the Tempest servers with requests, making them victims.
- At time \( t + \Delta \), the rogue clients terminated, and the update stream ceased.

In this experiment, \( t \) was 10 seconds, and \( \Delta \) was 30 seconds. The rogue clients used multiple streams of continuous IP multicast requests to saturate the processing capacity and cause kernel/NIC queues to drop packets. Additional background load was superimposed on the victim servers to further stress the system. These attacks did not cause the servers to crash but did cause them to become overloaded, drop packets, and return stale results.

Figure 5 shows the number of "stale" query results against time, binned in 2-second intervals. The Tempest gossip rate was set to once every 40 milliseconds. Throughout the overload period, the victim nodes dropped packets, and the Tempest repair protocols worked to resolve inconsistencies. Queries reaching the overloaded nodes could see stale data, as recent updates were lost.

Once the attack ended, Tempest recovered gracefully. The number of stale replies followed a tri-modal distribution: normal operational regime, response under heavy load (between 30 and 40 seconds), and a transient recovery period (between 40 and 65 seconds). Since the update stream ceased simultaneously with the attack, new updates did not contribute to clearing the stale state.

### Scalability in the Number of Services

To assess how Tempest scales in different dimensions, such as the size of collaborating services, the number of front-ends, and the number of replicas, we built a synthetic PetStore application on top of Tempest and evaluated it on the Emulab testbed. The application consisted of a battery of front-ends issuing requests to a "cloud" of services.

The services in the cloud had varying response time characteristics: some were I/O-intensive (e.g., an indexing service), others were CPU-intensive (e.g., a recommendation service), and some were both I/O and CPU-bound. We also considered the response time variances, noting that services performing multiple I/O operations are likely to suffer from scheduling delays. Lock contention within Tempest can also cause large response time variance. All PetStore services stored soft state using some form of a TempestMap or TempestSet.

Initially, we ran baseline experiments to measure the behavior of each type of service individually under normal load. The experiment involved two front-ends issuing request streams (half updates, half reads) of one query every 40 milliseconds in a closed loop to a single replicated service. The gossip rate was set to once every 100 milliseconds. We repeated the experiment for various numbers of replicas and for each type of service. Figure 6 shows the query latency for all services; the error bars represent standard error. Even for services with small response time variance, if they were I/O-bound, they exhibited large variances.

### Conclusion

The results demonstrate that Tempest provides significant performance benefits and can handle high levels of concurrency and overload gracefully. The system's ability to recover from attacks and maintain consistency under stress makes it a robust platform for building fault-tolerant, performance-critical services.