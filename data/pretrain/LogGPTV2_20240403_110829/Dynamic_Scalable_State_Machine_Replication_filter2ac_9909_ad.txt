switch. Each node is equipped with two network interfaces.
The switches are interconnected via a 20 Gbps link. The nodes
ran CentOS Linux 6.2 64-bit with kernel 2.6.32. Clients were
deployed on the Dell nodes; Paxos’s acceptors and servers were
deployed on the HP nodes.
B. Implementation
We use a B+-tree service to evaluate and compare the
SMR, P-SMR, and opt-PSMR techniques. Each entry includes
an 8-byte integer key, used as the tree index, and an 8-byte
value. The service supports all the commands described in
Section IV-A. There are two replicas and the tree is initialized
with 10 million keys on each replica.
 0 2 4 6 8Fail rate (%)010203040500200400600800100012001400Fails (Kcps)Throughput (Kcps) 0 5 10 15 20 25 300200400600800100012001400Latency (msec)Throughput (Kcps)AvgPassedFailedFig. 4.
The impact of dependent commands on the performance of SMR, P-SMR, opt-PSMR; x-axis shows the percentage of dependent commands in
the workload; the following metrics are shown: maximum throughput in Kilo commands executed per second (Kcps) (left); average latency in milli seconds
(bottom-right); the percentage of failed commands (top-right).
We implemented atomic multicast using Multi-Ring
Paxos [18]. Multicast groups in Multi-Ring Paxos are mapped
to one or more Ring Paxos instances [19]. In Multi-Ring Paxos,
a message can be addressed to a single group only, not to
multiple groups. To implement P-SMR and opt-PSMR, each
server thread ti in our prototypes belongs to two groups: one
group, gi, to which no other thread in the server belongs, and
one group gall, to which every thread in each server belongs.
The safety check function of opt-PSMR for the B+-tree
is implemented as follows. The key space is range parti-
tioned among the threads. Whenever a thread ti receives an
insert(k) or delete(k) operation that is optimistically
multicast, it ﬁrst locates the leaf node α in the B+-tree where
the key will be inserted in or deleted from. Node α’s parent
in the tree points to α so that any key within range β1..β2
will be directed to α. The safety check passes if the following
conditions hold: (a) the insertion or deletion of the key will
not cause structural changes in the tree (e.g., the leaf node has
space for the insert or will not result in a merged in the case
of a delete); (b) the largest key in the partition assigned to
thread t(i−1) is smaller than β1; and (c) the smallest key in
the partition assigned to thread t(i+1) is greater than β2. Note
that conditions (b) and (c) check that no thread other than ti
will be accessing node α during the insertion (or deletion).
C. Experimental setup
In all the experiments clients select the keys uniformly.
Each experiment (i.e., a point in the graphs) is performed
for 60 seconds out of which the ﬁrst and last 5 seconds are
discarded. We perform three sets of experiments:
•
•
The ﬁrst experiment measures the cost of
failed
commands in opt-PSMR (see Section V-D). Failed
commands pass through the agreement layer twice and
are expected to negatively impact latency.
The initial objective behind opt-PSMR is to overcome
the inefﬁciency of P-SMR at executing dependent
commands. In this experiment we vary the percentage
of dependent commands in the workload and seek to
see whether opt-PSMR achieves its goal in optimizing
P-SMR (see Section V-E).
• With a workload composed of dependent commands
only, as the number of threads in P-SMR increases the
performance reduces. In this experiment we compare
the performance of opt-PSMR versus P-SMR while
varying the number of threads (see Section V-F).
D. The Impact of failed commands on performance
Figure 3 shows the effect of failed commands on the
latency of opt-PSMR. There are 8 threads on each replica and
the workload is composed of insert and delete operations
only. Therefore, all the commands are optimistically multicast.
Differently from the algorithm, in our implementation when a
command fails, a replica notiﬁes the corresponding client to
resubmit the command. Thus, failed commands traverse the
path between the client and the server twice. In the algorithm,
as presented in Section IV-C, the replicas multicast failed
requests themselves without informing the clients.
The top left graph shows the fail rate versus the through-
put. As the throughput increases the fail rate decreases. The
reason for the decrease is that although the number of failed
commands increases (bottom left graph), this growth is not
proportional to the increase in throughput. The right graph
shows three curves for latency: the average latency for all the
commands, average latency for failed commands, and average
latency for passed commands. As expected, the latency of
failed commands is approximately twice the latency of passed
commands. Since the number of failed commands is much
lower than the number of passed commands, the impact of
fails on the average latency is negligible.
E. The impact of dependent commands on performance
Figure 4 shows the maximum performance of SMR, P-
SMR, and opt-PSMR with a workload composed of read,
insert, and delete operations. Replicas of P-SMR and
opt-PSMR contain 8 threads each. In P-SMR insert and
050010001500200025003000010255075100Throughput (Kcps)Percentage of dependent commandsP-SMR  SMR      opt-PSMR 0 2 4 6 8Fail rate (%) 0 3 6 9 12010255075100Latency (msec)Percentage of dependent commandsFig. 5. The impact of the number of threads on the performance of P-SMR and opt-PSMR; the following metrics are shown: maximum throughput in Kilo
commands executed per second (Kcps) (top-left); normalized per-thread throughput (bottom-left); fail rate and average latency in milliseconds (top-right); CPU
usage (bottom-right)
delete operations are multicast to all the groups and there-
fore delivered by all the worker threads. In opt-PSMR however,
these operations are optimistically multicast based on the keys
they access. While the x-axis shows the percentage of depen-
dent commands in P-SMR and SMR, it shows the percentage
of optimistically multicast commands in opt-PSMR.
As the percentage of dependent commands in the workload
increases, the throughput of P-SMR decreases and at about
50% falls below SMR’s throughput. opt-PSMR’s throughput
however, remains above SMR’s throughput even with 100% of
dependent commands in the workload (left graph). As the per-
centage of the dependent commands in the workload increases
from 0 to 1, P-SMR suffers a big reduction in throughput. This
is an evidence of P-SMR’s inefﬁciency at executing dependent
commands even when dependent commands constitute only a
small fraction of the workload. opt-PSMR outperforms P-SMR
by a factor of 2.4 times.
The top graph on the right shows the fail rate in opt-PSMR.
The low fail rate is the reason opt-PSMR outperforms SMR
and P-SMR irrespective to the percentage of the dependent
commands in the workload. Latency is shown in the bottom
right graph. Latency of opt-PSMR is slightly higher than
the latency of SMR and P-SMR, mainly because opt-PSMR
achieves a higher throughput.
F. The impact of the number of threads on performance
Figure 5 shows the scalability of opt-PSMR and P-SMR
with a workload composed of 100% dependent commands. As
the top left graph indicates, by adding more threads to the repli-
cas, the throughput of opt-PSMR increases and the throughput
of P-SMR decreases. Notice that the workload includes only
dependent commands and thus all the threads in P-SMR must
deliver these commands and synchronize to provide exclusive
access of the service to only one of the threads. As more
threads are added to P-SMR, the synchronization overhead
increases and negatively affects the throughput. The low fail
rate of opt-PSMR (top right graph) helps it achieve scalable
performance.
The bottom left graph shows the normalized per-thread
throughput. Although opt-PSMR does not reach perfect scala-
bility due to the failed commands, its scalability is better than
P-SMR’s. The values of the latency in the top right graph show
that opt-PSMR’s gain in throughput does not incur high costs
on latency. The bottom right graph shows the CPU usage of
the techniques. opt-PSMR has higher CPU consumption due
to the higher number of requests executed.
025050075010001250012468Throughput (Kcps)Number of threads 0 2 4 6 8Fail rate(%)  0 3 6 9 12012468Latency (msec)Number of threads 0 0.2 0.4 0.6 0.8 10124678per-thread normalized throughputNumber of threadsP-SMRopt-PSMR 0 100 200 300 400 500012468CPU (%)Number of threadsVI. RELATED WORK
In Section III we have provided a thorough discussion
on parallel state-machine replication and reviewed the related
work, in this section we review general-purpose approaches
that can be used to implement parallel replicas and then brieﬂy
overview optimistic approaches that are applied to replication
techniques.
General-purpose approaches. Allowing multiple threads to
execute commands concurrently may result in state and output
inconsistencies if dependent commands are scheduled differ-
ently in two or more replicas. In [20], [21], [22], [23] the
authors propose different approaches to enforcing deterministic
multithreaded execution of commands. These solutions impose
performance overheads and may require re-development of the
service using new abstractions. Another solution is to allow
one of the multithreaded replicas to execute commands non-
deterministically and log the execution path, which will be later
replayed by the rest of the replicas. Logging and replaying
have been mainly developed for debugging and security rather
than fault tolerance [24], [25], [26], [27], [28], [29], [30].
These approaches typically have high overhead due to logging
and may suffer from inaccurate replay, leading to differences
among original and secondary copies.
Optimistic techniques. Optimistic or speculative execution
has been suggested before as a mechanism to reduce the
latency of agreement problems. For example, in [31], [32]
clients are included in the execution of the protocol to reduce
the latency of Byzantine fault-tolerant agreement. In [9],
[10] the authors introduce atomic broadcast with optimistic
delivery in the context of replicated databases. Similar to [11]
the motivation is to overlap the execution of transactions or
commands with the ordering protocol, optimistically assuming
that the outcome of the agreement layer will comply with
the execution order. Our optimistic strategy differs from these
approaches in that it only involves clients and replicas and not
the agreement layer. Moreover for some applications, a safety
check is sufﬁcient to avoid the need for execution rollbacks,
as we showed with a B+-tree example.
VII. CONCLUSION
State-machine replication is a well established replication
technique and has been extensively discussed in the literature.
In this paper, we concentrated on works that deal with adapting
state-machine replication to parallel services. We reviewed
existing proposals and compared their architectures. Our com-
parison showed that among existing techniques, P-SMR has a
more scalable architecture in that unlike other approaches its
design model does not include centralized components. We
built on the scalable design of P-SMR and by identifying
technique based on an
its shortcomings proposed a novel
optimistic strategy that was able to signiﬁcantly boost
its
performance.
REFERENCES
[1] L. Lamport, “Time, clocks, and the ordering of events in a distributed
system,” Communications of the ACM, vol. 21, no. 7, pp. 558–565,
1978.
[2] F. B. Schneider, “Implementing fault-tolerant services using the state
machine approach: A tutorial,” ACM Computing Surveys, vol. 22, no. 4,
pp. 299–319, 1990.
[3] R. Kotla and M. Dahlin, “High throughput byzantine fault tolerance,”
in DSN, 2004.
[4] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi, and
M. Dahlin, “Eve: Execute-verify replication for multi-core servers,” in
OSDI, 2012.
[5] P. J. Marandi, C. E. Bezerra, and F. Pedone, “Rethinking state-machine
replication for parallelism,” ICDCS, 2014.
[6] F. Pedone and A. Schiper, “Optimistic atomic broadcast,” in DISC,
1998.
[7] L. Lamport, “Fast Paxos,” Distributed Computing, vol. 19, no. 2, pp. 79–
103, 2006.
[8] A. L. P. F. de Sousa, J. O. Pereira, F. Moura, and R. C. Oliveira,
“Optimistic total order in wide area networks,” in SRDS, 2002.
[9] R. Jim´enez-Peris, M. Pati˜no Mart´ınez, B. Kemme, and G. Alonso,
“Improving the scalability of fault-tolerant database clusters,” ICDCS,
2002.
[10] B. Kemme, F. Pedone, G. Alonso, and A. Schiper, “Processing trans-
actions over optimistic atomic broadcast protocols,” ICDCS, 1999.
[11] P. J. Marandi, M. Primi, and F. Pedone, “High performance state-
machine replication,” DSN, 2011.
[12] T. D. Chandra and S. Toueg, “Unreliable failure detectors for reliable
distributed systems,” J. ACM, vol. 43, no. 2, pp. 225–267, 1996.
[13] L. Lamport, “The part-time parliament,” ACM Transactions on Com-
puter Systems, vol. 16, pp. 133–169, May 1998.
[14] H. Attiya and J. Welch, Distributed Computing: Fundamentals, Simu-
lations, and Advanced Topics. Wiley-Interscience, 2004.
[15] A. D. Birrell and B. J. Nelson, “Implementing remote procedure calls,”
ACM Transactions on Computer Systems, vol. 2, no. 1, pp. 39–59, 1984.
[16] A. S. Tanenbaum, Distributed operating systems. Pearson Education
India, 1995.
[17] N. Santos and A. Schiper, “Achieving high-throughput state machine
replication in multi-core systems,” in ICDCS, 2013.
[18] P. J. Marandi, M. Primi, and F. Pedone, “Multi-Ring Paxos,” in DSN,
2012.
[19] P. J. Marandi, M. Primi, N. Schiper, and F. Pedone, “Ring Paxos: A
high-throughput atomic broadcast protocol,” in DSN, 2010.
[20] A. Aviram, S.-C. Weng, S. Hu, and B. Ford, “Efﬁcient system-enforced
deterministic parallelism,” in OSDI, 2010.
[21] T. Bergan, N. Hunt, L. Ceze, and S. D. Gribble, “Deterministic process
groups in dos,” in OSDI, 2010.
J. Devietti, B. Lucia, L. Ceze, and M. Oskin, “DMP: deterministic
shared memory multiprocessing,” in ASPLOS, 2009.
[22]
[23] A. Thomson and D. J. Abadi, “The case for determinism in database
systems,” Proc. VLDB Endow., vol. 3, pp. 70–80, Sept. 2010.
[24] G. Altekar and I. Stoica, “ODR: output-deterministic replay for multi-
core debugging,” in SOSP, 2009.
[25] G. W. Dunlap, D. G. Lucchetti, M. A. Fetterman, and P. M. Chen,
“Execution replay of multiprocessor virtual machines,” in VEE, 2008.
[26] P. Montesinos, L. Ceze, and J. Torrellas, “Delorean: Recording and
deterministically replaying shared-memory multiprocessor execution
efﬁciently,” in ISCA, 2008.
[27] S. Park, Y. Zhou, W. Xiong, Z. Yin, R. Kaushik, K. H. Lee, and S. Lu,
“PRES: probabilistic replay with execution sketching on multiproces-
sors,” in SOSP, 2009.
[28] M. Ronsse and K. De Bosschere, “Recplay: a fully integrated practical
record/replay system,” ACM Trans. Comput. Syst., vol. 17, pp. 133–152,
May 1999.
[29] K. Veeraraghavan, D. Lee, B. Wester, J. Ouyang, P. M. Chen, J. Flinn,
and S. Narayanasamy, “DoublePlay: parallelizing sequential logging
and replay,” SIGPLAN Not., vol. 47, pp. 15–26, Mar. 2011.
[30] M. Xu, R. Bodik, and M. D. Hill, “A “ﬂight data recorder” for enabling
full-system multiprocessor deterministic replay,” in ISCA, 2003.
[31] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong, “Zyzzyva:
speculative byzantine fault tolerance,” SOSP, 2007.
[32] B. Wester, J. Cowling, E. B. Nightingale, P. M. Chen, J. Flinn, and
B. Liskov, “Tolerating latency in replicated state machines through
client speculation,” in NSDI, 2009.