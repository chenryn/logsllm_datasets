InputRunningvarianceRunningmean...omittedforsimplicity...Loss(a)backdooredtrainingoperationdata(b)normaltrainingSumlossesReLUBatchNormBatchNormRunningvarianceRunningmeanLossReLUTypeConvolutionConvolutionInputConvolutionBatchNormLossReLUTable 6: Comparison of backdoors and adversarial examples.
Features
Adversarial Examples
Universal
[8, 13, 46, 52, 61]
Non-universal
[26, 64, 90]
black-box [64], none* black-box [52]
no
required
no
limited [20]
yes
no
required
no
no
yes
Attacker’s access to model
Attack modiﬁes model
Inference-time access
Universal and small pattern
Complex behavior
Known defenses
* For an untargeted attack, which does not control the resulting label, it is possible to attack without model access [90].
Blind
(this paper)
change code
yes
optional
yes
yes
no
Poisoning
[10, 28, 92]
change data
yes
required
yes
no
yes
Backdoors
Trojaning
[29, 55, 103]
change model
yes
required
yes
no
yes
attacks [40, 44, 98] train the model so that the backdoor sur-
vives transfer learning and ﬁne-tuning. Lin et al. [49] demon-
strated backdoor triggers composed of existing features, but
the attacker must train the model and also modify the input
scene at inference time.
Attacks of [51, 72, 101] assume that the attacker controls
the hardware on which the model is trained and/or deployed.
Recent work [14, 29] developed backdoored models that can
switch between tasks under an exceptionally strong attack:
the attacker’s code must run concurrently with the model and
modify the model’s weights at inference time.
8.2 Adversarial examples
Adversarial examples in ML models have been a subject
of much research [26, 43, 53, 65]. Table 6 summarizes the
differences between different types of backdoor attacks and
adversarial perturbations.
Although this connection is mostly unacknowledged in the
backdoor literature, backdoors are closely related to UAPs,
universal adversarial perturbations [61], and, speciﬁcally, ad-
versarial patches [8]. UAPs require only white-box [8] or
black-box [13, 52] access to the model. Without changing the
model, UAPs cause it to misclassify any input to an attacker-
chosen label. Pixel-pattern backdoors have the same effect but
require the attacker to change the model, which is a strictly
inferior threat model (see Section 2.5).
An important distinction from UAPs is that backdoors need
not require inference-time input modiﬁcations. None of the
prior work took advantage of this observation, and all pre-
viously proposed backdoors require the attacker to modify
the digital or physical input to trigger the backdoor. The only
exceptions are [3] (in the context of federated learning) and a
concurrent work by Jagielski et al. [39], demonstrating a poi-
soning attack with inputs from a subpopulation where trigger
features are already present.
Another advantage of backdoors is they can be much
smaller. In Section 4.1, we showed how a blind attack can
introduce a single-pixel backdoor into an ImageNet model.
Backdoors can also trigger complex functionality in the model:
see Sections 4.2 and 4.3. There exist adversarial examples
that cause the model to perform a different task [20], but the
perturbation covers almost 90% of the image.
In general, adversarial examples can be interpreted as fea-
tures that the model treats as predictive of a certain class [35].
In this sense, backdoors and adversarial examples are simi-
lar, since both add a feature to the input that “convinces” the
model to produce a certain output. Whereas adversarial ex-
amples require the attacker to analyze the model to ﬁnd such
features, backdoor attacks enable the attacker to introduce this
feature into the model during training. Recent work showed
that adversarial examples can help produce more effective
backdoors [63], albeit in very simple models.
9 Conclusion
We demonstrated a new backdoor attack that compromises
ML training code before the training data is available and
before training starts. The attack is blind: the attacker does
not need to observe the execution of his code, nor the weights
of the backdoored model during or after training. The attack
synthesizes poisoning inputs “on the ﬂy,” as the model is
training, and uses multi-objective optimization to achieve high
accuracy simultaneously on the main and backdoor tasks.
We showed how this attack can be used to inject single-
pixel and physical backdoors into ImageNet models, back-
doors that switch the model to a covert functionality, and
backdoors that do not require the attacker to modify the input
at inference time. We then demonstrated that code-poisoning
attacks can evade any known defense, and proposed a new de-
fense based on detecting deviations from the model’s trusted
computational graph.
Acknowledgments
This research was supported in part by NSF grants 1704296
and 1916717, the generosity of Eric and Wendy Schmidt
by recommendation of the Schmidt Futures program, and a
Google Faculty Research Award. Thanks to Nicholas Carlini
for shepherding this paper.
USENIX Association
30th USENIX Security Symposium    1517
References
[1] Akshay Agrawal, Akshay Naresh Modi, Alexandre Pas-
sos, Allen Lavoie, Ashish Agarwal, Asim Shankar, Igor
Ganichev, Josh Levenberg, Mingsheng Hong, Rajat
Monga, and Shanqing Cai. TensorFlow Eager: A multi-
stage, Python-embedded DSL for machine learning. In
SysML, 2019.
[2] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data
poisoning attacks against autoregressive models. In
AAAI, 2016.
[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deb-
orah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. In AISTATS, 2020.
[4] Lukas Biewald. Experiment tracking with weights and
biases, 2020. Software available from wandb.com.
[5] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-
In
soning attacks against support vector machines.
ICML, 2012.
[6] Battista Biggio and Fabio Roli. Wild patterns: Ten
years after the rise of adversarial machine learning.
Pattern Recognition, 84:317–331, 2018.
[7] Alex Birsan.
Dependency confusion: How I
hacked into Apple, Microsoft and dozens of other
companies. The story of a novel supply chain
attack.
https://medium.com/@alex.birsan/
dependency-confusion-4a5d60fec610, 2021.
[8] Tom B Brown, Dandelion Mané, Aurko Roy, Martín
Abadi, and Justin Gilmer. Adversarial patch. In NIPS
Workshops, 2017.
[9] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo,
Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian
Molloy, and Biplav Srivastava. Detecting backdoor at-
tacks on deep neural networks by activation clustering.
In SafeAI@AAAI, 2019.
[10] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and
Dawn Song. Targeted backdoor attacks on deep learn-
ing systems using data poisoning. arXiv:1712.05526,
2017.
[11] Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader,
Chen Zhu, Christoph Studor, and Tom Goldstein. Cer-
tiﬁed defenses for adversarial patches. In ICLR, 2020.
[12] Edward Chou, Florian Tramèr, Giancarlo Pellegrino,
and Dan Boneh. SentiNet: Detecting physical attacks
against deep learning systems. In DLS, 2020.
[13] Kenneth T Co, Luis Muñoz-González, Sixte de Mau-
peou, and Emil C Lupu. Procedural noise adversarial
examples for black-box attacks on deep convolutional
networks. In CCS, 2019.
[14] Robby Costales, Chengzhi Mao, Raphael Norwitz,
Bryan Kim, and Junfeng Yang. Live Trojan attacks on
deep neural networks. In CVPR Workshops, 2020.
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and
Serge Belongie. Class-balanced loss based on effective
number of samples. In CVPR, 2019.
[16] Jean-Antoine Désidéri. Multiple-gradient descent
algorithm (MGDA) for multiobjective optimization.
Comptes Rendus Mathématique, 350(5-6):313–318,
2012.
[17] Bao Gia Doan, Ehsan Abbasnejad, and Damith Ranas-
inghe. DeepCleanse: A black-box input sanitization
framework against backdoor attacks on deep neural
networks. arXiv:1908.03369, 2019.
[18] Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly
detection and backdoor attack detection via differential
privacy. In ICLR, 2020.
[19] Ruian Duan, Omar Alrawi, Ranjita Pai Kasturi, Ryan
Elder, Brendan Saltaformaggio, and Wenke Lee. To-
wards measuring supply chain attacks on package man-
agers for interpreted languages. In NDSS, 2021.
[20] Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha
Sohl-Dickstein. Adversarial reprogramming of neural
networks. In ICLR, 2019.
[21] N Benjamin Erichson, Dane Taylor, Qixuan Wu, and
Michael W Mahoney. Noise-response analysis for
rapid detection of backdoors in deep neural networks.
arXiv:2008.00123, 2020.
[22] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes,
Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash,
Tadayoshi Kohno, and Dawn Song. Physical adversar-
ial examples for object detectors. In WOOT, 2018.
[23] Emden Gansner, Eleftherios Koutsoﬁos, and Stephen
https://www.
North. Drawing graphs with dot.
graphviz.org/pdf/dotguide.pdf, 2015.
[24] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen,
Damith C Ranasinghe, and Surya Nepal. STRIP: A
defence against trojan attacks on deep neural networks.
In ACSAC, 2019.
[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning. MIT Press, 2016.
[26] Ian Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial ex-
amples. In ICLR, 2015.
[27] Sven Gowal, Krishnamurthy Dvijotham, Robert Stan-
forth, Rudy Bunel, Chongli Qin, Jonathan Uesato,
Relja Arandjelovic, Timothy Mann, and Pushmeet
Kohli. On the effectiveness of interval bound prop-
agation for training veriﬁably robust models. In NIPS
Workshops, 2018.
[28] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
BadNets: Identifying vulnerabilities in the machine
In NIPS Workshops,
learning model supply chain.
2017.
[29] Chuan Guo, Ruihan Wu, and Kilian Q Weinberger. Tro-
janNet: Embedding hidden Trojan horse models in
neural networks. arXiv:2002.10078, 2020.
1518    30th USENIX Security Symposium
USENIX Association
[30] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and
Dawn Song. TABOR: A highly accurate approach to
inspecting and restoring trojan backdoors in AI sys-
tems. arXiv:1908.01763, 2019.
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
CVPR, 2016.
[32] Sanghyun Hong, Varun Chandrasekaran, Yi˘gitcan
Kaya, Tudor Dumitra¸s, and Nicolas Papernot. On the
effectiveness of mitigating data poisoning attacks with
gradient shaping. arXiv:2002.11497, 2020.
[33] Jeremy Howard and Sylvain Gugger. fastai: A layered
API for deep learning. Information, 11(2):108, 2020.
[34] Xijie Huang, Moustafa Alzantot, and Mani Srivastava.
NeuronInspect: Detecting backdoors in neural net-
works via output explanations. arXiv:1911.07399,
2019.
[35] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-
gan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features.
In NeurIPS, 2019.
[36] Sergey Ioffe and Christian Szegedy. Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift. In ICML, 2015.
[37] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free
sparse convex optimization. In ICML, 2013.
[38] Matthew Jagielski, Alina Oprea, Battista Biggio,
Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipu-
lating machine learning: Poisoning attacks and coun-
termeasures for regression learning. In S&P, 2018.
[39] Matthew Jagielski, Giorgio Severi, Niklas Pousette
Harger, and Alina Oprea. Subpopulation data poison-
ing attacks. arXiv:2006.14026, 2020.
[40] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and
Ting Wang. Model-reuse attacks on deep learning
systems. In CCS, 2018.
[41] Faiq Khalid, Muhammad Abdullah Hanif, Semeen
Rehman, Rehan Ahmed, and Muhammad Shaﬁque.
TrISec: Training data-unaware imperceptible security
attacks on deep neural networks. In IOLTS, 2019.
[42] Sergey Kolesnikov.
Catalyst: Accelerated DL
https://github.com/catalyst-team/
R&D.
catalyst, 2018.
[43] Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
Adversarial examples in the physical world. In ICLR
Workshops, 2017.