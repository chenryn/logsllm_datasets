title:POSTER: How Dangerous is My Click? Boosting Website Fingerprinting
By Considering Sequences of Webpages
author:Asya Mitseva and
Jan Pennekamp and
Johannes Lohm&quot;oller and
Torsten Ziemann and
Carl Hoerchner and
Klaus Wehrle and
Andriy Panchenko
POSTER: How Dangerous is My Click? Boosting Website
Fingerprinting By Considering Sequences of Webpages
Johannes Lohm√∂ller
RWTH Aachen University
RWTH Aachen University
Jan Pennekamp
Asya Mitseva
Brandenburg Technical University
Aachen, Germany
Aachen, Germany
PI:EMAIL
PI:EMAIL
Brandenburg Technical University
Brandenburg Technical University
Cottbus, Germany
PI:EMAIL
Torsten Ziemann
Cottbus, Germany
PI:EMAIL
Klaus Wehrle
RWTH Aachen University
Aachen, Germany
PI:EMAIL
Carl Hoerchner
Cottbus, Germany
PI:EMAIL
Andriy Panchenko
Brandenburg Technical University
Cottbus, Germany
PI:EMAIL
ABSTRACT
Website fingerprinting (WFP) is a special case of traffic analysis,
where a passive attacker infers information about the content of
encrypted and anonymized connections by observing patterns of
data flows. Although modern WFP attacks pose a serious threat to
online privacy of users, including Tor users, they usually aim to
detect single pages only. By ignoring the browsing behavior of users,
the attacker excludes valuable information: users visit multiple
pages of a single website consecutively, e.g., by following links. In
this paper, we propose two novel methods that can take advantage
of the consecutive visits of multiple pages to detect websites. We
show that two up to three clicks within a site allow attackers to
boost the accuracy by more than 20% and to dramatically increase
the threat to users‚Äô privacy. We argue that WFP defenses have to
consider this new dimension of the attack surface.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Pseudonymity, anonymity and un-
traceability; ‚Ä¢ Networks ‚Üí Network privacy and anonymity.
KEYWORDS
Traffic Analysis; Website Fingerprinting; Web Privacy
ACM Reference Format:
Asya Mitseva, Jan Pennekamp, Johannes Lohm√∂ller, Torsten Ziemann, Carl
Hoerchner, Klaus Wehrle, and Andriy Panchenko. 2021. POSTER: How
Dangerous is My Click? Boosting Website Fingerprinting By Considering
Sequences of Webpages. In Proceedings of the 2021 ACM SIGSAC Conference
on Computer and Communications Security (CCS ‚Äô21), November 15‚Äì19, 2021,
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea
¬© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8454-4/21/11.
https://doi.org/10.1145/3460120.3485347
Virtual Event, Republic of Korea. ACM, New York, NY, USA, 3 pages. https:
//doi.org/10.1145/3460120.3485347
1 INTRODUCTION
Today, Tor [2] is the most popular low-latency anonymization net-
work used to hide the identity (i.e., IP address) of Internet users
and to bypass country-level censorship. To achieve anonymity, Tor
users encrypt their data in multiple layers and transmit it through
a chain of three volunteer nodes. Thus, Tor promises to hide the
relationship between users and their communication partners from
a local passive observer, e.g., an ISP, located on the link between the
Tor user and the first anonymization node [2]. However, Tor leaks
information about the number, direction, and timing of transmitted
packets, which enables the mounting of sophisticated attacks such
as website fingerprinting (WFP) [7‚Äì9]. In WFP, the attacker aims
to identify the content (i.e., the website visited) of encrypted and
anonymized connections by analyzing patterns of communication.
He collects traces of multiple page loads for each of his websites
of interest, extracts patterns (i.e., fingerprints) from the recorded
traffic, and applies machine learning (ML) to train a classifier to rec-
ognize them. Finally, he uses the trained classifier to detect which
website has been visited by observing an unknown trace of a real
user. Although modern WFP attacks [4, 8, 9] achieve more than
90% of classification accuracy in laboratory settings, their efficiency
in real world is still highly debated due to the use of unrealistic
assumptions and the huge universe size of the World Wide Web.
Currently, related work [4, 7‚Äì9] mainly focuses on the detection
of concrete index pages through isolated page loads, instead of the
site a visited page belongs to (the de-facto goal of a real adversary).
Only a few works [7, 8] examine a more realistic scenario, in which
users can visit both index and non-index pages of different websites.
However, these studies do not analyze the danger of WFP when
users browse multiple pages of a given website. On the other hand,
real users visit several pages of a single site consecutively, e.g., by
following links. Hence, if the adversary can exploit the additional
information leaked through the set of pages belonging to the same
Session 8: Poster & Demo Session CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2411website and visited by a user one after another, we argue that WFP
will become vastly more dangerous than previously expected.
In line with this revised evaluation setting, we propose two
novel WFP strategies, voting-based and HMM-based, that consider
the set of pages of a single site accessed consecutively by a user.
Although Cai et al. [1] have already apllied a Hidden Markov Model
(HMM) to model a specific user behavior, the authors used a very
limited dataset and analyzed neither the influence of the number
of observed pages of a website nor the impact of different user
behaviors on the accuracy‚Äîthe main contributions of our work. By
using our WFP strategies, we show that two, at most three, clicks
within a website allow to boost the accuracy by more than 20% and
brings it into the alarming area. Moreover, our methods improve
the attack even without the knowledge about the exact sequence
of visited pages, rendering it even more dangerous.
2 OUR FINGERPRINTING STRATEGIES
We aim to detect a website by observing a number of pages of
that specific site that are visited by a user one after another. These
consecutive visits leak information about the classification that
can be sourced to refine single predictions for individual pages. We
analyze two strategies that exploit this leakage: (ùëñ) voting-based
combining the predictions for separate pages of a single site without
considering the order of their visits, and (ùëñùëñ) HMM-based using the
knowledge about the sequence of visited pages to detect the website.
Voting-based. We use a classifier that is trained on different
websites represented through both their index and non-index pages.
For each testing page, the classifier computes a set of probability
values associated with the likelihood that the given page load be-
longs to each of these sites. Next, for each testing set of observed
pages, we multiply the probability values of these pages. As a re-
sult, for each website class, we obtain a single probability for each
testing set of observed pages and the website class with the highest
likelihood yields our final prediction.
HMM-based. We create a separate HMM model for each web-
site, in which pages correspond to different states and state transi-
tion probabilities represent the probability a user would navigate
from one page to another. As the majority of websites consists of
a large number of pages, the use of a separate state for a single
page does not scale. Thus, we use clustering to aggregate several
webpages that look similar and have the same link connectivity to
other pages of the same website into a single HMM state. As the
number of clusters varies for different websites, we use the DB-
SCAN clustering algorithm [3], which does not require any prior
information about the number of clusters to be created. The set
of created clusters represent the set of hidden states in our HMM
model and we train a separate classifier on each of these clusters.
Beside the set of hidden states, we also need to define the set
of observations, the set of transition probabilities indicating the
likelihood of generating a given observation upon transitioning to
a certain hidden state, the set of initial probabilities, and the set
of observation probabilities to complete the HMM model for each
website. The set of observations corresponds to the set of predicted
cluster labels for each testing page. To derive the set of transition
probabilities, we use two sources of data: (ùëñ) randomly-generated
user browsing sessions describing sequences of pages, and (ùëñùëñ) a
sitemap graph of each website containing available pages and the
link relationships between them. The set of start probabilities is the
relative frequency of clusters (counted for a set of training sessions)
containing the first page in a session. The set of observation proba-
bilities describes the confusion between the two sets of predicted
labels (observations) and the real labels (hidden states), i.e., how
many training pages labeled as class ùëñ are predicted as class ùëó. To
obtain the final prediction for a sequence of pages, we sum the
predicted probability values of each page for each website and then
multiply the aggregated probabilities of the pages in the sequence.
The website class with the highest likelihood is our final prediction.
3 DATASET
A typical user browsing session usually contains pages of less pop-
ular websites, e.g., local newspapers, small sport clubs [5]. Thus,
we compiled a dataset of 100 websites that consists of both less
popular websites covering different categories, different layouts,
and contents from different regions in the world and Alexa Top
websites. For each website, we then create a sitemap graph that is
used to collect randomly-generated user browsing sessions.
Generating Sitemap Graphs. For each website, we created a
sitemap graph containing data about available webpages and the
link relationship between them. Although some websites offer a
hierarchical overview documents of their pages, these documents
do not always provide data on page linkability. Thus, we used a