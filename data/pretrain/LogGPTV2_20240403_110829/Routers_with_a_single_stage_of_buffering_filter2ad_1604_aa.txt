title:Routers with a single stage of buffering
author:Sundar Iyer and
Rui Zhang and
Nick McKeown
Routers with a Single Stage of Buffering*
Sundar Iyer, Rui Zhang, Nick McKeown
Computer Systems Laboratory, Stanford University,
Ph: (650)-725 9077, Fax: (650)-725 6949
Stanford, CA 94305-9030
{sundaes, rzhang, nickm}@stanford.edu
ABSTRACT
Most high performance routers today use combined input and out-
put queueing (CIOQ). The CIOQ router is also frequently used as
an abstract model for routers: at one extreme is input queueing, at
the other extreme is output queueing, and in-between there is a
continuum of performance as the speedup is increased from 1 to N
(where N is the number of linecards). The model includes architec-
tures in which a switch fabric is sandwiched between two stages of
buffering. There is a rich and growing theory for CIOQ routers,
including algorithms,
throughput results and conditions under
which delays can be guaranteed. But there is a broad class of archi-
tectures that are not captured by the CIOQ model, including rout-
ers with centralized shared memory, and load-balanced routers. In
this paper we propose an abstract model called Single-Buffered
(SB) routers that
includes these architectures. We describe a
method called Constraint Sets to analyze a number of SB router
architectures. The model helped identify previously unstudied
architectures, in particular the Distributed Shared Memory router.
Although commercially deployed, its performance is not widely
known. We find conditions under which it can emulate an ideal
shared memory router, and believe it to be a promising architec-
ture. Questions remain about its complexity, but we find that the
memory bandwidth, and potentially the power consumption of the
router is lower than for a CIOQ router.
CATEGORIES AND SUBJECT DESCRIPTORS
C.2.6 [Internetworking]: Routers
GENERAL TERMS
Algorithms, Performance, Design.
KEYWORDS
Routers, Switching, Buffers, Constraint Sets.
I. INTRODUCTION
A. Background
The first Internet routers consisted of linecards connected to a
shared backplane. Arriving packets were written into a central pool
of shared buffer memory where they waited their turn to depart.
* This work was funded by NSF ANI-9872761-001, the Industrial Technol-
ogy Research Institute (Taiwan), the Stanford Networking Research Cen-
ter, and the Lillie Family Stanford Graduate Fellowship.
Permission to make digital or hard copies of all or part of this work for per-
sonal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior spe-
cific permission and/or a fee.
SIGCOMM’02, August 19-23, 2002, Pittsburgh, Pennsylvania, USA.
Copyright 2002 ACM 1-58113-570-X/02/0008…$5.00.
The reasons for using a shared memory architecture are well
known. First, the router’s throughput is maximized: Like an output
queued switch, a shared memory router is work-conserving and so
achieves 100% throughput and minimizes the average queueing
delay of packets. Network operators prefer routers that can guaran-
tee 100% throughput so that they can maximize the utilization of
their expensive long-haul links. Second, a shared memory router
can control the rate given to each flow and the delay of individual
packets using weighted fair queueing [1] and its variants [2][3][4].
In a shared memory router, the shared buffer memory must have
sufficient bandwidth to accept packets from and write packets to
all of the linecards at the same time. In other words, the shared
memory for a router with N linecards each connected to a line at
rate R, must have a bandwidth of 2NR.
Since the first routers were introduced, the capacity of commer-
cial routers1 has increased by about 2.2 times every 18 months
(slightly faster than Moore’s Law). Routers can continue to use
centralized shared memory only if memory bandwidth keeps up
with the increased capacity of the router. Unfortunately, this is not
the case. Router buffers are built from commercial DRAMs, which
are optimized for size rather than speed, and the random access
time to commercial DRAMs has increased by only 1.1 times every
18 months (slower than Moore’s Law) [5].2 By the mid-1990s,
router capacity grew to a point where central shared memory could
no longer be used, and it became popular to use input queueing
instead. The linecards were connected to a non-blocking crossbar
switch which was configured by a centralized scheduling algo-
rithm. From a practical point of view, input queueing allows the
memory to be distributed to each linecard, where it can be added
incrementally. More importantly, each memory need only run at a
rate 2R (instead of 2NR) enabling higher capacity routers to be
built. Theoretical results showed that: (1) With virtual output
queues (VOQs) and a maximum weight matching algorithm an
input queued router can achieve 100% throughput [6][20], (2)
With a speedup of two, and with combined input and output
queueing (CIOQ), the router can emulate an ideal shared memory
router [7], and (3) with a speedup greater than two, and WFQ
schedulers at both inputs and outputs, the router can provide delay
and bandwidth guarantees [8][9].
1. We define the capacity of a router to be the sum of the maximum data
rates of its linecards, NR. For example, we will say that a router with 16
OC192c linecards has a capacity of approximately 160Gb/s.
2. It is interesting to ask whether SRAM — which tracks the speed of
Moore’s Law — could be used instead. Unfortunately, SRAM is not dense
enough. The largest commercial SRAM device today is approximately
16Mbits. Router buffers are sized to be about
bits. A 160Gb/s
router with an RTT of 0.25 seconds requires 40Gbits of buffering, or 2,500
SRAM devices! Given that router capacity roughly tracks SRAM density,
SRAM will continue to be impractical for shared memory routers.
RTT R×
251TABLE 1 The CIOQ model for switch architectures.
Type
Number of
memories
BW of each
memory
Total BW
of memories
Crossbar Speed
(if applicable)
Comment
Input Queued
Output Queued
CIOQ
Speedup of two
N
N
2N
2R
2NR
NR
100% throughput with maximum weight matching [6], or
randomized algorithms [13].
N 1+(
)R
N N 1+(
)R
Work conserving, 100% throughput, delay guarantees.
3R
6NR
2NR
With maximal size matching: 100% throughput [14].
With a specific algorithm can emulate OQ with WFQ [7].
Table 1 summarizes some well-known results for CIOQ routers.
While the results in Table 1 might be appealing to the router archi-
tect, the algorithms required by the theoretical results are not prac-
tical at high speed because of the complexity of the scheduling
algorithms. And so the theoretical results have not made much dif-
ference to the way routers are built. Instead, most routers use a
heuristic scheduling algorithm such as iSLIP [10] or WFA [11],
and a speedup between one and two. Performance studies are lim-
ited to simulations that suggest most of the queueing takes place at
the output, so WFQ schedulers are usually placed on the egress lin-
ecards to provide differentiated qualities of service. While this
might be a sensible engineering compromise, the resulting system
has unpredictable performance. There are no throughput, fairness
or delay guarantees, and the worst case is not known.
In summary, CIOQ has emerged as a common router architec-
ture, but the performance of practical CIOQ routers is difficult to
predict. This is not very satisfactory given that CIOQ routers make
up such a large fraction of the Internet infrastructure. Our goal is to
find more tractable and practical router architectures, and this leads
us to consider a different model, one that we call the Single Buff-
ered (SB) router.
B. Single Buffered routers
Whereas a CIOQ router has two stages of buffering that “sand-
wich” a central switch fabric (with purely input queued and purely
output queued routers as special cases), a SB router has only one
stage of buffering sandwiched between two interconnects. Figure 1
illustrates both architectures. A key feature of the SB architecture
is that it has only one stage of buffering. Another difference is in
the way that the switch fabric operates. In a CIOQ router, the
switch fabric is a non-blocking crossbar switch, while in an SB
router, the two interconnects are defined more generally. For
example, the two interconnects in an SB router are not necessarily
the same, and the operation of one might constrain the operation of
the other. We will explore one architecture in which both intercon-
nects are built from a single crossbar switch.
A number of existing router architectures fall into the SB model,
such as the input queued router (in which the first stage intercon-
nect is a fixed permutation, and the second stage is a non-blocking
crossbar switch), the output queued router (in which the first stage
interconnect is a broadcast bus, and the second stage is a fixed per-
mutation), and the shared memory router (in which both stages are
independent broadcast buses). It is our goal to include as many
architectures under the umbrella of the SB model as possible, then
find tools for analyzing their performance. We divide the SB router
into two classes: (1) Routers with randomized switching or load-
balancing, for which we can at best determine statistical perfor-
mance metrics, such as the conditions under which they achieve
100% throughput. We call these Randomized SB routers; and (2)
Routers with deterministically scheduled switching, for which we
can hope to find conditions under which they emulate a conven-
tional shared memory router and/or can provide delay guarantees
for packets. We call these Deterministic SB routers.
In this paper we will only study Deterministic SB routers. But
for completeness, we describe here some examples of both Ran-
domized and Deterministic SB routers. For example, the well-
known Washington University ATM Switch [15] — which is
essentially a buffered Clos network with buffering in the center
stage — is an example of a Randomized SB architecture. Simi-
larly, the Parallel Packet Switch (PPS) [16] is an example of a
Deterministic SB architecture, in which arriving packets are deter-
ministically distributed by the first stage over buffers in the central
stage, and then recombined in the 3rd stage.
In the SB model, we allow — where needed — the introduction
of additional (usually small) coordination buffers, so long as they
are not used because of congestion. For example, in the Washing-
ton University ATM Switch, resequencing buffers are used at the
output because of the randomized load-balancing at the first stage.
In one version of the PPS, fixed size coordination buffers are used
at the input and output stages [17].
(a) CIOQ Architecture
Inter-
connect
Inter-
connect
(b) Single Buffered (SB) Architecture
Figure 1: A comparison of the CIOQ and Single Buffered (SB)
router architectures.
252TABLE 2 : Routers according to the Single Buffered architecture.
Type
# of
memories
BW of
memory
total BW
crossbar BW
(if applicable)
Comment
Input Queued
Output Queued
Parallel Packet Switch
(PPS) [16]
Buffered PPS [17]
Two-Stage (Chang [22])
Two-Stage
(Keslassy [23])
Shared Memory
Parallel Shared Memory
(PSM) or Bus-based
Distributed Shared
Memory (DSM)
(Section II and III)
Crossbar-based
Distributed Shared
Memory
(Section IV)
N
N
kN
kN
kN
N
2N
1
k
k
N
N
N
N
FCFS Crossbar-based
PSM and DSM
(Section V)
PIFO Crossbar-based
PSM and DSM
(Section V)
(
(
2h
1–
)xN
3h
2–
)xN
2R
)R
N 1+(
2R N 1+(
) k⁄
) k⁄
3R N 1+(
R N 1+(
) k⁄
2NR
)R
N N 1+(
2N N 1+(
)R
3N N 1+(
)R
N N 1+(
)R
NR
100% throughput with MWM.
Gives best theoretical performance.
Emulates FCFS OQ.
Emulates OQ with WFQ.
Emulates FCFS OQ.
2R
2R
2NR
3NR k⁄
4NR k⁄
3R
4R
4R
6R
R h⁄
R h⁄
2NR
4NR
2NR
3NR
4NR
3NR
4NR
4NR
6NR
)xNR
(
2h
----------------------------
1–
h
)xNR
(
3h
----------------------------
2–
h
100% throughput with mis-sequencing.
2NR
100% throughput, delay guarantees, no mis-sequencing.
Gives best theoretical performance.
Emulates FCFS OQ.
Emulates OQ with WFQ.
4NR
6NR
5NR
8NR
4NR
6NR
yNR
yNR
Emulates FCFS OQ, but crossbar schedule complex.
Emulates FCFS OQ, with simple crossbar schedule.
Emulates OQ with WFQ, but crossbar schedule complex.
Emulates OQ with WFQ, with simple crossbar schedule.
Emulates FCFS OQ.
Emulates OQ with WFQ.
FCFS Crossbar-based DSM switch with memories slower
than R, where xNR and yNR are memory and crossbar
speeds of the DSM.
PIFO Crossbar-based DSM switch with memories slower
than R, where xNR and yNR are memory and crossbar
speeds of the DSM.
Other examples of the SB architecture include the load-balanc-
ing switch recently proposed by Chang [22] (which is a Random-
ized SB and achieves 100% throughput, but mis-sequences
packets), and the Deterministic SB variant by Keslassy [23] (which
has delay guarantees and doesn’t mis-sequence packets, but
requires an additional coordination buffer). Table 2 shows a collec-
tion of results for different SB routers, some of which — for Deter-
ministic SB routers — are proved later in this paper.
We’ve found that within each class of SB routers (Deterministic
and Randomized), performance can be analyzed in a similar way.
For example, Randomized SB routers are usually variants of the
Chang load-balancing switch, and so they can be shown to have
100% throughput using the standard Loynes construction [22][24].
Likewise, the Deterministic SB routers that we have examined can
be analyzed using Constraint Sets (described in Section II) to find
conditions under which they can emulate ideal shared memory
routers. By construction, Constraint Sets also provide switch
scheduling algorithms.
In what follows, we describe two Deterministic SB architectures
that seem practically interesting, but have been overlooked in the
academic literature. As we will see, Constraint Sets can be used to
find conditions under which both router architectures can emulate
an ideal shared memory router.3 We call the first architecture the
Parallel Shared Memory (PSM) router, which has a centralized
shared memory that is decomposed into a number of parallel mem-
ories. The second architecture we call the Distributed Shared
Memory (DSM) router, in which memory is distributed to each
3. In Section VII we describe a third Deterministic SB Router called the
Parallel Packet Switches (PPS) which we studied in previous work.
253linecard. At first glance, the DSM router looks like an input
queued router, because each linecard contains buffers, and there is
no central shared memory. However, the buffers on a linecard do
not necessarily hold packets that arrived from, or are destined to
that linecard. The buffers are a shared and distributed resource
available to all linecards. From a practical viewpoint, the DSM
router has the appealing characteristic that buffering is added
incrementally with each linecard. This architecture is similar to
that employed by Juniper Networks in a commercial router [26],
although analysis of the router’s performance has not been pub-
lished.4
Perhaps the most interesting outcome of this paper is the com-
parison between two routers that emulate an ideal shared memory
router that performs weighted fair queueing (WFQ). The CIOQ
router requires 2N memories, each running at a speed of 3R, for a
total memory bandwidth of 6NR. In Section IV we show that the
DSM router requires N memories running at a speed of 4R, for a
total memory bandwidth of 4NR, with simple scheduling algo-
rithms. In Section VI we consider the implementation complexity
of different DSM routers.
C. Performance metrics
Throughout this paper we will be using memory bandwidth as a
means to compare different router architectures. It serves as a good
metric for two reasons: (1) Routers are, and will continue to be,
limited by the bandwidth of commercially available memories. All
else being equal, a router with smaller overall memory bandwidth
requirements can have a higher capacity, and (2) A router with
higher memory bandwidth will, in general, consume more power.
Core routers are frequently limited by the power that they consume
(because they are backed up by batteries) and dissipate (because
they must use forced air cooling). The total memory bandwidth
indicates the total bandwidth of the high-speed serial links that
connect the memories to control logic. In current systems, the
power dissipated by high speed serial links often accounts for over
50% of the router’s power.
We will not be using the commonly used metric known as
“speedup”. The term speedup is used differently by different
authors, and there is no accepted standard definition. For example,
the input queues in a CIOQ router with a “speedup” of two per-
form two read operations for every write. Is the speedup two or
one and a half? So instead, we will use the term “bandwidth”. In
our example above, the input queues have a memory bandwidth of
3R.
II. THE PARALLEL SHARED MEMORY ROUTER
An obvious question to ask is: If the capacity of a shared mem-
ory router is larger than the bandwidth of a single memory device,
why don’t we just use lots of memories in parallel, as shown in
Figure 2? This is not as simple as it first seems. If the width of the
memory data bus equals a minimum length packet (about 40
bytes), then each packet can be (possibly segmented and) written
4. The Juniper router appears to be a Randomized SB router. In the DSM
router, the address lookup (and hence the determination of the output port)
is performed before the packet is buffered, whereas in [26] the address
lookup is performed afterwards, suggesting that the Juniper router does not
use the outgoing port number, or departure order, when choosing which
linecard will buffer the packet.
DRAM consisting of k memories with random access time T
Write Access Time = T
Read Access Time = T
2NR
R
R
1
N
R
R
1
N
Arriving Packets
Departing Packets
Figure 2: Memory hierarchy of the PSM router, showing a large
DRAM memory. The DRAM memory has a total bandwidth of
at least 2NR. The logical DRAM memory consists of multiple
separate DRAMs each of which run at a slower rate.
into memory. But if the width of the memory is wider than a mini-
mum length packet,5 it is not obvious how to utilize the increased
memory bandwidth. We cannot simply write (read) multiple pack-
ets to (from) the same memory location as they generally belong to
different queues. The shared memory contains multiple queues (at
least one queue per output, usually more).