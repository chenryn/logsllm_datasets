(8)
Step 5: Recording θ0 and θ1. We store the current sampling vec-
tors θ0 and θ1 and go back to Step 3. Step 3, Step 4, and Step 5 repeat
until the average values of all elements in θ0 and θ1 converge.
Scoring Each Word. The aforementioned procedure can iter-
3.2.2
atively generate a series of samples and score them. We have stored
the corresponding θ0 and θ1. Let θk
denote the stored vector θ L in
L
the k-th iteration. These sampling values can be used to calculate
the spam score of each word by averaging the overall probability
of a word in θ0(i) and θ1(i) from all iterations, i.e., for each word
wi (cid:60) Sr , the spam score si is calculated as follows:
si =
1
K
(k)
(i)
θ
1
(k)
(i) + θ
0
(k)
1
θ
(i) ,
(9)

k
where K represents the total number of sampling iterations.
Reconstructing Spam and Ham Word Corpora. After getting
3.2.3
the spam scores of all words, we reconstruct the spam and ham
word corpora that can better capture the spam and ham patterns,
respectively. A word with a higher spam score means that any
message containing it has a higher probability of being a spam. We
rank the spam scores of all words and set a threshold to reconstruct
the spam and ham corpora. In particular, we set a spam threshold
to be 0.8 and collect all words with spam scores higher than 0.8 to
serve as the new spam corpus. Meanwhile, the ham threshold is set
to 0.4, with all words having the spam scores below 0.4 chosen to
serve as the new ham corpus. When scoring words in seed corpora,
we may initialize a word with higher score if it is deemed spam-
prone; otherwise, we set it with a lower score. In the late case, the
sampling process in Section 3.2 can help to find some other spam
words with higher confidence. Up to this point, we have addressed
reconstructing two more confident spam and ham corpora, with
each word in the two corpora being associated with a spam score.
3.3 Training Spam Word Model
The reconstructed spam and ham corpora as well as their accompa-
nying spam scores serve as the labeled dataset for training a spam
word model. Such a model can be put to use for encoding each
future message with a set of spam scores. To this end, we adopt
a neural network (NN) model for training the latent patterns by
1069Platform-Oblivious Anti-Spam Gateway
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
means of encoded words. As the NN model takes the high dimen-
sional vectors as its input, we first have to encode each word in
the reconstructed corpora via a high dimensional vector. Here, we
employ the natural language processing (NLP) model to encode
each word into a high dimensional vector for richly mining latent
patterns. For example, for any two words having similar syntactic
or semantic patterns, the NLP model will output alike vectors. This
is critically important in capturing drift spammer patterns.
NLP Model Selection. There exist a cluster of NLP models that
aim to get word encoding vectors, like word2vec [27], GloVe [33],
and others, which are well trained for directly embedding all words
into the high dimensional vector space. The embedded vectors then
represent the syntactic (structural) and semantic (meaning) patterns
of words from the respective words corpus. Such NLP models are in
an unsupervised manner, therefore applicable to train our model for
encoding the word vectors. However, this way is time-consuming
unnecessarily without any performance guarantee. Instead, we rely
on pre-trained models to get the encoded high dimensional vector
of each word. There are lots of NLP models trained specifically for
different platforms, such as SMS [57], Email [20], and Twitter[22].
Our empirical study has unveiled that these NLP models can be
used across different platforms. Here, we select the model trained
from Twitter corpus [33] to serve as our NLP model, which covers
huge amounts of both normal and spam words.
Word Vector Extraction. The selected NLP model has a dictionary-
like format, allowing us to look up the corresponding vector of
a given word. This vector is then labeled with the spam score
associated to the respective word. Each returned vector is denser,
which thus covers a set of words that have a similar structure or
meaning. In the case for a spammer to substitute one spam word
with another similar one in order to evade detection, the dense
vector can still cover it due to vector similarity. If the selected NLP
model fails to recognize some words present in the reconstructed
word corpora, we encode them with the “unknown” vectors and
reassign high scores to them, since they tend to be spam words.
Training Spam Word Model. The dense vectors (denoted as v)
of all words in the reconstructed word corpora and their associated
spam scores can be considered as the labeled ground-truth dataset
to train our employed neural network for in-depth learning on
inherent relationships among those dense vectors, with an aim
at generating a spam word scoring model. We consider a simple
five-layer neural network model, including one Input layer, three
Hidden layers, and one Output layer. The Output layer adopts the
standard sigmoid function, with a cross-entropy loss minimized by
the gradient descent on the function output.
3.4 Outlier Detection
With the trained spam word model, we are ready to employ it for
predicting all words’ spam scores in the target dataset and to encode
each message for the use of outlier detection.
Pre-processing Target Dataset. In Section 3.1, we have mined the
stem words from each message and stored them in Si = {w1, w2,· · · }.
We then use the selected NLP model to look up the dense vector of
each word in Si. If a word does not appear in the NLP model, it is
encoded with an “unknown” vector and assigned to a high spam
score. Each message is thus represented in the following format,
M = {v1, v2, ...}, where vi represents the dense vector of a corre-
sponding word wi. The trained spam word model from Section 3.3
is used to predict the spam score of each dense vector vi, say si.
The message is represented by a list of scores, with each element
holding the value of corresponding word’s spam score. We rank all
spam scores in this vector and truncate each list to 10 elements, with
0 as padding values if the list includes less than 10 elements, . Then,
a message is converted in the form of scores list s = {s1, s2, ..., s10}.
Outlier Detection. With encoded spam scores for each message,
we employ the Magnitude outlier from the prominent outlier detec-
tion method [7], to expose the outlier property of spam messages.
Let S = {s1, s2, ..., sN } denote the encoded functional data for all
messages in the target dataset. To reveal the magnitude outlier char-
acteristic of a message with its encoded vector of s, we calculate the
intercept (denoted as ˆαj) and slope (denoted as ˆβj) of the linear re-
gression model of s in discrete version over each of other messages’
encoding sj ∈ S. The intercept ˆαj is expressed by ˆαj = ¯s − ˆβj ¯sj,
where ¯s and ¯sj are the average values of all spam scores in s and sj,
Cov(s,sj)
respectively, and ˆβj is the slope, defined as: ˆβj =
Var(sj) , where
Cov(s, sj) is the covariance between s and sj and Var(sj) is variance
of sj. The magnitude index of each message can be calculated by:
Im(s,S) =
(10)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n
n
j=1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
ˆαj
After deriving the magnitude indices of all messages, we can rank
index values from the largest to the lowest. The spam messages’
property will be exposed to appear with larger values than non-
spam messages in general.
4 EXPERIMENTS
We implement our platform-oblivious detection system and conduct
extensive experiments to evaluate its performance. The main goal of
this section is twofold. First, we run our system on different datasets
from three social platforms and classify their respective spams to
show its effectiveness in exposing spams’ outlier property. Second,
we compare our system with existing supervised and unsupervised
methods in terms of spam classification performance. Besides, the
necessity of each design component and the impacts of various
parameters are also evaluated.
4.1 Implementation
System Settings. We screen the spam and ham seed corpora from
the messages with top 1% ALER and MCER measures, respectively,
in the target dataset. Notably, the selection of 1% shall be validated
in Section 4.5. The spam and ham scores are set to be 0.8 and
0.4, respectively, in the seed corpora. Such two thresholds are also
applied to identify the spam and ham words in the reconstruction
of new word corpora. The dimension of embedded word vectors is
the same as pre-trained NLP model [33], i.e., 25. The neural network
model parameters adopted in Section 3.3 are given in Table 1. In the
output layer, the mean squared error (MSE) is employed as the loss
i (si − ˆsi)2, where n is the total number of
function, i.e., MSE = 1
n
words in both reconstructed spam and ham corpora, si is the spam
score of a word wi, and ˆsi represents its predicted score.
n
1070ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Yihe Zhang, Xu Yuan, and Nian-Feng Tzeng
Table 1: Parameters of the neural network model
Layer Type
Input
Fully Connected ReLU
Fully Connected ReLU
Fully Connected ReLU
Output
Drop off rate
0.5
0.3
0.1
# of Neurons
25
64
32
8
1
Table 2: The spams, hams, and spam ratios of four datasets
dataset
Kaggle SMS
Metsis Email
Twitter Trending
Twitter Normal
Size
5,572
20,681
677,938
5,823,230
Spam
747
4,146
108,470
355,217
Ham
4,825
16,545
569,468
5,468,013
Spam Ratio
13.4%
20.0%
16.0%
6.1%
Datasets. We conduct experiments on 4 real-world datasets from
three platforms, i.e., SMS, Email, and Twitter, depicted as follows.
• Kaggle SMS dataset [5] is a set of human-labeled cell-phone
messages collected for research. There are a total of 5, 574
messages included, with each message labeled by a “ham”
(legitimate) or “spam” tag. Our experiment uses all messages
in this dataset for evaluation.
• Metsis Email Dataset [26] is a dataset including the email
sources from Enron dataset, SpamAssassin corpus and others.
We take the labeled 16, 545 ham and 4, 136 spam emails for
our experiments.
• Twitter Trending Dataset [4] includes a total of 677, 938 tweets,
collected in 2019, with focus on users who posted trending
topics. A total of 108, 470 tweets are labeled as spams via the
diversified approaches [61]of checking suspended accounts,
clustering, and the rule-based method to pre-process the raw
dataset and then performing manual checking.
• Twitter Normal Dataset [3] covers 2.5 million users collected
from Twitter networks in 2019. There are a total of 5, 823, 230
labeled tweets, in which 355, 217 of them are labeled as spams
via the same approach as in [61] with the combination of
several approaches and the manually checking is finally con-
ducted.
Table 2 summarizes the statistical information of the four datasets.
Compared Methods. We compare our solution to the existing
both unsupervised and supervised methods on spam detection. The
unsupervised methods include Alien-l and Alien-s [30], OUSLD [34],
JSF [58], Hashing [12], and Gibbs [15], in which Alien-l and Alien-s
are outlier-based methods. The supervised methods include Bayesian
Inference [37], C4.5 [16], AdaBoost [60], SVM [48], and Neural Ne-
towrk (NN) [10].
Evaluation Metrics. We evaluate the performance of spam detec-
tion by using such standard metrics as recall (Rec = DS′
), precision
T S
(Prs = DS′
), where T S represents
DS
the number of spams in the dataset, DS denotes the number of
detected spams, DS′ means the number of detected spams that are
indeed spams (i.e. true spams).
), and F1 score (F1 = 2 × Rec×Prs
Rec +Prs
(a) Kaggle SMS Dataset
(b) Metsis Email Dataset
(c) Twitter Trending Dataset
(d) Twitter Normal Dataset
Figure 7: The ranking of outlier scores of all messages.
Figure 8: The Lorenz curve and its three zones, colored in
orange, yellow and blue to indicate the Spam Zone, Uncertain
Zone, and Ham Zone, respectively.
4.2 Outlier Exposure
We implement our proposed system with the setting in Section 4.1
and run it on four target datasets. The outlier results are ranked
from the largest values to the lowest. Figures 7(a), (b), (c), and (d)
show the ranked results from the datasets of Kaggle SMS, Email,
Twitter Trending, and Twitter Normal, respectively, where the x-
axis represents the ranking indices and the y-axis represents the