On the other hand, the level of expertise or non-expertise
in computer security does inﬂuence users’ perception of se-
curity risk, as demonstrated by Ashgarpour et al. [1]. Two
experiments were conducted: a quantitative study used to
approximate the mental models of computer users with re-
gards to common security risks, and qualitative interviews
with experts and non-experts. The results of these two ex-
periments suggest that the mental models of people con-
cerned with security risks are strongly correlated to their
computer security level of expertise.
In summary, we note that all studies reviewed cover sepa-
rately one or a few of the factors described above. However,
none of these studies cover all of these factors or study the
relationship between them. Thus, one of our main contribu-
99tion is to use the clinical trial method to analyse the impacts
of all these individual factors on infection by malware to gain
a better understanding of the causes of these infections.
3. STUDY DESCRIPTION
This ﬁrst study of its kind was conducted as a 4-month
proof-of-concept study involving only 50 participants in or-
der to prove its feasibility as an alternative experimental per-
formance evaluation approach in computer security. The de-
tails of the methodology have been published elsewhere [14],
but we nonetheless provide a brief summary here. The study
monitored real-world computer usage through diagnostics
and logging tools, monthly interviews and questionnaires,
and in-depth investigation of any potential infections. The
study had the following goals:
1. Develop an eﬀective methodology to evaluate anti-virus
products in real-world environment;
2. Determine how malware infects computer systems and
identify source of malware infections;
3. Determine how phenomena such as the conﬁguration
of the system, the environment in which the system
is used, and user behaviour aﬀect the probability of
infection of a system;
The 50 participants were recruited through posters and
newspaper advertisements on the Universit´e de Montr´eal
campus (where the ´Ecole Polytechnique is located). A short
on-line questionnaire was used to collect initial demographic
information. Using these proﬁles, we categorised interested
volunteers based on their gender, age group, status and ﬁeld
of work/study. We randomly chose a sample from each cat-
egory in order to have a diverse and representative sample
of users that included students and employees from various
ﬁelds.
3.1 Ethical and Privacy Considerations
Since the study involved human subjects, the entire project
had to undergo strict review by the Comit´e d’´evaluation des
risques informatiques (C´ERI) and the Ethics Review Board
of the university. The Board imposed certain restrictions on
the study such as limits to the type of (potentially) personal
information kept, the length of time data could be kept,
the purpose of the research, and adequate remuneration for
participation in the study.
All raw data and statistics generated during the experi-
ment were anonymised, as they were only identiﬁable through
a unique number attributed to the laptop. Only the project
leader knew what subject corresponded to which number,
and this only for administrative purposes. This personal in-
formation was destroyed three months after the end of the
study. It is therefore not possible to associate collected in-
formation with the identity of the subject.
All data collected was kept in a locked cabinet in a high-
security zone, which was protected with three-factor authen-
tication (biometrics, PIN and ID card). This work zone is
completely isolated from the Internet and the university net-
work, and only personnel authorised within the context of
this project had access to the data. The security policy of
the laboratory was also applied to the deletion of all per-
sonal data related to the experiment. This policy applies to
all information whether on paper or electronic media, and
conforms with Government of Canada standards.
The use of collected data during this experiment was bound
to the stated research objectives of the project. Nonetheless,
in circumstance where the law imposes it, such as the inad-
vertent discovery of information leading a reasonable person
to believe that a (serious) crime has been committed or is
about to be committed, we would have had to report this
information to the appropriate authorities (law enforcement
agencies, etc.).
Furthermore, given that the experiment required the han-
dling of malware ﬁles, special precautions were taken in or-
der to protect the university’s IT infrastructure. For ex-
ample, all ﬁles identiﬁed as potentially malicious were en-
crypted before being stored in the high security zone of the
lab.
3.2 Equipment
The laptops that were provided to the subjects all had
identical conﬁgurations, with the following software installed:
Windows 7 Home Premium; Trend Micro’s Titanium Maxi-
mum Security (Trend Micro’s premium AV product for home
users); monitoring and diagnostic tools including Hijack-
This, ProcessExplorer, Autoruns, SpyBHORemover, Spy-
DLLRemover, tshark, WinPrefetchView, WhatChanged; and
custom Perl scripts developed for this experiment.
These scripts automated the execution of the tools and
compiled statistical data about system conﬁguration, the en-
vironments in which the system is used, and the manner in
which the system is used. The data compiled by our scripts
included: the list of applications installed and the list of ap-
plications for which updates are available; the number and
the type of web sites visited; the number and the type of ﬁles
downloaded; the list of browser plug-ins installed; the num-
ber of diﬀerent hosts to which the laptop communicates per
day; the list of the diﬀerent locations from which the laptop
establishes connection to the Internet; the average number
of hours per day the laptop is connected to the Internet; and
the average number of hours per day the laptop is on.
The AV product was centrally managed on our own server,
in a manner similar as is usually done for corporate installa-
tions to centralise distribution of signature ﬁle updates. All
the AV clients installed on the laptops were thus sending rel-
evant information to our server about any malware detected
or suspected infections as they occured.
Before deployment, we benchmarked the laptops by run-
ning tools and recording the output. The recorded infor-
mation included: a hash of all ﬁles plus information about
whether the ﬁles were signed; a list of auto-start programs;
a list of processes; a list of registry keys; a list of browser
helper objects (BHO); a list of the ﬁles loaded during the
booting process; and a list of the pre-fetch ﬁles.
In order to avoid biases in user behaviour and at the same
time limit the liability of the university, the laptops were
sold to the participants at an advantageous, below retail-
market price, with laptops staying in their possession at the
end of the study.
3.3 Experimental Protocol
The study consisted of 5 in-person sessions: an initial ses-
sion where participants received their laptop and instruc-
tions, followed by monthly 1-2 hour sessions where we per-
formed analysis to determine if the laptop was infected.
100To encourage the participants to remain in the study un-
til its end, we paid them to attend the monthly in-person
sessions. If participants completed all required sessions, the
entire cost of the laptop would be reimbursed, along with
an additional compensation. We encouraged participants
to conﬁgure their laptop as they desired and use it as they
would normally use their own computer. The only restric-
tions applied during the experiment were that the partici-
pants not format the hard drive, not replace the operating
system, not create a disk partition, not install any other AV
product on the laptop, and not delete our software and tools.
Each month, participants booked an appointment via an
on-line calendar system hosted on our server. During these
monthly sessions, participants completed an on-line ques-
tionnaire about their computer usage and experience. The
questionnaire was intended to assess the participant’s ex-
perience with the AV product and gain insights about how
the laptop was used. Meanwhile, the experimenter collected
the local data compiled by the automated scripts. Diag-
nostics tools were also executed on the laptop to determine
if an infection was suspected. If the AV product detected
any malware over the course of the month, or if our diag-
nostics tools indicated that the laptop may be infected, we
requested additional written consent from the participant to
collect speciﬁc data, such as the browser history, the tshark
log ﬁles (i.e. network traﬃc data), and the suspected ﬁle(s),
in order to help us identify the means and the source of the
infection.
In the last visit, participants completed an on-line exit
survey about their experience during the study. The aim
of this ﬁnal survey was to identify activities or mindsets
that may have unduly inﬂuenced the experimental results.
We requested that participants keep the experiment data
stored on their laptops for an additional three months, so
that if we discovered that further analysis was necessary,
we could contact them and seek their permission to collect
and analyse additional relevant data. Finally, we provided
them with a procedure for deleting the diagnostic tools and
the scripts, as well as the experiment data stored on their
laptop.
4. RESULTS AND DISCUSSION
Our analysis focuses on several aspects, ﬁrst examining
the number and type of detections found during the study,
and secondly by exploring how user characteristics and be-
havioural patterns may have aﬀected the likelihood of get-
ting infected.
4.1 Threats Detected by AV
During the 4-month study, 380 ﬁles were detected on 19
diﬀerent user machines by the AV product being evaluated.
However, some of these ﬁles were detected twice or more
on the same user machine. Removing these repetitions, we
obtain a total of 95 detections.
In terms of overall virulence, this indicates that over a
period of 4 months, 38% of our population was exposed to
malware. In truth, this ﬁgure far exceeded the expectations
of the members of the research team, largely based on their
own experience (e.g. number of AV warnings over an equiv-
alent period of time). More importantly, however, these
results would indicate that, if they are representative of the
whole user population, almost 1 out of 2 newly installed ma-
chine would be infected within 4 months if they had not had
an AV installed! This ﬁgure might seem at ﬁrst alarming
and surprising, but in fact the Eurostat report mentioned
earlier [6] indicates that over a period of 12-months in 2010,
31% of users reported a virus infection on their home com-
puters, while 84% of these users reported having some kind
of security software installed (AV, anti-spam, ﬁrewall, etc.).
Thus, if these ﬁgures are to be trusted a theoretical 38%
exposure rate should not be surprising.
In terms of the evolution of the number of infections over
time, we can see that the level of detections is very similar
for each month, contradicting the hypothesis that users are
most at-risk when they ﬁrst start using their machines. This
is shown in Figure 1 where the distribution of the detections
without repetition for each month is depicted.
Figure 1: Unique malware detections by month
Finally, in terms of type of malware each of these detec-
tions was classiﬁed based on the information provided by
the AV product. Figure 2 shows the distribution of malware
detections by type. As we can see, almost all detections
were classiﬁed as trojans, while viruses and adware have a
relatively weak representation.
Figure 2: Malware detections by type
These ﬁgures are somewhat similar to those reported for
overall infections by other AV vendors. For example, the
ﬁrst 2012 quarterly report from Panda Security [18], indi-
cates that trojans account for most detections with a ratio
of 63.30%, while worm, virus, adware and other have respec-
tively ratios of 8.39%, 7.90%, 7.81% and 9.60%. Nonethe-
less, the diﬀerences with our results could be partially at-
tributed to diﬀerences in the classiﬁcation methods. For ex-
ample, a ﬁle can be classiﬁed as a trojan by the AV product
being evaluated and as a virus by another product. Further-
more, statistical error could be signiﬁcant since our results
are only based on a collection of 95 malware samples, while
those of Panda Security are probably based on thousands of
diﬀerent samples.
101While a detailed analysis of the causes and means by
which these threats ended up on the computers still remains
to be done in future work, we already know that 17 of these
malware propagated through portable storage devices.
4.2 Missed Detections
Our experimental protocol [14, 13] describes in detail the
monthly procedure for identifying and classifying suspicious
ﬁles that were not detected by the AV. This process of iden-
tiﬁcation and classiﬁcation is based on user reporting of
suspicious machine behaviour, the analysis of logs from the
monitoring tools, the results of automated queries to on-line
sources with respect to processes found on the machine, ﬁle
and start-up programme databases (obtained automatically
by scripts that we wrote), and any other relevant piece of
information that the technician conducting the review might
deem relevant.
Suspicious ﬁles found on the computer were classiﬁed into
four categories: dangerous, suspicious, safe and unrated. All
ﬁles marked as dangerous, suspicious and unrated were sub-
jected to a more in-depth analysis. When we suspected that
a ﬁle might be dangerous, additional data were collected
with the consent of the user, including the actual browsing
history, the suspicious ﬁle, and other related ﬁles present on
the computer.
Our analysis identiﬁed 20 possible infections on 12 diﬀer-
ent machines. The most useful detection tool was Hijack-
This, which was involved in identifying 18 of the suspected
infections. SpyBHORemover helped us ﬁnd one additional
infection. The last suspected infection was reported by the
user, who called the project manager using the provided
contact number when he suspected that his machine had
been infected. All suspicious ﬁles were captured during the
monthly visits, except for the user-reported suspected infec-
tion. While the logs show the location and ﬁlename, the
ﬁle could not be retrieved as it seems that the suspected
malware uninstalled itself between the time the user called
in and the following lab visit. All captured ﬁles (19 out
of 20) were later scanned with the evaluated AV product
to see if they would be detected a posteriori. Even several
months after the end of the experiment, none were detected
by the AV product or identiﬁed as a potential threat. We
scanned the captured ﬁles a posteriori with the VirusTotal
service to compare the results obtained by several AV prod-
ucts and to compare these later results with those obtained
a few months earlier. Additionally, we searched the Inter-
net to ﬁnd as much detail as we could for each of these 20
detections. As a result of this analysis, we classiﬁed two of
the samples as clean, seven as unwanted software, nine as
adware, one as deﬁnite malware, and one as suspected (but
unconﬁrmed) malware.
The detected adware samples were either BHO or tool-
bars. In all cases, they were unknowingly installed by the
users. Their eﬀects included changing the web browser home
page, redirecting web searches, or displaying advertisements.
Further analysis will be required to determine if these ad-
ware are indeed malicious, in that they show additional be-
haviour that might have further consequences for the user
than those described (e.g. theft of personal/private informa-
tion). While we have not yet analysed in detail the two sus-
pected malware samples, we have conﬁrmed that one of them
is rogueware. As previously mentioned, the corresponding
user contacted us to inform us that his laptop was probably
infected.
It turned out that the laptop was infected with
the fake “AV Security Scanner”. Windows were regularly
appearing to inform the user that harmful software was on
his computer and every application started was killed except
for web browsers. In order to get rid of these infections, the
user was invited to register and provide his contact and pay-
ment information. At that moment, the user suspected that
he may be infected and contacted us. As explained before,
since the ﬁles disappeared from the computer before it was
brought in for inspection, it was not possible for us to verify
if the AV product detected this threat a posteriori.
Overall, 18 threats have been detected on 10 machines,
which represents 20% of the users. One ﬁrst point of compar-
ison is the above-mentioned Eurostat report. Unfortunately,
that report does not provide separate infection statistics for
the population with and without AV installed. However,
if we assume a theoretical comparison population with the
same ratio of with/without AV (84% and 16%, respectively)
and the same infection rates as we observed or inferred (20%
and 38%), this would give a combined infection rate of 23%,
in comparison with the self-reported 31% combined infec-
tion ratio of the Eurostat report. It has been said that the
Eurostat report might have been an underestimation due
to users only being able to notice a fraction of the actual
infections, but it can be equally argued that they might
be exaggerated due to users being paranoid and attributing
performance problems to “viruses”. Closer examination of
the Eurostat results indicate a strong variance of reported
infection percentage by EU countries, e.g. low twenties for
Germany, Netherlands, Finland, and 40% and higher for
many Eastern European countries. Thus, it would appear
the lower infection ratio observed for our Canadian users
might be related to geographical factors (whether location
or cultural).
Another point of comparison is the SurfRight report [25].
Over a period of 55 days, 107,435 users used the Scan Cloud
product, 73% of which were found to have an up-to-date
AV product installed. Of those, Scan Cloud found that 32%
were infected, while 46% of unprotected machines were in-
fected. In comparison with our 20% and 38% ratio, it would
appear that our sample population was less at risk than
those using SurfRight’s Scan Cloud. One possible explana-
tion is simply that one of the motivations for using such a
product is that the user already suspects that his machine
is infected, probably a good indicator that it already is.
In all cases, it is important to point that straight compar-
ison of these numbers is not signiﬁcant given the fact that
the deﬁnition and classiﬁcation methods for threats in these
studies are quite diﬀerent. In our case we depend on a clas-
siﬁcation given by the Trend Micro AV product and our own
investigations, similarly as for the SurfRight date, while the
Eurostat results totally depend on user’s self-assessment.
4.3 User Proﬁling and Behaviour
We examined whether an increase in certain types of user
behaviour leads to a higher probability of the users’ system
being infected with malware. We also investigated whether
user demographic factors and characteristics had any bear-
ing on incidences of infection.
4.3.1 Characteristics and demographic factors