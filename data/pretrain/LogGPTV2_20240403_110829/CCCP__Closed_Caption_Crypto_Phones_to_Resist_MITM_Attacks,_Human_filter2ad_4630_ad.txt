  Improve	
  Security	
  Improve	
  Usability	
  Reduce	
  Call	
  Rejec8on	
  Improve	
  Checksum	
  Comparison	
  Session F3:  Crypto PitfallsCCS’17, October 30-November 3, 2017, Dallas, TX, USA1333Table 1: The projected usability and security properties of CCCP contrasted with the traditional design. The highlighted cells
represent the key security and usability improvements offered by CCCP over the traditional designs under the required task
of Checksum Comparison.
Checksum Size
Data MITM Different Voice Converted Voice Matching Checksums Original Speaker
(Optional)
SECURITY
Voice MITM (Optional)
USABILITY
Traditional Design
CCCP
Short
Short (and Long)
Poor
Excellent
Poor
Good
Poor
Fair
Poor
Very Good
Poor
Good
mismatching checksum due to potential transcription errors.
False Positive Rate of Checksum Comparison (FPRcc) denotes
the probability of accepting such instances.
Moreover, for security assessment against the voice MITM at-
tack, we are interested in determining how often the user fails
to detect a different speaker’s voice or a converted voice. False
Positive Rate of Speaker Verification (FPRsv) denotes the proba-
bility of accepting such attacked instances.
(2) Accuracy in Benign Settings: For usability assessment, we
are interested in finding how often the system rejects matching
checksums spoken in an original speaker’s voice. False Negative
Rate of Checksum Comparison (FNRcc) represents the prob-
ability of rejecting a valid checksum by the transcriber (due
to failure of the transcriber, and potential failure of users in
correctly speaking the checksum).
In addition, as an optional task in CCCP, we would like to
find out how often the users reject the caller announcing the
matching checksum spoken in the original speaker’s voice. False
Negative Rate of Speaker Verification (FNRsv) represents the
probability of the listener rejecting a valid speaker.
(3) Efficiency: The delays incurred in performing the Checksum
Comparison and/or Speaker Verification tasks, referred to as
completion time, may impact the overall usability of the system.
This delay might arise due to: (1) users speaking the check-
sum (referred to as “Duration of Checksum”), (2) users’ delay
in verifying the speaker (referred to as “Tsv”), and (3) users
requesting the other party to repeat the checksum (referred to
as “Replay Rate” or “RR”). The delay may prolong the process
and establishment of the phone call. Perhaps less significant is
the time taken by the transcriber (referred to as “Tcc” in our
study).
(4) Comparison with Traditional Crypto Phones: As a base-
line for our study, we intend to compare the performance and
accuracy of CCCP with traditional 2-word and 4-word Crypto
Phones.
4.2 System Setup
To show the feasibility of our CCCP model in practice, and to sup-
port the security and usability study, we developed an application
for web-based clients to make web-based VoIP to our system. Next,
we describe the main components of this setup in more detail as
listed in Figure 7.
6
Figure 7: The study implementation and setup
Web-based Interface: The web-based interface was developed in
PHP, JavaScript, and HTML5 and was the entry point for the partic-
ipants in the study. It consisted of web-based WebRTC (Web-based
Real-time Communications) voice client, and a database client to
connect to the database server to read questions and store partic-
ipants’ responses. It also included a web-based audio recorder to
record the voices of the participants when they spoke the checksum.
The web-server was hosted on an Amazon Elastic Compute Cloud
(Amazon EC2) t1.micro instance with up to 3.3Ghz CPU, 1GB of
Memory, and Debian 8 Jessie operating system.
Web-based Voice Application: We set up a softswitch server on
the same EC2 machine as the webserver. We installed and config-
ured FreeSWITCH 1.6.7 as the softswitch [16]. We configured the
security group (firewall) on the EC2 instance to accept web and
voice communication protocols (HTTP, HTTPS, WS, WSS, SIP, and
RTP). The open source FreeSWITCH software supports VoIP proto-
cols including Session Initiation Protocol (SIP), IVR, and WebRTC
that are essential components to connect the web-based clients to
the switch.
We designed and implemented the web-based VoIP client based
on SIP session initiation protocol, and WebRTC transport protocol.
We used the SIP server on our cloud-based telephony platform
to initiate the session. The web-based voice client uses sipML5
open source HTML5 SIP client API [39], and supports Dual Tone
Multi-Frequency signaling (DTMF).
In our study (described in Section 4.3), we configured the IVR
system on FreeSWITCH system to play the instructions, voice
recordings of speakers and checksum challenges (from the pre-
recorded audio files of the original speaker, different speaker and
converted voice), based on the DTMF signals it receives from the
web- based application (i.e., clicking buttons on the web-page is
Web ApplicationWeb-based VoIP Clientand IVR Navigator HTML5 Audio RecorderInstructionsResponse RecorderIVR on FreeSWITCHStudy WebsitesFreeSWITCHTelephonyPostgreSQL ServerQuestionnaire FormsServerAudio File StorageSession F3:  Crypto PitfallsCCS’17, October 30-November 3, 2017, Dallas, TX, USA1334Speaker Verification). Then the participants were given instruction
on how to perform the main study tasks, i.e., how to establish a call,
speak the checksum, and verify the speaker.
In the main study phase, first, participants were asked to make a
web-based VoIP call to our soft-switch through the web-based appli-
cation. Once they get connected to the telephony system, they could
listen to the instructions through the IVR and read the displayed
instructions about how to proceed in each step of the study. Second,
the voice of a speaker was played for 2 minutes and participants
were instructed to get familiar with the speaker’s voice4. Third,
participants were asked to speak a displayed checksums picked
from CMU_ARCTIC sentences (listed in Section A.1) (“Checksum
Speaking” task), and fourth to verify a speaker who speaks the same
checksum (Speaker Verification task). The manual Speaker Verifi-
cation task was performed online and the automated Checksum
Comparison task was performed offline.
In the Checksum Speaking task, participants were asked to speak
the displayed checksum. The spoken audio was recorded and up-
loaded to our system for offline transcription and analysis. In the
Speaker Verification task, they were asked to listen to a voice that
speaks the checksum and verify if the voice is the one that they
originally got familiar with. In this part of the experiment, the sam-
ples of the original speaker’s voice, the different speaker’s voice,
and the converted voice, were played randomly one at a time.
The total number of challenges presented in the main study phase
consisted of 18 samples. The samples included: 9 samples of short-
size checksums (4 words) and 9 samples of long-size checksums (8
words). An equal number of samples of the original speaker, differ-
ent speaker and converted voices (9 samples each) were played.
We published a Human Intelligence Task (HIT) on Amazon Me-
chanical Turk (MTurk) and recruited 66 subjects. The study was
approved by our university’s IRB. Participants who completed the
HIT were compensated $1.25 each. The average duration of the ex-
periment was around 20 minutes. We chose the incentive based on
similar MTurk HITs (e.g., [59]). Appendix A.3 shows a screenshot
of several other similar studies.
4.4 Statistical Analysis Methodology
All results of statistical significance of our data analysis pursued
in the next section are reported at a 95% confidence level. The
non-parametric Friedman test is used to test for the existence of sta-
tistical differences within the groups, and, if it succeeded, Wilcoxon
Singed-Rank test is used to examine in which pairs the differences
occurred. The statistically significant pairwise comparisons are
reported with Bonferroni corrections.
5 ANALYSIS AND RESULTS
5.1 Demographic Information
There were 55.8% males and 44.2% females among the 66 partici-
pants in our study. Most of the participants were between 18 and
44 years old (33.8% 18-24 years, 32.5% 25-34 years, 23.1% 35-44
4In line with [58, 59], our study was designed to test a scenario involving a unfamiliar
speaker’s voice. Current Crypto Phones also ask the users to verify that the voice
speaking the checksum matches the voice used in the rest of the conversation for an
unfamiliar speaker.
7
Figure 8: The protocol flow and methodology of our CCCP
human factors study
translated to DTMF and is sent to the softswitch). To randomize the
ordering of the displayed and played checksums, we generated a
18 × 18 Latin Square3 using which we set the IVR to play the audio
samples. The same ordering was used to display the checksum that
the participant should speak.
Response Database: We set up a PostgreSQL database on our
Amazon EC2 server to store the answers that participants provide
to the demographic questionnaire, and to the Speaker Verification .
Audio Storage: We recorded the audio collected from the partici-
pants in the same EC2 server.
Voice Dataset We picked one male speaker
from the
CMU_ARCTIC US English dataset [11] as the original speaker of
the study (victim of the attack). As a different speaker (simple voice
MITM attacker), we picked another male speaker from the same
dataset. We converted voice of the different speaker (attacker) to
the voice of the original speaker (victim) using the Festvox voice
transformation tool [5]. This type of voice synthesis was used
in [58, 59] to perform the conversion attack against traditional
Crypto Phones. We used 100 sentences spoken by the victim and
the attackers to train the voice conversion system.
4.3 Study Protocol
The study design and protocol flow are shown in Figure 8. The
study is in line with [59]–the participants are asked to speak the
checksum and verify the speaker, but, unlike [59], they do not
compare the checksum.
In the pre-study step, we asked the participants to follow a link to
our web-based VoIP application. The participants were first asked to
fill out a demographic questionnaire. These questions poll for each
participant’s age, gender and education. An additional question
was asked for participants’ familiarity with VoIP applications. Also,
they were asked if their first language is English, and whether
they suffer from any hearing impairments (relevant to the task of
3Latin square is an n × n array filled with n different symbols, each occurring exactly
once in each row and exactly once in each column.
Call the ExtensionNumberFamiliarize with the Speaker’s VoiceVerify the SpeakerShort:4-wordsLong:8-wordsSpeak the ChecksumAnswer Pre-study QuestionsOriginalspeaker’sVoiceDifferentSpeaker’sVoiceConvertedVoiceStudy the performance of manual Speaker Verification:-Ability to detect Voice MITM-Accuracy in benign caseStudy a real VoIP setup with audio recording Large, diverse samplesStudy the performance of automated Checksum Comparison: -Ability to detect Data MITM -Accuracy in benign caseGoalsSession F3:  Crypto PitfallsCCS’17, October 30-November 3, 2017, Dallas, TX, USA1335years, 8.0% 45-54 years, and the rest 55-64 years). 26.1% of partici-
pants were high school graduates, 32.7% had a college degree, 33.4%
had Bachelor’s degree and 7.9% had Master’s degree. This analysis
shows that participants represent a diverse population by gender,
age, and education. More than 96.7% of the participants declared
that they did not have any hearing impairment5.
5.2 Design of Checksum Comparison with
Standard Transcribers
Using the collected data, we ran a basic analysis of several speech to
text tools including Google Speech API [17], Apple Mac Dictation
[35], IBM Watson Speech to Text Service, and CMUSphinx [12]. We
transcribed 100 audio samples checksums (about 800 words) using
each of the tools.
For CMUSphinx, we used the Sphinx4 transcriber demo devel-
oped in Java, which showed very high latency. Therefore, we dis-
carded it from the study. To test Google Speech, we played back
the audio files using an iPhone 7 and used the dictation add-on
on Google Docs to transcribe the audio. Similarly to test Apple
Mac dictation, we enabled the Dictation and Speech on a MacBook
laptop, played back the audio files on the phone and transcribed
the audio. To test IBM Watson tool, we set up the environment
on Watson Developer Cloud and developed a Java application that
executes curl requests to the transcriber.
The average Word Error Rate (WER) from this pilot experiment
was 12% for Google Speech API, 10% for Apple Mac Dictation, and
10% for IBM Watson service. This preliminary evidence demon-
strates the strong capability of the existing transcription tools ap-
plied to SAS checksum transcription, which can yield to robust
Crypto Phones systems.
We selected the IBM Watson Speech to Text service for the rest
of our analysis, due to its high accuracy and simplicity of devel-
opment. We developed a Java application that uses IBM Watson
Speech to Text service to build an automated comparison tool. After
receiving the result of transcription, we processed the output (gen-
erated in JSON format) to compare the transcribed audio against
the checksum. We stored the result of the analysis in the database.
Although IBM Watson Speech to Text is a cloud-based tool, we
are not suggesting a cloud-based service. We note that our idea is
not limited to this tool, considering the high performance of many
transcribers (built on top of large training models). Other on-board
tools with similar accuracies, such as Nuance [23], can be integrated
into the apps.
5.3 Resistance to Attacks
Robustness against Data MITM: We first analyze the instances
of accepting mismatching checksums (FPRcc), which leads to the
success of a data MITM attack as shown in column 3 of Table 2.
Given the frequency and simplicity of data MITM (as per Figure 5),
FPRcc is the most important parameter in CCCP.
To measure FPRcc, we compared the incorrectly transcribed
words against all the words in our checksum dictionary. The results
show that regardless of checksum size, FPRcc for all samples is
5Given a small fraction of potential hearing impaired users, we report our analysis
results in aggregate corresponding to all participants.
8
Table 2: Analysis of the data MITM attack. FNRr (the actual
FNR of the system) shows the error when at most half of the
words in the checksum are transcribed incorrectly. FPRcc re-
mains the same with or without the relaxed mode.
Checksum
Size
4-word
8-word
Checksum
Duration
5.79s (1.81)
10.28s (2.87)
FPRcc
0%
0%
FNRr
4.38%
7.44%
FNRcc
24.57%
63.17%
Table 3: Analysis of Speaker Verification for the Different
Speaker attack
Checksum
Size
4-word
8-word
Checksum
Duration
2.72s
6.07s
FPRsv Tsv
4.33s
6.56%
12.13%
5.54s
RR
5.56%
4.04%
Table 4: Analysis of Speaker Verification for the Voice Con-
version attack
Checksum
Size
4-word
8-word
Checksum
Duration
3.85s
5.50s
FPRsv
18.43%
20.45%
Tsv
5.27s
5.91s
RR
6.57%
1.51%
0%. Knowing that none of the incorrectly transcribed sequences
matched any possible checksum shows that it is highly unlikely
or impossible for the transcriber to decode an attacked checksum
to the valid checksum for a particular session. This result is the
most encouraging outcome of our CCCP solution that proves how
automated Checksum Comparison can eliminate the possibility of
data MITM attacks.
Robustness against Voice MITM: Our study shows that FPRsv
for the different speaker attack is at most 12%, which suggests that
CCCP can resist this attack in a large majority of cases. The result
is shown in column 3 of Table 3.
For the voice conversion attack, our study shows a higher FPRsv
(about 20%) compared to different speaker’s voice (participants were
less successful in detecting the attack since the voice is now more
similar to the victim’s voice). The result is shown in column 3 of
Table 4.
Again, since FPR under data MITM attack is 0%, FPRsv in Table
4 captures the rate with which the voice conversion attack will
succeed. These results suggest that even the sophisticated voice
conversion attack can be detected reasonably well for practical
purposes. We recall that this attack is not easy to launch due to the
delays introduced in the voice channel and the need to collect prior
voice samples [58].
5.4 Accuracy in Benign Setting
Accuracy of Checksum Comparison in Benign Case: The er-
rors that happened in transcribing might be the result of incorrect
pronunciation by the human user or incorrect transcription by the
transcriber tool. The collected audio samples in our experiment
were over 1100 files (over 200 minutes) and we could not manually
Session F3:  Crypto PitfallsCCS’17, October 30-November 3, 2017, Dallas, TX, USA1336verify if the participants had spoken all the words accurately. How-
ever, in a random selection of 50 audio files, we did not notice any
incorrect pronunciation of the words. Therefore, we assume that the
reported error rate is related to incorrect automated transcription
and not incorrect pronunciation by the users.
We analyze FNRcc based on the number of incorrect words in
each checksum. FNRcc is 24.57% and 63.17% for 4-word and 8-word
checksum, respectively, when at least one word is incorrectly tran-