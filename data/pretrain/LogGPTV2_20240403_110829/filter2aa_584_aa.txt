Anonymity Loves Company:
Usability and the Network Eﬀect
Roger Dingledine and Nick Mathewson
The Free Haven Project
{arma,nickm}@freehaven.net
Abstract. A growing ﬁeld of literature is studying how usability im-
pacts security [4]. One class of security software is anonymizing networks—
overlay networks on the Internet that provide privacy by letting users
transact (for example, fetch a web page or send an email) without re-
vealing their communication partners.
In this position paper we focus on the network eﬀects of usability on
privacy and security: usability is a factor as before, but the size of the
user base also becomes a factor. We show that in anonymizing networks,
even if you were smart enough and had enough time to use every system
perfectly, you would nevertheless be right to choose your system based
in part on its usability for other users.
1
Usability for others impacts your security
While security software is the product of developers, the security it provides is
a collaboration between developers and users. It’s not enough to make software
that can be used securely—software that is hard to use often suﬀers in its security
as a result.
For example, suppose there are two popular mail encryption programs: Heavy-
Crypto, which is more secure (when used correctly), and LightCrypto, which is
easier to use. Suppose you can use either one, or both. Which should you choose?
You might decide to use HeavyCrypto, since it protects your secrets better.
But if you do, it’s likelier that when your friends send you conﬁdential email,
they’ll make a mistake and encrypt it badly or not at all. With LightCrypto,
you can at least be more certain that all your friends’ correspondence with you
will get some protection.
What if you used both programs? If your tech-savvy friends use HeavyCrypto,
and your less sophisticated friends use LightCrypto, then everybody will get as
much protection as they can. But can all your friends really judge how able they
are? If not, then by supporting a less usable option, you’ve made it likelier that
your non-savvy friends will shoot themselves in the foot.
The crucial insight here is that for email encryption, security is a collabora-
tion between multiple people: both the sender and the receiver of a secret email
must work together to protect its conﬁdentiality. Thus, in order to protect your
own security, you need to make sure that the system you use is not only usable
by yourself, but by the other participants as well.
This observation doesn’t mean that it’s always better to choose usability over
security, of course: if a system doesn’t address your threat model, no amount
of usability can make it secure. But conversely, if the people who need to use a
system can’t or won’t use it correctly, its ideal security properties are irrelevant.
Hard-to-use programs and protocols can hurt security in many ways:
• Programs with insecure modes of operation are bound to be used unknow-
ingly in those modes.
• Optional security, once disabled, is often never re-enabled. For example,
many users who ordinarily disable browser cookies for privacy reasons wind
up re-enabling them so they can access sites that require cookies, and later
leaving cookies enabled for all sites.
• Badly labeled oﬀ switches for security are even worse: not only are they more
prone to accidental selection, but they’re more vulnerable to social attackers
who trick users into disabling their security. As an example, consider the
page-long warning your browser provides when you go to a website with an
expired or otherwise suspicious SSL certiﬁcate.
• Inconvenient security is often abandoned in the name of day-to-day eﬃ-
ciency: people often write down diﬃcult passwords to keep from forgetting
them, and share passwords in order to work together.
• Systems that provide a false sense of security prevent users from taking real
measures to protect themselves: breakable encryption on ZIP archives, for
example, can fool users into thinking that they don’t need to encrypt email
containing ZIP archives.
• Systems that provide bad mental models for their security can trick users
into believing they are more safe than they really are: for example, many
users interpret the “lock” icon in their web browsers to mean “You can safely
enter personal information,” when its meaning is closer to “Nobody can read
your information on its way to the named website.”1
2
Usability is even more important for privacy
We described above that usability aﬀects security in systems that aim to pro-
tect data conﬁdentiality. But when the goal is privacy, it can become even more
important. Anonymizing networks such as Tor [8], JAP [3], Mixminion [6], and
Mixmaster [12] aim to hide not only what is being said, but also who is com-
municating with whom, which users are using which websites, and so on. These
systems have a broad range of users, including ordinary citizens who want to
avoid being proﬁled for targeted advertisements, corporations who don’t want
to reveal information to their competitors, and law enforcement and government
intelligence agencies who need to do operations on the Internet without being
noticed.
1 Or more accurately, “Nobody can read your information on its way to someone who
was able to convince one of the dozens to hundreds of CAs conﬁgured in your browser
that they are the named website, or who was able to compromise the named website
later on. Unless your computer has been compromised already.”
Anonymity networks work by hiding users among users. An eavesdropper
might be able to tell that Alice, Bob, and Carol are all using the network, but
should not be able to tell which of them is talking to Dave. This property is
summarized in the notion of an anonymity set—the total set of people who,
so far as the attacker can tell, might be the one engaging in some activity of
interest. The larger the set, the more anonymous the participants.2 When more
users join the network, existing users become more secure, even if the new users
never talk to the existing ones! [1, 2] Thus, “anonymity loves company.”3
In a data conﬁdentiality system like PGP, Alice and Bob can decide by
themselves that they want to get security. As long as they both use the software
properly, no third party can intercept the traﬃc and break their encryption.
However, Alice and Bob can’t get anonymity by themselves: they need to par-
ticipate in an infrastructure that coordinates users to provide cover for each
other.
No organization can build this infrastructure for its own sole use. If a single
corporation or government agency were to build a private network to protect its
operations, any connections entering or leaving that network would be obviously
linkable to the controlling organization. The members and operations of that
agency would be easier, not harder, to distinguish.
Thus, to provide anonymity to any of its users, the network must accept
traﬃc from external users, so the various user groups can blend together.
In practice, existing commercial anonymity solutions (like Anonymizer.com)
are based on a set of single-hop proxies. In these systems, each user connects
to a single proxy, which then relays the user’s traﬃc. Single proxies provide
comparatively weak security, since a compromised proxy can trivially observe
all of its users’ actions, and an eavesdropper only needs to watch a single proxy
to perform timing correlation attacks against all its users’ traﬃc. Worse, all users
need to trust the proxy company to have good security itself as well as to not
reveal user activities.
The solution is distributed trust: an infrastructure made up of many inde-
pendently controlled proxies that work together to make sure no transaction’s
privacy relies on any single proxy. With distributed-trust anonymity networks,
users build tunnels or circuits through a series of servers. They encrypt their
traﬃc in multiple layers of encryption, and each server removes a single layer of
encryption. No single server knows the entire path from the user to the user’s
chosen destination. Therefore an attacker can’t break the user’s anonymity by
compromising or eavesdropping on any one server.
2 Assuming that all participants are equally plausible, of course. If the attacker sus-
pects Alice, Bob, and Carol equally, Alice is more anonymous than if the attacker
is 98% suspicious of Alice and 1% suspicious of Bob and Carol, even though the
anonymity sets are the same size. Because of this imprecision, research is moving
beyond simple anonymity sets to more sophisticated measures based on the attacker’s
conﬁdence [7, 14].
3 This catch-phrase was ﬁrst made popular in our context by the authors of the
Crowds [13] anonymity network.
Despite their increased security, distributed-trust anonymity networks have
their disadvantages. Because traﬃc needs to be relayed through multiple servers,
performance is often (but not always) worse. Also, the software to implement a
distributed-trust anonymity network is signiﬁcantly more diﬃcult to design and
implement.
Beyond these issues of the architecture and ownership of the network, how-
ever, there is another catch. For users to keep the same anonymity set, they need
to act like each other. If Alice’s client acts completely unlike Bob’s client, or if
Alice’s messages leave the system acting completely unlike Bob’s, the attacker
can use this information. In the worst case, Alice’s messages stand out entering
and leaving the network, and the attacker can treat Alice and those like her as
if they were on a separate network of their own. But even if Alice’s messages
are only recognizable as they leave the network, an attacker can use this infor-
mation to break exiting messages into “messages from User1,” “messages from
User2,” and so on, and can now get away with linking messages to their senders
as groups, rather than trying to guess from individual messages [6, 11]. Some of
this partitioning is inevitable: if Alice speaks Arabic and Bob speaks Bulgarian,
we can’t force them both to learn English in order to mask each other.
What does this imply for usability? More so than with encryption systems,
users of anonymizing networks may need to choose their systems based on how
usable others will ﬁnd them, in order to get the protection of a larger anonymity
set.
3
Case study: usability means users, users mean security
We’ll consider an example. Practical anonymizing networks fall into two broad
classes. High-latency networks like Mixminion or Mixmaster can resist strong
attackers who can watch the whole network and control a large part of the
network infrastructure. To prevent this “global attacker” from linking senders to
recipients by correlating when messages enter and leave the system, high-latency
networks introduce large delays into message delivery times, and are thus only
suitable for applications like email and bulk data delivery—most users aren’t
willing to wait half an hour for their web pages to load. Low-latency networks
like Tor, on the other hand, are fast enough for web browsing, secure shell, and
other interactive applications, but have a weaker threat model: an attacker who
watches or controls both ends of a communication can trivially correlate message
timing and link the communicating parties [5, 10].
Clearly, users who need to resist strong attackers must choose high-latency
networks or nothing at all, and users who need to anonymize interactive appli-
cations must choose low-latency networks or nothing at all. But what should
ﬂexible users choose? Against an unknown threat model, with a non-interactive
application (such as email), is it more secure to choose security or usability?
Security, we might decide. If the attacker turns out to be strong, then we’ll
prefer the high-latency network, and if the attacker is weak, then the extra
protection doesn’t hurt.
But since many users might ﬁnd the high-latency network inconvenient, sup-
pose that it gets few actual users—so few, in fact, that its maximum anonymity
set is too small for our needs. In this case, we need to pick the low-latency sys-
tem, since the high-latency system, though it always protects us, never protects
us enough; whereas the low-latency system can give us enough protection against
at least some attackers.
This decision is especially messy because even the developers who implement
these anonymizing networks can’t recommend which approach is safer, since they
can’t predict how many users each network will get and they can’t predict the
capabilities of the attackers we might see in the wild. Worse, the anonymity
research ﬁeld is still young, and doesn’t have many convincing techniques for
measuring and comparing the protection we get from various situations. So even
if the developers or users could somehow divine what level of anonymity they
require and what their expected attacker can do, the researchers still don’t know
what parameter values to recommend.
4
Case study: against options
Too often, designers faced with a security decision bow out, and instead leave
the choice as an option: protocol designers leave implementors to decide, and
implementors leave the choice for their users. This approach can be bad for
security systems, and is nearly always bad for privacy systems.
With security:
• Extra options often delegate security decisions to those least able to under-
stand what they imply. If the protocol designer can’t decide whether the
AES encryption algorithm is better than the Twoﬁsh encryption algorithm,
how is the end user supposed to pick?
• Options make code harder to audit by increasing the volume of code, by
increasing the number of possible conﬁgurations exponentially, and by guar-
anteeing that non-default conﬁgurations will receive little testing in the ﬁeld.
If AES is always the default, even with several independent implementations
of your protocol, how long will it take to notice if the Twoﬁsh implementation
is wrong?
Most users stay with default conﬁgurations as long as they work, and only
reconﬁgure their software as necessary to make it usable. For example, suppose
the developers of a web browser can’t decide whether to support a given exten-
sion with unknown security implications, so they leave it as a user-adjustable
option, thinking that users can enable or disable the extension based on their
security needs. In reality, however, if the extension is enabled by default, nearly
all users will leave it on whether it’s secure or not; and if the extension is dis-
abled by default, users will tend to enable it based on their perceived demand
for the extension rather than their security needs. Thus, only the most savvy
and security-conscious users—the ones who know more about web security than
the developers themselves—will actually wind up understanding the security
implications of their decision.
The real issue here is that designers often end up with a situation where they
need to choose between ‘insecure’ and ‘inconvenient’ as the default conﬁguration—
meaning they’ve already made a mistake in designing their application.
Of course, when end users do know more about their individual security
requirements than application designers, then adding options is beneﬁcial, espe-
cially when users describe their own situation (home or enterprise; shared versus
single-user host) rather than trying to specify what the program should do about
their situation.
In privacy applications, superﬂuous options are even worse. When there are
many diﬀerent possible conﬁgurations, eavesdroppers and insiders can often tell
users apart by which settings they choose. For example, the Type I or “Cypher-
punk” anonymous email network uses the OpenPGP encrypted message format,
which supports many symmetric and asymmetric ciphers. Because diﬀerent users
prefer diﬀerent ciphers, and because diﬀerent versions of encryption programs
implementing OpenPGP (such as PGP and GnuPG) use diﬀerent cipher suites,
users with uncommon preferences and versions stand out from the rest, and get
little privacy at all. Similarly, Type I allows users to pad their messages to a
ﬁxed size so that an eavesdropper can’t correlate the sizes of messages passing
through the network—but it forces the user to decide what size of padding to
use! Unless a user can guess which padding size will happen to be most popular,
the option provides attackers with another way to tell users apart.
Even when users’ needs genuinely vary, adding options does not necessarily
serve their privacy. In practice, the default option usually prevails for casual
users, and therefore needs to prevail for security-conscious users even when it
would not otherwise be their best choice. For example, when an anonymizing
network allows user-selected message latency (like the Type I network does),
most users tend to use whichever setting is the default, so long as it works.
Of the fraction of users who change the default at all, most will not, in fact,
understand the security implications; and those few who do will need to decide
whether the increased traﬃc-analysis resistance that comes with more variable
latency is worth the decreased anonymity that comes from splitting away from
the bulk of the user base.
5
Case study: Mixminion and MIME
We’ve argued that providing too many observable options can hurt privacy, but