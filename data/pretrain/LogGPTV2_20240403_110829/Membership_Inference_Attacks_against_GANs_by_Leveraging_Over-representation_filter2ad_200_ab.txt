return ùëãMCScore
return 1 if ùë•MCScore ‚â• ùúè else 0
‚ä≤ ùúè: threshold
Our attack consists of three steps, which is also described in Al-
gorithm 1. In the first step, we construct MC scores that represent
the degree of over-representation in each region. In order to esti-
mate MC scores, we first train a substitute model by querying the
target model. The training process of the substitute model can be
regarded as a model extraction attack against the GAN [7], which
aims to duplicate the target GAN model including its functionality
and implicit data distribution. In this way, data used for training the
substitute model is considered as members of the model, and data
sampled from the target model but not used for training the substi-
tute model is regarded as nonmembers. Therefore, we can know
the members and nonmembers for this substitute model, which is
utilized to construct MC scores. Here, we leverage the discrimina-
tor‚Äôs outputs instead of raw samples to split regions (Algorithm 1,
line 5-7). we convert these outputs to a fixed interval through the
sigmoid function because they have different ranges for different
discriminators. Since the discriminator‚Äôs output is a single value,
we do not need to perform clustering, instead directly dividing the
range of outputs into ùëò parts, i.e. ùëò regions. Finally, we calculate the
MC score on each part through the substitute model (Algorithm 1,
line 8).
In the second step, we assign the MC score to each suspect sample
from the target dataset ùëãtarget, based on the distance between each
suspect sample and each region (Algorithm 1, line 10-16). In the last
step, a sample with MC score higher than a threshold is predicted
as a member (Algorithm 1, line 17-18).
Note that, our method is similar to the membership inference at-
tacks against discriminative models [15] where the top-1 confidence
scores of a discriminative model are used as the features. However,
our attack does not make any assumption about the training set
while the attack against discriminative models needs a shadow
dataset that is from the same distribution of the training set.
0102030405060708090100Region0.00.20.40.60.81.0MC ScoreSession 8: Poster & Demo Session CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2388Table 1: Attack performance (SD: standard deviation).
Target Model Methods
Precision (%)
Precision (%)
Recall (%)
StyleGAN
StyleGAN
PGGAN
PGGAN
Ours
LOGAN
Ours
LOGAN
Mean (SD)
96.00 (8.00)
55.65 (0.15)
59.00 (2.04)
51.83 (0.10 )
Maximum
100.00
55.85
61.17
52.01
Mean (SD)
0.03 (0.007)
55.64 (0.15)
0.39 (0.20)
51.83 (0.10)
AUCROC/
Accuracy (%)
Mean (SD)
50.02 (0.003)
55.65 (0.15)
50.06 (0.04)
51.83 (0.10)
3 PRELIMINARY RESULTS
Datasets. We perform all of some experiments with the FFHQ
dataset [9], which contains 70, 000 human face images. We split
the dataset into two parts: a training set for model training (60, 000
images) and a test set that is not used for training. A target set is
used to evaluate the performance of membership inference attacks.
It consists of the equal number of member samples (randomly
selected from the training set) and nonmember samples (randomly
selected from the test set). In our experiments, images are resized
to 64 √ó 64 and the size of a target set is 20, 000.
Target Models. We choose PGGAN [8] and StyleGAN [9] as our
target models to be attacked, considering their excellent perfor-
mance and widespread adoption. In our experiments, target mod-
els with the best Fr√©chet Inception Distance (FID) [6] during the
training progress are selected. Specifically, the FID of target model
StyleGAN and PGGAN are 5.05 and 6.59, respectively.
Attack Evaluation. We use precision as a key indicator to evaluate
the attack performance because it can better capture the severity of
the leakage of a training set. The precision of an attack refers to the
ratio of real-true member samples in all the positive inferences. We
also report recall, accuracy and AUCROC. We compare our attack
approach with the prior work LOGAN [5], due to the similar attack
scenario. The suggested hyperparameters of LOGAN are used, and
for our method, we set the threshold as 99.99th percentile of all
MC scores of the target set and the number of clusters is 100. In all
experiments, we repeat 5 times to evaluate attack performance.
Results. Table 1 shows a comparison of different attack methods.
Overall, our method can achieve much higher mean precision than
LOGAN on both target models, even if the accuracy or AUCROC is
about 50%. We also report the maximum precision as a reference
because it can be considered as a worst-case for target models.
Our method can achieve 100% maximum precision in some cases,
which indicates these samples predicted as members are all real-true
samples. Our attack method achieves high precision at the expense
of recall because we only consider samples with higher MC scores.
It means that not all training samples can be easily inferred and
there only exist some vulnerable samples in a training set. This
is consistent with observations made on other machine learning
models, i.e., language models or classification models [1, 2, 11].
4 CONCLUSION
In this poster, we have presented a novel membership inference
attack against GANs from the perspective of precision. Our method
leverages over-representation regions of a GAN model to make
inferences. Initial experimental evaluations showed that our method
can achieve a high-precision membership inference even though
the overall attack accuracy is around 50% for a well-trained model.
[2] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
et al. 2021. Extracting Training Data from Large Language Models. In Proceedings
of USENIX Security Symposium (USENIX Security). USENIX Association, 2633‚Äì
2650.
[3] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. 2020. Gan-leaks: A taxon-
omy of membership inference attacks against generative models. In Proceedings
of ACM SIGSAC Conference on Computer and Communications Security (CCS).
ACM, 343‚Äì362.
[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Proceedings of Annual Conference on Neural Information Processing Systems
(NeurIPS). Curran Associates, Inc., 2672‚Äì2680.
[5] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019.
LOGAN: Membership inference attacks against generative models. In Proceedings
on Privacy Enhancing Technologies, Vol. 2019. Sciendo, 133‚Äì152.
[6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a
local nash equilibrium. In Proceedings of Annual Conference on Neural Information
Processing Systems (NeurIPS). Curran Associates, Inc., 6626‚Äì6637.
[7] Hailong Hu and Jun Pang. 2021. Model Extraction and Defenses on Generative
Adversarial Networks. arXiv preprint arXiv:2101.02069 (2021).
We hope that our study highlights the necessity that model owners
should systematically evaluate the privacy risks when sharing their
models, including the worst-case conditions.
As future work, we aim to relax our assumption and generalize
our approach to more challenging attack scenarios. In addition, it
will be interesting to design possible defense measures against our
new attack. In the literature, differential privacy has been shown as a
promising approach to defend against privacy attacks. However, an
effective differential privacy strategy to train a GAN that produces
high-quality images still needs to be developed in the future.
ACKNOWLEDGMENTS
This work is supported by the National Research Fund, Luxem-
bourg (Grant No. 13550291).
REFERENCES
[1] Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The secret sharer: Evaluating and testing unintended memorization in neural net-
works. In Proceedings of USENIX Security Symposium (USENIX Security). USENIX
Association, 267‚Äì284.
[8] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive
Growing of GANs for Quality, Stability, and Variation. In Proceedings of Interna-
tional Conference on Learning Representations (ICLR).
[9] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architec-
ture for generative adversarial networks. In Proceedings of IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). IEEE, 4401‚Äì4410.
[10] Klas Leino and Matt Fredrikson. 2020. Stolen memories: Leveraging model
memorization for calibrated white-box membership inference. In Proceedings of
USENIX Security Symposium (USENIX Security). USENIX Association, 1605‚Äì1622.
[11] Yunhui Long, Lei Wang, Diyue Bu, Vincent Bindschaedler, Xiaofeng Wang, Haixu
Tang, Carl A Gunter, and Kai Chen. 2020. A Pragmatic Approach to Member-
ship Inferences on Machine Learning Models. In Proceedings of IEEE European
Symposium on Security and Privacy (EuroS&P). IEEE, 521‚Äì534.
[12] C Meehan, K Chaudhuri, and S Dasgupta. 2020. A non-parametric test to de-
tect data-copying in generative models. In International Conference on Artificial
Intelligence and Statistics.
[13] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2019. ML-Leaks: Model and Data Independent Membership
Inference Attacks and Defenses on Machine Learning Models. In Proceedings of
Network and Distributed Systems Security Symposium (NDSS). Internet Society.
[14] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In Proceedings of IEEE
Symposium on Security and Privacy (S&P). IEEE, 3‚Äì18.
[15] Liwei Song and Prateek Mittal. 2021. Systematic evaluation of privacy risks of
machine learning models. In Proceedings of USENIX Security Symposium (USENIX
Security). USENIX Association, 2615‚Äì2632.
[16] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
risk in machine learning: Analyzing the connection to overfitting. In Proceedings
of IEEE Computer Security Foundations Symposium (CSF). IEEE, 268‚Äì282.
Session 8: Poster & Demo Session CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2389