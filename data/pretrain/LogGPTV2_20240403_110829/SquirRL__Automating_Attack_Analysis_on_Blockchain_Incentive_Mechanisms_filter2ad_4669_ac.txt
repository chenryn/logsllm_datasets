mining experiments, agent types also include an agent-speciﬁc
“follower fraction” γi that speciﬁes the probability that honest
nodes follow a particular agent’s chain in the case of multi-way
ties; this parameter models an agent’s network penetration and
generalizes the previously-described parameter γ.
D. State space representation: Feature extraction
The goal of SquirRL is to ﬁnd a policy (or strategy) π :
S → ∆(A), where ∆(A) denotes the probability simplex over
actions in A. We let π(·|s) denote the mapping from a state
s to a distribution over actions. Ideally, we would implement
this π as a lookup table. However, in general, the state space
S is the set of all possible blockDAGs, which is inﬁnite.
The canonical way of solving this problem is with function
approximation [67]; instead of making π a lookup table, we
implement it with a different function. Concretely, for some
d ∈ N, let f : Rd → ∆(A), and ϕ : S → Rd. We then let π(·|s) =
f (ϕ(s)). f is usually chosen to be a neural network. ϕ is called
the feature extraction method, and designing a good ϕ for
blockchains will be the focus of this section.
We write a general framework for specifying ϕ in blockchain
incentive mechanism problems. Our suggestions for how to
construct ϕ may be lossy or redundant in general; however, we
will show that for a variety of blockchain incentive problems, it
can be used by SquirRL to achieve near-optimal performance.
To motivate our procedure, we want to design features that
are descriptive enough to ﬁnd attacks on protocols. Recall
that in an MDP, the value function V : S → R is deﬁned as
V (s) := E[∑∞
is the optimal
t (st ,st+1)|s0 = s] where a∗
t=0 ηtRa∗
t
action at time step t wrt the entire process starting from time
0 [67]. We want ϕ such that infg Es∼µ[(V (s)− g(ϕ(s)))2] < ε
for some small ε ≥ 0, where µ is some distribution over initial
states. Deﬁne Wϕ(s) := {s(cid:48) : ϕ(s(cid:48)) = ϕ(s)}. If g(ϕ(s(cid:48))) = V (s)
for all s(cid:48) ∈ W (s), then it is clear that ε = 0. This suggests a
procedure for choosing ϕ: choose it so that states s that map
to the same value of ϕ(s) have the same V (s) (or close to the
same value). In other words, we want to ﬁnd the features of s,
ϕ(s) that determine V (s) relatively closely. Then our optimal
value approximator ˆg would simply be Vϕ−1, where the inverse
maps to any state in the pre-image of ϕ.
Once we deﬁne a suitable ϕ, we can either rely on DQNs
to learn ˆg directly (which in turn gives a policy), or we can
use policy gradient methods to directly learn a strategy that
operates on the features ϕ(s).
We assume that some subset of blocks in each state s belongs
to the agent. In most blockchains, the value of the state V (s)
is determined by a few properties: (1) score: how likely is a
protocol to choose the agent’s subset as the canonical one?
(2) instantaneous reward: how much reward would the agent
receive if its subset is chosen? (3) permitted actions: what
actions are allowed in a given state? Note that to estimate the
value function, an agent should track these quantities both for
its own blocks and for the visible blocks of the other players.
1) Score: Given a blockDAG T , the score of a connected
subgraph C ⊆ T determines C’s chance of being selected as
the canonical subgraph, i.e., the subgraph on which honest
nodes build. The score of subgraph C can also depend
on observable environmental variables E = (e1, . . . ,em). For
example, these could represent an agent’s level of resource
allocation. Hence, we deﬁne score function L(C,T,E) as a
mapping from (C,T,E) to a r-dimensional vector. We propose
to let ϕ(s)[1 : r] = L(C,T,E), where we use array notation to
show that the ﬁrst r dimensions of the features are the score.
Example (Bitcoin, Ethereum, Fruitchains): In these protocols,
the fork with the longest length is adopted as the canonical
chain. So L(C,T,E) = [len(C),H] where len(C) is the length
from the last globally accepted block and the block that the
agent is currently mining on, and H is the length of the longest
public chain from the last globally-accepted block.
Example (GHOST [64]): GHOST chooses the heaviest subtree
at each node to arrive at a canonical chain. Let G be the
set of nodes that GHOST would traverse by choosing the
heaviest weighted subtree at each node (starting from the last
globally accepted block), assuming that all blocks were known
to GHOST, including any of the agent’s unpublished blocks.
Then we can let L(C,T,E) = [|G∩C|,|G|].
Example (Casper FFG + Ethereum): Two things impact
the likelihood of a chain becoming the canonical chain in
this protocol: its length, and the proportion of the votes that
it has received. So L(C,T,E) = [len(C),H,v] where v is the
proportion of votes that the agent has received so far for a
checkpoint on its chain.
2) Instantaneous reward: Blockchain protocols vary in
block reward allocation schemes, ranging from rewarding only
longest-chain blocks (e.g., Bitcoin, GHOST), to uncle-based
rewards (e.g., Ethereum [78], Fruitchains [53]). These reward
mechanisms directly impact the values of a state, so they must
also be part of feature extraction.
One commonality among block reward schemes is that
they are awarded on a per-unit basis for some unit (blocks,
fruits, uncles, etc.). Deﬁne U(s) as the set of units that are
relevant to the agent in the current state. Then deﬁne the
features of these units (e.g. how recently they were mined,
their reward content, etc) as feat(U(s)), where the dimension of
feat(U(s)) is r(cid:48) ∈ N. We propose to include these features in our
feature mapping ϕ. Concretely, we let ϕ(s)[1 : r] = L(C,T,E)
and ϕ(s)[r + 1 : r + r(cid:48) + 1] = feat(U(s)).
In general, this can give a high-dimensional feature mapping,
which will cause training to be hard. However, in many
protocols, feat(U(s)) can be very low-dimensional, as we
demonstrate in the following example.
Example (Bitcoin): Block rewards in Bitcoin are awarded
equally to every block on the canonical chain. Therefore,
for a selﬁsh miner in the single-agent setting, feat(U(s)) =
[∑B∈C 1{B /∈ M}] = [len(C)], where M is the established canon-
ical chain and B is a block.
3) Permitted actions: From a given state s, an agent may
only be allowed to take a subset of actions in A, which inﬂuences
s’s value. For instance, in [56], an agent can only match (i.e.
cause a tie between two longest chains) if it already had an
unpublished block at the same height as a block the honest
party publishes.
We deﬁne permitted actions as a binary array act(s) ∈
{0,1}|A| where |A| is the size of the action space. act(s)[i] = 1
if the ith action is available at state s and 0 otherwise. We
propose to have ϕ(s)[r +r(cid:48) +2 : r +r(cid:48) +2 +|A|] = act(s). While
|A| is usually small, it can sometimes be compressed.
Example (Bitcoin): With the score features from both the
honest party and the strategic agent, as well as observed
environmental variables, we can completely determine the
permitted actions of a state. Deﬁne an environmental variable
f ork ∈ {relevant, irrelevant, active}, which describes
the most recent event in the system [19], [56]. f ork = relevant
if an honest node just mined a block. f ork = active if the
agent just tried to match another party’s block; in this case,
recall that the environment chooses the agent’s block over
the honest one w.p. γ. Otherwise, f ork = irrelevant. [56]
showed that these three features are sufﬁcient for determining
the permitted actions of a state. Because we are already storing
scores, we only need one additional feature, f ork, in place of
the permitted actions array.
The described Bitcoin feature mapping matches the feature
mapping in [56] and was shown in [56] to achieve ε = 0. In
other words, policy iteration was able to ﬁnd a g such that
g(ϕ(s)) = V (s) for all s. Therefore, in this case, our procedure
found a sufﬁciently descriptive feature mapping. We provide an
instantiation for Ethereum in Appendix B, and demonstrate the
features’ sufﬁciency empirically by surpassing state-of-the-art
selﬁsh mining rewards for Ethereum.
In all of our feature extractor instantiations, we implement
a block limit to bound the maximum block sequence length.
Deﬁnition 1. (Block limit of B) We say that we run an
experiment with a block limit of B if the following is true:
6
Attack type
Selﬁsh mining (Bitcoin)
Selﬁsh mining (Ethereum)
Selﬁsh mining + voting (Casper FFG)
Block withholding (Bitcoin)
Feature space
[2]× [B]2
[2]6 × [B]2
[2]× [2]× [B]3 ×R×R
0
Action space
[3]
[3]
[5]
R
TABLE II: Dimensionality of the feature and action spaces. B
is the block limit, from Deﬁnition 1. Let [x] (cid:44) {0, . . . ,x}, and
R the real numbers.
When the length of the longest public chain from the last
global consensus block is ≥ B, or the length of the agent’s
private chain from the last global consensus block is ≥ B,
then the attacker must publish its private chain or adopt the
longest public chain. The attacker publishes its private chain
if its private chain is longer than the longest public chain, and
adopts the longest public chain otherwise.
Table II gives the feature space and action space dimen-
sionality for the attacks studied in this paper. These are derived
from the feature space extraction functions in Section IV-D.
Note that more complex attacks like the fork-after-withholding
(FAW) attack [35] have comparable dimensionalities for the
feature and action spaces. The FAW attack can be described
as selﬁsh mining, except the agent may have p + 1 separate
private chains (one for each pool), and blocks mined as part of
a pool have their rewards shared with the pool. In light of this,
it’s clear that the feature space will include an extra [B]p factor,
since the attacker must keep track of its private chains for each
pool. Next, the agent also faces selﬁsh mining decisions for
each pool, as well as decisions on how much hash power to
inﬁltrate each pool, which gives the action space size 4p×|R|p.
As p is usually a small constant ≤ 3 [35], this is unlikely to
signiﬁcantly affect feasibility.
Note: The above modeling process requires domain knowl-
edge to specify feasible classes of attacks; this may limit the
generality of the approach. However, (a) within an attack class,
SquirRL can help identify attack policies, which is difﬁcult in
general, and (b) once we identify a feasible set of attack classes,
composing different types of attacks becomes easy (for instance,
selﬁsh mining composed with voting attacks on Casper FFG,
which we detail in Section VII-A). This approach subsumes
the attack-by-attack approach typically employed today.
Operationally, those searching for new attack classes will
have to deﬁne a new feature extraction function. The more
carefully one deﬁnes this function, the smaller of a search space
the RL agent will have to explore, and the more performant
the agent will be. Hence, there is a tradeoff between feature
compactness and completeness for the RL agent.
E. Order of operations
An important adjunct to design of the action space and
feature extraction is a policy for sequencing agents’ actions
and disseminating rewards. In our environment, we enforce
three properties that are motivated by our problem domain:
(1) Synchronous action selection. We assume all actions of
strategic agents are recorded synchronously, after seeing the
actions of the honest party (if it exists). This is needed to
prevent the adversary from observing the actions of other
strategic players and reacting accordingly; in this case, the
last strategic agent to choose its action would have an unfair
advantage. However, we do allow for a rushing adversary who
sees the honest party’s actions before deciding how to act,
consistent with [45], [56]. In Section VI-B, we illustrate how
this assumption breaks down in the multi-agent setting.
(2) Delayed execution of passive behavior. Once actions are
recorded, we must apply them in some order. We have chosen
always to apply the "passive" action last. To see why, consider
the following scenario: In Bitcoin, suppose an honest agent is
mining on the public chain in the presence of two strategic
players, Alice and Bob. Now suppose Alice overrides and Bob
adopts (i.e., takes the passive action). We know that actions
are collected synchronously, but if they were also processed
synchronously, Alice’s chain would become the main chain,
while Bob would have adopted the previous main chain, which
is now stale. This behavior is unrealistic because a strategic
Bob would choose to mine on Alice’s override block if it were
to abandon its private chain.
(3) Delayed multi-agent rewards. In the multi-agent setting,
rewards should not be immediately allocated following an
override or adopt action. Unlike the single-agent setting, there is
still a possibility of the strategic party’s chain being overridden
by another strategic party. Hence, we only allocate a block
reward if all agents acknowledge the block, i.e., adopt it.
F. RL algorithm
We employ different deep reinforcement learning algorithms,
depending on the adversarial model. In the single agent setting,
we use Deep Dueling Q-Networks (DDQN) [70] and in the
multiple agent setting, we use Proximal Policy Optimization
(PPO) [57]. In our experiments, we have found that DDQN
converges faster than PPO in the single agent setting for Bitcoin:
with a block limit of 5, α = 0.4, γ = 0, DDQN converges in
roughly 105 steps in the environment, while PPO takes an order
of magnitude more steps to converge.
However, DDQN can fail when there are multiple adaptive
agents because the Markov assumption no longer holds. Al-
though PPO is not immune to this problem, it has been used
successfully for multi-agent games [6], [7], [52], and we found
it to be more stable than the alternatives in the multi-agent
setting.
G. Implementation
We used OpenAI Gym [10] to construct our environments
and execute RL algorithms on them. OpenAI Gym provides a
generic interface for implementing environments. In our case,
this environment speciﬁes a model for the target incentive
mechanism M . The environments we have implemented
provide a template for users to easily instantiate their own
blockchain protocols (namely, blockchain structures ranging
from pure chains to generic DAGs). We use the RLLib [39]
training interface to train our agents on state-of-the-art RL al-
gorithms and list the relevant hyperparameters for the following
experiments. These experiments were run on 20 computational
nodes, each equipped with a Quadro P2000 GPU. Similar nodes
cost $0.52/hr to rent on Amazon Web Services in 2020, at the
time of writing [1].1
1Our implementation can be found at https://github.com/wuwuz/SquirRL.
7