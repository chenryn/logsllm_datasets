## PostgreSQL 分析型SQL优化case一例 - 分析业务逻辑,分区,dblink异步并行  
### 作者                                                                        
digoal                                                                                                                 
### 日期                                                                                                                 
2020-02-15                                                                                                            
### 标签                                                                                                                 
PostgreSQL , 分析型sql , dblink 异步调用 , 分区 , 优化器 , 业务逻辑 , 子查询   
----               
## 背景      
在一些分析系统中, 很常见的一种需求, 任意维度的分析, sql不好优化, sql通常也是程序拼接而成, 而且程序不太会对sql进行语法本身的优化, 所以可能会有大量的嵌套或子查询出现.  
针对这个case, 本文进行了一系列sql语义的优化, 包括业务逻辑的分析, sql执行计划的分析, 异步并行的方法.   
同时对此case, 给内核提出了一系列的优化方向.   
## 某新零售的分析系统分析case  
订单表, 包括用户的下单记录, 商家ID, 下单时间, 订单价格等, 大小如下:   
```  
                       List of relations  
  Schema   |  Name   | Type  |   Owner   | Size  | Description   
-----------+---------+-------+-----------+-------+-------------  
 bi | t_order | table | digoal | 28 GB |   
(1 row)  
```  
问题sql如下:   
```  
select count(buyer_id) num, diff_days, CASE  WHEN payment >= 0 and payment = 50 and payment = 100 and payment = 200  THEN '[200,)'  END   as buyer_payment   
from(    
select sum(payment) payment, diff_days, buyer_id   
from(    
select buyer_id, sum(payment) payment,  CASE  WHEN days >= 0 and days = 30 and days = 60 and days = 120  THEN '[120,)'  END   as diff_days    
from(    
select buyer_id, payment, (now()::date-date_last_pay_time::date) days   
from (    
select o.buyer_id, sum(o.payment) payment, count(distinct o.date_pay_time) purchase_times , max(o.date_pay_time) date_last_pay_time    
from bi.t_order o    
where o.status in (1,4,5,6,7,9)  AND o.sellerId= xxxxxxxxxxxx and o.date_pay_time is not null  
group by o.buyer_id   
) as extractdata   
where purchase_times >= 1   and purchase_times = '0'::double precision) AND (groupdata.payment = '50'::double precision) AND (groupdata.payment = '100'::double precision) AND (groupdata.payment = '200'::double precision) THEN '[200,)'::text ELSE NULL::text END)  
   ->  Sort  (cost=12946623.28..12946623.78 rows=200 width=72)  
         Sort Key: groupdata.diff_days, (CASE WHEN ((groupdata.payment >= '0'::double precision) AND (groupdata.payment = '50'::double precision) AND (groupdata.payment = '100'::double precision) AND (groupdata.payment = '200'::double precision) THEN '[200,)'::text ELSE NULL::text END)  
         ->  Subquery Scan on groupdata  (cost=12946572.84..12946615.64 rows=200 width=72)  
               ->  GroupAggregate  (cost=12946572.84..12946610.14 rows=200 width=48)  
                     Group Key: (CASE WHEN ((((now())::date - (extractdata.date_last_pay_time)::date) >= 0) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 30) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 60) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 120) THEN '[120,)'::text ELSE NULL::text END), extractdata.buyer_id  
                     ->  GroupAggregate  (cost=12946572.84..12946604.64 rows=200 width=48)  
                           Group Key: (CASE WHEN ((((now())::date - (extractdata.date_last_pay_time)::date) >= 0) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 30) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 60) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 120) THEN '[120,)'::text ELSE NULL::text END), extractdata.buyer_id  
                           ->  Sort  (cost=12946572.84..12946575.91 rows=1230 width=48)  
                                 Sort Key: (CASE WHEN ((((now())::date - (extractdata.date_last_pay_time)::date) >= 0) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 30) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 60) AND (((now())::date - (extractdata.date_last_pay_time)::date) = 120) THEN '[120,)'::text ELSE NULL::text END), extractdata.buyer_id  
                                 ->  Subquery Scan on extractdata  (cost=11986707.68..12946509.71 rows=1230 width=48)  
                                       ->  GroupAggregate  (cost=11986707.68..12946389.79 rows=1230 width=32)  
                                             Group Key: o.buyer_id  
                                             Filter: ((count(DISTINCT o.date_pay_time) >= 1) AND (count(DISTINCT o.date_pay_time)   Sort  (cost=11986707.68..12146039.66 rows=63732791 width=24)  
                                                   Sort Key: o.buyer_id  
                                                   ->  Gather  (cost=0.00..3725174.96 rows=63732791 width=24)  
                                                         Workers Planned: 32  
                                                         ->  Parallel Seq Scan on t_order o  (cost=0.00..3725174.96 rows=1991650 width=24)  
                                                               Filter: ((sellerid = 'xxxxxxxxxxxx'::bigint) AND (status = ANY ('{1,4,5,6,7,9}'::integer[])))  
 JIT:  
   Functions: 25  
   Options: Inlining true, Optimization true, Expressions true, Deforming true  
(24 rows)  
```  
查询结果和耗时  
```  
   num    | diff_days | buyer_payment   
----------+-----------+---------------  
    51692 | [0,30)    | [0,50)  
    13022 | [0,30)    | [100,200)  
     3560 | [0,30)    | [200,)  
    50216 | [0,30)    | [50,100)  
 11315785 | [120,)    | [0,50)  
  2541471 | [120,)    | [100,200)  
   711984 | [120,)    | [200,)  
  6957021 | [120,)    | [50,100)  
       17 | [120,)    |   
   128596 | [30,60)   | [0,50)  
    46335 | [30,60)   | [100,200)  
    15399 | [30,60)   | [200,)  
   133719 | [30,60)   | [50,100)  
   372008 | [60,120)  | [0,50)  
   134772 | [60,120)  | [100,200)  
    39495 | [60,120)  | [200,)  
   356866 | [60,120)  | [50,100)  
       15 | [60,120)  |   
(18 rows)  
Time: 118562.607 ms (01:58.563)  
```  
分析:  
1、没必要用索引, 优化器也知道, 过滤不好, 大多数都是符合条件的 7000多万.  
2、并行仅用来做scan和条件过滤, 过滤后还是有7000多万行, 上面的Node 很多, 而且全部都没有用并行, 只有最后一个node才把结果收敛成几行, 中间基本上都是7000万记录在node之间传递.   
瓶颈:   
大量node未并行, memcpy多.   
### 优化思路1:  
分析业务逻辑, 减少subquery的嵌套层次, 从而减少node的层次, 从而提升性能.   
业务逻辑如下:  
```  
status in (1,4,5,6,7,9)  AND sellerId= xxxxxxxxxxxx and date_pay_time is not null 的订单:   
付款1次(所有 date_pay_time 相同) 的 用户的人群在以下两个维度的分布:   
最后一次付款时间距离现在天数, 区间分布   
总付款金额, 区间分布   
```  
了解业务逻辑后, 我们可以直接改写sql如下, 用window窗口函数得到付款1次(所有 date_pay_time 相同) 的订单记录, 然后按区间聚合.  
```  
select   
count(distinct buyer_id) num,   
CASE  WHEN days >= 0 and days = 30 and days = 60 and days = 120  THEN '[120,)'  END   as diff_days ,   
CASE  WHEN payment >= 0 and payment = 50 and payment = 100 and payment = 200  THEN '[200,)'  END   as buyer_payment   
from (  
select buyer_id,   
min(date_pay_time) over w as mmin,   
max(date_pay_time) over w as mmax,   
sum(payment) over w as payment,   
date(now()) - date(date_pay_time) as days   
from   
bi.t_order o  
where o.status in (1,4,5,6,7,9)  AND o.sellerId= xxxxxxxxxxxx and o.date_pay_time is not null  
window w as (partition by buyer_id)   
) t  
where mmin = mmax   
group by diff_days, buyer_payment   
order by diff_days, buyer_payment;  
```  
从执行计划可以看到, 嵌套降低了.  
```  
 GroupAggregate  (cost=13688617.64..13693271.69 rows=39969 width=72)  
   Group Key: (CASE WHEN ((t.days >= 0) AND (t.days = 30) AND (t.days = 60) AND (t.days = 120) THEN '[120,)'::text ELSE NULL::text END), (CASE WHEN ((t.payment >= '0'::double precision) AND (t.payment = '50'::double precision) AND (t.payment = '100'::double precision) AND (t.payment = '200'::double precision) THEN '[200,)'::text ELSE NULL::text END)  
   ->  Sort  (cost=13688617.64..13689331.50 rows=285544 width=72)  
         Sort Key: (CASE WHEN ((t.days >= 0) AND (t.days = 30) AND (t.days = 60) AND (t.days = 120) THEN '[120,)'::text ELSE NULL::text END), (CASE WHEN ((t.payment >= '0'::double precision) AND (t.payment = '50'::double precision) AND (t.payment = '100'::double precision) AND (t.payment = '200'::double precision) THEN '[200,)'::text ELSE NULL::text END)  
         ->  Subquery Scan on t  (cost=11082851.18..13662742.57 rows=285544 width=72)  
               Filter: (t.mmin = t.mmax)  
               ->  WindowAgg  (cost=11082851.18..12938888.15 rows=57108830 width=36)  
                     ->  Sort  (cost=11082851.18..11225623.25 rows=57108830 width=24)  
                           Sort Key: o.buyer_id  
                           ->  Gather  (cost=0.00..3725174.96 rows=57108830 width=24)  
                                 Workers Planned: 32  
                                 ->  Parallel Seq Scan on t_order o  (cost=0.00..3725174.96 rows=1784651 width=24)  
                                       Filter: ((date_pay_time IS NOT NULL) AND (sellerid = 'xxxxxxxxxxxx'::bigint) AND (status = ANY ('{1,4,5,6,7,9}'::integer[])))  
 JIT:  
   Functions: 20  
   Options: Inlining true, Optimization true, Expressions true, Deforming true  
(16 rows)  
```  
查询结果和耗时:  
```  
   num    | diff_days | buyer_payment   
----------+-----------+---------------  
    51692 | [0,30)    | [0,50)  
    13022 | [0,30)    | [100,200)  
     3560 | [0,30)    | [200,)  
    50216 | [0,30)    | [50,100)  
 11315785 | [120,)    | [0,50)  
  2541471 | [120,)    | [100,200)  
   711984 | [120,)    | [200,)  
  6957021 | [120,)    | [50,100)  
       17 | [120,)    |   
   128596 | [30,60)   | [0,50)  
    46335 | [30,60)   | [100,200)  
    15399 | [30,60)   | [200,)  
   133719 | [30,60)   | [50,100)  
   372008 | [60,120)  | [0,50)  
   134772 | [60,120)  | [100,200)  
    39495 | [60,120)  | [200,)  
   356866 | [60,120)  | [50,100)  
       15 | [60,120)  |   
(18 rows)  
Time: 89316.965 ms (01:29.317)  
```  
查询时间从118秒降低到89秒, 优化效果不明显.  
### 优化方法2, 允许一定的结果差异  
从逻辑来看, 实际上用户要统计的是不同区间的唯一buyer id个数, 所以可以按buyer_id分区, 每个分区单独计算, 最后再进行汇总. 这个本身应该是数据库优化器应该支持的, 类似mpp的执行形态.   