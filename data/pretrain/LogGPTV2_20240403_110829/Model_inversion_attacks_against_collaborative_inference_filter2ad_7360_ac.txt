the input sample by querying the Inverse-Network.
First, the adversary generates a bag of samples X = (x1, x2,
..., xm ) to query the target system, and observes the corre-
sponding intermediate outputs V = ( fθ1(x1), fθ1(x2), ..., fθ1(xm ))
(Lines 10-17 in Algorithm 2). We consider three cases for se-
lecting X: (1) the adversary has access to the original dataset
used for training fθ , and adopts it as X; (2) the adversary
does not have the original training set. Instead, he has a dif-
ferent set following the same distribution; (3) the adversary
has neither the original dataset or its distribution. He has
to randomly generate some samples. In our experiment, we
generate pure noise sampled from the standard Gaussian
distribution (zeros mean, unit variance) to form X.
Next, the adversary can directly train an Inverse-Network
−1
θ1 using V as the training input and X as the training target
f
(Lines 19-29). We initialize the Inverse-Network with Xavier
initialization [13]. We leverage l2 norm in the pixel space as
the loss function (Equation 4), and stochastic gradient de-
scent (SGD) to train the Inverse-Network. It is worth noting
that the architecture of the Inverse-Network is not necessar-
ily related to the target model fθ1. In our experiment, we use
an entirely different architecture.
−1
θ1 = arдminд
f
1
m
||д( fθ1(xi )) − xi )||2
(4)
m(cid:88)
i =1
ˆx0 = Inverse(д, fθ1(x0))
Algorithm 2 Black-box model inversion attack
1: Function BlackBoxAttack(fθ1, fθ1(x0))
2: /* fθ1: the target model */
3: /* x0: the target sensitive input to recover */
4: /* fθ1(x0): the intermediate layer output */
5: X=GenerateTrainingSet()
6: д=TrainInverseNet(X, fθ1)
7:
8: return ˆx0
9:
10: Function GenerateTrainingSet()
11: if known training set then
12:
13: else if Known training distribution then
14:
15: else
16:
17: return X
18:
19: Function TrainInverseNet(X, fθ1)
20: /* k: BatchSize */
21: /* ϵ: StepSize */
22: д(0)=Init()
23: while (t < T ) do
24:
25:
X=data.TrainingSet
X= NewSet ∼ data.TrainingSet
X=GaussianNoise
(cid:80)k
i =1 ||д( fθ1(xi )) − xi||2
Randomly sample x1, x2...xk from X
L(д(t )= 1
k
д(t +1) = д(t ) − ϵ ∗ ∂L(д (t ) )
∂д (t )
t+=1
2
26:
27:
28: end while
29: return д(T )
30:
31: Function Inverse(д, fθ1(x0))
32:
33: return ˆx0
ˆx0=д( fθ1(x0))
Once the Inverse-Network f
−1
θ1 is obtained, the adversary
can recover any inference sample from the intermediate-
−1
level value: x = f
θ1 (v). This approach is more efficient than
rMLE: (1) for each target sample, the adversary only needs
to pass through the Inverse-Network once, while in rMLE,
an iterative process is required to solve the optimization
problem; (2) calculating the inversed input is parameter-free,
while rMLE requires tuning the parameters (λ, β in Eq. 3).
5.2 Evaluation
Figures 4, 5 and 6 show the recovered images of two datasets
under three different circumstances. Table 4 shows the PSNR
and SSIM metrics of these attacks. From these recovery re-
sults, we draw some conclusions.
First, the adversary can recover the input with black-box
access for most cases. The quality of the recovered images
in MNIST is very high when the split point is in conv1 or
ReLU2. For CIFAR10, the recovered images still maintain high
quality when the split point is in a shallow layer (conv11).
They become relatively vague and lose certain details when
the split point is in a deeper layer, e.g. layer ReLU32 for the
CIFAR10 dataset.
Second, we observe that there is no significant difference
between the cases where the adversary uses the same train-
ing set, or a different set with the same distribution to train
the Inverse-Network. For MNIST, the attack with a different
set are even slightly better than the ones with the same set.
However, when the adversary does not know the training
data distribution, and adopts randomly generated samples,
the attack effects drop significantly. This is especially promi-
nent in the case of the CIFAR10 split in the ReLU22 layer. We
conclude that the knowledge of the training data distribution
is important to recover samples from deep layers.
(a) MNIST.
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
Figure 4: Recovered inputs in black-box attacks (same
training data)
6 QUERY-FREE ATTACKS
The Inverse-Network approach requires the adversary to
be able to query the target model, to generate the data set
for training f
. In this section, we consider the query-free
setting, where the adversary cannot query the system, and
does not know the client-side model information. The basic
idea is that the adversary first reconstructs a shadow model,
which can imitate the target model’s behavior, and then uses
−1
θ
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
Figure 5: Recovered inputs in black-box attacks (dif-
ferent training data, same distribution).
(a) MNIST.
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
Figure 6: Recovered inputs in black-box attacks (dif-
ferent distributions).
rMLE over this shadow model to recover the sensitive input
samples.
6.1 Shadow Model Reconstruction
The problem in our consideration is: how can the adversary
reconstruct a shadow model of the former model layers, fθ1,
Table 4: PSNR (db) and SSIM for black-box attacks
(a) MNIST.
Dataset
same set
same dist
rand set
same set
same dist
rand set
MNIST
conv1 ReLU2
39.64
20.35
20.81
40.72
7.72
14.76
0.7334
0.9887
0.8046
0.9950
0.7188
0.4310
PSNR
SSIM
CIFAR10
conv11 ReLU22 ReLU32
49.88
49.02
48.59
0.9993
0.9992
0.9996
19.81
19.36
12.79
0.6939
0.6802
0.2930
15.42
13.95
12.37
0.3124
0.2196
0.0440
only with the knowledge of the latter layers fθ2 and a dataset
S drawn from the same distribution as the original training
set? He cannot query the model with specified samples to
get the intermediate values.
The key insight of our approach is that, if the shadow
′
θ1, it should be able to classify
model is reconstructed as f
the input with high accuracy when combined with the later
layers fθ2:
θ1(xi )), for (xi , yi ) ∈ S
′
yi ∼ fθ2( fθ1(xi )) ∼ fθ2( f
(5)
Then the task of model reconstruction can be translated
into minimizing the classification error of the composition
′
of the two models: fθ2( f
θ1(xi )) versus yi. Equation 6 shows
the loss function for training the model, where m is the
number of samples in S, CrossEntropy is the cross-entropy
′
θ1 is
loss. Equivalently, this means the training process of f
′
supervised at the output layer of fθ2. Once the model f
θ1 is
reconstructed, the adversary can perform model inversion
attacks using the rMLE technique in Section 4.
′
θ1 = arдminд
f
1
m
m(cid:88)
i =1
CrossEntropy( fθ2(д(xi )), yi )
(6)
Algorithm 3 describes the query-free attacks. There are
two phases: ① offline shadow model reconstruction (Lines
10-21) and ② online model inversion (Line 7). The shadow
model reconstruction only needs to be performed once. Then
all the input samples can be recovered using the same shadow
model, by inferencing only once for each input .
In the shadow model reconstruction phase, a training set
and an initial network are required (Lines 13-14). We con-
sider four cases with different adversary’s capabilities in two
dimensions, i.e. training set and network structure: (1) the
adversary’s dataset S is the same as the original set used
for training fθ . He also knows the network structure of fθ1;
(2) the adversary has a different dataset S from the original
training set, but follows the same distribution. This assump-
tion is reasonable, because there exist various public datasets
for different tasks. He knows the network structure of fθ1;
(3) the adversary has the same training set. But he does not
know the structure of fθ1. He has to use an alternative one
Algorithm 3 Query-free model inversion attack
1: Function QueryFreeAttack(fθ1, fθ2, fθ1(x0))
2: /* fθ1: the target model */
3: /* fθ2: the known model */
4: /* x0: the target sensitive input to recover */
5: /* fθ1(x0) the intermediate layer output */
ˆfθ1=ModelReconstruction(S, fθ2)
6:
ˆx0 = WhiteboxAttack( ˆfθ1, fθ1(x0), T , λ, ϵ)
7:
8: return ˆx0
9:
10: Function ModelReconstruction(fθ2)
11: /* k: BatchSize */
12: /* ϵ: StepSize */
13: S=GenerateTrainingSet()
14: д0=InitArchitecture()
15: while (t < T ) do
16:
(cid:80)k
Randomly sample x1, x2...xk and labels y1, y2...yk
i =1 yi ( fθ2(дt (xi )) + (1−yi )(1− fθ2(дt (xi ))
L(дt )= 1
k
д(t +1) = дt − ϵ ∗ ∂L(дt )
∂дt
t+=1
from S
17:
18:
19:
20: end while
21: return д(T )
for the shadow model. We assume that both the target model
and the shadow model are convolutional neural networks,
but with different numbers of layers and filters, as well as
filter sizes. Table 5 shows the network structure configura-
tions used in our experiments; (4) the adversary does not
know the training set nor the network structure.
After the training set and network structure are deter-
mined, the adversary can adopt SGD to optimize the loss
function of the composition of the two models. We choose
the cross-entropy loss because it performs well on image
classification tasks. Other loss functions can be leveraged, if
the adversary aims to find inverses of the DNN for different
tasks. Once the shadow model is obtained, the adversary can
use rMLE (Algorithm 1) to recover the inputs.
Table 5: Neural network configurations for query-free
attacks.
Dataset
MNIST
CIFAR10
Target Model
Layer
One 5X5 conv layer