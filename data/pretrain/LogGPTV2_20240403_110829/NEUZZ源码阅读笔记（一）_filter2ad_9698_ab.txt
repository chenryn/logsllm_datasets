    # shuffle training samples
    seed_list = glob.glob('./seeds/*')
    seed_list.sort()
    SPLIT_RATIO = len(seed_list)
    rand_index = np.arange(SPLIT_RATIO)
    np.random.shuffle(seed_list)
    new_seeds = glob.glob('./seeds/id_*')
    call = subprocess.check_output
然后程序开始获取最大的种子文件的大小，大致就是调用了`ls`指令的方法
    # get MAX_FILE_SIZE
    cwd = os.getcwd()
    max_file_name = call(['ls', '-S', cwd + '/seeds/']).decode('utf8').split('\n')[0].rstrip('\n')
    MAX_FILE_SIZE = os.path.getsize(cwd + '/seeds/' + max_file_name)
然后就是建立四个文件夹
     # create directories to save label, spliced seeds, variant length seeds, crashes and mutated seeds.
    os.path.isdir("./bitmaps/") or os.makedirs("./bitmaps")
    os.path.isdir("./splice_seeds/") or os.makedirs("./splice_seeds")
    os.path.isdir("./vari_seeds/") or os.makedirs("./vari_seeds")
    os.path.isdir("./crashes/") or os.makedirs("./crashes")
  * bitmaps：用来存放每个种子文件的路径覆盖率信息
  * splice_seeds：用来存放被切割的种子
  * vari_seeds：用来存放变长的种子
  * crashes：用来存放产生crashes的种子
然后就是利用afl-showmap获取每一个seed的原始输出路径覆盖率信息
    # obtain raw bitmaps
    raw_bitmap = {}
    tmp_cnt = []
    out = ''
    for f in seed_list:
        tmp_list = []
        try:
            # append "-o tmp_file" to strip's arguments to avoid tampering tested binary.
            if argvv[0] == './strip':
                out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + argvv + [f] + ['-o', 'tmp_file'])
            else:
                out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + argvv + [f])
        except subprocess.CalledProcessError:
            print("find a crash")
        for line in out.splitlines():
            edge = line.split(b':')[0]
            tmp_cnt.append(edge)
            tmp_list.append(edge)
        raw_bitmap[f] = tmp_list
    counter = Counter(tmp_cnt).most_common()
我们先来回顾一下`afl-showmap`的用法
    afl-showmap [ options ] -- /path/to/target_app [ ... ]
    Required parameters:
      -o file       - file to write the trace data to
    Execution control settings:
      -t msec       - timeout for each run (none)
      -m megs       - memory limit for child process (0 MB)
      -Q            - use binary-only instrumentation (QEMU mode)
      -U            - use Unicorn-based instrumentation (Unicorn mode)
      -W            - use qemu-based instrumentation with Wine (Wine mode)
                      (Not necessary, here for consistency with other afl-* tools)
    Other settings:
      -i dir        - process all files in this directory, must be combined with -o.
                      With -C, -o is a file, without -C it must be a directory
                      and each bitmap will be written there individually.
      -C            - collect coverage, writes all edges to -o and gives a summary
                      Must be combined with -i.
      -q            - sink program's output and don't show messages
      -e            - show edge coverage only, ignore hit counts
      -r            - show real tuple values instead of AFL filter values
      -s            - do not classify the map
      -c            - allow core dumps
    This tool displays raw tuple data captured by AFL instrumentation.
    For additional help, consult /usr/local/share/doc/afl/README.md.
    Environment variables used:
    LD_BIND_LAZY: do not set LD_BIND_NOW env var for target
    AFL_CMIN_CRASHES_ONLY: (cmin_mode) only write tuples for crashing inputs
    AFL_CMIN_ALLOW_ANY: (cmin_mode) write tuples for crashing inputs also
    AFL_CRASH_EXITCODE: optional child exit code to be interpreted as crash
    AFL_DEBUG: enable extra developer output
    AFL_FORKSRV_INIT_TMOUT: time spent waiting for forkserver during startup (in milliseconds)
    AFL_KILL_SIGNAL: Signal ID delivered to child processes on timeout, etc. (default: SIGKILL)
    AFL_MAP_SIZE: the shared memory size for that target. must be >= the size the target was compiled for
    AFL_PRELOAD: LD_PRELOAD / DYLD_INSERT_LIBRARIES settings for target
    AFL_QUIET: do not print extra informational output
可以看到NEUZZ使用的afl-showmap的开启选项主要是：
  * -q：关闭沉程序的输出，不显示消息
  * -e：仅显示边缘覆盖率，忽略命中率
  * -o：写入覆盖率的文件路径
  * -m：对于子进程的内存限制
  * -t：超时时间
如果afl-showmap错误退出，就会说明找到了一个可以触发Crash的种子文件，然后借用`splitlines`方法，`tmp_cnt`可以得到边缘覆盖个数。`raw_bitmap[f]
= tmp_list`可以得到原始的bitmap
起始阶段 `fuzzer` 会进行一系列的准备工作，为记录插桩得到的目标程序执行路径，即 `tuple` 信息
  * `trace_bits` 记录当前的tuple信息
  * `virgin_bits` 用来记录总的tuple信息
  * `virgin_tmout` 记录fuzz过程中出现的所有目标程序的timeout时的tuple信息
  * `virgin_crash` 记录fuzz过程中出现的crash时的tuple信息
AFL为每个代码块生成一个随机数，作为其“位置”的记录；随后，对分支处的”源位置“和”目标位置“进行异或，并将异或的结果作为该分支的key，保存每个分支的执行次数。用于保存执行次数的实际上是一个哈希表，大小为`MAP_SIZE=64K`
    # save bitmaps to individual numpy label
    label = [int(f[0]) for f in counter]
    bitmap = np.zeros((len(seed_list), len(label)))
    for idx, i in enumerate(seed_list):
        tmp = raw_bitmap[i]
        for j in tmp:
            if int(j) in label:
                bitmap[idx][label.index((int(j)))] = 1
之后就是本轮训练出来的每个bitmap保存到对应的numpy标签矩阵里面，方便以后训训练
    # label dimension reduction
    fit_bitmap = np.unique(bitmap, axis=1)
    print("data dimension" + str(fit_bitmap.shape))
之后就是`unique`函数去除其中重复的元素,也就是去除会产生相同路径信息的种子，并按元素由大到小返回一个新的无元素重复的所有的bitmap
     # save training data
    MAX_BITMAP_SIZE = fit_bitmap.shape[1]
    for idx, i in enumerate(seed_list):
        file_name = "./bitmaps/" + i.split('/')[-1]
        np.save(file_name, fit_bitmap[idx])
之后就是保存对应的bitmaps
###  3.4 build_model
    def build_model():
        batch_size = 32
        num_classes = MAX_BITMAP_SIZE
        epochs = 50
        model = Sequential()
        model.add(Dense(4096, input_dim=MAX_FILE_SIZE))
        model.add(Activation('relu'))
        model.add(Dense(num_classes))
        model.add(Activation('sigmoid'))
        opt = keras.optimizers.adam(lr=0.0001)
        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[accur_1])
        model.summary()
        return model
这里就是很简单的利用Tensorflow和Keras构建的一个网络模型，其基本结构是Sequential序贯模型，序贯模型是函数式模型的简略版，为最简单的线性、从头到尾的结构顺序，不分叉，是多个网络层的线性堆叠。我们可以通过将层的列表传递给Sequential的构造函数，来创建一个Sequential模型，
也可以使用`.add()`方法将各层添加到模型中，这里采用的就是`.add()`方法
模型需要知道它所期待的输入的尺寸（shape）。出于这个原因，序贯模型中的第一层（只有第一层，因为下面的层可以自动的推断尺寸）需要接收关于其输入尺寸的信息，后面的各个层则可以自动的推导出中间数据的shape，因此不需要为每个层都指定这个参数。有以下几种方法来做到这一点
这里我们使用一个2D 层 `Dense`，通过参数 `input_dim` 指定输入尺寸，Dense层就是所谓的全连接神经网络层
之后跟的是以`relu`函数的一个激活函数层。之后又跟了一个大小为`MAX_BITMAP_SIZE`的全连接层。最后又跟了一个以`sigmoid`为函数的一个激活函数层
>
> 神经网络模型由三个完全连接的层组成。隐藏层使用ReLU作为其激活函数。我们使用sigmoid作为输出层的激活函数来预测控制流边缘是否被覆盖。神经网络模型训练了50个阶段（即整个数据集的50次完整通过），以达到较高的测试精度（平均约95%）。由于我们使用一个简单的前馈网络，因此所有10个程序的训练时间都不到2分钟
在训练模型之前，我们需要配置学习过程，这是通过compile方法完成的，他接收三个参数：
  * **优化器 optimizer：** 它可以是现有优化器的字符串标识符，如 `rmsprop` 或 `adagrad`，也可以是 Optimizer 类的实例
  * **损失函数 loss：** 模型试图最小化的目标函数。它可以是现有损失函数的字符串标识符，如 `categorical_crossentropy` 或 `mse`，也可以是一个目标函数
  * **评估标准 metrics：** 对于任何分类问题，你都希望将其设置为 `metrics = ['accuracy']`。评估标准可以是现有的标准的字符串标识符，也可以是自定义的评估标准函数
这里我们使用了优化器`keras.optimizers.Adam()`，在监督学习中我们使用梯度下降法时，学习率是一个很重要的指标，因为学习率决定了学习进程的快慢（也可以看作步幅的大小）。如果学习率过大，很可能会越过最优值，反而如果学习率过小，优化的效率可能很低，导致过长的运算时间，所以学习率对于算法性能的表现十分重要。而优化器`keras.optimizers.Adam()`是解决这个问题的一个方案。其大概的思想是开始的学习率设置为一个较大的值，然后根据次数的增多，动态的减小学习率，以实现效率和效果的兼得
我们这里使用了一个参数`lr=0.0001`表示学习率
看参数我们使用了`binary_crossentropy`函数作为损失函数，也就是二进制交叉熵
使用keras构建深度学习模型,我们会通过`model.summary()`输出模型各层的参数状况
至此我们的网络模型构建完毕
###  3.5 train
在构建完模型后，我们就开始了训练的过程
    def train(model):
        loss_history = LossHistory()
        lrate = keras.callbacks.LearningRateScheduler(step_decay)
        callbacks_list = [loss_history, lrate]
        model.fit_generator(train_generate(16),
                            steps_per_epoch=(SPLIT_RATIO / 16 + 1),
                            epochs=100,
                            verbose=1, callbacks=callbacks_list)
        # Save model and weights
        model.save_weights("hard_label.h5")
都是常规的训练设置，最后将获得的模型保存到一个叫`hard_label.h5`的文件中，`save_weights()`保存的模型结果，
**它只保存了模型的参数，但并没有保存模型的图结构** ，可以节省空间与提高效率
###  3.6 gen_mutate2
这又是整个程序中一个巨大且重要的模块，主要用途是利用生成的梯度信息指导未来的种子变异
    # grenerate gradient information to guide furture muatation
    def gen_mutate2(model, edge_num, sign):
        tmp_list = []
        # select seeds
        print("#######debug" + str(round_cnt))
        if round_cnt == 0:
            new_seed_list = seed_list
        else:
            new_seed_list = new_seeds
        if len(new_seed_list) < edge_num:
            rand_seed1 = [new_seed_list[i] for i in np.random.choice(len(new_seed_list), edge_num, replace=True)]
        else:
            rand_seed1 = [new_seed_list[i] for i in np.random.choice(len(new_seed_list), edge_num, replace=False)]
        if len(new_seed_list) < edge_num:
            rand_seed2 = [seed_list[i] for i in np.random.choice(len(seed_list), edge_num, replace=True)]
        else:
            rand_seed2 = [seed_list[i] for i in np.random.choice(len(seed_list), edge_num, replace=False)]
        # function pointer for gradient computation
        fn = gen_adv2 if sign else gen_adv3
        # select output neurons to compute gradient
        interested_indice = np.random.choice(MAX_BITMAP_SIZE, edge_num)
        layer_list = [(layer.name, layer) for layer in model.layers]
        with open('gradient_info_p', 'w') as f:
            for idxx in range(len(interested_indice[:])):
                # kears's would stall after multiple gradient compuation. Release memory and reload model to fix it.
                if idxx % 100 == 0:
                    del model
                    K.clear_session()
                    model = build_model()
                    model.load_weights('hard_label.h5')