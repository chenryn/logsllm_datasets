command can be set higher than in the ping command. There is a need to have
more frequent ping requests, as high-frequent calls to ping strengthen the LLC
proﬁle and thus decrease the number of traces needed to detect the ping requests
in LLC. Therefore, we used hping in our Amazon EC2 experiments.
4 Application Detection Results
In this section, we explain the experiment setup used to collect data and make
our scenario applicable.
4.1 Experiment Setup
For the experiments, we have used the following two setups;
– Native Environment: In this setup, the applications are running on a
native Ubuntu 14.04 version with no virtualization. The processor is a 10 core
Intel(R) Xeon(R) E5-2670 v2 CPU clocked at 2.50 GHz. The purpose of this
scenario is to run experiments in a controlled environment with minimal
noise and to show the high success rate of our methods. In addition, this
processor is the same type of processor mainly used in Amazon EC2 cloud.
– Cloud Environment: In this setup, Amazon EC2 cloud servers are used to
implement our experiments in a cross-VM scenario. In Sao Paulo region, the
processors are same with the one used in native environment with a modiﬁed
Xen hypervisor. The instance type is medium.m3 which has 1 vCPU. The
aim of this setup is to show the huge thread of our scenario in a public cloud.
In this setup, there are two co-located VMs in the same physical machine
sharing the LLC which is veriﬁed by the techniques [17].
16
To evaluate our approach on a broad range of commonly used yet diﬀerent
applications, we decided to use the Phoronix test benchmarks as sample appli-
cations for classiﬁcation [2]. We performed classiﬁcation experiments on these
applications in three diﬀerent scenarios. As baseline experiments we ﬁrst per-
formed the experiments in the above-described native scenario, both by monitor-
ing L1 cache leakages and also by monitoring LLC leakages. The former shows
the potential of L1 cache leakages if they are accessible. The latter assumes a
realistic observation scenario for any process running on the same system. Fi-
nally, we performed the same experiments on Amazon EC2 cloud to show the
feasibility in a noisy and cross-VM scenario. In this public cloud scenario, only
LLC is proﬁled to classify benchmarks since each VM has only one thread in
the core and they do not reside on the same core. For both L1 cache and LLC
experiments, our methodology is applied to 40 diﬀerent Phoronix benchmark
tests in including cryptography, gaming, compressing, SQL, apache and so on
Appendix. Last but not least, we present a scenario where we only try to detect
the presence of a single application, the ping detection described in Section 3.4.
4.2 Application Detection in Native Environment
We ﬁrst performed experiments in the native environment.
Monitoring L1 Cache In native case, ﬁrst we implemented our proﬁling on
L1 cache. There are two types of cache structure namely, data and instruction.
Therefore, in our experiments we proﬁled each of them separately. In our pro-
cessor there are SL1 = 64 sets for each L1-data and L1-instruction cache and
the sets are 8 way associative.
The proﬁling and application code run on the same core to detect misses in
L1 cache. Hence, the hyper-threading feature of Intel processors is used. Before
the training data is collected, an idle case of L1-data and L1-instruction sets are
monitored and base Probe values are recorded. For L1-data the base value is
around 65 clock cycles and for L1-instruction it is around 75 clock cycles. Hence,
the outlier threshold is chosen as τo = 150 for both data and instruction cache.
For the conversion from raw data to binary data the threshold value is τo,d = 80
for data cache and τo,d = 90 for instruction cache. The number of traces collected
per set for each data set is NT = 10, 000. Therefore, the total number of traces
is equal to 640,000 which belongs to one data set for L1-instruction or L1-data.
To compute the sampling frequency, we checked the total Prime&Probe time
and it is almost same for all sets in L1 cache which is around Tcc = 200 clock
cycle. Hence, the sampling frequency is Fs = 2.5GHz/200 = 12.5M Hz for L1
cache proﬁling. Fs for L1 cache is higher than LLC proﬁling because the number
of ways in L1 sets is smaller than LLC sets and accessing to L1 cache lines is
faster than LLC lines. Thus, the resolution of L1 proﬁling is higher than LLC
proﬁling which results more distinct feature vectors and high success rates in
ML algorithm.
After Fs is determined, FFT can be applied to traces. The outcome of FFT
is NT /2 which is equal to 5,000 frequency components in our case. This process
17
Fig. 9. Success rate graph for varying number of sets to train the data
is applied to all 64 sets in data and instruction cache for each test. Hence, the
feature vector of a test consists of 320,000 frequency components after all sets
are concatenated. The number of data sets per test is ND = 60 which means the
training data is a matrix of the size 2, 400 × 320, 000.
To classify the training data, ﬁrst 10-fold cross validation is implemented in
SVM. For cross-validation we implement both C-SVC and nu-SVC SVM types
in the LIBSVM library. Our results show that C-SVC gives better success rates,
so we preferred this option. For the kernel type, the linear option is chosen since
the success rate is much higher than for the other options. After these options
are chosen kernel parameters and the penalty parameter for error are optimized
by LIBSVM. In both training and test phases the chosen parameters are used
to implement SVM. Therefore, there is no user interaction to choose the best
parameters and the steps are automated.
In cross-validation experiments, we show the eﬀect of number of L1 sets on
success rate. If only 1 set is used to generate the training model, the cross-
validation success rate is 46.8% for instruction and 60.71% for data cache. With
the increasing number of sets, the cross-validation success rate for data and
instruction cache is increasing to 95.74% and 97.95%, respectively in Figure 9.
For training and individual success rate of test, 60 data sets per test are
trained where the SVMMODEL is obtained with C-SVC and linear kernel op-
tions. With the cross-validation technique, the success rate for instruction cache
is higher than data cache. The reason behind this is some of the Phoronix tests
do not use L1-data cache however all tests use L1-instruction cache. Therefore,
extracting the feature vectors for tests in instruction cache is more successful
than L1-data cache.
The results also show that the cross-validation success rate is 98.65% if all
information in L1 cache (both instruction and data) is used in the machine
learning algorithm. To achieve this success rate we used all 64 cache sets and in
1248163264Number of sets406080100Success rate(%)datainstruction18
Fig. 10. Success rate for diﬀerent tests in L1-data (blue) and L1-instruction (yellow).
The last bar represents the average of success rates for 40 tests
total we have 50 × 640, 000 size feature vectors per test. Therefore, the size of
training data is 2, 000 × 640, 000.
LLC Results L1 proﬁling is not realistic in the real world since the probability
of two co-located VMs in the same core is really low. Therefore, before switching
to public cloud we implemented our attack in LLC with a cross-core scenario.
The number of cores is NC = 10 in our processor and the number of set-slice
pairs solving the equation in 3 s mod 64 = o is SL3 = 25 · 10 = 320 where
NLLCB = 11 because of 2,048 LLC sets in total and the number of oﬀset bits is
No = 6. o is the set number which is the most used one in L1 proﬁling for that
test. Therefore, we have 320 set-slice pairs in total to monitor.
Before collecting data for every test, the idle case of each set is monitored to
determine the base value (τb). τb changes between 90 and 110 clock cycle among
diﬀerent set-slice pairs. Hence, for each set-slice pair τb is diﬀerent. The outlier
threshold (τo) is 250 clock cycle. The threshold value (τc) for the conversion from
raw data to binary data is τb + 15 clock cycle. After obtaining the binary data,
it is trivial to ﬁnd the noisy sets. If the number of cache misses is higher than
100 in a set-slice pair, it is marked as noisy. These noisy sets are not processed
when the data is collected.
While collecting the training data 10,000 traces are collected per set-slice
pair. The active sets are determined by checking the number of cache misses
in each set-slice pair excluding the noisy sets. If the number of cache misses is
higher than 300, then that set-slice pair is marked as active and they are included
in Fourier transform.
The Prime&Probe timings change between 1,800 and 2,200 clock cycle so the
sampling frequency (Fs) is taken 1.3 MHz. After FFT is applied to active sets,
the left symmetric side of the outcome is recorded. The length of the ﬁngerprint
Test Number0510152025303540Avg.Success Rate (%)010203040506070809010011019
Fig. 11. LLC success rate with varying number of frequency components
for a set-slice pair consists of 5,000 frequency components. If there are 6 active
sets for a test, the ﬁngerprint of each active set are combined by element-wise
in each data set and ﬁnal ﬁngerprint is obtained from one data set per test.
For LLC experiments, we used 40 diﬀerent benchmark tests to proﬁle in LLC.
The number of data set per test is 50 and the length of vector for each feature
vector is 5,000. After collecting the data the training model is generated and the
cross-validation is applied to training data.
For the cross-validation, same options in L1 proﬁling are used in SVM. The
success rate for LLC test in average is 77.65% with 5,000 frequency components.
With the decreasing number of frequency components the success rate drops to
45% in Figure 11.
The details of success rates for diﬀerent tests are presented in Figure 12 by
using 10-fold cross-validation technique. The results are obtained from 5,000
frequency components and 60 data sets per test. The lowest recognition rate is
13% for GMPBENCH test since the success rate for this test is low in L1-data
cache in Figure 10.
4.3 Application Detection on EC2 Cloud
To show the applicability of ML technique to real world, we also perform our
proﬁling method on Amazon EC2. The challenges of performing the experiments
on a public cloud are hypervisor noise and the noise of other VMs in the mon-
itored sets. Therefore, some set-slice pairs are marked as active even if those
pairs are not used by target VM. Redundant cache misses in the active sets also
pose a problem. During Fourier Transform, these cache misses may cause shifts
in frequency domain. To overcome these diﬃculties, SVM technique is applied
to the data, and as a result, the success rate gets higher.
The number of tests decreases in cross-VM scenario since some tests do not
work properly and some of them have installation problems on Amazon EC2.
Number of feature values1005001000150020002500300040005000Success Rate(%)050100NativeAmazon EC220
Fig. 12. LLC success rates for diﬀerent tests in native scenario. The blue bar represents
the success rate for diﬀerent tests. The last bar shows the average success rate for all
tests
Fig. 13. LLC success rates for diﬀerent tests in cloud scenario. The blue bar represents
the success rate for diﬀerent tests. The last bar shows the average success rate for all
25 tests
Thus, the number of tests used in this experiment decreases to 25. To classify the
diﬀerent benchmark tests, same process in LLC proﬁling is used, then training
data is processed in SVM. The result is lower than native case because of the
aforementioned types of noise. The 10-fold cross-validation result is 60.22% in
Figure 11 with 5,000 frequency components. This result shows that on public
cloud the classiﬁcation success rate drops with increasing noise.
The success rates for individual tests change between 16% and 100% in Fig-
ure 13. The success rate decreases when the hypervisor and other VMs noise
aﬀect the cache miss patterns. Even though the success rate is lower than native
Test Number0510152025303540Avg.Success Rate (%)0102030405060708090100110100401001001001009228924278386358988366688382139378789810010010010010088608072733310087937578,5Test Number012345678910111213141516171819202122232425Avg.Success Rate (%)010203040506070809010011060.2248323844100843610092824468100687822281652100883652524421
Fig. 14. Cache miss pattern of received ping requests in LLC
scenario, this result demonstrates the applicability of out method in the cloud
platform.
4.4 Ping detection on EC2
To detect the co-located VMs with spy VM, ping requests are sent by one of the
VMs controlled by the spy in the same region. The purpose of this is to decrease
RTT and increase the frequency of ping requests. At the same time, spy VM 2
monitors 320 set-slice pairs since the processor has 10 slices and 32 diﬀerent set
numbers satisfying s mod 64 = 0.
The set-slice pairs are very noisy on the cloud therefore even if the candidate
VM is not co-located with the spy VM, there are some active sets in LLC because
of the noise from other VMs. However, when the frequency domain of active sets
is checked by the spy, there is no dominant frequency component or the dominant
frequency components are not consistent with the ping frequency. If the target
VM is co-located with the spy VM, then the periodic cache misses can be seen
in one of the active sets in Figure 14.
After applying Fourier Transform with an appropriate Fs, the dominant fre-
quencies are clearly seen in Figure 15. In order to calculate the frequency domain
the sampling frequency Fs should be computed before the frequency transforma-
tion is applied to the data. After averaging all LLC sets, Fs is determined to be
around 1,800 clock cycle. The normal CPU frequency of the processor is 2.5 GHz
so Fs is equal to 1.56 MHz on Amazon EC2 VMs. When the ping requests are
sent every 0.4 ms from Spy VM2, then Spy VM1 can monitor the cache misses
in active sets as in Figure 14. When the frequency domain is generated, the
frequency components overlap with the frequency of ping requests in Figure 15.
Prime and Probe Sample0200040006000800010000Cache access00.511.5222
Fig. 15. Frequency components of ping requests in LLC
5 Countermeasures
In this section we discuss some details to prevent LLC proﬁling in the cloud.
Disabling Huge Pages: In order to create eviction sets for Prime&Probe pro-
ﬁling Huge Pages should be allocated by the spy. Therefore, if the Huge Pages
are disabled by the VMM, then creating eviction sets process becomes harder
and the eﬃciency of the monitoring other VMs drops signiﬁcantly.
Private LLC Slices: Co-located VMs can use whole LLC if they are physically
in the same machine. Therefore, if LLC is separated for each VM in the cloud,
the proﬁling phase will be impossible. Nevertheless, this change in the cache
architecture is a painful process for cloud providers and it is not eﬃcient.
Adding Noise to LLC: One way to avoid LLC proﬁling is to add noise by
ﬂushing some cache lines. Therefore, even if there is no evicted line in a LLC
set by the monitored application, the spy assumes there is a cache miss. With
additional noise the frequency domain representation of the hit-miss trace will
change and the success rate of the ML algorithms will decrease.
Page Coloring: Page coloring is a software technique that manages how mem-
ory pages are mapped to cache lines. Shi et al. [31] introduced page coloring to
partition LLC dynamically to limit cache side channel leakage in multi-tenant
clouds. Hence, spy VMs cannot interfere with other VMs in the cloud. How-
ever, this method introduces performance overheads and the performance of a
program can drastically change between runs.
6 Conclusion
In this paper we tackled the problem of automating cache attacks using machine
learning. Speciﬁcally, we devised a technique to extract features from cache ac-
cess proﬁles which subsequently are used to train a model using support vector
Frequency (Hz)×10400.511.5200.20.40.60.823
machines. The model is used later for classiﬁcation applications based on their
cache access proﬁles. This allows, for instance, a cloud instance to spy on appli-
cations co-located on the same server. Even further, our technique can be used
as a discovery phase of a vulnerable application, to be succeeded by a more
sophisticated ﬁne grain attack. We validated our models on test executions of
40 applications bundled in the Phoronix benchmark suite. Using L1 and LLC
cache access proﬁles our trained model achieves classiﬁcation rates of 98% and
78%, respectively. Even further, our model achieves a 60% (for a suite of 25
applications) in the noisy cross-VM setting on Amazon EC2.
6.1 Acknowledgments
This work is supported by the National Science Foundation, under grants CNS-
1618837 and CNS-1314770.
References
1. AWS IP Address Ranges. https://ip-ranges.amazonaws.com/ip-ranges.json.
2. Phoronix Test Suite Tests. https://openbenchmarking.org/tests/pts.
3. Aciic¸mez, O., Schindler, W., and C¸ etin K. Koc¸. Cache Based Remote Timing
Attack on the AES. In CT-RSA 2007, pp. 271–286.
4. Acıic¸mez, O. Yet Another MicroArchitectural Attack: Exploiting I-Cache.
Proceedings of the 2007 ACM Workshop on Computer Security Architecture.
In
5. Akata, Z., Perronnin, F., Harchaoui, Z., and Schmid, C. Good practice
IEEE Transactions on Pattern
in large-scale learning for image classiﬁcation.
Analysis and Machine Intelligence 36, 3 (2014), 507–520.
6. Bates, A., Mood, B., Pletcher, J., Pruse, H., Valafar, M., and Butler,
K. Detecting Co-residency with Active Traﬃc Analysis Techniques. In Proceedings
of the 2012 ACM Workshop on Cloud Computing Security Workshop.
2004.
7. Bernstein, D. J.
on AES,
Cache-timing
attacks
URL:
http://cr.yp.to/papers.html#cachetiming.
8. Bonneau, J., and Mironov, I. Cache-Collision Timing Attacks against AES. In
CHES 2006, vol. 4249 of Springer LNCS, pp. 201–215.
9. Bosman, E., Razavi, K., Bos, H., and Giuffrida, C. Dedup est machina: Mem-
ory deduplication as an advanced exploitation vector. In 2016 IEEE Symposium
on Security and Privacy (SP) (May 2016), pp. 987–1004.