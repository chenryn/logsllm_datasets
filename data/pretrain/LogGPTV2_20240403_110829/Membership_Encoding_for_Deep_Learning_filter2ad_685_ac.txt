Magnitude
DNS
VGG-16
Magnitude
FC-0
FC-1
FC-0
FC-1
CNN-1
FC-0
CNN-1
FC-0
FC-0
FC-1
Huffman
59.1× (15.6)
56.0× (2.09)
153× (5.98)
129× (0.91)
42.8× (0.84)
59.1× (39.0)
89.5× (1.09)
333× (4.70)
119× (3280)
88.4× (720)
Weightless
60.1× (15.3)
64.3× (1.82)
174× (5.27)
195× (0.60)
51.6× (0.70)
74.2× (31.1)
114.4× (0.86)
496× (3.16)
157× (2500)
85.8× (740)
1.02×
1.15×
1.13×
1.51×
1.21×
1.25×
1.28×
1.49×
1.31×
0.97×
4.2 COMPRESSING WEIGHT ENCODINGS
For sending a model over a network, an additional stage of compression can be used to optimize for
size. Han et al. (2016) propose using Huffman coding for their, and we use arithmetic coding, as
described in Section 3.2. The results in Table 3 show that while Deep Compression gets relatively
more beneﬁt from a ﬁnal compression stage, Weightless remains a substantially better scheme overall.
Prior work by Mitzenmacher (2002) on regular Bloom ﬁlters has shown that they can be optimized
7
Under review as a conference paper at ICLR 2018
Figure 4: Weightless exploits sparsity more effectively than Deep Compression. By setting pruning
thresholds to produce speciﬁc nonzero ratios, we can quantify sparsity scaling. There is no loss of
accuracy at any point in this plot.
for better post-compression size. We believe a similar method could be used on Bloomier ﬁlters, but
we leave this for future work.
4.3 SCALING WITH SPARSITY
Recent work continues to demonstrate better ways to extract sparsity from DNNs (Guo et al., 2016;
Ullrich et al., 2017; Narang et al., 2017), so it is useful to quantify how different encoding techniques
scale with increasing sparsity. As a proxy for improved pruning techniques, we set the threshold for
magnitude pruning to produce varying ratios of nonzero values for LeNet5 FC-0. We then perform
retraining and clustering as usual and compare encoding with Weightless and Deep Compression (all
without loss of accuracy). Figure 4 shows that as sparsity increases, Weightless delivers far better
compression ratios. Because the false positive rate of Bloomier ﬁlters is controlled independent of
the number of nonzero entries and addresses are hashed not stored, Weightless tends to scale very
well with sparsity. On the other hand, as the total number of entries in CSR decreases, the magnitude
of every index grows slightly, offsetting some of the beneﬁts.
5 CONCLUSION
This paper demonstrates a novel lossy encoding scheme, called Weightless, for compressing sparse
weights in deep neural networks. The lossy property of Weightless stems from its use of the Bloomier
ﬁlter, a probabilistic data structure for approximately encoding functions. By ﬁrst simplifying a
model with weight pruning and clustering, we transform its weights to best align with the properties
of the Bloomier ﬁlter to maximize compression. Combined, Weightless achieves compression of up
to 496×, improving the previous state-of-the-art by 1.51×.
We also see avenues for continuing this line of research. First, as better mechanisms for pruning
model weights are discovered, end-to-end compression with Weightless will improve commensurately.
Second, the theory community has already developed more advanced—albeit more complicated—
construction algorithms for Bloomier ﬁlters, which promise asymptotically better space utilization
compared to the method used in this paper. Finally, by demonstrating the opportunity for using lossy
encoding schemes for model compression, we hope we have opened the door for more research on
encoding algorithms and novel uses of probabilistic data structures.
8
WeightlessDeep Compression>3xUnder review as a conference paper at ICLR 2018
REFERENCES
Apple. The future is here: iphone x. https://www.apple.com/newsroom/2017/09/
the-future-is-here-iphone-x/, 2017.
Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of
the ACM, 13(7):422–426, 1970.
Andrei Broder and Michael Mitzenmacher. Network applications of bloom ﬁlters: A survey. In
Internet Mathematics, 2004.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings
of KDD, 2006.
Denis Charles and Kumar Chellapilla. Bloomier ﬁlters: A second look. In European Symposium on
Algorithms, 2008.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. The Bloomier ﬁlter: an efﬁcient
data structure for static support lookup tables. In Proceedings of the ﬁfteenth annual ACM-SIAM
symposium on Discrete algorithms, 2004.
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Q Weinberger, and Yixin Chen. Compressing
neural networks with the hashing trick. In Proceedings of the 32nd International Conference on
Machine Learning, 2015.
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limit of network quantization. In
5th International Conference on Learning Representations, 2017.
François Chollet. Keras. https://github.com/fchollet/keras, 2017.
Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting
parameters in deep learning. In Advances in Neural Information Processing Systems 26, 2013.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev. Compressing deep convolutional
networks using vector quantization. arXiv, 1412.6115, 2014. URL http://arxiv.org/abs/
1412.6115.
GSMA. Half of the world’s population connected to the mobile internet by 2020, according to
new gsma ﬁgures. https://www.gsma.com/newsroom/press-release/, Novemeber
2014.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efﬁcient DNNs. In
Advances in Neural Information Processing Systems 29, 2016.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In Proceedings of the 32nd International Conference on Machine
Learning, 2015.
Song Han, Huizi Mao, and Bill Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding. In 4th International Conference on Learning
Representations, 2016.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Advances in Neural Information Processing Systems 6, 1993.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
arXiv:1503.0253, 2015.
Yann Lecun and Corinna Cortes.
http://yann.lecun.com/exdb/mnist/.
The MNIST database of handwritten digits.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In Advances in Neural
Information Processing Systems 2, 1989.
9
Under review as a conference paper at ICLR 2018
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11), 1998.
David J.C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University
Press, fourth printing edition, 2005.
Michael Mitzenmacher. Compressed bloom ﬁlters. Transactions on Networks, 10(5), 2002.
Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich Elsen†. Exploring sparsity in recurrent
neural networks. In 5th International Conference on Learning Representations, 2017.
Qualcomm.
Snapdragon neural processing engine now available on qualcomm devel-
https://www.qualcomm.com/news/releases/2017/07/25/
oper network.
snapdragon-neural-processing-engine-now-available-qualcomm-developer,
2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
large scale visual recognition challenge. International Journal of Computer Vision, 115(3), 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, 2015.
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems 29, 2015.
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression.
In 5th International Conference on Learning Representations, 2017.
Shengjie Wang, Haoran Cai, Jeff Bilmes, and William Noble. Training compressed fully-connected
networks with a density-diversity penalty. In 5th International Conference on Learning Represen-
tations, 2017.
6 APPENDIX
6.1 CONSTRUCTING BLOOMIER FILTERS
Given a set of key value pairs, in this case weight addresses and cluster indexes, a Bloomier ﬁlter
is constructed as follows. The task of construction is to ﬁnd a unique location in the table for each
key such that the value can be encoded there. For each to be encoded key a neighborhood of 3 hash
digests are generated, indexes of these 3 are named iotas.
To begin, the neighborhoods of all keys are computed and the unique digests are identiﬁed. Keys
with unique neighbors are removed from the list of keys to be encoded. When a key is associated
with a unique location, its iota value (i.e., the unique neighbor index into the neighborhood of hashs),
is saved in an ordered list along with the key itself. The process is repeated for the remaining keys
and continues until either all keys identify a unique location or none can be found. In the case that
this search fails a different hash function can be tried or a larger table is required (increasing m).
Once the unique locations and ordering is known, the encoding can begin. Values are encoded into
the ﬁlter in the reverse order in which the unique locations are found during the search described
above. This is done such that as non-unique neighbors of keys collide, they can still resolve the
correct encoded values. For each key, the unique neighbor (indexed by the saved iota value) is set
as the XOR of all neighbor ﬁlter entries (each a t-bit length vector), a random mask M, and the
key’s value. As the earlier found unique key locations are populated in the ﬁlter, it is likely that the
neighbor values will be non-zero. By XORing together them in reverse order the encoding scheme
guarantees that the correct value is retrieved. (The XORs can be thought of cancelling out all the
erroneous information except that of the true value.)
An implementation of the Bloomier ﬁlter will be released along with publication.
6.2 SUPPLEMENTAL MATERIAL
10
Under review as a conference paper at ICLR 2018
Figure 5: Plot of retraining (as described in Section 3.2) VGG16’s FC-1 after encoding FC-0 in a
Bloomier ﬁlter. The baseline validation accuracy (on 25000 samples) is 30% and after encoding FC-0,
with a Bloomier ﬁlter, this increases to 33%. As the plots hows, after a few epochs pass the error
introduced by the lossy encoding is recovered. In comparison, test accuracy after encoding FC-0
and before retraining FC-1 is 38.0% and goes down by 2% to 36.0% which maintains the model’s
baseline performance.
Figure 6: Sensitivity analysis of t with respect to the number of weight clusters, k, for FC-0 in
LeNet-300-100. As per Figure 2, model error decreases with the number of false positives, which are
determined by the difference between t and k. This is emphasized here as k = 8 performs strictly
better than higher k values as it results in the strongest encoding given t.
11
02468Epochs29.530.030.531.031.532.032.533.0Validation Error (%)45678t0102030405060Error (%)k = 8k = 12k = 16k = 20Under review as a conference paper at ICLR 2018
Figure 7: Demonstrating iso-accuracy comparisons between previous state-of-the-art DNN com-
pression techniques and Weightless for FC-0 in LeNet5. In comparison to Table 3, we see that with
respect to accuracy, Weightless’ compression ratio scales signiﬁcantly better than Deep Compression.
This is advantageous in applications that are amenable to reasonable performance loss. To generate
this plot we swept the pruning threshold for both Weightless and Deep Compression, and t from 6 to
9 for Weightless.
12
WeightlessDeep Compression5.6x