stage 1, action quantization act is triggered, quantizing the
packet length using QL=5 and setting the resulting quantized
packet length (md.binIndex quant) to 16. The object md stores
the metadata carried over across the pipeline stages.
In the second stage, the truncation table matches against
the quantized packet
length (refer to Figure 4) and trig-
gers the truncation act action, which returns the bin offset
md.bin offset within the ﬂow marker and sets a truncation ﬂag
md.trunc flag in order to inform the downstream stages that
this packet’s corresponding ﬂow marker should be incremented.
In case no match exists in the truncation table, the truncation
ﬂag is not set, and the packet is not accounted for.
Next, since the ﬂow matching rule is not installed in FT1,
the packet is not matched until it reaches stage 4, where
FT2 is located. Upon matching the flow id and verifying
that md.trunc flag is set, the set flow data act2 action is
triggered. This action computes md.rg cell offset by adding
md.bin offset and flow offset loaded by the control plane
into FT2. The resulting value is used to index a cell in RG2
which is then incremented. To index the correct partition of the
register grid, we use control ﬂow logic to test which ﬂow table
partition was matched, triggering the respective reg grid act2
action that updates the ﬂow marker on the corresponding register
grid partition in the next pipeline stage.
Optimizing per-packet computations: To reduce the com-
plexity of the P4 program, we leverage the capabilities offered
by the control plane to ofﬂoad the computation of complex
operations from the data plane. This results in a program
sampled in Listing 1 which implements four simple actions
that can be performed within single stages of the pipeline.
Speciﬁcally, we ofﬂoad two operations into the control plane:
a) Computing a ﬂow offset within the register grid: The index
of the register grid where a ﬂow marker is located can be easily
calculated in the control plane. Since the FMA parameterization
is known prior to the loading of the P4 program on the switch,
the control plane can compute the number of bins used by a
ﬂow marker in a given conﬁguration. Thus, when a new ﬂow
is matched, the collector installs a rule where the ﬂow offset is
given by the number of ﬂow rules installed in a particular ﬂow
table partition times the number of bins composing a marker.
Upon matching, the ﬂow offset is passed as an argument to
action set flow data act2 (line 14).
b) Computing a bin offset within a ﬂow marker: To index
a bin within a ﬂow marker two values must be added: the
ﬂow offset, and the bin offset. While the former can be
computed as described in the previous paragraph, the latter
is computed by the truncation operator. The quantized packet
length passed as an argument to the action responsible for
performing truncation (truncation act, line 8) is computed
by action quantization act (line 3) using a simple bit shift.
Then, the translation between a quantized packet length and the
corresponding bin offset can also be computed ofﬂine once a
speciﬁc FMA parameterization is known, and later loaded by the
md.binIndex_quant =
(bit) (md.pkt_length >> bin_width_shift);
md.bin_offset = new_index;
md.trunc_flag = flag;
1 // triggered by the quantization table
2 // bin_width_shift depends on the Quantization Level (QL)
3 action quantization_act(bit bin_width_shift){
4
5
6 }
7 // triggered by the truncation table
8 action truncation_act(bit new_index, bool flag) {
9
10
11 }
12 //triggered by the flow table (FT_2)
13 // compute the offset of the bin in the RG partition
14 action set_flow_data_act2(bit flow_offset) {
15
16
17 //triggered after matching the flow table (FT_2)
18 action reg_grid_act2() {
19
20
21
22 }
Listing 1: P4 code fragment that implements the actions performed per-
packet by the FMA. The complexity of the truncation operator and of the
computation of ﬂow marker offsets is ofﬂoaded to the control plane and the
resulting values are loaded through MAUs.
bit value;
reg_grid2.read(value, md.rg_cell_offset);
reg_grid2.write(md.rg_cell_offset, value+1);
md.rg_cell_offset = flow_offset + md.bin_offset;
control plane into the truncation table (refer to Section IV-A).
Pre-computing these values in the control plane saves stateful
memory and pipeline’s stages for either monitoring more ﬂows
or executing other forwarding behaviors.
VII. EVALUATION
Here, we present our experimental evaluation of FlowLens
aimed at analyzing the accuracy of ML-based ﬂow classiﬁcation
tasks and the efﬁciency of the switch resources usage.
A. Metrics and Methodology
Our experiments aim at identifying a particular class of
ﬂows denoted as the target class. For instance, when using
FlowLens for covert channel detection, the target class can
be covert trafﬁc. We assess the quality of FlowLens using
the following set of metrics: accuracy, i.e., the percentage of
ﬂows that were correctly classiﬁed in their class, false positive
rate (FPR), i.e., ﬂows that do not belong to the target class
but were erroneously classiﬁed as part of the target, and false
negative rate (FNR), i.e., ﬂows of the target class that were
ﬂagged as not belonging to the class. We also resort to related
metrics such as precision – ratio of the number of relevant
ﬂows retrieved to the total number of relevant and irrelevant
ﬂows retrieved – and recall – ratio of the number of relevant
ﬂows retrieved to the total number of relevant ﬂows.
We train our system to be able to identify speciﬁc target
class ﬂows within the context of three usage scenarios:
Covert channel detection: We train our system to identify
Skype ﬂows carrying covert channels encoded by two cen-
sorship resistance tools: Facet [38] and DeltaShaper [6]. We
train two independent FlowLens applications, for Facet and for
DeltaShaper trafﬁc, using a balanced dataset including covert /
legitimate samples of recorded ﬂows. The trafﬁc is classiﬁed
using the XGBoost [7] classiﬁer, based on the packet length
distribution of the sampled ﬂows.
Website ﬁngerprinting: We train a second FlowLens applica-
tion to identify webpages browsed through encrypted tunnels.
8
Table I.
SCALABILITY OF FLOWLENS.
Table II.
HARDWARE RESOURCE CONSUMPTION.
Use Case
Covert Channels
Website Fgpt.
Botnet Detection
FMA Conﬁguration
(cid:104)QLP L=4, Top-N=10(cid:105)
(cid:104)QLP L=5(cid:105)
(cid:104)QLP L=4, QLIP T =6(cid:105)
Marker
Raw Dist.
20B
94B
302B
3000B
3000B
10200B
Scaling
150×
32×
34×
Resources
Usage
8.46%
Computational
eMatch xBar Gateway VLIW
3.39%
5.21%
Memory
TCAM SRAM
0.00% 38.54%
We leverage the dataset made available by Herrman et al. [29].
This dataset has been widely used for the evaluation of novel
website ﬁngerprinting techniques [93, 60], and it contains traces
of webpage accesses over OpenSSH. Websites are ﬁngerprinted
resorting to the Multinomial Na¨ıve-Bayes classiﬁer [29], which
leverages the packet length distribution of the incoming and
outgoing data in a connection as features. This classiﬁer
also allows us to illustrate how FlowLens can accommodate
alternative truncation schemes whenever a given classiﬁer does
not return a ranking of feature importance (Section VII-E).
Botnet detection: Our last FlowLens application aims at
detecting the presence of botnet chatter among legitimate P2P
trafﬁc. We use the dataset produced by Rahbarinia et al. [64],
which comprises trafﬁc ﬂows produced by four benign P2P
applications (uTorrent, Vuze, Frostwire, and eMule), and two
P2P botnets (Waledac and Storm). Malicious ﬂows can be
identiﬁed by analyzing packet length and inter-packet timing
distributions resorting to a Random Forest classiﬁer [64].
We simulate the classiﬁcation of ﬂows of a given target
class in software based on a set of application-speciﬁc ﬂow
samples. We also conﬁgured all the classiﬁers (Multinomial
Na¨ıve-Bayes, XGBoost, and Random Forest) to use the same
hyperparameters suggested by the papers we drew our use-cases
from. Throughout the evaluation, we assess the performance of
different FlowLens conﬁgurations while exposing the system
to a workload that, to the best of our abilities, mimics those
described in the literature. However, we highlight the adoption
of a single holdout test instead of the cross-validation approach
employed in other representative works [7, 53]. The reason is
that, when applying truncation (Section-IV), FlowLens employs
a pre-training step to obtain a feature ranking from the classiﬁer.
Then, it uses the top-N most important ones to ﬁll the Truncation
Table (Figure-4). Since cross-validation returns an average of
the results obtained by multiple holdout models trained with
different splits of the dataset, the resulting top-N features would
not directly translate to be the top-N ones found in a particular
model instance, namely in the model to be deployed on the
switch for classiﬁcation. Further, we chose a 50/50 holdout to
increase the amount of unseen (test) data and better assess the
generalization ability of the classiﬁer.
B. Overall Performance
To give a general insight into the performance of FlowLens,
Table I presents the scalability gains of our system when it
is used to classify ﬂows for covert channel detection, website
ﬁngerprinting, and botnet trafﬁc detection while displaying an
accuracy loss of at most 3% when compared with the use of
complete packet frequency distributions. For these experiments,
we generated the possible combinations of ﬂow markers for
the three considered use case scenarios, and assessed whether
they allow for accurate ﬂow classiﬁcation despite their compact
size. Packet lengths (PL) vary from 1 to 1500 bytes (MTU),
and each cell of a ﬂow marker has a size of 2 bytes.
These results show that, when the quantization and trunca-
tion parameters are properly ﬁne-tuned (i.e., QL and truncation
table), FlowLens can monitor at least 32 times more ﬂows
when compared to the baseline setup without compression, i.e.,
QL=0 and truncation disabled. Our system can also reach a
150 fold increase in its monitoring capacity when detecting
covert channels. This is achieved for QL=4 and by selecting
the top-10 most relevant bins for truncation. In this case, with
a ﬂow marker as small as 20 bytes, FlowLens manages to
achieve a classiﬁcation accuracy of 93%, only 3% shorter than
the result obtained using raw packet length distributions. For
website ﬁngerprinting, the ﬂow marker is larger (94 bytes)
because we face a multi-class classiﬁcation problem – different
websites are better classiﬁed resorting to different bins. Thus,
the truncation table is conﬁgured to map all quantized packet
lengths. Lastly, for botnet chatter detection, we combine the
quantization of packet inter-arrival time distribution (IPT) with
the PL distribution. In this case, we achieve a marker size of
302 bytes which enables the bookkeeping of >30× ﬂows.
In general, the absolute number of ﬂows that FlowLens
can handle ultimately depends on the switches’ available
SRAM. The NDA we have signed with Toﬁno prevents us from
disclosing the amount of switch memory but other sources [51]
reveal that current switches feature hundreds of MBs of SRAM.
C. Hardware Resource Efﬁciency
To evaluate the efﬁciency of FlowLens’s hardware resource
usage on the switch, we focus independently on the data plane
and on the control plane. As for the data plane, Table II shows
the average hardware resource consumption of FlowLens across
all stages of the switch. The table shows that besides the
SRAM required for the tables and register, the consumption
of other resources is negligible. Since our ﬂow matching logic
entirely relies on exact matching, the FMA’s ﬂow table does not
consume any of the TCAM resources on the switch. In tandem
with the deployment of ﬂow tables in SRAM, FlowLens leaves
over 60% of SRAM available. Overall, these results suggest that
FlowLens makes enough room for the concurrent execution of
many other common forwarding behaviors, like access control,
rate limiting or encapsulation, that do not necessarily require
an extensive use of the stateful memory in the switch pipeline.
On the control plane, the switch has sufﬁcient resources
to ﬁt all models used by FlowLens and to readily classify
ﬂows. In particular, the botnet chatter detection is our largest
model, occupying only 140MB of memory, and 5.6MB of
storage when compressed. In contrast, the model for covert