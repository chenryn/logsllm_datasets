mation; however, at the same time, they do not have the information
that they require for a particular type of incident. As a result, the
participant stated that they waste a lot of time researching how the
information fits the infrastructure.
4.1.5 High False Positive Rate
When SOC systems classify and indicate a legitimate activity as a
malicious one, it is called a false positive.
Fifteen participants stated that false positives are not a major
issue for their SOC systems.
Common reasons for having high false positive rates include hav-
ing low visibility across the board, low SOC maturity, and employee
mistakes. Regarding employee mistakes, P11 said:
Sometimes, employees do something that they should not in
the organization. That is the time when we see false positive
ratio go up.
All the participants manually tune false positives: correcting alerts
one by one when they encounter false positives. Regarding tuning,
P15 said:
We are pretty good at figuring false positives out and fixing
them. We call it tuning. We have a pretty good cycle for tuning.
However, P11 stated that the level of tuning is a trade-off, as too
much tuning would cause the systems to miss real incidents. The
participant said:
False positives are always a balancing act. You can tune out
false positives, but you have to be very careful how you do it. If
you want, I can eliminate 100% false positives, but I am going
to miss some of the true positives.
4.1.6 Poor Usability of SOC Systems
A SOC can be populated with excellent and top-notch tools and
technologies; however; if those tools are extremely difficult to mas-
ter and use, they will never be fully used. In that case, the SOC
analysts would struggle with the tool instead of focusing on the
incident.
Nine participants gave high ratings to the usability of their SOC
systems and tools. The consensus among them was that usabil-
ity varies by each tool however, overall, it has not been a major
problem.
Participants P2, P12, and P16 reported some concerns.
P2 thinks that if a SOC employee must master many tools at the
same time, and if the SOC team is newly formed, the usability level
decreases. When the interviewer asked about the level of usability
in their SOC, the participant stated that their maturity is increasing
over time and usability increases along with it. The participant said:
I would put that as a four out of five, it is a very large array of
tools that we have and given the maturity. However, the tools
that we are using in our daily operations are okay. I would say
three out of five a couple of months ago, but now, it is a four.
P16, who manages an outsourced SOC, stated that, in some cases,
clients request that they use a tool that does not have the ideal
usability level that they would normally prefer.
P12 argued that usability depends on user education. P12 said:
It all depends on user education. If a team has to work on a
particular tool, they are fully educated by the team members,
and if it is required, the vendor is brought in to train them.
Regarding usability, we observed from our results that all partici-
pant SOCs lack a shared interface that groups all their tools in one
location. When the interviewer asked P3 about such an interface,
P3 said:
That would be a dream come true situation.
4.1.7 Low Situational Awareness
A high level of Situational Awareness (SA) requires adequate knowl-
edge of the environment and the organization’s mission to improve
decision-making capabilities [13]. In the context of a SOC, the de-
gree of SA depends on certain criteria, such as the level of employee
awareness of the organization’s mission, their responsibilities, and
their knowledge of the types of attacks and adversaries that the
organization may face.
Sixteen participants reported that they have high SA; however,
some mentioned that there are still some factors that either make
maintaining high SA difficult or decrease SA.
According to P2, the size of their network has an impact on SA.
Regarding this, the participant said:
The size of the network, interconnections, the overall interac-
tions with other teams. So, it is never going to be 100%.
Similarly, when the interviewer asked about their level of SA, P8
stated that their SOC monitors many other organizations and added:
I would say that we are understanding the majority of the
things that we see but due to the fact that there are many
organizations that we are monitoring that is an ever amount
of change with those organizations.
Two of our manager participants who manage outsourced SOCs
(P5 and P18), stated that they would rate their SA as poor due to not
having enough visibility, for the reasons discussed in Section 4.1.1.
4.1.8 Insufficient Analyst Training
In all our participants’ SOCs, either internal or commercial training,
is provided to employees. Internal training methods include hands-
on experiences through simulation platforms, senior employees
educating juniors, junior employees shadowing seniors, security
lectures, and tool training. Commercial training methods include
training programs that are offered by vendors, security certification
programs, and tool training packages that are purchased from the
vendors along with the related tool.
Two of our participants (P3 and P11), expressed that they do
not believe their training programs to be effective. When the in-
terviewer asked P3 about how they would change their current
training methods, the participant responded:
I would love to change them because the old ones are not work-
ing. This is for the majority of the companies; otherwise, phish-
ing campaigns would not be successful; ransomware would not
pass through.
P9, who is a SOC analyst, expressed their disappointment that their
organization does not provide any commercial training programs.
The participant stated that their organization only provides in-
formal internal training and added that they would prefer formal
training that is offered by an external vendor.
P5 stated their concern regarding the importance of tool training
and said:
One of the biggest issues that I see, most organizations, they
buy the new technology, but they do not buy the training.
[...] These days tools tend to be extremely complex. [...] Most
products are not easy to use and require training.
4.2 Mismatched Issues
Here, we describe the issues discussed by our participants where
they have the least agreement. We selected these issues by observing
the gap between the analyst–manager difference in agreement,
shown in Figure 2.
4.2.1 Speed of Response and Level of Automation
Three out of five participants who work in an outsourced SOC
reported factors that negatively affect their response speed.
P17, who is an analyst in an outsourced SOC, stated that their re-
sponse speed relies on client’s capabilities because, per the provider/-
client agreement, they are responsible for only detecting an incident
and their client is supposed to respond. When asked about if they
could rate their SOC response speed, P17 said:
We do not do the final actions to remediate so we cannot be
very fast. It depends on the customer’s capabilities for the final
resolution to be made. Our first response is fast, but depending
on the customer overall, it can be slow.
P16, who manages an outsourced SOC, was concerned about not
having enough staff and rated their speed four out of five. P16 said:
We are not fully staffed. If we had that staff, I would give it a
five.
P5, who manages an outsourced SOC, stated that visibility is para-
mount for quick response speed and it tends to be slow because of
issues such as potential restrictions in the provider/client agreement
or the client’s inability to report a full inventory list (Section 4.1.1).
Compared to outsourced SOCs, the ratio of our participants that
indicated issues regarding response speed was lower.
We observed that ten out of thirteen participants who work in
internal SOCs reported fast response speed without any issues. The
other three participants, P2, P11, and P15, also rated their SOC
response speed as fast; however, they each provided a reason which
affects their response speed negatively.
According to P2, low maturity is the reason for having speed
issues. Specifically, when the interviewer asked P2 if they could
rate their response speed, the participant responded:
I would give a four out of five. Maturity is the reason. Incident
response is serious so that they have to coordinate between
different departments outside of security. Legal and corporate
communications. The house-in procedures are relatively new.
P15 was concerned about their response speed against new attacks.
P15 said:
We are pretty good, but there are other things if it is a new type
of thing, we are not as good as we should be if it is malware,
phishing, like standard attacks we are great but if it is new, we
are having problems making sense out of it.
To improve the speed of response, managers consider automation
as an effective and critical approach. P11, who is a manager in an
internal SOC, stated that they are not satisfied with the current
speed of their SOC and said:
No, I am never happy. We are measuring the mean time to
mitigate an attack, and we count the priority and severity of
the attack, 4 being the least prioritized. In order to mitigate a
P0 we have 30 minutes, P1 60 minutes, P2 120 minutes, P3 24
hours, P4 48 hours. Our current success is %87, which is good,
but I am not happy.
The interviewer followed up with a question to inquire about a
solution for better speed and asked:
What is the solution to increasing that percentage?
The participant replied:
Automation.
Seven of our manager participants stated that they always want
more automation. When the interviewer asked P5 if they need more
automation, the participant said:
I always need more automation.
However, P4, an analyst participant, criticized this point of view
and said:
So, automation is not a Catch-22 type of a problem where it is
becoming a buzzword for vendors, and unfortunately, we have
a lot of management that sees this more important than the
work itself, but not realizing that they are getting less quality.
Nevertheless, another analyst participant, P7, from another per-
spective, expressed their satisfaction for automating the metric
collection, by stating:
Well, every morning, we have to go through all 20 clients and
manually get the metrics from them. Before I had to do all
of them manually, now they are done in a couple of minutes
instead of an hour.
Lastly, during our interviews, another issue that surfaced was the
trustworthiness of automation. When asked if they fully trust their
automation, P16 responded:
So, we do have regular checks for those systems to make sure
that they are working correctly and functioning correctly.
One of our analyst participants, P6, also argued that automation
should not be fully trusted. When asked about the reason, the
participant said:
Yes, because of false positives.
4.2.2 Evaluation Metrics
Organizations use metrics to understand if their SOC is performing
as intended. Furthermore, these metrics also help to shape plans
and identify current problems.
Metrics are of two types: quantitative and qualitative. While
quantitative metrics are used for data, such as the number of inci-
dents or the average time of detection, qualitative metrics are used
to measure the quality instead of quantity, such as the severity level
of an incident.
All of our manager participants stated that they use both quali-
tative and quantitative metrics, with quantitative being more domi-
nant. Quantitative metrics that our participants mentioned include:
number of incidents, number of vulnerabilities discovered, num-
ber of tickets per analyst, time of ticket creation, elapsed time of
resolution, elapsed time of remediation, elapsed time of mitiga-
tion, number of total tickets that are created and resolved, mean
detection time, mean response time, mean time of incident closure,
time taken to react to an incident, number of incidents that are not
closed, and number of known attacks prevented.
Regarding qualitative metrics, our participants mentioned only
incident severity level and vulnerability severity level.
Although quantitative metrics are dominantly used by our man-
ager participants, some of them consider these metrics inaccurate
and ineffective.
When asked about bad practices regarding metrics that are used
in SOCs, P14 responded:
The metrics that just show you numbers instead of the im-
pact are generally not so effective. Because numbers do not
necessarily tell you anything.
P15 stated that their SOC is only responsible for detection and, after
they detect incidents, they hand those cases to other teams and
wait for their completion, which renders some metrics inaccurate.
Analysts consider quantitative metrics that are used by their
managers ineffective. Furthermore, they consider these metrics as
just a tool that their managers use to show false improvement.
When asked if they think that the metrics that are used in their
SOC are effective or not, P12 said:
Not necessarily, because they do not always show the true
effectiveness of a SOC because it is very difficult to measure.
P4 expressed an extreme disapproval toward quantitative metrics.
The participant said:
A useless metric can be the number of events because generally,
people will not take into account false positive numbers (that
are included to the number of events) [...] I think the reason is
that the lack of understanding of security of the upper-class
management as well as, it is a way to make it seem like your
SOC is improving.
4.2.3 Tool Functionality
A SOC is composed of teams that use those tools to investigate
incidents. Some of these tools, such as Security Information and
Event Management systems (SIEM), which are typically used as a
major tool for managing incidents and consuming logs, are vital for
SOC operations. The responsibility for these sets of tools change.
Analysts can be responsible for a few sets of tools, whereas their
managers are responsible for the whole set of tools in the SOC,
including critical tools such as SIEM.
P13, who is a manager, stated that they did not have many cases
during the time that they were the manager of the SOC; however,
they provided a case in which their SIEM malfunctioned. The par-
ticipant said:
I cannot say we had many but one case we had an issue with
a SIEM malfunctioning for a little while, it took a week for the
vendor to fix it.
Another area of the SOC in which managers are responsible is the
scaling and integration of new tools. Managers are also responsi-
ble for legacy tools and their effects on the SOC. P15, who is the
manager, stated their concerns about a legacy tool and said:
Every once in a while we are having problems with tools, we
are having one right now, we have a legacy tool for a SOC, it is
showing its age, we are trying to migrate out of it, but it takes
time [...] SOC cannot do its job while it is down, but we have
backup systems in place.
Backup systems act as a fail-safe in SOCs in case of a malfunction.
Two of our manager participants, P14 and P18, discussed these
mechanisms.
P14 discussed their fail-safe mechanisms in their SOC and said:
Yeah, we experienced malfunctions, we have fail-safe mecha-
nisms though we can always roll back and fix things.
Similarly, P18 stated that they did not have any malfunctioning
case before and gave the reason as:
We actually got a quite redundant architecture in terms of
high availability failover and my experience I haven’t seen
anything like that.
SOC managers can also control various teams that belong to or
work with the SOC. They can assign teams for such cases and
mitigate the problem. Our manager participant, P16, stated that
their engineering team takes care of such issues and said:
You will have issues with tools all the time and that’s where
engineers are involved to make sure that these issues are taken
care of quickly so we are able to keep those systems up and
running, you always have that possibility to have tool issues.
Unlike their managers, our analyst participants do not think that
tool functionality is a problem in their SOC. Three out of eight of
our analyst participants indicated not having a tool functionality
issue.
P4, who is an analyst in a non-tiered SOC, considers tool func-
tionality issues insignificant for their SOC. The participant stated
that manual work should always make up for those functionality
problems. P4 said:
If there is a situation that a tool-set does not work, we always
are to do it manually. If anyone in our group does not have
that capability, we train them.
Maturity is another obstacle regarding tool functionality. If the
SOC is young or the SOC team is young, analysts could have issues
utilizing the full potential of tools.
P7, our analyst participant who works in a tiered SOC, stated the
fact that low maturity could affect them, but did not, and highlighted
that their tools do not have any functionality issues. P7 said:
We are still young, but the tools that we used worked for us.
4.3 Other Issues
In our study, we designed manager-specific and analyst-specific
questions due to the different scopes of work. Also, the interviewer
asked the participants if they have any other problems with their
organization’s SOC that was not inquired in the prior questions.
We grouped their responses in Table 3, and we present other issues
that we found as follows.
4.3.1 Lack of a Communication Channel Between Managers
and Analysts
Proper communication channels between managers and their an-
alysts are an effective way for analysts to report such cases to
their managers. P2, our analyst participant, reported that they have
issues regarding their Threat Intelligence (TI) systems; however,
their manager disagreed with them stating no issues with their
TI systems. As a follow-up, the interviewer asked their opinion
on how to eliminate this problem, and the participant stated that
they do not have a way to suggest any improvement ideas to their
supervisors.
If you had the opportunity to talk to your manager about this
(TI issue), what would you tell them?
The participant replied:
We can give some feedback on why it would be helpful to use
the data. Right now, we may only use the data to kind of look
back at and make correlations on versus actually leveraging
the data and action. So that would probably be some of the
biggest recommendation that we (as analysts) can make is
actually using the data, using it to make actions versus using
it as a reference point.
Participant ID
P1
P2