TCP, and UDP pools.
Round robin
This is the default load-balancing method, which distributes
requests in the order of the list of servers in the upstream pool.
You can also take weight into consideration for a weighted
round robin, which you can use if the capacity of the upstream
servers varies. The higher the integer value for the weight, the
more favored the server will be in the round robin. The algo‐
rithm behind weight is simply statistical probability of a weigh‐
ted average.
Least connections
This method balances load by proxying the current request to
the upstream server with the least number of open connections.
Least connections, like round robin, also takes weights into
account when deciding to which server to send the connection.
The directive name is least_conn.
Least time
Available only in NGINX Plus, least time is akin to least con‐
nections in that it proxies to the upstream server with the least
number of current connections but favors the servers with the
lowest average response times. This method is one of the most
2.4 Load-Balancing Methods | 15
sophisticated load-balancing algorithms and fits the needs of
highly performant web applications. This algorithm is a value-
add over least connections because a small number of connec‐
tions does not necessarily mean the quickest response. A
parameter of header or last_byte must be specified for this
directive. When header is specified, the time to receive the
response header is used. When last_byte is specified, the time
to receive the full response is used. The directive name is
least_time.
Generic hash
The administrator defines a hash with the given text, variables
of the request or runtime, or both. NGINX distributes the load
among the servers by producing a hash for the current request
and placing it against the upstream servers. This method is very
useful when you need more control over where requests are sent
or for determining which upstream server most likely will have
the data cached. Note that when a server is added or removed
from the pool, the hashed requests will be redistributed. This
algorithm has an optional parameter, consistent, to minimize
the effect of redistribution. The directive name is hash.
Random
This method is used to instruct NGINX to select a random
server from the group, taking server weights into consideration.
The optional two [method] parameter directs NGINX to ran‐
domly select two servers and then use the provided load-
balancing method to balance between those two. By default the
least_conn method is used if two is passed without a
method. The directive name for random load balancing is
random.
IP hash
This method works only for HTTP. IP hash uses the client IP
address as the hash. Slightly different from using the remote
variable in a generic hash, this algorithm uses the first three
octets of an IPv4 address or the entire IPv6 address. This
method ensures that clients are proxied to the same upstream
server as long as that server is available, which is extremely
helpful when the session state is of concern and not handled by
shared memory of the application. This method also takes the
16 | Chapter 2: High-Performance Load Balancing
weight parameter into consideration when distributing the
hash. The directive name is ip_hash.
2.5 Sticky Cookie
Problem
You need to bind a downstream client to an upstream server using
NGINX Plus.
Solution
Use the sticky cookie directive to instruct NGINX Plus to create
and track a cookie:
upstream backend {
server backend1.example.com;
server backend2.example.com;
sticky cookie
affinity
expires=1h
domain=.example.com
httponly
secure
path=/;
}
This configuration creates and tracks a cookie that ties a down‐
stream client to an upstream server. In this example, the cookie is
named affinity, is set for example.com, expires in an hour, cannot
be consumed client-side, can be sent only over HTTPS, and is valid
for all paths.
Discussion
Using the cookie parameter on the sticky directive creates a cookie
on the first request that contains information about the upstream
server. NGINX Plus tracks this cookie, enabling it to continue
directing subsequent requests to the same server. The first positional
parameter to the cookie parameter is the name of the cookie to be
created and tracked. Other parameters offer additional control
informing the browser of the appropriate usage, like the expiry time,
domain, path, and whether the cookie can be consumed client side
or whether it can be passed over unsecure protocols.
2.5 Sticky Cookie | 17
2.6 Sticky Learn
Problem
You need to bind a downstream client to an upstream server by
using an existing cookie with NGINX Plus.
Solution
Use the sticky learn directive to discover and track cookies that
are created by the upstream application:
upstream backend {
server backend1.example.com:8080;
server backend2.example.com:8081;
sticky learn
create=$upstream_cookie_cookiename
lookup=$cookie_cookiename
zone=client_sessions:2m;
}
This example instructs NGINX to look for and track sessions by
looking for a cookie named COOKIENAME in response headers, and
looking up existing sessions by looking for the same cookie on
request headers. This session affinity is stored in a shared memory
zone of 2 MB that can track approximately 16,000 sessions. The
name of the cookie will always be application specific. Commonly
used cookie names, such as jsessionid or phpsessionid, are typi‐
cally defaults set within the application or the application server
configuration.
Discussion
When applications create their own session-state cookies, NGINX
Plus can discover them in request responses and track them. This
type of cookie tracking is performed when the sticky directive is
provided the learn parameter. Shared memory for tracking cookies
is specified with the zone parameter, with a name and size. NGINX
Plus is directed to look for cookies in the response from the
upstream server via specification of the create parameter, and
searches for prior registered server affinity using the lookup param‐
18 | Chapter 2: High-Performance Load Balancing
eter. The value of these parameters are variables exposed by the HTTP
module.
2.7 Sticky Routing
Problem
You need granular control over how your persistent sessions are
routed to the upstream server with NGINX Plus.
Solution
Use the sticky directive with the route parameter to use variables
about the request to route:
map $cookie_jsessionid $route_cookie {
~.+\.(?P\w+)$ $route;
}
map $request_uri $route_uri {
~jsessionid=.+\.(?P\w+)$ $route;
}
upstream backend {
server backend1.example.com route=a;
server backend2.example.com route=b;
sticky route $route_cookie $route_uri;
}
This example attempts to extract a Java session ID, first from a
cookie by mapping the value of the Java session ID cookie to a vari‐
able with the first map block, and second by looking into the request
URI for a parameter called jsessionid, mapping the value to a vari‐
able using the second map block. The sticky directive with the
route parameter is passed any number of variables. The first non‐
zero or nonempty value is used for the route. If a jsessionid cookie
is used, the request is routed to backend1; if a URI parameter is
used, the request is routed to backend2. Although this example is
based on the Java common session ID, the same applies for other
session technology like phpsessionid, or any guaranteed unique
identifier your application generates for the session ID.
2.7 Sticky Routing | 19
Discussion
Sometimes, you might want to direct traffic to a particular server
with a bit more granular control. The route parameter to the
sticky directive is built to achieve this goal. Sticky route gives you
better control, actual tracking, and stickiness, as opposed to the
generic hash load-balancing algorithm. The client is first routed to
an upstream server based on the route specified, and then subse‐
quent requests will carry the routing information in a cookie or the
URI. Sticky route takes a number of positional parameters that are
evaluated. The first nonempty variable is used to route to a server.
Map blocks can be used to selectively parse variables and save them
as other variables to be used in the routing. Essentially, the sticky
route directive creates a session within the NGINX Plus shared
memory zone for tracking any client session identifier you specify to
the upstream server, consistently delivering requests with this ses‐
sion identifier to the same upstream server as its original request.
2.8 Connection Draining
Problem
You need to gracefully remove servers for maintenance or other rea‐
sons while still serving sessions with NGINX Plus.
Solution
Use the drain parameter through the NGINX Plus API, described in
more detail in Chapter 5, to instruct NGINX to stop sending new
connections that are not already tracked:
$ curl -X POST -d '{"drain":true}' \
'http://nginx.local/api/3/http/upstreams/backend/servers/0'
{
"id":0,
"server":"172.17.0.3:80",
"weight":1,
"max_conns":0,
"max_fails":1,
"fail_timeout":
"10s","slow_start":
"0s",
"route":"",
20 | Chapter 2: High-Performance Load Balancing
"backup":false,
"down":false,
"drain":true
}
Discussion
When session state is stored locally to a server, connections and per‐
sistent sessions must be drained before it’s removed from the pool.
Draining connections is the process of letting sessions to a server
expire natively before removing the server from the upstream pool.
You can configure draining for a particular server by adding the
drain parameter to the server directive. When the drain parameter
is set, NGINX Plus stops sending new sessions to this server but
allows current sessions to continue being served for the length of
their session. You can also toggle this configuration by adding the
drain parameter to an upstream server directive.
2.9 Passive Health Checks
Problem
You need to passively check the health of upstream servers.
Solution
Use NGINX health checks with load balancing to ensure that only
healthy upstream servers are utilized:
upstream backend {
server backend1.example.com:1234 max_fails=3 fail_timeout=3s;
server backend2.example.com:1234 max_fails=3 fail_timeout=3s;
}
This configuration passively monitors the upstream health, setting
the max_fails directive to three, and fail_timeout to three sec‐
onds. These directive parameters work the same way in both stream
and HTTP servers.
Discussion
Passive health checking is available in the Open Source version of
NGINX. Passive monitoring watches for failed or timed-out connec‐
tions as they pass through NGINX as requested by a client. Passive
2.9 Passive Health Checks | 21
health checks are enabled by default; the parameters mentioned here
allow you to tweak their behavior. Monitoring for health is impor‐
tant on all types of load balancing, not only from a user experience
standpoint, but also for business continuity. NGINX passively moni‐
tors upstream HTTP, TCP, and UDP servers to ensure that they’re
healthy and performing.
2.10 Active Health Checks
Problem
You need to actively check your upstream servers for health with
NGINX Plus.
Solution
For HTTP, use the health_check directive in a location block:
http {
server {
...
location / {
proxy_pass http://backend;
health_check interval=2s
fails=2
passes=5
uri=/
match=welcome;
}
}
# status is 200, content type is "text/html",
# and body contains "Welcome to nginx!"
match welcome {
status 200;
header Content-Type = text/html;
body ~ "Welcome to nginx!";
}
}
This health check configuration for HTTP servers checks the health
of the upstream servers by making an HTTP request to the URI '/'
every two seconds. The upstream servers must pass five consecutive
health checks to be considered healthy. They are considered unheal‐
thy if they fail two consecutive checks. The response from the
upstream server must match the defined match block, which defines
the status code as 200, the header Content-Type value as 'text/
22 | Chapter 2: High-Performance Load Balancing
html', and the string "Welcome to nginx!" in the response body.
The HTTP match block has three directives: status, header, and
body. All three of these directives have comparison flags, as well.
Stream health checks for TCP/UDP services are very similar:
stream {
...
server {
listen 1234;
proxy_pass stream_backend;
health_check interval=10s
passes=2
fails=3;
health_check_timeout 5s;
}
...
}
In this example, a TCP server is configured to listen on port 1234,
and to proxy to an upstream set of servers, for which it actively
checks for health. The stream health_check directive takes all the
same parameters as in HTTP with the exception of uri, and the
stream version has a parameter to switch the check protocol to udp.
In this example, the interval is set to 10 seconds, requires two passes
to be considered healthy, and three fails to be considered unhealthy.
The active-stream health check is also able to verify the response
from the upstream server. The match block for stream servers, how‐
ever, has just two directives: send and expect. The send directive is
raw data to be sent, and expect is an exact response or a regular
expression to match.
Discussion
Active health checks in NGINX Plus continually make requests to
the source servers to check their health. These health checks can
measure more than just the response code. In NGINX Plus, active
HTTP health checks monitor based on a number of acceptance cri‐
teria of the response from the upstream server. You can configure
active health-check monitoring for how often upstream servers are
checked, how many times a server must pass this check to be con‐
sidered healthy, how many times it can fail before being deemed
unhealthy, and what the expected result should be. The match
parameter points to a match block that defines the acceptance crite‐
ria for the response. The match block also defines the data to send to
2.10 Active Health Checks | 23
the upstream server when used in the stream context for TCP/UPD.
These features enable NGINX to ensure that upstream servers are
healthy at all times.
2.11 Slow Start
Problem
Your application needs to ramp up before taking on full production
load.
Solution
Use the slow_start parameter on the server directive to gradually
increase the number of connections over a specified time as a server
is reintroduced to the upstream load-balancing pool:
upstream {
zone backend 64k;
server server1.example.com slow_start=20s;
server server2.example.com slow_start=15s;
}
The server directive configurations will slowly ramp up traffic to
the upstream servers after they’re reintroduced to the pool. server1
will slowly ramp up its number of connections over 20 seconds, and
server2 over 15 seconds.
Discussion
Slow start is the concept of slowly ramping up the number of
requests proxied to a server over a period of time. Slow start allows
the application to warm up by populating caches, initiating database
connections without being overwhelmed by connections as soon as
it starts. This feature takes effect when a server that has failed health
checks begins to pass again and re-enters the load-balancing pool.
24 | Chapter 2: High-Performance Load Balancing
2.12 TCP Health Checks
Problem
You need to check your upstream TCP server for health and remove
unhealthy servers from the pool.
Solution
Use the health_check directive in the server block for an active
health check:
stream {
server {
listen 3306;
proxy_pass read_backend;
health_check interval=10 passes=2 fails=3;
}
}
The example monitors the upstream servers actively. The upstream
server will be considered unhealthy if it fails to respond to three or
more TCP connections initiated by NGINX. NGINX performs the
check every 10 seconds. The server will only be considered healthy
after passing two health checks.
Discussion
TCP health can be verified by NGINX Plus either passively or
actively. Passive health monitoring is done by noting the communi‐
cation between the client and the upstream server. If the upstream
server is timing out or rejecting connections, a passive health check
will deem that server unhealthy. Active health checks will initiate
their own configurable checks to determine health. Active health
checks not only test a connection to the upstream server, but can
expect a given response.
2.12 TCP Health Checks | 25