via
m
via G. If C receives both messages, she can
S and m
easily reconstruct the original codeword by computing
∗) ⊕ m
(m ⊕ m
, but neither S nor G can notice that the
codeword has been sent. This principle can be extended
to arbitrary users and trust models by the common con-
cept of secret sharing [26].
of the same length as m and to send m ⊕ m
∗
∗
∗
∗
As we can see, a probabilistic deﬁnition of informa-
tion is very ﬁne-grained, and cryptographic primitives
like secret sharing even ensure that pieces carrying no
information at all may be combined to provide the full
amount of secret knowledge.
In our example, the joint view of S and G is sufﬁ-
cient to determine the desired information. More gen-
erally, the solution for arbitrary ﬂow graphs is again to
consider a cut in the ﬂow graph, cf. Deﬁnition 3.2. The
joint view of the machines contained in the cut should
be sufﬁcient to determine whether or not the speciﬁc in-
formation has been sent. This yields a more general def-
inition of recognition non-interference, based on n third
parties T1, . . . , Tn.
Deﬁnition 3.5 (Recognition Non-Interference) Let a
ﬂow policy G = (∆,E) for a structure ( ˆM , S ) be given.
Then ( ˆM , S ) fulﬁlls this policy G iff for all H, L and for
H,L = ( ˆM , S ,
all non-interference conﬁgurations conf n in
{H, L, T1, . . . , Tn}, A) ∈ Confn in
H,L,I( ˆM , S ) the follow-
Figure 4. Sketch of the deﬁnition of recog-
nition non-interference for the ﬂow policy
(cid:4) are guesses
of Figure 1. The bits b
at b.
∗ and b
3.4 Recognition Non-Interference
Similar to the previous example we ﬁrst develop our
ideas for a simple policy, the one with only three users
of Figure 1. Intuitively, our deﬁnition states that if there
is information ﬂow from H to L, then the user T can also
recognize this ﬂow if it wants to, i.e., it could also guess
the initial bit b with non-negligible advantage. We call
this notion recognition non-interference. It is shown in
Figure 4.
The notion that an arbitrary given machine T “could”
guess a bit is formalized by a machine D, called a dis-
tinguisher, which outputs the bit based only on the in-
formation the third party T got during the run, i.e., the
view of T. More formally, a machine D is called a dis-
tinguisher if it only performs one single (probabilistic)
transition resulting in precisely one non-empty output
that only contains one bit. As the view of T in a run r
is written r(cid:5)T, this probabilistic assignment of the bit b
(cid:4)
(cid:4) ← D(r(cid:5)T, 1k). The second input 1k repre-
is written b
sents the security parameter k; it is given in unary rep-
resentation because polynomial-time is measured in this
security parameter.
Deﬁnition 3.4 (Recognition Non-Interference for One
Policy) Let the standard ﬂow policy of Figure 1 for a
structure ( ˆM , S ) be given. Then ( ˆM , S ) fulﬁlls the
recognition non-interference requirement for this policy
H,L =
iff for all non-interference conﬁgurations conf n in
( ˆM , S ,{H, L, T}, A) ∈ Confn in
H,L,I( ˆM , S ) the following
holds: If
P (b = b
∗
|
;
H,L ,k
r ← runconf n in
b := r(cid:5)pH bit!;
∗ := r(cid:5)p∗
b
L bit
?) ≥ 1
2
+ ns(k)
holds for ns(k) (cid:10)∈ SMALL, then there exists a distin-
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
ing holds: If
P (b = b
∗
|
;
H,L ,k
r ← runconf n in
b := r(cid:5)pH bit!;
∗ := r(cid:5)p∗
b
L bit?) ≥ 1
2
+ ns(k)
holds for ns (cid:10)∈ SMALL, then for all cuts ˆC for H, L,
(cid:10)∈
there exists a distinguisher D and a function ns(cid:4)
SMALL such that
P (b = b
(cid:4)
|
;
H,L ,k
r ← run conf n in
b := r(cid:5)pH bit!;
(cid:4) ← D(r(cid:5) ˆC , 1k)) ≥ 1
b
2
+ ns(cid:4)(k).
For the computational case, the distinguisher has to be
polynomial-time.
✸
The deﬁnition can be extended to predicates on ns, ns(cid:4)
like Deﬁnition 3.4.
3.5 Example: Dealing with Public-Key Opera-
tions
Deﬁnition 3.5 is very general. However, if we look
closely, we see that it is too strict for many cases. Recall
our example of the secretary. Assume that the machine
of the CEO is guarded by a cryptographic ﬁrewall that
only allows messages from the secretary and the good
customers to pass. However, if the CEO is malicious
and wants to receive information from outside that the
secretary cannot recognize, she can do so by creating
a public key of an asymmetric encryption system and
broadcast it to the bad customers.
It does not matter
whether or not the secretary and the good customers no-
tice the transmission of this key. Now the bad customer
simply encrypts its bit b and sends it to the CEO via the
secretary. The secretary cannot guess the bit b with non-
negligible probability using this information, or he could
break the underlying encryption scheme. Thus this im-
plementation with a cryptographic ﬁrewall does not ful-
ﬁll Deﬁnition 3.5.
The problem here is that the knowledge of the secre-
tary and the CEO is not shared, i.e., only the CEO has
the secret key to decrypt incoming messages.
The problem can be solved by either prohibiting that
the CEO outputs messages to outside users, which seems
fairly unrealistic, or by letting the secretary know the se-
crets of the CEO. The second possibility is quite realis-
tic for this kind of example, as the CEO probably does
not want to be inﬂuenced by bad customers, and hence
shares a lot of knowledge with her secretary already by
letting him screen her mail. Such a CEO that wants to be
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
b
H
L
b*
/
T
view
rand
D
b'
Figure 5. Sketch of
recognition non-
interference for trusted recipients for the
ﬂow policy of Figure 4.
protected will surely not create a public encryption key
on her own and spread it over the net. We model this by
additionally giving the distinguisher the content of the
random tape of C, where C is considered as a probabilis-
tic Turing machine. We can thus keep the distinguisher
non-interactive. This is illustrated in Figure 5.
3.6 Recognition Non-Interference for Trusted
Recipients
This example yields a less strict deﬁnition of recog-
nition non-interference, which we call recognition non-
interference for trusted recipients. In the following, we
denote the content of the random tape of a probabilis-
tic Turing machine M in a run r by randomr(M), and
similarly for sets.
For arbitrary policies things again become a bit more
complicated. If we want to prove H (cid:10)❀ L, and if we
have a cut ˆC with respect to H and L, then we need the
random tapes of all users “between” the cut and L. More
formally, this means that for a given cut ˆC in a ﬂow
policy, the random tape of a user Ti is needed iff Ti and
L lie in the same component of the graph after removing
the cut from the graph. We denote the set of these users
by T ˆC .
Non-Interference
Deﬁnition 3.6 (Recognition
for
Trusted Recipients) Let a ﬂow policy G = (∆,E) for
a structure ( ˆM , S ) be given. Then ( ˆM , S ) fulﬁlls the
recognition non-interference requirement for trusted
recipients for this policy G iff for all H, L and for all
H,L = ( ˆM , S ,
non-interference conﬁgurations conf n in
{H, L, T1, . . . , Tn}, A) ∈ Confn in
H,L,I( ˆM , S ) the follow-
ing holds: If
P (b = b
∗
|
;
H,L ,k
r ← run conf n in
b := r(cid:5)pH bit!;
∗ := r(cid:5)p∗
b
L bit?) ≥ 1
2
+ ns(k)
holds for ns (cid:10)∈ SMALL, then for all cuts ˆC for H, L,
(cid:10)∈
there exists a distinguisher D and a function ns(cid:4)
SMALL such that
P (b = b
(cid:4)
|
;
H,L ,k
r ← runconf n in
b := r(cid:5)pH bit!;
(cid:4) ← D(r(cid:5) ˆC , randomr(T ˆC ), 1k))
b
≥ 1
2
+ ns(cid:4)(k).
For the computational case, the distinguisher has to be
polynomial-time.
✸
The deﬁnition can be extended to predicates on ns, ns(cid:4)
like Deﬁnition 3.4.
4 Downgrading of Information
Requiring that no information ﬂow at all shall occur
is too strict for many practical purposes. As a typical ex-
ample consider the model of Bell-LaPadula, consisting
of several groups of users along with a total relation ≥
on the groups. Here H1 ≥ H2 for two groups H1 and
H2 means that no information should ﬂow from H1 to
H2. This corresponds to a ﬂow graph with Hi (cid:10)❀ Hj
iff Hi ≥ Hj. Assume for instance that the groups Hi
are ascending military ranks. As people of higher rank
are allowed to learn more secret information, no infor-
mation shall ﬂow to people of lower rank. So far, this
can be expressed nicely using our above examples and
deﬁnitions. However, this also means that an ofﬁcer is
not allowed to send any message to its soldiers. This
problem is typically solved in practice by allowing in-
formation to be downgraded.
A typical way of downgrading is that a trusted user
may do it. In the military example, an ofﬁcer may tra-
ditionally be trusted to classify information correctly by
attaching paper labels to it. Similarly, he may explic-
itly attach labels to digital inputs, e.g., H (high) or L
(low) to each input m. This situation is not restricted to
trusted human users, but also applies to trusted processes
“above” the currently considered system. This system
may be a multi-level network or a distributed operat-
ing system, and certain applications may be allowed to
operate on multiple levels, including downgrades. The
security to be deﬁned is that of the underlying system,
without knowing details of the trusted applications.
Security for this case essentially means that a low
user L should only get information from H via the sys-
tem if that information was explicitly downgraded by H.
Thus, if L can guess a bit b that H knows, a distinguisher
should exist that can also guess that bit, based only on
the explicitly downgraded information. This is sketched
in Figure 6.
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
b
H
b'
D
b*
L
(H/L, m)
L-part
^
M
Figure 6. Downgrading by trusted user H.
The untrusted user L should learn nothing
from H via the system except what can be
derived from H’s downgraded information
alone.
We ﬁrst deﬁne our downgrading syntax, and then the