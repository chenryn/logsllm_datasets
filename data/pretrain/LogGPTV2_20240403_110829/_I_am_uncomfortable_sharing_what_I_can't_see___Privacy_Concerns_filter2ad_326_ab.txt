### 3. Method

We now describe our survey and data analysis procedures.

#### 3.1 Survey Study

To address our research questions, we conducted an online survey to explore the privacy and security concerns of visually impaired people (VIPs) who use camera-based assistive technologies to share images. The survey considered three types of human assistants: a family member, a friend, and a volunteer or crowd-worker. We employed a between-subjects design, randomly assigning participants to one of these three categories. Each participant was then presented with three within-subjects scenarios (home, office, and restaurant), which included questions about possible foreground and background objects in the image. Participants took approximately 20-30 minutes to complete the survey.

##### 3.1.1 Selection of Scenarios

Our survey examined participants' concerns related to sharing information across three different scenarios: home (a residential space), office (place of employment), and restaurant (dining establishment). These scenarios were chosen based on prior studies. Church and Oliver found that over 70% of mobile information seeking occurs in familiar contexts such as home or the office [27]. Abdolrahmani et al. reported the use of mobile devices and assistive applications by VIPs in restaurants, homes, and offices [3]. These scenarios represent real-life engagements for VIPs in private, semi-private, and public places, respectively. Each participant was presented with all three scenarios in random order.

##### 3.1.2 Foreground and Background Object Selection

In the survey, we defined "foreground" objects as the primary objects about which users ask questions, and "background" objects as those present in the photo but not the primary focus. To determine the list of foreground and background objects, we first analyzed the VizWiz dataset [34], which contains 20,000 publicly available images and associated questions from visually impaired users. The dataset was cleaned to remove sensitive information, but we obtained 200 sensitive images from the authors to ensure a comprehensive selection. Two researchers independently categorized the images and then reached a consensus on representative groups.

From this analysis, we identified five major privacy violations: address information (e.g., on envelopes), prescription labels, credit card information, contents of digital screens (e.g., computer screens), and the presence of faces or other body parts (both of the user and bystanders). Our selected foreground and background objects are representative of the objects and questions asked by VIPs, as observed in prior studies [19, 33].

In each scenario, we assumed only one foreground object, as this is typical when asking questions in such systems. We listed 10 potential background objects that could be present in the image along with the foreground object. Six background objects were common to all scenarios, while the rest were specific to each scenario. For example, "restaurant bill" was included in the restaurant scenario but not in the others. Table 1 provides a detailed list of the foreground and background objects used in the three scenarios.

##### 3.1.3 Measuring Privacy Concern

We asked the following three questions for each scenario (see Appendix A for the full survey instrument):

**Q1.** How comfortable would you feel asking for help (about a foreground object) from a sighted assistant by sharing an image? This question varied slightly based on the scenario. Participants responded on a 5-point Likert scale: (1) extremely uncomfortable, (2) somewhat uncomfortable, (3) neither uncomfortable nor comfortable, (4) somewhat comfortable, (5) extremely comfortable.

**Q2.** How comfortable would you feel if the following background objects were present in the image? This question also varied based on the foreground object and the scenario, using the same Likert scale.

**Q3.** Please briefly explain your selection above. This was an open-ended question where participants explained their comfort levels regarding sharing photos or videos with a human assistant.

##### 3.1.4 Organization of the Survey

The survey consisted of 32 questions, both open-ended and close-ended. The survey instrument was organized as follows (see Appendix A for the full survey):

- **Consent form.**
- **Device and technology usage:** Questions about the electronic devices and assistive technologies used, frequency of camera use and image sharing, and details about visual impairments.
- **Assistance-seeking behavior:** Questions about the kind of help sought from sighted people, whether they shared images or made video calls, and the types of questions they usually ask.
- **Scenarios:** Three scenarios presented in random order, each with three questions about the foreground object, background objects, and an explanation for their selections. Each participant was assigned to a single type of human assistant.
- **Sensitive information sharing:** Questions about whether they had ever shared a photo containing sensitive information and their most recent experience sharing an image with a sighted person.
- **Demographics:** Five demographic questions (age, gender, race or nationality, education, and occupation).

##### 3.1.5 Recruitment

The survey was conducted on Qualtrics, an accessible survey platform, over one month between August and September 2018. We shared our recruitment sign-up form through email lists of various organizations, including the National Federation of the Blind (NFB) and the American Council of the Blind (ACB). Participants had to meet the following criteria: (1) living in the United States for at least five years, (2) 18 years of age or older, and (3) visually impaired. Qualified participants received a unique, non-reusable survey link via email.

##### 3.1.6 Sample Validity Considerations

The survey was shared only with a curated list of VIPs managed by reputable organizations. NFB and ACB reviewed our study information and forwarded the recruitment email to their mailing lists. One researcher interacted with each participant to confirm their level of visual impairment. We retained data only from participants who sufficiently described their level of visual impairment in their responses. Our compensation structure was designed to encourage high-quality responses.

##### 3.1.7 Compensation and Ethical Considerations

Participants were recruited from different organizations, and we used a random-drawing approach for compensation. After collecting 155 responses, we performed a random drawing, selecting 15 (10%) participants and sending them $20 Amazon e-gift certificates. The study and compensation scheme were approved by our institution's ethics review board (IRB).

##### 3.1.8 Pilot Study

We conducted an in-person online survey and follow-up interviews with four male individuals to identify any accessibility issues. Three participants were blind, and one had low vision. Their ages ranged from 25 to 55-or-older, and all were employed full-time. They used Jaws and Google’s TalkBack as screen readers. The pilot study took 40-60 minutes per participant, and they were compensated with $20 cash. We addressed any accessibility issues identified in the first phase and finalized the survey after the second phase.

#### 3.2 Data Analysis Procedure

##### 3.2.1 Quantitative Analysis

We used non-parametric statistical tests due to the data not meeting the assumptions of parametric tests. Our dependent variable was the comfort level for sharing information, and independent variables included human assistants, scenarios, and objects. We conducted Kruskal-Wallis tests, Wilcoxon rank sum tests, Friedman rank sum tests, and Wilcoxon signed rank tests to analyze the data. Post hoc tests included Dunn’s test with Benjamini-Hochberg correction and pairwise Wilcoxon signed rank tests.

##### 3.2.2 Sample Size Power Analysis

A power analysis indicated that 50 participants per condition would provide enough statistical power to detect small-sized effects (α=0.05, 1−β=0.90).

##### 3.2.3 Qualitative Analysis

Qualitative answers were independently coded by two researchers using a bottom-up approach. They met weekly to iteratively code a subset of open-ended responses. Responses were coded into seven reasons: burden, impression, indifference, relevance, professionalism, trust, and security. Cohen’s Kappa was computed for inter-rater reliability, and disagreements were discussed. After two rounds of coding, the researchers achieved an acceptable average pairwise Cohen’s Kappa score of 0.8 or greater.

### 4. Findings: Quantitative Analysis

#### 4.1 Demographics and Technology Usage

A total of 165 people participated in the survey, with 155 completing it. Participants were randomly assigned to one of three conditions: friends (54), family (50), and volunteers or crowd-workers (51). The sample was 59.4% female and 40.6% male. Age distribution was 29.3% (18-34 years), 33.3% (35-54 years), and 37.4% (55+ years). Employment status varied, with 37.6% employed full-time, 20.8% retired, 17.4% unemployed and looking for work, 16.1% employed part-time, and 8.1% students. In terms of visual impairment, 61.2% were totally blind, and 38.8% had varying levels of VIP. Most participants (60.4%) were visually impaired since birth.

Participants reported using various camera-based assistive technologies, with the most popular being Seeing AI (80%), TapTapSee (70.3%), BeMyEyes (69.6%), and KNFB Reader (65.8%). Almost all (96%) had been using these technologies for more than a year. The primary sighted supporters were family and friends (80%), followed by volunteers or crowd-workers (65%). Assistance was sought primarily for reading documents (81.8%), identifying objects (67.7%), identifying colors (63.7%), and seeking subjective opinions (30.8%).

#### 4.2 Selective Content Disclosure

To understand the impact of background content on sharing preferences, we analyzed...