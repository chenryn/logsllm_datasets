sharing of private visual information.
3 Method
We now describe our survey and data analysis procedures.
3.1 Survey study
To answer our research questions, we conducted an online sur-
vey on the privacy and security concerns of people with VIPs
who share images using camera-based assistive technologies.
In the survey, we considered three different human-sourced
assistive technologies by varying the type of human assistant
(a family member, a friend, and a volunteer or crowd-worker)
and conducted a between-subjects survey through random
assignment based on these three types of assistants. Each of
these surveys had three within-subjects (randomly ordered)
scenarios (home, ofﬁce, and restaurant) with each having
questions about possible foreground and background objects
in the image. Participants took approximately 20–30 minutes
to complete the survey.
3.1.1 Selection of scenarios
Our surveys captured peoples’ concerns related to sharing
information across three different scenarios: in home (lo-
cated within a residential space), ofﬁce (located at the place
of employment), and restaurant (located at a dining estab-
lishment) settings. These scenarios were grounded in prior
studies. Church and Oliver found that more than 70 percent
of mobile information seeking was performed in familiar con-
texts such as at home or the ofﬁce [27]. Abdolrahmani et
al. reported the use of mobile devices and assistive applica-
tions by people with VIPs in restaurants, home, and ofﬁce
scenarios [3]. These scenarios are representative of real-life
engagements for people with VIPs in private, semi-private,
and public places respectively. Each participant was presented
all three scenarios (within-subjects) in random order.
3.1.2 Foreground and background object selection
In the survey, we referred to the objects that users ask the ques-
tion about as “foreground” objects (primary objects) and the
objects which are present in the photo but not primary objects
as “background” objects. To determine the list of foreground
and background objects included in the survey, we ﬁrst ex-
plored the VizWiz dataset [34]. This dataset is derived from
a natural visual question answering system where visually
impaired users took images and recorded spoken questions
and sent them to crowd workers. Since most camera-assisted
technologies follow a similar approach, the publicly available
VizWiz dataset illustrated common privacy issues that may
arise while using such a service.
The dataset comprises 20,000 publicly available images
and the associated questions (as text) about the images. This
dataset was cleaned and released by the authors so as to re-
move any images with sensitive information. To reduce bias
in our selection of objects, we contacted the authors and ob-
tained these sensitive images (200). These images had the
sensitive portions redacted but contained enough informa-
tion about the type of object (e.g., faces were blurred). We
randomly selected 1,000 images from the publicly available
images along with the 200 sensitive images. Two researchers
individually categorized the images into groups. They then
met and came to consensus on a representative set of groups.
After analyzing the dataset, we observed ﬁve major privacy
violations as foreground objects or background objects in the
images: address information (e.g., on envelopes), prescrip-
tion labels, credit card information, contents of digital screens
(e.g., computer screen), and the presence of the face or other
body parts of the user (as well as of bystanders). Our selected
foreground and background objects are thus representative of
the objects and questions asked by people with VIPs as also
observed in prior studies [19, 33].
In each scenario, we assumed only one foreground object
in the image, since that is the typical use case when asking
questions in such systems. We included objects that are the
combination of sensitive, personally identiﬁable, ﬁnancial,
and miscellaneous objects that people asked questions about
in the VizWiz application. Later, we listed 10 background
objects that could possibly be present in the image along with
the foreground object in that given scenario. The selection of
the background objects for each scenario varied slightly; six
objects were common to all scenarios whereas the rest were
speciﬁc to the scenario description. For example, we added
‘restaurant bill’ for the restaurant scenario but did not include
1932    29th USENIX Security Symposium
USENIX Association
that in the other two scenarios. Table 1 describes background
and foreground objects used in the three scenarios.
3.1.3 Measuring privacy concern
We asked the following three questions (paraphrased) for each
scenario (see Appendix A for our survey instrument):
Q1. How comfortable would you feel asking for help (about
a foreground object) from a sighted assistant by sharing an
image? This question varied slightly based on the scenario.
Participants were asked to select from a 5-point Likert scale:
(1) extremely uncomfortable (2) somewhat uncomfortable (3)
neither uncomfortable nor comfortable (4) somewhat com-
fortable (5) extremely comfortable.
Q2. How comfortable would you feel if the following back-
ground objects were present in the image? This question var-
ied slightly based on the foreground object and the scenario.
The question used the same Likert scale mentioned above.
Q3. Please brieﬂy explain your selection above. This was
an open-form question. Participants were asked to explain
their selections for feeling comfortable or uncomfortable
while sharing photos or videos with a human assistant.
3.1.4 Organization of the survey
The survey consisted of 32 questions in both open-ended and
close-ended form. The survey instrument was organized as
follows (see Appendix A for the survey instrument):
• Consent form.
• Questions about which (if any) electronic devices and as-
sistive technologies the participant uses, how frequently
they use the camera and share images online, and ques-
tions on their level and duration of visual impairments.
• Questions about the kind of help they seek from sighted
people, whether they shared images or made video calls
to a sighted person for seeking help, and what questions
they usually ask.
• Three scenarios, presented in random order (within sub-
jects), each with three questions about the foreground
object, background objects (in random order), and an ex-
planation for their selections. Note that each participant
was assigned to a single assistive technology (type of
human assistant), and these questions were asked in the
context of one kind of human assistant.
• Questions about whether they had ever shared a photo
containing sensitive information and their most recent
experience sharing an image with a sighted person.
• Five demographic questions (age, gender, race or nation-
ality, education, and occupation).
3.1.5 Recruitment
The survey was conducted on Qualtrics (an accessible survey
platform) over a period of one month between August and
September 2018. We shared our recruitment sign-up form
through email lists of various organizations including the
National Federation of the Blind (NFB) and the American
Council of the Blind (ACB). We asked visually impaired as-
sistive technology users to sign-up through a form provided if
they met the following criteria: participants had to be (1) liv-
ing in the United States for at least ﬁve years to help control
for cultural variability [48]; (2) 18 years of age or older; and
(3) visually impaired. Researchers screened the qualiﬁed par-
ticipants and personally emailed each participant a unique
survey link. The link was not reusable, and each participant
could participate in the survey only once.
3.1.6 Sample validity considerations
The survey was shared only with a curated list of VIPs man-
aged by reputable organizations. NFB and ACB reviewed
our study information for relevance and then forwarded our
recruitment email to their mailing lists. Based on organiza-
tion membership and list curation our recruitment email went
to only those people who had VIPs. Next, one researcher
interacted with each individual participant and inquired about
their level of visual impairment and blindness. Additionally,
we recruited (or retained the data of) only those participants
who sufﬁciently described their level of VIPs in their free-text
responses in our survey and sign-up instruments. Finally, our
compensation structure (see Section 3.1.7) was chosen in part
to provide high-quality responses.
3.1.7 Compensation and ethical considerations
We recruited the participants from different organizations and
could not anticipate the number of participants before initiat-
ing the survey. Therefore, we picked a random-drawing ap-
proach as opposed to a straight payment, and the participants
were told upfront about the compensation in the recruitment
email as well as the consent form. A rafﬂe-based approach is
also less likely to invite abuse and instead stimulate voluntary
participation and high-quality answers [17]. After collecting
155 responses, we performed the random drawing, selected
15 (10%) participants, and sent $20 Amazon e-gift certiﬁcates
to each of them. We emailed them the link of the e-gift cer-
tiﬁcates within three days of performing the random drawing.
The study and compensation scheme was approved by our
institution’s ethics review board (IRB).
3.1.8 Pilot study
We conducted an in-person online survey and a follow-up
interview with four male individuals to identify any accessi-
bility issues with our survey instrument. Three of the pilot
USENIX Association
29th USENIX Security Symposium    1933
Scenario
Restaurant
Ofﬁce
Home
Foreground
object/task
Identifying the
type of soda can
Differentiating
similar sized
medicine bottles
Matching
scarf/tie with
dress/suit
Background objects
Credit card; Your face or body part; Restaurant bill; The book you were reading; Other
people sitting at the next table; Other foods you ordered; Medical prescription; Messy
area; Laptop screen; Your reﬂection on a laptop screen
Medical prescription; Your face or body part; Credit card; Mail containing your and
your friend’s addresses; Messy area; Photo frame with your family picture; Laptop
screen; Ofﬁcial documents, Your co-worker’s face or body part; Food items
Your face or body part; Mail containing your and your friend’s addresses; Credit card;
Messy area; Photo frame with your family picture; Laptop screen; Medical prescription;
Your reﬂection in the laptop screen; The book you were reading; Food items
Table 1: List of foreground and background objects
participants were blind and one had low vision. Their ages
ranged from 25 to 55-or-older with full-time employment.
Three participants participated in the survey using computers
and one from a mobile phone. They used Jaws and Google’s
TalkBack as screen readers. We requested them to point out
any accessibility issues they faced while participating in the
survey. We also requested that they suggest improvements
to our survey. The pilot study took around 40–60 minutes
for each participant. Participants were compensated with $20
cash for participating in the pilot.
We conducted the pilot study in two phases, interviewing
two participants at each phase. We identiﬁed any accessibility
issues in the ﬁrst phase and conducted the second phase with
the revised version. In the ﬁrst phase, participants reported
varying levels of accessibility issues they faced in the survey,
such as difﬁculties in navigating through the text ﬁelds, not
having a progress bar, and minor confusion about the wording
of some questions. We addressed the issues mentioned by
the participants after the ﬁrst phase and conducted the second
phase one week later. At this phase, the participants did
not raise any accessibility issues and thus we ﬁnalized the
survey. During the follow-up interview, participants also
suggested the modiﬁcation of the list of objects based on the
scenario, and we modiﬁed the existing objects based on their
suggestions.
3.2 Data analysis procedure
We now describe our quantitative and qualitative analysis
procedures.
3.2.1 Quantitative analysis
We used non-parametric versions for all of our statistical tests
as our data do not meet the assumptions of parametric tests,
such as normality and equal variance of errors. We have
one dependent variable (comfort level for sharing informa-
tion) and several independent variables (human assistants,
scenarios, objects). To analyze our data, we conducted an
overall Kruskal-Wallis test (for multiple groups and between
subjects), a Wilcoxon rank sum test (for two groups and
between subjects), a Friedman rank sum test (for multiple
groups and within subjects), and a Wilcoxon signed rank test
(two groups within subjects) across all conditions to see if
there was any signiﬁcant difference in the measured variables
among the conditions. We followed the Kruskal-Wallis tests
with a Dunn’s post hoc test with a Benjamini-Hochberg cor-
rection, where we compared speciﬁc pairs. For the Friedman
rank sum test, we performed a pairwise Wilcoxon signed rank
test as the post hoc test.
3.2.2 Sample size power analysis
We performed a power analysis to estimate the sample size
required to produce statistically signiﬁcant ﬁndings. The anal-
ysis showed that 50 participants per condition would provide
enough statistical power to detect 0.25 (‘small’) sized effects
(α= 0.05,1−β= 0.90).
3.2.3 Qualitative analysis
All qualitative answers were independently coded in a bottom
up approach by two researchers. The researchers met weekly
to iteratively and redundantly code a subset of open-ended
responses from the survey. Each subset comprised of a com-
bination of the audience and scenario. The researchers coded
each response into one of seven reasons for their information
sharing practices: ‘burden’ (does not want to bother family
or friends), ‘impression’ (does not want to feel embarrassed
or awkward), ‘indifferent’ (does not mind if information is
shared), ‘relevance’ (does not want to share any unneces-
sary information), ‘professionalism’ (does not want to share
with volunteers), ‘trust’ (has more faith in friends or family
members), and ‘security’ (does not want identity to be com-
promised). The researchers computed Cohen’s Kappa among
two raters for each subset, and discussed disagreements af-
ter coding a subset of qualitative data. After two rounds of
redundant coding, the researchers reached an acceptable aver-
age pairwise Cohen’s Kappa score of 0.8 or greater for each
subset combination of audience and scenario.
1934    29th USENIX Security Symposium
USENIX Association
4 Findings: Quantitative Analysis
We now present our quantitative ﬁndings based on our statis-
tical analyses. We ﬁrst report our participants’ demographics
relative to their technology usage. Next we present ﬁndings
about the types of content participants were selectively dis-
closing and concerns related to disclosure behavior. We then
present ﬁndings about the audiences participants were selec-
tively disclosing to and emergent issues related to audience
and disclosure. Finally we present additional factors that
affect information disclosure.
4.1 Demographics and technology usage
A total of 165 people participated in our survey, although
some participants did not complete the survey. After re-
moving the incomplete responses, our ﬁnal sample for the
study comprised 155 participants with visual impairments.
Of these participants, 54 received the ‘friends’ condition, 50
received the ‘family’ condition, and 51 received the ‘volun-
teer or crowd-workers’ condition. Of these 155 participants,
92 (59.4%) identiﬁed themselves as female and 63 (40.6%)
as male. Among our participants, 44 (29.3%) were between
18-to-34 years old, 50 (33.3%) participants were between
35-to-54 years old, and 56 (37.4%) of the participants were
55 years or older. As for their professional background, 56
(37.6%) participants reported being employed full-time, 31
(20.8%) as retired, 26 (17.4%) as unemployed and looking for
work, 24 (16.1%) as employed part-time, and 12 (8.1%) as
a student. Among the participants, 101 (61.2%) were totally
blind, whereas 64 (38.8%) live with different levels of VIP
such as ‘low vision’ and ‘blind in one eye and low vision
in the other.’ More than half of the participants, 96 (60.4%),
were visually impaired since birth, whereas the rest became
visually impaired afterward: 34 (21.4%) since childhood, 15
(9.4%) since early adulthood (18-40 years old), 11 (6.9%)
since middle adulthood (41-60 years old), and 3 (1.9%) since
late adulthood (61+ years old).
Participants also reported their use of various camera-based
assistive technologies and their assistance-seeking behaviors.
Some of the most popular assistive technologies used by the
participants were Seeing AI (80%), TapTapSee (70.3%), Be-
MyEyes (69.6%), and KNFB Reader (65.8%). Almost all
participants, 144 (96%), reported using assistive technologies
for more than a year. To explore the role of human assistance
in their lives, participants were asked whom they usually
asked for help and their purposes of seeking help from them.
The primary sighted supporters for people with VIPs are fam-
ily and friends (133, 80%), although a majority of participants
reported receiving help from volunteers or crowd-workers as
well (100, 65%). Only four (2.4%) participants reported never
seeking help from anyone, and we excluded their data from
the analysis. Participants also reported how they sought help
from sighted people: 122 (81.8%) for reading documents ,
101 (67.7%) for identifying objects, 95 (63.7%) for identify-
ing color, and 46 (30.8%) for seeking subjective opinions (e.g.
how the participant looked in new clothing).
4.2 Selective content disclosure
To understand whether the type of background content has
any effect on the sharing preference of users, we analyzed