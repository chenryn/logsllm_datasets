### Detection Systems and Data Sampling

Detection systems can generate malicious examples at a low cost, while random sampling can provide benign examples. However, the initial labeled dataset often lacks all the malicious families we aim to detect and is not representative of the deployment environment. ILAB (Interactive Labeling for Anomaly Detection) iteratively enriches the initial labeled dataset to make it more representative of the deployment environment. The iterations continue until the annotation budget \( B \) is exhausted.

At each iteration, \( b_{\text{uncertain}} \) annotation queries are generated using uncertainty sampling to improve the detection model. Additionally, \( b_{\text{families}} = b - b_{\text{uncertain}} \) instances are queried for annotation using rare category detection to avoid sampling bias (see Figure 3).

### 4.1 Uncertainty Sampling

A binary probabilistic detection model \( M \) is learned from the annotated instances in \( D_L \). We use a discriminant linear model, specifically logistic regression [10]. Linear models are preferred by computer security experts who distrust black-box models [27]. These models are interpretable because the coefficients associated with each feature represent their contribution to the detection model. Discriminant models are known to perform better than generative ones in active learning settings [47]. Furthermore, training and applying a logistic regression model is fast, reducing the expert's waiting time between iterations. Our approach is flexible, allowing the expert to choose a different model class if needed.

To enhance the impact of training instances from rare malicious families, the logistic regression model is learned with sample weights inversely proportional to the family's proportion in the training dataset:
\[ \beta(x, y, z) = \frac{|D_L|}{|\{(x', y', z') \in D_L \mid y' = y \land z' = z\}|} \]
The weights are capped at 100, \( \hat{\beta} = \min(\beta, 100) \), to prevent very rare families from having too much influence. This weighted learning is crucial for detecting rare malicious families effectively.

The model \( M \) computes the probability \( p(x) \) that an unlabeled instance \( x \in D_U \) is malicious:
\[ \forall x \in D_U, \quad p(x) = P_M(y = \text{Malicious} \mid x) \]

**Annotation Queries:**
The \( b_{\text{uncertain}} \) unlabeled instances closest to the decision boundary of \( M \) are selected for annotation:
\[ \arg \min_{x \in D_U} |p(x) - 0.5| \]
These instances are uncertain, and their annotations help improve the detection model. This step corresponds to uncertainty sampling [20], a common active learning method. However, uncertainty sampling can suffer from sampling bias [29]. To mitigate this, we also perform rare category detection.

### 4.2 Rare Category Detection

Rare category detection is applied separately to instances likely to be malicious and benign according to the detection model \( M \). Not all families are present in the initial labeled dataset, and rare category detection [26] helps discover unknown families to avoid sampling bias. It is necessary to run rare category detection on both predicted malicious and benign instances, as a whole malicious family might be misclassified (see Figure 2).

Let \( D_{\text{Malicious}}^L \) be the set of malicious instances already annotated by the expert. A multi-class logistic regression model is learned from \( D_{\text{Malicious}}^L \) to predict the family of the instances. Each family \( f \) is modeled with a Gaussian distribution \( N(\mu_f, \Sigma_f) \), depicted as an ellipsoid in Figure 3. The mean \( \mu_f \) and diagonal covariance matrix \( \Sigma_f \) are learned using Gaussian Naive Bayes [10].

**Annotation Queries:**
The family annotation budget \( b_{\text{families}} \) is evenly distributed among the different families. ILAB first asks the expert to annotate instances likely to belong to an unknown family:
\[ \arg \min_{x \in C_f \setminus D_{\text{Malicious}}^L} p_N(\mu_f, \Sigma_f)(x) \]
These instances are at the edge of the ellipsoid and have a low likelihood of belonging to the family \( f \).

Next, ILAB queries representative examples of each family:
\[ \arg \max_{x \in C_f \setminus D_{\text{Malicious}}^L} p_N(\mu_f, \Sigma_f)(x) \]
These instances are close to the center of the ellipsoid and have a high likelihood of belonging to the family \( f \).

Half the budget is allocated to low-likelihood instances, and the other half to high-likelihood instances. Low-likelihood instances are likely to belong to new families, fostering discovery. However, they may be outliers that impair the detection model. High-likelihood instances provide more representative examples, improving the generalization of the detection model.

### 5 Comparison with State-of-the-Art Labeling Strategies

### 5.1 Datasets

Labeling strategies are generic methods applicable to any detection problem once features are extracted. We consider two detection problems: (1) detection of malicious PDF files using the Contagio dataset, and (2) network intrusion detection using the NSL-KDD dataset. These datasets are non-representative of real-world data but are suitable for comparing labeling strategies.

**Contagio Dataset:**
- 11,101 malicious and 9,000 benign PDF files.
- Transformed into 113 numerical features similar to those proposed by Smutz and Stavrou [35,36].

**NSL-KDD Dataset:**
- 58,630 malicious and 67,343 benign instances.
- Each instance has 7 categorical and 34 numerical features, encoded into 122 binary features.

**Table 1: Description of Public Datasets**

| Dataset      | #instances | #features | #malicious families | #benign families |
|--------------|------------|-----------|---------------------|------------------|
| Contagio 10% | 10,000     | 113       | 16                  | 30               |
| NSL-KDD 10%  | 74,826     | 122       | 19                  | 15               |

Neither dataset has a realistic proportion of malicious instances. We uniformly sub-sampled the malicious class to get 10% of malicious instances, resulting in the Contagio 10% and NSL-KDD 10% datasets.

### 5.2 Labeling Strategies

We compare ILAB with uncertainty sampling [20], Aladin [40], and G¨ornitz et al.'s labeling method [14]. Since there are no open-source implementations, we implemented them in Python using scikit-learn [25]. All implementations are released for future research.

**Uncertainty Sampling [20]:**
- Trains a binary logistic regression model on labeled instances.
- Queries the \( b \) most uncertain predictions (closest to the decision boundary).
- No additional parameters.

**G¨ornitz et al. Labelling Strategy [14]:**
- Trains a semi-supervised anomaly detection model (SVDD) on labeled and unlabeled instances.
- Queries instances close to the decision boundary and with few malicious neighbors.
- Parameters: \( c, r, \gamma, \eta_U, \eta_L, \kappa, k, \delta \).
- Initial values: \( c \) (mean of unlabelled and benign instances), \( r \) (average distance to \( c \)), \( \gamma = 1 \), \( \eta_U, \eta_L \) (inverse of instance counts), \( k = 10 \), \( \delta = 0.5 \).

**Aladin [40]:**
- Runs rare category detection on all data.
- Queries uncertain instances between families and low-likelihood instances.
- No additional parameters.
- Uses multi-class logistic regression and Gaussian Naive Bayes.

**ILAB:**
- Additional parameter: \( b_{\text{uncertain}} \) (set to 10% of \( b \)).
- Balances querying instances near the decision boundary and avoiding sampling bias.
- Models trained similarly to Aladin.

### 5.3 Results

The datasets are split into an active learning dataset (90%) and a validation dataset (10%). Simulations are run with an oracle providing ground truth labels and families. All strategies are run with \( b = 100 \) annotations per iteration. The annotation budget is \( B = 1000 \) for Contagio 10% and \( B = 2000 \) for NSL-KDD 10%.

Experiments are conducted on a dual-socket computer with 64GB RAM and Intel Xeon E5-5620 CPUs (2.40 GHz, 4 cores, 2 threads/core). Each strategy is run 15 times, and average performance with 95% confidence intervals is reported.

**Comparison Metrics:**
- Number of known families across iterations (Figure 4a).
- Performance of detection models on the validation dataset (Figure 4b).
- Execution time of query generation algorithms (Figure 4c).

**Figures:**
- Figure 4a: Number of known families over iterations.
- Figure 4b: Detection model performance on the validation dataset.
- Figure 4c: Execution time of query generation algorithms.