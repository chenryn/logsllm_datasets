回归类问题实际上有很多相同之处，最大的区别就在于它们的因变量不同，其他的基本都差不多。正是因为如此，这些回归可以归于同一个家族，即广义线性模型（generalizedlinear model）。
这一家族中的模型形式基本上都差不多，不同的就是因变量不同：
* 如果是连续的，就是多重线性回归；
* 如果是二项分布，就是Logistic回归；
* 如果是Poisson分布，就是Poisson回归；
* 如果是负二项分布，就是负二项回归。
Logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的Logistic回归。
Regression问题的常规步骤为：
. 寻找h函数（即hypothesis）
. 构造J函数（损失函数）
. 想办法使得J函数最小并求得回归参数（θ）
==== LogisticRegression
关于逻辑回归的wiki：
预测函数h::
Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：
+
image::images/ml-sigmoid-function.png[]
Sigmoid图形::
+
image::images/ml-sigmoid-graph.png[]
预测函数::
+
image::images/ml-logistic-regression-prediction-function.png[]
+
此处hƟ(x)的值得含义表示结果取1的概率，所以对于输入x分类结果为类别1和类别0的概率分别为：
+
image::images/ml-logistic-regression-classification-probability.png[]
损失函数J::
+
image::images/ml-logistic-regression-loss-J.png[]
之后就是用梯度下降法求使J函数最小的一列Ɵ值，为了提高计算效率，简化计算过程，会对数据进行向量化，使用矩阵运算来代替循环运算。
求得Ɵ值后有可能会产生过拟合的问题，主要原因是选取了过多的特征进行拟合。应运而生的减少过拟合的方法就是人为减少特征数量和用正则化的惩罚系数。
Syntax：
    fit LogisticRegression [algo_params]  from  [into model_name]
Parameters:
* ：使用本模型对目标字段进行训练，必选
*  ：使用这些特征字段对目标字段进行分类和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对LogisticRegression模型的参数设置，可选，不填时用默认参数设置
** penalty : str, ‘l1’ or ‘l2’, default: ‘l2’
+
Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.
** dual : bool, default: False
+
Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.
** C : float, default: 1.0
+
Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.
** fit_intercept : bool, default: True
+
Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.
** intercept_scaling : float, default 1.
+
Useful only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic_feature_weight.
+
Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.
** max_iter : int, default: 100
+
Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge.
** random_state : int seed, RandomState instance, default: None
+
The seed of the pseudo random number generator to use when shuffling the data. Used only in solvers ‘sag’ and ‘liblinear’.
** solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}, default: ‘liblinear’
+
Algorithm to use in the optimization problem.
+
For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ isfaster for large ones.
+
For multiclass problems, only ‘newton-cg’, ‘sag’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.
+
‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty.
+
Note that ‘sag’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.
** tol : float, default: 1e-4
+
Tolerance for stopping criteria.
** multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’
+
Multiclass option can be either ‘ovr’ or ‘multinomial’. If the option chosen is ‘ovr’, then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Works only for the ‘newton-cg’, ‘sag’ and ‘lbfgs’ solver.
** verbose : int, default: 0
+
For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.
** warm_start : bool, default: False
+
When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver.
** n_jobs : int, default: 1
+
Number of CPU cores used during the cross-validation loop. If given a value of -1, all cores are used.
样本数据分析：样本中是2016年8月13日Dota2游戏全球各个服务器中2小时以内所有游戏比赛的结果数据。Dota2是一款10人同时在线并且5vs5的多人即时对战的网络游戏，每次的获胜方为首先摧毁对方基地建筑的那一方（天辉or夜魇），数据中用1或-1来表示。
1. Team won the game (1 or -1)
2. Cluster ID (related to location)
3. Game mode (eg All Pick)
4. Game type (eg. Ranked)
对属性分析可以看出我们应该用feature2、3、4来训练并预测第一个属性也就是比赛结果，即获胜方是那一边
训练集样本量一共65500条，首先使用训练集的数据进行fit
  appname:ml_spl AND tag:binary_classification_ly_train | where !empty(json.TeamWon) | fit LogisticRegression json.TeamWon from json.GameType, json.GameMode, json.ClusterID into logistic_train | table json.TeamWon, predicted_json.TeamWon
image::images/ml-logisticregression-fit-example.png[]
测试集样本量一共10294条，使用测试集进行apply
  appname:ml_spl AND tag:binary_classification_ly_test | where !empty(json.TeamWon) | apply logistic_train | table json.TeamWon, predicted_json.TeamWon
image::images/ml-logisticregression-apply-example.png[]
==== SGDClassifier
关于随机梯度下降的wiki：
梯度下降（GD）是最小化风险函数、损失函数的一种常用方法，随机梯度下降分类器即用随机梯度下降法来训练线性分类器，所以当它的参数loss选定为某些值时其训练结果可以等同于其他线性分类器，如当loss为log时，其训练结果就是一个logisticregression分类器。
随机梯度下降的思路如下：
image::images/ml-sgd-cost.png[]
用损失函数对Ɵ求偏导，用每个样本来更新这个Ɵ：
image::images/ml-sgd-model.png[]
随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。
Syntax：
    fit SGDClassifier [algo_params]  from  [into model_name]
Parameters:
* ：使用本模型对目标字段进行训练，必选
*  ：使用这些特征字段对目标字段进行分类和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对SGDClassifier模型的参数设置，可选，不填时用默认参数设置
** loss : str, ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’
+
The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM. The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.
** penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’
+
The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.
** alpha : float
+
Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to ‘optimal’.
** l1_ratio : float
+
The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.
** fit_intercept : bool
+
Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.
** n_iter : int, optional
+
The number of passes over the training data (aka epochs). The number of iterations is set to 1 if using partial_fit. Defaults to 5.
** shuffle : bool, optional
+
Whether or not the training data should be shuffled after each epoch. Defaults to True.
** random_state : int seed, RandomState instance, or None (default)
+
The seed of the pseudo random number generator to use when shuffling the data.
** verbose : integer, optional
+
The verbosity level
** epsilon : float
+
Epsilon in the epsilon-insensitive loss functions; only if loss is ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’. For ‘huber’, determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold.
** n_jobs : integer, optional
+
The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means ‘all CPUs’. Defaults to 1.
** learning_rate : string, optional
+
The learning rate schedule:
+
*** ‘constant’: eta = eta0
*** ‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]
*** ‘invscaling’: eta = eta0 / pow(t, power_t)
+
where t0 is chosen by a heuristic proposed by Leon Bottou.
** eta0 : double
+
The initial learning rate for the ‘constant’ or ‘invscaling’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.
** power_t : double
+
The exponent for inverse scaling learning rate [default 0.5].
** warm_start : bool, optional
+
When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
** average : bool or int, optional
+
When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.