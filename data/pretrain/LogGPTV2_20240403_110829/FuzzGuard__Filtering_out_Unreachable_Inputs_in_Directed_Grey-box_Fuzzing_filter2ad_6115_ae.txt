1.7
2.0
1.9
1.7
2.0
2.0
1.0
1.0
1.0
1.0
1.0
1.0
1.1
1.0
1.1
1.0
1.4
1.0
4.8
1.7
2.6
time with TAFLGo. T+FG can be calculated as follows:
T+FG = TAFLGo −∑i∈If iltered
ti + TFG
where If iltered is the inputs ﬁltered out by FuzzGuard and ti
stands for the time spent on executing the target program with
the input i.
Note that, the last input in IAFLGo is the ﬁrst PoC generated
by AFLGo (if the target program crashes, e.g., #1 and #2 in
Table 1) or the last input generated by AFLGo before timeout
(no crash happens, e.g., #8 and #9 in Table 1). We emphasize
that FuzzGuard does not know whether a given input is the last
one or not. In the fuzzing process, FuzzGuard treats the last
input in the same way as the previous inputs. Comparing to
FuzzGuard, a method randomly dropping inputs in IAFLGo will
randomly decide to drop the last input or not. From Table 1
we can see that FuzzGuard drops 65.1% inputs on average. If
the same number of inputs (65.1%) is dropped by the random
method, the last input (a possible PoC, e.g., #1 and #2 in
Table 1) could also be dropped with the possibility of 65.1%.
In contrast, the false negative rate of FuzzGuard is 0.02%
(see Section 6.3), which means that even if 65.1% inputs are
dropped by FuzzGuard, the possibility of dropping the PoC
is only 0.02%.
Landscape. The results are shown in Table 1. Those 45 bugs
in Table 1 include 27 CVEs found in the last 3 years and
USENIX Association
29th USENIX Security Symposium    2263
18 newly undisclosed bugs (see Section 6.5). In our evalu-
ation, the undisclosed bugs (e.g., Line 6 in Table 1) were
found when FuzzGuard performing target fuzzing on other
vulnerabilities (e.g., CVE-2017-17501, Line 4 in Table 1).
Note that the buggy code of this undisclosed bug is actually
not our target in this process. Then, we set the newly found
buggy code as the target and tried to utilize AFLGo to repro-
duce it. Unfortunately, in the time limit (200 hours), AFLGo
failed to trigger the bug. Neither could AFLGo+FuzzGuard
trigger the bug. However, AFLGo+FuzzGuard did save the
time from 200 hours to 23.4 hours (8.5 times speedup).
From the table, we ﬁnd that for all the bugs, FuzzGuard
can increase the runtime performance of AFLGo from 1.3×
to 17.1× (see the “Speedup” column in Table 1, where
Speedup = TAFLGo/T+FG). The average performance is in-
creased by 5.4×. Note that such performance boost is added
to a DGF (i.e., AFLGo) which has already been optimized.
Understanding the performance boost. To understand the
performance of FuzzGuard for different programs and bugs,
we further study the relationship among the speedup, the time
that the model starts to train and the ratio of unreachable
inputs, etc.
we measure the number of unique functions and constraints5
in the path to each bug in Table 1. From the table, we can see
that the average number of unique functions and constraints
are 15.5 thousand and 315.9 million, respectively. Over 50%
of the bugs are guarded by thousands of constraints (e.g.,
the bugs in GraphicsMagick and ImageMagick). For these
bugs, FuzzGuard achieves the speedup from 1.4 to 15.9. For
some bugs guarded by millions constraints (e.g., #13 and #18
in Table 1), FuzzGuard achieves over 10× speedup. The re-
sults show that FuzzGuard can handle complicated functions
well, which could be quite time-consuming for traditional
constraint solving.
Cost. In our evaluation of the 45 bugs in Table 1, the time
spent on training the online model is 60 minutes on average,
which includes 13.5% for data collection, 0.5% for data em-
bedding and 86% for the training process. Note that the time
spent on training only takes 6% of the time for input genera-
tion by the fuzzer (15 hours on average). The total time spent
by FuzzGuard is 1.4 hours on average, which only takes 9.2%
of the total time of the fuzzing (T+FG in Table 1) and 2.5%
of the total time of the fuzzing process performed by AFLGo
(TAFLGo in Table 1). Such a time period is enough for a fuzzer
to process 704 thousand inputs, which is far more efﬁcient
than directly executing the target program for testing.
Figure 3: Start time of the ﬁrst training in FuzzGuard.
• The earlier the model is trained, the more time could be
saved. Figure 3 shows the time that each model starts to be
trained for the bugs in Table 1 (the red bar). We can see that
the model trained later (e.g., #20, #24, #27) achieved no more
than 3.3× speedup, while the model trained earlier could
achieve over 17× speedup. This is mainly because the earlier
the buggy node gains balanced labeled data, the earlier the
model can be trained for ﬁltering out unreachable inputs to
the buggy code. As a result, more inputs could be ﬁltered out
for saving the time on unnecessary executions.
• The more reachable inputs generated by the carrier fuzzer,
the less effective FuzzGuard is. For example, as shown in
Table 1, when more than 40% of the inputs are reachable (the
column “UR." is the ratio of unreachable inputs), the speedup
gained by FuzzGuard is less than 2 times (e.g., the bug #1,
#12 and #45 in Table 1). In a special case, if there are no
if-statements or constraints in the path from the entry point to
the target buggy code, all the generated inputs are reachable.
So there is no need to train a deep learning model.
Complicated Functions. To evaluate FuzzGuard on handling
complicated functions with multiple constraints and branches,
Figure 4: Evaluation on execution time in fuzzing process.
To understand the upper limit of of the fuzzing time that
FuzzGuard could save, we perform a 24-hour fuzzing on 45
vulnerabilities (shown in Table 1) using AFLGo. From Fig-
ure 4, we can see that the average execution time of the target
program is over 88% of the total time of fuzzing, which means
that the average upper limit of the fuzzing time that Fuzz-
Guard could save is about 88%. The time cost of FuzzGuard
should be less than the limit.
6.3 Accuracy
We measure the accuracy of FuzzGuard. The accuracy is
based on whether the reachability is correctly judged. The
more accurate it behaves, the more unreachable inputs could
be ﬁltered out. Note that no PoC will be missed since the
ﬁltered inputs will be saved in the PUI, which will further
5As it is very hard to check whether a constraint is dependent on inputs due
to inaccuracy of taint analysis, we count the number of all unique constraints.
Such problem also happens in symbolic execution.
2264    29th USENIX Security Symposium
USENIX Association
be checked by an updated model. A more accurate model
may ﬁnd the reachable ones in the pool and let the target
program execute with them, which in theory will not have
false negatives. However, in real execution, we usually set a
timeout for fuzzing. In this case, if a false negative input is left
in the pool without being found before the timeout, it will be
missed. Fortunately, in our evaluation of the 45 bugs, no PoC
is found in the PUI due to the accurate model. We deﬁne false
positive rate as follows: f pr = Nf p/Nn × 100%, where Nn
represents the number of the unreachable inputs generated by
AFLGo, and Nf p is the number of inputs that cannot reach the
buggy code but be viewed as reachable ones by FuzzGuard.
The false negative rate is: f nr = Nf n/Np × 100%, where Np
represents the number of the reachable inputs, and Nf n is the
number of reachable inputs but be ﬁltered out by FuzzGuard.
The higher the f pr, the more time is spent on executions with
unreachable inputs. The higher the f nr, the more likely the
PoC is executed late in the fuzzing. The accuracy is calculated
by acc =
Np+Nn−Nf p−Nf n
.
Np+Nn
Figure 5: The accuracy of FuzzGuard.
From Figure 5, we can ﬁnd that FuzzGuard is very accurate
(ranging from 92.5% to 99.9%). The average accuracy is
98.7%. The false positive rate for all vulnerabilities is 1.9%
on average. Note that false positives do not let a PoC be
missed. Neither do they increase the time spent on executing
the inputs (such inputs are always executed by the program if
there is no FuzzGuard). The false negative rate is negligible,
which is 0.02% on average. There are only 4 vulnerabilities
that have false negatives, and the highest one is 0.3%. We
further check those false negatives manually and conﬁrm that
there is no PoC in those inputs. Even if a PoC is included, as
mentioned previously, FuzzGuard will save it to the PUI for
further testing by updated models (no PoC will be missed).
Such an accurate model enables FuzzGuard to have high
performance.
The main reason for false positives and false negatives is
due to lack of balanced representative data. For example, an
unreachable input could be predicted by FuzzGuard as reach-
able (i.e., a false positive) if it is similar enough to previous
reachable inputs. The execution path of the input can also
be similar to the path to the buggy code (covering some pre-
dominating nodes of the buggy code). But some bytes in the
input stop the execution to the buggy code eventually. A false
negative may let the program reach the target buggy code
through an execution path that is never seen before. If those
new execution paths could be learned by the model, the pre-
diction will be more accurate. In our evaluation, the number
of unseen paths becomes less after long-time fuzzing, which
is probably the reason for the low false positive rate.
6.4 Contribution of Individual Techniques
To investigate the individual contribution of the step-
forwarding approach and the representative data selection,
we measure the performance boost with and without each
technique for all the bugs in Table 1. In particular, to be fair in
the comparison, for each bug to test, we use the same sequence
of inputs. We ﬁrst perform the evaluation without the step-
forwarding approach, and record the performance increase
(column FG1 in Table 1). Then we do not use representa-
tion data selection and record the corresponding performance
increase (column FG2 in Table 1). The results indicate that
FuzzGuard (with both the two techniques) can gain 5.4×
speedup compared to the vanilla AFLGo implementation,
while FuzzGuard without step-forwarding and FuzzGuard
without representative data selection can gain only 2.6× and
4.4× speedup, respectively.
We also made further analysis. As we know, the step-
forwarding approach is designed to help FuzzGuard to get
balanced data earlier in the fuzzing process, further to let
the training process start earlier. So we want to measure how
much step-forwarding can help. We record the start time of the
ﬁrst training with and without the step-forwarding approach
(see Figure 3). The x-axis in the ﬁgure shows the bug index in
Table 1, and the y-axis gives the start time in hours. From the
ﬁgure, we ﬁnd that if step-forwarding is not used, FuzzGuard
fails to start the training process for 14 bugs (e.g., #5 , #6
and #7) due to lack of balanced data. For other bugs, even if
the training process starts, the time of start will be postponed
by 17.4 hours on average compared with the model using
step-forwarding. This also postpones the ﬁltering process and
ﬁnally impacts the overall performance.
Regarding representative data selection, we also measure
its impact on the accuracy of the model. For each bug, we
record the model’s accuracy with and without using represen-
tative data selection. The results are shown in Figure 5. The
x-axis shows the bug index and y-axis gives the accuracy of
the model. From the ﬁgure, on average, representative data
selection increases the accuracy by 4.4%. For some cases
(#14, #21 and #40 in Figure 5), the accuracy of the model
decreases dramatically without representative data selection.
Based on the individual evaluations above, we ﬁnd that Fuzz-
Guard needs both step-forwarding and the representative data
selection for efﬁciency and accuracy.
6.5 Findings
Interestingly, in our evaluation, we ﬁnd 23 undisclosed bugs
(4 of them are zero-day vulnerabilities). Note that the buggy
code of the undisclosed bugs is actually not our target. The
USENIX Association
29th USENIX Security Symposium    2265
goal of FuzzGuard is to increase the efﬁciency of fuzzing
by removing unreachable inputs, instead of triggering new
bugs. All the bugs found by FuzzGuard+AFLGo could even-
tually be discovered by AFLGo. The undisclosed bugs are
patched in the new versions of the corresponding programs.
For the four zero-day vulnerabilities, we successfully gain the
CVE numbers6. The vulnerabilities are triggered when we
perform target fuzzing on other vulnerabilities. For example,
CVE-2018-20189 is found in the fuzzing process of CVE-
2017-17501; and CVE-2019-7663 is found in the fuzzing
process of CVE-2016-10266. Also, we discover CVE-2019-
7581 and CVE-2019-7582 when verifying CVE-2016-9831.
After manually analyzing the undisclosed bugs and zero-day
vulnerabilities, we ﬁnd that their locations are quite near the
buggy code (i.e., the destination in targeted fuzzing). For ex-
ample, List 2 and List 3 show the call stacks of triggering
CVE-2017-17501 and CVE-2018-20189 respectively. The
ﬁrst 8 pre-dominating nodes are the same for both the two
call stacks, while only the last basic blocks differ. We guess
the code near the buggy code could be more likely to contain
a new bug7.
7 Understanding
Our evaluation results show that FuzzGuard is highly effec-
tive to ﬁlter out unreachable inputs, with an average accuracy
of 98.7%. We want to understand from the features why Fuzz-
Guard has such a good performance. If the learned features by
FuzzGuard are reasonable, the results of FuzzGuard are also
understandable. To achieve this goal, our idea is to extract the
features from the model and analyze them manually. How-
ever, as we know, the high-dimensional features extracted by
the deep neural network are hard to be understood directly.
Inspired by saliency maps [8], our idea is to project the fea-
tures to individual bytes (referred to as the key features), and
to check whether the key features could impact the execution
of the target program.
In particular, to get the key features, we design a mask-
based approach to obtain the corresponding key bytes of an
input used by the model. The basic idea is as follows: we
use a mask (i.e., a vector with the same length as the input)
to cover the bytes of the input x (the covered ﬁelds are set
to 0). If the covered input has the same prediction result
as the uncovered one (i.e., f (mask · x) = f (x), where f is
the CNN model used by FuzzGuard), the covered ﬁelds will
not impact the prediction result, which means that they are
not the key features. By increasing the number of covered
ﬁelds in the input step by step, we could acquire all the key
features in the end. The mask at this time is referred to as the
maximum mask. For example, an input is shown in Figure 6.
The mask sets the value of the shaded part of the input to
0. When f (x) = f (m · x), the shaded part will not impact
6CVE-2018-20189, CVE-2019-7581, CVE-2019-7582, CVE-2019-7663.
7One reason could be that both the two pieces of code are written by the
same developer.
the reachability of the input x. So we shade more bytes and
iterate this process. The problem here is that the covered
ﬁelds have too many combinations. So our idea is to leverage
gradient descent to calculate the maximum mask. In particular,
we adjust the mask according to the deviation between the
predicted label yp and the real label y of x until yp = y. To
utilize this approach, we design a loss function that considers
not only the deviation between the predicted and actual values,
but also the coverage rate in the mask as follows:
i − yi)2
∑m
i=1(yx
m
∑n
i=1 maski
loss =
+
n
where n is the number of bytes of mask and m is the length of
y mentioned in Section 4.3. When the gap between yp and y
is minimal and the number of covered bytes is maximum, the
uncovered bytes in x are the key features, which are the ﬁelds
in the input affecting the reachability viewed by FuzzGuard.
In this way, the key features could be compared with the
constraints in the target program to check whether the key
features can really impact the execution.
Figure 6: A PoC of CVE-2018-20189.
For example, the PoC (a PNG ﬁle) of CVE-2018-20189
is shown in Figure 6. The key features in this PoC are un-
shaded. After manual analysis, we verify that the ﬁeld from
offset 0x0e to 0x0f (bits_per_pixel in List 1) in the in-
put decides the execution direction of the branch in Line
6; and the ﬁelds from offset 0x0c to 0x0d (number_colors
in List 1) in the input impact the execution. For example,
when bits_per_pixel  8. Through the above analysis, we can
conﬁrm that the key features do affect the reachability of the
input, which means that the model successfully captures the
ﬁelds as features when the number of such inputs is enough