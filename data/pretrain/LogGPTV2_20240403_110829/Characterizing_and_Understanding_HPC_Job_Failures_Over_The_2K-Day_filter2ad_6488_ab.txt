from the perspective of users, studying only RAS fatal events
is not enough, because many fatal events may not actually
affect the jobs’ execution. As mentioned in Di et al.’s analysis
work [2], the fatal events recorded in the RAS log refer to
potential issues, which could be ﬁxed by the system’s self-
healing mechanism automatically in time or may not happen
to any submitted jobs at runtime. Accordingly, to understand
the real impact of system’s fatal events on users’ jobs, we must
investigate the job-scheduling log and task execution log. In
addition, to understand the I/O behaviors of the failed jobs,
we combine the Darshan I/O behavior log with the job log.
Based on the Cobalt job-scheduling log, we analyze sta-
tistical features such as the distribution of jobs based on
exit statuses for both normal jobs and failed jobs. We also
characterize the distributions for speciﬁc metrics such as job
queuing time, execution time, and I/O behavior; and we ex-
plore the best-ﬁt distribution of a job’s execution time by using
the maximum likelihood estimation (MLE) method. These
analyses disclose the job failure features from the perspective
of users, leading to signiﬁcant beneﬁts: (1) helping system
administrators understand the behavior and root cause of the
4
RAS LogTask Execution LogCobalt Job Scheduling LogDARSHAN I/O LogCorrelation between fatal events and failed jobs (Section VI)Overall Job features & Job failure features (Section V)Analysis of Fatal Events (Section VI)Map RAS events to tasksMap signals to jobsMap I/O behaviorsto jobsfailed jobs more deeply, as well as make a comparison with
the normal jobs; and (2) helping fault tolerance researchers or
users understand the system reliability to user jobs and MTTI
in order to improve fault tolerance for their applications.
We also explore how the jobs’ normal termination and dif-
ferent types of abnormal terminations correlate with other sig-
niﬁcant attributes and metrics, including wall time requested,
real execution time, number of nodes used, total core-hours,
user name, project name, queue name, allocated resource
locations, and machine partition. We construct the contingency
tables1 for the attributes and metrics versus a job’s exit status.
A contingency table contains rich information related to the
mutual correlation, based on which one can understand the
detailed frequency for each value-combination, so we mainly
demonstrate the correlations using the contingency tables in
our analysis. In addition, we adopt a χ2 statistic analysis to
assess whether some attribute or ﬁeld is likely correlated to
the exit status. Speciﬁcally, if the χ2 statistic calculated is
greater than the critical value (with conﬁdence level of 99.9%)
from the Chi-Square distribution, we can claim that the two
categories are non independent.
We split the attributes into two categories—identity type and
number type—that are coped with separately in our analysis.
The identity-type attributes each have relatively low numbers
of values, and each value can be represented in the form
of a string text. For instance, user name, project name, and
machine partition all belong to the identity-type category. In
our analysis, we build a hash table for each identity-type
attribute and calculate the corresponding probability regarding
different execution exit statuses. By comparison, the number-
type attributes (such as execution time and core-hours) are
the ﬁelds whose values are recorded in the form of numbers
(e.g.,
integer values or ﬂoating-point values). Because of
the unlimited or numerous values for each of the number-
type attributes, we need to split their values into consecutive
intervals in log scale, which can signiﬁcantly improve the
analysis efﬁciency because of the considerably reduced time
complexity and memory overhead. The execution times, for
instance, would be split into 10 intervals: [0, 10 minutes), [10
minutes, 20 minutes), [20 minutes, 40 minutes), ··· for our
probability analysis. Table III lists all the four number-type
attributes and their consecutive intervals in our study.
TABLE III
CLASSIFICATION POLICIES OF NUMBER-TYPE ATTRIBUTES
Attribute
Policy
# Intervals
real execution time
[0,10m),[10m,20m),[20m,40m], · · · , [85.3h,∞]
10
10
10
10
core-hours
[0,1000), [1000,2000),[2000,4000), · · · , [512,000, ∞)
[0], [1,10], [11,20], [21,30],· · · , [641,1320], [1320,∞)
[0], [1,10], [11,20], [21,30],· · · , [641,1320], [1320,∞)
# consecutive tasks
# multilocation tasks
We also explore the correlation between RAS fatal events
and job exit status. The RAS log has many duplicated mes-
sages, so we have to perform duplication ﬁltering before
analyzing the system fatal events. To this end, we adopt the
weighted-similarity-based spatiotemporal message ﬁlter that
was developed in the open-source LogAider tool [31]. We
1A contingency table is in the form of matrix that displays the frequency
distribution (i.e., the number of the value combinations) across two ﬁelds.
5
also improve the ﬁltering ability by excluding fatal events
occurring during the system maintenance period and by taking
into account the system reservation periods marked by the
system administrator. The total number of fatal events can
be reduced to 1,299 compared with originally 2.6 million
duplicated fatal messages in the 2001-day logging period. The
mean time between fatal events (MTBFE) is about 1.54 days.
Note that the fatal events here do not represent system failures
or interruptions from the perspective of users but represent
all the “potential” severe issues of the system. In fact, some
fatal events may not affect user jobs at all, although they
really caused malfunctions to some parts of the system. Using
our elaborate mapping from RAS events to job failures, we
calculate the MTTI to be 3.5 days for the whole Mira system
in terms of user jobs. We also identify the locality features of
system’s fatal events that affect the job executions.
V. EXPLORATION OF JOB FAILURE PROPERTIES
job features,
In this section, we explore the job failure properties by
investigating the Cobalt job log, task execution log, and I/O
behavior log. In addition, we compare the overall job features
and normal
in order to identify the speciﬁc
features of the failed jobs and their correlations with other
metrics. We highlight takeaways/lessons in the following text.
A total of 377,144 jobs were submitted or scheduled during
the 5.5 years considered for this study. Based on our analysis
using the task execution log, we classiﬁed 99,245 of these
jobs as abnormal, although 116,787 jobs terminated with
nonzero exit statuses according to the Cobalt job-scheduling
log. We summarize the 10 most frequent types of terminations
(including normal jobs) in Table IV. We observe that about
three-fourths of the jobs exited normally in the end. Of the
one-fourth of the jobs that terminated abnormally, a large
majority, 55.8%, were killed by the system because of timeout
(i.e., execution time exceeding the wall time requested). This
cause differs signiﬁcantly from the characterization of Blue
Waters jobs [8], of which 14% of the failed jobs were due to
timeout. The jobs that exited because of code bugs constituted
7.25%, and the jobs terminating because of problematic script
or operation (e.g., no such ﬁle/directory) constituted about
0.94%. The task execution log records the related RAS event
ID for any job that is failed because of system reliability and
also the corresponding cobalt job ID, based on which we
can calculate the system-reliability related job failures. Our
characterization shows only 0.17% failed jobs were due to
system reliability, representing about 0.6% of all the failed
jobs. (Takeaway 1): A large majority (99.4%) of
job
failures were attributed to user behaviors instead of system
reliability. This is compared to other reports (98.5% in Blue
Waters [8] and 97% in Franklin (at NERSC) [32]).
In the following text, we divide job termination types into a
ﬁne granularity (9 categories) and determine their correlations
with many important attributes, including user name, project
name, job’s execution structure (core-hours, execution scale,
job length), resource allocation, I/O behavior, and RAS event.
TABLE IV
TASKS’ EXIT STATUS AND DESCRIPTION
Percentage
73.79%
14.62%
7.35%
2.84%
0.94%
0.17%
0.13%
0.065%
0.062%
0.014%
Description
normal job termination (e.g., with exit code 0).
Timeout, i.e., killed due to exceeding the wall ime.
termination because of serious bugs (such as SIGABRT and SIGSEGV).
SIGKILL signal: killed in the mid of execution.
I/O-related issue, such as ’`no such ﬁle/directory’ and ‘permission issue’
jobs killed by system’s RAS fatal event.
abnormal termination with the exit signal 36.
signal 4: illegal instruction (e.g., mismatched CPU architecture or permission issue)
signal 5: caught exceptions due to possible bugs during debugging
signal 8: erroneous arithmetic operation, such as division by zero
project. The χ2 signiﬁcance value based on Table V, for
example, is calculated as 50,026. Hence, we have the following
takeaway. (Takeaway 3): Users and projects exhibit a strong
correlation with the exit statuses with a conﬁdence level
of 99.9% (corresponding to the critical value of 126.1),
indicating that users and projects often fail with speciﬁc
reasons (or exit codes) particularly.
Root Cause
Normal
Timeout
Bug
Kill
IO
RAS
Unknown
SIGILL
SIGTRAP
SIGFPE
TABLE V
CONTINGENCY TABLE WITH USER NAME AND EXIT CODE
PPPPPP
user
exit
NM
26689
26668
13062
1963
6555
5139
4719
5536
5452
4132
NM
26951
19765
16168
15525
8407
6764
6028
4774
5755
4931
TO
348
68
519
3436
228
64
1173
167
103
484
TO
679
431
194
428
99
501
274
831
203
847
TABLE VI
BG
17
7
304
1647
43
1310
27
91
12
775
BG
74
50
54
117
88
706
389
907
306
81
KL
31
62
716
73
99
41
83
79
14
143
KL
52
554
173
134
12
151
110
144
55
120
u1
u2
u3
u4
u5
u6
u7
u8
u9
u10
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
IO
59
13
81
139
31
0
150
4
49
20
RS
1
7
50
23
25
5
3
0
1
0
UK
1
5
0
0
0
20
0
1
0
0
IO
75
48
43
52
66
87
8
20
21
22
RS
2
40
25
40
1
1
3
2
4
6
UK
0
4
2
0
0
3
22
0
11
0
SI
0
4
0
0
0
0
0
3
0
0
SI
0
0
4
0
0
15
5
0
3
4
ST
0
0
0
0
0
0
0
0
0
0
ST
3
0
0
0
0
9
0
0
0
1
SF
0
0
0
0
0
0
0
0
0
0
SF
0
0
0
0
0
4
0
0
0
0
This analysis differs from other work [8] focusing mainly on
overall statistics regarding resource usage (such as core-hours).
A. Features based on Users and Projects
Mira had a total of 1,295 users and 627 projects throughout
the 2,001 days of usage. To understand their distinct features,
we characterize the distribution of the number of jobs and
core-hours across users and projects, as shown in Fig. 2. In
the ﬁgure, we observe that the job counts and core-hours
per user and project are largely different, following a typical
Pareto principle, or 80/20 rule (i.e., a large majority of the
jobs or core-hours are actually attributed to only a very
small population). In absolute terms, we have the following
takeaway. (Takeaway 2): Only 15% of users contribute
80.4% jobs and 88% core-hours, which means that the
job count features actually roughly follow an 85/15 rule.
(a) Distribution of Job Count
(b) Distribution of Core-Hours
Fig. 2. Job Count/Core-Hours Based on Users/Projects
In addition, our characterization shows that for most users
and projects, the number of normal jobs is about 1.5∼2X as
high as the number of failed jobs, and job failure ratios (i.e.,
the ratio of failed job count to total job count per user/project)
can differ signiﬁcantly with user/projects. Speciﬁcally, about
12% users have 0 failures and about 35% users suffer from
a failure ratio of 50+%. Hence, it is worth investigating the
failed job features based on users and projects. We will discuss
this issue with speciﬁc exit codes in the following text.