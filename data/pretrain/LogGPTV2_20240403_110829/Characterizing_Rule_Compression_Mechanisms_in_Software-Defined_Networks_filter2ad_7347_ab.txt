 65536
 16384
 4096
 1024
 256
 64
 16
 4
/32
/28
/24
/20
/16
 0
 20
 40
 60
 80
 100
 120
 140
 160
Time (minutes)
(b)
Fig. 1. (a) Maximum number of concurrent rules as we vary the timeout after which
rules expire and (b) average number of rules over time as we vary the IP preﬁx size
from /32 to /16.
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
307
Abilene trace have their last 11 bits zeroed out. These two datasets are typical
for two important OpenFlow switch usage scenarios: at the edge and at the core
of a network.
Rule Generation. Since neither of the two networks where the traces were
collected is OpenFlow-enabled, we simulate the operation of an OpenFlow net-
work to determine the set of rules that would be installed to handle the traﬃc.
We ﬁrst identify all ﬁve-tuple ﬂows (src IP, dst IP, src port, dst port, protocol)
in each dataset and assume that each ﬂow must be handled by one rule (i.e.,
with no wildcards). We create matches on ﬁve ﬁelds rather than all 12 supported
by existing OpenFlow switches because these are the ﬁelds for which our traces
include information. We assume a switch with a single ﬂow table, conforming to
OpenFlow v1.0, which is implemented on most switches on the market.
Table 2. Comparison of various rule management methods. For each method, we show
for both datasets the maximum number of concurrent rules and the 95th percentile
value (across minutes) of operations per second. Percentages for number of rules and
ops/sec are in comparison to the default OpenFlow operation of using a 60 s timeout (for
the manual techniques) and to the IP-only rules with 60 s timeout (for the automatic
aggregation).
Campus
# rules
no mgmt. 60 s timeout 115 K
11 K
Timeouts (Sect. 4.2)
ops/sec
46
176
Abilene
# rules
12 M
100.5 K
ops/sec
1255
2800
- 30 s
- 10 s
- 5 s
Match ﬁelds (Sect. 4.3)
7,982 (–27 %) 200 (+14 %) 53 K (–47 %)
2,914 (+1.2 %)
6,757 (–39 %) 233 (+32 %) 29 K (–71 %)
3,214 (+12 %)
6,509 (–41 %) 247 (+40 %) 21 K (–79 %)
3,631 (+26 %)
- dest-only
- IP-only
7,052 (–36 %) 73 (–59 %)
75 K (–26 %)
1,949 (–32 %)
4,460 (–59 %) 125 (–29 %)
53 K (–47 %)
1,215 (–58 %)
Wildcard (IP granularity) (Sect. 4.4)
- \24
- \16
- \8
IP-only, 60 s
4,460
8218 (–25 %)
8479 (–23 %)
8225 (–25 %)
69 (–61 %)
-
-
66 (–63 %)
100 K (–0 %)
2,784 (–3 %)
66 (–63 %)
99 K (–1.5 %) 2,752 (–4 %)
125
53 K
1,215
Simple aggregation (Sect. 5.1)
T = 100 %
3,568 (–20 %) 121 (–3 %)
46 K (–13 %)
1,277 (+5 %)
Aggressive aggregation (Sect. 5.2)
T = 25 %
T = 50 %
T = 75 %
3,122 (–30 %) 106 (–15 %)
1,695 (–62 %) 69 (–45 %)
2,676 (–40 %) 85 (–32 %)
40 K (–24 %)
1,189 (–2 %)
43 K (–19 %)
1,234 (–1 %)
45 K (–15 %)
1,265 (0 %)
308
C. Yu et al.
As the data sets do not have any information about the actual out port
number used for every ﬂow, we use the following heuristics to determine the
action of each rule. Since the Abilene dataset contains next-hop IP information,
we associate every next-hop IP with a unique out port. For the Campus data set,
we simulate a 24-port switch, where every ﬂow is assigned to an out port based
on its destination IP preﬁx. We assume a reactive OpenFlow deployment (i.e.,
the installation of the rule corresponding to a ﬂow is triggered by the ﬁrst packet
in the ﬂow and its removal by the timeouts), as it oﬀers a dynamic model for
rule management and a worst case scenario for evaluation (because it maximizes
the total number of rules that are generated).
Evaluation Metrics. To measure the eﬀectiveness of rule reduction techniques,
we use the maximum value across time of the total number of rules installed
on the switch at any moment in time. To measure the side eﬀects of reducing
the number of rules, we measure the rate of controller-to-switch operations (to
estimate overhead).
4 Manual Rule Management
In this section, we study manual solutions for reducing the number of rules on an
OpenFlow switch. These are solutions that programmers must proactively use in
their code. We derive them from personal communication with SDN operators
and previous work [5,27]. These mechanisms limit the time a rule stays on the
switch (through rule expiration timeouts), the space occupied by a rule on the
switch (by reducing the number of ﬁelds to match on), or the total number of
rules (by using wildcards).
4.1 Not Managing Rules
Figure 1(a) shows the number of concurrent rules that would have to be held
on an OpenFlow switch that forwards the ﬂows in the Campus dataset. We
assume rules do not expire (TO = ∞) and contain exact matches on all ﬁve
ﬁelds mentioned in Sect. 3. Since rules never expire, their number is continually
increasing as new ﬂows arrive, reaching a maximum of 115,323 rules at the end
of the trace. We repeat the experiment for the Abilene data and ﬁnd that it
generates more than 12 M rules. Recall however that the Abilene IPs have their
last 11 bits zeroed, therefore the rules are essentially wildcard rules; the number
of exact match rules will be much higher. These numbers exceed the maximum
number of ﬂows supported by all but one of the OpenFlow switches described
in Table 1.
4.2 Timeouts
We vary the soft timeout for each rule from 5 s to 60 s (the default timeout value
in OpenFlow). Rules with short timeouts are expunged sooner and may need to
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
309
be reinstalled if there are subsequent packets matching the rule. Large timeouts
keep the rule in memory longer and are suited for long ﬂows with lower packet
arrival rates. Figure 1(a) and Table 2 show that, as the soft timeout becomes
smaller, the number of concurrent rules decreases and the rate of operations
increases. Current switches typically handle around 275 operations (i.e., rule
installations or deletions) a second [5] and could support the 95th percentile
operation load in the Campus dataset but not in the Abilene trace.
4.3 Match Fields
Having fewer match ﬁelds should decrease the memory footprint of an Open-
Flow rule. We consider two smaller matches: on IP-only (source and destination,
no ports) and on destination-only (destination IP and port, no source). Table 2
shows that both destination-only and IP-only matches lower the number of con-
current rules by at least 26 %, as compared to 5-tuple rules with 60 s timeout.
Though these rule savings are signiﬁcant, the maximum number of concurrent
rules with the Abilene trace is still quite high compared to the memory capacity
of three of the OpenFlow switches in Table 1. While fewer rules result in a lower
rate of operations on the switch, since the ﬂow arrival is not uniform, the 95th
percentile rate of operations per second in the Abilene trace is over 4x higher
than the threshold of 275.
4.4 Wildcards
Wildcard-based rules cover a larger part of the ﬂow-space and thus, fewer rules
are necessary. However, they limit (1) the expressivity of the conﬁguration
because they cannot perform ﬁne-grained matching (e.g., for multipathing [21]),
and (2) the application’s visibility into the network because the controller cannot
request statistics on the individual ﬂows that match the rule.
r1:
src tree
r1:
r6:
r4:
r5:
r2:
r3:
r4:
r5:
r7:
r4:
r5:
r7:
dst tree
r1:
r2:
r3:
r1:
r6:
r5:
r4:
r5:
r4:
r5:
r4:
(a)
(b)
(c)
Fig. 2. Simple binary tree aggregation. For simplicity, we represent the subtrees cor-
responding to the last two bits of source and destination IPs. See Fig. 3 for example
rules mapped on these subtrees (Color ﬁgure online).
310
C. Yu et al.
r1:1.1.1.0/31
r6:1.1.1.2/31
r1:2.1.1.0/31
Src Rules
Dst Rules
r4:1.1.1.0
r5:1.1.1.1
r2:1.1.1.2
r3:1.1.1.3
r2:2.1.1.0
r3:2.1.1.0
r5:2.1.1.1
r4:2.1.1.3
r6:2.1.1.0
r4:1.1.1.0
r5:1.1.1.1
r1:1.1.1.0/31
r6:1.1.1.2/31
r7:1.1.1.0/30
r4:1.1.1.0
r5:1.1.1.1
r7:1.1.1.0/30
r6:2.1.1.0
r5:2.1.1.1
r4:2.1.1.3
r1:2.1.1.0/31
r7:2.1.1.0/31
r5:2.1.1.1
r4:2.1.1.3
r7:2.1.1.0/31
Fig. 3. Examples of rules mapping to the subtrees in Fig. 2 (Color ﬁgure online).
To evaluate the eﬀect of wildcards on the ﬂow table size, we consider the
original 5-tuple rules, as well as the destination-only and IP-only rules. For each
rule, we introduce wildcards in the rightmost bits of IP addresses, eﬀectively
reducing them to preﬁxes. Figure 1(b) and Table 2 show that the average number
of rules over each minute decreases as we vary the IP preﬁx size from /32 to /16.
The savings (23 % in the Campus data) come at the expense of more policy
violations (30 % of packets are forwarded diﬀerently). Combining wildcards with
fewer match ﬁelds further reduces the number of rules, but not always suﬃciently
enough to ﬁt into the memory of all switches in Table 1. The reduction in number
of rules is lesser in the Abilene data because it includes only /21 addresses. As
with limiting the match ﬁelds, using wildcards reduces the expressivity of the
installed conﬁguration and our ability to retrieve information about the original
rule set (e.g., counters) as the rule semantics change.
4.5 Summary
The most consistently eﬀective way to reduce the number of rules is by lower-
ing rule expiration timeouts. Although it introduces a large network overhead
because of the increased control channel operation rate, it preserves the original
rule semantics and the controller’s ability to query the counters of the original
rules. Other approaches limit the control channel overhead at the expense of
changing the original rule semantics.
No manual rule compression method is a panacea: as Table 2 shows, even in
the best case compression scenario, the number of rules for Abilene cannot ﬁt
on half of the switches in Table 1. In reality, the type of traﬃc and the goal of
network operators, in addition to rule compression algorithms, play a large role
in determining how to ﬁt the conﬁguration on switches.
5 Automatic Rule Management
We now consider the scenario where the OpenFlow controller uses an automatic
mechanism to reduce the number of rules. To the best of our knowledge there is
no existing mechanism for rule space compression for SDN controllers. Existing
rule compression approaches focus on IP routing table compaction [15,23] or
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
311
minimizing packet classiﬁers in TCAM [6,17]. They use binary trees or decision
trees to identify redundant and similar rules and focus on simple IP-based rules
or on how to optimize ranges that cannot be stored as a simple preﬁx. Their
applicability to OpenFlow is not clear yet as OpenFlow rules are more com-
plex (up to 12 matching ﬁelds) [14]. Furthermore, IP-based rule management
techniques cannot easily adapt to incremental rule changes.
To understand the potential of automatic rule compression, we propose a sim-
ple approach, based on the work of Liu [15], that uses binary trees to identify
and aggregate related rules. In doing so, our goal is to provide a simple com-
pression baseline. We do not seek to either introduce a novel OpenFlow table
compaction method or to fully replicate and compare with previous rule aggre-