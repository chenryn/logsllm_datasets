is possible to further reduce the variance by
performing post-processing steps that use prior knowledge to
adjust the estimations. For example, exploiting the property
that all frequency counts are non-negative can reduce the
variance; however, simply turning all negative estimations to
0 introduces a systematic positive bias in all estimations.
By also ensuring the property that the sum of all estima-
tions must add up to 1, one ensures that the sum of the
biases for all estimations is 0. However, even though the
biases cancel out when summing over the whole domain,
they still exist. There are different post-processing methods
that were explicitly proposed or implicitly used. They will
result in different combinations of variance reduction and bias
distribution. Selecting a post-processing method is similar to
considering the bias-variance tradeoff in selecting a machine
learning algorithm.
We study the property of several post-processing methods,
aiming to understand how they compare under different set-
tings, and how they relate to each other. Our goal is to identify
efﬁcient post-processing methods that can give accurate esti-
mations for a wide variety of queries. We ﬁrst present the
baseline method that does not do any post-processing.
• Base: We use the standard FO as presented in Section III
to obtain estimations of each value.
Base has no bias, and its variance can be analytically
computed (e.g., using [31]).
A. Baseline Methods
When the domain is large, there will be many values in
the domain that have a zero or very low true frequency; the
estimation of them may be negative. To overcome negativity,
we describe three methods: Base-Pos, Post-Pos, and Base-Cut.
• Base-Pos: After applying the standard FO, we convert all
negative estimations to 0.
This satisﬁes non-negativity, but the sum of all estimations
is likely to be above 1. This reduces variance, as it turns
erroneous negative estimations to 0, closer to the true value.
As a result, for each individual value, Base-Pos results in an
estimation that is at least as accurate as the Base method. How-
ever, this introduces systematic positive bias, because some
negative noise are removed or reduced by the process, but the
positive noise are never removed. This positive bias will be
reﬂected when answering subset queries, for which Base-Pos
results in biased estimations. For larger-range queries, the bias
can be signiﬁcant.
Lemma 1. Base-Pos will introduce positive bias to all values.
Proof. The outputs of standard FO are unbiased estimation,
which means for any v,
= E(cid:104) ˜fv · 1[ ˜fv ≥ 0]
(cid:105)
+ E(cid:104) ˜fv · 1[ ˜fv  0.
After enforcing non-negativity constraints, the bias will be
E [ f(cid:48)
• Post-Pos: For each query result, if it is negative, we convert
it to 0.
This method does not post-process the estimated distribution.
Rather, it post-processes each query result individually. For
subset queries, as the results are typically positive, Post-Pos
is similar to Base. On the other hand, when the query is on a
single item, Post-Pos is equivalent to Base-Pos.
Post-Pos still introduces a positive bias, but the bias would
be smaller for subset queries. However, Post-Pos may give
inconsistent answers in the sense that the query result on A∪B,
where A and B are disjoint, may not equal the addition of the
query results for A and B separately.
• Base-Cut: After standard FO, convert everything below
some sensitivity threshold to 0.
The original design goal for frequency oracles is to recover
frequencies for frequent values, and oftentimes there is a sen-
sitivity threshold so that only estimations above the threshold
are considered. Speciﬁcally, for each value, we compare its
estimation with a threshold
1 − α
d
σ,
(7)
where d is the domain size, F −1 is the inverse of cummulative
distribution function of the standard normal distribution, and
σ is the standard deviation of the LDP mechanism (i.e., as in
Equation (5)). By Base-Cut, estimations below the threshold
are considered to be noise. When using such a threshold, for
any value v ∈ D whose original count is 0, the probability that
it will have an estimated frequency above T (or the probability
a zero-mean Gaussian variable with standard deviation δ is
above T ) is at most α
d . Thus when we observe an estimated
frequency above T , the probability that the true frequency of
the value is 0 is (by union bound) at most d× α
d = α. In [14],
it is recommended to set α = 5%, following conventions in
the statistical community.
T = F −1(cid:16)
(cid:17)
Empirically we observe that α = 5% performs poorly,
because such a threshold can be too high when the population
size is not very large and/or the  is not
large. A large
threshold results in all except for a few estimations to be
below the threshold and set to 0. We note that the choice
of α is trading off false positives with false negatives. Given
a large domain, there are likely between several and a few
dozen values that have quite high frequencies, with most of
the remaining values having low true counts. We want to keep
an estimation if it is a lot more likely to be from a frequent
value than from a very low frequency one. In this paper, we
choose to set α = 2, which ensures that the expected number
of false positives, i.e., values with very low true frequencies
but estimated frequencies above T , to be around 2. If there are
4
around 20 values that are truly frequent and have estimated
frequencies above T ,
then ratio of true positives to false
positives when using this threshold is 10:1.
This method ensures that all estimations are non-negative. It
does not ensure that the sum of estimations is 1. The resulting
estimations are either high (above the chosen threshold) or
zero. The estimation for each item with non-zero frequency
is subject to two bias effects. The negative bias effect is
caused by the situation when the estimations are cut to zero.
The positive effect is when large positive noise causes the
estimation to be above the threshold, the resulting estimation
is higher than true frequency.
B. Normalization Method
We now explore several methods that normalize the esti-
mated frequencies of the whole domain to ensure that the sum
of the estimates equals 1. When the estimations are normalized
to sum to 1, the sum of the biases over the whole domain has
to be 0.
Lemma 2. If a normalization method adjusts the unbiased
estimates so that
they add up to 1,
the sum of biases it
introduces over the whole domain is 0.
Proof. Denote f(cid:48)
post-processing. By linearity of expectations, we have
v as the estimated frequency of value v after
(E [ f(cid:48)
v ] − fv) = E
fv = E [ 1 ] − 1 = 0
(cid:34)(cid:88)
v∈D
f(cid:48)
v
(cid:35)
−(cid:88)
v∈D
(cid:88)
v∈D
One standard way to do such normalization is through
additive normalization:
• Norm: After standard FO, add δ to each estimation so that
the overall sum is 1.
The method is formally proposed for the centralized set-
ting [16] of DP and is used in the local setting, e.g., [28],
[22]. Note the method does not enforce non-negativity. For
GRR, Hadamard Response, and Subset Selection, this method
actually does nothing, since each user reports a single value,
and the estimations already sum to 1. For OLH, however, each
user reports a randomly selected subset whose size is a random
variable, and Norm would change the estimations. It can be
proved that Norm is unbiased:
Lemma 3. Norm provides unbiased estimation for each value.
v =
v∈D( ˜fv + δ) = 1. As the frequency oracle outputs unbiased
Proof. By the deﬁnition of Norm, we have (cid:80)
(cid:80)
estimation, i.e., E(cid:104) ˜fv
(cid:105)
v∈D f(cid:48)
(cid:34)(cid:88)
(cid:35)
E(cid:104) ˜fv
(cid:88)
+ d · E [ δ ] = 1 + d · E [ δ ]
(cid:34)(cid:88)
= fv, we have
= 1 = E
( ˜fv + δ)
(cid:35)
(cid:105)
v∈D
v∈D
f(cid:48)
v
E
=
v∈D
=⇒ E [ δ ] = 0
v ] = E(cid:104) ˜fv + δ
Thus E [ f(cid:48)
(cid:105)
= E(cid:104) ˜fv
(cid:105)
+ 0 = fv.
Besides sum-to-one, if a method also ensures non-negativity,
we ﬁrst state that it introduces positive bias to values whose
frequencies are close to 0.
Lemma 4. If a normalization method adjusts the unbiased
estimates so that they add up to 1 and are non-negative, then
it introduces positive biases to values that are sufﬁciently close
to 0.
Proof. As the estimates are non-negative and sum up to 1,
some of the estimates must be positive. For a value close to
0, there exists some possibility that its estimation is positive;
but the possibility its estimation is negative is 0. Thus the
expectation of its estimation is positive, leading to a positive
bias.
Lemma 4 shows the biases for any method that ensures
both constraints cannot be all zeros. Thus different methods
are essentially different ways of distributing the biases. Next
we present three such normalization methods.
• Norm-Mul: After standard FO, convert negative value to 0.
Then multiply each value by a multiplicative factor so that
the sum is 1.
More precisely, given estimation vector ˜f, we ﬁnd γ such that
(cid:88)
v∈D
max(γ × ˜fv, 0) = 1,
v = max(γ× ˜fv, 0) as the estimations. This results
and assign f(cid:48)
in a consistent FO. Kairouz et al. [19] evaluated this method
and it performs well when the underlying dataset distribution
is smooth. This method results in positive biases for low-
frequency items, but negative biases for high-frequency items.
Moreover, the higher an item’s true frequency, the larger the
magnitude of the negative bias. The intuition is that here γ
is typically in the range of [0, 1]; and multiplying by a factor
may result in the estimation of high frequency values to be
signiﬁcantly lower than their true values. When the distribution
is skewed, which is more interesting in the LDP case, the
method performs poorly.
• Norm-Sub: After standard FO, convert negative values to
0, while maintaining overall sum of 1 by adding δ to each
remaining value.
More precisely, given estimation vector ˜f, we want to ﬁnd δ
such that
max( ˜fv + δ, 0) = 1
(cid:88)
v∈D
Then the estimation for each value v is f(cid:48)
v = max( ˜fv + δ, 0).
This extends the method Norm and results in consistency.
Norm-Sub was used by Kairouz et al. [19] and Bassily [3]
to process results for some FO’s. Under Norm-Sub,
low-
frequency values have positive biases, and high-frequency
items have negative biases. The distribution of biases, however,
is more even when compared to Norm-Mul.
• Norm-Cut: After standard FO, convert negative and small
positive values to 0 so that the total sums up to 1.
5
We note that under Norm-Sub, higher frequency items have
higher negative biases. One natural idea to address this is to
turn the low estimations to 0 to ensure consistency, without
changing the estimations of high-frequency values. This is
the idea of Norm-Cut. More precisely, given the estimation
v∈D max( ˜fv, 0) ≤ 1,
we simply change each negative estimations to 0. When
v∈D max( ˜fv, 0) > 1, we want to ﬁnd the smallest θ such
vector ˜f, there are two cases. When (cid:80)
(cid:80)
that
(cid:88)
˜fv ≤ 1
v∈D| ˜fv≥θ
Then the estimation for each value v is 0 if ˜fv < θ and ˜fv
if ˜fv ≥ θ. This is similar to Base-cut in that both methods
change all estimated values below some thresholds to 0. The
differences lie in how the threshold is chosen. This results in
non-negative estimations, and typically results in estimations
that sum up to 1, but might result in a sum < 1.
C. Constrained Least Squares
From a more principled point of view, we note that what
we are doing here is essentially solving a Constraint Inference
(CI) problem, for which CLS (Constrained Least Squares) is
a natural solution. This approach was proposed in [16] but
without the constraint that the estimates are non-negative (and
it leads to Norm). Here we revisit this approach with the
consistency constraint (i.e., both requirements in Deﬁnition 2).
• CLS: After standard FO, use least squares with constraints
(summing-to-one and non-negativity) to recover the values.
Speciﬁcally, given the estimates ˜f by FO, the method outputs
f(cid:48) that is a solution of the following problem:
minimize: ||f(cid:48) − ˜f||2
subject to: ∀vf(cid:48)
f(cid:48)
v = 1
v ≥ 0(cid:88)
v
We can use the KKT condition [21], [20] to solve the
problem. The process is presented in Appendix A. In the
solution, we partition the domain D into D0 and D1, where
D0 ∩ D1 = ∅ and D0 ∪ D1 = D. For v ∈ D0, assign f(cid:48)
v = 0.
For v ∈ D1,
v = ˜fv − 1
f(cid:48)
|D1|
(cid:17)
(cid:16)(cid:80)
Norm-Sub is
Square (CLS)
− 1|D1|
v∈D1
Sub.
the
solution to the Constraint Least
formulation to the problem, and δ =
˜fv − 1
is the δ we want to ﬁnd in Norm-
(cid:33)
˜fv − 1
(cid:32)(cid:88)
v∈D1
D. Maximum Likelihood Estimation
Another more principled way of looking into this problem is
to view it as recovering distributions given some LDP reports.
For this problem, one standard solution is Bayesian inference.
In particular, we want to ﬁnd the f(cid:48) such that
(cid:104)
(cid:105)
f(cid:48)|˜f
Pr
Pr
=
(cid:104)˜f|f(cid:48)(cid:105) · Pr [f(cid:48)]
(cid:104)˜f
(cid:105)
Pr
(8)
and (cid:80)
v f(cid:48)
is maximized. Note that we require f(cid:48) satisﬁes ∀vf(cid:48)
v ≥ 0
v = 1. In (8), Pr [f(cid:48)] is the prior, and the prior
distribution inﬂuence the result. In our setting, as we assume
there is no such prior, Pr [f(cid:48)] is uniform. That is, Pr [f(cid:48)] is
a constant. The denominator Pr
is also a constant that
does not inﬂuence the result. As a result, we are seeking
for f(cid:48) which is the maximal likelihood estimator (MLE), i.e.,
Pr
(cid:104)˜f|f(cid:48)(cid:105)
is maximized.
(cid:104)˜f
(cid:105)
For this method, Peter et al. [19] derived the exact MLE
solution for GRR and RAPPOR [14]. We compute Pr
using the general form of Equation (4), which states that, given