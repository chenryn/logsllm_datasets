title:An Experimental Evaluation to Determine if Port Scans are Precursors
to an Attack
author:Susmit Panjwani and
Stephanie Tan and
Keith M. Jarrin and
Michel Cukier
An Experimental Evaluation to Determine if Port Scans are Precursors to an Attack1
Susmit Panjwani, Stephanie Tan, Keith M. Jarrin, and Michel Cukier 
Center for Risk and Reliability 
Department of Mechanical Engineering 
University of Maryland 
College Park, MD 20742 
{spanjwan, sjt, kmj, mcukier}@umd.edu
Abstract
This  paper  describes  an  experimental  approach  to
determine  the  correlation  between  port scans and
attacks. Discussions in  the  security  community  often
state that port scans should be considered as precursors 
to an  attack.  However,  very  few  studies  have  been 
conducted  to  quantify  the  validity  of  this  hypothesis.  In
this  paper,  attack  data  were  collected  using a test-bed
dedicated to  monitoring  attackers.  The  data  collected
consist of port  scans,  ICMP  scans,  vulnerability  scans,
successful  attacks  and  management 
traffic.  Two 
experiments were performed to validate the hypothesis of
linking port scans and vulnerability scans to the number 
of  packets  observed  per  connection.  Customized  scripts 
were  then  developed  to  filter  the  collected data and
group them on the basis of scans and attacks between a 
source and destination IP address pair. The correlation
of the  filtered  data  groups  was  assessed.  The  analyzed 
data  consists  of  forty-eight  days  of  data collection for
two target computers on a heavily utilized subnet. 
1.  Introduction 
Traditional  approaches  to  security validation have
not been quantitative,  focusing  instead  on  specifying
procedures that should be followed during the design of
a system  (e.g.,  the  Security  Evaluation  Criteria  [1,  2]).
When  quantitative  methods  have  been  used,  they have
typically been very  formal  (e.g.,  [3]),  aiming  to  prove 
that certain security properties hold given a specified set
of  assumptions,  or  quite  informal, using a team of
experts (often called a “red team,” [4]) that is skilled in
1 This research was supported by NSF CAREER award 0237493.
the practice of  security  and  has  complete  knowledge  of 
the system being studied.
An  alternative  approach,  which  has  received  much
less attention by the security community, has been to try 
to quantify the behavior of an attacker, and his impact on
the ability of a system to provide certain security-related
properties. Goseva-Popstojanova et  al.  [5]  presented  a 
model of an  intrusion  tolerant  system  using  a  state
transition diagram. Jha et al. [6] combined modeling, the
use of  formal  logic,  and  a  Bayesian  analysis  approach. 
Ortalo et al. [7] modeled the system as a privilege graph
(that is similar to the scenario graph in [6]). Combining
the privilege  graph  with  assumptions  on  the  attacker
behavior,  the  authors  obtained  an  attack state graph. A
variable  called  the  effort was  introduced  characterizing 
the ease or difficulty to reach a given privilege level. The 
mean  effort  to  security  failure  (METF)  was  then
estimated based on experimental data.
This paper focuses on  trying  to  assess  one  specific
attacker behavior  based  on  experimental  data.  More
specifically, this paper estimates the correlation between
a  port  scan  and  an  attack.  Such  estimates  can  then be
used in one  of  the  previously  mentioned  models  for 
quantifying  security.  This  paper  describes  a  test-bed
using  target  computers  for  monitoring  attackers and
collecting attack data. Various scripts were developed to
filter and analyze the data. We describe each step taken 
to filter the original traffic (that consisted of management
and malicious activity)  into  various  scans  and  attacks
directed  to  the  target  computers.  The  classification  into 
scans and attacks is based on the number of packets per 
connection. Two experiments were conducted to indicate
the relevance  of  such  classification.  The  correlation 
between scans and attacks were studied by first focusing
on  the  scans  and  identifying  if  attacks followed them,
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:49 UTC from IEEE Xplore.  Restrictions apply. 
then  analyzing  the  attacks  and  identifying  the  ones  that
had been preceded by a scan. 
The  paper  is  organized  as  follows. Section 2
precisely defines the different types of scans considered
in this paper: port scans, ICMP scans, and vulnerability
scans. Section 3 reviews the literature tackling the issue
of port scan characterization and filtering. In Section 4,
we  describe  the  test-bed  used  for  the  experiment. We
also  conducted  two  additional  experiments  to  better 
characterize two types of scans in Section 5. Section 6 is
dedicated to the filtering  and  analysis  of  the  collected
data. Finally,  the  results  obtained  on  the  correlation
between scans and attacks are presented in Section 7. 
2.  Definitions 
In the introduction, we used the term of port scan as a 
general term that stands for “checking for an exploitable
target.” We now present a more precise definition of the
different scans discussed  in  this  paper.  A  scan  can  be 
defined  as  a  reconnaissance  technique  in  which the
attacker  tries  to  determine  something about the target
host (i.e., Is the host alive? What services are running on
the host?  What  is  the  host’s  operating  system?  Is  there
an exploitable vulnerability?)  The  different  types  of
scans  considered  in  this  paper  are  ICMP  scans,  port 
scans and vulnerability scans.
An ICMP scan is used to check the availability of a
target  machine  and  to  fingerprint the target operating
system. An ICMP scan uses the information provided by
ICMP  control  messages.  An  ICMP scan provides less
information compared to a port or vulnerability scan.
A port scan is used to check for open or closed ports
and for used  or  unused  services.  The  services  may  or 
may  not  have  a  vulnerability  that  the  attacker  could
exploit.  How  these  port  scans  establish a connection,
terminate  a  connection,  and  exchange  messages  in  the 
a  successful/unsuccessful  connection  or 
event of
termination  of  a  connection 
is described in [8].
Moreover, the implementation of  the  TCP/IP  stack  is 
operating system  dependent  and  hence  the  attacker  can 
use this  information  to  fingerprint  the  target  operating
system.
We have covered the scanning techniques that make
use  of  the  TCP  protocol suite to gather information.
Since  the  information  that
is  leaked  by  using  these
protocol-dependent scanning methods can be defined in
terms of the type of packets and the flow of the packets 
in a connection, algorithms for detecting these scans can 
be  developed.  Such  algorithms  are  more difficult
to
develop when the  scanning  method  is  checking  for 
specific vulnerabilities within  specific  services  or
applications. This type of scanning method is known as 
vulnerability scans. More precisely, a vulnerability scan
can be used to fingerprint the presence or absence of an 
exploitable  vulnerability.  Since  vulnerabilities differ,
most
techniques to fingerprint  them  will  also  differ,
making it difficult  to  develop  a  generic  algorithm  to
detect them.
3.  Related Work
Most  of  the  related  work  on  port scans focuses on
characterizing  port  scans  and  filtering them from
network  traffic.  Intrusion  detection  systems  like Snort
[9] and BRO [10] detect port scans based on models that
look for IP addresses that make more than X connections
in  Y  seconds.  NSM  [11]  uses  a  similar algorithm that
checks for  source  IP  addresses  making  connections  to
more than fifteen other hosts. GrIDS [12] creates activity 
graphs  representing  aggregated  network  activity in
which  nodes  are  represented  by  hosts  and  a  connection
or  traffic  is  represented  by  edges.  Furthermore, GrIDS
analyzes  these  graphs  to  detect  large-scale  attacks. 
Anomaly-based  approaches  are  also  used  to detect port
scans. Emerald [13] constructs statistical profiles for the
subjects.  In  addition,  Emerald matches a short-term
weighted  profile  of  a  subject  behavior to a long-term
weighted profile. Port scans for example are detected as
a sudden increase in SYN traffic from a single source IP
address. Ertoz et al. [14] uses a heuristic-based approach 
to  develop  algorithms  and  techniques  to  determine
source  IP  addresses  of  port  scans  with  fewer  false
positives.
Apart from port scan detection, some work exists on
characterizing  the  distribution  of  the  type  of  scans  as
seen in traffic generated  on  a  production  sub-network
[15]  and  in  radiated  traffic  (i.e.,  sent  to  non-existent IP
addresses) [16]. Lee et al. [15] also classified port scans 
based  on  the  number  of  hosts  and ports scanned by the
source host in a given amount of time.
Some  case  studies  analyzing  attacks  and scans are
available in [17, 18]. Each of these case studies focused 
on  one  exploit,  detailing  how  the  vulnerability was
probed and how  the  vulnerability  was  exploited  by
describing  the  different  steps  of  the  attack.  These
examples are  based  on  forensics  to  understand  step  by
step how the target computers were compromised.
4.  Experimental Setup
experimental 
test-bed 
The
is  based  on 
target
computers built  for  the  sole  purpose  of  being  attacked.
Other computers closely monitor these target computers
but  attackers  do  not  notice  that  they  are  observed.  This 
architecture thus allows:  1)  collecting  data  at  the  host, 
application  and  network  levels,  2)  filtering  user  traffic 
from attacker traffic, 3) correlating data collected at the
host and network  level  and  4)  controlling  the  target
computers from an  isolated  monitoring  network.  Since
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:49 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1. Test-bed Architecture
there  are  no  real  users  on  the target computers, there is 
no concern associated  with  filtering  user  traffic  from  at 
tack traffic.  This  approach  also  avoids  the  issue  of 
having to store huge amounts of data since no user traffic
data is collected.
This  architecture  is  similar  to  the  one  developed  by
the  Honeynet  project  [18].  However,  we  are  not  using
the same tools  for  data  collection  and  monitoring  (e.g., 
Ethereal  instead  of  tcpdumps  because  of  the  better
graphical  and  analysis  capability,  correlation  scripts
instead of the honeynet management console, the use of 
Snort  for  the  sole  purpose  of  event  alerting,  tracelog 
instead of Sebek [19] (on Windows)). Moreover, a data
filtering  module,  an  image  control  module and a data
correlation  module  have  been  developed. The
experimental results provided in this paper are based on 
the test-bed shown  in  Figure  1  consisting  of  two  target
computers.
Functional descriptions of the components in Figure
1 are as follows: 
the
from 
(cid:120)(cid:3) Access Control:  This  module  restricts 
the
propagation  of  attacks 
target
computers by using a reverse firewall in which
outbound  connections  are  monitored  instead of
inbound  connections.  Customized 
firewall
scripts  were  developed  to  limit  attackers  from
initiating outbound connections and at the same
time allowing the data collection engine to send
data  securely  to  the  management  network. The
scripts were developed on the IPTables firewall
running on Linux RedHat 9. 
(cid:120)(cid:3) Data Collection: This module collects real time
data at the network, host and application level.
Data  in  the  format  of  tcpdumps  are  collected
using Ethereal [20].  The  data  are  uploaded
every  six  hours  on  a  database  located on the
testbed, using MySQL version 4 as a buffer for 
temporary  data storage.  These  data  are  then 
uploaded  daily  on  a  centralized database
running  Oracle  9i.  Customized  scripts were
developed  in  Perl  to  parse the data into the
Oracle database. 
(cid:120)(cid:3) Event  Logging:  This  is  a  sub-module that aids
data  storage  by  collecting  and  storing  system,
application and security  logs.  Syslog  data  are 
collected using ‘syslogd’  running  on  Fedora 
Core  1.  As  for  the  data  collected  in  the  data
collection  module,  these  logs  are  uploaded
every  six  hours  on  a  database  using  MySQL
version 4. Finally, these data are uploaded each
day on a centralized database using Oracle 9i.
failure 
(cid:120)(cid:3) Event Alerting: This module is used to alert the
research  team  about  any  attacker  activity and
system 
immediate
administrative  response.  These  alerts can also
be used for forensic purpose to understand data
and  event  interactions  in  sequence.  We are
using Swatch [21]  to  monitor  syslogs  data  and 
allowing 
for
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:49 UTC from IEEE Xplore.  Restrictions apply. 
(cid:120)(cid:3)
Snort  [9]  alerts  in  real  time.  Swatch  runs  on 
RedHat 9 and uses send-mail.
Image Control: This  module  controls 
the
deployment and  the  maintenance  of  current
versions of operating systems, applications and
the  release  of  patches  on  the  target machines.
This module also  allows  re-imaging  the  target
computers  in  case  of  corruption.  We  are  using 
Ghost Enterprise  version  8  developed  by
Symantec [22].
(cid:120)(cid:3) Data  Filtering:  This  module  filters  out
the
management traffic  generated  on  the  network
from  the  collected  traffic.  The  resulting  traffic
thus only  consists  of  malicious  activity.  The
filtering is done at  multiple  stages  in  the  data 
collection  and  analysis  process  and  is  detailed
in Section 6. 
(cid:120)(cid:3) Data  Correlation:  This  module analyzes the
filtered data to produce  the  results  given  in 
Section 6 and 7 using Perl and PL/SQL scripts
on an Oracle database. 
5. Port Scans and Vulnerability Scans 
As mentioned in Section 4, the data collected consists
of  management  traffic  and  malicious activity since no
“normal” user was using  either  of  the  two  target
computers. The malicious traffic collected includes port
scans,  vulnerability  scans  and  attacks.  To  improve data
analysis  data,  we  conducted  two  experiments  to better
characterize port scans and vulnerability scans.
5.1 Characterization of Port Scans 
As can be seen from [8] three packets are sufficient 
to  finish  a  TCP  handshake  and  establish a connection.
The  information  about  open  ports  and  services can be
gathered by using as few as two packets. To corroborate
these  specifications  experimentally  and  to evaluate the
distribution of port  scans  we  developed  an  experiment
based on a well-known network scanner. 
More  specifically,  we  used  an  isolated network
consisting of two  computers.  On  one  computer,  we  ran 
the  network  scanner  Nmap  version 3.75 for Windows
[23]. On the other computer, we recorded all the packets 
going  through  the  network  using  the network protocol
analyzer  Ethereal  version  0.10.7  [20]. We ran all the
different basic types of port scans available in Nmap and 
used Ethereal
to capture,  measure,  and  group  the
connections based on the number of packets. The results
of the number of packets per connection and the number
of connections associated with  a  given  number  of 
packets when running  Nmap  are  provided  in  Figure  2. 
Note that we did not observe any connection with some
specific number of packets  (e.g.,  5,  6,  10,  11).  From
Figure 2, over 19,946 (i.e., 99.76%) port scans consist of 
two packets  per  connection,  19,961  (i.e.,  99.83%)  port
scans consist of three or less packets per connection and 
19,971 (i.e., 99.88%) port  scans  consist  of  four  or  less
packets  per  connection.  These  results  show  that  five
packets  per  connection  can  be  used  as  a  threshold to
characterize  port  scans.  This threshold  is  confirmed  in
theory since full TCP  handshakes  consist  of  three
packets  and  a  fourth  packet  is  included  as  a  possible
reset packet. 
No. Packets
in Connection 
2
3
4
7
8
9
12
17
18
21
33
Total N/A
No. of 
Connections
19,946
15
10
2
4
10
1
1
1
1
3
19,994
Percentage of 
Connections
99.76
0.075
0.05
0.01
0.02
0.05
0.005
0.005
0.005
0.005
0.015
100
Figure 2. Nmap: Analysis of the Number of Packets 
per Connection 
5.2 Characterization of Vulnerability Scans 
and  generated  CSV 
Since the collected traffic data consist of attacks and
vulnerability  scans  besides  ICMP  and  port scans, we
analyzed  vulnerability  scans  to  see  if  the number of
packets  could  also  be  used 
for  characterization.
Therefore, we ran NeWT 2.1 [24], a Windows version of