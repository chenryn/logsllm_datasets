214
212
210
28
20
214
212
210
28
20
DupLESS
Convergent Encryption
Google Drive
DupLESS
Dropbox
213
211
29
27
24
28
212
216
20
24
28
212
216
DupLESS
Google Drive
DupLESS
Dropbox
213
211
29
27
24
28
212
216
20
24
28
212
216
File size (KB)
File size (KB)
Figure 7: (Left) Median time to store (top two graphs) and retrieve (bottom two graphs) as a function of ﬁle size. (Top Right)
Median time to delete a ﬁle as a function of ﬁle size. (Bottom Right) Median time to copy a ﬁle as a function of ﬁle size. All axes
are log-scale and error bars indicate one standard deviation. Standard deviations are displayed only for base Dropbox/Google Drive
times to reduce cluttering.
of KS and cryptographic operations is about 5%, while
storing the key ﬁle accounts for 12%. Our implementa-
tion of DLput stores the content and key ﬁles simultane-
ously, by spawning a new thread for storing the key, and
waiting for both the stores to complete before ﬁnishing.
If DLput exits before the key store thread completes, i.e.,
if the key is uploaded asynchronously, then the overhead
drops to 14%. On the other hand, uploading the ﬁles se-
quentially by storing the content ﬁle ﬁrst, and then stor-
ing the key, incurs a 54% overhead (for 1 MB ﬁles).
Bandwidth overhead. We measured the increase in
transmission bandwidth due to DupLESS during storage.
To do so, we used tcpdump and ﬁltered out all trafﬁc un-
related to Dropbox and DupLESS. We took from this the
total number of bytes (in either direction). For even very
small ﬁles, the Dropbox API incurs a cost of about 7 KB
per upload. Figure 8 (middle) shows the ratio of band-
width used by DupLESS to that used by plain Dropbox
as ﬁle size increases. Given the small constant size of the
extra ﬁle sent by DupLESS, overhead quickly diminishes
as ﬁles get larger.
Storage overhead. DupLESS incurs storage overhead,
due to the encrypted ﬁle name, the MLE key, and the
MAC. The sizes of these components are independent of
the length of the ﬁle. Let n denote the length of the ﬁle-
name in bytes. Then, encrypting the ﬁlename with SIV
and encoding the result with base64 encoding consumes
2n + 32 bytes. Repeating the process for the content
and key ﬁles, and adding extensions brings the ﬁle name
overhead to 4n + 72 − n = 3n + 72 bytes. The contents of
the key ﬁle include the MLE key, which is 16 bytes long
in our case, and the 32 byte HMAC output, and hence
48 bytes together. Thus, the total overhead for a ﬁle with
an n-byte ﬁlename is 3n + 120 bytes. Recall that if the
ﬁle size is smaller than 1 KB, then canDedup rejects the
ﬁle for deduplication. In this case, the overhead from en-
crypting and encoding the ﬁle name is n + 32 bytes, since
only one ﬁle is stored. Randomized encryption adds 16
bytes, bringing the total to n + 48 bytes.
To assess the overall effect of this in practice, we
collected a corpus of around 2,000 public Amazon vir-
tual machine images (AMIs) hosting Linux guests. The
AMIs were gathered using techniques similar to those
used previously [14, 28], the difference being that we
as well downloaded a snapshot of the full ﬁle system
for each public AMI. There are 101,965,188 unique ﬁles
across all the AMIs, with total content size of all ﬁles be-
ing 2,063 GB. We computed cryptographic hashes over
the content of all ﬁles in the dataset, in order to simulate
the storage footprint when using plain deduplication as
well as when using DupLESS. This dataset has signiﬁ-
cant redundancy, as one would expect, given that many
AMIs are derivative of other AMIs and so share com-
mon ﬁles. The plain dedup storage required for the ﬁle
contents is just 335 GB. DupLESS with the dedupability
190  22nd USENIX Security Symposium 
USENIX Association
12
DupLESS
Dropbox
s
d
n
o
c
e
s
i
l
l
i
m
n
i
e
m
T
i
213
211
29
27
d
a
e
h
r
e
v
o
h
t
d
i
w
d
n
a
B
1.8
1.6
1.4
1.2
1
1.4
1.3
1.2
1.1
t
e
s
a
t
a
d
.
c
n
e
f
o
e
z
i
s
e
v
i
t
a
l
e
R
20
24
Number of ﬁles
28
212
21
24
27
210
213
216
1,000 2,000
4,000
6,000
8,000
File size (KB)
Threshold size in bytes
Figure 8: (Left) Median time to list a directory as a function of number of ﬁles in the directory. Both axes are logscale and error
bars are one standard deviation. (Middle) Network bandwidth overhead of DupLESS as a function of ﬁle size (log-scale axis) for
store operations. (Right) The ratio of space required when DupLESS is used for the AMI dataset and when plain dedup is used, as
a function of the dedupable threshold length.
length threshold used by canDedup (see Section 6) set
to zero (all ﬁles were dedupable) requires 350 GB, or an
overhead of about 4.5%. In this we counted the size of
the ﬁlename and path ciphertexts for the DupLESS esti-
mate, though we did not count these in the base storage
costs. (This can only inﬂate the reported overhead.)
We also measure the effect of higher threshold val-
ues, when using non-dedupable encryption. Setting the
threshold to 100 bytes saves a few hundred megabytes in
storage. This suggests little beneﬁt from deduping small
ﬁles, which is in line with previous observations about
deduplication on small ﬁles [61].
Figure 8 plots the storage used for a wide range of
threshold values. Setting a larger threshold leads to im-
proved security (for those ﬁles) and faster uploads (due
to one less SSput request) and appears to have, at least
for this dataset, only modest impact on storage overheads
for even moderately sized thresholds.
The above results may not extend to settings with sig-
niﬁcantly different workloads. For example, we caution
when there is signiﬁcantly less deduplication across the
corpus, DupLESS may introduce greater overhead.
In
the worst case, when there is no deduplication what-
soever and all 1 KB ﬁles with long names of about
100 characters, the overhead will be almost 30%. Of
course here one could have canDedup force use of non-
dedupable encryption to reduce overhead for all ﬁles.
Overhead of other operations. The time to perform
DLmove, DLdelete, and DLlist operations are reported
in Figure 7 and Figure 8 for Dropbox. In these opera-
tions, the DupLESS overheads and the data sent over the
network involve just the ﬁlenames, and do not depend on
the length of the ﬁle. (The operations themselves may
depend on ﬁle length of course.) The overhead of Dup-
LESS therefore remains constant. For DLlist, DupLESS
times are close to those of plain Dropbox for folders with
twice as many ﬁles, since DupLESS stores an extra key
encapsulation ﬁle for each user ﬁle. We also measured
the times for DLsearch and DLcreate, but in these cases
the DupLESS overhead was negligible.
8 Security of DupLESS
We argued about the security of the KS protocols and
client encryption algorithms in sections 5 and 6. Now,
we look at the big picture, the security of DupLESS as a
whole. DupLESS provides security that is usually signif-
icantly better than current, convergent encryption based
deduplicated encryption architectures, and never worse.
To expand, security is “hedged,” or multi-tiered, and we
distinguish three tiers, always assuming that the adver-
sary has compromised the SS and has the ciphertexts.
The optimistic or best case is that
the adversary
does not have authorized access to the KS. Recall that
both OPRFv1 and OPRFv2 need clients to authenticate
ﬁrst, before requesting queries, meaning that in this set-
ting, the attacker cannot obtain any information about
message-derived keys. These keys are effectively ran-
dom to the attacker. In other words, all data stored on
the SS is encrypted with random keys, including ﬁle con-
tents, names and paths. The attacker can only learn about
equality of ﬁle contents and the topology of the ﬁle sys-
tem (including ﬁle sizes). Thus, DupLESS provides, ef-
fectively, semantic security. In particular, security holds
even for predictable messages. By using the SIV DAE
scheme, and generating tags over the ﬁle names, ﬁle con-
tents and keys, DupLESS ensures that attempts by the SS
to tamper with client data will be detected.
The semi-optimistic, or next best case is that the ad-
versary, having compromised one or more clients, has
remote access to the KS but does not have the KS’s se-
cret key. Here, security for completely predictable ﬁles
is impossible. Thus, it is crucial to slow down brute-
force attacks and push the feasibility threshold for the
attacker. We saw in Section 5 that with the right rate-
USENIX Association  
22nd USENIX Security Symposium  191
13
limiting setup (Bounded, with appropriate parameters),
brute-force attacks can be slowed down signiﬁcantly. Im-
portantly, attackers cannot circumvent the rate-limiting
measures, by say, repeating queries.
Finally, the pessimistic case is that the adversary has
compromised the KS and has obtained its key. Even then,
we retain the guarantees of MLE, and speciﬁcally CE,
meaning security for unpredictable messages [18]. Ap-
propriate deployment scenarios, such as locating the KS
within the boundary of a large corporate customer of a
SS, make the optimistic case the most prevalent, result-
ing in appreciable security gains without signiﬁcant in-
crease in cost. The security of non-deduplicated ﬁles, ﬁle
names, and path names is unaffected by these escalations
in attack severity.
9 Conclusions
We studied the problem of providing secure outsourced
storage that both supports deduplication and resists
brute-force attacks. We design a system, DupLESS, that
combines a CE-type base MLE scheme with the ability to
obtain message-derived keys with the help of a key server
(KS) shared amongst a group of clients. The clients in-
teract with the KS by a protocol for oblivious PRFs, en-
suring that the KS can cryptographically mix in secret
material to the per-message keys while learning nothing
about ﬁles stored by clients.
These mechanisms ensure that DupLESS provides
strong security against external attacks which compro-
mise the SS and communication channels (nothing is
leaked beyond ﬁle lengths, equality, and access patterns),
and that the security of DupLESS gracefully degrades
in the face of comprised systems. Should a client be
compromised, learning the plaintext underlying another
client’s ciphertext requires mounting an online brute-
force attacks (which can be slowed by a rate-limited KS).
Should the KS be compromised, the attacker must still
attempt an ofﬂine brute-force attack, matching the guar-
antees of traditional MLE schemes.
The substantial increase in security comes at a mod-
est price in terms of performance, and a small increase in