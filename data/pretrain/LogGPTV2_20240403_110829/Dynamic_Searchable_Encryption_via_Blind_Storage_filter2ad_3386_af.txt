retrieving both these index ﬁles. Adding documents involves
updating only the second kind of index ﬁles (using an append
operation of the clear storage). Also, removing a newly added
document involves updating only the second kind of index
ﬁles, which is straightforward (except for efﬁciency concerns,
addressed below). But in removing an original document, we
need to ensure that the information on keywords in it that
are not searched for (for e.g., the number of such keywords)
remains secret. This is achieved by a lazy deletion strategy. The
index ﬁle of a keyword (for the original set of documents) is
not updated until that keyword is searched for. At that point, if
the client learns that a document listed in that index has been
deleted, the index is updated accordingly. This update can be
649
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:56 UTC from IEEE Xplore.  Restrictions apply. 
The construction uses a blind-storage system BSTORE, and a pseudorandom permutation Ψ(cid:48) for mapping document IDs (with versioning)
to pseudorandom document indices. It also uses a clear-storage system CLEARSTORE (see text).
• SSE.keygen: Let KSSE = (KBSTORE, K∂ID) where KBSTORE is generated by BSTORE.Keygen and K∂ID is a key for the PRP Ψ(cid:48).
• SSE.indexgen:
1) Firstly, for each document ∂, assign a pseudorandom ID η∂ = Ψ(cid:48)
2) For each keyword w that appears in at least one document, construct an index ﬁle with ﬁle-ID indexw that contains η∂ for each
document ∂ that contains the keyword w. No speciﬁc format is required for the data in this ﬁle; in particular, it could contain a
“thumbnail” (of ﬁxed size) about each document in the list.
K∂ID (id∂), where id∂ is the document ID.
3) Next, initialize a blind-storage system with the collection of all these index ﬁles (using BSTORE.Build).
4) Also, (outside of the blind-storage system) upload encryptions of all the documents labeled with their pseudorandom document index
• SSE.remove: To minimize the amount of information leaked, and for efﬁciency purposes, we rely on a lazy delete strategy.
1) Given a document ID id∂, check if a document with index η∂ = Ψ(cid:48)
K∂ID (id∂) exists, and if so remove it, using the ﬁle system interface
of the server. The index ﬁles (in the blind storage or the clear storage) are not updated for the keywords in this document right away,
but only during a subsequent search operation (see below).
• SSE.add: To add a document ∂ to the document collection, ﬁrst call SSE.remove to remove any earlier copy of a document with the
same document ID. Then proceed as follows:
1) Compute a pseudorandom document index η∂ = Ψ(cid:48)
2) Generate a random tag tag and add it to the document (say, as a preﬁx, before or after encrypting the document). Encrypt the
K∂ID (id∂).
η∂.
document and upload it, as in the SSE.indexgen phase, using the label η∂.
3) Then, for each keyword w that appears in this document, use the append facility of the clear-storage scheme to append a record
consisting of (η∂, tag) to the ﬁle with ﬁle-ID indexw to include η∂. Note that the append operation will create a ﬁle in the clear
storage system, if it does not already exist.
• SSE.search: Given a keyword w, retrieve and update the index ﬁles with ﬁle-ID indexw and indexw as follows:
1) Retrieve the index ﬁle indexw from the blind storage system using the ﬁrst stage of update operation of the blind storage scheme.
Also, retrieve the index ﬁle indexw from the clear storage system, using the ﬁrst stage of its update operation. All the documents
containing the keyword w have their document indices listed in these two index ﬁles. Attempt to retrieve all these documents listed
from the server.
2) Some of the documents listed in the index ﬁle indexw could have been removed. Complete the blind storage update operation on
the ﬁle indexw to erase the removed ﬁles from its list, without changing the size of the ﬁle.
3) Some of the documents listed in the index ﬁle indexw may have been removed or replaced with newer versions. Complete the clear
storage update operation on the ﬁle indexw to remove from its list any document that could not be retrieved, or for which the listed
tag did not match the one in the retrieved document. (Both the update operations are completed in the same round).
One could add an extra round to ﬁrst check just the tags of the documents before retrieving the documents themselves.
Fig. 8: Searchable Encryption Scheme BSTORE-SSE
carried out in a single update operation of the blind storage
scheme, with little overhead.
In fact, for removing newly added documents too, we
follow a similar lazy delete strategy, for efﬁciency purposes.
(Otherwise, during a delete operation, the client will need
to fetch the index ﬁles for all the keywords in the deleted
document in order to update them, unless the server is willing
to carry out a small amount of computation.) However, we
need to account for the possibility that a document ID could
be reused and that a later version may not have a keyword
present in an earlier version. We associate a random tag with
a document to check if the version listed in an index is the
same as the current version.
Properly instantiated, this simple idea yields strictly better
security than prior dynamic searchable encryption schemes
[18], [10] which revealed more information about keywords
not searched for, especially when removing documents.
Clear Storage. To store the index ﬁles for newly added
documents, our SSE scheme uses a “clear storage” scheme
CLEARSTORE that supports the following operations:
• Files labeled with ﬁle-IDs can be stored (in the clear, without
any encryption). A two-stage update operation can be used
to read this ﬁle and then write back an updated version
(which could be shorter).
• In addition,
there is an efﬁcient append operation,
that
allows appending a record (of ﬁxed size) to the ﬁle in
constant time (without having to retrieve the entire ﬁle and
update it).
Note that a standard ﬁle-system interface provided by the
server can support all these operations. But the append op-
eration may not be supported by a cloud storage provider. In
this case, it can be implemented by the client, as we consider
in our evaluation.
We consider a simpliﬁed version of the SCATTERSTORE to
implement CLEARSTORE with efﬁcient append. In this imple-
mentation, to store a ﬁle f = (idf , dataf ), the ﬁle data dataf is
stored (unencrypted) in a subset of blocks of a pseudorandom
set (cid:98)Sf ⊆ Sf. We use a separate ﬁle-system interface (without
shortest preﬁx of Sf that contains (cid:98)Sf
append) to store ﬁxed-size header ﬁles labeled with the ﬁle-
name idf; This header ﬁle stores an index indicating the last
block of Sf that is occupied by the ﬁle (i.e., the length of the
14 To append a record to
a ﬁle, the client retrieves the header block via the ﬁle-system
interface, using the ﬁle-name idf. Then it generates Sf, and
recovers the ith block in Sf, where i is the index stored in the
header block. Then it checks if there is enough space in this
14The ﬁrst block of the data could also be stored in the header ﬁle. Note
that then it is possible that the header block itself contains all the data of the
ﬁle; in this case the index indicating the last block is set to 0.
650
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:56 UTC from IEEE Xplore.  Restrictions apply. 
last block, and if so adds the record there. Else, it generates κ
more entries in Sf, fetches those blocks from the clear storage,
adds the record to the ﬁrst empty block in this sequence,15
and updates the index of the last block stored in the header
ﬁle accordingly. Note that the number of blocks fetched is a
constant on average provided a block is large enough to contain
(say) κ records; the number of blocks written back is at most
two (and on the average, close to 1).
Choice of parameters. We instantiate BSTORE-SSE with
our SCATTERSTORE constructions. By choosing the parameter
κ for SCATTERSTORE, we can ensure that a single search
operation can typically be completed in one and half rounds
of interaction. This is because the typical size of an index ﬁle
could ﬁt into a few blocks, and by choosing κ = 80 as we
do in our experiments, the index ﬁle can often be retrieved
without having to fetch more blocks. However, in the worst
case (e.g., searching for the keyword “the,” as we report), two
and half rounds of interaction will be needed.
Theorem 2: Protocol BSTORE-SSE securely realizes FSSE
against honest-but-curious adversaries.
Proof Sketch: The security of this scheme is fairly straight-
forward to establish, since it uses the blind storage scheme as
a blackbox, and involves no other cryptographic primitive. All
the information available to the server from the blind storage
scheme as used in this construction (i.e., the access patterns of
the index ﬁles) is easily derived from the information that the
server is allowed to have in the searchable encryption scheme.
In other words, a simulator can simulate to the server all the
messages in the protocol using the information it obtains in
the ideal world. The details are straightforward, and hence
omitted.
VI.
IMPLEMENTATION DETAILS
We implemented prototypes of our blind storage and
searchable encryption schemes. The code was written in
C++ using open-source libraries. We used Crypto++ [1] for
the block cipher (AES) and collision-resistant hash function
(SHA256) implementations.
As our schemes only require upload and download interface
and do not require any computation to be performed on the
server, they can be implemented on commercially available
cloud storage services. As a proof of concept, we further
implemented a C++ API to interface with Dropbox’s Python
API. This enables a Dropbox user to use a C++ implementation
of BSTORE-SSE (using SCATTERSTORE) with Dropbox as
the server. In our Dropbox implementation, each block in the
SCATTERSTORE scheme is kept as a ﬁle in Dropbox. We
recommend using SCATTERSTORE with a block size that is
a multiple of the block size in the cloud storage provider’s
storage (typically, 4KB).
VII. SEARCHABLE ENCRYPTION EVALUATION
For concreteness, we will compare the performance of our
SSE scheme with that of the recent scheme in [18], as one
15Unlike in the case of blind-storage, if no empty block is found among
the blocks fetched, the client can go on to fetch more blocks. This also allows
one to optimistically fetch a smaller number of blocks, without a signiﬁcant
penalty.
of the most efﬁcient dynamic SSE schemes in the literature,
implemented in a comparable setting. The more recent work
of [6] offers a possibly more optimized version of this protocol
(without dynamic functionality), but
is harder to compare
against experimentally, as the reported implementation was in
a high performance computing environment. We remark that
for the case of simple keyword searches (which is not the focus
of [6]), the construction of [6] is similar to that of [10], [18],
and is expected to show similar performance.
We focus on computational costs; space and communica-
tion overheads in the prior constructions are often not reported
making a direct comparison hard.
• The computation times reported are for the client. In our
case the server is devoid of any computation (beyond simple
storage tasks) and hence this constitutes all the computation
in the system. In contrast, in previous SSE schemes, the
server’s computation is often much more than that of the
client. Thus it would already be a signiﬁcant improvement
if our client computation costs are comparable to the client
computation costs in prior work. As we shall see, this is
indeed the case.
• There are several possible engineering optimizations in the
Blind-Storage scheme which can signiﬁcantly improve the
performance of the SSE scheme (for instance, the ﬁrst one
listed in Section IV-B3 cuts down the time taken for the
search operation by a factor of α or more). None of these
optimizations have been implemented in the prototype used
for evaluation.
Datasets. We use two datasets to evaluate our searchable
encryption scheme, emails and documents.
1) For emails we use the Enron dataset [2] which was also
used by [18] and several other works. From the Enron e-
mail dataset, we selected a 256MB subset, consisting of about
383,000 unique keywords and 20,695,000 unique (document,
keyword) pairs. In the experiments involving smaller amounts
of data, subsets of appropriate sizes were derived from these
datasets.
2) For documents, we created a dataset with 1GB of four types
of documents, namely PDF, Microsoft PowerPoint, Microsoft
Word and Microsoft Excel. The documents were obtained by
searching for English language documents with ﬁletypes pdf,
ppt, doc and xls, using Google search. The resulting collection
consists of 1556 documents (roughly evenly distributed among
the four ﬁletypes), with over 214,000 unique keywords and
about 1,372,000 unique (document, keyword) pairs.
Experiments. The code was compiled without any optimiza-
tions on Apple Mac OS X. We used a well provisioned laptop
– with Intel Core i7 3615QM processor, 8GB memory, running
Mac OS X 10.9 – for the experiments, keeping in mind that
the typical user of our system will use searchable encryption
on a cloud via her personal computer, just the same way a
cloud storage service like Dropbox is used. This is in contrast
with prior research which typically evaluated their work on
large servers with large amounts of memory.
As we shall see below, our scheme is highly scalable
and practically efﬁcient. We cannot offer a direct comparison
between our performance speeds and that of [18], because of
651
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:56 UTC from IEEE Xplore.  Restrictions apply. 
different hardware conﬁguration and limited test equipment
information presented in [18]. Nevertheless, our evaluation
shows that our scheme should be signiﬁcantly more efﬁcient
than that of [18].
A. Micro-benchmarks – File-keyword pair analysis
In [18], micro-benchmarks were used to evaluate the SSE
operations. We do the same for the SSE.indexgen algorithm.
(For our search, add and delete operations, the performance
is essentially independent of the total number of ﬁle-keyword
pairs already stored in the system, and this micro-benchmark
does not provide a meaningful evaluation of these operations.
These operations are evaluated differently, as explained below.)
Figure 9 shows micro-benchmarks for our scheme. The
parameters used in the scheme are held constant, and are the
same as detailed in the next section. Each data point is an
average of 5 runs of SSE.indexgen. Note that the amortized
per-pair time falls as the number of pairs increases, before
tending to 1.58µs; this is because our SSE.indexgen operation
involves encrypting the whole array D (which has the same
size in all the experiments), and this overhead does not increase
with the number of pairs.
Compared to the time for index generation operation re-
ported in [18], our performance is signiﬁcantly better. [18]
reports a per-pair time of 35µs for the same operation. Thus
our index generation operation is an order of magnitude faster.
2) Index Generation: Index generation is computationally