Seed 
fenhelua.com 
hefulena.com 
2 
2 
C&C domain 
C&C domain 
edirneli.net 
Seed 
Max score 
glamfromeast.com 
datalinksol.com 
babystophouse.com 
3 
4 
5 
a site or clicking a link in an email). We develop a new
C&C communication detector (whose details are given in
Section IV-C) that utilizes a combination of enterprise-speciﬁc
and generic features. Interestingly, the detected C&C domains
and the hosts contacting them can be used to seed the same
algorithm and identify related suspicious domains and com-
promised hosts.
The output in both modes of operation is a list of suspicious
domains in decreasing order of their scores and the list of hosts
contacting them. These are presented to the enterprise SOC
for further investigation after additional context information
(e.g., domain registration) is added to help the analyst during
investigation.
Rare domains 
After providing an overview of our system, we give here
more technical details of our methods.
IV. SYSTEM DETAILS
Fig. 2: Application of belief propagation.
of iterations is reached, and returns a list of labeled malicious
domains ordered by suspiciousness level.
The belief propagation algorithm depends on a number of
parameters (e.g., threshold for C&C communication, threshold
for domain similarity, etc.). These parameters are tailored to a
particular enterprise after training for a month to determine the
contribution of each relevant feature in computing the speciﬁc
values for that enterprise.
B. Example
Figure 2 shows an example of applying the belief propa-
gation algorithm. Starting from two seed domains (marked in
red), three hosts (Hosts 1-3) are added to the graph in the ﬁrst
iteration. In second iteration, two C&C domains contacted by
Host 1 are detected and added to the graph, as well as another
host contacting these domains. In third iteration, in absence of
additional C&C activity, the domain of maximum score among
all rare domains contacted by Hosts 1-4 is added to the graph.
The algorithm continues to score rare domains visited by hosts
in the graph and incrementally build the bipartite graph until
the stopping condition is met. In this example, two distinct
communities of malicious domains belonging to two attack
campaigns are highlighted.
C. Modes of operation
Our detection method operates in two modes. In the ﬁrst,
called SOC hints, we use the incidents that the enterprise SOC
investigated as starting points (or seeds) in the belief propaga-
tion algorithm. Given either hosts or domains conﬁrmed mali-
cious, the algorithm identiﬁes other related malicious domains
(likely part of the same campaign) and internal compromised
hosts that were unknown previously. This mode automates the
manual investigation process that the SOC team performs and
captures relationships between domains used by attackers in
different stages of a campaign.
In the no-hint mode, we don’t
leverage existing seeds
of known malicious activity. Our insight here is that C&C
communications are automated, high-frequency activities dis-
tinctive from human-generated behavior (e.g., user visiting
A. Datasets, normalization and reduction
LANL dataset. The ﬁrst dataset we used consists of
anonymized DNS logs collected from the LANL internal
network over 2 months (February and March 2013). It includes
DNS queries initiated by internal hosts, responses from the
LANL DNS servers, event timestamps, and IP addresses of the
sources and destinations. All of the IP addresses and domain
names are anonymized consistently. The dataset also includes
20 simulated attack campaigns representative of the initial
stages of APT infection.
The LANL dataset consists of 3.81 billion DNS queries
and 3.89 billion DNS responses, amounting to 1.15 TB. To
allow efﬁcient analysis, we employ a number of data reduction
techniques. We ﬁrst restrict our analysis only to A records, as
they record the queries to domain names and their responses
(IP addresses) and information in other records (e.g., TXT)
is redacted and thus not useful. This step prunes 30.4% of
DNS records on average per day. We also ﬁlter out queries
for internal LANL resources (as our focus is on detecting
suspicious external communications), and queries initiated by
mail servers (since we aim at detecting compromised hosts).
AC dataset. The second dataset AC consists of two months
(January and February 2014) of logs collected by web proxies
that intercept HTTP/HTTPs communications at the border of
a large enterprise network with over 100,000 hosts. The logs
include the connection timestamp, IP addresses of the source
and destination, full URL visited, and additional ﬁelds speciﬁc
to HTTP communications (HTTP method, status code, user-
agent string, web referer, etc.). We also obtained a list of
domain IOCs used by the enterprise SOC.
Analyzing the AC web proxy dataset proved difﬁcult due to
its large scale and various inconsistencies. On average 662GB
of data is generated daily, resulting in a total of 38.14TB of
data over two months. This dataset is 33 times larger than the
LANL dataset, and much richer in information. However, the
AC dataset has some inconsistencies due to multiple time zones
of collection devices and dynamic assignment of IP addresses.
We omit here a description of our normalization procedure,
but we converted all timestamps into UTC and IP addresses to
hostnames (by parsing the DHCP and VPN logs collected by
4848
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:58:29 UTC from IEEE Xplore.  Restrictions apply. 
the organization). We then extract the timestamp, hostname,
destination domain, destination IP, user-agent string, web
referer and HTTP status code ﬁelds for our analysis. We do
not consider destinations that are IP addresses.
s
n
i
a
m
o
d
f
o
r
e
b
m
u
N
 1e+07
 1e+06
 100000
 10000
 1000
All
Filter internal queries
Filter internal servers
New destinations
Rare destinations
03-04
03-05
03-06
03-08
03-09
03-10
03-07
Day
Fig. 3: The number of domains encountered daily in LANL
after data reduction for the ﬁrst week of March.
Rare destinations. In the analysis and results presented in
the following sections, we focus on “rare” destinations in
our datasets. Our insight is that popular legitimate websites
(visited by a large user population) are better administered and
less likely to be compromised, but connections to uncommon
destinations may be indicative of suspicious behavior. More
speciﬁcally, we deﬁne rare destinations as: new domains (not
visited before by any internal hosts) that are also unpopular
(visited by a small number of internal hosts). We set the
threshold at 10 hosts based on discussion with the SOC.
To determine the rare destinations, we use the ﬁrst month
of data for proﬁling and build a history of external destinations
visited by internal hosts. We “fold” the domain names to
second-level (e.g., news.nbc.com is folded to nbc.com),
assuming that
this captures the entity responsible for the
domain. We maintain a history of (folded) destinations queried
by internal hosts, updated at the end of each day to include
all new domains from that day. A domain is considered new
on a particular day if it is not in the history.
Following the steps detailed above, we greatly reduce the
volume of data as shown in Figure 3. On average, starting from
80K hosts and 400K domains in the LANL dataset, we retain
only 3.3K hosts and 31.5K domains after reduction. In the AC
dataset, we reduce from 120K hosts and 600K domains to an
average of 20K hosts and 59K rare domains daily.
B. Belief Propagation Algorithm
The goal of the belief propagation (BP) algorithm, as
explained in Section III-A, is to detect communities of ma-
licious domains that belong to the same attack campaign. The
BP algorithm can be applied in two modes: with hints of
compromised hosts provided by SOC, or without hints. In the
latter case the C&C communication detector is run ﬁrst to
identify a set of potential C&C domains and hosts contacting
them. These are given as seeds to the same BP algorithm.
Algorithm 1 gives pseudocode for BP starting from a set of
compromised hosts H, and set of malicious domains M.
The algorithm maintains several variables: R the set of
rare domains contacted by hosts in H and N the set of
newly labeled malicious domains (in a particular iteration).
In each iteration, the algorithm ﬁrst detects suspicious C&C-
like domains among set R using function Detect C&C whose
exact implementation will be provided next section. If no
suspicious C&C domains are found, the algorithm computes
a similarity score for all rare domains in R with function
Compute SimScore. The domain of maximum score (if
above a certain threshold Ts) is included in set M. Finally
the set of compromised hosts is expanded to include other
hosts contacting the newly labeled malicious domain(s). The
algorithm iterates until the stopping condition is met: either no
new domains are labeled as malicious (due to their scores being
below the threshold) or the maximum number of iterations has
been reached. The output is an expanded lists of compromised
hosts H and malicious domains M.
It’s important to note that domain scores are computed as
weighted sums of features, where the weights are determined
through supervised learning (using linear regression). Thus,
the algorithm is a novel combination of belief propagation,
an unsupervised graph inference algorithm, with a supervised
learning method.
Algorithm 1 [Belief Propagation]
/* H ← set of seed hosts */
/* M ← set of seed domains */
/* dom host is a mapping from a domain to set of hosts contacting it */
/* host rdom is a mapping from a host to set of rare domains visited */
function BELIEF PROPAGATION(H,M):
R ← ∪h∈H host rdom[h]
while (not stop condition) do
N ← ∅
for dom in R \ M do
/* set of newly labeled malicious domains */
score[dom] ← Compute SimScore(dom)
if N = ∅ then
if Detect C&C(dom) then
N ← N∪ {dom}
R ← R\ {dom}
for dom in R \ M do
max score ← max(score[dom])
D ← domains of maximum score
if max score ≥ Ts then
N ← N ∪ D
if N (cid:6)= ∅ then
M ← M ∪ N
H ← H ∪ (∪d∈N dom host[d])
R ← R ∪ (∪h∈H host rdom[h])
C. Detection of C&C communication
Dynamic histograms. As discussed in Section II-A backdoors
initiate automated communication with C&C domains to allow
attackers access into the victim environment. We aim at detect-
ing automated connections with fairly regular timing patterns,
but be resilient to outliers (e.g., large gaps in communication)
and randomization between connections. For every rare domain
contacted by a host with a certain minimum frequency (set
at 4) during the daily observation window we generate the
histogram of inter-connection intervals and compare it to that
of a periodic distribution.
To be resilient to bin alignment we propose a dynamic
histogram method. We set up a maximum bin width W and
cluster the inter-connection intervals of successive connections
from a host
to a domain (using a Greedy approach). We
then deﬁne the bins dynamically from the generated clusters.
We compare the resulting histogram with that of a periodic
distribution with period equal to the highest-frequency interval.
For comparing the two histograms we choose the Jeffrey
4949
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:58:29 UTC from IEEE Xplore.  Restrictions apply. 
divergence metric motivated by the fact that it is “numerically
stable and robust to noise and size of histogram bins” [36].
Finally we label the communications between a host and a
domain automated if the statistical distance between the two
histograms is at most JT . The bin width W and threshold JT
control the resiliency of the method to outliers and randomiza-
tion between connections. We discuss their selection according
to the LANL dataset in Section V-B.
Additional features. For each rare automated domain we
extract six additional features for the C&C detector:
Domain connectivity features: We consider the number of hosts
contacting the domain (NoHosts) called domain connectivity
and the number of hosts with automated connections to the
domain (AutoHosts). The intuition here is that most rare
legitimate domains are contacted by only one host, but the
probability of multiple hosts contacting a rare domain increases
when the hosts are under the control of the same attacker.
Web connection features: Based on discussions with SOC,
web connections with no referer may indicate automated
connections (not
initiated by a user). To capture this, we
include a feature NoRef denoting the fraction of hosts (among
all hosts contacting that domain) that use no web referer.
Software conﬁgurations in an enterprise are more homoge-
nous than in other networks (e.g., university campus), and as
such we expect that most user-agent strings are employed by
a large population of users. With this intuition, the rare user-
agent strings, those used by a small number of hosts, might
indicate unpopular software installed on the user machine
which can potentially be associated with suspicious activities.
We consider a feature RareUA denoting the fraction of hosts
that use no UA or a rare UA when contacting the domain.
To determine the popularity of UA strings, we maintain a
history of UAs encountered across time and the hosts using
those UAs. The UA history is built during the training phase
for a period of one month and then updated daily based on
new ingested data. A UA is considered rare (after the training
period of one month) if it is used by less than a threshold of
hosts (set at 10 based on SOC recommendation).
Registration data features: Attacker-controlled sites tend to use
more recently registered domains than legitimate ones [25]. In
addition, attackers register their domains for shorter periods of
time to minimize their costs in case the campaign is detected
and taken down. We query WHOIS information and extract
two features: DomAge (number of days since registration), and
DomValidity (number of days until the registration expires).
Scoring automated domains. We employ a supervised learn-
ing model for computing domain scores. We found 841
automated rare domains in the AC dataset in February. We
split this data into two sets, the ﬁrst two weeks used for
training and the last two weeks for testing. We also extract
the six features described above and query VirusTotal to get
an indication of the domain’s status. Domains with VirusTotal
score greater than 1 are labeled as “reported” and other
domains as “legitimate”.
Using the set of domains in the training set, we train
a linear regression model to predict the label of a domain
(reported or legitimate). The regression model outputs a weight
 1
 0.8
 0.6
 0.4
 0.2
s
n
i
a
m
o
d
f
o
n
o
i
t
c
a
r
F
 0
 0
 0.1
 0.2
Reported
 0.3
 0.4
Domain score
Legitimate
 0.5
 0.6
 0.7
 0.8
Fig. 4: CDFs of reported and legitimate domain scores.
for each feature, as well as the signiﬁcance of that feature. The
ﬁnal score for each automated domain is a linear combination
of feature values weighted by regression coefﬁcients. The
higher the score, the more suspicious the domain. Among all