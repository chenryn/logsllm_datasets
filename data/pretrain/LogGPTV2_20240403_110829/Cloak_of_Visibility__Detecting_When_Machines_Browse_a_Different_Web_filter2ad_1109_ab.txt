Browser
Context
Context
Context
Context
– 
– 
– 
–
–
– 
–
– 
–
–
–
–
capabilities such as content spinning and keyword stufﬁng.
We use this tear down later in Section IV to design an
anti-cloaking system capable of defeating all of the cloaking
techniques we discover. We make no claim our investigation
exhaustively covers cloaking software or whether our ten
particular programs are widely used by miscreants. Indeed, the
price and skill set required to operate some of the packages en-
sures their exclusivity to only the most afﬂuent or sophisticated
miscreants. That said, in conducting our analysis, we observed
that the set of cloaking techniques among the packages we
analyzed quickly converged to a ﬁxed set of signals. This may
indicate that while our coverage of cloaking software may be
incomplete, the best cloaking techniques are frequently shared
(or copied) much like exploit kits.
A. Cloaking Software Analysis
Of the cloaking applications we analyzed, only one (co-
incidentally, the most expensive) protected itself with a tool
with no publicly available unpacker. Languages for the various
cloaking applications ranged from C++, Perl, JavaScript, and
PHP. Sophistication ranged from drop-in scripts and plugins
for Wordpress pages, while others included a custom compi-
lation of Nginx for serving cloaked content. For each of the
applications, we manually investigated the underlying cloaking
logic and any embedded blacklists.
B. Cloaking Techniques
Cloaking techniques among the services we analyzed span a
gamut of network, browser, and contextual ﬁngerprinting. We
present a detailed breakdown of key capabilities in Table I.
These techniques only scratch the surface of browser ﬁnger-
printing that can otherwise enumerate screen size, font lists,
header orders, and divergent JavaScript and HTML imple-
mentations [3], [7], [8], [20]–[23], [29]. Intuitively, cloaking
services need only to deliver a ﬁngerprinting technique to
consumers that works. Without external pressure, there is no
reason for cloaking developers to adapt toward more complex
approaches proposed in research.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Breakdown of crawling bots covered by the
blacklist-as-a-service. The top ﬁve search engines include
Bing, Yahoo, Google, Baidu, and Yandex.
TABLE III: Breakdown of the types of businesses targeted
by the static blacklist, including the number of subnets and IP
addresses in those ranges.
Crawler
msn.com
yahoo.com
googlebot.com
baidu.com
yandex.com
Other
Unknown
Operator
Bing
Yahoo
Google
Baidu
Yandex
Ask, Lycos, etc.
–
Blacklisted IPs Overall
40.01%
27.82%
9.97%
5.25%
2.02%
2.06%
12.87%
21,672
15,069
5,398
2,846
1,092
1,117
6,972
1) Network Fingerprinting
IP Address: Some crawlers make no effort to obfuscate the
IP addresses ranges they appear from. This allows cloaking
services to enumerate the set of bot IPs. Of the ten cloaking
services we examine, four embed an IP blacklist; the others
allowed operators to upload their own IP blacklist. Of the four
IP blacklists, three mirrored the same blacklist-as-a-service
available for a $350 annual fee. The list, updated twice daily,
contained 54,166 unique IP addresses tied to popular search
engines and crawlers at the time of our analysis. This same
service provided a capability for cloaking clients to report
back all IP addresses and HTTP headers tied to incoming
visitors to a centralized server which the blacklist service
recommended for timely detection of new crawling activities.
We note that, despite us crawling sites that we later learned
via side-channels relied on this blacklist, our IPs were never
identiﬁed (potentially due to limited crawling or ineffective
detection on the service’s part).
Examining the blacklist-as-a-service in detail, we ﬁnd that
reverse DNS queries on the IP addresses surface crawlers
from Bing, Yahoo, Google, Baidu, and Yandex as detailed
in Table II—the top ﬁve search engines based on Alexa
ranking [1]. A tiny fraction of IPs relate to Ask, Lycos, and
smaller search engines. Consequently, any attempt to de-cloak
content from these IPs—even with seemingly organic browser
proﬁles—will fail. Another 6,972 IPs (12.9%) have no rDNS
information and appear from a distinct /16 CIDR block than
the top ﬁve search engines (which operate out of 77 CIDR
/16 blocks). We examine whether any of these overlap with
contemporaneous lists of proxies including 1,095 Tor exit
nodes and 28,794 HideMyAss IPs. We ﬁnd no evidence of
Tor or HideMyAss blacklisting.
The last of the four blacklists contained a signiﬁcant static
list of CIDR blocks encompassing 51,713,860 IP addresses
from 122 business entities as annotated in the blacklist
(shown in Table III). The coverage advertised by the blacklist
includes 30 security and anti-virus companies (e.g., Avira,
Comodo, Kaspersky) as well as 9 popular search engines
(e.g., Google, Microsoft, Yahoo). The list also covers multiple
hosting providers, public clouds (e.g., Amazon), registrars,
and proxy networks such as TOR that are unlikely sources
of organic trafﬁc. We also ﬁnd CIDR blocks that blacklist
entire ISPs, which as the blacklist author annotates, serve
Entity Type
Hosting providers
Security companies
Internet service providers
Search companies
Other companies
Proxy networks
Academic institutions
Hacker collectives
Individuals
Registrars
Total
Distinct Entities Subnets
508
346
49
32
12
10
14
4
4
4
983
42
30
27
9
3
3
2
2
2
2
122
IP Coverage
12,079,768
541,860
1,419,600
1,392,640
34,628
1,334
17,106,682
60
780
19,136,508
51,713,860
the crawlers for some security companies. Finally, the list
contains a few academic institutions (e.g., MIT, Rutgers),
hacker collectives (e.g., Germany’s Chaos Computer Club),
and individual researchers. We note that the abnormally large
number of IP addresses associated with academic institutions
results from universities such as MIT controlling an entire
Class A subnet that the blacklist owner opted to cover.
Interestingly, when comparing the blacklist-as-a-service and
the static list of IPs, we ﬁnd only two subnets in common, both
of which belong to Google. This indicates that the blacklist-as-
a-service is geared toward targeting search engines for SEO,
whereas the second blacklist focuses on security companies
and researchers.
Reverse DNS: In the event a crawler appears from a non-
blacklisted IP, four of the ten cloaking services perform a
rDNS lookup of a visitor’s IP. In the absence of a NXDOMAIN
error, the software compares the rDNS record against a list
of domains substrings belonging to Google (1e100, google),
Microsoft, Yahoo, Baidu, Yandex, Ask, Rambler, DirectHit,
and Teoma. Some of the cloaking services in turn add newly
identiﬁed crawler IPs to their embedded blacklists. As such,
any anti-cloaking pipeline must at a minimum crawl from IPs
with non-overlapping (or completely absent) rDNS informa-
tion.
targeting is left
Geolocation: We ﬁnd that four of the ten cloaking services
allow geographic targeting at a country level granularity based
on mappings. One goes so far as to embed a duplicate of
the MaxMind public GeoIP list for live querying. We do
not observe any pre-conﬁgured list of blocked geo origins;
all
to the software’s operator. This poses
a signiﬁcant challenge to anti-cloaking pipelines as network
infrastructure must support arbitrary network vantage points
around the globe. However, as we previously discussed, most
cloaking services fail to block Tor or major proxy providers.
As such, we consider these services as a potentially acceptable
sources of IP diversity.
746746
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IV: User-Agent substrings used by cloaking software
to identify crawlers.
Crawler User-Agent substrings
altavista
gigablast
sogou
aol
google
spider
askj
jeeves
teoma
baidu
lycos
yahoo
bingbot
msn
yandex
crawler
slurp
2) Browser Fingerprinting
User-Agent: Well-behaving search and advertisement crawlers
announce their presence with specialized User-Agent strings
commonly advertised on the operator’s website. Examples
include Google’s googlebot; Microsoft’s bingbot and msnbot;
and Yahoo’s slurp. We ﬁnd cloaking services ubiquitously rely
on User-Agent comparisons to explicitly block Google, Bing,
Yahoo, Ask, Baidu, Teoma, and Yandex. The cloaking services
also carve out generic exceptions for crawler and spider
to trigger cloaking logic. We provide a detailed breakdown
of the exact substring matching patterns in Table IV. Some
substrings capture overly broad applications. One cloaking
product contends with this fact by including a whitelist cover-
ing code.google.com/appengine (third-party services operating
on Google’s infrastructure) and via translate.google.com (in-
coming translation requests likely from users); however, all
other services make no exception.
JavaScript & Flash: JavaScript and Flash (or the lack thereof)
can serve as both a crawler ﬁngerprinting technique as well as
a redirection delivery method. We ﬁnd three cloaking services
rely on JavaScript execution as a technique for blocking
rudimentary crawlers. We ﬁnd no support for Flash-based
cloaking,
though there is evidence of such attacks in the
past [9]. One service also allows for operators to input custom
JavaScript ﬁngerprinting logic executed on each visit—a po-
tential route to conﬁgure SEO-focused cloakers for drive-by
exploit targeting, though we argue such attacks are orthogonal
and more likely to come via exploit kits [12]. For our study,
we opt to support both JavaScript and Flash when crawling to
cover all possible bases.
3) Contextual Fingerprinting
HTTP Referer: The ﬁnal ubiquitous cloaking technique
we observe across blackhat applications involves scanning
the HTTP Referer of incoming visitors to verify users
originate from search portals. The default whitelist matches
major crawlers (previously discussed in Table IV), though
miscreants can disable this feature. This technique prevents
crawlers from harvesting URLs and visiting them outside the
context they ﬁrst appeared. We contend with this cloaking
approach by always spooﬁng a HTTP Referer, the details
of which we expand on in Section IV.
Incoming Keywords: Keyword cloaking—supported by two
services—takes HTTP Referer cloaking a step further and
checks the validity of the search keywords that brought a
purported visitor to a miscreant’s page. Mechanistically, the
cloaking software extracts the list of search terms embedded
in the HTTP Referer using a set of customized regular
expressions for each search portal and then compares the
terms against an operator-provided list of negative keywords
and positive keywords (e.g., viagra, Tiffany) associated with
a page. The software triggers cloaking logic if a HTTP
Referer contains any negative keywords or lacks any pos-
itive keywords. We adapt to this technique by embedding
expected keywords in our HTTP Referer—derived from ad
targeting criteria or page content. We note that since Google
has been serving search results over TLS to logged-in users
since 2013, and therefore not passing keywords in the HTTP
referrer, the effectiveness of keyword cloaking has diminished.
We have found that keyword cloaking is no longer pivotal, as
it has been in previous works [32].
Time Window: Timing-based cloaking prohibits visitors from
accessing uncloaked content more than once in a speciﬁc time
window. The two services we ﬁnd offering this capability rely
on server side logs of a visitor’s IP address. Any repeat visitors
within 24 hours are redirected to cloaked content as if they
were a crawler. This raises a potential issue of false negatives
for an anti-cloaking pipeline without a sufﬁciently large IP
pool.
Order of Operations: While most of the cloaking services
we study rely on a single doorway page through which all
cloaking logic triggers, two of the services supports multiple
hops. Upon visiting a page, the software sets a short lived
cookie (e.g., seconds). When legitimate users interact with
the page, such as clicking on a URL,
the next doorway
checks whether this cookie is present and within the expiration
window. This allows cloaked websites to enforce a speciﬁc
sequence of actions (e.g., visit, click) where crawlers would
likely enqueue the URL for visiting at a later time or on
an entirely different machine. Our pipeline consolidates all