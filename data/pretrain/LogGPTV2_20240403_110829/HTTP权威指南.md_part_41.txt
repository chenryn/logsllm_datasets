目录了。和robots.txt标准一样，鼓励但并不强制使用这个标签。
机器人排斥标签是以如下形式，通过HTML的META标签来实现的：
1. 机器人的META指令
机器人META指令有几种不同的类型，而且随着时间的推移，以及搜索引擎及机器
人对其行为和特性集的扩展，很可能还会添加一些新的指令。最常用的两个机器人
META指令如下所示。
• NOINDEX 237
告诉机器人不要对页面的内容进行处理，忽略文档（也就是说，不要在任何索引
或数据库中包含此内容）。
Web机器人 ｜ 249
• NOFOLLOW
告诉机器人不要爬行这个页面的任何外连链接。
除了 NOINDEX 和 NOFOLLOW 之外，还有相对应的 INDEX 指令、FOLLOW 指令、
NOARCHIVE指令以及ALL和NONE指令。下面对这些机器人META标签指令进行
了总结。
• INDEX
告诉机器人它可以对页面的内容进行索引。
• FOLLOW
告诉机器人它可以爬行页面上的任何外连链接。
• NOARCHIVE
告诉机器人不应该缓存这个页面的本地副本。24
• ALL
等价于INDEX、FOLLOW。
• NONE
等价于NOINDEX、NOFOLLOW。
与所有HTML的META标签类似，机器人META标签必须出现在HTML页面的
HEAD区域中：
...
...
注意，标签的名称robots和内容都是大小写无关的。
很显然，不能发出一些会产生冲突或重复的指令，比如：
注24： 那些运行Google搜索引擎的人引入这个META标签，是为了向网管提供一种不允许Google提供其
内容缓存页面的手段。此标签还可以与META NAME="googlebot"一起使用。
250 ｜ 第9章
这种指令的行为很可能是未定义的，肯定会随机器人实现的不同而有所不同。
2. 搜索引擎的META标签
我们刚刚讨论了机器人的META标签，可以用来控制Web机器人的爬行和索引行
为。所有的机器人META标签中都包含了name="robots"属性。 238
还有很多其他类型的META标签可用，包括表9-5所示的各种标签。对内容索引型
搜索引擎机器人来说，DESCRIPTION和KEYWORDS META标签都非常有用。
表9-5 其他META标签指令
name= content= 描 述
DESCRIPTION  允许作者为Web页面定义一个短小的文本摘要。很多搜索引擎都
会查看META DESCRIPTION标签，允许页面作者指定一些短小
的摘要来描述其Web页面
KEYWORDS  关联一个由逗号分隔的Web页面描述词列表，为关键字搜索提供
帮助
REVISIT-AFTER25  告诉机器人或搜索引擎应该在指定天数之后重访页面，估计那时
候页面可能会发生变化
9.5 机器人的规范
1993年，Web机器人社会的先驱Martijn Koster为Web机器人的编写者们编写了
一个指南列表。有些建议已经过时了，但有很多建议仍然非常有用。在http://www.
robotstxt.org/wc/guidelines.html 上可以找到 Martijn 的原始论文“Guidelines for
Robot Writers”。
表9-6是为机器人设计者和操作人员提供的现代更新，这些更新的建议主要还是建
立在原始列表的思想和内容之上的。大部分指南都是针对万维网机器人提出的；但
它们同样适用于较小规模的爬虫。
注25： 这个指令很可能没有得到广泛的支持。
Web机器人 ｜ 251
表9-6 Web机器人操作员指南
操作指南 描 述
(1) 识别
识别你的机器人 用HTTP的User-Agent字段将机器人的名字告诉Web服务器。这样可
以帮助管理员理解机器人所做的事情。有些机器人还会在User-Agent
首部包含一个描述机器人目的和策略的URL
识别你的机器 确保机器人是从一台带有DNS条目的机器上运行的，这样Web站点才能
够将机器人的IP地址反向DNS为主机名。这有助于管理者识别出对机器
人负责的组织
239 识别联络人 用HTTP的From字段提供一个联络的E-mail地址
(2)操作
保持警惕 机器人可能会惹一些麻烦或引发一些抱怨。其中一些是由那些行为有偏差
的机器人造成的。一定要小心，注意保持机器人的正常行为。如果机器人
要全天候运行，就要格外小心。需要有操作人员不间断地对机器人进行监
视，直到它有了丰富的经验为止
做好准备 开始一次重要的机器人之旅时，一定要通知你所在的组织。你的组织可能
要观测网络带宽的耗费，作好应对各种公共查询的准备
监视并记录日志 机器人应该装备有丰富的诊断和日志记录工具，这样才能记录进展、识别
所有的机器人陷阱，进行完整性检查看看工作是否正常。监视并记录机器
人行为的重要性怎么强调也不过分。问题和抱怨总是会有的，对爬虫行为
的详细记录，有助于机器人操作者回溯所发生的事情。不管是为了调试出
错的Web爬虫，还是为了在不合理的投诉面前为其行为进行辩护，监视
和记录工作都是非常重要的
学习并适应 在每次爬行中你都会学到新的东西。要让机器人逐步适应，这样，它在每
次爬行之后都会有所进步，并能避开一些常见的陷阱
(3) 约束自己的行为
对URL进行过滤 如果一个URL指向的好像是你不理解或不感兴趣的数据，你可能会希望
跳过它。比如，以.Z、.gz、.tar或者.zip结尾的URL很可能是压缩文件
或归档文件。以.exe结尾的URL可能就是程序。以.gif、.tif、.jpg结尾
的URL很可能是图片。要确保你得到的就是你想要的
过滤动态URL 通常，机器人不会想去爬行来自动态网关的内容。机器人不知道应该如何
正确地格式化查询请求，并将其发送给网关，而它得到的结果也很可能是
错误的或临时的。如果一个URL中包含了cgi，或者有一个“?”，机器人
可能就不会去爬行这个URL了
对Accept首部进行过滤 机器人应该用HTTP的Accept首部来告诉服务器它能够理解哪种内容
遵循robots.txt 机器人应该接受站点上robots.txt的控制
252 ｜ 第9章
（续）
操作指南 描 述
制约自己 机器人应该记录访问每个站点的次数以及访问的时间，并通过这些信息来
确保它没有太频繁地访问某个站点。机器人访问站点的频率高于几分钟一
次时，管理员就要起疑心了。机器人每隔几秒钟就访问一次站点时，有些
管理员就会生气了。机器人尽可能频繁地去访问一个站点，将所有其他流
量都拒之门外时，管理员就会暴怒起来。
总之，应该限制机器人，使其每分钟最多只发送几条请求，并确保每条请
求之间有几秒钟的间隔。还应该限制对站点的访问总次数，以防止环路的
出现
(4) 容忍存在环路、重复和其他问题
处理所有返回代码 必须做好处理所有HTTP状态码的准备，包括各种重定向和错误码。还应
该对这些代码进行记录和监视。如果某站点出现大量不成功的结果，就应
该对其进行调查。可能是很多URL过期了，或者服务器拒绝向机器人提
供这些文档
规范URL 试着将所有URL都转化为标准形式来消除常见的别名
积极地避免环路的出现 努力地检测并避免环路的出现。将操纵爬虫的过程当作一个反馈回路。应
该将问题的结果和解决方法回馈到下一次爬行中，使爬虫在每次迭代之后
都能表现得更好 240
监视陷阱 有些环路是故意造成的恶意环路。这些环路可能很难检测。有的站点会带
有一些怪异的URL，要监视对这类站点进行的大量访问。这种情况可能就
是陷阱
维护一个黑名单 找到陷阱、环路、故障站点和不希望机器人访问的站点时，要将其加入一
个黑名单，不要再次访问这些站点
(5) 可扩展性
了解所需空间 事先通过数学计算明确你要解决的问题规模有多大。你可能会对应用程序
完成一项机器人任务所需的内存规模感到非常吃惊，这是由Web庞大的
规模造成的
了解所需带宽 了解你有多少网络带宽可用，以及在要求的时间内完成机器人任务所需的
带宽大小。监视网络带宽的实际使用情况。你很可能会发现输出带宽（请
求）要比输入带宽（响应）小得多。通过对网络使用情况的监视，可能还
会找到一些方法来更好地优化你的机器人，通过更好地使用其TCP连接
更好地利用网络带宽26
了解所需的时间 了解机器人完成其任务所需花费的时间，检查这个进度是否与自己的估计
相符。如果机器人的耗费与自己的估计相去甚远，可能就会有问题，需要
进行调查
注26： 更多有关TCP性能优化的内容请参见第4章。
Web机器人 ｜ 253
（续）
操作指南 描 述
分而治之 对大规模的爬行来说，很可能需要使用更多的硬件来完成这项工作，可以
使用带有多个网卡的大型多处理器服务器，也可以使用多台较小的计算机
共同配合工作
(6) 可靠性
彻底测试 在将机器人放出去之前，要对其进行彻底的内部测试。作好非现场测试准
备时，要先进行几次小型的处女航。收集大量结果并对性能和内存使用情
况进行分析，估计一下它们会怎样累积成较大问题
检查点 所有严谨的机器人都要保存其进展的快照，出现故障时可以从那里重新开
始。故障总是存在的：你会发现一些软件的bug，硬件也会出故障。大规
模机器人不能在每次出现这种情况时都从头开始。一开始就要设计检查点/
重启机制
故障恢复 预测故障的发生，对机器人进行设计，使其能够在发生故障时继续工作
(7) 公共关系
做好准备 机器人可能会让很多人感到困惑。要作好快速响应其询问的准备。制定一
个Web页面政策声明，对机器人进行描述，其中包括创建robots.txt文件
的详细指南
充分理解 有些与你联系，讨论机器人问题的人是了解情况并赞成的，有些人则很幼
稚。少数人会异常愤怒。有些人看起来好像都要发疯了。去争辩机器人的
努力有多么重要通常是没什么效果的。向他们解释拒绝机器人访问标准，
如果他们仍然很不高兴，就立即将投诉者的URL从爬行列表中删除，并
将其加入黑名单
积极响应 大多数不满意的网管都只是不太了解机器人。如果你能够进行迅速且专业
的响应，90%的投诉都会很快消失。另一方面，如果你等好几天才响应，
241 而机器人在继续访问这个站点，你面对的就将是一个非常愤怒的对手
9.6 搜索引擎
得到最广泛使用的Web机器人都是因特网搜索引擎。因特网搜索引擎可以帮助用户
找到世界范围内涉及任意主题的文档。
现在Web上很多最流行的站点都是搜索引擎。很多Web用户将其作为起始点，它
们会为用户提供宝贵的服务，帮助用户找到他们感兴趣的信息。
Web爬虫为因特网搜索引擎提供信息，它们获取Web上的文档，并允许搜索引擎
创建与本书后面的索引类似的索引，用以说明哪些文档中有哪些词存在。搜索引擎
是Web机器人的主要来源——让我们来快速了解一下它们是如何工作的。
254 ｜ 第9章
9.6.1 大格局
Web发展的初期，搜索引擎就是一些相当简单的数据库，可以帮助用户在Web上
定位文档。现在，Web上有数十亿可供访问的页面，搜索引擎已经成为因特网用户
查找信息不可缺少的工具。它们在不断地发展，以应对Web庞大的规模，因此，现
在已经变得相当复杂了。
面对数十亿的Web页面，和数百万要查找信息的用户，搜索引擎要用复杂的爬虫
来获取这数十亿Web页面，还要使用复杂的查询引擎来处理数百万用户产生的查
询负荷。
我们来考虑一下产品级Web爬虫的任务，它要获取搜索索引所需的页面，它要发出
数十亿条HTTP请求。如果每条请求都要花半秒钟的时间（对有些服务器来说可能
慢了，对另一些服务器来说可能快了27），（对十亿份文件来说）就要花费：
0.5秒×（100 0000 000）/（60秒/天）×（60分/小时）×（24小时/天）
如果请求是连续发出的，结果差不多是5700天！很显然，大型爬虫得更聪明一些，
要对请求进行并行处理，并使用大量机器来完成这项任务。但由于其规模庞大，爬
行整个Web仍然是件十分艰巨的任务。
9.6.2 现代搜索引擎结构
现在的搜索引擎都构建了一些名为“全文索引”的复杂本地数据库，装载了全世界
的Web页面，以及这些页面所包含的内容。这些索引就像Web上所有文档的卡片
目录一样。 242
搜索引擎爬虫会搜集Web页面，把它们带回家，并将其添加到全文索引中去。同