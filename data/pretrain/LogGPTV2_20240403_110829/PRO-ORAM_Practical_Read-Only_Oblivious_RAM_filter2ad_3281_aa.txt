title:PRO-ORAM: Practical Read-Only Oblivious RAM
author:Shruti Tople and
Yaoqi Jia and
Prateek Saxena
PRO-ORAM: Practical Read-Only Oblivious RAM
Shruti Tople∗
Microsoft Research
Yaoqi Jia
Zilliqa Research
Prateek Saxena
NUS
Abstract
Oblivious RAM is a well-known cryptographic primitive to
hide data access patterns. However, the best known ORAM
schemes require a logarithmic computation time in the general
case which makes it infeasible for use in real-world applica-
tions. In practice, hiding data access patterns should incur a
constant latency per access.
In this work, we present PRO-ORAM— an ORAM construc-
tion that achieves constant latencies per access in a large class
of applications. PRO-ORAM theoretically and empirically guar-
antees this for read-only data access patterns, wherein data is
written once followed by read requests. It makes hiding data
access pattern practical for read-only workloads, incurring
sub-second computational latencies per access for data blocks
of 256 KB, over large (gigabyte-sized) datasets. PRO-ORAM
supports throughputs of tens to hundreds of MBps for fetch-
ing blocks, which exceeds network bandwidth available to
average users today. Our experiments suggest that dominant
factor in latency offered by PRO-ORAM is the inherent network
throughput of transferring ﬁnal blocks, rather than the com-
putational latencies of the protocol. At its heart, PRO-ORAM
utilizes key observations enabling an aggressively parallelized
algorithm of an ORAM construction and a permutation opera-
tion, as well as the use of trusted computing technique (SGX)
that not only provides safety but also offers the advantage of
lowering communication costs.
1 Introduction
Cloud storage services such as Dropbox [4], Google Drive [8],
Box [2] are becoming popular with millions of users upload-
ing Gigabytes of data everyday [6]. However, outsourcing data
to untrusted cloud storage poses several privacy and security
issues [5]. Although encryption of data on the cloud guar-
antees data conﬁdentiality, it is not sufﬁcient to protect user
privacy. Research has shown that access patterns on encrypted
data leak substantial private information such as secret keys
∗Work done as a Ph.D student at National University of Singapore (NUS)
and user queries [25, 27]. One line of research to stop such
inference is the use of Oblivious RAM (ORAM) [22]. ORAM
protocols continuously shufﬂe the encrypted data blocks to
avoid information leakage via the data access patterns.
Although a long line of research has improved the perfor-
mance overhead of ORAM solutions [20, 32, 37, 40, 42, 43], it
is still considerably high for use in practice. Even the most
efﬁcient ORAM solutions incur at least logarithmic latency
to hide read / write access patterns [20, 34, 43], which is the
established lower bound for the general case. Ideally, hiding
access patterns should incur a constant access (communica-
tion) latency for the client, independent of the size of data
stored on the cloud server, and constant computation time per
access for the cloud server. To reduce the logarithmic access
time to a constant, we investigate the problem of designing
solutions to hide speciﬁc patterns instead of the general case.
We observe that a large number of cloud-based storage
services have a read-only model of data consumption. An
application can be categorized in this model when it offers
only read operations after the initial upload (write) of the
content to the cloud. For example, services hosting photos
(e.g., Flickr, Google Photos, Moments), music (e.g., Itunes,
Spotify), videos (e.g., NetFlix, Youtube) and PDF documents
(e.g., Dropbox, Google Drive) often exhibit such patterns. Re-
cently, Blass et al. have shown that designing an efﬁcient
construction is possible for “write-only” patterns wherein the
read accesses are not observable to the adversary (e.g. in log-
ging or snapshot / sync cloud services) [18]. Inspired by such
specialized solutions, we ask whether it is possible to achieve
constant latency to hide read-only access patterns? As our
main contribution, we answer the above question afﬁrmatively
for all cloud-based data hosting applications.
1.1 Approach
We propose PRO-ORAM— a practical ORAM construction for
cloud-based data hosting services offering constant latency for
read-only accesses. PRO-ORAM incurs a constant computation
and communication latency per access making it a promising
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 197solution to use in a large class of real-world applications. The
key idea to achieve constant latencies is to decompose every
request to read a data block into two separate sub-tasks of
“access” and “shufﬂe” which can execute in parallel. How-
ever, simply parallelizing the access and shufﬂe operations is
not enough to achieve constant latencies. Previous work that
employs such parallelization for the general case would incur
a logarithmic slowdown even for read-only accesses due to
the inherent design of the underlying ORAM protocols [41].
In designing PRO-ORAM, we make two important observa-
tions that allow us to achieve constant latency. First, we ob-
serve that there exists a simple ORAM construction — the
square-root ORAM [22] — which can be coupled with a
secure permutation (or shufﬂe) [33] to achieve idealized efﬁ-
ciency in the read-only model. A naïve use of this ORAM con-
struction incurs a worst-case overhead of O(N log2 N) to shuf-
ﬂe the entire memory with N data blocks. The non-updatable
nature of read-only data allows us to parallelize the access
√
and shufﬂe operations on two separate copies of the data. This
N) latency per access.
results in a de-amortized O(
Second, we design a secure method to distribute the work
done in each shufﬂe step among multiple computational units
√
without compromising the original security guarantees. Our
construction still performs O(
N) work per access but it is
parallelized aggressively to execute in a constant time. As-
√
suming a sufﬁcient number of cores, PRO-ORAM distributes
the total shufﬂing work among O(
N) threads without leak-
ing any information. Although the total computation work
is the same as in the original shufﬂe algorithm, the latency
reduces to a constant for read streaks1. With these two ob-
servations, we eliminate the expensive O(N log2 N) operation
from stalling subsequent read access requests in PRO-ORAM.
Thus, we show that a basic ORAM construction is better for
hiding read data access patterns than a complex algorithm that
is optimized to handle the general case. Further, we present
a proof for the correctness and security of PRO-ORAM. Our
improved construction of the shufﬂe algorithm maybe of in-
dependent interest, as it is widely applicable beyond ORAM.
PRO-ORAM can be applied opportunistically for applications
that expect to perform long streaks of read accesses inter-
mixed with infrequent writes, incurring a non-constant cost
only on write requests. Therefore, PRO-ORAM extends obliv-
iousness to the case of arbitrary access patterns, providing
idealized efﬁciency for “read-heavy” access patterns (where
long streaks of reads dominate). To reduce trust on software,
PRO-ORAM assumes the presence of a trusted hardware (such
as Intel SGX [1], Sanctum [19]) or a trusted proxy as assumed
in previous work on ORAMs [16, 28, 41].
1.2 Results
We implement PRO-ORAM prototype in C/C++ using Intel SGX
Linux SDK v1.8 containing 4184 lines of code [9]. We eval-
1A read streak is a sequence of consecutive read operations.
uate PRO-ORAM using Intel SGX simulator for varying ﬁle
/ block sizes and total data sizes. Our experimental results
demonstrate that the latency per access observed by the user
is a constant of about 0.3 seconds to fetch a ﬁle (or block)
of size 256 KB. Our empirical results show that PRO-ORAM
is practical to use with a throughput ranging from 83 Mbps
for block size of 100 KB to 235 Mbps for block size of 10
MB. These results are achieved on a server with 40 cores.
In real cloud deployments, the cost of a deca-core server is
about a thousand dollars [10]; so, the one-time setup cost
of buying 40 cores worth of computation seems reasonable.
Thus, PRO-ORAM is ideal for sharing and accessing media ﬁles
(e.g., photos, videos, music) having sizes of few hundred KB
on today’s cloud platforms. PRO-ORAM’s throughput exceeds
the global average network bandwidth of 7 Mbps asserting
that the inherent network latency dominates the overall access
time rather than computation latencies in PRO-ORAM [7].
Contributions. We summarize our contributions below:
• Read-only ORAM. We present PRO-ORAM— a practical
and secure read-only ORAM design for cloud-based data
hosting services. PRO-ORAM’s design utilizes sufﬁcient
computing units equipped with a trusted hardware prim-
itive.
• Security Proof. We provide a security proof to guarantee
that our PRO-ORAM construction provides obliviousness
in the read-only data model.
• Efﬁciency Evaluation. PRO-ORAM is highly practical with
constant latency per access for ﬁxed block sizes and
provides throughput ranging from 83 Mbps for a block
size of 100 KB to 235 Mbps for a block size of 10 MB.
2 Overview
Our main goal is to ensure two important characteristics: a)
hide read data access patterns on the cloud server; and b)
achieve constant time to access each block from the cloud.
2.1 Setting: Read-Only Cloud Services
Many applications offer data hosting services for images (e.g.,
Flickr, Google Photos, Moments), music (e.g., Itunes, Spotify),
videos (e.g., NetFlix, Youtube), and PDF documents (e.g.,
Dropbox, Google Drive). In these applications, either the
users (in the case of Dropbox) or the service providers (such
as NetFlix, Spotify) upload their data to the cloud server.
Note that the cloud provider can be different from the service
provider, for example, Netﬂix uses Amazon servers to host
their data. After the initial data is uploaded, users mainly
perform read requests to access the data from the cloud.
Let a data owner upload N ﬁles each having a ﬁle identiﬁer
to the cloud. A ﬁle is divided into data blocks of size B
198          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX AssociationUntrusted 
d 
 Server 
Trusted 
 Client 
Untrusted Storage 
4 
2 
(cid:23)
3 
(cid:24)
(cid:25)
(cid:26)
(cid:27)
Encrypted data 
(cid:2)(cid:14)(cid:13)(cid:15)(cid:16)(cid:14)(cid:13)(cid:11)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:4)(cid:14)(cid:10)(cid:18)(cid:20)(cid:6)(cid:16)(cid:9)(cid:1)(cid:4)(cid:18)(cid:6)(cid:7)(cid:12)(cid:1)
(cid:5)(cid:16)(cid:19)(cid:17)(cid:18)(cid:9)(cid:8)(cid:1)
(cid:1)(cid:3)(cid:6)(cid:16)(cid:8)(cid:20)(cid:6)(cid:16)(cid:9)(cid:1)
Upload 
(cid:23)
(cid:24)
2  3  4 
(cid:26)(cid:25)
(cid:27)
Read 
(cid:24)
Figure 1: Baseline setting: Cloud-provider with trusted hard-
ware and a compromised software stack. User uploads data
and makes read requests
and stored in an array on the untrusted storage at the server.
Each block is accessed using its corresponding address in
the storage array. To handle variable length ﬁles, one can
split large ﬁles into several data blocks and maintain a ﬁle
to blocks mapping table. However, for simplicity, we assume
each ﬁle maps to a single block and hence use the terms ﬁle
and block interchangeably in this paper. When a user requests
to fetch a ﬁle, the corresponding data block is read from the
storage array and is sent to the user. To ensure conﬁdentiality
of the data, all the ﬁles are encrypted using a cryptographic
key. The data is decrypted only on the user machine using the
corresponding key.
2.2 Threat Model
Leakage of access patterns is a serious issue and has been
shown to leak critical private information in several settings
such as encrypted emails, databases and others [25, 27]. In
our threat model, we consider that the adversary has complete
access to the encrypted storage on the cloud. An attacker can
exploit the vulnerabilities in the cloud software to gain access
to the cloud infrastructure including the storage system which
hosts encrypted content [5, 12]. Hence, we consider the cloud
provider to be untrusted with a compromised software stack.
The cloud provider can trace the requests or ﬁle access pat-
terns of all the users accessing the encrypted data. We restrict
each request to only read the data from the server. Essentially,
the adversary can observe the exact address accessed in the
storage array to serve each requested ﬁle. Along with access
to the storage system, the adversary can observe the network
trafﬁc consisting of requested data blocks sent to each user.
Scope. Our main security goal is to guarantee obliviousness
i.e., hide read access patterns of users from the cloud provider.
Although we consider a compromised server, we do not de-
fend against a cloud provider refusing to relay the requests
to the user. Such denial of service attacks are not within the
scope of this work. We only focus on leakage through address
access patterns and do not block other channels of leakage
such as timing or ﬁle length [44]. For example, an adversary
can observe the number of blocks fetched per request or the
frequency of requesting ﬁles to glean private information
about the user. However, our system can beneﬁt from existing
solutions that thwart these channels using techniques such as
padding ﬁles with dummy blocks and allowing ﬁle requests
at ﬁxed interval respectively [15].
2.3 Baseline: Trusted H/W in the Cloud
A well-known technique to hide data access patterns is us-
ing Oblivious RAM (ORAM) [22]. In ORAM protocols, the
encrypted data blocks are obliviously shufﬂed at random to
unlink subsequent accesses to the same data blocks. Standard
ORAM solutions guarantee obliviousness in a trusted client
and an untrusted server setting. It generally uses a private
memory called stash at the client-side to perform oblivious
shufﬂing and re-encryption of the encrypted data. In the best
case, this results in a logarithmic communication overhead be-
tween the client and the server [43]. To reduce this overhead,
previous work has proposed the use of a trusted hardware /
secure processor [16, 28] in the cloud or a trusted proxy [41].
This allows us to establish the private stash and a small trusted
code base (TCB) to execute the ORAM protocol in the cloud.
That is, instead of the client, the trusted component on the
cloud shufﬂes the encrypted data, thereby reducing the com-
munication overhead to a constant. Further, the trusted compo-
nent can verify the integrity of the accessed data and protect
against a malicious cloud provider [41]. Figure 1 shows the
architecture for our baseline setting with a trusted hardware
and a compromised software stack on the cloud.
In this work, we consider the above cloud setup with a
trusted hardware as our baseline. Speciﬁcally, we assume the
cloud servers are equipped with Intel SGX-enabled CPUs.
SGX allows creating hardware-isolated memory region called
enclaves in presence of a compromised operating system.
With enclaves, we have a moderate size of private storage
inaccessible to the untrusted software on the cloud. Further,
we assume that the trusted hardware at the cloud provider
is untampered and all the guarantees of SGX are preserved.
We do not consider physical or side-channel attacks on the
trusted hardware [26, 29, 31, 38, 45]. Defending against these
attacks is out of scope but our system can leverage any security
enhancements available in the future implementation of SGX
CPUs [30]. In practice, SGX can be replaced with any other
trusted hardware primitive available in the next-generation
cloud servers.
2.4 Solution Overview
We present a construction called PRO-ORAM— a Practical
Read-Only ORAM scheme that achieves constant computa-
tion latencies for read streaks. PRO-ORAM is based on square-
root ORAM but can be extended by future work to other
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 199ORAM approaches. It incurs default latency of the square-
root ORAM approach in case of write operations. Thus, one
can think of PRO-ORAM as a specialization for read streaks,
promising most efﬁciency in applications that are read-heavy,
but without losing compatibility in the general case.
Key Insight 1. The dominant cost in any ORAM scheme
comes from the shufﬂing step. In square-root ORAM, the
shufﬂing step is strictly performed after the access step [22].
This allows the shufﬂe step to consider any updates to the
blocks from write operations. Our main observation is that
for read-only applications, the algorithm need not wait for all
the accesses to ﬁnish before shufﬂing the entire dataset. The
key advantage in the read-only model is that the data is never
modiﬁed. Thus, we can decouple the shufﬂing step from the
logic to dispatch an access. This means the shufﬂe step can
execute in parallel without stalling the read accesses. We
give a proof for the correctness and security of PRO-ORAM in
Section 5. Although prior work has considered parallelizing
the access and shufﬂe step [41], our observations only apply to
the read-only setting, and our speciﬁc way achieves constant
latency which was not possible before.
Key Insight 2. Our second important observation allows us
√
to reach our goal of constant latency. We observe that the
Melbourne Shufﬂe algorithm performs O(
N) computation
√
operations for each access where each operation can be exe-
cuted independently [33]. Hence, the O(
N) computations
can be performed in parallel (multi-threaded) without break-
ing any security or functionality of the original shufﬂe algo-
rithm. This ﬁnal step provides us with a highly optimized
Melbourne Shufﬂe scheme which when coupled with square-
root ORAM incurs constant computation latency per access.
We further exploit the structure of the algorithm and propose
pipelining based optimizations to improve performance by
a constant factor (Section 4.4). We remark that our efﬁcient
version of the shufﬂe algorithm maybe of independent interest
and useful in other applications [21, 33].
Note that PRO-ORAM is compatible with data access pat-
terns that have writes after read streaks, since it can default to
running a synchronous (non-parallel) shufﬂe when a write is
encountered — just as in the original square-root ORAM. Of
course, the constant latency holds for read streaks and read-
heavy applications beneﬁt from this specialized construction.
Comparison to Previous Work. The most closely related
work with respect to our trust assumptions and cloud infras-
tructure is ObliviStore [41]. This protocol has the fastest
performance among all other ORAM protocols when used
in the cloud setting [17]. Similar to PRO-ORAM, ObliviStore
parallelizes the access and shufﬂe operations using a trusted
proxy for cloud-based data storage services.
We investigate whether ObliviStore’s construction can at-
tain constant latency when adapted to the read-only model.
We highlight that although the high-level idea of paralleliz-
ing ORAM protocol is similar to ours, ObliviStore differs
from PRO-ORAM in various aspects. ObliviStore is designed
to hide arbitrary patterns in the general case and hence uses