# PRO-ORAM: Practical Read-Only Oblivious RAM

## Authors
- Shruti Tople, Microsoft Research
- Yaoqi Jia, Zilliqa Research
- Prateek Saxena, National University of Singapore (NUS)

## Abstract
Oblivious RAM (ORAM) is a well-known cryptographic primitive designed to hide data access patterns. However, the best-known ORAM schemes typically require logarithmic computation time, making them infeasible for many real-world applications. In practice, hiding data access patterns should incur a constant latency per access.

In this work, we introduce PRO-ORAM, an ORAM construction that achieves constant latencies per access in a wide range of applications. PRO-ORAM theoretically and empirically guarantees this for read-only data access patterns, where data is written once followed by read requests. This makes hiding data access patterns practical for read-only workloads, with sub-second computational latencies per access for 256 KB data blocks over large (gigabyte-sized) datasets. PRO-ORAM supports throughputs of tens to hundreds of MBps for fetching blocks, which exceeds the network bandwidth available to most users today. Our experiments show that the dominant factor in the latency offered by PRO-ORAM is the inherent network throughput for transferring final blocks, rather than the computational latencies of the protocol.

At its core, PRO-ORAM leverages key observations enabling an aggressively parallelized algorithm for ORAM construction and a permutation operation, as well as the use of trusted computing techniques like Intel SGX. These techniques not only provide security but also reduce communication costs.

## 1. Introduction

Cloud storage services such as Dropbox, Google Drive, and Box are becoming increasingly popular, with millions of users uploading gigabytes of data daily. However, outsourcing data to untrusted cloud storage poses significant privacy and security risks. While encrypting data on the cloud ensures data confidentiality, it does not protect against access pattern leakage, which can reveal private information such as secret keys and user queries. One approach to mitigate this issue is the use of Oblivious RAM (ORAM), which continuously shuffles encrypted data blocks to prevent information leakage via access patterns.

Despite significant research efforts to improve ORAM performance, the overhead remains high. Even the most efficient ORAM solutions incur at least logarithmic latency to hide read/write access patterns, which is the established lower bound for the general case. Ideally, hiding access patterns should incur a constant access (communication) latency for the client, independent of the size of the data stored on the cloud server, and constant computation time per access for the cloud server.

To reduce the logarithmic access time to a constant, we focus on designing solutions for specific access patterns instead of the general case. We observe that many cloud-based storage services follow a read-only model, where data is uploaded once and then accessed only for reading. Examples include photo, music, video, and document hosting services. Inspired by specialized solutions for "write-only" patterns, we explore whether it is possible to achieve constant latency for read-only access patterns. Our main contribution is a positive answer to this question, presenting PRO-ORAM, a practical ORAM construction for read-only accesses.

### 1.1 Approach

PRO-ORAM is a practical ORAM construction for cloud-based data hosting services, offering constant latency for read-only accesses. The key idea to achieve constant latencies is to decompose each request to read a data block into two separate sub-tasks: "access" and "shuffle," which can execute in parallel. However, simply parallelizing these operations is not sufficient to achieve constant latencies. Previous work that employs such parallelization for the general case still incurs a logarithmic slowdown due to the inherent design of underlying ORAM protocols.

In designing PRO-ORAM, we make two important observations:
1. **Square-Root ORAM Coupled with Secure Permutation**: We observe that the square-root ORAM [22] can be coupled with a secure permutation (or shuffle) [33] to achieve idealized efficiency in the read-only model. A naive use of this ORAM construction incurs a worst-case overhead of \(O(N \log^2 N)\) to shuffle the entire memory with \(N\) data blocks. The non-updatable nature of read-only data allows us to parallelize the access and shuffle operations on two separate copies of the data, resulting in a de-amortized \(O(\sqrt{N})\) latency per access.
2. **Parallelized Shuffling**: We design a secure method to distribute the work done in each shuffle step among multiple computational units without compromising security. Our construction performs \(O(\sqrt{N})\) work per access but parallelizes it to execute in constant time. Assuming a sufficient number of cores, PRO-ORAM distributes the total shuffling work among \(O(\sqrt{N})\) threads without leaking any information. Although the total computation work is the same as in the original shuffle algorithm, the latency reduces to a constant for read streaks. With these observations, we eliminate the expensive \(O(N \log^2 N)\) operation from stalling subsequent read access requests in PRO-ORAM.

Thus, we show that a basic ORAM construction is better for hiding read data access patterns than a complex algorithm optimized for the general case. We also present a proof for the correctness and security of PRO-ORAM. Our improved construction of the shuffle algorithm may be of independent interest, as it is widely applicable beyond ORAM.

PRO-ORAM can be applied opportunistically for applications that expect to perform long streaks of read accesses intermixed with infrequent writes, incurring a non-constant cost only on write requests. Therefore, PRO-ORAM extends obliviousness to the case of arbitrary access patterns, providing idealized efficiency for "read-heavy" access patterns. To reduce trust on software, PRO-ORAM assumes the presence of trusted hardware (such as Intel SGX or Sanctum) or a trusted proxy, as assumed in previous ORAM work.

### 1.2 Results

We implement a PRO-ORAM prototype in C/C++ using the Intel SGX Linux SDK v1.8, consisting of 4184 lines of code. We evaluate PRO-ORAM using the Intel SGX simulator for varying file/block sizes and total data sizes. Our experimental results demonstrate that the latency per access observed by the user is a constant of about 0.3 seconds to fetch a file (or block) of size 256 KB. Empirical results show that PRO-ORAM is practical to use, with a throughput ranging from 83 Mbps for a block size of 100 KB to 235 Mbps for a block size of 10 MB. These results are achieved on a server with 40 cores. In real cloud deployments, the cost of a deca-core server is about a thousand dollars, so the one-time setup cost of buying 40 cores worth of computation seems reasonable.

Thus, PRO-ORAM is ideal for sharing and accessing media files (e.g., photos, videos, music) with sizes of a few hundred KB on today's cloud platforms. PRO-ORAM’s throughput exceeds the global average network bandwidth of 7 Mbps, indicating that the inherent network latency dominates the overall access time rather than the computation latencies in PRO-ORAM.

**Contributions:**
- **Read-Only ORAM**: We present PRO-ORAM, a practical and secure read-only ORAM design for cloud-based data hosting services. PRO-ORAM’s design utilizes sufficient computing units equipped with trusted hardware.
- **Security Proof**: We provide a security proof to guarantee that our PRO-ORAM construction provides obliviousness in the read-only data model.
- **Efficiency Evaluation**: PRO-ORAM is highly practical, with constant latency per access for fixed block sizes and providing throughput ranging from 83 Mbps for a block size of 100 KB to 235 Mbps for a block size of 10 MB.

## 2. Overview

Our main goal is to ensure two important characteristics:
1. Hide read data access patterns on the cloud server.
2. Achieve constant time to access each block from the cloud.

### 2.1 Setting: Read-Only Cloud Services

Many applications offer data hosting services for images, music, videos, and PDF documents. In these applications, either the users (e.g., Dropbox) or the service providers (e.g., Netflix, Spotify) upload their data to the cloud server. After the initial data is uploaded, users mainly perform read requests to access the data from the cloud.

Let a data owner upload \(N\) files, each with a file identifier, to the cloud. Each file is divided into data blocks of size \(B\) and stored in an array on the untrusted storage at the server. Each block is accessed using its corresponding address in the storage array. For simplicity, we assume each file maps to a single block and use the terms "file" and "block" interchangeably. When a user requests to fetch a file, the corresponding data block is read from the storage array and sent to the user. To ensure data confidentiality, all files are encrypted using a cryptographic key, and the data is decrypted only on the user's machine using the corresponding key.

### 2.2 Threat Model

Leakage of access patterns is a serious issue and has been shown to leak critical private information in several settings, such as encrypted emails and databases. In our threat model, we consider that the adversary has complete access to the encrypted storage on the cloud. An attacker can exploit vulnerabilities in the cloud software to gain access to the cloud infrastructure, including the storage system hosting encrypted content. Thus, we consider the cloud provider to be untrusted with a compromised software stack. The cloud provider can trace the requests or file access patterns of all users accessing the encrypted data. We restrict each request to only read the data from the server. Essentially, the adversary can observe the exact address accessed in the storage array to serve each requested file. Along with access to the storage system, the adversary can observe the network traffic consisting of requested data blocks sent to each user.

**Scope**: Our main security goal is to guarantee obliviousness, i.e., to hide read access patterns of users from the cloud provider. Although we consider a compromised server, we do not defend against a cloud provider refusing to relay requests to the user. Such denial of service attacks are not within the scope of this work. We focus on leakage through address access patterns and do not block other channels of leakage such as timing or file length. For example, an adversary can observe the number of blocks fetched per request or the frequency of requesting files to glean private information about the user. However, our system can benefit from existing solutions that thwart these channels using techniques such as padding files with dummy blocks and allowing file requests at fixed intervals.

### 2.3 Baseline: Trusted H/W in the Cloud

A well-known technique to hide data access patterns is using Oblivious RAM (ORAM). In ORAM protocols, the encrypted data blocks are obliviously shuffled at random to unlink subsequent accesses to the same data blocks. Standard ORAM solutions guarantee obliviousness in a trusted client and an untrusted server setting. They generally use a private memory called a stash at the client-side to perform oblivious shuffling and re-encryption of the encrypted data. In the best case, this results in a logarithmic communication overhead between the client and the server. To reduce this overhead, previous work has proposed the use of trusted hardware/secure processors or trusted proxies. This allows us to establish the private stash and a small trusted code base (TCB) to execute the ORAM protocol in the cloud. That is, instead of the client, the trusted component on the cloud shuffles the encrypted data, thereby reducing the communication overhead to a constant. Further, the trusted component can verify the integrity of the accessed data and protect against a malicious cloud provider. Figure 1 shows the architecture for our baseline setting with a trusted hardware and a compromised software stack on the cloud.

In this work, we consider the above cloud setup with trusted hardware as our baseline. Specifically, we assume the cloud servers are equipped with Intel SGX-enabled CPUs. SGX allows creating hardware-isolated memory regions called enclaves in the presence of a compromised operating system. With enclaves, we have a moderate size of private storage inaccessible to the untrusted software on the cloud. We assume that the trusted hardware at the cloud provider is untampered and all the guarantees of SGX are preserved. We do not consider physical or side-channel attacks on the trusted hardware. Defending against these attacks is out of scope, but our system can leverage any security enhancements available in future implementations of SGX CPUs. In practice, SGX can be replaced with any other trusted hardware primitive available in next-generation cloud servers.

### 2.4 Solution Overview

We present a construction called PRO-ORAM, a Practical Read-Only ORAM scheme that achieves constant computation latencies for read streaks. PRO-ORAM is based on square-root ORAM but can be extended to other ORAM approaches. It incurs the default latency of the square-root ORAM approach in case of write operations. Thus, one can think of PRO-ORAM as a specialization for read streaks, promising the most efficiency in applications that are read-heavy, but without losing compatibility in the general case.

**Key Insight 1**: The dominant cost in any ORAM scheme comes from the shuffling step. In square-root ORAM, the shuffling step is strictly performed after the access step. This allows the shuffle step to consider any updates to the blocks from write operations. Our main observation is that for read-only applications, the algorithm need not wait for all the accesses to finish before shuffling the entire dataset. The key advantage in the read-only model is that the data is never modified. Thus, we can decouple the shuffling step from the logic to dispatch an access. This means the shuffle step can execute in parallel without stalling the read accesses. We provide a proof for the correctness and security of PRO-ORAM in Section 5. Although prior work has considered parallelizing the access and shuffle steps, our observations apply specifically to the read-only setting, achieving constant latency, which was not possible before.

**Key Insight 2**: Our second important observation allows us to reach our goal of constant latency. We observe that the Melbourne Shuffle algorithm performs \(O(\sqrt{N})\) computation operations for each access, where each operation can be executed independently. Hence, the \(O(\sqrt{N})\) computations can be performed in parallel (multi-threaded) without breaking any security or functionality of the original shuffle algorithm. This final step provides us with a highly optimized Melbourne Shuffle scheme, which, when coupled with square-root ORAM, incurs constant computation latency per access. We further exploit the structure of the algorithm and propose pipelining-based optimizations to improve performance by a constant factor (Section 4.4). We note that our efficient version of the shuffle algorithm may be of independent interest and useful in other applications.

Note that PRO-ORAM is compatible with data access patterns that have writes after read streaks, as it can default to running a synchronous (non-parallel) shuffle when a write is encountered, just as in the original square-root ORAM. The constant latency holds for read streaks, and read-heavy applications benefit from this specialized construction.

**Comparison to Previous Work**: The most closely related work with respect to our trust assumptions and cloud infrastructure is ObliviStore [41]. This protocol has the fastest performance among all other ORAM protocols when used in the cloud setting. Similar to PRO-ORAM, ObliviStore parallelizes the access and shuffle operations using a trusted proxy for cloud-based data storage services. We investigate whether ObliviStore’s construction can attain constant latency when adapted to the read-only model. Although the high-level idea of parallelizing the ORAM protocol is similar to ours, ObliviStore differs from PRO-ORAM in various aspects. ObliviStore is designed to hide arbitrary patterns in the general case and hence uses more complex algorithms, leading to higher latencies even in the read-only setting.