title:On the Accuracy of Tor Bandwidth Estimation
author:Rob Jansen and
Aaron Johnson
On the Accuracy of Tor Bandwidth
Estimation
Rob Jansen(B) and Aaron Johnson
U.S. Naval Research Laboratory, Washington, D.C., USA
{rob.g.jansen,aaron.m.johnson}@nrl.navy.mil
Abstract. The Tor network estimates its relays’ bandwidths using relay
self-measurements of client traﬃc speeds. These estimates largely deter-
mine how existing traﬃc load is balanced across relays, and they are used
to evaluate the network’s capacity to handle future traﬃc load increases.
Thus, their accuracy is important to optimize Tor’s performance and
strategize for growth. However, their accuracy has never been measured.
We investigate the accuracy of Tor’s capacity estimation with an analy-
sis of public network data and an active experiment run over the entire
live network. Our results suggest that the bandwidth estimates under-
estimate the total network capacity by at least 50% and that the errors
are larger for high-bandwidth and low-uptime relays. Our work suggests
that improving Tor’s bandwidth measurement system could improve the
network’s performance and better inform plans to handle future growth.
1 Introduction
Tor [12] is an anonymous communication overlay network with thousands of relays
that forward over 200 Gbit/s of traﬃc for millions of daily clients [3,25] in order
to provide unlinkability between the source and destination of traﬃc ﬂows.
In order to balance client traﬃc across the relays, Tor relies on TorFlow to esti-
mate of the speed at which relays can forward traﬃc through the network [30],
and these forwarding capacity estimates are essential to both the performance and
security of the network [5,21,22,31]. A relay’s capacity estimate is derived from
a self-measurement called the observed bandwidth: the highest throughput it has
sustained over any ten second period over the last ﬁve days (see Sect. 2). This mea-
sure is imprecise and may be inaccurate in many realistic cases: (i) a new relay will
not have forwarded any traﬃc and thus will be estimated to have a low capacity
regardless of its available resources; (ii) a relay that is used inconsistently may not
sustain a high throughput long enough to result in an accurate capacity estimate;
and (iii) a relay that is underutilized will underestimate its capacity. TorFlow uses
relays’ capacity estimates as the basis for its relay selection algorithm that drives
more user traﬃc load to higher-capacity relays [30]. Therefore, inaccurate capacity
estimates could result in sub-optimal load balancing which would degrade user-
perceived network performance and security [21].
Inaccurate capacity estimates also make it more diﬃcult to understand how
to prioritize research and development eﬀort in order to plan future network
c(cid:2) Springer Nature Switzerland AG 2021
O. Hohlfeld et al. (Eds.): PAM 2021, LNCS 12671, pp. 481–498, 2021.
https://doi.org/10.1007/978-3-030-72582-2_28
482
R. Jansen and A. Johnson
improvements [32]. For example, obtaining funding to improve Tor scalability is
more challenging without understanding the current limits of the network [27].
Improper network management also complicates relay recruitment and retention,
and may dissuade the development of incentive schemes [13,14,17–19,26,28].
In this paper, we explore the inconsistency in Tor’s estimated relay capacities
using: (i) passive measurements collected by relays and published by Tor met-
rics [3]; and (ii) an active relay speed test measurement experiment. In Sect. 3
we study variability in relay capacity estimates, which we use as an indica-
tion of inaccurate estimation. We ﬁnd signiﬁcant variation in relays’ advertised
bandwidths: the capacity estimates of 25% of relays vary by more than 41%,
the capacity estimates of 10% of relays vary by 71% or more, and some relays’
capacity estimates vary by more than 200%. We ﬁnd that higher variation is
associated with lower capacity relays and with relays that are online less fre-
quently. In Sect. 4 we present an active speed test experiment, through which
we ﬁnd that: (i) Tor underestimates its total capacity by about 50%; (ii) most
relays increased their capacity estimate following our experiment (some by a 10×
or greater factor); and (iii) larger error is associated with high-capacity relays,
exit relays, and relays with lower uptimes than with other types of relays. Our
results suggest that indeed relay underutilization is a cause of signiﬁcant error
in capacity estimates.
Our work provides the ﬁrst systematic exploration of the error in Tor’s capac-
ity estimation technique, and our results suggest that improvements to capacity
estimates could signiﬁcantly improve load balancing and network performance.
Our research artifacts are available at https://torbwest-pam2021.github.io.
2 Background and Related Work
The Tor Network: The Tor network consists of thousands of relays forward-
ing traﬃc for millions of clients [3,25]. To assist in balancing client traﬃc load
across relays, Tor assigns a weight to each relay according to an estimate of the
relay’s available bandwidth and publishes relay information (including addresses,
weights, and various other ﬂags) in a network consensus document [2, Sect. 3.4.1].
To use the network, a Tor client downloads the consensus and computes selection
probabilities from the weights. The client builds a circuit through a series of typi-
cally three relays, using the selection probabilities to choose a relay for each posi-
tion; relays with the Exit ﬂag typically serve in the exit position, relays with the
Guard ﬂag (but not the Exit ﬂag) typically serve in the entry position, and relays
with neither ﬂag serve in the middle position [11]. The client tunnels application
data (e.g., web requests) through the constructed circuit, rotating to new circuits
every 10 min (or when they browse to new websites). Although circuits rotate fre-
quently, clients generally use long-term entry Guard relays [9] to help prevent pre-
decessor attacks [34]. To be a Guard, Tor requires that a relay maintain high uptime:
the percentage of hours during which it is online.
Relay Bandwidth: A relay’s forwarding capacity is the maximum sustainable
rate at which it can forward traﬃc through the network and is useful for balanc-
On the Accuracy of Tor Bandwidth Estimation
483
ing traﬃc load across relays. Relay operators do not directly report the true for-
warding capacity of their relays, so Tor uses a heuristic to estimate it. Each relay
calculates its observed bandwidth by tracking the highest throughput that it was
able to sustain for any 10 s period during each of the last 5 days [10, Sect. 2.1.1].
To bootstrap the observed bandwidth calculation, a relay conducts a bandwidth
self-test when it starts by creating four circuits through Tor and sending 125 KiB
over each circuit; if this process completes within 10 ss, the relay will start with
an observed bandwidth of 4 · 125/10 = 50 KiB/s (≈410 Kbits/s) [8]. Additional
remote measurements are conducted by TorFlow [30] (discussed below), and the
observed bandwidth is updated over time as a relay forwards client traﬃc. Relay
operators may limit the amount of bandwidth a relay consumes by conﬁguring
average bandwidth and burst bandwidth options, which control the reﬁll rate and
size of an internal token bucket rate limiter. Every 18 h, relays publish a server
descriptor ﬁle [10, Sect. 2.1.1] which contains their latest observed, average, and
burst bandwidth values. A relay’s advertised bandwidth is the minimum of the
observed and average bandwidths published by the relay, and is used as a basis
for load balancing.
Load Balancing: Tor uses a measurement tool called TorFlow [30] to assist
in balancing client traﬃc across relays. TorFlow measures relay performance by
creating two-hop circuits through each relay and downloading ﬁles ranging in
size from 16 KiB to 64 MiB from a known server through the circuit. TorFlow
produces relay weights by: (i) computing the ratio of the measurement speed of
each relay to the mean measurement speed of all relays; and (ii) multiplying each
relay’s ratio by its advertised bandwidth. The relay weights are published in the
consensus and used to compute relay selection probabilities as described above.
Related Work: Previous work has established that TorFlow is insecure and
vulnerable to manipulation, in part because a relay can detect when it is being
measured [5,21,22,33]. Several alternative bandwidth measurement systems that
produce relay weights have been proposed. SmarTor [4] and Simple Bandwidth
Scanner [24] are similar in measurement design to TorFlow and suﬀer from simi-
lar limitations. EigenSpeed proposes that relays conduct peer measurement, and
produces per-ﬂow throughput estimates rather than estimates of relay forward-
ing capacity [31]. PeerFlow is a passive peer measurement system that proposes a
secure aggregation inference technique to produce relay capacity estimates from
multiple peers observations [22]. TightRope proposes a centralized approach for
optimally balancing load given a set of accurate capacity weights [7], and Ting
focuses on measuring latencies between relays [6].
Dingledine outlines the lifecycle of a new relay and explains that it can take
three days for a relay to be measured by TorFlow, several weeks for a relay
to obtain the Guard ﬂag, and even longer to reach steady state [8]. Dingledine
motivates the need for further analysis of Tor metrics data to better understand
relay operations in the real world. Using both passive and active measurements,
our work provides the ﬁrst systematic exploration of the error in Tor’s capacity
estimation technique. More recently, Greubel et al. analyze load distribution in
Tor and ﬁnd that relays with more forwarding capacity are associated with larger
484
R. Jansen and A. Johnson
relay weights [15]. Although we are focused on measuring the accuracy of for-
warding capacity estimates rather than relay weights, the association established
by Greubel et al. will aid in explaining some of our results.
3 Analysis of Tor Metrics Data
To better understand the accuracy of Tor’s capacity-estimation heuristic, we ana-
lyze publicly available Tor metrics data [3]. Relays passively measure through-
put over time and publish bandwidth information in their server descriptors
[10, Sect. 2.1.1], while the load-balancing weights that TorFlow derives from the
bandwidth information are published in network consensus ﬁles [2, Sect. 3.4.1].
The Tor Project has collected these documents for over a decade [3], and we ana-
lyze the data published throughout the 52 week period starting on 2018-08-01.
Relay Capacity Variation: A relay with a perfect capacity estimation algo-
rithm would consistently report the same advertised bandwidth; thus, variation
in advertised bandwidths indicates inaccurate capacity estimation. Let A(r, w)
be the sequence of advertised bandwidths published by relay r during week w.
We quantify the variability in A(r, w) by computing the relative standard devi-
ation (RSD) as
RSD(A(r, w)) = stdev(A(r, w))/mean(A(r, w))
(1)
where stdev() and mean() compute the standard deviation and mean, respec-
tively. Higher RSDs are associated with more ﬂuctuation of the capacity estimate
around the expected capacity and indicate error in the estimation.
We summarize the variability in the estimated relay capacity for relay r
by computing the mean of RSD(A(r, w)) over all 1 ≤ w ≤ n weeks in which r
published at least one valid server descriptor. We remove potential sources of bias
by considering a server descriptor for r valid unless: (i) it was published before
r was measured (i.e., before r appeared in a consensus without the unmeasured
ﬂag); or (ii) it was published during a week in which a change in r’s average or
burst bandwidth options caused a reduction in the advertised bandwidth. We
call mean(RSD(A(r, 1)), . . . , RSD(A(r, n))) the mean weekly RSD for relay r.
We compute mean weekly RSDs for only those relays that were not ﬂagged as
unmeasured in at least one consensus (to avoid potential bias from bootstrapping
new relays). Although we suppose that the true forwarding capacity of each relay
does not often change (i.e., relays do not often upgrade to faster network access
links), computing the RSD on a weekly basis ensures that any upgrades that do
occur during one of the weeks in our analysis period are likely to only aﬀect a
small fraction of the n total weeks that we consider (and thus have a small eﬀect
on the mean weekly RSD summary statistic).
Analysis Results: We compute mean weekly RSDs for relays over n = 52
weeks, where w = 1 includes the seven days starting on 2018-08-01 and w = 52
includes the seven days starting on 2019-07-24. During this analysis period,
34,850 unique relays appeared across 8,736 consensus ﬁles (many more than are
On the Accuracy of Tor Bandwidth Estimation
485
Fig. 1. The distribution of the mean weekly RSD over all valid relays. For each valid
relay r, we compute the RSD for each week that r published a valid server descriptor,
and then compute the mean over all such weeks to get the mean weekly RSD for r.
online at any given time due to churn). Of these, 11,296 (32%) were never mea-
sured (i.e., never appeared in any consensus without the unmeasured ﬂag), and
an additional 1,503 (4.3%) were measured but did not publish a valid descriptor
(as explained above). We consider the remaining 22,051 relays (63%) as valid in
our analysis, and we compute the mean weekly RSDs for these valid relays.
Figure 1 compares the distribution of the mean weekly RSD over all such
valid relays and over distinct subsets that are separated by common relay char-
acteristics (position, uptime, advertised bandwidth, and selection probability).
Over all relays (the solid line in each subﬁgure), we ﬁnd that the reported adver-
tised bandwidths exhibit signiﬁcant variation. The mean over all relays of the
mean weekly RSD is 27%, while 25% and 10% of the relays have a mean weekly
RSD of 35% and 66% or more, respectively (and a non-trivial fraction of relays
have RSDs of 100% or greater). Such variation is larger than expected when the
true capacity does not change. We also ﬁnd that the largest RSDs are associ-
ated with lower capacity relays and relays that are online less frequently, as we
explain next.
Position: A relay’s position is that in which it serves most frequently throughout
the year. We compare mean weekly RSDs across relays of diﬀerent positions
in Fig. 1a. We observe that guard relays exhibited signiﬁcantly lower variation
in their advertised bandwidths than did exits and middles: compared to exits,
486
R. Jansen and A. Johnson
guards’ RSDs dropped from 16% to 7.0% at P50 and from 71% to 23% at P90.
Tor requires that relays must be stable with high uptime to receive the Guard
ﬂag, which may help explain this result.
Uptime. A relay’s uptime is the percentage of hours during which it was online
over the entire year. We compare mean weekly RSDs across relays with diﬀerent
ranges of uptime in Fig. 1b. We observe that relays with lower uptime were
correlated with larger mean weekly RSDs: 25% of the lowest uptime (≤121 days)
and highest uptime (>243 days) relays had mean weekly RSDs of 46% or more
and 6.7% or more, respectively. This result suggests that relays that are less
consistently available are underutilized by Tor clients and are thus unable to
observe enough traﬃc to reach their true capacity.
Advertised Bandwidth. We compare mean weekly RSDs across relays with dif-
ferent ranges of mean advertised bandwidths (here, the mean is computed over