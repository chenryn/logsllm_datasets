tion time. We will later relax these assumptions.
We deﬁne the expected ﬂow transmission time, denoted by
T i for any ﬂow i, to be the remaining ﬂow size divided by
its maximal sending rate Rmax
. The maximal sending rate
Rmax
is the minimum of the sender NIC rate, the switch link
rates, and the rate that receiver can process and receive.
Whenever network workload changes (a new ﬂow arrives,
or an existing ﬂow terminates), the centralized scheduler
recomputes the ﬂow transmission schedule as follows:
i
i
1. Be = available bandwidth of link e, initialized to e’s
2. For each ﬂow i, in increasing order of T i:
line rate.
(a) Let Pi be ﬂow i’s path.
(b) Send ﬂow i with rate Rsch
(c) Be ← Be− Rsch
for each e ∈ Pi.
i
i = min∀e∈Pi (Rmax
i
, Be).
Distributed Algorithm: We eliminate the unrealistic as-
sumptions we made in the centralized algorithm to construct
a fully distributed realization of our design. To distribute
its operation, PDQ switches propagate ﬂow information via
explicit feedback in packet headers. PDQ senders maintain
a set of ﬂow-related variables such as ﬂow sending rate and
ﬂow size and communicate the ﬂow information to the in-
termediate switches via a scheduling header added to the
transport layer of each data packet. When the feedback
reaches the receiver, it is returned to the sender in an ACK
packet. PDQ switches monitor the incoming traﬃc rate of
each of their output queues and inform the sender to send
data with a speciﬁc rate (R>0) or to pause (R=0) by an-
notating the scheduling header of data/ACK packets. We
present the details of this distributed realization in the fol-
lowing sections.
3.1 PDQ Sender
Like many transport protocols, a PDQ sender sends a
SYN packet for ﬂow initialization and a TERM packet for
ﬂow termination, and resends a packet after a timeout. The
sender maintains standard data structures for reliable trans-
mission, including estimated round-trip time and states (e.g.,
timer) for in-ﬂight packets. The PDQ sender maintains sev-
its current sending rate (RS, initial-
eral state variables:
ized to zero), the ID of the switch (if any) who has paused
the ﬂow (P S, initialized to ø), ﬂow deadline (DS, which
is optional), the expected ﬂow transmission time (T S, ini-
tialized to the ﬂow size divided by sender NIC rate), the
inter-probing time (I S, initialized to ø), and the measured
RTT (RT T S, estimated by an exponential decay).
The sender sends packets with rate RS. If the rate is zero,
the sender sends a probe packet every I S RTTs to get rate
information from the switches. A probe packet is a packet
with a scheduling header but no data content.
S
On packet departure, the sender attaches a scheduling
header to the packet, containing ﬁelds set based on the val-
ues of each of the sender’s state variables above. RH is
always set to the maximal sending rate Rmax
, while the re-
maining ﬁelds in the scheduling header are set to its current
maintained variables. Note that the subscript H refers to
a ﬁeld in the scheduling header; the subscript S refers to a
variable maintained by the sender; the subscript i refers to
a variable related to the ith ﬂow in the switch’s ﬂow list.
Whenever an ACK packet arrives, the sender updates its
ﬂow sending rate based on the feedback: T S is updated
based on the remaining ﬂow size, RT T S is updated based
on the packet arrival time, and the remaining variables are
set to the ﬁelds in the scheduling header.
Early Termination: For deadline-constrained ﬂows, when
the incoming ﬂow demand exceeds the network capacity,
there might not exist a feasible schedule for all ﬂows to meet
their deadlines. In this case, it is desirable to discard a min-
imal number of ﬂows while satisfying the deadline of the
remaining ﬂows. Unfortunately, minimizing the number of
tardy ﬂows in a dynamic setting is an NP-complete prob-
lem.2
Therefore, we use a simple heuristic, called Early Termi-
nation, to terminate a ﬂow when it cannot meet its deadline.
Here, the sender sends a TERM packet whenever any of the
following conditions happen:
1. Deadline is past (Time > DS).
2. The remaining ﬂow transmission time is larger than
the time to deadline (Time + T S > DS).
3. The ﬂow is paused (RS = 0), and the time to deadline
is smaller than an RTT (Time + RT T S > DS).
3.2 PDQ Receiver
A PDQ receiver copies the scheduling header from each
data packet to its corresponding ACK. Moreover, to avoid
the sender overrunning the receiver’s buﬀer, the PDQ re-
ceiver reduces RH if it exceeds the maximal rate that re-
ceiver can process and receive.
3.3 PDQ Switch
The high-level objective of a PDQ switch is to let the
most critical ﬂow complete as soon as possible. To this end,
switches share a common ﬂow comparator, which decides
2Consider a subproblem where a set of concurrent ﬂows that
share a bottleneck link all have the same deadline. This sub-
problem of minimizing the number of tardy ﬂows is exactly
the NP-complete subset sum problem [11].
129ﬂow criticality, to approximate a range of scheduling disci-
plines.
In this study, we implement two disciplines, EDF
and SJF, while we give higher priority to EDF. In particu-
lar, we say a ﬂow is more critical than another one if it has
smaller deadline (emulating EDF to minimize the number
of deadline-missing ﬂows). When there is a tie or ﬂows have
no deadline, we break it by giving priority to the ﬂow with
smaller expected transmission time (emulating SJF to mini-
mize mean ﬂow completion time). If a tie remains, we break
it by ﬂow ID. If desired, the operator could easily override
the comparator to approximate other scheduling disciplines.
For example, we also evaluate another scheduling discipline
incorporating ﬂow waiting time in §7.
The switch’s purpose is to resolve ﬂow contention: ﬂows
can preempt less critical ﬂows to achieve the highest possi-
ble sending rate. To achieve this goal, the switches maintain
state about ﬂows on each link (§3.3.1) and exchange infor-
mation by tagging the scheduling header. To compute the
rate feedback (RH ), the switch uses a ﬂow controller (con-
trolling which ﬂows to send; §3.3.2) and a rate controller
(computing the aggregate ﬂow sending rate; §3.3.3).
3.3.1 Switch State
In order to resolve ﬂow contention, the switch maintains
state about ﬂows on each link. Speciﬁcally, it remembers the
most recent variables () obtained
from observed packet headers for ﬂow i, which it uses to
decide at any moment the correct sending rate for the ﬂows.
However, we do not have to keep this state for all ﬂows.
Speciﬁcally, PDQ switches only store the most critical 2κ
ﬂows, where κ is the number of sending ﬂows (i.e., ﬂows
with sending rate RS > 0). Since PDQ allocates as much
link bandwidth as possible to the most critical ﬂows until the
link is fully utilized, the κ most critical ﬂows fully utilize
the link’s bandwidth; we store state for 2κ ﬂows in order
to have suﬃcient information immediately available to un-
pause another ﬂow if one of the sending ﬂows completes.
The remaining ﬂows are not remembered by the switch, until
they become suﬃciently critical.
The amount of state maintained at the switch thus de-
pends on how many ﬂows are needed to ﬁll up a link. In
most practical cases, this value will be very small because
(i) PDQ allows critical ﬂows to send with their highest pos-
sible rates, and (ii) switch-to-switch links are typically only
1− 10× faster than server-to-switch links, e.g., current data
center networks mostly use 1 Gbps server links and 10 Gbps
switch links3, and the next generation will likely be 10 Gbps
server links and 40 or 100 Gbps switch links. However, if
a ﬂow’s rate is limited to something less than its NIC rate
(e.g., due to processing or disk bottlenecks), switches may
need to store more ﬂows.
Greenberg et al. [12] demonstrated that, under a produc-
tion data center of a large scale cloud service, the number
of concurrent ﬂows going in and out of a machine is almost
never more than 100. Under a pessimistic scenario where ev-
ery server concurrently sends or receives 100 ﬂows, we have
an average of 12,000 active ﬂows at each switch in a VL2 net-
work (assuming ﬂow-level equal-cost multi-path forwarding
and 24 10-Gbps Ethernet ports for each switch, the same as
done in [12]). Today’s switches are typically equipped with
3For example, the NEC PF5240 switch supports 48 × 1
Gbps ports, along with 2 × 10 Gbps ports; Pronto 3290
switch provides 48 × 1 Gbps ports and 4 × 10 Gbps ports.
4 − 16 MByte of shared high-speed memory4, while storing
all these ﬂows requires 0.23 MByte, only 5.72% of a 4 MByte
shared memory. Indeed, in our simulation using the trace
from [6], the maximum memory consumption was merely 9.3
KByte.
Still, suppose our memory imposes a hard upper limit M
on the number of ﬂows the switch can store. PDQ, as de-
scribed so far, will cause under-utilization when κ > M and
there are paused ﬂows wanting to send. In this underuti-
lized case, we run an RCP [10] rate controller—which does
not require per-ﬂow state—alongside PDQ. We inform RCP
that its maximum link capacity is the amount of capacity
not used by PDQ, and we use RCP only for the less critical
ﬂows (outside the M most critical) that are not paused by
any other switches. RCP will let all these ﬂows run simul-
taneously using the leftover bandwidth. Thus, even in this
case of large κ (which we expect to be rare), the result is
simply a partial shift away from optimizing completion time
and towards traditional fair sharing.
3.3.2 The Flow Controller
The ﬂow controller performs Algorithm 1 and 3 whenever
it receives a data packet and an ACK packet, respectively.
The ﬂow controller’s objective is to accept or pause the ﬂow.
A ﬂow is accepted if all switches along the path accept it.
However, a ﬂow is paused if any switch pauses it. This
diﬀerence leads to the need for diﬀerent actions:
Pausing:
If a switch decides to pause a ﬂow, it simply up-
dates the “pauseby” ﬁeld in the header (P H ) to its ID. This
is used to inform other switches and the sender that the ﬂow
should be paused. Whenever a switch notices that a ﬂow is
paused by another switch, it removes the ﬂow information
from its state. This can help the switch to decide whether
it wants to accept other ﬂows.
Acceptance: To reach consensus across switches, ﬂow ac-
ceptance takes two phases: (i) in the forward path (from
source to destination), the switch computes the available
bandwidth based on ﬂow criticality (Algorithm 2) and up-
dates the rate and pauseby ﬁelds in the scheduling header;
(ii) in the reverse path, if a switch sees an empty pauseby
ﬁeld in the header, it updates the global decision of accep-
tance to its state (P i and Ri).
We now propose several optimizations to reﬁne our design:
Early Start: Given a set of ﬂows that are not paused by
other switches, the switch accepts ﬂows according to their
criticality until the link bandwidth is fully utilized and the
remaining ﬂows are paused. Although this ensures that the
more critical ﬂows can preempt other ﬂows to fully utilize
the link bandwidth, this can lead to low link utilization when
switching between ﬂows. To understand why, consider two
ﬂows, A and B, competing for a link’s bandwidth. Assume
that ﬂow A is more critical than ﬂow B. Therefore, ﬂow A is
accepted to occupy the entire link’s bandwidth, while ﬂow
B is paused and sends only probe packets, e.g., one per its
RTT. By the time ﬂow A sends its last packet (TERM),
the sender of ﬂow B does not know it should start sending
data because of the feedback loop delay. In fact, it could
take one to two RTTs before ﬂow B can start sending data.
4For example, the “deep-buﬀered” switches like Cisco Cata-
lyst 4500, 4700, 4900 and 4948 series have 16 MByte shared
memory, while shallow-buﬀered switches like Broadcom Tri-
umph and Scorpion have 4 MByte shared memory [3].
130Although the RTT in data center networks is typically very
small (e.g., ∼150 µs), the high-bandwidth short-ﬂow nature
makes this problem non-negligible. In the worst case where
all the ﬂows are short control messages (<10 KByte) that
could ﬁnish in just one RTT, links could be idle more than
half the time.
nearly-completed ﬂows ((cid:80)
To solve this, we propose a simple concept, called Early
Start, to provide seamless ﬂow switching. The idea is to
start the next set of ﬂows slightly before the current send-
ing ﬂows ﬁnish. Given a set of ﬂows that are not paused by
other switches, a PDQ switch classiﬁes a currently sending
ﬂow as nearly completed if the ﬂow will ﬁnish sending in
K RTTs (i.e., T i < K × RT T i), for some small constant
K. We let the switch additionally accept as many nearly-
completed ﬂows as possible according to their criticality and
subject to the resource constraint: aggregated ﬂow transmis-
sion time (in terms of its estimated RTT) of the accepted
iT i/RT T i) is no larger than K.
The threshold K determines how early and how many ﬂows
will be considered as nearly-completed. Setting K to 0 will
prevent concurrent ﬂows completely, resulting in low link
utilization. Setting K to a large number will result in con-
gested links, increased queue sizes, and increased completion
times of the most critical ﬂows. Any value of K between 1
and 2 is reasonable, as the control loop delay is one RTT and
the inter probing time is another RTT. In our current im-
plementation we set K = 2 to maximize the link utilization,
and we use the rate controller to drain the queue. Algo-
rithm 2 describes this in pseudocode, and we will show that
Early Start provides seamless ﬂow switching (§5).
Dampening: When a more critical ﬂow arrives at a switch,
PDQ will pause the current ﬂow and switch to the new ﬂow.
However, bursts of ﬂows that arrive concurrently are com-
mon in data center networks, and can potentially cause fre-
quent ﬂow switching, resulting in temporary instability in