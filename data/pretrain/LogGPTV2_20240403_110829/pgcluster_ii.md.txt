PGCluster-II
Clustering system of PostgreSQL
using Shared Data
Atsushi MITANI - PI:EMAIL
HA
First Italian PostgreSQL Day
PGDay 2007 – July 6,7 2007 – Prato, Italy
Agenda
Requirement
PGCluster
New Requirement
PGCluster-II
Structure and Process sequence
Pros & Cons
As a background
RReeqquuiirreemmeenntt
PPGGCClluusstteerr
NNeeww RReeqquuiirreemmeenntt
PPGGCClluusstteerr--IIII
SSttrruuccttuurree aanndd PPrroocceessss sseeqquueennccee
PPrrooss && CCoonnss
Original requirement
Target application
Web application
Heavy session load
High availability
with ordinary servers
No down time
High performance for data read
More than 90% sessions were data read query.
First step
RReeqquuiirreemmeenntt
PPGGCClluusstteerr
NNeeww RReeqquuiirreemmeenntt
PPGGCClluusstteerr--IIII
SSttrruuccttuurree aanndd PPrroocceessss sseeqquueennccee
PPrrooss && CCoonnss
PGCluster(2002-)
Synchronous & Multi-master Replication system
Query based replication
DB node independent data can replicate
now(),random()
No single point of failure
Multiplex load balancer, replication server and
cluster DBs.
Automatic take over
Restore should do by manually
Add cluster DB and replication server on the fly.
Version upgrade as well
Structure of PGCLuster
clusterDB
Client
Load
session
balancer
replicator
clusterDB
Load
replicator
balancer
Client
session
clusterDB
Pros & Cons of PGCluster
Enough HA
 Performance issue
Enough performance
 Very bad for data
writing load
for data reading
load Maintenance issue
Cost
 Document issue
Ordinary PC servers
BSD license SW
5 years later
RReeqquuiirreemmeenntt
PPGGCClluusstteerr
NNeeww RReeqquuiirreemmeenntt
PPGGCClluusstteerr--IIII
SSttrruuccttuurree aanndd PPrroocceessss sseeqquueennccee
PPrrooss && CCoonnss
Requirement is changed
Target application
Web application
OLTP application
HA and HP
HP is required even for data write
Service stop is not allowed
Coexistence of HA and HP
HA and HP conflict each other
HHAA required redundancy
HP required quick response
Performance point of view
Replication scales for data reading (not
writing)
Parallel query has effect in both
However it is not easy to add redundancy (HA).
Shared Data Clustering also scales for both
However, it is not suitable for large data.
Shared Disk needs redundancy.
As a solution
RReeqquuiirreemmeenntt
PPGGCClluusstteerr
NNeeww RReeqquuiirreemmeenntt
PPGGCClluusstteerr--IIII
SSttrruuccttuurree aanndd PPrroocceessss sseeqquueennccee
PPrrooss && CCoonnss
What is the PGCluster-II
Data shared clustering system
Storage data shared by shared disk
NFS,GFS,GPFS(AIX) etc.
SAN/NAS
Cache and lock status shared by Virtual IPC
Detail as following slides
Concept of Shared Data
Virtual shared IPC
Cluster Cluster Cluster
DB DB DB
Shared Disk
Inside of PGCluster-II
RReeqquuiirreemmeenntt
PPGGCClluusstteerr
NNeeww RReeqquuiirreemmeenntt
PPGGCClluusstteerr--IIII
SSttrruuccttuurree aanndd PPrroocceessss sseeqquueennccee
PPrrooss && CCoonnss
Virtual IPC
Share semaphore and shared memory during DB nodes
Write it to remote nodes through cluster
process
Read it from local node directory
Signal and message queue are out of scope
Structure of PGCluster-II
DB node DB node
rw
rw
req
pgcluster pgcluster
IPC
IPC req req
r r
postmaster postmaster
Shared disk
Semaphore
To Lock control
How many semaphores are using?
Depends on the “max-connections”setting
In default, 7 x 16 semaphores are used.
Mapping table is required
Semid = 100 to LOCK Semid = 80 to LOCK
Semid sem_num index Semid sem_num index
2/15 LOCK
99 2 14 79 2 14
100 2 15 80 2 15
Shared Memory
Communicate during each backend processes
Store data of logs, caches, buffers and so on
Single shared memory is allocated
But it is divided a number of peaces
more than 100 entry pointer are existing.
Issues of Shared Memory
Activity issue
Size is not big but update frequency is very
high
Contents issue
It is including memory/function address
If copy shared memory to other server, other DB
server may be crashed (depend on the OS).
Address Data Type Label Address Data Type Label
&1000 &1004 Char * Data &2000 &1004 Char * Data
&1004 1 OID Oid &2004 1 OID Oid
&1008 &1012 Char * Next &2008 &1012 Char * Next
&1012 &1024 Char * Data &2012 &1024 Char * Data
Solution
Mask table & localization table
It worked, but very bad performance
Data changed to offset from address
char * ptr -> Size offset
It’s still over head, but better than
before.
benchmark
pgbench 1 pgbench 2
550 600
500 550
500
450
450
400
400
350
350
300 列 H 列 B
列 I 300 列 C
250
250
200
200
150
150
100
100
50 50
0 0
行 3 行 4 行 5 行 6 行 7 行 8 行 3 行 4 行 5 行 6 行 7 行 8
As a result
RReeqquuiirreemmeenntt
PPGGCClluusstteerr
NNeeww RReeqquuiirreemmeenntt
PPGGCClluusstteerr--IIII
SSttrruuccttuurree aanndd PPrroocceessss sseeqquueennccee
PPrrooss && CCoonnss
Pros & Cons
Required large RAM.
Easy to add a node for
redundancy / replace. Writing performance is
not good yet.
Data writing
performance does not Nothing expands
slow by adding node.
Cost
Big improve to data
Shared disk system is
reading / many
expensive
connection load.
TODO
Performance should more improve.
Narrow down the target shared memory data.
It should send multi memory data at once.
Release source code
ASAP
Documentation as well
Thank you
Ask us about PGCluster
PI:EMAIL
Ask me about PGCluster-II
PI:EMAIL