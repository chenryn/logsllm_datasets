title:Modeling the Coverage and Effectiveness of Fault-Management Architectures
in Layered Distributed Systems
author:Olivia Das and
C. Murray Woodside
Modeling the Coverage and Effectiveness of Fault-Management Architectures in 
Layered Distributed Systems
Dept. of Systems and Computer Engineering, Carleton University, Ottawa, Canada
email: PI:EMAIL, PI:EMAIL
Olivia Das, C. Murray Woodside
Abstract
Increasingly, fault-tolerant distributed software applica-
tions  use  a  separate  architecture  for  failure  detection
instead  of  coding  the  mechanisms  inside  the  application
itself. Such a structure removes the intricacies of the failure
detection  mechanisms  from  the  application,  and  avoids
repeating them in every program. However, successful sys-
tem reconfiguration now depends on the management archi-
tecture 
and
reconfiguration),  and  on  management  subsystem  failures,
as  well  as  on  the  application.  This  paper  presents  an
approach  which  computes  the  architecture-based  system
reconfiguration  coverage  simultaneously  with  its  perform-
ability.
detection 
(which 
fault 
does 
both 
1.  Introduction
Fault-tolerant  computer  systems  are  designed  with
redundancy  to  mask  and  tolerate  failures.  However,  the
redundancy is ineffective if mechanisms are not in place to
detect  and  recover  from  a  fault.  [1,  2,  3].  An  accurate
dependability  analysis  must  consider  the  system  detection
and  recovery  behavior  in  addition  to  the  system  structure
and its provision of redundancy.
The  use  of  a  separate  architecture  for  failure  detection
and  reconfiguration  is  becoming  more  popular  among
fault-tolerant  distributed  applications  (instead  of  handling
the faults within the application itself) [4, 5, 6]. Such usage
promotes software reuse and also eases the development of
the  software  application.  Most  of  these  systems  are
structured  in  layers  with some  kind of  user-interface  tasks
at  the  topmost  layer,  making  requests  to  various  layers  of
servers.  Client-server  systems  and  Open  Distributed
Processing systems such as DCE, ANSA and CORBA are
also  structured  in  this  way.  [7]  introduced  an  approach  to
express layered failure and repair dependencies while [8, 9,
10]  provided  an  efficient  algorithm  for 
identifying
equivalent  system  states  from  performance  viewpoint,  in
these 
assume
instantaneous  perfect  detection  and  reconfiguration,  and
systems.  However, 
studies 
these 
independent  failures  and  repairs  (some  kinds  of  failure
dependency were modeled in [10]). It is the purpose of the
present  work 
fault  management
architecture and its failures into the analysis.
incorporate 
the 
to 
Other  work  analyzes the  effect  of  software architecture
(and not the management architecture) on reliability and is
given by Trivedi and his co-workers [11, 12].
in 
This 
fault 
layered 
investigates 
coverage 
systems  with  a 
(and
paper 
fault-
performability) 
management architecture, extending the work in [8, 10]. In
reliability modeling, the usual approach to model coverage
is  to  have  three  states  {not  failed,  failed  covered  (which
implies  that  the  system  has  automatically  detected  and
recovered  from  the  fault),  failed  not  covered  (which
implies  that  the  global  system  failure  has  occurred  due  to
the  fault  regardless  of  the  state  of  the  system)}  for  each
component  and  then  combine  the  aspects  of  behavioral
decomposition,  sum-of-disjoint  products  and  multi-state
solution methods [2, 3]. However, in layered systems, there
may  be  multiple  reconfiguration  points  that  must  be
activated  in  order  to  tolerate  a  single  failure.  Success  at  a
reconfiguration  point  depends  on  the  system  structure  and
the connectivity between the source of failure and the point
of reconfiguration in the fault management architecture. In
fact, a failure may be fully, partially or not covered at all by
the  system,  depending  on  how  many  of  the  necessary
reconfigurations  are  successful.  Different  degrees  of
coverage 
in  different  operational
configurations  of  the  system.  When  a  failure  is  partially
covered, the system may end up failed or may operate with
degraded performance, compared to the fully covered case. 
typically 
result 
This  work  captures  the  effect  of  partial  coverage  of  a
failure  which  is  necessary  for  full  performability  analysis
of  the  system.  This  work  considers  detecting  and  reacting
only  to  crash-stop  failures,  in  which  an  entity  becomes
inactive  after  failure,  and  not  to  other  complex  failure
modes  such  as  Byzantine  failures  [13].  The  solution
strategy  for  obtaining  the  expected  reward  rate  of  the
system  in  this  paper  involves  state-space  enumeration  and
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
combines  min-paths  generation  algorithms,  AND-OR
graph analysis with the Layered Queueing Analysis [14]. 
The rest of the paper is organized as follows: Section 2
describes  the  layered  systems  and  their  fault  management
architectures. Section 3 describes the failure propagation in
layered systems and Section 4 describes the propagation of
knowledge  about  a  failure  or  a  repair  event  in  a  fault
management 
the
performability  computation  algorithm  and  Section  6
compares  the  effect  of  coverage  of  four  different  fault
management  architectures  on  the  expected  steady-state
reward rate of a system.
architecture.  Section  5  presents 
2.  Layered Systems with a Detection/Reconfig- 
uration Architecture
The class of systems analyzed in this work has a layered
or  tiered  architecture  for  the  applications.  Figure  1
illustrates  the class  with an example, using a notation that
was  also  used  in  [8,  9,  10].  There  is  a  set  of  users,  which
may  be  people  at  terminals  or  at  PC  workstations,
accessing  applications,  which  in  turn  access  back-end
servers.  The  rectangles  in  the  figure  represent  tasks  (i.e.
operating system processes) such as UserA, AppA, Server1
with  entries,  which  are  service  handlers  embedded  in  the
tasks (there may be several entries such as eA-1, eB-1 in a
task). The arrows designate service requests from one entry
to another, with an implied reply. Tasks block to receive the
reply, as in standard remote procedure calls. We restrict the
analysis  to  models  with  no  cycles  of  requests,  as  cycles
may lead to deadlock.
NUserA = 50
userA UserA
UserB
userB
NUserB = 100
procA
proc1
proc3
procB
eA
AppA
serviceA
#1
#2
#1
eB
AppB
serviceB
#2
proc2
eA-1
eB-1
Server1
eA-2
eB-2
Server2
proc4
Figure 1. A layered model of a client-server system with 
two groups of users. Server2 is the backup of Server1.
applications  specific  to  the  department  (tasks  AppA  and
AppB), which in turn access enterprise data servers Server1
and  Server2.  Server1  is  the  primary  server  to  both
departments; if it fails they will use Server2 until Server1 is
working again.
A. Reconfiguration
The  alternative  targeting  of  requests  is  indicated  in
Figure  1  by showing an abstraction  called  “serviceA”  and
“serviceB”  for  the  data  access  service  required  by  the
Applications.  This  service  has  alternative  request  arrows
attached  to it, with labels “#n”  showing the priority of the
target.  A  request  goes  to  the  highest-priority  available
server,  which  is determined  by  a  reconfiguration decision.
In  [8,  10]  the  decision  was  assumed  to  be  made  by  the
Application,  based  on  perfect  information.  Here  the
decision will be  made by the  management  subsystem, and
will  be  conditioned  by  its  knowledge  of  the  status  of
system  components.  It  can  respond  not  only  to  processor
failures,  but  also  to  software  failures  (task  crashes  or
operating  system  crashes).  Network  components  can  be
included in the model as well, but for simplicity we will not
consider that here, so network connections will be assumed
not to fail.
The special property of layered systems is that a failure
of a server or processor in one layer can cause many tasks
that  depend  on  its  services  (at  any  layer  in  the  system)  to
fail,  unless  they  have  an  alternative.  In  general,  there  can
be  any  number  of  layers  in  the  system,  and  network
components can be included. The notation in Figure 1 was
introduced  in  [8]  as  “Fault  Tolerant  Layered  Queueing
Networks”  (FTLQNs)  and  is  based  on  layered  queueing
networks  (LQNs)  [14].  Non-blocking  and  multi-threaded
tasks, and asynchronous interactions, can be included. The
model 
failure
operational 
dependencies,  and  [10]  showed  how 
this  could  be
generalized  to  abstract  “failure  dependency  factors”  that
model  some  forms  of  dependency  among  individual
failures.
captures 
layered 
and 
for  each 
reachable  configuration 
The  general  strategy  of  the  analysis  is  to  compute  the
performance 
(with
different  choices  of  alternative  targets  for  requests)  and
combine  it  with  the  probability  of  each  configuration
occurring,  to  find  performability  measures.  This  is  similar
to the Dynamic Queueing Network approach given in [15,
16].
In  Figure  1,  there  are  50  UserA  users  who  could  be
working  on  one  department  of  an  enterprise,  and  100
UserB  users  in  another  department.  Each  group  makes
to
primary  access 
through  a  departmental 
server 
B. Management Components
The  management  components  and  relationships  are
indicated  in  Figure  2,  following  [17].  Applications  have
embedded  modules  (Subagents)  which  may  be  configured
to  send  heartbeat  messages  in  response  to  timer  interrupts
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
(indicating they are alive) to a local Agent, or to a manager
directly.  A  node  may  have  an  Agent  task  which  monitors
the  operating  system  health  status and  all the  processes in
the  node,  and  there  may  be  one  or  more  Manager  tasks
which  collect  status 
information  from  agents,  make
decisions, 
reconfigure.
Reconfiguration  can  be  handled  by  a subagent  (to  cause a
task  or  an  ORB  to  retarget  its  requests)  or  an  agent  (to
restart a task, or reboot a node altogether). 
issue  notifications 
and 
to 
The agents and managers are described in this paper as
if they are free-standing processes, even though in practice
some  of  these  components  may  be  combined  with  other
components  in  a  dependability  ORB  [18,  19],  or  an
application management system [20].
Manager
Agent
AppA
Agent
Subagent Server1
Agent
Server2
Figure 2. Management components and relationships
Failures  of  system  entities  are  detected  by  mechanisms
such as heartbeats, timeouts on periodic polls, and timeouts
on requests between application tasks. Heartbeat messages
from  an  application  task  can  be  generated  by  a  special
heartbeat  interrupt  service  routine  which  sends  a  message
to  a  local  agent  or  to  a  manager,  every  time  an  interrupt
occurs,  as  long  as  the  task  has  not  crashed.  Heartbeat
messages  for an entire  node can  be  generated by  an  agent
configured similarly, to show that the  node is functioning;
the  agent  could  query  the  operating  system  health  status
before  sending  its  message.  Heartbeat  information  once
collected  can  be  propagated  among  the  agents  and
managers  to  act  as  a  basis  for  decisions,  made  by
reconfiguration modules.
An entity that cannot initiate heartbeat messages may be
able to respond to messages from an agent or manager; we
can  think  of  these  as  status  polls.  The  responses  give  the
same  information  as  heartbeat  messages.  Polls  to  a  node
could be implemented as pings, for instance.
C. Management Architecture
The  architecture  model  described  here  will  be  called
MAMA, Model for Availability Management Architectures.
The model has four types of components: application tasks
(which  may  include  subagent  modules),  agent  tasks,
manager tasks, and the processors they all run on (network
failures  are  for  the  time  being  ignored).  There  are  three
types  of  connectors:  alive-watch,  status-watch  and  notify.
These  connectors  are  typed  according  to  the  information
they  convey,  in  a  way  which  supports  the  analysis  of
knowledge  of  the  system  status  at  different  points  in  the
management system. 
(cid:127)
Components  have  ports  which  are  attached 
to
connectors in certain roles. The roles are defined as part of
the connector type. The connector types and the roles they
support are:
(cid:127)
Alive-watch connectors, with roles monitor and moni-
tored. They only convey data to detect crash failure of 
the component in the monitored role, to the component 
in the monitor role. A typical example is a connector to 
a single heartbeat source.
Status-watch connectors, also with roles monitor and 
monitored. They may convey the same data about the 
monitored component, but also propagate data about 
the status of other components to the component in the 
monitor role. A typical example is a connector to a 
node agent, conveying full information on the node sta-
tus, including its own status.
Notify connectors, with roles subscriber and notifier. 
The component in the notifier role propagates status 
data that it has received to a component in a subscriber 
role, however it does not include data on its own status.
Manager and Agent tasks can be connected in any role;
an  Application  task  can  be  connected  in  the  roles
monitored,  or  subscriber.  A  Processor  is  a  composite
component  that  contains  a  cluster  of  tasks  that  execute
there.  If  the processor  fails, all its  enclosed  tasks fail.  The
Processor can only be connected in the monitored role to an
alive-watch  connector  (which  might  convey  a  ping,  for
example).
(cid:127)
Upon  occurrence  of  a  failure  or  repair  of  a  task  or  a
processor,  the  occurrence  is  first  captured  via  alive-watch
or status-watch connections and the information propagates
through  status-watch  and  notify  connections,  to  managers
which  initiate  system  reconfiguration.  Reconfiguration
commands  are  sent  by  notify  connections.  Cycles  may
occur  in  the  architecture;  we  assume  that  the  information
flow  is managed so  as to not cycle. In  this  work, we  note
that  if  a  task  watches  a  remote  task,  then  it  also  has  to
watch  the  processor  executing the remote task, in  order to
distinguish  between  the  processor  failure  and  the  task
failure.
Figure 3 shows a graphical notation for various types of
components,  ports,  connectors  and  roles  based  on  the
customized  UML  notation  for  conceptual  architecture  as
defined in [21]. The component types and connector types
will  be  shown  as  classes  in  this  work.  In  order  to  avoid
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
cluttering in the MAMA diagrams, the role names such as
monitor,  monitored,  notifier  and  subscriber  have  been
omitted from them.  
role
port
AT
Application Task 
Component
AGT