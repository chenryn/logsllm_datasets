# Modeling the Coverage and Effectiveness of Fault-Management Architectures in Layered Distributed Systems

**Authors:**
- Olivia Das
- C. Murray Woodside

**Affiliation:**
Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada

**Emails:**
- PI:EMAIL
- PI:EMAIL

## Abstract
Increasingly, fault-tolerant distributed software applications are adopting a separate architecture for failure detection rather than embedding these mechanisms within the application itself. This separation removes the complexity of failure detection from the application and avoids redundancy in every program. However, successful system reconfiguration now depends on the management architecture (which handles both detection and reconfiguration) and the robustness of the management subsystem, as well as the application. This paper presents an approach to compute the architecture-based system reconfiguration coverage and its performability simultaneously.

## 1. Introduction
Fault-tolerant computer systems are designed with redundancy to mask and tolerate failures. However, this redundancy is ineffective without mechanisms to detect and recover from faults [1, 2, 3]. An accurate dependability analysis must consider the system's detection and recovery behavior, in addition to its structure and redundancy.

The use of a separate architecture for failure detection and reconfiguration is gaining popularity among fault-tolerant distributed applications [4, 5, 6]. This approach promotes software reuse and simplifies development. Most of these systems are structured in layers, with user-interface tasks at the topmost layer, making requests to various server layers. Client-server systems and Open Distributed Processing (ODP) systems such as DCE, ANSA, and CORBA follow this structure. [7] introduced a method to express layered failure and repair dependencies, while [8, 9, 10] provided an efficient algorithm for identifying equivalent system states from a performance perspective. These studies, however, assume instantaneous perfect detection and reconfiguration, and independent failures and repairs (with some failure dependencies modeled in [10]).

The present work aims to incorporate the fault management architecture and its failures into the analysis. Other research, such as that by Trivedi and his co-workers [11, 12], focuses on the effect of software architecture (not the management architecture) on reliability.

This paper investigates the coverage and performability of fault management architectures in layered systems, extending the work in [8, 10]. In reliability modeling, the standard approach to model coverage involves three states for each component: not failed, failed covered (the system has automatically detected and recovered from the fault), and failed not covered (a global system failure has occurred due to the fault). However, in layered systems, multiple reconfiguration points may need to be activated to tolerate a single failure. The success of reconfiguration depends on the system structure and the connectivity between the failure source and the reconfiguration point in the fault management architecture. A failure may be fully, partially, or not covered at all, depending on the success of the necessary reconfigurations. Different degrees of coverage can result in different operational configurations. Partial coverage can lead to degraded system performance compared to full coverage.

This work captures the effect of partial failure coverage, which is essential for a complete performability analysis. It considers only crash-stop failures, where an entity becomes inactive after failure, and not more complex failure modes such as Byzantine failures [13]. The solution strategy for obtaining the expected reward rate involves state-space enumeration and combines min-paths generation algorithms, AND-OR graph analysis, and Layered Queueing Analysis [14].

The rest of the paper is organized as follows: Section 2 describes layered systems and their fault management architectures. Section 3 discusses failure propagation in layered systems, and Section 4 covers the propagation of knowledge about failure or repair events in a fault management architecture. Section 5 presents the performability computation algorithm, and Section 6 compares the coverage of four different fault management architectures on the expected steady-state reward rate of a system.

## 2. Layered Systems with a Detection/Reconfiguration Architecture
The systems analyzed in this work have a layered or tiered architecture for applications. Figure 1 illustrates this class with an example using a notation also used in [8, 9, 10]. There is a set of users, who may be people at terminals or PC workstations, accessing applications that in turn access back-end servers. The rectangles in the figure represent tasks (i.e., operating system processes) such as UserA, AppA, Server1, with entries that are service handlers embedded in the tasks (e.g., eA-1, eB-1). Arrows designate service requests from one entry to another, with implied replies. Tasks block to receive the reply, similar to standard remote procedure calls. We restrict the analysis to models with no cycles of requests to avoid deadlock.

### 2.1 Reconfiguration
Figure 1 shows an abstraction called "serviceA" and "serviceB" for the data access services required by the applications. Service requests go to the highest-priority available server, determined by a reconfiguration decision. In [8, 10], this decision was assumed to be made by the application based on perfect information. Here, the decision is made by the management subsystem, conditioned by its knowledge of the status of system components. It can respond to processor and software failures (task crashes or operating system crashes). Network components can be included in the model, but for simplicity, we assume network connections do not fail.

In layered systems, a failure in one layer can cause many dependent tasks in other layers to fail unless they have alternatives. The notation in Figure 1, introduced in [8] as "Fault Tolerant Layered Queueing Networks" (FTLQNs), is based on layered queueing networks (LQNs) [14]. Non-blocking and multi-threaded tasks, and asynchronous interactions, can be included. The model captures layered and operational dependencies, and [10] showed how this could be generalized to abstract "failure dependency factors" that model some forms of dependency among individual failures.

The general strategy of the analysis is to compute the performance (with different choices of alternative targets for requests) and combine it with the probability of each configuration occurring to find performability measures. This is similar to the Dynamic Queueing Network approach given in [15, 16].

In Figure 1, there are 50 UserA users and 100 UserB users, each group making primary access through a departmental server.

### 2.2 Management Components
Figure 2 shows the management components and relationships, following [17]. Applications have embedded modules (Subagents) that send heartbeat messages in response to timer interrupts, indicating they are alive, to a local Agent or a Manager. A node may have an Agent task that monitors the operating system health status and all processes in the node. There may be one or more Manager tasks that collect status information from agents, make decisions, and reconfigure. Reconfiguration can be handled by a Subagent (to retarget requests) or an Agent (to restart a task or reboot a node).

Agents and Managers are described as free-standing processes, though in practice, some components may be combined with others in a dependability ORB [18, 19] or an application management system [20].

### 2.3 Management Architecture
The architecture model described here is called MAMA (Model for Availability Management Architectures). It has four types of components: application tasks (which may include subagent modules), agent tasks, manager tasks, and processors (network failures are ignored for now). There are three types of connectors: alive-watch, status-watch, and notify. These connectors convey different types of information to support the analysis of system status knowledge at different points in the management system.

- **Components** have ports attached to connectors in specific roles.
- **Alive-watch connectors** have roles monitor and monitored, conveying data to detect crash failures.
- **Status-watch connectors** also have roles monitor and monitored, conveying data about the monitored component and propagating data about other components.
- **Notify connectors** have roles subscriber and notifier, propagating status data without including the notifier's own status.

Manager and Agent tasks can be connected in any role; Application tasks can be connected in the roles monitored or subscriber. A Processor is a composite component containing a cluster of tasks. If the processor fails, all enclosed tasks fail. The Processor can only be connected in the monitored role to an alive-watch connector.

Upon a failure or repair, the occurrence is captured via alive-watch or status-watch connections, and the information propagates through status-watch and notify connections to managers, which initiate reconfiguration. Reconfiguration commands are sent via notify connections. Information flow is managed to avoid cycles. If a task watches a remote task, it must also watch the processor executing the remote task to distinguish between processor and task failures.

Figure 3 shows a graphical notation for various types of components, ports, connectors, and roles based on a customized UML notation for conceptual architecture [21]. To avoid clutter, role names are omitted from MAMA diagrams.