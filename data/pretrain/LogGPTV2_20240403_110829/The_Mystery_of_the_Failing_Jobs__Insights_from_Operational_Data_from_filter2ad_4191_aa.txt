title:The Mystery of the Failing Jobs: Insights from Operational Data from
Two University-Wide Computing Systems
author:Rakesh Kumar and
Saurabh Jha and
Ashraf Mahgoub and
Rajesh Kalyanam and
Stephen Lien Harrell and
Xiaohui Carol Song and
Zbigniew Kalbarczyk and
William T. Kramer and
Ravishankar K. Iyer and
Saurabh Bagchi
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
The Mystery of the Failing Jobs: Insights from
Operational Data from Two University-Wide
Computing Systems
Rakesh Kumar1,2, Saurabh Jha3, Ashraf Mahgoub2, Rajesh Kalyanam2, Stephen L Harrell2, Xiaohui Carol Song2,
Zbigniew Kalbarczyk3, William T Kramer3, Ravishankar K Iyer3, Saurabh Bagchi2
1: Microsoft, PI:EMAIL; 2: Purdue University, {amahgoub, rkalyana, slh, cxsong, sbagchi}@purdue.edu;
3: University of Illinois at Urbana-Champaign, {sjha8, kalbarcz, wtkramer, rkiyer}@illinois.edu
Abstract—Node downtime and failed jobs in a computing
cluster translate into wasted resources and user dissatisfaction.
Therefore understanding why nodes and jobs fail in HPC clusters
is essential. This paper provides analyses of node and job
failures in two university-wide computing clusters at two Tier
I US research universities. We analyzed approximately 3.0M
job execution data of System A and 2.2M of System B with
data sources coming from accounting logs, resource usage for
all primary local and remote resources (memory, IO, network),
and node failure data. We observe different kinds of correlations
of failures with resource usages and propose a job failure
prediction model to trigger event-driven checkpointing and avoid
wasted work. Additionally, we present user history based resource
usage and runtime prediction models. These models have the
potential to avoid system related issues such as contention, and
improve quality of service such as lower mean queue time, if
their predictions are used to make a more informed scheduling
decision. As a proof of concept, we simulate an easy backﬁll
scheduler to use predictions of one of these models, i.e., runtime
and show the improvements in terms of lower mean queue
time. Arising out of these observations, we provide generalizable
insights for cluster management to improve reliability, such as,
for some execution environments local contention dominates,
while for others system-wide contention dominates.
Index Terms—HPC, Production failure data, Data analytics,
Compute clusters
I. INTRODUCTION
“THE PHOENIX MUST BURN TO EMERGE.”
Janet Fitch
Large-scale high performance computing (HPC) systems
have become common in academic, industrial, and govern-
ment for compute-intensive applications, including large-scale
parallel applications. These HPC systems solve problems that
would take millennia on personal computers, but managing
such large shared resources can be challenging and requires
administrators to balance requirements from a diverse set
of users. Large, focused organizations can afford to buy
centralized resources, and choose to manage and operate
it at academic organizations through a central IT organi-
zation. These are funded by federal funding agencies (like
the National Science Foundation in the US) and individual
researchers write grant proposals to get access to compute time
TABLE I: Summary of data analyzed (all production jobs) for the
two university-wide clusters. The percentages in parentheses refer
to the raw counts and node seconds. Sharing allows multiple jobs
to run on the same node.
Computing Cluster
Duration
# jobs
System A
Mar 2015-Jun 2017
2,908k
System B
Feb-June 2017
2,219k
shared
non-shared
# single
# multi
total
# single
# multi
total
# unique users
1,125k (38.7%, 15.8%)
28k (1.0%, 1.9%)
1,153k (39.7%, 17.7%)
1,348k (46.3%, 18.4%)
407k (14.0%, 63.9%)
1,755k (60.3%, 82.3%)
617
-
-
-
1,640k (73.9%, 5.4%)
580k (26.1%, 94.6 %)
2,219k (100%)
467
on these systems. Examples of such systems include Comet
at the University of California San Diego, Blue Waters at the
University of Illinois at Urbana-Champaign, and Frontera at
the University of Texas at Austin.
Another trend in many universities’ IT acquisition is the
adoption of the community cluster model. Here, research
groups buy assets (nodes and other hardware) in a central
computing cluster, which is then assembled and managed by
the central IT organization. These clusters have ﬂexible usage
policies, such that partners in a community cluster have ready
access to the capacity they purchase, but they can use more
resources when other groups’ nodes are unused. This allows
for opportunistic use for the end users and higher resource
utilization for the cluster managers. System administrators take
care of security patches, software installation and upgrades,
and hardware repair, as well as space and cooling require-
ments. The community cluster model has become a foundation
of the research cyber-infrastructure at many universities. For
example, Purdue has run such a program since 2006 with 10
generations of clusters to date and in 2018 they provided 431M
CPU hours. This model is also being successfully used at the
Universities of Rochester, Delaware, and Texas at Austin.
This paper studies the reliability of jobs that run on two
clusters that follow the two operational models introduced
above. Our analyses are based on two centrally managed
computing clusters called System A and System B, at Purdue
University and University of Illinois at Urbana-Champaign
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00034
158
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Key observations and recommendations for the two university-wide computing clusters, System A and System B. Wherever
possible, we separate the recommendation for cluster provider (P) and user (U).
Observations
O1: Although System A has a local IO ca-
pacity of 100MB/s, local IO-related failure
rate starts rising with utilization as low as 3-
6MB/s. Similarly, the remote IO capacity of
System B is 1.1TB/s while remote IO-related
failures are observed with a utilization of only
46MB/s for a given job.[Fig 4, 5]
O2: Similar to prior studies, a linearly in-
creasing relationship is observed between job
failure rates and job runtime durations in
System B. However, System A shows the exact
opposite trend. [Fig 6 ]
O3: A signiﬁcant portion of jobs fail with out-
of-memory exceptions, even in cases where
free memory of more than 10GB is available
on a node (System A). Moreover, the memory
utilization is less than 65% of the available
capacity in 50% of the failed jobs [Sec V-A]
O4: Although System B is 43X larger in
scale compared to System A, system issues are
responsible for over 53% of the failed jobs in
System A, while it is only 4% for System B.
Moreover, sharing a node does not increase
the fraction of jobs failing due to system is-
sues. Additionally, both System A and System
B have signiﬁcant user-related failures (33%
and 48% of all failures). [Table IV]
O5: Contention for remote resources with ex-
ecuting elements outside the node is dominant
in non-shared environment, while the con-
tention with other jobs executing on the same
node is dominant for a shared environment.
[Sec V-B, V-C, V-D]
O6: A signiﬁcant fraction of total compute
resources are used by jobs that hit against the
walltime for both System A (33%) and System
B (43%). [Table IV]
Recommendations
R1: (P) With in-depth analysis, we notice that the majority of the jobs are sending random
access requests instead of sequential. Expectedly, we do observe the local IO threshold in
case of shared jobs (3 MB/s) to be less when compared with non-shared jobs (6 MB/s)
for System A. Accordingly, both local storage and network ﬁle system reaches saturation
with much less utilization than expected. Online monitoring of IO utilization is required to
identify contention thresholds and hence take proactive remedy decisions.
R2: (P) Our analysis shows that the majority of failed jobs in System A are due to newly
submitted jobs that fail due to startup issues (like making huge system resources demand).
On the other hand, users of System B submit codes that are more mature and the long-
running jobs are exposed to more faults in space, time or both. For System B, we recommend
using job runtime duration as a feature while predicting job failure probabilities, and hence
identify optimal checkpointing frequencies (as we propose in section VI-A) (U) For System
A, system admins can encourage new users to test their jobs on test environments and
overcome any startup issues before running on the full-scale cluster.
R3: (P) (i) Use user history based memory usage prediction model (Sec. V-A) while making
scheduling decisions for the shared jobs. (ii) Monitor and start taking preemptive measures
when the application gets close to the memory capacity [43].
R4: (P) (i) This analysis measures quantitatively how beneﬁcial it is to use resiliency
features in HPC (such as System B) and dedicated system administrators for the particular
cluster. (ii) Monitor resource usages of the jobs and take proactive actions when resource
exhaustion is being approached such as in [50], [61]. (iii) Use failure prediction model like
ours (Sec. VI-A) to dynamically change checkpointing frequency with failure probability to
complement current optimal periodic checkpointing techniques [15], [47]. (U) User-related
job failures: (i) Use static analysis tools for checking errors in job submission scripts,
environment setup, and user code [9], [53], [81]. (ii) Use small scale testing e.g., using
containers before submitting to a large cluster.
R5: (P) (i) Use user-based resource usage prediction while making scheduling decisions
(Sec V-C,V-D). (ii) Adopt resource isolation technologies (such as containers) for shared
environment, to reduce failures due to local contention. (U) (i) Use dynamic reconﬁguration
of applications based on current resource availability [50], such as reconﬁguring the number
of threads or network timeout.
R6: Signiﬁcant loss of work can happen when the program is terminated upon hitting
walltime. (P) (i) Like System B, provide extra cycles to enable an application to take a
checkpoint when job termination is signaled due to walltime. (ii) For long-running jobs,
perform checkpointing with dynamically varying frequency, say using our failure prediction
model (Sec. VI-A).
respectively. The details of the source data are provided in
Table I. For System A, which comprises 4,640 cores and 1,160
Xeon Phi acceleators, we consider a total of 3.0M jobs over
a period of 28 months (March 2015–June 2017). We have
released the entire data used in the analyses in this paper
into an open source repository as part of an ongoing NSF
project [10]. For System B, which consists of 396,000 CPU
cores and 4,229 GPU accelerators, we analyze about 2.2M
jobs over a period of 5 months (February–June 2017) of which
approximately 26% are multi-node jobs, including some which
are very large in execution scale reaching up to 358,400 cores.
A subset of the data for System B is available at [38]. This
dataset represents the most comprehensive one in terms of
variety of data sources analyzed publicly to date for failure
characteristics, whether of a university compute infrastructure
or otherwise. The paper performs 3 different categories of
analysis—examination of failure root causes through job exit
status codes, likelihood of failure with resource usage for both
local (memory, local IO) and remote (remote IO, network)
resources, and effect of job runtime on failures. We use
these analyses to drive two actionable decisions—changing the
checkpoint frequency and scheduling jobs through backﬁlling.
New insights and old insights in new environments. The
analyses in this paper shed new light relative to prior studies
of system usage and failures in large-scale computing clusters
in the following ways. We present
the key
observations and the implications for administering central
compute clusters with general-purpose needs. First, our paper
in Table II,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
159
looks at two acquisition and operation models for research
computing clusters at two large universities. The heterogeneity
of jobs and the expertise level of the users together with the
relatively smaller size of the IT system administration staff
for maintaining such clusters have important implications for
job reliability. For example, the continuous node reachability
in this environment is lower than reported in prior studies
of focused, dedicated computing clusters, such as, US De-
partment of Energy-run supercomputing clusters [11], [67] or
highly instrumented and highly managed cloud clusters [64],
[78]. Second, we categorize jobs into 5 different categories
based on their exit codes. A number of prior works have
done similar categorization into 3 categories [8], [21]. We go
one step further and split failed jobs category into 3 further
categories, i.e., user-related, system-related or user/system-
related (or indeterminate) failures (Sec. IV). This is important
because the mitigations are likely to be different for these
categories. Third, we consider ﬁne-grained system usage data
for the different resources, local to a node as well as remote,
and identify their implication for job failures. In some cases,
we see increased job failure rates due to local contention
(such as, for memory on a node) while in some cases we
see the effect of congestion for remote resources (such as, for
networking bandwidth and parallel ﬁle system for non-shared
jobs in System A). For some cases, there is no correlation found
(such as, memory-related failures in multi-node jobs). Taken
in totality, our analyses indicate which job categories (single-
node vs multi-node, shared vs non-shared) put contention
on which kinds of resources, and correspondingly at what
quantitative level, to the point of increasing job failure rates.
This can directly feed back into the acquisition and upgrade
decisions made by IT staff. More coarse-grained data and anal-
ysis, such as, aggregate job failure rate [67], coarse-grained
resource utilization metrics [12], [64], or the effect simply of
the execution time of the job on its failure probability [14],
[19], cannot shed such detailed light. Additionally, we use
user historical usage information to predict resource usages
of jobs currently in queue,
i.e., even before a job starts
executing. We show how the prediction helps a backﬁlling-
based scheduler to improve cluster utilization. Finally, we
build a failure prediction model based on resource usages,
which triggers checkpointing when the likelihood of failure is
high.
The paper is structured as follows. Section II provides
details of the two systems while Section III describes the data
sources. In Section IV, we analyze job failure categories and
in Section V, we analyze the impact on job failures of resource
usages, and present resource usage prediction models. Finally,
Section VI shows the applications of failure prediction and
runtime prediction models. We then discuss threats to validity,
related work and conclude the paper.
II. SYSTEM DETAILS
Table III provides system speciﬁcation of two university-
based HPC systems. System A is hosted at Purdue University
TABLE III: System Details
Unit
Compute
nodes,
System A
580