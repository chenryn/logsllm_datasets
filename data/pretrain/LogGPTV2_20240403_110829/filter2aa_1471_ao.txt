ffffa483fd824960   1825dd19e8 [11/30/2020 20:50:16.326] 
(Interrupt) [None] NWF (1828d80a68
[11/30/2020 20:50:21.326])
ffffa48410c07eb8   1825e2d9c6 [11/30/2020 20:50:16.364] 
(Interrupt) [None] NW P (27ef6380)
ffffa483f75bde38   1825e6f8c4 [11/30/2020 20:50:16.391] 
(Interrupt) [None] NW P (27ef6380)
ffffa48407108e60   1825ec5ae8 [11/30/2020 20:50:16.426] 
(Interrupt) [None] NWF (1828e74b68
[11/30/2020 20:50:21.426])
ffffa483f7a194a0   1825fe1d10 [11/30/2020 20:50:16.543] 
(Interrupt) [None] NWF (18272f4a10
[11/30/2020 20:50:18.543])
ffffa483fd29a8f8   18261691e3 [11/30/2020 20:50:16.703] 
(Interrupt) [None] NW P (11e1a300)
ffffa483ffcc2660   18261707d3 [11/30/2020 20:50:16.706] 
(Interrupt) [None] NWF (18265bd903
[11/30/2020 20:50:17.157])
ffffa483f7a19e30   182619f439 [11/30/2020 20:50:16.725] 
(Interrupt) [None] NWF (182914e4b9
[11/30/2020 20:50:21.725])
ffffa483ff9cfe48   182745de01 [11/30/2020 20:50:18.691] 
(Interrupt) [None] NW P (11e1a300)
ffffa483f3cfe740   18276567a9 [11/30/2020 20:50:18.897] 
(Interrupt)
               Wdf01000!FxTimer::_FxTimerExtCallbackThunk 
(Context @ ffffa483f3db7360) NWF
                                       (1827fdfe29 
[11/30/2020 20:50:19.897]) P (02faf080)
ffffa48404c02938   18276c5890 [11/30/2020 20:50:18.943] 
(Interrupt) [None] NW P (27ef6380)
ffffa483fde8e300   1827a0f6b5 [11/30/2020 20:50:19.288] 
(Interrupt) [None] NWF (183091c835
[11/30/2020 20:50:34.288])
ffffa483fde88580   1827d4fcb5 [11/30/2020 20:50:19.628] 
(Interrupt) [None] NWF (18290629b5
[11/30/2020 20:50:21.628])
In this example, you can mostly see No-wake (NW) enhanced
timers, with their minimum due time shown. Some are periodic (P)
and will keep being reinserted at expiration time. A few also have a
maximum due time, meaning that they have a tolerance specified,
showing you the latest time at which they might expire. Finally,
one enhanced timer has a callback associated with it, owned by the
Windows Driver Foundation (WDF) framework (see Chapter 6 of
Part 1 for more information on WDF drivers).
System worker threads
During system initialization, Windows creates several threads in the System
process, called system worker threads, which exist solely to perform work on
behalf of other threads. In many cases, threads executing at DPC/dispatch
level need to execute functions that can be performed only at a lower IRQL.
For example, a DPC routine, which executes in an arbitrary thread context
(because DPC execution can usurp any thread in the system) at DPC/dispatch
level IRQL, might need to access paged pool or wait for a dispatcher object
used to synchronize execution with an application thread. Because a DPC
routine can’t lower the IRQL, it must pass such processing to a thread that
executes at an IRQL below DPC/dispatch level.
Some device drivers and executive components create their own threads
dedicated to processing work at passive level; however, most use system
worker threads instead, which avoids the unnecessary scheduling and
memory overhead associated with having additional threads in the system.
An executive component requests a system worker thread’s services by
calling the executive functions ExQueueWorkItem or IoQueueWorkItem.
Device drivers should use only the latter (because this associates the work
item with a Device object, allowing for greater accountability and the
handling of scenarios in which a driver unloads while its work item is active).
These functions place a work item on a queue dispatcher object where the
threads look for work. (Queue dispatcher objects are described in more detail
in the section “I/O completion ports” in Chapter 6 in Part 1.)
The IoQueueWorkItemEx, IoSizeofWorkItem, IoInitializeWorkItem, and
IoUninitializeWorkItem APIs act similarly, but they create an association
with a driver’s Driver object or one of its Device objects.
Work items include a pointer to a routine and a parameter that the thread
passes to the routine when it processes the work item. The device driver or
executive component that requires passive-level execution implements the
routine. For example, a DPC routine that must wait for a dispatcher object
can initialize a work item that points to the routine in the driver that waits for
the dispatcher object. At some stage, a system worker thread will remove the
work item from its queue and execute the driver’s routine. When the driver’s
routine finishes, the system worker thread checks to see whether there are
more work items to process. If there aren’t any more, the system worker
thread blocks until a work item is placed on the queue. The DPC routine
might or might not have finished executing when the system worker thread
processes its work item.
There are many types of system worker threads:
■    Normal worker threads execute at priority 8 but otherwise behave like
delayed worker threads.
■    Background worker threads execute at priority 7 and inherit the same
behaviors as normal worker threads.
■    Delayed worker threads execute at priority 12 and process work items
that aren’t considered time-critical.
■    Critical worker threads execute at priority 13 and are meant to
process time-critical work items.
■    Super-critical worker threads execute at priority 14, otherwise
mirroring their critical counterparts.
■    Hyper-critical worker threads execute at priority 15 and are otherwise
just like other critical threads.
■    Real-time worker threads execute at priority 18, which gives them the
distinction of operating in the real-time scheduling range (see Chapter
4 of Part 1 for more information), meaning they are not subject to
priority boosting nor regular time slicing.
Because the naming of all of these worker queues started becoming
confusing, recent versions of Windows introduced custom priority worker
threads, which are now recommended for all driver developers and allow the
driver to pass in their own priority level.
A special kernel function, ExpLegacyWorkerInitialization, which is called
early in the boot process, appears to set an initial number of delayed and
critical worker queue threads, configurable through optional registry
parameters. You may even have seen these details in an earlier edition of this
book. Note, however, that these variables are there only for compatibility
with external instrumentation tools and are not actually utilized by any part
of the kernel on modern Windows 10 systems and later. This is because
recent kernels implemented a new kernel dispatcher object, the priority
queue (KPRIQUEUE), coupled it with a fully dynamic number of kernel
worker threads, and further split what used to be a single queue of worker
threads into per-NUMA node worker threads.
On Windows 10 and later, the kernel dynamically creates additional
worker threads as needed, with a default maximum limit of 4096 (see
ExpMaximumKernelWorkerThreads) that can be configured through the
registry up to a maximum of 16,384 threads and down to a minimum of 32.
You can set this using the MaximumKernelWorkerThreads value under the
registry key HKLM\SYSTEM\CurrentControlSet\Control\Session
Manager\Executive.
Each partition object, which we described in Chapter 5 of Part 1, contains
an executive partition, which is the portion of the partition object relevant to
the executive—namely, the system worker thread logic. It contains a data
structure tracking the work queue manager for each NUMA node part of the
partition (a queue manager is made up of the deadlock detection timer, the
work queue item reaper, and a handle to the actual thread doing the
management). It then contains an array of pointers to each of the eight
possible work queues (EX_WORK_QUEUE). These queues are associated
with an individual index and track the number of minimum (guaranteed) and
maximum threads, as well as how many work items have been processed so
far.
Every system includes two default work queues: the ExPool queue and the
IoPool queue. The former is used by drivers and system components using
the ExQueueWorkItem API, whereas the latter is meant for
IoAllocateWorkItem-type APIs. Finally, up to six more queues are defined
for internal system use, meant to be used by the internal (non-exported)
ExQueueWorkItemToPrivatePool API, which takes in a pool identifier from
0 to 5 (making up queue indices 2 to 7). Currently, only the memory
manager’s Store Manager (see Chapter 5 of Part 1 for more information)
leverages this capability.
The executive tries to match the number of critical worker threads with
changing workloads as the system executes. Whenever work items are being
processed or queued, a check is made to see if a new worker thread might be
needed. If so, an event is signaled, waking up the
ExpWorkQueueManagerThread for the associated NUMA node and
partition. An additional worker thread is created in one of the following
conditions:
■    There are fewer threads than the minimum number of threads for this
queue.
■    The maximum thread count hasn’t yet been reached, all worker
threads are busy, and there are pending work items in the queue, or
the last attempt to try to queue a work item failed.
Additionally, once every second, for each worker queue manager (that is,
for each NUMA node on each partition) the ExpWorkQueueManagerThread
can also try to determine whether a deadlock may have occurred. This is
defined as an increase in work items queued during the last interval without a
matching increase in the number of work items processed. If this is
occurring, an additional worker thread will be created, regardless of any
maximum thread limits, hoping to clear out the potential deadlock. This
detection will then be disabled until it is deemed necessary to check again
(such as if the maximum number of threads has been reached). Since
processor topologies can change due to hot add dynamic processors, the
thread is also responsible for updating any affinities and data structures to
keep track of the new processors as well.
Finally, once every double the worker thread timeout minutes (by default
10, so once every 20 minutes), this thread also checks if it should destroy any
system worker threads. Through the same registry key, this can be configured
to be between 2 and 120 minutes instead, using the value
WorkerThreadTimeoutInSeconds. This is called reaping and ensures that
system worker thread counts do not get out of control. A system worker
thread is reaped if it has been waiting for a long time (defined as the worker
thread timeout value) and no further work items are waiting to be processed
(meaning the current number of threads are clearing them all out in a timely
fashion).
EXPERIMENT: Listing system worker threads
Unfortunately, due to the per-partition reshuffling of the system
worker thread functionality (which is no longer per-NUMA node as
before, and certainly no longer global), the kernel debugger’s
!exqueue command can no longer be used to see a listing of system
worker threads classified by their type and will error out.
Since the EPARTITION, EX_PARTITION, and
EX_WORK_QUEUE data structures are all available in the public
symbols, the debugger data model can be used to explore the
queues and their manager. For example, here is how you can look
at the NUMA Node 0 worker thread manager for the main (default)
system partition:
Click here to view code image
lkd> dx ((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueueManagers[0]
((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueueManagers[0]      : 0xffffa483edea99d0 [Type: 
_EX_WORK_QUEUE_MANAGER *]
    [+0x000] Partition        : 0xffffa483ede51090 [Type: 
_EX_PARTITION *]
    [+0x008] Node             : 0xfffff80467f24440 [Type: 
_ENODE *]
    [+0x010] Event            [Type: _KEVENT]
    [+0x028] DeadlockTimer    [Type: _KTIMER]
    [+0x068] ReaperEvent      [Type: _KEVENT]
    [+0x080] ReaperTimer      [Type: _KTIMER2]
    [+0x108] ThreadHandle     : 0xffffffff80000008 [Type: 
void *]
    [+0x110] ExitThread       : 0x0 [Type: unsigned long]
    [+0x114] ThreadSeed       : 0x1 [Type: unsigned short]
Alternatively, here is the ExPool for NUMA Node 0, which
currently has 15 threads and has processed almost 4 million work
items so far!
Click here to view code image
lkd> dx ((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][0],d
((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][0],d        : 0xffffa483ede4dc70 [Type: 
_EX_WORK_QUEUE *]
    [+0x000] WorkPriQueue     [Type: _KPRIQUEUE]
    [+0x2b0] Partition        : 0xffffa483ede51090 [Type: 
_EX_PARTITION *]
    [+0x2b8] Node             : 0xfffff80467f24440 [Type: 
_ENODE *]
    [+0x2c0] WorkItemsProcessed : 3942949 [Type: unsigned 
long]
    [+0x2c4] WorkItemsProcessedLastPass : 3931167 [Type: 
unsigned long]
    [+0x2c8] ThreadCount      : 15 [Type: long]
    [+0x2cc (30: 0)] MinThreads       : 0 [Type: long]
    [+0x2cc (31:31)] TryFailed        : 0 [Type: unsigned 
long]
    [+0x2d0] MaxThreads       : 4096 [Type: long]
    [+0x2d4] QueueIndex       : ExPoolUntrusted (0) [Type: 
_EXQUEUEINDEX]
    [+0x2d8] AllThreadsExitedEvent : 0x0 [Type: _KEVENT *]
You could then look into the ThreadList field of the
WorkPriQueue to enumerate the worker threads associated with
this queue:
Click here to view code image
lkd> dx -r0 @$queue = ((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->
    ExPartition)->WorkQueues[0][0]
@$queue = ((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][0]                 : 0xffffa483ede4dc70 
[Type: _EX_WORK_QUEUE *]
lkd> dx Debugger.Utility.Collections.FromListEntry(@$queue-
>WorkPriQueue.ThreadListHead,
    "nt!_KTHREAD", "QueueListEntry")
Debugger.Utility.Collections.FromListEntry(@$queue-
>WorkPriQueue.ThreadListHead,
    "nt!_KTHREAD", "QueueListEntry")
    [0x0]            [Type: _KTHREAD]
    [0x1]            [Type: _KTHREAD]
    [0x2]            [Type: _KTHREAD]
    [0x3]            [Type: _KTHREAD]
    [0x4]            [Type: _KTHREAD]
    [0x5]            [Type: _KTHREAD]
    [0x6]            [Type: _KTHREAD]
    [0x7]            [Type: _KTHREAD]
    [0x8]            [Type: _KTHREAD]
    [0x9]            [Type: _KTHREAD]
    [0xa]            [Type: _KTHREAD]
    [0xb]            [Type: _KTHREAD]
    [0xc]            [Type: _KTHREAD]
    [0xd]            [Type: _KTHREAD]
    [0xe]            [Type: _KTHREAD]
    [0xf]            [Type: _KTHREAD]
That was only the ExPool. Recall that the system also has an
IoPool, which would be the next index (1) on this NUMA Node
(0). You can also continue the experiment by looking at private
pools, such as the Store Manager’s pool.
Click here to view code image
lkd> dx ((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][1],d
((nt!_EX_PARTITION*)(*
(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][1],d        : 0xffffa483ede77c50 [Type: 
_EX_WORK_QUEUE *]
    [+0x000] WorkPriQueue     [Type: _KPRIQUEUE]
    [+0x2b0] Partition        : 0xffffa483ede51090 [Type: 
_EX_PARTITION *]
    [+0x2b8] Node             : 0xfffff80467f24440 [Type: 
_ENODE *]
    [+0x2c0] WorkItemsProcessed : 1844267 [Type: unsigned 
long]
    [+0x2c4] WorkItemsProcessedLastPass : 1843485 [Type: 
unsigned long]
    [+0x2c8] ThreadCount      : 5 [Type: long]
    [+0x2cc (30: 0)] MinThreads       : 0 [Type: long]
    [+0x2cc (31:31)] TryFailed        : 0 [Type: unsigned 
long]
    [+0x2d0] MaxThreads       : 4096 [Type: long]
    [+0x2d4] QueueIndex       : IoPoolUntrusted (1) [Type: 
_EXQUEUEINDEX]
    [+0x2d8] AllThreadsExitedEvent : 0x0 [Type: _KEVENT *]
Exception dispatching
In contrast to interrupts, which can occur at any time, exceptions are
conditions that result directly from the execution of the program that is
running. Windows uses a facility known as structured exception handling,
which allows applications to gain control when exceptions occur. The
application can then fix the condition and return to the place the exception
occurred, unwind the stack (thus terminating execution of the subroutine that
raised the exception), or declare back to the system that the exception isn’t
recognized, and the system should continue searching for an exception
handler that might process the exception. This section assumes you’re
familiar with the basic concepts behind Windows structured exception
handling—if you’re not, you should read the overview in the Windows API
reference documentation in the Windows SDK or Chapters 23 through 25 in
Jeffrey Richter and Christophe Nasarre’s book Windows via C/C++
(Microsoft Press, 2007) before proceeding. Keep in mind that although
exception handling is made accessible through language extensions (for
example, the __try construct in Microsoft Visual C++), it is a system
mechanism and hence isn’t language specific.
On the x86 and x64 processors, all exceptions have predefined interrupt
numbers that directly correspond to the entry in the IDT that points to the
trap handler for a particular exception. Table 8-12 shows x86-defined
exceptions and their assigned interrupt numbers. Because the first entries of
the IDT are used for exceptions, hardware interrupts are assigned entries later
in the table, as mentioned earlier.
Table 8-12 x86 exceptions and their interrupt numbers
Interrupt 
Number
Exception
Mnemon
ic
0
Divide Error
#DE
1
Debug (Single Step)
#DB
2
Non-Maskable Interrupt 
(NMI)
-
3
Breakpoint
#BP
4
Overflow
#OF
5
Bounds Check (Range 
#BR
Exceeded)
6
Invalid Opcode
#UD
7
NPX Not Available
#NM
8
Double Fault
#DF
9
NPX Segment Overrun
-
10
Invalid Task State 