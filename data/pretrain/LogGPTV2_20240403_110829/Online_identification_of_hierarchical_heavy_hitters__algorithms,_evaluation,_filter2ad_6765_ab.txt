associated with it, with probabilistic bounds on the reconstruction
accuracy. The achievable accuracy is a function of both the number
of hash functions (H), and the size of hash tables (K). The base-
line scheme uses a separate sketch data structure per distinct preﬁx
length combination in all the dimensions.
Baseline variant 2:
Lossy Counting-based solution (lc), which
uses a deterministic, single-pass, sampling-based HH detection al-
gorithm called Lossy Counting (see [27]). Lossy Counting uses two
parameters: (cid:15) and (cid:30), where 0 (cid:20) (cid:15) (cid:28) (cid:30) (cid:20) 1. At any instant,
let N be the total number of items in the input data stream. Lossy
Counting can correctly identify all heavy-hitter keys whose frequen-
cies exceed (cid:30)N. lc provides lower and upper bounds on the count
associated with a heavy hitter. The gap between the two bounds is
guaranteed to be at most (cid:15)N. The space overhead for the algorithm
is O( 1
(cid:15) log((cid:15)N )). The Lossy Counting algorithm can be modiﬁed to
work with byte data instead of count data. All the complexity and
accuracy results still apply except that we need to replace N with
SU M. We use this adapted version in our evaluation.
We note that the algorithm in [12] is also based on Lossy Count-
ing. So we expect its accuracy to be similar to that of lc. In addition,
while their algorithm is normally much more efﬁcient than lc, the
worst-case amortized update cost is comparable to lc (the worst-case
scenario can occur when the keys in the input stream are uniformly
distributed, which can be caused by events like a distributed denial-
of-service attack using spoofed source addresses). So although we
do not directly compare against their algorithm, we expect the per-
formance of lc to be indicative of the worst-case performance of
their algorithm.
3.2 A trie-based solution to 1-d HHH detection
Our goal is to identify the preﬁxes (considering that we use the
destination IP as the key) that are responsible for an amount of traf-
ﬁc that exceeds a given threshold. We would like to do so while
maintaining minimal state and performing a minimum number of
update operations for each arriving ﬂow or packet.
The hierarchical nature of the problem reminds us of the clas-
sical IP lookup problem in which for every received packet the IP
destination ﬁeld in the packet header is used to search for a longest
matching preﬁx in a set of given IP preﬁxes (also known as a routing
table). The difference between our particular situation and the IP
lookup problem is that in the IP lookup problem the set of preﬁxes is
given as an input and is often static. In contrast, we need to generate
dynamically (based on the packet arrival pattern) the set of preﬁxes
that are associated with the heavy hitters.
Despite the difference, however, we are able to develop an effec-
tive solution to 1-d HHH detection by adapting an existing solution
to the static IP lookup problem – the trie-based solution proposed by
Srinivasan et al. [32].
Trie is a simple data structure. Each node in a one-bit trie has at
most two child nodes, one associated with bit 0 and the other with bit
1. Srinivasan et al. [32] have extended on the basic idea of one-bit
tries to create more reﬁned multi-bit tries that are better suited for
the IP lookup problem. Our algorithm is designed and implemented
for m-bit tries, where each node of the trie has 2m children, similar
to the idea of the multi-bit tries. However for simplicity we describe
our algorithm using one-bit tries.
The trie data structure. We maintain a standard trie data struc-
ture (as illustrated in Figure 1). Each node n in the trie is associated
with a preﬁx p(cid:3) identiﬁed by the path between the root of the trie
and the node. Array n:child contains pointers to the children of
n. Field n:depth gives the depth of n. Field n:fringe indicates
whether n is a fringe node – we consider n as a fringe node if af-
ter its creation, we see less than Tsplit amount of trafﬁc associated
with destination preﬁx p; otherwise, we consider n as an internal
node. Field n:volume records the volume of trafﬁc associated with
preﬁx p that we see after n is created and before n becomes an inter-
nal node. Field n:subtotal gives the total volume of trafﬁc for the
entire subtrie rooted at n, excluding the portion already accounted
for by n:volume. Fields n:miss copy and n:miss split represent
estimated volume of trafﬁc missed by node n (i.e., trafﬁc that is as-
sociated with preﬁx p but appears before the creation of n). The
copy-all and the splitting rules are used to compute n:miss copy
and n:miss split, respectively (details to follow). The last four
volume related ﬁelds are used to estimate the total volume of traf-
ﬁc that is associated with preﬁx p. We will describe the estimation
algorithm later in this section.
// vol type is the data type for volume
typedef struct f
child[(cid:1)];
trie (cid:3)
int
depth;
boolean fringe;
vol type volume;
vol type subtotal;
vol type miss copy;
vol type miss split;
// child[i] points to the i-th child
// the depth of this node
// true iff volume for entire subtrie  comprises of the longest matching preﬁx in both
the dimensions. The array is indexed by the lengths of the preﬁxes
p1 and p2. In the case of IPv4 preﬁxes, for a 1-bit trie-based solution,
W = 32.
Updating the summary data structure.
For every incoming
packet we ﬁrst update the individual 1-dimensional tries, which re-
turn the longest matching preﬁx in each of the dimensions. This
gives us two preﬁxes p1 and p2 with lengths l1 and l2 respectively.
Next the two lengths are used as an index to identify the hash ta-
ble H[l1][l2].  is then used as a lookup key in the hash
table H[l1][l2]. Subsequently, the volume ﬁeld of the entry associ-
ated with the key is incremented. This process is repeated for every
arriving packet. Figure 6 illustrates the basic algorithm.
For every packet three update operations are performed, one op-
eration in each of the two 1-dimensional tries, and one operation
in at most one of the hash-tables. This results in a very fast algo-
rithm. The memory requirement in the worst case is O((W 2=(cid:15))2) =
O(W 4=(cid:15)2), due to the use of cross-producting. But in practice, we
expect the actual memory requirement to be much lower.
Reconstructing volumes for 2-d internal nodes. To compute the
total volume for the internal nodes, we just need to add the volume
for each element in the hash tables to all its ancestors. This can be
implemented by scanning all the hash elements twice. During the