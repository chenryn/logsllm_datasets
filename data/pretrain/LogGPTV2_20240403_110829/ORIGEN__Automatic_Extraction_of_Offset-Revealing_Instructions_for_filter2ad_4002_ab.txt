For the rest of this section, we focus on ORI identiﬁcation.
The oﬀsets of data structure ﬁelds are often very small, and
small constant numbers are pervasive in binary code. There-
fore, we use a diﬀerent solution. We ﬁrst dynamically trace
the binary program and identify a set of instructions that
access the speciﬁed data structure ﬁelds (which is described
in “Dynamic Labeling”). We call these instructions “ORI
candidates”. Based on ORI candidates, we perform static
analysis to ﬁlter out false ORIs and discover more ORIs,
which is described in “ Static ORI Discovery”.
Dynamic ORI Labeling.
The goal of dynamic labeling is to collect a set of instruc-
tions that either read or write the given data structure ﬁeld
deﬁned in the abstract proﬁle. To do so, we need to know
not only when an instance of the data structure is created
and later destroyed, but the lifetime of data structure in-
stance during the program execution. With the aid of the
information about live data structure instances, we can pin-
point the instructions that access their speciﬁc ﬁelds during
tracing the program execution.
To this end, we should have certain knowledge about data
structures in the base version of the software. There are
three types of information we need to know about the data
structures in the base version: 1) The functions which create
and delete the data structure instances of interest; 2) Data
structure deﬁnitions that are relevant to the analysis task;
3) Actual oﬀset for each data structure ﬁeld of interest.
We hook functions which create and delete the data struc-
ture instances of interest during the binary execution to la-
bel the live data structure instances in the memory. We can
further identify all instructions which have write or read
operation on these live data structure instances by moni-
toring all the memory read and write operations during the
execution. The data structure deﬁnition and its ﬁeld oﬀ-
set information can help to extract ORIs in these identi-
ﬁed instructions. For the programs with source code, such
knowledge can be easily obtained. Even for the many binary
programs (e.g., Windows), we can still obtain the knowledge
from documentation of APIs. For the binary programs with
limited documentation, we have to rely on reverse engineer-
ing to retrieve the needed knowledge. This is a reasonable
assumption, because without this knowledge, memory anal-
ysis is not even possible in the ﬁrst place.
As for our running example shown in Figure 1, we have to
know the deﬁnition of session_state and a global variable
active_state pointing to this structure in OpenSSH6.4. More-
over, for the data structure ﬁelds of interest, we need to know
their actual oﬀsets within the data structure session_state.
Furthermore, we hook the alloc_session_state() function
to keep track of the creation of session_state. As OpenSSH
sever never frees the session_state instance, we do not hook
any other functions.
When tracing the program execution, we may face several
situations: (1) if an instruction does not access the ﬁeld
of interest at all, we simply drop it; (2) if an instruction
accesses multiple data structure ﬁelds at diﬀerent times, we
also drop it due to its ambiguity; (3) if an instruction is
observed to only access a single ﬁeld of interest and the
constant value carried in it matches with the ﬁeld’s actual
oﬀset, we treat this instruction to be an ORI; and (4) if an
instruction is observed to only access a single ﬁeld of interest
but it does not carry a constant or the constant value does
not match with the ﬁeld’s actual oﬀset, we keep it as an
ORI source. Although this instruction is not a real ORI by
deﬁnition, it may lead us to ﬁnd a real ORI through the
following static analysis.
Static ORI Discovery.
Based on the ORIs and ORI sources labeled through dy-
namic analysis, we further perform static analysis to discover
more ORIs which are missed by dynamic analysis.
Starting from an identiﬁed memory access instruction (ei-
ther ORI or ORI source), we perform the backward data-
ﬂow analysis to know how the memory operand is computed.
More speciﬁcally, we perform backward data-ﬂow analysis
on the memory operand in that instruction, and look for
a variable that holds the base address and a constant value
that holds the oﬀset. For example, in Figure 4, the memory-
access instruction at 0x402, which is the source for the ORI
at 0x3fe, is ﬁrst identiﬁed via dynamic analysis. ORI, by
deﬁnition is an instruction of the form ‘base + oﬀset’ where
oﬀset is equal to the oﬀset within the object that the ac-
cess corresponds to. We ﬁrst perform backward data-ﬂow
analysis from the ORI-source to reach the ORI, then, we ex-
0x80037324: mov eax, [edx+0Ch]         R  offset 0x170 base: 0x80090a08 type: session_state0x800370a4: mov dword ptr [eax+8], 0 W offset 0x15c base: 0x80090a08 type: session_state0x80046659: mov edx, [eax+21Ch]       R  offset 0x21c base: 0x80090a08 type: session_state0x80037324: mov eax, [edx+0Ch]         R  offset 0x160 base: 0x80090a08 type: session_state0x80045624: mov ecx, [esi+214h]         R  offset 0x214 base: 0x80090a08 type: session_stateFigure 4: Static discovery of ORIs.
tend the analysis to identify the source of the base register.
With the base register identiﬁed, ﬂow-insensitive forward-
data-ﬂow analysis on the base register reveals all the ORIs
present in the function. That is, in Figure 4, an ORI source
at 0x402 is ﬁrst identiﬁed via dynamic analysis. Then, the
corresponding ORI is identiﬁed at 0x3fe. The register con-
taining the base address is identiﬁed as ecx@0.
From the variable that holds the base address, we per-
form forward data-ﬂow analysis within the same function to
discover more ORIs. If we observe a constant value being
added to the base, and that value matches with one of our
data structure ﬁelds in the proﬁle, whichever instruction car-
ries this constant is a new ORI. In Figure 4, we start from
ecx@0 and perform forward data-ﬂow analysis and discover
ORIs at 0x412, 0x41b and 0x423.
To accomplish the said data-ﬂow analysis, the x86 code is
ﬁrst converted into an IR-SSA form (column 2 in Figure 4)
and the use-def and def-use chains are directly derived from
them [28]. Then, the deﬁnitions are recursively propagated
by substituting them into the uses until each of the state-
ments is composed of only the entry point deﬁnitions (e.g.,
ecx@0 in Figure 4). Column 3 in Figure 4 presents the
IR statements after substitution. In the end, we identify a
statement to be an ORI if and only if (1) The expression
contains a ‘base + oﬀset’1 form where base is equal to the
previously identiﬁed source of the base register (e.g., ecx@0
in the example) and (2) The oﬀset equals to a valid oﬀset
value within the proﬁle.
4. PROFILE LOCALIZATION
For each symbol deﬁned in the abstract proﬁle, we have
one or (often) multiple ORIs for the base version of a binary.
To localize the proﬁle for a new version of the binary, we try
to ﬁnd instructions in the new binary that match with these
ORI signatures and update the proﬁle based on the abstract
proﬁle and identiﬁed ORIs in the new binary.
4.1 ORI Identiﬁcation
We consider matching ORI signatures in a new binary as
a code search problem, and leverage the existing code search
technique to conduct the proﬁle localization.
To precisely label ORIs in a new binary, we need to con-
duct a CFG-based code search approach. The assumption
is that two versions of a binary share the similar control
1Oﬀset of 0 is a special case where the memory access ap-
pears like a regular dereference.
ﬂow graphs. This has been substantiated by existing lit-
eratures [10, 27, 33], and many other works also apply this
assumption into many applications [23]. The CFG-based
code search considers a instruction with the similar position
in the control ﬂow structure as a match. In this way, even
if the ORI in the new binary has a diﬀerent representation,
the CFG-based code search can still ﬁnd it, as long as two
versions of the binary share similar control ﬂow graphs.
The CFG-based search includes the control ﬂow graph
extraction and graph matching. We leverage the existing
tool BinDiﬀ [10] to achieve the CFG-based code search. It
has two advantages. Firstly, its control ﬂow graph matching
and instruction alignment perfectly suit our usage scenario.
Secondly, it is a mature tool with good runtime performance.
Therefore, we utilize Bindiﬀ to match the base version of a
binary with the new version.
4.2 Proﬁle Generation
The output of Bindiﬀ is a one-to-one mapping between
instructions of two binaries. We can generate the proﬁle for
the new binary, according to the abstract proﬁle and the
mapping. The proﬁle generation is to walk through each
symbol in the abstract proﬁle and update the ﬁeld oﬀset
information based its correspondent ORIs.
To this end, ORIGEN locates ORIs in the new binary
based on the mapping, identiﬁes all ORIs for each sym-
bol, and updates oﬀset information based on these identiﬁed
ORIs. ORIGEN can locate the semantically equivalent ORI
instructions in the new binary by looking up the instruction
mapping. It considers instructions mapped by ORIs of the
base version as qualiﬁed ORIs. ORIGEN clusters all iden-
tiﬁed ORIs by their symbol names, and update the oﬀset
information for each symbol based on its ORI cluster.
By the ORI deﬁnition in Section 3.1, we know each symbol
involves the object type and ﬁeld name. Each ORI cluster
have one or more ORIs. If there is only one ORI in the clus-
ter, we can directly extract its oﬀset information from the
ORI and assign it to the symbol. In most cases, the ORI
cluster contains multiple ORIs. We adopt the voting mech-
anism to update the oﬀset of a symbol. This is because that
the CFG-based code search could introduce the erroneous-
ness, and this could wrongly consider some instructions as
ORIs. Without false ORIs, the ORIs for the same sym-
bol share the same oﬀset value. False ORIs will break this
consistency and generate diﬀerent oﬀset values to confuse
ORIGEN. The voting mechanism is designed to automat-
x86 IR - SSA formAfter substitutionfunction_entry:	0x3fc: mov ebx, ecx	0x3fe: lea edx, [ecx+92h]! = >	 =  + >>0x402: mov eax, [edx]	0x408: cmp eax, 0	0x40b: jz label = >>	! =  + >>>	!0x40d: mov eax, 45h = >	0x412: mov [ebx+104h],eax + >> = > + >> = >0x418: mov eax, 20h = >0x41b: mov [ebx+118h],eax + >> = >	 + >> = >!0x421: xor eax, eax	0x423: mov [ebx+92h], eax = >	 = >	 + >> = >!! + >> = >label: retBase    +       OffsetStatically 	
discovered	
ORIsecx holds an input argumentORI sourceFigure 5: The statistics of the data types and the average number of ORIs to the ﬁeld type in the OpenSSH
dataset.
ically ﬁlter oﬀset values from false ORIs and improve the
accuracy of the proﬁle generation.
Considering each oﬀset value as a vote from its ORI, the
voting mechanism will rank all oﬀset values by the number
of votes, and select the oﬀset with the largest number of
votes as the true oﬀset for the symbol. Repeat this process,
the proﬁle generation will assign each symbol with an oﬀset
value and generate the proﬁle for the new binary.
4.3 Error Correction
It is possible that ORIGEN fails to update the oﬀset value
for a symbol in the new binary, if all of ORIs of some symbol
in the abstract proﬁle are misidentiﬁed in the new binary.
We adopt two strategies to resolve this problem.
The ﬁrst strategy is the conservative strategy. We can
ﬁlter out symbols with the high possibility to be wrongly
labeled in the generated proﬁle. Each symbol has a cluster.
We use the variance from the set of oﬀset values in the clus-
ter to determine its false possibility. A threshold is set to
determine whether the symbol is ﬁltered or not. If the vari-
ance of the symbol value is above the threshold, we consider
this symbol as a false and ﬁlter it out.
The second strategy is that we do not discard any sym-
bol in the proﬁle. Instead, we apply the proﬁle to conduct
the memory analysis. During the memory scanning, we col-
lect the values from these symbols, and screen false ones by
heuristics. Once we found some abnormal values, we ﬁlter
the symbol from the proﬁle.
We also can combine two strategies together to conduce
the error correction. In all, the error correction can greatly
reduce the false positive rate for the generated proﬁle. This
is substantiated by the experiment in Section 8.
5.
IMPLEMENTATION
We have implemented the prototype of ORIGEN in C
and Python. More speciﬁcally, we write the dynamic label-
ing plugin for DECAF [18] in C. As a whole-system dynamic
analysis platform, we use DECAF to trace a user-level pro-
gram, an entire OS kernel, or a speciﬁc kernel module. Be-
sides, we write an IDA Pro plugin for static binary analysis,
based on IDA-decompiler [4]. We leverage BinDiﬀ for the
ORI search. The entire ORIGEN has around 300 lines of C
code and 2K lines of Python code.
6. EXPERIMENTS
This section empirically evaluates ORIGEN. First, we
represent the experiment setup in Section 6.1, and then
we systematically evaluate the accuracy of ORIGEN in the
cross-version setting in Section 6.2 and Section 6.3. In Sec-
tion 6.5, we apply ORIGEN into two use cases: memory
forensics and VMI. Finally, we evaluate the runtime perfor-
mance of ORIGEN in Section 6.6.
6.1 Experiment Setup
All experiments are conducted on a machine with Intel(R)
Core i5 @ 2.9GHz and 16 GB DDR3-RAM running 64-
bit Ubuntu 14.04. We evaluate ORIGEN on four sets of
software families:
including Windows, Linux, OpenSSH and
dm_crypt, as shown in Table 1. To verify the accuracy of
the proposed method, we systematically evaluate ORIGEN
on OpenSSH family. For the rest of the software families, we
conduct case study analysis on some representative versions.
The experimental set is representative for the following
reasons: 1) the set is a suﬃcient sampling of real-word soft-
wares. The versions in our experiments cover a span of 13
years of OpenSSH, from 2.2.0p1 in 2002 to 6.8p1 in 2015; 2)
the data types and the structs in OpenSSH are rich and rep-
resentative. For example, there are 1,904 structs and 22,618
ﬁelds in total for 40 versions of OpenSSH. Figure 5 illustrates
the number of unique data types in each version. The size
and diversity of the data should provide a systematic and ob-
jective evaluation for the proposed approach; 3) the source
code of OpenSSH provide a gold standard for evaluating the
performance of ORIGEN.
Evaluation Metrics: We employ precision to evaluate
the performance of ORIGEN. Given a source version s, our
task is to predict the oﬀsets of correspondent data types
in the target version t. The oﬀset precision for the target
version is calculated from:
precision =
|δ|
|s ∩ t| ,
(1)
where |s∩ t| represents the total number of shared data ﬁeld
names in the two versions, and |δ| represents the number
of correctly predicted oﬀsets. The ground truth of the data
ﬁeld names can be directly obtained from the source code of
OpenSSH. Note, the source code is not used in prediction.
0 20 40 60 80 100 2.2.0p1 2.3.0p1 2.5.1p1 2.9.9p1 3.0.1p1 3.1p1 3.2.2p1 3.3p1 3.4p1 3.7.1p1 3.8.1p1 3.9p1 4.0p1 4.1p1 4.2p1 4.3p2 4.4p1 4.5p1 4.6p1 4.7p1 4.9p1 5.0p1 5.1p1 5.2p1 5.3p1 5.4p1 5.5p1 5.6p1 5.7p1 5.8p1 5.9p1 6.0p1 6.1p1 6.2p1 6.3p1 6.4p1 6.5p1 6.6p1 6.7p1 6.8p1 Count Different versions of OpenSSH in the experiment #Unique data types Average #ORI for each  field type Program # of Ver
Windows
Linux
OpenSSH
dm_crypt
3
9
40
8
Start Ver
Ver
XP3
2.6.32
2.2.0
3.5
End Ver
Ver
Date
2001 Wind7
2010