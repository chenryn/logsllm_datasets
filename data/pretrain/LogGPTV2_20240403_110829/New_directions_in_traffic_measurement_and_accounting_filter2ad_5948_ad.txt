The cost of this improvement in accuracy is an increase
in the size of the ﬂow memory. We need enough memory to
hold the samples from both measurement intervals6. There-
fore the expected number of entries is bounded by 2O· C/T .
To bound with high probability the number of entries we
use the normal curve and the standard deviation of the the
number of sampled packets during the 2 intervals which is
2Cp(1 − p).
Example: For an oversampling of 20 and acceptable prob-
ability of overﬂow equal to 0.1%, the ﬂow memory has to
have at most 4,207 entries to preserve entries.
4.1.4 The effect of early removal
The eﬀect of early removal on the proportion of false neg-
atives depends on whether or not the entries removed early
are reported. Since we believe it is more realistic that im-
plementations will not report these entries, we will use this
assumption in our analysis. Let R < T be the early removal
threshold. A ﬂow at the threshold is not reported unless one
of its ﬁrst T − R bytes is sampled. Therefore the probability
−O(T−R)/T . If we use
of missing the ﬂow is approximately e
an early removal threshold of R = 0.2∗ T , this increases the
probability of missing a large ﬂow from 2∗ 10
−7
with an oversampling of 20.
−9 to 1.1∗ 10
Early removal reduces the size of the memory required by
limiting the number of entries that are preserved from the
previous measurement interval. Since there can be at most
C/R ﬂows sending R bytes, the number of entries that we
6We actually also keep the older entries that are above the
threshold. Since we are performing a worst case analysis we
assume that there is no ﬂow above the threshold, because if
there were, many of its packets would be sampled, decreasing
the number of entries required.
keep is at most C/R which can be smaller than OC/T , the
bound on the expected number of sampled packets. The
expected number of entries we need is C/R + OC/T .
To bound with high probability the number of entries we
use the normal curve. If R ≥ T /O the standard deviation
is given only by the randomness of the packets sampled in
one interval and is Cp(1 − p).
Example: An oversampling of 20 and R = 0.2T with over-
ﬂow probability 0.1% requires 2,647 memory entries.
4.2 Multistage ﬁlters
In this section, we analyze parallel multistage ﬁlters. We
only present the main results. The proofs and supporting
lemmas are in [6]. We ﬁrst deﬁne some new notation:
• b the number of buckets in a stage;
• d the depth of the ﬁlter (the number of stages);
• n the number of active ﬂows;
• k the stage strength is the ratio of the threshold and
the average size of a counter. k = T b
C , where C de-
notes the channel capacity as before. Intuitively, this
is the factor we inﬂate each stage memory beyond the
minimum of C/T
Example: To illustrate our results numerically, we will
assume that we solve the measurement example described
in Section 4 with a 4 stage ﬁlter, with 1000 buckets at each
stage. The stage strength k is 10 because each stage memory
has 10 times more buckets than the maximum number of
ﬂows (i.e., 100) that can cross the speciﬁed threshold of 1%.
4.2.1 The quality of results for multistage ﬁlters
As discussed in Section 3.2, multistage ﬁlters have no false
negatives. The error of the traﬃc estimates for large ﬂows is
bounded by the threshold T since no ﬂow can send T bytes
without being entered into the ﬂow memory. The stronger
the ﬁlter, the less likely it is that the ﬂow will be entered into
the ﬂow memory much before it reaches T . We ﬁrst state
an upper bound for the probability of a small ﬂow passing
the ﬁlter described in Section 3.2.
k
.
T
Lemma 1. Assuming the hash functions used by diﬀerent
stages are independent, the probability of a ﬂow of size s <
T (1−1/k) passing a parallel multistage ﬁlter is at most ps ≤
T−s(cid:17)d
(cid:16) 1
The proof of this bound formalizes the preliminary anal-
ysis of multistage ﬁlters from Section 3.2. Note that the
bound makes no assumption about the distribution of ﬂow
sizes, and thus applies for all ﬂow distributions. The bound
is tight in the sense that it is almost exact for a distribution
that has (cid:9)(C − s)/(T − s)(cid:10) ﬂows of size (T − s) that send all
their packets before the ﬂow of size s. However, for realistic
traﬃc mixes (e.g., if ﬂow sizes follow a Zipf distribution),
this is a very conservative bound.
Based on this lemma we obtain a lower bound for the
expected error for a large ﬂow.
Theorem 2. The expected number of bytes of a large ﬂow
undetected by a multistage ﬁlter is bound from below by
E[s − c] ≥ T (cid:18)1 −
d
k(d − 1)(cid:19) − ymax
(1)
where ymax is the maximum size of a packet.
329This bound suggests that we can signiﬁcantly improve the
accuracy of the estimates by adding a correction factor to
the bytes actually counted. The down side to adding a cor-
rection factor is that we can overestimate some ﬂow sizes;
this may be a problem for accounting applications.
4.2.2 The memory requirements for multistage ﬁlters
We can dimension the ﬂow memory based on bounds on
the number of ﬂows that pass the ﬁlter. Based on Lemma 1
we can compute a bound on the total number of ﬂows ex-
pected to pass the ﬁlter.
Theorem 3. The expected number of ﬂows passing a par-
allel multistage ﬁlter is bound by
E[npass] ≤ max  b
k − 1
, n(cid:18) n
kn − b(cid:19)d! + n(cid:18) n
kn − b(cid:19)d
(2)
Example: Theorem 3 gives a bound of 121.2 ﬂows. Using
3 stages would have resulted in a bound of 200.6 and using 5
would give 112.1. Note that when the ﬁrst term dominates
the max, there is not much gain in adding more stages.
In [6] we have also derived a high probability bound on
the number of ﬂows passing the ﬁlter.
Example: The probability that more than 185 ﬂows pass
the ﬁlter is at most 0.1%. Thus by increasing the ﬂow memo-
ry from the expected size of 122 to 185 we can make overﬂow
of the ﬂow memory extremely improbable.
4.2.3 The effect of preserving entries and shielding
Preserving entries aﬀects the accuracy of the results the
same way as for sample and hold: long lived large ﬂows have
their traﬃc counted exactly after their ﬁrst interval above
the threshold. As with sample and hold, preserving entries
basically doubles all the bounds for memory usage.
Shielding has a strong eﬀect on ﬁlter performance, since
it reduces the traﬃc presented to the ﬁlter. Reducing the
traﬃc α times increases the stage strength to k ∗ α, which
can be substituted in Theorems 2 and 3.
5. COMPARING MEASUREMENT METH-
ODS
In this section we analytically compare the performance
of three traﬃc measurement algorithms: our two new algo-
rithms (sample and hold and multistage ﬁlters) and Sampled
NetFlow. First, in Section 5.1, we compare the algorithms
at the core of traﬃc measurement devices. For the core
comparison, we assume that each of the algorithms is given
the same amount of high speed memory and we compare
their accuracy and number of memory accesses. This allows
a fundamental analytical comparison of the eﬀectiveness of
each algorithm in identifying heavy-hitters.
However, in practice, it may be unfair to compare Sam-
pled NetFlow with our algorithms using the same amount
of memory. This is because Sampled NetFlow can aﬀord to
use a large amount of DRAM (because it does not process
every packet) while our algorithms cannot (because they
process every packet and hence need to store per ﬂow en-
tries in SRAM). Thus we perform a second comparison in
Section 5.2 of complete traﬃc measurement devices. In this
second comparison, we allow Sampled NetFlow to use more
memory than our algorithms. The comparisons are based
Measure
Relative error
Memory accesses
Sample
and hold
√
2
M z
1
Multistage
Sampling
ﬁlters
1+10 r log10(n)
M z
1 + log10(n)
1√
M z
1
x
Table 1: Comparison of the core algorithms: sample
and hold provides most accurate results while pure
sampling has very few memory accesses
on the algorithm analysis in Section 4 and an analysis of
NetFlow taken from [6].
5.1 Comparison of the core algorithms
In this section we compare sample and hold, multistage
ﬁlters and ordinary sampling (used by NetFlow) under the
assumption that they are all constrained to using M memory
entries. We focus on the accuracy of the measurement of a
ﬂow (deﬁned as the standard deviation of an estimate over
the actual size of the ﬂow) whose traﬃc is zC (for ﬂows of
1% of the link capacity we would use z = 0.01).
The bound on the expected number of entries is the same
for sample and hold and for sampling and is pC. By mak-
ing this equal to M we can solve for p. By substituting in
the formulae we have for the accuracy of the estimates and
after eliminating some terms that become insigniﬁcant (as
p decreases and as the link capacity goes up) we obtain the
results shown in Table 1.
For multistage ﬁlters, we use a simpliﬁed version of the
result from Theorem 3: E[npass] ≤ b/k + n/kd. We increase
the number of stages used by the multistage ﬁlter logarith-
mically as the number of ﬂows increases so that a single
small ﬂow is expected to pass the ﬁlter7 and the strength
of the stages is 10. At this point we estimate the memory
usage to be M = b/k + 1 + rbd = C/T + 1 + r10C/T log10(n)
where r depends on the implementation and reﬂects the rel-
ative cost of a counter and an entry in the ﬂow memory.
From here we obtain T which will be the maximum error of
our estimate of ﬂows of size zC. From here, the result from
Table 1 is immediate.
The term M z that appears in all formulae in the ﬁrst
row of the table is exactly equal to the oversampling we de-
ﬁned in the case of sample and hold. It expresses how many
times we are willing to allocate over the theoretical mini-
mum memory to obtain better accuracy. We can see that
the error of our algorithms decreases inversely proportional
to this term while the error of sampling is proportional to
the inverse of its square root.
The second line of Table 1 gives the number of memory
locations accessed per packet by each algorithm. Since sam-
ple and hold performs a packet lookup for every packet8,
its per packet processing is 1. Multistage ﬁlters add to the
one ﬂow memory lookup an extra access to one counter per
stage and the number of stages increases as the logarithm of
7Conﬁguring the ﬁlter such that a small number of small
ﬂows pass would have resulted in smaller memory and fewer
memory accesses (because we would need fewer stages), but
it would have complicated the formulae.
8We equate a lookup in the ﬂow memory to a single memory
access. This is true if we use a content associable memo-
ry. Lookups without hardware support require a few more
memory accesses to resolve hash collisions.
330the number of ﬂows. Finally, for ordinary sampling one in
x packets get sampled so the average per packet processing
is 1/x.
Table 1 provides a fundamental comparison of our new
algorithms with ordinary sampling as used in Sampled Net-
Flow. The ﬁrst line shows that the relative error of our
√
algorithms scales with 1/M which is much better than the
M scaling of ordinary sampling. However, the second
1/
line shows that this improvement comes at the cost of requir-
ing at least one memory access per packet for our algorithms.
While this allows us to implement the new algorithms us-
ing SRAM, the smaller number of memory accesses (< 1)
per packet allows Sampled NetFlow to use DRAM. This is
true as long as x is larger than the ratio of a DRAM mem-
ory access to an SRAM memory access. However, even a
DRAM implementation of Sampled NetFlow has some prob-
lems which we turn to in our second comparison.
5.2 Comparing Measurement Devices
Table 1 implies that increasing DRAM memory size M
to inﬁnity can reduce the relative error of Sampled NetFlow
to zero. But this assumes that by increasing memory one
can increase the sampling rate so that x becomes arbitrarily
If x = 1, there would be no error since every
close to 1.
packet is logged. But x must at least be as large as the ratio
of DRAM speed (currently around 60 ns) to SRAM speed
(currently around 5 ns); thus Sampled NetFlow will always
have a minimum error corresponding to this value of x even
when given unlimited DRAM.
With this insight, we now compare the performance of
our algorithms and NetFlow in Table 2 without limiting
NetFlow memory. Thus Table 2 takes into account the un-
derlying technologies (i.e., the potential use of DRAM over
SRAM) and one optimization (i.e., preserving entries) for
both our algorithms.
We consider the task of estimating the size of all the ﬂows
above a fraction z of the link capacity over a measurement
interval of t seconds. In order to make the comparison possi-
ble we change somewhat the way NetFlow operates: we as-
sume that it reports the traﬃc data for each ﬂow after each
measurement interval, like our algorithms do. The four char-
acteristics of the traﬃc measurement algorithms presented
in the table are: the percentage of large ﬂows known to be
measured exactly, the relative error of the estimate of a large
ﬂow, the upper bound on the memory size and the number
of memory accesses per packet.
Note that the table does not contain the actual memory
used but a bound. For example the number of entries used
by NetFlow is bounded by the number of active ﬂows and
the number of DRAM memory lookups that it can perfor-
m during a measurement interval (which doesn’t change as
the link capacity grows). Our measurements in Section 7
show that for all three algorithms the actual memory usage
is much smaller than the bounds, especially for multistage
ﬁlters. Memory is measured in entries, not bytes. We as-
sume that a ﬂow memory entry is equivalent to 10 of the
counters used by the ﬁlter because the ﬂow ID is typical-
ly much larger than the counter. Note that the number of
memory accesses required per packet does not necessarily
translate to the time spent on the packet because memory
accesses can be pipelined or performed in parallel.
We make simplifying assumptions about technology evo-
lution. As link speeds increase, so must the electronics.
Therefore we assume that SRAM speeds keep pace with link
capacities. We also assume that the speed of DRAM does
not improve signiﬁcantly ([18] states that DRAM speeds im-
prove only at 9% per year while clock rates improve at 40%
per year).
We assume the following conﬁgurations for the three al-
gorithms. Our algorithms preserve entries. For multistage
ﬁlters we introduce a new parameter expressing how many
times larger a ﬂow of interest is than the threshold of the
ﬁlter u = zC/T . Since the speed gap between the DRAM
used by sampled NetFlow and the link speeds increases as
link speeds increase, NetFlow has to decrease its sampling
rate proportionally with the increase in capacity9 to provide
the smallest possible error. For the NetFlow error calcula-
tions we also assume that the size of the packets of large
ﬂows is 1500 bytes.
Besides the diﬀerences (Table 1) that stem from the core
algorithms, we see new diﬀerences in Table 2. The ﬁrst big
diﬀerence (Row 1 of Table 2) is that unlike NetFlow, our
algorithms provide exact measures for long-lived large ﬂows
by preserving entries. More precisely, by preserving entries
our algorithms will exactly measure traﬃc for all (or almost
all in the case of sample and hold) of the large ﬂows that
were large in the previous interval. Given that our measure-
ments show that most large ﬂows are long lived, this is a big
advantage.
Of course, one could get the same advantage by using an
SRAM ﬂow memory that preserves large ﬂows across mea-
surement intervals in Sampled NetFlow as well. However,
that would require the router to root through its DRAM
ﬂow memory before the end of the interval to ﬁnd the large
ﬂows, a large processing load. One can also argue that if
one can aﬀord an SRAM ﬂow memory, it is quite easy to do
Sample and Hold.
The second big diﬀerence (Row 2 of Table 2) is that we
can make our algorithms arbitrarily accurate at the cost of
increases in the amount of memory used10 while sampled
NetFlow can do so only by increasing the measurement in-
terval t.
The third row of Table 2 compares the memory used by
the algorithms. The extra factor of 2 for sample and hold
and multistage ﬁlters arises from preserving entries. Note
that the number of entries used by Sampled NetFlow is
bounded by both the number n of active ﬂows and the num-
ber of memory accesses that can be made in t seconds. Fi-
nally, the fourth row of Table 2 is identical to the second
row of Table 1.
Table 2 demonstrates that our algorithms have two advan-
tages over NetFlow: i) they provide exact values for long-
lived large ﬂows (row 1) and ii) they provide much better
accuracy even for small measurement intervals (row 2). Be-
sides these advantages, our algorithms also have three more
advantages not shown in Table 2. These are iii) provable
lower bounds on traﬃc, iv) reduced resource consumption
for collection, and v) faster detection of new large ﬂows. We
now examine advantages iii) through v) in more detail.
9If the capacity of the link is x times OC-3, then one in x