product net (SPN)iscreatedforagiventraceandWF-net,e.g.,asinFig.5.For
a formal definition of a SPN, we refer to [4]. Note that each transition corre-
sponds to a (prefix-)alignment move. Figure5b shows the corresponding SPN’s
state space, on which the shortest path search is executed. Note that each edge
representsanalignmentmoveand,hence,hasassignedcosts.Inthegivenexam-
ple, theinitial state/marking for theshortestpath searchis [p,p ] andthegoal
0 1
states are all states/markings containing p.
2
4 Incremental Prefix-Alignment Computation
In this section, we initially present the main idea of incrementally computing
prefix-alignments on an event stream. Next, we introduce the proposed imple-
mentation of our approach. Subsequently, we present two extensions to improve
the practical applicability of the approach.
Scalable Online Conformance Checking 385
4.1 Background
The core idea of incrementally computing prefix-alignments is to continue a
shortestpathsearchonanextendedsearchspaceuponreceivinganewevent.For
each process instance, we cache its current SPN and extend it upon receiving a
new event. Moreover, we store intermediate search-results and reuse them when
we continue the search. Next, we briefly list the main steps of the algorithm.
Assume an event stream and a reference process model as input.
1. We receive a new event. For instance, consider (c ,b) describing that the
1
activity b was executed for the process instance identified by case-id c .
1
2. WeextendtheSPNforprocessinstancec bythenewactivityb.Forexample,
1
assumethatwepreviouslyreceivedtheevent(c ,a).ConsiderFig.5ashowing
1
theextensionoftheSPNhighlightedbydashedgrayelements.Notethatwhen
extending a SPN by a new activity, new transitions are added representing a
log move on the new activity and potential synchronous moves.
3. Wecontinuethesearchforashortestpathonthestatespaceoftheextended
SPN from previously cached intermediate search-results, i.e., states already
explored/investigated.
4. We return the prefix-alignment and cache the search-results.
For a detailed overview, we refer to [21]. Next, we introduce the proposed
implementation of the incremental prefix-alignment computation approach.
4.2 Implementation
In this section, we present a horizontally scalable and fault-tolerant online con-
formance checking application based on Apache Kafka [15]. In the following, we
explain the system’s architecture using Fig.6.
Apache Kafka is a distributed log processing/streaming system that is
designed for handling high throughput of event data while maintaining low
latency. Kafka implements the publish/subscribe pattern [14]. The key prop-
erty of a publisher is that the messages are not directed to a specific subscriber.
Instead, a middleware-component stores the produced messages and categorizes
them according to given criteria. Independent of the publishers, subscribers can
process published messages. In our implementation, Kafka acts as the middle-
warecomponent for event storageand offersAPIsfor publishing event data and
subscribingtoit.InKafka’sterminology,producers correspondtopublishersand
consumers to subscribers. A producer, in our case a single event stream, sends
a message to one of potentially many Kafka Brokers. Brokers, which are usually
distributedoverseveralphysicalnodes,formaKafka cluster.InFig.6,wedepict
a Kafka cluster with three brokers.
Kafka stores messages, i.e., the events capturing the execution of process
instances, in topics. To avoid a broker holding all messages, which is impossible
in some cases, topics can be split into any number of partitions. In our imple-
mentation, the events from the event stream are published to the event-topic.
386 D. Schuster and G. J. Kolhof
Fig.6. Architecture overview of the proposed implementation
The structure of this topic is depicted in Fig.6 within the dashed box in the
Kafka Cluster. Further, we assume that the topic is divided into three parti-
tions.Partitionsarevisualizedasarrayswhereeachcellcontainsamessage,i.e.,
an event containing a case-id and an activity label. The first tuple entry refers
to a case identifier of a process instance which is also the key of the message.
The second entry is the payload consisting of the activity label. Kafka’s default
partitioning strategy ensures that messages with the same key are written to
the same partition. Hence, events belonging to the same case are routed to the
same partition. Moreover, it is shown that the topic called event-topic is repli-
catedonce.Thus,thereareatotalofsixpartitions,whichareevenlydistributed
among the brokers (indicated by boxes labeled with P1–P3).
Kafka achieves horizontal scalability through consumers, who read messages
accordingtotheirorderinthecorrespondingpartition.Whenaconsumergroup
subscribestoatopic,Kafkaassignseachpartitiontoamemberinthegroups.t.
eachpartitionisprocessedbyexactlyoneconsumer.Thus,thevariousconsumers
can process a topic in parallel. Hence, the maximum degree of parallelism of an
application is determined by the number of partitions of a topic. Consequently,
the way to scale a system that uses Kafka is to simply add more consumers to
a group and increase the number of partitions.
Kafka Streams is a client library used to build services that stream data
from Kafka topics. Consider the three instances of the prefix-alignment-service
in Fig.6. Each instance calculates prefix-alignments for the process instances
Scalable Online Conformance Checking 387
whose events are written to the assigned partitions. Since the event-topic is
divided into three partitions, each partition is assigned to one instance.
4.3 Direct Synchronizing
In this section, we introduce an extension of the proposed implementation to
improvethecalculationtime.Inparticular,weexplainhowwecanskipshortest
path searches in cases wherewe can immediately returnthe shortestpath onan
extended search space upon receiving a new event.
The idea of direct synchronizing is to skip shortest path searches in cases
where we can simply extend the previous calculated prefix-alignment by a syn-
chronous move on the newly observed activity. Hence, we execute the first and
second step as described in Sect.4.1. Next, we check if we can add a syn-
chronousmovewithoutexecutingashortestpathsearch.Wedepictthischeckin
Algorithm 1.
Algorithm 1: Direct synchronizing
input: N=(P,T,F,λ) // reference process model, i.e., a sound WF-net
σ=σ·a∈A∗ with a∈A //extended trace
γ∈ (A∪{})×(T∪{}) ∗ // previous prefix-alignment for σ and N
begin
1 S=(PS,TS,FS,λS) with M i ← create/extend SPN for N and σ;
2 σ γ ← extract sequence of transitions from TS corresponding to γ;
3 let M s.t. (S,M i)−σ −→γ (S,M);
4 for (t,t)∈TS do // iterate overtransitions from SPN S
5 if (S,M)[(t,t)∧λS (t,t) =(a,a) then
// transition (t,t) is enabled and represents a synchronous move on the
new activity a 
6 return γ· a,(t,t) ; // append sync. move to prefix-alignment
7 apply standard approach; // direct synchronizing is not possible
AsinputweassumeareferenceprocessmodelN,theextendedtraceσ where
σ represents the previous trace for the corresponding case, and the previously
calculated prefix-alignment γ of the trace σ and N. First, we create/extend
the SPN for the extended trace and the reference process model (line 1). Next,
we translate the previous prefix-alignment γ to a sequence of transitions in the
SPNS(line2).Notethatthisisalwayspossiblebecauseevery(prefix-)alignment
corresponds to a sequence of transitions in the corresponding SPN. Moreover,
since we always extend the SPN upon receiving a new event, such sequence in
theextendedSPNexiststhatcorrespondstoγ.Giventhesequenceoftransitions
σ γ,wedeterminethestateM wheretheprevioussearchstopped(line3).Next,
wecheckifwecandirectlyexecuteatransitionrepresentingasynchronousmove
388 D. Schuster and G. J. Kolhof
forthenewactivityainthisstate(line5).Iftrue,wesimplyextendtheprevious
prefix-alignmentbyasynchronousmoveandreturn(line6).Otherwise,weapply
the standard approach, i.e., the third and fourth step described in Sect.4.1.
The main reason why it is beneficial to do the presented pre-check before
actually continuing the shortest path search is to avoid heuristic recalculations.
Such heuristic function is part of the used heuristic search algorithm to increase
search efficiency and estimates for each state the costs to reach a goal state.
Sincethesearchspacegetsextended,andalsothegoalstatesaredifferentineach
incrementalsearch,suchheuristicrecalculationsareneededforeachincremental
search.Asshownin[21],heuristicrecalculationsinvolveahighcalculationeffort.
Thus,avoidingtrivialshortestpathsearchescanpotentiallyspeeduptheprefix-
alignment calculation.
4.4 Prefix Caching
In this section, we introduce prefix-caching for the incremental prefix-alignment
approach. In an online environment, where multiple process instances of the
sameprocessarerunning,itislikelythatthesequencesofactivitiesperformedis
similartosomedegree.Thus,onewantstoavoidrecalculatingprefix-alignments
for event sequences that were already observed in the past. By applying prefix-
caching, we avoid solving identical shortest path problem multiple times for
process instances that share a certain prefix.
Table 1. Conceptual idea of prefix-caching
Processed events Cached prefixes
(c 1,a) a
(c 1,a),(c 2,a) a
(c 1,a),(c 2,a),(c 2,b) a,a,b
(c 1,a),(c 2,a),(c 2,b),(c 1,b) a,a,b
Fig.7. Overview of prefix-caching within our proposed implementation
Scalable Online Conformance Checking 389
For instance, assume the event stream (c ,a),(c ,a),(c ,b),(c ,b),... .
1 2 2 1
ConsiderTable1,whereweshowthecachedprefixeswhileprocessingthesample
eventstream. Per cachedprefixwesave intermediate searchresults representing
thecurrentstateofthesearch,i.e.,alreadyexploredstatesfromthesearchspace.
Moreover, we save the SPN and the prefix-alignment per cached prefix. Hence,
we are either able to immediately return the prefix-alignment if we have calcu-
lateditalreadyforagivenprefixortocontinuethesearch.Inthegivenexample
(Table1), we can skip two shortest path problems and return immediately a
prefix-alignment, i.e., upon receiving the event (c ,a) and (c ,b).
2 1
In Fig.7, we depict the prefix caching approach integrated within the pro-
posed implementation. Assume a new event (c ,d) arrives. First, we fetch the
1
prefix observed so far for case c and extend it by the new activity d. Note that
1
theaggregatecontainsallinformationstoredforagivencase.Next,wecheckthe
prefix-cache if we have processed the given prefix before. If this is the case, we
have a cache hit. Hence, we update the SPN, the already explored search space
andtheprefix-alignmentintheaggregateofthegivencasec .Thus,weimmedi-
1
ately return a prefix-alignment and do not solve a shortest path problem. If we
haveacachemiss,weapplythestandardapproach,i.e.,wefetchtheSPN,extend
it by the new activity and continue the shortest path search from the already
explored search space (Sect.4.1). At this stage, we can also optionally use the
direct synchronizing approach presented in Sect.4.3 by first checking if we can
directly append a synchronous move to the previous prefix-alignment. Finally,
we update the aggregate of the case and return a prefix-alignment. Moreover,
we send the new observed prefix to the prefix cache.
Sinceavailablememoryisfinite,thesizeofthecachehastobelimited.Con-
sequently, a cached prefix has to be replaced when a new prefix is written to
thecache that has already reachedits maximum capacity. For instance, popular
cache replacement algorithms are: least recently used (LRU), most recently used
(MRU) and least frequently used (LFU) [19]. Many extensions to these general
strategies have been proposed that address problems such as large memory con-
sumptionformetadata.OneexampleisthestrategyTinyLFU [13].Insummary,
the strategy evaluates for a new cache candidate whether it should be added to
the cache at the expense of deleting an already cached candidate. For the prefix
caching we use an in-process TinyLFU cache for each node in the cluster. Thus,
for each node in the Kafka cluster, an independent cache is maintained.
Alternatively, a distributed cache can be used. However, this requires that
the prefix-alignments, including the whole search status, have to be serialized
and transported over the network. This introduces a significant overhead due to
network latency and high computational cost for serialization.
390 D. Schuster and G. J. Kolhof
Fig.8.Visualizationofatime-compressedeventstream.Eachdotrepresentsanevent.
Theoriginaleventstreamiscompressedby50%s.t.thatrelativetimedistancebetween
the events is remained w.r.t. the original event stream
5 Experimental Evaluation
In this section, we present an experimental evaluation of the proposed imple-
mentationandthetwopresentedextensions.First,wedescribetheexperimental
setup. Subsequently, we discuss the results.
Table 2. Specifications of the hardware used in the experimental setup
Node Memory CPU
1–2 128 GB RAM 2x Xeon 5115 Gold @ 2.40GHz base
3–5 512 GB RAM 2x Xeon 5115 Gold @ 2.40GHz base
Fig.9. Average computation time (ms) per trace
5.1 Experimental Setup
In the conducted experiments, we use publicly available, real-life event logs [10–
12,17]. Fromtheevent logs, we generateanevent stream byemitting theevents
accordingtotheirtimestamps.Sincetheusedeventlogscoveralargetimespan,
we apply time-compression as visualized in Fig.8. Moreover, we discovered a
Scalable Online Conformance Checking 391
reference model for each event log with the Inductive Miner [16]. We conducted
experiments for the two presented extensions presented in Sect.4:
1. PL: plain version (Sect.4.1 and Sect.4.2)
2. DS: extension direct synchronizing (Sect.4.3)
3. CA: extension prefix caching (Sect.4.4)
4. DSC: both extensions, i.e., direct synchronizing and prefix caching
We use a five node Kafka cluster with each broker running on a separate physi-
cal machine. Consider Table2 for detailed specifications. In cases where we use
prefix-caching, we set the cache size to 100 prefixes per instance.
5.2 Results
Consider Fig.9 showing the average computation time per trace for the four
different versions. For all tested event logs, we observe that applying the two
proposed extensions, i.e., DSC, leads to a significant speed-up of the calculation
compared to the original PL version. Moreover, we observe synergetic effects of
bothproposedextensions,i.e.,DSandCAcomparedtoDSC,foralllogsexcept
for BPI Ch. 2017. Interestingly, we observe that for the BPI Ch. 2017 log, the
extension DS performs worse than PL. This can be explained by the fact that
for this event log, direct synchronization could be applied only in a few cases.
Hence, the additional check, i.e., line 1–6 in Algorithm 1, is causing the higher
computation time. However, for the other tested event logs, both extensions
significantly reduce the calculation time.
Fig.10. Distribution of consumer lag count for the four algorithm-variants using the
BPI Ch. 2020 domestic log [12] with a replay time of 10min
InFig.10,weshowthedistributionoftheconsumerlagforthefourvariants.
We observe that the extensions, especially DSC including both extensions, lead
to a significant improvement of the consumer lag, i.e., number of queued states
for consumption (reconsider Fig.10). Note that we replay the log in only 10min
whereastheoriginallogspans890days,i.e.,thetimedifferencebetweentheear-
liest and latest event in the event log. Regarding the consumer lag distribution,
we observe similar results for the other tested event logs.
In Figs.11, 12, 13 and 14 we depict the flow of messages in and messages
consumed forthedifferenteventlogs.Pereventlog,wecomparethePLversion,
which serves as a baseline, and the DSC version, which includes both proposed
392 D. Schuster and G. J. Kolhof
Fig.11. Messages in vs. messages consumed for RT event log
Fig.12. Messages in vs. messages consumed for BPI Ch. 2019 event log
Fig.13. Messages in vs. messages consumed for BPI Ch. 2020 event log