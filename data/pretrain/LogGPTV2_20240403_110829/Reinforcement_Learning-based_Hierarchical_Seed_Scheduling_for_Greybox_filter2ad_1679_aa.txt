title:Reinforcement Learning-based Hierarchical Seed Scheduling for Greybox
Fuzzing
author:Jinghan Wang and
Chengyu Song and
Heng Yin
Reinforcement Learning-based Hierarchical Seed
Scheduling for Greybox Fuzzing
Jinghan Wang, Chengyu Song, Heng Yin
University of California, Riverside
PI:EMAIL, {csong, heng}@cs.ucr.edu
Abstract—Coverage  metrics  play  an  essential  role  in  greybox 
fuzzing.  Recent  work  has  shown  that  ﬁne-grained  coverage 
metrics could allow a fuzzer to detect bugs that cannot be covered 
by  traditional  edge  coverage.  However,  ﬁne-grained  coverage 
metrics  will  also  select  more  seeds,  which  cannot  be  efﬁciently 
scheduled by existing algorithms. This work addresses this prob-
lem by introducing a new concept of multi-level coverage metric 
and the corresponding reinforcement-learning-based hierarchical 
scheduler.  Evaluation  of  our  prototype  on  DARPA  CGC  showed 
that our approach outperforms AFL and AFLFAST signiﬁcantly: it 
can detect 20% more bugs, achieve higher coverage on 83 out of 
180  challenges,  and  achieve  the  same  coverage  on  60  challenges. 
More  importantly,  it  can  detect  the  same  number  of  bugs  and 
achieve  the  same  coverage  faster.  On  FuzzBench,  our  approach 
achieves  higher  coverage  than  AFL++  (Qemu)  on  10  out  of  20 
projects.
I.
INTRODUCTION
Greybox fuzzing is a state-of-the-art testing technique that 
has been widely adopted by the industry and has successfully 
found  tens  of  thousands  of  vulnerabilities  in  widely  used 
software.  For  example,  the  OSS-Fuzz  [20]  project  has  found 
more  than  10,000  bugs  in  popular  open-sourced  projects  like 
OpenSSL  since  its  launch  in  December  2016.
As  shown  in  Figure  1,  greybox  fuzzing  can  be  modeled 
as  a  genetic  process  where  new  inputs  are  generated  through 
mutation  and  crossover/splice.  The  generated  inputs  are  se-
lected according to a ﬁtness function. Selected inputs are then 
added back to the seed pool for future mutation. Unlike natural 
evolution, due to the limited processing capability, only a few 
inputs  from  the  seed  pool  will  be  scheduled  to  generate  the 
next batch of inputs. For example, a single fuzzer instance can 
only  schedule  one  seed  at  a  time.
The  most  common  ﬁtness  function  used  by  off-the-shelf 
fuzzers like American Fuzzy Lop (AFL) [55] is edge coverage, 
i.e.,  inputs  that  cover  new  branch(es)  will  be  added  to  the
seed  pool,  as  its  goal  is  to  achieve  higher  edge  coverage  of
the  code.  While  most  fuzzers  are  coverage-guided  (i.e.,  use
new  coverage  as  the  ﬁtness  function),  recent  research  has
shown  that  the  genetic  process  can  also  be  used  to  discover
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2021
21-25  February  2021, Virtual 
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24486
www.ndss-symposium.org
Fig. 1: Overview of greybox fuzzing
a diversity of program properties by using a variety of ﬁtness
functions [26], [32], [34].
An important property of a ﬁtness function (e.g., a coverage
metric) is its ability to preserve intermediate waypoints [32].
To better illustrate this, consider ﬂipping a magic number
check a = 0xdeadbeef as an example. If a fuzzer only
considers edge coverage, then the probability of generating
the correct a with random mutations is 232. However, if the
fuzzer can preserve important waypoints, e.g., by breaking
the 32-bit magic number into four 8-bit number [25], then
solving this checking will be much more efﬁcient since the
answer can be generated from a sequence as 0xef, 0xbeef,
0xadbeef, and 0xdeadbeef. This check can also be solved
faster by understanding distances between current value of a
and the target value [12]–[14], [18], [42]. More importantly,
recent research has shown that many program states cannot be
reached without saving critical waypoints [30], [45].
Wang et al. [45] formalize the ability to preserve inter-
mediate waypoints as the sensitivity of a coverage metric.
Conceptually, a more sensitive metric would lead to more
program states (e.g., code coverage). However, the empirical
evaluation of [45] shows that this is not always the case. The
reason is that, a more sensitive coverage metric will select
more seeds, which could cause seed explosion and exceed the
fuzzer’s ability to schedule. As a result, many seeds may never
be scheduled or be scheduled without enough time/power to
make a breakthrough [9].
In this work, we aim to address the seed explosion prob-
lem with a hierarchical scheduler. Speciﬁcally, fuzzing can
be modeled as a multi-armed bandit (MAB) problem [49],
where the scheduler needs to balance between exploration and
exploitation. With a more sensitive coverage metric like branch
distance, exploitation can be considered as focusing on solving
a hard branch (e.g., magic number check), and exploration can
Test	CaseSeedSeedSchedulingSeed	MutationSeed	SelectionInstrumentedProgramSeed	PoolNewSeedinteresting!FitnessFunctionbe considered as exercising an entirely different function. Our
crucial observation is that when a coverage metric Cj is more
sensitive than Ci, we can use Cj to save all the intermediate
waypoints without losing the ability to discover more program
states; but at the same time, we can use Ci to cluster seeds
into a representative node and schedule at node level to achieve
better exploration. More speciﬁcally, the scheduler will choose
a node ﬁrst, and then choose a seed in that node. Based on
this observation, we propose to organize the seed pool as a
multi-level tree where leaf nodes are real seeds and internal
nodes are less sensitive coverage measurements. The closer
a node is to the leaf, the more sensitive the corresponding
coverage measurement is. Then we can utilize the existing
MAB algorithms to further balance between exploitation and
exploration.
To validate our idea, we implemented two prototypes: one
AFL-HIER based on AFL and the other AFL++-HIER based
on AFL++. We performed extensive evaluation on the DARPA
Cyber Grand Challenge (CGC) dataset
[10] and Google
FuzzBench [21] benchmarks. Compared to AFLFAST [9],
AFL-HIER can ﬁnd more bugs in CGC (77 vs. 61). AFL-HIER
also achieved better coverage in about 83 of 180 challenges
and the same coverage on 60 challenges. More importantly,
AFL-HIER can ﬁnd the same amount of bugs and achieve the
same coverage faster than AFLFAST. On FuzzBench, AFL++-
HIER achieved higher coverage on 10 out of 20 projects than
AFL++ (Qemu).
support
multi-armed bandits model.
Contributions. This paper makes the following contributions:
• We propose multi-level coverage metrics that bring a
novel approach to incorporate sensitive coverage metrics
in greybox fuzzing.
• We design a hierarchical seed scheduling algorithm to
the multi-level coverage metric based on the
• We implement our approach as an extension to AFL and
AFL++ and release the source code at https://github.com/
bitsecurerlab/aﬂplusplus-hier.
• We evaluate our prototypes on DARPA CGC and Google
FuzzBench. The results show that our approach not only
can trigger more bugs and achieve higher code coverage,
but also can achieve the same coverage faster than existing
approaches.
II. BACKGROUND
A. Greybox Fuzzing
Algorithm 1 illustrates the greybox fuzzing process in more
detail. Given a program to fuzz and a set of initial seeds, the
fuzzing process consists of a sequence of loops named rounds.
Each round starts with selecting the next seed for fuzzing from
the pool according to the scheduling criteria. The scheduled
seed is assigned to a certain amount of power that determines
how many new test cases will be generated in this round.
Next, test cases are generated through (random) mutation and
crossover based on the scheduled seed. Compared to blackbox
and whitebox fuzzing, the most distinctive step of greybox
fuzzing is that, when executing a newly generated input, the
fuzzer uses lightweight instrumentations to capture runtime
features and expose them to the ﬁtness function to measure the
“quality” of a generated test case. Test cases with good quality
will then be saved as a new seed into the seed pool. This step
allows a greybox to gradually evolve towards a target (e.g.,
more coverage). The effectiveness and efﬁciency of greybox
fuzzing depend on the following factors.
Algorithm 1: Greybox Fuzzing Algorithm
Input: target program P , set of initial seeds S0
Output: unique seed set S∗,
bug-triggering seed set Sv
Data: seed s and test case I
s ← SelectNextSeedToFuzz(S∗)
s.power ← AssignPower()
while s.power > 0 do
s.power ← s.power − 1
I ← MutateSeed(s)
status ← RunAndEval(I)
if status is Bug then
else if status is NewCovExplored then
Sv ← Sv ∪ {I}
S∗ ← S∗ ∪ {I}
S∗ ← S0
Sv ← ∅
while true do
1 Function Main(P , S0):
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21 End
end
PayReward(s)
else
end
end
continue // drop I
Test case measurement. As a genetic process, the ﬁtness
function of the fuzzer decides what kind of program prop-
erties the fuzzer can discover [32]. While fuzzing has been
successfully applied to many different domains in recent years
with different ﬁtness functions, the most popular one is still
code coverage (i.e., a test case that triggers new coverage will
be saved as a new seed). However, coverage measurements
can be diverse. Notably, AFL [55] measures the edge coverage
of test cases. More precisely, it maintains a global map where
the hashed value of an edge (i.e., the pair of the current basic
block address and the next basic block address) is used as an
indexing key to access the hit_count of the edge, which
records how many times it has been taken so far. The hit counts
are bucketized into small powers of two. After a test case
completes its execution, the global hit_count map will be
updated according to its edges, and it will be selected as a
new seed if a new edge is found or the hit count of one edge
increases into a new bucket. As we can see, this measurement
does not consider the order of edges and can miss interesting
seeds is the hash of a new edge collides with the hash of an
already covered edge [19].
Seed scheduling criteria. The limited processing capability
makes it essential
to prioritize some seeds over others in
order to maximize the coverage. For example, AFL [55]
prefers seeds with small sizes and short execution time to
achieve a higher fuzzing throughput. Furthermore, it maintains
2
a minimum set of seeds that stress all the code coverage
so far, and focus on fuzzing them (i.e., prefers exploitation).
AFLFAST [9] models greybox fuzzing as a Markov chain and
prefers seeds exercising paths that are rarely exercised, as
high-frequency paths tend to be covered by invalid test cases.
LIBFUZZER [39] prefers seeds generated later in a fuzzing
campaign. Entropic [7] prefers seeds with higher information
gains.
bandit algorithms that perform impressively. Speciﬁcally, they
construct a conﬁdence interval to estimate each arm’s true
reward, and select the arm with the highest UCB each time.
Notably, the conﬁdence interval is designed to shrink when the
arm with its reward is sampled more. As a result, while the
algorithm tends to select arms with high average rewards, it
will periodically try less explored arms since their estimated
rewards have wider conﬁdence intervals.
Seed mutation strategy. The mutation strategy decides how
likely a new test case could trigger new coverage(s) and be
selected as a new seed. Off-the-shelf fuzzers like AFL and
LIBFUZZER use random mutation and crossover. Recent work
aims to improve the likelihood by using data-ﬂow analysis
to identify which input bytes should be mutated [18], [37],
[52], by using directed searching [12], [13], [40], [42], and by
learning the best mutation strategies [11], [29].
Fuzzing throughput. Fuzzing throughput is another critical
factor that decides how fast a fuzzer can discover new cov-
erage. AFL [55] uses the fork server and persistent mode to
reduce initialization overhead, thus improving the throughput.
Xu et al. [51] proposed new OS primitives to improve fuzzing
throughput further. FirmAFL [57] uses augmented emulation
to speed-up fuzzing ﬁrmware. Because high throughput
is
the key factor that allows greybox fuzzers to beat whitebox
fuzzers in practice, one must pay special attention to the
trade-off between throughput and the above three factors
(coverage measurement, scheduling algorithm, and mutation
strategy). That is, improvements of the above three factors
at the cost of throughput may unexpectedly result in worse
fuzzing performance.
B. Multi-Armed Bandit Model
The multi-armed bandit model offers a fundamental frame-
work for algorithms that
learn optimal resource allocation
policies over time under uncertainty. The term “bandit” comes
from a gambling scenario where the player faces a row of slot
machines (also known as one-armed bandits) yielding random
payoffs and seeks the best strategy of playing these machines
to gain the highest long-term payoffs.
In the basic formulation, a multi-armed bandit problem is
deﬁned as a tuple (A,R), where A is a known set of K arms
(or actions) and Ra(r) = P[r|a] is an unknown but ﬁxed
probability distribution over rewards. At each time step t the
agent selects an arm at, and observes a reward rt ∼ Rat. The
objective is to maximize the cumulative rewards(cid:80)T
t=1 rt.
Initially, the agent has no information about which arm is
expected to have the highest reward, so it tries some randomly
and observes the rewards. Then the agent has more information
than before. However, it has to face the trade-off between
“exploitation” of the arm that is with the highest expected
reward so far, and “exploration” to obtain more information
about the expected rewards of the other arms so that it does
not miss out on a valuable one by simply not trying it enough
times.
Various algorithms are proposed to make the optimal
trade-off between exploitation and exploration of arms. Upper
Conﬁdence Bound (UCB) algorithms [5] are a family of
3
na
√
maximizes Q(a) + C ×(cid:113) log(N )
Take UCB1 [2], which is almost the most fundamental one,
as an example. It starts with selecting each arm once to obtain
an initial reward. Then at each time step, it selects arm a that
where Q(a) is the average
reward obtained from arm a, C is a predeﬁned constant that is
usually set to
2, N is the overall number of selections done
so far, and na is the number of times arm a has been selected.
Seed scheduling can be modeled as a multi-armed bandit
problem where seeds are regarded as arms [49], [54]. However,
to make the fuzzer beneﬁt from this model, such as maximizing
the code coverage, we need to design the reward of scheduling
a seed carefully.
III. MULTI-LEVEL COVERAGE METRICS
In this section, we discuss what are multi-level coverage
metrics and why they are useful for greybox fuzzing.
A. Sensitivity of Coverage Metrics
Given a mutation-based greybox fuzzer, a fuzzing cam-
paign starts with a set of initial seeds. As the fuzzing goes
on, more seeds are added into the seed pool through mutating
the existing seeds. By tracking the evolution of the seed pool,
we can see how each seed can be traced back to an initial
seed via a mutation chain, in which each seed is generated
from mutating its immediate predecessor. If we consider a bug
triggering test case as the end of a chain and the corresponding
initial seed as the start, those internal seeds between them
serve as waypoints that allow the fuzzer to gradually reduce
the search space to ﬁnd the bug [32].
The coverage metric used by a fuzzer plays a vital role
in creating such chains, from two main aspects. First, if the
chain terminates earlier before reaching the bug triggering test
case, then the bug may never be discovered by the fuzzer.
Wang et al. [45] formally model this ability to preserve critical