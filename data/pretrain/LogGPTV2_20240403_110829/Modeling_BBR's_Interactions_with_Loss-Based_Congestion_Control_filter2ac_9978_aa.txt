# Title: Modeling BBR's Interactions with Loss-Based Congestion Control

## Authors:
- Ranysha Ware
- Matthew K. Mukerjee
- Srinivasan Seshan
- Justine Sherry

### Affiliations:
- Carnegie Mellon University
- Nefeli Networks
- Carnegie Mellon University
- Carnegie Mellon University

## Abstract
BBR is a new congestion control algorithm (CCA) deployed in Chromium QUIC and the Linux kernel. As the default CCA for YouTube, which accounts for over 11% of Internet traffic, BBR has become a significant player in Internet congestion control. Recent scrutiny has highlighted BBR's fairness or friendliness to other connections, with multiple research groups reporting undesirable outcomes when BBR competes with traditional CCAs. One such outcome is that a single BBR flow consumes a fixed 40% of link capacity when competing with up to 16 loss-based algorithms like Cubic or Reno. This short paper presents the first model capturing BBR's behavior in competition with loss-based CCAs. Our model, validated through practical experiments, shows that under competition, BBR becomes window-limited by its 'in-flight cap', which determines BBR's bandwidth consumption. By modeling the value of BBR’s in-flight cap under varying network conditions, we can predict BBR’s throughput with a median error of 5% against Cubic and 8% against Reno.

## 1. Introduction
In 2016, Google introduced a new congestion control algorithm called BBR [4, 5]. Now the default CCA for Google services, including YouTube, which commands 11% of US Internet traffic, BBR significantly impacts a large fraction of Internet connections. This paper focuses on BBR’s behavior—specifically, its fairness or friendliness—when competing with legacy, loss-based CCAs such as Reno and Cubic.

Previous studies have observed two key phenomena. First, in shallow-buffered networks, BBR's bandwidth probing phase causes buffer overflows and bursty loss for competing flows, leading to Cubic and Reno nearly starving for bandwidth. This issue is expected to be addressed in BBRv2 [7, 11].

In residential capacity links (e.g., 10-100 Mbps) with deep buffers, studies [4, 9, 14, 16, 17] have generated conflicting reports on how BBR shares bandwidth with competing Cubic and Reno flows. We [17] and others [9, 14] observed a single BBR flow consuming a fixed 35-40% of link capacity when competing with up to 16 Cubic flows. These findings contradict early presentations on BBR [4], which suggested BBR was generous to competing Cubic flows. The current state of affairs is confusing, with no clear explanation for the empirically observed behaviors.

This paper models BBR’s behavior when it competes with traditional, loss-based congestion control algorithms in residential, deep-buffered networks. The key insight is that while BBR is rate-based when running alone, it degrades to window-based transmission under competition. BBR’s window is set to a maximum 'in-flight cap' calculated as \(2 \times \text{RTT}_{\text{est}} \times \text{Btlbw}_{\text{est}}\), where \(\text{RTT}_{\text{est}}\) and \(\text{Btlbw}_{\text{est}}\) are BBR’s estimates of the baseline RTT and its share of bandwidth.

The original BBR publication presented the in-flight cap as a safety mechanism to handle delayed ACKs [5]. However, this mechanism is the key factor controlling BBR’s share of link capacity under competition. Our model focuses on how BBR estimates its in-flight cap under different network conditions, allowing us to predict BBR’s share of link capacity for long-lived flows. The in-flight cap is influenced by several parameters, but notably, the number of competing loss-based (Cubic or Reno) flows does not affect it. Hence, BBR’s sending rate is not influenced by the number of competing traditional flows, explaining why BBR is considered 'unfair' to Cubic and Reno in multi-flow settings [9, 17].

In the following sections, we discuss our testbed in §2 and early measurements of BBR’s fairness in §3. We provide a primer on the BBR algorithm in §4, develop our analysis of BBR in §5, connect our results to related work in §6, and conclude in §7.

## 2. Testbed
Throughout this paper, we present experiments conducted in the testbed illustrated in Figure 1a. Each experiment involves three servers: a server/sender, a BESS [10] software switch, and a client/receiver. All servers run Linux 4.13 (with internal TCP pacing), have Intel E5-2660V3 processors, and dual-port Intel X520 10Gb NICs. Senders and receivers use iPerf 3 [1] to generate and receive traffic. Within BESS, traffic is serviced at a configurable rate below the link capacity to introduce queueing. The queue size is set relative to BDP, rounded to the nearest power-of-two. To configure delay, we hold all ACKs for a configurable amount of time. Unless otherwise noted, we set the bandwidth to 10 Mbps and RTT to 40 ms, following Google’s parameters in IETF presentations [4, 6].

## 3. BBR in Competition
A critical concern when deploying a new CCA on the Internet is how it will interact with existing algorithms. Will the new CCA be fair to existing CCAs, or starve them? An early BBR presentation [4] provided some insights, showing a graph measuring one BBR flow versus one Cubic flow over four minutes, illustrating a correlation between the bottleneck queue size and BBR’s bandwidth consumption.

We replicated Google’s experiments, as shown in Figure 1b, and other studies [14] did the same. These graphs suggest that BBR is generous to existing CCAs in typical buffer-bloated networks, especially to Cubic. Subsequent studies [9, 14, 17] questioned both the results and the implication of generosity. Some data [17] showed that BBR converged to different rates—around 40% of the link capacity for queue sizes up to 32×BDP, matching the Reno graph but not the Cubic graph. Figures 3 and 2 show that this discrepancy is due to differing experimental conditions and the time it takes for BBR to converge to its steady-state share of link capacity. BBR quickly matches Reno’s queue occupancy, but takes longer to scale up when competing with Cubic (Figure 3). The average goodput depends on the measurement duration (Figure 2b), and convergence can take several minutes in very deep buffered networks (Figure 2b).

Another set of experiments [9, 17] suggests that BBR may consume more than its fair share of link capacity. Figure 1c shows the goodput over time of BBR versus 16 Cubic flows in a 40 ms × 10 Mbps scenario. BBR consumes an outsized share of bandwidth, leaving just over half to be shared by the sixteen other connections. Relying solely on empirical studies leaves us with an incomplete understanding of BBR’s characteristics. To fully understand BBR’s behavior and predict its performance in unobserved scenarios, we turn to modeling in the rest of this paper.

## 4. BBR Primer
BBR is designed as a rate-based algorithm. It maintains two key variables: \(\text{Btlbw}_{\text{est}}\), BBR’s estimate of the available throughput, and \(\text{RTT}_{\text{est}}\), BBR’s estimate of the baseline round-trip time. BBR paces packets at the \(\text{Btlbw}_{\text{est}}\) rate. Assuming no queueing and instantaneous ACKs, BBR should never have more than \(\text{Btlbw}_{\text{est}} \times \text{RTT}_{\text{est}}\) unacknowledged packets outstanding.

As a failsafe and to keep the pipe full in networks that delay or aggregate ACKs, BBR implementations impose an 'in-flight cap'—it will never allow more than \(2 \times \text{Btlbw}_{\text{est}} \times \text{RTT}_{\text{est}}\) unacknowledged packets. This cap is the central parameter controlling BBR’s link utilization in competition with Cubic and Reno.

To estimate \(\text{Btlbw}_{\text{est}}\) and \(\text{RTT}_{\text{est}}\), BBR cycles through a simple state machine (Figure 4). BBR sends at a fixed rate \(\text{BWest}\). It sets its initial rate using a version of 'slow start' and then 'probes for bandwidth' (ProbeBW in Figure 4) every 8 RTTs. During this stage, BBR inflates the rate to \(1.25 \times \text{Btlbw}_{\text{est}}\) and observes the achieved throughput during that interval. BBR then updates \(\text{Btlbw}_{\text{est}}\) and \(\text{RTT}_{\text{est}}\) based on the observed throughput and RTT.