vertised MTU show the BIND default of 4096 B (94%), but some
other MTUs also occur, notably 512 B (3.1%), 2048 B (1.6%) and
1280 B (0.3%).
The prevalence of DNSSEC-enabled resolvers does not mean
transition to broad use of DNSSEC will prove painless, however.
For EDNS sessions with an advertised MTU of ≥ 1800 B, 13%
failed to fetch the large EDNS-enabled reply and 1.9% for the
medium-sized one. This ﬁnding suggests a common failure where
the DNS resolver is connected through a network that either won’t
carry fragmented UDP trafﬁc or assumes that DNS replies never ex-
ceed 1500 B (since edns_medium is unlikely to be fragmented).
Since DNSSEC replies will likely exceed 1500 B, the prevalence
of this problem suggests a potentially serious deployment issue that
will require changes to the resolver logic.
The RELEASE data includes a full validation of DNS MTU up to
4 KB. We ﬁnd that despite not advertising a large MTU, almost all
sessions (95%) used a resolver capable of receiving messages over
512 B. However, a signiﬁcant number of sessions (15%) exhibited
a measured DNS MTU of 1472 B (equivalent to an IP MTU of
1500 B), suggesting an inability to receive fragmented trafﬁc. This
even occurred for 11% of sessions that explicitly advertised an ex-
plicit EDNS MTU > 1472 B. This can cause unpredictable time-
outs and failures if DNS replies (particularly the potentially large
records involved in DNSSEC) exceed the actual 1472 B MTU.
A similar problem exists in the clients themselves, but of-
ten due to a different cause. When the client directly requests
edns_large, edns_medium, and edns_small from the
server, 14.1%/4.3%/1.3% failed, respectively. This suggests two
additional difﬁculties: network devices assuming DNS replies do
not exceed 512 B (both edns_large and edns_medium fail)
or networks that do not handle EDNS at all (all three fail).10 We
ﬁnd this high failure rate quite problematic, as sound DNSSEC val-
idation requires implementation on the end host’s stub resolver to
achieve end-to-end security, which requires that end hosts can re-
ceive large, EDNS-enabled DNS messages.
Another concern comes from the continued lack of DNS port
randomization [5]. This widely publicized vulnerability was over a
year old when we ﬁrst released Netalyzr, but 5% of sessions used
monotone or ﬁxed ports in DNS requests, However, no major ISP
showed signiﬁcant problems with this test.
In terms of DNS performance, it appears that DNS resolvers may
constitute a bottleneck for many users. 9% of the sessions required
300 ms more time to look up a name within our domain versus the
base round-trip time to our server, and 4.6% required more than
600 ms. (We can attribute up to 100 ms of the increase to the fact
that our DNS server resides within our own institution, while the
back-end servers are hosted at Amazon EC2’s East Coast location.)
9 32% of sessions exhibit BIND’s default handling of glue,
CNAMEs, 0x20, EDNS, and DNSSEC.
10 The failures we observe could instead be due to heavy packet
loss. However such failures should not particularly favor one type
of query over another, yet we observe only 0.09% of sessions for
which edns_medium succeeded while edns_small failed.
255DOMAIN
www.nationwide.
co.uk
ad.doubleclick.net
www.citibank.com
windowsupdate.
microsoft.com
www.microsoft.com
mail.yahoo.com
mail.google.com
www.paypal.com
www.google.com
www.meebo.com
ALL LOOKUPS (%)
FAILED BLOCKED
OPENDNS (%)
FAILED CHANGED
2.3
1.6
1.3
0.8
0.8
0.7
0.4
0.4
0.3
0.4
 10 KB. Manual examination
of a few cases veriﬁed that the applet received a proper HTTP re-
sponse for the image with a reduced Content-Length header,
and thus the network did indeed change the image (presumably to
save bandwidth by re-encoding the .jpg with a higher loss rate)
rather than merely truncate the request.
In-path processes also only rarely interrupt ﬁle transfers. Only
0.7% of all sessions failed to correctly fetch the .mp3 ﬁle and 0.9%
for the .exe. Slightly more, 1.2%, failed to fetch the .torrent
ﬁle, suggesting that some networks ﬁlter on ﬁle type. However,
10% ﬁltered the EICAR test “virus”, suggesting signiﬁcant deploy-
ment of either in-network or host-based AV. As only 0.36% failed
to fetch all four test-ﬁles, these results do not reﬂect proxies that
block all of the tests.
5.6
ISP Proﬁles
Table 3 illustrates some of the policies that Netalyzr observed
for the 15 most common ISPs. We already discussed the relative
lack of SMTP blocking above. A few ISPs do not appear to ﬁlter
Windows trafﬁc; however, they might block these ports inbound,
which we cannot determine since Netalyzr does not perform in-
bound scanning.
Another characteristic we see reﬂects early design decisions still
in place today: many DSL providers initially offered PPPoE con-
nections rather than IP over Ethernet, while DOCSIS-based cable-
modems always used IP-over-Ethernet. For Verizon, only 9% of
Verizon customers whose reverse name suggests FiOS (ﬁber to the
home) manifest the PPPoE MTU, while 68% of the others do.
A ﬁnal trend concerns the growth of NXDOMAIN wildcard-
ing, especially ISPs wildcarding all names rather than just www
names. During Netalyzr’s initial release, Comcast had yet to im-
plement NXDOMAIN wildcarding, but began wildcarding during
Fall 2009.
We also conﬁrmed that the observed policies for Comcast match
their stated policies. Comcast has publicly stated that they will
block outbound trafﬁc on the Windows ports, and may block out-
bound SMTP with dynamic techniques [6]. When they began
widespread deployment of their wildcarding, they also stated that
they would only wildcard www addresses, but we did observe the
results of an early test deployment that wildcarded all addresses for
a short period of time.
6. RELATED WORK
There is a substantial existing body of work on approaches for
measuring various aspects of the Internet. Here we focus on those
related to our study in the nature of the measurements conducted or
how data collection occurred.
Network performance. Dischinger et al. studied network-level
performance characteristics, including link capacities, latencies, jit-
ter, loss, and packet queue management [8]. They used measure-
ment packet trains similar to ours, but picked the client machines
by scanning ISP address ranges for responding hosts, subsequently
probing 1,894 such hosts autonomously. In 2002 Saroiu et al. stud-
ied similar access link properties as well as P2P-speciﬁc aspects of
17,000 Napster ﬁle-sharing nodes and 7,000 Gnutella peers [22].
They identiﬁed probe targets by crawling the P2P overlays, and
identiﬁed a large diversity in bandwidth (only 35% of hosts ex-
ceeded an upload bandwidth of 100Kb/s, 8% exceeded 10Mbps,
between 8% and 25% used dial-up modems, and at least 30% had
more than 3Mb/s downstream bandwidth) and latency (the fastest
20% of hosts exhibited latencies under 70ms, the slowest 20% ex-
ceeded 280ms). Maier et al. analyzed residential broadband traf-
ﬁc of a major European ISP [15], ﬁnding that round-trip laten-
cies between users and the ISP’s border gateway often exceed that
between the gateway and the remote destination (due to DSL in-
terleaving), and that most of the observed DSL lines used only a
small fraction of the available bandwidth. Ritacco et al. [21] de-
veloped a network testsuite that like Netalyzr is driven by a Java
applet. Their work is intended as an exploratory study, focusing
more on performance in general and wireless networks and their
device populations in particular. (While numerous techniques have
been developed for measuring network performance, we do not dis-
cuss these further in keeping with our main focus on ways that users
have their connectivity restricted or shaped, rather than end-to-end
performance.)
Network neutrality. Several studies have looked at the degree
to which network operators provide different service to different
types of trafﬁc. Dischinger et al. studied 47,000 sessions conducted
using a downloadable tool, ﬁnding that around 8% of the users ex-
perienced BitTorrent blocking [9]. Bin Tariq et al. devised NANO,
a distributed measurement platform, to detect whether a given ISP
induces degraded performance for speciﬁc classes of service [24].
They evaluate their system in Emulab, using Click conﬁgurations to
synthesize “ISP” discrimination, and source synthetic trafﬁc from
PlanetLab nodes. Beverly et al. leveraged the “referral” feature of
Gnutella to conduct TCP port reachability tests from 72,000 unique
Gnutella clients, ﬁnding that Microsoft’s network ﬁlesharing ports