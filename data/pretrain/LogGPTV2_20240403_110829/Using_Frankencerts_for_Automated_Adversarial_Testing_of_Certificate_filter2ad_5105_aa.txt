title:Using Frankencerts for Automated Adversarial Testing of Certificate
Validation in SSL/TLS Implementations
author:Chad Brubaker and
Suman Jana and
Baishakhi Ray and
Sarfraz Khurshid and
Vitaly Shmatikov
2014 IEEE Symposium on Security and Privacy
Using Frankencerts for Automated Adversarial
Testing of Certiﬁcate Validation
in SSL/TLS Implementations
Chad Brubaker ∗ †
Suman Jana†
Sarfraz Khurshid†
Vitaly Shmatikov†
Baishakhi Ray‡
∗Google
†The University of Texas at Austin
‡University of California, Davis
Abstract—Modern network security rests on the Secure Sock-
ets Layer (SSL) and Transport Layer Security (TLS) protocols.
Distributed systems, mobile and desktop applications, embedded
devices, and all of secure Web rely on SSL/TLS for protection
against network attacks. This protection critically depends on
whether SSL/TLS clients correctly validate X.509 certiﬁcates
presented by servers during the SSL/TLS handshake protocol.
We design, implement, and apply the ﬁrst methodology for
large-scale testing of certiﬁcate validation logic in SSL/TLS
implementations. Our ﬁrst ingredient is “frankencerts,” synthetic
certiﬁcates that are randomly mutated from parts of real cer-
tiﬁcates and thus include unusual combinations of extensions
and constraints. Our second ingredient is differential testing: if
one SSL/TLS implementation accepts a certiﬁcate while another
rejects the same certiﬁcate, we use the discrepancy as an oracle
for ﬁnding ﬂaws in individual implementations.
Differential testing with frankencerts uncovered 208 dis-
crepancies between popular SSL/TLS implementations such as
OpenSSL, NSS, CyaSSL, GnuTLS, PolarSSL, MatrixSSL, etc.
Many of them are caused by serious security vulnerabilities. For
example, any server with a valid X.509 version 1 certiﬁcate can act
as a rogue certiﬁcate authority and issue fake certiﬁcates for any
domain, enabling man-in-the-middle attacks against MatrixSSL
and GnuTLS. Several
implementations also accept certiﬁcate
authorities created by unauthorized issuers, as well as certiﬁcates
not intended for server authentication.
We also found serious vulnerabilities in how users are warned
about certiﬁcate validation errors. When presented with an
expired, self-signed certiﬁcate, NSS, Safari, and Chrome (on
Linux) report that the certiﬁcate has expired—a low-risk, often
ignored error—but not that the connection is insecure against a
man-in-the-middle attack.
These results demonstrate that automated adversarial testing
with frankencerts is a powerful methodology for discovering
security ﬂaws in SSL/TLS implementations.
I.
INTRODUCTION
Secure Sockets Layer (SSL) and its descendant Transport
Layer Security (TLS) protocols are the cornerstone of Internet
security. They are the basis of HTTPS and are pervasively
used by Web, mobile, enterprise, and embedded software to
provide end-to-end conﬁdentiality, integrity, and authentication
for communication over insecure networks.
SSL/TLS is a big, complex protocol, described semi-
formally in dozens of RFCs. Implementing it correctly is
a daunting task for an application programmer. Fortunately,
many open-source implementations of SSL/TLS are available
for developers who need to incorporate SSL/TLS into their
software: OpenSSL, NSS, GnuTLS, CyaSSL, PolarSSL, Ma-
trixSSL, cryptlib, and several others. Several Web browsers
include their own, proprietary implementations.
In this paper, we focus on server authentication, which
is the only protection against man-in-the-middle and other
server impersonation attacks, and thus essential for HTTPS
and virtually any other application of SSL/TLS. Server authen-
tication in SSL/TLS depends entirely on a single step in the
handshake protocol. As part of its “Server Hello” message,
the server presents an X.509 certiﬁcate with its public key.
The client must validate this certiﬁcate. Certiﬁcate validation
involves verifying the chain of trust consisting of one or
more certiﬁcate authorities, checking whether the certiﬁcate is
valid for establishing SSL/TLS keys, certiﬁcate validity dates,
various extensions, and many other checks.
Systematically testing correctness of the certiﬁcate val-
idation logic in SSL/TLS implementations is a formidable
challenge. We explain the two main hurdles below.
First problem: generating test inputs. The test inputs, i.e.,
X.509 certiﬁcates, are structurally complex data with intricate
semantic and syntactic constraints. The underlying input space
is huge with only a tiny fraction of the space consisting of
actual certiﬁcates. A simple automated technique, such as
random fuzzing, is unlikely to produce more than a handful of
useful inputs since a random string is overwhelmingly unlikely
to even be parsable as a certiﬁcate.
Some test certiﬁcates can be created manually, but writing
just a small suite of such complex inputs requires considerable
effort; manually creating a high-quality suite is simply infea-
sible. Furthermore, the testing must include “corner cases”:
certiﬁcates with unusual combinations of features and exten-
sions that do not occur in any currently existing certiﬁcate but
may be crafted by an attacker.
Second problem: interpreting the results of testing. Given a
test certiﬁcate and an SSL/TLS implementation, we can record
whether the certiﬁcate has been accepted or rejected, but that
does not answer the main question: is the implementation
correct,
if the
certiﬁcate is rejected, is the reason given for rejection correct?
Manually characterizing test certiﬁcates as valid or invalid
is the accepted certiﬁcate valid? And,
i.e.,
© 2014, Chad Brubaker. Under license to IEEE.
DOI 10.1109/SP.2014.15
114
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:03 UTC from IEEE Xplore.  Restrictions apply. 
and writing the corresponding assertions for analyzing the
outputs observed during testing does not scale. A naive ap-
proach to automate this characterization essentially requires
re-implementing certiﬁcate validation, which is impractical and
has high potential for bugs of its own. Interpreting the results
of large-scale testing requires an oracle for certiﬁcate validity.
Our contributions. We design, implement, and evaluate the
ﬁrst approach for systematically testing certiﬁcate validation
logic in SSL/TLS implementations. It solves both challenges:
(1) automatically generating test certiﬁcates, and (2) automat-
ically detecting when some of the implementations do not
validate these certiﬁcates correctly.
The ﬁrst step of our approach is adversarial input gen-
eration. By design, our generator synthesizes test certiﬁcates
that are syntactically well-formed but may violate many of the
complex constraints and internal dependencies that a valid cer-
tiﬁcate must satisfy. This enables us to test whether SSL/TLS
implementations check these constraints and dependencies.
To “seed” the generator, we built a corpus of 243,246
real SSL/TLS certiﬁcates by scanning the Internet. Our gen-
erator broke them down into parts, then generated over 8
million frankencerts by mutating random combinations of
these parts and artiﬁcial parts synthesized using the ASN.1
grammar for X.509. By construction, frankencerts are parsable
as certiﬁcates, yet may violate X.509 semantics. They include
unusual combinations of critical and non-critical extensions,
rare extension values, strange key usage constraints, odd
certiﬁcate authorities, etc. Testing SSL/TLS implementations
with frankencerts exercises code paths that rarely get executed
when validating normal certiﬁcates and helps elicit behaviors
that do not manifest during conventional testing.
Our second insight is that multiple, independent imple-
mentations of X.509 certiﬁcate validation—the very same
implementations that we are testing—can be used as an oracle
to detect ﬂaws in validation logic. For each frankencert, we
compare the answers produced by OpenSSL, NSS, GnuTLS,
CyaSSL, PolarSSL, MatrixSSL, OpenJDK, and Bouncy Castle.
These SSL/TLS libraries are supposed to implement the same
certiﬁcate validation algorithm and, therefore, should agree
on every certiﬁcate. Differences in the implementations of
functionality left unspeciﬁed by the X.509 standard may cause
a “benign” discrepancy, but most discrepancies mean that some
of the disagreeing SSL/TLS implementations are incorrect.
Our differential mutation testing of SSL/TLS implementa-
tions on 8,127,600 frankencerts uncovered 208 discrepancies
between the implementations, many of which are caused by
serious ﬂaws. For example, MatrixSSL silently accepts X.509
version 1 certiﬁcates, making all MatrixSSL-based applications
vulnerable to man-in-the-middle attacks: anyone with a valid
version 1 certiﬁcate can pretend to be an intermediate certiﬁ-
cate authority (CA), issue a fake certiﬁcate for any Internet
domain, and that certiﬁcate will be accepted by MatrixSSL.
In GnuTLS, our testing discovered a subtle bug in the
handling of X.509 version 1 certiﬁcates. Due to a mismatch
between two ﬂags, the code that intends to accept only locally
trusted version 1 root certiﬁcates is actually accepting any
version 1 CA certiﬁcate,
including fake ones from mali-
cious servers. This bug could not have been found without
frankencerts because it is not triggered by any real certiﬁcate
from our corpus (but, of course, a man-in-the-middle attacker
could craft a malicious certiﬁcate to exploit this vulnerability).
Many vulnerabilities are caused by incorrect or missing
checks on the restrictions that root CAs impose on lower-level
CAs. MatrixSSL does not check path length constraints. If
a restricted CA (e.g., a corporate CA whose authority only
extends to a particular enterprise) creates a new intermediate
CA, who then issues certiﬁcates for any Internet domain,
these certiﬁcates will be accepted by MatrixSSL. GnuTLS,
CyaSSL, and PolarSSL do not check key usage constraints. As
a consequence, an attacker who compromises the code signing
key of some company can use it to spoof that company’s
servers in TLS connections. Most of these ﬂaws could not
have been discovered without frankencerts because incorrect
validation logic is only triggered by certiﬁcates of a certain
form, not by “normal” certiﬁcates.
Even if an SSL/TLS implementation correctly rejects a
certiﬁcate,
the reason given to the user is very important
because Web browsers and other interactive applications often
allow the user to override the warning. For example, if the
warning is that
this may
indicate a lazy system administrator but does not imply that
the connection is insecure. Because the risk is low, the user
may click through the warning. If, on the other hand, the
certiﬁcate is not issued by a legitimate certiﬁcate authority,
this means that the server could have been impersonated and
the connection may be insecure.
the certiﬁcate expired yesterday,
Our differential testing uncovered serious vulnerabilities in
how SSL/TLS implementations report errors. When presented
with an expired, self-signed certiﬁcate, NSS reports that the
certiﬁcate has expired but not that the issuer is invalid. This
vulnerability found its way into Web browsers such as Chrome
on Linux and Safari. Since users tend to click through expired-
certiﬁcate warnings—and are advised to do so [1]—this ﬂaw
gives attackers an easily exploitable vector for man-in-the-
middle attacks against all users of these Web browsers.
In summary, adversarial test input generation and differen-
tial mutation testing on millions of “frankencerts” synthesized
from parts of real certiﬁcates is a powerful new technique
for uncovering deep semantic errors in the implementations
of SSL/TLS, the most important network security protocol.
II. RELATED WORK
A. Security of SSL/TLS implementations
We are not aware of any prior work on systematic, auto-
mated discovery of certiﬁcate validation vulnerabilities in the
implementations of SSL/TLS clients.
Moxie Marlinspike demonstrated several ﬂaws in the im-
plementations of SSL/TLS certiﬁcate validation [55, 56, 57],
including the lack of CA bit checking in Microsoft’s Cryp-
toAPI as of 2002 [54]. More recently, the same vulnerability
was discovered in the SSL implementation on Apple iOS [40].
Georgiev et al. carried out a study of certiﬁcate validation
vulnerabilities caused by the incorrect use of SSL/TLS APIs,
as opposed to ﬂaws in the implementations of these APIs [31].
Georgiev et al. focus primarily on the incorrect validation
115
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:03 UTC from IEEE Xplore.  Restrictions apply. 
of hostnames in server certiﬁcates at a different level in the
software stack—in applications, transport libraries, and Web-
services middleware. Fahl et al. analyzed incorrect usage of
SSL in Android apps [29]. The class of certiﬁcate validation
vulnerabilities analyzed in this paper is complementary to and
has little overlap with the vulnerabilities discovered in [29, 31].
Unlike [29, 31], we developed an automated technique for
discovering certiﬁcate validation vulnerabilities.
A survey of security issues in SSL/TLS can be found
in [16]. Cryptographic ﬂaws in SSL/TLS implementations
and the protocol itself—including compression, initialization,
padding of cipher modes and message authentication codes,
etc.—can be exploited to attack conﬁdentiality, especially
when the protocol is used for HTTPS (HTTP over SSL) [3,
24, 72]. By contrast, this paper is about authentication ﬂaws.
Flaws in SSL server implementations can be exploited
for chosen-ciphertext attacks, resulting in private key compro-
mise [8, 9]. Flaws in pseudo-random number generation can
produce SSL/TLS keys that are easy to compromise [38, 50].
Hash collisions [77] and certiﬁcate parsing discrepancies
between certiﬁcate authorities (CAs) and Web browsers [44]
can trick a CA into issuing a valid leaf certiﬁcate with the
wrong subject name, or even a rogue intermediate CA cer-
tiﬁcate. By contrast, we focus on verifying whether SSL/TLS
implementations correctly handle invalid certiﬁcates.
Large-scale surveys of SSL certiﬁcates “in the wild” can be
found in [19, 25, 27, 78]. Because their objective is to collect
and analyze certiﬁcates, not to ﬁnd certiﬁcate validation errors
in SSL/TLS implementations, they are complementary to this
paper: for example, their certiﬁcate corpi can be used to “seed”
frankencert generation (Section VII). Delignat-Lavaud et al.
note that GnuTLS ignores unsupported critical extensions [19],
matching what we found with automated testing.
Akhawe et al. surveyed SSL warnings in Web browsers [1].
One of their recommendations is to accept recently expired
certiﬁcates. As we show in Section IX, several Web browsers
show just the “Expired certiﬁcate” warning even if the expired
certiﬁcate is not issued by a trusted CA and the connection is
thus insecure. Akhawe and Felt performed a large-scale user
study of the effectiveness of browser security warnings [2].
One of their ﬁndings is that users are less likely to click
through an “Expired certiﬁcate” warning than through an
“Untrusted issuer” warning, possibly because the former tend
to occur at websites that previously did not produce any
warnings. Amann et al. demonstrated that certain signs of man-
in-the-middle attacks, such as certiﬁcates never seen before for
a given domain or issued by an unusual CA, can be caused
by benign changes in the CA infrastructure [4]. SSL security
indicators in mobile Web browsers were studied in [5, 6].
The focus of this paper is on server certiﬁcate authentica-
tion, which is the most common usage pattern for SSL certiﬁ-
cates. The other direction, i.e., client certiﬁcate authentication,
was analyzed in [21, 60]. Our adversarial testing techniques for
ﬁnding bugs in the client-side validation of server certiﬁcates
can also be applied to the implementations of server-side
validation of client certiﬁcates.
Several recent high-proﬁle vulnerabilities highlighted the
need for thorough security analysis of SSL/TLS implemen-
tations. The implementation of the SSL/TLS handshake in
Mac OS and iOS accidentally did not check whether the key
used to sign the server’s key exchange messages matches the
public key in the certiﬁcate presented by the server, leaving this
implementation vulnerable to server impersonation [49] (this
vulnerability is not caused by incorrect certiﬁcate validation).
In GnuTLS, certain errors during certiﬁcate parsing were
accidentally interpreted as successful validation, thus enabling
server impersonation [33]. We discuss the latter vulnerability
in more detail in Section VIII.
B. Software testing
Our work introduces a novel black-box testing ap-
proach to address two foundational software testing prob-
lems—generation of test inputs and validation of program
outputs (aka the “oracle” problem)—in the context of ﬁnd-
ing security bugs, speciﬁcally in SSL/TLS implementations.
Researchers have extensively studied these two problems
over the last few decades in a number of contexts and de-
veloped various automated techniques to address them. For
example, techniques using grammars [48, 52, 58, 75, 79],
constraints [13, 53], dedicated generators [18], fuzzing [36],
symbolic execution [12, 35, 45, 47, 74], and genetic algo-
rithms [7] provide automated generation of inputs for black-
box and white-box testing, while techniques using correctness
speciﬁcations [15], differential testing [59], and metamorphic
testing [14] provide automated validation of program outputs.
Differential black-box testing has been successfully used to
ﬁnd parsing discrepancies between antivirus tools that can help
malware evade detection [42].
The use of grammars in testing dates back to the
1970s [62] and has provided the basis for randomized [52, 58,
75, 79] and systematic [48] techniques for ﬁnding application
bugs. The most closely related work to ours is Yang et
al.’s Csmith framework, which used random grammar-based
generation of C programs to discover many bugs in production
C compilers [79]. The key difference between Csmith and our
work is input generation. Csmith uses purely grammar-based
generation without actual C programs and hence only produces
input programs with language features that are explicitly
supported by its generation algorithm. Moreover, the design
goal of Csmith is to generate safe programs that have a unique
meaning and no undeﬁned behaviors. This allows Csmith to
use a straightforward test oracle that performs identity compar-
ison on outputs for differential testing. By contrast, our goal is
to explore behaviors of SSL/TLS implementations that are not
exercised by valid certiﬁcates and thus more likely to contain
security bugs. Hence, our test generator does not need to
ensure that test outputs conform to a restricted form. To detect
validation errors, we cluster certiﬁcates into “buckets” based
on the outputs produced by each SSL/TLS implementation
when presented with a given certiﬁcate, with each bucket
representing a discrepancy between the implementations. As
explained in Section IX, multiple discrepancies may be caused
by the same underlying implementation error (in our testing,
15 root causes led to 208 discrepancies).
Clustering test executions is a well-explored area, e.g.,
to diagnose the causes of failed executions by reducing the
number of failures to inspect [32, 41, 43, 61] or to distinguish
failing and passing executions in the context of a single
116
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:03 UTC from IEEE Xplore.  Restrictions apply. 
implementation [20]. We use clustering and differential testing
in tandem to identify incorrect behavior in the context of
multiple implementations tested together.
Our test input generator combines parts of existing real
certiﬁcates and also injects synthetic artiﬁcial parts using
operations that resemble combination and mutation in genetic
algorithms [39]. In principle, it may be possible to deﬁne
a genetic algorithm for certiﬁcate generation by customizing
genetic combination and mutation with respect to the SSL
certiﬁcate grammar, ﬁelds, their values, extensions, etc. The