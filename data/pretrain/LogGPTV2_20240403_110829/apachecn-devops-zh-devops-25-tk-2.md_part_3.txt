状态分为两部分；`Cluster-wide`和`NodeGroups`。集群范围状态的`ScaleUp`部分显示缩放为`InProgress`。此刻，这里有`3`准备好的节点。
如果我们向下移动到`NodeGroups`，我们会注意到每个承载我们的节点的组都有一个。在 AWS 中，这些组映射到自动缩放组，在 Google 中映射到实例组，在 Azure 中映射到自动缩放。配置中的一个`NodeGroups`有`ScaleUp`部分`InProgress`。在该组中，`1`节点是`ready`。`cloudProviderTarget`值应该设置为高于`ready`节点数量的数字，我们可以得出结论，集群自动缩放器已经增加了该组中所需的节点数量。
Depending on the provider, you might see three groups (GKE) or one (EKS) node group. That depends on how each provider organizes its node groups internally.
现在我们知道集群自动缩放器正在扩展节点，我们可能会探究是什么触发了该操作。
让我们描述`api`吊舱并检索它们的事件。由于我们只需要那些与`cluster-autoscaler`相关的，我们将使用`grep`来限制输出。
```
 1  kubectl -n go-demo-5 \
 2      describe pods \
 3      -l app=api \
 4      | grep cluster-autoscaler
```
关于 GKE 的输出如下。
```
  Normal TriggeredScaleUp 85s cluster-autoscaler pod triggered scale-up: [{... 1->2 (max: 3)}]
  Normal TriggeredScaleUp 86s cluster-autoscaler pod triggered scale-up: [{... 1->2 (max: 3)}]
  Normal TriggeredScaleUp 87s cluster-autoscaler pod triggered scale-up: [{... 1->2 (max: 3)}]
  Normal TriggeredScaleUp 88s cluster-autoscaler pod triggered scale-up: [{... 1->2 (max: 3)}]
```
我们可以看到几个 Pods 触发了`scale-up`事件。这些是处于待定状态的吊舱。这并不意味着每个触发器都会创建一个新节点。集群自动缩放器足够智能，知道它不应该为每个触发器创建新节点，但在这种情况下，一两个节点(取决于缺少的容量)就足够了。如果这被证明是错误的，它将在一段时间后再次扩大规模。
让我们检索组成集群的节点，看看是否有任何变化。
```
 1  kubectl get nodes
```
输出如下。
```
NAME                                     STATUS     ROLES    AGE     VERSION
gke-devops25-default-pool-7d4b99ad-...   Ready         2m45s   v1.9.7-gke.6
gke-devops25-default-pool-cb207043-...   Ready         2m45s   v1.9.7-gke.6
gke-devops25-default-pool-ce277413-...   NotReady      12s     v1.9.7-gke.6
gke-devops25-default-pool-ce277413-...   Ready         2m48s   v1.9.7-gke.6
```
我们可以看到一个新的工作节点被添加到集群中。它还没有准备好，所以我们需要等一会儿，直到它完全投入使用。
请注意，新节点的数量取决于承载所有 Pods 所需的容量。您可能会看到一个、两个或更多新节点。
![](img/27fff93a-5bb4-452a-bc2f-e480ec2c725a.png)
Figure 2-2: The Cluster Autoscaler process of scaling up nodes
现在，让我们看看我们的豆荚发生了什么。请记住，上次我们检查它们时，有相当多的处于待定状态。
```
 1  kubectl -n go-demo-5 get pods
```
输出如下。
```
NAME    READY STATUS  RESTARTS AGE
api-... 1/1   Running 1        75s
api-... 1/1   Running 0        75s
api-... 1/1   Running 0        75s
api-... 1/1   Running 1        75s
api-... 1/1   Running 1        75s
api-... 1/1   Running 3        105s
api-... 1/1   Running 0        75s
api-... 1/1   Running 0        75s
api-... 1/1   Running 1        75s
api-... 1/1   Running 1        75s
api-... 1/1   Running 0        75s
api-... 1/1   Running 1        75s
api-... 1/1   Running 0        75s
api-... 1/1   Running 1        75s
api-... 1/1   Running 0        75s
db-0    2/2   Running 0        107s
db-1    2/2   Running 0        67s
db-2    2/2   Running 0        28s
```
群集自动缩放器增加了节点组中所需的节点数量(例如，AWS 中的自动缩放组)，从而创建了一个新节点。一旦调度程序注意到集群容量的增加，它就将挂起的 Pods 调度到新节点。几分钟之内，我们的集群扩展了，所有扩展的 Pods 都在运行。
![](img/3126993e-31ae-4184-bee7-068f2752fa4c.png)
Figure 2-3: Creation of the new node through node groups and rescheduling of the pending Pods
那么，集群自动缩放器使用什么规则来决定何时扩展节点？
# 管理节点纵向扩展的规则
集群自动缩放器通过库贝应用编程接口上的手表监控吊舱。它每 10 秒检查是否有任何不可切割的 Pods(可通过`--scan-interval`标志配置)。在这种情况下，当 Kubernetes 调度程序找不到可以容纳它的节点时，Pod 是不可聚合的。例如，Pod 可以请求比任何工作节点上可用的内存都多的内存。
Cluster Autoscaler assumes that the cluster is running on top of some kind of node groups. As an example, in the case of AWS, those groups are **Autoscaling Groups** (**ASGs**). When there is a need for additional nodes, Cluster Autoscaler creating a new node by increasing the size of a node group.
集群自动缩放器假设请求的节点将在 15 分钟内出现(可通过`--max-node-provision-time`标志配置)。如果该期限到期，并且没有注册新节点，则如果 Pods 仍处于挂起状态，它将尝试扩展不同的组。它还将在 15 分钟后移除未注册的节点(可通过`--unregistered-node-removal-time`标志配置)。
接下来，我们将探讨如何缩小集群。
# 缩小集群
扩展集群以满足需求是至关重要的，因为它允许我们托管完成(某些)服务级别协议所需的所有副本。当需求下降且我们的节点未得到充分利用时，我们应该缩减规模。考虑到我们的用户不会遇到由于集群中有太多硬件而导致的问题，这并不是必需的。尽管如此，如果我们要减少开支，我们就不应该有未充分利用的节点。未使用的节点会导致金钱浪费。这在所有情况下都是正确的，尤其是在运行在云中并且只为我们使用的资源付费的情况下。即使在我们已经购买了硬件的本地，也必须缩减和释放资源，以便其他集群可以使用它们。
我们将通过应用一个新的定义来模拟需求的减少，该定义将重新定义 HPAs 阈值为`2`(最小值)和`5`(最大值)。
```
 1  kubectl apply \
 2      -f scaling/go-demo-5.yml \
 3      --record
 4
 5  kubectl -n go-demo-5 get hpa
```
后一个命令的输出如下。
```
NAME REFERENCE      TARGETS          MINPODS MAXPODS REPLICAS AGE
api  Deployment/api 0%/80%, 0%/80%   2       5       15       2m56s
db   StatefulSet/db 56%/80%, 10%/80% 3       5       3        2m57s
```
我们可以看到`api` HPA 的最小值和最大值变为`2`和`5`。目前复制品的数量仍然是`15`，但是很快就会下降到`5`。HPA 已经更改了部署的副本，所以让我们等到它推出后再看看 Pods。
```
 1  kubectl -n go-demo-5 rollout status \
 2      deployment api
 3
 4  kubectl -n go-demo-5 get pods
```
后一个命令的输出如下。
```
NAME    READY STATUS  RESTARTS AGE
api-... 1/1   Running 0        104s
api-... 1/1   Running 0        104s
api-... 1/1   Running 0        104s
api-... 1/1   Running 0        94s
api-... 1/1   Running 0        104s
db-0    2/2   Running 0        4m37s
db-1    2/2   Running 0        3m57s
db-2    2/2   Running 0        3m18s
```
让我们看看`nodes`发生了什么。
```
 1  kubectl get nodes
```
输出显示，我们仍然有四个节点(或者在我们缩小部署之前您的节点数)。
鉴于我们还没有达到仅三个节点的期望状态，我们可能想再看一下`cluster-autoscaler-status`配置图。
```
 1  kubectl -n kube-system \
 2      get configmap \
 3      cluster-autoscaler-status \
 4      -o yaml
```
输出限于相关部分，如下所示。
```