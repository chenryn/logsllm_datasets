title:Performance and availability aware regeneration for cloud based multitier
applications
author:Gueyoung Jung and
Kaustubh R. Joshi and
Matti A. Hiltunen and
Richard D. Schlichting and
Calton Pu
201O IEEEIIFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
Performance 
and Availability 
Aware Regeneration 
For Cloud Based Multitier 
Applications 
Gueyoung Jungt Kaustubh R. Joshi+ Matti A. Hiltunen+ Richard D. Schlichting
+ Calton Put 
tCollege of Computing 
Georgia Institute 
of Technology 
Atlanta, GA, USA 
+ AT&T Labs Research 
180 Park Ave. 
Florham Park, NJ, USA 
{gueyoung.jung,calton}@cc.gatech.edu 
{kaustubh,hiltunen,rick}@research.att.com 
trends in system and data center design are chang­
systems are increasingly 
on large numbers of cheap, less reliable 
to a decrease 
commod­
in MTBF of the 
Multitier 
thus leading 
For example, 
for a cluster 
current 
ing the role of repair. 
running 
ity components, 
system components. 
erage of 1000 node failures/yr 
cluster 
while, skilled 
pensive 
to achieve 
placement, 
portable 
tain tightly 
pletely 
emerging. 
packed individual 
non-serviceable, 
economies 
and increasing 
manpower is quickly 
"data-center 
resource, 
Google reported 
an av­
in their typical 
1800 node 
MTBF of 8.76 hours [11]. Mean­
the most ex­
becoming 
thus encouraging 
data center operators 
of scale by batching 
repairs 
MTTR in the process. 
and re­
In fact, 
in a box" designs 
(e.g., 
components 
i.e., with an infinite 
[15]) that con­
that are com­
MTTR, are 
Abstract 
improved  availability 
compared 
to a sys­
high availability 
while maintain­
components 
to 
software 
of a system whenever 
failures 
oc­
enables agile system de­
components 
hardware 
resources 
in a 
can be cheaply 
how these facili­
solutions 
to the clas­
and allocated 
This paper examines 
By regenerating 
Virtual 
machine technology 
in which software 
fashion. 
the redundancy 
ployments 
moved, replicated, 
controlled 
ties can be used to provide enhanced 
sic problem of ensuring 
ing performance. 
restore 
cur; we achieve 
tem with a fixed redundancy 
controlling 
component 
using information 
formance 
the resulting 
consider 
tier enterprise 
racks, clusters, 
pendence. 
proach provides better 
degradation 
tional 
applications 
and data centers 
of system response 
an environment 
predictions 
performance 
Simulation 
approaches. 
from queuing 
placement 
results 
availability 
models, 
we ensure that 
is minimized. 
degradation 
We 
in which a collection 
operates across multiple 
hosts, 
inde­
failure 
show that our proposed ap­
to maximize 
of multi­
and significantly 
lower 
times compared 
to tradi­
about application 
control flow and per­
level. Moreover; by smartly 
and resource 
allocation 
1. Introduction 
These trends imply that applications 
will increasingly 
in which parts of the infrastructure 
of software components 
is 
used to ensure high availability. 
The 
must be high enough to tolerate 
addi­
take place. Maintain­
under the low MTBF and high MTTR 
state. 
failures 
until repairs 
eventually 
Replication 
in environments 
operate 
are in a failed 
a standard 
technique 
level of redundancy 
tional 
ing such redundancy 
conditions 
licenses) 
head (e.g., 
state replication). 
tive time-to-repair 
by maintaining 
that can be quickly 
deployed 
cause the spares represent 
We present a solution 
is expensive 
and may have a significant 
unutilized 
(e.g., 
standby 
Meanwhile, 
cost of hardware 
and software 
performance 
over­
reducing 
effec­
spare resources 
is inefficient be­
automatically 
that ensures high availability 
High availability 
and low response 
time are crucial, 
al­
for the multi tier ap­
functionality 
Ensuring 
that implement 
to be deployed 
several 
requirements 
critical 
business 
for 
high availability 
the 
po­
though often conflicting, 
plications 
many enterprises. 
applications 
tentially 
spanning 
deployment 
Redundancy 
ware components 
(MTBF) and quick repair or replacement 
ponents, 
with sufficient 
data centers, 
impose a performance 
and replication 
is traditionally 
ensured 
of failed com­
i.e., low mean time to repair (MTTR). However, 
redundancy, 
requires 
penalty. 
while distributed 
with high mean time between  failures 
(e.g., 
good performance 
all sys­
by employing 
no idle standby resources) 
and lim­
while maintaining 
tem resources 
ited levels of replication. 
resource 
we regenerate 
nents and deploy them on the remaining 
the required 
applications 
regeneration-based 
the affected software compo­
resources 
goals of all the 
This 
availability 
in the system are met as long as possible. 
and performance 
Specifically, 
can provide 
approach 
fails, 
so that 
when a hardware 
by using reliable hard­
resources. 
high availability 
978-1-4244-7501-8/10/$26.00 
©201O IEEE 
497 
DSN 2010: Jung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
with far less redundancy 
replacement. 
than designs 
oriented 
around static 
for the 
sharing 
and avail­
algorithm 
has to 
for ensuring 
and resource 
The proper placement, 
For high availability, 
components 
and low response 
replicas 
clusters 
increased 
allocation, 
is crucial 
high avail­
time of the applications 
The placement 
goals of performance 
regenerated 
ability 
the computing 
resources. 
deal with the conflicting 
ability. 
need to be placed in different 
centers, 
while the resulting 
affect the application's 
formance 
applications, 
tion of a component 
to be the bottleneck 
sult, the hosts where the other tiers of the application 
placed may become underutilized. 
tions share hardware 
the problem becomes even more complex. 
may 
of a component 
or even different 
data 
network latency 
may 
good per­
where poor placement 
database 
and resource 
server) 
with multi tier 
alloca­
for the whole application 
When multiple 
time. Ensuring 
resources 
response 
may cause it 
applica­
and as a re­
(e.g., 
(e.g., 
are 
We consider 
a set of multitier 
distributed 
across multiple 
applications 
and a shared 
racks, clus­
in cloud computing), 
becomes particularly  challenging 
for greater 
failure 
independence. 
We 
framework that reconfigures 
this set 
on the resource 
pool in reaction 
to failures 
a user-defined  availability 
level for as long as 
and to do so in a way that minimizes 
performance, 
response 
our solution 
To preserve 
a management 
pool of resources 
ters, and data centers 
present 
of applications 
to maintain 
possible, 
time degradation. 
not only regenerates 
may also redeploy 
pacted by the failure. 
optimization 
algorithm 
ity and performance 
exhibit 
solution 
tends queuing 
to predict 
multi tier applications 
sources. 
is deployed 
a novel hierarchical 
but 
and places the failed components 
those components 
We introduce 
that balances 
that were not im­
the needs of availabil­
to produce target configurations 
degradation. 
that 
Our 
through an online controller 
that ex­
developed 
utilization 
in [17] 
of the 
and resource 
models we have previously 
the performance 
the least amount of performance 
in different 
deployments 
over the re­
2. Architecture 
We consider 
a consolidated 
data-center 
environment 
A are to be deployed 
in 
into hierar­
across a number of 
The physical 
hosts are organized 
applications 
hosts H located 
of racks, clusters, 
and management. 
networking 
which a set of multitier 
across a set of physical 
data centers. 
chical groupings 
cilitate 
represented 
the set of resource 
clusters, 
tionship 
DataCenter!' 
chy with 20 machines 
hierarchy 
by a resource 
are 
These groupings 
(R, 5:.R), where R is 
machines, 
hosting 
e.g., Hostl 5:.R Rackl 5:.R 
Figure I shows an example resource 
individual 
and 5:.R specifies 
and data-centers) 
between these groups, 
groups (i.e., 
across four racks in two 
and data centers 
to fa­
distributed 
rela­
hierar­
racks, 
Data Center Level 
L(DataCenter)= 
lOOms 
MTBF(DataCenter)=5yrs 
Rack Level 
L(Rack)=5ms 
MTBF (Rack)= I year 
Host Level 
L(Host)= I Ousec 
MTBF(Host)= I month 
Figure 
1. Resource 
Levels Example 
Two resource 
data centers. 
groups are said to be at the same 
"level" rl E RL if they are of the same type, e.g., Rackl 
and Rack2. A resource 
The example in the figure has three levels. 
represents 
such that Hostl 5:.'R DataCenter! 
directly 
hosted in DataCenter! 
5:.'R 
5:.R 
that Hostl is 
of the hosting 
indicates 
The relation 
relation 
or indirectly. 
the transitive 
is trivially 
closure 
either 
at the same level as itself. 
Hosts are interconnected 
by a data center network and 
between hosts depends on how close 
hosts placed in 
between them 
in the hierarchy, 
i.e., 
the network latency 
they are to one another 
the same rack have a lower network latency 
than hosts across different 
tency than hosts in different 
L(rl) the maximum latency 
at resource 
tween failures 
eral, MTBF increases 
MTBF for hosts is smaller 
is smaller 
this paper, we consider 
ent resource 
level rl. Finally, 
with increasing 
for each resource 
racks, which have a lower la­
We denote by 
data centers. 
between two hosts separated 
we denote the mean time be­
level r by MTBFr. In gen­
level, 
resource 
than MTBF for a rack, which 
still than the MTBF for an entire data center. 
only fail-stop  failures 
at the differ­
levels. 
i.e., 
In 
type must be at least 2. Each application 
The number of replicas 
Each application 
web server, 
a consists 
of a set Na of component 
each of which contains 
types Ta. For example, 
used in our testbed 
a 
buying, 
database), 
replicated 
that correspond 
transaction 
site benchmark 
to login, 
of 
a are given by reps(n). To 
the replication 
level for each 
types (e.g., 
several 
components. 
type n E Na for application 
avoid single points of failure, 
component 
may support 
multiple 
the RUBiS [5] auction 
has transactions 
searching, 
a sequence 
nents. The workload Wa for the application 
terized 
types, i.e., {w It E Ta}, and the workload 
system as W =  {wala E A}. Each application 
replica nk executes 
machine [3] on 
a physical 
is allocated 
a fractional 
cap(nk) that is enforced 
VMs belonging 
by 
by Xen's credit-based 
may be hosted on 
host that it can share with other VMs. Each VM 
share of the host's CPU denoted 
calls between application 
compo­
to a single application 
in its own Xen virtual 
Each transaction 
by the set of request 
and selling. 
of function 
profile, 
can be charac­
for the entire 
component 
browsing, 
can initiate 
scheduler. 
rates for each of its transaction 
978-1-4244-7501-8/101$26.00 
©2010 IEEE 
498 
DSN 2010: lung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEEIIFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
level (e.g., 
performance. 
in poor application 
a candidate 
across a high resource 
sulting 
producing 
each application 
new configuration. 
level unacceptable, 
and request 
of the performance 
If an application 
it can reduce its desired 
candidate 
configuration, 
across data centers) 
after 
Therefore, 
informs 
the controller 
it can expect in the 
configuration. 
another 
reliability 
finds this performance 
level 
re­
a SAN), so that any 
can be reinstantiated 
VM reinstantia­
3. Metrics 
and Models 
and is commonly 
this window of reduced replication 
decisions. 
state availability 
host anywhere 
any physical 
erally,  the 
cation's 
due to higher network latency. 
higher the resource 
VMs are distributed, 
in the resource 
hierarchy. 
level across which an appli­
the worse is its performance 
Gen­
Finally, 
as is common in data center environments, 
we 
(e.g., 
the VM. Although 
executing 
network (e.g., 
across data centers), 
on a server that has failed 
by copying the VM's disk image 
host by using its disk image. When shared SAN 
and subsequently 
could also be applied 
assume a shared storage 
VM residing 
on another 
is not available 
tion can still be achieved 
to the target, 
our approach 
to disk failures, 
in this 
paper we assume they are handled within the storage 
ray using RAID technology. 
volatile 
application. 
multitier 
vide clustered 
through data replication 
operational. 
state replication 
tion techniques 
Our approach 
ar­
loss of 
by the 
it is assumed to be provided 
designed 
for 
systems such as Tomcat or MySQL servers 
pro­
modes to ensure volatile 
for applications 
most components 
can be provided 
as long as at least one replica 
If protection 
using VM-level 
data is needed, 
For example, 
Protection 
is realized 
that do not support 
replica­
by a runtime controller 
such as [8]. 
against 
that 
is 
the system and chooses the best system config­
occurs. 
The controller 
in an operations 
and is tied to the output of a monitoring 
center that manages the target 
system. 
is facilitated 
by many off­
products 
or recovery 
data centers. 
when a failure 
such as IBM's Tivoli, 
monitor aggregation 
monitors 
uration 
executes 
system, 
Centralized 
the-shelf 
used in commercial 
pose are deterministic, 
ability 
cation. 
covery (repair/replacement) 
ing system, 
reconfigures 
the system to maintain 
standard 
VM that contains 
can either migrate 
CPU share allocated 
can be ensured 
When an alarm regarding 
machine techniques. 
the VM to another 
an application 
the controller 
and therefore 
is received 
the desired 
virtual 
using traditional 
The algorithms 
we pro­
the controller 
avail­
state machine repli­
a machine failure 
or re­
from the monitor­
the applications 
in 
replication 
levels using 