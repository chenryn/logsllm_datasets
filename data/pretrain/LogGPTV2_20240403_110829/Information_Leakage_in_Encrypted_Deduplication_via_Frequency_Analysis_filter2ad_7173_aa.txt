title:Information Leakage in Encrypted Deduplication via Frequency Analysis
author:Jingwei Li and
Chuan Qin and
Patrick P. C. Lee and
Xiaosong Zhang
Information Leakage in Encrypted Deduplication
via Frequency Analysis
Jingwei Li1, Chuan Qin2, Patrick P. C. Lee2, Xiaosong Zhang1
1Center for Cyber Security, University of Electronic Science and Technology of China
2Department of Computer Science and Engineering, The Chinese University of Hong Kong
PI:EMAIL, PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—Encrypted deduplication seamlessly combines en-
cryption and deduplication to simultaneously achieve both data
security and storage efﬁciency. State-of-the-art encrypted dedu-
plication systems mostly adopt a deterministic encryption ap-
proach that encrypts each plaintext chunk with a key derived
from the content of the chunk itself, so that identical plaintext
chunks are always encrypted into identical ciphertext chunks for
deduplication. However, such deterministic encryption inherently
reveals the underlying frequency distribution of the original
plaintext chunks. This allows an adversary to launch frequency
analysis against the resulting ciphertext chunks, and ultimately
infer the content of the original plaintext chunks.
In this paper, we study how frequency analysis practically
affects information leakage in encrypted deduplication storage,
from both attack and defense perspectives. We ﬁrst propose a
new inference attack that exploits chunk locality to increase the
coverage of inferred chunks. We conduct trace-driven evaluation
on both real-world and synthetic datasets, and show that the
new inference attack can infer a signiﬁcant fraction of plaintext
chunks under backup workloads. To protect against frequency
analysis, we borrow the idea of existing performance-driven
deduplication approaches and consider an encryption scheme
called MinHash encryption, which disturbs the frequency rank
of ciphertext chunks by encrypting some identical plaintext
chunks into multiple distinct ciphertext chunks. Our trace-driven
evaluation shows that MinHash encryption effectively mitigates
the inference attack, while maintaining high storage efﬁciency.
I. INTRODUCTION
To manage massive amounts of data in the wild, modern
storage systems employ deduplication to eliminate content du-
plicates and save storage space. The main idea of deduplication
is to store only data copies, called chunks, that have unique
content among all already stored chunks. Field studies have
demonstrated that deduplication achieves signiﬁcant storage
savings in production environments, for example, by 50% in
primary storage [33] and up to 98% in backup storage [45].
Deduplication is also adopted by commercial cloud storage
services (e.g., Dropbox, Google Drive, Bitcasa, etc.) for cost-
efﬁcient outsourced data management [22], [34].
In the security context, combining encryption and dedupli-
cation, referred to as encrypted deduplication, is essential for
protecting against content leakage in deduplication storage.
Conventional (symmetric) encryption is incompatible with
deduplication, as it requires that users encrypt data with
their own distinct secret keys, thereby encrypting duplicate
plaintext chunks into distinct ciphertext chunks. To preserve
deduplication effectiveness, encrypted deduplication ensures
9
10
y
c
n
e
u
q
e
r
F
6
10
3
10
0
10
 0
 0.2
 0.4
 0.6
 0.8
 1
CDF of Chunks
Fig. 1. Frequency distribution of chunks in the FSL dataset.
that ciphertext chunks originated from duplicate plaintext
chunks can still be deduplicated. Message-locked encryption
(MLE) [9] formalizes a cryptographic primitive to address
the issue, by encrypting each chunk with a secret key that
is derived from the chunk itself via some one-way function.
For example, convergent encryption [16] is one classical
instantiation of MLE by deriving the secret key through the
hash of a chunk. On top of MLE, several storage systems
address additional security issues, such as brute-force attacks
[8], key management failures [17], side-channel attacks [28],
and access control [38].
However, we argue that existing MLE implementations (see
Section VIII) still cannot fully protect against content leakage,
mainly because their encryption approaches are deterministic.
That is, each ciphertext chunk is encrypted by a key that is
deterministically derived from the original plaintext chunk.
Thus, an adversary, which can be malicious users or storage
system administrators, can analyze the frequency distribution
of ciphertext chunks and infer the original plaintext chunks
based on classical frequency analysis [36]. In addition, prac-
tical storage workloads often exhibit non-uniform frequency
distributions in terms of the occurrences of chunks with the
same content, thereby allowing the adversary to accurately
differentiate chunks by their frequencies in frequency analysis.
Figure 1 depicts the frequency distribution of chunks in the
real-world FSL dataset used in our evaluation (see Section V).
We observe that the frequency distribution varies signiﬁcantly:
99.8% of chunks occur less than 100 times, while around 30
chunks occur over 10,000 times.
The deterministic nature of MLE makes encrypted dedupli-
cation vulnerable to frequency analysis. In the simplest form
of the attack, an adversary ﬁrst obtains prior knowledge of
frequency distributions of plaintext chunks (e.g., by unintended
data release [6] or data breaches [20]), counts the frequencies
of all ciphertext chunks, and ﬁnally infers their corresponding
plaintext chunks based on the frequency distribution of cipher-
text chunks. While previous studies [3], [7] have addressed the
possibility of launching frequency analysis against MLE-based
storage and also proposed cryptographic mechanisms to miti-
gate the issue, their investigations are theoretically driven. The
practical implications of frequency analysis against encrypted
deduplication remain unexplored.
Contributions: In this paper, we conduct an in-depth study of
how frequency analysis practically affects information leakage
in encrypted deduplication. Our study spans both attack and
defense perspectives, and is speciﬁcally driven by the charac-
teristics of storage workloads in deduplication systems.
On the attack side, we propose a locality-based attack to en-
hance the severity of classical frequency analysis by exploiting
chunk locality, which is prevalent in backup workloads. Chunk
locality states that chunks are likely to re-occur together with
their neighboring chunks across backups. In practice, changes
to backups often appear in few clustered regions of chunks,
while the remaining regions of chunks will appear in the same
order in previous backups. Previous studies have exploited
chunk locality to improve deduplication performance [30],
[47], [49]. Here, we adapt this idea from a security perspective
into frequency analysis: if a plaintext chunk M corresponds to
a ciphertext chunk C, then the neighboring plaintext chunks
of M are likely to correspond to the neighboring ciphertext
chunks of C. Our trace-driven evaluation, using both real-
world and synthetic datasets, shows that the locality-based
attack can identify signiﬁcantly more ciphertext-plaintext pairs
than classical frequency analysis. For example, for the real-
world FSL dataset, we ﬁnd that the locality-based attack can
infer a fraction of 17.8% of the latest backup data, while
the basic attack based on the direct application of classical
frequency analysis can only infer 0.0001% of data. In addition,
if a limited fraction (e.g., 0.2%) of plaintext information of the
latest backup is leaked, the inference rate of the locality-based
attack can reach up to 27.1%.
On the defense side, our key insight of combating frequency
analysis is to disturb the frequency ranking of ciphertext
chunks. To this end, we borrow the idea from previous
performance-driven deduplication approaches [10], [38], [47].
We consider an encryption scheme called MinHash encryption,
which derives an encryption key based on the minimum
ﬁngerprint over a set of adjacent chunks, such that some iden-
tical plaintext chunks can be encrypted into multiple distinct
ciphertext chunks. Our trace-driven evaluation shows that Min-
Hash encryption effectively mitigates the locality-based attack,
while maintaining high storage efﬁciency as demonstrated in
previous deduplication approaches. For example, for the real-
world dataset, if we repeat our attack evaluation that 0.2% of
plaintext information of the latest backup is leaked, MinHash
encryption can now suppress the inference rate of the locality-
based attack to below 0.45%; meanwhile, it achieves a storage
saving of up to 83.61%, which is only 3-4% less than that of
original chunk-based deduplication.
II. BASICS
Following Section I, we elaborate the basics of deduplica-
tion, encrypted deduplication, and frequency analysis.
A. Deduplication
Deduplication can be viewed as a coarse-grained compres-
sion technique to save storage space. While it can operate
at the granularities of ﬁles or chunks, this paper focuses on
chunk-based deduplication as it achieves more ﬁne-grained re-
dundancy elimination. Speciﬁcally, a storage system partitions
input data into variable-size chunks through content-deﬁned
chunking (e.g., Rabin ﬁngerprinting [39]), which identiﬁes
chunk boundaries that match speciﬁc content patterns so as
to remain robust against content shifts [18]. We can conﬁgure
the minimum, average, and maximum chunk sizes in content-
deﬁned chunking for different granularities. After chunking,
each chunk is identiﬁed by a ﬁngerprint, which is computed
from the cryptographic hash of the content of the chunk. Any
two chunks are said to be identical if they have the same
ﬁngerprint, and the collision probability that two non-identical
chunks have the same ﬁngerprint is practically negligible [11].
Deduplication requires that only one physical copy of identical
chunks is kept in the storage system, while any identical chunk
refers to the physical chunk via a small-size reference.
To check if any identical chunk exists, the storage system
maintains a ﬁngerprint index, a key-value store that holds
the mappings of all ﬁngerprints to the addresses of physical
chunks that are currently stored. For each ﬁle, the storage
system also stores a ﬁle recipe that lists the references to all
chunks of the ﬁle for future reconstruction.
B. Encrypted Deduplication
Encrypted deduplication ensures that all physical chunks are
encrypted for conﬁdentiality (i.e., data remains secret from
unauthorized users and even storage system administrators),
while the ciphertext chunks that are originated from identical
plaintext chunks can still be deduplicated for storage savings.
As stated in Section I, message-locked encryption (MLE) [9] is
a formal cryptographic primitive to achieve encrypted dedupli-
cation, in which each chunk is encrypted by a symmetric key
that is derived from the chunk itself. Thus, identical plaintext
chunks will be encrypted into the identical ciphertext chunks,
thereby preserving deduplication effectiveness.
MLE is inherently vulnerable to the ofﬂine brute-force
attack [9], which allows an adversary to determine which
plaintext chunk is encrypted into an input ciphertext chunk.
Suppose that the adversary knows the set of chunks from
which the plaintext chunk is drawn. Then it can launch the
brute-force attack as follows: for each chunk from the set, it
ﬁnds the chunk-derived key (whose key derivation algorithm is
supposed to be publicly available), encrypts the chunk with the
chunk-derived key, and ﬁnally checks if the output ciphertext
chunk is identical to the input ciphertext chunk. If so, the
plaintext chunk is the answer. Thus, MLE can only achieve
security for unpredictable chunks [9], meaning that the size of
the set of chunks is sufﬁciently large, such that the brute-force
attack becomes infeasible.
To protect against
the brute-force attack, DupLESS [8]
realizes server-aided MLE, which outsources MLE key man-
agement to a dedicated key manager that is only accessible
by authenticated clients. Each authenticated client ﬁrst queries
the key manager for the chunk-derived key. Then the key
manager computes and returns the key via a deterministic key
derivation algorithm that takes the inputs of both the chunk
ﬁngerprint and a system-wide secret maintained by the key
manager itself. This makes the resulting ciphertext chunks
appear to be encrypted by a random key from the adversary’s
point of view. In addition, the key manager limits the rate
of key generation to slow down any online brute-force attack
for querying the encryption key. If the key manager is secure
from adversaries, server-aided MLE ensures security even for
predictable chunks; otherwise, it still maintains security for
unpredictable chunks as in original MLE [9].
Most existing implementations of MLE-based encrypted
deduplication, either realized as convergent encryption or
server-aided MLE, follow the notion of deterministic encryp-
tion, which ensures that identical plaintext chunks always form
identical ciphertext chunks to make deduplication possible.
Thus, they inherently become vulnerable to frequency analysis
as we show in this paper. Some encrypted deduplication
designs are based on non-deterministic encryption [3], [7], [9],
[31], yet they still keep deterministic components [9], incur
high performance overhead [31], or require cryptographic
primitives that are not readily implemented [3], [7]. We
elaborate the details in Section VIII.
C. Frequency Analysis
Frequency analysis [32] is a classical
inference attack
that has been historically used to recover plaintexts from
substitution-based ciphertexts, and is known to be useful for
breaking deterministic encryption. In frequency analysis, an
adversary has access to a set of plaintexts and a set of
ciphertexts, and its attack goal is to relate each ciphertext to
the plaintext in both sets. To launch the attack, the adversary
ranks the available plaintexts and ciphertexts separately by
frequency, and maps each ciphertext to the plaintext in the
same frequency rank. In this paper, we examine how frequency
analysis can be used to attack encrypted deduplication.
III. THREAT MODEL
We focus on backup workloads, which have substantial
content redundancy and are proven to be effective for dedu-
plication in practice [45], [49]. Backups are copies of primary
data (e.g., application states, ﬁle systems, and virtual disk
images) over time. They are typically represented as weekly
full backups (i.e., complete copies of data) followed by daily
incremental backups (i.e., changes of data since the last full
backup). Our threat model focuses on comparing different
versions of full backups from the same primary data source at
different times. In the following discussion, we simply refer
to “full backups” as “backups”.
We consider an adversary that launches frequency analysis
against an encrypted deduplication storage system that applies
MLE-based deterministic encryption (e.g., convergent encryp-
tion [16] and server-aided MLE [8]) to each chunk of a backup.
We assume that the adversary is honest-but-curious, meaning
that it does not change the prescribed protocols of the storage
system and modify any data in storage.
To launch frequency analysis, the adversary should have
access to auxiliary information [36] that provides ground
truths about
the backups being stored. In this work, we
model the auxiliary information as the plaintext chunks of
a prior (non-latest) backup, which may be obtained through
unintended data releases [6] or data breaches [20]. Clearly, the
success of frequency analysis heavily depends on how accurate
the available auxiliary information describes the backups [36].
Our focus is not to address how to obtain accurate auxiliary
information, which we pose as future work; instead, given the
available auxiliary information, we study how an adversary can
design a severe attack based on frequency analysis and how
we can defend against the attack. We also evaluate the attack
given publicly available auxiliary information (see Section V).
Based on the available auxiliary information (which de-
scribes a prior backup), the primary goal of the adversary is
to infer the content of the plaintext chunks that are mapped to
the ciphertext chunks of the latest backup. The attack can be
based on two modes:
• Ciphertext-only mode: It models a typical case in which
the adversary can access the ciphertext chunks of the
latest backup (as well as the auxiliary information about
a prior backup).
• Known-plaintext mode: It models a more severe case
in which a powerful adversary not only can access the
ciphertext chunks of the latest backup and the auxiliary
information about a prior backup as in ciphertext-only
mode, but also knows a small fraction of the ciphertext-
plaintext chunk pairs about the latest backup (e.g., from
stolen devices [15]).
In both attack modes, we assume that the adversary can
monitor the processing sequence of the storage system and
access the logical order of ciphertext chunks of the latest
backup before deduplication. Our rationale is that existing
deduplication storage systems [47], [49] often process chunks
in logical order, so as to effectively cache metadata for efﬁcient
deduplication. On the other hand, the adversary cannot access
any metadata information (e.g.,
index, ﬁle
recipes of all ﬁles). In practice, we do not apply deduplication
to the metadata, which can be protected by conventional
encryption. For example, the ﬁle recipes can be encrypted by
user-speciﬁc secret keys. Also, the adversary cannot identify
which prior backup a stored ciphertext chunk belongs to by
analyzing the physical storage space, as the storage system can
store ciphertext chunks in randomized physical addresses or
commercial public clouds (the latter is more difﬁcult to access
directly).
the ﬁngerprint