tems, where certain programs (e.g., /usr/bin/passwd) are allowed
to escalate privileges on invocation (e.g., run as the root user),
whereas others are conﬁned to user permissions. Current methods
to allow escalation of privileges on invocation are either inﬂexible
or fail to prevent a compromised parent process from exploiting
ownerships. One approach is to statically associate ownerships to
event handlers, as adopted by some DIFC systems [21, 11]. How-
ever, we do not want declassiﬁers to be able to run with ownerships
of all the users that have approved its use. Alternatively, systems
use the security labels of the parent process and/or the program to
be executed to determine its authority, such as SELinux [26] tran-
sition rules. However, simply checking the label of a program ﬁle
does not prevent a compromised event handler from running modi-
ﬁed binaries, applying ownerships to the wrong objects, or invoking
programs with ownership for the wrong operations.
Pileus includes a mechanism for computing legal delegations
of a user’s ownership to an event handler by relating the invo-
cation of an event handler approved to hold such ownership to a
speciﬁc operation from that user. When an operation is received,
Pileus maps the operation to a set of authorized delegations that
are allowed for the operations execution. These authorized dele-
gations are called ownership authorizations, which are tuples of
the form auth = (h(e), own(e), args(e)), where: (1) h(e) is
the hash of the event handler e’s code; (2) own(e) are the own-
erships to be delegated to the event handler e; and (3) args(e) =
{(arg1, ind1)...(argn, indn)} is a speciﬁcation of speciﬁc argu-
ment values argi and index values indi input to the event handler
e. Only if the program corresponding to the hash value h(e) is in-
voked by the user whose tags correspond to own(e) with the argu-
ments speciﬁed by arg(e) does the resulting event handler obtain
that user’s ownerships.
Pileus constructs ownership authorizations primarily from user
operations. When a user runs a operation, Pileus records the user’s
ownerships and maps the operation’s arguments to the expected ar-
guments for the speciﬁc invocations. Operation-speciﬁc rules are
necessary to deﬁne these mappings, but fortunately, there are not
a large number of event handler invocations that require owner-
ship. The identity of approved endorsers and declassiﬁers for par-
ticular operations must be provided ahead of time by the users,
but this decision may be aided by a trusted third party chosen by
each user. To return a volume as shown in Figure 2(b), Alice only
needs to specify the hash of the volume declassiﬁer that she trusts,
h(vd). The secrecy label of the volume declassiﬁer and the argu-
ment it is spawned with (in this case, the volume ID to release)
will be inferred from Alice’s operation submitted to the cloud,
producing a ownership authorization Ta = (h(vd),{a},{volume-
id [IN DEX]}).
4.4 Selecting Nodes for Spawning Handlers
In Pileus, spawning an event handler on a node means that the
user trusts the chosen cloud node (i.e., its kernel and Pileus dae-
mon) to protect her data security. This trust may be misplaced if
the node is not capable of protecting the user’s data security be-
cause it may include adversarial code in its trusted computing base
or be compromised.
Since we do not assume the ability to detect adversarial nodes, in
Pileus, we devise a spawn scheduling algorithm that allows cloud
to act on behalf of users to select cloud nodes that are most likely
to be capable of protecting the user’s data security. This algorithm
leverages users’ security constraints (e.g., a conﬂict-of-interest pol-
icy among users) and functional constraints of cloud vendor to ﬁnd
legal nodes for spawning an event handler for a user. In addition,
the spawn scheduling algorithm aims to select nodes in a manner
that minimizes the additional likelihood that a user’s operation may
be compromised given the current state and history of the node.
The foundation of the spawn scheduling algorithm is a data struc-
n ← i
return
◃ Set of available node that already hold o
◃ A cloud node that satisﬁes cloud policy
Input: Ownership graph G, ownership to delegate o, and cloud policy P
Output: Selected cloud node n
1: for i ∈ TCB(o, G) do
if i meets P then
2:
3:
4:
5:
6: end for
7: Φ ← SORT_BY_CURRENT(G)
8: for S ∈ Φ do
9:
10:
11:
12:
13:
14:
15:
16: end for
S′ ← SORT_BY_HISTORY(S, G) ◃ Further sort nodes by history
for k ∈ S′ do
n ← k
return
◃ Sorted nodes by current users
if k meets P then
Figure 5: Spawn scheduling algorithm
end if
end if
end for
ture that represents the state of the cloud nodes running event han-
dlers, which we call the ownership graph. An ownership graph,
G = (V, E), where: (1) vertices v ∈ V are the set of ownerships
granted per cloud node, v = (o, n), where u is a user’s ownership
and n is a node and (2) edges (u, v) ∈ E are the ownership del-
egations from vertex u on one node to vertex v on a second node.
The ownership graph enables us to reason about two main things:
(1) the set of nodes that are already part of a user’s TCB by virtue
of having been delegated with that user’s ownership and therefore
trusted to protect that user’s data security and (2) the set of own-
erships that must be managed by each node in order for that node
to protect all those users’ data security requirements from attacks
from other nodes.
The spawn scheduling algorithm is illustrated in Figure 9. The
algorithm takes the ownership graph G, the particular ownership
to be delegated o, and the security and functional policies for the
cloud P . First, Pileus will always try to spawn event handlers on
cloud nodes that are already part of a user’s TCB (i.e., the same
cloud node or other nodes that currently hold the user’s ownership).
Only if these nodes are not available (e.g., they do not meet cloud
policies), will the event handler be spawned on another cloud node
because such as choice would expand a user’s TCB.
If a new cloud node must be introduced into a user’s TCB, this
node must also satisfy the cloud policy P and must minimize the
likelihood of compromise. Two metrics are considered to minimize
the likelihood of compromise. First, Pileus will try to spawn on a
node that least number of users are currently using. A best case
scenario is that the user will be the only one on the node. Second,
Pileus will try to spawn on a node with shortest history (i.e., least
number of users have used the cloud node since its last reboot).
This reduces the likelihood of the node being "polluted" (e.g., has
backdoors implanted by attackers) by other users. Ideally, the cloud
node is freshly installed from a clean state3.
4.5 Revoking Authority
Spawn may expand a user’s TCB to new cloud nodes. However,
we do not want the cloud node to be in a user’s TCB forever. Thus,
it is important to revoke the user’s ownership from a cloud node
once the node is no longer involved in serving the user’s opera-
tion, reducing the attack window available to adversary if the node
becomes compromised.
One challenge is to ﬁgure out when a cloud node is no longer
involved in executing a user’s operation. Fortunately, it is straight-
forward when using event handlers in the cloud. Since cloud ser-
3In Pileus, we periodically re-image a cloud node to restore it to a pristine
state, using a mechanism similar to cloud veriﬁer [35].
57
vices are stateless, one request will have only one response. Thus,
when an event handler returns a response, it indicates that the event
handler has completed its job. It is then safe to revoke the user’s
ownership for running that event handler from that cloud node. In
case an event handler never returns (e.g., due to node failure, or an
adversary trying to extend its attack window), a request timeout can
be used as a signal of revocation of 4. However, if a node causes
timeouts frequently, the cloud vendors may detect this behavior as
an anomaly. Using event handler completions for revocation en-
ables Pileus to reduce the temporal attack window for adversaries
to compromise a node running a particular user’s service.
The revocation protocol is shown in Figure 4(b). The revocation
protocol in Pileus leverages the response generated by an event han-
dler q as a signal of revocation. After q completes, it would return a
single response to p (step 1), and response is proxied by Dq to Dp
(step 2). After Dp receives the response, it will send a revocation
request to the OR (step 3), revoking any authority delegated to the
target node via Dq. The OR validates the revocation request using
its ownership graph, and it removes the edge along with the owner-
ships held by the target node in the graph. In addition, the OR will
also purge any authority tokens that are associated with the delega-
tion. This effectively invalidates the ownership held by the Pileus
daemon Dq, so Dq will no longer be able to spawn event handlers
on other cloud nodes using that authority token. The OR conﬁrms
the revocation (step 4), and Dp proxies the response back to the
requesting event handler p (step 5). Compared with an expiration
based strategy [52], revocation in Pileus is more timely. Author-
ities are revoked from a cloud node immediately after the event
handler running on top completes its job, leaving no unnecessary
cloud nodes within a user’s TCB.
5.
IMPLEMENTATION
The Pileus implementation consists of introducing the new,
trusted Pileus services (Section 5.1), decomposing OpenStack
cloud services to run on Pileus (Sections 5.2 and 5.3), and ensuring
Pileus can enforce isolation among users and between handlers and
each node’s privileged processes (Section 5.4).
5.1 Trusted Services
In Pileus, there are two globally trusted services: the Pileus ini-
tiator and the Pileus ownership registry. The Pileus initiator serves
as a trusted portal for cloud users to initiate their cloud operations.
It (1) collects the declassiﬁer or endorser that the user wants to use
in her operation and maps it to the corresponding hash; (2) infers
the input arguments to the declassiﬁer or endorser from the user’s
operation and (3) maps the user’s credentials to corresponding tags.
It then constructs the ownership authority for the user’s operation
and collaborate with the ownership registry to spawn the ﬁrst event
handler (often event handler of the API service) for the user’s oper-
ation. Its code base is ∼500 SLOC. The Pileus ownership registry
maintains the ownership graph, and runs the spawn scheduling pro-
tocol to ensure that the delegation of user’s authority meets cloud
policy.
Its code base is ∼1200 SLOC. Both Pileus initiator and
ownership registry are implemented in Python.
5.2 Event Handlers
The existing code for the OpenStack cloud services forms the
basis for the event handlers in Pileus-OpenStack. Our observa-
tion is that OpenStack cloud services5 are constructed using an
4Current cloud platforms use request timeouts to detect node failures.
5In this work, we focused on 10 cloud services as a proof-of-concept: nova-
api, nova-scheduler, nova-conductor, nova-compute, nova-network, glance-
api, glance-registry, cinder-api, cinder-scheduler and cinder-volume. These
58
event-dispatch loop. When an event arrives at a cloud service, it
is dispatched to a worker thread for processing that event. Across
events, the cloud services retain no persistent state, freeing each
worker thread to run independently on one event, just as we re-
quire for event handlers. We extract the worker thread code from
the cloud services that leverage this event-dispatch loop design to
create event handlers. Each event handler is invoked as part of a
single cloud operation performed by a single user, enabling Pileus
to conﬁne them to the user’s security label.
Event handlers may run helper programs, such as legacy Linux
utilities (e.g., qemu-img), to complete their processing on the host.
These helper programs are ephemeral as well—they are executed
to complete one job and exit after the job’s completion. In Pileus,
helper programs’ processes inherit their labels from the event han-
dlers that invoked them, so they are conﬁned by the Pileus.
The advantage of DIFC approach is that we did not need to un-
derstand all of OpenStack’s code. Majority of event handlers are
decomposed as-is from worker threads of cloud services. Only a
few required modiﬁcation in order to invoke declassiﬁers or en-
dorsers or to interact with the database, as we will discuss later.
The total modiﬁcations we made to event handlers constitute ∼300
SLOC out of the ∼120,000 SLOC (not counting Python libraries
used) for the 10 OpenStack cloud services in our deployment.
5.3 Pileus Daemon
Events in OpenStack are implemented in either one of two forms:
HTTP requests or messages from the message queue. Thus, cloud
services often share the same underlying implementation of the dis-
patcher loop—different types of cloud services only differ in the
speciﬁc implementation of event handlers. Thus, the bulk of Pileus
Daemon implementation comes directly from the dispatcher loop
of cloud services. We mainly augmented it with the ability to man-
age labels and ownerships for event handlers and to execute the
spawn and revocation protocols discussed in the design. The to-
tal code base for Pileus Daemon is ∼5000 SLOC, out of which
∼3,800 SLOC originates from the dispatcher loop of OpenStack
cloud services and ∼1,200 SLOC are our additions.
5.4 Pileus Enforcement Mechanism
Pileus Kernel. Most of the cloud objects that are managed by cloud
services are ﬁles (e.g., images, VM disks). For Pileus, we built a
DIFC kernel to enforce access control over these objects on each
cloud node. The DIFC kernel is mainly implemented as a kernel
security module, leveraging the Linux Security Module hooks [50].
Unlike Flume [21], we did not modify the system call interface. In-
stead, we implemented the label and ownership operations through
an interface similar to device drivers. A user space library is created
for exposing an API abstraction to Pileus Daemon and DIFC-aware
event handlers. The kernel module is ∼9,000 SLOC and the user-
space library is ∼1,200 SLOC.
Network Namespace. Network Service (e.g., nova-network) con-
ﬁgures network objects such as Linux bridges, software switches,
and iptables, on a cloud node, in order to manage networking for
VMs. These network objects are challenging for access control
since they are kernel resources which are traditionally treated as
single objects by the kernel from an access control perspective.
Thus, a vulnerability in an event handler may enable one user to
modify the network conﬁguration of another user. For example,
inappropriate processing of ﬁrewall rules [42] allowed one user to
services implemented the core functionality of a cloud platform. The rest
OpenStack cloud services are designed in a similar fashion that will plan to
include in future.
Bridge
Firewall 
Rules
Bridge
Alice-VM
S = {a}
Bob-VM
S = {b}
Firewall 
Rules
Bridge
Bridge
Firewall 
Rules
Alice-VM
S = {a}
net_ns_1
Bob-VM
S = {b}
net_ns_2
Figure 6: Isolating networking objects via network namespace.
block the network connection of another. To mitigate such vulner-
abilities, Pileus must isolate each user’s network objects in order to
provide effective access control.
To achieve this goal, we leverage the network namespace ab-
straction of the Linux kernel in our implementation. A network
namespace is logically a separate copy of the network stack, with
its own network devices, routes, and ﬁrewall rules. Network objects
can be isolated into different network namespaces. By running each
event handler with access only to its user’s network namespaces,
the Pileus kernel restricts each event handler to its user’s network-
ing objects. Figure 6 shows an overview of our implementation for
nova-network. Each VM is attached to a private Linux bridge that
runs in its own network namespace. To access physical network,
the two private bridges are connected to a host bridge via veth
pairs. The host bridge runs in the native network namespace and
contains physical network interface as one of its ports. When an
event handler speciﬁes a ﬁrewall rule, the rule will be applied to
the private bridge instead of the host one, therefore eliminating its
potential effects on other user’s VMs.
Database. OpenStack relies on legacy database servers to store
database objects (e.g., metadata of user such as their SSH keys). To
extend information ﬂow control to the database objects, we rely on
the security framework built inside of the database servers. SEPost-
greSQL [36] assigns labels to database objects and enforces access
control over all requests to database objects. For each access re-
quest, the label of the requestor as well as the database objects are
passed to a security server (the Pileus kernel in our case), which
performs the access decision, enabling consistent enforcement of
access control for system and database objects. The downside of
this approach is, however, the database server must be fully trusted,
since it runs the enforcement mechanism.
In order to obtain the label of the requestor (i.e., the querying
event handler), we disable all remote connections to the database.
Instead, an event handler requesting database access must spawn
a special event handler, called DB-Client, on the cloud node that
hosts the database server. The DB-Client will inherit its parent’s
label through the Pileus spawn mechanism, and connect to the
database server via a Unix domain socket. The database will obtain
the DB-Client’s label from the socket descriptor (e.g., using the get-
peercon API in libselinux [22]). This prevents an adversarial cloud
node from accessing arbitrary data from the database server, since
an adversarial node can only spawn DB-Clients with the labels of
users for which it holds ownerships.
Libvirtd. In OpenStack, VM objects (e.g., VMs, containers) are
managed by virtualization drivers which are often daemon pro-