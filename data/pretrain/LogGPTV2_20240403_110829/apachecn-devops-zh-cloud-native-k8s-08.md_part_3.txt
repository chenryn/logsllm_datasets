荚间相似性和反相似性允许您根据节点上已经存在的其他荚来决定荚应该如何运行。由于集群中的 pods 数量通常比节点数量大得多，并且一些 Pods 相似性和反相似性规则可能有些复杂，因此如果您在多个节点上运行多个 Pods，该功能会给集群控制平面带来很大的负载。因此，Kubernetes 文档不建议在集群中的大量节点上使用这些功能。
荚果亲和力和反亲和力的工作原理完全不同——在讨论它们如何结合之前，让我们先单独看看它们。
## 荚果亲和力
与节点相似性一样，让我们深入 YAML，讨论 Pod 相似性规范的组成部分:
pod-with-pod-affinity.yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: not-hungry-app-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: hunger
            operator: In
            values:
            - "1"
            - "2"
        topologyKey: rack
  containers:
  - name: not-hungry-app
    image: not-hungry-app:latest
```
就像节点关联性一样，Pod 关联性允许我们在两种类型之间进行选择:
*   `preferredDuringSchedulingIgnoredDuringExecution`
*   `requiredDuringSchedulingIgnoredDuringExecution`
同样，类似于节点相似性，我们可以有一个或多个选择器——称为`labelSelector`，因为我们选择的是 Pods，而不是节点。`matchExpressions`功能与节点相似性相同，但 Pod 相似性增加了一个名为`topologyKey`的全新密钥。
`topologyKey`本质上是一个选择器，它限制了调度程序应该查看同一选择器的其他 Pods 是否正在运行的范围。这意味着 Pod 亲缘关系不仅仅意味着同一节点上的其他相同类型的 Pod(选择器)；它可以表示多个节点的组。
让我们回到本章开头的失败域示例。在该示例中，每个机架都是自己的故障域，每个机架有多个节点。为了将这个概念扩展到`topologyKey`，我们可以用`rack=1`或`rack=2`标记机架上的每个节点。然后我们可以使用`topologyKey`机架，就像我们在 YAML 一样，指定调度程序应该检查在具有相同`topologyKey`的节点上运行的所有 Pods(在这种情况下，这意味着在相同机架中的`Node 1`和`Node 2`上的所有 Pods)，以便应用 Pods 相似性或反相似性规则。
综上所述，我们的例子 YAML 告诉调度程序的是:
*   此 Pod *必须在标签为`rack`的节点上进行调度，标签的值`rack`将节点分成组。*
*   然后，该 Pod 将被安排在一个组中，该组中已经存在一个运行标签为`hunger`且值为 1 或 2 的 Pod。
本质上，我们将集群划分为拓扑域(在本例中是机架)，并规定调度程序只在共享相同拓扑域的节点上一起调度相似的单元。这与我们的第一个故障域示例相反，在该示例中，如果可能的话，我们不希望 pods 共享同一个域，但也有一些原因可能会让您希望将类似的 pods 保留在同一个域中。例如，在多租户环境中，租户希望在一个域上拥有专用硬件租户，您可以确保属于某个租户的每个 Pod 都被调度到完全相同的拓扑域。
同样可以使用`preferredDuringSchedulingIgnoredDuringExecution`。在我们讨论的反亲和力之前，这里有一个关于荚果亲和力和`preferred`类型的例子:
pod-with-pod-affinity2.yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: not-hungry-app-affinity
spec:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: Lt
              values:
              - "3"
          topologyKey: rack
  containers:
  - name: not-hungry-app
    image: not-hungry-app:latest
```
和以前一样，在这个代码块中，我们使用小于(`Lt`)运算符将`weight`(在本例中为`50`)与表达式匹配。这种相似性将促使调度程序尽最大努力在它所在的节点上调度 Pod，或者在同一机架上的另一个节点上调度 Pod，该节点运行的 Pod 的`hunger`小于 3。调度程序使用`weight`来比较节点，如节点关联性一节中所述–*用节点关联性*控制 Pods(参见`pod-with-node-affinity4.yaml`)。具体在这个场景中，`50`的权重没有任何区别，因为亲缘关系列表中只有一个条目。
Pod 反亲和使用相同的选择器和拓扑扩展了这一范式——让我们详细看看它们。
## 荚果抗亲和性
Pod 反关联性允许您防止 Pod 在与匹配选择器的 Pod 相同的拓扑域上运行。它们实现了与 Pod 相似性相反的逻辑。让我们深入一些 YAML，并解释这是如何工作的:
pod-with-pod-anti-affinity . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: hungry-app
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: In
              values:
              - "4"
              - "5"
          topologyKey: rack
  containers:
  - name: hungry-app
    image: hungry-app
```
类似于 Pod 亲和，我们使用`affinity`键作为位置来指定我们在`podAntiAffinity`下的反亲和。此外，与 Pod 相似，我们有能力使用`preferredDuringSchedulingIgnoredDuringExecution`或`requireDuringSchedulingIgnoredDuringExecution`。我们甚至对选择器使用与 Pod 相似性相同的语法。
语法上唯一的实际区别是在`affinity`键下使用了`podAntiAffinity`。
那么，这个 YAML 是做什么的？在这种情况下，我们向调度程序推荐(一个`soft`要求)，它应该尝试在一个节点上调度这个 Pod，在这个节点上，它或任何其他具有相同`rack`标签值的节点没有任何运行有`hunger`标签值为 4 或 5 的 Pod。我们告诉调度程序*尽量不要将这个 PODS 和任何额外的饥饿 PODS*放在同一个域中。
这一特性为我们提供了一种按故障域分离机架的好方法——我们可以将每个机架指定为一个域，并通过一个自己的选择器赋予它一个反关联性。这将使调度程序安排 Pod 的克隆(或者尝试以首选的相似性)到不在同一故障域中的节点，从而在域故障的情况下为应用提供更高的可用性。
我们甚至可以选择结合荚果亲和力和反亲和力。让我们看看这是如何工作的。
## 结合亲和力和反亲和力
这是一种情况，在这种情况下，您可以真正给集群控制平面施加不适当的负载。将 Pod 相似性与反相似性相结合，可以允许将难以置信的细微差别的规则传递给 Kubernetes 调度器，该调度器承担着实现这些规则的艰巨任务。
让我们看一些结合了这两个概念的 YAML 部署规范。请记住，相似性和反相似性是应用于 Pods 的概念，但是我们通常不指定没有控制器(如部署或复制集)的 Pods。因此，这些规则应用于 YAML 部署中的 Pod 规范级别。为了简明起见，我们只显示了这个部署的 Pod 规范部分，但是您可以在 GitHub 存储库中找到完整的文件:
抗肥胖和亲和力兼备的 PODS
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hungry-app-deployment
# SECTION REMOVED FOR CONCISENESS  
     spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - other-hungry-app
            topologyKey: "rack"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - hungry-app-cache
            topologyKey: "rack"
      containers:
      - name: hungry-app
        image: hungry-app:latest
```
在这个代码块中，我们告诉调度程序这样对待我们部署中的 Pod:Pod 必须被调度到带有`rack`标签的节点上，这样它或任何其他带有`rack`标签且相同值的节点就有一个带有`app=hungry-label-cache`的 Pod。
其次，如果可能的话，调度器必须尝试将 Pod 调度到具有`rack`标签的节点，使得它或具有`rack`标签和相同值的任何其他节点没有运行具有`app=other-hungry-app`标签的 Pod。
简而言之，我们希望`hungry-app`的 Pod 运行在与 `hungry-app-cache`相同的拓扑中，如果可能的话，我们不希望它们运行在与`other-hungry-app`相同的拓扑中。
因为权力越大，责任越大，我们的 Pod 亲和和反亲和工具是强大和降低性能的同等部分，所以 Kubernetes 确保对您可以使用它们的可能方式设置一些限制，以防止奇怪的行为或重大的性能问题。
## Pod 亲和力和抗亲和力限制
亲和和反亲和最大的限制是不允许使用空白`topologyKey`。如果不限制调度程序将什么视为单个拓扑类型，可能会发生一些意想不到的行为。
第二个限制是，默认情况下，如果你使用的是反亲和的硬版本–`requiredOnSchedulingIgnoredDuringExecution`，你不能只使用任何标签作为`topologyKey`。
Kubernetes 只会让你使用`kubernetes.io/hostname`标签，这实质上意味着如果你使用`required`反亲和，每个节点只能有一个拓扑。这种限制对于`prefer`反亲和力或亲和力都不存在，甚至对于`required`也不存在。可以更改此功能，但需要编写自定义准入控制器–我们将在 [*第 12 章*](12.html#_idTextAnchor269)*Kubernetes 安全与合规*和 [*第 13 章*](13.html#_idTextAnchor289)*使用 CRDs 扩展 Kubernetes*中讨论。
到目前为止，我们关于放置控件的工作还没有讨论名称空间。然而，对于荚果亲和力和反亲和力，它们确实具有相关性。
## Pod 相似性和反相似性名称空间
由于 Pod 亲和力和反亲和力基于其他 Pod 的位置导致行为的改变，所以命名空间是决定哪个 Pod 支持或反对亲和力或反亲和力的相关部分。
默认情况下，调度程序将只查看创建具有相似性或反相似性的 Pod 的命名空间。对于前面的所有示例，我们没有指定名称空间，因此将使用默认名称空间。
如果您想添加一个或多个 Pods 将影响相似性或反相似性的命名空间，您可以使用以下 YAML:
pod-with-anti-affinity-namespace . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: hungry-app
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: In
              values:
              - "4"
              - "5"
          topologyKey: rack
          namespaces: ["frontend", "backend", "logging"]
  containers:
  - name: hungry-app
    image: hungry-app
```
在这个代码块中，调度程序将在尝试匹配反关联性时查看前端、后端和日志名称空间(如您在`podAffinityTerm`块中的`namespaces`键上所见)。这允许我们在验证调度程序的规则时约束调度程序对哪些名称空间进行操作。
# 总结
在这一章中，我们了解了 Kubernetes 提供的一些不同的控件，以便通过调度程序强制执行某些 Pod 放置规则。我们了解到，既有“硬”要求，也有“软”规则，后者是调度程序尽了最大努力，但不一定能阻止违反规则的 Pods 被放置。我们还了解了您可能希望实施计划控制的几个原因，例如现实中的故障域和多租户。
我们了解到有一些简单的方法可以影响 Pod 的放置，比如节点选择器和节点名称——此外还有一些更高级的方法，比如污点和容忍，Kubernetes 本身也默认使用这些方法。最后，我们发现 Kubernetes 提供了一些高级工具，用于节点和 Pod 关联以及反关联，这允许我们创建复杂的规则集供调度程序遵循。
在下一章中，我们将讨论 Kubernetes 上的可观测性。我们将学习如何查看应用日志，我们还将使用一些优秀的工具来实时查看集群中发生的情况。
# 问题
1.  节点选择器和节点名称字段有什么区别？
2.  Kubernetes 如何使用系统提供的污点和容忍？什么原因？
3.  为什么在使用多种类型的 Pod 亲和力或反亲和力时要小心？
4.  对于三层 web 应用，出于性能原因，您如何平衡跨多个故障区域的可用性和托管？给出一个使用节点或 Pod 亲和力和反亲和力的例子。
# 进一步阅读
*   有关默认系统污点和容忍的更深入的解释，请前往[https://kubernetes . io/docs/concepts/scheduling-驱逐/污点和容忍/#基于污点的驱逐](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions)。