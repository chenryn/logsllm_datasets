performs other 5 fuzzers in terms of vulnerability discovery.
The last three columns of Table 1 show the number of
USENIX Association
29th USENIX Security Symposium    2585
Table 2: Number of unique crashes (average and maximum count in 5 runs) found in real world programs by various fuzzers.
Applications
tiff2pdf
libwpd
ﬁg2dev
readelf
nm
c++ﬁlt
ncurses
libsndﬁle
libbson
tiffset
libsass
cﬂow
nasm
Total
AFL
Average Max
0
0
12
0
0
30
18
13
0
46
0
47
15
181
0
0
8
0
0
18
7
4
0
22
0
9
5
73
CollAFL-br
Average Max
0
3
20
0
0
32
23
20
0
49
0
35
42
229
0
1
11
0
0
7
12
8
0
43
0
17
20
119
Angora
Average Max
0
0
0
27
0
0
0
0
0
0
0
0
12
39
0
0
0
21
0
0
0
0
0
0
0
0
6
27
GREYONE
Average
6
21
40
28
16
268
28
23
6
83
8
32
157
716 (+501%)
Max
12
58
79
38
72
575
37
33
12
122
12
185
212
1447 (+631%)
Table 3: Number of unique paths and edges (average in 5 runs) found in real world programs by various fuzzers. Numbers in red
are path/edge coverages of the second best fuzzer.
Path Coverage
Edge Coverage
Applications
tiff2pdf
readelf
ﬁg2dev
ncurses
libwpd
c++ﬁlt
nasm
tiffset
nm
libsndﬁle
AFL
2638
4519
697
1985
4113
9791
7506
1373
2605
911
CollAFL-br
3278
4782
764
2241
3856
9746
7354
1390
2725
848
Angora
3344
5212
105
1024
1145
1157
3364
1126
2493
942
GREYONE (INC)
5681(+69.9%)
6834(+32%)
1622(+112%)
2926(+30.6%)
5644(+37.2%)
10523(+8%)
9443(+25.8%)
1757(+26%)
4342(+59%)
1185(+25.8%)
AFL
6261
6729
934
2082
5906
6387
6553
3856
5387
2486
CollAFL-br
6776
6955
1754
2151
5839
6578
6616
3900
5526
2392
Angora
6820
7395
489
1736
4034
3684
4766
3760
5235
2525
GREYONE (INC)
8250(+20.9%)
8618(+14.5%)
2460(+40.2%)
2787(+28.2%)
7978(+35.1%)
7101(+8%)
8108(+22.5%)
4361(+11.8%)
8482(+53.5%)
2975(+17.8%)
vulnerabilities that are previously unknown, known by ven-
dors only and conﬁrmed by CVE respectively. We reported
the 105 vulnerabilities we found to upstream vendors, and
learned that 25 of them are known by the vendors (but not the
public). Among the remaining 80 unknown vulnerabilities,
41 vulnerabilities are conﬁrmed by CVE.
4.3 Unique Crashes Evaluation
In general, the more unique crashes a fuzzer ﬁnds, the more
vulnerabilities it could ﬁnd too. Thus, the number of unique
crashes is also an important metric for fuzzers. Due to the
randomness, we evaluated not only the average but also the
maximum number of unique crashes found in 5 runs.
Table 2 shows the detailed evaluation results. GREYONE
outperforms all other fuzzers in all applications. Especially in
tiff2pdf, nm, and libsass, only GREYONE reported unique
crashes and other fuzzers all failed.
Among the 5 runs, GREYONE on average found 716 unique
crashes in all applications, which is 501% more than the sec-
ond best fuzzer (i.e., CollAFL). In the maximum run, GREY-
ONE found 1447 unique crashes in all applications, which is
631% more than the second best fuzzer.
To better examine the efﬁciency of each fuzzer, we also
evaluated the growth trend of unique crashes found by them,
as shown in Figure 14 in the Appendix. It shows that, GREY-
ONE had a steady and stronger growth trend on all applica-
tions. Furthermore, GREYONE is also the ﬁrst fuzzer that
reported crashes in almost all applications.
4.4 Code Coverage Evaluation
Since a fuzzer can only ﬁnd vulnerabilities in code that it has
explored, code coverage is therefore an important metric for
coverage-guided fuzzers.
Table 3 shows the average number of unique paths and
edges found by each fuzzer for ten applications. In addition,
the improvement of GREYONE compared to the second best
fuzzer is also evaluated and showed in the table.
In terms of path coverage, GREYONE outperforms the sec-
ond best fuzzer by at least 25% in 9 out of ten applications.
In the last application c++filt, GREYONE outperforms the
second best by 8%. In terms of new edge coverage, GREY-
ONE outperforms the second best fuzzer in all applications,
on average by 25.5%.
We also evaluated the growth trend of code explored by
fuzzers, and presented the path coverage in Fig 13 and edge
coverage in Fig 16. It shows that GREYONE has an impressive
stronger growth trend than all other fuzzers in all applications.
4.5 Evaluation on LAVA-M
To directly compare the results with other papers, we tested
applications in the LAVA-M data set for 24 hours (rather than
60 hours) and repeated 5 times.
2586    29th USENIX Security Symposium
USENIX Association
Table 4: The number of bugs found by various fuzzer tools on LAVA-M in 24 hours.
who
LAVA-M AFL
0
1
2
1
md5sum
base64
uniq
CollAFL-br
2
3
2
1
Honggfuzz
4
3
6
4
VUzzer
49
12
15
24
CollAFL-br+laf
245
37
44
21
QSYM
1252(+43)
57(+0)
44(+4)
28(+1)
Angora
1438(+95)
57(+0)
44(+4)
28(+1)
GREYONE
2136(+327)
57(+4)
44(+4)
28(+1)
Listed bugs
2136
57
44
28
Table 5: Number of unique paths, unique edge, unique crashes (average count in 5 runs with 60 hours each time) and total
vulnerabilities (5 runs with 60 hours each time) found in real world programs by QSYM-* (QSYM+master AFL+ slave AFL)
and GREYONE-* (GREYONE +slave AFL).
Average Unique Paths
Average Unique Edges
Total Vulnerabilities
Applications
Readelf
Nm
C++ﬁlt
Tiff2pdf
Tiffset
Libwpd
libsndﬁle
Fig2dev
Nasm
libncurses
Average Improvement
QSYM-*
9028
4218
10988
4856
1897
8279
1375
1218
9184
2837
-