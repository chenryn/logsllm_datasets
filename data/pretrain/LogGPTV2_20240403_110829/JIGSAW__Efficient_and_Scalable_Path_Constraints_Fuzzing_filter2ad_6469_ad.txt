sign goal is to improve the search throughput, we first evaluated
JIGSAW’s throughput and compared it with Bitwuzla’s local
search mode. Similar to the previous experiment, all constraints
were first loaded into memory, then passed to the solver one
by one. For each set of constraints, we ran the corresponding
experiments 30 times and report the average and standard
deviation. Table V shows the result. The first half is for nested
branch constraints and the second half is for the last branch
constraints. On nested branch constraints, our search throughput
926
ranges from 61.1K to 4.6M inputs/sec with a single thread.
For last branch constraints, as fewer functions need to be
evaluated, our search throughput is much higher, ranging from
755.3K to 13.4M inputs/sec. The throughput on nm is much
higher than others because only about 25% of the collected
constraints are solvable; so JIGSAW spent more time searching
for a result with the JIT’ed functions. To put these numbers
into context, Angora’s search throughput ranges from 58 (file)
to 3363 (libpng) inputs/sec on the same machine. On average
(geomean across all programs) JIGSAW’s throughput (on nested
branch constraints) is about two orders of magnitude higher
(373×). Compared to Bitwuzla, JIGSAW’s throughput is also
much higher. Based on this experiment, we believe the answer
to RQ1 is yes: our approach indeed can significantly improve
the search throughput.
Single-thread Branch Flipping Rate. Next, we compared
our branch flipping rate with popular SMT solvers. As shown
in Table VI, JIGSAW’s branch flipping rate is also very good
when compared to SMT solvers: JIGSAW can beat other solvers
on branch flipping rate, including Yices2 and Bitwuzla (because
of the shorter timeout setup). On average, JIGSAW is 14.4×
faster than Z3 on solving nested branch constraints and 119.7×
faster on solving single branch constraints. Based on this
comparison, we believe the answer to RQ2 is yes: when the
search throughput is high enough, even with a simple search
heuristic, JIGSAW can flip branches faster than state-of-the-art
tools.
Solving Time Breakdown. Table VII shows the accumulated
time spent on different components of JIGSAW (preprocessing,
JIT, and fuzzing), and the average function cache hit rate. The
timeout is at 1K iterations. As we can see, even with a high
cache hit rate (99.99%), a significant portion of time is still
spent on JIT compilation. Therefore, the performance could
be worse if without the code cache or with an even slower
JIT procedure (e.g., that used by [53, 61]). Similarly, we can
further improve the performance by using a faster JIT engine
and by making the cache persistent.
Effectiveness of Function Cache. To better understand
the impact of our normalized AST to function cache, we
did a comparison using 20K constraints from readelf. The
result is shown in Table VIII. As we can see, when we
enable cache with full AST matching, the cache hit rate
is only 66.9%, the JIT time is reduced by 64.3%, and the
throughput is mildly increased by 72.1%, compared to no
function cache. With our optimization that normalizes the AST
before matching, the cache hit rate increases significantly to
99.9%. The corresponding JIT time is reduced by 97.9%, and
the throughput is increased by 3.3× compared to no function
cache.
Multi-thread Performance.
In this subsection, we evaluate
JIGSAW’s scalability to multiple cores. We focus on two main
performance metrics: search throughput and branch flipping
rate. We tested with 8-, 16-, 24-, 32-, 40-, and 48-threads, each
thread is pinned to a real CPU core (not hyper-thread). For
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:56 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VI: The branch flipping rate of single thread JIGSAW and comparison with popular SMT solvers. BZLA is the abbreviation of
Bitwuzla. The first half of the table shows the results of nested branch constraints; the second half shows the results of single branch
constraints.
Nested Branch Constraints
Single Branch Constraints
Program
objdump
size
nm
readelf
libpng
tiff2pdf
file
tcpdump
openssl
sqlite3
vorbis
mbedtls
libxml2
libjpeg-turbo
Geomean
Yices2 BZLA-LS-100K BZLA-6MS
23.9
134.2
114.1
436.1
4540.4
486.4
52.6
18.1
53.0
870.7
55.2
220.9
26.8
104.1
147.1
1298.6
2.4
44.3
25429.2
93.1
42.2
3531.4
2.1
204.4
19.3
54.3
281.4
22.7
26.7
54.8
26.4
155.1
29.9
1448.3
14.6
6.3
46.4
4.1
40.3
STP Z3-50MS JIGSAW-1K Yices2 BZLA-LS-100K BZLA-6MS
38.6
132.7
754.7
82.5
269.6
44.1
43.5
435.5
46.6
1338.0 2622.4
36.1
12.7
562.0
4.5
100.1
300.0
1296.0
6679.5
433.9
736.1
155.5
267.7
1850.1
303.4
12510.5
168.3
98.4
3925.3
38.3
588.9
21.9 K
13.9 K
39.7 K
11.5 K
14.4 K
40.5 K
22.4 K
23.7 K
6.6
70.2 K
1.2 K
4.5 K
44.0 K
92.4
6.8 K
0.6 K
0.6 K
1.3 K
0.4 K
0.9 K
1.5 K
0.9 K
0.7 K
1.1 K
2.3 K
0.3 K
0.8 K
2.3 K
192.6
0.8 K
STP Z3-50MS JIGSAW-1K
73.4 K
34.7 K
41.3 K
38.7 K
28.8 K
98.8 K
61.0 K
27.7 K
16.3 K
156.2 K
8.0 K
14.4 K
113.7 K
8.2 K
35.7 K
0.5 K
0.6 K
0.5 K
1.0 K
0.1 K
1.2 K
0.4 K
0.5 K
0.8 K
2.0 K
0.2 K
16.3
0.3 K
53.7
0.3 K
0.9 K 2.4 K
0.7 K 1.1 K
1.4 K 4.4 K
0.6 K 1.1 K
0.9 K 1.2 K
1.9 K 10.2 K
1.1 K 2.9 K
1.1 K 4.0 K
1.1 K 0.6 K
2.3 K 7.7 K
0.4 K 2.0 K
0.4 K 0.3 K
2.1 K 5.7 K
226.5
214.7
0.9 K 1.2 K
22.5
54.4
294.5
42.9
120.3
24.6
14.0
92.5
21.7
896.5
7.0
5.3
191.6
4.7
41.0
23.4
6.8
488.5
2.6
61.0
TABLE VII: Accumulated solving time breakdown of JIGSAW, when
solving all constraints using 1000 iterations as the timeout.
Preprocessing
1328s
JIT
462s
Searching Cache Hit Rate
4403s
99.99%
TABLE VIII: Benefits of using function cache, when solving 20,000
constraints from readelf.
Hit Rate JIT Searching
Caching
N/A 33.9s
Disabled
Full AST
66.9% 12.1s
Normalized AST 99.9% 0.7s
12.6s
12.6s
12.6s
Throughput
229K inputs/s
394K inputs/s
747K inputs/s
comparison, we also tried multi-threaded Z3 where each thread
uses a separate Z3 context and solver.
Figure 3 shows
the results. Overall, adding more
threads/cores can help JIGSAW increase the throughput and
branch flipping rate. The geomean of JIGSAW’s throughput can
reach 12.5M inputs/sec for solving nested branch constraints
and 74.7M inputs/sec for solving single branch constraints. The
geomean of JIGSAW’s branch flipping rate can reach 11.3K
branches/sec and 860.0K branches/sec, respectively. For Z3,
we did not observe much improvement when adding more
parallelism, due to lock contention. Based on this experiment,
we conclude that the answer to RQ3 is yes: our approach can
scale well to multiple cores.
To put the numbers into context, Xu et al. reported a
Fig. 3: Average search throughput and branch flipping rate of JIGSAW
on multiple cores.
1027
throughput of around 6.5M inputs/sec when fuzzing libpng
with libFuzzer, using 120 CPU cores and their new OS
primitives [74]. For libpng, the peak throughput of JIGSAW,
using 48 cores, can reach 18.1M inputs/sec for solving nested
branch constraints and 36.9M inputs/sec for solving single
branch constraints; which is about 22.1× and 30.9× faster than
single thread mode, respectively. The corresponding branch
flipping rate can reach 15.0K branches/sec for solving nested
branch constraints and 895.1K branches/sec for solving single
branch constraints; which is also about 21.4× and 31.0× faster
than single thread mode, respectively.
B. End-to-End Fuzzing Performance
In this subsection, we evaluate the effectiveness of JIGSAW
on coverage-guided test generation. We choose the Z3 solver as
the main comparing target in the end-to-end fuzzing evaluation
for the considerations below:
• Z3 is widely adopted by recently concolic executors, such
as QSYM [77], SymCC [56], SymQEMU [57], and Fuz-
zolic [10]. Using Z3 makes it easier to tell how much
performance gain is from our DFSan-based constraint
collection engine, and how much is from JIGSAW.
• All other solvers are not as robust as Z3 and can get stuck
in the middle of a fuzzing campaign because they either do
not provide APIs to specify a timeout (Yices2) or that API
does not work well (STP).
Concolic Execution Performance. We first compare JIGSAW
with other state-of-the-art concolic execution (CE) engines and
fuzzers on flipping all symbolic branches along execution traces
of a fixed set of seeds. As argued in [57], this experiment setup
removes the path scheduling variable from the comparison so
the result can better reflect the end-to-end branch flipping
performance (i.e., path constraints collection + constraints
solving). The first two configurations to compare are Z3-10s
and Z3-50ms, which share the same hybrid fuzzing driver as
JIGSAW but use Z3 as the solver. The 10 seconds timeout is the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:56 UTC from IEEE Xplore.  Restrictions apply. 
181624324048#Core020004000600080001000012000Thousand inputs/secThroughputFlipping Rate020004000600080001000012000Flipping rate (branches/second)Nested branches181624324048#Core01020304050607080Million inputs/secThroughputFlipping Rate0100200300400500600700800900Flipping rate (K branches/second)Last branchTABLE IX: Comparison of concolic execution engines on flipping all
symbolic branches along a single execution trace. The top half shows
the execution time, the bottom half shows the basic-block coverage
measured by SanitizerCoverage.
Programs JIGSAW Z3-10s Z3-50ms Angora SymCC Fuzzolic
48.2h
readelf
52.2h
objdump
nm
48.2h
5.2h
size
20.9h
libxml2
5843
readelf
objdump
4689
3123
nm
2259
size
libxml2
6022
12.6h
89.5h 546.6h
29.6h 411.5h 373.5h
3.2h
29.3h