C4.4
1
2
4
8
N/A
Price
$0.085 / hr
$0.34 / hr
$0.68 / hr
$0.015 / hr
$0.03 / hr
$0.06 / hr
$0.12 / hr
$0.24 / hr
$0.48 / hr
$0.96 / hr
$0.10 / CPU hr
$0.12 / hr
$0.24 / hr
$0.48 / hr
$0.96 / hr
Table 3: Information of the cloud instances we benchmark. With C3,
the ﬁrst six CPU hours per day per application are free.
Some providers offer both Linux and Windows instances with
the latter being slightly more expensive due to licensing fees. For
experiments that depend on the type of OS, we compare instances
from both OSes. For others, we choose Linux instances to reduce
the cost of our experiment.
Figure 1 shows the ﬁnishing time of a CPU intensive task, a
memory intensive task, and a disk I/O intensive task. Each bar
shows the median and the 5th/95th percentiles of the measured
samples. The same convention is used for all other ﬁgures with-
out special notice. We omit the results of other benchmark tasks
that show similar trends. For each instance type, we instantiate 10
instances and repeat each task 20 times per instance, i.e., a total of
200 samples per task per instance type. We only show the ﬁrst four
instance types of C2 because the others have similar performance
for the reason we soon describe. The I/O and multiple-threaded re-
6k
s
a
T
r
e
p
t
s
o
C
d
e
z
i
l
a
m
r
o
N
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
single thread
multiple thread
.
1
1
C
.
2
1
C
.
3
1
C
.
1
2
C
.
2
2
C
.
3
2
C
.
4
2
C
3
C
.
1
4
C
.
2
4
C
.
3
4
C
.
4
4
C
k
s
a
T
r
e
p
t
s
o
C
d
e
z
i
l
a
m
r
o
N
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
single thread
multiple threads
.
1
1
C
.
2
1
C
.
3
1
C
.
1
2
C
.
2
2
C
.
3
2
C
.
4
2
C
3
C
.
1
4
C
.
2
4
C
.
3
4
C
.
4
4
C
k
s
a
T
r
e
p
t
s
o
C
d
e
z
i
l
a
m
r
o
N
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
single thread
multiple threads
.
1
1
C
.
2
1
C
.
3
1
C
.
1
2
C
.
2
2
C
.
3
2
C
.
4
2
C
.
1
4
C
.
2
4
C
.
3
4
C
.
4
4
C
(a) CPU
(b) Memory
(c) Disk I/O
Figure 2: The per-task monetary cost on each type of cloud instance.
sults are not available for C3 because it does not support local disk
access or multi-threading.
From the results, we can see that price-comparable instances of-
fered by different providers have widely different CPU and memory
performance. For example, C4.1 and C1.1 are in the same pricing
tier with the former being only 30% more expensive per hour per
instance, but twice as fast as the latter. Alternatively, C1.2 and C1.3
offer better performance than their counterparts from C4 and are on
average 50% more expensive.
Note that across providers the instance types appear to be con-
structed in different ways. For C1, the high-end instances (C1.2
and C1.3) have shorter ﬁnishing times in both single and multiple
threaded CPU/memory tests. This is perhaps due to two reasons.
First, besides having more CPU cores, the high-end instances may
have faster CPUs. Second, the low-end instance may suffer from
higher resource contention, due to high load and poor resource mul-
tiplexing techniques (e.g., CPU time sharing) that open a tenant to
interference from other colocated tenants.
In contrast, for C4, the ﬁnishing times do not improve signif-
icantly for the single threaded tests when we alter the instances
from low-end to high-end, while the amortized running times of
the multi-threaded tests are greatly reduced. This suggests that all
the C4 instances might share the same type of physical CPU and
they either have similar levels of resource contention or are better
at avoiding interference.
Interestingly, instances of C2 have the same performance re-
gardless of their prices. This might be explained by the work-
conserving CPU sharing policy of C2, where a virtual instance can
fully use all physical CPUs on a machine if there is no contention,
and when colocated instances compete for CPUs, the high-end in-
stances are given larger weight in the competition. Under such
policy, we expect to observe interference and poor performance at
times of high load. However, we found this to never happen in our
experiments, suggesting that C2’s data centers were lightly loaded
throughout our experiment period.
Unlike CPU and memory intensive tasks, the disk I/O intensive
task exhibits high variation on some C1 and C4 instances, probably
due to interference from other colocated instances [22]. Further, the
multi-threaded I/O performance is worse than the single-threaded
performance, perhaps because interleaved requests from multiple
threads are harder to optimize than requests from the same thread.
On the contrary, instances from C2 are much more stable perhaps
due to better I/O scheduling techniques or lightly loaded physical
machines.
5.1.1 Performance at Cost
Figure 2 shows the monetary cost to run each task. We see that
for single-threaded tests, the smallest instances of most providers
are the most cost-effective compared to other instances of the same
providers. The only exception is C1.1, which is not as cost-
)
s
(
e
m
T
i
 800
 700
 600
 500
 400
 300
 200
 100
 0
Provisioning Latency
Booting Latency
Total Latency
C1
Linux
C1
Win
C2
Linux
C2
Win
C4
Win
Figure 3: The scaling latencies of the lowest end instance of each cloud
provider.
effective as C1.2, because the latter has much higher performance
due to faster CPU or lower contention.
Surprisingly, for multi-threaded tests the high-end instances such
as C1.3 and C4.4 with more CPU cores are not more cost-effective
than the low-end ones. There are two possible reasons. First, the
prices of high-end instances are proportional to the number of CPU
cores, and thus do not provide any cost advantage per core. Sec-
ond, although high-end instances are assigned more CPU cores,
they still share other system resources such as memory bus and I/O
bandwidth. Therefore, memory or I/O intensive applications do not
gain much by using high-end instances as long as the applications
do not run out of memory or disk space. This suggests that for
parallel applications it might be more cost-effective to use more
low-end instances rather than fewer high-end ones.
5.1.2 Scaling Latency
Finally, we compare the scaling latency of various providers’ in-
stances. To save cost, we only measure the scaling latency for the
smallest instance of each provider. For providers that support both
Linux and Windows instances, we test both choices to understand
how different OSes affect the scaling latency, especially the boot-
ing latency. We run Ubuntu 9.04 for Linux instances and Windows
Server 2008 for Windows ones. For each cloud and each OS type,
we sequentially allocate 20 instances and measure the time between
the request for a new instance and when that instance becomes
reachable. We attribute the kernel uptime of the instance once it
becomes available to be the time to boot and the remaining latency
as the time to provision or otherwise set up the VM. We drop C3
here because it does not allow manual requests for instances.
Figure 3 shows the scaling latencies for three clouds and dif-
ferent OS types. All cloud providers can allocate new instances
quickly with the average scaling latency below 10 minutes. C1
and C2 can even achieve latency within 100 seconds for Linux in-
stances. The latency of C4 is larger. We see that across providers,
Windows instances appear to take longer time to create than Linux
7 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u