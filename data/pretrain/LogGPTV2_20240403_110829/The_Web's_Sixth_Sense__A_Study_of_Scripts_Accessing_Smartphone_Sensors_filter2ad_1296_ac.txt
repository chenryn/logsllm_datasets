WPM captures HTTP data in the browser (not on the wire, after
it leaves the browser), our analysis was able to cover encrypted
HTTPS data as well.
Figure 3: Overlap across different datasets.
Table 6: Domains of the scripts that send spoofed sensor data
to remote servers.
Domain (PS+1)
Top
site
498
b2c.com
247
perimeterx.net
1136
wayfair.com
3616
moatads.com
22935
queit.in
982
kayak.com
1573
priceline.com
541
fiverr.com
4470
lulus.com
5860
zazzle.com
∗ ‘A’: accelerometer, ‘G’: gyroscope, ‘O’: orientation, ‘P’: proximity, ‘L’: light
Sensors∗ Encoding
A, O, P, L
A
A
O
A, O
A
A
A
A
A
# of
sites
53
45
7
5
3
1
1
1
1
1
base64
base64
base64
raw
raw
base64
base64
base64
base64
base64
4.3 Crawl Comparison
In this section, we compare the results from our three data sets,
US1, US2, and EU1. Figure 3 highlights the overlap and differences
between the three crawls, presented as a Venn diagram. We conjec-
ture that there are two main reasons for the observed differences
between the results. First, most popular web sites are dynamic and
change the ads, and sometimes the contents, that are displayed
with each load. This is supported by the fact that although there
are significant differences between the sites where sensors were
accessed, the overlap between script URLs and domains is generally
high.
Second, the location of the crawl appears to make a difference.
The script domains in the two US crawls have more overlap (Jaccard
index 0.86) than when comparing either US crawl to the one from
the EU (Jaccard indices 0.79 and 0.83), even though all three were
collected around the same time period. The absolute number of
sites accessing sensors in the EU crawl was also smaller than in
the US crawls by about a third (EU1: 2 469, US1: 3 695, US2: 3 400).
It is possible that stricter privacy regulation in the EU, such as
the EU’s General Data Protection Regulation (GDPR) [32], may be
responsible for this disparity, but we leave a full exploration of this
question as future work.
5 UNDERSTANDING SENSOR USE CASES
Having identified scripts that access sensor APIs, we next focus
on identifying the purpose of these scripts. To make this analysis
tractable, we first use clustering to identify groups of similar scripts
and then manually analyze the sensor uses cases.
5.1 Clustering Methodology
Clustering Process. In this section we will briefly describe the
overall clustering process. We cluster JavaScript programs in three
phases to generalize the clustering result as much as possible, and
to accommodate for clustering errors that may have been caused
by random noise introduced by the potentially varying behavior of
scripts, such as incomplete page loads or intermittent crawler fail-
ures. Figure 4 highlights the three phases of the clustering process.
JS
DBSCAN
Merge
Classify
Cluster
Figure 4: The three phases for clustering scripts.
In the first phase, we apply off-the-shelf DBSCAN [69], a density-
based cluster algorithm, to generate the initial clusters, using the
script features described in Section 3. In the second phase, we try
to generalize the clustering results by merging clusters that are
similar. We do this in an iterative manner where in each round we
determine the pair of clusters, merging which would result in the
least amount of reduction in the average silhouette coefficient.5 This
process is repeated until any new merges would reduce the average
silhouette coefficient reduced by more than a given threshold (δ).
In the last phase, we try to see if certain samples that are catego-
rized as noisy can be classified into one of the other core clusters.
The reason behind this step is to see if certain scripts were incor-
rectly clustered due to differences in their behavior across different
5Here, we only consider clusters that are not labeled as noisy
1012832421156166512096US1US2EU1No.ofsites100200798411314624US1US2EU1No.ofscriptURLs24355817236498US1US2EU1No.ofscriptdomainswebsites. The same script may exhibit a different behavior when
publishers (first parties) use different features of the script, or when
the script execution depends on the loading of dynamic content
such as ads. To perform classification we use a random forest clas-
sifier [70], where the non-noisy cluster samples, labeled with their
corresponding cluster label, serve as the training data. We then
try to classify the noisy samples as members of one of the core
clusters. We relabel the scripts only if the prediction probability by
the classifier is greater than a given threshold (θ). Pseudo-code (in
Python) for the three phases is provided in appendix A.
Validation Methodology. To validate our clustering results and to
determine the different use cases for accessing sensor data we take
the following two steps: First, we generate an average similarity
score per cluster by computing the pairwise code difference between
two scripts using the Moss tool [3]. Next, if the average similarity
score for a given cluster is lower than a certain threshold (ϵ), we
manually analyze five random scripts from that cluster, otherwise
(if higher than the threshold) we manually analyze three random
scripts per cluster.
For manual analysis we follow a protocol of steps given below:
• Inspect the code description, copyright statements, software
license, links to public repositories, if any, to search for any
stated purpose of the script.
• Statically analyze the registered sensor event listeners to
determine how sensor data is used.
• If static analysis fails (e.g., due to obfuscation), load a page
that embeds the script and debug over the USB, using Chrome
developer tools with break points enabled for sensors event
listeners, to analyze runtime behavior.
• Check if sensor data is leaving the browser, i.e., if the script
makes any HTTP/HTTPS requests containing sensor data.
• If the script sends some encoded data, try decoding the pay-
load.
5.2 Clustering Scripts
We next describe the detailed process and results of clustering the
scripts. We first try to cluster scripts based on low-level features de-
scribed in section 3.4. Recall that low-level features include browser
properties accessed (by either get or set method) and function calls
made (using call or addEventListener) by the script. The reason
behind the use of low-level features is that it provides us with
a comprehensive overview of the script’s functionality. We start
by only considering scripts that access any of the four sensors
we study: motion, orientation, proximity or light. We found 916
such scripts in our US1 dataset. Next, we cluster these scripts us-
ing DBSCAN [69]. Figure 5 highlights the clustering results where
the x axis represents the silhouette coefficient per cluster. We see
that there are 39 distinct clusters generated by DBSCAN, of which
around 24% scripts are labeled as noisy (i.e., scripts that are labeled
as ‘-1’). The red and blue vertical lines in the figure present the
average silhouette coefficient with and without the noisy samples,
respectively.
In order to generalize our clustering results we attempt to merge
similar clusters, i.e., clusters that result in the least amount of re-
duction in silhouette coefficient when merged (see appendix A for
code). We set the total reduction in silhouette coefficient to 0.01
Figure 5: Clustering scripts based on low-level features.
(i.e., δ = 0.01). Doing so reduces the total number of clusters to 36
but certain clusters such as cluster number 37 becomes a bit more
noisy. Figure 6 highlight the merged clustering results.
Figure 6: Merging similar clusters until average silhouette
coefficient reduce by 0.01.
Finally, we check if certain noisy samples (i.e., scripts that are
labeled as ‘-1’) can be classified into one of the other core clusters
−0.8−0.6−0.4−0.20.00.20.40.60.81.0SilhouettecoeﬃcientvaluesClusterlabel-1012345678910111213141516171819202122232425262728293031323334353637−0.8−0.6−0.4−0.20.00.20.40.60.81.0SilhouettecoeﬃcientvaluesClusterlabel-10134567891011121314151617181920212225262728293031323334353637with a certain probability. To do this we use a random forest clas-
sifier, where scripts from non-noisy clusters (i.e., clusters that are
labeled with a value ≥ 0) are used as training data and scripts that
are noisy are used as testing data. We only relabel noisy samples
if the prediction probability, θ ≥ 0.7.6 Also we update labels in
batches of five samples at a time. Figure 7 highlights the outcome
of this final step. This reduces the total fraction of scripts labeled as
noisy from 24% to 21%. However, as evident from Figure 7 this also
increases the chance of certain clusters becoming slightly more
noisy (e.g., clusters 16). The average silhouette coefficient with-
out the remaining noisy samples (i.e., ignoring the cluster labeled
as ‘-1’) after this phase is close to 0.8 which is an indication that
the clustering outcomes are within an acceptable range. For un-
derstanding the impact geo-location, we also ran our clustering
techniques on the EU1 dataset and obtained similar results. We
found a total of 46 clusters with approximately 23% of the scripts
labeled as noisy. In section 5.4, we will briefly discuss how in spite
of the total number of clusters being larger compared to the US1
dataset they represent similar use cases.
We plot the distribution of these scores in Figure 8. We note
that scripts within the same cluster tend to have high similarity; in
particular, 81% of pairs have a similarity score exceeding 0.7. Like-
wise, scripts from different clusters tend to be dissimilar, with 94%
of samples showing a similarity score of 0.1 or less. This suggests
that the clusters are identifying groups of scripts that have high
source-level similarity.
We also compute the average pairwise similarity score for each
cluster to guide our manual analysis. For clusters with high average
pairwise similarity scores (ϵ > 0.7), we manually inspect three
randomly-chosen scripts from each cluster. For clusters with lower
similarity scores, we inspect five random scripts per cluster.7
Figure 8: Distribution of intra- and inter-cluster similarity.
5.4 Real-world Use Cases
Table 7 summarizes the different use cases that we have identified
through our manual inspection. The table also highlights the aver-
age pairwise code similarity score per cluster, computed through
Moss [3]. It should be noted that the high similarity scores likely
result from sites using or copying code from common libraries,
whereas the low scores result from scenarios where only small
parts of the scripts were either reused or copied from other scripts.
We see that there are broadly seven different use cases for accessing
sensor data, among which around 37% of the scripts collect sensor
data to perform tracking and analytics such as audience recogni-
tion, ad impression verification, and session replay. We also see that
around 18% scripts use sensor data to determine whether a device
is a smartphone or a bot to deter fraud. Interestingly, 70% and 76%
of the scripts described in these two categories, respectively, have
been identified to be doing some combination of canvas, webrtc,
audio_context or battery fingerprinting.
We found similar use cases for the EU1 dataset. Around 19% of the
scripts were found to use sensor data to distinguish bots from real
smartphones. We did, however, see somewhat a lower percentage of
scripts (around 31%) involved in tracking and analytics. We found
this group of scripts to be loaded on only 330 sites whereas for the
US1 dataset this number was more than three times bigger (1198).
7For any clusters with five scripts or fewer, we manually inspect all the scripts.
Figure 7: Classifying noisy samples using non-noisy sam-
ples as ground truth, only if prediction probability ≥ 0.7.
5.3 Validating Clustering Results
We use the Moss [3] service, which measures source code similarity
to detect plagiarism, in order to validate the results of our clustering.
We use Moss to calculate the similarity scores between pairs of
scripts from the same cluster. For comparison, we also calculate the
similarity between pairs of scripts from different clusters; in this
case, we limit ourselves to five random scripts per cluster due to
the rate limitations imposed by Moss.
6This value was empirically set by manually spot checking how effective the classifi-
cation results were.
−0.8−0.6−0.4−0.20.00.20.40.60.81.0SilhouettecoeﬃcientvaluesClusterlabel-10134567891011121314151617181920212225262728293031323334353637020406080100Similarityscore(%)0.00.10.20.30.40.50.6ProbabilityinterclustersimilarityintraclustersimilarityTable 7: Potential sensor access use cases.
# of Sites Ranked ...
Avg. Sim.(%)
1–1K 1K–10K 10K–100K per cluster
Cluster ID∗
34
7, 19, 20
0, 3, 5, 12, 18
22, 25, 27, 31
26, 30
6, 14, 33
8, 11, 21, 28, 29, 32
1, 4, 9, 10, 13, 15
16, 17, 35, 36, 37
-1
∗ Clusters with bolded IDs consist of scripts that have been identified as performing some combination of canvas, webrtc, audio_context or battery fingerprinting
99
89, 91, 43
93, 69, 23, 95, 99
92, 36, 81, 83
37, 60
97, 98, 99
99, 96, 99, 11, 43, 89 Reacting to orientation, tilt, shake [35, 42, 45]
99, 68, 98, 99, 92, 99 Tracking, analytics, fingerprinting and
91, 91, 38, 87, 40
4
Description
Use sensor data to add entropy to random numbers [77]
Checks what HTML5 features are offered [22, 24, 54]
Differentiating bots from real devices [33, 64]
Parallax Engine that reacts to orientation sensors [86]
Automatically resize contents in page or iframe [10]
audience recognition [34, 75]
Scripts clustered as noisy
% of
JS
0.4
11.2
17.7
3.4
3.2
6.7
36.8
20.6
# of
Sites
4
114
413
35
103
533
1198
1804
0
2
10
1
0
18
24
16
0
32
53
2
9
118
144
176
4
80
350
32
94
397
1030
1612
5.5 Analysis of Specific Scripts
While manually analyzing scripts for clustering validation, we un-
covered interesting uses of the sensor data. Here we will briefly
discuss two such scripts. The first script comes from doublever-
ify.com [26], an ad impression verification company. One of their
scripts computes statistical features such as average and variance
of motion sensor data before sending it to their remote server. Such
statistical features have been shown to be useful for fingerprinting
smartphones [19]. The code segment is provided in appendix B
(Listing 2). Since this script was sending statistical data instead