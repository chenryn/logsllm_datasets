Fig. 19: Earpiece vs radar power levels in normal and noisy setting
audio is feeble and becomes easily buried under the noise floor.
The power levels when the earpiece is playing is close to 0dB
relative to ambient power levels, thus it is hard to distinguish
whether the earpiece is playing sound or silent. The figure
also depicts the performance of radar under both conditions.
Evidently, the existence of external noise does not interfere
with the signal strength of detection by the radar since the
radar picks up the vibrations directly from the source of the
vibration. In addition, Fig 20 shows samples of spectrograms
from testing in the noisy environment with the loudspeaker.
The microphone spectrum mainly consists of white noise
Fig. 20: Audio spectrograms (range = 3ft) from (a) Raw radar data
(b) Reconstructed audio by mmSpy before masking (b) Reconstructed
audio by mmSpy after masking (c) Ground truth (d) Microphone co-
located with the radar. mmSpy’s spectra closely match the ground
truth whereas a co-located microphone only detects noise.
whereas the detection from mmSpy matches closely with true
audio. mmSpy detects vibrations from the source (earpiece),
thus free of interference from ambient sound.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
0102030405060User ID020406080100Accuracy(%)All DataPixel (77GHz)S20 (60GHz)S20 (77GHz)S20 (Sp, 77GHz)10203040506070Accuracy (%)With AdaptationWithout Adaptation123456Range (ft)01234PowerLevel(dB)Earpiece (w. ambient noise)Earpiece (w. noise jam)Radar Data (w. ambient noise)Radar Data (w. noise jam)Robustness to Electromagnetic Interference: We evaluate
the robustness with respect
to four kinds of interference
(i) Interference in the microwave spectrum as generated by
software defined radios USRP N210 [24] in 2.4 GHz spectrum
with a bandwidth of 5 MHz. This can emulate interference due
to a number of real world electronic devices such as microwave
Owens, Bluetooth streaming, Zigbee sensors etc. (ii) Interfer-
ence in the 2.4 GHz and 5 GHz spectrum using off the shelf
WiFi hardware (MacBook Pro 15) that generates video traffic.
(iii) Interference in the mmWave spectrum using network de-
vices based on 802.11ad protocol with Netgear XR700 router
[26] and a MG360 network adapter [14] that generate video
traffic. (iv) Interference in the mmWave spectrum caused by
other radar devices (IWR6843ISK). In all cases, the interferer
was co-located with the radar so as to measure the performance
under the most challenging case. Fig. 21 summarizes the
results. The results are taken from IWR6843ISK radar placed
at a distance of 3ft from the phone (“S20 (60 GHz) setting”).
As expected, the microwave spectrum effectively does not have
any influence on the mmWave radars since they operate in
different frequency bands. On the other hand, even though the
mmWave interference can happen in the same frequency band
as the radar, our experiments reveal that this does not affect
the accuracy in any significant way. This is because of the
following reasons (i) The networking protocols use traditional
modulation schemes such as OFDM [65], whereas radar uses
FMCW. Because of the difference in modulation, the OFDM
or other non-FMCW signals will have less interference on a
FMCW radar that primarily latches onto chirps. (ii) Another
FMCW radar ceases to have any interference. The lack of
clock synchronization will create an interference peak at the
radar at a different position than the reflection from phone.
This is automatically eliminated by the static multipath elim-
ination algorithms in mmSpy. Moreover, automatic filtering at
the hardware level can typically happen even for a small clock
offset. This observation is consistent with the documentation
by Texas Instruments [13].
Fig. 21: Accuracy under various interference settings (range = 3ft)
Train/Test split across Phone Models and Frequency
Bands: We discuss the feasibility of training and testing on
different brands of phones in Appendix A.
Effect of Hand Coverage of Phone: We have been able to
extend the evaluation to include humans holding the phone in
the hand (as if engaged in a casual conversation) as depicted
in Figure 22. The distance between the phone and the radar is
approximately 3ft. Our results indicate that a sufficient amount
of information still exists despite two interfering factors: (i)
Fig. 22: A test subject holding the phone at a distance of 3 ft from
the radar device.
Partial coverage of the body of the phone by the human hand.
(ii) Any micro-motion (due to breathing, heartbeats, muscular
twitches, etc.) in the human body which interferes with the
audio sensing. Figure 23a shows the raw audio captured when
the phone is held in the hand, with the audio content on the
phone being “four”. The artifacts due to body motion and
vibrations are evident from the figure. Figure 23b shows the
zoomed-in version of Figure 23a where the audio contents
are now visible. We apply a simple threshold to eliminate the
body motion artifact (caused due to micro-motions), as well
as interpolation to fill out the gap after eliminating the body
motion artifacts. The recovered audio is depicted in Figure
23c. We process the recovered audio with the classification
model presented in Figure 11 (from the paper submission).
The accuracy under “S20 (77 GHz)” setting (the dataset being
AudioMNIST) is 58.8%. In contrast, when the phone was held
on a tripod, the corresponding accuracy at the same distance
was 65.09%. Similarly, for the case of “S20 (Sp, 77GHz)”
(the dataset being Speech Commands), the accuracy with hand
coverage is “52.74%”, whereas when the phone was held on
a tripod, the corresponding accuracy was “60.74%”.
Experiments on Longer Speech Sentences including Songs
and Music: We have been able to capture multi-word sen-
tences, and even music, in order to demonstrate the capabilities
of mmSpy. The performance of the audio reconstruction model
in Fig. 10 is independent of the length of the audio. Therefore,
we can readily use the model to extract audio even if the
audio includes multiple words or sentences. We have used
the network to extract audio from actual sentences including
speech, music, and song. A few examples of spectrograms as
well as the corresponding sound recordings (headphones are
recommended to listen to the recordings) are included in this
document, for three categories: (i) Speech – “I have a dream ..
” speech by Dr. Martin Luther King Jr. in Figure 26 (ii) Song
with background music – “Twinkle Twinkle little star .. ” in
Figure 27, and (iii) Music (Turkish March) in Figure 28. We
believe the speech content is evident from the recordings.
Cost of Model Training: Training the classifier using syn-
thetic data takes 10.29 minutes on average. Since the domain
adaptation is done on a smaller set of real examples, the
adaptation for the classifier takes 43.8 seconds on average.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
1221
11
No InterferenceUSRPWiFiWiGigmmWaveSource of Interference10203040506070Accuracy (%)(a)
(b)
(c)
Fig. 23: Hand Coverage Experiments (a) Body motion artifacts can be seen in the raw audio (b) Zoomed in version of the raw audio. The
recording is here [7] (c) The recovered audio after body motion filtering. The recording is here: [6].
After the network is adapted, the inference per sample takes
15.3ms on average. Training the enhancer took longer since
each sample is converted into multiple inputs – the training
time using synthetic data on an average is 17.2 minutes.
The adaptation of the enhancer took longer as well – 2.17
minutes on average, and finally, the inference per spectrogram
takes 54.7ms. All evaluation was done on the desktop whose
configuration is specified in the implementation subsection.
VI. RELATED WORK
Sensing Applications with RF: There is a lot of recent inter-
est in using WiFi-like communication devices for RADAR-
like sensing applications in addition to more conventional
applications in communication and networking. LiquID [45]
identifies permittivties of liquid materials by measuring the
slow-down and attenuation of UWB RF signals. RF-EATS [55]
can sense food materials in containers by measuring reflection
of backscattered RFID stickers attached on the container. RF-
avatar [101] shows capabilities of beyond-the-wall 3D imaging
using RF signals. While the above works use microwave
frequencies, with the proliferation of 5G, there has been a lot
of recent interest in using mmWave frequencies for sensing
applications. mSense [93] can classify upto 21 liquids by
measuring reflection of mmWave signals. mmVib can detect
vibrations for classifying machines as well as monitoring their
health in a number of industrial IoT applications [62]. Pointil-
lism [35] can detect objects such as cars for applications in
autonomous driving. Osprey [81] uses mmWave technologies
for geometry sensing. Osprey estimates the depth of tire by
utilizing concepts of synthetic aperture radars to create an
image of the tire thread being placed over tires of cars. By
identifying anomalies in the tire image, debris, and the wear
and tear is detected.
In contrast to the above applications
mmSpy exploits the high precision sensing capabilities of
the small wavelength of mmWave spectrum for exposing a
security vulnerability. WaveSpy [66] spies apps running on
a system by analyzing reflections from its screen. Material
properties such as permittivity change with on-screen color
patterns used by apps. This manifests as SNR/phase changes
in reflected mmWave signals. In contrast to detecting material
properties, mmSpy analyzes vibrations of a known material.
Speech Sensing with RF: Acoustic eavesdropping of loud-
speakers have been shown in [92], where phase variations of
radio frequency (RF) reflections in the microwave frequencies
collected from a large antenna array are exploited to detect
digits. In a similar spirit, UWHear [90] uses high resolution
UltraWideBand (UWB) RF reflections for detecting multiple
speakers in the environment, and shows the feasibility on
a problem on digit classification. WaveEar [95] can detect
speech signals using custom mmWave hardware based on
reflection from a human throat. More recently, RadioMic [77]
uses mmWave to detect speech signals from loudspeakers,
humans, and objects. In contrast to these works, mmSpy shows
the feasibility of eavesdropping earpiece devices used in phone
calls. While loudspeaker or human throat vibrations can be
stronger, thus can also be eavesdropped by a co-located micro-
phone, the earpiece vibrations are very minute and inaudible
to a microphone co-located with the attacking radar device.
Nevertheless, mmSpy exploits small wavelength of mmWave
signals to show the capability of audio reconstruction as well
as speech classification.
Eavesdropping with cameras and lasers: Works in [41], [42]
detect sounds played in a room using camera. In particular,
sound waves induce vibration in objects (paper bags, bottles
etc). By capturing such vibration patterns using a high speed
camera, feasibility of recovering sound is shown even from the
outside of a sound-proof room. In contrast to the above works,
which detect stronger vibrations from loudspeakers, mmSpy
detects minute vibrations from an earpiece device. Similarly,
laser microphones have been popularly used for eavesdropping
in a passive manner. However mmWave antennas are much
smaller in size in comparison to laser microphones thus
making them easier to conceal. Additionally, the presence of
laser microphones is detectable [4], while mmWave signals
can conceal
themselves within ambient mmWave signals.
Given that mmWave based 5G is a popular communication
technology, this allows the adversary to conceal themselves
among ambient mmWave signals. Lamphone [73] can eaves-
drop acoustic vibrations that are are already in the air by
analyzing vibrations of a light bulb. However, the sound is also
audible to a colocated microphone near the bulb. In contrast,
mmSpy detects weaker sound sources by picking up vibrations
directly from the source, even if it us inaudible to a colocated
microphone near the mmWave hardware.
Motion Sensor based Attacks: Gyrophone [69] detects
the speech content from an external loudspeaker (subwoofer)
using smartphone gyroscope sensors placed on the same
surface (for example, shared table). Classification of 11 digits
(0-9 and ”oh”) is shown feasible. Speechless [30] shows
the sensitivity of smartphone accelerometers to eavesdrop on
speech content from a loudspeaker source sharing the same
surface as the phone (subwoofers, laptops etc). Spearphone
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
1222
12
0.511.522.5Time (s)-0.500.5AmplitudeArtifacts due to body motion0.511.522.5Time (s)-505Amplitude10-3Validspeech00.10.20.30.40.50.60.70.80.91Time (s)-1-0.500.51Amplitude[31] performs gender and speaker classification, and detection
of keywords by spying on smartphone speech content with
builtin accelerometers. AccelEve [33] proposes an attack in a
similar setting to Spearphone. Apart from gender and speaker
classification, they perform digit and alphabet classification.
AccelWord [100] shows the feasibility of detecting the wakeup
keywords of voice commands such as ”Okay Google”, and
”Hi Galaxy” using accelerometers. PitchIn [59] shows the
feasibility of eavesdropping ambient speech by fusing data
from multiple non-acoustic sensors (accelerometers, gyro-
scope, geophone etc) with low sampling rate. In contrast to
such works that exploit motion sensors, mmSpy launches a
remote attack with mmWave devices.
Attacks on mobile sensors: Ghosttalk [64] shows the
capability of injecting fake data into analog sensors by directly
inducing voltages into the circuitry by an external RF excita-
tion, thus critically compromising IoT systems including heart
monitors. DolphinAttack [98] shows the feasibility of injecting
inaudible voice commands to attack voice assistants. Mole [89]
uses a smartwatch accelerometer to spy on the contents of a
user’s typing. S3 [49] detects drawings on a tablet using an
apple pencil by exploiting variations in magnetic fields sensed
by the magnetometer. Accelerometer sensors are also known
to reveal passwords as entered on the touchscreen of a phone
[76]. The smartphone magnetometers are even shown to be
capable of identifying the operating systems and the pattern
of applications in a nearby desktop by monitoring the spinning
of hard-drives which are made of magnetic materials [37]. In
contrast, mmSpy performs an attack on spying speech contents
based on radar reflections.
Domain adaptation: Transfer-learning based domain adapta-
tion is popular in vision and speech processing. For example,
AlexNet model [63] pretrained on ImageNet database [43] was
fine-tuned for classifying images in the medical domain[103],
remote-sensing [60] and breast-cancer [74]. Similarly, a pre-
trained BERT language model [44] was fine-tuned for tasks
such as text-summarizing [99], question answering [82] etc.
This significantly reduces the burden of training for a new task.
In a similar spirit, we use pre-trained model from synthetic
radar data. While this provides a good enough synthetic model
to begin with, we adapt the model with real radar data. Noted
in Section IV, our domain adaptation trains only a few layers
such that it significantly decreases the overhead of training.
VII. DISCUSSION, LIMITATIONS, AND FUTURE WORK
Eavesdropping a User under Mobility: Results in Fig. 23
show that enough information for the attack exists despite
partial coverage of the phone by the human hand. However,
not being able to attack a user who is in motion (such as
walking) is a limitation of the current work. While sufficient
vibration information still exists, the motion of the user might
create interference which needs to be eliminated. We have
some preliminary ideas for canceling body motion based
on emerging recent works. Wistress [56] uses self-similarity
matrices to identify and cancel out artefacts that are caused
by small muscular movements to extract heartbeat signals. We
will explore such opportunities in the future.
Relevance of the Attack in Context of 5G Applications:
Table II outlines applications (both current and future) that
reply on 5G technology. We believe the attack is relevant in
the context of applications outlined in the table.
Application
Autonomous Driving
Industrial IoT
Healthcare
5G Communication
Augmented and Virtual Reality
Remote Sensing
Smart Cities
Frequency Band
77-81GHz [50], [68], 76GHz [102], [15]
77-81GHz [62]
76GHz [96], 77-81GHz [56]
60GHz [75], [19], [2]
77-81GHz [97], [54], 60GHz ([47], Google Soli)
60GHz [94]
[2], [3], [1]
TABLE II: Use of mmWave bands in various applications.
Defense: The vibration sensor on the phone can be used to
produce noisy vibrations [84] such that the accuracy of vibra-
tion detection in mmSpy using the phases of the reflections is
reduced. Similar to white box adversarial attacks on machine
learning models [39], we can generate minimal noise using
vibrations that is enough to confuse mmSpy’s models, while
still having negligible impact on user experience. Another
possible defense against mmSpy is to surround the end of the
earpiece that is not facing the ears with a vibration dampening
material. For example, materials such as q-pads, or borosilicate
paints are commonly used in the music industry to eliminate
unwanted vibrations [25]. Evaluation of the above ideas for
defense would be a part of our future work.
Automatic Speech Recognition: While mmSpy demonstrates
the feasibility of the attack on isolated speech recognition, we
plan to extend to recognition of continuous speech. Automatic
speech recognition (ASR) models based on LSTM, attentions,
and language modeling [40] are popular in continuous speech
recognition where the boundaries between successive words
can be blurred. While training such deep learning based
models requires an extensive amount of datasets, we plan to
adopt a procedure similar to synthetic training data proposed
here to bootstrap the training process.
VIII. CONCLUSION
This paper shows the feasibility of eavesdropping phone calls
by detecting minute vibrations produced by the earpiece device
used in phone calls using mmWave radars. While the sensor
data is very noisy, mmSpy proposes a range of techniques from
statistical noise correction, machine learning based modeling,
as well as domain adaptation to develop robust models for
speech recognition with low overhead of training. Extensive
measurements demonstrate the feasibility of the attack. The
proliferation of off-the-shelf mmWave devices both for 5G
networking as well as in sensing applications makes this attack
of critical concern in the context of speech privacy.
IX. ACKNOWLEDGEMENT
We are grateful to the reviewers and the anonymous shep-
herd for feedback. This research was partially supported by
NSF grants CNS-2008384, and CNS-1956276. We also thank