application traﬃc.
We considered two broad explanations—extraordinary per-
sistent latency due to oversized queues associated with low-
bandwidth links, or extraordinary temporary, initial latency
due to MAC-layer time slot negotiation or device wake-up.
In this section, we ﬁnd that the latter appears to be a
more likely explanation, qualitatively consistent with prior
investigations of GPRS performance characteristics [4], but
showing quantitatively more signiﬁcant delay.
313Figure 12: Bottom: Diﬀerence between initial latency and
second probe latency; values around 1 indicate that both re-
sponses arrive at about the same time, values near zero in-
dicate that the RTTs were about the same. The second line
includes only those where RT T1 > max(RT T2 . . . RT Tn).
Top: The probability that, given RT T1 − RT T2 on the x-
axis, that RT T1 > max(RT T2 . . . RT Tn).
We extracted 236,937 IP addresses from the 20150206 ISI
dataset (February 2015), including all addresses with a me-
dian RTT of at least one second. To select only responsive
addresses that still had high latency, for each of these IP ad-
dresses, we sent two pings, separated by ﬁve seconds, with
a timeout of 60 seconds. We omit 151,769 addresses that
did not respond to either probe and 1,994 addresses that
responded, on average, within 200ms.
Of the 83,174 addresses that remain, we wait approxi-
mately 80 seconds before sending ten pings, once per second
with the same 60-second timeout. We next classify how the
round trip time of the ﬁrst ping, RT T1, diﬀers from those
of the rest of the responded pings, RT T2 . . . RT Tn, where n
may be smaller than 10 if responses are missing. For most
of these addresses, 51,646, the ﬁrst response took longer
than the maximum of the rest. This suggests that roughly
2/3 of high latency observations are a result of negotiation
or wake-up rather than random latency variation or per-
sistent congestion. For 11,874, median(RT T2 . . . RT Tn)  max (50663)-1010.00.20.40.60.81.0probability0246810RTT_1 - min(RTT_2..RTT_n) 0.00.20.40.60.81.0CDF 314Pattern
Low latency, then decay
Loss, then decay
Sustained high latency and loss
High latency between loss
Pings Events Addrs
10
33
14
12
615
1528
2994
12
13
81
21
12
Table 7: We observed distinct patterns of latency and loss
near high latency responses, classifying all 5149 pings above
100 seconds from the sample.
same underlying action on the network, but we leave them
separate since there are substantially many of each.
Another characteristic pattern is that a high round trip
time is followed by several responses of even greater latency,
possibly with intermittent losses. This behavior is usually
sustained for several minutes with latencies remaining higher
than normal (>10 seconds) throughout the duration: we
call this behavior Sustained high latency and loss. Finally,
there are some cases where a single ping has a latency > 100
seconds and is preceded and followed by loss. We call these
cases High latency between loss.
We count the number of occurrences of each pattern in
Table 7. For each pattern, we show the number of pings
greater than 100 seconds that were part of that pattern, the
number of instances of that pattern occurring, and the num-
ber of unique addresses for which it occurred. We observe
that the majority of events and addresses are Loss, then de-
cay, yet almost twice as many pings are part of Sustained
high latency and loss.
6.5 Summary
High latencies appear to be a property mainly of cellular
Autonomous Systems, though a few also appear on satellite
links. Latencies in the ISI data that are regularly above one
second seem to be caused by the ﬁrst-ping behavior associ-
ated with several addresses, where the ﬁrst ping in a stream
of pings has higher latency than the rest. Egregiously high
latencies, i.e., latencies greater than a hundred seconds, oc-
cur in two broad patterns.
In the ﬁrst, latencies steadily
decay with each probe, as if clearing a backlog. In the sec-
ond, latencies are continuously high and are accompanied
by loss, as if the network link is oversubscribed.
7. CONCLUSIONS AND DISCUSSION
Researchers use tools like ping to detect network out-
ages, but generally guessed at the timeout after which a
ping should be declared “failed” and an outage suspected.
The choice of timeout can aﬀect the accuracy and timeli-
ness of outage detection:
if too small, the outage detector
may falsely assert an outage in the presence of congestion;
if too large, the outage detector may not pass the outage
along quickly for conﬁrmation or diagnosis.
We investigated the latencies of responses in the ISI sur-
vey dataset to determine a good timeout, considering the
distributions of latencies on a per-destination basis. Fore-
most, latencies are higher than we expected, based on con-
ventional wisdom, and appear to have been increasing. We
show that these high latencies are not an artifact of measure-
ment choices such as using ICMP or the particular vantage
points or probing schemes used, although diﬀerent data sets
vary somewhat. We show that high latencies are not caused
by links with a substantial base timeout, such as satellite
links. Finally, we showed that in many instances, the ini-
Figure 14: Percentage of addresses in a /24 preﬁx showing
a drop from the initial to the maximum.
sponsive addresses within each preﬁx that dropped from the
initial ping to the maximum of the rest in Figure 14. Several
preﬁxes did not have an initial latency greater than the max-
imum; these typically had very few responsive addresses. In
other preﬁxes, most addresses showed a reduction. Finally,
the 51,646 that showed a reduction from the initial ping are
from only 1,083 preﬁxes. Of the 161 preﬁxes that had only
one address with above one-second median latency, only 39
showed a reduced from the initial RTT to the maximum
of the rest. Taken together, we believe this distribution of
addresses across relatively few preﬁxes indicates that the
wake-up behavior is associated with some providers but not
restricted to them.
6.4 Patterns associated with RTTs greater than
100 seconds
Finally, we look at addresses with extraordinarily high la-
tencies ( greater than 100 seconds); in particular, we want
to understand whether these high latencies are an instance
of a ﬁrst-ping-like behavior, where wireless negotiation or
buﬀering during intermittent connectivity creates the high
value, or, on the other hand, are instances of extreme con-
gestion. To separate the two types of events, we consider a
sequence of probes, looking for whether or not the latency
diminishes after a ping beyond 100 seconds.
We sample 3,000 of 38,794 addresses whose 99th percentile
latency was greater than 100 seconds in the IT63c (20150206)
dataset. Of this sample, 1,400 responded. We sent each
address 2000 ICMP Echo Request packets using Scamper,
spaced by 1 second. To collect responses with very high
delays without altering the Scamper timeout, we simultane-
ously run tcpdump to capture packets.
Ping samples that saw a round trip time above 100 seconds
exist in the context of a few very distinct patterns. Often,
a series of successive ping responses would be delivered to-
gether almost simultaeously, leading to a steady decay in
their round trip times. For example, after 136 seconds of
no response from IP address 191.225.110.96, we received all
136 responses over a one second interval: every subsequent
response’s round-trip latency was 1 second lower than the
previous. This pattern is sometimes preceded by a relatively
low latency ping ( max(RTT_2..RTT_n)0.00.20.40.60.81.0315tial communication to cellular wireless devices is largely to
blame for high latency measures. Similar spikes that may
be consistent with handoﬀ also dissipate over time, to more
conventional latencies that support application traﬃc. With
this data, researchers should be able to reason about what
to expect in terms of false outage detection for a given time-
out and how to design probing methods to account for these
behaviors.
Our initial hypothesis was that it would be a simple mat-
ter to conﬁrm that widely used timeout values would be ad-
equate for studying outages, or failing that, that one or two
additional seconds would be enough. However, as memory
capacity and performance becomes less of a limiting factor,
we believe that the lesson of this work is to design network
measurement software to approach outage detection using a
method comparable to that of TCP: send another probe af-
ter 3 seconds, but continue listening for a response to earlier
probes, at least for a duration based, at least in part, on the
error rates implied by Table 2. We plan to use 60 seconds
when we need a timeout, and avoid timeouts otherwise.
Acknowledgments
We would like to express our sincere appreciation to the
authors of the ISI Internet survey for both publishing their
data and designing their data collection method to collect
the unmatched responses that enabled this analysis.
We also would like to thank Zakir Durumeric for incor-
porating our changes into https://github.com/zmap/zmap/
blob/master/src/probe modules/module icmp echo time.c in
order to support explicit matching of responses and calcu-
lating round trip times in the stateless Zmap.
This research was supported in part by ONR grant N00173-
13-1-G001.
8. REFERENCES
[1] Fred Baker. Requirements for IP version 4 routers.
IETF RFC-1812, June 1995.
[2] Chadi Barakat, Nesrine Chaher, Walid Dabbous, and
Eitan Altman. Improving TCP/IP over geostationary
satellite links. In Global Telecommunications
Conference, 1999. GLOBECOM’99, volume 1, pages
781–785, 1999.
[3] R. Braden, Editor. Requirements for internet hosts –
communication layers. IETF RFC-1122, October 1989.
[4] Rajiv Chakravorty, Andrew Clark, and Ian Pratt.
GPRSWeb: Optimizing the web for GPRS links. In
MOBISYS, May 2003.
[5] Zakir Durumeric, Eric Wustrow, and J Alex
Halderman. ZMap: Fast Internet-wide Scanning and
Its Security Applications. In USENIX Security, pages
605–620, 2013.
[6] Nick Feamster, David G. Andersen, Hari
Balakrishnan, and M. Frans Kaashoek. Measuring the
eﬀects of Internet path faults on reactive routing. In
ACM SIGMETRICS, 2003.
[7] John Heidemann, Yuri Pradkin, Ramesh Govindan,
Christos Papadopoulos, Genevieve Bartlett, and
Joseph Bannister. Census and Survey of the Visible
Internet. In IMC, 2008.
[8] Philip Homburg. [atlas] timeout on ping
measurements.
http://www.ripe.net/ripe/mail/archives/ripe-
atlas/2013-July/000891.html, July 2013. Posting to
the ripe-atlas mailing list.
[9] ISI ANT Lab. Internet address survey binary format
description. http://www.isi.edu/ant/traces/topology/
address surveys/binformat description.html.
[10] Ethan Katz-Basset, Harsha V. Madhyastha, John P.
John, Arvind Krishnamurthy, David Wetherall, and
Thomas Anderson. Studying black holes in the
internet with Hubble. In NSDI, 2008.
[11] Landernotes.
https://wiki.isi.edu/predict/index.php/LANDER:
internet address survey reprobing it54c-20130524.
[12] Mathew J. Luckie, Anthony J. McGregor, and
Hans-Werner Braun. Towards improving packet
probing techniques. In IMW, pages 145–150, San
Francisco, CA, November 2001.
[13] Matthew Luckie. Scamper: A Scalable and Extensible
Packet Prober for Active Measurement of the Internet.
In IMC, pages 239–245, 2010.
[14] Harsha V. Madhyastha, Tomas Isdal, Michael Piatek,
Colin Dixon, Thomas Anderson, Aravind
Krishnamurthy, and Arun Venkataramani. iPlane: An
information plane for distributed services. In OSDI,
Seattle, WA, November 2006.
[15] Ina Minei and Reuven Cohen. High-speed internet
access through unidirectional geostationary satellite
channels. In IEEE Journal on Selected Areas in
Communications, 1999.
[16] Jeﬀrey Mogul. Broadcasting Internet datagrams.
IETF RFC-919, October 1984.
[17] Vern Paxson. End-to-end routing behavior in the
Internet. In ACM SIGCOMM, pages 25–38, Palo Alto,
CA, August 1996.
[18] Lin Quan, John Heidemann, and Yuri Pradkin.
Trinocular: Understanding Internet Reliability
Through Adaptive Probing. In ACM SIGCOMM,
pages 255–266, 2013.
[19] RIPE NCC. Atlas. http://atlas.ripe.net.
[20] SamKnows. Test methodology white paper, 2011.
[21] Aaron Schulman and Neil Spring. Pingin’ in the rain.
In IMC, Berlin, November 2011.
[22] Neil Spring, David Wetherall, and Thomas Anderson.
Scriptroute: A public Internet measurement facility.
In USITS, pages 225–238, Seattle, WA, March 2003.
[23] Ming Zhang, Chi Zhang, Vivek Pai, Larry Peterson,
and Randy Wang. PlanetSeer: Internet path failure
monitoring and characterization in wide-area services.
In OSDI, San Francisco, CA, December 2004.
316