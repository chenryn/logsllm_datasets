title:Secure Multi-party Computation of Differentially Private Heavy Hitters
author:Jonas B&quot;ohler and
Florian Kerschbaum
Secure Multi-party Computation of Differentially Private Heavy
Hitters
Jonas Böhler
SAP Security Research
Karlsruhe, Germany
PI:EMAIL
Florian Kerschbaum
University of Waterloo
Waterloo, Canada
PI:EMAIL
ABSTRACT
Private learning of top-𝑘, i.e., the 𝑘 most frequent values also called
heavy hitters, is a common industry scenario: Companies want to
privately learn, e.g., frequently typed new words to improve sug-
gestions on mobile devices, often used browser settings, telemetry
data of frequent crashes, heavily shared articles, etc.
Real-world deployments often use local differential privacy, where
distributed users share locally randomized data with an untrusted
server. Central differential privacy, on the other hand, assumes
access to the raw data and applies the randomization only once, on
the aggregated result. These solutions either require large amounts
of users for high accuracy (local model) or a trusted third party
(central model). We present multi-party computation protocols HH
and PEM of sketches (succinct data structures) to efficiently com-
pute differentially private top-𝑘: HH has running time linear in the
size of the data and is applicable for very small data sets (hundreds
of values), and PEM is sublinear in the data domain and provides
better accuracy than HH for large data sizes. Our approaches are
efficient (practical running time, requiring no output reconstruction
as other sketches) and more accurate than local differential privacy
even for a small number of users. In our experiments, we were
able to securely compute differentially private top-𝑘 in less than 11
minutes using 3 semi-honest computation parties distributed over
the Internet with inputs from hundreds of users (HH) and input
size that is independent of the user count (PEM, excluding count
aggregation).
CCS CONCEPTS
• Security and privacy → Data anonymization and sanitiza-
tion; Privacy protections; Privacy-preserving protocols.
KEYWORDS
Heavy Hitters, Differential Privacy, Secure Multi-party Computa-
tion, Sketches
ACM Reference Format:
Jonas Böhler and Florian Kerschbaum. 2021. Secure Multi-party Computa-
tion of Differentially Private Heavy Hitters. In Proceedings of the 2021 ACM
SIGSAC Conference on Computer and Communications Security (CCS ’21),
November 15–19, 2021, Virtual Event, Republic of Korea. ACM, New York, NY,
USA, 17 pages. https://doi.org/10.1145/3460120.3484557
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8454-4/21/11.
https://doi.org/10.1145/3460120.3484557
1 INTRODUCTION
The goal of federated top-𝑘 discovery is to learn the 𝑘 most frequent
values, also called heavy hitters, in a distributed data set. Differential
privacy (DP) [34, 36], a rigorous privacy notion, restricting what
can be inferred about any individual in the data, is widely deployed
to mitigate privacy risks and regulatory concerns, when the data is
generated by users. E.g., Apple deploys DP to privately learn fre-
quently typed words on mobile devices to improve auto-complete
suggestions and to detect websites with large resource consump-
tions to optimize the browsing experience in iOS and macOS [8, 74].
Google privately detects popular Chrome browser settings [40, 42]
as well as busy times for businesses in Google Maps [17]. Also,
Microsoft deploys differentially private telemetry data collection in
Windows 10 (Creators Fall Update) across millions of devices [30]
and LinkedIn’s Audience Engagement API lets marketers query,
e.g., DP top-𝑘 shared articles among users with a specific skill set
[70, 71]. Real-world deployments [8, 17, 30, 40, 42] mainly imple-
ment the local model of DP, i.e., users locally randomize their data
and send it to an untrusted aggregator. In the central model, e.g.,
used by LinkedIn [70, 71] and the US Census bureau [1], a trusted
party has access to the raw data, which only needs to apply ran-
domization once, on the aggregated result. The local model has
fewer assumptions (no trusted party) but generally requires up to
exponentially more samples to achieve the optimal accuracy offered
by the central model at the same privacy parameter [52].
We present a novel alternative with central model accuracy, that
requires no trusted party, and is efficiently computable. Our proto-
col provides high accuracy even for a small number of users, which
is the most challenging regime for DP [16, 18, 66], via efficient secure
multi-party computation (MPC), which enables parties to jointly
compute a function without revealing their inputs [45]. The straight-
forward algorithms to accurately detect heavy hitters are inefficient
in MPC [61], and hence we employ sketches, clever approximate
algorithms with succinct data representation, for streams [27] or
large domains [78] to make the secure multi-party computation
efficient. Typically, sketches store counters indexed by multiple
hash functions, e.g., count-min sketch [27, 61] or Bloom filters
[40, 42], and local-model users apply domain reduction (e.g., hash-
ing) [11, 12, 78]. However, hash-based techniques require costly
reconstruction, e.g., iterating/hashing the entire domain to find
heavy hitters, whereas our sketches directly store heavy hitters or
their bit prefixes. Our key insight is that adapting suitable sketches
(without reconstruction) enables efficient MPC of DP heavy hitters
with high accuracy. We present two MPC protocols to discover the
differentially private top-𝑘 on distributed user data: HH and PEM.
Our protocols are based on state-of-the-art solutions for heavy
hitter detection – HH is build upon non-private detection in data
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2361streams [27], and PEM adapts the local DP method from [78] – real-
ized as efficient MPC implementations of central DP randomizations
[32, 33, 79]. HH has running time linear in the size of the data and
is applicable for very small data sets (hundreds of values). PEM is
sublinear in the data domain (i.e., linear in the bit-length of domain
size) and provides better accuracy than HH for a large number of
users (thousands to millions). We improve upon related work for
MPC of DP heavy hitters [20, 61, 65], which is linear in the size of
the data, and requires reconstruction or is not optimized for DP
resulting in lower accuracy (see Section 6 for details). In summary,
our contributions are DP heavy hitter MPC protocols
• with high accuracy even for small data sets or few users
(Section 5),
• that are efficient (practical running times, reconstruction-
free) (Section 5),
• secure in the semi-honest model and extendable to malicious
adversaries (Section 4.5),
• both implemented and evaluated in two MPC frameworks,
SCALE-MAMBA [4] and MP-SPDZ [53], using 3 semi-honest
computation parties in a WAN with 100 ms network delay,
100 Mbits/s bandwidth achieving running times of less than
11 minutes (Section 5).
In Section 2, we describe preliminaries. In Section 3, we describe
an ideal version our protocol with a trusted third party, which
we later replace with MPC. We present our secure multi-party
computation of DP heavy hitters in Section 4. We provide a detailed
evaluation in Section 5, describe related work in Section 6 and
conclude in Section 7.
2 PRELIMINARIES
Next, we detail the problem, and introduce preliminaries for secure
multi-party computation and differential privacy.
Problem Description: We consider a set of 𝑛 input parties
P = {𝑃1, . . . , 𝑃𝑛}, where party 𝑃𝑖 holds a datum 𝑑𝑖, and 𝐷 =
{𝑑1, . . . , 𝑑𝑛} ∈ 𝑈 𝑛 denotes the combined data set with underly-
ing data domain 𝑈 . We want to find heavy hitters, more formally:
Definition 1 (Top-𝑘 or Heavy Hitter). Datum 𝑑 ∈ 𝐷 is a
top-𝑘 element if its frequency 𝑓𝑑 in 𝐷 is among the 𝑘 most frequent
elements, where 𝑓𝑑 = |{𝑥 | 𝑥 ∈ 𝐷 and 𝑥 = 𝑑}|/|𝐷|.
We release at most 𝑘 heavy hitters like Durfee and Rogers [32],
as thresholding might drop small noisy counts (unlikely to be heavy
hitters). Durfee and Rogers note that for flat histograms, i.e., all
counts are similar, thresholding outputs nothing instead of (almost)
uniformly random elements, which provides more data insights
(i.e., flat histogram) than randomness.
We define accuracy like Wang et al. [78] as the normalized cu-
mulative rank (NCR). Unlike the F1 score, NCR also considers an
element’s frequency:
𝑐∈C
𝑐′∈C𝑘
Definition 2 (Normalized Cumulative Rank (NCR)). Let
C𝑘 denote the actual top-𝑘 values and C the presumed top-𝑘 (re-
turned by HH, PEM). The normalized cumulative rank of C is
𝑟(𝑐′), where rank 𝑟(𝑐𝑖) = 𝑘 + 1 − 𝑖 for 𝑖-th most
frequent element 𝑐𝑖 ∈ C𝑘 and zero otherwise.
Basically, detecting the most frequent element increases the cu-
mulative rank by 𝑘, the second most frequent element adds another
𝑟(𝑐)/
maximum score𝑐′∈C𝑘
𝑟(𝑐′) = 𝑘(𝑘 + 1)/2.
𝑘 − 1, etc., and the sum is normalized to [0, 1] by dividing it with
We aim for efficient MPC of DP heavy hitters using sketches
for approximate counts. Straightforward, exact MPC requires a
counter per domain element, i.e., for 𝑛 data values from domain
of size 𝑢 the running time complexity is in 𝑂(𝑢𝑛). Related work
for MPC either requires costly reconstruction linear in 𝑢, or is not
optimized for DP and does not provide high accuracy for small data
sets (see Section 6). Our protocols HH, resp. PEM, enable 𝑂(𝑛𝑡),
resp. 𝑂(𝑔𝑐 log(𝑐)), for small parameters 𝑡 ∈ 𝑂(1), 𝑐 = 𝑂𝜂(𝑘), 𝑔  0. Typically, 𝛿 is negligible
in the size of the data [37]. While we present pure DP mechanisms
in the following our protocols apply them in combination with
𝛿-based thresholds from [32, 79], thus, our protocols satisfy ap-
proximate DP. The DP definition can be adapted to fit different
deployment models (which we compare in Section 2) and next we
describe two variations: local and computational DP. Note that
Definition 3 assumes that mechanism M has access to the raw
data 𝐷. In a distributed setting, where each user locally random-
izes her value 𝑣, the notion of local DP (LDP) [52] is used. Here,
the output changes for any input 𝑣, 𝑣′ ∈ 𝐷 are 𝜖-bounded, i.e.,
Pr[M(𝑣) ∈ 𝑆] ≤ exp(𝜖)·Pr[M(𝑣′) ∈ 𝑆]. Definition 3 holds against
an unbounded adversary, however, we consider computationally-
bounded, semi-honest parties for a joint secure computation real-
ized with (𝑡, 𝑚)-secret sharing. We use the definition from Eigner
et al. [39], where VIEW
Π(𝐷) is the view of party 𝑝 during the exe-
𝑝
cution of protocol Π on input 𝐷, including all exchanged messages
and internal state, and 𝜅 is a security parameter:
Definition 4 (Distributed Differential Privacy). A ran-
domized protocol Π implemented among 𝑚 computation parties P =
{𝑃1, . . . , 𝑃𝑚}, satisfies distributed differential privacy w.r.t. a coali-
tion C ⊂ P of semi-honest computation parties of size 𝑡, if the follow-
ing condition holds: for any neighbors 𝐷, 𝐷′ and any possible set 𝑆 of
views for protocol Π,