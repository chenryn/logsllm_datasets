(a)
(b)
In context of
GOAL
In context of
C2.1.2
Value of calibration
parameter for Pitot
Probe
CONTEXT
SOLUTION
SOLUTION
Figure 2.
(a) Safety case fragment showing an (abridged) argument for safe computation of angle of attack (b) Conﬁdence quantiﬁcation.
To illustrate,
the BN in Figure 2(b) gives the prior
conﬁdence in the argument of Figure 2(a). The BN structure
mirrors the initial decomposition of the top-level claim and
it also captures the uncertainty in the argument formulation,
i.e., the assurance deﬁcits [9]. Each node in the BN and its
associated distribution characterizes the different sources of
uncertainty in the argument. The BN completely speciﬁes
the prior conﬁdence in the argument by modeling the
uncertainty that the claim is accepted, conditional on the
uncertainties identiﬁed for the argument.
Thus, stochastic (aleatory) uncertainty in the sensor val-
ues is inﬂuenced by the failure probability (rate) of the
pitot probe. This is quantiﬁable, e.g.,
through statistical
testing of the sensor. On the other hand, uncertainty in
the correctness of the speciﬁcation is both aleatory and
epistemic in nature. The former appears since a parameter
in the function speciﬁcation is obtained from wind tunnel
calibration of the pitot probe and due to the uncertainty in
the calibration experiments. We take the conﬁdence level
of the calibration experiments as the quantitative conﬁdence
value for the corresponding BN node, Accurate Calibration.
Epistemic uncertainty in the speciﬁcation correctness is
given by subjectively quantifying the conﬁdence that the
correct formula has been used [13], after domain experts
review the speciﬁcation against ﬂight control theory.
Even though a formal proof of correct implementation is
available, there is uncertainty along the veriﬁcation chain
induced through a combination of uncertainties surrounding
the assumptions made, e.g., in the speciﬁcations, and the
tools being used. We gauged the uncertainty that the proof
is correct, using subjective judgment based on feedback from
the tool developers. Thus, the node Proof Correct in the BN
of Figure 2(b) conveys the notion that there is, a priori, very
high conﬁdence that the proof should be trusted.
There also exists uncertainty in the sufﬁciency of the sub-
claims, i.e., whether the sub-claims G1.1, G2.1, and G2.2
are appropriate and sufﬁcient to infer the parent claim G1.
Indeed, arguments can only ever be deemed “complete” (or
compelling) insofar as the uncertainty in the reasoning is
sufﬁciently low. For instance, the node Argument Sufﬁcient
indicates that comparatively lower conﬁdence exists in the
sufﬁciency of the argument, adjudged as medium. Uncer-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
tainty also exists whether the claims made and the strategies
used to decompose the claims have been applied in the
appropriate context. Again, these uncertainties are epistemic
and we quantiﬁed it subjectively.
Based on the speciﬁed distributions and the conditional
independence assumptions as encoded by the BN structure,
the results of probabilistic modeling would suggest
that
high conﬁdence be placed in the overall argument (shown
by the distribution on the node Claim Accepted). However,
this evaluation is only an initial step towards conﬁdence
assessment and several challenges exist (discussed subse-
quently) associated with the quantiﬁcation,
interpretation
and validation of the results from the model.
IV. DISCUSSION
Thus far, we have described our experience with software
safety case development in terms of our methodology for
safety analysis and argumentation as applied to the autopilot
software in the Swift UAS.
In particular, the ﬁrst two authors performed the safety
analysis in close cooperation with the Swift UAS engineer-
ing team. Subsequently, they also constructed the manu-
ally created part of the safety case. In doing so, domain
knowledge that was previously implicit was made explicit
and classiﬁed as assumption, justiﬁcation or evidence, as
appropriate, for use in the safety case. The functional safety
requirements were formalized, in part, by the ﬁrst author
who also deﬁned the axioms and function speciﬁcations
required for formal veriﬁcation. The third author functioned
as the external independent safety case expert for evaluating
(and subsequently enhancing) the arguments made. Then,
we presented the safety case to the Swift UAS engineering
team (the designers and the range safety ofﬁcer) for their
feedback.
In this section, we reﬂect upon the insights gained and
lessons learned from several perspectives.
A. Safety Argument Comprehension
Our initial assumptions in the choice of a graphical nota-
tion, such as the GSN, for documenting the ASSC and the
Swift UAS safety case were that a graphical notation would
be intuitively easier to comprehend, and enable us to better
communicate the reasoning used in the arguments therein.
However, our ﬁrst experience through this exercise suggests
that a tabular equivalent representation is also useful to have
and, in some situations, might be better received.
We hypothesize that this is due, in part, to the (relatively
small) learning curve associated with a new notation, the
unfamiliarity of the Swift UAS engineering team with the
syntax and semantics of the GSN notation, and the preva-
lence of tabular representations for capturing development
artifacts such as requirements, hazard logs, hazard analysis
results, traceability, etc. Constructing a tabular equivalent for
an argument represented using the GSN is straightforward,
but presents the potential for some misinterpretation if table
semantics are not well speciﬁed.
We also observed that safety case fragments containing
hazard mitigation arguments over an architectural break-
down were, sometimes, misinterpreted as being equivalent to
other graphical notations in safety analysis that model failure
conditions over architectures, e.g., fault trees and event trees.
An additional impediment that we noted, to better com-
prehending the arguments as we presented them, was the
need to query the arguments and answer “what-if” questions.
Although this is desirable in terms of guiding design for de-
pendability/safety,
identifying shortcomings/improvements
in the system and, practically, also for facilitating navigation
through large goal structures, the purpose of the safety argu-
ment is primarily to convey that which has been achieved to
assure safety. We hypothesize that this arose, again in part,
from mistaking the semantics of the GSN notation with that
of notations which permit such analyses, e.g., fault trees,
and unfamiliarity, in general, with the safety case concept.
The engineers also preferred to view the safety case as an
information log providing a transparent record that safety
concerns have received sufﬁcient consideration, rather than
as an argument assuring safety. Indeed, one point made
during the feedback we received, was that ﬂight tests are
the ultimate evidence of system safety. This is valid, but
can be reconciled with our methodology by observing the
following: since the Swift UAS is undergoing development,
our safety case is interim in the safety case lifecycle [14].
By itself, it is insufﬁcient to completely justify the top-
level claim (Section III-C), nor should that be expected of
it. Rather, the evidence of safe ﬂight and other evidence
from operation (which is contingent on mission speciﬁc
conﬁgurations and weather conditions) forms part of the
operational safety case [14], which we have yet to create.
A key lesson learned here is that communicating the
safety argument in a comprehensible way to the relevant
stakeholders requires a mechanism that (a) well abstracts the
data that are not relevant to the stakeholders, e.g., through
the choice of a modular argument, and (b) mainly presents
the information relevant for their role in system development
and operation.
B. Safety Argument Design
Our assurance approach was mainly to address all the
identiﬁed hazards; therefore, we adopted a hazard-directed
style of argumentation, an instantiation of the hazard-
directed breakdown pattern [15]. Since we are also con-
cerned with the safety of software, our safety case includes
explicit correctness arguments to demonstrate and justify
that software contributions to the identiﬁed system hazards
are acceptable. Here, the evidence comprises formal proofs
that the code correctly implements the formalized software
safety requirements, and where the latter are functional
safety requirements that negate the hazards.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
However, like the design of software architectures, an
argument structure varies depending on the attributes that
it needs to satisfy. Whereas architectures are designed to
satisfy quality attributes such as performance, reliability
and modiﬁability, arguments are designed to satisfy various
attributes such as compliance, comprehensibility, validity,
and maintainability. Reﬂecting on the experience gained, the
safety case we developed for the Swift UAS is not explicitly
modularized, in comparison with those created using the
modular, contract-based safety case concept [8].
Thus, our safety case structure would have been different
(more modular) if, in addition, we were concerned with
compliance, maintainability, and reducing the cost of re-
certiﬁcation (in the event of change).
An important objective for a modular safety case is to
establish a correspondence between the modules in the
safety case and the components in the system and software
architecture. For our safety case, and since our concern was
mainly to show how software contributions to hazards were
acceptable, splitting the structure of the safety case into
slices of arguments, each covering one hazard, offered a
better strategy for addressing that concern (i.e., in terms
of highlighting traceability between hazards and software
behavior over correspondence between argument structures
and software components).
Similarly, the overall structure of the safety case for the
Swift UAS would have been different if mere compliance
was our objective, e.g., compliance with many safety stan-
dards in the UK requires demonstrating that all residual risks
are As Low As Reasonably Practicable (ALARP)2. This
would require a cost-beneﬁt analysis, speciﬁcally to show
that deploying additional risk reduction measures would
require grossly disproportionate costs. We did not explicitly
consider the costs of risk reduction in our safety case since
the ALARP principle is not currently applicable to the Swift
UAS. However, it is worth noting that cost-beneﬁt analysis
is part of the design trade-off that the development team
performed during systems engineering.
C. Assessment of Conﬁdence
1) Challenges: We selected a Bayesian paradigm as the
basis for quantitative conﬁdence analysis since it affords a
common probabilistic framework in which to reason about
both subjective information and quantitative data. However,
several issues exist relevant to quantiﬁcation, validation and
interpretation of the BN model.
First, there is a need to justify the basic BN structure and
the assumptions of conditional independence. This could be
achieved, in part, by automatically generating the BN from
the GSN-based safety argument, where for each source of
uncertainty identiﬁed, a corresponding node (or nodes) exists
in the BN. Next, specifying the prior probabilities for the
2http://www.hse.gov.uk/risk/theory/alarp.htm
QUANTITATIVE MEASURES FOR THE SWIFT UAS SAFETY CASE
Table I
Measure
Total number of claims
Coverage of considered hazards
Coverage of high-level safety requirements
Coverage of low-level safety requirements
Code covered by auto-generated claims
Value
144
0.7344
0.8667
1
0.9215
leaf nodes (nodes with no outgoing arcs) is straightforward
where empirical data is available (e.g., the node Accurate
Calibration, in Figure 2(b)).
When only subjective judgment is available (e.g., the node
Correct Formula in Figure 2(b)), quantifying conﬁdence and
selecting an appropriate prior distribution is problematic
despite extensive research on belief elicitation methods [16].
One way to address this issue, we believe, is to identify
relevant metrics using techniques such as the Goal-Question-
Metric (GQM) method [17], and to correlate these metrics
to conﬁdence levels based on a deﬁned quality model, e.g.,
we hypothesize that a metric such as coverage (by a safety
argument) of hazards (in a hazard list) would correlate with
conﬁdence in the sufﬁciency of the argument.
For intermediate nodes (i.e., nodes with both incoming
and outgoing arcs, e.g., the node Computation Correct in
Figure 2(b)), we use a parametric form to specify the condi-
tional prior probability distribution (not shown), where some
of the parameters model the strength of correlation between
the intermediate nodes and its immediate parents. Here,
greater investigation is required to justify the parameters