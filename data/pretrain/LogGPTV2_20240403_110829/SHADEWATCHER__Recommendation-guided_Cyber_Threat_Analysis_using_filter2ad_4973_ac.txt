(ztmp, connect, 128.55.12.73:54) in Figure 1a as examples. As
both network sockets are represented as eztmp +econnect, they
are embedded nearby in the vector space, indicating similar
semantics, which mirrors our domain knowledge of labeling
both of them as network scanning targets. Despite its effec-
tiveness, a signiﬁcant limitation exists in TransE — a single
relation type may correspond to multiple entities, causing 1-
to-N, N-to-1, or N-to-N problems [53]. We demonstrate how
this limitation affects cyber threat analysis in § VIII-C.
To overcome this issue, we employ TransR [33], which
learns a separate representation for a system entity conditioned
on different relations. It allows us to assign distinctive se-
mantics to the same entity under different relation contexts.
More formally, TransR maps system entities h, t and relation
r into embeddings eh, et ∈ Rd, er ∈ Rk. For each relation
r, it speciﬁes a projection matrix Wr ∈ Rd×k to transform
system entities from a d-dimensional entity space to a k-
t = etWr.
dimensional relation space, i.e., er
Afterward, TransR measures the plausibility score of a given
tuple (h, r, t) as follows:
h = ehWr, er
f (h, r, t) = (cid:107)er
h + er − er
t(cid:107) ,
where (cid:107)·(cid:107) denotes the L1-norm distance function. A lower
score of f (h, r, t) suggests that the tuple is more likely to be
observed in a KG and otherwise not.
To optimize the representation learning of TransR, we resort
to a margin-based pairwise ranking loss, which enforces the
plausibility score of a valid (observed in a KG) tuple to be
lower than that of a corrupted (unobserved) tuple:
Lf irst =
σ(f (h, r, t) − f (h(cid:48), r(cid:48), t(cid:48)) + γ),
(cid:88)
(cid:88)
(h,r,t)∈GK
(h(cid:48),r(cid:48),t(cid:48)) /∈GK
where (h, r, t) holds in a KG, while (h(cid:48), r(cid:48), t(cid:48)) does not
exist in the KG; γ is the hyper-parameter that controls the
margin between valid and corrupted tuples; σ(x) is the softplus
activation function. Following mainstream recommendation
systems [54], [55], we generate corrupted tuples by replacing
one system entity in a valid tuple with a random entity. In
summary, minimizing this loss of the ﬁrst-order modeling
allows us to encode the semantic and behavioral similarities
into system entity representations.
B. Modeling the Higher-order Information
Beyond direct (ﬁrst-hop) connections, multi-hop paths are
inherent in a KG. Such higher-order connectivities not only
Fig. 3: An illustration of SHADEWATCHER’s recommendation.
behavior, before extracting system entity interactions. To do
so, we ﬁrst identify all data objects in a PG, then perform a
forward depth-ﬁrst search on individual data objects to extract
subgraphs (see [31] for more details), and ﬁnally merge two
subgraphs if one subgraph is a subset of the other.
Intuitively, a behavior summarizes a series of interactions
between a data object and its interactive entities, which
separately indicate the initiator and targets of interactions.
For example, given the interactions gtcache→uname and
gtcache→162.66.239.75:53, we observe that an executable
attempts to collect system conﬁgurations and scan network
services. Based on this intuition, SHADEWATCHER converts
interactions in behaviors into a bipartite graph (BG), where
two disjoint node sets are data objects and system entities,
and edges connecting two sets reﬂect interactions.
Combining Provenance Graph and Bipartite Graph. Con-
sidering system entity interaction as a relation beyond system
calls, both PG and BG are formulated as sets of entity-relation-
entity tuples. We thus align system entities to merge them
into a KG as deﬁned in § III-A. SHADEWATCHER provides
the capability to store a KG into databases (PostgreSQL [49]
or Elasticsearch [50]) so that the KG can be queried without
being built from scratch for downstream cyber threat analysis.
We also integrate Kibana [51] into SHADEWATCHER as the
visualization front-end to facilitate attack investigation2.
VI. RECOMMENDATION MODEL
Figure 3 illustrates the workﬂow of SHADEWATCHER’s rec-
ommendation model. It mainly consists of three components:
1) modeling the ﬁrst-order information, which parameterizes
system entities as embeddings (i.e., vectorized representations)
through their usage contexts; 2) modeling the higher-order
information, which updates system entity representations by
recursively propagating information from multi-hop neighbor-
ing entities; 3) learning to detect threats, which predicts an
interaction’s probability of being adversarial on the top of two
system entity representations.
A. Modeling the First-order Information
Having coupled security concepts of system entity interac-
tions and entity contexts with recommendation concepts of
user-item interactions and item properties, we are aware that
2A demonstration can be found at https://youtu.be/Kd8t0YnPAvY.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:26 UTC from IEEE Xplore.  Restrictions apply. 
494
GNNDetectionKnowledge GraphEmbeddingEmbedding!"#$%GNNGNNGNNConcatenationConcatenation…%&%'supplement similarities among system entities but also exhibit
how system entities inﬂuence each other. For example, a two-
hop path /proc/25/stat r0−→gtcache r0−→/proc/27/stat shows the
similarity between /proc/25(27)/stat as they both interact with
gtcache; the /etc/passwd r1−→gtcache r4−→146.153.68.151:80 de-
scribes how sensitive user information is transmitted out of
an enterprise. Clearly, modeling higher-order connectivity can
help us localize potential adversaries by revealing system
entity relationships. However, solely using TransR is unable
to characterize such high-order information.
To capture high-order connectivity, we adopt graph neural
network (GNN) [34] to integrate multi-hop paths into system
entity representations. Speciﬁcally, given a system entity h,
one GNN module recursively updates its representation by
propagating and aggregating messages from neighbors:
h = er
, z(l−1)Nh
h ∈ Rdl
h = g(z(l−1)
z(l)
where z(l)
is the dl-dimensional representation of
h at the l-th propagation layer, z(l−1)
∈ Rdl−1 is that of
previous layer, and z(0)
h is initialized by embeddings
derived from TransR; Nh is h’ one-hop neighbors (aka ego-
network [56]), and z(l−1)Nh
∈ Rdl−1 memorizes the information
being propagated from h’s (l − 1)-hop neighbors; g(·) is the
aggregation function which combines the representation of an
entity with the information propagated from its neighbors.
As we can see, both information propagation and aggregation
functions play essential roles in a GNN module.
In terms of information propagation, as different neighbor-
ing entities should contribute unequally to the ego entity h,
we devise an attention mechanism [29] to discriminate the
importance of system entity neighbors:
),
h
h
(cid:88)
z(l−1)Nh
=
α(h, r, t)z(l−1)
,
t
(h,r,t)∈Nh
where α(h, r, t) is the attention function to control how much
information is propagated from t to h conditioned on a certain
relation r. We design it as follows:
α(h, r, t) = er
t
(cid:62)tanh(er
h + er),
t , er
where er
h, and er are system entity embeddings obtained
from TransR. The attention scores across all neighboring enti-
ties are further normalized by the softmax function. Through
this attentive information propagation, we are able to high-
light informative signals from relevant entities and ﬁlter out
uninformative signals from irrelevant entities.
In terms of information aggregation, we adopt the Graph-
Sage Aggregator [57] to update system entity representations:
)W(l)),
g(z(l−1)
||z(l−1)Nh
) = LeakyReLU((z(l−1)
, z(l−1)Nh
where ·||· is the concatenation operator between two vectors;
W(l) ∈ R2d(l−1)×d(l)
is a transformation matrix at the l-th
propagation layer to distill useful information. As such, we can
integrate the messages of multi-hop neighbors into an entity’s
original representation z(0)
to form a new representation z(l)
h .
h
Speciﬁcally, the number of hops in integrating neighboring
entities is determined by the number of propagation layers L.
h
h
C. Learning to Detect Threats
Having obtained the representations of system entities, we
move on to threat detection — learning to classify system
entity interactions into normal and adversarial.
After L iterations of information propagation and aggre-
gation, we obtain a series of representations for entity h,
{z(0)
h ,··· , z(L)
h }, which encode different-order information in
a KG. Here we employ a simple concatenation operator to
merge them into the ﬁnal representation:
h = z(0)||···||z(L)
z∗
h .
The concatenation introduces no additional parameters to
optimize and preserves information pertinent to different prop-
agation layers, which has achieved promising performance in
recent recommendation systems [57], [58].
Given any interaction (h, interact, t), we apply the inner
product on system entity representations to predict how likely
system entity h would not interact with another entity t:
ˆyht = z∗
h
(cid:62)z∗
t .
If the probability ˆyht is larger than a pre-deﬁned threshold,
we further ﬂag the interaction as a potential cyber threat. To
meet this principle, we learn parameters in the GNN module
by optimizing a widely-used pairwise loss [59]:
(cid:88)
(cid:88)
Lhigher =
σ(ˆyht − ˆyh(cid:48)t(cid:48)),
(h,r0,t)∈GK
(h(cid:48),r0,t(cid:48)) /∈GK
where r0 denotes the interact relation; the interactions ob-
served in a KG built upon normal audit records are viewed as
negative (benign) instances; meanwhile, we randomly sample
interactions unobserved in the KG as positive (potentially
malicious) instances. Note that our sampled interactions do
not necessarily reﬂect cyber threats. We further explain their
impacts on threat detection in Appendix C.
By combining the losses of the ﬁrst-order modeling and the
higher-order modeling, we minimize the following objective
function to learn parameters in our recommendation model:
L = Lf irst + Lhigher + λ(cid:107)Θ(cid:107) ,
where Θ = {eh, er, et, Wr, W(l)|h, t ∈ E, r ∈ R(cid:48), l ∈
{1,··· , L}} is the set of trainable model parameters; λ is
the hyper-parameter that controls the L2 regularization term
to address the over-ﬁtting problem [60].
D. Model Adaption
As system behavior changes, SHADEWATCHER may raise
false alarms for benign system entity interactions unobserved
at the training stage. Consequently, it is necessary to keep
up with the evolution of interactions. In practice, analysts in
security operations centers would continuously sift through
threat alarms to ﬁlter out false positives. Therefore, a natural
way to generalize SHADEWATCHER to evolving interactions
is to include analysts in the loop for dynamic updates.
Towards this end, we provide an option for analysts to give
new labels on false-positive interactions, allowing SHADE-
WATCHER to use false alarms as additional supervision to
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:26 UTC from IEEE Xplore.  Restrictions apply. 
495
distributes the batch to an idle thread under its control. After-
ward, the thread produces a provenance subgraph into a graph
queue. Meanwhile, subgraphs in the graph queue are continu-
ously consumed by the generator to construct the ﬁnal PG. In
our current implementation, the parallel pipeline only works
on audit records in the Common Data Model format [62]
(e.g., DARPA TC dataset format) as it allows independent
record processing. The massive dependencies among audit
records generated by commodity auditing frameworks bring
non-negligible efforts to support concurrent processing [10].
For example, the ﬁle descriptor used in a read record is always
deﬁned in open or socket records. We believe that the parallel
or even distributed PG construction by itself is an interesting
research topic, and we leave such an extension to future work.
Recommendation Model Training: We implement our rec-
ommendation model using Google Tensorﬂow [63]. The model
is optimized by Adam optimizer [64], where the batch size,
margin γ, and normalization λ are ﬁxed at 1024, 1, and 10−5.
We train the model for 30 epochs with an early stop strategy
— the training will be terminated if the accuracy does not
increase on the validation set for ﬁve successive epochs. To
mitigate the over-ﬁtting problem, we further employ a dropout
technique [65] with a dropping ratio of 0.2.
We initialize model parameters Θ with Xavier [66]. For
hyper-parameters, we apply a grid search: the learning rate is
tuned in {0.0001, 0.001, 0.01}; the embedding size of system
entities is searched in {16, 32, 64}; the number of propagation
layers in GNNs is tuned in {1, 2, 3}; and the threshold is
searched in {-1, -0.5, 0, 0.5, 1}. In light of the best accuracy,
we report results in a setting with the learning rate as 0.001,
the embedding size as 32, two propagation layers with hidden
dimensions as 32 and 16, and the threshold as -0.5.
VIII. EVALUATION
We evaluate SHADEWATCHER on four aspects: 1) How
effective is SHADEWATCHER as a threat detection system?
(§ VIII-B) 2) How do ﬁrst-order and high-order information
facilitate detection? (§ VIII-C) 3) What is the capability of
the model adaption to reduce false alarms? (§ VIII-D) 4) How
efﬁcient is SHADEWATCHER? (§ VIII-E)
All experiments are performed on a server with Intel Xeon
E5-2620 v4 CPUs @ 2.10GHz, 64 GB physical memory, and
an NVIDIA Tesla V100 GPU. The OS is Ubuntu 16.04.3 LTS.
A. Dataset
In our evaluation, we use both a public DARPA TRACE
dataset [48] (henceforth called TRACE dataset) for the repro-
duction of experimental results, as well as a simulated dataset
to explore SHADEWATCHER’s efﬁcacy in practice. Table II
summarizes the statistics of provenance graphs built upon our
experimental datasets. Note that the dataset statistics are not
necessarily the same as those of existing studies due to differ-
ent noise reduction strategies and system entity granularities.
For example, BEEP [67] partitions long-running processes into
ﬁner-grained execution units, causing a signiﬁcant increase in
the overall volume of system entities.
Fig. 4: Parallel Provenance Graph Construction.
revise its recommendation model. For example, suppose (gt-
cache, interact, /proc/27/stat) has been detected as malicious
but later manually veriﬁed as a false alarm. To avoid future
mistakes for similar interactions, SHADEWATCHER feeds this
interaction as a new negative instance to retrain its model.
More speciﬁcally, to verify the nature of an alarm, ana-
lysts need to reconstruct the attack scenario by tracking the
provenance between two system entities in the potentially
malicious interaction. The main challenge faced by analysts is
to understand previously unseen interactions, e.g., the ﬁrst time
a program loads a conﬁguration ﬁle. To facilitate interpretation
of such interactions, an intuitive approach is to incorporate ad-
ditional auxiliary information — e.g., binary analysis, program
comprehension, and network monitoring — into the KG so
that analysts can reason about new interactions from different
aspects, which we leave for future work.
We acknowledge that low-quality (e.g., false) feedback may
mislead the recommendation model. However, as SHADE-
WATCHER provides ﬁne-grained detection signals that high-
light key indicators of an attack, analysts have a high chance
to correctly differentiate true and false alarms.
VII. IMPLEMENTATION
We develop SHADEWATCHER in 11K lines of C++ code
and 3K lines of Python code. We present important technical
details in the implementation3.
System Auditing Collection: To collect whole-system audit
records, we make use of the Linux Audit with a ruleset cover-
ing 32 types of commonly-used system calls (see Appendix D
for details). Once an audit record is generated, it is processed
into a JSON format and shipped to SHADEWATCHER through
Apache Kafka [61] in a stream fashion.