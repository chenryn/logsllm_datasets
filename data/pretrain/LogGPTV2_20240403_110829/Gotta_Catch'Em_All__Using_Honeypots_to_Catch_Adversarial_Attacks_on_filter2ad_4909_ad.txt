98.7% 98.2% 100%
PGD BPDA SPSA FGSM
94.1%
98.3%
97.3%
94.8%
100%
97.6%
99.4%
97.5%
100%
97.2%
100%
96.3%
Model
MNIST
GTSRB
CIFAR10
FS
MagNet
LID
Trapdoor
FS
MagNet
LID
Trapdoor
FS
MagNet
LID
Trapdoor
FS
YouTube
MagNet
Face
LID
Trapdoor
Detector FPR
73%
CW EN
99% 100% 94%
PGD BPDA SPSA FGSM
Avg
Succ.
98%
97%
94%
5%
96%
100% 94%
96%
5.7% 83% 87% 100% 97%
92%
95%
89% 86% 96%
98%
5%
86%
94%
97% 98% 100% 100% 100%
5%
98%
45%
90%
5% 100% 99% 71%
94%
100% 95%
4.7% 90% 89% 100% 100% 92%
91% 81% 100% 67% 100% 100% 90%
5%
97%
5%
96% 97% 98%
98%
97%
97%
33%
5% 100% 100% 69%
78%
100% 93%
94%
7.4% 88% 82% 95%
90%
92%
90% 88% 95%
5%
96%
97%
94% 94% 100% 99% 100%
5%
97%
59%
5% 100% 100% 66%
88%
68%
80%
95%
96%
98%
97%
7.9% 89% 91% 98%
85%
96%
92%
81% 79% 89%
5%
72%
99% 98% 100% 97%
5%
96%
95%
98%
98%
66%
96%
79%
to the original model. This performance drop can potentially be fur-
ther reduced by optimizing the conﬁguration of trapdoors, which
we leave as future work.
Accuracy of Detecting Adversarial Inputs. We run each of the
six attacks to ﬁnd adversarial perturbations against each label of
the model and then run our trapdoor-based detection to examine
whether an input is adversarial or benign. The adversarial detection
success rate is above 94.0% at a FPR of 5% (and 88.3% for FPR of
2%). The detailed results are listed in Table 4.
These results show that, for the all-label defense, adversarial de-
tection accuracy drops slightly compared to the single-label defense.
The drop is more visible for YouTube Face, which has signiﬁcantly
more labels (1,283). We believe that as more trapdoors are injected
into the model, some of them start to interfere with each other, thus
reducing the strength of the shortcuts created in the feature space.
This could potentially be ameliorated by carefully placing trapdoors
with minimum interference in the feature space. Here, we apply
a simple strategy described in Section 4.2 to create separation be-
tween trapdoors in the input space. This works well with a few la-
bels (i.e. 10, 43). For models with many labels, one can either apply
greedy, iterative search to replace “interfering” trapdoor patterns, or
develop an accurate metric to capture interference within the injec-
tion process. We leave this to future work.
For the all-label defense, trapdoor-
Summary of Observations.
enabled detection works well across a variety of models and adver-
sarial attack methods. The presence of a large number of trapdoors
only slightly degrades normal classiﬁcation performance. Overall,
our defense achieves more than 94% attack detection rate against
CW, PGD, ElasticNet, SPSA, FGSM, and more than 97% attack de-
tection rate against BPDA, the strongest known attack.
6.4 Comparison to Other Detection Methods
Table 5 lists, for all-label defenses, the attack detection AUC for our
proposed defense and for three other existing defenses (i.e. feature
squeezing (FS) [50], MagNet [33], and latent intrinsic dimensional-
ity (LID) [31] described in Section 2.2). For FS, MagNet, and LID,
we use the implementations provided by [31, 33, 50]. Again we
consider the four tasks and six attack methods as above.
Feature Squeezing (FS). FS can effectively detect gradient-based
attacks like CW and ElasticNet, but performs poorly against FGSM,
PGD, and BPDA, i.e. the detection success rate even drops to 33%.
These ﬁndings align with existing observations [30, 50].
MagNet. MagNet performs poorly against gradient-based attacks
(CW, ElasticNet) but better against FGSM, PGD, and BPDA. This
aligns with prior work, which found that adaptive gradient-based
attacks can easily defeat MagNet [9].
LID has ≥ 72% detec-
Latent Intrinsic Dimensionality (LID).
tion success rate against all six attacks. In comparison, trapdoor-
based detection achieves at least 94% on all six attacks. Like [2], our
results also conﬁrm that LID fails to detect high conﬁdence adver-
sarial examples. For example, when we increase the “conﬁdence”
parameter of the CW attack from 0 (default) to 50, LID’s detection
success rate drops to below 2% for all four models. In comparison,
trapdoor-based detection maintains a high detection success rate
(97-100%) when conﬁdence varies from 0 to 100. Detection rate
reaches 100% when conﬁdence goes above 80. This is because high
conﬁdence attacks are less likely to get stuck to local minima and
more likely to follow strong “shortcuts” created by the trapdoors.
6.5 Methods for Computing Neuron Signatures
We study how the composition of trapdoor (neuron) signature af-
fects adversarial detection. Recall that, by default, our trapdoor-
based detection uses the neuron activation vector right before the
softmax layer as the neuron signature of an input. This “signature”
is compared to the trapdoor signatures to determine if the input is
an adversarial example. In the following, we expand the composi-
tion of neuron signatures by varying (1) the internal layer used to
extract the neuron signature and (2) the number of neurons used,
and examine their impact on attack detection.
First, Figure 10 in Appendix shows the detection success rate
when using different layers of the GTSRB model to compute neu-
ron signatures. Past the ﬁrst two convolutional layers, all later lay-
ers lead to detection success greater than 96.20% at 5% FPR. More
importantly, choosing any random subset of neurons across these
later layers produces an effective activation signature. Speciﬁcally,
sampling n neurons from any but the ﬁrst two layers of GTSRB pro-
duces an effective trapdoor signature with adversarial detection suc-
cess rate always above 96%. We ﬁnd this to be true for a moderate
value of n∼900, much smaller than a single convolutional layer. We
conﬁrm that these results also hold for other models, e.g. CIFAR10.
It is important that small sets of neurons randomly sampled across
multiple model layers can build an effective signature. We leverage
this ﬂexibility to defend against our ﬁnal countermeasure (§7.2).
7 ADAPTIVE ATTACKS
Beyond static adversaries, any meaningful defense must withstand
countermeasures from adaptive attackers with knowledge of the de-
fense. As discussed in §3.1, we consider two types of adaptive ad-
versaries: skilled adversaries who understand the target Fθ could
have trapdoors without speciﬁc knowledge of the details, and or-
acle adversaries, who know all details about embedded trapdoors,
including their trapdoor shape, location, and intensity. Since the or-
acle adversary is the strongest possible adaptive attack, we use its
detection rate as the lower bound of our detection effectiveness.
We ﬁrst present multiple adaptive attacks separated into two broad
categories. First, we consider removal approaches that attempt to
detect and remove backdoors from the target model Fθ , with the
eventual intent of generating adversarial examples from the cleaned
model, and using them to attack the deployed model Fθ . Second, we
consider evasion approaches that do not try to disrupt the trapdoor,
and instead focus on ﬁnding adversarial examples that cause the
desired misclassiﬁcation while avoiding detection by the trapdoor
defense. Our results show that removal approaches fail because the
injection of trapdoors largely alters loss functions, and even adver-
sarial examples from the original, trapdoor-free model do not trans-
fer to the trapdoored model.
Finally, we present advanced attacks developed in collaboration
with Dr. Nicholas Carlini during the camera ready process. We de-
scribe two customized attacks he proposed against trapdoors and
show that they effectively break the base version of trapdoors. We
also offer preliminary results that show potential mitigation effects
via inference-time signature randomization and multiple trapdoors.
We leave further exploration of these mechanisms (and more pow-
erful adaptive attacks) to future work.
7.1 Trapdoor Detection and Removal
Backdoor Countermeasures (Skilled Adversary). We start by
considering existing work on detecting and removing backdoors
from DNNs [27, 28, 37, 48]. A skilled adversary who knows that
a target model Fθ contains trapdoors may use existing backdoor
removal methods to identify and remove them. First, Liu et al. pro-
poses to remove backdoors by pruning redundant neurons (neuron
pruning) [27]. As previous work demonstrates [48], normal model
accuracy drops rapidly when pruning redundant neurons. Further-
more, pruning changes the decision boundaries of the pruned model
signiﬁcantly from those of the original model. Hence, adversarial
examples that fool the pruned model do not transfer well to the orig-
inal, since adversarial attacks only transfer between models with
similarly decision boundaries [14, 45].
We empirically validated this on a pruned single-label defended
MNIST, GTSRB, CIFAR10, and YouTube Face models against the
six different attacks. We prune neurons as suggested by [27]. How-
ever, we observe that normal accuracy of the model drops rapidly
while pruning (> 32.23% drop). Due to the signiﬁcant discrepancy
between the pruned and the original models, adversarial samples
crafted on the pruned model do not transfer to the original trap-
doored model. Attack success is  95% success at 5% false positive rate, for all six attacks (FGSM,
PGD, CW, EN, BPDA, SPSA).
If somehow an attacker obtained access to the full training dataset
used by the model and used it to build a surrogate model, they
could reproduce the original clean model. We consider this possi-
bility later in this subsection.
Unlearning the Trapdoor (Oracle Adversary). The goal of this
countermeasure is to completely remove trapdoors from the target
model Fθ so that attackers can use it to generate adversarial sam-
ples to attack Fθ . Prior work has shown that adversarial attacks
can transfer between models trained on similar data [14, 45]. This
implies that attacks may transfer between cleaned and trapdoored
versions of the target model.
For this we consider an oracle attacker who knows everything
about a model’s embedded trapdoors, including its exact shape and
intensity. With such knowledge, oracle adversaries seek to construct
a trapdoor-free model by unlearning the trapdoors.
However, we ﬁnd that such a transfer attack (between Fθ and
unl e ar n) fails. We
a version of it with the trapdoor unlearned Fθ
validate this experimentally using a single-label defended model.
The high level results are summarized in Table 6. We create a new
version of each trapdoored model using backdoor unlearning tech-
niques [5, 48], which reduce the trapdoor injection success rate
from 99% to negligible rates (around 2%). Unsurprisingly, the trap-
door defense is unable to detect adversarial samples constructed on
unl e ar n, with only 7.42% detection success
the cleaned model Fθ
rate at 5% FPR for GTSRB. However, these undetected adversarial
samples do not transfer to the trapdoored model Fθ . For all six at-
tacks and all four models, the attack success rate on Fθ ranges from
0% to 6.7%. We hypothesize that this might be because a trapdoored
unl e ar n
model Fθ must learn unique trapdoor distributions that Fθ
does not know. This distributional shift causes signiﬁcant differ-
ences that are enough to prevent adversarial examples from trans-
ferring between models.
Oracle Access to the Original Clean Model. Unlearning is un-
likely to precisely recover the original clean model (before the trap-
door). Finally, we consider the strongest removal attack possible: an
oracle attacker that has somehow obtained access to (or perfectly re-
produced) the original clean model. We evaluate the impact of using
the original clean model to generate adversarial attacks on Fθ .
We are surprised to learn that adding the trapdoor has introduced
signiﬁcant changes in the original clean model, and has thus de-
stroyed the transferability of adversarial attacks between them. In