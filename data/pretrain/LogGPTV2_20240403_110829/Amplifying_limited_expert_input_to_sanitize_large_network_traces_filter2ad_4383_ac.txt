10
15
20
25
30
Number of clusters
35
40
45
0.6
10
20
30
40
60
Number of clusters
50
70
80
90
(a) UNV-DNS
(b) KDDCup-FTP
(c) Wireshark-SMB
Figure 4. Average Fùõº when 40%, 80% or 100% of the sensitive tokens in representatives are marked (at random) by a simulated worker.
information, yielding a recall of 1.0 but potentially very low
precision ‚Äî we will generally set ùõº ‚â• 1 in our analysis.
In the analysis that follows, we present results from a user
study in which professional administrators were recruited to
participate, in order to gain a better understanding of the
effectiveness of our approach in enabling them to identify
sensitive Ô¨Åelds. To estimate parameter settings for this study,
we Ô¨Årst conducted a simulation-based analysis (with no hu-
man interaction) to evaluate the effectiveness of propagating
marked tokens in the representatives (i.e., tokens identiÔ¨Åed
as sensitive) to the remainder of the dataset.
A. Exploring the Parameter Space
One advantage of our technique is in generating a limited
number of representative packets that capture the character-
istics of the packets in the dataset. That said, the manner
in which we do so could impact our identiÔ¨Åcation accuracy.
Therefore, to choose the most appropriate parameters for
our user study (in particular, the number of clusters to use),
we performed an analysis in which we simulated a single
worker who marked (identiÔ¨Åed) each instance of a sensitive
Ô¨Åeld independently and with a Ô¨Åxed probability. We reiterate
that the sole purpose of the simulation-based analysis was to
provide guidance on parameter choice for the Ô¨Åeld study that
followed. With that in mind, we made certain assumptions
(about independence) for the simulated user to simplify the
task of exploring the parameter space. We then measured the
F-score when mapping these random markings of sensitive
Ô¨Åelds to the full dataset, as described in Section III-F (though
using the inputs of only a single simulated worker, not two
in combination). In this evaluation, the simulated worker did
not mark non-sensitive Ô¨Åelds as sensitive, leading to higher
precision than might occur in practice (though the precision
on the full dataset was nevertheless always less than 1.0).
This was done to focus on the effects of recall or, more
speciÔ¨Åcally, F-score with ùõº ‚â• 1.
Below, we evaluate Fùõº of our technique across different
numbers of clusters and for ùõº = 1.2. We selected 2000
samples from each original dataset using the sampling
described in Section III-B. We also controlled the number of
representative packets by Ô¨Åxing it irrespective of the number
of clusters. SpeciÔ¨Åcally, the number of representative packets
was chosen to be 140 in the UNV-DNS dataset, 108 in the
KDDCup-FTP dataset and 120 in the Wireshark-SMB
dataset; these numbers constituted only 0.70%, 0.34% and
0.54% of the total number of packets in each dataset,
respectively. These numbers of representatives resulted from
using the technique described in Section III at the Ô¨Ånest
clustering (i.e., yielding the most clusters). This number
was then Ô¨Åxed as the target number of representatives in
Algorithm 1 when fewer clusters were allowed.
The average F-score for each simulated case and dataset is
shown in Figure 4. Each point in this Ô¨Ågure is the average of
Ô¨Åve runs. The standard deviation is 0.006, 0.020 and 0.025
across all datapoints for the UNV-DNS, KDDCup-FTP and
Wireshark-SMB datasets, respectively. While our primary
use for these simulations is parameter selection (see below),
we pause to make three observations from these Ô¨Ågures.
First, the number of clusters has a large impact on how
well the process works, even when identiÔ¨Åcation of sensitive
tokens in the representatives is perfect. For example, in
Figures 4(b)‚Äì4(c), a clustering with too few clusters decays
the F-score to roughly only 60% of its optimal. We presume
this occurs because with enough clusters, clusters better
separate the packets of different message types, yielding
higher quality representatives. Second, once an adequate
number of clusters is attained, the F-scores are robust to
imperfect identiÔ¨Åcation of sensitive tokens in the represen-
tatives. Third, when there are sufÔ¨Åciently many clusters, the
F-scores that can be realized indicate that the Ô¨Ånal outcome
can be quite successful (e.g., F-scores near 1.0 in all cases).
Based on these tests, we selected 40 clusters for the tests
described in the rest of the paper. For our datasets, this
provides a good balance between minimizing the number
of clusters that workers are asked to inspect and providing
the opportunity for good accuracy in identifying sensitive
data, once worker markings are applied to the full dataset.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:32:40 UTC from IEEE Xplore.  Restrictions apply. 
500When selecting the number of clusters in practice, we expect
that an expert would be helpful here, as well; that is, the
expert can be used to judge the quality of the clustering
based on the visual similarity of the packets in each cluster.
Thereafter, these clusters can be reÔ¨Åned iteratively using the
approach presented in Section III-C.
B. User Study with Professional Network Administrators
Our techniques for selecting representatives and incor-
porating user feedback about those representatives to san-
itize the full dataset (Section III) are not dependent on
any particular method for soliciting that feedback from
users. However, we expect that the method of presenting
representative packets to users will have a large impact
on their abilities to identify and mark sensitive tokens.
Two design decisions we made‚Äîbased on what is known
about visual pattern recognition by humans‚Äîwere to present
representative packets as groups based on the clusters to
which they belonged (Section III-C) and to present
the
representatives in their aligned forms (Section III-D).
To determine the impact of these design decisions, and
more generally, to evaluate the utility of our overall ap-
proach, we conducted an IRB-approved user study with par-
ticipants recruited from our department‚Äôs Technical Support
Center and the university‚Äôs Information Technology Service
group. All participants were professional administrators with
good networking background and familiarity with inspecting
packet
traces as part of routine network monitoring or
diagnostic duties. We targeted professional administrators as
they are the natural audience for our tool; after all, they
are likely the people who would be tasked with the job
of sanitizing network data before its release. This stringent
criterion for selecting study participants, however, severely
limited the available pool of participants at our university,
resulting in our study population of size 15. We note that
we obtained consent from these 15 participants only after
signiÔ¨Åcant efforts to recruit them. These participants are
considered our ‚Äúworkers‚Äù in the remaining discussion.
1) Study Design: Recall that our primary goal was to
assess the impact of both clustering and alignment in helping
workers uncover potentially sensitive Ô¨Åelds in packet pay-
loads. To that end, each worker was tasked with identifying
the seven speciÔ¨Åed Ô¨Åelds of interest within the packets
displayed via a graphical user interface (see appendix). The
study itself comprised four trials in which the payloads of
packets were presented to the subjects in different ways.
Each trial employed a set ‚Ñõ of representative packets. How-
ever, the payloads of these representatives were displayed in
different forms in the four trials as follows:
Trial I (Clustering+Alignment). The representative pack-
ets ‚Ñõ were partitioned according to the clusters from which
they were selected. The representative packets were dis-
played to the worker, one cluster per page, in their aligned
forms produced during their selection (Section III-D).
Trial II (Alignment+NoClustering). The representative
packets ‚Ñõ were partitioned randomly into blocks. The total
number of blocks was the same as the number of clusters
in Trial I, but the representatives were evenly distributed
across all blocks. The representatives were displayed to the
worker, one block per page. Since the packets in each block
were randomly selected and not aligned with each other,
we aligned these packets using the method described in
Section III-D and presented them in that aligned form to
the worker.
Trial III (Clustering+NoAlignment). The representative
packets ‚Ñõ were partitioned according to the clusters from
which they were selected. The representatives were dis-
played one cluster per page, in their original form (un-
aligned).
Trial IV (NoClustering+NoAlignment). The representa-
tive packets ‚Ñõ were partitioned randomly into blocks. The
total number of blocks was the same as the number of
clusters in Trial I, but the representative packets were evenly
distributed across all the blocks. The representative packets
were displayed to the worker, one block per page, with each
packet displayed in its original form (unaligned).
For the user study, we chose to use the UNV-DNS
and KDDCup-FTP datasets because they contain diverse
types of potentially sensitive information. Two groups of
representative packets, one for each dataset, were gener-
ated by applying the techniques in Section III to the two
datasets separately. In each trial for a given subject, only
one set of representative packets were used, that is, either
‚Ñõ(UNV-DNS) or ‚Ñõ(KDDCup-FTP).
Each worker undertook all four trials in individual meet-
ings over a period of several weeks, with at least three days
between trials. To avoid any learning effects across trials,
we incorporated several additional design elements into our
study. First, for the trials taken by each worker, we ensured
that the datasets used were evenly split across the trials.
Second, to prevent displaying the same representatives on
the same page in any two trials for a particular user, we
ensured that Trials I and III displayed different representative
packets. This constraint was also applied to the two non-
clustering trials (i.e., Trials II and IV). Third, the order
of the trials was randomly chosen (per subject), and we
ensured that no two trials that used the same data (e.g.,
‚Ñõ(UNV-DNS)) were undertaken back-to-back.
Moreover, to limit any factors due to fatigue, the worker
was restricted to only one trial per meeting. Meetings were
limited to roughly 30 minutes in length, with the exception
of the Ô¨Årst meeting where the worker was given a brief
introduction (with ample time for questions and answers),
and time to familiarize herself with the GUI using an
artiÔ¨Åcial dataset. All trials were administered on a dedicated
laptop, in a location of the subject‚Äôs preference. At each
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:32:40 UTC from IEEE Xplore.  Restrictions apply. 
501meeting, the worker was asked to mark any occurrence of
the speciÔ¨Åed Ô¨Åeld types, with timeliness as a secondary goal.
Care was taken to ensure that the subject was not asked to
mark content she thought could be sensitive, as doing so
would be subjective and would inevitably lead to uncertainty
about what should, or should not, be marked.4 That is,
her job was to simply mark any tokens in the displayed
sequences that she believed to be a domain name, an IP
address, a Ô¨Åle (or directory) name, a user name, a password,
a host name, or an email address.
1
0.9
0.8
0.7
Œ±
F
0.6
0.5
0.4
0.3
0.2
(UNV‚àíDNS)
R
(KDDCup‚àíFTP)
R
Figure 5. F-scores (ùõº = 1.2) per worker in Trial I
2) Results: Figure 5 shows box-and-whisker plots of Fùõº
of Trial I for all workers, with ùõº = 1.2. Note that these
F-scores were computed using the worker‚Äôs precision and
recall on the representatives only, rather than after applying
their markings to the full dataset. Each box represents
the Ô¨Årst, second, and third quartiles; whiskers cover the
remaining points. Figure 5 illustrates that the workers were
generally much more successful
in identifying sensitive
Ô¨Åelds in FTP packets, in some cases reaching an F-score
exceeding 0.95. The results on ‚Ñõ(UNV-DNS) were not as
encouraging; no F-score greater than 0.70 was achieved by
any worker. We believe this reÔ¨Çects the substantial challenge
represented by DNS payloads, where the variety of locations
in which IP addresses can appear makes identiÔ¨Åcation of
such Ô¨Åelds a real challenge.
This motivates the need to select only the best workers
for identifying sensitive data, and then to employ multiple
workers; see Section III-F. For the remainder of our study,
we chose the two best workers as determined by their F-
scores on a randomly selected 20% of the representatives
that each marked. This choice simulates a scenario in which
the expert marked 20% of the representatives, and then
workers were tasked with marking the remaining 80%. The
chosen workers were selected based on their F-scores using
the expert-marked data as ground truth. In our tests, we
4Consider, for example, the username ‚Äúanonymous‚Äù which is not uncom-
mon in FTP; is it sensitive, or not?
possessed ground truth and so did not need to involve an
expert directly.
Best worker
1
2
Combined
Best worker
1
2
Combined
(a) UNV-DNS
Recall
0.504
0.833
0.900
Precision
0.991
0.674
0.930
(b) KDDCup-FTP
Recall
1.000
0.958
1.000
Precision
0.974
0.974
0.974
F-score
0.631
0.760
0.912
F-score
0.989
0.964
0.989
RESULTS OF APPLYING MARKINGS TO THE FULL DATASET FOR A
SINGLE WORKER AND THE COMBINED WORKERS
Table I
Once the two best workers were selected in this way,
their markings were applied to the full dataset as described
in Section III-F. Table I provides the F-scores for the full
datasets when applying each of these workers‚Äô markings
individually and then in combination. As these results show,
in the case of the UNV-DNS dataset (Table I(a)), the recall
of the combined case increases up to 0.9 with a small loss of
precision when we incorporate the opinions of the two best
workers. For the KDDCup-FTP dataset, the measurement of
the single worker versus the combined result remains very
close (nearly 1.0) because each worker already had high
recall and precision on that dataset.
3) On Understanding Mixed Effects: While the previous
results show that substantial improvement in accuracy can be
achieved by picking the best workers and combining their in-
put, it is yet to be shown that the clustering and/or alignment
aspects of our approach are indeed factors in boosting the
workers‚Äô performance. To explore the extent to which these
two components inÔ¨Çuence a worker‚Äôs performance, we Ô¨Årst
show (in Figure 6) box-and-whisker plots of Fùõº across the
four trials for the selected best workers. Notice that Trials
I‚ÄìIII generally outperform Trial IV. Notice as well that in
Figure 6(b), Trial I performs the best, and offers a substantial
improvement over Trial IV.
To gain a deeper understanding of the statistical signif-
icance of these trends, we apply a mixed-effect regression
model to analyze the four trials of the selected best workers
shown in Figure 6. A mixed-effect model is an extension of
the general linear regression model that allows for correla-
tions within observations [18]. For instance, in our context,
this would mean that we consider the performance (say, in
terms of efÔ¨Åciency) of a particular worker across different
datasets to be correlated, but consider that of different
workers to be independent. Conceptually, the mixed effect
regression model can be formulated as: ùë¶ = Ô¨Åxed eÔ¨Äects +
random eÔ¨Äects + error, where the random effects control for