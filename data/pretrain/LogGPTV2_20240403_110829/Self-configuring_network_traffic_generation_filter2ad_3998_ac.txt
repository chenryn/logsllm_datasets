as 300 or 600 seconds, and a reasonable bound on round-
trip times. We determine how many sessions are required to
generate the required byte volume over the duration of an
2There are also inaccuracies that are related to the coarse-
grained nature of ﬂow records. Since we subtract contribu-
tions of headers (including connection setup and teardown
packets and associated latency) to ﬁle sizes, we would really
like the start and end timestamps to reﬂect the ﬁrst and
ﬁnal payload packets of a ﬂow.
3The DAG capture point was situated one hop after the
6509. Diﬀerences in timestamps due to propagation and
forwarding delays were assumed to be negligible.
interval by mimicking the action of Harpoon sessions as they
alternate between waiting for an amount of time drawn from
the PInterConnection distribution and requesting ﬁles whose
sizes are drawn from the PF ileSize distribution4. We succes-
sively increment the number of sessions, imitating the action
of each session and noting when we surpass the requisite byte
volume for the given interval.
In practice, we run the algorithm for each interval N
times, using the mean number of sessions required to gener-
ate the necessary volume. As shown in the validation exper-
iments of Harpoon, tuning the number of active sessions to
the required byte volume using the technique we outline here
leads to an accurate match with the original byte, packet,
and ﬂow volumes over relatively coarse intervals.
BytesGenerated = 0
SessionsRequired = 0
# Total bytes generated by mimicked
# sessions.
# Sessions required to create the
# intended volume.
while BytesGenerated < IntendedByteVolume: # We want to generate at least as many
# bytes that were originally sent.
SessionsRequired += 1
# One more session is required...
ElapsedTime = 0
# ElapsedTime holds amount of time the
# current session has been active
# during this interval.
while ElapsedTime < IntervalDuration:
# This loop mimics the action
# of a single session.
ElapsedTime += InterConnectionTimes.next() # Get the next inter-connection
# time from the empirical distribution.
BytesGenerated += FileSizes.next() # Get the next file size from
# the empirical distribution (and
# assume the file is transferred
# in the current interval).
end while
end while
# The number of sessions needed to generate required byte volume is
# now stored in the variable SessionsRequired.
Figure 4: Pseudocode for algorithm used to de-
termine the number of sessions that should be ac-
tive over a given interval to produce a certain byte
volume.
Intended byte volume, IntervalDuration,
PF ileSize and PInterConnection distributions are given
as inputs to the algorithm.
Interval Duration The value IntervalDuration is the time
granularity over which we match byte, packet, and ﬂow vol-
umes between the originally measured ﬂow data and Har-
poon. The duration can be selected somewhat arbitrar-
ily, but there are two practical constraints. First, since we
choose a source and destination address for each active ses-
sion at the start of each interval, there is a tradeoﬀ between
4Note that the connection initiation schedule deﬁned by the
PInterConnection distribution is made independent of net-
work feedback.
the length of an interval and how well the spatial distri-
bution can be approximated over the course of a full test.
With longer intervals, there is less opportunity for sampling
each spatial distribution. With shorter intervals, there is
an increased internal overhead from managing sessions and
timers.
In our experience, intervals such as ﬁve minutes
work well. Five minutes also happens to be a common inter-
val over which ﬂow records are aggregated (as implemented
by the widely used flow-tools [36]). The second practical
consideration is that IntervalDuration should be at least
as long as the maximum inter-connection time. One reason
is that a session may randomly get an inter-connection time
that causes it to be idle for the entire interval. Another
reason is that at the end of an interval, there may be some
sessions that are waiting for an inter-connection time to ex-
pire before initiating a connection – we do not prematurely
halt sessions at the end of an interval to ensure sampling the
longest inter-connection times. The sessions that are wait-
ing for the next connection time at the end of an interval
are technically active but not engaged in ﬁle transfers and
count against the target number of active sessions for the
new interval. The eﬀect of these temporarily frozen sessions
is that Harpoon may not generate the intended volumes for
every interval after the ﬁrst. Setting IntervalDuration to
a large enough value along with setting an appropriate cap
on the maximum inter-connection time when processing the
original ﬂow data resolves this potential problem.
The value IntervalDuration is used during parameteri-
zation for determining the PActiveSessions distribution and
is also used to control traﬃc generation. The values used
for each phase need not be the same. This decoupling en-
ables time compression (or expansion) relative to the orig-
inal data during traﬃc generation. Harpoon can either be
set to match the aggregate volume of the original intervals
or to match the bitrate. For example, if N TCP sessions are
measured from ﬂow logs in interval Ij and M TCP sessions
are measured in interval Ij+1, Harpoon could be conﬁgured
to initiate N + M sessions over a test interval Ik to real-
ize similar aggregate volume. To match the original bitrate,
the traﬃc generation IntervalDuration is simply set to a
smaller value than the duration used for parameterization.
However, there is a potential pitfall when matching bitrates
by using a shorter traﬃc generation IntervalDuration. If
the interval is too short, there may be insuﬃcient sampling
of the PInterConnection and PF ileSize distributions with the
conﬁgured number of active sessions. The result is that the
byte and packet volumes generated over the interval may
vary far from the expected value.
While it is true that our use of rather coarse intervals for
matching original volumes tends to ignore issues of packet
arrivals over short (sub-RTT) time scales, this is intentional.
Additional testbed parameters could have been required with
a Harpoon setup (such as a distribution of round-trip times,
link capacities, MTUs, etc.) to match parameters derived
from a live trace, but at this point we do not specify such
conﬁgurations. Creating the necessary volumes over longer
time scales to produce self-similarity and diurnal patterns in
a way that real application traﬃc is generated is the intent
and domain of Harpoon. We believe it is preferable to allow
burstiness over short time scales to arise endogenously from
TCP closed-loop control and network state, rather than to
exogenously attempt to force such state (e.g., by introduc-
ing a random packet dropping element along a path to eﬀect
a certain loss rate). We demonstrate limitations of Harpoon
with respect to short time scales in Section 5.3.
4.2 Trafﬁc Generation
Harpoon is implemented as a client-server application.
Figure 5 illustrates the conceptual aspects of the implemen-
tation. A Harpoon client process consists of threads that
generate ﬁle requests. The hierarchical session/connection
model is realized by individual threads, and the distribution
for active sessions maps to a number of threads actively mak-
ing ﬁle requests. In each thread, connections are initiated
according to the inter-connection time distribution. The du-
ration of any ﬁle transfer is dictated by the dynamics of the
transport protocol and the underlying testbed conﬁguration.
Inter-connection times, on the other hand, are followed in-
dependently of the transport layer so an active session may
be multiplexing multiple concurrent connections. A Har-
poon server process consists of threads that service Harpoon
client requests. The server controls the sizes of ﬁles trans-
ferred according to the input distribution.
Figure 5: Harpoon software architecture.
A
core session manager controls dynamically loadable
traﬃc generator plugin modules and through an
XML/RPC interface indirectly handles remote re-
quests to stop or start plugins, load new conﬁgura-
tions, or retrieve plugin statistics.
While TCP ﬁle transfers are controlled by protocol dy-
namics, UDP dictates no control over packet emissions. Cur-
rently, Harpoon can send UDP datagrams at roughly con-
stant bit rates (conﬁgurable, but by default 10kbps), at ﬁxed
intervals, and at exponentially distributed intervals. Param-
eters used for these traﬃc sources in our tests are summa-
rized in Table 3. Unlike the constant bit rate source type,
the latter two UDP source types have no notion of multi-
packet ﬁles or inter-connection times; all “ﬁles” consist of
a single datagram whose size depends on ﬁle size distribu-
tions for each source type. As such, ﬁxed interval and ex-
ponentially distributed interval sources do not require any
control messages to coordinate when packets are sent. The
same is not true for the constant bit rate sources, where we
must ensure that ﬁle requests take place according to the ﬁle
size and inter-connection request distributions. Each client
maintains a TCP control connection with the target server
over the duration of an active UDP session. File requests
are made over this channel and UDP data is then sent by
the server. The client sends the port number of a locally
bound UDP socket in order for the server to set the proper
destination of UDP traﬃc. The client additionally sends
a datagram size and rate for the server to use when send-
ing data, and a unique request identiﬁer. Once the server
ﬁnishes sending the requested ﬁle, it sends a completion in-
dication to the client for the original request identiﬁer.
In addition to the distributional input parameters, each
emulation. . .testbedsupport functionsadditionallogging, statistics,Other pluginsbitrate generatorUDP constantgeneratorTCP trafficmanagementremoteXML/RPC. . .. . .thread/session managerharpoon software architectureselfconfigurationTable 3: Summary of Harpoon conﬁguration param-
eters for UDP sources used in validation tests.
Source Type
Constant Packet Rate
Fixed-interval Periodic Ping-Pong
Exponentially Distributed Ping-Pong
Key Parameter
Rate = 10kbps
Interval = 64 seconds
λ = 30 milliseconds
Harpoon client is given a range of local IPv4 addresses to use
as a source address pool, and a range of IPv4 addresses and
associated ports of target Harpoon servers. Address ranges
are speciﬁed as CIDR preﬁxes to Harpoon. The source ad-
dresses may be bound to physical interfaces on the client
host or to address aliases assigned to an interface. When
starting a new user, a thread picks a source address and
destination address from this pool. The address pools are
constructed in such a way that random selection of addresses
generates the required spatial frequency distribution.
Harpoon is designed in a modular fashion to enable rel-
atively easy addition of new traﬃc models. Traﬃc genera-
tion modules for TCP, UDP constant bit-rate stream, and
the ﬁxed and exponential interval UDP datagram sources
are implemented as plugin modules that are dynamically
loaded and controlled by a core Harpoon thread manager.
Harpoon reads XML parameter ﬁles produced by the self-
conﬁguration components, loading and starting the neces-
sary plugin modules.
Management of resources and tools within large-scale test-
bed environments can be challenging. To counter this prob-
lem we implemented an HTTP server in Harpoon, along
with an XML parser to enable remote management via XML-
RPC [15]. Users can remotely reset, stop, and gather statis-
tics from running Harpoon processes. Traﬃc module object
ﬁles can be disseminated to remote Harpoon processes and
conﬁgured by distributing XML parameter ﬁles from a cen-
tral location.
Currently, a single Harpoon process can produce a few
hundreds of megabits per second of network traﬃc using rea-
sonably provisioned commodity workstations. Performance
is largely dependent on the nature of the inter-connection
time distribution (because of timer management) and the
number of active sessions. The memory footprint of Har-
poon (client or server) is relatively small, normally less than
10MB with modestly sized input distributions. The code
is written in C++ and currently compiles and runs under
FreeBSD, Linux, MacOS X, and Solaris. Porting to new
platforms is limited only by support for POSIX threads, ca-
pability for dynamic loading of objects (e.g., dlopen() and
friends), the C++ standard template library, and the eXpat
XML library[5].
5. VALIDATION
In this section we validate the ability of Harpoon to gen-
erate traﬃc that qualitatively exhibits the same statistical
signatures as the distributional models used as input.
We used two diﬀerent data sets to self-parameterize Har-
poon in our validation tests. Our ﬁrst data set consisted
of one week of Netﬂow records collected between July 31,
2002 and August 6, 2002. The second data set was a se-
ries of packet traces from the University of Auckland, taken
on June 11 and 12, 2001, from which we constructed ﬂow
records. We modiﬁed the crl flow tool in the CoralReef
software suite [3] to produce wire-format Netﬂow version 5
records from the packet traces. We refer to these data sets
as “Wisconsin” and “Auckland” below, and they are sum-
marized in Table 4.