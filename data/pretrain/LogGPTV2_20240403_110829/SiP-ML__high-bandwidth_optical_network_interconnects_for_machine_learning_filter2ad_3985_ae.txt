grained allocation of circuits (with less bandwidth per circuit). The
latter enables SiP-ML to align circuit bandwidth to traffic demands
more closely, resulting in less wasted bandwidth. Fig. 8 shows the
time-to-accuracy vs. number of OCSs for a one-shot circuit con-
figuration of the Transformer model. Performance improves with
more OCSs, but benefits are marginal beyond 12 OCSs. Also, un-
surprisingly, a larger bandwidth per GPU (B) reduces sensitivity
to the number of OCSs; it has more headroom, thus masking the
inefficiencies caused by fewer OCSs.
Fig. 9 shows how future OCSs with faster reconfiguration time
could improve the total training time of a Transformer model. For
a reconfiguration delay of d, we use the traffic matrix of the past 5d
seconds to reconfigure the circuit allocations. We maintain circuits
for 5d to amortize the reconfiguration delay overhead. As expected,
reducing the reconfiguration delay always helps. However, note
that for d > 300µs, a one-shot allocation outperforms a dynamic
reconfiguration. Once again, higher bandwidth per GPU masks
inefficiencies, and one-shot allocation performs as well as rapid
dynamic reconfiguration.
Impact of scale. Fig. 10 compares the training time of Resnet50
and Transformer on different network architectures across different
scales, with B = 8 Tbps of bandwidth per GPU. As in Fig. 5, we
see that SiP-OCS and SiP-Ring are close to the performance of
665
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
#GPUs
32
128
512
Fabric Latency
1 µsec
1×
2.11×
4.27×
3 µsec
0.99×
2.10×
4.04×
10 µsec
0.83×
1.52×
3.03×
30 µsec
0.73×
1.36×
2.49×
100 µsec
0.64×
1.29×
2.03×
Table 1: Impact of interconnect latency on the scaling effi-
ciency. Training speed-ups are normalized by the speed-up
at 32 GPUs with 1µsec latency.
the ideal Elect-Flat at all scales, with SiP-Ring occasionally slightly
worse. With Elect-Cluster, the training time improves up to a certain
scale, and then the benefits taper off as the low server-to-server
bandwidth becomes a bottleneck. Once again, ResNet scales quite
well with Elect-Cluster, in line with current practice [31]. But larger
models and those less amenable to large-batch training, such as
Transformer, can benefit significantly from SiP-ML’s high per-GPU
bandwidth at moderate-to-large system scales.
Impact of network latency. Network fabric latency can play an
important role in scaling ML workloads at multi Tbps network
speeds. Table 1 shows the impact of different minimum interconnect
latencies on training performance. The results show the training
speedup relative to an Elect-Flat network with 32 GPUs with B =
10 Tbps, and 1 µs fabric latency. Latencies above ∼10µs degrade
performance. This suggests another potential advantage of optical
networks over electrical switching fabrics, the latter can suffer
from variable latency due to the presence of buffers. To compare to
the best-case performance of the baselines, our simulations do not
model buffering within electrical fabrics, as this depends on factors
such as the details of the transport protocols [93, 94].
SiP-Ring reconfiguration delay. While Tbps SiP-enabled chiplets
are just about to hit the market [8, 63, 95], their reconfiguration
latency has not been evaluated. To evaluate the reconfiguration
latency of SiP-ML’s ring topology, we build a small-scale testbed
(details in §4.4). Our testbed includes a thermo-optic SiP chip which
has six micro-ring resonators (MRRs). To hit 10 Tbps bandwidth
we must package 400 MRRs (each modulating light at 25 Gbps). As
a result, our testbed only supports 10 Gbps bandwidth. Rather than
bandwidth, we focus on validating reconfigurability. Our measure-
ments show a reconfiguration delay of 25 µs (Fig. 12b and Fig. 12c
in §4.4).
4.4 Testbed
To benchmark the switching time and throughput of a SiP-based
architecture, we build a small-scale testbed.
Testbed setup. Fig. 11a shows a photograph of our experimental
testbed. We built a three-node prototype of SiP-ML using FPGA
development boards (to emulate GPUs), and a thermo-optic SiP chip
which has six micro-ring resonators (MRRs). Each MRR is tuned to
select one wavelength by receiving the appropriate bias signal from
the bias control board. We use Stratix V FPGAs to emulate the GPU
training workflow, as no commercial GPU chip supports optical
interfaces. Our FPGAs have 50 Mb embedded memory and 1152 MB
DDR3 memory. The FPGAs are programmed and configured as indi-
vidual compute nodes with their own local memory. The controller
logic is implemented using one of the FPGAs. A digital-to-analog
converter (DAC) provides the necessary bias signals to the SiP chip
(a) Photo
(b) Logical schematic
Figure 11: SiP-ML’s testbed.
to cause a state change in the MRRs, depending on the scheduling
decision. We use commodity SFP+ transceivers connected to the
high-speed serial transceiver port on the FPGA board to achieve
the conversion between electrical and optical domains. Our three
input wavelengths are λ1 =1546.92 nm, λ2 =1554.94 nm, and λ3 =
1556.55 nm. Our SiP optical chip consists of six MRRs (we use three
of them as shown in Fig. 11b) to select and forward any of the wave-
lengths to the target emulated GPUs. To evaluate our prototype, we
implement 2D convolutional computation workloads in Verilog to
perform data fetching, computing, and storing between emulated
GPU nodes. A GPU node can get access to the other GPU node’s
memory and perform read/write operations, similar to how real
GPUs communicate today.
Example: programming the MRRs. We set the first configura-
tion such that GPU1 is connected to GPU2; this means MRR1 is
tuned to select and forward λ2 to GPU1, while MRR2 is tuned to
select and forward λ1 to GPU2. For simplicity of the configura-
tion logic, MRR3 is always tuned to λ1 but is effectively in idle
mode, as the optical power of λ1 has been dropped through MRR2.
To change the state to Configuration2 where GPU1 is connected
to GPU3, MRR1 should be tuned to select and forward λ3, while
MRR2 should be detuned from λ1 for the optical power of λ1 to pass
through MRR3 to GPU3. Note that in this configuration, MRR3, can
remain tuned to λ1.
Testbed limitations. Our use of commodity FPGAs and transceivers
is driven by pragmatic concerns. It allows us to implement work-
loads without needing separate modulation logic at the transmitter
or demodulation logic at the receiver. Packets are forwarded to the
SFP+ transceiver which modulates the light for us. However, this
method has limitations as well. Implementing convolutional neural
networks in an FPGA, rather than a GPU as would be the case in
the actual system, introduces complex Verilog logic with overhead
on (de)serializing the remote memory access commands.
To validate the feasibility of our optical design, we answer the
following four key questions. (i) What is the impact of using MRRs
to select/bypass wavelengths on throughput? (ii) How fast can we
reconfigure the MRRs to dynamically tune to appropriate wave-
lengths? (iii) What is the end-to-end switching time? (iv) What is
the impact of our scheduling algorithm on throughput?
MRRs as select/bypass interfaces. We first examine the selec-
t/bypass functions of our MRR-based interfaces. A transceiver chan-
nel is instantiated on the FPGA and a SFP+ optical transceiver at
666
Rx BufferCompute CoreCacheCreate requestClassifierTx BufferResp.MEMRemote dataSync.Local dataStratix V FPGA board Req.Zoomed in10mm10mmFiber I/OMRRsBias control boardSiP ChipMUXλ1λ2λ3λ1,λ2,λ3λ2/λ3λ1/λ3λ1/λ2MRR1MRR2MRR3GPU1GPU2GPU3Fiber ringTraffic Matrix Prediction Wavelength AllocationMicro Ring Bias VoltagesControllerSiP switchSiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
(a) Micro-ring select/bypass through-
put
(b) Micro-ring reconfiguration time
(c) End-to-end reconfiguration time
(d) End-to-end throughput
Figure 12: Testbed benchmarks.
1546.92 nm is used to perform the throughput measurements for
select, bypass and loopback cases. As shown in Fig. 12a, the through-
put measurement of the select mode (the MRR tuned at 1546.92 nm)
is the curve in black while the result for bypassing the MRR is in
blue. The red curve is the baseline measurement where the optical
transmitter is connected directly to the receiver channel without
coupling the optical signal in/out the SiP chip. Our measurements
show in all three cases, the throughput is 9 to 9.3 Gbps confirming
the feasibility of the idea of using MRRs as select/bypass interfaces.
MRR reconfiguration time. To measure the reconfiguration time
of our MRRs, we place InGaAs PIN photodetectors after MRR1 and
MRR2 in Fig. 11b and change the bias voltage from Config1 to
Config2, where MRR1 and MRR2 are tuned into and out of reso-
nance with λ1. We switch light between the two photodetectors by
applying different bias signals to the SiP chip every 125 µs. The pho-
todetectors convert the received photocurrent into voltage. We use
an oscilloscope to measure real time light intensity and can there-
fore measure the reconfiguration speed. Fig. 12b shows the receive
signal at the photodetectors. In one case, the signal reaches stable
state in approximately 20 µs, and in another case, it takes only 8.4 µs.
This is because tuning the MRR into the chosen wavelength is faster
than tuning out of that wavelength due to our use of the thermal
tuning effect. We conservatively, consider 25 µs as the switching
time in our simulations. This experiment micro-benchmarks the
micro-ring reconfiguration time; additional time might be required
for transceivers to start decoding bits. This additional time is not
fundamental, and next we show how we measured the end-to-end
reconfiguration time between FPGAs.
End-to-end reconfiguration time. The end-to-end reconfigura-
tion time includes the MRRs’ reconfiguration time, the transceivers’
locking time, and the handshaking time between newly connected
nodes. The distribution of end-to-end switching time between
Config1 and Config2 is shown in Fig. 12c. We perform 300 mea-
surements to obtain the distribution, showing that the average
switching time to Config1 is 13 µs and Config2 is 15 µs. Indeed, it
is reasonable that the fastest end-to-end reconfiguration time may
be less than the micro-ring reconfiguration time, as the receiver
at the FPGA receives enough optical power to start the synchro-
nization process before stabilization of the light output power. As
described above, the micro-ring reconfiguration times for tuning
and detuning are not equal, leading to two distinct distributions.
The additional variations in the distribution of the reconfiguration
time are a consequence of the time required for the transceiver to
lock onto the new signal and carry out the handshaking protocol.
Putting it all together. We also measure the achieved throughput
while changing the scheduling slot length between the two config-
urations. We conduct five different case studies with slot lengths of
64, 128, 256, 512 and 1000 µs and measure the ideal throughput. The
curve in blue in Fig. 12d indicates the switching state from GPU3
to GPU2 lasting the duration set by the experiment; the curve in
red indicates the switching from GPU2 to GPU3. As the plot shows,
the link can achieve above 90% of the ideal throughput, when the
scheduling slot length is 220 µs. This is because the end-to-end
reconfiguration takes only about 20 µs; hence, having a scheduling
slot 10 times larger will result in near optimal throughput.
5 DISCUSSION
Power budget and scalability. Optical power loss is a key mea-
sure for any optical system. To estimate the D of our SiP-Ring
topology, we measure the loss of light in our testbed. Our experi-
ments indicate that the loss per MRR is negligible (0.125–0.025 dB
per MRR). However, coupling the light in and out of each node
creates 0.5 dB loss because each I/O interface has an input and
output coupler with loss. Overall, the total loss incurred by passing
through each node on SiP-Ring is 0.625–0.525 dB. Hence, assuming
a 10 dB power budget based on transmit power and receiver sen-
sitivity [96], SiP-Ring can send light to 16 back-to-back neighbors
without requiring amplification. At first blush, it appears infeasible
to scale SiP-Ring, as building a cluster with more than 16 nodes
needs amplifiers which add non-linear noise to the system. How-
ever, SiP-Ring can capture path length limitations in its placement
algorithm. For instance, the path length in our evaluations is lim-
ited to 16 nodes (Appendix A.1). This is because the placement
algorithm is able to place GPUs locally close to each other such
that every GPU only interacts with, at most, a GPU that is 15 nodes
away (i.e., the node degree is 16). As a result, SiP-Ring’s design can
take path length into account to scale to large numbers of nodes.
Cost of SiP-ML. The entire field of silicon photonics is based on
the concept that the fundamental way to reduce the cost of photonic
devices is to leverage the high volume manufacturing capabilities
of the silicon electronics industry. As a result, it is impossible to
provide an accurate cost estimation for SiP-ML. Prior work has
built TeraPHY SiP interfaces with size 8.86 mm × 5.5 mm [20, Slide
41]. This area contains optical transmit, receive, and MRRs. The
cost of manufacturing this SiP interface is $44,082 for a volume of
20 chips ($4,408/chip) based on 2020 Europractice pricelist [97].5
5Europractice is an EC initiative that provides the industry and academia with a
platform to develop smart integrated systems, ranging from advanced prototype
design to volume production. The cost is listed as AC80,000 on page 10 under imec
Si-Photonics iSiPP50G; the volume is listed as 20 samples on page 6 under iSiPP50G.
667
99.19.29.300.51Throughput(Gbps)CDFLoopbackBypassSelect20μs8.4μs05010015020001234Time(µs)RxSignal(Volts)MRR1MRR21015202500.10.20.3ReconﬁrgurationTime(µs)FrequencyConfig.1Config.202004006008001,0000.60.81Slotlength(µs)NormalizedThroughputGPU3→GPU2GPU2→GPU3SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
Hence, assuming the cost will drop by a factor of 10 at mass produc-
tion, our current cost estimation for each SiP interface in SiP-ML is
≈$440. We further estimate the cost of on-chip electrical circuitry
(drivers, MRR’s tuning control logic, and CMOS transimpedance
amplification) to be ≈$300. This estimate is based on Europractice
pricelist for a 10 mm2 chip area [14, 19, 98, 99].6 Another approach
to observe the potential cost effectiveness of SiP solutions is to
look at it from the standpoint of pluggable transceivers and active
copper cables. Today’s SiP-based pluggable optics at 100 Gbps cost
roughly $1/Gbps (SiP PSM4 and CWDM4). In comparison, a non
SiP-based SR-4 pluggable transceiver is around $3/Gbps (multimode
and VCSEL based). Similarly, a 400 Gbps SR8 is $3/Gbps, while a
SiP based 400 Gbps DR4 and FR4 is projected to be $1/Gbps. We
note that there is a large distinction between the cost of commodity
DWDM transponders used in wide-area networks and SiP-ML’s
SiP interfaces. In particular, DWDM transponders are designed
to operate at long distances; this imposes strict challenges on the
laser, manufacturing, forward-error correction, photodiode sen-
sitivity, modulation scheme, and light coupling. In contrast, SiP
interfaces are designed for short distances and do not require coher-
ent detection; hence, they can take advantage of the development
and commercialization of photonics components for short distance
datacenters.
6 RELATED WORK
Our work builds on two lines of related work.
Software/hardware systems for distributed ML. Many soft-
ware platforms and techniques have focused on enabling large-
scale distributed machine learning in recent years [100–105]. In
particular several papers focus on enabling large-scale data par-
allel training [45, 100–104, 106]. Relevant to this paper, several
aim to reduce communication overhead using techniques such
as compression [107–110], asynchronous updates [28, 111–114],
partially-exchanged gradients [115], and smart parameter propa-
gation [2, 45, 116–119]. In addition, a variety of algorithmic ap-
proaches have been developed to accelerate communication among
devices customized for the underlying network [120], or to improve