tation by using it to isolate faults in an extension to a
database server. While fault isolation decreases the per-
formance of the extension itself, the total effect is small,
signiﬁcantly less than the overhead of running the exten-
sion run in a separate process, because communication
between the extension and the main server becomes in-
expensive.
3 CISC architectures
The approach of Wahbe et al. is not immediately appli-
cable to CISC architectures like the Intel IA-32 (i386 or
“x86”), which feature variable-length instructions. (The
IA-32’s smaller number of registers also makes dedicat-
ing several registers undesirable, though its 32-bit im-
mediates mean that only 2 would be needed.) Implicit
in the previous discussion of Wahbe et al.’s technique
was that jumps were restricted to a single stream of in-
structions (each 4-byte aligned, in a typical RISC archi-
tecture). By contrast, the x86 has variable-length in-
structions that might start at any byte. Typically code
has a single stream of intended instructions, each fol-
lowing directly after the last, but by starting at a byte
in the middle of an intended instruction, the processor
can read an alternate stream of instructions, generally
nonsensical. If code were allowed to jump to any byte
USENIX Association
Security ’06: 15th USENIX Security Symposium
211
Figure 2: Illustration of the instruction alignment enforced by our technique. Black ﬁlled rectangles represent instructions of various
lengths present in the original program. Gray outline rectangles represent added no-op instructions. Instructions are not packed as
tightly as possible into chunks because jump targets must be aligned, and because the rewriter cannot always predict the length of
an instruction. Call instructions (gray ﬁlled box) go at the end of chunks, so that the addresses following them are aligned.
offset, the SFI implementation would need to check the
safety of all of these alternate instruction streams; but
this would be infeasible. The identity of the hidden in-
structions is a seemingly random function of the precise
encodings of the intended ones (including for instance
the eventual absolute addresses of forward jump targets),
and most modiﬁcations to hidden instructions would gar-
ble the real ones.
To avoid this problem, our PittSFIeld tool artiﬁcially
enforces its own alignment constraints on the x86 archi-
tecture. Conceptually, we divide memory into segments
we call chunks whose size and location is a power of two,
say 16, bytes. PittSFIeld inserts no-op instructions as
padding so that no instruction crosses a chunk boundary;
every 16-byte aligned address holds a valid instruction.
Instructions that are targets of jumps are put at the be-
ginning of chunks; call instructions go at the ends of
chunks, because the instructions after them are the tar-
gets of returns. This alignment is illustrated schemat-
ically in Figure 2. Furthermore, jump instructions are
checked so that their target addresses always have their
low 4 bits zero. This transformation means that each
chunk is an atomic unit of execution with respect to in-
coming jumps:
it is impossible to execute the second
instruction of a chunk without executing the ﬁrst. To
ensure that an otherwise unsafe operation and the check
of its operand cannot be separated, PittSFIeld addition-
ally enforces that such pairs of instructions do not cross
chunk boundaries, making them atomic. Thus, our tech-
nique does not need dedicated registers as in classic SFI.
A scratch register is still required to hold the effective
address of an operation while it is being checked, but it
isn’t required that the same register be used consistently,
or that other uses of the register be prohibited. (For rea-
sons of implementation simplicity, though, our current
system consistently uses %ebx.)
4 Optimizations
The basic technique described in Section 3 ensures the
memory and control-ﬂow safety properties we desire, but
as described it imposes a large performance penalty. This
section describes ﬁve optimizations that reduce the over-
head of the rewriting process, at the expense of making
it somewhat more complex. The ﬁrst three optimizations
were described by Wahbe et al., and are well known; the
last two have, as far as we know, not previously been
applied to SFI implementations.
Special registers. The register %ebp (the ‘frame
pointer’ or ‘base pointer’) is often used to access local
variables stored on the stack, part of the data region.
Since %ebp is generally set only at the start of a function
but then used repeatedly thereafter, checking its value at
each use is inefﬁcient. A better strategy is to make sure
that %ebp’s value is a safe data pointer everywhere by
checking its value after each modiﬁcation. This policy
treats %ebp like the reserved registers of Wahbe et al.,
but since %ebp is already reserved by the ABI for this
purpose, the number of available general-purpose regis-
ters is not decreased.
Guard regions. The technique described in the previ-
ous paragraph for optimizing the use of %ebp would be
effective if %ebp were only dereferenced directly, but in
fact %ebp is often used with a small constant offset to
access the variables in a function’s stack frame. Usually,
if %ebp is in the data region, then so is %ebp + 10,
but this would not be the case if %ebp were already near
the end of the data region. To handle this case efﬁciently,
we follow Wahbe et al. in using guard regions, areas in
the address space directly before and after the data re-
gion that are also safe for the sandboxed code to attempt
to write to.
If we further assume that accesses to the guard region
can be efﬁciently trapped (such as by leaving them un-
mapped in the page table), we can optimize the use of the
stack pointer %esp in a similar way. The stack pointer is
similar to %ebp in that it generally points to the stack and
is accessed at small offsets, but unlike the frame pointer,
it is frequently modiﬁed as items are pushed onto and
popped off the stack. Even if each individual change is
small, each must be checked to make sure that it isn’t the
change that pushes %esp past the end of the allowable
region. However, if attempts to access the guard regions
are trapped, every use of %esp can also serve as a check
of the new value. One important point is that we must be
careful of modiﬁcations of %esp that do not also use it.
212
Security ’06: 15th USENIX Security Symposium
USENIX Association
The danger of a sequence of small modiﬁcations is illus-
trated in the example of Figure 1: each call to alloca
decrements %esp by a small amount but does not use
it to read or write. Our system prevents this attack by
requiring a modiﬁed %esp to be checked before a jump.
Ensure, don’t check. A ﬁnal optimization that was
included in the work of Wahbe et al. has to do with the
basic philosophy of the safety policy that the rewriting
enforces. Clearly, the untrusted code should not be able
to perform any action that is unsafe; but what should hap-
pen when the untrusted code attempts an unsafe action?
The most natural choice would be to terminate the un-
trusted code with an error report. Another possibility,
however, would be to simply require that when an unsafe
action is attempted, some action consistent with the se-
curity policy occurs instead. For example, instead of a
jump to a forbidden area causing an exception, it might
instead cause a jump to some arbitrary other location in
the code region. The latter policy can be more efﬁcient
because no branch is required: the code simply sets the
bits of the address appropriately and uses it. If the ad-
dress was originally illegal, it will ‘wrap around’ to some
legal, though likely not meaningful, location.
There are certainly applications (such as debugging)
where such arbitrary behavior would be unhelpful. How-
ever, it is reasonable to optimize a security mechanism
for the convenience of legitimate code, rather than of il-
legal code. Attempted jumps to an illegal address should
not be expected to occur frequently in practice: it is the
responsibility of the code producer (and her compiler),
not the code user, to avoid them. Our rewriting tool sup-
ports both modes of operation, but we here follow Wahbe
et al.in concentrating on the more efﬁcient ensure-only
mode, which we consider more realistic. Experiments
described in a previous report [18] show that the check-
ing mode introduces an average of 12% further overhead
over the ensure-only mode on some realistic examples.
One-instruction address operations. For an arbitrar-
ily chosen code or data region, the sandboxing instruc-
tion must check (or, according to the optimization above,
ensure) that certain bits of an address are set, and others
are clear. This requires two instructions: an AND in-
struction to turn some bits off and an OR instruction set
others. By further restricting the locations of the sand-
box regions, however, the number of instructions can be
reduced to one. We choose the code and data regions so
that their tags have only a single bit set, and then reserve
from use the region of the same size starting at address 0,
which we call the zero-tag region (because it corresponds
to a tag of 0). With this change, bits in the address only
need to be cleared, and not also set.
PittSFIeld by default uses code and data regions of
16MB each, starting at the addresses 0x10000000 and
0x20000000 respectively. The code sequence to en-
sure that an address in %ebx is legal for the data region
is:2
and
$0x20ffffff, %ebx
This instruction turns off all of the bits in the tag ex-
cept possibly the third from the top, so the address will
be either in the data region or the zero-tag region. On
examples such as the set of larger programs appearing
in a previous report [18], disabling this optimization in-
creases PittSFIeld’s overhead over normal execution by
about 10%.
Efﬁcient returns. A ﬁnal optimization helps PittS-
FIeld take advantage of the predictive features of mod-
ern processors. Indirect jumps are potentially expensive
for processors if their targets cannot be accurately pre-
dicted. For general indirect jumps, processors typically
keep a cache, called a ‘branch target buffer’, of the most
recent target for a jump instruction. A particularly com-
mon kind of indirect jump is a procedure return, which
on the x86 reads a return address from the stack. A naive
implementation would treat a return as a pop followed
by a standard indirect jump; for instance, an early ver-
sion of PittSFIeld translated a ret instruction into (in
this example and the next, the ﬁnal two instructions must
be in a single chunk):
popl
and
jmp
%ebx
$0x10fffff0, %ebx
*%ebx
However, if a procedure is called from multiple loca-
tions, the single buffer slot will not be effective at pre-
dicting the return address, and performance will suffer.
In order to deal more efﬁciently with returns, modern
x86 processors keep a shadow stack of return addresses
in a separate cache, and use this to predict the destina-
tions of returns. To allow the processor to use this cache,
we would like PittSFIeld to return from procedures using
a real ret instruction. Thus PittSFIeld modiﬁes the re-
turn address and writes it back to the stack before using
a regular ret. In fact, this can be done without a scratch
register:
and
ret
$0x10fffff0, (%esp)
On a worst case example, like a recursive implementa-
tion of the Fibonacci function, this optimization makes
an enormous difference, reducing 95% overhead to 40%.
In more realistic examples, the difference is smaller but
still signiﬁcant; for the SPECint2000 benchmarks dis-
cussed in Section 7, disabling this optimization increases
2Assembly language examples use the GAS, or ‘AT&T’, syntax
standard on Unix-like x86-based systems, which puts the destination
last.
USENIX Association
Security ’06: 15th USENIX Security Symposium
213
the average overhead from 21% to 27%, and almost dou-
bles the overhead for one program, 255.vortex.
With the exception of this optimization, the rest of
the PittSFIeld system can maintain its security policy
even if arbitrary changes to the data region occur be-
tween instructions, because instructions always move ad-
dresses to registers before checking them. However, the
ret instruction unavoidably uses the stack, so this op-
timization is applicable under the more limited attack
model in which untrusted data changes come from a sin-
gle untrusted thread. The optimization should not be
used if multiple threads run in the same data sandbox,
or if other untrusted memory changes (such as memory-
mapped I/O) might occur in parallel.
5 Veriﬁcation
The intended use of PittSFIeld is that the compilation
and the rewriting of the code are performed by the un-
trusted code producer, and the safety policy is enforced
by a separate veriﬁcation tool. This architecture is fa-
miliar to users of Java: the code producer writes source
code and compiles it to byte code using the compiler of
her choice, but before the code user executes an applet
he checks it using a separate byte code veriﬁer. (One dif-
ference from Java is that once checked, our code is exe-
cuted more or less directly; there is no trusted interpreter
as complex as a Java just-in-time compiler.) The impor-
tance of having a small, trusted veriﬁer is also stressed
in work on proof-carrying code [20]. Though the advan-
tages of this architecture are well known, they have been
neglected by some previous SFI implementations, lead-
ing to predictable problems with usability and security
(see Section 10.1).
Responsibility for ensuring the safety of execution in
the PittSFIeld system lies with a veriﬁer which examines
the rewritten code just prior to execution, conservatively
checking properties which, if true, ensure that execution
of the code will not violate the system’s security policy.
In a more complex system, one could imagine the rewrit-
ing process supplying hints describing why the rewrit-
ten code satisﬁes the security policy (like the proof in a
proof-carrying code system), but PittSFIeld’s policies are
simple enough that this is not necessary. In particular,
the veriﬁer does not require debugging or symbol-table
information; the veriﬁer must disassemble the rewritten
code, but the rewriter ensures that the disassembly can
be performed in a single pass without respect to function
boundaries. The role of the veriﬁer is to prove that the
rewritten code is safe, so its design is best thought of as
automating such a proof. Section 9 will describe in more
detail how that intuitive proof can be formalized.
To understand how the veriﬁcation works, it is helpful
to borrow concepts from program analysis, and think of
it as a conservative static analysis. The veriﬁer checks a
property of the program’s execution, roughly that it never