1) Syncing procedure: Insert every node w on path P that
2) Update
3)
entry
response.map[(typei, bidi, vi)]
is currently not in subtree into subtree.
from
(b, x) to (true, x). If x (cid:4)= ⊥, reply value x for the
request (typei, bidi, vi) to Sequencer, and delete the
entry.
If fake.read = 0, ﬁnd block bidi in subtree, and create
responses to requests in queue request.map[bidi] as
follows:
• Pop a request (type, bidi, v) from the queue.
• Let w be the current value of block bidi.
• If type = write, set the value of bidi to v.
• If entry response.map[(type, bidi, v)] = (true,⊥),
reply value w for the request (type, bidi, v) to
Sequencer, and delete the entry.
• Else, if response.map[(type, bidi, v)] = (false,⊥),
set the entry to (false, w).
4)
Repeat the above steps until request.map[bidi] is empty.
If fake.read = 0, assign block bidi a new random path
pos.map[bidi] $← {0, 1}D.
FLUSH(pid):
on path pid in subtree and stash,
1) For every block bid(cid:2)
do:
• Push block bid(cid:2)
to the lowest node in the intersec-
tion of path pid and pos.map[bid(cid:2)] that has less
than Z blocks in it. If no such node exists, keep
block bid(cid:2)
in stash.
Increment #paths and push pid into queue write.queue.
2)
3) For every node that has been updated, add (local)
timestamp t = #paths.
WRITE-BACK(c):
1) Pop out k paths pid1,··· pidk from write.queue.
2) Copy these k paths in subtree to a temporary space S.
3) Encrypt paths in S using secret key key.
4) Write-back the encrypted paths in S to the server with
(server) timestamp c. Wait for response.
5) Upon waking up with write conﬁrmation, delete nodes
in subtree that are on paths pid1,··· pidk, with (local)
timestamp smaller than or equal to c · k, and are not on
any path in PathReqMultiSet.
Fig. 5: Pseudocode description of TaORAM.
To ensure the former, we timestamp every node in subtree
(locally) to record when it is last updated. (See Step 3 of
Algorithm FLUSH in Figure 5, and note that this timestamp is
different from the version number used as a server timestamp.)
To ensure the latter,
the Processor maintains a multi-set
PathReqMultiSet that tracks the set of paths requested but not
yet returned.7 (See Step 3 and 4 of algorithm READ-PATH and
Step 6 of WRITE-BACK in Figure 5.)
3) Step 3 – Non-Blocking Flushing: So far, though requests
are concurrently processed at their arrival, the ﬂush and write-
7We remark that PathReqMultiSet must be necessarily a multi-set, as the
same path may be requested more than once.
back operations are still done sequentially, in the same order
their corresponding logical requests arrive (in batches of k).
We further remove this synchronization.
First, we decouple the order in which paths are ﬂushed from
the order in which logical requests arrive: As soon as a path is
retrieved (“synched” with the subtree, and used for answering
client request), the Processor ﬂushes the path immediately,
even if the paths for some previous requests have not yet been
returned (remember that they could well be late due to the
asynchronous nature of the network). Furthermore, we make
write-back operations asynchronous: As soon as k new paths
are inserted into subtree and ﬂushed, the Processor writes-
206206
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
back these k paths to the server, irrespective of the status of
any other operations (e.g., some previous write-back requests
may still be pending)— therefore, in the rest of the paper, we
call k the write-back threshold. In summary, ﬂush and write-
back operations are performed as soon as they are ready to be
performed. (See the pseudocode of Module Processor.)
This brings two challenges. First, since paths may be ﬂushed
in an order different from that they were requested, it is no
longer clear whether the stash size is bounded (at least the
analysis of Path ORAM does not directly apply as a black
box). We show that this is indeed the case, and provide the
proof below.
Lemma 1. The stash size of TaORAM is bounded by
any function R(N ) = ω(log N ) (e.g. R(N ) = (log N ) ·
(log log log N )), except with negligible probability in N.8
The second challenge is ensuring server consistency when
multiple write-back operations end up being concurrent. In an
asynchronous network, these requests may arrive at the server
out-of-order, causing the server to be updated incorrectly. To
address this problem, we mark each node stored at the server,
as well as each write-back request, with a version number (or
“server timestamp”), and the server can only overwrite a node
if the write-back request is of a newer version. (See Step 4 of
WRITE-BACK; we omit the server algorithm due to lack of
space.)
Proof of Lemma 1: We only give a proof sketch. A more
formal proof is rather tedious and requires repeating many of
the technical steps in the stash analysis of Path ORAM with
little change.
We show that given any execution trace T of TaORAM
with a sequence of logical requests r1, r2,··· , one could
2,··· of the same length
come up with another sequence r(cid:2)
(modiﬁed and permuted from the original sequence based
on the execution trace) which when fed to Path ORAM
sequentially yields the same stash.
1, r(cid:2)
By design of TaORAM, whenever the Processor receives
a request ri = (typei, bidi, vi) with typei = read/write, it
immediately issues a path-read request to the server, fetching
either the path (cid:4)i = pos.map(bidi) assigned to block bidi (in
$← U (in
the case of real read), or a randomly chosen path (cid:4)i
the case of fake read). Furthermore, upon receiving the path
(cid:4)j corresponding to request rj from the server, the Processor
ﬂushes the path immediately. The execution trace T contains
the time tj at which each path (cid:4)j corresponding to request rj
is ﬂushed. Order the time points chronologically tj1
<
··· . We observe that the contents of the stash are determined
by the sequence of events of ﬂushing over paths (cid:4)j1 , (cid:4)j2 ,··· ,
where if the jk’th request corresponds to a real read, then the
block bidjk is assigned to a new path, and if the jk’th request
corresponds to a fake read, no new assignment occurs.
quests r(cid:2)
Suppose we execute Path ORAM with a sequence of re-
k = rjk if the jk’th
2,··· sequentially, where r(cid:2)
1, r(cid:2)
< tj2
8In fact, the statement can be made more concrete, as the probability of
overﬂowing is roughly c−R for some constant c and stash size R.
207207
request corresponds to a real read, and otherwise r(cid:2)
k is a
“special request” for ﬂushing path (cid:4)jk without assigning new
paths to any blocks, (and suppose that the same random coins
are used for assigning new paths as in execution trace T ).
At any point, the contents of the stash is identical to that of
TaORAM with execution trace T .
It was shown in [38] that the stash size of Path ORAM
when executed without “special requests” is bounded by any
function R(N ) = ω(log N ) with overwhelming probability.
Since the “special requests” only involve ﬂushing a path
without assignment new paths (in other words, they only put
blocks at lower positions on the path), the probability that the
stash size exceeds R(N ) decreases. Therefore, the stash size
of TaORAM is also bounded by R(N ) with overwhelming
probability.
4) Step 4 – Response Timing and Sequencer: The above
description considers only the obliviousness of the communi-
cation between the server and the Processor. Indeed, by the
use of “fake reads”, every read-path request to the server
fetches an independent random path. Their timing, as well
as that of the write-back requests, are completely determined
by the timing of (the arrival of) logical requests and the
schedule of asynchronous network. Hence,
the Processor-
server communication is oblivious of the logical requests.
Another aspect that has been neglected (on purpose) so
far is the timing of replies (to logical requests). Consider
the scenario where a sequence of repetitive logical requests
arrives in a burst, triggering a real read (for the assigned
path), followed by many fake reads (for random paths).
When the real read returns, the requested block is found;
but, if the Processor replies to all logical requests in one
shot and an adversary observes this event, it can infer that
there are likely repetitions. To eliminate this leakage,
the
Processor only replies to a request when the corresponding
read-path request has returned, even if it is a fake read. To
achieve this, the Processor uses a response map, denoted
as response.map, that maps each request (type, bid, v) to a
tuple response.map[(type, bid, v)] = (b, w) indicating whether
this request is ready to be replied to (i.e., b = true if the
corresponding read-path request has returned) and what the
answer w is. A request is replied to only when both b = true
and w (cid:8)= ⊥. (See Step 2 and 3 of ANSWER-REQUEST.)
Unfortunately, a more subtle leakage of information still
exists in an asynchronous network, and is exploited by our
attack against CURIOUS in Section II-B. To see this, consider
again the above scenario with one real-read followed by many
fake reads. If in addition the real-read is indeﬁnitely delayed
due to the asynchrony of the network, the requested block
is not retrieved and none of the requests can be answered
(even if all fake-reads return without delay). This delay of
replies again leaks information; we have explained how an
adversary can use this information to violate obliviousness in
Section II-B. In order to prevent this attack, TaORAM runs
an additional auxiliary module, the Sequencer, whose sole
function is enforcing that logical requests are replied to in the
same order as they arrive.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
C. Client Memory Consumption
The client memory of an ORAM scheme contains both
temporary data related to on-going processing of requests, and
permanent data that keeps the state of the ORAM scheme.
Since the latter needs to be stored even when there is no
request present, it is also called the client storage. In TaORAM,
the client storage consists of the position map, the stash, and
the secret key key, of size respectively O(N log N ), ω(log N ),
and λ (the security parameter); thus,
TaORAM Client Storage Size = O(N log N + λ) ,
which is the same as Path ORAM.
On the other hand, unlike Path ORAM and other sequential
ORAM schemes, the size of temporary data in TaORAM (and
other concurrent cloud storage system such as [36]) depends
on the number I of concurrent “incomplete” (more details
below) logical requests. The number I in turn depends on
various (dynamically changing) parameters, from the rate of
arrival of logical requests, to the schedule of asynchronous
network, to the processing power of the server and client.
Hence, we analyze the size of temporary data w.r.t. I. For
TaORAM, we say that (the processing of) a logical request
is incomplete, if it has not yet been answered, or updates
induced by the request (due to being a write request itself
and/or ﬂushing) has not been committed to the server. For
each incomplete request, TaORAM keeps temporary data of
size O(log N ), leading to
TaORAM Temporary Data Size = O(I log N ) .
In a normal execution where the rate of processing and
the rate of arrival of logical requests are “balanced”, since
TaORAM writes-back to the server after every k paths are
retrieved and ﬂushed, the number I of incomplete requests is
roughly k; hence,
Normal TaORAM Memory Consumption
= O(k log N + N log N + λ) .
Of course, a malicious adversary can drive the number I to
be very large, by simply preventing write-back operations to
complete. When this is a concern, we can let the system
halt whenever I reaches a certain threshold (note that I is
known to the adversary, and thus this operation does not break
obliviousness of the scheme).
D. Partitioning
It may be often advantageous to store our tree in a dis-
tributed fashion across multiple partitions, e.g. to prevent I/O
and bandwidth bottlenecks.
TaORAM is easily amenable to partitioning, without the
need of storing an additional partition table as in previous
systems [36], [37], [4]. If m = 2i partitions are desired,
we can simply “remove” the top i levels of the tree, storing
them in TaORAM’s local memory. (Note that this requires
storing O(m) additional data blocks locally, but this number
is generally not too large.) Then, the rest of the tree can be
thought as a forest of m sub-trees (the root of each sub-tree
is one of the nodes at the i-th level of the original tree). One
can then store each of these sub-trees on a different partition.
Note that the scheme remains unchanged – the only dif-
ference is in the data-fetch logic. The tree is now distributed
across m partitions, and the TaORAM’s local memory. When
a path is to be fetched, one retrieves the contents of the
ﬁrst i levels on the path from the local memory, and the
remaining levels from the appropriate partition. Every access
being on a random path, the load on the partitions is uniformly
distributed.
E. Security
The following theorem summarizes our security statement
for TaORAM. The proof, given in Appendix B, follows from
two facts: First, from our use of the sequencer module,
the i-th operation is not answered until all
ensuring that
previous operations are answered. Second, from the fact that
all requests retrieve random paths.
Theorem 1 (TaORAM security). Assume that the underlying
encryption scheme is IND-CPA secure. Then TaORAM is aaob-
secure.
F. Correctness
It
is a priori not clear whether the system behaves as
expected, or say (for example) we may return inconsistent
or outdated values for different requests. Proving correctness
of the scheme, therefore, becomes a non-trivial issue in the
asynchronous setting (which is in fact even harder than
proving security). In Appendix D, we prove that TaORAM
exhibits atomic semantic, i.e., completed operations appear (to
an external observer) as if they took effect atomically at some
point during their invocation and their response. (We provide
formal deﬁnitions for correctness in Appendix C.)
The core of the proof lies in showing that the fresh-subtree
invariant mentioned above always holds (i.e., the contents
in the local storage at
the proxy is the most up-to-date).
Operations then take effect when a write operation writes its
value into, or when a value is retrieved from the proxy’s local
storage.
Remark. We note that packet dropping or delays have a very
isolated impact on TaORAM. Indeed, loss of some of the
read-path/write-back operations will not result in stalling the
system (just in slightly increased memory consumption). This
is in sharp contrast to the background shufﬂing process of
ObliviStore [36], which cannot be halted at any point as
otherwise the system will stall.
V. EXPERIMENTS
The experiments evaluate TaoStore in two different test
environments: simulation based and real world deployment.
We start by providing a detailed analysis of TaoStore’s per-
formance by deploying the untrusted server to a public cloud
(AWS[1]). We then compare TaoStore with ObliviStore and
Path ORAM in the hybrid cloud setting using a simulation
208208
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
we apply the read-write lock mechanism [12] at the bucket
level to control concurrent accesses to the shared buckets in
the local cache.9 If a thread wants to perform an insert, an
update or a delete operation on a bucket, it has to acquire a
write lock for this bucket, to which it gains exclusive access.
In contrast, for read operations, it is enough for the thread to
acquire a read lock, which still allows several threads to access
the same bucket for reading at the same time. The stash is