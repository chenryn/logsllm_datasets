### Optimized Text

The proposed model, which demonstrates the best performance, achieves 32% precision and 95% recall. This indicates high sensitivity to clear fault-signaling messages but low overall precision, leading to a high false alarm rate.

Zheng et al. [165] propose a method based on Recurrent Neural Networks (RNNs) to estimate the Remaining Useful Life (RUL) of system components. They argue that the temporal nature of sensor data justifies the use of Long Short-Term Memory (LSTM)-based RNNs due to their ability to model long-term dependencies. The authors experiment with three public RUL datasets and compare their method with other machine learning approaches in terms of Root Mean Squared Error (RMSE). Their approach shows the best RUL prediction performance, achieving an RMSE of 2.80, which is a 54.47% improvement over Convolutional Neural Networks (CNNs).

### System Failure Prediction

Instead of focusing on physical component failures, future system availability can be estimated through symptomatic evidence and dependency modeling assumptions at the software level. Past approaches for system failure prediction are primarily based on log analysis, which constitutes the most frequent data source. Key Performance Indicators (KPIs) and hardware metrics, typically used for shorter prediction windows, are also associated with failure detection. System failure prediction can be applied at different abstraction levels depending on the target software component under investigation (e.g., job, task, container, virtual machine, or node).

Some early contributions include failure prediction in the IBM supercomputer BlueGene project. For instance, Lianget al. [81] apply various classification algorithms to predict fatal failures within fixed time windows using real event logs. Input features are parsed from structured logs based on severity, total count, and distribution of log events within an observation window. Features from the previous window are used to predict failures in the next one. In the evaluation phase, four classification algorithms (SVM, RIPPER, BMNN, k-NN) are compared using precision, recall, and F-score. The size of the prediction window significantly impacts the final detection performance. The experiments suggest that an ideal window size of 6 to 12 hours balances utility effectiveness and accuracy. The Bi-modal nearest neighbor approach proposed in the paper provides the best overall results, achieving an F-measure of 70% (and 50%) with 12-hour (and 6-hour) prediction windows, respectively.

Cohen et al. [31] investigate an approach based on Tree-augmented Bayesian Networks (TANs) to associate observed variables with abstract service states, forecast, and detect Service Level Objective (SLO) violations and failures in three-tiered Web services. The system observes system metrics such as CPU time, disk reads, swap space, and KPI-related measures like the number of served requests, all interdependently modeled. The optimal graph structure, including the ideal subset of input metrics, is selected using a greedy strategy. The approach, originally developed for detection, can also be used for diagnosis due to the interpretability properties of TANs (see Section 4.4.2). Two separate experiments validate the approach: a multi-tiered Java server application and an Apache server testbed, both injected with synthetic workloads varying in connections, request rate, and type. The results for forecasting, applied 1 or 5 minutes in advance of failures, show high detection rates (83–93%) and false-alarm rates ranging from 9.1% to 24% depending on the experiment.

Salfner et al. [123] train Hidden Semi-Markov Models (HSMMs) for online prediction of failures in event sequences collected from error logs. One HSMM is trained on failing sequences, and another on non-failing sequences. The sequence likelihood of the models determines which scenario is more likely to occur. The approach is evaluated on logs from a commercial telecommunications system and compared with other prediction techniques, achieving an F-measure of 0.7419 (precision 0.852, recall 0.657) with a false-positive rate of 0.0145.

Chalermarrewonget al. [23] propose a system availability prediction framework for data centers based on autoregressive models and fault-tree analysis. Their Autoregressive Integrated Moving Average (ARIMA) model works on time series of workload and hardware metrics, where thresholds are set to detect component-level symptoms such as high CPU temperature, bad disk sectors, and memory exhaustion. These symptoms, along with the tree structure, determine the availability state deterministically. This approach implicitly provides granular information for diagnosis. Experimental results show high prediction accuracy (97%) but a high false alarm rate (precision is 53%).

The HORA prediction system [119] adopts a holistic approach, integrating architectural knowledge with online time-series data to predict Quality of Service (QoS) violations and service failures in distributed software systems. Component dependency and failure propagation models, in the form of Bayesian Networks, associate component failures predicted from system metrics with autoregressive predictors to system-wide problems. Tested on a microservice-based application, the new approach is compared to a monolithic approach (not using architectural knowledge) by injecting memory leaks, node crashes, and system overloads during execution. The HORA approach achieves higher recall (83.3% vs. 69.2%) and AUC ROC (0.920 vs. 0.837, +9.9%) compared to the monolithic approach. The proposed approach is more viable when a high false alarm rate (≥10%) is acceptable, while the monolithic approach is more suitable for predictions in the 0–10% false alarm range.

Fronza et al. [44] describe a failure prediction approach based on text analysis of system logs and Support Vector Machines (SVMs). First, log files are parsed and encoded using Random Indexing, where index vectors are assigned to each operation in a log line to obtain a latent representation of the text. Context vectors for each operation are then computed by scanning the log corpus, allowing co-occurrence of operations to be considered. Sequences of operations are represented as the weighted sum of context vectors corresponding to the operations encoded. These sequence representations serve as input to a weighted SVM, trained to minimize false negatives. Results show that a weighted approach can compensate for a decrease in specificity (always greater than 80%) to improve recall (50%).

Similarly, in Reference [160], RNNs are applied for failure prediction from logs. The approach includes a clustering algorithm to group similar logs, a pattern matcher to identify common templates, a log feature extractor based on natural language processing, and a sequential neural network architecture based on LSTM cells for predicting the failure status within the predictive period. The use of LSTMs is motivated by their sequential modeling abilities and the rarity of failures. The approach is compared, using real data collected from two enterprise server clusters, with other machine learning methods (SVM, logistic regression, random forest) in terms of precision-recall AUC. At 70% precision, the LSTM method shows the highest failure sensitivity with a recall value of 0.909. LSTMs are able to detect signs of failures earlier than other methods (on average, 73 minutes earlier).

Islam et al. [61] also use LSTM networks in a large characterization study of job failures conducted on a workload trace dataset by Google. Failures are predicted at the job and task levels, where a job is a collection of tasks and each task is a single-machine command or program. Failures are predicted from resource usage measures, performance data, and task information (completion status, user/node/job attributes). For task failure prediction, the final F1-score of the system is 0.87 (precision = 0.89, recall = 0.85, False Positive Rate (FPR) = 0.11). For job failure prediction, the approach achieves an F1-score of 0.81 (precision = 0.80, recall = 0.83, FPR = 0.20). The prediction algorithm can be used to reduce resource waste (by 12–20% in the experiments) by re-submitting jobs and tasks that are likely to fail.

### Failure Detection

In this section, we discuss reactive failure management methods, which operate to limit the consequences of failures after they have occurred. These methods are motivated by the fact that, even with the most advanced prevention and prediction techniques, the occurrence rate of failures can never be fully reduced to zero. Some reactive approaches, such as root-cause analysis and anomaly detection, can also provide better understanding of how failures are causally related and propagate over time, or comprehend the temporal characteristics (variance, burst frequency, periodicity, seasonality) associated with failures.

Failure detection via monitoring operations can be a complex and tedious task for human operators. Chen et al. [26] report that, in the administration of a commercial website, 75% of the recovery time was spent on average for detection. Automatic discovery of performance problems and errors allows operators to dedicate less time identifying service-related problems while providing insights into which failures must be prioritized in the diagnosis step based on the frequency observed in the detection phase. Automated failure detection is based on a variety of monitoring tools, ranging from simple print statements (which constitute the fundamental unit of system logs) to more complex instrumentation techniques or entire frameworks. We divide our discussion of failure detection into anomaly detection, Internet traffic classification, and log enhancement techniques.

### Anomaly Detection

According to our quantitative results [113], failure detection is treated as an anomaly detection problem in the majority of contributions related to IT system management. Anomaly detection is a multidisciplinary task that deals with finding patterns in data that do not conform to expected behavior [24]. Interest in anomaly detection is motivated by the possibility of obtaining actionable information to deal with diseases, frauds, cyber-attacks, system outages, and faults, depending on the target domain. Anomaly detection operates by deriving a model of normal behavior and testing new observations against this model. In the AIOps context, anomaly detection is applied under the assumption that failures generate irregular behavior in IT systems (errors) across a set of measurable entities (or symptoms), such as KPIs, metrics, logs, or execution traces. Anomaly detection is also used to detect cyber-attacks, congestion, and sub-optimal resource utilization, which can all be causes of future failures. Because obtaining labeled examples is time-consuming, anomaly detection systems typically rely on unsupervised learning.

Three prominent techniques used in this context are clustering [11, 127], dimensionality reduction [72, 151], and neural network autoencoders [7, 131, 150, 159]. One of the earliest examples of unsupervised learning for system workload characterization is Magpie [11], a behavioral modeling toolchain for distributed systems. Magpie's instrumentation is based on fine-grained, in-kernel event logging tools available under Windows, capable of measuring resource consumption and execution times accurately. The events registered with this tool are used to reconstruct requests and apply behavioral clustering to obtain a small set of unique request types, each associated with a specific workload model. Behavioral clustering allows for more precise modeling of realistic workloads than grouping requests by URL. A realistic workload model is beneficial for identifying anomalous resource consumption and enhancing capacity planning and performance testing.

At the network level, Lakhina et al. [72] monitor network links via SNMP data to detect and diagnose anomalies in network traffic. They apply Principal Component Analysis (PCA) on link flow measurements collected over time to separate traffic into normal and anomalous subspaces. They classify principal components into normal and abnormal (setting a threshold on the explained variance) and then identify anomalies by reconstructing new observations using abnormal components. If the reconstruction error exceeds a predefined threshold, the new data point is considered anomalous. Their approach is validated using synthetic and real volume anomalies, the latter detected with traditional class forecasting algorithms. In a follow-up work [73], a similar approach based on traffic feature distribution entropy is proposed. The paper shows how the same subspace method can exploit packet information, such as source and target destination addresses or ports, to detect security threats and service outages, as well as provide preliminary diagnostic information.

Several approaches apply unsupervised learning on univariate time series constructed from KPI and metric observations [127, 150]. Sharma et al. [127] propose CloudPD, an end-to-end failure tolerance system performing failure detection, diagnosis, and classification in virtualized cloud environments (see also Section 4.4.2). The paper proposes collecting a variety of measures at the virtual machine (VM) and physical machine levels, including resource utilization, operating system variables, and application performance metrics. The paper then proposes three different unsupervised machine learning approaches for anomaly detection: k-nearest neighbors (k-NN), Hidden Markov Models (HMMs), and k-means clustering. The k-NN detection approach is evaluated on three different benchmarks, where it outperforms four proposed baselines with an average higher recall (83–87%) and a lower false alarm rate (12–17%). Donut [150] performs anomaly detection on seasonal KPI time series using deep Variational Autoencoders (VAEs) and window sampling. The approach is compared on three different datasets of 18 KPIs to Opprentice [86] and a baseline VAE performance in terms of F-score (ranging from 0.75 to 0.9), AUC (0.7–0.9), and average alert delay (4 to 12 minutes). One of the conclusions of the authors is that autoencoder approaches require abnormal samples in addition to normal behavior samples.

Recent time-series approaches focus on multivariate anomaly detection using autoencoders [7, 131, 159]. Two advantages of a multi-dimensional analysis are the ability to model inter-correlations between different metrics/components and to provide interpretable results for root-cause analysis by attributing the detected anomaly to a specific metric or component. MSCRED [159] uses a convolutional-recurrent (ConvLSTM) autoencoder architecture to capture temporal patterns at different scales and thus identify abnormal time steps. In the evaluation study, MSCRED outperforms the baselines (SVM, ARMA, GMM, CNN) with an F1-score of 0.82–0.89, while being able to identify anomalies on scales of w = 10, 30, 60 seconds. By computing the reconstruction errors on individual time sequences, MSCRED can also identify root causes more effectively (top-3 recall = 0.75–0.80). OmniAnomaly [131] proposes a recurrent VAE model using stochastic variable connection and planar normalizing flow. Similar to MSCRED, the reconstruction error can be used to interpret anomalies and connect back to individual time series. OmniAnomaly achieves a detection F1-score of 0.8599 on three real datasets, including server metric data (SMD [108]) collected from a large-scale Internet company. SMD is also used for evaluation in USAD [7], which applies adversarial-based techniques to the autoencoder architecture for faster and more stable training. In USAD, the anomaly score is parametrized so that the sensitivity can be adjusted rapidly and on multiple levels for real industrial applications. On five real datasets, USAD achieves results comparable to OmniAnomaly (F1 = 0.791 for both), while reducing training time by 547 times on average.