Our Analyze API accepts a `token_filters` parameter as the official way of
adding token filters to the chain (we also support `filter` which is
depracated):
    GET _analyze
    {
      "text": "An apple",
      "tokenizer": "whitespace",
      "token_filters": [
        "lowercase", "stop"
      ]
    }
However, creating an analyzer using indexing setting only accept `filter` (no
S) which is confusing. I think the following should be accepted (if not the
only default):
    PUT index
    {
      "settings": {
        "analysis": {
          "analyzer": {
            "my_analyzer": {
              "tokenizer": "whitespace",
              "token_filters": [ <--- broken (expects "filter")
                "lowercase",
                "stop"
              ]
            }
          }
        }
      }
    }
It's very easy to change the code to accept both `filter`, `filters` and
`token_filters` but I tend to think that we should go strict here and say we
only accept `token_filters` and automatically upgrade existing setting when we
recover. This will require (some) more work as that infra is not there yet.