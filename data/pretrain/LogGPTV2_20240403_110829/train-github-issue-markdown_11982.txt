Memory keeps growing at every iteration. My pytorch version 0.4.0a0+4d405a4.
    import torch
    from torch.autograd import grad, Variable
    from torchvision import models
    model = models.resnet50().cuda()
    for k in range(20):
        x   = Variable(torch.rand(8, 3, 224, 224).cuda(), requires_grad=True)
        dx, = grad(model(x).sum(), x, create_graph=True)
        y = model(x + dx).sum()
        y.backward()
This code is from Double backward memory leak. It work well and no memory
leak. But use
    y = dx.mean()
    y.backward()
the memory leak is happen.