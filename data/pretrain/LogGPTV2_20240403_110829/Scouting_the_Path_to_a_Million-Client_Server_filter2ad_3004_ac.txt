6 Related Work
As we present throughout the paper, there has been signiﬁcant work improving
diﬀerent components of the stack including scheduling [18,24,38,39] and back-
pressure [46]. However, they fail to consider the interactions between diﬀerent
components, and none of the existing optimized components was tested with a
load larger than 50k ﬂows. Our work deﬁnes a broader category of limitations
and looks at the complicated interaction between diﬀerent components.
Much of the focus of the previous work has been on scaling servers in terms
of aggregate traﬃc intensity in terms of packets transmitted per second, while
348
Y. Zhao et al.
maintaining low latency [2,13,28,34,36]. Some recent proposals address scaling
the whole stack to handle a large number of ﬂows [26,29,33,37]. mTcp [26] is a
scalable user-space TCP/IP stack built over kernel-bypass packet I/O engines,
but the evaluation was only performed at a maximum of 16k ﬂows. Further, it
focuses on improving connection locality and reducing system overhead without
paying much attention to scheduling and backpressure. Other systems are evalu-
ated at a few thousands ﬂows [29] and up to twenty thousand ﬂows [33,36,37,44].
These systems improve speciﬁc functionality (e.g., RPC performance or trans-
port layer performance) by dedicating network interfaces to individual applica-
tion or by optimizing the kernel TCP/IP stack, with typical emphasis on short
lived ﬂows. In this paper, we are more concerned with scaling to hundreds of
thousands of long-lived ﬂows where transport and scheduling are implemented.
To the best of our knowledge, this is the ﬁrst such study.
Another observation is that hardware oﬄoad solutions [22,40,41] alone can-
not completely solve the problem. Careful hardware design can help reduce the
latency of complex operations [40]. However, data structure issues do not dis-
appear when implemented in hardware. In addition, admission control requires
careful coordination between the software part of the stack, including the appli-
cation, and the hardware part of the stack.
7 Relevance of Findings to Other Stacks
In this paper, we focus on the Linux stack because of its ubiquitous usage in
both industry and academia. However, most of our ﬁndings focus on abstract
functions that are needed in a stack in order to eﬃciently handle a large num-
ber of ﬂows. For example, admission control can avoid overwhelming the stack
resources by relying on per-ﬂow scheduling and accurate batching sizing. The
lack of similar functions in any stack can lead to performance degradation as the
number of ﬂows grows. Further, the need for better data structures for schedul-
ing and demultiplexing can lead to signiﬁcant CPU savings. Contrarily, some of
the problems we deﬁne are Linux speciﬁc, arising from components developed by
companies to handle their speciﬁc workloads. For example, autosizing was devel-
oped by Google, making problems like overpacing a Linux-speciﬁc problem.
Some stacks inherently solve some of the problems we have identiﬁed. For
instance, Snap [31] provides per-ﬂow scheduling providing eﬃcient backpressure.
Further, stacks that rely on lightweight threading and asynchronous messages
like Snap and Shenango might not suﬀer signiﬁcant performance degradation
due to lock contention. However, none of them handles all problems The goal of
our work is to identify abstract functions that stacks will have to implement in
order to scale.
Some of the problems we have identiﬁed are only exposed at a very large
number of ﬂows. To the best of our knowledge, these problems are yet to be
handled by any stack. For instance, delays introduced due to cache misses will
require innovation in speculative pre-fetching based on network behavior. Fur-
ther, network accelerators and programmable hardware components will require
Scouting the Path to a Million-Client Server
349
new techniques to coordinate their behavior with changes in the load generated
by the software component of the stack.
8 Conclusion
In this paper, we identify the diﬀerent bottlenecks that arise when we scale the
number of ﬂows to hundreds of thousands in a fully implemented stack. As we
present throughout the paper, there have been eﬀorts to address some of the
individual problems in isolation. However, integrating and testing such solutions
at the scale of hundreds of thousands to millions of long-lived simultaneously-
active ﬂows remains an open problem. We hope that this paper sheds some light
on the pain points that stack designers should pay attention to when building
next generation stacks that scale to terabits per second and millions of ﬂows.
A Linux Stack Overview
Packet transmission in an end-host refers to the process of a packet traversing
from user space, to kernel space, and ﬁnally to NIC in packet transmission pro-
cess. The application generates a packet and copies it into the kernel space TCP
buﬀer. Packets from the TCP buﬀer are then queued into Qdisc. Then there are
two ways to a dequeue packet from the Qdisc to the driver buﬀer: 1)dequeue
a packet immediately, and 2) schedule a packet to be dequeued later through
softriq, which calls net tx action to retrieve packet from qdisc (Fig. 10).
Fig. 10. Packet Transmission
350
Y. Zhao et al.
(a) Throughput
(b) CPU Usage
(a) Throughput
(b) CPU Usage
(c) RTT
(d) Retransmission
(c) RTT
(d) Retransmission
Fig. 11. Overall performance of the net-
work stack as a function of the number of
ﬂows with ﬁxed TSO disabled and 1500
MTU size
Fig. 12. Overall performance of the net-
work stack as a function of the number of
ﬂows with TSO enabled and 9000 MTU
size
B Parameter Conﬁguration
Table 2 shows all the parameters we have used in our setup.
Table 2. Tuning parameters
Parameter
RX-Ring
Tuned
MAX [4096]
net.core.netdev max backlog
65536
net.core.tcp max syn backlog 65536
net.ipv4.tcp rmem
net.ipv4.tcp wmem
net.ipv4.tcp mem
8192 65536 16777216
8192 87380 16777216
768849 1025133 1537698
net.core.somaxconn
65535
net.netﬁlter.nf conntrack max 600000
TSO,GSO
interrupt moderation
irqbalance
enabled
enabled
disabled
C Overall Stack Performance
We ﬁnd that the trends shown in Fig. 2 remain the same regardless of packet
rate. In particular, we disable TSO, forcing the software stack to generate MTU
Scouting the Path to a Million-Client Server
351
packets. This ensures that the packet rate remains relatively constant across
experiments. Note that we perform experiments with a maximum number of
100k ﬂows. We try two values for the MTU: 1500 Bytes and 9000 Bytes. As
expected, the performance of the server saturates at a much lower number of
ﬂows when generating packets of 1500 Bytes (Fig. 11). This is because the packet
rate increases compared to the experiments discussed in Sect. 3. One the other
hand, the performance of the server when using 9000 Byte packets is similar to
that discussed in Sect. 3 (Fig. 12).
D FQ v.s. PFIFO
We compare the fq with pfifo fast qdiscs in terms of enqueueing latency
(Fig. 13). The time to enqueue a packet into pfifo fast queue is almost constant
while the enqueue time for fq increases with the number of ﬂows. This is because
the FQ uses a tree structure to keep track of every ﬂow and the complexity of
insertion operation is O(log(n)). The cache miss when fetching ﬂow information
from the tree also contributes to the latency with large number of ﬂows.
Fig. 13. Enqueue time
Fig. 14. BBR v.s. CUBIC
E Packet Rate with Zero Drops
We veriﬁed that BBR and CUBIC has similar CPU usage when PPS is ﬁxed
(Fig. 14). We disable TSO and GSO to ﬁx the packet size and set MTU size to
7000 to eliminate CPU bottleneck. We also observe that with more than 200k
ﬂows, CUBIC consumes slightly more CUBIC than BBR because CUBIC reacts
to packet drop by reducing packet size, thus generating more packets.
352
Y. Zhao et al.
References
1. High-performance,
feature-rich netxtreme R(cid:2) e-series dual-port 100g pcie eth-
ernet nic. https://www.broadcom.com/products/ethernet-connectivity/network-
adapters/100gb-nic-ocp/p2100g
2. Intel DPDK: Data plane development kit (2014). https://www.dpdk.org/
3. IEEE standard for ethernet - amendment 10: Media access control parameters,
physical layers, and management parameters for 200 gb/s and 400 gb/s operation.
IEEE Std 802.3bs-2017 (Amendment to IEEE 802.3-2015 as amended by IEEE’s
802.3bw-2015, 802.3by-2016, 802.3bq-2016, 802.3bp-2016, 802.3br-2016, 802.3bn-
2016, 802.3bz-2016, 802.3bu-2016, 802.3bv-2017, and IEEE 802.3-2015/Cor1-2017),
pp. 1–372 (2017)
4. Microprocessor trend data (2018). https://github.com/karlrupp/microprocessor-
trend-data
5. IEEE 802.3 Industry Connections Ethernet Bandwidth Assessment Part II (2020)
6. dstat-Linux man page (2020). https://linux.die.net/man/1/dstat
7. FlowQueue-Codel
https://tools.ietf.org/id/draft-ietf-aqm-fq-codel-02.
(2020).
html
8. neper: a Linux networking performance tool (2020). https://github.com/google/
neper
9. Netﬂix Help Center: Internet Connection Speed Recommendations (2020). https://
help.netﬂix.com/en/node/306
10. netstat-Linux man page (2020). https://linux.die.net/man/8/netstat
11. Perf Manual (2020). https://www.man7.org/linux/man-pages/man1/perf.1.html
12. ss-Linux man page (2020). https://linux.die.net/man/8/ss
13. Belay, A., Prekas, G., Klimovic, A., Grossman, S., Kozyrakis, C., Bugnion, E.:
{IX}: a protected dataplane operating system for high throughput and low latency.
In: 11th {USENIX} Symposium on Operating Systems Design and Implementation
({OSDI} 14), pp. 49–65 (2014)
14. Benvenuti, C.: Understanding Linux Network Internals. O’Reilly Media, Inc. (2006)
15. Brouer, J.D.: Network stack challenges at increasing speeds. In: Proceedings of the
Linux Conference, pp. 12–16 (2015)
16. Cardwell, N., Cheng, Y., Gunn, C.S., Yeganeh, S.H., Jacobson, V.: BBR:
congestion-based congestion control. Queue 14(5), 20–53 (2016)
17. Cavalcanti, F.R.P., Andersson, S.: Optimizing Wireless Communication Systems,
vol. 386. Springer, Cham (2009). https://doi.org/10.1007/978-1-4419-0155-2
18. Checconi, F., Rizzo, L., Valente, P.: Qfq: Eﬃcient packet scheduling with tight
guarantees. IEEE/ACM Trans. Networking 21(3)(2013)
19. Chen, Q.C., Yang, X.H., Wang, X.L.: A peer-to-peer based passive web crawling
system. In: 2011 International Conference on Machine Learning and Cybernetics,
vol. 4, pp. 1878–1883. IEEE (2011)
20. Dumazet, E., Corbet, J.: TCP small queues (2012). https://lwn.net/Articles/
507065/
21. Dumazet, E., Corbet, J.: Tso sizing and the FQ scheduler (2013). https://lwn.net/
Articles/564978/
22. Firestone, D., et al.: Azure accelerated networking: Smartnics in the public cloud.
In: 15th {USENIX} Symposium on Networked Systems Design and Implementa-
tion ({NSDI} 2018), pp. 51–66 (2018)
23. Geer, D.: Chip makers turn to multicore processors. IEEE Computer 38(2005)
Scouting the Path to a Million-Client Server
353
24. Hedayati, M., Shen, K., Scott, M.L., Marty, M.: Multi-queue fair queuing. In: 2019
USENIX Annual Technical Conference (USENIX ATC 2019) (2019)
25. Hock, M., Veit, M., Neumeister, F., Bless, R., Zitterbart, M.: TCP at 100 gbit/s-
tuning, limitations, congestion control. In: 2019 IEEE 44th Conference on Local
Computer Networks (LCN), pp. 1–9. IEEE (2019)
26. Jeong, E., et al.: MTCP: a highly scalable user-level {TCP} stack for multicore
systems. In: 11th {USENIX} Symposium on Networked Systems Design and Imple-
mentation ({NSDI} 2014), pp. 489–502 (2014)
27. Kalia, A., Kaminsky, M., Andersen, D.: Datacenter RPCs can be general and fast.
In: 16th {USENIX} Symposium on Networked Systems Design and Implementa-
tion ({NSDI} 2019), pp. 1–16 (2019)
28. Kaufmann, A., Peter, S., Sharma, N.K., Anderson, T., Krishnamurthy, A.: High
performance packet processing with ﬂexnic. In: Proceedings of the Twenty-First
International Conference on Architectural Support for Programming Languages
and Operating Systems, pp. 67–81 (2016)
29. Kaufmann, A., Stamler, T., Peter, S., Sharma, N.K., Krishnamurthy, A., Anderson,
T.: TAS: TCP acceleration as an OS service. In: Proceedings of the Fourteenth
EuroSys Conference, 2019, pp. 1–16 (2019)
30. Li, Y., Cornett, L., Deval, M., Vasudevan, A., Sarangam, P.: Adaptive interrupt
moderation (Apr 14 2015), uS Patent 9,009,367
31. Marty, M., et al.: Snap: a microkernel approach to host networking. In: Proceedings
of the 27th ACM Symposium on Operating Systems Principles. SOSP 2019, pp.
399–413 (2019)
32. Mogul, J.C., Ramakrishnan, K.: Eliminating receive livelock in an interrupt-driven
kernel. ACM Trans. Comput. Syst. 15(3), 217–252 (1997)
33. Moon, Y., Lee, S., Jamshed, M.A., Park, K.: Acceltcp: accelerating network appli-
cations with stateful TCP oﬄoading. In: 17th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 2020), pp. 77–92 (2020)
34. Ousterhout, A., Fried, J., Behrens, J., Belay, A., Balakrishnan, H.: Shenango:
Achieving high CPU eﬃciency for latency-sensitive datacenter workloads. In: Pro-
ceedings of USENIX NSDI 2019 (2019)
35. Radhakrishnan, S., .: {SENIC}: Scalable {NIC} for end-host rate limiting. In:
11th {USENIX} Symposium on Networked Systems Design and Implementation
({NSDI} 2014), pp. 475–488 (2014)
36. Rizzo, L.: Netmap: a novel framework for fast packet i/o. In: 21st USENIX Security
Symposium (USENIX Security 2012), pp. 101–112 (2012)
37. Rotaru, M., Olariu, F., Onica, E., Rivi`ere, E.: Reliable messaging to millions of
users with migratorydata. In: Proceedings of the 18th ACM/IFIP/USENIX Mid-
dleware Conference: Industrial Track, pp. 1–7 (2017)
38. Saeed, A., Dukkipati, N., Valancius, V., Lam, T., Contavalli, C., Vahdat, A.:
Carousel: scalable traﬃc shaping at end-hosts. In: Proceedings of ACM SIGCOMM
2017 (2017)
39. Saeed, A., et al.: Eiﬀel: Eﬃcient and ﬂexible software packet scheduling. In: Pro-
ceedings of USENIX NSDI 2019 (2019)
40. Shrivastav, V.: Fast, scalable, and programmable packet scheduler in hardware.
In: Proceedings of the ACM Special Interest Group on Data Communication. SIG-
COMM 2019 (2019)
41. Stephens, B., Akella, A., Swift, M.: Loom: ﬂexible and eﬃcient {NIC} packet
scheduling. In: 16th {USENIX} Symposium on Networked Systems Design and
Implementation ({NSDI} 19), pp. 33–46 (2019)
354
Y. Zhao et al.
42. Stephens, B., Singhvi, A., Akella, A., Swift, M.: Titan: Fair packet scheduling
for commodity multiqueue nics. In: 2017 {USENIX} Annual Technical Conference
(USENIX ATC 2017), pp. 431–444 (2017)
43. Sun, L., Kostic, P.: Adaptive hardware interrupt moderation, January 2 2014. uS
Patent App. 13/534,607
44. Yasukata, K., Honda, M., Santry, D., Eggert, L.: Stackmap: low-latency networking
with the {OS} stack and dedicated nics. In: 2016 {USENIX} Annual Technical
Conference ({USENIX}{ATC} 2016), pp. 43–56 (2016)
45. Zhang, T., Wang, J., Huang, J., Chen, J., Pan, Y., Min, G.: Tuning the aggres-
sive TCP behavior for highly concurrent http connections in intra-datacenter.
IEEE/ACM Trans. Networking 25(6), 3808–3822 (2017)
46. Zhao, Y., Saeed, A., Zegura, E., Ammar, M.: ZD: a scalable zero-drop network stack
at end hosts. In: Proceedings of the 15th International Conference on Emerging
Networking Experiments and Technologies, pp. 220–232 (2019)