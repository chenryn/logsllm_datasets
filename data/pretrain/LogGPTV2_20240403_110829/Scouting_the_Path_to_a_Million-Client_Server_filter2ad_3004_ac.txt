### 6. Related Work

In this paper, we have discussed significant efforts to improve various components of the network stack, including scheduling [18, 24, 38, 39] and back-pressure [46]. However, these works often fail to consider the interactions between different components, and none of the existing optimized components has been tested with a load larger than 50,000 flows. Our work defines a broader category of limitations and examines the complex interactions between different components.

Much of the previous research has focused on scaling servers in terms of aggregate traffic intensity, measured in packets per second, while maintaining low latency [2, 13, 28, 34, 36]. Some recent proposals aim to scale the entire stack to handle a large number of flows [26, 29, 33, 37]. For example, mTcp [26] is a scalable user-space TCP/IP stack built over kernel-bypass packet I/O engines, but its evaluation was limited to a maximum of 16,000 flows. Additionally, it focuses on improving connection locality and reducing system overhead without much attention to scheduling and back-pressure. Other systems have been evaluated at a few thousand flows [29] and up to twenty thousand flows [33, 36, 37, 44], with an emphasis on specific functionalities such as RPC performance or transport layer performance, typically for short-lived flows. In contrast, our work is more concerned with scaling to hundreds of thousands of long-lived flows, where both transport and scheduling are implemented. To the best of our knowledge, this is the first such study.

Another observation is that hardware offload solutions [22, 40, 41] alone cannot completely solve the problem. While careful hardware design can reduce the latency of complex operations [40], data structure issues do not disappear when implemented in hardware. Furthermore, admission control requires careful coordination between the software and hardware parts of the stack, including the application.

### 7. Relevance of Findings to Other Stacks

In this paper, we focus on the Linux stack due to its widespread use in both industry and academia. However, most of our findings pertain to abstract functions necessary in any stack to efficiently handle a large number of flows. For example, admission control can prevent overwhelming the stack resources by relying on per-flow scheduling and accurate batching sizing. The lack of similar functions in any stack can lead to performance degradation as the number of flows increases. Additionally, better data structures for scheduling and demultiplexing can result in significant CPU savings. Conversely, some of the problems we define are specific to Linux, arising from components developed by companies to handle their specific workloads. For instance, autosizing, developed by Google, makes overpacing a Linux-specific problem.

Some stacks inherently address some of the issues we have identified. For example, Snap [31] provides per-flow scheduling, offering efficient back-pressure. Stacks that rely on lightweight threading and asynchronous messages, such as Snap and Shenango, may not suffer significant performance degradation due to lock contention. However, none of these stacks addresses all the problems. The goal of our work is to identify abstract functions that stacks must implement to scale effectively.

Some of the problems we have identified only become apparent at very large numbers of flows. To the best of our knowledge, these issues have yet to be addressed by any stack. For example, delays introduced by cache misses will require innovation in speculative pre-fetching based on network behavior. Additionally, network accelerators and programmable hardware components will need new techniques to coordinate their behavior with changes in the load generated by the software component of the stack.

### 8. Conclusion

In this paper, we identify the different bottlenecks that arise when scaling the number of flows to hundreds of thousands in a fully implemented stack. While there have been efforts to address some of these individual problems in isolation, integrating and testing such solutions at the scale of hundreds of thousands to millions of long-lived, simultaneously active flows remains an open challenge. We hope that this paper sheds light on the pain points that stack designers should pay attention to when building next-generation stacks capable of handling terabits per second and millions of flows.

### A. Linux Stack Overview

Packet transmission in an end-host involves a packet traversing from user space to kernel space and finally to the NIC. The application generates a packet and copies it into the kernel space TCP buffer. Packets from the TCP buffer are then queued into Qdisc. There are two ways to dequeue a packet from the Qdisc to the driver buffer: 1) dequeue a packet immediately, or 2) schedule a packet to be dequeued later through softirq, which calls `net_tx_action` to retrieve the packet from the Qdisc (Fig. 10).

**Figure 10. Packet Transmission**

### B. Parameter Configuration

Table 2 shows all the parameters used in our setup.

**Table 2. Tuning Parameters**

| Parameter                      | Tuned Value |
|--------------------------------|-------------|
| RX-Ring                        | MAX [4096]  |
| net.core.netdev_max_backlog    | 65536       |
| net.core.tcp_max_syn_backlog   | 65536       |
| net.ipv4.tcp_rmem              | 8192, 65536, 16777216 |
| net.ipv4.tcp_wmem              | 8192, 87380, 16777216 |
| net.ipv4.tcp_mem               | 768849, 1025133, 1537698 |
| net.core.somaxconn             | 65535       |
| net.netfilter.nf_conntrack_max | 600000      |
| TSO, GSO                       | Enabled     |
| Interrupt Moderation           | Disabled    |
| irqbalance                     | Enabled     |

### C. Overall Stack Performance

We find that the trends shown in Fig. 2 remain consistent regardless of the packet rate. Specifically, we disable TSO, forcing the software stack to generate MTU packets, ensuring that the packet rate remains relatively constant across experiments. Note that we perform experiments with a maximum of 100,000 flows. We test two MTU values: 1500 Bytes and 9000 Bytes. As expected, the server's performance saturates at a much lower number of flows when generating 1500 Byte packets (Fig. 11). This is because the packet rate increases compared to the experiments discussed in Section 3. On the other hand, the server's performance with 9000 Byte packets is similar to that discussed in Section 3 (Fig. 12).

**Figure 11. Overall performance of the network stack as a function of the number of flows with fixed TSO disabled and 1500 MTU size**

**Figure 12. Overall performance of the network stack as a function of the number of flows with TSO enabled and 9000 MTU size**

### D. FQ vs. PFIFO

We compare the FQ and pfifo_fast qdiscs in terms of enqueueing latency (Fig. 13). The time to enqueue a packet into the pfifo_fast queue is almost constant, while the enqueue time for FQ increases with the number of flows. This is because FQ uses a tree structure to track every flow, and the complexity of the insertion operation is O(log(n)). Cache misses when fetching flow information from the tree also contribute to the latency with a large number of flows.

**Figure 13. Enqueue time**

### E. Packet Rate with Zero Drops

We verified that BBR and CUBIC have similar CPU usage when the PPS (Packets Per Second) is fixed (Fig. 14). We disable TSO and GSO to fix the packet size and set the MTU size to 7000 to eliminate CPU bottlenecks. We also observe that with more than 200,000 flows, CUBIC consumes slightly more CPU than BBR because CUBIC reacts to packet drops by reducing the packet size, thus generating more packets.

**Figure 14. BBR vs. CUBIC**

### References

1. High-performance, feature-rich NetXtreme R-series dual-port 100G PCIe Ethernet NIC. https://www.broadcom.com/products/ethernet-connectivity/network-adapters/100gb-nic-ocp/p2100g
2. Intel DPDK: Data Plane Development Kit (2014). https://www.dpdk.org/
3. IEEE Standard for Ethernet - Amendment 10: Media Access Control Parameters, Physical Layers, and Management Parameters for 200 Gb/s and 400 Gb/s Operation. IEEE Std 802.3bs-2017 (Amendment to IEEE 802.3-2015 as amended by IEEE’s 802.3bw-2015, 802.3by-2016, 802.3bq-2016, 802.3bp-2016, 802.3br-2016, 802.3bn-2016, 802.3bz-2016, 802.3bu-2016, 802.3bv-2017, and IEEE 802.3-2015/Cor1-2017), pp. 1–372 (2017)
4. Microprocessor Trend Data (2018). https://github.com/karlrupp/microprocessor-trend-data
5. IEEE 802.3 Industry Connections Ethernet Bandwidth Assessment Part II (2020)
6. dstat-Linux man page (2020). https://linux.die.net/man/1/dstat
7. FlowQueue-Codel. https://tools.ietf.org/id/draft-ietf-aqm-fq-codel-02.html (2020)
8. neper: A Linux Networking Performance Tool (2020). https://github.com/google/neper
9. Netflix Help Center: Internet Connection Speed Recommendations (2020). https://help.netflix.com/en/node/306
10. netstat-Linux man page (2020). https://linux.die.net/man/8/netstat
11. Perf Manual (2020). https://www.man7.org/linux/man-pages/man1/perf.1.html
12. ss-Linux man page (2020). https://linux.die.net/man/8/ss
13. Belay, A., Prekas, G., Klimovic, A., Grossman, S., Kozyrakis, C., Bugnion, E.: IX: A Protected Dataplane Operating System for High Throughput and Low Latency. In: 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pp. 49–65 (2014)
14. Benvenuti, C.: Understanding Linux Network Internals. O’Reilly Media, Inc. (2006)
15. Brouer, J.D.: Network Stack Challenges at Increasing Speeds. In: Proceedings of the Linux Conference, pp. 12–16 (2015)
16. Cardwell, N., Cheng, Y., Gunn, C.S., Yeganeh, S.H., Jacobson, V.: BBR: Congestion-Based Congestion Control. Queue 14(5), 20–53 (2016)
17. Cavalcanti, F.R.P., Andersson, S.: Optimizing Wireless Communication Systems, vol. 386. Springer, Cham (2009). https://doi.org/10.1007/978-1-4419-0155-2
18. Checconi, F., Rizzo, L., Valente, P.: QFQ: Efficient Packet Scheduling with Tight Guarantees. IEEE/ACM Trans. Networking 21(3) (2013)
19. Chen, Q.C., Yang, X.H., Wang, X.L.: A Peer-to-Peer Based Passive Web Crawling System. In: 2011 International Conference on Machine Learning and Cybernetics, vol. 4, pp. 1878–1883. IEEE (2011)
20. Dumazet, E., Corbet, J.: TCP Small Queues (2012). https://lwn.net/Articles/507065/
21. Dumazet, E., Corbet, J.: TSO Sizing and the FQ Scheduler (2013). https://lwn.net/Articles/564978/
22. Firestone, D., et al.: Azure Accelerated Networking: SmartNICs in the Public Cloud. In: 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2018), pp. 51–66 (2018)
23. Geer, D.: Chip Makers Turn to Multicore Processors. IEEE Computer 38 (2005)
24. Hedayati, M., Shen, K., Scott, M.L., Marty, M.: Multi-Queue Fair Queuing. In: 2019 USENIX Annual Technical Conference (USENIX ATC 2019) (2019)
25. Hock, M., Veit, M., Neumeister, F., Bless, R., Zitterbart, M.: TCP at 100 Gbit/s - Tuning, Limitations, Congestion Control. In: 2019 IEEE 44th Conference on Local Computer Networks (LCN), pp. 1–9. IEEE (2019)
26. Jeong, E., et al.: MTCP: A Highly Scalable User-Level TCP Stack for Multicore Systems. In: 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2014), pp. 489–502 (2014)
27. Kalia, A., Kaminsky, M., Andersen, D.: Datacenter RPCs Can Be General and Fast. In: 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2019), pp. 1–16 (2019)
28. Kaufmann, A., Peter, S., Sharma, N.K., Anderson, T., Krishnamurthy, A.: High Performance Packet Processing with FlexNIC. In: Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 67–81 (2016)
29. Kaufmann, A., Stamler, T., Peter, S., Sharma, N.K., Krishnamurthy, A., Anderson, T.: TAS: TCP Acceleration as an OS Service. In: Proceedings of the Fourteenth EuroSys Conference, 2019, pp. 1–16 (2019)
30. Li, Y., Cornett, L., Deval, M., Vasudevan, A., Sarangam, P.: Adaptive Interrupt Moderation (Apr 14 2015), US Patent 9,009,367
31. Marty, M., et al.: SNAP: A Microkernel Approach to Host Networking. In: Proceedings of the 27th ACM Symposium on Operating Systems Principles. SOSP 2019, pp. 399–413 (2019)
32. Mogul, J.C., Ramakrishnan, K.: Eliminating Receive Livelock in an Interrupt-Driven Kernel. ACM Trans. Comput. Syst. 15(3), 217–252 (1997)
33. Moon, Y., Lee, S., Jamshed, M.A., Park, K.: AccelTCP: Accelerating Network Applications with Stateful TCP Offloading. In: 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2020), pp. 77–92 (2020)
34. Ousterhout, A., Fried, J., Behrens, J., Belay, A., Balakrishnan, H.: Shenango: Achieving High CPU Efficiency for Latency-Sensitive Datacenter Workloads. In: Proceedings of USENIX NSDI 2019 (2019)
35. Radhakrishnan, S., et al.: SENIC: Scalable NIC for End-Host Rate Limiting. In: 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2014), pp. 475–488 (2014)
36. Rizzo, L.: Netmap: A Novel Framework for Fast Packet I/O. In: 21st USENIX Security Symposium (USENIX Security 2012), pp. 101–112 (2012)
37. Rotaru, M., Olariu, F., Onica, E., Rivière, E.: Reliable Messaging to Millions of Users with MigratoryData. In: Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference: Industrial Track, pp. 1–7 (2017)
38. Saeed, A., Dukkipati, N., Valancius, V., Lam, T., Contavalli, C., Vahdat, A.: Carousel: Scalable Traffic Shaping at End-Hosts. In: Proceedings of ACM SIGCOMM 2017 (2017)
39. Saeed, A., et al.: Eiffel: Efficient and Flexible Software Packet Scheduling. In: Proceedings of USENIX NSDI 2019 (2019)
40. Shrivastav, V.: Fast, Scalable, and Programmable Packet Scheduler in Hardware. In: Proceedings of the ACM Special Interest Group on Data Communication. SIGCOMM 2019 (2019)
41. Stephens, B., Akella, A., Swift, M.: Loom: Flexible and Efficient NIC Packet Scheduling. In: 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19), pp. 33–46 (2019)
42. Stephens, B., Singhvi, A., Akella, A., Swift, M.: Titan: Fair Packet Scheduling for Commodity Multi-Queue NICs. In: 2017 USENIX Annual Technical Conference (USENIX ATC 2017), pp. 431–444 (2017)
43. Sun, L., Kostic, P.: Adaptive Hardware Interrupt Moderation, January 2 2014. US Patent App. 13/534,607
44. Yasukata, K., Honda, M., Santry, D., Eggert, L.: StackMap: Low-Latency Networking with the OS Stack and Dedicated NICs. In: 2016 USENIX Annual Technical Conference (USENIX ATC 2016), pp. 43–56 (2016)
45. Zhang, T., Wang, J., Huang, J., Chen, J., Pan, Y., Min, G.: Tuning the Aggressive TCP Behavior for Highly Concurrent HTTP Connections in Intra-Datacenter. IEEE/ACM Trans. Networking 25(6), 3808–3822 (2017)
46. Zhao, Y., Saeed, A., Zegura, E., Ammar, M.: ZD: A Scalable Zero-Drop Network Stack at End Hosts. In: Proceedings of the 15th International Conference on Emerging Networking Experiments and Technologies, pp. 220–232 (2019)

This version of the text is more coherent, professional, and easier to read. It maintains the technical details while improving the overall flow and clarity.