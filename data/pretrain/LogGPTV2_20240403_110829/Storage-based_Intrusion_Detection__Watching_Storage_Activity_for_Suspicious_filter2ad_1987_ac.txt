ﬁle modiﬁcations described above. Especially interesting
for this analysis is the use of exec() redirection by four
of the kernel rootkits. With such redirection, the exec()
system call uses a replacement version of a targeted pro-
gram, while other system calls return information about
or data from the original. As a result, any tool relying on
the accuracy of system calls to check ﬁle integrity, such as
Tripwire, will be fooled.
All of these rootkits are detected using our storage IDS
rules—they all put their replacement programs in the orig-
inals’ directories (which are watched), and four of the six
actually move the original ﬁle to a new name and store
their replacement ﬁle with the original name (which also
triggers an alert). However, future rootkits could be mod-
iﬁed to be less obvious to a storage IDS. Speciﬁcally, the
original ﬁles could be left untouched and replacement ﬁles
could be stored someplace not watched by the storage IDS,
such as a random user directory—neither would gener-
ate an alert. With this approach, ﬁle modiﬁcation can be
completely hidden from a storage IDS unless the rootkit
wants to reinstall the kernel modiﬁcation after a reboot.
To accomplish this, some original ﬁles would need to be
changed, which forces intruders to make an interesting
choice: hide from the storage IDS or persist beyond the
next reboot.
142
12th USENIX Security Symposium 
USENIX Association
3.3 Anecdotal experience
During the writing of this paper, one of the authors hap-
pened to be asked to analyze a system that had been
recently compromised. Several modiﬁcations similar to
those made by the above rootkits were found on the sys-
tem. Root’s .bash profile was modiﬁed to run the zap2
log scrubber, so that as soon as root logged into the sys-
tem to investigate the intrusion, the related logs would
be scrubbed. Several binaries were modiﬁed (ps, top,
netstat, pstree, sshd, and telnetd). The binaries
were setup to hide the existence of an IRC bot, running
out of the directory ‘/dev/.. /’. This experience helps
validate our choice of “rootkits” for study, as they appear to
be representative of at least one real-world intrusion. This
intrusion would have triggered at least 8 storage IDS rules.
uring an IDS to look for suspicious content. Rules can be
speciﬁed as signatures that are compared against ﬁles’ con-
tents. Similarly, ﬁlename expression grammars (like those
provided in scripting languages) could be used to describe
suspicious ﬁlenames.
Less guidance exists for the other two categories of warn-
ing signs: update patterns and content integrity. We do not
currently know how to specify general rules for these cat-
egories. Our approach has been to fall back on Tripwire-
style rules; we hard-code checking functions (e.g., for non-
append update or a particular content integrity violation)
and then allow an administrator to specify on which ﬁles
they should be checked (or that they should be checked for
every ﬁle). More general approaches to specifying detec-
tion rules for these categories of warning signs are left for
future work.
4 Design of a Storage IDS
4.2 Secure administration
To be useful in practice, a storage IDS must simultane-
ously achieve several goals. It must support a useful set of
detection rules, while also being easy for human admin-
istrators to understand and conﬁgure. It must be efﬁcient,
minimizing both added delay and added resource require-
ments; some user communities still accept security mea-
sures only when they are “free.” Additionally, it should be
invisible to users at least until an intrusion detection rule is
matched.
This section describes four aspects of storage IDS design:
specifying detection rules, administering a storage IDS se-
curely, checking detection rules, and responding to suspi-
cious activity.
4.1 Specifying detection rules
Specifying rules for an IDS is a tedious, error prone activ-
ity. The tools an administrator uses to write and manipu-
late those rules should be as simple and straightforward as
possible. Each of the four categories of suspicious activity
presented earlier will likely need a unique format for rule
speciﬁcation.
The rule format used by Tripwire seems to work well for
specifying rules concerned with data and attribute modiﬁ-
cation. This format allows an administrator to specify the
pathname of a ﬁle and a list of properties that should be
monitored for that ﬁle. The set of watchable properties are
codiﬁed, and they include most ﬁle attributes. This rule
language works well, because it allows the administrator
to manipulate a well understood representation (pathnames
and ﬁles), and the list of attributes that can be watched is
small and well-deﬁned.
The methods used by virus scanners work well for conﬁg-
The security administrator must have a secure interface to
the storage IDS. This interface is needed for the admin-
istrator to conﬁgure detection rules and to receive alerts.
The interface must prevent client systems from forging or
blocking administrative requests, since this could allow a
crafty intruder to sneak around the IDS by disarming it. At
a minimum, it must be tamper-evident. Otherwise, intrud-
ers could stop rule updates or prevent alerts from reaching
the administrator. To maintain compromise independence,
it must be the case that obtaining “superuser” or even ker-
nel privileges on a client system is insufﬁcient to gain ad-
ministrative access to the storage device.
Two promising architectures exist for such administration:
one based on physical access and one based on cryptogra-
phy. For environments where the administrator has phys-
ical access to the device, a local administration terminal
that allows the administrator to set detection rules and re-
ceive the corresponding alert messages satisﬁes the above
goals.
In environments where physical access to the device is
not practical, cryptography can be used to secure com-
munications. In this scenario, the storage device acts as
an endpoint for a cryptographic channel to the adminis-
trative system. The device must maintain keys and per-
form the necessary cryptographic functions to detect mod-
iﬁed messages, lost messages, and blocked channels. Ar-
chitectures for such trust models in storage systems ex-
ist [14]. This type of infrastructure is already common for
administration of other network-attached security compo-
nents, such as ﬁrewalls or network intrusion detection sys-
tems. For direct-attached storage devices, cryptographic
channels can be used to tunnel administrative requests and
alerts through the OS of the host system, as illustrated in
Figure 2. Such tunneling simply treats the host OS as an
USENIX Association
12th USENIX Security Symposium 
143
Operating System
File
System
Device
Driver
Client
Cryptographic tunnel
through the client OS
Storage
Requests
IDS
Configuration
and
Alerts
Admin Console
Figure 2: Tunneling administrative commands through client sys-
tems. For storage devices attached directly to client systems, a crypto-
graphic tunnel can allow the administrator to securely manage a storage
IDS. This tunnel uses the untrusted client OS to transport administrative
commands and alerts.
untrusted network component.
For small numbers of dedicated servers in a machine room,
either approach is feasible. For large numbers of storage
devices or components operating in physically insecure en-
vironments, cryptography is the only viable solution.
4.3 Checking the detection rules
Checking detection rules can be non-trivial, because rules
generally apply to full pathnames rather than inodes. Addi-
tional complications arise because rules can watch for ﬁles
that do not yet exist.
For simple operations that act on individual ﬁles (e.g.,
READ and WRITE), rule veriﬁcation is localized. The de-
vice need only check that the rules pertaining to that spe-
ciﬁc ﬁle are not violated (usually a simple ﬂag comparison,
sometimes a content check). For operations that affect the
ﬁle system’s namespace, veriﬁcation is more complicated.
For example, a rename of a directory tree may impact a
large number of individual ﬁles, any of which could have
IDS rules that must be checked. Renaming a directory re-
quires examining all ﬁles and directories that are children
of the one being renamed.
In the case of rules pertaining to ﬁles that do not currently
exist, this list of rules must be consulted when operations
change the namespace. For example, the administrator may
want to watch for the existence of a ﬁle named /a/b/c
even if the directory named /a does not yet exist. However,
a single ﬁle system operation (e.g., mv /z /a) could cause
the watched ﬁle to suddenly exist, given the appropriate
structure for z’s directory tree.
4.4 Responding to rule violations
Since a detected “intruder action” may actually be legiti-
mate user activity (i.e., a false alarm), our default response
is simply to send an alert to the administrative system or
the designated alert log ﬁle. The alert message should con-
tain such information as the ﬁle(s) involved, the time of the
event, the action being performed, the action’s attributes
(e.g., the data written into the ﬁle), and the client’s identity.
Note that, if the rules are set properly, most false positives
should be caused by legitimate updates (e.g., upgrades)
from an administrator. With the right information in alerts,
an administrative system that also coordinates legitimate
upgrades could correlate the generated alert (which can
include the new content) with the in-progress upgrade; if
this were done, it could prevent the false alarm from reach-
ing the human administrator while simultaneously verify-
ing that the upgrade went through to persistent storage cor-
rectly.
There are more active responses that a storage IDS could
trigger upon detecting suspicious activity. When choosing
a response policy, of course, the administrator must weigh
the beneﬁts of an active response against the inconvenience
and potential damage caused by false alarms.
One reasonable active response is to slow down the sus-
pected intruder’s storage accesses. For example, a storage
device could wait until the alert is acknowledged before
completing the suspicious request. It could also artiﬁcially
increase request latencies for a client or user that is sus-
pected of foul play. Doing so would provide increased time
for a more thorough response, and, while it will cause some
annoyance in false alarm situations, it is unlikely to cause
damage. The device could even deny a request entirely if it
violates one of the rules, although this response to a false
alarm could cause damage and/or application failure. For
some rules, like append-only audit logs, such access con-
trol may be desirable.
Liu, et al. proposed a more radical response to detected
intrusions: isolating intruders, via versioning, at the ﬁle
system level [22]. To do so, the ﬁle system forks the ver-
sion trees to sandbox suspicious users until the administra-
tor veriﬁes the legitimacy of their actions. Unfortunately,
such forking is likely to interfere with system operation,
unless the intrusion detection mechanism yields no false
alarms. Speciﬁcally, since suspected users modify differ-
ent versions of ﬁles from regular users, the system faces a
difﬁcult reintegration [20, 41] problem, should the updates
be judged legitimate. Still, it is interesting to consider em-
bedding this approach, together with a storage IDS, into
storage systems for particularly sensitive environments.
A less intrusive storage-embedded response is to start ver-
sioning all data and auditing all storage requests when an
intrusion is detected. Doing so provides the administra-
144
12th USENIX Security Symposium 
USENIX Association
tor with signiﬁcant information for post-intrusion diagno-
sis and recovery. Of course, some intrusion-related infor-
mation will likely be lost unless the intrusion is detected
immediately, which is why Strunk et al. [38] argue for al-
ways doing these things (just in case). Still, IDS-triggered
employment of this functionality may be a useful trade-off
point.
5 Storage-based intrusion detection
in an NFS server
To explore the concepts and feasibility of storage-based in-
trusion detection, we implemented a storage IDS in an NFS
server. Unmodiﬁed client systems access the server using
the standard NFS version 2 protocol [40]2, while storage-
based intrusion detection occurs transparently. This section
describes how the prototype storage IDS handles detection
rule speciﬁcation, the structures and algorithms for check-
ing rules, and alert generation.
The base NFS server is called S4, and its implementa-
tion is described and evaluated elsewhere [38]. It inter-
nally performs ﬁle versioning and request auditing, using
a log-structured ﬁle system [32], but these features are not
relevant here. For our purposes, it is a convenient NFS
ﬁle server with performance comparable to the Linux and
FreeBSD NFS servers. Secure administration is performed
via the server’s console, using the physical access control
approach.
5.1 Specifying detection rules
Our prototype storage IDS is capable of watching for a va-
riety of data and metadata changes to ﬁles. The administra-
tor speciﬁes a list of Tripwire-style rules to conﬁgure the
detection system. Each administrator-supplied rule is of
the form: {pathname, attribute-list}—designating which
attributes to monitor for a particular ﬁle. The list of at-
tributes that can be watched is shown in Table 2. In ad-
dition to standard Tripwire rules, we have added two ad-
ditional functions that can be speciﬁed on a per-ﬁle ba-
sis. The ﬁrst watches for non-append changes, as described
earlier; any truncation or write anywhere but at the previ-
ous end of a ﬁle will generate an alert. The second checks
a ﬁle’s integrity against the password ﬁle integrity rule dis-
cussed earlier. After every write, the ﬁle must conform to
the rigid structure of a password ﬁle (7 colons per line),
and all of the shells must be contained in the “acceptable”
list.
In addition to per-ﬁle rules, an administrator can choose to
2The use of the NFSv2 protocol is an artifact of the server implemen-
tation the IDS is built into, but makes no difference in the areas we care
about.
Metadata
• data modiﬁcation time
• ﬁle permissions
• device number
• inode number
• ﬁle owner group
• inode modiﬁcation time
• access time
• link count
• ﬁle owner
• ﬁle type
• ﬁle size
• any modiﬁcation
• password structure
Data
• append only
Table 2: Attribute list. Rules can be established to watch these at-
tributes in real-time on a ﬁle-by-ﬁle basis.
enable any of three system-wide rules: one that matches on
any operation that rolls-back a ﬁle’s modiﬁcation time, one
that matches on any operation that creates a “hidden” di-
rectory (e.g., a directory name beginning with ‘.’ and hav-
ing spaces in it), and one that looks for known (currently
hard-coded) intrusion tools by their sizes and SHA-1 di-
gests. Although the system currently checks the digests on
every ﬁle update, periodic scanning of the system would
likely be more practical. These rules apply to all parts of
the directory hierarchy and are speciﬁed as simply ON or
OFF.
Rules are communicated to the server through the use of
an administrative RPC. This RPC interface has two com-
mands (see Table 3). The setRule() RPC gives the IDS
two values: the path of the ﬁle to be watched, and a set of
ﬂags describing the speciﬁc rules for that ﬁle. Rules are re-
moved through the same mechanism, specifying the path
and an empty rule set.
5.2 Checking the detection rules
This subsection describes the core of the storage IDS. It
discusses how rules are stored and subsequently checked
during operation.
5.2.1 Data structures
Three new structures allow the storage IDS to efﬁciently
support the detection rules: the reverse lookup table, the
inode watch ﬂags, and the non-existent names table.
Reverse lookup table: The reverse lookup table serves
two functions. First, it serves as a list of rules that the server
is currently enforcing. Second, it maps an inode number to
a pathname. The alert generation mechanism uses the lat-
ter to provide the administrator with ﬁle names instead of
inode numbers, without resorting to a brute-force search of
the namespace.