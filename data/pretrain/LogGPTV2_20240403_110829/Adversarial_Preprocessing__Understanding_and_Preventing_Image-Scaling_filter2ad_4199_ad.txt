As a remedy, we propose to limit the number of changed
pixels. To build on the successful attacks against OpenCV
and TensorFlow, we allow 2 pixels to be freely changed in the
optimization from Eq. (4) while using images with β ∈ [4,5).
The goal is to ﬁnd a modiﬁcation for these pixels, such that
the convolution over the whole kernel yields the target value.
Figure 13: User study on image-scaling attacks with respect to objective O2.
The attack is successful if only the source image S is visible (red).
To increase the chances to obtain a feasible solution, we
additionally allow the remaining pixels to be changed by at
most 10. We rerun the experiment from the previous section
with this new constraint and report results for 120 image pairs
with β ∈ [4,5) for bilinear and bicubic scaling, respectively.
Results. The added constraint severely impacts the success
rate of the attack. The rate drops to 0% for bilinear scaling
and to 0.83% for bicubic scaling. That is, the objective O1
is not reached anymore. In the majority of cases, no feasible
solution exists and several columns of the source image are
not modiﬁed. Only in a single case, the attack is successful
for bicubic scaling. However, the attack image shows obvious
traces from the target image, clearly revealing the attack.
5.3.2 Attacking Area Scaling
Area scaling stands out from the other algorithms as it em-
ploys a uniform weighting of pixels and operates on rectan-
gular blocks instead of columns and rows. As a result, the
original attack by Xiao et al. [35] is not applicable to this
scaling algorithm. To attack area scaling, we thus propose
two novel attack strategies.
The ﬁrst strategy aims at slightly changing all pixels of
a block to control its average. That is, we seek a minimal
perturbation under the L1 norm such that the average of the
block becomes the targeted value. For a target value t, we
solve the following optimization problem:
min((cid:107) ˜∆(cid:107)1) s.t. (cid:13)(cid:13)avg( ˜S + ˜∆)−t(cid:13)(cid:13)∞
(11)
where ˜S is the current block, ˜∆ its perturbation and ε a small
threshold. The L1 norm in Eq. (11) leads to an equally dis-
(cid:54) ε ,
1372    29th USENIX Security Symposium
USENIX Association
NearestBilinearBicubic0255075100SuccessRate[%]OpenCVPillowTensorFlow234510050100Speciﬁed[%]CV—Nearest234510050100CV—Linear234510050100CV—Cubic234510050100Speciﬁed[%]TF—Nearest234510050100TF—Linear234510050100TF—Cubic234510050100ScalingRatioSpeciﬁed[%]PIL—Nearest234510050100ScalingRatioPIL—LinearonlySvisiblebothvisibleonlyTvisible234510050100ScalingRatioPIL—Cubicchanges are thus required to obtain a similar output after
scaling. We report results for the 100 best novel source-target
pairs in the following.
As before, both the L1 and L0 attack are successful in 100%
of all cases regarding objective O1. However, the attack again
largely overwrites the source image, such that the target is
visible in all cases. The examples from Figure 21 in Ap-
pendix D underline that the attack fails to keep the changes
minimal, although the source and target are similar to each
other. The average PSNR value is 16 dB for L1 and 12 dB
for L0. Both are slightly higher than in the non-selective sce-
nario but still far too low compared to successful examples
from Section 5.2.
Summary. We conclude that area scaling is robust against
the different adaptive attacks considered in this work, as well
as the selection of source images. These attacks are a best
effort for assessing the security of area scaling and conﬁrm
our theoretical analysis from Section 4.2. In summary, we
recommend using area scaling when the uniform weighting
of pixels does not impact any following analysis steps.
5.4 Defense 2: Non-Adaptive Attack
We proceed with evaluating our novel defenses for recon-
structing images (Section 4.3). In particular, we combine the
selective median and random ﬁlter with a vulnerable scaling
algorithm and test the robustness of the combination. As at-
tacks, we consider all manipulated images from Section 5.2
that satisfy the objectives O1 and O2 for one scaling algo-
rithm. This includes attacks against nearest-neighbor scaling
from all imaging libraries as well as attacks against bilinear
and bicubic scaling from OpenCV and TensorFlow.
Evaluation O1. Our two defenses prevent all attacks. When
they are employed, no attack image succeeds in reaching
objective O1 for the respective scaling algorithm. The image
reconstruction effectively removes the manipulated pixels and
thereby prevents a misclassiﬁcation of the images.
Evaluation O2. As the original image content is recon-
structed, the visual difference between the source and the
reconstructed images are minimal. Figure 15 depicts the dis-
tribution of PSNR values between each source and attack
image—before and after reconstruction. The quality con-
siderably increases after restoration and reaches high PSNR
values above 25 dB. Figure 22 in Appendix D provides some
examples before and after reconstruction.
Reconstruction Accuracy. Table 4 depicts the success rate
of reconstructing the attack image’s original prediction, that
is, we obtain the prediction of its actual source image. The
median ﬁlter recovers the predictions in almost all cases suc-
cessfully. For the random ﬁlter, the success rate is slightly
reduced due to the noise from the reconstruction.
Figure 14: Adaptive attack against area scaling: (a) Distribution of PSNR
values and (b) the average number of changed pixels by the L0-based attack.
tributed manipulation of the pixels in each block. The results
for the L2 norm are equivalent and thus omitted.
The second strategy aims at adapting only a few pixels
of a block while leaving the rest untouched. In this case,
we optimize the L0 norm, since only the number of changed
pixels counts. Our attack works as follows for a current image
block: if the target value is larger than the current average,
the adversary iteratively sets pixels in the source to Imax until
the target is reached. If the target is smaller, we iteratively
set pixels to 0. Note that the last value generally needs to be
adapted, such that the average becomes the target value.
Results. With respect to objective O1, both the L1 and L0
attack are successful in 100% of the images. However, both
variants fail reaching objective O2 in all of the cases. A man-
ual inspection of the images reveals that the source is largely
overwritten by both attacks and parts of the target become vis-
ible in all attack images. Figure 14(a) provides results on this
experiment by showing the distribution of PSNR values over
all source-attack image pairs. The average PSNR is 8.6 dB
for L1 and 6.7 dB for L0, which corresponds to a very low
similarity between the source and the attack image. In addi-
tion, Figure 14(b) depicts the distribution of changed pixels
for the L0 attack. While for the majority around 50% of the
pixels are changed, a few images only require to change 28%.
Still, this is too much to achieve objective O2. Figure 20 in
Appendix D shows the ﬁve best images from our evaluation
with the smallest number of changed pixels. In all cases, the
source image cannot be recognized anymore.
5.3.3 Selective Source Image
In addition to the two adaptive attacks, we also examine area
scaling under a more challenging scenario. In this scenario,
the adversary selects the most suitable source image for a ﬁxed
target. As a result, the class of the source image is arbitrary
and potentially suspicious, yet the attack becomes stronger
due to the selected combination of source and target. We
implement this strategy as follows: For each target image T ,
we choose the source image S, for which the scaled version
has the smallest average distance to the target image. Fewer
USENIX Association
29th USENIX Security Symposium    1373
02.557.51012.51500.20.4PSNRbetweensourceimageandattackimageDensityL1normL0norm020406080100024Averagenumberofchangedpixelsperattackimage[%]DensityFigure 15: PSNR distribution before and after attack image reconstruction
for median and random ﬁlter on OpenCV’s scaling algorithms. Results for
the other scaling algorithms are similar and thus omitted.
In addition, we also measure the impact of both ﬁlters
on benign, unmodiﬁed images. The median ﬁlter runs with
almost no loss of accuracy. The random ﬁlter induces a small
loss which can be acceptable if a low run-time overhead of
this defense is an important criterion in practice.
Run-time Evaluation. Finally, we evaluate the run-time per-
formance of the two proposed defenses. To this end, we
apply the defenses along with different scaling algorithms to
2,000 images and measure the average run-time per image.
The test system is an Intel Xeon E5-2699 v3 with 2.4 GHz.
Our measurements are shown in Figure 16 on a logarithmic
scale in microseconds. Area scaling as well as our defenses
introduce a notable overhead and cannot compete with the
insecure nearest-neighbor scaling in performance. However,
in comparison to a pass through the VGG19 model, our de-
fenses are almost an order of magnitude faster and induce a
neglectable overhead for deep learning systems.
Summary. This experiment shows that the median and ran-
dom ﬁlter provide effective defenses against non-adaptive
attacks. In contrast to robust scaling, the defenses prevent the
attack and reconstruct the original prediction.
Library
Algorithm
OpenCV
TF
Pillow
Nearest
Bilinear
Bicubic
Nearest
Bilinear
Bicubic
Nearest
Median
Random
Attacks
Unmod.
Attacks
Unmod.
99.6%
100.0%
100.0%
99.6%
100.0%
100.0%
100.0%
99.0%
99.4%
99.2%
99.0%
98.9%
99.4%
99.6%
89.3%
97.7%
91.4%
88.9%
97.7%
91.7%
88.1%
89.1%
98.0%
93.4%
89.1%
97.7%
92.0%
90.4%
Table 4: Performance of defense in terms of recovering correct outputs from
the attack images, and impact on benign images.
Figure 16: Run-time performance of nearest-neighbor and area scaling as
well as our defenses in combination with nearest-neighbor scaling. Addition-
ally, a forward pass of VGG19 is shown.
5.5 Defense 2: Adaptive Attacks
Finally, it remains to investigate the robustness of the two pro-
posed defenses against an adaptive adversary who is aware
of the defenses and adapts her attack accordingly. We thus
develop two strategies that aim at misleading the image re-
construction of attack images. Both strategies attempt to ma-
nipulate the reconstruction of the pixels p ∈ P , such that they
keep their value after applying the median or random ﬁlter.
Median Filter. Our attack strategy for the median ﬁlter is as
follows: Given a window Wp around p ∈ P , we denote by m
the current median of Wp. Note that p is not part of Wp (see
Figure 10). The adversary seeks a manipulation of the pixels
in Wp, such that m = p. Hence, applying the median ﬁlter
will not change p and the adversarial modiﬁcation remains.
Without loss of generality, we assume that m < p. In order to
increase m, the adversary needs to set more pixels to the value
of p. We start with the highest pixel value that is smaller than
p and set it to p. We continue with this procedure until the
median equals p. In Appendix C, we show that this attack
strategy is optimal regarding the L0, L1, and L2 norm if the
windows Wp do not overlap. A smaller number of changes
to the image cannot ensure that m = p. These results give
a ﬁrst intuition on the robustness of the median ﬁlter. A
considerable rewriting is necessary to change the median,
even in the overlapping case where an adversary can exploit
dependencies across windows.
In our experiments, we vary the maximum fraction δ of
allowed pixel changes per window. This bound allows us to
measure the defense’s robustness depending on the L0 norm.
Random Filter. For the random ﬁlter, our attack strategy
increases the probability that the target value in a window Wp
is selected. To this end, we let the adversary set a fraction δ
of all pixels in Wp to p. To minimize the number of changes
to the image, we replace only those pixels in the window with
the smallest absolute distance to p. This strategy is optimal in
the sense that manipulation with fewer changes would result
in a lower probability for hitting the target value p.
Results. Figure 17 shows the success rate of the adaptive
attacks regarding objective O1 for OpenCV and TensorFlow.
The results for Pillow’s nearest-neighbor scaling are similar
1374    29th USENIX Security Symposium
USENIX Association
10203040506000.050.100.15DensityNearestwithmedianﬁlter102030405060Bicubicwithmedianﬁlter10203040506000.050.100.15PSNRvalueDensityNearestwithrandomﬁlter102030405060PSNRvalueBicubicwithrandomﬁlterBeforereconstructionAfterreconstructionNearestscalingAreascalingMedianﬁlterRandomﬁlterVGG19model103104105Runtime[µs]6 Related Work
Closest to our work are different attacks and defenses from
the area of adversarial machine learning [see 3, 21]. For ex-
ample, approaches for creating and detecting adversarial ex-
amples share related objectives [e.g., 4, 6, 14, 17, 23]. More-
over, techniques for manipulating machine learning models
revolve around a similar problem setting. These techniques
change training data or model parameters to obtain targeted
responses [e.g., 2, 10, 15, 34]. While not directly related,
methods for memberships and property inference [e.g., 9, 27]
as well as model inversion and extraction [e.g., 20, 33] also
constitute threats to machine learning. Our work extends this
line of research by examining the preprocessing step. We pro-
vide a comprehensive analysis of image-scaling attacks and
derive defenses for prevention. In a concurrent work [24], we
study the application for the poisoning scenario. Moreover,
we note that image-scaling attacks further bridge the gap be-
tween adversarial learning and multimedia security where the
latter also considers adversarial signal manipulations [1, 22].
Finally, image-scaling attacks differ from prior work in
two important properties: (a) The attacks affect all further
steps of a machine learning system. They are thus agnostic to
feature extraction and learning models, giving rise to general
adversarial examples and poisoning. (b) Fortunately, we can
show that the vulnerability underlying image-scaling attacks
can be effectively mitigated by defenses. This rare success
of defenses in adversarial machine learning is rooted in the
well-deﬁned structure of image scaling that fundamentally
differs from the high complexity of deep learning models.
7 Conclusion
Image-scaling attacks exploit vulnerabilities in the prepro-
cessing of machine learning with considerable impact on
computer vision. In this paper, we provide the ﬁrst in-depth
analysis of these attacks. Based on insights from this analysis,
we propose different defenses that address the root cause of
the attacks rather than ﬁxing their symptoms.
For evaluating our defenses, we consider an adaptive ad-
versary who has full knowledge about the implementation of
scaling algorithms and our defense strategy. Our empirical
results show that image-scaling attacks can be prevented effec-
tively under this threat model. The proposed defenses can be
easily combined with existing imaging libraries and require
almost no changes to machine learning pipelines. Further-
more, our ﬁndings are not limited to the considered scaling
algorithms and enable developers to vet their own scaling
techniques for similar vulnerabilities.
Overall, our work provides novel insights into the security
of preprocessing in machine learning. We believe that further
work is necessary to identify and rule out other vulnerabilities
in the different stages of data processing to strengthen the
security of learning-based systems.
Figure 17: Success rate of the adaptive attacks against defenses with respect
to objective O1. Note that O2 is not satisﬁed (see Figure 18).
and thus omitted. The adaptive attacks need to considerably
modify pixels so that the manipulated images are classiﬁed
as the target class. The median ﬁlter is robust until 40%
of the pixels in each window can be changed. Against the
random ﬁlter, a higher number of changed pixels is necessary
to increase the probability of being selected.
With respect to goal O2, both defenses withstand the adap-
tive attacks and thus remain secure. Rewriting 20% of the
pixels already inserts clear traces of manipulation, as exem-
pliﬁed by Figure 23 in Appendix D. In all cases, the attack
image is a mix between source- and target class. In addition,
Figure 18 shows the results from our user study for the me-
dian ﬁlter. The participants identify the attacks in the vast