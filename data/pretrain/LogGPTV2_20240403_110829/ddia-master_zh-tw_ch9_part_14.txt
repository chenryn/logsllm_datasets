在任何领导者被允许决定任何事情之前，必须先检查是否存在其他带有更高纪元编号的领导者，它们可能会做出相互冲突的决定。领导者如何知道自己没有被另一个节点赶下台？回想一下在 “[真相由多数所定义](ch8.md#真相由多数所定义)” 中提到的：一个节点不一定能相信自己的判断 —— 因为只有节点自己认为自己是领导者，并不一定意味著其他节点接受它作为它们的领导者。
相反，它必须从 **法定人数（quorum）** 的节点中获取选票（请参阅 “[读写的法定人数](ch5.md#读写的法定人数)”）。对领导者想要做出的每一个决定，都必须将提议值传送给其他节点，并等待法定人数的节点响应并赞成提案。法定人数通常（但不总是）由多数节点组成【105】。只有在没有意识到任何带有更高纪元编号的领导者的情况下，一个节点才会投票赞成提议。
因此，我们有两轮投票：第一次是为了选出一位领导者，第二次是对领导者的提议进行表决。关键的洞察在于，这两次投票的 **法定人群** 必须相互 **重叠（overlap）**：如果一个提案的表决透过，则至少得有一个参与投票的节点也必须参加过最近的领导者选举【105】。因此，如果在一个提案的表决过程中没有出现更高的纪元编号。那么现任领导者就可以得出这样的结论：没有发生过更高时代的领导选举，因此可以确定自己仍然在领导。然后它就可以安全地对提议值做出决定。
这一投票过程表面上看起来很像两阶段提交。最大的区别在于，2PC 中协调者不是由选举产生的，而且 2PC 则要求 **所有** 参与者都投赞成票，而容错共识演算法只需要多数节点的投票。而且，共识演算法还定义了一个恢复过程，节点可以在选举出新的领导者之后进入一个一致的状态，确保始终能满足安全属性。这些区别正是共识演算法正确性和容错性的关键。
#### 共识的局限性
共识演算法对于分散式系统来说是一个巨大的突破：它为其他充满不确定性的系统带来了基础的安全属性（一致同意，完整性和有效性），然而它们还能保持容错（只要多数节点正常工作且可达，就能取得进展）。它们提供了全序广播，因此它们也可以以一种容错的方式实现线性一致的原子操作（请参阅 “[使用全序广播实现线性一致的储存](#使用全序广播实现线性一致的储存)”）。
尽管如此，它们并不是在所有地方都用上了，因为好处总是有代价的。
节点在做出决定之前对提议进行投票的过程是一种同步复制。如 “[同步复制与非同步复制](ch5.md#同步复制与非同步复制)” 中所述，通常资料库会配置为非同步复制模式。在这种配置中发生故障切换时，一些已经提交的资料可能会丢失 —— 但是为了获得更好的效能，许多人选择接受这种风险。
共识系统总是需要严格多数来运转。这意味著你至少需要三个节点才能容忍单节点故障（其余两个构成多数），或者至少有五个节点来容忍两个节点发生故障（其余三个构成多数）。如果网路故障切断了某些节点同其他节点的连线，则只有多数节点所在的网路可以继续工作，其余部分将被阻塞（请参阅 “[线性一致性的代价](#线性一致性的代价)”）。
大多数共识演算法假定参与投票的节点是固定的集合，这意味著你不能简单的在丛集中新增或删除节点。共识演算法的 **动态成员扩充套件（dynamic membership extension）** 允许丛集中的节点集随时间推移而变化，但是它们比静态成员演算法要难理解得多。
共识系统通常依靠超时来检测失效的节点。在网路延迟高度变化的环境中，特别是在地理上散布的系统中，经常发生一个节点由于暂时的网路问题，错误地认为领导者已经失效。虽然这种错误不会损害安全属性，但频繁的领导者选举会导致糟糕的效能表现，因系统最后可能花在权力倾扎上的时间要比花在建设性工作的多得多。
有时共识演算法对网路问题特别敏感。例如 Raft 已被证明存在让人不悦的极端情况【106】：如果整个网路工作正常，但只有一条特定的网路连线一直不可靠，Raft 可能会进入领导者在两个节点间频繁切换的局面，或者当前领导者不断被迫辞职以致系统实质上毫无进展。其他一致性演算法也存在类似的问题，而设计能健壮应对不可靠网路的演算法仍然是一个开放的研究问题。
### 成员与协调服务
像 ZooKeeper 或 etcd 这样的专案通常被描述为 “分散式键值储存” 或 “协调与配置服务”。这种服务的 API 看起来非常像资料库：你可以读写给定键的值，并遍历键。所以如果它们基本上算是资料库的话，为什么它们要把工夫全花在实现一个共识演算法上呢？是什么使它们区别于其他任意型别的资料库？
为了理解这一点，简单了解如何使用 ZooKeeper 这类服务是很有帮助的。作为应用开发人员，你很少需要直接使用 ZooKeeper，因为它实际上不适合当成通用资料库来用。更有可能的是，你会透过其他专案间接依赖它，例如 HBase、Hadoop YARN、OpenStack Nova 和 Kafka 都依赖 ZooKeeper 在后台执行。这些专案从它那里得到了什么？
ZooKeeper 和 etcd 被设计为容纳少量完全可以放在记忆体中的资料（虽然它们仍然会写入磁碟以保证永续性），所以你不会想著把所有应用资料放到这里。这些少量资料会透过容错的全序广播演算法复制到所有节点上。正如前面所讨论的那样，资料库复制需要的就是全序广播：如果每条讯息代表对资料库的写入，则以相同的顺序应用相同的写入操作可以使副本之间保持一致。
ZooKeeper 模仿了 Google 的 Chubby 锁服务【14,98】，不仅实现了全序广播（因此也实现了共识），而且还构建了一组有趣的其他特性，这些特性在构建分散式系统时变得特别有用：
* 线性一致性的原子操作
  使用原子 CAS 操作可以实现锁：如果多个节点同时尝试执行相同的操作，只有一个节点会成功。共识协议保证了操作的原子性和线性一致性，即使节点发生故障或网路在任意时刻中断。分散式锁通常以 **租约（lease）** 的形式实现，租约有一个到期时间，以便在客户端失效的情况下最终能被释放（请参阅 “[程序暂停](ch8.md#程序暂停)”）。
* 操作的全序排序
  如 “[领导者和锁](ch8.md#领导者和锁)” 中所述，当某个资源受到锁或租约的保护时，你需要一个防护令牌来防止客户端在程序暂停的情况下彼此冲突。防护令牌是每次锁被获取时单调增加的数字。ZooKeeper 透过全序化所有操作来提供这个功能，它为每个操作提供一个单调递增的事务 ID（`zxid`）和版本号（`cversion`）【15】。
* 失效检测
  客户端在 ZooKeeper 伺服器上维护一个长期会话，客户端和伺服器周期性地交换心跳包来检查节点是否还活著。即使连线暂时中断，或者 ZooKeeper 节点失效，会话仍保持在活跃状态。但如果心跳停止的持续时间超出会话超时，ZooKeeper 会宣告该会话已死亡。当会话超时时（ZooKeeper 称这些节点为 **临时节点**，即 ephemeral nodes），会话持有的任何锁都可以配置为自动释放。
* 变更通知
  客户端不仅可以读取其他客户端建立的锁和值，还可以监听它们的变更。因此，客户端可以知道另一个客户端何时加入丛集（基于新客户端写入 ZooKeeper 的值），或发生故障（因其会话超时，而其临时节点消失）。透过订阅通知，客户端不用再透过频繁轮询的方式来找出变更。
在这些功能中，只有线性一致的原子操作才真的需要共识。但正是这些功能的组合，使得像 ZooKeeper 这样的系统在分散式协调中非常有用。
#### 将工作分配给节点
ZooKeeper/Chubby 模型执行良好的一个例子是，如果你有几个程序例项或服务，需要选择其中一个例项作为主库或首选服务。如果领导者失败，其他节点之一应该接管。这对单主资料库当然非常实用，但对作业排程程式和类似的有状态系统也很好用。
另一个例子是，当你有一些分割槽资源（资料库、讯息流、档案储存、分散式 Actor 系统等），并需要决定将哪个分割槽分配给哪个节点时。当新节点加入丛集时，需要将某些分割槽从现有节点移动到新节点，以便重新平衡负载（请参阅 “[分割槽再平衡](ch6.md#分割槽再平衡)”）。当节点被移除或失效时，其他节点需要接管失效节点的工作。
这类任务可以透过在 ZooKeeper 中明智地使用原子操作，临时节点与通知来实现。如果设计得当，这种方法允许应用自动从故障中恢复而无需人工干预。不过这并不容易，尽管已经有不少在 ZooKeeper 客户端 API 基础之上提供更高层工具的库，例如 Apache Curator 【17】。但它仍然要比尝试从头实现必要的共识演算法要好得多，这样的尝试鲜有成功记录【107】。
应用最初只能在单个节点上执行，但最终可能会增长到数千个节点。试图在如此之多的节点上进行多数投票将是非常低效的。相反，ZooKeeper 在固定数量的节点（通常是三到五个）上执行，并在这些节点之间执行其多数票，同时支援潜在的大量客户端。因此，ZooKeeper 提供了一种将协调节点（共识，操作排序和故障检测）的一些工作 “外包” 到外部服务的方式。