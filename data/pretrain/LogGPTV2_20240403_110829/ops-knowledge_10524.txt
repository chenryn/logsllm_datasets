User: I'm using Python to transfer (via scp) and database a large number of files.  One of the servers I transfer files to has odd ssh config rules to stop too many ssh requests from a single location.  The upshot of this is that my python script, currently looping through files and copying via os.system, hangs after a few files have been transferred.
Is there a way in which Python could open up an ssh or other connection to the server, so that each file being transferred does not require an instance of ssh login?
Thanks,
Assistant: This is not really python specific, but it probably depends on what libraries you can use.
What you need is a way to send files through a single connection. 
(This is probably better suited to superuser or severfault.com though.)

Create tarfile locally, upload it and unpack at target? 

Maybe you could even run 'tar xz' remotely and upload the file on stdin over SSH? (As MichaelDillon says in the comment, Python can create the tarfile on the fly...)

Is SFTP an option? 
Rsync over SSH?
Twisted is an async library that can handle many sockets/connections at once. Is probably overkill for your solution though,

Hope it helps.