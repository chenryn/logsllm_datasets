TIP
Network policies apply to only new connections being established. If an
incoming connection is permitted by the ingress policy rules, then any outgo-
ing traffic related to that connection will be permitted without defining indi-
vidual egress rules for each possible client.
Listing 10.19 defines a complete network policy for the H2 database. For ingress, it
defines a rule that allows connections to TCP port 9092 from pods with the label app:
natter-api. This allows the main Natter API pods to talk to the database. Because no
other ingress rules are defined, no other incoming connections will be accepted. The
policy in listing 10.19 also lists the Egress policy type but doesn’t define any egress
rules, which means that all outbound connections from the database pods will be
blocked. This listing is to illustrate how network policies work; you don’t need to save
the file anywhere.
NOTE
The allowed ingress or egress traffic is the union of all policies that
select a pod. For example, if you add a second policy that permits the data-
base pods to make egress connections to google.com then this will be allowed
even though the first policy doesn’t allow this. You must examine all policies
in a namespace together to determine what is allowed.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-network-policy
  namespace: natter-api
spec:
  podSelector:                
    matchLabels:              
      app: natter-database    
Listing 10.19
Token database network policy
Apply the policy to pods with the 
app=natter-database label.
377
Securing incoming requests
  policyTypes:
    - Ingress     
    - Egress      
  ingress:
    - from:                     
        - podSelector:          
            matchLabels:        
              app: natter-api   
      ports:                   
        - protocol: TCP        
          port: 9092           
You can create the policy and apply it to the cluster using kubectl apply, but on Mini-
kube it will have no effect because Minikube’s default networking components are not
able to enforce policies. Most hosted Kubernetes services, such as those provided by Goo-
gle, Amazon, and Microsoft, do support enforcing network policies. Consult the docu-
mentation for your cloud provider to see how to enable this. For self-hosted Kubernetes
clusters, you can install a network plugin such as Calico (https://www.projectcalico.org)
or Cilium (https://cilium.readthedocs.io/en/v1.6/).
 As an alternative to network policies, Istio supports defining network authorization
rules in terms of the service identities contained in the client certificates it uses for
mTLS within the service mesh. These policies go beyond what is supported by net-
work policies and can control access based on HTTP methods and paths. For exam-
ple, you can allow one service to only make GET requests to another service. See
http://mng.bz/4BKa for more details. If you have a dedicated security team, then ser-
vice mesh authorization allows them to enforce consistent security controls across the
cluster, allowing API development teams to concentrate on their unique security
requirements.
WARNING
Although service mesh authorization policies can significantly harden
your network, they are not a replacement for API authorization mechanisms.
For example, service mesh authorization provides little protection against the
SSRF attacks discussed in section 10.2.7 because the malicious requests will be
transparently authenticated by the proxies just like legitimate requests. 
10.4
Securing incoming requests
So far, you’ve only secured communications between microservice APIs within the
cluster. The Natter API can also be called by clients outside the cluster, which you’ve
been doing with curl. To secure requests into the cluster, you can enable an ingress
controller that will receive all requests arriving from external sources as shown in fig-
ure 10.7. An ingress controller is a reverse proxy or load balancer, and can be config-
ured to perform TLS termination, rate-limiting, audit logging, and other basic security
controls. Requests that pass these checks are then forwarded on to the services within
the network. Because the ingress controller itself runs within the network, it can be
included in the Linkerd service mesh, ensuring that the forwarded requests are auto-
matically upgraded to HTTPS.
The policy applies to both 
incoming (ingress) and 
outgoing (egress) traffic.
Allow ingress only from pods with 
the label app=natter-api-service 
in the same namespace.
Allow ingress only 
to TCP port 9092.
378
CHAPTER 10
Microservice APIs in Kubernetes
DEFINITION
A Kubernetes ingress controller is a reverse proxy or load balancer
that handles requests coming into the network from external clients. An
ingress controller also often functions as an API gateway, providing a unified
API for multiple services within the cluster.
NOTE
An ingress controller usually handles incoming requests for an entire
Kubernetes cluster. Enabling or disabling an ingress controller may therefore
have implications for all pods running in all namespaces in that cluster.
To enable an ingress controller in Minikube, you need to enable the ingress add-on.
Before you do that, if you want to enable mTLS between the ingress and your services
you can annotate the kube-system namespace to ensure that the new ingress pod that
gets created will be part of the Linkerd service mesh. Run the following two com-
mands to launch the ingress controller inside the service mesh. First run
kubectl annotate namespace kube-system linkerd.io/inject=enabled
and then run:
minikube addons enable ingress
This will start a pod within the kube-system namespace running the NGINX web
server (https://nginx.org), which is configured to act as a reverse proxy. The ingress
controller will take a few minutes to start. You can check its progress by running
the command: 
kubectl get pods -n kube-system --watch
Pod
Pod
Ingress
controller
An ingress controller acts as a gateway for external
clients. The ingress routes requests to internal services
and can terminate TLS and apply basic rate-limiting.
Figure 10.7
An ingress controller acts as a gateway for all requests from 
external clients. The ingress can perform tasks of a reverse proxy or load 
balancer, such as terminating TLS connections, performing rate-limiting, 
and adding audit logging.
379
Securing incoming requests
After you have enabled the ingress controller, you need to tell it how to route requests
to the services in your namespace. This is done by creating a new YAML configuration
file with kind Ingress. This configuration file can define how HTTP requests are
mapped to services within the namespace, and you can also enable TLS, rate-limiting,
and other features (see http://mng.bz/Qxqw for a list of features that can be enabled).
  Listing 10.20 shows the configuration for the Natter ingress controller. To allow
Linkerd to automatically apply mTLS to connections between the ingress controller and
the backend services, you need to rewrite the Host header from the external value (such
as api.natter.local) to the internal name used by your service. This can be achieved by
adding the nginx.ingress.kubernetes.io/upstream-vhost annotation. The NGINX
configuration defines variables for the service name, port, and namespace based on the
configuration so you can use these in the definition. Create a new file named natter-
ingress.yaml in the kubernetes folder with the contents of the listing, but don’t apply it
just yet. There’s one more step you need before you can enable TLS.
TIP
If you’re not using a service mesh, your ingress controller may support
establishing its own TLS connections to backend services or proxying TLS
connections straight through to those services (known as SSL passthrough).
Istio includes an alternative ingress controller, Istio Gateway, that knows how
to connect to the service mesh.
apiVersion: extensions/v1beta1
kind: Ingress                   
metadata:
  name: api-ingress         
  namespace: natter-api     
  annotations:
    nginx.ingress.kubernetes.io/upstream-vhost:                      
      "$service_name.$namespace.svc.cluster.local:$service_port"     
spec:
  tls:                          
    - hosts:                    
        - api.natter.local      
      secretName: natter-tls    
  rules:                                        
    - host: api.natter.local                    
      http:                                     
        paths:                                  
          - backend:                            
              serviceName: natter-api-service   
              servicePort: 4567                 
To allow the ingress controller to terminate TLS requests from external clients, it
needs to be configured with a TLS certificate and private key. For development, you
can create a certificate with the mkcert utility that you used in chapter 3:
mkcert api.natter.local
Listing 10.20
Configuring ingress
Define the Ingress 
resource.
Give the ingress rules a name 
in the natter-api namespace.
Rewrite the Host
header using the
upstream-vhost
annotation.
Enable TLS by providing 
a certificate and key.
Define a route to direct 
all HTTP requests to the 
natter-api-service.
380
CHAPTER 10
Microservice APIs in Kubernetes
This will spit out a certificate and private key in the current directory as two files with
the .pem extension. PEM stands for Privacy Enhanced Mail and is a common file for-
mat for keys and certificates. This is also the format that the ingress controller needs.
To make the key and certificate available to the ingress, you need to create a Kubernetes
secret to hold them.
DEFINITION
Kubernetes secrets are a standard mechanism for distributing pass-
words, keys, and other credentials to pods running in a cluster. The secrets
are stored in a central database and distributed to pods as either filesystem
mounts or environment variables. You’ll learn more about Kubernetes secrets
in chapter 11.
To make the certificate available to the ingress, run the following command:
kubectl create secret tls natter-tls -n natter-api \
  --key=api.natter.local-key.pem --cert=api.natter.local.pem
This will create a TLS secret with the name natter-tls in the natter-api name-
space with the given key and certificate files. The ingress controller will be able to
find this secret because of the secretName configuration option in the ingress config-
uration file. You can now create the ingress configuration to expose the Natter API to
external clients:
kubectl apply -f kubernetes/natter-ingress.yaml
You’ll now be able to make direct HTTPS calls to the API:
$ curl https://api.natter.local/users \
  -H 'Content-Type: application/json' \
  -d '{"username":"abcde","password":"password"}'
{"username":"abcde"}
If you check the status of requests using Linkerd’s tap utility, you’ll see that requests
from the ingress controller are protected with mTLS:
$ linkerd tap ns/natter-api
req id=4:2 proxy=in  src=172.17.0.16:43358 dst=172.17.0.14:4567 
➥ tls=true :method=POST :authority=natter-api-service.natter-
➥ api.svc.cluster.local:4567 :path=/users
rsp id=4:2 proxy=in  src=172.17.0.16:43358 dst=172.17.0.14:4567
➥ tls=true :status=201 latency=322728µs
You now have TLS from clients to the ingress controller and mTLS between the ingress
controller and backend services, and between all microservices on the backend.4
4 The exception is the H2 database as Linkerd can’t automatically apply mTLS to this connection. This should
be fixed in the 2.7 release of Linkerd.
381
Summary
TIP
In a production system you can use cert-manager (https://docs.cert-
manager.io/en/latest/) to automatically obtain certificates from a public
CA such as Let’s Encrypt or from a private organizational CA such as Hashi-
corp Vault.
Answers to pop quiz questions
1
c. Pods are made up of one or more containers.
2
False. A sidecar container runs alongside the main container. An init container
is the name for a container that runs before the main container.
3
a, b, c, d, and f are all good ways to improve the security of containers.
4
e. You should prefer strict allowlisting of URLs whenever possible.
5
d and e. Keeping the root CA key offline reduces the risk of compromise and
allows you to revoke and rotate intermediate CA keys without rebuilding the
whole cluster.
6
True. A service mesh can automatically handle most aspects of applying TLS to
your network requests.
7
a, b, c, and d.
Summary
 Kubernetes is a popular way to manage a collection of microservices running
on a shared cluster. Microservices are deployed as pods, which are groups of
related Linux containers. Pods are scheduled across nodes, which are physical
or virtual machines that make up the cluster. A service is implemented by one
or more pod replicas.
 A security context can be applied to pod deployments to ensure that the con-
tainer runs as a non-root user with limited privileges. A pod security policy can be
applied to the cluster to enforce that no container is allowed elevated privileges.
 When an API makes network requests to a URL provided by a user, you should
ensure that you validate the URL to prevent SSRF attacks. Strict allowlisting of
permitted URLs should be preferred to blocklisting. Ensure that redirects are
Pop quiz
7
Which of the following are tasks are typically performed by an ingress controller?
a
Rate-limiting
b
Audit logging
c
Load balancing
d
Terminating TLS requests
e
Implementing business logic
f
Securing database connections
The answer is at the end of the chapter.
382
CHAPTER 10
Microservice APIs in Kubernetes
also validated. Protect your APIs from DNS rebinding attacks by strictly validat-
ing the Host header and enabling TLS.
 Enabling TLS for all internal service communications protects against a variety
of attacks and limits the damage if an attacker breaches your network. A service
mesh such as Linkerd or Istio can be used to automatically manage mTLS con-
nections between all services.
 Kubernetes network policies can be used to lock down allowed network com-
munications, making it harder for an attacker to perform lateral movement
inside your network. Istio authorization policies can perform the same task
based on service identities and may be easier to configure.
 A Kubernetes ingress controller can be used to allow connections from external
clients and apply consistent TLS and rate-limiting options. By adding the ingress
controller to the service mesh you can ensure connections from the ingress to
backend services are also protected with mTLS.
383
Securing
service-to-service APIs
In previous chapters, authentication has been used to determine which user is
accessing an API and what they can do. It’s increasingly common for services to talk
to other services without a user being involved at all. These service-to-service API
calls can occur within a single organization, such as between microservices, or
between organizations when an API is exposed to allow other businesses to access
data or services. For example, an online retailer might provide an API for resellers
to search products and place orders on behalf of customers. In both cases, it is the
API client that needs to be authenticated rather than an end user. Sometimes this is
needed for billing or to apply limits according to a service contract, but it’s also
essential for security when sensitive data or operations may be performed. Services
are often granted wider access than individual users, so stronger protections may
This chapter covers
 Authenticating services with API keys and JWTs
 Using OAuth2 for authorizing service-to-service 
API calls
 TLS client certificate authentication and 
mutual TLS
 Credential and key management for services
 Making service calls in response to user requests
384
CHAPTER 11
Securing service-to-service APIs
be required because the damage from compromise of a service account can be
greater than any individual user account. In this chapter, you’ll learn how to authenti-
cate services and additional hardening that can be applied to better protect privileged
accounts, using advanced features of OAuth2.
NOTE
The examples in this chapter require a running Kubernetes installa-
tion configured according to the instructions in appendix B.
11.1
API keys and JWT bearer authentication
One of the most common forms of service authentication is an API key, which is a sim-
ple bearer token that identifies the service client. An API key is very similar to the
tokens you’ve used for user authentication in previous chapters, except that an API
key identifies a service or business rather than a user and usually has a long expiry
time. Typically, a user logs in to a website (known as a developer portal) and generates an
API key that they can then add to their production environment to authenticate API
calls, as shown in figure 11.1.
api.example.com
developers.example.com
client.foo.com
Request access
API key
API key
GET/accounts?api_key=....
A developer requests access to the
API from the developer portal.
The portal generates an API key
that is sent on API requests to
authenticate the client.
Developer
Figure 11.1
To gain access to an API, a representative of the organization 
logs into a developer portal and requests an API key. The portal generates the 
API key and returns it. The developer then includes the API key as a query 
parameter on requests to the API.
385
The OAuth2 client credentials grant
Section 11.5 covers techniques for securely deploying API keys and other credentials.
The API key is added to each request as a request parameter or custom header.
DEFINITION
An API key is a token that identifies a service client rather than a
user. API keys are typically valid for a much longer time than a user token,
often months or years.
Any of the token formats discussed in chapters 5 and 6 are suitable for generating API
keys, with the username replaced by an identifier for the service or business that
API usage should be associated with and the expiry time set to a few months or years
in the future. Permissions or scopes can be used to restrict which API calls can be
called by which clients, and the resources they can read or modify, just as you’ve done
for users in previous chapters—the same techniques apply.
 An increasingly common choice is to replace ad hoc API key formats with standard
JSON Web Tokens. In this case, the JWT is generated by the developer portal with
claims describing the client and expiry time, and then either signed or encrypted with
one of the symmetric authenticated encryption schemes described in chapter 6. This
is known as JWT bearer authentication, because the JWT is acting as a pure bearer token:
any client in possession of the JWT can use it to access the APIs it is valid for without
presenting any other credentials. The JWT is usually passed to the API in the Authori-
zation header using the standard Bearer scheme described in chapter 5.
DEFINITION
In JWT bearer authentication, a client gains access to an API by pre-
senting a JWT that has been signed by an issuer that the API trusts.
An advantage of JWTs over simple database tokens or encrypted strings is that you can
use public key signatures to allow a single developer portal to generate tokens that
are accepted by many different APIs. Only the developer portal needs to have access
to the private key used to sign the JWTs, while each API server only needs access to
the public key. Using public key signed JWTs in this way is covered in section 7.4.4,
and the same approach can be used here, with a developer portal taking the place of
the AS.
WARNING
Although using JWTs for client authentication is more secure than