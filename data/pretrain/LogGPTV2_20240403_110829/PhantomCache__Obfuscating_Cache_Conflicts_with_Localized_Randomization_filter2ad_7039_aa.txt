title:PhantomCache: Obfuscating Cache Conflicts with Localized Randomization
author:Qinhan Tan and
Zhihua Zeng and
Kai Bu and
Kui Ren
PhantomCache: Obfuscating Cache Conﬂicts
with Localized Randomization
Qinhan Tan∗
Zhejiang University
PI:EMAIL
Zhihua Zeng∗
Zhejiang University
PI:EMAIL
Kai Bu(cid:63)
Zhejiang University
PI:EMAIL
Kui Ren
Zhejiang University
PI:EMAIL
Abstract—Cache conﬂicts due to deterministic memory-to-
cache mapping have long been exploited to leak sensitive infor-
mation such as secret keys. While randomized mapping is fully
investigated for L1 caches, it still remains unresolved about how
to secure a much larger last-level cache (LLC). Recent solutions
periodically change the mapping strategy to disrupt the crafting
of conﬂicted addresses, which is a critical attack procedure to
exploit cache conﬂicts. Remapping, however, increases both miss
rate and access latency. We present PhantomCache for securing
an LLC with remapping-free randomized mapping. We propose a
localized randomization technique to bound randomized mapping
of a memory address within only a limited number of cache sets.
The small randomization space offers fast set search over an
LLC in a memory access. The intrinsic randomness still sufﬁces
to obfuscate conﬂicts and disrupt efﬁcient exploitation of con-
ﬂicted addresses. We evaluate PhantomCache against an attacker
exploring the state-of-the-art attack with linear-complexity. To
secure an 8-bank 16 MB 16-way LLC, PhantomCache conﬁnes
randomization space of an address within 8 sets and brings
only 1.20% performance degradation on individual benchmarks,
0.50% performance degradation on mixed workloads, and 0.50%
storage overhead per cache line, which are 2x and 9x more efﬁ-
cient than the state-of-the-art solutions. Moreover, PhantomCache
is solely an architectural solution and requires no software change.
I.
INTRODUCTION
Cache conﬂicts have long been exploited to leak sensitive
information such as secret keys. Although memory isolation
among processes can be enforced for the sake of security,
caches are still shared among different processes. A cache
access of a process may cause a cache hit or miss to another
process accessing the cache. A cache hit and a cache miss
differ in access latency, the timing of which can be observed
by an attacker so as to infer the access behavior of a victim
process. Consider the AES algorithm for example. A single bit
of the secret key may determine whether a branch is executed.
This leads to different cache access behaviors and therefore
different cache timings. Through a timing side channel attack,
∗Qinhan Tan and Zhihua Zeng have contributed equally and are considered
to be co-ﬁrst authors.
(cid:63)Kai Bu is the corresponding author.
Network and Distributed Systems Security (NDSS) Symposium 2020
23-26 February 2020, San Diego, CA, USA
ISBN 1-891562-61-4
https://dx.doi.org/10.14722/ndss.2020.24086
www.ndss-symposium.org
an attacker can infer secret AES keys in use [5], [19], [26],
[33], [44], [48].
Last level caches (LLCs) are the main target of conﬂict-
based cache timing attacks. This is because an LLC is shared
among all cores and it is difﬁcult to prevent cache conﬂicts
between attacker processes and victim processes in the LLC.
This sharing feature of LLCs also brings severe risk of
information leakage in web applications and cloud services. For
example, an attacker is able to use unprivileged Javascript code
to obtain secret information of a victim [33]. When the victim’s
browser runs the Javascript code, the attack is launched and the
victim’s cache access behaviors are monitored. The snooped
access behaviors then can be used to infer secret information.
Moreover, cache conﬂicts in an LLC also allow attackers
to build robust cache covert channels in cloud environment.
Maurice et al. [30] succeeded in building LLC-based covert
communication channels between processes on Amazon EC2.
The covert channel supports a transmission rate of more than
45 KBps and a 0% error rate even in the presence of high
system activity.
Most countermeasures against conﬂict-based cache timing
attacks [6], [15], [23], [28], [39], [41], [42], [46] either fall
short of strong security or sacriﬁce system functionality. For
example, detection-based solutions [8], [9], [13], [46], [47] may
simply consider frequent cache misses of speciﬁc addresses
as suspicious and trigger an event alarm. Such solutions,
however, are usually threshold based and vulnerable to false
negatives. To prevent conﬂict-based attacks, cache partition [23],
[41], [42] and fuzzy time [39] break the premises of cache
sharing and time measurements, respectively. While throttling
attacks, they affect system functionality in terms of lower cache
space utilization and inaccurate time measurements for normal
processes.
As a fundamental countermeasure, randomized mapping
aims to break deterministic cache conﬂicts by randomly
mapping a memory block to a cache location [25], [42],
[43]. This method does not reduce cache conﬂicts. Instead,
it improves the difﬁculty of observing and exploiting cache
conﬂicts. Once the placement policy is randomized, the memory-
to-cache mapping for any address is not ﬁxed. In this scenario,
the attacker can hardly ﬁnd and exploit addresses that may
conﬂict with the victim addresses. This is because with the
existence of randomness, the attacker cannot create cache
conﬂicts with certainty. While efﬁcient randomized-mapping
on small L1 caches have been fully investigated [24], [25],
[42], [43], it still awaits efﬁcient implementation on a much
larger last-level cache (LLC). The major challenge is how to
guarantee fast lookup for a block that may be randomly mapped
to anywhere in a large LLC. It is impractical to enforce a full-
scale search across the entire LLC. Previous solutions for LLCs
resort to indirect randomized mapping. They ﬁrst introduce
implicit mapping to lengthen the time for ﬁnding conﬂicted
addresses. Then they conduct dynamic remapping to change the
mapping strategy from time to time so as to bring randomness.
They, however, have recently been found insecure against
the state-of-the-art attacking algorithm with linear complexity
[35], [40], [45]. To prevent the state-of-the-art attack, these
countermeasures need an extremely frequent remapping, which
will cause unacceptable performance overhead [35].
Skewed-cache based designs provide a partial solution to
the above problem. Such designs scatter the cache lines in a
cache set into different cache partitions, and adopt random
placement among these partitions [35], [45]. The uncertainty
in memory-to-cache mapping increase the cost of the attack,
because an attacker needs to repeat the old procedures for
many times to succeed with high probability. However, since
the total number of possible cache locations of a physical
address is not enhanced, these designs still need the inefﬁcient
dynamic remapping. To secure a 2 MB 16 way LLC against
the linear-complexity attack, the state-of-the-art ScatterCache
[45] brings 2% slowdown and 5% storage overhead per cache
line, requiring both hardware and software changes.
In this paper, we present PhantomCache, an LLC-favorable
countermeasure against conﬂict-based cache timing attacks
without inefﬁcient dynamic remapping. It leverages our newly
proposed localized randomization to mitigate conﬂict-based
cache timing attacks. In contrast with global randomization,
localized randomization restricts the randomization space to
a small range of cache locations. Each time a memory block
enters the cache, it is randomly placed at a location from its
ﬁxed mapping range. Because the mapping range is small,
all of the locations in it can be checked in parallel during a
cache access to search for the needed memory block. This is
more practical and efﬁcient than global randomized mapping.
Although the randomness is limited, PhantomCache is still
sufﬁcient to prevent conﬂict-based cache timing attacks.
On a 16 MB 16-way LLC where PhantomCache maps an
address to one of 8 random locations, our analysis shows that an
attacker using the linear-complexity attack algorithm needs 500+
years to succeed. PhantomCache achieves such strong security
without dynamic remapping. This is because PhantomCache
preserves a sufﬁciently large randomization space by randomly
mapping an address across several sets from the entire LLC.
In contrast, the number of possible mapping locations for an
address by skewed-cache based solutions is equal to the number
of locations in only one set of the undivided cache. This is
why they need dynamic remapping to preserve security at the
cost of efﬁciency [35], [45].
In summary, we make the following contributions to
mitigating conﬂict-based cache timing attacks.
• We propose the localized randomization technique that
can protect large caches (such as LLCs) against conﬂict-
based cache timing attacks without inefﬁcient dynamic
remapping (Section III).
• We explore a series of hardware-efﬁcient design
strategies to realize localized randomization in Phan-
Fig. 1.
Prime+Probe attack [26], [30] infers (a) the victim access upon a
cache miss and (b) the victim no-access upon all cache hits in a 4-way cache
set.
tomCache. The mapping function to realize localized
randomization only imposes one clock cycle latency
per cache access (Section IV). Our analysis shows that
these efﬁcient design strategies do not compromise the
security (Section V).
• We propose an efﬁcient design to integrate Phan-
tomCache into the multi-banked LLC architecture
(Section IV-E). This improves parallelism and reduces
the overhead of cache access.
• We implement PhantomCache using the ChampSim
simulator and validate its performance using extensive
SPEC CPU 2017 benchmarks. We evaluate Phantom-
Cache implementation on an 8-bank 16 MB 16 way
LLC. It brings only 1.20% performance degradation on
individual benchmarks, 0.50% performance degrada-
tion on mixed workloads, and 0.50% storage overhead
per cache line, which are 2x and 9x more efﬁcient
than the state-of-the-art skewed-cache based solutions.
Moreover, PhantomCache requires no software change
(Section VII-F).
II. PROBLEM
In this section, we review the basics of conﬂict-based
cache timing attacks and underline the inefﬁciency of prior
countermeasures based on randomized mapping.
A. Conﬂict-based Cache Timing Attack
Conﬂict-based cache timing attacks exploit cache conﬂicts
to reveal the cache access behavior of processes [40]. To infer
whether the victim has accessed a memory address, the attacker
need craft another memory address that maps to the same cache
line with the memory address of interest. The attacker then
periodically accesses the crafted address. The ﬁrst access makes
the corresponding data block cached. For the next access, a
cache hit indicates that the cached block is not replaced. The
attacker makes sure that the victim has not accessed the memory
address of interest in between its two accesses. Otherwise, a
cache miss occurs and it reveals the memory access behavior of
the victim. Since modern processors use set associative caches,
the attacker can only deduce which set a memory address maps
to rather than the exact cache line. Therefore, the attacker has
to craft a set of addresses that map to an entire cache set, which
is called an eviction set. Periodical access and timing inference
then involve all or most of the addresses in the eviction set.
2
AttackerVictim1. Prime2. Accesshitmisshithit3. Probe(a)AttackerVictim1. Prime2. No Accesshithithithit3. Probe(b)Fig. 2. Memory address format: virtual address and physical address.
Prime+Probe [26], [30], a representative conﬂict-based
attack, exercises eviction addresses through ﬁrst priming and
then probing a cache set. During the wait interval in between,
the victim may or may not access the cache set. We enumerate
both cases in Figure 1. In Figure 1(a), the attacker ﬁrst primes
the entire cache set by accessing addresses in the eviction
set. While the attacker waits, the victim accesses a memory
address that maps to the primed cache set. This leads to
a cache replacement over, for example, the second cache
line in Figure 1(a). When the wait interval terminates, the
attacker probes the cache set by accessing eviction addresses
again. Since the second cache line is replaced by the victim,
the attacker encounters a cache miss there and infers the
access action of the victim. In contrast, if the victim has not
accessed the cache set (Figure 1(b)), the attacker experiences
all cache hits upon probing and infers the victim’s non-access
action. Note that Evict+Reload [16], [22] jointly exploits cache
conﬂicts and shared blocks between the attacker and victim
processes. It is therefore usually handled by countermeasures
involving shared memory, say forbidding shared memory
between Virtual Machines (VMs) [38].
B. Minimal Eviction Set
A successful conﬂict-based attack relies on a minimal
eviction set that satisﬁes two properties—identical mapping
and minimal cardinality [26], [40]. First, identical mapping
requires that all eviction addresses map to the same cache
set. As shown in Figure 1, if some eviction addresses map
to other cache sets, they may still cause cache misses to the
attacker even though the victim does not access the example
cache set. This fails the Prime+Probe attack. Second, minimal
cardinality requires that the number of eviction addresses be
minimized to set associativity. An eviction-set cardinality less
than set associativity is insufﬁcient for priming an entire
cache set. On the other hand, if the eviction set contains
more addresses than a cache set can hold, they lead to cache
conﬂicts among themselves and cause cache misses irrelevant
to the victim access. This fails the Prime+Probe attack as
well. To satisfy identical mapping and minimal cardinality, the
algorithmic essence for ﬁnding a minimal eviction set features
two respective core building blocks [40]. One is to initialize an
eviction set with candidate addresses that likely map to the same
cache set. The other is to iteratively remove addresses whose
absence will not make the remaining eviction set ineffective.
Candidate address sampling. For sampling initial candidate
addresses, the attacker exploits the deterministic memory-to-
cache mapping strategy on modern processors. That is, the
index bits of a physical address are directly used as the index
of the cache set that the address maps to. What makes it
exploitable is that part or even all of the index bits can be
overlapped by a virtual address and its corresponding physical
address. As shown in Figure 2, a virtual address consists of
a frame number and a page offset while its corresponding
3
physical address consists of a tag, index bits, and a line offset.