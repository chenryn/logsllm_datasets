1https://bit.ly/2wjR2bb
Figure 3: Implementation of GAA’s policy as a general recur-
rent neural network.
p0(s0)∏T
t=1 p(st|st−1,αt−1)π(αt−1|st−1).
n , . . . , 1
In the context of RL, the objective above has been in-
tensively studied and various mature algorithms such as
policy gradient descent [54] or Q-learning [57] have been
proposed to solve it. We expect our GAA can be seam-
lessly fused into the learning process of the underlying
model with a similar behavior as statistical GARs. There-
fore, we propose to approximately model the chained term
t=1 p(st|st−1,αt−1)π(αt−1|st−1) in the joint probability den-
∏T
sity with a general Recurrent Neural Network (RNN [27,59]).
The full computational graph of our proposed implementation
is illustrated in Fig. 3. Starting from the initial state s0 ∼ p0
and initial action α0 := ( 1
n ), we formulate the auxil-
iary RNN as follows ∀t ∈ {0, . . . ,T −1},αt+1 = hψ(st+1,αt ),
where hψ denotes certain recurrent unit with parameter ψ,
with its range as a subset of Sn. Practically, such a condition
can be easily realized with a softmax layer [10]. For details,
please see Section 5.1.
Therefore, the optimization objective of GAA is refor-
mulated as minψ Es0∼p0[∑T−1
t=0 γt ( ˆfB(θt+1)− ˆfB(θt ))], where
θt is uniquely determined with the update rule conditioned
on αt−1 and θt−1. By expansion of ˆfB, we can formulate
the ﬁnal optimization objective of GAA in episode i as
t=0 γt ∑z∈B f (θt+1,z)− f (θt ,z), where θ0 is initial-
minψ
ized randomly while α0 in episode i always inherits value
from αT in episode i− 1. Our learning algorithm is listed in
Algorithm 1.
S ∑T−1
1
4.4 Analytical Results
In this part, we present theoretical evidence on Byzantine
robustness of distributed learning with GAA when the Byzan-
tine ratio is ﬁxed or ﬂuctuates with uncertainty. Please note
in the following analysis we focus on the empirical version
of f on the training set, as the omitted leap from our proved
results to f is guaranteed by standard results in generalization
theory [58]. For the same reason, we maintain the notation
f for its empirical version. We assume the loss function f
is convex and η-smooth with pointwise bounded gradient
(cid:107)∇ f(cid:107)2 ≤ M. For non-convex objective, our results can be ex-
1646    29th USENIX Security Symposium
USENIX Association
4.4.2 Provable Robustness with a Fluctuated Byzantine
Ratio.
As a comparison, Byzantine worker detection with GAA
can be conducted in a more natural way.
Theorem 2. After t steps of gradient descent with GAA when
the Byzantine ratio ﬂuctuates randomly other than 1, Algo-
rithm 1 yields a parameter θt s.t.
f (θt )− f (θ∗) <
2RM + M√
t
+
SηR2
t +
√
2(cid:107) f(cid:107)∞
where R is the diameter of parameter space.
(cid:112)KL(Pm||D)
(2)
Corollary 2. Speciﬁcally, if Pm = D a.e., the learning pro-
cess will asymptotically converge to the global optimum with
convergence rate O(1/
√
t).
Intuitively, Theorem 2 suggests, although there is still a
guarantee for GAA to attain the sub-optimum in this case, the
error term on the right of (2) is independent from β and is
slightly larger than the one in (1). It is mainly because GAA
in this case would pose all its credit on one single worker that
is never compromised and therefore the distributed learning
system degrades to a single-noded version when Byzantine
ratio ﬂuctuates. Similarly, Corollary 2 proves the convergence
of GAA in this more challenging case when a golden-labeled
validation set is available.
4.5 Byzantine Worker Detection & Behavior
Analysis
In principle, when a policy is learned on how to determine
an optimal action αt according to the current state st and
the historical information, our GAA is expected to master a
good knowledge of the undertaking Byzantine attacks. Gen-
erally speaking, since the action proposed by our GAA is
always constrained in Sn, it is therefore reasonable to view
each component of αt as the credit on the corresponding
worker. Speciﬁcally, we present its application in detection
and behavioral pattern analysis of Byzantine workers below.
4.5.1 Byzantine Worker Detection. When the Byzantine
ratio is ﬁxed, accurate detection of Byzantine workers can
help accelerate the learning process by eliminating potential
Byzantine workers at an early stage. Therefore, we suggest
detection algorithms should aim at selecting K most suspi-
cious workers at iteration t. Although most statistical GARs
are not directly applicable for detection tasks, we ﬁnd one
exception is GeoMed, for which we provide a straightforward
extension as follows.
Procedure 1 (GeoMed+). Given Qt = {V t
Step 1. Initialize Ot = {}
Step 2. Ot ← i∗ := argmaxi∈{1,...,n} ∑V t
Step 3. Qt ← Qt\{V t
i∗}
Step 4. If |Ot| = K, output Ot. Otherwise, go to Step 2.
n},
1, . . . ,V t
i −V t
j(cid:107)
j∈Qt (cid:107)V t
Procedure 2 (GAA+).
Step 1. Find K smallest coordinate of αt.
Step 2. Output the corresponding index set as Ot
4.5.2 Byzantine Behavior Analysis. When the Byzantine
ratio ﬂuctuates with unknown patterns, detecting temporal
characteristics is a much more challenging task compared
with the aforementioned case. Barely any previous statistical
GARs can be adapted for addressing this task due to their lack
of interpretability, while our proposed GAA can be applied
directly for Byzantine behavior analysis with visualizations.
In this case, we can visualize the policy sequence {αt} to
understand the temporal patterns of Byzantine attacks. A con-
crete demonstration on a situation when the Byzantine ratio
ﬂuctuates periodically is presented in Section 6.5.
5 Overview of Evaluations
5.1 Overall Settings
5.1.1 Benchmark Systems. We build GAA into the dis-
tributed learning process of four benchmark systems for text
and image classiﬁcation listed in Table 2. On MNIST and
CIFAR-10, each worker shares a copy of the training set, while
on Yelp and Healthcare, each worker has its local dataset. In
all the cases, the loss function f is set as the cross entropy loss
between the prediction of classiﬁer g and the ground-truth.
More details are provided in Appendix A.3.
Table 2: Summary of the benchmark systems.
Model
Task
# Samples
# Parameters
# Workers
MNIST
MLP
Hand-Written Digits
(10-class)
60k
(Shared)
25,450
50
CIFAR-10
ResNet-18
Objects
(10-class)
60k
(Shared)
11,173,962
Yelp [1]
MLP
Sentiment
(2-class)
Healthcare [2]
MLP
Disease
(10-class)
20k per worker
(Local)
10,272
20k per worker
(Local)
33,130
50
10
50
5.1.2 Attacking Patterns. We consider the following three
attack patterns of the adversary.
• Static Attack: All the βn compromised workers play the
role of Byzantine workers during the whole learning pro-
cess.
• Pretense Attack: In this case, the βn manipulated workers
pretend to be benign in the ﬁrst L rounds and start the attack
from the (L + 1)-th round.
• Randomized Attack: At beginning, each compromised
worker (βn in total) is assigned with its role ri(0) by the
adversary. During the learning process, it changes its role
with a probability q at a period of p rounds.
USENIX Association
29th USENIX Security Symposium    1647
It is worth to notice, the ﬁrst pattern is a realization for the
case in Section 4.4.1, when the Byzantine ratio is ﬁxed over
time, while the pretense and randomized attacks correspond
to the setting in Section 4.4.2 when the Byzantine ratio ﬂuc-
tuates with or without uncertainty. Moreover, the latter two
patterns are designed as adaptive attacks on the RL mecha-
nism adopted by GAA. Both randomized attack and pretense
attack attempt to mislead GAA into making wrong credit as-
signments, by letting the manipulated workers pretend to be
benign and submit normal gradients in a certain time span of
the learning process.
5.1.3 Tampering Algorithms. In experiments, we evaluate
the impact of two realizations of the tampering algorithm T .
• Random Fault (RF) [11]. For RF, Byzantine workers sub-
mit noisy gradients sampled from a multi-dimensional
Gaussian N (µ,σ2I). In our experiments, we take µ =
(0.5, . . . ,0.5) ∈ Rd and σ = 2× 10−6.
• Adaptive Fault (AF). For AF, we consider an adversary
has some knowledge of the quasi-validation set, which al-
lows the manipulated workers to submit well-crafted gra-
dients that can tempt GAA to assign them with high cred-
its and meanwhile maximize the overall training loss. We
provide the details on the implementation of this fault in
Section 6.3.
5.1.4 Implementation Details of GAA. We implement the
recurrent unit hψ of GAA in the following experiments as
a fully connected, feed-forward neural network with no hid-
den layer, with an input layer of size (3n + 2)× d (i.e., the
dimension of concatenation of st and αt) and an output layer
of size d with softmax activation. For other common hyper-
parameter settings in Algorithm 1, we set the learning rate λ
as 0.05, discount factor γ as 0.9, the episode length T as 5,
the number of episode N as 5. Each benign worker computes
the gradient on randomly sampled mini-batch of size 64 for
MNIST & CIFAR-10 and 256 for Yelp & Healthcare.
5.1.5 Choice of the Quasi-Validation Set B. For MNIST
and CIFAR-10, we set the quasi-validation set as a random
mini-batch of training samples. For Yelp and Healthcare, we
implement the quasi-validation set as a small subset of sam-
ples from similar data domains. On Yelp, each worker holds
20k restaurants’ reviews (randomly selected from the raw
restaurant reviews) from one of the 10 US states with the
most recorded Yelp reviews (including Arizona, Illinois and so
on). We randomly sampled 1k reviews from South California,
which is not in the top-10 states, as the full quasi-validation
set. On Healthcare, each worker holds 20k treatment descrip-
tions from local hospitals in one of the 50 different states,
while we use a subset of descriptions from Alaska as the full
quasi-validation set, which contains 1k records in total. For
all our experiments on Yelp and Healthcare, we use less than
10 random samples from the full quasi-validation set as the
working quasi-validation set.
5.2 Summary of Results
We highlight some experimental ﬁndings below.
• Robustness - GAA effectively defends the 4 benchmark
systems against 3 attacking patterns and 2 tampering algo-
rithms, with a wide range of conﬁgurations. It helps the
underlying systems achieve comparable performance in
limited rounds as if the systems were not under attacks.
• Efﬁciency - The time efﬁciency of GAA is on a similar scale
with previous statistical defenses.
• Interpretablity - A well-trained GAA provides informa-
tive and interpretable traces that can be used for Byzantine
worker detection and behavior pattern visualization.
6 Results & Analysis
6.1 Robustness against Static Attacks
Figure 4: Test accuracy of the benchmark systems under static
attacks when different defenses are applied up to a ﬁxed
round.
6.1.1 Comparison with Baselines. We compare the Byzan-
tine robustness of our proposed GAA with 6 baselines under
static attacks with RF: (A) Classical GAR (B) Brute-Force
(C) GeoMed (D) Krum (E) Bulyan and (F) Classical GAR
without attack. We include the last baseline for measuring the
degradation of each method under attacks. We set the Byzan-
tine ratio β in the static attack as 0.2, 0.5, 0.7, where 0.2 is a
tolerable Byzantine ratio for all the baselines and 0.5 corre-
sponds to the breaking point of the baselines. Fig. 4 shows
the ﬁnal test accuracy of the four benchmark systems with
different defenses equipped, up to 5k,10k,20k,40k rounds re-
spectively. As Bulyan is not executable when n ≥ 4m + 3, the
corresponding result is not collected when β ≥ 0.5. Moreover,
Brute-Force on MNIST, CIFAR-10 & Healthcare and Bulyan
on CIFAR-10 fail to ﬁnish the learning in 10 days due to the
high time complexity (we provide evaluations in Section 6.1.2
and Table 1), the corresponding results are not reported.
Results & Analysis. As we can see from Fig. 4, when the
Byzantine ratio is as small as 0.2, each baseline method is ob-
served to be Byzantine robust, which conforms to the reported
results in previous works [31]. In this case, our GAA also
1648    29th USENIX Security Symposium
USENIX Association
Figure 5: Learning curves of GAA against randomized attacks and pretense attacks.
helps the underlying model achieve a similar test accuracy.
Noticeably, the robustness of our GAA is strongly demon-
strated by its comparable performance to classical GAR with-
out attack, when the Byzantine workers are in majority. For
example, as the β = 0.5 cases represent the breaking point
of Brute-Force, Krum and GeoMed, on Yelp the benchmark
systems with the baseline defenses perform no better than a
random guesser, while GAA helps the system achieve over
80% accuracy, which is very close to the 84.5% accuracy
when the system is under no attack. A similar phenomenon
was observed even when we further enlarge the Byzantine
ratio to 0.7. These results imply GAA does complement the
existing defenses when the Byzantine ratio is larger than 0.5.
6.1.2 Time Efﬁciency. We measure the time cost of our de-
fense and provide a tentative comparison with previous de-
fenses. We run the four benchmark systems with different
defenses under the same static attack in the previous part and
record the time cost of 100 iterations with 10 repetitions in
the same environment described in Appendix A.1. Table 3
lists the running time of different defenses in each case. As