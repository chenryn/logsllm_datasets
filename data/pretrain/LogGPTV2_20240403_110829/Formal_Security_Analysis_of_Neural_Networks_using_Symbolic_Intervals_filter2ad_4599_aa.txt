title:Formal Security Analysis of Neural Networks using Symbolic Intervals
author:Shiqi Wang and
Kexin Pei and
Justin Whitehouse and
Junfeng Yang and
Suman Jana
Formal Security Analysis of Neural Networks 
using Symbolic Intervals
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang,  
and Suman Jana, Columbia University
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Formal Security Analysis of Neural Networks using Symbolic Intervals
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana
Columbia University
Abstract
Due to the increasing deployment of Deep Neural Net-
works (DNNs) in real-world security-critical domains
including autonomous vehicles and collision avoidance
systems, formally checking security properties of DNNs,
especially under different attacker capabilities, is becom-
ing crucial. Most existing security testing techniques for
DNNs try to ﬁnd adversarial examples without providing
any formal security guarantees about the non-existence
of such adversarial examples. Recently, several projects
have used different types of Satisﬁability Modulo Theory
(SMT) solvers to formally check security properties of
DNNs. However, all of these approaches are limited by
the high overhead caused by the solver.
In this paper, we present a new direction for formally
checking security properties of DNNs without using SMT
solvers. Instead, we leverage interval arithmetic to com-
pute rigorous bounds on the DNN outputs. Our approach,
unlike existing solver-based approaches, is easily paral-
lelizable. We further present symbolic interval analysis
along with several other optimizations to minimize over-
estimations of output bounds.
We design, implement, and evaluate our approach as
part of ReluVal, a system for formally checking security
properties of Relu-based DNNs. Our extensive empirical
results show that ReluVal outperforms Reluplex, a state-
of-the-art solver-based system, by 200 times on average.
On a single 8-core machine without GPUs, within 4 hours,
ReluVal is able to verify a security property that Reluplex
deemed inconclusive due to timeout after running for
more than 5 days. Our experiments demonstrate that
symbolic interval analysis is a promising new direction
towards rigorously analyzing different security properties
of DNNs.
1
Introduction
In the last ﬁve years, Deep Neural Networks (DNNs) have
enjoyed tremendous progress, achieving or surpassing
human-level performance in many tasks such as speech
recognition [19], image classiﬁcations [30], and game
playing [46]. We are already adopting DNNs in security-
and mission-critical domains like collision avoidance and
autonomous driving [1, 5]. For example, unmanned Air-
craft Collision Avoidance System X (ACAS Xu), uses
DNNs to predict best actions according to the location and
the speed of the attacker/intruder planes in the vicinity. It
was successfully tested by NASA and FAA [2, 33] and is
on schedule to be installed in over 30,000 passengers and
cargo aircraft worldwide [40] and US Navy’s ﬂeets [3].
Unfortunately, despite our increasing reliance on
DNNs, they remain susceptible to incorrect corner-case
behaviors: adversarial examples [48], with small, human-
imperceptible perturbations of test inputs, unexpectedly
and arbitrarily change a DNN’s predictions. In a security-
critical system like ACAS Xu, an incorrectly handled
corner case can easily be exploited by an attacker to cause
signiﬁcant damage costing thousands of lives.
Existing methods to test DNNs against corner cases
focus on ﬁnding adversarial examples [7, 16, 31, 32, 37,
39, 41, 42, 51] without providing formal guarantees about
the non-existence of adversarial inputs even within very
small input ranges. In this paper, we focus on the problem
of formally checking that a DNN never violates a security
property (e.g., no collision) for any malicious input pro-
vided by an attacker within a given input range (e.g., for
attacker aircraft’s speeds between 0 and 500 mph).
Due to non-linear activation functions like ReLU, the
general function computed by a DNN is highly non-linear
and non-convex. Therefore it is difﬁcult to estimate the
output range accurately. To tackle these challenges, all
prior work on the formal security analysis of neural net-
works [6,12,21,25] rely on different types of Satisﬁability
Modulo Theories (SMT) solvers and are thus severely lim-
ited by the efﬁciency of the solvers.
We present ReluVal, a new direction for formally check-
ing security properties of DNNs without using SMT
solvers. Our approach leverages interval arithmetic [45] to
compute rigorous bounds on the outputs of a DNN. Given
the ranges of operands (e.g., a1 ∈ [0,1] and a2 ∈ [2,3]),
interval arithmetic computes the output range efﬁciently
using only the lower and upper bounds of the operands
(e.g., a2 − a1 ∈ [1,3] because 2− 1 = 1 and 3− 0 = 3).
Compared to SMT solvers, we found interval arithmetic
to be signiﬁcantly more efﬁcient and ﬂexible for formal
USENIX Association
27th USENIX Security Symposium    1599
analysis of a DNN’s security properties.
Operationally, given an input range X and security prop-
erty P, ReluVal propagates it layer by layer to calculate
the output range, applying a variety of optimization to
improve accuracy. ReluVal ﬁnishes with two possible
outcomes: (1) a formal guarantee that no value in X vi-
olates P (“secure”); and (2) an adversarial example in X
violating P (“insecure”). Optionally, ReluVal can also
guarantee that no value in a set of subintervals of X vi-
olates P (“secure subintervals”) and that all remaining
subintervals each contain at least one concrete adversarial
example of P (“insecure subintervals”).
A key challenge in ReluVal is the inherent overestima-
tion caused by the input dependencies [8, 45] when in-
terval arithmetic is applied to complex functions. Specif-
ically, the operands of each hidden neuron depend on
the same input to the DNN, but interval arithmetic as-
sumes that they are independent and may thus compute
an output range much larger than the true range. For
example, consider a simpliﬁed neural network in which
input x is fed to two neurons that compute 2x and −x
respectively, and the intermediate outputs are summed to
generate the ﬁnal output f (x) = 2x− x. If the input range
of x is [0,1], the true output range of f (x) is [0,1]. How-
ever, naive interval arithmetic will compute the range of
f (x) as [0,2]− [0,1] = [−1,2], introducing a huge over-
estimation error. Much of our research effort focuses on
mitigating this challenge; below we describe two effective
optimizations to tighten the bounds.
First, ReluVal uses symbolic intervals whenever possi-
ble to track the symbolic lower and upper bounds of each
neuron. In the preceding example, ReluVal tracks the
intermediate outputs symbolically ([2x,2x] and [−x,−x]
respectively) to compute the range of the ﬁnal output
as [x,x]. When propagating symbolic bound constraints
across a DNN, ReluVal correctly handles non-linear func-
tions such as ReLU and calculates proper symbolic upper
and lower bounds. It concretizes symbolic intervals when
needed to preserve a sound approximation of the true
ranges. Symbolic intervals enable ReluVal to accurately
handle input dependencies, reducing output bound estima-
tion errors by 85.67% compared to naive extension based
on our evaluation.
Second, when the output range of the DNN is too large
to be conclusive, ReluVal iteratively bisects the input
range and repeats the range propagation on the smaller
input ranges. We term this optimization iterative interval
reﬁnement because it is in spirit similar to abstraction
reﬁnement [4, 18]. Interval reﬁnement is also amenable
to massive parallelization, an additional advantage of Re-
luVal over hard-to-parallelize SMT solvers.
Mathematically, we prove that interval reﬁnement on
DNNs always converges in ﬁnite steps as long as the DNN
is Lipschitz continuous which is true for any DNN with
ﬁnite number of layers. Moreover, lower values of Lips-
chitz constant result in faster convergence. Stable DNNs
are known to have low Lipschitz constants [48] and there-
fore the interval reﬁnement algorithm can be expected
to converge faster for such DNNs. To make interval re-
ﬁnement even more efﬁcient, ReluVal uses additional
optimizations that analyze how each input variable inﬂu-
ences the output of a DNN by computing each layer’s
gradients to input variables. For instance, when bisecting
an input range, ReluVal picks the input variable range that
inﬂuences the output the most. Further, it looks for input
variable ranges that inﬂuence the output monotonically,
and uses only the lower and upper bounds of each such
range for sound analysis of the output range, avoiding
splitting any of these ranges.
We implemented ReluVal using around 3,000 line of
C code. We evaluated ReluVal on two different DNNs,
ACAS Xu and an MNIST network, using 15 security prop-
erties (out of which 10 are the same ones used in [25]).
Our results show that ReluVal can provide formal guar-
antees for all 15 properties, and is on average 200 times
faster than Reluplex, a state-of-the-art DNN veriﬁer using
a specialized solver [25]. ReluVal is even able to prove
a security property within 4 hours that Reluplex [25]
deemed inconclusive due to timeout after 5 days. For
MNIST, ReluVal veriﬁed 39.4% out of 5000 randomly
selected test images to be robust against up to |X|∞ ≤ 5
attacks.
This paper makes three main contributions.
• To the best of our knowledge, ReluVal is the ﬁrst
system that leverages interval arithmetic to provide
formal guarantees of DNN security.
• Naive application of interval arithmetic to DNNs is
ineffective. We present two optimizations – sym-
bolic intervals and iterative reﬁnement – that signif-
icantly improve the accuracy of interval arithmetic
on DNNs.
• We designed, implemented, evaluated our techniques
as part of ReluVal and demonstrated that it is on
average 200× faster than Reluplex, a state-of-the-art
DNN veriﬁer using a specialized solver [25].
2 Background
2.1 Preliminary of Deep Learning
A typical feedforward DNN can be thought of as a
function f : X → Y mapping inputs x ∈ X (e.g., im-
ages, texts) to outputs y ∈ Y (e.g., labels for image
classiﬁcation, texts for machine translation). Speciﬁ-
cally, f is composed of a sequence of parametric func-
tions f (x;w) = fl( fl−1(··· f2( f1(x;w1);w2)···wl−1),wl),
1600    27th USENIX Security Symposium
USENIX Association
where l denotes the number of layers in a DNN, fk de-
notes the corresponding transformation performed by k-
th layer, and wk denotes the weight parameters of k-th
layer. Each fk∈1,...l performs two operations: (1) a lin-
ear transformation of its input (i.e., either x or the output
from fk−1) denoted by wk · fk−1(x), where f0(x) = x and
fk(cid:5)=0(x) is the output of fk denoting intermediate output
of layer k while processing x, and (2) a nonlinear trans-
formation σ (wk · fk−1(x)) where σ is the nonlinear acti-
vation function. Common activation functions include
sigmoid, hyperbolic tangent, or ReLU (Rectiﬁed Linear
Unit) [38]. In this paper, we focus on DNNs using ReLU
(Relu(x) = max(0,x)) as the activation function as it is
one of the most popular ones used in the modern state-of-
the-art DNN architectures [17, 20, 47].
2.2 Threat Model
Target system. In this paper, we consider all types of
security-critical systems, e.g., airborne collision avoid-
ance system for unmanned air-crafts like ACAS Xu [33],
which use DNNs for decision making in the presence
of an adversary/intruder. DNNs are becoming increas-
ingly popular in such systems due to better accuracy and
less performance overhead than traditional rule-based sys-
tems [24]. For example, an aircraft collision avoidance
system’s decision making process can use DNNs to pre-
dict the best action based on sensor data of the current
speed and course of the aircraft, those of the adversary,
and distances between the aircraft and nearby intruders.
Figure 1: The DNN in the victim aircraft (ownship)
should predict a left turn (upper ﬁgure) but unexpect-
edly advises to turn right and collide with the intruder
(lower ﬁgure) due to the presence of adversarial inputs
(e.g., if the attacker approaches at certain angles).
Security properties. In this paper, we focus on input-
output-based security properties of DNN-based systems
that ensure the correct action in the presence of adversar-
ial inputs within a given range. Input-output properties are
well suited for the DNN-based systems as their decision
logic is often opaque even to their designers. Therefore,
unlike traditional programs, writing complete speciﬁca-
tions involving internal states is often hard.
For example, consider a security property that tries
to ensure that a DNN-based car crash avoidance system
predicts the correct steering angle in the presence of an
approaching attacker vehicle: it should steer left if the
attacker approaches it from right. In this setting, even
though the ﬁnal decision is easy to predict for humans,
the correct outputs for the internal neurons are hard to
predict even for the designer of the DNN.
Attacker model. We assume that the inputs an adver-
sary can provide are bounded within an interval speciﬁed
by a security property. For example, an attacker aircraft
has a maximum speed (e.g., it can only move between 0
and 500 mph). Therefore, the attacker is free to choose
any value within that range. This attacker model is, in
essence, similar to the ones used for adversarial attacks
on vision-based DNNs where the attacker aims to search
for visually imperceptible perturbations (within certain
bound) that, when applied on the original image, makes
the DNN predict incorrectly. Note that, in this setting, the
imperceptibility is measured using a Lp norm. Formally,
given a computer vision DNN f , the attacker solves fol-
lowing optimization problem: min(Lp(x(cid:6) − x)) such that
f (x) (cid:5)= f (x(cid:6)), where Lp(·) denotes the p-norm and x(cid:6) − x
is the perturbation applied to original input x. In other
words, the security property of a vision DNN being robust
against adversarial perturbations can be deﬁned as: for
any x(cid:6) within a L-distance ball of x in the input space,
f (x) = f (x(cid:6)).
Unlike the adversarial images, we extend the attacker
model to allow different amount of perturbations to dif-
ferent features. Speciﬁcally, instead of requiring overall
perturbations on input features to be bounded by L-norm,
our security properties allow different input features to
be transformed within different intervals. Moreover, for
DNNs where the outputs are not explicit labels, unlike
adversarial image, we do not require the predicted label
to remain the same. We support properties specifying
arbitrary output intervals.
An example. As shown in Figure 1, normally, when the
distance (one feature of the DNN) between the victim
ship (ownship) and the intruder is large, the victim ship
advisory system will advise left to avoid the collision
and then advise right to get back to the original track.
However, if the DNN is not veriﬁed, there may exist one
speciﬁc situation where the advisory system, for certain
approaching angles of the attacker ship, advises the ship
incorrectly to take a right turn instead of left, leading to a
USENIX Association
27th USENIX Security Symposium    1601
fatal collision. If an attacker knows about the presence of
such an adversarial case, he can speciﬁcally approach the
ship at the adversarial angle to cause a collision.
Distance from