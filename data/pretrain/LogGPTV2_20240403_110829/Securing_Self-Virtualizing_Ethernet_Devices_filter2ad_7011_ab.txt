trafﬁc using the same pause frame structure. PFC oper-
ates on individual trafﬁc classes, as deﬁned by Annex I
of the IEEE 802.1Q standard [7]. Up to 8 trafﬁc classes
can be deﬁned for PFC per link.
3.4 Attacking VMs via Flow Control
Direct device assignment enables malicious guests to
attack the Ethernet network via well-known Layer 2
attacks [14, 17, 47, 56, 73, 75].
Even when using
virtualization-aware switching extensions such as the
Virtual Edge Port Aggregator (VEPA) [30,31], all guests
with direct access to the VFs of the same PF still share
the same physical link to the edge switch, and the edge
switch still allocates processing resources per link.
Since both 802.3x and 802.1Qbb perform ﬂow control
on a link-level basis, and the link is shared between VMs,
any ﬂow control manipulation by a single VM will affect
the PF and all VFs associated with this PF. This means
that a malicious VM is capable of controlling the band-
width and latency of all VMs that share the same adapter.
The malicious VM can pause all trafﬁc on the link by
sending 802.3x pause frames and can stop a speciﬁc traf-
ﬁc class by sending 802.1Qbb pause frames. To stop all
trafﬁc on a 10 Gbps Ethernet link, an attacker needs to
transmit pause frames at a rate of 300 frames/second,
which is about 155 Kbps of bandwidth. The attacker can
fully control the bandwidth and latency of all tenant VMs
with minimal required resources and without any coop-
eration from the host or from other guest VMs.
4 Attack Evaluation
4.1 Experimental Setup
We constructed a lab setup in which we perform and
evaluate the ﬂow-control attack described in the previous
section. We use a Dell PowerEdge R420 server, which is
a dual socket with six cores per socket, with Intel Xeon
E5-2420 CPUs running at 1.90GHz. The chipset is the
Intel C600 series. The server includes 16GBs of mem-
ory and an SRIOV-capable Intel NIC (10GbE 82599 or
1GbE I350) installed in PCIe generation 3 slots with two
VFs enabled.
We use the KVM Hypervisor [50] and Ubuntu server
13.10 with 3.11.0 x86 64 kernel for the host, guest VMs,
and the client. Each guest is created with 2GBs of mem-
ory, two virtual CPUs, and one VF directly assigned to it.
Client and host machines are identical servers connected
to the same dedicated switch, as shown in Figure 3.
To achieve consistent results, the server’s BIOS proﬁle
is performance optimized, all power optimizations are
tuned off, and Non-Uniform Memory Access (NUMA) is
enabled. The guest virtual CPUs are pinned to the cores
on the same NUMA node to which the Intel PF is con-
nected. The host allocates to the guest memory from the
same NUMA node as well.
For our 1GbE environment, we use an Intel Ethernet
I350-T2 network interface connected to a Dell Power-
Connect 6224P 1Gb Ethernet switch. For our 10GbE
environment, we use an Intel 82599 10 Gigabit TN net-
work interface connected to an HP 5900AF 10Gb Ether-
net switch.
Host and client use their distribution’s default drivers
with default conﬁguration settings. Guest VMs use ver-
sion 2.14.2 of the ixgbevf driver for the Intel 10G
82599 Ethernet controller virtual function and the default
igbvf version 2.0.2-k for the Intel 1G I350 Ethernet
controller virtual function. Ethernet ﬂow control IEEE
802.3x is enabled on switch ports. We set the Ethernet
Maximal Transfer Unit (MTU) to 1500 bytes on all Eth-
ernet switches and network interfaces in our tests.
4.2 Benchmark Methodology
We conduct a performance evaluation according to the
methodology in RFC 2544 [25]. For throughput tests,
we use an Ethernet frame size of 1518 bytes and measure
maximal throughput without packet loss. Each through-
put test runs for at least 60 seconds and we take the aver-
age of 5 test cycles. To measure latency, we use 64 and
1024 byte messages. Each latency test runs at least 120
seconds and we measure the average of at least 15 test
cycles. (While RFC 2544 dictates running 20 cycles, we
obtained plausible results after 15 cycles; thus, we de-
cided to reduce test runtime by running each test only 15
cycles.)
Benchmark Tools: We measure throughput and la-
tency with two well-known network benchmark utilities:
iperf [3] and netperf [45]. We use the iperf TCP
stream test to measure throughput and the netperf
TCP RR test to measure latency. The iperf and
netperf clients are run on the client machine, while
USENIX Association  
24th USENIX Security Symposium  339
host
SRIOV NIC
VF1
Figure 3: Setup scheme
the iperf and netperf servers are run on VM1. We
measure on the client the bandwidth and latency from the
client to VM1.
Trafﬁc Generators: In addition to the trafﬁc gener-
ated by the benchmark tools, we use tcpdump [44] to
capture trafﬁc and tcpreplay [5] to send previously
captured and modiﬁed frames at the desired rate.
Testbed Scheme: The testbed scheme is shown in Fig-
ure 3. Our testbed consists of two identical servers, one
acting as client and the other as the host with SRIOV ca-
pable NIC. We conﬁgure two VFs on the host’s SRIOV
PF. We assign VF1 to guest VM1 and VF2 to guest
VM2. Client and host are connected to the same Ethernet
switch. We generate trafﬁc between VM1 and the client
using iperf and netperf. VM2 is the attacking VM.
4.3 Flow-Control Attack Implementation
it
We use tcpreplay [5] to send specially crafted 802.3x
pause frames at the desired rate from the malicious
VM2.1 When the switch receives a pause frame from
VM2,
inhibits transmission of any trafﬁc on the
link between the switch and the PF, including the traf-
ﬁc between the client and VM1, for a certain num-
ber of pause time quanta. Sending pause frames
from VM2, we can manipulate the bandwidth and la-
tency of the trafﬁc between VM1 and the client. The
value of pause time of each pause frame is 0xFFFF
pause quanta units. Knowing the link speed, we can
calculate the pause frame rate, as described in Section 3,
and impose precise bandwidth limits and latency delays
on VM1. The results of the attack in both 1GbE and
10GbE environments are presented in Section 4.4.
1 We use 802.3x pause frames for the sake of simplicity, but we
could have used PFC frames instead. PFC uses exactly the same ﬂow
control mechanism and has the same MAC control frame format. The
only difference between PFC frames and pause frames is the addition
of seven pause time ﬁelds in PFC that are padded in 802.3x frames.
4.4 Attack Results
Figures 4 and 5 show the results of the pause frame at-
tack on victim throughput in the 1GbE and 10GbE en-
vironments respectively. Figures 4a and 5a show victim
(VM1) throughput under periodic attack of VM2. Every
10 seconds, VM2 transmits pause frames for 10 seconds
at 30 frames/second (as shown in Figure 4a) and at 300
frames/second (as shown in Figure 5a). In this test we
measure the throughput of the victim system, VM1. The
ﬁgures clearly show that VM2 can gain complete control
over VM1 throughput: starting from the tenth second,
the attacker completely stops trafﬁc on the link for ten
seconds.
Figure 6 shows the results of the pause frame attack
on victim latency in the 10GbE environment. Figure 6a
shows victim latency under the same periodic attack de-
scribed above. In this test we use 64B and 1024B mes-
sages. For better result visualization, we lowered the at-
tack rate to 150 pause frames/second. Figure 6a shows
that the attacker can increase victim latency to 250% by
running the attack at a rate of only 150 frames/second.
Victim throughput Figures 4b and 5b display
throughput of VM1 as a function of the rate of pause
frames VM2 sends. From Figure 4b we can see that
VM2 can pause all trafﬁc on the 1GbE link with al-
most no effort, by sending pause frames at a rate of 30
frames/second. For the 10GbE link, VM2 needs to work
a little bit harder and raise its rate to 300 frames/second.
This test’s results conﬁrm the calculations shown in Ta-
ble 1. Figures 7a and 7b conﬁrm that the measured vic-
tim throughput is exactly as predicted. In other words, it
is easily and completely controlled by the attacker.
These tests show that a malicious VM can use the
pause frame attack to control the throughput of other
VMs with precision. Furthermore, we see that the pause
frame attack requires minimal effort from the attacker
and will be hard to detect amid all the other network
trafﬁc. To halt all transmissions on the 10GbE link,
the attacker only needs to send 64B pause frames at
300 frames/second. 300 frames/second is approximately
0.002% of the 14.88 million frames/second maximum
frame rate for 10GbE.2 Discovering such an attack can
be quite challenging, due to the low frame rate involved,
especially on a busy high-speed link such as 10GbE or
40GbE.
Victim latency Figure 6b shows the victim’s latency
as a function of the attacker’s pause frame rate. In this
test we measure the latency of 64 byte messages and
1024 byte messages. We see that the ﬁgures for both 64B
2 The maximum frame rate equals the link speed divided by the sum
of sizes of the preamble, frame length and inter-frame gap.
340  24th USENIX Security Symposium 
USENIX Association
1000
800
600
400
200
0
r
e
d
n
u
t
u
p
h
g
u
o
r
h
t
m
i
t
c
v
i
]
s
/
b
M
[
s
k
c
a
t
t
a
c
d
o
i
r
e
p
i
10
8
6
4
2
0
r
e
d
n
u
t
u
p
h
g
u
o
r
h
t
m
i
t
c
v
i
]
s
/
b
G
[
s
k
c
a
t
t
a
i
c
d
o
i
r
e
p
t
u
p
h
g
u
o
r
h
t
m
i
t
c
v
i
]
s
/
b
M
[
1000
800
600
400
200
0
 0
 50
 100
 150
 200
 250
 300
pause frames attacker sends
 each second [frames/second]
(b)
 0
 10
 20
 30
 40
 50
 60
 70
time [seconds]
(a)
Figure 4: Pause frame attack: victim throughput in 1GbE environment
t
u
p
h
g
u
o
r
h
t
m
i
t
c
v
i
]
s
/
b
G
[
10
8
6
4
2
0
 0
 100
 200
 300
pause frames attacker sends
 each second [frames/second]
(b)
 10
 20
 30
 40
 50
 60
 70
time [seconds]
(a)
Figure 5: Pause frame attack: victim throughput in 10GbE environment
r
e
d
n
u
y
c
n
e
t
a
l
m
i
t
c
v
i
]
s
µ
[
k
c
a