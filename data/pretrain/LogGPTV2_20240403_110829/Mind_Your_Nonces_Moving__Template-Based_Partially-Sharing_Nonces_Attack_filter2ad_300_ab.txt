1
2(cid:96)
610void GetTRN(BYTE *rand, WORD len)
{
WORD i;
open RNG clock ;
for (i = 0; i  n then this k is discarded4 and the random
number generation procedure is carried out again, otherwise
this k is used as the nonce. Note that for the recommend-
ed parameters in [20], n is large enough5 which means that
with very low probability (≤ 2−32) k will be discarded.
Note that the line:
rand[i] = RNGDATA;
4Note that if this case happens then in our attack the cor-
responding power trace should be discarded as well.
5n=0xFFFFFFFE FFFFFFFF FFFFFFFF FFFFFFFF 7203DF6B
21C6052B 53BBF409 39D54123
moves the random number from the register RNGDATA to a
variable rand[i] in RAM, and this process may leak informa-
tion about the moving data due to the MOV instruction.
3.2 Building the Templates and Finding the
Partial Collisions of Nonces
Note that our target smartcard supports the APDU (Ap-
plication Protocol Data Unit) for random number generation
which also calls the function GetTRN and outputs the ran-
dom numbers. This is the case for most of the real world
smartcards as well, for example in the PIN veriﬁcation appli-
cation of EMV speciﬁcations [7], a get-challenge command
should be sent from the terminal to the smartcard by an
APDU. The smartcard then generates the random number
and returns it to the terminal as the challenge. To make
our template attack more powerful, i.e., to avoid the strong
assumption that the attacker knows the nonces during the
template building phase, the templates will be built by send-
ing the random number generation APDU instead of the
signing APDU, to the smartcard. The reason for doing this
is that the random numbers, which are needed to build tem-
plates, will be returned from the smartcard, thus no known
secrets are required.
In order to specify the time interval corresponding to the
MOV instructions, we send the APDU that generates 8 bytes
of random numbers for several thousand times and record
the power traces corresponding to the power consumption of
the random numbers generation; one of these power traces
is illustrated in the top ﬁgure of Fig. 1. Then we align all
the traces according to the part of the ﬁrst random byte
and calculate the correlation coeﬃcient between the power
consumption and the data, see the bottom of Fig. 1. We
can see that the points of power traces between 13.6µs to
14.1µs have a relatively strong correlation with the data.
Zooming in this part, we get Fig. 2, and we believe that the
MOV instruction falls in this part.
We also try to align the power traces with the other bytes,
however these bytes show lower correlation than the ﬁrst and
thus the ﬁrst byte of the random number generation is used
to mount our attack.6
Now the problems we need to handle are how to decide
the template building and matching strategies.
First, although the MOV instruction leaks the Hamming
weight due to the precharged bus, we cannot distinguish the
values with the same Hamming weight. Fortunately, there
are only one value corresponding to the Hamming weights 0
6Our earlier experiments tried to detect the collisions of the
MSBN by measuring the distances or correlation coeﬃcients
directly from the power consumption patterns (i.e., without
templates), but ended up with failures due to the low success
rate caused by noise.
0100200300400500600700−1001020304050611and 8, respectively. Thus, we only choose the two templates
that corresponds to 0x00 and 0xFF to build.
Second, the partially-sharing(known) nonces attack is a
lattice attack that does not tolerate mistakes [17], usually
the attack fails due to a single long vector. However, due to
the noise, the values are diﬃcult to be recovered without
false alarm. A simple example is that two power traces
correspond to the same value (e.g. two power traces with
the common value 0x00 are on the top in Fig. 3) might have
a more signiﬁcant diﬀerence than those with diﬀerent values
(e.g. two power traces with values 0x00 and 0x01 are on the
bottom in Fig. 3). These will result in incorrectly estimating
the values that are transferred, and the corresponding vector
in the lattice might be too long. To overcome this obstacle,
our solution is two-folds:
– A signal processing technique, i.e. PCA, is applied to
the original traces to amplify the diﬀerence of the power con-
sumption between values with diﬀerent Hamming weights.
In [1], Archambeau et al. also proposed to apply PCA to the
traces before the template attack, with the purpose of choos-
ing the most interesting points which beneﬁt the template
attack. Diﬀerent from their scenario, we have more traces
than the number of points to be considered. As a conse-
quence, we use the original routine as described in Sect. 2.2
to get better results. Moreover, PCA is used in a diﬀerent
way from that of [1]. In [1], PCA is applied inter-class (as
stated in [24]), i.e., the mean traces obtained according to
diﬀerent values are used to calculate the covariance matrix
ˆΣ(cid:48) and the traces to be matched are also transformed ac-
cording to the eigendecompose of this matrix. However, in
our case, we calculate the covariance also intra-class, that is,
we apply PCA to the traces before averaging. In this way,
the noise distribution could be better used. For the traces to
be matched, an independent PCA is applied instead of using
the eigenvectors of the template building phase, in order to
better character the distribution of these traces.
– The templates are used in a way that is diﬀerent from
the classical manner as that in [5]. In [5], the templates cor-
responding to all the values (or all Hamming weights) are
built, and the target traces are matched with the templates
by the maximum-likelihood principle to decide the value (or
Hamming weight) of the secret. Note that in the case of
stream ciphers or block ciphers, normally the secret key will
keep the same each time we do the encryption. Thus the at-
tacker can use multiple matching traces with the same value
to enhance the success rate. However, in our case, the nonce
varies each time so that we have only a single trace to match
for each value. Remember that we only use two templates;
thus we look through the traces to ﬁnd the matched value
for each of the two templates, instead of searching over the
templates for each trace. Moreover, we decide not to use the
maximum-likelihood principle that are used commonly in
the template attack. Instead, we use an opposite approach;
the “minimum-likelihood principle” is our decision principle
that detects the values with the lowest Bayesian probabili-
ty. The reason is that in the experiment we found that the
success rate is higher in this case. Speciﬁcally, for the power
traces after signal processing, we match them with each of
the two templates corresponding to 0x00 and 0xFF. Then we
sort the traces with the log-likelihood and choose the mini-
mum ones. Usually, we can set a threshold and keep only the
ones below this threshold; these traces are expected to have
the values that are the same as (when the precharged values
Figure 3: Overlap of two
traces (top: traces with the
same value; bottom: traces
with diﬀerent values)
Figure 4: Correlation co-
eﬃcient (top) and standard
deviation (bottom)
of the template building and matching phases are diﬀerent)
or the complement of (when the precharged values of the
template building and matching phases are the same) that
of the templates. The signatures chosen according to the
same template can then be paired to be used in the lattice
attack.
Following the above technique, we now describe the de-
tailed steps of our template building and matching phases.
Template Building Phase.
1. Keep on sending the APDU for random number genera-
tion to the smartcard and sort the recorded traces according
to the ﬁrst byte of the random numbers. For 1,800,000 ran-
dom numbers, we obtain about 7,000 traces for each of the
256 values.
2. Align all the kept traces and then apply PCA to them.
In order to enhance the eﬃciency, PCA should be only
applied to the “sensitive points” in the traces that are corre-
lated to the values. This can be easily done by calculating
the correlation coeﬃcients between the values and the trace
points, since we know the random numbers. However, this
might cause a problem that in the matching phase, when one
has to do the same transformation, i.e. PCA, to the corre-
sponding part of the analyzed traces, he has no idea about
how to choose the proper points due to the unknown nonces.
As an alteration, we will calculate the standard deviations
of each point of the template traces, and try to ﬁnd the re-
lation between the standard deviations and the correlation
coeﬃcients (see Fig. 4). We know from Fig. 4 that the peak
of the standard deviations appears at the same point as that
of the correlation coeﬃcients.
Then we select a number of points before and after the
peak of the standard deviation, to which PCA is applied. See
Fig. 5 for the power traces of values with diﬀerent Hamming
weights after PCA, which shows more signiﬁcant diﬀerence
than those before PCA.
3. For the traces corresponding to the values 0x00 and 0xFF
after PCA transformation, two templates are built by select-
ing a certain number of points with high variances. Since the
eigendecomposition used in our PCA has sorted the eigen-
values, we select the points on the right hand side whose
variances are large.
Template Matching Phase. In the matching phase, the
APDU that corresponds to the signing command is sent to
the smartcard, a power trace of the whole signing procedure
is shown in Fig. 6. The part on the power trace that corre-
sponds to the generation and transfer of k is marked; in the
very beginning of this part, we found a very similar interval
as that in Fig. 2. We believe that this is the part that we
0100200300400500600700−2002040600100200300400500600700−2002040600100200300400500600700−101time010020030040050060070005106122 , sd
1 , sd
1), (rd
2)),··· ,
If we can obtain d pairs of signatures (((r1
((rd
2))) with collided MSBN, then they all fulﬁll
Eq. (5); the recovery of private key dA can be seen as an
2 − si
HNP problem. Denote ti = si
(i = 1,··· , d).
2, ui = si
1 − ri
1 − si
1), (r1
2 + ri
1, s1
2, s1
1
In our experiments, we use embedding method as shown
in Sect. 2.3 to solve the CVP. To avoid the fraction in com-
putation and balance the values in coordinates, one needs to
modify the coeﬃcients of the row matrix (1) by some scaling
factor. As a result, we construct a CVP in the lattice below:
1732 · n
0
...