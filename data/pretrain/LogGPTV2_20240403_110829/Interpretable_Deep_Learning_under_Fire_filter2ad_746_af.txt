interpreters is consistent with that reported in [10], with slight
variation due to the difference of underlying DNNs.
Interpreter
Measure
GRAD
43.1
CAM MASK
43.8
45.2
RTS
34.2
Table 2. Performance of the interpreters in this paper in a weakly
semi-supervised localization task (with ResNet as the classiﬁer).
Attacks – We implement all the variants of ADV2 in § 3 on
the PGD framework. In addition, we also implement ADV2 on
a spatial transformation framework (STADV) [2]. We compare
ADV2 with regular PGD [35], a universal ﬁrst-order adversar-
ial attack. For both ADV2 and PGD, we assume the setting
of targeted attacks, in which the adversary attempts to force
the DNNs to misclassify the adversarial inputs into randomly
designated classes. The parameter settings of all the attacks
are summarized in Appendix B.
Q1. Attack Effectiveness (Prediction)
We ﬁrst evaluate the effectiveness of ADV2 in terms of
deceiving target DNNs. The effectiveness is measured using
attack success rate, which is deﬁned as
Attack Success Rate (ASR) =
#successful trials
#total trials
and misclassiﬁcation conﬁdence (MC), which is the probabil-
ity assigned by the DNN to the target class ct.
P
A
100% 100% 98% 100% 100% 100% 96% 100%
(0.99)
(1.0)
(0.99)
(0.98)
(1.0)
(1.0)
(0.98)
(1.0)
Table 3. Effectiveness of PGD (P) and ADV2 (A) against different
classiﬁers and interpreters in terms of ASR (MC).
Table 3 summarizes the attack success rate and misclassi-
ﬁcation conﬁdence of ADV2 and PGD against different com-
binations of classiﬁers and interpreters. Note that as PGD is
only applied on the classiﬁer, its effectiveness is agnostic to
the interpreters. To make fair comparison, we ﬁx the max-
imum number of iterations as 1,000 for both attacks. It is
GRAD CAM MASK RTS GRAD CAM MASK RTS
ResNet
100% (1.0)
DenseNet
100% (1.0)
1664    29th USENIX Security Symposium
USENIX Association
observed that ADV2 achieves high success rate (above 95%)
and misclassiﬁcation conﬁdence (above 0.98) across all the
cases, which is comparable with the regular PGD attack. We
thus have the following conclusion.
Observation 1
Despite its dual objectives, ADV2 is as effective as
regular adversarial attacks in deceiving target DNNs.
Q2. Attack Effectiveness (Interpretation)
Next we evaluate the effectiveness of ADV2 in terms of gen-
erating similar interpretations to benign inputs. Speciﬁcally,
we compare the interpretations of benign and adversarial in-
puts, which is crucial for understanding the security implica-
tions of using interpretability as a means of defenses [13, 57].
Due to the lack of standard metrics for interpretation plausi-
bility, we use a variety of measures in our evaluation.
Figure 5: Average L1 distance between benign and adversarial (PGD,
ADV2) attribution maps.
ing that different interpreters may inherently feature varying
robustness against ADV2. (iii) The effectiveness of ADV2
seems insensitive to the underlying DNN. On both ResNet
and DenseNet, it achieves similar L1 measures.
IoU Test – Another quantitative measure for the similar-
ity of attribution maps is the intersection-over-union (IoU)
score. It is widely used in object detection [21] to compare
model predictions with ground-truth bounding boxes. For-
mally, the IoU score of a binary-valued map m with respect
to a baseline map m◦ is deﬁned as their Jaccard similarity:
IoU(m) = |O(m)∩ O(m◦)|/|O(m)∪ O(m◦)|, where O(m) de-
notes the set of non-zero dimensions in m. In our case, as the
values of attribution maps are ﬂoating numbers, we ﬁrst apply
threshold binarization on the maps.
Figure 4: Attribution maps of benign and adversarial (PGD, ADV2)
inputs with respect to GRAD, CAM, MASK, and RTS on ResNet.
Visualization – We ﬁrst qualitatively compare the interpre-
tations of benign and adversarial (PGD, ADV2) inputs. Fig-
ure 4 show a set of sample inputs and their attribution maps
with respect to GRAD, CAM, MASK, and RTS (more samples
in Appendix C1). Observe that in all the cases, the ADV2
inputs generate interpretations perceptually indistinguishable
from their benign counterparts. In comparison, the PGD inputs
are easily identiﬁable by inspecting their attribution maps.
LLLppp Measure – Besides qualitatively comparing the attribu-
tion maps of benign and adversarial inputs, we also measure
their similarity quantitatively. By considering attribution maps
as matrices, we measure the L1 distance between benign and
adversarial maps. Figure 5 summarizes the results (other Lp
measures in Appendix C1). For comparison, we normalize
all the measures to [0,1] by dividing them by the number of
pixels.
We have the following observations. (i) Compared with
PGD, ADV2 generates attribution maps much more similar
to benign cases. The average L1 measure of ADV2 is more
than 60% lower than PGD across all the interpreters. (ii) The
effectiveness of ADV2 varies with the target interpreter. For
instance, compared with other interpreters, the difference be-
tween PGD and ADV2 is relatively marginal on GRAD, imply-
Figure 6: IoU scores of adversarial attribution maps (PGD, ADV2)
with respect to benign maps.
Following a typical rule used in the object detection task
[21] where a detected region of interest (RoI) is considered
positive if its IoU score is above 0.5 with respect to a ground-
truth mask, we thus consider an attribution map as plausible if
its IoU score exceeds 0.5 with respect to the benign attribution
map. Figure 6 compares the average IoU scores of adversarial
maps (PGD, ADV2) with respect to the benign cases. Observe
that ADV2 achieves IoU scores above 0.5 across all the inter-
preters, which are more than 40% higher than PGD in all the