User: Ok, I am writing a java based Hadoop MR task. Part of the task is calling an outside python script as a new Process, passing it information and reading it back the resut. I have done this a few times before without problems when not working with hadoop.
I can also call a single python script as a new process in hadoop when it does not import anything, or only things that are on the nodes' python install.
the current python script calls an import on another script which is usually just sitting in the same directory and that works fine when not running on hadoop.
In hadoop I have added both files to the distributed cache so I do not understand why the script could not import the other one.
Assistant: Ok, I figured this one out by myself.
The problem the python process runs into is that the HDFS uses symlinks. Python on the other hand does not accept symlinks as valid files so will not import from them if in the same directory.
Instead of adding each file to the Distributed Cache, you can add the directory to the cache, then any calls to the directory are via the symlink but calls to files are then in the actual directory, allowing the python process to import libraries