## 开放讨论请思考一下，如果我们要使用 AdminClient去增加某个主题的分区，代码应该怎么写？请给出主体代码。欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。![](Images/a7d15815f9efb5693db5b2d278244658.png){savepage-src="https://static001.geekbang.org/resource/image/c8/bf/c89da43deab85fe7cb06acec867aa5bf.jpg"}
# 33 \| Kafka认证机制用哪家？你好，我是胡夕。今天我要和你分享的主题是：Kafka 的认证机制。
## 什么是认证机制？所谓认证，又称"验证""鉴权"，英文是authentication，是指通过一定的手段，完成对用户身份的确认。认证的主要目的是确认当前声称为某种身份的用户确实是所声称的用户。在计算机领域，经常和认证搞混的一个术语就是授权，英文是authorization。授权一般是指对信息安全或计算机安全相关的资源定义与授予相应的访问权限。举个简单的例子来区分下两者：认证要解决的是你要证明你是谁的问题，授权要解决的则是你能做什么的问题。在 Kafka 中，认证和授权是两套独立的安全配置。我们今天主要讨论 Kafka的认证机制，在专栏的下一讲内容中，我们将讨论授权机制。
## Kafka 认证机制自 0.9.0.0 版本开始，Kafka正式引入了认证机制，用于实现基础的安全用户认证，这是将 Kafka上云或进行多租户管理的必要步骤。截止到当前最新的 2.3 版本，Kafka支持基于 SSL 和基于 SASL 的安全认证机制。**基于 SSL 的认证主要是指 Broker 和客户端的双路认证**（2-wayauthentication）。通常来说，SSL加密（Encryption）已经启用了单向认证，即客户端认证 Broker的证书（Certificate）。如果要做 SSL认证，那么我们要启用双路认证，也就是说 Broker 也要认证客户端的证书。``{=html}对了，你可能会说，SSL 不是已经过时了吗？现在都叫 TLS（Transport LayerSecurity）了吧？但是，Kafka 的源码中依然是使用 SSL 而不是 TLS来表示这类东西的。不过，今天出现的所有 SSL 字眼，你都可以认为它们是和TLS 等价的。Kafka 还支持通过 SASL 做客户端认证。**SASL是提供认证和数据安全服务的框架**。Kafka 支持的 SASL 机制有 5种，它们分别是在不同版本中被引入的，你需要根据你自己使用的 Kafka版本，来选择该版本所支持的认证机制。1.  GSSAPI：也就是 Kerberos 使用的安全接口，是在 0.9 版本中被引入的。2.  PLAIN：是使用简单的用户名 / 密码认证的机制，在 0.10 版本中被引入。3.  SCRAM：主要用于解决 PLAIN 机制安全问题的新机制，是在 0.10.2    版本中被引入的。4.  OAUTHBEARER：是基于 OAuth 2 认证框架的新机制，在 2.0 版本中被引进。5.  Delegation Token：补充现有 SASL 机制的轻量级认证机制，是在 1.1.0    版本被引入的。
## 认证机制的比较Kafka为我们提供了这么多种认证机制，在实际使用过程中，我们应该如何选择合适的认证框架呢？下面我们就来比较一下。目前来看，使用 SSL 做信道加密的情况更多一些，但使用 SSL 实现认证不如使用SASL。毕竟，SASL 能够支持你选择不同的实现机制，如 GSSAPI、SCRAM、PLAIN等。因此，我的建议是**你可以使用 SSL 来做通信加密，使用 SASL 来做 Kafka的认证实现**。SASL 下又细分了很多种认证机制，我们应该如何选择呢？SASL/GSSAPI 主要是给 Kerberos 使用的。如果你的公司已经做了 Kerberos认证（比如使用 Active Directory），那么使用 GSSAPI是最方便的了。因为你不需要额外地搭建 Kerberos，只要让你们的 Kerberos管理员给每个 Broker 和要访问 Kafka 集群的操作系统用户申请 principal就好了。总之，**GSSAPI 适用于本身已经做了 Kerberos认证的场景，这样的话，SASL/GSSAPI 可以实现无缝集成**。而 SASL/PLAIN，就像前面说到的，它是一个简单的用户名 /密码认证机制，通常与 SSL 加密搭配使用。注意，这里的 PLAIN 和 PLAINTEXT是两回事。**PLAIN 在这里是一种认证机制，而 PLAINTEXT 说的是未使用 SSL时的明文传输**。对于一些小公司而言，搭建公司级的 Kerberos可能并没有什么必要，他们的用户系统也不复杂，特别是访问 Kafka集群的用户可能不是很多。对于 SASL/PLAIN而言，这就是一个非常合适的应用场景。**总体来说，SASL/PLAIN的配置和运维成本相对较小，适合于小型公司中的 Kafka 集群**。但是，SASL/PLAIN 有这样一个弊端：它不能动态地增减认证用户，你必须重启Kafka集群才能令变更生效。为什么呢？这是因为所有认证用户信息全部保存在静态文件中，所以只能重启Broker，才能重新加载变更后的静态文件。我们知道，重启集群在很多场景下都是令人不爽的，即使是轮替式升级（RollingUpgrade）。SASL/SCRAM 就解决了这样的问题。它通过将认证用户信息保存在ZooKeeper 的方式，避免了动态修改需要重启 Broker的弊端。在实际使用过程中，你可以使用 Kafka提供的命令动态地创建和删除用户，无需重启整个集群。因此，**如果你打算使用SASL/PLAIN，不妨改用 SASL/SCRAM 试试。不过要注意的是，后者是 0.10.2版本引入的。你至少要升级到这个版本后才能使用**。SASL/OAUTHBEARER 是 2.0 版本引入的新认证机制，主要是为了实现与 OAuth 2框架的集成。OAuth是一个开发标准，允许用户授权第三方应用访问该用户在某网站上的资源，而无需将用户名和密码提供给第三方应用。Kafka不提倡单纯使用 OAUTHBEARER，因为它生成的不安全的 JSON WebToken，必须配以 SSL 加密才能用在生产环境中。当然，鉴于它是 2.0版本才推出来的，而且目前没有太多的实际使用案例，我们可以先观望一段时间，再酌情将其应用于生产环境中。Delegation Token 是在 1.1版本引入的，它是一种轻量级的认证机制，主要目的是补充现有的 SASL 或 SSL认证。如果要使用 Delegation Token，你需要先配置好 SASL 认证，然后再利用Kafka 提供的 API 去获取对应的 Delegation Token。这样，Broker和客户端在做认证的时候，可以直接使用这个 token，不用每次都去 KDC获取对应的 ticket（Kerberos 认证）或传输 Keystore 文件（SSL 认证）。为了方便你更好地理解和记忆，我把这些认证机制汇总在下面的表格里了。你可以对照着表格，进行一下区分。![](Images/cfea83d54f8e364a2a1f3ddfbd059f1c.png){savepage-src="https://static001.geekbang.org/resource/image/4a/3d/4a52c2eb1ae631697b5ec3d298f7333d.jpg"}
## SASL/SCRAM-SHA-256 配置实例接下来，我给出 SASL/SCRAM 的一个配置实例，来说明一下如何在 Kafka集群中开启认证。其他认证机制的设置方法也是类似的，比如它们都涉及认证用户的创建、Broker端以及 Client 端特定参数的配置等。我的测试环境是本地 Mac 上的两个 Broker 组成的 Kafka 集群，连接端口分别是9092 和 9093。
### 第 1 步：创建用户配置 SASL/SCRAM 的第一步，是创建能否连接 Kafka集群的用户。在本次测试中，我会创建 3 个用户，分别是 admin 用户、writer用户和 reader 用户。admin 用户用于实现 Broker 间通信，writer用户用于生产消息，reader 用户用于消费消息。我们使用下面这 3 条命令，分别来创建它们。    $ cd kafka_2.12-2.3.0/$ bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[password=admin],SCRAM-SHA-512=[password=admin]' --entity-type users --entity-name adminCompleted Updating config for entity: user-principal 'admin'.    $ bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[password=writer],SCRAM-SHA-512=[password=writer]' --entity-type users --entity-name writerCompleted Updating config for entity: user-principal 'writer'.    $ bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[password=reader],SCRAM-SHA-512=[password=reader]' --entity-type users --entity-name readerCompleted Updating config for entity: user-principal 'reader'.在专栏前面，我们提到过，kafka-configs脚本是用来设置主题级别参数的。其实，它的功能还有很多。比如在这个例子中，我们使用它来创建SASL/SCRAM认证中的用户信息。我们可以使用下列命令来查看刚才创建的用户数据。    $ bin/kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type users  --entity-name writerConfigs for user-principal 'writer' are SCRAM-SHA-512=salt=MWt6OGplZHF6YnF5bmEyam9jamRwdWlqZWQ=,stored_key=hR7+vgeCEz61OmnMezsqKQkJwMCAoTTxw2jftYiXCHxDfaaQU7+9/dYBq8bFuTio832mTHk89B4Yh9frj/ampw==,server_key=C0k6J+9/InYRohogXb3HOlG7s84EXAs/iw0jGOnnQAt4jxQODRzeGxNm+18HZFyPn7qF9JmAqgtcU7hgA74zfA==,iterations=4096,SCRAM-SHA-256=salt=MWV0cDFtbXY5Nm5icWloajdnbjljZ3JqeGs=,stored_key=sKjmeZe4sXTAnUTL1CQC7DkMtC+mqKtRY0heEHvRyPk=,server_key=kW7CC3PBj+JRGtCOtIbAMefL8aiL8ZrUgF5tfomsWVA=,iterations=4096这段命令包含了 writer 用户加密算法 SCRAM-SHA-256 以及 SCRAM-SHA-512对应的盐值 (Salt)、ServerKey 和 StoreKey。这些都是 SCRAM机制的术语，我们不需要了解它们的含义，因为它们并不影响我们接下来的配置。