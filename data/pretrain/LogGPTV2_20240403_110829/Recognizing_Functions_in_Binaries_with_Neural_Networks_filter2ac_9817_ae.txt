information exceeds the advantages mentioned in the
previous paragraph.
6 Discussion
Limitations. As with most other machine learning ap-
proaches, ours assumes that the same underlying genera-
tive process has created both the training set and the test
set. If similar patterns from the training data do not ex-
hibit themselves in the test data, our approach will fail to
correctly identify the functions.
As a pathological case, consider what would happen if
long sequences of instructions which have no effect were
inserted at arbitrary locations in the binary, including in
the middle of function prologues. Such insertions would
cause the internal structure of the binary to differ from
what the model saw in the training data, even though
it has no affect on the functionality. We might easily
remove these instruction sequences if they were simply
NOPs (0x90 in x86), but we can imagine the ability to
create arbitrarily complicated ones especially if they are
allowed to be long. Results from computability theory,
such as Rice’s theorem, suggest that it could be very dif-
ﬁcult (if not impossible) to ﬁlter out such sequences from
the binary through static analysis.
USENIX Association  
24th USENIX Security Symposium  623
13
P
gcc
R
ELF x86
F1
P
icc
R
F1
P
gcc
R
ELF x86-64
F1
P
icc
R
F1
O0 99.89% 99.95% 99.92% 99.85% 99.94% 99.90% 99.72% 99.56% 99.64% 99.77% 99.51% 99.64%
O1 99.37% 98.29% 98.82% 99.62% 98.22% 98.91% 98.87% 96.80% 97.83% 99.35% 96.97% 98.15%
O2 99.20% 98.45% 98.82% 99.57% 99.28% 99.43% 97.18% 96.58% 96.88% 98.93% 97.76% 98.34%
O3 99.28% 98.77% 99.02% 99.50% 99.31% 99.40% 96.83% 96.69% 96.76% 98.99% 97.66% 98.33%
PE x86
R
P
PE x86-64
F1
P
R
F1
Od 98.96% 99.43% 99.19% 99.52% 99.39% 99.45%
O1 98.89% 97.21% 98.04% 99.48% 98.68% 99.08%
O2 99.05% 98.60% 98.82% 99.50% 99.14% 99.32%
Ox 99.16% 98.63% 98.90% 99.61% 99.14% 99.37%
Table 8: Performance of our function start identiﬁcation model on different subsets of the dataset. Each percentage
value represents the precision, recall, or F1 score on the binaries of a particular architecture, compiler, and optimization
level combination.
As for RNNs, since they accumulate and transfer in-
formation in a sequential manner, the input from these
irrelevant instructions could easily overwrite the parts of
the hidden state necessary for making correct predictions
about the locations of the function boundaries.
In some cases, we can foresee that our approach will
require preprocessing of the data in order to obtain good
results. For example, binaries which decompress or de-
crypt themselves at runtime would not contain recogniz-
able code within the binary stored on disk. Given that
such obfuscations affect all static binary analysis tech-
niques, previous works have addressed the problem of
detecting and reversing such transformations [10, 21].
Segmented results.
In Table 8, we delineate how the
accuracy results for function start identiﬁcation with our
model (as described in Section 5.3) differed among dif-
ferent subsets of the binaries as further segmented by
compiler and optimization level.
As we might expect, the model does best when run on
binaries compiled without any optimizations (labeled as
O0 and Od in the table), given that those tend to have
very clear indications at the beginnings of functions.
Nevertheless, the model’s performance remains roughly
constant on binaries compiled with more optimizations,
with the exception of gcc on Linux for the x86-64 archi-
tecture where the F1 score decreased by about 2.9 per-
centage points. Given that the training data contains ex-
amples with every optimization level and compiler used
for testing, we would hope that the model can learn to
recognize functions in all such cases. However, it seems
that gcc can produce relatively challenging examples
with more optimizations enabled. Since the x86-64 ABI
passes some function arguments in registers, it is possible
to avoid any manipulation of the stack and base pointers
upon function entry.
Error analysis. We randomly sampled some of the bi-
naries to manually inspect the errors made by the model
in them. Speciﬁcally, we selected 5 binaries for each
combination of compiler, optimization level, architec-
ture, and OS, then examined the errors to identify some
common features between them.
Here are some observations we made:
• Given the bidirectionality of the model, it seems to
exploit the appearance of frequently-occurring se-
quences at the ends of the previous function in ad-
dition to typical function prologues. One obvious
example are ret and its variants, used to return
from function execution. The compiler also often
inserted padding between functions (such as nop
(0x90) and other no-op instructions with longer en-
codings, or in Windows binaries, int3 which trig-
gers an interrupt), the end of which the model would
use to recognize the beginnings of functions.
• As a consequence of the above, false positives of-
ten occurred after nop, ret, and other instructions
which usually appear at the end of a function. In
fact, it would also ﬁnd false positives within imme-
diate values encoded into the code if they contained
0x90 or 0xc3, the encodings of those instructions.
• False negatives often occurred when instructions
that would typically occur in the middle of func-
tions occurred at the beginning of a function, as we
might expect. The ﬁrst byte of the program was of-
ten also falsely not recognized as a function start,
presumably due to the lack of context previous to it.
624  24th USENIX Security Symposium 
USENIX Association
14
• As documented by Bao et al. [2], icc will generate
functions with multiple entry points. Many of the
false negatives occurred at the second entry points
to functions, given that the instructions before it are
not the ones which usually end functions.
• The behavior of the model was not easily character-
ized by simple rules on short sequences of instruc-
tions; for example, while many false positives oc-
curred after nop and ret, this did not mean that
the model marked all (or even a large fraction) of
such positions as function starts. For relatively dif-
ﬁcult cases like these, the precise content of the sur-
rounding bytes might have a complicated effect on
the answer produced by the model.
random ﬁeld to impose some structure between predic-
tions for related instructions. Karampatziakis [8] tackles
the related problem of accurate static disassembly using
similar machine-learning tools, and Jacobson et al. [7]
extend the prior work by Rosenblum et al. to ﬁngerprint-
ing library wrappers which appear in binaries. Bao et
al. [2] also address function identiﬁcation using super-
vised learning, but use weighted preﬁx trees which re-
quire much less computation than Rosenblum et al.’s ap-
proach to train, but still seems to give results with high
accuracy.
Some tools built for binary analysis provide function
identiﬁcation as part of their functionality, usually us-
ing relatively simple heuristics or hand-coded signatures:
Dyninst [6] and IDA Pro are some examples.
Future work. Although we have seen some experi-
mental evidence about the performance of the RNN un-
der various conditions, we lack a clear explanation of
the internal mechanics of the model. One potential ap-
proach towards an explanation proceeds through an anal-
ysis of the eigenvector structure by linearizing the state
of the network as it evolves over time and analyzing
which eigenvectors of the linearized systems carry the
task-relevant information [12]. This analysis can provide
an understanding of how the network ignores irrelevant
information while selecting, integrating, and communi-
cating relevant information, and allows identiﬁcation of
which eigenvector(s) of the linearized system are respon-
sible for these tasks performed by the network. However,
if the neural network’s parameters are available to adver-
saries interested in disrupting the accuracy of the model,
they may be able to use such analyses to more effectively
add extra instructions which are not orthogonal to the
eigenvectors carrying the task-relevant information, thus
preventing its transmission and signiﬁcantly affecting the
RNN’s performance.
7 Related Work
Function identiﬁcation. Given that function identiﬁ-
cation serves as a basis for many applications within bi-
nary analysis, it should not surprise that many past papers
have discussed the topic. For example, Kruegel et al. [9]
identify functions as a prelude to static disassembly, and
Theiling [18] for inferring control-ﬂow graphs. How-
ever, these do not focus speciﬁcally on function identi-
ﬁcation as a speciﬁc problem, so here we point out some
other works that do.
Rosenblum et al. [14] ﬁrst framed the function identi-
ﬁcation problem as a task for machine learning. They
combine a logistic regression classiﬁer that uses “id-
ioms” (short patterns of instructions) with a conditional
Neural networks. Much research using neural net-
works have focused on domains with continuous input
data, such as vision and speech. In contrast, binary code
contains discrete, multinomial values, where there typi-
cally exists no obvious ordering relationship between the
possible values (unlike intensities of light or sound, for
example).
Natural language processing also involves multino-
mial values (typically sequences of words), and neural
networks have been successfully used for some applica-
tions there. Bengio et al. [3] ﬁrst used neural networks to
make a language model. Language models give a proba-
bility distribution over the next word in a sentence given
the words so far, and see usage in machine translation
and speech recognition. Mikolov et al. [11] moved to
using a RNN. More recently, Sutskever et al. [17], Bah-
danau et al. [1], and Cho et al. [5] have used RNNs for
machine translation, and Vinyals et al. [20] for parsing.
We could not ﬁnd any previous works which applied
neural networks to binary code, but some use them on
source code. Zaremba and Sutskever [22] attempt to train
recurrent neural networks to evaluate short Python pro-
grams. Mou et al. [13] learn a vector representation from
ASTs for supervised classiﬁcation of programs.
8 Conclusion
In this paper, we proposed a new machine-learning-based
approach for function identiﬁcation in binary code based
on recurrent neural networks. To our knowledge, there
exists no previous works which apply neural networks
to any problems in binary analysis. We address this gap
by demonstrating how to use recurrent neural networks
for function identiﬁcation, and empirically show drastic
reductions in computation time despite achieving com-
parable or better accuracy on a prior test suite. We hope
USENIX Association  
24th USENIX Security Symposium  625
15
that this work can serve as an inspiration for further ad-
vancements in binary analysis through neural networks.
Acknowledgements
We would like to thank Philipp Moritz for help with
the initial implementation and experiments. We ac-
knowledge Kevin Chen, Warren He, and the anony-
mous reviewers for their helpful feedback. This work
was supported in part by FORCES (Foundations Of Re-
silient CybEr-Physical Systems), which receives sup-
port from the National Science Foundation (NSF award
numbers CNS-1238959, CNS-1238962, CNS-1239054,
CNS-1239166); by the National Science Foundation un-
der award CCF-0424422; and by DARPA under award
HR0011-12-2-005.
References
[1] BAHDANAU, D., CHO, K., AND BENGIO, Y. Neural ma-
chine translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473 (2014).
[2] BAO, T., BURKET, J., WOO, M., TURNER, R., AND BRUMLEY,
D. Byteweight: Learning to recognize functions in binary code.
In 23rd USENIX Security Symposium (USENIX Security 14) (San
Diego, CA, Aug. 2014), USENIX Association, pp. 845–860.
[3] BENGIO, Y., DUCHARME, R., VINCENT, P., AND JANVIN, C.
A neural probabilistic language model. The Journal of Machine
Learning Research 3 (2003), 1137–1155.
[4] BERGSTRA, J., BREULEUX, O., BASTIEN, F., LAMBLIN,
P., PASCANU, R., DESJARDINS, G., TURIAN, J., WARDE-
FARLEY, D., AND BENGIO, Y. Theano: a CPU and GPU math
expression compiler. In Proceedings of the Python for Scientiﬁc
Computing Conference (SciPy) (June 2010). Oral Presentation.
[5] CHO, K., VAN MERRIENBOER, B., GULCEHRE, C.,
BOUGARES, F., SCHWENK, H., AND BENGIO, Y. Learning
phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078 (2014).
[6] HARRIS, L. C., AND MILLER, B. P. Practical analysis of
stripped binary code. ACM SIGARCH Computer Architecture
News 33, 5 (2005), 63–68.
[7] JACOBSON, E. R., ROSENBLUM, N., AND MILLER, B. P. La-
beling library functions in stripped binaries. In Proceedings of
the 10th ACM SIGPLAN-SIGSOFT workshop on Program anal-
ysis for software tools (2011), ACM, pp. 1–8.
[8] KARAMPATZIAKIS, N. Static analysis of binary executables us-
ing structural svms. In Advances in Neural Information Process-
ing Systems (2010), pp. 1063–1071.
[9] KRUEGEL, C., ROBERTSON, W., VALEUR, F., AND VIGNA, G.
Static disassembly of obfuscated binaries. In USENIX security
Symposium (2004), vol. 13, pp. 18–18.
[10] MARTIGNONI, L., CHRISTODORESCU, M., AND JHA, S. Om-
niunpack: Fast, generic, and safe unpacking of malware.
In
Computer Security Applications Conference, 2007. ACSAC 2007.
Twenty-Third Annual (2007), IEEE, pp. 431–441.
[11] MIKOLOV, T., KARAFI ´AT, M., BURGET, L., CERNOCK `Y, J.,
AND KHUDANPUR, S. Recurrent neural network based language
model.
In INTERSPEECH 2010, 11th Annual Conference of
the International Speech Communication Association, Makuhari,
Chiba, Japan, September 26-30, 2010 (2010), pp. 1045–1048.
[12] MOAZZEZI, R. Change-based population coding. PhD thesis,
UCL (University College London), 2011.
[13] MOU, L., LI, G., LIU, Y., PENG, H., JIN, Z., XU, Y., AND
ZHANG, L. Building program vector representations for deep
learning. arXiv preprint arXiv:1409.3358 (2014).
[14] ROSENBLUM, N. E., ZHU, X., MILLER, B. P., AND HUNT,
K. Learning to analyze binary computer code. In AAAI (2008),
pp. 798–804.
[15] SCHMIDHUBER, J. Long short-term memory: Tutorial on lstm
recurrent networks. http://people.idsia.ch/~juergen/
lstm/sld004.htm, 2003.
[16] SUTSKEVER, I. Training recurrent neural networks. PhD thesis,
University of Toronto, 2013.
[17] SUTSKEVER, I., VINYALS, O., AND LE, Q. V. Sequence to
sequence learning with neural networks. In Advances in Neural
Information Processing Systems (2014), pp. 3104–3112.
[18] THEILING, H. Extracting safe and precise control ﬂow from bi-
naries. In Real-Time Computing Systems and Applications, 2000.
Proceedings. Seventh International Conference on (2000), IEEE,
pp. 23–30.
[19] TIELEMAN, T., AND HINTON, G. Lecture 6.5—RmsProp: Di-
vide the gradient by a running average of its recent magnitude.
COURSERA: Neural Networks for Machine Learning, 2012.
[20] VINYALS, O., KAISER, L., KOO, T., PETROV, S., SUTSKEVER,
I., AND HINTON, G. Grammar as a foreign language. arXiv
preprint arXiv:1412.7449 (2014).
[21] YAN, W., ZHANG, Z., AND ANSARI, N. Revealing packed mal-
ware. Security & Privacy, IEEE 6, 5 (2008), 65–69.
[22] ZAREMBA, W., AND SUTSKEVER, I. Learning to execute. arXiv
preprint arXiv:1410.4615 (2014).
[23] ZAREMBA, W., SUTSKEVER, I., AND VINYALS, O. Recurrent
neural network regularization. arXiv preprint arXiv:1409.2329
(2014).
A Backpropagation
We can view backpropagation as repeated application of
the chain rule. We sketch how it works using the follow-
ing example of a three-layer network:
h1 = f1(x;θ1)
h2 = f2(h1;θ2)
ˆy = f3(h2;θ3)
where θi = (Wi,bi), and we have named all of the inter-
mediate hidden values for convenience of reference. We
wish to minimize the error between the predicted ˆy and
the true value y. For example:
L = d(y, ˆy) =(cid:31) ˆy− y(cid:31)2
Then we can compute the following partial derivatives
using the chain rule:
= 2( ˆy− y)
∂ ˆy
∂ h2
∂ h2
∂ h1
∂ L
∂ ˆy
∂ L
∂ h2
=
=
∂ L
∂ ˆy
∂ L
∂ h2
∂ L
∂ h1
16
∂ L
∂θ3
∂ L
∂θ2
∂ L
∂θ1
=
=
=
∂ L
∂ ˆy
∂ L
∂ h2
∂ L
∂ h1
∂ ˆy
∂θ3
∂ h2
∂θ2
∂ h1
∂θ1
626  24th USENIX Security Symposium 
USENIX Association