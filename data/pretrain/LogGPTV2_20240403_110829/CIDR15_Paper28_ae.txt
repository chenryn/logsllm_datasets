|---|---|---|
| tors (e.g. sorting), the execution is fully pipeline-able, which |5.1 |Runtime Code Generation |
minimizes the memory consumption for storing intermedi-ate results. When processed in memory, the tuples have a canonical in-memory row-oriented format.Operators that may need to consume lots of memory are designed to be able to spill parts of their working set to disk if needed. The operators that are spillable are the hash join, (hash-based) aggregation, sorting, and analytic function
Runtime code generation using LLVM [8] is one of the techniques employed extensively by Impala’s backend to improve execution times. Performance gains of 5x or more are typical for representative workloads.LLVM is a compiler library and collection of related tools. Unlike traditional compilers that are implemented as stand-alone applications, LLVM is designed to be modular and
evaluation. 	reusable. It allows applications like Impala to perform just-Impala employs a partitioning approach for the hash join and aggregation operators. That is, some bits of the hash value of each tuple determine the target partition and the remaining bits for the hash table probe. During normal oper-ation, when all hash tables fit in memory, the overhead of the partitioning step is minimal, within 10% of the performance of a non-spillable non-partitioning-based implementation. When there is memory-pressure, a “victim” partition may be spilled to disk, thereby freeing memory for other partitions to complete their processing. When building the hash tables for the hash joins and there is reduction in cardinality of the build-side relation, we construct a Bloom filter which is then passed on to the probe side scanner, implementing a simple version of a semi-join.in-time (JIT) compilation within a running process, with the full benefits of a modern optimizer and the ability to generate machine code for a number of architectures, by exposing separate APIs for all steps of the compilation process.Impala uses runtime code generation to produce query-specific versions of functions that are critical to performance. In particular, code generation is applied to “inner loop” func-tions, i.e., those that are executed many times (for every tuple) in a given query, and thus constitute a large portion of the total time the query takes to execute. For example, a function used to parse a record in a data file into Impala’s in-memory tuple format must be called for every record in every data file scanned. For queries scanning large tables, this could be billions of records or more. This function must| Query Time (sec) | 40 | 
 | 
 | 
 | simple aggregation queries. Code generation speeds up the | simple aggregation queries. Code generation speeds up the |
|---|---|---|---|---|---|---|
| Query Time (sec) |30 |    |    |    |execution by up to 5.7x, with the speedup increasing with |execution by up to 5.7x, with the speedup increasing with |
| Query Time (sec) |30 |    |    |    |the query complexity. |the query complexity. || Query Time (sec) |20 |    |    |    |5.2 |I/O Management |
| Query Time (sec) |20 |    |    |    |Efficiently retrieving data from HDFS is a challenge for |Efficiently retrieving data from HDFS is a challenge for |
| Query Time (sec) |10 |    |    |    |all SQL-on-Hadoop systems. In order to perform data scans |all SQL-on-Hadoop systems. In order to perform data scans || Query Time (sec) |10 |    |    |    |from both disk and memory at or near hardware speed, Im- |from both disk and memory at or near hardware speed, Im- |
| Query Time (sec) |0 |    |    |    |pala uses an HDFS feature called short-circuit local reads [3] |pala uses an HDFS feature called short-circuit local reads [3] |
| Query Time (sec) |0 |    |    |    |to bypass the DataNode protocol when reading from local |to bypass the DataNode protocol when reading from local || Query Time (sec) |0 |select count(*)  |select  |TPC-H Q1 |disk. Impala can read at almost disk bandwidth (approx. |disk. Impala can read at almost disk bandwidth (approx. |
| Query Time (sec) |0 |select count(*)  |select  |TPC-H Q1 |100MB/s per disk) and is typically able to saturate all avail- |100MB/s per disk) and is typically able to saturate all avail- || Query Time (sec) |0 |from lineitem |count(l_orderkey)  |count(l_orderkey)  |100MB/s per disk) and is typically able to saturate all avail- |100MB/s per disk) and is typically able to saturate all avail- |
| Query Time (sec) |0 |from lineitem |from lineitem |from lineitem |able disks. We have measured that with 12 disks, Impala is |able disks. We have measured that with 12 disks, Impala is |capable of sustaining I/O at 1.2GB/sec. Furthermore, HDFS