 **System information**
  * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS10.14.4
  * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone7 & iPhone6
  * TensorFlow installed from (source or binary):binary
  * TensorFlow version: tensorflow-lite-gpu:0.0.0
  * Python version:3.7
  * Installed using virtualenv? pip? conda?:cocoapod
  * Bazel version (if compiling from source):
  * GCC/Compiler version (if compiling from source):
  * CUDA/cuDNN version:
  * GPU model and memory:16G memory
**Describe the problem**
I am building my codes with tensorflow_lite_gpu.framework on iPhone 7. The
codes is just like Google recommends:
    _model = FlatBufferModel::BuildFromFile(modelPathCString);
            ops::builtin::BuiltinOpResolver resolver;
            InterpreterBuilder(*_model, resolver)(&_interpreter);
            _delegate = NewGpuDelegate(nullptr);  // default config
            _interpreter->ModifyGraphWithDelegate(_delegate);
    ...//Other codes
When I use the model mobilenet_v1_1.0_224.tflite which Google provides, I get
warning: `WARNING: 25 cannot be handled by this delegate. Only the first 30
ops will run on the GPU, and the remaining 1 on the CPU` from console after
excuting `_interpreter->ModifyGraphWithDelegate(_delegate)`. But the model
deeplabv3_257_mv_gpu.tflite is good which Google provides too. As for my own
model, the result is like mobilenet_v1_1.0_224.tflite does. Someone please
help me.
PS: Here is my podfile:
    platform :ios, '10.0'
    target "SpeechExample" do
    pod 'TensorFlowLiteGpuExperimental'
    end