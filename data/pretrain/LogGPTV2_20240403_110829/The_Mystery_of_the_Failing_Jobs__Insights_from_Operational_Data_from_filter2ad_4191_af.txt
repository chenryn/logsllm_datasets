International Conference for High Performance Computing, Networking,
Storage and Analysis, pages 108–121, Nov 2018.
[17] Christina Delimitrou and Christos Kozyrakis.
Paragon: Qos-aware
scheduling for heterogeneous datacenters. In ACM SIGPLAN Notices,
volume 48, pages 77–88. ACM, 2013.
[18] Catello Di Martino, Zbigniew Kalbarczyk, Ravishankar K Iyer, Fabio
Baccanico, Joseph Fullop, and William Kramer. Lessons learned from
the analysis of system failures at petascale: The case of Blue Waters. In
Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP
International Conference on, pages 610–621. IEEE, 2014.
[19] Catello Di Martino, William Kramer, Zbigniew Kalbarczyk, and Rav-
ishankar Iyer. Measuring and understanding extreme-scale application
resilience: A ﬁeld study of 5,000,000 hpc application runs.
In De-
pendable Systems and Networks (DSN), 2015 45th Annual IEEE/IFIP
International Conference on, pages 25–36. IEEE, 2015.
[20] THE GEEK DIARY.
in
Linux
Killer
https://www.thegeekdiary.com/what-is-out-of-memory-oom-killer-
in-linux-causes-troubleshooting-mitigation/.
(Causes,
What
is Out-of-Memory
(OOM)
Troubleshooting, Mitigation).
[21] N. El-Sayed, H. Zhu, and B. Schroeder. Learning from failure across
multiple clusters: A trace-driven approach to understanding, predicting,
and mitigating job terminations.
In 2017 IEEE 37th International
Conference on Distributed Computing Systems (ICDCS), pages 1333–
1344, June 2017.
169
[22] Martin Ester, Hans-Peter Kriegel, J¨org Sander, Xiaowei Xu, et al.
A density-based algorithm for discovering clusters in large spatial
databases with noise. In Kdd, volume 96, pages 226–231, 1996.
[23] Todd Evans, William L Barth, James C Browne, Robert L DeLeon,
Thomas R Furlani, Steven M Gallo, Matthew D Jones, and Abani K
Patra. Comprehensive resource use monitoring for hpc systems with
tacc stats. In Proceedings of the First International Workshop on HPC
User Support Tools, pages 13–21. IEEE Press, 2014.
Job failure prediction in
In 2009 14th
[24] H. Fadishei, H. Saadatfar, and H. Deldari.
grid environment based on workload characteristics.
International CSI Computer Conference, pages 329–334, Oct 2009.
[25] Fahimeh Farahnakian, Pasi Liljeberg, and Juha Plosila. Lircup: Linear
regression based cpu usage prediction algorithm for live migration of
virtual machines in data centers. In 2013 39th Euromicro Conference
on Software Engineering and Advanced Applications, pages 357–364.
IEEE, 2013.
[26] Dror G Feitelson and Ahuva Mu’alem Weil. Utilization and predictabil-
ity in scheduling the ibm sp2 with backﬁlling. In Proceedings of the First
Merged International Parallel Processing Symposium and Symposium on
Parallel and Distributed Processing, pages 542–546. IEEE, 1998.
[27] S. Fu and C. Xu. Exploring event correlation for failure prediction in
coalitions of clusters. In SC ’07: Proceedings of the 2007 ACM/IEEE
Conference on Supercomputing, pages 1–12, Nov 2007.
[28] Joshi Fullop et al. A diagnostic utility for analyzing periods of degraded
job performance. In Proc. Cray User Group, 2014.
[29] A. Gainaru, F. Cappello, M. Snir, and W. Kramer. Fault prediction
In SC ’12:
under the microscope: A closer look into hpc systems.
Proceedings of
the International Conference on High Performance
Computing, Networking, Storage and Analysis, pages 1–11, Nov 2012.
[30] Ana Gainaru, Franck Cappello, Marc Snir, and William Kramer. Failure
prediction for hpc systems and applications: Current situation and open
issues.
Int. J. High Perform. Comput. Appl., 27(3):273–282, August
2013.
[31] Saurabh Gupta, Tirthak Patel, Christian Engelmann, and Devesh Tiwari.
Failures in large scale systems: Long-term measurement, analysis, and
implications. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, SC ’17,
pages 44:1–44:12, New York, NY, USA, 2017. ACM.
[32] Ling Huang, Jinzhu Jia, Bin Yu, Byung-Gon Chun, Petros Maniatis, and
Mayur Naik. Predicting execution time of computer programs using
sparse polynomial regression. In Proceedings of the 23rd International
Conference on Neural Information Processing Systems - Volume 1,
NIPS’10, pages 883–891, USA, 2010. Curran Associates Inc.
[33] T. Islam and D. Manivannan. Predicting application failure in cloud: A
machine learning approach. In 2017 IEEE International Conference on
Cognitive Computing (ICCC), pages 24–31, June 2017.
[34] Tanzima Zerin Islam, Kathryn Mohror, Saurabh Bagchi, Adam Moody,
Bronis R De Supinski, and Rudolf Eigenmann. Mcrengine: a scalable
checkpointing system using data-aware aggregation and compression.
Scientiﬁc Programming, 21(3-4):149–163, 2013.
[35] S. Jha, V. Formicola, C. D. Martino, M. Dalton, W. T. Kramer,
Z. Kalbarczyk, and R. K. Iyer. Resiliency of hpc interconnects: A
case study of interconnect failures and recovery in blue waters. IEEE
Transactions on Dependable and Secure Computing, 15(6):915–930,
2018.
[36] S. Jha, A. Patke, J. Brandt, A. Gentile, M. Showerman, E. Roman, ,
Z. Kalbarczyk, W. T. Kramer, and R. Iyer. A study of network congestion
in two supercomputing high-speed interconnects.
In 2019 IEEE 26th
Annual Symposium on High-Performance Interconnects (HOTI), Aug
2019.
[37] Saurabh Jha, Archit Patke, Jim Brandt, Ann Gentile, Benjamin Lim,
Mike Showerman, Greg Bauer, Larry Kaplan, Zbigniew Kalbarczyk,
William Kramer, and Ravi Iyer. Measuring congestion in high-
performance datacenter interconnects. In 17th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 20), pages 37–57,
Santa Clara, CA, February 2020. USENIX Association.
[38] Saurabh Jha, Archit Patke, Mike Showerman, Jeremy Enos, Greg Bauer,
Zbigniew Kalbarczyk, Ravishankar Iyer, and William Kramer. Monet -
blue waters network dataset (https://doi.org/10.13012/B2IDB-2921318
V1), 2019.
[39] M. Karo, R. Lagerstrom, M. Kohnke, and C. Albing. The application
level placement scheduler. In Cray User Group - CUG, 2008.
[40] Yiannos Kryftis, Constandinos X Mavromoustakis, George Mastorakis,
Evangelos Pallis, Jordi Mongay Batalla, Joel JPC Rodrigues, Ciprian
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
Dobre, and Georgios Kormentzas. Resource usage prediction algorithms
for optimal selection of multimedia content delivery methods. In 2015
IEEE international conference on communications (ICC), pages 5903–
5909. IEEE, 2015.
[41] The Hebrew University Experimental Systems Lab.
[42] H. Li, Y. Wu, Y. Chen, C. Wang, and Y. Huang. Application exe-
cution time prediction for effective cpu provisioning in virtualization
environment. IEEE Transactions on Parallel and Distributed Systems,
28(11):3074–3088, Nov 2017.
[43] Jack Li, Calton Pu, Yuan Chen, Vanish Talwar, and Dejan Milojicic.
Improving preemptive scheduling with application-transparent check-
pointing in shared clusters. pages 222–234, 11 2015.
[44] Xiuqiao Li, Nan Qi, Yuanyuan He, and Bill McMillan. Practical resource
usage prediction method for large memory jobs in hpc clusters. In Asian
Conference on Supercomputing Frontiers, pages 1–18. Springer, 2019.
[45] Y. Ling, F. Liu, Y. Qiu, and J. Zhao. Prediction of total execution time
for mapreduce applications. In 2016 Sixth International Conference on
Information Science and Technology (ICIST), pages 341–345, May 2016.
[46] C. Liu, J. Han, Y. Shang, C. Liu, B. Cheng, and J. Chen. Predicting of
job failure in compute cloud based on online extreme learning machine:
A comparative study. IEEE Access, 5:9359–9368, 2017.
[47] Yudan Liu, R. Nassar, C. Leangsuksun, N. Naksinehaboon, M. Paun, and
S. L. Scott. An optimal checkpoint/restart model for a large scale high
performance computing system. In 2008 IEEE International Symposium
on Parallel and Distributed Processing, pages 1–9, April 2008.
[48] Ashraf Mahgoub, Paul Wood, Sachandhan Ganesh, Subrata Mitra, Wolf-
gang Gerlach, Travis Harrison, Folker Meyer, Ananth Grama, Saurabh
Bagchi, and Somali Chaterji. Raﬁki: a middleware for parameter
tuning of nosql datastores for dynamic metagenomics workloads.
In
Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference,
pages 28–40, 2017.
[49] Ashraf Mahgoub, Paul Wood, Alexander Medoff, Subrata Mitra, Folker
{SOPHIA}: Online
Meyer, Somali Chaterji, and Saurabh Bagchi.
reconﬁguration of clustered nosql databases for time-varying workloads.
In 2019 {USENIX} Annual Technical Conference ({USENIX}{ATC}
19), pages 223–240, 2019.
[50] Amiya K Maji, Subrata Mitra, Bowen Zhou, Saurabh Bagchi, and
Akshat Verma. Mitigating interference in cloud services by middleware
reconﬁguration.
In Proceedings of the 15th International Middleware
Conference, pages 277–288. ACM, 2014.
[51] Catello Di Martino, Saurabh Jha, William Kramer, Zbigniew Kalbar-
czyk, and Ravishankar K Iyer. Logdiver: a tool for measuring resilience
of extreme-scale systems and applications.
In Proceedings of the 5th
Workshop on Fault Tolerance for HPC at eXtreme Scale, pages 11–18.
ACM, 2015.
[52] Andr´ea Matsunaga and Jos´e AB Fortes. On the use of machine
learning to predict the time and resources consumed by applications.
In Proceedings of the 2010 10th IEEE/ACM International Conference
on Cluster, Cloud and Grid Computing, pages 495–504. IEEE Computer
Society, 2010.
[53] Dirk Merkel. Docker:
lightweight
linux containers for consistent
development and deployment. Linux Journal, 2014(239):2, 2014.
[54] Subrata Mitra, Suhas Javagal, Amiya K Maji, Todd Gamblin, Adam
Moody, Stephen Harrell, and Saurabh Bagchi. A study of failures
in community clusters: The case of conte.
In Software Reliability
Engineering Workshops (ISSREW), 2016 IEEE International Symposium
on, pages 189–196. IEEE, 2016.
[55] Subrata Mitra, Ignacio Laguna, Dong H Ahn, Saurabh Bagchi, Martin
Schulz, and Todd Gamblin. Accurate application progress analysis for
large-scale parallel debugging. In Proceedings of the ACM Symposium
on Programming Language Design and Implementation (PLDI), vol-
ume 49, pages 193–203. ACM, 2014.
[56] Tudor Miu and Paolo Missier. Predicting the execution time of workﬂow
activities based on their input features.
In Proceedings of the 2012
SC Companion: High Performance Computing, Networking Storage and
Analysis, SCC ’12, pages 64–72, Washington, DC, USA, 2012. IEEE
Computer Society.
[57] Sara Mustafa, Iman Elghandour, and Mohamed A. Ismail. A machine
learning approach for predicting execution time of spark jobs. Alexan-
dria Engineering Journal, 57(4):3767 – 3778, 2018.
[58] B. Nie, J. Xue, S. Gupta, T. Patel, C. Engelmann, E. Smirni, and
D. Tiwari. Machine learning models for gpu error prediction in a
large scale hpc system. In 2018 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN), pages 95–106,
June 2018.
[59] University of Michigan. Interpreting Torque Return Codes. https://arc-
ts.umich.edu/software/torque/return-codes/, 2018.
[60] Stack Overﬂow.
Are there any standard exit status codes in
https://stackoverﬂow.com/questions/1101957/are-there-any-
Linux?
standard-exit-status-codes-in-linux, 2012.
[61] KyoungSoo Park and Vivek S Pai. Comon: a mostly-scalable moni-
toring system for planetlab. ACM SIGOPS Operating Systems Review,
40(1):65–74, 2006.
[62] Fabrizio Petrini, Darren J Kerbyson, and Scott Pakin. The case of the
missing supercomputer performance: Achieving optimal performance on
the 8,192 processors of asci q.
In SC’03: Proceedings of the 2003
ACM/IEEE conference on Supercomputing, pages 55–55. IEEE, 2003.
[63] I. Pietri, G. Juve, E. Deelman, and R. Sakellariou. A performance model
to estimate execution time of scientiﬁc workﬂows on the cloud. In 2014
9th Workshop on Workﬂows in Support of Large-Scale Science, pages
11–19, Nov 2014.
[64] Charles Reiss, Alexey Tumanov, Gregory R Ganger, Randy H Katz, and
Michael A Kozuch. Heterogeneity and dynamicity of clouds at scale:
Google trace analysis. In Proceedings of the Third ACM Symposium on
Cloud Computing, page 7. ACM, 2012.
[65] Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, and Albert Y
Zomaya. On modelling and prediction of total cpu usage for applications
in mapreduce environments. In International conference on Algorithms
and architectures for parallel processing, pages 414–427. Springer,
2012.
[66] Florian Schmidt, Mathias Niepert, and Felipe Huici. Representation
learning for resource usage prediction. arXiv preprint arXiv:1802.00673,
2018.
[67] Bianca Schroeder and Garth Gibson. A large-scale study of failures in
high-performance computing systems. IEEE Transactions on Depend-
able and Secure Computing, 7(4):337–350, 2010.
[68] Warren Smith, Ian Foster, and Valerie Taylor. Predicting application run
times with historical information. Journal of Parallel and Distributed
Computing, 64(9):1007 – 1016, 2004.
[69] Ozan Sonmez, Nezih Yigitbasi, Alexandru Iosup, and Dick Epema.
Trace-based evaluation of job runtime and queue wait time predictions
in grids. In Proceedings of the 18th ACM International Symposium on
High Performance Distributed Computing, HPDC ’09, pages 111–120,
New York, NY, USA, 2009. ACM.
[70] Niyazi Sorkunlu, Varun Chandola, and Abani K. Patra. Tracking system
behaviour from resource usage data. CoRR, abs/1705.10756, 2017.
[71] Garrick Staples. Torque resource manager. In Proceedings of the 2006
ACM/IEEE Conference on Supercomputing, SC ’06, New York, NY,
USA, 2006. ACM.
[72] Taraneh Taghavi, Maria Lupetini, and Yaron Kretchmer. Compute job
memory recommender system using machine learning. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 609–616. ACM, 2016.
[73] S. Tahvili, M. Saadatmand, M. Bohlin, W. Afzal, and S. H. Ameerjan.
Towards execution time prediction for manual test cases from test spec-
iﬁcation. In 2017 43rd Euromicro Conference on Software Engineering
and Advanced Applications (SEAA), pages 421–425, Aug 2017.
[74] D. Tiwari, S. Gupta, and S. S. Vazhkudai. Lazy checkpointing: Exploit-
ing temporal locality in failures to mitigate checkpointing overheads on
extreme-scale systems. In 2014 44th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, pages 25–36, June
2014.
[75] Dan Tsafrir, Yoav Etsion, and Dror G. Feitelson. Backﬁlling using
system-generated predictions rather than user runtime estimates. IEEE
Trans. Parallel Distrib. Syst., 18(6):789–803, June 2007.
[76] Nitin H. Vaidya. Impact of checkpoint latency on overhead ratio of a
IEEE Transactions on Computers, 46(8):942–
checkpointing scheme.
947, 1997.
[77] Davide Del Vento, Thomas Engel, Siddhartha S. Ghosh, David L. Hart,
Rory Kelly, Si Liu, and Richard Valent. System-level monitoring of
ﬂoating-point performance to improve effective system utilization.
In
State of the Practice Reports, SC ’11, pages 5:1–5:6, New York, NY,
USA, 2011. ACM.
[78] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Oppen-
heimer, Eric Tune, and John Wilkes. Large-scale cluster management
at google with borg. In Proceedings of the Tenth European Conference
on Computer Systems, page 18. ACM, 2015.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
170
[79] M. Wu, X. Sun, and H. Jin. Performance under failures of high-end
computing. In SC ’07: Proceedings of the 2007 ACM/IEEE Conference
on Supercomputing, pages 1–11, Nov 2007.
[80] Michael R. Wyatt, II, Stephen Herbein, Todd Gamblin, Adam Moody,
Dong H. Ahn, and Michela Taufer. Prionn: Predicting runtime and
io using neural networks.
In Proceedings of the 47th International
Conference on Parallel Processing, ICPP 2018, pages 46:1–46:12, New
York, NY, USA, 2018. ACM.
interval. Communications of the ACM, 17(9):530–531, 1974.
[81] Tianyin Xu, Xinxin Jin, Peng Huang, Yuanyuan Zhou, Shan Lu, Long
Jin, and Shankar Pasupathy. Early detection of conﬁguration errors to
reduce failure damage. In OSDI, pages 619–634, 2016.
[82] John W Young. A ﬁrst order approximation to the optimum checkpoint
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
171