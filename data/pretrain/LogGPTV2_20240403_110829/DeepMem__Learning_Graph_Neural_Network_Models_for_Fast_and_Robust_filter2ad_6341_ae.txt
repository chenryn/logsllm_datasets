# DeepMem: Memory Forensic Analysis with Graph-Based Kernel Object Detection

## Performance on Small Objects
DeepMem, like many other pointer-based approaches [19], may not perform optimally for small objects with few or no pointers. Our approach models objects based on both the content of the objects and the topological relations between them. Small objects that lack pointers are often insufficiently informative and have weak or no relations with other nodes in memory. Consequently, very little information can be gathered from other nodes to make inferences about these objects. However, important kernel objects such as _EPROCESS, _ETHREAD, and _DRIVER_OBJECT are sufficiently large, allowing our approach to achieve over 99.6% recall and over 99.5% precision, which is adequate for general memory forensic purposes.

## Data Diversity and Validity
To generate diverse memory dumps, we simulate random user actions and allocate kernel objects in random positions in memory, as described in the evaluation section. Despite these efforts, our dataset may still lack sufficient diversity. To enhance diversity, researchers can use different physical machines, load different drivers, etc. Nonetheless, our evaluation demonstrates the feasibility of DeepMem in a homogeneous environment, such as an enterprise network with identical computer configurations or a cloud environment where VMs are instantiated from the same base image. We use Volatility to label memory dumps as ground truth. According to [25], Volatility achieves zero false positives (FPs) and false negatives (FNs) for most of its plugins for non-malicious dumps, ensuring the accuracy of our training set labels. Additionally, other solutions, such as DECAF [15], can be used to label memory dumps.

## Cross-Operating System Versions
In the evaluation phase, we demonstrated the robustness of our approach in scenarios such as pool tag attacks, Direct Kernel Object Manipulation (DKOM) process hiding, and random byte mutations. This indicates that our approach can tolerate small changes and manipulations in memory, making it useful in real-world applications. For example, our approach can adapt to system changes across versions and patches. Future work will further explore this adaptability.

## Related Work
Memory forensic analysis aims to extract semantic content of interest from the volatile memory of various platforms and operating systems, including Windows [5, 12], Linux [19, 20], and Android [19, 26–28]. Kernel object recognition is a fundamental task in this field. Approaches can be categorized into two main types based on memory search methods: list-traversal [5, 21] and signature-based scanning [3, 4, 11, 20, 23, 32].

### List-Traversal Approaches
List-traversal approaches start searching objects from the global root in memory, gradually expanding the search scope by traversing along pointer directions. KOP [5] uses inter-procedural points-to analysis to compute all possible types for generic pointers, resolves type ambiguities using a pattern matching algorithm, and leverages knowledge of kernel memory pool boundaries to recognize dynamic arrays.

### Signature Scanning Approaches
Signature scanning approaches sequentially scan the memory image, testing whether observed subsequences match designed object signatures to determine the object type. SigGraph [20] utilizes point-to relations between different objects to generate non-isomorphic signatures for data structures in an OS.

### Dimsum
Dimsum [19] constructs boolean constraints from data structure definitions and memory page contents to build graphical models, recognizing data structure instances in un-mappable memory. It performs probabilistic inference to generate results ranked by probabilities. An object is detected if it satisfies all constraints. While Dimsum has a slightly higher false negative rate than Value-Invariant and SigGraph, it has significantly fewer false positives. Dimsum requires creating many boolean variables for each memory location, making the factor graph large and expensive to resolve. A pre-processing phase reduces the number of locations to test, but it may not be robust if attackers find a way to evade this phase.

### Comparison with DeepMem
DeepMem is fundamentally different. Unlike list-traversal, which relies on finding a special global root, DeepMem examines every segment in memory, evaluating segments and their connections comprehensively. Pointers are used only for topological information propagation, so an unlinked pointer has minimal impact, unlike list-traversal, which fails if a link is broken. Unlike signature-based approaches, which require accurate and robust signatures, DeepMem learns pointer and non-pointer constraints automatically, capturing non-linear relations between nodes. It does not rely on operating system source code or data structure definitions, making it more expressive, accurate, and robust.

We also leverage deep learning techniques [18] in graph modeling, such as node embedding and classification. We use a modified Graph Neural Network [30] to model nodes, preserving local content and contextual topological information through information propagation. Other researchers [8, 38, 39] also use contextual information in graphs. We use Fully-Connected Neural Networks for node property inference, achieving end-to-end learning without domain knowledge or human intervention.

## Conclusion
In this paper, we proposed DeepMem, a graph-based kernel object detection approach. By constructing a whole memory graph and collecting information through topological information propagation, we can scan memory dumps and infer objects of various types quickly and robustly. DeepMem is advanced because it does not rely on operating system source code or kernel data structures, automatically generates features from raw bytes in memory dumps, and is resistant to attacks like pool tag manipulation and DKOM process hiding.

## Acknowledgements
We appreciate the anonymous reviewers for their valuable comments. We thank Zhenxiao Qi for collecting the dataset, Abhishek Srivastava for providing technical suggestions on TensorFlow, and Sri Shaila for proofreading. The research is supported by the National Science Foundation under Grant No. 1664315, 1719175, TWC-1409915, and FORCES (Foundations Of Resilient CybEr-Physical Systems), which receives support from the National Science Foundation (NSF award numbers CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166). This work is also supported by the Center for Long-Term Cybersecurity from UC Berkeley. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation.

## References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. TensorFlow: A System for Large-Scale Machine Learning. In OSDI.
...
[42] Fan Zhou, Yitao Yang, Zhaokun Ding, and Guozi Sun. 2015. Dump and Analysis of Android Volatile Memory on WeChat. In Communications (ICC), 2015 IEEE International Conference on.