blocks and dynamically replaces existing big matrix blocks from
cache. In addition, we can also minimize the total cache misses. In
the non-interactive mode, i.e., where a user provides the entire work
load, we replace the cache using furthest in future policy [20]. It is
particularly possible in our case since the work load is known and
most importantly the code is data oblivious meaning data access
3 We discuss the security guarantees of our framework in more detail in section 4.
does not depend on input dataset content rather only on the size.
The furthest in future is known as an optimal policy, where we
replace the cache element that will be required furthest in the
future. On the other hand, in the interactive mode, we replace in
least frequently used model.
Next, the operation in this program is Multiply that performs ma-
trix multiplication over BigMatrix XT and X, which are blocked
into (x2, x3) and (x4, x5). The output will be blocked into x2, x5. So
on and so forth. Now we can compute the over all cost in term of
variables x as follows
3.6 Block Size Optimization
In our experiments, we observed that the cost of each operation
varies depending on the block size. So, we propose an optimization
mechanism that reduces the total cost of a sequence of operations.
We formalize this optimization by assuming that the input program
can be represented as a directed acyclic graph (DAG) of operations.
Let, O = {o1, o2, ..., on} be the set of operations, M = {M1, M2,
...} be all big matrices in the computation that are divided into
blocks, B ∈ Rd be the block dimensions, where d is the number
of dimensions (for simplicity we are considering d = 2), B =
{B1, B2, ...} be the sets of the block dimensions of BigMatrix set
M, Π(oi ,Mi , Bi) is the processing cost of oi on Mi that is blocked
as Bi size blocks, ∆(M, Bi , Bj) is the cost of converting the block
size of BigMatrix M from Bi to Bj, λ(oi , B, B) is the peak memory
required to perform operation oi with input BigMatrix blocked in
B and output BigMatrix blocked in B.
Next, we define functions that will help us define the cost func-
tion. Let, P = {ρ1, ρ2, ..., ρm} be a program defined as DAG of
operation, Op(ρi) ∈ O operation of node ρi, InNodes(ρi) ⊆ P is
the node that is input of ρi, InBlks(ρi) is the sets of input blocks di-
mension of node ρi, InBlks(ρi)[j] is the jth input block dimension
of node ρi, OutBlk(ρi) is the output block dimension of node ρi,
InBigM(ρi) is the set of input BigMatrix of node ρi, OutBigM(ρi) is
the output BigMatrix of node ρi.
Therefore, the cost of operation of node ρi can be defined in the
following:
+ 
cost(ρi) = Π(Op(ρi), InBigM(ρi), InBlks(ρi))
(cid:2)∆(OutBigM(ρj), OutBlk(ρj), InBlks(ρi)[j])(cid:3)
ρj ∈InNodes(ρi)
subject to: λ(Op(ρi), InBigM(ρi), OutBigM(ρi))  50") would be compiled into A = where(person,
’C:3;V:50;O:=’), (assuming that, ‘age’ is in the third column).
Here, the condition is encoded in postfix notation [31]. More specif-
ically, the C:3 part of the expression means the third column, v:50
means value 50 and O:= means operation equal. We choose postfix
notation because it is easy to evaluate. The compiler can also parse
join queries such as:
( 1 )
I = s q l ( " SELECT ∗
FROM person p
JOIN person \ _income pi
ON p . i d = pi . i d
WHERE p . age > 50
AND pi . income > 1 0 0 0 0 0 " )
which will be converted as follows
. . .
t 1 = where ( person ,
/ / person . age i s
in column 3
'C : 3 ; V : 5 0 ; O: = ' )
t 2 = z e r o s ( person . rows ,
set_column ( t2 , 0 ,
t 3 = get_column ( person ,
t 3 )
2 )
0 )
/ / person . i d i s
in column 0
set_column ( t2 , 1 ,
t 1 )
'C : 1 ; V : 1 0 0 0 0 0 ;O: = ' )
t 4 = where ( person_income ,
t 5 = z e r o s ( person_income . rows ,
set_column ( t5 , 0 ,
t 6 = get_column ( person_income ,
t 6 )
2 )
0 )
/ / person_income . i d i s
in column 0
set_column ( t5 , 1 ,
A = j o i n ( t3 ,
t5 ,
. . .
t 4 )
' c : t 1 . 0 ; c : t 2 . 0 ; O: = ' , 1 )
Our compiler also takes into consideration of SQL optimiza-
tions. In our implementation, we applied a few standard heuristics
such as pushing selection operations [27]. In our future work, we
are considering utilizing optimization engine from popular open-
source databases. However, we also observed that most of these
optimizations heavily depend on existing index and data stored in
the database (e.g., predicate sensitivity). In contrast, our datasets
are encrypted and we protect against data access patterns so index
utilization is not an option for us. Furthermore, optimizations that
depend on data distribution are not applicable due to the sensi-
tive information disclosure issues. It is worth mentioning that, we
only support subset of standard SQL in our current implementation
and our join query requires an additional parameter k that is the
maximum number of row matches from the first table to the second.
3.8 BigMatrix Storage
Next, we briefly discuss how we serialize, encrypt, load and store
the big matrices.
Serialization and Encryption. One important aspect of our frame-
work is that it provides transparent security for large datasets. First,
we compute the number of blocks we need to keep in memory
to perform the intended operations. Next, we compute the total
number of elements that we can keep in memory. Based on these
two values, we partition our matrix into smaller blocks. Also, it is
possible to have edge blocks in a BigMatrix, which does not have
the same number of elements compared to the rest of the blocks.
We serialize each individual block matrix and encrypt the block
with authenticated encryption AES-GCM [26], and store MAC of
all blocks into their header. We also store the total number rows
and the total number of columns into the header. Essentially with
information from header we can find out the necessary details of a
given block and ensure the authenticity and integrity of the indi-
vidual blocks. Finally, we serialize and encrypt the header. Figure 3
illustrates our serialization process.
Figure 3: Serialization of a Sample BigMatrix.
Storing and Loading mechanism. As we explained in section 2,
we create secure enclaves using Intel SGX API. To write a BigMa-
trix from enclave to disk we designed init_big_matrix_store,
store_block, and store_header OCall functions. The first func-
tion initialized an empty file for a BigMatrix and assign a randomly
generated matrix id to the BigMatrix. The second function stores
a block of a particular BigMatrix. The third one stores the header
of the BigMatrix. We need to call the store header function after
writing of all the blocks because our header contains MAC of all
the individual blocks to provide the integrity protection. Similarly,
we defined load_header and load_block OCall functions to load
header and blocks of an existing BigMatrix, respectively.
During code execution in the execution engine, we also keep
an internal table of id and header MAC. Every time we store a
BigMatrix using store_header function, we store the header MAC
and matrix id. Every time we load a BigMatrix using load_header
function, we check the header MAC and stop execution in case of
MAC mismatch.
3.9 Writing Customized Operations
In addition to our own basic operations, an expert programmer can
provide customized code to be executed as operations. We designed
our code in such a way that the user just needs to provide us an im-
plementation of a predefined abstract class and add the class name
in a configuration file. During the build process our build script will
look into the configuration file, generate call table for execution en-
gine. Our internal operations are also implemented using the same
mechanism. However, building customized method requires code
building and can easily introduce unintentional vulnerabilities. Fur-
thermore, the programmers need to guarantee data obliviousness
of the implementation. In our current implementation, compiler
considers the input sizes as trace of the implementation. In addition,
the current version of our language does not support functions yet.
We are planning to add the function support in future version.
BigMatrix(0, 0)(0, 0)(4, 3)(0, 1)(0, 1)(4, 3)... ... ...HeaderBigMatrix SerializationHeader MACMatrix InfoMAC(0,0)MAC(0,1)... ... ...HeaderSerializationEncryptionHeaderIVMACEncrypted Serialized MatrixMatrix... ... ...4 SECURITY ANALYSIS
In this section, we give an overview of the oblivious execution guar-
antees provided by our system. As we discuss in subsection 3.4, our
framework is designed to detect any modification to the underlying
data and program execution. Furthermore, we assume that due to
SGX capabilities, a malicious attacker cannot observe the register
contents. So an attacker can only observe the memory and disk
access patterns. Below, we formally define what is leaked during
the program execution for an adversary that can observe only mem-
ory and disk access patterns. Protection against other type of side
channel attacks such as timing, energy consumption is outside the
scope of this work.
4.1 Composition Security
Let, D = {D1, .., Dα } be the input data, I = {I1, ...Iα } be the
encrypted input data, R = {R1, ..., Rβ} be the intermediate output
set, R = {R1, ...Rβ} be the encrypted intermediate output set, O be
the output, O be the encrypted output, F = {F1, ..., Ff } be the set
of available oblivious functions, where each function Fi takes the
predefined number of inputs from I∪R and outputs the predefined
number of outputs from R ∪ {O} set. Cη = {F1, ..., Fη} be what
the code participants agreed on. Here, Cη is a combination of η
functions from F.
• Input Access Pattern (Ap): Suppose Fi is the ith func-
tion executed in Cη and during the execution Fi accessed
{I1, ...,Iz}, i.e., Fi depends on {I1, ...,Iz}, then, Ap i
= {1,
..., z}. Finally, Ap(Hη) is defined as the sequence of all the
Ap i
. The input access pattern captures the access sequence
of input data during the secure code execution.
• Intermediate Access Pattern (Bp): Suppose Fi is the ith
function executed in Cη and during the execution Fi ac-
cessed {R1, ..., Rz}, i.e., Fi depends on {R1, ..., Rz}, then,
= {1, ..., z}. Finally, Bp(Hη) is defined as the sequence
Bp i
of all the Bp i
. The intermediate access pattern captures the
access sequence of intermediate data during the secure code
execution.
• Intermediate Update Pattern(Up): Suppose Fi is the ith
function executed in Cη and during the execution Fi modi-
fies {R1, ..., Rz}, e.g., Fi outputs on {R1, ..., Rz}, then, Up i
= {1, ..., z}. Finally, Up(Hη) is defined as the sequence of all
the Up i
. The intermediate update pattern captures the up-
date of intermediate data during the secure code execution.
• History(Hη): The history of the system isHη = (D, R, O, Cη).
• Trace (λ): Let |Ii| be the size of encrypted input Ii, |Ri| be
the size of intermediate output Ri, and |O| be the size of
the output. Then, trace λ(Hη) = {(|I1|.., |Iα |), (|R1|.., |Rβ |),
|O|, Ap(Hη), Bp(Hη), Up(Hη)}. Trace can be considered
as the maximum amount of information that a data owner
allows its leakage to an adversary.
• View (v): The view of an adversary observing the system is
v(Hη) = {I, R, O}. View is the information that is accessi-
ble to an adversary.
Now, there exists a probabilistic polynomial time simulator S that
can simulate the adversary’s view of the history from the trace.
Theorem 4.1. The proposed function composition does not reveal
anything other than the view v.