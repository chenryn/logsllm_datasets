and perform a trafﬁc conﬁrmation attack [37], which in
our system will happen with probability c2, where c is
the fraction of compromised bandwidth in the network
— the same probability as in the current Tor network.
Although route ﬁngerprinting does not result in a di-
rect loss of anonymity in PIR-Tor, the information leaked
could be used by the adversary to relate connections from
the same user and construct behavioral proﬁles. In turn,
these proﬁles can lead to the re-identiﬁcation of users
directly [16] or by combining them with publicly avail-
able databases [14, 30, 43]. We note that the linkabil-
ity of circuits is not a problem unique to PIR-Tor, and
that features other than partitioning the network (e.g.,
cookies [36], session timing [17], or frequently accessed
hosts [17]) can be used in the current Tor network to pro-
ﬁle users.
Impact of ﬁngerprinting on PIR-Tor
6.1
Before diving into the analysis we note that the number
of relays (or descriptors) in each PIR block is irrelevant
for the result. Fingerprinting attacks are based on the
clients’ knowledge of relays in the network, but in PIR-
Tor clients retrieve blocks that may contain one or more
descriptors. Hence, either the client knows about all the
descriptors in a block or she does not know any of them.
Thus, from the point of view of the adversary all relays in
a block are equivalent, regardless of how many descrip-
tors are in this block; only the number of blocks matters
when computing the probabilities we use in our analysis.
We consider an adversary that controls the receiver of
the communication, and thus can observe the exit relay
chosen by the client. Additionally, she may also control
the exit relay hence also learning the middle relay in the
client’s circuit.
6.1.1 One PIR request per circuit construction
If the computation and communication cost for clients
and directory servers in dealing with PIR queries is small
(as when ITPIR is used), clients could request new de-
scriptors for each circuit construction. Regardless of
the selection algorithm used, due to the PIR properties,
the adversary cannot distinguish which block is retrieved
from the database with each query and hence she gains
no information as to which relays are known to the client.
In this setting the adversary must assume that all relays
are known to the client, and PIR-Tor ﬁngerprinting resis-
tance is equivalent to that of the current Tor network.
Nevertheless, when CPIR is used we must expect lim-
itations both in bandwidth and computation capabilities.
Therefore, each time the client obtains a set of descrip-
tors with a CPIR query, these descriptors may have to be
reused across multiple circuit rebuilds. In the next sec-
tion we evaluate the impact of this reuse on the privacy
protection offered by PIR-Tor.
6.1.2 Reusing descriptors for circuit construction
In our analysis we assume that the attacker observes
the exit relay (respectively exit and middle relays) of a
client’s circuit. As we have already discussed, this does
not directly leak information about the client’s identity
and anonymity is preserved. However, the adversary can
still proﬁle clients based on their network knowledge,
eventually leading to de-anonymization [14, 16, 30, 43].
The adversary can construct a behavioral proﬁle with
all connections she observes coming from exit relays (re-
spectively exit and middle relays) that belong to the same
PIR block. If the selection algorithm is such that many
clients have knowledge of a block (recall that all relays
in the block are equivalent for the attacker) the proﬁle
recovered by the adversary is an aggregate proﬁle of all
these users, jeopardizing the de-anonymization of indi-
vidual clients. On the other hand, if the choice of relays
is unique to each client the proﬁle recovered by the ad-
versary accurately reﬂects the behavior of an individual
user and the danger of de-anonymization grows. There-
fore, it is desirable that clients share choices such that
the adversary can only obtain aggregated proﬁles that
reduce her precision when re-identifying clients. Other
ways than relay selection for the attacker to link and/or
discriminate clients’ connections [17, 36] are left out of
the scope of our analysis.
In this section, we evaluate the protection against pro-
ﬁling provided by PIR-Tor when descriptors have to be
reused across circuit constructions. We aim to answer
the question “how precisely can the adversary assign an
observed connection (exit relay, or exit and middle) to
a unique client?”. We use as a metric the fraction α of
clients that could be initiators of a connection (i.e., the
expected fraction of clients that have knowledge of the
PIR-Tor block containing the relay(s) observed by the
adversary). The larger the fraction of clients that may
know the observed relay, the better privacy users enjoy
because the adversary can only construct aggregate pro-
ﬁles. We note that even if the adversary is actually col-
lecting information from a single user, she cannot be sure
that this is the case based on the PIR-Tor relay selection
algorithm; she must assume that the proﬁle she observes
may contain sessions from multiple users. We also note
that, based on the relay selection algorithm, the adver-
sary cannot link connections from a user routed through
different exit relays. This is because the PIR properties
prevent the attacker from learning any relation between
the descriptors retrieved by a client. Hence, the connec-
tions of one client routed through exit relays in different
PIR blocks are unlinkable and the adversary must assign
them to different proﬁles (that may or may not contain
information about other users).
If the adversary observes connections coming from the
exit relay e, the fraction of clients α that may know this
relay are those who retrieved from the database the block
containing e. In PIR-Tor we assume that clients retrieve
a set B of b blocks every time they query the directory
server, hence the fraction of clients that have knowledge
of the block containing e is: α = (1 − (1 − Pr[e])b),
where Pr[e] is the probability of choosing the block con-
taining e as one of the b retrieved blocks, and depends
on the algorithm used for the selection of relays. For
simplicity in our analysis we assume that there is only a
single standard exit policy.
We explained in Section 5 that for load balancing, re-
lays with higher capacities are selected with a higher
probability. We described two criteria for selecting re-
lays: a bandwidth-based criterion (BW), and the Snader-
Borisov criterion (SB). To evaluate the BW criterion ac-
cording to a realistic bandwidth distribution we captured
a snapshot of the Tor consensus directory on 9 February
2011. This directory includes 649 exit relay descriptors
after removing the slowest one-eighth of the total relays
that are not used to relay trafﬁc at all in the current Tor
network [31]. For the evaluation of SB we computed the
probability Pr[e] according to the algorithm introduced
in [41]. Given the function fs(x) = (1−2sx)
(1−2s) a value x
is drawn uniformly at random from [0, 1), and the block
with index (cid:98)Nblocks × fs(x)(cid:99) is selected. The inverse of
(x) = (log2(1 −
the function fs(x) is the function f−1
(1 − 2s) · x))/s. Then, the probability of selecting a
block containing the relay e in the i-th position of the list
((i − 1)/Nblocks). We
is Pr[e] = f−1
use s = 1, which results in a probability distribution near
to uniform, and s = 10, which results in a distribution
very skewed towards the relays offering high bandwidth.
Figure 2 shows box plots3 describing the distribution
(i/Nblocks) − f−1
s
s
s
3The line in the middle of the box represents the median of the
distribution of α. The lower and upper limits of the box correspond,
respectively, to the ﬁrst (Q1) and third quartiles (Q3) of the distribution.
We also show the outliers: relays e which are chosen with values that
are “far” from the rest of the distribution (α > Q3 + 1.5(Q3 − Q1)
database, respectively. In the latter case, the adversary
still captures aggregated proﬁles of 190 clients for the
median relay.
If besides the receiver of the communication the ad-
versary also controls the exit relay, then she can observe
the middle and exit relays of the client’s path. Let us call
the observed exit relay e, and the observed middle relay
m. The fraction of clients knowing the blocks contain-
ing these relays is: Pr[e, m ∈ B] = (1 − (1 − Pr[e])b) ·
(1−(1−Pr[m])b), where Pr[e] and Pr[m] depend on the
path selection algorithm. Hence, Pr[e, m ∈ B] is orders
of magnitude smaller than Pr[e ∈ B] increasing the ac-
curacy of proﬁling, as it becomes less likely that clients
share knowledge of both exit and middle relays.
We note that the results above represent the case in
which clients only retrieve b = 1 blocks per PIR query.
If the clients retrieve more blocks they can signiﬁcantly
improve their privacy protection (α grows approximately
linearly with b). Moreover, if clients retrieve b > 1
blocks each time, they divide by b the number of cir-
cuits routed by each of the known exit relays. Finally,
we would like to stress that client’s proﬁles are only link-
able until they refresh their network knowledge. If, as in
the current Tor network, this happens each 3 hours and
circuits are rebuilt every 10 minutes, the adversary can
link data from only 18/b circuits. We have shown in this
section that, even though it does not break anonymity,
reusing descriptors breaks the unlinkability of circuits.
In order to prevent the attack we have discussed, clients
should request new blocks from the directory server (or
from the guard nodes if ITPIR is used) often or in groups
of several blocks such that the reuse of descriptors is min-
imized.
7 Performance Evaluation of Computa-
tional PIR
We now present experimental results for the CPIR archi-
tecture. We chose standard security parameters for the
CPIR scheme [1] ((cid:96)0 = 19 and N = 50), and computed
the client/server computation times and communication
costs by running an implementation of this scheme [15].
The hardware was a dual Intel Xeon E5420 2.50 GHz
quad-core machine running Ubuntu Linux 10.04.1. Note
that for our evaluation, we used only a single core, which
is equivalent to a standard desktop machine today.
We set the descriptor size to be 2 100 bytes (the maxi-
mum descriptor size measured from the current Tor net-
work), and set the exit database to be half the size of the
middle database [45]. We varied the number of relays
in a PIR database, and computed a) PIR server computa-
tion, b) total communication, and c) client computation.
Data transfer for CPIR schemes can be reduced us-
Figure 2: BW and SB(s) selection: evolution of α with
the database size.
of α for the different selection algorithms. We choose
two database sizes to show the performance of the al-
gorithms when the network scales. A small database
that contains 649 exit relays (as in the current Tor net-
work) divided in 13 blocks for optimal performance of
the CPIR algorithm (note that if ITPIR is used there is
no need to reuse relays across circuit rebuilds).4 The sec-
ond database contains 1M relays, divided in 148 blocks.
In the BW case, we construct the distribution of band-
width amongst the relays by concatenating copies of the
original list downloaded from the Tor network. We refer
the reader to the extended version of this paper [26] for
a more detailed analysis of the evolution of α when the
network scales.
The median of the BW distribution is α = 0.016; that
is, 1600 clients have knowledge of each relay when the
network is used by 100 000 users.5 When the network
grows, the median of α diminishes to 0.0012. As there
are more blocks in the database clients have more choice,
and so they share knowledge of fewer relays.
We can see that SB(1) offers the best protection (a
larger fraction of clients know a relay), but as clients’
choice of relays, and hence blocks, is nearly uniform it
does not load balance the network. The means of SB(1)
and SB(10) are similar; however, SB(10) has a greater
variance. SB(10) yields medians of α = 0.022 and
α = 0.0019 when there are 13 and 148 blocks in the
or α < Q1 − 1.5(Q3 − Q1)).
We have cut the ﬁgure’s y axis for better visibility. The ﬁgure does
not show two outliers for the 13-block BW and SB(10) plots that have
α = 0.38 and α = 0.63, respectively.
4For a database of r 2100-byte descriptors and recursion parameter
(see Section 7) R = 2, the optimal number of blocks is approximately
1.50 · 3√
r.
5The Tor project reports an estimate of Tor users between 100 000
and 250 000 in January 2011 (http://metrics.torproject.
org/users.html). We take 100 000 to represent the worst-case
scenario for the clients.
BWSB(1)SB(10)BWSB(1)SB(10)00.050.10.150.20.250.3α(13 blocks)(148 blocks)(a) Server computation
(b) Total communication
(c) Client computation
Figure 3: CPIR cost. R denotes the recursion parameter in CPIR.
ing the recursive construction by Kushilevitz and Ostro-
vsky [21] without much increase in computational cost;
this recursion can be implemented in a single round of in-
teraction between the client and the server. We denote the
recursion parameter in CPIR using R. If we denote the
number of relays in the database by n, then the commu-
nication cost of CPIR in our architecture is proportional
to 8R · n1/(R+1).
Figure 3 depicts the server computation, communica-
tion, and client computation as a function of the number
of Tor relays for varying values of the recursion param-
eter R. Increasing R reduces communication (and client
computation) drastically while having only a small im-
pact on server computation. Note that for beyond R = 3,
communication increases again, because the term 8R in
the communication overhead becomes dominant. We can
see that when the number of relays is less than 20 000, the
server computational overhead using R = 2 is smaller
than R = 3, while the communication overhead using
R = 2 and R = 3 is about the same. Beyond 20 000
relays, using R = 3 results in signiﬁcant communication
savings as compared to R = 2, while the server compu-
tational overhead is about the same for both parameters.
For the remainder of this discussion, we use R = 2. We
can see that as the network size scales, the communica-
tion overhead of CPIR is an order of magnitude smaller
than trivial download of the database. Interestingly, even
at the current network size, the communication overhead
of CPIR is smaller than a trivial download.
Now we discuss the issue of creating multiple cir-
cuits within a 3-hour interval (after which the directory
databases are refreshed and clients request new descrip-
tors). In this scenario, the trivial download has the ad-
vantage that any number of circuits can be created. Tor
clients rebuild a circuit after every 10 minutes, so they
could create 18 circuits every 3 hours with the commu-
nication overhead of a single trivial download. On the
other hand, the PIR-based architecture would require 18
PIR queries for middle nodes and another 18 for exit
nodes. We can see that unless the number of relays in the
database is greater than 40 000, trivial download is go-
ing to be more efﬁcient than performing multiple CPIR
queries. Instead, we propose to perform b < 18 queries
for both middle and exit nodes, and reuse existing blocks
for more circuits. As we discuss in the security analysis,
reusing blocks does not affect the anonymity of a single
circuit, but may break the unlinkability of multiple cir-
cuits.
We now study some particular scaling scenarios in
more detail. For each of the following scenarios, we
will compute the number of cores required to support
the clients. Figure 4 depicts the required number of
CPU cores as a function of relays and clients. We also
study the communication overhead of CPIR-Tor, along
with a comparative analysis with the current Tor proto-
col. For this analysis, we set the number of blocks b = 1.
Note that both computation and communication over-
head for CPIR-Tor scale linearly with the desired number
of blocks. Our results are summarized in Table 1.
Scenario 1: Current Tor Size. Total number of si-
multaneous relays is 2 000. Total number of simulta-
neous clients is 250 000. For 2 000 relays, server com-
pute time is 0.2 second. The number of exit nodes is
around 1 000, and the corresponding server compute time
is 0.1 seconds. Thus to download a block from both the
middle and the exit databases, the total server compute
time is 0.3 seconds. Note that we are proposing to down-
load a block every 3 hours. A single directory server
would thus be able to support 36 000 clients(cid:0) 3·60·60
(cid:1).
0.3
The total number of cores required to support 250 000
clients is only 7. As of February 2011, the size of the
Tor network consensus is 560 KB, while the total size