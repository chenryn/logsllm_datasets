γ
Proof. If Pr[t] ∈ {0, 1}, γ(t) = 1 and the lemma trivially
holds. If 0 < Pr[t] < 1, Bayes’ theorem gives us
Pr[t | S] =
1
1 + Pr[S|¬t]
Pr[S|t]
.
Pr[¬t]
Pr[t]
(8)
1Although we talk about adversaries with bounded priors,
we use the term restricted instead of bounded here, as DB al-
ready denotes the family of bounded MI distributions in [14].
Theorem 2. A mechanism A satisﬁes (γ, D[a,b]
some 0 < a ≤ b < 1, if A satisﬁes -bounded DP, where
B )-PMP, for
(cid:40)
(cid:16) (1−a)γ
e =
min
γ+b−1
1−aγ , γ+b−1
b
(cid:17)
Proof. Recall that satisfying -bounded diﬀerential privacy
is equivalent to satisfying (e, DB)-PMP. Using (11), we want
γ = max
(e − 1)b + 1,
e
(e − 1)a + 1
.
(12)
b
(cid:18)
if aγ < 1,
otherwise .
(cid:19)
Solving for  yields the desired result.
γ
Note that when aγ ≥ 1, the ﬁrst condition of PMP, namely
Pr [t | S] ≤ γ Pr[t] is trivially satisﬁed. Thus, in this case
we have to satisfy only the second condition, Pr [¬t | S] ≤
, which is satisﬁed if γ = (e − 1)b + 1. We thus arrive
Pr[¬t]
at a full characterization of the level of diﬀerential privacy
to satisfy, if we wish to guarantee a certain level of positive
membership-privacy for subfamilies of DB. For a ﬁxed level
of privacy γ and 0 < a ≤ b < 1, protecting against adver-
saries from a family D[a,b]
B will correspond to a weaker level
of diﬀerential privacy, and thus to less perturbation of the
mechanism’s outputs, compared to the distribution family
DB. Therefore, by considering a more restricted adversarial
setting, we could indeed reach a higher utility for a constant
level of protection against positive membership disclosure.
These results lead to the following simple model for the
selection of an appropriate level of diﬀerential privacy, in a
restricted adversarial setting.
Selecting a level of DP
1: Identify a practically signiﬁcant adversarial model cap-
tured by some distribution family D[a,b]
B .
2: Select a level γ of PMP, providing appropriate bounds
on the adversary’s posterior belief.
3: Use Theorem 2 to get the corresponding level of DP.
As an example, assume a PMP parameter of 2 is consid-
ered to be a suitable privacy guarantee. If our adversarial
model is captured by the family DB, then (ln 2)-DP provides
the necessary privacy. However, if a reasonable adversarial
setting is the family D0.5
B , then the same privacy guaran-
tees against membership disclosure are obtained by satisfy-
ing (ln 3)-DP, with signiﬁcantly less data perturbation.
3.3 Selecting the Bounds [a, b] in Practice
Selecting appropriate bounds [a, b] on an adversary’s prior
belief (about an individual’s presence in a dataset) is pri-
mordial for our approach, yet might prove to be a diﬃcult
task in practice. One possibility is to focus on privacy guar-
antees in the presence of a particular identiﬁed adversarial
threat. In Section 4.2, we will consider a famous attack on
genome-wide association studies, and show how we can de-
ﬁne bounds on the adversary’s prior, in the presumed threat
model. Such bounds are inherently heuristic, as they derive
from a particular set of assumptions about the adversary’s
power, that might fail to be met in practice. However, we
will show in Section 3.4, that our methods also guarantee
some (smaller) level of privacy against adversaries whose
prior beliefs fall outside of the selected range.
Finally, another use-case of our approach is for obtaining
upper bounds on the utility that a mechanism may achieve,
Figure 2: Bounds on an adversary’s posterior belief when
satisfying (2, DB)-PMP.
By Theorem 1, we know that providing (γ, DB)-PMP is
equivalent to satisfying (ln γ)-bounded DP. By Lemma 2,
we then get
Pr[t | S] ≤
1
1 + γ−1 1−Pr[t]
Pr[t]
=
Pr[¬t | S] ≥
1
1 + γ Pr[t]
Pr[¬t]
=
γ · Pr[t]
(γ − 1) Pr[t] + 1
Pr[¬t]
(γ − 1) Pr[t] + 1
.
(9)
(10)
From this lemma, we get that γ(t) < γ for all entities
for which 0 < Pr[t] < 1. Thus, as mentioned previously,
(γ, DB)-PMP actually gives us a privacy guarantee stronger
than the bounds (2) and (3), for all priors bounded away
from 0 or 1. To illustrate this, Figure 2 plots the two dif-
ferent bounds on the posterior probability, when satisfying
(2, DB)-PMP.
Let A be a mechanism satisfying (γ, DB)-PMP. If we were
to consider only those distributions in DB corresponding to
prior beliefs bounded away from 0 and 1, then A would
essentially satisfy PMP for some privacy parameter larger
than γ. This privacy gain can be quantiﬁed as follows. From
Lemma 3, we immediately see that if we satisfy (γ, DB)-
PMP, then we also satisfy (γ(cid:48), D[a,b]
(γ − 1)b + 1,
B )-PMP, where
= max
(cid:19)
(cid:18)
. (11)
(cid:48)
γ
γ
t∈U γ(t) = max
(γ − 1)a + 1
As γ(cid:48) < γ, this result shows (quite unsurprisingly) that if
we consider a weaker adversarial model, our privacy guaran-
tee increases. Conversely, we now show that for a ﬁxed pri-
vacy level, the relaxed adversarial model requires less data
perturbation. Suppose we ﬁx some positive membership-
privacy parameter γ. We know that to provide (γ, DB)-
PMP, we have to satisfy (ln γ)-DP. However, our goal here
is to satisfy (γ, D[a,b]
B )-PMP for a tight value of γ. The fol-
lowing theorem shows that a suﬃcient condition to protect
positive membership-privacy against a bounded adversary is
to provide a weaker level of diﬀerential privacy.
00.20.40.60.8100.10.20.30.40.50.60.70.80.91priorposterior  posteriorboundwithγposteriorboundwithγ(t)when guaranteeing γ-PMP against a so-called uninformed
adversary.
If the dataset size N and the size of the uni-
verse U are known, such an adversary a priori considers all
individuals as part of the dataset with equal probability N|U| .
3.4 Risk-Utility Tradeoff
We have shown that focusing on a weaker adversary leads
to higher utility, yet we must also consider the increased pri-
vacy risk introduced by this relaxation. Suppose our goal is
to guarantee e-PMP. If we consider the adversarial model
captured by the full family DB, A must satisfy -DP. If we
instead focus on the relaxed family D[a,b]
B , it suﬃces to guar-
antee (cid:48)-DP, where (cid:48) is obtained from Theorem 2.
Now suppose our mechanism satisﬁes (e, D[a,b]
B )-PMP, but
there actually is an entity for which the adversary has a
prior Pr[t] /∈ ([a, b] ∪ {0, 1}). Although our mechanism will
not guarantee that conditions (2) and (3) hold for this en-
tity, a weaker protection against membership disclosure still
holds. Indeed, since our mechanism satisﬁes (cid:48)-DP, it also
satisﬁes (e(cid:48)
, DB)-PMP by Theorem 1, and thus guarantees
that bounds (2) and (3) will hold with a factor of e(cid:48)
, rather
than e. In conclusion, satisfying -DP corresponds to sat-
isfying e-PMP for all entities, regardless of the adversary’s
prior. Alternatively, satisfying (cid:48)-DP is suﬃcient to guaran-
tee e-PMP for those entities for which the adversary has a
bounded prior Pr[t] ∈ [a, b] ∪ {0, 1}, and a weaker level of
e(cid:48)
-PMP for entities whose membership privacy was already
severely compromised to begin with.
3.5 Relation to Prior Work
A number of previous relaxations of diﬀerential privacy’s
adversarial model have been considered. We discuss the re-
lations between some of these works and ours in this section.
A popular line of work considers distributional variants
of diﬀerential privacy, where the dataset is assumed to be
randomly sampled from some distribution known to the ad-
versary. Works on Diﬀerential-Privacy under Sampling [13],
Crowd-Blending Privacy [8], Coupled-Worlds Privacy [2] or
Outlier Privacy [15] have shown that if suﬃciently many
users are indistinguishable by a mechanism, and this mech-
anism operates on a dataset obtained through a robust sam-
pling procedure, diﬀerential privacy can be satisﬁed with
only little data perturbation. Our work diﬀers in that we
make no assumptions on the indistinguishability of diﬀer-
ent entities, and that our aim is to guarantee membership
privacy rather than diﬀerential privacy. Another main dif-
ference is in the prior distributions of the adversaries that we
consider. Previous works mainly focus on the unbounded-
DP case, and thus are not directly applicable to situations
where the size of the dataset is public. Furthermore, pre-
viously considered adversarial priors are either uniform [13,
2] or only allow for a ﬁxed number of known entities [8, 15].
Finally, very few results are known on how to design general
mechanisms satisfying distributional variants of DP. In our
work, we show how diﬀerent levels of DP, for which eﬃcient
mechanisms are known, can be used to guarantee PMP for
various adversarial models. Alternatively, Diﬀerential Iden-
tiﬁability [12] was shown in [14] to be equivalent to PMP
under a family of prior distributions slightly weaker than
the ones we introduce here, namely where all entities have
a prior Pr[t] ∈ {0, β} for some ﬁxed β.
4. EVALUATION
Having provided a theoretical model for the characteri-
zation of DP for adversaries with bounded priors, we now
evaluate the new tradeoﬀ between privacy and utility that
we introduce when considering adversarial models captured
by a family D[a,b]
B . We can view an adversary with a prior
in this family as having complete certainty about the size
of the dataset, as well as some degree of uncertainty about
its contents. Scenarios that nicely ﬁt this model, and have
been gaining a lot of privacy-focused attention recently, are
genome-wide association studies (GWAS). We will use this
setting as a case study for the model we propose for the
selection of an appropriate DP parameter.
4.1 Genome-Wide Association Studies
Let us begin with some genetic background. The human
genome consists of about 3.2 billion base pairs, where each
base pair is composed of two nucleobases (A,C,G or T).
Approximately 99.5% of our genome is common to all hu-
man beings. In the remaining part of our DNA, a single nu-
cleotide polymorphism (SNP) denotes a type of genetic
variation occurring commonly in a population. A SNP typ-
ically consists of a certain number of possible nucleobases,
also called alleles. An important goal of genetic research is
to understand how these variations in our genotypes (our
genetic material), aﬀect our phenotypes (any observable
trait or characteristic, a particular disease for instance).
We are concerned with SNPs that consist of only two alle-
les and occur on a particular chromosome. Each such SNP
thus consists of two nucleobases, one on each chromosome.
An example of a SNP is given in Figure 3. In a given pop-
ulation, the minor allele frequency (MAF) denotes the
frequency at which the least common of the two alleles oc-
curs on a particular SNP.
Figure 3: Example of a SNP. Alice has two G alleles on this
fragment and Bob has one G allele and one A allele.
We use the standard convention to encode the value of
a SNP as the number of minor alleles it contains. As an
example, if a SNP has alleles A and G, and A is the minor
allele, then we encode SNP GG as 0, SNPs AG and GA as 1,
and SNP AA as 2. The MAF corresponds to the frequency
at which SNP values 1 or 2 occur.
Genome-wide association studies (GWAS) are a partic-
ular type of case-control studies. Participants are divided
into two groups, a case group containing patients with a
particular phenotype (a disease for instance) and a control
group, containing participants without the attribute. For
each patient, we record the values of some particular SNPs,
in order to determine if any DNA variation is associated
with the presence of the studied phenotype. If the value of
a SNP appears to be correlated (negatively or positively)
to the phenotype, we say that the SNP is causative, or
associated with the phenotype.
A standard way to represent this information is through a
contingency table for each of the considered SNPs. For a
particular SNP, this table records the number of cases and
controls having a particular SNP value. An example of such
a table is given hereafter, for a GWAS involving 100 cases
and 100 controls. From this table, we can, for instance, read
that 70% of the cases have no copy of the minor allele. We
can also compute the MAF of the SNP as 40+2·50
2·200 = 0.35.
SNP value
Cases
Controls
Total
0
1
2
Total
70
10
20
100
40
30
30
100
110
40