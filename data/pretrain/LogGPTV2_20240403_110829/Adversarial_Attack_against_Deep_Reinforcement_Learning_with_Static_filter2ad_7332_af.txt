### Generative Models

#### PixelDefend
Song et al. (2017) introduced a method called PixelDefend, which leverages generative models to defend against adversarial examples. The authors observed that adversarial examples typically reside in low-probability regions of the training distribution, regardless of the attack type or target model. They found that neural density models are particularly effective at detecting human-imperceptible adversarial perturbations. Based on this discovery, Song et al. proposed PixelDefend, which purifies perturbed images by returning them to the training data distribution. Additionally, they noted that PixelDefend can be combined with other model-specific defenses, enhancing the recovery capabilities of various state-of-the-art defense methods. Experimental results (e.g., Fig. 11) demonstrated that PixelDefend significantly improves the performance of these defense methods.

#### Defense-GAN
Samangouei et al. (2018) made the first attempt to construct a defense model against adversarial attacks using Generative Adversarial Networks (GANs) (Radford et al., 2015). They proposed a new defense policy named Defense-GAN, which utilizes generative models to enhance robustness against both black-box and white-box attacks. This approach does not alter the structure of the classifier or the training process, making it applicable to any classification model. Defense-GAN is effective against various adversarial attacks without requiring knowledge of the adversarial example generation process. Experimental results showed that Defense-GAN outperforms existing defense technologies and can be used as a standalone defense mechanism.

### Discriminative Model

Wang et al. explored a fast method to automatically identify adversarial examples generated for the Value Iteration Network (VIN) path planning, as the success of generating such examples was not guaranteed (Liu et al., 2017). To estimate the success of an attack, they compared the paths on a pair of maps: the normal map and the adversarial map. By visualizing the paths, they transformed different attack results into distinct categories of path images. This allowed them to analyze the possible scenarios of adversarial maps and define four categories: Unreached Path (UrP), Fork Path (FP), Detour Path (DP), and Unchanged Path (UcP).

Based on these categories, they implemented a training-based identification method combining path feature comparison and path image classification. UrP and UcP were identified through path feature comparison, while DP and FP were identified through path image classification. Experimental results (e.g., Fig. 12) showed that this method achieves high accuracy and faster identification compared to manual observation.

### Characterizing Adversarial Subspaces

Ma et al. (2018) provided the first analysis of how adversarial perturbations affect the Local Intrinsic Dimensionality (LID) (Houle, 2017) characteristic of adversarial regions. They empirically demonstrated that LID characteristics can help distinguish adversarial examples generated by several state-of-the-art attacks. Their experiments on five attack strategies (FGSM, BIM-a, BIM-b, JSMA, Opt) across three benchmark datasets (MNIST, CIFAR-10, SVHN) showed that LID-based methods outperform most state-of-the-art methods. Ma et al. concluded that their LID analysis not only motivates new directions for effective adversarial defense but also provides challenges for developing new adversarial attacks, thereby improving our understanding of the vulnerabilities in Deep Neural Networks (DNNs).

### Conclusion and Discussion

This paper presents a comprehensive survey of adversarial attacks in the context of reinforcement learning under AI security. Reinforcement learning is crucial for AI applications ranging from Atari games to Connected and Automated Vehicle Systems (CAVs). Ensuring the security of these systems is more critical than ever. Huang et al. (2017) discovered that adversarial attacks are effective against neural networks in reinforcement learning, inspiring innovative research in this area. Our work reviews significant contributions, focusing on the most influential and interesting works in the field. We provide a detailed introduction to the literature on adversarial attacks in various reinforcement learning applications and briefly analyze the most valuable defense technologies (Table 4).

Although reinforcement learning systems are vulnerable to adversarial attacks, complete black-box attacks are rare, making it difficult for adversaries to attack these systems in practice. Given the high activity in this research direction, it is expected that highly reliable reinforcement learning systems will be available in the future to support critical security applications in AI.

### Acknowledgements

The authors thank Professors Wenjia Niu and Jiqiang Liu for their guidance. This research is supported by the National Natural Science Foundation of China (No. 61672092), Science and Technology on Information Assurance Laboratory (No. 614200103011711), the Project (No. BMK2017B02-2), Beijing Excellent Talent Training Project, the Fundamental Research Funds for the Central Universities (No. 2017RC016), the Foundation of China Scholarship Council, and the Fundamental Research Funds for the Central Universities of China under Grants 2018JBZ103.

### Funding

This research is supported by the National Natural Science Foundation of China (No. 61672092), Science and Technology on Information Assurance Laboratory (No. 614200103011711), the Project (No. BMK2017B02-2), Beijing Excellent Talent Training Project, the Fundamental Research Funds for the Central Universities (No. 2017RC016), the Foundation of China Scholarship Council, and the Fundamental Research Funds for the Central Universities of China under Grants 2018JBZ103.

### Availability of Data and Materials

Not applicable.

### Authors’ Contributions

TC conceived and designed the study. TC and YX wrote the paper. JL, WN, ET, and ZH reviewed and edited the manuscript. All authors read and approved the manuscript.

### Authors’ Information

Wenjia Niu obtained his Bachelor's degree from Beijing Jiaotong University in 2005 and his PhD from the Chinese Academy of Sciences in 2010, both in Computer Science. He is currently a professor at Beijing Jiaotong University, with research interests in AI Security, Agent, and Data Mining. He has published over 50 research papers in renowned international journals and conferences, including KAIS (Elsevier), ICDM, CIKM, and ICSOC. He has published two edited books and served on the Steering Committee of ATIS (2013-2016) and as the PC Chair of ASONAM C3'2015. He has been a PhD Thesis Examiner at Deakin University and a guest editor for several journals, including the Chinese Journal of Computer, Enterprise Information System, Concurrency and Computing: Practice and Experience, and Future Generation Computer Systems. He is a member of both IEEE and ACM.

### Competing Interests

The authors declare that they have no competing interests.

### Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Received: 31 October 2018  
Accepted: 12 February 2019

### References

[A list of references follows, formatted consistently with the original text.]