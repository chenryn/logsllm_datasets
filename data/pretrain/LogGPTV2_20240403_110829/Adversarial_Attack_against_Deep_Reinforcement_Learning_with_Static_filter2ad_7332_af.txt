Generative models
• PixelDefend
Song et al. (2017) proposed a method named Pix-
elDefend which can utilized generative models to defend
against adversarial examples.
In this paper, authors
showed that the adversarial examples mainly lie on low
probability regions of training distribution, regardless of
the attack type and target model. Moreover, they found
that neural density model outperform on detecting the
human invisible adversarial perturbations, and based on
this discovery, Song et al. proposed a new approach
named PixelDefend which can purifies a perturbed image
return to the distribution of training data. Meanwhile,
they announced that PixelDefend can be utilized as
a novel family of methods which can combined with
other model-specific defenses. Experimental results (e.g.,
Fig. 11) showed that PixelDefend can greatly improves
the recovery capability of varieties state-of-art defense
methods.
• Defense-GAN
Samangouei et al. (2018) gave the first attempt to con-
struct a defense model against adversarial attack based on
GAN (Radford et al. 2015). They proposed a new defense
policy named Defense-GAN which takes use of generation
model to improve the robustness against Black/White-
Box Attack. Moreover, any classification model can uti-
lize the Defense-GAN proposed in this paper, and will
not change the structure of classifier or the process for
training. Defense-GAN can be used as a defense tech-
nology that can against any adversarial attack as such
method does not assume knowledge of the process for
generating the adversarial examples. The experimental
results showed that Defense-GAN proposed in this paper
is effective when against different adversarial attacks, and
can improve the performance on existing defense tech-
nologies.
Discriminative model
Since it is not guaranteed that the generated adversarial
examples will obstruct the VIN path planning success-
fully generated in Liu et al. (2017), Wang et al. explored
a fast approach to automatically identify VIN adversar-
ial examples. In order to estimate whether an attack is
successful, they compared the difference between the two
paths on a pair of maps, the normal map and the adver-
sarial map. By visualizing the pair of paths on a path
image, they transformed the different attack results into
different categories of path images. In this way, they ana-
lyzed the possible scenarios of the adversarial maps and
define the categories of the predicted path pairs. They
divided the results into four limited categories, which
are the unreached path (UrP) class, the fork path (FP)
class, the detour path (DP) class and the unchanged
path (UcP) class. Based on the categories definition, they
implemented a training-based identification method by
combining the path feature comparison and path images
classification.
In this method, the UrP and UcP can be identified
through path feature comparison and the DP and FP
can be identified through path image classification. The
experimental results showed that this method can achieve
Fig. 11 The example for PixelDefend (Song et al. 2017). The first image denote the original clean image in CIFAR-10 (Krizhevsky et al. 2014), and the
remaining pictures represent the adversarial examples based on varieties attack methods which have been shown above each example, and the
predicted label has been shown on the bottom. Meanwhile, the second line denotes the corresponding purified images
Chen et al. Cybersecurity            (2019) 2:11 
Page 19 of 22
(a)
(b)
(c)
(d)
Fig. 12 Four categories of VIN adversarial maps. The first line denotes the original maps, the second line represents the adversarial eaxmples
generated, and the third line is the extracted path image. a The UrP. b The FP. c The DP. d The UcP
Table 4 Different attacks targeted by different defense technologies
PGM
Modifying input
IFGSM
CDG
DeepFool
C&W
JSMA
ITGSM
FGSM
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Modifying the objective function
FGSM
DeepFool
C&W
Adversarial training
Ensemble adversarial training
Cascade adversarial training
Principled adversarial training
Gradient band-based adversarial training
Data randomization
Input transformations
Input gradient regularization
Adding stability term
Adding regularization term
Dynamic quantized activation function
Stochastic activation pruning
Modifying the network structure
Defensive distillation
High-level representation Guided Denoiser
Add detector subnetwork
Multi-model-based defense
Generative models
Characterizing adversarial subspaces
(cid:2)
(cid:2)
FGSM
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Small perturbations
(cid:2)
PGD
(cid:2)
(cid:2)
(cid:2)
IFGSM
DeepFool
C&W
JSMA
BIM
Opt
ITGSM
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Chen et al. Cybersecurity            (2019) 2:11 
Page 20 of 22
a high-accuracy and faster identification than manual
observation method (e.g., Fig. 12).
Characterizing adversarial subspaces
Ma et al. (2018) gived the first attempt to explain the
extent adversarial perturbation can effect the Local Intrin-
sic Dimensionality (LID) (Houle 2017) characteristic of
adversarial regions. Moreover, they showed empirically
that LID characteristics can facilitate the distinction of
adversarial examples generated by several state-of-art
attacks. Meanwhile, they proved that LID can be utilized
to differentiate adversarial examples, and the experimen-
tal results show that among the five attack strategies
(FGSM (Goodfellow et al. 2014a), BIM-a (Saad 2003),
BIM-b (Saad 2003), JSMA (Papernot et al. 2016b), Opt)
based on three benchmark data sets (MNIST (LeCun et al.
2010), CIFAR-10 (Krizhevsky et al. 2014), SVHN (Netzer
et al. 2011)) considered for this paper, the method based
on LID can outperform against most state-of-art methods.
Ma et al. announced that their analysis of LID charac-
teristic for adversarial region, not only can motivates new
direction for effective adversarial defense, but also pro-
vides more challenges for the development of new adver-
sarial attacks, meanwhile, enable us to better understand
the vulnerabilities of DNNs (LeCun et al. 1989).
Conclusion and discussion
In this paper, we give the very first attempt to con-
duct a comprehensive survey on adversarial attacks in
the context of reinforcement learning under AI security.
Reinforcement learning is a workhorse for AI applications
ranging from Atari Game to Connected and Automated
Vehicle System (CAV), hence, how to build a reliable rein-
forcement learning system to support the security critical
applications in AI, has become a concern which is more
critical than ever. However, Huang et al. (2017) discovered
that the interesting attack mode adversarial attack also be
effective when targeting neural networks under reinforce-
ment learning, which has inspired innovative researches
in this direction. Therefore, our work reviews such con-
tributions, and mainly focus on the most influential and
interesting works in this field. We give a comprehensive
introduction to the literatures on adversarial attack under
various fields of reinforcement learning applications, and
briefly analyze the most valuable defense technologies
against existing adversarial attacks (Table 4).
Although, the RL system does exist the security vulner-
ability of “Adversarial attack”, by the survey on existing
adversarial attack technologies, it is found that the exist of
complete Black-box attacks are rare (complete Black-box
attack means that the adversary has no idea of the tar-
get model, and can not interact with the target agent at
all), which makes it very difficult for adversaries to attack
the reinforcement learning system in practice. Moreover,
owing to the very high activity in this research direction,
it can be expected that, in the future an largely reliable
reinforcement learning system will be available to support
critical security applications in AI.
Acknowledgements
The authors would like to thank the guidance of Professor Wenjia Niu and
Professor Jiqiang Liu. Meanwhile this research is supported by the National
Natural Science Foundation of China (No. 61672092), Science and Technology
on Information Assurance Laboratory (No. 614200103011711), the Project (No.
BMK2017B02-2), Beijing Excellent Talent Training Project, the Fundamental
Research Funds for the Central Universities (No. 2017RC016), the Foundation
of China Scholarship Council, the Fundamental Research Funds for the Central
Universities of China under Grants 2018JBZ103.
Funding
This research is supported by the National Natural Science Foundation of
China (No. 61672092), Science and Technology on Information Assurance
Laboratory (No. 614200103011711), the Project (No. BMK2017B02-2), Beijing
Excellent Talent Training Project, the Fundamental Research Funds for the
Central Universities (No. 2017RC016), the Foundation of China Scholarship
Council, the Fundamental Research Funds for the Central Universities of China
under Grants 2018JBZ103.
Availability of data and materials
Not applicable.
Authors’ contributions
TC conceived and designed the study. TC and YX wrote the paper. JL, WN, ET,
and ZH reviewed and edited the manuscript. All authors read and approved
the manuscript.
Authors’ information
Wenjia Niu obtained his Bachelor degree from Beijing Jiaotong University in
2005, PhD degree from Chinese Academy of Science in 2010, all in Computer
Science. Now he is currently a professor in Beijing Jiaotong University. His
research interests are AI Security, Agent and Data Mining. He has published
more than 50 research papers, including a number of regular papers in the
famous international journals and Conferences, such as KAIS (Elsevier), ICDM,
CIKM and ICSOC. He has published 2 edited books. He serves on the Steering
Committee of ATIS (2013-2016) and the PC Chair of ASONAM C3’2015. He has
been PhD Thesis Examiner of Deakin University, Title Page Click here to
access/download;Title Page;cover letter.pdf the guest editor for the Chinese
Journal of Computer, Enterprise Information System, Concurrency and
Computing: Practise and Experience, and Future Generation Computer
Systems, etc.. He is also members both of the IEEE and ACM.
Competing interests
The authors declare that they have no competing interests.
Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 31 October 2018 Accepted: 12 February 2019
References
Akhtar N, Mian A (2018) Threat of adversarial attacks on deep learning in
computer vision: A survey. arXiv preprint arXiv:1801.00553
Bai X, Niu W, Liu J, Gao X, Xiang Y, Liu J (2018) Adversarial Examples
Construction Towards White-Box Q Table Variation in DQN Pathfinding
Training. In: 2018 IEEE Third International Conference on Data Science in
Cyberspace (DSC). IEEE. pp 781–787
Behzadan V, Munir A (2017) Vulnerability of deep reinforcement learning to
policy induction attacks. In: International Conference on Machine Learning
and Data Mining in Pattern Recognition. Springer, Cham. pp 262–275
Bougiouklis A, Korkofigkas A, Stamou G (2018) Improving Fuel Economy with
LSTM Networks and Reinforcement Learning. In: International Conference
on Artificial Neural Networks. Springer, Cham. pp 230–239
Chen et al. Cybersecurity            (2019) 2:11 
Page 21 of 22
Carlini N, Wagner D (2016) Defensive distillation is not robust to adversarial
examples. arXiv preprint arXiv:1607.04311
Carlini N, Wagner D (2017) Towards evaluating the robustness of neural
networks. In: 2017 IEEE Symposium on Security and Privacy (SP). IEEE.
pp 39–57
Chen QA, Yin Y, Feng Y, Mao ZM, Liu HX (2018a) Exposing Congestion Attack
on Emerging Connected Vehicle based Traffic Signal Control. In: Network
and Distributed Systems Security (NDSS) Symposium
Chen T, Niu W, Xiang Y, Bai X, Liu J, Han Z, Li G (2018b) Gradient band-based
adversarial training for generalized attack immunity of a3c path finding.
arXiv preprint arXiv:1807.06752
Dhillon GS, Azizzadenesheli K, Bernstein JD, Kossaifi J, Khanna A, Lipton ZC,
Anandkumar A (2018) Stochastic activation pruning for robust adversarial
defense. In: International Conference on Learning Representations. https://
openreview.net/forum?id=H1uR4GZRZ
Drucker H, Le Cun Y (1992) Improving generalization performance using
double backpropagation. IEEE Trans Neural Netw 3(6):991–997
Farahmand AM (2011) Action-gap phenomenon in reinforcement learning. In:
Advances in Neural Information Processing Systems. pp 172–180
Goodall C, El-Sheimy N (2017) System and method for intelligent tuning of
Kalman filters for INS/GPS navigation applications: U.S. Patent No.
9,593,952. Washington, DC: U.S. Patent and Trademark Office
Goodfellow IJ, Shlens J, Szegedy C (2014) Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572
Goodfellow IJ, Shlens J, Szegedy C (2014) Explaining and harnessing
adversarial examples. CoRR abs/1412.6572. 1412.6572
Guo C, Rana M, Cisse M, van der Maaten L (2018) Countering adversarial
images using input transformations. In: International Conference on
Learning Representations. https://openreview.net/forum?id=SyJ7ClWCb
Guo X, Singh S, Lee H, Lewis RL, Wang X (2014) Deep learning for real-time
Atari game play using offline Monte-Carlo tree search planning. In:
Advances in neural information processing systems. pp 3338–3346
He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp 770–778
Houle ME (2017) Local intrinsic dimensionality I: an extreme-value-theoretic
foundation for similarity applications. In: International Conference on
Similarity Search and Applications. Springer, Cham. pp 64–79
Huang S, Papernot N, Goodfellow I, Duan Y, Abbeel P (2017) Adversarial
attacks on neural network policies. arXiv preprint arXiv:1702.02284
Jaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo JZ, Silver D, Kavukcuoglu K
(2016) Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397
Jia YJ, Zhao D, Chen QA, Mao ZM (2017) Towards secure and safe appified
automated vehicles. In: 2017 IEEE Intelligent Vehicles Symposium (IV). IEEE.
pp 705–711
Krizhevsky A., Hinton G. (2009) Learning multiple layers of features from tiny
images. Technical report, University of Toronto 1(4):7
Krizhevsky A, Nair V, Hinton G (2014) The cifar-10 dataset. online: http://www.
cs.toronto.edu/kriz/cifar.html
Kurakin A, Goodfellow I, Bengio S (2016) Adversarial machine learning at scale.
arXiv preprint arXiv:1611.01236
LeCun Y, Cortes C, Burges C (2010) Mnist handwritten digit database 2. AT&T
Labs [Online]. Available: http://yann.lecun.com/exdb/mnist
LeCun Y, Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD
(1989) Backpropagation applied to handwritten zip code recognition.
Neural Comput 1(4):541–551
Liang Y, Machado MC, Talvitie E, Bowling M (2016) State of the art control of
atari games using shallow reinforcement learning. In: Proceedings of the
2016 International Conference on Autonomous Agents & Multiagent
Systems. International Foundation for Autonomous Agents and
Multiagent Systems. pp 485–493
Liao F, Liang M, Dong Y, Pang T, Hu X, Zhu J (2018) Defense against adversarial
attacks using high-level representation guided denoiser. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition.
pp 1778–1787
Lin Y-C, Hong Z-W, Liao Y-H, Shih M-L, Liu M-Y, Sun M (2017) Tactics of
adversarial attack on deep reinforcement learning agents. arXiv preprint
arXiv:1703.06748
Liu J, Niu W, Liu J, Zhao J, Chen T, Yang Y, Xiang Y, Han L (2017) A Method to
Effectively Detect Vulnerabilities on Path Planning of VIN. In: International
Conference on Information and Communications Security. Springer,
Cham. pp 374–384
Ma X, Li B, Wang Y, Erfani SM, Wijewickrema S, Houle ME, Schoenebeck G,
Song D, Bailey J (2018) Characterizing adversarial subspaces using local
intrinsic dimensionality. arXiv preprint arXiv:1801.02613
Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A (2017) Towards deep
learning models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083
Markov A (1907) Investigation of a remarkable case of dependent trials. Izv Ros
Akad Nauk 1
Martínez-Tenor Á, Cruz-Martín A, Fernández-Madrigal JA (2018) Teaching
machine learning in robotics interactively: the case of reinforcement
learning with Lego Mindstorms. Interact Learn Environ:1–14
Metzen JH, Genewein T, Fischer V, Bischoff B (2017) On detecting adversarial
perturbations. CoRR abs/1702.04267. 1702.04267
Miyato T, Maeda SI, Ishii S, Koyama M (2018) Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE
Trans Pattern Anal Mach Intell PP(99):1
Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D,
Riedmiller M (2013) Playing atari with deep reinforcement learning. arXiv
preprint arXiv:1312.5602
Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A,
Riedmiller M, Fidjeland AK, Ostrovski G, et al (2015) Human-level control
through deep reinforcement learning. Nature 518(7540):529
Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D,
Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement
learning. In: International conference on machine learning. pp 1928–1937
Moosavi-Dezfooli S-M, Fawzi A, Fawzi O, Frossard P (2017) Universal adversarial
perturbations. arXiv preprint
Na T, Ko JH, Mukhopadhyay S (2018) Cascade adversarial machine learning
regularized with a unified embedding. arXiv preprint arXiv:1708.02582
Netzer Y, Wang T, Coates A, Bissacco A, Wu B, Ng AY (2011) Reading digits in
natural images with unsupervised feature learning
Ohn-Bar E, Trivedi MM (2016) Looking at humans in the age of self-driving and
highly automated vehicles. IEEE Trans Intell Veh 1(1):90–104
Papernot N, McDaniel P, Wu X, Jha S, Swami A (2016a) Distillation as a defense
to adversarial perturbations against deep neural networks. In: 2016 IEEE
Symposium on Security and Privacy (SP). IEEE. pp 582–597
Papernot N, McDaniel P, Jha S, Fredrikson M, Celik ZB, Swami A (2016b) The
limitations of deep learning in adversarial settings. In: 2016 IEEE European
Symposium on Security and Privacy (EuroS&P). IEEE. pp 372–387
Papernot N, Mcdaniel P, Goodfellow I, Jha S, Celik ZB, Swami A (2016c)
Practical black-box attacks against deep learning systems using adversarial
examples. arXiv preprint arXiv:1602.02697 1(2):3
Radford A, Metz L, Chintala S (2015) Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint
arXiv:1511.06434
Rakin AS, Yi J, Gong B, Fan D (2018) Defend deep neural networks against
adversarial examples via fixed anddynamic quantized activation functions.
arXiv preprint arXiv:1807.06714
Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional networks for
biomedical image segmentation. In: International Conference on Medical
image computing and computer-assisted intervention. Springer, Cham.
pp 234–241
Ross AS, Doshi-Velez F (2017) Improving the adversarial robustness and
interpretability of deep neural networks by regularizing their input
gradients. arXiv preprint arXiv:1711.09404
Saad Y (2003) Iterative methods for sparse linear systems, vol. 82. siam
Samangouei P, Kabkab M, Chellappa R (2018) Defense-gan: Protecting
classifiers against adversarial attacks using generative models. arXiv
preprint arXiv:1805.06605
Schulman J, Levine S, Abbeel P, Jordan MI, Moritz P (2015) Trust Region Policy
Optimization. In: Icml Vol. 37. pp 1889–1897
Shalev-Shwartz S, Shammah S, Shashua A (2016) Safe, multi-agent,
reinforcement learning for autonomous driving. arXiv preprint
arXiv:1610.03295
Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche G,
Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M, et al (2016)
Mastering the game of go with deep neural networks and tree search.
Nature 529(7587):484
Sinha A, Namkoong H, Duchi J (2018) Certifiable distributional robustness with
principled adversarial training. In: International Conference on Learning
Representations. https://openreview.net/forum?id=Hk6kPgZA-
Chen et al. Cybersecurity            (2019) 2:11 
Page 22 of 22
Song Y, Kim T, Nowozin S, Ermon S, Kushman N (2017) Pixeldefend: Leveraging
generative models to understand and defend against adversarial
examples. arXiv preprint arXiv:1710.10766
Srisakaokul S, Zhong Z, Zhang Y, Yang W, Xie T (2018) Muldef:
Multi-model-based defense against adversarial examples for neural
networks. arXiv preprint arXiv:1809.00065
Swiderski F, Snyder W (2004) Threat modeling. Microsoft Press
Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, Fergus R
(2013) Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199
Tamar A, Wu Y, Thomas G, Levine S, Abbeel P (2016) Value iteration networks.
In: Advances in Neural Information Processing Systems. pp 2154–2162
Touretzky DS, Mozer MC, Hasselmo ME (eds) (1996) Advances in Neural
Information Processing Systems 8: Proceedings of the 1995 Conference,
vol. 8. Mit Press
Tramèr F, Kurakin A, Papernot N, Boneh D, McDaniel PD (2017) Ensemble
adversarial training: Attacks and defenses. CoRR abs/1705.07204.
1705.07204
Vincent P, Larochelle H, Bengio Y, Manzagol P-A (2008) Extracting and
composing robust features with denoising autoencoders. In: Proceedings
of the 25th international conference on Machine learning. ACM.
pp 1096–1103
Watkins C, Dayan P (1992) Machine learning. Technical Note: Q-Learning
8:279–292
Wold S, Esbensen K, Geladi P (1987) Principal component analysis.
Chemometrics and intelligent laboratory systems 2(1-3):37–52
Xiang Y, Niu W, Liu J, Chen T, Han Z (2018) A PCA-Based Model to Predict
Adversarial Examples on Q-Learning of Path Finding. In: 2018 IEEE Third
International Conference on Data Science in Cyberspace (DSC). IEEE.
pp 773–780
Xie C, Wang J, Zhang Z, Zhou Y, Xie L, Yuille AL (2017) Adversarial examples for
semantic segmentation and object detection. CoRR abs/1703.08603.
1703.08603
Xie C, Wang J, Zhang Z, Ren Z, Yuille A (2018) Mitigating adversarial effects
through randomization. In: International Conference on Learning
Representations. https://openreview.net/forum?id=Sk9yuql0Z
Xiong W, Droppo J, Huang X, Seide F, Seltzer M, Stolcke A, Yu D, Zweig G
(2016) Achieving human parity in conversational speech recognition. arXiv
preprint arXiv:1610.05256
Yan Z, Guo Y, Zhang C (2018) Deepdefense: Training deep neural networks
with improved robustness. CoRR abs/1803.00404. 1803.00404
Yang T, Xiao Y, Zhang Z, Liang Y, Li G, Zhang M, Li S, Wong T-W, Wang Y,
Li T, et al (2018) A soft artificial muscle driven robot with reinforcement
learning. Sci Rep 8(1):14518
Zhang J, Lu C, Fang C, Ling X, Zhang Y (2018) Load Shedding Scheme with
Deep Reinforcement Learning to Improve Short-term Voltage Stability. In:
2018 IEEE Innovative Smart Grid Technologies-Asia (ISGT Asia). IEEE.
pp 13–18
Zheng S, Song Y, Leung T, Goodfellow I (2016) Improving the robustness of
deep neural networks via stability training. In: Proceedings of the ieee
conference on computer vision and pattern recognition. pp 4480–4488
Zhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, Fei-Fei L, Farhadi A (2017)
Target-driven visual navigation in indoor scenes using deep reinforcement
learning. In: 2017 IEEE international conference on robotics and
automation (ICRA). IEEE. pp 3357–3364