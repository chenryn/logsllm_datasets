z(d,u) ≥ yu − yd
That is, if au is 1 but ad is 0, or in other words d computes using πY, variable x(d,u) is forced to 1. Later,
when we minimize the total cost, we multiply x(d,u) by the weight of (d, u), which is the number of times
14
However, a wrinkle arises here. Since there are multiple def-use chains that start at d, the min-cut edge
the min-cut edge of (d, u) executes. Note that if au − ad (or yu − yd) is −1, then x(d,u) (or z(d,u)) would be
0 because of the interval restriction: x(d,u), z(d,u) ∈ {0, 1}.
of (d, u) may already cover a diﬀerent def-use (d, u(cid:48)) yielding constraints
x(d,u(cid:48)) ≥ au(cid:48) − ad
x(d,u) ≥ au − ad
too strong: since x(d,u) already covers (d, u(cid:48)), if both (d, u) and (d, u(cid:48)) require conversion, it is suﬃcient to
perform conversion along the min-cut edge of (d, u); conversion along the min-cut edge of (d, u(cid:48)) would be
redundant. (Clearly, there may be more than one uses for each def, but there is only a single def per use,
due to the SSA property.) We therefore introduce the notion of subsumption.
Deﬁnition 1. Def-use chain (d, u) subsumes def-use chain (d, u(cid:48)), denoted (d, u) ⊇ (d, u(cid:48)), if and only if
min cut(d, u) dominates u(cid:48), or in other words, all paths from d to u(cid:48) go through min cut(d, u).
Intuitively, subsumption means that conversion of d at the min-cut edge of (d, u) covers (d, u(cid:48)) as well,
and there is no need to introduce conversion at the min-cut edge of (d, u(cid:48)). There is no natural case for
subsumption in our running example. For the sake of argument, assume there is a use of rem3 deﬁned at
line 14, in the outer loop at line 20. Then there are def-use chains (14,15) and (14,20). min cut(14, 15) is
edge 14 → 15, and min cut(14, 20) is edge 17 → 20. However, (14,15) subsumes (14,20). Assuming that both
uses, 15 and 20, require conversion, then placing a conversion at 14 → 15 covers (14,15) and (14,20). If 15
does not require conversion but 20 does, then placing a conversion at the less costly edge 17 → 20 suﬃces.
The above deﬁnition gives rise to a directed graph with nodes for all def-use chains (d, u) for d, and
edges due to subsumption: there is an edge from (d, u) to (d, u(cid:48)) if and only if (d, u) ⊇ (d, u(cid:48)). Strongly
connected components (SCCs) in this graph imply several (d, u)’s with the same min-cut edge. We therefore
collapse SCCs into equivalence classes with a representative e—each equivalence class is covered by a min-
cut edge e—and extend the ordering to the representative edges e. For example, suppose we have a chain
d → u1 → u2 → n in one block, where u1 and u2 are uses of d. Suppose we have n → u3 where u3 is a use
in the immediately enclosing block. (d, u1) and (d, u2) are in the same equivalence class with representative
edge d → u1, and (d, u3) is in another class, with representative edge n → u3. We have d → u1 ⊇ n → u3.
e , similar to variables x(d,u) and z(d,u) we
e ∈ {0, 1} denotes whether there
e and zd
e and zd
introduced earlier. In the integer program we use only variables xd
is an Y2A conversion of the variable deﬁned at d on edge e.
We now introduce a new set of constraint variables, xd
e . xd
Therefore, our constraints become:
+ ··· + xd
ek
xd
e1
≥ au − ad
where ek is the representative of (d, u)’s equivalence class, and ei ⊇ ei+1 for 1 ≤ i ≤ k − 1. These constraints
state that if (d, u) requires conversion from Arithmetic to Yao’s protocol, it is suﬃcient to execute that
conversion along a min-cut edge for some (d, u(cid:48)) that subsumes (d, u), even when that edge is not the
min-cut edge for (d, u) itself.
In the above constraint, edge ek is the representative edge for the equivalence class of (d, u). If (d, u) is
a backward chain, then ek is the back edge in u’s block, and e1, . . . ek−1 are forward edges totally ordered
by subsumption. If (d, u) is a forward chain, then all edges are forward edges and totally ordered by sub-
sumption: e1 ⊇ e2 ··· ⊇ ek−1. This structure of constraints that account for conversion helps establish total
unimodularity of the constraint matrix, as we detail in the following section.
To summarize, we have constraints that account for conversion from πY to πA:
+ ··· + xd
ek
xd
e1
≥ au − ad
and parallel constraints that account for conversion A2Y:
≥ yu − yd
+ ··· + zd
zd
e1
ek
15
(2)
(3)
Objective Function The integer programming problem must ﬁnd an assignment for variables an, yn, xd
e
(cid:80)
and zd
e that satisﬁes the above constraints, and minimizes the cost of running the program. The total cost
is the sum of execution cost and conversion cost:
n (an · cA
n · wn + yn · cY
n · wn)
(cid:80)
d,e (xd
e · cY 2A · we + zd
e · cA2Y · we)
+
The ﬁrst summation term models the cost of execution of program statements and is straight-forward. E.g.,
if n runs using πA then its cost would be cA
n . The cost of a single run of n is multiplied by wn, the number
of times n executes. In MPC-source wn is always statically known. The second term models conversion cost
and is less straight-forward. It iterates over all d, e pairs where d is a deﬁnition and e is a min-cut edge
representing some (d, u) (more precisely, an equivalence class of (d, u)’s). we is the number of times the
min-cut edge e executes. Again, in MPC-source we is always statically known. To see the intuition behind
the second term, suppose we have two forward def-use chains (d, u) and (d, u(cid:48)) where (d, u) subsumes (d, u(cid:48))
but not the other way around. (d, u)’s representative is min-cut edge e and (d, u(cid:48))’s representative is e(cid:48). The
e(cid:48) · cY 2A · we(cid:48). If the assignments to
term that accounts for conversions of d (just Y2A), is xd
e(cid:48) · cY 2A · we(cid:48), just
ad and au entail conversion, then xe is 1, and therefore, xe(cid:48)
as expected, since (d, u) subsumes (d, u(cid:48)). Conversely, if ad and au do not entail conversion, then xe is 0.
If (d, u(cid:48)) does require conversion, we will have xe(cid:48)
= 1, thus converting deﬁnition d we(cid:48) times only, where
we(cid:48)  2 protocols changes
the structure of the underlying constraint matrix, in a way that total unimodularity no longer holds. A way
to see this is the following: we used constraints of the type au − ad to capture conversions to arithmetic
from a diﬀerent protocol. In the binary (m = 2) case, ad = 0 implies that node d was computed in πY, and
therefore, au − ad = 1 induces a Y2A conversion. When m = 3, au − ad = 1 would induce a Y2A or a
B2A conversion. As in general, πi2πj and πi(cid:48)2πj conversions have diﬀerent costs, devising the corresponding
constraints to capture conversions becomes non-trivial, and the matrix is no longer totally unimodular.
16
J :(cid:80)
(a) MPC-source with def-use chains
(b) Linearization of B1 and B2
(c) Linearization of B3: Parallel(S)
Figure 3. Natural Schedule. There are no backward def-use chains in B1, and therefore B1 is parallelized, executing
n1(B11) and n1(B12) in parallel, as shown at the top of Fig. (b). (We assume each loop has bound 2. n1(B11) denotes
the execution of n1 in the ﬁrst iteration of B1, and n1(B12) in the second.) There is a backward def-use chain in B2,
(n2, n3) and therefore B2 cannot be parallelized. The two iterations of B2 happen sequentially. There is no backward
def-use chain in B3, therefore B3 can be parallelized too, resulting in the ﬁnal schedule shown in (c). Fig. 3(c) shows
concrete def-use chains. There are 8 concrete def-use chains, shown with dashed arrows, that correspond to (n1, n2),
and there are 2 def-use chains that correspond to (n1, n6). Conversion due to (n1, n2) is amortized over 4 parallel
executions, however conversion due to (n1, n6) is amortized over 2.
5.3 From IP CMPC(S) to IP Linear(S)
In this section we show that under the assumption that all st ∈ Linear(S) that map to the same n ∈ CMPC(S)
are assigned the same share, the protocol assignment that minimizes the objective function of IP CMPC(S)
minimizes the objective function of IP Linear(S) as well.
We deﬁne “abstraction” function α : Linear(S) → CMPC(S) and “concretization” function γ : (CMPC(S) ×
CMPC(S)) → 2Linear(S) that will help us formalize and establish equivalence (cf. Appendix D.2). Function
α(st) returns the node n in CMPC(S) that st maps to. Function γ((d, u)) takes a def-use chain in CMPC(S),
and returns the set of deﬁnitions std such that (std, stu) is a def-use chain in Linear(S), and α(std) = d,
and α(stu) = u. Intuitively, γ((d, u)) returns all distinct std, such that there are distinct constraints in
IP Linear(S)
xstd ≥ astu − astd s.t. α(std) = d, α(stu) = u
and thus
xstd ≥ aα(stu) − aα(std) equiv. xstd ≥ au − ad
We note that we abuse notation slightly, by using a and y interchangeably in IP Linear(S) and in IP CMPC(S).
Theorem 3. Consider a protocol assignment PA = (an, yn) that minimizes IP CMPC(S). If for every pair
st, st(cid:48) ∈ Linear(S), α(st) = α(st(cid:48)) ⇒ ast = ast
, then aα(st) = an, yα(st) = yn minimizes
IP Linear(S).
(cid:48) ∧ yst = yst
(cid:48)
6 Scheduling and Parallelization
Scheduling speciﬁes the order in which diﬀerent instructions should be executed and, in particular, which
instructions should be executed in parallel. Scheduling and parallelization have been extensively studied in the
17
n5	n3	n1	n2	n6	n4	B3	B1	B2	n2(B11)				n2(B12)	n1(B11)				n1(B12)	n2(B11)(B21)				n2(B12)(B21)	n1(B11)(B21)				n1(B12)(B21)	n2(B11)(B22)				n2(B12)(B22)	n1(B11)(B22)				n1(B12)(B22)	n4(B22)	n3(B21)	n3(B22)	n4(B22)	MPC-source:	Dashed	arrow	is		a	backward	def-use.	thus,		B1	can	be	parallelized,		B2	cannot,		and	B3	can	be	parallelized.	LinearizaGon	starts	from	innermost		to	outermost	loop.	Linearizing	B1.	Linearizing	B2	(cannot	parallelize)	n5	n3	n1	n2	n6	n4	B3	B1	B2	n2(B11)				n2(B12)	n1(B11)				n1(B12)	n2(B11)(B21)				n2(B12)(B21)	n1(B11)(B21)				n1(B12)(B21)	n2(B11)(B22)				n2(B12)(B22)	n1(B11)(B22)				n1(B12)(B22)	n4(B22)	n3(B21)	n3(B22)	n4(B22)	MPC-source:	Dashed	arrow	is		a	backward	def-use.	thus,		B1	can	be	parallelized,		B2	cannot,		and	B3	can	be	parallelized.	LinearizaGon	starts	from	innermost		to	outermost	loop.	Linearizing	B1.	Linearizing	B2	(cannot	parallelize)	n5	n3	n1	n2	n6	n4	B3	B1	B2	n2(B11)				n2(B12)	n1(B11)				n1(B12)	n2(B11)(B21)				n2(B12)(B21)	n1(B11)(B21)				n1(B12)(B21)	n2(B11)(B22)				n2(B12)(B22)	n1(B11)(B22)				n1(B12)(B22)	n4(B22)	n3(B21)	n3(B22)	n4(B22)	MPC-source:	Dashed	arrow	is		a	backward	def-use.	thus,		B1	can	be	parallelized,		B2	cannot,		and	B3	can	be	parallelized.	LinearizaGon	starts	from	innermost		to	outermost	loop.	Linearizing	B1.	Linearizing	B2	(cannot	parallelize)	n2(B11)(B21)(B31)				n2(B12)(B21)(B31)	n1(B11)(B21)(B31)				n1(B12)(B21)(B31)	n2(B11)(B22)(B31)				n2(B12)(B22)(B31)	n1(B11)(B22)(B31)				n1(B12)(B22)(B31)	n4(B22)(B31)	n3(B21)(B31)	n3(B22)(B31)	n4(B22)(B31)	n5(B31)	n6(B31)	n2(B11)(B21)(B31)				n2(B12)(B21)(B31)	n1(B11)(B21)(B31)				n1(B12)(B21)(B31)	n2(B11)(B22)(B31)				n2(B12)(B22)(B31)	n1(B11)(B22)(B31)				n1(B12)(B22)(B31)	n4(B22)(B31)	n3(B21)(B31)	n3(B22)(B31)	n4(B22)(B31)	n5(B31)	n6(B31)	compilers and parallel programming literature. However, the applicability to MPC of known algorithms and
results on loop parallelization, is not well-understood. We conjecture (and leave for future work) that MPC-
structure can be exploited to build provably optimal schedules. In this section, we describe a natural schedule
that targets common patterns occurring in MPC applications. We believe that existing work [B¨us+18; BK15],
uses essentially the same approach to scheduling, however, we are the ﬁrst to formally and explicitly describe
the schedules.
The original ABY framework takes a greedy parallelization approach: whenever something is paralleliz-
able, assign to the parallel operation the protocol which, when amortized is optimal. Clearly this does not
always yield the optimal assignment. More recent versions of the framework [B¨us+18] employ heuristics from
parallel programming to detect parallelization [Wil+94; IJT91]. Although this might, at times yield a faster
execution, there are no guarantees, in general, that the heuristically discovered scheduling is better than no
parallelization or full parallelization. In fact, one can construct examples in which the cost of conversion after
the parallelized node supersedes the beneﬁts of amortization. For example, a single EQ (equality check) is
processed faster with πY but allows for better amortization when processed with πB.
To avoid the ambiguity introduced by scheduling and parallelization, OPA can be parameterized by an
explicit scheduler. In the following we describe how we deﬁne such a scheduler (§6.1). We describe a natural
parallelization schedule (§6.2), and what restriction we impose on a given schedule (§6.3). The restriction
guarantees that the solution of IP CMPC(S) is a solution to the IP Linear(S), and natural parallelization schedules
meet the restriction.
6.1 Scheduler
We deﬁne schedulers over Linear(S)—recall, these are the linear CFGs corresponding to the linearized MPC.
Linear(S) can be extended to capture parallel execution of the program, by grouping multiple statements
into one hyper-node (aka parallel node). All st’s grouped into a parallel node can execute in parallel.
Parallel(S) is the sequence of parallel nodes P1 → P2 → ··· → Pn, where P1 executes before P2, P2 executes
before P3, etc. We say that Parallel(S) is a parallelization of Linear(S) if and only if for every def-use chain
(std, stu) ∈ Linear(S), std is in hyper-node Pi, stu is in hyper-node Pj, and Pi executes before Pj. The
restriction is necessary to preserve program correctness—a deﬁnition must execute before all its uses.
Deﬁnition 3 (OPA-scheduler). An OPA scheduler S for Linear(S) is a mapping from Linear(S) to a
parallelization (schedule) Parallel(S).
6.2 A Natural Schedule
A natural schedule arises as follows. Assume MPC-source, as shown in Fig. 3(a). If a loop B is such that
there is no backward def-use chain that ends in B—and thus, there are no data dependencies from iteration
k to iteration (k + 1) of B—then we schedule B’s iterations in parallel by grouping corresponding nodes
into a hyper-node; otherwise, we schedule the iterations sequentially, as in Linear(S). We call the former case
a parallel loop, and the latter case a sequential loop. For example, the innermost loop B1 in Fig. 3(a) is a
parallel loop. The schedule of B1 is shown at the top of Fig. 3(b). n1(B11) and n1(B12) are scheduled in the
same hyper-node, say P1, and n2(B11) and n2(B12) are scheduled in P2, and P1 executes before P2. Loop
B2 is a sequential loop. The schedule of B2 is shown in Fig. 3(b) as well. Since there is a backward def-use
chain B2’s iterations are scheduled sequentially.
We construct a natural schedule inductively, from the innermost (level 0) towards the outermost (level
D) loop. Assume a schedule Sk : P1 → P2 . . . Pl at level k, enclosed in a loop block B with bound b at
level (k + 1). If B is a parallel loop, then the new schedule Sk+1 is constructed by grouping together all
Pi(B1), Pi(B2), . . . , Pi(Bl). S(k+1) is
P1(B1) . . . P1(Bb) → P2(B1) . . . P2(Bb) → ··· → Pl(B1) . . . Pl(Bb)
Conversely, if B is a sequential loop, S(k+1) is constructed by sequencing Sk b times:
P1(B1) → ··· → Pl(B1) → P1(B2) → ··· → Pl(B2) → ··· → Pl(Bb)