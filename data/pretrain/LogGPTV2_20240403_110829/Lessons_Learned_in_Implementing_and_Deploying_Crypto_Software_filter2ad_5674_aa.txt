# Lessons Learned in Implementing and Deploying Cryptographic Software

**Author:** Peter Gutmann  
**Affiliation:** University of Auckland  
**Conference:** 11th USENIX Security Symposium  
**Location:** San Francisco, California, USA  
**Date:** August 5-9, 2002  
**Publisher:** The USENIX Association  
**Contact Information:**
- **Phone:** 1 510 528 8649
- **FAX:** 1 510 548 5738
- **Email:** [PI:EMAIL]
- **Website:** [http://www.usenix.org](http://www.usenix.org)

**Copyright Notice:**
Â© 2002 by The USENIX Association. All rights reserved. Individual papers remain the property of the author or the author's employer. Permission is granted for noncommercial reproduction of this work for educational or research purposes, provided that this copyright notice is included. USENIX acknowledges all trademarks herein.

---

## Abstract

In recent years, strong cryptographic tools have become more widely available. However, experience has shown that implementers often misuse these tools, compromising their security properties. Part of the blame lies with the tools themselves, which can make it easy to use them incorrectly. Just as a chainsaw manufacturer would not produce a model without safety features, security software designers must consider safeguards to prevent users from harming themselves or others. This paper examines common problem areas in cryptographic software and provides design guidelines to minimize damage from (mis-)use by inexperienced users. These issues are based on extensive real-world experience and represent areas that frequently cause problems in practice.

---

## 1. Introduction

Over the past five years, the basic tools for strong encryption have become more widespread, gradually replacing less secure "snake oil" products. It is now relatively easy to obtain software that uses well-established, robust algorithms like triple DES and RSA. Unfortunately, this has not eliminated the snake oil problem but has merely shifted it to a different form.

Determined programmers can create ineffective cryptographic products using any tools. What makes the new generation of dubious crypto products more problematic is that they no longer display obvious warning signs. For example, a proprietary, million-bit-key one-time pad built from encrypted prime cycles is an obvious red flag, but a file encryptor using Blowfish with a 128-bit key derived from an MD5 hash of an 8-character uppercase ASCII password appears safe until further analysis reveals its flaws. This type of second-generation snake oil, which looks authentic but is not, can be referred to as "naugahyde crypto."

Most cryptographic software assumes that the user is knowledgeable and will choose appropriate algorithms, manage keys securely, and use the software safely. However, most implementers are everyday programmers who may lack detailed cryptographic knowledge. As a result, products with genuine naugahyde crypto are created. Sometimes, these issues are discovered, such as when encryption keys are generated from the process ID and time, or when the RC4 keystream is reused multiple times, allowing plaintext recovery through a simple XOR. More often, however, these issues go unnoticed, leading to the deployment of products that provide only illusory security.

This paper explores ways in which cryptographic software developers and providers can avoid creating and deploying software that can be used to create naugahyde crypto. The experiences presented here come from developing and supporting the open-source cryptlib toolkit, which has provided the author with valuable insights into common misuses and problem areas. Additional feedback was provided by users and developers involved in other open-source cryptographic projects.

All events reported here are based on real experiences, though details have been anonymized, particularly where the users involved have significant legal resources. Some interesting stories were excluded but are referenced indirectly in the text. While there are fewer references than usual, the reader can be assured that all events mentioned are real, and it is likely that they have either used or been part of the use of the products discussed.

---

## 2. Existing Work

There is limited published research on proactively ensuring that cryptographic software is used securely, as opposed to fixing problems after they are discovered. Most authors focus on presenting algorithms and mechanisms, leaving implementation details to the developer. An earlier study on why cryptosystems fail primarily addressed banking security but predicted that implementers lacking skills in security integration and management would build systems with vulnerabilities.

Another paper examined user interface problems in encryption software, an area that needs further attention from human-computer interaction (HCI) researchers. The author of a widely-used cryptography book wrote a follow-up work to address the issue of poorly designed security systems by people who had read his first book. He found that the weak points were not in the mathematics but in the programming. His follow-up work addresses security as a process rather than a product, while this paper focuses on the most common errors made by non-cryptographers.

In addition to these works, there are several general-purpose references covering security issues in application design and implementation, aimed at helping developers avoid common problems like buffer overflows, race conditions, and access control issues. This paper, in contrast, specifically addresses problems that occur when end-users misuse security software and suggests design guidelines to help mitigate such misuse.

---

## 3. Cryptographic Software Problems and Solutions

Cryptographic and security software can be misused in many ways. This section covers some of the more common problem areas, providing examples of misuse and suggesting solutions that developers can adopt to minimize potential issues. While there is no universal fix for all problems, and some have social or economic bases that cannot be easily solved through technology, the guidelines presented here aim to alert developers to certain problem areas and provide assistance in addressing them.

### 3.1 Private Keys Should Not Be Exposed

One of the primary design features of cryptlib is that it never exposes private keys to external access. The most frequently asked question about cryptlib is, "How do I export private keys in plaintext form?" The reasons given range from logical (e.g., generating a test key) to dubious (e.g., sharing the same private key across all servers) and even bizarre (e.g., "I just want to do it").

In some cases, the need to share private keys is driven by financial concerns. If a company has spent $495 on a Verisign certificate downloaded to a Windows machine, they may not want to spend the same amount again for the same thing in a different format. As a result, the private key is exported from the Windows key store (where any Windows application can use it) into Netscape, OpenSSL, BSAFE, and cryptlib (although cryptlib makes it difficult to import keys of unknown provenance). Eventually, every encryption-enabled application on the system has a copy of the key, and it may be spread across multiple systems for use by different developers or system administrators. Reusing a single private key to save CA fees is popular, especially among Windows users.

The extent of private key sharing across applications and machines is alarming. Users often do not understand the value of private key data, treating it as just another piece of information that can be copied wherever convenient. For example, a few years ago, a company developed a PGP-based encrypted file transfer system for a large customer. The system used a 2048-bit private key stored in plaintext on disk because the software ran as a batch process and could not wait for a password to be entered. One day, the customer called to say they had lost the private key file and asked the company's programmers to reconstruct it. This caused concern until a developer pointed out that copies of the private key were stored on a file server along with the source code and in other locations with the application binaries. Further investigation revealed that developers had also copied the key to their own machines during the development process for testing. Some of these machines had later been passed on to new employees with their original contents intact. The file server on which the development work was stored had its hard drives upgraded, and the old drives (with the key on them) had been put on a shelf.