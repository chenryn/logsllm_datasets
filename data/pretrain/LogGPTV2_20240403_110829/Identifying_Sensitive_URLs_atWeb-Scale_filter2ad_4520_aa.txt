title:Identifying Sensitive URLs atWeb-Scale
author:Srdjan Matic and
Costas Iordanou and
Georgios Smaragdakis and
Nikolaos Laoutaris
Identifying Sensitive URLs at Web-Scale
Srdjan Matic
TU Berlin
Georgios Smaragdakis
TU Berlin
Costas Iordanou
Cyprus University of Technology
Nikolaos Laoutaris
IMDEA Networks Institute
ABSTRACT
Several data protection laws include special provisions for protect-
ing personal data relating to religion, health, sexual orientation, and
other sensitive categories. Having a well-defined list of sensitive
categories is sufficient for filing complaints manually, conducting
investigations, and prosecuting cases in courts of law. Data protec-
tion laws, however, do not define explicitly what type of content
falls under each sensitive category. Therefore, it is unclear how to
implement proactive measures such as informing users, blocking
trackers, and filing complaints automatically when users visit sensi-
tive domains. To empower such use cases we turn to the Curlie.org
crowdsourced taxonomy project for drawing training data to build
a text classifier for sensitive URLs. We demonstrate that our clas-
sifier can identify sensitive URLs with accuracy above 88%, and
even recognize specific sensitive categories with accuracy above
90%. We then use our classifier to search for sensitive URLs in a
corpus of 1 Billion URLs collected by the Common Crawl project.
We identify more than 155 millions sensitive URLs in more than 4
million domains. Despite their sensitive nature, more than 30% of
these URLs belong to domains that fail to use HTTPS. Also, in sen-
sitive web pages with third-party cookies, 87% of the third-parties
set at least one persistent cookie.
CCS CONCEPTS
· Security and privacy → Privacy protections; · Information
systems → World Wide Web; · Networks → Network measure-
ment.
ACM Reference Format:
Srdjan Matic, Costas Iordanou, Georgios Smaragdakis, and Nikolaos Laoutaris.
2020. Identifying Sensitive URLs at Web-Scale. In ACM Internet Measurement
Conference (IMC ’20), October 27ś29, 2020, Virtual Event, USA. ACM, New
York, NY, USA, 15 pages. https://doi.org/10.1145/3419394.3423653
1 INTRODUCTION
The Web is full of domains in which most people would rather not
to be seen by third-party tracking services. Indeed, being tracked
on a cancer discussion forum, a dating site, or a news site with
non-mainstream political affinity, is at the core of some of the most
fundamental anxieties that several people have about their online
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IMC ’20, October 27ś29, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8138-3/20/10. . . $15.00
https://doi.org/10.1145/3419394.3423653
privacy. Many people visit such sites in incognito mode. This can
provide some privacy in some cases, but it has been shown that
tracking can be performed regardless as was demonstrated in recent
studies [10, 38, 87].
The European General Data Protection Regulation (GDPR) [37]
includes specific clauses that put restrictions on the collection and
processing of sensitive personal data, defined as any data łrevealing
racial or ethnic origin, political opinions, religious or philosophical be-
liefs, or trade union membership, also genetic data, biometric data for
the purpose of uniquely identifying a natural person, data concerning
health or data concerning a natural persons sex life or sexual orienta-
tionž. Other governments and administrations around the world,
e.g., in California (California Consumer Privacy Act (CCPA) [76]),
Canada [63], Israel [79], Japan [65], and Australia [62], are following
similar paths [40, 44].
The above laws are setting the tone regarding the treatment of
sensitive personal data, and provide a legal framework for filing
complaints, conducting investigations, and even pursuing cases in
court. Such measures are rather reactive, i.e., they take effect long
after an incident has occurred. To increase further the protection
of sensitive personal data, proactive measures should also be put in
place. For example, the browser, or an add-on program, can inform
the user whenever he visits URLs pointing to sensitive content.
When on such sites, trackers can be blocked, and complaints can
be automatically filed. Implementing such services hinges on the
ability to automatically classify arbitrary URLs as sensitive and
it cannot be achieved simply by installing the popular AdBlock
extension or visiting the web site in incognito mode, because none
of those solutions checks the actual content of web page.
At the same time, determining what is truly sensitive is easier
said than done. As discussed earlier, legal documents merely pro-
vide a list of sensitive categories, but without any description, or
guidance about how to judge what content falls within each one of
them. This can lead to a fair amount of ambiguity since, for example,
the word łHealthž appears both on web pages about chronic dis-
eases, sexually transmitted diseases, and cancer, but also on pages
about healthy eating, sports, and organic food. For humans it is
easy to disambiguate and recognize that the former are sites about
sensitive content, whereas the latter, not so much. The problem
becomes further exacerbated by the fact that within a web domain,
different sections and individual pages may touch upon very diverse
topics. Therefore, commercial services that assign labels to top level
domains, become inadequate for detecting sensitive URLs that may
appear deeper in these domains. The purpose of this paper is to
demonstrate how to solve the above mentioned ambiguity problem
and to develop an efficient mechanism to evaluate the extent of the
sensitive content on the open Web.
IMC ’20, October 27–29, 2020, Virtual Event, USA
Matic et al.
Our contributions: As with all classification tasks, to train a clas-
sifier for sensitive personal data, one needs a high quality training
set with both sensitive and non sensitive pages. Our first major
contribution is the development of a semi-automated methodology
for compiling such a training set by filtering the Curlie [30] crowd-
sourced web taxonomy project. We develop a novel and scalable
technique that uses category labels and the hierarchical structure
of Curlie to address the core ambiguity challenge. Our carefully
selected training set comprises 156k sensitive URLs. To the best of
our knowledge this is the largest dataset of its type1.
We then consider different classification algorithms and perform
elaborate feature engineering to design a series of classifiers for
detecting sensitive URLs. We examine both meta-data driven clas-
sifiers that use only the URL, title, and meta description of a page,
as well as classifiers that use the text of web pages. We apply our
classifier on the largest publicly available snapshot of the (English
speaking) Web and estimate, for the first time, the percentage of
domains and URLs involving sensitive personal data. Finally, we
look within the identified sensitive web pages and report our pre-
liminary observations regarding the privacy risks of people visiting
these pages.
Our findings:
• We show that classifying URLs as sensitive based on the categories
and content of their corresponding top-level domain is inaccurate.
This means that popular domain classifications services such as
Alexa and SimilarWeb may either fail to identify sensitive URLs
below non-sensitive top level domains, or mis-classify as sensitive,
non-sensitive URLs below a seemingly sensitive top level domain.
This should not come as a surprise, given that such services are
either general purpose, or are optimized for other tasks that have
nothing to do with classifying sensitive content. In essence, DNS-
based blocking/domain blacklisting of sensitive content becomes
ineffective at the URL level.
• On the positive side, we show that Bayesian classifiers based on
word frequency can detect sensitive URLs with an accuracy of at
least 88%. However, meta-data based classification, and text-based
classification with do not seem to perform well. Also, word em-
bedding techniques such as Word2Vec and Doc2Vec yield marginal
benefits for our classification task.
• When it comes to detecting specific sensitive categories, such as
those defined by GDPR: Health, Politics, Religion, Sexual Orienta-
tion, Ethnicity, our classifier achieves a high classification accuracy
as well. For specific categories, such as Health (98%), Politics (92%),
Religion (97%), our classifier achieves an accuracy that exceeds the
basic classification accuracy between sensitive and non-sensitive
URLs (88%).
• Applying our classifier on a Common Crawl snapshot of the
English speaking Web (around 1 Billion URLs), we identify 155
million sensitive URLs in more than 4 million domains. Health,
Religion, and Political Beliefs are the most popular categories with
around 70 millions, 35 millions, and 32 millions URLs respectively.
• Looking among the identified sensitive URLs we reach the conclu-
sion that sensitive URLs are handled as any other URL, without any
1For the benefit of other research efforts in the field, at the following URL we make
publicly available our classifier and the categories we used to train it:
https://bitbucket.org/srdjanmatic/sensitive_web/
special provision for the privacy of users. For example, we show
that 30% of sensitive URLs are hosted in domains that fail to use
HTTPS. Also, in sensitive web pages with third-party cookies, 87%
of the third-parties sets at least one persistent cookie.
2 EXTRACTING TRAINING DATA FROM A
HUMAN-LABELED WEB TAXONOMY
The starting point for the creation of any classifier is a solid training
set. This is a compelling requirement to understand the extent
of the content related to sensitive personal data on the Web. In
such case, the training set should be of high quality, well assorted
and large to allow the classifier to deal with a wide range of web
pages. Unfortunately, to the best of our knowledge, such a dataset
is not readily available. In this section we explain how we built
the training set for our classifier using hundreds of thousands of
carefully selected URLs.
2.1 Limitations of Existing Commercial
Taxonomy Services
Previous work relied on security solutions from vendors such as
McAfee [53] and Symantec [78] to categorize URLs [8, 73, 75]. Most
of these services are focused on fighting malware, and, therefore,
their taxonomy includes a limited number of generic labels which
categorize Effective Second Level Domains (ESLDs). Alexa [11]
and SimilarWeb [74] are other extremely popular, but non security-
oriented, solutions that characterize web sites at the domain level.
An inherent limitation of all those approaches is that the service
cannot accurately categorize subdomains that are used for differ-
ent purposes than the original ESLD. In particular scenarios this
might not be an issue, especially if a web site is homogeneous in
terms of content, or when the objective is to characterize just the
domain [69]. An example are web sites labeled as pornography
where the majority of web pages actually contains pornographic
material [45].
On the contrary, when the objective is to characterize individual
web pages, all of the above services start having problems. This
is especially true for web sites such as news portals and blogging
services, that include diverse and non-homogeneous content. Limi-
tations are further exacerbated when the categories of interest are
sensitive ones. In such cases commercial services have low coverage
and, even when they do, they still suffer from the ambiguity prob-
lem mentioned earlier. For example, in the Alexa top domains for
Health, we find the US National Institute of health and UN World
Health Organization in the top two positions. Looking at top-20
entries, we find also several fitness related web sites in the list. Be-
ing tracked while visiting such domains is probably less worrisome
than when the domain relates to cancer or HIV treatment.
In addition to the coverage and ambiguity, another possible
source of problems are the labels that services use. On one side
those labels could be few and generic, without the ability to provide
additional details (e.g., sub-categories). On the other, commercial
services typically lack transparency in terms of how they assign
labels to domains. Even in scenarios where such issues are not a lim-
itation, oftentimes commercial services offer expensive APIs which
are made available only to a small and targeted elite. This translates
in an audience which is composed exclusively of advertisers that
Identifying Sensitive URLs at Web-Scale
IMC ’20, October 27–29, 2020, Virtual Event, USA
further divided into sub-categories that provide additional granu-
larity up to maximum depth of 14 nested layers.
Why we chose it? We chose Curlie for several reasons. First, unlike
Alexa and SimilarWeb, it categorizes full URLs instead of just ESLDs.
Second, the number of its categories is several orders of magnitude
greater than those used by analogous commercial solutions [31, 91].
Third, the organization of the dataset in a hierarchical ontology
allows us to efficiently navigate through the category tree and
extract all the URLs that belong to a particular category. Finally,
access to Curlie is free and not subject to any rate limitation.
Data collection. In March 2017 Curlie stopped redistributing weekly
RDF2 dumps, and, therefore, we created a crawler to download
the most recent information [27, 28, 84]. We focus only on English
content and thus, our crawler is seeded with the paths of the top-
categories visible at https://curlie.org/en. For each seed path, the
crawler performs a depth-first search to collect all the URLs included
under that particular branch. It is common that a sub-category
contains links to another sub-category on a completely different
branch, and the crawler keeps track of all the processed categories
to avoid entering loops. After completing the crawling, we collected
1,525,865 URLs that belong to 344,227 categories.
Characterizing the collected data. By inspecting how the URLs are
spread across the top-categories, we notice that half of the collected
URL belongs to Regional. This is a meta-category that acts as aggre-
gator and groups other top-categories while providing information
at the regional or country level. The remaining 15 top-categories are
relatively balanced, with an average of 58,600 URLs per category.
The only exceptions are Adult and News that contain less than
10,000 elements each. Such layout confirms that Curlie editors have
a wide range of interests, and that the collected dataset contains
enough variety for building a well assorted training set.
Next, we investigate the dataset coverage in terms of different
web sites from which the URLs are sampled. We characterize web
sites through their Fully Qualified Domain Name (FQDN), and
across the entire dataset we observe 1,137,997 unique FQDNs. On
average, each FQDN is represented by 1.3 URLs, but this distribution
is extremely skewed and only 4% of FQDNs have two or more URLs.
This small set of web sites contributes with 431,707 URLs, which
corresponds to approximately one third of the entire dataset. This is
a potential problem, because if we train the classifier with content
obtained from a limited number of web sites, we run the risk of
ending up with an over-fitted classifier that will not generalize
well to unknown domains. To test if our dataset contains enough
variety, we manually inspect the top-100 FQDNs in terms of overall
number of URLs associated to them. Collectively, such web sites
account for 10.4% of all the Curlie URLs, and each one of the top-32
contributors has more than 1,000 unique URLs. In Table 1 we include
the top-10 FQDNs. The values in the second column show that those
FQDNs are associated to thousands of categories, which in turn
cover the vast majority of Curlie top-categories (third column). In
the last column we point out services that allow users to participate
in the creation of new content. Common examples are services
where users can build their own web site (e.g., www.angelfire.com
2RDF or Resource Description Framework is a family of World Wide Web Consor-
tium specifications for conceptual description or modeling of information that is
implemented in web resources, e.g. URLs.
Figure 1: From Web to Sensitive Web. Mixing and matching
crowd-sourcing with automated and manual filtering to cre-
ate the largest ever training set for sensitive content classi-
fiers.
want to make sure their ads are placed in appropriate contexts [2]
or to proprietary solutions that work only when content is served
through a specific platform [3].
2.2 The Curlie Dataset
To overcome the limitations described above, we choose to build our
training set by selecting sensitive URLs from Curlie [30], the largest
publicly available taxonomy of web pages. In the following sections
we provide details about Curlie, its content and our methodology
for distinguishing sensitive from non-sensitive web pages. Figure 1
illustrates how we blend crowd-sourcing (done by Curlie) with
automated and a manual steps (done by us) on the łthin-waistž of
an overall methodology that can identify the sensitive part of the
Web (Section. 4). The manual step at the łthin-waistž of the overall
process needs to be performed only once to identify (un-ambiguous)
GDPR-sensitive categories that can then be used repeatedly to draw
from the Curlie truly sensitive URLs.
What is it? Curlie is an open source project and the successor of
DMOZ, a community-based effort to categorize popular web pages
across the Internet [88]. Thanks to the collaboration of 92,000 ed-
itors that manually evaluate and organize web pages [29], Curlie
represents one of largest human-edited directories of the Web. Edi-
tors join Curlie by applying to edit a category that corresponds to
their interests, and each editor is responsible for reviewing submis-