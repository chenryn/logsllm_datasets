title:On the Potential for Discrimination via Composition
author:Giridhari Venkatadri and
Alan Mislove
On the Potential for Discrimination via Composition
Giridhari Venkatadri
Northeastern University
Alan Mislove
Northeastern University
ABSTRACT
The success of platforms such as Facebook and Google has been
due in no small part to features that allow advertisers to target ads
in a fine-grained manner. However, these features open up the po-
tential for discriminatory advertising when advertisers include or
exclude users of protected classes—either directly or indirectly—in
a discriminatory fashion. Despite the fact that advertisers are able
to compose various targeting features together, the existing mitiga-
tions to discriminatory targeting have focused only on individual
features; there are concerns that such composition could result in
targeting that is more discriminatory than the features individually.
In this paper, we first demonstrate how compositions of individ-
ual targeting features can yield discriminatory ad targeting even
for Facebook’s restricted targeting features for ads in special cat-
egories (meant to protect against discriminatory advertising). We
then conduct the first study of the potential for discrimination that
spans across three major advertising platforms (Facebook, Google,
and LinkedIn), showing how the potential for discriminatory adver-
tising is pervasive across these platforms. Our work further points
to the need for more careful mitigations to address the issue of
discriminatory ad targeting.
CCS CONCEPTS
• Security and privacy → Social aspects of security and pri-
vacy; • Information systems → Online advertising; Social
networks.
KEYWORDS
Targeted advertising, Advertising platforms, Discriminatory ad
targeting
ACM Reference Format:
Giridhari Venkatadri and Alan Mislove. 2020. On the Potential for Discrimi-
nation via Composition. In ACM Internet Measurement Conference (IMC ’20),
October 27–29, 2020, Virtual Event, USA. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3419394.3423641
1 INTRODUCTION
Online advertising platforms such as Facebook, Google, and
LinkedIn leverage their rich user databases to allow advertisers
to target ads to particular users on their platforms. While the ability
to selectively target relevant users is advantageous to advertisers—
potentially offering them better value for their ad budget—such
Permission to make digital or hard copies of all or part of this work for personal or classroom use
is granted without fee provided that copies are not made or distributed for profit or commercial
advantage and that copies bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
IMC ’20, October 27–29, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8138-3/20/10...$15.00
https://doi.org/10.1145/3419394.3423641
targeting raises concerns that advertisers could knowingly (or un-
knowingly) target users in order to selectively exclude users of
certain sensitive populations (such as users of particular genders,
ages, races, or other historically disadvantaged groups). Such dis-
criminatory targeting, while concerning in and of itself, could also
run afoul of law for advertisements related to housing, credit, and
employment, where special legal protections exist [1–3].
This concern of discriminatory targeting was first raised in the
context of Facebook’s platform (the largest and most mature of
these platforms), where it was shown that an advertiser could ex-
plicitly exclude users with certain “ethnic affinities” (such as African
American) when targeting housing ads [16]. Subsequent research
demonstrated that the problem was not limited to options that
explicitly mentioned a protected class, and many additional op-
tions exist that are strongly correlated with protected classes [37].
In response to the uproar—including lawsuits from the National
Fair Housing Alliance [14] and the U.S. Department of Housing
and Urban Development (HUD) [15]—Facebook made a number of
changes to its targeting options. These changes included deploying
a restricted interface for housing, credit, and employment ads that
has more limited targeting options [12].
Unfortunately, there are two key omissions in terms of under-
standing and protecting against discrimination in ad targeting. First,
the previous discussion and proposed mitigations have focused on
individual targeting options that happen to be correlated with a par-
ticular sensitive population; indeed, Facebook’s above-mentioned
restrictions to mitigate discriminatory advertising primarily fo-
cused on disabling access to many individual targeting options.
However, these advertising platforms typically allow advertisers
to compose multiple such options together in various ways. Thus,
two (or more) targeting options that are individually only mildly
correlated with a protected class (and therefore only mildly dis-
criminatory), may end up being more significantly correlated when
used in conjunction with each other. For example, the population
interested in electrical engineering, or the population interested in
sports cars, might each be somewhat skewed towards men; however,
the population interested in electrical engineering and sports cars
might be significantly more skewed. As a result, limiting individual
targeting options alone may be insufficient to prevent discrimina-
tory advertising. Indeed, if composing targeting options in general
tends to yield more skew than individual targeting options, even
an honest advertiser who uses multiple targeting options may end
up inadvertently running an ad in a discriminatory manner.
The second key omission is that most of the focus in the press and
academia has been on Facebook, and less attention has been paid
thus far to the potential for similar discrimination via targeting on
other advertising platforms. It is important to study other platforms,
as they may offer different targeting options (and different methods
of composition), driven by varying views of user data, and varying
advertiser demands.
IMC ’20, October 27–29, 2020, Virtual Event, USA
G. Venkatadri et al.
To address this situation, this paper makes four key contribu-
tions: First, after providing background in § 2 and detailing our
methodology in § 3, we show in § 4.1 how targeting compositions
enable discriminatory targeting even given Facebook’s significantly
curtailed individual targeting options for housing, credit, or employ-
ment ads. Second, in § 4.2, we perform the first examination of the
targeting options present on Google, and LinkedIn’s platforms. We
show the existence of individual targeting options that can be used
to discriminate toward or against particular ages and genders on all
platforms; this is especially concerning on a platform liked LinkedIn
that focuses exclusively on users’ employment-related needs. Third,
in § 4.3, we show that (i) the composition of targeting options is
a vector for abuse that could potentially affect all three platforms
studied; (ii) combining targeting options generally tends to make
them more discriminatory, indicating the potential for inadvertent
discriminatory targeting by even well-meaning advertisers; and (iii)
skewed compositions exist even when highly skewed individual
options are removed. Fourth, while targeting compositions typically
only let an advertiser reach a small fraction of a given protected
class, our results indicate that (i) this limited reach is still large
enough to suffice for most advertisers, and (ii) an advertiser could
increase the fraction reached by targeting across multiple targeting
compositions.
The existence of skewed targeting options and compositions is,
in many cases, likely a reflection of platform users’ interests and
preferences. For example, users of a particular protected class might
be more likely to find certain products relevant, and might not be in-
terested in ads pertaining to certain other products. However, when
attempting to prevent discriminatory advertising for ads in certain
categories, our results underscore the need to carefully consider
compositions of targeting options when designing mitigations; we
discuss specific implications in § 5.
2 BACKGROUND
We now discuss ad platforms’ targeting interfaces and features, and
then overview related work.
2.1 Advertising platforms
Ad platforms offer a wide variety of targeting features to advertisers;
here we focus on the features offered by Facebook, Google, and
LinkedIn. The set of users resulting from a given set of targeting
options is referred to as an audience.
Attribute-based targeting allows targeting by user attributes:
in addition to age, gender, and location, all platforms support a
default list of attributes that an advertiser can browse and choose
from [21, 23, 38]. Additionally, these platforms support (potentially
open) sets of custom attributes that advertisers can either search
for (offered by all platforms), or define in a custom manner (offered
by Google [23]).
Ad placement targeting allows targeting of where (or in what
context) their ad appears. Google has the most extensive targeting
options of this kind, allowing advertisers to specify which (first-
or third-party) websites, apps, and videos to show ads on, either
directly [25], or by specifying particular keywords/topics [24, 28].
Facebook and LinkedIn also provide (comparatively limited) target-
ing options of this kind [20, 33].
Activity-based targeting allows targeting based on visits or ac-
tions on advertisers’ websites and apps [17, 22, 26].1
PII-based targeting allows targeting specific users by uploading
personally identifying information (PII), such as names, and email
addresses [7, 34, 42]. The platform then internally matches the PII
and creates an audience of users.
Lookalike targeting allows advertisers to target sets of users
similar to those in activity-based or PII-based targeting audiences
they create [18, 27, 39].
Targeting compositions allow advertisers to combine targeting
options of different kinds (via logical and), and they additionally
support compositions via boolean rules even for multiple targeting
options of the same kind [19, 30, 35].2
2.2 Facebook’s restricted interface
In order to settle a lawsuit, Facebook introduced a restricted ad
interface for ads in the protected areas of housing, employment,
and credit; this interface has limited targeting options compared
to the original interface [6]. Ages and genders cannot be targeted,
a smaller list of targeting attributes is supported, and excluding
users with particular attributes is disallowed. Additionally, Looka-
like Audiences are replaced by “Special Ad Audiences” that are,
according to Facebook, “adjusted to comply with the audience se-
lection restrictions associated with your campaign’s chosen Special
Ad Category and our ad policies” [6]. PII-based, activity-based, and
ad placement targeting are available, however.
2.3 Related work
The potential for discrimination via individual targeting attributes
(even facially neutral ones) was first demonstrated in the context
of Facebook by Speicher et al. [37], subsequent to the demonstra-
tion by ProPublica [16] of the possibility of explicitly excluding
users with certain “ethnic affinities” when targeting housing ads
on Facebook. Our work demonstrates that this problem is not lim-
ited to Facebook, and is made worse due to platforms’ support for
targeting compositions.
Other work [4, 5, 11, 31, 32, 40] has demonstrated and discussed
the implications of skewed outcomes (across races, genders, and
political affiliations) arising from the working of the ad platforms’
delivery mechanism (rather than from targeting). In addition, prior
work demonstrated that particular real-world ads were delivered
by Google’s platform to users in a skewed manner [11, 36], without
inferring specific causes for this skewed outcome, while Datta et
al. [10] explored the legal implications of various potential causes
for this skewed outcome.
3 METHODOLOGY
We next describe our methodology, which directly uses the ad plat-
forms’ targeting features to measure the skew of different targetings.
1In order to do so, the advertiser places a tracking pixel from the ad platform on their website, letting
the website track visitors’ actions.
2The form of these boolean rules varies across ad platforms and kinds of targetings; the rules only
accommodate boolean-or in some cases, while they could be and of or-terms in other cases.
On the Potential for Discrimination via Composition
IMC ’20, October 27–29, 2020, Virtual Event, USA
We focus on the sensitive attributes gender and age, as ad platforms
typically have access to these and offer options to explicitly target
these attributes.3
Metrics To account for varying underlying distributions of users
across different sensitive populations and platforms, we use a metric
called the representation ratio [37], inspired by the disparate impact
metric historically used to detect discrimination in employment
and housing allocation [9].
This metric focuses only on the (implicit) audience RA of users
who might find a given ad relevant; within this audience, it measures
whether users from a given sensitive population RAs (represented
by a value s of a corresponding sensitive attribute) are more (or
less) likely to be included in a given audience T A targeted by an
advertiser, compared to users with a different value of the sensitive
attribute RA¬s :
|T A ∩ RAs |/|RAs |
|T A ∩ RA¬s |/|RA¬s | ,
rep_ratios(T A, RA) =
(1)
For the purposes of this paper, in line with prior work [37], we
assume RA is the set of all U.S.-based users (and thus RAs is the
set of all U.S.-based users with a value s for the sensitive attribute).
Thus, a representation ratio of 1 is ideal and means users from
RAs and RA¬s are equally likely to be included in the targeted
audience; however, an unacceptably high (or low) value could be
1.25 or above (or 0.8 and below), as per the well-known four-fifths
rule [8] for measuring disparate impact, indicating over- or under-
representation (respectively) of the given sensitive population [37].
In addition, we measure the recall of the ad targeting, which
we define as |T A ∩ RAs | when the targeting selectively includes
users from RAs , and as |T A∩ RA¬s | when the targeting selectively
excludes users from RAs (i.e, includes users from RA¬s ). We next
briefly describe how we target the different audiences in Equation 1,
and how we measure the sizes of these audiences.
Targeting audiences To target the audiences in Equation 1, we
leverage the fact that the studied ad platforms allow targeting by
location, gender, and age, in addition to any other fine-grained
targeting options.4 While Facebook’s restricted interface does not
allow targeting by gender or age, we instead use the corresponding
targeting option on Facebook’s normal interface to measure the
representation ratio.
To measure |Rs |, we target all U.S. users, and additionally tar-
get users who have a value s for the given sensitive attribute. To
measure |T A ∩ RAs |, we further add in the targeting options corre-
sponding to T A. For the given sensitive attribute (age or gender),
we measure |Rs | and |T A ∩ RAs | as above for each value of s; we
then compute |RA¬s | as Σs′∈¬s|RAs′|, and compute |T A ∩ RA¬s |
as Σs′∈¬s|T A ∩ RAs′|.
On these ad platforms, we select the campaign objective of
“Reach” in order to reach the largest set of people.5 Besides, on
Google (which allows different types of ad campaigns), we focus on
3For age, we consider the age ranges 18-24, 25-34, 35-54, and 55+, as these are most granular target-
ing options common to the three ad platforms we study.
4While LinkedIn does not have separate targeting options for targeting by gender, or age, its list
of detailed attribute-based targeting options includes user genders and age ranges; these detailed
targeting options can be combined by performing a logical-and of a series of logical-or terms; thus,
for a given ad targeting, we additionally target a particular gender or age range by adding the
corresponding targeting attribute via a logical-and.
5On Google and LinkedIn, we select the closest corresponding objectives “Brand awareness and
reach” and “Brand awareness” respectively.
the “Display” campaign type as it covers Google’s entire ad network
and corresponds to the broadest reach.
Measuring audience sizes To measure the sizes of these audi-
ences, we leverage the audience size estimates provided by the ad
platforms’ targeting interfaces.These numbers are intended to aid
advertisers when they are selecting targeting options, and they give
a measure of the size of the audience resulting from a given target-
ing. While the estimate provided by Google’s ad platform, as per
the UI, is the “estimated number of impressions that your settings
and targeting could theoretically reach”, the estimates provided by
Facebook and LinkedIn measure the count of users in the audience
(“the size of the audience that’s eligible to see your ad”, and “the
number of LinkedIn members who match your targeting criteria”,
according to the respective interfaces). We find that the estimated
number of impressions on Google’s ad platform depends on a fre-
quency capping setting which restricts how often an ad is shown
to the same user [29]; we set the setting to its most restrictive value
(one impression across the campaign every month per-user).
Automating size queries We use our browser’s web inspector
tool to identify the underlying API calls made by the targeting UIs
whenever the selected set of targeting options is altered; we then
automate these calls with a Python script. While the API calls made
by Facebook and LinkedIn are unobfuscated, the API calls made
by Google consist of obfuscated json; by manually varying the
targeting options systematically, we find a mapping between the
targeting options and particular keys and values in the obfuscated
json.
Understanding size estimates Since audience size estimates
have been shown not to be exact size estimates in the context
of Facebook [41], we study the granularity and consistency of these
to understand if they are obfuscated in any way. In brief, we use 100
back-to-back repeated calls for 20 random targeting options and 20
random compositions and find that across all three platforms, the
returned estimates are consistent.6
To study the granularity of the estimates, we combine the results
of over 80,000 various distinct API calls we make for each ad plat-
form (spanning a variety of ad targetings), and find that the size
estimates across all the platforms are granular: while Facebook’s
estimates have two significant digits (with a minimum returned
value of 1,000); Google’s estimates have one significant digit (until
100,000), and two significant digits thereafter; LinkedIn’s estimates
on the other hand have two significant digits (starting at 300). 7 Such
rounding could mean the measured representation ratios (based
on the rounded estimates) could be either higher or lower than the
actual representation ratio (corresponding to the exact audience
sizes). However, we confirm that even allowing for the representa-
tion ratios to take their least skewed values (subject to the rounding
ranges), we find very similar degrees of skew in targetings across
our experiments.
Obtaining targeting options As discussed in § 2, each of the
three ad platforms we study provides a plethora of targeting op-
tions. However, to limit the number of possible targeting options
6It is possible that an ad platform may obfuscate the audience size corresponding to a given ad
targeting by always adding the same specific noise sample.
7While Facebook’s estimates had a minimum of 1,000; Google’s and LinkedIn’s estimates had a
minimum of 40 and 300 respectively, with 0 returned below that minimum.
IMC ’20, October 27–29, 2020, Virtual Event, USA
G. Venkatadri et al.
we need to study, we focus on the default list of user attributes for
attribute-based targeting on each platform; for Google, in addition,
we consider the default list of topics on its topic targeting feature
(that lets an advertiser place ads solely on webpages corresponding
to a particular topic).8 We collect 393 and 667 attributes for Face-
book’s restricted and normal interface, respectively; 873 attributes
and 2,424 topics for Google; and 552 attributes for LinkedIn.
Discovering the most skewed compositions To limit the query
load, we avoid an exhaustive crawl and use a greedy approach to
discover an approximate (lower bound) set of most skewed tar-