title:Automated Response Using System-Call Delay
author:Anil Somayaji and
Stephanie Forrest
USENIX Association
Proceedings of the
9th USENIX Security Symposium
Denver, Colorado, USA
August 14 –17, 2000
THE ADVANCED COMPUTING SYSTEMS ASSOCIATION
© 2000 by The USENIX Association
Phone: 1 510 528 8649
FAX: 1 510 548 5738
All Rights Reserved
Email: PI:EMAIL
For more information about the USENIX Association:
WWW: http://www.usenix.org
Rights to individual papers remain with the author or the author's employer.
 Permission is granted for noncommercial reproduction of the work for educational or research purposes.
This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
Automated Response Using System-Call Delays
Anil Somayaji
Dept. of Computer Science
University of New Mexico
Albuquerque, NM 87131
PI:EMAIL
Stephanie Forrest
Santa Fe Institute
Santa Fe, NM 87501
Dept. of Computer Science
University of New Mexico
Albuquerque, NM 87131
PI:EMAIL, PI:EMAIL
Abstract
Automated intrusion response is an important unsolved
problem in computer security. A system called pH (for
process homeostasis) is described which can success-
fully detect and stop intrusions before the target system
is compromised. In its current form, pH monitors ev-
ery executing process on a computer at the system-call
level, and responds to anomalies by either delaying or
aborting system calls. The paper presents the rationale
for pH, its design and implementation, and a set of initial
experimental results.
1 Introduction
This paper addresses a largely ignored aspect of com-
puter security—the automated response problem. Pre-
viously, computer security research has focused almost
entirely on prevention (e.g., cryptography, ﬁrewalls and
protocol design) and detection (e.g., virus and intru-
sion detection). Response has been an afterthought,
generally restricted to increased logging and adminis-
trator email. Commercial intrusion detection systems
(IDSs) are capable of terminating connections, killing
processes, and even blocking messages from entire net-
works [3, 12, 22]; in practice, though, these mechanisms
cannot be widely deployed because the risk of an in-
appropriate response (e.g., removing a legitimate user’s
computer from the network) is too high. Thus, IDSs be-
come burdens, requiring administrators to analyze and
respond to almost every detected anomaly. In an era of
expanding connectivity and ubiquitous computing, we
must seek solutions that reduce the system administra-
tor’s workload, rather than increasing it. That is, our
computers must respond to attacks autonomously.
In earlier work, we and others have demonstrated several
methods of anomaly detection by which large classes of
intrusions can be detected, e.g., [1, 27, 17, 16]. Good
anomaly detection, however, comes at the price of per-
sistent false positives. Although more sophisticated
methods will no doubt continue to be developed, we
believe that it is infeasible to eliminate false positives
completely. There are several reasons for this. First,
computers live in rich dynamic environments, where in-
evitably there are new patterns of legitimate activity not
previously seen by the system — a phenomenon known
as perpetual novelty (see Hofmeyr [21] for an empirical
model of the rate at which new patterns appear in a lo-
cal area network). Second, proﬁles of legitimate activity
change continually, as computers and users are added or
deleted, new software packages or patches are added to
a system, and so forth. Thus, the normal state of the sys-
tem is evolving over time. Finally, there is inherent am-
biguity in the distinction between normal and intrusive
(or abnormal) activities. For example, changes to system
conﬁguration ﬁles are legitimate if performed by a sys-
tem administrator; however, the very same actions are a
security violation conducted done by a non-privileged
user or an outside attacker. Thus, any automated re-
sponse system must be designed to account for persis-
tent false-positives, evolving deﬁnitions of normal, and
ambiguity about what constitutes an anomaly.
We have chosen to focus on automated response mech-
anisms which will allow a computer to preserve its own
integrity (i.e. stay “alive” and uncompromised), rather
than ones that help discover the source or method of an
intrusion. Within this context, we believe that the best
way to approach the automated response problem is by
designing a system in which a computer autonomously
monitors its own activities, routinely making small cor-
rections to maintain itself in a “normal” state. In biol-
ogy, the maintenance of a stable (normal) internal en-
vironment is known as homeostasis. All living systems
employ a wide range of homeostatic mechanisms in or-
der to survive under ﬂuctuating environmental condi-
tions. We propose that computer systems should sim-
ilarly have mechanisms which strive to maintain a sta-
ble environment inside the computer, even in the face of
wide variations in inputs. Under this view, automated
response is recast from a monolithic all-or-nothing ac-
tion (which if incorrect can have dire consequences) to
a set of small, continually occurring changes to the state
of the system. With this view, occasional false alarms
are not problematic, because they have small impact. In
earlier papers, we have advocated a view of computer
security based on ideas from immunology [16, 34, 20].
This paper naturally extends that view by recognizing
that immune systems are more properly thought of as
homeostatic mechanisms than pure defense mechanisms
[26].
In the following sections, we describe a working imple-
mentation of these ideas—a set of extensions to a Linux
kernel which does not interfere with normal operation
but can successfully stop attacks as they occur. We call
the system pH (short for process homeostasis). To cre-
ate pH, we extended our earlier intrusion-detection work
using system calls [16] by connecting system calls with
feedback mechanisms that either delay or abort anoma-
lous system calls.
Delays form a natural basis for interfering with program
behavior: small delays are typically imperceptible to a
program, and are minor annoyances to a user. Longer
delays, however, can trigger timeouts at the application
and network levels, effectively terminating the delayed
program. By implementing the delays as an increasing
function of the number of recent anomalous sequences,
pH can smoothly transition between normal execution
and program termination.
This paper makes two principal contributions. First, it
demonstrates the feasibility of monitoring every active
process at the system-call level in real-time, with min-
imal impact on overall performance. Second, it intro-
duces a practical, relatively non-intrusive method for au-
tomatically responding to anomalous program behavior.
The paper proceeds as follows. First, we review our
system call monitoring and anomaly detection method.
Next, we explain the design and implementation of pH.
We then demonstrate pH’s effectiveness at stopping at-
tacks, show through benchmarks that it runs with low
overhead, and describe what it is like to actually use
pH on a workstation. After a review of related work,
we conclude with a discussion of limitations and future
work.
2 Background
Both the monitoring and the response components of pH
use ideas introduced in [16]. What follows is a descrip-
tion of our original testing methodology, with which we
gathered on-line data for off-line analysis. Subsequent
sections explain how these techniques were modiﬁed to
create pH.
To review, we monitored all the system calls (without
arguments) made by an executing program on a per-
process basis. That is, each time a process was invoked,
we began a new trace, logging all the system calls for
that process. Thus, for every process the trace consists
of an ordered list (a time-series) of the system calls it
made during its execution. For commonly executed pro-
grams, especially those that run with privilege, we col-
lected such traces over many invocations of the program,
when it was behaving normally. We then used the col-
lection of all such traces (for one program) to develop an
empirical model of its normal behavior.
Once the system had been trained on a sufﬁcient num-
ber of normal program executions, the model was tested
on subsequent invocations of the program. The hope was
that the model would recognize most normal behavior as
“normal” and most attacks as “abnormal.” Our method
thus falls into the category of anomaly intrusion detec-
tion.
Given a collection of system call traces, how do we use
them to construct a model? This is an active area of
research in the ﬁeld of machine learning, and there are
literally hundreds of good methods available to choose
from, including hidden Markov models, decision trees,
neural networks, and a variety of methods based on de-
terministic ﬁnite automata (DFAs). We chose the sim-
plest method we could think of within the following con-
straints. First, the method must be suitable for on-line
training and testing. That is, we must be able to con-
struct the model “on the ﬂy” in one pass over the data,
and both training and testing must be efﬁcient enough
to be performed in real-time. Next, the method must
be suitable for large alphabet sizes. Our alphabet con-
sists of all the different system calls—typically about
200 for UNIX systems. Finally, the method must create
models that are sensitive to common forms of intrusion.
Traces of intrusions are often 99% the same as normal
traces, with very small, temporally clumped deviations
from normal behavior. In the following, we describe a
simple method, which we call “time-delay embedding”
[16]. Warrender [38] compared time-delay embedding
with several other common machine learning algorithms
and discovered that it is remarkably accurate and efﬁ-
cient in this domain.
We deﬁne normal behavior in terms of short n-grams of
system calls. Conceptually, we deﬁne a small ﬁxed size
window and “slide” it over each trace, recording which
calls precede the current call within the sliding window.
The current call and a call at a ﬁxed preceding window
position form a “pair,” with the contents of a window of
length x being represented by x−1 pairs. The collection
of unique pairs over all the traces for a single program
constitutes our model of normal behavior for the pro-
gram.1
More formally, let
S = alphabet of possible system calls
T = trace
w = window size, 2 ≤ w ≤ τ
P = proﬁle
= the sequence t0, t1, . . . , tτ−1, ti ∈ S
= set of patterns associated with T and w
= {(cid:5)si, sj(cid:6)k : si, sj ∈ S, 1 ≤ k < w
∃p : 0 ≤ p < τ − k,
tp = si,
tp+k = sj,
For example, suppose we had as normal the following
sequence of calls:
execve, brk, open, fstat, mmap, close, open,
mmap, munmap
and a window size of 4. We slide the window across
the sequence, and for each call we encounter, we record
what call precedes it at different positions within the
window, numbering them from 0 to w − 1, with 0 be-
ing the current system call. So, for this trace, we get the
following windows:
1Our original paper on using system calls for intrusion detection
[16] used a technique called “lookahead pairs.” pH uses the original
lookahead pairs algorithm as described here, except that it looks be-
hind instead of ahead. Later papers [20, 38] report results based on
recording full sequences. We reverted to lookahead pairs because it is
simple to implement and extremely efﬁcient.
position 3
position 2
position 1
execve
brk
open
fstat
mmap
close
execve
brk
open
fstat
mmap
close
open
execve
brk
open
fstat
mmap
close
open
mmap
current
execve
brk
open
fstat
mmap
close
open
mmap
munmap
When a call occurs more than once in a trace, it will
likely be preceded by different calls in different contexts.
We compress the explicit window representation by join-
ing together lines with the same current value (note the
open and mmap rows):
position 1
current
execve
execve
brk
brk, close
open
open
fstat
fstat, open
mmap
close
mmap
munmap mmap
position 2
position 3
execve, mmap
brk
open, close
fstat
open
fstat
execve
brk, mmap
open
close
This table can be stored using a ﬁxed-size bit array. If |S|
is the size of the alphabet (number of different possible
system calls) and w is the window size, then we can store
the complete model in a bit array of size: |S|×|S|×(w−
1). Because w is small (6 is our standard default), our
current implementation uses a 200×200 byte array, with
masks to access the individual bits.
At testing time, system call pairs from test traces are