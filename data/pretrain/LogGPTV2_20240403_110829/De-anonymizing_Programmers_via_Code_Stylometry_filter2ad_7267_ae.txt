classical stylometric analysis requires 5,000 words.
Additionally, our results raise a number of questions
that motivate future research. First, as malicious code
is often only available in binary format, it would be in-
teresting to investigate whether syntactic features can be
partially preserved in binaries. This may require our fea-
ture set to be improved in order to incorporate informa-
tion obtained from control ﬂow graphs.
Second, we would also like to see if classiﬁcation ac-
curacy can be further increased. For example, we would
like to explore whether using features that have joint in-
formation gain alongside features that have information
gain by themselves improve performance. Moreover, de-
signing features that capture larger fragments of the ab-
stract syntax tree could provide improvements. These
changes (along with adding lexical and layout features)
may provide signiﬁcant improvements to the Python re-
sults and help generalize the approach further.
Finally, we would like to investigate whether code can
be automatically normalized to remove stylistic informa-
tion while preserving functionality and readability.
8 Acknowledgments
This material is based on work supported by the ARO
(U.S. Army Research Ofﬁce) Grant W911NF-14-1-
0444, the DFG (German Research Foundation) under the
project DEVIL (RI 2469/1-1), and AWS in Education
Research Grant award. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect
those of the ARO, DFG, and AWS.
References
[1] The tigress diversifying c virtualizer, http://tigress.cs.arizona.edu.
[2] Google code jam, https://code.google.com/codejam, 2014.
[3] Stunnix, http://www.stunnix.com/prod/cxxo/, November 2014.
[4] ABBASI, A., AND CHEN, H. Writeprints: A stylometric ap-
proach to identity-level identiﬁcation and similarity detection in
cyberspace. ACM Trans. Inf. Syst. 26, 2 (2008), 1–29.
[5] AFROZ, S., BRENNAN, M., AND GREENSTADT, R. Detecting
hoaxes, frauds, and deception in writing style online.
In Secu-
rity and Privacy (SP), 2012 IEEE Symposium on (2012), IEEE,
pp. 461–475.
[6] AIKEN, A., ET AL. Moss: A system for detecting software pla-
giarism. University of California–Berkeley. See www. cs. berke-
ley. edu/aiken/moss. html 9 (2005).
[7] BREIMAN, L. Random forests. Machine Learning 45, 1 (2001),
5–32.
[8] BURROWS, S., AND TAHAGHOGHI, S. M. Source code author-
ship attribution using n-grams. In Proc. of the Australasian Doc-
ument Computing Symposium (2007).
[9] BURROWS, S., UITDENBOGERD, A. L., AND TURPIN, A. Ap-
plication of information retrieval techniques for source code au-
thorship attribution. In Database Systems for Advanced Applica-
tions (2009), Springer, pp. 699–713.
[10] DING, H., AND SAMADZADEH, M. H. Extraction of java pro-
gram ﬁngerprints for software authorship identiﬁcation. Journal
of Systems and Software 72, 1 (2004), 49–57.
[11] ELENBOGEN, B. S., AND SELIYA, N. Detecting outsourced stu-
dent programming assignments. Journal of Computing Sciences
in Colleges 23, 3 (2008), 50–57.
[12] FRANTZESKOU, G., MACDONELL, S., STAMATATOS, E., AND
GRITZALIS, S. Examining the signiﬁcance of high-level pro-
gramming features in source code author classiﬁcation. Journal
of Systems and Software 81, 3 (2008), 447–460.
[13] FRANTZESKOU, G., STAMATATOS, E., GRITZALIS, S.,
CHASKI, C. E., AND HOWALD, B. S.
Identifying authorship
by byte-level n-grams: The source code author proﬁle (scap)
method. International Journal of Digital Evidence 6, 1 (2007),
1–18.
[14] FRANTZESKOU, G., STAMATATOS, E., GRITZALIS, S., AND
KATSIKAS, S. Effective identiﬁcation of source code authors
using byte-level information.
In Proceedings of the 28th In-
ternational Conference on Software Engineering (2006), ACM,
pp. 893–896.
[15] GRAY, A., SALLIS, P., AND MACDONELL, S. Software foren-
sics: Extending authorship analysis techniques to computer pro-
grams.
[16] HAYES, J. H., AND OFFUTT, J. Recognizing authors: an exam-
ination of the consistent programmer hypothesis. Software Test-
ing, Veriﬁcation and Reliability 20, 4 (2010), 329–356.
[17] INOCENCIO, R. U.s. programmer outsources own job to china,
surfs cat videos, January 2013.
268  24th USENIX Security Symposium 
USENIX Association
14
[18] KOTHARI, J., SHEVERTALOV, M., STEHLE, E., AND MAN-
CORIDIS, S. A probabilistic approach to source code authorship
identiﬁcation. In Information Technology, 2007. ITNG’07. Fourth
International Conference on (2007), IEEE, pp. 243–248.
[19] KRSUL, I., AND SPAFFORD, E. H. Authorship analysis: Iden-
tifying the author of a program. Computers & Security 16, 3
(1997), 233–257.
[20] LANGE, R. C., AND MANCORIDIS, S. Using code metric his-
tograms and genetic algorithms to perform author identiﬁcation
for software forensics.
In Proceedings of the 9th Annual Con-
ference on Genetic and Evolutionary Computation (2007), ACM,
pp. 2082–2089.
[21] MACDONELL, S. G., GRAY, A. R., MACLENNAN, G., AND
SALLIS, P. J. Software forensics for discriminating between
program authors using case-based reasoning, feedforward neural
networks and multiple discriminant analysis.
In Neural Infor-
mation Processing, 1999. Proceedings. ICONIP’99. 6th Interna-
tional Conference on (1999), vol. 1, IEEE, pp. 66–71.
[22] NARAYANAN, A., PASKOV, H., GONG, N. Z., BETHENCOURT,
J., STEFANOV, E., SHIN, E. C. R., AND SONG, D. On the
feasibility of internet-scale author identiﬁcation. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE, pp. 300–
314.
[23] PELLIN, B. N. Using classiﬁcation techniques to determine
source code authorship. White Paper: Department of Computer
Science, University of Wisconsin (2000).
[24] PIKE, R. The sherlock plagiarism detector, 2011.
[25] PRECHELT, L., MALPOHL, G., AND PHILIPPSEN, M. Finding
plagiarisms among a set of programs with jplag. J. UCS 8, 11
(2002), 1016.
[26] QUINLAN, J. Induction of decision trees. Machine learning 1, 1
(1986), 81–106.
[27] ROSENBLUM, N., ZHU, X., AND MILLER, B. Who wrote this
code? identifying the authors of program binaries. Computer
Security–ESORICS 2011 (2011), 172–189.
[28] SHEVERTALOV, M., KOTHARI, J., STEHLE, E., AND MAN-
CORIDIS, S. On the use of discretized source code metrics for au-
thor identiﬁcation. In Search Based Software Engineering, 2009
1st International Symposium on (2009), IEEE, pp. 69–78.
[29] SPAFFORD, E. H., AND WEEBER, S. A. Software forensics:
Can we track code to its authors? Computers & Security 12, 6
(1993), 585–595.
[30] STOLERMAN, A., OVERDORF, R., AFROZ, S., AND GREEN-
STADT, R. Classify, but verify: Breaking the closed-world as-
sumption in stylometric authorship attribution. In IFIP Working
Group 11.9 on Digital Forensics (2014), IFIP.
[31] WIKIPEDIA. Saeed Malekpour, 2014.
November-2014].
[Online; accessed 04-
[32] YAMAGUCHI, F., GOLDE, N., ARP, D., AND RIECK, K. Model-
ing and discovering vulnerabilities with code property graphs. In
Proc of IEEE Symposium on Security and Privacy (S&P) (2014).
[33] YAMAGUCHI, F., WRESSNEGGER, C., GASCON, H., AND
RIECK, K. Chucky: Exposing missing checks in source code
for vulnerability discovery.
In Proceedings of the 2013 ACM
SIGSAC Conference on Computer & Communications Security
(2013), ACM, pp. 499–510.
A Appendix: Keywords and Node Types
AdditiveExpression
ArgumentList
BitAndExpression
Callee
CastTarget
ConditionalExpression
ElseStatement
Expression
ForStatement
Identiﬁer
IdentiﬁerDeclType
IncDecOp
Label
OrExpression
ParameterType
RelationalExpression
ShiftExpression
SizeofOperand
UnaryExpression
WhileStatement
AndExpression
ArrayIndexing
BlockStarter
CallExpression
CompoundStatement
ContinueStatement
EqualityExpression
ExpressionStatement
FunctionDef
IdentiﬁerDecl
IfStatement
InclusiveOrExpression
MemberAccess
Parameter
PrimaryExpression
ReturnStatement
Sizeof
Statement
UnaryOp
Argument
AssignmentExpr
BreakStatement
CastExpression
Condition
DoStatement
ExclusiveOrExpression
ForInit
GotoStatement
IdentiﬁerDeclStatement
IncDec
InitializerList
MultiplicativeExpression
ParameterList
PtrMemberAccess
ReturnType
SizeofExpr
SwitchStatement
UnaryOperator
Table 10: Abstract syntax tree node types
Table 10 lists the AST node types generated by Joern
that were incorporated to the feature set. Table 11 shows
the C++ keywords used in the feature set.
alignas
auto
case
class
continue
double
export
friend
long
not
or_eq
reinterpret_cast
static
template
try
unsigned
wchar_t
alignof
bitand
catch
compl
decltype
dynamic_cast
extern
goto
mutable
not_eq
private
return
static_assert
this
typedef
using
while
and
bitor
char
const
default
else
false
if
namespace
nullptr
protected
short
static_cast
thread_local
typeid
virtual
xor
and_eq
bool
char16_t
constexpr
delete
enum
ﬂoat
inline
new
operator
public
signed
struct
throw
typename
void
xor_eq
asm
break
char32_t
const_cast
do
explicit
for
int
noexcept
or
register
sizeof
switch
true
union
volatile
Table 11: C++ keywords
USENIX Association  
24th USENIX Security Symposium  269
15
B Appendix: Original vs Obfuscated Code
Figure 6: A code sample X
Figure 6 shows a source code sample X from our
dataset that is 21 lines long. After obfuscation with Ti-
gress, sample X became 537 lines long. Figure 7 shows
the ﬁrst 13 lines of the obfuscated sample X.
Figure 7: Code sample X after obfuscation
270  24th USENIX Security Symposium 
USENIX Association
16