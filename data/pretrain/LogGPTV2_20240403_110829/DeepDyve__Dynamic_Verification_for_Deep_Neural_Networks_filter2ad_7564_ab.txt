misclassifications.
Existing Attacks. Recently, DNNs are shown to be vulnerable
to fault injection attacks [26]. In [12], the authors estimated that
40â€“50% of the parameters in a DNN model could lead to an ac-
curacy drop greater than 10% when bit-flips occur in their data
representation. While it was shown that quantized DNNs are more
resilient to fault injection attacks, the recent progressive BitFlip
Attack (BFA) [33] proposed by Rakin et. al. can reduce the accuracy
of a quantized ResNet-18 from 68.9% to 0.1% with only 13 bit-flips.
BFA combines gradient ranking and progressive search to identify
those vulnerable bits that degrade model accuracy significantly
when flipped. The follow-up work DeepHammer by Yao el al. [44]
proves the effectiveness of BFAs in practice, by row-hammering
against various real DNN systems.
Existing Defenses. For DNNs implemented on floating-point
machines, only a small fraction of the dynamic range provided by
Table 1: A Motivational example.
Checker DNN Size Accuracy Computational Overhead
âˆ¼21%
âˆ¼13%
âˆ¼16%
A
B
C
1%
5%
10%
80%
92%
94%
Figure 1: Comparison of the maximum activation value be-
tween benign and failure cases (because of fault attack) from
a quantized (8-bit integer) classifier.
the data type is used. If a fault makes the magnitude of intermediate
output values huge, it is likely to lead to a failure. Based on this ob-
servation, Li et al. proposed to use a simple threshold to detect those
faults that lead to intermediate outputs beyond it [23]. To be specific,
before deployment, they record the value ranges (ğ‘‹ğ‘šğ‘–ğ‘›, ğ‘‹ğ‘šğ‘ğ‘¥) of
the output for each layer. After deployment, the output value range
is then checked in the run-time. They consider a fault is detected if
there are output values beyond the range (1.1 Ã— ğ‘‹ğ‘šğ‘–ğ‘›, 1.1 Ã— ğ‘‹ğ‘šğ‘ğ‘¥).
This anomaly detector has very little hardware overhead, but its
detection capabilities are quite limited, especially for recent attacks
on quantized DNN systems. We show an example in Figure 1 where
the threshold-based anomaly detector fails to detect faults in quan-
tized classifiers. For the threshold detector to detect faults, at least
the maximum activation value should be beyond the normal range.
However, our experiment shows that the maximum activation val-
ues from all failure cases (due to hardware fault) are not bigger than
the normal boundary.
Li et al. [24] design a replication-based error detection technique
for deep neural networks. However, their overhead is quite high.
They spend 40% overhead to reach 60% fault coverage on CIFAR-10.
In the DeepHammer work [44], the authors discuss a few potential
mitigation techniques but do not provide any quantitative results
for their effectiveness.
2.2 Threat Model
In this work, we consider the attacker is trying to compromise the
accuracy of a DNN system by maliciously injecting faults into it.
Unlike crafting malicious inputs to fool DNN systems, we target
faults presented in the system internals, i.e., processing elements,
buffered weights, and intermediate values stored in on-chip buffers
or memories, and so on. We consider the attacker succeeds if the
modelâ€™s output class is different from the one obtained in an attack-
free environment.
We assume the attackers have full knowledge of the DNN and
its deployment on the device, including neural network topology,
parameters, and low-level implementation details, e.g., the position
of intermediate values stored in memories. Weak attackers could
launch random bit-flips, while for strong attackers, they can pre-
cisely locate and launch fault injection in the processing pipeline.
Moreover, we assume the same transient faults would not occur
in consecutive DNN inference runs. Firstly, reliability threats rarely
occur and the probability to occur repeatedly in a short period is
negligible. Secondly, it is very difficult, if not impossible, to launch
the same faults repeatedly in a DNN system. For example, in [44],
launching a rowhammer attack requires long preparation time
(several minutes). As long as the DNN inference time is short (and
usually it is), attackers do not have sufficient time to launch the
same attack in the second run.
2.3 Motivation
For any classification problem, there could be many DNN models
with different size/accuracy trade-offs to solve it. Although big
models often have higher accuracy, many inputs can be correctly
handled by small models. Therefore, we could employ a smaller
checker DNN to perform the same task, and they should output the
same results in most cases when faults do not occur. In this way, the
task model can be dynamically verified for online error detection
and recovery. Note that, it is not possible to achieve deterministic
dynamic verification for such systems because the outputs of a
simple model cannot achieve 100% consistency with that of the
original model.
As discussed earlier, because the checker DNN is less accurate,
there will be false positives and false negatives. Generally speaking,
the larger the checker DNN is, the more accurate it is [40], but it
does not necessarily lead to larger computational overhead. We
use the following example to illustrate the impact of checker DNN
design.
Suppose the task DNN model is with âˆ¼100% accuracy, and there
are three candidate checker DNN models: A, B, and C. Their relative
sizes compared to the task DNN and their classification accuracy
are shown in the second and the third column of Table 1. Consider
transient fault-induced failures are rare events, the computational
overhead of the three models is estimated in the fourth column,
which is the sum of the computational cost of the checker DNN
itself (as it is always on) and the re-computation cost on the task
DNN when the checker DNN produces a different classification
result (false positive cases).
If the misclassifications caused by faults are evenly distributed
among all classes, the fault coverage would be similar to the ac-
curacy of the checker DNN. For this particular example, checker
B can achieve âˆ¼92% fault coverage with 5% hardware cost and
âˆ¼13% computational cost, which significantly outperforms existing
fault-tolerant solutions for DNN designs. This has motivated the
proposed DeepDyve solution in this paper.
In practice, fault-induced misclassifications are usually not evenly
distributed. Moreover, misclassifying different classes often have
5060708090100110120130avtivation values020406080100frequencybenignfailureFigure 2: DeepDyve architecture and the design flow.
different risk implications for safety-critical systems. Therefore, for
any possible checker DNN, we need to consider the risk impact
and apply fault simulation to evaluate its effectiveness. This is a
time-consuming process. Therefore, the critical challenge is how to
efficiently explore the solution space of all possible checker DNN
designs to find the one with an optimized risk/overhead trade-off.
3 DEEPDYVE OVERVIEW
Figure 2 (a) depicts the proposed DeepDyve architecture. It con-
tains three parts: the original network, the checker DNN, and the
comparator. The checker DNN is a smaller and simpler DNN model
which approximates the original network and the original task.
An input instance is processed by both of the two models. Their
outputs are checked by the comparator and accepted if consistent.
Otherwise, the input instance is subject to a re-computation by the
original model, and the new prediction is accepted, regardless of
whether the two model outputs are consistent or not.
Design Goals. We first formally define the evaluation metrics (e.g.
coverage and overhead). We evaluate the cost of checker DNN
design by the introduced overhead. The overhead of the checker
DNNs can be calculated as follows:
ğ‘‚(ğ‘†) =
ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ğ‘ ğ‘šğ‘ğ‘™ğ‘™
ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ğ‘ğ‘–ğ‘”
,
(1)
,
ğ¹ ğ¿ğ‘‚ğ‘ƒ (ğ‘›ğ‘’ğ‘¡ğ‘ğ‘–ğ‘”)
ğ¹ ğ¿ğ‘‚ğ‘ƒ (ğ‘›ğ‘’ğ‘¡ğ‘ ğ‘šğ‘ğ‘™ğ‘™) + (1 âˆ’ ğ‘ƒğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘¡) Ã— ğ¹ ğ¿ğ‘‚ğ‘ƒ (ğ‘›ğ‘’ğ‘¡ğ‘ğ‘–ğ‘”)
ğ‘‚(ğ¶) =
(2)
ğ‘‚(ğ‘†) and ğ‘‚(ğ¶) stand for the storage overhead and computational
overhead, respectively, wherein ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘  stands for the storage re-
quirement of model parameters with unit of Mega Bytes (ğ‘€ğµ), and
ğ¹ ğ¿ğ‘‚ğ‘ƒ function calculates the number of multiply-accumulation
operations in the network. The computational overhead contains
two parts: FLOP of the small network (static overhead) and the
re-computation overhead (dynamic overhead) when the small net-
work output is different from that of the big one (with probability
1 âˆ’ ğ‘ƒğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘¡).
The detection ability of DeepDyve is characterized by the cover-
age rate. A popular definition of fault coverage would be number of
classification failures detected among all mis-classifications caused
by faults, as show in in Equation 3. ğ·ğ¹ğ‘–,ğ‘— stands for the detected
failures mis-classified from class ğ‘– to class ğ‘—, and ğ‘‡ ğ¹ğ‘–,ğ‘— denotes the
total failures from class ğ‘– to class ğ‘— when faults occur. To take the
different risk impact of different failures on safety-critical applica-
tion into consideration, we introduce a new metric called weighted
coverage, abbreviated as ğ‘Š ğ¶ğ‘œğ‘£. in Equation 4. ğ¼ğ‘–,ğ‘— is the risk impact
if class ğ‘– is mis-classified into class ğ‘—, which will be defined later
in this Section. Note that ğ¶ğ‘œğ‘£. is a special case of ğ‘Š ğ¶ğ‘œğ‘£. when all
misclassifications have the same risk impact. In later text, we use
coverage and weighted coverage interchangeably and they both
refer to weighted coverage if not specified.
.
(3)
ğ‘– ğ‘— ğ·ğ¹ğ‘– ğ‘—
ğ‘– ğ‘— ğ‘‡ ğ¹ğ‘– ğ‘—
ğ¶ğ‘œğ‘£. =
ğ‘– ğ‘— ğ·ğ¹ğ‘– ğ‘— Ã— ğ¼ğ‘– ğ‘—
ğ‘– ğ‘— ğ‘‡ ğ¹ğ‘– ğ‘— Ã— ğ¼ğ‘– ğ‘—
ğ‘Š ğ¶ğ‘œğ‘£. =
, âˆ€ğ‘–, ğ‘— âˆˆ ğ‘ and ğ‘– â‰  ğ‘— .
(4)
Design Stages. Under the guidance of the design goals, there are
mainly two stages in designing of the checker DNN and we show
them in Figure 2 (b). The first stage is architecture exploration,
where we initialize the architecture of the checker DNN. Given a
task model, a pool of checker DNN candidates with the same task of
the given model are generated with model compression techniques.
Then, one of them is picked from the pool by evaluating their over-
head and fault coverage, detailed in Section 4. The second stage is
task exploration, where we try to manipulate the classification tasks
performed by the checker DNN to achieve better coverage/overhead
trade-off. That is, we can find a better solution by providing more
design options at the task level, detailed in Section 5.
To solve the above design exploration problems, we define the
following three matrices:
â€¢ Risk impact matrix I âˆˆ RNÃ—N. In safety-critical DNN appli-
cations, the risk impact of different misclassifications may
vary significantly from the system perspective. Each entry
in I denotes the cost of the corresponding misclassification
(the larger the value, the higher the cost). As the actual risk
impact depends on the application, the values in the impact
matrix should be carefully determined by system designers.
â€¢ Risk probability matrix R âˆˆ RNÃ—N, where the entry ğ‘…ğ‘– ğ‘— de-
notes the probability that the ğ‘–-th class is misclassified to
ğ‘—-th class when faults occur, and ğ‘ denotes the total number
of classes. Risk probability matrix is obtained from fault in-
jection experiments. Figure 3 shows an example of R drawn
from CIFAR-10 dataset by performing random fault injection
on VGG-16 for 400,000 times. From this example, we can
OutputConvInputConv filterConvOutputComparatorOriginal DNNChecker DNN NoRe-computeYesAccept(a) Architecture overviewArchitecture Exploration(b) Design stagesTask Exploration1.0x0.8x0.5xCandidate Checker DNN modelsOriginal DNNInit.Checker DNN Candidate Architecture GenerationArchitecture SelectionChecker DNNCandidate TasksN-class(N-1)-class(N-2)-classTask SelectionCandidate Task GenerationInitial Checker DNN Table 2: ResNet-10 with different width multiplier.
Accuracy(%) O(S) (MB) O(C) (GFLOPs)
ğ›¼
1.0
0.7
0.5
0.3
97.54
96.94
96.20
95.75
1.23
0.60
0.31
0.12
0.06
0.03
0.02
0.01
Figure 3: An example of risk probability matrix obtained
from CIFAR-10 (normalized for visualization).
observe fault-induced misclassifications are far from evenly
distributed.