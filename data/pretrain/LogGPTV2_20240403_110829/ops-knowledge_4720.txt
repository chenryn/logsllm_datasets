User: I'm using celery with django and am storing the task results in the DB.
I'm considering having a single set of workers reading messages from a single message broker. Now I can have multiple clients submitting celery tasks and each client will have tasks and their results created/stored in a different DB.
Even though the workers are common, they know which DB to operate upon for each task.
Can I have duplicate task ids generated because they were submitted by different clients pointing to different DBs?
Thanks,
Assistant: Eventually you will have duplicates. Many people ignore this issue because it is a "low probability", and then are surprised when it hits them. And then a story leaks how someone was logged into another uses Facebook account. If you require them to always be unique then you will have to prefix each ID with something that will never repeat - like current date and time with microseconds.
And if that is not good enough, because there still is a even tinier chance of a collision, you can create a small application that will generate those prefixes, and will add an counter (incremented after each hash request, and reset every couple seconds) to the date and microseconds. It will have to work in single-threaded mode, but this will be a guarantee to generate unique prefixes that won't collide.