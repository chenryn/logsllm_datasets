- 找出重复冗余索引
- 索引不包含NULL
- 短索引
- 排序的索引问题
- like语句前%不会使用索引
- 列上运算问题
- NOT IN会进行全表扫描
## 数据库结构优化
- 选择合适的数据类型
- 范式化
- 反范式化
- 垂直拆分
![2020310201242](/assets/2020310201242.jpg)
使用垂直切分将按数据库中表的密集程度部署到不同的库中
切分后部分表无法join，只能通过接口方式解决，提高了系统复杂度，存在分布式事务问题
- 水平拆分
![2020310201126](/assets/2020310201126.jpg)
当一个表的数据不断增多时，水平拆分是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力
### 分库分表
同上面的水平拆分，每张表或者每个库只存储一定量的数据，当需要进行数据读写时，根据唯一ID取模得到数据的位置
**为什么分库分表能提高性能**
将一张表的数据拆分成多个n张表进行存放，然后使用第三方中间件（MyCat或者Sharding-JDBC）可以并行查询
**一些分库分表中间件**
cobar，tddl，atlas，**sharing-jdbc**，**my-cat**
**系统迁移到分库分表**
如何将一个单裤单表的系统动态迁移到分库分表上去
- 停机迁移
禁止全部数据写入，编写一个程序，将单库单表的数据写到分库分表上
![批注 2020-03-20 163424](/assets/批注%202020-03-20%20163424.png)
- 双写迁移
新系统部署后，每条数据都会在老库和新库写一遍
同时后台开启一个数据库迁移工具，这个工具负责把老库的数据写到新库去，写到新库的条件是，老库有的数据新库没有或者是 老库的数据更新时间比新库的新
工具会比较新库与老库的每一条数据，只有每条数据都一致，才算完成，否则继续新一轮迁移
这样工具几轮操作过去后，新老库的数据就一致了
![批注 2020-03-20 163921](/assets/批注%202020-03-20%20163921.png)
**动态扩容缩容的分库分表方案**
- 停机扩容
同上，只不过上面那是从单个数据库到多个数据库，这次这个是多个数据库到多个数据库
但是不推荐这种做法，原因是数据量很大，数据很难在短时间内转移完毕
- 第一次分库分表，就一次性给他分个够
32 个库，每个库 32 个表
这里可以多个库都在同一台机器上，当不够用的时候，可以将这些库转移到新机器上
这样，数据的逻辑位置没有发生改变，也避免扩容缩容带来的数据迁移问题
- 级联同步迁移
比较适合数据从自建机房向云上迁移的场景，在切写的时候需要短暂的停止写入
![2022816205127](/assets/2022816205127.webp)
#### 唯一ID生成
- 使用一个单点系统来做自增ID的获取
  - 必须要能应对时钟回拨，或者服务器异常重启之后计数器不会重复的问题
  - redis、数据库自带的自增
  - 多个节点的ID获取无法并行
- 使用多个单点系统，每个系统自增ID设置相同的步长不同的初始值，这样就能保证这些节点ID不会重复
  - 但这种方式注定了节点数量不能变化
为了避免每次生成都需要一次调用，在需要产生新的全局 ID 的时候，每次单点服务都向数据库批量申请 n 个 ID，在本地用内存维护这个号段，并把数据库中的 ID 修改为当前值 +n，直到这 n 个 ID 被耗尽；下次需要产生新的全局 ID 的时候，再次到数据库申请一段新的号段
- UUID
  - UUID组成部分:当前日期和时间+时钟序列+随机数+全局唯一的IEEE机器识别号
  - 比较长，无法保证趋势递增，做索引时查询效率低
- 系统时间
  - 可以使用业务字段来拼接避免重复
- 雪花算法
  - 一个 64 位的 long 型的 id，第一个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号
  - 单个节点内无法并行
  - 多个节点可以并行
  - 可以支撑每秒400万+的ID生成，在某些实现里，如果某一毫秒内的计数器被耗尽达到上限，会死循环直至这 1ms 过去
### 拆分策略
使用水平拆分时，操作一条数据，要在哪张表找到它
- 哈希取模
- 范围，ID范围，时间范围
- 映射表
### 拆分后的问题
- 事务
  - 使用分布式事务
- 连接
  - 原来的连接需要分解成多个单表查询，在应用层进行连接
- ID唯一性
  - 全局唯一ID（GUID）
  - 每个分片指定ID范围
  - 分布式ID生成器，雪花算法
## 数据访问优化
### 减少请求的数据量
- SELECT 只返回必要的列
- 使用LIMIT只返回必要的行
- 在内存缓存数据避免查询数据库
### 减少扫描行数
使用索引覆盖来覆盖查询
## 查询方式优化
### 分解大查询
一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源
### 分解大连接查询
将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联
- 可以有效利用缓存
- 减少锁竞争
- 应用层拼接数据，数据库拆分更容易，从而做到高性能和可伸缩
- 单表查询效率可能比连接高
# 配置优化
- 设置文件最大打开数
- 设置最大连接数
- 设置back_log
  - 存放等待连接的堆栈大小
- interactive_timeout
- 缓冲区
  - key_buffer_size
  - query_cache_size
  - record_buffer_size
  - read_rnd_buffer_size
  - sort_buffer_size
  - join_buffer_size
  - tmp_table_size
  - table_cache
  - max_heap_table_size
  - thread_cache_size
  - thread_concurrency
  - wait_timeout
- 关于InnoDB
# 执行顺序
![](/assets/202339154439.png)
- FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1
- ON: 对虚表VT1进行ON筛选，只有那些符合``的行才会被记录在虚表VT2中。
- JOIN： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, rug from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。
- WHERE： 对虚拟表VT3进行WHERE条件过滤。只有符合``的记录才会被插入到虚拟表VT4中。
- GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5.
- CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6.
- HAVING： 对虚拟表VT6应用having过滤，只有符合``的记录才会被 插入到虚拟表VT7中。
- SELECT： 执行select操作，选择指定的列，插入到虚拟表VT8中。
- DISTINCT： 对VT8中的记录进行去重。产生虚拟表VT9.
- ORDER BY: 将虚拟表VT9中的记录按照``进行排序操作，产生虚拟表VT10.
- LIMIT：取出指定行的记录，产生虚拟表VT11, 并将结果返回。