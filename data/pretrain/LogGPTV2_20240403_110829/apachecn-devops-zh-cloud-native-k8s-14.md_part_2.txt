特使中的集群代表一种逻辑服务，在这种服务中，请求可以被路由到侦听器中基于的路由。在云原生环境中，一个集群可能包含多个可能的 IP 地址，因此它支持负载平衡配置，例如*循环*。
### 特使端点
最后，端点在集群中被指定为服务的一个逻辑实例。特使支持从一个应用编程接口获取一个端点列表(这本质上是在 Istio 服务网格中发生的)以及它们之间的负载平衡。
在 Kubernetes 上的生产特使部署中，很可能会使用某种形式的动态、应用编程接口驱动的特使配置。特使的这个特性叫做 xDS，由 Istio 使用。此外，还有其他使用特使和 xDS 的开源产品和解决方案，包括大使应用编程接口网关。
出于本书的目的，我们将研究一些静态(非动态)特使配置；这样，我们就可以将配置的每一部分分开，当我们回顾 Istio 时，您会对每件事的工作原理有一个很好的了解。
现在，让我们进入特使配置的设置，其中单个 Pod 需要能够将请求路由到两个服务，即*服务 1* 和*服务 2* 。设置如下所示:
![Figure 14.2 – Outbound envoy proxy](img/B14790_14_002.jpg)
图 14.2–出站代理代理
如您所见，我们的应用 Pod 中的特使边车将具有路由到两个上游服务的配置，即*服务 1* 和*服务 2* 。两种服务都有两个可能的端点。
在特使 xDS 的动态设置中，端点的 Pod IPs 将从 API 加载，但是为了我们的审查目的，我们将在端点中显示静态 Pod IPs。我们将完全忽略 Kubernetes 服务，而是以循环配置直接访问 Pod IPs。在服务网格场景中，特使也将被部署在所有目的地 Pods 上，但是我们现在将保持简单。
现在，让我们看看这个网络图是如何在 YAML 特使配置中配置的(您可以在代码库中找到它的全部内容)。当然，这与 YAML 的 Kubernetes 资源有很大的不同——我们稍后会讲到这一部分。整个配置涉及很多 YAML，所以让我们一点一点来看。
### 了解特使配置文件
首先，让我们看一下配置的前几行——关于特使设置的一些基本信息:
特使-配置. yaml:
```
admin:
  access_log_path: "/dev/null"
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
```
如您所见，我们为特使的`admin`指定了一个端口和地址。与以下配置一样，我们将特使作为边车运行，因此地址将始终是本地的–`0.0.0.0`。接下来，我们从一个 HTTPS 听众开始我们的听众列表:
```
static_resources:
  listeners:
   - address:
      socket_address:
        address: 0.0.0.0
        port_value: 8443
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager
          stat_prefix: ingress_https
          codec_type: auto
          route_config:
            name: local_route
            virtual_hosts:
            - name: backend
              domains:
              - "*"
              routes:
              - match:
                  prefix: "/service/1"
                route:
                  cluster: service1
              - match:
                  prefix: "/service/2"
                route:
                  cluster: service2
          http_filters:
          - name: envoy.filters.http.router
            typed_config: {}
```
如您所见，对于每个特使侦听器，我们都有一个侦听器的本地地址和端口(该侦听器是 HTTPS 侦听器)。然后，我们有一个过滤器列表——尽管在这种情况下，我们只有一个。每个特使过滤器类型的配置略有不同，我们不会逐行查看(更多信息请查看[https://www.envoyproxy.io/docs](https://www.envoyproxy.io/docs)特使文档)，但是这个特定过滤器匹配两条路线，`/service/1`和`/service/2`，并将它们路由到两个特使集群。仍然在 YAML 的第一个 HTTPS 监听器部分，我们有 TLS 配置，包括证书:
```
      transport_socket:
        name: envoy.transport_sockets.tls
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
          common_tls_context:
            tls_certificates:
              certificate_chain:
                inline_string: |
              private_key:
                inline_string: |
```
如你所见，这个配置通过了一个`private_key`和一个`certificate_chain`。接下来，我们有第二个也是最后一个侦听器，一个 HTTP 侦听器:
```
  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 8080
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            name: local_route
            virtual_hosts:
            - name: backend
              domains:
              - "*"
              routes:
              - match:
                  prefix: "/service1"
                route:
                  cluster: service1
              - match:
                  prefix: "/service2"
                route:
                  cluster: service2
          http_filters:
          - name: envoy.filters.http.router
            typed_config: {}
```
如您所见，这种配置与我们的 HTTPS 监听器非常相似，只是它监听不同的端口，并且不包括证书信息。接下来，我们进入集群配置。在我们的例子中，我们有两个集群，一个用于`service1`，一个用于`service2`。`service1`第一关:
```
  clusters:
  - name: service1
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    load_assignment:
      cluster_name: service1
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: service1
                port_value: 5000
```
接下来，`Service 2`:
```
  - name: service2
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    load_assignment:
      cluster_name: service2
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: service2
                port_value: 5000
```
对于这些集群中的每一个，我们指定请求应该被路由到哪里，以及哪个端口。例如，对于我们的第一个集群，请求被路由到`http://service1:5000`。我们还为连接指定了负载平衡策略(在本例中为循环)和超时。现在我们有了特使配置，我们可以继续创建我们的 Kubernetes Pod，并在特使配置的同时注入我们的边车。我们还将把这个文件分成两部分，因为它有点太大，无法理解:
特使-边车-部署. yaml:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: my-service
    spec:
      containers:
      - name: envoy
        image: envoyproxy/envoy:latest
        ports:
          - containerPort: 9901
            protocol: TCP
            name: envoy-admin
          - containerPort: 8786
            protocol: TCP
            name: envoy-web
```
可以看到，这是典型的 YAML 部署。在这种情况下，我们实际上有两个容器。首先是特使代理容器(或边车)。它监听两个端口。接下来，再往下移动到 YAML，我们有了第一个容器的卷挂载(保存特使配置)以及一个启动命令和参数:
```
        volumeMounts:
          - name: envoy-config-volume
            mountPath: /etc/envoy-config/
        command: ["/usr/local/bin/envoy"]
        args: ["-c", "/etc/envoy-config/config.yaml", "--v2-config-only", "-l", "info","--service-cluster","myservice","--service-node","myservice", "--log-format", "[METADATA][%Y-%m-%d %T.%e][%t][%l][%n] %v"]
```
最后，我们在 Pod 中有了第二个容器，它是一个应用容器:
```
- name: my-service
        image: ravirdv/http-responder:latest
        ports:
        - containerPort: 5000
          name: svc-port
          protocol: TCP
      volumes:
        - name: envoy-config-volume
          configMap:
            name: envoy-config
            items:
              - key: envoy-config
                path: config.yaml
```
如您所见，这个应用在端口`5000`上响应。最后，我们还有 Pod 级别的卷定义，以匹配特使容器中装载的特使配置卷。在我们创建部署之前，我们需要用我们的特使配置创建一个`ConfigMap`。我们可以使用以下命令来实现这一点:
```
kubectl create cm envoy-config 
--from-file=config.yaml=./envoy-config.yaml
```
这将导致以下输出:
```
Configmap "envoy-config" created
```
现在，我们可以使用以下命令创建部署:
```
kubectl apply -f deployment.yaml
```
这将导致以下输出:
```
Deployment "my-service" created
```
最后，我们需要我们的下游服务，`service1`和`service2`。为此，我们将继续使用`http-responder`开源容器映像，它将在端口`5000`上做出响应。部署和服务规范可以在代码存储库中找到，我们可以使用以下命令创建它们:
```
kubectl create -f service1-deployment.yaml
kubectl create -f service1-service.yaml
kubectl create -f service2-deployment.yaml
kubectl create -f service2-service.yaml
```
现在，我们可以测试一下我们的特使配置了！从我们的`my-service`容器中，我们可以使用`/service1`路径向端口`8080`上的本地主机发出请求。这应该指向我们的一个`service1`Pod  IPs。要发出这个请求，我们使用以下命令:
```
Kubectl exec  -it -- curl localhost:8080/service1
```
我们已经建立了服务来响应他们的请求。请看我们的`curl`命令的以下输出:
```
Service 1 Reached!
```
现在我们已经了解了特使如何使用静态配置工作，让我们继续讨论基于特使的动态服务网格——Istio。
# 向 Kubernetes 添加服务网格
一个*服务网格*模式是边车代理的逻辑扩展。通过将侧车代理连接到每个 Pod，服务网格可以控制服务对服务请求的功能，例如高级路由规则、重试和超时。此外，通过让每个请求都通过代理，服务网格可以实现服务之间的相互 TLS 加密，以提高安全性，并可以让管理员难以置信地观察到他们群集中的请求。
有几个服务网格项目支持 Kubernetes。最受欢迎的如下:
*   *浪费*
*   *左*
*   *熊掌*
*   领事
这些服务网格中的每一个都有不同的服务网格模式。 *Istio* 可能是最受欢迎和最全面的单一解决方案，但也相当复杂。 *Linkerd* 也是一个成熟的项目，但是更容易配置(虽然它使用自己的代理而不是特使)。*领事*是除了其他提供商之外支持特使的选项，不仅仅是在 Kubernetes 上。最后，*库马*是一个基于特使的选项，也越来越受欢迎。
探索所有选项超出了本书的范围，所以我们将坚持使用 Istio，因为它通常被认为是默认的解决方案。也就是说，所有这些网格都有的优缺点，在计划采用服务网格时，每一个都值得一看。
## 在 Kubernetes 群岛上建立 Istio
虽然 Istio 可以用 Helm 安装，但是 Helm 安装选项不再是官方支持的安装方式。
相反，我们使用`Istioctl`命令行界面工具将带配置的 Istio 安装到我们的集群上。这种配置可以完全定制，但就本书而言，我们将只使用“演示”配置:
1.  在集群上安装 Istio 的第一步是安装 Istio CLI 工具。我们可以通过以下命令来完成，该命令将安装最新版本的 CLI 工具:
    ```
    curl -L https://istio.io/downloadIstio | sh -
    ```