title:On the Resilience of Biometric Authentication Systems against Random
Inputs
author:Benjamin Zi Hao Zhao and
Hassan Jameel Asghar and
Mohamed Ali Kâafar
On the Resilience of Biometric Authentication
Systems against Random Inputs
Benjamin Zi Hao Zhao
University of New South Wales
and Data61 CSIRO
PI:EMAIL
Hassan Jameel Asghar
Macquarie University
and Data61 CSIRO
PI:EMAIL
Mohamed Ali Kaafar
Macquarie University
and Data61 CSIRO
PI:EMAIL
Abstract—We assess the security of machine learning based
biometric authentication systems against an attacker who submits
uniform random inputs, either as feature vectors or raw inputs,
in order to ﬁnd an accepting sample of a target user. The average
false positive rate (FPR) of the system, i.e., the rate at which an
impostor is incorrectly accepted as the legitimate user, may be
interpreted as a measure of the success probability of such an
attack. However, we show that the success rate is often higher than
the FPR. In particular, for one reconstructed biometric system
with an average FPR of 0.03, the success rate was as high as
0.78. This has implications for the security of the system, as an
attacker with only the knowledge of the length of the feature space
can impersonate the user with less than 2 attempts on average.
We provide detailed analysis of why the attack is successful, and
validate our results using four different biometric modalities and
four different machine learning classiﬁers. Finally, we propose
mitigation techniques that render such attacks ineffective, with
little to no effect on the accuracy of the system.
I.
INTRODUCTION
Consider a machine learning model trained on some user’s
data accessible as a black-box API for biometric authentica-
tion. Given an input (a biometric sample), the model outputs
a binary decision, i.e., accept or reject, as its prediction for
whether the input belongs to the target user or not. Now
imagine an attacker with access to the same API who has never
observed the target user’s inputs. The goal of the attacker is to
impersonate the user by ﬁnding an accepting sample (input).
What is the success probability of such an attacker?
Biometric authentication systems are generally based on ei-
ther physiological biometrics such as ﬁngerprints [1], face [2],
[3], and voice [4], [5]), or behavioral biometrics such as
touch [6] and gait [7], the latter category generally used for
continuous and implicit authentication of users. These systems
are mostly based on machine learning: a binary classiﬁer is
trained on the target user’s data (positive class) and a subset
of data from other users (negative class). This process is used
to validate the performance of the machine learning classiﬁer
and hence the biometric system [8], [7], [9], [10], [11], [12],
[13], [14], [15], [16]. The resulting proportion of negative
samples (other users’ data) successfully gaining access (when
Network and Distributed Systems Security (NDSS) Symposium 2020
23-26 February 2020, San Diego, CA, USA
ISBN 1-891562-61-4
https://dx.doi.org/10.14722/ndss.2020.24210
www.ndss-symposium.org
they should have been rejected) produces the false positive
rate (FPR, also referred as False Acceptance Rate). The target
user’s model is also veriﬁed for their own samples, establishing
the false reject rate (FRR). The parameters of the model can
be adjusted to obtain the equal error rate (EER) at which point
the FPR equals FRR.
Returning to our question, the FPR seems to be a good
indicator of the success probability of ﬁnding an accepting
sample. However, this implicitly assumes that the adversary is
a human who submits samples using the same human computer
interface as other users, e.g., a smartphone camera in case of
face recognition. When the model is accessible via an API the
adversary has more freedom in choosing its probing samples.
This may happen when the biometric service is hosted on
the cloud (online setting) or within a secure enclave on the
user’s device (local setting). In particular, the attacker is free
to sample uniform random inputs. It has previously been stated
that the success probability of such an attack is exponentially
small [17] or it can be derived from the FPR of the system [18],
[19].1
In this paper, we show that uniform random inputs are
accepted by biometric systems with a probability that is often
higher and independent of the FPR. Moreover, this applies
to the setting where the API to the biometric system can be
queried using feature vectors after processing raw input as
well as at the raw input level. A simple toy example with
a single feature can illustrate the reason for the efﬁcacy of the
attack. Suppose the feature is normalized within the interval
[0, 1]. All of target user’s samples (the positive class) lie in
the interval [0, 0.5) and the other users’ samples (the negative
class) lie in the interval (0.5, 1]. A “classiﬁer” decides the
decision boundary of 0.5, resulting in identically zero FRR and
FPR. However, a random sample has a 50% chance of being
accepted by the biometric system.2 The success of the attack
shows that the FPR and FRR, metrics used for reporting the
accuracy of the classiﬁer, cannot alone be used as proxies for
assessing the security of the biometric authentication system.
Our main contributions are as follows:
1We note that these observations are made for distance-based authentication
algorithms and not machine-learning model based algorithms. See Sections
V-C and VIII for more details.
2This example is an oversimpliﬁcation. In practice the training data is almost
never nicely separated between the two classes. Also, in higher dimensions
one expects exponentially small volume covered by samples from the positive
and negative classes as is explained in Section III.
• We theoretically and experimentally show that in machine
learning-based biometric authentication systems, the ac-
ceptance region, deﬁned as the region in feature space
where the feature vectors are accepted by the classiﬁer,
is signiﬁcantly larger than the true positive region, i.e., the
region where the target users samples lie. Moreover, this
is true even in higher dimensions, where the true positive
region tends to be exponentially small [20].
• As a consequence of the above, we show that an attacker
who has access to a biometric system via a black-box
feature vector API, can ﬁnd an accepting sample by simply
generating random inputs, at a rate which in many cases
is higher than implicated by the FPR. For instance, the
success probability of the attack is as high as 0.78 for one
of the systems whose EER is only 0.03. The attack requires
minimum knowledge of the system:
the attacker only
needs to know the length of the input feature vector, and
permissible range of each feature value (if not normalized).
• We show that the success rate of a random input attack can
also be higher than FPR if the attacker can only access the
API at the raw input level (before feature extraction). For
instance, on one system with an EER of 0.05, the success
rate was 0.12. We show that the exponentially small region
spanned by these raw random inputs rarely overlaps with
the true positive region of any user in the system, owing to
the success probability of the attack. Once again the attack
only requires minimum knowledge of the system, i.e., the
range of values taken by each raw input.
• To analyze real-world applicability of the attack, we re-
construct four biometric authentication schemes. Two of
them are physiological, i.e., face recognition [3] and voice
authentication [4]. The other two use behavioral
traits,
i.e., gait authentication [21], and touch (swipes) authen-
tication [6], [22]. For each of these modalities, we use
four different classiﬁers to construct
the corresponding
biometric system. The classiﬁers are linear support vector
machines (SVM), radial SVM, random forests, and deep
neural networks. For each of these systems, we ensure
that our implementation has comparable performance to
the reference.
• Our experimental evaluations show that the average ac-
ceptance region is higher than the EER in 9 out of
16 authentication conﬁgurations (classiﬁer-modality pairs),
and only one in the remaining 7 has the (measured) average
acceptance region of zero. Moreover, for some users this
discrepancy is even higher. For example, in one user model
(voice authentication using random forests) the success rate
of the random (feature) input is 0.55, when the model’s
EER is only 0.03, consistent with the system average EER
of 0.03.
• We propose mitigation techniques for both the random
feature vector and raw input attacks. For the former, we
propose the inclusion of beta-distributed noise in the train-
ing data, which “tightens” the acceptance region around
the true positive region. For the latter, we add feature
vectors extracted from a sample of raw inputs in the
training data. Both strategies have minimal
impact on
the FPR and TPR of the system. The mitigation strategy
renders the acceptance region to virtually 0 for 6 of the 16
authentication conﬁgurations, and for 15 out of 16, makes
it lower than FPR. For reproducibility, we have made our
codebase public.3
We note that a key difference in the use of machine learning
in biometric authentication as compared to its use in other areas
(e.g., predicting likelihood of diseases through a healthcare
dataset) is that the system should only output its decision:
accept or reject [23], and not the detailed conﬁdence values,
i.e., conﬁdence of the accept or reject decision. This makes
our setting different from membership inference attacks where
it is assumed that the model returns a prediction vector, where
each element is the conﬁdence (probability) that the associated
class is the likely label of the input sample [24], [25]. In other
words, less information is leaked in biometric authentication.
Conﬁdence vectors can potentially allow an adversary to ﬁnd
an accepting sample by using a hill climbing approach [18],
for instance.
II. BACKGROUND AND THREAT MODEL
A. Biometric Authentication Systems
The use of machine learning for authentication is a binary
classiﬁcation problem.4 The positive class is the target user
class, and the negative class is the class of one or more other
users. The target user’s data for training is obtained during
the registration or enrollment phase. For the negative class,
the usual process is to use the data of a subset of other users
enrolled in the system [27], [8], [7], [9], [10], [11], [12], [13],
[14], [15], [16]. Following best machine learning practice, the
data (from both classes) is split into a training and test set. The
model is learned over the training set, and the performance of
the classiﬁer, its misclassiﬁcation rate, is evaluated on the test
set.
A raw biometric sample is usually processed to extract
relevant features such as ﬁngerprint minutiae or frequency
energy components of speech. This deﬁnes the feature space
for classiﬁcation. As noted earlier, the security of the bio-
metric system is evaluated via the misclassiﬁcation rates of
the underlying classiﬁer. Two types of error can arise. A
type 1 error is when a positive sample (target user sample)
has been erroneously classiﬁed as negative, which forms the
false reject rate (FRR). Type 2 error occurs when a negative
sample (from other users) has been misclassiﬁed as a positive
sample, resulting in the false positive rate (FPR). By tuning
the parameters of the classiﬁer, an equal error rate (EER) can
be determined which is the rate at which FRR equals FPR.
One can also evaluate the performance of the classiﬁer through
the receiver operator characteristic (ROC) curve, which shows
the full relationship between FRR and FPR as the classiﬁer
parameters are varied.
Once a biometric system is set up, i.e., classiﬁer trained, the
system takes as input a biometric sample and outputs accept
or reject. In a continuous authentication setting, where the
user is continually being authenticated in the background, the
biometric system requires a continuous stream of user raw
inputs. It has been shown that in continuous authentication
systems the performance improves if the decisions is made on
3Our code is available at: https://imathatguy.github.io/Acceptance-Region
4We note that sometimes a discrimination model [26] may also be consid-
ered where the goal is to identify the test sample as belonging to one of n
users registered in the system. Our focus is on the authentication model. Also,
see Section VII.
2
the average feature vector from a set of feature vectors [28],
[22], [29].
camera, and can thus programmatically submit input samples.
There are a number of ways in which this is possible.
B. Biometric API: The Setting
We consider the setting where the biometric system can
be accessed via an API. More speciﬁcally, the API can be
queried by submitting a biometric sample. The response is
a binary accept/reject decision.5 The biometric system could
be local, in which case the system is implemented in a secure
enclave (a trusted computing module), or cloud-based (online),
in which the decision part of the system resides in a remote
server. We consider two types of APIs. The ﬁrst type requires
raw biometric samples, e.g., the input image in case of face
recognition. The second type accepts a feature vector, implying
that the feature extraction phase is carried out before the API