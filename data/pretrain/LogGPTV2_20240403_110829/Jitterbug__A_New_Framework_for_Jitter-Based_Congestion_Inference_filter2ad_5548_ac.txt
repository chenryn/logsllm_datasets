Amplitude Signals
Figure 7 shows that only the jitter dispersion method labels periods of elevated
latency as periods of congestion. We believe that the stability in the jitter time
series at periods of elevated latency impedes the KS-test method’s inferences.
This jitter stability may be due to small buﬀers (diﬀerences between peak and
valley values is 30 ms) or traﬃc engineering on the far side network, which
in this case is a large Content Provider. On the other hand, the high ampli-
tude of the phase transitions in the jitter dispersion time series allows the JD
method to detect diﬀerences in the mean value of this signal during periods of
elevated latency. We note that the change point detection module is not capable
of detecting period of elevated latency between January 1, 2018 and January
Jitterbug: A New Framework for Jitter-Based Congestion Inference
167
Fig. 7. KS-test and Jitter dispersion congestion inferences for a periodic small-
amplitude signal. Only the jitter dispersion method infers congestion from this recur-
rent pattern, which we speculate relates to small buﬀers that keep jitter itself relatively
stable. Remarkably, the change point detection algorithm was not able to capture some
periods of elevated latency. Red-ﬁlled intervals indicate periods of congestion. (Color
ﬁgure online)
3, 2018. (The slightly smoother transition during this period trace could have
hindered the accuracy of the change point detection algorithm.)
5.3 Scenario 3: One-Oﬀ Period of Elevated Latency with No
Congestion
Fig. 8. KS-test and Jitter dispersion congestion inferences for a one-oﬀ event suspected
as a route change. Inferences for this case indicate no congestion. Red-ﬁlled intervals
indicate periods of congestion. (Color ﬁgure online)
Figure 8 shows an example in which neither method infers congestion. In this
case, we do not observe any change in either the jitter time series or the jitter
dispersion either before or after the period of elevated latency. We suppose that
this period corresponds to a route change based on the stability of the jitter
time series and the clean proﬁle of the min time series during the transition.
Since there is no simultaneous increase in near-side RTT (orange curve in Fig. 8
top panel), we believe that a route changed in the reverse path from the far-side
router.
168
E. Carisimo et al.
5.4 Scenario 4: One-Oﬀ Period of Elevated Latency with Congestion
Fig. 9. KS-test and Jitter dispersion congestion inferences for a one-oﬀ congestion
event. In this case, both methods infer congestion during periods of elevated latency.
Red-ﬁlled intervals indicate periods of congestion. (Color ﬁgure online)
(Figure 9) Congestion inferences from both methods partially agree on classifying
this one-time episode of high amplitude latency spikes as a period of congestion.
Detection of multiple change points, and the fact that the period in between has
slightly smaller mean value in the min time series, generate that the period of
congestion inferred is smaller than the actual period of elevated latency.
5.5 Scenario 5: One-Oﬀ Event During Recurrent Periods
of Elevated Latency
The biggest challenge for latency-based congestion detection is to distinguish
congestion-induced elevated latency from other path anomalies, such as a route
change. Figure 10 shows two examples of KS-test and jitter dispersion conges-
tion inferences when route changes occur in the middle of recurrent periods of
elevated latency. In these cases, we conﬁrm that the events occurring on March
20, 2017 at 12pm (Fig. 10a) and on April 20, 2017 before midnight (Fig. 10b) are
route changes in the internal network of the ISP since the near- (orange) and far-
side (blue) min time series detect an elevation simultaneously. As we expected
for a route change, these events do not show any change in jitter signals. Our
method used the jitter dispersion metric to correctly rule out a candidate con-
gestion period as a route change (rather than congestion), due to low jitter
dispersion which we know is not strongly correlated with congestion dynam-
ics. This example illustrates the importance of jitter dynamics in detection of
network congestion events.
Jitterbug: A New Framework for Jitter-Based Congestion Inference
169
(a)
(b)
Fig. 10. Two examples of suspected route changes in the middle of recurrent periods of
elevated latency. Neither method inferred any congestion. Red-ﬁlled intervals indicate
periods of congestion. (Color ﬁgure online)
5.6 Scenario 6: Change Point Detection Over-Detects Change
Points
We use an additional set of examples to investigate how the memory feature
compensates for weaknesses in change point detection algorithms, speciﬁcally
when algorithms are over-sensitive and create too many intervals.
Figure 11 shows examples of how memory improves the accuracy of congestion
inferences in diﬀerent circumstances. Figure 11a and 11b shows how memory
increases the accuracy of congestion inferences in the presence of over-partitioned
periods of elevated latency. While this feature increases the number of intervals
labeled as periods of congestion in the presence of multiple change points, it is
not able to ﬁx all of them. Figure 11c and 11d show how memory extends the
inferred period of congestion where there is a legitimate change point during this
period. These ﬁgures show a persistent increase in the minimum RTT baseline,
which we suspect was due to a route change during a period of congestion. We
assume that the lack of RTT measurements below that baseline corresponds to
speed-of-light constraints induced by the more circuitous path used during the
period of congestion.
5.7 Scenario 7: Adjusting JD Threshold to Minimize False Positives
Figure 12 shows examples of how one can adjust the JD threshold to minimize
false positives in congestion inferences. Figure 12a and 12b compare congestion
inferences using JD thresholds of 0.25 ms and 0.5 ms, respectively. In this exam-
ple, the jitter dispersion ranges from 0.26 to 92.64 ms, showing a ﬂat curve for
most of the period and a one-oﬀ event that generates a large spike. Due to the
ﬂatness of the curve we selected two thresholds close to the baseline jitter dis-
persion values (D+0.25 and D+0.5 ms), and inferred a period of congestion if
jitter dispersion exceeded these thresholds, which in this case means the jitter
dispersion doubled or tripled. We found that our ﬁrst threshold (0.25) was too
sensitive, since a small perturbation in jitter dispersion, in addition to a false
positive inference from the change point algorithm, generated a false positive
congestion inference.
170
E. Carisimo et al.
(a) Memoryless congestion inference
(b) memory congestion inference
(c) Memoryless congestion inference
(d) Memory congestion inference
Fig. 11. Examples of how the memory feature improves accuracy of congestion infer-
ences in the presence of over-partitioned intervals and other path anomalies. a) and b)
display how memory maximizes congestion inferences in scenarios where change point
detection algorithms overﬁt detection, breaking the time series into too many inter-
vals. c) and d) show another example of how memory can inform congestion inference
when a route change occurs within a period of congestion. Red-ﬁlled intervals indicate
periods of congestion. (Color ﬁgure online)
5.8 Scenario 8: False Negatives in Change Point Detection
One desired characteristic of a change point detection algorithm is the ability
to precisely detect the beginning and ending points (all of them) of all periods
elevated latency. In practice this is not possible for every time series, and in our
case the lack of change points hinder the accuracy of congestion inferences. We
use additional examples to investigate the accuracy of the change point detection
algorithms we included in Jitterbug.
Figure 13 shows two pairs of examples where the precision of Interval Detec-
tion varies depending on the algorithm being applied and the traﬃc scenario:
BCP is more precise that HMM (Fig. 13a and 13b) and HMM is more precise
than BCP (Fig. 13c and 13d)). Figure 13a shows a scenario where HMM misses
several consecutive change points, creating a prolonged period that does not
precisely capture the periods of congestion in that measurement. For the same
scenario, Fig. 13b shows that BCP correctly infers those periods of congestion.
Conversely, Fig. 13c shows a scenario in which HMM is more accurate than BCP
at detecting change points (Fig. 13d).
6 Comparative Evaluation of Jitterbug
The current version of Jitterbug allows users to infer congestion using 4 diﬀer-
ent conﬁgurations by changing: (i) the change point detection algorithm (BCP
or HMM, see Sect. 3.2), or (ii) the congestion inference method (KS-test or jit-
ter dispersion, see Sect. 3.3). In this section we compare Jitterbug inferences
Jitterbug: A New Framework for Jitter-Based Congestion Inference
171
(a) JD threshold = 0.25 ms
(b) JD threshold = 0.5ms
Fig. 12. Adjusting the JD threshold can mitigate false positive in congestion inferences.
a) shows that a too-sensitive threshold can yield errors even in the presence of a ﬂat
jitter dispersion time series. b) shows how small adjustments in this threshold can
mitigate false positive congestion inferences. Red-ﬁlled intervals indicate periods of
congestion. (Color ﬁgure online)
for each conﬁguration, ﬁrst comparing the KS-test and JD methods to each
other (Sect. 6.1), and cross-validated with the state-of-the-art congestion detec-
tion methods [12] (Sect. 6.2).
6.1 Comparing Inferences of KS-Test and JD Methods
Table 2 compares congestion inferences of KS-test and jitter dispersion methods
for the same interval using diﬀerent change point detection alternatives (BCP
on the left hand-side and HMM on the right hand-side). The results show no
signiﬁcant variations related to the change point detection used for the infer-
ences. KS-test and jitter dispersion indicate the same congestion status for most
intervals since the fraction of intervals equally labelled is 0.67 (128/192) and
0.64 (129/201) when using BCP and HMM, respectively. The jitter dispersion
method tends to label more intervals as period of congestion than the KS-test
method where the fraction of intervals considered as periods of congestion only by
jitter dispersion is 0.29 (56/192) and 0.32 (63/201) for BCP and HMM, respec-
tively. The KS-test method labels fewer intervals as period of congestion since
this method only detects a narrow type of congestion signature in which conges-
tion implies a change in jitter regime. For instance, when random components
of latency are more signiﬁcant than queueing delay, this noise limits the ability
of KS-test to detect a change in the jitter regime. In addition, we found that the
KS-test is unable to detect congestion generating changes of jitter regimes when
a bottleneck router buﬀer is small. We suspect that small buﬀers do not allow us
to observe jitter ﬂuctuations to classify them as a change of jitter regime. Active
traﬃc engineering strategies could keep jitter within a certain band. Despite that
the KS-test method eﬀectively infers congestion for a narrow type of congestion
signature, we have included this method for its simplicity to detect congestion
in cases with a large signal-to-noise ratio.
6.2 Comparing Inferences with Cross-Validation Data
We validate KS-test and jitter dispersion congestion inferences using CAIDA’s
autocorrelation-based congestion inferences as cross-validation data. In the pres-
172
E. Carisimo et al.
(a) Example A: HMM misses some change
points
(b) Example A: BCP detects all change
points
(c) Example B: HMM detects more change
points than BCP (Fig. 13d)
(d) Example B: BCP misses some change
points
Fig. 13. Two pairs of examples showing the limitations of change point detection algo-
rithms to detect all change points (vertical dashed lines). a) shows an example where
HMM is not able to capture some change points in contrast to BCP that detects all of
them (b). c) shows an example where HMM is a more accurate than BCP at detect-
ing change points (d) Red-ﬁlled intervals indicate periods of congestion. (Color ﬁgure
online)
ence of recurrent congestion, CAIDA’s congestion inferences count the number
of 15-min intervals with elevated latency. Using this schema, CAIDA’s conges-
tion inferences report the daily congestion severity of a link with a variable
that ranges from 0 to 964. We use Jitterbug outputs to generate the same daily
estimations.
Figure 14 shows how close are the daily congestion estimations of Jitterbug
and CAIDA’s congestion inference data. We also compared estimations with a
maximum diﬀerence of 10% (in number of congested 15-min intervals), and the
fraction of days that agree to within this 10% margin rises to 76–80% depend-
ing on the combination (80% for JD method using BCP). The most prominent
discrepancies in this evaluation corresponds to two categories: (i) Jitterbug false
positive inferences in periods with no congestion, and (ii) one-oﬀ congestion
events detected by Jitterbug but not present in CAIDA’s congestion inference
data since CAIDA’s method only attempts to infer recurrent (periodic) conges-
tion episodes.
4 One day has 96 periods of 15 min.
Jitterbug: A New Framework for Jitter-Based Congestion Inference
173
Table 2. Fraction (and total number) of (dis)agreements for diﬀerent methodologies.
The bar on top means a scenario with no congestion.
BCP
CKS
CKS
SUM CKS
CKS
SUM
HMM
CJD
CJD
0.43 (82)
0.29 (56)
0.04 (8)
0.47 (90) 0.39 (80)
0.24 (46) 0.53 (92) 0.31 (63)
0.04 (9)
0.24 (49) 0.57 (112)
0.43 (89)
SUM 0.72 (138) 0.28 (54) 192
0.70 (143) 0.28 (58) 201
(a) BCP
(b) HMM
Fig. 14. Cumulative distribution function of the diﬀerences between the estimated
daily time of congestion by autocorrelation-based methods and Jitterbug. These meth-
ods show remarkable similarity: 52% of days show no diﬀerence in inference regardless
of change point detection method or congestion-detection method (KS vs JD).
7 Lessons Learned
In this section we enumerate important aspects we have identiﬁed for jitter-based
congestion inference.
1. Jitter and jitter dispersion signatures provide meaningful informa-
tion to identify congestion events as periods of elevated latency.
We found that periods of congestion manifest in RTT latency measurements
not only as periods of elevated latency, but also changes in jitter (and jitter-
derivated signals) time series.
2. Jitter signals allowed us to discard periods of elevated latency gen-
erated by other path anomalies, e.g., route changes. Including jitter-
based analysis in the detection of congestion events allowed us to diﬀerenti-
ate congestion events from other path anomalies. In non-congestion-related
events, jitter and jitter dispersion time series tend not to change during peri-
ods of elevated latency.
174
E. Carisimo et al.
3. Period of elevated latency only to the far-side does not necessar-
ily mean congestion. We noticed that the simultaneous periods of elevated
latency to near- and far-sides suggest a route change in the internal network of
the ISP but a period of elevated latency to the far-side only does not necessar-
ily indicate congestion. Although in many cases a period of elevated latency
to the far-side only indicates a growth in the buﬀer occupancy of the inter-
domain link, this event could also suggest a route change only in the reverse
path from the far-side router. We use jitter and jitter dispersion to identify
traces with elevations only to the far-side router but not corresponding to
congestion events.
4. Shallower increments of RTT values when a link transitions to a