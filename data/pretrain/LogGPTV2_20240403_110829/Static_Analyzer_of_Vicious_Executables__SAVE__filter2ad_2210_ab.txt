### Measurement and Similarity Report

**Figure 6: Executables (SAVE) - Static Analyzer for Vicious Code**

1. **Decompression and Parsing:**
   - The Portable Executable (PE) code is optionally decompressed.
   - The code is then passed through a PE file parser, which generates an intermediate representation consisting of a Windows API calling sequence.
   - Each Windows API is mapped to a global 32-bit integer ID. The 16 most significant bits represent the Win32 module (dynamically linked library), and the last 16 bits represent a specific API within that module.
   - The API calling sequence is a series of these global ID numbers, representing the static calling sequence of the corresponding APIs.

2. **Similarity Measure:**
   - The generated API calling sequence is compared to a known malware sequence or signature from a signature database.
   - This comparison is performed using a similarity measure module, which generates a similarity report.
   - The detection decision is based on this similarity report.

### PE Binary Parser

The PE binary parser transforms the PE binary file into an API calling sequence. It uses two components:
- **W32Dasm Version 8.9:** A commercial disassembler by URSoftWare Co. that disassembles the PE code and outputs assembly instructions, imported modules, imported APIs, and resource information.
- **Text Parser:** This parses the output from W32Dasm to generate a static API calling sequence, which serves as our signature.

### 4.1 Similarity Measures

A signature is an API sequence of a known virus. Let \( V_s \) denote the vector of the signature, and \( V_u \) denote the vector of the unknown (suspicious) PE binary file. To determine if the new executable with signature \( V_u \) is an obfuscated version of the virus represented by \( V_s \), we measure the similarity between \( V_s \) and \( V_u \).

#### 4.1.1 Euclidean Distance

One of the most common measures is the Euclidean distance:

\[
D(V_s, V_u) = \sqrt{\sum_{i=1}^{n} (V_{s,i} - V_{u,i})^2}
\]

However, Euclidean distance may not be a good similarity measure in some situations. For example, consider the following vectors:

- \( V_1 = (1, 9, 1, 9, 1, 9, 1, 9, 1, 9, 1, 9) \)
- \( V_2 = (9, 1, 9, 1, 9, 1, 9, 1, 9, 1, 9, 1) \)
- \( V_3 = (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5) \)

Using Euclidean distance:
- \( D(V_1, V_2) = 27.72 \)
- \( D(V_1, V_3) = 13.86 \)

Most people would perceive \( V_1 \) and \( V_2 \) as having high similarity, while \( V_3 \) is quite different. Sequence alignment can help solve this problem [12].

#### 4.1.2 Sequence Alignment

Consider the sequences "WANDER" and "WADERS." The best alignment should be:

```
WANDER-
WA-DERS
```

The optimal alignment can be conceptualized using a matrix. Each position in the matrix corresponds to a position in the first and second sequences. Any alignment of the sequences corresponds to a path through the grid.

**Figure 7: Optimal Alignment Algorithm**

Using paths in the grid to represent alignments, the score of the best path up to each position is calculated. The score is the sum of the score for the element pair (0 for mismatches and 1 for matches) and the highest score in the grid above and to the left of the cell.

**Figure 8: Conceptualization of Optimal Alignment Algorithm**

After applying the algorithm, the two original sequences become "WANDER-" and "WA-DERS." In our case, API sequences \( V_s \) and \( V_u \) are inserted with some zeros to generate \( V_s' \) and \( V_u' \), which have optimal alignment.

The algorithm has a complexity of \( O(l_s \times l_u) \), where \( l_s \) and \( l_u \) are the lengths of sequences \( V_s \) and \( V_u \).

#### 4.1.3 Similarity Functions

We apply traditional similarity functions on \( V_s' \) and \( V_u' \). Popular measures include the cosine measure, extended Jaccard measure, and Pearson correlation measure.

- **Cosine Measure:**

\[
C(V_s', V_u') = \frac{V_s' \cdot V_u'}{\|V_s'\| \|V_u'\|}
\]

- **Extended Jaccard Measure:**

\[
J(V_s', V_u') = \frac{V_s' \cdot V_u'}{\|V_s'\|^2 + \|V_u'\|^2 - V_s' \cdot V_u'}
\]

- **Pearson Correlation Measure:**

\[
P(V_s', V_u') = \frac{\sum (V_{s,i}' - \bar{V_s'}) (V_{u,i}' - \bar{V_u'})}{\sqrt{\sum (V_{s,i}' - \bar{V_s'})^2 \sum (V_{u,i}' - \bar{V_u'})^2}}
\]

These measures are used because no single measure provides the best results for all sequences. Table 1 illustrates how these measures mutually correct each other.

### Similarity Report Generation

In the current version of SAVE, we calculate the mean value of the cosine, Jaccard, and Pearson measures between a virus signature and a suspicious binary file. For a particular file, let \( S_i \) denote the similarity value between virus signature \( i \) and the suspicious binary file. The similarity report is generated by calculating \( S_i \) for each virus signature in the database. The index of the largest entry in the similarity report indicates the most likely virus. By comparing this largest value with a threshold (e.g., 0.9), we make a decision on whether the binary file is malware and what type it is.

**Table 1: Mutual Correction Between Measures**

| Measure | Example 1 | Example 2 | Example 3 |
|---------|-----------|-----------|-----------|
| Cosine  | 0.9316    | 0.8631    | 0.8160    |
| Jaccard | 0.8160    | 0.8631    | 0.9316    |
| Pearson | 0.8631    | 0.9316    | 0.8160    |

**Table 2: Preliminary Results of MyDoom Worm and Other Malware Detection**

| Scanner/Proxy | MyDoom | Worm A | Worm B | Virus C |
|---------------|---------|--------|--------|---------|
| Scanner 1     | ✔       | ✖      | ?      | ✖       |
| Scanner 2     | ✔       | ?      | ✖      | ?       |
| ...           | ...     | ...    | ...    | ...     |

((✔) indicates detection, (✖) indicates failure to detect, and (?) indicates only an "alert"; all scanners used are the most updated versions.)

---

This optimized text is more structured, clear, and professional, making it easier to understand the process and methods involved in the similarity measurement and detection of malicious executables.