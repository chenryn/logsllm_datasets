Compiler
Native (GCC)
Native (Clang)
Successful
64
38
Failed
786
812
TABLE II: RIPE security benchmark results produced
by FEX. Columns 2 and 3 show the number of
successful and failed attacks respectively.
has almost 2× less successful attacks: Clang prevents
indirect attacks via buffers in BSS and Data segments due
to a smarter layout of objects in these segments.
V. RELATED WORK
Benchmark suites. Systems researchers developed a
plethora of benchmark suites, varying in their age,
targeting, and diversity. Dhrystone is a 30-year-old but
still wildly used set of synthetic integer benchmarks
[23]. Because of its age and code atypical for modern
programs, its substitute Coremark suite was developed
in 2009 [24]. However, both these suites are targeted
for embedded systems and have a limited diversity of
included programs. Recently, new benchmark suites
were released, covering more scenarios and stressing
particular parts of systems (ﬂoating-point operations,
instruction/data cache pressure, etc.) [2, 14, 25, 26]. For
example, MiBench is a comprehensive set of 35 embedded
applications targeting areas such as networking, security,
automotive, and telecommunications [27].
Aside from the high number of included programs
and their diversity, benchmark suites are characterized by
their targeting. LINPACK [28] is targeted for vectorizable
computations, MediaBench [29]—for media applications,
BioPerf [30]—for bioinformatics, MineBench [31]—for
data mining area, HPC Challenge [32] and NAS [33]—for
high-performance computing.
The focus of this work are benchmarks to analyze the
impact of static and dynamic instrumentation techniques.
Thus, benchmarks that test the whole hardware/software
stack (Phoronix [34]) or large-scale systems (YCSB [35]
and CloudSuite [8]) are not in the scope of FEX.
Orthogonally to our work, recent research efforts
concentrate on evaluation of diversity and redundancy of
benchmark suites. For example, several studies analyzed
the redundancy of SPEC and PARSEC benchmark suites,
i.e., what is the minimum number of programs and
inputs needed to obtain statistically signiﬁcant results
[36, 37]. Some papers compare different benchmark
suites, e.g., PARSEC and SPLASH [6, 38].
for
and
methodologies
Tools
performance
measurements. To our knowledge, FEX is the only
meta-framework to combine several benchmark suites.
However, other research tools exist that strive to provide
more stable, reproducible, and statistically signiﬁcant
results of performance evaluations.
It is widely known that measurement bias can perturb
evaluation and lead to incorrect performance results [5].
Additionally, abnormal behavior called workload ﬂurries
is frequently observed in real workloads (which are
later used as inputs in benchmarks) and can thus lead
549
to instable results [39]. One solution to this problem
is “shaking” the input workload to achieve a better
distribution of results [40]. We believe this can be
seamlessly integrated in FEX.
Stabilizer is a tool to achieve statistically sound
performance evaluation by re-randomizing the memory
layout—which turns out to be the leading factor of
measurement bias—to achieve normal distribution of
results [41]. Coz is another tool for better performance
measurements [42]. Its main focus is on highlighting
performance bottlenecks in complex software with the
help of causal proﬁling, by virtually speeding up separate
parts of code. Stabilizer was evaluated only on SPEC
CPU2006, and Coz—only on PARSEC. Both tools could
beneﬁt from FEX: they could be plugged into FEX for
quick evaluation on other benchmark suites.
Kalibera and Jones provide not a tool but a set of
guidelines to perform statistically sound experiments with
the minimum number of repetitions for each benchmark
[43]. The users are encouraged to follow these guidelines
when performing their experiments with FEX.
Another related category of measurement tools is
comprised of proﬁlers [44, 45] and tracers [46, 47]. These
tools are mainly used for measuring various runtime
parameters and analyzing performance bottlenecks.
Some of them (e.g., perf [44]) provide basic results
processing, such as calculation of mean values and
standard deviations over several runs. Yet, all of them are
single-application single-conﬁguration tools, i.e., they are
not capable of aggregating and comparing measurement
results of multiple benchmarks and/or different
build conﬁgurations. Moreover, control of experiment
procedure is out of their scope. Therefore, we consider Fex
to be orthogonal and complementary to this class of tools.
VI.CONCLUSION AND FUTURE WORK
FEX started as a small set of frequently reused
scripts and quickly matured in a full-ﬂedged evaluation
framework. During months of internal usage (e.g., in
our evaluation of Intel MPX [48] and SGXBounds [1]),
we constantly refactored and expanded the framework
to accommodate our growing needs, until the point we
decided it can be useful for others. The end result is
FEX, a software system evaluation framework that is
extensible, practical, and reproducible.
Currently, FEX has a number of limitations. The
framework provides no statistical analysis functionality
(except basic statistics such as standard deviation).
We plan to integrate statistical numpy/scipy Python
packages in the framework to allow for advanced
statistical methods and hypothesis testing.
system (e.g.,
We would like to combine FEX with a continuous
facilitate
integration
Evaluation-Driven Development
to Test-
Driven Development). Furthermore, we wish to support
a graphic user interface, since an ability to observe
intermediate results will simplify and shorten the process
of setting up and debugging experiments.
to
(similar
Jenkins)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
FEX supports only single-machine experiments. We
are investigating ways to build distributed experiments,
e.g., using the Fabric library.
Finally, since we rely on the Docker infrastructure,
we do not guarantee reproducability on levels below
user space, i.e., on different hardware setups or across
different kernel versions. However, FEX outputs various
environment details, so that the complete experimental
setup is stored in the log ﬁle.
FEX is available at https://github.com/tudinfse/fex.
REFERENCES
[1] D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia,
P. Felber, and C. Fetzer, “SGXBounds: Memory Safety for Shielded
Execution,” in EuroSys, 2017.
[2] J. L. Henning, “SPEC CPU2006 benchmark descriptions,” ACM
SIGARCH Computer Architecture News, 2006.
[3] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and
C. Kozyrakis, “Evaluating MapReduce for multi-core and
multiprocessor systems,” in HPCA, 2007.
[4] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC
benchmark suite: Characterization and architectural implications,”
in PACT, 2008.
[5] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney,
“Producing wrong data without doing anything obviously wrong!”
in ASPLOS, 2009.
[6] C. Bienia, S. Kumar, and K. Li, “PARSEC vs. SPLASH-2: A
Quantitative Comparison of Two Multithreaded Benchmark Suites
on Chip-Multiprocessors,” in WWC, 2008.
[7] V. Saxena, Y. Sabharwal, and P. Bhatotia, “Performance evaluation
and optimization of random memory access on multicores with
high productivity,” in HiPC, 2010.
[8] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee,
D. Jevdjic, C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsaﬁ,
“Clearing the clouds: a study of emerging scale-out workloads
on modern hardware,” in ASPLOS, 2012.
[9] C. Collberg and T. A. Proebsting, “Repeatability in computer
systems research,” Communications of the ACM, 2016.
[10] G. R. Brammer, R. W. Crosby, S. J. Matthews, and T. L. Williams,
“Paper mâché: Creating dynamic reproducible science,” Procedia
Computer Science, 2011.
[11] S. Perianayagam, G. R. Andrews, and J. H. Hartman, “Rex: A
toolset for reproducing software experiments,” in BIBM, 2010.
[12] D. Merkel, “Docker: Lightweight linux containers for consistent
development and deployment,” Linux Journal, 2014.
[13] C. Boettiger, “An introduction to docker for reproducible research,”
SIGOPS OS Review, 2015.
[14] C. Sakalis, C. Leonardsson, S. Kaxiras, and A. Ros, “Splash-3:
A properly synchronized benchmark suite for contemporary
research,” in ISPASS, 2016.
[15] “nginx: The Architecture of Open Source Applications,” http:
//www.aosabook.org/en/nginx.html, 2016, accessed: Oct, 2016.
[16] J. Wilander, N. Nikiforakis, Y. Younan, M. Kamkar, and W. Joosen,
“RIPE: Runtime intrusion prevention evaluator,” in ACSAC, 2011.
[17] W. McKinney, “pandas: a foundational python library for data
analysis and statistics,” Python for High Performance and Scientiﬁc
Computing, 2011.
[18] J. D. Hunter, “Matplotlib: A 2d graphics environment,” Computing
in Science Engineering, 2007.
[19] Jekyll, “Directory structure – Jekyll,” https://jekyllrb.com/docs/
structure/, accessed: Dec, 2016.
[20] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov,
“AddressSanitizer: A fast address sanity checker,” in ATC, 2012.
[21] “Apache HTTP server project,” http://httpd.apache.org/, 2016,
accessed: Oct, 2016.
[22] C. Lattner and V. Adve, “LLVM: A compilation framework for
lifelong program analysis and transformation,” in CGO, 2004.
[23] R. P. Weicker, “Dhrystone: A synthetic systems programming
benchmark,” Communications of ACM, 1984.
[24] “EEMBC – CoreMark
Processor Benchmark,”
//www.eembc.org/coremark/, 2016, accessed: Nov, 2016.
–
http:
[25] C. Bienia and K. Li, “PARSEC 2.0: A new benchmark suite for
chip-multiprocessors,” in MoBS, 2009.
[26] R. M. Yoo, A. Romano, and C. Kozyrakis, “Phoenix Rebirth:
Scalable MapReduce on a Large-scale Shared-memory System,”
in WWC, 2009.
[27] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge,
and R. B. Brown, “MiBench: A Free, Commercially Representative
Embedded Benchmark Suite,” in WWC, 2001.
[28] J. J. Dongarra, P. Luszczek, and A. Petitet, “The LINPACK
benchmark: Past, present, and future,” Concurrency and Computation:
Practice and Experience, 2003.
[29] C. Lee, M. Potkonjak, and W. H. Mangione-Smith, “MediaBench:
for Evaluating and Synthesizing Multimedia and
A Tool
Communicatons Systems,” in MICRO, 1997.
[30] D. Bader, Y. Li, T. Li, and V. Sachdeva, “BioPerf: A Benchmark
Suite to Evaluate High-Performance Computer Architecture on
Bioinformatics Applications,” in WWC, 2005.
[31] R. Narayanan, B. Ozisikyilmaz, J. Zambreno, G. Memik, and
A. Choudhary, “Minebench: A benchmark suite for data mining
workloads,” in WWC, 2006.
[32] P. R. Luszczek, D. H. Bailey, J. J. Dongarra, J. Kepner, R. F. Lucas,
R. Rabenseifner, and D. Takahashi, “The HPC Challenge (HPCC)
Benchmark Suite,” in SC, 2006.
[33] D. H. Bailey, E. Barszcz, J. T. Barton, D. S. Browning, R. L. Carter,
L. Dagum, R. A. Fatoohi, P. O. Frederickson, T. A. Lasinski, R. S.
Schreiber, H. D. Simon, V. Venkatakrishnan, and S. K. Weeratunga,
“The NAS Parallel Benchmarks—Summary and Preliminary
Results,” in SC, 1991.
[34] “Phoronix test suite,” http://www.phoronix-test-suite.com/, 2016,
accessed: Nov, 2016.
[35] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears,
“Benchmarking cloud serving systems with YCSB,” in SoCC, 2010.
[36] A. Phansalkar, A. Joshi, and L. K. John, “Analysis of redundancy
and application balance in the SPEC CPU2006 benchmark suite,”
in ISCA, 2007.
[37] C. Bienia and K. Li, “Fidelity and scaling of the PARSEC
benchmark inputs,” in WWC, 2010.
[38] N. Barrow-Williams, C. Fensch, and S. Moore, “A Communication
Characterization of SPLASH-2 and PARSEC,” in WWC, 2009.
[39] D. Tsafrir and D. G. Feitelson, “Instability in parallel job scheduling
simulation: The role of workload ﬂurries,” in IPDPS, 2006.
[40] D. Tsafrir, K. Ouaknine, and D. G. Feitelson, “Reducing
performance evaluation sensitivity and variability by input
shaking,” in MASCOTS, 2007.
[41] C. Curtsinger and E. D. Berger, “Stabilizer: Statistically sound
performance evaluation,” in ASPLOS, 2013.
[42] C. Curtsinger and E. Berger, “Coz: Finding code that counts with
causal proﬁling,” in SOSP, 2015.
[43] T. Kalibera and R. Jones, “Rigorous benchmarking in reasonable
time,” in ISMM, 2013.
[44] “perf:
Linux
proﬁling with
performance
counters,”
https://perf.wiki.kernel.org, 2017, accessed: Apr, 2017.
[45] J. Reinders, VTune performance analyzer essentials.
Intel Press, 2005.
[46] R. McDougall, J. Mauro, and B. Gregg, Solaris Performance and
Tools: DTrace and MDB Techniques for Solaris 10 and OpenSolaris.
Prentice Hall PTR, 2006.
[47] M. Desnoyers and M. R. Dagenais, “The LTTng tracer: A low
impact performance and behavior monitor for GNU/Linux,” in
OLS, 2006.
[48] O. Oleksenko, D. Kuvaiskii, P. Bhatotia, P. Felber, and C. Fetzer, “In-
tel MPX Explained: An Empirical Study of Intel MPX and Software-
based Bounds Checking Approaches,” arXiv:1702.00719, 2017.
550
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply.