Moore et al. provided another valuable data set collected
on Code Red worm during the whole day of July 19th [27].
Not like the data collected by Goldsmith and Eichman, which
were recounted at each hour, Moore et al. recorded the time
of the ﬁrst attempt of each infected host to spread the worm
to their networks. Thus the number of infected hosts in their
data is a non-decreasing function of time. The number of
infected hosts observed is shown in Fig. 2 as a function of
time t.
When rebooted, a Code Red infected computer went back
to susceptible state and could be reinfected again [4]. How-
ever, this would not aﬀect the number of infected hosts
shown in Fig. 2 — a reinfected host would use the same
source IP to scan, thus it would not be recounted into the
data collected by Moore et al..
Moore et al. considered patching and ﬁltering too when
they collected Code Red data [27]. The authors observed
that during the course of the day, many initially infected ma-
chines were patched, rebooted, or ﬁltered and consequently
ceased to probe the Internet. A host that was previously
Figure 2: Observed Code Red propagation — num-
ber of infected hosts (from Caida.org)
infected was considered by the authors to be deactivated af-
ter no further unsolicited traﬃc was observed from it. The
number of observed deactivated hosts over time is shown in
Fig. 3.
Since Code Red worm was programmed to stop spreading
after 00:00 UTC July 20th, the number of infected hosts
stopped increasing after 00:00 UTC. Otherwise the curve
in Fig. 2 would have kept increasing to some extent. The
abrupt rise in host inactivity in Fig. 3 at 00:00 UTC is
also due to the worm design of stopping infection at the
midnight.
We are interested in the following issues: How can we ex-
plain these Code Red worm propagation curves shown in
Fig. 1, 2, and Fig. 3? What factors aﬀect the spreading be-
havior of an Internet worm? Can we derive a more accurate
model for an Internet worm?
3. USING EPIDEMIC MODELS TO MODEL
CODE RED WORM PROPAGATION
Computer viruses and worms are similar to biological viruses
on their self-replicating and propagation behaviors. Thus
the mathematical techniques which have been developed for
the study of biological infectious diseases might be adapted
to the study of computer viruses and worms propagation.
140where k = βN . Using the same value k = 1.8 as what used
in [31], the dynamic curve of a(t) is plotted in Fig. 4.
Classical simple epidemic model
a(t)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Figure 3: Observed Code Red propagation — num-
ber of deactivated hosts (from Caida.org)
0
0
5
10
15
20
time:  t
25
30
35
40
In epidemiology area, both stochastic models and deter-
ministic models exist for modeling the spreading of infec-
tious diseases [1, 2, 3, 15]. Stochastic models are suitable
for small-scale system with simple virus dynamics; deter-
ministic models are suitable for large-scale system under the
assumption of mass action, relying on the law of large num-
ber [2]. When we model Internet worms propagation, we
consider a large-scale network with thousands to millions of
computers. Thus we will only consider and use determinis-
tic models in this paper. In this section, we introduce two
classical deterministic epidemic models, which are the bases
of our two-factor Internet worm model. We also point out
their problems when we try to use them to model Internet
worm propagation.
In epidemiology modeling, hosts that are vulnerable to
be infected by virus are called susceptible hosts; hosts that
have been infected and can infect others are called infectious
hosts; hosts that are immune or dead such that they can’t
be infected by virus are called removed hosts, no matter
whether they have been infected before or not. A host is
called an infected host at time t if it has been infected by
virus before t, no matter whether it is still infectious or is
removed [2] at time t. In this paper, we will use the same
terminology for computer worms modeling.
3.1 Classical simple epidemic model
In classical simple epidemic model, each host stays in one
of two states: susceptible or infectious. The model assumes
that once a host is infected by a virus, it will stay in infec-
tious state forever. Thus state transition of any host can
only be: susceptible → infectious [15]. The classical simple
epidemic model for a ﬁnite population is
dt = βJ(t)[N − J(t)],
dJ(t)
(1)
where J(t) is the number of infected hosts at time t; N is the
size of population; and β is the infection rate. At beginning,
t = 0, J(0) hosts are infectious and the other N − J(0) hosts
are all susceptible.
Let a(t) = J(t)/N be the fraction of the population that
is infectious at time t . Dividing both sides of (1) by N 2
yields the equation used in [31]:
dt = ka(t)[1 − a(t)],
da(t)
(2)
Figure 4: Classical simple epidemic model (k = 1.8)
Let S(t) = N − J(t) denote the number of susceptible
hosts at time t. Replace J(t) in (1) by N − S(t) and we get
dt = −βS(t)[N − S(t)].
dS(t)
(3)
Equation (1) is identical with (3) except for a minus sign.
Thus the curve in Fig. 4 will remain the same when we
rotate it 180 degrees around the (thalf , 0.5) point where
J(thalf ) = S(thalf ) = N/2. Fig. 4 and Eq. (2) show that
at the beginning when 1 − a(t) is roughly equal to 1, the
number of infectious hosts is nearly exponentially increased.
The propagation rate begins to decrease when about 80% of
all susceptible hosts have been infected.
Staniford et al.
[31] presented a Code Red propagation
model based on the data provided by Eichman [18] up to
21:00 UTC July 19th. The model captures the key behavior
of the ﬁrst half part of the Code Red dynamics. It is essen-
tially the classical simple epidemic model (1). We provide,
in this paper, a more detailed analysis that accounts for two
important factors involved in Code red spreading. Part of
our eﬀort is to explain the evolution of Code Red spreading
after the beginning phase of its propagation. Although the
classical epidemic model can match the beginning phase of
Code Red spreading, it can’t explain the later part of Code
Red propagation: during the last ﬁve hours from 20:00 to
00:00 UTC, the worm scans kept decreasing (Fig. 1).
From the simple epidemic model (Fig. 4), the authors in
[31] concluded that Code Red came to saturating around
19:00 UTC — almost all susceptible IIS servers online on
July 19th had been infected around that time. The numer-
ical solution of our model in Section 6, however, shows that
only about 60% of all susceptible IIS servers online have
been infected around 19:00 UTC on July 19th.
3.2 Classical general epidemic model: Kermack-
Mckendrick model
In epidemiology area, Kermack-Mckendrick model consid-
ers the removal process of infectious hosts [15]. It assumes
that during an epidemic of a contagious disease, some infec-
tious hosts either recover or die; once a host recovers from
the disease, it will be immune to the disease forever — the
hosts are in “removed” state after they recover or die from
the disease. Thus each host stays in one of three states at
any time: susceptible, infectious, removed. Any host in the
141system has either the state transition “susceptible → infec-
tious → removed” or stays in “susceptible” state forever.
Let I(t) denote the number of infectious hosts at time t.
We use R(t) to denote the number of removed hosts from
previously infectious hosts at time t. A removed host from
the infected population at time t is a host that is once in-
fected but has been disinfected or removed from circulation
before time t. Let J(t) denote the number of infected hosts
by time t, no matter whether they are still in infectious state
or have been removed. Then
J(t) = I(t) + R(t).
(4)
Based on the simple epidemic model (1), Kermack-Mckendrick
model is 
 dJ(t)/dt = βJ(t)[N − J(t)]
dR(t)/dt = γI(t)
J(t)
= I(t) + R(t) = N − S(t)
(5)
where β is the infection rate; γ is the rate of removal of
infectious hosts from circulation; S(t) is the number of sus-
ceptible hosts at time t; N is the size of population.
Deﬁne ρ ≡ γ/β to be the relative removal rate [3]. One
interesting result coming out of this model is
dI(t)
dt
> 0 if and only if S(t) > ρ.
(6)
Since there is no new susceptible host to be generated,
the number of susceptible hosts S(t) is a monotonically de-
creasing function of time t. If S(t0)  t0. In other words, if the
initial number of susceptible hosts is smaller than some crit-
ical value, S(0) < ρ, there will be no epidemic and outbreak
[15].
The Kermack-Mckendrick model improves the classical
simple epidemic model by considering that some infectious
hosts either recover or die after some time. However, this
model is still not suitable for modeling Internet worm propa-
gation. First, in the Internet, cleaning, patching, and ﬁlter-
ing countermeasures against worms will remove both suscep-
tible hosts and infectious hosts from circulation, but Kermack-
Mckendrick model only accounts for the removal of infec-
tious hosts. Second, this model assumes the infection rate
to be constant, which isn’t true for a rampantly spreading
Internet worm such as the Code Red worm.
We list in Table. 1 some frequently used notations in this
paper. The “removed” hosts are out of the circulation of a
worm — they can’t be infected anymore and they don’t try
to infect others.
4. A NEW INTERNET WORM MODEL: TWO-
FACTOR WORM MODEL
The propagation of a real worm on the Internet is a com-
plicated process. In this paper we will consider only contin-
uously activated worms. By this we mean that a worm on
an infectious host continuously tries to ﬁnd and infect other
susceptible hosts, as was the case of the Code Red worm
incident of July 19th.
In real world, since hackers write the codes of worms arbi-
trarily, worms usually don’t continuously spread forever, for
example, the Code Red worm stopped propagation at 00:00
UTC July 20th. Any worm models, including ours, can only
model the continuous propagation before that stopping time.
We can only predict such stopping event by manually ana-
lyzing the worm code.
In this paper, we consider worms that propagate without
the topology constraint, which was the case of Code Red.
Topology constraint means that an infectious host may not
be able to directly reach and infect an arbitrary suscepti-
ble host — it needs to infect several hosts on the route to
the target before it can reach the target. Most worms, such
as Code Red, belong to the worms without topology con-
straint. On the other hand, email viruses, such as Melissa
[6] and Love Bug [5], depend on the logical topology deﬁned
by users’ email address books to propagate. Their prop-
agations are topology dependent and need to be modelled
by considering the properties of the underlining topology,
which will not be discussed in this paper.
4.1 Two factors affecting Code Red worm prop-
agation
By studying reports and papers on the Code Red incident
of July 19th, we ﬁnd that the following two factors, which
are not considered in traditional epidemic models, aﬀected
Code Red worm propagation:
• Human countermeasures result in removing both sus-
ceptible and infectious computers from circulation —
during the course of Code Red propagation, an increas-
ing number of people became aware of the worm and
implemented some countermeasures: cleaning compro-
mised computers, patching or upgrading susceptible
computers, setting up ﬁlters to block the worm traf-
ﬁc on ﬁrewalls or edge routers, or even disconnecting
their computers from Internet.
• Decreased infection rate β(t), not a constant rate β —
the large-scale worm propagation have caused conges-
tion and troubles to some Internet routers [7, 8, 10,
33], thus slowed down the Code Red scanning process.
Human countermeasures, cleaning, patching, and ﬁlter-
ing, played an important role in defending against Code Red
worm. Microsoft reported that the IIS Index Server patch
was downloaded over one million times by August 1st, 2001
[14]. Code Red worm stopped propagation on 00:00 UTC
July 20th and was programmed to reactivate on August 1st.
But the scheduled recurrence of the worm on August 1st
2001 was substantially less damaging than its appearance
on July 19th because large number of machines had been
patched [9].
During the course of Code Red propagation on July 19th,
many initially infected machines were patched, rebooted, or
ﬁltered and consequently ceased to probe networks for sus-
ceptible hosts [27]. Moore et al. provided data on the num-
ber of deactivated hosts over time [27] (Fig. 3). A host that
was previously infected was considered to be deactivated af-
ter the authors of [27] observed no further unsolicited traﬃc
from it. Figure 3 shows that the number of deactivated hosts
kept increasing during the day and the number is not small:
Fig. 3 shows that among those 350000 infected computers
(Fig. 2), more than 150000 infected computers have already
been deactivated before Code Red worm ceased propagation
at 00:00 UTC July 20th.
The large-scale Code Red worm propagation on July 19th
could have caused congestion and troubles to some Internet
routers, thus slowed down the Code Red scanning process.
142Table 1: Notations in this paper
Notation Explanation
S(t)
I(t)
R(t)
Q(t)
N
J(t)
C(t)
β(t)
D(t)
Number of susceptible hosts at time t
Number of infectious hosts at time t
Number of removed hosts from the infectious population at time t
Number of removed hosts from the susceptible population at time t
Total number of hosts under consideration, N = I(t) + R(t) + Q(t) + S(t)
Number of infected hosts at time t, i.e., J(t) = I(t) + R(t)
Total number of removed hosts at time t, i.e., C(t) = R(t) + Q(t)
Infection rate at time t
Infection delay time in simulation, representing the time for a Code Red worm to ﬁnd an IIS server
As the Code Red rampantly swept the Internet on July 19th,
more and more computers were infected and then sent out
worm scan traﬃc continuously. Fig. 2 shows that at least
350, 000 computers were infected during that day. Consid-
ering that one infected computer had 99 threads continu-
ously scanning in parallel and there were so many infected
computers on July 19th, the worm propagation would have
generated huge number of small scanning packets. Although
the volume of these packets was relatively small compared
to the normal Internet traﬃc, the huge quantity of these
packets could have caused congestion and troubles to some
routers, especially edge routers with limited resources [7, 8,
10, 33].
Because Code Red worm generates random IP addresses
to scan, many of these IP addresses, for example, some
broadcast IP addresses or unknown addresses, will not be
seen or be rarely seen by edge routers when these routers
work under normal conditions. Thus during Code Red ram-
pant spreading on July 19th, the huge quantity of packets
with abnormal destination IP addresses would have caused
congestion and troubles to some edge routers [8]. According
to one major router vendor [7, 8], the large number of Code
Red scans sent to random IP addresses caused some edge
routers to ﬁll up their ARP caches, exhaust their memories
and restart. The high traﬃc load also triggered the defects
in some routers [7], and caused some low-end routers to re-
boot.