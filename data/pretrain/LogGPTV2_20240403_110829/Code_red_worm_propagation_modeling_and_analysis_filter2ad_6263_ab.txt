### Moore et al. Data Set on Code Red Worm

Moore et al. provided a valuable data set collected during the entire day of July 19th, which captured the propagation of the Code Red worm [27]. Unlike the data collected by Goldsmith and Eichman, which were recorded hourly, Moore et al. documented the first attempt of each infected host to spread the worm to their networks. Consequently, the number of infected hosts in their data is a non-decreasing function of time. The observed number of infected hosts over time is illustrated in Figure 2.

### Reboot and Reinfection Dynamics

When a Code Red-infected computer was rebooted, it returned to a susceptible state and could be reinfected [4]. However, this did not affect the number of infected hosts shown in Figure 2, as a reinfected host would use the same source IP address to scan, thus not being recounted in the data collected by Moore et al.

### Patching and Filtering Considerations

Moore et al. also accounted for patching and filtering measures when collecting their data [27]. They observed that many initially infected machines were patched, rebooted, or filtered, thereby ceasing to probe the Internet. A previously infected host was considered deactivated if no further unsolicited traffic was detected from it. The number of observed deactivated hosts over time is shown in Figure 3.

### Worm Propagation Cessation

The Code Red worm was programmed to stop spreading after 00:00 UTC on July 20th. Therefore, the number of infected hosts stopped increasing after this time. The abrupt rise in host inactivity at 00:00 UTC in Figure 3 is due to the worm's design to halt infection at midnight.

### Research Questions

We are interested in the following issues:
- How can we explain the Code Red worm propagation curves shown in Figures 1, 2, and 3?
- What factors affect the spreading behavior of an Internet worm?
- Can we derive a more accurate model for an Internet worm?

## Using Epidemic Models to Model Code Red Worm Propagation

Computer viruses and worms share similarities with biological viruses in terms of self-replication and propagation. Thus, mathematical techniques developed for studying biological infectious diseases can be adapted to model computer virus and worm propagation.

### Classical Simple Epidemic Model

In classical simple epidemic models, each host is in one of two states: susceptible or infectious. The model assumes that once a host is infected, it remains infectious indefinitely. The state transition is: susceptible â†’ infectious [15].

The classical simple epidemic model for a finite population is given by:
\[ \frac{dJ(t)}{dt} = \beta J(t) [N - J(t)] \]
where \( J(t) \) is the number of infected hosts at time \( t \), \( N \) is the population size, and \( \beta \) is the infection rate. Initially, at \( t = 0 \), \( J(0) \) hosts are infectious, and the remaining \( N - J(0) \) hosts are susceptible.

Let \( a(t) = \frac{J(t)}{N} \) be the fraction of the population that is infectious at time \( t \). Dividing both sides of the equation by \( N^2 \) yields:
\[ \frac{da(t)}{dt} = k a(t) [1 - a(t)] \]
where \( k = \beta N \). Using \( k = 1.8 \) as in [31], the dynamic curve of \( a(t) \) is plotted in Figure 4.

### Classical General Epidemic Model: Kermack-McKendrick Model

The Kermack-McKendrick model considers the removal process of infectious hosts [15]. It assumes that some infectious hosts either recover or die, becoming immune to the disease. Each host is in one of three states: susceptible, infectious, or removed.

Let \( I(t) \) denote the number of infectious hosts at time \( t \), and \( R(t) \) denote the number of removed hosts from previously infectious hosts at time \( t \). Then:
\[ J(t) = I(t) + R(t) \]

The Kermack-McKendrick model is:
\[ \frac{dJ(t)}{dt} = \beta J(t) [N - J(t)] \]
\[ \frac{dR(t)}{dt} = \gamma I(t) \]
\[ J(t) = I(t) + R(t) = N - S(t) \]
where \( \beta \) is the infection rate, \( \gamma \) is the rate of removal of infectious hosts, and \( S(t) \) is the number of susceptible hosts at time \( t \).

Define \( \rho \equiv \frac{\gamma}{\beta} \) as the relative removal rate [3]. An interesting result from this model is:
\[ \frac{dI(t)}{dt} > 0 \text{ if and only if } S(t) > \rho \]

Since there are no new susceptible hosts generated, \( S(t) \) is a monotonically decreasing function of time \( t \). If the initial number of susceptible hosts is smaller than some critical value, \( S(0) < \rho \), there will be no epidemic outbreak [15].

### Limitations of the Kermack-McKendrick Model

The Kermack-McKendrick model improves the classical simple epidemic model by considering the removal of infectious hosts. However, it is not suitable for modeling Internet worm propagation because:
1. Cleaning, patching, and filtering countermeasures remove both susceptible and infectious hosts from circulation, but the model only accounts for the removal of infectious hosts.
2. The model assumes a constant infection rate, which is not true for rapidly spreading Internet worms like Code Red.

### Notations Used in This Paper

| Notation | Explanation |
|----------|-------------|
| \( S(t) \) | Number of susceptible hosts at time \( t \) |
| \( I(t) \) | Number of infectious hosts at time \( t \) |
| \( R(t) \) | Number of removed hosts from the infectious population at time \( t \) |
| \( Q(t) \) | Number of removed hosts from the susceptible population at time \( t \) |
| \( N \) | Total number of hosts under consideration, \( N = I(t) + R(t) + Q(t) + S(t) \) |
| \( J(t) \) | Number of infected hosts at time \( t \), i.e., \( J(t) = I(t) + R(t) \) |
| \( C(t) \) | Total number of removed hosts at time \( t \), i.e., \( C(t) = R(t) + Q(t) \) |
| \( \beta(t) \) | Infection rate at time \( t \) |
| \( D(t) \) | Infection delay time in simulation, representing the time for a Code Red worm to find an IIS server |

## A New Internet Worm Model: Two-Factor Worm Model

The propagation of real-world worms on the Internet is a complex process. We consider continuously activated worms, such as the Code Red worm, which continuously try to find and infect other susceptible hosts.

### Factors Affecting Code Red Worm Propagation

By analyzing reports and papers on the Code Red incident of July 19th, we identified two factors affecting its propagation that are not considered in traditional epidemic models:
- **Human Countermeasures:** These actions result in the removal of both susceptible and infectious computers from circulation. As awareness of the worm increased, people implemented countermeasures such as cleaning compromised computers, patching or upgrading susceptible computers, setting up filters, or disconnecting computers from the Internet.
- **Decreased Infection Rate:** The large-scale propagation caused congestion and troubles to some Internet routers, slowing down the Code Red scanning process.

### Human Countermeasures and Their Impact

Human countermeasures, including cleaning, patching, and filtering, played a crucial role in defending against the Code Red worm. Microsoft reported that the IIS Index Server patch was downloaded over one million times by August 1st, 2001 [14]. Although the worm was programmed to reactivate on August 1st, the recurrence was less damaging due to the large number of machines that had been patched [9].

During the propagation on July 19th, many initially infected machines were patched, rebooted, or filtered, ceasing to probe networks for susceptible hosts [27]. Figure 3 shows the number of deactivated hosts over time, indicating that more than 150,000 out of 350,000 infected computers were deactivated before the worm ceased propagation at 00:00 UTC on July 20th.

### Network Congestion and Router Troubles

The large-scale propagation of the Code Red worm on July 19th likely caused congestion and troubles to some Internet routers, particularly edge routers with limited resources. The worm's random IP address generation led to a high volume of small scanning packets, causing congestion and filling up ARP caches, exhausting memories, and triggering defects in some routers [7, 8, 10, 33].

This comprehensive analysis provides a foundation for developing a more accurate model for Internet worm propagation, taking into account the unique factors that influence their behavior.