page fault
2.5.15
2.5.15-lsm
73
8.545
142
25
4874
0.974
4
73
8.811
143
27
4853
0.990
5
% Overhead
with LSM
0%
3.1%
0.7%
8%
-0.4%
1.6%
25%
Local communication bandwidth in MB/s,
larger is better:
Test Type
pipe
AF Unix
TCP
ﬁle reread
mmap reread
bcopy (libc)
bcopy (hand)
mem read
mem write
2.5.15
537
98
257
306
368
191
148
368
197
2.5.15-lsm
542
116
235
306
368
191
151
368
197
% Overhead
with LSM
-0.9%
-18.4%
8.6%
0%
0%
0%
-2%
0%
0%
Table 2: LMBench Microbenchmarks, 4 processor machine
5.2.1 Microbenchmark: LMBench
We used LMBench [31] for microbenchmarking. LM-
Bench was developed speciﬁcally to measure the perfor-
mance of core kernel system calls and facilities, such as
ﬁle access, context switching, and memory access. LM-
Bench has been particularly effective at establishing and
maintaining excellent performance in these core facili-
ties in the Linux kernel.
Process tests, times in µseconds, smaller is better:
Test Type
null call
null I/O
stat
open/close
select TCP
sig inst
sig handl
fork proc
exec proc
sh proc
2.5.15
0.44
0.67
29
30
23
1.14
5.23
182
745
4334
2.5.15-lsm
0.44
0.71
29
30
23
1.15
5.24
182
747
4333
% Overhead
with LSM
0%
6%
0%
0.5%
0%
0.9%
0.2%
0%
0.3%
0%
File and VM system latencies in µseconds,
smaller is better:
Test Type
0K ﬁle create
0K ﬁle delete
10K ﬁle create
10K ﬁle delete
mmap latency
prot fault
page fault
2.5.15
2.5.15-lsm
96
31
157
45
3246
0.899
3
96
31
158
46
3158
1.007
3
% Overhead
with LSM
0%
0%
0.6%
2.2%
-2.7%
12%
0%
Local communication bandwidth in MB/s,
larger is better:
Test Type
pipe
AF Unix
TCP
ﬁle reread
mmap reread
bcopy (libc)
bcopy (hand)
mem read
mem write
2.5.15
630
125
222
316
378
199
168
378
206
2.5.15-lsm
597
125
220
313
368
191
149
396
197
% Overhead
with LSM
5.2%
0%
0.9%
0.9%
2.6%
4%
11.3%
2.6%
4.4%
Table 3: LMBench Microbenchmarks, 1 processor machine
Machine Type
4 CPUs
1 CPU
2.5.15
2.5.15-lsm
92
341
92
342
% Overhead
with LSM
0%
0.3%
Table 4: Linux Kernel Build Macrobenchmarks, time in seconds
have not identiﬁed the testing problem.
The worst case overhead was 5.1% for select(),
2.7% for open/close, and 3.1% for ﬁle delete. The
open, close, and delete results are to be expected
because the kernel repeatedly checks permission for
each element of a ﬁlename during pathname resolution,
magnifying the overhead of these LSM hooks. The per-
formance penalty for select() stands out as an op-
portunity for optimization, which is conﬁrmed by mac-
robenchmark experiments in Section 5.2.3.
Similar results for running the same machine with a UP
kernel are shown in Table 3. One should also bear in
mind that these are microbenchmark ﬁgures; for com-
prehensive application-level impact, see Sections 5.2.2
and 5.2.3.
5.2.2 Macrobenchmark: Kernel Compilation
Our ﬁrst macrobenchmark is the widely used kernel
compilation benchmark, measuring the time to build the
Linux kernel. We ran this test on a 4-processor SMP
machine (four 700 MHz Xeon processors, 1 GB RAM,
ultra wide SCSI disk) using both a SMP and UP kernel.
The single processor test executed the command time
make -j2 bzImage and the 4-processor test ex-
ecuted the command time make -j8 bzImage,
with the results shown in Table 4. The result is basi-
cally zero overhead for the LSM patch, the worst case
being 0.3%.
We compared a standard Linux 2.5.15 kernel against a
2.5.15 kernel with the LSM patch applied and the de-
fault capabilities module loaded, run on a 4-processor
700 MHz Pentium Xeon computer with 1 GB of RAM
and an ultra-wide SCSI disk, with the results shown in
Table 2. In most cases, the performance penalty is in the
experimental noise range. In some cases, the LSM ker-
nel’s performance actually exceeded the standard kernel,
which we attribute to experimental error (typically cache
collision anomalies [24]). The 18% performance im-
provement for AF Unix in Table 2 is anomalous, but we
5.2.3 Macrobenchmarks: Webstone
Using Webstone [33] we benchmarked the overhead im-
posed on a typical server application — a webserver.
We collected data showing the overhead of both a ba-
sic LSM kernel and an LSM kernel with the SELinux
module loaded. The SELinux module uses the Netﬁlter
based hooks, so all three kernels have Netﬁlter support
compiled in, and are based on the 2.5.7 Linux kernel.
Connection rate measured in connections per second.
Connection rate measured in connections per second.
Server
Server
Number
connection
connection
Server
Server
Number
connection
connection
of
clients
8
16
24
32
rate
2.5.7
916.56
917.64
917.44
918.91
rate
2.5.7-lsm
870.98
869.79
872.28
876.17
%
Overhead
4.97%
5.21%
4.92%
4.65%
of
clients
8
16
24
32
rate
2.5.7
916.56
917.64
917.44
918.91
rate
2.5.7-SEL
766.58
766.48
765.56
764.80
%
Overhead
16.4%
15.5%
16.6%
16.8%
Table 5: UP Webstone results comparing LSM to standard kernel.
Table 7: UP Webstone results comparing SELinux to standard ker-
nel.
Connection rate measured in connections per second.
Server
Server
Number
connection
connection
of
clients
8
16
24
32
rate
2.5.7
1206.05
1206.74
1214.54
1207.30
rate
2.5.7-lsm
1115.29
1117.61
1130.13
1125.89
%
Overhead
7.53%
7.39%
6.95%
6.74%
Table 6: SMP Webstone results comparing LSM to standard kernel.
The standard kernel was compiled with Netﬁlter sup-
port. The LSM kernel was compiled with support for
the Netﬁlter based hooks and used the default superuser
logic. The SELinux kernel was compiled with support
for SELinux and the Netﬁlter based hooks. The SELinux
module was also stacked with the capabilities module, a
typical SELinux conﬁguration. We ran these tests on a
dual 550MHz Celeron with 384MB RAM. The NIC was
a Gigabit Netgear GA302T on a 32-bit 33MHz PCI bus.
The webserver was Apache 1.3.22-0.6 (Red Hat 6.2 up-
date).
Netﬁlter is a critical issue here. The 5–7% overhead
observed in the LSM benchmarks in Tables 5 and 6 is
greater than we would like. A separate experiment con-
ﬁgured with LSM and Netﬁlter but without the Netﬁl-
ter LSM hooks showed the more desirable 1–2% perfor-
mance overhead. This is consistent with the worst case
5% overhead in TCP select observed in Section 5.2.1,
and identiﬁes the Netﬁlter LSM hooks as critical for op-
timization.
The UP benchmark data in Table 7 shows that SELinux
imposes about 16% overhead on connection rate, and we
found similar overhead in throughput. The SMP bench-
mark data in Table 8 shows about 21% overhead on con-
nection rate, and we found similar overhead in through-
put. The greater overhead for the SMP test is likely due
Connection rate measured in connections per second.
Server
Server
Number
connection
connection
of
clients
8
16
24
32
rate
2.5.7
1206.05
1206.74
1214.54
1207.30
rate
2.5.7-SEL
949.56
949.74
952.28
956.76
%
Overhead
21.3%
21.3%
21.6%
20.1%
Table 8: SMP Webstone results comparing SELinux to standard ker-
nel.
to locking issues. Note that these overhead rates are spe-
ciﬁc to the SELinux module (a particularly popular mod-
ule) and that performance costs for other modules will
vary.