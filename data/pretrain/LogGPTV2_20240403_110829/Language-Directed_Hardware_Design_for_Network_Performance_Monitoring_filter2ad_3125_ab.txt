filter(R, pred) where R is some stream containing performance
metadata (e.g., pktstream), and the filter predicate pred may in-
volve packet headers, performance metadata, or both. The result of
a filter is another stream that contains only tuples satisfying the
predicate.
Computing stateless functions over packets. Marple lets users
compute functions of the fields available in the incoming stream, to
express new quantities of interest. A simple example is rounding
packet timestamps to an ‘epoch’:
result = map(pktstream, [tin/epoch_size], [epoch]);
The map operator evaluates the expression tin/epoch_size, written
over the fields available in the tuple stream, and produces a new field
epoch. The general form of this construct is map(R, [expression],
[field]) where a list of expressions over fields in the input stream
R creates a list of new fields in the map output stream.
Aggregating statefully over multiple packets. Marple allows
aggregating statistics over multiple tuples at user-specified granular-
ities. For example, the following query counts packets belonging to
each transport-level flow (i.e., 5-tuple):
result = groupby(pktstream, [5tuple], count)
Here, the groupby partitions the incoming pktstream into sub-
streams based on the transport 5-tuple, and then applies the ag-
gregation function count to count the number of tuples in each
substream. Marple allows users to write flexible order-dependent ag-
gregation functions over the tuples of each substream. For example,
a user can track latency spikes for each connection by maintaining
an exponentially weighted moving average (EWMA) of queueing
latencies:
result = groupby(pktstream, [5tuple, switch], ewma);
def ewma([avg], [tin, tout]):
avg = ((1-alpha)*avg) + (alpha*(tout-tin));
Here the aggregation function ewma evolves an EWMA avg using
the current value of avg and incoming packet timestamps. Unlike the
previous count example, the EWMA aggregation function depends
on the order of packets being processed.
groupbys take the general form groupby(R, [aggFields],
fun), where the aggregation function fun operates over tuples shar-
ing attributes in a list aggFields of headers and performance meta-
data. This construct is inspired by folds in functional program-
ming [45]. Such order-dependent folds are challenging to express
in existing query languages. For instance, SQL only allows order-
independent commutative aggregations, whether built-in (e.g., count,
average, sum) or user-defined.
The aggregation function fun is written in an imperative form,
with two arguments: a list of state variables and a list of relevant
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Narayana et al.
incoming tuple fields. Each statement in fun can be an assignment
to an expression (x = ...), a branching statement (if pred {...}
else {...}), or a special emit() statement that controls the output
stream of the groupby. Below, we show an example of an aggrega-
tion that detects a new connection:
result = groupby(pktstream, [5tuple], new_flow);
def new_flow([fcount], []):
if fcount == 0:
fcount = 1
emit()
The output of a groupby is a stream containing the aggregation fields
(e.g., 5-tuple) and the aggregated values (e.g., fcount). The output
stream contains only tuples for which the emit() statement is en-
countered during execution of the aggregation function. For example,
the output stream of new_flow consists of the first packet of every
new transport-level connection. If the function has no emit()s, the
user can still read the aggregated fields and their current aggregated
state values as a table.
Chaining together multiple queries. Because all Marple con-
structs produce and consume streams, Marple allows users to write
queries that take in the results of previous queries as inputs. A stream
of tuples flows from one query to the next, and each query may add
or filter out information from the incoming tuple, or even drop the
tuple entirely. For example, the program below tracks the size dis-
tribution of flowlets, i.e., bursts of packets from the same 5-tuple
separated by more than a fixed time amount delta.
fl_track = groupby(pktstream, [5tuple], fl_detect);
def fl_detect([last_time, size], [tin]):
if (tin - last_time > delta):
emit()
size = 1
else:
size = size + 1
last_time = tin
The function fl_detect detects new flowlets using the last time a
packet from the same flow was seen. Because of the emit() state-
ment’s location, the flowlet size from fl_track is only streamed out
to other operators upon seeing the first packet of a new flowlet.
fl_bkts = map(fl_track, [size/16], [bucket]);
fl_hist = groupby(fl_bkts, [bucket], count);
The map fl_bkts bins the flowlet size emitted by fl_track into a
bucket index, which is used to count the number of flowlets in the
corresponding bucket in fl_hist.
Joining results across queries. Marple provides a zip operator
that “joins” the results of two queries to check whether two condi-
tions hold simultaneously. Consider the example of detecting the
fan-in of packets from many connections into a single queue, charac-
teristic of TCP incast [61]. This can be checked by combining two
distinct conditions: (1) the number of active flows in a queue over a
short interval of time is high, and (2) the queue occupancy is large.
A user can first compute the number of active flows over the
current epoch using two aggregations:
R1 = map(pktstream, [tin/epoch_size], [epoch]);
R2 = groupby(R1, [5tuple, epoch], new_flow);
R3 = groupby(R2, [epoch], count);
The number of active flows in this epoch can be combined with the
queue occupancy information in the original packet stream through
the zip operator:
R4 = zip(R3, pktstream);
result = filter(R4, qsize > 100 and count > 25);
The result of a zip operation over two input streams is a single
stream containing tuples that are a concatenation of all the fields in
the two streams, whenever both input streams contain valid tuples
processed from the same original packet tuple. A zip is a special
kind of stream join where the result can be computed without having
to synchronize the two streams, because tuples of both streams
originate from pktstream. The result of the zip can be processed
like any other stream: the filter in the result query checks the
two incast conditions above.
We did not find a need for more general joins akin to joins in
streaming query languages like CQL [30]. Streaming joins have
semantics that can be quite complex and may produce large results,
i.e., O(#pkts2). Hence, Marple restricts users to simple zip joins.
We show several examples of Marple queries in Fig. 7. For in-
stance, Marple can express measurements of simple counters, TCP
reordering of various forms, high-loss connections, flows with high
end-to-end network latencies, and TCP fan-in.
Restrictions on Marple queries. Some aggregations are chal-
lenging to implement over a network-wide stream. For example,
consider an EWMA over some packet field across all packets seen
anywhere in the entire network, while processing packets in the order
of their tout values. Even with clock synchronization, this aggrega-
tion is hard to implement because it requires us to either coordinate
between switches or stream all packets to a central location.
Marple’s compiler rejects queries with aggregations that need to
process multiple packets at multiple switches in order of their tout
values. Concretely, we only allow aggregations that relax one of
these three conditions, and thus either
(1) operate independently on each switch, in which case we natu-
rally partition queries by switch (e.g., a per-flow EWMA of
queueing latencies on a particular switch), or
(2) operate independently on each packet, in which case we have
the packet perform the coordination by carrying the aggre-
gated state to the next switch on its path (e.g., a rolling average
link utilization seen by the packet along its path), or
(3) are associative and commutative, in which case independent
switch-local results can be combined in any order to produce
a correct overall result for the network [15], e.g., a count of
how many times packets from a flow appeared throughout the
network. In this case, we rely on the programmer to annotate
the aggregation function with the assoc and comm keywords.
3 SCALABLE AGGREGATION AT LINE RATE
How should switches implement Marple’s language constructs? We
require instructions on switches that can aggregate packets into
per-flow state (groupby), transform packet fields (map), stream only
packets matching a predicate (filter), or merge packets that satisfy
two previous queries (zip).
Language-Directed Hardware Design
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
evicted. When it is evicted, we merge the flow’s value just before
eviction with its value in the backing-store using a merge function,
and write the merged result to the backing store. In our design,
the switch only writes to the backing store but never reads from it,
which helps avoid non-deterministic latencies. The backing store
may be stale relative to the on-chip cache if there have been no
recent evictions. We remedy this by forcing periodic evictions.
To merge a flow’s new aggregated value in the switch cache
with its old value in the backing store correctly, the cache needs
to maintain and send auxiliary state to the backing store. A naïve
usage of auxiliary state is to store relevant fields from every packet
of a flow, so that the backing store can simply run the aggregation
function over the entire packet stream when merging. However, in
a practical implementation, the auxiliary state should be bounded
in size and not grow with the number of packets in the flow. Over
the next four subsections, we describe two classes of queries that are
mergeable with a small amount of auxiliary state (§3.1 and §3.2),
discuss queries that are not mergeable (§3.4), and provide a general
condition for mergeability that unifies the two classes of mergeable
queries and separates them from non-mergeable queries (§3.5).
3.1 The associative condition
A simple class of mergeable aggregations is associative functions.
Suppose the aggregation function on state s is s = op(s, f ), where
op is an associative operation and f is a packet field. Then, if op has
an identity element I and a flow’s default value on insertion is s0 = I,
it is easy to show that this function can be merged using the function
op(sbacking,scache), where sbacking and scache are the value in the
backing store and the value just evicted from the cache, respectively.
The associative condition allows us to merge aggregation functions
like addition, max, min, set union, and set intersection.
3.2 The linear-in-state condition
Consider the EWMA aggregation function, which maintains a mov-
ing average of queueing latencies across all packets within a flow.
The aggregation function updates the EWMA s as follows:
s = (1− α ) · s + α · (tout −tin)
We initialize s to s0. Suppose a flow F is evicted from the on-chip
cache for the first time and written to the backing store with an
EWMA of sbacking.3 The first packet from F after F’s eviction is
processed like a packet from a new flow in the on-chip cache, starting
with the state s0. Assume that N packets from F then hit the on-chip
cache, resulting in the EWMA going from s0 to scache. Then, the
correct EWMA scorrect (i.e., for all packets seen up to this point)
satisfies:
scorrect − (1− α )Nsbacking = scache − (1− α )Ns0
scorrect = scache + (1− α )N (sbacking − s0)
So, the correct EWMA can be obtained by: (1) having the on-chip
cache store (1− α )N as auxiliary state for each flow after each up-
date, and (2) adding (1− α )N (sbacking − s0) to scache when merging
scache with sbacking.
We can generalize this example. Let p be a vector with the headers
and performance metadata from the last k packets of a flow, where k
3When a flow is first evicted, it does not need to be merged.
Figure 3: Marple’s key-value store vs. a traditional cache
Of the four language constructs, map, filter, and zip, are state-
less: they operate on packet fields alone and do not modify switch
state. Such stateless manipulations are already supported on emerg-
ing programmable switches that support programmable packet
header processing [3, 13, 25, 33]. On the other hand, the groupby
construct needs to maintain and update state on switches.
Stateful manipulation on a switch for a groupby is challenging
for two reasons. First, the time budget to update state before the
next packet arrives can be as low as a nanosecond on high-end
switches [56]. Second, the switch needs to maintain state propor-
tional to the number of aggregated records (e.g., per flow), which
may grow unbounded with time. We address both challenges using a
programmable key-value store in hardware, where the keys represent
aggregation fields and values represent the state being updated by
the aggregation function. Our key-value store has a ‘split’ design:
a small and fast on-chip key-value store on the switch processes
packets at line rate, while a large and slow off-chip backing store
allows us to scale to a large number of flows.
High-speed switch ASICs typically feature an ingress and egress
pipeline shared across multiple ports, running at a 1 GHz clock rate
to support up to a billion 64-byte packets per second of aggregate
capacity [33]. To handle state updates from packets arriving at 1
GHz, the on-chip key-value store must be in SRAM on the switch
ASIC. However, the SRAM available for monitoring on an ASIC
(§5.2) is restricted to tens of Mbits (about 10K–100K flows).
To scale to a larger number of flows, the on-chip key-value store
serves as a cache for the larger off-chip backing store. In traditional
cache designs, cache misses require accessing off-chip DRAM with
non-deterministic latencies [37] to read off the stored state. Because
the aggregation operation requires us to read the value in order to
update it, the entire state update operation incurs non-deterministic
latencies in the process. This results in stalls in the switch pipeline.
Unfortunately, pipeline stalls affect the ability to provide guarantees
on line-rate packet processing (10–100 Gbit/s) on all ports.
We design our key-value store to process packets at line rate even
on cache misses (Fig. 3). Instead of stalling the pipeline waiting
for a result from DRAM, we treat the incoming packet as the first
packet from a new flow and initialize the flow’s state to an initial
value. Subsequent packets from the same flow are aggregated within
the newly created flow entry in the key-value store, until the flow is
KeyValueKeyValuekeyMergeevictedkeyKeyValueKeyValueBacking store (DRAM)WriteevictedkeyCache (SRAM)Cache (SRAM)Backing store (DRAM)HitMissUpdateInitializekeyHitMissUpdateInitialize/UpdateReadTraditional cacheMarple’sdesignSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Narayana et al.
is an integer determined at query compile time (§4.3). We can merge
any aggregation function with state updates of the form S = A(p) ·
S + B(p), where S is the state, and A(p) and B(p) are functions of
the last k packets. We call this condition the linear-in-state condition
and say that A(p) and B(p) are functions of bounded packet history.
The requirement of bounded packet history is important. Consider
the TCP non-monotonic query from Fig. 7, which counts the num-
ber of packets with sequence numbers smaller than the maximum
sequence number seen so far. The aggregation can be expressed as
count = count + (maxseq > tcpseq) ? 1 : 0
While the update superficially resembles A(p) · S + B(p), the coeffi-
cient B(p) is a function of maxseq, the maximum sequence number
so far, which could be arbitrarily far back in the stream. Intuitively,
since B(p) is not a function of bounded packet history, the auxiliary
state required to merge count is large. §3.5 formalizes this intuition.
In contrast, the slightly modified TCP out-of-sequence query from
Fig. 7 is linear-in-state because it can be written as
count = count + (lastseq > tcpseq) ? 1 : 0
where lastseq, the previous packet’s sequence number, depends
only on the last 2 packets: the current and the previous packet. Here,
A(p) and B(p) are functions of bounded packet history, with k = 2.
Merging queries that are linear-in-state requires the switch to store
the first k and most recent k packets for the key since it (re)appeared
in the key-value store; details are available in the accompanying tech
report [15]. An aggregation function is linear-in-state if, for every
variable in the function, the state update satisfies the linear-in-state