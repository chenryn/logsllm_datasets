[46] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Ried-
miller. 2014. Striving for simplicity: The all convolutional net. arXiv preprint
arXiv:1412.6806 (2014).
[47] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. arXiv preprint arXiv:1703.01365 (2017).
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In International Conference on Learning Representations.
[49] Mingxing Tan, Ruoming Pang, and Quoc V Le. 2020. Efficientdet: Scalable and
efficient object detection. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. 10781–10790.
[50] Brandon Tran, Jerry Li, and Aleksander Madry. 2018. Spectral signatures in
backdoor attacks. arXiv preprint arXiv:1811.00636 (2018).
[51] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2018. Clean-label
backdoor attacks. (2018).
581MISA: Online Defense of Trojaned Models using Misattributions
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
[52] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2019.
consistent backdoor attacks. arXiv preprint arXiv:1912.02771 (2019).
Label-
[53] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, and
Sudipta Chattopadhyay. 2019. Model agnostic defence against backdoor attacks
in machine learning. arXiv preprint arXiv:1908.02203 (2019).
[54] Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy,
Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
2020. NNoculation: broad spectrum and targeted treatment of backdoored DNNs.
arXiv preprint arXiv:2002.08313 (2020).
[55] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor
attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP).
IEEE, 707–723.
[56] Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun Yao, Haitao
Zheng, and Ben Y Zhao. 2021. Backdoor Attacks Against Deep Learning Systems
in the Physical World. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 6206–6215.
[57] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. 2019.
Detecting AI Trojans Using Meta Neural Analysis. arXiv preprint arXiv:1910.03137
(2019).
[58] Zhaoyuan Yang, Naresh Iyer, Johan Reimann, and Nurali Virani. 2019. Design
of intentional backdoors in sequential models. arXiv preprint arXiv:1902.09972
(2019).
[59] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. 2019. Latent Backdoor
Attacks on Deep Neural Networks. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security. 2041–2055.
[60] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolu-
tional networks. In ECCV. Springer, 818–833.
[61] Yi Zeng, Won Park, Z Morley Mao, and Ruoxi Jia. 2021. Rethinking the Backdoor
Attacks’ Triggers: A Frequency Perspective. arXiv preprint arXiv:2104.03413
(2021).
[62] Xinqiao Zhang, Huili Chen, and Farinaz Koushanfar. 2021. TAD: Trigger Approx-
imation based Black-box Trojan Detection for AI. arXiv preprint arXiv:2102.01815
(2021).
[63] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue
Lin. 2020. Bridging Mode Connectivity in Loss Landscapes and Adversarial
Robustness. arXiv preprint arXiv:2005.00060 (2020).
A APPENDIX
A.1 Neural Network Architectures
This section shows the Neural Network architectures we used to
train the Trojaned models for MNIST, Fashion MNIST, and GTSRB,
as shown in Table 10.
A.2 Triggers
Static Triggers. Static triggers (applied in the patch-based
A.2.1
approach) are placed in the same location every time we poison
an image. We consider triggers that fully, partially, and barely ob-
structs/overlaps with the main part of the image. These locations
include Top Middle (TM), Center Middle/Bottom Middle (M), and
Bottom Right (BR) of an image.
For grayscale images (MNIST, Fashion MNIST), we use white
and gray trigger colors, while for colored images, we use yellow,
purple, and white triggers with a black or blue background as well as
randomly colored triggers. Fig. 14 shows example images poisoned
with a random trigger and their attribution map.
Figure 15: Example images poisoned with a spread-out Trig-
ger of 16 pixels. The top row shows the 2 poisoned images
and their corresponding attribution maps (on the right of
each image) derived from the image and the Trojaned model.
The second row shows the same images without the trigger
and their corresponding attribution maps derived from the
image and the same Trojaned model.
Figure 14: Example images poisoned with a random Trigger
that is concentrated inside a 5x5 square. The top row shows
the 2 poisoned images and their corresponding attribution
maps (on the right of each image) derived from the image
and the Trojaned model. The second row shows the same
images without the trigger and their corresponding attribu-
tion maps derived from the image and the same Trojaned
model.
Figure 16: Example images poisoned with noise. The top row
shows the 2 poisoned images and their corresponding attri-
bution maps (on the right of each image) derived from the
image and the Trojaned model. The second row shows the
same images without the trigger and their corresponding at-
tribution maps derived from the image and the same Tro-
janed model.
For spread-out triggers we randomly choose n pixels spread
out in the image to change their color to white or yellow. In our
experiments, n is between 9 to 16. Fig. 15 shows example images
poisoned with a spread-out trigger and their attribution map. We
Image 1 poisoned with random 5x5 triggerAttributions of poisoned image 1Min:-0.6667 Max:3.9906Image 2 poisoned with random 5x5 triggerAttributions of poisoned image 2Min:-0.9878 Max:4.585Original image 1Attributions of image 1Min:-0.2531 Max:0.5878Original image 2Attributions of image 2Min:-0.8983 Max:1.87030100200202010020042024501001500.500.250.000.250.5050100150200250101Image 1 poisoned with spread-out triggerAttributions of poisoned image 1Min:-3.5927 Max:8.4443Image 2 poisoned with spread-out triggerAttributions of poisoned image 2Min:-1.6283 Max:7.2881Original image 1Attributions of image 1Min:-0.3344 Max:0.638Original image 2Attributions of image 2Min:-1.2268 Max:2.145701002005050100200505501001500.50.00.55010015020025021012Image 1 poisoned with noiseAttributions of poisoned image 1Min:-3.2855 Max:2.9802Image 2 poisoned with noiseAttributions of poisoned image 2Min:-1.6866 Max:1.9831Original image 1Attributions of image 1Min:-0.4411 Max:0.789Original image 2Attributions of image 2Min:-1.2854 Max:1.772605010015020020250100150200250101501001500.50.00.550100150200250101582ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha
Table 10: Model Architectures.
Dataset
MNIST
Fashion MNIST
CIFAR10
GTSRB
NN Architecture
Conv2D(32, (3,3)) + ReLU +
Conv2D(64, (3, 3)) + ReLU +
MaxPooling2D(2,2) + Dropout +
Dense(128) + ReLU + Dropout + Dense(10) + Softmax
Conv2D(64, (12, 12)) + ReLU + MaxPooling2D(2, 2) + Dropout +
Conv2D(32, (8, 8)) + ReLU + MaxPooling2D(2, 2) + Dropout +
Dense(256) + ReLU + Dropout + Dense(10) + Softmax
Conv2D(32, (3, 3)) + ReLU + BatchNorm + Conv2D(32, (3, 3)) + ReLU + BatchNorm + MaxPooling2D(2,2) + Dropout +
Conv2D(64, (3, 3)) + ReLU + BatchNorm + Conv2D(64, (3, 3)) + ReLU + BatchNorm + MaxPooling2D(2,2) + Dropout +
Conv2D(128, (3, 3)) + ReLU + BatchNorm + Conv2D(128, (3, 3)) + ReLU + BatchNorm + MaxPooling2D(2,2) + Dropout +
Dense(10) + Softmax
Conv2D(8, (5, 5)) + ReLU + BatchNorm + MaxPooling2D(2, 2) +
2(Conv2D(16, (3, 3)) + ReLU + BatchNorm + MaxPooling2D(2, 2)) +
2(Conv2D(32, (3, 3)) + ReLU + BatchNorm + MaxPooling2D(2, 2)) +
Dense(128) + ReLU + BatchNorm + Dropout + Dense(43) + Softmax
can see from Fig. 15 that the reverse-engineered trigger from high-
attributed values will not be the actual trigger which explains the
low TPR of our method. However, intermediate-layer attributions
improve the TPR of our method as mentioned in the experimental
section.
We refer to Instagram filters, smooth triggers and noise triggers
as static as they don’t require specifying a particular location.
Figure 18: Example images poisoned with the Instagram fil-
ter Toaster. Example images poisoned with noise. The top
row shows the 2 poisoned images and their corresponding
attribution maps (on the right of each image) derived from
the image and the Trojaned model. The second row shows
the same images without the trigger and their correspond-
ing attribution maps derived from the image and the same
Trojaned model.
Fig 17. As shown in the experimental section the attributions over
the input layer don’t reveal this type of trigger.
Finally, we used the following Instagram filters: Skyline, Toaster,
and Walden. Example images poisoned with Instagram Filters along
with their attribution map over the input layer is shown in Fig 18.
As shown in the experimental section the attributions over the
input layer don’t reveal this type of trigger.
A.2.2 Dynamic Triggers. Dynamic triggers can be generated by
sampling a trigger from a set of triggers and a location from a
predefined set of locations every time we poison an image [40].
The set of triggers consists of triggers with random values for a
given height and width generated by the TrojAI tool. The set of
predefined locations consists of 9 locations scattered throughout the
image, representing combinations of top, middle, bottom with left,
center, right. We trained 12 models for each dataset, with dynamic
triggers of shape 3x3, 5x5, 6x6, and 8x8. The set of triggers includes
Figure 17: Example images poisoned with the smooth trig-
ger. The top row shows the 2 poisoned images and their cor-
responding attribution maps (on the right of each image) de-
rived from the image and the Trojaned model. The second
row shows the same images without the trigger and their
corresponding attribution maps derived from the image and
the same Trojaned model.
Image-based triggers (smooth triggers and noise triggers) have
the same size as the input. Instagram filters apply a transformation
to the input. For noise triggers, we follow the approach of [10] to
add random noise to the image:(cid:101)x = x + δ, δ ∈ [−20, 20]H×W ×3,
where δ is determined randomly. In Fig. 16 we provide 3 example
images poisoned with noise and their corresponding attribution
maps. For smooth triggers we used the trigger provided by the
authors of [61] and also produced 10 more smooth triggers using
their approach. Example images poisoned with smooth triggers
along with their attribution map over the input layer is shown in
Image 1 poisoned with the smooth triggerAttributions of poisoned image 1Min:-5.6109 Max:4.148Image 2 poisoned with the smooth triggerAttributions of poisoned image 2Min:-5.234 Max:11.775Original image 1Attributions of image 1Min:-4.52 Max:3.9769Original image 2Attributions of image 2Min:-9.7466 Max:8.0160.20.40.60.81.05.02.50.02.55.00.20.40.60.81.010505100.20.40.60.81.0420240.00.20.40.60.8505Image 1 poisoned with Instagram Filter 'Toaster'Attributions of poisoned image 1Min:-0.8681 Max:1.0555Image 2 poisoned with Instagram Filter 'Toaster'Attributions of poisoned image 2Min:-0.2187 Max:0.4341Original image 1Attributions of image 1Min:-0.5684 Max:0.5419Original image 2Attributions of image 2Min:-1.2498 Max:1.38370501001502001.00.50.00.51.0501001502002500.40.20.00.20.4501001500.500.250.000.250.5050100150200250101583MISA: Online Defense of Trojaned Models using Misattributions
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
10 triggers that are created by choosing a random assignment of
values between 0 and 255. Example dynamic triggers with the
corresponding clean images and their attributions are shown in
Fig 19.
Figure 19: Example images poisoned with a dynamic trigger.
The top row shows the 2 poisoned images and their corre-
sponding attribution maps (on the right of each image) de-
rived from the image and the Trojaned model. The second
row shows the same images without the trigger and their
corresponding attribution maps derived from the image and
the same Trojaned model.
In Table 11, we give the number of attribution maps used for
training the SVMs. For example, for each MNIST Trojaned model,
we trained one SVM on the 8000 clean attributions that were derived
from the model and the corresponding 8000 clean images of the
evaluation set.
Table 11: Number of attribution maps (derived from clean
images) involved in the training of each SVM.
Fashion MNIST
Dataset
MNIST
CIFAR10
GTSRB
# SVM training instances
8000
8000
8000
10104
A.3 Examples supporting Experimental
Results
In Fig. 20 we compare the attributions from large and small triggers.
As mentioned in the experimental section larger triggers are not
always reverse-engineered correctly.
In Fig. 21 we show how the location of grayscale triggers can
affect the attribution and the ability of the method to recover a good
portion of the trigger. As mentioned in the experimental section
this happens mostly due to the use of a black baseline.
A.4 Percentage of Poisoning during Training
In this paper, we poison the minimum number of training instances
required to obtain a Trojan model. We provide the percentage of
training instances that are poisoned in Table 12. We observe that in-
creasing this percentage can lead to higher attribution values for the
Figure 20: Comparison of reverse-engineered triggers for
large and small square triggers. Top row: image with
8x8 white square trigger, attribution map, and reverse-
engineered trigger. Second row: image with 3x3 white square
trigger, attribution map, and reverse-engineered trigger.
Figure 21: Comparison of reverse-engineered triggers for
different positions of the same trigger. Top row: image with
trigger in the center. Second row: Image with trigger in the
bottom right.
Table 12: Percentage of training instances poisoned during
training of the NN models.
Dataset
MNIST
Fashion MNIST
GTSRB
Trigger Type
Static (except Noise)
Noise
Dynamic
Noise
Dynamic
Noise
Dynamic
Static (except Noise)
Static (except Noise)
Poisoning
1%
20%
10%
1%
20%
10%
10%
20%
10%
Trojan trigger in certain cases, as shown in Figures 23, 24, 25, 26, 27,
and 28.
Image 1 poisoned with dynamic 6x6 triggerAttributions of poisoned image 1Min:-0.8343 Max:1.8234Image 2 poisoned with dynamic 6x6 triggerAttributions of poisoned image 2Min:-1.8893 Max:7.4451Original image 1Attributions of image 1Min:-0.3976 Max:0.7305Original image 2Attributions of image 2Min:-1.0693 Max:1.465150100150200101100200505501001500.50.00.5501001502002501010102030051015202530Image with trigger50100150200250Attribution mapMin:-0.3919 Max:1.81161010102030051015202530Reverse-engineered TriggerFinal TPR: 39.8%0.00.20.40.60.81.00102030051015202530Image with trigger50100150200250Attribution mapMin:-0.6705 Max:3.3112020102030051015202530Reverse-engineered TriggerFinal TPR: 97.9%0.00.20.40.60.81.00102001020Image with triggerAttribution mapMin:-0.1502 Max:0.44330102001020Reverse-engineered Trigger0102001020Image with triggerAttribution mapMin:-0.178 Max:0.67290102001020Reverse-engineered Trigger01002000.250.000.25010020001002000.50.00.50100200584ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha
Figure 26: Attribution values for the square Trigger of size
3x3 across models with increasing percentage of poisoning
during training.
Figure 22: Comparison of attribution maps between clean
and Trojaned images for different sizes of a trigger added
in the center. and range of values for an image with a small
trigger, an image with a large trigger, and a clean image. Top
row: image with and without the triggers. Second row: Cor-
responding attribution maps.
Figure 27: Attribution values for the square Trigger of size
5x5 across models with increasing percentage of poisoning
during training.
Figure 23: Attribution values for the reverse lambda Trigger
of size 3x3 across models with increasing percentage of poi-
soning during training.
Figure 28: Attribution values for the square Trigger of size
8x8 across models with increasing percentage of poisoning
during training.
Figure 24: Attribution values for the reverse lambda Trigger
of size 5x5 across models with increasing percentage of poi-
soning during training.
Figure 25: Attribution values for the reverse lambda Trigger
of size 8x8 across models with increasing percentage of poi-
soning during training.
0200102030Image with 3x3 trigger0200102030Image with 8x8 trigger0200102030Image without triggerAttribution mapMin:-0.9245 Max:5.2287Attribution mapMin:-0.3414 Max:0.9866Attribution mapMin:-0.3361 Max:0.743701002000.50.00.501002005051002000.50.00.5Example poisoned imagewith reverse lambda 3x3 triggerpoisoning 1%Min:-0.3741 Max:2.0232poisoning 10%Min:-0.8747 Max:3.6131poisoning 20%Min:-1.106 Max:4.2986poisoning 30%Min:-1.3198 Max:3.61601002002101220242024202Example poisoned imagewith reverse lambda 5x5 triggerpoisoning 1%Min:-0.4706 Max:1.384poisoning 10%Min:-0.6175 Max:1.9374poisoning 20%Min:-0.8511 Max:2.5824poisoning 30%Min:-0.8457 Max:2.1122010020010110120221012Example poisoned imagewith reverse lambda 8x8 triggerpoisoning 1%Min:-0.2052 Max:0.8896poisoning 10%Min:-0.2119 Max:1.0804poisoning 20%Min:-0.562 Max:1.2362poisoning 30%Min:-0.2581 Max:1.055801002000.50.00.51.00.50.00.51.01.00.50.00.51.01.00.50.00.51.0Example poisoned imagewith 3x3 square triggerpoisoning 1%Min:-0.662 Max:1.2933poisoning 10%Min:-0.9505 Max:1.7068poisoning 20%Min:-0.6353 Max:1.1378poisoning 30%Min:-1.0164 Max:1.53320100200101101101101Example poisoned imagewith 5x5 square triggerpoisoning 1%Min:-0.1854 Max:0.4607poisoning 10%Min:-0.3825 Max:0.5714poisoning 20%Min:-0.2261 Max:0.6397poisoning 30%Min:-0.3775 Max:0.574901002000.250.000.250.50.00.50.50.00.50.50.00.5Example poisoned imagewith 8x8 square triggerpoisoning 1%Min:-0.2462 Max:0.4932poisoning 10%Min:-0.2097 Max:0.4554poisoning 20%Min:-0.2306 Max:0.4978poisoning 30%Min:-0.1421 Max:0.429201002000.250.000.250.250.000.250.250.000.250.250.000.25585