To address the second challenge, we present a tech-
nique that tests whether a site is vulnerable but that
avoids blocking the site for a noticeable amount of time.
The basic idea is to start with very small payloads that
do not require more computation time than normal web
requests, and to then slowly increase the payload – just
long enough to claim with conﬁdence that the site could
be exploited if an attacker used larger payloads. To de-
cide on the size of payloads sent to live websites, we run
experiments on locally installed web servers that use the
vulnerable packages.
An alternative to experimenting with live websites
would be to locally install open-source web applications.
We discarded this idea because it would limit the scale of
our study to the few web sites that disclose their server-
side code, because it would remain unclear whether the
results generalize to real-world sites, and because we
could not study which counter-measures are deployed in
practice.
3.1
Identifying Websites with Server-side
JavaScript
We consider the most popular one million websites ag-
gregated by Alexa3 as candidate sites for our study.
Many of these websites do not use JavaScript on the
server-side and analyzing all the websites against our ex-
ploits is prohibitive. Instead, we select sites that run the
currently most popular framework for JavaScript-based
web servers, Express4. To this end, we make a request
to each of the one million websites and check whether
the header X-Powered-By is “Express”. The framework
sets this value by default on a fresh installation. In to-
tal, 2,846 sites set this header which account for a mar-
ket share of around 0.3%, consistent with estimates by
others.5 Because headers may be ﬁltered to prevent at-
tackers from targeted attacks and because frameworks
other than Express exist, our selection of sites is likely
yield an underapproximation of the impact of ReDoS.
Figure 4 shows the number of Express-based websites
in batches of 100,000 sites, ordered by popularity. We
observe that Express tends to be used by the more pop-
ular websites, conﬁrming the importance of studying the
security of JavaScript-based servers.
3http://www.alexa.com/
4https://expressjs.com/
5https://w3techs.com/technologies/details/
ws-nodejs/all/all
Figure 4: Number of server-side JavaScript websites
within a given popularity range.
3.2 Finding ReDoS Vulnerabilities in Li-
braries
Our methodology relies on knowing previously un-
known, or at least not yet ﬁxed, ReDoS vulnerabilities
in popular npm modules. Similar to previous work [43],
we consider a regular expression to be vulnerable if we
can construct inputs of linearly increasing size that cause
the matching time of the expression to increase super-
linearly. To identify previously unknown vulnerabilities,
we use a combination of automated and manual analy-
sis, similar to what a potential attacker might do. This
technique is not the contribution of this paper, but rather
a way to enable our study. In principle, any other way of
identifying ReDoS vulnerabilities could be used instead,
including existing analyses [43], which however, are cur-
rently not available for JavaScript.
At ﬁrst, we download the 10,000 most popular mod-
ules and extract their regular expressions by traversing
the abstract syntax trees of the JavaScript code. This
yields a total of 324,791 regular expressions, with a mean
of 63.67, a median of 5.00 and a maximum of 19,791 per
module. After removing regular expressions that con-
tain no repetitions, and hence are immune to algorithmic
complexity attacks, we obtain a total of 138,123 expres-
sions, with mean 37.93 and median 4.00 per module.
Next, we semi-automatically search for regular ex-
pression patterns that are known to be vulnerable. For ex-
ample, we search for expressions containing repetitions
of a negated group followed by a character. The second
regular expression in Figure 6 is an example because it
contains the subexpression [^=]+=. A regular expres-
sion that is not anchored with a start anchor and contains
this pattern is likely to be vulnerable. The reason is that
the repetition group is generic enough to contain most
of the possible preﬁxes and the = character guarantees
that there exists a failing sufﬁx. For example, the regular
expression /ab[^=]+=/ can be exploited using a long
string "abababab..".
Given a set of possibly exploitable regular expression,
364    27th USENIX Security Symposium
USENIX Association
 0 100 200 300 400 500 6000-100K100K-200K200K-300K300K-400K400K-500K500K-600K600K-700K700K-800K800K-900K900K-1MNumber of sites using ExpressPopularity rankwe manually inspect the context in which the regular ex-
pressions are used. The goal is to ﬁnd matching oper-
ations on data that may be delivered through an HTTP
request to a web server. To this end, we focus on (i)
modules included in the Express framework, (ii) middle-
ware modules that extend this framework, and (iii) mod-
ules that manipulate HTTP request components, such as
the body or a speciﬁc header. For regular expressions
in these modules, we keep only those with a possible
data ﬂow from the package interface or from an HTTP
header to the regular expression. Overall, it took one of
the authors only a couple of days to ﬁnd 25 such vul-
nerabilities in widely used npm modules, showing that
a skilled individual can attack real-world websites with
moderate effort. A more powerful attacker could easily
detect a larger number of vulnerabilities and perform a
larger-scale attack.
3.3 Creating Exploits
Based on the ReDoS vulnerabilities in npm modules,
we create exploits targeted at web servers that use these
modules. The main idea is to hypothesize how a server-
side web application might use a module. To this end,
we set up a fresh Express installation and implement an
example web application that uses the module. For ex-
ample, for a package that parses the user agent, we build
an application that parses the user agent of every HTTP
request for the main page, which might be used to track
visitors. Next, we try to create an HTTP request where
user-controlled data reaches the vulnerable regular ex-
pression, and craft input values that trigger an unusu-
ally long matching time. For crafting the input, we try
to confuse the regular expression engine by forcing it to
backtrack because the input can be matched in multiple
ways [21, 43]. While creating exploits, we assume that
the maximum header size is 81,750 characters, which is
the default in Express.js. If we succeed in crafting an in-
put that takes more than ﬁve seconds, we consider the
vulnerability as exploitable and consider it for the re-
mainder of the study.
To further assess the impact of the exploits, we mea-
sure how much longer it takes to process a crafted input
compared to a random string of the same length. We
use two ways of measuring the time. First, we mea-
sure the matching time of the regular expression, i.e., the
time needed to check whether a string matches the regu-
lar expression. Second, we measure the time of an entire
HTTP request, called response time. The response time
may include various other components, such as HTTP
parsing and serialization, DNS resolving, routing time
for the package, and dealing with HTTP retransmissions
or package fragmentation. To measure the response time
of a site, we request its main page. For complex sites,
this measure underapproximates the time a human user
needs to wait for the page to load, because complex sites
require separate requests for images, etc.
3.4 ReDoS Analysis of Websites
The next step is to measure how many websites are vul-
nerable to a ReDoS attack based on one of the exploits.
The main challenge is to draw meaningful conclusions
about the harm that an attacker could cause, without ac-
tually attacking live websites. During our initial experi-
ments we sent one request with a crafted header that ap-
peared to make the analyzed website unresponsive for al-
most a minute. The goal of our methodology is to avoid
this type of mistake.
We address this challenge by triggering requests with
increasing input sizes, using both crafted and random in-
puts, while measuring the response times. Based on lo-
cally performed experiments, we choose input sizes that
are unlikely to block the server for more than a small,
conﬁgurable amount of time (we use two seconds in our
experiments).
If the response time with crafted inputs
grows faster than with random inputs, then we classify
the website as exploitable.
Measuring the response time in a reliable way is non-
trivial due to DNS resolving, network caching, delays,
retransmissions, and other inﬂuencing factors. Another
issue is how to determine whether the response time is
larger than another in a statistically reliable way. We ad-
dress these issues by adapting a technique originally used
for comparing the performance of software running on a
virtual machine [16, 29]. The basic idea is to repeatedly
measure the response time and to conclude that crafted
inputs cause a higher response time than random inputs
only if we observe a statistically signiﬁcant difference.
More speciﬁcally, to measure the response time for a
given input, we ﬁrst repeat the request nw times to “warm
up” the connection, e.g., to ﬁll network caches, and then
repeat the request another nm times while recording the
response times. Given k pairs of increasingly large ran-
dom and crafted inputs (irandom,icra f ted), where the two
inputs in a pair have the same size, we obtain k pairs
(Trandom and Tcra f ted) of sets of time measurements (with
|Trandom| = |Tcra f ted| = nm). For each input size, we com-
pare the conﬁdence intervals of the values in Trandom and
Tcra f ted and conclude that the response times differ if and
only if the intervals do not overlap. If the response times
differ for all k input sizes, we quantify the difference
for an input size as the difference between T random and
T cra f ted, where T is the average of the times in T . For
k input sizes, this comparison gives a sequence of differ-
ences d1, ..,dk. Finally, we consider a website to be ex-
ploitable if d1 < d2 < .. < dk. Intuitively, this means that
the response times for random and crafted inputs have a
USENIX Association
27th USENIX Security Symposium    365
statistically signiﬁcant difference, and that this difference
increases when the input size increases.
To execute these measurements, we need to pick val-
ues for nw, nm, k, and the k input sizes. We use nw=three,
nm=ﬁve, and k = 5 because these values are large enough
to draw statistically relevant conclusions for most web-
sites yet small enough to not disturb the analyzed server.
For picking the k input sizes, the challenge is to ensure
that measure a difference when there is one without re-
peatedly causing the server to block for a longer period
of time. We address this challenge by experimenting on
a locally installed version of the vulnerable package and
by choosing input sizes that take approximately 100ms,
200ms, 500ms, 1s and 2s to respond to.
Our setup allows us to assess whether a website could
be exploited without actually attacking it. Since we take
measurements in a sequential manner and since the over-
all number of requests per site is small, we allow legiti-
mate users to be served between our requests. Moreover,
the servers of popular websites implement some kind of
redundancy, such as multiple Node.js instances in a clus-
ter, i.e., our measurements are likely to block only one
such instance at a time. In contrast, an attacker would
likely send both more requests and requests with larger
inputs, which can cause severe harm to vulnerable sites,
as we show in Section 4.3.
3.5 Analysis of Mitigation Techniques
Some sites reject requests with large headers and instead
return a “400 Bad Request” error. This mitigation can
limit the damage of ReDoS attacks. To measure whether
a site uses this mitigation technique, we create benign
requests of different sizes and measure how often a site
rejects a request.
4 Results
This section presents the results of applying the method-
ology described in Section 3 to live, real websites. We
perform our measurements using three different ma-
chines depending on the experiments: a ThinkPad 440s
laptop with four Intel i7 CPUs and 12GB memory (Sec-
tion 4.1), a third party commercial web server with
512MB memory (Section 4.3 and 4.4) and a server with
48 Intel Xeon CPUs and 64GB memory (from Sec-
tion 4.6 on).
4.1 Vulnerabilities and Exploits
Figure 5 shows the modules for which we found at least
one vulnerable regular expression that can be exploited
through the module’s interface. At the time of perform-
ing our experiments, each vulnerability was working on
Module
Version
debug
lodash
mime
ajv
tough-cookie
fresh
moment
forwarded
underscore.string
ua-parser-js
parsejson
useragent
no-case
marked
content-type-parser
platform
timespan
string
content
slug
htmlparser
charset
mobile-detect
ismobilejs
dns-sync
2.6.8
4.17.4
1.3.6
5.2.2
2.3.2
0.5.0
2.18.1
0.1.0
3.3.4
0.7.14
0.0.3
2.2.1
2.3.1
0.3.6
1.0.1
1.3.4
2.3.0
3.3.3
3.0.5
0.9.1
1.7.7
1.0.0
1.3.6
0.4.1
0.1.3
Number of
dependencies
16,055
49,305
2,798
758
302
197
14,421
31
2,486
225
19
191
18
2,624
8
128
34
911
9
499
178
36
101
50
7
Downloads
in July 2017
54,885,335
44,147,504
22,314,018
17,542,357
15,981,922
14,151,270
10,102,601
9,883,630
7,277,966
5,332,979
4,897,928
3,515,292
3,321,043
3,012,792
2,337,147
757,174
523,290
421,700
316,083
151,004
138,563
112,001
107,672