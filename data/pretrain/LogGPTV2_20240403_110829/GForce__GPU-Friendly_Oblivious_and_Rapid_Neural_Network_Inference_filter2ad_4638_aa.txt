title:GForce: GPU-Friendly Oblivious and Rapid Neural Network Inference
author:Lucien K. L. Ng and
Sherman S. M. Chow
GForce: GPU-Friendly Oblivious and 
Rapid Neural Network Inference
Lucien K. L. Ng and Sherman S. M. Chow, 
The Chinese University of Hong Kong, Hong Kong
https://www.usenix.org/conference/usenixsecurity21/presentation/ng
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.GForce: GPU-Friendly Oblivious and Rapid Neural Network Inference
Lucien K. L. Ng
Sherman S. M. Chow∗
Department of Information Engineering
The Chinese University of Hong Kong
Department of Information Engineering
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
Shatin, N.T., Hong Kong
Abstract
Neural-network classiﬁcation is getting more pervasive. It
captures data of the subjects to be classiﬁed, e.g., appearance
for facial recognition, which is personal and often sensitive.
Oblivious inference protects the data privacy of both the query
and the model. However, it is not as fast and as accurate as its
plaintext counterpart. A recent cryptographic solution Delphi
(Usenix Security 2020) strives for low latency by using GPU
on linear layers and replacing some non-linear units in the
model at a price of accuracy. It can handle a query on CIFAR-
100 with ∼68% accuracy in 14s or ∼66% accuracy in 2.6s.
We propose GForce, tackling the latency issue from the
root causes instead of approximating non-linear computations.
With the SWALP training approach (ICML 2019), we pro-
pose stochastic rounding and truncation (SRT) layers, which
fuse quantization with dequantization between non-linear and
linear layers and free us from ﬂoating-point operations for ef-
ﬁciency. They also ensure high accuracy while working over
the severely-ﬁnite cryptographic ﬁeld. We further propose
a suite of GPU-friendly secure online/ofﬂine protocols for
common operations, including comparison and wrap-around
handling, which beneﬁt non-linear layers, including our SRT.
With our two innovations, GForce supports VGG-16, at-
taining ∼73% accuracy over CIFAR-100 for the ﬁrst time, in
0.4s. Compared with the prior best non-approximated solution
(Usenix Security 2018), GForce speeds up non-linear layers in
VGG by >34×. Our techniques shed light on a new direction
that utilizes GPU throughout the model to minimize latency.
1 Introduction
Machine learning is becoming more prevalent. Deep neural
networks (DNNs) achieved great success, notably in image
recognition tasks with applications in surveillance or medical
check. These applications often process sensitive or at least
personal data. Clients can be reluctant to hand in their data
to the model owner (or the server). Meanwhile, sending the
∗Supported by General Research Fund (CUHK 14210319) of UGC, HK.
model to the clients for evaluation is often impossible, not to
say its ﬁnancial and privacy implications.
Oblivious inference resolves this dilemma. The server with
a deep neural network DNN(·) can return the classiﬁcation
result DNN(x) to any client while remains oblivious about x
without leaking its model DNN(·). From the perspective of
computation nature, a neural network can be divided into
linear layers and non-linear layers. Cryptographic solutions
often handle linear layers and non-linear layers separately,
such as using additive homomorphic encryption (AHE) and
garbled circuits (GC), respectively, but these tools impose
high overheads. A recurrent research problem is how to per-
form secure computations of non-linear functions efﬁciently.
1.1 Two Open Challenges
We reckon GPU as a promising tool for reducing latency. It is
highly-optimized for computing in parallel, accelerating DNN
computation when compared with CPU, primarily on paral-
lelizable linear operations. Delphi [18], the state-of-the-art
cryptographic framework, utilizes GPU to accelerate linear
layers but fails to beneﬁt non-linear layers. Instead, Delphi en-
courages the training scheme to replace ReLU layers by their
quadratic approximation (i.e., x2), which lowers the latency
but still sacriﬁces accuracy. The non-linear computations, in-
cluding those remaining ReLU layers and maxpool layers, are
still handled by less efﬁcient tools such as GC. Unfortunately,
it is unclear how GC can leverage GPU parallelism.
Most (plaintext) neural networks (especially those with
high accuracy) run over ﬂoating-point numbers (“ﬂoats”) with
large ﬂuctuations in the model parameters (in magnitude rep-
resented by 256 bits). In contrast, cryptographic frameworks,
utilizing primitives such as AHE, GC, and secret sharing,
mostly handle values in a small magnitude (usually 20 ∼ 40
bits) range. Extending the bit-width inside the cryptographic
tools for higher precision slows down all operations. Some
recent works (e.g., XONN [21]) adopt binarized neural net-
works with accuracy lower than the original one. The inherent
tension between accuracy and efﬁciency remains.
USENIX Association
30th USENIX Security Symposium    2147
1.2 Our Contributions
This paper tackles the latency versus accuracy issue from the
root causes. Our framework, which we call GForce, is a new
paradigm of oblivious inference based on specially-crafted
cryptographic protocols and machine-learning advances.
On the machine-learning front, we formulate stochastic
rounding and truncation (SRT) layers, making a quantization-
aware training scheme SWALP [28] more compatible with
(our) cryptographic tools. SWALP trains a DNN under a low-
precision setting while keeping accuracy, but its extra process-
ing introduces latency during oblivious inference. Our SRT
layer serves as a “swiss-army knife,” which contributes to re-
duced latency and communication complexity while keeping
the intermediate values of DNN evaluation “small.”
On the cryptography front, we propose a suite of GPU-
friendly protocols for both linear layers and common non-
linear layers to enjoy the power of GPU parallelism. It en-
ables an elegant approach to oblivious inference, in contrast
to existing approaches of switching between different crypto-
graphic primitives (e.g., arithmetic, boolean, and Yao’s shares)
across different layers (e.g., three-non-colluding-servers ap-
proaches [27]) or customizing alternatives (e.g., polynomial
approximation [5] or replacement with square [18]).
High-accuracy Networks in the Low-precision Setting.
To overcome the low-precision issue that bars our way to our
high-accuracy goal, we adopt SWALP [28], a scheme to ﬁt
a neural network into the low-precision setting. It takes as
inputs the DNN architecture, hyper-parameters, and training
data and returns a trained DNN whose linear layers can run
in a low-precision environment. SWALP reported that the
accuracy loss due to the low-precision setting is <1pp [28].
While it sounds ﬁtting our purpose exactly, making it secure
and efﬁcient is still not easy (see Section 2). SWALP requires
(de)quantization for intermediate DNN results, which can be
seen as truncation that conﬁnes the magnitude range to pre-
vent overﬂow. Secure computation of the needed operations,
especially stochastic rounding, is rarely explored. A recent
work [27] explicitly mentioned that truncation is expensive.
To reduce computation and the complexity of individual
cryptographic operation, we formulate SRT layers, which fuse
dequantization, quantization, and stochastic rounding. Such
formulation may inspire further improvement in the seamless
integration of machine learning and cryptography.
As the tension between working in a limited plaintext space
and not risking overﬂowing still exists, we also derive param-
eters for striking a balance under such an inherent trade-off.
GPU-Friendly Protocols for Non-linear Operations. It is
unclear how we can leverage GPU for non-linear layers. For
the ﬁrst time, we propose a suite of GPU-friendly protocols
for primitive operations in popular non-linear layers and our
newly formulated stochastic rounding and truncation layers.
Secure comparison is a core functionality necessary for
computing ReLU (approximated by Delphi) and maxpool
layers (failed to be optimized by Delphi). Existing secure
comparison protocols involve computations that fail to lever-
age the power of GPU. Our technical contribution here is a
semi-generic approach that transforms AHE-centric protocols
to their functionally-equivalent GPU-friendly version. We call
it our SOS trick (see Section 3.3), which stands for secure
online/ofﬂine share computation. Our protocols have a lower
online communication cost than their GC-based counterparts.
Moreover, to twist the performance to the extreme, we design
our protocols with the precision constraints of cryptographic
tools and GPUs in mind. We also need to develop GPU-
friendly protocols for truncation and wrap-around handling
to enable GForce to run in low-precision without error.
All our protocols do not require any approximation. Us-
ing them over a DNN can attain its original accuracy in the
(low-precision) plaintext setting. Concretely, when compared
with prior works that also avoid approximating ReLU units
(Gazelle [11] and Falcon [13]), GForce is at least 27× faster
when handling a large number (217) of inputs (see Table 4
in Section 4.1). As a highlight, for a CIFAR-100 recognition
task (see Section 4.2), GForce attains 72.84% accuracy with
0.4s. (The prior best result by Delph handles a query in 14.2s
with 67.81% accuracy or 2.6s with 65.77% accuracy.)
To summarize, we make the following contributions.
1) We complement quantization-aware training with our
stochastic rounding and truncation layers that normalize inter-
mediate results and reduce computational and communication
complexities throughout the model while keeping accuracy.
2) We propose a suite of protocols for non-linear operations,
which exploits GPU parallelism and reduces latency.
3) We implement our framework and demonstrated our supe-
rior accuracy and efﬁciency, notably, even over the state-of-
the-art approach of using three non-colluding servers [27].
4) Technical insights in GForce, e.g., SWALP adoption, SRT
layer, and GPU-friendly secure protocols for (non-)linear lay-
ers, can beneﬁt some existing and future frameworks.
2 Technical Overview
GForce is an online/ofﬂine GPU/CPU design. In the ofﬂine
phase when the query is unknown, some precomputation is
done without knowing the actual query. Upon receiving a
(private) query in the online phase, we ask the GPU to quickly
perform“masked” linear computation (in batch) online, even
for non-linear layers. All our cryptographic protocols share
this core feature. In particular, GForce only precomputes
the relatively costly AHE-related operations ofﬂine. Online
computations use the much more efﬁcient additive secret
sharing (SS), which provides the masking we need. Both
AHE and SS operate over ﬁxed-point numbers in Zq.
2148    30th USENIX Security Symposium
USENIX Association
Issues in using GPU for Cryptography
2.1
Low-precision Setting in GPU. GPU is optimized for 32-
bit and 64-bit ﬂoats while supporting 24-bit and 52-bit integer
arithmetic operations, respectively. Overﬂowing (on GPU’s
integer part) will lead to precision loss or even trash the values
represented in ﬂoats. We need to ensure the value being secret-
shared does not exceed 52 bits after each GPU addition and
multiplication. It only leaves us ∼20 bits as the plaintext
space, which we call bit-width, denoted by (cid:96).
Furthermore, secure protocols, including those we propose,
have computation and communication costs of at least Ω((cid:96)).
Running under less bit-width is vital for performance.
Quantization to the Low-precision Setting. DNN opera-
tions are mostly over ﬂoats. A careful quantization is needed
to store them in ﬁxed points; otherwise, they may overﬂow
when they are too large or become 0 when they are too small.
2.2 GPU-Friendly Secure Comparison
GForce focuses on leveraging GPU for comparison, which
is a crucial operation in non-linear layers, including ReLU
and maxpool in many popular neural networks (e.g., [10, 22]).
These non-linear layers can be securely computed via the
secure comparison protocol of Damgård–Geisler–Krøigaard
(DGK protocol) [7]; however, it heavily relies on AHE and
other non-linear operations that are still inefﬁcient over GPU.
A novel component of GForce is its GPU-friendly secure
comparison protocol, which we built by ﬁrst decomposing the
original DGK protocol into a bunch of linear operations and
inexpensive non-linear operations, e.g., bit-decomposition on
plaintexts. We also prove that, as long as the values in those
linear operations are not leaked, those non-linear operations
are safe to perform without protection. We can then adopt the
online/ofﬂine GPU/CPU paradigm to speed up all layers.
2.3 Issues in Oblivious Inference with SWALP
To run neural networks over a low bit-width ﬁnite ﬁeld
for high performance while maintaining accuracy, we use
Stochastic Weight Averaging in Low-Precision Training
(SWALP) [28] for linear layers. Intuitively, as SWALP trains
a DNN under low bit-width integers, its trained parameters
and hence its accuracy are optimized for ﬁxed-point integers.
Using SWALP within a cryptographic framework poses
several challenges. Speciﬁcally, the (de)quantization scales
up/down and rounds up the values according to the maximum
magnitude among all input values. A direct adoption requires
the rather inefﬁcient secure computation of maximum, round-
ing, and division. Furthermore, we still need to dequantize the
output of linear layers before feeding it back to the non-linear
layers (the second row of Figure 1); this would bring us back
to securely computing over ﬂoating-point numbers.
Garbled Circuit (not used by GForce)
Additive Homomorphic Encryption
Secret Sharing
GC (§1, §7)
AHE (§1, §3.1)
SS (§2, §3.1)
DGK (§2.2, §3.4) Damgård et al.’s secure comparison
SWALP
(§1, §2.4, §3.7)
Stochastic Weight Averaging
in Low-Precision Training
Table 1: Acronyms for Existing Concepts
2.4 Stochastic Rounding and Truncation
Precomputing Maximum Magnitudes.
Instead of ﬁnd-
ing the maximum, we employ the heuristics (Section 3.7.1)
of gathering statistics from training data to estimate for the
queries, which ﬁxes the required parameters in advance. Only
a few bits of information (per layer) need to be shared with
the client for (de)quantization (more in Section 5.1).
Fusing (De)Quantization. We observe that we can bring