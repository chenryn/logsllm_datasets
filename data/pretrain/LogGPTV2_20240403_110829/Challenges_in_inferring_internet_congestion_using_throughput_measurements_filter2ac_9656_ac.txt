times in different geographical regions and which may have vastly
different performance characteristics. Routing policies and varying
client vantage point diversity can lead to signiﬁcant differences in
the number of measurements traversing different interconnects. On
the other hand, aggregating measurements that cross different IP
links between the same pair of routers may be acceptable, as load
balancing generally ensures an even distribution of ﬂows across par-
allel links. The range of possible scenarios highlights the impor-
tance of inferring the set of IP or router-level links that comprise
the AS-level aggregation. Once the set of all IP links traversed by
measurements from a server AS to a client AS are identiﬁed, it is
possible to separate the NDT tests according to the IP link traversed,
and evaluate whether different IP links comprising an AS-level ag-
gregate do indeed show similar behavior. Unfortunately, the com-
plexity of router-level interconnection may render path information
from Paris traceroute insufﬁcient to accurately identify the inter-
domain connection between two networks (the MAP-IT algorithm
could fail or produce an incorrect inference). We need dedicated
tools such as bdrmap [26] running on the server-side infrastructure
to map interdomain borders, which could utilize additional measure-
ments beyond traceroutes (e.g., alias resolution), and traceroutes in
both directions associated with an NDT test, to accurately pinpoint
the interdomain link traversed by each NDT test.
5 PLACEMENT OF TESTING SERVERS
Placement of servers for throughput testing has the primary objec-
tive of minimizing latency to the client (§ 2). We propose two ad-
ditional considerations for using these measurement infrastructures
to infer congestion on interdomain links. First, paths from within
the access ISP to the test servers should cover as many interconnec-
tions of the access AS as possible. Second, measured paths should
be representative of paths that normal, user-generated trafﬁc from
the clients traverse. We estimate, for two throughput-measurement
platforms – M-Lab and Ookla’s Speedtest.net – the set of interdo-
main interconnections of an access network that are covered, i.e.,
whether a test to any server from that platform run from a client in
the access network would traverse a given interdomain link of that
access network.
5.1 Methodology to assess coverage
Measuring interdomain connectivity of access ISPs:
To measure the coverage of interdomain interconnections of access
ISPs that the currently deployed server-side measurement infras-
tructure can provide, we ﬁrst need to identify the set of interdo-
main interconnections of those access ISPs. For this purpose we
take a different approach from that in Section 4, where we had no
option but to use existing traceroutes from M-Lab servers to clients
in access ISPs. Here, we use vantage points inside access ISPs to
launch comprehensive topology measurements outward toward the
whole Internet. CAIDA operates a large measurement infrastructure
consisting of more than a hundred Archipelago (Ark) [11] vantage
points, many of which are hosted by access networks of interest.
For this study, we employed 16 Ark vantage points (VPs) located
in 9 access ISPs in the U.S.: 5 in Comcast, 3 in Time Warner Ca-
ble, 2 in Cox, and one each in Verizon, CenturyLink, Sonic, RCN,
Frontier, and AT&T. These vantage points are located in 8 of the
top 10 broadband access providers in the U.S.; we have at least one
VP in each of the top 5 providers. We focused on VPs in the U.S.
for two reasons. First, M-Lab’s focus is predominantly U.S.-centric.
Second, recent disputes about congestion at interdomain links of
access ISPs focused on U.S.-based access networks, and reports re-
leased by M-Lab [27] focused on U.S.-based networks.
To compile the set of interdomain interconnections of a given ac-
cess network visible from an Ark VP in that network, we utilized
bdrmap [26], an algorithm that accurately (the authors of [26] val-
idated the algorithm to more than 90% accuracy on their ground
truth data) infers all interdomain interconnections of a VP network
visible from that VP. In the collection phase, bdrmap issues tracer-
outes from the VP toward every routed BGP preﬁx, and performs
alias resolution (from the VP itself) on IP addresses seen from that
VP in the traceroutes. We performed the data collection for bdrmap
from our set of VPs in January and February 2017. In the analysis
phase, we ran bdrmap using the collected topology data along with
AS-relationship inferences from CAIDA’s AS-rank algorithm for
January 2017 [12], and a list of address blocks belonging to IXPs
obtained from PeeringDB [34] and PCH [32]. bdrmap outputs a set
of interdomain interconnections for each VP, i.e., a set of border
routers and neighboring networks, annotated with the type of rout-
ing relationship (customer, provider, peer, or unknown) between the
VP network and the neighbor.
Table 3 shows, for each Ark monitor from which we ran bdrmap,
the number of interdomain interconnections discovered at the AS
and router level. We also classify the AS interconnections as cus-
tomer, provider or peer using the aforementioned AS-relationship
data. The data reveals the interconnection diversity in this set of
access providers; some access providers such as AT&T, Verizon,
Comcast and CenturyLink also operate large transit networks with
thousands of customers and tens of peers. More importantly, the
data highlights the scale of interdomain interconnection between
large access networks. The largest access networks have hundreds
of interdomain interconnections at the router-level. Even a relatively
small provider such as RCN has 87 interconnections at the AS-level
and 101 at the router-level.
Measuring the coverage of interdomain links
To ascertain the set of interdomain links that were covered using
the M-Lab or Speedtest.net servers, we performed traceroutes from
each Ark VP toward each of the M-Lab and Speedtest.net servers.
We use the output of bdrmap to identify the interdomain link, if any
(at both the router and AS-level) traversed by the traceroute. If the
traceroute from a VP to a testing server S traverses a router-level
interdomain link r corresponding to the AS-level link A, then we
classify AS A and the router-level interconnection r with AS A as
covered by the server S.
Measuring the paths to popular web content
We also wanted to ascertain the intersection between the intercon-
nections that are covered using either the M-Lab or Speedtest.net
server infrastructure, and those on the paths toward popular web
content from each access ISP. For each domain in the Alexa top
500 U.S. sites [3], we scraped the default page and extracted all
subdomains. We performed DNS lookups of those domains at the
IMC ’17, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
Network
Ark VP
ALL borders
AS
Router
Comcast
Verizon
TWC
Cox
CenturyLink
Sonic
RCN
Frontier
AT&T
bed-us
mry-us
atl2-us
wbu2-us
bos5-us
mnz-us
ith-us
lex-us
san4-us
msy-us
san2-us
aza-us
wvi-us
bed3-us
igx-us
san6-us
1333
1336
1327
1050
1279
1423
720
676
660
482
488
1729
96
87
56
2283
2896
2874
1785
1485
1768
2187
968
935
865
623
639
2439
106
101
73
3336
CUST borders
AS
Router AS
PROV borders
Router
1115
1118
1107
897
1070
1304
588
547
535
363
370
1572
6
35
29
1738
1740
1318
1129
1293
1988
662
613
599
410
424
2186
6
38
30
2123
2872
3
3
3
4
3
12
3
3
3
4
4
3
4
1
3
12
37
43
20
23
16
32
28
29
26
13
15
7
5
5
6
127
PEER borders
AS
Router
41
41
41
48
40
21
28
27
28
21
21
42
10
36
17
40
541
478
139
131
159
49
83
83
65
27
29
99
10
41
29
132
Table 3: Statistics from our border identiﬁcation process. We ran bdrmap in Jan-Feb 2017 on a wide variety of networks in terms of
size. While each of the measured networks provides broadband access, several networks such as AT&T, Verizon, CenturyLink and
Comcast provide transit, which is reﬂected in the large number of AS customers. From the point of view of congestion measurement,
the number of peers (and particularly the number of router-level peer interconnections) is important.
Bordermap
Mlab
Speedtest
Bordermap
Mlab
Speedtest
S
A
s
n
o
i
t
c
e
n
n
o
c
r
e
t
n
i
 10000
 1000
 100
 10
 1
r
e
t
u
o
R
s
n
o
i
t
c
e
n
n
o
c
r
e
t
n
i
 10000
 1000
 100
 10
 1
C
O
M
-
1
C
O
M
-
2
C
O
M
-
3
C
O
M
-
4
C
O
M
-
5
V
Z
T
W
C
-
1
T
W
C
-
2
T
W
C
-
3
C