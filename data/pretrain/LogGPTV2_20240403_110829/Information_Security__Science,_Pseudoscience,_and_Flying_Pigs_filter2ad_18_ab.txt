network  security architecture.  While this  was done at a 
low  level  of  assurance,  nothing  prevents  the  same 
techniques from being applied at high levels of assurance. 
Another approach to securing the client in a network 
is to use a “thin client” that does not execute application 
software.  All application s run on a server and the client 
provides only interface functions. 
are 
A  problem  faced  by  both  clients  and  servers  is  the 
boot  process.    Many  of  today’s  platforms  effectively 
accept  firmware  updates  from  anywhere,  making  an 
attractive target for subversion.  The Trusted Computing 
Platform  Alliance  has  define  a  valuable  framework  for 
addressing this problem, but the solution must incorporate 
verifiable protection if it is to counter malicious software 
attacks [6]. 
technique 
is  analogous 
Cryptographic sealing of objects within directories is 
another  example  of  getting  around  intractable  problems. 
This 
to  database  “guard” 
architectures  [7]  that  provided  limited  solutions  to  the 
problem  of  building  trusted  database  systems.  The  near 
term potential for a truly secure directory is roughly nil.  
However if a trusted system seals the objects it places in 
an  insecure  directory,  then  many  of  the  problems 
associated with insecure directories goes away. 
Security engineering on a systems basis is the key to 
success  without  having  to  solve  all  of  the  unsolvable 
problems.    Enough  tools  exist  today  to  solve  the  major 
problems  that  we  face  today.    But  these  tools  are  only 
useful  if  we  have  the  basic  ability  to  secure  data  on  a 
somewhat 
requiring 
handcrafted  solutions  is  bound  to  fail  largely  because 
there simply is not enough expertise to apply to every site 
requiring  strong  security. 
  We  need  commercially 
available products, especially appliances that can provide 
verifiable demonstrable protection for systems.   
  Any  solution 
routine  basis. 
2.5.  Measuring System Security  
How  do  we  know  if  a  system  offers  verifiable 
protection?  If  we  buy  a  commercial  product  (e.g.,  an 
Internet  Appliance)  that  offers  verified  protection,  what 
metric can be used to measure the strength of its security, 
particularly in the context of an overall system? A simple 
business  metric  is  whether  the  use  of  the  appliance  to 
protect against massive loss can be insured, and the cost 
of 
transactions 
the  premium  per  million-dollars  of 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:10:17 UTC from IEEE Xplore.  Restrictions apply. 
protected  by  the  appliance.  A  truly  meaningful  product 
evaluation  would  permit  an  insurer  to  quantify  the  risks 
enough to issue such a policy.  Such an evaluation must 
have a well-defined systems context.   
Evaluation of a subsystem does the insurer little good 
if  protection  depends  on  software  outside  the  target  of 
evaluation.  The  Trusted  Computer  System  Evaluation 
Criteria (TCSEC) [4], is a system evaluation criteria.  Its 
Trusted  Network  Interpretation  (TNI)  [8]  imposes  the 
context of a “network security architecture” that permits 
components of systems to be individually evaluated in a 
way that ensures the eventual composition of the overall 
system will be secure.   On the other hand, the Common 
Criteria  [9]  provides  a  framework  for  evaluations  which 
do  not  necessarily  answer  the  question  “is  the  system 
secure”.  Common Criteria evaluations need not provide a 
system  context  and  therefore  the  insurer  would  have  to 
perform their own systems evaluation.   
2.6.  Science and Pseudoscience 
"The  time  has  come,"  the  Walrus  said,    "To 
talk  of  many  things:  Of  shoes  and  ships  and 
sealing wax,  Of cabbages and kings, And why the 
sea  is  boiling  hot,    And  whether  pigs  have 
wings."[10] 
The audience of this essay is intended to include the 
student  of  computer  science  having  an  interest  in 
information security.  Such students are likely aware that, 
as commonly used, the term “computer science” can often 
be  closer  to  “pseudoscience”.      In  the  field  of  computer 
and  network  information  security,  the  challenge  of 
separating science from pseudoscience is quite acute.  In 
some  cases,  computer  security  pseudoscience  operates 
from  a  flawed  theory.    In  other  cases,  as  in  flying  pigs, 
there  is  not  so  much  as  a  working  theory  to  explain  a 
proposed solution. 
The  terms  “science”  and  pseudoscience  are  used  as 
per the “Skeptic’s Dictionary” [11] at SkepDic.com: 
A pseudoscience is set of ideas based on theories put 
forth  as  scientific  when  they  are  not  scientific.  A 
theory is scientific if and only if it explains a range of 
empirical  phenomena  and  can  be  empirically  tested 
in  some  meaningful  way.  Scientific  testing  usually 
involves  deducing  empirical  predictions  from  the 
theory.  To  be  meaningful,  such  predictions  must,  at 
least in theory, be possible to be false. This quality of 
scientific  theories  was  called  falsifiability  by  Karl 
Popper.  A  pseudoscientific  theory  claims  to  be 
scientific, i.e., be falsifiable, but either the theory is 
not  really  falsifiable  or  it  has  been  falsified  but  its 
adherents  refuse  to  accept  that  the  theory  has  been 
refuted.  
Pseudoscience  is  evident  in  what  historically  has 
is  often 
been  called  application  security,  which 
characterized  by  not  having  any  clearly  defined  security 
policy.    Examples  include  firewalls,  intrusion  detection 
systems,  and  virtually  all  security  offered  by  Microsoft 
and similar vendor’s products. 
Another application of pseudoscience involves strong 
mechanisms  on  weak  foundations:    Examples  includes 
most  cryptography  products  (i.e.,  PGP,  S/MIME,  SSL, 
VPNs, etc.).  It also includes techniques such as layering 
type-enforcement  on  top  of  weak  operating  systems  and 
additions of access control mechanisms to Linux or Free 
BSD Unix. 
Flying  pigs  are  the  goal  of  too  much  sponsored 
research,  and  the  recommendation  of  many  blue  ribbon 
panels.    For  example,  a  recent  pronouncement  from  the 
National  Research  Council  [12]  suggested  that  research 
focus  on  something  called 
the  “three  axioms  of 
insecurity”  and  pursue  techniques  for  making  a  network 
more secure than any of its constituent components.  The 
striking lack of any credible working theory is enough to 
offend  the  sensibilities  of  even  the  pseudoscientist.  
Indeed  “limitations  on  new  research”  was  one  of  the 
loudest  complaints  stimulated  by  the  establishment  of 
objective criteria and real world tools for handling system 
composition.  Yes,  it  would  be  nice  if  we  could  make 
systems  more  secure  than  their  constituent  components. 
On  the  other  hand,  it  is  hardly  new  to  find  a  desire  to 
make a silk purse from sows ears – flying or otherwise. 
leverage 
Verifiable protection  has progressed and  matured to 
the  point  where  products  can  be  built,  incrementally 
evaluated and used within heterogeneous networks. Some 
hard  problems  remain,  however  the  pressing  challenges 
of today are amenable to divide and conquer techniques 
the  power  of  verified  protection.  
that 
Notwithstanding 
advances  have  not 
vanquished pseudoscience, which today dictates too much 
of our allocation of resources for protecting our computer 
resources.  By  examining  the  history  of  the  advances  of 
the  science  we can perhaps better understand the depths 
of our decline in the state-of-art of application of science. 
these 
that, 
2.7.  Epochs of Scientific Advances 
In  the  previous  section,  we  described  the  general 
problem of separating science from pseudoscience.  The 
plethora  of  pseudoscientific  pursuits  in  this  field  far 
exceeds our scope.  Therefore, the remainder of the essay 
will  largely  focus  on  scientific  advances  with  little 
attention  paid 
to  contemporaneous  pseudoscientific 
adventures except where the contrast is instructional. 
following 
sections  provide  a  historical 
perspective  on  how  it  is  we  achieved  the  capability  of 
verified  protection  and  why  it  is  that  verified  protection 
provides  important  solutions  to  important  problems.    A 
set of historical epochs is presented to provide context to 
the  accumulation  of  scientific  knowledge  and 
its 
The 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:10:17 UTC from IEEE Xplore.  Restrictions apply. 
engineering application.  This is  not a  history essay and 
no claim is made as to its historical completeness.   
Figure 3 identifies three epochs in the advancement 
of the science of computer security, followed by an epoch 
of decline.  Next these are reviewed chronologically. 
Security Kernel 
Concept & 
Kernel 
Prototypes 
Develop Criteria 
and Make 
Available 
Commercial 
Evaluations 
TCB Subsets for 
System Composition 
and Incremental 
Evaluation 
Blurring of 
Science & 
Pseudoscience, 
e.g., the Common 
Criteria 
1970 
1980 
1990 
2000
Figure 3: Epochs of Advance Followed by Decline
3.  Security Kernel 
This  epoch  culminated  in  the  mid-1970s  with  the 
definition,  prototyping  and  fielding  of  security  kernels.  
Prior to this, secure systems were defined by vendors and 
aerospace  companies  as  being  systems  purchased  with 
extra software and functions added to make them secure.   
Add-on  security  and  applications-level  security  was 
promoted by vendors, much as it is today.  Since some of 
the very earliest applications of computers, there has been 
a recognized critical need for security.  Initially this need 
arose in the military context.   
3.1.  Multilevel Security 
security 
enforcement. 
  Specifically, 
Development  of  early  military  systems  concluded 
that  some  portions  of  the  system  require  particularly 
strong 
this 
enforcement  was  necessary  to  protect  data  whose  loss 
would  cause  extremely  grave  damage  to  the  nation. 
Systems  that  handled  such  data,  and  simultaneously 
included interfaces and users who were not authorized to 
access such data, came to be known as “multilevel”. Note 
this  term  was  never  intended  to  mean  “hierarchical”,  it 
also  applied 
to  non-hierarchical,  well-defined  user 
groups.    The  security  policy  enforced  by  these  systems 
came  to  be  known  as  mandatory  access  controls  as 
introduced in section 2.1, “Verifiable Protection”. 
One of the first examples of a multilevel system was 
the SAGE air defense system of the 1950s.  Much of the 
computer  security  focus  was  then  within  the  context  of 
“nuclear  safety”  and  had  to  do  with  the  integrity  of 
guidance data for nuclear-armed anti-aircraft missiles on 
American  soil.    Additionally,  the  system  had  to  protect 
the 
aspects  of  weapons 
characteristics,  as  well  as  air  defense  tactics.  SAGE 
supported 
communication 
interfaces,  which  meant  something  had 
to  prevent 
sensitive  information  from  being  disclosed  or  modified 
secrecy  of 
unclassified 
clear-text 
technical 
via  the  unclassified  interfaces.      The  early  sixties  saw 
these  same  issues  as  part  of  missile  defense  systems, 
which  solved  some  of  the  problems  through  use  of 
encryption  to  protect  communications.    These  early 
“multilevel”  systems  can  be  viewed  as  deploying 
applications  security  with  no  real  allocation  of  security 
policy enforcement to distinct components or subsystems. 
One  of  the  earliest  computer  systems  specifically 
designed  as  a  secure  system  for  enforcing  a  mandatory 
policy was a Southeast Asia Air Force tactical air control 
system  with  a  digital  interface  to  NSA  intelligence 
systems  for  sanitization  of  data.    The  system  was  built 
with computers specially built in a cleared environment, 
rather  than  commercial  equipment,  because  of  security 
considerations.  Also, cleared programmers developed all 
of  the  software  specifically  because  of  the  concern  of 
malicious  software  (e.g.,  trap  doors).    In  addition, 
programmers  enforced  software  development  practices 
that  were  a  precursor  to  the  Life-Cycle  Assurance 
requirement  of  the  TCSEC  [4]  for  verified  protection.  
This was one of the earliest systems consciously built and 
certified  as  a  multilevel  system  that  internally  separated 
data of dramatically different sensitivities. 