pragmatic, challenge was getting the right tools in place
and executing the experiments in a time-efﬁcient manner.
We describe each of these challenges, and our approach
to meeting them, in more detail below.
5.1 Experimental Design Challenges and
Tools
Ideally, our idea of Web-based inference detection would
be tested on authentic documents for which privacy is a
chief concern. For example, a corpus of medical records
being prepared for release in response to a subpoena
would be ideal for evaluating the ability of our tech-
niques to identify sensitive topics. However, such a cor-
pus is hard to come by for obvious reasons. Similarly,
a collection of anonymous blogs would be ideal for test-
ing the ability of our techniques to identify individuals,
but such blogs are hard to locate efﬁciently. Indeed, the
excitement over the recently released AOL search data,
as illustrated by the quick appearance of tools for min-
ing the data (see, for example, [44, 4]), demonstrates the
widespread difﬁculty in ﬁnding data appropriate for vet-
ting data mining technologies, of which our inference de-
tection technology is an example.4
Given the difﬁculties of ﬁnding unequivocally sensi-
tive data on which to test our algorithms, we used in-
stead publicly available information about an individual,
which we anonymized by removing the individual’s ﬁrst
and last names.
In most cases, the public information
about the individual, thus anonymized, appeared to be a
decent substitute for text that the individual might have
authored on their blog or Web page.
All of our experiments rely on Java code we wrote
for extracting text from html, on calculation of an ex-
tended form of TF.IDF (see deﬁnition below) for identi-
fying keywords in documents and on the Google SOAP
search API [18] for making Web queries based on those
keywords.
Our code for extracting text from html uses standard
techniques for removing html tags. Because our experi-
ments involved repeated extractions from similarly for-
matted html pages (e.g Wikipedia biographies) it was
most expedient to write our own code, customized for
those pages, rather than retroﬁtting existing text extrac-
tion code such as is available in [3].
76
16th USENIX Security Symposium
USENIX Association
As mentioned above, in order to determine if a word
is a keyword we use the well known TF.IDF metric (see,
for example, [28]). The TF.IDF “rank” of a word in a
document is deﬁned with respect to a corpus, C. We
state the deﬁnition next.
Deﬁnition 1 Let D be a document that contains the
word W and is part of a corpus of documents, C. The
term frequency (TF) of W with respect to D is the num-
ber of times W occurs in D. The document frequency
(DF) of W with respect to the corpus, C, is the total num-
ber of documents in C that contain the keyword W . The
TF.IDF value associated with W is the ratio: T F/DF .
Our code implements a variant of TF.IDF in which we
ﬁrst use the British National Corpus (BNC) [27] to stem
lexical tokens (e.g. the tokens “accuse”, “accused”, “ac-
cuses” and “accusing” would all be mapped to the stem
“accuse”). We then use the BNC again to associate with
each token the DF of the corresponding stem (i.e. “ac-
cuse” in the earlier example).
there are open
As with text extraction from html,
source (and commercial) offerings
for calculating
TF.IDF based on a reference corpus. We did not, how-
ever, have a reference corpus on which to base our cal-
culations, and thus opted to write our own code to com-
pute TF.IDF based on the DF values reported in the BNC
(which is an excellent model for the English language as
a whole, and thus presumably also for text found on the
Web).
Our ﬁnal challenge was experimental run-time. Al-
though we did not invest time optimizing our text ex-
traction code for speed it nevertheless proved remark-
ably efﬁcient in comparison with the time needed to ex-
ecute Google queries and download Web pages. In addi-
tion, Google states that they place a constraint of 1, 000
queries per day for each registered developer on the
Google SOAP Search API service [18]. This constraint
required us to amass enough Google registrations in or-
der to ensure our experiments could run uninterrupted; in
our case, given the varying running times of our experi-
ments, 17 registrations proved enough. The delay caused
by query execution and Web page download caused us to
modify our algorithms to do a less thorough search for
inferences than we had originally intended. These modi-
ﬁcations almost certainly cause our algorithms to gener-
ate an incomplete set of inferences. However, it is also
important to note that despite our efforts, our results con-
tain some links that should have been discarded because
they either don’t represent new information (e.g. scrapes
of the site from which we extracted keywords) or don’t
connect the keywords in our query to the sensitive words
in a meaningful way (e.g. an online dictionary covering a
broad swath of the English language). Hence, it is possi-
ble to improve upon our results by changing the parame-
ters of our basic experiments to either do more ﬁltering
of the query results or analyze more of the query results
and require a majority contain the sensitive word(s).
We describe each experiment in detail below.
5.2 Web-based De-anonymization
As discussed in section 4 one of our goals is to demon-
strate how keyword extraction can be used to warn the
end-user of impending identiﬁcation. Our inference
detection technology accomplishes this by constantly
amassing keywords from online content proposed for
posting by the user (e.g. blog entries) and issuing Web
queries based on those keywords. The user is alerted
when the hits returned by those queries return their name,
and thus is warned about the risk of posting the content.
We simulated this setting with Wikipedia biographies
standing in for user-authored content. We removed
the biography subject’s name from the biography and
viewed the personal content in the biography as being
a condensed version of the information an individual
might reveal over many posts to their blog, for example.
From these “anonymized” biographies we extracted key-
words. Subsets of keywords formed queries to Google.
A portion of the returned hits were then searched for the
biography subject’s name and a ﬂag was raised when a
hit that was not a Wikipedia page contained a mention
of the biography’s subject. For efﬁciency reasons, we
limited the portion and number of Web pages that were
examined. In more detail, our experiment consists of the
following steps:
Input: a Wikipedia biography, B:
1. Extract the subject, N, of the biography, B, and
parse N into a ﬁrst name, N1, optional middle name
or middle initial, N0
1, and a last name, N2 (where
Nj is empty if a name in that position is not given
in the biography).5
2. Extract the top 20 keywords from the Wikipedia bi-
ography, B, forming the set, SB, through the fol-
lowing steps:
(a) Extract the text from the html.
(b) Calculate the enhanced TF.IDF ranking of
each word in the extracted text (section 5.1).
If present, remove N1, N0
1 and N2 from this
list, and select the top 20 words from the re-
maining text as the ordered set, SB.
3. For x = 20, 19, . . . , 1, issue a Google query on the
top x keywords in SB. Denote this query by Qx.
For example, if W1, W2, W3 are the top 3 keywords,
the Google query Q3 is: W1 W2 W3, with no
Hx be the set of hits
additional punctuation. Let
USENIX Association
16th USENIX Security Symposium
77
Identifiability of California individuals with Wikipedia 
biographies
Identifiability of Illinois individuals with Wikipedia Biographies
d
e
i
f
i
t
n
e
d
i
s
l
i
a
u
d
v
d
n
i
i
f
o
e
g
a
t
n
e
c
r
e
P
100
90
80
70
60
50
40
30
20
10
0
0
5
10
15
20
25
Maximum number of keywords used to identify an individual
0
5
10
15
20
25
Maximum number of keywords used to identify an individual
100
90
80
70
60
50
40
30
20
10
0
s
l
i
a
u
d
v
d
n
i
i
d
e
i
f
i
t
n
e
d
i
f
o
e
g
a
t
n
e
c
r
e
P
Figure 1: Using 20 keywords per person, extracted from each resident’s Wikipedia biography, the percentage of individuals who were identiﬁable
based on x keywords or less for x = 1, . . . , 20. The graph on the left shows results for the 234 biographies of California residents in Wikipedia
and the graph on the right shows the results for the 106 biographies of Illinois residents in Wikipedia.
returned by issuing query Qx to Google with the re-
strictions that the hits consist solely of html or text6
and that no hits from the en.wikipedia.org Web site
be returned.
4. Let Hx,1, Hx,2, Hx,3 ∈ Hx be the ﬁrst, second and
third hits (respectively) resulting from query Qx.7
For x = 20, 19, . . . , 1, determine if Hx,1, Hx,2 and
Hx,3 contain references to subject, N, by search-
ing for contiguous occurrences of N1, N0
1 and N2
(meaning, no words appear in between the words in
a name) within the ﬁrst 5000 lines of html in each of
Hx,1, Hx,2 and Hx,3. Record any such occurrences.
Output: SB, each query Qx that contains N1, N0
1 and
N2 contiguously in at least one of the three examined
hits, and the url of the particular hit(s).
We ran this test on the 234 biographies of California
residents, and the 106 biographies of Illinois residents
contained in Wikipedia. The results for both states are
shown in Figure 1 and are very similar. In each case, 10
or fewer keywords (extracted from the Wikipedia biog-
raphy) sufﬁce to identify almost all the individuals. Note
that statistics in Figure 1 are based solely on the output
of the code, with no human review.
We also include example results (keywords, url, biog-
raphy subject) in Figure 2. These results illustrate that
the associations a person has may be as useful for identi-
fying them as their personal attributes. To highlight one
example from the ﬁgure, 50% of the ﬁrst page of hits re-
turned from the Google query “nﬂ nicole goldman fran-
cisco pro” are about O. J. Simpson (including the top 3
hits), but there is no reference to O. J. Simpson in any of
the ﬁrst page of hits returned by the query “nﬂ francisco
pro”. Hence, the association of O. J. Simpson with his
wife (Nicole) and his wife’s boyfriend (Goldman) is very
useful to identifying him in the pool of professional foot-
ball players who once were members of the San Fran-
cisco 49ers.
PERFORMANCE. In our initial studies, there was wide
variation, from a few minutes to over an hour, in the total
time it took to process a single biography, B, depending
on the length of the Web pages returned and the num-
ber of hits. Hence, in order to efﬁciently process a suf-
ﬁciently large number of biographies we restricted the
code to only examining the ﬁrst 5000 lines of html in the
returned hits from a given query, and to only search the
ﬁrst 3 hits returned from any given query. With these
restrictions, each biography took around 20 minutes to
process, with some variation due to differences in biog-
raphy length. In total, our California experiments took
around 78 hours and our Illinois experiments took about
35 hours. Our experimental code does not keep track
of the number of queries issued per registration and do-
ing so may yield better performance because switch-
ing between registrations occurred only upon receiving
a Google SOAP error and so caused some delay.
Our code was not optimized for performance and im-
provements are certainly possible. In particular, our main
slow down came from the text extraction step. One im-
provement would be to cache Web sites to avoid repeat
extractions.
78
16th USENIX Security Symposium
USENIX Association
http://www.utexas.edu/features/archive/2004/election policy.html
http://www.pbs.org/wgbh/amex/reagan/peopleevents/pande08.html
http://www.sourcewatch.org/index.php?title=Edwin Meese III
http://www.brainyhistory.com/years/1997.html
http://www.amazon.com/Kung-Fu-Complete-Second-Season/
dp/B0006BAWYM
http://www.voicenet.com/˜lpadilla/pearl.html
http://www.indianahistory.org/pop hist/people/ade.html
Name of Person
Ronald Reagan
Caspar Weinberger
Edwin Meese
O. J. Simpson
David Carradine
Jimmy Doolittle
George Ade
campaigned soviets