to the population of na¨ıve users (that is, those users with no
trust values or identical trust values).
Let P be a random (cid:96)-hop connection through the onion
routing network, with Pi the ith router. Let S be the source
(user) and ∆ the destination. Let p be a connection made
by user u. Let AR ⊆ R ∪ D be the routers and destina-
tions compromised by the adversary. Let the path positions
observed by the adversary be O = {i : pi ∈ AR ∨ pi−1 ∈
AR ∨ pi+1 ∈ AR ∨ (i = (cid:96) ∧ ∆ ∈ AR)}. Let q1 be the proba-
bility of u making a connection consistent with the routers
in p observed and not observed by AR:
q1 =
„ 1
Y
«Y
X
n + 1
i /∈O
r∈R\AR
P r[Pi = pi|S = u] ·
i∈O
P r[Pi = r|S = u].
Let q2 be the probability that a na¨ıve user other than u
makes a connection consistent with AR and p:
( 0
“ n
”
n+1
q2 =
m−(cid:96)|R\AR||{i /∈O}|
if p1 ∈ AR
otherwise
These expressions follow from the facts that each user is
equally likely to be the source of a given connection and that
na¨ıve users choose routers uniformly at random.
Finally, let Y be the conditional probability that the source
of a connection is u:
Y (AR, p) =
q1
q1 + q2
.
Y depends on the routers and destinations observed by
the adversary and the probability distribution with which u
selects a path. The routers in A depend on the trust values
of the routers, and the destinations it observes are ﬁxed. Y
therefore is probabilistic with a distribution that depends on
the path distribution of u and the trust values. The trust
values are given, but we can choose the path distribution of
u to optimize the distribution of Y .
There are several plausible criteria on the distribution of Y
to use when optimizing the path distribution: the expecta-
179tion E[Y ], a weighted expectation E[f (Y )] for some weight
function f , the probability P r[Y ≥ α] for some α ∈ (0, 1],
and so on. Such criteria might lead to diﬀerent optimal
path distributions, because distributions of Y are not neces-
sarily comparable (one may not stochastically dominate the
other). We choose the expected value of Y as our criterion.
4.1.3 A Downhill Algorithm
Given the above framework, how can the users utilize trust
to better protect their connections? As noted, more trust
means lower chance of compromise, hence lower chance of
observing the user, adjacent routers in the path, or the des-
tination. On the other hand, a more trusted router is more
likely to be associated with the user by someone who knows
what adversary the user is trying to avoid. This suggests
a routing algorithm in which the user chooses routers from
sets with a decreasing minimum trust threshold (or, equiv-
alently, increasing maximum risk of compromise). As these
sets increase in size, they are more likely to contain a com-
promised router, but what that compromised router will see
is also less identifying to the user.
Let (cid:96) be the length of the path. Let the acceptable level
of risk in the ith hop be ρ : [(cid:96)] → [0, 1] such that ρ(i) <
ρ(i + 1). The set of routers at the ith level is the “trust set”
Ti = {r ∈ R : c(r) ≤ ρ(i)}. The user chooses the ith hop
independently and uniformly at random from Ti.
We can assume that the ﬁnal trust set T(cid:96) includes all of
the routers R. The destination links are observed in this
case, so we can’t give the adversary any more observations
by including all of R in a ﬁnal trust set. And doing so
may prevent the adversary from observing a trust set that
is smaller than R and thus more associated with the user.
We want to set the parameters (cid:96) and ρ(i) to minimize
the expected posterior probability E[Y ]. The straightfor-
ward algorithm for minimizing the expected value simply
iterates over all possible path lengths and ways to set the
trust thresholds at each path length. Let λ ≤ m be the
maximum allowed path length. In practice, we only want to
consider path lengths up to the point at which the latency
becomes too high. The Tor network, for example, uses path
lengths of 3. Let ν be the number of distinct trust values
among the routers. The number of iterations is then O(λνλ).
For each iteration, expected value is determined by cal-
culating the posterior probability the adversary assigns to u
for each possible set of compromised routers and each path:
X
Y
Y
X
c(a)
(cid:96)Y
E[Y ] =
AR⊆R
a∈AR
(1 − c(a))
a /∈AR
P r[Pi = pi|S = u]Y (AR + d, p).
p∈R(cid:96)
i=1
Calculating the expectation from this expression involves
summing over all 2m possible adversary subsets, which takes
far too long for any reasonably-sized anonymity network.
However, in practice we do not expect the user to be able
to make more than a handful of meaningful distinctions be-
tween routers on the basis of trust. If the number ν of dis-
tinct trust values is small, we can speed up the computation
of the expectation by using the fact that our path distribu-
tion chooses all routers with the same trust value with equal
probability.
4.1.4 Analyzing The Downhill Algorithm
We have calculated the optimal thresholds and the result-
ing expected posteriors for several plausible situations using
a user population of n = 1000. The results appear in Ta-
ble 1. They are given next to the anonymity of two other
path algorithms for comparison:
i) the user chooses each
hop uniformly at random from the most-trusted nodes only
and ii) the user chooses each hop uniformly at random from
all routers. We also compare the results to a lower bound on
E[Y ] of cmin = minr c(r). (The ﬁrst node is compromised
with probability at least cmin, and Y = 1 in this case.) The
situations we consider involve three diﬀerent trust values, so
we consider path lengths up to three. In Tables 1(b) and
1(c), the optimal thresholds skip one possible trust value
and only use a two-hop path.
Table 1: Examples of optimal thresholds
(a) Small trusted and untrusted sets, for example when the
user has information about a few good routers and a few
bad routers, and has little information for the rest.
# Routers
Prob. of compromise
Optimal thresholds
5
0.01
0.01
1000
0.1
0.1
10
0.9
0.9
Downhill Trusted Random Lower bnd.
E[Y ]
0.0274
0.2519
0.1088
0.01
(b) Small trusted, medium semi-trusted, large untrusted
sets, for example when the adversary is strong, but the
user and her friends run some routers.
# Routers
Prob. of compromise
Optimal thresholds
5
.001
0.05
50
0.05
0.5
1000
0.5
Downhill Trusted Random Lower bnd.
E[Y]
0.0550
0.1751
0.4763
0.001
(c) Equally large trusted, semi-trusted, and untrusted sets,
for example when the user assigns trust based on geo-
graphic regions.
# Routers
Prob. of compromise
Optimal thresholds
350
0.1
0.1
350
0.5
0.9
350
0.9
Downhill Trusted Random Lower bnd.
E[Y]
0.1021
0.1027
0.5000
0.1
The table shows that using trust levels improves anonymity
in each case against random route selection by factors of at
least 4.0 and as much as 8.6. Similarly, we see improvements
in each against the trusted-router strategy, from just a slight
increase in the third situation when there are relatively many
trusted routers to over a factor 3.1 improvement in the sec-
ond situation when there are many untrusted routers. We
can also see that in the ﬁrst and third situations we achieve
anonymity on the order of the best possible. Interestingly,
we notice that in the ﬁrst situation, using highly-trusted
routers exclusively is worse than randomly choosing routers,
but only because there are few highly-trusted routers. Using
the downhill algorithm avoids that problem.
Figure 1 examines the eﬀect of varying some of the trust
values in the situation of Table 1(a). It shows the eﬀect of
180Figure 1: Anonymity when varying trust values of Table 1(a).
But there is a more immediate concern. In the above calcu-
lations, we have assumed that the trust value assigned to a
router by the user reﬂects the correct a priori probability of
compromise of that router by the relevant adversary. What
if the user was not correct in her assignment of trust?
4.1.5 Correctness and Accuracy of Trust Assignments
To assign trust values, the user must rely on some outside
knowledge. This external information might include knowl-
edge of the organizations or individuals who run routers—
including both knowledge of their technical competence and
the likelihood that those running a given router would intend
to attack the user. Trust values might also be aﬀected by
computing platforms on which a router is running, geopo-
litical information about the router, knowledge about the
hosting facility where a router might be housed or the service
provider(s) for its access to the underlying communications
network, and many other factors.
The process of assigning trust values is clearly uncertain,
and we cannot expect the user to correctly assign values to
all routers. Therefore, we consider the eﬀect of errors in
the believed trust values on the user’s anonymity. First, we
derive a bound on the eﬀect that error in the trust value for
a single router has on our anonymity metric, E[Y ]. Second,
we calculate the eﬀect of a couple of types of errors in a
speciﬁc scenario.
Let r ∈ R be some router with an error of  in its assumed
trust value. Let E[Y ] be the expected posterior probability
when the probability that r is compromised is c(r) + . Let
Si = Ti\Ti+1. Let k1 be the ﬁrst path position non-adjacent
to the user for which r can be chosen, that is, for r ∈ S1,
let k1 = 2, otherwise let k1 be such that r ∈ Sk1 . Let
k2 be such that r ∈ Sk2 . Let µi be expected number of
uncompromised routers in Si given that r is uncompromised.
Let µmin = min1≤i≤(cid:96) µi. Let P be a random path chosen by
u according to the downhill algorithm. Let the probability
that r is chosen i times in P be
πr(i) = P r[|{j : Pj = r}| = i]
! k1+i−1Y
≤
(cid:96)
i
(cid:96)Y
(1 − 1/|Tj|).
(1)
(2)
1/|Tj|
j=k1
j=k1+i
Let the ratio of the probability of u choosing a given router
s ∈ Ti at the ith step to the probability of a given na¨ıve user
doing the same be αi = m/|Ti|.
Figure 2: Anonymity when increasing number of
high-trust nodes and decreasing number of medium-
trust nodes in Table 1(a).
varying just high trust values, keeping the others at their
original value, and the eﬀects of the same process with the
medium and low trust values. We can see that the change
in the anonymity E[Y ] is roughly linear in the change in the
trust values, and that the rate of the change increases as the
trust change aﬀects hops closer to the source. Furthermore,
we see that choosing only trusted routers performs badly
when the largest trust value isn’t high, random selection
performs badly when the average trust value isn’t high, and
the downhill-path algorithm always performs better than
both and often much better.
Figure 2 examines the eﬀect of trading oﬀ the number
of high-trust and medium-trust nodes in the situation of
Table 1(a). That is, it shows the variation in anonymity
for that situation when there are x high-trust nodes and
1005 − x medium-trust nodes. The graph shows that using
the downhill-trust or trusted-only algorithms quickly ben-
eﬁt from having larger numbers of high-trust nodes. This
is because a selection of these is likely to be observed but