When the application executes a syscall, it will trap into
the syscall frontend (Sec. V-C). This component is the entry
point into both the monitor and the “kernel” (ring 0 code) of
the virtual environment. The syscall frontend also manages all
accesses to application state, such as its address space.
The frontend forwards all syscall events to the variant
manager (Sec. V-D), which is responsible for synchronizing
all variants and enforcing the security policies. The variant
managers communicate with each other using a ring buffer
similar to that proposed by Hosek and Cadar [22]. Speciﬁcally,
one of the variants (the leader) performs all of the actual
syscalls and sends the corresponding events to the other
variants (followers), who consume the events in their own time.
The followers only execute a small set of these syscalls as well
(e.g., memory management) as most I/O should happen only
once (e.g., sending data over a socket). Unlike traditional MVX
systems, the variants can run asynchronously most of the time,
removing the great performance bottleneck of running variants
in lockstep. However, unbridled asynchronicity is not safe. It
would allow one variant to achieve arbitrary code execution
with calls like exec, or information disclosure with calls
like write. Instead, we use the known distinction between
security-sensitive and non-security-sensitive syscalls [42], [43]
and enforce selective lock-step execution, where the set of
sensitive calls varies depending on the security policy.
The variant manager is also responsible for deciding when
a syscall should really be executed (leader) or when the
results should simply be copied (followers). When the variant
manager decides to execute a syscall, it sends it to the syscall
backend (Sec. V-E). In a naive implementation, the backend
would simply forward all syscalls to the real kernel. However,
each syscall in Dune requires a costly VM exit. To reduce
these costs, we implemented a set of (memory management
and getpid-like) syscalls directly in our monitor. Further
libOS-style optimizations are also possible—for instance, by
using a userspace network stack such as IX [29], or by batching
syscalls [44].
The variant manager uses
the namespace manager
(Sec. V-F) to ensure all information available to variants is the
same (including PIDs, ﬁle descriptors, and timing information),
and ﬁnally the detector (Sec. V-G) to semantically compare the
execution of the variants for divergence.
V. MvArmor: FAST AND SECURE MVX
We now describe each of MvArmor’s components in detail.
A. Variant generator
A fundamental question in MVX is to what extent the
variants should differ. Unconstrained variation makes it impos-
sible to detect attacks from divergence, as everything may be
different. Conversely, insufﬁcient variation is also undesirable,
as there may not be any divergence for an attack. Fortunately,
for memory errors the straightforward solution is to vary the
address space layout and keep everything else the same, as
these differences should normally not affect program execution
App1
App2
. . .
Appn
syscall
Variant
generator
Syscall frontend
Security manager
Dispatch syscall
Variant manager
Execute syscall
Generate policy
Detector
Syscall backend
Namespace manager
vmcall
Kernel
Fig. 2. Overview of all MVX design components.
but will make a difference in the case of malicious memory
actions. In this section, we identify several techniques that
offer strong protection and detection against different classes
of memory error exploits and we detail the corresponding
implementation strategy in our current MvArmor prototype.
For our analysis, we assume the now common PIE binary
organization [45], but our design can, in principle, also handle
non-PIE binaries by marking static program segments as non-
relocatable and gracefully reduce security guarantees.
First of all, by using non-overlapping address spaces across
variants (pioneered by [23]), any absolute spatial attack (i.e.,
attack relying on absolute code/data addresses) is already
rendered ineffective. By ensuring that memory pages do not
overlap across variants, a pointer can only be valid in at most
one variant at a time and will thus deterministically crash all
others. This already stops common code-reuse attacks such
as ROP [37] and information disclosure attacks such as JIT-
ROP [46], as they rely on the absolute position of memory
pages. In fact, even any other attempts (e.g., buffer overreads)
to disclose code or data pointers will also cause divergence,
as different values will be leaked to the attacker at the syscall
level (e.g., over a socket) by construction. To enforce non-
overlapping address spaces across variants, we randomize each
variant using ASLR and then constrain ASLR not to reuse
address ranges across variants. Since our MVX system resides
in ring 0 of the virtualized environment, it has full control over
the page tables, making ASLR modiﬁcations simple. MvArmor
implements this technique for all the memory regions (i.e.,
code, data, stack, etc.) in each variant.
To also deterministically stop relative spatial attacks (i.e.,
attacks relying on relative code/data addresses), our variant
generation strategy must be able to provide strong guarantees
against buffer overﬂows/underﬂows and partial pointer over-
writes. We observe that, for this purpose, our strategy must
simply ensure that offsets between memory objects are non-
overlapping. For example, if the size between objects on, say,
the heap in a follower is as large as the entire (normal and
compact) heap in the leader, any offset added to a pointer can
only be valid in one of these variants. In other words, this
design ensures non-overlapping offset spaces across variants,
rendering all the relative spatial attacks ineffective. MvArmor
implements this novel technique for all the heap objects, by
434
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:26 UTC from IEEE Xplore.  Restrictions apply. 
t
o
o
r
-
n
o
n
X
M
V
3
g
n
i
r
0
g
n
i
r
MVX app
syscall
libDune
MvArmor
vmcall
Dune module
Kernel
App
syscall
r
i
n
g
3
V
M
X
r
o
o
t
r
i
n
g
0
Fig. 3. Control ﬂow of syscalls with and without Dune.
using the standard “compact” allocator in the leader and a
custom “sparse” allocator in the followers. Extending such
guarantees to all the other memory objects is, in principle,
possible, but source-level information is generally necessary to
accurately decouple stack [47] and data [48] objects—although
binary-level approximations are at times possible [49].
While the strategies described thus far can provide de-
terministic protection against all the spatial attacks, they are
alone insufﬁcient
to stop temporal attacks (e.g., use-after-
free exploits). Unfortunately, ensuring deterministic protection
guarantees against generic temporal attacks is not practical
without source-level information [50]. A practical binary-level
alternative is to ensure probabilistic temporal safety guaran-
tees. In our design, this is done by using different (randomized)
memory allocators across variants and, to further limit the
attack surface, by approximating type-safe memory reuse [51]
at the binary level. MvArmor enforces probabilistic temporal
safety for all the heap objects, randomizing the standard allo-
cator with random inter-object gaps in the leader. In addition,
MvArmor overapproximates type-safe memory reuse using per-
size memory pools in our custom allocator in the followers (but
much less conservative binary-level approximations based on
allocation-time backtraces are also possible [51]).
While the implementation of all the proposed protection
techniques can introduce signiﬁcant overhead when all com-
bined together on a single variant, their overhead can, in most
cases, be completely masked across variants with our MVX
design. In MvArmor, followers are faster than the leader, as
they do not execute most syscalls and thus waste several
cycles waiting for the leader. Our measurements show that,
for our baseline MvArmor implementation (i.e., without any
protection enabled) on (I/O bound) server applications, the
followers spend around 4,000 cycles on average per syscall
waiting for the leader. Especially when syscalls do not require
lockstep behavior, the idle periods leave sufﬁcient time for
the followers to spend more time in more expensive alloca-
tor abstractions implementing our protection techniques. This
strategy provides strong security guarantees while reducing the
run-time overhead of the end-to-end solution.
B. Security manager
The security manager generates policies that allow users
to make trade-offs between security and performance. Specif-
ically, a policy speciﬁes whether each syscall is considered
non-sensitive (event-streaming, meaning the leader can execute
it without synchronization), or sensitive (requiring lockstep
execution with other variants).
Security policies specify behavior at the level of the whole
system, individual syscalls, or even speciﬁc arguments (e.g.,
“If the execute bit in a permissions ﬂag is set then. . . ”). In
MvArmor, we propose the following policies for each of the
aforementioned classes of attacks (but others are possible):
that are able to leak data (e.g., write).
mprotect/mmap with execute permissions set.
• Code execution: enforce full checks on execve and
• Information disclosure: enforce full checks on I/O syscalls
• Comprehensive: full checks on all syscalls.
In practice, the Code execution policy performs as efﬁ-
ciently as a policy where no syscalls are considered sensitive,
since the syscalls considered by the Code execution policy are
rarely executed in most applications.
The Comprehensive security policy, in turn, is useful to
provide a generic catch-all strategy (and a lower bound on
performance) but, given a target threat model, may provide
comparable security to more tailored policies such as Code
execution and Information disclosure. The key insight is that
such security policies may delay detection of failed attacks,
but they do deterministically and immediately stop successful
attempts for all the attacks considered in the threat model.
C. Syscall frontend
When the application executes a syscall instruction, the
execution will trap from ring 3 into ring 0—kernel space.
By running the application and the monitor in a virtualized
environment, all syscalls will trap into the monitor instead
of the actual kernel. MvArmor is based on Dune [28], which
leverages virtualization to provide applications with access to
privileged CPU features in a safe way. For this purpose, Dune
relies on Intel VT-x extensions to allow a core to temporarily
switch from the normal kernel (VMX root) into virtualized
mode (VMX non-root) via the Dune hypervisor. Dune sets up
ring 0 code for both VMX root and non-root mode, as shown in
Figure 3. Since our monitor runs in privileged mode, it can also
access other features, such as page tables and interrupts. While
the same effect could be achieved by a kernel module or by
modifying the kernel directly, MvArmor completely separates
the monitor from the rest of the system, with no systemwide
TCB increase.
The syscall frontend receives syscall traps from libDune
and forwards them to the variant manager. In addition, the
frontend is responsible for access to the application state, such
as reads and writes to its address space.
Not all syscalls incur a trap on modern Linux systems;
in every application, the kernel sets up a shared library (the
virtual dynamic shared object—i.e., vDSO), which contains
code to execute a selection of syscalls without trapping into
the kernel. Using Dune, we can still intercept vDSO calls, by
mapping our own code containing syscall instructions in
place of the original vDSO.
435
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:26 UTC from IEEE Xplore.  Restrictions apply. 
D. Variant manager
Upon receiving a syscall event from the frontend,
the
variant manager synchronizes with the other variants. Every
process has a ring buffer it shares with the respective processes
in other variants. Upon completion of a syscall, the leader
pushes it
into the ring buffer together with its arguments
and return value. The followers compare their arguments to
those of the leader, and either execute the syscall themselves,
or use the return value provided by the leader. Speciﬁcally,
the variant manager has a per-syscall table to determine the
appropriate behavior. Certain syscalls should execute only once
(e.g., socket-related syscalls), while others should execute in
every variant (e.g., memory management calls). For the former,
the followers simply copy the return value of the leader.
The ring buffers resemble those in Varan [22] and provide
efﬁcient communication without locking. We use atomic op-
erations to update the ring buffer entries and busy-waiting to
consume them. As syscalls like select and epoll_wait
may block for a long time, followers stop busy-waiting after
some time and go to sleep instead. Doing so is expensive
due to the VM exits required for both sleep and wake-up
calls. Assuming there is no shortage of cores, a more efﬁcient
solution is to sleep using Intel’s monitor/mwait instructions
as they incur no VM exit. Because our Dune-based virtualized
environment runs in privileged mode, it can trivially use them
where normal applications cannot.
As mentioned earlier,
the security policy determines
whether each syscall should run in lockstep, in which case
the leader waits for all followers after pushing the arguments
into the ring buffer. The followers then compare the syscalls
as usual and then pause while the leader ﬁnishes the syscall.
Doing so for every syscall (most conservative security policy)
has a higher performance impact.
For multi-threaded applications, we force the followers to
adhere to the order of syscalls of the leader. This strategy seeks
to prevent divergent behavior due to non-deterministic schedul-
ing decisions. This loose form of deterministic multithreading
(DMT) was shown to be generally sufﬁcient for previous MVX
systems [32], [22]. Full DMT semantics could be used if issues
do occur (such as divergence because of benign data races), at
the cost of larger overhead [52], [53], [54].
E. Syscall backend
When a variant needs to execute a syscall, it forwards
the call to the syscall backend. Forwarding syscalls to the
kernel requires costly VM exits, so the syscall backend tries
to execute the syscall locally when possible. This is currently
implemented for memory management syscalls, but could be,
for example, extended to include userspace network stacks
such as IX [29]—which is also based on Dune. Besides
eliminating VM exits, userspace network stacks also improve
the overall performance of the application.
MVX monitors can be susceptible to time-of-check-to-
time-of-use (TOCTOU) attacks, where an attacker modiﬁes the
arguments of a syscall in memory from a different thread after
the arguments are checked by the monitor but before they are
read by the kernel. This works because the arguments passed
to the syscalls are usually pointers to buffers or structures
in the address space of the application, copied separately by
the monitor (for checking) and the kernel (for execution). We
solve this by directly passing the pointers to the copied data
structures (in the monitor) to the kernel. Because no additional
copying is required, this introduces no performance overhead.
F. Namespace manager
The namespace manager ensures the variants do not diverge
accidentally by eliminating all variant-speciﬁc information.
For instance, if variants were to have access to their kernel-
assigned PIDs or timing information, they may (directly or
indirectly) use such data in a conditional, leading to divergent
behavior. The namespace manager therefore assigns virtual
PIDs and TIDs to every process and thread using a hierarchical
structure: when a variant creates threads in quick succession,
these must get the same virtual TID in all variants, regardless
of the order they actually appear on the system or the thread
that executed its clone operation earlier.
We similarly virtualize ﬁle descriptors, as only the leader
has access to all of them. For instance, followers do not
have access to sockets or ﬁles opened as writable. Since
the kernel assigns ﬁle descriptors in an incremental fashion
per process and the followers open fewer ﬁles (e.g., read-
only ﬁles) than the leader, these numbers start to diverge.
The namespace manager therefore maintains a mapping of
virtualized ﬁle descriptors to real (per-variant) ﬁle descriptors.
The same holds for epoll-related identiﬁers, including the user
data ﬁeld. The epoll_wait syscall returns user-deﬁned data
previously registered when a socket has an I/O event. Since
these values can be pointers (that differ per variant), they have
to be mapped back to the socket and then to the variant-speciﬁc
user data that should be returned for that socket.
Timing information should also not differ among variants,
as this is often used for logging or seeding the random number
generator. Since MvArmor has full control over the page tables
of the application, it can easily intercept all vDSO syscalls.
While not commonly used, we also disable the rdtsc in-
struction so that it traps into the monitor.
We ensure determinism in random number generation by
allowing only the leader to open ﬁles like /dev/random.
Pseudorandom number generators are generally seeded with
information already virtualized by the namespace manager and
require no additional effort to work correctly. We similarly
limit access to the /proc ﬁle system to the leader. Without
binary instrumentation, there is no easy way of interposing
rdrand instructions. By disabling the corresponding bit in
the cpuid implementation of the hypervisor, most normal
applications and libraries will not use it (e.g., OpenSSL). While
we have not observed the need to check on this further, we
could also conﬁgure the virtual environment to trap into the
hypervisor when the instruction is executed (via a bit in the
control structure of the virtual environment).
G. Detector
Since a syscall can take no more than six arguments,
many calls expect pointers to data structures which hold
more information (e.g., a buffer or struct). For a full
comparison between variants, the monitor therefore performs
a deep semantic copy and comparison of such arguments [23],
436
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:26 UTC from IEEE Xplore.  Restrictions apply. 
[32]. In MvArmor, the detector component performs both of
these functions.
H. Implementation
MvArmor consists of a library implementing all the com-
ponents in our design except for the frontend and backend.
We developed two implementations of these components: our
high-performance hardware-virtualization approach using the
Dune sandbox and a ptrace implementation for development
and debugging. These implementations call our shared library
for every syscall and expose several functions such as how
to access the monitored applications’ address space and how
to allocate memory across variants. The library itself consists
of around 5,000 lines of C code, whereas the frontends and
backends include around 500 lines of C code each.
The Dune sandbox, which is used to implement the default
frontend and backend of MvArmor, allows for arbitrary appli-
cations to run in Dune. The sandbox loads a given binary using
its own loader. It also implements bounds checking on any
pointer passed to a syscall to prevent sandboxed applications
from accessing ring 0 state such as the sandbox itself, the Dune
library, or the monitor. We slightly modiﬁed both Dune and
the sandbox to meet our requirements for the monitor, such as
security ﬁxes and more callbacks.
To implement
the protection techniques discussed in
Sec. V-A, we used a modiﬁed version of libumem2, a Linux
userspace port of the Solaris slab allocator [55], [56]. This
implementation served as a basis for our custom “sparse”
allocator. In particular, by limiting the number of objects
allowed (i.e., 1 object) per slab and adding padding (i.e.,
the leader’s maximum heap size) to every slab, we enforce
non-overlapping offset spaces between leader and followers.
By preserving the natural per-size pooling architecture of
libumem, we overapproximate type-safe memory reuse. Fur-
thermore, since we want to retain the standard (randomized) al-
locator in the leader (to preserve security, but also performance
guarantees in the slower leader), we assume both the standard
and our custom allocator as trusted to prevent the monitor from
detecting divergence caused by the different allocators (e.g.,
different syscalls to map memory). Note that while our custom