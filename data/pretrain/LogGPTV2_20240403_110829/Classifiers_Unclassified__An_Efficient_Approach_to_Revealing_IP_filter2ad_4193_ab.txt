Inputs:
T : the original trace
R: classiﬁcation
L: list of potential matching ﬁelds
Output:
M : set of combinations of matching ﬁelds
function LocateMatchingFields(T, R, L)
Initialize number of ﬁelds k to 1
M = Set()
while k ≤ Length(L) do
for F in combination(k,L) do
T (cid:48) = copy(T )
for f ield in F do
T (cid:48) = permute(f ield)
R(cid:48) = classiﬁcation(T (cid:48))
if R(cid:48) (cid:54)= R then
M .add(F )
k += 1
Return M
We assume that a matching rule applies to a ﬁeld,
i.e., a region of a packet identiﬁed in the previous step.
However, this alone does not tell us the matching rule
because it is not speciﬁc. For example, if the ﬁeld con-
tains Host: prefix.netflix.com, it does not neces-
sarily imply that the matching rule is “First ﬁnd the
Host ﬁeld, then ﬁnd any string that contains netﬂix ”
(e.g., the rule could also be “ﬁnd any Host ﬁeld that
ends with netﬂix.com”).
To reveal the precise matching rule, we conduct tests
that randomize and otherwise alter subsets of content in
each ﬁeld. More generally, our approach is to permute
content in matching ﬁelds to test hypotheses about the
minimal set of content in matching ﬁelds that reliably
triggers classiﬁcation for a speciﬁc application (see Al-
gorithm 1). This entails replacing content with random
bytes, as well as adding and removing content. Be-
cause these generated ﬂows include traﬃc from multiple
sources of content, we refer to them as FrankenFlows.
2.3.1 Building a FrankenFlow
To build a FrankenFlow, we start with a base ﬂow
that is associated only with the application-layer pro-
tocol, i.e., HTTP or HTTPS. To do so, we build a ﬂow
that contains a dummy value for every known match-
ing ﬁeld for a classiﬁer (revealed by the analysis in §2.2),
where each ﬁeld’s value contains random content. For
example, a base ﬂow for HTTP traﬃc is a valid GET
request and response pair where all the HTTP header
values are replaced with random content:
GET {random text} HTTP/1.1
Host: {random text}
User-Agent: {random text}
...
241Likewise, a base ﬂow for HTTPS content is a TLS
handshake with any SNI and server-certiﬁcate ﬁelds re-
placed with random content. We then automatically
generate new FrankenFlows by replacing dummy values
with payloads from matching ﬁelds in observed network
traces.
2.3.2 Extracting Matching Rules
After isolating ﬁelds used for classiﬁcation, we further
randomize substrings of matching ﬁelds to isolate the
portions of ﬁeld triggering matching rules. Based on
the resulting tests, we construct a minimal multi-level
matching rule (i.e., a rule that contains the minimum
number of ﬁelds to trigger a match) that is consistent
with our observations.
For example, consider qq.com downloads. First, we
observe that it is an HTTP GET request, a feature shared
by many applications. Second, we ﬁnd that the Host
ﬁeld is used for matching. Next, we modify the Host
ﬁeld by adding, removing, and replacing ﬁeld values
with random strings. During this process, we ﬁnd that if
dl.xyz.qq.com is replaced with random.qq.random, it
is classiﬁed as QQ, and if replaced with dl.xyz.random,
then it is classiﬁed as generic “HTTP”. Last, if we re-
place the string with random.dl.random.qq.random or
random.qq.random.dl.random, it is classiﬁed as QQ
Download (a diﬀerent class from “QQ”). We therefore
infer that classiﬁer rule is: 1) HTTP GET request, 2) Host
header contains *dl*.qq.* or *.qq.*dl*.
To determine the impact of keyword location on clas-
siﬁcation, for each line in the HTTP header we ran-
domly added, moved, removed, and replaced bytes both
inside and outside of the identiﬁed keyword region.
These tests reveal rules that match on simple keywords
anywhere in a ﬁeld, only at the beginning of a ﬁeld, or
only at the end of a ﬁeld.
2.3.3 Rule Prioritization
Our methodology also reveals how matching rules
are prioritized when a ﬂow matches multiple rules si-
multaneously. To determine the priority of matching
rules for diﬀerent matching ﬁelds, we simply build all
combinations of FrankenFlows with diﬀerent matching
ﬁeld content, and determine the relative priority of each
rule by inspecting the classiﬁcation result. For exam-
ple, if a FrankenFlow contains Host: netflix.com and
User-Agent: Pandora... and is classiﬁed as Netﬂix
regardless of the order the ﬁelds appear in the ﬂow, then
we conclude that Netﬂix’s rule has a higher matching
priority than Pandora’s. Otherwise we infer that order
determines matching priority.
For the case of multiple matches in the same ﬁeld
(e.g., Host: netflix.youtube.com), we explore all
combinations of matching strings, by concatenating
each matching string into one ﬁeld value in a Franken-
Flow. When the classiﬁer selects one application that
matches, we then determine whether the position of the
corresponding matching string matters (e.g., was the
application selected because the matching ﬁeld was the
ﬁrst to appear in the FrankenFlow?).
Speciﬁcally, we move the matching string to the end
of the concatenated string.
If the classiﬁcation result
changes, then order also impacted rule matching (e.g.,
there is a tie-breaker based on position); otherwise, we
know that order is not a factor. Once we determine the
impact of position on classiﬁcation for an application,
we remove its string from the FrankenFlow and repeat
these steps for the application that is classiﬁed next.
2.4 Efﬁciency
An important property for our methodology is eﬃ-
ciency, both in terms of time and data usage required
to learn matching rules. We now evaluate this, and dis-
cuss an optimization that improves eﬃciency.
In addition to FrankenFlow optimizations, we lever-
age the observation that classiﬁcation occurs using only
the ﬁrst two packets exchanged between client and
server for HTTP/S traﬃc for the three devices we study.
As a result, we need only conduct tests using only the
ﬁrst packet sent by a client and server. Note that if our
assumption is incorrect (which is not the case for any
devices we tested), we simply continue to replay larger
fractions of ﬂows until we are able to identify classiﬁca-
tion rules.
This approach signiﬁcantly reduces the data and time
needed for each test. For example, the size of a typical
streaming video traﬃc replay is reduced from 30 MB
to just 2 KB in our testbed (where we have immediate,
ground-truth classiﬁcation results). In our testbed, we
tested all combinations of FrankenFlows using a single
client and server in 14 hours.
In contrast, the na¨ıve approach of permuting every
bit of two 1460 B packets (one for client request and
one for server response) would require 2n tests, where
n is the number of bits (23,200). Suﬃce it to say this
number is enormous. Even if we were to permute only
80 bits of a ﬁeld value, it would require 1.2x1024 tests,
requiring ≈ 1016 years if each test takes one second.
Outside our testbed, we detect T-Mobile’s Binge On
by checking for zero-rated traﬃc against our data plan,
as done previously [8]. Here, we use 10 KB Franken-
Flows, to avoid attributing data charges to background
traﬃc. Identifying the matching rules for an application
in Binge On required on average 400 KB of data.
3. EXPERIMENTAL RESULTS
We now present results from our detailed look into
the traﬃc-shaping device in our testbed, as it contained
a large number of matching rules and provided us with
ground-truth classiﬁcation results.5 In the next section,
we summarize results from two other devices.
Table 1 lists the application categories our device de-
tected and the number of matching rules we triggered in
5This device vendor is consistently identiﬁed as one of the key
companies in the global DPI market [4].
242Application category Number of examples
Streaming Applications
Web Applications
File Transfer
VoIP
Instant Messaging
Games
Mail
Security
P2P
33
32
10
9
7
5
5
2
1
Table 1: Categories of applications detected by our de-
vice using test traﬃc gathered from user traces. Our traces
provide good coverage across a variety of application types.
Header
URI
Host
User-Agent
Content-Type
Example Value
site.js?h={...}-nbcsports-com
User-Agent: Pandora 5.0.1 {...}
Content-Type: video/quicktime
Host: www.netﬂix.com
Application
NBC Sports
Netﬂix
Pandora
QuickTime
Table 2: HTTP matching ﬁelds and examples of applica-
tions classiﬁed by them. Matching keywords are in bold
font.
each category. The device identiﬁed 104 diﬀerent classes
of traﬃc, covering 9 of the 13 categories it supports.6
3.1 Matching Fields
In this section, we identify the ﬁelds used for match-
ing rules in the packet shaper. Recall that our Franken-
Flows are generated using HTTP/S traﬃc, so our re-
sults only apply to these protocols.
HTTP.
We ﬁnd that this device ﬁrst identiﬁes
HTTP traﬃc by looking for a request that starts with
GET, then checks for application-speciﬁc content in the
following headers: URI, Host, User-Agent (in the GET
request) and Content-Type (in the GET response). Ta-
ble 2 shows examples of matching ﬁelds. We note that in
many cases, it is trivial to modify these ﬁelds to avoid
classiﬁcation, indicating that matching ﬁelds are not
particularly resilient to adversarial behavior. Further,
we will show in the next subsection that many of these
rules can lead to false positives.
HTTPS. When applications use HTTPS, the en-
crypted TLS connection prevents the device from in-
specting HTTP headers. In this case, the device iden-
tiﬁes the corresponding application by matching text
anywhere in the ﬁrst two packets of the TLS hand-
shake. These include strings in the TLS Server Name
Indication (SNI) extension, and in the server-supplied
SSL certiﬁcate, which contains ﬁelds such as the Common
Name and Subject Alternate Names (SAN) list.
Interestingly, the classiﬁer does not parse ﬁelds in the
TLS handshake: our FrankenFlows were classiﬁed as an
application even if the TLS handshake contained invalid
data, so long as a keyword in a matching rule appeared
in the packet payload. Similar to the case of HTTP, this
6The remaining 4 categories are for intranet traﬃc.
would allow misclassiﬁcation, e.g., by putting a keyword
in the SAN list that matches a diﬀerent application.
3.2 Matching Rules
Our analysis revealed that the HTTP matching rules
used by the shaper can be described by a series of key-
word matches on text in matching ﬁelds.7 Examples
of diﬀerent matching rules include: facebook (Face-
book), .spotify. (Spotify), music.qq (QQ Music),
instagram.com (Instagram), storage.live.com (Sky-
Drive), and itunes.apple (iTunes). For HTTPS traf-
ﬁc, the matching rules are simply text strings. Exam-
ples include netflix, facebook and cloudfront.
None of the Host-header-based matching rules spec-
ify where in the matching ﬁeld they occur. For exam-
ple, netflix.youtube.com and youtube.netflix.com
would both be classiﬁed as Netﬂix.
In the case of
matches of the User-Agent string, we ﬁnd that 28 cases
match only at the beginning of the string (e.g., Viber,
Pandora, Pinterest, WhatsApp) and 4 cases can match
anywhere in the string (e.g., YouTube, Twitter). We
could ﬁnd no general pattern for when location of key-
words played a role in matching rules.
Interestingly, these matching rules are surprisingly
brittle. For example, we already identiﬁed how Net-
ﬂix and YouTube can be misidentiﬁed. Further, we did
not ﬁnd any restriction on whether a string appears
at the end of a line, something that would avoid this
case. As another example, we found that traﬃc from