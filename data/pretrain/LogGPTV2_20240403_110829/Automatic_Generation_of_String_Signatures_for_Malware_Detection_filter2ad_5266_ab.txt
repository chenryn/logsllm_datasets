lent discrimination power, rejecting 99% of FP signatures and accepting almost
half of TP signatures.
Fig. 1. Fractions of FP and TP sequences
with probabilities below the X value
Fig. 2. TP rate comparison between
models with varying pruning thresholds
and varying training set sizes
Automatic Generation of String Signatures for Malware Detection
107
To evaluate the eﬀectiveness of Hancock’s information gain-based pruning
algorithm, we used two sets of models: non-pruned and pruned. The former were
trained on 50 to 100 Mbytes of goodware. The latter were trained on 100 Mbytes
of goodware and pruned to various sizes. For each model, we then computed its
TP rate at the probability threshold that yields a 2% FP rate. Figure 2 shows
these TP rates of goodware models versus the model’s size in memory. In this
case, pruning can roughly halve the goodware model size while oﬀering the same
TP rate as the pruned model derived from the same training set.
3.2 Library Function Recognition
A library is a collection of standard functions that implement common operations,
such as ﬁle IO, string manipulation, and graphics. Modern malware authors use
library functions extensively to simplify development, just like goodware authors.
By construction, variants of a malware family are likely to share some library func-
tions. Because these library functions also have a high probability of appearing in
goodware, Hancock needs to remove them from consideration when generating
string signatures. Toward this goal, we developed a set of library function recog-
nition techniques to determine whether a function in a malware ﬁle is likely to be
a library function or not.
A popular library identiﬁcation technique is IDA Pro’s Fast Library Iden-
tiﬁcation and Recognition Technology (FLIRT) [15], which uses byte pattern
matching algorithms (similar to string signature scanning) to quickly determine
whether a disassembled function matches any of the signatures known to IDA
Pro.1 Although FLIRT is very accurate in pinpointing common library func-
tions, it still needs some improvement to suit Hancock’s needs. First, FLIRT is
designed to never falsely identify a library. To achieve this, FLIRT ﬁrst tries to
identify the compiler type (e.g., Visual C++ 7.0, 8.0, Borland C++, Delphi, etc.)
of a disassembled program and applies only signatures for that compiler. For ex-
ample, vcseh signatures (Structured Exception Handling library signatures) will
only be applied to binary ﬁles that appear to have been compiled with Visual
C++ 7 or 8. This conservative approach can lead to false negatives (a library
function not identiﬁed) because of failure in correctly detecting the compiler
type. In addition, because FLIRT uses a rigorous pattern matching algorithm
to search for signatures, small variation in libraries, e.g., minor changes in the
source code, diﬀerent settings in compiler optimization options or use of diﬀerent
compiler versions to build the library, could prevent FLIRT from recognizing all
library functions in a disassembled program.
In contrast to FLIRT’s conservative approach, Hancock’s primary goal is to
eliminate false positive signatures. It takes a more aggressive stance by being
willing to mistake non-library functions for library functions. Such misidentiﬁ-
cation is acceptable because it prevents any byte sequence that is potentially
1 IDA Pro ships with a database of signatures for about 120 libraries associated with
common compilers. Each signature corresponds to a binary pattern in a library
function.
108
K. Griﬃn et al.
associated with a library function from being used as a malware signature. We
exploited this additional latitude with the following three heuristics:
Universal FLIRT Heuristic. This heuristic generalizes IDA Pro’s FLIRT
technique by matching a given function against all FLIRT signatures, regardless
of whether they are associated with the compiler used to compile the function.
This generalization is useful because malware authors often post-process their
malware programs to hide or obfuscate compiler information in an attempt to
deter any reverse engineering eﬀorts. Moreover, any string signatures extracted
from a function in a program compiled by a compiler C1 that looks like a li-
brary function in another compiler C2 are likely to cause false positives against
programs compiled by C2 and thus should be rejected.
Library Function Reference Heuristic. This heuristic identiﬁes a library
function if the function is statically called, directly or indirectly, by any known
library function. The rationale behind this heuristic is that since a library cannot
know in advance which user program it will be linked to, it is impossible for
a library function to statically call any user-written function, except callback
functions, which are implemented through function pointers and dynamically
resolved. As a result, it is safe to mark all children of a library function in its
call tree as library functions. Speciﬁcally, the proposed technique disassembles
a binary program, builds a function call graph representation of the program,
and marks any function that is called by a known library function as a library
function. This marking process repeats itself until no new library function can
be found.
In general, compilers automatically include into an executable binary certain
template code, such as startup functions or error handling, which IDA Pro also
considers as library functions as well. These template functions and their callees
must be excluded in the above library function marking algorithm. For example,
the entry point function start and mainCRTstartup in Visual C++-compiled
binaries are created by the compiler to perform startup preparation (e.g., execute
global constructors, catch all uncaught exceptions) before invoking the user-
deﬁned main function.
3.3 Code Interestingness Check
The code interestingness check is designed to capture the intuitions of Symantec’s
malware analysis experts about what makes a good string signature. For the most
part, these metrics identify signatures that are less likely to be false positives.
They can also identify malicious behavior, though avoiding false positives is the
main goal. The code interestingness check assigns a score for each “interesting”
instruction pattern appearing in a candidate signature, sums up these scores, and
rejects the candidate signature if its sum is below a threshold, i.e. not interesting
enough. The interesting patterns used in Hancock are:
– Unusual constant values. Constants sometimes have hard-coded values
that are important to malware, such as the IP address and port of a command
Automatic Generation of String Signatures for Malware Detection
109
and control server. More importantly, if a signature has unusual constant
values, it is less likely to be a false positive.
– Unusual address oﬀsets. Access to memory that is more than 32 bytes
from the base pointer can indicate access to a large class or structure. If
these structures are unique to a malware family, then accesses to particular
oﬀsets into this structure are less likely to show up in goodware. This pattern
is not uncommon among legitimate Win32 applications. Nonetheless, it has
good discrimination power.
– Local or non-library function calls. A local function call itself is not
very distinctive, but the setup for local function calls often is, in terms of
how it is used and how its parameters are prepared. In contrast, setup for
system calls is not as interesting, because they are used in many programs
and invoked in a similar way.
– Math instructions. A malware analyst at Symantec noted that malware
often perform strange mathematical operations, to obfuscate and for various
other reasons. Thus, Hancock looks for strange sequences of XORs, ADDs,
etc. that are unlikely to show up in goodware.
4 Signature Candidate Filtering
Hancock selects candidate signatures using techniques that assess a candidate’s
FP probability based solely on its contents. In this section, we describe a set
of ﬁltering techniques that remove from further consideration those candidate
signatures that are likely to cause a false positive based on the signatures’ use
in malware ﬁles.
These diversity-based techniques only accept a signature if it matches variants
of one malware family (or a small number of families). This is because, if a byte
sequence exists in many malware families, it is more likely to be library code –
code that goodware could also use. Therefore, malware ﬁles covered by a Hancock
signature should be similar to one another.
Hancock measures the diversity of a set of binary ﬁles based on their byte-
level and instruction-level representations. The following two subsections de-
scribe these two diversity measurement methods.
4.1 Byte-Level Diversity
Given a signature, S, and the set of ﬁles it covers, X, Hancock measures the
byte-level similarity or diversity among the ﬁles in X by extracting the byte-level
context surrounding S and computing the similarity among these contexts.
More concretely, Hancock employs the following four types of byte-level
signature-containing contexts for diversity measurement.
Malware Group Ratio/Count. Hancock clusters malware ﬁles into groups
based on their byte-level histogram representation. It then counts the number
of groups to which the ﬁles in X belong. If this number divided by the number
110
K. Griﬃn et al.
of ﬁles in X exceeds a threshold ratio, or if the number exceeds a threshold
count, Hancock rejects S. These ﬁles cannot be variants of a single malware
family, if each malware group indeed corresponds to a malware family.
Signature Position Deviation. Hancock calculates the position of S within
each ﬁle in X, and computes the standard deviation of S’s positions in these
ﬁles. If the standard deviation exceeds a threshold, Hancock rejects S, because
a large positional deviation suggests that S is included in the ﬁles it covers for
very diﬀerent reasons. Therefore, these ﬁles are unlikely to belong to the same
malware family. The position of S in a malware ﬁle can be an absolute byte
oﬀset, which is with respect to the beginning of the ﬁle, or a relative byte oﬀset,
which is with respect to the beginning of the code section containing S.
Multiple Common Signatures. Hancock attempts to ﬁnd another common
signature that is present in all the ﬁles in X and is at least 1 Kbyte away from
S. If such a common signature indeed exists and the distance between this
signature and S has low standard deviation among the ﬁles in X, then Hancock
accepts S because this suggests the ﬁles in X share a large chunk of code
and thus are likely to be variants of a single malware family. Intuitively, this
heuristic measures the similarity among ﬁles in X using additional signatures
that are suﬃciently far away, and can be generalized to using the third or fourth
signature.
Surrounding Context Count. Hancock expands S in each malware ﬁle in
X by adding bytes to its beginning and end until the resulting byte sequences
become diﬀerent. For each such distinct byte sequence, Hancock repeats the same
expansion procedure until the expanded byte sequences reach a size limit, or
when the total number of distinct expanded byte sequences exceeds a threshold.
If this expansion procedure terminates because the number of distinct expanded
byte sequences exceeds a threshold, Hancock rejects S, because the fact that
there are more than several distinct contexts surrounding S among the ﬁles in
X suggests that these ﬁles do not belong to the same malware family.
4.2 Instruction-Level Diversity
Although byte-level diversity measurement techniques are easy to compute and
quite eﬀective in some cases, they treat bytes in a binary ﬁle as numerical
values and do not consider their semantics. Given a signature S and the set
of ﬁles it covers, X, instruction-level diversity measurement techniques, on the
other hand, measure the instruction-level similarity or diversity among the ﬁles
in X by extracting the instruction-level context surrounding S and computing
the similarity among these contexts.
Enclosing Function Count. Hancock extracts the enclosing function of S in
each malware ﬁle in X, and counts the number of distinct enclosing functions.
If the number of distinct enclosing functions of S with respect to X is higher
than a threshold, Hancock rejects S, because S appears in too many distinct
Automatic Generation of String Signatures for Malware Detection
111
contexts among the ﬁles in X and therefore is not likely to be an intrinsic part
of one or a very small number of malware families. To determine if two enclosing
functions are distinct, Hancock uses the following three identicalness measures,
in decreasing order of strictness:
– The byte sequences of the two enclosing functions are identical.
– The instruction op-code sequences of the two enclosing functions are identi-
cal. Hancock extracts the op-code part of every instruction in a function, and
normalizes variants of the same op-code class into their canonical op-code.
For example, there are about 10 diﬀerent X86 op-codes for ADD, and Han-
cock translates all of them into the same op-code. Because each instruction’s
operands are ignored, this measure is resistant to intentional or accidental
polymorphic transformations such as re-locationing, register assignment, etc.
– The instruction op-code sequences of the two enclosing functions are iden-
tical after instruction sequence normalization. Before comparing two op-
code sequences, Hancock performs a set of de-obfuscating normalizations
that are designed to undo simple obfuscating transformations, such as re-
placing “test esi, esi” with “or esi, esi”, replacing “push ebp; mov
ebp, esp” with “push ebp; push esp; pop ebp”, etc.
5 Multi-Component String Signature Generation
Traditionally, string signatures used in AV scanners consist of a contiguous se-
quence of bytes. We refer to these as single-component signature (SCS). A nat-
ural generalization of SCS is multi-component signatures (MCS), which consist
of multiple byte sequences that are potentially disjoint from one another. For
example, we can use a 48-byte SCS to identify a malware program; for the same
amount of storage space, we can create a two-component MCS with two 24-byte
sequences. Obviously, an N-byte SCS is a special case of a K-component MCS
where each component is of size N
K . Therefore, given a ﬁxed storage space budget,
MCS provides more ﬂexibility in choosing malware-identifying signatures than
SCS, and is thus expected to be more eﬀective in improving coverage without
increasing the false positive rate.
In the most general form, the components of a MCS do not need to be of
the same size. However, to limit the search space, in the Hancock project we
explore only those MCSs that have equal-sized components. So the next ques-
tion is how many components a MCS should have, given a ﬁxed space budget.
Intuitively, each component should be suﬃciently long so that it is unlikely to
match a random byte sequence in binary programs by accident. On the other
hand, the larger the number of components in a MCS, the more eﬀective it is
in eliminating false positives. Given the above considerations and the practical
signature size constraint, Hancock chooses the number of components in each
MCS to be between 3 and 5.
Hancock generates the candidate component set using a goodware model and
a goodware set. Unlike SCS, candidate components are drawn from both data
and code, because intuitively, combinations of code component signatures and
112
K. Griﬃn et al.
data component signatures make perfectly good MCS signatures. When Han-
cock examines an N
K -byte sequence, it ﬁnds the longest substring containing this
sequence that is common to all malware ﬁles that have the sequence. Hancock
takes only one candidate component from this substring. It eliminates all se-
quences that occur in the goodware set and then takes the sequence with the
lowest model probability. Unlike SCS, there is no model probability threshold.
Given a set of qualiﬁed component signature candidates, S1, and the set of
malware ﬁles that each component signature candidate covers, Hancock uses
the following algorithm to arrive at the ﬁnal subset of component signature
candidates used to form MCSs, S2:
1. Compute for each component signature candidate in S1 its eﬀective coverage
value, which is a sum of weights associated with each ﬁle the component
signature candidate covers. The weight of a covered ﬁle is equal to its coverage
count, the number of candidates in S2 already covering it, except when the
number of component signatures in S2 covering that ﬁle is larger than or
equal to K, in which case the weight is set to zero.
2. Move the component signature candidate with the highest eﬀective cover-
age value from S1 to S2, and increment the coverage count of each ﬁle the
component signature candidate covers.
3. If there are still malware ﬁles that are still uncovered or there exists at least
one component signature in S1 whose eﬀective coverage value is non-zero,
go to Step 1; otherwise exit.
The above algorithm is a modiﬁed version of the standard greedy algorithm for
the set covering problem. The only diﬀerence is that it gauges the value of each
component signature candidate using its eﬀective coverage value, which takes
into account the fact that at least K component signatures in S2 must match a
malware ﬁle before the ﬁle is considered covered. The way weights are assigned
to partially covered ﬁles is meant to reﬂect the intuition that the value of a
component signature candidate to a malware ﬁle is higher when it brings the
ﬁle’s coverage count from X − 1 to X than that from X − 2 to X − 1, where X