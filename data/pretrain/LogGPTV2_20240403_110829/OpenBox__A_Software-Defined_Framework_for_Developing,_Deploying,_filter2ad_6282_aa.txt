title:OpenBox: A Software-Defined Framework for Developing, Deploying,
and Managing Network Functions
author:Anat Bremler-Barr and
Yotam Harchol and
David Hay
OpenBox: A Software-Deﬁned Framework for
Developing, Deploying, and Managing
Network Functions
Anat Bremler-Barr ∗
PI:EMAIL
PI:EMAIL
∗ School of Computer Science, The Interdisciplinary Center, Herzliya, Israel
PI:EMAIL
† School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel
Yotam Harchol †
David Hay†
ABSTRACT
We present OpenBox — a software-deﬁned framework
for network-wide development, deployment, and man-
agement of network functions (NFs). OpenBox eﬀec-
tively decouples the control plane of NFs from their data
plane, similarly to SDN solutions that only address the
network’s forwarding plane.
OpenBox consists of three logic components. First,
user-deﬁned OpenBox applications provide NF speciﬁ-
cations through the OpenBox north-bound API. Sec-
ond, a logically-centralized OpenBox controller is able
to merge logic of multiple NFs, possibly from multiple
tenants, and to use a network-wide view to eﬃciently
deploy and scale NFs across the network data plane.
Finally, OpenBox instances constitute OpenBox’s data
plane and are implemented either purely in software or
contain speciﬁc hardware accelerators (e.g., a TCAM).
In practice, diﬀerent NFs carry out similar process-
ing steps on the same packet, and our experiments in-
deed show a signiﬁcant improvement of the network per-
formance when using OpenBox. Moreover, OpenBox
readily supports smart NF placement, NF scaling, and
multi-tenancy through its controller.
CCS Concepts
•Networks → Middle boxes / network appliances;
Programming interfaces; Network control algorithms;
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22–26, 2016, Florianopolis, Brazil
© 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934875
Figure 1: The general architecture of the Open-
Box framework.
Keywords
Network functions; Middleboxes; Software-Deﬁned Net-
works
1.
INTRODUCTION
Software-deﬁned networking (SDN) has been a tremen-
dous game-changer, as it decouples the control plane
of network forwarding appliances (e.g., switches and
routers) from their data plane. SDN has succeeded
in solving important problems in the forwarding plane,
such as cost, management, multi-tenancy, and high en-
try barriers that limit innovation.
However, in current SDN solutions, such as Open-
Flow [15], only the forwarding appliances are software-
deﬁned, while the other data plane appliances continue
to suﬀer from all of the above problems. Moreover,
these appliances, which are usually referred to as net-
work functions (NFs) or middleboxes, often suﬀer from
additional, more complex problems as well. Yet studies
show that NFs constitute 40%-60% of the appliances
511
deployed in large-scale networks [39].
In this paper
we present OpenBox, a framework and a protocol that
make network functions software-deﬁned, using a logi-
cally centralized controller.
Traditionally, each middlebox was marketed as a sin-
gle piece of hardware, with its proprietary software al-
ready installed on it, for a high price. This prevented,
as noted above, on-demand scaling and provisioning.
The call for network function virtualization (NFV) [11]
aims to reduce the cost of ownership and management
of NFs by making NFs virtual appliances, running on
top of a hypervisor or in a container. While NFV im-
proves on-demand scaling and provisioning, it does not
solve other problems such as the limited and separate
management of each NF.
Network traﬃc nowadays usually traverses a sequence
of NFs (a.k.a. a service chain). For example, a packet
may go through a ﬁrewall, then through an Intrusion
Prevention System (IPS), and then through a load bal-
ancer, before reaching its destination. A closer look into
these NFs shows that many of them process the packets
using very similar processing steps. For example, most
NFs parse packet headers and then classify the packets
on the basis of these headers, while some NFs modify
speciﬁc header ﬁelds, or also classify packets based on
Layer 7 payload content. Nonetheless, each NF has its
own logic for these common steps. Moreover, each NF
has its own management interface: each might be man-
aged by a diﬀerent administrator, who does not know,
or should not know, about the existence and the logic
of the other NFs.
Our OpenBox framework addresses the challenges of
eﬃcient NF management by completely decoupling the
control plane of a NF from its data plane using a newly
deﬁned communication protocol [35], whose highlights
are presented in this paper. The observation that many
NFs have similar data planes but diﬀerent control logic
is leveraged in OpenBox to deﬁne general-purpose (yet
ﬂexible and programmable) data plane entities called
OpenBox Instances (OBIs), and a logically-centralized
control plane, which we call the OpenBox Controller
(OBC). NFs are now written as OpenBox Applications
on top of the OBC, using a northbound programming
API. The OBC is in charge of deploying application
logic in the data plane and realizing the intended behav-
ior of the applications in the data path. The OpenBox
protocol deﬁnes the communication channel between
the OBC and the OBIs.
To the best of our knowledge, we are the ﬁrst to intro-
duce such a general framework for software-deﬁned NFs,
which includes speciﬁcations for a logically-centralized
controller and its northbound API for NF application
development, for an extensible data plane instance (OBI),
and for a communication protocol between the two. We
also propose a novel algorithm for merging the core
logic of multiple NF applications in the control plane,
such that computationally-intensive procedures (e.g.,
packet classiﬁcation or DPI) are only performed once
for each packet. Since this implies that packets may
traverse a smaller number of physical locations, latency
is reduced and resources can be reallocated to provide
higher throughput. We compare our work to previ-
ous works that apply SDN ideas to the middlebox do-
main [2, 17, 18, 33, 38] in Section 7.
We have implemented a prototype of the OpenBox
framework and we provide a simple installation script
that installs the entire system on a Mininet VM [28],
so users can easily create a large-scale network environ-
ment and test their network functions. All our code is
publicly available at [29].
OpenBox promotes innovation in the NF domain. De-
velopers can develop and deploy new NFs as OpenBox
applications, using basic building blocks (e.g., header
classiﬁcation) provided by the framework. Furthermore,
the capabilities of the OpenBox data plane can be ex-
tended beyond these basic building blocks: an applica-
tion can provide an extension module code in the con-
trol plane. This module can then be injected into the
corresponding OBIs in the data plane, without having
to recompile or redeploy them.
OpenBox is designed to also support hardware-based
OBIs, which use speciﬁc hardware to accelerate the data
plane processing. For example, certain web optimizers
may require speciﬁc video encoding hardware. However,
the OBI with this hardware does not have to implement
the entire set of processing logic deﬁned by the Open-
Box protocol.
Instead, it can be chained with other,
software- or hardware-based OBIs, which would pro-
vide the additional logic. This reduces the cost of the
specialized OBI and the eﬀort required to develop the
NF application that uses it. Developers may also create
a purely-software version of their optimizer OBI, which
will be used by the OpenBox framework to scale up at
peak load times.
2. ABSTRACTING PACKET PROCESS-
ING
We surveyed a wide range of common network func-
tions to understand the stages of packet processing per-
formed by each. Most of these applications use a very
similar set of processing steps. For example, most NFs
do some sort of header-based classiﬁcation. Then, some
of them (e.g., translators, load balancers) do some packet
modiﬁcation. Others, such as intrusion prevention sys-
tems (IPSs) and data leakage prevention systems (DLP),
further classify packets based on the content of the pay-
load (a process usually referred to as deep packet inspec-
tion (DPI)). Some NFs use active queue management
before transmitting packets. Others (such as ﬁrewalls
and IPSs) drop some of the packets, or raise alerts to
the system administrator.
In this section we discuss and present the abstraction
of packet processing applications required to provide a
framework for the development and deployment of a
wide range of network functions.
512
(a) Firewall
(b) Intrusion Prevention System (IPS)
Figure 2: Sample processing graphs for ﬁrewall and intrusion prevention system NFs.
Abstract Block Name Role
FromDevice
ToDevice
Discard
HeaderClassiﬁer
RegexClassiﬁer
HeaderPayload
Classiﬁer
NetworkHeader
FieldRewriter
Alert
Log
ProtocolAnalyzer
GzipDecompressor
HtmlNormalizer
BpsShaper
FlowTracker
VlanEncapsulate
VlanDecapsulate
Read packets from
interface
Write packets to
interface
Drop packets
Classify on
header ﬁelds
Classify using
regex match
Classify on header
and payload
Rewrite ﬁelds
in header
Send an alert
to controller
Log a packet
Classify based
on protocol
Decompress HTTP
packet/stream
Normalize HTML
packet
Limit data rate
Mark ﬂows
Push a VLAN tag
Pop a VLAN tag
Class
T
T
T
C
C
C
M
St
St
C
M
M
Sh
M
M
M
2.1 Processing Graph
Packet processing is abstracted as a processing graph,
which is a directed acyclic graph of processing blocks.
Each processing block represents a single, encapsulated
logic unit to be performed on packets, such as header
ﬁeld classiﬁcation, or header ﬁeld modiﬁcation. Each
block has a single input port (except for a few special
blocks) and zero or more output ports. When handling
a packet, a block may push it forward to one or more of
its output ports. Each output port is connected to an
input port of another block using a connector.
The notion of processing blocks is similar to Click’s
notion of elements [23] and a processing graph is simi-
lar to Click’s router conﬁguration. However, the Open-
Box protocol hides lower level aspects such as the Click
push/pull mechanism, as these may be implementation-
speciﬁc. A processing block can represent any operation
on packets, or on data in general. A processing block
can buﬀer packets and coalesce them before forwarding
them to the next block, or split a packet.
In our implementation, described in Section 4, we use
Click as our data plane execution engine. We map each
OpenBox processing block to a compound set of Click
elements, or to a new element we implemented, if no
Click element was suitable.
Figure 2 shows sample processing graphs for a ﬁre-
wall network function (Fig. 2(a)) and an IPS network-
function (Fig. 2(b)). The ﬁrewall, for example, reads
packets, classiﬁes them based on their header ﬁeld val-
ues, and then either drops the packets, sends an alert to
the system administrator and outputs them, or outputs
them without any additional action. Each packet will
traverse a single path of this graph.
Some processing blocks represent a very simple oper-
ation on packets, such as dropping all of them. Others
may have complex logic, such as matching the packet’s
payload against a set of regular expressions and out-
putting the packet to the port that corresponds to the
ﬁrst matching regex, or decompressing gzip-compressed
HTTP packets.
Our OpenBox protocol deﬁnes over 40 types of ab-
stract processing blocks [35]. An abstract processing
block may have several implementations in the data
plane, depending on the underlying hardware and soft-
ware in the OBI. For example, one block implementa-
tion might perform header classiﬁcation using a trie in
Table 1: Partial
list of abstract processing
blocks. The class property is explained in Sec-
tion 2.2.1.
software while another might use a TCAM for this task
[42]. As further explained in Section 3 and in the pro-