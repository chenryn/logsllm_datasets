title:Evolve or Die: High-Availability Design Principles Drawn from Googles
Network Infrastructure
author:Ramesh Govindan and
Ina Minei and
Mahesh Kallahalla and
Bikash Koley and
Amin Vahdat
Evolve or Die: High-Availability Design Principles
Drawn from Google’s Network Infrastructure
Ramesh Govindan†?, Ina Minei†, Mahesh Kallahalla†, Bikash Koley†, Amin Vahdat†
†Google
?University of Southern California
Abstract
Maintaining the highest levels of availability for content
providers is challenging in the face of scale, network
evolution, and complexity. Little, however, is known about
the network failures large content providers are susceptible
to, and what mechanisms they employ to ensure high avail-
ability. From a detailed analysis of over 100 high-impact
failure events within Google’s network, encompassing many
data centers and two WANs, we quantify several dimensions
of availability failures. We ﬁnd that failures are evenly
distributed across different network types and across data,
control, and management planes, but that a large number of
failures happen when a network management operation is
in progress within the network. We discuss some of these
failures in detail, and also describe our design principles for
high availability motivated by these failures. These include
using defense in depth, maintaining consistency across
planes, failing open on large failures, carefully preventing
and avoiding failures, and assessing root cause quickly.
Our ﬁndings suggest that, as networks become more com-
plicated, failures lurk everywhere, and, counter-intuitively,
continuous incremental evolution of the network can, when
applied together with our design principles, result in a more
robust network.
CCS Concepts
•Networks ! Control path algorithms; Network reliabil-
ity; Network manageability;
Keywords
Availability; Control Plane; Management Plane
1.
INTRODUCTION
Global-scale content providers offer an array of increas-
ingly popular services ranging from search, image sharing,
social networks, video dissemination, tools for online col-
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted.
SIGCOMM ’16, August 22–26, 2016, Florianopolis, Brazil
© 2016 Copyright held by the owner/author(s).
ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934891
58
laboration, online marketplaces, and cloud services. To sup-
port these services, they build data centers and WANs with
a global reach, both to interconnect their data centers and to
achieve client proximity. Providers optimize their networks
to provide high throughput, low latency, and high availabil-
ity. Some or all of these characteristics correlate with in-
creased revenue [3, 23].
While much has been written about content provider
network design and performance [18, 35, 6, 11],
little
is known about network availability challenges faced by
content providers. What kind of availability guarantees do
content providers strive to achieve? What challenges do they
face in meeting these guarantees? What kinds of failures are
they susceptible to? How do they achieve high availability
in the face of these failures? This paper sheds light on some
of these questions based on operational experience at one
large content provider, Google.
Google runs three qualitatively different types of networks
(Section 2): data center networks, designed from merchant
silicon switches, with a logically centralized control plane; a
software-deﬁned WAN called B4 that supports multiple traf-
ﬁc classes and uses centralized trafﬁc engineering; and an-
other global WAN called B2 for user-facing trafﬁc that em-
ploys decentralized trafﬁc engineering. We strive to main-
tain high availability in these networks: for example, for
user-facing trafﬁc, Google’s internal availability target is no
more than a few minutes downtime per month.
Maintaining this high availability is especially difﬁcult for
three reasons (Section 3.2). The ﬁrst is scale and hetero-
geneity: Google’s network spans the entire globe, and at this
scale, failure of some component in the network is common
[9]. The second is velocity of evolution: the network is con-
stantly changing in response to increasing trafﬁc demand as
well as the rollout of new services. The third is management
complexity: while the control plane has been evolving to deal
with complexity of the network, the management plane [12]
has not kept pace.
In spite of these challenges, our network infrastructure
and services deliver some of the highest availability levels in
the industry across dozens of individual services. We have
maintained this availability despite experiencing several sub-
stantial failure events. Examples of such failures include a
single bug taking out connectivity to a datacenter, a single
line card failure taking down an entire backbone router, and
a single misconﬁguration resulting in the complete failure
of a WAN’s control plane. We carefully document (in post-
mortem reports) and root-cause each signiﬁcant new failure,
and also draw principles for avoiding, localizing, and recov-
ering from failures, such that subsequent failures are unlikely
and the ones that do take place are rarely visible to our end
users.
Contributions. In this paper, we make three contributions
by analyzing over 100 post-mortem reports of unique1 high-
impact failures (Section 4) within a two year period. First,
we present a quantitative analysis of different dimensions of
availability failures in Google (Section 5). We ﬁnd that the
failure rate is roughly comparable across the three types of
networks we have (data center networks, B2, and B4). We
also ﬁnd that each of these networks is susceptible to hard-
ware/data plane failures, as well as failures in the control
plane and the management plane. 80% of the failures last
between 10 mins and 100 mins, signiﬁcantly larger than the
availability targets for our network. Nearly 90% of the fail-
ures have high impact: high packet losses, or blackholes to
entire data centers or parts thereof. Finally, we ﬁnd that,
when most of these failures happen, a management opera-
tion was in progress in the vicinity.
failures;
Second, we classify failures by a few root-cause cate-
gories (Section 6), and ﬁnd that, for each of data, control
and management planes, the failures can be root-caused to a
handful of categories. Examples of such categories include:
risk assessment
lack of consistency between
control plane components; device resource overruns; link
ﬂaps; incorrectly executed management operation; and so
forth. We quantify the distribution of failures across these
categories, but also discuss in detail actual failures within
some categories. This categorization is more ﬁne-grained
than simply root causing to hardware failure, software bug,
or human error, allowing us to draw important lessons in
improving network availability.
Third, we discuss high availability design principles
drawn from these failures (Section 7). Our qualitative
analysis and root cause categorization all suggest no single
mechanism or technique can address a signiﬁcant fraction
of Google’s availability failures. First, defense in depth
is required to detect and react to failures across different
layers and planes of the network and can be achieved
by containing the failure radius and developing fallback
strategies. Second, fail-open preserves the data plane when
the control plane fails. Third, maintaining consistency
across data, control, and management planes can ensure safe
network evolution. Fourth, careful risk assessment, testing,
and a uniﬁed management plane can prevent or avoid
failures. Fifth, fast recovery from failures is not possible
without high-coverage monitoring systems and techniques
for root-cause analysis. By applying these principles,
together with the counter-intuitive idea that the network
should be continuously and incrementally evolved, we have
managed to increase the availability of our networks even
while its scale and complexity has grown many fold. We
conclude with a brief discussion on open research problems
in high-availability network design.
1Each post-mortem report documents a unique, previously unseen failure.
Subsequent instances of the same failure are not documented.
59
Figure 1: Google’s Global Network
2. GOOGLE’S NETWORK
In this section, we discuss a simpliﬁed model of Google’s
network. This discussion will give context for some of the
descriptions of failures in later sections, but omits some of
the details for brevity.
The Networks. Conceptually, Google’s global network,
one of the largest in the world, consists of three qualitatively
different components (Figure 1): a set of campuses, where
each campus hosts a number of clusters; a WAN, called B2,
that carries trafﬁc between users and the clusters; and an
internal WAN called B4 [18] responsible also for carrying
trafﬁc among clusters. The rationale for the two WANs, and
for the differences in their design, is discussed in [18].
Google has, over the years, designed several generations
of cluster networks; in our network today, multiple genera-
tions of clusters co-exist. These cluster designs employ vari-
ants of a multi-stage Clos topology (see [35]), with individ-
ual switches using successive generations of merchant sili-
con. The bottom layer of the Clos network consists of ToR
switches providing connectivity to a rack of servers. The
middle layers of the Clos network have, over different gen-
erations of fabrics, consisted of differently-sized aggregate
sub-fabrics, typically called superblocks. The top layers of
these aggregation fabrics comprise core switches, or aggre-
gates thereof called spine blocks.
At a given geographical location or metro, Google may
have more than one data center.
In earlier generations of
Google’s cluster design, servers within a single data center
were interconnected by a single fabric, and fabrics within a
metro were, in turn, interconnected through a cluster aggre-
gation fabric to deliver sufﬁcient capacity within the metro.
The most recent generation can scale to interconnect mul-
tiple cluster fabrics. The cluster aggregation fabrics and the
newest generation fabrics, in turn, connect to the two WANs,
B2 and B4 via cluster aggregation routers (CARs). In the
older generations, a CAR was a separate aggregation fabric,
and itself a multistage Clos network. In the newest genera-
tion fabric, some of the super or middle blocks in the fabric
are used as the CARs.
Each CAR connects to a B4 switch [18] in a metro called a
B4BR (or B4 border router). A B4BR itself is also a multi-
stage switching network, built from merchant silicon. The
FabricB2BRB2CRFabricB2BRB2CRB2B4    CARB4BRCARB4BRB4 BundleB2 BundleClusters...Other ISPsB4BRB4 WAN consists of point-to-point bundles between B4BRs
in different metros. Each bundle is a complex interconnect
between two B4BRs, designed to achieve high aggregate ca-
pacity by aggregating a large number of physical links. Traf-
ﬁc between clusters in different metros may traverse several
B4BRs, as described below.
Each CAR also connects to two B2 border routers
(B2BRs).
These commercially available routers also
provide signiﬁcant aggregate capacity using proprietary
internal switching fabrics. The B2 WAN consists of B2BRs
interconnected using a network of B2 core routers (B2CRs).
B2CRs also connect with edge routers which peer with
Google’s customers and transit networks. The interconnects
between all of these devices are also bundles of physical
links2 providing high aggregate capacity.
Control and Data Planes. The three networks differ
qualitatively in the design of their control and data planes.
Cluster networks consist of a logically centralized control
plane responsible for establishing forwarding rules at fabric
switches. The control plane is decomposed, for software
modularity reasons,
in
concert with each other: a Fabric Controller (FC) that
computes paths within the fabric; a Routing Agent (RA)
that speaks BGP and IS-IS with neighboring border routers
and performs IP route computation; and an OpenFlow
Controller (OFC) that programs switches by interacting
with OpenFlow Agents (OFAs)
running on individual
switches. To achieve this programming, the OFC relies on
information from the FC and the RA. Packet forwarding
within the fabric uses ECMP in order to better utilize the
rich connectivity of Clos fabrics.
into three components that act
B4’s control plane also uses logical centralization, but
applies this centralization at two levels. Within a B4BR,
the control plane is organized as in clusters, consisting of
the same three components (FC, RA and OFC) as discussed
above, and data plane forwarding uses ECMP to better
utilize the aggregate capacity of the router. Across the B4
WAN, however, the centralized control plane is architected
differently because the WAN topology is qualitatively
different from that of clusters. Speciﬁcally, the control plane
consists of four components that work in concert [18, 21];
the B4 Gateway extracts network states from and programs
control plane state in B4BRs; a Topology Modeler computes
a model of the current WAN topology, including current
capacity constraints at and between B4BRs, using network
state extracted from the Gateway; a TE Server computes
site-level TE paths between B4BRs using the topology
model, as well as trafﬁc demand presented to the network;
and a Bandwidth Enforcer (BwE, [21]) estimates the trafﬁc
demand needed for the TE Server, and also enforces offered
load by applications to pre-determined limits. The data
plane uses encapsulation (tunneling) to effect TE paths,
and the B4BR control plane translates a site-level TE path
into one or more intra-B4BR fabric paths, or one or more
inter-B4BR bundles.
Finally, the B2 network’s control plane is similar to that
of other large ISPs. B2 uses IS-IS internally, speaks E-BGP
with CARs, B4BRs, and external peers, and employs route-
reﬂection to scale BGP route dissemination and computa-
tion. Most trafﬁc on B2 is engineered using MPLS tun-
nels. RSVP establishes or tears down the tunnels, and MPLS
auto-bandwidth [27] adapts tunnel capacities in response to
changes in demand. B2 uses MPLS priorities to accommo-
date different classes of trafﬁc.
The Workload and Service Architectures. The three net-
works collectively serve two kinds of customers:
internal
customers, and user-facing services. Internal customers use
the clusters for distributed storage and distributed computa-
tions; for example, search indices are stored in distributed
storage that may be replicated across clusters, and indices
are (re)computed using distributed computations running on
servers within data centers. Both storage and computation
can generate signiﬁcant amounts of network trafﬁc.
From the network’s perspective, user-facing services can
be viewed as a two-tier hierarchy. Front-ends receive user
requests; a front-end is a software reverse proxy and cache
that parses the service request, and determines which back-
end, of many, the request should be load-balanced to. Back-
ends (which themselves are typically multi-tiered) fulﬁl the
request and return the response. Load balancers determine
which front-end and back-end a request is sent to: typically
DNS load-balancing is used to load-balance requests from
users to the frontend, and a load-balancer keeps track of ag-
gregate requests and backend load to load-balance requests
from frontends to backends. This design permits scale-out
of services in response to increasing demand. More interest-
ing, the use of load-balancers provides a level of indirection
that enables operators to dynamically re-conﬁgure services
in response to network (or other failures). Thus, for exam-
ple, front-ends or back-ends in a cluster can be drained (i.e.,