## 论文的主要贡献 {#159.html#-}首先，**这篇论文的主要"卖点"就是较少利用、或者说没有利用传统意义上的数据驱动的棋局**。第一篇论文里的AlphaGo以及后面的一些版本，都是主要利用人类的棋局作为重要的训练数据，采用监督学习（SupervisedLearning）和强化学习结合的形式。在 AlphaGo Zero这个版本里，人类的棋局被彻底放弃，而完全依靠了强化学习，从完全随机（Random）的情况开始，"进化"出了具有人类经验的各种走法的围棋策略，并且达到了非常高的竞技水平。可以说这是本篇论文的核心贡献。``{=html}在核心的模型方面也有不少改进，比如一个很大的改进就是**把策略网络（PolicyNetwork）和价值网络（ValueNetwork）合并**，这样就能更加有效地用简单统一的深度模型来对这两个网络进行建模。另外，整个模型的输入特征也有变化，从深度模型提取特征外加人工挑选特征，到这篇文章提出的**完全依靠棋盘的图像信息来自动抓取特征**，可谓是减少人工干预的一个重要步骤。文章的另一大看点是实验结果。作者们展示了新的 AlphaGo Zero模型能够战胜之前很多版本的模型，最令人惊奇的可能莫过于 AlphaGo Zero在"自学"的过程中，还"悟"到了很多人类在围棋学习过程中领悟的棋局招数。
## 论文的核心方法 {#159.html#-}AlphaGo Zero模型的核心起源于一个简单的深度网络模型。这个深度网络的输入是棋盘当前位置的表达（Representation）以及过去的历史信息，输出一个走子的概率以及对应的价值。这个价值是用来描述当前棋手能够赢的概率。刚才我们已经说了，这个深度网络集合了策略网络和价值网络，形成了这么一个统一的评价整个棋盘的神经网络。在具体的网络架构方面，AlphaGoZero采用了计算机视觉领域最近流行的**残差架构**（ResNet），可以说也是这个方法的一个小创新。有了这个基本的神经网络之后，作者们就需要和强化学习结合起来。具体来说，在每一个位置的时候，算法都会去执行一个**蒙特卡罗树搜索**（MonteCarlo TreeSearch），对当前的神经网络模型输出的走子策略进行一个修正，或者可以认为是"加强"。这个蒙特卡罗树搜索的输出结果依然是走子的概率，但是这个概率往往比之前单从神经网络得到的要强。然后，更新神经网络的参数，使得参数尽可能地接近蒙特卡罗树搜索的结果。那么，什么是蒙特卡罗树搜索？简单来说，就是我们从当前的棋盘情况出发，对整个棋盘产生的所有可能性进行有限制情况的搜索，也就是说，不是"穷举法"。大体说来，从某一个可能性走到下一个可能性主要是依靠下一个可能性发生的概率，以及通过神经网络来判断是否能赢的可能性。整个算法最开始的时候是从随机的位置初始化，然后通过对神经网络的更新，以及每一个迭代通过利用蒙特卡罗树进行搜索，从而找到更加合适的神经网络模型的参数，整个算法非常简单明了。不管是结构上还是复杂度上都比之前的版本要简洁不少。文章反复强调公布的算法可以在单机上运行（基于Google Cloud 的 4 TPU 机器），相比于最早的 AlphaGo 需要使用 176 个GPU，也可以看到整个模型的进化效果。
## 方法的实验效果 {#159.html#-}AlphaGo Zero 的实验效果是惊人的。从模拟中看，大约 20小时后，这个版本的模型就能够打败依靠数据的监督学习版本的 AlphaGo了。而到了 40 小时后，这个版本已经可以打败挑战了李世石的AlphaGo。也就是说，不依靠任何人类棋局，AlphaGo Zero 在不到 2天的运算时间里，就能够达到顶级的人类水平。除了可以打败之前的 AlphaGo版本以外，这个版本相比于监督学习的版本，在大约 20小时以后也可以更好地预测人类对战的走子。并且随着训练时间的推移，这种预测的准确性还在不断提升。刚才我们也提到了，AlphaGo Zero在自我训练的对战中，在不依靠人类数据的情况下，的确是发现了相当多的人类熟悉的对战套路。然而，有一些人类在围棋历史中较早发现的套路却没有或者较晚才在AlphaGo Zero的训练历史中习得。这打开了很多问题，比如发生这样情况的原因究竟是什么等等。最后，作者们展示了 AlphaGo Zero 非常强大的实战能力，在和之前最强的AlphaGo 版本，也就是 AlphaGo Master 的对战中，AlphaGo Zero 取得了 100 比0 的绝对优势。而相同的 AlphaGo Master 与人对弈的成绩是 60 比 0。
## 小结 {#159.html#-}今天我为你讲了发表在《自然》杂志上的这篇关于 AlphaGo Zero的论文，这篇文章介绍了一个简洁的围棋人工智能算法，结合深度学习和强化学习，不依靠人类的信息。一起来回顾下要点：第一，关注这篇文章主要作者的信息，我们可以推断出文章的一些变化方向。第二，这篇文章有两大看点，一是很少或者几乎没有利用人类的棋局数据，二是得到了显著的实验结果。第三，文章提出的核心模型将策略网络和价值网络合并，与强化学习相结合。最后，给你留一个思考题，有人说 AlphaGo Zero并不是完全不依靠人类信息，比如围棋本身的规则就是很强的监督信息；再比如，不管每一步的走动如何，棋局最后是输是赢，依然是很强的信息。那么，AlphaGoZero 到底是不是还是依赖了很强的数据呢？我们能不能把 AlphaGo Zero看做是监督学习的产物呢？你怎么看？欢迎你给我留言，和我一起讨论。![](Images/5f1a3d2ca933c759573c72ee2ba198b7.png){savepage-src="https://static001.geekbang.org/resource/image/ef/b2/efd991ee74e55356bb2776f3d8d375b2.jpg"}
# 059 \| 2017人工智能技术发展盘点今天是大年初一，在这里先给你拜个年，祝新年新气象，新年新开始！在今天这个辞旧迎新的日子里，我们对过去一年的人工智能技术发展做一个简单的盘点，梳理思路，温故知新。**2017年，对于人工智能整个领域的发展，是举足轻重的一年**。这一年，人工智能的各个领域都蓬勃发展，我们目睹了一些在AI 发展史上的标志性事件。比如，从人工智能的技术上看，人工智能系统AlphaGo Master与人类世界实时排名第一的棋手柯洁展开围棋人机对决，最终连胜三盘；从人工智能的投入上看，很多互联网公司都先后成立单独的人工智能研发机构，像阿里巴巴的达摩院；从云服务和人工智能结合的发展来看，2017年谷歌在这一方向发展迅猛，不仅在中国开设了研发中心，还宣布已经有超过万家企业和组织正在使用谷歌的人工智能接口。今天我希望能够从几个关键领域和发展方向出发，在繁多的科技进步中，理清关键信息，对过去一年的产业动态和发展做出点评，给你一个清晰而简单的信息参考。
## 人工智能在棋牌上的迅猛发展 {#160.html#-}**2017 年的一个标志性的事件，无疑是 AlphaGo在围棋这项运动中的"收官"表现**。虽然 2016 年 AlphaGo战胜李世石之后，很多人依然对人类能够在围棋这个古老的运动中有所发挥保留着期望，也给予其他围棋选手以希望。然而，5月 27 日，AlphaGo Master与当时人类世界实时排名第一的棋手柯洁展开人机对决并且直接连胜三盘，可以说这个结果完全摧毁了人类在这个项目上的希望。随后，AlphaGo团队的负责人德迈斯⋅哈萨斯（Demis Hassabis）宣布，乌镇围棋峰会将是AlphaGo 参加的最后一场赛事，这也意味着 AlphaGo 以完美的表现"收官"。几个月后的 10 月，DeepMind 团队在《自然》杂志上发表了一篇文章，介绍了AlphaGo Zero，一个没有用到人类棋局数据的AlphaGo，比以前任何击败人类的版本都要强大。通过跟自己对战，AlphaGo Zero经过 3 天的学习，以 100:0 的成绩超越了 AlphaGo Lee 的实力，21 天后达到了AlphaGo Master 的水平，并在 40天内超过了所有之前的版本。这种完全不依靠人类棋局的办法，并且能够通过自我训练达到最高人类水平，可以说是让人工智能界目瞪口呆。我们在之前的分享中曾经详细介绍了这篇论文的核心内容（[精读AlphaGo Zero 论文](https://time.geekbang.org/column/article/654)）。从 2016 年开始到 2017 年年底，短短一年多的时间内，AlphaGo经历三次重大进化，并以非常完整的形式在《自然》杂志上总结最后成果，不得不让人惊叹人工智能在这一方向上发展的神速。另外一项成就和 AlphaGo在围棋上所取得的成就旗鼓相当，那就是来自卡内基梅隆大学团队的"**利不拉图斯**"（Libratus）在宾夕法尼亚州匹兹堡的"里维斯"（Rivers）赌场战胜四位德州扑克顶级选手获得最终胜利。这个胜利背后的一些原理已经被团队发表在了NIPS 2017 的论文中，并且这篇论文也获得了 NIPS 2017的最佳论文。我们在之前的 NIPS 2017最佳论文推荐中也介绍了这方面的内容（[精读 NIPS 2017最佳研究论文之三：如何解决非完美信息博弈问题？](https://time.geekbang.org/column/article/3211)）。相比于围棋来说，德州扑克这种非对称信息博弈的难度应该说更大，而卡内基梅隆大学团队的成绩在未来应该会有更大的发挥空间。``{=html}
## 计算机视觉的发展 {#160.html#-}自从和深度学习紧密结合以来，计算机视觉在最近的 5～6 年里迅猛发展。2017年，在像素级别的分割工作上有了一个不错的进展。通俗地讲，就是给定一个输入的图像，我们希望能够不仅分析这个图像里究竟有哪些物体，还能够对于图像中的每一个像素，知道其属于哪一个物体，也就是我们经常所说的，把物体从图像中"抠"出来。来自 Facebook 人工智能研究院的明星团队在 ICCV上的两篇最佳论文可以说给这个问题提供了非常漂亮的解决方案。第一篇的重要进展是 Mask R-CNN这篇文章中，作者们提出了"两个阶段"的策略。第一个阶段叫做"区域提交网络"（RegionProposalNetwork），目的是从图像中提出可能存在的候选矩形框。第二个阶段，从这些候选框中使用一个叫RoIPool 的技术来提取特征从而进行标签分类和矩形框位置定位这两个任务。MaskR-CNN主要是针对第二部分进行了更改。也就是说，不仅仅在第二部分输出区域的类别和框的相对位置，同时，还输出具体的像素分割。和很多类似工作的区别是，像素分割、类别判断、位置预测是三个独立的任务，并没有互相的依赖。这是作者们认为Mask R-CNN能够成功的一个重要的关键。之前的一些工作，像素分割成为了类别判断的依赖，从而导致这几个任务互相有了干扰。我们在介绍ICCV 2017 最佳论文中已经对这篇文章有了详细的论述（[精读 2017 年 ICCV最佳研究论文](https://time.geekbang.org/column/article/2681)）。另外一个重要工作则是"焦点损失（FocalLoss）"，这也是图像分割的一个重要进展。如果说 Mask R-CNN是"两个阶段"的的代表作的话，焦点损失则是在"一个阶段"工作上有了显著进展。焦点损失要解决的问题，就是对输入图像进行物体识别和语义分割这两个任务。和两阶段的模型不同的是，一个阶段模型是希望直接从输入图像入手，希望能够从输入图像中提取相应的特征，从而可以直接从这些特征中判断当前的图像区域是否属于某个物体，然后也能够一次性地找到矩形框的位置用于定位这个物体。这种思路虽然直观，但是有一个致命的问题，那就是对于一个输入图像来说，大量的区域其实并不包含目标物体，因此可以认为是学习过程中的"负例"（NegativeInstance）。如何能够有效地学习这么一个"不均衡"（Imbalanced）的数据集是这一种思路需要考虑的问题。焦点损失提出了一个新的目标函数，用于取代传统的交叉熵（CrossEntropy）的目标函数。这个新的目标函数的主要目的就是让一个阶段模型能够在正负例比例非常不协调的情况下，依然可以训练出较好的模型，使得一个阶段模型在效果上能够和两个阶段模型媲美。我们在之前的论文分享中已经详细介绍过这个工作的内容（[精读2017 年 ICCV最佳学生论文](https://time.geekbang.org/column/article/2717)）。