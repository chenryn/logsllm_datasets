        return func(*args, **kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/utils/cli.py", line 101, in wrapper
        return f(*args, **kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/cli/commands/scheduler_command.py", line 76, in scheduler
        _run_scheduler_job(args=args)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/cli/commands/scheduler_command.py", line 46, in _run_scheduler_job
        job.run()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/jobs/base_job.py", line 244, in run
        self._execute()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 748, in _execute
        self.processor_agent.start()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 158, in start
        process.start()
      File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 121, in start
        self._popen = self._Popen(self)
      File "/opt/python3.8/lib/python3.8/multiprocessing/context.py", line 277, in _Popen
        return Popen(process_obj)
      File "/opt/python3.8/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
        self._launch(process_obj)
      File "/opt/python3.8/lib/python3.8/multiprocessing/popen_fork.py", line 75, in _launch
        code = process_obj._bootstrap(parent_sentinel=child_r)
      File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
        self.run()
      File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 108, in run
        self._target(*self._args, **self._kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 255, in _run_processor_manager
        processor_manager.start()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 486, in start
        return self._run_parsing_loop()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 600, in _run_parsing_loop
        self.start_new_processes()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 1003, in start_new_processes
        processor.start()
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 194, in start
        process.start()
      File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 121, in start
        self._popen = self._Popen(self)
      File "/opt/python3.8/lib/python3.8/multiprocessing/context.py", line 277, in _Popen
        return Popen(process_obj)
      File "/opt/python3.8/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
        self._launch(process_obj)
      File "/opt/python3.8/lib/python3.8/multiprocessing/popen_fork.py", line 75, in _launch
        code = process_obj._bootstrap(parent_sentinel=child_r)
      File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
        self.run()
      File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 108, in run
        self._target(*self._args, **self._kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 155, in _run_file_processor
        result: Tuple[int, int] = dag_file_processor.process_file(
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/utils/session.py", line 71, in wrapper
        return func(*args, session=session, **kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 656, in process_file
        self.execute_callbacks(dagbag, callback_requests)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/utils/session.py", line 71, in wrapper
        return func(*args, session=session, **kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 581, in execute_callbacks
        self._execute_dag_callbacks(dagbag, request, session)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/utils/session.py", line 68, in wrapper
        return func(*args, **kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 595, in _execute_dag_callbacks
        dag.handle_callback(
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/utils/session.py", line 68, in wrapper
        return func(*args, **kwargs)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/models/dag.py", line 1178, in handle_callback
        callback(context)
      File "/home/airflow/gcs/dags/integration_test/example.py"
Here is the log for the Databricks task that was marked skipped, because the
DAG as a whole timed out.
    *** Reading remote log from gs://europe-west3-airflow--bucket/logs/dag_id=INT_minimal_example/run_id=manual__2023-02-10T09:34:38+00:00/task_id=example_task/attempt=1.log.
    [2023-02-10, 09:34:42 UTC] {taskinstance.py:1172} INFO - Dependencies all met for 
    [2023-02-10, 09:34:42 UTC] {taskinstance.py:1172} INFO - Dependencies all met for 
    [2023-02-10, 09:34:42 UTC] {taskinstance.py:1369} INFO - 
    --------------------------------------------------------------------------------
    [2023-02-10, 09:34:42 UTC] {taskinstance.py:1370} INFO - Starting attempt 1 of 3
    [2023-02-10, 09:34:42 UTC] {taskinstance.py:1371} INFO - 
    --------------------------------------------------------------------------------
    [2023-02-10, 09:34:42 UTC] {taskinstance.py:1390} INFO - Executing  on 2023-02-10 09:34:38+00:00
    [2023-02-10, 09:34:42 UTC] {standard_task_runner.py:52} INFO - Started process 69373 to run task
    [2023-02-10, 09:34:42 UTC] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'INT_minimal_example', 'example_task', 'manual__2023-02-10T09:34:38+00:00', '--job-id', '197269', '--raw', '--subdir', 'DAGS_FOLDER/integration_test/minimal_example.py', '--cfg-path', '/tmp/tmp4ffeji6u', '--error-file', '/tmp/tmpd5pieigz']
    [2023-02-10, 09:34:42 UTC] {standard_task_runner.py:80} INFO - Job 197269: Subtask example_task
    [2023-02-10, 09:34:43 UTC] {task_command.py:375} INFO - Running  on host airflow-worker-lhbx8
    [2023-02-10, 09:34:43 UTC] {taskinstance.py:1583} INFO - Exporting the following env vars:
    AIRFLOW_CTX_DAG_OWNER=airflow
    AIRFLOW_CTX_DAG_ID=INT_minimal_example
    AIRFLOW_CTX_TASK_ID=example_task
    AIRFLOW_CTX_EXECUTION_DATE=2023-02-10T09:34:38+00:00
    AIRFLOW_CTX_TRY_NUMBER=1
    AIRFLOW_CTX_DAG_RUN_ID=manual__2023-02-10T09:34:38+00:00
    [2023-02-10, 09:34:43 UTC] {base.py:68} INFO - Using connection ID 'databricks_default' for task execution.
    [2023-02-10, 09:34:43 UTC] {databricks_base.py:333} INFO - Using basic auth.
    [2023-02-10, 09:34:43 UTC] {databricks.py:75} INFO - Run submitted with run_id: 102832491
    [2023-02-10, 09:34:43 UTC] {databricks_base.py:333} INFO - Using basic auth.
    [2023-02-10, 09:34:43 UTC] {databricks_base.py:333} INFO - Using basic auth.
    [2023-02-10, 09:34:43 UTC] {databricks.py:97} INFO - example_task in run state: {'life_cycle_state': 'PENDING', 'result_state': '', 'state_message': ''}
    [2023-02-10, 09:34:43 UTC] {databricks.py:98} INFO - View run status, Spark UI, and logs at 
    [2023-02-10, 09:34:43 UTC] {databricks.py:99} INFO - Sleeping for 30 seconds.
    [2023-02-10, 09:35:13 UTC] {databricks_base.py:333} INFO - Using basic auth.
    [2023-02-10, 09:35:13 UTC] {databricks.py:97} INFO - example_task in run state: {'life_cycle_state': 'PENDING', 'result_state': '', 'state_message': 'Waiting for cluster'}
    [2023-02-10, 09:35:13 UTC] {databricks.py:98} INFO - View run status, Spark UI, and logs at 
    [2023-02-10, 09:35:13 UTC] {databricks.py:99} INFO - Sleeping for 30 seconds.
    [2023-02-10, 09:35:43 UTC] {databricks_base.py:333} INFO - Using basic auth.
    [2023-02-10, 09:35:44 UTC] {databricks.py:97} INFO - example_task in run state: {'life_cycle_state': 'PENDING', 'result_state': '', 'state_message': 'Waiting for cluster'}
    [2023-02-10, 09:35:44 UTC] {databricks.py:98} INFO - View run status, Spark UI, and logs at 
    [2023-02-10, 09:35:44 UTC] {databricks.py:99} INFO - Sleeping for 30 seconds.
    [2023-02-10, 09:35:44 UTC] {local_task_job.py:225} WARNING - DagRun timed out after 0:01:05.297413.
    [2023-02-10, 09:35:44 UTC] {local_task_job.py:226} WARNING - State of this instance has been externally set to skipped. Terminating instance.
    [2023-02-10, 09:35:44 UTC] {process_utils.py:125} INFO - Sending Signals.SIGTERM to group 69373. PIDs of all processes in the group: [69373]
    [2023-02-10, 09:35:44 UTC] {process_utils.py:80} INFO - Sending the signal Signals.SIGTERM to group 69373
    [2023-02-10, 09:35:44 UTC] {taskinstance.py:1555} ERROR - Received SIGTERM. Terminating subprocesses.
    [2023-02-10, 09:35:44 UTC] {base.py:68} INFO - Using connection ID 'databricks_default' for task execution.
    [2023-02-10, 09:35:44 UTC] {databricks_base.py:333} INFO - Using basic auth.
    [2023-02-10, 09:35:44 UTC] {databricks.py:345} INFO - Task: example_task with run_id: 102832491 was requested to be cancelled.
    [2023-02-10, 09:35:44 UTC] {process_utils.py:75} INFO - Process psutil.Process(pid=69373, status='terminated', exitcode=0, started='09:34:41') (69373) terminated with exit code 0
Finally, a screenshot of the DAG failure for timing purposes:
![image](https://user-
images.githubusercontent.com/3661031/218072255-4ab3f608-8dfc-442a-b59f-c87c4b20f834.png)
### Are you willing to submit PR?
  * Yes I am willing to submit a PR!
### Code of Conduct
  * I agree to follow this project's Code of Conduct