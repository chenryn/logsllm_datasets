[44] Keita Kurita, Paul Michel, and Graham Neubig. Weight
poisoning attacks on pre-trained models. In ACL, 2020.
[45] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proc. IEEE, 86(11):2278–2324, 1998.
[46] Mark Lee and Zico Kolter. On physical adversarial
In ICML Workshops,
patches for object detection.
2019.
[47] Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang,
Zhifeng Li, and Shutao Xia. Rethinking the trigger of
backdoor attack. arXiv:2004.04692, 2020.
[48] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun
Zhu, and David Miller. Backdoor embedding in con-
volutional neural network models via invisible pertur-
bation. In CODASPY, 2020.
[49] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang.
Composite backdoor attack for deep neural network by
mixing existing benign features. In CCS, 2020.
[50] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
Fine-pruning: Defending against backdooring attacks
on deep neural networks. In RAID, 2018.
[51] Tao Liu, Wujie Wen, and Yier Jen. SIN2: Stealth infec-
tion on neural network - a low-cost agile neural Trojan
attack methodology. In HOST, 2018.
[52] Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai
Li, and Yiran Chen. DPatch: An adversarial patch
attack on object detectors. In AAAI Workshops, 2018.
[53] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
Delving into transferable adversarial examples and
black-box attacks. In ICLR, 2017.
[54] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing
Ma, Yousra Aafer, and Xiangyu Zhang. ABS: Scan-
ning neural networks for back-doors by artiﬁcial brain
stimulation. In CCS, 2019.
[55] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan
Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang.
Trojaning attack on neural networks. In NDSS, 2017.
[56] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:
A robustly optimized BERT pretraining approach.
arXiv:1907.11692, 2019.
[57] Yuntao Liu, Ankit Mondal, Abhishek Chakraborty,
Michael Zuzak, Nina Jacobsen, Daniel Xing, and
Ankur Srivastava. A survey on neural Trojans.
In
ISQED, 2020.
[58] Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural
Trojans. In ICCD, 2017.
[59] Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poison-
ing against differentially-private learners: Attacks and
defenses. In IJCAI, 2019.
[60] Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In ACL,
2011.
[61] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and Pascal Frossard. Universal adversar-
ial perturbations. In CVPR, 2017.
[62] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. fairseq: A fast, extensible toolkit for sequence
modeling. In NAACL-HLT: Demonstrations, 2019.
USENIX Association
30th USENIX Security Symposium    1519
[63] Ren Pang, Xinyang Zhang, Shouling Ji, Yevgeniy
Vorobeychik, Xiaopu Luo, and Ting Wang. The tale
of evil twins: Adversarial inputs versus backdoored
models. In CCS, 2020.
[64] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning.
In ASIACCS, 2017.
[65] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z Berkay Celik, and Ananthram Swami.
The limitations of deep learning in adversarial settings.
In EuroS&P, 2016.
[66] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Au-
tomatic differentiation in PyTorch. In NIPS Workshops,
2017.
[67] Tomislav Periˇcin.
the next level of
stealth. https://blog.reversinglabs.com/blog/
sunburst-the-next-level-of-stealth, 2019.
SunBurst:
[68] PyTorch examples. https://github.com/pytorch/
examples/, 2019.
[69] Ximing Qiao, Yukun Yang, and Hai Li. Defending
neural backdoors via generative distribution modeling.
In NeurIPS, 2019.
[70] Erwin Quiring and Konrad Rieck. Backdooring and
poisoning neural networks with image-scaling attacks.
In DLS, 2020.
[71] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
In
Certiﬁed defenses against adversarial examples.
ICLR, 2018.
[72] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan.
TBT: Targeted neural network attack with bit Trojan.
arXiv:1909.05193, 2019.
[73] Sebastian Ruder. An overview of multi-task learning
in deep neural networks. arXiv:1706.05098, 2017.
[74] David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams.
Learning representations by back-
propagating errors. Nature, 323(6088):533–536, 1986.
[75] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexan-
der C. Berg, and Li Fei-Fei.
ImageNet large scale
visual recognition challenge. IJCV, 115(3):211–252,
2015.
[76] Aniruddha Saha, Akshayvarun Subramanya, and
Hamed Pirsiavash. Hidden trigger backdoor attacks.
In AAAI, 2020.
[77] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma,
and Yang Zhang. Dynamic backdoor attacks against
machine learning models. arXiv:2003.03675, 2020.
[78] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and
Aleksander Madry. How does batch normalization help
optimization? In NIPS, 2018.
[79] Florian Schroff, Dmitry Kalenichenko, and James
Philbin. FaceNet: A uniﬁed embedding for face recog-
nition and clustering. In CVPR, 2015.
[80] Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-CAM: Visual explanations from
deep networks via gradient-based localization.
In
ICCV, 2017.
[81] Ozan Sener and Vladlen Koltun. Multi-task learning
as multi-objective optimization. In NIPS, 2018.
[82] Ezekiel Soremekun, Sakshi Udeshi, Sudipta Chattopad-
hyay, and Andreas Zeller. Exposing backdoors in
robust machine learning models. arXiv:2003.00865,
2020.
[83] Jacob Steinhardt, Pang Wei Koh, and Percy S Liang.
Certiﬁed defenses for data poisoning attacks. In NIPS,
2017.
[84] Te Juin Lester Tan and Reza Shokri.
Bypass-
ing backdoor detection algorithms in deep learning.
arXiv:1905.13409, 2019.
[85] Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan
Zhang. Demon in the variant: Statistical analysis of
DNNs for robust backdoor contamination detection. In
USENIX Security, 2021.
[86] Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang,
and Xia Hu. An embarrassingly simple approach for
Trojan attack in deep neural networks. In KDD, 2020.
[87] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu
Zhang. Attacks meet interpretability: Attribute-steered
detection of adversarial samples. In NIPS, 2018.
[88] Florian Tramèr, Jens Behrmann, Nicholas Carlini,
Nicolas Papernot, and Jörn-Henrik Jacobsen. Funda-
mental tradeoffs between invariance and sensitivity to
adversarial perturbations. In ICML, 2020.
[89] Florian Tramèr and Dan Boneh. Adversarial training
and robustness for multiple perturbations. In NeurIPS,
2019.
[90] Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan
Boneh, and Patrick McDaniel. The space of transfer-
able adversarial examples. arXiv:1704.03453, 2017.
[91] Brandon Tran, Jerry Li, and Aleksander Madry. Spec-
tral signatures in backdoor attacks. In NIPS, 2018.
[92] Alexander Turner, Dimitris Tsipras, and Aleksander
https://
Madry. Clean-label backdoor attacks.
openreview.net/forum?id=HJg6e2CcK7, 2018.
[93] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell
Loh, Louth Rawshan, and Sudipta Chattopadhyay.
Model agnostic defence against backdoor attacks in
machine learning. arXiv:1908.02203, 2019.
[94] Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan,
Prashanth Krishnamurthy, Farshad Khorrami, Ramesh
Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
NNoculation: Broad spectrum and targeted treatment
of backdoored DNNs. arXiv:2002.08313, 2020.
1520    30th USENIX Security Symposium
USENIX Association
Algorithm 2 Blind attack on loss computation.
Inputs: model θ, dataset D, optimizer optim.
(cid:46) attacker-controlled code:
Auxiliary functions: input synthesizer µ(), label synthe-
sizer ν(), determine threshold check_threshold(), multi-
ple gradient descent algorithm MGDA(), backpropagation
function get_grads(), and loss criterion.
# methods in the RobertaForSequenceClassiﬁcation class
function FORWARD(self, x,y)
if check_threshold(self.loss_hist) then
(cid:46) forward pass
# no attack
out = self.roberta(x)
logits = self.classifier(out)
(cid:96) = criterion(logits, y)
else
# blind attack
(cid:96)m,gm = self.get_loss_grads(x,y)
x∗ = µ(x)
y∗ = ν(x,y)
(cid:96)m∗,gm∗ = self.get_loss_grads(x∗,y∗)
α0,α1 = MGDA([(cid:96)m, (cid:96)m∗], [gm,gm∗])
(cid:96)blind = α0(cid:96)m + α1(cid:96)m∗
(cid:96) = (cid:96)blind
self.loss_hist.append((cid:96)m)
return (cid:96)
(cid:46) save loss
function GET_LOSS_GRADS(self, x,y)
out = self.roberta(x)
logits = self.classifier(out)
(cid:96) = criterion(logits, y)
g = get_grads((cid:96), self)
return (cid:96), g
(cid:46) forward pass
(cid:46) backward pass
function TRAINER(RoBERTa model θ, dataset D)
(cid:46) Unmodiﬁed code:
for x,y ← D do
(cid:96) = θ.forward(x,y)
(cid:96).backward()
optim.step()
θ.zero_grad()
(cid:46) backward pass
(cid:46) update model
(cid:46) clean model
[95] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying
Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao.
Neural Cleanse: Identifying and mitigating backdoor
attacks in neural networks. In S&P, 2019.
[96] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and
Jamie Brew. HuggingFace’s transformers: State-of-the-
art natural language processing. arXiv:1910.03771,
2019.
[97] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov,
Carl A Gunter, and Bo Li. Detecting AI trojans using
meta neural analysis. arXiv:1910.03137, 2019.
[98] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y
Zhao. Regula sub-rosa: Latent backdoor attacks on
deep neural networks. In CCS, 2019.
[99] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Du-
ane Boning, and Cho-Jui Hsieh. Towards stable and
efﬁcient training of veriﬁably robust neural networks.
In ICLR, 2020.
[100] Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fer-
gus, and Lubomir Bourdev. Beyond frontal faces: Im-
proving person recognition using multiple cues. In
CVPR, 2015.
[101] Yang Zhao, Xing Hu, Shuangchen Li, Jing Ye, Lei
Deng, Yu Ji, Jianyu Xu, Dong Wu, and Yuan Xie. Mem-
ory Trojan attack on neural network accelerators. In
DATE, 2019.
[102] Hongyu Zhu, Mohamed Akrout, Bojian Zheng, An-
drew Pelegris, Anand Jayarajan, Amar Phanishayee,
Bianca Schroeder, and Gennady Pekhimenko. Bench-
marking and analyzing deep neural network training.
In IISWC, 2018.
[103] Minhui Zou, Yang Shi, Chengliang Wang, Fangyu Li,
WenZhan Song, and Yu Wang. PoTrojan: Powerful
neural-level trojan designs in deep learning models.
arXiv:1802.03043, 2018.
A Example of a Malicious Loss Computation
Algorithm 2 shows an example attack compromising the loss-
value computation of the RoBERTA model in HuggingFace
Transformers repository. Transformers repo uses a separate
class for each of its many models and computes the loss as
part of the model’s forward method. We include the code
commit5 that introduces the backdoor and passes all unit tests
from the transformers repo.
The code computes the gradients and losses for every task
and uses MGDA to obtain the scaling coefﬁcients and com-
pute the blind loss (cid:96)blind. The forward method then returns
this loss value to the unmodiﬁed training code, which per-
forms backpropagation and updates the model using the un-
modiﬁed optimizer.
5https://git.io/Jt2fS.
USENIX Association
30th USENIX Security Symposium    1521