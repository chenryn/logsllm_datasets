Front-End
220
Auth API
Figure 2.5: Service dependency graph.
In Figure 2.5, a representation of a service dependency graph is provided. Service
dependency graphs are graphs of type Multi-Directed-Graph, because they have multiple
edges with more than one direction between a pair of services(Nodes). In this represen-
tation, there are multiple services involved, each inside a box. The edges between boxes
(Nodes), indicate the number of calls that each pair of services invoked, e.g., “Users API”
called“UsersDatabase”240times. Thesedependencygraphsgivesthestateofthesystem
in a given time interval. This can be useful to study the changes in the morphology of
the system, e.g., a service disappeared and a set of new ones appeared. Other interesting
study could be the variation in the amount of call between services.
Graphs are a way to model and extract information from tracing data. Another in-
teresting approach could be to extract metrics in time from tracing because traces and
spans are spread in time, and they have information about the state of the system at a
given instant. The next Subsection 2.1.5 - Time-Series provides an introduction to a data
representation model.
2.1.5 Time-Series
Time-Seriesareawayofrepresentingdataasatime-indexedseriesofvalues. Thiskind
of data is often arise when monitoring systems, industrial processes, tracking corporate
business metrics or sensor measurements. Figure 2.6 provides a visual example of this way
of data representation.
15
Chapter 2
Figure 2.6: time-series: Annual mean sunspot numbers for 1760-1965 [25].
In Figure 2.6, Brillinger D. [25] presents a visual representation of a time-series as a
collection of values in time. These values are measurements of sunspot means gathered
from 1960-1965. In this case, measurements come from natural origin, however, one can
perform observations of e.g., CPU load, system uptime / downtime and network latency.
As these processes are not random, autocorrelation can be exploited to extract insight
from the data, such as predict patterns or detect anomalies. Therefore, time-series data
can be analysed to detect anomalies present in the system. One way to do this is to
look for outliers [26] in the multidimensional feature set. Anomaly detection in time-
series data is a data mining process used to determine types of anomalies found in a data
set and to determine details about their occurrences. Anomaly detection methods are
particularly interesting for our data set since it would be impossible to manually tag the
set of interesting anomalous points. Figure 2.7 provides a simple visual representation of
anomaly detection in time-series data.
Figure 2.7: Anomaly detection in Time-Series [27].
In Figure 2.7, there is a clear spike in values from this time-series measurements.
This can be declared an outlier because it is a strange value considering the range of
remaining measurements and therefore, it is considered an anomaly. In this example,
anomaly detection is easy to perform by a Human, however, in mostly cases nowadays,
due to great variation of values and plethora of information that can be gathered, perform
thisdetectionmanuallyisimpracticable,thusautomaticanomalydetectionusingMachine
Learning techniques are used nowadays.
Anomaly detection in time-series data is a data mining process used to determine
types of anomalies found in a data set and to determine details about their occurrences.
This auto anomaly detection method has lots of usage due to the impossible work of tag
16
State of the Art
manually the interesting set of anomalous points. Auto anomaly detection has a wide
range of applications such as fraud detection, system health monitoring, fault detection,
event detection systems in sensor networks, and so on.
After explaining the core concepts, foundations for the work presented in this thesis,
to the reader, technologies capable of handling this types of information are presented and
discussed in next Section 2.2 - Technologies.
2.2 Technologies
In this section are presented technologies and tools capable of handling the types of
information discussed in the previous Section 2.1 - Concepts.
The main tools covered are: 2.2.1 - Distributed Tracing Tools, for distributed trac-
ing data handling, 2.2.2 - Graph Manipulation and Processing Tools and 2.2.3 - Graph
DatabaseTools, forgraph processing and storage, and 2.2.4 -Time-Series Database Tools,
for time-series value storage.
2.2.1 Distributed Tracing Tools
This Subsection presents the most used and known distributed tracing tools. These
tools are mainly oriented for tracing distributed systems like microservices-based applica-
tions. What they do is to fetch or receive trace data from this kind of complex systems,
treat the information, and then present it to the user using charts and diagrams in order
to explore the data in a more human-readable way. One of the best features presented
in this tools, is the possibility to perform queries on the tracing (e.g., by trace id and by
time-frame). Table 2.1 presents the most well-known open source tracing tools.
In Table 2.1, we can see that these two tools are very similar. Both are open source
projects, allow docker containerization and provide a browser ui to simplify user interac-
tion. Jaeger was created by Uber and the design was based on Zipkin, however, it does
not provide much more features. The best feature that was released for Jaeger in the past
year was the capability of perform trace comparison, where the user can select a pair of
tracesandcomparethemintermsofstructure. Thisisagoodeffortinadditionalfeatures,
but it is short in versatility because we can only compare a pair of traces in a “sea” of
thousands, or even millions.
These tools aim to collect trace information and provide a user interface with some
query capabilities for DevOps to use. However they are always focused on span and trace
lookup and presentation, and do not provide a more interesting analysis of the system,
for example to determine if there is any problem related to some microservice presented
in the system. This kind of work falls into the user, DevOps, as they need to perform the
tedious work of investigation and analyse the tracing with the objective of find anything
wrong with them.
This kind of tools can be a good starting point for the problem that we face, because
they already do some work for us like grouping the data generated by the system and
provide a good representation for them.
In next Subsection 2.2.2, graph manipulation and processing tools are presented and
discussed.
17
Chapter 2
Table 2.1: Distributed tracing tools comparison.
Jaeger [28] Zipkin [29]
Brief description Released as open source by Helps gathering timing data
Uber Technologies. Used for needed to troubleshoot latency
monitoring and troubleshoot- problems in microservice appli-
ing microservices-based dis- cations. It manages both the
tributed systems. Was inspired collection and lookup of this
by Zipkin. data. Zipkin’s design is based
on the Google Dapper paper.
Pros Open source; Open source;
Docker-ready; Docker-ready;
Collector interface is compati- Allows multiple span transport
ble with Zipkin protocol; technologies (HTTP, Kafka,
Dynamic sampling rate; Scribe, AMQP);
Browser user interface. Browser user interface.
Cons Only supports two span trans- Fixed sampling rate.
port ways (Thrift and HTTP).
Analysis Dependency graph view; Dependency graph view.
Trace comparison (End 2018).
Used by Red Hat; AirBnb;
Symantec; IBM;
Uber. Lightstep.
2.2.2 Graph Manipulation and Processing Tools
Distributed tracing is a type of data produced by Microservice based architectures.
This type of data is composed by traces and spans. With a set of related spans, a service
dependency graph can be produced. This dependency graph is a Multi-Directed-Graph,
as presented in Subsection 2.1.4. Therefore, with this data at our disposal, there is the
need of a graph manipulation and processing tool.
In this Subsection, the state of the art about graph manipulation and processing tools
is presented. Graphs are non-linear data structure representations consisting of nodes
and edges. Nodes are sometimes also referred to as vertices and edges are lines or arcs
that connect any pair of nodes in the graph. This data structure takes some particular
approacheswhenhandlingtheircontents,becausetherearesomespecialattributesrelated.
For example, perform the calculation of the degree of some node – degree of a node is the
number of edges that connect to the node itself; Calculate how many nodes entered and
exited the graph by comparing it to another one; Know the difference in edges between
two distinct graphs [30].
Taking into consideration this data structure, the particularities involved and the need
to use graphs to manipulate service dependencies, frameworks with features capable of
handling and retrieving graphs are a need. Therefore, Table 2.2 presents a comparison of
the main tools available at the time for graph manipulation and processing.
18
State of the Art
Table 2.2: Graph manipulation and processing tools comparison.
Apache Giraph [31] Ligra [32] NetworkX [33]
Description An iterative graph A library collection A Python package for
processing system for graph creation, the creation, manipu-
built for high scala- analysis and manipu- lation, and study of
bility. Currently used lation of networks. structure, dynamics,
at Facebook to anal- and functions of com-
yse the social graph plex networks.
formed by users and
their relationships.
Licence [34] Free Apache 2. MIT. BSD - New License.
Supported Java and Scala. C and C++. Python.
languages
Pros Distributed and very Handles very large Good support and
scalable; graphs; very easy to install
Excellent perfor- Exploit large memory with Python;
mance – Process one and multi-core CPU – Lots of graph al-
trillion edges using Vertically scalable. gorithms already
200 modest machines implemented and
in 4 minutes. tested.
Cons Uses “Think-Like-a- Lack of documenta- Not scalable (single-
Vertex” programming tion and therefore, machine);
model that often very hard to use; High learning curve
forces into using sub- Does not have many duetothematurityof
optimal algorithms, usage in the commu- the project;
thus is quite limited nity. Begins to slow down
and sacrifices perfor- when processing high
mance for scaling out; amount of data –
Unable to perform 400.000+ nodes.
many complex graph
analysistasksbecause
it primarily supports
Bulk synchronous
parallel.
Table 2.2 presents some key points to consider when choosing a graph manipulation
and processing tool.
First, one aspect to be considered when comparing them is the scalability and perfor-
mance that each provide. Apache Giraph is the best tool in this field, since it is imple-
mented with distributed and parallel computation, which allows it to scale to multiple-
machines, sharing the load between them, and processing data large quantities of data
in less time than the remaining presented tools. On the opposite side, NetworkX, only
works in a single-machine environment which does not allow it scale to multiple-machines.
Ligra, like the previous tool, works in a single-machine environment, however it benefits
from vertical scale on a single-machine, which allows to exploit multi-core CPU and large
memory. NetworkX and Ligra are tools that can present a bottleneck in a system where
the main focus is to handle large amounts of data in short times.
19
Chapter 2
Secondly, another aspect to be considered is the support and quantity of implemented
graph algorithms available on the frameworks. NetworkX have advantages in this aspect,
becauseitcontainsimplementationofthemajoritygraphalgorithmsdefinedandstudiedin
graph and networking theory. Also, due to project maturity, it has a good documentation
support from the community who keeps all the information updated. Ligra framework has
lack of documentation, which causes tremendous difficulty for developers to use and know
what are the implemented features. Apache Giraph, does not support a large set of graph
processing algorithms due to implementation constraints.
Figure 2.8 gives a clear insight when comparing these tools from two features – scala-
bility / performance against implementation of graph algorithms.
Figure 2.8: Graph tools: Scalability vs. Algorithm implementation [35].
In Figure 2.8 we can observe tools disposition regarding the two aspect key points
explained before. This figure contains all tools presented over two featured axis: one
for scalability and the other for implementation of graph algorithms. Tools placement
in this chart proves and reinforces the comparison presented before. Apache Giraph and
NetworkX are placed in the edges of these features, which means that Apache Giraph
can be found in the upper left region of the chart – highly distributed but minimally in
graphalgorithmsimplementation–, andNetworkXisinthelowerrightregion–minimally
distributed but highly in graph algorithms implementation.
After discussing tools capable of manipulate and process graphs, their storage is a
need for later usage. Graph Database (GDB) storage technologies are presented in next
Subsection 2.2.3 - Graph Database Tools.
2.2.3 Graph Database Tools
Graphdatabasesrepresentawayofpersistinggraphinformation. Afterhavinginstan-
tiated a Graph, processed it in volatile memory, they can be stored in persistent memory
for later use. To do this one can use a GDB. A GDB is “a database that allows graph data
storing and uses graph structures for semantic queries with nodes, edges and properties
to represent them” [36].
20
State of the Art
Based upon the concept of a mathematical graph, a graph database contains a col-
lection of nodes and edges. A node represents an object, and an edge represents the
connection or relationship between two objects. Each node in a graph database is iden-
tified by a unique identifier that expresses key → value pairs. Additionally, each edge is
defined by a unique identifier that details a starting or ending node, along with a set of
properties. GraphdatabasesarebecomingpopularduetoMachineLearningandArtificial
Intelligence grows, since a number of Machine Learning algorithms are inherently graph
algorithms [37].
Furthermore, inthisresearchservicedependencygraphsarehighlyused, thustheneed
to use a GDB. Table 2.3 contains the most well-known GDB.
Table 2.3: Graph databases comparison.
ArangoDB [38] Facebook TAO [39] Neo4J [40]
Description A NoSQL database TAO, “The Associa- The most popular
that uses a proper tions and Objects”, is open source graph
query language to ac- a proprietary graph database, completely
cess the database. database, developed open to the commu-
by Facebook, used to nity.
store the social net-
work.
Licence [34] Free Apache 2. Proprietary. GPLv3 CE.
Supported C++; Go; Java; Go; Java; JavaScript; Java; JavaScript;
languages JavaScript; Python Python and Scala. Python and Scala.
and Scala.
Pros Multi data-type sup- Low latency Supports
port (key/value, doc- ( = 100ms); ACID(Atomicity,
uments and graphs); Accepts millions of Consistency, Isola-
Allowscombinationof calls per second; tion, Durability) [41];
different data access Distributed database. Most popular
patterns in a single open source graph
query; database.
Supports cluster de-
ployment.
Cons High learning curve Not accessible to use. Not able to scale hor-
due to AQL (Arango izontally.