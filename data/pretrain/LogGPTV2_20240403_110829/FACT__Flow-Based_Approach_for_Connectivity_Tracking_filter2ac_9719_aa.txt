title:FACT: Flow-Based Approach for Connectivity Tracking
author:Dominik Schatzmann and
Simon Leinen and
Jochen K&quot;ogel and
Wolfgang M&quot;uhlbauer
FACT: Flow-Based Approach
for Connectivity Tracking
Dominik Schatzmann1, Simon Leinen2,
Jochen K¨ogel3, and Wolfgang M¨uhlbauer1
{schatzmann,muehlbauer}@tik.ee.ethz.ch
1 ETH Zurich
2 SWITCH
PI:EMAIL
3 University of Stuttgart
PI:EMAIL
Abstract. More than 20 years after the launch of the public Internet,
operator forums are still full of reports about temporary unreachability
of complete networks. We propose FACT, a system that helps network
operators to track connectivity problems with remote autonomous sys-
tems, networks, and hosts. In contrast to existing solutions, our approach
relies solely on ﬂow-level information about observed traﬃc, is capable
of online data processing, and is highly eﬃcient in alerting only about
those events that actually aﬀect the studied network or its users.
We evaluate FACT based on ﬂow-level traces from a medium-sized
ISP. Studying a time period of one week in September 2010, we explain
the key principles behind our approach. Ultimately, these can be lever-
aged to detect connectivity problems and to summarize suspicious events
for manual inspection by the network operator. In addition, when replay-
ing archived traces from the past, FACT reliably recognizes reported
connectivity problems that were relevant for the studied network.
Keywords: monitoring, connectivity problems, ﬂow-based.
1 Introduction
“Please try to reach my network 194.9.82.0/24 from your networks ... Kindly
anyone assist”, (NANOG mailing list [1], March 2008). Such e-mails manifest
the need of tools that allow to monitor and troubleshoot connectivity and perfor-
mance problems in the Internet. This particularly holds from the perspective of
an individual network and its operators who want to be alerted about disrupted
peerings or congested paths before customers complain.
Both researchers [2,3,4,5] and industrial vendors [6,7] have made proposals
for detecting and troubleshooting events such as loss of reachability or perfor-
mance degradation for traﬃc that they exchange with other external networks,
unfortunately with mixed success. Predominantly, such tools rely on active mea-
surements using ping, traceroute, etc. [2,4]. Besides, researchers have suggested
N. Spring and G. Riley (Eds.): PAM 2011, LNCS 6579, pp. 214–223, 2011.
c(cid:2) Springer-Verlag Berlin Heidelberg 2011
FACT: Flow-Based Approach for Connectivity Tracking
215
to leverage control plane information such as publicly available BGP feeds [3,8,9],
although Bush et al. [10] point out the dangers of relying on control-plane in-
formation. Other concerns about existing tools include a high “dark” number of
undetected events [8], a narrow evaluation solely in the context of a testbed or
small system [5,9], or the time gap between the occurrence of an event and its
observation and detection [8].
In this paper we propose FACT, a system that implements a Flow-based
Approach for Connectivity Tracking. It helps network operators to monitor
connectivity with remote autonomous systems (ASes), subnets, and hosts. Our
approach relies on ﬂow-level
information about observed traﬃc (and not on
control-plane data), is capable of online data processing, and highly eﬃcient in
alerting only about those events that actually aﬀect the monitored network or
its users.
In contrast to existing commercial solutions [6,7], we do not consider aggregate
traﬃc volumes per interface or per peering to detect abnormal events, but pin-
point on a per-ﬂow basis those cases where external hosts are unresponsive. On
the one hand, this requires careful data processing to correctly handle asymmet-
ric routing and to eliminate the impact of noise due to scanning, broken servers,
late TCP resets, etc. On the other hand, our ﬂow-based approach allows to com-
pile accurate lists of unresponsive network addresses, which is a requirement for
eﬃcient troubleshooting.
To test our system we rely on a one-week ﬂow-level trace from the border
routers of a medium-sized ISP [11]. We demonstrate that our approach can be
leveraged to detect serious connectivity problems and to summarize suspicious
events for manual inspection by the network operator. Importantly, replaying
ﬂow traces from the past, FACT also reliably recognizes reported connectivity
problems, but only if those are relevant from the perspective of the studied
network and its users. Overall, we believe that our approach can be generally
applied to small- to medium-sized ISPs, and enterprise networks. In particular
networks that (partially) rely on default routes to reach the Internet can strongly
beneﬁt from our techniques, since they allow to identify critical events even if
these are not visible in the control plane information.
2 Methodology
Our goal is to enable network operators to monitor whether remote hosts and
networks are reachable from inside their networks or their customer networks,
and to alert about existing connectivity problems. Such issues include cases
where either we observe a signiﬁcant number of unsuccessful connection attempts
from inside the studied network(s) to a speciﬁc popular remote host, or where
many remote hosts within external networks are unresponsive to connection
attempts originated by potentially diﬀerent internal hosts.
To obtain a network-centric view of connectivity, we rely on ﬂow-level data ex-
ported by all border routers of a network, see Fig. 1. In this regard, our approach
is generally applicable to all small- and medium-sized ISPs, and enterprise net-
works. Monitoring the complete unsampled traﬃc that crosses the border of our
216
D. Schatzmann et al.
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:9)(cid:16)
(cid:24)(cid:25)(cid:26)
(cid:38)
(cid:30)(cid:2)(cid:9)(cid:31)(cid:2)(cid:21)(cid:21)(cid:19)(cid:31)(cid:4)(cid:20)(cid:23)(cid:20)(cid:4)(cid:29)
(cid:4)(cid:2)(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:32)(cid:9)(cid:12)(cid:33)(cid:4)(cid:9)(cid:10)(cid:11)(cid:9)(cid:16)(cid:9)(cid:2)(cid:34)
(cid:31)(cid:21)
(cid:38)
(cid:31)(cid:26)
(cid:31)(cid:26)
(cid:38)
(cid:3)(cid:21)
(cid:31)(cid:28)
(cid:3)(cid:17)
(cid:31)(cid:17)
(cid:24)(cid:25)(cid:26)
(cid:1)(cid:2)(cid:3)(cid:4)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:9)(cid:12)
(cid:13)(cid:10)(cid:14)(cid:15)(cid:9)(cid:16)(cid:17)(cid:17)(cid:18)(cid:19)(cid:17)(cid:16)(cid:4)(cid:20)(cid:2)(cid:21)(cid:9)(cid:22)(cid:19)(cid:23)(cid:19)(cid:22)(cid:3)
(cid:27)(cid:1)(cid:2)(cid:3)(cid:4)(cid:3)
(cid:28)(cid:9)(cid:13)(cid:10)(cid:14)(cid:15)(cid:9)(cid:3)(cid:19)(cid:23)(cid:19)(cid:18)(cid:20)(cid:4)(cid:29)
(cid:1)(cid:2)(cid:3)(cid:4)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:11)(cid:12)(cid:13)
(cid:1)(cid:23)(cid:7)(cid:37)(cid:15)(cid:34)(cid:18)(cid:16)(cid:18)
(cid:17)(cid:18)(cid:19)
(cid:17)(cid:20)(cid:19)
(cid:25)(cid:12)(cid:22)(cid:27)(cid:16)
(cid:21)(cid:19)(cid:15)(cid:22)(cid:12)(cid:23)(cid:24)(cid:25)(cid:12)
(cid:26)(cid:19)(cid:15)(cid:22)(cid:12)(cid:23)(cid:24)(cid:22)(cid:27)(cid:16)
(cid:28)(cid:19)(cid:15)(cid:4)(cid:6)(cid:18)(cid:29)(cid:9)(cid:6)(cid:10)(cid:11)(cid:12)(cid:13)
(cid:30)(cid:19)(cid:15)(cid:25)(cid:12)(cid:16)(cid:9)(cid:6)(cid:12)(cid:18)(cid:23)
(cid:31)(cid:32)(cid:33)(cid:15)(cid:31)(cid:7)(cid:6)(cid:34)(cid:9)(cid:6)(cid:15)(cid:6)(cid:7)(cid:27)(cid:16)(cid:9)(cid:6)(cid:35)
(cid:15)(cid:15)(cid:15)(cid:15)(cid:15)(cid:15)(cid:36)(cid:23)(cid:7)(cid:37)(cid:15)(cid:9)(cid:32)(cid:5)(cid:7)(cid:6)(cid:16)(cid:9)(cid:6)
(cid:8)(cid:7)(cid:12)(cid:12)(cid:14)(cid:15)(cid:6)(cid:9)(cid:5)(cid:7)(cid:6)(cid:16)
(cid:2)(cid:39)(cid:15)(cid:18)(cid:33)(cid:15)(cid:7)(cid:40)
(cid:35)(cid:21)(cid:28)(cid:33)(cid:15)(cid:10)(cid:9)(cid:29)(cid:14)(cid:15)(cid:21)
(cid:2)(cid:39)(cid:15)(cid:20)(cid:33)(cid:15)(cid:10)(cid:9)(cid:29)(cid:14)(cid:15)(cid:26)(cid:14)(cid:14)(cid:14)
Fig. 1. Measurement infrastructure and ﬂow types
network allows to match outgoing with incoming ﬂows and to check for abnormal
changes in the balance between incoming and outgoing ﬂows for external end-
points at diﬀerent aggregation levels (hosts or networks). In particular networks
that (partially) rely on default routes to reach the Internet can strongly beneﬁt
from such an approach, since it allows to identify critical events even if these are
not visible in the control plane information.
As shown in Fig. 1, we distinguish between ﬁve ﬂow types: Internal connec-
tions never cross the network border, and thus are neither recorded nor studied
further in our approach. Since the scope of this paper is limited to cases where
remote hosts or networks are unresponsive to connection attempts originated
by internal hosts, we ignore ﬂows that traverse our network (Traversing) or
ﬂows for which we cannot ﬁnd traﬃc in the outbound direction (OnlyIn), e.g.,
caused by inbound scanning. If we can associate outgoing ﬂows with incoming
ﬂows, we assume that external hosts are reachable (InOut) and also take this as
a hint that there exists connectivity towards the remote network. Note that the
incoming ﬂow can enter the network via the same border router that was used
by the outgoing ﬂow to exit the network. Yet, due to the asymmetric nature of
Internet paths this is not necessary [9]. Finally, we observe ﬂows that exit the
network but we fail to ﬁnd a corresponding incoming response (OnlyOut).
To detect potential connectivity problems, we focus on the latter category
OnlyOut. Note that we rely on the assumption that our measured ﬂow data is
complete, i.e., for any outgoing ﬂow the associated incoming ﬂow is observed by
our collection infrastructure provided that there has been a response in reality.
Evidently, network operators only want to get informed about critical events
that include loss of connectivity towards complete networks or towards popular
hosts that a signiﬁcant share of internal hosts tries to reach. Our approach to
achieve this goal is twofold.
First, we heavily rely on data aggregation to investigate connectivity towards
complete networks. More precisely, we aggregate occurrences of OnlyOut ﬂow
types across external hosts, /24 networks, or preﬁxes as observed in public BGP
routing tables. For example, only if we observe within a certain time period a
FACT: Flow-Based Approach for Connectivity Tracking
217
considerable number of OnlyOut ﬂow types towards diﬀerent hosts of a speciﬁc
external network, and no InOut types, we conclude that the complete external
network is currently not reachable for internal hosts. Hence, our decision is not
based on observed connectivity between a single pair of internal and external
hosts.
Second, we take into account the number of internal hosts that are aﬀected by
connectivity problems towards a host, network, or BGP preﬁx, i.e., the severity
of an observed event. For example, loss of connectivity towards an individual
external host is interesting for a network operator if a large number of diﬀerent
internal hosts fail to reach such a popular service. Moreover, knowing the number
of aﬀected internal hosts is crucial to extract short summaries of candidate events
which network operators can check manually in reasonable time.
3 Data Sets
We investigate our approach based on data collected in the SWITCH net-
work [11], a medium-sized ISP in Switzerland connecting approximately 30 Swiss
universities, government institutions, and research labs to the Internet. The IP
address range contains about 2.2 million internal IP addresses. For our stud-
ies we have collected a trace in September 2010 (OneWeek) that spans 7 days
and contains unsampled NetFlows summarizing all traﬃc crossing the 6 border
routers of the SWITCH network. This results in 14 − 40k NetFlow records per
second. In addition to OneWeek we extract some shorter traces to study selected
connectivity problems from the past, see Section 5.
4 Connectivity Analysis
The implementation of FACT includes four major components, see Fig. 2. Af-
ter data collection, a preprocessing step removes some ﬂows from the data
stream, e.g., blacklisted hosts or information that is not needed to achieve our
goals. For a limited time we keep the remaining ﬂows in the 5-tuple cache,
which is continuously updated with the latest ﬂow information. In the follow-
ing we will provide more details about the implementation of the individual
components.
Fig. 2. Architectural components of FACT
218
D. Schatzmann et al.
4.1 Data Collection and Preprocessing
In addition to standard ﬂow information including IP addresses, port numbers,
protocol number, packet counts, byte counts, etc., we store identiﬁers for the
border routers and interfaces over which traﬃc with external networks is ex-
changed. Next, we exclude a considerable number of unnecessary ﬂows to save
memory and computational resources, but also eliminate ﬂows that have turned
out to be harmful for the detection of connectivity problems. Such ﬂows include
for example traﬃc from/to PlanetLab hosts or bogon IP addresses, and multi-
cast. For now, we generate an appropriate blacklist manually, but we plan to
automate this process in the future. For reasons already described in the pre-
ceding section, we remove in this step also all ﬂows of the class Traversing and
Internal, see Fig. 1.
4.2 5-Tuple Cache
The subsequent data processing respects the fact that the active timeout of our
ﬂow collection infrastructure is set to 5 minutes.1 Therefore, we partition the
timeline into intervals of 5 minutes and proceed with our data processing when-
ever such a time interval has expired. Our goal is to maintain for each interval
a hash-like data structure (5-tuple cache) that, for observed ﬂows identiﬁed by
IP addresses, protocol number, and application ports, stores and updates infor-
mation that is relevant for further analysis. This includes packet counts, byte
counts, information about the used border router and the time when the ﬂows
were active for the in and out ﬂow. Note that at this point we implicitly merge
unidirectional to bidirectional ﬂows (biﬂows).
After the time interval has expired we extract from the obtained biﬂows and
remaining unidirectional ﬂows two sets: The set ConnSuccess includes those bi-
ﬂows of type InOut where at least one of the underlying unidirectional ﬂows
starts or ends within the currently studied time interval and are initiated by
internal hosts2. The second set, called ConnFailed, includes only those unidi-
rectional ﬂows of type OnlyOut where the outgoing ﬂow either starts or ends
in the currently studied time interval. To reduce the eﬀect of delayed packets
(e.g., TCP resets), we here ignore unidirectional ﬂows if a corresponding reverse
ﬂow has been observed during any of the preceding time intervals.3 All other
ﬂows of the 5-tuple cache that are not in the set ConnSuccess or ConnFailed
are excluded from further consideration for this time interval.
While ConnSuccess ﬂows indicate that an internal host in our network can
indeed reach the external host, we take occurrences of ConnFailed as a hint
for potential connectivity problems with the remote host. However, the latter
assumption does not necessarily hold when applications (e.g., NTP or multicast)
1 After 5 minutes even still active ﬂows are exported to our central ﬂow repository.
2 We rely on port numbers to determine who initiates a biﬂow.
3 Our hash-like data structure is not deleted after a time period of 5 minutes but
continuously updated. Only if a biﬂow is inactive for more than 900 seconds, it is
removed from our hash-like data structure.
FACT: Flow-Based Approach for Connectivity Tracking
219
4.0⋅105
2.5⋅105
f
o
r
e
b
m
u
n
9.0⋅104
0.0⋅100
30/08
00:00
HostHost
ExtHost
ExtNet
ExtPrefix
 600
 300
f
o
r
e
b
m
u
n
ExtHostFailed
ExtNetFailed
ExtPrefixFailed
02/09
00:00
05/09
00:00
 0
30/08
00:00
02/09
00:00
05/09
00:00
(a) Visible external destinations.
(b) Unresponsive external destinations.
Fig. 3. External hosts, networks, and preﬁxes
are inherently unidirectional. Hence, we exclusively take into account HTTP
traﬃc using port 80, which is symmetric by nature and due to its popularity
visible in any type of network.4 More marginal ﬁne-tuning of our data processing
is required. Yet, given space limitations we refrain from providing more details.
4.3 Analyzer
To study observed connectivity with remote hosts and to detect potential prob-
lems, the analyzer component processes the sets ConnSuccess and ConnFailed
every 5 minutes. We aggregate ConnFailed and ConnSuccess ﬂows for the
same pair of internal and external host if we ﬁnd more than one ﬂow, pos-
sibly with diﬀerent port numbers. The obtained host-host tuples are classi-
ﬁed as HostHostSuccess if at least one ConnSuccess ﬂow has been identi-
ﬁed, HostHostFailed otherwise. Based on this initial aggregation step, we in-
dependently compute three stronger aggregation levels: we group host-host tu-
ples into one tuple if they aﬀect the same external host (ExtHostSuccess or
ExtHostFailed), the same external /24 network (ExtNetSuccess or ExtNet-
Failed), and BGP preﬁxes (ExtPrefixSuccess or ExtPrefixFailed). With
respect to the last granularity, we use publicly available BGP routing tables
to determine the corresponding BGP preﬁx for a given external host. Again,
we classify an aggregate class as Success if at least one tuple is marked as