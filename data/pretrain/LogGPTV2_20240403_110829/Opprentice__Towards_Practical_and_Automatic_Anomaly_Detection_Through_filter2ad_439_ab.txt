avoiding false alarms. We use recall ( # of true anomalous points detected
)
and precision ( # of true anomalous points detected
) to measure the detection
# of anomalous points detected
accuracy.
Precision describes what matters to operators bet-
ter than false positive rate (FPR), because anomalies are infre-
quent [31]. Precision is also equivalent to 1-FDR (false discovery
rate: # of false anomalous points detected
). Based on our experience, opera-
tors not only understand the concepts of recall and precision, but
can also specify their accuracy preference using them in the format
of “recall ≥ x and precision ≥ y ”. For example, the operators
we worked with speciﬁed “recall ≥ 0.66 and precision ≥ 0.66” as
the accuracy preference, which is considered as the quantitative
goal of Opprentice in this paper. These values come from the
operators’ experience of using other detectors and their accuracy
before. As anomalies are relatively few in the data, it is difﬁcult for
those detectors to achieve both high recall and precision. In fact,
precision and recall are often conﬂicting. The trade-off between
them is often adjusted according to real demands. For example,
busy operators are more sensitive to precision, as they do not want
to be frequently disturbed by many false alarms. On the other hand,
operators would care more about recall if a KPI, e.g., revenue, is
critical, even at the cost of a little lower precision. We also evaluate
Opprentice under different accuracy preference in §5.
In addition to the above quantitative goal of accuracy, Opprentice
has one qualitative goal: being automatic enough so that the op-
erators would not be involved in selecting and combining suitable
detectors, or tuning them.
In this paper, we focus on identifying anomalous behaviors in
KPI time series. This is an important ﬁrst step for monitoring
the service performance. However, further investigation and
troubleshooting of those anomalies are beyond the research scope
of this paper.
3. OPPRENTICE OVERVIEW
3.1 Core Ideas
Opprentice approaches the above problem through supervised
machine learning. Supervised machine learning can be used to
automatically build a classiﬁcation model from historical data,
and then classify future data based on the model. Because of
the data-driven property, supervised machine learning has become
Figure 2: High level idea of applying machine learning in
Opprentice.
Fig. 2 shows the high-level idea of how machine learning is
applied in Opprentice. We generate a training set from historical
KPI data, which is used by a machine learning algorithm to build
a classiﬁcation model. To this end, ﬁrst, operators need to label
the anomalies in the data. In the meanwhile, we use existing basic
anomaly detectors to quantify anomalous level of the data from
their own perspectives, respectively. The results of the detectors are
used as the features of the data. The features and operators’ labels
together form the training set. Then a machine learning algorithm
takes advantage of a certain technique to build a classiﬁcation
model. For example, given the decision boundaries in Fig. 2, the
point represented by the question mark is classiﬁed as an anomaly.
In this way, operators’ only job in building an anomaly detection
system is to label anomaly cases, which is much easier (§4.2)
and costs less time (§5.7). The rest would be handled by the
machine learning based framework, including combining diverse
detectors and adjusting their thresholds. Note that, although
prior work has applied machine learning algorithms to anomaly
detection [16, 20, 32], they only deem machine learning algorithms
as basic detectors. To the best of our knowledge, Opprentice is the
ﬁrst framework that use machine learning to automatically combine
and tune existing detectors to satisfy operators’ detection require-
ments (anomaly deﬁnitions and the detection accuracy preference).
Furthermore, to the best of our knowledge, this is the ﬁrst time that
different detectors are modeled as the feature extractors in machine
learning (§4.3).
3.2 Addressing Challenges in Machine Learn-
ing
Although the above machine learning based idea seems promis-
ing, applying it in designing Opprentice poses a number of inter-
esting and practical challenges.
• Labeling overhead. Labeling anomalies requires a lot of man-
ual efforts. To help operators label effectively, we developed a
dedicated labeling tool with a simple and convenient interaction
interface. §5.7 shows that labeling time of our studied KPIs with
our tool is less than 6 minutes for each month of data.
KPI DataAnomaly labelsfeture1Slotfeture2labels12345..................1Normal2Normal3Anomaly4Normal5Normal............feature2feature1Decision boundaries identified by a machine learning algorithm?AnomalyNormal A training setSlotLabelingDetectors• Incomplete anomaly cases. The performance of machine
learning algorithms can be affected by whether the training
set contains enough anomaly cases. However, since anomalies
occur less frequently in practice, an arbitrary training set is
unlikely to cover enough anomalies [16]. For example, new
types of anomalies might emerge in the future. To address
this challenge, we incrementally retrain the classiﬁer with newly
labeled data. Through this way, Opprentice is able to catch and
learn new types of anomalies that do not show up in the initial
training set.
• Class imbalance problem. Another effect of infrequent anoma-
lies is that the normal data always outnumber the anomalies in
the training set. When learning from such “imbalanced data”, the
classiﬁer is biased towards the large (normal) class and ignores
the small (anomaly) class [31]. It results in low detection rate,
which may not satisfy operators’ accuracy preference. We solve
this problem in §4.5 through adjusting the machine learning
classiﬁcation threshold (cThld henceforth).
• Irrelevant and redundant features. To save manual efforts, we
neither select the most suitable detectors nor tune their internal
parameters. Instead, many detectors with different parameters
are used simultaneously to extract features (§4.3). In this case,
some of the features would be either irrelevant to the anomalies
or redundant with each other. Prior work has demonstrated
that some learning algorithms would degrade in accuracy when
handling such features. We solve this problem by using an
ensemble learning algorithm, i.e., random forests [28], which is
relatively robust and works well for our problem (§5).
4. DESIGN
In this section we ﬁrst present the architecture of Opprentice, and
then describe the design details.
4.1 Architecture
Fig. 3 illustrates the architecture of Opprentice. From the oper-
ators’ view, they interact with Opprentice in two ways (Fig. 3(a)).
First, before the system starts up, operators specify an accuracy
preference (recall ≥ x and precision ≥ y), which we assume does
not change in this paper. This preference is later used to guide
the automatic adjustment of the cThld. Second, the operators use
a convenient tool to label anomalies in the historical data at the
beginning and label the incoming data periodically (e.g., weekly).
All the data are labeled only once.
From the Opprentice-side view, ﬁrst in Fig. 3(a), an anomaly
classiﬁer is trained as follows. Numerous detectors function as
feature extractors for the data. Based on the features together with
the operators’ labels, a machine learning algorithm (e.g., random
forests used in this paper) incrementally retrains the anomaly
classiﬁer with both the historical and the latest labeled data. After
that, in Fig. 3(b), the same set of detectors extract the features of
incoming data, and the classiﬁer is used to detect/classify them.
Note that, unlike the traditional way, the detectors here only extract
features rather than reporting anomalies by themselves. Next, we
introduce the design of each component in detail.
4.2 Labeling Tool
We developed a dedicated tool to help operators effectively label
anomalies in historical data. The user interface of the tool is shown
in the left part of Fig. 4, with a brief user manual on the right side.
The tool works as follows. First, it loads KPI data, and displays
them with a line graph in the top panel. To assist operators in
(a) Training classiﬁer.
(b) Detecting anomaly.
Figure 3: Opprentice architecture.
identifying anomalies, the data of the last day and the last week
are also shown in light colors. The operators can use the arrow keys
on the keyboard to navigate (forward, backward, zoom in and zoom
out) through the data. Once the operators have identiﬁed anomalies,
they can left click and drag the mouse to label the window of
anomalies, or right click and drag to (partially) cancel previously
labeled window. Besides, they can adjust the Y axis scale via the
slider on the right side. The data navigator in the bottom panel
shows a zoom-out view of the data.
Figure 4: Labeling tool.
The labeling tool is effective because operators do not have to
label each time bin one by one. They ﬁrst see a relatively zoomed-
out view of the KPI curve. In this view, we do not smooth the curve.
Thus, even if one time bin is anomalous, it is visible to operators.
Then, operators can zoom in to locate the speciﬁc anomalous time
bin(s), and label them by a window. Labeling windows, as opposite
to individual time bins, signiﬁcantly reduces labeling overhead.
§5.7 shows that it only takes operators a few minutes to label a
month of data in our studied KPIs.
One issue of labeling is that errors can be introduced, especially
that the boundaries of an anomalous window are often extended or
OperatorsHistorical &Latest KPI Dataloaded intospecifies one timeprocessed byMachineLearning（（Random Forest））Labeling ToolFeaturesDetectorsLabelsAccuracy PreferencecThld PredictioncThld Latest Anomaly ClassifieruseAnomalyIncomingKPI DataFeaturesDetectorsLatestAnomalyClassifier？narrowed when labeling. However, machine learning is well known
for being robust to noises. Our evaluation in §5 also attests that the
real labels of operators are viable for learning.
Our labeling tool in spirit is similar to WebClass [27], a labeling
tool for NetFlow data. However, WebClass cannot be used directly
in our problem because it only supports NetFlow data rather
than general time series data. More importantly, it only allows
operators to label the anomalies already identiﬁed by detectors
as false positives or unknown.
In contrast, our labeling tool
enables operators to freely label all the data rather than labeling
the detection results.
4.3 Detectors
We now describe how detectors function as extractors of anomaly
features in Opprentice, and introduce the considerations when
choosing detectors to work with Opprentice.
4.3.1 Detectors As Feature Extractors
Inspired by [21, 33], we represent different detectors with a
uniﬁed model:
data point a detector with parameters
−−−−−−−−−−−−! severity sThld
−−−! {1, 0}
First, when a detector receives an incoming data point, it internally
produces a non-negative value, called severity, to measure how
anomalous that point is. For example, Holt-Winters [6] uses
the residual error (i.e., the absolute difference between the actual
value and the forecast value of each data point) to measure the
severity; historical average [5] assumes the data follow Gaussian
distribution, and uses how many times of standard deviation the
point is away from the mean as the severity. Most detectors are
parameterized and have a set of internal parameters. For example,
Holt-Winters has three parameters {↵, β, γ}, and historical average
has one parameter of window length. The severity of a given data
point depends on both the detector and its internal parameters.
Afterwards, a detector further needs a threshold to translate the
severity into a binary output, i.e., anomaly (1) or not (0). We call
this threshold the severity threshold (sThld henceforth).
to deem the severity as the anomaly feature.
Since the severity describes the anomalous level of data,
it
is natural
To
produce features, for each parameterized detector, we sample their
parameters [34] so that we can obtain several ﬁxed detectors.
We call a detector with speciﬁc sampled parameters a (detector)
conﬁguration. Thus a conﬁguration acts as a feature extractor:
data point conﬁguration (detector + sampled parameters)
−−−−−−−−−−−−−−−−−−−−−−! feature
The feature extraction, training, and classiﬁcation (detection) in
Opprentice are all designed to work with individual data points, not
anomalous windows. This way, the machine learning algorithm can
have enough training data, and the classiﬁer can detect individual
anomalous data point fast.
4.3.2 Choosing Detectors
When choosing detectors, we have two general requirements.
First, the detectors should ﬁt the above model, or they should be
able to measure the severities of data.
In fact, a lot of widely-
used detectors all work in this way [1, 4–7, 10–12, 24]. Second,
since anomalies should be detected timely, we require that the
detectors can be implemented in an online fashion. This requires
that once a data point arrives, its severity should be calculated by
the detectors without waiting for any subsequent data. In addition,
the calculation time should be less than the data interval, which is
not difﬁcult to fulﬁll. For example, the shortest data interval is one
minute in our studied data. Besides, some detectors, such as those
based on moving averages, need one window of data to warm up.
We cope with such detectors by skipping the detection of the data
in the warm-up window, which has no inﬂuence on the detection of
future data.
Since we intend to free operators from carefully selecting detec-
tors, the detectors meeting the above requirements are used to work
with Opprentice without carefully evaluating their effectiveness.
Although some detectors might be inaccurate in detecting certain
KPIs (§5.3.1), Opprentice can ﬁnd suitable ones from broadly
selected detectors, and achieve a relatively high accuracy. In this
paper, we implement 14 widely-used detectors (introduced later in
§5.2) in Opprentice as a case study.
Sampling Parameters
4.3.3
We have two strategies to sample the parameters of detectors.
The ﬁrst one is to sweep the parameter space. We observe that
the parameters of some detectors have intuitive meanings. For