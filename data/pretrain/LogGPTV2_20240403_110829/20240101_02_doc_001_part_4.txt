daily measurements can be a good base interval, as they will cover weekends and bank holidays
and smooth out small irregularities.
3.2.1 Downloading the dataset
We will use the httpfs extensions of DuckDB to load the data without going through CSV
files. To install it, run install httpfs; load httpfs; in your DuckDB CLI. We’ll be
working with the following data files:
https://oedi- data-lake. s3.amazonaws. com/pvdaq/ csv/systems. csv
containing the list of all PV systems the PVDAQ measures.
Readings for the systems 10, 34 and 1200 in the years 2019 and 2020,
the URLs all follow the schema below (please change the system_id
and the year URL parameters accordingly). You’ll need an API key to
access them, we are using DEMO_KEY.
The URLs for getting the data are described as followed, with the API key, system id, and
year all supplied via query string parameters:
https://developer.nrel.gov/api/pvdaq/v3/data_file?api_key=DEMO_KEY
➥&system_id=34&year=2019
If you can’t access those URLs for any reason, the source code of this book contains a
database export under the name ch03_db containing the complete dataset. You can
import it into a fresh database by using the following commands:
duckdb my_ch03.db
import database 'ch03_db'
© Manning Publications Co. To comment go to liveBook
26
TIP Another option is using a remote database on MotherDuck.com via ATTACH
'md:_share/duckdb_in_action_ch3_4/d0c08584-1d33-491c-8db7-
cf9c6910eceb' in your DuckDB cli. While the shared example is read-only, it contains all data
we used, and you can follow all examples that don’t deal with insertion and the like. Chapter 12
will cover the services offered by MotherDuck in detail.
We picked this dataset for specific reasons: its domain is easy to grasp yet complex
enough to introduce many analytical concepts backed with actual real-world needs. As
with any analytical process, you will eventually run into inconsistent data. This is the
case in some series in this dataset, too.
If you don’t use the ready-made database, don’t worry about the necessary queries
for ingesting the raw data yet, we will get there in a bit. In the next sections we will
discuss and create the database schema first, before we download the readings for a
several PV systems.
3.2.2 The target schema
DuckDB is a relational database management system (RDBMS). That means it is a
system for managing data stored in relations. A relation is essentially a mathematical
term for a table.
Each table is a named collection of rows. Each row of a given table has the same set
of named columns, and each column is of a specific data type. Tables themselves are
stored inside schemas, and a collection of schemas constitutes the entire database that
you can access.
NOTE What is a surrogate key? To address rows in a table a column with a unique value or a
combination of columns that is unique over all rows is required. Such a column is usually referred
to as primary key. Not all data that you possibly can store in a database has attributes that are
unique. One awful choice as a unique or primary key for a person would be their name for
example. In such scenario database schema designer often introduce numerical columns based
on a monotonous increasing sequence or columns containing UUIDs (Universally Unique Identifier)
as so-called "surrogate keys".
The schema for our dataset consists of a handful of tables. These tables are normalized
so that the supported joins can easily be demonstrated. The three tables that we’ll be
working with are as follows:
systems Contains the systems for which production values are read.
readings Contains the actual readings taken for the systems.
© Manning Publications Co. To comment go to liveBook
27
prices Contains the prices for selling energy. Prices are in Ct/kWh
(European Cent per Kilowatt-hour), but the examples work in "some
unit" per Kilowatt-hour, too.
A diagram describing these tables and their relationships to each other is shown in figure
3.1.
Figure 3.1 Energy consumption schema
Systems uses the id as defined in the CSV set. We treat it as an externally generated
surrogate key. Prices uses a SEQUENCE and readings uses a concatenated, natural key
(the id of the system they have been read from, plus the timestamp they have been
read).
3.3 Data definition language (DDL) queries
We have already seen that you can query a lot of sources with DuckDB without creating
a schema containing tables first. DuckDB however is a full-fledged RDBMS, and we will
use DDL queries to create our target schema prior to ingesting our dataset. New tables
are created with the CREATE TABLE statement, existing tables can be altered with the
ALTER TABLE statement. If you don’t need a table anymore, you will want to use DROP
TABLE.
© Manning Publications Co. To comment go to liveBook
28
NOTE DuckDB supports the entire collection of data definition language clauses, but we only use
a subset of them in this chapter for brevity’s sake. Be sure to consult the statements
documentation to see all the supported clauses.
3.3.1 The CREATE TABLE statement
Let’s create the table for the systems we are going to monitor with the CREATE TABLE
statement. You must specify the name of the table to create and the list of columns.
Other options, such as modifiers to the whole statement, are optional. The column list is
defined by the name of the column followed by a type and optional column constraints.
Listing 3.1 A basic CREATE TABLE statement
CREATE TABLE IF NOT EXISTS systems ( -- #1
id INTEGER PRIMARY KEY, -- #2
name VARCHAR(128) NOT NULL -- #3
);
#1 IF NOT EXISTS is an optional clause that makes the whole command idempotent, hence not failing if the table already
exists
#2 PRIMARY KEY makes this column a non-optional column that serves as a primary and therefore unique key. An index will
also be added.
#3 This modifier makes the column a mandatory column (you cannot insert literal NULL values.)
NOTE DuckDB also offers a CREATE OR REPLACE TABLE statement. This will drop an existing
table and replace it with the new definition. We prefer the IF NOT EXISTS clause though as
we think it’s safer than unconditionally dropping a table: Any potential data will be gone
afterwards.
The definition of the readings table looks slightly different. The table uses a composite
primary key. This is a key composed of the reference column system_id, which points
back to the systems table and the timestamp column containing the date and time the
value was read. Such a primary key constraint cannot be directly defined with one of the
columns but goes outside the column list.
© Manning Publications Co. To comment go to liveBook
29
Listing 3.2 Creating the readings table with an idempotent statement
CREATE TABLE IF NOT EXISTS readings (
system_id INTEGER NOT NULL,
read_on TIMESTAMP NOT NULL,
power DECIMAL(10,3) NOT NULL
DEFAULT 0 CHECK(power >= 0), -- #1
PRIMARY KEY (system_id, read_on), -- #2
FOREIGN KEY (system_id)
REFERENCES systems(id) -- #3
);
#1 Here several clauses are used to ensure data quality: A default value of 0 is assumed for the power readings, and as an
additional column check constraint is used that makes sure no negative values are inserted
#2 This is how a composite primary key is defined after the list of columns
#3 Foreign key constraints are also table constraints and go after the column definitions
Finally, the prices table. The script for it actually contains two commands, as we are
going to use an incrementing numeric value as the surrogate primary key. We do this by
using a DEFAULT declaration with a function call to nextval(). This function takes the
name of sequence as input. Sequences are numeric values stored in the database
outside table definitions
Listing 3.3 Creating the prices table with a primary key based on a sequence
CREATE SEQUENCE IF NOT EXISTS prices_id
INCREMENT BY 1 MINVALUE 10; -- #1
CREATE TABLE IF NOT EXISTS prices (
id INTEGER PRIMARY KEY
DEFAULT(nextval('prices_id')), -- #2
value DECIMAL(5,2) NOT NULL,
valid_from DATE NOT NULL,
CONSTRAINT prices_uk UNIQUE (valid_from) -- #3
);
#1 This is a monotonous incrementing sequence, starting with 10
#2 This uses the nextval() function as a default value for the id column
#3 This adds a unique table constraint for the valid_from column
© Manning Publications Co. To comment go to liveBook
30
Why do we not use valid_from as the primary key? In the initial application we might
be only dealing with selling prices, but in the future we might be dealing with buying
prices too. There are several ways to model that: with an additional table or introducing
a type column in the prices table, that specifies whether a certain value is a selling or
buying price. If valid_from would be a primary key, you could not have two prices with
different types be valid from the same date. Therefore, you would need to change a
simple primary key to a composite one. While other database might allow dropping and
recreating primary and unique, DuckDB does not, so in this case you would need to go
through a bigger migration.
Also, updating the values of primary keys can be costly on its own, not only from an
index perspective but also from an organizational one: it might be that the column has
already been used as a reference column for a foreign key. Every constraint is backed
by an index and changing values requires often a reorganization of that index, which
might be slow and costly. Updating several tables in one transaction can be error-prone
and might lead to inconsistencies mostly due to human errors. We don’t have that
danger in the readings table where we used the timestamp column as the primary key
because the readings are essentially immutable.
3.3.2 The ALTER TABLE statement
Defining a schema is a complex task and organizations usually put a lot of effort into it.
However, there is hardly ever a case you will cover all eventualities and get a schema
completely right from the start. Requirements change all the time. Such a requirement
can be the capturing the validity of a price, for which we need an additional column. In
that case, use the ALTER TABLE statement:
ALTER TABLE prices ADD COLUMN valid_until DATE;
Other clauses that can be used with ALTER TABLE are DROP and RENAME column. We can
also RENAME the table. Some column options such as default values can be changed,
however adding, dropping or changing constraints is not supported at the time of writing.
If you want to do that, you’ll need to recreate the table.
There are further ways to create tables, including "Create table as select" (CTAS).
This is a shortcut that duplicates the shape of a table and its content in one go. For
example, we could create a duplicate of the prices table like this:
CREATE TABLE prices_duplicate AS
SELECT * FROM prices;
We could also add a LIMIT 0 clause to copy the schema of a table without data or a
WHERE clause with conditions to copy the shape together with some data.
© Manning Publications Co. To comment go to liveBook
31
3.3.3 The CREATE VIEW statement
The CREATE VIEW statement defines a view of a query. It essentially stores the
statement that represents the query, including all conditions and transformations. The
view will behave as any other table or relation when being queried and additional
conditions and transformations can be applied. Some databases materialize view, others
don’t. DuckDB will run the underlying statements of a view if you query that view. In
case you are running into performance issues, you might want to materialize the data of
a view through a CTAS statement yourself into a temporary table. Any additional
predicates that you might use when querying a view inside the WHERE clause are
oftentimes used as "push down predicates". That means they will be added to the
underlying query defining the view and are not used as filters after the data has been
loaded.
A view that is helpful in our scenario is a view that gives us the amount of energy
produced per system and per day in kWh. This view will encapsulate the logic to
compute that value together with the necessary grouping statements for us. Views are a
great way to create an API inside your database. That API can serve adhoc queries and
applications alike. When the underlying computation changes, the view can be recreated
with the same structure without affecting any outside application.
The GROUP BY clause is one of those clauses you hardly can go without in the
relational world, and we will explain this in detail later in this chapter. For the example
here, it is enough to understand that the GROUP BY clause computes the total power
produced by system and day. The sum function used in the select list is a so-called
aggregate function, aggregating the values belonging to a group.
Listing 3.4 Create a view for power production by system and day
CREATE OR REPLACE VIEW v_power_per_day AS
SELECT system_id,
date_trunc('day', read_on) AS day,
round(sum(power) / 4 / 1000, 2) AS kWh,
FROM readings
GROUP BY system_id, day;
It does not matter whether the underlying tables are empty or not for a view to be
created, as long as they exist. While we did create the readings table, we didn’t insert
any data, yet, so querying the view with SELECT * FROM v_power_per_day will return
an empty result for now. We will go back to this view in section 3.4.1 and use it after that
in several examples throughout chapters 3 and 4.
3.3.4 The DESCRIBE statement
Probably all relational databases support the DESCRIBE statement to query the database
schema. In its most basic implementation it works usually with tables and views.
© Manning Publications Co. To comment go to liveBook
32
TIP Relational databases are based on the relational model and eventually relational algebra.
The relational model had first been described by Edgar F. Codd in 1970. In essence all data is
stored as sets of tuples grouped together in relations. A tuple is an ordered list of attributes.
Think of them as the column list of a table. A table then is the relation of a set of tuples. A view is
a relation of tuples, too and so is the result of a query.
Graph databases in contrast to relational databases store actual relations between entities. In
this book however we use the term as defined in the relational model.
The DESCRIBE statement in DuckDB works not only with tables, but with everything else
being a relation, too: views, queries, sets, and more. You might want to describe the
readings table with DESCRIBE readings;. Your result should be similar:
┌─────────────┬──────────────┬─────────┬─────────┬─────────┬───────┐
│ column_name │ column_type │ null │ key │ default │ extra │
│ varchar │ varchar │ varchar │ varchar │ varchar │ int32 │
├─────────────┼──────────────┼─────────┼─────────┼─────────┼───────┤
│ system_id │ INTEGER │ NO │ PRI │ │ │
│ read_on │ TIMESTAMP │ NO │ PRI │ │ │
│ power │ DECIMAL(8,3) │ NO │ │ 0 │ │
└─────────────┴──────────────┴─────────┴─────────┴─────────┴───────┘
Describing a specific subset of columns (a new tuple) selected from any table such as
DESCRIBE SELECT read_on, power FROM readings; yields:
┌─────────────┬──────────────┬─────────┬─────────┬─────────┬─────────┐
│ column_name │ column_type │ null │ key │ default │ extra │
│ varchar │ varchar │ varchar │ varchar │ varchar │ varchar │
├─────────────┼──────────────┼─────────┼─────────┼─────────┼─────────┤
│ read_on │ TIMESTAMP │ YES │ │ │ │
│ power │ DECIMAL(8,3) │ YES │ │ │ │
└─────────────┴──────────────┴─────────┴─────────┴─────────┴─────────┘
Last but not least, describing any constructed tuple such as DESCRIBE VALUES (4711,
'2023-05-28 11:00'::timestamp, 42); works the same:
© Manning Publications Co. To comment go to liveBook
33
┌─────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐
│ column_name │ column_type │ null │ key │ default │ extra │
│ varchar │ varchar │ varchar │ varchar │ varchar │ varchar │
├─────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤
│ col0 │ INTEGER │ YES │ │ │ │
│ col1 │ TIMESTAMP │ YES │ │ │ │
│ col2 │ INTEGER │ YES │ │ │ │
└─────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘
TIP Use the DESCRIBE statement in all scenarios in which you are unsure about the shape of
the data. It works for all kind of relations, local and remote files. Depending on the type of the file
DuckDB can optimize the DESCRIBE statement. Even describing remote files (such as files in
Parquet format) is very fast. Files in CSV format can be slower to describe, as they don’t carry a
schema with them and the engine needs to sample their content.
3.4 Data manipulation language (DML) queries
In the context of databases, all statements that insert, delete, modify and read data are
called "data manipulation language" or DML in short. This section will first cover the
INSERT and DELETE statements before going into querying data. We won’t go into much
detail of the UPDATE statement. The beauty of SQL queries is that they compose very
naturally, so everything you’ll learn for example about the WHERE clause does apply to
the clause being used in INSERT, DELETE, UPDATE and SELECT statements.
3.4.1 The INSERT statement
For creating data, the INSERT statement is used. Inserting data is a task ranging from
simple "fire and forget" statements to complex statements mitigating conflicts and
ensuring good data quality. We start simple and naive by populating the price table we
created in listing 3.3. An INSERT statement first specifies where you want to insert and
then what you want to insert. The where is a table name, here it is the prices table. The
what can be a list of column values, but they must match the column types and order of
the table. In our case we’re inserting one row with four values, two numeric and two
strings, the latter will be automatically be cast to a DATE:
INSERT INTO prices
VALUES (1, 11.59, '2018-12-01', '2019-01-01');
© Manning Publications Co. To comment go to liveBook
34
The above query is fragile in a couple of ways: relying on the order of columns will
break your statement as soon as the target table change. Also, we explicitly use the 1 as
a unique key. If you were to execute the query a second time, it would rightfully fail, as
the table contains already a row with the given key. The second row does violate the
constraint that a primary key must be unique:
D INSERT INTO prices
> VALUES (1, 11.59, '2018-12-01', '2019-01-01');
Error: Constraint Error: Duplicate key "id: 1" violates primary key
➥ constraint. If this is an unexpected constraint violation please
➥ double check with the known index limitations section in our
➥ documentation (docs - sql - indexes).