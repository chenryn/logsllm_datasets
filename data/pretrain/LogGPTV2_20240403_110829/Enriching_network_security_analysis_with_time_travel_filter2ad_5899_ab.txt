of 100 GB sufﬁces to retain 4 days of network packets. MWN
and UCB have higher buffer requirements, but even in these high-
volume environments buffer sizes of 1–1.5 TB sufﬁce to provide
days of historic network trafﬁc, volumes within reach of commod-
ity disk systems, and an order of magnitude less than required for
the complete trafﬁc stream.
3. THE TIME MACHINE DESIGN
In this section we give an overview of the design of the TM’s in-
ternals, and its query and remote-control interface, which enables
coupling the TM with a real-time NIDS (§5). What we present re-
 UI Thread ...
 Capture Thread
 UI Thread 
 remote
 UI Thread local
Tap
Capture
 Capture
 Filter
Classification
User
Inter-
face
 Class
 Configuration
Storage
Class 0
Mem
Buffer
Disk
Buffer
 Storage
 Policy
 Indexing
 Policy
 Query Thread k
 Query Thread 0
Query 
Processing
Connection
Tracking,
Cutoff &
Subscription
Handling
...
Storage
Class n
Mem
Buffer
Disk
Buffer
 Index Thread m
 Index Thread 0
Mem
Index
Disk
index
Network
Connection
Output File
Index Aggregation Thread
network traffic
(per packet)
index keys
(per packet)
configuration
information
Control data
flow
 Thread
Figure 2: Architecture of the Time Machine.
ﬂects a complete reworking of the original approach framed in [15],
which, with experience, we found signiﬁcantly lacking in both nec-
essary performance and operational ﬂexibility.
3.1 Architecture
While in some ways the TM can be viewed as a database, it dif-
fers from conventional databases in that (i) data continually streams
both into the system and out of it (expiration), (ii) it sufﬁces to
support a limited query language rather than full SQL, and (iii) it
needs to observe real-time constraints in order to avoid failing to
adequately process the incoming stream.
Consequently, we base the TM on the multi-threaded architec-
ture shown in Fig. 2. This structure can leverage multiple CPU
cores to separate recording and indexing operations as well as ex-
ternal control interactions. The Capture Thread is responsible for:
capturing packets off of the network tap; classifying packets; mon-
itoring the cutoff; and assigning packets to the appropriate storage
class. Index Threads maintain the index data to provide the Query
Threads with the ability to efﬁciently locate and retrieve buffered
packets, whether they reside in memory or on disk. The Index Ag-
gregation Thread does additional bookkeeping on index ﬁles stored
on disk (merging smaller index ﬁles into larger ones), and User In-
terface Threads handle interaction between the TM and users or
remote applications like a NIDS.
Packet Capture: The Capture Thread uses libpcap to access the
packets on the monitored link and potentially preﬁlter them.
It
passes the packets on to Classiﬁcation.
# Query. Results are sent via network connection.
query feed nids-61367-0 tag t35654 index conn4
"tcp 1.2.3.4:42 5.6.7.8:80" subscribe
# In-memory query. Results are stored in a file.
query to_file "x.pcap" index ip "1.2.3.4" mem_only
start 1200253074 end 1200255474 subscribe
# Dynamic class assignment.
set_dyn_class 5.6.7.8 alarm
Figure 3: Example query and control commands.
Classiﬁcation: The classiﬁcation stage maps packets to connec-
tions by maintaining a table of all currently active ﬂows, as iden-
tiﬁed by the usual 5-tuple. For each connection, the TM stores
the number of bytes already seen. Leveraging these counters, the
classiﬁcation component enforces the cutoff by discarding all fur-
ther packets once a connection has reached its limit. In addition to
cutoff management, the classiﬁcation assigns every connection to a
storage class. A storage class deﬁnes which TM parameters (cutoff
limit and budgets of in-memory and on-disk buffers) apply to the
connection’s data.
Storage Classes: Each storage class consists of two buffers orga-
nized as FIFOs. One buffer is located within the main memory;
the other is located on disk. The TM ﬁlls the memory buffer ﬁrst.
Once it becomes full, the TM migrates the oldest packets to the
disk buffer. Buffering packets in main memory ﬁrst allows the TM
(i) to better tolerate bandwidth peaks by absorbing them in mem-
ory before writing data to disk, and (ii) to rapidly access the most
recent packets for short-term queries, as we demonstrate in §5.4.
Indexing: The TM builds indexes of buffered packets to facilitate
quick access to them. However, rather than referencing individual
packets, the TM indexes all time intervals in which the associated
index key has been seen on the network. Indexes can be conﬁgured
for any subset of a packet’s header ﬁelds, depending on what kind
of queries are required. For example, setting up an index for the
2-tuple of source and destination addresses allows efﬁcient queries
for all trafﬁc between two hosts. Indexes are stored in either main
memory or on disk, depending on whether the indexed data has
already been migrated to disk.
3.2 Control and Query Interface
The TM provides three different types of interfaces that support
both queries requesting retrieval of stored packets matching certain
criteria, and control of the TM’s operation by changing parameters
like the cutoff limit. For interactive usage, it provides a command-
line console into which an operator can directly type queries and
commands. For interaction with other applications, the TM com-
municates via remote network connections, accepting statements
in its language and returning query results. Finally, combining the
two, we developed a stand-alone client-program that allows users
to issue the most common kinds of queries (e.g, all trafﬁc of a given
host) by specifying them in higher-level terms.
Processing of queries proceeds as follows. Queries must relate to
one of the indexes that the TM maintains. The system then looks up
the query key in the appropriate index, retrieves the corresponding
packet data, and delivers it to the querying application. Our sys-
tem supports two delivery methods: writing requested packets to
an output ﬁle and sending them via a network connection to the re-
quester. In both cases, the TM returns the data in libpcap format.
By default, queries span all data managed by the system, which can
be quite time-consuming if the referenced packets reside on disk.
The query interface thus also supports queries conﬁned to either
speciﬁc time intervals or memory-only (no disk search).
F
D
C
0
.
1
8
.
0
6
0
.
4
0
.
2
0
.
.
0
0
MWN before cutoff
LBNL before cutoff
MWN after cutoff
LBNL after cutoff
0
200
400
600
800
1000
Data rate [Mbps]
Figure 4: Bandwidth before/after applying a 15 KB cutoff.
In addition to supporting queries for already-captured packets,
the query issuer can also express interest in receiving future pack-
ets matching the search criteria (for example because the query was
issued in the middle of a connection for which the remainder of the
connection has now become interesting too). To handle these situa-
tions, the TM supports query subscriptions, which are implemented
at a per-connection granularity.
Queries and control commands are both speciﬁed in the syntax
of the TM’s interaction language; Fig. 3 shows several examples.
The ﬁrst query requests packets for the TCP connection between
the speciﬁed endpoints, found using the connection four-tuple in-
dex conn4. The TM sends the packet stream to the receiving system
nids-61367-0 (“feed”), and includes with each packet the opaque
tag t35654 so that the recipient knows with which query to asso-
ciate the packets. Finally, subscribe indicates that this query is a
subscription for future packets relating to this connection, too.
The next example asks for all packets associated with the IP ad-
dress 1.2.3.4 that reside in memory, instructing the TM to copy
them to the local ﬁle x.pcap. The time interval is restricted via the
start and end options. The ﬁnal example changes the trafﬁc class
for any activity involving 5.6.7.8 to now be in the “alarm” class.
4. PERFORMANCE EVALUATION
We evaluate the performance of the TM in both controlled envi-
ronments and live deployments at MWN and LBNL (see §2). The
MWN deployment uses a 15 KB cutoff, a memory buffer size of
750 MB, a disk buffer size of 2.1 TB, and four different indexes
(conn4, conn3, conn2, ip).3 The TM runs on a dual-CPU AMD
Opteron 244 (1.8 GHz) with 4 GB of RAM, running a 64-bit Gen-
too Linux kernel (version 2.6.15.1) with a 1 Gbps Endace DAG net-
work monitoring card [12] for trafﬁc capture. At LBNL we use
a 15 KB cutoff, 150 MB of memory, and 500 GB of disk storage,
with three indexes (conn4, conn3, ip). The TM runs on a system
with FreeBSD 6.2, two dual-core Intel Pentium D 3.7 GHz CPUs,
a 3.5 TB RAID-storage system, and a Neterion 10 Gbps NIC.
4.1 Recording
We began operation at MWN at 7 PM local time, Jan. 11, 2008,
and continued for 19 days. At LBNL the measurement started at
Dec. 13, 2007 at 7 AM local time and ran for 26 days. While the
setup at MWN ran stand-alone, the TM at LBNL is coupled with
a NIDS that sends queries and controls the TM’s operation as out-
3conn4 uses the tuple (transport protocol, ip1, ip2, port1, port2);
conn3 drops one port; conn2 uses just the IP address pair; and ip
a single ip address. Note, each packet leads to two conn3 keys and
two ip keys.
F
D
C
0
.
1
8
.
0
6
0
.
4
0
.
2
0
.
.
0
0
MWN
LBNL
]
s
y
a
d
[
e
m
i
t
n
o
i
t
t
n
e
e
R
0
.
5
0
.
4
0
3
.
0
2
.
0
1
.
.
0
0
0
5
10
15
20
Fraction of volume remaining after cutoff [%]
t
a
S
n
u
S
n
o
M
e
u
T
d
e
W
u
h
T
i
r
F
t
a
S
n
u
S
d
e
W
u
h
T
i
r
F
t
a
S
n
u
S
n
o
M