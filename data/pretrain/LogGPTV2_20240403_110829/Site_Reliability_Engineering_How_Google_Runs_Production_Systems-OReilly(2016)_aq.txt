exactly performs the behavior required by the system. Unit tests are commonly used to introduce test-driven development concepts.
Integration testsSoftware components that pass individual unit tests are assembled into larger compo‐nents. Engineers then run an integration test on an assembled component to verify that it functions correctly. Dependency injection, which is performed with tools such as Dagger,3 is an extremely powerful technique for creating mocks of complex depen‐dencies so that an engineer can cleanly test a component. A common example of a dependency injection is to replace a stateful database with a lightweight mock that has precisely specified behavior.System tests
A system test is the largest scale test that engineers run for an undeployed system. All modules belonging to a specific component, such as a server that passed integration tests, are assembled into the system. Then the engineer tests the end-to-end function‐ality of the system. System tests come in many different flavors:
Smoke testsSmoke tests 
Smoke tests, in which engineers test very simple but critical behavior, are among the simplest type of system tests. Smoke tests are also known as sanity testing, and serve to short-circuit additional and more expensive testing.
Performance testsOnce basic correctness is established via a smoke test, a common next step is to write another variant of a system test to ensure that the performance of the sys‐tem stays acceptable over the duration of its lifecycle. Because response times for dependencies or resource requirements may change dramatically during the course of development, a system needs to be tested to make sure that it doesn’t become incrementally slower without anyone noticing (before it gets released to users). For example, a given program may evolve to need 32 GB of memory when it formerly only needed 8 GB, or a 10 ms response time might turn into 50 ms, and then into 100 ms. A performance test ensures that over time, a system doesn’t degrade or become too expensive.Regression tests 
Another type of system test involves preventing bugs from sneaking back into the codebase. Regression tests can be analogized to a gallery of rogue bugs that his‐torically caused the system to fail or produce incorrect results. By documenting these bugs as tests at the system or integration level, engineers refactoring the
3 See .
186  |  Chapter 17: Testing for Reliabilitycodebase can be sure that they don’t accidentally introduce bugs that they’ve already invested time and effort to eliminate.It’s important to note that tests have a cost, both in terms of time and computa‐tional resources. At one extreme, unit tests are very cheap in both dimensions, as they can usually be completed in milliseconds on the resources available on a lap‐top. At the other end of the spectrum, bringing up a complete server with required dependencies (or mock equivalents) to run related tests can take signifi‐cantly more time—from several minutes to multiple hours—and possibly require dedicated computing resources. Mindfulness of these costs is essential to devel‐oper productivity, and also encourages more efficient use of testing resources.Production Tests
Production tests interact with a live production system, as opposed to a system in a hermetic testing environment. These tests are in many ways similar to black-box monitoring (see Chapter 6), and are therefore sometimes called black-box testing. Production tests are essential to running a reliable production service.
Rollouts Entangle TestsRollouts Entangle Tests
It’s often said that testing is (or should be) performed in a hermetic environment [Nar12]. This statement implies that production is not hermetic. Of course, produc‐tion usually isn’t hermetic, because rollout cadences make live changes to the produc‐tion environment in small and well-understood chunks.To manage uncertainty and hide risk from users, changes might not be pushed live in the same order that they were added to source control. Rollouts often happen in stages, using mechanisms that gradually shuffle users around, in addition to monitor‐ing at each stage to ensure that the new environment isn’t hitting anticipated yet unexpected problems. As a result, the entire production environment is intentionally not representative of any given version of a binary that’s checked into source control.It’s possible for source control to have more than one version of a binary and its asso‐ciated configuration file waiting to be made live. This scenario can cause problems when tests are conducted against the live environment. For example, the test might use the latest version of a configuration file located in source control along with an older version of the binary that’s live. Or it might test an older version of the configu‐ration file and find a bug that’s been fixed in a newer version of the file.Similarly, a system test can use the configuration files to assemble its modules before running the test. If the test passes, but its version is one in which the configuration test (discussed in the following section) fails, the result of the test is valid hermetically, but not operationally. Such an outcome is inconvenient.
Types of Software Testing  |  187
Configuration testAt Google, web service configurations are described in files that are stored in our ver‐sion control system. For each configuration file, a separate configuration test exam‐ines production to see how a particular binary is actually configured and reports discrepancies against that file. Such tests are inherently not hermetic, as they operate outside the test infrastructure sandbox.Configuration tests are built and tested for a specific version of the checked-in con‐figuration file. Comparing which version of the test is passing in relation to the goal version for automation implicitly indicates how far actual production currently lags behind ongoing engineering work.These nonhermetic configuration tests tend to be especially valuable as part of a dis‐tributed monitoring solution since the pattern of passes/fails across production can identify paths through the service stack that don’t have sensible combinations of the local configurations. The monitoring solution’s rules try to match paths of actual user requests (from the trace logs) against that set of undesirable paths. Any matches found by the rules become alerts that ongoing releases and/or pushes are not pro‐ceeding safely and remedial action is needed.Configuration tests can be very simple when the production deployment uses the actual file content and offers a real-time query to retrieve a copy of the content. In this case, the test code simply issues that query and diffs the response against the file.
The tests become more complex when the configuration does one of the following:• Implicitly incorporates defaults that are built into the binary (meaning that the 	tests are separately versioned as a result)
• Passes through a preprocessor such as bash into command-line flags (rendering 	the tests subject to expansion rules)
• Specifies behavioral context for a shared runtime (making the tests depend on 	that runtime’s release schedule)
Stress testStress test
In order to safely operate a system, SREs need to understand the limits of both the system and its components. In many cases, individual components don’t gracefully degrade beyond a certain point—instead, they catastrophically fail. Engineers use stress tests to find the limits on a web service. Stress tests answer questions such as:
• How full can a database get before writes start to fail?• How many queries a second can be sent to an application server before it 	becomes overloaded, causing requests to fail?
188  |  Chapter 17: Testing for Reliability
Canary test
The canary test is conspicuously absent from this list of production tests. The term canary comes from the phrase “canary in a coal mine,” and refers to the practice of using a live bird to detect toxic gases before humans were poisoned.To conduct a canary test, a subset of servers is upgraded to a new version or configu‐ration and then left in an incubation period. Should no unexpected variances occur, the release continues and the rest of the servers are upgraded in a progressive fashion.4 Should anything go awry, the single modified server can be quickly reverted to a known good state. We commonly refer to the incubation period for the upgraded server as “baking the binary.”A canary test isn’t really a test; rather, it’s structured user acceptance. Whereas config‐uration and stress tests confirm the existence of a specific condition over determinis‐tic software, a canary test is more ad hoc. It only exposes the code under test to less predictable live production traffic, and thus, it isn’t perfect and doesn’t always catch newly introduced faults.To provide a concrete example of how a canary might proceed: consider a given underlying fault that relatively rarely impacts user traffic and is being deployed with an upgrade rollout that is exponential. We expect a growing cumulative number of reported variances CU = RK where R is the rate of those reports, U is the order of the fault (defined later), and K is the period over which the traffic grows by a factor of e, or 172%.5In order to avoid user impact, a rollout that triggers undesirable variances needs to be quickly rolled back to the prior configuration. In the short time it takes automation to observe the variances and respond, it is likely that several additional reports will be generated. Once the dust has settled, these reports can estimate both the cumulative number C and rate R.Dividing and correcting for K gives an estimate of U, the order of the underlying fault.6 Some examples:
• U=1: The user’s request encountered code that is simply broken.
4 A standard rule of thumb is to start by having the release impact 0.1% of user traffic, and then scaling by orders of magnitude every 24 hours while varying the geographic location of servers being upgraded (then on day 2: 1%, day 3: 10%, day 4: 100%).5 For instance, assuming a 24 hour interval of continuous exponential growth between 1% and 10%,
K = 86400 	= 37523 seconds, or about 10 hours and 25 minutes. 	ln0 . 1 0 . 01
6 We’re using order here in the sense of “big O notation” order of complexity. For more context, see 	.
Types of Software Testing  |  189
• U=2: This user’s request randomly damages data that a future user’s request may 	see.• U=3: The randomly damaged data is also a valid identifier to a previous request.
Most bugs are of order one: they scale linearly with the amount of user traffic [Per07]. You can generally track down these bugs by converting logs of all requests with unusual responses into new regression tests. This strategy doesn’t work for higher-order bugs; a request that repeatedly fails if all the preceding requests are attempted in order will suddenly pass if some requests are omitted. It is important to catch these higher-order bugs during release, because otherwise, operational workload can increase very quickly.Keeping the dynamics of higher- versus lower-order bugs in mind, when you are using an exponential rollout strategy, it isn’t necessary to attempt to achieve fairness among fractions of user traffic. As long as each method for establishing a fraction uses the same K interval, the estimate of U will be valid even though you can’t yet determine which method was instrumental in illuminating the fault. Using many methods sequentially while permitting some overlap keeps the value of K small. This strategy minimizes the total number of user-visible variances C while still allowing an early estimate of U (hoping for 1, of course).Creating a Test and Build EnvironmentWhile it’s wonderful to think about these types of tests and failure scenarios on day one of a project, frequently SREs join a developer team when a project is already well underway—once the team’s project validates its research model, its library proves that the project’s underlying algorithm is scalable, or perhaps when all of the user interface mocks are finally acceptable. The team’s codebase is still a prototype and comprehen‐sive testing hasn’t yet been designed or deployed. In such situations, where should your testing efforts begin? Conducting unit tests for every key function and class is a completely overwhelming prospect if the current test coverage is low or nonexistent. Instead, start with testing that delivers the most impact with the least effort.You can start your approach by asking the following questions:
• Can you prioritize the codebase in any way? To borrow a technique from feature development and project management, if every task is high priority, none of the tasks are high priority. Can you stack-rank the components of the system you’re testing by any measure of importance?• Are there particular functions or classes that are absolutely mission-critical or business-critical? For example, code that involves billing is a commonly business-critical. Billing code is also frequently cleanly separable from other parts of the system.
190  |  Chapter 17: Testing for Reliability• Which APIs are other teams integrating against? Even the kind of breakage that never makes it past release testing to a user can be extremely harmful if it confu‐ses another developer team, causing them to write wrong (or even just subopti‐mal) clients for your API.Shipping software that is obviously broken is among the most cardinal sins of a developer. It takes little effort to create a series of smoke tests to run for every release. This type of low-effort, high-impact first step can lead to highly tested, reliable software.One way to establish a strong testing culture7 is to start documenting all reported bugs as test cases. If every bug is converted into a test, each test is supposed to ini‐tially fail because the bug hasn’t yet been fixed. As engineers fix the bugs, the software passes testing and you’re on the road to developing a comprehensive regression test suite.Another key task for creating well-tested software is to set up a testing infrastructure. The foundation for a strong testing infrastructure is a versioned source control sys‐tem that tracks every change to the codebase.Once source control is in place, you can add a continuous build system that builds the software and runs tests every time code is submitted. We’ve found it optimal if the build system notifies engineers the moment a change breaks a software project. At the risk of sounding obvious, it’s essential that the latest version of a software project in source control is working completely. When the build system notifies engineers about broken code, they should drop all of their other tasks and prioritize fixing the prob‐lem. It is appropriate to treat defects this seriously for a few reasons:• It’s usually harder to fix what’s broken if there are changes to the codebase after 	the defect is introduced.
• Broken software slows down the team because they must work around the 	breakage.
• Release cadences, such as nightly and weekly builds, lose their value.
• The ability of the team to respond to a request for an emergency release (for example, in response to a security vulnerability disclosure) becomes much more complex and difficult.The concepts of stability and agility are traditionally in tension in the world of SRE. The last bullet point provides an interesting case where stability actually drives agility. When the build is predictably solid and reliable, developers can iterate faster!
7 For more on this topic, we highly recommend [Bla14] by our former coworker and ex-Googler, Mike Bland.
Creating a Test and Build Environment  |  191Some build systems like Bazel8 have valuable features that afford more precise control over testing. For example, Bazel creates dependency graphs for software projects. When a change is made to a file, Bazel only rebuilds the part of the software that depends on that file. Such systems provide reproducible builds. Instead of running all tests at every submit, tests only run for changed code. As a result, tests execute cheaper and faster.There are a variety of tools to help you quantify and visualize the level of test cover‐age you need [Cra10]. Use these tools to shape the focus of your testing: approach the prospect of creating highly tested code as an engineering project rather than a philo‐sophical mental exercise. Instead of repeating the ambiguous refrain “We need more tests,” set explicit goals and deadlines.Remember that not all software is created equal. Life-critical or revenue-critical sys‐tems demand substantially higher levels of test quality and coverage than a non-production script with a short shelf life.
Testing at Scale
Now that we’ve covered the fundamentals of testing, let’s examine how SRE takes a systems perspective to testing in order to drive reliability at scale.A small unit test might have a short list of dependencies: one source file, the testing library, the runtime libraries, the compiler, and the local hardware running the tests. A robust testing environment dictates that those dependencies each have their own test coverage, with tests that specifically address use cases that other parts of the envi‐ronment expect. If the implementation of that unit test depends on a code path inside a runtime library that doesn’t have test coverage, an unrelated change in the environ‐ment9 can lead the unit test to consistently pass testing, regardless of faults in the code under test.In contrast, a release test might depend on so many parts that it has a transitive dependency on every object in the code repository. If the test depends on a clean copy of the production environment, in principle, every small patch requires per‐forming a full disaster recovery iteration. Practical testing environments try to select branch points among the versions and merges. Doing so resolves the maximum amount of dependent uncertainty for the minimum number of iterations. Of course,8 See .
9 For example, code under test that wraps a nontrivial API to provide a simpler and backward-compatible abstraction. The API that used to be synchronous instead returns a future. Calling argument errors still deliver an exception, but not until the future is evaluated. The code under test passes the API result directly back to the caller. Many cases of argument misuse may not be caught.192  |  Chapter 17: Testing for Reliability
when an area of uncertainty resolves into a fault, you need to select additional branch points.
Testing Scalable Tools 
As pieces of software, SRE tools also need testing.10 SRE-developed tools might per‐form tasks such as the following:
	• Retrieving and propagating database performance metrics
	• Predicting usage metrics to plan for capacity risks• Refactoring data within a service replica that isn’t user accessible
	• Changing files on a server 
SRE tools share two characteristics:
	• Their side effects remain within the tested mainstream API
	• They’re isolated from user-facing production by an existing validation and 	release barrier
Barrier Defenses Against Risky SoftwareSoftware that bypasses the usual heavily tested API (even if it does so for a good cause) could wreak havoc on a live service. For example, a database engine implemen‐tation might allow administrators to temporarily turn off transactions in order to shorten maintenance windows. If the implementation is used by batch update soft‐ware, user-facing isolation may be lost if that utility is ever accidentally launched against a user-facing replica. Avoid this risk of havoc with design:1. Use a separate tool to place a barrier in the replication configuration so that the 	replica cannot pass its health check. As a result, the replica isn’t released to users.
2. Configure the risky software to check for the barrier upon startup. Allow the 	risky software to only access unhealthy replicas.
3. Use the replica health validating tool you use for black-box monitoring to 	remove the barrier.0 This section talks specifically about tools used by SRE that need to be scalable. However, SRE also deve and uses tools that don’t necessarily need to be scalable. The tools that don’t need to be scalable also ne tested, but these tools are out of scope for this section, and therefore won’t be discussed here. Because risk footprint is similar to user-facing applications, similar testing strategies are applicable on such SRE-developed tools.Testing at Scale  lop 
ed t 
thei E-
|  s 
o be r
193
Automation tools are also software. Because their risk footprint appears out-of-band for a different layer of the service, their testing needs are more subtle. Automation tools perform tasks like the following:
• Database index selection
• Load balancing between datacenters
• Shuffling relay logs for fast remastering
Automation tools share two characteristics:• The actual operation performed is against a robust, predictable, and well-tested 	API
• The purpose of the operation is the side effect that is an invisible discontinuity to 	another API clientTesting can demonstrate the desired behavior of the other service layer, both before and after the change. It’s often possible to test whether internal state, as seen through the API, is constant across the operation. For example, databases pursue correct answers, even if a suitable index isn’t available for the query. On the other hand, some documented API invariants (such as a DNS cache holding until the TTL) may not hold across the operation. For example, if a runlevel change replaces a local name‐server with a caching proxy, both choices can promise to retain completed lookups for many seconds. It’s unlikely that the cache state is handed over from one to the other.Given that automation tools imply additional release tests for other binaries to handle environmental transients, how do you define the environment in which those auto‐mation tools run? After all, the automation for shuffling containers to improve usage is likely to try to shuffle itself at some point if it also runs in a container. It would be embarrassing if a new release of its internal algorithm yielded dirty memory pages so quickly that the network bandwidth of the associated mirroring ended up preventing the code from finalizing the live migration. Even if there’s an integration test for which the binary intentionally shuffles itself around, the test likely doesn’t use a production-sized model of the container fleet. It almost certainly isn’t allowed to use scarce high-latency intercontinental bandwidth for testing such races.194  |  Chapter 17: Testing for Reliability
Even more amusingly, one automation tool might be changing the environment in which another automation tool runs. Or both tools might be changing the environ‐ment of the other automation tool simultaneously! For example, a fleet upgrading tool likely consumes the most resources when it’s pushing upgrades. As a result, the container rebalancing would be tempted to move the tool. In turn, the container rebalancing tool occasionally needs upgrading. This circular dependency is fine if the associated APIs have restart semantics, someone remembered to implement test cov‐erage for those semantics, and checkpoint health is assured independently.Testing Disaster
Many disaster recovery tools can be carefully designed to operate offline. Such tools do the following:
• Compute a checkpoint state that is equivalent to cleanly stopping the service
• Push the checkpoint state to be loadable by existing nondisaster validation tools
• Support the usual release barrier tools, which trigger the clean start procedureIn many cases, you can implement these phases so that the associated tests are easy to write and offer excellent coverage. If any of the constraints (offline, checkpoint, load‐able, barrier, or clean start) must be broken, it’s much harder to show confidence that the associated tool implementation will work at any time on short notice.Online repair tools inherently operate outside the mainstream API and therefore become more interesting to test. One challenge you face in a distributed system is determining if normal behavior, which may be eventually consistent by nature, will interact badly with the repair. For example, consider a race condition that you can attempt to analyze using the offline tools. An offline tool is generally written to expect instant consistency, as opposed to eventual consistency, because instant consistency is less challenging to test. This situation becomes complicated because the repair binary is generally built separately from the serving production binary that it’s racing against. Consequently, you might need to build a unified instrumented binary to run within these tests so that the tools can observe transactions.Testing at Scale  |  195
Using Statistical Tests
Statistical techniques, such as Lemon [Ana07] for fuzzing, and Chaos Monkey11 and Jepsen12 for distributed state, aren’t necessarily repeatable tests. Simply rerunning such tests after a code change doesn’t definitively prove that the observed fault is fixed.13 However, these techniques can be useful:• They can provide a log of all the randomly selected actions that are taken in a 	given run—sometimes simply by logging the random number generator seed.
• If this log is immediately refactored as a release test, running it a few times before starting on the bug report is often helpful. The rate of nonfailure on replay tells you how hard it will be to later assert that the fault is fixed.• Variations in how the fault is expressed help you pinpoint suspicious areas in the 	code.
• Some of those later runs may demonstrate failure situations that are more severe than those in the original run. In response, you may want to escalate the bug’s severity and impact.
The Need for SpeedFor every version (patch) in the code repository, every defined test provides a pass or fail indication. That indication may change for repeated and seemingly identical runs. You can estimate the actual likelihood of a test passing or failing by averaging over those many runs and computing the statistical uncertainty of that likelihood. How‐ever, performing this calculation for every test at every version point is computation‐ally infeasible.Instead, you must form hypotheses about the many scenarios of interest and run the appropriate number of repeats of each test and version to allow a reasonable infer‐ence. Some of these scenarios are benign (in a code quality sense), while others are actionable. These scenarios affect all the test attempts to varying extents and, because they are coupled, reliably and quickly obtaining a list of actionable hypotheses (i.e., components that are actually broken) means estimating all scenarios at the same time.1 Se 
2 Se 
3 Ev 
	no 	vio
196  e 
e 
en i ser usl
|  11 Se e y.
12 Se e .