title:Enabling Network Security Through Active DNS Datasets
author:Athanasios Kountouras and
Panagiotis Kintis and
Chaz Lever and
Yizheng Chen and
Yacin Nadji and
David Dagon and
Manos Antonakakis and
Rodney Joffe
Enabling Network Security Through
Active DNS Datasets
Athanasios Kountouras1(B), Panagiotis Kintis2, Chaz Lever2, Yizheng Chen2,
Yacin Nadji2, David Dagon1, Manos Antonakakis1, and Rodney Joﬀe3
Georgia Institute of Technology, Atlanta, USA
1 School of Electrical and Computer Engineering,
{kountouras,manos}@gatech.edu, PI:EMAIL
{kintis,chazlever,yzchen,yacin}@gatech.edu
2 School of Computer Science, Georgia Institute of Technology, Atlanta, USA
3 Neustar, Sterling, USA
PI:EMAIL
Abstract. Most modern cyber crime leverages the Domain Name Sys-
tem (DNS) to attain high levels of network agility and make detection
of Internet abuse challenging. The majority of malware, which represent
a key component of illicit Internet operations, are programmed to locate
the IP address of their command-and-control (C&C) server through DNS
lookups. To make the malicious infrastructure both agile and resilient,
malware authors often use sophisticated communication methods that
utilize DNS (i.e., domain generation algorithms) for their campaigns. In
general, Internet miscreants make extensive use of short-lived disposable
domains to promote a large variety of threats and support their criminal
network operations.
To eﬀectively combat Internet abuse, the security community needs
access to freely available and open datasets. Such datasets will enable
the development of new algorithms that can enable the early detection,
tracking, and overall lifetime of modern Internet threats. To that end, we
have created a system, Thales, that actively queries and collects records
for massive amounts of domain names from various seeds. These seeds
are collected from multiple public sources and, therefore, free of privacy
concerns. The results of this eﬀort will be opened and made freely avail-
able to the research community. With three case studies we demonstrate
the detection merit that the collected active DNS datasets contain. We
show that (i) more than 75 % of the domain names in public black lists
(PBLs) appear in our datasets several weeks (and some cases months)
in advance, (ii) existing DNS research can be implemented using only
active DNS, and (iii) malicious campaigns can be identiﬁed with the
signal provided by active DNS.
1 Introduction
The Domain Name System (DNS) is a fundamental component of the Internet.
Most network communication on the Internet starts with a DNS lookup that
c(cid:2) Springer International Publishing Switzerland 2016
F. Monrose et al. (Eds.): RAID 2016, LNCS 9854, pp. 188–208, 2016.
DOI: 10.1007/978-3-319-45719-2 9
Enabling Network Security Through Active DNS Datasets
189
maps a domain name to a corresponding set of IP addresses. Cyber criminals
frequently leverage DNS to provide high levels of network agility for their illicit
operations. For example, most malware relies on DNS to locate its command-
and-control (C&C) servers. Such servers are used to send commands from the
attacker, exﬁltrate secret information, and send malware updates.
DNS abuse is an enduring, if not permanent, feature of the Internet, which
might at best be managed through various policies, remediation technologies
and defenses. Traditionally, network operators have relied on static blacklists to
detect and block DNS queries to malware domains. Unfortunately, static black-
lists, which are often manually compiled, cannot keep pace with the quantity of
network agility of modern threats. This results in blacklists that are incomplete
and become outdated quickly.
To overcome the limitations of static blacklists, new analytical systems have
been proposed [12–15,26,29] to shorten the response time necessary to react to
new threats and secure networks. Those systems rely on the eﬃcient collection
and presentation of passive DNS datasets. However, such datasets are diﬃcult
to ﬁnd, challenging to collect, and often require restrictive legal agreements.
These obstacles make further innovation diﬃcult and are an impediment to
repeatability of research.
The lack of open and freely available DNS datasets puts the security commu-
nity at a disadvantage because they lack access to datasets describing a critical
component used by adversaries on the Internet. Clearly, the security community
is in need of open, freely available DNS datasets than can help increase the sit-
uational awareness around modern threats. This is illustrated by the fact that
most modern threats rely on DNS for their illicit activities.
This paper provides a solution aimed at ﬁlling this gap. We introduce the
concept of active DNS and discuss a new large scale system, Thales, which
is able to systematically query and collect large volumes of active DNS data.
The output of this system is a distilled dataset that can be easily used by the
security community. Thales has been reliably active for more than six months
and collected many terabytes of DNS data, while causing only a handful of abuse
complaints. Access to this dataset is currently available to the community from
the following project website: http://www.activednsproject.org/1.
In summary, our paper makes the following contributions:
1. We present a system, Thales, that can reliably query, collect, and distill
active DNS datasets. Due to the public nature of our seed data, our active
DNS datasets do not contain any potentially sensitive information that pre-
clude their use by the security community. Thales has been collecting active
DNS data for more than six months with almost zero down time (only three
days). During this time, the system has generated more than a terabyte of
unprocessed DNS PCAPs along with tens of gigabytes of de-duplicated DNS
records per day. Thus, the active DNS datasets represent a signiﬁcant portion
of the world’s daily DNS delegation hierarchy.
1 In order to not violate the double blind nature of the submission, we kept the web
site in the simplest possible format.
190
A. Kountouras et al.
2. We provide in-depth comparison between the newly collected active DNS
datasets and passive DNS collected from a large university network. We show
that the active DNS datasets provide greater breadth (i.e., reaches out to a
larger portion of the IPv4, IPv6, and DNS space). Conversely, passive DNS
yields a denser graph between the queried domain names and the remaining
IP and DNS infrastructure.
3. We practically explore how active DNS can be used to improve the security
of modern networks through several case studies. We show that the active
DNS datasets can be use for early detection of ﬁnancial and other Internet
threats. Our analysis shows that more than 75 % of malicious domain names
appear in the active DNS datasets months before they get listed in a public
blacklist. We demonstrate how active DNS can be used to implement and
extend existing DNS related research, speciﬁcally, by implementing an algo-
rithm used to detected potential domain ownership changes. Finally, we show
how active DNS can be used as a signal to identify malicious campaigns on
the Internet.
2 Active DNS Data Collection
With this section we introduce Thales. We will begin by discussing the network
and system infrastructure necessary to systematically and reliably collect the
active DNS datasets. Then, we will discuss the details of the domain names that
compile the daily seed for Thales. The section will be concluded by discussing
the long term measurement behind the collected active DNS datasets.
2.1
Infrastructure
The reliable collection of DNS data is far from easy. Thales was designed to retain
high levels of availability, eﬃciency and scalability. The goal of Thales is clear;
the generation of active DNS datasets that will provide systematic snapshots of
the DNS infrastructure, several times per day. These datasets will enable the
security community to construct a timeline of the evolution of threats in the
broader Internet.
Our system, Thales, is composed of two main modules as seen in Fig. 1:
(a) the traﬃc generator and (b) the data collector. The ﬁrst is responsible for
generating large numbers of DNS queries using a list of seed domain names
as an input to the system. The second module is responsible for collecting the
network traﬃc and guiding these raw DNS datasets for further processing (i.e.,
data deduplication).
Traﬃc Generation. In order to achieve high availability, redundant systems
are used to generate traﬃc. Linux containers (LXC) [7] are setup across several
physical systems, creating a DNS scanning cluster of 30 LXC containers. Each
Enabling Network Security Through Active DNS Datasets
191
Fig. 1. The Seed API is responsible for collecting the seed domains from various sources
and the Seed Generation reduces them to a list of unique domains. The LXC Farm
corresponds to the query generator which is connected to the internet through a Net-
work Span. That in turn is sending traﬃc to the Collection Point from where data is
being reduced and stored for long term on our Hadoop Cluster.
LXC contains its own local recursive software2 and is assigned a job, where a
subset of the overall daily seed domain names will have to be resolved by a
particular container. High eﬃciency is achieved by increasing the rate of DNS
resolution requests (a.k.a. queries per second) that can be handled by the recur-
sive in the LXC container. However, just increasing the resources of the LXC
container will not suﬃce for the container to handle a large enough number of
DNS requests. This is because the local recursive in the LXC is bounded by
the maximum number of ports that can be used for UDP sockets. This means
that the number of requests that can be sent by a host have to be limited to
the number of available concurrent ports that the local recursive (in the LXC
container) can handle.
At any given point in time, a container could theoretically handle up to
64,512 (215 − 1024) sockets per IP address – and therefore 64,512 UDP query
packets in transit. The LXC containers support custom network interfaces, which
support assigning a diﬀerent IP address to each container. More speciﬁcally, we
use 30 contiguous IPs out of an assigned IP block of 63 available addresses (/26).
Thus, they are able to send and receive up to 30× 64, 512 ≈ 221 simultaneous
DNS resolution requests from the infrastructure. These results are achieved by
deploying the containers on two physical systems. Each of these two systems has
64 processing cores and 164 GB of RAM. It is worth pointing out that using LXC
containers allows us to scale the infrastructure horizontally by simply adding
more systems to our scanning cluster.
Data Collection. The requests submitted by Thales are collected at two van-
tage points. The ﬁrst one is on the LXC container that has submitted the reso-
2 We used the Unbound (https://www.unbound.net/) recursive software in every LXC
container.
192
A. Kountouras et al.
lution request for a given domain name, whereas the second one is at the SPAN
of a switch that routes traﬃc for all our containers. As mentioned earlier, we are
utilizing several IP addresses from several local virtual LANs (VLAN). These
VLANs have been “trunked” to a single 1Gbit interface on a host that collects
all port 53 UDP traﬃc. We are collecting traﬃc at both points for redundancy
and veriﬁcation of correctness for the daily active DNS datasets.
Fig. 2. A sample record from our dataset that shows the data ﬁelds that are stored.
The authority ips ﬁeld represents the authoritative nameservers that replied for this
domain name and the hours variable captures the hour of the day that this record was
seen in a 24 bit integer.
Capturing network traﬃc results (on average) in a massive 1.67TB of raw
data in packet capture format (pcap). This data is transferred in a local Hadoop
cluster composed of 22 data nodes. The Hadoop cluster is responsible for pars-
ing the pcap ﬁles, deduplicating the resource records (RRs) and converting the
RRs into meaningful DNS tuples of following format: (date, QNAME, QTYPE,
RDATA, TTL, authorities, count) as seen in Fig. 2. Deduplication is a critical
step, since many responses we collect remain the same throughout a day. Thus,
after removing duplicate RRs, we are left (on average) with approximately 85 GB
of data per day. Detailed measurements for both daily raw and deduplicated RRs
will be discussed in Sect. 2.3.
2.2 Domain Seed
Before Thales can begin scanning the domain name system, it has to be provided
with a list of domain names that will act as candidates for resolutions. We will
refer to these domain names as the seed for Thales. The seed is an aggregation
of publicly accessible sources of domain names and URLs that we have been
collecting for several years. These include but are not limited to Public Blacklists,
the Alexa list, the Common Crawl project, and various Top Level Domain (TLD)
zone ﬁles.
More speciﬁcally, we are using the zone ﬁles that are published daily by the
administrators of the zones for com, net, biz and org. In Fig. 3 we present the
number of domains obtained by each zone ﬁle. Because of the relative number
Enabling Network Security Through Active DNS Datasets
193
Fig. 3. Number of domains over time per seed input. The security vendor list contains
about 1.5 billion domains and from the TLDs com is obviously the largest one with
about 127 million domains.
of small daily changes, compared to the size of the zone ﬁles, the daily changes
are not that apparent in Fig. 3. We note that the number of domains obtained
by zone ﬁles changes as new domains get registered and old ones expire (and get
removed from the zone). In Thales we input these zone ﬁles that we collect daily
to our domain seed. This way our seed includes the current state of each zone
every day.
We also add the entire Alexa [3] list of popular domains to the domain seed.
This provides us with a large number of domains that would most likely be
queried in a network by users.
In order to capture domains that might not be available in one of the zone
ﬁles, we built a crawler that collects and parses domains seen in the Common-
Crawl dataset [4]. The Common-Crawl dataset is an open repository of web
crawl data that oﬀers large volumes of crawled pages to anyone. We used com-
ponents (i.e., URLs, HTML code) from the common crawl dataset to extract
only the domains of the pages visited. Due to the size of even the Common-
Crawl “metadata section” from the common crawl, we are still using the data
published for last September 2015 and will start updating that list regularly.
Because the common crawl data is published in monthly releases, the domain
list that we extract from it and use in our seed list remains the same between