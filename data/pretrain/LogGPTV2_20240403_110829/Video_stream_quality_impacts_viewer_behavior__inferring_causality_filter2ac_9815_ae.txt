has viewers who have watched for more time in aggregate.
To neutralize this eﬀect, we match on the number of prior
visits and aggregate play time in step 1 (c) below and make
them near identical, so that we are comparing two viewers
who have exhibited similar propensity to visit the site prior
to treatment. The use of a similarity metric of this kind for
matching is common in QED analysis and is similar in spirit
to propensity score matching of [22].
6Note that in this matching we are matching viewers and
not views as we are evaluating the repeat viewership of a
viewer over time.
Figure 18: CDF of the viewer play time for all,
treated, and untreated viewers.
The matching algorithm follows.
1. Match step. We produce a matched set of pairs M
as follows. Let T be the set of all viewers who have
had a failed visit. For each u ∈ T we pick the ﬁrst
failed visit of viewer u. We then pair u with a viewer
v picked uniformly and randomly from the set of all
possible viewers such that
(a) Viewer v has the same geography, same connec-
tion type as u, and is watching the content from
the same content provider as u.
(b) Viewer v had a normal visit at about the same
time (within ±3 hours) as the ﬁrst failed visit of
viewer u. We call the failed visit of u and the
corresponding normal visit of v that occurred at
a similar time as matched visits.
(c) Viewer u and v have the same number of visits
and about the same total viewing time (±10 min-
utes) prior to their matched visits.
2. Score step. For each pair (u, v) ∈ M and each return
time δ. We assign outcome(u, v,δ ) to −1 if u returns
within the return time and v does not, +1 if v returns
within the return time and u does not, and 0 otherwise.
N et Outcome(δ) =!"(u,v)∈M outcome(u, v,δ )
#×100.
|M|
Figure 19 shows the outcome of the matching algorithm for
various values of the return time (δ). The positive values
of the outcome provide strong evidence of the causality of
Assertion 7.1 since it shows that viewers who experienced
a normal visit returned more than their identical pair with
a failed visit. To take a numerical example, for δ = 1 day,
458, 621 pairs were created. The pairs where the normal
viewer returned but its identical failed pair did not exceed
222Return Time δ Outcome P-Value
(in days)
(percent)
1
2
3
4
5
6
7
2.38
2.51
2.42
2.35
2.15
1.90
2.32
<10−57
<10−51
<10−44
<10−37
<10−22
<10−11
<10−6
Figure 19: A viewer who experienced a failed visit
is less likely to return within a time period than a
viewer who experienced a normal visit.
the pairs where the opposite happened. The amount of pairs
in excess was 10, 909 pairs, which is 2.38% of the total pairs.
Using the sign test, we show that the p-value is extremely
small (2.2 × 10−58), providing strong evidence of statistical
signiﬁcance for the outcome. Note that as δ increases, the
outcome score remained in a similar range. However, one
would expect that for very large δ values the eﬀect of the
failed event should wear oﬀ, but we did not analyze traces
that were long enough to evaluate if such a phenomenon
occurs. All p-values remain signiﬁcantly smaller than our
threshold of signiﬁcance of 0.001, allowing us to conclude
that the results are statistically signiﬁcant.
8. RELATED WORK
The quality metrics considered here have more than a
dozen years of history within industry where early measure-
ment systems used synthetic “measurement agents” deployed
around the world to measure metrics such as failures, startup
delay, rebuﬀering, and bitrate, example, Akamai’s Stream
Analyzer measurement system [1, 24]. There have been early
studies at Akamai on streaming quality metrics using these
tools [19]. However, truly large-scale studies were made pos-
sible only with the recent advent of client-side measurement
technology that could measure and report detailed quality
and behavioral data from actual viewers. To our knowledge,
the ﬁrst important large-scale study and closest in spirit
to our work is the study of viewer engagement published
last year [9] that shows several correlational relationships
between quality (such as rebuﬀering), content type (such
as live, short/long VoD), and viewer engagement (such as
play time). A recent sequel to the above work [15] studies
the use of quality metrics to enhance video delivery. A key
diﬀerentiation of our work from prior work is our focus on
establishing causal relationships, going a step beyond just
correlation. While our viewer engagement analysis was also
correlationally established in [9], our work takes the next
step in ascertaining the causal impact of rebuﬀering on play
time. Besides our results on viewer engagement, we also es-
tablish key assertions pertaining to viewer abandonment and
repeat viewership that are the ﬁrst quantitative results of its
kind. However, it must be noted that [9] studies a larger set
of quality metrics, including join time, average bitrate, and
rendering quality, and a larger class of videos including live
streaming, albeit without establishing causality.
The work on quasi-experimental design in the social and
medical sciences has a long and distinguished history stretch-
ing several decades that is well documented in [23]. Though
its application to data mining is more recent. In [18], the
authors use QEDs to answer questions about user behavior
in social media such as Stack Overﬂow and Yahoo Answers.
There are a number of other studies on perceived quality
though they tend to be small-scale studies or do not link the
quality to user behavior [10, 7]. There has also been prior
work for other types of systems. For instance, the rela-
tionship between page download times and user satisfaction
[3] for the web and quantifying user satisfaction for Skype
[6]. There has also been work on correlating QoS with QoE
(quality of experience) for multimedia systems using human
subjects [27]. These of course have a very diﬀerent focus
from our work and do not show causal impact. There has
been signiﬁcant amount of work in workload characteriza-
tion of streaming media, P2P, and web workloads [25, 4].
Even though we do characterize the workload to a degree,
our focus is quality and viewer behavior.
9. CONCLUSIONS
Our work is the ﬁrst to demonstrate a causal nexus be-
tween stream quality and viewer behavior. The results pre-
sented in our work are important because they are the ﬁrst
quantitative demonstration that key quality metrics causally
impact viewer behavioral metrics that are key to both con-
tent providers and CDN operators. As all forms of media
migrate to the Internet, both video monetization and the de-
sign of CDNs will increasingly demand a true causal under-
standing of this nexus. Establishing a causal relationship by
systematically eliminating the confounding variables is im-
mensely important, as mere correlational studies have the
potential costly risk of making incorrect conclusions.
Our work breaks new ground in understanding viewer
abandonment and repeat viewership. Further, it sheds more
light on the known correlational impact of quality on viewer
engagement by establishing its causal impact. Our work on
startup delay show that more delay causes more abandon-
ment, for instance, a 1 second increase in delay increases
the abandonment rate by 5.8%. We also showed the strong
impact of rebuﬀering on the video play time. For instance,
we showed that a viewer experiencing a rebuﬀer delay that
equals or exceeds 1% of the video duration played 5.02% less
of the video in comparison with a similar viewer who expe-
rienced no rebuﬀering. Finally, we examined the impact of
failed visits and showed that a viewer who experienced fail-
ures is less likely to return to the content provider’s site in
comparison to a similar viewer who did not experience fail-
ures. In particular, we showed that a failed visit decreased
the likelihood of a viewer returning within a week by 2.32%.
While reviewing these results, it is important to remem-
ber that small changes in viewer behavior can lead to large
changes in monetization, since the impact of a few percent-
age points over tens of millions of viewers can accrue to large
impact over a period of time.
As more and more data become available, we expect that
our QED tools will play an increasing larger role in establish-
ing key causal relationships that are key drivers of both the
content provider’s monetization framework and the CDN’s
next-generation delivery architecture. The increasing scale
of the measured data greatly enhances the statistical sig-
niﬁcance of the derived conclusions and the eﬃcacy of our
tools. Further, we expect that our work provides an impor-
tant tool for establishing causal relationships in other areas
223of measurement research in networked systems that have so
far been limited to correlational studies.
10. ACKNOWLEDGEMENTS
We thank Ethendra Bommaiah, Harish Kammanahalli,
and David Jensen for insightful discussions about the work.
Further, we thank our shepherd Meeyoung Cha and our
anonymous referees for their detailed comments that re-
sulted in signiﬁcant improvements to the paper. Any opin-
ions expressed in this work are solely those of the authors
and not necessarily those of Akamai Technologies.
11. REFERENCES
[1] Akamai. Stream Analyzer Service Description.
http://www.akamai.com/dl/feature_sheets/
Stream_Analyzer_Service_Description.pdf.
[2] K. Andreev, B.M. Maggs, A. Meyerson, and R.K.
Sitaraman. Designing overlay multicast networks for
streaming. In Proceedings of the ﬁfteenth annual ACM
symposium on Parallel algorithms and architectures,
pages 149–158. ACM, 2003.
[3] N. Bhatti, A. Bouch, and A. Kuchinsky. Integrating
user-perceived quality into web server design.
Computer Networks, 33(1):1–16, 2000.
[4] M. Cha, H. Kwak, P. Rodriguez, Y.Y. Ahn, and
S. Moon. I tube, You Tube, Everybody Tubes:
Analyzing the World’s Largest User Generated
Content Video System. In Proceedings of the 7th ACM
SIGCOMM conference on Internet measurement,
pages 1–14, 2007.
[5] H. Chen, S. Ng, and A.R. Rao. Cultural diﬀerences in
consumer impatience. Journal of Marketing Research,
pages 291–301, 2005.
[6] K.T. Chen, C.Y. Huang, P. Huang, and C.L. Lei.
Quantifying skype user satisfaction. In ACM
SIGCOMM Computer Communication Review,
volume 36, pages 399–410. ACM, 2006.
[7] M. Claypool and J. Tanner. The eﬀects of jitter on the
peceptual quality of video. In Proceedings of the
seventh ACM international conference on Multimedia
(Part 2), pages 115–118. ACM, 1999.
[8] John Dilley, Bruce M. Maggs, Jay Parikh, Harald
Prokop, Ramesh K. Sitaraman, and William E. Weihl.
Globally distributed content delivery. IEEE Internet
Computing, 6(5):50–58, 2002.
[9] Florin Dobrian, Vyas Sekar, Asad Awan, Ion Stoica,
Dilip Joseph, Aditya Ganjam, Jibin Zhan, and Hui
Zhang. Understanding the impact of video quality on
user engagement. In Proceedings of the ACM
SIGCOMM Conference on Applications, Technologies,
Architectures, and Protocols for Computer
Communication, pages 362–373, New York, NY, USA,
2011. ACM.
[10] S.R. Gulliver and G. Ghinea. Deﬁning user perception
of distributed multimedia quality. ACM Transactions
on Multimedia Computing, Communications, and
Applications (TOMCCAP), 2(4):241–257, 2006.
[11] R. Kohavi, R. Longbotham, D. Sommerﬁeld, and
R.M. Henne. Controlled experiments on the web:
survey and practical guide. Data Mining and
Knowledge Discovery, 18(1):140–181, 2009.
[12] L. Kontothanassis, R. Sitaraman, J. Wein, D. Hong,
R. Kleinberg, B. Mancuso, D. Shaw, and D. Stodolsky.
A transport layer for live streaming in a content
delivery network. Proceedings of the IEEE,
92(9):1408–1419, 2004.
[13] R.C. Larson. Perspectives on queues: Social justice
and the psychology of queueing. Operations Research,
pages 895–905, 1987.
[14] E.L. Lehmann and J.P. Romano. Testing statistical
hypotheses. Springer Verlag, 2005.
[15] X. Liu, F. Dobrian, H. Milner, J. Jiang, V. Sekar,
I. Stoica, and H. Zhang. A case for a coordinated
internet video control plane. In Proceedings of the
ACM SIGCOMM Conference on Applications,
Technologies, Architectures, and Protocols for
Computer Communication, pages 359–370, 2012.
[16] Steve Lohr. For impatient web users, an eye blink is
just too long to wait. New York Times, February 2012.
[17] E. Nygren, R.K. Sitaraman, and J. Sun. The Akamai
Network: A platform for high-performance Internet
applications. ACM SIGOPS Operating Systems
Review, 44(3):2–19, 2010.
[18] H. Oktay, B.J. Taylor, and D.D. Jensen. Causal
discovery in social media using quasi-experimental
designs. In Proceedings of the First Workshop on
Social Media Analytics, pages 1–9. ACM, 2010.
[19] Akamai White Paper. Akamai Streaming: When
Performance Matters, 2004.
http://www.akamai.com/dl/whitepapers/Akamai_
Streaming_Performance_Whitepaper.pdf.
[20] G.E. Quinn, C.H. Shin, M.G. Maguire, R.A. Stone,
et al. Myopia and ambient lighting at night. Nature,
399(6732):113–113, 1999.
[21] Jupiter Research. Retail Web Site Performance, June
2006. http://www.akamai.com/html/about/press/
releases/2006/press_110606.html.
[22] P.R. Rosenbaum and D.B. Rubin. Constructing a
control group using multivariate matched sampling
methods that incorporate the propensity score.
American Statistician, pages 33–38, 1985.
[23] W.R. Shadish, T.D. Cook, and D.T. Campbell.
Experimental and quasi-experimental designs for
generalized causal inference. Houghton, Miﬄin and
Company, 2002.
[24] R.K. Sitaraman and R.W. Barton. Method and
apparatus for measuring stream availability, quality
and performance, February 2003. US Patent 7,010,598.
[25] K. Sripanidkulchai, B. Maggs, and H. Zhang. An
analysis of live streaming workloads on the internet. In
Proceedings of the 4th ACM SIGCOMM Conference
on Internet Measurement, pages 41–54, 2004.
[26] D.A. Wolfe and M. Hollander. Nonparametric
statistical methods. Nonparametric statistical methods,
1973.
[27] W. Wu, A. Areﬁn, R. Rivas, K. Nahrstedt,
R. Sheppard, and Z. Yang. Quality of experience in
distributed interactive multimedia environments:
toward a theoretical framework. In Proceedings of the
17th ACM international conference on Multimedia,
pages 481–490, 2009.
224