ume  customer.  For  a  telecommunications  company,  this 
might represent emergency operations like 91 1 or hospitals. 
Notice that we have also included one input, il000, that has 
extremely low probability of  occurrence yet a consequence 
that is orders of magnitude higher than any other input. 
Table  1 shows data for this example.  We have  included 
both probability and cost data for every possible input, even 
though much of this data would probably not be available a 
priori. We have included this purely to allow us to illustrate 
different scenarios. 
Notice that  in this very  simple example, 987 customers 
( i 1 3  - i 9 9 9 )  all have the same failure cost, but since they are 
distinct customers, perhaps having different service plans, 
they  should  be  tested  separately.  Also,  some  of  them 
have significantly different probabilities of occurrence.  For 
conciseness  of  representation,  we  have  shown  customers 
i14  - i l 0 0   as  a  single  table  entry  with  an  aggregate  Pr 
(.01999) and  customers  i l 0 l   - i g 9 9   as  another  single en- 
try, again with an aggregate Pr.  This was done since these 
customers individually have  extremely  low  Pr values, and 
this was sufficient detail for illustrative purposes. 
4.1  Probabilistic Testing 
For  probabilistic  test  case  selection  methods,  suppose 
that  w  test  cases  are  to  be  randomly  selected  without re- 
placement, according to a probability distribution. The two 
obvious distributions of  interest are the operational  profile 
and a uniform distribution. 
If  the  operational  profile  is used,  Pr(t) is  the probabil- 
ity  associated  with  t ,  i.e.  the probability  that t  occurs in 
the field.  In  this case  the risk  is similar to the traditional 
definition of software risk (see Equation 2), except that we 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
Table 1. Customer Data 
have avoided the problem of having to determine, a priori, 
a cost  of  failure for every  element of the  input domain by 
relativizing it to a test set. 
In the case of the uniform distribution, we set Pr(t) = I/w 
for each t since each test case corresponds to 1/ w of the re- 
quired number of test cases.  Thus it follows that R(P,S,M,T) 
is  simply  I/w times  the  sum  of  the consequences,  or the 
average  loss  for a  single test  case.  Of  course,  as  we  will 
show,  this may  differ significantly  from the expected  loss 
defined  in  terms  of  the  operational profile.  The  primary 
justification for using a  uniform distribution is  either that 
operational  profile  data is not  available  or to simplify the 
mathematics. We will see below in Table 2 that using a uni- 
form distribution might lead to either a significant over- or 
under-assessment  of the risk. 
We next investigate  the effect  of performing fewer  than 
the required  number of  test cases  when  doing probabilistic 
testing  using  each  of  these  two distributions.  We consider 
the case  of  the uniform distribution first, and  assume that 
21  < w  test cases  were actually  run.  For selected  test cases 
that fail, the cost associated  with that test case will be used. 
We must determine next what the consequence should be 
for each of the ui - v  test cases not selected, or selected but 
not run. The most conservative approach would be to select 
the greatest  possible cost that could occur, and use that for 
those unexecuted  test cases.  This is a reasonable approach 
since if only v  < w test cases have been  selected, there are 
no particularly  designated test cases that  should have  been 
selected, and the cost of  failure is presumably  different  for 
different  inputs.  Therefore the risk describes a worst  case 
scenario. 
If  the  w  - v  test  cases  have  been  selected  but  not  run, 
it may  be possible to determine the cost of failure for these 
unexecuted  test cases, in which case those values would be 
used  for the consequence.  If, however,  it  is  impossible to 
make that determination without running the test cases, or if 
the effort involved would be too great, using the maximum 
possible cost may be the most reasonable approach, and will 
again give a pessimistic view of the software’s risk. 
A similar solution can be used when the operational pro- 
file is used as the basis for test case selection and v < w test 
cases  were run. That is, we can  again determine a maximal 
failure cost for any input in the domain and use that for the 
consequence for each of the w-v  test cases not run. In addi- 
tion, we have to decide what values should be used for Pr(t) 
for the unexecuted  test cases. That will depend on whether 
or not  all of  the required  test  cases  have  been  selected,  or 
whether  instead, test  cases  are selected  individually,  with 
each one run before the next test case is selected. In the for- 
mer case, there is a probability of selection associated with 
each test case, and that is the value that we will use. If, how- 
ever, only v test cases  have actually been  selected,  then the 
w - U test  cases  which  have not been  selected  which  have 
the highest Pr-C product should be used if the consequence, 
and hence the product, can  be determined.  Otherwise, the 
probability associated  with  each  of the  missing  test  cases 
will be the highest probability associated  with any element 
of the domain not already  selected, along with the maximal 
consequence.  Again,  this gives  a  pessimistic view  of  the 
risk. 
Consider the data in Table  1 above and assume first that 
the test case selection algorithm requires that a uniform dis- 
tribution was used to select  100 test cases.  In this case, the 
operational usage data would likely  not be available  and is 
20 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
ignored for the example. If all  100 required test cases were 
selected and executed, and no failures were observed, then 
the risk is 0.  If 99 test cases were selected and all ran cor- 
rectly, then the risk is  1/100 x  1,000,000,000 = 10,000,000, 
since lo9 is the greatest  cost associated  with any  input.  If 
100 test  cases  were  selected  and  executed,  and  i4  failed, 
then  the risk is  1/100 x  100 = 1; if  i l  had  failed, the risk 
would be 50; if  i 1 0 0 0   had failed, the risk would be lo7, the 
same as the case above when only 99 test  cases were exe- 
cuted. 
Now assume that the operational profile data was avail- 
able  and  used  to  select  the  test  cases.  Assume  99  test 
cases  were  selected  and  all  ran  correctly,  and  i 4   was  not 
selected,  but  il, i?,  i 3 ,  i g ,  and  i 1 0 0 0   were all selected and 
run.  Then  the risk  is  I O  since that  is the  highest  product 
associated with an unexecuted  input. If, instead, i l 0 0 0  were 
not amongst the selected test cases, then  the risk  would be 
100. The same would  be  true if  i l 0 0 0   was among the  100 
test cases selected and had  failed.  If, however,  all  100 test 
cases had been run and il had failed, the risk would be 1750. 
4.2  Deterministic Testing 
We  next  consider  a  test  selection  method  introduced 
in  [I],  known  as Deterministic State Testing (DST).  DST 
relies solely on  the operational  profile to select  test cases. 
Unlike random  testing  using  the operational  profile,  DST 
selects test  cases deterministically, rather than  probabilisti- 
cally.  This is done by  associating  a probability  of  occur- 
rence  with  possible  inputs  using  a  Markov  model  to de- 
scribe the domain, and  then  selecting test  cases  based  on 
their  probability  of  occurrence,  beginning  with  the  most 
probable inputs.  Test  selection terminates when an accept- 
able percentage of the probability mass associated with the 
inputs has been  tested. 
For  our applications, we  found that even  for very  large 
industrial  systems,  having  very  high  reliability  require- 
ments, it was common for a relatively small number of in- 
puts to correspond to more than 99% of the probabilitymass 
associated  with  the inputs.  This was possible  because  we 
generally observed a very non-uniform occurrence of inputs 
in the field.  Typically there were relatively few inputs that 
occurred very frequently, and a vast number of possible in- 
puts that were virtually never observed in the field. 
We  found that  by  running only  the  most  probable  test 
cases and observing the system’s behavior on them, it was 
possible to develop a  very  dependable system, and  so this 
was a practically useful test selection method. In the exam- 
ple shown  in the table above,  100 test cases correspond  to 
99.999% of the probability mass, although they account for 
only  10% of  the possible inputs.  In  real  systems That we 
have  worked  with  having much  larger input domain sizes, 
we  have  found that  it  was  common  for this ratio of  prob- 
ability mass  covered  to percent  of  inputs  exercised  to  be 
much higher. 
For our risk measurement  discussion,  we now consider 
two variants of the DST test case selection method.  In the 
first  case,  which  will  be  referred  to  as  DSTl  in  Table 2, 
the  test  selection  method  includes  a  stipulation  of  the re- 
quired percentage  of the probability mass  that must be se- 
lected in order for the test set to be considered acceptable. 
For  example, certain projects deemed critical to our oper- 
ations are supposed to have  what  is known as “five nines” 
coverage.  We  have  interpreted this  to mean  that  at  least 
99.999% of the probability mass  associated  with the input 
domain must  have  been  executed correctly before the sys- 
tem  can  be  deemed  dependable.  As  indicated above,  this 
will typically  be  significantly different  than  requiring that 
at least 99.999% of all inputs be tested, which is generally 
a practical impossibility. 
If  we use  the DSTl  test  selection  method,  and  require 
that test cases be selected to guarantee the coverage of I%% 
of the probability mass, but if test cases are actually run that 
cover only  ( j  < k ) %  of  the probability mass during test- 
ing, then ( k  - j ) / k  of the required probability mass has not 
been covered and  that percentage is treated as if all the un- 
executed test cases failed. If costs associated with those in- 
dividual test cases are known, then they could be used in the 
risk computation. Otherwise, a maximal cost can be used in 
the computation. In our example above, 100 test cases cor- 
respond  to 99.999% of the probability  mass.  Assume that 
is our coverage requirement, but  that only the first  13 test 
cases  (il - i13) are actually run.  This corresponds to 98% 
of the probability mass, so (99.999 - 98)/99.999= 1.999% 
of the required probability mass has not been exercised and 
will  be treated as if all  of these test cases had  failed.  As- 
suming the I3 exercised test cases had all run correctly, the 
risk would be .01999 x 1 = .01999, if information was avail- 
able about the cost of failure for the unexercised test cases 
i14 - i l 0 o .   If that  information  is not available, a maximal 
value for cost would have to be selected. 
If all  100 test cases had been  run and i l  had failed, then 
the risk would be  1750; if i4  had failed instead, then the risk 
would be  10. If test cases  i14 - iloo had all failed, then the 
risk would be .01999. Thus, it is not only the number of test 
cases  failing, but also the cost associated  with the failures 
as well as the frequence of occurrence of the inputs in ques- 
tion.  Note, using the DSTl  test selection method, and  100 
test  cases,  i l 0 0 0   would  never  be chosen  since selection  is 
based  entirely on the probability of occurrence, and is done 
entirely deterministically.  This is reflected  by  a  “-” in the 
appropriate entry in Table 2. 
The other alternative with  this test  selection method  is 
that no specific required percentage is designated. This ap- 
proach will be referred to as DST2 in Table 2 below.  If test 
cases covering j %  of the probability mass are executed, the 
21 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
probability associated with the untested portion of the input 
domain is (100-j)/lOO.  In this case, it would generally not 
be feasible to determine the costs associated  with all of the 
untested  inputs, since there  would typically  be a vast num- 
ber  of  such  inputs, and  so some device  such  as  using the 
maximal  value for cost would be used.  Again using our ex- 
ample, assume that only the first 13 test cases have been run, 
corresponding to 98% of the probability mass.  Then 2%  has 
not been covered.  If each of the  13 executed test cases  had 
run correctly, and it were known that each of the remaining 
elements of the domain had a failure cost of  1, then the risk 
would be .02. If not, then a maximal value would have to be 
selected, in this case  lo9. The risk would then be .02 x  lo9 
= 20,000,000. If  100 test cases had been run, corresponding 
to 99.999% of the probability mass, and only i l  had failed, 
then the risk would be .00001 x lo9 + .35 x 5000 = 11,750. 
The first element  corresponds to the unexecuted  test cases, 
while the second element  corresponds to the input that was 
executed but failed. If, instead, i4 had failed, the risk would 
be  10,010. These last two examples again  assume that the 
failure cost  associated  with each  of the unrun  inputs is un- 
known, but a maximal  value for failure cost (lo9) is known. 
As was the case for the DSTl test selection method choos- 
ing  100 test cases, i1000  would never be selected. 
There is still another way of using the DST test case  se- 
lection  method.  In  this case, the operational profile  is ad- 
justed for cost of failure so that very rare events (like i 1 0 0 0 )  
with high consequence of failure are made to look like they 
happen frequently and therefore should be selected for test- 
ing.  A  complete  description of  this  approach appears  in 
Reference  [ 161. 
4.3  Subdomain-based Testing 
Subdomain-based testing is a different sort of test selec- 
tion  method.  The idea  here  is  to divide the  input domain 