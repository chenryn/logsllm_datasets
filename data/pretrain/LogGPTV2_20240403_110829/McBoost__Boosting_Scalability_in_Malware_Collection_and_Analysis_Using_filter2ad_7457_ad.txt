KM
57.9%
-
Accuracy
0.870
0.429
FP
0.0
0.875
DR
0.727
0.833
AUC
0.930
0.472
Table 4: Comparison between McBoost and the approach presented
in [9] on a difﬁcult dataset (FP = false positive rate, DR = detection
rate).
Comparison to Previous Work. Table 4 shows the com-
parison between McBoost and the approach proposed by
Kolter et al in [9], which we called KM. We implemented
the classiﬁer proposed in [9] according to the description
given in the paper, and we set the same values for the pa-
rameters such as the value of n, the number of features to be
selected etc., as suggested in the paper. We trained KM sim-
ilarly to McBoost, using 80% of the PMDset and NPMD-
set as malware dataset, and 80% of BDset for the benign
dataset. Afterwards we tested both McBoost and KM on the
same test set consisting of 20% of PBDset (packed benigns)
and NPMDset (non-packed malware). The results reported
in Table 4 conﬁrm the fact that KM is biased towards de-
tecting packed executables as malware and non-packed ex-
ecutables as benign, regardless of the nature of the hidden
or non-packed code. On the other hand, McBoost still has
an accuracy of 87% and an AUC of 0.93 even in the case of
such a “difﬁcult” dataset.
3.4 Discussion of the Results
We would like to emphasize that the most important re-
sult is the value of the AUC of McBoost, which is equal
to 0.977 in our experiments, because the AUC can be
interpreted as the probability of scoring a malware exe-
cutable higher than a benign executable [5], in terms of
P (malware|e). Since our system is intended for priori-
tizing (according to the ranking given by McBoost’s output,
P (malware|e)) the analysis of the most suspicious bina-
ries, a value of the AUC close to 1 (which is the maximum
possible value) is intuitively more important than the ac-
curacy, whose value is dependent from the a priori class
probabilities and from the detection threshold.
The utility of McBoost is intuitive. Suppose we need
to analyze a large set of executables downloaded by a P2P
or web executable crawler, or by and executable sniffer
308308
in order to collect samples of new (zero-day) malware.
In this case, once we ﬁlter out the known malware us-
ing signature-based AV-software, we expect the dataset will
contain mostly benign executables and a small fraction of
unknown malware. Also, we expect most of the benign to
be non-packed (in Section 3.1.1 we found that only 1.2%
of the executable of a typical Windows XP home user in-
stallation were packed). Given that non-packed executables
can be classiﬁed in around 1.06 seconds in average, Mc-
Boost allows us to quickly ﬁlter out most of the benign ex-
ecutables because they will pass from module A to module
C directly, and will receive a low P (malware|e) (i.e., a
low rank). On the other hand, existing approaches for ana-
lyzing executables downloaded/collected from the Internet
most likely have to run each unknown executable (e.g., af-
ter running AV tools, and checking a whitelist) through an
unpacker and/or a program analyzer. As suggested by our
results above, it takes at least several minutes to analyze
each executable just to determine whether there is hidden
code and if so extract the code. If we assume that the ma-
jority of the executables are non-packed benigns, this pro-
cess is very wasteful and inefﬁcient. Therefore, McBoost
can achieve huge time savings when processing a large col-
lection of executables from the Internet. Without McBoost
we would need to analyze all of the binaries (after black-
list/white-list ﬁltering) using expensive analysis techniques,
thus increasing the overall cost of the analysis to the point
of infeasibility.
The total processing time for the validation of McBoost
using the 4,330 executables (3,830 malware plus 503 be-
nigns, as described above), was about 195 hours (slightly
more than 8 days). On the other hand, processing all the
4,330 test samples directly using our dynamic unpacker
(i.e., assuming we do not have our module A classiﬁer)
would take about 347 hours (slightly less than 14 and 1
2
days). Therefore, McBoost required only 56.2% of the time
compared to running all the executables through module B
or similar binary analysis tools. It is worth noting, though,
that our test dataset contained more malware samples than
benigns (88.4% of the executables were malware). As men-
tioned before, by crawling P2P networks (or the Internet in
general) looking for executables may very likely produce
(after white- and black-listing) a dataset containing a small
percentage of new malware and a large percentage of non-
packed benigns. In this case we expect the time saving due
to McBoost to be much higher. For example, if we collected
a dataset that contains about 85% of non-packed benign
executables, 2% of packed benign, and 13% of unknown
malware (which for the sake of this example we assume
all packed), we expect McBoost will require only around
13.4% of the time, compared to using only tools based on
dynamic binary analysis similar to module B.
4 Related Work
In [16], Royal et al. proposed Polyunpack, an universal
unpacker based on a combination of static and dynamic bi-
nary analysis. Given a packed PE executable, a static view
of the code is constructed ﬁrst. If the executable tries to ex-
ecute any code that is not present in the static view, Polyun-
pack will detect this as an attempt to running hidden code
and will try to reconstruct the original executable.
Renovo, a different and somewhat more effective tool
for universal unpacking using dynamic binary analysis, was
presented by Kang et al. in [8]. Renovo is able to distin-
guish among different layers of unpacking and dump the
memory pages that contain the hidden code for each layer.
In [17] Shultz et al. present data mining techniques
for detecting new malicious executables. They extract sev-
eral features from the executables, for example the list of
DLLs used by the binary, the list of DLL function calls,
and the number of different system calls used within each
DLL. Also they analyze byte sequences extracted from the
hexdump of an executable (i.e., its hexadecimal represen-
tation) [17]. Our work is different from [17] because we
adopt a different approach. We ﬁrst distinguish between
packed and non-packed executables, and then (if needed)
we extract and classify the hidden or non-packed code into
malware or benign. Also, we measure a different set of fea-
tures extracted from PE executables, compared to [17].
N-gram analysis for malware detection has been studied
in a number of works [9, 15, 25]. To the best of our knowl-
edge, among these the work closest to ours is [9]. Kolter
et al. [9] use n-gram analysis on entire PE ﬁles to distin-
guish between malware and benign executables. However,
they do not distinguish between packed and non-packed ex-
ecutables. They collect 1,651 malware samples and 1,971
benign samples [9]. They take their dataset of malware as
it is without considering whether the executables they col-
lected are packed or not, and show that their best classiﬁer
(Boosted J48) achieves an AUC(cid:39) 0.996. Because most of
today’s malware are packed [4, 20], not taking this into ac-
count during the training and test of the classiﬁer may pro-
duce over-optimistic results. In the presence of a training
dataset containing mainly packed malware and non-packed
benign, the approach of Kolter et al. may actually be biased
in distinguishing between packed and non-packed executa-
bles, instead of malware vs. benign executables, as we show
in Section 3. Although we use n-gram analysis in our Mc-
Boost, our approach is signiﬁcantly different from [9]. We
ﬁrst recognize that most of the malware are packed, and that
only distinguishing between packed and non-packed exe-
cutables may not be enough to actually detect malicious
executables. Therefore, we ﬁrst classify executables into
packed and non-packed, and for the packed executables we
try to extract the hidden code using an universal unpacker
309309
similar in principle to Renovo [8]. We then classify the
extracted hidden code (or the non-packed code, if an exe-
cutable is found to be non-packed) into either malicious or
benign.
5 Conclusion
We presented McBoost, a fast statistical malware detec-
tion tool intended to improve the scalability of existing mal-
ware collection and analysis techniques. We discussed how
McBoost can be used in case when a large collection of bi-
naries that contains both unknown (zero-day) malware and
benign executables needs to be analyzed in order to discover
new malware samples for which a detection signature can
be written. McBoost allows us to quickly and accurately
ﬁlter out most of the benign and prioritize further detailed
analysis of the remaining suspicious binaries. This allows
us to signiﬁcantly reduce the workload of tools (and hu-
mans) that perform detailed binary analysis.
We evaluated the accuracy and performance of the in-
dividual modules in McBoost as well as the system as a
whole. The results showed that McBoost has an over-
all classiﬁcation accuracy of 87.3% and an AUC equal to
0.977. In addition, the running time of McBoost on our test
data shows that the overal computation time for analyzing
large sets of executables can be reduced to only a fraction
(e.g., 13.4%) of the time needed if we only used dynamic-
analysis-based tools.
References
[1] F. Bellard. Qemu, a fast and portable dynamic translator. In
USENIX Annual Technical Conference, 2005.
[2] D. Bilar. Opcode as predictors for malware. International
Journal of Electronic Security and Digital Forensics, 1(2),
2007.
[3] L. Breiman.
Bagging predictors. Machine Learning,
24(2):123–140, 1996.
[4] T. Brosch and M. Morgenstern. Runtime packers: The hid-
den problem? Presented at Black Hat USA 2006.
[5] C. Cortes and M. Mohri. Conﬁdence intervals for the area
In NIPS 2004: Advances in Neural
under the roc curve.
Information Processing Systems, 2004.
[6] R. Duin. The combining classiﬁer: to train or not to train?
In International Conference on Pattern Recognition (ICPR),
2002.
[7] A. Kalafut, A. Acharya, and M. Gupta. A study of malware
in peer-to-peer networks. In ACM SIGCOMM conference on
Internet measurement, 2006.
[8] M. G. Kang, P. Poosankam, and H. Yin. Renovo: A hidden
code extractor for packed executables. In WORM ’07: Pro-
ceedings of the 5th ACM Workshop on Recurring Malcode,
2007.
[9] J. Z. Kolter and M. A. Maloof. Learning to detect and clas-
sify malicious executables in the wild. Journal of Machine
Learning Research, 7:2721–2744, 2006.
310310
[10] L. I. Kuncheva. Combining Pattern Classiﬁers: Methods
and Algorithms. Wiley-Interscience, 2004.
[11] A. Niculescu-Mizil and R. Caruana. Predicting good prob-
abilities with supervised learning. In International Confer-
ence on Machine Learning (ICML), 2005.
[12] R. Perdisci, G. Gu, and W. Lee. Using an ensemble of one-
class svm classiﬁers to harden payload-based anomaly de-
tection systems. In International Conference on Data Min-
ing (ICDM), 2006.
[13] R. Perdisci, A. Lanzi, and W. Lee. Classiﬁcation of packed
executables for accurate computer virus detection. Patter
Recognition Letters, 29(14):1941–1946, 2008.
[14] M. B. Prince, L. Holloway, E. Langheinrich, B. M. Dahl,
and A. M. Keller. Understanding how spammers steal your
e-mail address: An analysis of the ﬁrst six months of data
In 2nd Conference on Email and
from project honey pot.
Anti-Spam (CEAS), 2005.
[15] D. K. S. Reddy and A. K. Pujari. N-gram analysis for com-
puter virus detection. Journal in Computer Virology, 2(3),
2006.
[16] P. Royal, M. Halpin, D. Dagon, R. Edmonds, and W. Lee.
Polyunpack: Automating the hidden-code extraction of
In Annual Computer Security
unpack-executing malware.
Applications Conference (ACSAC), 2006.
[17] M. G. Schultz, E. Eskin, E. Zadok, and S. J. Stolfo. Data
mining methods for detection of new malicious executables.
In IEEE Symposium on Security and Privacy, 2001.
[18] A. Shevchenko. The evolution of self-defense technologies
in malware, 2007. http://www.viruslist.com/
analysis?pubid=204791949.
[19] S. Shin, J. Jung, and H. Balakrishnan. Malware prevalence
in the kazaa ﬁle-sharing network. In ACM SIGCOMM Con-
ference on Internet measurement, 2006.
[20] A. Stepan.
Improving proactive detection of packed
http://www.virusbtn.
malware, March 2006.
com/virusbulletin/archive/2006/03/
vb200603-packed.dkb.
[21] Y. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski,
S. Chen, and S. T. King. Automated web patrol with strider
honeymonkeys: Finding web sites that exploit browser vul-
nerabilities. In NDSS, 2006.
[22] T. Werner. PE Hunter, 2007. http://honeytrap.
mwcollect.org/pehunter.
[23] Y. Yang and J. O. Pedersen. A comparative study on feature
selection in text categorization. In International Conference
on Machine Learning (ICML), 1997.
[24] H. Yin, D. Song, M. Egele, C. Kruegel, and E. Kirda.
Panorama: capturing system-wide information ﬂow for mal-
ware detection and analysis. In CCS ’07: 14th ACM confer-
ence on Computer and communications security, 2007.
[25] B. Zhang, J. Yin, J. Hao, D. Zhang, and S. Wang. Malicious
codes detection based on ensemble learning. In Autonomic
and Trusted Computing (ATC), 2007.
[26] J. Zhuge, T. Holz, X. Han, C. Song, and W. Zou. Collecting
autonomous spreading malware using high-interaction hon-
eypots. In ICICS 2007, 2007.