hypervisors require hardware virtualization extensions (e.g.,
NoHype [39]). The majority of current many-core platforms
do not provide virtualization support and adding such ex-
tensions to the cores has scalability disadvantages. Further-
more, virtualization itself increases the runtime overhead of
VMs [15, 36]. For these reasons, we focus on alternatives
that enable logical partitions on many-core platforms with-
out requiring processor virtualization.
Figure 3: Intel SCC address translation. On each core,
an MMU translates virtual addresses to physical addresses
and a look-up-table (LUT) translates physical addresses to
system-wide addresses.
to an on-tile or to an oﬀ-tile resource. Cores access external
DRAM memory, memory-mapped peripherals, and memory
on other tiles, using oﬀ-tile memory addresses. Using an
on-tile memory address, a core can access on-tile memory,
registers, and its own LUT.
3. DESIGN ALTERNATIVES
3.1 Hypervisor Designs
The typical way to realize logical partitions is using a hy-
pervisor that is a privileged software layer that manages the
execution of multiple operating systems on shared physical
resources. The hypervisor is responsible for the isolation of
the operating systems, and thus it is part of the system TCB.
A common approach to reduce the risk of TCB compromise
is to minimize the implementation size of the hypervisor and
its runtime attack surface. We call these properties small
footprint (P1) and reduced interaction (P2).
The hypervisor footprint is typically measured in terms
of the source lines of code (LOC). The smallest hypervi-
sors available today leverage hardware virtualization exten-
sions and have a footprint of around 4K LOC [46]. In con-
trast, hypervisors that implement all the virtualization and
management functionality in software, are typically above
100K LOC [40].
The runtime attack surface mainly depends on the in-
teraction of the hypervisor with the untrusted components
in the system and in particular with the operating systems
running in the logical partitions. While a typical hypervi-
sor interacts with the operating systems during their launch,
operation and shutdown, a disengaged hypervisor limits its
interaction with the OS to the time of the OS launch and
shutdown [39]. Such reduced interaction is beneﬁcial for se-
curity, as the OS-hypervisor interfaces tend to contain bugs
and may allow hypervisor exploitation [31].
Below, we discuss diﬀerent hypervisor designs and analyze
their ability to achieve properties P1 and P2. Hypervisors
can be broadly classiﬁed into three categories, as illustrated
in Figure 4.
Traditional hypervisor. A single hypervisor instance
manages all the cores in the platform and interacts with
the operating systems running in the logical partitions (Fig-
ure 4.a).
COREMemory Management UnitNETWORK INTERFACELook-Up-Table (LUT)Physical AddressVirtual AddressDRAMLUTI/ORegistersOff-tileAnother TileMemoryOn-tileOn-tileMemorySystem-wide AddressFigure 4: Hypervisor design alternatives. A hypervisor can be organized as (a) a single instance that manages all cores,
(b) a distributed kernel that treats the underlying platform as a distributed system, or (c) a centralized kernel that allows
each OS to execute directly on the hardware.
Figure 5: Security-enhanced Intel SCC. We propose two security enhancements. First, a LUT protection mechanism that
restricts access to LUTs to a single core. The master core can conﬁgure all LUTs on the platform through the Network-on-Chip
(NoC). Second, we add a boot interface to enable the SCC to operate as a stand-alone processor.
4. LOGICAL PARTITIONS IN
MANY-CORE PROCESSORS
Hardware virtualization extensions were designed for sys-
tems where each core is shared between the hypervisor and
an operating system. Processors with virtualization exten-
sions provide a de-privileged execution mode for the oper-
ating systems and at least two levels of address translation.
Similar techniques are not required to implement logical par-
titions using a centralized hypervisor on many-core systems,
because the hypervisor and the operating systems can run
on separate cores. In logical partitioning schemes that use a
centralized hypervisor, all the cores use only one of the two
privilege modes (hypervisor/OS), and therefore do not need
to support both.
We observe that if the hypervisor and the operating sys-
tems run on diﬀerent cores, a small and disengaged hyper-
visor can be realized when the underlying processor incor-
porates mechanisms that conﬁne each core (and hence the
OS running on top) to its own set of resources. Since all re-
sources in many-core processors are memory-mapped, this
essentially reduces to restricting access of each core to a set
of system addresses. We refer to this mechanism as address-
space isolation.
On many-core systems, address-space isolation can be re-
alized without introducing additional functionality into the
processor cores. For example, the Network-on-Chip (NoC)
that connects the cores to the system main memory can be
enhanced to enforce address-space isolation. NoC-based ac-
cess control techniques have been studied in the context of
embedded systems to isolate applications (e.g., [17, 34]).
We use the Intel SCC architecture as a case study and
show that address-space isolation can be realized with very
simple enhancements to the processor NoC. We chose the In-
tel SCC because it has simple cores that are capable of run-
ning OS instances independently and its memory address-
ing scheme provides a good basis for implementing address-
space isolation.
4.1 Security-enhanced Intel SCC
To support address-space isolation, and thus secure logical
partitions, we propose the following two, simple changes to
the Intel SCC architecture:
• LUT protection. The ﬁrst enhancement is a LUT
protection mechanism that allows modiﬁcations of all
LUTs only from a single master core. In practice, this
can be implemented by restricting access to LUTs from
all but one core, as illustrated in Figure 5. The master
core can conﬁgure all LUTs in the platform over the
NoC. At runtime, the (read-only) LUT conﬁguration
at each core enforces address-space isolation for the op-
erating system running on top of it. This modiﬁcation
allows to enforce access control not only to memory,
but also to peripherals and to other cores.
• Stand-alone boot. The second enhancement is a
bootable ROM module (boot interface) that allows
the Intel SCC to be used as a stand-alone processor.
This modiﬁcation obviates the need for a host plat-
form and hence the need to include the host OS in the
system TCB. The proposed modiﬁcation is inline with
the recent trend of stand-alone many-core processors,
instead of their use as co-processors [20].
The required hardware modiﬁcations are small and do not
aﬀect the scalability of the platform. In fact, the modiﬁca-
............C1C2CnPROCESSOR Hypervisor  (H)OS1OS2OSn............C1C2CnPROCESSORHOS1OS2OSn......Traditional Hypervisor Distributed Hypervisor HH............C1C2CnPROCESSORHOS1OS2Centralised Hypervisor(a)(b)(c)RMCNoCNoCTTileRouterMemory Controller Network on ChipMCMCMCMCRRMaster CoreL2 cachePentiumL2 cachePentiumL2 cacheNETWORK INTERFACEPentiumOTHER TILEL2 cacheXXLUTsLUTsXBoot InterfaceHYPERVISOR TILEFigure 6: SCC enhancement emulation. We use a cus-
tomized Fiasco microkernel running on each core of the SCC
platform to emulate the LUT protection features.
tions we propose do not increase the complexity of the Intel
SCC cores at all.
In the Intel SCC architecture, each tile contains two cores
that must share an on-tile memory region (for registers)
due to the granularity of the addressing mechanism and the
available on-tile memory. Consequently, our simple hard-
ware modiﬁcation supports logical partitions at the gran-
ularity of a tile (two cores). To enable per-core partitions,
the Intel SCC architecture should be enhanced to completely
separate the address space of the two cores on each tile. In
this paper, we focus on demonstrating the concept of NoC-
based address-space isolation and realize per-tile partitions.
4.2 Emulated Implementation
We provide an implementation where we emulate the pro-
posed hardware enhancements. The purpose of this imple-
mentation is to demonstrate that we can provide address-
space isolation on the Intel SCC platform simply by con-
trolling the access of the cores to their LUTs.
Our setup consists of an Intel Xeon server that connects
to the Intel SCC co-processor via a PCI Express (PCIe)
interface. The host runs Ubuntu 10.10 and includes a driver
for the Intel SCC. The driver is used to enable user-space
access to individual cores (e.g., to reset them) and to the
DRAM on the Intel SCC to load applications from the host.
To emulate the boot functionality, we use the host to load
code on the master core (core 0) of Intel SCC and start it. To
realize the LUT protection, our prototype uses a Fiasco mi-
crokernel [33]. Figure 6 shows the components in the kernel
implementation. A Bootstrapper module starts the Fiasco
microkernel when a core is powered on. We modiﬁed the
Rootpager module to enforce that the OS running on top of
the microkernel cannot modify LUTs, as the Rootpager me-
diates all address space requests in the Fiasco architecture.
To demonstrate address-space isolation, we run L4Linux
[42] (a Linux ﬂavor compatible with Fiasco) on top of the
enhanced microkernel. On cores other than the master core,
the L4Linux instances were not able to access system re-
sources (memory, cores, I/O devices) other than the ones
allocated for them, and importantly they were not able to
change their own LUT conﬁguration.
5. MANY-CORE CLOUD
In this section, we describe a complete IaaS architecture
that leverages the security-enhanced Intel SCC platform.
Figure 7 illustrates the main components of a typical IaaS
cloud infrastructure. Customers log in via a management
service which also allows them to start, stop, or delete their
virtual machines. Each VM runs an independent operating
system. A storage service provides persistent storage for
customer VMs and their data. A VM is assigned to, and
Figure 7: IaaS cloud infrastructure. A typical IaaS in-
frastructure consists of three main components: a comput-
ing node, a management service and a storage service. Our
computing node is a security-enhanced Intel SCC, connected
to a cryptographic module and hardware-virtualized periph-
erals. We consider the parts shown in gray untrusted.
runs on, one of many computing nodes. A hypervisor on the
computing node manages VMs and ensures isolation.
The computing node has access to an hardware crypto-
graphic module that securely stores the private key of an
asymmetric key pair. The corresponding public key is cer-
tiﬁed by the cloud provider or by a trusted authority. The
cryptographic module decrypts VM images encrypted under
its public key, before the VM is loaded on a core of the com-
puting node. The decryption interface is available to the hy-
pervisor of the computing node and it is disconnected from
the other cloud components (e.g., the management and the
storage services). Hardware cryptographic modules, such as
IBM cryptographic co-processors [8], are commonly used in
cloud platforms.
In our architecture, the computing node is based on the
security-enhanced Intel SCC platform and equipped with
hardware-virtualized peripherals. For example, a network
card with the Intel SR-IOV technology [23] can be conﬁgured
to create multiple virtual functions. Each VM is allocated a
separate virtual function to access the network and storage
on a networked ﬁle-system. The many-core processor also
has a chipset (e.g., one supporting the Intel VT-d exten-
sions [24]) that can be used to re-route interrupts from the
hardware-virtualized network card to the associated cores
on the many-core processor.
5.1 Security Goals
We consider two types of threats.
In cross-VM attacks
a malicious VM tries to access the data of a victim VM
running on the same platform.
In cloud platform attacks,
compromised components of the cloud architecture (e.g., the
management service or the storage service) try to access the
data of the victim VM. Our goal is to design a cloud architec-
ture that withstands both types of attacks. In Figure 7, we
highlight the cloud components that we consider untrusted.
We exclude hardware attacks and consider the underly-
ing hardware trustworthy. Similarly, we exclude denial-of-
service attacks because the solutions to them are comple-
mentary to our work (see Section 6 for further discussion).
Side-channel attacks are discussed in Section 5.3.
BootstrapperFiascoRootpagerLUT ProtectionL4Linux VMSecurity Enhanced Intel's SCCComputing node Management serviceUser managementVM managementStorageserviceHardware-virtualized PeripheralsUser VMsCryptographic ModuleFigure 8: Cloud architecture and operation. The boot interface starts the hypervisor on master core. The hypervisor
starts the CNI service which fetches VM images from the storage service. The hypervisor sets up new partitions, uses a
cryptographic module to decrypt VM images and loads them for execution.
5.2 Architecture and Operation
Figure 8 depicts a cloud architecture based on the security-
enhanced Intel SCC and its operations.
The hypervisor boots on the master core at power on (Step
1) and resets the LUTs of all cores. To reduce the TCB size
we follow a technique known as hypervisor factorization [18,
43] and run non-critical services of the computing node in
separate partitions. For example, after boot, the hypervisor
starts a Compute Node Interface (CNI) service in a separate
logical partition (Step 2). This service fetches VMs over the
network to run them on the computing node but is not part
of the TCB as it may include complex networking libraries.
The hypervisor may also start other services, such as those
for resource accounting, in their own partitions.
Customers encrypt their VM images under the public key
of the cryptographic module and upload them to the cloud.
Customers may also inject keys into their encrypted VM
images to enable authentication upon login (e.g., via SSH).
When the VM is scheduled for execution, the CNI ser-
vice fetches the encrypted VM from the storage service and
loads it to memory (Step 3). The hypervisor picks a free
core and conﬁgures its LUT to enable access to an exclu-
sive memory region and instances of virtualized peripherals
(e.g., a virtual network card). The hypervisor copies the
encrypted VM image to a memory region shared exclusively
with the cryptographic module. The cryptographic module
decrypts the VM image using the private key and the hyper-
visor loads it into the memory region that was allocated for
the new partition. Finally, the hypervisor resets that core
to start the VM’s execution (Step 4).
At VM shutdown, the hypervisor clears the memory as-
signed to that VM, so that it can be securely re-allocated
to other VMs, and resets the LUT of the freed core. If a
VM must save sensitive data, it can encrypt the data under
its own key before shutting down. The ciphertext is stored
on the networked ﬁle-system and will be available upon the
next execution of the VM.