relevant external event (speciﬁed as a query policy) and computes
MacroRule(@C,sw,inPort0,dstMAC0,act,Prio0) :-
UpdateEvent(@C,sw,srcMac), HighestP(@C,Prio),
PktIn(@sw,inPort1,srcMAC,dstMAC1), inPort0=*,
dstMAC0=srcMAC, act=fwd(inPort1),
Prio0=Prio1+10
Figure 6: NDlog translation of the self-learning switch.
and installs the new entry. The self-learning switch from Figure 5
is an example of such a policy; Figure 6 shows the rule that it is
translated to. The rule directly corresponds to the if-then part
in Figure 5, which forwards packets to newly observed MAC ad-
dresses to the correct port, and otherwise (in the else branch) falls
back on the existing policy. Once translated in this way, it is easy
to see the provenance of a dynamic policy change: it is simply the
packet that triggered the change.
6.2 BGP debugging
Our second case study focuses on BGP. There is a rich literature on
BGP root-cause analysis, and a variety of complex real-world prob-
lems have been documented. Here, we focus exclusively on the
question whether Y! can be used to diagnose BGP problems with
negative symptoms, and we ignore many other interesting ques-
tions, e.g., about incentives and incremental deployment. (Brieﬂy,
we believe that the required infrastructure and the privacy impli-
cations would be roughly comparable to those of [27]; in a par-
tial deployment, some queries would return partial answers that are
truncated at the ﬁrst vertex from a network outside the deployment.)
To apply Y!, we follow the approach from [35] and write a sim-
ple declarative program that describes how the BGP control plane
makes routing decisions. Our implementation is based on an ND-
log encoding of a general path vector protocol provided by the au-
thors of [30]; since this code was generic and contained no spe-
ciﬁc policies, we extended it by adding simple routing policies that
respect the Gao-Rexford guidelines and import/export ﬁlters that
implements the valley-free constraint. This yielded essentially a
declarative speciﬁcation of an ISP’s routing behavior. With this,
we could capture BGP message traces from unmodiﬁed routers, as
described in [35], and infer the provenance of the routing decisions
by replaying the messages to our program.
7. EVALUATION
In this section, we report results from our experimental evaluation
of Y! in the context of SDNs and BGP. Our experiments are de-
signed to answer two high-level questions: 1) is negative prove-
nance useful for debugging realistic problems? and 2) what is the
cost for maintaining and querying negative provenance?
We ran our experiments on a Dell OptiPlex 9020 workstation,
which has a 8-core 3.40 GHz Intel i7-4770 CPU with 16 GB of
RAM. The OS was Ubuntu 13.04, and the kernel version was 3.8.0.
7.1 Prototype implementation
For our experiments, we built a prototype of Y! based on the Rapid-
Net declarative networking engine [24]. We instrumented Rapid-
Net to capture provenance for the NDlog programs it executes,
and we added provenance storage based on the R-tree implementa-
tion from [14]. To experiment with SDNs, we set up networks in
Mininet [20]. Since NDlog is not among the supported controllers,
we wrote a simple proxy for Trema [29] that translates controller
messages to NDlog tuples and vice versa. To capture the prove-
nance of packets ﬂowing through the network, we set up port mir-
roring on the virtual switches and used libpcap to record packet
traces on the mirrored ports. Since Y!’s provenance graphs only
use the packet headers, we capture only the ﬁrst 96 bytes of header
and its timestamp.
8
SDN1 NAPPEAR([t1,t2], packet(@D, PROTO=HTTP))
SDN2 NAPPEAR([t1,t2], packet(@D, PROTO=ICMP))
SDN3 NAPPEAR([t1,t2], packet(@D, PROTO=SQL))
SDN2 APPEAR (t3, packet(@D, PROTO=ICMP))
SDN3 APPEAR (t3, packet(@D, PROTO=SQL))
Q1
Q2
Q3
Q4
Q5
Q6 BGP1 NAPPEAR([t1,t2], bestroute(@AS2, TO=AS7))
Q7 BGP2 NAPPEAR([t1,t2], packet(@AS7, SRC=AS2))
Q8 BGP3 NAPPEAR([t1,t2], bestroute(@AS2, TO=AS7))
Q9 BGP4 NAPPEAR([t1,t2], bestroute(@AS2, TO=AS7))
Table 2: Queries we used in our experiments
To demonstrate that our approach does not substantially affect
throughput and latency, we also built a special Trema extension that
can capture the provenance directly, without involving RapidNet.
This extension was used for some of experiments in Section 7.5, as
noted there. Other than that, we focused more on functionality and
complexity than on optimizing performance; others have already
shown that provenance can be captured at scale [15], and the in-
formation Y! records is not substantially different from theirs – Y!
merely uses it in a different way.
7.2 Usability: SDN debugging
For our SDN experiments, we used Mininet to create the following
three representative SDN scenarios:
• SDN1: Broken ﬂow entry. A server receives no requests
because an overly general ﬂow entry redirects them to a dif-
ferent server (taken from Section 2.3).
• SDN2: MAC spooﬁng. A host, which is connected to the
self-learning switch from Figure 5, receives no responses to
its DNS lookups because another machine has spoofed its
MAC address.
• SDN3: Incorrect ACL. A ﬁrewall, intended to allow In-
ternet users to access a web server W and internal users a
database D, is misconﬁgured: Internet users can access only
D, and internal users only W .
Each scenario consists of four hosts and three switches. For all
three scenarios, we use Pyretic programs that have been translated
to NDlog rules (Section 6.1) and are executed on RapidNet; how-
ever, we veriﬁed that each problem also occurs with the original
Pyretic runtime. Note that, in all three scenarios, positive prove-
nance cannot be used to diagnose the problem because there is no
state whose provenance could be queried.
The ﬁrst three queries we ask are the natural ones in these sce-
narios: in SDN1, we ask why the web server is not receiving any
requests (Q1); in SDN2, we ask why there are no responses to the
DNS lookups (Q2); and in SDN3, we ask why the internal users
cannot get responses from the database (Q3). To exercise Y!’s
support for positive provenance, we also ask two positive queries:
why a host in SDN2 did receive a certain ICMP packet (Q4), and
why the internal database is receiving connections from the Inter-
net (Q5). To get a sense of how useful negative provenance would
be for debugging realistic problems in SDNs, we ran diagnostic
queries in our three scenarios and examined the resulting prove-
nance. The ﬁrst ﬁve rows in Table 2 show the queries we used. The
full responses are in the long version of this paper [33], but we do
not have the space to discuss all of them here; hence, we focus on
Q1 from scenario SDN1, which asks why HTTP requests are no
longer appearing at the web server.
Figure 7 shows the provenance generated by Y! for Q1. The
explanation reads as follows: HTTP requests did not arrive at the
HTTP server (V1) because there was no suitable ﬂow entry at the
Figure 7: Answer to Q1, as returned by Y!
Figure 8: Topology for the BGP1 scenario.
switch (V2). Such an entry could only have been installed if a
HTTP packet had arrived (V3a+b) and caused a table miss, but
the latter did not happen because there already was an entry – the
low-priority entry (V4) – that was forwarding HTTP packets to a
different port (V5a-c), and that entry had been installed in response
to an earlier DNS packet (V6a-f). We believe that “backtraces” of
this kind would be useful in debugging complex problems.
7.3 Usability: BGP debugging
For our BGP experiments, we picked four real BGP failure scenar-
ios from our survey (Section 2.4):
• BGP1: Off-path change. In the topology from Figure 8,
AS 2 initially has a route to AS 7 via AS 1,3,4,5,6, but loses
that route when a new link is added between AS 8 and AS 9
(neither of which is on the path). This is a variant of a sce-
nario from [27].
• BGP2: Black hole. A buggy router advertises a spurious
/32 route to a certain host, creating a “black hole” and pre-
venting that host from responding to queries.
• BGP3: Link failure. An ISP temporarily loses connectivity,
due to a link failure at one of its upstream ASes.
• BGP4: Bogon list. A network cannot reach a number of
local and federal government sites from its newly acquired
IP preﬁx because that preﬁx was on the bogon list earlier.
We set up small BGP topologies, using between 4 and 18 simu-
lated routers, to recreate each scenario. In each scenario, we then
asked one query: why AS 2 in scenario BGP1 has no route to AS 7
(Q6), why a host in scenario BGP2 cannot reach the black-holed
host (Q7), why the ISP in scenario BGP3 cannot reach a certain
9
ABSENCE(t=[15s,185s], HTTP Server, packet(@HTTP Server, HTTP)) V1 ABSENCE(t=[1s,185s], S2, flowTable(@S2, HTTP, Forward, Port1)) V2 EXISTENCE(t={81s,82s,83s} in [15s,185s], S1, packet(@S1, HTTP))  V3-­‐a EXISTENCE(t=[81s,now], S1,  flowTable(@S1, Ingress HTTP,Forward,Port1))  V3-­‐b EXISTENCE(t={81s,85s,86s}, S2, flowTable(@S2, HTTP, Forward, Port2)) V4 EXISTENCE(t=[81s], Controller, packetIn(@Controller, HTTP))  V5-­‐a ABSENCE(t=[1,80s], S2,  flowTable(@S2, HTTP,*,*))  V5-­‐b ABSENCE(t=[1,80s], S1,  packet(@S1, HTTP))  V5-­‐c EXISTENCE(t=[81s], Controller, policy(@Controller, Inport=1,Forward,Port2)  V6-­‐a EXISTENCE(t=[63s], Controller, packetIn(@Controller, DNS))  V6-­‐b EXISTENCE(t=[62s], S1, packet(@S1, DNS))  V6-­‐c EXISTENCE(t=[61s,now], S1,  flowTable(@S1, Ingress DNS,Forward,Port1))  V6-­‐d ABSENCE(t=[1,61s], S1, flowTable(@S1, DNS,*,*))  V6-­‐e ABSENCE(t=[1,61s], S1,  packet(@S1, DNS))  V6-­‐f AND AND AND AND AND The server did not get any HTTP request since t=15s because the flow entry was missing at an upstream switch.  The flow entry could only has been inserted in response to a HTTP packet. Such packets only arrived at t=81s, 82s, 83s.  But that HTTP packet was handled by an existing flow entry at that switch, and was therefore not sent to the controller.  The existing flow entry was derived from a policy which was triggered by a DNS  packet at t=62s.  ... ... AS 5AS 6AS 7AS 4AS 3AS 1AS 8AS 9AS 2Figure 9: Answer to Q6, as returned by Y!
Figure 11: Raw provenance for query Q1 before post-
processing.
Figure 10: Size of the provenance with some or all heuristics
disabled.
AS (Q8), and why the network in scenario BGP4 cannot connect
to a particular site (Q9). Table 2 shows the speciﬁc queries. As
expected, Y! generated the correct response in all four scenarios;
here, we focus on one speciﬁc query (Q6/BGP1) due to lack of
space. The other results (available in [33]) are qualitatively similar.
Figure 9 shows the provenance generated by Y! for query Q6.
The explanation reads as follows: AS 2 has no route to AS 7 (V1-
a) because its previous route expired (V1-b) and it has not received
any new advertisements from its provider AS 1 (V1-c). This is
because AS 1 itself has no suitable route: its peer AS 3 stopped
advertising routes to AS 7 (V2a-c) because AS 3 only advertises
customer routes to AS 7 due to the valley-free constraint (V3-a).
AS 3 previously had a customer route but it disappeared (V3-b).
Although AS 3 continues to receive the customer route from AS 4
(V3-c), the peer route through AS 8 (V4) is preferred because it has
a shorter AS path (V3-d). The provenance of the peer route could
be further explored by following the graph beyond V4.
7.4 Complexity
Recall from Section 4 that Y! uses a number of heuristics to sim-
plify the provenance before it is shown to the user. To quantify
how well these heuristics work, we re-ran the queries in Table 2
with different subsets of the heuristics disabled, and we measured
the size of the corresponding provenance graphs.
Figure 10 shows our results. Without heuristics, the provenance
contained between 55 and 386 vertices, which would be difﬁcult
for a human user to interpret. The pruning heuristics from Sec-
tion 4.1 generally remove about half the vertices, but the size of the
10
Figure 12: Turnaround time for the queries in Table 2.
provenance remains substantial. However, the super-vertices from
Section 4.2 are able to shrink the provenance considerably, to be-
tween 4 and 24 vertices, which should be much easier to interpret.
To explain where the large reduction comes from, we show the
raw provenance tree (without the heuristics) for Q1 in Figure 11.
The structure of this tree is typical of the ones we have generated:
a “skeleton” of long causal chains, which typically correspond to
messages and events propagating across several nodes, and a large
number of small branches. The pruning heuristics remove most
of the smaller branches, while the super-vertices “collapse” the
long chains in the skeleton. In combination, this yields the much-
simpliﬁed tree from Figure 7.
7.5 Run-time overhead
Disk storage: Y! maintains two data structures on disk: the packet
traces and the historical R-tree. The size of the former depends on
the number of captured packets; each packet consumes 120 bytes
of storage. To estimate the size of the latter, we ran a program
that randomly inserted and removed flowEntry tuples, and we
measured the number of bytes per update. We found that, for trees
with 103 to 106 updates, each update consumed about 450 byte of
storage on average.
These numbers allow us to estimate the storage requirements in
a production network. We assume that there are 400 switches that
each handle 45 packets per second, and that the SDN controller
generates 1,200 ﬂow entries per second. Under these assumptions,
a commodity hard disk with 1TB capacity could easily hold the
provenance for the most recent 36 hours. If necessary, the storage
cost could easily be reduced further, e.g., by compressing the data,
by storing only a subset of the header ﬁelds, and/or by removing
redundant copies of the headers in each ﬂow.
ABSENCE(t=[55s,65s], AS2, bestRoute(@AS2,  Prefix=AS7, Type=Any, Cost=Any, Next=Any)) TIMEOUT(t=[39s], AS2, bestRoute(@AS2, Prefix=AS7, Type=Provider , Cost=6, Next=AS1)) ABSENCE(t=[39s,65s], AS2, advertisement(@AS2,  Prefix=AS7, Cost=Any, Next=Any)) ABSENCE(t=[39s,65s], AS1, bestRoute(@AS1,  Prefix=AS7, Type=Any, Cost=Any, Next=Any)) TIMEOUT(t=[37s], AS1, bestRoute(@AS1,  Prefix=AS7, Type=Peer, Cost=5, Next=AS3)) ABSENCE(t=[37s,65s], AS1, advertisement(@AS1,  Prefix=AS7, Cost=Any, Next=Any)) ABSENCE(t=[37s,65s], AS3, bestRoute(@AS3  Prefix=AS7, Type=Customer, Cost=Any, Next=Any)) DELETE(t=[37s], AS3, bestRoute(@AS3,  Prefix=AS7, Type=Customer, Cost=4, Next=AS4)) EXISTENCE(t=[38s,40s, …, 64s], AS3,  advertisement(@AS3, Prefix=AS7, Cost=4, Next=AS4)) EXISTENCE(t=[37s,65s], AS3, bestRoute(@AS3,  Prefix=AS7, Type=Peer, Cost=3, Next=AS8)) ... EXISTENCE(t=[37s], AS3, advertisement(@AS3,  Prefix=AS7, Cost=3, Next=AS8)) AND AND AND AND V1-­‐a V1-­‐b V1-­‐c V2-­‐a V2-­‐c V2-­‐b V3-­‐a V3-­‐b V3-­‐c V3-­‐d V4 AS2’s previous route to AS7 expired at t=39s, and after that, AS2 never received  any advertisement from its provider (AS1). AS1 stopped advertising route to AS2 because its  own route to AS7 expired at t=37s, and since then,  its peer (AS3) has sent no more advertisements. AS3 would only advertise customer routes to AS1.  At t=37s, its route to AS7 got updated to a peer route. Although after t=37s, AS3 continued to receive customer  routes from AS4, its best route remains the peer route  because it is shorter. At t=37s, AS3 received the  peer route to AS7 from AS8. ...  0 50 100 150 200 250 300 350 400Q1Q2Q3Q4Q5Q6Q7Q8Q9Vertices in responseQueryWithout heuristicsPruning onlyPruning + Super-verticesrootV1V3-aV3-aV3-aV3-bV2V4V6-aV5-a.........Materialized VerticesIntermediate VerticesInconsistent/Repeated VerticesV#Labels in Summarized TreeMore Vertices... 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35Q1Q2Q3Q4Q5Q6Q7Q8Q9Time (seconds)QueryGraph constructionR-Tree lookupsPacket recorder lookupsPostprocessing(a) Size of query results
(b) Use of storage space
Figure 13: Scalability results
(c) Query turnaround time
Latency and throughput: Maintaining provenance requires some
additional processing on the SDN controller, which increases the
latency of responses and decreases throughput. We ﬁrst measured
this effect in our prototype by using Cbench to send streams of
PacketIn messages, which is a current standard in evaluating Open-
Flow controllers [4]. We found that the 95th percentile latency in-