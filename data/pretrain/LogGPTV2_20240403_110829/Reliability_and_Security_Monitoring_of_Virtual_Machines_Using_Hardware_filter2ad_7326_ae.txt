how long Ninja was in the Sleep state and when the next check
would be performed. With the measured interval and checking
time, we could launch transient attacks that avoided detection.
However, that particular strategy did not work on H-Ninja, as
it does not generate a /proc ï¬le in the target VM. Table III
shows result of a trial of this method, in which each interval
was sampled 30 times.
Rootkit combined attacks: In a more substantial attack,
we combined a privilege escalation exploit with a rootkit,
which was able to hide processes. After the terminal was
escalated, we immediately ran the rootkit to prevent Ninja
from discovering the presence of the terminal, bypassing both
versions of Ninja.
Spamming attacks: We increased the execution time of the
function that iterated over the process list by launching a
large number of valid processes together with one privilege
escalated process. The purpose was to increase the scanning
time so that the escalated process can complete before the scan
reached it. Note that a blocking H-Ninja is protected against
this attack. See the bottom of Fig. 6 for an illustration.
2) Active Monitoring with HT-Ninja: To show the beneï¬ts
of HyperTapâ€™s active monitoring mechanism, we compared the
detectability of the three versions of Ninja (O-Ninja, H-Ninja,
and HT-Ninja) against real exploits, coupled with the attack
strategies described in Section VIII-C1. It is worth mentioning
that both O-Ninja and H-Ninja are vulnerable to DKOM
rootkits, e.g., SucKIT, because they only use OS invariants.
Our experiments showed that O-Ninja with a 0-second
checking interval was quickly defeated by a privilege esca-
lation exploit CVE-2013-1763 [36] combined with spamming
and a rootkit.6 The attack was performed as follows: (i) a
number of idle processes were created; (ii) the exploit code
was run to grant root privileges to the current process; and
(iii) with root privileges, the rootkit was installed to remove
the escalated process from the process list. We repeated the
attack 300 times and have timed the attack to take âˆ¼4ms on
an Intel(R) Core(TM)2 Duo CPU E8400 3GHz CPU. Without
creating extra processes on a system with 31 processes run-
ning, O-Ninja can detect âˆ¼10% attacks. When we introduced
100 idle processes, the detection probability was reduced to
single-digit percentages (between 2 and 3%). With 200 or
more idle processes, O-Ninja cannot detect any attacks (with
the exception of outliers (cid:3) 1%).
To test H-Ninja, we used the same privilege escalation +
rootkit combination as with O-Ninja. Since the attack was
6https://github.com/ivyl/rootkit
22222222
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:24 UTC from IEEE Xplore.  Restrictions apply. 
	C	3"C
	 
	 	3" 	
. &#+$+(
&#''&)#"
$3'#"(.(-("
  &$('49=#"+&&"(5
  &$('4>#"+&&"(5
  &$('49#"+&&"(5
$&#+$+(
/'(!  
,&
&(!)'(4(/$D#+ 5
&/'(#":+'"&'(&,& '
+&'#"'(33#-&#	"#
2'%&(4:5(#??! $ '
#!$ &&#+$+(
&(!)4(/$D#(5
&(!)4(/$D #"5
&(!)4(/$D"(5
&(!)4(/$D'#&(5
 #$/;8?=+'0>888
 &(;8?=+'0>888
 ;8?=+'0>888
 #$/: 20 ms, the detection probability became < 5%.
Although an attacker with no access to side channels must
rely on a certain amount of luck to defeat O-Ninja and H-
Ninja, his/her probability of success can be increased by
spamming (O-Ninja) or by reasoning that administrators would
not wish to incur the penalty of scanning the process tree with
a frequency in the single-digit milliseconds. Also, the speed of
the attack at 4 ms was extremely naÂ¨Ä±ve; a more sophisticated
attacker should be able to do better.
Since it uses active monitoring, HT-Ninja was able to detect
all attacks in all tested scenarios.
A main limitation of HT-Ninja, as well as O-Ninja and H-
Ninja, is that they do not detect privilege escalation attacks that
occur within the context of â€œwhite listedâ€ processes. Those
processes, many of which are setuid programs, are ignored
by Ninja. An attacks that compromises (e.g., using buffer
overï¬‚ow) and executes malicious code within the context of
a white listed process would not be detected.
IX. PERFORMANCE EVALUATION
We conducted experiments to measured the performance
overhead of individual HyperTap auditors as well as the
combined overhead of running multiple auditors. We measured
the runtime of the UnixBench7 performance benchmark when
(i) each auditor was enabled, and (ii) all three auditors are
enabled. The target VM was a SUSE 11 Linux VM with 2
vCPUs and 1GiB of RAM. The host computer ran SUSE
11 Linux and the KVM hypervisor, with an 8 core Intel i5
3.07GHz processor and 8 GiB of RAM. The results were
illustrated in Fig. 7. The baseline is the execution time
when running the workloads in the VM without HyperTap
integrated, and the reported numbers are the average of ï¬ve
runs of the workloads.
In most cases, the performance overhead of running all three
auditors simultaneously was (i) only slightly higher than that
of running the slowest auditor, HT-Ninja, individually, and
(ii) substantially lower than the summation of the individual
overheads of all auditors. That result demonstrates the beneï¬ts
of HyperTapâ€™s uniï¬ed logging mechanism.
For the Disk I/O and CPU intensive workloads, all three
auditors together produced less than 5% and 2% performance
losses, respectively. The Disk I/O intensive workloads appear
to have incurred more overhead than CPU intensive workloads
because they generated more VM Exit events, at which point
some monitoring code was triggered.
For
the
context
switching and system call micro-
benchmarks, all three auditors together induced about 10% (or
less) and 19% performance losses, respectively. It is important
to note that those micro-benchmarks were designed to measure
the performance of individual speciï¬c operations without
any useful processing; they do not necessarily represent the
performance overhead of general applications. The relatively
high overhead was caused by the HyperTap routines enabled
for logging those benchmarked operations. Since only HT-
Ninja needs to log system calls, it was the primary source
of the overhead in the system call micro-benchmark case.
X. CONCLUSIONS
This paper presents principles for unifying RnS monitoring.
We identify the boundary dividing the logging and auditing
phases in monitoring processes. That boundary allows us
to unify and develop dependable logging mechanisms. We
demonstrate the need for an isolated root of trust and ac-
tive monitoring to support a wide variety of RnS monitors.
We applied those principles when developing HyperTap, a
framework that provides uniï¬ed logging, based on hardware
invariants, to safeguard VM environments. The feasibility of
the framework was demonstrated through the implementation
and evaluation of three monitors: Guest OS Hang Detection,
Hidden RootKit Detection, and Privilege Escalation Detection.
In all cases, the use of architectural invariants was central to
the high quality and performance observed in the experiments.
We presented additional analysis of the method so that other
reliability and security monitors can be built on top of the
HyperTap framework.
7http://code.google.com/p/byte-unixbench/
23232323
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:24 UTC from IEEE Xplore.  Restrictions apply. 
XI. ACKNOWLEDGMENTS
This material is based upon work supported in part by
the National Science Foundation under Grant No. CNS 10-
18503 CISE, by the Army Research Ofï¬ce under Award
No. W911NF-13-1-0086, by the National Security Agency
(NSA) under Award No. H98230-14-C-0141, by the Air Force
Research Laboratory and the Air Force Ofï¬ce of Scientiï¬c
Research under agreement No. FA8750-11-2-0084, by an IBM
faculty award, and by Infosys Corporation. Any opinions,
ï¬ndings, and conclusions or recommendations expressed in
this material are those of the author and do not necessarily
reï¬‚ect the views of the National Science Foundation, or other
organizations.
REFERENCES
[1] M. Bishop, â€œA model of security monitoring,â€ in Fifth Annual Computer
Security Applications Conference.
IEEE, 1989, pp. 46â€“52.
[2] S. Bahram, X. Jiang, Z. Wang, M. Grace, J. Li, D. Srinivasan, J. Rhee,
and D. Xu, â€œDKSM: Subverting virtual machine introspection for fun
and proï¬t,â€ in 29th IEEE Symposium onReliable Distributed Systems,
2010, pp. 82â€“91.
[3] B. D. Payne, M. Carbone, M. Sharif, and W. Lee, â€œLares: An architecture
for secure active monitoring using virtualization,â€ in Security and
Privacy, 2008. SP 2008. IEEE Symposium on.
IEEE, 2008, pp. 233â€“
247.
[4] H. Moon, H. Lee, J. Lee, K. Kim, Y. Paek, and B. B. Kang, â€œVigilare:
Toward snoop-based kernel integrity monitor,â€ in In Proc. of the 2012
ACM Conference on Computer and Communications Security, ser. CCS
â€™12. New York, NY, USA: ACM, 2012, pp. 28â€“37.
â€œNinja:
for
[5] T.
R.
Flo,
Privilege
escalation
Ubuntu
detec-
Manual,
tion
http://manpages.ubuntu.com/manpages/lucid/man8/ninja.8.html, 2005.
gnu/linux,â€
system
[6] R. G. Ragel and S. Parameswaran, â€œIMPRES: Integrated monitoring for
processor reliability and security,â€ in In Proc. of the 43rd Annual Design
Automation Conference, ser. DAC â€™06. New York, NY, USA: ACM,
2006, pp. 502â€“505.
[7] N. Nakka, Z. Kalbarczyk, R. K. Iyer, and J. Xu, â€œAn architectural
framework for providing reliability and security support,â€ in Dependable
Systems and Networks, 2004 International Conference on.
IEEE, 2004,
pp. 585â€“594.
[8] K. Pattabiraman, â€œAutomated derivation of application-aware error
and attack detectors,â€ Ph.D. dissertation, Champaign, IL, USA, 2009,
aAI3363053.
[9] X. Jiang, X. Wang, and D. Xu, â€œStealthy malware detection and monitor-
ing through VMM-based out-of-the-box semantic view reconstruction,â€
vol. 13, no. 2. New York, NY, USA: ACM, Mar. 2010, pp. 12:1â€“12:28.
[10] B. D. Payne, M. de Carbone, and W. Lee, â€œSecure and ï¬‚exible moni-
toring of virtual machines,â€ in Twenty-Third Annual Computer Security
Applications Conference (ACSAC).
IEEE, 2007, pp. 385â€“397.
[11] T. Garï¬nkel and M. Rosenblum, â€œA virtual machine introspection based
architecture for intrusion detection,â€ in In Proc. Network and Distributed
Systems Security Symposium, 2003, pp. 191â€“206.
[12] B. Dolan-Gavitt, T. Leek, M. Zhivich, J. Gifï¬n, and W. Lee, â€œVirtuoso:
Narrowing the semantic gap in virtual machine introspection,â€ in Secu-
rity and Privacy (SP), 2011 IEEE Symposium on.
IEEE, 2011, pp.
297â€“312.
[13] O. S. Hofmann, A. M. Dunn, S. Kim, I. Roy, and E. Witchel, â€œEnsuring
operating system kernel integrity with osck,â€ in In Proc. of the Sixteenth
International Conference on Architectural Support for Programming
Languages and Operating Systems, ser. ASPLOS XVI. New York,
NY, USA: ACM, 2011, pp. 279â€“290.
[14] R. Hund, T. Holz, and F. C. Freiling, â€œReturn-oriented rootkits: Bypass-
ing kernel code integrity protection mechanisms,â€ in In Proc. of the 18th
USENIX Security Symposium, 2009, pp. 383â€“398.
[15] J. Rhee, R. Riley, D. Xu, and X. Jiang, â€œDefeating dynamic data
kernel rootkit attacks via vmm-based guest-transparent monitoring,â€
in International Conference on Availability, Reliability and Security
(ARES).
IEEE, 2009, pp. 74â€“81.
[16] B. Dolan-Gavitt, T. Leek, J. Hodosh, and W. Lee, â€œTappan zee (north)
bridge: mining memory accesses for introspection,â€ in In Proc. of the
2013 ACM SIGSAC conference on Computer &#38; communications
security, ser. CCS â€™13. New York, NY, USA: ACM, 2013, pp. 839â€“
850.
[17] S. T. Jones, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau, â€œAntfarm:
Tracking processes in a virtual machine environment,â€ in In Proc. of the
USENIX Annual Technical Conference, 2006, pp. 1â€“14.
[18] â€”â€”, â€œVmm-based hidden process detection and identiï¬cation using
lycosid,â€ in In Proc. of the Fourth ACM SIGPLAN/SIGOPS International
Conference on Virtual Execution Environments, ser. VEE â€™08. New
York, NY, USA: ACM, 2008, pp. 91â€“100.
[19] A. Dinaburg, P. Royal, M. Sharif, and W. Lee, â€œEther: Malware analysis
via hardware virtualization extensions,â€ in In Proc. of the 15th ACM
Conference on Computer and Communications Security, ser. CCS â€™08.
New York, NY, USA: ACM, 2008, pp. 51â€“62.
[20] F. Zhang, K. Leach, K. Sun, and A. Stavrou, â€œSpectre: A dependable
introspection framework via system management mode,â€ in In Proc. of
The 43rd Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSNâ€™13), June 2013.
[21] D. Pelleg, M. Ben-Yehuda, R. Harper, L. Spainhower, and T. Adeshiyan,
â€œVigilantâ€“out-of-band detection of failures in virtual machines,â€ Oper-
ating systems review, vol. 42, no. 1, p. 26, 2008.
[22] G. J. Popek and R. P. Goldberg, â€œFormal requirements for virtualizable
third generation architectures,â€ pp. 121â€“, 1973.
[23] L. Wang, Z. Kalbarczyk, W. Gu, and R. K. Iyer, â€œAn os-level framework
for providing application-aware reliability,â€ in Dependable Computing,
2006. PRDCâ€™06. 12th Paciï¬c Rim International Symposium on.
IEEE,
2006, pp. 55â€“62.
[24] J. Demme, M. Maycock, J. Schmitz, A. Tang, A. Waksman, S. Sethu-
madhavan, and S. Stolfo, â€œOn the feasibility of online malware detection
with performance counters,â€ SIGARCH Comput. Archit. News, vol. 41,
no. 3, pp. 559â€“570, Jun. 2013.
[25] K. S. Yim, Z. T. Kalbarczyk, and R. K. Iyer, â€œQuantitative analysis of
long-latency failures in system software,â€ in Dependable Computing,
2009. PRDCâ€™09. 15th IEEE Paciï¬c Rim International Symposium on.
IEEE, 2009, pp. 23â€“30.
[26] A. Kivity, Y. Kamay, D. Laor, U. Lublin, and A. Liguori, â€œkvm: the
linux virtual machine monitor,â€ in In Proc. of the Linux Symposium,
vol. 1, 2007, pp. 225â€“230.
[27] J. Butler and G. Hoglund, â€œViceâ€“catch the hookers,â€ Black Hat USA,
vol. 61, 2004.
[28] D. Sd, â€œLinux on-the-ï¬‚y kernel patching without lkm,â€ Phrack Magazine
#58, Article 7, http://www.phrack.org/issues.html?id=7&issue=58, 2001.
[29] T. Garï¬nkel, â€œTraps and pitfalls: Practical problems in system call
interposition based security tools,â€ in In Proc. of the Network and
Distributed Systems Security Symposium, vol. 33, 2003.
[30] N. Provos, â€œImproving host security with system call policies,â€ in
the 12th USENIX Security Symposium, vol. 1, no. 8.
In Proc. of
Washington, DC, 2003, p. 10.
[31] A. P. Kosoresow and S. Hofmeyer, â€œIntrusion detection via system call
traces,â€ Software, IEEE, vol. 14, no. 5, pp. 35â€“42, 1997.
[32] J. Criswell, N. Geoffray, and V. S. Adve, â€œMemory safety for low-level
software/hardware interactions.â€ in USENIX Security Symposium, 2009,
pp. 83â€“100.
[33] J. Criswell, A. Lenharth, D. Dhurjati, and V. Adve, â€œSecure virtual
architecture: A safe execution environment for commodity operating
systems,â€ in In Proc. of Twenty-ï¬rst ACM SIGOPS Symposium on
Operating Systems Principles, ser. SOSP â€™07. New York, NY, USA:
ACM, 2007, pp. 351â€“366.
[34] D. Cotroneo, R. Natella, and S. Russo, â€œAssessment and improvement
of hang detection in the linux operating system,â€ in Reliable Dis-
tributed Systems, 2009. SRDSâ€™09. 28th IEEE International Symposium
on.
IEEE, 2009, pp. 288â€“294.
[35] T. Ormandy, â€œThe gnu c library dynamic linker expands $origin in se-
tuid library search path,â€ http://seclists.org/fulldisclosure/2010/Oct/257,
2010, [Online; accessed 29-April-2013].
[36] SecurityFocus, â€œLinux kernel cve-2013-1763 local privilege escalation
vulnerability,â€ http://www.securityfocus.com/bid/58137/info, 2013, [On-
line; accessed 29-April-2013].
[37] S. Jana and V. Shmatikov, â€œMemento: Learning secrets from process
footprints,â€ in Security and Privacy (SP), 2012 IEEE Symposium on,
2012, pp. 143â€“157.
24242424
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:24 UTC from IEEE Xplore.  Restrictions apply.