### Severity Levels and Incident Examples

Incidents are categorized into three severity levels, ranging from SEV3 (lowest severity) to SEV1 (highest severity). The SEV level reflects the highest impact of an incident and is not downgraded as the resolution progresses. Table 3 provides examples of incidents for each SEV level.

| Level | Incident Examples |
|-------|-------------------|
| **SEV3** | Redundant or contained system failures, minor system impairments that do not significantly affect customer experience, internal tool failures. |
| **SEV2** | Service outages affecting specific Facebook features, regional network impairments, critical internal tool outages that put the site at risk. |
| **SEV1** | Entire Facebook product or service outage, data center outage, major portions of the site being unavailable, outages affecting multiple products or services. |

**Table 3: SEV Levels and Incident Examples**

### Distribution of Network-Related SEVs in 2017

Figure 4 illustrates the distribution of network-related SEVs among different network devices in 2017. We draw four key conclusions from this figure:

1. **Core Devices**: While Core devices have the highest number of SEVs, their impact is generally low, with approximately 81% of SEVs being SEV3, 15% being SEV2, and 4% being SEV1.
2. **RSWs**: RSWs have nearly as many incidents as Core network devices, but with a more significant impact.

**Figure 4: The distribution of SEVs and their level among different network devices in 2017.**

### Trend Analysis of SEV Levels Over Time

Figure 5 shows the trend of each SEV level over the years, normalized to the total number of devices in the population during that year. A significant inflection point occurred in 2015, corresponding to the deployment of fabric data center networks. Prior to 2015, the rate of SEV3s grew almost exponentially.

**Figure 5: The number of network-related SEVs over time, normalized to the number of deployed network devices.**

### Correlation Between Engineers and SEVs

We hypothesized that an increase in the number of engineers working on network devices might lead to more SEVs. To test this, we used publicly available data [71] to plot the yearly rate of SEVs per employee, our best proxy for engineers, working at Facebook. The trends resembled those in Figure 5. To understand why, we plotted the normalized number of switches at Facebook versus the number of Facebook employees in Figure 6. Switches grew in proportion to employees, leading us to conclude that the number of engineers working on network devices does not correlate with an increase in network device failures.

**Figure 6: The normalized number of switches at Facebook versus the number of Facebook employees.**

### Incident Distribution

- **Stable incident rates for fabric devices over time.**
- **Increasing RSW incident rates over time.**
- **Most incidents on high bandwidth or low redundancy devices.**

The incident rate measures the frequency of incidents induced by each device type but does not reflect the overall amount of network incidents that need to be handled in the field. Figure 7 shows the distribution of incidents caused by each type of network device on a yearly basis. Cluster-specific network devices (CSAs and CSWs) have shown a decreasing proportion of incidents over time, as more fabric-based data centers are built. Fabric network devices have not demonstrated large increases in incidents, suggesting good fault tolerance. RSW-related incidents have been steadily increasing, driven by the growth in the rack population and Facebook’s single RSW design for cost-effectiveness.

**Figure 7: Fraction of network incidents per year broken down by device type.**

### Incidents by Network Design

- **Cluster device incidents scale super-linearly.**
- **Lower incidents per device for fabric devices.**
- **Half the fabric device incidents versus cluster.**

Data center topologies play a crucial role in network reliability. Figure 9 shows how the proportion of network incidents from different network topologies has changed over time. Fabric topologies have been adopted steadily since 2015, leading to a decrease in cluster network incidents. Fabric networks have consistently had lower incidents per device compared to classic cluster networks.

**Figure 9: Number of incidents for each network design type, normalized to a fixed baseline, the total number of SEVs in 2017.**

### Reliability of Data Center Switches

- **Failure rate varies by three orders of magnitude across switch types.**
- **Fabric switches fail 3.2× less frequently than cluster switches.**
- **Larger networks increase incident resolution time.**

We analyzed the reliability of Facebook data center switches using SEV timing data to measure mean time between incidents (MTBI) and the 75th percentile (p75) incident resolution time (p75IRT). MTBI did not change more than 10× across each switch type, except for CSAs, which saw a two-order-of-magnitude increase in MTBI after operational improvements. In 2017, fabric network switches failed 3.2× less frequently than cluster network switches, attributed to simpler switches, software management, and automated repairs.

**Figure 12: Mean time between incidents in hours for different network device types. Note that the y-axis is in logarithmic scale.**

**Figure 13: p75 incident resolution time for each switch type by year.**

**Figure 14: Positive correlation between p75IRT and the number of switches.**

### Intra Data Center Reliability Implications

- **Software-based failure tolerance techniques for data center networks.**
- **High variance in both edge MTBF and MTTR.**

At Facebook's scale, we focus on developing software systems that are more tolerant to network hardware failures, featuring data replication and failover techniques, rather than over-provisioning networking devices. We aim to move from defensive software techniques to co-designing software alongside network infrastructure, making software aware of the reliability characteristics of the network.

**Figure 15: Edge MTBF in hours as a function of the percentage of edges with that MTBF or lower.**

### Inter Data Center Reliability

In this section, we study the reliability of backbone networks. We analyze network failures between Facebook’s data centers over eighteen months, from October 2016 to April 2018, comprising tens of thousands of real-world events.