into three severity levels from SEV3 (lowest severity) to SEV1 (high-
est severity). A SEV level reflects the high water mark for an in-
cident. A SEV’s level is never downgraded to reflect progress in
resolving the SEV. Table 3 provides examples of incidents for each
SEV level.
Level
SEV3
SEV2
SEV1
Incident Examples
Redundant or contained system failures, system im-
pairments that do not affect or only minimally affect
customer experience, internal tool failures.
Service outages that affect a particular Facebook fea-
ture, regional network impairment, critical internal
tool outages that put the site at risk.
Entire Facebook product or service outage, data cen-
ter outage, major portions of the site are unavailable,
outages that affect multiple products or services.
Table 3: SEV levels and incident examples.
Figure 4 shows how all the network-related SEVs in 2017 were
distributed among network devices. We draw four conclusions from
Figure 4 that complement our raw incident rate findings from §5.2:
(1) While Core devices having the highest number of SEVs, their
impact is typically low, with around 81% of SEVs being level
3, 15% being level 2, and 4% being level 1. RSWs have nearly
as many incidents as Core network devices, with impact
Figure 4: The distribution of SEVs and their level among dif-
ferent network devices in 2017.
Figure 5 shows how the rate of each SEV level has changed
over the years, normalized to the total number of devices in the
population during that year. While we do not disclose the absolute
size of the population, we note that it is orders of magnitude larger
than similar studies, such as Turner et al. [74]. The main conclusion
we draw from Figure 5 is that the overall rate of SEVs per device
had an inflection point in 2015, corresponding to the deployment of
fabric data center networks. This was a significant turnaround, as
prior to 2015, the rate of SEV3s grew at a nearly exponential rate.
Figure 5: The number of network-related SEVs over time
normalized to the number of deployed network devices.
We wondered whether more engineers working on network
devices led to more SEVs. To test this theory, we used publicly
0.00.20.40.60.81.0SEV1SEV2SEV3Fraction of IncidentsCoreCSACSWESWSSWFSWRSWN=82%N=13%N=5%ClusterFabric0E+01E-32E-33E-32011201220132014201520162017Incidents per deviceSEV3SEV2SEV1Fabric deployedSEV3SEV2SEV1IMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
available data [71] to plot the yearly rate of SEVs per employee, our
best proxy for engineers, working at Facebook, and found that the
trends resembled those in Figure 5. To understand why, we plot the
normalized number of switches at Facebook versus the number of
Facebook employees in Figure 6. Switches grew in proportion to
employees. We conclude that for our study, the number of engineers
working on network devices does not correlate with an increase in
network device failures.
Figure 6: The normalized number of switches at Facebook
versus the number of Facebook employees.
5.4 Incident Distribution
• Stable incident rates for fabric devices over time
• RSW incident rates increasing over time
• Most incidents on high bandwidth or low redundancy devices
The incident rate measures the frequency of the incidents in-
duced by each device type. However, it does not reflect the overall
amount of the network incidents that need to be handled in the field.
Figure 7 shows the distribution of incidents caused by each type of
network device on a yearly basis. We can see that network devices
specific to the cluster network (CSAs and CSWs) have smaller pro-
portion of incidents over time, as more and more fabric-based data
centers are built and turned up (we analyze this trend in Section 5.5).
Interestingly, we can observe that devices in the fabric network
have not demonstrated any large increases in incidents over time.
This again suggests that fabric-based data center topologies with
automated failover provide good fault tolerance. On the other hand,
RSW-related incidents have been steadily increasing over time (a
finding that corroborates Potharaju et al. [62, 63]). This is partially
driven by an increase in the size of the rack population over time. In
addition, this is also a result of Facebook’s data center network de-
sign, where we use only one single RSW as the Top-Of-Rack (TOR)
switch. Some other companies, especially cloud service providers
and enterprises, typically design with two TORs and have each
server connected to both TORs for redundancy. At Facebook’s
scale, we find that it is more cost-effective to handle RSW failures
in software using replication and distribution of server resources
than to use redundant RSWs in every rack.
We also plot the relative number of incidents induced by each
device type in Figure 8, using a fixed baseline, the total number of
SEVs in 2017. There is a general increase in number of incidents
across all device types until 2015 (with incidents due to some devices
like FSWs and ESWs continuing to grow). The increasing number
of incidents can be attributed to the rapid growth of Facebook’s
data center infrastructure: from 2011 to 2017 the total number of
network device SEVs increased by 9.4×.
Figure 8: The number of incidents per year for different net-
work device types, normalized to a fixed baseline, the total
number of SEVs in 2017.
We find that rack switches contribute to around 28% of the
service-level incidents that occurred in the past year (2017). This
is due to the sheer size of the rack switch population (an affect
we examine in §5.5), which leads to a noticeable reliability effect
on the software systems running in data centers. In addition, we
find that Core network devices that connect data centers and the
backbone contribute to around 34% of service-level incidents com-
pared to other network devices. This is because network devices
with higher bisection bandwidth tend to affect a larger number
of connected downstream devices and are thus correlated with
widespread impact when these types of devices fail.
Figure 7: Fraction of network incidents per year broken
down by device type.
5.5 Incidents by Network Design
• Cluster device incidents scale super-linearly
0.00.20.40.60.81.0050001000015000200002500030000Normalized switchesEmployees0.00.10.20.30.40.52011201220132014201520162017Fraction of incidentsCoreCSACSWESWSSWFSWRSWCoreCSAESWCSWFSWSSWRSWClusterFabricFabric deployed0.00.10.20.30.42011201220132014201520162017Fraction of incidentsCoreCSACSWESWSSWFSWRSWCoreCSAESWCSWFSWSSWRSWClusterFabricFabric deployedA Large Scale Study of Data Center Network Reliability
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
• Lower incidents per device for fabric devices
• Half the fabric device incidents versus cluster
As revealed in §5.2 and §5.4, data center topologies play an
important role on the network reliability. Figure 9 shows how the
proportion of network incidents from different network topologies
has changed over time. The proportion is calculated by aggregating
network incidents across all of the devices that make up the network
design, so every data point in Figure 9 is normalized to the same
common baseline. For the cluster network, the devices are the CSAs
and the CSWs. For the fabric network, the devices are the ESWs,
the SSWs, and the FSWs. We make three major findings.
First, note that fabric topologies have been adopted steadily since
2015. Hence, an inflection point in network incidents of the cluster
network occurs with the introduction of the fabric network in 2015.
This corresponds to when Facebook began to replace classic net-
works with data center fabric networks. Similarly, fabric networks
have seen an increase in incident rates over time.
Figure 9: Number of incidents for each network design type
normalized to a fixed baseline, the total number of SEVs in
2017.
Figure 10 shows another way to understand this trend: by com-
paring the incidents for each network type to the size of its respec-
tive population of devices (i.e., incidents per device). What we see
is that for classic cluster network devices, incidents have scaled
super-linearly with population size until around 2014, when it be-
came challenging to made additional reliability improvements to
the classic cluster network design. Since its introduction in 2015, the
fabric based network design has consistently had lower incidents
per device compared to the classic cluster network design.
Figure 10: Incidents for each network design type normal-
ized to the size of their respective populations over time.
Second, in the last year (2017), the number of incidents for data
center fabric devices was around 50% of that of classic cluster de-
vices. While the proportion of incidents from fabric topologies has
increased over time, the number of incidents per device has re-
mained relatively low compared to classic network topologies (as
Figure 3 showed). Intuitively, this is because the software managed
fault tolerance and automated remediation provided by fabric net-
work topologies can mask some failures which could otherwise
cause incidents in classic network topologies.
Third, we can see that since 2015, as the fabric network has been
adopted more widely and the cluster network has stopped being
deployed, the incident rates for the network devices associated
with each type of network design show a corresponding trend. We
also plot the population breakdown of device types deployed in
Facebook’s data centers over the seven-year span in Figure 11. Aside
from illustrating the proliferation of RSWs in the fleet, it shows that
an inflection point occurs in 2015, when the population of CSWs
and CSAs begin to decrease and the population of FSWs, SSWs, and
ESWs begins to increase. This is due to the adoption of data center
fabric topologies across more Facebook data centers.
Figure 11: Population breakdown by network device type
over the 7-year span.
0.00.20.40.60.81.02011201220132014201520162017Fraction of incidentsClusterFabricFabric deployed0.000.010.020.030.040.052011201220132014201520162017Incidents per deviceClusterFabricFabric deployed1E-41E-31E-21E-11E+02011201220132014201520162017Fraction of switchesCoreCSACSWESWSSWFSWRSWCoreCSAESWCSWFSWSSWRSWClusterFabricFabric deployedIMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
We conclude that, compared to the cluster network, data cen-
ter fabric networks contributed to around 50% of the number of
intra data center network incidents in the past year (2017). We find
that data center fabric networks are more reliable due to their sim-
pler, commodity-chip based switches and automated remediation
software that dynamically adapts to and tolerate device failures.
5.6 Switch Reliability
• Failure rate varies by three orders of magnitude across switch types
• Fabric switches fail 3.2× less frequently than cluster switches
• Larger networks increase incident resolution time
We analyze last the reliability of Facebook data center switches.
We use SEV timing data to measure mean time between incidents
(MTBI) and 75th percentile (p75) incident resolution time (p75IRT).
p75IRT deserves additional explanation. Engineers at Facebook
document resolution time, not repair time, in a SEV. Resolution
time exceeds repair time and includes time engineers spend on
prevention. To prevent occasional months-long incident recovery
times from dominating the mean, we examine the 75th percentile
incident resolution time.
MTBI. We measure the average time between the start of two
consecutive incidents for MTBI. Figure 12 plots MTBI for each
switch type by year. We draw three conclusions from the data.
Third, we find in 2017, fabric network switches fail 3.2× less
frequently than cluster network switches, with an average MTBI
of 2636818 device-hours compared to 822518 device-hours. We
attribute the higher reliability of fabric switches to their simple
switches, software management, and automated repairs.
p75IRT. We measure the average time between the start and the
resolution of incidents for p75IRT. Figure 13 plots p75IRT for each
switch type by year.
Figure 13
We find over the seven years from 2011 to 2017, p75IRT in-
creased similarly across switch types. The increase happened with-
out changes to individual switch design, operation, and manage-
ment. To explain the overall increase in p75IRT, we plot p75IRT
versus the normalized number of switches at Facebook in Figure 14.
Figure 12: Mean time between incidents in hours for differ-
ent network device types. Note that the y axis is in logarith-
mic scale.
First, we find over the seven years from 2011 to 2017, MTBI did
not change more than 10× across each switch type, except CSAs. In
2015, in response to frequent CSA maintenance incidents, engineers
strengthened CSA operational procedure guidelines, adding checks
to ensure operators drained CSAs before performing maintenance,
for example. These operational improvements increased CSA MTBI
by two orders of magnitude between 2014 and 2016.
Second, we find in 2017, MTBI varied by three orders of mag-
nitude across switch types from 39 495 device-hours for Cores to
9 958 828 device-hours for RSWs. If we compare MTBI to switch
type population size in 2017 in Figure 11, we find devices with
larger population sizes tend to have larger MTBIs. This is not mere
correlation. Facebook designs its switches to ensure their rate of
failure does not overwhelm engineers or automated repair systems.
Figure 14
We observe a positive correlation between p75IRT and number
of switches. At Facebook, we find larger networks increase the time
humans take to resolve network incidents. We attribute part of the
increased resolution time to more standardized processes for releas-
ing fixes to production. Today, switch configuration and software
changes go through more thorough review processes, testing, and
deployment than in the past. We observe the additional time spent
in resolution contributes toward more reliable fixes and reduces
the likelihood of repeat incidents.
1E+31E+41E+51E+61E+71E+82011201220132014201520162017Mean time between incidents (hours)CoreCSACSWESWSSWFSWRSWCoreCSAESWCSWFSWSSWRSWClusterFabricFabric deployed1E-11E+01E+11E+21E+32011201220132014201520162017p75 incident resolution time (hours)CoreCSACSWESWSSWFSWRSWFabric deployed0.00.20.40.60.81.0050100150200250300350Normalized switchesp75 incident resolution time (hours)A Large Scale Study of Data Center Network Reliability
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
5.7 Intra Data Center Reliability Implications
Software-based failure tolerance techniques for data center
networks. At Facebook’s scale, we have made an effort to spend
the time to develop software systems that are more tolerant to
network hardware failures (featuring data replication and failover
techniques) than to over-provision networking devices, such as
rack switches, which make up the majority of Facebook’s network
device fleet. We are interested in understanding how we can move
from designing defensive software techniques that treat the net-
work as a black box with potentially unknown failure modes toward
co-designing software alongside network infrastructure. This will
involve making software aware of the reliability characteristics of
the network, such as those we have reported in this study.
• High variance in both edge MTBF and MTTR
We first analyze the MTBF and MTTR of the edges in Facebook’s
backbone network. An edge fails when a combination of planned
fiber maintenances or unplanned fiber cuts sever its backbone and
Internet connectivity. An edge recovers when repairs restore its
backbone and Internet connectivity.
MTBF. The solid line in Figure 15 plots edge MTBF in hours as a
function of the percentage of edges with that MTBF or lower. Most
edges fail infrequently because fiber vendors strive to maintain
reliable links. 50% of edges fail less than once every 1710 h, or
2.3 months. And 90% of edges fail less than once every 3521 h, or
4.8 months.
Software assisted networks as an alternative to software de-
fined networks. At Facebook we have attempted to strike the right
balance between network reliability and software complexity in our
infrastructure. Instead of shouldering the burden of maintaining
fully programmable network topologies, we have opted for simpler
fabric-based topologies assisted by centralized automated reme-
diation software. Yet, it is unclear how much automation should
be employed, how this automation can be standardized across an
industry, and how to provide the flexibility for large scale web ser-
vice companies to easily deploy and configure it without relying
on their own custom purpose hardware.
Preparing software systems for network incidents. We ob-
serve that it is challenging for software systems to prepare for net-
work incidents. At Facebook, we run periodical tests, including both
fault injection testing [9, 73] and disaster recovery testing [46, 66],
to exercise the reliability of our production systems by simulating
different types of network failures, such as device outages and dis-
connection of an entire data center. Yet despite these efforts, we still
experience system-level issues caused by network incidents. We
are exploring proactive solutions to help our production systems
anticipate and defend against network incidents.
6 INTER DATA CENTER RELIABILITY
In this section, we study the reliability of backbone networks. We
analyze network failures between Facebook’s data centers over
the course of eighteen months, from October 2016 to April 2018,
comprising tens of thousands of real world events, comparable in