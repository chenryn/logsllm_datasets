of C but small Nu∈C and ρu∈C, just considering about the
participation rate ρu∈C will fail to tease out elite Sybil users.
We then take ρu∈C into consideration to construct the ﬁnal
index, Sybilness, to determine a speciﬁc user’s legitimacy. To
be speciﬁc, for assigning a Sybilness score f to each user u
on Dianping, we take a weighted average method on Nu∈C
with respect to each of the corresponding coefﬁcients ρu∈C,
for all C, as shown below:
ρu∈C · Nu∈C .
f (u) =
C
(4)
Eventually, we use the Sybilness score f (·) to determine the
perceived likelihood that a user is an elite Sybil user or not
(Note: Sybilness score here can be greater than 1.).
Annotating reviews posted by elite Sybil users. Since not
all reviews posted by elite Sybil Users are fake, we annotate
each reviews with a score deﬁned as ρu∈C · PC(k), for any k.
This score can be used as a criterion to ﬁlter fake reviews or
regulate the frequency of CAPTHCHAs.
V. EVALUATION
(cid:88)
We implement ELSIEDET and evaluate it on a large-scale
dataset of Dianping. Our evaluation covers the following
aspects: Sybil community detection, elite Sybil user detection,
and system performance.
8
A. Data Collection
In this section, we will introduce the datasets used and
propose the methodology we use to gain the ground-truth data.
TABLE III
BREAKDOWNS OF STORES
# Stores
# Overhyped
Stores
Percentage of
Overhyped Stores
Type
Cinema
Hotel
Restaurant
Entertainment
Wedding Service
Beauty Store
Fitness Center
Living Service
Scenic Spots
Shopping
Infant Service
Car
235
1,738
22,474
1,384
320
1,460
326
863
1,243
2,466
216
148
67
71
134
1,244
73
8
35
7
10
14
22
0
0
0
30.21%
7.71 %
5.54 %
5.27 %
2.50 %
2.40 %
2.15 %
1.16 %
1.13 %
0.89 %
0 %
0 %
0 %
Decoration Company
Dataset. We develop a Python-based crawler to analyze
HTML structure of store pages and user pages on Dianping.
All reviews were crawled by the web crawler from January
1, 2014 to June 15, 2015. Starting from the four hand-picked
overhyped stores (the seed list) in the training set belonging
to the same Sybil organization, which we discovered during
our month-long investigation. We then crawled outwards—
crawling one level down of all users who wrote reviews in
these stores and extended the store list that was commented
by these users. Second, we crawled all reviews appearing in
these stores and collected all users of these reviews to form a
user list. The web crawler repeated these steps until reaching
32, 940 stores on the store list. Eventually, our resulting data
set has 10, 541, 931 reviews, 32, 933 stores, and 3, 555, 154
users. We will make all of our data used publicly available in
the future. Furthermore, we categorize the stores crawled into
13 types (see breakdowns in Table III). In Table III, the 13 cat-
egories are shown in decreasing order in terms of percentage
of overhyped stores. Followed by our detection methodology,
surprisingly, we ﬁnd that more than 30% overhyped stores are
pertinent to cinemas. The main remaining overhyped stores
are hotels, restaurants, and places of entertainment.
Ground-truth dataset. Similar to the previous research [13,
29, 40], we rely on manually labeled data for Sybil community
detection. In order to classify the communities as benign or
Sybil using supervised learning, a subset of the communities
needs to be labeled. To carry out the labeling, we actively
exchanged ideas with Dianping of how high-proﬁle Sybil users
resemble. Particularly, the ﬁnal manual labeling considers the
following three criteria. If two of them are satisﬁed, then a
community is labeled as a Sybil community.
(a) Massive ﬁltered reviews by Dianping signify that a large
proportion of reviews posted in a community are ﬁltered by
Dianping’s Sybil detection system. Reviews that Dianping has
classiﬁed as illegitimate using a combination of algorithmic
techniques, simple heuristics, and human expertise. Filtered
reviews are not published on Dainping’s store/user pages. If
we ﬁnd a great proportion of reviews existing in our dataset but
missing on Dianping’s main listings, this indicates that these
reviews have been ﬁltered. Although a review can be ﬁltered
for many reasons, such as overly-ﬂorid or low-quality reviews,
ﬁltered reviews are, of course, partial indicators of being Sybil.
If massive reviews have been ﬁltered in a community, then the
community has a high possibility to be Sybil.
(b) Duplicate user reviews mean that reviews posted by a
user belonging to a community only serve one or two store(s)
with similar content. To our observation, reviews posted by
a benign user of a community are often evenly distributed
in miscellaneous stores. The existence of duplication signiﬁes
that Sybil users are more addicted to boosting review ratings
in only a few stores in a community. This feature is stricter
than the collusive reviews deﬁned in this paper.
(c) Spatio-temporal review pattern means that an unusual
sudden jump with respect to the number of reviews of a target
restaurant/store in a community is consistent with a collusive
action of the Sybil community, by rule of thumb. Normally,
the reviews of a store are evenly distributed since its inception.
Hence, if many stores appearing in a community demonstrate
unreasonable spatio-temporal patterns, then the community is
highly likely to be Sybil.
To do this, we did not hire Amazon Mechanical Turk
(AMT) to accomplish the tasks because scrutinizing those
reviews requires deep familiarity with Chinese language and
the Dianping platform per se. Instead, we hired 5 Chinese un-
dergraduate students to classify communities as either benign
or Sybil. For the rare cases where there was not a consensus,
we used voting. For example, a community would be labeled
as Sybil if and only if the 5 votes are SSSBB, SSSSB or
SSSSS, with S representing Sybil and B representing benign.
B. Results and Detection Accuracy
Accuracy of Sybil community detection. For the dataset
used, ELSIEDET detects in total 710 communities. By using
the multiple criteria shown above, we randomly picked up 170
communities as ground truth and labeled 117 Sybil communi-
ties as well as 53 benign communities. The assumption that a
community only takes a binary classiﬁcation can be justiﬁed
by the empirical percentage of Sybil (resp. benign) users
taking up in the designated Sybil (resp. benign) communities.
To justify this, we took a look at each of the 1, 969 users
of 74 communities (54 Sybil vs. 20 benign) obtained from
ground truth (which is more than 10% of the total amount of
communities), still by following the above criteria to check
each user in communities. We conclude that 96.85% of the
users are designated to the correct community labels. With
8 features tabulated in Table II, we also compare several
classiﬁers implemented by scikit-learn library [1]. We perform
grid search to determine optimal parameters for each clas-
siﬁer and evaluate their performance on weighted precision,
weighted recall, weighted F1 score, and AUC (Area under
the Curve of ROC) using 5-fold cross-validation. As shown in
Table IV, support vector machine (SVM) performs best among
9
we label 12, 292 elite Sybil users in total. Instead of binary
classiﬁcation, ELSIEDET ranks elite Sybil users according to
the Sybilness score function (see Equation (4)).
To carry out the ultimate validation on elite Sybil users
detected from ELSIEDET, we rely on human knowledge. In
concrete, for each detected elite Sybil user, we manually
categorize his or her reviews into two types, suspicious
reviews and normal reviews, by inspecting Sybil campaign
time intervals. The manual check then considers the following
three criteria (by rule of thumb): (i) this user is involved in
vast Sybil campaigns; (ii) the intent of suspicious reviews is
aligned with that of Sybil campaigns. For example, in order to
boost reputation in a Sybil campaign, the suspicious reviews
should be 5-star; (iii) suspicious reviews set apart from normal
reviews in terms of spatio-temporal characteristics. If a user
satisﬁes all three criteria, we validate that he or she is an elite
Sybil user. We emphasize that the criteria of manual validation
are stricter than holding the two conditions carried out by
ELSIEDET.
Finally, of all the top 1, 000 suspicious elite Sybil users that
our system ﬂags, through manual validation, we conclude that
938 are indeed elite Sybil users, which leads to a precision rate
of 93.8%. We also randomly sampled 1, 000 ﬂagged users to
generalize the validation results, which also leads to a high
precision rate of 90.7%.
C. System Performance
We evaluate the efﬁciency of ELSIEDET in a server with
Intel CPU E3-1220 v3 @ 3.10GHz and 16G memory. Since
ELSIEDET has to compute potential collusion set and the
pairwise similarity between potential collusive users to con-
struct Sybil social links, this step would be the bottleneck of
efﬁciency. Instead, we implement a parallel program for this
step based on the observation that the computation for each
user is independent. Finally, we implement a single-threaded
program to complete following steps. Specially, for Dianping’s
dataset, the step of computing the pairwise similarity takes
approximately 110 minutes and the remaining steps take
approximately 22 minutes.
VI. MEASUREMENT AND ANALYSIS
In this section, we analyze the behavior of elite Sybil users
and communities. First, we compare the behavior patterns
among benign users, regular Sybil users, and elite Sybil users.
We then discuss the relation between Sybil communities and
elite Sybil users and review manipulation in chain stores.
We study reviews, not belonging to Sybil tasks, posted by
elite Sybil users to speculate their strategies to camouﬂage
fake reviews. Finally, we demonstrate two temporal dynamics
characterized by user posting period and Sybil campaign
duration.
A. Comparison with Regular Sybil Users
Here, we try to explore the distribution of different types of
user levels. Figure 9 shows that the distribution of levels of
users is unevenly distributed for each type of users. As we can
Fig. 7. Comparison between benign and Sybil communities by percentage of
ﬁltered reviews
all classiﬁers with 96.45% F1 score and 99.42% AUC, using
Gaussian (RBF) kernel with parameters chosen C = 18 and
γ = 0.09.
TABLE IV
CLASSIFICATION PERFORMANCE
Classiﬁer
Decision tree
SVM
GNB
KNN
Ada boost
Random forest
F1
Recall
Precision
AUC
93.80 % 92.90 % 93.60 % 92.83 %
96.74 % 96.47 % 96.45% 99.42%
94.21 % 93.44 % 93.57 % 97.64 %
96.75 % 96.47 % 96.50 % 97.45 %
93.84 % 93.54 % 93.60 % 97.92 %
93.16 % 94.01 % 92.99 % 97.42 %
We then apply our trained classiﬁers to predict each commu-
nity. As a result, ELSIEDET identiﬁes 566 Sybil communities
with 22, 324 users, and 144 benign communities with 5, 222
users. Surprisingly, detected Sybil communities signiﬁcantly
outnumber detected benign communities. It is perhaps because
in the community clustering process, the constraints of posting
time and review ratings pose limitations on forming benign
communities. Most benign users are thereby pruned by apply-
ing the Louvain method.
Recall in Section II-A, we note that not all ﬁltered re-
views are fake reviews (some are viewed as useless reviews).
Through our experiments, we conﬁrm that the fake reviews
classiﬁed are more likely to be ﬁltered. As shown in Figure 7,
we compare the percentage of ﬁltered reviews in benign and
Sybil communities, respectively. We observe that
the per-
centage of ﬁltered reviews of Sybil communities signiﬁcantly
outweighs that of benign communities with respect to the
same cumulative probability. Speciﬁcally, we see that 80% of
Sybil communities (resp. benign communities) have more than
80% (resp. less than 50%) of reviews ﬁltered. We conclude
that ﬁltered reviews are more likely fake, which validates the
accuracy of our detection methodology.
Accuracy of elite Sybil users detection. ELSIEDET considers
a user u as an elite Sybil user if the following two conditions
hold: (i) if the user u does not belong to any community;
and (ii) the user participation rate ρu∈C is larger than 0.5
(that is, the average participation rate of users in commu-
nity C), for any community C. According to this criterion,
10
(a) Comparison on the number of fake reviews
(b) Comparison on the percentage of fake reviews (c) Comparison on percentage of ﬁltered reviews
Fig. 8. Comparison between elite Sybil users and regular Sybil users
elite Sybil users post massive reviews not belonging to Sybil
tasks (smoke-screening) to mimic genuine users. Surprisingly,
the distribution of regular Sybil users roughly follows the
Pareto principle (also known as the 80-20 rule) that more than
60% of all the reviews posted by 20% of regular Sybil users
are fake. In contrast, as we can see from Figure 8(b), we show
that only 20% of all the reviews posted by more than 80% of
elite Sybil users are fake, recognizing that the principle also
applies in reverse.
Figure 8(c) plots the CDF of the number of users in terms