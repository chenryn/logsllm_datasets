that there are correlations in the bit strings. So, we tested our
PUF responses with the following ﬁve estimations for non-IID
sources [31]. Each test yields an estimation on min-entropy
and the overall estimated min-entropy is the minimum of these
ﬁve values. The tests are conﬁgured with a conﬁdence level
of 95%.
a) Collision Test: The collision test measures the mean
time to the ﬁrst collision in a dataset. Based on these collision
times, the collision statistic tries to estimate the probability
of the most-likely state. For biased noise sources toward an
output or state the test will result in a low entropy estimate, say
when there is a short mean time until a collision. Longer mean
times on collisions end up with in higher entropy estimates.
b) Partial Collection Test: The partial collection test
computes the entropy of a dataset based on how many distinct
values in the output space are observed. Low entropy estimates
are output for datasets that contain a small number of distinct
symbols, and high entropy estimates are the output when the
bit strings diversify quickly.
c) Markov Test: The Markov test consists of different
Markov processes, from ﬁrst-order up to nth-order. In a ﬁrst-
order Markov process, the output state depends only on the
current state and in an nth-order Markov process, the output
state depends on the current and all previous n-1 states. To
detect dependencies, the test builds a Markov model to be used
as a template for a given source. The min-entropy estimates
result from measuring the dependencies between consecutive
outputs from the noise source. Thereby the estimates are not
based on an estimate of min-entropy per output, but on the
entropy present in any chain of outputs.
d) Compression Test: The compression test estimates the
entropy rate by compressing the input data set. As compression
method the Maurer Universal Statistic [32] is used. It generates
a dictionary of values, and then computes the average number
of samples required to write an output based on the dictionary.
e) Frequency Test: The frequency statistic models the
probability distribution of the given data set. The entropy
estimation is based on the occurrence of the most-likely
symbol.
(a) ρmax = .62.
(b) ρmax = .74.
(c) ρmax = .86.
Fig. 6.
Inter and intra Hamming distance distributions of simulated data.
TABLE III
CTW COMPRESSION RATES ON REAL DEVICE MEASUREMENTS FOR
DIFFERENT UPPER CORRELATION LIMITS ρmax. NOTE THAT THE DATA
SHOWS AN UNCOMPRESSABILITY, DUE TO THEIR SMALL SIZE AND IS
MENTIONED FOR THE PURPOSE OF VERIFICATION.
Size
Size
compression rate compression rate
ρmax uncompressed compressed of measurements
(bytes)
(bytes)
.50
.53
.56
.59
.62
.65
.68
.71
.74
.77
.80
.83
.86
.89
.92
.95
.98
148
164
164
164
164
192
254
254
295
331
292
413
451
496
605
645
978
(bits/byte)
8.25676
8.22561
8.22561
8.22561
8.22561
8.20312
8.16929
8.16929
8.15254
8.12991
8.13356
8.12107
8.11973
8.10282
8.0843
8.08062
8.05828
random ﬁle
(bits/byte)
8.23649
8.18902
8.18902
8.18902
8.18902
8.18229
8.14173
8.14173
8.13559
8.12085
8.13356
8.10412
8.10200
8.09476
8.08099
8.07752
8.05419
165
181
181
181
181
209
272
272
313
349
309
432
470
515
624
664
998
A. Entropy Estimation of Measured Data
We estimated the entropy of the responses with several
different upper correlation limits ρmax from the 70 measured
devices.
1) Inter and Intra Hamming Distances: Fig. 5 shows the
inter and intra Hamming distance distributions of the measured
data for three different values of ρmax. The inter distance
distribution is ﬁtted by a normal distribution. The mean of
the ﬁt is close to 50%. The intra distance distribution is based
on the Monte Carlo simulation (10,000 runs) that we explained
in Section V-B. To be able to identify a device securely, it is
important that the intra and intra distance distributions overlap
just with negligible probability, which is the case here. The
best result do we receive for ρmax = .86.
2) CTW Compression: The compression method was con-
ﬁgured with a tree depth of 6 and we used a Krichevski-
Troﬁmov estimator [25]. It is important to note, that CTW
compression does not work efﬁciently with the small sizes
we give here as input, so all resulting compression rates are
above 100%. Still, would the bit strings have major statistical
defects, then a compression would be possible even with these
small input sizes. For the purpose of veriﬁcation we also tried
to compress truly random bits with the same input sizes as our
responses, yielding similar results. Therefore, our bit strings
show an uncompressability. The results can be found in Table
III.
3) NIST Randomness Test: We used the NIST randomness
tests as described in Section VI-3 on our bit strings. The
minimum pass p(cid:48) rate for each statistical test is approximately
8, because we chose our number of samples n = 10. The
results consist of two values per test and one symbol – the
ﬁrst value is the P-value and the second value represents the
number of passed runs p, where p ≥ p(cid:48) to pass a test. The
third symbol indicates if all conditions for a passed test are
met ((cid:88)) or not (×). The results indicate a high entropy in our
bit strings, since all except three tests fail. Nevertheless, the
tests are not that meaningful because the input size to these
tests is very small.
4) NIST Min-Entropy Estimation: Due to the short overall
bit strings we derived from our measurements, the NIST Min-
Entropy Estimation were not able to calculate valid results. So
we omit these tests in this section.
B. Entropy Estimation on Simulated PUF Responses
We estimated the entropy of bit strings, which offspring
from our measurements from real devices. However, the gen-
erated bit strings are not long enough to generate meaningful
results on entropy estimation. Therefore we repeat the entropy
estimation on simulated data, too. For a conservative estimate
we choose the minimum of our estimated entropy value for
further constructions.
We also validated to concatenate and partly replace simu-
lated bits with the one from our real measurements and we
found no signiﬁcant difference.
1) Inter and Intra Hamming Distances: Fig. 6 shows the
inter and intra Hamming distance distributions of the simulated
data (1,000 runs for both intra and inter distances) for the same
NIST RANDOMNESS TESTS FOR DIFFERENT UPPER CORRELATION LIMITS ρmax. THE FIRST VALUE PER TEST IS THE P-VALUE. THE SECOND VALUE IS
THE NUMBER OF PASSED RUNS p, WHERE p ≥ p(cid:48) TO PASS A TEST. THE THIRD SYMBOL S INDICATES IF ALL CONDITIONS FOR A PASSED TEST ARE MET
TABLE IV
((cid:88)) OR NOT (×).
ρmax
Frequency
Block
Frequency
Cumul.
Sums
Runs
FFT
Approx.
Entropy
Serial
Linear
Complexity
P-value p S
P-value p S
P-value
p S
P-value p S
p S
P-value
P-value
p S
P-value p S
P-value p S
10 (cid:88) .00009 10 ×
9 (cid:88) .54520 8.5 (cid:88) .35049 10 (cid:88) .00430 10 (cid:88) .53415 10 (cid:88) .23641
9 (cid:88) .01791
.03517
9 (cid:88) .91141 10 (cid:88) .00430 10 (cid:88) .03517 10 (cid:88) .37373 9.5 (cid:88) .35049
9 (cid:88) .06688
8 (cid:88)
9 (cid:88) .12233
.03517
9 (cid:88) .91141 10 (cid:88) .00430 10 (cid:88) .03517 10 (cid:88) .37373 9.5 (cid:88) .35049
9 (cid:88) .06688
8 (cid:88)
9 (cid:88) .12233
.03517
8 (cid:88)
9 (cid:88) .91141 10 (cid:88) .00430 10 (cid:88) .03517 10 (cid:88) .37373 9.5 (cid:88) .35049
9 (cid:88) .06688
9 (cid:88) .12233
.03517
9 (cid:88) .91141 10 (cid:88) .00430 10 (cid:88) .03517 10 (cid:88) .37373 9.5 (cid:88) .35049
9 (cid:88) .06688
8 (cid:88)
9 (cid:88) .12233
.03517
8 (cid:88) .73992 10 (cid:88) .00095 10 (cid:88) .73992 10 (cid:88) .44232
10 (cid:88) .00204
9 (cid:88) .32824
8 (cid:88)
8 (cid:88) .06688
.53415
9 (cid:88) .43112 9.5 (cid:88) .12233 10 (cid:88) .00888 10 (cid:88) .06688 10 (cid:88) .63703
10 (cid:88) .35049 10 (cid:88)
.73992 10 (cid:88) .53415
10 (cid:88) .35049 10 (cid:88)
9 (cid:88) .43112 9.5 (cid:88) .12233 10 (cid:88) .00888 10 (cid:88) .06688 10 (cid:88) .63703
.73992 10 (cid:88) .53415
9 (cid:88) .54520 9.5 (cid:88) .73992
9 (cid:88) .00095 10 (cid:88) .00888 10 (cid:88) .63095
9 (cid:88) .21331
9 (cid:88)
.91141 10 (cid:88) .53415
10 (cid:88) .01791 10 (cid:88)
9 (cid:88) .53415 10 (cid:88) .44232 9.5 (cid:88) .01791 10 (cid:88) .00888 10 (cid:88) .53415 10 (cid:88) .63703
.21331
9 (cid:88) .63703 9.5 (cid:88) .12233 10 (cid:88)
9 (cid:88) .35049
10 (cid:88) .91141 10 (cid:88) .06688
.35049 10 (cid:88) .73992 10 (cid:88) .53415
9 (cid:88) .21331 10 (cid:88) .53415
10 (cid:88) .73992 10 (cid:88)
.99147 10 (cid:88) .53415 10 (cid:88) .63703 9.5 (cid:88) .35049 10 (cid:88) .00888
10 (cid:88) .21331 10 (cid:88) .03517 10 (cid:88) .21331 10 (cid:88) .40340 9.5 (cid:88) .35049 10 (cid:88)
.21331 10 (cid:88) .21331 10 (cid:88) .44232
10 (cid:88) .12233 10 (cid:88)
10 (cid:88) .53415 10 (cid:88) .00204 10 (cid:88) .53415 10 (cid:88) .53415
.35049 10 (cid:88) .53415 10 (cid:88) .14010
8 (cid:88) .35049 10 (cid:88) .01791 10 (cid:88) .53415 10 (cid:88) .63703 9.5 (cid:88) .91141 10 (cid:88)
9 (cid:88) .53415 10 (cid:88) .23641
.03517
8 (cid:88) .14010
9 (cid:88) .73992 10 (cid:88) .35049 10 (cid:88) .91141 10 (cid:88) .51687
10 (cid:88) .06688 10 (cid:88)
9 (cid:88) .00430
.73992
5 × .00107
7 × .91141
8 (cid:88)
9 (cid:88) .44232 9.5 (cid:88) .53415 10 (cid:88)
.73992
9 (cid:88) .01791
9 (cid:88) .73992
0
.50
.53
.56
.59
.62
.65
.68
.71
.74
.77
.80
.83
.86
.89
.92
.95
.98
values of ρmax. The results are comparable to those from the
measured data.
2) CTW Compression: Again, we conﬁgured the compres-
sion method with a tree depth of 6 and we used a Krichevski-
Troﬁmov estimator [25]. The compression rate is given in bits
per byte, meaning that bit strings with full entropy result in
a compression rate of 8 bits/byte. Our compression results
indicate, that the quantized bit strings up a correlation upper
limit ρmax of 0.71 have nearly full entropy. With an increasing
ρmax the compression rate drops. The results can be found in
Table VIa. Since CTW compression gives us an upper bound
on the entropy, meaning the entropy of our bit strings can be
less, but not more, this bound is also given in Fig. 7.
3) NIST Randomness Test: We used the NIST randomness
tests as described in Section VI-3 on our simulated bit strings.
The minimum pass rate p(cid:48) for each statistical test is approxi-
mately 96, because we chose our number of samples n = 100.
However, most of the NIST randomness tests failed, so we
omit the actual results at this place. We hypothesize the reasons
are that our bit strings do not have full entropy, but nearly full
entropy as seen in Table VIa, and that the random number
generator used for generating the simulated bit strings is not
truly random itself.
4) NIST Min-Entropy Estimation: The ﬁve tests for a min-
entropy estimation were conﬁgured to analyze 8-bit symbols,
to have a comparable symbol size as the CTW compression.
Four tests gave invalid results, indicated with a ⊥, as output.
We also veriﬁed the estimated min-entropy values with a
symbol size of 16 bits, where all results were valid, and the