    Entry 2 - Interrupt (0x2) Device Exclusive (0x1)
Flags (LATCHED 
Level 0x1, Vector 0x1, Group 0, Affinity 0xffffffff
...
  TranslatedResourceList at 0xffffce0713517bb0  Version 1.1  Interface 0xf  Bus #0
    Entry 0 - Port (0x1) Device Exclusive (0x1)
Flags (PORT_MEMORY PORT_IO 16_BIT_DECODE 
Range starts at 0x60 for 0x1 bytes
    Entry 1 - Port (0x1) Device Exclusive (0x1)
Flags (PORT_MEMORY PORT_IO 16_BIT_DECODE 
Range starts at 0x64 for 0x1 bytes
    Entry 2 - Interrupt (0x2) Device Exclusive (0x1)
Flags (LATCHED 
Level 0x7, Vector 0x70, Group 0, Affinity 0xff
The device node tells you that this device has a resource list with three entries, one of which 
is an interrupt entry corresponding to IRQ 1. (The level and 
level and 
level
vector numbers represent the GSIV 
vector numbers represent the GSIV 
vector
IRQL as 7 (this is the level number) and the interrupt vector as 0x70.
level number) and the interrupt vector as 0x70.
level
On ACPI systems, you can also obtain this information in a slightly easier way by reading the 
extended output of the !acpiirqarb command introduced earlier. As part of its output, it displays 
the IRQ to IDT mapping table:
Interrupt Controller (Inputs: 0x0-0x77):
(01)Cur:IDT-70 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
(02)Cur:IDT-80 Ref-1 Boot-1 edg hi    Pos:IDT-00 Ref-0 Boot-1 lev unk 
(08)Cur:IDT-90 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
50 
CHAPTER 8 System mechanisms
(09)Cur:IDT-b0 Ref-1 Boot-0 lev hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
(0e)Cur:IDT-a0 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(10)Cur:IDT-b5 Ref-2 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(11)Cur:IDT-a5 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(12)Cur:IDT-95 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(14)Cur:IDT-64 Ref-2 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(17)Cur:IDT-54 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(1f)Cur:IDT-a6 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(41)Cur:IDT-96 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
resources, and other related concepts, see Chapter 6 in Part 1.
Line-based versus message signaled–based interrupts
Shared interrupts are often the cause of high interrupt latency and can also cause stability issues. They 
are typically undesirable and a side effect of the limited number of physical interrupt lines on a com-
Memory Stick, Secure Digital, and other formats, all the controllers that are part of the same physical 
-
ent device drivers as a shared interrupt vector. This adds latency as each one is called in a sequence to 
determine the actual controller that is sending the interrupt for the media device.
A much better solution is for each device controller to have its own interrupt and for one driver to 
manage the different interrupts, knowing which device they came from. However, consuming four tra-
ditional IRQ lines for a single device quickly leads to IRQ line exhaustion. Additionally, PCI devices are 
each connected to only one IRQ line anyway, so the media card reader cannot use more than one IRQ 
Other problems with generating interrupts through an IRQ line is that incorrect management of the 
IRQ signal can lead to interrupt storms or other kinds of deadlocks on the machine because the signal 
-
cally receive an EOI signal as well.) If either of these does not happen due to a bug, the system can end 
interrupts provide poor scalability in multiprocessor environments. In many cases, the hardware has 
manager selected for this interrupt, and device drivers can do little about it.
message-signaled 
interrupts (MSI). Although it was an optional component of the standard that was seldom found in 
client machines (and mostly found on servers for network card and storage controller performance), 
most modern systems, thanks to PCI Express 3.0 and later, fully embrace this model. In the MSI world, a 
this is essentially treated like a Direct Memory Access (DMA) operation as far as hardware is concerned. 
This action causes an interrupt, and Windows then calls the ISR with the message content (value) and 
(09)Cur:IDT-b0 Ref-1 Boot-0 lev hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
(0e)Cur:IDT-a0 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(10)Cur:IDT-b5 Ref-2 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(11)Cur:IDT-a5 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(12)Cur:IDT-95 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(14)Cur:IDT-64 Ref-2 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(17)Cur:IDT-54 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(1f)Cur:IDT-a6 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk 
(41)Cur:IDT-96 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
resources, and other related concepts, see Chapter 6 in Part 1.
CHAPTER 8 System mechanisms
51
the address where the message was delivered. A device can also deliver multiple messages (up to 32) to 
the memory address, delivering different payloads based on the event.
which is introduced in PCI 3.0, adds support for 32-bit messages (instead of 16-bit), a maximum of 2048 
different messages (instead of just 32), and more importantly, the ability to use a different address 
(which can be dynamically determined) for each of the MSI payloads. Using a different address allows 
the MSI payload to be written to a different physical address range that belongs to a different proces-
sor, or a different set of target processors, effectively enabling nonuniform memory access (NUMA)-
aware interrupt delivery by sending the interrupt to the processor that initiated the related device 
request. This improves latency and scalability by monitoring both load and the closest NUMA node 
during interrupt completion.
In either model, because communication is based across a memory value, and because the content 
is delivered with the interrupt, the need for IRQ lines is removed (making the total system limit of MSIs 
equal to the number of interrupt vectors, not IRQ lines), as is the need for a driver ISR to query the 
device for data related to the interrupt, decreasing latency. Due to the large number of device inter-
latency further by directly delivering the interrupt data to the concerned ISR.
utilize the term “GSIV” instead of IRQ because  it more generically describes an MSI vector (which is 
(GPIO) pin on an embedded device. And, additionally, on ARM and ARM64 systems, neither of these 
8-16, you can see the Device Manager on two computer systems showing both traditional IRQ-based 
GSIV assignments, as well as MSI values, which are negative. 
FIGURE 8-16 IRQ and MSI-based GSIV assignment.
52 
CHAPTER 8 System mechanisms
Interrupt steering
-
tween 2 and 16 processors in a single processor group, Windows enables a piece of functionality called 
interrupt steering to help with power and latency needs on modern consumer systems. Thanks to this fea-
ture, interrupt load can be spread across processors as needed to avoid bottlenecking a single CPU, and 
the core parking engine, which was described in Chapter 6 of Part 1, can also steer interrupts away from 
parked cores to avoid interrupt distribution from keeping too many processors awake at the same time.
Interrupt steering capabilities are dependent on interrupt controllers— for example, on ARM systems 
with a GIC, both level sensitive and edge (latched) triggered interrupts can be steered, whereas on APIC 
systems (unless running under Hyper-V), only level-sensitive interrupts can be steered. Unfortunately, 
why Windows also implements an additional interrupt redirection model to handle these situations.
When steering is enabled, the interrupt controller is simply reprogrammed to deliver the GSIV to a 
redirection must be used, then 
all processors are delivery targets for the GSIV, and whichever processor received the interrupt manu-
ally issues an IPI to the target processor to which the interrupt should be steered toward.
-
ity through a system information class that is handled by KeIntSteerAssignCpuSetForGsiv as part of the 
Real-Time Audio capabilities of Windows 10 and the CPU Set feature that was described in the “Thread 
of processors that can be chosen by the user-mode application, as long as it has the Increase Base 
Priority privilege, which is normally only granted to administrators or local service accounts.
Interrupt affinity and priority
InterruptPolicyValue in the 
https://docs.
microsoft.com/en-us/windows-hardware/drivers/kernel/interrupt-affinity-and-priority.
CHAPTER 8 System mechanisms
53
TABLE 8-5 
Policy
Meaning
IrqPolicyMachineDefault
default machine policy, which (for machines with less than eight logical 
processors) is to select any available processor on the machine.
IrqPolicyAllCloseProcessors
On a NUMA machine, the Plug and Play manager assigns the in-
terrupt to all the processors that are close to the device (on 
the same node). On non-NUMA machines, this is the same as 
IrqPolicyAllProcessorsInMachine.
IrqPolicyOneCloseProcessor
On a NUMA machine, the Plug and Play manager assigns the interrupt 
to one processor that is close to the device (on the same node). On non-
NUMA machines, the chosen processor will be any available processor 
on the system.
IrqPolicyAllProcessorsInMachine
The interrupt is processed by any available processor on the machine.
IrqPolicySpecifiedProcessors
IrqPolicySpreadMessagesAcrossAllProcessors
Different message-signaled interrupts are distributed across an optimal 
set of eligible processors, keeping track of NUMA topology issues, if pos-
sible. This requires MSI-X support on the device and platform.
IrqPolicyAllProcessorsInGroupWhenSteered
The interrupt is subject to interrupt steering, and as such, the interrupt 
should be assigned to all processor IDTs as the target processor will be 
dynamically selected based on steering rules.
priority, based on the values in Table 8-6.
TABLE 8-6 IRQ priorities
Priority
Meaning
IrqPriorityUndefined
No particular priority is required by the device. It receives the default priority (IrqPriorityNormal).
IrqPriorityLow
The device can tolerate high latency and should receive a lower IRQL than usual (3 or 4).
IrqPriorityNormal
The device expects average latency. It receives the default IRQL associated with its interrupt vec-
tor (5 to 11).
IrqPriorityHigh
The device requires as little latency as possible. It receives an elevated IRQL beyond its normal 
assignment (12).
As discussed earlier, it is important to note that Windows is not a real-time operating system, and 
as such, these IRQ priorities are hints given to the system that control only the IRQL associated with 
the interrupt and provide no extra priority other than the Windows IRQL priority-scheme mechanism. 
Because the IRQ priority is also stored in the registry, administrators are free to set these values for 
drivers should there be a requirement of lower latency for a driver not taking advantage of this feature.
54 
CHAPTER 8 System mechanisms
Software interrupts
Although hardware generates most interrupts, the Windows kernel also generates software interrupts 
for a variety of tasks, including these:
I 
Initiating thread dispatching
I 
Non-time-critical interrupt processing
I 
Handling timer expiration
I 
Asynchronously executing a procedure in the context of a particular thread
I 
Supporting asynchronous I/O operations
These tasks are described in the following subsections.
Dispatch or deferred procedure call (DPC) interrupts
A DPC is typically an interrupt-related function that performs a processing task after all device inter-
rupts have already been handled. The functions are called deferred because they might not execute 
immediately. The kernel uses DPCs to process timer expiration (and release threads waiting for the 
DPC IRQL but not really through a regular kernel DPC). Device drivers use DPCs to process interrupts 
and perform actions not available at higher IRQLs. To provide timely service for hardware interrupts, 
Windows—with the cooperation of device drivers—attempts to keep the IRQL below device IRQL lev-
els. One way that this goal is achieved is for device driver ISRs to perform the minimal work necessary 
to acknowledge their device, save volatile interrupt state, and defer data transfer or other less time-
critical interrupt processing activity for execution in a DPC at DPC/dispatch IRQL. (See Chapter 6 in Part 
1 for more information on the I/O system.)
In the case where the IRQL is passive or at APC level, DPCs will immediately execute and block all 
other non-hardware-related processing, which is why they are also often used to force immediate 
execution of high-priority system code. Thus, DPCs provide the operating system with the capability 
can no longer continue executing, perhaps because it has terminated or because it voluntarily enters a 
wait state, the kernel calls the dispatcher directly to perform an immediate context switch. Sometimes, 
however, the kernel detects that rescheduling should occur when it is deep within many layers of code. 
In this situation, the kernel requests dispatching but defers its occurrence until it completes its current 
activity. Using a DPC software interrupt is a convenient way to achieve this delayed processing.
synchronize access to scheduling-related kernel structures. This disables additional software interrupts 
and thread dispatching. When the kernel detects that dispatching should occur, it requests a DPC/dis-
patch-level interrupt; but because the IRQL is at or above that level, the processor holds the interrupt in 
check. When the kernel completes its current activity, it sees that it will lower the IRQL below DPC/dis-
patch level and checks to see whether any dispatch interrupts are pending. If there are, the IRQL drops 
to DPC/dispatch level, and the dispatch interrupts are processed. Activating the thread dispatcher by 
CHAPTER 8 System mechanisms
55
using a software interrupt is a way to defer dispatching until conditions are right. A DPC is represented 
by a DPC object, a kernel control object that is not visible to user-mode programs but is visible to de-
vice drivers and other system code. The most important piece of information the DPC object contains 
is the address of the system function that the kernel will call when it processes the DPC interrupt. DPC 
routines that are waiting to execute are stored in kernel-managed queues, one per processor, called 
DPC queues. To request a DPC, system code calls the kernel to initialize a DPC object and then places it 
in a DPC queue.
By default, the kernel places DPC objects at the end of one of two DPC queues belonging to the 
processor on which the DPC was requested (typically the processor on which the ISR executed). A 
device driver can override this behavior, however, by specifying a DPC priority (low, medium, medium-
high, or high, where medium is the default) and by targeting the DPC at a particular processor. A DPC 
targeted DPC. If the DPC has a high priority, the kernel inserts the 
DPC object at the front of the queue; otherwise, it is placed at the end of the queue for all other priorities.
IRQL (APC or passive level), the kernel processes DPCs. Windows ensures that the IRQL remains at DPC/
is, the kernel “drains” the queue), calling each DPC function in turn. Only when the queue is empty will 
the kernel let the IRQL drop below DPC/dispatch level and let regular thread execution continue. DPC 
A timer expires, and the kernel
queues a DPC that will release
any threads waiting on the
timer. The kernel then
requests a software interrupt.
When the IRQL drops below
DPC/dispatch level, a DPC
interrupt occurs.
The dispatcher executes each DPC routine
in the DPC queue, emptying the queue as
it proceeds. If required, the dispatcher also
reschedules the processor.
After the DPC interrupt,
control transfers to the 
(thread) dispatcher.
High
Power failure
DPC/dispatch
APC
Passive
DPC
queue
IRQL setting
table
•••
DPC
DPC
Dispatcher
1
2
4
3
DPC
FIGURE 8-17 Delivering a DPC.
DPC priorities can affect system behavior another way. The kernel usually initiates DPC queue 
draining with a DPC/dispatch-level interrupt. The kernel generates such an interrupt only if the DPC is 
directed at the current processor (the one on which the ISR executes) and the DPC has a priority higher 
than low. If the DPC has a low priority, the kernel requests the interrupt only if the number of outstand-
ing DPC requests (stored in the DpcQueueDepth 
threshold (called MaximumDpcQueueDepth 
processor within a time window is low.
56 
CHAPTER 8 System mechanisms
is either high or medium-high, the kernel immediately signals the target CPU (by sending it a dispatch IPI) 
to drain its DPC queue, but only as long as the target processor is idle. If the priority is medium or low, the 
number of DPCs queued on the target processor (this being the DpcQueueDepth again) must exceed a 
threshold (the MaximumDpcQueueDepth) for the kernel to trigger a DPC/dispatch interrupt. The system 
idle thread also drains the DPC queue for the processor it runs on. Although DPC targeting and priority 
8-7 summarizes the situations that initiate DPC queue draining. Medium-high and high appear, and are, 
in fact, equal priorities when looking at the generation rules. The difference comes from their insertion in 
the list, with high interrupts being at the head and medium-high interrupts at the tail.
TABLE 8-7 DPC interrupt generation rules
DPC Priority
DPC Targeted at ISR’s Processor
DPC Targeted at Another Processor
Low
DPC queue length exceeds maximum DPC queue 
length, or DPC request rate is less than minimum 
DPC request rate
DPC queue length exceeds maximum DPC queue 
length, or system is idle
Medium
Always
DPC queue length exceeds maximum DPC queue 
length, or system is idle
Medium-High