as each data item is replicated on a range of at least 1/p, it is easy
to see that the query will reach a node that holds every data item
(refer to Figure 3). Each server that receives the query matches it
against its data items and returns the matches (or the best matches if
the query is for a very popular term) to the front-end server, which
assembles the ﬁnal list and returns it to the client.
The description above captures the basic idea of the ROAR al-
gorithm, but not the whole story. The real beneﬁt comes from an
additional observation: if the front-end server chooses a partition-
ing value pq for a query that is larger than p, the algorithm still
matches all the data items. By default though, this would waste
effort, as the query might hit more than one server that holds the
same data item (as shown in Figure 4). However, if we embed the
Figure 4: Duplicate matches are possible when pq > p is used.
In this case, r = 4, p = 3 and pq = 4.
value pq into the query, the servers can divide up the matching task
by object ID so that no two servers match the same data item. To
do this deterministically, a server that receives a query with logical
destination idquery only runs the query against data items (objects)
that satisfy the following two conditions:
idobject < idquery
idobject + 1/pq ≥ idquery
(3)
(4)
Data items that do not satisfy the second condition will be matched
by the preceding server that received a sub-query (Figure 5(a)),
while data items failing the ﬁrst condition will be matched by the
server receiving the following sub-query (Figure 5(b)).
There are two main reasons why it is so useful to be able to run
queries with values of p greater than the bare minimum:
• Spreading a query across more nodes decreases latency. ROAR
can dynamically trade off latency for total throughput (or
if the nodes are not saturated, power consumption) without
needing to ﬁrst change the replication level.
• Allowing different values of pq to be used for queries allows
the partitioning to be changed while still serving queries.
4.3 Adding Nodes
To function correctly, each server just needs to know its ID range.
Typically, this must match up with the ranges of its immediate
neighbors on the ring.
When a server joins the overlay, it is inserted between two other
servers on the ring. The query load seen by a server is directly
proportional to the fraction of the ring it is responsible for. Thus
a simple strategy for inserting nodes is to pick the most heavily
loaded node, and insert the new node as its neighbor.
To start with, the new node has an inﬁnitely small range, and so
does not yet receive any queries. The nodes begins by replicating
all the data items that traverse its ID. This download could be from
its neighbor, but more likely it will be from a back-end ﬁlesystem
to avoid putting extra load on an already loaded server.
Once the data download has ﬁnished, the new node communi-
cates directly with its two neighbors to determine which of them
is most loaded.
It now starts to grow its range into that of the
most loaded neighbor, requesting additional data items that over-
lap the range as it grows. Every few seconds it updates the front
end servers with its new range, and also updates its neighbor so
that the neighbor can drop data items in the overlapping range.
As the new node’s range grows, its load will start to increase.
Once the new node’s load starts to approach that of its neighbors,
294idobject:
logical position
of object
match
no match as      
idobject + 1/pq < idquery 
1/pq
idobject + 1/p:
max extent of 
replication range
of object
range of 
node a
range of 
node b
range of 
node c
1/pq
range of 
node d
preceding
sub-query
sub-query
at idquery
(a) Match by ﬁrst sub-query
idobject:
logical position
of object
no match:
query before 
object
match:
idobject + 1/pq < idquery 
1/pq
idobject + 1/p:
max extent of 
replication range
of object
range of 
node a
range of 
node b
range of 
node c
1/pq
range of 
node d
preceding
sub-query
sub-query
at idquery
(b) Match by second sub-query
Figure 5: Avoiding duplicate matching in ROAR.
the rate of replication is slowed to a low background rate. In fact,
nodes always compare load with their neighbors and expand their
range very slowly into that of a more loaded neighbor. In this way,
the nodes progressively distribute themselves around the ring, not
with equal ranges, but with ranges that are the correct size to bal-
ance the load on the nodes, even if the nodes have heterogeneous
processing power.
4.4 Removing Nodes
A node can be removed from the ring in a controlled manner
by informing its neighbors that its load is now inﬁnite. The two
neighbors will grow their ranges into the range of the node to be
removed by downloading the additional data needed. This data is
typically a small fraction of the data a node already has: only 1/nth
of the data on a node starts or ﬁnishes at that node; it is this data
that the neighbor will not already have. A neighbor of a shut-down
node will need to download 1/2nth of the data on average, if it
takes over half of the neighbor’s range and ranges are equal before
the node is removed.
The query load will increase by as much 50% on the neighbors
of the node being shut down, as their range has increased by 50%.
However, in practice the neighbors’ neighbors will expand their
ranges as they see the load start to increase, so this upper bound is
not normally reached.
What happens though if a node fails without warning? The fail-
ure will be discovered very quickly by the front-end servers, so they
know not to route any more queries to it. However, we still want to
match the data-items the failed node would have answered.
The front end server avoids starting a query on a failed node, but
it ignores other failures when deciding the starting point. When it
needs to send a query to a failed node, it uses a fall-back strategy.
Each data item was replicated over an average of r servers that span
a range of 1/p; any of these servers could match the query instead
of the failed node. We need to split the sub-query that would have
been sent to the failed node in two because some data items’ range
might have ended on the failed node and some might have started
(a) A failure causing a missed match.
(b) Failure-handling in ROAR.
Figure 6: A node failure can cause a query to miss a match.
ROAR prevents this by splitting the failed node’s sub-query in
two and sending these to its predecessor and successor nodes.
on the failed node, as in Fig. 6(a). So long as we send the sub-query
to two nodes, one before and one after the failed node, and so long
as these nodes are not more than 1/p apart, then we are sure to
match every data item that the failed node could match (Fig. 6(b)).
To spread the extra load across the maximum number of nodes
we choose a pair of new targets for the sub-query as follows:
1. Let f aillo be the lowest ID held by the failed node and f ailhi
be the highest ID held by the failed node.
2. Pick a new ﬁrst sub-query target idq1 randomly such that:
f ailhi − (1/p − δ) < idq1 < f aillo.
δ is a small value that captures any uncertainty in the value
of 1/p. It is chosen so that 1/p − δ is guaranteed to be less
than 1/pold for all recently used values of pold.
3. Choose a new second sub-query target idq2 such that:
idq2 = idq1 + (1/p − δ)
This ensures the new sub-queries are never so far apart that a
data item’s range can fall between them and be missed.
4. Send both new sub-queries, but in the query request specify
the original query ID. This is so that the only data items to be
matched are those that the failed node would have matched,
avoiding overlap with other sub-queries. Additionally, be-
cause the two new subqueries are maximally separated by
almost 1/p, their data sets are maximally disjoint, so they
will produce very few duplicate matches.
The overall effect is that immediately after a node has failed and
before any node has had a chance to download any failed items,
all the queries are still being responded to correctly. The number
of sub-queries being sent has increased by a fraction of 1/n be-
cause one extra query is needed for those queries that would have
hit the failed node. The total matching load does not increase as
nodes do not duplicate each other’s work, but approximately 2n/p
nodes share the extra 1/nth of the load, so their load temporarily
increases by a fraction of 2/p.
The same algorithm applies for multiple failed nodes, but if ei-
ther of the new sub-queries hits a second failed node, the process is
simply repeated from step (2), choosing a new random value.
4.5 Changing the Replication Level
So far we have seen that for a given replication level r, we can
partition queries for varying values of pq, so long as pq · r ≥ n.
However, if, in an attempt to keep query latency low we are consis-
tently running with values of pq signiﬁcantly larger than the min-
imum needed, then it does not make sense to keep sending all the
295updates to all the nodes. Maintaining a replication level higher than
needed requires extra bandwidth, using CPU and network capacity
that could have been used to serve queries. Instead, we want to
repartition by reducing r, hence increasing the minimum p.
If p is increased and r decreased, all the ROAR nodes have to do
is drop a few objects from their local store. As it is always safe to
run queries with higher pq than needed, the front-end servers can
just switch to the new pq immediately, and let the ROAR nodes
discard data in their own time.
Conversely, a ROAR system may discover that it is running with
pq · r = n, using the minimum currently-available partitioning
level. If the query latency is well below threshold, then p is proba-
bly too large, costing CPU cycles and hence increasing energy re-
quirements2. If p is really excessive, nodes will saturate, and query
delay will rapidly increase.
, r must increase, and this is done by repli-
cating each object 1/p − 1/p
further round the ring. The ROAR
servers need to download the required objects from the ﬁlesystem,
which can take some time. Further, the nodes will not all complete
the download simultaneously. For correctness, when decreasing p
to p
, the front-end servers continue to partition queries p ways un-
til they receive positive conﬁrmation that every one of the ROAR
nodes has obtained all the extra data needed. Only then do they
switch to partitioning queries p
To decrease p to p
(cid:2)
(cid:2)
(cid:2)
(cid:2)
ways.
4.6 Load Balancing
The mean query rate seen by a node is directly proportional its
range. To balance load, ROAR uses a slow background process
in which each node extends its range into that of a more loaded
neighbor. The goal is not to even out ranges, but to even out load
so that a node’s range is in accordance with its processing power.
If ROAR indexes N items in total, the number that need to be
stored on a node i with a range of size gi is the number of items
that intersect the start of the node’s range plus the number of items
that start within the node’s range; this is N/p + N · gi. On average
1/p = r¯g, so for sensible values of r, the N/p term dominates,
and the amount of data stored by each node is fairly even between
nodes, even if their ranges vary considerably.
However, although the mean query rate at a node depends on
gi, by choosing a random starting point on the ring for a query,
we subject ourselves to the normal statistical variations associated
with random processes. When we implemented ROAR it became
clear that these variations could adversely affect load balancing suf-
ﬁciently to impact query delay.
To greatly reduce this effect, we make use of “the power of two
choices”[17]. When a front-end server partitions a query, it chooses
two IDs at random on the ring and computes the expected delays
in each resulting conﬁguration of p servers. It will then choose the
conﬁguration that ﬁnishes ﬁrst. To do so, the front-end maintains
statistics about each server’s processing power and RTT, as well
as the tasks that have been assigned to that server and have not
completed. To compute the expected ﬁnish time on a given server,
the frontend simply uses RT T +size/CP U, if the server is idle; if
not, it also takes into account the ﬁnish times of the existing tasks.
If servers are heterogeneous, there may be some sub-queries in
the chosen conﬁguration that ﬁnish much later than the rest, neg-
atively impacting query delay.
In this case, ROAR implements
an optional load balancing mechanism at the frontend: before it
sends the sub-queries, the frontend checks if the slowest server is
expected to be more than 100ms behind the fastest one; if so the
2The reader may think that the effect is negligible, but the temperature in our air-
◦
C hotter when our 47 ROAR nodes were fully loaded
conditioned machine room ran 4
than when they are idling. We have since upgraded our A/C system.
front-end uses a very similar mechanism to that described for han-
dling failures. It splits the sub-query it expects to be slow in two,
and reschedules the new subqueries. This continues until the dif-
ference in delay is below threshold, or the effective pq reaches a
predeﬁned limit.
This mechanism complements the range load balancing mecha-
nism, as it functions on a much shorter timescale: it can reduce de-
lays even if ranges are not assigned according to processing power,
at the extra cost of increasing pq.
5. EXPERIMENTAL EVALUATION
To evaluate ROAR we built a prototype application and deployed
it on 47 servers in the HEN testbed at UCL and on 1000 servers on
Amazon’s EC2 [1]. We also simulated ROAR extensively to exam-
ine scalability, but simulation fails to capture issues such as context
switch overhead and I/O bottlenecks that impact real-world perfor-
mance, so all the results below are from our testbed deployments.
The evaluation has two major goals. First, we wish to see how
p impacts the properties of the system, including the average query
delay, throughput, and system load. This gives insight into the
range of values that are appropriate for p in practice, and tell us
whether changing p has any sizable impact.
Second, we wish to evaluate ROAR. How does throughput and
query delay scale with the number of nodes involved in the search?
How easy is it to change p at runtime? How does ROAR cope with
failures? How well do the load balancing mechanisms work?
5.1 The Application
Ideally we would have liked to evaluate ROAR using a full-
blown web search application distributed across thousands of servers,
as this is the most widely used distributed rendezvous application.
Unsurprisingly though, such large-scale search engines are not
freely available for experimentation. We considered implementing
a miniature search engine, but at small scale the query setup costs
tend to dominate the query times, so the results would not be so
meaningful. In the end we decided that to run a small scale exper-
iment but still see meaningful results, we needed a more difﬁcult
matching application, where the matching costs would be compar-
atively large. Such an application still beneﬁts signiﬁcantly from
being parallelized on the scales we can achieve on our testbed.
The application we chose to stress ROAR is called Privacy Pre-
serving Search (PPS). Our system allows untrusted servers to match
encrypted queries against encrypted metadata. The servers only
learn the outcome of the match, not the contents of the query or
the metadata [19]. It can be used, for instance, to protect privacy
in online storage, such as Google Docs [2]. This application is
CPU intensive because of the cryptographic operations required to
perform a match. Files are encrypted before being stored, and en-
crypted metadata is also created and stored on the servers to allow
the searching. When the user wants to retrieve some ﬁles, PPS
runs queries to ﬁnd which ﬁles the user is interested in. These
queries are run on the servers, so the client platform can be ex-
tremely lightweight, such as a mobile phone.
In PPS, users each have many ﬁles (perhaps on the order of mil-
lions) for which they provide searchable metadata, and PPS’s job is
to answer queries for that data. To create metadata for our tests we
used the ﬁles from a Linux ﬁlesystem. The test queries used ran-
domly chosen keywords. From a usability point of view, we impose
a delay bound of one second that the PPS system must meet.
We have two versions of PPS that exhibit different ﬁxed costs.
PPS is written in Java, and the cost of running the Java garbage
collector is not negligible. PPS_LM (low memory) forces a run of
the garbage collector immediately after ﬁnishing a query. This has
296PPS_LM
PPS_LC
Query Delay Variation with Dataset Sizes
 5
 4
 3
 2
 1
 0
 600
 800  1000
 0
 200
 400
Thousands of objects
Throughput Variation with Dataset Sizes
 250
 200
 150
 100
 50
 0