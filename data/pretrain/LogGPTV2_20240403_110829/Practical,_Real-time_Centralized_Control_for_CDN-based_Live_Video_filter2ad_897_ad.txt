assume caching and/or multicast with a client group can
eﬃciently distribute videos to individual users. Edge
clusters are randomly placed in the 100 largest cities
(determined by number of requests from that city) and
each client group is connected to the 3 nearest edge
clusters with 150 Mbps overlay links. (Cities in our
traces are anonymized, so each city ID is randomly
assigned a coordinate on a 2D grid to estimate latency.)
In addition, we assign each link a cost, loosely mod-
eling a CDN’s cost for transferring data on that link.
Source-reﬂector and reﬂector-edge link costs vary from
10 to 50 units over a normal distribution. Links from
edge clusters to client groups are handled diﬀerently:
half have a cost of 0, since CDNs often strike deals with
edge ISPs [2]; the remaining half vary from 1 to 5.
Methodology: We break each trace into one minute
windows and compute distribution trees for each window
using VDN and three additional strategies:
• Everything Everywhere (EE)—This strawman naively
tries to stream all videos to all edge clusters so clients
can simply connect to the nearest cluster.
• Overlay Multicast (OM)—This strawman represents
an “optimal” overlay multicast-like scheme. Each
video channel individually computes the distribution
tree with the highest quality (found using our in-
teger program). This is eﬀectively VDN without
coordination across channels.
• CDN—We model a DNS-based CDN that extensively
monitors links and servers [2, 29, 35, 40]. As there is
not public information on the speciﬁc algorithm used
to produce these DNS mappings, we use the following
model (based on measurement studies [32, 40] and
319Avg. Bitrate
Cost / Sat. Req.
EE
624
174
OM
CDN
VDN
EE
OM
CDN
VDN
2,725
1.1
2,725
1.54
2,725
1
Avg. Bitrate
Cost / Sat. Req.
0.08
167K 1.1
2,725
2,725
2.0
2,725
1
Avg. Bitrate
Cost / Sat. Req.
EE
812
7.7
OM
CDN VDN
1,641
4.1
2068
1.2
3,454
1
Clients at BR 12% 100% 100% 100%
Table 1: Average Day trace.
Clients at BR 0%
Table 2: Large Event trace.
100% 100% 100%
Clients at BR 25% 34% 54% 78%
Table 3: Heavy-Tail trace.
(a) Avg. client bitrate.
(b) % at requested bitrate.
(c) Processing time.
Figure 12: Scaling load: increasing the number of videos.
a high-level description [2, 35]): upon receiving a
request for a new video, a cluster picks the parent
with the highest path capacity that is already sub-
scribed to that video. If no parents are subscribed, it
picks the parent with the highest path capacity. ASes
are mapped to edge clusters that are geographically
close (based on their city ID) and lightly loaded. Un-
like OM, CDN does not focus on optimal end-to-end
paths, just individual overlay links. This model is
more ﬁne-grained than an actual CDN as it considers
each video independently [40]. CDN assumes that
server selection is stored in a DNS cache with a TTL
of 30 seconds [40]. We also test a variant, CDN-1,
with a 1 second TTL. Note that CDN-1 would cause a
large number of DNS requests, especially if combined
with VDN’s per-video control.
Metrics: We use three performance metrics:
• Average Client Bitrate—The average bitrate de-
• Cost / Satisﬁed Request (Cost / Sat. Req.)—The
cost of data transfer per client who receives the bi-
trate they request, i.e., the sum over all links of (link
cost × usage) / number of satisﬁed requests.
• % of Clients Satisﬁed at Requested Bitrate (Clients
at BR)—What percentage of the client requests
were served at the bitrate they requested? (Clients
not served will re-request at lower bitrates.)
livered to each client in the trace.
7.1.1 Trace results
Tables 1, 2, and 3 summarize the results across our work-
loads. Each number is the average across one minute
time windows in the trace. In Average Day and Large
Event, VDN, CDN, and OM serve all videos at their re-
quested quality (thus achieving the best possible average
bitrate for the trace). Additionally, VDN reduces the
delivery cost by 1.5-2× compared to CDN. As CDN and
VDN both satisfy all clients, this decrease in cost must
come from VDN ﬁnding lower cost distribution trees
than CDN. The Large Event workload is easy to satisfy
as almost all edge clusters should receive the four sports
games. OM is as eﬀective as VDN in both workloads.
Heavy-Tail is the toughest workload to coordinate.
VDN provides a 1.7× improvement in quality, while serv-
ing 24% more clients at their requested bitrate. This
is because other schemes react to requests individually;
DNS-based schemes like CDN get “locked in” to decisions
until DNS records time-out, making it hard to coordinate
streams, whereas VDN performs optimization across all
requests simultaneously. With OM, the lack of coordina-
tion causes a 44% degradation in satisﬁed requests and
a 4× increase in cost.
7.1.2 Exploring the parameter space
Next we use our traces to evaluate the control plane
scalability and the topology sensitivity of VDN. Through-
out, we compute naive upper bounds (UB) on “average
bitrate” and “% satisﬁed at requested bitrate” by com-
paring the demand placed on each level in the topology
to the aggregate capacity at that level.
Control plane scalability: As we increase the num-
ber of videos and the size of the topology, we are inter-
ested in (1) the quality of the assignments VDN makes
and (2) the time it takes to compute those assignments.
In Figure 12, we augment Heavy-
Number of videos:
Tail with increasing numbers of videos and requests,
keeping the video/request ratio, topology, and capacity
constant. As we stress the system, it becomes more
diﬃcult to place videos. Thus, coordination becomes
more important with less spare capacity in the network.
Since VDN considers all streams simultaneously, unlike
CDN and OM, as load increases the gap between them
grows in terms of both quality (up to 1.6×; Figure 12a)
and the number of clients satisﬁed at the requested
bitrate (Figure 12b). CDN-1 does marginally better
0246810#ofVideos(Thousands)010002000300040005000600070008000Avg.Bitrate(Kbps)VDNCDNCDN-1OMEEUB0246810#ofVideos(Thousands)2030405060708090100%Sat.atRequestedBRVDNCDNCDN-1OMEEUB0246810#ofVideos(Thousands)050100150200DecisionTime(s)VDNCDNOMEE320(a) % at requested bitrate.
Figure 13: Scaling network size:
number of edge clusters.
(b) Processing time.
increasing the
(a) Avg. client bitrate.
Figure 15: Bottleneck location: improvement over
CDN with the bottleneck between source/reﬂector links,
reﬂector/edge cluster links, and edge cluster/client links.
(b) Cost.
(a) Avg. client bitrate.
(b) % at requested bitrate.
((cid:120) , (cid:121), (cid:122)) indicates
Figure 14: Topology sensitivity:
reﬂectors are connected to (cid:120) sources, edge clusters to (cid:121)
reﬂectors, and client groups to (cid:122) edge clusters.
than CDN, but a system with asynchronous control
plane updates (e.g., VDN) does substantially better, as
expected. Interestingly, OM satisﬁes fewer clients than
CDN, most likely due to OM grabbing key resources
early, starving later clients.
As expected, VDN’s improved assignments come at
the cost of longer decision times (Figure 12c). However,
in this experiment, we intentionally pushed the system
outside the bounds of reality; in realistic scenarios for
this topology and workload (up to 6,000 videos), decision
time remains under 60 seconds (in line with §5). In the
real world, if a CDN expects to serve upwards of 6,000
videos in a heavy-tail workload, we imagine the network
capacity would be upgraded as well.
Network size: We expand Average Day to 10K videos (to
increase demand) and vary the number of edge clusters
(Figure 13). We see that VDN maintains the ability to
satisfy roughly 90% of clients at their requested bitrate
(eﬀectively the naive upper bound) in under 60 seconds
for this workload (as opposed to Heavy-Tail, which re-
quired 190 seconds; Figure 12c).
Topology sensitivity: Next, we explore the impact
of the network topology on VDN, CDN, CDN-1, OM, and
EE. We vary two aspects of the topology: (1) the degree
of connectivity between tiers of the CDN and (2) the
aggregate network capacity between tiers.
Network connectivity: Figure 14 shows the impact of
network connectivity. As we increase the number of links
between tiers, we decrease their individual capacities
(a) Quality vs. cost: the
weight of the cost term is var-
ied from 1 to 0.
Figure 16: VDN gives operators ﬁne-grained
control.
(b) Quantity vs. quality:
impact of video prioritization
strategies.
so the aggregate capacity between those tiers remains
constant. In general, a less connected topology is going
to be easier to manage as fewer links potentially means
fewer failures. CDN performs better in highly connected
topologies, likely because it has more opportunity to ﬁnd
upstream neighbors that already have the video they’re
looking for. VDN, on the other hand, is not signiﬁcantly
aﬀected; it is able to eﬀectively use a small number of
large links. OM does not beneﬁt from more connectivity
as it focuses on path quality rather than link quality.
Bottleneck placement: We evaluate the impact of the
location of capacity bottleneck. We begin with the
topology described in §7.1; the aggregate capacity from
sources to reﬂectors is 4 Gbps, from reﬂectors to edge
clusters is 30 Gbps, and from edge clusters to client
groups is 900 Gbps (denoted (4, 30, 900) and named
Source Constrained). We now construct two additional
topologies: Reﬂector Constrained (400, 30, 900) and Edge
Constrained (4000, 3000, 900).
Figure 15 shows VDN’s percentage improvement over
CDN as a function of number of videos (generated from
Average Day). We see the largest gains in Source Con-
strained; we expect this scenario to be the most realistic
since their long-haul links are more expensive than links
at the edges (as pointed to by Akamai [2, 7]). In all three
cases, VDN improves average bitrate (Figure 15a). It
also reduces cost up through 6,000 videos (Figure 15b),
at which point (in Source Constrained) it slightly in-
creases cost in favor of 28%-45% quality improvements—
next, we discuss how to explicitly control this tradeoﬀ.
0500100015002000#ofEdgeClusters102030405060708090100%Sat.atRequestedBRVDNCDNCDN-1OMEEUB0500100015002000#ofEdgeClusters010203040506070DecisionTime(s)VDNCDNOMEE(2,2,2)(4,3,3)(4,5,3)(4,8,3)(4,10,3)LinkConﬁg05001000150020002500Avg.Bitrate(Kbps)VDNCDNCDN-1OMEEUB(2,2,2)(4,3,3)(4,5,3)(4,8,3)(4,10,3)LinkConﬁg020406080100%Sat.atRequestedBRVDNCDNCDN-1OMEEUB0510152025#ofVideos(Thousands)051015202530354045%IncreaseinAvgBitrateSrc/ReﬂReﬂ/EdgeEdge/Client0510152025#ofVideos(Thousands)−20−10010203040%DecreaseinCostSrc/ReﬂReﬂ/EdgeEdge/Client0.40.50.60.70.80.91.01.1Cost(Normalized)1400160018002000220024002600280030003200AvgBitrate(Kbps)VDNCDNCDN-1OMQualityBalancedQuantityVideoPriorityScheme020406080100%Sat.atRequestedBR240p480p1080p4K321(a) Avg. client bitrate.
(b) Join time.
(c) Buﬀering ratio.
Figure 17: Client-side quality in testbed: increasing the number of videos.
7.1.3 Customizing VDN
Quality vs. cost: By adjusting the weight of the global
cost term in the objective function, operators can tune
the quality/cost tradeoﬀ. Figure 16a shows an ROC-like
curve depicting the average bitrate and data transfer
cost in Heavy-Tail as the weight of the cost term varies
from 1 to 0. For comparison, we plot CDN, CDN-1, and
OM’s performance on the same trace. VDN achieves
about a 1.7× increase in performance over CDN for the
same cost (1.5× over OM and CDN-1), or can reduce
cost by 60% at similar quality.
Quality vs. quantity: VDN allows operators to assign
each ((cid:118)(cid:105)(cid:100)(cid:101)(cid:111), (cid:98)(cid:105)(cid:116)(cid:114) (cid:97)(cid:116)(cid:101)) pair a priority. We test three prior-
ity assignment strategies: Quality ((cid:112)(cid:114) (cid:105)(cid:111)(cid:114) (cid:105)(cid:116) (cid:121) = (cid:98)(cid:105)(cid:116)(cid:114) (cid:97)(cid:116)(cid:101)),
Balance ((cid:112)(cid:114) (cid:105)(cid:111)(cid:114) (cid:105)(cid:116) (cid:121) = 1), and Quantity ((cid:112)(cid:114) (cid:105)(cid:111)(cid:114) (cid:105)(cid:116) (cid:121) =
1/(cid:98)(cid:105)(cid:116)(cid:114) (cid:97)(cid:116)(cid:101)). Quality favors serving high bitrate streams;
Quantity favors serving as many streams as possible.
Figure 16b shows the percentage of satisﬁed requests
for each strategy broken down per bitrate for Heavy-
Tail. Quality favors the 4K streams and Quantity favors
sending more streams overall, as expected. This allows
operators to not only control how things are delivered,
but what is delivered (e.g., ensuring premium customers
always receive HD streams even when many free cus-
tomers request SD streams).
7.2 End-to-end experiments
We answer two questions:
1. Is VDN highly responsive? VDN reacts to events
at a timescale of 200 milliseconds while staying
within 17% of the optimal decision.
2. Does VDN cope well with the issues of a wide-
area environment? Hybrid control allows VDN to
function well despite losing controller updates and
performs similarly to other schemes during high
link ﬂuctuations (traﬃc dynamics).
Setup and topology: We use 10 co-located nodes
on EC2 [8], each representing a cluster, conﬁgured in a
three-tiered CDN topology (described in §2). Two nodes
are sources, another two are reﬂectors, and the remaining
six are edge clusters. Each tier is fully connected to the