title:1RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters
author:Arjun Singhvi and
Aditya Akella and
Dan Gibson and
Thomas F. Wenisch and
Monica Wong-Chan and
Sean Clark and
Milo M. K. Martin and
Moray McLaren and
Prashant Chandra and
Rob Cauble and
Hassan M. G. Wassel and
Behnam Montazeri and
Simon L. Sabato and
Joel Scherpelz and
Amin Vahdat
1RMA: Re-envisioning Remote Memory Access for
Multi-tenant Datacenters
Arjun Singhvi‚Ä†‚Ä° Aditya Akella‚Ä†‚Ä° Dan Gibson‚Ä°
Hassan M. G. Wassel‚Ä° Behnam Montazeri‚Ä°
Sean Clark‚Ä° Milo M. K. Martin‚Ä° Moray McLaren‚Ä°
Amin Vahdat‚Ä°
‚Ä°Google Inc
‚Ä†University of Wisconsin - Madison
‚àóLilac Cloud
‚ó¶Unaffiliated
Thomas F. Wenisch‚Ä° Monica Wong-Chan‚Ä°
Simon L. Sabato‚àó
Prashant Chandra‚Ä° Rob Cauble‚Ä°
Joel Scherpelz‚ó¶
Abstract
Remote Direct Memory Access (RDMA) plays a key role in support-
ing performance-hungry datacenter applications. However, existing
RDMA technologies are ill-suited to multi-tenant datacenters, where
applications run at massive scales, tenants require isolation and se-
curity, and the workload mix changes over time. Our experiences
seeking to operationalize RDMA at scale indicate that these ills
are rooted in standard RDMA‚Äôs basic design attributes: connection-
orientedness and complex policies baked into hardware.
We describe a new approach to remote memory access ‚Äì One-
Shot RMA (1RMA) ‚Äì suited to the constraints imposed by our multi-
tenant datacenter settings. The 1RMA NIC is connection-free and
fixed-function; it treats each RMA operation independently, assist-
ing software by offering fine-grained delay measurements and fast
failure notifications. 1RMA software provides operation pacing, con-
gestion control, failure recovery, and inter-operation ordering, when
needed. The NIC, deployed in our production datacenters, supports
encryption at line rate (100Gbps and 100M ops/sec) with minimal
performance/availability disruption for encryption key rotation.
CCS Concepts
‚Ä¢ Networks ‚Üí Network design principles; Data center networks.
Keywords
Remote Memory Access; Connection Free; Congestion Control
ACM Reference Format:
Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica
Wong-Chan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant
Chandra, Rob Cauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L.
Sabato, Joel Scherpelz, Amin Vahdat. 2020. 1RMA: Re-envisioning Remote
Memory Access for Multi-tenant Datacenters. In Annual conference of the
ACM Special Interest Group on Data Communication on the applications,
technologies, architectures, and protocols for computer communication (SIG-
COMM ‚Äô20), August 10‚Äì14, 2020, Virtual Event, NY, USA. ACM, New York,
NY, USA, 14 pages. https://doi.org/10.1145/3387514.3405897
1
The scale, diversity, and performance requirements of modern data-
center applications, such as search, ads serving, video transcoding,
Introduction
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
¬© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7955-7/20/08.
https://doi.org/10.1145/3387514.3405897
and machine learning, demand networks that support high band-
width and operation (op) rates while achieving low tail latencies.
Remote Direct Memory Access (RDMA) is an attractive option for
such distributed systems because of the latency and op-rate benefits
provided by one-sided reads and writes, as these ops involve no
remote CPU and thus offer performance limited only by hardware
[1, 8, 12, 13, 17, 19‚Äì21, 24, 29, 41].
Industry-standard RDMA evolved from supercomputer environ-
ments and has been challenging to deploy in commercial datacen-
ters [17, 41]. RDMA assumes low-latency, reliable, ordered net-
works and supercomputing fabrics deliver on these expectations via
switch-enforced lossless link-level flow control, which allows an
RDMA-capable NIC (RNIC) to implement naive congestion control
and loss recovery schemes to react to congestion and drops. These
fabrics are commonly single-tenant (or statically partitioned), and
RDMA solutions for authorization, access control, fault recovery,
and privacy reflect single-tenant expectations.
In contrast, modern hyperscale datacenters are characterized by
multi-tenancy, wherein uncoordinated large-scale distributed ap-
plications share common infrastructure. A diverse, time-varying
application mix induces rapidly changing network traffic patterns.
Strong privacy and authentication are needed. These requirements
lead to tension with standard RDMA‚Äôs design choices:
‚Ä¢ Standard RDMA offers connections in hardware, an abstrac-
tion that aligned well with early RDMA applications, but one
that places fundamental limits on at-scale isolation, perfor-
mance, and fault-tolerance. With modern serving and storage
systems [2‚Äì4, 6, 7, 10, 15, 37] operating beyond ten-thousand-
server scale, per-connection hardware resources are easily ex-
hausted. Workarounds (e.g., connection sharing) lead to broken
isolation, which is further exacerbated under failures (¬ß2).
‚Ä¢ In our experience, congestion control algorithms need constant
iteration in response to deployment and application considera-
tions. Standard RNICs (and switches) bake significant portions
of congestion response into hardware; this leaves little opportu-
nity to adapt post-deployment.
‚Ä¢ As applications and infrastructure are mutually-untrusting,
multi-tenancy calls for line-rate encryption and application
support to manage provenance of encryption keys. Although
modern RNICs provide encryption, practical challenges arise:
encryption is intrinsically tied to the notion of connections; ap-
plications must trust lower levels of the stack to manage keys;
and there is no support for security-related management opera-
tions, such as encryption key rotation.
708
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
A. Singhvi et al.
Responsibility
RDMA
Inter-op ordering
NIC
Failure recovery
NIC
Flow and congestion
NIC and
control
Fabric
Security ops (e.g., Rekey) None
1RMA
Software
Software
NIC and
Software
NIC
Table 1: Division of responsibilities in standard RDMA and
1RMA. Moving a subset of functionality to software simplifies
hardware and enables more flexibility/rapid iteration.
We take a new approach to remote memory access (RMA) to
better match the constraints of our consolidated, multi-tenant data-
centers. Our approach delivers the performance advantages of stan-
dard one-sided RDMA‚Äîhigh bandwidth, high op rate, and low
latency‚Äîwhile also providing predictable tail performance, scalabil-
ity, fault tolerance, isolation, security features, and amenability to
rapid post-deployment iteration.
We achieve these goals by dividing responsibilities between hard-
ware and software, in a manner that represents a stark departure
from standard RDMA (Table 1). Our NIC hardware is extremely
simple, focused exclusively on fast, fixed-function primitives. We
offer no illusion of infinite resources (unlike RDMA; ¬ß2) and instead
manage the explicitly-finite hardware resources in software. To facil-
itate rapid iteration, software implements fault recovery, congestion
control, and ordering when needed.
Our clean-slate design ‚Äì One-Shot RMA (1RMA) ‚Äì embraces
several design idioms to achieve our objectives:
(1) No connections: 1RMA is connection-free. Hardware state does
not grow with the number of endpoint pairs. Freed from connection
semantics, the NIC can treat each op as independent of other ops,
leaving software to handle inter-op ordering when needed. 1RMA
assigns to software the duties of per-op retry and fault recovery
(hence, the name "One-Shot"), and instead provides simpler fail-fast
behavior: 1RMA hardware ensures timely completions (10√ó slowdown. 1RMA‚Äôs congestion control
converges to fair bandwidth shares in the presence of competing
applications almost immediately (25ùúás); separately reacting to local
congestion improves convergence speed by 20√ó. First-class support
for security reduces the unavailability period during encryption key
rotations to <1ùúás. These gains come at a minimal cost of 0.5 cores to
drive 100 Gbps line-rate, as 1RMA chunks large ops into 4KB ops,
and implements congestion control and op management in software.
2 Background and Motivation
Standard RDMA offers three different transport types, each of which
supports a different subset of ops (Table 2). The prevalent transport
is RC, or "reliable connected".
Queue Pairs (QP), Connections. An application establishes con-
nected queue pairs (QPs) between application-pairs via out-of-band
exchange of tokens (e.g., via RPC or librdmacm). The server-side