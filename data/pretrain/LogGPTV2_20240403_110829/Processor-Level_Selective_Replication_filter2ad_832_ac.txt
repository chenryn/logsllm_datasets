instructions  that  need  to  be  replicated.  In  a  similar 
manner,  the  set  of  critical  instructions  for  each 
critical  variable  are  extracted.  The  union  of  these 
different  sets  of  critical  instructions  is  calculated. 
This  procedure  is  repeated  for  each  input  i  in  the 
chosen  set  of  100  inputs.  The  set  of  critical 
instructions that is replicated is the union of the sets 
of critical instructions for all the inputs. Table 1 gives 
a  brief  description  of 
the  Siemens  suite  of 
benchmarks. 
2 In a RISC architecture, load/store instructions are used to access 
memory and the destination of arithmetic instructions is a register. 
schedule 
schedule2 
412
373
print_tokens 
727
print_tokens2
569
A priority scheduler for multiple 
job  tasks.  Given  a  list  of  tasks 
finds an optimal schedule 
Same operation as Schedule but 
a different implementation. 
Breaks  the  input  stream  into  a 
series 
tokens 
according to pre-specified rules.
Using  the  tokenizer  interface 
Breaks  the  input  stream  into  a 
series 
tokens 
according to pre-specified rules.
lexical 
lexical 
of 
of 
5.2.  Performance overhead 
The  software-level  implementation  evaluates  the 
performance  overhead  incurred  by  the  framework 
and modules in terms of additional processor cycles. 
Table 1: Siemens suite of benchmarks 
Benchmark 
#loc Description 
5.2.1.  Performance overhead categories 
to 
The experiments evaluate the following two kinds 
of performance overheads: 
1.  Framework  Overhead.    This  is  the  overhead 
incurred by the processor due to the presence of 
the framework without any modules instantiated. 
In such a case, the framework does not perform 
any checking and is decoupled from the pipeline. 
The overhead incurred in the performance of the 
the  memory  arbiter 
application 
introduced  to  arbitrate  memory  accesses  of  the 
processor and the RSE [18]. 
is  due 
in  comparison 
2.  Performance  Overhead  Due 
to  Selective 
Replication. The performance overhead incurred 
by  the  application  is  measured  in  terms  of  the 
number of additional cycles taken to execute the 
application 
the  baseline 
processor  (without  the  framework).  In  order  to 
the  need  for  selective  replication,  a 
show 
replication 
randomized 
mechanism 
(RANDOMREP) 
is  also  evaluated  where 
instructions are randomly replicated. In order to 
make a fair comparison between the randomized 
and selective replication approaches the fraction 
of replicated instructions is maintained the same, 
ensuring  that  the  overheads  are  approximately 
equal. 
There can be other sources of overhead due to the 
additional hardware introduced in the processor. The 
additional  circuitry  will  increase  the  capacitive  load 
on  pipeline  nodes.  This  will  in  turn  lead  to  a  slight 
increase  in  the  clock  cycle  time.  Because  we  are 
to 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:33 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007doing a functional simulation this factor of overhead 
is not included in our experiments. 
5.2.2.  Results 
(FULLREP).  The  overhead  of 
Figure  5  shows  the  overheads  incurred,  for 
different  applications,  due  to  the  framework  with 
selective  replication  (SELREP)  compared  to  full 
replication 
the 
framework  with  no  modules  instantiated  is  also 
shown  (Framework).  We  observe  that  the  overhead 
averaged  over  all  applications  and  combinations  of 
modules is 53.1% lower than the overhead due to full 
replication. 
Framework SELREP FULLREP
d
a
e
h
r
e
v
O
50%
40%
30%
20%
10%
0%
schedule
schedule2
print_tokens
print_tokens2
Figure 5: Performance overhead for Selective 
Replication 
An  average  over  all  the  Siemens  benchmarks 
shows that the overhead is 16.5% for SELREP. For 
SELREP  the  overhead  is  due  to  the  execution  of 
duplicate instructions in replicated mode, and due to 
the  switch  between  replicated  and  unreplicated 
modes.  The  overhead  varied  from  application  to 
application. For example, for schedule the overhead 
for SELREP was 11.9% (67.1% lower than that for 
FULLREP) whereas for print_tokens2 it was 21.8% 
(52.7% lower than that for FULLREP). 
5.3.  Error analysis 
In  this  section  we  describe  the  fault-injection 
analysis of the error coverage provided by Selective 
Replication. Firstly, the fault model is described and 
the  classification  of  the  outcomes  of  each  fault-
injection  experiment  is  presented.  The  injection 
experiments are conducted for each benchmark from 
the Siemens suite and the results reported.  
5.3.1.  Fault model 
An important component of the design of a fault-
injection experiment to evaluate error coverage is the 
fault  model.  It  describes  the  faults  that  are  being 
targeted  by  the  error-detection  mechanisms  and 
against  which  they  have  been  evaluated.  The  faults 
considered in our experiments are as follows: 
• 
Instruction  Errors.  Errors  in  the  instruction 
binary while the instruction is being executed in the 
pipeline.  These  errors  can  occur  during  the  transfer 
of  the  instruction  from  the  cache  to  the  pipeline  or 
while the instruction is being decoded in the pipeline. 
•  Data Errors. Errors in the output of a functional 
unit  that  may  be  written  to  a  register  or  used  as  an 
effective  address  for  a  memory  access  instruction. 
ECC in memory, cache, or registers does not protect 
against these errors. This is because the correct ECC 
would be calculated on the wrong data and written to 
registers. 
assignment/initialization 
This  fault  model  also  includes  some  software 
(an 
faults 
uninitialized  or  incorrectly  initialized  value  is  used) 
or checking (a check performed on the variable fails, 
which is equivalent to an incorrect value of a variable 
being  used)  [19].  The  error-detection  mechanisms 
detect 
irrespective  of 
whether they occur in software or hardware. 
the  symptoms  of  errors, 
such 
as 
the 
the 
timing 
The  SimpleScalar 
sim-outorder  performance 
simulator  simulates 
information  for 
instructions executing in a pipeline, i.e., it maintains 
the  information  of  which  instructions  are  present  in 
each  stage  of  the  pipeline  in  any  given  cycle.  The 
simulator,  however,  computes 
results  of 
executing the instructions in the dispatch stage, when 
it  allocates  an  entry  in  the  reorder  buffer  to  the 
instruction. It detects and reports any exceptions that 
result  from  execution  of  the  instruction  at  the 
dispatch stage itself, without waiting until the commit 
stage. Thus, the processor simulator does not support 
precise  exceptions.  The  replication  mechanism, 
however,  performs  the  checks  when  the  instruction 
has  arrived  at  the  commit  stage  and  is  ready  for 
commit.. 
Translating the simulator behavior to that of a real 
processor, let us assume that the  simulator  does  not 
raise an exception at the dispatch stage but allows the 
instruction 
the  commit  stage. 
Considering  this  behavior  of  the  processor  the  fault 
injection  outcomes  have  been  organized  into  the 
various categories shown in Table 2. 
to  proceed 
to 
5.3.2.  Fault-injection outcomes 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:33 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Table 2: Fault-injection outcomes 
Errors leading to a mismatch between the replicas.  
Description 
Errors that raise a simulator exception in the same 
instruction (PC) as the injected PC.  
Errors that are not detected by replication, but are 
injected when the processor is in replicated mode, 
and raise a simulator error in a different instruction 
than the one that was injected into. 
Errors that are detected by the simulator but occur in 
a different instruction than the injected instruction. 
Errors that do not cause simulator errors and hangs, 
and the output files match. 
Errors in which the simulator times out and kills the 
program 
Errors that do not cause simulator errors or timeouts, 
but result in the output files, differs from that of the 
golden run. 
Errors that are “Not Manifested” in the baseline case 
but are detected by the detection mechanism 
Error Impact 
Do not raise an exception, but are detected by the voter in 
the commit stage. 
Raise an exception in the commit stage of the injected 
instruction. Architected state would not be corrupted by 
these errors before the exception. 
Can be detected by the replication if the instruction had 
been allowed to complete. However, the architected state 
might have been corrupted by then. 
Detected by the system, but the architected state might have 
been corrupted by the instruction before the system detects 
these errors. 
Do not cause any visible effect on the outcome of the 
program. 
Cause the simulator to wait indefinitely for the program to 
complete 
System does not detect these errors, but results in an 
incorrect program outcome. Potentially, most dangerous of 
the error categories 
Do not affect application outcome and hence need not be 
detected 
Outcome 
Replication 
Detection 
Exception Raised 
Retrospectively 
Detected 
System Detection 
Not Manifested 
Program Hang 
Fail-Silent Violation 
Benign Error 
Detection 
5.3.3.  Error metrics 
The  two  metrics  derived  from  the  fault  injection 
outcomes are the percentage of errors detected by the 
technique and percentage of false positives, where an 
error  that  is  benign  in  the  baseline  is  detected  by  the 
technique. For any technique it is desirable to have the 
highest  detection  coverage  possible  and  as  few  false 
positives as possible, even though these are conflicting 
goals. 
5.3.4.  Error coverage for instruction errors 
Errors belonging to each type mentioned in Section 
5.3.1  are  injected.  Figure  6  presents  the  detection  by 
selective  replication  (SELREP),  averaged  over  the 
applications,  when  50  critical  variables  are  used  to 
select  the  critical  instructions  to  be  replicated.  The 
detection  of  selective  replication  is  compared  to  the 
outcomes  in  the  baseline  case,  when  randomized 
replication  (RANDOMREP)  is  used  and  when  full 
duplication (FULLREP) is used. 
These  results  show  that  selective  replication  of 
instructions affecting 50 critical variables detects about 
87% percent of the faults detected by FULLREP. Yet it 
incurs 59.1% less overhead and leads to 17.8% fewer 
benign  error  detection  scenarios  as  compared  to  full 
duplication. 
In Figure 6, the y-axis shows the different outcomes 
from  the  injection  of  instruction  errors.  The  x-axis 
shows  the  percentage  of  errors  that  fall  into  each 
outcome category. We can see that FULLREP detects 
about 71.2% of the manifested errors. The rest of the 
errors  raise  an  exception  at  the  injected  instruction 
itself  and  hence  can  be  detected  by  the  system  and 
recovered easily. Even though selective replication has 
a much lower overhead than full duplication, it detects 
62.5%  of  the  manifested  errors,  whereas  random 
replication  detects  only  50.9%  of  the  errors.  When 
random replication is used, the system detects 17% of 
the errors in a different instruction, which are difficult 
to  recover  from.  In  the  case  of  selective  replication, 
this contributes to only 4.2% of the errors. 
Full Rep SELREP Random Baseline
Benign Error
Detection
Exception Raised in
different instruction
4.2%
0.0%
17.0%
45.9%
48.5%
46.9%