but we have seen both situations where this is not enough
and where it is too much. Overall, however, one level of
calling context has proved to be a reasonable choice for a
wide variety of introspection tasks.
To better illustrate the boundaries of our technique, con-
sider Figure 2, which plots the address of data written by
diﬀerent tap points over time for four patterns of memory
access.
In the bottom two quadrants, we have cases that
are challenging, but currently well-supported by TZB. In
the bottom-left, a standard memcpy implementation on x86
makes a copy in 4-byte chunks using rep movsd, and then
does a two-byte movsw to get the remainder of the string.
Because the access occurs across two diﬀerent instructions,
TZB sees two diﬀerent tap points. Our tap point correlation
mechanism correctly deduces that the accesses are related,
however, because they operate on adjacent ranges in a short
span of time.
The case shown in the bottom right quadrant would be
tricky if we looked only at memory access spatially and not
temporally. Here, a utility function writes data out to a se-
rial port by making one-byte writes to a memory-mapped
I/O address.5 Because TZB sees these memory writes in
temporal order, ignoring the address, the data is seen nor-
mally and the analyses we describe all operate correctly.
5Although not reported in this paper, this case is one we
actually encountered while experimenting with an embedded
ﬁrmware.
Time Memory Address memcpygoogledmesg[...]doneInitmemmovele.cgoogserialUncompressTap ATap B841for later deterministic replay. Next, one can run one or
more analyses that seek out the desired information among
all memory accesses seen during the execution. Analyses in
TZB take the form of PANDA plugins that are called on each
memory access made during a replayed execution and, at the
end, write out a report on the tap points analyzed. Finally,
the tap points found should be validated to ensure that they
do, in fact, provide the desired information. Such assurance
can be gained either by examining the data in the tap point
in new executions, or by examining the code around the tap
point. This workﬂow is illustrated in Figure 3.
In this section, we describe three diﬀerent ways of ﬁnd-
ing tap points grouped according to a standard epistemic
classiﬁcation scheme [26]: searching for “known knowns”—
tap points where the content of the desired data is known;
searching for “known unknowns”—tap points where the kind
of data sought is known, but its precise format is not; and ﬁ-
nally “unknown unknowns”—tap points where the type and
format of the data sought are not known, and we are instead
simply trying to ﬁnd “interesting” tap points.
4.1 Known Knowns
The simplest case is ﬁnding data that one knows is likely
to be read or written by a tap point, and where the encoding
of the data is easily guessed. For example, to ﬁnd a tap point
that can be used to notify the hypervisor whenever a URL
is entered in a browser, one can visit a known sequence of
URLs, and then monitor all tap points, searching for speciﬁc
byte sequences that make up those URLs. The same holds
for other data whose representation when written to memory
is predictable: ﬁlenames, window titles, registry key names,
and so on. For this kind of data, simple string searching is
usually suﬃcient to zero in on the few tap points that handle
the data of interest, and in our experience it is one of the
most eﬀective techniques for ﬁnding useful tap points.
4.2 Known Unknowns
A second tap point application involves ﬁnding tap points
for things about which we have limited knowledge. We can
easily assemble corpora of exemplars to represent a seman-
tic class: English prose, kernel messages, or mail headers.
These examples need not come from tap points but can eas-
ily be collected directly from interacting with the operating
system itself. From such a corpus, we can readily build a
statistical model, with which we can build a distance mea-
sure for scoring and ranking tap points by how close their
contents are to the model.
In addition to such statistical methods, we can also search
using an oracle. This is the case, for example, with tap
points that write encryption keys. Although the exact key
may not be known in advance, we can check whether a given
byte string is a valid decryption key by trying to decrypt our
sample data.
4.3 Unknown Unknowns
The ﬁnal strategy for ﬁnding useful tap points is also the
least focused. If there is no speciﬁc introspection quantity
sought, one might instead wish to ﬁnd interesting tap points,
for some suitable deﬁnition of “interesting.” To support
this scenario, TZB oﬀers a form of unsupervised learning—
clustering—to group together tap points that handle similar
data. The idea is that one can then examine exemplars from
each cluster, rather than being forced to look through a large
Figure 3: The workﬂow for using TZB to locate
points at which to interpose for active monitoring.
The upper quadrants show cases that are currently not
handled by TZB. In the upper left, memmove copies a buﬀer
in reverse order when the source and destination overlap.
Thus, when viewed in temporal order, a copy of a string
like “12345678” would be seen by TZB as “56781234”. This
case is unlikely to be handled by TZB without a signiﬁ-
cant redesign, as its view of memory accesses is inherently
streaming.
Finally, the upper right, which represents the case of dmesg
on Linux, is an example of the “dilemma of context”. Al-
though the function, do_syslog, that writes log data to
memory is called from multiple places (creating multiple tap
points), it writes to the same contiguous buﬀer. Unlike the
memcpy case, a signiﬁcant amount of time may pass before
the next function calls do_syslog, and so our tap correla-
tion, which only considers memory accesses within a ﬁxed
time window, will not notice that the tap points ought to
be grouped together. We believe that this case could be
overcome with additional engineering work, but this is left
to future work.
4. SEARCH STRATEGIES
To ﬁnd useful tap points in a system—places from which
to extract data for introspection—using Tappan Zee Bridge,
one begins by creating a recording that captures the desired
OS or application behavior. For example, if the end goal is to
be notiﬁed each time a user loads a new URL in Firefox, one
would create a recording of Firefox visiting several URLs.
This recording is made by emulating the OS and application
inside of the dynamic analysis platform PANDA (described
in more detail in Section 5.1), which can capture and record
all sources of non-determinism with low overhead, allowing
Record Executiongoogle.comRecordednondeterministicinputsReplay Executiongoogle.comValidatebing.comTZB Analysis✘✔bing.com!D!@.cLegendStage done by handStage done automaticallyTap Pointgoogle.com010300eaﬁrefox.exe!D!@.c8.8.8.8842number of tap points. Thus, our use of clustering functions
as a form of data triage.
5.
IMPLEMENTATION
In this section, we describe both the dynamic analysis
platform employed to build TZB, but also TZB-speciﬁc al-
gorithmic and data-structure solutions.
5.1 PANDA
TZB makes extensive use of the Platform for Architecture-
Neutral Dynamic Analysis (PANDA), which was developed
by the authors in collaboration with Northeastern Univer-
sity.
PANDA is based upon version 1.0.1 of the QEMU machine
emulator [4]. QEMU is an excellent and common choice for
whole-system dynamic analysis for two main reasons. First,
performance is good (about 5x slowdown over native). Sec-
ond, every basic block of guest code is disassembled by the
host in order to emulate, which means that there are op-
portunities to interpose analyses at the basic block or even
instruction level, if desired. QEMU lowers instructions to
an intermediate language (IL) in order to employ a single
back-end code generator, the Tiny Code Generator (TCG).
This IL means dynamic analyses can potentially be writ-
ten once and re-used for all 14 architectures supported by
QEMU. Further, this version of QEMU is capable of booting
and running modern operating systems such as Windows 7
(earlier versions of QEMU such as 0.9.1 cannot).
There are three main aspects to PANDA that make it very
convenient for building dynamic analyses. First, PANDA
provides a plug-in architecture that readily permits writing
guest analyses in C and C++. Plug-in code is executed
from a number of standard callback locations: before and
after basic blocks, memory read and writes, etc. This is not
unlike the schemes employed in other whole-system dynamic
analysis platforms such as BitBlaze [29] and S2E [7]. In ad-
dition, plugins can export functionality that can then be
used in other plugins, allowing complex behavior to be built
up from simple components. From a software engineering
perspective, PANDA’s plugin architecture allows the various
analyses supported by TZB to be cleanly separated from the
main emulator, which makes for a much more comprehensi-
ble and maintainable codebase.
The second aspect of PANDA that makes it an excellent
dynamic analysis platform is nondeterministic record and
replay (RR). In our formulation of RR, we begin a record-
ing by invoking QEMU’s built-in snapshot capability. Sub-
sequently, we record all inputs to the CPU, including ins,
interrupts, and DMA. Recording imposes a small overhead
(10-20%) but not enough to perturb execution. During re-
play, we revert to a snapshot and proceed to pull CPU inputs
from a log when required. Unlike many other RR schemes,
we do not record and replay device inputs, which means we
cannot “go live” at any point during replay. But we can per-
form repeated replays of an entire operating system under
arbitrary instrumentation load without worrying about this
perturbing application or operating system operation. This
capability is vital to TZB: without record and replay, the
heavyweight analyses we perform would make the system
unusably slow.
The ﬁnal aspect of PANDA worth mentioning is its inte-
gration of LLVM. QEMU lowers basic blocks of guest code
to its own IL, which PANDA can, additionally, re-render
as basic blocks of LLVM code via a module extracted from
S2E. We omit further discussion of this capability as it is
not used by TZB.
5.2 Callstack Monitoring
As explained in Section 2, tap points need information
about the calling context. Keeping track of this informa-
tion requires some knowledge about the CPU architecture
on which the OS is running, and so we decided to encap-
sulate this task into a single plugin. TZB’s other analyses
can then query the current call stack to arbitrary depth by
invoking get_callers and not worry about the details de-
scribed in this section.
To track call stack information, the callstack plugin ex-
amines each basic block as it is translated, looking for an
(architecture-speciﬁc) call instruction (currently, we look for
call on x86 and bl and mov lr, pc on ARM). If the block
includes a call instruction, then we push the return address
onto a shadow stack after each time that block executes.
Detecting the return from a function does not require any
architecture-speciﬁc code. Before the execution of every ba-
sic block, we check whether the address we are about to
execute is at the top of the stack; if so, we pop it. We only
need to check the starting address of the basic block, be-
cause by deﬁnition a return terminates a basic block, so the
return address will always fall at the beginning of a block.
We note that these techniques may fail if traditional call-
return semantics are violated. For example, if a program
emulated calls and returns by manually pushing the return
address and using a direct jump, it would not be detected as
a call. However, for non-malicious compiler-generated code,
we have found that the algorithm described here works well.
5.3 Fixed String Searching
Searching for ﬁxed strings is one of the most eﬀective tools
for ﬁnding useful tap points. Because we have to sift through
many gigabytes of data that pass through tap points during
any given execution, it is vital that string search be eﬃcient
in both time and space.
To satisfy these constraints, we developed stringsearch,
a plugin which requires only one byte of memory per search
string and per tap point. This one-byte counter tracks, for
a given tap point, how many bytes of the search string have
been matched by the data seen at the tap point so far.
Whenever a byte is read from or written to memory, we
can check what the next byte in the search string is using
this position, and compare it to the byte passing through
the tap point.
If it matches, the counter is incremented;
if it does not match, the counter is reset to zero. When
the counter equals the length of the search string, we know
that the search string has passed through the tap point, and
we report a match. Note that because the counter is only
one byte, our matcher only supports strings up to 256 bytes
long; this cap could be easily raised to 65,536 bytes by using
a two-byte counter, at the cost of doubling the memory re-
quirements. Thus far, 256-byte strings have been more than
suﬃcient.
This eﬀectively implements a very simple deterministic
ﬁnite automaton (DFA) matcher. Indeed, we believe that it
should be possible to eﬃciently implement a streaming basic
regular expression matcher that requires only an amount
of memory logarithmic in the number of states needed to
843represent the expression. We leave this generalization to
future work, however.
5.4 Statistical Search and Clustering
Collecting bigram statistics on data that passes through
each tap point is an eﬃcient way to enable “fuzzy” search
based on some training examples, as well as enabling clus-
tering. To implement this we collect bigram statistics for all
tap points seen in execution, as well as for the exemplar; the
data seen at each tap point is thus represented as a sparse
vector with 65,536 elements (one for each possible pair of
bytes).
To search, we can then sort the tap points seen by taking
the distance (according to some metric) from the exemplar.
For our metric, we have chosen to use Jensen-Shannon di-
vergence [18], which is a smoothed and symmetrized version
of the classic Kullback-Leibler divergence [16] (also known
as information gain). We also examined the Euclidean and
cosine distance metrics, but found their performance to be
consistently worse. Jensen-Shannon divergence between two
probability distributions P and Q is deﬁned as:
(cid:18) P + Q
(cid:19)
2
JSD(P, Q) = H
− H(P ) + H(Q)
2
where H is Shannon entropy.
Bigram collection is done by maintaining, for each tap
point, two pieces of information:
(1) the last byte that
passed through the tap point, so that we can see bigrams
that span a single memory access; (2) a histogram of all byte
pairs seen at the tap point. The latter of these must be main-
tained sparsely: because our bigrams are based on bytes, a
dense histogram would require 65,536 integers’ worth of stor-
age per tap point. Given that most of the executions exam-
ined in this paper contain upwards of 500,000 tap points,
this would require more than 120GB of memory, which is
clearly infeasible (and wasteful, since most of those entries
would be zero).
Instead, we store the histogram sparsely, using a C++
Standard Template Library std::map. This
keeps memory usage down without sacriﬁcing any accuracy,
but it does introduce some extra complexity when process-
ing the resulting histograms, as our search software must
support sparse vectors rather than simple arrays. Because
of this additional complexity, we opted to implement the