### 5.2 Unified Loss Function for Regression and Classification

The loss function for predicting packet drops can be expressed as:
\[ \ell_d = - (1 - y_d^i) \log(1 - \hat{y}_d^i) - y_d^i \log(\hat{y}_d^i) \]
where \( y_d^i \) is 1 if the packet \( i \) is dropped and 0 otherwise, and \( \hat{y}_d^i \) is the predicted probability that packet \( i \) will be dropped. Similar objectives apply to packet modifications like ECN-bit prediction.

Both regression and classification tasks are modeled using a unified loss function, which is detailed in Section 5.4.

### 5.3 Scalable Feature Selection

With the above formulations, MimicNet must select features that map well to the target predictions. This step is critical in any machine learning problem, but MimicNet introduces an additional constraint: the features must be scalable.

A scalable feature remains meaningful regardless of the number of clusters in the simulation. For example, consider a packet entering the Mimic cluster from a Core switch and destined for a host within the cluster. The local index of the destination rack ([0, R) for a cluster of R racks) is a scalable feature because adding more clusters does not affect its value, range, or semantics. In contrast, the IP address of the source server is not a scalable feature, as it uniquely identifies the origin in a small setup but loses its uniqueness as more clusters are added.

Table 1 lists the scalable features in a typical data center network with ECMP and TCP, applicable to both ingress and egress packets. Other scalable features include priority bits, packet types, and ECN markings.

| Feature                         | Description                                            |
|---------------------------------|--------------------------------------------------------|
| Local rack                      | Local index of the destination rack                    |
| Local server                    | Local index of the destination server                  |
| Local cluster switch            | Local index of the destination cluster switch          |
| Core switch traversed           | Index of the Core switch traversed                     |
| Packet size                     | Size of the packet                                     |
| Time since last packet          | Time elapsed since the last packet                     |
| EWMA of the above feature       | Exponentially weighted moving average of the feature   |
| Count                           | Number of occurrences                                  |
| # Racks per cluster             | Number of racks in each cluster                        |
| # Servers per rack              | Number of servers in each rack                         |
| # Cluster switches per cluster  | Number of cluster switches in each cluster             |
| # Core switches                 | Number of Core switches                                |

MimicNet performs two transformations on the captured features: one-hot encoding the first four features to remove any implicit ordering of devices and discretizing the two time-related features as described in Section 5.2. All these features can be quickly determined using only the packet headers, switch routing tables, and the simulator itself.

### 5.4 DCN-Friendly Loss Functions

The next task is to select an appropriate training loss function. Several characteristics of this domain make it difficult to apply the objective functions of Section 5.2 directly.

#### Class Imbalances

Even in heavily loaded networks, adverse events like packet drops and ECN tagging are relatively rare. For instance, Figure 5a shows a trace of drops over a one-second period in a simulation of two clusters, where 99.7% of training examples are delivered successfully. A model trained to predict 'no drop' would achieve high accuracy, but this is not useful. Figure 5b exemplifies this effect using an LSTM trained with Binary Cross-Entropy (BCE) loss, which predicts a drop rate almost an order of magnitude lower than the true rate.

To address this class imbalance, MimicNet adopts a Weighted-BCE (WBCE) loss:
\[ \ell_d = -(1 - w) y_d^i \log(\hat{y}_d^i) - w (1 - y_d^i) \log(1 - \hat{y}_d^i) \]
where \( w \) is a hyperparameter controlling the weight on the drop class. Figures 5c and 5d show that weighting drops can significantly improve prediction accuracy. However, setting \( w \) too high can produce false positives. From our experience, a reasonable range for \( w \) is 0.6 to 0.8, and we use tuning techniques in Section 7.2 to find the best \( w \) for a given network configuration and target metric.

#### Outliers in Latencies

In latency, accurately learning tail behavior is crucial. Figure 6a shows latencies from the previous trace, where most values are low, but a few packets incur very large latencies during congestion. These outliers are important for accurately modeling the network.

Unfortunately, Mean Absolute Error (MAE) as a loss function fails to capture the importance of these values, as shown in the latency predictions of an MAE-based model (Figure 6b), which avoids predicting high latencies. The other common regression loss function, Mean Squared Error (MSE), has the opposite problem—it squares the loss for each sample and produces models that overvalue outliers (Figure 6c).

To address this, MimicNet uses the Huber loss:
\[ \ell_l = 
\begin{cases} 
\frac{1}{2} (y_l - \hat{y}_l)^2 & \text{if } |y_l - \hat{y}_l| \leq \delta \\
\delta |y_l - \hat{y}_l| - \frac{1}{2} \delta^2 & \text{otherwise}
\end{cases}
\]
where \( \delta \in \mathbb{R}^+ \) is a hyperparameter. The Huber loss assumes a heavy-tailed error distribution and uses squared and absolute losses under different conditions. Figure 6d shows results for a model trained with the Huber loss (\( \delta = 1 \)), reducing the inaccuracy (measured in MAE) of the 99th percentile latency from 13.2% to 2.6%.

#### Combining Loss Functions

To combine the above loss functions during model training, MimicNet normalizes all values and weights them using hyperparameters. Generally, a weight that favors latency over other metrics is preferable, as regression is a harder task than classification.

### 5.5 Generalizable Model Selection

Finally, with both features and loss functions, MimicNet can begin to model users’ clusters. The model should be able to approximate the mechanics of queues and interfaces, as well as cluster-local traffic and its reactions to network conditions.

Many models exist, and the optimal choice for both speed and accuracy depends on the target network. MimicNet supports any ML model, but currently leverages LSTMs due to their ability to learn complex underlying relationships in sequences of data without explicit feature engineering.

#### Ingress/Egress Decomposition

To simplify the required models and improve training efficiency, MimicNet models ingress and egress traffic separately. This approach is enabled by MimicNet’s requirement of strict up-down routing, intrinsic modeling of cluster-local traffic, and the assumption of fan-in congestion. While some inaccuracies arise from this decision, it provides a good speed/accuracy tradeoff for all architectures tested. Each direction of traffic uses an LSTM with an input layer and a stack of flattened, one-dimensional hidden layers. The hidden size is #features × #packets, where #packets is the number of packets in a sample, and #features is post one-hotting.

#### Congestion State Augmentation

LSTMs can retain memory between predictions to learn long-term patterns, but they are typically limited to memory on the order of tens or hundreds of samples. In contrast, traffic seen by a Mimic may exhibit self-similarity on the order of hundreds of thousands of packets. To address this, we augment the LSTM model with a piece of network domain knowledge: an estimation of the presence of congestion in each cluster’s network. Four distinct states are considered: (1) little to no congestion, (2) increasing congestion as queues fill, (3) high congestion, and (4) decreasing congestion as queues drain. These states are estimated by looking at the latency and drop rate of recently processed packets in the cluster. By breaking the network into these four coarse states, the LSTM can efficiently learn patterns over these regimes, each with distinct behaviors. This feature is added to the others in Table 1.

### 7. TUNING AND FINAL SIMULATION

MimicNet composes Mimics into a parallelized large-scale data center simulation. In addition to designing the internal and feeder models with scale-independence in mind, it ensures the models survive scaling with a hyper-parameter tuning phase.

#### 7.1 Composing Mimics

An \( N \)-cluster MimicNet simulation consists of a single real cluster, \( N-1 \) Mimic clusters, and a proportional number of Core switches. The real cluster continues to use the user implementation of Section 5.1, but users can add arbitrary instrumentation, e.g., by dumping pcaps or queue depths.

The Mimic clusters are constructed by taking the ingress/egress internal models and feeders developed in the previous sections and wrapping them with a thin shim layer. The layer intercepts packets arriving at the borders of the cluster, periodically takes packets from the feeders, and queries the internal models to predict the network’s effects. The output of the shim is either a packet, its egress time, and its egress location; or its absence. Adjacent hosts and Core switches are wired directly to the Mimic but are otherwise unaware of any change.

Aside from the number of clusters, all other parameters are kept constant from the small-scale to the final simulation. This includes the feeder models and traffic patterns, which take a size parameter but fix other parameters (e.g., network load and flow size).

#### 7.2 Optional Hyper-parameter Tuning

Mimic models contain several hyper-parameters that users can optionally tune: WBCE weight, Huber loss \( \delta \), LSTM layers, hidden size, epochs, and learning rate, among others. MimicNet provides a principled method of setting these by allowing users to define their own optimization function. This function evaluates end-to-end accuracy over arbitrary behavior in the simulation (e.g., tuning for accuracy of FCTs). Users can add hyper-parameters or end-to-end optimization functions depending on their use cases.

### 6. FEEDER MODELS

While the internal models can model the behavior of queues, routers, and internal traffic of a cluster, the complete trace of external traffic is still required for accurate results. Internal models incorporate the effects of intra-cluster traffic, but the LSTMs are trained on all external traffic, not just Mimic-Real.

To replace the remaining non-observable traffic, the internal models are augmented with a feeder that estimates the arrival rate of inter-Mimic traffic and injects it into the internal model. Creating a feeder model is challenging compared to internal cluster models, as inter-Mimic traffic is not present in the small-scale simulation and varies as the simulation scales. MimicNet addresses this by creating a parameterized and fully generative model that uses flow-level approximation techniques to predict the packet arrival rate of Mimic-Mimic traffic in different network sizes.

The feeder model is trained in parallel to the internal models. MimicNet first derives characteristic packet interarrival distributions for all external flows from the small-scale simulation, separated by their direction (ingress/egress). Simple log-normal or Pareto distributions produce reasonable approximations of these interarrival times. More sophisticated feeders can be trained and parameterized in MimicNet. During the full simulation, the feeders will take the hosts’ inter-cluster demand as a parameter, compute the packet arrival rates, and inject them into the internal model.