𝑖 ) log(1 − ˆ𝑦𝑑
𝑖 )
𝑖 ∈ [0, 1] is the
is 1 if 𝑖 is dropped and 0 otherwise, and ˆ𝑦𝑑
where 𝑦𝑑
𝑖
predicted probability that 𝑖 is dropped. Packet modifications like
ECN-bit prediction share a similar objective.
Both regression and classification tasks are modeled together with
a unified loss function, which we describe in Section 5.4.
5.3 Scalable Feature Selection
With the above formulations, MimicNet must next select features
that map well to the target predictions. While this is a critical step
in any ML problem, MimicNet introduces an additional constraint—
that the features be scalable.
A scalable feature is one that remains meaningful regardless of
the number of clusters in the simulation. Consider a packet that
enters the Mimic cluster from a Core switch and is destined for a
host within the cluster. The local index of the destination rack ([0,
𝑅) for a cluster of 𝑅 racks) would be a scalable feature as adding
more clusters does not affect the value, range, or semantics of the
feature. In contrast, the IP of the source server would NOT be a
scalable feature. This is because, with just two clusters, it uniquely
identifies the origin of the packet, but as clusters are added to the
simulation, never-before-seen IPs are added to the data.
Table 1 lists the scalable features in a typical data center network
with ECMP and TCP, applicable to both ingress and egress packets.
Other scalable features that are not listed include priority bits,
packet types, and ECN markings.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Q. Zhang et al.
MimicNet strikes a balance with the Huber loss [23]:
Feature
Local rack
Local server
Local cluster switch
Core switch traversed
Packet size
Time since last packet
EWMA of the above feature
Count
# Racks per cluster
# Servers per rack
# Cluster switches per cluster
# Core switches
single integer value
single real value (discretized)
single real value (discretized)
Table 1: Basic set of scalable features.
MimicNet performs two transformations on the captured fea-
tures: one-hot encoding the first four features to remove any implicit
ordering of devices and discretizing the two time-related features
as in Section 5.2. Crucially, all of these features can quickly be de-
termined using only packets’ headers, switch routing tables, and
the simulator itself.
5.4 DCN-friendly Loss Functions
The next task is to select an appropriate training loss function.
Several characteristics of this domain make it difficult to apply the
objective functions of Section 5.2 directly.
Class imbalances. Even in heavily loaded networks, adverse events
like packet drops and ECN tagging are relatively rare occurrences.
For example, Figure 5a shows an example trace of drops over a
one-second period in a simulation of two clusters. 99.7% of training
examples in the trace are delivered successfully, implying that a
model of loss could achieve high accuracy even if it always predicts
‘no drop.’ Figure 5b exemplifies this effect using an LSTM trained
using BCE loss on the same trace as above. It predicts a drop rate
of almost an order of magnitude lower than the true rate.
To address this instance of class imbalance, MimicNet takes a
cost-sensitive learning approach [13] by adopting a Weighted-BCE
(WBCE) loss:
ℓ𝑑 = −(1 − 𝑤) 𝑦𝑑
𝑖 − 𝑤(1 − 𝑦𝑑
𝑖 log ˆ𝑦𝑑
𝑖 ) log(1 − ˆ𝑦𝑑
𝑖 )
where 𝑤 is the hyperparameter that controls the weight on the drop
class. Figure 5c and 5d show that weighting drops can significantly
improve the prediction accuracy. We note, however, that setting
𝑤 too high can also produce false positives. From our experience,
0.6∼0.8 is a reasonable range, and we rely on tuning techniques in
Section 7.2 to find the best 𝑤 for a given network configuration and
target metric.
Outliers in latencies. In latency, an equivalent challenge is accu-
rately learning tail behavior. For example, consider the latencies
from the previous trace, shown in Figure 6a. While most values
are low, a few packets incur very large latencies during periods of
congestion; these outliers are important for accurately modeling
the network.
Unfortunately, MAE as a loss function fails to capture the im-
portance of these values, as shown in the latency predictions of
an MAE-based model (Figure 6b), which avoids predicting high
latencies. We note that the other common regression loss function,
Mean Squared Error (MSE), has the opposite problem—it squares
the loss for each sample and produces models that tend to overvalue
outliers (Figure 6c).
292
 𝐻𝛿(𝑦𝑙
(cid:40) 1
ℓ𝑙 =
2 (𝑦𝑙 − ˆ𝑦𝑙)2,
𝛿|𝑦𝑙 − ˆ𝑦𝑙| − 1
𝑖)
𝑖 , ˆ𝑦𝑙
if |𝑦𝑙 − ˆ𝑦𝑙| ≤ 𝛿,
otherwise
2𝛿2,
𝐻𝛿(𝑦𝑙 , ˆ𝑦𝑙) =
where 𝛿 ∈ R+ is a hyperparameter. Essentially, the Huber loss
assumes a heavy-tailed error distribution and uses the squared loss
and the absolute loss under different situations. Figure 6d shows
results for a model trained with the Huber loss (𝛿 = 1). In this
particular case, it reduces inaccuracy (measured in MAE) of the
99-pct latency from 13.2% to only 2.6%.
Combining loss functions. To combine the above loss functions
during model training, MimicNet normalizes all values and weights
them using hyperparameters. Generally speaking, a weight that
favors latency over other metrics is preferable as regression is a
harder task than classification.
5.5 Generalizable Model Selection
Finally, with both features and loss functions, MimicNet can begin
to model users’ clusters. The model should be able to learn to
approximate the mechanics of the queues and interfaces as well as
cluster-local traffic and its reactions to network conditions (e.g., as
a result of congestion control).
Many models exist and the optimal choice for both speed and
accuracy will depend heavily on the target network. To that end,
MimicNet can support any ML model. Given our desire for general-
ity, however, it currently leverages one particularly promising class
of models: LSTMs. LSTMs have gained recent attention for their
ability to learn complex underlying relationships in sequences of
data without explicit feature engineering [22].
Ingress/egress decomposition. To simplify the required mod-
els and improve training efficiency, MimicNet models ingress and
egress traffic separately. This approach is partially enabled by Mim-
icNet’s requirement of strict up-down routing, the intrinsic model-
ing of cluster-local traffic, and the assumption of fan-in congestion.
While there are still some inaccuracies that arise from this deci-
sion (e.g., the effect of shared buffers), we found that this choice
was another good speed/accuracy tradeoff for all architectures we
tested. For each direction of traffic, the LSTMs consist of an input
layer and a stack of flattened, one-dimensional hidden layers. The
hidden size is #features × #packets where #packets is the number of
packets in a sample, and #features is post one-hotting.
Congestion state augmentation. While in principle, LSTMs can
retain ‘memory’ between predictions to learn long-term patterns, in
practice, they are typically limited to memory on the order of 10s or
100s of samples. In contrast, the traffic seen by a Mimic may exhibit
self-similarity on the order of hundreds of thousands of packets.
Our problem, thus, exhibits properties of multiscale models [11].
Because of this, we augment the LSTM model with a piece of
network domain knowledge: an estimation of the presence of con-
gestion in each cluster’s network. Specifically, four distinct states
are considered: (1) little to no congestion, (2) increasing congestion
as queues fill, (3) high congestion, and (4) decreasing congestion as
queues drain. These states are estimated by looking at the latency
MimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
(a) Ground truth
(b) Pred w/ BCE
(c) Pred w/ 0.6 WBCE
(d) Pred w/ 0.9 WBCE
Figure 5: Ground truth and LSTM-predicted drops for a one-second test set using different loss functions. The y-axis is 1
for dropped, 0 for not. Ground truth has 0.3% drop rate and BCE loss has 0.01%. WBCE results in more realistic drop rates
depending on the weight (𝒘 = 0.6: 0.14%; 𝒘 = 0.9: 0.49%).
(a) Ground truth
(b) Pred w/ MAE (1.4 × 10−4)
(c) Pred w/ MSE (3.3 × 10−4)
(d) Pred w/ Huber (1.1 × 10−4)
Figure 6: Ground truth and LSTM-predicted latency (in seconds) for a one-second test set using different loss functions. With
each, we report the output of the objective, MAE (listed in parentheses). Unfortunately, using MAE directly as the loss function
fails to capture outliers. Instead, Huber produces more realistic results and a better eventual MAE score.
and drop rate of recently processed packets in the cluster. By break-
ing the network up into these four coarse states, the LSTM is able
to efficiently learn patterns over these regimes, each with distinct
behaviors. This feature is added to the others in Table 1.
surrounding queues, i.e., queues of the Core switch or the egress
queues on the Hosts; as prior work has noted, the majority of drops
and congestion are found elsewhere in the network [50].
7 TUNING AND FINAL SIMULATION
MimicNet composes Mimics into a parallelized large-scale data
center simulation. In addition to designing the internal and feeder
models with scale-independence in mind, it ensures the models
survive scaling with a hyper-parameter tuning phase.
7.1 Composing Mimics
An 𝑁 -cluster MimicNet simulation consists of a single real cluster,
𝑁 − 1 Mimic clusters, and a proportional number of Core switches.
The real cluster continues to use the user implementation of Sec-
tion 5.1, but users can add arbitrary instrumentation, e.g., by dump-
ing pcaps or queue depths.
The Mimic clusters are constructed by taking the ingress/egress
internal models and feeders developed in the previous sections
and wrapping them with a thin shim layer. The layer intercepts
packets arriving at the borders of the cluster, periodically takes
packets from the feeders, and queries the internal models with both
to predict the network’s effects. The output of the shim is, thus,
either a packet, its egress time, and its egress location; or its absence.
Adjacent hosts and Core switches are wired directly to the Mimic,
but are otherwise unaware of any change.
Aside from the number of clusters, all other parameters are kept
constant from the small-scale to the final simulation. That includes
the feeder models and traffic patterns, which take a size parameter
but fix other parameters (e.g., network load and flow size).
7.2 Optional Hyper-parameter Tuning
Mimic models contain at least a few hyper-parameters that users
can optionally choose to tune: WBCE weight, Huber loss 𝛿, LSTM
layers, hidden size, epochs, and learning rate among others. Mim-
icNet provides a principled method of setting these by allowing
users to define their own optimization function. This optimization
function is distinct from the model objectives or the loss functions.
Instead, they can evaluate end-to-end accuracy over arbitrary be-
havior in the simulation (for instance, tuning for accuracy of FCTs).
Users can add hyper-parameters or end-to-end optimization func-
tions depending on their use cases.
6 FEEDER MODELS
While the above (internal) models can model the behavior of the
queues, routers, and internal traffic of a cluster, the complete trace
of external traffic is still required to generate accurate results. In
the terminology of Figure 4, internal models bake in the effects of
the intra-cluster traffic, but the LSTMs are trained on all external
traffic, not just Mimic-Real.
To replace the remaining non-observable traffic, the internal
models are augmented with a feeder whose role is to estimate the
arrival rate of inter-Mimic traffic and inject them into the internal
model. Creating a feeder model is challenging compared to internal
cluster models as inter-Mimic traffic is not present in the small-scale
simulation and varies as the simulation scales. MimicNet addresses
this by creating a parameterized and fully generative model that
uses flow-level approximation techniques to predict the packet
arrival rate of Mimic-Mimic traffic in different network sizes.
The feeder model is trained in parallel to the internal models.
MimicNet first derives from the small-scale simulation characteris-
tic packet interarrival distributions for all external flows, separated
by their direction (ingress/egress). In our tests, we observed, as
others have in the past [8, 31] that simple log-normal or Pareto dis-
tributions produced reasonable approximations of these interarrival
times. Nevertheless, more sophisticated feeders can be trained and
parameterized in MimicNet. During the full simulation, the feeders
will take the hosts’ inter-cluster demand as a parameter, compute a