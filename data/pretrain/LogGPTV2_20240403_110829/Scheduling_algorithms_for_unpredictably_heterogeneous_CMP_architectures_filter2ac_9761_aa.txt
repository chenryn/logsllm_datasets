title:Scheduling algorithms for unpredictably heterogeneous CMP architectures
author:Jonathan A. Winter and
David H. Albonesi
Scheduling Algorithms for Unpredictably Heterogeneous
CMP Architectures
Jonathan A. Winter and David H. Albonesi
Computer Systems Laboratory, Cornell University
{winter, albonesi}@csl.cornell.edu
Abstract
In  future  large-scale  multi-core  microprocessors, 
hard errors and process variations will create dynamic
heterogeneity,  causing  performance  and  power 
characteristics  to  differ  among  the  cores  in  an 
unanticipated  manner.  Under  this  scenario,  naïve 
assignments  of  applications  to  cores  degraded  by 
various  faults  and  variations  may  result  in  large 
performance 
losses  and  power  inefficiencies.  We 
propose 
the 
Hungarian  Algorithm  and  artificial  intelligence  (AI) 
search 
future 
in  core  characteristics.  These  thread 
uncertainty 
assignment  policies  effectively  match  the  capabilities 
of  each  degraded  core  with  the  requirements  of  the 
applications,  achieving  an  ED2 only  3.2%  and  3.7% 
higher,  respectively,  than  a  baseline  eight  core  chip 
multiprocessor with no degradation, compared to over 
22% for a round robin policy.
scheduling  algorithms  based  on 
that  account 
techniques 
this 
for 
1. Introduction
The microprocessor industry has transitioned to the 
strategy of incorporating additional processor cores on 
a die with each new process generation. While this chip 
multiprocessor  (CMP)  approach  has  the  potential  to 
provide 
processing 
performance  on  a  single  die,  to  bring  this  vision  to 
fruition  in  the  long  term,  architects  must  address  a 
number  of 
them 
programmability, power consumption, and reliability.  
significant  challenges,  among 
supercomputer 
levels 
of 
In terms of the latter, while transient (soft) errors are 
the  primary  focus  today,  permanent  (hard)  errors  and 
circuit  variability  are  expected  to  become  a  major 
challenge  in  the  future  [4]. In  this  paper,  we  consider 
permanent  faults  and  variability  that  are  caused  by 
imperfections  in  the  manufacturing  process  and  from 
wear-out  over  the  lifetime  of  the  chip.  As  transistors 
continue  to  shrink  to  microscopic  dimensions,  the 
manufacturing  process  becomes
less  dependable, 
resulting  in  more  defective  transistors  and  wires. 
Moreover,  these  components  more  easily  wear  out 
when subjected to the stress of high levels of activity, 
power, and temperature.
Ultimately,  a  major  consequence  of  decreasing 
hardware  reliability  is  that  many  cores on the die will 
provide  performance/power  efficiency  levels  below 
that  to  which  they  were  designed.  Some  components 
will  have  faults,  certain  circuits  will  be  leakier  than 
normal,  and  some  transistors  on  critical  paths  will  be 
slower, thereby requiring reduced frequency for correct 
operation.  Manufacturers  will  not  have  the  option  of 
shipping  only  fully  functional  chips  as  this  will 
necessitate unaffordably low yields. Instead, in order to 
provide  reasonable  performance  at  acceptable  cost, 
future  CMPs  will  be  designed  to  tolerate  faults  and 
variations  and  operate in a degraded state [29]. These 
degradations are largely random physical processes that 
occur  during  manufacturing  and  usage.  Consequently, 
each  core  will  be  uniquely  affected  by  manufacturing 
and  wear-out  defects.  Thus,  the  resulting  degraded 
CMP  will  be  an unpredictably  heterogeneous  multi-
core  system,  even 
to  be
homogeneous. 
it  was  designed 
if 
can 
that 
explored 
processors 
A  number  of  previous  studies  [1,5,28,29,30,33] 
tolerate 
have 
manufacturing  and  wear-out  faults  and  variations, 
thereby keeping these chips operational. Prior research 
has  not  yet  addressed  the  problem  of  ensuring  that 
these  degraded  multi-core  processors  deliver  adequate 
performance  and  power  efficiency  throughout  their
expected  lifetime.  Even  if  processors  are  able  to 
continue  to  function  in  the  presence  of  errors  and 
variability, they may not deliver the minimum expected 
level of power/performance efficiency, causing them to 
be rendered unusable before their expected end of life. 
This  paper  addresses  this  issue  through  self-tuning 
operating system scheduling policies that use high-level 
system feedback to match application characteristics to 
the  degraded  cores  so  as  make  the  performance  and 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE42DSN 2008: Winter & Albonesipower 
imperceptible to the user. 
impact  of  hard  errors  and  variability 
While many prior reliability efforts examined purely 
hardware solutions [5,17,18,28,29,30,32,33], we take a 
combined  hardware/software  approach  to  address  this 
complex  scheduling  problem.  The  hardware  is  best 
capable of providing feedback on the performance and 
power  dissipation  of  threads  running  on  the  degraded 
processors. On the other hand, the operating system is 
best  situated  to  assess  the  overall  situation  and  to 
balance  the  requirements  of  each  application  from  a 
global perspective.
We  explore  two  methodologies  for  attacking  the 
scheduling problem. First, by assuming that application 
behavior changes slowly and that interactions between 
applications  are  limited,  we  reduce  the  scheduling 
problem  to  the  Assignment  Problem,  which  can  be 
solved  by  employing  the  Hungarian  Algorithm  [21]. 
Our second approach is to apply iterative optimization 
algorithms  that  have  been  shown  to  be  effective  on 
many 
similarly  difficult  combinatorial  problems 
[25,27].  These  iterative  techniques  operate  with  little 
domain-specific  knowledge,  are  easy  to  implement, 
have 
and 
continuously improve their solution, permitting the user 
to trade off algorithm runtime and solution quality. To 
our  knowledge,  our  study  is  the  first  work  to  apply 
iterative  optimization  algorithms  to  heterogeneous 
multi-core thread scheduling.
computational 
requirements, 
low 
2. Related Work
A  number  of  prior  research  areas  relate  closely  to 
our  degraded  multi-core  scheduling  problem.  First,  a 
number  of  efforts  address  architectural  techniques  to 
tolerate permanent errors resulting from manufacturing 
defects  or  wear-out  during  the  processor  lifetime. 
Shivakumar  et  al.  [29]  suggest  exploiting  inherent 
redundancy  in  the  processor  for  hard  error  tolerance. 
Srinivasan et al. [33] propose two methods to increase 
the  processor 
lifetime:  structural  duplication  and 
graceful  performance  degradation.  Schuchman  and 
Vijaykumar  [28]  develop  a  methodology  for  testing 
architecture  level  components  and  isolating  faults. 
Bower  el  al.  [5]  address  fault  diagnosis  and  unit  de-
configuration.  Distributed  built-in  self-testing  and 
checkpointing  techniques  are  devised  by  Shyam  et  al. 
[30] for detecting and recovering from defects. Finally, 
Aggarwal  et  al.  [1]  study  mechanisms  for  isolating 
faulty  components  in  a  CMP  and  reducing  an  error’s 
impact 
through  reconfiguration.  We  assume  such 
schemes are already implemented in our baseline CMP, 
permitting  cores  to  function  in  degraded  states.  Our 
work examines the next stage of the problem:  making 
most effective use of the resulting heterogeneous CMP 
by scheduling applications to match core and workload 
characteristics.
A  second  direction  of  prior  research  strives  to 
understand, model, and mitigate manufacturing process 
variations  (PV).  Most  work  on  PV  focuses  on  the 
semiconductor device and circuit level, but a number of 
researchers  have  devised  system-level  approaches. 
Humenay  et  al.  [10,  11]  examine  how  parameter 
variations  specifically  impact  multi-core  chips.  A 
number  of  studies  [17,18,22]  propose  techniques  to 
reduce  the  negative  impact  of  variations  on  frequency 
and  yield.  Many  of 
create 
heterogeneity on a CMP by disabling array elements or 
creating variable access times. 
the  mechanisms 
Other previous research uses the operating system to 
improve CMP energy efficiency. Juang et al. [13] argue 
for  coordinated  formal  control-theoretic  methods  to 
manage energy efficiency in multi-core systems. Li and 
Martínez  [16]  investigate  heuristics  that  adaptively 
change the number of cores used, and the chip voltage 
and  frequency 
to  optimize  power-performance  in 
parallel  applications.  Isci  et  al.  [12]  further  develop 
globally  aware  policies  to  dynamically  tune  DVFS  to 
workload  characteristics  to  maximize  performance 
under  a  chip-wide  power  constraint.  While  this  effort 
has similar elements to ours, they use DVFS to improve 
efficiency,  whereas  in  our  heterogeneous  system,  we 
use core scheduling. 
in 
the  focus 
homogeneous 
Most  papers  on  power-aware  multi-core  thread 
thermal 
scheduling  are  primarily  concerned  with 
control 
chip  multiprocessors 
[7,8,20,24,34].  In  heterogeneous  chip  multiprocessors 
architectures [3,15], the heterogeneity is designed into 
the  system  rather  than  the  unintentional  result  of 
hardware faults and variations. As a result, the degree 
and nature of heterogeneity is quite different. In Kumar 
et  al.  [15] 
is  on  multi-programmed 
performance and applications are scheduled on cores to 
best  match  execution  requirements.  However,  since 
only two types of cores are used, the solution space is 
small  and  thus  a  simple  sampling  scheme  achieves 
good assignments. Becchi and Crowley [3] extend that 
work 
for 
scheduling.  Our  scheduling  problem  is  far  more 
complex: 
of 
heterogeneous  organizations  can  arise  in  term  of 
frequency,  dynamic  power,  and  leakage  currents,  in 
addition to architectural parameters.
to  use  performance  driven  heuristics 
unpredictably 
an 
large 
number 
the  cores  are  not 
Kumar et al. [14] study heterogeneous architectures 
where 
few 
configurations.  The  goal  is  to  determine  how  much 
heterogeneity is necessary and how the cores should be 
designed to fit a given power budget. They focus on the 
restricted 
to  a 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE43DSN 2008: Winter & Albonesiarchitectural  design  issues  rather  than  the  scheduling 
aspect of the problem. Balakrishnan et al. [2] study the 
impact  of  asymmetry  in  core  frequency  on  parallel 
commercial  workloads  using  a  hardware  prototype. 
Ghiasi  et  al.  [9]  examine  heterogeneity  resulting  from 
cores  running  at  different  voltages  and  frequencies. 
While 
the  core  voltages  and 
frequencies,  we  investigate  cores  with  unpredictably 
heterogeneous frequencies and power output. We adapt 
the  workload  assignment to mitigate possible negative 
affects.
their  work  adapts 
Scheduling 
3. 
Unpredictable CMPs
Algorithms 
for 
We  propose  scheduling  algorithms  that  assign 
applications  to  cores  over  a  fixed,  relatively  short 
period  of  time.  Scheduling  decisions  are  periodically 
reassessed  to  account  for  large  application  phase 
changes,  programs  completing,  and  new  applications 
arriving to be processed. Our best algorithms consist of 
an exploration phase where samples of thread behavior 
on  different  cores  are  observed  and  a  steady  phase 
during  which  the  algorithm  runs  the  best  schedule  it 
found during the sampling phase.
in 
table  specifies 
this  paper.  The 
Table 1 compares the scheduling algorithms that we 
explore 
the 
complexity of each algorithm where N is the number of 
cores (and the number of applications). For comparison 
purposes,  we  also  implement  randomized  and  round 
robin  scheduling,  two  simple  algorithms  that  have 
worked well on past multi-core designs. 
3.1. Hungarian Scheduling Algorithm
The Hungarian scheduling algorithm is based on the 
Hungarian  Algorithm  developed  by  mathematicians  to 
solve the well-known Assignment Problem, also called 
Weighted  Bipartite  Matching  in  graph  theory  [21]. 
During  the  exploration  phase,  the  algorithm  samples 
application  performance  and  power  statistics  on  each 
core  and  picks  the  best  scheduling  assignment.  In 
general, finding the best schedule is extremely difficult 
because  threads  interact  during  execution  through 
contention  for  I/O  and  memory  bandwidth  as  well  as 
through heat conductivity between cores. Furthermore, 
program  behavior  is  dynamic  both  in  the  short-term 
time  frame  and  over  large  program  phases,  such  that 
sample information may not reflect future behavior.
In  order  to  simplify  the  problem,  our  algorithm 
assumes  that  there  are  no  such  interactions  between 
threads and that program behavior is static – at least for 
the  duration  of  the  exploration  phase.  Making  these 
assumptions  eliminates  the  interdependence  between 
execution  samples  running  simultaneously,  reducing 
the scheduling problem to the Assignment Problem.
The  Assignment  Problem  is  defined  as  follows. 
Given  an  N×N  cost  matrix  where  the  (i,j)  element 
represents  the  cost  of  running  application  i  on  core  j, 
find the assignment of applications to cores with lowest 
total cost. In our case, the elements of the cost matrix 
consist  of  the  normalized  energy-delay-squared  (ED2) 
product  obtained  by  first  sampling  the  execution  of 
applications  on  each  core.  For  each  application,  we 
divide each ED2 sample by the ED2 obtained during the 
first sampling interval to obtain the normalized values. 
Normalization  ensures  that  applications  are  treated 
fairly  by  the  scheduler  despite  any  differences  in  the 
absolute value of their performance and power data. 
Step 1: For each row of the matrix, find 
the smallest element and subtract it from 
every element in its row. Go to Step 2. 
Step 2: Find a zero (Z) in the resulting 
matrix.  If  there  is  no  starred  zero  in 
its  row  or  column,  star  Z.  Repeat  for 
each zero in the matrix. Go to Step 3. 
Step  3:  Cover  each  column  containing  a 
starred  zero.  If  N  columns  are  covered, 
the starred zeros describe a complete set 
of  unique  assignments  and  the  algorithm 
is done. Otherwise, go to Step 4. 
Step 4: Find a non-covered zero and prime 
it.  If  there  is  no  starred  zero  in  the 
row  containing  this  primed  zero,  go  to 
Step  5.  Otherwise,  cover  this  row  and 
uncover the column containing the starred 
zero.  Continue  in  this  manner  until  all 
zeros  are  covered.  Save  the  smallest 
uncovered value and go to Step 6. 
Step 5: Construct a series of alternating 