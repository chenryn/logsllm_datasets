to be maintained across kernel updates. Section 5.1 analyzes our checkpoint format and
code in more detail.
Chapter 4
Implementation
This section describes the implementation of our kernel update system. First, we describe
the process checkpoint-restore mechanism used for updating kernels. Then, we present
the interface to our update system that can be used by system-level utilities to interact
with our system. Finally, we describe the limitations of our current implementation and
methods for addressing the limitations.
4.1
Implementation of checkpoint and restore
The checkpoint save operation involves saving data structure values and is relatively
simple. The restore operation is more complicated because it requires recreating custom
processes from the checkpoint information, similar to creating the initial user process.
Hence, much of the description below focuses on the restore operation.
4.1.1 Threads
The stop_machine kernel function used for quiescing the system (see Section 3.2). It
schedules a thread on each CPU and each thread disables interrupts. This process waits
until all other threads go through a context switch. At which point, the kernel stores the
24
Chapter 4.
Implementation
25
register values and segment descriptors table entries in the thread_struct structure. We
store this context switch data in our checkpoint. To restore a thread, we spawn a kernel
thread for each thread stored in the checkpoint. Within the context of each spawned
thread, we invoke a function, that we created, similar to the execve system call. The
execve system call replaces the state of the calling process with a new process whose
state is obtained by reading an executable le from disk. Our function converts the
kernel thread into a user thread by loading the state from the checkpoint in memory. We
restore the saved register values and segment descriptors for the thread so that the new
kernel's context switch code can use these values to resume thread execution.
We restore the saved task_struct elds and reestablish the parent-child process hi-
erarchy by changing the parent and real_parent pointers so that they point to their
restored parents. We also make sure that all restored children are added to the list of
children associated with their parent, and for multi-threaded processes we add threads
to their thread group lists. After this setup, the kernel starts identifying the spawned
kernel thread as a regular user process.
4.1.2 Address Space
An address space consists of a set of memory mapping regions and page tables. Each
memory mapping region describes a region of virtual addresses and stores information
about the mapping such as protection bits and the backing store. The page table stores
the mapping from virtual pages to physical pages. Currently, our implementation sup-
ports the x86 architecture where the page table structure is specied by the hardware
and thus will not change across kernel versions.
Linux manages memory mapping regions using a top-level memory descriptor data
structure (mm_struct) and one or more memory region descriptors (vm_area_struct).
We store various elds associated with these data structures, including the start and
end addresses of each memory region, protection ags, whether or not the region is
Chapter 4.
Implementation
26
anonymous, and the backing store, as shown in Figure 3.1. For memory mapped les,
we store the le name and the oset of the le for the virtual memory region. We restore
these data structures by using the same functions that the kernel uses for allocating
them during the execve (mm_struct), mmap (vm_area_struct) and mprotect system
calls. These functions allow us to handle both anonymous regions and memory-mapped
les. For example, we restore a memory-mapped le region by reopening the backing le
and mapping it to the address associated with the region. The memory region structures
can be shared and we handle any such sharing as described earlier.
The x86 architecture uses multi-level pages tables, and the top-level page table is
called the page table directory. This page table format will not change across kernel
versions and so we do not copy page tables or user pages during the checkpoint and
restore. As a result, a process accesses the same page tables and physical pages before
and after the kernel update. However, one complication with restoring page tables is that
the Linux kernel executes in the address space context of the current user thread, and
it is mapped at the top of the virtual address space of all processes. The corresponding
page table entries for each process need to be updated after the kernel update. These
page table entries are located in the page table directory. The function that creates the
memory descriptor data structure (mm_struct) also initializes the page table directory
with the appropriate kernel page table entries. We initialize the rest of this new page
table directory from its pre-reboot version and then release the latter. At this point, we
notify the memory manager to switch all process pages from being reserved to allocated
to the new process.
4.1.3 Files
The Linux kernel uses three main data structures to track the les being used by a
process. The top-level fs_struct structure keeps track of the current working directory
and the root directory of a process (the root directory can be changed with the chroot
Chapter 4.
Implementation
27
system call). The le descriptor table contains the list of allocated le descriptors and
the corresponding open les. Finally, the le descriptor structure stores information
about each open le. All three structures can be independently shared between several
processes. For example, two processes might share the same working directory, may have
dierent le descriptor tables, and yet share one or more opened les.
For each process, we store its root and current working directory, list of open le
descriptors, and information about open les. Linux stores the current root and current
working directory of a process as dentry structures. In the checkpoint, we store them
as full path names. For les, we store its full path, inode number, current le position,
access ags, and le type. The full path of each le is obtained by traversing the linked
list of dentry structures. Each le structure has a pointer to a dentry structure, which
stores the le name. In turn, each dentry structure has a pointer to another dentry which
stores the name of the parent directory. For non-regular les (e.g., sockets, terminals),
we store additional data needed to restore them, as discussed in later sections.
When restoring each process, we call chroot to restore the current root and chdir
to restore the current working directory. Restoring open les requires calling functions
that together implement the open and dup system calls. We do not use these system
calls directly because our code has to be exible enough to handle restoring shared data
structures. For example, when the entire le descriptor table is shared between threads
(or processes), once the table is setup for a thread, les do not need to be opened or
duped in the second thread. To restore an open le, we rst call a function that creates
a le descriptor structure. Then we open the le using the ags, such as read/write,
non-blocking I/O, etc., that were saved in the checkpoint. Next, we use the lseek system
call to set the current le position. Then we dup the le descriptor so that it uses the
correct descriptor number, and nally, we install this descriptor in the le descriptor
table.
Temporary les require some additional steps before they can be restored. A tem-
Chapter 4.
Implementation
28
porary le is created when unlink is called on a le in use (an open le) and the link
count of the le reaches zero. In this case, all directory entries referencing the le have
been removed and the le cannot be opened again. However, the contents of the le
exist until all applications using the le release their reference to it, at which point the
le is deleted permanently. If a system crash occurs, temporary les need to be removed
(garbage collected) since they are not accessible in the system (no directory entries point
to these les and the processes that were using the le before the crash do not exist
any longer). File systems remove temporary les (when an open le's link count reaches
zero) by adding a reference to the le to an orphan list that is kept on disk. At mount
time, the orphan list is traversed and the temporary les are deleted. We do not want
the temporary les to be deleted so that the restored processes can continue using these
les. To do so, we create a hardlink to the temporary le when taking the checkpoint.
We use a function used by the hardlink system call to create a link to the le, but instead
of using the le name we use the inode of the temporary le as the source of the link.
We perform this step as part of deferred processing when the interrupts are enabled and
the disk can be accessed (See Section 3.2). During restore, a temporary le is handled
similar to a regular le, except that after it is opened, we invoke the unlink system call
to remove the reference to the le from the lesystem.
We ensure that all dirty le system buers are committed to disk by calling the
le-system wide sync operation as part of deferred kernel processing, as described in
Section 3.2. As a result, we do not need to save and restore the contents of the le-
system buer cache.
4.1.4 Network Sockets
A network socket provides the interface to the dierent Internet protocols supported by
the Linux kernel. Our update system currently supports UDP and TCP protocols because
these are the most common protocols used by applications. Other types of network
Chapter 4.
Implementation
29
protocols such as ICMP and raw IP sockets are typically used by utility applications and
have little state. As a result, they do not benet from our update approach and were not
implemented. The socket interface allows reading and writing packets using the read and
write system calls similar to les but it also provides operations such as bind, connect
and accept. The kernel represents sockets by le descriptors that store protocol state
associated with the socket.
Network applications and protocols must already handle network failures that cause
packets to be lost, duplicated or re-ordered. We rely on this behavior to simplify restoring
network connections. Once the application (and/or protocol) state is restored, the ap-
plication can handle any problems that arise as a result of packets being dropped during
the update. For example, TCP handles lost packets using retransmissions transparent to
TCP applications. From an application's perspective, the update process will seem like
a temporary network delay.
4.1.4.1 UDP Sockets
UDP is a stateless protocol for sending messages over the network. It does not provide
reliability, integrity or ordering, so this makes restoring UDP sockets straight forward.
When creating a checkpoint, we store the source and destination IP addresses and port
numbers of the socket. To restore a UDP socket, we call the socket function to create
the socket descriptor and then optionally call the bind function to assign a port number
to the socket. We discard any sent or received data that was still being processed by
the kernel and let the application handle packet loss. We rely on user utilities to set up
the IP address of the machine and the routing tables so that network communication is
possible after the update.
Chapter 4.
Implementation
30
4.1.4.2 TCP Sockets
The TCP protocol provides a reliable, stream-oriented network connection to socket
endpoints. It guarantees delivery without packet loss, reordering or duplication. TCP
runs above the unreliable IP protocol and makes these guarantees by requiring the receiver
to send acknowledgments for the data it has received to the sender. The sender assigns
a sequence number to each byte of data that it has sent. Depending on the received
acknowledgments, it decides to transmit the next packet in the sequence or it retransmits
already sent packets. The sender also keeps a timer for each packet sent and retransmits
packets if an acknowledgment is not received for the packet before the timer expires or
if it receives too many duplicate acknowledgments. TCP is bidirectional so each socket
acts as both a sender and a receiver.
Internally, when an application writes to a TCP socket, the user data is either split
or combined (Nagle algorithm) into segments of a certain size (e.g., maximum segment
size). After each segment is created it is added to the sender's write queue. TCP adds
a header to each segment in the write queue and passes it on to the lower layer protocol
(IP) that transmits the segment on the network. After the segment is transmitted, it is
moved to a retransmit queue in case it is needed for retransmission. A segment is taken
o the queue after an acknowledgment is received.
There are two types of TCP sockets, listen and communication sockets. Listen sockets
wait for incoming connection attempts and perform the TCP three-way handshake to
establish a connection. For these sockets, we save the local address and the port the
socket was listening on, as well as the maximum number of pending connections. This
state is specied by applications when creating sockets using the system call interface.
Restoring a listen socket involves issuing the same system calls as needed to create the
listen socket. A listening Unix socket creates a le name for the socket, but this name
does not get removed on a kernel reboot. We delete the name during restore, and the
socket restore code recreates this name. This le is owned by the restore process (root)
Chapter 4.
Implementation
31
and we use chown system call to change the ownership to the original owner.
Communication sockets are used to transmit data. These sockets maintain much
more state than the listen sockets. Some of this state, such as the last received sequence
number, acknowledged sequence number and the packets in the write and retransmit
queues, must be checkpointed because it is essential for correct TCP operation. Failing to
restore such state will result in lost data and termination of the restored TCP connection.
Other TCP state, such as the congestion avoidance state, is performance related and
does not need to be checkpointed for correct TCP operation. This state can be reset to
default values and the TCP implementation will automatically adjust them to reect the
network conditions. However, in some cases, ignoring the performance related state has
a signicant impact on TCP throughput as discussed below.
When restoring a connection, we allocate the socket descriptor and then restore the
connection state. This state consists of the source and destination addresses, port num-
bers, and sequence numbers of the received and sent data. Then we restore the contents
of the write and retransmit queues while preserving TCP headers so that segmentation
does not need to be performed again.
At this point, we can resume sending packets. However, which packet should be sent
initially? The sending side maintains two counters, snd.una, the rst unacknowledged se-
quence number (sender knows that all octets or bytes smaller than this sequence number
are acknowledged and does not keep them in its retransmit queue), and snd.nxt, the next
sequence number that the sender should send. Similarly, the receiving side maintains a
counter, rcv.nxt, the next sequence number expected on an incoming segment. Initially,
we started sending packets starting from the snd.una sequence number because the pack-
ets between snd.una and snd.nxt may not have arrived at the receiver during the update
and this would quickly perform retransmissions. However, this implementation would
deadlock occasionally. Upon inspection, we found that this deadlock would occur when
rcv.nxt > snd.una. In this case, the receiver had acknowledged packets between snd.una
Chapter 4.
Implementation
32
and rcv.nxt, but these acknowledgments had been dropped during the update. As a re-
sult, the sending side would observe acknowledgments for packets that it believed that it
had not yet sent. Linux kernel versions 2.6.28 and 2.6.29 discard these future acknowl-
edgments and our TCP connection would make no further progress at this point (later
Linux versions accept future acknowledgments). To x this problem, we started sending
packets from the snd.nxt sequence number, which is the same packet that TCP would
have sent before the update. The next acknowledgment would trigger retransmissions if
packets had been lost during the update.
During normal operation, TCP relies on incoming acknowledgments to advance its
transmit window and increase the window size. However, if acknowledgments are dropped
during the update, it may take a long time before they are retransmitted again. The
long retransmission timeout at the receiver becomes an issue when the ow control or
the congestion control window is so small that it does not allow the sender to send any
packets. To jump start the sending process, we set these windows so that at least one
packet can be sent after the connection is restored. After this packet is received, the
receiver sends an acknowledgment in response, which resumes communication quickly.
We found that TCP throughput declines signicantly (by almost 40%) if the times-
tamp and selective acknowledgment extensions are disabled. So even though they are not
essential for reestablishing the connection, our implementation saves and restores both
the timestamp state and selective acknowledgment state as described below.
The timestamp extension allows TCP to estimate round trip time more accurately,