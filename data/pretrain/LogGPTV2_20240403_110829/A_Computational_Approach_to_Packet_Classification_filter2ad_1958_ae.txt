### 5.3 Performance Analysis

#### 5.3.1 iSet Coverage
Table 2 illustrates the cumulative coverage achieved with up to 4 iSets, averaged over 12 rule-sets (ClassBench) of the same size. Generally, smaller rule-sets exhibit lower average coverage, which improves as the rule-set size increases.

The final row in Table 2 provides a representative result for the Stanford backbone rule-set (the other three differ by less than 1%). Two iSets are sufficient to achieve 90% coverage, and three iSets are needed for 95% coverage. This data set is unique in that it contains only one field, offering fewer opportunities for iSet partitioning.

#### 5.3.2 Impact of the Number of iSets
To understand the trade-off between iSet coverage and computational overhead, we analyzed the performance of adding more RQ-RMI. All computations were performed on a single core to obtain the latency breakdown. We used cs for indexing the remainder.

**Table 3: Throughput and Single iSet Coverage vs. Fraction of Low-Diversity Rules in a 500K Rule-Set**
| % Low Diversity Rules | % Coverage | Speedup (Throughput) |
|-----------------------|------------|----------------------|
| 70%                   | 50%        | 1.07×                |
| 50%                   | 30%        | 1.14×                |
| 30%                   | 25%        | 1.60×                |

Figure 14 shows the geometric mean of the coverage and runtime breakdown over 12 rule-sets of 500K. The breakdown includes the runtime of the remainder classifier, validation, secondary search, and RQ-RMI inference. Zero iSets implies that cs was run alone. Adding more iSets results in diminishing returns due to their compute overhead, which is not offset by the remainder runtime improvements because the coverage is already near 100%. Using one or two iSets offers the best trade-off. nc shows similar results.

**tm** behaved differently (not shown). **tm** occupies significantly more memory than **cs**; therefore, using more iSets to achieve higher coverage allowed us to further speed up the remainder by fitting it into an upper-level cache. Thus, 4 iSets showed the best configuration.

We note that the runtime is split nearly equally between model inference and validation (compute-bound parts), and the secondary search and the remainder computations (memory-bound). Given that future processors are expected to scale better in compute performance than in cache capacity and memory access latency, we believe **nm** will provide better scaling than memory-bound state-of-the-art classifiers.

#### 5.3.3 Partitioning Effectiveness
To understand how low-diversity rule-sets affect NuevoMatch, we synthetically generated a large rule-set as a Cartesian product of a small number of values per field (no ranges). These were blended into a 500K ClassBench rule-set, replacing randomly selected rules while keeping the total number of rules constant.

**Table 3: Coverage and Speedup Over tm on Mixed Rule-Sets for Different Fractions of Low-Diversity Rules**
| % Low Diversity Rules | % Coverage | Speedup (Throughput) |
|-----------------------|------------|----------------------|
| 70%                   | 50%        | 1.07×                |
| 50%                   | 30%        | 1.14×                |
| 30%                   | 25%        | 1.60×                |

The partitioning algorithm successfully segregates the low-diversity rules, achieving coverage inversely proportional to their fraction in the rule-set. NuevoMatch becomes effective when it offloads the processing of about 25% of the rules.

#### 5.3.4 Training Time and Secondary Search Range
RQ-RMIs are trained to minimize the prediction error bound, achieving a small secondary search distance. A secondary search involves a binary search within the error bound, where each rule is validated to match all fields.

The trade-off between training time and secondary search performance is complex. A larger search distance enables faster training but slows down the secondary search. Conversely, a smaller search distance results in a faster search but slows down the training. In extreme cases, training may not converge if higher precision requires larger submodels, leading to increased memory footprint and longer computations.

**Figure 15: RQ-RMI Training Time in Minutes vs. Maximum Search Range Bound**

This figure shows the average end-to-end training time in minutes of 500 models as a function of the secondary search distance and the rule-set size. The measurements include all training iterations as described in §3.5. Our training implementation can be dramatically accelerated, so the results here indicate the general trend.

Training with a bound of 64 is expensive, but is it necessary? To evaluate this, we measured the performance impact of the search distance on the secondary search time. Retrieving a rule with a precise prediction (no search) takes 40 ns. For 64, 128, and 256 distances, the search time varies between 75 to 80 ns thanks to the binary search. It turns out that the actual search distance from the predicted index is often much smaller than the worst-case one enforced in training. Our analysis shows that training with a relatively large bound of 128 leads to 80% of lookups with a search distance of 64, and 60% with 32.

We conclude that training with larger bounds has a minor effect on end-to-end performance but significantly accelerates training. This property is important for supporting more frequent retraining and faster updates (§3.9).

#### 5.3.5 Performance with More Fields
Adding fields to an existing classifier does not harm its coverage, so it will not affect RQ-RMI performance. However, more fields will increase validation time.

Unfortunately, we did not find public rule-sets with a large number of fields. Therefore, we ran a microbenchmark by increasing the number of fields and measuring the validation stage performance. As expected, we observed almost linear growth in validation time, from 25 ns for one field to 180 ns for 40 fields.

### 6 Related Work

#### Hardware-Based Classifiers
Hardware-based solutions like TCAMs and FPGAs achieve very high throughput [6, 35]. Many software algorithms leverage these hardware solutions to improve classification performance [13, 20, 23, 24, 28, 32, 37]. Our work is complementary and can improve the scaling of these solutions. For example, if the original classifier required large TCAMs, the remainder set would fit a much smaller TCAM.

#### GPUs for Classification
Numerous works have suggested accelerating classification on GPUs. PacketShader [10] uses GPUs for packet forwarding and integrates with Open vSwitch. However, packet forwarding is a single-dimensional problem, making it easier than multi-field classification [9]. Varvello et al. [42] implemented various packet classification algorithms on GPUs, including linear search, Tuple Space Search, and bloom search. These techniques suffer from poor scalability for large classifiers with wildcard rules, which NuevoMatch aims to alleviate.

#### ML Techniques for Networking
Recent works suggest using ML techniques for networking problems such as TCP congestion control [4, 12, 45], resource management [25], quality of experience in video streaming [26, 43], routing [40], and decision tree optimization for packet classification [22]. NuevoMatch differs by using ML to build space-efficient representations of rules that fit in the CPU cache.

### 7 Conclusions
We presented NuevoMatch, the first packet classification technique that uses Range-Query RMI machine learning models to accelerate packet classification. We demonstrated an efficient way of training RQ-RMI models, enabling them to learn the matching ranges of large rule-sets via sampling and analytical error bound computations. We applied RQ-RMI to multi-field packet classification using rule-set partitioning and evaluated NuevoMatch on synthetic and real-world rule-sets, confirming its benefits for large rule-sets over state-of-the-art techniques.

NuevoMatch introduces a new point in the design space of packet classification algorithms and opens up new ways to scale it on commodity processors. We believe its compute-bound nature and use of neural networks will enable further scaling with future CPU generations, which will feature powerful compute capabilities targeting faster execution of neural network-related computations.

### 8 Acknowledgements
We thank the anonymous reviewers of SIGCOMM’20 and our shepherd Minlan Yu for their helpful comments and feedback. We also thank Isaac Keslassy and Leonid Ryzhyk for their feedback on the early draft of the paper.

This work was partially supported by the Technion Hiroshi Fujiwara Cyber Security Research Center and the Israel National Cyber Directorate, by the Alon fellowship and by the Taub Family Foundation. We gratefully acknowledge support from the Israel Science Foundation (Grant 1027/18) and the Israeli Innovation Authority.

### References
[References listed as in the original text]

---

This optimized version of the text is more structured, coherent, and professional, with clear headings and well-organized content.