500K
20 .2 ± 18 .6
45 .1 ± 31 .6
80 .0 ± 14 .5
84 .2 ± 10 .5
28 .9 ± 22 .3
59 .6 ± 38 .9
96 .5 ± 8 .3
98 .8 ± 1 .5
34 .6 ± 25 .6
62 .6 ± 37 .1
98 .1 ± 4 .8
99 .4 ± 0 .6
38 .7 ± 27 .2
65 .1 ± 35 .7
98 .8 ± 2 .7
99 .7 ± 0 .2
183,376
57.8
91.6
96.5
98.2
)
s
n
(
e
m
T
i
1,000
800
600
400
200
0
)
%
(
e
g
a
r
e
v
o
C
0
1
2
3
4
5
6
Number of iSets
Remainder
Inference
Secondary Search
Coverage
Validation
Figure 14: Coverage and execution time breakdown of
NuevoMatch vs. varying number of iSets.
5.3 Performance analysis
5.3.1
iSet coverage. Table 2 shows the cumulative coverage
achieved with up to 4 iSets averaged over 12 rule-sets (ClassBench)
of the same size. The coverage of smaller rule-sets is worse on av-
erage, but improves with the size of the rule-set.
The last row shows a representative result for the Stanford back-
bone rule-set (the other three diﬀer within 1%). Two iSets are
enough to achieve 90% coverage and three are needed for 95%. This
data set diﬀers from ClassBench in that it contains only one ﬁeld,
providing fewer opportunities for iSet partitioning.
5.3.2
Impact of the number of iSets. We seek to understand the
tradeoﬀ between the iSet coverage of the rule-set and the computa-
tional overheads of adding more RQ-RMI. All computations were
performed on a single core to obtain the latency breakdown. We
use cs for indexing the remainder.
Table 3: Throughput and a single iSet coverage vs. the frac-
tion of low-diversity rules in a 500K rule-set.
% Low diversity rules % Coverage
Speedup (throughput)
70%
50%
30%
25%
50%
70%
1.07×
1.14×
1.60×
Figure 14 shows the geometric mean of the coverage and the
runtime breakdown over 12 rule-sets of 500K. The breakdown
includes the runtime of the remainder classiﬁer, validation, sec-
ondary search, and RQ-RMI inference. Zero iSets implies that cs
was run alone. Adding more iSets shows diminishing returns be-
cause of their compute overhead, which is not compensated by the
remainder runtime improvements because the coverage is already
saturated to almost 100%. Using one or two iSets shows the best
trade-oﬀ. nc shows similar results.
tm behaved diﬀerently (not shown). tm occupies much more
memory than cs; therefore, using more iSets to achieve higher cov-
erage allowed us to further speed up the remainder by ﬁtting it into
an upper level cache. Thus, 4 iSets showed the best conﬁguration.
We note that the runtime is split nearly equally between model
inference and validation (which are compute-bound parts), and
the secondary search and the remainder computations (which are
memory-bound). We expect the compute performance of future
processors to scale better than their cache capacity and memory
access latency. Therefore, we believe nm will provide better scal-
ing than memory-bound state-of-the-art classiﬁers.
5.3.3 Partitioning eﬀectiveness. We seek to understand how
low diversity rule-sets aﬀect NuevoMatch. To analyze that, we syn-
thetically generated a large rule-set as a Cartesian product of a
small number of values per ﬁeld (no ranges). We blended them into
a 500K ClassBench rule-set, replacing randomly selected rules with
those from the Cartesian product, while keeping the total number
of rules the same.
Table 3 shows the coverage and the speedup over tm on the
resulting mixed rule-sets for diﬀerent fractions of low-diversity
rules. The partitioning algorithm successfully segregates the low-
diversity rules the best, achieving the coverage inversely propor-
tional to their fraction in the rule-set. Note that NuevoMatch be-
comes eﬀective when it oﬄoads the processing of about 25% of the
rules.
5.3.4 Training time and secondary search range. RQ-RMIs are
trained to minimize the prediction error bound to achieve a small
secondary search distance. Recall that a secondary search involves
a binary search within the error bound, where each rule is vali-
dated to match all the ﬁelds.
The tradeoﬀ between training time and secondary search perfor-
mance is not trivial. A larger search distance enables faster train-
ing but slows down the secondary search. A smaller search dis-
tance results in a faster search but slows down the training. In ex-
treme cases, the training does not converge, since a higher preci-
sion might require larger submodels. However, increasing the size
of the submodels leads to a larger memory footprint and longer
computations.
i
g
n
n
i
a
r
T
e
m
T
i
)
s
e
t
u
n
m
i
(
40
30
20
10
0
64
500K rules
100K rules
10K rules
128
256
512
1024
Search Distance Bounds
Figure 15: RQ-RMI training time in minutes vs. maximum
search range bound.
Figure 15 shows the average end-to-end training time in min-
utes of 500 models as a function of the secondary search distance
and the rule-set size. The measurements include all training itera-
tions as described in §3.5. As mentioned (§4), our training imple-
mentation can be dramatically accelerated, so the results here in-
dicate the general trend.
Training with the bound of 64 is expensive, but is it really neces-
sary? To answer, we evaluate the performance impact of the search
distance on the secondary search time. We measure 40 ˙ns for re-
trieving a rule with a precise prediction (no search). For 64, 128 and
256 distances the search time varies between 75 to 80 ˙ns thanks to
the binary search. Last, it turns out that the actual search distance
from the predicted index is often much smaller than the worst-case
one enforced in training. Our analysis shows that in practice, train-
ing with a relatively large bound of 128 leads to 80% of the lookups
with a search distance of 64, and 60% with 32.
We conclude that training with larger bounds is likely to have
a minor eﬀect on the end-to-end performance, but signiﬁcantly ac-
celerate training. This property is important to support more fre-
quent retraining and faster updates (§3.9).
5.3.5 Performance with more fields. Adding ﬁelds to an existing
classiﬁer will not harm its coverage, so it will not aﬀect the RQ-
RMI performance. Nonetheless, more ﬁelds will increase validation
time.
Unfortunately, we did not ﬁnd public rule-sets that have a large
number of ﬁelds. Thus, we ran a microbenchmark by increasing
the number of ﬁelds and measuring the validation stage perfor-
mance. As expected, we observed almost linear growth in the val-
idation time, from 25ns for one ﬁeld to 180ns for 40 ﬁelds.
6 RELATED WORK
Hardware-based classiﬁers. Hardware-based solutions for clas-
siﬁcation such as TCAMs and FPGAs achieve a very high through-
put
[6, 35]. Consequently, many software algorithms take ad-
vantage of them, further improving classiﬁcation performance
[13, 20, 23, 24, 28, 32, 37]. Our work is complementary, but can
be used to improve scaling of these solutions. For example, if the
original classiﬁer required large TCAMs, the remainder set would
ﬁt a much smaller TCAM.
GPUs for classiﬁcation. Accelerating classiﬁcation on GPUs was
suggested by numerous works. PacketShader [10] uses GPU for
packet forwarding and provides integration with Open vSwitch.
However, packet forwarding is a single-dimensional problem, so it
is easier than multi-ﬁeld classiﬁcation [9]. Varvello et al. [42] imple-
mented various packet classiﬁcation algorithms in GPUs, includ-
ing linear search, Tuple Space Search, and bloom search. Nonethe-
less, these techniques suﬀer from poor scalability for large classi-
ﬁers with wildcard rules, which NuevoMatch aims to alleviate.
ML techniques for networking. Recent works suggest using ML
techniques for solving networking problems, such as TCP conges-
tion control [4, 12, 45], resource management [25], quality of expe-
rience in video streaming [26, 43], routing [40], and decision tree
optimization for packet classiﬁcation [22]. NuevoMatch is diﬀerent
in that it uses an ML technique for building space-eﬃcient repre-
sentations of the rules that ﬁt in the CPU cache.
7 CONCLUSIONS
We have presented NuevoMatch, the ﬁrst packet classiﬁcation tech-
nique that uses Range-Query RMI machine learning model for ac-
celerating packet classiﬁcation. We have shown an eﬃcient way of
training RQ-RMI models, making them learn the matching ranges
of large rule-sets, via sampling and analytical error bound com-
putations. We demonstrated the application of RQ-RMI to multi-
ﬁeld packet classiﬁcation using rule-set partitioning. We evaluated
NuevoMatch on synthetic and real-world rule-sets and conﬁrmed
its beneﬁts for large rule-sets over state-of-the-art techniques.
NuevoMatch introduces a new point in the design space of
packet classiﬁcation algorithms and opens up new ways to scale
it on commodity processors. We believe that its compute-bound
nature and the use of neural networks will enable further scaling
with future CPU generations, which will feature powerful compute
capabilities targeting faster execution of neural network-related
computations.
8 ACKNOWLEDGEMENTS
We thank the anonymous reviewers of SIGCOMM’20 and our shep-
herd Minlan Yu for their helpful comments and feedback. We
would also like to thank Isaac Keslassy and Leonid Ryzhyk for their
feedback on the early draft of the paper.
This work was partially supported by the Technion Hiroshi Fu-
jiwara Cyber Security Research Center and the Israel National Cy-
ber Directorate, by the Alon fellowship and by the Taub Family
Foundation. We gratefully acknowledge support from Israel Sci-
ence Foundation (Grant 1027/18) and Israeli Innovation Authority.
REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, Man-
junath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine
Learning. In USENIX OSDI.
[2] CAIDA. [n.d.]. The CAIDA UCSD Anonymized Internet Traces 2019. Retrieved
June 15, 2020 from http://www.caida.org/data/passive/passive_dataset.xml
[3] James Daly, Valerio Bruschi, Leonardo Linguaglossa, Salvatore Pontarelli, Dario
Rossi, Jerome Tollet, Eric Torng, and Andrew Yourtchenko. 2019. TupleMerge:
Fast Software Packet Processing for online Packet Classiﬁcation.
IEEE/ACM
Transactions on Networking (TON) 27, 4 (2019), 1417–1431.
[4] Mo Dong, Tong Meng, Doron Zarchy, Engin Arslan, Yossi Gilad, Brighten God-
frey, and Michael Schapira. 2018. PCC Vivace: Online-Learning Congestion Con-
trol. In USENIX NSDI.
[5] Daniel Firestone. 2017. VFP: A Virtual Switch Platform for Host SDN in the
Public Cloud. In USENIX NSDI.
[6] Daniel Firestone, Andrew Putnam, Sambrama Mundkur, Derek Chiou, Alireza
Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian M. Caulﬁeld,
Eric S. Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt
Humphrey, Jack Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye,
Gautham Popuri, Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Mad-
han Sivakumar, Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak
Bansal, Doug Burger, Kushagra Vaid, David A. Maltz, and Albert G. Greenberg.
2018. Azure Accelerated Networking: SmartNICs in the Public Cloud. In USENIX
NSDI.
[7] Pankaj Gupta and Nick McKeown. 1999. Packet Classiﬁcation on Multiple Fields.
In ACM SIGCOMM.
[8] Pankaj Gupta and Nick McKeown. 2000. Classifying Packets with Hierarchical
Intelligent Cuttings. IEEE Micro 20, 1 (2000), 34–41.
[9] Pankaj Gupta and Nick McKeown. 2001. Algorithms for Packet Classiﬁcation.
IEEE Network 15, 2 (2001), 24–32.
[10] Sangjin Han, Keon Jang, KyoungSoo Park, and Sue Moon. 2010. PacketShader:
A GPU-accelerated software router. In ACM SIGCOMM.
[11] Intel. 2019. Intel Nervana Neural Network Processors. Retrieved September 25,
2019 from https://www.intel.ai/nervana-nnp/
[12] Nathan Jay, Noga H. Rotman, Philip Brighten Godfrey, Michael Schapira, and
Aviv Tamar. 2018. Internet Congestion Control via Deep Reinforcement Learn-
ing. arXiv preprint arXiv:1810.03259 (2018).
[13] Naga Praveen Katta, Omid Alipourfard, Jennifer Rexford, and David Walker.
2016. CacheFlow: Dependency-Aware Rule-Caching for Software-Deﬁned Net-
works. In ACM SOSR.
[14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-
mization. arXiv:1412.6980 (2014).
[15] Jon M. Kleinberg and Éva Tardos. 2006. Algorithm Design. Addison-Wesley,
116–125.
[16] Kirill Kogan, Sergey Nikolenko, Ori Rottenstreich, William Culhane, and Patrick
Eugster. 2014. SAX-PAC (Scalable and expressive packet classiﬁcation). In ACM
SIGCOMM.
[17] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Jialin Ding, Ani
Kristo, Guillaume Leclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan.
2019. SageDB: A Learned Database System.
[18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neoklis Polyzotis. 2018.
The Case for Learned Index Structures. In ACM SIGMOD.
[19] Habana Labs. 2019. Habana AI Processors. Retrieved September 25, 2019 from
https://habana.ai/product
[20] Karthik Lakshminarayanan, Anand Rangarajan, and Srinivasan Venkatachary.
2005. Algorithms for Advanced Packet Classiﬁcation with Ternary CAMs. In
ACM SIGCOMM.
[21] Wenjun Li, Xianfeng Li, Hui Li, and Gaogang Xie. 2018. CutSplit: A Decision-
Tree Combining Cutting and Splitting for Scalable Packet Classiﬁcation. In IEEE
INFOCOM.
[22] Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. 2019. Neural Packet Classiﬁcation.
In ACM SIGCOMM.
[23] Alex X Liu, Chad R Meiners, and Yun Zhou. 2008. All-Match Based Complete
Redundancy Removal for Packet Classiﬁers in TCAMs. In IEEE INFOCOM.
[24] Yadi Ma and Suman Banerjee. 2012. A Smart Pre-classiﬁer to Reduce Power
Consumption of TCAMs for Multi-dimensional Packet Classiﬁcation. In ACM
SIGCOMM.
[25] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016.
Resource Management with Deep Reinforcement Learning. In ACM SIGCOMM
HotNets Workshop.
[26] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural Adaptive
Video Streaming with Pensieve. In ACM SIGCOMM.
[27] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Pe-
terson, Jennifer Rexford, Scott Shenker, and Jonathan Turner. 2008. OpenFlow:
Enabling Innovation in Campus Networks. ACM SIGCOMM CCR 38, 2 (2008),
69–74.
[28] Nina Narodytska, Leonid Ryzhyk, Igor Ganichev, and Soner Sevinc. 2019. BDD-
Based Algorithms for Packet Classiﬁcation. In Formal Methods in Computer
Aided Design FMCAD.
[29] Nvidia.
2019.
Nvidia
Retrieved
Plat-
form.
from
https://www.nvidia.com/en-us/deep-learning-ai/solutions/inference-platform/