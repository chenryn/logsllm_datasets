Next, we compute the cosine similarity between the vectors
of 1) detected troll and known troll accounts and 2) non-troll
and known troll accounts. Table II shows the language simi-
larity for the top keywords calculated in the previous section.
As it can be seen, for all words, detected troll accounts have a
higher similarity than undetected accounts when compared to
the language used by known trolls. We also perform a z-test to
show that known trolls use similar language as detected trolls
while the use of language by known trolls is different from the
one of non-trolls. To calculate the z-score, we use the cosine
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2167
similarities from each row of Table II as proportion, where
the population size is the number of messages containing a
certain word. Our results show that for three keywords (i.e.,
“people,” “money,” and “crypto”) the differences in language
show statistically signiﬁcant differences at the p < 0.05 level.
In the remaining cases, although the cosine similarity between
detected trolls and known trolls is higher than the one with
undetected accounts, the test is inconclusive, likely due to the
limited sample size.
Deep-Diving on the Keyword “Crypto”. To better illustrate
the difference and similarity in language used by the different
types of accounts we focus on the language surrounding
the keyword “crypto,” since the z-score returned statistically
signiﬁcant results for it, the topic of cryptocurrencies is of
great interest to the computer security community and under-
standing how state-sponsored operations attempt to inﬂuence
the ﬁeld is an unstudied area. The word “crypto” appears in
817 comments made by detected troll accounts and in 132
comments by known troll accounts. Also, there are 50 detected
troll accounts and 16 known troll accounts with comments
containing the word “crypto.”
Visualization. To visualize the language used in relation to our
keywords, we follow the methodology proposed by Zannettou
et al. [76]. Figure 3 present the graphs calculated from the
word “crypto.” Nodes are words and are connected by an
edge if the cosine similarity of their embedding vectors is
above a given threshold. The threshold for known troll and
detected troll accounts is set to 0.9, whereas for the undetected
accounts, the threshold is set to 0.68. These thresholds are
selected to keep approximately 100 nodes in each graph. We
chose 100 as the number of nodes to: 1) ensure consistency
with the word embedding analysis where we compared vectors
of Top-100 most similar words, and 2) have a reasonable
number of nodes for visualization. The graph is built from
the trained word2vec model and only nodes within two hops
from the keyword are included.
To visualize the graphs, we perform a number of steps. First,
we construct a weighted graph using the ForceAtlas2 layout
algorithm [25] where words with higher cosine similarities are
laid out closer in the graph space. Figure 3(a) shows the word
embedding graph for troll accounts and the words with a larger
font are common with detected troll accounts, Figure 3(b)
those of detected troll accounts and the highlighted words
are common with known troll accounts, while Figure 3(c)
shows the word embeddings for undetected accounts and the
highlighted words are common with known troll accounts.
Language Used. The graphs indicate that the language used by
detected troll accounts is indeed closer to that used by known
troll accounts. For “crypto,” the graph for the detected troll
accounts (Figure3(b)) has 33 words in common with known
troll accounts whereas the non-troll accounts one (Figure 3(c))
only has two (i.e. bitcoin and crypto).
As an additional indicator, we run the Louvain community
detection algorithm [5] on the graph, to identify “communi-
ties” of similar words. Words belonging to the same com-
munity are depicted with the same color. From Figure 3,
we observe that non-troll accounts discuss cryptocurrencies in
general, covering a wide variety of coins. On the other hand,
known troll and detected troll accounts talk about bitcoin in
particular and use more informal language.
Comment Examples. To further illustrate the differences in
language and topics covered by known, detected troll, and
non-detected accounts, we discuss a few, manually selected,
comments containing the word “crypto” by each class of
accounts. First, we look at three comments made by known
troll accounts containing the word “crypto.”
COMMENT 1: All my family members are trading
crypto
COMMENT 2: I feel like the reporter had an anti crypto
vibe to her.
COMMENT 3: Crypto is down another 5% since this
news broke. Fuck this gay Earth.
All comments take a pro-crypto stance and advocate the
trading of cryptocurrencies. They also seem invested in cryp-
tocurrencies and discredit a reporter if they are critical of
cryptocurrencies (in Comment 2) or show anger if their price
drops (in Comment 3).
The following comments are made by detected troll ac-
counts and similar to the known trolls, they take a strong
pro-crypto stance.
COMMENT 1: No we need to destroy them. no one
bashes crypto currencys.
COMMENT 2: I’m just living in Crypto 24/7. Everything
is all-right ...
Finally, the comments shown below are made by accounts
that were not detected as troll accounts by TROLLMAGNIFIER.
Contrary to the troll and detected accounts,
they express
frustration towards cryptocurrencies and an anti-crypto stance.
COMMENT 1: God am I sick of all these small little
start up based crypto coins.
COMMENT 2: Just be careful, no guarantees in crypto!
COMMENT 3: All crypto currencies will go to zero
eventually.
Time Series Evaluation. Another crucial veriﬁcation step is a
time series analysis of the activity of accounts. Our intuition is
that troll accounts will show a time synchronization that is not
evident for regular accounts. To conﬁrm this, we plot the time
series of known troll accounts, detected troll, and accounts
labeled as non-troll accounts by TROLLMAGNIFIER.
Figure 4(a) shows the time series plot for the comments.
The activity of detected accounts is much closer to known
troll accounts as compared to the non-troll accounts, especially
the peaks in 2018. Figure 4(b) shows the time series plot for
submissions. Similar to the comments activity, detected troll
and known troll accounts show a coordination pattern.
Next, we compute the Pearson correlation and lag for both
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2168
(a) Known trolls. Larger nodes represent words
common with (b).
(b) Detected trolls. Larger nodes represent
words common with (a).
(c) Non-trolls. Larger nodes (“crypto” and “bitcoin”) are
common with (a).
Fig. 3: A visualization of language usage in relation to the keyword “crypto” where nodes from the same community (detected
using the Louvain community detection method [5]) are depicted with the same color. It is evident that known trolls and trolls
detected by TROLLMAGNIFIER have more words in common than known trolls and non-trolls.
(a) Comments
(b) Submissions
Fig. 4: Time Series of Comments and Submissions.
time series to check the degree of similarity in the activities.
For submissions, the correlation between detected troll and
known troll accounts is 0.5, compared to the 0.068 between
undetected accounts and known troll accounts. Similarly, for
the ﬁrst group, the lag is -23 and it is -99 for the other. In the
case of comments, the correlation for the ﬁrst group is 0.553
and 0.334 for the negative group. The lag is 93 for the ﬁrst
group and 173 for the other, showing clearly that the detected
troll accounts are much more similar to known troll accounts
than undetected accounts are.
To improve our validation, we repeat this experiment only
considering the 424 accounts for which no individual account
indicator was satisﬁed (in Section V-E), and the same ﬁndings
still hold (0.49 correlation for submissions and 0.54 for
comments, with the group lag remaining unchanged). This
indicates that, although those accounts could not be conﬁrmed
individually, they present activity patterns as a group that are
close to the known troll accounts.
G. Summary.
When looking at the set of detected accounts as a group,
we ﬁnd that detected troll accounts push similar narratives, use
similar language to known troll accounts, and show a higher
degree of synchronization compared to accounts that are not
detected by TROLLMAGNIFIER.
H. Estimating the False Negative Rate
To estimate TROLLMAGNIFIER’s false negatives, we per-
form a qualitative analysis where we randomly pick unde-
tected Reddit accounts and check for signs that might be
indicative of them being trolls. We manually annotate a set
of 20 random accounts from the set labeled as non-trolls by
TROLLMAGNIFIER. Two authors of this paper independently
assessed the 20 accounts looking for inﬂammatory, insincere,
digressive, extraneous, or off-topic messages. Note that, while
posting this kind of messages is not an ultimate indicator that
an account is a troll, this allows us to establish an upper
bound for TROLLMAGNIFIER’s false negative rate. Annotator
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2169
cryptoonethinkmuchlikegoingstillwantgoodwayseenewalsogotimeusbetterwoulduserightgreatworldgetmakeevencasomethingneedcouldalreadysaidgovernmentthoughtsomeonegtanythingbackmoneyneverfindwellloveoldsincereadreallyanotherthingdaymaybemanypersonyearsmarketlookthingsfirstseemsfeeletcpartnewstrumpbigcountrybitcoinkeepworkalwayspointsaynothinglotknowtakedifferentactuallytwohttpsbestgotagreeusingmaysureguyyeteverythingprettybelieveespeciallymadepolicemightbecomelongsocialanyonearoundblackreallastmeanproblemeverycopslifesystemchildrenreasonbadcausebuystopmanfactwithoutlittlepostmattercryptofactunderstandpointstoryusedbitcoinstopcompletelytalkingusingmakestechopinionrelationshipsomeonepowerwithoutgrouprealalmostcalledmakingeasymayespeciallyendsimilaraccountrunningseereasonmarketsableexactlyusdifferentfullcaseshotblackfuturebuiltlikelycuntworkstakingcocklowerneedsresultlettradingmovehistorycoinssupportthatsreadshowleftworldwentmustlinefirstwithinnothingtryingpartyetfarmeanhardknowwellleastlifecouldwholefreecomeschangetrygivehighcomestartonebadseemactuallyanothereithersayingideabackstuffmaybemomentcryptocryptosbitcoinethbtcxrpblockchainshitcoinlitecoinmoneroshitcoinsbchbitcoinsicosnanotetherblockchainsusdtxmrxlmcossdappsstablecoinsbitfinexxtrabyteshodlingxvgstablecointrxbitsharesdigibyteraiblockssatoshibittrexvenprlbullrunicxbtcpgvtathmainnetxbyzencashbtcseosstakingvitalikrippledyorskycoinelastosliskxrapidlndecentraliseddapptestnetdecentralizedbtrashbitconnectblockstreambagholdersbuttcoinbitmainpreminedhodlfuddersfuddingsmartcashbytecoinjihanbcashvertcoinxrbantsharesasicsetnqtumfeelessbntylitecoinszecetheriumvtcsegwitlitepaysatoshiszrxsteemhuobizcashpivxmarketcapiotacswtkyfunfairtetherszksnarkstrustlessscalabilitydposdecentralizationledgerspermissionedcentralised02/1205/1208/1211/1202/1305/1308/1311/1302/1405/1408/1411/1402/1505/1508/1511/1502/1605/1608/1611/1602/1705/1708/1711/1702/1805/1808/18012345% of postsDetected TrollsKnown TrollsUndetected Accounts02/1205/1208/1211/1202/1305/1308/1311/1302/1405/1408/1411/1402/1505/1508/1511/1502/1605/1608/1611/1602/1705/1708/1711/1702/1805/1808/180.00.51.01.52.02.53.0% of postsDetected TrollsKnown TrollsUndetected Accounts1 labeled 2 accounts as trolls whereas Annotator 2 labeled 1
account as a troll. The account labeled as troll by Annotator
2 was also labeled as troll by Annotator 1, yielding an inter-
coder agreement of 95% and Cohen’s Kappa score of 0.64
(i.e., substantial agreement). Therefore, with 2 out of the 20
accounts labeled as trolls by at least one annotator, we estimate
that the false negative rate of TROLLMAGNIFIER is 10%.
I. Measuring the Reach of Troll Accounts
An important question when studying troll operations relates
to the impact they have on the platforms that they are active
on, and on their users. To this end, we now look at whether
the submissions and comments by troll accounts receive more
engagement than those by undetected accounts in our dataset.
Ideally, we would like to measure how many Reddit users saw
a certain post, but unfortunately our dataset does not contain
that information. Instead, we use the score, namely, the number
of upvotes minus the number of downvotes, as a proxy.
We ﬁnd that the troll accounts detected by TROLLMAG-
NIFIER have made 26,984 comments with a total score of
153,839. The average score per comment is 5.7, which is
higher than the one for comments posted by the other accounts
in our dataset i.e., 4.8. This suggests that troll accounts receive
more engagement than regular accounts on Reddit, opening
up interesting research questions to be further investigated in
future work.
J. Run-Time Performance
Last but not least, we discuss the scalability of running
TROLLMAGNIFIER on a large social network platform. We
tested TROLLMAGNIFIER on a server with two 12-core In-
tel(R) Xeon(R) Gold 6126 @ 2.60GHz CPUs and 768GB ram.
When testing the runtime performance of TROLLMAGNIFIER
in completing the various steps of the analysis pipeline, we
found that while most steps are completed quickly, retrieving
data from the Pushshift API is a signiﬁcant bottleneck. It
currently takes TROLLMAGNIFIER, on average, 226 seconds
to retrieve data for an account from the Pushshift API, 0.15
seconds to extract the features for our classiﬁer, and 0.005
seconds to perform the actual classiﬁcation. Taking aside
data extraction, it takes TROLLMAGNIFIER 2.5 seconds to
train our model. We expect
time to go
down signiﬁcantly when deployed in production by a large
company with a powerful infrastructure such as Reddit. Since
performing detection for an account can be done independently
from other detection tasks, this operation could be parallelized
for as many accounts as needed (e.g., all the accounts that
interacted with a known troll in a given day). We envision
a batch deployment in the wild, where the troll detection
algorithm is run at ﬁxed intervals, for example once a day, in a
similar fashion as proposed by state-of-the-art social network
abuse systems [8, 50].
the data retrieval
VI. CASE STUDIES
In this section, we present and discuss two case studies
selected from the accounts identiﬁed by TROLLMAGNIFIER
Fig. 5: An example of manufactured conﬂict between known
trolls and accounts detected by TROLLMAGNIFIER.
as troll accounts. We do so to shed light on the modus
operandi of the troll accounts, both with respect to spreading
disinformation/polarizing online discussion and pretending to
be real users by posting harmless content.
Case Study 1: Manufactured conﬂict. Former operatives of
Russian troll operations have explained that disinformation ac-
tors often worked in groups to polarize online discussions [57].
For instance, they would have an account post a message, and
other accounts vehemently disagree with it, aiming to attract
real users and further polarize the discussion on that subreddit.
In our work, we observed similar instances of trolls “team-
ing up” and posting in groups of 2-3 accounts on the same
submission, often replying to each other. Figure 5 shows one
such case of collaboration on the subreddit r/worldnews.
This is a snippet taken from a 5-year old submission that was
discussing then-President Obama siding with Turkey during
tensions with Russia. There are ﬁve accounts participating in
this thread, two of which are of interest to us: men_like,
a known troll account identiﬁed by Reddit and an alleged
troll account detected by TROLLMAGNIFIER, which we will
refer to as troll1 for privacy reasons. There is also a third
account that was deleted by its owner (marked as [deleted]
in Figure 5). In the comment thread, troll1 ﬁrst mentions
an incident to shame the Turkish military. Then, men_like
takes it further by suggesting that Turkish people are not to
be trusted at all. Finally, the now-deleted account accuses
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2170
troll1men_like of racism.
This is a textbook example of a manufactured controversy
on Reddit, designed to push a certain narrative with the goal
of inﬂuencing the real users on that Subreddit.
Case Study 2: Simulating Legitimate Activity. Another goal
of the trolls is to appear legitimate to other users, as well as
to Reddit itself, raising suspicion. To this end, it makes sense
for troll accounts to post content that is unrelated to their
“primary purpose.” In our dataset, we ﬁnd similarities in the
type of “benign” content being posted by several accounts.
An example is presented in Figure 6: the left-most picture
is posted by one of the known troll accounts identiﬁed by
Reddit, while the other two are posted by two troll accounts
detected by TROLLMAGNIFIER. These three accounts post
amusing pictures of dogs to look more “legitimate” and blend
into the Reddit community. Similarly, we ﬁnd 36 accounts that
participate in the r/Jokes subreddit and post funny memes.
The posting of general purpose and seemingly irrelevant
content/images is likely an attempt by the trolls to accumulate
karma score on their account and pose as legitimate users,
hence decreasing the likelihood of their detection.
VII. DISCUSSION
In this section, we discuss important implications of our
results for social media platforms. We also outline the ways in
which attackers can leverage this information to their beneﬁt.
Finally, we highlight some limitations of our study along with
possible future directions.
A. Implications for Social Media Platforms
Disinformation on social media has become one of the
most pressing issues in modern society, and is at the forefront
of trust and safety initiatives across essentially all popular
platforms. While related to the well-explored area of social