Fscore Targets
0.5
0.75
0.9
AC
AR
Detection Techniques
IC
NC NCAC NCAR ICAC ICAR
)
%
(
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
35
30
25
20
15
Dense Oracle D−Tree
Fault Models
1
2
3
4
5
6
AC
AR
Detection Techniques
IC
NC NCAC NCAR ICAC ICAR
)
%
(
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
30
25
20
15
10
5
0
Dense Oracle D−Tree
AC
AR
NC
Detection Techniques
IC
NCAC NCAR ICAC ICAR
Fig. 11. Performance overheads with varying F-Score targets (left), Fault Models (center), and Fault Rates (right).
Varying Fscore (faultRate=1e−3, faultModel=1)
Varying FaultModel (faultRate=1e−3, fscore>0.9)
Varying faultRate (fscore>0.9, faultModel=1)
)
%
(
t
e
g
r
a
T
g
n
i
t
e
e
m
s
e
c
i
r
t
a
M
f
o
#
80
60
40
20
Dense Oracle D−Tree
Fscore Targets
0.5
0.75
0.9
80
60
40
20
)
%
(
t
e
g
r
a
T
g
n
i
t
e
e
m
s
e
c
i
r
t
a
M
f
o
#
Dense Oracle D−Tree
Fault Models
1
2
3
4
5
6
)
%
(
t
e
g
r
a
T
g
n
i
t
e
e
m
s
e
c
i
r
t
a
M
f
o
#
80
60
40
20
0
Dense Oracle D−Tree
AC
AR
NC
Detection Techniques
IC
NCAC NCAR ICAC ICAR
AC
AR
Detection Techniques
IC
NC NCAC NCAR ICAC ICAR
AC
AR
Detection Techniques
IC
NC NCAC NCAR ICAC ICAR
Fig. 12. The success rate or frequency at which problems meet the given F-Score target with varying F-Score targets (left), Fault Models (center), and Fault
Rates (right).
no beneﬁt with preconditioning ( 2x). Of the 5 problems solved
successfully with CG-pre, only 1 of those same problems met
the accuracy target successfully with IR-pre.
The results also show that in the context of linear solvers the
dense checks can have fairly large performance overheads (30-
50%). For CG, the sparse check based implementation spent
17% less time in MVM operations on average than the tradi-
tional dense check-based implementations. This corresponds
to a total execution time that is 9% lower on average. For IR,
the sparse check based implementations spent 10% less time in
MVM operations than the dense check-based implementations
on average. This corresponds to 5% lower total execution time
on average.
The results show that the impact of larger setup overheads
for some of the techniques (e.g. clustering and precondition-
ing), in the context of both the IR and CG, is fairly negligible
(< 0.01%), since the amount of reuse is high. We observed
that the absolute amount of reuse in the context of CG is
dependent on the conditioning of the problem which impacts
the number of iterations required to reach the desired solution.
The error rate can also have an impact on the number of
iterations and hence the amount of reuse within the algorithm
Upon analyzing the performance of the techniques in the
different scenarios shown in Figure 13, we observed that
the overall overhead can vary greatly across different error
rates. For example, with a fault rate of zero (ﬁrst row of
Figure 13), the impact of recovery overheads is reduced and
the reduction in runtime overhead provided by the sparse fault
detection techniques can be more fully utilized. As the error
rate increases, the detection accuracy requirements become
more stringent and the total overhead from missed faults
and false positives must be properly balanced to provide the
lowest performance overhead. By conﬁguring the techniques to
minimize the overhead from missed faults and false positives,
the runtime beneﬁts for many of the sparse techniques is also
reduced to < 5%.
When the error rate is sufﬁciently large, the impact of recov-
ery overheads is again reduced allowing the sparse techniques
to reduce the runtime and total overheads further. In many of
the scenarios with smaller error rates, the NC based techniques
do poorly due to the larger time required to ﬁnd the smallest
singular value for many of the sparse problems.
When using the solvers with preconditioning, Figure 14
shows that the total beneﬁts from the sparse checks with
CG-pre were relatively small on average (5% − 10%). For
IR-pre, the sparse check based implementations spent about
30% − 40% less time overall than the dense check-based
implementations. CG-pre, IR-pre, CG, and IR are four real ap-
plication contexts that demonstrate that the sparse techniques
are frequently able to exploit structure and reuse in sparse
problems to reduce the overall overhead of algorithmic fault
tolerance compared to the traditional dense checks.
VI. CONCLUSIONS
Future Exascale computing system will be prone to errors
and severely energy constrained. On these systems it will
be critical to detect and correct applications to ensure that
applications can use them productively. This paper focuses
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
on low overhead fault detection for sparse linear algebra
algorithms which represents the core of a large class of HPC
and emerging applications.
Previously proposed techniques for detecting errors in dense
linear operations have high overhead (up to 100%, 32% on
average). In this paper, we propose a set of algorithmic
techniques that minimize the overhead of fault detection for
sparse problems. The techniques are based on two insights.
First, many sparse problems are well structured (e.g. diago-
nal, banded diagonal, block diagonal, etc.), which allows for
sampling techniques to produce good approximations of the
checks used for fault detection. These approximate checks are
acceptable for many sparse linear algebra applications. Second,
many linear applications have enough reuse that clustering
and preconditioning techniques can be used to make these
applications more amenable to algorithmic checks. We show
that the proposed techniques exploit these opportunities to
reduce overhead by up to 2x over traditional dense checks
and maintain high error detection accuracy over a larger set
of problems than the dense check. Further, the techniques also
reduce overhead in the context of larger algorithms that use
matrix-vector multiplications. Our experiments, which focus
on the iterative linear solvers CG and IR show that the beneﬁts
were up to 40% when considering only the MVM operations,
and up to 20% when considering non-MVM operations as
well.
ACKNOWLEDGMENT
This research was supported in part by the FCRP Gigascale
Systems Research Center (GSRC), Semiconductor Research
Corporation, and the National Science Foundation. Part of
this work was performed under the auspices of the U.S.
Department of Energy by Lawrence Livermore National Labo-
ratory under Contract DE-AC52-07NA27344 (LLNL-CONF-
546472) and was partially supported by the Department of
Energy Ofﬁce of Science (Advanced Scientiﬁc Computing
Research) Early Career Grant, award number NA27344.
REFERENCES
[1] International Technology Roadmap for Semiconductors. White
Paper, ITRS, 2010.
[2] J. Anﬁnson and F. T. Luk.
of algorithm-based fault
37:1599–1604, December 1988.
tolerance.
A linear algebraic model
IEEE Trans. Comput.,
[3] P. Banerjee, J.T. Rahmeh, C. Stunkel, V.S. Nair, K. Roy,
V. Balasubramanian, and J.A. Abraham. Algorithm-based fault
tolerance on a hypercube multiprocessor. Computers, IEEE
Transactions on, 39(9):1132 –1145, September 1990.
[4] Timothy A. Davis. University of ﬂorida sparse matrix collection.
NA Digest, 92, 1994.
[5] Asanovic et. al. The landscape of parallel computing research:
A view from berkeley. Technical Report UCB/EECS-2006-
183, EECS Department, University of California, Berkeley, Dec
2006.
[6] Berman et. al. Exascale computing study: Technology chal-
lenges in achieving exascale systems peter kogge, editor & study
lead. Technical report, DARPA IPTO, SEP 2008.
[7] E. Anderson et. al. Lapack: a portable linear algebra library
In Proceedings of the 1990
for high-performance computers.
ACM/IEEE conference on Supercomputing, Supercomputing
’90, pages 2–11, 1990.
[8] J. N. Glosli et. al. Extending stability beyond cpu millennium:
a micron-scale atomistic simulation of kelvin-helmholtz insta-
bility.
In Proceedings of the 2007 ACM/IEEE conference on
Supercomputing, SC ’07, pages 58:1–58:11, New York, NY,
USA, 2007. ACM.
[9] Jack Dongarra Andrew et. al. A sparse matrix library in c++
for high performance architectures, 1994.
[10] Keun Soo Yim et. al. Hauberk: Lightweight silent data corrup-
tion error detector for gpgpu. In Proceedings of the 2011 IEEE
International Parallel & Distributed Processing Symposium,
IPDPS ’11, 2011.
[11] L. S. Blackford et. al. An updated set of basic linear alge-
bra subprograms (blas). ACM Transactions on Mathematical
Software, 28:135–151, 2001.
[12] Man-lap Li et. al. Swat: An error resilient system, 2008.
[13] Mark Hall et. al. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10–18, November 2009.
[14] Nahmsuk Oh et. al. Control-ﬂow checking by software signa-
tures. IEEE Transactions on Reliability, 51:111–122, 2002.
[15] Nithin Nakka et. al. An architectural framework for providing
In In DSN, pages 585–594.
reliability and security support.
IEEE Computer Society, 2004.
[16] R. B. Lehoucq et. al. Arpack users guide: Solution of large scale
eigenvalue problems by implicitly restarted arnoldi methods.,
1997.
[17] Ronald F. Boisvert et. al. Matrix market: A web resource for
test matrix collections. In The Quality of Numerical Software:
Assessment and Enhancement, pages 125–137. Chapman and
Hall, 1997.
[18] Sarah Michalak et. al. Predicting the Number of Fatal Soft
Errors in Los Alamos National Laboratorys ASC Q Supercom-
puter. IEEE Transactions on Device and Materials Reliability,
5(3), 2005.
[19] Robert Falgout and Ulrike Yang. hypre : A library of high
performance preconditioners. In Computational Science, ICCS
2002, volume 2331 of Lecture Notes in Computer Science,
pages 632–641. Springer Berlin, Heidelberg, 2002.
[20] Kuang-Hua Huang and J.A. Abraham. Algorithm-based fault
tolerance for matrix operations. Computers, IEEE Transactions
on, C-33(6):518 –528, 1984.
[21] Anil K. Jain and Richard C. Dubes. Algorithms for clustering
data. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1988.
[22] C. Kong. Study of voltage and process variation’s impact on
the path delays of arithmetic units. In UIUC Master’s Thesis,
2008.
[23] Franklin T. Luk and Haesun Park. An analysis of algorithm-
based fault tolerance techniques. J. Parallel Distrib. Comput.,
5:172–184, April 1988.
Fault
[24] Amitabh Mishra and Prithviraj Banerjee. An algorithm-based
error detection scheme for the multigrid method. IEEE Trans.
Comput., 52(9):1089–1099, 2003.
[25] J. S. Plank, Y. Kim, and J. Dongarra.
tolerant ma-
trix operations for networks of workstations using diskless
checkpointing. Journal of Parallel and Distributed Computing,
43(2):125–138, June 1997.
[26] Y. Saad. Iterative Methods for Sparse Linear Systems. Society
for Industrial and Applied Mathematics, Philadelphia, PA, USA,
2nd edition, 2003.
[27] Cornelis Joost van Rijsbergen. Information Retrieval. 1979.
[28] P. Wesseling. Introduction to Multigrid Methods. Institute for
Computer Applications in Science and Engineering (ICASE),
1995.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
MV in cg
Total cg
MV in ir
Total ir
40
20
0
−20
−40
40
20
0
−20
−40
40
20
0
−20
−40
)
%
(
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o