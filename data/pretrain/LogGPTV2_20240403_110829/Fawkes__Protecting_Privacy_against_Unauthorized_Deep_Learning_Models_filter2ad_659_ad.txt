o
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 2
Protection Success Rate
Normal Classification Accuracy
 4
 6
 8
 1
 0.8
 0.6
 0.4
 0.2
 0
 10
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
2
i
n
o
s
n
e
m
D
i
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
-0.1
-0.2
-0.3
Original Images
Other Images
Target Images
Cloaked Images
2
i
n
o
s
n
e
m
D
i
Original Images
Other Images
Target Images
Cloaked Images
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
-0.1
-0.2
-0.3
-0.4
-0.2
 0
 0.2
 0.4
 0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
Number of Labels in Student Dataset
Dimension 1 
Dimension 1 
Figure 6: Protection performance im-
proves as
in
tracker’s model increases. (User/Tracker:
Web-Incept)
the number of
labels
Figure 7: Cloaking is less effective when
users and trackers use different feature
extractors. (User: VGG2-Dense, Tracker:
Web-Incept)
Figure 8: Cloaks generated on robust
models transfer better between feature
extractors. (User: VGG2-Dense, Tracker:
Web-Incept)
training images of faces, and Microsoft trains a model to rec-
ognize these faces. The API has a “training” endpoint that
must be called before the model will recognize faces, which
leads us to believe that Microsoft uses transfer learning to
train a model on user-submitted images.
Our normal cloaking method is 100% effective against the
Microsoft Azure Face API. Our robust cloaks also provide
100% protection against the Azure Face API. Detailed pro-
tection results are shown in Table 4.
Amazon Rekognition Face Veriﬁcation. Amazon Rekog-
nition [2] provides facial search services that the client can
use to detect, analyze, and compare faces. The API is used
by various large corporations including the NFL, CBS, and
National Geographic, as well as law enforcement agencies in
Florida and Oregon, and the U.S. Immigration and Customs
Enforcement agency (ICE).
It is important to note that Amazon Rekognition does not
speciﬁcally train a neural network to classify queried images.
Instead, it computes an image similarity score between the
queried image and the ground truth images for all labels. If
the similarity score exceeds a threshold for some label, Ama-
zon returns a match. Our cloaking technique is not designed
to fool a tracker who uses similarity matching. However,
we believe our cloaking technique should still be effective
against Amazon Rekognition, since cloaks create a feature
space separation between original and cloaked images that
should result in low similarity scores between them.
Table 4 shows that our normal cloaks only achieve a pro-
tection success rate of 34%. However, our robust cloaks
again achieve a 100% protection success rate.
Face++ [4] is a well-known face recognition sys-
Face++.
tem developed in China that claims to be extremely robust
against a variety of attacks (i.e. adversarial masks, makeup,
etc.). Due to its high performance and perceived robust-
ness, Face++ is widely used by ﬁnancial services providers
and other security-sensitive customers. Notably, Alipay uses
Face++’s services to authenticate users before processing
payments. Lenovo also uses Face++ services to perform face-
based authentication for laptop users.
Our results show that normal cloaking is completely inef-
fective against Face++ (0% protection success rate; see Ta-
ble 4). This indicates that their model is indeed extremely
robust against input perturbations. However, as before, our
robust cloaks achieve a 100% success rate.
Summary. Microsoft Azure Face API, Amazon Rekog-
nition and Face++ represent three of the most popular and
widely deployed facial recognition services today. The suc-
cess of Fawkes cloaking techniques suggests our approach is
realistic and practical against production systems. While we
expect these systems to continue improving, we expect cloak-
ing techniques to similarly evolve over time to keep pace.
7 Trackers with Uncloaked Image Access
Thus far we have assumed that the tracker only has access to
cloaked images of a user, i.e. the user is perfect in applying
her cloaking protection to her image content, and disassociat-
ing her identity from images posted online by friends. In real
life, however, this may be too strong an assumption. Users
make mistakes, and unauthorized labeled images of the user
can be taken and published online by third parties such as
newspapers and websites.
In this section, we consider the possibility of the tracker
obtaining leaked, uncloaked images of a target user, e.g. Al-
ice. We ﬁrst evaluate the impact of adding these images to
the tracker’s model training data. We then consider possible
mechanisms to mitigate this impact by leveraging the use of
limited sybil identities online.
7.1 Impact of Uncloaked Images
Intuitively, a tracker with access to some labeled, uncloaked
images of a user has a much greater chance of training a
model M that successfully recognizes clean images of that
user. Training a model with both cloaked and uncloaked user
images means the model will observe a much larger spread
of features all designated as the user. Depending on how M
is trained and the presence/density of other labels, it can a)
1598    29th USENIX Security Symposium
USENIX Association
classify both regions of features as the user; b) classify both
regions and the region between them as the user; or c) ignore
these feature dimensions and identify the user using some al-
ternative features (e.g. other facial features) that connect both
uncloaked and cloaked versions of the user’s images.
We assume the tracker cannot visually distinguish between
cloaked and uncloaked images and trains their model on both.
We quantify the impact of training with uncloaked images
using a simple test with cloaks generated from §5.2 and a
model trained on both cloaked and uncloaked images. Fig-
ure 10 shows the drop in protection success for FaceScrub
dataset as the ratio of uncloaked images in the training
dataset increases. The protection success rate drops below
39% when more than 15% of the user’s images are un-
cloaked.
Next, we consider proactive mitigation strategies against
leaked images. The most direct solution is to intentionally
release more cloaked images, effectively ﬂooding a potential
tracker’s training set with cloaked images to dominate any
leaked uncloaked images. In addition, we consider the use of
a cooperating secondary identity (more details below). For
simplicity, we assume that: trackers have access to a small
number of a user’s uncloaked images; the user is unaware of
the contents of the uncloaked images obtained by the tracker;
and users know the feature extractor used by the tracker.
7.2 Sybil Accounts
In addition to proactive ﬂooding of cloaked images, we ex-
plore the use of cooperative Sybil accounts to induce model
misclassiﬁcation. A Sybil account is a separate account con-
trolled by the user that exists in the same Internet commu-
nity (i.e. Facebook, Flickr) as the original account. Sybils
already exist in numerous online communities [67], and are
often used by real users to curate and compartmentalize con-
tent for different audiences [26]. While there are numerous
detection techniques for Sybil detection, individual Sybil ac-
counts are difﬁcult to identify or remove [60].
In our case, we propose that privacy-conscious users cre-
ate a secondary identity, preferably not connected to their
main identity in the metadata or access patterns. Its con-
tent can be extracted from public sources, from a friend,
or even generated artiﬁcially via generative adversarial net-
works (GANs) [32]. Fawkes modiﬁes Sybil images (in a man-
ner similar to cloaking) to provide additional protection for
the user’s original images. Since Sybil and user images re-
side in the same communities, we expect trackers will collect
both. While there are powerful re-identiﬁcation techniques
that could be used to associate the Sybil back to the original
user, we assume they are impractical for the tracker to apply
at scale to its population of tracked users.
To bolster cloaking effectiveness, the
Sybil Intuition.
user modiﬁes Sybil images so they occupy the same fea-
ture space as a user’s uncloaked images. These Sybil images
x2
Without Sybil
A
Leaked image of U
Test image of U
Cloaked image of U
Sybil image
With Sybil
A
(a)
x1
Decision Boundary
(b)
S
x1
Figure 9: Intuition behind Sybil integration visualized in a
2D feature space. Without Sybils, a tracker’s model will use
leaked training images of U to learn U’s true feature space
(left), leading to the correct classiﬁcation of images of U.
Sybil images S complicate the model’s decision boundary
and cause misclassiﬁcation of U’s images, even when leaked
images of U are present (right).
help confuse a model trained on both Sybil images and un-
cloaked/cloaked images of a user, increasing the protection
success rate. Figure 9 shows the high level intuition. Without
Sybil images, models trained on a small portion of uncloaked
(leaked) images would easily associate test images of the user
with the user’s true label (shown on left). Because the leaked
uncloaked images and Sybil images are close by in their fea-
ture space representations, but labeled differently (i.e. “ User
1” and “User 2”), the tracker model must create additional
decision boundaries in the feature space (right ﬁgure). These
additional decision boundaries decrease the likelihood of as-
sociating the user with her original feature space.
For simplicity, we explore the base case where the user
is able to obtain one single Sybil identity to perform fea-
ture space obfuscation on her behalf. Our technique becomes
even more effective with multiple Sybils, but provides much
of its beneﬁt with images labeled with a single Sybil identity.
Sybil images are created by
Creating Sybil images.
adding a specially designed cloak to a set of candidate im-
ages. Let xC be an image from the set of candidates the user
obtains (i.e. images generated by a GAN) to populate the
Sybil account. To create the ﬁnal Sybil image, we create a
cloak δ(xC, x) that minimizes the feature space separation be-
tween xC and user’s original image x, for each candidate. The
optimization is equivalent to setting x as the target and opti-
mizing to create xC ⊕ δ(xC, x) as discussed in §4. After choos-
ing the ﬁnal xc from all the candidates, a ready-to-upload
Sybil image xS = xC ⊕ δ(xC, x).
7.3 Efﬁcacy of Sybil Images
Sybil accounts can increase a user’s protection success rate
when the tracker controls a small number of a user’s un-
cloaked images. To experimentally validate this claim, we
choose a label from the tracker’s dataset to be the Sybil ac-
count (controlled by the user), and split the user’s images into
two disjoint sets: A contains images that were processed by
Fawkes, and whose cloaked versions have been shared on-
USENIX Association
29th USENIX Security Symposium    1599
e
t
a
R
s
s
e
c
c
u
S
n
o
i
t
c
e
t
o
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 0.2
 0.4
Ratio of Leaked Uncloaked Images
e
t
a
R
s
s
e
c
c
u
S
n
o
i
t
c
e
t
o
r
P
 1
 0.8