## $ grep -A2 "DAG_01" /var/log/airflow/airflow-scheduler.log | grep -A2
"09:23"  
[2016-11-22 13:24:26,660] {models.py:2660} INFO - Checking state for   
[2016-11-22 13:24:26,672] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:26,678] {jobs.py:514} INFO - Checking dependencies on 2
tasks instances, minus 0 skippable ones  
[2016-11-22 13:24:26,726] {base_executor.py:36} INFO - Adding to queue:
airflow run DAG_01 latest_only 2016-11-18T09:23:00 --local -sd
DAGS_FOLDER/DAG_01.py  
[2016-11-22 13:24:26,769] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:26,769] {jobs.py:514} INFO - Checking dependencies on 0
tasks instances, minus 0 skippable ones
## [2016-11-22 13:24:31,830] {models.py:2660} INFO - Checking state for
[2016-11-22 13:24:31,832] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:31,832] {jobs.py:514} INFO - Checking dependencies on 0
tasks instances, minus 0 skippable ones
## [2016-11-22 13:24:37,238] {models.py:2660} INFO - Checking state for
[2016-11-22 13:24:37,240] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:37,252] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.
## [2016-11-22 13:24:45,736] {models.py:2660} INFO - Checking state for
[2016-11-22 13:24:45,744] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:45,756] {jobs.py:514} INFO - Checking dependencies on 2
tasks instances, minus 0 skippable ones
## [2016-11-22 13:24:56,613] {models.py:2660} INFO - Checking state for
[2016-11-22 13:24:56,624] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:56,638] {jobs.py:514} INFO - Checking dependencies on 2
tasks instances, minus 0 skippable ones
[2016-11-22 13:24:56,680] {base_executor.py:36} INFO - Adding to queue:
airflow run DAG_01 latest_only 2016-11-18T09:23:00 --local -sd
DAGS_FOLDER/DAG_01.py  
[2016-11-22 13:24:56,823] {jobs.py:498} INFO - Getting list of tasks to skip
for active runs.  
[2016-11-22 13:24:56,824] {jobs.py:514} INFO - Checking dependencies on 0
tasks instances, minus 0 skippable ones
Eventually, we ended up just creating new DAG definitions with future start
dates and manually cleared the Redis queue.
_Additional Context_ :
Our scheduler is daemonized by upstart and runs with -n 5
Here is the template we use for our cron DAGs (note it's a jinja2 template):
# {{ansible_managed}}
from dateutil import parser
from airflow.operators import LatestOnlyOperator  
from airflow.operators import BashOperator  
from airflow.models import DAG
args = {  
'owner': 'airflow',  
'start_date': parser.parse('{{item.start_date}}'),  
'retries': 0,  
}
dag = DAG(  
dag_id='{{item.name}}',  
default_args=args,  
schedule_interval='{{item.schedule}}',  
max_active_runs=1,  
)
latest_only = LatestOnlyOperator(task_id='latest_only', dag=dag)
script = BashOperator(  
task_id='{{item.name}}',  
bash_command='{{item.command}}',  
default_args=args,  
dag=dag,  
)
script.set_upstream(latest_only)
One thing to note: We are on Airflow 2.7.1.3; however, we brought in the
operator through a plugin:
import datetime  
import logging
from airflow.models import BaseOperator, TaskInstance  
from airflow.plugins_manager import AirflowPlugin  
from airflow.utils.state import State  
from airflow import settings
class LatestOnlyOperator(BaseOperator):  
"""  
Allows a workflow to skip tasks that are not running during the most  
recent schedule interval.  
If the task is run outside of the latest schedule interval, all  
directly downstream tasks will be skipped.  
"""
    ui_color = '#e9ffdb'  # nyanza
    def execute(self, context):
        now = datetime.datetime.now()
        left_window = context['dag'].following_schedule(
            context['execution_date'])
        right_window = context['dag'].following_schedule(left_window)
        logging.info(
            'Checking latest only with left_window: %s right_window: %s '
            'now: %s', left_window, right_window, now)
        if not left_window < now <= right_window:
            logging.info('Not latest execution, skipping downstream.')
            session = settings.Session()
            for task in context['task'].downstream_list:
                ti = TaskInstance(
                    task, execution_date=context['ti'].execution_date)
                logging.info('Skipping task: %s', ti.task_id)
                ti.state = State.SKIPPED
                ti.start_date = now
                ti.end_date = now
                session.merge(ti)
            session.commit()
            session.close()
            logging.info('Done.')
        else:
            logging.info('Latest, allowing execution to proceed.')
class AirflowNextPlugin(AirflowPlugin):  
name = "airflow_next"  
operators = [LatestOnlyOperator]
**What you expected to happen** :
**How to reproduce it** :
**Anything else we need to know** :
Moved here from https://issues.apache.org/jira/browse/AIRFLOW-648