从源码来看，有以下的三点需要注意：
如果需要界定一个时间边界，
if（mi->rli->slave_running)
protocol->store_null();
，那就是数据从 Master 端写入 binlog 开始到 Slave 端应
图5-22
rotation
case)
如果按照日志中的 timestamp
第5章MySQL运维管理实践|201
---
## Page 224
202丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
点需要提一下，就是需要设置 server-id。
也可以查看。
的标注。
而 replace 则是更新替换表里的时间，无须考虑表里是否有数据，daemonize 是后台运行
试表，interval是间隔1秒钟，最小可以是0.01秒，update 是更新 test 库上的这个测试表，
准化管理。
可以使用 pt-heartbeat。这个工具使用起来也非常便捷，属于 pt 工具集的一部分。
志位数据到达从库的时间为准，得到的这个值就是延迟，如果说要得到较为准确的延迟情况，
值来衡量延迟。
志维度进行衡量，比如根据 binlog 的偏移量差异，或者是基于时间维度，通过时间戳差
差异带来的负面影响是大的，或者说这种延迟会是一种毛刺。
来作为标记，这个延迟其实是很小的，比如延迟是5秒，但是差异的日志量是2G，这种
test`.heartbeat
GRANT SELECT, PROCESS, SUPER, REPLICATION SLAVE ON *.
所以延迟是一个比较难以衡量的指标，在理解方式上存在较大的差异，可以基于日
Please read the DESCRIPTION section of the pt-heartbeat POD.
heartbeat h=10.127.128.99,u=pt _checksum,p=pt_
pt-heartbeat h='10.127.128.99',u='pt_checksum',p='pt_checksum',P=3306
我们来创建测试表，在后台启动这个心跳守护进程，其中的create-table 就是创建测
工具具体的参数可以参考 pt-heartbeat --help 来看到，在此我只给出要点。
grant all privileges on test.* to pt_checksum@'10.127.%.%';
使用 pt-heartbeat，我们需要创建一个用户 pt_checksum，这个统一的用户方便后续标
接下来的就是重点工作了，我们可以开启 monitor 选项来监控主从延迟的情况，有一
root
然后我们给予这个用户访问test数据库的权限。
2．使用 pt工具检测主从延迟
一种相对容易理解的延迟计算方式是基于心跳机制，周期性发送一个标志位，以这个标
主库上快速查看。
-create-table
-create-table
se
022:35？
10.127.%.%
00:00:00 perl/usr/local/bin/pt-
--update
IDENTIFIED BY
checksum,P=3306 -D test
--check but the
'pt_checksum
--daemonize
---
## Page 225
可以接受，新版本中的并行复制效果怎么样，在不同的版本中是否有改变，我们能否找
定的延迟的场景，通过主从来达到读写分离是个很不错的方案，但是延迟率到底有多高
清理掉这个文件。
要规范一些，使用 stop 选项来做，会生成一个临时文件，注意下次重新启动的话，需要
范围了。
出现明显差异。我们会在后面整体对比测试一下。
-D test--table=heartbeat
4m这样。
用frames来定制，比如默认是1m，5m，15m，我们可以定制，比如显示为1m，2m，3m，
如果想即查即看，就看一次，可以使用 check 选项，当然这个值就没有 frame 的时间
对于主从延迟，一直以来就是一个颇有争议的话题，在MySQL 阵营中，如果容忍-
Successfully created file/tmp/pt-heartbeat-sentinel
# pt-heartbeat h='10.127.xx.xx',u='pt_checksum',p='pt_checksum',P=3306
案例5-9：MySQL5.6、5.7版本并行复制测试
当然有进有出，我们开启了后台守护进程，本质上是个 perl 脚本，如果要停止，也
0.00
有的同学可能说，
0.00s
0.00s
0.00s
可以看到目前的环境中是没有任何延迟的，方括号里面的指标是什么意思，可以使
0.00s
0.00s
0.00s
0.00s
0.00s
0.00s
我们查看延迟的情况。
结果和 show variables like'server%'结果是一致的，更快速高效。
1 row in set (0.01 sec)
IServer_id|Host|Port|Master_id|Slave_UUID
show slave hosts;
130581
0.00s,
0.00s
0.00s,
0.00s,
0.00s,
0.00s,
0.00s,
怎么都显示为0，其实如果用 sysbench压一下，就立马或有延迟的
-D
133061
0.00s
0.00s,
0.00s，
0.00s,
0.00s
0.00s,
0.00s
0.00s，
-monitor
0.00s
0.00s
0.00s
-tab.
0.00s
C
.00s
.00s
.00s
20 | c6d66211-a645-11e6-a2b6-782bcb472f631
heartbeat
0.00s
0.00s
0.
00s
--monitor
-master-server-id=20
--master-server-id=20
第5章MySQL运维管理实践|203
-D test -stop
2m,3m,4m
--check
-一+
---
## Page 226
204丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
制的差别和改进点。
-D sysbenchtest
启动5.7的服务。避免多实例的并行干扰。
本的MySQL服务，另外一台服务器上搭建了从库。
到一些参考的数据来佐证，这一点上我们可以通过一些小测试来说明。
其中值得一提的是 5.7版本做了一些改进，slave-paralle-type 有如下的两个可选值：
mysql>start slave;
mysql>stop slave;
并行复制的基本配置 5.6 开启并行复制。
因为主从复制在5.6 和5.7还是存在一定的差别，我们就分别测试单线程和多线程复
pt-heartbeat
·DATABASE：基于库级别的并行复制，与5.6相同；
2.MySQL5.7版本开启并行复制
mysql>set
开启主从延迟检测：
pt-heartbeat
开启后台任务：
size=5000000
mysql-user=root
加压测试使用如下的 sysbench 脚本，持续时间 300秒。
5000000
sysbench /home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua --
查看主从延迟，
创建了10个表，然后插入了 500万数据来测试。
初始化数据采用了类似下面的脚本，5.6和5.7版本中都差不多。
为了基本能够达到同一个基准进行测试，
服务器上安装了 pt工具用来检测主从延迟,安装了新版本的 sysbench 来做加压测试。
数据库版本为5.6.23Percona分支，5.7.17MySQL官方版本。
首先来为了基本按照同一个参考标准，我们就在同一台服务器上安装了5.6、5.7版
1．并行复制配置
库：
--threads=50prepare
--create-table --interval=1
10.127.128.78
10.127.128.227
使用 pt-heartbeat 来完成。
h='10.127.128.78',u='pt_checksum',p='pt_checksum',P=3308
h='10.127.128.78',u='pt_checksum',p='pt_checksum',P=3307
 slave_parallel_workers=8;
3308
RHEL6U3
RHEL6U3
-mysql
mysq
：我先启动5.6的数据库服务，测试完毕，再
32G
32G
-socket=/home/mysql
--update --replace -daemonize
R710
--master
R710
--frames=5s --interval=5
mysql
---
## Page 227
单线程，如下图5-24所示。
化，那么就会有一个很本质的原因，那就是在主库端的更新是多线程，而从库端更新是
整个复制的流程中，看似存在多个节点会出现延迟的可能，而如果把这些工作都细
总体来看，MySQL5.6版本中的并行复制效率提升不够明显，5.7版本中的提升效果非常显著。
图5-23是得到的一个概览图，横轴是测试时间，纵轴是延迟时间。
mysql> stop slave;
mysql>
mysql> stop slave;
所以我们开启了logical_clock。
·LOGICAL_CLOCK：逻辑时钟，主库上怎么并行执行的，从库上也是怎么并行回放的。
set
set
0
global
global
 slave_parallel_workers=8;
图5-24
图5-23
Relay_log写
第5章MySQL运维管理实践|205
5.78线程
5.7单线程
5.68线程
---
## Page 228
206|MySQLDBA工作笔记：数据库管理、架构优化与运维开发
测试情况大体相似，耗时情况和延迟回落的趋势，基本也是相似的，而MySQL5.7版本的并
迅速回落；从回落的过程来看，MySQL5.6版本中的单线程和多线程同MySQL5.7版本中的
可以很清晰地看到，对于从库的延迟在加压完成后，延迟依旧会逐步增长，达到一个峰值后，
场景，调整了时间。
只是看这个图可能还看不出个所以然，所以想到了这一点，我就继续补充了一下测试的
迟的数据，这个时间我们也需要关注，至于主从一致后的延迟回落到底是什么样，我想
有10分钟的高强度并发，那么 10分钟后延迟不是立即消失的，从库得慢慢消化这个延
照数据库级来做的，在5.7版就全面改进了，可以实现表级了。
同步，而这个粒度可以逐步细化，比如数据库级，表级等，目前MySQL 5.6版本中是按
得尤其注意这个顺序，我们可以逐步来细分，首先对于同一个表的更新只能按照顺序来
录方式，而这个操作到了从库中也只能老老实实的按照顺序来应用，如果采用多线程就
志都是需要顺序写，在源端是多线程并发操作，而映射到日志中，必然是一个顺序的记
数据库级别的，在这种模式下因为粒度太粗导致复制延迟问题的改进不是很明显。
而在 MySQL5.6版本中引入了并行复制，这一点能够缓和原本的复制瓶颈。
制和 statement 格式做斗争，经过改进，有了row格式，也算是复制方向上的一大进步，
多线程存在一些待解决的难题，其中之一就是语句的顺序无法保证，无论如何，
但是复制的效率提升不是严格意义上质的飞跃，只能算是一个开篇，因为支持的是
中间箭头就是在指定的时间范围的加压测试，而右边的部分则是延迟回落的一个过程，
下面这个图5-26 花了我不少的时间去收集数据和整理。
其实这个图我感觉没有画完，因为大批量的事务并发处理，必然会导致延迟，比如
下图5-25是我测试的一个图，是MySQL5.6、5.7版本单线程与多线程的延迟对比图。
这样一个看似“存在即合理”方案在MySQL5.6版本以前都是这么做的。最早的复
00
g
图5-25
3537
5.78线程
5.7单线程
5.68线程
5.6单线程
日
---
## Page 229
--frames=5s--interval=5
出差别来，比如我们首先建立4个数据库，每个数据库下创建10个表。
后通过在从库开启并行复制来改进，对比测试。
来看，测试的场景还可以继续改进，可以更有针对性。
几乎一样，由此可以看出在这个测试场景中，并行复制没有派上用场；但是从另一角度
行复制相比而言就是一个亮点，数据加压后的延迟回落极快，整个过程耗时要少很多。
当然这个图也反映出来一些问题，那就是 MySQL 5.6 版本的单线程和多线程的结果
pt-heartbeat
size=2000000 --threads=30
sysbench /home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua --
size=2000000 --threads=30 prepare
mysql-
初始化一部分数据，对于 sysbenchtest1 如此，其他的几个数据库也是类似的操作。
 mysqladmin create sysbenchtest1
开启并行复制模式时，延迟如下：
几个简单的对比就可以说明。
查看延迟的情况。
然后开启 sysbench 测试。
SC
我们继续开启 sysbench 的加压测试，使用 pt工具同步检测延迟，花几分钟就可以看
怎么能够快速看到效果呢。
怎么改进呢，因为 5.6版本中是数据库级的复制，所以我们可以建立多个数据库，然
sysbenchtest
ock
ock
--mysql-host=localhost
-user=
Froot
8
0
8
8
-host=
--table=heartbeat
-mysql
oca
alhost
--time=300 run
--mysql-db=
-mysql-db=sysbenchtest1
图5-26
mysq
--monitor
=sysbenchtestl-
/mysql
第5章MySQL运维管理实践丨207
--tables=10-
线程
5.68线程
--table
---
## Page 230
208丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
图5-27和图5-28就是一个线上环境对比并行复制的性能测试差异，仅供参考。
测试已经有了一个比较清晰的结果。
，而在企业级环境中，在软件版本选型和业务上线前，需要做一些细致地压测，如下
当然想看到更加细致的图形对比，也不是一件难的事情，但基本上我们通过上述的
0
0.00s
.
0.00s
0.00s
再次切换回并行复制模式，
0.00s