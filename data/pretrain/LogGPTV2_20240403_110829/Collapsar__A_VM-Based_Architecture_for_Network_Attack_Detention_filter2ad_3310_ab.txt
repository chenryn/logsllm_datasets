logging, capturing of live image snapshots, and dynamic
honeypot creation and customization [38].
3.2 Assurance Modules
The Collapsar functional components create virtual pres-
ence of honeypots. Assurance modules provide neces-
sary facilities for attack investigation and mitigation of
associated risks.
3.2.1 Logging Module
Recording how an intruder exploits software vulnerabil-
ities is critical to understanding the tactics and strategies
of intruders [9]. All communications related to honey-
pots are highly suspicious and need to be recorded. How-
ever, the traditional Network Intrusion Detection Sys-
tem (NDIS) based on packet snifﬁng may become less
effective if the attack trafﬁc is encrypted.
In fact, it
has become common for intruders to communicate with
compromised hosts using encryption-capable backdoors,
such as trojaned sshd daemons. In order to log the details
of such attacks without intruders tampering with the log,
the logging module in each honeypot consists of sensors
embedded in the honeypot’s guest OS as well as log stor-
age in the physical machine’s host OS. As a result, log
collection is invisible to the intruder and the log storage
is un-reachable by the intruder.
3.2.2 Tarpitting Module
Deploying high-interaction honeypots is risky in that
they can be used by an intruder as a platform to launch a
second round of attack or to propagate worm. To mitigate
such risks, the Collapsar’s tarpitting module subverts in-
truder activities by (1) throttling out-going trafﬁc from
honeypots [41] by limiting the rate packets are sent (for
example TCP-SYN packets) or reducing average trafﬁc
volume and (2) scrutinizing out-going trafﬁc based on
known attack signatures, and crippling detected attacks
by invalidating malicious attack codes [7].
3.2.3 Correlation Module
Collapsar provides excellent opportunities to mine log
data for correlated events that an individual honeypot or
multiple independently operated honeypots cannot offer.
Such capability is enabled by the correlation module. For
example, the correlation module is able to detect network
scanning by correlating simultaneous or sequential prob-
ing (ICMP echo requests or TCP-SYN packets) of hon-
eypots that logically belong to different production net-
works. If the networks are probed within a short period
(such as in a couple of seconds), it is likely the network
is being scanned. The correlation module can also be
used to detect on-going DDoS attacks [35], worm prop-
agations [43], and hidden overlay networks such as IRC-
based networks or peer-to-peer networks formed by cer-
tain worms.
4 Implementation of Collapsar
In this section, we present the implementation details of
Collapsar. Based on virtual machine technologies, Col-
lapsar is able to support virtual honeypots running vari-
ous operating systems.
4.1 Trafﬁc Redirection
There are two approaches to transparent trafﬁc redirec-
tion:
the router-based approach and the end-system-
based approach. In the router-based approach, an inter-
mediate router or the edge router of a network domain
can be conﬁgured to activate the Generic Routing Encap-
sulation (GRE) [28, 29] tunneling mechanism to forward
honeypot trafﬁc to the Collapsar center. The approach
has the advantage of high network efﬁciency. However,
it requires the privilege of router conﬁguration. On the
other hand, the end-to-end approach does not require
access and changes to routers.
Instead, it requires an
application-level redirector in the target production net-
work for forwarding packets between the intruder and
the honeypot. In a fully cooperative environment such
as a university campus, the router-based approach may
be a more efﬁcient option, while in an environment with
multiple autonomous domains, the end-system-based ap-
proach may be adopted for easy deployment. In this pa-
per, we describe the design and implementation of the
end-system-based approach.
To more easily describe the end-system-based ap-
proach, let R be the default router of a production net-
work, H be the IP address of the physical host where the
redirector component runs, and V be the IP address of
the honeypot as appearing to the intruders. H, V , and
an interface of R, say I1, belong to the same network.
When there is a packet addressed to V , router R will re-
ceive it ﬁrst and then try to forward the packet based on
its current routing table. Since address V appears in the
same network as I1, R will send the packet over I1. To
successfully forward the packet to V , R needs to know
the corresponding MAC address of V in the ARP cache
table. If the MAC address is not in the table, an ARP
request packet will be broadcasted to get the response
from V . H will receive the ARP request. H knows that
there is no real host with IP address V . To answer the
query, H responds with its own MAC address, so that
the packet to V can be sent to H and the redirector in
H will then forward the packet to the Collapsar center.
Note that one redirector can support the virtual presence
of multiple honeypots in the same production network.
The redirector is implemented as a virtual machine
running our extended version of UML. This approach
adds considerable ﬂexibility to the redirector since the
VM is able to support policy-driven conﬁguration for
packet ﬁltering and forwarding, and can be conveniently
extended to support useful features such as packet log-
ging, inspection, and in-line rewriting. The redirector
has two virtual NICs: the pcap/libnet interface and the
tunneling interface. The pcap/libnet interface performs
the actual packet capture and injection. Captured pack-
ets will be echoed as input to the UML kernel. The redi-
rector kernel acts as a bridge, and performs policy-driven
packet inspection, ﬁltering, and subversion. The tunnel-
ing interface tunnels the inspected packets transparently
to the Collapsar center. For communication in the op-
posite direction, the redirector kernel’s tunneling inter-
face accepts packets from the Collapsar center and moves
them into the redirector kernel itself, which will inspect,
ﬁlter, and subvert the packets from the honeypots, and re-
inject the inspected packets into the production network
through the pcap/libnet interface.
4.2 Trafﬁc Dispatching
The Collapsar front-end is similar to a transparent ﬁre-
wall.
It dispatches incoming packets from redirectors
to their respective honeypots based on the destination
ﬁeld in the packet header. The front-end can also be im-
plemented using UML which creates another point for
packet logging, inspection, and ﬁltering.
Ideally, packets should be forwarded directly to the
honeypots after dispatching. However, virtualization
techniques in different VM enabling platforms compli-
cate this problem. In order to accommodate various VMs
(especially those using VMware), the front-end will ﬁrst
inject packets into the Collapsar network via an injection
interface. The injected packets will then be claimed by
the corresponding virtual honeypots and be moved into
the VM kernels via their virtual NICs. This approach
supports VMware-based VMs without any modiﬁcation.
However, it incurs additional overhead (as shown in Sec-
tion 5). Furthermore, it causes the undesirable cross-talk
between honeypots which logically belong to different
production networks. Synthetic cross-talk may decrease
the authenticity of Collapsar. A systematic solution to
this problem requires a slight modiﬁcation to the virtu-
alization implementation, especially the NIC virtualiza-
tion. Unfortunately, modifying the VM requires the ac-
cess to the VM’s source code. With open-source VM
implementations, such as UML, the injection interface
of the front-end can be modiﬁed to feed packets directly
into the VM (honeypot) kernels. As shown in Section 5,
considerable performance improvement will be achieved
with this technique.
4.3 Virtual Honeypot
The virtual honeypots in Collapsar are highly interactive.
They can be compromised and fully controlled by in-
truders. Currently, Collapsar supports virtual honeypots
based on both VMware and UML. Other VM enabling
platforms such as Xen [22], Virtual PC [10], and UM-
Linux [30] will also be supported in the future.
VMware is a commercial software system and is one
of the most mature and versatile VM enabling platforms.
A key feature is the ability to support various commod-
ity operating systems and to take snapshot of live vir-
tual machine images. Support for commodity operating
systems provides more diverse view of network attacks,
while image snapshot generation and restoration (with-
out any process distortion) add considerable convenience
to forensic analysis.
As mentioned in Section 4.2, the network interface
virtualization of VMware is not readily compatible with
Collapsar design. More speciﬁcally, VMware creates
a special vmnet, which emulates an inner bridge. A
VMware-hosted virtual machine injects packets directly
into the inner bridge, and receives packets from the inner
bridge. A special host process is created to be attached
to the bridge and acts as an agent to forward packets be-
tween the local network and the inner bridge. The ability
to read packets from the local network is realized by a
loadable kernel module called vmnet.o, which installs a
callback routine registering for all packets on a speci-
ﬁed host NIC via the dev add pack routine. The packets
will be re-injected into the inner-bridge. Meanwhile, the
agent will read packets from the inner-bridge and call the
dev queue xmit routine to directly inject packets to the
speciﬁed host NIC. It is possible to re-write the special
host process to send/receive packets directly to/from the
Collapsar front-end avoiding the overhead of injecting
and capturing packets twice - once in the front-end and
once in the special host process. This solution requires
modiﬁcations to VMware.
UML is an open-source VM enabling platform that
runs directly in the unmodiﬁed user space of the host
OS. Processes within a UML (the guest OS) are executed
in the virtual machine in exactly the same way as they
would be executed on a native Linux machine. Leverag-
ing the capability of ptrace, a special thread is created to
intercept the system calls made by any process thread in
the UML kernel, and redirect them to the guest OS ker-
nel. Meanwhile, the host OS has a separate kernel space,
eliminating any security impact caused by the individual
UMLs.
Taking advantage of UML being open source, we
enhance UML’s network virtualization implementation
such that each packet from the front-end can be imme-
diately directed to the virtual NIC of a UML-based VM.
This technique not only avoids the unnecessary packet
capture and re-injection, as in VMware, but also elimi-
nates the cross-talk between honeypots in the Collapsar
center.
4.4 Assurance Modules
Logging modules are deployed in multiple Collapsar
components including redirectors, front-ends, and hon-
eypots. Transparent to intruders, logging modules in dif-
ferent locations record attack-related information from
different view points. Simple packet inspection tools,
such as tcpdump [8] and snort [6] are able to record
plain trafﬁc, while embedded sensors inside the honeypot
(VM) kernel are able to uncover an intruder’s encrypted
communications. In section 6.1, we will present details
of several attack incidences demonstrating the power
of in-kernel logging. The in-kernel logging module
in VMware-based honeypots leverages an open-source
project called sebek [5], while in-kernel logging module
for UML-based honeypots is performed by kernort [31],
a kernelized snort [6].
Tarpitting modules are deployed in both the front-end
and redirectors. The modules perform in-line packet in-
spection, ﬁltering, and rewriting. Currently, the tarpit-
ting module is based on snort-inline [7], an open-source
project. It can limit the number of out-going connections
within a time unit (e.g., one minute) and can also com-
pare packet contents with known attack signatures in the
snort package. Once a malicious code is identiﬁed, the
packets will be rewritten to invalidate its functionality.
The Collapsar center provides a convenient venue to
perform correlation-based attack analysis such as wide-
area DDoS attacks or stepping stone attacks [42]. The
current prototype is capable of attack correlation based
on simple heuristics and association rules. However, the
Collapsar correlation module can be extended in the fu-
ture to support more complex event correlation and data
mining algorithms enabling the detection of non-trivial
attacks such as low and slow scanning and hidden over-
lay networks.
5 Performance Measurement
The VM technology provides effective support for high-
interaction honeypots. However, the use of virtual ma-
chines inevitably introduces performance degradation.
In this section, we ﬁrst evaluate the performance over-
head of two currently supported VM platforms: VMware
and UML. We then present the end-to-end networking
overhead caused by the Collapsar functional components
for trafﬁc redirection and dispatching.
To measure the virtualization-incurred overhead, we
use two physical hosts (with aliases seattle and tacoma,
respectively) with no background load, connected by a
lightly loaded 100Mbps LAN. Seattle is a Dell Pow-
erEdge server with a 2.6GHz Intel Xeon processor and
2GB RAM, while tacoma is a Dell desktop PC with a
1.8GHz Intel Pentium 4 processor and 768MB RAM.
A VM runs on top of seattle, and measurement packets
are sent from tacoma to the VM. The TCP throughput
is measured by repeatedly transmitting a ﬁle of 100MB
under different socket buffer size, while the latency is
measured using standard ICMP packets with different
payload sizes. Three sets of experiments are performed:
(1) from tacoma to a VMware-based VM in seattle, (2)
from tacoma to a UML-based VM in seattle, and (3)
from tacoma directly to seattle with no VM running.
The results in TCP throughput and ICMP latency are
shown in Figures 2(a) and 2(b), respectively. The curves
“VMware,” “UML,” and “Direct” correspond to experi-
ments (1), (2), and (3), respectively.
Figure 2(a) indicates that UML performs worse in
TCP throughput than VMware, due to UML’s user-level
virtualization implementation. More speciﬁcally, UML
uses a ptrace-based technique implemented at the user
level and emulates an x86 machine by virtualizing sys-
tem calls. On the other hand, VMware employs the
binary rewriting technique implemented in the kernel,
which inserts a breakpoint in place of sensitive instruc-
tions. However, both VMware and UML exhibit simi-
lar latency degradation because the (much lighter) ICMP
trafﬁc does not incur high CPU load therefore hiding the
difference between kernel and application level virtual-
ization. A more thorough and rigorous comparison be-
tween VMware and UML is presented in [22].
We then measure the performance overhead incurred
by the trafﬁc redirection and dispatching mechanisms of
Collapsar. We set up tacoma as the Collapsar front-end.
In a different LAN, we deploy a redirector running on a
machine with the same conﬁguration as seattle. The two
LANs are connected by a high performance Cisco 3550
router. A machine M in the same LAN as the redirector
serves as the “intruder” machine, connecting to the VM
(honeypot) running in seattle. Again, three sets of ex-
periments are performed for TCP throughput and ICMP
latency measurement: (1) from M to a VMware-based
honeypot in seattle, (2) from M to a UML-based hon-
eypot in seattle, and (3) from M to the machine hosting
the redirector (but without the redirector running). The
results are shown in Figures 3(a) and 3(b). The curves
“VMware,” “UML,” and “Direct” correspond to experi-
ments (1), (2), and (3), respectively.
Contrary to the results in Figures 2(a) and 2(b), the
UML-based VM achieves better TCP throughput and
ICMP latency than the VMware-based VM. We believe
this is due to the optimized trafﬁc dispatching mecha-
nism implemented for UML (Section 4.2). Another im-