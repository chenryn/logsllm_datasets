each Markov chain can produce any string within its domain
(digits, symbols, etc.), the Markov chain can be thought of
as a compact representation of a comprehensive dictionary,
capable of generating a complete list of strings and their
associated probabilities.
Speciﬁcally, four Markov chains are built from the train-
ing data: a 1-gram chain for symbols only, a 1-gram chain
for digits only, an n-gram chain for alphabetic strings of
length n or longer and a 1-gram chain for alphabetic strings
shorter than n.2 During training, the starting and tran-
sition probabilities for each chain are built. In addition to
building these probability tables, the training algorithm also
builds a dictionary by keeping track of every string seen, and
tabulating the frequencies with which they occur. By track-
ing the observed frequency for each string, the dictionar-
ies capture sequences that appear more frequently than the
product of the corresponding transition probabilities would
indicate, thereby signiﬁcantly increasing the number of pass-
words that can be guessed in a set interval of time.
With that background, the GP for a token of length L is
computed as:
GPM C (token) =max( ObservedF requency(token),
prob(token1,n) ∗ L−1(cid:11)
T P (tokeni−n+1,i, tokeni−n+2,i+1))
i=n
(1)
where tokeni,j is the substring from characters i to j and
T P is the transition probability from one n-gram to another.
For common tokens, GPM C is ObservedF requency, which is
the ratio of the number of occurrences of token to the total
number of strings seen of that same length. For uncommon
or previously unseen strings, GPM C is computed using the
probabilities within the Markov chain. For example, given
the token “troubador”, the algorithm multiplies the proba-
bility that a string starts with “tro”, the probability of tran-
sitioning from “tro” to “rou”, and so on. All probabilities
referenced are computed from the training set.3
The strength calculator combines the probabilities from
the dictionaries, Markov chains and PCFG to compute the
1All experiments presented in this paper were repeated on
a second random division of rockyou, with the results from
the second sets matching the ﬁrst.
2Tri-grams were used for alphabetic strings throughout the
analysis as they appear to be the most eﬀective.
3Since the GPM C s do not sum to 1, they are technically not
probabilities. However, “probability” captures the intuition
of what is being measured.
132
password’s GP as:
GP (password) = SP (password) ∗
(cid:11)
s∈SS(password)
GPM C (s)
(2)
where SP is the observed probability of the password
structure (e.g., L4S2D1L4) within the PCFG, and SS re-
turns substrings from the password based oﬀ the structure
(i.e. “pass**1word” would return {“pass”, “**”, “1”, “word”}).
4.3 Guessing Passwords
In order to determine real-world limits on what level of GP
might be considered secure, the algorithm to compute GPs
was used to create a password-cracking program. Algorithm
1 was used to generate all passwords at or below a threshold
GP (minGP ) using password structures, dictionaries and
Markov chains as described in Section 4.2.
Data: minGP, cracking data
Result: output list of recovered passwords
for each password structure SS do
try all passwords with GP ≥ minGP using Alg. 2;
end
Algorithm 1: Cracking Program
Data: currentGuess, cumulativeGP, password structure
SS, minGP, list of valid passwords
Result: output list of recovered passwords
if SS is empty then
if currentGuess is a valid password then
output G;
end
end
else
S = ﬁrst element of SS;
MC = the Markov chain used to guess S;
for each word W ∈ MC’s dictionary do
if cumulativeGP ∗ GPM C (W ) ≥ minGP then
recursive call with currentGuess + W,
cumulativeGP ∗ GPMC(W), SS − S;
end
end
L = list of words W built from MC such that
GPM C (W ) ≥ minGP/cumulativeGP ;
for each word W in L do
recursive call with currentGuess + W,
cumulativeGP ∗ GPMC(W), SS − S;
end
end
Algorithm 2: Guess Generating Algorithm
Algorithm 1 invokes the guess generating function (Algo-
rithm 2) for each password structure (e.g., L8D1S1). This
initial call to Algorithm 2 uses an empty currentGuess,
SP (SS) for cumulativeGP and the structure SS.
Whenever the guess generator (Algorithm 2) is called for
a particular password structure (e.g., L8D1S1), it ﬁlls in the
ﬁrst component of the structure (L8) and makes a recur-
sive call to ﬁll the remaining parts of the structure (D1S1).
At each stage of both algorithms, selections are made in
descending probability order: the most common password
Table 1: Run times for Algorithm 1
Min GP # Guesses Run Time, 12 cores
132M
1.6B
15.9B
136.3B
1092.7B
–
–
–
15 seconds
2 minutes
16 minutes
2.2 hours
17.3 hours
6 days (est)
1.5 months (est)
1 year (est)
−9
−10
−11
−12
−13
−14
−15
−16
10
10
10
10
10
10
10
10
structures are tried ﬁrst, and similarly for dictionary words
and strings built from the Markov chain’s probability tables.
This guides the program to more likely passwords early on.
Modifying Algorithm 1 to utilize multiple processors is a
simple matter of splitting the password structures into P
groups, where P is the number of processors to use. This
approach to distributing the work load is both simple and ef-
fective and can easily be implemented on a distributed basis.
Assuming an eﬃcient method of splitting and distributing
the password structures to the processors, if P is increased
by a factor of K, run time would be reduced by that same
factor. This is true only to a point, however: if there were as
many processors as structures, the run time from processor
to processor would vary greatly, depending on the structure
assigned. Nonetheless, an attacker with a large network or
botnet of PCs would be formidable.
Both the number of guesses actually made and the run
time for a range of GPs are shown in Table 14. The PC
used for these results has a 12-core Intel i7 CPU running
at 3.20GHz. The run time reﬂects the elapsed clock time
when all of the machine’s 12 cores were deployed. Based on
−15 can be considered fairly secure.
these results, a GP of 10
Consequently, in the remainder of this paper, passwords will
−16 to allow for an additional
be strengthened to a GP of 10
margin of safety.
4.4 Strengthening Algorithm
The basic strengthening algorithm used in this paper is
presented as Algorithm 3 which references a strengthening
database. The strengthening database refers to the collection
of password structures, Markov chains and dictionaries and
their associated probabilities as described in 4.2.
If these
same items are used in an attack, rather than in a strength-
ening system, they are referred to as a cracking database.
The way in which the strengthening database is built is
important, as will be detailed in Section 5.1.2. Passwords
in subset A (the original training data) are not strength-
ened and are fully processed into the strengthening database,
meaning that both the probability tables and dictionaries
are updated as described in Section 4.2. In contrast, when
subsets B and C are strengthened, both the original and
strengthened passwords are only partially processed into
the strengthening database, meaning that only the Markov
chain’s probability tables are updated, and the dictionaries
are not. The rationale for this will be detailed in Section
5.1.2.
For the analysis in this paper, it is assumed that any pass-
word successfully strengthened is accepted by the user. In
4Estimated times are based on a log-log regression.
133
Data: password list X, threshold guess probability TP,
number of edits N
Result: strengthed password list X’
create empty strengthening database SDB;
for each password PW in subset A of X do
fully process PW into SDB;
end
for each password PW in subsets B and C of X do
PW’ = PW;
thisGP = GP(PW’);
while thisGP > TP and maximum number of
attempts not exceeded do
make N edits to PW, yielding PW’;
thisGP = GP(PW’);
end
if thisGP ≤ TP then
output PW’ to X’;
partially process PW’ into SDB;
end
partially process PW into SDB;
end
Algorithm 3: Basic Strengthening Algorithm
practice, the user would have the opportunity to accept the
strengthened password or try again. Assuming all strength-
ened passwords are accepted is likely a best-case scenario: it
is possible that the subset of passwords approved by the user
may share (or lack) particular features, rendering those pass-
words more susceptible to guessing attacks. For instance,
users may be more accepting of an “X” inserted into their
password than a “|” or a diﬀerent letter.
Algorithm 3 is similar to the strengthening algorithm used
in [12], with no major conceptual diﬀerences:
there is a
training phase to build an initial strengthening database,
passwords are evaluated against this database, and both the
original user password and its strengthened counterpart are
incorporated into the database every invocation. At an im-
plementation level, there is a noteworthy diﬀerence: in [12],
every password is “fully processed” into the strengthening
database. However, those GP calculations utilized a static
dictionary for alphabetic strings, rather than a dictionary
dynamically built from the observed passwords. Hence, the
impact of the strengthening database being leaked is not
directly comparable to results presented in this paper.
While the strengthening algorithm in [10] could also apply
random edits to an initial password, there was a large con-
ceptual diﬀerence: passwords were altered without regard to
their initial strength. Consequently, while the ending pass-
word was almost certainly more secure than the initial pass-
word, there was no mechanism to guide the ending password
to a given level of strength.
4.5 Strengthened Datasets
A set of strengthened passwords was created by strength-
ening rockyou-1 and rockyou-2 according to Algorithm 3,
−16 or stronger.
applying one edit and targeting a GP of 10
A second set of strengthened passwords was created by run-
ning the original rockyou-1 and rockyou-2 sets through the
algorithm again, this time applying two edits. An edit could
consist of replacing one character with another, or inserting
a character into the password at any point with the decision
to replace or edit being randomly determined. As with Per-
suasive Text Passwords (PTP) [9, 10], no restrictions were
placed on the characters used in the edits – any printable
character (ASCII 32 to 126, inclusive) could be used.
In contrast, the authors of [12] placed constraints on the
edits: a character inserted into the middle of a string of digits
or symbols had to be of the same type, and no changes could
be made to an alpha string other than changing the case
of one of the letters. Thus, “password123” could be trans-
formed to “passwoRd123”, “password1213” or “password!123”
but not “pass5word123” or “password12h3”.
5. RESULTS
This section ﬁrst explores the ability of the strengthening
algorithm to prevent passwords from being guessed by a
PCFG-based attack. A second type of attack, guided brute
force (GBF) search, is then explored. The section concludes
with methods that can be taken to reduce the eﬀectiveness
of this second form of attack.
5.1 Resistance to PCFG-based Attacks
Algorithm 3 guarantees that every password which was
output met the required GP threshold, at the time it was
strengthened. However, when processed on a diﬀerent data
set (including the same strengthening database at a later
point in time), the calculated GP will vary. Thus, it is pos-
sible that a password deemed secure by the strengthening
database might be judged as weak when using a diﬀerent
set of data to measure strength. In this section, two pos-
sibilities for PCFG-based attacks are explored. In section
5.1.1, the viability of attacking the strengthened passwords
using out-of-band data is explored, while in section 5.1.2 the
impact of an accidental leak of the strengthening database
is investigated.
5.1.1 Attacking with Derived Data
A well-known attack vector is to train a password crack-
ing program on a leaked data set such as rockyou, and use
the derived data as the basis for an attack. However, it
would seem that a better course of action would be to take
the rockyou set, strengthen it using the known or derived
strengthening algorithm, and then use only those strength-
ened passwords as the training set for a password cracking
program. If the newly strengthened passwords are statisti-
cally similar to the passwords that are to be guessed, this
would be an eﬀective attack.
In analyzing the eﬀectiveness of this attack, rockyou-1 was
used as the basis to crack the passwords in rockyou-2. The
data needed by Algorithm 1 to mount a guessing attack
was calculated in two ways: from the original, unstrength-
ened rockyou-1 set and also from the rockyou-1 set that was
strengthened by Algorithm 3. In Table 2, a breakdown of
the GPs of the strengthened passwords under these attack
scenarios is given; the rows labeled Weak reﬂect the ﬁrst sce-
nario (using only the original rockyou-1 passwords), while
the rows labeled Strong reﬂect the second scenario (using
only strengthened rockyou-1 passwords). If the strengthen-
ing algorithm were perfect, the worst GP under any scenario
−16 and Table 2 would be
would be the level targeted, or 10
all 0s. Results are shown for both 1 and 2 edits.
The top panel in the table shows that, as expected, using
weak passwords as the basis for a PCFG-based attack on
strengthened passwords is largely ineﬀective. Even when
only 1 edit is applied, only 1.3% of the passwords could be
Table 2: GPs using Derived Data
−14 % 10
3.2
0.8
18.0
7.6
Edits % 10
1.3
0.3
2.5
0.4
−13 % 10
2.2
0.5
4.6
1.3
1
2
1
2
Data
Weak
Weak
Strong
Strong
−15
Table 3: GPs using Leaked Data
−15
All
All
–
–
Partial Edits % 10
0.0
0.0
58.2
28.2
21.2
6.4
1
2
1
2
1
2
Str.
Str.
−13 % 10
0.1
0.0
67.3
38.6
24.4
8.3
−14 % 10