1
Posw/ pol
(n=9,295)
Posw/o pol
(n=8,696)
95%
66%
25%
71%
20%
2%
87%
49%
12%
62%
16%
0%
TABLE IV: App analysis results for the app test set (n=40) and the percentages of practices’
occurrences in the full app set (n=17,991). More speciﬁcally, P os w/ pol and P os w/o pol are
showing what percentage of apps engage in a given practice for the subset of apps in the full app set
with a policy (n=9,295) and without a policy (n=8,696), respectively. We measure precision, recall,
and F-1 score for the positive and negative classes with the pos and neg subscripts designating the
respective scores.
using the same library [35]. As the top libraries have the
farthest reach [35] we focus on those. We used AppBrain [3]
to identify the ten most popular libraries by app count that
process device ID, location, or contact data. To the extent
we were able to obtain them we also analyzed previous
library versions dating back to 2011. After all, apps sometimes
continue to use older library versions even after the library
has been updated. For each library we opened a developer
account, created a sample app, and observed the data ﬂows
from the developer perspective. For these apps as well as
for a sample of Google Play store apps that implement the
selected libraries we additionally observed their behavior from
the outside by capturing and decrypting packets via a man-in-
the-middle attack and a fake certiﬁcate [54]. We also analyzed
library documentations. These exercises allowed us to evaluate
which data types were sent out to which third parties.
B. App Analysis Results
Performance Results for App Test Set. Before exploring the
analysis results for the full app set we discuss the performance
of our app analysis on a set of 40 apps (app test set), which
we selected randomly from the publishers in the policy test
set (to obtain corresponding app/policy test pairs for our
later performance analysis of potential privacy requirement
inconsistencies in § V-A). To check whether the data practices
in the test apps were correctly analyzed by our system we
dynamically observed and decrypted the data ﬂows from the
test apps to ﬁrst and third parties, performed a manual static
analysis for each test app with Androguard [20], and studied
the documentations of third party libraries. Thus, for example,
we were able to infer from the proper implementation of a
given library that data is shared as explained in the library’s
documentation. We did not measure performance based on
micro-benchmarks, such as DroidBench [4], as those do not
fully cover the data practices we are investigating.
In the context of potential inconsistencies (§ V-A) correctly
identifying positive instances of apps’ collection and sharing
practices is more relevant than identifying negative instances
because only practices that are occurring in an app need to be
covered in a policy. Thus, the results for the data practices with
rarely occurring positive test cases are especially noteworthy:
CC, SL, and SC all reached F-1pos = 1 indicating that our
static analysis is able to identify positive practices even if they
rarely occur. Further, the F-1pos scores, averaging to a mean
of 0.96, show the overall reliability of our approach. For all
practices the accuracy is also above the baseline of always
selecting the test set class that occurs the most for a given
practice. Overall, as shown in Table IV, our results demonstrate
the general reliability of our analysis.
Data Practice Results for Full App Set. For all six data
practices we ﬁnd a mean of 2.79 occurring practices per app
for apps with policies and 2.27 occurrences for apps without
policies. As all practices need to be described in a policy per
our privacy requirements (§ III-A), it is already clear that there
are substantial amounts of potential inconsistencies between
apps and policies simply due to missing policies. For example,
the SID practice was detected in 62% of apps that did not
have a policy (Table IV), which, consequently, appear to be
potentially non-compliant with privacy requirements. Further-
more, for apps that had a policy only 10% disclosed the SID
practice (Table II) while it occurred in 71% of apps (Table IV).
Thus, 61% of those apps are potentially non-compliant in this
regard. The only practices for which we cannot immediately
infer the existence of potential inconsistencies are the CC
and SC practices with policy disclosures of 56% and 6% and
occurrences in apps of 25% and 2%, respectively. We can think
of two reasons for this ﬁnding.
First,
to notify users of practices
there could be a higher sensitivity among app
publishers
related to con-
tact data compared to practices that
involve device iden-
tiﬁers and location data. Publishers may categorize con-
tact data more often as PII. Second, different from de-
vice ID and location data, contact information is often pro-
vided by the user through the app interface bypassing the
APIs that we consider for our static analysis (most notably,
the android.accounts.AccountManager.getAccounts
API). Thus, our result demonstrates that the analysis approach
has to be custom-tailored to each data type and that
the
user interface should receive heightened attention in future
works [62]. It also illustrates that our results only represent a
lower bound, particularly, for the sharing practices (SID, SL,
SC), which are limited to data sent to the ten publishers of the
libraries in Table III.
Limitations. There are various limitations of our static anal-
ysis. At the outset our approach is generally subject to the
same limitations that all static analysis techniques for Android
are facing, most notably, the difﬁculties of analyzing native
9
Practice
CID
CL
CC
SID
SL
SC
Acc
(n=40)
0.95
0.83
1
0.85
1
1
Accpol · Accapp
(n=40)
0.74
0.7
0.88
0.84
0.93
0.78
95% CI
(n=40)
0.83–0.99
0.67–0.93
0.91–1
0.7–0.94
0.91–1
0.91–1
Precpos
(n=40)
0.75
0.54
-
0.93
1
1
Recpos
(n=40)
1
1
-
0.74
1
1
F-1pos
(n=40)
0.86
0.7
-
0.82
1
1
F-1neg
(n=40)
0.97
0.88
1
0.87
1
1
MCC
(n=40)
0.84
0.65
-
0.71
1
1
TP, FP, TN, FN
(n=40)
6, 2, 32, 0
8, 7, 25, 0
0, 0, 40, 0
14, 1, 20, 5
3, 0, 37, 0
1, 0, 39, 0
Inconsistent
(n=9,050)
50%
41%
9%
63%
17%
2%
TABLE V: Results for identifying potential privacy requirement inconsistencies in the app/policy test set (n=40) and the
percentage of such potential inconsistencies for all 9,050 app/policy pairs (Inconsistent). Assuming independence of policy
and app accuracies, Accpol· Accapp, that is, the product of policy analysis accuracy (Table II) and app analysis accuracy
(Table IV), indicates worse results than the directly measured accuracy. The Matthews correlation coefﬁcient (MCC), which is
insightful for evaluating classiﬁers in skewed classes, indicates a positive correlation between observed and predicted classes.
code, obfuscated code, and indirect techniques (e.g., reﬂec-
tion). However, there are various considerations that ameliorate
exposure of our approach to these challenges. First, if an
app or a library uses native code, it cannot hide its access
to Android System APIs [35]. In addition, the use of native
code in ad libraries is minimal [45]. Indeed, we have rarely
encountered native code in our analysis. Similarly, the need to
interact with a variety of app developers effectively prohibits
the use of indirect techniques [9]. However, code obfuscation
in fact presents an obstacle. Our static analysis failed in 0.4%
(64/18,055) of all cases due to obfuscation (i.e., an app’s
Dex ﬁle completely in bytecode). However, our failure rate
improves over the closest comparable rate of 21% [62].
It is a further limitation of our approach that the identiﬁca-
tion of data practices occurs from the outside (e.g., server-side
code is not considered). While this limitation is not a problem
for companies’ analysis of their own apps, which we see as
a major application of our approach, it can become prevalent
for regulators, for instance. In many cases decrypting HTTPS
trafﬁc via a man-in-the-middle attack and a fake certiﬁcate will
shed some light. However, it appears that some publishers are
applying encryption inside their app or library. In those cases,
the analysis will need to rely on inferring the data practice in
question indirectly. For example, it remains possible to check
whether a library is properly implemented in an app according
to the library’s documentation, which lends evidence to the
inference that the app makes use of the documented practices.
Also, our results for the sharing practices only refer to
the ten third parties listed in Table III. The percentages for
sharing of contacts, device IDs, or locations would almost
certainly be higher if we would consider additional libraries.
In addition, our deﬁnition of sharing data with a third party
only encompasses sharing data with ad networks and analytics
libraries. However, as it was shown that ad libraries are the
recipients of data in 65% of all cases [35], we believe that this
deﬁnition covers a substantial portion of sharing practices. It
should be ﬁnally noted that our investigation does not include
collection or sharing of data that occurs through user input,
ofﬂine, or at app servers’ backends. However, as our analysis
already identiﬁes a substantial percentage of potentially non-
compliant apps, we think that there is value in our techniques
even with these limitations.
Runtime Performance. In terms of runtime performance,
using ten threads in parallel on an AWS EC2 instance
m4.10xlarge with 2.4 GHz Intel Xeon E5-2676 v3 (Haswell),
40 vCPU, and 160 GiB memory [2] the analysis of all 17,991
APKs took about 31 hours. The mean runtime is 6.2 seconds
per APK analysis.
V.
IDENTIFYING POTENTIAL INCONSISTENCIES
In this section we unite our policy (§ III) and app (§ IV)
analyses. We explore to which extent apps are potentially non-
compliant with privacy requirements (§ V-A) and show how
app metadata can be used to zoom in on sets of apps that have
a higher likelihood of non-compliance (§ V-B).
A. Potential Inconsistencies in Individual App/Policy Pairs
Potential Inconsistencies from a Developer’s Perspective.
As the results of a survey among app developers suggest a
lack of understanding privacy-best practices [7], it could be
that many of the potential
inconsistencies we encountered
are a consequence of this phenomenon as well. Especially,
many developers struggle to understand what type of data
third parties receive, and with limited time and resources even
self-described privacy advocates and security experts grapple
with implementing privacy and security protections [7]. In this
regard, our analysis approach can provide developers with a
valuable indicator for instances of potential non-compliance.
For identifying potential inconsistencies positive app classes
and negative policy classes are relevant. In other words, if a
data practice does not occur in an app, it does not need policy
coverage because there can be no inconsistency to begin with.
Similarly, if a user is notiﬁed about a data practice in a policy,
it is irrelevant whether the practice is implemented in the app
or not. Either way, the app is covered by the policy. Based on
these insights we analyze the performance of our approach.
Performance Results for App/Policy Test Set. To evaluate the
performance of our system for correctly identifying potential
privacy requirement inconsistencies we use a test set with
corresponding app/policy pairs (app/policy test set). The set
contains the 40 random apps from our app test set (§ IV-B)
and their associated policies from our policy test set (§ III-C3).
We associate an app and a policy if the app or its Play store
page links to the policy or if the policy explicitly declares
itself applicable to mobile apps. As only 23 policies satisfy this
requirement some policies are associated with multiple apps.
As shown in Table V, accuracy results range between 0.83
and 1 with a mean of 0.94. Although not fully comparable,
10
y
t
i
s
n
e
D
0.25
0.20
0.15
0.10
0.05
0.00
No. of Apps
2K
1.5K
1K
500
0
0
1
2
3
Number of Potential Privacy Requirement Inconsistencies
4
5
6
Fig. 6: For the full app/policy set (n = 9,050) we found that
2,455 apps have one potential inconsistency, 2,460 have two,
and only 1,461 adhere completely to their policy. Each app
exhibits a mean of 1.83 (16,536/9,050) potential inconsisten-
cies (with the following means per data practice: CID: 0.5,
CL: 0.41, CC: 0.09, SID: 0.63, SL: 0.17, SC: 0.02).
AsDroid achieved an accuracy of 0.79 for detecting stealthy
behavior [41] and Slavin et al. [62] report an accuracy of
0.8 for detecting discrepancies between app behavior and
policy descriptions. For the 240 classiﬁcation instances in the
app/policy test set—that is, classifying six practices for each
of the 40 app/policy pairs—our system correctly identiﬁed
32 potential inconsistencies (TP). It also returned ﬁve false
negatives (FN), 10 false positives (FP), and 193 true negatives