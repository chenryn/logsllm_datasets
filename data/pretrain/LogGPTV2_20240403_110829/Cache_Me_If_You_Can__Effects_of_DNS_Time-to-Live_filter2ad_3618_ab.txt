120 s
20190215
8974
8882
92
15845
285555
282001
281931
70
.uy-NS-new
600
2h
NS .uy
172800 s
86,400
20190304
8682
8536
96
15325
184243
184243
184209
34
Table 2: Resolver’s centricity experiments. Datasets avail-
able at [43].
We see three different TTLs: 172800 s (48 hours) for NS and
address records at the root, 3600 and 43200 s (1 and 12 hours) at the
.cl authoritative servers, and 43200 s when we explicitly ask the
name server itself for its own address record. Which TTL is used
depends on the implementation of the recursive resolver; although
RFC2181 [15] specifies the client’s TTL should take priority, but it
does not require recursive resolvers to actively fetch that value.
A second factor is that response components are returned in
different DNS message sections [33], and may be treated differently
by different implementations. Records are marked authoritative
(.cl’s NS record at the root), as answer (.cl’s NS record at .cl), or
additional (A records attached to the NS response at .cl).
Answers from the child have higher priority when the Authori-
tative Answer flag (AA flag) is set, though when a server resolves
the domain example.cl, it may choose to never contact the child
and instead may use the authority and additional records returned
by the parent. (For example, to resolve example.cl, a resolver can
use the A record of a.nic.cl as provided by the Roots in Table 1.)
This question has been previously defined as resolvers’ centric-
ity [13, 17, 18, 42]: resolvers using the TTL provided by the par-
ent authoritative (such as the Roots for .cl) servers are defined
as parent-centric, while child-centric resolvers will use the child
authoritative Original DNS specifications were unclear on which
to prefer, and RFC2181 clarified child records (and their TTLs) as
higher priority, but did not require resolvers to query for them [15].
Only DNSSEC validation requires fetching records from the child
zone, but DNSSEC validating resolver deployment is incomplete
today.
Resolvers employing in-resolver authoritative mirroring tech-
nologies, such as RFC7706 [29] or LocalRoot [20], or serving stale
content [30] (i.e., answering queries past TTL expiration only when
the NS records for the given domain are unresponsive) will exhibit
different perceived TTL caching behaviors. In the former case, re-
solvers implementing RFC7706 or LocalRoot, entire zones will be
transferred into a pseudo-authoritative server that runs in parallel
with a recursive resolver; no queries to these zones will likely be
seen exiting the recursive resolver [20], though questions to their
children will still be sent. For the latter case, resolvers serving stale
content, outgoing requests will likely continue to be seen on the
wire, but even when unanswered, resolvers will continue serving
(expired) answers to clients.
This example illustrates the complexity of the TTLs used by
different implementations. We next look at how these rules work
in practice.
103
3.2 TTLs as Seen in the Wild with Uruguay’s
.uy top-level domain
We next consider Uruguay’s country-code TLD .uy. We select
Uruguay because it’s ccTLD has two very different TTL values
in its NS record: 172800 s at the root, and only 300 s in their own
authoritative server (as of 2019-02-14), and 120 s for that server’s A
record.
These large differences allow us to study their effects on caching
from “the wild”. We use RIPE Atlas [44, 45], measuring each unique
resolver as seen from their ∼10k probes physically distributed
around the world. Atlas Probes are distributed across 3.3k ASes,
with about one third hosting multiple vantage points (VPs). Many
Atlas probes have multiple recursive resolvers, sometimes at differ-
ent locations, so we treat each combination of probe and unique
recursive resolver as a VP, since potentially each represents a differ-
ent perspective. We therefore see about 15k VPs from about 9k Atlas
Probes, with the exact number varying by experiment to do small
changes in probe and resolver availability. This definition of VP pro-
vides a dynamic view of what resolvers Atlas is using; it has some
overlap, due to Atlas probes that share resolvers, and changes over
time, due to complex recursive infrastructure [48]. It is also affected
by non-uniform distribution of RIPE Atlas probes [9]; we report
latency by region in Figure 10b to account for this distribution.
We make queries first for the NS record of .uy, then the A records
of its authoritative server a.nic.uy. In each case, we query from
each VP every 10 minutes (twice the shortest TTL of the NS records),
for either two or three hours (for the NS or A records). For each
query, we look for the TTL as seen in the answer section of the DNS
response, obtaining about 190k and 280k valid responses from .uy-
NS and a.nic.uy-A experiments, respectively. Table 2 summarizes
the experiments (we disregard responses that did not return the
answers we expected, typically from probes with hijacked DNS
traffic [35]).
Figure 1 shows the CDF of the valid TTLs from all VPs for .uy.
Even though previous studies have shown that DNS TTLs are some-
times manipulated by resolvers [48], we found that such manipu-
lation is very rare for TTLs shorter than 1h, at least as seen from
RIPE Atlas VPs [36]. (These results are also not affected by recursive
resolvers, shared, split, or existing caches, since our query intervals
are longer than the TTLs: 600 s vs 120 s and 300 s.)
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
G. C. M. Moura et al.
Figure 1: TTLs from VPs for .uy-NS and a.nic.uy-A queries.
Figure 3: CDF of A queries per resolver/query name.
Figure 2: TTLs from VPs for .google.co-NS queries.
Figure 4: CDF of minimum interarrival time of A queries
from each resolver/query-name.
As such, the vast majority of responses in Figure 1 follow the
child’s value, not the parent’s: 90% of .uy-NS are less than 300 s,
and 88% of a.nic.uy-A are less than 120 s. We conclude that most
resolvers are child-centric, preferring the TTL of the authoritative
server (following RFC2181 §5.4.1 [15]).
Roughly 10% of resolvers appear to be parent-centric, following
the 2-day TTL of the root zone (or these resolvers are manipulating
TTLs [19, 36]). In fact, about 2.9% of .uy-NS and 2.2% of a.nic.uy-A
show the full 172800 s TTL. Some of these resolvers include probes
using the OpenDNS public resolvers [39]. Later queries sent to
these resolvers confirm they are parent-centric for domains in the
Root zone (likely implementing RFC7706 [29]), and when the child
delegation’s authoritative name server is unreachable (§4.4).
Besides, we also found only one VP that had TTL values larger
than the parent delegation.
3.3 A Second-level Domain in the Wild
To confirm our observations that client-centric TTL preferences ex-
tend past top-level domains (RFC7706 does not apply to second-level
domains [29]), we repeat the experiment from §3.2 for google.co,
which is a popular second-level domain (SLD). The domain google.co
has two TTL values for its NS records: 900 s from the parent servers
(.co), and 345600 from the actual authoritative servers ns[1-4].
google.com (as of 2019-03-05). We query every 600 s, for one hour
(Table 2).
Figure 2 shows the CDF of observed TTLs for this second-level
domain, for 16k VPs. About 70% of all answers have TTLs longer
than 900 s—results that must come from the child authoritative
server. About 15% of all answers, many using Google public DNS,
have TTLs of 21,599 s, suggesting TTL capping. About 9% of all
answers have a TTL of exactly 900 s, suggesting a fresh value from
the parent authoritative server.
104
This experiment shows that resolvers querying second-level
domains, are similar to those querying TLDs, most choosing child-
centric TTLs.
3.4 Confirming Client-Centricity with Passive
Observations of .nl TLD
Prior sections showed that specific domains are mostly client-
centric, observing from the authoritative side, looking at who is
querying and what strategy they use (parent- or child-centric). Here
we study passive data for the the Netherlands zone, .nl, with about
5.8 million domain names in its zone [49].
At the time of this experiment (2019-03-06 to -07), the .nl ccTLD
had four authoritative servers (sns-pb.isc.org and ns[1-3].dns.nl),
each with multiple IP anycast sites [1]. We gather DNS data from
ns[1,3].dns.nl servers using ENTRADA [57], which saw more
than 6.5M queries for the two-day period. The ns[1,3].dns.nl A
records are listed in the parent zone (the root zone) with glue
records containing TTL values of 172800 s (2 days). The children’s
authoritative servers, however, contain only a 3600 s (1 hour) TTL
for the same A records.
We examine query interarrivals for each resolver to classify that
resolver as parent- or child centric. We find about 205k unique
resolver IP addresses, providing 13× more VPs than our experiment
using RIPE Atlas (§3.2 and §3.3).
We see 368k groups of resolver, query-name pairs, in which query-
name is one of the four NS records for .nl pairs, and we compute
the interarrival time between queries for each group. (In §3.2 we
considered client-side VPs; here instead our authority-side consid-
ers resolvers instead of Atlas probes, and pairs them with query
names since different records may have different TTLs in the cache.)
Figure 3 shows the CDF of the number of queries for each group
for the aggregate queries (the solid blue “all” line), and for those
 0 0.2 0.4 0.6 0.8 1 5 10 50 120 300 1000CDF TTL AnswersAnswers TTL(s)NS queriesA queries 0 0.2 0.4 0.6 0.8 1102103104105CDFTTL (s)TTL 900sTTL 345600s 0 0.2 0.4 0.6 0.8 1 1 2 5 10 20 50CDFQueries per IP-qnamefilteredall 0 0.2 0.4 0.6 0.8 1 1 2 5 10 20 50CDFInterarrival time (h)TTL 3600sTTL 173800sCache Me If You Can
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
queries where the interarrival time is more than 2 s (the red “filtered”
line). This filtering aims at removing duplicate queries that are
retransmissions, but we see that the curves are essentially identical.
More than half of the groups appear to be child-centric, since
52% send more than one query over the two days, suggesting they
are following the shorter child TTL. Another possible explanation
is that some recursive resolvers cap TTLs to less than 2 days (some
versions of BIND [25], however, use one week as the default maxi-
mum caching time).
Just less than half (about 48%) send only one query during ob-
servation. Since we only observe two of the four authoritative
servers, it is possible these resolvers made queries to non-observed
authoritative servers. (It is known that resolvers tend to rotate be-
tween authoritative servers [37]). Another possibility is that these
resolvers did not need to handle multiple queries for names under
.nl.
To investigate if these resolvers, which sent only one query
per query-name, are indeed parent-centric, we extract the unique
source IPs from the groups that sent only one query; which gives
us 122562 unique IP addresses. Around 14% of these IPs are also
present in groups that sent more than one query for other names.
For example, an IP that queries once for ns1.dns.nl, but queries 3
times for ns2.dns.nl. This suggests that at least 14% of the resolvers
in this group behave as child centric as well.
We gain greater confidence how many resolvers are child-centric
by looking at the minimum interarrival time for resolvers that send
multiple queries for the same name in Figure 4. Even observing only
two of the four authoritatives, we can conclude that most resolvers
use the child TTL. We also see “bumps” around multiples of one
hour. We believe that these bumps are resolvers returning to the
same server after the TTL expires.
We conclude that, even when observed from the authoritatives,
at least half recursive resolvers are child-centric.
4 THE MANY TTLS IN RESOLVING A
FULLY-QUALIFIED DOMAIN-NAME
We next turn to the second problem from §2: how do the differ-
ent parts of a FQDN, records (NS and A or AAAA), answer types
(authoritative answer, authority, and additional), and server config-
urations (in and out-of-bailiwick) interact to influence the effective
TTL lifetime of the originating request? Again, our goal is to un-
derstand which TTL or TTLs control caching.
We see which depend on each other through two controlled ex-
periments: one with an in-bailiwick server, ns1cachetest.net, and
the other with an out-of-bailiwick server. (We run these experi-
ments on different days to avoid interference.)
The key results of this section are to show that it does matter
where the authoritative server is located in the DNS hierarchy. For
in-bailiwick authoritative servers glue records drive cache
lifetimes, and the TTLs of the IP address and authoritative server
are frequently linked (a still valid A record will still expire when
its covering NS record expires). By contrast, out-of-bailiwick
servers use cached information about authoritative servers
for the full TTL lifetime.
105
A
...
M
t = 9min: redirect
to new ns3.sub
ns1
ns2
ns3.sub
ns3.sub
VP
.net
NS cachetest.net 172800
A ns[1,2].cachetest.net: 172800
cachetest.net
NS cachetest.net 3600
A ns[1,2].cachetest.net: 3600
NS sub.cachetest.net: 3600
A ns3.sub.cachetest.net: 7200
sub.cachetest.net
NS sub.cachetest.net: 3600
A ns3.sub.cachetest.net: 7200
clients
e.g.: Atlas Probes
Figure 5: TTLs and domains for in-bailiwick experi-
ment [43]. Italics indicate glue records.
4.1 Experimental Setup
Our experiments use a test domain (cachetest.net, from [36]) over
which we have complete control. This domain has two authoritative
servers: ns[1,2]cachetest.net, as can be seen in Figure 5, both
running Debian and BIND 9.1 on EC2 in Frankfurt, Germany.
We add this domain to the parent .net zone, which requires
adding both NS records in .net for our domain and glue records
for the addresses of our authoritative servers (italic in Figure 5). By
default, records in .net have a TTL of 172800 s, or 2 days.
Our cachetest.net one is run on authoritative servers running
in EC2 VMs. (We run our own DNS servers in our own VMs and
do not use Amazon’s Route53 [5] hosted DNS.) We set the TTLs
for the NS and A records in our zone to 3600 s.
As can be seen in Figure 5, recursive resolvers will find two
different TTLs for the same record (at both parent and child). Even
though most resolvers are expected to be child-centric (§3), for sake
of precision, we decided to rule out this influence by creating the
sub.cachetest.net zone. We configure this subzone in two different
experiments using a third dedicated EC2 VM also in Frankfurt.
4.2 Effective TTLs for Servers Inside the Served
Zone
We first look at how resolvers handle authoritative servers with
names in the served zone—those that are in-bailiwick. We show
that most recursives require both fresh NS and A records, and they
re-fetch even valid A records when the NS record expires.
For this experiment, we configure ns1.subcachetest.net as an
authoritative server for our subzone (sub.cachetest.net). We set
the TTL of its NS record to 3600 s and its A record TTL to 7200 s.
These TTLs are consistent in both the parent and child zones, so
recursives will have the same cache duration regardless of which
they prefer.
At t = 9 min., we renumber ns3.subcachetest.net, changing
its IP address to a different EC2 VM. This new VM also serves
this zone, but with some changes to the records so that old and
new authoritative servers return different answers. This end-to-end
check allows us to determine how caching works in between.
We test this potential dependency by querying the AAAA record
of PROBEID.sub.cachetest.net from all RIPE Atlas VPs every 600 s,
watching for the returned answer to change. Since the authorita-
tive’s NS and A records have different TTLs, the time at which
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
G. C. M. Moura et al.
Frequency
Duration
Query