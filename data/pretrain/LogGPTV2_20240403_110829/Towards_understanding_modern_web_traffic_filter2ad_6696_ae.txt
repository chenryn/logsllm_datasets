audio.
Ideal Byte Hit Rate We calculate the ideal bandwidth savings
achievable with a centralized proxy cache having inﬁnite cache
storage by the traditional object-level HTTP caching and content-
based caching. For object-level caching, we decide if an object is
cacheable by respecting cache-control headers.
If cacheable, we
check if the URLs and content lengths match as in Section 5.2. We
also calculate a slightly optimistic behavior of object-based caching
by discarding query strings from URLs in order to accommodate
the case where two URLs with different metadata actually belong
to the same object. For content-based caching, we vary the aver-
age chunk size from 128 bytes, 1 KB, 8 KB, to 64 KB. Note that
we apply content-based caching on compressed content without de-
compressing it, because the volume of compressed content such as
gzip or deflate is less than 1% in our data set.
In Figure 14, we observe that content-based caching with any
chunk size outperforms object-based caching. The cache hit rate of
object-level caching ranges from 27.0-37.1% (not shown in the ﬁg-
ure), but the actual byte hit rate is only 16.8-28.1%, which is lower
than the byte hit rates from a decade ago, but similar to that in more
recent studies [3, 13, 24, 30, 40, 63]. The hit rate of the optimistic
version (HTTP-OPT) is only slightly larger. On the other hand,
the lowest byte hit rate of content-based caching is 29.4-38.4%
with 64 KB chunks, and the highest byte hit rate is 42.0-50.6%
with 128 byte chunks, 1.8-2.5x larger than object-level caching’s
byte hit rate. The small chunk size performs better than the large
chunk sizes because of its ﬁner granularity. For example, 128 bytes
chunks can detect redundancy at the sentence-level, but 64 KB can
do only at the document-level.
Impact of Content Types Among many different content types,
we ﬁnd that text resources such as HTML, JavaScript, XML, and
CSS have much higher redundancy than binary resources such as
image, audio, video, and octet-stream. Figure 15 shows the ideal
redundancy by content type. In particular, JavaScript shows over
90% of redundancy with the smallest chunk size of 128 bytes. On
the other hand, video exhibits much lower redundancy of 20%,
illustrating the impact of long-tailed popularity in video content.
Object-based caching performs very poorly, and its redundancy
elimination for XML is one-eighth that of the gains with 128 byte
chunks in China.
We also ﬁnd that content-based caching works well regardless of
the content types, while the object-based caching is mainly effec-
tive for JavaScript and image trafﬁc only. Figure 16 depicts the con-
tribution of byte savings, basically showing which caching scheme
works best for which content type.
In object-based caching, the
contribution of JavaScript and image is relatively larger than that of
other content types. It should be noted that the contribution of bi-
nary resources such as video, audio, and octet-stream is extremely
low, implying that object-based caching is not suitable for them.
On the other hand, content-based caching provides more uniform
beneﬁts.
5.4 Origins of Redundancy
In order to understand how content-based caching approaches
provide more than double the byte hit rate than the object-based
caching approach, we quantify the contribution of redundancy from
different sources in Figure 17. We use the average chunk size of
128-bytes for this analysis.
Overall, we observe that about 40.3-58.6% of the total redun-
dancy is due to identical objects with the same URLs, which is
essentially the upper bound of the object-based caching approach’s
byte hit rate (object-hit). The other half of the total redun-
dancy is purely from the content-based caching approaches, and we
further break it into the following three sources. First, there exists
redundancy across the content changes of an object (intra-URL),
and it accounts for about 21.8-32.5% of the total redundancy. Sec-
ond, some objects with different URLs actually have identical con-
tent [32] (aliasing), and it represents 6.7-9.8% of the total re-
dundancy. Finally, the rest is due to the redundancy across dif-
ferent objects that have different URLs and non-identical content
(inter-URL), and it represents 12.8-20.0% of the total redun-
dancy. This analysis result implies that most of the additional sav-
ings from the content-based caching approaches come from its abil-
304)
%
(
y
c
n
a
d
n
u
d
e
R
 100
 75
 50
 25
 0
ht
m
l
ja
v
x
m
l
c
s
s
a
s
c
ript
HTTP
64-KB
8-KB
1-KB
128-B
i
m
a
g
e
o
ctet
a
u
dio
)
%
(
y
c
n
a
d
n
u
d
e
R
 100
 75
 50
 25
 0
vid
e
o
ht
m
l
ja
v
x
m
l
c
s
s
a
s
c
ript
HTTP
64-KB
8-KB
1-KB
128-B
i
m
a
g
e
o
ctet
a
u
dio
vid
e
o
(a) US
(b) CN
Figure 15: Ideal redundancy by content types: Text resources have higher redundancy than binary.
)
%
(
n
o
i
t
u
b
i
r
t
n
o
C
g
n
v
a
S
e
i
t
y
B
 100
 80
 60
 40
 20
 0
H
video
audio
octet
image
css
xml
javascript
html
)
%
(
n
o
i
t
u
b
i
r
t
n
o
C
g
n
v
a
S
e
i
t
y
B
 100
 80
 60
 40
 20
 0
H
video
audio
octet
image
css
xml
javascript
html
)
%
(
n
o
i
t
u
b
i
r
t
n
o
C
g
n
v
a
S
e
i
t
y
B
 100
 80
 60
 40
 20
 0
H
video
audio
octet
image
css
xml
javascript
html
6
T
T
4
-
K
P
B
8
-
K
1
-
K
1
2
B
B
8
-
B
6
T
T
4
-
K
8
-
K
1
-
K
1
2
B
B
P
B
8
-
B
6
T
T
4
-
K
8
-
K
1
-
K
1
2
B
B
P
B
8
-
B
(a) US
(b) CN
(c) BR
Figure 16: Byte saving contribution by content types: Content-based caching is effective for any content type, but object-based
caching works well only for JavaScript and image.
ity to detect and eliminate the redundancy in the content changes
of an object as well as redundancy across different objects.
In terms of content types, we ﬁnd that HTML and XML gener-
ally show relatively higher intra-URL redundancy than other con-
tent types.
It implies that they are frequently updated but their
content changes slowly. Also, aliasing in general accounts for a
small amount of the total redundancy, but we observe a signiﬁcant
amount of aliasing in XML and audio content types in Brazil. This
is again because of the popular use of the real time updates of sports
games (XML) and live streaming of audio in Brazil. These objects
have identical content but with different URLs. Finally, we see
that most of the redundancy in binary resources, especially video,
come from partial content overlaps (intra-URL + inter-URL) rather
than complete object matches (object-hit + aliasing). This is partly
because they are aborted before they are fully downloaded. We
examine the aborted transfers in more detail in Section 5.6.
5.5 Cache Storage Size
We simulate cache behavior with different cache storage sizes to
determine the required cache storage size for achieving close to the
ideal byte hit rate, but also include the metadata overhead (20 bytes
per chunk) of content-based caching in the byte hit rate calculation.
We use a simple LRU cache replacement policy as a ﬁrst step, and
leave for future work investigating more sophisticated policies [46].
In addition to object-based and content-based caching, we also
simulate multi-resolution chunking (MRC), a recently-developed
strategy that simultaneously exploits multiple chunk sizes [28] and
is well-suited for large storage sizes. MRC always favors large
chunks over small ones, and uses small chunks only when large
chunks are cache misses. It also caches all different chunk sizes for
the same content redundantly for the future reference. This way,
MRC minimizes the metadata overhead, disk accesses, and mem-
ory pressure at the cost of more disk space.
Figure 18 shows our simulation results in the United States and
China, which shows that content-based caching always outperforms
object-based caching regardless of cache storage size. However,
due to the signiﬁcant metadata overhead for ﬁxed 128 bytes chunks,
the actual byte hit rate of 128 byte chunks is similar to that of 1 KB
chunks. The saturation point of cache size is similar across the dif-
ferent caching approaches except for MRC. For example, beyond
100 GB of cache storage, the byte hit rate no longer increases in
the United States and China. The saturation point essentially indi-
cates the working set size of the trafﬁc, so increasing the cache size
beyond it gives no further beneﬁts. On the other hand, while MRC
performs relatively poorly when cache storage is small, it continues
to increase the byte hit rate beyond the saturation point, as the mul-
tiple chunk sizes reduce metadata overhead. The simulation has a
few missing data points because of the limitation of main memory
we have (16 GB) during the simulation. Also, the byte hit rate of
MRC with inﬁnite cache size is estimated from the ideal byte hit
rate of 128 byte chunks minus 1% overhead.
While increasing cache storage size gives diminishing returns
for object-based caching, using large cache storage with MRC is
highly beneﬁcial as it doubles the byte hit rate compared to object-
based caching. This option would be especially attractive in devel-
oping regions where the bandwidth is much more expensive than
disk storage [27]. Since a TB-sized disk costs less than $100, it
makes sense to allocate much more cache storage than was used 10
years ago, when disk sizes were in the tens of GB.
In our data set, we need about 800 GB for the United States and
China, 200 GB for France, and 400 GB for Brazil to achieve close
to the ideal byte hit rate with MRC. It is roughly four times of the
total trafﬁc size because MRC uses four different chunk sizes in
305)
%
(
y
c
n
a
d
n
u
d
e
R
f
o
s
n
g
i
r
i
O
 100
 75
 50
 25
 0
all
inter-URL
aliasing
intra-URL
object-hit
)
%
(
y
c
n
a