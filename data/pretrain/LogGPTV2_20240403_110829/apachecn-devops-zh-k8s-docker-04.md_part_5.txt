```
defaults
  log global
  mode tcp
  timeout connect 5000
  timeout client 50000
  timeout server 50000
frontend workers_https
  bind *:443
  mode tcp
  use_backend ingress_https
backend ingress_https
  option httpchk GET /healthz
  mode tcp
  server worker $worker1:443 check port 80
  server worker2 $worker2:443 check port 80
  server worker3 $worker3:443 check port 80
```
这告诉 HAProxy 创建一个名为`workers_https`的前端，并为传入的请求绑定 IP 地址和端口，使用 TCP 模式，并使用名为`ingress_https`的后端。
`ingress_https`后端包括使用端口 443 作为目的地的三个工作节点。检查端口是一个健康检查，将测试端口 80。如果服务器在端口 80 上回复，它将被添加为请求的目标。虽然这是 HTTPS 端口 443 规则，但我们仅使用端口 80 来检查来自 NGINX pod 的网络回复:
```
frontend workers_http
  bind *:80
  use_backend ingress_http
backend ingress_http
  mode http
  option httpchk GET /healthz
  server worker $worker1:80 check port 80
  server worker2 $worker2:80 check port 80
  server worker3 $worker3:80 check port 80
```
这个`frontend`部分创建了一个前端，接受端口 80 上的传入 HTTP 流量。然后，它将后端名为`ingress_http`的服务器列表用于端点。就像在 HTTPS 部分一样，我们使用端口 80 来检查在端口 80 上运行服务的任何节点。任何回复检查的端点都将被添加为 HTTP 流量的目的地，任何没有运行 NGINX 的节点都不会回复，这意味着它们不会被添加为目的地:
```
EOF
```
这就结束了我们文件的创建。最终文件将在`HAProxy`目录中创建:
```
# Start the HAProxy Container for the Worker Nodes
docker run --name HAProxy-workers-lb -d -p 80:80 -p 443:443 -v ~/HAProxy:/usr/local/etc/HAProxy:ro HAProxy -f /usr/local/etc/HAProxy/HAProxy.cfg
```
最后一步是用我们创建的包含三个工作节点的配置文件启动一个运行 HAProxy 的 Docker 容器，这三个工作节点暴露在端口 80 和 443 上的 Docker 主机上。
现在，您已经学习了如何为您的工作节点安装定制的 HAProxy 负载平衡器，让我们看看配置是如何工作的。
## 了解高速公路交通流量
集群将总共运行八个容器。其中六个容器将是标准的 Kubernetes 组件；即三个控制平面服务器和三个工作节点。另外两个容器是 KinD 的 HAProxy 服务器和您自己定制的 HAProxy 容器:
![Figure 4.13 – Custom HAProxy container running ](img/Fig_4.13_B15514.jpg)
图 4.13–定制的 HAProxy 容器正在运行
在练习中，这个集群输出与我们的双节点集群有一些不同。请注意，工作节点没有暴露在任何主机端口上。工作节点不需要任何映射，因为我们的新 HAProxy 服务器正在运行。如果您查看我们创建的 HAProxy 容器，它会暴露在主机端口 80 和 443 上。这意味着在端口 80 或 443 上对主机的任何传入请求都将被定向到定制的 HAProxy 容器。
默认的 NGINX 部署只有一个副本，这意味着入口控制器运行在单个节点上。如果我们查看 HAProxy 容器的日志，我们会看到一些有趣的东西:
```
 [NOTICE] 093/191701 (1) : New worker #1 (6) forked
[WARNING] 093/191701 (6) : Server ingress_https/worker is DOWN, reason: Layer4 connection problem, info: "SSL handshake failure (Connection refused)", check duration: 0ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[WARNING] 093/191702 (6) : Server ingress_https/worker3 is DOWN, reason: Layer4 connection problem, info: "SSL handshake failure (Connection refused)", check duration: 0ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[WARNING] 093/191702 (6) : Server ingress_http/worker is DOWN, reason: Layer4 connection problem, info: "Connection refused", check duration: 0ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[WARNING] 093/191703 (6) : Server ingress_http/worker3 is DOWN, reason: Layer4 connection problem, info: "Connection refused", check duration: 0ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
```
您可能已经注意到日志中的一些错误，例如 SSL 握手失败和`Connection refused`。虽然这些看起来确实像错误，但它们实际上是工作节点上的失败检查事件。请记住，NGINX 仅在单个 pod 中运行，由于我们的 HAProxy 后端配置中有所有三个节点，它将检查每个节点上的端口。任何未能回复的节点都不会用于负载平衡流量。在我们当前的配置中，这确实实现了负载平衡，因为我们在一个节点上只有 NGINX。但是，它确实为入口控制器提供了高可用性。
如果您仔细查看日志输出，您将看到在一个定义的后端上有多少服务器处于活动状态；例如:
```
check duration: 0ms. 1 active and 0 backup servers left.
```
日志输出中的每个服务器池显示 1 个活动端点，因此我们知道 HAProxy 已经在端口 80 和 443 上成功找到了一个 NGINX 控制器。
为了找出 HAProxy 服务器连接到了什么工作器，我们可以使用日志中的失败连接。每个后端都会列出失败的连接。例如，根据其他两个工作节点显示为`DOWN`的日志，我们知道正在工作的节点是`cluster01-worker2`:
```
Server ingress_https/worker is DOWN Server ingress_https/worker3 is DOWN
```
让我们模拟一个节点故障来证明 HAProxy 为 NGINX 提供了高可用性。
## 模拟库伯莱故障
请记住，KinD 节点是短暂的，停止任何容器可能会导致它在重新启动时失败。那么，既然我们不能简单地停止容器，我们如何模拟工作节点故障呢？
为了模拟故障，我们可以在一个节点上停止 kubelet 服务，这会提醒`kube-apisever`，这样它就不会在该节点上安排任何额外的 pods。在我们的例子中，我们想证明 HAProxy 正在为 NGINX 提供高可用性支持。我们知道运行的容器在`worker2`上，所以这就是我们想要“取下来”的节点
停止`kubelet`最简单的方法是向容器发送`docker exec`命令:
```
docker exec cluster01-worker2 systemctl stop kubelet
```
您将看不到此命令的任何输出，但是如果您等待几分钟让群集接收更新的节点状态，您可以通过查看节点列表来验证该节点已关闭:
```
kubectl get nodes.
```
您将收到以下输出:
![Figure 4.14 – worker2 is in a NotReady state ](img/Fig_4.14_B15514.jpg)
图 4.14–工人 2 处于未就绪状态
这验证了我们刚刚模拟了 kubelet 故障，并且`worker2`处于`NotReady`状态。
在 kubelet“故障”之前运行的任何 pods 都将继续运行，但是`kube-scheduler`在 kubelet 问题解决之前不会在节点上调度任何工作负载。因为我们知道 pod 不会在节点上重新启动，所以我们可以删除 pod，以便它可以在不同的节点上重新计划。
您需要获取 pod 名称，然后将其删除以强制重新启动:
```
kubectl get pods -n ingress-nginx
nginx-ingress-controller-7d6bf88c86-r7ztq
kubectl delete pod nginx-ingress-controller-7d6bf88c86-r7ztq -n ingress-nginx
```
这将迫使调度程序在另一个工作节点上启动容器。这也将导致 HAProxy 容器更新后端列表，因为 NGINX 控制器已经移动到另一个工作节点。
如果你再次查看的 HAProxy 日志，你会发现 HAProxy 已经更新了后端以包括`cluster01-worker3`，并且从活动服务器列表中删除了`cluster01-worker2`:
```
[WARNING] 093/194006 (6) : Server ingress_https/worker3 is UP, reason: Layer7 check passed, code: 200, info: "OK", check duration: 4ms. 2 active and 0 backup servers online. 0 sessions requeued, 0 total in queue.
[WARNING] 093/194008 (6) : Server ingress_http/worker3 is UP, reason: Layer7 check passed, code: 200, info: "OK", check duration: 0ms. 2 active and 0 backup servers online. 0 sessions requeued, 0 total in queue.
[WARNING] 093/195130 (6) : Server ingress_http/worker2 is DOWN, reason: Layer4 timeout, check duration: 2000ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[WARNING] 093/195131 (6) : Server ingress_https/worker2 is DOWN, reason: Layer4 timeout, check duration: 2001ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
```
如果您计划使用该高可用性集群进行额外的测试，您将希望在`cluster01-worker2`上重新启动 kubelet。如果您计划删除高可用性集群，您只需运行一个 KinD 集群删除，所有节点都将被删除。
## 删除 HAProxy 容器
一旦你删除了你的 KinD 集群，你需要手动移除我们添加的 HAProxy 容器。由于 KinD 没有创建我们的自定义负载平衡器，删除集群不会删除容器。
要删除自定义 HAProxy 容器，运行`docker rm`命令强制删除映像:
```
docker rm HAProxy-workers-lb –force
```
这将停止容器，并将其从 Docker 的列表中删除，允许您使用与未来的 KinD 集群相同的名称再次运行它。
# 总结
在本章中，您了解了名为 KinD 的 Kubernetes SIG 项目。我们详细讨论了如何在 KinD 集群中安装可选组件，包括作为 CNI 的 Calico 和作为入口控制器的 NGINX。最后，我们介绍了 KinD 集群中包含的 Kubernetes 存储对象的细节。
希望在本章的帮助下，您现在理解了使用 KinD 可以给您和您的组织带来的力量。它提供了一个易于部署、完全可配置的 Kubernetes 集群。单个主机上运行的集群数量理论上仅受主机资源的限制。
在下一章中，我们将深入探讨 Kubernetes 对象。我们称之为下一章 *Kubernetes 训练营*，因为它将涵盖大部分的基本 Kubernetes 对象以及每个对象的用途。下一章可以被认为是“Kubernetes 袖珍指南”它包含对 Kubernetes 对象和它们的作用以及何时使用它们的快速引用。
这是一个打包的章节，旨在为那些有 Kubernetes 经验的人提供复习课程，或者为那些刚接触 Kubernetes 的人提供速成课程。我们写这本书的目的是超越基本的 Kubernetes 对象，因为现在市场上有很多书很好地涵盖了 Kubernetes 的基础知识。
# 问题
1.  What object must be created before you can create a `PersistentVolumeClaim`?
    A.聚氯乙烯
    B.唱片
    C.`PersistentVolume`
    D.`VirtualDisk`
2.  KinD includes a dynamic disk provisioner. What company created the provisioner?
    A.微软
    B.CNCF(消歧义)
    C.VMware
    D.大牧场主
3.  If you create a KinD cluster with multiple worker nodes, what would you install to direct traffic to each node?
    A.负载平衡器
    B.代理服务系统
    C.没有任何东西
    D.网络负载平衡器
4.  True or false: A Kubernetes cluster can only have one CSIdriver installed.
    A.真实的
    B.错误的*