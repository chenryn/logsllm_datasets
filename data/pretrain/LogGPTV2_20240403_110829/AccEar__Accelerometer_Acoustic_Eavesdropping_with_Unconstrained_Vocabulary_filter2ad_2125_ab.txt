Accelerometer is a three-axis sensor that accurately senses
and measures acceleration. It is one of the primary sensors
embedded into smartphones and has been widely used for
gaming, health tracking, and activity recognition [17]–[19].
An accelerometer consists of springs, ﬁx electrodes, and an
electrode on a movable seismic mass. When an acceleration
is applied along a certain direction, the movable mass moves to
the opposite direction, thus changing the capacitance between
ﬁxed electrodes. Then the accelerometer can calculate the
acceleration by measuring the changed capacitance. In our
work, when a built-in speaker plays the audio, it will produce
vibrations which will be propagated to the accelerometer via
the motherboard. And the vibrations induce a movement of
the accelerometer’s mass, registering acceleration.
Android operating system allows apps to access accelerom-
eter data at various sampling rates. By requesting the SEN-
SOR DELAY FASTEST mode [20], an app can acquire sen-
sor data at the maximum sampling rate. However, due to
the limitations posed by different smartphone manufactur-
ers,
the maximum sampling rate of the accelerometer for
this mode can vary between 416∼500Hz [4] on modern
smartphones (more details in Section V). According to
theorem, the accelerometer can only
N yquist
capture the information below 250Hz while the sampling rate
of the accelerometer is 500Hz. To be able to reconstruct the
information in high frequency, we introduce the concept of
phonemes.
sampling
Phonemes are the smallest phonological units divided ac-
cording to the natural properties of speech [21]. We take the
English language as an example, the phonemes in English are
classiﬁed into two categories: vowels and consonants [22]. The
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1759
(cid:42)
(cid:93)
(cid:42)(cid:11)(cid:93)(cid:12)
(cid:91)
(cid:39)
(cid:53)(cid:72)(cid:68)(cid:79)
(cid:82)(cid:85)
(cid:41)(cid:68)(cid:78)(cid:72)
Fig. 2: The architecture of Generative Adversarial Networks
(cid:92)
(cid:93)
(cid:42)
(cid:92)
(cid:42)(cid:11)(cid:93)(cid:95)(cid:92)(cid:12)
(cid:91)
(cid:39)
(cid:53)(cid:72)(cid:68)(cid:79)
(cid:82)(cid:85)
(cid:41)(cid:68)(cid:78)(cid:72)
Fig. 3: The architecture of conditional Generative Adversarial
Networks
number of vowels is 20 and their energy mainly distributes
below 2000Hz, while the number of consonants is 28 and their
energy mainly distribute below 8000Hz [23]. However, the
accelerometer can only capture limited speech information due
to the restricted sampling rate. Fig. 1 shows the spectrogram
of the accelerometer data and corresponding spectrogram of
audio for vowels and consonants. We can observe there exist
unique patterns for each phoneme on the spectrograms of both
accelerometer and audio. Based on this observation, we can
devise an approach which learns the mappings between the
accelerometer data and the audio. Besides, it should have
the capability to automatically generate the missing high-
frequency components with the low-frequency accelerometer
data based on the previously learned mappings.
Generative Adversarial Networks (GAN) [24] is a ma-
chine learning method that engages a game between two neural
networks, namely, a generator G and a discriminator D. As
shown in Fig. 2, G aims to generate new data (such as image,
music, etc) from a noise vector z, while D aims to discriminate
the G(z) based on the ground truth x. During the training
process, G constantly evolves to generate new data to try
to deceive D as if it is real. Similarly, D also evolves to
discriminate the data generated by G as fake. The training
process terminates until D cannot differentiate between real
and the “fake” data generated by G. This implies the data
generated by G is indistinguishable from the ground truth.
However, conventional GANs lack the capability to generate
new data that meets desired constraints or conditions. A
conditional GAN (cGAN) [6], which architecture is shown
in Fig. 3, allows us to deﬁne a condition y on the input data
for a GAN. Different from the traditional GAN, the generator
G aims to generate data G(z|y) from a noise vector z but
under the input condition y. Besides, the discriminator D still
aims to discriminate the generated data from the ground truth
x. However, D also maps G(z|y) to the original data x via
the condition y. In the training process, G aims to learn such
a mapping and generate data that can deceit D. Therefore,
cGAN is a good candidate which can generate the lost high-
frequency components based on low-frequency accelerometer
data (condition).
IV. OUR AUDIO EAVESDROPPING ATTACK
In our proposed attack, the accelerometer is used to eaves-
drop on the audio played by the built-in speaker on a victim’s
smartphone. The whole process for the attack and its modules
are shown in Fig. 4. In this section, we ﬁrst deﬁne the
threat model and assumptions for our attack. We then describe
in detail the two major components of our attack: feature
extraction and speech reconstruction.
A. Threat model
In our threat model, we assume a spyware has been installed
on the victim’s smartphone that collects the accelerometer data
in the background. When the built-in speaker of the victim’s
phone plays the sound, the spyware records accelerometer data
on all three-axis at the maximum sampling rate in the back-
ground. Hence, the attacker can access the raw accelerometer
data to carry out the eavesdropping attack. We only focus on
accelerometer data since such sensor has higher sensitivity
than the gyroscope, as pointed out by previous research [4].
Different from the other related works, we assume the attacker
has no prior information about the audio playing from the
victim speaker, which implies there is no pre-established
vocabulary. It is worth noting that we carry out our attack on
the victim’s phone independently from internal and external
factors. For this reason, we assess its effectiveness under
several settings, such as the smartphone’s manufacturer and
model, audio output volume from the speaker, position (lying
on a table or hand-held), user movements (still or walking),
and real-world scenario (e.g. quiet room, restaurant, street).
B. Feature Extraction
In this module, we apply several processing steps to the
raw accelerometer data to derive a proper representation as
the input for our speech reconstruction module.
Zero-mean normalization: The raw accelerometer mea-
surements along x, y, z axis have different baseline value. For
example, the baseline value of z-axis is about 9.8 due to
the earth gravity while the other axes are 0. To exclude the
inﬂuence of earth gravity, we apply zero-mean normalization
to the raw data as follows,
sij − ¯si
σ
sij =
(1)
where the sij represents the j-th sample of the i-th axis, and
i = 1, 2, 3 denotes the x, y, z axis respectively, the ¯si denotes
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1760
Acceleration
raw 
data
Feature Extraction
Speech Reconstruction
Data
Preprocessing
Spectrogram
Ground 
Truth
cGAN
Model
cGAN
Training
Mel 
Spectrogram
High-quality 
Speech
Attacker
Inference
Fig. 4: The architecture of AccEar system
(a) Before High-pass ﬁlter
(b) After High-pass ﬁlter
Fig. 5: Spectrogram of accelerometer data with human move-
ment
the mean value of si, and the σ denotes the standard deviation
of si. After zero-mean normalization, the mean value of the
data for each axis is zero under stationary scenario.
High-pass ﬁlter: In real-world scenarios, human activities
could signiﬁcantly inﬂuence the accelerometer data. Fig. 5(a)
shows the spectrogram of accelerometer data with human
movement. We can observe that the human movement cor-
responds to a dominant component
in the low frequency.
Hence we use a high-pass ﬁlter with a threshold of 20Hz
to remove the impact of human movement1 while preserving
as much speech information as possible. The spectrogram of
the accelerometer signal after applying the high-pass ﬁlter is
shown in Fig. 5(b). The major difference between the original
and ﬁltered signals is that the high-frequency speech-related
components can be presented clearly after ﬁltering out the
low-frequency movement-based components.
Interpolation: As mentioned in Section III, the Android
operating system provides various sampling rate modes. How-
ever,
the system does not guarantee a ﬁxed time interval
between two measurements. To solve this problem, we apply
the linear interpolation approach to the accelerometer data
to ﬁll
the missing data. After interpolation, we obtain a
constant sampling rate at 1kHz for the accelerometer data. It
1The fundamental frequency of human speech is above 85Hz and the
perceptible frequency by the human ear is above 20Hz, the human activities
rarely affect the frequency components above 80Hz [4].
Fig. 6: Accelerometer data response to the played audio.
is worth noting that while the interpolation ﬁxes the unstable
time intervals in the original accelerometer data, it does not
introduce extra speech information [4].
Signal-to-spectrogram of Accelerometer data: After the
above steps, our accelerometer data is still three temporal
signals (one for each axis). As the input of cGAN requires
a two-dimensional image, we convert the accelerometer data
on the most responsive axis to an image-like spectrogram.
By comparing the waveform of the original audio with the
correspondent accelerometer data (as shown in Fig. 6), we can
observe that of z-axis is more responsive and less noisy than
x and y axes. Therefore, we choose the z-axis accelerometer
signal for the next conversion steps.
We divide the accelerometer signal into the ﬁxed length
segments of four seconds and apply the Short-Time Fourier
transform (STFT) on each segment as follows,
ST F T{s(t)}(τ, ω) ≡ S(τ, ω)
(cid:2) ∞
=
−∞
s(t)w(t − τ )e
−iωtdt
(2)
where w(τ ) is the window function (Hann window is ap-
plied in this work), and s(t) is the accelerometer data to be
transformed. S(τ, ω) is the Fourier transform of s(t)w(t − τ )
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1761
(cid:93)(cid:29)(cid:3)(cid:85)(cid:68)(cid:80)(cid:71)(cid:82)(cid:80)(cid:3)
(cid:81)(cid:82)(cid:76)(cid:86)(cid:72)
(cid:91)(cid:29)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:87)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)
(cid:82)(cid:73)(cid:3)(cid:3)(cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:3)(cid:68)(cid:88)(cid:71)(cid:76)(cid:82)
(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)
(cid:42)(cid:11)(cid:93)(cid:95)(cid:92)(cid:12)
(cid:92)(cid:29)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:87)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:82)(cid:73)(cid:3)(cid:3)
(cid:68)(cid:70)(cid:70)(cid:72)(cid:79)(cid:72)(cid:85)(cid:82)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)
(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:82)(cid:85)
(cid:92)(cid:29)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:87)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:82)(cid:73)(cid:3)(cid:3)
(cid:68)(cid:70)(cid:70)(cid:72)(cid:79)(cid:72)(cid:85)(cid:82)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)
(cid:39)(cid:76)(cid:86)(cid:70)(cid:85)(cid:76)(cid:80)(cid:76)(cid:81)(cid:68)(cid:87)(cid:82)(cid:85)
(cid:39)(cid:76)(cid:86)
(cid:41)(cid:68)(cid:78)(cid:72)
(cid:53)(cid:72)(cid:68)(cid:79)
Fig. 7: Networks architecture of our conditional Generative Adversarial Network for AccEar.
which represents the phase and amplitude of the signal over
time and frequency.
After the STFT, we obtain the spectral characteristics of
accelerometer data. Due to the magnitude of the spectral
characteristics is close to zero, we take a square root of
the STFT results. Then we perform the normalization on the
spectral characteristics to speed up the convergence of cGAN
in the audio reconstruction module.
Audio-to-spectrogram conversion: The audio reconstruc-
tion module requires the original audio as ground truth for
model training. Therefore, we also convert the original audio
into an image-like spectrogram following a similar process.
However, different from the above signal-to-spectrogram con-
version, we convert the audio signal to a Mel spectrogram.
The mathematical relationship between the ordinary frequency
scale and the Mel frequency scale can be expressed as follows
[25],
Mel(f ) = 2595 ∗ log10(1 + f /700)
(3)
where f refers to the frequency. This conversion is necessary
since the perception in a human ear is not linear in terms of
frequency. In particular, the human ear is more sensitive to low
frequencies than high frequencies [26]. The Mel scale [25] is
the nonlinear transformation of frequency which distorts the
original audio frequency for better human perception.
C. Speech Reconstruction
The purpose of eavesdropping is to reconstruct the original
audio via the accelerometer data. We adopt a GAN variant
to enhance the spectrogram of the accelerometer data via the
generation of the high-frequency features, which are absent
from such signal.
conditional Generative Adversarial Networks (cGAN): As
we mentioned above, traditional GAN can only generate the
new data close to the training samples from random noise.
However, our main purpose is to transform the spectrogram
of accelerometer data to the Mel spectrogram of corresponding
audio. To enable the model to generate the corresponding
Mel spectrogram according to the different spectrogram of ac-
celerometer data, we refer the conditional GAN approach and
take the spectrogram of accelerometer data as the condition.
Fig. 7 illustrates our network architecture of cGAN. The
input for our cGAN is the ground truth x (i.e.,
the Mel
spectrogram of original audio) and the condition y (i.e., the
spectrogram of accelerometer data). From the combination
of a noise vector z and condition y, the generator G gen-
erates G(z|y) as one of the inputs for the discriminator D.
Additionally,
the ground truth x and the condition y are
combined as another input of D, which represents the real
image under condition y. During the joint training process, D
tries to discriminate the G(z|y) from the ground truth x|y
while G tries to adjust its network parameters to generate
a G(z|y) which can fool D. For each phoneme in a word,
G automatically learns the mapping from accelerometer data
spectral features to speech spectral features through the zero-
sum game between G and D. Once the training process
completed, the generator G can correctly reconstruct a word
pronunciation via the accelerometer data, even if the word does
not appear in our training set.
Objective: To enable our reconstructed audio more closely
to the original audio, we deﬁne the loss function of magnitude
spectrogram of generated audio signals and original audio
signals [27]. It can be expressed as
LS = (cid:5)S(t, f ) − Sp(t, f )(cid:5)1 , t ∈ T, f ∈ F
(4)
where S(t, f ) and Sp(t, f ) are the magnitude spectrogram
representation of the generated audio signals and original
audio signals respectively.
According to cGAN [6], the generator G aims to minimize
log(1−D(G(z | y))) while discriminator D aims to maximize
log(1− D(G(z | y))), as if they are following the two-player
min-max game. The objective of the cGAN is as follows.
VcGAN (D, G) =Ex[log D(x | y)]+
min
G
max
D
Ez[log(1 − D(G(z | y)))]
(5)
where x is the ground truth, y is the condition, and z is the
noise prior. Combining the loss function of signals magnitude
and the objective of conditional GAN, our ﬁnal objective is
∗
L
= (cid:5)S(t, f ) − Sp(t, f )(cid:5)1 +
Ex∼pdata (x)[log D(x | y)]+
Ez∼pz(z)[log(1 − D(G(z | y)))], t ∈ T, f ∈ F
(6)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1762
5:
6:
7:
8:
9:
Generator Architecture: Traditional Encoder-Decoder net-
work in generator needs all information ﬂows to pass through
all layers. However, in the image to image translation prob-
lems, inputs and outputs are shared on the low-level infor-
mation that does not need to be considered for conversion
[28]. Therefore, it will increase the calculated costs and time
costs if we adopt the traditional Encoder-Decoder network.
To address this problem, we use U-Net [29] as the network
architecture of the generator. The whole U-Net architecture is
symmetrical, layers on the left are convolutional layers and
on the right are upsampling layers. The convolutional layers
extract the feature with square kernels of size 4× 4 and stride
value 2, and when the image passes a convolutional layer, its
size will be changed. The upsampling layers predict the pixel
label by decoding the feature. Different from the traditional
Encoder-Decoder network, the feature maps obtained from
each convolutional layer are concatenated to the corresponding
upsampling layer so that the feature maps of each layer can
be effectively used in subsequent calculations, this is known
as skip connections (the gray dashed line in the left panel of
Fig.7).
Discriminator Architecture: Our discriminator has three
convolutional layers. Different from the general discriminator,
we only discriminate the image at the scale of patches instead
of the entire image. It tries to classify each 30 × 30 patch in