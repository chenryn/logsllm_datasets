state transits unexpectedly from UNPAID to CANCELED, which
is an invalid ﬁnal state for a payment request. On the other
hand, Steve realizes that, in addition to the payment request
(line 1), the logs also report a cancellation request (line 2)
for the same order. Consequently, he suspects that the failure
might be related to the concurrent execution of both requests.
To clarify the suspicion that the cancellation request was the
culprit of the invalid state change, Steve decides to render the
execution causal graph in ShiViz [24], a popular space-time
diagram visualizer compatible with Horus.
Figure 4c illustrates the diagram rendered by ShiViz. It
comprises four process timelines regarding the Launcher,
Payment, Cancel and Order services, respectively. Each thick-
bordered number identiﬁes the corresponding LOG event in
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
// Find events that denote the beginning of the payment request and the noticed error.
MATCH
  (reqSnd:SND {host: ’Launcher’})-->(:RCV {host: ’Payment’}),
  (reqError:LOG {host: ’Launcher’})
WHERE
  reqError.message CONTAINS 'java.lang.RuntimeException: [Error Queue]’
  reqError.lamportLogicalTime > reqSnd.lamportLogicalTime 
WITH
  reqSnd.lamportLogicalTime as reqSndTime,
  min(reqError.lamportLogicalTime) as reqErrorTime 
MATCH
  (reqSnd:EVENT {host: ’Launcher’, lamportLogicalTime: reqSndTime}),
  (reqError:EVENT {host: ’Launcher’, lamportLogicalTime: reqErrorTime}) 
// getCausalGraph(startNode, endNode, onlyLogs) 
CALL horus.getCausalGraph(reqSnd, reqError, true) yield node
WITH reqSnd, reqError, node ORDER BY node.lamportLogicalTime ASC
WITH
  reqSnd.eventId as startEventId,
  reqError.eventId as endEventId,
  collect(node) as logs 
// Return:
// 1. startEventId: the event that denotes the start of the payment request
// 2. endEventId: the event that denotes the noticed error
// 3. logs: log messages containing the order identifier. 
UNWIND logs as log
WITH startEventId, endEventId, log
WHERE log.message CONTAINS ’652aaf9b’
RETURN startEventId, endEventId, collect(log.message) as logs 
a) Query to retrieve the events that denote the beginning of the payment request concerning 
the order '652aaf9b’ and the noticed error. The returned result contains the events’ 
identiﬁers and the logs containing the order identiﬁer.
// startEventId: hipster-1.europe-west2-c.c.horus-262311.internal64523
// endEventId: ts-launcher-LOG-584277764938
[Payment-1.1] - [URI:/pay][Request: {"orderId":"652aaf9b"}] 
[Cancel-1.1] - [URI:/cancelOrder][Request: {"orderId":"652aaf9b"}]
[Order-1.1] - [URI:/getById][Request: {"orderId":"652aaf9b"}] 
[Order-1.2] - Response: {"status":true, order":{"id":"652aaf9b", "status":"UNPAID"}}
[Order-2.1] - [URI:/getById][Request: {"orderId":"652aaf9b"}]
[Order-2.2] - Response: {"status":true, order":{"id":"652aaf9b", "status":"CANCELED"}}
[Payment-1.2] - Response: "false"
[Payment-2.1] - [URI:/drawBack][Request: {"userId":"c01d7008"}] 
[Payment-2.2] - Response: "true"  
[Cancel-1.2] - Response: {"status":true, "message":"Success."}
1
2
3
4
5
6
7
8
9
10
Launcher Payment Cancel Order
1
2
3
4
5
6
7
8
9
10
b) Causally-ordered log messages from the moment the request starts to the moment the 
error is noticed for order 652aaf9b. Each log statement is preﬁxed with its originating 
service’s name and its thread’s local counter.  
c) Space-time diagram rendered 
by ShiViz for the failed 
payment request.
Fig. 4: Reﬁnement queries, written in Cypher, used to debug the F13 failure, along with a snippet of the ShiViz diagram
generated for the causal graph produced by Horus after processing the queries.
Figure 4b. The execution path of the payment request under
analysis is highlighted in red.
In the diagram, Steve observes that log messages 1 and 2
appear at the same level, which conﬁrms that both payment
and cancellation requests started concurrently. Since both
requests depend on the order’s status, the diagram shows each
one issuing a getById request to the Order service ( 3 and
5 ). However, while the response sent to the Cancel service
indicated the status UNPAID ( 4 ), the one sent to the Payment
service revealed that the order status was CANCELED ( 6 ). Steve
thus conﬁrms his suspicion: somehow the order state changed
between events 4 and 5 .
From a closer look at the diagram, he identiﬁes that, after
receiving the order details, the Cancel service issued another
request back to the Order service, this time to update the order
state to CANCELED (see the red dashed box). Since this request
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
219
executed right before the arrival of the Payment’s request,
the order was already canceled at the time of the fetching.
However, the application requirements state that users cannot
pay orders that are already canceled.
Steve was ﬁnally able to explain the root cause of the failure:
as the order status was changed after the payment had started,
the payment request received the response false ( 7 ), which
then raised an exception in the Launcher service. The fact that
the TrainTicket permits a payment request to be performed
after an order cancellation represents an order violation, a
common type of distributed concurrency bugs.
To ﬁx this incorrect behavior once for all, Steve implements
a synchronization mechanism to enforce the correct execution
between payment and cancellation requests. It took a full day
of work, but he successfully managed to mark the issue ticket
as ”resolved” and accomplish his goal.
VII. PERFORMANCE EVALUATION
In this section, we conduct an experimental evaluation
of Horus aimed at assessing the beneﬁts and limitations of
our system with respect to: i) the scalability of the event
processing pipeline; ii) the performance of the logical time
assignment algorithm, and iii) the performance impact of
leveraging logical time during query processing.
To enable experiments with causal graphs of different sizes,
we implemented a synthetic event generator that mimicks an
arbitrary number of rounds of a synchronous client-server
scenario. In detail, it generates request-reply interactions be-
tween two processes P1 and P2 by creating the causal pairs
→ RCV P1. The output of this
SNDP1
micro-benchmark is thus an execution causal graph with N
events and 3N /2 − 2 edges in total (comprising both intra-
and inter-process edges).
→ RCVP2 and SNDP2
In the rest of this section, we describe the environment setup
and discuss the results for each experimental scenario.
A. Event Processing
Event processing consists of determining intra- and inter-
process causality for each event and storing the event and its
corresponding causal relationships in the graph database.
In this experiment, we evaluate how Horus scale in terms
of the number of processed events per time unit.
We conﬁgured two instances in Google Cloud Platform. The
ﬁrst instance is of type n1-standard-16 (16 vCPUs and 60GB
RAM) and hosts the Horus’ pipeline. The other instance, n1-
standard-8 (8 vCPUs and 30GB RAM), hosts event generator
clients. Each experiment ran for 15 minutes, with stress clients
performing an intensive workload by submitting as many
events as possible to Horus. The ﬂush interval is set to 100ms
and 200ms for events and causal relationships, respectively.
Figure 5 illustrates the evolution of the event processing
throughput as the amount of clients increases. The dashed line
indicates the incoming event rate measured in Apache Kafka.
Horus follows the incoming rate until the 18 clients setting,
which produces close to 6,000 events/second. Note that this
is a stress scenario, as the 18 clients generate in less than 4
t
u
p
h
g
u
o
r
h
T
)
c
e
s
/
s
t
n
e
v
e
(
 8k
 7k
 6k
 5k
 4k
 3k
 2k
 1k
 0 
 2  4  6  8  10  12  14  16  18  20  22
Clients
Incoming Rate
Horus
Fig. 5: Horus throughput as the number of clients increases.
The incoming event rate reveals the amount of events produced
by running clients in a stress scenario.
seconds the same amount of events generated by TrainTicket
in 6 minutes. Thus, Horus would scale to a setup with 1500+
microservices performing the same workload as TrainTicket.
When Horus reaches its maximum throughput, pending
events still remain in the event queues. Therefore, Horus will
still be able to process all events generated during workload
peaks and make them available for analysis, even with delay.
Horus architecture, however, allows for scale-out of causal-
ity encoders within the pipeline. To achieve this, one must
conﬁgure Horus to ensure the following: i) all events in a pro-
cess are processed by the same intra-process causality encoder,
thus preserving the program order; ii) the events belonging to
a causal pair are delivered to the same inter-process causality
encoder; and iii) the program order is preserved from the
intra-process encoder to the inter-process encoder. This allows
distributing event processing among several encoders without
requiring synchronization and still guarantees correctness in
constructing the causal execution graph.
In short, this experiment shows how many events can be
handled in real-time with a single event-processing server,
while knowing that, even when this rate is exceeded momen-
tarily, no events are lost.
B. Logical Time Assignment
Horus introduces an algorithm based on graph traversal to
assign logical time to events in the stored causal graph.
In this experiment, we compare the performance of the
Horus’ algorithm with the Falcon’s approach that, in turn,
resorts to a state-of-the-art SMT constraint solver to causally
order logs. We ﬁrst populated the Horus database using the
synthetic event generator. Then, we exported the unordered
events in the format compatible with the Falcon’s solver.
Figure 6 illustrates the evolution of the execution time of the
Falcon’s solver and the Horus’ algorithm for execution graphs
with different sizes. The Falcon’s solver depicts exponential
behavior as graph size increases whereas Horus evolutes
slightly linearly.
Falcon is thus unable to assign logical time to more than few
thousands of events in a timely manner. It spends more than
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
220
)
c
e
s
(
e
m
i
T
 1000
 100
 10
 1
 0.1
758.19
100.92
14.84
0.45
0.23
7.08
3.54
1.78
0.89
2500
5000 10000 20000 40000 80000
Execution Graph Size (#events)
Falcon Solver
Horus
 1000
)
s
m
(
e
m
i
T
 100
 10
109.0
3.46
1.89
1.84
5.04
1.78
 1
 100 
 1k
 10k
 100k
Execution Graph Size (#events)
Horus
Shortest Path
Fig. 6: Comparison between the execution time of Falcon and
Horus to assign logical clocks on different graph sizes.
Fig. 7: Comparison between the execution time of the shortest
path algorithm and Horus to answer query Q1, for different
graph sizes.
12 minutes for 10,000+ events, while the Horus’ algorithm
spends 7 seconds to assign the logical time.
This algorithm can perform with low execution time, even
when graph increases over time. In fact, running in a periodic
basis, the algorithm is able to resume the procedure from the
most recent event of each process timeline already with logical
timestamps and proceeding to the recently added events. Thus,
the execution time does not depend on the total amount of
events in the graph but the amount of unprocessed events.
In summary, the execution time of the proposed logical time
assignment algorithm scales with the amount of events and
causal relationships in the execution graph and thus is suitable
for real use cases.
C. Causal Graph Querying
Horus enables developers to reﬁne the causal graph using
the database’s built-in query language, as it provides ﬁltering
and grouping operations for graph algorithms. As discussed in
Section V, the built-in algorithms are not efﬁcient to answer
graph reﬁnement queries such as Q1 and Q2. Horus addresses
this limitation by annotating events with logical time.
In this section, we conduct separate evaluations for Q1 and
Q2 query types. For each type, we compose the corresponding
Cypher queries on each approach, one resorting to built-in path
traversal algorithms and the other leveraging logical time.
We setup a n1-standard-16 instance that hosts the Horus
pipeline, alongside the Neo4j database. For each experiment,
we populated the database using the synthetic event generator.
a) Evaluation for Q1.: The Cypher query that follows a
path-traversal-based approach makes use of the shortest path
algorithm to determine whether a causal path between the two