title:: Practical Cache Attacks from the Network
author:Michael Kurth and
Ben Gras and
Dennis Andriesse and
Cristiano Giuffrida and
Herbert Bos and
Kaveh Razavi
2020 IEEE Symposium on Security and Privacy
NetCAT: Practical Cache Attacks from the Network
Michael Kurth∗§, Ben Gras∗, Dennis Andriesse∗, Cristiano Giuffrida∗, Herbert Bos∗, and Kaveh Razavi∗
∗Department of Computer Science
Vrije Universiteit Amsterdam, The Netherlands
PI:EMAIL, PI:EMAIL, PI:EMAIL
{kaveh, herbertb, giuffrida}@cs.vu.nl
§Department of Computer Science
ETH Zurich, Switzerland
PI:EMAIL
Abstract—Increased peripheral performance is causing strain
on the memory subsystem of modern processors. For example,
available DRAM throughput can no longer sustain the trafﬁc
of a modern network card. Scrambling to deliver the promised
performance, instead of transferring peripheral data to and from
DRAM, modern Intel processors perform I/O operations directly
on the Last Level Cache (LLC). While Direct Cache Access
(DCA) instead of Direct Memory Access (DMA) is a sensible
performance optimization, it is unfortunately implemented with-
out care for security, as the LLC is now shared between the CPU
and all the attached devices, including the network card.
In this paper, we reverse engineer the behavior of DCA, widely
referred to as Data-Direct I/O (DDIO), on recent Intel processors
and present its ﬁrst security analysis. Based on our analysis, we
present NetCAT, the ﬁrst Network-based PRIME+PROBE Cache
Attack on the processor’s LLC of a remote machine. We show
that NetCAT not only enables attacks in cooperative settings
where an attacker can build a covert channel between a network
client and a sandboxed server process (without network), but
more worryingly, in general adversarial settings. In such settings,
NetCAT can enable disclosure of network timing-based sensitive
information. As an example, we show a keystroke timing attack
on a victim SSH connection belonging to another client on
the target server. Our results should caution processor vendors
against unsupervised sharing of (additional) microarchitectural
components with peripherals exposed to malicious input.
I. INTRODUCTION
Different processes running on a CPU may share microar-
chitectural components such as CPU caches for reasons of
efﬁciency. Given that these processes may belong to different
security domains, this sharing violates process isolation at the
microarchitectural level. Many existing attacks show that an
attacker can examine the modiﬁcations made to the shared
microarchitectural state by a victim operation to derive secret
information. Examples include leaking secret information from
victim processes [1, 2, 3, 4, 5, 6, 7] and cloud virtual
machines [8, 9, 10, 11]. These attacks can even be launched
from JavaScript in the browser [12, 13, 14]. The underlying
assumption behind all these attacks is that the attacker needs
either code execution or the ability to leverage victim code
running on the target processor to be able to observe modiﬁ-
cations in the microarchitectural state.
In this paper, we challenge this assumption and show that
on modern Intel processors, any attached peripheral such as
the Network Interface Card (NIC) can directly manipulate and
observe the state of the processor’s Last-Level Cache (LLC).
This is possible because the processor enables peripherals to
perform Direct Cache Access (DCA) instead of Direct Mem-
ory Access (DMA) for improved I/O performance. We explore
the security implications of this widely-deployed mechanism
for the ﬁrst time and show that an attacker can abuse it to
leak sensitive information from any peripheral that is exposed
to malicious input. To exemplify the threat, our proof-of-
concept exploit, NetCAT, can target a victim client of a DCA-
enabled server to leak that client’s private keystrokes in an
SSH session.
Existing microarchitectural attacks To leak sensitive infor-
mation with a microarchitectural attack, the attacker needs
to be able to measure the modiﬁcation that a victim makes
to a part of the microarchitectural state. For example,
in
a PRIME+PROBE attack, the attacker ﬁrst primes a shared
resource to put it in a known state. In the second step, the
attacker probes the same resource set by accessing it again.
If the accesses are now slower,
it means that a victim’s
secret operation has accessed the resource. This observation
is enough to leak secret information like cryptographic keys
from a process or a VM running on the same processor.
Furthermore, similar attacks have been mounted by executing
JavaScript [12, 13, 14, 15] and even through the network when
interacting with a vulnerable process [16, 17]. The JavaScript-
based attacks, while strengthening the threat model by not
requiring native code execution, are amenable to sandbox-
level mitigations [18] and still require the victim to execute the
attacker’s JavaScript code. Truly remote, network-only, attacks
relax the requirement for code execution on the target machine,
whether JavaScript or native code, by instead interacting with
a remote vulnerable process that happens to contain speciﬁc
gadgets (or otherwise cooperates with the client) on the remote
processor. Because these attacks do not have direct visibility
of the CPU cache state, they require a large number of time-
consuming network measurements as well as a vulnerable
(or cooperative) victim process, making them hard to use in
practice.
NetCAT This paper shows that it is possible to detect a single
LLC cache hit or miss on a remote processor from the network
on modern Intel platforms that are equipped with DDIO since
2012. This is possible since data center networks have become
increasingly fast, to the point that they allow a remote process
© 2020, Michael Kurth. Under license to IEEE.
DOI 10.1109/SP40000.2020.00082
20
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:59 UTC from IEEE Xplore.  Restrictions apply. 
to observe the timing difference between a network packet
that is served from the remote processor’s cache versus a
packet served from memory as we show for the ﬁrst time. This
basic capability allows us to build NetCAT, a PRIME+PROBE
attack on a portion of the remote processor’s LLC. NetCAT
can observe the activity of the remote processor as well as
other network clients that send trafﬁc to the remote processor.
For instance, using these observations, NetCAT can perform a
keystroke timing analysis to recover words typed by a victim
client in an SSH session with the target server. Compared to a
native local attacker, NetCAT’s attack from across the network
only reduces the accuracy of the discovered keystrokes on
average by 11.7% by discovering inter-arrival of SSH packets
with a true positive rate of 85%.
For NetCAT to surgically manipulate the remote LLC state
and perform our end-to-end keystroke timing analysis, we
need to overcome a number of challenges. First, we must
reverse engineer how DDIO [19, 20, 21] (Intel’s DCA tech-
nology) interacts with the LLC, information that is undocu-
mented and unknown prior to our work. Second, to perform
PRIME+PROBE, we must blindly build remote eviction sets by
crafting the right sequences of network packets. We show how
a variation of the work by Oren et al. [14] can successfully
build eviction sets over the network. Third, to perform our end-
to-end attack, we must track when the remote machine receives
SSH packets from the victim client. We describe a novel cache
set tracking algorithm that recovers the state of the NIC’s ring
buffer, which we use to track distinct SSH trafﬁc. We show
that the extracted cache activity is strong enough to perform
the necessary keystroke timing analysis and to recover typed
words successfully.
Contributions In summary, our contributions are as follows:
• We reverse engineer and provide the ﬁrst detailed analysis
of Intel DDIO, a technology that directly places I/O trafﬁc
in the processor’s LLC.
• We implement NetCAT, a practical network-based
PRIME+PROBE attack on the LLC of a remote processor.
• We implement an end-to-end keystroke timing attack
using NetCAT on the SSH session of a victim client
on the target server. A demo of the attack and addi-
tional information about NetCAT is available at https:
//www.vusec.net/projects/netcat.
II. BACKGROUND
In this section, we discuss the memory hierarchy, general
cache attacks and DCA, all of which are building blocks
for NetCAT. Furthermore, we discuss existing remote cache
attacks.
A. Memory Hierarchy
In order to speed up accesses to main memory, most
commodity processor architectures have multiple levels of
caching. The caches that are accessed ﬁrst (closer to the CPU
core) are usually smaller but faster than the caches closer to the
main memory. The caches are in place to leverage spatial and
temporal access patterns. In recent commodity processors, we
21
often ﬁnd a three-level hierarchy in which each CPU core has a
dedicated Level 1 (L1) and Level 2 (L2) cache. Moreover, the
CPU cores share a single Last Level Cache (LLC). Because
it is shared among cores, the LLC has a special role in cross-
core data accesses and recently also in PCIe data exchange,
as we discuss later in this section. Aside from cache speed
and size, cache design involves key properties such as cache
inclusivity versus exclusivity (or non-inclusivity) with respect
to other caches. As an example, in prior Intel server processors,
the LLC was inclusive with respect to L2, meaning that the
LLC had copies of all L2 cache lines. This changed with the
Skylake X microarchitecture, where the LLC is non-inclusive
with respect to L2, so that a cache line from L2 may not exist
in LLC.
B. Cache Attacks
Cache attacks belong to the more general class of microar-
chitectural attacks. The broad idea is to exploit the use of
shared resources on or around the CPU. An attacker leverages
these shared resources to typically steal (leak) information.
In cache attacks, the attacker builds a side channel based on
the timing information that can be observed in data fetches
from the different levels of caches or main memory. Such
timing information can be misused to gain information about
other processes and therefore reveal secrets. A successful side-
channel attack circumvents higher-level security mechanisms,
e.g., privilege separation.
Osvik et al. [1] pioneered the idea of PRIME+PROBE in
the context of L1 cache attacks. The PRIME+PROBE algorithm
consists of three steps: (1) Build cache eviction sets, (2) Prime:
Bring the cache to a known state by accessing the eviction
sets, (3) Probe: Access the eviction set again, during a victim
secret operation. Higher access times imply sets which the
victim process accessed.
Ristenpart et al. [22] used the PRIME+TRIGGER+PROBE
load measurement technique to detect keystroke activity on
L1 and L2 caches, allowing an attacker to infer activity on
virtual machines (VMs) that timeshare a core. Liu et al. [9]
extended PRIME+PROBE to the LLC, allowing the attacker to
extract secrets from co-hosted VMs without the need to share
the same core.
Browser-based Cache Attacks Browser-based cache attacks
strengthen the threat model of native code cache attacks
by executing from sandboxed JavaScript environments. They
exploit the fact that this JavaScript code executes logically
sandboxed but not at the microarchitectural level. Oren et
al. [14] introduced a non-canonical PRIME+PROBE attack from
JavaScript without any direct access to physical or virtual
addresses. Our eviction set building for NetCAT is based on
their approach. Gras et al. [13] used an EVICT+TIME attack
to break ASLR from JavaScript. Lipp et al. [23] launched a
keystroke timing attack from JavaScript to spy on the user
typing in the address bar. Frigo et al. [12] exploited the
integrated GPU with microarchitectural attacks to escape the
Firefox JavaScript sandbox on Android. All
these attacks
face a number of practical challenges such as needing a
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:59 UTC from IEEE Xplore.  Restrictions apply. 
CPU
Last Level Cache
1
y
a
W
2
y
a
W
3
y
a
W
4
y
a
W
...
7
1
y
a
W
9
1
y
a
W
0
2
y
a
W
Integrated Memory
Controller
PCIe Root Complex(cid:2)
y
r
o
m
e
M
n
a
M
i
PCIe Device 
(NIC, GPU, Storage) 
Fig. 1: Difference between direct cache access (orange) and direct
memory access (blue). Additionally, the available write allocation
cache lines for direct cache access in orange versus the others in
green.
high-precision timer and the presence of other sandbox-level
mitigations in modern browsers [18] and also require the
victim to execute the attacker’s JavaScript code.
C. Remote Cache Attacks
Existing, remote, network-only cache attacks use the fact
that they can observe the overall execution time after sending
a request to a web server. [24] shows a realistic remote-only
attack on OpenSSL. Bernstein [17] showed complete AES key
recovery from known-plaintext timings which Neve et al. [25]
further improve. Schwarz et al. [16] demonstrated a network-
based Spectre attack targeting speciﬁc code patterns (or gad-
gets) in a remote victim to disclose information. All of these
attacks are highly target-speciﬁc, and take a long time (hours to
days) due to the need to average over large amounts of network
packets to remove the network noise. Furthermore, they both
require a vulnerable (or otherwise cooperative) victim server
program containing speciﬁc gadgets. Such gadgets must ensure
the input-dependent operation accounts for most of the overall
execution time in order to leak information. In contrast, our
work is a generic, remote cache channel for which we show
a realistic and non-exhaustive list of example attack scenarios
that do not depend on the software target.
D. Direct Cache Access
In traditional architectures, where the NIC uses DMA, the
memory latency alone quickly becomes the bottleneck in
network I/O-focused workloads on 10 Gb/s interfaces [19].
To alleviate the DRAM bottleneck, [19] proposes DCA, an
architecture where PCIe devices can directly access the CPU’s
LLC. The DCA cache region is not dedicated or reserved
in the LLC, but allocating writes are statically limited to a
portion of the LLC to avoid thrashing caused by I/O bursts
or unconsumed data streams. Figure 1 illustrates a traditional
DMA access (the blue access ﬂow) versus a DCA access (the
orange access ﬂow).
Initially, Intel
implemented DCA using a prefetch hint
approach, in which a DMA write would trigger a memory
prefetch into the LLC after arriving in main memory, but this
required support from the device to hint at DCA and device
driver support to prefetch these DCA hints. Starting with the
Intel Xeon E5 and Xeon E7 v2 processor families in 2011,
server-grade CPUs implement DCA under the name of Data
Direct I/O Technology (DDIO) [20, 21, 26, 27], which is
entirely transparent to software and hardware. With DDIO,
a server machine is able to receive and send packet without
any trips to main memory in the optimal case. We will further
describe DDIO and reverse engineer its behavior in the next
sections.
III. THREAT MODEL
Our threat model targets victim servers with recent Intel
processors equipped with DDIO, enabled transparently by
default in all Intel server-grade processors since 2012. We
assume the attacker can interact with a target PCIe device
on the server, such as a NIC. For the purpose of instantiating
our attack in a practical scenario, we speciﬁcally assume the
attacker is on the same network as the victim server and can
send packets to the victim server’s NIC, thereby interacting
with the remote server’s DDIO feature. In particular, in our
example we launch a cache attack over the network to a target
server to leak secret information (such as keystrokes) from the
connection between the server and a different client. While we