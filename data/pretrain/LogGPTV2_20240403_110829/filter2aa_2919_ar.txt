as additional arguments to functions. Adding more arguments than a function expects does 
not change the functions behavior. However, it creates additional connections in the sea-of-
nodes which can lead to vulnerabilities as demonstrated in chapter 4.5.3 in the CVE-2020-
6418 vulnerability analysis. 
 118 
Using deterministic preprocessing the coverage was increased from 25.06 percent to 25.67 
percent which corresponds to 153,229 triggered edges of 596,937 possible edges. The 
final corpus contained 10,473 unique test cases. 
During deterministic preprocessing 596 crashes could be detected. Most of the crashes were 
due to native function calls and are therefore not exploitable. One exploitable vulnerability 
was identified, which was also detected by another researcher and was therefore a duplicate. 
More details can be found in chapter 5.4.1. 
Figure 9 visualizes the corpus coverage after the different processing stages. 
Figure 9: Corpus coverage at different stages 
 119 
5.2.2 Corpus of JavaScript code templates 
Changing assumptions within callbacks is the source of many bugs. Searching 
simultaneously for callback functions and code, which changes the correct assumption, 
results in a huge search space. Decoupling both actions into two separate tasks therefore 
significantly reduces the runtime to identify bugs. The identification of code, which changes 
assumptions, is performed during fuzzing. However, the identification of possible callback 
functions can already be performed in advance on the input corpus and with self-created 
code samples. 
To implement this, a new native JavaScript function was added to the source code of the v8 
engine. This function reflects its argument back to the fuzzer. The fuzzer can create code 
samples that call this function within possible callbacks. Since every function call uses a 
unique identifier as argument, the fuzzer can use the reflected feedback to calculate which 
function call was executed and can therefore identify callback functions. During later fuzzing, 
calls to this JavaScript function are replaced by fuzzed code. 
Such identified template files are stored in the second corpus. These templates specify at 
which locations the fuzzer should insert fuzzed code. To generate this template corpus, the 
fuzzer creates files such as the following: 
01: try { 
02: 
fuzzcommand("FUZZ_PRINT", "_PRINT_ID_1"); 
03: } catch (e) { 
04: 
fuzzcommand("FUZZ_PRINT", "_PRINT_ID_2"); 
05: } finally { 
06: 
fuzzcommand("FUZZ_PRINT", "_PRINT_ID_3"); 
07: } 
The fuzzcommand function is the native function that was added to the engine. It is based 
on the FUZZILLI_PRINT function from the fuzzilli fuzzer. The FUZZ_PRINT command sends 
the second argument back to the fuzzer when the code gets executed. In the above example, 
the _PRINT_ID_1 and _PRINT_ID_3 would be returned to the fuzzer and the fuzzer 
therefore knows that fuzzed code can be added at both locations. Since _PRINT_ID_2 was 
not reflected, the fuzzer does not add fuzzed code at this location. Instead, the print ID is 
changed to a random unique string before the template is added to the corpus. Changing 
the ID to a random unique string is important, otherwise combined templates, which are 
created during fuzzing, could have overlapping ID’s.  The generated template could look like: 
01: try { 
02: 
// Fuzz Code 
03: } catch (e) { 
04:  
fuzzcommand("FUZZ_PRINT", "_PRINT_ID_3af451dfc"); 
05: } finally { 
06: 
// Fuzz Code 
07: } 
During fuzzing, the fuzzer selects random templates and starts to add fuzzed code, for 
example, from the first corpus or from grammar definitions. When fuzzed code, which gets 
inserted into the above template in the second line, triggers an exception, the 
 120 
_PRINT_ID_3af451dfc string would get reflected. The fuzzer therefore uses the created 
sample as a new template because code at a new location was executed. Afterwards, the 
print command in the original template is removed to ensure that subsequent fuzzing does 
not detect the same template again.  
This approach allows to dynamically identify interesting code locations to reduce the search 
space. As example consider CVE-2015-6764 from chapter 4.3.2: 
01: var array = []; 
02: var funky = { 
03:     toJSON: function () { array.length = 1; gc(); return "funky"; } 
04: }; 
05: for (var i = 0; i < 10; i++) array[i] = i; 
06: array[0] = funky; 
07: JSON.stringify(array); 
Generating such a test case from an empty input in just one iteration is a time-consuming 
task because of the huge search space. The fuzzer would need to get lucky to correctly 
guess that the stringify functions accepts exactly one argument and that this argument must 
be an array. Moreover, that an element of the array must be an object with a custom defined 
toJSON callback function which then modifies the array length. A huge number of possible 
combinations of JavaScript code exists and therefore it would take a long time until the fuzzer 
finds accidently such an input.  
However, by using the above discussed approach, the search space can significantly be 
narrowed down. The fuzzer starts with the following simplified code:  
fuzzcommand("FUZZ_PRINT", Object.getOwnPropertyNames(this)); 
The code reflects back to the fuzzer all available global variables which includes the JSON 
object. Next, the fuzzer extracts the available functions for all these objects, including the 
JSON object: 
fuzzcommand("FUZZ_PRINT", Object.getOwnPropertyNames(JSON)); 
The fuzzer therefore knows that JSON.stringify() is one of the available methods. However, 
it does not know the number or types of arguments yet. Tradition fuzzers use grammar files 
which encode this information, however, creating such definitions is a time-consuming and 
error-prone task. To dynamically extract the information, the fuzzer starts a bruteforce 
approach where all possible combinations of arguments are tested. When the fuzzer passes 
an array as first argument, more code will get executed because the correct type and number 
of arguments were passed. This information is available to the fuzzer because of the edge 
coverage feedback mechanism. The fuzzer therefore knows how the function can be called 
and adds the code to the first corpus. This could be code like: 
01: var _var_1_ = []; 
02: JSON.stringify(_var_1_); 
 121 
Another possibility is that a valid function call is already stored in one of the corpus files from 
the browser test cases, which is very likely. In this case a code sample can also be found in 
the first corpus. 
After that, the fuzzer can insert JSON.stringify() function calls at random locations during 
fuzzing, for example, by splicing test cases. As next step, the fuzzer also tries during the 
deterministic preprocessing phase 2 to generate samples like the following: 
01: var _var_1_ = []; 
02: var _var_2_ = { 
03:     valueOf: function () { fuzzcommand("FUZZ_PRINT", "_PRINT_ID_1") }, 
04:     toJSON: function () { fuzzcommand("FUZZ_PRINT", "_PRINT_ID_2") }, 
05:     toISOString: function () { fuzzcommand("FUZZ_PRINT", "_PRINT_ID_3") }, 
06:     toDateString: function () { fuzzcommand("FUZZ_PRINT", "_PRINT_ID_4") }, 
07:     // Other callback functions 
08: }; 
09: _var_1_[0] = _var_2_; 
10: JSON.stringify(_var_1_); 
In this case the string _PRINT_ID_2 would be reflected and the fuzzer creates a new entry 
in the template corpus. As soon as the fuzzer starts fuzzing using this entry, fuzzed code will 
be added in the toJSON callback function. The code, which is added at this location, is taken 
from the first corpus which stores JavaScript code snippets and from the mutation engine. 
As soon as one of these snippets contains code which sets the length of _var_1_ to one and 
which triggers garbage collection, the bug is triggered and CVE-2015-6764 would be found. 
The likelihood that _var_1_ is modified by the fuzzer is also increased because the callback 
has a connection to _var_1_. Moreover, the fuzzer contains a special mutation strategy 
which modifies the length of an array, since this operation is a common building block found 
in most analyzed vulnerabilities. 
This description corresponds to the generation of the template corpus during the self-created 
test cases phase from Figure 8. New template files are also added during the preprocess 
corpus phase 2 performed on the downloaded JavaScript test cases. In this phase all test 
cases from the first corpus are preprocessed by inserting objects with callback functions, 
similar to _var_2_ in the above code. Newly identified callbacks in these cases would also 
be added to the template corpus. The code does not only inject callback objects at all 
possible locations, it also introduces callbacks using a variety of other techniques. For 
example, instantiated objects are proxied as well as global objects and functions. Moreover, 
native available classes are sub-classed, and their usage is replaced by the sub class which 
contains callbacks for every possible property and function. Properties or methods of global 
objects are redefined to callback functions. Several combinations of these techniques are 
used. For example, an object can be replaced by another object of a sub class of the original 
type which implements Symbol.Species which returns a different constructor. This 
constructor can then return a proxied object which modifies accessed properties on-the-fly, 
for example, by changing the prototype chain and introducing callbacks in them. 
 122 
Moreover, test cases must be rewritten before callbacks can be injected. For example, 
consider the following possible test case from the first corpus: 
01: /test*/g.exec("test123") 
This testcase must first be adapted to make use of the new keyword to ensure that it can be 
sub-classed: 
01: new RegExp("test*", "g").exec(new String("test123")) 
In addition to that, the testcase must be rewritten to first assign the object to a variable before 
operations are performed on it: 
01: var var_1_ = new RegExp("test*", "g"); 
02: var_1_.exec(new String("test123")) 
Assigning the object to a variable is important. If later callbacks are injected, for example by 
using the argument string, the code inserted into the callback must be able to modify the 
triggering object. In this example, a callback could be triggered inside the exec method and 
since the method is called on var_1_, the callback code should attempt to modify the var_1_ 
variable. Such a modification would not be possible in the original test case because the 
object was not assigned to a variable and therefore testcases must be rewritten first. 
Similar variable assignments should be used to make values passed to functions accessible, 
however, this is currently not supported. 
In total 9,867 template files were created using the developed script which brute forced 
possible callback locations or which manually marked code locations within control flow 
statements like loops. Furthermore, it was possible to automatically inject callback functions 
in 9,147 of the 10,473 corpus files. This resulted in 174,233 additional template files. The 
final corpus therefore contains 184,100 template files which mark code locations where the 
fuzzer should insert fuzzed code. 
During callback injection 249 crashes could be observed, but the root cause of these bugs 
was traced back to native functions, which are not enabled per default. These crashes 
therefore do not pose a risk to end users.  
 123 
5.2.3 Initial test case analysis and type reflection 
Every corpus entry initially undergoes an analysis that creates a state file. During this 
analysis, the data type of every variable in every code line is obtained. Using static analysis 
to extract this information would require a lot of engineering effort and therefore a dynamic 
approach was chosen. As previously mentioned, finding new corpus entries is a rare event 
and additional executions in such a case can therefore be neglected when the runtime is 
evaluated. 
To extract at runtime the data type of the used variables, the corpus entry is wrapped within 
the following code: 
01: try { 
02: 
var data_types = new Set() 
03: 
// Code of the original corpus test case 
04: } finally { 
05: 
fuzzcommand('FUZZ_PRINT', data_types); 
06: } 
In the current tested line, code is inserted which adds the data types of all variables to the 
data_types variable. This variable is reflected to the fuzzer in the finally block. The code is 
executed multiple times, one execution per code line. This is required because the insertion 
of a code line could result in an exception, for example, if the tested line corresponds to an 
argument list or is a line after a for-loop which is not enclosed by curly brackets. A set is 
used because variables can have different data types in the same code line, for example, if 
a function is invoked with different arguments. 
A similar strategy is used to extract the following information: 
• 
How often a line is executed. 
• 
If the line must end with a semicolon or a comma. 
• 
The number of required arguments of custom functions. This information can be 
queried by accessing the function_name.length property, but currently a static 
approach is used instead. 
• 
The name and type of custom properties. 
• 
Properties and functions of custom classes. 
• 
The length of every array in every line. 
• 
The number of variables, functions and classes found in the test case. 
• 
The average runtime of the testcase. 
• 
The test case size and if the test case results in unreliable coverage feedback. 
• 
Where code blocks start and end. This information is important when multiple test 
cases are spliced together during fuzzing. 
The state file is persisted as a Python pickle file. 
 124 
5.3 Fuzzing 
During fuzzing the fuzzer selects random template files and injects fuzzed code at the 
identified code locations. Moreover, fuzzed code is added at the beginning and end of the 
templates. To generate fuzzed code, the first corpus is used which contains JavaScript code 
snippets. A random number of code snippets are loaded and merged or spliced together. 
Moreover, a random number of mutations are applied to the generated test case. Examples 
of mutations are: 
• 
Removal of a code line. 
• 
Removal of a code block. 
• 
Wrapping a value in an object property. This can trigger escape analysis bugs, see 
chapter 4.5.5 for details. 
• 
Adding a function call at a random location which can change assumptions. 
• 
Performing actions on a variable like mathematical calculations. 
• 
Changing the length of an array and calling garbage collection. 
• 
Changing the length of an array, calling garbage collection and setting the length 
back to the original value. 
• 
Wrapping code within an if-statement. 
• 
Wrapping code within a loop with just one iteration. 
• 
Wrapping code in a function call. 
• 
Changing a value to a different value. 
Currently, just a few mutations are implemented because of the limited time frame of the 
thesis. The fuzzer can therefore further be improved by implementing more mutation 
strategies. 
 125 
5.4 Results 
Chapter 3 listed the following state-of-the-art evaluation methods to compare the 
performance of a fuzzer: 
• 
The LAVA test suite 
• 
The rode0day binaries 
• 
The DARPA Cyber Grand Challenge dataset 
• 
The FuzzBench project from Google 
These projects and datasets are related to fuzzing binary protocols or simple interactive 
applications but are not applicable to browser fuzzers. To evaluate the performance of the 
implemented improvements, the number of newly discovered security vulnerabilities and 
bugs are instead compared. This evaluation method follows the recommendation provided 
by Klees et al. [50]. 
Chapter 3 mentioned that the Chrome codebase has been fuzzed for several years by 
Google using over 25,000 CPU cores. External researchers are invited to develop fuzzers 
that run on this infrastructure to unveil new vulnerabilities. Furthermore, a dedicated security 
team constantly improves the fuzzers. 