(cid:2)DBj(cid:3)i + (cid:2)Z(cid:2)
j(cid:3)i for i ∈ {0, 1}.
Bj
Si truncates its shares of Δ element-wise to get (cid:5)(cid:2)Δ(cid:3)i(cid:6).
Si computes (cid:2)w(cid:3)i := (cid:2)w(cid:3)i − α|B|(cid:5)(cid:2)Δ(cid:3)i(cid:6) for i ∈ {0, 1}.
9:
10:
11: Parties run RecA((cid:2)w(cid:3)0,(cid:2)w(cid:3)1) and output w.
Fig. 4: The online phase of privacy preserving linear regression.
5:
6:
7:
8:
Bj
Bj
Bj
in the least signiﬁcant bit of the fractional part compared to
standard ﬁxed-point arithmetic.
We also note that if a decimal number z is negative, it will
be represented in the ﬁeld as 2l − |z|, where |z| is its absolute
value and the truncation operation changes to (cid:11)z(cid:12) = 2l −(cid:11)|z|(cid:12).
We prove the following theorem for both positive and negative
numbers in Appendix B.
Theorem 1. In ﬁeld Z2l, let x ∈ [0, 2lx ]∪ [2l − 2lx , 2l), where
l > lx+1 and given shares (cid:7)x(cid:8)0,(cid:7)x(cid:8)1 of x, let (cid:7)(cid:11)x(cid:12)(cid:8)0 = (cid:11)(cid:7)x(cid:8)0(cid:12)
and (cid:7)(cid:11)x(cid:12)(cid:8)1 = 2l−(cid:11)2l−(cid:7)x(cid:8)1(cid:12). Then with probability 1−2lx+1−l,
((cid:7)(cid:11)x(cid:12)(cid:8)0,(cid:7)(cid:11)x(cid:12)(cid:8)1) ∈ {(cid:11)x(cid:12) − 1,(cid:11)x(cid:12),(cid:11)x(cid:12) + 1} , where (cid:11)·(cid:12)
Rec
denotes truncation by lD ≤ lx bits.
A
The complete protocol between the two servers for the
online phase of privacy preserving linear regression is shown in
Figure 4. It assumes that the data-independent shared matrices
(cid:7)U(cid:8),(cid:7)V(cid:8),(cid:7)Z(cid:8),(cid:7)V(cid:3)(cid:8),(cid:7)Z(cid:3)(cid:8) were already generated in the ofﬂine
phase. Besides multiplication and addition of shared decimal
numbers, the protocol also requires multiplying the coefﬁcient
vector by α|B| in each iteration. To make this operation efﬁcient,
−k. Then the
we set α|B|
multiplication with α|B| can be replaced by having the parties
truncate k additional bits from their shares of the coefﬁcients.
We sketch a proof for the following Theorem on security of
to be a power of 2, i.e. α|B| = 2
the online protocol in Appendix D.
Theorem 2. Consider a protocol where clients distribute
arithmetic shares of their data among two servers who run
the protocol of Figure 4 and send the output
to clients.
In the Fof f line hybrid model,
this protocol realizes the
ideal functionality Fml of Figure 3 for the linear regression
function, in presence of a semi-honest admissible adversary
(see section III).
Effect of Truncation Error. Note that when the size of the
ﬁeld is large enough, truncation can be performed once per
iteration instead of once per multiplication. Hence in our
implementations, the truncation is performed (|B| + d) · t
times and by the union bound, the probability of failure in
25
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
the training is (|B| + d) · t · 2lx+1−l. For typical parameters
|B| = 128, d = 784, t = 1000, lx = 32, l = 64, the probability
of a single failure happening during the whole training is
−12. Moreover, even if a failure in the truncation
around 2
occurs, it is unlikely to translate to a failure in training. Such
a failure makes one feature in one sample invalid, yet the ﬁnal
model should not be affected by small changes in data, or
else the trainig strategy suffers from overﬁtting. In appendix C,
we conﬁrm these observations by running experiments on two
different datasets (MNIST [6] and Arcene [1]). In particular,
we show that accuracy of the models trained using privacy
preserving linear regression with truncation matches those of
plaintext training using standard arithmetic.
Efﬁciency Discussion. The dominating term in the computa-
tion cost of Figure 4 is the matrix multiplications in step 5
and 8. In each iteration, each party performs 4 such matrix
multiplications5, while in plaintext SGD training, according
to Equation 2, 2 matrix multiplications are performed. Hence,
the computation time for each party is only twice the time for
training on plaintext data.
The total communication of the protocol is also nearly
optimal. In step 1, each party sends an n × d matrix, which is
of the same size as the data. In step 4 and 7, |B| + d elements
are sent per iteration. Therefore, the total communication is
n·d+(|B|+d)·t = nd·(1+ E
d + E|B| ) for each party. In practice,
the number of epochs E is only 2-3 for linear and logistic
regressions and 10-15 for neural networks, which is much
smaller than |B| and d. Therefore, the total communication
is only a little more than the size of the data. The time spent
on the communication can be calculated by dividing the total
communication by the bandwidth between the two parties.
B. The Ofﬂine Phase
We describe how to implement the ofﬂine phase as a two-
party protocol between S0 and S1 by generating the desired
shared multiplication triplets. We present two protocols for
doing so based on linearly homomorphic encryption (LHE)
and oblivious transfer (OT). The techniques are similar to prior
work (e.g., [18]) but are optimized for the vectorized scenario
where we operate on matrices. As a result the complexity of
our ofﬂine protocol is much better than the naive approach of
generating independent multiplication triplets.
Recall that given shared random matrices (cid:7)U(cid:8) and (cid:7)V(cid:8), the
key step is to choose a |B| × d submatrix from (cid:7)U(cid:8) and a
column from (cid:7)V(cid:8) and compute the shares of their product. This
is repeated t times to generate (cid:7)Z(cid:8). (cid:7)Z(cid:3)(cid:8) is computed in the
same way with the dimensions reversed. Thus, for simplicity,
we focus on this basic step, where given shares of a |B| × d
matrix (cid:7)A(cid:8), and shares of a d × 1 matrix (cid:7)B(cid:8), we want to
compute shares of a |B| × 1 matrix (cid:7)C(cid:8) such that C = A × B.
We utilize the following relationship: C = (cid:7)A(cid:8)0 × (cid:7)B(cid:8)0 +
(cid:7)A(cid:8)0×(cid:7)B(cid:8)1 +(cid:7)A(cid:8)1×(cid:7)B(cid:8)0 +(cid:7)A(cid:8)1×(cid:7)B(cid:8)1. It sufﬁces to compute
(cid:7)(cid:7)A(cid:8)0 × (cid:7)B(cid:8)1(cid:8) and (cid:7)(cid:7)A(cid:8)1 × (cid:7)B(cid:8)0(cid:8) as the other two terms can
be computed locally.
5Party S1 can simplify the formula to E × (F − (cid:3)w(cid:4)) + (cid:3)X(cid:4) × F + (cid:3)Z(cid:4),
which has only 2 matrix multiplications.
LHE-based generation. To compute the shares of the product
(cid:7)A(cid:8)0 × (cid:7)B(cid:8)1, S1 encrypts each element of (cid:7)B(cid:8)1 using an
LHE and sends them to S0. The LHE can be initiated
using the cryptosystem of Paillier [38] or Damgard-Geisler-
Kroigaard(DGK) [17]. S0 then performs the matrix mul-
tiplication on the ciphertexts, with additions replaced by
multiplications and multiplications by exponentiations. Finally,
S0 masks the resulting ciphertexts by random values, and
sends them back to S1 to decrypt. The protocol can be found
in Figure 12 in the Appendix.
Here S1 performs d encryptions, |B| decryptions and S0
performs |B| × d exponentiations. The cost of multiplications
on the ciphertext is non-dominating and is omitted. The shares
of (cid:7)A(cid:8)1 × (cid:7)B(cid:8)0 can be computed similarly.
Using this basic step, the overall computation performed
in the ofﬂine phase per party is (|B| + d) · t encryptions,
(|B| + d) · t decryptions and 2|B| · d · t exponentiations. The
total communication is 2(|B| + d) · t ciphertexts, which is
much smaller than the size of the data. If we had generated the
multiplication triplets independently, the number of encryptions,
decryptions and the communication would increase to 2|B|·d·t.
Finally, unlike the online phase, all communication in the ofﬂine
phase can be done in one interaction.
OT-based generation. The shares of the product (cid:7)A(cid:8)0 × (cid:7)B(cid:8)1
can also be computed using OTs. We ﬁrst compute the shares
of the product (cid:7)aij · bj(cid:8) for all i = 1, . . . ,|B| and j = 1, . . . , d.
To do so, S1 uses each bit of bj to select two values computed
from aij using correlated OTs. In particular, for k = 1, . . . , l,
S0 sets the correlation function of COT to fk(x) = ai,j ·2k +x
mod 2l and S0, S1 run COT(rk, fk(x); bj[k]). If bj[k] = 0,
S1 gets rk; if bj[k] = 1, S1 gets ai,j · 2k + rk mod 2l. This
is equivalent to bj[k] · aij · 2k + rk mod 2l. Finally, S1 sets
(cid:7)aij · bj(cid:8)1 =
k=1 rk
mod 2l, and S0 sets (cid:7)aij · bj(cid:8)0 =
To further improve efﬁciency, authors of [18] observe that
for each k, the last k bits of aij · 2k are all 0s. Therefore,
only the ﬁrst l − k bits need to be transferred. Therefore, the
message lengths are l, l − 1, . . . , 1, instead of all being l-bits.
This is equivalent to running l instances of COT(l+1)/2. So far,
all the techniques described are as discussed in [18].
(cid:2)l
k=1(bj[k] · aij · 2k + rk) = aij · bj +
(cid:2)l
(cid:2)l
k=1(−rk) mod 2l.
The optimization described above does not improve the
computation cost of OTs. The reason is that in OT, each
message is XORed with a mask computed from the random
oracle applied to the selection bit. In practice, the random
oracle is instantiated by a hash function such as SHA256 or
AES, which at least has 128 bit output. Hence, the fact that l
is only 64 does not reduce time to compute the masks.
We further leverage the matrix structure to improve on this.
Note that a1j, . . . , a|B|j are all multiplied by bj, which means
the same selection bit bj[k] is used for all aijs. Equivalently, we
can view it as using bj[k] to select messages with length (l −
k)·|B| bits. Therefore, they can be masked by (cid:14) (l−k)·|B|
(cid:15) hash
outputs. For a reasonable mini-batch size, each multiplication
needs l
4 instances of COT128. In this way, the total number of
hashes can be reduced by 4 times and the total communication
can be reduced by half.
Finally, after computing (cid:7)aij ·bj(cid:8), the ith element of (cid:7)(cid:7)A(cid:8)0×
128
26
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
1
0.5
0
)
u
(
f
1
0
)
u
(
f
−0.5 0 0.5
−1
u
0
u
1
|B|·d·t·l
In total, both parties perform
(cid:2)d
Fig. 5: (a) Our new activation function. (b) RELU function.
(cid:7)B(cid:8)1(cid:8) can be computed by (cid:7)(cid:7)A(cid:8)0 × (cid:7)B(cid:8)1(cid:8)[i] =
j=0(cid:7)aij · bj(cid:8).
The shares of (cid:7)A(cid:8)1 × (cid:7)B(cid:8)0 can be computed similarly.
instances of COT128
and the total communication is |B| · d · t · l · (l + λ) bits.
In addition, a set of base OTs needs to be performed at the
beginning for OT extension. In Section VI-A we show that
the size communication for the OT-based generation is much
higher than LHE-based generation, yet the total running time
is faster. The reason is that, given OT extension, each OT
operation is very cheap (∼ 106 OTs per second).
2
C. Privacy Preserving Logistic Regression
In this section, we present a protocol to support privacy
preserving logistic regression. Besides issues addressed for
linear regression, the main additional challenge is to compute
the logistic function f (u) = 1
1+e−u on shared numbers. Note
that the division and the exponentiation in the logistic function
are computed on real numbers, which are hard to support using
a 2PC for arithmetic or boolean circuit. Hence, prior work
proposes to approximate the function using polynomials [10].
It can be shown that approximation using a high-degree
polynomial is very accurate [33]. However, for efﬁciency
reasons, the degree of the approximation polynomial in secure
computation is set to 2 or 3, which results in a large accuracy
loss of the trained model compared to logistic regression.
Secure computation friendly activation functions. Instead
of using polynomials to approximate the logistic function,
we propose a new activation function that can be efﬁciently
computed using secure computation techniques. The function
is described in Equation 4 and drawn in Figure 5(a).
⎧⎪⎨
⎪⎩
f (x) =
0,
x + 1
2 ,
1,
2
if x  1
2
2 ≤ x ≤ 1
2
(4)
The intuition for this choice of activation is as follows (we
also conﬁrm its effectiveness with experiments): as mentioned
in section II-A, the main reason logistic regression works well
for classiﬁcation problems is that the prediction is bounded
between 0 and 1. Therefore, it is very important for the two
tails of the activation function to converge to 0 and 1, and
both the logistic function and the function in Equation 4 have
such behavior. In contrast, approximation with low degree
polynomials fails to achieve this property. The polynomial
might be close to the logistic function in certain intervals, but
the tails are unbounded. If a data sample yields a very large
input u to the activation function, f (u) will be far beyond the
[0, 1] interval which affects accuracy of the model signiﬁcantly
in the backward propagation. Our choice of the activation
function is also inspired by its similarity to the RELU function
27
Logistic
MNIST
Arcene
98.64
86
Our approaches
ﬁrst
second
97.96
98.62
86
85
Polynomial Approx.
deg. 2
42.17
72
deg. 5
84.64
82
deg. 10
98.54
86
TABLE I: Accuracy (%) comparison of different approaches
for logistic regression.
(Figure 5(b)) used in neural networks. One of the justiﬁcations
used for replacing logistic function by the RELU function in
neural networks is that the subtraction of two RELU functions
with an offset yields the activation function of Equation 4
which in turn, closely imitates the logistic function.
Once we use the new activation function, we have two
choices when computing the backward propagation. We can
either use the same update function as the logistic function (i.e.
continue to compute the partial derivative using the logistic