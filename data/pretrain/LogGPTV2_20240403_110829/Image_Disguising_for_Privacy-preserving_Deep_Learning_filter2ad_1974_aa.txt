title:Image Disguising for Privacy-preserving Deep Learning
author:Sagar Sharma and
Keke Chen
Wright State University 
Wright State University 
CORE Scholar 
CORE Scholar 
Kno.e.sis Publications 
2018 
The Ohio Center of Excellence in Knowledge-
Enabled Computing (Kno.e.sis) 
Poster: Image Disguising for Privacy-preserving Deep Learning 
Poster: Image Disguising for Privacy-preserving Deep Learning 
Sagar Sharma 
Wright State University - Main Campus, PI:EMAIL 
Keke Chen 
Wright State University, PI:EMAIL 
Follow this and additional works at: https://corescholar.libraries.wright.edu/knoesis 
 Part of the Bioinformatics Commons, Communication Technology and New Media Commons, 
Databases and Information Systems Commons, OS and Networks Commons, and the Science and 
Technology Studies Commons 
Repository Citation 
Repository Citation 
Sharma, S., & Chen, K. (2018). Poster: Image Disguising for Privacy-preserving Deep Learning. , 39. 
https://corescholar.libraries.wright.edu/knoesis/1157 
This Article is brought to you for free and open access by the The Ohio Center of Excellence in Knowledge-Enabled 
Computing (Kno.e.sis) at CORE Scholar. It has been accepted for inclusion in Kno.e.sis Publications by an 
authorized administrator of CORE Scholar. For more information, please contact library-corescholar@wright.edu. 
Poster: Image Disguising for Privacy-preserving Deep Learning
Data Intensive Analysis and Computing (DIAC) Lab, Kno.e.sis Center, Wright State University
Sagar Sharma, Keke Chen
{sharma.74,keke.chen}@wright.edu
ABSTRACT
Due to the high training costs of deep learning, model developers
often rent cloud GPU servers to achieve better efficiency. However,
this practice raises privacy concerns. An adversarial party may be
interested in 1) personal identifiable information encoded in the
training data and the learned models, 2) misusing the sensitive mod-
els for its own benefits, or 3) launching model inversion (MIA) and
generative adversarial network (GAN) attacks to reconstruct repli-
cas of training data (e.g., sensitive images). Learning from encrypted
data seems impractical due to the large training data and expensive
learning algorithms, while differential-privacy based approaches
have to make significant trade-offs between privacy and model
quality. We investigate the use of image disguising techniques to
protect both data and model privacy. Our preliminary results show
that with block-wise permutation and transformations, surprisingly,
disguised images still give reasonably well performing deep neural
networks (DNN). The disguised images are also resilient to the
deep-learning enhanced visual discrimination attack and provide
an extra layer of protection from MIA and GAN attacks.
ACM Reference Format:
Sagar Sharma, Keke Chen. 2018. Poster: Image Disguising for Privacy-
preserving Deep Learning. In 2018 ACM SIGSAC Conference on Computer
and Communications Security (CCS ’18), October 15–19, 2018, Toronto, ON,
Canada. ACM, New York, NY, USA, Article 39, 3 pages. https://doi.org/10.
1145/3243734.3278511
1 INTRODUCTION
Deep Neural Networks (DNN) generate robust modeling results
across diverse domains such as image classification and natural
language processing. However, DNN training is resource and time
consuming. Model developers often utilize AWS’s elastic GPUs and
Google Cloud platform to train large-scale models. In such a setting,
a big concern is the privacy of sensitive training data and the model
that can be possibly used to explore private data [2].
One possible approach to addressing the privacy issue is to learn
models from encrypted data, however, it is too expensive to be
practical for deep learning yet. Recent advances in cryptography
have provided a few constructs for learning from encrypted data,
such as homomorphic encryptions, garbled circuits, and secret
sharing [3, 10]. A few attempts have been made to adopt these
constructs in deep learning, for example, secure gradient descent
[8]. However, due to the large training data and number of iterations
in learning DNNs, the protocols normally have impractical costs.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5693-0/18/10.
https://doi.org/10.1145/3243734.3278511
Differential privacy has been applied in deep learning [1, 11],
however the protocols are vulnerable to model inversion (MIA) [2]
and Generative Adversarial Network(GAN) attacks [5]. Further-
more, there is a significant tradeoff between utility and privacy -
large noises are needed to achieve meaningful privacy, which leads
to low-quality models [9, 11]. In the centralized setting, PrivyNet [7]
tries to hide private data by users constructing local shallow NNs
and sharing the intermediate representations to cloud for learning
the final model. However, the results show that the intermediate
representations are still visually identifiable.
Scope and contributions. We take a unique approach to bal-
ancing privacy and utility with image disguising. The intuition is
that deep learning is so powerful that it can pick up the unique
features for distinguishing even disguised image training data. The
question is how to design the proper disguising mechanisms that
can make the original content not (visually and algorithmically)
recognizable anymore, while still preserving the features that allow
DNNs to distinguish disguised images. We have studied a suite of
image disguising mechanisms that enable learning high-quality
DNN models on the disguised images, which can be applied in the
outsourced setting to protect both data and model privacy. Each
outsourced dataset gets a secret image transformation key. As long
as data owners keep their keys secret, the disguised images are re-
silient to the well-known attacks. Fascinatingly, the models learned
on the disguised images are high quality and work well in clas-
sifying the disguised images, comparable to the models built on
undisguised images. Our contributions are as follows:
(1) We have designed a suite of image disguising mechanisms
for preserving both privacy and utility of image-based DNN
learning in the outsourced setting.
(2) We have developed a toolkit for calibrating the the privacy
and utility of certain parameter settings for the disguising
mechanisms.
(3) With our approach, the current MIA and GAN attacks gener-
ate images in the disguised image forms, thus, providing no
additional information than the disguised training images.
(4) Our preliminary evaluation shows that the disguising mech-
anisms can effectively preserve data privacy and result in
surprisingly good-quality models.
2 ADVERSARIAL MODEL
We make some relevant security assumptions here: 1) We consider
ciphertext-only attacks, i.e., any cipher-plaintext image pair is un-
known to the adversary; 2) All infrastructures and communication
channels must be secure.
We consider the cloud provider to be an honest-but-curious
adversary. We concern with the privacy of the image datasets and
the learned models. An adversary may be interested in the contents
and identification of images that do not belong to it, or the learned
models; they may also misuse private models for its own benefits in
the outsourced setting, or launch MIA and GAN attacks to generate
pseudo-images that resemble the victim’s private data.
3 IMAGE DISGUISING FOR DEEP LEARNING
Assume a user owns a set of images for training, notated as pairs
{(Xi , yi)}, where Xi is the image pixel matrix and yi the correspond-
ing label. We formally define the disguising process as follows. Let
the disguising mechanism be a transformation TK , where K is the
secret key. By applying image disguising, the training data is trans-
formed to {(T(Xi), yi)}, which is used to train a DNN, denoted
as a function DT that takes disguised images T(X) and outputs a
predicted label ˆy. For any new data Xnew , the model application is
defined as DT (T(Xnew)).
Figure 1 shows how the framework works. A data owner dis-
guises her private images before outsourcing them to the cloud for
DNN learning. She transforms all of her images using one key. For
model application, she transforms new data with the same key.
Figure 1: Image disguising framework for DNN learning.
We consider a suite of image disguising mechanisms that can
be used individually or layered on top of one another depending
on the dataset characteristics and the desired privacy and utility.
Candidate mechanisms must hide the visually identifiable features
in the images, and provide a sufficiently large key space to be
resilient to ciphertext-only attacks. As a result, these mechanisms
inevitably affect the quality of learned DNNs. Hence, finding the
settings that provide both high security and model quality is crucial.
We start with the relatively weak block-wise permutation technique
and extend to other enhancements.
3.1 Block-wise Permutation
The block-wise permutation simply partitions an image and re-
arranges image blocks. Let an image Xp×p of p2 pixels be partitioned
into blocks of size k × l that are labeled sequentially as v =
. A pseudorandom permutation of the blocks, π(v),
shuffles the blocks and reassemble the image. Theoretically, with
large t it provides t! candidates, difficult for brute-force attacks.
However, such a mechanism is insufficient to hide the image content
yet, as the boundary, color, content shape, and texture of the original
neighboring blocks provide clues for adversaries to recover the
image - imagine the jigsaw puzzle! Figure 2 shows an example.
Thus, it has to be combined with other mechanisms.
3.2 Randomized Multidimensional
Transformations (RMT)
For an image represented as a pixel matrix X, a general form of
randomized multidimensional transformation is defined as X R + ∆,
where R can be a random orthogonal (i.e., rotation) or a random
projection matrix [12] and ∆ is a random additive noise matrix.
The matrix R acts as a key across the training data, while ∆ is re-
generated for each image and drawn uniformly at random from
[0, N] where N will be known as the noise level.
Figure 2: Block-wise Permutation of CIFAR-10 images. The
detail on each block can help easily rearrange blocks.
Figure 3: RMT transformation of CIFAR-10 images with or-
thogonal matrices and 25 noise level. It is difficult to visually
detect block-level details and reassemble them.
Block-wise application of RMT. As Figure 3 1 shows, apply-
ing RMT to the entire image may still preserve some visual features,
leaving hints to link back the original image. To further strengthen
the image privacy, we apply the block-wise RMT. Instead of picking
one private R for the entire image, we pick {R1, R2, . . . Rt} matri-
ces for the t blocks, respectively. Block-wise RMT can further be
combined with block-wise permutation.
4 CALIBRATING IMAGE DISGUISING
MECHANISMS
One major issue remains unaddressed: how to tune the parame-
ter settings for the designed mechanisms to meet desired privacy?
Our ultimate goal is to design a theoretically justifiable method for
evaluating the protection strengths of various disguising mecha-
nisms and their combinations. In our preliminary study, we design
a few tools to investigate the effect of different parameter settings.
Specifically, we introduce two new concepts: “visual privacy" for
quantifying the discernibility of disguised images, and “model mis-
usability" for quantifying the adversarial usability of the developed
models on real undisguised data.
Figure 4: Models trained on transformed images must not
perform well on undisguised images and vice versa.
Visual Privacy. The most straightforward approach to visually
identifying the disguised images is possibly employing humans to
visually examine the images. We move one step further by using
a trained DNN for this task as recent studies have shown that a
well-trained DNNs are comparable to or even better than human
visual recognition. Specifically, we pre-train a “DNN examiner"
model on the original image space and measure its accuracy in
classifying the transformed images. Let visual privacy be defined
as (1 − accuracy of the DNN examiner). We plan to develop more
DNN examiners for imitating human examiners’ behaviors, i.e.,
identifying the original neighboring blocks.
Model Mis-usability. Another task is to prevent abusing the
learned model, e.g., applying the model on the images captured
in public space. Specifically, we assess if the models trained on
disguised images also work in classifying undisguised images. Let’s
1More example figures are uploaded to https://sites.google.com/site/rmtfordl/.
Transformation T:Block-wise PermutationRMTBlock-wise RMTData OwnerDNN FrameworkCloudDT(T(X))X, yT(X), yT(Xnew)𝑌"newXnew8x 8 blocks2 x 2 blocks32 x 32 block2 x 2 blocksdefine “model mis-usability" as this testing accuracy. The lower the
testing accuracy is, the lower the chance of model misuse.
4.1 Resiliency to Model-based Attacks
Model inversion attacks such as GAN and MIA attacks have suc-
ceeded in exploiting deep learning models. For a given model, MIA
tries to reconstruct a part of training data; GAN attack allows adver-
sarial participant to reconstruct data owners’ training data. With
the link between the original images and the disguised images
hidden from adversaries by our mechanisms, these attacks only
reconstruct disguised images, which are useless as the disguised
training images are already accessible to adversaries.
5 EXPERIMENTS
We present our experimental findings on 1) model quality, 2) visual
privacy and 3) model mis-usability for the block-wise application of
RMT. We test the mechanisms in two prevalent DNN benchmarking
datasets: MNIST and CIFAR-10.
100
25
Noise Level
Simple
ResNet
block-wise MP
Mechanisms
Block size
{7 × 7}
{2 × 2}
block-wise MP + Permutation
Table 1: Parameter settings and CNN Architectures.
Architecture
Datasets
MNIST
CIFAR-10
Table 1 details the mechanisms, block size, and additive noise
level used for the datasets. We used a simple DNN architecture
for MNIST [6], and the more powerful ResNet [4] architecture for
CIFAR-10 dataset. For MNIST, we set the learning rate to 0.001 and