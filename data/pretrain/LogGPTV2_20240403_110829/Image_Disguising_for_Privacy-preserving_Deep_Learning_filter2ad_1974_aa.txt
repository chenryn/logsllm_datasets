# Title: Image Disguising for Privacy-Preserving Deep Learning

## Authors
- Sagar Sharma, Wright State University
- Keke Chen, Wright State University

## Affiliations
- Wright State University
- CORE Scholar
- Kno.e.sis Publications

## Publication Year
2018

## Abstract
Due to the high training costs of deep learning, model developers often rent cloud GPU servers to achieve better efficiency. However, this practice raises privacy concerns. An adversarial party may be interested in 1) personal identifiable information encoded in the training data and the learned models, 2) misusing the sensitive models for its own benefits, or 3) launching model inversion (MIA) and generative adversarial network (GAN) attacks to reconstruct replicas of training data (e.g., sensitive images). Learning from encrypted data is impractical due to the large training data and expensive learning algorithms, while differential-privacy-based approaches have to make significant trade-offs between privacy and model quality. We investigate the use of image disguising techniques to protect both data and model privacy. Our preliminary results show that with block-wise permutation and transformations, disguised images still yield reasonably well-performing deep neural networks (DNNs). The disguised images are also resilient to deep-learning-enhanced visual discrimination attacks and provide an extra layer of protection from MIA and GAN attacks.

## ACM Reference Format
Sagar Sharma, Keke Chen. 2018. Poster: Image Disguising for Privacy-Preserving Deep Learning. In 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS '18), October 15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, Article 39, 3 pages. https://doi.org/10.1145/3243734.3278511

## 1. Introduction
Deep Neural Networks (DNNs) generate robust modeling results across diverse domains such as image classification and natural language processing. However, DNN training is resource and time-consuming. Model developers often utilize AWS's elastic GPUs and Google Cloud platform to train large-scale models. In such a setting, a major concern is the privacy of sensitive training data and the model, which can be used to explore private data [2].

One possible approach to addressing the privacy issue is to learn models from encrypted data. However, it is too expensive to be practical for deep learning yet. Recent advances in cryptography have provided constructs for learning from encrypted data, such as homomorphic encryption, garbled circuits, and secret sharing [3, 10]. A few attempts have been made to adopt these constructs in deep learning, for example, secure gradient descent [8]. However, due to the large training data and number of iterations in learning DNNs, these protocols typically have impractical costs.

Differential privacy has been applied in deep learning [1, 11], but the protocols are vulnerable to model inversion (MIA) [2] and Generative Adversarial Network (GAN) attacks [5]. Furthermore, there is a significant trade-off between utility and privacy—large noises are needed to achieve meaningful privacy, which leads to low-quality models [9, 11]. In the centralized setting, PrivyNet [7] tries to hide private data by users constructing local shallow NNs and sharing the intermediate representations to the cloud for learning the final model. However, the results show that the intermediate representations are still visually identifiable.

### Scope and Contributions
We take a unique approach to balancing privacy and utility with image disguising. The intuition is that deep learning is so powerful that it can pick up the unique features for distinguishing even disguised image training data. The question is how to design the proper disguising mechanisms that can make the original content not (visually and algorithmically) recognizable anymore, while still preserving the features that allow DNNs to distinguish disguised images. We have studied a suite of image disguising mechanisms that enable learning high-quality DNN models on the disguised images, which can be applied in the outsourced setting to protect both data and model privacy. Each outsourced dataset gets a secret image transformation key. As long as data owners keep their keys secret, the disguised images are resilient to well-known attacks. Fascinatingly, the models learned on the disguised images are high quality and work well in classifying the disguised images, comparable to the models built on undisguised images. Our contributions are as follows:
1. We have designed a suite of image disguising mechanisms for preserving both privacy and utility of image-based DNN learning in the outsourced setting.
2. We have developed a toolkit for calibrating the privacy and utility of certain parameter settings for the disguising mechanisms.
3. With our approach, the current MIA and GAN attacks generate images in the disguised image forms, thus providing no additional information than the disguised training images.
4. Our preliminary evaluation shows that the disguising mechanisms can effectively preserve data privacy and result in surprisingly good-quality models.

## 2. Adversarial Model
We make some relevant security assumptions here: 1) We consider ciphertext-only attacks, i.e., any cipher-plaintext image pair is unknown to the adversary; 2) All infrastructures and communication channels must be secure.

We consider the cloud provider to be an honest-but-curious adversary. We are concerned with the privacy of the image datasets and the learned models. An adversary may be interested in the contents and identification of images that do not belong to it, or the learned models; they may also misuse private models for their own benefits in the outsourced setting, or launch MIA and GAN attacks to generate pseudo-images that resemble the victim’s private data.

## 3. Image Disguising for Deep Learning
Assume a user owns a set of images for training, notated as pairs \((X_i, y_i)\), where \(X_i\) is the image pixel matrix and \(y_i\) is the corresponding label. We formally define the disguising process as follows. Let the disguising mechanism be a transformation \(T_K\), where \(K\) is the secret key. By applying image disguising, the training data is transformed to \(\{(T(X_i), y_i)\}\), which is used to train a DNN, denoted as a function \(D_T\) that takes disguised images \(T(X)\) and outputs a predicted label \(\hat{y}\). For any new data \(X_{\text{new}}\), the model application is defined as \(D_T(T(X_{\text{new}}))\).

Figure 1 shows how the framework works. A data owner disguises her private images before outsourcing them to the cloud for DNN learning. She transforms all of her images using one key. For model application, she transforms new data with the same key.

### 3.1 Block-wise Permutation
The block-wise permutation simply partitions an image and rearranges image blocks. Let an image \(X_{p \times p}\) of \(p^2\) pixels be partitioned into blocks of size \(k \times l\) that are labeled sequentially as \(v\). A pseudorandom permutation of the blocks, \(\pi(v)\), shuffles the blocks and reassembles the image. Theoretically, with large \(t\), it provides \(t!\) candidates, making it difficult for brute-force attacks. However, such a mechanism is insufficient to hide the image content, as the boundary, color, content shape, and texture of the original neighboring blocks provide clues for adversaries to recover the image. Figure 2 shows an example. Thus, it has to be combined with other mechanisms.

### 3.2 Randomized Multidimensional Transformations (RMT)
For an image represented as a pixel matrix \(X\), a general form of randomized multidimensional transformation is defined as \(X R + \Delta\), where \(R\) can be a random orthogonal (i.e., rotation) or a random projection matrix [12], and \(\Delta\) is a random additive noise matrix. The matrix \(R\) acts as a key across the training data, while \(\Delta\) is regenerated for each image and drawn uniformly at random from \([0, N]\), where \(N\) is the noise level.

Block-wise application of RMT. As Figure 3 shows, applying RMT to the entire image may still preserve some visual features, leaving hints to link back to the original image. To further strengthen the image privacy, we apply the block-wise RMT. Instead of picking one private \(R\) for the entire image, we pick \(\{R_1, R_2, \ldots, R_t\}\) matrices for the \(t\) blocks, respectively. Block-wise RMT can further be combined with block-wise permutation.

## 4. Calibrating Image Disguising Mechanisms
One major issue remains unaddressed: how to tune the parameter settings for the designed mechanisms to meet desired privacy? Our ultimate goal is to design a theoretically justifiable method for evaluating the protection strengths of various disguising mechanisms and their combinations. In our preliminary study, we design a few tools to investigate the effect of different parameter settings. Specifically, we introduce two new concepts: "visual privacy" for quantifying the discernibility of disguised images, and "model misusability" for quantifying the adversarial usability of the developed models on real undisguised data.

### 4.1 Visual Privacy
The most straightforward approach to visually identifying the disguised images is possibly employing humans to visually examine the images. We move one step further by using a trained DNN for this task, as recent studies have shown that well-trained DNNs are comparable to or even better than human visual recognition. Specifically, we pre-train a "DNN examiner" model on the original image space and measure its accuracy in classifying the transformed images. Let visual privacy be defined as \((1 - \text{accuracy of the DNN examiner})\). We plan to develop more DNN examiners for imitating human examiners' behaviors, i.e., identifying the original neighboring blocks.

### 4.2 Model Misusability
Another task is to prevent abusing the learned model, e.g., applying the model on the images captured in public space. Specifically, we assess if the models trained on disguised images also work in classifying undisguised images. Let "model misusability" be defined as this testing accuracy. The lower the testing accuracy is, the lower the chance of model misuse.

### 4.3 Resiliency to Model-Based Attacks
Model inversion attacks such as GAN and MIA attacks have succeeded in exploiting deep learning models. For a given model, MIA tries to reconstruct a part of the training data; GAN attack allows an adversarial participant to reconstruct data owners’ training data. With the link between the original images and the disguised images hidden from adversaries by our mechanisms, these attacks only reconstruct disguised images, which are useless as the disguised training images are already accessible to adversaries.

## 5. Experiments
We present our experimental findings on 1) model quality, 2) visual privacy, and 3) model misusability for the block-wise application of RMT. We test the mechanisms in two prevalent DNN benchmarking datasets: MNIST and CIFAR-10.

### Parameter Settings and CNN Architectures
| Mechanisms | Block Size | Noise Level |
|------------|------------|-------------|
| Simple     | {7 × 7}    | 100         |
| ResNet     | {2 × 2}    | 25          |
| Block-wise MP | {2 × 2} | 25          |
| Block-wise MP + Permutation | {2 × 2} | 25          |

Table 1 details the mechanisms, block size, and additive noise level used for the datasets. We used a simple DNN architecture for MNIST [6], and the more powerful ResNet [4] architecture for the CIFAR-10 dataset. For MNIST, we set the learning rate to 0.001 and trained for 10 epochs. For CIFAR-10, we set the learning rate to 0.1 and trained for 200 epochs.

## Conclusion
Our preliminary results show that image disguising techniques can effectively protect both data and model privacy while maintaining high model performance. Future work will focus on further refining the disguising mechanisms and developing more robust evaluation methods.