the upper and lower limit for the dirty pages threshold, the number of dirty pages, and so on. When 
the cache manager system partition is initialized, all the needed system threads are started in the 
context of a System process which belongs to the partition. Each partition always has an associated 
minimal System process, which is created at partition-creation time (by the NtCreatePartition API). 
When the system creates a new partition through the NtCreatePartition API, it always creates and 
initializes an empty MI_PARTITION object (the memory is moved from a parent partition to the child, 
or hot-added later by using the NtManagePartition function). A cache manager partition object is 
created only on-demand. If no files are created in the context of the new partition, there is no need to 
create the cache manager partition object. When the file system creates or opens a file for caching ac-
cess, the CcinitializeCacheMap(Ex) function checks which partition the file belongs to and whether the 
partition has a valid link to a cache manager partition. In case there is no cache manager partition, the 
system creates and initializes a new one through the CcCreatePartition routine. The new partition starts 
separate cache manager-related threads (read-ahead, lazy writers, and so on) and calculates the new 
values of the dirty page threshold based on the number of pages that belong to the specific partition.
The file object contains a link to the partition it belongs to through its control area, which is initially 
allocated by the file system driver when creating and mapping the Stream Control Block (SCB). The 
partition of the target file is stored into a file object extension (of type MemoryPartitionInformation) 
and is checked by the memory manager when creating the section object for the SCB. In general, files 
are shared entities, so there is no way for File System drivers to automatically associate a file to a differ-
ent partition than the System Partition. An application can set a different partition for a file using the 
NtSetInformationFileKernel API, through the new FileMemoryPartitionInformation class.
Cache virtual memory management
Because the Windows system cache manager caches data on a virtual basis, it uses up regions of sys-
tem virtual address space (instead of physical memory) and manages them in structures called virtual 
address control blocks, or VACBs. VACBs define these regions of address space into 256 KB slots called 
views. When the cache manager initializes during the bootup process, it allocates an initial array of 
VACBs to describe cached memory. As caching requirements grow and more memory is required, the 
cache manager allocates more VACB arrays, as needed. It can also shrink virtual address space as other 
demands put pressure on the system.
At a file’s first I/O (read or write) operation, the cache manager maps a 256 KB view of the 256 KB-
aligned region of the file that contains the requested data into a free slot in the system cache address 
space. For example, if 10 bytes starting at an offset of 300,000 bytes were read into a file, the view that 
would be mapped would begin at offset 262144 (the second 256 KB-aligned region of the file) and 
extend for 256 KB.
CHAPTER 11 Caching and file systems
573
The cache manager maps views of files into slots in the cache’s address space on a round-robin 
basis, mapping the first requested view into the first 256 KB slot, the second view into the second 256 
KB slot, and so forth, as shown in Figure 11-3. In this example, File B was mapped first, File A second, 
and File C third, so File B’s mapped chunk occupies the first slot in the cache. Notice that only the first 
256 KB portion of File B has been mapped, which is due to the fact that only part of the file has been 
accessed. Because File C is only 100 KB (and thus smaller than one of the views in the system cache), it 
requires its own 256 KB slot in the cache.
System cache
View n
View 0
View 1
View 2
View 3
View 4
View 5
View 6
View 7
View 8
Section 0
Section 1
Section 0
Section 1
Section 2
Section 0
File A (500 KB)
File B (750 KB)
File C (100 KB)
FIGURE 11-3 Files of varying sizes mapped into the system cache.
The cache manager guarantees that a view is mapped as long as it’s active (although views can 
remain mapped after they become inactive). A view is marked active, however, only during a read 
or write operation to or from the file. Unless a process opens a file by specifying the FILE_FLAG_
RANDOM_ ACCESS flag in the call to CreateFile, the cache manager unmaps inactive views of a file as it 
maps new views for the file if it detects that the file is being accessed sequentially. Pages for unmapped 
views are sent to the standby or modified lists (depending on whether they have been changed), and 
because the memory manager exports a special interface for the cache manager, the cache manager 
can direct the pages to be placed at the end or front of these lists. Pages that correspond to views of 
files opened with the FILE_FLAG_SEQUENTIAL_SCAN flag are moved to the front of the lists, whereas 
all others are moved to the end. This scheme encourages the reuse of pages belonging to sequentially 
read files and specifically prevents a large file copy operation from affecting more than a small part of 
physical memory. The flag also affects unmapping. The cache manager will aggressively unmap views 
when this flag is supplied.
If the cache manager needs to map a view of a file, and there are no more free slots in the cache, it will 
unmap the least recently mapped inactive view and use that slot. If no views are available, an I/O error is 
returned, indicating that insufficient system resources are available to perform the operation. Given that 
views are marked active only during a read or write operation, however, this scenario is extremely unlikely 
because thousands of files would have to be accessed simultaneously for this situation to occur.
574 
CHAPTER 11 Caching and file systems
Cache size
In the following sections, we explain how Windows computes the size of the system cache, both virtu-
ally and physically. As with most calculations related to memory management, the size of the system 
cache depends on a number of factors.
Cache virtual size
On a 32-bit Windows system, the virtual size of the system cache is limited solely by the amount of 
kernel-mode virtual address space and the SystemCacheLimit registry key that can be optionally con-
figured. (See Chapter 5 of Part 1 for more information on limiting the size of the kernel virtual address 
space.) This means that the cache size is capped by the 2-GB system address space, but it is typically 
significantly smaller because the system address space is shared with other resources, including system 
paged table entries (PTEs), nonpaged and paged pool, and page tables. The maximum virtual cache 
size is 64 TB on 64-bit Windows, and even in this case, the limit is still tied to the system address space 
size: in future systems that will support the 56-bit addressing mode, the limit will be 32 PB (petabytes).
Cache working set size
As mentioned earlier, one of the key differences in the design of the cache manager in Windows from 
that of other operating systems is the delegation of physical memory management to the global 
memory manager. Because of this, the existing code that handles working set expansion and trimming, 
as well as managing the modified and standby lists, is also used to control the size of the system cache, 
dynamically balancing demands for physical memory between processes and the operating system.
The system cache doesn’t have its own working set but shares a single system set that includes 
cache data, paged pool, pageable kernel code, and pageable driver code. As explained in the section 
“System working sets” in Chapter 5 of Part 1, this single working set is called internally the system cache 
working set even though the system cache is just one of the components that contribute to it. For the 
purposes of this book, we refer to this working set simply as the system working set. Also explained in 
Chapter 5 is the fact that if the LargeSystemCache registry value is 1, the memory manager favors the 
system working set over that of processes running on the system.
Cache physical size
While the system working set includes the amount of physical memory that is mapped into views in the 
cache’s virtual address space, it does not necessarily reflect the total amount of file data that is cached 
in physical memory. There can be a discrepancy between the two values because additional file data 
might be in the memory manager’s standby or modified page lists.
Recall from Chapter 5 that during the course of working set trimming or page replacement, the 
memory manager can move dirty pages from a working set to either the standby list or the modified 
page list, depending on whether the page contains data that needs to be written to the paging file or 
another file before the page can be reused. If the memory manager didn’t implement these lists, any 
time a process accessed data previously removed from its working set, the memory manager would 
CHAPTER 11 Caching and file systems
575
have to hard-fault it in from disk. Instead, if the accessed data is present on either of these lists, the 
memory manager simply soft-faults the page back into the process’s working set. Thus, the lists serve 
as in-memory caches of data that are stored in the paging file, executable images, or data files. Thus, 
the total amount of file data cached on a system includes not only the system working set but the com-
bined sizes of the standby and modified page lists as well.
An example illustrates how the cache manager can cause much more file data than that containable 
in the system working set to be cached in physical memory. Consider a system that acts as a dedicated 
file server. A client application accesses file data from across the network, while a server, such as the 
file server driver (%SystemRoot%\System32\Drivers\Srv2.sys, described later in this chapter), uses 
cache manager interfaces to read and write file data on behalf of the client. If the client reads through 
several thousand files of 1 MB each, the cache manager will have to start reusing views when it runs out 
of mapping space (and can’t enlarge the VACB mapping area). For each file read thereafter, the cache 
manager unmaps views and remaps them for new files. When the cache manager unmaps a view, the 
memory manager doesn’t discard the file data in the cache’s working set that corresponds to the view; 
it moves the data to the standby list. In the absence of any other demand for physical memory, the 
standby list can consume almost all the physical memory that remains outside the system working set. 
In other words, virtually all the server’s physical memory will be used to cache file data, as shown in 
Figure 11-4.
Standby list
System working set
assigned to 
virtual cache
Other
~7 GB
960 MB
8 GB physical memory
FIGURE 11-4 Example in which most of physical memory is being used by the file cache.
Because the total amount of file data cached includes the system working set, modified page list, 
and standby list—the sizes of which are all controlled by the memory manager—it is in a sense the real 
cache manager. The cache manager subsystem simply provides convenient interfaces for accessing 
file data through the memory manager. It also plays an important role with its read-ahead and write-
behind policies in influencing what data the memory manager keeps present in physical memory, as 
well as with managing system virtual address views of the space.
To try to accurately reflect the total amount of file data that’s cached on a system, Task Manager 
shows a value named “Cached” in its performance view that reflects the combined size of the sys-
tem working set, standby list, and modified page list. Process Explorer, on the other hand, breaks up 
these values into Cache WS (system cache working set), Standby, and Modified. Figure 11-5 shows the 
system information view in Process Explorer and the Cache WS value in the Physical Memory area in 
the lower left of the figure, as well as the size of the standby and modified lists in the Paging Lists area 
near the middle of the figure. Note that the Cache value in Task Manager also includes the Paged WS, 
576 
CHAPTER 11 Caching and file systems
Kernel WS, and Driver WS values shown in Process Explorer. When these values were chosen, the vast 
majority of System WS came from the Cache WS. This is no longer the case today, but the anachronism 
remains in Task Manager.
FIGURE 11-5 Process Explorer’s System Information dialog box.
Cache data structures
The cache manager uses the following data structures to keep track of cached files:
I 
Each 256 KB slot in the system cache is described by a VACB.
I 
Each separately opened cached file has a private cache map, which contains information used
to control read-ahead (discussed later in the chapter in the “Intelligent read-ahead” section).
I 
Each cached file has a single shared cache map structure, which points to slots in the system
cache that contain mapped views of the file.
These structures and their relationships are described in the next sections.
Systemwide cache data structures
As previously described, the cache manager keeps track of the state of the views in the system cache 
by using an array of data structures called virtual address control block (VACB) arrays that are stored in 
nonpaged pool. On a 32-bit system, each VACB is 32 bytes in size and a VACB array is 128 KB, resulting 
CHAPTER 11 Caching and file systems
577
in 4,096 VACBs per array. On a 64-bit system, a VACB is 40 bytes, resulting in 3,276 VACBs per array. The 
cache manager allocates the initial VACB array during system initialization and links it into the sys-
temwide list of VACB arrays called CcVacbArrays. Each VACB represents one 256 KB view in the system 
cache, as shown in Figure 11-6. The structure of a VACB is shown in Figure 11-7.
VACB array list
VACB n entry
VACB 0 entry
VACB 1 entry
VACB 2 entry
VACB 3 entry
VACB 4 entry
VACB 5 entry
VACB 6 entry
VACB 7 entry
. . .
. . .
. . .
. . .
. . .
. . .
System cache
View n
View 0
View 1
View 2
View 3
View 4
View 5
View 6
View 7
. . .
. . .
. . .
. . .
. . .
. . .
VACB array list
VACB n entry
VACB 0 entry
VACB 1 entry
VACB 2 entry
VACB 3 entry
VACB 4 entry
VACB 5 entry
VACB 6 entry
VACB 7 entry
. . .
. . .
. . .
. . .
. . .
. . .
System cache
View n
View 0
View 1
View 2
View 3
View 4
View 5
View 6
View 7
. . .
. . .
. . .
. . .
. . .
. . .
System VACB array list
VACB 0 array
VACB 1 array
FIGURE 11-6 System VACB array.
Virtual address of data in system cache
Pointer to shared cache map
File offset
Active count
Link entry to LRU list head
Pointer to owning VACB array
FIGURE 11-7 VACB data structure.
578 
CHAPTER 11 Caching and file systems
Additionally, each VACB array is composed of two kinds of VACB: low priority mapping VACBs and high 
priority mapping VACBs. The system allocates 64 initial high priority VACBs for each VACB array. High 
priority VACBs have the distinction of having their views preallocated from system address space. When 
the memory manager has no views to give to the cache manager at the time of mapping some data, and 
if the mapping request is marked as high priority, the cache manager will use one of the preallocated 
views present in a high priority VACB. It uses these high priority VACBs, for example, for critical file system 
metadata as well as for purging data from the cache. After high priority VACBs are gone, however, any 
operation requiring a VACB view will fail with insufficient resources. Typically, the mapping priority is set 
to the default of low, but by using the PIN_HIGH_PRIORITY flag when pinning (described later) cached 
data, file systems can request a high priority VACB to be used instead, if one is needed.
As you can see in Figure 11-7, the first field in a VACB is the virtual address of the data in the system 
cache. The second field is a pointer to the shared cache map structure, which identifies which file is 
cached. The third field identifies the offset within the file at which the view begins (always based on 
256 KB granularity). Given this granularity, the bottom 16 bits of the file offset will always be zero, so 
those bits are reused to store the number of references to the view—that is, how many active reads 
or writes are accessing the view. The fourth field links the VACB into a list of least-recently-used (LRU) 
VACBs when the cache manager frees the VACB; the cache manager first checks this list when allocat-
ing a new VACB. Finally, the fifth field links this VACB to the VACB array header representing the array 
in which the VACB is stored.
During an I/O operation on a file, the file’s VACB reference count is incremented, and then it’s 
decremented when the I/O operation is over. When the reference count is nonzero, the VACB is active. 
For access to file system metadata, the active count represents how many file system drivers have the 
pages in that view locked into memory.
EXPERIMENT: Looking at VACBs and VACB statistics
The cache manager internally keeps track of various values that are useful to developers and 
support engineers when debugging crash dumps. All these debugging variables start with the 
CcDbg prefix, which makes it easy to see the whole list, thanks to the x command:
1: kd> x nt!*ccdbg* 
fffff800`d052741c nt!CcDbgNumberOfFailedWorkQueueEntryAllocations =  
fffff800`d05276ec nt!CcDbgNumberOfNoopedReadAheads =  
fffff800`d05276e8 nt!CcDbgLsnLargerThanHint =  