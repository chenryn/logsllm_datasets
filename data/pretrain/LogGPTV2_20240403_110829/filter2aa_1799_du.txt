### Memory Management and Caching in Windows

#### Dirty Pages Threshold and System Partitions
The dirty pages threshold, along with the number of dirty pages, is a critical aspect of memory management. When the cache manager system partition is initialized, all necessary system threads are started within the context of a System process associated with that partition. Each partition has a minimal System process, created at the time of partition creation using the `NtCreatePartition` API. 

When a new partition is created via `NtCreatePartition`, an empty `MI_PARTITION` object is also created. The memory for this new partition is either moved from a parent partition or hot-added later using the `NtManagePartition` function. A cache manager partition object is only created on-demand. If no files are created within the new partition, there is no need to create the cache manager partition object.

When the file system creates or opens a file for caching, the `CcInitializeCacheMap(Ex)` function checks which partition the file belongs to and whether the partition has a valid link to a cache manager partition. If no such link exists, the system creates and initializes a new cache manager partition through the `CcCreatePartition` routine. This new partition starts separate cache manager-related threads (e.g., read-ahead, lazy writers) and recalculates the dirty page threshold based on the number of pages in the specific partition.

The file object contains a link to its partition through its control area, initially allocated by the file system driver when creating and mapping the Stream Control Block (SCB). The partition information for the target file is stored in a file object extension (of type `MemoryPartitionInformation`) and is checked by the memory manager when creating the section object for the SCB. Generally, files are shared entities, so file system drivers cannot automatically associate a file with a different partition than the System Partition. However, an application can set a different partition for a file using the `NtSetInformationFileKernel` API, through the `FileMemoryPartitionInformation` class.

#### Cache Virtual Memory Management
The Windows system cache manager caches data on a virtual basis, using regions of system virtual address space rather than physical memory. These regions are managed in structures called Virtual Address Control Blocks (VACBs), which define the address space into 256 KB slots called views.

During the bootup process, the cache manager allocates an initial array of VACBs to describe cached memory. As caching requirements grow, the cache manager allocates more VACB arrays as needed. It can also shrink the virtual address space if other demands put pressure on the system.

At the first I/O operation (read or write) on a file, the cache manager maps a 256 KB view of the file's 256 KB-aligned region into a free slot in the system cache address space. For example, if 10 bytes starting at an offset of 300,000 bytes are read, the view mapped would begin at offset 262144 (the second 256 KB-aligned region) and extend for 256 KB.

Views of files are mapped into slots in the cacheâ€™s address space on a round-robin basis. In Figure 11-3, File B was mapped first, followed by File A and then File C. Only the first 256 KB portion of File B is mapped, as only part of the file has been accessed. Since File C is only 100 KB, it requires its own 256 KB slot in the cache.

The cache manager ensures that a view remains mapped as long as it is active, though views can remain mapped after becoming inactive. A view is marked active during a read or write operation. Unless a file is opened with the `FILE_FLAG_RANDOM_ACCESS` flag, the cache manager unmaps inactive views of a file as it maps new views, especially if the file is being accessed sequentially. Pages for unmapped views are sent to the standby or modified lists, depending on whether they have been changed. The cache manager can direct these pages to be placed at the end or front of these lists. Pages for files opened with the `FILE_FLAG_SEQUENTIAL_SCAN` flag are moved to the front of the lists, while others are moved to the end. This scheme encourages the reuse of pages from sequentially read files and prevents large file copy operations from affecting more than a small part of physical memory.

If the cache manager needs to map a view of a file and there are no free slots, it will unmap the least recently mapped inactive view and use that slot. If no views are available, an I/O error is returned, indicating insufficient system resources. Given that views are marked active only during I/O operations, this scenario is extremely unlikely, as thousands of files would need to be accessed simultaneously.

#### Cache Size
The size of the system cache, both virtually and physically, depends on several factors. On a 32-bit Windows system, the virtual size is limited by the amount of kernel-mode virtual address space and the `SystemCacheLimit` registry key. The maximum virtual cache size is 64 TB on 64-bit Windows, and even in this case, the limit is tied to the system address space size. In future systems supporting 56-bit addressing, the limit will be 32 PB.

The cache working set size is dynamically balanced by the global memory manager, which handles working set expansion and trimming, as well as managing the modified and standby lists. The system cache shares a single system working set that includes cache data, paged pool, pageable kernel code, and pageable driver code. If the `LargeSystemCache` registry value is set to 1, the memory manager favors the system working set over that of processes running on the system.

The total amount of file data cached includes the system working set, modified page list, and standby list. Task Manager shows a "Cached" value reflecting the combined size of these components. Process Explorer breaks these values into "Cache WS," "Standby," and "Modified." The "Cache" value in Task Manager also includes the "Paged WS," "Kernel WS," and "Driver WS" values shown in Process Explorer.

#### Cache Data Structures
The cache manager uses the following data structures to keep track of cached files:
- **VACB**: Each 256 KB slot in the system cache is described by a VACB.
- **Private Cache Map**: Each separately opened cached file has a private cache map, which contains information for controlling read-ahead.
- **Shared Cache Map**: Each cached file has a single shared cache map structure, which points to slots in the system cache containing mapped views of the file.

These structures and their relationships are described in the following sections.

**Systemwide Cache Data Structures**
The cache manager tracks the state of views in the system cache using an array of VACBs stored in nonpaged pool. On a 32-bit system, each VACB is 32 bytes, resulting in 4,096 VACBs per array. On a 64-bit system, a VACB is 40 bytes, resulting in 3,276 VACBs per array. The initial VACB array is allocated during system initialization and linked into the systemwide list of VACB arrays called `CcVacbArrays`. Each VACB represents one 256 KB view in the system cache.

Each VACB array includes low-priority and high-priority VACBs. The system allocates 64 initial high-priority VACBs for each VACB array. High-priority VACBs have preallocated views from system address space. When the memory manager has no views to give to the cache manager, and if the mapping request is marked as high priority, the cache manager will use one of the preallocated views. High-priority VACBs are used for critical file system metadata and purging data from the cache.

The first field in a VACB is the virtual address of the data in the system cache. The second field is a pointer to the shared cache map structure, identifying the cached file. The third field identifies the offset within the file at which the view begins. The fourth field links the VACB into a list of least-recently-used (LRU) VACBs, and the fifth field links the VACB to the VACB array header.

During an I/O operation, the file's VACB reference count is incremented and decremented when the operation is complete. When the reference count is nonzero, the VACB is active. For file system metadata, the active count represents how many file system drivers have the pages in that view locked into memory.

**Experiment: Looking at VACBs and VACB Statistics**
The cache manager internally keeps track of various values useful for debugging crash dumps. These debugging variables start with the `CcDbg` prefix, making them easy to list using the `x` command in the kernel debugger:

```plaintext
1: kd> x nt!*ccdbg* 
fffff800`d052741c nt!CcDbgNumberOfFailedWorkQueueEntryAllocations =  
fffff800`d05276ec nt!CcDbgNumberOfNoopedReadAheads =  
fffff800`d05276e8 nt!CcDbgLsnLargerThanHint =
```

This command provides a list of all `CcDbg` variables, which can be useful for developers and support engineers.