evaluation. Each model is trained for 100 iterations and each
iteration uses 64 images from MNIST [67] in a batch. In
total 1,000 sequences are collected, and the adversary’s goal
is to learn which model it corresponds to. Table IV shows the
models we use for evaluation. The third column of Table III
shows the parameters of AttBLSTM for this task.
TABLE IV: Machine-learning models evaluated under IN-
VISIPROBE.
Model
FC
CPF
CPCPF
DCGAN [68]
RNN-LSTM [69]
AutoEncoder [69]
RNN-Attention [70]
Resnet-18 [71]
Inception-v3 [72]
Regression
Description
A simple model with only 1 FC layer
A model with Conv-Pool-FC layers
A model with Conv-Pool-Conv-Pool-FC layers
Deep Convolutional GAN
RNN model with LSTM layer
CNN model with encoder and decoder
RNN model with Attention layer
A widely used CNN model for image recognition
A widely used CNN model for image recognition
A simple logistic regression model
Similar to the setting of Section VI-C, we split the se-
quences to 85% training and 15% testing, and evaluate the top-
N accuracy. The result shows the top-1 accuracy can achieve
100% for all models. We speculate such high accuracy is
caused by the long execution time for each iteration, which
leaves abundant information to be used by INVISIPROBE.
We also attempted to infer the layers composing a DNN
model from a delay sequence, however, we are unable to obtain
a satisfying result, because segmenting the sequence by layer
is very difﬁcult. Appendix B elaborates this attempt.
E. Webpage Inference with NVMe SSD
In this attack, we use the same set of webpages (Alexa top
100) and experiment setting (85% training and 15% testing)
as Attack 2 (Section VI-C), except that the attacker collects
delay sequence from NVMe SSD. The last column of Table III
shows the parameters of AttBLSTM for this task.
Overall,
the top-1 accuracy reaches 93.29%, and top-3
accuracy reaches 98.71% for the classiﬁcation mode. The
accuracy is comparable to that of Attack 2, though a slight
drop is observed. We found the main cause of the errors is
also the unstable network conditions.
Classiﬁcation Model
3-layer fully-connected
LSTM
Bi-directional LSTM
AttBLSTM
Top-1 Acc.
48.92%
84.25 %
87.38%
93.29%
Top-3 Acc.
64.44%
94.00%
96.10%
98.71%
Embedding Acc.
67.26%
83.00%
83.46%
88.17%
TABLE V: Accuracy (Acc.) of Attack 4 with different classi-
ﬁcation models.
Comparison between inference models. Comparing with a
normal neural network model, like a fully-connected model,
AttBLSTM adds three features, including long-sequence learn-
ing with LSTM, bi-directional sequence processing, and at-
tention mechanism. Here we rerun the experiment of Attack
4 by stripping off each feature, in order to learn how much
performance gain is brought by them. Table V shows the
comparison. We use the 3-layer fully-connected model as the
baseline, and LSTM signiﬁcantly improves the performance
(Top-1 accuracy raised from 48.92% to 84.25%), indicating
the recurrent models are much better positioned to handle
this task. BLSTM raises the Top-1 accuracy over LSTM by
3.13% and AttBLSTM improves over BLSTM by 5.91%,
suggesting these two features are necessary. The trend of Top-
3 classiﬁcation accuracy and embedding accuracy are similar.
Performance gain with kernel-bypass. We assume the
kernel-bypass driver (e.g., SPDK) has been installed on the
server for the attack leveraging NVMe SSD, as described in
Section III-B. The kernel-bypass driver is expected to help
the attack obtain a stable measurement of delays, and here we
evaluate this claim.
In particular, we rerun Attack 4 but use the standard system
calls to collect the delays. Speciﬁcally, sys_read system call
is repeatedly invoked to read a 4K Block from a ﬁle at random
positions. To force the OS to fetch data from NVMe disk
without using buffers, we open the ﬁle in O_DIRECT mode.
We look into the quality of sampled delays, and the result
proves our assumption. Table VI compares the average IOPS
(I/O operations per second) by the probe when one webpage
is tested, and it shows without SPDK, much fewer samples
can be collected (57,603 compared to 839,546). The reason is
that the context switching between ring 0 and ring 3 wastes
many CPU cycles. Besides, more random noise is introduced
with sys_read: the standard deviation of the collected delay
becomes 71 microseconds, while it is only 0.22 microseconds
for the SPDK case. Regarding the accuracy of Attack 4, it is
dropped to 90.13% and 96.53% for top-1 and top-3, shown in
Table VI.
SPDK
sys_read
Alibaba Cloud with io_uring
TABLE VI: IOPS of different probe designs and inference
accuracy for Attack 4.
IOPS
839,546
57,603
12,757
Top-1 Acc.
Top-3 Acc.
93.29%
90.13%
91.02%
98.71%
96.53%
97.77%
F. Attack on Public Cloud
The prior experiments are done in a lab environment. To
evaluate the practical impact of INVISIPROBE, we implement
Attack 4 in the Alibaba cloud. We did not implement Attack
1-3 because we cannot ﬁnd a public cloud matching the
attack condition, that two VMs with RDMA connection can
be rented. Tsai et al.
[24] tested their RDMA attack in
CloudLab [73], but we found CloudLab uses AMD CPUs,
which uses I/O die inside the CPU to forward PCIe trafﬁc.
Speciﬁcally, AMD systems do not break CPU’s PCIe port into
multiple ports with a switch. Instead, they use an I/O chip to
connect all PCIe ports, cores and memory controllers together,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
332
in which case a PCIe port not only conﬂict with other ports
but also cores and memories, producing much more noises.
One interesting observation we gain from inspecting the
public cloud is that I/O virtualization is extensively leveraged.
In this case, cloud vendors do not use the PCH to serve
SSDs and NICs together. I/O devices including SSDs, NICs
are all virtualized by hardwares, which are called Nitro Cards
by Amazon [74] and MOC Cards by Alibaba [75]. We call
them vcard collectively. The vcard rather than PCH act as an
I/O switch to handle PCIe trafﬁc. Particular for Alibaba, a
cloud server has only one MoC, no matter how many devices
it virtualizes. Therefore, similar to PCH, vcard becomes the
bottleneck for PCIe trafﬁc.
As for the end-to-end attack, we rented an instance (type
ecs.ebmc6.26xlarge) from Alibaba cloud and launched
Attack 4. We run the attacker’s code on the instance to access
NVMe SSD and the webpages are visited by Google Chrome
on the instance. We found directly running the attack code
is infeasible, because the virtualized SSD does not support
SPDK. Fortunately, the OS kernel of the instance supports
io_uring, a kernel-bypass framework provided by Linux.
Therefore, we substitute the part relying on SPDK with
io_uring and run the attack code. It turns out top-1 accuracy
and top-3 accuracy are dropped to 91.02% and 97.77%, as
shown in Table VI. The main reason is that the sampling
quality of INVISIPROBE degrades on vcard: only 12,757 IOPS
is produced. Yet, high attack effectiveness is demonstrated.
VII. DISCUSSION
A. Potential Mitigation
The evaluation result suggests INVISIPROBE can harm
another user’s security and privacy sharing a machine. On
the other hand, defending against INVISIPROBE is quite chal-
lenging. Other remote cache side-channel attacks [23], [24]
can be mitigated by isolation (e.g., LLC partitioning [23] and
separating protection domains [24]). However, simply isolating
trafﬁc between devices might eliminate the beneﬁt brought by
PCIe. Below we discuss a few directions.
Blocking high-resolution clock instructions. Assuming the
data center operator can decide what instructions are available
to the upper-layer applications, i.e., the probe code written
by the attacker, restricting the access to those instructions
might deter INVISIPROBE. For instance, when using NIC to
attack GPU, the hardware clock has to be obtained by the
adversary by setting ﬂags in the RDMA read queue. When
using NVMe SSD to attack NIC, RDTSCP needs to be invoked
by the adversary. However, legitimate applications also use
those primitives to measure their performance. Even if certain
instructions can be banned, e.g., RDTSCP, the attacker can
switch to other instructions, e.g., using multiple threads and
monitoring the interrupts (“Low-Requirement Interrupt Timing
Attack” of
Reducing the delay variance. As the attacker can construct
a probe quite sensitive to the delays caused by congestion, an-
other solution is to reduce the delay variance, so the congestion
[47]).
states of the PCIe link can be concealed. One can increase
the capacity of PCIe links, but doing so would incur high
upgrading cost and there is no guarantee that congestion will
never be triggered. Introducing noises, i.e., increasing delays,
would confuse the attacker, but the extra overhead could be
non-negligible, especially to scenarios that are delay-sensitive,
e.g., training deep neural networks.
Detecting the suspicious probe requests. The adversary
needs to maintain a non-empty queue of probe requests to
keep collecting measurement data for the whole life-cycle of
a victim operation. Blocking probe request individually is not
a feasible solution, as reading a small chunk of memory by
RDMA has legitimate use cases like distributed locks [76].
A more viable solution could be letting a security application
on I/O switch to inspect the request sequence, identify the
anomalies, and notify the data center operators. In fact, a
few recent works have shown that programmable network
switches can be leveraged to detect DDoS attacks [77], [78].
However, the current hardware of I/O switch does not support
this idea, i.e., not programmable. As revealed by our end-
to-end experiment (Section VI-F), I/O switch is virtualized by
some public cloud. Hence, an alternative approach is to deploy
the defense on the vcard, and we are discussing this approach
with cloud providers like Alibaba.
To notice,
though processor vendors like Intel provide
counters like PCM [79] to monitor PCIe bandwidth, it covers
PCIe stop inside the chip only. What happens at the PCIe
switch is oblivious to those counters, so the congestion caused
by INVISIPROBE cannot be detected.
Better QoS on I/O switch. Though programming I/O switch
to detect INVISIPROBE might be infeasible now due to the
hardware restrictions, improving the QoS logic of I/O switch to
better serve legitimate applications might deter INVISIPROBE.
For example, by prioritizing I/O devices of high IOPS (I/O per
second), the switch can guarantee relatively stable and lower
delay on those devices, which reduces the inference accuracy
of INVISIPROBE. PCIe congestion has been studied in PCIe
fabric and a few congestion-aware models were proposed [29],
[30]. Within the PCIe standard, virtual channel [80] aims
at a similar goal, by mapping dedicated physical resources
like buffers to high-priority transactions, eliminating resource
conﬂicts with the low-priority trafﬁc. We plan to investigate
whether they could thwart INVISIPROBE and the follow-up
counterattack in the future.
B. PCIe fabric
Resource disaggregation is a general trend for data centers
and cloud [81]. In a fully resource-disaggregated rack, I/O
devices are all connected to PCIe switches, which constitutes
a PCIe fabric, as shown by Figure 7. CPUs also connect to the
PCIe fabric as nodes. In this case, the attack surface will be
largely broadened: all I/O devices in the rack will be exposed
to attackers for probing. So far, there lacks comprehensive
security analysis of PCIe fabric and we believe this is an urgent
topic for the security community.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
333
Fig. 7: PCIe fabric.
C. Limitations
User-input inference. We aim to accurately infer the words
typed by a victim in Attack 1, though there is still room for
improvement (e.g., top-10 accuracy is 66.7%). Future work
could evaluate models other than HMM and use the context
to ﬁlter the inferred words. Given only the coarse-grained
feature, say keystroke timing, is available to the adversary,
INVISIPROBE cannot directly learn more sensitive information
like password and credit card numbers. Still, we believe the
word recovery example has demonstrated that PCIe congestion
side-channel should be mitigated.
Webpage inference. For Attack 2 and 4, we classiﬁed 100
webpages in both scenarios, which might be considered as a
small dataset. The main reason is that data collection is time-
consuming: every visit costs 10 seconds and each webpage
is visited repeatedly for 150 times. We plan to increase the
dataset in the future by running multiple machine instances.
On the other hand, as our dataset covers webpages of a
variety of categories, we believe the result is representative.
We also assume a close-world setting where the victim visits
the webpages that have been proﬁled. As a next step, we will
evaluate whether the victim is also vulnerable under the open-
world setting.
Model inference. For Attack 3, INVISIPROBE can tell which
model a victim is executing but the layers cannot be inferred
separately. We acknowledge this cannot fully reveal the model
structure. On the other hand, if the attacker can proﬁle a large
number of models ahead, and compute the distance between a
delay sequence to the proﬁles, she might ﬁnd a similar model
structure, which still helps her attempt in IP infringement and
adversarial attacks. We will investigate this option.
Combinations of attack and victim devices. We tested two
device combinations. Potentially, other device combinations
might be vulnerable as well, but due to the high cost of
testing one combination, we leave the exploration of other