n
a
b
k
n
a
b
k
n
a
b
bus
DRAM chips
(c) DRAM device
r
e
l
l
o
r
t
n
o
c
y
r
o
m
e
m
Figure 2: DRAM organization.
Figure 2b shows how cells are packed in a grid to form a
subarray [62]. A wordline spans a row of DRAM cells, typically
one or more KiB long, and is the minimum granularity of
DRAM array operation. Upon a DRAM access, the row decoder
drives the corresponding wordline, thus activating the row.
This allows the charge stored in each cell along the activated
row to be sensed by the sense amplifiers connected to the cells’
respective bitlines. Because cells in a column share a single
bitline, a subarray may have only one row active at a time [62].
Multiple subarrays are aggregated to form a larger array
referred to as a bank, and banks are in turn combined to form
a chip as depicted in Figure 2c. I/O circuitry within each chip
interfaces the individual banks with the external DRAM bus.
A set of DRAM chips that share a common bus is known as
a rank, and one or more ranks may be combined via rank
selection signals to form a single DRAM channel. The DRAM
bus is connected to a memory controller, which typically resides
within the processor die. Each DRAM access transfers one burst
of data that consists of multiple bus-width beats. For LPDDR4
DRAM, bursts are typically 32B or 64B long, and each beat is
16 bits long [44].
3.2. DRAM Timings and Errors
The memory controller interfaces with DRAM according to
manufacturer-specified timing parameters,2 which guarantee
correct DRAM behavior by providing enough time in between
DRAM commands for internal DRAM circuitry to stabilize.
Our work primarily deals with with DRAM refresh timings
(Section 3.2.1) and the data-retention errors that result from
violating refresh timing specifications (Section 3.2.2).
3.2.1. DRAM Refresh Timing. DRAM cell capacitors inher-
ently lose charge over time [79,80,104], potentially resulting in
data corruption. A cell’s retention time defines how long it can
reliably store data and typically varies between cells from mil-
liseconds to many hours [33,49,51,59,70,76,79,80,98,99,124]. To
prevent data loss, the memory controller regularly refreshes the
2We encourage the interested reader to refer to the JEDEC specification [44]
for an exhaustive list of all available parameters and their usage.
15
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:57:59 UTC from IEEE Xplore.  Restrictions apply. 
entire DRAM memory using periodic REF commands, which
are scheduled according to a timing parameter called the re-
fresh window (tREFW ). tREFW defines the maximum amount
of time allowed between consecutive refresh operations to a
given DRAM cell. In our work, we experimentally test LPDDR4
DRAM devices (Sections 6, 7, and 8), for which tREFW is 32ms
at typical operating conditions [44].
3.2.2. Violating Recommended Timings. We can in-
duce errors3 in real DRAM devices by deliberately violating
manufacturer-recommended timings. The resulting error dis-
tributions allow us to: 1) reverse-engineer various proprietary
DRAM microarchitectural characteristics [45, 70, 72] and 2) un-
derstand the behavior of different DRAM error mechanisms
(e.g., charge leakage [30, 79], circuit crosstalk [52, 61, 111]).
By increasing tREFW , we observe data-retention errors in
certain cells with higher charge leakage rates [30,49,59,80,124].
The quantity and locations of these errors depend on i) the
data pattern programmed into cells, ii) the layout of true- and
anti-cells in DRAM [65,79], and iii) environmental factors such
as operating temperature and voltage [15, 30, 51–54, 61, 75, 76,
79, 80, 98]. Section 4.3 discusses the statistical characteristics of
data-retention errors in greater depth.
3.3. Block Error-Correction Codes
Block coding enables data communication over a noisy channel
by breaking the data stream into datawords of length k, where
each element of the dataword is a symbol representing q bits of
data. During encoding, the ECC encoder maps each dataword
to a single codeword of length n using n – k redundant symbols.
Each symbol is a function (e.g., xor-reduce) of a subset of the
data symbols such that an error will cause one or more of these
functions to evaluate incorrectly. Encoding results in qk valid
codewords out of qn possible n-symbol words. Upon receiving
an n-symbol word that may contain erroneous symbol(s), the
ECC decoder attempts to determine the originally transmitted
dataword using a decoding algorithm.
As a demonstrative example, we consider a common decod-
ing algorithm for binary (i.e., q = 2) block codes known as
maximum-likelihood decoding, which uses Hamming distance
as a metric to find the closest valid codeword to a received
word. Using this approach, the error-correction capability, ort ,
is defined by the minimum Hamming distance, or d, between
any two valid codewords in the space of all valid codewords.
With d = 2, a single-symbol error can always be detected but
not always corrected since there may exist two valid code-
words equidistant from the received word.
In general, the
error-correction capability can be computed using the rela-
(cid:4), which shows that a minimum Hamming
tionship t = (cid:3) d–1
distance of at least 3 is necessary for single-symbol correction
and 5 for double-symbol correction.
2
When faced with more errors than the code can correct,
the decoding result is implementation-defined based on the
exact circuitry used to implement the encoding and decoding
3According to the IEEE TCRTS [2], a fault is a defect inherent to a system,
an error is a discrepancy between intended and actual behavior, and a failure
is an observed instance of incorrect behavior. We conform to this terminology
throughout this manuscript.
16
algorithms. This is because a code designer has complete free-
dom to choose the precise functions that map data symbols
to each redundant symbol, and the same errors induced in
two different code implementations can result in two different
post-correction words. In each implementation, the decoding
logic may i) manage to correct one or more actual errors, ii)
mistakenly do nothing, or iii) “miscorrect” a symbol that did
not have an error, effectively exacerbating the number of errors
in the decoding result.
Throughout this work, we follow a commonly used notation
for ECC block codes, in which a tuple (n, k, d) describes the
length of the codeword (n), the length of the dataword (k),
and the minimum Hamming distance (d), respectively. This
allows us to concisely express the type and strength of a block
code. However, certain codes are also well-known by name
(e.g., Repetition (REP) [22], Hamming Single-Error Correction
(HSC) [31], Bose-Chaudhuri-Hocquenghem (BCH) [6, 36]), and
we will use these names where appropriate.
4. Statistically Modeling DRAM and ECC
We begin by formalizing the relationship between pre- and
post-correction error distributions and expressing reverse-
engineering as a maximum a posteriori (MAP) estimation
problem. Our approach is grounded on the key idea that
pre-correction errors arise from physical error mechanisms
with known statistical properties, and because different ECC
schemes transform these distributions in different ways, we
can use what we know about both the pre- and post-correction
error distributions to disambiguate different ECC schemes. This
section provides a step-by-step derivation of EIN, the statistical
inference methodology we propose in this work.
4.1. Statistically Modeling Error Correction Codes
Consider an ECC mechanism implementing an (n, k, d) binary
block code as illustrated in Figure 3. The ECC encoding al-
gorithm fenc, ECC transforms a dataword w out of the set of
all possible datawords W = Zk
2 into a valid codeword c out
of the set of all possible valid codewords C ⊂ Zn
2. Likewise,
the decoding algorithm fdec, ECC transforms a codeword c
(po-
tentially invalid due to errors) out of the set of all possible
codewords C(cid:2)
out of the set
of all possible corrected datawords W(cid:2)
2 into a corrected dataword w
= Zn
(cid:2)
(cid:2)
= Zk
2.
(n, k, d) On-Die ECC Mechanism
dataword[k-1:0]
w ∈ W
ECC Encoder
fenc, (n, k, d) : W → C
codeword[n-1:0]
c ∈ C
CPU
corrected
dataword[k-1:0]
w’ ∈ W’
ECC Decoder
fdec, (n, k, d) : C’ → W’
potentially
erroneous 
codeword[n-1:0]
c’ ∈ C’
DRAM
Figure 3: Illustration of an on-die ECC mechanism implement-
ing an (n, k, d) binary block code.
fdec, ECC can be thought of as a deterministic mapping4 from
the finite set of inputs C(cid:2)
to a finite set of outputs W(cid:2)
:
fdec, ECC : C(cid:2) (cid:6)→ W(cid:2)
(1)
4While non-deterministic encoding/decoding algorithms exist, they are
typically not used with the simple ECCs found in DRAM. If more complex ECCs
must be considered (e.g., LDPC [7, 20]), our models can be extended to treat
the encoding/decoding functions as probabilistic transformations themselves.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:57:59 UTC from IEEE Xplore.  Restrictions apply. 
This means that for a particular ECC scheme fi, the probability
is determined by the probabilities that
of observing output w
(cid:2)} occur:
its corresponding inputs {c
(cid:2)
(cid:2)
j ) = w
(cid:2)
Pfi [w
] =
(cid:2)
j
∈ C(cid:2)
, ∀j : fdec, i(c
(cid:2)
(cid:2)
j ]
∀j : fdec, i(c
(cid:2)
j )=w
P[c
(cid:2)
(2)
From this perspective, if we know both 1) the ECC scheme fi
and 2) the frequency distribution of all possible input values ck,
we can calculate the corresponding distribution of all possible
output values.5 Inverting this relationship, if we experimentally
measure the frequency distribution of output values from a real
device, we can determine the probability of having made such
observations given 1) a suspected ECC scheme and 2) an ex-
pected frequency distribution of all possible inputs. Section 4.3
describes what we know about pre-correction error distribu-
tions and how we leverage this knowledge to disambiguate
different suspected ECC schemes.
4.2. Experimental Observables
Solving Equation 2 requires measuring the relative frequency
(cid:2)
distribution of post-correction datawords (i.e., w
i ). For exam-
ple, if we use 64-bit datawords, we have 264 unique datawords.
Unfortunately, a single DRAM device has on the order of mil-
lions of datawords, which is nowhere near enough to obtain a
representative sample of the full distribution.
Instead, we divide W(cid:2)
into N + 1 subsets, W(cid:2)
n, which each
comprise all possible datawords with n ∈ [0, N ] errors. Using
this approach, a relative frequency distribution of the W(cid:2)
n con-
tains only N + 1 categories, and even a single DRAM device
contains more than enough samples to obtain a representative
distribution. Experimentally, measuring the number of errors
in each dataword simply requires counting errors.6 We can
then rewrite Equation 2 in terms of the subsets W(cid:2)
n:
(cid:2)
j ]
(cid:2) ∈ W(cid:2)
Pfi [w
Pfi [c
n] =
(3)
(cid:2)
∀j : fdec, i(c
j )∈W(cid:2)
(cid:2)
n
Unfortunately, this approach requires knowing the exact
layout of ECC words in memory. This may be difficult since
multiple bits are read/written together at the granularity of a
burst (Section 3.1), and each burst may contain one or more
ECC words with an arbitrary bit-to-ECC-word mapping.
To circumvent this problem, we instead consider the prob-
ability of observing n errors per burst, where each burst com-
prises dataword(s) from one or more ECC schemes. Mathemat-
ically, the total number of errors in a burst is the sum of the
individual per-dataword error counts and is computed by con-
volving the per-dataword error-count distributions. Counting
errors at burst-granularity is independent of the layout of ECC
words within a burst assuming that ECC words are contained
within burst boundaries so that bursts can be read/written in-
dependently. However, if a different design is suspected, even
longer words (e.g., multiple bursts) may be used as necessary.
5Note that since ECC decoding is generally not injective (i.e., multiple
codewords may map to a single decoded dataword), we cannot determine
exactly which input produced an observed output.
6There is no fundamental reason for this choice beyond experimental con-
venience; if another choice is made, our analysis still holds but will need to be
modified to accommodate the new choice of W(cid:2)
n.
17
4.3. Statistically Modeling DRAM Errors
∈ C(cid:2)
To estimate the relative frequencies of the pre-correction code-
(cid:2)
words c
, we exploit the fact that errors arise from physical
j
phenomena that follow well-behaved statistical distributions.
Throughout this work, we focus on data-retention error distri-
butions since they are well-studied and are easily reproduced in
experiment. However, EIN is applicable to any experimentally-
reproducible error distribution whose statistical properties are
well-understood (e.g., reduced activation-latency [12, 56–58,
70, 72], reduced precharge-latency [12, 117, 118], reduced volt-
age [15], RowHammer [61, 89, 90]).
As described in Section 3.2.2, data-retention errors occur
when a charged cell capacitor leaks enough charge to lose its
stored value. This represents a “1” to “0” error for a charged
true-cell (i.e., programmed with data “1”), and vice-versa for
an anti-cell [19, 61, 79]. Due to random manufacturing-time
variations [23,59,61,70,71,76,137], certain cells are more prone
to data-retention errors than others [30, 49, 59, 79, 80, 124]. Fur-
thermore, absolute data-retention error rates depend on operat-
ing conditions such as refresh timings, data patterns, ambient
temperature, and supply voltage. Through extensive error char-
acterization studies, prior works find that, for a fixed set of
testing conditions (e.g., tREFW , temperature), data-retention
errors show no discernible spatial patterns [5, 30, 80, 112, 124]
and can be realistically modeled as uniform-randomly dis-
tributed [5, 57, 112] independent events [112].
To model an arbitrary pre-correction error distribution in
our analysis, we introduce an abstract model parameter θ that
encapsulates all state necessary to describe the distribution. In
general, θ is a set of two key types of parameters: i) experimen-
tal testing parameters (e.g., data pattern, timing parameters,
temperature) and ii) device microarchitectural characteristics
(e.g., spatial layout of true- and anti-cells). We incorporate θ
into our analysis as a dependency to the terms in Equation 3:
(cid:2) ∈ W(cid:2)
n] =
Pfi,θ[w
(cid:2)
j ]
Pfi,θ[c
(4)
(cid:2)
∀j : fdec, i(c
j )∈W(cid:2)
(cid:2)