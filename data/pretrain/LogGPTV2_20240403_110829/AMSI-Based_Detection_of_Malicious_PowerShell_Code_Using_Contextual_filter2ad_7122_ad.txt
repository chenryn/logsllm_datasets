### Data Collection and Deduplication

The dataset was collected over non-overlapping time periods. As described in Section IV, we deduplicated our labeled data to ensure that the test set contains only instances not seen in the training set. This implies that our True Positive Rate (TPR) results represent a lower bound on the actual TPR. In practice, many instances observed in the training data are likely to reappear in new data, which would typically be correctly classified by the detectors.

### Table III: Area Under the ROC Curve (AUC) and TPR per Model, FPR ≤ 10−3

| Model                          | AUC   | Train  | Validation | Test  |
|--------------------------------|-------|--------|------------|-------|
| Token-Char-FastText            | 0.994 | 0.949  | 0.929      | 0.894 |
| Token-Char-W2v                 | 0.995 | 0.972  | 0.922      | 0.810 |
| Token-Char                     | 0.991 | 0.997  | 0.928      | 0.775 |
| CNN-FastText                   | 0.987 | 0.939  | 0.916      | 0.769 |
| CNN-W2V                        | 0.994 | 0.976  | 0.944      | 0.779 |
| CNN                            | 0.994 | 0.999  | 0.943      | 0.711 |
| CNN-RNN-FastText               | 0.991 | 0.937  | 0.921      | 0.818 |
| CNN-RNN-W2V                    | 0.994 | 0.962  | 0.929      | 0.805 |
| CNN-RNN                        | 0.991 | 0.997  | 0.930      | 0.736 |
| CNN-4                          | 0.994 | 0.958  | 0.936      | 0.799 |
| Char-3-gram                    | 0.993 | 0.893  | 0.867      | 0.667 |
| Token-2-gram                   | 0.994 | 0.894  | 0.898      | 0.643 |

**Note:** Standard deviations are less than 0.005 on the validation set, 0.01 on the training set, 0.03 on the test set, and 0.003 for the AUC.

### Impact of Pretrained Embedding Layers

#### Overfitting and Generalization

The high TPR values on the training set, especially for models without pretrained embeddings, suggest overfitting. For example, the Token-Char model achieves a TPR of 0.997 on the training set but only 0.775 on the test set. In contrast, the Token-Char-FastText model, which uses a pretrained embedding, has a more balanced performance with a TPR of 0.949 on the training set and 0.894 on the test set. This indicates that the pretrained embedding helps reduce overfitting.

#### Comparison of FastText and W2V

In the Token-Char architecture, the FastText model outperforms the W2V model on the test set, achieving a TPR of 0.894 compared to 0.810 for W2V. This trend is consistent across other architectures, though the differences are less pronounced. The superior performance of FastText on the test set suggests it generalizes better, possibly due to its ability to interpret unseen tokens using sub-tokens.

### Key Conclusions

1. **Performance of DL Models**: Deep learning (DL) detection models significantly outperform traditional NLP models.
2. **Impact of Pretrained Embeddings**: Pretrained embeddings significantly improve TPR on the test set, with improvements of 11.9 pp for the Token-Char architecture, 8.2 pp for the CNN-RNN architecture, and 6.8 pp for the CNN architecture.
3. **Best Model Performance**: The best model, Token-Char-FastText, exceeds the TPR of the 4-CNN model from [15] by 9.5 pp.

### Importance of Analyzing Training Set TPR

Analyzing TPR on the training set, in addition to the validation set, is crucial to avoid selecting an overfitted model. When two models have similar TPRs on the validation set, the TPR on the training set can help determine which model will generalize better on unseen data.

### Contribution of Contextual Embeddings

#### Non-Labeled Data Contribution

To quantify the contribution of non-labeled data, we trained an embedding layer using only the training set and compared it with the original embedding trained on both the training set and the unlabeled dataset. The results, shown in Table IV, indicate that the unlabeled dataset significantly contributes to the TPR, especially for the Token-Char-FastText model, which saw a 7.1 pp increase in TPR when using the full dataset.

#### Detection Examples

1. **W2V Embedding Example**:
   - **Code Snippet**:
     ```powershell
     Invoke-WebRequest -Uri http:///ry.exe -OutFile ([System.IO.Path]::GetTempPath()+’c.exe’);
     powershell.exe Start-Process -Filepath ([System.IO.Path]::GetTempPath()+’c.exe’);
     ```
   - **Observation**: Replacing `Invoke-WebRequest` with its alias `IWR` caused a 5 pp decrease in the score for the CNN-RNN model with inline embedding but no change for the CNN-RNN-W2V model. This is because W2V maps `Invoke-WebRequest` and `IWR` to semantically equivalent vectors.

2. **FastText Embedding Example**:
   - **Tokens Analysis**: Tokens like `responsetext`, `responsebody`, and `xmlhttp` appeared benign based on the training set but were detected as malicious by the CNN-RNN-FastText model. Sub-tokens such as `http` and `spo` had a higher ratio of malicious instances, aiding in the detection.

### Summary

The use of pretrained embeddings, particularly FastText, significantly improves the generalization and TPR of deep learning models. The contribution of non-labeled data to the contextual embedding is substantial, and specific examples demonstrate how these embeddings enhance detection performance.