set were collected over disjoint periods of time. Moreover,
as we described in Section IV, we deduplicated our labeled
data, so that the test set contains only instances that were not
seen in the training set. This implies that our TPR results
are, in fact, a lower bound on the actual TPR because, in
practice, many instances that are observed in the training data
are likely to also appear in new data to which the detectors
9
TABLE III.
AREA UNDER THE ROC CURVE (AUC) AND TPR PER
MODEL, FPR≤ 10−3. STANDARD DEVIATIONS ARE LESS THAN 0.005 ON
THE VALIDATION SET, 0.01 ON THE TRAINING SET, 0.03 ON THE TEST SET
AND 0.003 FOR THE AUC
Model
Token-Char-FastText
Token-Char-W2v
Token-Char
CNN-FastText
CNN-W2V
CNN
CNN-RNN-FastText
CNN-RNN-W2V
CNN-RNN
CNN-4
Char-3-gram
Token-2-gram
AUC
0.994
0.995
0.991
0.987
0.994
0.994
0.991
0.994
0.991
0.994
0.993
0.994
Train
0.949
0.972
0.997
0.939
0.976
0.999
0.937
0.962
0.997
0.958
0.893
0.894
Validation
Test
0.929
0.922
0.928
0.916
0.944
0.943
0.921
0.929
0.930
0.936
0.867
0.898
0.894
0.810
0.775
0.769
0.779
0.711
0.818
0.805
0.736
0.799
0.667
0.643
are applied. Since TPR results on the training data are very
high, these duplicated instances are very likely to be classiﬁed
correctly. However, because of de-duplication, such instances
do not appear in our test set. A second possible explanation to
the lower performance on the test set is that the models were
overﬁtted to the validation set during the process of hyper-
parameters tuning and DL architecture selection.
Focusing on the DL models, it is noteworthy to observe
the impact of the pretrained embedding layer. First, inspecting
the results on the training set, the TPR of models without the
pretrained embedding is above 0.99 (these are the entries in red
font in the “Train” column). These extremely high TPR values
are a strong indication of overﬁtting. Indeed, the overﬁtting of
models without pretrained embeddings is evident from their
lower performance on the test set.
For instance, focusing on the results of the Token-Char
architecture, let us compare the results of the Token-Char and
the Token-Char-FastText models. On the training set, Token-
Char overﬁts with TPR of 0.997 while Token-Char-FastText
obtains a TPR of 0.949. On the validation set, Token-Char-
FastText’s TPR very slightly outperforms that of Token-Char.
The reduction in overﬁtting gained by using the pretrained
embedding is established by the results on the test set, where
Token-Char-FastText’s TPR improves over that of Token-Char
by almost 12 pp, from 0.775 to 0.894 (see the red-font entry
at the top of the “Test” column in Table III). Similar results
(although with smaller gaps) can be observed in the CNN-
RNN architecture, where the TPR on the test set improves
from 0.736 to 0.818, and in the CNN architecture, where it
improves from 0.711 to 0.769. A plausible explanation for
these results is that a pretrained embedding enables the model
to leverage contextual relationships that are absent from the
labeled dataset, thus becoming less susceptible to overﬁtting.
Next, we compare the results obtained when using the two
types of embedding – FastText and W2V. We start our com-
parison with the models of the Token-Char architecture, where
the differences in performance between the two embedding
algorithms on the test set are more signiﬁcant. On the training
set, the TPR of the W2V model exceeds that of FastText by
approximately 2.3 pp. As we’ve already observed, superior
performance on the training set
is often a sign of higher
overﬁtting and this seems to be the case also here. Indeed,
FastText takes the lead on the validation set and outperforms
W2V by approximately 0.7 pp. The gap becomes much more
signiﬁcant on the test set, where Token-Char-FastText is the
best model with a TPR of more than 0.89, exceeding the TPR
of Token-Char-W2V by almost 8.5 pp.
A similar trend, although much less pronounced, is ob-
served on the CNN and the CNN-RNN architectures. W2V’s
TP is superior to that of FastText on the training set (by 3.7 pp
and 2.5 pp, respectively), but the gaps are slightly decreased
on the validation set (2.8 pp and 0.8 pp, respectively) and
signiﬁcantly decreased or even reversed on the test set (1 pp
and -1.3 pp, respectively).
Collectively, these results seem to indicate that, in our
setting, models employing FastText are better at generalizing
as compared with those based on W2V. A possible explanation
is that FastText is better in interpreting tokens that were not
seen in the training set but appear in the validation or test sets.
This is because FastText utlilizes sub-tokens in the embedding
process.
Summing up our analysis of TPR results, we reach the
following key conclusions:
1)
2)
3)
The DL detection models signiﬁcantly outperform the
traditional NLP models.
Pretrained embedding signiﬁcantly improved TPR on
the test set: by 11.9 pp in the Token-Char architecture,
and by 8.2 and 6.8 pp in the CNN-RNN and CNN
architectures, respectively.
The TPR of our best model, Token-Char-FastText,
exceeds that of 4-CNN, the best model of [15], by
9.5pp.
Another, more general conclusion, is the following: in some
cases, it is important to analyze the TPR on the training set and
not only on the validation set alone, in order to avoid selecting
an overﬁtted model. As evident from our evaluation, when two
models reach more-or-less the same TPR on the validation set,
the TPR on the training set can help us determine which model
will generalize better on unseen data.
We proceed to analyze in ﬁner resolution the manner in
which contextual embedding improves detection performance.
C. The Contribution of Contextual Embeddings
In this section, we analyze the contribution of contextual
embedding. We start by measuring the contribution to the
model TPR that is gained by using non-labeled data in the
contextual embedding. We then describe and analyze speciﬁc
examples of malicious PowerShell tokens and code whose
detection is facilitated by using the embedding.
1) Contribution of Non-Labeled Data: In Section VII, we
evaluated 12 malicious-PowerShell-code detectors (see Table
III), 6 of which use a pretrained embedding layer. As we
saw, the pretrained embedding improves TPR signiﬁcantly on
all architectures. We remind the reader that the embedding
layer was trained using both the training set and the unlabeled
dataset. In order to quantify the contribution of the unlabeled
dataset by itself to the TPR of our detection models, we
generated an embedding layer using the training set only and
10
then measured the TPR of the resulting models (while keeping
the FPR below 0.001).
The results are presented by Table IV. The ’Inline’ column
presents the TPR for the models without contextual embedding
and the ’All data’ column presents it for the models with
an embedding trained using both the training set and the
unlabeled dataset.9. The ’training set only’ column presents
the TPR results of the new models, trained using the training
set only – without the unlabeled instances. As can be seen
by comparing the 2’nd and 3’rd columns of Table IV, all
the models except for Token-Char-FastText hardly beneﬁt at
all from the contextual embedding when it is trained using
the training set only. Thus, the contribution of the contextual
embedding for these 5 models should be fully attributed to
the usage of the unlabeled dataset (whose contribution can
be quantiﬁed by comparing the 3’rd and 4’th columns). The
explanation for this is, most probably, the fact that DL model
weights are optimized anyway w.r.t. the training set tokens by
the supervised training process.
The results for the Token-Char-FastText detector are sig-
niﬁcantly different. Training the contextual embedding solely
based on the training set improves TPR by approximately
4.8 pp over no pretrained embedding at all, while using also
scripts/modules from the unlabeled corpus increases TPR by
additional 7.1 pp.
2) Detection Examples: We now provide an example of
how the W2V embedding facilitates the detection of malicious
code. Consider the following short malicious code:
Invoke-WebRequest -Uri http:///ry.exe
-OutFile
([System.IO.Path]::GetTempPath()+’c.exe’);
powershell.exe Start-Process -Filepath
([System.IO.Path]::GetTempPath()+’c.exe’);
In the above code, Invoke-WebRequest is used
to a temporary folder
to fetch the payload, write it
and then execute it. Recall
the PowerShell command
that
Invoke-WebRequest has an alias – IWR. When replacing
in the above code the cmdlet Invoke-WebRequest by IWR,
the CNN-RNN model using the inline embedding scores the
altered script 5 pp lower, that is, it scores it as signiﬁcantly less
likely to be malicious. This decrease does not occur when the
CNN-RNN-W2V model is used. We now explain the reason
for this difference.
9These values also appear in Table III and are repeated here for facilitating
comparison.
TABLE IV.
TPR RESULTS WITHOUT CONTEXTUAL EMBEDDING
(’INLINE’), WITH CONTEXTUAL EMBEDDING USING TRAINING SET ONLY,
AND WITH CONTEXTUAL EMBEDDING USING BOTH THE UNLABELED
DATASET AND THE TRAINING SET.
Model
Token-Char-FastText
Token-Char-W2v
CNN-FastText
CNN-W2V
CNN-RNN-FastText
CNN-RNN-W2V
Inline
0.775
0.775
0.711
0.711
0.736
0.736
training set only
0.823
0.763
0.72
0.713
0.736
0.729
All data
0.894
0.810
0.769
0.779
0.818
0.805
11
Counting token appearances in the training set, we found
that the Invoke-WebRequest command appears in 1540
benign instances and in 6 malicious instances, while IWR
appears in 27 training set instances, all of which are benign.
This explains the decrease in score of the inline embedding
model. In the model that uses the W2V embedding, on the
other hand, the Invoke-WebRequest command and its
alias IWR were found to be semantically equivalent, since each
of the two vectors to which they were mapped by W2V is the
closest neighbor of the other. Consequently, when using the
CNN-RNN-W2V model, no decrease in the score is observed
when replacing the command by its alias.
Next, we provide an example of how FastText facilitates
detection by comparing the performance of the CNN-RNN
model (which does not use a contextual embedding) with that
of CNN-RNN-FastText. We conduct this comparison using
the CNN-RNN architecture rather than the Token-Char archi-
tecture, since the former only utilizes per-token information,
making it easier to pinpoint the contribution of the contextual
embedding.
The CNN-RNN-FastText model detected 143 instances that
were not detected by the CNN-RNN model. Out of these,
137 are TPs and 6 are FPs.10 Manually analyzing these code
instances, we were not able to identify any speciﬁc tokens
which could have contributed to the detection. Nevertheless,
our analysis of the newly-detected instances indicates that in
at least 41 of them, detection can be, at least partly, attributed
to the fact that FastText uses sub-tokens. We now provide an
example showcasing the possible contribution of sub-tokens.
Our analysis identiﬁed the following 3 tokens (hence-
forth referred to as the example tokens), one or more of
which appearing in 41 of
the newly detected instances:
’responsetext’, ’responsebody’ and ’xmlhttp’.
These 3 tokens seem rather benign based on the training set:
they were respectively seen in 44, 84 and 49 training set
instances, out of which only (respectively) 1, 2 and 2 were
malicious. We then analyzed the properties of their sub-tokens.
In addition to a signiﬁcant increase in the number of training
set instances that contain one or more of these sub-tokens
(which is to be expected), we found that some of them seem
suspicious based on the training set, as the ratio of malicious
training set instances in which they appear is relatively high,
facilitating the detection of the instances that contain them by
the model. Examples of such sub-tokens are:
•
•
’http’ appeared in 18,616 training set instances,
2,024 of which are malicious (10.8%).
’spo’ appeared in 7,296 instances, 656 of which are
malicious (8.9%).
Since FastText utilizes sub-tokens for its embedding pro-
cess, the vector representations assigned to tokens with similar
sub-tokens are relatively close to each other. Consequently, as
the above sub-tokens appear in a malicious context (mostly
as part of tokens other than the example tokens), the fact
that the tokens containing them are embedded to vectors that
are relatively close to those of the example tokens can assist
10On the other hand, only 34 instances detected by CNN-RNN with inline
embedding were not detected using FastText embedding. Out of these, 28 are
TPs and 6 are FPs.
the model in correctly classifying instances containing these
example tokens.