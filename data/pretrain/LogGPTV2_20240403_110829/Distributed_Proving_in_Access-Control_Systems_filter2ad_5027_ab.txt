a fact. In practice, facts would only be added to the set of
tactics after verifying the corresponding digital certiﬁcate.
φ ::= s signed φ(cid:1) | p says φ(cid:1)
φ(cid:1)
::= action (s, s) | p speaksfor p |
delegate(p, p, s)
4. Distributed Proof Generation
4.1. Proving Strategies
where s ranges over strings and p principals.
Note that the says and signed predicates are the
only formulas that can occur at top level.
The inference rules for manipulating formulas are also
straightforward (see Appendix A). For the purposes of
illustration, we present the SPEAKSFOR-E rule, which al-
lows principals to exercise delegated authority.
A says (B speaksfor A) B says F
A says F
(SPEAKSFOR-E)
3.2. Tactical Theorem Provers
to a
resource
To gain access
controlled by
Bob, Alice must produce a proof of
the formula
Bob says action(resource). To generate such proofs
automatically, we use a theorem prover.
In traditional approaches to distributed authorization,
credentials are distributed across multiple users. A sin-
gle user (either the requester of a resource or its owner,
depending on the model) is responsible for proving that
access should be allowed, and in the course of proving
the user may fetch credentials from other users. All users
except for the one proving access are passive; their only
responsibility is to make their credentials available for
download.
We propose a different model: each user is both a
repository of credentials and an active participant in the
proof-generation process.
In this model, a user who is
generating a proof is now able to ask other users not only
for their certiﬁcates, but also to prove for him subgoals
that are part of his proof. Each user has a tactical theorem
prover that he uses to prove both his own and other users’
goals. In such a system there are multiple strategies for
creating proofs.
One common strategy used by automated theo-
is to re-
rem provers, and the one we adopt here,
cursively decompose a goal (in this case,
the for-
mula Bob says action(resource)) into subgoals un-
til each of the subgoals can be proved. Goals can
be decomposed by applying inference rules. For ex-
the SPEAKSFOR-E rule allows us to prove
ample,
Bob says action(resource) if we can derive proofs
Eager The traditional approach, described above,
we recast
in our environment as the eager strategy
for generating proofs: a user eagerly keeps working
on a proof until the only parts that are missing are
credentials that she can download. More speciﬁcally
to our logic, to prove that she is allowed access to a
resource controlled by Bob, Alice must generate a proof
Proceedings of the 2005 IEEE Symposium on Security and Privacy (S&P’05) 
1081-6011/05 $ 20.00 IEEE
of the formula Bob says action(resource). The eager
approach is for Alice to keep applying tactics until the
only subgoals left are of the form A signed F and
then query the user A for the certiﬁcate A signed F .
In Alice’s case, her prover might suggest that a simple
way of generating the desired proof is by demonstrating
Bob signed action(resource),
in which case Alice
will ask Bob for the matching certiﬁcate.
For non-
trivial policies, Alice’s prover might not know of a
particular certiﬁcate that would satisfy the proof, but
would instead try to ﬁnd any certiﬁcate that matches
a particular form. For example, if Bob is unwilling to
provide Alice with the certiﬁcate she initially requested,
Alice might ask him for any certiﬁcates that match
Bob signed (A speaksfor Bob),
indicating that Bob
delegated his authority to someone else. If Bob provided
certiﬁcate Bob signed (Charlie speaksfor Bob),
a
Alice’s prover would attempt
to determine how a
certiﬁcate from Charlie would let her ﬁnish the proof.
Bob is able to select certiﬁcates in a manner that conveys
to Alice exactly the amount of authority that he wishes.
This is particularly beneﬁcial in an interactive system, in
which Bob the person (as opposed to Bob the network
node) can be asked to generate certiﬁcates on the ﬂy.
In the lazy strategy, then, as soon as Alice’s theorem
prover produces a subgoal of the form A says F , Alice
asks the node A (in the above example, Bob) to prove the
goal for her. In other words, Alice is lazy, and asks for as-
sistance as soon as she ﬁnds a subgoal that might be more
easily solved by someone else. In Section 5 we demon-
strate empirically the advantages of the lazy strategy.
Our prover assumes a cooperative environment in
which a malicious node may easily prevent a proof from
being found or cause a false proof to be generated. Our
system adopts the approach of prior work (e.g., [3, 15]),
in which the reference monitor veriﬁes the proof before al-
lowing access, which means that these attacks will merely
result in access being denied.
Lazy An inherent characteristic of the eager strategy
is that Alice’s prover must guess which certiﬁcates other
users might be willing to contribute. The guesses can
be conﬁrmed only by attempting to download each cer-
tiﬁcate. In any non-trivial security logic (that is, almost
any logic that allows delegation), there might be many
different combinations of certiﬁcates that Bob and others
could contribute to Alice that would allow her to complete
the proof. Asking for each of the certiﬁcates individu-
ally is very inefﬁcient. Asking for them in aggregate is
impractical—for example, not only might a principal such
as a certiﬁcation authority have an overwhelming number
of certiﬁcates, but it’s unlikely that a principal would al-
ways be willing to release all of his certiﬁcates to anyone
who asks for them.
With this in mind, we propose the lazy strategy for gen-
erating proofs. Recall that credentials (A signed F ) im-
ply beliefs (A says F ). The typical reason for Alice to ask
Bob for a credential Bob signed F is so that she could
use that credential to demonstrate that Bob has a belief
that can lead to Alice being authorized to perform a par-
ticular action. Alice is merely guessing, however, that this
particular credential exists, and that it will contribute to a
successful proof.
The lazy strategy is, instead of asking for Bob signed
F , to ask Bob to prove Bob says F . From Alice’s stand-
point this is a very efﬁcient approach: unlike in the eager
strategy, she won’t have to keep guessing how (or even
whether) Bob is willing to prove Bob says F ; instead she
will get the subproof (or a negative answer) with exactly
one request. From Bob’s standpoint the lazy approach
also has clear advantages: Bob knows what certiﬁcates he
has signed, so there is no need to guess; he simply assem-
bles the relevant certiﬁcates into a proof. Additionally,
4.2. A General Tactical Theorem Prover
We introduce a proving algorithm that, with minor
modiﬁcations, can produce proofs in either a centralized
(all certiﬁcates available locally) or distributed manner
(each node knows all of the certiﬁcates it has signed). The
distributed approach can implement either the eager or the
lazy strategy. We will use this algorithm to show that both
distributed proving strategies will successfully produce a
proof in all cases in which a centralized prover can pro-
duce a proof.
Our proving algorithm, which is derived from a stan-
dard backchaining algorithm (e.g., [22, p.288]), is shown
in Figure 1. The proving algorithm, bc-ask, takes as input
a list of goals, and returns either failure, if all the goals
could not be satisﬁed, or a substitution for any free vari-
ables in the goals that allows all goals to be satisﬁed si-
multaneously. The algorithm ﬁnds a solution for the ﬁrst
goal and recursively determines if that solution can be
used to produce a global solution. bc-ask proves a goal
in one of two fashions: locally, by applying tactics from
its knowledge base (Figure 1, lines 15–20); or remotely,
by iteratively asking for help (lines 10–14).
The helper function subst takes as parameters a sub-
stitution and a formula, returning the formula after re-
placing its free variables as described by the substitution.
compose takes as input two substitutions, θ1 and θ2, and
returns a substitution θ(cid:1) such that subst(θ(cid:1),F ) = subst(θ2,
subst(θ1, F )). rpcl takes as input a function name and
parameters and returns the result of invoking that func-
tion on the machine with address l. We assume that the
network does not modify or delete data, and that all mes-
sages arrive in a ﬁnite amount of time. unify takes as input
two formulas, F1 and F2, and determines if a substitution
Proceedings of the 2005 IEEE Symposium on Security and Privacy (S&P’05) 
1081-6011/05 $ 20.00 IEEE
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
global set KB
substitution bc-ask(
list goals,
substitution θ,
set failures)
(cid:2)
local substitution answer
local set failures
local formula q(cid:2)
if (goals = [ ] ∧ θ ∈ failures) then return ⊥
if (goals = [ ]) then return θ
q(cid:2) ← subst(θ, ﬁrst(goals))
l ← determine-location(q(cid:2))
(cid:2) ← failures
failures
if (l (cid:5)= localmachine)
while ((α ← rpcl(bc-ask(ﬁrst(goals), θ, failures
failures
answer ← bc-ask(rest(goals), α, failures)
if (answer (cid:5)= ⊥) then return answer
(cid:2) ← α ∪ failures
(cid:2)
(cid:2)))) (cid:5)= ⊥)
else foreach (P, q) ∈ KB
if ((θ(cid:2) ← unify(q, q(cid:2))) (cid:5)= ⊥)
while ((β ← bc-ask(P, compose(θ(cid:2), θ), failures
failures
answer ← bc-ask(rest(goals), β, failures)
if (answer (cid:5)= ⊥) then return answer
(cid:2) ← β ∪ failures
(cid:2)
(cid:2))) (cid:5)= ⊥)
return ⊥
/* knowledge base */
/* returns a substitution */
/* list of conjuncts forming a query */
/* current substitution, initially empty */
/* set of substitutions that are known
not to produce a complete solution */
/* a substitution that solves all goals */
/* local copy of failures */
/* result of applying θ to ﬁrst goal */
/* θ known not to produce global solution */
/* base case, solution has been found */
/* prove ﬁrst goal locally or remotely? */
/* make remote request */
/* prevent α from being returned again */
/* prove remainder of goals */
/* if answer found, return it */
/* investigate each tactic */
/* determine if tactic matches ﬁrst goal */
/* prove subgoals */
/* prevent β from being returned again */
/* prove remainder of goals */
/* if answer found, return it */
/* if no proof found, return failure */
Figure 1. bc-ask, our proving algorithm
θ exists such that subst(θ, F1) = subst(θ, F2), i.e., it deter-
mines if F1 and F2 can be made equivalent through free-
variable substitution. If such a substitution exists, unify
returns it. A knowledge base, KB, consists of a list of
tactics as described in Section 3.2. determine-location
decides whether a formula F should be proved locally or
remotely and, if remotely, by whom. Figure 2 shows an
implementation of determine-location for the lazy strat-
egy; an implementation for the eager strategy can be ob-
tained by removing line 1 and removing the if-then clause
from line 2. When bc-ask is operating as a centralized
prover, determine-location always returns localmachine.
tution, bc-ask will attempt to ﬁnd another solution for F
and then repeat the process.
The algorithm terminates when invoked with an empty
goal list. If the current solution has been marked as a fail-
ure, bc-ask returns failure (⊥) (line 5). Otherwise, bc-ask
will return the current solution (line 6).
Note that this algorithm does not explicitly generate a
proof. However, it is straightforward to design the goal
and tactics so that upon successful completion a free vari-
able in the goal has been uniﬁed with the proof [5].
We proceed to show that all of the strategies proposed
thus far are equivalent in their ability to generate a proof.
When proving a formula F locally, bc-ask will iter-
ate through each tactic in the knowledge base. If a tactic
matches the formula being proved (line 16), bc-ask will
attempt to prove all the subgoals of that tactic (line 17). If
the attempt is successful, bc-ask will use the resulting sub-
stitution to recursively prove the rest of the goals (line 19).
If the rest of the goals cannot be proved with the substi-
Theorem 1 For any goal G, a distributed prover using
tactic set T will ﬁnd a proof of G if and only if a central-
ized prover using T will ﬁnd a proof of G.
For the full proof, please see Appendix B.
Infor-
mally: By close examination of the algorithm, we show
by induction that bc-ask explores the same proof search
Proceedings of the 2005 IEEE Symposium on Security and Privacy (S&P’05) 
1081-6011/05 $ 20.00 IEEE
0 address determine-location(q)
θ ← unify(q, “A says F ”)
1
if (θ = ⊥) then θ ← unify(q, “A signed F ”)
2
if (θ = ⊥ ∨ is-local(subst(θ, “A”))) then return localmachine
3
else return name-to-addr(subst(θ, “A”))
4
/* returns machine that should prove q */
/* unify with constant formula “A says F ” ... */
/* ... or with “A signed F ” */
/* instantiate A to a principal, then return
* the corresponding address */
Figure 2. Algorithm for determining the target of a request
space whether operating as a centralized prover or as a
distributed prover. In particular, the centralized and dis-
tributed prover behave identically except when the dis-
tributed prover asks other nodes for help.
In this case,
we show that the distributed prover iteratively asks other
nodes for help (lines 10–14) in exactly the manner that a
centralized prover would consult its own tactics (lines 15–
20).
Corollary 1 For any goal G, a lazy prover using tactic
set T will ﬁnd a proof of G if an eager prover using tactic
set T will ﬁnd a proof of G.
Proof Sketch Lazy and eager are both strategies for dis-
tributed proving. By Theorem 1, if a lazy prover ﬁnds a
proof of goal G, then the centralized prover will also ﬁnd
a proof of G, and if a centralized prover can ﬁnd a proof
of G then an eager prover will also. 2
4.3. Distributed Proving with Multiple Tactic Sets
So far we have only considered systems in which the
tactic sets used by all principals are identical. This is only
realistic when all resources are in a single administrative
domain. It is possible, and indeed likely, that different do-
mains may use a different sets of tactics to improve per-
formance under different policies.
It is also likely that
different domains will use different security logics, which
would also necessitate different sets of tactics.
In this more heterogenous scenario, it is more difﬁcult
to show that a distributed prover will terminate. Since
each prover is allowed to use an arbitrary set of tactics,
asking a prover for help could easily lead to unproductive
cycles of expanding and reducing a goal without ever gen-
erating a proof. Consider the following example: Alice
has a tactic that will prove Alice says (Bob says F ) if
Alice has a proof of Bob says F . However, Bob has the
opposite tactic: Bob will say F if Bob has a proof of
Alice says (Bob says F ). If Bob attempts to prove Bob
says F by asking Alice for help, a cycle will develop in
which Bob asks Alice to prove Alice says (Bob says F ),