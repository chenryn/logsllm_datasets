the cluster over a predeﬁned measurement interval. While
other criteria could be used to ﬁlter clusters, data volume
is a categorization of inherent interest. A cluster containing
20 percent of all traﬃc is one that a network manager is
likely to care about, while a cluster that only contains a few
packets usually warrants less attention.
However, even with this restriction, the number of such
clusters identiﬁed in real traces is far too large to manage.
Since the precious resource is not network bandwidth or
CPU cycles, but a network manager’s time, verbose and un-
structured reports are not likely to be appreciated or useful.
Consequently, to maximize the eﬀectiveness of a traﬃc re-
port, we believe that there are four essential operations that
must be provided:
• Operation 1, Compute: Given a description of traf-
ﬁc as input (e.g., packet traces or NetFlow records),
compute the identity of all clusters with a traﬃc vol-
ume above a certain threshold. This is the base oper-
ation.
• Operation 2, Compress: Having found the base set
of clusters, one can compress the report considerably
by removing a cluster C from the report if cluster C’s
traﬃc can be inferred (within some error tolerance)
from that of cluster C(cid:1)
that is already in the report.
For example, if all the traﬃc is generated by a single
high-volume connection from source S to destination
D is high volume, then one can infer that the traﬃc
sent by S is also high volume. Thus one should retain
the S to D cluster for the detail it shows, and omit the
S cluster as it can be inferred from the S to D cluster.
Intuitively, the rule we use is to remove a more general
cluster if its traﬃc volume can be inferred (within some
error tolerance) from more speciﬁc clusters included in
the report.
• Operation 3, Compare: A good way to save the
manager time is to concisely show how the traﬃc mix
changes from day to day, or week to week. Computing
these deltas requires ﬁnding those high volume clusters
that have changed signiﬁcantly since the last report.
This is trickier than it seems, because a high volume
cluster on day 1 may now become low on day 2, or
vice versa. Worse, more general clusters need not be
larger than the sum of more speciﬁc non-overlapping
clusters. Thus combining deltas (Operation 3) with
compression (Operation 2) is much harder than just
implementing each operation in isolation.
• Operation 4, Prioritize: Even after compressing
the report and computing Deltas, it is still desirable to
prioritize the elements of the report in terms of their
potential level of interest to a manager. We choose
to equate the interest in a cluster to what we call its
unexpectedness. While there are many ways to deﬁne
this metric, we chose to use a relatively unsophisti-
cated approach that is easy to compute. We deﬁne
unexpectedness in terms of deviation from a uniform
model in which the contents of diﬀerent ﬁelds is mutu-
ally independent. For example, if preﬁx A sends 25%
of the traﬃc and preﬁx B receives 40% of all traﬃc,
then under the assumption of independence, we would
expect the traﬃc from A to B to be 25%*40%=10%
of the total traﬃc. If the actual traﬃc from A to B
is 15% of the total traﬃc instead of 10%, the clus-
ter is tagged with a score of 150%, indicating that it
is unexpectedly large by a factor of 1.5.
If the traf-
ﬁc from A to B is only 6%, then it is given a score
of 60%, indicating that it is unexpectedly small. The
closer a score is to 100%, the more boring it is, and the
less important it is to highlight to the user. This con-
struction of unexpectedness is, in eﬀect, a very simple
multi-dimensional gravity model.
3. ALGORITHMS
The last section motivated four fairly abstract operations
on sets of clusters. Here we describe the speciﬁc algorithms
we chose to implement these operations. These algorithms
form the engine that underlies the core of our AutoFocus
tool described in Section 4. Rather than directly present the
algorithms for the multi-dimensional case, we ﬁrst present
the simpler algorithms for the unidimensional (i.e., single
ﬁeld) case. Addressing this simpler case will help build in-
tuition. Furthermore AutoFocus also includes in its output
the simpler unidimensional results and the multidimensional
algorithms use the results of the unidimensional algorithms
to reduce their search space.
For some of the algorithms we also present theoretical up-
per bounds on the size of the report and on the algorithm’s
running time. Measurement results in the technical report
version of this paper [5] show that in reality reports are much
smaller than these upper bounds. Since our focus in this pa-
per is maximizing information transfer to the manager, not
algorithmic optimization; we believe that signiﬁcantly faster
algorithms that produce similar results may be possible.
In this section we use the terms dimension and ﬁeld inter-
changeably since each ﬁeld deﬁnes a dimension along which
we can classify. We use k for the number of ﬁelds. In the
actual system we implemented k = 5.
The sets (i.e., preﬁxes in IP address ﬁelds) for each ﬁeld
form a natural hierarchy in terms of set inclusion. This
can be described by a tree where the parent is always the
smallest superset of the child. The leaves of this tree are
individual values the ﬁeld can take. The root is always the
set of all possible values, *. The sets denoted by two nodes
are disjoint unless one of the nodes is an ancestor of the
other, in which case it is a superset of the other. We call
the number of levels in this tree the depth of the tree (the
maximum distance from the root to a leaf plus 1). We use
di for the depth of the hierarchy of the i-th of the k ﬁelds.
The hierarchy for IP addresses we use in this paper has a
depth of 26 that for port numbers has a depth of 3 while
for the protocol ﬁeld we use the simplest possible hierarchy
with a depth of d = 2.
The raw data we build our algorithms on is a simpliﬁed
version of NetFlow ﬂow records: each ﬂow record, which
we sometimes refer to as a “ﬂow” for conciseness, has a key
that speciﬁes exact values for all ﬁve ﬁelds and two counters,
one counting the packets that matched the key during the
measurement interval considered and one for the number of
bytes in those packets. Transforming a trace with packet
headers and timestamps into such ﬂow records leads to no
loss of information from the standpoint of traﬃc clusters.
We use n for the number of such records. Each traﬃc
cluster is made up of one or more ﬂow records and the cor-
responding byte and packet counters are the sum of the cor-
responding counters of the ﬂow records it includes. Note
that if a cluster contains exact values in all ﬁelds it is ex-
actly equivalent to a single ﬂow. For the rest of this section
we ignore that the ﬂow records contain two counters and
work with a single counter. Our algorithms use a thresh-
old H and focus on the traﬃc clusters that are above this
threshold. We use s for the ratio between the total traﬃc
T and the threshold s = T /H, so if H is 5% of the total
traﬃc, then s = 20.
3.1 Unidimensional clustering
First we concentrate on the problem of computing high
volume clusters on a single ﬁeld, such as the source IP ad-
dress. Note that even the unidimensional case is signiﬁcantly
more complex than traditional tools like FlowScan, in which
managers deﬁne a static hierarchy by pre-specifying which
subnets should be watched. By clustering automatically, we
10.8.0.0/28
500
10.8.0.0/29
120
10.8.0.8/29
380
0.8.0.0/30
50
10.8.0.4/30
70
10.8.0.8/30
305
75
10.8.0.12/30
10.8.0.2/31
50
10.8.0.6/31 70
10.8.0.8/31
270
10.8.0.10/31
35
75
10.8.0.14/31
15
35
30
40
160
110
35
75
10.8.0.2
10.8.0.3
10.8.0.6
10.8.0.7
10.8.0.8
10.8.0.9
10.8.0.10 10.8.0.14
Figure 1: Each individual IP address sending traﬃc
appears as a leaf. The traﬃc of an internal node is
the sum of the traﬃc of its children. Nodes whose
traﬃc is above H=100 (double circles) are the high
volume traﬃc clusters. The Web server 10.8.0.12 is
a large cluster in itself. While no individual DHCP
address is large enough, their aggregate 10.8.0.0/29
is, so it is listed as a large cluster.
do not need to deﬁne subnets; the tool will automatically
group addresses into “subnets” that contain a high volume
of traﬃc.
We use d to represent the depth of the hierarchy and m ≤
n to represent the number of distinct values of the ﬁeld in
the n ﬂow records of the input.
Computing Unidimensional Clusters
Before we describe our algorithms for computing the high
volume unidimensional clusters, it is useful to bound their
number. Consider the IP source address. A reasonable in-
tuition might be that a threshold H of 5% of the total traf-
ﬁc restricts the report size to 20, because there can be at
most 20 disjoint clusters, each contributing 5% of the traf-
ﬁc. Unfortunately, our deﬁnition of clusters allows clusters
to overlap. Thus, if 128.50.∗.∗ is a high volume cluster, then
128. ∗ . ∗ .∗ is as well. Fortunately, a given source address
cluster’s traﬃc can at most be counted in 25 other clus-
ters (the number of ancestors in its hierarchy tree – we do
not consider preﬁxes with lengths from 1 to 7). Therefore,
the maximum number of high volume clusters is not 20 but
roughly 20 · 26 = 520. In [5] we show that the size of all
such reports is bounded by 1 + (d − 1)s.
For the unidimensional case, we now describe the algo-
rithm to do Operation 1, computing the raw set of high
volume clusters. When the number of sets in the hierarchy
is relatively small, for example 257 for protocol and 65539
for port numbers, we can apply a brute force approach: keep
a counter for each set and traverse all n ﬂow records while
updating all relevant counters; at the end, list the clusters
whose counters have exceeded H.
If the number of possible values is much larger, as is the
case for IP addresses, we use another algorithm illustrated
by the example from Figure 1. As we go through the ﬂow
records, we ﬁrst build the leaf nodes that correspond to the
IP addresses that actually appear in the trace. For exam-
ple, there are only 8 possible source addresses (leaves) in the
trace that Figure 1 was built from. Thus, we make a pass
over the trace updating the counters of all the leaf nodes.
By the end of this pass, the leaf counters are correct; we also
initialize the counters of all nodes between these leaves and
the root to 0. In a second pass over this tree, we can deter-
mine which clusters are above threshold H by traversing in
post order (children before parents). Also, just before ﬁn-
ishing with each node, the algorithm must add its traﬃc to
the traﬃc of its parent. This way, when the algorithm gets
to each node its counter will reﬂect its actual traﬃc. The
memory requirement for this algorithm is O(1 + m(d − 1))
and it can be reduced to O(m+d) by generating the internal
nodes only as we traverse the tree. The running time of the
algorithm is O(n + 1 + m(d− 1)). No algorithm can execute
faster than O(n + 1 + (d − 1)s) because all algorithms need
to at least read in the input and print out the result.
Compressing Unidimensional Trafﬁc Reports
For the unidimensional case, we now describe the algorithm
for Operation 2, compressing the raw set of high volume
clusters. The complete list of all clusters above the thresh-
old is too large and most often it contains redundant infor-
mation. Even if a /8 (address preﬁx of length 8) contains
exactly the same amount of traﬃc as a more speciﬁc /24
preﬁx, all the preﬁxes with lengths in between are also high
volume clusters. More generally, perhaps an intermediate
preﬁx length like /16 has a little more traﬃc than the /24
it includes (or the sum of the traﬃc of several more speciﬁc
/24s already in the report) but not much more. Report-
ing the /16 adds little marginal value but takes up precious
space in the report. Removing the /16 on the other hand,
will mean that the manager’s estimate of the /16 may be
a little oﬀ. Thus we trade accuracy for reduced size.
In
general, deﬁne the compression threshold C as the amount
by which a cluster can be oﬀ.
In our experiments we de-
ﬁned C = H. We did so to avoid unintended errors: if the
manager wants all clusters above H, surely she realizes that
the report can be oﬀ by H in terms of missing clusters of
size smaller than H. By setting C = H, we are only adding
another way to be oﬀ by H. Also, setting C = H produces
the following simple but appealing result.
Lemma 1. The number of clusters above the threshold in
a non-redundant compressed report is at most s.
Proof Since none of the clusters in the report is redundant,
each has a traﬃc of at least C = H that was not reported
by any of its descendants. The sum of these diﬀerences is
at most T because each ﬂow can be associated with at most
one most speciﬁc cluster in the report and these ﬂows make
up the diﬀerence between that cluster and the more speciﬁc
ones. Therefore, a report can contain at most T /H = s