queues of all subscribed NFs. As shown in Figure 5, each NF
has a single incoming queue, but this can be linked to multi-
ple output ports from one or more other NFs/µStacks. Event
Messages are designed to be compact, so they only include
metadata such as the event type and snapshot. For packet
data or for state that may need to be modified by NFs, the
structure contains data pointers that can be used to reference
larger amounts of data, e.g., the packet that triggered the
PKT event or the TCP Monitor’s flow state. The Microboxes
"run loop" on each NF polls for incoming events and triggers
a handler function that the NF specifies for each event type.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
G. Liu et al.
{
on the DPI detection event’s type, the NF updates the sub-
scription to the appropriate callback function to perform
signature matching on subsequent packets that arrive in the
flow (lines 11-14). If an attack is detected, a new IDS_ALERT
message is published (line 20).
7 EVALUATION
Testbed Setup: Our experiments were performed on the
NSF CloudLab testbed [24] with "c220g2" type servers from
the Wisconsin site. Each server has Dual Intel Xeon E5-2660
v3 @ 2.60GHz CPUs (2*10 cores), a Dual-port Intel X520
10Gb NIC and 160GB memory. All servers run Ubuntu 14.04
with kernel 3.13.0-117-generic and use Intel’s DPDK v16.11.
We use two sets of traffic generators, mTCP-based [13] web
client and server for high speed tests, and Nginx 1.4.6 and
Apache Bench 2.3 for setting up multiple servers and splitting
up web traffic.
7.1 Protocol Stack Performance
Stack Consolidation: To evaluate the benefit of consoli-
dating the stack for chained NFs, we first port the mOS
middlebox library to our NFV platform. mOS by itself does
not support chaining, and assumes a single NF is used with
complete control over the NIC. A straightforward port of
chained mOS NFs (labeled "mos" in figures) uses a packet
interface between instances and performs stack processing
for each NF. In contrast, our Microboxes system (labeled
"mb" in figures) only needs to perform stack processing once,
and then forwards the stack events through the chain of
NFs. For both cases, the NFs themselves perform negligible
processing and we use a workload of 4000 concurrent web
clients accessing files 64B or 8KB long.
The results in Figure 6a and Figure 6b, show that as the
chain length increases, the throughput of Microboxes re-
mains mostly flat, while mOS drops quickly. For a chain
of 6 NFs, Microboxes outperforms mOS by up to 105% for
requests for a 64B file and 144% for 8KB file (including batch-
ing). Batching events at each NF contributes around 30% of
this improvement (compare mb-batch vs. mb); we use batch-
ing for the remaining experiments. Microboxes does see a
throughput decrease when the chain is longer than 6 since it
has to use cores on another socket and pay NUMA penalties.
As shown in Figure 6c, Microboxes also reduces latency
by 20% due to stack consolidation with longer NF chains.
mOS gets marginally better latency only when there is one
NF since it runs the stack and NF on the same core, while
Microboxes runs the stack on a separate core and cross-core
communication impacts the performance for this case.
Parallel Processing: To evaluate the performance of par-
allel processing, we compose multiple DPI NFs together in
a sequential or parallel manner. The first instance is set up
/ / map shared memory
to c o n t r o l l e r
cb_noop ) ;
c b _ d e t e c t ) ;
f o r p u b l i c a t i o n type
/ /
e n t e r
run loop
i f (m. dpi_app_type == HTTP)
mb_nf_mem_init ( c n t r l r ) ;
/ / announce s u b s c r i b e d types
mb_sub ( c n t r l r , PKT_TCP ,
mb_sub ( c n t r l r , DPI_DETECT ,
/ /
r e q u e s t an output port
port = mb_pub ( c n t r l r , EVENT_IDS_ALERT ) ;
mb_nf_run ( ) ;
1 void s t a r t I D S ( )
2
3
4
5
6
7
8
9 }
10 void c b _ d e t e c t ( DPI_DETECT_msg m)
11
12
13
14
15 }
16 void c b _ s q l ( PKT_TCP_msg m)
17
18
19
20
21
22 }
i f ( s q l _ i n j e c t i o n _ a t t a c k (m. payload ) )
else i f (m. dpi_app_type == SQL )
IDS_ALERT_msg m2 = mb_new_msg ( IDS_ALERT )
/ /
mb_publish_event ( port , m2)
in m2
f i l l
mb_up_sub ( c n t r l r , PKT_TCP , e . flow ,
mb_up_sub ( c n t r l r , PKT_TCP , e . flow ,
{
{
{
cb_http ) ;
c b _ s q l ) ;
}
Listing 1: IDS Signature Matching NF
Since all events arrive in a single queue, NFs process them
in arrival order regardless of event type. Our µStack defines
its TCP events using the mOS base events as building blocks,
so it inherits the same ordering enforced by mOS, and our
message communication mechanism guarantees the events
are processed in order for NFs within a chain. However, we
don’t provide ordering for NF internal events and we assume
this is handled by the developers.
Sample Applications: We have ported a set of middlebox
applications into Microboxes using our event and stack APIs.
Where possible, we have decomposed the applications into
separate NF modules and offloaded processing to the stack
rather than inside the NF. Table 2 lists out these applications
and the type of µStack they make use of.
These applications provide a range of examples with dif-
ferent protocol processing needs. Most applications have
been decomposed into multiple NF modules to further stress
our event system. In addition to writing our own NFs, we
also include the Lighthttp web server to illustrate support for
legacy applications. The web server uses our TCP endpoint
µStack. The Controller can also link together several of these
applications, for example to provide a Layer 7 HTTP load
balancer in front of the Lighttpd server. These deployments
involve multiple µStacks that Microboxes will automatically
keep consistent.
In Code Listing 1, we present simplified code for our IDS
Signature Matching NF (the full version performs more com-
plex analysis based on the pattern matching code of Snort).
The startIDS() function initializes subscriptions, setting
a default "no op" handler for incoming TCP packets and the
cb_detect() callback for events arriving from the DPI
NF that detects application-layer protocols (lines 4-5). Based
Microboxes: High Performance NFV with Customizable,
Asynchronous TCP Stacks and Dynamic Subscriptions
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
(a) 8KB Reqs
(b) 64B Reqs
(c) Latency
Figure 6: Microboxes improves throughput and latency by eliminating redundant stack processing in chains.
(a) Parallel NFs
(b) Multi-Stack
(c) DPI-Chain
Figure 7: (a) Parallel processing performance (b) Multistack performance (c) Event subscriptions impact
as the head of the chain and controls the parallelism for the
following instances. The throughput for parallel and sequen-
tial chains are similar (not shown). We can see the latency
difference from Figure 7a. As the number of NFs increases,
the latency for sequential chains increases linearly, while the
parallel chain is almost flat. For 6 NFs, parallel processing
can reduce latency 32% for a 64B web request and by 20% for
8KB web requests.
Scalability: We next evaluate the scalability of our µStack
when using multiple cores and two dual port NICs. We use
two pairs of clients and servers to generate the web traf-
fic (8KB HTTP file requests) through Microboxes. At each
stage of the experiment, we add one more core to host the
stack and use the RSS value to split flows across the stacks.
From Figure 7b, we can see a linear speedup for total pro-
cessing rate until the Ethernet link (2*10Gbps) becomes the
bottleneck.
These experiments show that our architecture can effi-
ciently run chains of NFs with improved throughput and
latency characteristics due to elimination of redundant work
and better parallelism. We next show how customizing the
stack for different NFs can provide further performance im-
provements while maintaining deployment flexibility.
7.2 Load Balancer: Flexibility and Speed
In this experiment, we show how our layered TCP stack
can provide different performance and functional require-
ments to meet an NF’s needs. We use Nginx as a web server
and Apache Bench as the client to generate web traffic. We
consider four different Microboxes NFs to load balance the
traffic: L4_LB, L7_LB and Bytestream Proxy, based on our
Monitor, Splicer and Split Proxy Stacks respectively. We com-
pare against a baseline of the simple DPDK L2 forwarder
example NF and HAProxy [2], a popular open source load
balancer which uses Linux kernel-based networking. We also
evaluate a "L7 LB + Cache" approach which combines our
HTTP load balancer with the Lighttpd server running on
the TCP Endpoint µStack. We consider the case where 50%
or 100% of the requests are redirected by the Splicer to the
local Lighttpd server that acts as a faster cache compared to
Nginx.
From Table 3, we can see L4_LB has the lowest latency
and highest throughput of the Microboxes solutions. This is
as expected since it only looks at header information, and
does minimal TCP processing (e.g., no bytestream recon-
struction). This is a valid choice for simple load balancing
scenarios where redirection does not need to be based on the
content of the request. The L7_LB adds more overhead since
it needs to redirect the connection and look at application
data to select a server. However, it also provides greater flex-
ibility since the destination server can be determined after
the HTTP GET request has been received. The range of appli-
cations supported by our system is best demonstrated by the
L7 LB + Cache test since it shows the value of being able to
deploy both middleboxes and end server applications on an
integrated platform. Using Lighttpd as a cache can increase
throughput by 29% or more compared to directing requests
to Nginx via the L2 forwarder. The Microboxes Bytestream
Proxy has the largest overheads as it needs to maintain con-
nections for both server side and client side. With this cost,
 0 1 2 3 4 5 6 1 2 3 4 5 6 7 8Throughput (Gbps)#NFsmosmbmb-batch 0 0.25 0.5 0.75 1 1.25 1 2 3 4 5 6 7 8Throughput (Gbps)#NFsmosmbmb-batch 0 50 100 150 200 1 2 3 4 5 6 7 8Response Time (us)#NFsmos-8KBmb-8KBmos-64Bmb-64B 0 40 80 120 160 200 1 2 3 4 5 6Latency (us)Chain Lengthseq-8KBpara-8KBseq-64Bpara-64B 0 4 8 12 16 20123456Throughput (Gbps)#Stacks 0 1 2 3 4 5 6Throughput (Gbps)Chain TypeAllClient Flows (CF)CF, Server EventsDynamicDynamic w/BuﬀerSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
G. Liu et al.
189
375
199
210
129
90
255
1
0.61
0.96
0.92
1.29
1.37
0.79
Latency(us) Reqs/s Norm.
NF Type
DPDK L2 Fwd
HAProxy
L4 LB
L7 LB
L7 LB + 50% Cache
L7 LB + 100% Cache
Bytestream Proxy
Table 3: The Bytestream Proxy, L4 and L7 load bal-
ancers use different µStacks to provide trade-offs in
load balancer flexibility and performance; all can out-
perform HAProxy, and integrating a cache with the
LB can increase throughput by 1.29X or more.
30,174
18,356
28,894
27,772
38,981
41,372
23,984
however, comes the opportunity to transform the bytestream
in arbitrary ways. HAProxy also uses two sockets, yet it has
substantially higher latency and lower throughput than the
other approaches since it is not based on an optimized stack
or an NFV IO platform.
This experiment demonstrates the customization that Mi-
croboxes can offer—depending on the needs of a specific load
balancer it can select the appropriate stack (minimalist TCP,
Splicer, or proxy stack). Compared to a traditional approach
like HAProxy, which is tightly coupled to a full TCP stack im-
plemented in the kernel, this can provide between 31% to 57%
improvement in throughput and a 32% to 47% reduction in
latency while only using one core. Extending this further to
use the endpoint stack to host a legacy web server as a cache
provides further benefits. To our knowledge, Microboxes is
the first NFV platform to support a unified deployment of
middleboxes and servers.
7.3 IDS: Customizing Subscriptions
In this experiment, we explore how the ability to filter the
stack events an NF subscribes to affects performance. We
chain two NF modules, DPI and Signature_Match, together
to work as an IDS middlebox. Events from the TCP Moni-
tor µStack go through DPI first and then Signature_Match.
The throughput of this chain is dominated by the Signa-
ture_Match module as it is much slower than DPI. We use
five different configurations to show how stack customiza-
tion and dynamic subscriptions can affect the performance.