## 解决办法1  
不同的实例使用不同的文件系统。  
例如  
```  
mkfs.ext4 /dev/mapper/vgdata01-lv01  
mkfs.ext4 /dev/mapper/vgdata01-lv02  
mount /dev/mapper/vgdata01-lv01 /data01  
mount /dev/mapper/vgdata01-lv02 /data02  
```  
两个数据库实例分别放在/data01和/data02  
限制/dev/mapper/vgdata01-lv01的IOPS，不会影响另一个文件系统。  
这种方法的弊端：如果实例数很多，需要拆分成很多个小的文件系统，不适合空间弹性管理和共用。  
## 解决办法2  
针对ext4  
正常情况下写数据的顺序如果你要修改metadata，必须确保metadata对应的块改变已经落盘，因此可能出现写metadata被迫要刷dirty data page的情况。  
![pic](20160107_02_pic_001.png)  
如果dirty data page刷盘很慢，就会导致metadata写受堵。而写metadata journal又是串行的，势必影响其他进程对metadata journal的修改。  
使用 data=writeback 加载ext4文件系统。  
这个方法的原理是写metadata前，不需要等待data写完。从而可能出现metadata是新的，但是数据是旧的情况。(例如inode是新的，data是旧的，可能某些inode引用的块不存在或者是旧的已删除的块)  
写metadata不等待写data，好处就是串行操作不好因为data受堵塞而堵塞。  
```  
       data={journal|ordered|writeback}  
              Specifies the journalling mode for file data.  Metadata is always journaled.  To use modes other than ordered on the root filesystem, pass the mode to  the  kernel  as  boot  parameter,  e.g.   root-  
              flags=data=journal.  
              journal  
                     All data is committed into the journal prior to being written into the main filesystem.  
              ordered  
                     This is the default mode.  All data is forced directly out to the main file system prior to its metadata being committed to the journal.  
              writeback  
                     Data  ordering  is not preserved - data may be written into the main filesystem after its metadata has been committed to the journal.  This is rumoured to be the highest-throughput option.  It  
                     guarantees internal filesystem integrity, however it can allow old data to appear in files after a crash and journal recovery.  
```  
弊端，文件系统或操作系统crash后，可能导致metadata和data不一致，出现脏块。  
## 解决办法3  
将用作journal块设备独立出来，在限制IOPS时，不限制journal块设备的IO(因为metadata journal的操作很少，也很快，没有必要限制)，只限制data块设备的IO。  
这种方法只适合xfs文件系统。  
ext4文件系统使用这种方法未达到效果，ext4分开journal dev方法如下，但是没有效果，你可以尝试一下。  
创建逻辑卷，一个放DATA，一个放journal  
```  
#pvcreate /dev/dfa  
#pvcreate /dev/dfb  
#pvcreate /dev/dfc  
#vgcreate aliflash /dev/dfa /dev/dfb /dev/dfc  
#lvcreate -i 3 -I 8 -L 1T -n lv01 aliflash  
#lvcreate -i 3 -I 8 -L 2G -n lv02 aliflash  
```  
创建journal块设备  
```  
#mkfs.ext4 -O journal_dev -b 4096 /dev/mapper/aliflash-lv02  
mke2fs 1.41.12 (17-May-2010)  
Discarding device blocks: done                              
Filesystem label=  
OS type: Linux  
Block size=4096 (log=2)  
Fragment size=4096 (log=2)  
Stride=2 blocks, Stripe width=6 blocks  
0 inodes, 525312 blocks  
0 blocks (0.00%) reserved for the super user  
First data block=0  
0 block group  
32768 blocks per group, 32768 fragments per group  
0 inodes per group  
Superblock backups stored on blocks:   
Zeroing journal device: done     
```  
创建ext4文件系统  
```  
#mkfs.ext4 -E stride=16,stripe-width=48 -J device=/dev/mapper/aliflash-lv02 /dev/mapper/aliflash-lv01  
mke2fs 1.41.12 (17-May-2010)  
Using journal device's blocksize: 4096  
Discarding device blocks: done                              
Filesystem label=  
OS type: Linux  
Block size=4096 (log=2)  
Fragment size=4096 (log=2)  
Stride=16 blocks, Stripe width=48 blocks  
67117056 inodes, 268437504 blocks  
13421875 blocks (5.00%) reserved for the super user  
First data block=0  
Maximum filesystem blocks=4294967296  
8193 block groups  
32768 blocks per group, 32768 fragments per group  
8192 inodes per group  
Superblock backups stored on blocks:   
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,   
        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968,   
        102400000, 214990848  
Writing inode tables: done                              
Adding journal to device /dev/mapper/aliflash-lv02: done  
Writing superblocks and filesystem accounting information: done  
This filesystem will be automatically checked every 31 mounts or  
180 days, whichever comes first.  Use tune2fs -c or -i to override.  
#ll /dev/mapper/aliflash-lv0*  
lrwxrwxrwx 1 root root 7 Jan  7 11:12 /dev/mapper/aliflash-lv01 -> ../dm-0  
lrwxrwxrwx 1 root root 7 Jan  7 11:12 /dev/mapper/aliflash-lv02 -> ../dm-1  
#ll /dev/dm-0  
brw-rw---- 1 root disk 253, 0 Jan  7 11:22 /dev/dm-0  
#ll /dev/dm-1  
brw-rw---- 1 root disk 253, 1 Jan  7 11:22 /dev/dm-1  
```  
挂载文件系统  
```  
#mount -o nobarrier,noatime,nodiratime,discard,defaults,nodelalloc /dev/mapper/aliflash-lv01 /data01  
```  
使用本文开头的方法，只限制/dev/mapper/vgdata01-lv01的IOPS，测试不能解决问题。  
XFS文件系统使用journal dev的方法  
```  
# mkfs.xfs -f -b size=4096 -l logdev=/dev/mapper/vgdata01-lv02,size=2136997888,sunit=16 -d agcount=9000,sunit=16,swidth=48 /dev/mapper/vgdata01-lv01   
# mount -t xfs -o nobarrier,nolargeio,logbsize=262144,noatime,nodiratime,swalloc,logdev=/dev/mapper/vgdata01-lv02 /dev/mapper/vgdata01-lv01 /data01  
```  
使用本文开头的方法，只限制/dev/mapper/vgdata01-lv01的IOPS，测试，问题得到解决。  
被限制IOPS的实例，IO使用确实只能达到限制值，所以在不改变原有需求的情况下，实现了相互不干扰。   
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")