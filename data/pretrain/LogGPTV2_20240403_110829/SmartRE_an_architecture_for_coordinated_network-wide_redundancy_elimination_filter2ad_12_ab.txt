B
C
D
D
A
B
C
D
A, B, C, D
B, C, D, A
C, D, A, B
D, A, B, C
A, B, C, D
A
A
A
A
A
1
1
1
1
B
B
B
B
B
1
1
1
C
C
C
C
C
1
1
D
D
D
D
D
1
Footprint before RE = 8 pkts * 4 hops = 32 
Total network footprint after RE = 28 
No savings from RE on interior links
Each router switches 8 packets
Total Network Footprint = 22 packets
33% savings
Routers switch 20% smaller pkts
e.g., R1 need not switch packets 6,7,8
Figure 1: Beneﬁts of a coordinated approach when RE devices
have constraints on memory size.
can be encoded or not, requires F random accesses to determine if
there is a match or not. Once matches are found, further processing
is required to actually create the encodings. On the other hand, de-
coding throughput is at least R/k. This is because each packet has
between 0 and k encodings. Thus, in this standalone case, decoding
is ≥ F/k times faster than encoding. Since k ≤ F , the decoding
throughput is clearly higher.
Throughput on a single link: Given this understanding of the
standalone encoding and decoding throughput, we can now con-
sider the throughput across a single link. For simplicity, let us as-
sume all packets are of the same size MSS . Suppose that the link
capacity is such that it can carry P MSS -sized packets per second.
For instance, if the link speed is 2.4Gbps (OC48), and MSS =
500B, then P = 6 × 105 and for an OC192 link P = 2.4 × 106.
Two cases arise:
1. Slow link (R/F ≥ P ): This means that line rate encod-
ing and decoding are possible; e.g., for an OC48 link where
R/F = 2 × 106 ≥ P = 6 × 105. In this case, the encoder
can encode up to P packets per second, each carrying up to
k matches. The decoder can decode each encoded packet.
2. Fast link (R/F < P ): This means that line rate encoding
is not possible. This is the case for OC192 and higher speed
links. (R/F = 2 × 106 < P = 2.4 × 106). In this case, the
encoder can encode no more than R/F packets per second;
a fraction of packets are left un-encoded to ensure line-rate
operation. Even though the decoder as a standalone operates
F/k times faster, its decoding throughput is now limited by
the encoding throughput immediately upstream. Thus, it is
limited to decoding R/F packets per second.
3.2 Motivating Examples
We present the examples in the context of a “bump-in-the-wire”
deployment where an RE middlebox is attached to router linecards.
Each RE device has pre-speciﬁed resource constraints. These cap-
ture hardware limitations (e.g., how many decoding actions can
the device perform per unit time?) or economic constraints (e.g.,
DRAM cost which could limit total memory per device).
These examples also apply when there are resource budgets per
router. For example, processing constraints induced by power/cooling
requirements are better modeled on a per-router/per-PoP basis rather
than per-middlebox. Also, software or virtualized RE deployments
(e.g., [14, 21]) would be characterized by per-router constraints.
As the following examples show, the naive hop-by-hop approach
described in the previous section severely constrains the effective-
ness of redundancy elimination.
Memory efﬁciency and router beneﬁts: Consider the scenario in
Figure 1. Suppose each RE device on the path has memory to store
only 1 packet for this path (since the devices are shared among the
paths that traverse the link), but the RE devices on the ﬁrst link can
89Hop-by-hop Redundancy Elimination
Coordinated Redundancy Elimination
I1
5E
5 enc/s
5 dec/s
10 enc/s, 20 dec/s
5 enc/s
5 dec/s
I1
5E
I2
5E
5E
I3
5E
I4
5D
R1
10E
10D
10E
R2
R3
10D
Assume each decoding saves X bytes
Total savings = 5X * 4 + 10X * 2 = 40X
I2
5E
5E
I3
5E
I4
0D
R1
10 enc/s, 20 dec/s
20D
R3
R2
0D
Assume each decoding saves X bytes
Total savings = 20X * 3 = 60X
Figure 2: Beneﬁts of coordination when RE devices have con-
straints on encoding/decoding throughput.
store 4 packets. Each store is managed in a FIFO fashion. The
hop-by-hop model yields no beneﬁts from RE on the interior links.
A coordinated approach can ensure that the different packets are
stored and decoded at different routers. This helps reduce the total
trafﬁc by 33%. There are secondary beneﬁts in that routers have
to switch smaller packets internally, thereby improving their effec-
tive switching capacity. This example shows that a coordinated
approach can more effectively use a given amount of memory.
Memory access constraints: Consider the example shown in Fig-
ure 2. Here, the links between ingresses I1. . .I4 and the core router
R1 are much slower than the core-core links. Assume that the en-
coding RE device at the slow link can perform 5 packet encodings
per second (this corresponds to case #1 from §3.1 where P = 5).
The encoding RE device at the fast links can perform 10 packet en-
codings per second (this corresponds to case #2 from §3.1 where
R/F = 10). Now, consider the decoding devices. The ones on the
slow links can decode 5 packets per second, while the ones on the
fast link can decode up to 20 packets per second (R/k = 20).
In the hop-by-hop case, the number of packets decoded by a
downstream RE device is the same as the number of packets en-
coded by the immediate upstream device. Assuming each decoding
saves X bytes, the hop-by-hop approach removes 40X bytes (5X
on 4 ingress-core router links, and 10X on two core-core links).
Consider an alternative coordinated scenario, in which the RE de-
vices on interior routers are not involved in encoding and can de-
code at the maximum rate. In this case, devices on R1 and R2 can
just forward encoded packets and R3 can allot its full decoding ca-
pacity. This will reduce the total network footprint by 20 × 3 × X.
(Since R3 is 3 hops away from the ingress, for each decoded packet
we save 3 hops in the network footprint). Also, some of the devices
perform no RE function; yet this architecture is 1.5× better than
the hop-by-hop approach.
Beneﬁts under partial deployment: In Figure 2, consider a par-
tial deployment scenario with no RE devices attached to router R1.
In the hop-by-hop approach, the total savings would only be 10X
(only on link R2-R3). Note that since the coordinated approach
did not involve R1, it provides 60X savings even with partial de-
ployment. Network operators can thus realize signiﬁcantly more
beneﬁts with partial deployment with a coordinated design.
The above examples demonstrate the beneﬁts of a hypothetical
intelligent and coordinated approach. Next, we describe how we
can implement this hypothetical approach in practice.
4. SmartRE DESIGN
In this section, we formally describe the design of SmartRE, an
architecture for redundancy elimination that draws on the principles
of spatially decoupling encoding and decoding responsibilities, and
coordinating the actions of RE devices for maximum efﬁciency.
Our description focuses on SmartRE as applied to an ISP network.
SmartRE synthesizes two ideas: packet caches for redundancy
elimination [29, 12] and cSamp [31]. SmartRE leverages ideas
from cSamp to split caching (and decoding) responsibilities across
multiple router hops in a network. It speciﬁes the caching respon-
Figure 3: Schematic depiction of SmartRE.
sibility of each RE device in terms of a hash-range per path per
device. Each device is responsible for caching packets such that
the hash of the packet header falls in its assigned ranges. By us-
ing the same hash function across the network and assigning non-
overlapping hash ranges across devices on the same path, SmartRE
leverages the memory resources efﬁciently without requiring ex-
pensive cache coordination protocols.
A network operator can specify different ISP-wide objectives,
e.g., minimizing network utilization, aiding trafﬁc engineering goals.
SmartRE uses a network-wide optimization framework that takes
into account the prevailing trafﬁc conditions (volume, redundancy
patterns), the network’s routing policies, and the capacities of indi-
vidual RE devices to assign encoding and decoding responsibilities
across the network to optimally satisfy the operator’s objectives.
4.1 System Overview
We focus our discussion on the design of three key elements
(Figure 3):
ingress nodes, interior nodes, and a central conﬁgu-
ration module. Ingress and interior nodes maintain caches storing
a subset of packets they observe.
Ingress nodes encode packets. They search for redundant con-
tent in incoming packets and encode them with respect to previ-
ously seen packets using the mechanism described in §2. In this
sense, the role of an ingress node is identical in the naive hop-by-
hop approach and SmartRE.
The key difference between the hop-by-hop approach and SmartRE
is in the design of interior nodes. First, interior elements need not
store all packets in their packet cache – they only store a subset
as speciﬁed by a caching manifest produced by the conﬁguration
module. Second, they have no encoding responsibilities. Interior
nodes only decode packets, i.e., expand encoded regions speciﬁed
by the ingresses using packets in their local packet cache.
The conﬁguration module computes the caching manifests to op-
timize the ISP objective(s), while operating within the memory and
packet processing constraints of network elements. Similar to other
proposals for centralized network management (e.g., [18, 15, 13]),
we assume that this module will be at the network operations cen-
ter (NOC), and has access to the network’s trafﬁc matrix, routing
policies, and the resource conﬁgurations of the network elements.
4.2 Network-wide Optimization
The conﬁguration module uses a network-wide view of trafﬁc
patterns and resource constraints to compute how and where de-
coding should be done to optimize ISP objectives.
Assumptions and Terminology: We assume that the trafﬁc ma-
trix (volume of trafﬁc in bytes and packets between every pair of
ingress-egress routers) and the routing path(s) between an ingress-
egress pair are known and given as inputs. We use the subscripts
p and q to indicate paths, r to denote a node (either a router or a
90bump-in-the-wire middlebox) and the notation r ∈ p to denote that
node r lies on the path p. vp is the total trafﬁc volume, in bytes,
ﬂowing on path p in a speciﬁc measurement interval. distance p,r
is the upstream latency (e.g., hop count, OSPF weights, physical
ﬁber distance) of path p up to node r. In our current framework,
distance p,r is speciﬁed in terms of the hop count.
We also assume that we know the redundancy proﬁle of the net-
work from historical trafﬁc data or using periodic reports from
ingress nodes. This redundancy proﬁle is speciﬁed in terms of
two constants for every pair of paths. These are (1) match p,q
(measured in packets), the number of matches that trafﬁc ﬂowing
through path p observes with trafﬁc on path q and (2) matchlen p,q
(in bytes) denoting the average match length observed within these
packets (this is bound by the MSS). As a special case, match p,p
and matchlen p,p capture intra-path redundancy. As such, our cur-
rent focus is on redundancy between paths with the same ingress.
The conﬁguration module maximizes the total savings (i.e., min-
imizing the network footprint or the link utilization-distance prod-
uct), while respecting the operating resource constraints: i.e., the
total available memory (Mr) and the total decoding processing
power (Lr) per node. A network operator could specify other network-
wide objectives as well.
Formulation: The key variables in the formulation are the dp,r
values. Each dp,r speciﬁes the fraction of trafﬁc on path p that node
r caches. We now describe how the variables dp,r are determined.
First, we model the packet store capacity constraints on each node:
∀r,
dp,r × vp ≤ Mr
(1)
X
p:r∈p
Next, we model the total packet processing capabilities on each
node. The processing capabilities are bound by the number of
memory operations that can be performed in unit time.1 For each
interior node, there are two types of memory operations that con-
tribute to the processing load: caching and decoding. We assume
for simplicity that both operations are equally expensive per-packet,
but it is easy to incorporate other models as well. The total num-
ber of packets that will be stored by r on path p is dp,r × vp
avgpktsize .
(avgpktsize appears because vp is in bytes but the load is per packet.)
The total number of matches that will be decoded by node r is
P
p,q:r∈p,r∈q dq,r × match p,q.2 Thus, we have
∀r,
X
p,r∈p
dp,r
vp
avgpktsize
+
X
p,q:r∈p,q
dq,r match p,q ≤ Lr
(2)
There is a natural constraint that the total range covered on each
path should be less than or equal to 1:
∀p,
X
r:r∈p
dp,r ≤ 1
(3)
Next, we compute the total savings in the network-wide foot-
print. The savings provided by node r for trafﬁc on path p (Sp,r)
depends on the redundancy that p shares with other paths that tra-
verse r and the caching responsibility that r has for these paths. It
also depends on the location of r on the path p – the more down-
stream r is (higher distance p,r), the greater savings it provides.
1We do not explicitly model CPU constraints because these are
subsumed by processing constraints imposed by memory accesses.
2Strictly speaking, this is an approximation that assumes that the
matches are uniformly spread out across the different dq,r ranges.
In practice, this is a reasonable assumption.
Sp,r =
X
q:r∈q
dq,r × distance p,r × match p,q × matchlen p,q (4)