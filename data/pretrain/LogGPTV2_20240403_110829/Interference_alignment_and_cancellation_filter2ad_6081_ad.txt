Data+Poll        
Contention 
Period
CF-
End
Data+Req
Data+Req
Data+Req
Data+Req
. . . . . 
Data+Req
Data+Req
ACKs
Downlink
Downlink
Beacon + 
ACK Map
Figure 9: IAC’s Extension to PCF.
CF-End
i
d
F
s
P
A
#
…….
C
R
C
Figure 10: The metadata in a DATA+Poll frame. This metadata is
broadcast by the leader AP alone to inform the clients in a downlink
transmission group of their decoding vectors and the other APs of their
encoding vectors.
requires knowing which clients are served concurrently. This is the
job of the concurrency algorithm, which divides clients with pending
trafﬁc into groups of concurrent transmissions that we call transmis-
sion groups. It further decides which AP serves which client in a
transmission group, and the values of the encoding and decoding vec-
tors. The process for deciding this is described in §7.2. In this section,
we focus on how to deliver packets in each transmission group.
Fig. 9 shows the series of events during a contention-free period.
At the beginning of a CFP, the leader AP sends a beacon. The leader
AP then steps through the downlink transmission groups, one at a
time, transmits their downlink packets with the help of other APs, and
polls the corresponding clients for uplink trafﬁc. This mode is similar
to current PCF behavior, except that in the current PCF the AP steps
through a list of individual clients, one at a time; whereas in IAC the
leader AP steps through a list of transmission groups.
(b.1) Downlink. The leader AP ﬁrst goes through the list of down-
link transmission groups. With the help of other APs, it sends a
DATA+Poll frame to each group. This frame has two parts. The
ﬁrst part, shown in Fig. 10, is broadcast by the leader AP alone, and
contains the ids of the clients in the group and their encoding and
decoding vectors. The ids are given to the clients upon association.
The encoding and decoding vectors are computed by the concurrency
algorithm which runs on the leader AP. The leader AP also includes a
frame id, Fid, the number of APs and a checksum of its broadcast.
Further, it sets the length of the DATA+Poll frame to the maximum
length of the packets in the transmission group, so that all clients know
when the frame ends. The second part of the frame is the combination
of concurrent transmissions by all APs. For the example of three APs
with 2-antennas each, this part has the three APs transmitting a packet
to each of the three clients in a transmission group.
Note that both the clients and APs listen to the leader AP as it broad-
casts the ﬁrst part of the DATA+Poll frame. In order to transmit
concurrently, the APs need to learn their encoding vectors. Similarly,
the clients need to learn the decoding vectors to be able to decode
their data. The clients and APs can use the checksum to test whether
they received the correct information. Note that the transmissions still
work ﬁne if any of the APs or the clients failed to hear the leader AP.
Speciﬁcally, the AP/client who failed to hear the leader AP, will not
transmit. The other transmissions can go as desired.
in which they transmit these acks is the same as the order of their ids
in the DATA+Poll frame. These acks are similar to synchronous
802.11 acks. In 802.11, they are sent one after each data packet. Here
the data packets are sent concurrently and all acks follow.
(b.2) Uplink. After going through all downlink groups, the leader
AP steps through the uplink groups. Similar to the downlink case, the
leader AP ﬁrst broadcasts a Grant frame specifying the ids of the
clients that will transmit on the uplink, and the encoding and decoding
vectors. The other APs listen to the encoding and decoding vectors
and wait for clients’ transmissions. The clients in an uplink group
use their encoding vectors to transmit simultaneously on the uplink.
Each client transmits a Data+Req frame. This frame contains the
client’s uplink data. If the client still has trafﬁc to send, the frame
will also contain a new request for transmission. Each AP listens to
the Data+Req frame and projects the received signal on the proper
decoding vector. This projection is orthogonal to the interfering
signals and hence it allows each AP to receive its client of interest.
One difference between the uplink and downlink is that, while each
client on the downlink can immediately ack its packet, the APs need to
decode successively using interference cancellation and hence cannot
send synchronous acks. The solution however is simple. During
the following contention period, the APs inform the leader AP of
successful receptions using Ethernet. The leader AP combines and
sends all acks at the beginning of the next CFP, by embedding them
in the beacon information as a bit map. This should not cause any
signiﬁcant delay since it allows all clients in the CFP mode to learn
about their previous packet before they get to send the next packet.
At the end of CFP, the leader AP sends a CF-End frame. This
allows the clients to go back to the contention mode, where they use
traditional point-to-point MIMO. A few points are worth noting.
(a) How do we deal with lost packets and retransmissions? If a
packet is lost on the uplink, the client discovers the loss from the
lack of an ack (at the beginning of the next CFP) and asks for a new
transmission slot next time it is polled. On the downlink, the corre-
sponding AP discovers the packet loss immediately, from the lack of
a client ack, and asks the leader AP to schedule a retransmission.
(b) Is it possible for various APs to make inconsistent decisions?
Only the leader AP makes decisions, while other APs are dumb
transmitters/receivers. Similar to clients, they receive their encoding
and decoding vectors for each transmission group over the medium
and use them without any modiﬁcation. They only inform the leader
AP in case a packet is lost, or the channel’s estimate has changed.
(c) How often do APs need to communicate over the Ethernet
and what do they exchange? As described in §4, APs exchange
the decoded packets over the Ethernet to perform interference can-
cellation. Further, the subordinate APs need to tell the leader AP
whenever a packet is lost or channel coefﬁcients to a client changes
by more than a threshold value. The APs can send this information as
annotation on packets they exchange to perform cancellation.
(d) How large is the Ethernet overhead? To minimize Ethernet
overhead, IAC connects the set of APs using a hub. This design
ensures that every decoded packet is broadcast only once to all APs
and to the switch that forwards the packet to its wired/ﬁnal destination.
In this design every packet is transmitted once and there is no extra
overhead. While a hub is less efﬁcient for a general Ethernet than a
switch, it is a natural choice to connect the IAC APs. This hubbed
network is then connected to the rest of the Ethernet via a switch.
After the DATA+Poll frame, the clients in the transmission group
send their acks, one after the other, using traditional MIMO. The order
(e) How large is the wireless overhead associated with IAC’s
MAC? IAC introduces metadata to coordinate clients and APs.
165Speciﬁcally, concurrent transmissions are preceded by a short broad-
cast from the leader AP to inform the client-AP pairs of their encoding
and decoding vectors. Such a broadcast message already exists in
802.11 PCF mode. 8 We only annotate these messages with extra
information that is a few bytes per client-AP pair. Assuming 1440
byte packets, the overhead of the metadata amounts to 1-2%. In
comparison, the throughput improvement expected from IAC is 1.5x
to 2x, which more than compensates for the loss.
7.2 Concurrency Algorithm
The concurrency algorithm runs at the leader AP. The leader AP
maintains a FIFO queue for trafﬁc pending for the downlink and a
similar queue for uplink requests learned from DATA+Poll frames
(see §7.1). Given the queues of uplink and downlink trafﬁc, the con-
currency algorithm generates the uplink and downlink transmission
groups. Without loss of generality, we will focus on the downlink.
There are multiple options for how to combine clients. The brute
force approach considers all combinations of clients with queued
packets and all different ways of assigning them to existing APs, com-
putes the encoding and decoding vectors, and estimates the throughput
of each combination. The throughput of a transmission group can be
estimated without any transmissions as:∑i log(1+k~vT
i Hi~wik2), where
the sum is over client-AP pairs, Hi is the channel for a pair, and ~vi and
~wi are the corresponding encoding and decoding vectors [29]. It then
creates transmission groups for the queued packets which maximize
throughput. There are two problems with such an approach. First, es-
timating the throughput for every combination of clients in the queue
is a combinatorial problem in the number of clients. Second, since
this approach focuses on maximizing throughput, it always prefers
clients with good channels and hence is unfair. Alternatively, one can
always create transmission groups by combining packets according
to their arrivals in the FIFO queue. This approach is simple and
gives each client a fair access to the medium, but is oblivious to the
throughput of a particular grouping. In practice, different groups may
yield signiﬁcantly different throughput gains (see §10.3).
(a) The Best of Two Choices.
IAC’s concurrency algorithm
balances the desire for high throughput with the need to be fair. To
prevent starvation and reduce delay, it always picks the head of the
FIFO queue as the ﬁrst packet in the current transmission group. To
reduce computational overhead, it picks other clients in the group
using the best of two choices, a standard approach for reducing the
complexity of combinatorial problems [23]. Say each group has three
clients, and we already picked the ﬁrst client in the group as the client
whose packet is at the head of the transmission queue. We randomly
pick two clients with queued packets as candidates for the second
position in the group. Similarly, we also randomly pick two clients
for the third position in the group. Now we estimate the throughput
for the four transmission groups formed by these potential candidate
clients and pick the group that optimizes throughput. As a result,
instead of computing throughput for every possible combination of
clients, we just compute it for four random client combinations.
Let us now consider the fairness of the approach. A client is
considered for transmission either because it is at the head of the
queue or because of a random choice. Both these cases give the
client a fair access to the medium. However, since after picking
the candidate clients, we still optimize for throughput, we need a
mechanism to ensure that clients that never maximize throughput get
picked. To do this, we assign a credit counter to each client. If the
client is considered as a result of a random choice, and is ignored since
8802.11 calls the Grant frame CF-Poll, i.e., it is a poll without downlink data.
it does not maximize throughput, the counter is incremented; but if it
is picked for transmission the counter is reset. If the counter crosses a
threshold, the client is selected as part of the group irrespective of the
throughput. This mechanism ensures that every client is part of some
group at least a minimum number of times.
8 Channel Estimation
In IAC, the APs estimate and convey the channels to the leader AP as
annotation on the decoded packets sent over the Ethernet.
(a) Uplink: To estimate the encoding and decoding vectors for the
uplink, we need the physical channel from each concurrent client
to each AP, as shown in Eqs. 3 and 4. In the absence of concurrent
clients, estimating this channel is a standard MIMO technique [2].
Thus, the ﬁrst time a client broadcasts an association message, all
APs estimate the channel from that client to themselves. Once the
APs have an initial estimate, they need to track it. This is done using
the client’s ack packets from the contention-free period, and its data
packets from the contention period. Both packet types are transmitted
without any concurrent transmissions. Hence they can be processed
using standard MIMO channel estimation [2].
Since the APs can estimate the channel from every ack the client
transmits, they obtain a frequent estimate of the channel. In static en-
vironments the channel is relatively stable and can be easily tracked at
this estimation frequency. Slight inaccuracy in estimating the channel
only means that the interference is not fully eliminated after applying
the encoding and decoding vectors. As long as most interference is
eliminated, the loss in throughput stays negligible.
(b) Downlink: Channel estimation is typically done at the re-
ceiver [7]. Thus, we have two options: either have clients estimate
and convey the channels to the leader AP when it polls them, or try
to have APs estimate the channel by exploiting reciprocity between
uplink and downlink channels. In our measurements, the latter option
worked with sufﬁcient accuracy and hence we adopt it. Reciprocity
means that the channel from node A to node B is the transpose of the
channel from B to A. Thus, an AP can use the uplink channel from a
particular client to infer the downlink channel to that client.
It is important to understand that channel reciprocity does not mean
that the link between two nodes A and B is symmetric. Reciprocity
(i.e., the kind that we care about in this paper) means that the channel
coefﬁcients are the same, but the noise or interference could be vastly
different. For example, if A transmits symbol x, node B receives yB =
Hx + nB. Similarly, in the opposite direction, node A receives yA =
Hx + nA. The channel multiplier, H, is the same, but the noise could
be much higher at A if it is close to a microwave oven. Hence, one may
see many packet drops at A but not at B, but this does not contradict
reciprocity. Reciprocity has been conﬁrmed in measurements [16, 28,
27] and is used in QUALCOMM’s 802.11n proposal [2].
Reciprocity cannot be applied directly without calibration to ac-
count for hardware differences between the tx and rx chains. The
calibration however can be computed once and does not change for
the same sender receiver pair. IAC uses a calibration method from
the QUALCOMM’s 802.11n proposal [2]. Let H d be the channel
between a particular AP and client pair, and H u the uplink channel
from that client to the same AP. Then:
(H d)T = CClient,rx H u CAP,tx,
(8)
where H T refers to the transpose of H, and CClient,rx and CAP,tx are
constant diagonal matrices that describe the extra attenuation and
delay observed by the signal in the transmit and receive hardware
chains on the client and the AP respectively.
166Figure 11: Testbed Topology.
9 Complexity
IAC multiplies each packet with an encoding vector at the transmitter
and projects on a direction orthogonal to interference at the receiver.
Both pre-coding and projection are general operations in MIMO
designs [2]. IAC also performs interference cancellation, which is
linear in the number of cancelled packets. Since, the packets cancelled
at an AP are already decoded at prior APs, all the packets can be
cancelled in parallel. Hence, the delay from cancellation can be made
independent of the number of cancelled packets.
10 Performance Evaluation
We evaluate IAC in a testbed of MIMO software radios. Each node
is a laptop connected to a 2-antenna USRP radio board and runs the
GNU-Radio software. To create a MIMO node, we equip each USRP
with two RFX2400 daughterboards. We also set the MUX value in
software to allow the FPGA to process samples from both antennas.
(a) Topology. Our testbed, shown in Fig. 11, has 20 nodes. Each
node has two antennas. All nodes are within radio range of each other
to ensure that concurrent transmissions are enabled by the existence
of multiple antennas, not by spatial reuse.
(b) Modulation. IAC uses the modulation/demodulation module as
a black-box and hence works with a variety of modulation schemes.
Our implementation, however, uses BPSK, which is the modulation
scheme that 802.11 uses at low rates.
(c) Parameters. We use the default GNU-Radio parameters. How-
ever, in order to drive two antennas at the same time, we double the
interpolation and decimation rates at the transmitter and the receiver.
Each packet consists of a 32-bit preamble, and 1500-byte payload.
(d) Compared Schemes. We compare the following:
• IAC: This is our implementation of IAC.
• 802.11-MIMO: There are multiple proposals for 802.11n [2, 1].
These schemes are all point-to-point, i.e., they allow only one
transmitter to access the medium at any point in time. They how-
ever differ in the amount of channel information available to the
transmitter, with more channel information leading to better perfor-
mance [29]. Since IAC uses full channel information, we compare
it with an 802.11 MIMO design with full channel information
available to both sender and receiver. This design is based on
QUALCOMM’s eigenmode enforcing [2] and uses an approach
that is proven optimal for point-to-point MIMO [29].
(e) Setup. In each experiment, we randomly pick some nodes to act
as APs and others to act as clients. We repeat the same experiment
with IAC and 802.11-MIMO. Three points are worth noting.
• First, we allow 802.11-MIMO access to the same number of APs
as IAC. Though 802.11-MIMO cannot use the additional APs for
concurrent transmissions, it can use them to increase diversity.
For example, if there are three APs, each 802.11-MIMO client
communicates with the AP to which it has the best SNR.
• Second, we use a simpliﬁed TDMA MAC for both IAC and 802.11-
MIMO. The MAC assigns the same number of transmission times-
lots to the two schemes. Consider an uplink scenario that involves
three clients and three APs. We start with the 802.11-MIMO exper-
iment and assign each client to its best AP. Each client transmits for
100 time slots, for a total of 300 time slots for the 802.11-MIMO
experiment. We follow with an IAC experiment where clients
transmit together for a total of 300 time slots. We then repeat the
experiment for a different client set. This simpliﬁed MAC allows
for a fair comparison between IAC and 802.11-MIMO because it
assigns the medium equally to each scheme. Implementing the