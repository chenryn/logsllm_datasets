title:On-the-Fly Verification of Rateless Erasure Codes for Efficient Content
Distribution
author:Maxwell N. Krohn and
Michael J. Freedman and
David Mazières
On-the-Fly Veriﬁcation of Rateless Erasure Codes
for Efﬁcient Content Distribution
Maxwell N. Krohn
Michael J. Freedman
MIT
NYU
PI:EMAIL
PI:EMAIL
David Mazi`eres
NYU
PI:EMAIL
Abstract— The quality of peer-to-peer content distribution
can suffer when malicious participants intentionally corrupt
content. Some systems using simple block-by-block downloading
can verify blocks with traditional cryptographic signatures and
hashes, but these techniques do not apply well to more elegant
systems that use rateless erasure codes for efﬁcient multicast
transfers. This paper presents a practical scheme, based on
homomorphic hashing, that enables a downloader to perform
on-the-ﬂy veriﬁcation of erasure-encoded blocks.
I. INTRODUCTION
Peer-to-peer content distribution networks (P2P-CDNs) are
trafﬁcking larger and larger ﬁles, but end-users have not
witnessed meaningful increases in their available bandwidth,
nor have individual nodes become more reliable. As a result,
the transfer times of ﬁles in these networks often exceed
the average uptime of source nodes, and receivers frequently
experience download truncations.
Exclusively unicast P2P-CDNs are furthermore extremely
wasteful of bandwidth: a small number of ﬁles account for a
sizable percentage of total transfers. Recent studies indicate
that from a university network, KaZaa’s 300 top bandwidth-
consuming objects can account for 42% of all outbound traf-
ﬁc [1]. Multicast transmission of popular ﬁles might drastically
reduce the total bandwidth consumed; however, traditional
multicast systems would fare poorly in such unstable networks.
Developments in practical erasure codes [2] and rateless
erasure codes [3], [4], [5] point to elegant solutions for both
problems. Erasure codes of rate r (where 0 < r < 1) map
a ﬁle of n message blocks onto a larger set of n=r check
blocks. Using such a scheme, a sender simply transmits a
random sequence of these check blocks. A receiver can decode
the original ﬁle with high probability once he has amassed a
random collection of slightly more than n unique check blocks.
At larger values of r, senders and receivers must carefully
coordinate to avoid block duplication. In rateless codes, block
duplication is much less of a problem: encoders need not pre-
specify a value for r and can instead map a ﬁle’s blocks to a
set of check blocks whose size is exponential in n.
When using low-rate or rateless erasure codes, senders and
receivers forgo the costly and complicated feedback protocols
often needed to reconcile truncated downloads or to maintain
a reliable multicast tree. Receivers can furthermore collect
blocks from multiple senders simultaneously. One can envision
an ideal situation, in which many senders transmit the same
ﬁle to many recipients in a “forest of multicast trees.” No
retransmissions are needed when receivers and senders leave
and reenter the network, as they frequently do.
A growing body of literature considers erasure codes in the
context of modern distributed systems. Earlier work applied
ﬁxed-rate codes to centralized multicast CDNs [6], [7]. More
current work considers rateless erasure codes in unicast, multi-
source P2P-CDNs [8], [9]. Most recently, SplitStream [10] has
explored applying rateless erasure codes to overlapping P2P
multicast networks, and Bullet [11] calls on these codes when
implementing “overlay meshes.”
There is a signiﬁcant downside to this popular approach.
When transferring erasure-encoded ﬁles, receivers can only
“preview” their ﬁle at the very end of the transfer. A receiver
may discover that, after dedicating hours or days of bandwidth
to a certain ﬁle transfer, he was receiving incorrect or useless
blocks all along. Most prior work in this area assumes honest
senders, but architects of robust, real-world P2P-CDNs cannot
make this assumption.
This paper describes a novel construction that lets recipi-
ents verify the integrity of check blocks immediately, before
consuming large amounts of bandwidth or polluting their
download caches. In our scheme, a ﬁle F is compressed down
to a smaller hash value, H(F ), with which the receiver can
verify the integrity of any possible check block. Receivers
then need only obtain a ﬁle’s hash value to avoid being duped
during a transfer. Our function H is based on a discrete-log-
based, collision-resistant, homomorphic hash function, which
allows receivers to compose hash values in much the same
way that encoders compose message blocks. Unlike more
obvious constructions, ours is independent of encoding rate
and is therefore compatible with rateless erasure codes. It is
fast to compute, efﬁciently veriﬁed using probabilistic batch
veriﬁcation, and has provable security under the discrete-log
assumption. Furthermore, our implementation results suggest
this scheme is practical for real-world use.
In the remainder of this paper, we will discuss our setting in
more detail (Sections II and III), describe our hashing scheme
(Section IV), analyze its important properties (Sections V
and VI), discuss related works (Section VII), and conclude
(Section VIII).
II. BRIEF REVIEW OF ERASURE CODES
In this paper, we consider the non-streaming transfer of very
large ﬁles over erasure channels such as the Internet. Typically,
a ﬁle F is divided into n uniformly sized blocks, known
of the input size n, and the parameters k and (cid:14). Finally, the n
message blocks and the n(cid:14)k auxiliary blocks are considered
together as a composite ﬁle F 0 of size n0 = n(1 + (cid:14)k), which
is suitable for encoding.
To construct the ith check block, the encoder randomly
samples a pre-speciﬁed probability distribution for a value di,
known as the check block’s degree. The encoder then selects di
blocks from F 0 at random, and computes their sum, ci. The
outputted check block is a pair hxi; cii, where xi describes
which blocks were randomly chosen from F 0. In practice, an
encoder can compute the degree di and the meta-data xi as
the output of a pseudo-random function on input (i; n). It
thus sufﬁces to send hi; cii to the receiving client, who can
compute xi with knowledge of n, the encoding parameters,
and access to the same pseudo-random function. See Figure 1
for a schematic example of precoding and encoding.
To recover the ﬁle, a recipient collects check blocks of
the form hxi; cii. Assume a received block has degree one;
that is, it has meta-data xi of the form fjg. Then, ci
is
simply the jth block of the ﬁle F 0, and it can be marked
recovered. Once a block is recovered, the decoder subtracts
it from the appropriate unrecovered check blocks. That is, if
the kth check block is such that j 2 xk, then bj is subtracted
from ck, and j is subtracted from xk. Note that during this
subtraction process, other blocks might be recovered. If so,
then the decoding algorithm continues iteratively. When the
decoder receives blocks whose degree is greater than one, the
same type of process applies; that is, all recovered blocks are
subtracted from it, which might in turn recover it.
In the encoding process, auxiliary blocks behave like mes-
sage blocks; in the decoding process, they behave like check
blocks. When the decoder recovers an auxiliary block, it then
adds it to the pool of unrecovered check blocks. When the
decoder recovers a message block, it simply writes the block
out to a ﬁle in the appropriate location. Decoding terminates
once all n message blocks are recovered.
In the absence of the precoding step, the codes are expected
to recover (1(cid:0)(cid:14))n message blocks from (1+(cid:15))n check blocks,
as n becomes large. The auxiliary blocks introduced in the
precoding stage help the decoder to recover the ﬁnal (cid:14)n blocks.
A sender speciﬁes (cid:14) and (cid:15) prior to encoding; they in turn
determine the encoder’s degree distribution and consequently
the number of block operations required to decode.
Online Codes, like the other three schemes, use bitwise
exclusive OR for both addition and subtraction. We note that
although XOR is fast, simple, and compact (i.e., XORing two
blocks does not produce carry bits), it is not essential. Any
efﬁciently invertible operation sufﬁces.
III. THREAT MODEL
Deployed P2P-CDNs like KaZaa consist of nodes who func-
tion simultaneously as publishers, mirrors, and downloaders
of content. Nodes transfer content by sending contiguous ﬁle
chunks over point-to-point links, with few security guarantees.
We imagine a similar but more powerful network model:
Fig. 1. Example Online encoding of a ﬁ ve-block ﬁle. bi are message blocks,
a1 is an auxiliary block, and ci are check blocks. Edges represent addition
(via XOR). For example, c4 = b2 +b3 +b5, a1 = b3 +b4, and c7 = a1 +b5.
as message blocks (or alternatively, input symbols). Erasure
encoding schemes add redundancy to the original n message
blocks, so that receivers can recover from packet drops without
explicit packet retransmissions.
Though traditional forward error correction codes such as
Reed-Solomon are applicable to erasure channels [12], decod-
ing times quadratic in n make them prohibitively expensive
for large ﬁles. To this effect, researchers have proposed a
class of erasure codes with sub-quadratic decoding times.
Examples include Tornado Codes [7], LT Codes [3], Raptor
Codes [5] and Online Codes [4]. All four of these schemes
output check blocks (or alternatively, output symbols) that are
simple summations of message blocks. That is, if the ﬁle F is
composed of message blocks b1 through bn, the check block
c1 might be computed as b1 + b2. The speciﬁcs of these linear
relationships vary with the scheme.
Tornado Codes, unlike the other three, are ﬁxed-rate. A
sender ﬁrst chooses a rate r and then can generate no more
than n=r check blocks. Furthermore, the encoding process
grows more expensive as r approaches zero. For multicast and
other applications that beneﬁt from lower encoding rates, LT,
Raptor and Online codes are preferable [9]. Unlike Tornado
codes, they feature rateless encoders that can generate an
enormous sequence of check blocks with state constant in n.
LT codes are decodable in time O(n ln(n)), while Tornado,
Raptor and Online Codes have linear-time decoders.
This paper uses Online Codes when considering the
speciﬁcs of the encoding and decoding processes; however, all
three rateless techniques are closely related, and the techniques
described are equally applicable to LT and Raptor Codes.
Online Codes. Online Codes consist of three logical com-
ponents: a precoder, an encoder and a decoder. A sender
initializes the encoding scheme via the precoder, which takes
as input a ﬁle F with n message blocks and outputs n(cid:14)k
auxiliary blocks. k is small constant such as 3, and (cid:14), a
parameter discussed later, has a value such as .005. The
precoder works by adding each message block to k distinct
randomly-chosen auxiliary blocks. An auxiliary block is thus
the sum of 1=(cid:14) message blocks on average. This process
need not be random in practice; the connections between the
message and auxiliary blocks can be a deterministic function
When a node wishes to publish F , he uses a collision-
resistant hash function such as SHA1 [13] to derive a succinct
cryptographic ﬁle handle, H(F ). He then pushes F into the
network and also publicizes the mapping of the ﬁle’s name
N (F ) to its key, H(F ). Mirrors maintain local copies of the
ﬁle F and transfer erasure encodings of it to multiple clients
simultaneously. As downloaders receive check blocks, they
can forward them to other downloaders, harmlessly “down-
sampling” if constrained by downstream bandwidth. Once a
downloader fully recovers F , he generates his own encoding
of F , sending “fresh” check blocks to downstream recipients.
Meanwhile, erasure codes enable downloaders to collect check
blocks concurrently from multiple sources.
This setting differs notably from traditional multicast set-
tings. Here, internal nodes are not mere packet-forwarders but
instead are active nodes that produce unique erasure encodings
of the ﬁles they redistribute.
Unfortunately, in a P2P-CDN, one must assume that adver-
sarial parties control arbitrarily many nodes on the network.
Hence, mirrors may be frequently malicious.1 Under these
assumptions, the P2P-CDN model is vulnerable to a host of
different attacks:
Content Mislabeling. A downloader’s original
lookup
mapped N (F ) ! H( ~F ). The downloader will then request
and receive the ﬁle ~F from the network, even though he
expected F .
Bogus-Encoding Attacks. Mirrors send blocks that are not
check blocks of the expected ﬁle, with the intent of thwarting
the downloader’s decoding. This has also been termed a
pollution attack [14].
Distribution Attacks. A malicious mirror sends valid check
blocks from the encoding of F , but not according to the
correct distribution. As a result, the receiver might experience
degenerate behavior when trying to decode.
Deployed peer-to-peer networks already suffer from ma-
licious content-mislabeling. A popular ﬁle may resolve to
dozens of names, only a fraction of which are appropriately
named. A number of solutions exist, ranging from simply
downloading the most widely replicated name (on the as-
sumption that people will keep the ﬁle if it is valid), to more
complex reputation-based schemes. In more interesting P2P-
CDNs, trusted publishers might sign ﬁle hashes. Consider the
case of a Linux vendor using a P2P-CDN to distribute large
binary upgrades. If the vendor distributes its public key in CD-
based distributions, clients can verify the vendor’s signature
of any subsequent upgrade. The general mechanics of reliable
ﬁlename resolution are beyond the scope this paper; for the
most part, we assume that a downloader can retrieve H(F )
given N (F ) via some out-of-band and trusted lookup.
This work focuses on the bogus-encoding attack. When
transferring large ﬁles, receivers will talk to many different
1We do not explicitly model adversaries controlling the underlying physical
routers or network trunks, although our techniques are also robust against these
adversaries, with the obvious limitations (e.g., the adversary can prevent a
transfer if he blocks the downloader’s network access).
mirrors, in series and in parallel. At the very least, the receiver
should be able to distinguish valid from bogus check blocks
at decoding time. One bad block should not ruin hundreds
of thousands of valid ones. Moreover, receivers have limited
bandwidth and cannot afford to communicate with all possible
mirrors on the network simultaneously. They would clearly
beneﬁt from a mechanism to detect cheating as it happens, so
they can terminate connections to bad servers and seek out
honest senders elsewhere on the network.
To protect clients against encoding attacks, P2P-CDNs
require some form of source veriﬁcation. That is, downloaders
need a way to verify individual check blocks, given a reliable
and compact hash of the desired ﬁle. Furthermore, this veriﬁ-
cation must not be interactive; it should work whether or not
the original publisher is online. The question becomes, should
the original publisher authenticate ﬁle blocks before or after
they are encoded? We consider both cases.
A. Hashing All Input Symbols
A publisher wishes to distribute an n-block ﬁle F . Assum-
ing Online Codes, he ﬁrst runs F through a precoder, yielding
an n0-block ﬁle F 0. He then computes a Merkle hash tree of
F 0 [15]. The ﬁle’s full hash is the entirety of the hash tree, but
the publisher uses the hash tree’s root for the ﬁle’s succinct
cryptographic handle. To publish, he pushes the ﬁle and the
ﬁle’s hash tree into the network, all keyed by the root of the
hash tree. Note that although the hash tree is smaller than the
original ﬁle, its size is still linear in n.
To download F , a client maps N (F ) to H(F ) as usual,
but now H(F ) is the root of the ﬁle’s hash tree. Next, the
client retrieves the rest of the hash tree from the network,
and is able to verify its consistency with respect
to its
root. Given this information, he can verify check blocks as
the decoding progresses, through use of a “smart decoder.”
As check blocks of degree one arrive, he can immediately
verify them against their corresponding leaf in the hash tree.
Similarly, whenever the decoder recovers an input symbol bj
from a check block hxi; cii of higher degree, the receiver
veriﬁes the recovered block bj against its hash. If the recovered
block veriﬁes properly, then the receiver concludes that hxi; cii
was generated honestly and hence is valid. If not, then the
receiver concludes that it is bogus.
In this process,
the decoder only XORs check blocks
with validated degree-one blocks. Consequently, valid blocks
cannot be corrupted during the decoding process. On the other
hand, invalid check blocks which are reduced to degree-one
blocks are easily identiﬁed and discarded. Using this “smart
decoder,” a receiver can trivially distinguish bogus from valid
check blocks and need not worry about the download cache
pollution described in [14]. The problem, however, is that a
vast majority of these block operations happen at the very
end of the decoding process—when almost n check blocks
are available to the decoder. Figure 2 exhibits the average