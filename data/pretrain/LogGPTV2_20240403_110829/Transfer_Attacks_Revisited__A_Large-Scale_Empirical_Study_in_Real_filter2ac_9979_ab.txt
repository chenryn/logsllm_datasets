with the same training data, which might inﬂuence the trans-
ferability of generated AEs.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1424
Table I: Conclusions comparison between lab settings and real settings.
Former conclusion in labs
Reﬁned conclusion in reality
All kind of transfer attacks discussed has a signiﬁcantly positive success
rate.
(1) Targeted algorithms are worse than untargeted algorithms in both
targeted and untargeted transfer attack. (2) Single-step algorithms are better
than iterative algorithms.
The effect of surrogate complexity, deﬁned by the number of layers and
parameters of the neural network, is non-monotonic. A surrogate with a
good depth can outperform simpler and deeper surrogates.
No dominant architecture is found to craft AEs that transfer better to cloud
models.
L2 norm of perturbation has far stronger correlation with transferability.
Increasing L2 norm while keeping L∞ ﬁxed makes good transferability
but large L∞ norm with small L2 norm can give poor success rate.
The gap between the posterior of logits is a better representative for
measuring transferability. A large κ cannot increase transferability in many
cases.
Consistency
Partial



Partial

Untargeted Attacks are easy to transfer and targeted at-
tacks almost never transfer. [29]
FGSM ≥ PGD ≥ CW in the sense of transferability. [41]
Surrogates with less complexity, deﬁned by smaller vari-
ability of the loss landscape, are better choices for transfer
attack. Experimentally, a surrogate with simpler structure
and stronger regularization is better. [15]
AEs crafted from VGG surrogates transfer well to all
targets, while others almost never transfer to targets in
different families. [41]
Relaxing L∞ norm perturbation constraint
creases transferability. [41]
largely in-
A larger logit gap between the adversarial class and the
second likely class, called κ value, increases untargeted
attack transferability. [12] [41]
(c) Model Complexity. For one speciﬁc architecture, models
with different complexities usually have different performance.
This phenomenon indicates that they have different capabilities
in capturing the distribution of the training data, thus affecting
the transferability of AE when used as surrogate models. In
this paper, since we are in the context of neural networks,
we deﬁne the model complexity by the number of layers
and parameters. On the contrary, Demontis et al. [15] deﬁne
the model complexity by the variability of the loss landscape
and the input gradient size. We experimentally conﬁrm their
conclusions on the input gradient size on local targets because
the gradients of the MLaaS models are non-transparent. For
more details, please refer to Appendix X-J. The variability of
the loss landscape is computationally expensive for the large
surrogates we use, thus we do not use this as the complexity
metric.
C. Surrogate Dataset
Surrogate dataset is the dataset used to train the surrogate
model. In general, the distribution of the surrogate data is
expected to approximate the distribution of the data used
to train the target. In this way, the similarity between the
surrogate and the target model is expected to increase so
that the AEs crafted on the surrogate are easier to transfer.
However, the surrogate datasets that an attacker possesses are
usually much smaller than those owned by the MLaaS systems.
To make the surrogate dataset capture the distribution of the
target model’s training data as well as possible, there are two
common data enrichment methods:
(a) Data Augmentation. Data augmentation is to apply
transformations on an image while maintaining the primary
patterns critical to the classiﬁcation. Applying augmentation
enforces a model to learn patterns under extended situations
and focus on the dominant patterns.
(b) Adversarial Training. Adversarial training is to enrich
the training data with their adversarial counterparts so that the
surrogate model can focus on robust features which are not
changeable by small perturbations [23], [45].
These methods allow the surrogate to learn better features
from the limited surrogate dataset.
D. Adversarial Algorithm
Adversarial algorithm is the white-box attack algorithm
(WBA) used to generate AEs on the surrogate model. Among
the WBAs, there are several common properties separating
them into different categories:
(a) Depending on the goal, WBAs can be categorized into
targeted and untargeted. Targeted algorithms aim to let the
model predict the target class, i.e., fθ(ˆx) = ct, where ct
denotes the target class. Untargeted algorithms merely want
the model to misclassify, i.e., fθ(ˆx) (cid:54)= co, where co denotes
the original predicted class.
(b) Depending on the optimization process, WBAs can be
categorized into single-step attacks which add perturbations
to the original input only once, and iterative attacks which
perturb the original input repeatedly until a certain condition
is satisﬁed.
III. THREAT MODEL
Figure 2 provides an overview of transfer attacks on MLaaS
platforms. The model structure and training dataset of the tar-
get platform model is obscure to an attacker. Additionally, an
attacker can only access the target platform model by sending
images to the MLaaS platform and getting their predictions via
the platform’s API. Therefore, an attacker can only manipulate
the input of the target platform model to perform an attack.
For ease of understanding, we formally present the following
attack-related deﬁnitions: (1) targeted AE is deﬁned as the AE
generated locally via targeted algorithms; (2) untargeted AE is
deﬁned as the AE generated locally via untargeted algorithms;
(3) targeted transfer attack is deﬁned as the targeted attack
against the MLaaS platforms using locally generated AEs; (4)
untargeted transfer attack is deﬁned as the untargeted attack
against the MLaaS platforms using locally generated AEs;
Following Chen et al. [14], we consider the case that
attackers maintain a pool of highly transferable AEs as well.
Under such condition, we are actually viewing transferability
as a property of AEs and every AE can be used to launch any
kind of transfer attack. It immediately leads to the following
two key differences: (1) Both targeted and untargeted AEs can
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1425
We apply the two data enlargement techniques discussed
in Section II-C. Speciﬁcally, for data augmentation, we use
color jitter, random afﬁne, random horizontal ﬂip, random
perspective, random rotation and random vertical ﬂip. They
are imposed on an image independently with a probability of
0.5. The detailed parameter settings can be found in Appendix
X-G. For adversarial training, we employ the naive adversarial
training (NAT) algorithm [23]. Thus, we have three variants
for each dataset: raw (the original dataset), augmented and
adversarial.
3) Adversarial Algorithm Settings:
In total, we employ
nine representative adversarial attacks. Among them there are
two targeted algorithms, BL-BFGS (simpliﬁed as BLB) [44]
and CW2 [12]. BLB and CW2 generate AEs by solving the
corresponding optimization problem using iterative methods.
Adversarial perturbations generated by them are optimized
in size and thus by nature small. Apart from these targeted
algorithms, we employ six untargeted adversarial algorithms
as well. DeepFool [32] is the only untargeted algorithm that
optimize the perturbation size, realized by approximating
the decision boundary. A generalized version of DeepFool
is the UAP algorithm [31], adding image-level adversarial
perturbations given by DeepFool together to obtain a universal
adversarial perturbation. Algorithms remained are built upon
another idea that AEs should be generated with Lp norm of
perturbation less than a predetermined norm budget. The root
algorithm is FGSM [19] which perturbs the original image
for once along the opposite direction of the gradient, i.e.,
ˆx = x −  · sign(∇loss(fθ(x); c), where loss(fθ(x); c) is the
loss function between the model prediction and the ground
truth, sign(·) denotes the sign function and  denotes the
perturbation budget. FGSM has its improved version RFGSM
[46] which adds a random perturbation to the image before
imposing gradient-based perturbation. This preprocessing is
claimed to penetrate the defense of gradient masking. In
addition, FGSM has an iterative counterpart PGD [30] which
executes the FGSM step iteratively for predetermined times.
Another variation of FGSM called Step-LLC and its itera-
tive counterpart LLC [24] minimize the loss w.r.t. the least
likely class rather than maximize the loss w.r.t. the correct
class. The perturbing process then becomes ˆx = x +  ·
sign(∇loss(fθ(x); c∗), where c∗ denotes the class the model
predicts with the least conﬁdence. These attacks have Lp norm
of adversarial perturbation less than or equal to the budget. We
use L∞ norm in the paper.
For algorithm implementations, we employ the codes from
the open source framework DEEPSEC [28]. We select ten
independent classes from the ImageNet and randomly sampled
20 images for each class as the seed image. For gender
classiﬁcation, we randomly select 200 original images as the
seed image, half male and half female. The combinations of
surrogate settings and attack algorithms form 180 different
settings in total for our evaluation. Under each setting, AEs
are generated on the same set of seed images and sent to
the target platform. Then metrics are computed based on the
responses.
Figure 2: Pipeline of transfer attack against MLaaS platforms.
Table II: Settings for each component of the surrogate model.
Component
Architecture
Complexity
Pretraining
Settings
Inception, VGG, ResNet
(ResNet Depth) 18, 34, 50
True, False
be used to perform targeted and untargeted transfer attacks. For
targeted transfer attacks, we take the predicted label of an AE
by the surrogate as the target label. (2) The correlation between
transferability and sample-level properties such as adversarial
conﬁdence and perturbation size can be easily examined.
IV. EVALUATION SETTINGS AND METRICS
A. Evaluation Settings
1) Surrogate Model Settings: For each surrogate model, we
evaluate multiple settings for each of its components discussed
in Section II-B. We use the implementations of models in
PyTorch [8] and TorchVision [9] libraries to conduct our
experiments. For different model complexities, we apply three
ResNet models with different depths: ResNet-18, ResNet-34
and ResNet-50. These depths are chosen because they are
ofﬁcially implemented. We apply Inception V3 [42], VGG-16
[40] and ResNet-18 [20] to study the impact of architecture.
For pretraining, we compare the raw ResNet models without
pretraining and the ResNet models pretrained on the whole
ImageNet dataset. For the object classiﬁcation task, we only
ﬁnetune the last fully connected layer and ﬁx other layers. For
the gender classiﬁcation task, we use the pretrained parameters
as an initialization and ﬁnetune the whole model. Empirical
result shows all pretrained surrogates have a signiﬁcant im-
provement on accuracy. Table II summarizes each component.
2) Surrogate Dataset Settings: For the image classiﬁcation
task, we use a subset of ImageNet [16] consisting of 10 classes.
Each class contains approximately 500 images cropped to
512×512×3. It is not an important reduction in the number of
samples because these classes have roughly 500-600 images
per class in the original ImageNet data. We do this because
the number of targets’ classes is less meaningful in attacking,
while the structure of the data is critical, as shown in Figure
1. Training the surrogate with a subset is to lower the cost and
have a better simulation of real attacks with limited resources.
Similarly, for gender classiﬁcation, we use a subset of the
Adience dataset [18]. We randomly pick 10,000 images from
the Adience dataset with half male and half female and crop
them to 384×384×3. For each of the two surrogate datasets,
we randomly split it into training set, validation set and test
set with the split ratio 8:1:1.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
1426
AdversarySurrogateModelSurrogateDatasetAttackAlgorithmAdversarialExamplesCloudModelResponsepreparegeneratesent totrainpredictevaluated by1234564) Cloud Experiment Settings: We conduct our experiments
on four leading commercial MLaaS platforms: Google Cloud
Vision [6], AWS Rekognition [2], Aliyun (Alibaba Cloud)
[1] and Baidu Cloud [4]. We send the AEs to these clouds
using the ofﬁcial API provided by each platform and save the
responses for evaluation.4
B. Evaluation Metrics
It is not trivial to determine whether a transfer attack is
successful in real settings for the multi-class classiﬁcation
tasks like ImageNet. Basically, a MLaaS platform’s response
to a certain input image can be denoted as P = {(l, s)|l ∈
L, s ∈ [0, 1]}, where l represents a label from the class set
L on the MLaaS platform and s denotes the conﬁdence score
for the prediction l. Consequently, two challenging problems
come up in matching a local class with the response from a
MLaaS platform:
(a) Label inconsistency problem: Since L is signiﬁcantly
different from the local class set C, l cannot be directly
associated with a local label c because l can either be a sub-
class or super-class of c. For example, the local class “weapon”
may correspond to classes in L that are sub-classes of it, such
as “gun” and “knife”, and the local class “baseball” may be
predicted as a super-class, such as “sport”. Additionally, L
differs across MLaaS platforms as well.
(b) Multiple predictions problem: While P consists of mul-
tiple predictions, the ground truth is a single label. Hence, it is
difﬁcult to fairly judge whether the MLaaS platform is correct,
since the expected predication might have a fairly high score
but is not a top-k label, as what we have seen in Figure 1b.
As a result, metrics like top-k accuracy can not be reasonably
applied because it only considers the prediction with the top
k scores.
To address the label inconsistency problem, we construct a
class mapping Mc = {lc|lc ∈ L} for each local class c and
each MLaaS platform. The content of the constructed mapping
can be found in the code repository. To construct Mc, we
inspect all the responses of a MLaaS platform to the original
image belonging to class c and manually select the relevant
classes from the responses. We then build an equivalence
dictionary T = {(c, Mc)|c ∈ C} for each MLaaS platform.
We discuss the validity and potential bias introduced by the
equivalence dictionary in Appendix X-A.
To address the multiple predictions problem, we select a
conﬁdence threshold σ for each platform to ﬁlter out the
predictions with low scores. The value of the threshold σ
is critical to the fairness of our evaluation and should be
meticulously chosen. Our principle is that the prediction ac-
curacy for clean images should not drop signiﬁcantly and low
conﬁdence predictions are ruled out as many as possible. To set
an appropriate threshold for each cloud platform, we measure
the prediction accuracies of the studied cloud platforms on the
original ImageNet data under different thresholds. As shown
4We exclude Google for gender classiﬁcation because it does not support
this function.
Figure 3: The accuracies of clouds with respect to different
conﬁdence thresholds.
in Figure 3, the threshold value largely affects the prediction
accuracy and the trend varies across platforms. We ﬁnd that
Google and AWS mainly set high scores for their responded
predictions, while Alibaba and Baidu attach low scores to
many responded predictions. Therefore, we use σ = 50% for
AWS and Google and σ = 10% for Alibaba and Baidu. The
difference in threshold mitigates the unfairness of setting a
global conﬁdence threshold to evaluate their robustness and
leaves the analysis of other factors, assumed to be indepen-
dent to the target model, unharmed. More discussions about
threshold cutting is provided in Appendix X-I.
Based on the discussions above, the matching between a
local class c and a response P from a MLaaS platform can
be formalized as: c match P ⇔ ∃(l, s) ∈ P, s > σ ∧ l ∈ Mc,
where Mc is the equivalence dictionary of c. For an AE with
an original label co but classiﬁed to be ct by the surrogate
model, we call it misclassiﬁed on the MLaaS platform if co
fails to match the response P and matched if ct matches
P . Note that since it is possible that both the correct label
and the target label are present in the response, a matched
AE is not necessarily misclassiﬁed. Therefore, there is no
strict relationship between the misclassiﬁcation rate and the
matching rate.
Following the extended deﬁnition of misclassiﬁed and
matched AEs, We further introduce two metrics for evaluating
the effectiveness of transfer attacks, which is aligned to the
previous studies in the lab settings. In short, transfer attack
aims to ﬁnd “similar mistakes” and what we do is to redeﬁne
what a mistake is. Note that the class mapping technique and
threshold cutting are used to evaluate the transfer success rate
rather than launching attacks.
Deﬁnition 1. We deﬁne misclassiﬁcation rate to be the number
of AEs that are misclassiﬁed on the MLaaS system divided
by the number of AEs sent to the MLaaS system. Similarly,
matching rate is deﬁned to be the number of AEs that are
matched on the MLaaS system divided by the number of AEs
sent to the MLaaS system.