extends Arnold so that it supports system-wide recording, which
accounts for every process execution in user-space. As a result,
Rain can replay any part of the system’s execution on-demand for
selective DIFT without losing completeness.
To replay the execution faithfully, Arnold records the return
value of system calls, IPC communications such as signal and sys-
tem V queues, and caches the data for every file or network I/O
system call. For multi-threading applications, the pthread library
in libc is hooked to record and enforce the order of the thread
switch. To handle shared memory, Arnold replays involved pro-
cesses cooperatively to regenerate shared data, and to improve the
replay reliability. It also records and replays random numbers as
well as RDTSC by using prctl(PR_SET_TSC,PR_TSC_SIGSEGV)
from [5].
Rain aims to detect and analyze attacks that may have previously
gone undetected, so instead of being confined to a predefined list of
known programs, Arnold’s process level record-replay technique
has been extended from a single process to system-wide executions.
Although Rain does not replay all of the recorded executions for
refining attack provenance owing to the reachability analysis (§6)
and selective DIFT (§7), this system-wide feature is critical as it
enables us to refine any demanded part of execution. We achieve
this by hooking the execve syscall inside of the kernel so that when
a program is loaded via execve, we force it to become a recording
process by creating the recording contexts. Throughout the analysis
Rain can replay each demanded execution independently to resolve
the associated fine-grained causality.
4.3 Storage Footprint
The storage use of Rain comes from system logging and recording.
We serialize the logging data using the Apache Avro [1] binary
format, which incurs around 500MB–2GB per day for desktop use
in our experiments. On the side of recording, the file, network I/O
cache constitutes most of the storage cost. To optimize storage use,
Arnold [19] applies compression, caches the data using “Copy-on-
RAW”, and manages the data pieces in a B-tree. In our experiment,
the record logs of system-wide executions (excluding the OS) on
a desktop produce around 2GB of storage per day. Therefore, the
storage cost is 2.5–4GB per day (or less than 1.5TB per year). With
the market price for a 2TB hard drive or cloud storage being around
50 US dollars, we believe that our storage cost is both reasonable
and affordable. Note that instead of selectively storing data [34],
we choose to store all of the raw data first and then generate the
provenance graph selectively, following a set of pruning algorithms
(§6). Our storage footprint reflects the size of raw data.
5 PROVENANCE GRAPH
We construct a graph structure called Provenance Graph which
contains the whole-system execution during the entire period of
monitoring. Rain uses this graph as the basic model to represent
system objects, events, and their causal relationships. We begin by
processing the syscall logs. When first constructed from system
Causality
Process-Object
Object-Process
Process-Process
Object-Object
Table 1: Granularity of analysis.
Granuality
Coarse level
Coarse + Fine level
Coarse + Fine level
Fine level
logging, the graph is coarse-grained. It is then pruned and refined
incrementally according to analysis requests. We use nodes to rep-
resent system objects and edges to represent causality between
system objects.
5.1 Nodes
Nodes in the provenance graph can be classified into two categories:
processes and objects. Processes represent user-level processes.
Objects represent files and network endpoints. All nodes contain a
timestamp that represents the time that the entity (represented by
the node) was generated by the operating system. Each process node
contains pid, tid, and process name. Each File node contains the
full path name of the file and inode, major, minor, and gen which
uniquely identify a file. Nodes representing network endpoints
contain the IP address and port of the network entity. Note that the
tracking scope of Rain does not extend beyond the target host. In
other words, we treat the remote host as a black box and do not
track its internal logic or state. This limitation can be addressed
if we apply Rain on the remote host and causally relate the two
hosts, but we will explore this issue in future work.
5.2 Causality Edges
Edges represent causal relationships between nodes in the prove-
nance graph. We define four types of edges: process-object causal-
ity, object-object causality, object-process causality, and process-
process causality. Among these causalities, we observe that only
the process-object causality can be tracked reliably by syscall-level
logging. The remaining causalities require either full or partial
fine-grained user-space tracking. We summarize these granularity
requirements in Table 1.
Process-Object Causality. Process-object causality, which de-
notes the causal relationship between a process and an object, is
established when a user-level program accesses a file or network
object. As the operating system provides these I/O services to user-
space, this causality can be captured by syscall logging (i.e., file or
network I/O) without false negatives. For example, the data can be
loaded from a file to the memory of process via a read syscall or
written to a file from the process via a write syscall. In the case of
a read syscall, the process has a directional edge to a file; and the
case of a write syscall, a file node has an edge to a process. Similar
causalities exist between processes and network endpoints. In the
case of a mmap syscall, the direction of causality is determined by
the syscall arguments such as PROT and FLAG.
Object-Process Causality. Object-process causality is established
when the object affects the execution of a process or its control
flow. Usually the process has an edge from an executable file if
that file is loaded and executed by the process. Rain captures this
causality by monitoring the execve syscall. The use of libraries is
typically tracked by open, mmap, or dlopen syscalls. However, this
causality may not be true by just analyzing syscalls. For example,
the developer may include libraries but not use them. To affirm
causality between processes and libraries, we need to further track
the control flow to see if its value (address) exists within a library’s
address space such as that described in [41]. Particularly for so-
phisticated attacks, an accurate object-process causality is crucial
for detecting control-flow hijacking and identifying the sources of
exploit payloads that could be used to access library functions. We
accurately determine this type of causality during the DIFT phase.
Object-Object Causality. Object-object causality occurs in the
case of data flow between two objects. The data flow inside of a pro-
cess starts from an inbound object (e.g., via read syscall) and ends
at an outbound object (e.g., via write syscall). One can infer this
type of causality by simply pairing inbound and outbound objects.
However, simply monitoring file or network I/O syscalls (e.g., our
motivating example in §1) or statically analyzing on the program
is inaccurate because we need to track the data propagation in the
user-level execution. As the dynamic taint analysis is prohibitively
expensive, Rain tracks the Object-object causalities during replay
(§7).
Process-Process Causality. Process-process causality is based on
the relationships between two processes. Processes can be causally
related if one is cloned by the other via the clone syscall, or they
could be related because of inter-process communications (IPC).
Some IPCs (e.g., pipe, message queues, and semaphores) can be
observed from syscalls. However, causality cannot be accurately
determined in the case of shared memory. Even though a mmap or
shmget syscall indicates the establishment of an IPC channel, it
does not necessarily mean data was actually exchanged between
processes. To track such causality, Rain relies on DIFT to monitor
data propagation among memory operations.
5.3 Graph Construction
We construct the provenance graph by linking nodes and edges
according to the above causality definitions. Our construction is
based on uniquely identifiable objects. Since pid and inode are
recycled if a process is terminated or file is deleted respectively,
we use path as a unique identifier of processes and files (as nodes)
since the collision of these objects with the same name is low in
practice. After being processed from system call logs, each entry
of a node or an edge is compressed and stored in a binary format.
When requested for analysis, those within in the time frame are
converted and imported into a graph database. In particular, we
used Neo4j [6].
Semantic-Preserving Aggregation. Naturally, edges from I/O
events such as read and write constitute a large portion of the
graph. However, many are called successively indicating a single
“large” read or write syscall execution. Therefore, we aggregate
these successive calls for conciseness as inspired by [52]. For ex-
ample, we merge two read syscalls as long as no other file system
call occurs between them (e.g., a “write” to the same file). Note that
we collapse these successive syscalls in an “indexing” style so that
we do not remove the unique semantics of each individual edge.
Thus, the selective DIFT still has the flexibility to perform taint
tracking between desired I/O syscalls. The aggregation alleviates
the traversing and storage costs of edges by 10%–50%.
6 COARSE-LEVEL PRUNING
After constructing the coarse-grained provenance graph, we prune
it to generate a security-sensitive provenance subgraph (i.e., SPS) in
two steps: a triggering analysis and a reachability analysis that uses
the results of the triggering analysis. The SPS will be used as the
target for selective DIFT, in which fine-grained causalities will be
resolved.
6.1 Triggering Pinpoint
In the initial provenance analysis, we apply a set of methods to
scan the logs and identify suspicious (i.e., security-sensitive) pro-
cesses, events, and objects. The triggering analysis relies on three
approaches: external signals, security policies, and customized com-
parisons. We perform the analysis offline by examining the prove-
nance graph. This process can be done earlier when system call
logs are available as with conventional intrusion detection systems
[7–9].
External Signals. External signals are notifications from partners
or third parties (e.g., an anti-virus company). For example, an ana-
lyst may receive advice from an anti-virus vendor to specifically
check for the existence of certain executable files. All events per-
formed by these executables can be labeled as triggering points. In
our motivating example, the victim receives a notice that the distri-
bution site of the FTP extension was compromised. This triggers an
analysis of all behaviors of the browser starting from the malicious
extension update.
Security Policy. Security policy checking also serves as a trigger-
ing pinpoint method. Based on administrative security policies, we
create a set of policies that define concerning events used as trigger-
ing points for analysis. These policies include processes interacting
with sensitive files or a sequence of events that deviates from the
typical pattern of system calls. For example, it is a violation that a
process reads from certain sensitive files and then sends read data
to an unknown remote host. Recent research has shown that the
detection of attacks based on system-call sequence analysis can be
improved with machine-learning techniques [35].
Customized Comparisons. We sometimes need to compare the
states of objects at different times or locations to identify suspicious
points. Take the data-tampering case in our motivating example. In
order to identify files that have been tampered with, we compare
the files (e.g., by comparing their hash digests) that have been down-
loaded via the browser extension to the original version of these
files. If they differ, the tampered files are used as triggering points.
This type of comparison typically requires application-specific se-
mantics which are useful but not the focus of this work. For exam-
ple, Gyrus [26] compares user interface (UI) inputs and network
outbound traffic to determine user intention discrepancies.
6.2 Reachability Analysis
Starting from the identified triggers, we perform a reachability
analysis to become aware of the potential original source(s) and
impacts. This analysis prunes out unrelated executions and enables
Algorithm 1 Coarse Level Pruning Interface
function UpStrmPru(TPoint)
GTraverse(TPoint,UP)
function DownStrmPru(TPoint)
GTraverse(TPoint,DOWN)
function PtP(TPoint_1,TPoint_2)
GTraverse(TPoint_1,DOWN)
GTraverse(TPoint_2,UP)
Mnodes ← FindMnodes(up_nodes,down_nodes)
RecoverPaths(Mnodes)
Algorithm 2 Information Flow Based Graph Pruning
Require: up_nodes, down_nodes
function regist_interference(ne,na)
if ne ∈ {read,recv} then
if ne.timestamp ≤ na.timestamp then
add_interference(ne,nd)
else if ne ∈ {write,send} then
add_interference(ne,nd)
if ne.timestamp ≥ na.timestamp then
function GTraverse(TPoint,Direction)
ngb_nodes ← read_graph(TPoint.uid)
for node ∈ ngb_nodes do
if Direction = UP then
if TPoint.type = Process then
if node (cid:60) {read,recv} then
continue
else if TPoint.type ∈ {File,Host} then
if node (cid:60) {write,send} then
continue
up_nodes(node) ← TPoint
regist_interference(node,TPoint)
GTraverse(node,UP)
else if Direction = DOWN then
if TPoint.type = Process then
if node (cid:60) {write,send} then
continue
else if TPoint.type ∈ {File,Host} then
continue
if node (cid:60) {read,recv} then
down_nodes(node) ← TPoint
regist_interference(TPoint,node)
GTraverse(node,DOWN)
point until the later point timestamp. Second, from the later point
we perform upstream pruning until the timestamp of the early
point. Then we inspect the two resulting subgraphs to identify the
intersection set, called meeting nodes (FindMnodes() in Algorithm
2). Along with pruning, each point maintains the tags of its parent
and ancestor nodes. Finally, we use the tags of meeting nodes to
construct the full paths (i.e., SPS) (RecoverPaths() in Algorithm 2).
The remaining causalities along the paths will be captured by the
selective DIFT.
Figure 3: Coarse-level pruning and fine-level refinement on the mo-
tivating example. The gray shaded node indicates that process P2 is
pruned from the SPS because of the negative interference between
files B and D. The red-shaded nodes represent files on the causality
path from origin F to attacker site A via file C.
our DIFT to focus on resolving fine-grained causality in the attack-
related executions. Although our DIFT is performed offline, the high
cost of DIFT is not eliminated but migrated which is also pointed
out by [27]. Hence we argue that it is still impractical to perform
full taint tracking, even if it is performed offline. With reachability
analysis, we set boundaries on the DIFT, avoiding tainting “dead”
branches or regions of the graph.
The reachability analysis extends the triggering points to un-
cover possible upstream origins, downstream impacts, and causality
paths between two points. Even though at this stage the graph in-
cludes only partial causalities (§5.2), computing the SPS on top of
it and pinpointing the part that desires further DIFT is sufficient
for capturing the the remaining causalities. With the SPS we can
efficiently perform DIFT with a clear scope rather than the whole
graph. We present the analysis interface in Algorithm 1 and the
graph traversing algorithms in Algorithm 2.
6.2.1 Upstream and Downstream Pruning. Upstream pruning
reversely scans the provenance graph from the triggering point
and prunes out unrelated nodes and edges. The analysis follows
the information flow and time sequence to extend the subgraph
such that Subject → (write/send ) → Object → (read/recv) →
Subject. For example, in Figure 3, from the attacker’s site (node “A”),
we scan the send or write events to A; after finding Firefox (node
“P1”), we further scan read or recv events that P1 performs earlier.
For the shared memory case in process-process causality, we also
scan for the syscall events that establish the IPC (e.g., shmget and