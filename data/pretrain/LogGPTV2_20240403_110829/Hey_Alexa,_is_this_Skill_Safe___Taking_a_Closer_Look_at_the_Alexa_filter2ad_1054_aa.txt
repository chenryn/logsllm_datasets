title:Hey Alexa, is this Skill Safe?: Taking a Closer Look at the Alexa
Skill Ecosystem
author:Christopher Lentzsch and
Sheel Jayesh Shah and
Benjamin Andow and
Martin Degeling and
Anupam Das and
William Enck
Hey Alexa, is this Skill Safe?: Taking a Closer Look
at the Alexa Skill Ecosystem
Christopher Lentzsch∗, Sheel Jayesh Shah†, Benjamin Andow‡§, Martin Degeling∗, Anupam Das† and William Enck†
∗ Ruhr-Universit¨at Bochum; {cl-imtm, martin.degeling}@ruhr-uni-bochum.de
† North Carolina State University; {sshah28, anupam.das, whenck}@ncsu.edu
‡ Google Inc.; PI:EMAIL
Abstract—Amazon’s voice-based assistant, Alexa, enables
users to directly interact with various web services through
natural language dialogues. It provides developers with the option
to create third-party applications (known as Skills) to run on top
of Alexa. While such applications ease users’ interaction with
smart devices and bolster a number of additional services, they
also raise security and privacy concerns due to the personal
setting they operate in. This paper aims to perform a systematic
analysis of the Alexa skill ecosystem. We perform the ﬁrst large-
scale analysis of Alexa skills, obtained from seven different skill
stores totaling to 90,194 unique skills. Our analysis reveals several
limitations that exist in the current skill vetting process. We
show that not only can a malicious user publish a skill under
any arbitrary developer/company name, but she can also make
backend code changes after approval to coax users into revealing
unwanted information. We, next, formalize the different skill-
squatting techniques and evaluate the efﬁcacy of such techniques.
We ﬁnd that while certain approaches are more favorable than
others, there is no substantial abuse of skill squatting in the
real world. Lastly, we study the prevalence of privacy policies
across different categories of skill, and more importantly the
policy content of skills that use the Alexa permission model
to access sensitive user data. We ﬁnd that around 23.3 % of
such skills do not fully disclose the data types associated with
the permissions requested. We conclude by providing some
suggestions for strengthening the overall ecosystem, and thereby
enhance transparency for end-users.
I.
INTRODUCTION
Voice-based computer interaction thrives on the ability to
enable users to interact with devices and services through
voice instead of keystrokes, mouse-movement or swipes. While
speech recognition has been an active ﬁeld of research for
many years, it has seen widespread adoption recently. As a re-
sult there has been a rapid growth of voice-based web services
such as Amazon Alexa [10]. Market research estimates that
3.25 billion devices with voice assistants are active today [32].
Amazon Alexa takes this opportunity to provide voice-
based service as a platform and is the market leader in this
area [30]. Developers can deploy applications that interact
§ This work was completed when the author was at IBM Research.
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2021 
21-25  February  2021, Virtual
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.23111
www.ndss-symposium.org
and provide functionality to end-users through Alexa enabled
devices such as the Amazon Echo [11]. Such voice-based
applications are called skills and are essentially apps that
run on top of Amazon Alexa. Given that Amazon Echos
are marketed for use at home and their microphones are
continuously on, using voice-based third-party applications
raise privacy concerns. Research shows that participants feel
uncomfortable knowing that information from their private
home has been shared or disclosed to third parties [40], [16],
[36]. Moreover, recent studies continue to show increasingly
sophisticated attacks on automated speech recognition sys-
tems [46], [20], [21] and on Alexa skills [56]. When Alexa
integrates with other smart home IoT devices such as smart
locks or smart cars,1 security implications arise. An attacker
can potentially expand her attack vector by deceiving a user
to simply invoke skills that sound very similar to authentic
skills. For example, ‘lincoln way’ (real skill) and ‘lincoln
weigh’ (ﬁctitious malicious skill) sound identical, but can
potentially trick Alexa into activating the wrong skill and
thereby enable the attacker to unlock a user’s car. With Alexa’s
current policy of automatically enabling skills that match an
invocation phrase, an adversary can potentially increase her
odds of launching successful attacks.
Given the widespread adoption of Alexa and the potential
for malicious actors to misuse skills, the goal of this paper is
to perform a systematic analysis of the Alexa skill ecosystem
and identify potential
loopholes that can be exploited by
malicious actors. In particular, we seek to answer the following
broad research questions: RQ1: What limitations exist in the
current skill vetting process? For this we thoroughly analyze
the various steps involved in registering a skill, and identify
potential ﬂaws in the overall system. RQ2: How effective are
skill squatting attacks? To address this question, we not only
scan the skill stores to identify skills with phonetically similar
invocation names, but also propose a semi-automated approach
to test which skills Alexa actually activate when presented
with potentially squatted skills. RQ3: Is the requirement of
providing a privacy policy link effective? Alexa mandates a
privacy policy link for skills that request certain permission
APIs. We study the prevalence of privacy policies in different
skill stores and analyze whether privacy policy links actually
serve their purpose of informing users of their data practices.
In this paper, we perform a large-scale analysis of skills
1 Example of a skill that interacts with cars: https://amazon.com/Alexa-
Skills-Smart-Home/b?ie=UTF8&node=14284863011, and with locks: https:
//amazon.com/Alexa-Skills-Smart-Home/b?ie=UTF8&node=14284863011
collected across seven different stores and thoroughly study the
whole skill ecosystem. We observe that a malicious actor can
easily obtain sensitive information that is typically protected
through a permission model by explicitly requesting such
information from end users through the voice interface. We
also see that an attacker can make stealthy changes to the
backend code to coax a user into revealing information that is
never invoked during the certiﬁcation process. Interestingly, we
also see that an attacker can register skills using well-known
developer names, something that can further help an adversary
to launch phishing attacks. Next, we ﬁnd some evidence of
skill squatting attempts, but in most cases such attempts are
intentional and not malicious in nature, where the developer
squats her own skills to improve the chance of the skills getting
activated by Alexa. Lastly, we see that only a small portion
of the skills actually link a privacy policy, and this situation
does not improve even for skills under the ‘kids’ and ‘health’
categories, which often draw more attention under existing
regulations such as COPPA [1], CCPA [48] and GDPR [2].
In summary, we make the following contributions:
• We perform the ﬁrst large-scale analysis of Alexa skills
across seven skills stores (US, UK, AU, CA, DE, JP, FR).
We make our data available to the research community for
further analysis. (§IV)
• We thoroughly analyze Amazon’s skill certiﬁcation pro-
cess, and identify several potential
loopholes that can
be exploited by a malicious actor to publish deceptive
skills. We also suggest guidelines for tightening up such
loopholes. (§V)
• We identify common techniques used to squat skills, in-
cluding one technique previously not discussed. We also
design a semi-automated approach to gauge the effective-
ness of various skill squatting techniques. We ﬁnd that
while some approaches are more successful than others,
there is no substantial malicious abuse in the wild, and at
times we see a developer squat her own skills to improve
coverage. (§VI)
• Lastly, we analyze the privacy policy content of skills.
On average only 24.2% of all skills provide a privacy
policy link and skills in the ‘kids’ category are one of the
biggest offenders. When contrasting skill permissions with
privacy policies we ﬁnd that 23.3% of the policies do not
properly address the requested data types associated with
the corresponding permissions. (§VII)
The remainder of this paper proceeds as follows. Section II
provides background on Alexa skills and Amazon’s skill certi-
ﬁcation process. Section III describes related work. Section IV
describes datasets. Section V investigates skill vetting process
(RQ1). Section VI investigates skill squatting (RQ2). Sec-
tion VII studies privacy policies (RQ3). Section VIII discusses
our recommendations. We conclude in Section IX.
II. BACKGROUND
A. Building an Alexa Skill
Amazon opened Alexa to third-party developers in June,
2015 [44] to create an ecosystem similar to apps on mobile
devices. There are two types of Alexa skills: native skills,
developed and maintained by Amazon; and custom skills cre-
ated by third-party developers. Custom skills must meet certain
Fig. 1: Interactive workﬂow of an Amazon Alexa skill.
requirements and undergo an approval process. Figure 1 shows
the overall data ﬂow when using a skill. When a user speaks
to an Alexa-enabled device, the audio is streamed to the Alexa
web service. There, speech recognition and natural language
processing techniques are used to identify phrases that match
known skills published through the Alexa developer console.2
Next, a structured JSON request is created and sent to a back-
end server registered with the matching skill (either hosted
in AWS or on some external server). The server processes
the request and responds accordingly. All speech recognition
and conversion is handled by Alexa in the cloud [13], skills
do not get access to raw audio data. Responses from skills
are parsed by Alexa and are rendered using the same voice
template for all skills. Every Alexa skill has an “interaction
model” deﬁning the words and phrases that users can utter to
interact with the skill. This interaction model is analogous to a
graphical user interface, where instead of clicking buttons and
selecting options from dialog boxes, users make their requests
and respond to questions by voice. The interaction model is
deﬁned when creating a custom skill. Following are elements
required to build a custom skill [12]:
• An invocation name that identiﬁes the skill. This name is
used to initiate a conversation with the skill. Invocation
names are not required to be globally unique. Alexa
provides guidelines for selecting invocation names [7].
• A set of intents representing actions that users can invoke
through the skill. An intent represents an action (triggering
a backend handler) that fulﬁlls a user’s spoken request.
For example, AMAZON.HelpIntent handles necessary
actions when the user utters ‘help’.
• A set of sample utterances that specify the words and
phrases users can use to invoke the desired intents. These
utterances are mapped to intents and this mapping forms
the interaction model for the skill.
• A cloud-based service that accepts structured requests (i.e.,
intents in JSON format) and then acts upon them. This
cloud-based service must be accessible over the Internet
and deﬁned as an endpoint when conﬁguring the skill.
• A conﬁguration that brings all of the above together,
so Alexa can route requests to the desired skill. This
conﬁguration is created through the developer console [9].
Skill developers only have limited access to user data.
2See https://developer.amazon.com/alexa/console
2
UserEcho deviceAlexa SKill InterfaceBackend Skill Service"Alexa, open daily quotes"Forward the recorded audio streamOnIntentDailyQuoteIntentDailyQuoteIntentFunctionResponseOutput Speech: "Enjoy the little things""Here is your quote for today: Enjoy the little things"Resolve spoken words to intentsForward response audio streamPermissionIf permissions are required invoke through companion appAs described above, requests are only forwarded to skills if
they match the interaction model. Importantly, utterances that
enables a skill, but are followed by information that does not
match any predeﬁned intent, are not forwarded. However, an
adversary is capable of registering dormant intents to exﬁltrate
sensitive data, more details are provided in Section V-D. Users
are also not directly identiﬁable as Amazon masks requests
with identiﬁers that stay consistent for each skill, but across
different skills the same user is assigned different identiﬁers.
B. Skill Certiﬁcation Process
The Alexa developer console enables developers to test and
submit their skills for veriﬁcation before they are made public
to end users. Once a skill is submitted for distribution, Amazon
validates certain requirements. These certiﬁcation requirements
typically include [15]:
• Ensuring the skill meets the Alexa policy guidelines, which
among many things includes making sure invocation names
do not infringe existing brand names without providing
proper afﬁliation.
• Performing all required voice interface and user experience
tests, which include reviewing the intent schema and the set
of sample utterances to ensure they are correct, complete,
and adhere to voice design best practices.
• Performing all required functional tests, which includes
checking whether the skill’s basic functionality matches
the information provided on the skill’s description ﬁeld.
• Ensuring the privacy policy link is a valid link. A privacy
policy link is required if the skill requests access to
sensitive data through the permission model.
• Ensuring the skill meets the security requirements for
hosting services at external servers (i.e., non AWS Lambda
servers), which includes checking whether the server re-
sponds to requests not signed by an Amazon-approved
certiﬁcate authority.
Once a skill successfully passes all the validation steps, it
ofﬁcially appears in the skill store. Any changes made to the
skill conﬁguration and interaction model after the veriﬁcation
step will require the developer to re-initiate the whole veriﬁca-
tion process. However, modiﬁcations to backend code change
does not trigger re-veriﬁcation (this can be exploited by an
attacker as discussed more in Section V-C).
III. RELATED WORK
Attacks on speech recognition systems. As voice-based
smart assistants have become more popular, we have also
seen new attacks emerge against automated speech recognition
systems (ASR). Several researchers have been successful in
developing adversarial examples to trick voice-based inter-
faces. Carlini et al. [20] demonstrated how input audio can be
synthesized in a way that it is unintelligible to humans, but are
interpreted as commands by devices. In a followup study Car-
lini et al. [21] formalized a technique for constructing adver-
sarial audio against Mozilla DeepSpeech with 100 % success
rate. Vaidya et al. [51] were similarly successful in changing
the input signal to ﬁt a target transcription. More recently,
Yuan et al. [53] showed that such hidden voice commands
can be easily embedded into songs without being noticed by a
human listener. Psychoacoustic models have also been used to
manipulate acoustic signals such that it becomes imperceptible
to humans [46]. Abdullah et al. [5] were able to exploit
knowledge of the signal processing algorithms commonly used
by voice processing systems (VPSecs) to successfully generate
hidden voice commands. Furthermore, a series of independent
studies have shown that it is possible to launch inaudible
voice attacks by modulating hidden commands on ultrasound
carriers [54], [47], [43]. However, attacks are mostly limited
to lab settings and rarely work over the air, instead attacks
are evaluated by directly feeding audio samples into the ASR
models.
Attacks on skills. Edelman et al. [27] were the ﬁrst to
ﬁnd thousands of domains with minor typographical varia-
tions on well-known web sites, a practice commonly known
as “typosquatting”. Their ﬁndings inspired a series of re-
search towards measuring and mitigating the domain squatting
threat [31], [42], [6], [50], [33]. Similarly, voice-squatting
attacks have also been shown to be feasible with Alexa skills.
Kumar et al. [35] ﬁrst showed that skill squatting attacks
can be launched when the invocation name of two different
skills are pronounced similarly. Zhang et al. [55] recently
introduced a new variant of the skill squatting attack where
an attacker can use a paraphrased invocation name to hijack
legitimate skills. This attack is based on the observation that
Alexa favors the longest matching skill name when processing
voice commands. In another concurrent work, Zhang et al. [56]
design a linguistic-model-guided fuzzing tool to systematically
discover the semantic inconsistencies in Alexa skills. They
state that the developer controlled backend can be abused
by the developer, for example by swapping legitimate audio
ﬁles with malicious audio ﬁles. However, they do not provide
details or demonstrate how this can be achieved.
Prevalence of privacy policy.
In addition to the technical
attack vectors to exﬁltrate user data or execute commands on
their behalf, there is also the possibility that skills themselves
can try to trick users into exposing sensitive data. Legal
regulations require companies to provide information to users
about how they process personal data and for what purposes.
Privacy policies have become the most important source for
obtaining information about data practices. The importance
of privacy policies for compliance with legal requirements
has increased since the introduction of the European Union’s
General Data Protection Regulation (GDPR) [2]. A recent
study by Degeling et al. [25] showed that the prevalence of
privacy policies has increased to 85 % for websites, not limited
to the European Union alone. However, several studies have
shown the inconsistencies between what privacy policies state
and what data is accessed [17]. For example, Libert [38] found
that only 15 % of the information ﬂowing from websites to
third parties such as tracking and analytic services, is disclosed
in the websites’ privacy policies. Earlier, Zimmeck et al. [58]
showed that 48 % of apps available in the Google Play store did