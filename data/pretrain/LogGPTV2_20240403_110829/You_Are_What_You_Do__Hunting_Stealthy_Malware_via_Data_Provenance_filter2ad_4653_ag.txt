Information Processing, vol. 27, pp. 297–314, 2019.
[63] S. T. King and P. M. Chen, “Backtracking intrusions,” in SOSP ’03.
ACM, 2003.
[64] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.
[65] B. Kolosnjaji, A. Zarras, G. Webster, and C. Eckert, “Deep learning for
classiﬁcation of malware system call sequences,” in Australasian Joint
Conference on Artiﬁcial Intelligence. Springer, 2016, pp. 137–149.
[66] D. Korczynski and H. Yin, “Capturing malware propagations with
code injections and code-reuse attacks,” in Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communications Security,
2017, pp. 1691–1708.
[67] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna, “Automat-
ing mimicry attacks using static binary analysis,” in USENIX Security
Symposium, vol. 14, 2005.
[68] Y. Kwon, F. Wang, W. Wang, K. H. Lee, W.-C. Lee, S. Ma, X. Zhang,
D. Xu, S. Jha, G. F. Ciocarlie et al., “Mci: Modeling-based causality
inference in audit logging for attack investigation.” in NDSS, 2018.
[69] Q. Le and T. Mikolov, “Distributed representations of sentences and
documents,” in International conference on machine learning, 2014,
pp. 1188–1196.
[70] K. H. Lee, X. Zhang, and D. Xu, “High Accuracy Attack Provenance
via Binary-based Execution Partition,” in NDSS, 2013.
[71] ——, “LogGC: Garbage collecting audit log,” in CCS. New York,
NY, USA: ACM, 2013, pp. 1005–1016.
[72] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 2008
IEEE, 2008,
Eighth IEEE International Conference on Data Mining.
pp. 413–422.
[73] Y. Liu, M. Zhang, D. Li, K. Jee, Z. Li, Z. Wu, J. Rhee, and P. Mittal,
“Towards a timely causality analysis for enterprise security,” in NDSS,
2018.
[74] S. Ma, X. Zhang, and D. Xu, “ProTracer: Towards Practical Prove-
nance Tracing by Alternating Between Logging and Tainting,” in
NDSS, 2016.
[75] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal
of machine learning research, vol. 9, no. Nov, pp. 2579–2605, 2008.
[76] F. Maggi, M. Matteucci, and S. Zanero, “Detecting intrusions through
system call sequence and argument analysis,” IEEE Transactions on
Dependable and Secure Computing, vol. 7, no. 4, pp. 381–395, 2008.
[77] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composi-
tionality,” in Advances in neural information processing systems, 2013,
pp. 3111–3119.
[78] M. Mimura and H. Tanaka, “A linguistic approach towards intrusion
detection in actual proxy logs,” in International Conference on Infor-
mation and Communications Security. Springer, 2018, pp. 708–718.
[79] A. Narayanan, M. Chandramohan, R. Venkatesan, L. Chen, Y. Liu,
and S. Jaiswal, “graph2vec: Learning distributed representations of
graphs,” arXiv preprint arXiv:1707.05005, 2017.
[80] D. Nguyen, J. Park, and R. Sandhu, “Adopting provenance-based
access control in openstack cloud iaas,” in International Conference
on Network and System Security. Springer, 2014, pp. 15–27.
[81] S. Omar, A. Ngadi, and H. H. Jebur, “Machine learning techniques for
anomaly detection: an overview,” International Journal of Computer
Applications, vol. 79, no. 2, 2013.
[82] R. Paccagnella, P. Datta, W. U. Hassan, C. W. Fletcher, A. Bates,
A. Miller, and D. Tian, “Custos: Practical tamper-evident auditing of
operating systems using trusted execution,” in Proc. of the Symposium
on Network and Distributed System Security (NDSS), 2020.
[83] K. Padmanabhan, Z. Chen, S. Lakshminarasimhan, S. S. Ramaswamy,
and B. T. Richardson, “Graph-based anomaly detection,” Practical
Graph Mining with R (2013), vol. 311, 2013.
[84] M. Pagliardini, P. Gupta, and M. Jaggi, “Unsupervised learning of
sentence embeddings using compositional n-gram features,” arXiv
preprint arXiv:1703.02507, 2017.
[85] C. Parampalli, R. Sekar, and R. Johnson, “A practical mimicry
attack against powerful system-call monitors,” in Proceedings of the
2008 ACM symposium on Information, computer and communications
security. ACM, 2008.
J. Park, D. Nguyen, and R. Sandhu, “A provenance-based access
control model,” in Privacy, Security and Trust (PST), 2012 Tenth
Annual International Conference on.
IEEE, 2012, pp. 137–144.
[86]
[87] G. P´ek, Z. L´az´ar, Z. V´arnagy, M. F´elegyh´azi, and L. Butty´an, “Mem-
brane: a posteriori detection of malicious code loading by memory
paging analysis,” in European Symposium on Research in Computer
Security. Springer, 2016, pp. 199–216.
[88] P. Perera and V. M. Patel, “Learning deep features for one-class
classiﬁcation,” IEEE Transactions on Image Processing, 2019.
[89] D. Pohly, S. McLaughlin, P. McDaniel, and K. Butler, “Hi-Fi: Col-
lecting High-Fidelity Whole-System Provenance,” in ACSAC, Orlando,
FL, USA, 2012.
[90] T. M. Research, “2019 Midyear Security Roundup: Evasive Threats,
Pervasive Effects,” Tech. Rep., Sep. 2019.
[91] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should I trust
you?”: Explaining the predictions of any classiﬁer,” in Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge
15
Discovery and Data Mining, San Francisco, CA, USA, August 13-17,
2016, 2016, pp. 1135–1144.
[92] A.-D. Schmidt, R. Bye, H.-G. Schmidt, J. Clausen, O. Kiraz, K. A.
Yuksel, S. A. Camtepe, and S. Albayrak, “Static analysis of executa-
bles for collaborative malware detection on android,” in 2009 IEEE
International Conference on Communications.
IEEE, 2009, pp. 1–5.
[93] B. Sch¨olkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and
J. C. Platt, “Support vector method for novelty detection,” in Advances
in neural information processing systems, 2000, pp. 582–588.
[94] M. Sebasti´an, R. Rivera, P. Kotzias, and J. Caballero, “Avclass: A
tool for massive malware labeling,” in International Symposium on
Research in Attacks, Intrusions, and Defenses.
Springer, 2016, pp.
230–253.
[95] M. Sharif, V. Yegneswaran, H. Saidi, P. Porras, and W. Lee, “Eureka:
A framework for enabling static malware analysis,” in European
Symposium on Research in Computer Security. Springer, 2008, pp.
481–500.
[96] X. Shu, D. Yao, and N. Ramakrishnan, “Unearthing stealthy program
attacks buried in extremely long execution paths,” in Proceedings of
the 22nd ACM SIGSAC Conference on Computer and Communications
Security, 2015, pp. 401–413.
[97] M. A. Siddiqui, A. Fern, R. Wright, A. Theriault, D. Archer, and
W. Maxwell, “Detecting cyberattack entities from audit data via multi-
view anomaly detection with feedback,” in Workshops at the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[98] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic rep-
resentations from tree-structured long short-term memory networks,”
arXiv preprint arXiv:1503.00075, 2015.
[99] N. Tavabi, P. Goyal, M. Almukaynizi, P. Shakarian, and K. Lerman,
language models,” in
“Darkembed: Exploit prediction with neural
Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[100] B. E. Ujcich, S. Jero, A. Edmundson, Q. Wang, R. Skowyra, J. Landry,
A. Bates, W. H. Sanders, C. Nita-Rotaru, and H. Okhravi, “Cross-app
poisoning in software-deﬁned networking,” in Proceedings of the 2018
ACM SIGSAC Conference on Computer and Communications Security,
2018, pp. 648–663.
[101] C. Wagner, A. Dulaunoy, G. Wagener, and A. Iklody, “Misp: The
design and implementation of a collaborative threat intelligence shar-
ing platform,” in Proceedings of the 2016 ACM on Workshop on
Information Sharing and Collaborative Security. ACM, 2016, pp.
49–56.
[102] D. Wagner and P. Soto, “Mimicry attacks on host-based intrusion
detection systems,” in Proceedings of the 9th ACM Conference on
Computer and Communications Security. ACM, 2002.
[103] Q. Wang, W. U. Hassan, A. Bates, and C. Gunter, “Provenance
tracing in the internet of things,” in Proceedings of the 9th USENIX
Conference on Theory and Practice of Provenance, 2017, pp. 9–9.
[104] ——, “Fear and logging in the internet of things,” in Network and
Distributed Systems Symposium, 2018.
[105] C. Wueest, “Internet security threat report - living off the land and
ﬁleless attack techniques,” 2017.
[106] K. Xu, K. Tian, D. Yao, and B. G. Ryder, “A sharper sense of self:
Probabilistic reasoning of program behaviors for anomaly detection
with context sensitivity,” in DSN.
IEEE, 2016.
[107] X. Yuan, O. Setayeshfar, H. Yan, P. Panage, X. Wei, and K. H. Lee,
“DroidForensics: Accurate reconstruction of android attacks via multi-
layer forensic logging,” in Proceedings of the 2017 ACM on Asia
Conference on Computer and Communications Security, 2017.
[108] Z. Yuan, Y. Lu, Z. Wang, and Y. Xue, “Droid-sec: deep learning in
android malware detection,” in ACM SIGCOMM Computer Commu-
nication Review, vol. 44, no. 4. ACM, 2014, pp. 371–372.
APPENDIX
A. Neural Document Embedding Models
representations of words. Recently, Le and Mikolov proposed
Paragraph Vector (i.e., doc2vec) [69], a straightforward ex-
tension of word2vec that is capable of learning distributed
representations of arbitrary length word sequences such as
sentences, paragraphs and even whole large documents.
Fig. 10: The PV-DM model for learning a paragraph vector.
PV-DM (Distributed Memory Model of Paragraph Vectors)
is one version of doc2vec. The core idea of PV-DM is
that a paragraph p can be represented as another vector
(i.e., paragraph vector) contributing to the prediction of the
next word in a sentence. In the PV-DM model as illustrated in
Figure 10, every paragraph is mapped to a paragraph vector,
represented by a column in a paragraph matrix and every word
is mapped to a word vector, represented by a column in a
word matrix. Then the paragraph vector and word vectors are
averaged or concatenated to predict the next word in a context.
The contexts are ﬁxed-length and sampled from a sliding
window over the paragraph. The paragraph vector is shared
across all contexts generated from the same paragraph but not
across paragraphs. The PV-DM model uses stochastic gradient
descent to train the paragraph vectors and word vectors. After
being trained, the paragraph vectors can be used as features for
the paragraph. At prediction time, the model also use gradient
descent to compute the paragraph vector for a new paragraph.
B. Comparison of Different Anomaly Detection Algorithms
In our current implementation, we use Local Outlier Factor
(LOF) [11] as the default anomaly detector. We compare
LOF with three other novelty detection or outlier detection
algorithms in path-level accuracy. The three baseline methods
are as follows:
• Isolation Forest [72]: This algorithm divides the data
points to different partitions. Outliers need less cuts to
be separated from other points while inliers need more
cuts.
• One-Class SVM [93]: The algorithm trains a hyper-plane
which separates all the training data from the origin while
maximizing the distance from the origin to the hyper-
plane.
• Robust Covariance (Elliptic Envelope) [9]: The algorithm
assumes that the data is Gaussian distribution and learns
an ellipse.
Word2vec [77] is one of the most well-known word embed-
ding methods. It uses a simple and efﬁcient feed forward neural
network architecture called “skip-gram” to learn distributed
In the evaluation of the above baseline methods, we follow
the same experiment protocol as we did for PROVDETECTOR
. For one-class SVM, we use the rbf kernel with nu set to 0.1
16
PWWWonClassiﬁerAverage/ConcatenateParagraph MatrixthecatsatParagraphidwhat baseline detection methods we select. Instead, we use
LIME [91], a model-agnostic prediction explanation tool, to
calculate and rank the “impact” of each single node in a path
to the ﬁnal detection. LIME also produces a numeric value to
evaluate how much the ﬁnal result would change, in case we
remove any node from the path.
We use LIME to calculate the “KEY” nodes for each benign
path and malicious path. A set of nodes are considered as KEY
nodes if they are the most impactful nodes identiﬁed by LIME
and PROVDETECTOR would give a different detection result if
we remove these nodes from the path. We try to ﬁnd if there
is a set of KEY nodes that are common across all the paths. If
so, it indicates that PROVDETECTOR has only learned single
node level features.
In our experiment, we ﬁnd that
there is not a set of
KEY nodes that can be shared by most of the paths. For
benign paths, 35% of the paths have their own unique KEY
node. On average, the number of paths that share the same
KEY node is 3.18. In other words, each KEY node is used
to impact 3 benign paths on average in PROVDETECTOR.
For malicious paths, about 50% of paths have their unique
KEY node. The average number of paths that share the same
KEY node is 3.1. In summary, combined with the results in
§VI-B2, PROVDETECTOR relies on path-level features instead
of single node level features to detect stealthy malware, which
is consistent with our design motivation.
TABLE V: Comparison of different anomaly detection algo-
rithms in path-level detection accuracy.
Algorithm
Local Outlier Factor
One-Class SVM
Isolation Forest
Robust Covariance
Precision
0.959
0.886
0.955
0.940
Recall
0.991
0.635
0.467
0.397
F1-Score
0.974
0.739
0.627
0.558
and gamma set to 0.5. For the other three models, we set the
contamination to 0.04. The results are summarized in Table V.
As shown in the table, LOF signiﬁcantly outperforms other
methods in terms of recall. This justiﬁes our design choice of
using LOF. We will further explain the results in §VI-C.
C. Additional Experiments to Interpret the Result of PROVDE-
TECTOR
In this section, we discuss several additional questions
about the results of PROVDETECTOR. These questions are:
methods?
• Why LOF outperforms the other three anomaly detection
• What is learned by PROVDETECTOR?
1) Why does LOF Perform Better: As shown in Table V,
LOF performs the best among the four evaluated algorithms.
This is because LOF does not rely on an assumption about the
distribution of the data. As shown in Figure 9, the embeddings
of paths have multiple clusters and do not follow any single
distribution.
Robust Covariance performs worst as it assumes the data
obeys approximately a Gaussian distribution and tries to learn
an ellipse to cover the normal data points. Consequently, it
may degrade when the data is not unimodal. Isolation Forest
and One-Class SVM outperform Robust Covariance because
they do not rely on any assumption on the distribution of data.
However, these two methods assume that the normal paths are
all from one cluster; thus they cannot achieve high detection
accuracy as high as LOF.
On the other hand, LOF detects anomalous data points by
measuring the local deviation of a given data point with respect
to its neighbors, making it typically suitable for the case where
different models in the data have different densities. As with
our data, different workloads may generate paths that have
different densities in distribution, thus LOF could achieve a
high detection accuracy.
2) What PROVDETECTOR Learns: There are two possible
kinds of features that PROVDETECTOR has learned: the path-
level feature or the single node level feature. If PROVDETEC-
TOR only learns single node level features, it could indicate
that PROVDETECTOR only memorizes a small set of nodes
to detect malicious paths. Still take the winword program as
an example, a “bad” detection model which only learns node
level features might predict a path as malicious if a previously
unseen process (e.g., PowerShell) node is in the path. Such
detection model can easily be evaded by attackers.
To answer this question, a naive method is to develop
baseline detection methods that only rely on single node
level features. However, this method may have a bias from
17