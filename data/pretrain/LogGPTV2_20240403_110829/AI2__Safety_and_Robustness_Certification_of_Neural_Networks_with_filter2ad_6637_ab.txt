function but with different parameters. The output of a layer
is formed by stacking the outputs of the neurons into a vector
or three-dimensional array. We will deﬁne the functionality in
terms of entire layers instead of in terms of individual neurons.
Reshaping of Inputs. Layers often take three-dimensional
inputs (e.g., colored images). Such inputs are transformed into
vectors by reshaping. A three-dimensional array x ∈ R
m×n×r
can be reshaped to xv ∈ R
m·n·r in a canonical way, ﬁrst by
depth, then by column, ﬁnally by row. That is, given x:
xv
= (x1,1,1 . . . x1,1,r x1,2,1 . . . x1,2,r . . . xm,n,1 . . . xm,n,r)
T .
Activation Function. Typically, layers in a neural network
perform a linear transformation followed by a non-linear
activation function. We focus on the commonly used rectiﬁed
linear unit (ReLU) activation function, which for x ∈ R is
deﬁned as ReLU(x) = max(0, x), and for a vector x ∈ R
as ReLU(x)=(ReLU(x1), . . . , ReLU(xm)).
ReLU to CAT. We can express the ReLU activation function
as ReLU = ReLUn ◦ . . .◦ ReLU1 where ReLUi processes the
ith entry of the input x and is given by:
m
ReLUi(x) = case (xi ≥ 0) : x,
case (xi < 0) : Ii←0 · x.
Ii←0 is the identity matrix with the ith row replaced by zeros.
Fully Connected (FC) Layer. An FC layer takes a vector
of size m (the m outputs of the previous layer), and passes
5
(m−p+1)×(n−q+1).
F p,q
i
: R
m×n×r → R
q(cid:2)
r(cid:2)
p(cid:2)
Fig. 4a shows an FC layer computation for x = (2, 3, 1).
Convolutional Layer. A convolutional layer is deﬁned by
a series of t ﬁlters F p,q = (F p,q
), parameterized by
the same p and q, where p ≤ m and q ≤ n. A ﬁlter F p,q
is a function parameterized by a three-dimensional array of
p×q×r and a bias b ∈ R. A ﬁlter takes a
weights W ∈ R
three-dimensional array and returns a two-dimensional array:
, .., F p,q
1
t
i
The entries of the output y for a given input x are given by:
Wi(cid:2),j(cid:2),k(cid:2) · x(i+i(cid:2)−1),(j+j(cid:2)−1),k(cid:2) + b).
yi,j = ReLU(
i(cid:2)=1
j(cid:2)=1
k(cid:2)=1
Intuitively, this matrix is computed by sliding the ﬁlter along
the height and width of the input three-dimensional array, each
time reading a slice of size p×q×r, computing its dot product
with W (resulting in a real number), adding b, and applying
ReLU. The function ConvF , corresponding to a convolutional
layer with t ﬁlters, has the following type:
ConvF : R
m×n×r → R
(m−p+1)×(n−q+1)×t.
As expected, the function ConvF returns a three-dimensional
array of depth t, which stacks the outputs produced by each
ﬁlter. Fig. 4b illustrates a computation of a convolutional layer
with a single ﬁlter. For example:
y1,1,1 = ReLU((1 · 0 + 0 · 4 + (−1) · (−1) + 2 · 0) + 1) = 2.
4×4×1. As
Here, the input is a three-dimensional array in R
the input depth is 1, the depth of the ﬁlter’s weights is also 1.
The output depth is 1 because the layer has one ﬁlter.
Convolutional Layer to CAT. For a convolutional layer
ConvF , we deﬁne a matrix W F whose entries are those of the
weight matrices for each ﬁlter (replicated to simulate sliding),
and a bias bF
consisting of copies of the ﬁlters’ biases. We
then treat the convolutional layer ConvF like the equivalent
FCW F ,bF . We provide formal deﬁnitions of W F and bF
in
Appendix A. Here, we provide an intuitive illustration of the
translation on the example in Fig. 4b. Consider the ﬁrst entry
y1,1 = 2 of y in Fig. 4b:
y1,1=ReLU(W1,1·x1,1+W1,2·x1,2+W2,1·x2,1+W2,2·x2,2+b).
When x is reshaped to a vector xv,
the four entries
x1,1, x1,2, x2,1 and x2,2 will be found in xv
1, xv
5 and xv
6,
respectively. Similarly, when y is reshaped to yv, the entry
y1,1 will be found in yv
1 = y1,1, we deﬁne
the ﬁrst row in W F such that its 1st, 2nd, 5th, and 6th entries
are W1,1, W1,2, W2,1 and W2,2. The other entries are zeros.
1. Thus, to obtain yv
2, xv
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
x
2
3
1
W
0
-1
1
1
2
0
b
-1
0
ReLU
y
3
0
+ -1 ) = 3
ReLU( 1 0 2 · 2
3
1
ReLU( 1 -1 0 · 2
3
1
+ 0 ) = 0
ReLU( 1 0 -1 2 ·
x
b
1
W
1 0
-1 2
0 4 2 1
-1 0 1 -2
3 1 2 0
0 1 4 1
ReLU( 1 0 -1 2 ·
ReLU
y
2 7 0
0 4 0
6 9 1
x
0 1 3 -2
2 -4 0 1
2 -3 0 1
-1 5 2 3
y
2 3
5 3
+ 1 ) = 2
max( 0 1 2 -4 ) = 2
+ 1 ) = 1
max( 3 -2 0 1 ) = 3
max( 2 -3 -1 5 ) = 5
max( 0 1 2 3 ) = 3
0
4
-1
0
2
0
4
1
(a) Fully connected layer FCW,b
(b) Convolutional layer Conv(W,b) (one ﬁlter)
(c) Max pooling layer MaxPool2,2
Fig. 4: One example computation for each of the three layer types supported by AI2.
9 × R
We also deﬁne the ﬁrst entry of the bias to be b. For similar
reasons, to obtain yv
2 = y1,2, we deﬁne the second row in W F
such that its 2nd, 3rd, 6th, and 7th entries are W1,1, W1,2, W2,1
and W2,2 (also b2 = b). By following this transformation, we
obtain the matrix W F ∈ R
⎛
W F =
⎜⎜⎜⎜⎝
16 and the bias bF ∈ R
⎛
bF
⎜⎜⎝
1 0 0 0 −1 2
0 0 0
0 0 0
0 0
0 1 0 0 0 −1 2 0 0
0 0 0
0 0
0 −1 2 0
0 0 1 0 0
0 0 0
0 0
0 0 0 0 1 0
0 0 0
0 0
1 0 0 0 −1 2 0 0
0 0 0 0 0
0 0
0 −1 2 0
0
0 0 0 0 0
0 0
0 0
0
0 0 0 0 0
1 0 0 0 −1 2 0
0
0 0 0 0 0
0 −1 2
0
0 0 0 0 0
0
0
0
0
0
0
0
0 0 −1 2
1 0 0
0 0 1 0
0 0 0
0 0 0
0
0
0
0 0 −1 2
=
1
1
1
1
1
1
1
1
1
⎞
⎟⎟⎟⎟⎠
⎞
⎟⎟⎠
1 0 0
9:
To aid understanding, we show the entries from W that appear
in the resulting matrix W F in bold.
Max Pooling (MP) Layer. An MP layer takes a three-
dimensional array x ∈ R
m×n×r and reduces the height m of
x by a factor of p and the width n of x by a factor of q (for p
and q dividing m and n). Depth is kept as-is. Neurons take as
input disjoint subrectangles of x of size p × q and return the
maximal value in their subrectangle. Formally, the MP layer
q ×r that for an
is a function MaxPoolp,q : R
input x returns the three-dimensional array y given by:
m×n×r → R
p × n
m
yi,j,k = max({xi(cid:2),j(cid:2),k | p · (i − 1) < i(cid:2) ≤ p · i
q · (j − 1) < j(cid:2) ≤ q · j}).
Fig. 4c illustrates the max pooling computation for p = 2,
q = 2 and r = 1. For example, here we have:
y1,1,1 = max({x1,1,1, x1,2,1, x2,1,1, x2,2,1}) = 2.
m·n·r → R
q ·r
(cid:2)
Max Pooling to CAT. Let MaxPool
p,q : R
be the function that is obtained from MaxPoolp,q by reshaping
(cid:2)
p,q(xv) = MaxPoolp,q(x)v. To
its input and output: MaxPool
represent max pooling as a CAT function, we deﬁne a series
(cid:2)
p,q:
of CAT functions whose composition is MaxPool
q ·r ◦ . . . ◦ f1 ◦ f MP.
p · n
(cid:2)
p,q = f m
MaxPool
p · n
m
xv
0
1
3
-2
2
-4
0
...
f MP
xMP
0
1
2
-4
3
-2
0
...
f1
2
3
-2
0
1
2