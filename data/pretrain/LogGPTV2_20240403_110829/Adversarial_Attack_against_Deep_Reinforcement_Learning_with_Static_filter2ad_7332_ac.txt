As the reward signal in many RL problems is sparse, an
adversary need not attack the RL agent at every time step.
Therefore, this adversarial attack tactic utilizes this unique
characteristic to attack selected subset of time steps of RL
agents. The core of strategically-timed attack is that the
adversary can minimize the expected accumulated reward
of target agent by strategically attacking less than  << L
time steps, to achieve the purpose of adversarial attack,
which can be formulated intuitively as an optimization
problem
t
where s1, ..., sL denotes the sequence of observations or
states, δ1, ..., δL is the sequence of perturbations, R1 rep-
resents the expected return at the first time step, b1, ..., bL
denotes when an adversarial example is applied, and the 
is a constant to limit the total number of attacks.
However, the optimization problem in 8 is a mixed
integer programming problem, which is difficult to solve.
min
b1,b2,...,bL,δ1,δ2,...,δL
¯st = st + btδt
bt ∈ 0, 1,
(cid:9)
R1(¯s1, ...,¯sL)
for all t = 1, ..., L
for all t = 1, ..., L
bt ≤ 
(8)
Hence, authors proposed a heuristic algorithm to solve
this task, with a relative action preference function c,
which computes the preference of the agent in taking the
most preferred action over the least preferred action at the
current state (similar to Farahmand (2011)).
For policy gradient-based methods such as A3C algo-
rithm, Lin et al. defined the function c as
c(st) = maxat
π(st, at) = minat
π(st, at)
(9)
where st denotes the state at time step t, and at denotes
the action at time step t, and π is the policy network which
maps the state-action pair (st, at) to a probability.
Meanwhile, for value-based methods such as DQN, the
function c can be defined as
T
T
(10)
T(cid:16)
T(cid:16)
e Q(st,at )
ak e Q(st,ak )
e Q(st,at )
ak e Q(st,ak )
− minat
c(st) = maxat
where Q denotes the Q-values of actions, and T denotes
the temperature constant.
• Enchanting Attack (EA)
The purpose for enchanting attack is to push the RL
agent to achieve the expected state sg after H steps under
the current state st at time step t. Under such attacking
approach, the adversary needs to specially design a series
of adversarial examples st+1 + δt+1, ..., st+H + δt+H, hence,
this tactic of attack is more difficult than strategically-
timed attack.
The first hypothesis assumed that we can take full con-
trol of the target agent, and enable to take any action in
any time step. Therefore, under such condition, this prob-
lem can be simplified to planning an action sequence,
Chen et al. Cybersecurity            (2019) 2:11 
Page 12 of 22
which can make agent to the target sate sg from state st.
For the second hypothesis, Lin et al. specially designed an
adversarial example st + δt to lure target agent to imple-
ment the first action in planned action sequence with
method proposed by Carlini and Wagner (2017). After
agent observes the adversarial examples and takes the first
action designed by adversary, the environment will return
a new sate st+1 and iterative build adversarial examples
in this way. The attack flow for enchanting attack can is
shown in Fig. 7.
For this work, strategically-time attack can achieve the
same effect as the traditional method (Huang et al. 2017),
while reduce the total time step for attacking. Moreover,
enchanting attack can lures target agent to take planned
action sequence, which suggests a new research idea for
the follow-up studies. Videos are available at http://yclin.
me/adversarial_attack_RL/.
Adversarial attack on VIN (AVI)
The main contribution for Liu et al. (2017) is that they
proposed a method for detecting potential attack which
can obstruct VIN effectiveness. They built a 2D navigation
task demonstrate VIN and studied how to add obstacles to
effectively affect VIN’s performance and propose a general
method suitable for different kinds of environment.
Their threat model assumed that the entire environ-
ment (including obstacles, starting point and destination)
is available, and they also know that the robot is trained
by VIN, meanwhile, it is easy to get the VIN planning path
and the theoretical path. Based on this threat model, they
summarized three rules which can effectively obstructing
VIN.• Rule 1: The father away from the VIN planning path,
the less disturbance to the path.
Such rule can be formulated as:
= ω1 min
v1yk
d1|d1= (cid:13)
(xr − ykr)2 + (xc − ykc)2,
(xr, xc)=x ∈ X,(ykr, ykc)=yk ∈ Y
(11)
(cid:17)
(cid:18)
where xr, xc is the coordinate of x, (ykr, ykc) is the coordi-
nate of yk, ω1 is the weight of v1.
• Rule 2: It is most likely to be success when adding
obstacles around the turning points on the path.
(cid:18)
(cid:17)
Such rule can be formulated as:
= ω2 min
v2yk
d2|d2= max(|tr,−ykr|,|tc − ykc|),
(tr, tc)=t ∈ T, (ykr, ykc) = yk ∈ Y
(12)
where (tr, tc) denotes the coordinate of t, (ykr, ykc) repre-
sents the coordinate of yk, ω2 is the weight for v2. The
formula considers the Chebyshev distance from yk to the
nearest turning point, and utilize the weight ω2 to control
the attenuation of v2.
• Rule 3: The closer the adding obstacle position is to
the destination, the less likely it is to change the path.
The representative for (xnr, xnc) is the coordinate of xn,
(ykr, ykc) denotes the coordinate of yk, ω3 is the weight for
v3. Hence, the formula can be concluded as:
= ω3 max(|xnr − ykr|,|xnc − ykc|), (xnr, xnc)
= xn, (ykr, ykc) = yk ∈ Y
(13)
this formula considers the Chebyshev distance from yk to
the destination, and utilize the weight ω3 to control the
attenuation of v3.
Calculating the value v considering three rules for each
available point, meanwhile, sorting the values to pick up
most valuable points S = y|vyk ∈ maxi V, y ∈ Y, V =
vy1, vy2, ..., vyk.
Liu’s method has great performance on automatically
finding vulnerable points of VIN and thus obstructing
navigation task, which can be shown in Fig. 8.
However, this work has not give an analysis of the
successful adversarial attack from the algorithm level,
but summarized the generation rules from the successful
black-box adversarial examples. Meanwhile, similar to the
v3yk
Fig. 7 Attacking flow for enchanting attack (Lin et al. 2017). Enchanting attack from the original state st, the whole processing flow can be concluded
as follow: 1) action sequence planning; 2) generating adversarial examples with target actions; 3) agent takes actions under adversarial example; 4)
environment gives the next sate st+1. Meanwhile, adversary utilizes the prediction model to attack the target agent with initial state st
Chen et al. Cybersecurity            (2019) 2:11 
Page 13 of 22
Fig. 8 Examples for adversarial examples successfully attack. The examples show that the method proposed in this paper do have ability to find
vulnerabilities under VIN pathfinding, and thus interfere the performance of agent automatic pathfinding. a Sample of testing set. b Available
Obstacle 1. c Available Obstacle 2. d Available Obstacle 3. e Available Obstacle 4. f Available Obstacle 5
work of Xiang et al. and Bai et al., the map size has too
many limitations. Only the size under 28 × 28 have been
experimentally verified, and such size is not enough to
prove the accuracy of the method proposed in this paper.
Summary for adversarial attack in reinforcement learning
We give summary on the attributions of adversarial
attacking methods described above, which can be shown
in Table 3.
FGSM (Goodfellow et al. 2014a), SPA (Xiang et al.
2018), WBA (Bai et al. 2018), and CDG (Chen et al.
2018b) belong to White-box attack, which have access
to the details related to training algorithm and corre-
sponding parameters of the target model. Meanwhile,
the PIA (Behzadan and Munir 2017), STA (Lin et al.
2017), EA (Lin et al. 2017), and AVI (Liu et al. 2017)
are Black-box attacks, in which adversary has no idea
of the details related to training algorithm and corre-
sponding parameters of the model, for the threat model
discussed in these literatures, authors assumed that the
adversary has access to the training environment bat has
no idea of the random initializations of the target pol-
icy, and additionally does not know what the learning
algorithm is.
For White-box attack policies, we summarize the
parameters utilized for such methods. SPA, WBA, CDG,
PIA, and AVI all have the specific target algorithm,
however, the target for FGSM, STA, anf EA is not
single reinforcement learning algorithm, in this sense,
such adversarial attack methods are more universal
adaptability.
Moreover, the learning way for these adversarial attack
methods are different, as FGSM, SPA, WBA, CDG, and
AVI are all “One-shot” learning, and PIA, STA, and
EA are “Iterative” learning. Additionally, for all attack
methods introduced here can generate adversarial exam-
ples to achieve the purpose of attacking successfully
under a relatively high confidence. The application sce-
nario for FGSM, PIA, STA, and EA are Atari game,
meanwhile, the scenario for SPA, WBA, CDG, and AVI
are all path planning. We also take a statistical anal-
ysis of the attack results for the algorithms discussed
above.
Defense technology against adversarial attack
Since the adversarial examples attack proposed by
Szegedy et al. (2013) in 2013, meanwhile, there are many
related researchers have investigated the approaches to
defense against adversarial examples. In this section, we
briefly discussed some representative attempts that have
been done to resist adversarial examples. Mainly divided
into three parts, which are modifying input, modify-
ing the objective function, and modifying the network
structure.
Chen et al. Cybersecurity            (2019) 2:11 
Page 14 of 22
d
e
s
a
e
r
c
n
i
i
e
m
T
/
n
o
i
t
a
n
i
t
s
e
d
h
c
a
e
r
o
t
e
b
a
n
U
l
i
g
n
n
n
a
P
h
t
a
P
l
i
g
n
n
n
a
p
h
t
a
p
l
i
g
n
n
n
a
p
h
t
a
p
l
l
a
m
r
o
n
o
N
l
a
m
r
o
n
o
N
i
g
n
n
n
a
P
h
t
a
P
l
i
g
n
n
n
a
P
h
t
a
P
l
n
o
i
t
c
a
g
n
o
r
w
g
n
k
a
T
i
e
m
a
G
i
r
a
t
A
n
o
i
t
c
a
g
n
o
r
w
g
n
k
a
T
i
n
o
i
t
c
a
g
n
o
r
w
g
n
k
a
T
i
n
o
i
t
c
a
g
n
o
r
w
g
n
k
a
T
i
e
m
a
G
i
r
a
t
A
e
m
a
G
i
r
a
t
A
e
m
a
G
i
r
a
t
A
i
g
n
n
n
a
p
h
t
a
p
l
l
a
m
r
o
n
o
N
i
g
n
n
n
a
P
h
t
a
P
l
e