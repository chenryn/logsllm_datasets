to target VMs [34]. Herzberg et al. [21] extended the work
of Ristenpart et al. by demonstrating new techniques for
deanonymizing internal IP addresses, revealing the network
topology, and testing for colocation. Xu et al. [40] again use
network topology as a means for multi-tenancy detection, but
do so systematically. Varadarajan et al. [36] demonstrated a
new technique for multi-tenancy detection by creating memory
bus contention using locked atomic operations. Their work also
revealed common VM placement strategies employed by IaaS
providers.
D. Comparison of Approaches
We show a high level comparison of previous approaches
in Table I. As demonstrated, our covert channel offers a
higher data transmission rate than the enumerated approaches
while still keeping a low error rate. We should give special
mention to Wang and Lee’s cache side channel, which offers
a comparable transmission rate of 3.2 Mbps. The authors
discuss how their side channel can also be used as a covert
channel. Unfortunately, they do not discuss the error rate in
their approach, nor the performance of their approach in a
cloud environment.
Furthermore our approach is noteworthy in that we present,
to the best of our knowledge, the ﬁrst multi-tenant detection
scheme using hardware hyperthreading. Our demonstration in
Section VII shows a clear detection threshold, while only
requiring a relatively small number of VMs to be launched
from cooperating accounts.
III. BACKGROUND
A. Memory Order Buffer
The memory ordering buffer (MOB) has been a key
microarchitectural component since the Intel Nehalem mi-
croarchitecture [11]. The MOB enables loads and stores to
be issued speculatively and execute out-of-order, ensures that
retired loads and stores occur in order with correct values,
and enforces Intel’s memory ordering model. Memory disam-
biguation and store-to-load forwarding are two features of the
MOB that allow loads and stores to be speculatively issued
and executed out-of-order, respectively.
Stores to memory are temporarily written into a store
buffer prior to commitment enabling the processor to continue
execution without having to stall, for instance because the
store is waiting on a busy L1 data cache write port. Delaying
writes also makes more efﬁcient use of bus bandwidth via
streaming. The store buffer comprises the virtual and physical
store address and the store data of executed stores [12]. As
long as a store has not been retired, it occupies a store buffer
entry slot. Once the store address and data are known, the store
data can be forwarded to any following load operations in a
process called store-to-load forwarding. This is an important
performance feature of modern processors because it saves
cycles by allowing the load to obtain its data directly from
the store without having to access the cache subsystem.
Both the store address and data must be known before store
data can be forwarded to a dependent load. Accordingly these
loads must wait, but younger loads that are independent of
the store should be allowed to issue and execute ahead of
the stalled load. Otherwise signiﬁcant performance slowdown
and underutilization of resources will occur in the case of
a stalling, long store/load dependency chain. Intel’s solution
to optimizing available instruction level parallelism (ILP) is
to allow younger loads to be speculatively issued and later
disambiguated in a process called memory disambiguation.
When a load speculatively issues, it takes its data from the L1
data cache, even when older store addresses are unknown, and
updates its load buffer slot. Prior to commitment the prediction
is veriﬁed. If a conﬂict exists, the load and all succeeding
instructions in the loads dependency chain are re-executed.
B. Memory Disambiguation Prediction
Memory disambiguation prediction for loads occurs early
in the Issue stage of the processor pipeline to optimize ILP
3
Processor Model
Intel Xeon E5-2690
Intel Core i7-3770
Intel Core i7-4770
Intel Core i7-6820HQ
Microarchitecture
Clock Freq Memory Order Buffer
OS/Kernel Version
Sandy Bridge
Ivy Bridge
Haswell
Skylake
2.9 GHz
3.4 GHz
3.5 GHz
2.7 GHz
SB: 36 / LB: 64
SB: 36 / LB: 64
SB: 42 / LB: 72
SB: 56 / LB: 72
CentOS 2.6.32
Arch Linux 4.10.8
Arch Linux 4.8.13
Arch Linux 4.10.1
TABLE II: Experimental platform speciﬁcations. SB means store buffer and LB means load buffer.
by allowing loads to speculatively execute ahead of stores that
have not yet been resolved. The exact details of the predic-
tion algorithm are undocumented. However, the Intel Core®
microarchitecture employed a hashed index lookup using the
load’s instruction pointer [13]. Each entry in the memory
disambiguation predictor behaved as a saturating counter that
was updated at instruction retirement. Prediction was veriﬁed
by comparing the address of all dispatched store operations
against the address of all younger loads. It is unclear, however,
if the memory disambiguation prediction algorithm is still in
place on newer Intel microarchitectures.
C. Coherency Snooping
Memory ordering is correctly maintained if a memory read
(load) results in the same value that was written by the most
recent memory write (store). Ordering must be maintained
between earlier loads and later writes as well as earlier and
later loads. Chowdhury and Carmean [7] outline a method
for maintaining ordering between memory operations in a
multiprocessor by snooping the load buffer before a store
operation completes using per-core memory snoop logic. In
response to committing a store to the L1 data cache, the snoop
logic compares the store address with every load address in the
load buffer. If a match is found, then an ordering violation is
triggered for the corresponding load instruction. This results in
the load, and preceeding speculatively executed instructions in
the load dependency chain, to be aborted and re-issued. The
snoop of the load buffer in the embodiment outlined in [7]
reveals that it is implemented at cache-line granularity, e.g.,
the lower 12-bits of the virtual and physical address.
D. 4K-Aliasing
Intel’s documents assume dependency between 4 KB sep-
arated loads and stores and calls this 4K-aliasing [11]. 4K-
aliasing occurs when the lower 12-bits of the address of a
load issued after a preceding store falsely matches in the store
buffer. However, the cause of 4K-aliasing is undocumented.
Recall that memory disambiguation prediction will attempt to
issue loads early to speculatively execute ahead of independent
stores. Perhaps the memory disambiguator prevents all loads
whose lower 12-bits match a store address in the store buffer
from being issued early. This is a relatively cheap decision
considering the stall cycles caused by the incorrect prediction
on independence.
Alternatively, 4K-aliasing could be the result of coherency
snooping. In this scenario, the front-end might allow all loads
to speculatively execute. Prior to commitment of any store,
the snoop logic will ﬁnd a false match in the load buffer on
the lower 12-bits of a load address in the load buffer. This
will cause the load to abort, and any instructions in the load’s
dependency chain to be re-issued. Perhaps Intel reasons that
loads to unique page frames are rare, or that virtual address
aliasing is uncommon.
In both case, however, the responsible microarchitectural
logic does not distinguish between threads or process. This
is stated explicitly in the patent describing coherency snoop-
ing [7]. We surmise that distinguishing between processes is
likely not employed during memory disambiguation prediction
based on other prediction logic at the front-end, e.g., the branch
target buffer and branch predictor.
IV.
4K-ALIASING WITHIN A SINGLE PROCESS
In this section, we aim to evaluate the timing characteristics
due to 4K-aliasing within a single process. This achieves
several goals. First, it veriﬁes the performance penalty due
to the 4K-aliasing event. Second, it allows us to determine
the conditions under which 4K-aliasing is observable. Finally,
we aim to establish a baseline of expected behavior within a
controlled environment prior to demonstrating the 4K-aliasing
covert communication channel.
A. Initial Benchmark and Experimental Setup
this event will
We initially want to verify the performance penalty caused
by 4K-aliasing without forcing the event to occur. The Intel
optimization guide suggests that
likely be
observable during a memory copy routine where the source
and destination buffer addresses are separated by 4 KB. Each
time the data to be copied is read, a 4 KB aligned base address
will falsely match with the 4 KB aligned base address of
the source buffer. A deterministic performance degradation
should be observable as memory disambiguation prediction
will incorrectly predict a dependency between the copy.
Figure 1 shows the performance penalty of 4K-aliasing in
the memory copy routine when the source and destination
buffers are separated by 4 KB. We evaluate the 4K-aliasing
effect on four recent Intel microarchiture families: Sandy
Bridge, Ivy Bridge, Haswell, and Skylake. Table II shows the
conﬁguration for the platforms tested. All systems use the Intel
64-bit ISA and 64-bit GNU libc 2.24 libraries. The results
show that the copy bandwidth drops every time an address is
aligned on 4 KB boundary. Subsequent copies to addresses that
do not align on a 4 KB boundary rapidly recover. Note that
the performance falls off after 16 KB as two array references
cannot be serviced within the same 32 KB L1 data cache.
The granularity of observable 4K-aliasing events for the
memory copy routine is too coarse to offer much insight into
the cycle latency of a falsely aliasing load. Ideally, we need to
be able to clearly distinguish the number of cycles required to
service a 4K-aliasing load versus the number of cycles required
to service all other memory load events. This will allow us
4
Fig. 1: Effect of 4K-aliasing in memory order buffer. SND:
Sandy Bridge, IVY: Ivy Bridge, HSW: Haswell, SKY: Skylake.
Fig. 2: Load latency due to 4K-aliasing in memory order buffer
when the load address is parametrically swept by 2 bytes with
respect to a constant store address.
to reliably convert the 4K-aliasing event into a stable covert
communication channel.
In order to understand the observable 4K-aliasing cycle
latency in an ideal setting we need to clarify the conditions
required for this scenario. We are interested in measuring
the cycle latency of a load issued after a preceding store
that falsely aliases in the store buffer. This can occur in two
situations. A later (in program order) load executes after an
earlier (in program order) store, or when a later write passes
an earlier store. The latter is referred to as a write-after-read
(WAR) data hazard. WAR hazards are undesirable because
executing a later store ahead of a load when their memory
addresses match will cause the read to load incorrect data.
In fact, Intel’s VTune performance analysis guideline [10]
describes 4K-aliasing as a side-effect of the memory order
buffer preventing WAR hazards.
Therefore, for our analysis we opted for measuring WAR
events. Listing 1 presents the baseline benchmarking code used
to measure the overhead due to 4K-aliasing within a single
process. The benchmark parametrically sweeps load addresses
by 2 bytes with respect to a constant store address, measuring
the associated latency. It is written such that the lower 12-bits
of one load address within the load array in the inner loop
will falsely alias with the lower 12-bits of a preceding store
address in the store buffer.
The actual measurement code is contained within the inline
assembly section of Listing 1. We use AT&T syntax to write
the assembly, which is read as inst. dest, src. The
ﬁrst two assembly instructions (lines 7 & 8) move the array
pointers for store_arr and load_arr to local integer
register r14 and r15 respectively. An immediate value is
stored at the array index pointed to by store_arr at line
11. Line 12 derefences load_arr and adds the value to the
value in r9. Note that we add a variable number of single
cycle add immediate instructions [17] to ensure that we do
not incorrectly alias with memory operations at lines 7 & 8.
In our experiments, we set the number of add instructions to
equal the average cycle latency required to access the L1 data
cache for the given microarchitecture family. When the load
address at line 12 aliases on the lower 12-bits with the store at
line 11, we expect the timestamp counter to report more cycles.
We use the rdtscp instruction to record cycles following the
Intel guideline [31].
Listing 1: Baseline 4K-aliasing latency benchmark.
1 for (uint64_t i = 0; i < ARR_SZ; i += 2) {
2
3
4
store_arr = &sarr[i];
start = rdtscp();
for (uint64_t j = 0; j < ARR_SZ; j += 2)←(cid:45)
{
load_arr = &larr[i];
asm volatile (
%0, %%r14
%1, %%r15
$0x1, %%r9
"movq
"movq
"add
...
"movq
"add
:
: "r" (store_arr), "r" (load_arr←(cid:45)
$0x2, (%%r14)
(%%r15), %%r9
\n\t"
\n\t"
\n\t"
\n\t"
\n\t"
)
: "%r9", "%r14", "%r15"
);
}
stop = rdtscp() - start;
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19 }
B. Single Process 4K-Aliasing Results
Figure 2 depicts the measured load latencies as a heatmap.
For a given row (y-axis) the store address is held constant,
while the load is incremented by 2 bytes. Longer latency