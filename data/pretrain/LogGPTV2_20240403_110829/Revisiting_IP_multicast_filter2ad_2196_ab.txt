next hop must in turn forward the packet along.
Figure 1 illustrates this process: A packet multicast to G by host
s arrives at Rs. From their BGP advertisements, Rs learns that pre-
ﬁxes a.b. ∗ .∗, c.d.e.∗, and e. f .∗ .∗ have members in G and com-
putes the multicast tree from the BGP paths from V to each of the
above preﬁxes and forwards one copy of the packet to Q along with
an encoding of the subtree in dashed-line and another copy to U
with an encoding of the subtree in dash-dot-dash style.
4.2 Discussion
To some extent, FRM can be viewed as extending MOSPF to
the inter-domain arena. This extension however is non-trivial be-
cause we do not have a complete network map at the inter-domain
level. The path-vector nature of BGP allows a router to compute
the shortest path(s) from itself to a set of receivers but not from any
source to a set of receivers. This complicates forwarding as a router
that receives a packet has no way of knowing which subset of re-
ceivers it should forward towards since it does not know whether it
lies on the shortest path from the source to those receivers. For ex-
ample, in Figure 1: R1 and R2 both have BGP entries for preﬁxes
c.d.e.∗ , a.b.∗ .∗ , and e. f .∗ .∗, and can hence infer the presence
of group members in these preﬁxes. However, when R1 receives a
packet from Rs, it has no easy way of knowing not to forward to-
ward e. f .∗ .∗ and likewise R2 towards c.d.e.∗ and a.b.∗ .∗. While
one might employ a limited form of ﬂood-and-prune this raises is-
sues similar to DVMRP in terms of scalability and vulnerability to
dynamics. An alternate option might be to change BGP to a pol-
icy compliant link-state protocol however this represents a major
overhaul of BGP which we avoid. FRM’s forwarding is instead
designed to exploit and live within the constraints of the informa-
tion BGP offers. Finally, we note that while PIM and Express too
leverage existing unicast routes they do so only in forwarding JOIN
messages towards the rendezvous point; packet delivery still relies
on group-speciﬁc forwarding state laid down by JOINs.
4.3 Tradeoffs
The core tradeoff FRM makes is to cut down on distributed pro-
tocol mechanism at the cost of demanding more from the internal
capabilities of routers. This offers both advantages and challenges.
On the positive side, we offer the following observations:
Parsimony in protocol mechanism. In terms of protocol
complexity the basic FRM framework requires: (1) extending BGP
to carry group membership information and (2) that an AS on occa-
sion ﬁlter some group for a downstream customer AS (for reasons
described in Section 5).
ISP control. Because group membership is explicitly advertised
through BGP, an ISP has ultimate (and easy) control over which
groups its customers subscribe to; e.g., to block an undesired group,
an ISP can simply drop it from its BGP advertisement. FRM also
allows ISP control over sources in its domain as border routers have
knowledge of (and control over!) the destination domains included
in the dissemination tree. As articulated by Holbrook et al., this
assists in source-based charging as an ISP can now infer the trafﬁc
“ampliﬁcation” due to a multicast transmission.
Ease of conﬁguration. FRM avoids the contentious selection
of RPs and new inter-domain protocols, instead piggybacking mem-
bership state over BGP.
Centralized route construction. In FRM, the multicast tree
is computed in its entirety by the source’s border router using exist-
ing unicast routes. This not only eliminates the need for a separate
multicast routing algorithm but also spares us new routing anoma-
lies [6, 7].
General service model. FRM supports a multi-source service
model with efﬁcient source-rooted trees.
The key challenges FRM faces include:
State requirements. FRM incurs the overhead of advertising
and maintaining group membership. While true for all multicast
protocols, FRM disseminates membership information more widely
than traditional protocols and hence incurs greater overhead. Specif-
ically, group state in FRM is aggregated per destination preﬁx rather
than on the basis of topology.
Unorthodox packet forwarding. Traditional packet forward-
ing involves a (longest preﬁx match) lookup on the destination ad-
dress to obtain the next hop along which to send the packet. By
contrast, in FRM, ﬁnding the next hop(s) requires that the access
border router scan its entire BGP table and that intermediate nodes
decipher the encoded tree. FRM faces the challenge of achieving
this in a manner that is both scalable and amenable to high-speed
forwarding.
Bandwidth overhead. FRM’s use of what is effectively a form
of multicast source routing incurs additional bandwidth costs.
The remainder of this paper presents the design and evaluation
of a protocol that addresses the above concerns.
5. DESIGN
The design of FRM comprises two (mostly separable) compo-
nents – group membership discovery and multicast packet forward-
ing. This section presents our solutions for each along with a qual-
itative evaluation of their resource requirements. Our design as-
sumes the license to quite signiﬁcantly modify a router’s inter-
nal operation though we do not modify unicast processing and re-
quire only a modest (and hardware-friendly) upgrade to forwarding
plane. This appears reasonable given vendors’ past willingness to
incorporate new multicast routing into their routers. Our discussion
of the overhead due to packet processing in routers follows standard
assumptions – that high speed forwarding is assisted if packets are
processed entirely on line cards and that the memory and process-
ing available at the route processor may be comparable to high end
machines but is more limited at line cards.
5.1 Advertising Group Membership
To leverage unicast routes, group membership information must
be maintained at the same granularity as unicast routing destina-
tions. For this, FRM augments BGP to include per-preﬁx group
membership information. A border router augments its BGP ad-
vertisements with a description of the group addresses active – i.e.,
with at least one member host – within its domain. Because simple
enumeration leaves little opportunity for scaling to large numbers
of groups, we encode active group addresses using bloom ﬁlters
which allow advertisements to be compressed in a manner that in-
troduces false postives but no false negatives and hence never re-
sults in service being denied to valid group members. The pos-
sibility of false positives however implies that a domain may on
occasion receive trafﬁc for a group it has no interest in. To handle
this, the receiving domain R can either simply drop the unwanted
trafﬁc or, similar to DVMRP, can inform the upstream domain U to
cease forwarding trafﬁc for that particular group. This latter can be
implemented by installing an explicit ﬁlter rule at U or by having R
recode its advertisement to U into multiple bloom ﬁlters such that
the offending false positive is eliminated. In this paper, we assume
the use of ﬁlter rules.
Through the intra-domain multicast protocol (Section 5.3), a bor-
der router discovers which groups are active in its local domain and
encodes these addresses into a group bloom ﬁlter, denoted GRP BF.
The length of a GRP BF is selected by reasoning in terms of the
number of ﬁlter entries an AS is allowed by its upstream ASes.
Each false positive results in a ﬁlter being installed at the upstream
provider’s network and hence, if an AS is allowed f upstream ﬁl-
ters, then we set its target false positive rate to MIN(1.0, f /(A−G))
where G is the number of groups to be encoded and A is the total
size of the multicast address space. This choice follows from the
observation that a false positive can only be triggered by one of
A− G addresses which improves scalability by allowing for appro-
priately smaller GRP BFs at large G; e.g., a domain with G ∼ A
ought only use a single bit that tells upstream domains to forward
all multicast trafﬁc its way. The ﬁlter size L is then computed using
the above false positive rate. For efﬁcient manipulation (compres-
sion, aggregation, expansion), we require that L be a power of two
and assume a well known maximum length Lmax.
A border router then piggybacks GRP BFs on its regular BGP ad-
vertisements. If customer preﬁxes are aggregated, a corresponding
aggregate GRP BF is computed as the bitwise-OR of the individ-
ual customer GRP BFs . Finally, based on its available memory, a
router can independently choose to compress a GRP BF of length L
by repeated halving wherein the ﬁlter is split in two halves that are
then merged by a bitwise-OR. Inversely, a previously compressed
bloom ﬁlter can be expanded by repeated concatenation to obtain
the desired length. Of course, both aggregation and compression
result in a corresponding increase in the false positive rate.
Memory requirements. The total memory due to GRP BF state
at a participant border router is on the order of the number of desti-
nation preﬁxes times the average GRP BF length. This can be non-
trivial – for example, our evaluation in Section 6 estimates GRP BF
memory for 170,000 preﬁxes and 1 million active groups at ap-
proximately 2 GB. Fortunately, FRM’s forwarding scheme does not
require that GRP BF state be stored in the forwarding tables on in-
dividual line cards and instead places GRP BF state in the BGP RIB
on the route processor. As such, the main impact due to the mem-
ory requirements for GRP BF state is the monetary cost of memory.
At even current memory prices, this should to be a minor increment
to overall router costs [5].
Bandwidth and processing requirements. In keeping with
the incremental nature of BGP, changes in GRP BFs are communi-
cated as deltas and hence the rate of updates depends primarily on
the rate at which groups are added to, or removed from, a GRP BF.
Advertisements are for the domain as a whole and hence require
updating only when the number of group members drops below
one or rises above zero and hence unlikely to ﬂuctuate rapidly, par-
ticularly if withdrawals are damped (as is likely [32]). Moreover,
updates are small – on the order of the number of bloom ﬁlter hash
functions for each added/deleted group.
In terms of processing,
GRP BF updates, unlike BGP route updates, do not trigger route re-
computations and only rarely require updating the actual forward-
ing tables on line cards (we describe when this is needed in the
following section). Instead, processing GRP BF updates is largely a
matter of updating the BGP RIB in the route processor’s memory.
Thus, both the frequency and processing overhead due to GRP BF
updates should be tractable.
5.2 Multicast Forwarding
FRM processes packets differently at the border router in the
access domain for the source (Rs), and border routers in the transit
core (Rt). We discuss each in turn.
Forwarding on GRP BF state at Rs. A packet multicast by
source s to a group G is delivered via the intra-domain multicast
routing protocol to Rs, the border router in the source’s domain. Rs
scans its BGP RIB, testing each GRP BF entry to identify the des-
tination preﬁxes with members in G and constructs the AS-level
multicast tree T (G) from the union of the individual AS-level paths
to each member preﬁx. T (G) can be computed in O(p×d) where p
is the number of preﬁxes and d the average AS path length. We as-
sume these operations are performed by the route processor where
GRP BF state is stored. While rather expensive, two factors render
this computational complexity manageable. First is simply that, as
an access router, Rs is under less forwarding load (in terms of both
number of groups and total packets) than core routers and is hence
better positioned to absorb this overhead. Second, and more valu-
able, is that Rs can cache, or even precompute, the results of the
lookup so that this computation is only invoked on the ﬁrst packet
sent to each group. Thus, the complexity of lookups on GRP BF
state is incurred only by access border routers and, even there, only
once for each group with active sources in the local domain.
Forwarding on cached state at Rs. As described above,
Rs caches the results of the initial lookup on a group address G.
Cached forwarding state is indexed by group address and hence
accessed by exact-match lookups. Many well-known techniques
exist for efﬁcient exact-match lookups and we assume that FRM
would employ any of these as appropriate – e.g., CAMs and direct-
memory data structures offer O(1) exact-match lookups while more
compact data structures achieve exact-match lookups in logarith-
mic time [33, 34]. The total memory requirements for cached for-
warding state depends on the number of groups with active sources
within the domain and the per-group forwarding state. The latter of
these depends on the size of the tree T(G) (we enumerate the exact
forwarding state Rs must cache in the discussion on forwarding at
Rt that follows). Our evaluation in Section 6 suggests that this state
could be mostly accommodated in RAM on line cards – for ex-
ample, our evaluation estimates a 400MB cache for a domain that
has simultaneously active sources for 1 million groups [35–37]. If
the memory on line cards cannot accommodate the entire cache,
one might only cache state for high data rate groups on line cards
leaving the route processor to handle forwarding for low data rate
groups. Our implementation achieves this with LRU cache replace-
ment.
In summary, caching replaces the linear scan of the BGP RIB’s
GRP BF state by an exact-match lookup on cached forwarding state
and, if needed, should be mostly achievable in line cards. We note
that Rs maintains per-group forwarding state. However, as men-
tioned earlier, we believe this scaling is reasonable here because
the number of groups (with active sources) in Rs’s domain is likely
lower than in core transit domains. In fact, the intra-domain multi-
cast protocol is likely to impose similar scaling.
Forwarding at Rt. Multicast delivery is now a matter of for-
warding the packet along T (G), the AS-level tree computed by Rs,
with appropriate packet replication at fanout domains. However,
as described in Section 4, Rs cannot simply forward the packet to
each of its next hop ASes on the tree as an interior AS does not
know which subset of destination preﬁxes it should in turn forward
to. Moreover, such an approach would impose forwarding state
and complexity akin to that at Rs on all routers – a scenario we’ve
argued against. We instead adopt an approach in which Rs commu-
nicates T(G) to intermediate routers. FRM implements this using
a “shim” header above the IP header into which Rs encodes the
edges from T (G). A tree edge from autonomous system A to B is
State
GRP BFs
cached
GRP BFs
encoded
links
scaling
O(|p|.g)
O(gs.T(gs))
AS
degree
lookup
linear
scan
exact
match
ﬁlter
match
used
at
Rs
Rs
Rt
stored
in
route
proc.
line
card
line
card
when
used
per
group
per pkt
per pkt
|p| is the total
Table 1: FRM: packet processing requirements.
number of preﬁxes at a BGP router and g the average groups per
preﬁx. gs is the number of groups with active sources in domain
s and T (gs) the average size of the dissemination trees for groups
with source in s.
assigned the unique label ‘A:B’ and Rs encodes these edge labels
into the shim header it constructs for each of its next hops. Hence,
in Figure 1, Rs would encode ‘Q:P’, ‘P:X’ and ‘P:Y’ in its packets
to R1 and ‘U:Z’ in those to R2. Note that our choice of encoding
edge labels is actually crucial in allowing Rs to disambiguate for-
warding responsibility amongst interior ASes and allows the shim
header inserted at Rs to be carried unchanged all the way to the des-
tination(s) with no updating at intermediate routers (e.g., this would
not be possible were Rs to encode only the AS numbers of nodes in
the tree).
For scalability reasons similar to those discussed in Section 5.1,
we encode the dissemination tree into the shim header using a
bloom ﬁlter (denoted TREE BF) and deal with false positives as
described in Section 5.1. However, unlike the GRP BF advertise-
ments, we require that the TREE BF be of ﬁxed length – or one of
a small set of well-known lengths – so as to be amenable to fast
processing in hardware. This raises the issue of picking an appro-
priate TREE BF length. A too small header can lead to high false
positive rates for large groups while a TREE BF length selected to
accommodate even the largest groups would be needlessly wasteful
in the per-packet overhead the shim header imposes. Our solution
instead is the following: we pick a ﬁxed TREE BF size of h bits,
a target false positive rate f and compute e, the number of edges
that can be encoded in h bits while maintaining a false positive rate
≤ f . We then use a standard bin-packing algorithm to decompose
the tree into groups of subtrees such that the number of edges in
each group is less than e. This bin-packing can be computed in a
single (typically partial) pass over T (G). Each group of subtrees is
then encoded into a single shim header and transmitted as a sepa-
rate packet. Note that this approach can cause certain links to see
multiple copies of a single multicast transmission.
The “tax” due to our source-encoded forwarding is thus twofold
(we quantify these in Section 6):
• in its bandwidth consumption, source-encoded forwarding
can be more inefﬁcient than traditional multicast due to the
per-packet shim header and redundant transmissions (on cer-
tain links) for groups too large to be encoded into a single
shim header.
• the per-group forwarding state cached at Rs must now in-
clude the shim header(s). I.e., for each cached group G, Rs
caches the list of next hop ASes and the shim header(s) asso-
ciated with each next hop.
in which case it forwards a copy of the packet to B. This offers two
advantages. The ﬁrst is that Rt’s “forwarding” state is essentially a
list of its neighbor edges. This state is independent of any partic-
ular group G and hence the number of such forwarding entries at
Rt depends only on its domain’s AS degree. Measurements report
per-domain AS degree distributions ranging from 1 to under 10,000
with a power-law distribution and hence we can expect the number
of forwarding entries at Rt to be low– potentially several orders of
magnitude lower than the number of multicast groups – and easily
accommodated on line cards.
For efﬁcient packet processing, we store neighbor edges in their
encoded representation; i.e., each edge is inserted into, and stored
as, a separate TREE BF bloom ﬁlter. The lookup operation at Rt is
then similar to standard ﬁlter matching – for each neighbor edge,
Rt checks whether the corresponding bits are set in the packet’s
TREE BF. There are a variety of options by which to implement this
but perhaps the simplest is to use TCAM with the bloom ﬁlter for
each neighbor edge stored in one TCAM row and all zero bits set to