### Machine Vulnerability and Risk Assessment

A machine is considered clean if it has not downloaded malicious files, contacted suspicious URLs, or had any malware processes running. Conversely, a machine will be assigned a high risk score (R) if it has previously downloaded a significant number of malicious files, contacted bad URLs, or had malware processes running. In such cases, the machine is likely to be vulnerable and may access malicious content again in the future.

### 4.2 System Operation

This section describes the training process for behavioral classifiers and the overall system operation. The system provides real-time classification results for files and URLs observed on a daily basis. By combining the classification results of files and URLs, Mastino can classify entire download events, represented as 3-tuples (u, f, m), where u is a URL, f is a file, and m is a machine.

To achieve this, the system automatically classifies new items (files and URLs) on the current day, leveraging historical data gathered from the previous days within a time window T. The historical data is used to construct an augmented tripartite download graph, where all nodes (files, URLs, and machines) are assigned an R value. We maintain a sliding window over all download events and set T to 10 days. This means that the beginning of T is 10 days before the start of the current day, \(d_c\).

For any unknown file or URL node on \(d_c\), we compute its feature vector using the procedure described in Section 3.2. This feature vector is then fed into the appropriate behavioral classifier: unknown files are classified by the file classifier, and unknown URLs are classified by the URL classifier. The classifier produces a score, which is compared against a previously learned detection threshold. If the score exceeds the threshold, the unknown node is labeled as malicious.

By combining the classification results for files and URLs, Mastino can detect malicious download events \(d = (u, f, m)\) where either u or f is labeled as malicious. Figure 5 illustrates how the system operates by maintaining a sliding window over all download events during T, generating a download graph, and training behavioral classifiers. The window T slides forward to train new classifiers for subsequent days and classify new and unknown files and URLs.

### 4.3 Training the Classifiers

To train the file and URL classifiers for use on the current day \(d_c\), we use the knowledge from the previous days within the time window T. The training dataset for the file classifier, for example, includes labeled feature vectors for all known good and bad file nodes in the download graph during T (see Figure 5). Section 3.2 explains the computation of feature vectors for files and URLs.

**Behavior-based Feature Computation During Training:**
For known file and URL nodes used for training, part of their behavior-based features are based on the R values of connected machines. However, these R values are computed based on the history of the files and URLs connected to them (Section 4.1). Using the assigned R values of machines to compute the behavior-based features for files and URLs during training would result in information leakage, giving the classifier an unfair advantage. To address this, we temporarily ignore the impact of a known node n on the R values of machines when computing its behavior-based features for the training dataset, treating n as if it were unknown.

### 5. Evaluation

We conducted extensive experiments to evaluate the system. This section first reports statistical information about the data, followed by details on the training and testing experiments. We then discuss the labeling of download events and the evaluation of the entire system. Finally, we present the results of feature analysis, efficiency measurements, and a detailed analysis of our findings with some interesting case studies.

### 5.1 Data Collection

The data for this work was collected from December 2013 to August 2014 by DIA agents from Trend Micro customers. The collected data includes download events \(d = (u, f, m)\) and associated network- and system-level information, as explained in Section 2. Over a time window T, Mastino generates a tripartite download graph. Table 2 and Table 3 provide statistics about the nodes and edges in the download graphs generated over multiple 10-day periods for each month of data.

### 5.2 Train and Test Experiments

This section presents the evaluation results of the system operation as discussed in Section 4.2. We define a time window T that tracks all download events during T. Our goal is to use trained classifiers based on labeled nodes in the download graph during T to detect new and unknown nodes on the current day, \(d_c\). Thus, \(d_c\) is our test day.

#### 5.2.1 Training and Test Datasets

First, we generate a training dataset using all known nodes in the graph during the training time window T, following the procedure detailed in Section 4.3. To prepare the test datasets and simulate real-world operation, we consider the nodes on \(d_c\) that were not present during T. This ensures that no information about the test samples was used during training, simulating the system's operational mode of labeling new and unknown nodes on \(d_c\).

We prepare three different groups of test datasets for files and URLs using nodes on \(d_c\):

- **New Nodes - \(F_n\) and \(U_n\):** \(F_n\) contains all files from download events \(d = (u, f, m)\) on \(d_c\) where the file f was never seen during T, but the URL u or machine m appeared during T. Similarly, \(U_n\) includes all new URLs from download events on \(d_c\) where the file f or machine m were seen during T.
- **New Download Events - \(F_e\) and \(U_e\):** \(F_e\) contains all files from download events \(d = (u, f, m)\) on \(d_c\) where none of the nodes appeared during T, but some ground truth exists for the URL u or machine m. For example, \(F_e\) might include a file connected to a new URL u and a new machine m, where m downloaded other malicious files from bad URLs on \(d_c\) and was labeled as bad. Similarly, we generate the test dataset \(U_e\) for URLs.
- **New Unknown Download Events - \(F_u\) and \(U_u\):** \(F_u\) contains all files from download events \(d = (u, f, m)\) on \(d_c\) where none of the nodes were observed during T, and no ground truth is available for the URL u and machine m. The test dataset \(U_u\) for URLs is constructed similarly.

#### 5.2.2 Multi-day Train and Test Evaluation

We perform train and test experiments to evaluate the generalization capabilities of our behavioral classifiers. First, we generate the three groups of test datasets for various test days. Then, we evaluate the performance of the trained classifiers on these test datasets.

For a single test day \(d_c\), we feed all file test samples in \(F_n\), \(F_e\), and \(F_u\) to the file classifier and record the prediction scores for each file in each test dataset separately. These scores are used to evaluate the performance of the classifiers on each specific test dataset on a single test day \(d_c\). To demonstrate the consistency of the classifier results, we combine the classification scores over multiple days and report the aggregate results.

We choose t testing periods, \(TP_Ri\), i = 1, 2, ..., t, each including k consecutive test days, \(d_{i,j}\), j = 1, 2, ..., k. For a specific testing period, \(TP_Rx\), we perform train and test experiments for each \(d_{x,j}\), j = 1, 2, ..., k. First, we train a set of classifiers by building a download graph over the time window T that ends just before \(d_{x,j}\) (e.g., a time window spanning 10 days before \(d_{x,j}\)). The classifiers are used to produce prediction scores for the test nodes on \(d_{x,j}\). The time window T is then slid forward to compute prediction scores for all k days belonging to testing period \(TP_Rx\). We store these prediction scores \(P_{x,j}\), j = 1, 2, ..., k, and finally, we aggregate all the prediction scores for the testing period \(TP_Rx\) and report the results.

To select the most suitable classification algorithm for Mastino, we experimented with different statistical classifiers and chose Random Forest [7] as it consistently provided the best results. Figure 6 shows the results of these experiments for files and URLs using the three groups of test datasets for seven (t = 7) testing periods selected at random from various months of our data. Each testing period contains 5 consecutive test days (k = 5), resulting in train and test evaluations for 35 days in total. We compute the ROC curves by varying the detection threshold on the classifierâ€™s output scores. Note that the ROCs show the true positive (TP) rates for false positives (FP) less than 1%.

As observed from the ROCs, the file and URL classifiers perform very well even for very low FPs. For example, the file classifier, when tested on \(F_e\), achieved an average TP of 90% while incurring only 0.5% FP, and even for an FP of 0.1%, the average TP is 82.5%. For URLs in \(U_e\), the average TP is 86.5% and 96% for FPs of 0.1% and 0.5%, respectively. The results for tests on \(F_u\) and \(U_u\) are particularly impressive, given that these test samples have no prior information.