that the machine is a clean one. In contrast, a machine will
be assigned a high R if in the past downloaded enough bad
ﬁles, contacted some bad URLs, or had malware processes
running. In this case, it is likely that this machine is vulner-
able and will access malicious content again in the future.
4.2 System Operation
In this section we describe how the behavioral classiﬁers
are trained and how the system operates. The system pro-
vides real-time classiﬁcation results for ﬁles and URLs ob-
served on daily basis. Furthermore, by combining the clas-
siﬁcation results of ﬁles and URLs, Mastino enables the
classiﬁcation of the entire download events, d = (u, f, m),
3-tuples of URLs, ﬁles, and machines. To do so, the system
automatically classiﬁes new items (ﬁles and URLs) on the
current day harnessing historical knowledge gathered from
the previous days in a time window T . The historical knowl-
edge is, in fact, the augmented tripartite download graph
that associates the items of download events together where
all nodes are assigned an R value. We keep a sliding win-
dow over all the download events and set T = 10 days. That
is, the beginning of T is set to 10 days before the start of
current day, dc. So we are interested in classifying all un-
known nodes observed on dc using the download graph that
is generated by considering all download events during T
(see Figure 5). Note the decision on the length of T is set
based on our evaluations which are reported in Section 5.6.
For any unknown ﬁle and URL node in dc, we compute its
feature vector by following the procedure described in Sec-
tion 3.2, and feed the feature vector of the unknown node to
the related behavioral classiﬁer, i.e. an unknown ﬁle will be
fed to the ﬁles classiﬁer and an unknown URL will be fed
to the URLs classiﬁer. The classiﬁer in return produces a
score which will be compared against a previously learned
detection threshold. If the produced score is above the de-
tection threshold the unknown node will be labeled as ma-
licious. Eventually, by combining the classiﬁcation results
for ﬁles and URLs, Mastino detects malicious download
events d = (u, f, m) where u or f were labeled as bad by
the classiﬁers. Figure 5 shows how does the system operate
by keeping a sliding window over all the download events
during T and using them to generate a download graph and
train behavioral classiﬁers. The window T slides forward to
train new classiﬁers for subsequent dc days and classify new
and unknown ﬁles and URLs.
Figure 5: System Operation
4.3 Training the Classiﬁers
In order to properly train the ﬁle and URL behavioral
classiﬁers to be used for detection on current day, dc, we
use the knowledge from previous days during T in the graph
to prepare training datasets of known ﬁles and URLs. The
training dataset for ﬁle classiﬁer, for instance, contains la-
beled feature vectors for all known good and bad ﬁle nodes
in the download graph during training time window T (see
Figure 5). Section 3.2 explains how feature vectors for ﬁles
and URLs are computed.
Computing Behavior-based Features During Train-
ing. Note that for known ﬁle and URL nodes that are used
for training, part of their behavior-based features are based
on R of machines connected to them. However, the R of ma-
chines was computed according to their history, i.e. the ﬁles
and URLs connected to them in the ﬁrst place (Sect.4.1).
So if we simply use the assigned R of machines to compute
the behavior-based features for ﬁles and URLs during train-
ing, we unfairly give the classiﬁer an advantage. In machine
learning, this phenomenon is known as information leakage.
To resolve this issue, before we compute behavior-based fea-
tures for a known node n, to be included in the training
dataset, we temporarily ignore any impact n had on deter-
mining R of machines, as if n was unknown.
5. EVALUATION
We performed numerous experiments to fully evaluate the
system. In this section, ﬁrst, we report some statistical in-
formation about the data. Second, we explain our train and
test experiment. Next, we discuss the labeling of download
events and evaluating the whole system. Then we present
the results of feature analysis and eﬃciency measurements.
Time   Day 1  Day 2Current day (dc)...       End of time window T  (e.g. Day 10)Real-time classification offiles & URLsTrain ClassifiersFile ClassifierURL ClassifierClassification SystemUMDetection of Malicious Download EventsStart of time window TAll download events during Time Window TFFUMUFMUFM788Finally, we present a detailed section on analyzing our re-
sults and some interesting case studies.
5.1 Data Collection
This work is based on the data collected from Dec 2013 to
Aug 2014 by the DIA agents from customers of Trend Micro.
The collected data contains download events d = (u, f, m)
and their associated network- and system-level information,
as explained in Section 2. During a time window T and
using the download events, Mastino generates a tripartite
download graph. Table 2 reports statistics about the nodes
in various tripartite download graphs over multiple T = 10
days for each month of data. Table 3 reports the number of
events that were observed during T as well as the number of
edges of the generated download graphs. The “Date” column
reports the month that the days of the time window belong
to.
Table 2: Node statistics for download graphs generated dur-
ing sample T = 10 days for each month of the data
Date
Files
URLs
Machines
Total Benign Malware Total Benign Malware Total Clean Vulnerable
Jan 144,435 1,976
Feb 127,369 2,040
Mar 120,584 1,801
Apr 102,922 1,732
1,643
May 96,289
1,708
Jun 79,310
Jul
74,543
1,622
1,021
1,668
1,432
3,744
2,904
1,875
1,479
124,306 15,121
112,310 12,056
106,041 11,291
99,883 12,092
92,665 12,707
77,401 15,338
73,434 11,591
39,183
37,266
34,596
32,594
27,174
23,424
22,775
121,177 431
110,231 956
100,098 1,347
780
92,696
877
84,347
590
69,881
65,646
868
19,533
17,236
13,882
16,998
15,299
16,544
13,005
Table 3: Download events and graph size statistics generated
during sample T = 10 days for each month of the data
Date Download events Unique Download events Edges
Jan
Feb
Mar
Apr
May
Jun
Jul
2,916,292
2,590,943
2,402,586
2,167,115
2,008,174
1,658,350
1,555,636
385,939
291,940
256,076
257,426
253,107
182,960
189,936
190,021
168,376
154,980
142,807
130,570
108,014
102,649
5.2 Train and Test Experiments
In this section we show the evaluation result of system
operation as it was discussed in Section 4.2. To this end
we have deﬁned a time window, T , which keeps track of all
download events that happened during T . We are interested
in using trained classiﬁers based on labeled nodes in the
download graph during T to enable detection of new and
unknown nodes on the current day, dc. So dc is our test day.
We ﬁrst discuss the preparation of training and test datasets
for a single day of experiment and then present the results
over multiple days of performing the tests.
5.2.1 Training and Test Datasets
First we generate a training dataset using all the known
nodes in the graph during training time window T by fol-
lowing the procedure detailed in Section 4.3. To prepare the
test datasets and to replicate the real-world operation of the
system we proceed as follows. To evaluate the classiﬁers, we
consider the nodes on dc, the test day, that were not present
during time window T . This ensures that no information re-
garding the test samples were ever used during training and
properly simulates the operative mode of the system where
we are only interested in labeling new and unknown nodes
on dc. We prepare three diﬀerent groups of test datasets for
ﬁles and URLs using nodes on dc as follows:
New Nodes - Fn and Un: Fn contains all ﬁles belonging
to download events d = (u, f, m) from test day, dc, for which
the ﬁle f was never seen during training time window, T ,
but the URL u or machine m appeared during T . Similarly,
Un is composed of all new URLs from download events on
dc, but the ﬁle f or machine m were seen during T .
New Download Events - Fe and Ue: Fe contains all ﬁles
belonging to download events d = (u, f, m) observed on dc
for which none of the nodes were ever appeared during the
training period T , however, some ground truth exists for the
URL u or machine m. For example, Fe might contain a ﬁle
that is connected to a new URL u and a new machine m
where m downloaded some other malicious ﬁles from some
bad URLs on that same test day dc, and, therefore, it was
labeled as bad on dc. In a similar fashion, we generate test
dataset Ue for URLs.
New Unknown Download Events - Fu and Uu: Fu
contains all ﬁles belonging to download events d = (u, f, m)
appeared on dc for which none of the nodes ever observed
during the training period T and no ground truth is avail-
able for the URL u and machine m whatsoever. URLs test
dataset Uu is constructed similarly.
5.2.2 Multi-day Train and Test Evaluation
We perform train and test experiments to evaluate the
generalization capabilities of our behavioral classiﬁers. First,
we generate all three groups of test datasets described in
Section 5.2.1 for various test days. Then we evaluate the
performance of our trained classiﬁers on the test datasets.
For example, for a single test day dc, we feed all ﬁle test
samples in Fn, Fe, and Fu to the ﬁle classiﬁer and record
the prediction scores for each ﬁle in each test dataset sepa-
rately. These scores are used to evaluate the performance of
the classiﬁers on each speciﬁc test dataset on a single test
day dc. However, we like to demonstrate that the classiﬁer
results are consistent for the entire dataset. So we combine
the classiﬁcation scores over multiple days and report the
aggregate results as follows.
We choose t testing periods, T P Ri, i = 1, 2, . . . , t, each
including k consecutive test days, di,j, j = 1, 2, . . . , k. For a
speciﬁc testing period, T P Rx, we perform train and test ex-
periment for each dx,j, j = 1, 2, . . . , k. That is, ﬁrst we train
a set of classiﬁers by building a download graph over the time
window, T , that ends just before dx,j (e.g., a time window
that spans 10 days before dx,j). The classiﬁers are used to
produce prediction scores for the test nodes on dx,j. The
time window T is then slid forward to compute prediction
scores for all k days belonging to testing period T P Rx. Then
we store these prediction scores Px,j, j = 1, 2, . . . , k. Finally,
we aggregate all the prediction scores for the testing period
T P Rx and report the results. To choose the most suitable
classiﬁcation algorithm for Mastino, we experimented with
diﬀerent statistical classiﬁers and picked Random Forest [7]
as it consistently provided the best results. Figure 6 shows
the results of these experiments for ﬁles and URLs using the
three groups of test datasets for seven (t = 7) testing periods
selected at random from various months of our data. Each
testing period contains 5 consecutive test days (k = 5), re-
sulting in train and test evaluation for 35 days, in total. We
compute the ROC curves by varying the detection threshold
on the classiﬁer’s output scores. Also note that the ROCs
show the true positive (TP) rates for false positives (FP)
less than 1%.
789(a) Fn test ﬁle nodes
(b) Fe test ﬁle nodes
(c) Fu test ﬁle nodes
(d) Un test URL nodes
Figure 6: Multi-day train and test for ﬁles (Fn, Fe, Fu) and URLs (Un, Ue, Uu) on seven diﬀerent testing periods (FP∈
[0%, 1%])
(e) Ue test URL nodes
(f) Uu test URL nodes
As it can be observed from the ROCs, the ﬁle and URL
classiﬁers perform very well even for very low FPs. For
example, the ﬁle classiﬁer when tested on Fe on average
achieved 90% TP while incurring only 0.5% FP, and even
for FP of 0.1%, average TP is 82.5%. For URLs in Ue the
average TP is 86.5% and 96% for FPs 0.1% and 0.5%, re-
spectively. Especially, the results of tests on Fu and Uu are
remarkably good. Remember that test samples of Fu and