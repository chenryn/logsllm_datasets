## 实时设计  
前面讲了如何高效的获得用户，接下来我们要看看如何实时的更新TAG了。    
### 流处理  
目的是实时的更新用户的TAG，比如一个用户，一天可能产生几万比浏览的跟踪记录，这些记录要合并到他的标签中。     
如果活跃用户达到亿级别，那么一天产生的更新流水就达到了万亿级别，这个怎么能实时的在数据库中处理呢？估计很多用户会使用T+1的方式，放弃实时性。     
但是实际上，并不是做不到的，比如我们可以使用PostgreSQL数据库的流处理功能来实现这种超高流水的更新。     
你可能要疑问了，数据库能处理流吗？数据如何在数据库中完成实时的更新呢?     
PostgreSQL社区的一个开源产品pipelinedb，（基于postgresql，与postgresql全兼容），就是用来干这个的，它会帮你实时的进行合并，（用户可以设置合并的时间间隔，或者累计的ROWS变更数）达到阈值后，进行持久化的动作，否则会先持续的在内存中进行更新。     
有两篇文章可以参考     
[《"物联网"流式处理应用 - 用PostgreSQL实时处理(万亿每天)》](../201512/20151215_01.md)     
[《流计算风云再起 - PostgreSQL携PipelineDB力挺IoT》](./20161220_01.md)     
当然如果用户没有实时的要求，T+1 就能满足需求的话，你大可不必使用pipelinedb.    
### 为什么要在数据库中完成流式处理  
我们知道，标签数据最后都要进到数据库后，才能施展数据库的圈人功能，完成圈人的查询，如果不在数据库中实现流计算，而是使用类似JSTROM的框架的话，实际上是使用JSTROM挡了一层，比如将1000亿次的更新转化成了1亿的更新。  
但是使用外部的流处理会引入一些问题  
1\. 额外增加了JSTROM所需的计算资源，并行效率实际上还不如pipelinedb  
2\. 用户查数据的时效性不如直接放在数据库中的流计算  
3\. 增加了开发成本  
## 压测  
进入压测环节，我选择了一台32CORE，2块SSD卡，512GB的内存的机器进行压测。     
存放3.2亿用户，每个用户4个数组字段，每个字段包括1000个元素，即4000*3.2亿 = 1.28万亿 user_tags。     
### 用例1  
10张表，每张表存储1000万用户，4个标签字段，使用tsvector存储标签。     
使用rum索引。    
```  
postgres=# create tablespace tbs1 location '/u01/digoal/tbs1';  
CREATE TABLESPACE  
postgres=# create tablespace tbs2 location '/u02/digoal/tbs2';  
CREATE TABLESPACE  
do language plpgsql $$  
declare  
  i int;  
  suffix text;  
  tbs text;  
begin  
  for i in 0..10 loop  
    if i=0 then  
      suffix := '';  
      tbs := 'tbs1';  
    elsif i >=1 and i/dev/null 2>&1 &  
```  
标签由500万个唯一ID+20个唯一ID的组合过程，每个tsvector中存放1000个这样的组合。    
### 用例2  
10张表，每张表存储1000万用户，4个标签字段，使用text[]存储标签。     
索引使用的是GIN索引，其他与用例1一致。     
```  
do language plpgsql $$  
declare  
  i int;  
  suffix text;  
  tbs text;  
begin  
  for i in 0..10 loop  
    if i=0 then  
      suffix := '';  
      tbs := 'tbs1';  
    elsif i >=1 and i=1 and i test$i.sql  
echo "insert into test$i (uid,s1,s2,s3,s4) select :uid, (select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s1,(select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s2,(select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s3, (select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s4 on conflict do nothing;" >> test$i.sql  