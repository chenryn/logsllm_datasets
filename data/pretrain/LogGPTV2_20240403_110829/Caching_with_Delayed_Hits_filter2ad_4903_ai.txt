0 (T)
(i)
cch(T −1)≤C.
0 (T)≤C = 
(i)
mem(T +1) =RHS.
(43)
(44)
(45)

(i)
f
x
i∈[M]
i∈[M]
This verifies the constraints (7)–(9) and proves that the cache sched-
ule defined in (31) is feasible.
□
Once we have Lemma 1, the only thing left is to show that the
MCMCF problem and the latency minimization problem have the
same objective function. This is easy to see once we compare the
objective functions (15) and (14) and apply Claim 2 from the proof
of Lemma 1.
A.3 Optimizations to Reduce Complexity
In this section, we provide implementation details of belatedly
for reducing complexity. Our overall approach is illustrated in Fig-
ure 6.
A.3.1 Pruning and Merging.
While the MCMCF formulation is conceptually simple, a naive imple-
mentation of the algorithm has serious practical limitations. Observe
that the number of flow variables in the MCMCF formulation is O(N ·
M). For a request sequence of size N =250,000 containing M =20,000
objects, the number of decision variables alone would be on the order
of 1010. Further, the total number of flow conservation constraints
is O(N ·M) (see (19)–(22)). In Gurobi, where decision variables are
encoded as 64-bit floating-point values, and constraint expressions
as vectors of 64-bit pointers to the relevant decision variables, simply
encoding the model would require well over 400 GB of memory.
In this section, we describe two optimizations to the above formu-
lationthatallowustosignificantlytightentheresourcerequirements
(memory and execution time) for solving the MCMCF problem and
to make it more tractable. Our goal is to be able to compute be-
latedly on a 32-core x86 server with 128 GB of RAM, for request
sequences containing N ≈250,000 requests, M ≈50,000 objects, and
any combination of z and C.
Caching Intervals. Since the majority of decision variables stem
from either (Vcch,n,Vcch,n+1) (cache-to-cache) or (Vcch,n,Vmem,x)
(cache-to-memory) edges, we first attempt to reduce the number
of elements in these sets. The key idea here is that, for each object,
the request sequence can be partitioned into disjoint intervals
(composed of one or more consecutive timesteps) where belatedly
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
Optimized caching intervals for a:
Naive caching intervals for a:
x1
x1
x2
x′
2
x3
Vcch,t−k
Vcch,t−1
x4
...
...
Vcch,t +3
a
...
Vcch,t +3 Vcch,t +4 Vcch,t +5 Vcch,t +6 Vcch,t +7
...
a
a
...
...
1
2
3
a
a
Vmem,t +4
Vmem,t +6
...
a
Vmem,t
Figure 23: A fragment of a request sequence highlighting nodes and
edges corresponding to object a (colored red), with Z =3.
is never incentivized to change its caching decision for that object;
we call these caching intervals.
To concretize this notion, consider the subproblem depicted in Fig-
ure 23. Per the original MCMCF formulation, there are four distinct
decision variables on edges between cache vertices corresponding
to a (denoted by x1, x2, x3, and x4). Now, consider the possibility of
routing flow along edges labeled 1, 2, and 3. All three edges have the
same capacity, cost-per-unit-flow, and destination node. Effectively,
the latency cost incurred by evicting a using any of these edges is
identical. However, observe that routing a’s flow along edge 2 in-
volves keeping a in the cache for one timestep longer than routing
it along edge 1. Similarly, routing a’s flow along edge 3 involves
keeping it in the cache for two additional timesteps. Since deferring
the eviction consumes valuable cache space (but yields no tangible
benefit in terms of latency cost), it is strictly better to evict a using
edge 1 (at timestep t +4) than using edges 2 or 3.
This simple observation gives us three major optimization oppor-
tunities. In particular, it enables us to:
• Eliminate the redundant edges 2 and 3 (along with the corre-
sponding decision variables).
• Replace x2, x3, and x4 with a single decision variable, x′
2. Since
edges 2 and 3 no longer exist, any flow entering cch(t +4) must
remaininthecacheuntilcch(t +7);inotherwords,belatedly’s
caching decision remains the same for the entire duration of
the interval [(t +4), (t +7)).
• Eliminate flow conservation constraints involving object a
for nodes cch(t +5) and cch(t +6). In the new representation, for
each object, i, we only need flow conservation constraints for
Vcache nodes corresponding to the end-points of i’s caching
intervals.
Lastly, this representation also allows us to bound the total num-
ber of caching intervals for any request sequence. Let ni denote the
number of requests to object i in a given request sequence of size
N . Observe that an endpoint of object i’s caching intervals is a Vcch
node that either corresponds to i being admitted into the cache, i
being evicted from it, or both. Since there are exactly ni admission
edges corresponding to object i, there must be at least ni endpoints
(or, equivalently, ni −1 intervals) corresponding to i. Conversely, in
the worst case, there are ni −1 additional Vcch nodes which have
eviction edges corresponding to object a. Thus, there may as many as
2ni −1 unique endpoints (or, equivalently, 2ni −2 caching intervals)
corresponding to i. The total number of caching intervals (for all
a
1
2
( a) = 6)
3
= 1 , c
( u
...
a
a
a
a
...
Vmem,t
cost(t) =3+2+1 =6
Figure 24: A fragment of a request sequence highlighting ingress
and egress edges for node Vmem,t , with Z =3.
Vcch,t−k
Vcch,t−1
...
...
a
Vcch,t +3
a
...
(u =1, c(a) =6)
2’
(u =1, c(a) =6)
1’
Figure 25: The optimized representation with backing store nodes
removed.
objects), K, can then be bounded as follows:

(ni −1)≤ K ≤ 
i∈[M]
i∈[M]
⇒ N −M ≤ K ≤ 2(N −M).
2(ni −1)
For a fragment of an empirical trace (CAIDA Chicago, 2014) con-
taining N =250,000 packets and M =37,725 objects (unique flows),
the total number of caching intervals is on the order of 400,000.
Compared to the naive formulation, this optimization reduces the
number of decision variables from 18×109 to 106, and the number
of model constraints from 9×109 to 106.
Optimizing Away Backing Store Nodes. Partitioning the global set
of nodes into cache nodes and backing store nodes is a convenient
abstraction since it allows us to reason about cache evictions and
admissions independently of one another. Unfortunately, this rep-
resentation also adds considerable overhead: excluding sink nodes,
there are N backing store nodes, each of which contributes one de-
cision variable on an edge (Vmem,T ,Vcch,T +Z), as well as one flow
conservation constraint. However, observe that, in our MCMCF for-
mulation, any flow entering a Vmem,T node must be routed to the
corresponding cache node, Vcch,T +Z . This leads us to our next op-
timization: replacing pairs of cache eviction and admission edges of
the form (Vcch,T ,Vmem,x) and (Vmem,x ,Vcch,x +Z) with a single edge
(Vcch,T ,Vcch,x +Z) with unit capacity and cost c(i)(Vcch,T ,Vcch,x +Z) =
c(i)(Vmem,x ,Vcch,x +Z) for object i.
As an example, consider the subproblem depicted in Figure 24.
Here, Vmem,t has two in-edges, labelled 1 and 2, and one out-edge,
labeled 3. Using the optimization strategy discussed above, we can
coalesce edges 1 and 3 into a single edge, 1’, with a capacity of 1 and
a cost-per-unit-flow of c(a) = 6. Similarly, we can coalesce edges 2
and 3 into a single edge, 2’. This effectively disconnects node Vmem,t
from the remainder of the flow graph, and we can safely remove
Caching with Delayed Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 26: Number of decision variables in the naive MCMCF
formulation versus belatedly for different application scenarios.
Our optimizations to the original MCMCF formulation re-
duces belatedly’s memory and compute requirements by
orders of magnitude. In Figure 26 we count the number of decision
variables in the MCMCF formulation given our naive construction
(§3.1) and our pruned version (§A.3.1). For all three application sce-
narios, the number of decision variables is reduced by three to four
orders of magnitude.
Empirically, the formulation provides tight bounds. While
solvinga‘relaxed’versionoftheproblemonlygivesusalower-bound
on the total latency (and not an implementable schedule), our ran-
domized rounding strategy and flow balance heuristics work well in
practice. For each application scenario, we perform 20 runs of belat-
edly sweeping different Z values and cache sizes. Across all three
scenarios, we see a median error of at most 0.05% and a maximum
error of 1.71%. Table 3 lists the relative error between the upper- and
lower-bounds of the solution generated by belatedly.
Mean Err.% Median Err.% Max. Err.%
Network
CDN
Storage
0.017
0.325
0.015
0.004
0.051
0.007
0.124
1.707
0.072
Table 3: Empirical bounds on belatedly’s error (calculated by
comparing the integer upper-bound to the relaxed lower-bound).
it from V . A visual representation of the optimized flow graph is
depicted is Figure 25. Overall, this optimization:
• Eliminates N decision variables corresponding to all N back-
• Eliminates N flow conservation constraints corresponding to
ing store to cache edges.
backing store nodes (excluding sink nodes).
For the aforementioned empirical trace, this optimization reduces
the total number of decision variables and model constraints by
another 25% (down to 750,000 each). Overall, the optimized MCMCF
formulation (expressed in Gurobi C++ format) occupies under 25
GB of memory.17
A.3.2 Rounding to Approximate Integer Solutions.
Recall that, since the integer version of MCMCF is NP-Complete,
we instead opt to solve a fractional (or relaxed) version of the prob-
lem by removing the integrality constraints. However, this often
results in solutions that do not map on to realistic caching strate-
gies.18 In this section, we describe our methodology for extracting
an implementable caching schedule from a fractional solution.
A naive, yet intuitive, strategy is to simply round any non-zero
fractions of evicted flows to 1, thereby always creating enough space
in the cache for the next object to be admitted; unfortunately, this
greedy rounding strategy does not generally work. It is easy to con-
struct request sequences where evicting too much flow results in
a violation of the cache capacity constraint several timesteps later.
Further, attempting to satisfy the constraint by randomly evicting
objects causes the upper-bound on the latency cost to diverge signif-
icantly from the true optimum. belatedly addresses this problem
in two ways:
(1) Instead of rounding all non-zero evicted flow fractions to 1, round-
ing is done with a probability corresponding to the fraction itself
(a form of randomized rounding). In other words, if the fraction of
(i)
evict(T) ∈ [0,1], then we
flow for object i evicted at timestep T is f
(i)
evict(T). This ensures that, in
perform eviction with probability f
expectation, the cache occupancy at any timestep is equal to the total
flow routed along the corresponding edge in the MCMCF solution.
(2) While randomized rounding works well in theory, it does not
guarantee that the cache capacity constraint is satisfied. In order to
enforce this, we introduce the notion of flow balance. The idea is to
track the expected amount of cached flow for each object (according
to the fractional solution); then, at any timestep, if the cache occu-
pancy would exceed the cache size, we evict the flow that is most
unbalanced (deviates the most from its expected cached fraction). In
practice, this is implemented using a priority queue.
A.4 belatedly Performance Evaluation
Recall that we apply two optimizations to make the MCMCF prob-
lem described in §3.1 tractable: first, we prune and merge states in
the flow graph to reduce the number of decision variables (§A.3.1);
second, we solve a ‘relaxed’ version of the problem, followed by
integer rounding (§A.3.2), to ensure that the algorithm terminates
in polynomial time. In this section, we evaluate the benefits of these
optimizations (using the naive MCMCF formulation as a baseline),
as well the impact of rounding on belatedly’s latency upper-bound.
17This includes overheads incurred by Gurobi’s internal data-structures; the raw model
itself is significantly more compact.
18For instance, the optimal fractional solution may involve caching half an object,
which is not particularly meaningful.
NetworkCDNStorage1001031061091012#Decision Variables(Log-scale)10.86B905K1.24B972K3.94B546KNaiveBELATEDLY