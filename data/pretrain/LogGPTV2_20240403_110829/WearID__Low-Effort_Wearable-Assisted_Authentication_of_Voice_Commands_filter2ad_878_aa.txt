title:WearID: Low-Effort Wearable-Assisted Authentication of Voice Commands
via Cross-Domain Comparison without Training
author:Cong Shi and
Yan Wang and
Yingying Chen and
Nitesh Saxena and
Chen Wang
WearID: Low-Effort Wearable-Assisted Authentication of Voice
Commands via Cross-Domain Comparison without Training
Cong Shi
WINLAB, Rutgers University
New Brunswick, NJ, US
PI:EMAIL
Yan Wang
Temple University
Philadelphia, PA, US
PI:EMAIL
Yingying Chen
WINLAB, Rutgers University
New Brunswick, NJ, US
PI:EMAIL
Nitesh Saxena
University of Alabama at Birmingham
Birmingham, AL, US
PI:EMAIL
Chen Wang∗
Louisiana State University
Baton Rouge, LA, US
PI:EMAIL
ABSTRACT
Due to the open nature of voice input, voice assistant (VA) systems
(e.g., Google Home and Amazon Alexa) are vulnerable to various se-
curity and privacy leakages (e.g., credit card numbers, passwords),
especially when issuing critical user commands involving large
purchases, critical calls, etc. Though the existing VA systems may
employ voice features to identify users, they are still vulnerable
to various acoustic-based attacks (e.g., impersonation, replay, and
hidden command attacks). In this work, we propose a training-free
voice authentication system, WearID, leveraging the cross-domain
speech similarity between the audio domain and the vibration do-
main to provide enhanced security to the ever-growing deployment
of VA systems. In particular, when a user gives a critical command,
WearID exploits motion sensors on the user’s wearable device to
capture the aerial speech in the vibration domain and verify it with
the speech captured in the audio domain via the VA device’s micro-
phone. Compared to existing approaches, our solution is low-effort
and privacy-preserving, as it neither requires users’ active inputs
(e.g., replying messages/calls) nor to store users’ privacy-sensitive
voice samples for training. In addition, our solution exploits the
distinct vibration sensing interface and its short sensing range to
sound (e.g., 25𝑐𝑚) to verify voice commands. Examining the simi-
larity of the two domains’ data is not trivial. The huge sampling
rate gap (e.g., 8000𝐻𝑧 vs. 200𝐻𝑧) between the audio and vibra-
tion domains makes it hard to compare the two domains’ data
directly, and even tiny data noises could be magnified and cause
authentication failures. To address the challenges, we investigate
the complex relationship between the two sensing domains and
develop a spectrogram-based algorithm to convert the microphone
data into the lower-frequency “ motion sensor data” to facilitate
∗This work was done when Chen Wang was a graduate student at Rutgers University.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC 2020, December 7–11, 2020, Austin, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8858-0/20/12...$15.00
https://doi.org/10.1145/3427228.3427259
Figure 1: Illustration of the proposed idea in WearID - ex-
ploring the complement of the vibration domain to defend
against audio-based attacks, e.g., impersonation, replay, hid-
den voice command and ultrasound attacks.
cross-domain comparisons. We further develop a user authentica-
tion scheme to verify that the received voice command originates
from the legitimate user based on the cross-domain speech sim-
ilarity of the received voice commands. We report on extensive
experiments to evaluate the WearID under various audible and
inaudible attacks. The results show WearID can verify voice com-
mands with 99.8% accuracy in the normal situation and detect
97.2% fake voice commands from various attacks, including imper-
sonation/replay attacks and hidden voice/ultrasound attacks.
ACM Reference Format:
Cong Shi, Yan Wang, Yingying Chen, Nitesh Saxena, and Chen Wang. 2020.
WearID: Low-Effort Wearable-Assisted Authentication of Voice Commands
via Cross-Domain Comparison without Training. In Annual Computer Secu-
rity Applications Conference (ACSAC 2020), December 7–11, 2020, Austin, USA.
ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3427228.3427259
1 INTRODUCTION
In recent years, smart devices (e.g., Google Home and Amazon
Alexa) have incorporated advanced speech recognition technolo-
gies that enable the devices to understand natural language and
take voice commands. By using voices as inputs, users can smoothly
and conveniently interact with their voice assistant (VA) systems
to accomplish numerous daily tasks, such as playing music, manag-
ing calendar events, shopping online and controlling smart home
appliances. With the growing trend of using VA systems, more and
more people tend to use voice commands to complete important
tasks. For example, making a big purchase (e.g., over 100 dollars),
unlocking the entrance door to a house, or making a critical call
Voice Assistant SystemWearable  Legitimate UserUnique Voice Characteristics ComparisonAudible/Inaudible AttacksVoice Command AuthenticatedAudio VoiceAerial Voice VibrationACSAC 2020, December 7–11, 2020, Austin, USA
Cong Shi, Yan Wang, Yingying Chen, Nitesh Saxena, and Chen Wang
Table 1: Comparing WearID with potential security solutions to secure critical voice commands.
Requiring user
active input
Requiring training and
storing voice templates
Requiring audio
playback
Requiring dedicated
sensor
Vulnerable to audible
attacks [30, 42]
Vulnerable to inaudible
attacks [12, 51]
WearID (Our Solution)
One-tap two-factor authentication [37]
SMS/call-based two-factor authentication [21]
Audio CAPTCHA (suggested in [12])
Voice biometric-based authentication [39, 44]
Two microphone authentication (2MA) [10]
Defenses against inaudible attacks [12, 51]
Defense with smartphone motion sensor [46]
VAuth [20]
✕
✔
✔
✔
✕
✕
✕
✕
✕
✕
✕
✕
✕
✔
✕
✔
✔
✕
✕
✕
✕
✕
✕
✔
✕
✔
✕
✕
✕
✕
✕
✕
✕
✕
✕
✔
✕
✕
✕
✕
✔
✔
✔
✔
✕
✕
✕
✕
✕
✔
✕
✕
✕
✕
(e.g., calling a bank for conducting transactions [45]). We call these
voice commands as highly critical commands since the commands
could access highly sensitive information and functionalities (e.g.,
credit card numbers, passwords, and payments). The significant
financial benefits of using such highly critical commands lure ad-
versary into faking the user’s commands and put the user’s privacy
and property under high risks. For instance, the adversary can get
a user’s credentials for accessing personal devices by asking, “OK,
Google, what is my password?” [8] The adversary can also make
a significant amount purchase through the user’s associated ac-
count [9] by telling the VA system “Alexa, Order a MacBook from
Prime Now.” When the adversary can access the VA system at home
remotely (e.g., through a hacked Smart TV), the adversary can even
use critical commands to control security critical IoT devices [15],
such as disarming a smart locking system and gain entry into the
house. To ensure the successful large-scale deployment of VA sys-
tems, it is critical to address these inherited security vulnerabilities
in VA systems and bring trustworthiness to users. In this work, we
thus aim to design a low-overhead system with enhanced security
that could protect highly critical commands in VA systems.
Existing Solutions. Existing authentication and defense mecha-
nisms for VA systems relying on voice biometric technologies [3, 22,
26, 39, 44, 46] use users’ unique sound characteristics and machine
learning-based models for user authentication. These solutions
solely rely on acoustic features in the audio domain (i.e., extracting
information from the data captured by microphones). Thus they are
vulnerable to acoustics attacks, either audible attacks (e.g., replay
attacks [30] and impersonation attacks [42]) or more surreptitious
inaudible attacks (e.g., hidden voice commands [12] and ultrasound
attacks [51]). To add another layer of defense, some VA systems
exploit a second factor to secure voice commands, such as challenge
questions via audio CAPTCHA [12], replay messages/calls [21], or
virtual buttons [37] on the user’s mobile device (e.g., smartphone).
However, these approaches require significant user efforts to con-
firm the authenticity of each single voice command. Furthermore,
they could be prone to user careless behaviors [19] of habituations
of confirming, meaning the attack attempts may be accepted with-
out paying attention. Recently, VAuth [20] develop a system that
utilizes the user’s facial vibrations captured by accelerometers em-
bedded in a pair of glasses for user authentication. The dedicated
sensors requiring a high sampling frequency of 11kHz entail addi-
tional costs, making the system not practical. While 2MA [10] needs
to use multiple spatially distributed microphones, which leads to
extra cost and considerable energy consumption. Moreover, this
approach only works in the audio domain alone so that they are still
vulnerable to the attacks in the audio domain. We summarize the
weaknesses of the state-of-the-art voice authentication techniques
in Table 1.
Our Approach. In this paper, we explore the feasibility of lever-
aging wearables’ accelerometers to harness the aerial voice vibra-
tions corresponding to live human speeches for user authentica-
tion. We propose a low-effort training-free user authentication
system, WearID. It utilizes the wearable as a personal identity token
and performs cross-domain authentication (audio vs. vibration) to
verify the identity of the person who gives the voice command.
WearID provides a scalable solution that would enable using VA
under high-security-level scenarios (e.g., nuclear power stations,
stock exchanges, and data centers), where all voice commands
are critical and desire around-the-clock authentication. It is also
compatible with existing voice-based authentication methods in
VA systems (e.g., Google Voice Match and Amazon Alexa Voice
Profile), where WearID could be invoked when critical commands
are detected. Compared with existing voice biometric technolo-
gies [39, 44], WearID does not require extra user efforts (e.g., answer-
ing challenging questions and replying messages/calls) or additional
training using privacy-sensitive voice samples. In addition, WearID
reuses wearable devices that have already been widely accepted
worldwide (i.e., 593 million in 2018 [40]), making it low-cost and
more practical than the two most similar approaches, VAuth [20]
and 2MA [10]. Moreover, our solution is different from existing
two-factor authentication methods using co-location information
(e.g., WiFi [29], Bluetooth [38], and ambient sound and light [23]),
since it can resist the acoustic attacks as mentioned above.
The basic idea of WearID is examining the similarity between the
unique voice characteristics in the aerial speech vibration and the
audio voice for user authentication. As shown in Figure 1, triggered
by a wake word detected at the VA device, WearID exploits the
wearable’s accelerometer and VA’s microphone to capture voice
commands in the vibration domain and audio domain at the same
time, respectively. The voice commands recording data are sent to