theory [32, 46, 47]. The ICE directly splits the training set
into two non-empty partitions: the proper training set and
the calibration set. The underlying algorithm is trained on
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
809
(a) TCE
(b) Approx-TCE
(c) ICE
(d) CCE
Fig. 3: Illustration of the different calibration splits employed by each of the
conformal evaluators showing the target of the p-value calculation, relative
points included in the bag, and points excluded from the calibration.
the proper training set, and p-values are computed for each
example in the calibration set. Unlike the TCE, p-values are
not calculated for every training point, but only for examples
in the calibration set, with the proper training set having no
role in the calibration at all. The ICE aims to inductively learn
a general rule on a single fold of the training set.
This induces signiﬁcantly less computational overhead than
the TCE and approx-TCE (see Table I) and in practice is
extremely fast, but also very informationally inefﬁcient. Only
a small proportion of the training data is used to calibrate
the conformal evaluator, when ideally we would use all of it.
Additionally, the performance of the evaluator depends heavily
on the quality of the split and the calibration set’s ability to
generalize to the remainder of the dataset. This results in some
uncertainty: an ICE may perform worse than a TCE due to a
lack of information, or better due to a lucky split.
C. Cross-Conformal Evaluator (CCE)
The Cross-Conformal Evaluator (CCE) draws on inspiration
from k-fold cross validation and aims to reduce both the
computational and informational
inefﬁciencies of the TCE
and ICE. Like the ICE, the CCE has a counterpart rooted
in conformal prediction theory [48].
The training set is partitioned into k folds of equal size. So
that a p-value is obtained for every training example, each fold
is treated as the calibration set in turn, with p-values calculated
as with an ICE, using the union of the k − 1 remaining folds
as the proper training set to ﬁt the underlying classiﬁer.
Finally we concatenate the p-values in a way which pre-
serves their statistical integrity when decision quality is evalu-
ated. We set aside the k ﬁt underlying models and correspond-
TABLE I: Runtime complexities and empirical runtime for conformal eval-
uator calibration where n is the number of training examples and p is the
proportion of examples included in the proper training set each split/fold.
CONFORMAL EVALUATOR
COMPLEXITY
O(n2)
TCE
Approx-TCE, 1/(1 − p) folds O(n/(1 − p))
O(pn)
ICE
O(pn/(1 − p))
CCE, 1/(1 − p) folds
RUNTIME IN §VI-B
est. 1.9 CPU yrs
46.1 CPU hrs
11.5 CPU hrs
36.6 CPU hrs
ing calibration sets for test time. When a new point arrives,
the prediction from each classiﬁer is evaluated against the
corresponding calibration set. The ﬁnal result is the majority
vote over the k folds, i.e., the prediction of a particular class
is accepted if the number of accepted classiﬁcations is greater
than k
2 , and rejected otherwise.
The CCE can be viewed as k ICEs, one per fold, and these
ICEs can operate in parallel to reduce computation time—if
the resources are available. However, there is an additional
memory cost with storing the separate models.
V. SOUND AND PRACTICAL TRANSCENDENT
Once p-values are calculated,
thresholds are derived to
decide when to accept or reject new test examples. Here we
revise and formalize the strategy used in Transcend [20] and
propose a more efﬁcient search strategy.
A. Calibration Phase
The ﬁrst phase of Transcend [20] is the calibration proce-
dure which searches for a set of per-class credibility thresholds
T = { τy ∈ [0, 1] : y ∈ Y }
with which to separate drifting from non-drifting points. Given
that low credibility represents a violation of conformal predic-
tion’s assumptions, these points are likely to be misclassiﬁed
by the underlying classiﬁer that similarly relies on the i.i.d.
assumption. Note that thresholds can be found with different
optimization criteria and it is also possible to threshold on a
combination of credibility and conﬁdence (see §VI-C).
Calibration aims to answer the question: “how low a cred-
ibility is too low?”, by analyzing the p-value distribution of
points in a representative, preferably stationary, environment
such as the training set. Which points are selected as calibra-
tion points depends on the underlying conformal evaluator, and
this comes with various trade-offs (see §IV). Typically, each
calibration point (or partition of the calibration set) is held out
and the underlying classiﬁer trained on the remaining points.
Then a class is predicted for the calibration point(s) with p-
values calculated with respect to that predicted class. This
process is repeated until all calibration points are assigned a
corresponding p-value. Using the ground truth, these p-values
can be partitioned into correct and incorrect predictions that
should be separated by T . Methods to ﬁnd an effective T
can be manual (e.g., picking a quartile visually using an alpha
assessment) or automated (e.g., grid search).
Figure 4 shows an example of the Transcend [20] thresh-
olding procedure on a toy dataset composed of two classes:
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
810
P-value targetIncluded in bagExcluded from calibration(a) First fold
(b) Second fold
(c) Third fold
(d) Alpha assessment
Fig. 4: Thresholding procedure applied to a linear SVM with approximate-TCE (3 folds). Four points highlighted with dotted outlines are left out as calibration
in each fold, with the decision boundary obtained with the remaining points as training. P-values, shown above or below each calibration point, are calculated
using the negated absolute distance from the decision boundary as an NCM. The shaded regions capture points which are more nonconform with respect to
the predicted class (blue for class (cid:32) and red for class (cid:35)). The alpha assessment (d) shows the distribution of p-values and per-class thresholds derived from
Q1 of the correctly classiﬁed points (see §V-D for a discussion of more complex search strategies for ﬁnding thresholds).
(a) Distances and NCMs
(b) P-value of new point
(c) Comparison to threshold
Fig. 5: Test-time procedure applied to a linear SVM and calibrated Transcend [20] with distances from hyperplane and corresponding nonconformity scores
shown in (a). In (b) a new test point is classiﬁed as class (cid:35). The p-value is calculated as the proportion of points belonging to (cid:35) with equal or greater
nonconformity scores (captured by the shaded region) than the new point. In (c), the new point is compared against the threshold for class (cid:35) as derived
during the calibration phase (Figure 4). As the p-value of the new point is greater than the threshold for the predicted class, the prediction is accepted.
(cid:32) and (cid:35). A linear SVM is paired with a TCE (§IV) to
generate NCMs and p-values for the binary classiﬁcation with
rejection task. The decision boundary is depicted as a solid line
and margins are drawn through support vectors with dotted
lines. Due to the use of approximate TCE, the dataset is
partitioned into folds, where each fold leaves out four points
for calibration and trains on the remainder. The three folds
are depicted in Figures 4a, 4b, and 4c. Calibration points are
shown with dotted outlines and are faded for class (cid:32).
In each fold, a p-value is calculated for each calibration
point as the proportion of other objects that are at
least
as dissimilar to the predicted class as the calibration point
itself. In the linear SVM setting shown, less similar objects
are those closest to the decision boundary (i.e., those with a
higher NCM) residing in the shaded area between the decision
boundary and the parallel line intersecting the point (blue for
class (cid:32) and red for class (cid:35)). The calculated p-values are
shown aligned above or below each calibration point.
To evaluate how appropriate an NCM is for a given model,
the p-values can be analyzed with an alpha assessment.
Here the distribution of p-values for each class are divided
into groups depending on whether the calibration point was
correctly or incorrectly predicted as that class. Given that
there may not be enough incorrectly classiﬁed examples to
perform the assessment with, it is standard to perform an
alpha assessment in a non-label-conditional manner, using
p-values computed with respect to all classes, not just each
point’s predicted class. The greater the margin separating the
distributions of correct and incorrect p-values, the better suited
an NCM is for a model. The alpha assessment in Figure 4d
shows the distribution of p-values for correctly and incorrectly
predicted calibration points for classes (cid:32) and (cid:35). Given the
size of the example dataset, the assessment is computed in
a label-conditional manner and the threshold is set at Q1 of
the p-values for correctly classiﬁed points (more insight into
threshold search strategies can be found in §V-D). Test points
generating p-values below this threshold will be rejected.
B. Test Phase
At test time, there are |Y| + 1 outcomes. When a new test
object z∗ arrives, its p-value pˆy
z∗ is calculated with respect
to the predicted class ˆy (label conditional). If pˆy
z∗ < τˆy, the
threshold for the predicted class, then the null hypothesis—that
z∗ is drifting relative to the training data and does not belong
z∗ ≥ τˆy, the
to ˆy—is approved and the prediction rejected. If pˆy
prediction is accepted and the object classiﬁed as ˆy.
Figure 5 follows on from the calibration example above.
Figure 5a illustrates the NCM being used: the negated absolute
distance from the hyperplane. In Figure 5b, a new test example
(cid:66) appears and is classiﬁed as class (cid:35). The p-value p(cid:35)(cid:66)
=
0.714 is calculated as the proportion of points belonging to
(cid:35) with equal or greater nonconformity scores than (cid:66). Finally,
Figure 5c shows p(cid:35)(cid:66)
p(cid:35)(cid:66)
compared against the threshold τ
(cid:35)
, the prediction is accepted.
≥ τ
(cid:35)
and, as
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
811
0000.200000.80.80.80.80.80.80.80.80.20.40.60.81.00.0CorrectIncorrectCorrectIncorrect-2-2-2-3-4-7+2+3+3+5+7-3-5-7-3-2-3-7-2-4-2+2-5-5-4-2-5-7-3-2-3-7-2-4-2-5-3-50.7140.20.40.60.81.00.0CorrectIncorrectCorrectIncorrectC. Rejection Cost
What happens to rejected points depends on the rest of the
detection pipeline. In a simple setting, rejected points may be
manually inspected and labeled by specialists. Alternatively,
they may continue downstream to further automated analyses
or to other ML algorithms such as unsupervised systems.
In all cases there will be some cost associated with rejecting
predictions. When choosing rejection thresholds, it is vital
to keep this cost in mind and weigh it against the potential
performance gains. The Tesseract framework [35] deﬁnes three
important metrics to use when tuning or evaluating a system
for mitigating time decay.
Performance ensures that robustness against concept drift is
measured appropriately depending on the end goal (e.g., high
F1 score or high TPR at an acceptable FPR threshold).
Quarantine cost measures the cost incurred by rejections.
This is important for putting the performance of kept elements
in perspective—there will often be a trade-off between the
amount of rejections and higher performance on kept points.
Labeling cost measures the manual effort needed to ﬁnd
ground truth labels for new points. While this is more pertinent
to retraining strategies, it is related to the overhead associated
with rejection as many may need to be manually labeled. As an
example, Miller et al. [27] estimate that the labeling capacity
for an average company is 80 samples per day.
D. Improving the Threshold Search
Here we model the calibration procedure as an optimization
problem for maximizing a given performance metric (e.g.,
F1, Precision, or Recall of kept elements). Usually this max-
imization is subject to some constraint on another metric, for
example, it is trivial to attain high F1 performance in kept
elements by accepting very few high quality predictions, but
this will cause many correct predictions to be rejected.
Formally, given n calibration points, we represent this as:
F(Y , ˆY , P ;T )
arg max
subject to G(Y , ˆY , P ;T ) ≥ C ,
T
(6)
where Y and ˆY are n-dimensional vectors of ground truth
and predicted labels respectively, P is a |Y| × n-dimensional
matrix of calibration p-values, and T = { τy ∈ [0, 1] | y ∈ Y }
is the set of thresholds. The objective function F maps these
inputs to the metric of interest in R, for example F1 of kept
elements, while G maps to the metric to be constrained, such
as the number of per-class rejected elements. C is the threshold
value that bounds the constraint function.
Given this formalization, we propose an alternative random
search strategy to replace the exhaustive grid search used in
the original paper [20]. In the exhaustive grid search, each
possible combination of thresholds over all classes is tested
systematically, considering some ﬁxed range of variables
V = {v : v ∈ [0, 1]}. However, this suffers from the curse of
dimensionality [9], resulting in |V ||Y| total trials, growing ex-
ponentially with the number of classes. Additionally, reducing
the granularity of the range considered in V increases the risk
of ‘skipping’ over an optimal threshold combination. Similarly,
often many useless threshold combinations are considered
(where one is either too high or too low). This failure to evenly
cover subspaces of interest worsens as the dimensionality
increases [10], making it especially problematic for multiclass
classiﬁcation. The granularity for V can be chosen manually
based on intuition, however this results in parameters which
are difﬁcult to reproduce and transfer to other settings.
It has been shown for hyperparameter optimization that
random search is able to ﬁnd combinations of variables at
least as optimal as those found with full grid search over the
same domain, at a fraction of the computational cost [10]. We
apply these ﬁndings to the threshold calibration and replace
the exhaustive grid search with a random search (Algorithm 1).
We choose random combinations of thresholds in the interval
[0, 1], keeping track of the thresholds that maximize our chosen
metric given the constraints (see §V-D). The search continues
until either of two conditions are met. A limit is set on the
number of iterations, determined by the time and resources
that are available for the calibration. Intuitively a higher limit
will increase the likelihood of ﬁnding better thresholds and so
acts as the upper bound of the optimization. Secondly, a stop
condition can be set. In this work we consider a no-update
approach in which the search will stop once a ﬁxed point is
found, i.e., if there is no improvement to performance after a
certain number of consecutive iterations. Note that this search
procedure can be easily parallelized.
We empirically compare the two search strategies in §VI-D.
VI. EXPERIMENTAL EVALUATION
We evaluate our novel evaluators when faced with grad-
ual concept drift caused by the evolution of real malicious
Android apps over time (§VI-B), the performance gained by
including conﬁdence scores (§VI-C), how our random search
implementation fares against exhaustive search (§VI-D), how
the evaluators compare to alternative methods (§VI-E), and
perform on PE and PDF malware domains (§VI-F).
A. Experimental Settings
Prototype. We implement TRANSCENDENT as a Python
library encompassing out new proposals as well as the func-
tionality of the original Transcend [20]. We release the code
as open source—note that this is the ﬁrst publicly available
implementation of Transcend in any form.
Dataset. We ﬁrst focus on malware detection in the Android
domain. We sample 232,848 benign and 26,387 malicious apps
from AndroZoo [2]. This allows us to demonstrate efﬁcacy
when faced with a natural, surreptitious concept drift. The
apps span 5 years, from Jan 2014 through to Dec 2018.
We use the Tesseract [35] framework to temporally split the
dataset, ensuring that Tesseract’s constraints are accounted for
to remove sources of spatial and temporal experimental bias.
Training and calibration are performed using apps from 2014
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
812
(a) Training [20]
(b) Test [20]
(c) Training
(d) Test at 1 year
(e) Test at 2 years
(f) Test at 3 years
(g) Test at 4 years
Fig. 6: Frequency distributions of features depicting covariate shift between training and test malware examples. The data from Jordaney et al. [20], displayed
in (a) and (b), shows a sudden and signiﬁcant shift, while the data used in §VI, displayed in (c–g), shows a more subtle, natural drift occurring over time.
and testing is evaluated over the concept drift that occurs over
the remaining period on a month-by-month basis.
Eliminating Sampling Bias. The original evaluation of
Transcend [20] artiﬁcially simulated concept drift by fusing
two datasets: Drebin [6] and Marvin [25], a process which
may have induced experimental bias [7] and made it easier to
detect drifting examples. Figure 6 shows a visibly signiﬁcant
covariate shift in the distribution of features for training and
test malware examples from Jordaney et al. [20], with a