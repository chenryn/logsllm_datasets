type is deﬁned in the type ﬁeld.
The header of a data packet contains length of the route
that the packet travels (rlen), an index into the packet’s
route (ridx), the ﬂow identiﬁer (ﬂow), source (src), destina-
tion (dst), the sequence number (seq), the packet checksum
(checksum), length of the payload (plen), the packet’s route
(route), and ﬁnally the payload.
The size of endpoints allows for up to 65,536 nodes. Fur-
ther, the route ﬁeld could be up to 128 bits. We use 3 bits
for each hop to select the forwarding link (i.e., we assume at
most eight links per node) and increase ridx every time we
forward a packet. This allows us to store routes with up to
42 hops, which is sufﬁcient for current rack-scale computers
and even non-minimal routing strategies.
Apart from source, destination and the packet checksum,
broadcast packets include the weight of the ﬂow (weight),
ﬂow’s priority (priority), the demand (demand) in Kbps be-
tween the nodes (up to 4 Tbps), the broadcast spanning tree
id (tree), and the currently used routing strategy between the
two nodes (rp).
5. EVALUATION
In our evaluation, we answer the following questions:
(i). how effective is R2C2’s congestion control mechanism
in achieving high throughput and low queuing?, (ii). what is
the trafﬁc overhead introduced by broadcast?, (iii). what is
the cost of rate computation?, and ﬁnally (iv). what are the
beneﬁts of supporting per-ﬂow routing selection?
We adopted the following methodology. First, we use
R2C2’s implementation atop Maze to empirically verify the
feasibility and performance of our design, to quantify the
computation overhead, and to cross-validate our packet-level
simulator. Then, we use the simulator to investigate the per-
formance of R2C2 at scale and under different workloads.
Our results indicate that R2C2 achieves high throughput
and fairness while only requiring small queues and the over-
head imposed by broadcast is negligible. The computation
cost depends on the frequency at which rates are recom-
puted. However, for realistic workloads, this cost is reason-
able. Finally, by enabling individual ﬂows to use different
routing protocols and by dynamically selecting among them
based on the observed workload, R2C2 achieves higher per-
formance than what would be possible using only a single
routing protocol for all ﬂows.
(a) Flow throughput.
(b) Queue occupancy.
Figure 7: Cross-validation of the ﬂow throughput and maxi-
mum queue occupancy between Maze and the simulator us-
ing a 4x4 2D torus topology.
5.1 Emulation results
We deploy our Maze platform on a 16-server RDMA clus-
ter. Each server is equipped with two 2.4 GHz Intel Xeon
E5-2665 CPUs and 24 GB of memory. The servers are con-
nected using Quad Data Rate (QDR) InﬁniBand. We emu-
lated a 4x4 2D torus virtual topology with a bandwidth of
5 Gbps per virtual link.
We generate a synthetic workload, comprising 1,000
ﬂows of 10 MB each. We assume Poisson ﬂow arrivals
with a mean inter-arrival time of 1 ms. We use the random
packet spraying routing protocol. We measure the through-
put and the maximum occupancy experienced by each queue
throughout the entire experiment. We then repeated the
same experiment in the simulator, using the same topology
and workload. Results in Figure 7 show that our packet-
level simulator exhibits high accuracy, both in terms of ﬂow
throughput and queuing occupancy. These cross-validation
results improve our conﬁdence in the large-scale simulation
experiments presented in Section 5.2.
Computation overhead. Next, we evaluate the computa-
tion overhead introduced by R2C2 when recomputing ﬂow
rates. To show the behavior at scale, we ran the same work-
load in the simulator using a 512-node 3D Torus and we
recorded the ﬂow arrival and departure events at each node.
To account for the increase in scale, we reduce the ﬂow
inter-arrival time to 1 us. We then replayed these traces in
Maze and measured the execution time of the rate recompu-
tation. We ran this benchmark on two different CPU cores:
a 2.4GHz Intel Xeon E5-2665 and a 1.66 GHz Intel Atom
D510 [57]. The ﬁrst one is representative of today’s data
center servers. The latter, instead, is a ﬁrst-generation (2009)
low-power CPU architecture. This could be used, for in-
stance, as a cheap, dedicated core on each SoC to handle the
execution of R2C2.
Figure 8 plots the 99th percentile of the CPU overhead for
different values of the recomputation interval ρ. We com-
pute the overhead by dividing the time taken to recompute
the rates by the value of ρ. This means that if the overhead
is higher than 100% (the horizontal line in the chart), the re-
computation would not ﬁnish in time and, hence, the interval
is not feasible. As discussed in Section 3.3.2, we use a batch-
based design in which we only consider the ﬂows that last
more than one interval. Therefore, the longer the interval,
the lower the number of ﬂows considered, and this explains
why high values of ρ exhibit lower overhead. For exam-
ple, for ρ=500 us, the median overhead on the Intel Xeon is
559Figure 8: The 99th percentile of the CPU overhead on one
2.4 GHz Intel Xeon E5-2665 core and on a 1.66 GHz Intel
Atom D510 core when simulating a 512-node workload with
1 us ﬂow inter-arrival time.
Figure 10: The CDF of FCT for short ﬂows (size  1 MB) for ﬂow inter-arrival time τ=1us.
3D torus. This is the same size and topology as the
AMD SeaMicro 15000-OP. We assume a link bandwidth
of 10 Gbps and a per-link latency of 100 ns. We leave 5%
headroom and, except where otherwise noted, we use a re-
computation interval of 500 us. In our experiments, we use
a synthetic workload modeled after trafﬁc patterns observed
in production data centers [2, 4, 25]. Flow’s source and des-
tination are randomly chosen following a uniform distribu-
tion. The ﬂow sizes are generated from a Pareto distribution
with shape parameter 1.05 and mean 100 KB [3]. This gen-
erates a heavy-tailed workload where 95% of the ﬂows are
less than 100 KB, as is commonly observed in data centers.
We assume Poisson ﬂow arrivals and we consider ﬂow inter-
arrival times varying from 1 us to 100 us. To stress our sys-
tem, we also consider an extreme ﬂow inter-arrival time of
100 ns, which corresponds to a workload with 1010 ﬂows/s
with a peak of 2,241 simultaneous ﬂows. This is up to two
orders of magnitude lower than the arrival times observed in
a recent study on a production cluster [32], which reports a
median arrival time of 10 us for a 1,500-server network (i.e.,
three times bigger than our simulated network).
Flow completion time and queuing. We start our analysis
by measuring the ﬂow completion time achieved by R2C2.
We compare our approach against TCP and against an ide-
alized baseline, per-ﬂow queues (PFQ), that uses back-
pressure and per-ﬂow queues at each node. This baseline
is impractical because, apart from forwarding complexity at
rack nodes, it results in very high buffering requirements.
However, it is useful in our study because it provides the up-
per bound of the performance achievable by any rate control
protocol for minimal and non-minimal routing. For TCP, we
use an ECMP-like routing protocol, which selects a single
path between source and destination, based on the hash of
the ﬂow ID. This ensures that packets belonging to the same
ﬂow are routed onto the same path as required by TCP. How-
ever, we assign different shortest paths to different ﬂows be-
560Figure 12: The 99th-percentile of FCT for short ﬂows (size
 1 MB) normalized against TCP for different ﬂow inter-
arrival times.
tween the same endpoints. For the idealized baseline and
R2C2, we use random packet spraying.
Figure 10 and 11 show the CDF of the ﬂow completion
time (FCT) for short ﬂows (size  1 MB) respectively. We do
not show the results for intermediate sizes as they are quali-
tatively similar to the ones for the short ﬂows. As expected,
TCP achieves the worst performance both for short and long
ﬂows. At the 99th percentile, TCP yields a 3.21x higher
FCT for short ﬂows and a 2.55x lower average throughput
for long ﬂows compared to R2C2. The reason is due to its
high queuing occupancy (for short ﬂows) and its dependency
on a single-path routing (for long ﬂows). The latter aspect
underlines the importance of exploiting path diversity.
The results also show that for short ﬂows R2C2 closely
matches the FCT of the PFQ conﬁguration and it does so by
only using a single queue per port. This conﬁrms that our
protocol is able to achieve fairness among ﬂows without re-
quiring per-ﬂow state at the intermediate nodes. This is par-
ticularly important for many data center applications, which
are sensitive to load imbalance and long tail latency [21].
For long ﬂows, the gap between R2C2 and PFQ increases.
The is due to the fact that a) R2C2 uses a different fairness
model that trades off utilization for computation tractability
(§3.3.1) and b) it uses headroom to absorb bursts (§3.3.2).
To investigate the performance of R2C2 at different loads,
in Figure 12 and 13 we show the 99th percentile of the FCT
(short ﬂows) and the average throughput (long ﬂows) nor-
malized against TCP for different inter-arrival times τ. As
expected, at very high load (τ=100 ns), the performance of
R2C2 deviates from the PFQ’s ideal one as the queues start
building up due to the inaccuracy introduced by our periodic
recomputation. However, as noted, this scenario represents
an extreme case; we believe that ﬂow inter-arrival times of
561Figure 15: The median and 95th-percentile of the normalized
difference between the ideal and computed rates against the
recomputation interval (ﬂow inter-arrival time τ=1 us).
Figure 18: Aggregate throughput achieved by our routing
selection heuristic (Adaptive) normalized against the three
baselines for different load values.
Figure 19: Control trafﬁc with varying number of concurrent
long ﬂows per server (ﬂow inter-arrival time τ=1 us).
Figure 16: The median and 95th-percentile of the difference
between the ideal and computed rates against the ﬂow inter-
arrival time (recomputation interval ρ=500 us).
(a) Short ﬂows.
(b) Long ﬂows.
Figure 17: The 99th-percentile of FCT (short ﬂows) and av-
erage throughput (long ﬂows) against the headroom.
the performance of R2C2 is not particularly sensitive to the
choice of the headroom parameter. Overall, we found that,
for our target workloads, a 5% headroom represents a good
trade-off. For example, for an inter-arrival time of 1 us, com-
pared to the case in which no headroom is used, a 5% head-
room yields a 21.9% reduction in FCT for short ﬂows at the
99th percentile while the reduction in the average throughput
for long ﬂows is less than 3%.
Custom routing protocols. Next, we explore the behavior
of R2C2’s ﬂexible routing stack. In particular, we aim to:
i) demonstrate the beneﬁts of having multiple routing proto-
cols running concurrently (as opposed to a single, network-
wide, routing) and ii) to evaluate the performance of our
adaptive selection of routing protocols. We consider a work-
load in which a fraction L of nodes generates a long-running
ﬂow each to another randomly chosen node such that every
node is the source and the destination of at most one ﬂow.
We chose this workload for its relative simplicity, which