ducing to 576 bytes suggest the reductions are due
to the hosts ﬁltering PTB messages but having PM-
TUD blackhole detection enabled.
 1.0
 0.8
 0.6
 0.4
 0.2
n
o
i
t
c
a
r
F
 0
 0
256
1280
576
1480
 200
 400
 600
 800  1000  1200  1400
Packet Size
Figure 5: The subsequent size of a segment after a
PTB message is sent, for each MTU tested. Most
webservers behave as expected and reduce their
packet size to the size advised, although most web-
servers will not reduce their packet size below 552
bytes if asked.
70% of the population behave as expected and reduce their
packet size to the MTU advised. 5% reduce their packet size
40 bytes further, perhaps to ensure they send packets that
will easily clear the link concerned. Few servers will reduce
their packet size below 552 bytes in response to a PTB with
MTU 256. Instead, most clear the DF bit to allow packets
to be fragmented. The TBIT tool used in [1] classiﬁes these
systems as not attempting PMTUD, though it is clear from
tests with other MTUs that most will attempt and success-
fully complete PMTUD. Systems that reduce their packet
size to 552 bytes but do not clear the DF bit are failing
PMTUD due to a bug in the operating system of the web-
server; FreeBSD is one operating system with this bug [21].
Correcting these bugs will cut the failure rate for MTU 256
in half. The choice of 256 as an MTU value is not optimal
for measuring the impact of middleboxes ﬁltering PTB mes-
sages, because (1) most operating systems disable PMTUD
when a PTB with this MTU is received, and (2) some reduce
their packet size to a threshold but do not clear the DF bit.
For the 1280 MTU test, 2.7% reduce their segment size
to 576 bytes, and 91% of these reductions also clear the DF
bit. This behaviour indicates the use of blackhole detection
by the webserver, such as that done by Windows [16], in the
presence of a ﬁrewall that discards PTB messages. Figure 6
shows the delay between the ﬁrst PTB sent and the web-
server transmitting packets that will pass a link with a 1280
byte MTU. 91% of the webservers that reduce their packet
size to 1280 do so within one second. In addition, 94% of
all webservers that appear to take action based on receiving
a PTB message also do so within three seconds. However,
77% of the webservers that reduce their packet size to 576
do so at least 20 seconds after the ﬁrst PTB is sent, and
are likely to be Windows hosts that have inferred a PM-
TUD blackhole; 74% of these webservers are unresponsive
to ping. The Clear DF line represents 118 webservers that
cleared the DF bit but did not reduce their packet size so
it would clear a link with a 1280 byte MTU. 71% of these
webservers respond within three seconds, though the tail is
long, with 8% waiting 43 seconds to take their action.
We searched the data for evidence of other webservers
using techniques to avoid PMTUD failures if they do not
receive a PTB message when they should. Linux imple-
ments two techniques, neither of which are enabled by de-
fault. The ﬁrst technique begins by sending large packets,
and if the ﬁrst large packet is lost twice and no PTB mes-
sage is received, then after nine seconds the host will revert
to sending 552 byte packets with the DF bit cleared. The
second technique is PLPMTUD [14]; Linux begins by send-
ing 552 byte packets by default, and after the window size
has grown to 11 segments it will try sending a 1064 byte
segment with the DF bit set. MacOS since 10.6 also im-
plements blackhole detection; it begins by sending a small
segment followed by a ﬂight of several large packets. If the
large packets do not cause acknowledgements, and the re-
transmission of the ﬁrst large packet is not acknowledged
either, it will lower its packet size to 1240 bytes and clear
the DF bit. We observed no evidence of any of these in
our data, though the tests are unlikely to ﬁnd evidence of
the ﬁrst or third techniques unless a middlebox discards our
PTB messages before they reach the destination.
106MTU tested
1480
Tokyo
1280
New York
1480
1280
San Diego
1480
1280
Amsterdam
1480
1280
Christchurch
1480
1280
576
Early TCP reset
No data packets
Too small
PMTUD success
PMTUD failure
0.1%
0.3%
0.1%
0.1%
0.2%
0.1%
35.9% 16.8%
32.9% 17.0% 32.7% 17.6% 33.4% 17.2% 34.2% 17.3%
62.7% 80.5% 44.1% 66.0% 80.7% 65.9% 80.1% 64.9% 80.0% 64.1% 79.9%
2.4%
nil
0.1%
–
2.4% 55.7%
0.9%
2.2%
0.9%
2.2%
0.1%
nil
1.0%
2.2%
0.9%
0.1%
0.1%
1.0%
0.2%
0.3%
0.1%
nil
0.2%
0.6%
0.1%
0.4%
0.2%
0.6%
Table 3: Results of our IPv6 experiments. The results across vantage points are similar. At least 80%
of the webservers we measure will reduce their packet size when asked, but less than half will include a
fragmentation header as RFC 2460 requires when the MTU supplied is 576 bytes.
Server MSS Portion Fail rate (1280)
1440
1220
1420
1380
other
83.6%
5.3%
5.0%
3.8%
1.8%
1.6%
–
2.2%
23.5%
–
Table 4: Top four MSS values advertised for IPv6
and their corresponding failure rate. As with IPv4,
IPv6 systems that advertise an MSS of 1380 are
much more likely to fail PMTUD.
5.
IPV6 PMTUD BEHAVIOUR
As noted in section 3.3, there are only 1,067 unique IPv6
addresses that are globally routable in the Alexa list of the
top million websites [11]. We could not establish a connec-
tion to 15%, which we exclude from the following results so
the summary statistics for PMTUD behaviour are not un-
derestimated. However, the results are encouraging for the
910 websites for which we could test PMTUD behaviour.
Table 3 shows the results of our IPv6 experiments. The
proportion of servers that fail at PMTUD for 1280 bytes is
2.4%. The corresponding failure rate for the IPv4 servers for
this population is 4.4%. This may be due to fewer webservers
having an IPv6 ﬁrewall enabled where they currently have
an IPv4 ﬁrewall. As with the IPv4 tests, we assessed the
responsiveness of each IPv6 webserver to ping. For the sys-
tems where PMTUD is successful 96% respond to ping, but
for the failures 50% do so. Table 4 shows the top four server
MSS values seen, and their corresponding failure rates. As
with IPv4, systems that advertise an MSS of 1380 are much
more likely to fail PMTUD.
Less than half of the webservers successfully complete PM-
TUD when sent a PTB message with an MTU of 576. In
this scenario, we expect to receive all packets with an IPv6
fragmentation header even if the packets are not sent in
fragments. This restricts our ability to directly test if a
webserver receives and acts on a PTB message if it sends
packets too small to test with an MTU of 1280 bytes. Ta-
ble 3 contains results for Tokyo; we saw nearly identical
results and behaviour with the other vantage points. We
did not observe many TCP retransmissions in these tests;
72% of the failures saw just one retransmission, usually at
least 45 seconds after the PTB was sent, before the TCP
connection timed out. It is possible that another software
bug is behind this result; some ﬁrewalls discard IPv6 pack-
ets with a fragmentation header if they are not part of a
fragmented series [22], even if their only rule is to accept all
IPv6 packets.
For 32% of 1480 MTU tests and 17% of 1280 MTU tests,
we are not able to measure PMTUD behaviour because the
packets we receive are too small. Because this ﬁgure is rela-
tively large compared with the IPv4 population of section 4,
we examined the packet traces for further evidence of PM-
TUD behaviour related to tunnelled IPv6 connectivity be-
tween our vantage points and the webservers. We ﬁltered
TCP connections where the maximum packet size received
was less than the maximum packet size suggested by the
server’s MSS advertisement, and then looked for clusters of
maximum packet size values. For all vantage points, we see
clusters at 1480 and 1280, and for Tokyo we also see a cluster
at 1476 corresponding to GRE [23]. However, the propor-
tion of hosts that send packets smaller than their MSS is
only 20%, and these clusters account for half of the maxi-
mum packet sizes we see. This indicates 10% of TCP ses-
sions in our tests were carried over a tunnel, suggesting that
tunnelled IPv6 connectivity is not very prevalent. This re-
sult is consistent with other measurements of the prevalence
of IPv6 tunnelling [24], which have shown the percentage of
tunnelled IPv6 reducing from 40% in 2004 to 10% in 2010.
6. CONCLUSION
Path MTU Discovery is a relatively simple algorithm to
implement, but it depends on packet too big messages being
generated by routers, forwarded by middleboxes, and correct
action being taken by end-hosts. This paper shows the pro-
portion of webservers that receive and correctly act on PTB
messages is at least 80% for both IPv4 and IPv6. We identi-
ﬁed two strategies to signiﬁcantly cut the IPv4 failure rate.
First, operating systems that refuse to lower their packet
size below a threshold must be modiﬁed to not set the DF
bit if the Path MTU is lower than the threshold. Second,
the middleboxes that rewrite the TCP MSS to 1380 bytes
should be debugged to ensure they correctly forward PTB
messages, if it is possible to identify their manufacturers.
A third strategy, which is arguably the most challenging,
is to educate system administrators about the necessity of
forwarding PTB messages.
Acknowledgements
This work was supported by New Zealand Foundation for
Research Science and Technology (FRST) contract UOW-
X0705. The following provided data or access to hosts to
conduct experiments from: Emile Aben (RIPE NCC), Daniel
Andersen (CAIDA), Kenjiro Cho (IIJ), Bill Owens (NYSER-
Net) and Bill Walker (SNAP Internet).
1077. REFERENCES
[1] Alberto Medina, Mark Allman, and Sally Floyd.
Measuring interactions between transport protocols
and middleboxes. In IMC ’04, pages 336–341,
Taormina, Sicily, Italy, October 2004.
[2] J. Mogul and S. Deering. Path MTU Discovery. RFC
1191, IETF, November 1990.
[3] J. McCann, S. Deering, and J. Mogul. Path MTU
Discovery for IP version 6. RFC 1981, IETF, August
1996.
[4] C.A. Kent and J.C. Mogul. Fragmentation considered
harmful. ACM SIGCOMM Computer Communication
Review, 17(5):390–401, 1987.
[5] K. Lahey. TCP problems with path MTU discovery.
RFC 2923, IETF, September 2000.
[6] Richard van den Berg and Phil Dibowitz. Over-zealous
security administrators are breaking the Internet. In
Proceedings of LISA ’02: Sixteenth Systems
Administration Conference, pages 213–218, Berkeley,
CA, November 2002.
[7] Geoﬀ Huston. A tale of two protocols: IPv4, IPv6,
MTUs and fragmentation. http:
//www.potaroo.net/ispcol/2009-01/mtu6.html
[8] Geoﬀ Huston. Mutterings on MTUs.
.
http://www.potaroo.net/ispcol/2009-02/mtu.html
.
[9] Iljitsch van Beijnum. IPv6 and path MTU discovery
black holes. http:
//www.bgpexpert.com/article.php?article=117
[10] S. Deering and R. Hinden. Internet protocol, version 6
(IPv6) speciﬁcation. RFC 2460, IETF, December 1998.
.
[11] Alexa. Top 1,000,000 sites.
http://www.alexa.com/topsites.
[12] TBIT: TCP behaviour inference tool.
http://www.icir.org/tbit/.
[13] Matthew Luckie, Kenjiro Cho, and Bill Owens.
Inferring and debugging path MTU discovery failures.
In IMC ’05, pages 193–198, San Francisco, CA, USA,
October 2005.
[14] M. Mathis and J. Heﬀner. Packetization layer path
MTU discovery. RFC 4821, IETF, March 2007.
[15] Matt Mathis. Raising the Internet MTU.
http://staff.psc.edu/mathis/MTU/.
[16] Microsoft. Documentation for
EnablePMTUDBHDetect registry key.
[17] F. Gont. ICMP attacks against TCP. RFC 5927,
IETF, July 2010.
[18] Wget 1.12. http://www.gnu.org/software/wget/.
[19] Matthew Luckie. Scamper.
http://www.wand.net.nz/scamper/.
[20] WITS: Waikato Internet traﬃc storage.
http://www.wand.net.nz/wits/waikato/.
[21] Matthew Luckie. kern/146628: [patch] TCP does not
clear DF when MTU is below a threshold. http:
//www.freebsd.org/cgi/query-pr.cgi?pr=146628.
[22] Matthew Luckie. kern/145733: [patch] ipfw ﬂaws with
IPv6 fragments. http:
//www.freebsd.org/cgi/query-pr.cgi?pr=145733.
[23] S. Hanks, T. Li, D. Farinacci, and P. Traina. Generic
routing encapsulation (GRE). RFC 1701, IETF,
October 1994.
[24] RIPE Labs. Untunneling IPv6.
http://labs.ripe.net/content/untunneling-ipv6.
108