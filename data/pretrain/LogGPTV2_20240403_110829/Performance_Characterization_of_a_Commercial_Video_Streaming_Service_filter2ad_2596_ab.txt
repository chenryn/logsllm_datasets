status, and (3) Dread: time to read the chunk’s ﬁrst byte and write
it to the socket, including the delay to read from local disk or back-
end. The backend latency (DBE) is measured at the CDN service
and includes network delay. Characterizing backend service prob-
lems is out of scope for this work; we found that such problems are
relatively rare.
A key limitation of player-side instrumentation is that applica-
tion layer metrics capture the mix of download stack latency, net-
work latency, and server-side latency. To isolate network perfor-
mance from end-host performance, we measure the end-to-end net-
work path at the CDN host kernel’s TCP stack. Since kernel-space
latencies are relatively very low, it is reasonable to consider this
view as representative of the network path performance. Specif-
ically, the CDN service snapshots the Linux kernel’s tcp_info
structure for the player TCP connection (along with context of the
chunk being served). The structure includes TCP state such as
smoothed RTT, RTT variability, retransmission counts, and sender
congestion window. We sample the path performance periodically
every 500ms2; this allows us to observe changes in path perfor-
mance.
Download Phase. The download phase is the window between
arrivals of the ﬁrst and the last bytes of the chunk at the player,
i.e., the last-byte delay, DLB. It depends on the chunk size, which
depends on chunk bitrate and duration. To identify chunks suffer-
ing from low throughput, on the client side we record the requested
bitrate and the last-byte delay. To understand the network path per-
formance and its impact on TCP, we snapshot TCP variables from
the CDN host kernel at least once per-chunk (as described above).
Playout Phase. As a chunk is downloaded, it is added to the play-
back buffer. If the playback buffer does not contain enough data,
the player pauses and waits for sufﬁcient data; in case of an already
playing video, this causes a rebuffering event. We instrument the
player to measure the number (bufcount) and duration of rebuffer-
ing events (bufdur) per-chunk played.
Each chunk must be decoded and rendered at the client. In the
absence of hardware rendering (i.e., GPU), chunk frames are de-
coded and rendered by the CPU, which makes video quality sen-
sitive to CPU utilization. A slow rendering process drops frames
to keep up with the encoded frame rate. To characterize render-
ing path problems, we instrument the Flash player to collect the
average rendered frame rate per chunk (avgf r) and the number of
dropped frames per chunk (dropf r). A low rendering rate, how-
ever, is not always indicative of bad performance; for example,
when the player is in a hidden tab or a minimized window, video
frames are dropped to reduce CPU load [14]. To identify these sce-
narios, the player collects a variable (vis) that records if the player
is visible when the chunk is displayed. Table 2 summarizes the
metrics collected for each chunk at the player and CDN.
2.2 Per-session Instrumentation
In addition to per-chunk milestones, we collect session metadata;
see Table 3. A key to end-to-end analysis is to trace session per-
formance from the player through the CDN (at the granularity of
chunks). We implement tracing by using a globally unique session
ID and per-session chunk IDs.
2The frequency is chosen to keep overhead low in production.
Location
Statistics
Player (Delivery)
Player (Rendering)
CDN (App layer)
CDN (TCP layer)
sessionID, chunkID, DF B , DLB , bitrate
bufdur, bufcount, vis, avgf r, dropf r
sessionID, chunkID, DCDN (wait, open, and
read), DBE , cache status, chunk size
CWND, SRTT, SRTTVAR, retx, MSS
Table 2: Per-chunk instrumentation at player and CDN.
Location
Statistics
Player
CDN
sessionID, user IP, user agent, video length
sessionID, user IP, user agent, CDN PoP, CDN
server, AS, ISP, connection type, location
Table 3: Per-session instrumentation at player and CDN.
3. MEASUREMENT DATASET
We study 65 million VoD sessions (523m chunks) with Yahoo,
collected over a period of 18 days in September 2015. These ses-
sions were served by a random subset of 85 CDN servers across the
US. Our dataset predominantly consists of clients in North America
(over 93%).
Figure 3(a) shows the cumulative distribution of the length of
the videos. All chunks in our dataset contain six seconds of video
(except, perhaps, the last chunk).
We focus on desktop and laptop sessions with Flash-based play-
ers. The browser distribution is as follows: 43% Chrome, 37%
Firefox, 13% Internet Explorer, 6% Safari, and about 2% other
browsers; the two major OS distributions in the data are Windows
(88.5% of sessions) and OS X (9.38%). We do not consider cellu-
lar users in this paper since the presence of ISP proxies affects the
accuracy of our ﬁndings.
The video viewership and popularity of videos is heavily skewed
towards popular content; see Figure 3(b). We ﬁnd that top 10% of
most popular videos receive about 66% of all playbacks.
Data preprocessing to ﬁlter proxies. A possible pitfall in our
analysis is the existence of enterprise or ISP HTTP proxies [35],
since the CDN server’s TCP connection would terminate at the
proxy, leading to network measurements (e.g., RTT) reﬂecting the
server-proxy path instead of the client. We ﬁlter sessions using a
proxy when: (i) we see different client IP addresses or user agents [34]
between HTTP requests and client-side beacons3, or (ii) the client
IP address appears in a very large number of sessions (e.g., more
more minutes of video per day than there are minutes in a day).
After ﬁltering proxies, our dataset consists of 77% of sessions.
Ethical considerations: Our instrumentation methodology is based
on logs/metrics about the trafﬁc, without looking at packet pay-
load or video content. For privacy reasons, we do not track users
(through logging) hence we cannot study access patterns of individ-
ual users. Our analysis uses client IP addresses internally to iden-
tify proxies and perform coarse-grained geo-location; after that, we
use opaque session IDs to study the dataset.
4. CHARACTERIZING PERFORMANCE
In this section, we characterize the performance of each compo-
nent of the end-to-end path, and show the impact on QoE. Prior
work has shown that important stream-related factors affect the
QoE: startup delay, rebuffering ratio, video quality (average bi-
3A beacon is a message sent back from the client to the analytic
servers, carrying information.
501Latency
Description
DF B
DLB
DCDN
DBE
DDS
rtt0
Time to fetch the ﬁrst byte
Time to download the chunk (ﬁrst to last byte)
CDN latency (= Dwait + Dopen + Dread)
Backend latency in cache miss
Client’s download stack latency
Network round-trip time during the ﬁrst-byte ex-
change
Table 4: Latency notations and their description
(a) CCDF of video lengths
(one month)
(b) Rank vs. popularity (one
day)
Figure 3: Length and popularity of videos in the dataset.
trate), and the rendering quality [14, 37]. They have developed
models for estimating QoE scores of videos by assigning weights
to each of these stream metrics to estimate a user behavior metric
such as abandonment rate.
We favor looking at the impact on individual QoE factors instead
of a single QoE score to assess the signiﬁcance of performance
problems. This is primarily because of the impact of content on
user behavior (and hence, QoE). First, user behavior may be differ-
ent for long-duration content such as Netﬂix videos (e.g., users may
be more patient with a longer startup delay) than short-duration
content (our case). Second, the type of content being viewed im-
pacts user behavior (and hence the weights of QoE factors). For
example, the startup delay for a news video (e.g., “breaking news”)
may be more important to users than the stream quality; while for
sports videos, the quality may be very important. Given the variety
of Yahoo videos, we cannot use a one-size-ﬁts-all set of weights
for a QoE model. Moreover, the results would not generalize to
all Internet videos. Instead, we show the impact of each problem
directly on the QoE factors.
4.1 Server-side Performance Problems
Yahoo uses the Apache Trafﬁc Server (ATS), a popular caching
proxy server [2], to serve HTTP requests. The trafﬁc engineering
system maps clients to CDN nodes using a function of geography,
latency, load, cache likelihood, etc. In other words, the system tries
to route clients to a server that is likely to have a hot cache. The
server ﬁrst checks the main memory cache, then tries the disk, and
ﬁnally sends a request to a backend server if needed.
Server latencies are relatively low, since the CDN and the back-
end are well-provisioned. About 5% of sessions, however, expe-
rience a QoE problem due to the server, and the problems can be
persistent as we show below. Figure 4 shows the impact of the
server-side latency for the ﬁrst chunk on the startup delay (time to
play) at the player.
Figure 4: Impact of server latency on QoE (startup time), error
bars show the interquartile range (IQR).
Figure 5: CDN latency breakdown across all chunks.
1. Asynchronous disk read timer and cache misses cause high
server latency. Figure 5 shows the distribution of each component
of CDN latency across chunks; it also includes the distribution of
total server latency for chunks broken by cache hit and miss. Most
of the chunks have a negligible waiting delay (Dwait  10ms),
the chance of future read delays increases; the mean ratio of high-
latency chunks in sessions with at least one such chunk is 60%
(median of 60%).
One possible cause for persistent latency, even when the cache
hit ratio is high, is a highly loaded server that causes high serv-
ing latency; however, our analysis shows that server latency is not
correlated with load4. This is because the CDN servers are well
provisioned to handle the load.
Instead, the unpopularity of the content is a major cause of the
persistent server-side problems. For less popular videos, the chunks
often need to come from disk, or worse yet, the backend server.
Figure 6(a) shows the cache miss percentage versus video rank
(most popular video is ranked ﬁrst) using data from one day. The
cache miss ratio drastically increases for unpopular videos. Even
on a cache hit, unpopular videos experience higher server delay, as
shown in Figure 6(b). The ﬁgure shows mean server latency af-
ter removing cache misses (i.e., no backend communication). The
unpopular content generally experiences a higher latency due to
higher read (seek) latency from disk.
Take-away. The persistence of cache misses could be addressed by
pre-fetching the subsequent chunks of a video session after the ﬁrst
miss. Pre-fetching of subsequent chunks would particularly help
with unpopular videos since backend latency makes up a signiﬁcant
part of their overall latency and could be avoided.
When an object cannot be served from local cache, the request
will be sent to the backend server. For a popular object, many con-
current requests may overwhelm the backend service; thus, the ATS
retry timer is used to reduce the load on the backend servers; the
timer introduces extra delay for cases where the content is available
on local disk.
3. Load vs. performance due to cache-focused client mapping.
We have observed that more heavily loaded servers offer lower
CDN latency (note that CDN latency does not include the network
latency, but only the time a server takes to start serving the ﬁle).
This result was initially surprising since we expect busier servers
to have worse performance; however, this can be explained by the
cache-focused mapping CDN feature: As a result of cache-based
4We estimated load as of number of parallel HTTP requests, ses-
sions, or bytes served per second.
assignment of clients to CDN servers, servers with less popular
content have more chunks with either higher read latency as the
content is not fresh in memory (and the ATS retry-timer), or worse
yet, need to be requested from backend due to cache-misses.
While unpopular content leads to lower performance, because of
lower demand it also produces fewer requests, hence servers that