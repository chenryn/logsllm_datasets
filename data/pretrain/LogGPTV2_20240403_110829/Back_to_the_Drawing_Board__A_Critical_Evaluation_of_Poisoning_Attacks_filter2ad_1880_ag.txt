model misclassify the samples with adversary-chosen back-
door trigger. Backdoor attacks are semantic, if the trigger
is naturally present in samples [3], [61] and artificial if the
trigger needs to manually added at test time [67]. Bagdasaryan
at al. [3] demonstrate a constrain-and-scale attack against
simple Average AGR to inject semantic backdoor in the global
model. They show that their attacks achieve accuracy of >90%
on backdoor task in a next word prediction model. Wang et
al. [61] propose data and model poisoning attacks to inject
backdoor to misclassify out-of-distribution samples. Xie et
al. [67] show how multiple colluding clients can distribute
backdoor trigger to improve the stealth of poisoned updates.
Backdoor (as well as targeted) attacks can be further divided
in specific-label and arbitrary-label attacks. For a backdoored
test sample, specific-label attack aims to misclassify it
to
a specific target class, while arbitrary-label attack aims to
misclassify it to any class.
Note that, trivial extensions of the targeted and backdoor
attack algorithms to mount untargeted attacks cannot succeed,
because untargeted attacks aim at affecting almost all FL
clients and test inputs. For instance, a simple label flipping
based data poisoning [61] can insert a backdoor in FL with
state-of-the-art defenses. However, such label flipping based
untargeted poisoning attacks have no effect even on unpro-
tected FL (Section V-A).
2) Existing Defenses Against Targeted and Backdoor At-
tacks: In Section II-B, we discuss the defenses against untar-
geted poisoning in detail. Here, we review existing defenses
against targeted and backdoor attacks. FoolsGold [26] identi-
fies clients with similar updates as attackers, but incur very
high losses in performances as noted in [25]. Sun et al. [58]
investigate efficacy of norm-bounding to counter targeted
poisoning and, as we will show,
is also effective against
untargeted poisoning. CRFL [66] counters backdoor attacks by
providing certified accuracy for a given test input, but incurs
large losses in FL performance (Table I). Defenses based on
pruning techniques [60], [63] remove parts of model that are
affected by targeted/backdoor attacks, and hence cannot be
used against untargeted attacks which affect the entire model.
n′(cid:80)
Algorithm 1 Our PGA model poisoning attack algorithm
1: Input: ∇{i∈[n′]}, θg, fagr, Dp
i∈[n′] ∥∇i∥
2: τ = 1
3: θ′ ← ASGA(θg, Dp)
4: ∇′ = θ′ − θg
5: ∇′ = fproject(fagr,∇′, τ,∇{i∈[n′]})
6: Output ∇′
▷Compute norm threshold
▷τ is given for norm-bounding AGR
▷Update using stochastic gradient ascent
▷Compute poisoned update
▷Scale ∇′ appropriately
Algorithm 2 The projection function (fproject) of our PGA
from Section IV-B3.
1: Input: fagr, ∇′, τ, ∇{i∈[n′]}
2: d∗ = 0
▷Initialize maximum deviation
3: γ∗ = 1 ▷Optimal scaling factor that maximizes deviation in (1)
4: ∇′ = ∇′×τ
▷Scale ∇′ to have norm τ
∥∇′∥
5: ∇b = favg(∇{i∈[n′]})
▷Compute reference benign update
6: for γ ∈ [1, Γ] do
∇′′ = γ · ∇′
7:
d = ∥fagr(∇′′
8:
γ∗ = γ if d > d∗
9:
10:
γ = γ + δ
11: end for
12: Output γ∗ · ∇′
{i∈[m]},∇{i∈[n′]}) − ∇b∥
▷Update optimal γ
▷Update γ
B. Missing details of our data and model poisoning attacks
from Sections IV-B2 and IV-B3
1) Missing data poisoning attack methods: Multi-krum.
Following [55], our attack aims to maximize the number of
poisoned updates in the selection set (S) of Multi-krum AGR
(Section II-C3). As the size of S is fixed, maximizing the
number of poisoned updates in S implicitly means minimizing
the number of benign updates. This objective is formalized as:
argmax
Dp⊂D′
p
m′ = |{∇ ∈ ∇′
{i∈[m]}|∇ ∈ S}|
(3)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1368
p is all the available labels flipped data and m′ is the
where D′
final number of poisoned updates in S of Multi-krum.
We solve (3) based on an observation: In Figure 2-(d) we
vary |Dp| and plot the fraction of corresponding poisoned up-
dates that Multi-krum selects. Let |D|avg be the average dataset
size of benign clients, e.g., |D|avg is 23.7 for FEMNIST.
Note from Figure 2-(d) that, even for |Dp| slightly higher
than |D|avg, Multi-krum easily discards most of the poisoned
updates. Only when |Dp| is small (∼10), Multi-krum selects
most of the poisoned updates. Hence, we sample Dp ⊂ D′
p,
where we vary |Dp| ∈ [0.5 · |D|avg, 3 · |D|avg], and check the
poisoning impact of Dp on Multi-krum; to reduce variance,
we repeat this 10 times for each |Dp|. We report the results
for Dp with the maximum poisoning impact.
Trimmed-mean. For Trimmed-mean AGR (Section II-C4),
we use the objective in (2), but it is cumbersome to solve it
directly. Hence, similar to our attacks on Average and Norm-
bounding AGRs, we use large |Dp| for poisoned data on each
of the compromised clients. Our approach is based on the
observation in Figure 2-(c): The higher the |Dp| (obtained
using DLF/SLF strategies),
the higher the Trimmed-mean
objective value, i.e., ∥∇p − ∇b∥.
2) Missing model poisoning attack methods: Multi-krum.
Similar to our DPA (Section IV-B2), the objective of our MPA
on Multi-krum is to maximize the number of poisoned updates
in the selection set S. We aim to find a scaling factor γ for
∇′ such that maximum number of ∇′′ = γ∇′ are selected in
S. This is formalized below:
argmax
γ∗∈R
m = |{∇ ∈ ∇′′
{i∈[m]}|∇ ∈ S}|
(4)
To solve the optimization in (4), our fproject searches for the
maximum γ in a pre-specified range [1, Γ] such that Multi-
krum selects all the scaled poisoned updates. Specifically, in
Algorithm 2, instead of computing the deviation (line-8), we
compute the number of ∇′′ selected in S and update γ∗ if S
has all of ∇′′s.
Trimmed-mean. Here, we directly plug Trimmed-mean al-
gorithm in Algorithm 2 (line-8). Our attack is similar to
that of [55], but instead of using one of several perturbation
vectors, ω’s, we use stochastic gradient ascent to tailor ω to
the entire FL setting ( e.g., θg, data, optimizer, etc.) to improve
the attack impact.
C. Experimental setup
Real-world FL datasets [1], [49] are proprietary and cannot
be publicly accessed. Hence, we follow the literature on
untargeted poisoning in FL [5], [23], [55], [59] and focus on
image and categorical datasets. But, we ensure that our setup
embodies the production FL [32], e.g., by using large number
of clients with extremely non-iid datasets.
1) Datasets and Model Architectures: FEMNIST [13],
[18]
is a character recognition classification task with 3,400
clients, 62 classes (52 for upper and lower case letters and 10
for digits), and 671,585 grayscale images. Each client has data
of her own handwritten digits or letters. Considering the huge
Figure 6: Even with a very large number of FL rounds (5,000),
the state-of-the-art model poisoning attacks with M=0.1%
cannot break the robust AGRs (Section V-B).
number of clients in real-world cross-device FL (up to 1010),
we further divide each of the clients’ data in p ∈ {2, 5, 10}
non-iid parts using Dirichlet distribution [42] with α = 1.
Increasing the Dirichlet distribution parameter, α, generates
more iid datasets. Unless specified otherwise, we set p = 10,
i.e., the total number of clients is 34,000. We use LeNet [35]
architecture.
CIFAR10 [34]
is a 10-class classification task with 60,000
RGB images (50,000 for training and 10,000 for testing), each
of size 32 × 32. Unless specified otherwise, we consider
1,000 total FL clients and divide the 50,000 training data
using Dirichlet distribution [42] with α = 1. We use VGG9
architecture with batch normalization [56].
Purchase [51]
is a classification task with 100 classes and
197,324 binary feature vectors each of length 600. We use
187,324 of total data for training and divide it among 5,000
clients using Dirichlet distribution with α = 1. We use
validation and test data of sizes 5,000 each. We use a fully
connected network with layer sizes {600, 1024, 100}.
2) Details of Federated learning and attack parameters:
For FEMNIST, we use 500 rounds, batch size, β = 10, E = 5
local training epochs, and in the eth round use SGD optimizer
with a learning rate η = 0.1 × 0.995e for local training;
we select n = 50 clients per round and achieve baseline
accuracy Aθ=82.4% with N=34,000 clients. For CIFAR10,
we use 1,000 rounds, β = 8, E = 2, and in the eth round use
SGD with momentum of 0.9 and η = 0.01× 0.9995e; we use
n = 25 and achieve Aθ=86.6% with N=1,000. For Purchase,
we use 500 rounds, β = 10, E = 5, and in the eth round
use SGD with η = 0.1 × 0.999e; we use n = 25 and achieve
Aθ=81.2% with N=5,000.
We generate large poisoned data Dp required for our DPAs
(Section IV-B2) by combining the dataset of compromised
clients and adding Gaussian noise to their features. We round
the resulting feature for categorical Purchase dataset.
D. Explanations of effects of |D|avg from Section V-C2
At M=1%, Iθ’s of STAT-OPT on CIFAR10 + Normb reduce
with increase in |D|avg. This is because, increasing |D|avg
improves the quality of updates of benign clients, but does
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1369
01k2k3k4k5kNumber of FL rounds20406080Global model accuracy (%)CIFAR10Normb + STAT-OPTMKrum + DYN-OPTTrmean + PGA01k2k3k4k5kNumber of FL rounds020406080FEMNISTNormb + DYN-OPTMKrum + DYN-OPTTrmean + PGATable V: The architecture of the surrogate model that we use
to emulate the unknown architecture setting (Section V-C4).
Layer name
Convolution + Relu
Convolution + Relu
Max pool
Max pool
Fully connected + Relu
Softmax
Layer size
5 × 5 × 32
5 × 5 × 64
2 × 2
2 × 2
1024
62
Figure 7: As discussed in Section V-C4, impacts of the DPA-
DLF attack from Section IV-B2 reduce if the architectures of
the surrogate and the global model are different.
not improve the attacks. Hence, when the benign impact of
benign updates overpowers the poisoning impact of poisoned
updates, Iθ’s reduce.
On the other hand, Iθ’s of any attacks on FEMNIST with
robust AGRs do not change with varying |D|avg. This is
because, FEMNIST is an easy task, and therefore, the presence
of compromised clients does not affect the global models.
Interestingly, Iθ of MPAs on CIFAR10 with Average AGR
increases with |D|avg. This is because, due to the difficulty
of CIFAR10 task, MPAs on CIFAR10 with Average AGR
are very effective and when the server selects even a single
compromised client, it completely corrupts the global model.
E. Miscellaneous figures
Figure 8: All data poisoning attacks have negligible impacts on
cross-silo FL, when compromised clients are concentrated in a
few silos or distributed uniformly across silos (Section V-D).
Below, we provide all the missing figures and the corre-
sponding sections in main paper.
• Figure 6 for Section V-B shows the impacts of strongest
of model poisoning attacks on robust AGRs over a very
large number of FL rounds.
• Figure 7 for Section V-C4 shows impact of unknown
architecture on our state-of-the-art data poisoning attacks
from Section IV-B2. Table V shows the convolutional
neural network architecture that the adversary uses as a
substitute to the true LeNet architecture.
• Figure 8 for Section V-D shows impacts of data poisoning
attacks on cross-silo FL.
• Figure 10 for Section V-C3 shows impacts of poisoning
attacks for increasing the number of clients selected in
each FL round.
• Figures 9 and 11 for Section V-C2 show the attack
impacts and accuracy of the global model, respectively,
when the average size of benign clients’ local data
increases.
• Figure 12 for Section V-C2 shows attack impacts (on the
left y-axes) and global model accuracy (on the right y-
axes) for Multi-krum and Trimmed-mean robust AGRs
for CIFAR10 and FEMNIST datasets.
Figure 9: With 1% compromised clients, increasing |D|avg
has no clear pattern of effects of on attack impacts, but it
increases the global model accuracy as shown in Figure 11.
Figure 12 shows the plots of attack impacts and the global
model accuracy for Multi-krum and Trimmed-mean AGRs.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1370
00.010.1110Compromised clients %0246810121416Attack impact (%)The unknown model architecture caseNo attackLenet AvgLenet NormbLenet TrmeanConv AvgConv NormbConv Trmean00.010.1110Compromised clients %020406080100Attack impact (%)FEMNIST + Cross-silo FLNo attackAvgNormbMkrumTrmean00.010.1110Compromised clients %020406080100CIFAR10 + Cross-silo FL2050100200020406080100Attack impact (%)FEMNIST + AverageDPA-DLFMPANo attack2050100200020406080100FEMNIST + Norm-boundDPA-DLFSTAT-OPTDYN-OPTPGANo attack10.012.516.725.050.0Average local data size020406080100Attack impact (%)CIFAR10 + AverageDPA-SLFMPANo attack10.012.516.725.050.0Average local data size020406080100CIFAR10 + Norm-boundDPA-SLFSTAT-OPTDYN-OPTPGANo attackFigure 10: As discussed in Section V-C3, the number of clients, n, chosen in each FL round has no noticeable effect on the
attack impacts, with the exception of model poisoning on Average AGR. We use M = 1% of compromised clients.
Figure 11: Effect on the accuracy of global models of the
average of local dataset sizes, |D|avg, of the benign clients,
with 1% compromised clients. As discussed in Section V-C2,
increasing |D|avg increases the accuracy of the global models.
Figure 12: We make observations similar to Average and
Norm-bound AGRs (Figures 9, 11 in Section V-C2) for Multi-
krum and Trimmed-mean about the effect of |D|avg on the
attack impacts (left y-axes, solid lines) and on the global model
accuracy (right y-axes, dotted lines), with M=1%. All y-axes
are from 0 to 100.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1371
10305070020406080100Attack impact (%)FEMNIST + AverageDPA-DLFMPANo attack10305070020406080100FEMNIST + Norm-boundDPA-DLFSTAT-OPTDYN-OPTPGANo attack10305070020406080100FEMNIST + Multi-krum10305070020406080100FEMNIST + Trimmed-mean1020253040FL clients per round020406080100Attack impact (%)CIFAR10 + AverageDPA-SLFMPANo attack1020253040FL clients per round020406080100CIFAR10 + Norm-boundDPA-SLFSTAT-OPTDYN-OPTPGANo attack1020253040FL clients per round020406080100CIFAR10 + Multi-krum1020253040FL clients per round020406080100CIFAR10 + Trimmed-mean2050100200020406080100Global model accuracy (%)FEMNIST + AverageDPA-DLFMPANo attack2050100200020406080100FEMNIST + Norm-boundDPA-DLFSTAT-OPTDYN-OPTPGANo attack10.012.516.725.050.0Average local data size020406080100Global model accuracy (%)CIFAR10 + AverageDPA-SLFMPANo attack10.012.516.725.050.0Average local data size020406080100CIFAR10 + Norm-boundDPA-SLFSTAT-OPTDYN-OPTPGANo attack2050100200020406080100Attack impact (%)FEMNIST + Multi-krumDPA-DLFSTAT-OPTDYN-OPTPGANo attack2050100200FEMNIST + Trimmed-mean10.012.516.725.050.0Average local data size020406080100Attack impact (%)CIFAR10 + Multi-krum10.012.516.725.050.0Average local data sizeCIFAR10 + Trimmed-mean020406080100Global model ccuracy (%)020406080100Global model ccuracy (%)