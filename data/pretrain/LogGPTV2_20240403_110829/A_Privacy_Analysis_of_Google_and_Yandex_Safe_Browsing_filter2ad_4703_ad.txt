106
100
101
102
104
105
106
103
Hosts
(a)
Mean
Alexa hosts
Random hosts
102
103
104
Hosts
(d)
25
20
15
10
5
0
105
106
102
103
(b)
Min.
104
Hosts
(e)
(c)
Max.
25
20
15
10
5
0
105
106
102
103
105
106
104
Hosts
(f)
Fig. 5: Distribution of URLs and decompositions on hosts from the two datasets. Figure (a) presents the distribution of URLs
over hosts, while (b) presents its cumulative distribution. (c) shows the distribution of decompositions over hosts. (d), (e) and
(f) present the mean, minimum and maximum number of URL decompositions on the hosts.
where {xi} are the n data points. The standard error of the
estimate is given by: σ = ˆα−1√
n = 0.0004.
Fig. 5c presents the number of unique decompositions per
host for the two datasets. These numbers are very close to
that of the number of URLs and hence verify a similar power
law distribution. The domains that cover the majority of the
decompositions contain URLs that are difﬁcult to re-identify
(due to Type I collisions). While, the domains representing the
tail of the distribution provide URLs which can be re-identiﬁed
with high certainty using only a few preﬁxes.
In Fig. 5d,5e,5f, we present
the mean, minimum and
maximum number of decompositions of URLs per domain in
the two datasets. We observe that 51% of the random domains
present a maximum of 10 decompositions for a URL, while
the same is true for 41% of the Alexa domains. The minimum
value on the other hand is higher for random domains: 71% of
the Alexa domains have a minimum of 2 per URL, while the
fraction is up to 86% in case of random domains. The average
number of decompositions for over 46% of the hosts from the
two datasets lies in the interval [1, 5]. Hence, a URL on these
hosts can generate on an average a maximum of
= 10
Type I collisions on two preﬁxes. As a result, URLs on these
hosts can be re-identiﬁed using only a few preﬁxes.
5
2
(cid:2)
(cid:3)
We now consider Type II collisions. A Type II collision
354
occurs on URLs that share common decompositions. Thus,
in order for a Type II collision to occur,
the number of
decompositions per domain must be at least 232. However, the
maximum number of decompositions per domain from either
of the datasets is of the order of 107 (see Fig. 5c), which is
smaller that 232. This implies that Type II collisions do not
occur for any of the hosts in our datasets.
As for Type I collisions, we observed that the number of
collisions found was proportional to the number of unique
decompositions on the host. On the one hand, we found several
domains from both the datasets for which the number of such
collisions was as high as 1 million, while on the other hand, we
found over a hundred thousand domains for which the number
of such collisions was less than 20. Hence, many of the non-
leaf URLs on these hosts can be re-identiﬁed by inserting as
less as 3 preﬁxes per URL. Interestingly, we observed that
56% of the domains in the random dataset do not have Type
I collisions, while the same is true for around 60% of the
domains from the Alexa dataset. Consequently, URLs on these
domains can be easily re-identiﬁed using only 2 preﬁxes.
D. A Tracking System based on SB
Our analysis shows that it is possible to re-identify certain
URLs whenever multiple preﬁxes corresponding to them are
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:43:08 UTC from IEEE Xplore.  Restrictions apply. 
sent to the servers. Relying on this fact, GOOGLE and YANDEX
could potentially build a tracking system based on GSB and
YSB. The robustness of the system would depend on the
maximum number of preﬁxes per URL that they choose to
include in the client’s database. In the following, we denote
this parameter by δ. Clearly, the larger is the δ, the more robust
is the tracking tool. We note that its value is however chosen
according to the memory constraints on the client’s side.
#preﬁxes to be included.
Algorithm 1: Preﬁxes to track a URL
Data: link: a URL to be tracked and a bound δ: max.
Result: A list track-preﬁxes of preﬁxes to be included.
1 decomps, track-preﬁxes, type1-coll ← [ ]
2 dom ← get_domain(link)
3 urls ← get_urls(dom)
4 for url ∈ urls do
6 if |decomps| ≤ 2 then
decomps ← decomps ∪ get_decomps(url)
for decomp ∈ decomps do
5
7
8
12
13
14
15
16
17
18
19
20
9 else
10
11
track-preﬁxes ← track-preﬁxes ∪
32-prefix(SHA-256(decomp))
type1-coll ← get_type1_coll(link)
common-preﬁxes ←
32-prefix(SHA-256(dom)) ∪
32-prefix(SHA-256(link))
if link is a leaf or |type1-coll| == 0 then
track-preﬁxes ← common-preﬁxes
else
if
|type1-coll| ≤ δ then
track-preﬁxes ← common-preﬁxes
for type1-link ∈ type1-coll do
track-preﬁxes ←
32-prefix(SHA-256(type1-link))
∪ track-preﬁxes
else
track-preﬁxes ← common-preﬁxes
/* Only SLD can be tracked. */
The tracking system would essentially work as follows.
First, GOOGLE and YANDEX choose a δ ≥ 2, and build
a shadow database of preﬁxes corresponding to at most δ
decompositions of the targeted URLs. Second, they push those
preﬁxes in the client’s database. GOOGLE and YANDEX can
identify individuals (using the SB cookie) each time their
servers receive a query with at least two preﬁxes present in
the shadow database.
The crucial point to address is how they may choose the
preﬁxes for a given target URL. In Algorithm 1, we present
a simple procedure to obtain these preﬁxes for a target URL
given a bound δ. The algorithm ﬁrst identiﬁes the domain that
hosts the URL, which in most cases will be a Second-Level
Domain (SLD) (see Line 2). Using their indexing capabilities,
the SB providers then recover all URLs hosted on the domain
and then obtain the set of unique decompositions of the URLs
(Line 3-5). Now, if the number of such decompositions is less
than 3, then the preﬁxes to be included for the target URL
355
are those corresponding to these decompositions (Line 6-8).
Otherwise, the algorithm determines the number of Type I
collisions on the URL (Line 9-20). If no Type I collision exists
or if the URL represents a leaf, then only two preﬁxes sufﬁce
to re-identify the URL. One of these preﬁxes is chosen to be
the preﬁx of the URL itself and the other one can be that of
any arbitrary decomposition. We however choose to include
the preﬁx of the domain itself (Line 11-13). If the number of
Type I collisions is non-zero but less than or equal to δ, then
the preﬁxes of these Type I URLs are also included (Line 15-
18). Finally, if the number of Type I collisions is greater than
δ, then the URL cannot be precisely tracked. Nevertheless,
including the preﬁx of the URL and that of its domain allows
to re-identify the SLD with precision (Line 19-20). We note
that if the preﬁxes are inserted according to this algorithm, the
probability that the re-identiﬁcation fails is
(cid:2)
(cid:3)
δ .
1
232
To illustrate the algorithm, let us consider the following
CFP URL for a conference named PETS: petsymposium.org/
2016/cfp.php. We ﬁrst assume that the SB providers wish to
identify participants interested in submitting a paper. Hence,
the target URL is the CFP URL. The URL has 3 decompo-
sitions similar to the ones given in Table III. Since the target
URL is a leaf, preﬁxes for the ﬁrst and last decompositions
would sufﬁce to track a client visiting the target URL. Now, let
us consider the case when the SB provider wishes to track a
client’s visit on petsymposium.org/2016 web page. The target
URL yields Type I collisions with: petsymposium.org/2016/
links.php and petsymposium.org/2016/faqs.php. Thus, the SB
provider would include the preﬁxes corresponding to these
2 URLs, that of petsymposium.org/2016 and of the domain
petsymposium.org/. In total, only 4 preﬁxes sufﬁce in this
case. Now, whenever the SB server receives the last
two
preﬁxes, then it learns that the client has visited the target
URL. Additionally, this allows the server to track the other 2
URLs that create Type I collisions. The same is possible for
the DSN CFP URL, however the SB server would need to
include many more preﬁxes as the SLD of the DSN CFP page
has over 50 Type I URLs.
It is also possible to re-identify a URL by aggregating re-
quests sent by the client. This can be achieved by exploiting the
temporal correlation between the queries of a user. YANDEX
and GOOGLE can identify the requests of a given user thanks
to the SB cookie. A user visiting: petsymposium.org/2016/
cfp.php (with preﬁx 0xe70ee6d1) is very likely to visit the
submission website: petsymposium.org/2016/submission/(with
preﬁx 0x716703db). Instead of looking at a single query, the
SB server now needs to correlate two queries. A user making
two queries for the preﬁxes 0xe70ee6d1 and 0x716703db
in a short period of time is planning to submit a paper.
VIII. BLACKLIST ANALYSIS
In this section, we present an analysis of the blacklists
provided by GOOGLE and YANDEX. Our analysis has the
objective to identify URLs that match multiple preﬁxes. These
URLs hence provide concrete examples of URLs/domains that
can be tracked by the SB providers.
A. Inverting the Digests
As a ﬁrst step in our analysis, we recover the preﬁx lists
of GOOGLE and YANDEX. We then use these lists to query
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:43:08 UTC from IEEE Xplore.  Restrictions apply. 
their servers using their respective APIs. This allows us to
obtain the lists of full digests. Our second step is an attempt
to identify the URLs which correspond to these preﬁxes. To
this end, we harvested phishing and malware URLs, domains,
and IP addresses from several sources and tested for their
belonging to the blacklists of preﬁxes. The list of all our
sources can be found in [29]. We also harvested 1,240,300
malware URLs, 151,331 phishing URLs and 2,488,828 URLs
of other categories from BigBlackList [30]. Lastly, we obtained
106,923,807 SLDs from the DNS Census 2013 project [31].
The project provides a public dataset of registered domains and
DNS records gathered in the years 2012-2013. We included
the last dataset to determine the percentage of preﬁxes in the
local database that correspond to SLDs. A description of all
the datasets employed in our analysis is given in Table VIII.
TABLE VIII: Dataset used for inverting 32-bit preﬁxes.
Dataset
Malware list
Phishing list
BigBlackList
DNS Census-13
Description
malware
phishing
malw., phish., porno, others
second-level domains
#entries
1,240,300
151,331
2,488,828
106,923,807
The results of our experiments are shown in Table IX.
We observe that reconstruction for GOOGLE preﬁxes using
Malware, Phishing and BigBlackList datasets is inconclusive:
5.9% for malwares and 0.1% for phishing websites. For
YANDEX, the situation is better but the majority of the database
still remains unknown. However, the DNS Census-13 dataset
produces a much larger reconstruction for all the lists except
that of the phishing database. The rate is as high as 55% for
YANDEX ﬁles. We highlight that phishing domains are short-
lived and since the DNS Census-13 dataset dates back to 2013,
the result of the reconstruction for phishing lists is very limited,
only 2.5% for GOOGLE and 5.6% for YANDEX. Nevertheless,
these results demonstrate that 20% of the GOOGLE malware
list represents SLDs, while 31% of the preﬁxes in the YANDEX
malware lists correspond to SLDs. Relying on our analysis of
the previous sections, we may conclude that these preﬁxes can
be re-identiﬁed with very high certainty.
It is pertinent to compare the result of our reconstruction
with a similar attempt with another list in the past. German
censorship federal agency called BPjM maintains a secret list
of about 3,000 URLs believed to be unsuitable for women
and children. The list is anonymized and distributed in the
form of MD5 or SHA-1 hashes as the “BPJM-Modul” [32].
Though similar to the lists handled by GOOGLE and YANDEX,
hackers have been able to retrieve 99% of the cleartext entries.
We have applied the same approach, yet the reconstruction
rate obtained has not been equally high. This proves that in
order to reconstruct the database in cleartext, one would need
high crawling capabilities and hence it is not achievable for
general users. Furthermore, unlike the BPjM list, the blacklists
provided by GSB and YSB are extremely dynamic. This
requires a user to regularly crawl web pages on the web, which
renders the reconstruction even more difﬁcult.
B. Orphan Preﬁxes
We now look for orphan preﬁxes in GSB and YSB. An
entry in the preﬁx list is called an orphan if no 256-bit digest
matches it in the corresponding list of full digests. We also look
for URLs in the Alexa list which generate an orphan preﬁx.
Table X presents the results of our ﬁndings. Both GOOGLE
and YANDEX have orphans. While GOOGLE has 159 orphan
preﬁxes, for YANDEX the numbers are astonishingly high. 43%
for ydx-adult-shavar, 99% for ydx-phish-shavar,
100% of
95% for ydx-sms-fraud-shavar and
the
and
ydx-yellow-shavar are orphans. We did not ﬁnd
any URL in the Alexa list matching a GOOGLE orphan preﬁx.
But there are 660 URLs with one parent: the preﬁx matches
one full digest. For YANDEX, we found 271 URLs matching
an orphan preﬁx and 20,220 URLs with one parent.
ydx-mitb-masks-shavar
preﬁxes
in
The presence of orphan preﬁxes is very difﬁcult to justify.
Moreover, the behavior of a browser on these preﬁxes is not
consistent. Some of the orphan preﬁxes are considered as
false positives by YANDEX while others are declared as true
positives. There are three possible explanations to argue the
presence of orphans. First, that there is an inconsistency be-
tween the preﬁx lists and the corresponding lists of full digests.
This could be due to a misconﬁguration, latency in the update
or the result of a development error. This particularly might
hold for GOOGLE since very few preﬁxes are orphans. Second,
that the services have intentionally noised the database in order
to mislead attackers who may try to re-identify URLs from the
preﬁxes. The last argument being that these SB providers might
have tampered with their preﬁxes’ database. The presence of
large number of orphans for YANDEX proves that it is possible
to include any arbitrary preﬁx in the blacklists.
C. Presence of Multiple Preﬁxes
The inclusion of multiple preﬁxes for a URL is not
a hypothetical situation. Instead, our experiments with the
databases show that GOOGLE and YANDEX indeed include
multiple preﬁxes for a URL. We employ the Alexa list and the
BigBlackList as test vectors for our experiments. The Alexa
list has been used in our experiments to determine if GOOGLE
or YANDEX indulge in any abusive use of SB.
In case of BigBlackList, we found 103 URLs creating 2
hits in the YANDEX preﬁx lists. Moreover, we found one URL
which creates 3 hits and another one which creates 4 hits. The
results on the Alexa list are particularly interesting. We found
26 URLs on 2 domains that create 2 hits each in the malware
list of GOOGLE. As for the phishing list, we found 1 URL
that creates 2 hits. For YANDEX, we found 1352 such URLs
distributed over 26 domains. 1158 of these URLs create hits
in ydx-malware-shavar while the remaining 194 are hits
in ydx-porno-hosts-top-shavar. We present a subset
of these URLs in Table XI. The large number of such URLs is
essentially due to Type I collisions. Nevertheless, these URLs
are spread over several domains which shows that YANDEX
actively includes several preﬁxes for a URL. This is however
less evident for GOOGLE. We reiterate that the corresponding
domains and in some cases even the URLs are re-identiﬁable.
This allows YANDEX, for instance, to learn the nationality of
a person by the version of xhamster.com he visits. The most
interesting example is that of the domain teenslovehugecocks.
com. The domain may allow YANDEX to identify pedophilic
traits in a user through domain re-identiﬁcation.
356
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:43:08 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IX: Matches found with our datasets.
#matches (%match)
GOOGLE
YANDEX
list name