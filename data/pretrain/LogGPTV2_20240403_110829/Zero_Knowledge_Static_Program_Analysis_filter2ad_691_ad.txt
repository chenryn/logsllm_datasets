and 𝑎𝑟𝑟[𝑡 + 1] as auxiliary input. The circuit then performs the
computation of Step 5-8 in Algorithm 1. This is the combination
of several cases: if the condition in Step 5 is false, the circuit does
nothing and keeps 𝑡 unchanged; otherwise, the circuit compares
𝑎𝑟𝑟[𝑡] and 𝑎𝑟𝑟[𝑡 +1] to the flows of 𝑙′ in the control flow graph. If 𝑙′
has only one flow, we only compare it with 𝑎𝑟𝑟[𝑡] and set 𝑡 = 𝑡 + 1;
otherwise we compare the two flows with 𝑎𝑟𝑟[𝑡], 𝑎𝑟𝑟[𝑡 + 1] and
set 𝑡 = 𝑡 + 2. How to compute the condition of Step 5 and fetch the
flows of 𝑙′ are explained later. In this way, the circuit to deal with
the queue operation for each iteration is of constant size. Finally,
we need to make sure that the auxiliary input 𝑎𝑟𝑟[𝑡], 𝑎𝑟𝑟[𝑡 + 1]
in every iteration is consistent with the entire trace of the queue.
This cannot be done by a linear scan, as 𝑡 increases by 0, 1 or 2 in
different cases. Instead, we view them as memory read at address 𝑡
and 𝑡 + 1 from the trace of the queue and validate their correctness
Figure 2: Our circuit checking the correctness of the abstract interpretation in Algorithm 1.
by memory consistency checks, which we describe later. We will
use the memory checking techniques extensively below.
The second component in each iteration is the instruction fetch.
As shown in Algorithm 1, a flow (𝑙, 𝑙′) is popped from queue 𝑊 in
step 4 and we need to fetch line 𝑙 from the program committed by
the prover in order to determine the transfer function A𝑝,𝑙 and the
IDs of the variables touched by line 𝑙. If we view the program as
a memory indexed by the line number, this instruction fetch is a
classical read operation of a random access memory. Thus we ask
the prover to provide the expected content of line 𝑙 in the program,
and validate its correctness by memory consistency checks.
The third component is updating the states of variables in step 5
and 6. As explained in Section 2.2, 𝑆 = {𝑠𝑙}𝑛 and each 𝑠𝑙 consists of
the states of all variables. Thus we use an 𝑛 × 𝑣 matrix to represent
𝑆, where 𝑣 is the total number of variables in the program. Every
value in the matrix is one of the states in the Lattice val♯, initialized
to ⊥ (also mapped to a particular value in the field). In step 5 of
Algorithm 1, we perform memory read at 𝑙 to obtain the states
of variables used by A𝑝,𝑙; in step 6, if step 5 is true, we perform
memory write to 𝑙′ to update the states of variables in 𝑠𝑙′.
Another operation in step 7 is to extract all lines following 𝑙′ if
the states 𝑠𝑙′ is updated. Thanks to our representation of programs,
one line can have at most two following lines in the intra-procedural
analysis. Thus this operation is a memory read from the control
flow graph represented as an 𝑛 × 2 table. We let the prover provide
the content of line 𝑙′ in the table, and launch another memory
consistency check to ensure its correctness. The flows are then
compared to 𝑎𝑟𝑟[𝑡] and 𝑎𝑟𝑟[𝑡 + 1] as explained above for the push
operation in the queue.
Memory consistency check. We used the memory consistency
check heavily in the design above, and here we described our tech-
niques in details. Memory checking is commonly used in RAM-
based zero-knowledge proof schemes to validate the correctness
of memory accesses by a circuit efficiently. To check 𝑇 memory ac-
cesses, the circuit size is 𝑂(𝑇 polylog(𝑇)), instead of 𝑂(𝑀𝑇) naively
where 𝑀 is the size of the entire memory. Ben-Sasson et al. [14] in-
troduced a memory checking scheme using permutation networks,
which is later refined in [16, 21, 73]. In this paper, we use a mem-
ory checking scheme called offline memory checking, introduced
recently in [61, 62] based on the idea of the earlier work [18].
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2957Offline memory checking. Formally, we view the memory as a vec-
tor of key value pairs (𝑘, v), where v itself can be a vector of values.
Each Read(𝑘) operation fetches the value v associated with key 𝑘,
and each Write(𝑘, v′) changes the value associated with 𝑘 to v′.
The offline memory checking technique [18, 62] reduces the
correctness of memory operations to a set relationship. In particular,
it introduces two timestamps 𝑡 and 𝑡𝑠, a read set 𝑅𝑆 and a write set
𝑊 𝑆. 𝑡𝑠 is the current timestamp, incremented by 1 after every Read
or Write operation. 𝑡 (for a key/address 𝑘) is the timestamp when
the value at 𝑘 is accessed last time. Now the memory becomes a
vector of tuples (𝑘, v, 𝑡). When performing a memory operation
(Read(𝑘) or Write(𝑘, v′)),
• Fetch the tuple (𝑘, v, 𝑡) from the memory and add it to 𝑅𝑆,
• Update the current timestamp as 𝑡𝑠 = 𝑡𝑠 + 1,
• For a Read, add the tuple (𝑘, v, 𝑡𝑠) to 𝑊 𝑆 and update the memory
at 𝑘 as (𝑘, v, 𝑡𝑠); for a Write, add the tuple (𝑘, v′, 𝑡𝑠) to 𝑊 𝑆 and
update the memory at 𝑘 as (𝑘, v′, 𝑡𝑠).
Observe that all the elements in the 𝑅𝑆 and 𝑊 𝑆 are unique as the
timestamps are increasing, and the 𝑅𝑆 is always lacking behind
𝑊 𝑆 by the last state of the memory. Therefore, it is shown that the
memory is correctly executed if and only if Init ∪ 𝑊 𝑆 = 𝑅𝑆 ∪ Final,
where Init and Final denote sets representing the initial state and
the final state of the memory.
Checking memory consistency in circuits. In our zero-knowledge
abstract interpretation scheme, all information regarding the mem-
ory operations are provided by the untrusted prover. Therefore, it
requires additional input and checks to perform the offline memory
checking. Taking the instruction fetch as an example. We view the
instructions in the secret program as a read-only memory. The
secret program was already committed by the prover and it is not
hard to check that the program is well-formed in our programming
language. Therefore, the initial state of the memory is well-defined.
During the execution of the worklist algorithm, the current times-
tamp 𝑡𝑠 is simply the numbering of the current iteration. We add a
counter starting from 0 and increase it by 1 in each iteration, thus
𝑡𝑠 is always correctly computed. As the instructions are read-only,
in every instruction fetch the prover provides the line number 𝑙
and its value v, and we use them both for the read set 𝑅𝑆 and the
write set 𝑊 𝑆. The only missing part is the timestamp 𝑡 when the
instruction was last accessed. We ask the prover to further provide
𝑡 for every read operation as an auxiliary input. In addition, we
check that 𝑡 ≤ 𝑡𝑠 in the circuit. With all of the information above,
each read operation adds (𝑙, v, 𝑡) to 𝑅𝑆 and (𝑙, v, 𝑡𝑠) to 𝑊 𝑆.
Some components in our scheme, such as updating the states
of variables, has both memory read and memory write operations.
Similar to the case above, the states of all variables are initialized to
all 0s, which defines the initial state of the memory correctly. The
timestamp 𝑡𝑠 can again be correctly computed with the iterations
of Algorithm 1. When writing to a memory address with key 𝑘 and
value v′ (computed by the transfer function and the lattice), we add
(𝑘, v′, 𝑡𝑠) to 𝑊 𝑆. However, the algorithm never uses the original
value v. In this case, we ask the prover to provide both v and the
timestamp 𝑡 as auxiliary inputs. The circuit again checks that 𝑡 ≤ 𝑡𝑠
in every memory operation. We show in Appendix C that with this
additional check of 𝑡 ≤ 𝑡𝑠 in both cases ensures the correctness of
the memory operations.
Set relationship. To complete our memory consistency check, we
again rely on the characteristic polynomials of the sets, as in [21,
61, 62]. We first compress the tuple by a random linear combina-
tion H(𝑘, v, 𝑡) = 𝑘 + 𝑟 · 𝑡 +𝑖 𝑟𝑖+2𝑣𝑖, where 𝑟 is randomly chosen
ℎ𝐴(𝑥) =𝑎∈𝐴(H(𝑎) − 𝑥). The circuit computes the characteris-
by the verifier. Then the characteristic polynomial of a set 𝐴 is
tic polynomials ℎ𝑅𝑆(𝑥), ℎ𝑊 𝑆(𝑥), ℎInit(𝑥), ℎFinal(𝑥), and checks that
ℎ𝑊 𝑆(𝛾) · ℎInit(𝛾) = ℎ𝑅𝑆(𝛾) · ℎFinal(𝛾), for a random 𝛾 chosen by
the verifier. This guarantees that Init ∪ 𝑊 𝑆 = 𝑅𝑆 ∪ Final with
overwhelming probability by the Schwarz-Zippel lemma.
The circuit size of our memory checking technique is 𝑂(𝑇 log𝑇)
for 𝑇 memory operations. Note that the schemes in [61, 62] achieve
linear complexity as the verifier knows the memory access pattern.
In our case, all the information are provided by the prover and
validated in the circuit, thus these schemes are not sufficient. Com-
paring to existing memory checking techniques in [14, 16, 21, 73],
though our complexity is asymptotically the same, concretely in
our scheme the circuit only checks 𝑡 ≤ 𝑡𝑠 in each memory write,
while the existing schemes check both the memory addresses and
the timestamps are sorted. Our circuit is smaller in practice, and
our memory checking scheme may be of independent interest.
Lattice operations and transfer functions. Lattice operations
and transfer functions in step 5 and 6 of Algorithm 1 are problem-
dependent. Generally speaking, after fetching the states of variables
from 𝑠𝑙, we implement the circuit to compute the transfer function
A𝑝,𝑙 on the state of each variable, denoted as 𝑠∗
𝑙 = A𝑝,𝑙 (𝑠𝑙). Then
for every variable, we implement the compare operation on 𝑠∗𝑙
and 𝑠′
and outputs 1 if 𝑠∗𝑙 /⊑ 𝑠𝑙′. As the lattice val♯ is a partially
ordered set, the circuit size for the compare function is quadratic
in the number of states in val♯ in the worst case. Finally, if the
compare function outputs 1 for any variable, we implement the
join operation of the lattice 𝑠𝑙′ = 𝑠𝑙′ ⊔ 𝑠∗
in step 6 in the circuit. The
size of the circuit in this step varies a lot for different analyses. In
this paper, we focus on specific ones illustrated below. Compiling
these functions to circuits automatically and efficiently is left as an
interesting future work.
Here we give two examples we use in the experiments later:
tainting analysis and interval analysis. For the tainting analysis, the
lattice is small and finite, consisting of only two values: UnTainted
and Tainted. The transfer function monitors the flow of tainting
information. For statements of assignment such as 𝑎 = 𝑥1 𝑜𝑝 𝑥2, it
sets 𝑎 to Tainted if either 𝑥1 or 𝑥2 is Tainted. Statements of If ...
else and While do not change the state at all. Looking ahead, for
inter-procedure analysis with function calls in Section 4, depending
on the applications some functions are defined as tainting sources,
sanitizers or safe procedures. When seeing these function calls, the
transfer function sets the state of the variable to Tainted, UnTainted
or same as the input respectively. The compare and the join oper-
ations are also very simple. We define UnTainted < Tainted, and
UnTainted∪ Tainted = Tainted in the lattice. Therefore, in tainting
analysis, the transfer function, the compare and the join operation
can be implemented as circuits of constant size.
For the interval analysis, the lattice has an infinite size. It consists
of all the intervals of the form [𝑙, 𝑟], where 𝑙 and 𝑟 are integers in
most cases, and can be ∞ and −∞ as well to denote uncertainty. The
transfer function computes the possible range of variables based
𝑙
𝑙
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2958on the instructions and the input. For example, an instruction 𝑎 = 2
changes the state of 𝑎 to [2, 2]; an instruction 𝑎 = 𝑥1 +𝑥2 with input
states 𝑥1 = [𝑙1, 𝑟1], 𝑥2 = [𝑙2, 𝑟2] set the state of 𝑎 to [𝑙1 + 𝑙2, 𝑟1 + 𝑟2];
an instruction 𝑎 = 𝑥1−𝑥2 with input states 𝑥1 = [𝑙1, 𝑟1], 𝑥2 = [𝑙2, 𝑟2]
set the state of 𝑎 to [𝑙1−𝑟2, 𝑟1−𝑙2]. The compare operation is defined
as the subset relationship of intervals, i.e., [𝑙1, 𝑟1] ⊆ [𝑙2, 𝑟2] if 𝑙1 ≥ 𝑙2
and 𝑟1 ≤ 𝑟2, which is a partial ordering. The join operation returns
the tightest interval that contains the union of the two intervals,
i.e., [𝑙1, 𝑟1] ∪ [𝑙2, 𝑟2] = [min(𝑙1, 𝑙2), max(𝑟1, 𝑟2)]. The size of circuits
for these functions is linear to the bit-length of the integers.
Widening. Our scheme also supports the widening operator ∇ with
a small overhead. We add a counter to each line of code initialized to
0 and increase it by 1 every time the line is analyzed in the worklist
algorithm. When the counter reaches the predefined threshold, the
algorithm forces the early convergence, i.e., setting the bounds to
∞ and −∞ in the interval analysis. Updating the counter for each
line of code also consists of memory accesses, thus it is convenient
to store it together with the states of the variables in an 𝑛 × (𝑣 + 1)
table, and slightly modify the transfer function to include the case
above. This approach only introduces a small overhead on the size
of the entire circuit.
Complexity. Overall, with all the building blocks explained above,
the total size of the circuits for the examples we consider is 𝑂(𝑇 ·
𝑣 + 𝑇 log𝑇), where 𝑇 is the number of iterations of the worklist
algorithm and 𝑣 is the number of variables. The first term 𝑂(𝑇 ·
𝑣) hides the complexity of transfer functions, compare and join
operations depending on different analyses. This is asymptotically
the same as the plain worklist algorithm in Algorithm 1 up to
a logarithmic factor. In practice, we observe that the first term
updating the states of all variables is the dominating part.
3.3 Putting Everything Together
After reducing the correct execution of the worklist algorithm to the
circuit in Figure 2, we then apply the a generic zero knowledge proof
scheme as the backend on the circuit and complete the construction
of our zero-knowledge abstract interpretation scheme. The formal
protocol is described in Protocol 1 in Appendix D. We have the
following theorem:
Theorem 1. Protocol 1 is a zero-knowledge abstract interpretation
scheme by Definition 2.
The theorem follows the correct reduction to the circuit in Fig-
ure 2, the correctness, soundness and zero-knowledge of the back-
end. We give a proof sketch in Appendix D.
Complexity. The overall complexity depends on the backend of
zero knowledge proof scheme. For example, using the pairing-based
SNARK [42], the prover time is 𝑂(𝑇 · 𝑣 log𝑇 + 𝑇 log2 𝑇), the proof
size is 𝑂(1) and the verifier time is 𝑂(1), where 𝑇 is the number of
iterations of the worklist algorithm and 𝑣 is the number of variables;
using the ZKP in [61], both the prover time and the verifier time
are 𝑂(𝑇 · 𝑣 + 𝑇 log𝑇), and the proof size is 𝑂((cid:112)𝑇 · 𝑣 + 𝑇 log𝑇).
4 PROVING INTER-PROCEDURAL ANALYSIS
In this section, we show how to extend our construction of zero-
knowledge abstract interpretation in Section 3 to inter-procedural
analysis for programs with function calls.
fdef ::= fname(𝑥, ..., 𝑥) begin stmt ... stmt end
stmt ::= ... | 𝑥 = fname(𝑥, ..., 𝑥)
Figure 3: Function calls in our programming language.
4.1 Inter-procedural Abstract Interpretation
Inter-procedural abstract interpretation is more demanding because
it takes function definitions and function calls into account. Com-
plications arise when dealing with the mechanism of arguments.
Generally speaking, at the beginning of a function call, the program
switches into a new variable scope, and pass all the arguments from
the caller’s variable scope to the new one. At the end of a function