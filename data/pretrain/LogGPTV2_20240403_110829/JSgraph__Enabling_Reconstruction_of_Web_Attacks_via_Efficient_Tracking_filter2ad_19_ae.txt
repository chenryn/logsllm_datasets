a large set of popular websites. To this end, we leverage
the list of top 1,000 most popular websites according to
Alexa.com. Because it is very time consuming to manually
visit all these websites, we created an automated browsing
process. Speciﬁcally, we implemented a tool that allows us to
automatically visit the top 1,000 websites, and browse on each
one for about two minutes. To roughly mimic the browsing
behavior of a human user, during the two minute time interval,
our system clicks on three randomly selected links, in an
attempt to navigate through different pages on each site. For
this, we leverage xdotool10, and program it to send a random
number of Tab plus Enter keystrokes, to simulate a click on
a random link. To account for variability in the performance
measurement due to random inputs, we visit each website 5
times. Overall, our automated browsing system spent about 167
hours browsing on these top websites. In order to perform this
experiment, we used a machine with 32 CPU cores (AMD
Opteron 6380) and 128 GB of RAM, and 10 QEMU-based
virtual machines running Linux Ubuntu 14.04.
Linux – manual browsing (Linux Top10): With this experiment,
we further explore JSgraph’s performance on ten top US web-
sites. This includes performing searches on Google, watching
videos on Youtube, browsing on Facebook, sending emails in
Gmail, posting tweets on Twitter, browsing on Reddit, etc.
We used JSgraph to manually browse on each of these highly
dynamic websites for about ﬁve minutes, using a Linux-based
Dell Inspiron 15 laptop with a Core-i7 Intel CPU and 8GB of
RAM.
Android - manual browsing (Android Top10): We repeated
the experiment outlined above on an Android-v6.0 Google
Pixel-C tablet with an Nvidia X1 quad-core CPU and 3GB of
RAM. To this end, we compiled an APK version of JSgraph,
and used the adb bridge to collect JSgraph audit logs and
TRACE_EVENT measurements for analysis.
B. Performance Traces
We now provide some details on how we leveraged
Chromium’s TRACE_EVENT instrumentation infrastructure
for proﬁling JSgraph’s performance. We use three types
of trace events: TRACE_EVENT0, TRACE_EVENT_BEGIN0,
and TRACE_EVENT_END0.
at
the
records
function,
placed
spent
TRACE_EVENT0
the
on executing the whole function. We add this at
hooks.
beginning
In
to
didStartProvisionalLoad to monitor the exact time
all
addition, we
TRACE_EVENT_BEGIN0
beginning
the
of
execution
a
time
When
of
JSgraph’s
add
instrumentation
10https://github.com/jordansissel/xdotool
11
hxxp://install.getsportscore.com/?pid=51851&clickid=US149496919611924653623609...Logic Order: 3310Script_664,667-670Other Scripts on the PageLogic Order: 3311,3314-3317PARENT-CHILDScript_666hxxp://code.jquery.com/jquery-1.11.3.min.jsLogic Order: 3313PARENT-CHILDScript_671hxxp://i3j3u3u9.ssl.hwcdn.net/common/scripts/base_new.js?v=1.56Logic Order: 3318PARENT-CHILDScript_672InlineLogic Order: 3319PARENT-CHILDEvent_Callback:mousemovePosition: (832,352)Logic Order: 3374DEFINITIONBuiltin(1224,15)Error_CallbackLogic Order: 3387Builtin(1224,15)Event_Callback:DOMContentLoadedLogic Order: 3337DEFINITIONJ(1,30241)REGISTEREvent_Callback:mouseoverPosition: (832,352)Logic Order: 3373DEFINITION(3,5172)Event_Callback:clickPosition: (830,371)Logic Order: 3375DEFINITION(3,5172)Scheduled_CallbackLogic Order: 3362DEFINITION(498,28)Event_Callback x3:readystatechangeLogic Order: 3364-3366DEFINITION(272,46)Event_Callback x3:readystatechangeLogic Order: 3367-3369DEFINITION(272,46)Event_Callback x3:readystatechangeLogic Order: 3370-3372DEFINITION(272,46)Event_Callback x3:readystatechangeLogic Order: 3384-3386DEFINITION(272,46)Event_Callback x3:readystatechangeLogic Order: 3394-3396DEFINITION(272,46)Get Cookie x16:__lpval=pid=51851&subid=2777&clickid=US1494969196119246536...Logic Order: 3320-3335EXECUTELoad Image:hxxps://www.google-analytics.com/r/collect?v=1&ampLogic Order: 3336EXECUTEGet Cookie x9:__lpval=pid=51851&subid=2777&clickid=US1494969196119246536...Logic Order: 3347-3352,3354-3356EXECUTEXMLHTTP requestURL: hxxp://sendmepixel.com/pixel.aspx?name=getsportscore&type=pageload&...Logic Order: 3353EXECUTELoad Image:hxxps://www.google-analytics.com/collect?v=1&ampLogic Order: 3357EXECUTEXMLHTTP requestURL: hxxp://sendmepixel.com/pixel.aspx?name=getsportscore&type=pageData&...Logic Order: 3358EXECUTEREGISTERREGISTERREGISTERREGISTERREGISTERREGISTERXMLHTTP requestURL: hxxp://sendmepixel.com/pixel.aspx?name=getsportscore&type=gb_detected&...Logic Order: 3363EXECUTEREGISTERGet Cookie x3:__lpval=pid=51851&subid=2777&clickid=US1494969196119246536...Logic Order: 3377-3379EXECUTELoad Image:hxxps://www.google-analytics.com/collect?v=1&ampLogic Order: 3380EXECUTEXMLHTTP requestURL: hxxp://sendmepixel.com/pixel.aspx?name=getsportscore&type=InstallAttempt&...Logic Order: 3381EXECUTEExtension InstallURL: hxxps://chrome.google.com/webstore/detail/fciohdpjmgn...Logic Order: 3382EXECUTEREGISTERREGISTERXMLHTTP requestURL: hxxp://sendmepixel.com/pixel.aspx?name=getsportscore&type=InstallCanceled&...Logic Order: 3388EXECUTEGet Cookie x3:__lpval=pid=51851&subid=2777&clickid=US1494969196119246536...Logic Order: 3389-3391EXECUTELoad Image:hxxps://www.google-analytics.com/collect?v=1&ampLogic Order: 3392EXECUTEREGISTER(a) phishing interface
(b) backward tracking graph (partial)
(c) forward tracking graph
Fig. 12: Analysis of phishing attack with key-logger
CallFunctionEnd
and
when a user navigation is request, and to CallFunctionStart and
RunCompiledScriptStart to monitor the start of each JavaScript
code execution. Furthermore, we add TRACE_EVENT_END0
to
to
record the end of each JavaScript code execution, and
allow us to separately analyze JS execution time from
page/DOM construction and idle times. Also, we inject
TRACE_EVENT_END0 into loadEventFired, to monitor
the ﬁring of page/frame load events.
RunCompiledScriptEnd,
overhead:
Using this instrumentation, we measure four types of
• The page load overhead measures the time spent ex-
ecuting JSgraph’s code between the time the web
page ﬁrst starts loading and when the load event11
is ﬁred for
that same page. The baseline is repre-
sented by the execution time spent by the browser
(excluding the time spent
into JSgraph’s hooks) be-
tween calls to the didStartProvisionalLoad and
loadEventFired instrumentation hooks.
• Similarly, the DOM construction overhead measures the
time spent by JSgraph’s code (and related baseline ex-
ecution time) in between when the ﬁrst DOM node is
inserted in the DOM tree for the page and when the user
triggers the navigation to a new page (excluding the time
spent in JS execution).
• The JS execution overhead is measured by considering
the total time spent by the browser to execute JS code
during a given browsing session. Essentially, we sum up
all time intervals in between RunCompiledScriptStart and
11https://developer.mozilla.org/en-US/docs/Web/Events/load
12
RunCompiledScriptEnd, and between CallFunctionStart
and CallFunctionEnd.
• The overall overhead is measured by considering the en-
tire time spent on a page. For instance, this is often equal
to the time in between when a request to load the page is
made, and when the user triggers the navigation to a new
page. Speciﬁcally, we can measure this time interval by
measuring the time distance between consecutive calls to
the didStartProvisionalLoad hook.
In summary, to compute JSgraph’s overhead relative to the
original Chromium code, we use the following simple formula:
T−O , where o is the relative overhead, O is the absolute
o = O
time spent on JSgraph’s code execution, and T denotes the
time interval between browser events as discussed above (T −
O is the baseline time).
C. Experimental Results
Table I lists the results of the three experiments performed
to measure JSgraph’s overhead described in Section V-A. Each
row indicates the results for one of the three experiments. The
columns correspond to the four types of overhead measure-
ments we described in Section V-B. Each table cell reports
the median and 95-th percentile of the relative overhead, o,
seen during the experiments.
The page load column is particularly signiﬁcant, since
high loading time overhead could frustrate a user and drive
them away from a web page (the relation between page load
time and user satisfaction has been established in previous re-
search [13]). As can be seen from Table I, the 95-th percentile
for the page load overhead is at most 8.2%.
hxxp://192.168.56.101/phpechocms/index.php?module=forum&show=thread&id=4Logic Order: 28USER NAVScript_62hxxp://ATTACK_DOMAIN_1/fake_facebook.jsLogic Order: 37PARENT-CHILDEvent_Callback:keypressKeyCode: 104Logic Order: 54DEFINATION(6,30)REGISTERScheduled_CallbackLogic Order: 55DEFINATION(15,27)REGISTERLoad Image:hxxp://ATTACK_DOMAIN_2/?c=%5B%7B%22t%22%3A-1403065436%7D%5DLogic Order: 56EXECUTEhxxp://192.168.56.101/phpechocms/index.php?module=forum&show=thread&id=4Logic Order: 28Script_48hxxp://192.168.56.101/phpechocms/js/global.jsLogic Order: 29PARENT-CHILDScript_60hxxps://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.jsLogic Order: 34PARENT-CHILDScript_61InlineLogic Order: 35PARENT-CHILDScript_62hxxp://ATTACK_DOMAIN_1/fake_facebook.jsLogic Order: 37PARENT-CHILDScript_63InlineLogic Order: 38PARENT-CHILDScript_64InlineLogic Order: 40PARENT-CHILDEvent_Callback:DOMContentLoadedLogic Order: 42DEFINITIONS(1,31986)REGISTERDocument.writeLogic Order: 36EXECUTEScheduled_Callback x11Logic Order: 43-53DEFINITION(15,27)REGISTEREvent_Callback:keypressKeyCode: 104Logic Order: 54DEFINITION(6,30)REGISTERScheduled_CallbackLogic Order: 55DEFINITION(15,27)REGISTEREvent_Callback:keypressKeyCode: 101Logic Order: 57DEFINITION(6,30)REGISTERScheduled_CallbackLogic Order: 58DEFINITION(15,27)REGISTEREvent_Callback:keypressKeyCode: 108Logic Order: 60DEFINITION(6,30)REGISTEREvent_Callback:keypressKeyCode: 108Logic Order: 61DEFINITION(6,30)REGISTERScheduled_CallbackLogic Order: 62DEFINITION(15,27)REGISTER......Scheduled_Callback x6Logic Order: 123-128DEFINITION(15,27)REGISTERDocument.writeLogic Order: 39EXECUTEDocument.writeLogic Order: 41EXECUTELoad Image:hxxp://ATTACK_DOMAIN_2/?c=%5B%7B%22t%22%3A-1403065436%7D%5DLogic Order: 56EXECUTELoad Image:hxxp://ATTACK_DOMAIN_2/?c=%5B%7B%22t%22%3A-1403065197%7D%5DLogic Order: 59EXECUTELoad Image:hxxp://ATTACK_DOMAIN_2/?c=%5B%7B%22t%22%3A-1403065108%7D%2C%7B...Logic Order: 63EXECUTETABLE I: Performance overhead (50th- and 95th-percentile) percentage overhead
Experiment
Linux Top1K
Linux Top10
Android Top10
Overall
0.5%, 3.1%
1.6%, 3.7%
1.5%, 4.7%
Page load
3.2%, 7.4%
3.3%, 5.7%
3.9%, 8.2%
DOM Construction
0.2%, 1.6%
0.6%, 1.2%
0.4%, 1.7%
JS Execution
6.8%, 20.1%
9.6 %, 17.1%
10.2%, 17.3%
instrumentation hooks are not called by Chromium.
With JSgraph enabled, the browser was able to perform
4143 runs/s12; whereas with JSgraph disabled, the browser
performed 4341 runs/s13. Using the relative overhead deﬁnition
deﬁned in Section V-B, this translates to about 4.6% overhead.
These results show that JSgraph performed approximately as
in the Linux Top10 experiments (on the same device) reported
in the JS Execution column of Table I.
E. Storage Requirements
The storage requirements for JSgraph are limited. In the
experiments reported in Table I, rows 1-2 (Linux-based exper-
iments), we observed that a total of 50 minutes of very active
browsing on 10 highly dynamic, popular websites resulted in
37 MB of compressed audit logs. This means the average disk
space requirement is only about 0.74 MB per minute of active
browsing. Assuming 8 hours of active browsing per work day,
multiplied by 262 workdays per year, gives us less than 84GB
of audit logs per network user per year, or less than 84TB
of storage for 1,000 network users, for one entire year. For
mobile devices, this requirements reduce even further, to 0.34
MB/minute, or less than 42TB of storage for 1,000 network
users for one year. This is likely due to the more limited web
content typically delivered by websites to resource-constrained
mobile devices. Considering the low cost of archival storage,
this represents a sustainable cost for an enterprise network.
VI. DISCUSSION
Our proof-of-concept implementation of JSgraph has some
limitations. For instance, as discussed in Section II, with more
engineering effort we could instrument all Blink/V8 bindings
that have an impact on any aspect of the page. However, we
should notice that our current instrumentations capture all such
bindings that are activated by JS code running on popular
websites. Therefore, adding audit log instrumentation to rarely
used APIs is unlikely to signiﬁcantly affect our overhead
estimates, for example.
We should also point out that while the Chromium code
based tends to evolve fairly rapidly, porting JSgraph to newer
versions of Chromium is possible with reasonable effort. In
fact, a large part of the effort for our research team was
to design the system and identify how to extend the Dev-
Tools instrumentation infrastructure to enable the necessary
ﬁne-grained audit logs without introducing high overhead or
altering the browser’s functionalities. Now that this research
task has been performed, and because the DevTools inspector
instrumentation infrastructure is fairly stable, porting our ef-
forts to newer versions of Chrome mostly involves engineering
time. This also implies that, with adequate engineering effort,
JSgraph updates could be deployed with a timeline comparable
to Chrome browser releases. Furthermore, to facilitate deploy-
ability JSgraph could integrate a way for administrators to
enable/disable logging, or to whitelist highly sensitive websites
that should be excluded from recording.
12Archived results: http://dromaeo.com/?id=268497
13Archived results: http://dromaeo.com/?id=268495
13
(a) Linux Top1K Experiment
(b) Linux Top10 and Android Top10 Experiments
Fig. 13: Overhead and baseline execution time for page loads
Linux Top1K experiment results indicate the median page
load overhead is only about 3.2%. The JS execution time
overhead median value is also low, at 6.8%. Note that the
results for Linux Top10 and Android Top10 experiments are
also very similar, even though those experiments involved very
active browsing by a human user.
The three graphs in Figure 13 provide further insight into
the performance of JSgraph during the page load phase of
all the experiments reported in Table I. The X-axis represents
the number of domains crawled during the experiment, while
the Y-axis represents time in microseconds, in log scale. In all
the graphs, the solid blue curve represents the base execution
time (i.e., T −O) spent by the browser, excluding any JSgraph
overhead. The curve is obtained by plotting the absolute
execution time for each website visit (i.e. each domain will
be represented at multiple points on the X-axis). The instances
are arranged in increasing order of the baseline execution time.
The red marker indicates the overheads introduced by JSgraph.
We can see that in all the 3 graphs the overhead is about one
order of magnitude smaller than the baseline execution time.
D. Dromaeo Performance Benchmark
To further analyze the overhead introduced by JSgraph, we
make use of Dromaeo, a JavaScript performance benchmark
suite from Mozilla (see dromaeo.com). Using a modern laptop
running Ubuntu Linux, we ran the Dromaeo tests two times: (1)
with JSgraph enabled, thus including the overhead discussed
in Section V-B; and (2) with JSgraph disabled, so that our
110100100010000100000Time (ms) - log scaleBaseline timeOverhead10100100010000100000Time (ms) - log scaleLinux Top10Baseline timeOverheadAndroid Top10VII. ADDITIONAL RELATED WORK
Along with the previous works discussed in Section I-C,
there exist other studies that are related to JSgraph from
different aspects, as discussed below.
Graph-based Forensic Analysis. Causal graphs that show the
causality relations between subjects (e.g., process) and objects
(e.g., ﬁle) are widely used in system-level attack analysis [24],
[15], [23], [25], [27]. They record important system events
(e.g., system calls) at runtime and analyze them in a post-
mortem attack analysis. Recently, a series of works [28], [33],
[32] have proposed to provide accurate and ﬁne-grained attack
analysis. They divide long-running processes into multiple
autonomous execution units and identify causal dependencies
between units. A node in their causal graphs represents ﬁne-
grained execution unit instead of a process in the previous
system call based approaches and an edge shows causal
relations between those units. Bates at el. [1] propose a
novel technique for auditing data provenance of web service
components, called Network Provenance Functions (NPFs).