title:Exploiting Temporal Persistence to Detect Covert Botnet Channels
author:Fr&apos;ed&apos;eric Giroire and
Jaideep Chandrashekar and
Nina Taft and
Eve M. Schooler and
Dina Papagiannaki
Exploiting Temporal Persistence to Detect
Covert Botnet Channels
Frederic Giroire1, Jaideep Chandrashekar2, Nina Taft2, Eve Schooler2,
and Dina Papagiannaki2
1 Project Mascotte, I3S(CNRS/UNS)/INRIA
2 Intel Research
Abstract. We describe a method to detect botnet command and con-
trol traﬃc and individual end-hosts. We introduce the notion of “desti-
nation traﬃc atoms” which aggregate the destinations and services that
are communicated with. We then compute the ”persistence”, which is
a measure of temporal regularity and that we propose in this paper,
for individual destination atoms. Very persistent destination atoms are
added to a host’s whitelist during a training period. Subsequently, we
track the persistence of new destination atoms not already whitelisted
in order to identify suspicious C&C destinations. A particularly novel
aspect is that we track persistence at multiple timescales concurrently.
Importantly, our method does not require any a-priori information about
destinations, ports, or protocols used by the C&C communication, nor
do we require payload inspection. We evaluate our system using exten-
sive user traﬃc traces collected from an enterprise network, along with
collected botnet traces.
We demonstrate that our method correctly identiﬁes a botnet’s C&C
traﬃc, even when it is very stealthy. We also show that ﬁltering outgoing
traﬃc with the constructed whitelists dramatically improves the perfor-
mance of traditional anomaly detectors. Finally, we show that the C&C
detection can be achieved with a very low false positive rate.
1 Introduction
A botnet is a collection of compromised end-hosts all under the control of a
particular bot-master (or bot-herder). The recruited end-hosts, also called drones
or zombies, are marshalled and controlled by the bot-herders via a command
and control (in short, C&C) traﬃc channel to carry out a number of malevolent
activities. For example, they are used to launch DDoS attacks, send SPAM,
harvest personal information from zombie hosts, stage social engineering attacks,
and so on. Botnets are so eﬀective at delivering these services that there is an
thriving (underground) economy based around buying or renting botnets [1].
Today’s commercial malware prevention methods, typically host based HIPS and
AV engines, are well suited to identifying and countering previously identiﬁed
and analyzed threats. However, contemporary botnets are extremely adaptable
and able to churn our variants at a very high volume, using polymorphism and
E. Kirda, S. Jha, and D. Balzarotti (Eds.): RAID 2009, LNCS 5758, pp. 326–345, 2009.
c(cid:2) Springer-Verlag Berlin Heidelberg 2009
Exploiting Temporal Persistence to Detect Covert Botnet Channels
327
packing engines, which can easily overwhelm existing defenses (a particular AV
vendor reports collecting 3000 distinct malware samples daily on average [2]).
In contrast to signature scanning based methods, which target known threats,
statistical anomaly detection methods are often employed to detect new threats;
these operate by looking for deviations in traﬃc feature distributions caused
by the malware. These methods can detect and ﬂag zombie hosts that have
been activated and generating a signiﬁcant (noticeable) volume of traﬃc (DDoS
attacks, SPAM, click-fraud, etc). However, it may be a considerable period of
time between a host joining a botnet to the time that is instructed to carry out
a malicious task; often by then it is too late, as the zombie has completed its
purpose. Therefore, even as detecting a botnet in the act of performing some
detrimental activity should be a goal, it is far more critical to block the initial
recruitment vector, or failing that to detect the C&C traﬃc between the drone
and bot-herder, so as to deactivate the channel and render the drone useless.
More critically, information gathered about the C&C infrastructure may be used
to take down the botnet as a whole.
In this paper, we present and validate a method to detect the C&C commu-
nications at an endhost. We were motivated by the observation that a recruited
host needs to be in touch with its C&C server to be ready to carry any particu-
lar activity. It will reconnect, for each new activity, each time it is repurposed,
or resold, and so on. Intuition suggests that such visits will happen with some
regularity; indeed without frequent communication to a C&C server, the bot
becomes invisible to the bot herder. However, this communication is likely to
be very lightweight and spaced out over irregular large time periods. This helps
the botnet be stealthy and hence harder to expose. We thus seek to design a
detector that monitors a user’s outgoing traﬃc in order to expose malicious
destinations that he visits with some temporal regularity, even if infrequently.
In order to discern these from normal destinations a user frequents, we build
whitelists based on a new kind of IP destination address aggregation which we
call destination atoms. A destination atom is an aggregation of destinations that
is intended to capture the service connected to. For example, we view google.com
as a destination service, because a user’s request to google.com will be answered,
over time, by many diﬀerent servers with diﬀerent IP addresses. We build these
destination atoms using a series of heuristics. Then, to capture the nebulous
idea of “lightweight repetition”, we introduce a measure called persistence. Our
whitelists contain destination atoms that exhibit a given level of persistence.
With this whitelist in place, detection proceeds by tracking persistence to con-
tacted (non whitelisted) destinations. When the computed persistence becomes
high enough, the destination is ﬂagged as a potential C&C endpoint. The method
we describe in this paper is particularly well suited to be deployed inside enter-
prise networks. In such networks, a level of administrative control is enforced
over what applications may or may not be installed by end-users, which results
in traﬃc from individual end-hosts being easier to analyze and attribute to par-
ticular applications.
328
F. Giroire et al.
The regularity with which a zombie contacts its bot-herder will diﬀer from
botnet to botnet; moreover, we cannot predict the communication frequency
that will be used in tomorrow’s botnet. We therefore propose to track persis-
tence over multiple timescales simultaneously so as to expose a wide variety of
communication patterns. We develop a simple and practical mechanism to track
persistence over many observation windows at the same time.
There are various styles by which botnets can communicate to command con-
trol centers, including IRC channels, P2P overlays, centralized and decentralized
C&C channels. Our goal here is to try to uncover the C&C activity for the class
of bots that employ a high degree of centralization in their infrastructure and
where the communication channel lasts for an extended period. We do not target
botnets where the communication between zombie and bot-herder is limited to
a few connections, or those where the zombie is programmed to use a completely
new C&C server at each new attempt.
We validate and assess our scheme using two datasets; one consists of (clean)
traces collected directly on enterprise endhosts, and the second consists of traces
of live bot malware. For our method to be practical, it is important that the
created whitelists be stable, i.e., they do not require frequent updating. In addi-
tion, it is essential that whitelists be small so that they require little storage and
can be matched against quickly. Using data traces from a large corpus of enter-
prise users, we will show that the constructed whitelists, composed of destination
atoms and based on their persistence values, exhibit both of these properties. We
then overlay the malware traces on top of the user traces, and run our detectors
over this combined traﬃc trace. We manually extracted the C&C traﬃc from the
botnet malware traces in order to compute false positives and false negatives.
We show that our method identiﬁes the C&C traﬃc in all the bots we tested. Fi-
nally, we also demonstrate that there is a positive side beneﬁt, of identifying the
persistent destination atoms. The sensitivity of HIDS traﬃc anomaly detectors
can be dramatically improved, by ﬁrst ﬁltering the traﬃc through the whitelists.
This allows a larger fraction of our endhosts to catch the attack traﬃc, while
also speeding up the overall detection time.
2 Related Work
There are three potential avenues with which we could mitigate the botnet prob-
lem as described in [3]: preventing the recruitment, detecting the covert C&C
channel, and detecting attacks being launched from the (activated) drones. A
number of previous works has addressed the ﬁrst avenue ([4,5] among others),
and in this paper we chieﬂy address the second avenue (and the third, albeit
indirectly). Our method detects the covert channel end-points by tracking per-
sistence, and we are able to detect attacks by ﬁltering out whitelisted (normally
persistent) traﬃc and subsequently applying well established thresholding meth-
ods, borrowing from the domain of statistical anomaly detection.
In [6] the authors devise a method to detect covert channel communications
carried over IRC with a scoring metric aimed at diﬀerentiating normal IRC chan-
nels from those used by botnets based on counts of protocol ﬂags and common
Exploiting Temporal Persistence to Detect Covert Botnet Channels
329
IRC commands. Our own work diﬀers in that we do not require protocol pay-
loads, nor are we restricted to IRC activity. Another detection approach, BotH-
unter [7], chains together various alarms that correspond to diﬀerent phases of a
host being part of a botnet Our own method does not attempt to identify such
causality and is also able to detect botnet instances for which other identifying
alarms do not exist. Other approaches to detecting botnet traﬃc involve corre-
lating traﬃc patterns to a given destination, across a group of users [8]. Our own
work is complementary, focusing on the individual end-host (and can thus detect
single instances of zombies); we can envision combining the approaches together.
Botminer [9] attacks the detection problem by ﬁrst independently clustering
presumed malicious traﬃc and normal traﬃc and then performing a cross-
correlation across these to identify hosts that undertake both kinds of communi-
cation. These hosts are likely to be part of an organized botnet. Our own work
diﬀers in that it is primarily an end-host based solution, and we do not attempt
to correlate activities across hosts. Also, we do not attempt to identify attack
traﬃc in the traﬃc stream. We focus purely on the nature of communication
between the end-hosts and purported C&C destinations, looking for regularity
in this communication. Orthogonal to the problem of detecting the botnets and
their activities, and following the increasing sophistication being applied in the
design of the more common botnets in operation today, there has been a great
deal of interest in characterizing their structure, organization and operation. A
description of the inner workings, speciﬁcally, the mechanisms used by the Storm
botnet to run SPAM campaigns is described in [10]. The work in [11] examines
the workings of fast-ﬂux service networks, which are becoming more common
today as a way to improve the robustness of botnet C&C infrastructures.
The area of traﬃc anomaly detection is fairly well established, and many de-
tectors have been proposed in the past [12,13]. Some methods build models of
normal behavior for particular protocols and correlate with observed traﬃc. An
indicator of abnormal traﬃc behavior, often found in scanning behaviors, is a
unusual number of connection attempts that fail, as detailed in [12]. Another
interesting idea, described in [13], in which the authors try to identify the par-
ticular ﬂow that caused the infection by analyzing patterns of communications
traﬃc from a set of hosts simultaneously. All of these approaches are comple-
mentary to our work and we allow for any type of traﬃc feature based anomaly
detector to be integrated into the system we describe. Finally, [14] describes a
method to build host proﬁles based on communication patterns of outgoing traf-
ﬁc where the proﬁles are used to detect the spread of worms. This is diﬀerent
from our goal in this paper, to detect botnet C&C activity, which is a stealthier
phenomenon. A more fundamental diﬀerence between our approaches is that we
employ the notion of persistence to incorporate temporal information.
To end this discussion we strongly believe, given the severity of the issue, there is
not a single silver bullet solution that can tackle the botnet threat; a combination
of mechanisms will be needed to eﬀectively mitigate the problem. We view our own
work as complementary to existing approaches that are focused on preventing the
botnet recruitment or else protecting against the infection vector.
330
F. Giroire et al.
3 Methodology
While botnets vary a great deal in organization and operation, a common be-
havior across all of them is that each zombie needs to communicate regularly
with a C&C server to verify its liveness. In order to keep the C&C traﬃc under
the radar, most botnets tend to keep this communication very lightweight, or
stealthy. However because the bot will visit its C&C server repeatedly over time,
failing which the bot-herder might simply assume the zombie to be inactive, we
are motivated to try to expose this low frequency event. To do this, we introduce
a notion called destination atoms (an aggregation of destinations), and a metric
called persistence to capture this “lightweight” yet “regular” communication.
We design a C&C detection method that is based upon tracking the persistence
of destination atoms. In order to diﬀerentiate whether a new destination atom
exhibiting persistence is malicious or benign, we need to develop whitelists of
persistent destinations that the user or his legitimate applications normally visit.
The intuition for our method is as follows: an end-host, on any particular day,
may communicate with a large set of destination end-points. However, most of
these destinations are transient; they are communicated with a few times and
never again. When traﬃc from the host is tracked over longer periods, the set of
destinations visited regularly is a (much) smaller and stable set. Presumably, this
set consists of sites that the user visits often (such as work related, news and en-
tertainment websites), as well as sites contacted by end-host applications (such
as mail servers, blogs, news sites, patch servers, RSS feeds, and so on). If the set
of destinations with high regularity is not very dynamic, then such a set can de-
ﬁne a behavioral signature of the end-host and can be captured in a whitelist that
requires infrequent updating (once learned). We will see in our user study, that
indeed such a whitelist is quite stable. This means that should a new destination
appear, one that is persistent and malicious, the event stands out, enabling detec-
tion. This is precisely what we expect to happen when an end-host is subverted,
recruited into a botnet and begins to communicate with its C&C servers.
In order to keep the whitelists compact and more meaningful, we use a set of
heuristics to aggregate individual destination endpoints into destination atoms,
which are logical destinations or services. For example, the particular addresses
that are mapped to google.com vary by location and time, but this is irrelevant
to the end user who really only cares about the google ”service”. The same
is often true for mail servers, print services, and so on. For our purpose, we
primarily care about the network service being connected to, not so much the
actual destination address.
Given a destination end-point (dstIP, dstPort, proto), we obtain the atom
(dstService, dstPort, proto), by extracting the service from the IP address using
the heuristics described below (Table 1 shows a few mappings from endpoints
to destination atoms):
1. If the source and destination belong to diﬀerent domains, the service name
is simply the second level domain name of the destination (e.g., cisco.com,
yahoo.com)
Exploiting Temporal Persistence to Detect Covert Botnet Channels
331
Table 1. Example destination atoms contacted by somehost.intel.com. Notice that
the intel hosts, being in the same domain, are mapped onto the third level domain,
and the google destinations to the second level domain.
Dest. Name
Destination address
(143.183.10.12, 80, tcp)
(134.231.12.19, 25, tcp)
cw-in-f97.google.com (google.com, 80,tcp)
(216.239.57.97, 80, tcp)
(209.85.137.104, 80, tcp) mg-in-f104.google.com (google.com, 80,tcp)
www.inet.intel.com (inet.intel.com, 80, tcp)
smtp-gw.intel.com (smtp-gw.intel.com, 25, tcp)
Dest. Atom
2. If the source and destination belong to the same domain, then the service
is the third level domain name (e.g., mail.intel.com, print.intel.com). We
diﬀerentiate these situations because we expect a host to communicate with
a larger set of destinations in its own domain, as would be the case in an
enterprise network.
3. When higher level application semantics are available (such as in enter-
prise IT departments), we can use the following type of heuristic. Con-
sider the passive FTP service, which requires two ports on the destination