4
2
0
2
−
3000
4000
2000
4000
interrupts
memory
disk
8000
10000
6000
Tick
l
e
u
a
V
3
2
1
0
3000
4000
0
100
200
300
Ticks into Window
Original
Reconstructed
400
500
Fig. 16: Weights of Spirit’s ﬁrst subsystem, sorted by weight magni-
tude. The compression stage has identiﬁed a phenomenon that affects
many of the components.
Fig. 18: The anomaly signals of the representatives of the ﬁrst three
subsystems for the SQL cluster.
Fig. 17: Sorted weights of Spirit’s third subsystem. Most of the
weight is in a small subset of the components.
Fig. 19: Reconstruction of a portion of Liberty’s admin signal using
the subsystems, including the periodic anomalies.
information (e.g., is the problem more likely to be in Rack
1 or Rack 2?). Our analysis shows that topology is often a
reasonable proxy for behavioral groupings. The representative
signal for the ﬁrst subsystem of many of the systems are
aggregate signals: the aggregate signal summarizing interrupts
in the SQL cluster, the mail-format logs from Mail cluster,
the set of compute nodes in Liberty and Spirit, the components
in Rack D of Thunderbird, and Rack 35 of BG/L. On the other
hand, our experiments also revealed a variety of subsystems
for which the representative signals were not topologically
related. In other words, topological proximity does not imply
correlated behavior nor does correlation imply topological
proximity. For example, based on Figure 14, an administrator
for Stanley would know to think about the laser sensors and
planner software, together, as a subsystem.
A representative signal is also useful for quickly under-
standing what behaviors a subsystem describes. Figure 18
shows the anomaly signals of the representatives of the SQL
cluster’s ﬁrst three subsystems. Based on the representatives,
we can infer that these subsystems correspond to interrupts,
application memory usage, and disk usage, respectively, and
that these subsystems are not strongly correlated.
4) Collective failures: Behavioral subsystems can describe
collective failures. On Thunderbird, there was a known system
message suggesting a CPU problem: “kernel: Losing
some ticks... checking if CPU frequency
changed.” Among the signals generated for Thunderbird
were signals that indicate when individual components output
the message above. It turns out that this problem had nothing
to do with the CPU; in fact, an operating system bug was
causing the kernel to miss interrupts during heavy network
activity. As a result, these messages were typically generated
around the same time on multiple different components. Our
method automatically notices this behavior and places these
indicator signals into a subsystem: all of the ﬁrst several
hundred most strongly-weighted signals in Thunderbird’s third
subsystem were indicator signals for this “CPU” message.
Knowing about this spatial correlation would have allowed
administrators to diagnose the bug more quickly [25].
5) Missing Values and Reconstruction: Our analysis can
deal gracefully with missing data because it explicitly guesses
at the values it will observe during the current tick before
observing them and adjusting the subsystem weights (see
Section III-B). If a value is missing, the guessed value may
be used, instead.
We can also output a reconstruction of the original anomaly
signals using only the information in the subsystems (i.e.,
the weights and the eigensignals), meaning an administrator
can answer historical questions about what the system was
doing around a particular time, without the need to explicitly
archive all the historical anomaly signals (which doesn’t scale).
Figure 19 shows the reconstruction of a portion of Liberty’s
admin anomaly signal. Most of this behavior is captured by
the ﬁrst subsystem, for which admin is representative.
Allowing older values to decay permits faster tracking
of new behavior at the expense of seeing long-term trends.
Figure 20 shows the reconstruction of one of Liberty’s indi-
cator signals, with decay. The improvement in reconstruction
accuracy when using decay is apparent from Figure 21, which
shows the relative reconstruction error for the SQL cluster.
The behavior of this cluster changed near the end of the log
as a result of an upgrade; the analysis with decay adapts to
this change more easily.
D. Delays, Skews, and Cascades
In real systems, interactions may occur with some delay
(e.g., high latency on one node eventually causes trafﬁc to be
rerouted to a second node, which causes higher latency on that
second node a few minutes later) and may involve subsystems.
We call these interactions cascades.
1) Cascades: The logs were rich with instances of individ-
ual signals and behavioral subsystems with lag correlations.
l
e
u
a
V
5
1
0
1
5
0
Original
Reconstructed
l
e
u
a
V
6
4
2
0
2
−
Third Eigensignal
Fourth Eigensignal 
disk
swap