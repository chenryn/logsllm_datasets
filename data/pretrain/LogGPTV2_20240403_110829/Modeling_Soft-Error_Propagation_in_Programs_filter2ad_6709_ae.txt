### Manipulation of Corrupted Bits

As discussed in Section IV-C, we assume that only specific instructions, such as comparisons, logical operators, and casts, have masking effects to simplify our calculations. We also assume that other instructions do not mask corrupted bits. However, this assumption is not always valid, as other instructions can also cause masking. For example, division operations like `fdiv` can average out corrupted bits in the mantissa of floating-point numbers, thereby masking errors.

### Threats to Validity

#### Benchmarks
As mentioned in Section V-A1, we selected 11 programs to cover a wide variety of domains rather than relying on a single benchmark suite. This approach is necessary because there is no standard benchmark suite for reliability evaluation, unlike performance evaluation. Our results may be specific to the chosen benchmarks, although we have not observed significant variations. Other studies in this domain make similar choices [9], [21].

#### Platforms
In this work, we focus on CPU programs for TRIDENT. Graphic Processing Units (GPUs) are another important platform for reliability studies. We attempted to run TRIDENT on GPU programs but were hindered by the lack of automated tools for code analysis and fault injection on GPUs. Preliminary results using small CUDA kernels (instrumented manually) confirm the accuracy of TRIDENT, but more rigorous evaluation is needed.

#### Program Input
Due to the time-consuming nature of high-fidelity fault injection experiments (Section V-C), each program was run with only one input. This is consistent with most other studies in this area [9], [10]. Di Leo et al. [8] found that the SDC probabilities of programs can vary under different inputs. In future work, we plan to consider multiple inputs.

#### Fault Injection Methodology
We use LLFI, a fault injector that operates at the LLVM IR level, to inject single-bit flips. While this method is accurate for estimating SDC probabilities [30], [25], its accuracy for other failure types remains an open question. Given our focus on SDCs, this choice is appropriate.

### Comparison with ePVF and PVF

ePVF (enhanced PVF) is a recent modeling technique for error propagation in programs [9]. It shares the same goal as TRIDENT: predicting the SDC probability of a program, both at the aggregate and instruction levels. ePVF is based on PVF [27], which stands for Program Vulnerability Factor. The main difference is that PVF does not distinguish between crash-causing faults and SDCs, leading to poor SDC prediction accuracy [9]. ePVF improves PVF's accuracy by excluding most crashes from SDC predictions. However, ePVF cannot differentiate between benign faults and SDCs, reducing its overall accuracy [9]. This is because ePVF only models error propagation in static data-dependent instruction sequences and in memory if the sequence ends with a store instruction, ignoring control-flow and other memory parts. Both ePVF and PVF, like TRIDENT, require no fault injection for SDC prediction and can be implemented at the LLVM IR level. We implemented both techniques using LLVM and compared their results with TRIDENT's.

The long running time of fault injection often limits the derivation of program vulnerabilities at finer granularity (i.e., SDC probabilities of individual instructions). The main advantage of modeling techniques is their predictive power and speed, but existing techniques suffer from poor accuracy due to gaps in the models. The key question we address is whether we can combine the advantages of both approaches by constructing a model that is both accurate and scalable.

Since crashes and SDCs are mutually exclusive, ePVF computes a closer result to SDC probability by removing crash-causing faults [9]. However, ePVF's crash propagation model requires a detailed Data Dependency Graph (DDG) of the entire program's execution, which is extremely time-consuming and resource-intensive. As a result, ePVF can only be executed on programs with up to a million dynamic instructions [9]. To address this, we modified ePVF by replacing its crash propagation model with measured results from fault injection, assuming ePVF identifies 100% of crashes accurately. This comparison is conservative as it overestimates ePVF's accuracy.

We used TRIDENT, ePVF, and PVF to compute the SDC probabilities of the same benchmarks and workloads, comparing them with fault injection as the ground truth. The number of randomly sampled faults was 3,000. The results, shown in Figure 9, indicate that ePVF consistently overestimates SDC probabilities with a mean absolute error of 36.78%, while TRIDENT has a mean absolute error of 4.75%. PVF has an even larger mean absolute error of 75.19% because it does not identify crashes. These observations align with those reported by Fang et al. [9]. The average SDC probability measured by fault injection is 13.59%, while ePVF and PVF predict 52.55% and 90.62%, respectively. TRIDENT predicts 14.83% and is significantly more accurate.

### Related Work

There is extensive research on estimating program error resilience through fault injection [7], [11], [13], [14], [20], [30], or by modeling error propagation [9], [10], [27]. Fault injection is simple but has limited predictive power. Shoestring [10] was one of the first papers to model resilience without fault injection. However, Shoestring stops tracing error propagations after control-flow divergence and assumes any fault propagating to a store instruction leads to an SDC. This is similar to removing certain components in our model, which we show is not very accurate. Additionally, Shoestring does not quantify SDC probabilities, unlike TRIDENT.

Gupta et al. [12] investigate the resilience characteristics of different failures in large-scale systems but do not propose automated techniques to predict failure rates. Lu et al. [21] and Li et al. [18] identify vulnerable instructions by characterizing different features, developing efficient heuristics, but their techniques do not quantify error propagation, limiting their accuracy in pinpointing SDC probabilities.

Sridharan et al. [27] introduce PVF, an analytical model that eliminates microarchitectural dependency to approximate SDC probabilities. While fast, PVF has poor accuracy because it does not distinguish between crashes and SDCs. Fang et al. [9] introduce ePVF, which derives tighter bounds on SDC probabilities by omitting crash-causing faults. However, both techniques focus on static data dependency and do not consider error propagation beyond control-flow divergence, leading to large gaps in SDC predictions.

Hari et al. [13], [14] propose a technique to obtain a comprehensive resilience profile without exhaustive fault injections by leveraging execution similarity to identify similar error propagations. While they reduce the number of fault injections, this approach still requires many injections and several hours on a 200-node cluster. TRIDENT offers a significantly faster solution, requiring no fault injections.

### Conclusion

In this paper, we proposed TRIDENT, a three-level model for soft error propagation in programs. TRIDENT abstracts error propagation at the static instruction, control-flow, and memory levels, and does not require fault injection. We implemented TRIDENT in the LLVM compiler and evaluated it on 11 programs. TRIDENT achieves comparable accuracy to fault injection but is much faster and scalable for predicting overall SDC probabilities and SDC probabilities of individual instructions. We also demonstrated that TRIDENT can guide selective instruction duplication techniques and is significantly more accurate than simpler models.

For future work, we plan to extend TRIDENT to consider multiple inputs and platforms other than CPUs, such as GPUs or special-purpose accelerators.

### Acknowledgment

This research was partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery Grants and Strategic Project Grants (SPG) Programmes. We thank the anonymous reviewers of DSN'18 for their insightful comments and suggestions.

### References

[1] IEEE standard for floating-point arithmetic. https://standards.ieee.org/findstds/standard/754-2008.html, 2008. IEEE Std 754-2008.
[2] Hasan Metin Aktulga, Joseph C Fogarty, Sagar A Pandit, and Ananth Y Grama. Parallel reactive molecular dynamics: Numerical methods and algorithmic techniques. Parallel Computing, 38(4):245–259, 2012.
[3] Rizwan A Ashraf, Roberto Gioiosa, Gokcen Kestor, Ronald F DeMara, Chen-Yong Cher, and Pradip Bose. Understanding the propagation of transient errors in HPC applications. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, page 72. ACM, 2015.
[4] Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai Li. The PARSEC benchmark suite: Characterization and architectural implications. In 17th International Conference on Parallel Architectures and Compilation Techniques, pages 72–81. ACM, 2008.
[5] Shuai Che, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy W Sheaffer, Sang-Ha Lee, and Kevin Skadron. Rodinia: A benchmark suite for heterogeneous computing. In International Symposium on Workload Characterization (IISWC 2009), pages 44–54. IEEE, 2009.
[6] Cristian Constantinescu. Intermittent faults and effects on reliability of integrated circuits. In Reliability and Maintainability Symposium, page 370. IEEE, 2008.
[7] Jeffrey J Cook and Craig Zilles. A characterization of instruction-level error derating and its implications for error detection. In International Conference on Dependable Systems and Networks (DSN), pages 482–491. IEEE, 2008.
[8] Domenico Di Leo, Fatemeh Ayatolahi, Behrooz Sangchoolie, Johan Karlsson, and Roger Johansson. On the impact of hardware faults–an investigation of the relationship between workload inputs and failure mode distributions. Computer Safety, Reliability, and Security, pages 198–209, 2012.
[9] Bo Fang, Qining Lu, Karthik Pattabiraman, Matei Ripeanu, and Sudhanva Gurumurthi. ePVF: An enhanced program vulnerability factor methodology for cross-layer resilience analysis. In Proceedings of the International Conference on Dependable Systems and Networks (DSN), pages 168–179. IEEE, 2016.
[10] Shuguang Feng, Shantanu Gupta, Amin Ansari, and Scott Mahlke. Shoestring: Probabilistic soft error reliability on the cheap. In Architectural Support for Programming Languages and Operating Systems, pages 385–396, 2010.
[11] Weining Gu, Zbigniew Kalbarczyk, Ravishankar K Iyer, and Zhenyu Yang. Characterization of Linux kernel behavior under errors. In International Conference on Dependable Systems and Networks (DSN), page 459. IEEE, 2003.
[12] Saurabh Gupta, Tirthak Patel, Christian Engelmann, and Devesh Tiwari. Failures in large scale systems: Long-term measurement, analysis, and implications. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, page 44. ACM, 2017.
[13] Siva Kumar Sastry Hari, Sarita V Adve, Helia Naeimi, and Pradeep Ramachandran. Relyzer: Exploiting application-level fault equivalence to analyze application resiliency to transient faults. In Architectural Support for Programming Languages and Operating Systems, pages 123–134, 2012.
[14] Siva Kumar Sastry Hari, Radha Venkatagiri, Sarita V Adve, and Helia Naeimi. Ganges: Gang error simulation for hardware resiliency evaluation. In ACM/IEEE 41st International Symposium on Computer Architecture (ISCA), pages 61–72. IEEE, 2014.
[15] John L Henning. SPEC CPU2000: Measuring CPU performance in the new millennium. Computer, 33(7):28–35, 2000.
[16] I Karlin. Lulesh programming model and performance ports overview. https://codesign.llnl.gov/pdfs/lulesh_Ports.pdf. [Accessed Apr. 2016].
[17] Chris Lattner and Vikram Adve. LLVM: A compilation framework for lifelong program analysis & transformation. In International Symposium on Code Generation and Optimization, page 75. IEEE, 2004.
[18] Guanpeng Li, Qining Lu, and Karthik Pattabiraman. Fine-grained characterization of faults causing long latency crashes in programs. In IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pages 450–461. IEEE, 2015.
[19] Guanpeng Li and Karthik Pattabiraman. Modeling input-dependent error propagation in programs. In Proceedings of the International Conference on Dependable Systems and Networks (DSN), 2018.
[20] Guanpeng Li, Karthik Pattabiraman, Chen-Yang Cher, and Pradip Bose. Understanding error propagation in GPGPU applications. In International Conference for High Performance Computing, Networking, Storage and Analysis, pages 240–251. IEEE, 2016.
[21] Qining Lu, Guanpeng Li, Karthik Pattabiraman, Meeta S Gupta, and Jude A Rivers. Configurable detection of SDC-causing errors in programs. ACM Transactions on Embedded Computing Systems (TECS), 16(3):88, 2017.
[22] George B Mathews. On the partition of numbers. Proceedings of the London Mathematical Society, 1(1):486–490, 1896.
[23] Nahmsuk Oh, Philip P Shirvani, and Edward J McCluskey. Control-flow checking by software signatures. Transactions on Reliability, 51(1):111–122, 2002.
[24] Vijay Janapa Reddi, Meeta S Gupta, Michael D Smith, Gu-yeon Wei, David Brooks, and Simone Campanoni. Software-assisted hardware reliability: Abstracting circuit-level challenges to the software stack. In Design Automation Conference, pages 788–793. IEEE, 2009.
[25] Behrooz Sangchoolie, Karthik Pattabiraman, and Johan Karlsson. One bit is (not) enough: An empirical study of the impact of single and multiple bit-flip errors. In International Conference on Dependable Systems and Networks (DSN), pages 97–108. IEEE, 2017.
[26] Marc Snir, Robert W Wisniewski, Jacob A Abraham, Sarita V Adve, Saurabh Bagchi, Pavan Balaji, Jim Belak, Pradip Bose, Franck Cappello, Bill Carlson, et al. Addressing failures in exascale computing. Institute for Computing in Science (ICiS). More infor, 4:11, 2012.
[27] Vilas Sridharan and David R Kaeli. Eliminating microarchitectural dependency from architectural vulnerability. In 15th International Symposium on High Performance Computer Architecture.
[28] Student. The probable error of a mean. Biometrika, pages 1–25, 1908.
[29] Ricardo Taborda and Jacobo Bielak. Large-scale earthquake simulation: Computational seismology and complex engineering systems. Computing in Science & Engineering, 13(4):14–27, 2011.
[30] Jiesheng Wei, Anna Thomas, Guanpeng Li, and Karthik Pattabiraman. Quantifying the accuracy of high-level fault injection techniques for hardware faults. In 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pages 375–382. IEEE, 2014.