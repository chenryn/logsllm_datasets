Manipulation of Corrupted Bits: As mentioned in Sec-
tion IV-C, we assume only instructions such as comparisons,
logical operators and casts have masking effects to simplify our
calculations, and that none of the other instructions mask the
corrupted bits. However, this is not always the case as other
instructions may also cause masking. For example, division
operations such as fdiv may also average out corrupted bits in
the mantissa of ﬂoating point numbers, and hence mask errors.
B. Threats to Validity
Benchmarks: As mentioned in Section V-A1, we choose
11 programs to encompass a wide variety of domains rather
than sticking to just one benchmark suite (unlike performance
evaluation, there is no standard benchmark suite for reliability
evaluation). Our results may be speciﬁc to our choice of
benchmarks, though we have not observed this to be the case.
Other work in this domain makes similar decisions [9], [21].
Platforms: In this work, we focus on CPU programs
for TRIDENT. Graphic Processing Units (GPU) are another
important platform for reliability studies. We have attempted
to run TRIDENT on GPU programs, but were crippled by the
lack of automated tools for code analysis and fault injection
on GPUs. Our preliminary results in this domain using small
CUDA kernels (instrumented manually) conﬁrm the accuracy
of TRIDENT. However, more rigorous evaluation is needed.
Program Input: As the high-ﬁdelity fault injection exper-
iments take a long time (Section V-C), we run each program
only under 1 input. This is also the case for almost all other
studies we are aware of in this space
[9], [10]. Di Leo et
at. [8] have found SDC probabilities of programs may change
under different program inputs. We plan to consider multiple
inputs in our future work.
Fault Injection Methodology: We use LLFI, a fault injec-
tor that works at the LLVM IR level, to inject single bit ﬂips.
While this method is accurate for estimating SDC probabilities
of programs [30], [25], it remains an open question as to how
accurate it is for other failure types. That said, our focus in
this paper is SDCs, and so this is an appropriate choice for us.
C. Comparison with ePVF and PVF
ePVF (enhanced PVF) is a recent modeling technique for
error propagation in programs [9]. It shares the same goal with
TRIDENT in predicting the SDC probability of a program,
both at the aggregate level and instruction level. ePVF is
Fig. 9: Overall SDC Probabilities Measured by FI and Predicted by TRIDENT, ePVF and PVF (Margin of Error: ±0.07% to
±1.76% at 95% Conﬁdence)
based on PVF [27], which stands for Program Vulnerability
Factor. The main difference is that PVF does not distinguish
between crash-causing faults and SDCs, and hence its accuracy
of SDC prediction is poor [9]. ePVF improves the accuracy
of PVF by removing most crashes from the SDC prediction.
Unfortunately, ePVF cannot distinguish between benign faults
and SDCs, and hence its accuracy suffers accordingly [9]. This
is because ePVF only models error propagation in static data-
dependent instruction sequence and in memory if the static
data-dependent instruction sequence ends with a store instruc-
tion, ignoring error propagation to control-ﬂow and other parts
of memory. Both ePVF and PVF, like TRIDENT, require no
FI in their prediction of SDC, and can be implemented at the
LLVM IR level3. We implement both techniques using LLVM,
and compare their results with TRIDENT’s results.
its long running time often limits the FI approach from
deriving program vulnerabilities at ﬁner granularity (i.e., SDC
probabilities of individual instructions). The main advantage
of modeling techniques is that they have predictive power
and are signiﬁcantly faster, but existing techniques suffer from
poor accuracy due to important gaps in the models. The main
question we answer in this paper is that whether we can
combine the advantages of the two approaches by constructing
a model that is both accurate and scalable.
Since crashes and SDCs are mutually exclusive, by remov-
ing the crash-causing faults, ePVF computes a relatively closer
result to SDC probability than PVF [9]. However, the crash
propagation model proposed by ePVF in identifying crashes
requires a detailed DDG of the entire program’s execution,
which is extremely time-consuming and resource hungry. As
a result, ePVF can be only executed in programs with a
maximum of a million dynamic instructions in practice [9].
To address this issue and reproduce ePVF on our benchmarks
and workloads (average 109 million dynamic instructions), we
modify ePVF by replacing its crash propagation model with
the measured results from FI. In other words, we assume ePVF
identiﬁes 100% of the crashes accurately, which is higher than
the accuracy of the ePVF model. Hence, this comparison is
conservative as it overestimates the accuracy of ePVF.
We use TRIDENT, ePVF and PVF to compute the SDC
probabilities of the same benchmarks and workloads, and then
compare them with FI which serves as our ground truth. The
number of randomly sampled faults are 3,000. The results are
shown in Figure 9. As shown, ePVF consistently overestimates
the SDC probabilities of the programs with a mean absolute
error of 36.78% whereas it is 4.75% in TRIDENT. PVF results
in an even larger mean absolute error of 75.19% as it does
not identify crashes. The observations are consistent with those
reported by Fang et al. [9]. The average SDC probability
measured by FI is 13.59%. ePVF and PVF predict it as 52.55%
and 90.62% respectively, while TRIDENT predicts it as 14.83%
and is signiﬁcantly more accurate as a result.
VIII. RELATED WORK
There is a signiﬁcant body of work on estimating the
error resilience of a program either through FI [7], [11],
[13], [14], [20], [30], or through modeling error propagation
in programs [9], [10], [27]. The main advantage of FI is
that it is simple, but it has limited predictive power. Further,
3ePVF was originally implemented using LLVM, but not PVF.
Shoestring [10] was one of the ﬁrst papers to attempt to
model the resilience of instructions without using fault injec-
tion. Because Shoestring is not publicly available, we cannot
directly compare it with our TRIDENT. However, Shoestring
stops tracing error propagations after control-ﬂow divergence,
and assumes that any fault that propagates to a store instruction
leads to an SDC. Hence, it is similar to removing fc and fm
in our model and considering only fs , which we show is
not very accurate. Further, Shoestring does not quantify SDC
probabilities of programs and instructions, unlike TRIDENT.
Gupta et al. [12] investigate the resilience characteristics
of different failures in large-scale systems. However, they do
not propose automated techniques to predict failure rates. Lu
et al. [21], Li et al. [18] identify vulnerable instructions by
characterizing different features of instructions in programs.
While they develop efﬁcient heuristics in ﬁnding vulnerable
instructions in programs,
their techniques do not quantify
error propagation, and hence cannot accurately pin-point SDC
probabilities of individual instructions.
Sridharan et al. [27] introduce PVF, an analytical model
which eliminates microarchitectural dependency from archi-
tectural vulnerability to approximate SDC probabilities of
programs. While the model requires no FIs and is hence
fast, it has poor accuracy in determining SDC probabilities
as it does not distinguish between crashes and SDCs. Fang
et al. [9] introduce ePVF, which derives tighter bounds on
SDC probabilities than PVF, by omitting crash-causing faults
from the prediction of SDCs. However, both techniques focus
on modeling the static data dependency of instructions, and do
not consider error propagation beyond control-ﬂow divergence,
which leads to large gaps in the predictions of SDCs (as we
showed in Section VII-C).
Finally, Hari et al. [13], [14] propose a technique to obtain a
comprehensive resilience proﬁle of programs without needing
exhaustive FIs. They prune the FI space by leveraging the
similarity in executions to identify similar error propagations
in programs. While they reduce the number of FIs by orders of
magnitude, this approach still requires many FIs to obtain the
resilience proﬁle, requiring several hours on a 200 node cluster.
TRIDENT offers a signiﬁcantly faster solution, requiring no FIs.
IX. CONCLUSION
In this paper, we proposed TRIDENT, a three-level model
for soft error propagation in programs. TRIDENT abstracts
error propagation at static instruction level, control-ﬂow level
and memory level, and does not need any fault
injection
(FI). We implemented TRIDENT in the LLVM compiler, and
evaluated it on 11 programs. We found that TRIDENT achieves
comparable accuracy as FI, but is much faster and scalable both
for predicting the overall SDC probabilities of programs, and
the SDC probabilities of individual instructions in a program.
We also demonstrated that TRIDENT can be used to guide
selective instruction duplication techniques, and is signiﬁcantly
more accurate than simpler models.
As future work, we plan to extend TRIDENT to consider
(1) Multiple inputs of a program [19], and (2) Platforms other
than CPUs, such as GPUs or special-purpose accelerators.
ACKNOWLEDGEMENT
This research was partially supported by the Natural Sci-
ences and Engineering Research Council of Canada (NSERC)
through the Discovery Grants and Strategic Project Grants
(SPG) Programmes. We thank the anonymous reviewers of
DSN’18 for their insightful comments and suggestions.
REFERENCES
[1]
IEEE standard for ﬂoating-point arithmetic. https://standards.ieee.org/
ﬁndstds/standard/754-2008.html, 2008. IEEE Std 754-2008.
[2] Hasan Metin Aktulga, Joseph C Fogarty, Sagar A Pandit, and Ananth Y
Grama. Parallel reactive molecular dynamics: Numerical methods and
algorithmic techniques. Parallel Computing, 38(4):245–259, 2012.
[3] Rizwan A Ashraf, Roberto Gioiosa, Gokcen Kestor, Ronald F DeMara,
Chen-Yong Cher, and Pradip Bose. Understanding the propagation of
transient errors in hpc applications. In Proceedings of the International
Conference for High Performance Computing, Networking, Storage and
Analysis, page 72. ACM, 2015.
[4] Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai Li. The
parsec benchmark suite: Characterization and architectural implications.
In 17th International Conference on Parallel Architectures and Compi-
lation Techniques, pages 72–81. ACM, 2008.
[5] Shuai Che, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy W
Sheaffer, Sang-Ha Lee, and Kevin Skadron. Rodinia: A benchmark suite
for heterogeneous computing. In International Symposium on Workload
Characterization (IISWC 2009), pages 44–54. IEEE, 2009.
[6] Cristian Constantinescu. Intermittent faults and effects on reliability of
integrated circuits. In Reliability and Maintainability Symposium, page
370. IEEE, 2008.
Jeffrey J Cook and Craig Zilles. A characterization of instruction-level
error derating and its implications for error detection. In International
Conference on Dependable Systems and Networks(DSN), pages 482–
491. IEEE, 2008.
[7]
[8] Domenico Di Leo, Fatemeh Ayatolahi, Behrooz Sangchoolie, Johan
Karlsson, and Roger Johansson. On the impact of hardware faults–an
investigation of the relationship between workload inputs and failure
mode distributions. Computer Safety, Reliability, and Security, pages
198–209, 2012.
[9] Bo Fang, Qining Lu, Karthik Pattabiraman, Matei Ripeanu, and Sud-
hanva Gurumurthi. ePVF: An enhanced program vulnerability factor
methodology for cross-layer resilience analysis. In Proceedings of the
International Conference on Dependable Systems and Networks (DSN),
pages 168–179. IEEE, 2016.
[10] Shuguang Feng, Shantanu Gupta, Amin Ansari, and Scott Mahlke.
Shoestring: Probabilistic soft error reliability on the cheap. In Archi-
tectural Support for Programming Languages and Operating Systems,
pages 385–396, 2010.
[11] Weining Gu, Zbigniew Kalbarczyk, Ravishankar K Iyer, and Zhenyu
Yang. Characterization of linux kernel behavior under errors.
In
International Conference on Dependable Systems and Networks(DSN),
page 459. IEEE, 2003.
[12] Saurabh Gupta, Tirthak Patel, Christian Engelmann, and Devesh Tiwari.
Failures in large scale systems: long-term measurement, analysis, and
implications. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, page 44.
ACM, 2017.
[13] Siva Kumar Sastry Hari, Sarita V Adve, Helia Naeimi, and Pradeep
Ramachandran. Relyzer: Exploiting application-level fault equivalence
In Architectural
to analyze application resiliency to transient faults.
Support for Programming Languages and Operating Systems, pages
123–134, 2012.
[14] Siva Kumar Sastry Hari, Radha Venkatagiri, Sarita V Adve, and
Helia Naeimi. Ganges: Gang error simulation for hardware resiliency
evaluation. In ACM/IEEE 41st International Symposium on Computer
Architecture (ISCA), pages 61–72. IEEE, 2014.
John L Henning. SPEC CPU2000: Measuring CPU performance in the
new millennium. Computer, 33(7):28–35, 2000.
I Karlin. Lulesh programming model and performance ports overview.
https://codesign.llnl.gov/pdfs/lulesh_Ports.pdf. [Accessed Apr. 2016].
[15]
[16]
[17] Chris Lattner and Vikram Adve. LLVM: A compilation framework for
lifelong program analysis & transformation. In International Symposium
on Code Generation and Optimization, page 75. IEEE, 2004.
[18] Guanpeng Li, Qining Lu, and Karthik Pattabiraman.
Fine-grained
characterization of faults causing long latency crashes in programs.
In IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN), pages 450–461. IEEE, 2015.
[19] Guanpeng Li and Karthik Pattabiraman. Modeling input-dependent
In Proceedings of the International
error propagation in programs.
Conference on Dependable Systems and Networks (DSN), 2018.
[20] Guanpeng Li, Karthik Pattabiraman, Chen-Yang Cher, and Pradip Bose.
In Inter-
Understanding error propagation in GPGPU applications.
national Conference for High Performance Computing, Networking,
Storage and Analysis, pages 240–251. IEEE, 2016.
[21] Qining Lu, Guanpeng Li, Karthik Pattabiraman, Meeta S Gupta, and
Jude A Rivers. Conﬁgurable detection of sdc-causing errors in pro-
grams. ACM Transactions on Embedded Computing Systems (TECS),
16(3):88, 2017.
[22] George B Mathews. On the partition of numbers. Proceedings of the
London Mathematical Society, 1(1):486–490, 1896.
[23] Nahmsuk Oh, Philip P Shirvani, and Edward J McCluskey. Control-
ﬂow checking by software signatures. Transactions on Reliability,
51(1):111–122, 2002.
[24] Vijay Janapa Reddi, Meeta S Gupta, Michael D Smith, Gu-yeon Wei,
David Brooks, and Simone Campanoni. Software-assisted hardware
reliability: abstracting circuit-level challenges to the software stack. In
Design Automation Conference, pages 788–793. IEEE, 2009.
[25] Behrooz Sangchoolie, Karthik Pattabiraman, and Johan Karlsson. One
bit is (not) enough: An empirical study of the impact of single and
In International Conference on Dependable
multiple bit-ﬂip errors.
Systems and Networks (DSN), pages 97–108. IEEE, 2017.
[26] Marc Snir, Robert W Wisniewski, Jacob A Abraham, Sarita V Adve,
Saurabh Bagchi, Pavan Balaji, Jim Belak, Pradip Bose, Franck Cap-
pello, Bill Carlson, et al. Addressing failures in exascale computing.
Institute for Computing in Science (ICiS). More infor, 4:11, 2012.
[27] Vilas Sridharan and David R Kaeli. Eliminating microarchitectural
In 15th International
dependency from architectural vulnerability.
Symposium on High Performance Computer Architecture.
[28] Student. The probable error of a mean. Biometrika, pages 1–25, 1908.
[29] Ricardo Taborda and Jacobo Bielak. Large-scale earthquake simulation:
computational seismology and complex engineering systems. Comput-
ing in Science & Engineering, 13(4):14–27, 2011.
Jiesheng Wei, Anna Thomas, Guanpeng Li, and Karthik Pattabiraman.
Quantifying the accuracy of high-level fault injection techniques for
hardware faults. In 44th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN), pages 375–382. IEEE,
2014.
[30]