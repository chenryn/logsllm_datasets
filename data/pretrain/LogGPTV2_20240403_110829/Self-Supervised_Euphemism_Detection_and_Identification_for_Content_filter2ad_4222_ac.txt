keywords. A moderator can use this to ﬁlter content that may
need to be moderated. 2) Euphemism identiﬁcation: Learn the
meaning of euphemisms. This can be used by the moderator to
understand context, and individually review content that uses
euphemisms.
As shown in Figure 1, these two tasks are complementary and
form, together, a content moderation pipeline. The euphemism
detection task takes as input (a) the raw text corpus, and (b) a
list of target keywords (e.g., heroin, marĳuana, ecstasy, etc.).
The expected output is an ordered ranked list of euphemism
candidates, sorted by model conﬁdence. The euphemism
identiﬁcation module takes as input a euphemism (e.g., weed)
and outputs a probability distribution over the target keywords
in the list. For example, if we feed the euphemism weed into
this module, the output should be a probability distribution
over keywords, with most of the mass on marĳuana.
Remark: We use the term “category” to denote a topic (i.e.,
drug, weapon, sexuality). We use “target keyword” to refer to
the speciﬁc keyword in each category users might be trying
to use euphemisms for (e.g., “marĳuana” and “heroin” are
examples of target keywords in the drug category).
IV. Proposed Approach
We next discuss in detail our proposed euphemism detection
approach in Section IV-A and the proposed euphemism
identiﬁcation approach in Section IV-B.
A. Euphemism Detection
We formulate the euphemism detection problem as an unsu-
pervised ﬁll-in-the-mask problem and solve it by combining
self-supervision with a Masked Language Model (MLM), an
important modeling idea behind BERT [15]. Our proposed
approach to euphemism detection has three stages (represented
in Figure 2): 1) Extracting contextual information, 2) Filtering
out uninformative contexts, and 3) Generating euphemism
candidates.
Contextual information extraction. Taking as input all the
target keywords, this stage ﬁrst extracts the masked sentences of
all the keywords. Here, a masked sentence refers to a sentence
excluding the target keyword. Taking the ﬁrst example sentence
in Table III as an example, the corresponding masked sentence
is “This 22 year old former [MASK] addict who i did drugs with
was caught this night.”. A collection of all masked sentences
of the target keywords serves as the source of the relevant and
crucial contextual information.
Denoising contextual information. Not all masked sentences
are equally informative. There may be instances where the mask
token (i.e., “[MASK]”) can be ﬁlled by more than one target
term, or words unrelated to the target terms, without aﬀecting
the quality of the sentence. The fourth example sentence in
Table III is one such case, where the masked sentence “Why is
it so hard to ﬁnd [MASK]?” is not speciﬁc to a drug; the mask
token can be ﬁlled by many words, including nouns such as
“jobs”, “gold” and even pronouns such as “him”. Such masked
sentences (example sentences 4–6 in Table III) are generic and
lack relevant context for disambiguating a polysemous word.
To ﬁlter such generic masked sentences, we propose a
self-supervised approach that makes use of the Masked Lan-
guage Model (MLM) proposed in BERT [15]. Recall that
self-supervision involves creating a new learning task from
unlabeled data. An MLM aims to ﬁnd suitable replacements
of the masked token, and outputs a ranked list of potential
replacement terms. We start by ﬁne-tuning the “bert-base-
uncased” pre-trained model1 to the language of the of domain-
speciﬁc body of text (for instance, a collection of Reddit posts
for identifying drug-related euphemisms).
Empirically, we ﬁnd that if a masked sentence is speciﬁc
to a target category (e.g., drug names), words related to the
target category will be ranked highly in the replacement list. In
contrast, if the masked sentence is generic, the highly ranked
replacements are more likely to be random words unrelated
to the target category (e.g., “jobs”, “gold”, “him”). Therefore,
we set an MLM threshold 𝑡 to ﬁlter out the generic masked
sentences. Considering the ranked list of replacements for
the mask token, if any target keyword appears in the top 𝑡
replacement candidates for the masked sentence, we consider
the masked sentence to be a valid instance of a context.
Otherwise, it is considered to be a generic one and ﬁltered out.
We set the threshold 𝑡 to 𝑡 = 5 in our experiments and discuss
its sensitivity in Section VI-B.
Candidate euphemism generation. Once we have (a) a pre-
trained language model that is ﬁne-tuned to the text corpus
of interest, and (b) a ﬁltered list of masked sentences, we are
ready to generate euphemism candidates. For each masked
sentence 𝑚, and for each word candidate 𝑐 in the vocabulary
(i.e., all words available in the BERT pre-trained model), we
compute its MLM probability (the probability of the word
occurring in 𝑚 as predicted by the language model) ℎ𝑐,𝑚 by
a pre-trained BERT model. Therefore, given a set of masked
sentences, the weight 𝑤𝑐 of a word candidate 𝑐 is calculated
as: 𝑤𝑐 =𝑚(cid:48) ℎ𝑐,𝑚(cid:48). The ﬁnal generation stage simply ranks
all word candidates by their weights.
To clarify, we use the masked language model twice—
once for ﬁltering the masked sentences and a second time
for generating the euphemism candidates from the masked
sentences.
B. Euphemism Identiﬁcation
Once the euphemisms are detected, we aim to identify what
target keyword each euphemism refers to. Taking the second
and third example sentences in Table II, we want to identify that
“ice” refers to “methamphetamine” and “pot” to “marĳuana”.
Euphemism identiﬁcation has been acknowledged as a highly
challenging task [9], due to two problems:
• Resource challenge: No publicly-available, curated datasets
are adequate to exhaustively learn a growing list of
mappings between euphemisms and their target keywords.
Moreover, it is unclear what linguistic and ontological
resources one would need to automate this task.
• Linguistic challenge: The distinction in meaning between
the target keywords (e.g., cocaine and marĳuana) is often
subtle and diﬃcult to learn from raw text corpora alone.
Even human experts are often unable to accurately identify
what a euphemism refers to by looking at a single sentence.
A second linguistic challenge is related to the ambiguity
1https://huggingface.co/transformers/model_doc/bert.html#
bertformaskedlm
Figure 2. An overview of the euphemism detection framework.
Figure 3. An overview of the euphemism identiﬁcation framework.
of the euphemism itself. A given euphemism can be used
in a euphemistic or non-euphemistic sense, adding the
extra layer of linguistic nuance (Table IV).
We tackle the resource challenge by designing a self-
supervised learning scheme. We extract all sentences that
include the target keywords (e.g., cocaine, marĳuana, heroin),
mask the target keywords, and consider the masked sentences
as training samples. This allows us to automatically construct
a labeled dataset, where the input samples are the masked
sentences, and their respective target keywords are labels.
To address the linguistic challenge, we adopt a coarse-to-ﬁne-
grained classiﬁcation scheme. Such hierarchical schemes have
shown better discriminative performance in various tasks [86]–
[88]. The coarse classiﬁer is a binary classiﬁer that outputs
whether a sentence is related to a speciﬁc category (e.g., drug)
or not. It aims to ﬁlter out sentences where the euphemism
candidates do not occur in a euphemistic sense. The ﬁne-grained
classiﬁer is a multi-class classiﬁer trained on the curated dataset
from the self-supervised learning scheme; this aims to learn
a speciﬁc mapping from the masked sentence to the target
keyword. We discuss the details of these classiﬁers below; ﬁrst,
we step through an example of the end-to-end pipeline.
Example: Suppose our euphemism detection pipeline outputs
the term “weed”. We aim to generate a probability distribution
over target keywords, with most of the mass on marĳuana
(Figure 3). Assume that we already have a trained coarse
classiﬁer and a trained ﬁne-grained classiﬁer (training details
will be discussed below in IV-B1). We ﬁrst extract all masked
sentences that previously contained “weed" from the text corpus.
Second, using the coarse classiﬁer, we ﬁlter out the masked
sentences that are unrelated to the target category (i.e., all
masked sentences that do not discuss something drug-related).
Then, we use the ﬁltered masked sentences as inputs to the
ﬁne-grained multi-class classiﬁer, and obtain the target keyword
label for each masked sentence. We now have a list of labels for
the euphemism “weed” (e.g., 36,100 “marĳuana” labels, 4,200
“ecstasy” labels, etc.) and the ﬁnal output for a euphemism is a
probability distribution by the number of labels for each target
keyword.
1) Training Details: As discussed above, two classiﬁers need
to be trained: 1) A coarse classiﬁer to ﬁlter out the masked
sentences of the euphemism words not associated with their
euphemistic sense and, 2) A multi-class classiﬁer to determine
the target keyword to which the euphemism refers.
Coarse Classiﬁer: The coarse classiﬁer is a binary classiﬁer
that decides whether a masked sentence is related to the target
keywords or not. Obtaining positive instances is easy: we collect
all the masked sentences of the target keywords (e.g., we obtain
the masked sentences from Table III). To obtain the negative
instances, we adopt a negative sampling approach [13]; we
randomly choose a sentence in the whole text corpus and
randomly mask a token. Since the corpus is large and diverse,
heroinecstasymarijuanacocaineopium…•vendor	review	fishwithscales35g	___	5g	heroin	4•I	did	order	a	gram	of	___	from	kingopi•best	___	vendors	in	euat	the	moment•I’m	thinking	of	getting	some	___	delivered	to	the	hotel•you	can	pay	for	___	clones	with	bitcoin•……Target KeywordsMasked Sentences•vendor	review	fishwithscales35g	___	5g	heroin	4•I	did	order	a	gram	of	___	from	kingopi•you	can	pay	for	___	clones	with	bitcoin•……Filtered Masked Sentencesweedcokeblueberrybananaspotgold…Euphemism CandidatesExtracting masked sentences from the text corpus (e.g., Reddit)Filtering masked sentences which are not related to the target sensesGenerating candidates by Masked Language Model using BERTweed•iwant	to	buy	a	gram	of	___	but	if	thats7$	will	ihave	enough	to	cover	shipping•___	is	everywhere•inormally	dontcare	for	___	but	it	just	enhanced	the	benzo	and	opiate	high•___	is	fucking	brilliant	full	of	lovely	crystals	and	resin	sticky	fingers	with	a	very	pungent	smell•……EuphemismMasked Sentences•inormally	dontcare	for	___	but	it	just	enhanced	the	benzo	and	opiate	high•___	is	fucking	brilliant	full	of	lovely	crystals	and	resin	sticky	fingers	with	a	very	pungent	smell•……Filtered Masked SentencesExtracting masked sentences from the text corpus (e.g., Reddit)Coarse Classification: Filter the non-target related sentencesFine-Grained Classification00.10.20.30.4Probabilitywe assume the randomly chosen masked sentence is unrelated
to the target keyword. With high probability, this assumption
is correct. To create a balanced dataset, we select as many
negative instances as there are positive ones. This set of positive
and negative instances constitutes the training set, with masked
sentences and their respective labels to indicate whether a
masked sentence is related to the target keywords or not. We
use 70% of the data instances for training, 10% for validation,
and 20% for testing. We select an LSTM recurrent neural
network model [89] with an attention mechanism [90] for its
ability to learn to pay attention to the correct segments of
an input sequence. We obtain 98.8% training accuracy and
90.1% testing accuracy. Our experiments also include other
classiﬁcation models—we discuss our selection in Section
VI-A1.
Multi-class Classiﬁer: As presented above, we use as inputs
the masked sentences and as labels the target keywords.
Empirically, we obtained good performance from a multinomial
logistic regression classiﬁer [91]. We ﬁrst represent each word
as a one-hot vector2. We then represented each sentence as the
average of its member words’ encodings. By using the same
data splitting ratio as the coarse classiﬁer, we obtain a training
accuracy of 55% and a testing accuracy of 24% for the drug
dataset (described in Section V). As a point of comparison,
with 33 target names in the drug dataset a random guess would
yield an accuracy of 3.3%. We discuss the results for other
classiﬁcation models in Section VI-A2.
V. Empirical Evaluation
In this section, we empirically evaluate the performance of
our proposed approach and compare with that a set of baseline
models on both euphemism detection (in Section V-B) and
euphemism identiﬁcation (in Section V-C).
A. Experimental Setup
We implemented all models in Python 3.7 and conducted
all the experiments on a computer with twenty 2.9 GHz Intel
Core i7 CPUs and one GeForce GTX 1080 Ti GPU.
Datasets: We empirically validate our proposed model on three
separate datasets related to three broad areas of euphemism
usage: drugs, weapons, and sexuality. For the algorithm to be
applicable to a dataset, we require two kinds of inputs: 1) the
raw text corpus from which we extract the euphemisms and
their masked sentences, and 2) a list of target keywords (e.g.,
heroin, marĳuana, ecstasy, etc.). For the purpose of carrying
out a quantitative evaluation of the euphemism detection and
identiﬁcation approaches and comparing them with prior art,
we rely on a ground truth list of euphemisms and their target
keywords. Ideally, such a list should contain all euphemisms
for the evaluation of euphemism detection, and a one-to-one
mapping from each euphemism to its actual meaning, for the
evaluation of euphemism identiﬁcation.
2One-hot encoding is used to represent a categorical variable whose values
do not have an ordinal relationship. The one-hot encoding of a word 𝑣𝑖 ∈ 𝑉 ,
where 𝑉 denotes the vocabulary, is a |𝑉 |-dimensional vector of all zeros
except for a 1 at the 𝑖th index.
• Drug dataset: From a publicly available data repository
[92], we extracted 1,271,907 posts from 46 distinct
“subreddits”3 related to drugs and dark web markets,
including the largest ones—“Bitcoin” (565,614 posts),
“Drugs” (373,465 posts), “DarkNetMarkets” (125,300
posts), “SilkRoad” (22,989 posts), “DarkNetMarket-
sNoobs” (22,699 posts). A number of these subreddits
were banned from the platform in early 2018 [93]. As a
result, the posts collected were authored between February
9, 2008 and December 31, 2017. While online drug trade
dates back (at least) to USENET groups in the 1990s, it
truly picked up mainstream traction with the emergence
of the Silk Road in 2011. Our data corpus captures these
early days, as well as the more mature ecosystem that
followed [94].
For ground truth, we use a list of drug names and
corresponding euphemisms compiled by the (USA) Drug
Enforcement Administration [95]. This list is intended
as a practical reference for law enforcement personnel.
Due to the rapidly evolving language used in the drug-use
subculture, it cannot be comprehensive or error-free, but
it is the most reliable ground truth available to us.
• Weapon dataset: The raw text corpus comes from a
combination of the corpora collected by Zanettou et al.
[96], Durrett et al. [6], Portnoﬀ et al. [7] and the examples
in Slangpedia4. The combined corpus has 310,898 posts.
Both the ground truth list of weapon target keywords
and the respective euphemisms are obtained from The
Online Slang Dictionary5 (one of the most comprehensive
slang thesaurus available), Slangpedia, and The Urban
Thesaurus6.
• Sexuality dataset: The raw text corpus comes from the Gab
social networking services7. We use 2,894,869 processed
posts, collected from Jan 2018 to Oct 2018 by PushShift.8
Both the ground truth list of sexuality target keywords
and the euphemisms are obtained from The Online Slang
Dictionary.
B. Euphemism Detection
We evaluate the performance of euphemism detection in this
section.
Evaluation Metric: For each dataset, the input is an unordered
list of target keywords and the output is an ordered ranked
list of euphemism candidates. Given the nature of the output,
we evaluate the output using the metric precision at 𝑘 (𝑃@𝑘),
which is commonly used in information retrieval to evaluate
how well the search results corresponded to a query [97]. 𝑃@𝑘,
ranging from 0 to 1, measures the proportion of the top 𝑘 gen-
erated results that are correct (in our case, valid euphemisms),
which we calculate with respect to the ground truth list for
3Forums hosted on the Reddit website, and associated with a speciﬁc topic.
4https://slangpedia.org/
5http://onlineslangdictionary.com/
6https://urbanthesaurus.org/
7https://gab.com/
8Available at https://ﬁles.pushshift.io/gab/
each dataset. In cases where an algorithm recovers only one
word of a multi-word euphemism (e.g., “Chinese" instead of
“Chinese tobacco"), we treat the candidate as incorrect. Because
of the known shortcoming that 𝑃@𝑘 fails to take into account
the positions of the relevant documents [98], we report 𝑃@𝑘
for multiple values of 𝑘 (𝑘 = 10, 20, 30, 40, 50, 60, 80, 100) to
resolve the issue.
We are unable to measure recall for the following two reasons:
1) Some euphemisms in the ground truth list do not appear in
the text corpus at all and using recall as a measure can result