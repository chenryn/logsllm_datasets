keywords. A moderator can use this to ï¬lter content that may
need to be moderated. 2) Euphemism identiï¬cation: Learn the
meaning of euphemisms. This can be used by the moderator to
understand context, and individually review content that uses
euphemisms.
As shown in Figure 1, these two tasks are complementary and
form, together, a content moderation pipeline. The euphemism
detection task takes as input (a) the raw text corpus, and (b) a
list of target keywords (e.g., heroin, marÄ³uana, ecstasy, etc.).
The expected output is an ordered ranked list of euphemism
candidates, sorted by model conï¬dence. The euphemism
identiï¬cation module takes as input a euphemism (e.g., weed)
and outputs a probability distribution over the target keywords
in the list. For example, if we feed the euphemism weed into
this module, the output should be a probability distribution
over keywords, with most of the mass on marÄ³uana.
Remark: We use the term â€œcategoryâ€ to denote a topic (i.e.,
drug, weapon, sexuality). We use â€œtarget keywordâ€ to refer to
the speciï¬c keyword in each category users might be trying
to use euphemisms for (e.g., â€œmarÄ³uanaâ€ and â€œheroinâ€ are
examples of target keywords in the drug category).
IV. Proposed Approach
We next discuss in detail our proposed euphemism detection
approach in Section IV-A and the proposed euphemism
identiï¬cation approach in Section IV-B.
A. Euphemism Detection
We formulate the euphemism detection problem as an unsu-
pervised ï¬ll-in-the-mask problem and solve it by combining
self-supervision with a Masked Language Model (MLM), an
important modeling idea behind BERT [15]. Our proposed
approach to euphemism detection has three stages (represented
in Figure 2): 1) Extracting contextual information, 2) Filtering
out uninformative contexts, and 3) Generating euphemism
candidates.
Contextual information extraction. Taking as input all the
target keywords, this stage ï¬rst extracts the masked sentences of
all the keywords. Here, a masked sentence refers to a sentence
excluding the target keyword. Taking the ï¬rst example sentence
in Table III as an example, the corresponding masked sentence
is â€œThis 22 year old former [MASK] addict who i did drugs with
was caught this night.â€. A collection of all masked sentences
of the target keywords serves as the source of the relevant and
crucial contextual information.
Denoising contextual information. Not all masked sentences
are equally informative. There may be instances where the mask
token (i.e., â€œ[MASK]â€) can be ï¬lled by more than one target
term, or words unrelated to the target terms, without aï¬€ecting
the quality of the sentence. The fourth example sentence in
Table III is one such case, where the masked sentence â€œWhy is
it so hard to ï¬nd [MASK]?â€ is not speciï¬c to a drug; the mask
token can be ï¬lled by many words, including nouns such as
â€œjobsâ€, â€œgoldâ€ and even pronouns such as â€œhimâ€. Such masked
sentences (example sentences 4â€“6 in Table III) are generic and
lack relevant context for disambiguating a polysemous word.
To ï¬lter such generic masked sentences, we propose a
self-supervised approach that makes use of the Masked Lan-
guage Model (MLM) proposed in BERT [15]. Recall that
self-supervision involves creating a new learning task from
unlabeled data. An MLM aims to ï¬nd suitable replacements
of the masked token, and outputs a ranked list of potential
replacement terms. We start by ï¬ne-tuning the â€œbert-base-
uncasedâ€ pre-trained model1 to the language of the of domain-
speciï¬c body of text (for instance, a collection of Reddit posts
for identifying drug-related euphemisms).
Empirically, we ï¬nd that if a masked sentence is speciï¬c
to a target category (e.g., drug names), words related to the
target category will be ranked highly in the replacement list. In
contrast, if the masked sentence is generic, the highly ranked
replacements are more likely to be random words unrelated
to the target category (e.g., â€œjobsâ€, â€œgoldâ€, â€œhimâ€). Therefore,
we set an MLM threshold ğ‘¡ to ï¬lter out the generic masked
sentences. Considering the ranked list of replacements for
the mask token, if any target keyword appears in the top ğ‘¡
replacement candidates for the masked sentence, we consider
the masked sentence to be a valid instance of a context.
Otherwise, it is considered to be a generic one and ï¬ltered out.
We set the threshold ğ‘¡ to ğ‘¡ = 5 in our experiments and discuss
its sensitivity in Section VI-B.
Candidate euphemism generation. Once we have (a) a pre-
trained language model that is ï¬ne-tuned to the text corpus
of interest, and (b) a ï¬ltered list of masked sentences, we are
ready to generate euphemism candidates. For each masked
sentence ğ‘š, and for each word candidate ğ‘ in the vocabulary
(i.e., all words available in the BERT pre-trained model), we
compute its MLM probability (the probability of the word
occurring in ğ‘š as predicted by the language model) â„ğ‘,ğ‘š by
a pre-trained BERT model. Therefore, given a set of masked
sentences, the weight ğ‘¤ğ‘ of a word candidate ğ‘ is calculated
as: ğ‘¤ğ‘ =ğ‘š(cid:48) â„ğ‘,ğ‘š(cid:48). The ï¬nal generation stage simply ranks
all word candidates by their weights.
To clarify, we use the masked language model twiceâ€”
once for ï¬ltering the masked sentences and a second time
for generating the euphemism candidates from the masked
sentences.
B. Euphemism Identiï¬cation
Once the euphemisms are detected, we aim to identify what
target keyword each euphemism refers to. Taking the second
and third example sentences in Table II, we want to identify that
â€œiceâ€ refers to â€œmethamphetamineâ€ and â€œpotâ€ to â€œmarÄ³uanaâ€.
Euphemism identiï¬cation has been acknowledged as a highly
challenging task [9], due to two problems:
â€¢ Resource challenge: No publicly-available, curated datasets
are adequate to exhaustively learn a growing list of
mappings between euphemisms and their target keywords.
Moreover, it is unclear what linguistic and ontological
resources one would need to automate this task.
â€¢ Linguistic challenge: The distinction in meaning between
the target keywords (e.g., cocaine and marÄ³uana) is often
subtle and diï¬ƒcult to learn from raw text corpora alone.
Even human experts are often unable to accurately identify
what a euphemism refers to by looking at a single sentence.
A second linguistic challenge is related to the ambiguity
1https://huggingface.co/transformers/model_doc/bert.html#
bertformaskedlm
Figure 2. An overview of the euphemism detection framework.
Figure 3. An overview of the euphemism identiï¬cation framework.
of the euphemism itself. A given euphemism can be used
in a euphemistic or non-euphemistic sense, adding the
extra layer of linguistic nuance (Table IV).
We tackle the resource challenge by designing a self-
supervised learning scheme. We extract all sentences that
include the target keywords (e.g., cocaine, marÄ³uana, heroin),
mask the target keywords, and consider the masked sentences
as training samples. This allows us to automatically construct
a labeled dataset, where the input samples are the masked
sentences, and their respective target keywords are labels.
To address the linguistic challenge, we adopt a coarse-to-ï¬ne-
grained classiï¬cation scheme. Such hierarchical schemes have
shown better discriminative performance in various tasks [86]â€“
[88]. The coarse classiï¬er is a binary classiï¬er that outputs
whether a sentence is related to a speciï¬c category (e.g., drug)
or not. It aims to ï¬lter out sentences where the euphemism
candidates do not occur in a euphemistic sense. The ï¬ne-grained
classiï¬er is a multi-class classiï¬er trained on the curated dataset
from the self-supervised learning scheme; this aims to learn
a speciï¬c mapping from the masked sentence to the target
keyword. We discuss the details of these classiï¬ers below; ï¬rst,
we step through an example of the end-to-end pipeline.
Example: Suppose our euphemism detection pipeline outputs
the term â€œweedâ€. We aim to generate a probability distribution
over target keywords, with most of the mass on marÄ³uana
(Figure 3). Assume that we already have a trained coarse
classiï¬er and a trained ï¬ne-grained classiï¬er (training details
will be discussed below in IV-B1). We ï¬rst extract all masked
sentences that previously contained â€œweed" from the text corpus.
Second, using the coarse classiï¬er, we ï¬lter out the masked
sentences that are unrelated to the target category (i.e., all
masked sentences that do not discuss something drug-related).
Then, we use the ï¬ltered masked sentences as inputs to the
ï¬ne-grained multi-class classiï¬er, and obtain the target keyword
label for each masked sentence. We now have a list of labels for
the euphemism â€œweedâ€ (e.g., 36,100 â€œmarÄ³uanaâ€ labels, 4,200
â€œecstasyâ€ labels, etc.) and the ï¬nal output for a euphemism is a
probability distribution by the number of labels for each target
keyword.
1) Training Details: As discussed above, two classiï¬ers need
to be trained: 1) A coarse classiï¬er to ï¬lter out the masked
sentences of the euphemism words not associated with their
euphemistic sense and, 2) A multi-class classiï¬er to determine
the target keyword to which the euphemism refers.
Coarse Classiï¬er: The coarse classiï¬er is a binary classiï¬er
that decides whether a masked sentence is related to the target
keywords or not. Obtaining positive instances is easy: we collect
all the masked sentences of the target keywords (e.g., we obtain
the masked sentences from Table III). To obtain the negative
instances, we adopt a negative sampling approach [13]; we
randomly choose a sentence in the whole text corpus and
randomly mask a token. Since the corpus is large and diverse,
heroinecstasymarijuanacocaineopiumâ€¦â€¢vendor	review	fishwithscales35g	___	5g	heroin	4â€¢I	did	order	a	gram	of	___	from	kingopiâ€¢best	___	vendors	in	euat	the	momentâ€¢Iâ€™m	thinking	of	getting	some	___	delivered	to	the	hotelâ€¢you	can	pay	for	___	clones	with	bitcoinâ€¢â€¦â€¦Target KeywordsMasked Sentencesâ€¢vendor	review	fishwithscales35g	___	5g	heroin	4â€¢I	did	order	a	gram	of	___	from	kingopiâ€¢you	can	pay	for	___	clones	with	bitcoinâ€¢â€¦â€¦Filtered Masked Sentencesweedcokeblueberrybananaspotgoldâ€¦Euphemism CandidatesExtracting masked sentences from the text corpus (e.g., Reddit)Filtering masked sentences which are not related to the target sensesGenerating candidates by Masked Language Model using BERTweedâ€¢iwant	to	buy	a	gram	of	___	but	if	thats7$	will	ihave	enough	to	cover	shippingâ€¢___	is	everywhereâ€¢inormally	dontcare	for	___	but	it	just	enhanced	the	benzo	and	opiate	highâ€¢___	is	fucking	brilliant	full	of	lovely	crystals	and	resin	sticky	fingers	with	a	very	pungent	smellâ€¢â€¦â€¦EuphemismMasked Sentencesâ€¢inormally	dontcare	for	___	but	it	just	enhanced	the	benzo	and	opiate	highâ€¢___	is	fucking	brilliant	full	of	lovely	crystals	and	resin	sticky	fingers	with	a	very	pungent	smellâ€¢â€¦â€¦Filtered Masked SentencesExtracting masked sentences from the text corpus (e.g., Reddit)Coarse Classification: Filter the non-target related sentencesFine-Grained Classification00.10.20.30.4Probabilitywe assume the randomly chosen masked sentence is unrelated
to the target keyword. With high probability, this assumption
is correct. To create a balanced dataset, we select as many
negative instances as there are positive ones. This set of positive
and negative instances constitutes the training set, with masked
sentences and their respective labels to indicate whether a
masked sentence is related to the target keywords or not. We
use 70% of the data instances for training, 10% for validation,
and 20% for testing. We select an LSTM recurrent neural
network model [89] with an attention mechanism [90] for its
ability to learn to pay attention to the correct segments of
an input sequence. We obtain 98.8% training accuracy and
90.1% testing accuracy. Our experiments also include other
classiï¬cation modelsâ€”we discuss our selection in Section
VI-A1.
Multi-class Classiï¬er: As presented above, we use as inputs
the masked sentences and as labels the target keywords.
Empirically, we obtained good performance from a multinomial
logistic regression classiï¬er [91]. We ï¬rst represent each word
as a one-hot vector2. We then represented each sentence as the
average of its member wordsâ€™ encodings. By using the same
data splitting ratio as the coarse classiï¬er, we obtain a training
accuracy of 55% and a testing accuracy of 24% for the drug
dataset (described in Section V). As a point of comparison,
with 33 target names in the drug dataset a random guess would
yield an accuracy of 3.3%. We discuss the results for other
classiï¬cation models in Section VI-A2.
V. Empirical Evaluation
In this section, we empirically evaluate the performance of
our proposed approach and compare with that a set of baseline
models on both euphemism detection (in Section V-B) and
euphemism identiï¬cation (in Section V-C).
A. Experimental Setup
We implemented all models in Python 3.7 and conducted
all the experiments on a computer with twenty 2.9 GHz Intel
Core i7 CPUs and one GeForce GTX 1080 Ti GPU.
Datasets: We empirically validate our proposed model on three
separate datasets related to three broad areas of euphemism
usage: drugs, weapons, and sexuality. For the algorithm to be
applicable to a dataset, we require two kinds of inputs: 1) the
raw text corpus from which we extract the euphemisms and
their masked sentences, and 2) a list of target keywords (e.g.,
heroin, marÄ³uana, ecstasy, etc.). For the purpose of carrying
out a quantitative evaluation of the euphemism detection and
identiï¬cation approaches and comparing them with prior art,
we rely on a ground truth list of euphemisms and their target
keywords. Ideally, such a list should contain all euphemisms
for the evaluation of euphemism detection, and a one-to-one
mapping from each euphemism to its actual meaning, for the
evaluation of euphemism identiï¬cation.
2One-hot encoding is used to represent a categorical variable whose values
do not have an ordinal relationship. The one-hot encoding of a word ğ‘£ğ‘– âˆˆ ğ‘‰ ,
where ğ‘‰ denotes the vocabulary, is a |ğ‘‰ |-dimensional vector of all zeros
except for a 1 at the ğ‘–th index.
â€¢ Drug dataset: From a publicly available data repository
[92], we extracted 1,271,907 posts from 46 distinct
â€œsubredditsâ€3 related to drugs and dark web markets,
including the largest onesâ€”â€œBitcoinâ€ (565,614 posts),
â€œDrugsâ€ (373,465 posts), â€œDarkNetMarketsâ€ (125,300
posts), â€œSilkRoadâ€ (22,989 posts), â€œDarkNetMarket-
sNoobsâ€ (22,699 posts). A number of these subreddits
were banned from the platform in early 2018 [93]. As a
result, the posts collected were authored between February
9, 2008 and December 31, 2017. While online drug trade
dates back (at least) to USENET groups in the 1990s, it
truly picked up mainstream traction with the emergence
of the Silk Road in 2011. Our data corpus captures these
early days, as well as the more mature ecosystem that
followed [94].
For ground truth, we use a list of drug names and
corresponding euphemisms compiled by the (USA) Drug
Enforcement Administration [95]. This list is intended
as a practical reference for law enforcement personnel.
Due to the rapidly evolving language used in the drug-use
subculture, it cannot be comprehensive or error-free, but
it is the most reliable ground truth available to us.
â€¢ Weapon dataset: The raw text corpus comes from a
combination of the corpora collected by Zanettou et al.
[96], Durrett et al. [6], Portnoï¬€ et al. [7] and the examples
in Slangpedia4. The combined corpus has 310,898 posts.
Both the ground truth list of weapon target keywords
and the respective euphemisms are obtained from The
Online Slang Dictionary5 (one of the most comprehensive
slang thesaurus available), Slangpedia, and The Urban
Thesaurus6.
â€¢ Sexuality dataset: The raw text corpus comes from the Gab
social networking services7. We use 2,894,869 processed
posts, collected from Jan 2018 to Oct 2018 by PushShift.8
Both the ground truth list of sexuality target keywords
and the euphemisms are obtained from The Online Slang
Dictionary.
B. Euphemism Detection
We evaluate the performance of euphemism detection in this
section.
Evaluation Metric: For each dataset, the input is an unordered
list of target keywords and the output is an ordered ranked
list of euphemism candidates. Given the nature of the output,
we evaluate the output using the metric precision at ğ‘˜ (ğ‘ƒ@ğ‘˜),
which is commonly used in information retrieval to evaluate
how well the search results corresponded to a query [97]. ğ‘ƒ@ğ‘˜,
ranging from 0 to 1, measures the proportion of the top ğ‘˜ gen-
erated results that are correct (in our case, valid euphemisms),
which we calculate with respect to the ground truth list for
3Forums hosted on the Reddit website, and associated with a speciï¬c topic.
4https://slangpedia.org/
5http://onlineslangdictionary.com/
6https://urbanthesaurus.org/
7https://gab.com/
8Available at https://ï¬les.pushshift.io/gab/
each dataset. In cases where an algorithm recovers only one
word of a multi-word euphemism (e.g., â€œChinese" instead of
â€œChinese tobacco"), we treat the candidate as incorrect. Because
of the known shortcoming that ğ‘ƒ@ğ‘˜ fails to take into account
the positions of the relevant documents [98], we report ğ‘ƒ@ğ‘˜
for multiple values of ğ‘˜ (ğ‘˜ = 10, 20, 30, 40, 50, 60, 80, 100) to
resolve the issue.
We are unable to measure recall for the following two reasons:
1) Some euphemisms in the ground truth list do not appear in
the text corpus at all and using recall as a measure can result