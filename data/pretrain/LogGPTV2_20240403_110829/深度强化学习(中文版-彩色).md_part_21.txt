贝尔曼方程:
对状态价值和动作价值的贝尔曼方程分别为：
v π(s)=E a∼π(·|s),s′∼p(·|s,a)[R(s,a)+γv π(s′ )] (2.81)
q π(s,a)=E s′∼p(·|s,a)[R(s,a)+γE a′∼π(·|s′)[q π(s′ ,a′ )]] (2.82)
贝尔曼最优方程：
对状态价值和动作价值的贝尔曼最优方程分别为：
v∗(s)=maxE s′∼p(·|s,a)[R(s,a)+γv∗(s′ )] (2.83)
a
q∗(s,a)=E s′∼p(·|s,a)[R(s,a)+γmaxq∗(s′ ,a′ )] (2.84)
a′
2.7.2 基于价值的优化
基于价值的优化（Value-BasedOptimization）方法经常需要在（1）基于当前策略的价值函数
估计和（2）基于所估计的价值函数进行策略优化这两个过程之间交替。然而，估计一个复杂的
价值函数并不容易，如图2.17所示。
从之前小节中我们可以看到，Q-Learning可以被用来解决强化学习中一些简单的任务。然而，
现实世界或者即使准现实世界中的应用也都可能有更大和更复杂的状态动作空间，而且实际应用
中很多动作是连续的。比如，在围棋游戏中有约10170 个状态。在这些情况下，Q-Learning中的
传统查找表（LookupTable）方法因为每个状态需要有一条记录（Entry）而每个状态-动作对也需
要一条Q(s,a)记录而使其可扩展性（Scalability）有待提升。实践中，这个表中的值需要一个一
个地更新。所以基于表格（Tabular-Based）的Q-Learning对内存和计算资源的需求可能是巨大的。
此外，在实践中，状态表征（StateRepresentations）通常也需要人为指定成相匹配的数据结构。
价值函数拟合
为了将基于价值的强化学习应用到相对大规模的任务上，函数拟合器（FunctionApproxima-
tors）可用来应对上述限制条件（图2.18）。图2.18总结了不同类型的价值函数拟合器。
• 线性方法（LinearMethods）：拟合函数是权重θ和特征实数向量ϕ(s)=(ϕ (s),ϕ (s)),···,
1 2
ϕ (s))T 的线性组合，其中s是状态。拟合函数表示为v(s,θ) = θTϕ(s)。TD(λ)方法因使
n
用线性函数拟合器而被证明在一定条件下可以收敛(Tsitsiklisetal.,1997)。尽管线性方法的
84
2.7 策略优化
动作或观察量空间
简单且离散 复杂或连续
表格方法 价值函数拟合
线性方法 非线性方法 其他方法
神经网络 决策树、最近邻、
多项式、傅立叶基、
基于核的方法等
粗略编码、地砖编
码、径向基函数等
图2.17 求解价值函数的方法概览
V (s; w) Q (s,a; w) Q (s,a 1; w) ... Q (s,a n; w)
w w w
s s a s
状态价值函数 动作价值函数 动作价值函数
（连续动作） （离散动作）
图2.18 不同的价值函数拟合方式。内含参数w的灰色方框是函数拟合器
收敛性保证很诱人，但实际上在使用该方法时特征选取或特征表示ϕ(s)有一定难度。如下
是线性方法中构建特征的不同方式：
– 多项式（Polynomials）：基本的多项式族（PolynomialFamilies）可以用作函数拟合的特
征矢量（FeatureVectors）。假设每一个状态s=(S ,S ,··· ,S )T是一个d维向量，那
1 2 Qd
么我们有一个d维的多项式基（PolynomialBasis）ϕ (s) = d Sci,j，其中每个c
i j=1 j i,j
是集合{0,1,··· ,N}中的一个整数。这构成秩（Order）为N 的多项式基和(N +1)d
个不同的函数。
85
第2章 强化学习入门
– 傅立叶基（FourierBasis）：傅立叶变换（FourierTransformation）经常用于表示在时间
域或频率域的序列信号。有N +1个函数的一维秩为N 的傅立叶余弦（Cosine）基为
ϕ (s)=cos(iπs)，其中s∈[0,1]且i=0,1,··· ,N。
i
– 粗略编码（CoarseCoding）：状态空间可以从高维缩减到低维，例如用一个区域覆盖决
定过程（DeterminationProcess）来进行二值化表示（BinaryRepresentation），这被称为
粗略编码。
– 瓦式编码（TileCoding）：在粗略编码中，瓦式编码对于多维连续空间是一种高效的特
征表示方式。瓦式编码中特征的感知域（ReceptiveField）被指定成输入空间的不同分
割（Partitions）。每一个分割称为一个瓦面（Tilling），而分割中的每一个元素称为一个
瓦片（Tile）。许多有着重叠感知域的瓦面往往被结合使用，以得到实际的特征矢量。
– 径向基函数（Radial Basis Functions，RBF）：径向基函数自然地泛化了粗略编码，粗
略编码是二值化的，而径向基函数可用于[0,1]内的连续值特征。典型的RBF是以高
exp(−∥s−ci∥2
斯函数（Gaussian）的形式ϕ (s) = )，其中s是状态，c 是特征的原型
i 2σ2 i
i
（Prototypical）或核心状态（CenterState），而σ 是特征宽度（FeatureWidth）。
i
• 非线性方法（Non-LinearMethods）：
– 人工神经网络（ArtificialNeuralNetworks）：不同于以上的函数拟合方法，人工神经网络
被广泛用作非线性函数拟合器，它被证明在一定条件下有普遍的拟合能力（Universal
ApproximationAbility）(Leshnoetal.,1993)。基于深度学习技术，人工神经网络构成了
现代基于函数拟合的深度强化学习方法的主体。一个典型的例子是 DQN算法，使用
人工神经网络来对Q值进行拟合。
• 其他方法：
– 决策树（DecisionTrees）：决策树(Pyeattetal.,2001)可以用来表示状态空间，通过使
用决策节点（DecisionNodes）对其分割。这构成了一种重要的状态特征表示方法。
– 最近邻（NearestNeighbor）方法：它测量了当前状态和内存中之前状态的差异，并用
内存中最接近状态的值来近似当前状态的值。
使用价值函数拟合的好处不仅包括可以扩展到大规模任务，以及便于在连续状态空间中进
行从所见状态到未见过状态的泛化，而且可以减少或缓解人为设计特征来表示状态的需要。对
于无模型方法，拟合器的参数 w 可以用蒙特卡罗（Monte-Carlo，MC）或时间差分（Temporal
Difference，TD）学习来更新，可以对批量样本进行参数更新而非像基于表格的方法一样逐个
更新。这使得处理大规模问题时有较高的计算效率。对基于模型的方法，参数可以用动态规划
（DynamicProgramming，DP）来更新。关于MC、TD和DP的细节在之前已经有所介绍。
可能的函数拟合器包括特征的线性组合、神经网络、决策树和最近邻方法等。神经网络因其
很好的可扩展性和对多样函数的综合能力而成为深度强化学习方法中最实用的拟合方法。神经网
络是一个可微分方法，因而可以基于梯度进行优化，这提供了在凸（Convex）函数情况下收敛到
最优的保证。然而，实践中，它可能需要极大量的数据来训练，而且可能造成其他困难。
86
2.7 策略优化
将深度学习问题扩展到强化学习带来了额外的挑战，包括非独立同分布（NotIndependently
andIdenticallyDistributed）的数据。绝大多数监督学习方法建立在这样一个假设之上，即训练数
据是从一个稳定的独立同分布(Schmidhuber,2015)中采样得到的。然而，强化学习中的训练数据
通常包括高度相关的样本，它们是在智能体和环境交互中顺序得到的，而这违反了监督学习中的
独立性条件。更糟的是，强化学习中的训练数据分布通常是不稳定的，因为价值函数经常根据当
前策略来估计，或者至少受当前策略对状态的访问频率影响，而策略是随训练一直在更新的。智
能体通过对在状态空间探索不同部分来学习。所有这些情况违反了样本数据来自同分布的条件。
在强化学习中使用价值函数拟合对表征方式也有一些实际要求，而如果没有适当地考虑到这
些实际要求，将可能导致发散的情况的发生(Achiametal.,2019)。具体来说，不稳定性和发散带来
的危险在以下三个条件同时发生时就会产生：（1）在一个转移分布（DistributionofTransitions）上
训练，而这个分布不满足由一个过程自然产生且这个过程的期望值被估计（比如在离线学习中）
的条件；（2）可扩展的函数拟合，比如，线性半梯度（Semi-Gradient）；（3）自举（Bootstrapping），
比如DP和TD学习。这三个主要属性只有在它们被结合时会导致学习的发散，而这被称为死亡
三件套（theDeadlyTriad）(VanHasseltetal.,2018)。在使用函数拟合的方式不足够公正的情况下，
基于价值的方法使用函数拟合时可能会有过估计或欠估计（Over-/Under-Estimation）的问题。举
例来说，原始DQN有Q值过估计（Over-Estimation）的问题(VanHasseltetal.,2016)，这在实践
中会导致略差的学习表现，而Double/DuelingDQN技术被提出来缓解这个问题。总体来说，使用
策略梯度的基于策略的方法相比基于价值的方法有更好的收敛性保证。
基于梯度的价值函数拟合
考虑参数化的价值函数Vπ(s)=Vπ(s;w)或Qπ(s,a)=Qπ(s,a;w)，我们可以基于不同的估
计方法得到相应的更新规则。优化目标被设置为估计函数Vπ(s;w)（或Qπ(s,a;w))和真实价值
函数v (s)（或q (s,a)）间的均方误差（Mean-SquaredError，MSE）：
π π
J(w)=E [(Vπ(s;w)−v (s))2] (2.85)
π π
或
J(w)=E [(Qπ(s,a;w)−q (s,a))2] (2.86)
π π
因此，用随机梯度下降（StochasticGradientDescent）法所得到的梯度为
∆w =α(Vπ(s;w)−v (s))∇ Vπ(s;w) (2.87)
π w
87
第2章 强化学习入门
或
∆w =α(Qπ(s,a;w)−q (s,a))∇ Qπ(s,a;w) (2.88)
π w
其中梯度对批中的每一个样本进行计算，而权重以一种随机的方式进行更新。上述等式中的目
标价值函数 v 或 q 通常是被估计的，有时使用一个目标网络（DQN 中）或一个最大化算子
π π
（Q-Learning中）等。我们在这里展示价值函数的一些基本估计方式。
对MC估计，目标值是用采样的回报G 估计的。因此，价值函数参数的更新梯度为
t
∆w =α(Vπ(S ;w )−G )∇ Vπ(S ;w ) (2.89)
t t t t wt t t
或
∆w =α(Qπ(S ,A ;w )−G )∇ Qπ(S ,A ;w ) (2.90)
t t t t t+1 wt t t t
对TD(0)，根据式(2.84)表示的贝尔曼最优方程，目标值是时间差分的目标函数R +γV (S ;w )，
t π t+1 t
因此：
∆w =α(Vπ(S ;w )−(R +γV (S ;w )))∇ Vπ(S ;w ) (2.91)
t t t t π t+1 t wt t t
或
∆w =α(Qπ(S ,A ;w )−(R +γQ (S ,A ;w ))∇ Qπ(S ,A ;w )) (2.92)
t t t t t+1 π t+1 t+1 t wt t t t
对TD(λ)，目标值是λ-回报即Gλ，因此更新规则是
t
∆w =α(Vπ(S ;w )−Gλ )∇ Vπ(S ;w ) (2.93)
t t t t wt t t
或
∆w =α(Qπ(S ,A ;w )−Gλ )∇ Qπ(S ,A ;w ) (2.94)
t t t t t wt t t t
不同的估计方式对偏差和方差有不同的侧重，这在之前的小节中已经有所介绍，比如MC和TD
估计方法等。
88
2.7 策略优化
例子：深度Q网络
深度 Q 网络（DQN）是基于价值优化的典型例子之一。它使用一个深度神经网络来对 Q-
Learning中的Q值函数进行拟合，并维护一个经验回放缓存（ExperienceReplayBuffer）来存储
智能体-环境交互中的转移样本。DQN也使用了一个目标网络QT，而它由原网络Q的参数副本
来参数化，并且以一种延迟更新的方式，来稳定学习过程，也即缓解深度学习中非独立同分布数
据的问题。它使用如式(2.88)中的MSE损失，以及用贪心的拟合函数r+γmax a′QT(s′,a′)替
代真实价值函数q 。
π
经验回放缓存为学习提供了稳定性，因为从缓存中采样到的随机批量样本可以缓解非独立同
分布的数据问题。这使得策略更新成为一种离线的（Off-Policy）方式，由于当前策略和缓存中来
自先前策略的样本间的差异。
2.7.3 基于策略的优化
在开始介绍基于策略的优化（Policy-BasedOptimization）之前，我们首先介绍在强化学习中
常见的一些策略。如之前小节中所介绍，强化学习中的策略可以被分为确定性（Deterministic）和
随机性（Stochastic）策略。在深度强化学习中，我们使用神经网络来表示这两类策略，称为参数
化策略（ParameterizedPolicies）。具体来说，这里的参数化指抽象的策略用神经网络（包括单层
感知机）参数来，而非其他参量来表示。使用神经网络参数θ，确定性和随机性策略可以分别写