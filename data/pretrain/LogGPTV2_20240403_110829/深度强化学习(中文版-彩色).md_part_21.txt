### 贝尔曼方程

#### 状态价值和动作价值的贝尔曼方程
状态价值函数 \( v_\pi(s) \) 和动作价值函数 \( q_\pi(s, a) \) 的贝尔曼方程分别为：
\[
v_\pi(s) = \mathbb{E}_{a \sim \pi(\cdot | s), s' \sim p(\cdot | s, a)} [R(s, a) + \gamma v_\pi(s')] \quad (2.81)
\]
\[
q_\pi(s, a) = \mathbb{E}_{s' \sim p(\cdot | s, a)} [R(s, a) + \gamma \mathbb{E}_{a' \sim \pi(\cdot | s')} [q_\pi(s', a')]] \quad (2.82)
\]

#### 贝尔曼最优方程
状态价值函数 \( v^*(s) \) 和动作价值函数 \( q^*(s, a) \) 的贝尔曼最优方程分别为：
\[
v^*(s) = \max_a \mathbb{E}_{s' \sim p(\cdot | s, a)} [R(s, a) + \gamma v^*(s')] \quad (2.83)
\]
\[
q^*(s, a) = \mathbb{E}_{s' \sim p(\cdot | s, a)} [R(s, a) + \gamma \max_{a'} q^*(s', a')] \quad (2.84)
\]

### 2.7.2 基于价值的优化

基于价值的优化（Value-Based Optimization）方法通常需要在以下两个过程之间交替进行：
1. 基于当前策略的价值函数估计。
2. 基于所估计的价值函数进行策略优化。

然而，估计一个复杂的价值函数并不容易。例如，在围棋游戏中有大约 \( 10^{170} \) 个状态。在这种情况下，传统的Q-Learning查找表方法因为每个状态和状态-动作对都需要一条记录而难以扩展。此外，基于表格的Q-Learning对内存和计算资源的需求可能非常大。实践中，状态表示通常也需要人为指定成相匹配的数据结构。

#### 价值函数拟合

为了将基于价值的强化学习应用到相对大规模的任务上，可以使用函数拟合器来应对上述限制。图2.18总结了不同类型的价值函数拟合器。

##### 线性方法
拟合函数是权重 \( \theta \) 和特征向量 \( \phi(s) \) 的线性组合，其中 \( s \) 是状态。拟合函数表示为 \( v(s, \theta) = \theta^T \phi(s) \)。TD(λ) 方法在使用线性函数拟合器时被证明在一定条件下可以收敛 (Tsitsiklis et al., 1997)。尽管线性方法具有收敛性保证，但在实际使用中特征选取或特征表示 \( \phi(s) \) 有一定难度。以下是构建特征的不同方式：
- **多项式**：基本的多项式族可以用作函数拟合的特征向量。假设每一个状态 \( s = (S_1, S_2, \ldots, S_d)^T \) 是一个 \( d \) 维向量，则我们有一个 \( d \) 维的多项式基 \( \phi_i(s) = \sum_{j=1}^d c_{i,j} S_j \)，其中每个 \( c_{i,j} \) 是集合 {0, 1, ..., N} 中的一个整数。这构成秩为 \( N \) 的多项式基和 \( (N+1)^d \) 个不同的函数。
- **傅立叶基**：一维秩为 \( N \) 的傅立叶余弦基为 \( \phi_i(s) = \cos(i \pi s) \)，其中 \( s \in [0, 1] \) 且 \( i = 0, 1, \ldots, N \)。
- **粗略编码**：状态空间可以从高维缩减到低维，通过二值化表示来进行区域覆盖决定过程。
- **瓦式编码**：对于多维连续空间，瓦式编码是一种高效的特征表示方式。瓦面中的每个元素称为一个瓦片，许多有着重叠感知域的瓦面结合使用以得到实际的特征向量。
- **径向基函数 (RBF)**：典型的RBF是以高斯函数的形式 \( \phi_i(s) = \exp(-\|s - c_i\|^2 / (2\sigma_i^2)) \)，其中 \( s \) 是状态，\( c_i \) 是特征的核心状态，而 \( \sigma_i \) 是特征宽度。

##### 非线性方法
- **人工神经网络**：人工神经网络广泛用作非线性函数拟合器，并被证明在一定条件下具有普遍的拟合能力 (Leshno et al., 1993)。基于深度学习技术，人工神经网络构成了现代基于函数拟合的深度强化学习方法的主体。例如，DQN算法使用人工神经网络来拟合Q值。

##### 其他方法
- **决策树**：决策树可以通过使用决策节点对其分割来表示状态空间。
- **最近邻方法**：它测量当前状态和之前状态的差异，并用最接近状态的值来近似当前状态的值。

使用价值函数拟合的好处包括可以扩展到大规模任务、便于在连续状态空间中进行泛化，以及减少或缓解人为设计特征的需要。对于无模型方法，拟合器的参数 \( w \) 可以用蒙特卡罗 (MC) 或时间差分 (TD) 学习来更新；对于基于模型的方法，参数可以用动态规划 (DP) 来更新。

### 2.7.3 基于梯度的价值函数拟合

考虑参数化的价值函数 \( V_\pi(s; w) \) 或 \( Q_\pi(s, a; w) \)，我们可以基于不同的估计方法得到相应的更新规则。优化目标是估计函数 \( V_\pi(s; w) \)（或 \( Q_\pi(s, a; w) \)）和真实价值函数 \( v_\pi(s) \)（或 \( q_\pi(s, a) \)）间的均方误差 (MSE)：
\[
J(w) = \mathbb{E}[(V_\pi(s; w) - v_\pi(s))^2] \quad (2.85)
\]
或
\[
J(w) = \mathbb{E}[(Q_\pi(s, a; w) - q_\pi(s, a))^2] \quad (2.86)
\]

因此，用随机梯度下降 (SGD) 法所得到的梯度为：
\[
\Delta w = \alpha (V_\pi(s; w) - v_\pi(s)) \nabla_w V_\pi(s; w) \quad (2.87)
\]
或
\[
\Delta w = \alpha (Q_\pi(s, a; w) - q_\pi(s, a)) \nabla_w Q_\pi(s, a; w) \quad (2.88)
\]

其中梯度对批中的每一个样本进行计算，而权重以一种随机的方式进行更新。目标价值函数 \( v_\pi \) 或 \( q_\pi \) 通常是被估计的，有时使用一个目标网络（如DQN中）或一个最大化算子（如Q-Learning中）。我们在这里展示价值函数的一些基本估计方式。

- **蒙特卡罗 (MC) 估计**：目标值是用采样的回报 \( G_t \) 估计的。因此，价值函数参数的更新梯度为：
  \[
  \Delta w = \alpha (V_\pi(S_t; w_t) - G_t) \nabla_w V_\pi(S_t; w_t) \quad (2.89)
  \]
  或
  \[
  \Delta w = \alpha (Q_\pi(S_t, A_t; w_t) - G_t) \nabla_w Q_\pi(S_t, A_t; w_t) \quad (2.90)
  \]

- **TD(0)**：根据贝尔曼最优方程，目标值是时间差分的目标函数 \( R_t + \gamma V_\pi(S_{t+1}; w_t) \)，因此：
  \[
  \Delta w = \alpha (V_\pi(S_t; w_t) - (R_t + \gamma V_\pi(S_{t+1}; w_t))) \nabla_w V_\pi(S_t; w_t) \quad (2.91)
  \]
  或
  \[
  \Delta w = \alpha (Q_\pi(S_t, A_t; w_t) - (R_t + \gamma Q_\pi(S_{t+1}, A_{t+1}; w_t))) \nabla_w Q_\pi(S_t, A_t; w_t) \quad (2.92)
  \]

- **TD(λ)**：目标值是 λ-回报 \( G^\lambda_t \)，因此更新规则是：
  \[
  \Delta w = \alpha (V_\pi(S_t; w_t) - G^\lambda_t) \nabla_w V_\pi(S_t; w_t) \quad (2.93)
  \]
  或
  \[
  \Delta w = \alpha (Q_\pi(S_t, A_t; w_t) - G^\lambda_t) \nabla_w Q_\pi(S_t, A_t; w_t) \quad (2.94)
  \]

不同的估计方式对偏差和方差有不同的侧重，这在之前的小节中已经有所介绍，比如MC和TD估计方法等。

### 深度Q网络 (DQN)

深度Q网络（DQN）是基于价值优化的典型例子之一。它使用一个深度神经网络来拟合Q-Learning中的Q值函数，并维护一个经验回放缓存来存储智能体-环境交互中的转移样本。DQN还使用了一个目标网络 \( Q_T \)，其参数由原网络 \( Q \) 的参数副本延迟更新，以稳定学习过程并缓解深度学习中非独立同分布数据的问题。它使用如式 (2.88) 中的MSE损失，并用贪心的拟合函数 \( r + \gamma \max_{a'} Q_T(s', a') \) 替代真实价值函数 \( q_\pi \)。

经验回放缓存为学习提供了稳定性，因为从缓存中采样到的随机批量样本可以缓解非独立同分布的数据问题。这使得策略更新成为一种离线的方式，由于当前策略和缓存中来自先前策略的样本间的差异。

### 2.7.3 基于策略的优化

在开始介绍基于策略的优化（Policy-Based Optimization）之前，我们首先介绍在强化学习中常见的一些策略。如之前小节中所介绍，强化学习中的策略可以被分为确定性（Deterministic）和随机性（Stochastic）策略。在深度强化学习中，我们使用神经网络来表示这两类策略，称为参数化策略（Parameterized Policies）。具体来说，这里的参数化指抽象的策略用神经网络（包括单层感知机）参数来表示。使用神经网络参数 \( \theta \)，确定性和随机性策略可以分别写为：

- **确定性策略**：\( \pi(a | s; \theta) = \delta(a - \mu(s; \theta)) \)，其中 \( \mu(s; \theta) \) 是神经网络输出的动作。
- **随机性策略**：\( \pi(a | s; \theta) = \mathcal{N}(a | \mu(s; \theta), \sigma(s; \theta)) \)，其中 \( \mu(s; \theta) \) 和 \( \sigma(s; \theta) \) 分别是神经网络输出的均值和标准差。

这些策略可以通过策略梯度方法进行优化，从而实现更高效的策略学习。