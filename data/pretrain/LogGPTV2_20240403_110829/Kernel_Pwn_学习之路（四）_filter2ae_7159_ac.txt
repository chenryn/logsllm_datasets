        .code64
        .org 0x200
    SYM_CODE_START(startup_64)
        /*
         * 64bit entry is 0x200 and it is ABI so immutable!
         * We come here either from startup_32 or directly from a
         * 64bit bootloader.
         * If we come here from a bootloader, kernel(text+data+bss+brk),
         * ramdisk, zero_page, command line could be above 4G.
         * We depend on an identity mapped page table being provided
         * that maps our entire kernel(text+data+bss+brk), zero page
         * and command line.
         */
        /* Setup data segments. */
        xorl    %eax, %eax
        movl    %eax, %ds
        movl    %eax, %es
        movl    %eax, %ss
        movl    %eax, %fs
        movl    %eax, %gs
        /*
         * Compute the decompressed kernel start address.  It is where
         * we were loaded at aligned to a 2M boundary. %rbp contains the
         * decompressed kernel start address.
         *
         * If it is a relocatable kernel then decompress and run the kernel
         * from load address aligned to 2MB addr, otherwise decompress and
         * run the kernel from LOAD_PHYSICAL_ADDR
         *
         * We cannot rely on the calculation done in 32-bit mode, since we
         * may have been invoked via the 64-bit entry point.
         */
        /* Start with the delta to where the kernel will run at. */
    #ifdef CONFIG_RELOCATABLE
        leaq    startup_32(%rip) /* - $startup_32 */, %rbp
        movl    BP_kernel_alignment(%rsi), %eax
        decl    %eax
        addq    %rax, %rbp
        notq    %rax
        andq    %rax, %rbp
        cmpq    $LOAD_PHYSICAL_ADDR, %rbp
        jge    1f
    #endif
        movq    $LOAD_PHYSICAL_ADDR, %rbp
    1:
        /* Target address to relocate to for decompression */
        movl    BP_init_size(%rsi), %ebx
        subl    $_end, %ebx
        addq    %rbp, %rbx
        /* Set up the stack */
        leaq    boot_stack_end(%rbx), %rsp
        /*
         * paging_prepare() and cleanup_trampoline() below can have GOT
         * references. Adjust the table with address we are running at.
         *
         * Zero RAX for adjust_got: the GOT was not adjusted before;
         * there's no adjustment to undo.
         */
        xorq    %rax, %rax
        /*
         * Calculate the address the binary is loaded at and use it as
         * a GOT adjustment.
         */
        call    1f
    1:    popq    %rdi
        subq    $1b, %rdi
        call    .Ladjust_got
        /*
         * At this point we are in long mode with 4-level paging enabled,
         * but we might want to enable 5-level paging or vice versa.
         *
         * The problem is that we cannot do it directly. Setting or clearing
         * CR4.LA57 in long mode would trigger #GP. So we need to switch off
         * long mode and paging first.
         *
         * We also need a trampoline in lower memory to switch over from
         * 4- to 5-level paging for cases when the bootloader puts the kernel
         * above 4G, but didn't enable 5-level paging for us.
         *
         * The same trampoline can be used to switch from 5- to 4-level paging
         * mode, like when starting 4-level paging kernel via kexec() when
         * original kernel worked in 5-level paging mode.
         *
         * For the trampoline, we need the top page table to reside in lower
         * memory as we don't have a way to load 64-bit values into CR3 in
         * 32-bit mode.
         *
         * We go though the trampoline even if we don't have to: if we're
         * already in a desired paging mode. This way the trampoline code gets
         * tested on every boot.
         */
        /* Make sure we have GDT with 32-bit code segment */
        leaq    gdt(%rip), %rax
        movq    %rax, gdt64+2(%rip)
        lgdt    gdt64(%rip)
        /*
         * paging_prepare() sets up the trampoline and checks if we need to
         * enable 5-level paging.
         *
         * paging_prepare() returns a two-quadword structure which lands
         * into RDX:RAX:
         *   - Address of the trampoline is returned in RAX.
         *   - Non zero RDX means trampoline needs to enable 5-level
         *     paging.
         *
         * RSI holds real mode data and needs to be preserved across
         * this function call.
         */
        pushq    %rsi
        movq    %rsi, %rdi        /* real mode address */
        call    paging_prepare
        popq    %rsi
        /* Save the trampoline address in RCX */
        movq    %rax, %rcx
        /*
         * Load the address of trampoline_return() into RDI.
         * It will be used by the trampoline to return to the main code.
         */
        leaq    trampoline_return(%rip), %rdi
        /* Switch to compatibility mode (CS.L = 0 CS.D = 1) via far return */
        pushq    $__KERNEL32_CS
        leaq    TRAMPOLINE_32BIT_CODE_OFFSET(%rax), %rax
        pushq    %rax
        lretq
    trampoline_return:
        /* Restore the stack, the 32-bit trampoline uses its own stack */
        leaq    boot_stack_end(%rbx), %rsp
        /*
         * cleanup_trampoline() would restore trampoline memory.
         *
         * RDI is address of the page table to use instead of page table
         * in trampoline memory (if required).
         *
         * RSI holds real mode data and needs to be preserved across
         * this function call.
         */
        pushq    %rsi
        leaq    top_pgtable(%rbx), %rdi
        call    cleanup_trampoline
        popq    %rsi
        /* Zero EFLAGS */
        pushq    $0
        popfq
        /*
         * Previously we've adjusted the GOT with address the binary was
         * loaded at. Now we need to re-adjust for relocation address.
         *
         * Calculate the address the binary is loaded at, so that we can
         * undo the previous GOT adjustment.
         */
        call    1f
    1:    popq    %rax
        subq    $1b, %rax
        /* The new adjustment is the relocation address */
        movq    %rbx, %rdi
        call    .Ladjust_got
    /*
     * Copy the compressed kernel to the end of our buffer
     * where decompression in place becomes safe.
     */
        pushq    %rsi
        leaq    (_bss-8)(%rip), %rsi
        leaq    (_bss-8)(%rbx), %rdi
        movq    $_bss /* - $startup_32 */, %rcx
        shrq    $3, %rcx
        std
        rep    movsq
        cld
        popq    %rsi
    /*
     * Jump to the relocated address.
     */
        leaq    .Lrelocated(%rbx), %rax
        jmp    *%rax
    SYM_CODE_END(startup_64)
在这里将完成内核解压的准备工作。内核解压的主函数代码位于`/source/arch/x86/boot/compressed/misc.c`中的
`decompress_kernel`函数中，此处不再分析。
内核解压完成以后，程序返回`secondary_startup_64`函数(实现于`/source/arch/x86/kernel/head_64.S`)。在这个函数中，我们开始构建
`identity-mapped pages`，并在那之后检查NX位，配置 `Extended Feature Enable Register`，使用
`lgdt`指令更新早期的`Global Descriptor Table`。
    SYM_CODE_START(secondary_startup_64)
        UNWIND_HINT_EMPTY
        /*
         * At this point the CPU runs in 64bit mode CS.L = 1 CS.D = 0,
         * and someone has loaded a mapped page table.
         *
         * %rsi holds a physical pointer to real_mode_data.
         *
         * We come here either from startup_64 (using physical addresses)
         * or from trampoline.S (using virtual addresses).
         *
         * Using virtual addresses from trampoline.S removes the need
         * to have any identity mapped pages in the kernel page table
         * after the boot processor executes this code.
         */
        /* Sanitize CPU configuration */
        call verify_cpu
        /*
         * Retrieve the modifier (SME encryption mask if SME is active) to be
         * added to the initial pgdir entry that will be programmed into CR3.
         */
        pushq    %rsi
        call    __startup_secondary_64
        popq    %rsi
        /* Form the CR3 value being sure to include the CR3 modifier */
        addq    $(init_top_pgt - __START_KERNEL_map), %rax
    1:
        /* Enable PAE mode, PGE and LA57 */
        movl    $(X86_CR4_PAE | X86_CR4_PGE), %ecx
    #ifdef CONFIG_X86_5LEVEL
        testl    $1, __pgtable_l5_enabled(%rip)
        jz    1f
        orl    $X86_CR4_LA57, %ecx
    1:
    #endif
        movq    %rcx, %cr4
        /* Setup early boot stage 4-/5-level pagetables. */
        addq    phys_base(%rip), %rax
        movq    %rax, %cr3
        /* Ensure I am executing from virtual addresses */
        movq    $1f, %rax
        ANNOTATE_RETPOLINE_SAFE
        jmp    *%rax
    1:
        UNWIND_HINT_EMPTY
        /* Check if nx is implemented */
        movl    $0x80000001, %eax
        cpuid
        movl    %edx,%edi
        /* Setup EFER (Extended Feature Enable Register) */
        movl    $MSR_EFER, %ecx
        rdmsr
        btsl    $_EFER_SCE, %eax    /* Enable System Call */
        btl    $20,%edi        /* No Execute supported? */
        jnc     1f
        btsl    $_EFER_NX, %eax
        btsq    $_PAGE_BIT_NX,early_pmd_flags(%rip)
    1:    wrmsr                /* Make changes effective */
        /* Setup cr0 */
        movl    $CR0_STATE, %eax
        /* Make changes effective */