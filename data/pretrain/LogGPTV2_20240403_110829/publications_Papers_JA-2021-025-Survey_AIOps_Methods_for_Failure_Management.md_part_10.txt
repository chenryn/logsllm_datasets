### Introduction
The majority of existing works in the field of failure management utilize only one or a few types of data. In contrast, multimodal approaches, which can integrate various data sources, may prove to be more effective and robust when dealing with new observations, thanks to the increased system visibility they provide.

### Survey of AIOps Methods for Failure Management
In this survey, we analyzed 100 contributions across different categories, data sources, and target components. The following table summarizes the papers grouped by their employed data sources, targets, and categories.

#### Table 8: Papers Analyzed in the Survey Grouped by Employed Data Sources, Targets, and Categories

| **Paper(s)** | **Category** | **Data Sources** | **Targets** |
|--------------|--------------|------------------|-------------|
| [35,38,52,94,102,117,153] | Software Defect Prediction | System Logs, Metrics | Software |
| [42,50,76,98,103,115,141] | Software Defect Prediction | System Logs, Metrics | Software |
| [105,128] | Fault Injection | System Logs, Metrics, Network | Software |
| [4,49,136] | Software Rejuvenation | System Logs, Metrics, Network | Software |
| [21,137] | Software Rejuvenation | System Logs, Metrics, Network, Hardware | Software |
| [62,95,114] | Checkpointing | System Logs, Metrics | Software |
| [77,78,89,104,138] | Hardware Failure Prediction | System Logs, Metrics | Hardware |
| [54,88,143,165,167] | Hardware Failure Prediction | System Logs, Metrics | Hardware |
| [101,148,149,164] | Hardware Failure Prediction | System Logs, Metrics | Hardware |
| [37] | Hardware Failure Prediction | System Logs, Metrics, Network | Hardware |
| [33] | Hardware Failure Prediction | System Logs, Metrics, Network | Hardware |
| [161] | Hardware Failure Prediction | System Logs, Metrics, Network | Hardware |
| [23] | System Failure Prediction | System Logs, Metrics | System |
| [31] | System Failure Prediction | System Logs, Metrics | System |
| [61] | System Failure Prediction | System Logs, Metrics, Network, Hardware | System |
| [119] | System Failure Prediction | System Logs, Metrics, Network, Hardware | System |
| [81] | System Failure Prediction | System Logs, Metrics, Network | System |
| [123] | System Failure Prediction | System Logs, Metrics, Network, Hardware, Application | System |
| [44,160] | System Failure Prediction | System Logs, Metrics | System |
| [151] | Anomaly Detection | System Logs, Metrics | System |
| [7,131,159] | Anomaly Detection | System Logs, Metrics, Network | System |
| [86,150] | Anomaly Detection | System Logs, Metrics | System |
| [127] | Anomaly Detection | System Logs, Metrics, Network, Hardware | System |
| [72] | Anomaly Detection | System Logs, Metrics, Network | System |
| [73] | Anomaly Detection | System Logs, Metrics, Network, Hardware | System |
| [12,18,41,45,92,162] | Anomaly Detection | System Logs, Metrics | System |
| [29] | Anomaly Detection | System Logs, Metrics, Network, Hardware | System |
| [11] | Anomaly Detection | System Logs, Metrics, Network | System |
| [26,87] | Anomaly Detection | System Logs, Metrics | System |
| [8,43,97,142] | Internet Traffic Classification | System Logs, Metrics, Network | Network |
| [163,168] | Log Enhancement | System Logs | System |
| [1,30,85,121,145,156] | Fault Localization | System Logs, Metrics, Network, Hardware | Software, Hardware |
| [110] | Fault Localization | System Logs, Metrics, Network, Hardware | Software, Hardware |
| [80,132] | Fault Localization | System Logs, Metrics | Software, Hardware |
| [83] | Fault Localization | System Logs, Metrics | Software, Hardware |
| [9] | Fault Localization | System Logs, Metrics, Network, Hardware | Software, Hardware |
| [154] | Root-cause Diagnosis | System Logs, Metrics | System |
| [6] | Root-cause Diagnosis | System Logs, Metrics, Network, Hardware | System |
| [64] | Root-cause Diagnosis | System Logs, Metrics | System |
| [25] | Root-cause Diagnosis | System Logs, Metrics | System |
| [120] | RCA-Others | System Logs, Metrics | System |
| [14,32] | RCA-Others | System Logs, Metrics, Network, Hardware | System |
| [84] | RCA-Others | System Logs, Metrics | System |
| [3] | RCA-Others | System Logs, Metrics | System |
| [126,158] | Incident Triage | System Logs, Metrics | System |
| [140,166] | Solution Recommendation | System Logs, Metrics | System |
| [82] | Solution Recommendation | System Logs, Metrics, Network, Hardware | System |
| [124] | Recovery | System Logs, Metrics | System |

### Observations and Future Directions
We observed that some areas of failure management have received less scientific interest compared to others. For example, the recovery task, although fundamental and concrete in dealing with failures, still has a limited number of contributions. Similarly, failure prevention is concentrated around a few tasks, despite the potential for many other unexplored methods. Currently, most approaches for failure prevention are applied online and focus exclusively on current and future state characteristics. Introducing assumptions and information about the system's working principles could lead to more actionable insights. For instance, model-based prevention would allow operators to estimate in advance the risks associated with particular actions, such as a canary release or a server shutdown.

Additionally, the advent of virtualization technologies requires new research focusing on specific targets (e.g., hypervisors, virtual machines, containers, etc.), creating new tasks such as hypervisor anomaly detection and container failure prediction.

Finally, the application of novel AI approaches, such as Deep Learning, has already translated into a variety of new methods for failure prediction, anomaly detection, and root-cause analysis. Future breakthroughs in AI are likely to further advance AIOps.

### Concluding Remarks and Future Work
In this work, we explored a variety of AIOps approaches for the management of failures in IT systems. Our survey study, which analyzed 100 contributions, provided a comprehensive overview of the topic in terms of goals, sources, and methods. We highlighted the limitations of current approaches and discussed possibilities for expanding and integrating the current areas of research in AIOps. We hope that the results presented here will support researchers and engineers working with AIOps and contribute to future investigations. As AIOps is an active and increasingly popular research area, we expect future work to update this article with new references and expand the discussion with newly established topics.

### References
[1] Rui Abreu, Peter Zoeteweij, and Arjan J.C. van Gemund. 2009. Spectrum-based multiple fault localization. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering. IEEE, 88–99. https://doi.org/10.1109/ASE.2009.25

[2] Armen Aghasaryan, Eric Fabre, Albert Benveniste, Renée Boubour, and Claude Jard. 1998. Fault detection and diagnosis in distributed systems: An approach by partially stochastic Petri nets. Discrete Event Dyn. Syst. 8, 2 (1998), 203–231. https://doi.org/10.1023/a:1008241818642

[3] Marcos K. Aguilera, Jeffrey C. Mogul, Janet L. Wiener, Patrick Reynolds, and Athicha Muthitacharoen. 2003. Performance debugging for distributed systems of blackboxes. ACM SIGOPS Operat. Syst. Rev. 37, 5 (Dec. 2003), 74–89. https://doi.org/10.1145/1165389.945454

[4] Javier Alonso, Jordi Torres, Josep Ll. Berral, and Ricard Gavalda. 2010. Adaptive online software aging prediction based on machine learning. In Proceedings of the IEEE/IFIP International Conference on Dependable Systems Networks (DSN’10). IEEE, 507–516. https://doi.org/10.1109/dsn.2010.5544275

[5] J. Arlat, M. Aguera, L. Amat, Y. Crouzet, J.-C. Fabre, J.-C. Laprie, E. Martins, and D. Powell. 1990. Fault injection for dependability validation: A methodology and some applications. IEEE Trans. Softw. Eng. 16, 2 (Feb. 1990), 166–182. https://doi.org/10.1109/32.44380

[6] Mona Attariyan, Michael Chow, and Jason Flinn. 2012. X-Ray: Automating root-cause diagnosis of performance anomalies in production software. In Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation (OSDI’12). USENIX Association, 307–320. https://doi.org/10.5555/2387880.2387910

[7] Julien Audibert, Pietro Michiardi, Frédéric Guyard, Sébastien Marti, and Maria A. Zuluaga. 2020. USAD: Unsupervised anomaly detection on multivariate time series. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’20). ACM, New York, NY, 3395–3404. https://doi.org/10.1145/3394486.3403392

[8] Tom Auld, Andrew W. Moore, and Stephen F. Gull. 2007. Bayesian neural networks for internet traffic classification. IEEE Trans. Neural Netw. 18, 1 (Jan. 2007), 223–239. https://doi.org/10.1109/tnn.2006.883010

[9] Paramvir Bahl, Ranveer Chandra, Albert Greenberg, Srikanth Kandula, David A. Maltz, and Ming Zhang. 2007. Towards highly reliable enterprise network services via inference of multi-level dependencies. In Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM’07). ACM, New York, NY, 13–24. https://doi.org/10.1145/1282380.1282383

[10] Chetan Bansal, Sundararajan Renganathan, Ashima Asudani, Olivier Midy, and Mathru Janakiraman. 2020. DeCaf: Diagnosing and triaging performance issues in large-scale cloud services. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP’20). ACM, New York, NY, 201–210. https://doi.org/10.1145/3377813.3381353

[11] Paul Barham, Rebecca Isaacs, Richard Mortier, and Dushyanth Narayanan. 2003. Magpie: Online modelling and performance-aware systems. In Proceedings of the 9th Conference on Hot Topics in Operating Systems, Vol. 9. USENIX Association, 15. https://doi.org/10.5555/1251054.1251069

[12] Ivan Beschastnikh, Yuriy Brun, Michael D. Ernst, and Arvind Krishnamurthy. 2014. Inferring models of concurrent systems from logs of their behavior with CSight. In Proceedings of the 36th International Conference on Software Engineering (ICSE’14). ACM, New York, NY, 468–479. https://doi.org/10.1145/2568225.2568246

[13] Netflix Technology Blog. 2016. Netflix Chaos Monkey Upgraded. Retrieved from https://netflixtechblog.com/netflix-chaos-monkey-upgraded-1d679429be5d.

[14] Peter Bodik, Moises Goldszmidt, Armando Fox, Dawn B. Woodard, and Hans Andersen. 2010. Fingerprinting the data center: Automated classification of performance crises. In Proceedings of the 5th European Conference on Computer Systems (EuroSys’10). ACM, New York, NY, 111–124. https://doi.org/10.1145/1755913.1755926

[15] A.T. Bouloutas, S. Calo, and A. Finkel. 1994. Alarm correlation and fault identification in communication networks. IEEE Trans. Commun. 42, 2/3/4 (1994), 523–533. https://doi.org/10.1109/tcomm.1994.577079

[16] L. C. Briand, J. W. Daly, and J. K. Wust. 1999. A unified framework for coupling measurement in object-oriented systems. IEEE Trans. Softw. Eng. 25, 1 (1999), 91–121. https://doi.org/10.1109/32.748920

[17] Broadcom. 2020. AIOps—Broadcom. Retrieved from https://www.broadcom.com/products/software/aiops.

[18] Andy Brown, Aaron Tuor, Brian Hutchinson, and Nicole Nichols. 2018. Recurrent neural network attention mechanisms for interpretable system log anomaly detection. In Proceedings of the 1st Workshop on Machine Learning for Computing Systems (MLCS’18). ACM, New York, NY, Article 1, 8 pages. https://doi.org/10.1145/3217871.3217872

[19] Lisa Burnell and Eric Horvitz. 1995. Structure and chance: Melding logic and probability for software debugging. Commun. ACM 38, 3 (Mar. 1995), 31–ff. https://doi.org/10.1145/203330.203338

[20] K.L. Butler and J.A. Momoh. 1999. A neural net based approach for fault diagnosis in distribution networks. In Proceedings of the IEEE Power Engineering Society, Vol. 1. IEEE, 353–356. https://doi.org/10.1109/PESW.1999.747478

[21] V. Castelli, R.E. Harper, P. Heidelberger, S.W. Hunter, K.S. Trivedi, K. Vaidyanathan, and W.P. Zeggert. 2001. Proactive management of software aging. IBM J. Res. Dev. 45, 2 (Mar. 2001), 311–332. https://doi.org/10.1147/rd.452.0311

[22] Raghavendra Chalapathy and Sanjay Chawla. 2019. Deep Learning for Anomaly Detection: A Survey. Retrieved from http://arxiv.org/abs/1901.03407.

[23] Thanyalak Chalermarrewong, Tiranee Achalakul, and Simon Chong Wee See. 2012. Failure prediction of data centers using time series and fault tree analysis. In Proceedings of the IEEE 18th International Conference on Parallel and Distributed Systems. IEEE, 794–799. https://doi.org/10.1109/icpads.2012.129

[24] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. Comput. Surveys 41, 3 (July 2009), 15:1–15:58. https://doi.org/10.1145/1541880.1541882

[25] M.Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. 2002. Pinpoint: Problem determination in large, dynamic Internet services. In Proceedings of the International Conference on Dependable Systems and Networks. IEEE, 595–604. https://doi.org/10.1109/DSN.2002.1029005

[26] Mike Y. Chen, Anthony Accardi, Emre Kiciman, Jim Lloyd, Dave Patterson, Armando Fox, and Eric Brewer. 2004. Path-based failure and evolution management. In Proceedings of the 1st Conference on Symposium on Networked Systems Design and Implementation (NSDI’04). USENIX Association, 23. Retrieved from https://dl.acm.org/doi/10.5555/1251175.1251198.

[27] Xin Chen, Charng-Da Lu, and Karthik Pattabiraman. 2014. Failure analysis of jobs in compute clouds: A Google cluster case study. In Proceedings of the IEEE 25th International Symposium on Software Reliability Engineering. IEEE, 167–177. https://doi.org/10.1109/issre.2014.34

[28] S.R. Chidamber and C.F. Kemerer. 1994. A metrics suite for object-oriented design. IEEE Trans. Softw. Eng. 20, 6 (June 1994), 476–493. https://doi.org/10.1109/32.295895

[29] Michael Chow, David Meisner, Jason Flinn, Daniel Peek, and Thomas F. Wenisch. 2014. The mystery machine: End-to-end performance analysis of large-scale Internet services. In Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation (OSDI’14). USENIX Association, 217–231. Retrieved from https://dl.acm.org/doi/10.5555/2685048.2685066

[30] Holger Cleve and Andreas Zeller. 2005. Locating causes of program failures. In Proceedings of the 27th International Conference on Software Engineering (ICSE’05). ACM, New York, NY, 342–351. https://doi.org/10.1145/1062455.1062522

[31] Ira Cohen, Moises Goldszmidt, Terence Kelly, Julie Symons, and Jeffrey S. Chase. 2004. Correlating instrumentation data to system states: A building block for automated diagnosis and control. In Proceedings of the 6th Conference on Symposium on Operating Systems Design and Implementation (OSDI’04). USENIX Association, 16. Retrieved from https://dl.acm.org/doi/10.5555/1251254.1251270.

[32] Ira Cohen, Steve Zhang, Moises Goldszmidt, Julie Symons, Terence Kelly, and Armando Fox. 2005. Capturing, indexing, clustering, and retrieving system history. In Proceedings of the 20th ACM Symposium on Operating Systems Principles (SOSP’05). ACM, New York, NY, 105–118. https://doi.org/10.1145/1095810.1095821

[33] Carlos H.A. Costa, Yoonho Park, Bryan S. Rosenburg, Chen-Yong Cher, and Kyung Dong Ryu. 2014. A system software approach to proactive memory-error avoidance. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC’14). IEEE, 12 pages. https://doi.org/10.1109/SC.2014.63

[34] A. Csenki. 1990. Bayes predictive analysis of a fundamental software reliability model. IEEE Trans. Reliabil. 39, 2