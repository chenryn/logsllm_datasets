1.618
0.484
28.307
window size becomes 100, 000, which contains of about
10% of all tweets with URLs per hour, the running time
is only 6.9 minutes (see Figure 9). Next, we estimate the
time required to classify a single URL. Our system currently
uses 100 crawling threads to concurrently visit URL redi-
rect chains; on average, each thread requires 2.42 s to visit
a single URL redirect chain. If the window size is 100, 000,
we need 28.307 ms to process a single URL (see Table 7);
thus, our system can process about 127, 000 URLs per hour.
Therefore, our system can handle 10% of the tweet sam-
ples, the level provided by the Gardenhose access role, in
real time. By increasing the number of crawling threads,
we can process more than 10% of the tweet samples. For
instance, if we use 1, 000 crawling threads, we can process
about 576, 000 URLs per hour. Even if we do this, the cur-
rent implementation cannot process all the tweets, because
we would have to process a single URL in less than 3.6 ms
to handle 1, 000, 000 URLs per hour.
Figure 9. Running time for each component
to process a tweet window
4.6 Real-time Detection and Sliding Window
The real-time version of WARNINGBIRD uses a sliding
window technique for achieving good latency and detection
coverage. A small window gives immediate results; how-
ever, it cannot catch suspicious URLs that repeat after long-
time intervals. A large window has good detection cover-
age; however, its latency is bad. A sliding window is a
well-known technique for taking advantage of both small
and large windows. Let w denote the window size and s
denote the sliding size (s ≤ w). Whenever a sliding win-
dow system receives s new items, it processes the previous
w − s items and the s new items at the same time. There-
fore, the latency of this method depends on s and its de-
tection coverage depends on w. Currently, we have set w at
0102030405060Ratio of reoccurred URLs (%) Day suspiciousbenign050100150200250300350400450500Number of unique suspicious URLs Day groupno group0501001502002503003504004501000020000400006000080000100000Time (s) Window size classificationfeature extractiondomain groupingpicious. Therefore, the detection accuracy of our system
given the sample data is about 86.3%.
5 Discussion
In this section, we discuss some limitations of our sys-
tem and possible evasion techniques.
Dynamic redirection: Currently, WARNINGBIRD uses a
static crawler written in Python. Because it can handle
only HTTP redirections, it will be ineffective on pages
with embedded dynamic redirections such as JavaScript or
Flash redirection. Therefore, WARNINGBIRD will desig-
nate pages with embedded dynamic redirection as entry
point URLs. This determination causes inaccuracy in some
of the feature values, including the redirect chain lengths,
positions of the entry point URLs, and the number of dif-
ferent landing URLs. Therefore, in the future we will use
customized Web browsers to retrieve redirect chains fully.
Multiple redirections: Web pages can embed several ex-
ternal pages and different content. Therefore, some pages
can cause multiple redirections. Because our system cur-
rently only considers HTTP redirection and does not con-
sider page-level redirection, it cannot catch multiple redi-
rections. Therefore, we need customized browsers to catch
and address multiple redirections.
Coverage and scalability: Currently, our system only
monitors one percent of the samples from the Twitter pub-
lic timeline, because our accounts have the Spritzer access
role. As shown in Section 4, if our accounts were to take on
the Gardenhose access role, which allows the processing of
10% of the samples, our system could handle this number
of samples in real time. The current implementation, how-
ever, cannot handle 100% of the Twitter public timeline.
Therefore, we must extend WARNINGBIRD to a distributed
detection system, for instance, Monarch [24], to handle the
entire Twitter public timeline.
Feature evasion methods: Attackers can fabricate the fea-
tures of their attacks to evade our detection system. For
instance, they could use short redirect chains, change the
position of their entry point URLs, and reuse initial and
landing URLs. These modiﬁcations, paradoxically, would
allow previous detection systems to detect their malicious
URLs. Attackers may also be able to reduce the frequency
of their tweets to bypass our detection system. However,
this will also reduce the number of visitors to their mali-
cious pages. Features derived from tweet information, how-
ever, are relatively weak at protecting against forgery, as
many researchers have already pointed out [21, 24, 31]. At-
tackers could use a large number of source applications and
Twitter accounts, use similar tweet texts, and carefully ad-
just the numbers of followers and friends of their accounts
Figure 10. Time difference between Warning-
Bird’s detection of suspicious accounts and
Twitter’s suspension within a day
10, 000 and s at 2, 000. Every 12 minutes, the real-time ver-
sion of WARNINGBIRD returns suspicious URLs that have
appeared in the previous hour. Because our system can pro-
cess 10, 000 collected tweets in less than one minute (see
Figure 9), we can detect suspicious URLs with only one-
minute time lags. Between August 6 and August 18, 2011,
the real-time WARNINGBIRD reported 4, 336 unique suspi-
cious URLs without system errors.
4.7 Comparison with Twitter
We compare the efﬁciency of WARNINGBIRD with that
of Twitter’s detection system. For the comparison, we sam-
pled 14, 905 accounts detected by the real-time WARNING-
BIRD between September 1 and October 22, 2011. To com-
pare their efﬁciencies, we measured the time difference be-
tween WARNINGBIRD’s detection and Twitter’s suspension
of the accounts. We monitored the WARNINGBIRD to ob-
tain newly detected suspicious accounts and then checked
the status of each account every 15 s until it was suspended,
within a day. Among the sampled accounts, 5, 380 ac-
counts were suspended within a day; 37.3% of them was
suspended within a minute, another 42.5% of them was
suspended within 200 minutes, and the remaining 20.7%
of them was suspended within a day (see Figure 10). The
average time difference is 13.5 minutes; therefore, our de-
tection system is more efﬁcient than that of Twitter. We
also checked the statuses of the sampled accounts on Octo-
ber 28, 2011 to verify the accuracy of our system. Among
the 14, 905 accounts, 9, 250 accounts were suspended. We
then randomly selected 500 accounts from the remaining
5, 655 active accounts to manually check their suspicious-
ness. Among the 500 accounts, 320 accounts were sus-
020406080100 0 200 400 600 800 1000 1200 1440CDF (%)Time difference between detection and suspension (min)to increase the standard deviation values. In addition, they
could increase the standard deviation of their account cre-
ation date if they own or have compromised older accounts.
Although these features are weak, attackers have to con-
sume their resources and time to fabricate these features.
Therefore, using these features is still meaningful. The
strongest evasion method is deﬁnitely to increase the num-
ber of redirect servers. This method, however, would re-
quire many resources and large ﬁnancial investment on the
part of the attackers.
6 Related Work
6.1 Twitter Spam Detection
Many Twitter spam detection schemes have been intro-
duced. Most have focused on how to collect a large num-
ber of spam and non-spam accounts and extract the features
that can effectively distinguish spam from non-spam ac-
counts. To detect spam accounts, some schemes investigate
collected data manually [2, 28], some use honey-proﬁles to
lure spammers [16, 23], some monitor the Twitter public
timeline to detect accounts that post tweets with blacklisted
URLs [11, 31], and some monitor Twitter’s ofﬁcial account
for spam reporting, @spam [21].
Much preliminary work [2, 11, 16, 23, 28] relies on ac-
count features including the numbers of followers and
friends, account creation dates, URL ratios, and tweet text
similarities, which can be efﬁciently collected but easily
fabricated. To avoid feature fabrication, recent work [21,31]
relies on more robust features extracted from the Twit-
ter graph. Yang et al. [31] focused on relations between
spam nodes and their neighboring nodes such as a bi-
directional link ratio and betweenness centrality, because
spam nodes usually cannot establish strong relationships
with their neighboring nodes. They also introduced other
features based on timing and automation. Song et al. [21]
considered the relations between spam senders and re-
ceivers such as the shortest paths and minimum cut, be-
cause spam nodes usually cannot establish robust relation-
ships with their victim nodes. The extraction of these robust
features, however, is time and resource consuming.
6.2 Suspicious URL Detection
Many suspicious URL detection schemes have been pro-
posed. They can be classiﬁed into either static or dy-
namic detection systems. Some lightweight static detec-
tion systems focus on the lexical features of a URL such
as its length, the number of dots, or each token it has [19],
and also consider underlying DNS and WHOIS informa-
tion [17, 18]. More sophisticated static detection systems,
such as Prophiler [3], additionally extract features from
HTML content and JavaScript codes to detect drive-by
download attacks. However, static detection systems can-
not detect suspicious URLs with dynamic content such as
obfuscated JavaScript, Flash, and ActiveX content. There-
fore, we need dynamic detection systems [4, 7, 24, 29, 30]
that use virtual machines and instrumented Web browsers
for in-depth analysis of suspicious URLs. Nevertheless, all
of these detection systems may still fail to detect suspicious
sites with conditional behaviors.
6.3 ARROW: Generating Signatures to Detect
Drive-by Downloads
Zhang et al. have developed ARROW [32], which also
considers a number of correlated URL redirect chains to
generate signatures of drive-by download attacks. It uses
honeyclients to detect drive-by download attacks and col-
lect logs of HTTP redirection traces from the compromised
honeyclients. From these logs, it identiﬁes central servers
that are contained in a majority of the HTTP traces to the
same binaries and generates regular expression signatures
using the central servers’ URLs. ARROW also groups do-
main names with the same IP addresses to avoid IP fast ﬂux
and domain ﬂux [12, 22].
Although the methods for detecting central servers in
ARROW and for detecting entry point URLs in WARNING-
BIRD are similar, there are three important differences be-
tween these two systems. First, ARROW’s HTTP traces are
redirect chains between malicious landing pages and mal-
ware binaries. Therefore, ARROW cannot be applied to
detect other Web attacks, such as spam, scam, and phishing
attacks, which do not have such redirect chains to enable
the downloading of malware binaries. Moreover, if hon-
eyclients cannot access malicious landing pages owing to
conditional redirections, ARROW cannot obtain any HTTP
traces. Second, ARROW focuses on how to generate the
signatures of central servers that redirect visitors to the same
malware binaries, whereas WARNINGBIRD focuses on how
to measure the suspiciousness of entry point URLs. Third,
ARROW relies on logs of HTTP traces to detect central
servers. Therefore, it cannot detect suspicious URLs in real
time. In contrast, WARNINGBIRD is a real-time system.
7 Conclusion
Previous suspicious URL detection systems are weak at
protecting against conditional redirection servers that dis-
tinguish investigators from normal browsers and redirect
them to benign pages to cloak malicious landing pages.
In this paper, we propose a new suspicious URL detection
system for Twitter, WARNINGBIRD. Unlike the previous
systems, WARNINGBIRD is robust when protecting against
conditional redirection, because it does not rely on the fea-
tures of malicious landing pages that may not be reachable.
Instead, it focuses on the correlations of multiple redirect
chains that share redirection servers. We introduced new
features on the basis of these correlations, implemented
a real-time classiﬁcation system using these features, and
evaluate the system’s accuracy and performance. The eval-
uation results showed that our system is highly accurate
and can be deployed as a real-time system to classify large
samples of tweets from the Twitter public timeline. In the
future, we will extend our system to address dynamic and
multiple redirections. We will also implement a distributed
version of WARNINGBIRD to process all tweets from the
Twitter public timeline.
References
[1] D. Antoniades, I. Polakis, G. Kontaxis, E. Athanasopoulos,
S. Ioannidis, E. P. Markatos, and T. Karagiannis. we.b: The
web of short URLs. In Int. World Wide Web Conf. (WWW),
2011.
[2] F. Benevenuto, G. Magno, T. Rodrigues, and V. Almeida.
In Collaboration, Elec-
Detecting spammers on Twitter.
tronic messaging, Anti-Abuse and Spam Conf. (CEAS),
2010.
[3] D. Canali, M. Cova, G. Vigna, and C. Kruegel. Prophiler:
A fast ﬁlter for the large-scale detection of malicious web
pages. In Int. World Wide Web Conf. (WWW), 2011.
[4] Capture-HPC. https://projects.honeynet.org/
capture-hpc.
[5] Y.-W. Chen and C.-J. Lin. Combining SVMs with various
feature selection strategies. In Feature Extraction, volume
207 of Studies in Fuzziness and Soft Computing, pages 315–
324. 2006.
[6] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia. Who is
In Annual
tweeting on Twitter: Human, bot, or cyborg?
Computer Security Applications Conf. (ACSAC), 2010.
[7] M. Cova, C. Kruegel, and G. Vigna. Detection and analysis
of drive-by-download attacks and malicious JavaScript code.
In Int. World Wide Web Conf. (WWW), 2010.
[8] P. Eckersley. How unique is your web browser? In Privacy
Enhancing Technologies (PET), 2010.
[9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A library for large linear classiﬁcation.
Journal of Machine Learning Research, 9:1871–1874, 2008.
[10] Google. Google safe browsing API. http://code.
google.com/apis/safebrowsing.
[11] C. Grier, K. Thomas, V. Paxson, and M. Zhang. @spam:
The underground on 140 characters or less. In ACM Conf.
Computer and Communications Security (CCS), 2010.
[12] T. Holz, C. Gorecki, K. Rieck, and F. C. Freiling. Measuring
In Network and
and detecting fast-ﬂux service networks.
Distributed System Security Symp. (NDSS), 2008.
[13] P. Jaccard. The distribution of ﬂora in the alpine zone. The
New Phytologist, 11(2):37–50, 1912.
[14] A. Kapravelos, M. Cova, C. Kruegel, and G. Vigna. Escape
from monkey island: Evading high-interaction honeyclients.
In SIG SIDAR Conf. Detection of Intrusions and Malware &
Vulnerability Assessment (DIMVA), 2011.
[15] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter,
a social network or a news media? In Int. World Wide Web
Conf. (WWW), 2010.
[16] K. Lee, J. Caverlee, and S. Webb. Uncovering social spam-
mers: Social honeypots + machine learning. In ACM SIGIR
Conf., 2010.
[17] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Beyond
blacklists: Learning to detect malicious web sites from sus-
picious URLs. In ACM SIGKDD Int. Conf. Knowledge Dis-
covery and Data Mining (KDD), 2009.
[18] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identify-
ing suspicious URLs: An application of large-scale online
learning. In Int. Conf. Machine Learning (ICML), 2009.
[19] D. K. McGrath and M. Gupta. Behind phishing: An ex-
amination of phisher modi operandi. In USENIX Workshop
Large-Scale Exploits and Emergent Threats (LEET), 2008.
[20] M. A. Rajab, L. Ballard, N. Jagpal, P. Mavrommatis, D. No-
jiri, N. Provos, and L. Schmidt. Trends in circumventing
web-malware detection. Technical report, Google, 2011.
[21] J. Song, S. Lee, and J. Kim. Spam ﬁltering in Twitter using
sender-receiver relationship. In Int. Symp. Recent Advances
in Intrusion Detection (RAID), 2011.
[22] B. Stone-Gross, M. Cova, L. Cavallaro, B. Gilbert, M. Szyd-
lowski, R. Kemmerer, C. Kruegel, and G. Vigna. Your botnet
is my botnet: Analysis of a botnet takeover. In ACM Conf.
Computer and Communications Security (CCS), 2009.
[23] G. Stringhini, C. Kruegel, and G. Vigna. Detecting spam-
mers on social networks. In Annual Computer Security Ap-
plications Conf. (ACSAC), 2010.
[24] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song. Design
and evaluation of a real-time URL spam ﬁltering system. In
IEEE Symp. Security and Privacy (Oakland), 2011.
[25] K. Thomas, C. Grier, V. Paxson, and D. Song. Suspended
accounts in retrospect: An analysis of twitter spam. In In-
ternet Measurement Conf. (IMC), 2011.
[26] TweetAttacks. Twitter marketing software that breaks the
limits. http://tweetattacks.com.
[27] Twitter Developers. Streaming API. https://dev.
twitter.com/docs/streaming-api.
[28] A. Wang. Don’t follow me: Spam detecting in Twitter. In
Int. Conf. Security and Cryptography (SECRYPT), 2010.
[29] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski,
S. Chen, and S. King. Automated web patrol with Strider
HoneyMonkeys: Finding web sites that exploit browser vul-
In Network and Distributed System Security
nerabilities.
Symp. (NDSS), 2006.
[30] C. Whittaker, B. Ryner, and M. Nazif. Large-scale automatic
classiﬁcation of phising pages. In Network and Distributed
System Security Symp. (NDSS), 2010.
[31] C. Yang, R. Harkreader, and G. Gu. Die free or live hard?
empirical evaluation and new design for ﬁghting evolving
Twitter spammers. In Int. Symp. Recent Advances in Intru-
sion Detection (RAID), 2011.
[32] J. Zhang, C. Seifert, J. W. Stokes, and W. Lee. ARROW:
Generating signatures to detect drive-by downloads. In Int.
World Wide Web Conf. (WWW), 2011.