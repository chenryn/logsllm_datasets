# N-Way Fail-Over Infrastructure for Reliable Servers and Routers

**Authors:**
- Yair Amir
- Ryan Caudy
- Ashima Munjal
- Theo Schlossnagle
- Ciprian Tutu

**Affiliation:**
Johns Hopkins University
{yairamir, wyvern, munjal, theos, ciprian}@cnds.jhu.edu
Computer Science Department

## Abstract

Maintaining the availability of critical servers and routers is a significant concern for many organizations. At the lowest level, IP addresses represent the global namespace by which services are accessible on the Internet.

We introduce Wackamole, a fully distributed software solution based on a provably correct algorithm that negotiates the assignment of IP addresses among the currently available servers upon detection of faults. This reallocation ensures that at any given time, each public IP address of the server cluster is covered exactly once, as long as at least one physical server survives the network fault. The same technique is extended to support highly available routers.

The paper presents the design considerations, algorithm specification, and correctness proof, discusses practical usage for server clusters and routers, and evaluates the system's performance.

## 1. Introduction

Maintaining the availability of critical network servers is a crucial concern for many organizations. Server redundancy is the traditional approach to provide availability in the presence of failures. From the client perspective, a network-accessible service is resolved via a set of public IP addresses specified for this service. Therefore, the continued availability of a service via these IP addresses is a prerequisite for providing uninterrupted service to the client. Each of the serviceâ€™s public IP addresses must be covered by exactly one physical server at any given time. If no physical server covers a public IP address, clients will not receive any service. Conversely, if more than one physical server covers the same IP address, the network might malfunction, and clients may not be served correctly.

A sizable market exists for hardware solutions that maintain the availability of IP addresses, usually via a gateway that hides the actual servers behind a smart switch or router in a centralized manner. We present Wackamole, a high-availability tool for clusters of servers. Wackamole ensures that all the public IP addresses of a service are available to its clients. It is a fully distributed software solution based on a provably correct algorithm that negotiates the assignment of IP addresses among the available servers upon detection of faults and recoveries, providing N-way fail-over, so that any one of a number of servers can cover for any other.

Using a simple algorithm that utilizes strong group communication semantics, Wackamole demonstrates the application of group communication to address a critical availability problem at the core of the system, even in the presence of cascading network or server faults and recoveries. We also demonstrate how the same architecture is extended to provide a similar service for highly available routers.

The remainder of this paper is organized as follows:
- Section 2 introduces the system architecture.
- Section 3 describes the system model and the core algorithm behind Wackamole and discusses its correctness.
- Section 4 analyzes practical considerations and presents two applications for the system.
- Section 5 presents performance results concerning the reconfiguration time of Wackamole clusters.
- Section 6 discusses related work.
- Section 7 concludes the paper.

## 2. System Overview

Our solution has three main components, as shown in Figure 1:

1. **IP Address Control Mechanism**: Acquires and releases IP addresses.
2. **State Synchronization Algorithm (Wackamole Algorithm)**: Manages the logical assignment of virtual IP addresses among the currently connected members.
3. **Membership Service Provided by a Group Communication Toolkit**: Maintains a membership service among the currently connected servers and notifies the synchronization algorithm of any view changes due to server crashes and recoveries, or network partitions and remerges.

### Figure 1: Wackamole Architecture

```
System 1
+-----------------+   +-----------------+   +-----------------+
| Wackamole       |   | State           |   | IP Address      |
|                 |   | Synchronization |   | Manager         |
|                 |   | Algorithm       |   |                 |
+-----------------+   +-----------------+   +-----------------+
| Group           |   | IP Addresses    |   | xxx.yyy.221.151|
| Communication   |   | xxx.yyy.221.152|   | xxx.yyy.221.155|
| System          |   +-----------------+   +-----------------+
+-----------------+
| Network         |
+-----------------+

System 2
+-----------------+   +-----------------+   +-----------------+
| Wackamole       |   | State           |   | IP Address      |
|                 |   | Synchronization |   | Manager         |
|                 |   | Algorithm       |   |                 |
+-----------------+   +-----------------+   +-----------------+
| Group           |   | IP Addresses    |   | xxx.yyy.221.153|
| Communication   |   | xxx.yyy.221.154|   | xxx.yyy.221.156|
| System          |   +-----------------+   +-----------------+
+-----------------+
| Network         |
+-----------------+
```

The group communication toolkit maintains a membership service among the currently connected servers and notifies the synchronization algorithm of any view changes due to server crashes and recoveries, or network partitions and remerges. The synchronization algorithm manages the logical assignment of virtual IP addresses among the currently connected members, avoiding conflicts that can occur upon merges and recoveries and covering the "holes" that can arise as a result of a crash or partition.

The IP address control mechanism enforces the decisions of the synchronization algorithm by acquiring and releasing the IP addresses accordingly. These mechanisms are highly specific to the operating system on which the Wackamole system runs.

The correctness of the system depends on the assumption that the group communication system provides an accurate view of the current network connectivity. If there is additional connectivity beyond that reported by the group communication system, there may be conflicts in the assignment of IP addresses. Conversely, if the group communication system does not detect the disconnection of a server from the current membership in a timely manner, the IP addresses that were covered by that server will be unavailable to the clients, as the system will not reconfigure without the proper notification.

## 3. The Wackamole Algorithm

In this section, we present the state synchronization algorithm that forms the core of the Wackamole system and discuss its correctness, given the assumption that the membership notifications issued by the group communication system reflect the actual network status.

### 3.1 System Model

To formally identify the problem that Wackamole attempts to solve, we define the system model and introduce the correctness properties that the algorithm and the implemented system need to maintain.

We consider a set \( S = \{s_1, s_2, \ldots, s_m\} \) of servers that provide service to outside client applications. The servers are all located in the same Local Area Network (LAN), but are susceptible to crashes and temporary network partitions. During a network partition, the servers are separated into two or more components that are unable to communicate with each other.

Client applications access the services through IP addresses in the set \( I = \{i_1, i_2, \ldots, i_n\} \). The servers in \( S \) are responsible for covering the set \( I \) of virtual IP addresses. We refer to the IP addresses in \( I \) as virtual to distinguish them from the stationary default IP addresses used by the servers for intercommunication.

Client applications are oblivious to the stationary IP addresses of the servers in \( S \) or to the possible partitioning that may exist among the servers.

To guarantee correct service, the following properties need to be maintained:

**Property 1 (Correctness)**: Every IP address in the set \( I \) is covered exactly once by a server in each subset \( S_k \), where \( S_k \) is a maximal connected component whose servers are in the operational (RUN) state.

**Property 2 (Liveness)**: If there is a time \( t \) from which a set of connected servers does not experience any crashes/recoveries or network partitions/merges, the servers will switch to the operational (RUN) state.

To guarantee these properties, we rely on the group communication system to follow the Virtual Synchrony properties [6, 14] in partitionable systems and to provide Agreed message delivery. The Virtual Synchrony property specifies that any two servers that advance together from one membership to the next will deliver an identical set of messages in the first membership. The Agreed delivery property guarantees that the messages will be delivered in the same order at all servers. Additionally, we assume that the group communication system provides a membership service that provides each server in the group with a uniquely ordered list of the currently connected participants.

### 3.2 Algorithm Specification

The algorithm runs according to the state machine presented in Figure 2.

#### Figure 2: Wackamole Algorithm

```
REALLOCATION COMPLETE
BALANCE TIMEOUT
VIEW_CHANGE
GATHER
RUN
BALANCE_MSG
BALANCE
VIEW_CHANGE
BALANCE COMPLETE
```

Each server maintains a table `current_table` that contains the virtual IP allocation during the current membership. During normal operation, the algorithm is in the `RUN` state. In this state, each server is responsible for a set of virtual IP addresses and will answer all the requests directed to those IP addresses. While in the `RUN` state, the `current_table` information is conflict-free, and the complete IP set is covered, maintaining the correctness guarantees of the algorithm.

When the group communication system delivers a `VIEW CHANGE` event, a backup of the IP table is created, and a `STATE MSG` is sent to every member of the new view containing the information about the IP addresses managed by the server and the identifier of the view in which it is initiated. The algorithm then moves to the `GATHER` state.

#### Algorithm 1: RUN State

```python
1: when: VIEW CHANGE do
2:   old_table = current_table
3:   send STATE MSG
4:   state = GATHER
5:   Change IPs()
```

In the `GATHER` state, each server incorporates the information received through the `STATE MSGs` in its `current_table` variable and checks for the existence of conflicts in the IP allocation. If members from previously partitioned components are merged together, conflicts are expected since each component covers the full IP address set. `ResolveConflicts()` is a deterministic procedure invoked as soon as a new `STATE MSG` is received, that checks whether the server that sent this message introduces any conflict with respect to the information already gathered about the set of covered IP addresses. If a conflict is detected, the server drops the overlapping addresses, thus restoring consistency at the network level as soon as possible.

When all the state messages (`STATE MSG`) have been received, each server invokes a deterministic procedure `ReallocateIPs()`. During the `ReallocateIPs()` procedure, the servers ensure that all the virtual IPs are covered by a server in the current configuration. Specifically, the procedure relies on the uniquely ordered membership list provided by the group communication system to distributively decide which server covers which IP address.

If the `GATHER` state is interrupted by a cascading `VIEW CHANGE` event, the server clears its `current_table`, discarding the information already collected and reverting to the information in the `old_table`, then sends a new `STATE MSG` to all members of the new configuration.

#### Algorithm 2: GATHER State

```python
1: when: receive STATE MSG with current view id do
2:   update current_table
3:   ResolveConflicts()
4:   if (received STATE MSG from all current_table.members) then
5:     ReallocateIPs()
6:     state = RUN
7: when: VIEW CHANGE do
8:   clear current_table
9:   send STATE MSG
10: when: BALANCE MSG do
11:   ignore
```

### 3.3 Correctness of the Algorithm

We consider a subset of servers \( S' \subset S \) that are in the operational `RUN` state. Between the servers in \( S' \), each virtual IP address is covered exactly once. We consider a `VIEW CHANGE` event that is detected by members of \( S' \). According to the group communication guarantees, all members of \( S' \) will receive `VIEW CHANGE` notifications, even though they may be disconnected from each other. Following the algorithm, each server will proceed to the `GATHER` state and send a state message containing its local knowledge base.

Letâ€™s consider a server \( s \) that was part of \( S' \) and is now part of \( S'' \) as indicated by the group communication. The group communication system provides every \( s \) in \( S'' \) with an identically ordered list of all the servers in \( S'' \).

**Lemma 1**: For every connected set \( S' \) of servers in the `RUN` state, every IP address is covered at most once by a server in \( S' \).

**Proof**:
We consider a set of servers that are connected after a view change event, as indicated by the group communication notification. In order for the servers to advance to the `RUN` state, the state transfer algorithm, executed in the `GATHER` state, needs to complete. Therefore, we consider the situation where a set of servers \( S' \) does not detect further `VIEW CHANGE` events until they exit the `GATHER` state.

Assume that upon receiving the last `STATE` message in the `GATHER` state (line 4 in Algorithm 2), there exists a virtual IP address \( vip \) that is covered by two servers \( p \) and \( q \) in \( S' \). According to the Virtual Synchrony and Agreed delivery guarantees of the group communication, both \( p \) and \( q \) received all the state messages that were sent during the `GATHER` state, therefore they received their own state messages. According to the management of the `current_table` variable from the algorithm (line 2 in Algorithm 1 and lines 2, 8 in Algorithm 2) and the fact that only `STATE MSGs` generated in the current view are considered (line 1 of Algorithm 2), the variable will accurately reflect at this point the state of the currently connected component. During this stage, following the algorithm, the servers do not acquire new IP addresses, so both \( p \) and \( q \) were already covering \( vip \) from their previous memberships. From the algorithm, in the `ResolveConflicts()` procedure (line 3 in Algorithm 2), when \( p \) receives the state message from \( q \), it will notice the conflict in the coverage of \( vip \) and will adjust its IP coverage table and release \( vip \) if \( p \) appears in the membership list of \( S' \) before \( q \). The same reasoning applies for \( q \); therefore, it is impossible for \( vip \) to be covered by both \( p \) and \( q \) at this point. Furthermore, both \( p \) and \( q \) will have the same view of the virtual IP coverage. Note that reaching agreement does not assume any particular relation between the initial states of \( p \) and \( q \) or of the other members of \( S' \).

When all the state messages have been received, each server will execute the `ReallocateIPs()` procedure. During this procedure, a server may acquire new IP addresses only if there is a virtual IP that is not covered by any server in \( S' \). Since all the servers have the same view of the coverage table, they will all detect the same set of IP addresses that need to be covered. Furthermore, since they all have the same uniquely ordered list of the membership of \( S' \), the procedure `ReallocateIPs()` will guarantee that each unallocated virtual IP address will be covered by exactly one server in \( S' \). This concludes the proof of the lemma. 

**Lemma 2**: During the `RUN` state, every virtual IP address in the set \( R \) is covered by at least one server.

**Proof**:
According to the algorithm, after a view-change, if the connectivity remains stable, allowing the `GATHER` procedure to complete, all the connected servers will execute the `ReallocateIPs()` procedure. During this procedure, each server ensures that all the virtual IP addresses are covered. Since the `ReallocateIPs()` procedure is deterministic and all servers have the same view of the coverage table, they will collectively ensure that every virtual IP address is covered by at least one server. This completes the proof of the lemma.