title:N-Way Fail-Over Infrastructure for Reliable Servers and Routers
author:Yair Amir and
Ryan Caudy and
Ashima Munjal and
Theo Schlossnagle and
Ciprian Tutu
N-Way Fail-Over Infrastructure for Reliable Servers and Routers
Yair Amir
Ryan Caudy
Ashima Munjal
Theo Schlossnagle
Ciprian Tutu
Johns Hopkins University
{yairamir, wyvern, munjal, theos, ciprian}@cnds.jhu.edu
Computer Science Department
Abstract
Maintaining the availability of critical servers and
routers is an important concern for many organizations.
At the lowest level, IP addresses represent the global
namespace by which services are accessible on the In-
ternet.
We introduce Wackamole, a completely distributed
software solution based on a provably correct algorithm
that negotiates the assignment of IP addresses among the
currently available servers upon detection of faults. This
reallocation ensures that at any given time any public IP
address of the server cluster is covered exactly once, as
long as at least one physical server survives the network
fault. The same technique is extended to support highly
available routers.
The paper presents the design considerations, algo-
rithm speciﬁcation and correctness proof, discusses the
practical usage for server clusters and for routers, and
evaluates the performance of the system.
1
Introduction
Maintaining the availability of critical network servers
is an important concern for many organizations. Server
redundancy is the traditional approach to provide avail-
ability in the presence of failures. From the client per-
spective, a network-accessible service is resolved via a set
of public IP addresses speciﬁed for this service. There-
fore, the continued availability of a service via these IP
addresses is a prerequisite for providing uninterrupted
service to the client. In order to function correctly, each of
the service’s public IP addresses has to be covered by ex-
actly one physical server at any given time. If no physical
server covers a public IP address, the clients will not re-
ceive any service. On the other hand, if more than one
physical server is covering the same IP address at any
time, the network might not function properly and clients
may not be served correctly.
A sizable market exists for hardware solutions that
maintain the availability of IP addresses, usually via a
gateway that hides the actual servers behind a smart
switch or router in a centralized manner. We present
Wackamole, a high availability tool for clusters of servers.
Wackamole ensures that all the public IP addresses of a
service are available to its clients. Wackamole is a com-
pletely distributed software solution based on a provably
correct algorithm that negotiates the assignment of IP ad-
dresses among the available servers upon detection of
faults and recoveries, and provides N-way fail-over, so
that any one of a number of servers can cover for any
other.
Using a simple algorithm that utilizes strong group
communication semantics, Wackamole demonstrates the
application of group communication to address a critical
availability problem at the core of the system, even in the
presence of cascading network or server faults and recov-
eries. We also demonstrate how the same architecture is
extended to provide a similar service for highly-available
routers.
The remainder of this paper is organized as follows.
Section 2 introduces the system architecture. Section 3
describes the system model and the core algorithm behind
the engine of Wackamole and discusses its correctnes.
Section 5 analyzes practical considerations and presents
two applications for the system. Section 6 presents per-
formance results concerning the reconﬁguration time of
Wackamole clusters. We discuss related work in Section
7 and conclude in Section 8.
2 System Overview
Our solution has three main components, presented in
Figure 1:
• An IP address control (acquire and release) mecha-
nism.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:00:51 UTC from IEEE Xplore.  Restrictions apply. 
System 1
Wackamole
State
Sychronization
Algorithm
IP
Address
Manager
Group Communication
System
IP Addresses
xxx.yyy.221.151
xxx.yyy.221.152
xxx.yyy.221.155
System 2
Wackamole
IP Addresses
xxx.yyy.221.153
xxx.yyy.221.154
xxx.yyy.221.156
State
Sychronization
Algorithm
IP
Address
Manager
Group Communication
System
Network
Figure 1. Wackamole Architecture
• A state synchronization algorithm (the Wackamole
Algorithm).
• A membership service provided by a group commu-
nication toolkit.
The group communication toolkit maintains a mem-
bership service among the currently connected servers
and notiﬁes the synchronization algorithm of any view
changes that occur due to server crashes and recoveries,
or network partitions and remerges.
The synchronization algorithm manages the logical as-
signment of virtual IP addresses among the currently con-
nected members, avoiding conﬂicts that can occur upon
merges and recoveries and covering the “holes” that can
arise as a result of a crash or partition.
The IP address control mechanism enforces the deci-
sions of the synchronization algorithm by acquiring and
releasing the IP addresses accordingly. These mech-
anisms are highly speciﬁc to the operating system on
which the Wackamole system runs.
The correctness of the system is dependent on the as-
sumption that the group communication system provides
an accurate view of the current network connectivity. If
there is additional connectivity beyond that reported by
the group communication system, there may be conﬂicts
in the assignment of IP addresses. On the other hand, if
the group communication system does not detect the dis-
connection of a server from the current membership in a
timely manner, the IP addresses that were covered by that
server will be unavailable to the clients, since the system
will not reconﬁgure without the proper notiﬁcation.
3 The Wackamole Algorithm
In this section we present the state synchronization al-
gorithm that forms the core of the Wackamole system
and discuss its correctness, given the assumption that the
membership notiﬁcations issued by the group communi-
cation system reﬂect the actual network status.
3.1 System Model
In order to formally identify the problem that Wack-
amole attempts to solve, we deﬁne the system model and
introduce the correctness properties that the algorithm
and the implemented system need to maintain.
We consider a set S={s1, s2, ..., sm} of servers that
provide service to outside client applications. The servers
are all located in the same Local Area Network (LAN),
but are susceptible to crashes and temporary network par-
titions1 . During a network partition, the servers are sep-
arated into two or more components that are unable to
communicate with each other.
The client applications access the services through IP
addresses in the set I={i1, i2, ..., in}. The servers in S are
responsible for covering the set I of virtual IP addresses.
We refer to the IP addresses in I as virtual in order to
distinguish them from the stationary default IP addresses,
that do not change, used by the servers for intercommu-
nication.
The client applications are oblivious to the stationary
IP addresses of the servers in S or to the possible parti-
tioning that may exist among the servers.
In order to guarantee correct service, the following
properties need to be maintained.
Property 1 (Correctness) Every IP address in the set I is
covered exactly once by a server in each subset Sk, where
Sk is a maximal connected component whose servers are
in the operational (RUN) state.
Property 2 (Liveness) If there is a time t from which
a set of connected servers does not experience any
crashes/recoveries or network partitions/merges,
the
servers will switch to the operational (RUN) state.
In order to guarantee these properties we rely on the
group communication system to follow the Virtual Syn-
chrony properties [6, 14] in partitionable systems and to
provide Agreed message delivery. The Virtual Synchrony
property speciﬁes that any two servers that advance to-
gether from one membership to the next one, will deliver
an identical set of messages in the ﬁrst membership. The
Agreed delivery property guarantees that additionally, the
messages will be delivered in the same order at all servers.
Furthermore we assume that the group communication
system provides a membership service that provides each
server in the group a uniquely ordered list of the currently
connected participants.
3.2 Algorithm Speciﬁcation
The algorithm runs according to the state machine pre-
sented in Figure 2.
1Partitions can occur even in LAN environments due, for instance,
to a switch failure in one of the subnetworks.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:00:51 UTC from IEEE Xplore.  Restrictions apply. 
REALLOCATION COMPLETE
BALANCE TIMEOUT
VIEW_CHANGE
GATHER
RUN
BALANCE_MSG
BALANCE
VIEW_CHANGE
BALANCE COMPLETE
Figure 2. Wackamole Algorithm
Each server maintains a table current table that con-
tains the virtual IP allocation during the current member-
ship. During normal operation, the algorithm is in the
RUN state.
In this state, each server is responsible for
a set of virtual IP addresses and will answer all the re-
quests directed to those IP addresses. While in the RUN
state, the current table information is conﬂict-free and
the complete IP set is covered, maintaining the correct-
ness guarantees of the algorithm. When the group com-
munication system delivers a VIEW CHANGE event, a
backup of the IP table is created and a STATE MSG is
sent to every member of the new view containing the in-
formation about the IP addresses managed by the server
and the identiﬁer of the view in which it is initiated. The
algorithm then moves to the GATHER state.
Algorithm 1 RUN State
1: when: VIEW CHANGE do
2:
3:
4:
5: when: receive BALANCE MSG do
6:
old table = current table
send STATE MSG
state = GATHER
Change IPs()
In the GATHER state each server incorporates the in-
formation received through the STATE MSGs in its cur-
rent table variable and checks for the existence of con-
ﬂicts in the IP allocation; if members from previously
partitioned components are merged together, conﬂicts are
expected since each component covers the full IP address
set. ResolveConﬂicts() is a deterministic procedure in-
voked as soon as a new STATE MSG is received, that
checks whether the server that sent this message intro-
duces any conﬂict with respect to the information already
gathered about the set of covered IP addresses. If a con-
ﬂict is detected, the server drops the addresses that are
overlapping, thus restoring consistency at the network
level as soon as possible.
When all the state messages (STATE MSG) have been
received, each server invokes a deterministic procedure
Reallocate IPs(). During the Reallocate IPs() procedure,
the servers make sure that all the virtual IPs are covered
by a server in the current conﬁguration.
In particular,
the procedure relies on the uniquely ordered membership
list provided by the group communication system to dis-
tributely decide which server covers which IP address.
If the GATHER state is interrupted by a cascad-
ing VIEW CHANGE event, the server clears its cur-
rent table, discarding the information already collected
and reverting to the information in the old table, then
sends a new STATE MSG to all members of the new con-
ﬁguration.
Algorithm 2 GATHER State
1: when: receive STATE MSG with current view id do
2:
3:
4:
update current table
ResolveConﬂicts()
if
rent table.members) then
STATE MSG from all
(received
cur-
Reallocate IPs()
state = RUN
5:
6:
7: when: VIEW CHANGE do
8:
9:
10: when: BALANCE MSG do
11:
clear current table
send STATE MSG
ignore
3.3 Correctness of the Algorithm
We consider a subset of servers S’< S that are in the
operational RUN state. Between the servers in S’, each
virtual IP address is covered exactly once. We consider a
VIEW CHANGE event that is detected by members of
S’. According to the group communication guarantees,
all members of S’ will receive VIEW CHANGE notiﬁ-
cations, even though they are possibly disconnected from
each other. Following the algorithm, each server will pro-
ceed to the GATHER state and send a state message con-
taining its local knowledge base. Let’s consider a server
s that was part of S’ and is now part of S” as indicated
by the group communication. The group communication
system provides every s in S” an identically ordered list
of all the servers in S”.
Lemma 1 For every connected set S’ of servers in the
RUN state, every IP address is covered at most once by a
server in S’.
Proof:
We consider a set of servers that are connected after
a view change event, as indicated by the group commu-
nication notiﬁcation. In order for the servers to advance
to the RUN state, the state transfer algorithm, executed
in the GATHER state, needs to complete. Therefore we
consider the situation where a set of servers S’ does not
detect further VIEW CHANGE events until they exit the
GATHER state.
Let’s assume that upon receiving the last STATE mes-
sage in the GATHER state (line 4 in Algorithm 2) there
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:00:51 UTC from IEEE Xplore.  Restrictions apply. 
exists a virtual IP address vip that is covered by two
servers p and q in S’. According to the Virtual Synchrony
and Agreed delivery guarantees of the group communi-
cation, both p and q received all the state messages that
were sent during the GATHER state, therefore they re-
ceived their own state messages. According to the man-
agement of the current table variable from the algorithm
(line 2 Algorithm 1 and lines 2,8 Algorithm 2) and the fact
that only STATE MSGs generated in the current view are
considered (line 1 of Algorithm 2), the variable will ac-
curately reﬂect at this point the state of the currently con-
nected component. During this stage, following the algo-
rithm, the servers don’t acquire new IP addresses, there-
fore both p and q were already covering vip from their
previous memberships. From the algorithm, in the Re-
solve Conﬂicts() procedure (line 3 in Algorithm 2), when
p receives the state message from q, it will notice the con-
ﬂict in the coverage of vip and will adjust its IP coverage
table and release vip if p appears in the membership list of
S’ before q. The same reasoning applies for q; therefore
it is impossible for vip to be covered by both p and q at
this point. Furthermore, both p and q will have the same
view of the virtual ip coverage. Note that reaching agree-
ment does not assume any particular relation between the
initial states of p and q or of the other members of S’.
When all the state messages have been received each
server will execute the Reallocate IPs() procedure. Dur-
ing this procedure a server may acquire new IP addresses
only if there is a virtual IP that is not covered by any
server in S’. Since all the servers have the same view of
the coverage table, they will all detect the same set of
IP addresses that need to be covered. Furthermore, since
they all have the same uniquely ordered list of the mem-
bership of S’ the procedure Reallocate IPs() will guaran-
tee that each unallocated virtual IP address will be cov-
ered by exactly one server in S’. This concludes the proof
of the lemma.
2
Lemma 2 During the RUN state, every virtual IP ad-
dress in the set R is covered by at least one server.
Proof:
According to the algorithm, after a view-change, if the
connectivity remains stable allowing the GATHER pro-
cedure to complete, all the connected servers will exe-