I want to implement a transformer toolkit(a set of modules that will compose
some kind of DL model with attention ).I have said a toolkit because the
today's landscape of the transformer research is very diverse and I think that
it will be better to have the separate modules ( attention layer , concat
layer ,add& norm layer etc) and mix and match them rather to have one or two
ready layer that will be very general and not good for any specific task