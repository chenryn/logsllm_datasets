title:The impact of address allocation and routing on the structure and
implementation of routing tables
author:Harsha Narayan and
Ramesh Govindan and
George Varghese
The Impact of Address Allocation and Routing on the
Structure and Implementation of Routing Tables
Harsha Narayan
University of California, San
Diego
PI:EMAIL
Ramesh Govindan
University of Southern
California
PI:EMAIL
George Varghese
University of California, San
Diego
PI:EMAIL
ABSTRACT
The recent growth in the size of the routing table has led to an
interest in quantitatively understanding both the causes (e.g., multi-
homing) as well as the effects (e.g., impact on router lookup imple-
mentations) of such routing table growth. In this paper, we describe
a new model called ARAM that deﬁnes the structure of routing ta-
bles of any given size. Unlike simpler empirical models that work
backwards from effects (e.g., current preﬁx length distributions),
ARAM approximately models the causes of table growth (alloca-
tion by registries, assignment by ISPs, multihoming and load bal-
ancing). We show that ARAM models with high ﬁdelity three ab-
stract measures (preﬁx distribution, preﬁx depth, and number of
nodes in the tree) of the shape of the preﬁx tree — as validated
against 20 snapshots of backbone routing tables from 1997 to the
present. We then use ARAM for evaluating the scalability of IP
lookup schemes, and studying the effects of multihoming and load
balancing on their scaling behavior. Our results indicate that algo-
rithmic solutions based on multibit tries will provide more preﬁxes
per chip than TCAMs (as table sizes scale toward a million) unless
TCAMs can be engineered to use 8 transistors per cell. By con-
trast, many of today’s SRAM-based TCAMs use 14-16 transistors
per cell.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tions—routing
General Terms
Measurement, Performance
Keywords
Routing tables, modeling, IP lookups
1.
INTRODUCTION
In recent years, Internet measurement and modeling studies have
focused on Internet topologies [5], paths [6], and routing behav-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’03, August 25–29, 2003, Karlsruhe, Germany.
Copyright 2003 ACM 1-58113-735-4/03/0008 ...$5.00.
ior [7, 8]. Only recently has there been an exploration of the struc-
ture and growth of the global routing table. This exploration has
been sparked by dramatic growth in the table size (from 30,000 to
120,000) in the last few years. Growth in table size has become a
popular topic for mailing lists, and in the trade press [9] because of
its alarming implications for router vendors. However, the growth
in the table also leads to two natural research questions. Q1. How is
this growth caused and how will changes in the relative prevalence
of various causes affect table size? Q2. How does growth impact
router implementations?
It is fairly well known that some of the causes of growth are the
failure to aggregate properly, load balancing and multihoming. In
a recent measurement study, Bu et al. [10] provide valuable initial
insight into some of these causes by showing, for example, that
multihoming contributes 20-30% of the preﬁxes in current rout-
ing tables and that this factor is on the rise. However, they stop
short of exploring the sensitivity of table growth to changes in these
causes. Furthermore, while they provide some link between causes
and the total number of preﬁxes, they do not explore a link between
causes (e.g., multihoming) and the structure of the routing table.
The structure of the routing table can be thought of as the shape of
the tree (e.g., trie) induced by the set of preﬁxes in the table.
Why might the structure of the table1 be of interest — as opposed
to merely the size of the table? Table structure is crucial because
it helps to answer question Q2 about the impact on router imple-
mentations. In particular, some router vendors do IP lookups based
on compressed multibit tries [11, 12]. The amount of high speed
memory required by such schemes (which is a ﬁrst order measure
of the implementation cost) is directly dependent on the structure of
the routing table. Intuitively, if preﬁxes tend to bunch together (as
opposed to being randomly scattered), multibit tries will result in
very compact and affordable implementations even for large rout-
ing tables.
The issue is becoming particularly pressing because Content Ad-
dressable Memory (CAM) manufacturers are targeting offerings
for large core router forwarding tables. Traditional problems with
ternary CAMs (TCAMs) such as the scaling of the match logic and
power consumption are being surmounted using innovative tech-
niques. However, TCAMs still are much less dense than memories
(14-16 transistor cells for SRAM-based TCAMs as opposed to 6
transistors for an SRAM cell). Thus it appears that for the same
number of transistors, an algorithmic implementation (based, say,
on compressed multibit tries) can handle a larger number of pre-
ﬁxes than a TCAM.
While this is the hypothesis, and some are working to provide
1In this paper, we use the term routing table to mean the collection
of preﬁxes associated with routing entries, and ignore other route
attributes (AS paths, next hops, BGP communities etc.).
algorithmic solutions for IP lookups as an alternative to TCAM so-
lutions, there is little hard evidence to make a case one way or the
other. The problem is that if the present exponential growth rate
were to continue, one would expect core router tables to reach a
million preﬁxes in another 6 years or so. Routers shipping today
thus may have to support up to 1 million preﬁxes to be usable for 5
years, often the minimum period considered for such capital equip-
ment.
Thus to reasonably compare TCAMs versus algorithmic lookup
solutions one needs some way of generating realistic table sizes
that are several times the size of today’s routing tables. More im-
portantly, to accurately model the storage of algorithmic (e.g., trie-
based) solutions, one has to ensure that the generated tables accu-
rately reﬂect the structure of current (and hopefully future) routing
tables. The upshot of this argument is that router vendors today
need a good model of the structure of current tables that can be
projected to the future.
Note that an accurate model can also help a chip vendor selling
an algorithmic solution predict the amount of memory required to
support a given number of preﬁxes. A model that reﬂects underly-
ing causes is also useful in its own right to understand table growth,
which we claim should be as much a phenomenon of independent
interest as is Internet topology or BGP convergence.
1.1 Routing Table Models
lowing properties:
In this paper, we introduce a model of routing table structure
called ARAM. ARAM contains recognizable elements of the pro-
cesses which govern routing table structure: the allocation of ad-
dress space from registries to ISPs, and the advertisement of ad-
dress preﬁxes into the routing system determined by routing prac-
tices such as multi-homing and load balancing. We use ARAM to
study the storage requirement of IP lookup schemes as a function of
table size. Because ARAM has parameters that can control the rela-
tive weight of allocation and routing processes, we can explore the
impact of variations in the degree of multihoming or load balanc-
ing, for example, on the storage requirements of lookup schemes.
We argue that an ideal routing table model should have the fol-
• Causal: Given there are well known causes of routing ta-
ble growth such as multihoming and load balancing [10], a
model should ideally reﬂect these intuitive driving forces in
order to provide increased understanding. An alternative is
to employ an empirical model which ignores causes, and di-
rectly encodes chosen measures of current tables such as pre-
ﬁx length distribution.
• Parameterizable: Merely reﬂecting the current structure of
the routing table is dangerous because vast increases in some
causes (such as multihoming) can lead to very different rout-
ing tables. Thus, an ideal model should have parameters
(tuning knobs) that can control the effects of various param-
eters to enable a systematic sensitivity analysis.
• Parsimonious: To effectively use and reason with a model,
the model should have as few parameters as possible. In the
limit one could characterize the shape of the current rout-
ing table by providing the preﬁx length distribution for all
possible initial 8-bit values of preﬁx bits. While this might
characterize the shape better, it uses 256 * 32 parameters.
• Accurate: The model should match the “shape” of existing
Ideally, shape comparisons should use ab-
routing tables.
stract measures that can be used to compare any two preﬁx
tries, regardless of the particular lookup schemes.
• Predictive: The model should be able to accurately predict
the memory used by various lookup implementations as mea-
sured on existing tables. Unlike the previous goal where val-
idation is done using abstract shape measures, here the goal
is to validate a model using more concrete measures such as
the memory used by a representative IP lookup scheme.
The design of ARAM attempts to satisfy these sometimes con-
ﬂicting goals. It derives representative routing tables in three steps
that model the processes by which preﬁxes enter the table: ﬁrst
registries allocate address blocks to ISPs, ISPs advertise their al-
locations into the routing table and assign address space to cus-
tomers, and customer in turn can advertise more-speciﬁc preﬁxes
(i.e., “punch holes”) to effect specialized routing (e.g., backup or
load balancing). The model uses ﬁve parameters: one for the num-
ber of allocations in the ﬁrst step, and two each for the two remain-
ing steps that govern the frequency and extent of ISP and customer
routing practice. While the model can be made more accurate by
using more parameters, we decided to err in the direction of parsi-
mony.
By three abstract measures of tree shape (preﬁx length distribu-
tion, preﬁx depth, and number of tree nodes), ARAM’s routing ta-
bles compare well (Section 2.3.2) with twenty routing tables span-
ning the last ﬁve years. For example, for the preﬁx length distri-
bution, ARAM exhibits less than 6% error for each of those twenty
tables. To match a speciﬁc routing table, we used the same number
of allocations as an input to ARAM as had been handed out by the
registries at the time the routing table was taken. For the other four
parameters, a single set of values was sufﬁcient to produce a match.
Finally, for each of the twenty routing tables we chose, ARAM’s
matching routing tables also closely matched the number of tran-
sistors required for a compressed multibit trie implementation (Sec-
tion 3). Our scheme of choice is the Tree Bitmap algorithm, due to
Eatherton and Dittia [12]. It is less dated than the seminal Lulea
scheme [11] and has fast updates (unlike the Lulea scheme). How-
ever, we believe the results would not change signiﬁcantly for other
implementations because the underlying abstract property that de-
termines trie storage is the shape (i.e., the relationships between
preﬁxes) in the routing table.
We make no claim that ARAM captures all the aspects of rout-
ing and allocation practice, that it cannot be further tuned, or that
there are no other effective causal models. However, our valida-
tion of ARAM (Section 2.3.2) gives us some conﬁdence in our
use of ARAM to generate larger table sizes and explore variations
In particular, we use ARAM
in routing and allocation practice.
to shed some light on the somewhat acrimonious debate between
TCAMs and algorithmic IP lookup schemes. Leaving aside power
and match scaling, our results indicate that algorithmic schemes
can provide more density (preﬁxes per unit area) than TCAMs. Of
course, our results are subject to assumptions (e.g., that the cur-
rent growth trends continue), and thus should only be considered
an initial (albeit quantitative) contribution to an ongoing debate.
Section 2 is devoted to the ARAM model and its validation, and
Section 3 is devoted to using ARAM to predict IP lookup perfor-
mance. Finally, Section 4 compares ARAM to previous work, and
Section 5 states our conclusions.
2. ARAM: THE MODEL AND ITS VALIDA-
TION
In this section, we discuss current allocation and routing practice,
and then describe how ARAM generates routing table preﬁxes. We
then validate ARAM’s routing tables. As well, we validate each
individual aspect of the model.
2.1 Introduction
In order to allow meaningful extrapolation of routing tables, ARAM
attempts to explicitly model the factors that shape routing tables
and the relationship between preﬁxes. Broadly speaking, there are
two mechanisms that shape current routing tables: the allocation of
preﬁxes by registries, and the advertisements of those preﬁxes (or
more speciﬁcs thereof) in BGP tables by ISPs and their customers.
Both these mechanisms are only informally codiﬁed, if at all. For
this reason, we use the terms allocation practice and routing prac-
tice respectively to refer to them.
2.1.1 Address Allocation Practice
In this section, we describe the essential details of the hierar-
chical allocation of IPv4 addresses and address preﬁxes. ARAM
attempts to capture some, but not all of these details. There are cer-
tain local variations in allocation practice that ARAM ignores [4].
The IPv4 address allocation hierarchy has four levels. The Inter-
net Assigned Numbers Authority (IANA, currently administered by
the Internet Corporation for Assigned Names and Numbers (ICANN))
delegates blocks of addresses to Regional Internet Registries (RIRs).
These, in turn, allocate portions of their address blocks to Local In-
ternet Registries (LIRs). With few exceptions, LIRs correspond to
ISPs. In turn LIRs assign parts of their address space to “end-users”
(organizations or smaller ISPs).
This hierarchy for address allocation has two logical functions.
Decentralization of allocation from IANA to the RIRs and then to
the LIRs promotes manageability of address allocation. Moreover,
the fact that LIRs mostly correspond to ISPs promotes (at least in
theory) the scaling of the routing system by enabling aggregatable
address assignment.
We now describe the various levels of this hierarchy in some
detail.
IANA is the guardian of the entire IPv4 address space. It dele-
gates parts of the space to the RIRs in units of /8 on-demand. IANA
holds 113 /8s [13], of which 35 have currently been delegated to
various RIRs (the rest of the address space is made up of historical
allocations and the class B space). In order to qualify for a new /8,
an RIR has to have used up 80% of its existing /8 or demonstrate
that it cannot meet an allocation request with its current /82.
There are four RIRs (ARIN in North America, RIPE in Europe,
APNIC in Asia-Paciﬁc and LACNIC, the newest RIR responsible
for South America). Each RIR is responsible for address manage-
ment in its designated geographic region. RIRs codify their address
management practices in policy documents [14, 15, 16]. Although
there are minor regional variations in policy [4] the practices of the
various RIRs are qualitatively similar.
An RIR usually allocates address preﬁxes (address blocks aligned
on a bit boundary) to an LIR. Address preﬁxes handed out to the
LIR are called allocations. Though LIRs are usually ISPs, some-
times an RIR will allocate addresses directly to large end-users.
Such allocations from an RIR to an end-user go by various names:
”Direct Assignment”, ”Direct Allocation”, ”Provider Independent
Allocation” or ”Provider Independent Assignment”.
The smallest size of an initial allocation made by an RIR is /20.
Further allocations are made when the LIR has assigned 80% of its
previous allocations to customers. The size of subsequent alloca-
tions is determined by the usage rate of past allocations and the ex-
pected growth rate (ISPs have to make a business case to the RIRs
to demonstrate their expected growth rate). RIRs try to ensure, but
do not guarantee, that an allocation to an LIR is aggregatable with
2To reduce administrative load,
this policy is currently being
changed so that an RIR may get enough /8s to last for 18 months at
a burn rate that is determined by its recent rate of allocations.
prior allocations.
The lowest rung of the address allocation hierarchy is that be-
tween ISPs and their customers. Address blocks handed out by
ISPs to their customers are called assignments. The size of these
address blocks depends upon customer demand. It is difﬁcult to
gauge how ISPs manage their allocated space, but we believe that
most ISPs use a ﬁrst-ﬁt or best-ﬁt algorithm to make assignments [17,
18], regardless of whether the space assigned to an individual cus-
tomer is aggregatable. Our belief is based on the fact that two freely
available software packages for making assignments (FreeIPdb [19]
and NorthStar [20]) both use a best-ﬁt algorithm to choose address
blocks to make assignments.
2.1.2 Elements of Routing Practice
Allocation practice determines how, and to whom, address blocks
are assigned. Routing practice determines which of these address
blocks appears in the routing table, and in what form (either in its
entirety, as sub-blocks, or as more speciﬁcs of a block). ARAM at-
tempts to capture the dominant routing practices that we have been
able to infer from the data. It does not incorporate several arcane
routing practices [4].
The ideal, espoused by CIDR, is that each allocation to an ISP
has exactly one entry in the routing table, and no assignments ap-
pear in the routing table. That is, each customer advertises its as-
signment as a route in BGP (or perhaps using static routing or shar-
ing an IGP with the ISP, although these practices are probably less
prevalent now), but the ISP aggregates these routes, and advertises
to its peers or its upstream provider the address preﬁx representing
its allocated space. Reality is far from this.
Deviations from this ideal are caused by a variety of routing
practices. Most of these routing practices result from multihom-
ing. We use this term in a very general sense to include customers
connected to multiple upstream providers and ISPs buying transit
from multiple providers. In the following paragraphs, we describe
these routing practices and their effect on the routing table.
Some ISPs split their allocation, and advertise the split preﬁxes
separately in BGP. For example, the allocation made to UUNET,
63.64.0.0/10 is sometimes advertised as four /12s rather than
directly as a /10. There are two generic examples of ISPs that may
adopt this practice, both of which are attempts to engineer trafﬁc
ﬂows. A small ISP may split its allocation and advertise differ-
ent split preﬁxes to different upstream providers, thereby effecting
“load sharing”: i.e., spreading in-bound trafﬁc across different up-
stream providers. A large national ISP may have an internal level
of hierarchy in address allocation. That is, it may split its allocation
into address preﬁxes, one each for the different geographic regions
where it has infrastructure—customers from a particular region get
address space from the address preﬁx assigned to that region. The
ISP then advertises these split preﬁxes without aggregating them;
this allows it to have more ﬁne-grain control of routing. (There are
some other examples where an allocation appears to have been split
in the routing table [4]).