Phase III: Log simulation and policy re(cid:12)ne-
ment. The algorithm described above did not take
into account that (cid:12)le labels will change as the opera-
tions contained in the log (cid:12)le are performed. (If we did
not make the simplifying assumption that the labels
are static, then the analysis would become too complex
due to mutual dependencies between policy choices and
(cid:12)le labels.) To rectify this problem, we \simulate" the
accesses found in the log. We track the integrity lev-
els of objects and subjects during this simulation, and
report all accesses that cause a violation of the policy
generated in the previous step. The violation reports
are aggregated based on the subject (or object), and
are sorted in decreasing order of the number of occur-
rences, i.e., the report lists the subject (or object) with
the highest number violations (cid:12)rst. Subjects with high
con(cid:13)ict counts are suggestive of programs that may
need to be trusted, or untrusted programs that cannot
be used.
Based on the con(cid:13)ict report, the policy developer
may re(cid:12)ne the set of trusted, benign, or untrusted pro-
grams. If so, the analysis is redone. In general, more
than one iteration of re(cid:12)nement may be needed, al-
though in our experience, one iteration has proven to
be su(cid:14)cient.
Phase IV: Policy generation for new applica-
tions. New (cid:12)les get created in the system. In addi-
tion, new applications may become available over time.
In both cases, we cannot rely on any \logs" to generate
1These three conditions characterize the access by most ap-
plications to their preference (cid:12)les | the context in which the
redirect policy was motivated.
policies for them. Our approach is as follows.
For objects that are created after policy deployment,
their labels will be set to be the same as that of the
subject that created them. The default policy for such
newly created objects is that their labels can be down-
graded when they are written by lower integrity sub-
jects. In addition, accesses to these objects are logged.
The resulting log is analyzed periodically using the
same criteria described above to re(cid:12)ne the initial pol-
icy. For instance, if the object is used repeatedly by
high-integrity subjects, the policy would be set so that
writes by lower-integrity subjects are denied.
If a new software package is installed, labels for the
objects in the package are computed from the trust
level of the package, which must be speci(cid:12)ed at the
time of installation. The policies for these (cid:12)les are then
re(cid:12)ned over time, as the package is used by the user.
3.3. Soundness of Policies
Recall that the policies derived above are based on ac-
cesses observed on an unprotected system. Being un-
protected, it is possible for the log to have been com-
promised due to malicious untrusted code. Thus, an
important question is whether the soundness of the de-
rived policies is compromised due to the use of such
logs. An important feature of our policy generation
technique is that this does not happen. Thus, if the
generated policies are enforced on a newly installed sys-
tem, these policies will preserve its integrity.
Observation 2 As long as the programs identi(cid:12)ed as
trusted are indeed trustworthy, the policies generated
above will preserve system integrity even if the access
logs were compromised due to attacks.
Proof sketch: Recall that preserving system integrity
means that integrity-critical objects should never be
written by low-integrity subjects. Observe that all
integrity-critical objects are initialized to preserve-high
in the (cid:12)rst phase of the policy generation algorithm.
The propagation steps in this phase can add to the set
of objects marked preserve-high, but not remove any
objects. In the next phase, note that \downgrade ob-
ject" policy is applied only to those objects that aren’t
marked preserve-high. All other policy choices ensure
that object labels will not be downgraded. Thus, the
generated policy will ensure that the labels of integrity-
critical objects remain high.
Observe that if the logs were compromised, far too
many con(cid:13)icts may be reported during policy genera-
tion. Worse, because the compromised logs may not
re(cid:13)ect the behavior of programs on an uncompromised
system, the generated policies may cause many accesses
(not recorded in the log) to be denied, which can make
the system unusable. Both these symptoms are sug-
254
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:10:00 UTC from IEEE Xplore.  Restrictions apply. 
gestive of a compromised log (cid:12)le. The policy developer
needs to obtain a new, uncompromised log and rerun
the policy generation algorithm2.
The above observation indicates that the primary
If
weakness of PPI arises due to trusted programs.
they are incorrectly identi(cid:12)ed, or if they contain ex-
ploitable vulnerabilities, they can compromise end-user
security objectives. This factor motivates features and
techniques in the next section that limit and reduce the
scope of trust.
4. Limiting Trust
Unlimited and unrestricted trust is often the weakest
link in security, so we have incorporated features in
PPI to reduce the scope and nature of trust placed on
di(cid:11)erent programs. We describe these features below,
followed by a discussion of how these features are used
to address important applications such as software in-
stallers, browsers and email handlers, window systems,
and so on.
Invulnerable and Flow-Aware Applications.
All outputs of an invulnerable applications continue to
have high integrity even after reading low-integrity in-
puts. An example would be an ssh server that can
be trusted to protect itself from potentially malicious
network inputs, and maintain its high integrity.
Flow-aware applications can simultaneously handle
inputs with di(cid:11)erent integrity levels. They keep track
of which inputs a(cid:11)ect which outputs, and label the
outputs appropriately.
(Our enforcement framework
provides the primitives for (cid:13)ow-aware applications to
control the labels on their outputs.) Flow-awareness
is natural for some applications such as web-browsers
that already keep track of the associations between
their input actions and output actions. (Web browsers
use this type of information to enforce the \same ori-
gin policy [15].") Alternatively, automated techniques
such as runtime taint-tracking [25, 36, 27] may be used
to achieve (cid:13)ow-awareness.
A generic technique to mitigate the risk due to ex-
cessive trust is to deploy defenses against the most
common ways of exploiting applications using mali-
cious inputs, e.g., address-space randomization [5, 35]
or taint-tracking3 [25, 36, 27]. This technique can be
combined with a more speci(cid:12)c risk mitigation mecha-
nism described below that limits trust to certain con-
texts.
2Recall that end-users are not expected to generate policies,
so they won’t experience the security failures that result due to
compromised logs; and hence we don’t expect this possibility to
negatively impact end-user experience.
3Taint-tracking is preferable due to the weaknesses of ASR
against local attacks.
Context-aware Trust. A key observation is that
programs are rarely designed to accept untrusted in-
puts on every input channel. For instance, while an
ssh server may be robust against malicious data re-
ceived over the network, it cannot protect itself from
malicious con(cid:12)guration (cid:12)les, shared libraries or exe-
cutables. Our approach, therefore, is to limit trust to
the speci(cid:12)c input contexts in which an application is
believed to be capable of protecting itself. For an ssh
server, this may be captured by explicitly stating that
it is invulnerable to inputs received on port 22.
With respect to (cid:12)les, one approach for limiting trust
is to enumerate all the (cid:12)les read by an application in
advance, and identify those that can have low integrity.
This is far too cumbersome (or may not even be fea-
sible) since the number of (cid:12)les read by an application
may be very large (or unbounded). An alternative ap-
proach is to categorize a (cid:12)le as \data input" or \con-
trol input" (con(cid:12)guration or a library), and to permit
a trusted application to read low-integrity data inputs
but not control inputs. But manual identi(cid:12)cation of
data and control inputs would be cumbersome.
In-
stead, we rely on some of the properties of our policy
synthesis techniques to achieve roughly the same e(cid:11)ect.
Speci(cid:12)cally, note that con(cid:12)guration and library inputs
will be read during practically every run of an appli-
cation. As such, these (cid:12)les will be marked \preserve-
high" in phase I of the policy generation algorithm,
and hence the application will not be exposed to low-
integrity con(cid:12)guration or library (cid:12)les4.
4.1. Limiting Trust on Key Applications
Software Installers pose a particular challenge in
the context of integrity protection. Previous techniques
simply designated software installers as \trusted."
This designation is problematic in the context of con-
temporary package management systems, where pack-
ages may contain arbitrary installation scripts that
need to be run by the installer. During this run, they
may need to modify (cid:12)les in system directories, and
hence scripts cannot be run with low privileges.
We have developed a new approach to address this
software installation problem. In our approach, the in-
staller consists of two processes: a \worker" that runs
as a low-integrity subject (but may have root privi-
leges), and performs installation actions. To ensure
4This does not protect against the possibility that the appli-
cation may, in a subsequent run, read a di(cid:11)erent con(cid:12)guration
(cid:12)le. However, this is usually the result of running the appli-
cation with a command-line option or an environment variable
setting that causes it to read a di(cid:11)erent con(cid:12)guration/library
(cid:12)le. These \inputs" are provided by a parent process, and hence
are trusted, since the parent itself must have high-integrity in
order for a child process to have high integrity.
255
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:10:00 UTC from IEEE Xplore.  Restrictions apply. 
that this low-integrity subject can overwrite system
(cid:12)les if needed, a redirection policy is applied to all (cid:12)les
written by this subject. A second high integrity \su-
pervisor" subject runs after the (cid:12)rst one completes.
It veri(cid:12)es that the actions performed during installa-
tion were legitimate. In particular, it ensures that (a)
the modi(cid:12)cations made to the package management
database are exactly those that were made to the (cid:12)le
system, and (b) all the (cid:12)les installed are those that can
be overwritten by a low-integrity subject. If the veri(cid:12)-
cation succeeds, the supervisor renames the redirected
copies of (cid:12)les so that they replace their original ver-
sions. Otherwise, all the redirected copies are removed
and the installation aborted.
Web Browser and Email Handler. Web browser
and email client act as conduits for data received
from the network.
In our system, both web browser
and email handler are considered (cid:13)ow-aware applica-
tions. Speci(cid:12)cally, data received by a browser can be
deemed high or low integrity based on the source of
data and other factors such as the use of SSL. For the
Mozilla browser used in our experiments, we built a
small external program that uses the contents of a (cid:12)le
\downloads.rdf" to correlate network inputs with the
(cid:12)les written by the browser, and to label these (cid:12)les ac-
cordingly. We wrote a similar program for pine email
reader.
X-Server and Other Desktop Daemons. GUI-
based applications, called X-clients, need to access the
X-Server. To ensure continued operation of benign as
well as untrusted X-client applications, the X-Server
should be made invulnerable on input channels where
they accept data from untrusted clients. We mitigate
the risk due to this trust in two ways. First, X-server
is made invulnerable only on inputs received via sock-
ets used to connect to an untrusted client. Second, we
make use of the X security extension [33] to restrict
low-integrity applications so that they cannot perpe-
trate attacks on other windows that would otherwise
be possible.
Unfortunately, due to the design of the GNOME
desktop system, there are some servers (e.g., gconfd)
that are used by multiple X-clients and need to be
trusted in order to obtain a working system. We
are currently investigating techniques to limit trust on
these applications. Some of the recent results from the
SE-Linux project [31, 18] could be applicable in this
context.
File Utilities. Applications that can run at multi-
ple trust levels can sometimes introduce some usability
issues, speci(cid:12)cally, when they are used to operate on
input (cid:12)les with varying trust levels. We modi(cid:12)ed cp
Figure 2. PPI System Architecture
and mv to make them (cid:13)ow-aware, so that the output
(cid:12)les correctly inherit the label from the input (cid:12)les.
5. Enforcement Framework
a user-level
Our design is a hybrid system consisting of two com-
ponents:
library and a kernel-resident
checker. Integrity-critical enforcement actions are per-
formed by the kernel component, while \functional-
ity enhancement" features are relegated to the library.
For instance, the kernel component does not deal with
redirection policy. Moreover, while it supports the no-
tion of trusted subjects, it does not concern itself with
mechanisms for limiting trust, which are provided by
the user-level component. While the kernel enforce-
ment component is always trusted, the user-level com-
ponent is trusted only when it operates in the context
of a high-integrity process.
In our implementation, the kernel level component
is realized using LSM (Linux Security Modules) [34],
which has now become part of the standard Linux ker-
nel. We use the security hooks of LSM to enforce in-
formation (cid:13)ow policies. Although our policy frame-
work allows for policies to be a function of objects as
well as subjects, for simplicity, the policies enforced
by the kernel component have limited dependence on
subjects. (More (cid:13)exible subject-dependent policies can
be realized using the user-level component.) This en-
ables kernel-enforced policies to be stored with objects
using extended (cid:12)le attributes available on major Linux
(cid:12)le systems (including ext2, xfs, and reiserfs). Poli-
cies as well as integrity labels are stored using these
attributes. Speci(cid:12)cally, a 3-bit attribute integobj is
used to store the integrity level of a (cid:12)le. (For extensi-
bility, our implementation uses eight integrity levels.)
A 11-bit policy is associated with each (cid:12)le, consisting
of two parts. The (cid:12)rst part pertains to read and write
operations performed on the (cid:12)le:
(cid:15) down obj (3 bits) indicates the lowest integrity level
to which this object can be downgraded.
256
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:10:00 UTC from IEEE Xplore.  Restrictions apply. 
(cid:15) log obj (1 bit) indicates whether accesses to this ob-
ject should be logged. This feature could be used
for auditing. In our implementation, it provides the
mechanism for generating the logs used for policy
generation.
The second part of the policy pertains to the use of a
(cid:12)le to instantiate a subject, i.e., when the (cid:12)le is exe-
cuted. It consists of the following components:
(cid:15) down sub (3 bits) indicates the lowest integrity level
to which a process that executes this object can be
downgraded.
(cid:15) log sub (2 bits) indicates whether accesses made by
this subject should be logged. A second bit indicates
whether this policy should be inherited by descen-
dants of a subject.
(cid:15) invul sub (1 bit) indicates if this subject is invulnera-
ble. No distinction is made among various subclasses
of trusted applications described in Section 4 | it is
up to the user-level component to implement distinc-
tions such as (cid:13)ow-awareness and context-awareness.
(cid:15) super sub (1 bit) allows a subject to modify the labels
associated with objects in the system. Naturally, this
capability should be highly restricted. In our imple-
mentation, there is one administrative program that
has this capability.
When PPI system is initialized, the extended attributes
associated with all the (cid:12)les are populated using the la-
bels and policies generated using the techniques in Sec-
tion 3. New (cid:12)les inherit the integrity of the subject cre-
ating them. The log bits are set by default, super sub
and invul sub bits are cleared, and the down sub and
down obj bits are set to zero. (A lower integrity level
or a higher downgrade level may be requested by any
subject.)
After a fork, the child inherits the parent’s at-
tributes, including its integrity level. After an exec,
the integrity of the subject is reduced (but can never
be increased) to that of the (cid:12)le being executed. The su-
per sub policy is inherited from an executable (cid:12)le only