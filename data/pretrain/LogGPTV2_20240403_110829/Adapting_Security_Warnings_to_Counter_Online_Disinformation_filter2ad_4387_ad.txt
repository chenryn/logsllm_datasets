to behave (“ﬁnding alternative sources of information”).
Harm Warnings The harm warnings contained less text
and used forceful language, colors, and icons to suggest a
serious threat (see Figure 3b). The warnings used either a
skull-and-crossbones icon or a policeman icon, and the con-
tent was colored white against a red background. The most
extreme warning design simply said: “WARNING: This web-
site is dangerous.” The other three warning designs were titled
“Security Alert” and indicated in their messages that the threat
had to do with information quality.
4.2 Task Design
We aimed to keep procedures for the crowdworker study
as similar to the laboratory study as possible, but the different
setting and research questions necessitated three changes.
First, because crowdworkers participate remotely and use
their own computers, we could not easily measure their brows-
ing, insert warnings, or control search queries or results. In-
stead of using live Google searches, we developed a web
application to guide participants through the experiment and
realistically simulate a search engine (Figure 4). We popu-
lated results for each query from real Google results for the
query, including the snippet for each search result. In order to
simulate the story page after clicking a search result—and to
ensure that participants saw the same content—we used full-
page screenshots of story pages. Participants could browse
these screenshots similar to real webpages (Figure 5). Unlike
in the laboratory study, we speciﬁed the queries to use and
did not direct participants to speciﬁc sources.
Second, crowdworkers participated in our study to earn
a wage. This is a very different motivation than that of our
laboratory subjects. Crowdworkers may be more focused on
completing the task quickly (in order to earn the task fee)
than on engaging meaningfully with the study and behaving
as “good” research participants. One way we addressed the
risk was to ensure that only workers with track records of
submitting quality work participated in the study (see Sec-
tion 4.6). We also offered a bonus of $1 (43% of the base
fee) to participants who correctly answered all four search
questions. The bonus incentivized crowdworkers to engage
with the tasks, read instructions carefully, seek accurate infor-
mation, and take disinformation warnings seriously.
Finally, we used a series of surveys in lieu of directly ob-
serving participants and conducting interviews. Each partici-
pant completed a pre-task survey about their partisan align-
ment, surveys in each round about their behavior and percep-
tion of the warning, and a post-task demographic survey.9
Search Tasks As in the laboratory study, we selected facts
for participants to retrieve that were reported by multiple
sources and were obscure enough that participants would
likely be unfamiliar with the topic or sources. We also ensured
that all search results came from news outlets and that no
two results came from the same outlet, giving participants a
greater variety of choices for sources of information. All four
tasks pertained to events in the U.S. to make the topics more
relevant to our U.S.-based participant population. We also
designed the treatment tasks to cover political scandals, so
that participants would ﬁnd it plausible that news outlets might
publish disinformation about these topics. Table 3 presents
the queries for control and treatment rounds in the study.
Procedures After accepting our job on Amazon Mechani-
cal Turk, participants navigated to our study web application.
The landing page displayed instructions and a visual guide
for the study user interface, then directed participants to begin
the ﬁrst search round.
Each round consisted of a research task where the partici-
pant used our simulated search engine to ﬁnd a particular fact.
The participant began on a generic search query page, which
speciﬁed the fact to search for and the query to use (Figure 4).
9We provide survey details in supporting materials [100].
Table 2: We developed eight interstitial warning designs for the crowdworker study. Figure 3 shows sample designs.
Harm (white on red background)
Informativeness (black on white background)
h1
Skull
h2
Skull
h3
h4
i1
i2
i3
i4
Policeman
Policeman
Exclamation
Policeman
Policeman
Exclamation
WARNING Security Alert Security Alert Security Alert False or
Fake News Warning False or
ID
Icon
Title
Primary
message
This website
is dangerous.
This website
contains
misleading
or false
information.
This website
is dangerous.
This website
contains
misleading
or false
information.
Details None
None
None
Consider
ﬁnding
alternative
sources of
information.
Misleading
Content
Warning
This website
presents itself
as news, but it
contains
information
that experts
have identiﬁed
to be false or
misleading
This website spreads
disinformation: lies,
half-truths, and
non-rational
arguments intended
to manipulate public
opinion.
Misleading
Content
Warning
This website
contains
misleading
or false
information.
This website
contains
misleading
or false
information.
This website spreads
disinformation: lies,
half-truths, and
non-rational
arguments intended
to manipulate public
opinion.
Consider
ﬁnding
alternative
sources of
information.
Fake News Warning
This website
presents itself
as news, but it
contains
information
that experts
have identiﬁed
to be false or
misleading
Consider
ﬁnding
alternative
sources of
information.
It can be difﬁcult to
tell the difference
between real news
and disinformation,
but it poses a serious
threat to national
security, election
integrity, and
democracy.
It can be difﬁcult to
tell the difference
between real news
and disinformation,
but it poses a serious
threat to national
security, election
integrity, and
democracy.
(a) Informative warning design i2
(b) Harm warning design h3
Figure 3: Examples of warning designs in the crowdworker study. Table 2 describes all eight warning designs.
When the participant submitted the query, our study appli-
cation presented a search results page populated with eight
results. Clicking on a result led to a story page containing the
news article that the search result snippet described (Figure 5).
These story pages were full-size screenshots of real news arti-
cle webpages, allowing participants to scroll though and read
the articles as if they were browsing the real webpages.
Three of the results in each round were target results: the
search result snippets clearly pertained to the query topic,
and the story pages contained articles that clearly provided
the answer to the task question. The other ﬁve results were
nontarget results, which linked to story pages that did not
contain the answer. Some nontarget results could be readily
identiﬁed as irrelevant from the results page snippet (i.e., they
Table 3: The crowdworker study involved two control rounds and two treatment rounds. We report outcomes in Table 4.
Round
Control 1
Control 2
Participant Instructions
A train from the Massachusetts Bay Transportation Authority derailed in Boston on June 8.
Please ﬁnd the color of the line that the train was on using the query “MBTA derailment.”
In Alabama, the Muscogee tribe ﬁled a lawsuit over a casino that was built on sacred ground.
Please ﬁnd the name of the casino using the query “muscogee nation casino lawsuit.”
Treatment 1 Please ﬁnd the names of the two Clatskanie School Board members who were recalled in
May 2018 using the query “clatskanie school board recall.”
Mike Sanders, the Executive of Jackson County, Kansas, was sentenced to prison in 2018
for embezzlement. Please ﬁnd the name of the political party to which he belonged using
the query “mike sanders jackson county.”
Treatment 2
contained some search terms but in a different context), while
other nontarget results were germane to the query topic but
the story page did not contain the answer to the task question.
We ordered search results so that the target results would be
easy to ﬁnd: the top result was always a target result, and the
other two target results appeared randomly within the top ﬁve
results. The rest of the results appeared in random order.
On each story page, an instruction box repeated the ques-
tion and allowed submitting an answer or returning to the
search results. If the participant returned to the results, each
result they previously clicked was grayed out and disabled.
In treatment rounds, the participant saw an interstitial warn-
ing the ﬁrst time they clicked a target result. Our multi-armed
bandit algorithm (Section 4.5) selected the warning design
that the participant encountered. All warnings included two
buttons: “Go back” or “Dismiss and continue.” If the user
clicked “Go back” or the browser back button, they returned
to the search results. The user did not encounter a warning
when they clicked on any other result. If the user clicked “Dis-
miss and continue,” they were taken to the story page, where
they could submit an answer or return to the search results.
When the participant submitted an answer, they were pre-
sented with a survey about why they chose particular search
results. This survey was a misdirection to maintain the false
premise that the experiment was studying search engine us-
age. In control rounds, submitting the survey led to the next
round. In treatment rounds, the next page was a second survey,
designed to capture whether the participant comprehended the
purpose of the warning and whether the participant perceived
a risk of harm (see Section 4.3). This survey also included an
attention check question so that we could discard responses
from participants who did not carefully read the instructions.
After completing all four rounds, participants were navi-
gated to a ﬁnal demographic survey, then compensated.
4.3 Measuring Participant Perceptions
Informativeness We designed three survey questions to
measure whether participants comprehended the purpose of a
warning. Recall that in our laboratory study, participants who
misunderstood a warning typically believed the warning was
related to malware or another security threat. We grounded
our informativeness questions in this observation, asking par-
ticipants to indicate whether the warning was about three
topics: malware (incorrect), information theft (incorrect), and
disinformation (correct). The survey presented the following
statements to participants and asked about agreement on a
5-point Likert scale (“Strongly disagree” to “Strongly agree”).
• in f o1: The warning said that this site is trying to install
malware.
my information.
• in f o2: The warning said that this site is trying to steal
• in f o3: The warning said that this site contains false or
misleading information.
We used the survey responses to compute an informative-
ness score ip,w in the range [−2,2], which captured partici-
pant p’s certainty that warning w was about disinformation.
ip,w = 2 if p “strongly agreed” that w was about false or
misleading information and “strongly disagreed” that w was
about malware and stealing information. For each point de-
viation from these “correct” responses on the Likert scales,
we reduced ip,w by 1, resulting in a lower score when the
participant was uncertain in their answer or had an incorrect
understanding of the warning. The scoring formula was:
ip,w = max(−2, in f o3 − in f o2 − in f o1 − 1)