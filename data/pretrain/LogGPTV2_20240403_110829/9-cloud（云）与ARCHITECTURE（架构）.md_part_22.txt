\"@timestamp\" =\> \"2018-09-15T08:40:57.000Z\",
\"type\" =\> \"syslog\",
\"host\" =\> \"192.168.1.65\",
\"priority\" =\> 86,
\"timestamp\" =\> \"Sep 15 16:40:57\",
\"logsource\" =\> \"se5\",
\"program\" =\> \"sshd\",
\"pid\" =\> \"26133\",
\"severity\" =\> 6,
\"facility\" =\> 10,
\"facility_label\" =\> \"security/authorization\",
\"severity_label\" =\> \"Informational\"
7）filter grok插件
grok插件：
解析各种非结构化的日志数据插件
grok使用正则表达式把飞结构化的数据结构化
在分组匹配，正则表达式需要根据具体数据结构编写
虽然编写困难，但适用性极广
\[root@logstash \~\]# vim /etc/logstash/logstash.conf
input{
stdin{ codec =\> \"json\" }
file {
path =\> \[ \"/tmp/a.log\", \"/var/tmp/b.log\" \]
sincedb_path =\> \"/var/lib/logstash/sincedb\"
start_position =\> \"beginning\"
type =\> \"testlog\"
}
tcp {
host =\> \"0.0.0.0\"
port =\> \"8888\"
type =\> \"tcplog\"
}
udp {
host =\> \"0.0.0.0\"
port =\> \"9999\"
type =\> \"udplog\"
}
syslog {
port =\> \"514\"
type =\> \"syslog\"
}
}
filter{
grok{
match =\> \[\"message\", \"(?\reg)\"\]
}
}
output{
stdout{
codec =\> \"rubydebug\"
}
}
\[root@se5 \~\]# yum -y install httpd
\[root@se5 \~\]# systemctl restart httpd
\[root@se5 \~\]# vim /var/log/httpd/access_log
192.168.1.254 - - \[15/Sep/2018:18:25:46 +0800\] \"GET / HTTP/1.1\" 403
4897 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101
Firefox/52.0\"
复制/var/log/httpd/access_log的日志到logstash下的/tmp/a.log
\[root@logstash \~\]# vim /tmp/a.log
192.168.1.254 - - \[15/Sep/2018:18:25:46 +0800\] \"GET / HTTP/1.1\" 403
4897 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101
Firefox/52.0\"
\[root@logstash \~\]# /opt/logstash/bin/logstash -f
/etc/logstash/logstash.conf
//出现message的日志，但是没有解析是什么意思
Settings: Default pipeline workers: 2
Pipeline main started
{
\"message\" =\> \".168.1.254 - - \[15/Sep/2018:18:25:46 +0800\] \\\"GET
/ HTTP/1.1\\\" 403 4897 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 6.1;
WOW64; rv:52.0) Gecko/20100101 Firefox/52.0\\\"\",
\"@version\" =\> \"1\",
\"@timestamp\" =\> \"2018-09-15T10:26:51.335Z\",
\"path\" =\> \"/tmp/a.log\",
\"host\" =\> \"logstash\",
\"type\" =\> \"testlog\",
\"tags\" =\> \[
\[0\] \"\_grokparsefailure\"
\]
}
若要解决没有解析的问题，同样的方法把日志复制到/tmp/a.log，logstash.conf配置文件里面修改grok
查找正则宏路径
\[root@logstash \~\]# cd /opt/logstash/vendor/bundle/ \\
jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/
\[root@logstash \~\]# vim grok-patterns //查找COMBINEDAPACHELOG
COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
\[root@logstash \~\]# vim /etc/logstash/logstash.conf
\...
filter{
grok{
match =\> \[\"message\", \"%{COMBINEDAPACHELOG}\"\]
}
}
\...
解析出的结果
\[root@logstash \~\]# /opt/logstash/bin/logstash -f
/etc/logstash/logstash.conf
Settings: Default pipeline workers: 2
Pipeline main started
{
\"message\" =\> \"192.168.1.254 - - \[15/Sep/2018:18:25:46 +0800\]
\\\"GET /noindex/css/open-sans.css HTTP/1.1\\\" 200 5081
\\\"http://192.168.1.65/\\\" \\\"Mozilla/5.0 (Windows NT 6.1; WOW64;
rv:52.0) Gecko/20100101 Firefox/52.0\\\"\",
\"@version\" =\> \"1\",
\"@timestamp\" =\> \"2018-09-15T10:55:57.743Z\",
\"path\" =\> \"/tmp/a.log\",
\"host\" =\> \"logstash\",
\"type\" =\> \"testlog\",
\"clientip\" =\> \"192.168.1.254\",
\"ident\" =\> \"-\",
\"auth\" =\> \"-\",
\"timestamp\" =\> \"15/Sep/2018:18:25:46 +0800\",
\"verb\" =\> \"GET\",
\"request\" =\> \"/noindex/css/open-sans.css\",
\"httpversion\" =\> \"1.1\",
\"response\" =\> \"200\",
\"bytes\" =\> \"5081\",
\"referrer\" =\> \"\\\"http://192.168.1.65/\\\"\",
\"agent\" =\> \"\\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0)
Gecko/20100101 Firefox/52.0\\\"\"
}
步骤二：安装Apache服务，用filebeat收集Apache服务器的日志，存入elasticsearch
1）在之前安装了Apache的主机上面安装filebeat
\[root@se5 \~\]# yum -y install filebeat
\[root@se5 \~\]# vim/etc/filebeat/filebeat.yml
paths:
\- /var/log/httpd/access_log //日志的路径，短横线加空格代表yml格式
document_type: apachelog //文档类型
elasticsearch: //加上注释
hosts: \[\"localhost:9200\"\] //加上注释
logstash: //去掉注释
hosts: \[\"192.168.1.67:5044\"\] //去掉注释,logstash那台主机的ip
\[root@se5 \~\]# systemctl start filebeat
\[root@logstash \~\]# vim /etc/logstash/logstash.conf
input{
stdin{ codec =\> \"json\" }
beats{
port =\> 5044
}
file {
path =\> \[ \"/tmp/a.log\", \"/var/tmp/b.log\" \]
sincedb_path =\> \"/dev/null\"
start_position =\> \"beginning\"
type =\> \"testlog\"
}
tcp {
host =\> \"0.0.0.0\"
port =\> \"8888\"
type =\> \"tcplog\"
}
udp {
host =\> \"0.0.0.0\"
port =\> \"9999\"
type =\> \"udplog\"
}
syslog {
port =\> \"514\"
type =\> \"syslog\"
}
}
filter{
if \[type\] == \"apachelog\"{
grok{
match =\> \[\"message\", \"%{COMBINEDAPACHELOG}\"\]
}}
}
output{
stdout{ codec =\> \"rubydebug\" }
if \[type\] == \"filelog\"{
elasticsearch {
hosts =\> \[\"192.168.1.61:9200\", \"192.168.1.62:9200\"\]
index =\> \"filelog\"
flush_size =\> 2000
idle_flush_time =\> 10
}}
}
\[root@logstash logstash\]# /opt/logstash/bin/logstash \\
-f /etc/logstash/logstash.conf
打开另一终端查看5044是否成功启动
\[root@logstash \~\]# netstat -antup \| grep 5044
tcp6 0 0 :::5044 :::\* LISTEN 23776/java
\[root@se5 \~\]# firefox 192.168.1.65 //ip为安装filebeat的那台机器
回到原来的终端，有数据
2）修改logstash.conf文件
\[root@logstash logstash\]# vim logstash.conf
\...
output{
stdout{ codec =\> \"rubydebug\" }
if \[type\] == \"apachelog\"{
elasticsearch {
hosts =\> \[\"192.168.1.61:9200\", \"192.168.1.62:9200\"\]
index =\> \"apachelog\"
flush_size =\> 2000
idle_flush_time =\> 10
}}
}
浏览器访问Elasticsearch，有apachelog，如图-16所示：
![image015](media/image156.png){width="6.886805555555555in"
height="2.2125in"}
# \-\-\--NSD ARCHITECTURE DAY05 and DAY06\-\-\-\-\-\-\-\-\--
# 大数据
![LINUXNSD_V01ARCHITECTUREDAY05_006](media/image157.png){width="7.264583333333333in"
height="3.8402777777777777in"}
![LINUXNSD_V01ARCHITECTUREDAY05_009](media/image158.png){width="7.264583333333333in"
height="4.950694444444444in"}
# Hadoop
![LINUXNSD_V01ARCHITECTUREDAY05_010](media/image159.png){width="7.264583333333333in"
height="4.152083333333334in"}
# Hadoop三大核心组件
![LINUXNSD_V01ARCHITECTUREDAY05_017](media/image160.png){width="6.915972222222222in"
height="4.747222222222222in"}
![LINUXNSD_V01ARCHITECTUREDAY05_019](media/image161.png){width="7.164583333333334in"
height="5.413888888888889in"}
## HDFS结构
![LINUXNSD_V01ARCHITECTUREDAY05_020](media/image162.png){width="6.455555555555556in"
height="4.841666666666667in"}
Hadoop实现了一个分布式文件系统（Hadoop Distributed File
System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high
throughput）来访问应用程序的数据，适合那些有着超大数据集（large data
set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming
access）文件系统中的数据。
HDFS是hadoop体系中数据存储管理的基础,是一个高度容错的的系统,用于在低成本的通用硬件上运行
**Client:**将数据切块每块128M,与NameNode交互获取数据块文件位置,经过NameNode指定的位置,与
DataNode交互,并存入指定的某个位置
**NameNode :**是一个通常在 HDFS
实例中的单独机器上运行的软件。它负责管理文件系统名称空间和控制外部
客户机的访问。NameNode 决定是否将文件映射到 DataNode 上的复制块上。
**fsimage:**数据块映射信息,NameNode在一个称为 Fsimage
的文件中存储所有关于文件系统名称空间的信息。
**fsedits:**包含所有事务的记录文件（这里是 EditLog）,数据变更日志
**Secondary NameNode :**定期合并Fsimage和fsedits文件,推送给NameNode,
> 紧急情况下,可辅助恢复NameNode
>
> Secondary NameNode 并非 NameNode的热备
**Datanode:**实际数据存储节点,汇报存储信息给NameNode
**文件操作:**
可见，HDFS
并不是一个万能的文件系统。它的主要目的是支持以流的形式访问写入的大型文件。
如果客户机想将文件写到 HDFS
上，首先需要将该文件缓存到本地的临时存储。如果缓存的数据大于所需的 HDFS
块大小，创建文件的请求将发送给 NameNode。NameNode 将以 DataNode
标识和目标块响应客户机。
同时也通知将要保存文件块副本的
DataNode。当客户机开始将临时文件发送给第一个 DataNode
时，将立即通过管道方式将块内容转发给副本
DataNode。客户机也负责创建保存在相同
HDFS名称空间中的校验和（checksum）文件。
在最后的文件块发送之后，NameNode
将文件创建提交到它的持久化元数据存储（在 EditLog 和 FsImage 文件）。
## MapReduce结构
![LINUXNSD_V01ARCHITECTUREDAY05_025](media/image163.png){width="7.264583333333333in"
height="4.979861111111111in"}
MapReduce是处理大量半结构化数据集合的编程模型。编程模型是一种处理并结构化特定问题的方式。
最简单的 MapReduce应用程序至少包含 3 个部分：一个 Map 函数、一个 Reduce
函数和一个 main 函数。main
函数将作业控制和文件输入/输出结合起来。在这点上，Hadoop
提供了大量的接口和抽象类，从而为
Hadoop应用程序开发人员提供许多工具，可用于调试和性能度量等。
MapReduce 本身就是用于并行处理大数据集的软件框架。MapReduce
的根源是函数性编程中的 map 和 reduce
函数。它由两个可能包含有许多实例（许多 Map 和 Reduce）的操作组成。Map
函数接受一组数据并将其转换为一个键/值对列表，输入域中的每个元素对应一个键/值对。Reduce
函数接受 Map
函数生成的列表，然后根据它们的键（为每个键生成一个键/值对）缩小键/值对列表。
MapReduce和Hadoop是相互独立的，实际上又能相互配合工作得很好。源自于Google的MapReduce论文,JAVA实现分布式计算框架
tracker:跟踪器 task:任务
**JobTracker:**MapReduce的应用程序
Master节点,只有一台管理所有作业/任务的监控,错误处理等,将任务分解成一些列任务,并分派给TaskTracker
**TaskTracker:**
slave节点,一般多台,运行Map Task和Reduce
Task,并与JobTracker交互,汇报任务状态
**Map Task:**
解析每条数据记录.传递给用户编写的map()并执行,将输出结果写入本地磁盘,
如果为map-only作业,直接写入HDFS
**Reducer Task:**
从Map
Task的执行结果中,远程读取输入数据,对数据进行排序,将数据按照分组传递给用户编写的reduce函数执行
## Yarn结构
# Hadoop单机安装与配置
单机版
\[root@nn01 \~\]# yum -y install java-1.8.0-openjdk-devel
\[root@nn01 \~\]# ls
eip Hadoop.zip
\[root@nn01 \~\]# unzip Hadoop.zip
Archive: Hadoop.zip
inflating: hadoop/hadoop-2.7.6.tar.gz
extracting: hadoop/kafka_2.10-0.10.2.1.tgz
inflating: hadoop/zookeeper-3.4.10.tar.gz
\[root@nn01 \~\]# ls
eip hadoop Hadoop.zip
\[root@nn01 \~\]# cd hadoop/
\[root@nn01 hadoop\]# ls
hadoop-2.7.6.tar.gz kafka_2.10-0.10.2.1.tgz zookeeper-3.4.10.tar.gz
\[root@nn01 hadoop\]# tar -xf hadoop-2.7.6.tar.gz
\[root@nn01 hadoop\]# ls
hadoop-2.7.6 kafka_2.10-0.10.2.1.tgz
hadoop-2.7.6.tar.gz zookeeper-3.4.10.tar.gz