## 创建集群
有了 Kops，我们可以部署任何规模的集群。出于本指南的目的，我们将通过让工作节点和主节点跨越三个可用性区域来部署生产就绪型集群。我们将使用美国东部 1 区，主人和工人都将是`t2.medium`实例。
要为此集群创建配置，您可以运行以下`kops create`命令:
Kops-create-cluster.sh
```
kops create cluster \
    --node-count 3 \
    --zones us-east-1a,us-east-1b,us-east-1c \
    --master-zones us-east-1a,us-east-1b,us-east-1c \
    --node-size t2.medium \
    --master-size t2.medium \
    ${NAME}
```
要查看已创建的配置，请使用以下命令:
```
kops edit cluster ${NAME}
```
最后，要创建我们的集群，运行以下命令:
```
kops update cluster ${NAME} --yes
```
集群创建过程可能需要一些时间，但是一旦完成，您的`kubeconfig`应该被正确配置为在您的新集群中使用 kubectl。
# 完全从头开始创建集群
完全从头开始创建 Kubernetes 集群是一个多步骤的努力，可能会跨越本书的多个章节。但是，由于我们的目的是让您尽快启动并运行 Kubernetes，因此我们将避免描述整个过程。
如果你对从头开始创建集群感兴趣，无论是出于教育原因还是需要精细定制集群，一个很好的指南是 *Kubernetes The Hard Way* ，这是一个由*凯尔西·海托华*编写的完整集群创建教程。可以在[https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)找到。
现在我们已经解决了这个问题，我们可以继续手动集群创建过程的概述。
## 配置您的节点
首先，您需要一些基础设施来运行 Kubernetes。一般来说，虚拟机是一个很好的选择，尽管 Kubernetes 也可以在裸机上运行。如果您在一个无法轻松添加节点的环境中工作(这消除了云的许多扩展优势，但在企业环境中绝对是可能的)，您将需要足够的节点来满足您的应用需求。这更可能是空气间隙环境中的一个问题。
您的一些节点将用于主控制平面，而其他节点将仅用作工作人员。从内存或中央处理器的角度来看，没有必要使主节点和工作节点完全相同——您甚至可以拥有一些更弱和更强大的工作节点。这种模式导致非同构集群，其中某些节点更适合特定的工作负载。
## 为 TLS 创建 Kubernetes 证书颁发机构
为了使正常运行，所有主要的控制平面组件都需要 TLS 证书。要创建这些证书，需要创建一个**证书颁发机构**(**CA**)，该机构将依次创建顶级域名证书。
要创建证书颁发机构，需要启动公钥基础设施 ( **公钥基础设施**)。对于这个任务，您可以使用任何 PKI 工具，但是在 Kubernetes 文档中使用的工具是 cfssl。
一旦为所有组件创建了 PKI、CA 和 TLS 证书，下一步就是为控制平面和工作节点组件创建配置文件。
## 创建配置文件
需要为`kubelet`、`kube-proxy`、`kube-controller-manager`和`kube-scheduler`组件创建配置文件。他们将使用这些配置文件中的证书向`kube-apiserver`进行认证。
## 创建 etcd 集群并配置加密
创建数据加密配置是通过具有数据加密机密的 YAML 文件来处理的。此时，需要启动`etcd`集群。
为此，在每个节点上使用`etcd`流程配置创建`systemd`文件。然后在每个节点上使用`systemctl`来启动`etcd`服务器。
这是`etcd`的样本`systemd`文件。其他控制平面组件的`systemd`文件与此类似:
示例系统控制平面
```
[Unit]
Description=etcd
Documentation=https://github.com/coreos
[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
```
这个服务文件为我们的`etcd`组件提供了一个运行时定义，它将在每个主节点上启动。为了在我们的节点上实际启动`etcd`，我们运行以下命令:
```
{
  sudo systemctl daemon-reload
  sudo systemctl enable etcd
  sudo systemctl start etcd
}
```
这将启用`etcd`服务，并在节点重启时自动重启。
## 引导控制平面组件
在主节点上引导控制平面组件类似于创建`etcd`集群的过程。`systemd`为每个组件——应用编程接口服务器、控制器管理器和调度器——创建文件，然后使用`systemctl`命令启动每个组件。
之前创建的配置文件和证书也需要包含在每个主节点上。
让我们看一下`kube-apiserver`组件的服务文件定义，分为以下几个部分。`Unit`部分只是对我们的`systemd`文件的简要描述:
```
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
```
API-服务器-系统-示例
第二部分是服务的实际启动命令，以及要传递给服务的任何变量:
```
[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.10.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
```
最后，`Install`部分允许我们指定一个`WantedBy`目标:
```
Restart=on-failure
RestartSec=5
 [Install]
WantedBy=multi-user.target
```
`kube-scheduler`和`kube-controller-manager`的服务文件将非常类似于`kube-apiserver`的定义，一旦我们准备好启动节点上的组件，过程就很容易了:
```
{
  sudo systemctl daemon-reload
  sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
  sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
}
```
类似于`etcd`，我们希望确保服务在节点关闭时重启。
## 引导工作节点
在工人节点上也有类似的故事。需要使用`systemctl`创建和运行`kubelet`、容器运行时、`cni`和`kube-proxy`的服务规范。`kubelet`配置将指定前述的顶级域名证书，以便它可以通过应用编程接口服务器与控制平面通信。
让我们看看我们的`kubelet`服务定义是什么样子的:
忽必烈系统实例
```
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service
[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
```
可以看到，这个服务定义引用了`cni`、容器运行时和`kubelet-config`文件。`kubelet-config`文件包含我们需要的员工的顶级域名信息。
引导工人和主节点后，通过使用作为 TLS 设置的一部分创建的管理`kubeconfig`文件，集群应该可以运行。
# 总结
在本章中，我们回顾了创建 Kubernetes 集群的几种方法。我们研究了使用 minikube 创建最少的本地集群，在 Azure、AWS 和 Google Cloud 上的托管 Kubernetes 服务上设置集群，使用 Kops 资源调配工具创建集群，最后从零开始手动创建集群。
现在我们已经掌握了在几个不同的环境中创建 Kubernetes 集群的技能，我们可以继续使用 Kubernetes 来运行应用。
在下一章中，我们将学习如何在 Kubernetes 上开始运行应用。您所获得的关于 Kubernetes 如何在架构级别工作的知识应该会让您更容易理解接下来几章中的概念。
# 问题
1.  minikube 有什么用？
2.  使用托管 Kubernetes 服务有哪些缺点？
3.  Kops 和 Kubeadm 相比如何？主要区别是什么？
4.  Kops 支持哪些平台？
5.  手动创建集群时，如何指定主要的集群组件？它们是如何在每个节点上运行的？
# 进一步阅读
*   Kubernetes 官方文件:[https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)
*   *硬道*:[https://github . com/keleyhightower/kubrites—硬道](https://github.com/kelseyhightower/kubernetes-the-hard-way)