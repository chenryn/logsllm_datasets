in preserving privacy and integrity of requests. The server may
deviate from the protocol, in an attempt to learn about the
client’s requests or to tamper with the result. Our system’s
threat surface is broken into several parts:
1) Security of memory: First, the memory accesses made
by the SGX enclave to any memory outside the enclave. These
are completely exposed to the server and must preserve privacy
and integrity of the underlying data. These accesses inherit the
security of the underlying memory protection (e.g., ORAM),
which we detail in Section III-C.
3
2) Security of enclave execution: Second, the SGX enclave’s
execution as it is orchestrating accesses to external memory. At
a high level, SGX only provides privacy/integrity guarantees
for enclave virtual memory. Running ORAM controller code in
an enclave does not, by itself, ensure obliviousness. External
server software (which shares the hardware with the enclave)
can still monitor any interactions the enclave makes with the
outside world (e.g., syscalls, etc.), how the enclave uses shared
processor resources such as cache [6], [35] and how/when the
enclave suffers page faults [53]. Our system has mechanisms to
preserve privacy and integrity despite the above vulnerabilities.
We formalize this security guarantee in Section III-A and map
SGX to these deﬁnitions in Section III-B.
3) Security across enclave termination: Third, recovery and
security given enclave termination. An important caveat of SGX
is that the OS can terminate enclave execution at any time.
This has been shown to create avenues for replay attacks [25],
and risks irreversible data-loss. We develop novel protocols in
Section V to make the ORAM+enclave system fault tolerant
and secure against arbitrary enclave terminations.
4) Security non-goals: We do not defend against hardware
attacks (e.g., power analysis [20] or EM emissions [36]),
compromised manufacturing (e.g., hardware trojans [54]) or
denial of service attacks.
III. PRELIMINARIES
A. Oblivious Enclave Execution
We now formalize oblivious execution for enclaves that
we set out to achieve in our system. We ﬁrst give a general
deﬁnition for enclave-based trusted execution, that deﬁnes the
client API, security guarantees, and where privacy leakages can
occur. In the next section, we describe exactly what privacy
and integrity threats are present in Intel SGX in particular, and
the challenges in protecting them.
To help us formalize the deﬁnition, we deﬁne a pair of
algorithms Load and Execute, that are required by a client to
load a program into an enclave, and execute it with a given
input.
1) Load(P) → (EP, φ): The load function takes a program
P, and produces an enclave EP, loaded with P along with a
proof φ, which the client can use to verify that the enclave did
load the program P.
2) Execute(EP, in) → (out, ψ): The execute function, given
an enclave loaded with a program P, feeds the enclave with
an input in, to produce a tuple constituting of the output out,
and proof ψ which the client can use to verify that the output
out was produced by the enclave EP executing with input in.
Execution also produces trace(EP,in), which captures the
execution trace induced by running the enclave EP with the
input in which is visible to the server. This trace(EP,in) contains
all the powerful side channel artifacts that the adversarial server
can view, such as cache usage, etc. These are discussed in detail
in the case of Intel SGX in Section III-B5, below.
3) Security: When a program P is loaded in an enclave,
and a set of inputs −→y
:= (inM, ..., in1) are executed by
it results in an adversarial view V(−→y ) :=
this enclave,
(trace(EP,inM), ..., trace(EP,in1)). We say that an enclave exe-
cution is oblivious, if given two sets of inputs −→y and −→z ,
their adversarial views V(−→y ) and V(−→z ) are computationally
indistinguishable to anyone but the client.
B. Intel SGX
In this section we give a brief introduction to Intel Software
Guard Extensions (SGX) and highlight aspects relevant to
ZeroTrace. (See [1], [9] for more details on SGX.) Intel SGX is
a set of new x86 instructions that enable code isolation within
virtual containers called enclaves. In the SGX architecture,
developers are responsible for partitioning the application into
enclave code and untrusted code, and to deﬁne an appropriate
IO communication interface between them. In SGX, security
is bootstrapped from an underlying trusted processor, not trust
in a remote software stack. We now describe how Intel SGX
implements the Load(P) and Execute(EP, in) functions from
the previous section.
1) Load(P) → (EP, φ): A client receives a proof φ that its
intended program P (and initial data) has been loaded into an
enclave via an attestation procedure. Code loaded into enclaves
is measured by SGX during initialization (using SHA-256)
and signed with respect to public parameters. The client can
verify the measurement/signature pair to attest that the intended
program was loaded via the Intel Attestation Service.
2) Execute(EP, in) → (out, ψ): SGX protects enclave
program execution by isolating enclave code and data in
Processor Reserved Memory (PRM), referred to as Enclave
Page Cache (EPC), which is a subset of DRAM that gets set
aside securely at boot time. Cache lines read into the processor
cache from the EPC are isolated from non-enclave read/writes
via hardware paging mechanisms, and encrypted/integrity
checked at the processor boundary. Cryptographic keys for
these operations are owned by the trusted processor. Thus, data
in the EPC is protected (privacy and integrity-wise) against
certain physical attacks (e.g., bus snooping), the operating
system (direct inspection of pages, DMA), and the hypervisor.
3) Paging: In Intel SGX, the EPC has limited capacity. To
support applications with large working sets, the OS performs
paging to move pages in and out of the EPC on demand.
Hardware mechanisms in SGX ensure that all pages swapped
in/out of the EPC are integrity checked and encrypted before
being handed to the OS. Thus, the OS learns only that a page
with a public address needed to be swapped, not the data in
the page. Special pages controlled by SGX (called VA pages)
implement an integrity tree over swapped pages. In the event the
system is shutdown, the VA pages and (consequently) enclave
data pages are lost.
4) Enclave IO:
is the developer’s responsibility to
partition applications into trusted and untrusted parts and to
deﬁne a communication interface between them. The literature
has made several proposals for a standard interface, e.g., a
POSIX interface [40].
It
5) Security Challenges in Intel SGX: We now detail aspects
of Intel SGX that present security challenges and motivate the
design of ZeroTrace.
a) Software side channels: Although SGX prevents an
adversary from directly inspecting/tampering with the contents
of the EPC, it does not protect against multiple software-based
side channels. In particular, SGX enclaves share hardware
4
resources with untrusted applications and delegate EPC paging
to the OS. Correspondingly, the literature has demonstrated
attacks that extract sensitive data through hardware resource
pressure (e.g., cache [6], [35], [48] and branch predictor [21])
and the application’s page-level access pattern [7], [53].
b) EPC scope: Since the integrity veriﬁcation tree for
EPC pages is located in the EPC itself (in VA pages), SGX does
not support integrity (with freshness) guarantees in the event
of a system shutdown [25]. More generally, SGX provides no
privacy/integrity guarantees for any memory beyond the EPC
(e.g., non-volatile disk). Ensuring persistent integrity for data
and privacy/integrity for non-volatile data is delegated to the
user/application level.
c) No direct IO/syscalls: Code executing within an
enclave operates in ring-3 user space and is not allowed to
perform direct IO (e.g., disk, network) and system calls. If an
enclave has to make use of either, then it must delegate it to
untrusted code running outside of the enclave.
6) Additional Challenges In Enclave Design: We now
summarize additional properties of Intel SGX (1.0) that
make designing prevention methods against the above issues
challenging.
a) EPC limit: Currently, the size of EPC is physically
upper bounded by 128 MB by the processor. Around 30 MB
of EPC is used for bookkeeping, leaving around 95 MB of
usable memory. As mentioned above, EPC paging alleviates
this problem but reveals page-level access patterns. However
EPC paging is expensive and can cost between 3x and 1000x
depending on the underlying page access pattern (Figure 3
in [2]).
b) Context switching: At any time, the OS controls
when enclave code starts and stops running. Each switch incurs
a large performance overhead – the processor must save the
state needed to resume execution and clear registers to prevent
information leakages. Further, it is difﬁcult to achieve persistent
system integrity if the enclave can be terminated/swapped at
any point in its execution.
C. ORAM
We now describe the popular deﬁnition for ORAM from the
literature [42], [43]. Afterwards, we provide additional details
for the Path ORAM [43] and Circuit ORAM [49] schemes,
used in this paper.
An ORAM scheme can be used to store and retrieve
blocks of memory on a remove server, such that the server
learns nothing about the data access patterns. Informally, no
information should be leaked about: (a) the data being accessed,
(b) whether the same/different data is being accessed relative
to a prior access (linkability), (c) whether the access is a read
or write.
1) Correctness: The ORAM construction is correct if it
returns, on input −→y , data that is consistent with −→y with
probability ≥ 1 - negl(|−→y |), i.e. the ORAM may fail with
probability negl(|−→y |).
2) Security: Let
−→y := ((opM, aM, dataM), ..., (op1, a1, data1))
denote a data request sequence of length M where each opi
denotes a read(ai) or a write(ai) operation. Speciﬁcally, ai
denotes the identiﬁer of the block being read or written, and
datai represents the data being written. In this notation, index
1 corresponds to the most recent load/store and index M
corresponds to the oldest load/store operation. Let ORAM(−→y )
denote the (possibly randomized) sequence of accesses to the
remote storage given the sequence of data requests −→y . An
ORAM construction is said to be secure if for any two data
request sequences −→y and −→z of the same length, their access
patterns ORAM(−→y ) and ORAM(−→z ) are computationally
indistinguishable to anyone but the client.
D. Path ORAM
We now give a summary of Path ORAM [43], one of
the ORAMs used in our implementation. Which ORAM is
used isn’t fundamental, and this can be switched behind the
memory controller interface. That said, ORAM bandwidth to
untrusted storage and ORAM controller trusted ‘client’ storage
are inversely proportional [42], [43], [49]. Further, the SGX
and oblivious settings decrease performance when using larger
controller storage (due to EPC evictions [25] and the cost
of running oblivious programs; see Section VI). Path ORAM
provides a middle ground here: better bandwidth/larger storage
than Circuit ORAM [49]; worse bandwidth/smaller storage than
SSS ORAM [42].
1) Server Storage: Path ORAM stores N data blocks, where
B is the block size in bits, and treats untrusted storage as a
binary tree of height L (with 2L leaves). Each node in the tree
is a bucket that contains ≤ Z blocks. In the case of a bucket
having < Z blocks, remaining slots are padded with dummy
blocks.
2) Controller Storage: The Path ORAM controller storage
consists of a stash and a position map. The stash is a set
of blocks that Path ORAM can hold onto at any given time
(see below). To keep the stash small (negligible probability
of overﬂow), experiments show Z ≥ 4 is required for the
stash size to be bound to ω(log N ) [43]. The position map is a
dictionary that maps each block in Path ORAM to a leaf in the
server’s binary tree. Thus, the position map size is O(LN ) bits.
3) Operation: As stated above, each block in Path ORAM
is mapped to a leaf bucket in the server’s binary tree via the
position map. For a block a mapped to leaf l, Path ORAM
guarantees that block a is currently stored in (i) some bucket
on the path from the tree’s root to leaf l, or (ii) the stash. Then,
to perform a read/write request to block a (mapped to leaf l),
we perform the following steps: First, read the leaf label l for
the block a from the position map. Re-assign this block to
a freshly sampled leaf label l(cid:48), chosen uniformly at random.
Second, fetch the entire path from the root to leaf bucket in
server storage. Third, retrieve the block from the combination
of the fetched path and the local stash. Fourth, write back the
path to the server storage. In this step the client must push
blocks in the stash as far down the path as possible, while
keeping with the main invariant. This strategy minimizes the
number of blocks in the stash after each access and is needed
to achieve a small (logarithmic) stash size.
4) Security intuition: The adversary’s view during each
access is limited to the path read/written (summarized by the
leaf in the position map) during each access. This leaf is re-
assigned to a uniform random new leaf on each access to
the block of interest. Thus, the adversary sees a sequence of
uniform random-sampled leaves that are independent of the
actual access pattern.
5) Extension: Recursion. The Path ORAM position map
is O(N ) bits, which is too large to ﬁt in trusted storage
for large N. To reduce the client side storage to O(1), Path
ORAM can borrow the standard recursion trick from the ORAM
constructions of Stefenov et al. [42] and Shi et al. [37]. In short,
the idea is to store the position map itself as a smaller ORAM
on the server side and then recurse. Each smaller “position
map” ORAM must be accessed in turn, to retrieve the leaf
label for the original ORAM.
6) Extension: Integrity. Path ORAM assumes a passive
adversary by default. To provide an integrity guarantee with
freshness, one can construct a Merkle tree mirrored [43] onto
the Path ORAM tree, which adds a constant factor to the
bandwidth cost. We remark that when ORAM recursion is
used, an integrity mechanism is also required to guarantee
ORAM privacy [34].
Both integrity veriﬁcation and ORAM recursion will be
needed in our ﬁnal design to achieve a performant system
against active attacks.
E. Circuit ORAM
We now brieﬂy highlight the differences between Circuit
ORAM [49] and Path ORAM. In the interest of space, we
describe our work using Path ORAM as the memory controller
since it is the conceptually simpler ORAM. Circuit ORAM was
designed with the intent of having smaller ‘circuit complexity’1
while managing ORAM controller storage, which also improves
efﬁciency when running ORAMs in a data oblivious manner.