interrupt, it has exercised some new basic block and is marked
as coverage-increasing. Oracle binary construction requires
prior knowledge of the target binary‚Äôs basic block addresses.
We leverage Dyninst‚Äôs static control-Ô¨Çow analysis to output
a list of basic blocks, then iterate through that list in using
binary Ô¨Åle IO to insert the interrupts. To prevent interrupts
triggering before forkserver initialization, we do not consider
functions executed prior to the forkserver callback in 
(e.g., , , ,
).
We use SIGTRAP for our interrupt for two reasons: (1)
it has long been used for Ô¨Åne-grain execution control and
analysis (e.g., gdb [51], [52] and kernel-probing [53], [54]);
and (2) its binary representation‚Äî0xCC‚Äîis one byte long,
making it possible to overwrite basic blocks of all sizes.
D. Tracer Binary
If the oracle determines a test case to be coverage-
increasing, UnTracer extracts its new code coverage by ex-
ecuting it on a separate tracer binary‚Äîa coverage tracing-
instrumented version of the target binary. We utilize Dyninst
to statically instrument the tracer with a forkserver for fast
execution, and coverage callbacks inserted in each of its basic
blocks for coverage tracing. Upon execution, a basic block‚Äôs
callback appends its corresponding basic block address to a
trace Ô¨Åle located in UnTracer‚Äôs working directory.
In an early version of UnTracer, we observed that coverage
traces containing repeatedly-executing basic blocks add signif-
icant overhead in two ways: Ô¨Årst, recording individual blocks
multiple times‚Äîa common occurrence for binaries exhibiting
looping behavior‚Äîslowed UnTracer‚Äôs trace writing operations;
second, trace reading is also penalized, as subsequent block-
level unmodiÔ¨Åcation operations are forced to process any
repeatedly-executing basic blocks. To mitigate such overhead,
we optimize tracing to only record uniquely-covered basic
blocks as follows: in the tracer forkserver, we initialize a
global hashmap data structure to track all covered basic blocks
unique to each trace; as each tracing child is forked,
it
inherits the initial hashmap; upon a basic block‚Äôs execution,
its callback utilizes hashmap lookup to determine if the block
has been previously covered in the current execution; if not, the
callback updates the current trace log and updates the hashmap.
With this optimization, for each coverage-increasing test case,
UnTracer records a set of all uniquely-covered basic blocks,
thus reducing the overhead resulting from logging, reading,
and processing the same basic block multiple times.
E. Unmodifying the Oracle
When a test case triggers the oracle‚Äôs software interrupt,
it is marked as coverage-increasing and UnTracer removes its
interrupts from its newly-covered basic blocks to ensure no
future test case with the non-new coverage is marked coverage-
increasing. For each newly-covered basic block reported in an
coverage-increasing test case‚Äôs trace log, UnTracer replaces
the inserted interrupt with the original byte found in the target
binary‚Äîeffectively resetting it to its pre-modiÔ¨Åed state. Doing
so means any future test cases executing this basic block
will no longer trigger the interrupt and subsequently not be
misidentiÔ¨Åed as coverage-increasing.
We observe that even coverage-increasing test cases often
have signiÔ¨Åcant overlaps in coverage. This causes UnTracer to
attempt unmodifying many already-unmodiÔ¨Åed basic blocks,
resulting in high overhead. To mitigate this, we introduce a
hashmap data structure for tracking global coverage. Much
like the hashmap used for per-trace redundant basic block
Ô¨Åltering, before unmodifying any basic block from the trace
log, UnTracer determines if the block has been seen in any
previous trace via hashmap lookup. If so, the basic block is
skipped. If not, its interrupt is removed, and the basic block is
added to the hashmap. Thus, global coverage tracking ensures
that only newly-covered basic blocks are processed.
VI. TRACING-ONLY EVALUATION
Our evaluation compares UnTracer against
tracing all
test cases with three widely used white- and black-box bi-
nary fuzzing tracing approaches‚ÄîAFL-Clang (white-box) [5],
AFL-QEMU (black-box dynamically-instrumented) [5], and
AFL-Dyninst (black-box statically-instrumented) [10] on eight
real-world benchmarks of different type.
Our experiments answer the following questions:
1)
How does UnTracer (coverage-guided tracing) per-
form compared to tracing all test cases?
2) What factors contribute to UnTracer‚Äôs overhead?
3)
How is UnTracer‚Äôs overhead impacted by the rate of
coverage-increasing test cases?
A. Evaluation Overview
We compare UnTracer‚Äôs performance versus popular white-
and black-box fuzzing tracing approaches: AFL-Clang, AFL-
QEMU, and AFL-Dyninst. These tracers all work with the
(cid:24)(cid:26)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:59 UTC from IEEE Xplore.  Restrictions apply. 
Package
Benchmark
Version
Class
Basic Blocks
Test Cases (¬∑106)
libarchive
libksba
cjson
libjpeg
poppler
binutils
audiofile
tcpdump
bsdtar
cert-basic
cjson
djpeg
pdftohtml
readelf
sfconvert
tcpdump
3.3.2
1.3.5
1.7.7
9c
0.22.5
2.30
0.2.7
4.9.2
archiv
crypto
web
image
doc
dev
audio
net
31379
9958
1447
4844
54596
21249
5603
33743
21.06
10.73
25.62
14.53
1.21
14.89
10.17
27.14
Coverage-
increasing
Ratio
1.47E‚àí5
1.50E‚àí5
1.48E‚àí5
1.33E‚àí5
7.85E‚àí5
8.98E‚àí5
3.91E‚àí2
3.73E‚àí5
500ms
Timeouts
0
0
0
12133
0
0
1137609
0
TABLE III.
INFORMATION ON THE EIGHT BENCHMARKS USED IN OUR EVALUATION IN SECTIONS VI AND VII AND AVERAGES OVER 5 24-HOUR
DATASETS FOR EACH BENCHMARK.
same fuzzer, AFL, and they cover the tracing design space
including working with white- and black-box binaries as
well as static and dynamic binary rewriting. Our evaluations
examine each tracer‚Äôs overhead on eight real-world, open-
source benchmarks of different type, common to the fuzzing
community. Table III provides benchmark details. To smooth
the effects of randomness and ensure the most fair comparison
of performance, we evaluate tracers on the same Ô¨Åve input
datasets per benchmark. Each dataset contains the test cases
generated by fuzzing that benchmark with AFL-QEMU for 24
hours. Though our results show UnTracer has less than 1%
overhead after one hour of fuzzing, we extend all evaluations
to 24 hours to better match previous fuzzing evaluations.
We conÔ¨Ågure AFL to run with 500ms timeouts and leave
all other parameters at their defaults. We modify AFL so that
all non-tracing functionality is removed (e.g., progress reports)
and instrument its run_target() function to collect per-
test case timing. To address noise from the operating system
and other sources, we perform eight trials of each dataset.
For each set of trials per dataset, we apply trimmed-mean de-
noising [55] on each test case‚Äôs tracing times; the resulting
times represent each test case‚Äôs median tracing performance.
All trials are distributed across two workstations‚Äîeach
with Ô¨Åve single-core virtual machines. Both host systems run
Ubuntu 16.04 x86 64 operating systems, with six-core Intel
Core i7-7800X CPU @ 3.50GHz, and 64GB RAM. All 10
virtual machines run Ubuntu x86 64 18.04 using VirtualBox.
We allocate each virtual machine 6GB of RAM.4
B. Experiment Infrastructure
To narrow our focus to tracing overhead, we only record
time spent executing/tracing test cases. To maintain fairness,
we run all tracers on the same Ô¨Åve pre-generated test case
datasets for each benchmark. For dataset generation, we imple-
ment a modiÔ¨Åed version of AFL that dumps its generated test
cases to Ô¨Åle. In our evaluations, we use QEMU as the baseline
tracer (since our focus is black-box tracing) to generate the
Ô¨Åve datasets for each benchmark.
Our second binary‚ÄîTestTrace‚Äîforms the backbone
of our evaluation infrastructure. We implement this using a
modiÔ¨Åed version of AFL‚Äîeliminating components irrelevant
to tracing (e.g., test case generation and execution monitoring).
Given a benchmark, pre-generated dataset, and tracing mode
(i.e., AFL-Clang, AFL-QEMU, AFL-Dyninst, or none (a.k.a.
baseline)), TestTrace: (1) reproduces the dataset‚Äôs test cases
one-by-one, (2) measures time spent tracing each test case‚Äôs
coverage, and (3) logs each trace time to Ô¨Åle. For UnTracer,
we include both the initial full-speed execution and any time
spent handling coverage-increasing test cases.
C. Benchmarks
Our benchmark selection is based on popularity in the
fuzzing community and benchmark type. We Ô¨Årst
identify
candidate benchmarks from several popular fuzzers‚Äô trophy
cases5 and public benchmark repositories [5], [56], [4], [3],
[57]. To maximize benchmark variety, we further partition can-
didates by their overall type‚Äîsoftware development, image
processing, data archiving, network utilities, audio process-
ing, document processing, cryptography, and web develop-
ment. After we Ô¨Ålter out several candidate benchmarks based
on incompatibility with our tracers (e.g., Dyninst-based instru-
mentation crashes on openssl), we select one benchmark per
category: bsdtar (archiv), cert-basic (crypto), cjson
(web), djpeg (image), pdftohtml (doc), readelf (dev),
sfconvert (audio), and tcpdump (net).
For each benchmark, we measure several other metrics
with potential effects on tracing overhead: number of basic
blocks; and average number of generated test cases, average
rate of coverage-increasing test cases, and average number of
500ms timeouts in 24 hours. Benchmark basic block totals
are computed by enumerating all basic blocks statically using
Dyninst [25]. For counting timeouts, we examined the statistics
reported by afl-fuzz-saveinputs during dataset gen-
eration; using our speciÔ¨Åed timeout value (500ms), we then
averaged the number of timeouts per benchmark among its
datasets. Lastly, for each benchmark, we counted and averaged
the number of test cases generated in all of its 24-hour datasets.
We compile each benchmark using Clang/LLVM, with all
compiler options set to their respective benchmark-speciÔ¨Åc
defaults. Below, we detail our additional tracer-speciÔ¨Åc bench-
mark conÔ¨Ågurations.
1) Baseline: AFL‚Äôs forkserver-based execution model (also
used by UnTracer‚Äôs interest oracle and tracer binaries) adds a
substantial performance improvement over execve()-based
execution [50]. As each fuzzing tracer in our evaluation
leverages forkserver-based execution, we design our ‚Äúground-
truth‚Äù benchmark execution models to represent the fastest
known execution speeds: a statically-instrumented forkserver
without any coverage tracing. We use a modiÔ¨Åed copy of
AFL‚Äôs assembler (afl-as) to instrument baseline (forkserver-
only) benchmark versions. In each benchmark trial, we use
4Across all trials, we saw no benchmarks exceeding 2GB of RAM usage.
5A fuzzer‚Äôs ‚Äútrophy case‚Äù refers to a collection of bugs/vulnerabilities
reportedly discovered with that fuzzer.
(cid:24)(cid:26)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:59 UTC from IEEE Xplore.  Restrictions apply. 


'
$




&
%
$





#

()*	+%,-
()*	./

-
'






"
!




	 

 
  
  
   
 

    



Per-benchmark relative overheads of UnTracer versus black-box
Fig. 7.
binary tracers AFL-QEMU and AFL-Dyninst.
its baseline execution speeds as the basis for comparing each
fuzzing tracers‚Äô overhead.
2) AFL-Clang: As compiling with AFL-GCC failed for
some binaries due to changes in GCC, we instead use AFL-
Clang.
3) AFL-QEMU: We only need to provide it the original
uninstrumented target binary of each benchmark in our evalu-
ation.
4) AFL-Dyninst: For our AFL-Dyninst evaluations, we
instrument each binary using AFL-Dyninst‚Äôs instrumenter with
conÔ¨Åguration parameters bpatch.setDelayedParsing
set
true; bpatch.setLivenessAnalysis and
bpatch.setMergeTramp false;
all other
and leave
conÔ¨Åguration parameters at their default settings.
to