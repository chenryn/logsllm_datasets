interrupt, it has exercised some new basic block and is marked
as coverage-increasing. Oracle binary construction requires
prior knowledge of the target binary’s basic block addresses.
We leverage Dyninst’s static control-ﬂow analysis to output
a list of basic blocks, then iterate through that list in using
binary ﬁle IO to insert the interrupts. To prevent interrupts
triggering before forkserver initialization, we do not consider
functions executed prior to the forkserver callback in 
(e.g., , , ,
).
We use SIGTRAP for our interrupt for two reasons: (1)
it has long been used for ﬁne-grain execution control and
analysis (e.g., gdb [51], [52] and kernel-probing [53], [54]);
and (2) its binary representation—0xCC—is one byte long,
making it possible to overwrite basic blocks of all sizes.
D. Tracer Binary
If the oracle determines a test case to be coverage-
increasing, UnTracer extracts its new code coverage by ex-
ecuting it on a separate tracer binary—a coverage tracing-
instrumented version of the target binary. We utilize Dyninst
to statically instrument the tracer with a forkserver for fast
execution, and coverage callbacks inserted in each of its basic
blocks for coverage tracing. Upon execution, a basic block’s
callback appends its corresponding basic block address to a
trace ﬁle located in UnTracer’s working directory.
In an early version of UnTracer, we observed that coverage
traces containing repeatedly-executing basic blocks add signif-
icant overhead in two ways: ﬁrst, recording individual blocks
multiple times—a common occurrence for binaries exhibiting
looping behavior—slowed UnTracer’s trace writing operations;
second, trace reading is also penalized, as subsequent block-
level unmodiﬁcation operations are forced to process any
repeatedly-executing basic blocks. To mitigate such overhead,
we optimize tracing to only record uniquely-covered basic
blocks as follows: in the tracer forkserver, we initialize a
global hashmap data structure to track all covered basic blocks
unique to each trace; as each tracing child is forked,
it
inherits the initial hashmap; upon a basic block’s execution,
its callback utilizes hashmap lookup to determine if the block
has been previously covered in the current execution; if not, the
callback updates the current trace log and updates the hashmap.
With this optimization, for each coverage-increasing test case,
UnTracer records a set of all uniquely-covered basic blocks,
thus reducing the overhead resulting from logging, reading,
and processing the same basic block multiple times.
E. Unmodifying the Oracle
When a test case triggers the oracle’s software interrupt,
it is marked as coverage-increasing and UnTracer removes its
interrupts from its newly-covered basic blocks to ensure no
future test case with the non-new coverage is marked coverage-
increasing. For each newly-covered basic block reported in an
coverage-increasing test case’s trace log, UnTracer replaces
the inserted interrupt with the original byte found in the target
binary—effectively resetting it to its pre-modiﬁed state. Doing
so means any future test cases executing this basic block
will no longer trigger the interrupt and subsequently not be
misidentiﬁed as coverage-increasing.
We observe that even coverage-increasing test cases often
have signiﬁcant overlaps in coverage. This causes UnTracer to
attempt unmodifying many already-unmodiﬁed basic blocks,
resulting in high overhead. To mitigate this, we introduce a
hashmap data structure for tracking global coverage. Much
like the hashmap used for per-trace redundant basic block
ﬁltering, before unmodifying any basic block from the trace
log, UnTracer determines if the block has been seen in any
previous trace via hashmap lookup. If so, the basic block is
skipped. If not, its interrupt is removed, and the basic block is
added to the hashmap. Thus, global coverage tracking ensures
that only newly-covered basic blocks are processed.
VI. TRACING-ONLY EVALUATION
Our evaluation compares UnTracer against
tracing all
test cases with three widely used white- and black-box bi-
nary fuzzing tracing approaches—AFL-Clang (white-box) [5],
AFL-QEMU (black-box dynamically-instrumented) [5], and
AFL-Dyninst (black-box statically-instrumented) [10] on eight
real-world benchmarks of different type.
Our experiments answer the following questions:
1)
How does UnTracer (coverage-guided tracing) per-
form compared to tracing all test cases?
2) What factors contribute to UnTracer’s overhead?
3)
How is UnTracer’s overhead impacted by the rate of
coverage-increasing test cases?
A. Evaluation Overview
We compare UnTracer’s performance versus popular white-
and black-box fuzzing tracing approaches: AFL-Clang, AFL-
QEMU, and AFL-Dyninst. These tracers all work with the
(cid:24)(cid:26)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:59 UTC from IEEE Xplore.  Restrictions apply. 
Package
Benchmark
Version
Class
Basic Blocks
Test Cases (·106)
libarchive
libksba
cjson
libjpeg
poppler
binutils
audiofile
tcpdump
bsdtar
cert-basic
cjson
djpeg
pdftohtml
readelf
sfconvert
tcpdump
3.3.2
1.3.5
1.7.7
9c
0.22.5
2.30
0.2.7
4.9.2
archiv
crypto
web
image
doc
dev
audio
net
31379
9958
1447
4844
54596
21249
5603
33743
21.06
10.73
25.62
14.53
1.21
14.89
10.17
27.14
Coverage-
increasing
Ratio
1.47E−5
1.50E−5
1.48E−5
1.33E−5
7.85E−5
8.98E−5
3.91E−2
3.73E−5
500ms
Timeouts
0
0
0
12133
0
0
1137609
0
TABLE III.
INFORMATION ON THE EIGHT BENCHMARKS USED IN OUR EVALUATION IN SECTIONS VI AND VII AND AVERAGES OVER 5 24-HOUR
DATASETS FOR EACH BENCHMARK.
same fuzzer, AFL, and they cover the tracing design space
including working with white- and black-box binaries as
well as static and dynamic binary rewriting. Our evaluations
examine each tracer’s overhead on eight real-world, open-
source benchmarks of different type, common to the fuzzing
community. Table III provides benchmark details. To smooth
the effects of randomness and ensure the most fair comparison
of performance, we evaluate tracers on the same ﬁve input
datasets per benchmark. Each dataset contains the test cases
generated by fuzzing that benchmark with AFL-QEMU for 24
hours. Though our results show UnTracer has less than 1%
overhead after one hour of fuzzing, we extend all evaluations
to 24 hours to better match previous fuzzing evaluations.
We conﬁgure AFL to run with 500ms timeouts and leave
all other parameters at their defaults. We modify AFL so that
all non-tracing functionality is removed (e.g., progress reports)
and instrument its run_target() function to collect per-
test case timing. To address noise from the operating system
and other sources, we perform eight trials of each dataset.
For each set of trials per dataset, we apply trimmed-mean de-
noising [55] on each test case’s tracing times; the resulting
times represent each test case’s median tracing performance.
All trials are distributed across two workstations—each
with ﬁve single-core virtual machines. Both host systems run
Ubuntu 16.04 x86 64 operating systems, with six-core Intel
Core i7-7800X CPU @ 3.50GHz, and 64GB RAM. All 10
virtual machines run Ubuntu x86 64 18.04 using VirtualBox.
We allocate each virtual machine 6GB of RAM.4
B. Experiment Infrastructure
To narrow our focus to tracing overhead, we only record
time spent executing/tracing test cases. To maintain fairness,
we run all tracers on the same ﬁve pre-generated test case
datasets for each benchmark. For dataset generation, we imple-
ment a modiﬁed version of AFL that dumps its generated test
cases to ﬁle. In our evaluations, we use QEMU as the baseline
tracer (since our focus is black-box tracing) to generate the
ﬁve datasets for each benchmark.
Our second binary—TestTrace—forms the backbone
of our evaluation infrastructure. We implement this using a
modiﬁed version of AFL—eliminating components irrelevant
to tracing (e.g., test case generation and execution monitoring).
Given a benchmark, pre-generated dataset, and tracing mode
(i.e., AFL-Clang, AFL-QEMU, AFL-Dyninst, or none (a.k.a.
baseline)), TestTrace: (1) reproduces the dataset’s test cases
one-by-one, (2) measures time spent tracing each test case’s
coverage, and (3) logs each trace time to ﬁle. For UnTracer,
we include both the initial full-speed execution and any time
spent handling coverage-increasing test cases.
C. Benchmarks
Our benchmark selection is based on popularity in the
fuzzing community and benchmark type. We ﬁrst
identify
candidate benchmarks from several popular fuzzers’ trophy
cases5 and public benchmark repositories [5], [56], [4], [3],
[57]. To maximize benchmark variety, we further partition can-
didates by their overall type—software development, image
processing, data archiving, network utilities, audio process-
ing, document processing, cryptography, and web develop-
ment. After we ﬁlter out several candidate benchmarks based
on incompatibility with our tracers (e.g., Dyninst-based instru-
mentation crashes on openssl), we select one benchmark per
category: bsdtar (archiv), cert-basic (crypto), cjson
(web), djpeg (image), pdftohtml (doc), readelf (dev),
sfconvert (audio), and tcpdump (net).
For each benchmark, we measure several other metrics
with potential effects on tracing overhead: number of basic
blocks; and average number of generated test cases, average
rate of coverage-increasing test cases, and average number of
500ms timeouts in 24 hours. Benchmark basic block totals
are computed by enumerating all basic blocks statically using
Dyninst [25]. For counting timeouts, we examined the statistics
reported by afl-fuzz-saveinputs during dataset gen-
eration; using our speciﬁed timeout value (500ms), we then
averaged the number of timeouts per benchmark among its
datasets. Lastly, for each benchmark, we counted and averaged
the number of test cases generated in all of its 24-hour datasets.
We compile each benchmark using Clang/LLVM, with all
compiler options set to their respective benchmark-speciﬁc
defaults. Below, we detail our additional tracer-speciﬁc bench-
mark conﬁgurations.
1) Baseline: AFL’s forkserver-based execution model (also
used by UnTracer’s interest oracle and tracer binaries) adds a
substantial performance improvement over execve()-based
execution [50]. As each fuzzing tracer in our evaluation
leverages forkserver-based execution, we design our “ground-
truth” benchmark execution models to represent the fastest
known execution speeds: a statically-instrumented forkserver
without any coverage tracing. We use a modiﬁed copy of
AFL’s assembler (afl-as) to instrument baseline (forkserver-
only) benchmark versions. In each benchmark trial, we use
4Across all trials, we saw no benchmarks exceeding 2GB of RAM usage.
5A fuzzer’s “trophy case” refers to a collection of bugs/vulnerabilities
reportedly discovered with that fuzzer.
(cid:24)(cid:26)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:59 UTC from IEEE Xplore.  Restrictions apply. 


'
$




&
%
$





#

()*	+%,-
()*	./

-
'






"
!




	 

 
  
  
   
 

    



Per-benchmark relative overheads of UnTracer versus black-box
Fig. 7.
binary tracers AFL-QEMU and AFL-Dyninst.
its baseline execution speeds as the basis for comparing each
fuzzing tracers’ overhead.
2) AFL-Clang: As compiling with AFL-GCC failed for
some binaries due to changes in GCC, we instead use AFL-
Clang.
3) AFL-QEMU: We only need to provide it the original
uninstrumented target binary of each benchmark in our evalu-
ation.
4) AFL-Dyninst: For our AFL-Dyninst evaluations, we
instrument each binary using AFL-Dyninst’s instrumenter with
conﬁguration parameters bpatch.setDelayedParsing
set
true; bpatch.setLivenessAnalysis and
bpatch.setMergeTramp false;
all other
and leave
conﬁguration parameters at their default settings.
to