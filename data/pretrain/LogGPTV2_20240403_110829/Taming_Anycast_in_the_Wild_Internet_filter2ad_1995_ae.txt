RIPE Atlas probes for various minimum group sizes
Figure 15: The RIPE Atlas catchment for a site in India be-
fore (black) and after (red) adding a new provider.
v
1
2
3
Rank AS
G
0.0091
H 0.0046
0.0042
I
J
0.0299
0.0060
K
L
0.0041
9998
9999
10000
P
0
0
0
326
327
349
M
1.0211
1.0107
1.0097
0.0616
0.0584
0.0547
Table 4: The ASes with the largest and smallest rank mis-
matches.
Networks with at least Ptarдet probes are considered well covered.
To determine an appropriate value for Ptarдet , we consider the
error between our beacon data and RIPE Atlas measurements as in
Section 3.3, but here we consider increasing the minimum number
of RIPE Atlas probes required for each group.
Figure 14 shows the impact of minimum group size on the RTT
error. As expected, increasing group size reduces error, with 20
probes providing comparable error to 50, with notable improve-
ments over 10 probes, in particular above the 80th percentile. We
therefore take our Ptarдet to be 20.
The intuition of M is that significant networks covered by few
or no probes will have high mismatch values, while networks with
low relative volume (v) or high probe coverage (i.e., P > Ptarдet )
result in a low M.
Table 4 presents the largest and smallest mismatch (M) values;
we found these values to be consistent over a month, but we omit
those findings due to space considerations. As expected, some net-
works that contribute a significant number of client requests are
well covered by RIPE Atlas (e.g., AS J and AS K are both signifi-
cant end-user networks). The highly mismatched ASes are more
interesting. Generally, these fall into three categories: cellular net-
works, networks in poorly covered regions, and networks that
aren’t sources of typical CDN traffic.
The top three highest mismatched ASes (AS G, H, and I) are
large US cellular providers. While it is possible that these represent
mixed networks [33], we confirm, by inspection of the CDN’s HTTP
access logs, that these are likely to be cellular clients. Low probe
coverage in cellular networks is a known gap in the RIPE Atlas
platform. The second category contains networks in geographic
areas with known low probe coverage. This includes ASes from
India and the Middle East. For both of these categories, the missing
networks have significant relative volumes. However, there are
relatively few of them: only 10 have v greater than .002. Such a
small set of networks may be examined directly with other tools.
The final category of high-mismatch networks includes those
that are not traditionally end-user networks for CDNs. These gen-
erally consisted of cloud-service providers in which users may host
a vaiety of services. The majority of these networks had no probes
at the time of measurement, but have notable relative volumes.
Examination of requests from these networks suggests that they
are primarily web services: for example, search engine crawlers,
VPNs and web proxies, and generic cloud services. These are not
representative of traditional CDN traffic and are out-of-scope for
DailyCatch.
Despite some limitations, RIPE Atlas provides strong coverage
for many important end-user networks, allowing DailyCatch to
grant visibility into the provider-level impact of announcements.
Ultimately, DailyCatch itself is generic, and could use any mea-
surement platform that provides RTT measurements to arbitrary
targets and catchment information for each vantage point.
5.2 Anycast Errors
Beyond the complex inter-domain behaviors described above, any-
cast networks must further contend with networks that behave in
unexpected ways. We consider a real-world example involving the
global anycast announcements from the studied CDN. Figure 15
maps the catchment of a site in India before (black dots) and after
(red dots) announcing to a single new peer. In this case, this change
shifted the catchment, and resulted in nearly all traffic entering the
network from a single interconnect for the provider, likely due to
an upstream misconfiguration which liberally re-announced the
block.
Figure 16 further examines the performance impacts from this
configuration change, showing the percentage increase in RTTs
between these configurations. Of the 54 impacted ASes, we can
see that for about 20% of them saw little degradation, and indeed
some did not change catchment. However, beyond that, we notice
significant degradation, up to about 1, 200% (13ms to over 170ms),
coming from an AS that was previously served in the United States
that now traveled to India.
The impact of any announcement change is tangled: an upstream
provider’s incentives are often not the same as the anycast opera-
tor’s, and network misconfigurations are often opaque, in particular
051015202530RTT Difference(ms)0.00.20.40.60.81.0CDF of groups2102050Taming Anycast in the Wild Internet
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
There has been a significant body of work exploring the eco-
nomic and performance trade-offs of Internet connectivity [5, 37].
Others have examined the increasing flatness of the Internet topol-
ogy [13, 20, 22, 26, 35]. In DailyCatch, we are only concerned any-
cast performance impacts: different providers behave differently.
Measuring and responding to conditions as providers are added –
regardless of any underlying trends – is essential.
WhyHigh [25] is a system that helps to find problematic be-
haviors by prefix, surfacing the most troublesome, and helping
to determine their root cause. While similar, DailyCatch focuses
on anycast networks, and in particular on controlled experiments
to compare anycast configurations. In [12], the authors present a
generalized CDN measurement system. DailyCatch is orthogonal,
as it focuses on a method for tuning anycast policy, an issue [12]
agrees is complex.
7 CONCLUSION
In this paper, we have presented a measurement and optimization
proposal for large, many-provider anycast networks, in the context
of a global CDN. We have shown that anycast networks with more
network providers interact with the Internet in an observably dif-
ferent way than those with fewer providers, with incoming traffic
using a more diverse set of, more often short, paths. We further
demonstrated how announcement configurations can be manip-
ulated to provide performance improvements, and observed that
care must be taken to avoid suffering performance degradation.
Finally, we presented DailyCatch, a tool for performing active
measurements of anycast configurations, which provides sub-AS-
level visibility into the impacts of changes. We demonstrated that
DailyCatch is able to detect both performance improvements, as
well as degradations, that may arise from idiosyncratic network poli-
cies. Finally, we provided thorough measurement of the coverage
of the RIPE Atlas platform, providing clarity into which networks
are well represented.
REFERENCES
[1] CIDR report. https://www.cidr-report.org/as2.0/.
[2] RIPE Atlas. https://atlas.ripe.net.
[3] root-servers.org. https://root-servers.org/.
[4] RouteViews. http://www.routeviews.org.
[5] A. Ahmed, Z. Shafiq, H. Bedi, and A. R. Khakpour. Peering vs. transit: Performance
[6] Z. Al-Qudah, S. Lee, M. Rabinovich, O. Spatscheck, and J. Van der Merwe. Anycast-
[7] H. A. Alzoubi, S. Lee, M. Rabinovich, O. Spatscheck, and J. Van der Merwe.
comparison of peering and transit interconnections. Proc. of ICNP ’17, 2017.
aware transport for content delivery networks. In Proc. of WWW ’09, 2009.
Anycast CDNs revisited. In Proc. of WWW ’08, 2008.
practical architecture for an anycast CDN. ACM Trans. Web, 2011.
2005.
[9] H. Ballani and P. Francis. Towards a global IP anycast service. SIGCOMM CCR,
[10] H. Ballani, P. Francis, and S. Ratnasamy. A measurement-based deployment
[8] H. A. Alzoubi, S. Lee, M. Rabinovich, O. Spatscheck, and J. Van Der Merwe. A
[11] M. Calder, A. Flavel, E. Katz-Bassett, R. Mahajan, and J. Padhye. Analyzing the
proposal for IP anycast. In Proc. of IMC ’06, 2006.
performance of an anycast CDN. In Proc. of IMC ’15, 2015.
[12] M. Calder, R. Gao, M. Schröder, R. Stewart, J. Padhye, R. Mahajan, G. Anantha-
narayanan, and E. Katz-Bassett. Odin: Microsoft’s scalable fault-tolerant CDN
measurement system. In Proc. of NSDI ’18, 2018.
Are we one hop away from a better Internet? In Proc. of IMC ’15, 2015.
of anycast on DNS root name servers. In RIPE-393, 2006.
many sites are enough? In Proc. of PAM ’17, pages 188–200. Springer, 2017.
[13] Y.-C. Chiu, B. Schlinker, A. B. Radhakrishnan, E. Katz-Bassett, and R. Govindan.
[15] R. de Oliveira Schmidt, J. Heidemann, and J. H. Kuipers. Anycast latency: How
[14] L. Colitti, E. Romijn, H. Uijterwaal, and A. Robachevsky. Evaluating the effects
Figure 16: The % increase in RTT by AS for those impacted
by the misconfiguration.
when there are many providers. Inferring the behavior of all net-
works, on a global scale, requires significant modelling and measure-
ment effort, alongside prior knowledge of the likely routing policies
of third-party networks. Routing tables are constantly changing [1],
limiting the use of any particular heuristics. DailyCatch provides
visibility, revealing the source of such issues: this makes it an effec-
tive tool for their detection and mitigation.
6 RELATED WORK
Anycast has been a significant field of study within the networking
community. Many studies have focused on the behavior of anycast
for DNS infrastructure [14, 17, 29–31]. Other works have studied
non-DNS anycast deployments [11, 18, 23], or systems built on top
of anycast [6–9, 21]. In [36], the authors examine anycast stability,
and how it impacts file download completion. These works are
largely orthogonal: with the visibility that DailyCatch provides,
many of these systems could likely be improved.
In [10], the authors explore the implications of anycast configu-
rations and attempt to provide a guiding heuristic for future deploy-
ments, ultimately suggesting that a single provider would provide
consistent behavior. In [15], the authors determine the number of
anycast sites necessary to achieve reasonable latency, character-
ising the diminishing returns of new sites, showing, via unicast
measurements, that anycast may make poor decisions. In [11, 12]
the authors propose a DNS based work around for poorly perform-
ing anycast. More recently, [27] examined anycast performance
and proposed a BGP-communities based solution for controlling it.
While important for understanding the nature of the choices
made when using anycast, many of these systems rely on compari-
son against unicast behavior, which may not be achievable without
significant infrastructure [12] or not yet deployed features [27]. We
argue, instead, for an approach that pursues behavior that can be
achieved today, via explicit anycast configurations.
In [16] the authors present Verfploeter, a system for mapping
anycast catchments using ping responses directed to anycast ad-
dresses. While an extremely powerful tool for mapping catchments,
Verfploeter does not provide RTT or path information, both critical
components in managing anycast performance.
Systems such as Google’s Espresso [38] and Facebook’s Edge-
Fabric [34] send small samples of traffic on alternative paths to
provide regular feedback. However, they are designed for manag-
ing egress traffic. Here, we are considering ingress traffic, which
relies on changes in BGP announcement configuration.
020040060080010001200% Increase in RTT0.000.250.500.751.00CDF of ASesIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Stephen McQuistin, Sree Priyanka Uppu, and Marcel Flores
[16] W. B. de Vries, R. de O. Schmidt, W. Hardaker, J. Heidemann, P.-T. de Boer, and
A. Pras. Broad and load-aware anycast mapping with Verfploeter. In Proc. of
IMC ’17, 2017.
[17] X. Fan, JHeidemann, and R. Govindan. Evaluating anycast in the domain name
system. In Proc. of INFOCOM ’13, 2013.
[18] A. Flavel, P. Mani, D. Maltz, N. Holt, J. Liu, Y. Chen, and O. Surmachev. FastRoute:
A scalable load-aware anycast routing architecture for modern CDNs. In Proc. of
NSDI ’15, 2015.
[19] R. Fontugne, A. Shah, and E. Aben. The (thin) bridges of as connectivity: Measur-
ing dependency using as hegemony. In Proc. of PAM ’18, pages 216–227. Springer,
2018.
[20] R. Fontugne, A. Shah, and E. Aben. The (thin) bridges of as connectivity: Mea-
[21] M. J. Freedman, K. Lakshminarayanan, and D. Mazières. OASIS: Anycast for any
suring dependency using as hegemony. In Proc. of PAM ’18, 2018.
service. In Proc. of NSDI ’06, 2006.
[22] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. The flattening Internet topology: Natural
evolution, unsightly barnacles or contrived collapse? In Proc. of PAM ’08, 2008.
[23] D. Giordano, D. Cicalese, A. Finamore, M. Mellia, M. M. Munafò, D. Z. Joumblatt,
and D. Rossi. A first characterization of anycast traffic from passive traces. In
Proc. of TMA 2016, 2016.
[24] T. Holterbach, E. Aben, C. Pelsser, R. Bush, and L. Vanbever. Measurement
Vantage Point Selection Using A Similarity Metric. In Proc. of ANRW ’17. ACM,
2017.
[25] R. Krishnan, H. V. Madhyastha, S. Jain, S. Srinivasan, A. Krishnamurthy, T. An-
derson, and J. Gao. Moving beyond end-to-end path information to optimize
CDN performance. In Proc. of IMC ’09, 2009.
net inter-domain traffic. In Proc. of SIGCOMM ’10, 2010.
problems, & potential. In Proc. of SIGCOMM ’18, 2018.
[26] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, and F. Jahanian. Inter-
[27] Z. Li, D. Levin, N. Spring, and B. Bhattacharjee. Internet anycast: Performance,
[28] K. Lindqvist and J. Abley. Operation of anycast services. RFC 4786, Dec. 2006.
[29] Z. Liu, B. Huffaker, M. Fomenkov, N. Brownlee, and K. Claffy. Two days in the
[31] M. Müller, G. C. M. Moura, R. de O. Schmidt, and J. Heidemann. Recursives in
life of the DNS anycast root servers. In Proc. of PAM ’07, 2007.
[30] G. Moura, R. d. O. Schmidt, J. Heidemann, W. B. de Vries, M. Muller, L. Wei, and
C. Hesselman. Anycast vs. DDoS: Evaluating the November 2015 root DNS event.
In Proc. of IMC ’16.
the wild: Engineering authoritative DNS servers. In Proc. of IMC ’17, 2017.
[32] R. V. Oliveira, D. Pei, W. Willinger, B. Zhang, and L. Zhang. In search of the
elusive ground truth: the internet’s as-level connectivity structure.
In ACM
SIGMETRICS Performance Evaluation Review, volume 36, pages 217–228. ACM,
2008.
[33] J. P. Rula, F. E. Bustamante, and M. Steiner. Cell spotting: Studying the role of
[35] Y. Shavitt and U. Weinsberg. Topological trends of Internet content providers. In
cellular networks in the Internet. In Proc. of IMC ’17, 2017.
[34] B. Schlinker, H. Kim, T. Cui, E. Katz-Bassett, H. V. Madhyastha, I. Cunha, J. Quinn,
S. Hasan, P. Lapukhov, and H. Zeng. Engineering egress with edge fabric: Steering
oceans of content to the world. In Proc. of SIGCOMM ’17, 2017.
Proc. of SIMPLEX ’12, 2012.
2017.
[36] L. Wei and J. Heidemann. Does anycast hang up on you? In Proc. of TMA ’17,
[37] F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, and W. Willinger. Leveraging
interconnections for performance: The serving infrastructure of a large CDN. In
Proc. of SIGCOMM ’18, 2018.
[38] K.-K. Yap, M. Motiwala, J. Rahe, S. Padgett, M. Holliman, G. Baldus, M. Hines,
T. Kim, A. Narayanan, A. Jain, V. Lin, C. Rice, B. Rogan, A. Singh, B. Tanaka,
M. Verma, P. Sood, M. Tariq, M. Tierney, D. Trumic, V. Valancius, C. Ying, M. Kalla-
halla, B. Koley, and A. Vahdat. Taking the edge off with Espresso: Scale, reliability
and programmability for global Internet peering. In Proc. of SIGCOMM ’17, 2017.