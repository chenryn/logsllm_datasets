### RIPE Atlas Probes for Various Minimum Group Sizes

**Figure 15: The RIPE Atlas Catchment for a Site in India Before (Black) and After (Red) Adding a New Provider.**

| Rank | AS | Mismatch (M) |
|------|----|--------------|
| 1    | G  | 0.0091       |
| 2    | H  | 0.0046       |
| 3    | I  | 0.0042       |
| 4    | J  | 0.0299       |
| 5    | K  | 0.0060       |
| 6    | L  | 0.0041       |
| 7    | P  | 0            |
| 8    | 0  | 0            |
| 9    | 0  | 0            |
| 10   | 326| 1.0211       |
| 11   | 327| 1.0107       |
| 12   | 349| 1.0097       |
| 13   | M  | 0.0616       |
| 14   | N  | 0.0584       |
| 15   | O  | 0.0547       |

**Table 4: The ASes with the Largest and Smallest Rank Mismatches.**

Networks with at least \( P_{\text{target}} \) probes are considered well-covered. To determine an appropriate value for \( P_{\text{target}} \), we consider the error between our beacon data and RIPE Atlas measurements as described in Section 3.3, but here we focus on increasing the minimum number of RIPE Atlas probes required for each group.

**Figure 14** shows the impact of the minimum group size on the Round-Trip Time (RTT) error. As expected, increasing the group size reduces the error. Specifically, 20 probes provide comparable error to 50 probes, with notable improvements over 10 probes, particularly above the 80th percentile. Therefore, we set \( P_{\text{target}} \) to 20.

The intuition behind \( M \) is that significant networks covered by few or no probes will have high mismatch values, while networks with low relative volume (\( v \)) or high probe coverage (i.e., \( P > P_{\text{target}} \)) result in a low \( M \).

**Table 4** presents the largest and smallest mismatch (\( M \)) values. We found these values to be consistent over a month, but we omit those findings due to space considerations. As expected, some networks that contribute a significant number of client requests are well-covered by RIPE Atlas (e.g., AS J and AS K are both significant end-user networks). The highly mismatched ASes are more interesting and generally fall into three categories: cellular networks, networks in poorly covered regions, and networks that are not sources of typical CDN traffic.

The top three highest mismatched ASes (AS G, H, and I) are large US cellular providers. While it is possible that these represent mixed networks [33], we confirm, by inspecting the CDN’s HTTP access logs, that these are likely to be cellular clients. Low probe coverage in cellular networks is a known gap in the RIPE Atlas platform. The second category includes networks in geographic areas with known low probe coverage, such as ASes from India and the Middle East. For both of these categories, the missing networks have significant relative volumes. However, there are relatively few of them: only 10 have \( v \) greater than 0.002. Such a small set of networks may be examined directly with other tools.

The final category of high-mismatch networks includes those that are not traditionally end-user networks for CDNs. These generally consist of cloud-service providers where users may host a variety of services. The majority of these networks had no probes at the time of measurement but have notable relative volumes. Examination of requests from these networks suggests that they are primarily web services, such as search engine crawlers, VPNs, web proxies, and generic cloud services. These are not representative of traditional CDN traffic and are out-of-scope for DailyCatch.

Despite some limitations, RIPE Atlas provides strong coverage for many important end-user networks, allowing DailyCatch to grant visibility into the provider-level impact of announcements. Ultimately, DailyCatch itself is generic and could use any measurement platform that provides RTT measurements to arbitrary targets and catchment information for each vantage point.

### Anycast Errors

Beyond the complex inter-domain behaviors described above, anycast networks must also contend with networks that behave in unexpected ways. We consider a real-world example involving the global anycast announcements from the studied CDN. **Figure 15** maps the catchment of a site in India before (black dots) and after (red dots) announcing to a single new peer. In this case, this change shifted the catchment, resulting in nearly all traffic entering the network from a single interconnect for the provider, likely due to an upstream misconfiguration that liberally re-announced the block.

**Figure 16** further examines the performance impacts from this configuration change, showing the percentage increase in RTTs between these configurations. Of the 54 impacted ASes, about 20% saw little degradation, and some did not change catchment. However, beyond that, we notice significant degradation, up to about 1,200% (from 13ms to over 170ms), coming from an AS that was previously served in the United States and now travels to India.

The impact of any announcement change is complex: an upstream provider’s incentives are often not the same as the anycast operator’s, and network misconfigurations are often opaque, particularly when dealing with many providers. Inferring the behavior of all networks, on a global scale, requires significant modeling and measurement effort, alongside prior knowledge of the likely routing policies of third-party networks. Routing tables are constantly changing [1], limiting the use of any particular heuristics. DailyCatch provides visibility, revealing the source of such issues, making it an effective tool for their detection and mitigation.

### Related Work

Anycast has been a significant field of study within the networking community. Many studies have focused on the behavior of anycast for DNS infrastructure [14, 17, 29–31]. Other works have studied non-DNS anycast deployments [11, 18, 23] or systems built on top of anycast [6–9, 21]. In [36], the authors examine anycast stability and how it impacts file download completion. These works are largely orthogonal: with the visibility that DailyCatch provides, many of these systems could likely be improved.

In [10], the authors explore the implications of anycast configurations and attempt to provide a guiding heuristic for future deployments, ultimately suggesting that a single provider would provide consistent behavior. In [15], the authors determine the number of anycast sites necessary to achieve reasonable latency, characterizing the diminishing returns of new sites, showing via unicast measurements that anycast may make poor decisions. In [11, 12], the authors propose a DNS-based workaround for poorly performing anycast. More recently, [27] examined anycast performance and proposed a BGP-communities-based solution for controlling it.

While important for understanding the nature of the choices made when using anycast, many of these systems rely on comparison against unicast behavior, which may not be achievable without significant infrastructure [12] or not yet deployed features [27]. We argue instead for an approach that pursues behavior that can be achieved today, via explicit anycast configurations.

In [16], the authors present Verfploeter, a system for mapping anycast catchments using ping responses directed to anycast addresses. While an extremely powerful tool for mapping catchments, Verfploeter does not provide RTT or path information, both critical components in managing anycast performance.

Systems such as Google’s Espresso [38] and Facebook’s Edge-Fabric [34] send small samples of traffic on alternative paths to provide regular feedback. However, they are designed for managing egress traffic. Here, we are considering ingress traffic, which relies on changes in BGP announcement configuration.

### Conclusion

In this paper, we have presented a measurement and optimization proposal for large, many-provider anycast networks, in the context of a global CDN. We have shown that anycast networks with more network providers interact with the Internet in an observably different way than those with fewer providers, with incoming traffic using a more diverse set of, more often short, paths. We further demonstrated how announcement configurations can be manipulated to provide performance improvements, and observed that care must be taken to avoid suffering performance degradation.

Finally, we presented DailyCatch, a tool for performing active measurements of anycast configurations, which provides sub-AS-level visibility into the impacts of changes. We demonstrated that DailyCatch is able to detect both performance improvements and degradations that may arise from idiosyncratic network policies. Additionally, we provided a thorough measurement of the coverage of the RIPE Atlas platform, providing clarity into which networks are well-represented.

### References

[1] CIDR report. https://www.cidr-report.org/as2.0/.

[2] RIPE Atlas. https://atlas.ripe.net.

[3] root-servers.org. https://root-servers.org/.

[4] RouteViews. http://www.routeviews.org.

[5] A. Ahmed, Z. Shafiq, H. Bedi, and A. R. Khakpour. Peering vs. transit: Performance comparison of peering and transit interconnections. Proc. of ICNP ’17, 2017.

[6] Z. Al-Qudah, S. Lee, M. Rabinovich, O. Spatscheck, and J. Van der Merwe. Anycast-aware transport for content delivery networks. In Proc. of WWW ’09, 2009.

[7] H. A. Alzoubi, S. Lee, M. Rabinovich, O. Spatscheck, and J. Van der Merwe. Anycast CDNs revisited. In Proc. of WWW ’08, 2008.

[8] H. A. Alzoubi, S. Lee, M. Rabinovich, O. Spatscheck, and J. Van Der Merwe. A practical architecture for an anycast CDN. ACM Trans. Web, 2011.

[9] H. Ballani and P. Francis. Towards a global IP anycast service. SIGCOMM CCR, 2005.

[10] H. Ballani, P. Francis, and S. Ratnasamy. A measurement-based deployment proposal for IP anycast. In Proc. of IMC ’06, 2006.

[11] M. Calder, A. Flavel, E. Katz-Bassett, R. Mahajan, and J. Padhye. Analyzing the performance of an anycast CDN. In Proc. of IMC ’15, 2015.

[12] M. Calder, R. Gao, M. Schröder, R. Stewart, J. Padhye, R. Mahajan, G. Ananthanarayanan, and E. Katz-Bassett. Odin: Microsoft’s scalable fault-tolerant CDN measurement system. In Proc. of NSDI ’18, 2018.

[13] Y.-C. Chiu, B. Schlinker, A. B. Radhakrishnan, E. Katz-Bassett, and R. Govindan. Are we one hop away from a better Internet? In Proc. of IMC ’15, 2015.

[14] L. Colitti, E. Romijn, H. Uijterwaal, and A. Robachevsky. Evaluating the effects of anycast on DNS root name servers. In RIPE-393, 2006.

[15] R. de Oliveira Schmidt, J. Heidemann, and J. H. Kuipers. Anycast latency: How many sites are enough? In Proc. of PAM ’17, pages 188–200. Springer, 2017.

[16] W. B. de Vries, R. de O. Schmidt, W. Hardaker, J. Heidemann, P.-T. de Boer, and A. Pras. Broad and load-aware anycast mapping with Verfploeter. In Proc. of IMC ’17, 2017.

[17] X. Fan, J. Heidemann, and R. Govindan. Evaluating anycast in the domain name system. In Proc. of INFOCOM ’13, 2013.

[18] A. Flavel, P. Mani, D. Maltz, N. Holt, J. Liu, Y. Chen, and O. Surmachev. FastRoute: A scalable load-aware anycast routing architecture for modern CDNs. In Proc. of NSDI ’15, 2015.

[19] R. Fontugne, A. Shah, and E. Aben. The (thin) bridges of AS connectivity: Measuring dependency using AS hegemony. In Proc. of PAM ’18, pages 216–227. Springer, 2018.

[20] R. Fontugne, A. Shah, and E. Aben. The (thin) bridges of AS connectivity: Measuring dependency using AS hegemony. In Proc. of PAM ’18, 2018.

[21] M. J. Freedman, K. Lakshminarayanan, and D. Mazières. OASIS: Anycast for any service. In Proc. of NSDI ’06, 2006.

[22] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. The flattening Internet topology: Natural evolution, unsightly barnacles or contrived collapse? In Proc. of PAM ’08, 2008.

[23] D. Giordano, D. Cicalese, A. Finamore, M. Mellia, M. M. Munafò, D. Z. Joumblatt, and D. Rossi. A first characterization of anycast traffic from passive traces. In Proc. of TMA 2016, 2016.

[24] T. Holterbach, E. Aben, C. Pelsser, R. Bush, and L. Vanbever. Measurement vantage point selection using a similarity metric. In Proc. of ANRW ’17. ACM, 2017.

[25] R. Krishnan, H. V. Madhyastha, S. Jain, S. Srinivasan, A. Krishnamurthy, T. Anderson, and J. Gao. Moving beyond end-to-end path information to optimize CDN performance. In Proc. of IMC ’09, 2009.

[26] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, and F. Jahanian. Inter-net inter-domain traffic. In Proc. of SIGCOMM ’10, 2010.

[27] Z. Li, D. Levin, N. Spring, and B. Bhattacharjee. Internet anycast: Performance, problems, & potential. In Proc. of SIGCOMM ’18, 2018.

[28] K. Lindqvist and J. Abley. Operation of anycast services. RFC 4786, Dec. 2006.

[29] Z. Liu, B. Huffaker, M. Fomenkov, N. Brownlee, and K. Claffy. Two days in the life of the DNS anycast root servers. In Proc. of PAM ’07, 2007.

[30] G. Moura, R. d. O. Schmidt, J. Heidemann, W. B. de Vries, M. Muller, L. Wei, and C. Hesselman. Anycast vs. DDoS: Evaluating the November 2015 root DNS event. In Proc. of IMC ’16.

[31] M. Müller, G. C. M. Moura, R. de O. Schmidt, and J. Heidemann. Recursives in the wild: Engineering authoritative DNS servers. In Proc. of IMC ’17, 2017.

[32] R. V. Oliveira, D. Pei, W. Willinger, B. Zhang, and L. Zhang. In search of the elusive ground truth: the internet’s AS-level connectivity structure. In ACM SIGMETRICS Performance Evaluation Review, volume 36, pages 217–228. ACM, 2008.

[33] J. P. Rula, F. E. Bustamante, and M. Steiner. Cell spotting: Studying the role of cellular networks in the Internet. In Proc. of IMC ’17, 2017.

[34] B. Schlinker, H. Kim, T. Cui, E. Katz-Bassett, H. V. Madhyastha, I. Cunha, J. Quinn, S. Hasan, P. Lapukhov, and H. Zeng. Engineering egress with Edge Fabric: Steering oceans of content to the world. In Proc. of SIGCOMM ’17, 2017.

[35] Y. Shavitt and U. Weinsberg. Topological trends of Internet content providers. In Proc. of SIMPLEX ’12, 2012.

[36] L. Wei and J. Heidemann. Does anycast hang up on you? In Proc. of TMA ’17, 2017.

[37] F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, and W. Willinger. Leveraging interconnections for performance: The serving infrastructure of a large CDN. In Proc. of SIGCOMM ’18, 2018.

[38] K.-K. Yap, M. Motiwala, J. Rahe, S. Padgett, M. Holliman, G. Baldus, M. Hines, T. Kim, A. Narayanan, A. Jain, V. Lin, C. Rice, B. Rogan, A. Singh, B. Tanaka, M. Verma, P. Sood, M. Tariq, M. Tierney, D. Trumic, V. Valancius, C. Ying, M. Kallahalla, B. Koley, and A. Vahdat. Taking the edge off with Espresso: Scale, reliability, and programmability for global Internet peering. In Proc. of SIGCOMM ’17, 2017.