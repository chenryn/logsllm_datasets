of commodity operating systems. To validate this assertion,
we perform a direct comparison of CUSTOS to prior work:
SGX-Log [57], [123]; a logger based on the TPM2 hardware’s
9EC2 instances with hardware SGX capability are not currently offered.
TABLE V: Microbenchmarks on Logger operations. We report the
median execution times over 500 runs,
including results for the
Logging phase using both ecalls and Hotcalls. We compare to SGX-
Log [57] and a TPM2 [34] logger (same log messages), as well as
BGLS signatures [40] (smaller ﬁxed-size messages), parameterizing
to optimize the performance of each. CUSTOS’ logging phase is the
only one that can scale to more than a million events per second.
Phase
Initialization
Startup
Logging
Logging (Hotcalls)
Commitment
Shutdown
CUSTOS
94.55 ms
109.10 ms
4.71 µs
0.92 µs
128.87 µs
188.98 µs
SGX-Log
–
–
0.80 ms
0.79 ms
–
–
TPM2
–
–
20 ms
–
734 ms
–
BGLS
–
–
31.89 ms
–
–
–
extend (Logging Phase) and quote (Commit Phase) oper-
ations [34]; and Hartung et al.’s scheme based on BGLS sig-
natures [41]. For SGX-Log and BGLS, we conservatively set
highly-favorable parameters for performance.10 We focus here
on the critical Logging phase, which dominates performance
cost. SGX-Log takes a median time of 0.80 ms (ecalls) and
0.79 ms (Hotcalls), TPM extend operations take 20 ms, and
the BGLS-based scheme takes 31.89 ms. Further, the TPM
implementation also requires a quote operation to produce a
proof, adding an additional median cost of 734 ms per block.
From these results, it can be seen that CUSTOS outperforms
existing solutions by three to ﬁve orders of magnitude. Another
way to compare the performances of these logging systems is
to consider the maximum throughput they can scale to support.
Using the numbers from Table V, we compute that CUSTOS
could process up to 1,086,956 log events per second. On the
other hand, SGX-Log could process at most 1,266 events per
second, the TPM2-based solution could reach at most 50 events
per second, and the BGLS approach could only sign at most 31
events per second. While the actual log throughput ultimately
also depends on how many events the underlying logging
framework can process and send to through the secure logging
phase, if we consider that single hosts can produce hundreds
of thousands of system calls per second [24], then CUSTOS is
the only solution that can scale to match such throughput.
2) Vanilla Linux Audit Comparison: Finally, we instrument
auditd to measure the average time that insecure Vanilla
Linux Audit takes to process a single event as compared to
CUSTOS. We run this microbenchmark by measuring the time
auditd takes to process 40,000 identical log events. We
ﬁnd that CUSTOS-enabled auditd takes an average of 6.61
µs/event whereas Vanilla auditd takes an average of 5.67
µs/event. CUSTOS thus imposes an average 16.6% overhead
on unmodiﬁed, insecure auditd. This reported overhead is
conservative in that our measurement did not capture the time
required for auditd to ﬂush events to disk, making the
overhead imposed by CUSTOS lower in practice. We conclude
that CUSTOS’ tamper-evident logging protocol imposes a rea-
sonable overhead over insecure Linux Audit’s log processing
time. We will analyze the impact of this overhead on real
applications in Section X-B.
10We conﬁgured SGX-Log to use a block-size of 1000 log messages to
avoid including in our measurement the impact of SGX-Log’s sealing phase,
and we conﬁgured the BGLS-based scheme with n = 1000 and l = 1000.
10
TABLE VI: Application benchmark results. We report the medians
over 10 runs. For httpd and NGINX, we used apache bench [119]
conﬁgured to send 100,000 requests from a single thread and output
the average time per request. For Redis, we used the built-in redis-
benchmark conﬁgured to send 250,000 requests from a single thread
and output the average time per request. We ran the Blast benchmark
in two conﬁgurations: ﬁrst, limiting it to one thread only; second,
letting it use all the CPU threads available.
Test Type
nginx
apache2
redis
blast
blast-multicore
Vanilla
72 µs
75 µs
23,520 ns
938.641 s
222.791 s
CUSTOS
73 µs
76 µs
23,932 ns
954.104 s
237.027 s
Overhead
1.39%
1.33%
1.75%
1.65%
6.39%
B. Logger Macrobenchmarks
To evaluate the system-wide runtime overhead of CUSTOS,
we use the point-to-point setup to measure the performance of
a series of application benchmarks while running CUSTOS in
the background. In particular, we benchmark three server ap-
plications (httpd [120], NGINX [87] and Redis [106]) and one
scientiﬁc-computing application (Blast [27]). Our choice of
server-oriented applications is dictated by the intended deploy-
ment environment (enterprise servers); further, the benchmarks
we choose are known to generate larger-than-average system
call loads [76]. Note that given our intensive conﬁguration of
Linux Audit, we were unable to run classic OS benchmark
suites including UnixBench [72] and LMBench [82].11
We run each application benchmark 10 times and report the
results in Table VI. The runtime overheads of our CUSTOS’
implementation in the ﬁrst four benchmarks are all under
2%. This is because CUSTOS does not add cycles to the
execution of system processes besides auditd, which runs
asynchronously from the processes that the audit stream de-
scribes. However, CUSTOS still incurs overhead due to its use
of Hotcalls and its interaction with the Architectural Enclave
Service Manager service (aesmd). This cost is most visible
in the blast-multicore benchmark, which is CPU-bound and
conﬁgured to use all the 8 logical cores of our experimental
machine. However, even in this scenario, CUSTOS’ runtime
overhead against insecure Vanilla auditd remains at a rea-
sonable 6.39%. We conclude that CUSTOS’ extensions to
Linux Audit impose acceptable system performance costs.12
C. Audit Protocol
We next use the point-to-point setup to characterize the
performance of CUSTOS’ audit protocol, used in both the cen-
tralized and the decentralized auditing scenarios. Intuitively,
we expect the cost of an audit to be dominated by the time
required to transmit and process log data, which are orders
of magnitude larger than CUSTOS’ protocol messages. To
conﬁrm that, we measure the end-to-end time required by
audits from the perspective of the Auditor, which spans the
11This limitation, shared with prior work [67], [76], is due to Linux Audit
not scaling to support the intensive loads of these suites.
12When deployed in CPUs with a low number of cores, the cost of using
Hotcalls may outweigh its gains. In these cases, CUSTOS can be conﬁgured
to use standard ecalls at the cost of a slightly decreased throughput (Table V).
Fig. 4: Time required by an auditor to complete an audit by number
of logs in the response. The measurements include all operations from
the moment of challenge generation to the moment of result transmis-
sion. The cost of an audit grows linearly with the size of the response.
moment of challenge generation to the moment of result
transmission (Figure 3, 2 - 15 ), while varying the number of
logs transmitted. We set the period between audits (T ) to 1
second, then throttle our workload script to generate variable
levels of logs per second. We consider 33 different workload
levels ranging from 1 to 9900 logs/second, repeating each
workload level 100 times. The results can be found in Figure
4. As expected, the time to complete an audit grows linearly
with the size of the log.
Another way to evaluate the practicality of our audit
protocol is to ask the question “For a given T , at what log
size would it take more than T seconds to complete an audit
challenge?” This log size represents the breakdown point of
the system, where audit challenges are now issued with greater
frequency than they can be responded to. Extrapolating from
the above test with T set to 1 s, we ﬁnd that the Logger would
need to generate 145,000 events per second for the processing
time of an audit to exceed 1 s. We therefore conclude that, like
the CUSTOS logging mechanism, our audit protocol is practical
even for hosts under extreme load.
D. Audit Frequency
We now use our distributed setup to evaluate our decen-
tralized auditing protocol. As the cost of an individual audit is
identical in both the centralized and decentralized variants, we
here turn our attention to the frequency at which each node
is audited in a realistic CUSTOS’ deployment. To this end,
we run CUSTOS on a cluster of N=100 nodes, each with an
auditing interval of T =10 s, replication factor r=1 and a ﬁxed
number of challenges per round w.13 We instrument nodes to
initiate their ﬁrst audits at uniform offsets of T to smooth
the network impact. We then capture the times at which each
node gets audited over a period of 10 minutes of observation,
repeating the experiment with w=[1, 2, 4]. For each challenge
received by each node, we compute the time passed since the
same node received the last challenge.
13While r=1 is not the most secure conﬁguration for G4, the goal of
this experiment is to capture audit frequency, which does not depend on r.
Similarly, our choice of a constant (not worst case) logging rate and the use
of SGX in simulation mode only minimally affect audit frequency.
11
 0 10 20 30 40 50 60 70 80 0 2000 4000 6000 8000 10000Time [ms]No. of logs in the responseCUSTOS’ small protocol messages, which are dominated by
the baseline cost of log transmission.
Based on this result, it is clear that challenge frequency
imposes no meaningful difference in network cost. More
challenges simply decrease the size of the average log block
and probabilistically reduce the time before each block is
replicated (cf. Section IX-A). Because of this, it is to the
advantage of the administrator to use many audit challenges
(T and w) when deploying CUSTOS; doing so will verify and
replicate logs sooner with minimal increase in network cost.
Conversely, modifying the replication factor r would increase
the network cost due to the larger amount of log data to
transmit. This increase in network (and storage) requirements
reﬂects the cost that must be paid for a stronger assurance of
log availability against a distributed adversary.
F. Enclave Memory Usage
For CUSTOS to be minimally invasive, it is not enough
to achieve low performance and network overheads. As we
mentioned in Section III-3, an important constraint of Intel
SGX is its limited amount of protected memory (128 MB on
current hardware), which is shared across all enclaves on the
system. To evaluate the memory usage of CUSTOS’ enclave
in our implementation, we use the ofﬁcial Enclave Memory
Measurement tool (sgx_emmt) from the Intel SGX SDK,
which returns the stack peak usage and heap peak usage in KB
during enclave’s execution. We launch it on our SGX-enabled
machine used in the point-to-point setup, while running both
Logger and Auditor on it. The reported peak memory usage
of CUSTOS is 5 KB of stack and 16 KB of heap, for a total
of 21 KB of memory, which is less than 0.02% of SGX’s
protected memory. This usage does not increase with the rate
of log events, since only one event at a time is processed
inside CUSTOS’ enclave during its operations. We conclude
that CUSTOS’ utilizes enclave memory efﬁciently.
XI. ATTACK CASE STUDY
In this section, we empirically demonstrate the effective-
ness of CUSTOS at detecting log tampering by presenting a
case study based on a realistic attack scenario. In particular, we
simulate the nation-state APT attack scenario “Firefox Back-
door w/ Drakon In-Memory” from the DARPA Transparent
Computing program dataset [59]. This attack starts with a
remote exploit of Firefox 54.0.1, which allows the attacker
to open a command shell that gives them unprivileged access
to the host; this shell is later used to download a malicious
binary ﬁle (“Drakon”) onto the victim host, which is then
run to achieve root access and give the attacker persistent
access to the host. However, unlike the original attack, our
adversary is aware that their attack can be detected from the
logs (through, e.g., [83], [23]) and therefore proceeds to erase
them immediately after achieving root access. We focus on log
erasure since it is known to be used in real-world attacks [56],
[37], [104], [16], [17], [90], [77], [9].
We simulated this attack by replaying its trace on one CUS-
TOS-enabled host v, which we chose at random from the 100
nodes of our distributed setup (see Section X). We conﬁgured
CUSTOS with parameters T =10 s, r=1, and w=4. The results
are shown in the timeline of Figure 6. CUSTOS replicated on
Fig. 5: Cumulative distribution function of the frequency at which
nodes are audited with varying number of challenges issued per audit
round (w). Results are based on a 100-node network over 10 minutes
of observation, with nodes initiating audit rounds every T =10 s. When
w=4, 98.4% of the time nodes were challenged again within T of
receiving the last challenge.
TABLE VII: Size of protocol messages in our CUSTOS’ implemen-
tation. The reported sizes include the size of one or more ECDSA
signatures (64 bytes) per message.
Message Type
Challenge
Result
Response metadata