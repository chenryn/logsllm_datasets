0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
l
a
n
o
i
t
i
d
n
o
C
e
g
a
r
e
v
A
r
o
r
r
E
l
a
n
o
i
t
i
s
o
P
y
t
i
l
a
c
i
t
i
r
C
r
o
r
r
E
l
a
n
o
i
t
i
s
o
P
l
t
n
e
m
e
c
a
p
s
i
D
m
u
m
x
a
M
i
y
t
i
l
a
c
i
t
i
r
C
r
o
r
r
E
0
0
50
100
150
200
250
Comparison Index
to the bottom graph we see that max displacement error
reveals a structure that is similar to that revealed by posi-
tional error. However, the structure of max displacement
error quickly collapses as the number of background
errors increases.
The existence of such criticality structures suggest
the possibility of deploying computational armoring
strategically to the most critical comparisons. Methods
like triple modular redundancy [22]—applied here to list
comparisons—are expensive, and targeting via criticality
might help increase the “bang for the buck” of such
729
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:19 UTC from IEEE Xplore.  Restrictions apply. 
techniques.
C. Criticality Structures Depend on Error Measures
The strict correctness error measure is a degenerate
case, but we also found that the comparison critical-
ity can vary greatly given different graduated error
measures. Using quick sort, for example, we measured
comparison criticality for the ﬁrst 250 comparisons using
both normalized positional error and inversions error.
In Figure 3 we see the results: With positional error
criticality, the ﬁrst N = 52 comparisons have much
greater criticalities than all other comparisons in the sort.
This occurs because the ﬁrst N comparisons of quick
sort are responsible for sorting the list into a ‘top’ half
and ‘bottom’ half with all items less than the pivot in the
bottom half and all items greater than the pivot in the
top half. A faulty comparison in the ﬁrst N comparisons
leads to the miscompared item being placed, on average,
about N/2 away from its correct position.
On the other hand, for inversions error criticality, the
ﬁrst N comparisons have lower criticalities than all other
comparisons. We suspect this is because items misplaced
in either the top or bottom half of the list will tend to
move toward the center where the halves meet, so even
many faults in the ﬁrst N comparisons will tend to add
only a single inversion to the error measure.
Fig. 3. Different error measures generate different criticality
structures. See text for details.
Positional Error Criticality and Inversions Criticality 
       Vs. Comparison Index For Quick Sort
0.04
Though positional error and inversions error, like all
error measures, agree on the meaning of perfectly sorted,
they measure signiﬁcantly different list properties, and
their criticality structures are therefore quite different.
Although we may hope to ﬁnd general principles, it
is important to understand that wise choice of error
measure requires not only sensitivity to likely faults, but
also to the needs of a computation’s end-user.
D. Criticality Structure is Highly Dependent on the
Algorithm
As a ﬁnal illustration, consider the bubble sort results
in Figure 4. Bubble sort’s O(N 2) comparisons give
it a great deal of redundancy, so the criticalities in
Figure 4 are much smaller than in, say, Figure 3—but
in addition, the details of its criticality structures emerge
most clearly at relatively high background error rates. It
is unsurprising that bubble sort’s last N comparisons are
most critical, but we, at any rate, hadn’t anticipated the
small but distinct length N periodic structure throughout
bubble sort’s execution, indicating increased criticality in
the last half of each pass through the list.
Fig. 4. Periodic criticality in bubble sort facing a non-zero
background error rate. The bottom graph is a subgraph of the
top graph. See text.
Positional Error Criticality Vs. Comparison Index 
in Bubble Sort
y
t
i
l
a
c
i
t
i
r
C
r
o
r
r
E
l
a
n
o
i
t
i
s
o
P
0.004
0.0035
0.003
0.0025
0.002
0.0015
0.001
0.0005
0
0
500
0.001
0.0008
0.0006
0.0004
0.0002
0
300
y
t
i
l
a
c
i
t
i
r
C
r
o
r
r
E
l
a
n
o
i
t
i
s
o
P
Background Failure Rate = 0%
Background Failure Rate = 10%
Background Failure Rate = 20%
1000
2000
Comparison Index
1500
2500
3000
Background Failure Rate = 0%
Background Failure Rate = 10%
Background Failure Rate = 20%
350
400
Comparison Index
450
500
Background Failure Rate = 0%
Background Failure Rate = 10%
Background Failure Rate = 20%
Background Failure Rate = 0%
Background Failure Rate = 10%
Background Failure Rate = 20%
y
t
i
l
a
c
i
t
i
r
C
s
n
o
i
s
r
e
v
n
I
y
t
i
l
a
c
i
t
i
r
C
r
o
r
r
E
l
a
n
o
i
t
i
s
o
P
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
0.04
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
0
100
150
50
Comparison Index
200
250
All three sorting algorithms displayed structures re-
lated to the input size of N = 52: First N criticality in
730
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:19 UTC from IEEE Xplore.  Restrictions apply. 
quick sort, last N criticality in merge sort, and a period
N oscillation in bubble sort.
IV. FUTURE WORK AND CONCLUSIONS
Moving forward, we want to apply criticality to other
problems and algorithms, especially those traditionally
approached via strict correctness. We also hope to inves-
tigate additional fault modes, selected to help hardware
designers minimize computational damage by triaging
and shaping faults, rather than trying to suppress them
all. And ﬁnally, though the presented data was obtained
numerically, the observed relationships between input
size and criticality raise the possibility of developing
predictions of comparison criticality analytically, or by
extrapolation from small instances.
In this paper we have introduced the idea of criticality,
and demonstrated the application of graduated error
measures—beyond strict correctness—in a case study of
traditional sorting algorithms. We observed that the algo-
rithms possessed previously-unseen criticality structures,
which can change signiﬁcantly depending on the chosen
error measure, and which can persist in the face of rising
background failures.
The “fault-intolerant core” computing style has scaled
vastly beyond what von Neumann predicted for it over
sixty years ago [23]. The extreme-scale computing
community—tasked with “living in the future”—is in-
creasingly perceiving now what will later become com-
mon wisdom: Computing must embrace fault tolerance
and robustness throughout the computational stack, all
the way to the end-user.
Extreme scale will lead the way.
REFERENCES
[1] K. Ferreira, J. Stearley, J. H. Laros III, R. Oldﬁeld, K. Pedretti,
R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold, “Eval-
uating the viability of process replication reliability for exascale
systems,” in Proceedings of 2011 International Conference for
High Performance Computing, Networking, Storage and Analy-
sis. ACM, 2011, p. 44.
[2] D. H. Ackley, “Beyond efﬁciency,” Communications of the ACM,
vol. 56, no. 10, pp. 38–40, 2013.
[3] 2013 43rd Annual IEEE/IFIP International Conference on De-
pendable Systems and Networks (DSN), Budapest, Hungary, June
24-27, 2013.
IEEE, 2013.
[4] A. Aviˇzienis, “Fault-tolerance and fault-intolerance: Comple-
mentary approaches to reliable computing,” SIGPLAN Not.,
vol. 10, no. 6, pp. 458–464, Apr. 1975. [Online]. Available:
http://doi.acm.org/10.1145/390016.808469
[5] S. K. Reinhardt and S. S. Mukherjee, “Transient fault detection
via simultaneous multithreading,” in ACM SIGARCH Computer
Architecture News, vol. 28, no. 2. ACM, 2000, pp. 25–36.
[6] T. Vijaykumar, I. Pomeranz, and K. Cheng, “Transient-fault
recovery using simultaneous multithreading,” ACM SIGARCH
Computer Architecture News, vol. 30, no. 2, pp. 87–98, 2002.
[7] M. Gomaa, C. Scarbrough, T. Vijaykumar, and I. Pomeranz,
“Transient-fault recovery for chip multiprocessors,” in Computer
Architecture, 2003. Proceedings. 30th Annual International Sym-
posium on.
IEEE, 2003, pp. 98–109.
[8] J. Ray, J. C. Hoe, and B. Falsaﬁ, “Dual use of superscalar
datapath for transient-fault detection and recovery,” in Microar-
chitecture, 2001. MICRO-34. Proceedings. 34th ACM/IEEE In-
ternational Symposium on.
IEEE, 2001, pp. 214–224.
[9] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I.
August, “Swift: Software implemented fault tolerance,” in Pro-
ceedings of the international symposium on Code generation and
optimization.
IEEE Computer Society, 2005, pp. 243–254.
[10] P. P. Shirvani, N. R. Saxena, and E. J. McCluskey, “Software-
implemented edac protection against seus,” Reliability, IEEE
Transactions on, vol. 49, no. 3, pp. 273–284, 2000.
[11] N. Oh, P. P. Shirvani, and E. J. McCluskey, “Error detection
by duplicated instructions in super-scalar processors,” Reliability,
IEEE Transactions on, vol. 51, no. 1, pp. 63–75, 2002.
[12] R. Venkatasubramanian, J. P. Hayes, and B. T. Murray, “Low-cost
on-line fault detection using control ﬂow assertions,” in On-Line
Testing Symposium, 2003. IOLTS 2003. 9th IEEE.
IEEE, 2003,
pp. 137–143.
[13] J. F. Meyer, “On evaluating the performability of degradable
computing systems,” Computers, IEEE Transactions on, vol. 100,
no. 8, pp. 720–731, 1980.
[14] R. Ghosh, K. S. Trivedi, V. K. Naik, and D. S. Kim, “End-to-end
performability analysis for infrastructure-as-a-service cloud: An
interacting stochastic models approach,” in Dependable Comput-
ing (PRDC), 2010 IEEE 16th Paciﬁc Rim International Sympo-
sium on.
IEEE, 2010, pp. 125–132.
[15] J. P. Sterbenz, E. K. C¸ etinkaya, M. A. Hameed, A. Jabbar,
S. Qian, and J. P. Rohrer, “Evaluation of network resilience, sur-
vivability, and disruption tolerance: analysis, topology generation,
simulation, and experimentation,” Telecommunication systems,
vol. 52, no. 2, pp. 705–736, 2013.
[16] J. Elliott, M. Hoemmen, and F. Mueller, “Resilience in numerical
methods: A position on fault models and methodologies,” arXiv
preprint arXiv:1401.3013, 2014.
[17] K. V. Palem, L. N. Chakrapani, Z. M. Kedem, A. Lingamneni,
and K. K. Muntimadugu, “Sustaining moore’s law in embedded
computing through probabilistic and approximate design: retro-
spects and prospects,” in Proceedings of the 2009 international
conference on Compilers, architecture, and synthesis for embed-
ded systems. ACM, 2009, pp. 1–10.
[18] V. K. Chippa, S. T. Chakradhar, K. Roy, and A. Raghunathan,
“Analysis and characterization of inherent application resilience
for approximate computing,” in Proceedings of the 50th Annual
Design Automation Conference. ACM, 2013, p. 113.
[19] N. Nethercote and J. Seward, “Valgrind: a framework for heavy-
weight dynamic binary instrumentation,” ACM Sigplan Notices,
vol. 42, no. 6, pp. 89–100, 2007.
[20] J. Sloan, D. Kesler, R. Kumar, and A. Rahimi, “A numerical
optimization-based methodology for application robustiﬁcation:
Transforming applications for error tolerance,” in Dependable
Systems and Networks (DSN), 2010 IEEE/IFIP International
Conference on.
IEEE, 2010, pp. 161–170.
[21] V. Estivill-Castro and D. Wood, “A survey of adaptive sorting
algorithms,” ACM Computing Surveys (CSUR), vol. 24, no. 4,
pp. 441–476, 1992.
[22] R. E. Lyons and W. Vanderkulk, “The use of triple-modular
redundancy to improve computer reliability,” IBM J. Res. Dev.,
vol. 6, no. 2, pp. 200–209, Apr. 1962. [Online]. Available:
http://dx.doi.org/10.1147/rd.62.0200
[23] J. von Neumann, “The general and logical theory of automata,”
in Cerebral Mechanisms in Behaviour: the Hixon Symposium
(1948), L. A. Jeffress, Ed. Wiley, 1951, pp. 15–19, also appears
as pages 302–306 in A.H. Taub, editor, John von Neumann
Collected Works: Volume V – Design of Computers, Theory of
Automata and Numerical Analysis, Pergamon Press, 1963.
731
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:19 UTC from IEEE Xplore.  Restrictions apply.