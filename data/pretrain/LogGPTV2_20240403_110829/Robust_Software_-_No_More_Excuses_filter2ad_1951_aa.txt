title:Robust Software - No More Excuses
author:John DeVale and
Philip Koopman
Robust Software – No More Excuses
John DeVale
Intel Corporation
Intel Labs
PI:EMAIL
Philip Koopman
Carnegie Mellon University
ECE Department
PI:EMAIL
Abstract
Software developers identify two main reasons why soft-
ware systems are not made robust: performance and prac-
ticality.
This work demonstrates the effectiveness of
general techniques to improve robustness that are practical
and yield high performance. We present data from treating
three systems to improve robustness by a factor of 5 or
more, with a measured performance penalty of under 5% in
nearly every case, and usually under 2%.
We identify a third possible reason why software sys-
tems are not made robust: developer awareness. A case
study on three professional development groups evaluated
their ability to estimate the robustness of their software.
Two groups were able to estimate their software’s robust-
ness to some extent, while one group had more divergent
results. Although we can overcome the technical chal-
lenges, it appears that even experienced developers can
benefit from tools to locate robustness failures and training
in robustness issues.
1. Introduction
As our society becomes more dependent on the complex
interactions among electronic systems, the ability of these
systems to tolerate defects, errors, and exceptions is critical
to achieving service goals. Every aspect of life is becoming
dependent on computers, and the software that runs on
them. From banking to traffic control, weapons systems to
a trip to the grocery store, the things we take for granted are
now irrevocably tied to the correct functionality of comput-
ing systems.
This is not an entirely unfamiliar problem. Military,
aerospace, medical, and financial systems have always
been built to be as tolerant of faults as practical. Though
they have not always been as robust as their designers may
have hoped [21] [27] [28], the effort to build robust systems
was made.
Unfortunately, it is not unusual for system developers to
pay too little attention to the need for building robust sys-
tems. Operating systems have shown to have at times poor
robustness [24][12]. Commercial distributed computing
frameworks such as CORBA client software tend to exhibit
problems as well [31]. Even complex military distributed
simulation frameworks have a lower, but significant, rate of
robustness problems [9].
The past few years have seen a number of research ef-
forts that measure, to some degree, some aspect of software
fault tolerance or robustness [22][6][16][4]. Although none
of these tools can reasonably be expected to be the ultimate
authority on the measurement of system reliability, each
can provide developers useful insight into some aspects of
the system that arguably contribute to reliability.
Although the availability of these tools might lead one to
expect that system developers would use them to steadily
improve, this does not seem to be the case. While some
groups have pursued using such tools to improve their sys-
tems, many more do not. We found it puzzling that some
developers would not be interested in improving their prod-
ucts, and strove to understand the phenomenon.
In the course of the work done throughout the Ballista
Project, we were afforded the opportunity to interact with a
number of development groups. Some of the systems these
groups were developing fall within the more traditional ar-
eas accustomed to the need for fault tolerance (military),
while others were in areas whose need for fault tolerance
and robust exception handling are only now being realized.
These interactions provided some insight as to why some
groups were reluctant to fix issues unless they could be
traced directly to a total system failure.
Among the developers with whom we interacted, the
primary concerns cited to justify not fixing all robustness
problems found were: performance and practicality. Both
of these issues bear close scrutiny.
It is readily apparent
that the need for any approach to address robustness issues
must be easy to implement to reduce development cost.
Additionally, run time performance must be fast on all plat-
forms, and built solely on top of existing hardware without
the need for architectural changes to enhance performance.
This is largely because the majority of systems being built
use commodity hardware and processors, which are usually
manufactured with speed, cost, and quality in mind rather
than supporting software fault tolerance.
To address the developer’s concerns we demonstrate the
effectiveness of general techniques to improve robustness
that are practical and yield high performance. We present
data from treating three systems to remove all detectable ro-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:55 UTC from IEEE Xplore.  Restrictions apply. 
bustness failures, with a measured performance penalty of
under 5% in nearly every case, and usually under 2%.
Although we can demonstrate solutions to the technical
challenges involved in building robust systems, there re-
mains the issue of familiarity, or awareness first explored
by Maxion in [30]. Maxion concludes that through the ex-
posure to a few simple ideas and concepts, student pro-
gramming groups will produce more robust code. The
obvious observation that can be made is that during their
course work, the students in Maxion’s study had not been
effectively exposed to the types of exceptional conditions
that occur in real world applications, or potential methods
of dealing with them. One might wonder if such a result is
indicative of any developer, or just students. Put another
way, are professional development teams any more capable
of building robust code than the control groups in Maxion’s
study?
To gain insight into this issue we present data from a
case study performed using 3 professional development
teams. The study attempts to determine how well the devel-
opers are able to predict the exception handling characteris-
tics of their code. If their predictions were accurate, one
might conclude that given the proper tools, and time they
could build robust software systems. If they were not accu-
rate, it is uncertain that they would be able to develop a ro-
bust system, even if they were asked to do so.
2. Previous Work
The literature on exceptions and exception handling is
vast. Exception handling has been studied since the incept
of computing, and is important for detecting and dealing
with not only problems and errors, but also expected condi-
tions. The research falls largely into three major categories
with respect to exception handling: how to describe it; how
to perform it; and how to do it quickly.
2.1. Describing Exception Handling
Exception handling code can be difficult to represent in
terms of design and documentation, largely because it gen-
erally falls outside normal program flow, and can occur at
virtually any point in a program. Accordingly, a large body
of work has been created to help develop better methods to
describe, design and document the exception handling fa-
cilities of a software system.
Early work strove to discover multiple ways to handle
exceptional conditions [17] [13]. Over the years two meth-
ods have come to dominate current
implementations.
These methods are the termination model and the resump-
tion model [11].
to the
In current systems the two main exception handling
models manifest themselves as error return codes and sig-
nals. It has been argued that the termination model is supe-
rior
the
implementation of resumption model semantics via signals
in operating systems provides only large-grain control of
signal handling, typically at the task level resulting in the
termination of the process (e.g. SIGSEGV). This can make
resumption model
[5].
Indeed,
it difficult to diagnose and recover from a problem, and is a
concern in real-time systems that cannot afford large-scale
disruptions in program execution.
Implementations of the termination model typically re-
quire a software module to return an error code (or set an er-
ror flag variable such as
in POSIX) in the event of an
exceptional condition. For instance a function that includes
a division operation might return a divide by zero error code
if the divisor were zero. The calling program could then de-
termine that an exception occurred, what it was, and per-
haps determine how to recover
POSIX
standardizes ways to use error codes, and thus provides por-
table support for the error return model in building robust
systems [20].
from it.
At a higher level of abstraction, several formal frame-
works for representing exception handling and recovery
have been developed [18]. These methods attempt to build
an exception handling framework that is easy to use and un-
derstand around a transactional workflow system.
Highly hierarchical object oriented approaches seek to
build flexible and easy to use frameworks that bridge the
gap between representation and implementation [8]. Yet
another approach is to use computational reflection to
separate the exception handling code from the normal com-
putational code [10].
2.2 Performing Exception Handling
Exception handling mechanisms can often make code
generation and understanding difficult. This is a problem
throughout the development lifecycle. Easing the burden of
developing, testing, and maintaining software with excep-
tion handling constructs through better code representation
is important for not only reducing costs, but improving
product quality. Consequently, there is a large field of re-
lated work.
One common way of easing the burden of writing effec-
tive exception handling code is through code and macro li-
braries. This type of approach has the benefit of being
easily assimilated into existing projects, and allows devel-
opers to use traditional programming languages [26] [19]
[15] [3]. More aggressive approaches go beyond simple
compiler constructs build entire frameworks [14] [33] or
language constructs [29].
The focus of this research is more along the lines of iden-
tifying exceptional conditions before an exception is gener-
ated (in an efficient manner), rather than developing
exception handling mechanisms that are easier to use. As
such, the most closely related work is Xept [36]. The Xept
method is a way in which error checking can be encapsu-
lated in a wrapper, reducing flow-of-control disruption and
improving modularity. It uses a tool set to facilitate inter-
cepting function calls to third party libraries to perform er-
ror checking. Xept is a somewhat automated version of the
relatively common manual “wrapper” technique used in
many high availability military systems.
Xept has influenced the research presented here, and the
work leading up to it. The research presented here uses the
idea of avoiding exception generation in order to harden a
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:55 UTC from IEEE Xplore.  Restrictions apply. 
software interface against robustness failures. Further, it
explores the practical limits of such hardening, in terms of
detection capabilities and performance cost. In some cases,
a tool such as Xept might work well as a mechanism for im-
plementing checks discussed in our work. Unfortunately it
does incur the overhead of at least one additional function
call in addition to the tests performed per protected code
segment. Though the Xept check functions can not be
inlined due to the structure of its call-intercept methodol-
ogy, it is not difficult to imagine practical modifications to
the technology that would allow the inlining optimization.
2.3 High Performance Exception Handling
In today’s high performance culture, the desire for fast
exception handling is obvious. Once exceptions are gener-
ated, it can be difficult to recover from them in a robust
fashion. The previous work discussed in this section
largely focuses on generating, propagating, and handling
exceptions as quickly as possible. That is complementary
to the work presented herein. This work is mainly inter-
ested in developing methods of including enhanced error
detection in software systems to detect incipient excep-
tional conditions to the maximum extent possible before
they generate exceptions, and to do so without sacrificing
performance.
Exception delivery cost can be substantial, especially in
heavily layered operating systems where the exception
needs to propagate through many subsystems to reach the
handling program.
In [35], the authors present a hard-
ware/software solution that can reduce delivery cost by an
order of magnitude. In [38], the use of multithreading is ex-
plored to handle hardware exceptions such as TLB misses
without squashing the main instruction thread. The work
presented herein may benefit from multithreading technol-
ogies that are beginning to emerge in new commercial pro-
cessor designs by allowing error checking threads to run in
parallel. However, synchronizing checking threads with the
main execution thread may prove to be costly in terms of
execution overhead, and certainly machine resources. This
work performs checks in the main thread, building them
such that processors using enhanced multiple branch pre-
diction hardware[32] and block caches[2] can simply exe-
cute checks in parallel and speculatively bypass them with
little or no performance cost.
In [37] the authors propose a hardware architecture to al-
low the rapid validation of software and hardware memory
accesses. Their proposed architecture imposed only a 2%
speed penalty. Unfortunately, the authors also determined
that without the special hardware, the scheme was too
costly to implement in software alone. Other work pro-
poses a code transformation technique to detect memory
exceptions,
resulting in performance overheads of
130%-540% [1].
This work expands on these ideas in some key areas. It
creates a simple, generically applicable construct for excep-
tion detection. It quantifies the performance cost of using
the construct to provide robust exception detection and han-
dling, and it discusses ways in which emerging micropro-
cessor technologies will improve the construct’s perfor-
mance even further.
3. Methodology
One of the problems with simply allowing an exception
to occur and cleaning up after it later is the questionable via-
bility of any post-exception cleanup effort. The POSIX
standard does not guarantee process state after signal deliv-
ery, and it is possible that the function in which the excep-
tion occurred was altering the system state in a way that is
impossible or difficult to undo. Additionally, some proces-
sors on the market do not support precise exceptions.
Given these issues, there is little guarantee that a portable
program can recover from an exceptional condition in a
graceful fashion.
For this reason our approach is to detect all possible ex-
ceptional conditions before any calculations are performed
or actions are taken. This is accomplished through the rig-
orous validation of input data. Once the data has been vali-
dated, processing can continue as normal.
The result of this preemptive detection is that the process
has the opportunity to respond gracefully to exceptional
conditions before process state is altered. Addressing the
issue of providing the mechanisms to handle exceptional
conditions within applications is beyond the scope of this
work, although we note that in many cases a simple retry
can be effective. However, the main point of this work is to
provide the opportunity to handle exceptions without incur-
ring the overhead of a process restart, whereas in the past
there was no portable way to do so in practice.
A generically applicable method for hardening a soft-
ware interface is to harden each element of a function’s in-
coming parameters. This corresponds to creating wrappers
on a per-parameter basis. We have found that linking hard-
ening code to data types is a scalable approach, and mirrors
the abstraction of testing to data types within the software
interface as is done in the Ballista testing service [6].
The main benefit from basing wrapper checks on data
type is enhancing the modularity and reuse of checks.
Checks can be completely encapsulated in a function call,
ReturnType function foo(dataTypeA a)
{
if (!checkDataTypeA(a))
{
return ErrorCondition
}
.
.
.
Perform normal calculations
.
.
.
Return result
}
Figure 1. Pseudocode illustrating entry
checks for exceptional conditions.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:55 UTC from IEEE Xplore.  Restrictions apply. 
and used as needed to harden against robustness fail-
ures. Upon entering any hardened function, a wrap-
per function is invoked for each of the incoming
parameter values (see Figure 1). There are some in-
stances where such an abstraction breaks, for in-
stance when values may hold special context or
exceptional values dependent on the functionality of
the method being protected. Nonetheless, even when
this occurs the function can be hardened with a
slightly customized version of the generic hardening
procedure at small cost. (This is analogous of creat-
ing custom parameter tests for application-specific
data types based on inheritance from generic data
types, as is done in the Ballista test harness.)
One of the problems encountered when fixing robust-
ness issues due to memory problems (e.g. improperly allo-
cated, or uninitialized memory) is lack of information
accessible by user level programs. Although most systems
and memory allocation schemes track the information
needed to determine if a memory block is exceptional or
not, the information is generally not exposed to the user.
This fundamentally limits how well a developer can detect
memory related exceptional conditions.
To remove this fundamental limitation we slightly mod-
ified malloc() and provided function hooks that could read
the data needed to validate dynamically allocated memory
at the user level. Such a technique could be used to store
and retrieve other context dependent information pertain-
ing to the memory block. For our purposes, it was sufficient