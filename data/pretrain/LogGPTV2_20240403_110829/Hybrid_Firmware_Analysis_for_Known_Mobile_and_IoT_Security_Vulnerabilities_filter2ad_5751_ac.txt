3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Feature Name
binary deﬁned fun call num
min stack depth
max stack depth
avg stack depth
std stack depth
instruction num
unique instruction num
call instruction num
arithmetic instruction num
branch instruction num
load instruction num
store instruction num
max branch frequency
max arith frequency
mem heap access
mem stack access
mem lib access
mem anon access
mem others access
library call num
syscall num
TABLE II: Dynamic features used in PATCHECKO.
Feature Description
number of binary-deﬁned function calls during execution
the minimal stack depth during execution
the maximal stack depth during execution
the average stack depth during execution
the standard deviation stack depth during execution
number of executed instruction
number of executed unique instruction
number of call instruction
number of arithmetic instruction
number of branch instruction
number of load instruction
number of store instruction
the maximal number of frequency of the executed same branch instruction
the maximal number of frequency of the executed same arithmetic instruction
number of accessing heap memory space
number of accessing stack memory space
number of accessing library memory space
number of accessing anonymous mapping memory space
number of accessing others part memory space
number of library function calls during execution
number of system calls during execution
Now that we have the capability of extracting dynamic
features of a target function, we next present the algorithm
for calculating function similarity for a given pair of functions
and their extracted feature sets.
C. Calculating Function Semantic Similarity
For each function pair, (f, g), PATCHECKO computes a se-
mantic similarity measure based on the dynamic feature vector
distance between the two functions. Distance has been used
in data mining contexts with dimensions representing features
of the objects. In particular, PATCHECKO uses Minkowski dis-
tance [37] as our similarity measures based on each function’s
feature vector. Different behaviors result in slightly different
values of the corresponding coordinates in the feature vectors.
We now explore the distance measure in detail.
The Minkowski distance is a generalized form of the Eu-
clidean distance (if p=2) and of the Manhattan distance (if
p=1). In our case, we set p=3 for Minkowski distance. The
general equation is as follows,
(cid:4)1/p
dk(f, g) =
|xi − yi|p
.
(1)
(cid:2)
n(cid:3)
i=1
In Minkowski distance equation, f represents the CVE func-
tion and g represents the candidate function in the target
ﬁrmware. k is the k-th execution environment used. x rep-
resents the dynamic feature vector of f and y represents the
dynamic feature vector of g. P is set to 3.
We compute the similarity of each pair of (f,g) in multiple
execution environments. So we compute their similarity by
averaging the similarity distance over the execution environ-
ments. We set K as the number of execution environments
used. We deﬁne
sim(f, g) =
1
K
K(cid:3)
i=1
dk(f, g).
(2)
Finally, we feed the dynamic feature vector of each candi-
date function into the similarity computing equation. We can
378
get a list of ranking (function, similarity distance) pairs (see
Figure 5). This is the ﬁnal component for the identiﬁcation
of known vulnerabilities. We now design the ﬁnal component
that allows us to perform patch presence detection.
D. Patch Detection
We noticed that a patch typically introduces few changes
to a vulnerable function. However, these minor changes can
still have a signiﬁcant impact to make the pre- and post-patch
functions dissimilar - this intuition is conﬁrmed in Section V.
Based on this notion, PATCHECKO uses a differential engine
to collect both static and dynamic similarity measures in order
to determine if a vulnerable function has been patched.
Given a vulnerable function fv, a patched function fp, and
a target function ft, the differential engine will ﬁrst generate
three values: the static features of fv, fp, and ft, and the
dynamic semantic similarity scores of simv vs. simt and
simp vs. simt, as well as the differential signature between
Sv and Sp. The static features are the same aforementioned
48 different quantiﬁed features and the dynamic semantic
similarity scores are the aforementioned function similarity
metrics. The differential signatures are an additional metric
that compares the CFG structures, i.e., the CFG topologies
of two functions as well as the semantic information, e.g.,
function parameters, local variables, and library function calls.
IV. IMPLEMENTATION AND CASE-STUDY
We implemented the PATCHECKO framework on Ubuntu
18.04 in its AMD64 ﬂavor. Our experiments are conducted
on a server equipped with one Intel Xeon E51650v4 CPU
running at 128 GB memory, 2TB SSD, and 4 NVIDIA 1080
Ti GPU. During both training and evaluation, 4 GPU cards
were used. As in the design, PATCHECKO consists of four
main components: a feature extractor, a deep learning model,
a dynamic analysis engine, and a differential analysis engine
for patch detection.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:40 UTC from IEEE Xplore.  Restrictions apply. 
A. Feature Extractor and Deep Learning
The input for the feature extractor is the disassembled binary
code of the target function. We assume the availability and
the correctness of function boundaries by building on top
of IDA Pro [19], a commercial disassembler tool used for
extracting binary program features. As such, we implemented
the feature extractor as a plugin for IDA Pro. We developed
two versions of the plugin: a GUI-version and command line-
version (for automation). Since PATCHECKO works on cross-
platform binaries, the plugin can support different architectures
(x86, amd64 and ARM 32/64 bit) for feature extraction.
We implement the neural network modeling, training and
classiﬁcation based on Keras [8] and TensorFlow [2]. We use
TensorBoard [6] to visualize the whole training procedure.
B. Dynamic Analysis Engine
As was mentioned in the design section, the key challenges
for dynamic analysis are the preparation of the inputs for the
engine as well as the instrumentation of target functions for
tracing dynamic information.
Input preparation. As was mentioned in Section III-B,
PATCHECKO needs to efﬁciently prepare the execution en-
vironment. To perform dynamic analysis without having to
load the entire binary, we utilize DLL injection to execute
compact execution binaries that correspond to a single target
function. In particular, we use the dynamic loading function
(e.g. dlopen()) to load the dynamic shared object binary ﬁle
which returns an opaque “handle” for the loaded object. This
handle is employed with other useful functions in the dlopen
API, such as dlsym. Using dlsym, we can directly ﬁnd the
exported functions based on the exported function’s name. We
then execute the targeted function.
Of course, a library binary will contain a large number of
different functions, some of which are non-exported functions.
As such, we must ﬁnd a way to export these functions for
further analysis. PATCHECKO uses LIEF [36] to export func-
tions into executable binaries. Such a transformation allows
PATCHECKO to instrument a candidate function that was found
at a given address by using dlopen and dlsym. Thus, any
candidate function can be exported and executed without run-
ning the whole binary. This approach has excellent reliability
and efﬁciency, since we can focus on targeted function without
having to spawn the entire binary. Furthermore, we use Lib-
Fuzzer [22] to fuzz candidate functions and generate different
input sets. For the execution environment, we manually choose
concrete initial values for different global variables.
Instrumentation. Because we are targeting heterogeneous
mobile/IoT ecosystems, we choose to implement the same in-
strumentation of PATCHECKO on two dynamic instrumentation
frameworks: IDA Pro and GDB. Speciﬁcally, we implemented
a plugin based on GDB and GDBServer for Android and
Android Things platforms, and a plugin based on IDA Pro
and debugserver for IOS platforms.
C. Case Study
To facilitate the understanding of our implementation de-
tails, we will provide an ongoing example to show how we
can locate a known CVE vulnerability and how we can ensure
whether the vulnerability has been patched or not patched in
Android Things ﬁrmwares. Android Things is an embedded
IoT-speciﬁc operating system platform by Google.
Known CVE vulnerability function discovery. We chose one
CVE vulnerability, CVE-2018-9412, from Android Security
Bulletins [34]. This is a DoS vulnerability in the function
removeUnsynchronization of the library libstagefright.
In order to simplify the case study, we generated these
binaries directly from the source codes of both the vulnerable
and patched libstagefright libraries. We compiled both
versions using Clang with optimization level O0. Although
PATCHECKO never uses the source code for its analysis,
Figure 6 shows the source code and assembly code of the
patched CVE-2018-9412 for illustration. We elaborate on
the components of this ﬁgure in the following subsection.
Generating a training dataset. We compiled 100 Android
libraries from their source code using Clang. The compiler
is set to emit code in x86, amd64, ARM 32-bit, and ARM
64-bit with optimization levels O0, O1, O2, O3, Oz, Ofast.
In total, we obtained 2,108 library binary ﬁles1. We provide
more details in Section V.
Feature extraction. We use our feature extraction plugin to
extract the features on top of IDA Pro. Once we get the raw
features, PATCHECKO will reﬁne the raw features to generate
the feature vector. PATCHECKO extracted all function features
from the library libstagefright.so and identiﬁed a
total of 5,646 functions and generated 5,646 function feature
vectors.
Vulnerability detection by deep learning. Once the features
are extracted, we use the training model for detection. We also
use the vulnerable and patched functions as a baseline. Our
model identiﬁed 252 candidate functions that are based on
the vulnerable function’s feature vector while generating 971
candidate functions based on the patched function’s feature
vector. We also compared the feature vectors of the vulnerable
and patched functions to check whether they are similar and
found them to be dissimilar, i.e., the patched version has
signiﬁcantly different features than the vulnerable version.
Looking at the source code in Figure 6, one can intuit that
the patched version is signiﬁcantly different. For instance, the
patch removed the memmove function and added one more
if condition for value checking. Similarly, one can observe
the difference in the number of basic blocks at the assembly
level.
Dynamic analysis engine. Not only are the numbers of
vulnerable candidate functions (252) and patched functions
(971) from the last step very large, but the candidate functions
are also very similar. As such, it would be difﬁcult to locate
the target vulnerability function by manual inspection. We
therefore use the dynamic analysis engine to generate dynamic
information for each function. We ﬁrst use LibFuzzer to
generate the different
inputs for the vulnerability function
removeUnsynchronization. We tested that these inputs
worked with both the vulnerable and patched functions. As
before, we use the input to test each candidate function and
remove any functions that crashed. Using the input-function
validation, we obtain 38 candidate functions for the vulnerable
function and 327 candidate functions for the patched func-
tion. For these candidate functions, PATCHECKO’s dynamic
1Some compiler optimization levels didn’t work for certain instances
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:40 UTC from IEEE Xplore.  Restrictions apply. 
379
(cid:7)(cid:20)(cid:15)(cid:16)(cid:12)(cid:18)(cid:8)(cid:9)(cid:15)(cid:12)(cid:1)(cid:2)(cid:6)(cid:4)(cid:1)(cid:3)(cid:17)(cid:11)(cid:12)
(cid:7)(cid:20)(cid:15)(cid:16)(cid:12)(cid:18)(cid:8)(cid:9)(cid:15)(cid:12)(cid:1)(cid:6)(cid:17)(cid:20)(cid:18)(cid:10)(cid:12)(cid:1)(cid:3)(cid:17)(cid:11)(cid:12)(cid:1)(cid:21)(cid:14)(cid:19)(cid:13)(cid:1)(cid:5)(cid:8)(cid:19)(cid:10)(cid:13)
(cid:5)(cid:8)(cid:19)(cid:10)(cid:13)(cid:12)(cid:11)(cid:1)(cid:2)(cid:6)(cid:4)(cid:1)(cid:3)(cid:17)(cid:11)(cid:12)
if (mData[i] == 0xff && mData[i + 1] == 0x00) {
memmove(&mData[i + 1], &mData[i + 2], mSize - i - 2);
--mSize;
void ID3::removeUnsynchronization() {
-
for (size_t i = 0; i + 1 < mSize; ++i) {
-
-
-
+
+ size_t writeOffset = 1;
+ for (size_t readOffset = 1; readOffset < mSize; ++readOffset) {
+ if (mData[readOffset - 1] == 0xff && mData[readOffset] == 0x00)
+       {
+
continue;
}
mData[writeOffset++] = mData[readOffset];
+
}
+
+ if (writeOffset < mSize) {
+
mSize = writeOffset;
+ }
+
}
Fig. 6: Vulnerable code with the associated patch of CVE-2018-9412.
analysis engine will generate the dynamic information. For
instrumentation in Android Things, we use gdbserver to collect
the dynamic features on the Android Things device. Table III
shows the part of dynamic feature vector proﬁling for the
vulnerable candidate functions. In the next subsection, we
analyze why candidate_29 is the vulnerable function.
Calculating function similarity. We use the three aforemen-
tioned similarity metrics to calculate the function similarity.
The top 10 ranking results for the vulnerable function are listed
in Table IV and the top 10 ranking results for the patched func-
tions are listed in Table V. For the vulnerable function results,
we see that candidate_29 is the top-ranked candidate, i.e.,
according to the rule of similarity distance algorithm, if this
distance is small, there will be a high degree of similarity. We
can also see a signiﬁcant difference between the top candidate
and the second candidate (candidate_27). As such, we
conclude that candidate_29 is the vulnerable function.
Diving deeper into the results in Table III, we can observe
why the distance between the dynamic features is so small.
The two highlighted rows indicate candidate_29 and the
ground truth vulnerable function. Referring back to Table II,
we know that F 13 represents the max frequency for the same
branch instruction and F 14 represents the max frequency for
the same arithmetic instruction. We can that candidate_29
is the only candidate function that has the same frequency
numbers as the vulnerable function. It is important to note
that this analysis was only enabled by dynamic analysis–static
analysis would not have been able to identify these dynamic
features.
For the patched case, Table V only shows the results for the
top 10 ranking candidate functions due to page limitations. In
this case, candidate_102, on average, is the top-ranked
candidate despite being the incorrect function. However, we
can see that candidate_29 is ranked in a very close second,
while there is a signiﬁcant difference with the third candidate.
Intuitively, we can narrow down the candidate functions to the
top two and can assume that candidate_29 is likely to be
the associated candidate vulnerable function. However, at this
point we cannot tell whether the function is patched.
Differential analysis engine. According the previous steps,
we can consider candidate_29 is the target function. But
it is still not clear whether it is patched. We collect static
features (e.g. j___aeabi_memmove), dynamic semantic
similarity scores (34.7 V.S. 65.6), and the differential
signatures (j___aeabi_memmove, if condition). Based on
these metrics, the differential analysis engine concludes the
target function is still vulnerable and not patched.
V. EVALUATION
In this section, we evaluate PATCHECKO with respect to its
search accuracy and computation efﬁciency. In particular, we
evaluate the accuracy of our deep learning model, the dynamic
analysis engine, and the differential analysis engine using a
dataset containing ground truth.
A. Data Preparation
In our evaluation, we collected three datasets: 1) Dataset I
for training the deep learning model and evaluating the accu-
racy of the deep learning model; 2) Dataset II for collecting
known CVE vulnerabilities and for building our vulnerability
database. 3) Dataset III for evaluating the accuracy and per-
formance of the deep learning model, the dynamic analysis
engine, and the differential analysis engine for real world
mobile/IoT ﬁrmware;
Dataset I: This dataset is used for neural network training
and baseline comparison. It consists of binaries compiled
from source code, providing us with the ground truth. We
consider two functions compiled from the same source code
function are similar, and dissimilar if they are from different
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:40 UTC from IEEE Xplore.  Restrictions apply. 
380
the vulnerable version of
TABLE III: The dynamic feature vector proﬁling for part of candidate functions of
removeUnsynchronization in the library libstagefright.so. F 1,...,F 21 represents different dynamic features 1
to 21 showed in Table II. In the last row, the vulnerable function is from our vulnerability database.
F 17
0
0
0
0
0
0
0
0