# AI与安全系列（2）：破坏模型完整性——数据投毒攻击

##### 译文声明
本文为翻译文章，仅供参考。具体内容和含义以原文为准。

## 前言
本文是《AI与安全》系列文章的第三篇。在前文中，我们介绍了黑客对AI发起的攻击的基本概念，主要分为三类：破坏模型完整性、可用性和机密性的攻击。其中，破坏模型完整性的攻击又可分为两类：针对模型的逃逸攻击和针对数据的数据投毒攻击。本文将重点介绍数据投毒攻击。

## 1. 数据投毒攻击概述
数据投毒攻击是指通过向训练数据中注入恶意数据，从而影响模型的性能和准确性。以下是一些具体示例：

- **无人驾驶汽车**：与直接构造对抗样本的逃逸攻击不同，数据投毒攻击可以通过输入伪造的实时数据或历史数据来干扰智能汽车的路径识别模型，使其按照攻击者的意图行驶。
- **问答式机器人**：如微软小冰、QQ小冰等智能交互AI，通过大量语料库进行学习，并将用户对话数据纳入训练集。攻击者可以通过故意“调教”这些AI，使其说出不当言论或发表敏感内容。
- **业务安全领域**：阿里集团安全资深总监路全曾在CCF大会上分享了一个案例，说明爬虫如何通过不断发送低级流量来污染模型，进而让高级爬虫绕过防御机制。

## 2. 数据投毒的根本原因
数据投毒的根本原因在于传统的机器学习方法假设输入数据是正确的，没有考虑有人会故意篡改数据分布。例如，在时间滑窗生成模型时，我们通常期望今天的数据与上周的数据分布相似。然而，这种假设给攻击者提供了机会，他们可以通过注入恶意数据来改变模型的行为。

## 3. 数据投毒攻击分类及解决方案
### 3.1 模型偏斜 (Model Skewing)
模型偏斜是指攻击者通过污染训练数据，使模型对好坏输入的分类发生偏移，从而降低其准确性。例如，Google反滥用研究团队曾报告称，一些垃圾邮件制造者试图通过将大量垃圾邮件标记为非垃圾邮件来干扰Gmail的分类器。

#### 解决方案
1. **合理数据采样**：确保少数实体（如IP或用户）不会占训练数据的大部分。可以限制每个用户贡献的样本数量，或使用衰减权重处理大量样本。
2. **模型比较**：新训练的分类器应与旧分类器进行对比，评估变化程度。可以进行暗启动测试，在相同流量上比较两个模型的输出结果，或对部分流量进行A/B测试和回溯测试。
3. **标准数据集**：构建包含精心策划的攻击和正常数据的标准数据集。只有当模型在该数据集上的表现达标时，才能投入生产环境。

### 3.2 反馈武器化 (Feedback Weaponization)
反馈武器化是指攻击者利用用户反馈系统来攻击合法用户和内容。例如，在智能客服系统中，攻击者可以通过持续给予正确答案低评分和错误答案高评分来干扰模型。

#### 解决方案
- 不要在反馈和惩罚之间建立直接循环。应在对模型进行惩罚前，验证反馈的真实性，并结合其他特征进行综合评估。

### 3.3 后门攻击 (Backdoor Attacks)
后门攻击是一种特殊的模型投毒攻击，通过训练得到深度神经网络中的隐藏模式。当且仅当输入为特定触发样本时，模型才会产生特定的隐藏行为；否则，模型表现正常。例如，在MNIST手写数据集中，有学者通过数据投毒方法成功注入了后门，达到99%以上的攻击成功率，但不影响正常样本的识别性能。

## 补充阅读
[1] Xiao H, Biggio B, Brown G, et al. Is feature selection secure against training data poisoning?[C]//International Conference on Machine Learning. 2015: 1689-1698.  
[2] Li B, Wang Y, Singh A, et al. Data poisoning attacks on factorization-based collaborative filtering[C]//Advances in neural information processing systems. 2016: 1885-1893.  
[3] Alfeld S, Zhu X, Barford P. Data Poisoning Attacks against Autoregressive Models[C]//AAAI. 2016: 1452-1458.  
[4] Marsh D O, Myers G J, Clarkson T W, et al. Fetal methylmercury poisoning: clinical and toxicological data on 29 cases[J]. Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society, 1980, 7(4): 348-353.  
[5] 陈宇飞, 沈超, 王骞,等. 人工智能系统安全与隐私风险[J]. 计算机研究与发展, 2019, 56(10).