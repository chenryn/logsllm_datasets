to 1: Pi p(xi|xj) = 1 for all j, which means that the like-
lihoods must be normalized.
4.4 Training the Spectrogram Model
Training this model means estimating the optimal Θ for
Equation (4) and is not as straight forward as training a
single Markov-chain since multiple chains are interacting
when attemping to ﬁt the data. For a single submodel, the
likelihood function is concave in the data and parameters,
indicating that a single optimal solution for the parameter
setting θ exists. This solution can be recovered by setting
the ﬁrst order derivatives of the likelihood function to zero
and solving and resolves to simply counting the number of
gram-to-gram transitions within the data then normalizing
the matrices so that each row sums to 1. This was done
previously for single-gram transition models [15, 16]. The
likelihood space for a mixture of Markov-chains, however,
is not concave in the parameters and data – a linear mixture
of concave functions does not preserve concavity. This re-
moves the single-optimal-solution property and makes solv-
ing for the optimal parameter setting more complex. To
train a mixture model, we utilize an ML procedure known
as Expectation Maximization (EM). The approach utilizes
a gradient ascent algorithm to iteratively maximize a lower
bound on the likelihood function until no improvement is
noted with respect to the estimated parameters.
Let p(D|Θ) denote the likelihood of observing a dataset
of independent training samples, represented as D. From
Bayes Theorem it’s known that the optimal setting for pa-
rameter Θ is the one that maximizes the joint likelihood of
the observation set. If p(D|θ) is concave in parameter space
then there is a unique optimal solution which can be recov-
ered by solving the gradient of p(D|Θ) to zero and solving
for Θ. As previously mentioned, for a single Markov-chain,
this is simple. P (D|Θ) for mixture of Markov-chains, how-
ever, is not concave in the joint parameter and data space
– the hidden states, while increasing model capacity, also
remove the concavity property since summations over con-
cave functions do not preserve concavity. This removes
the guarantee of the existence of a unique optimal solution.
To train this mixture model, we must instead make use of
an alternative machine learning procedure known as Ex-
pectation Maximization (EM). EM is a popular parameter
estimation technique that was ﬁrst introduced in the sim-
pler form by Dempster et. al. [8] in 1977. It is a proce-
dure for optimizing non-concave functions through gradi-
ent ascent and contains two core steps: the Expectation
step (E-step) calculates the joint likelihood of observing the
training set given current estimates of model parameters Θ
and the Maximization step (M-step) ﬁnds the gradient
of a concave lower-bound on the likelihood function and
moves the parameter estimate in that direction. At each
steps, the model estimates are updated in the direction of
the gradient thus maximizing P (D|θ) in an iterative proce-
dure. These two steps guarantee monotonic convergence to
a local-maximum for P (D|θ) and can be alternated until no
likelihood-gain is achieved. The EM update rules must be
derived on a per model basis. We describe the EM update
rules for the Spectrogram mixture model below:
E-STEP: In the E-step, we need to solve P (D|Θ) for
our mixture of Markov-chains model. The likelihood of ob-
serving D is the product of the likelihoods of the individual
samples:
pG(D|Θ) =
|D|
Y
d=1
pG(xd|Θ)
(5)
The bold faced variable xd denotes a string of arbitrary
length in D. Next, a lower bound on the expected
value is needed. This can be recovered with Jensen’s
inequality which states that given a concave function
f (x), we have the identity f (P x) ≥ P f (x). Using log
for f (x), instead of solving for Equation (5) directly, we
can solve log(cid:16)Q|D|
d=1 pG(xd|Θ)(cid:17). This makes ﬁnding the
gradient more tractable. Since logarithms are monotonic
transformations, the optimal Θ is equivalent for both func-
tions:
arg max
Θ
log pG(D|Θ) = arg max
Θ
pG(D|Θ)
This means that maximizing the equation in log-space
yields the same solution as in the original space. Next, we
plug Equation (3) into Equation (5) and solve for the new
likelihood function.
log pG(D|Θ) = log
|D|
Y
d=1
pG(xd|Θ)
=
|D|
X
d=1
log
M
X
s=1
πs
N
G−1
Y
i=G
Y
j=i
p(xi|xi−j ; θs)
1/N
(6)
(7)
|D|
X
d=1
≥
M
X
s=1
log πs +
1
N
N
G−1
X
i=G
X
j=i
log p(xd,i|xd,i−j; θs)
(8)
Equation (8) describes the new lower bound on the likeli-
hood functions which we have to maximize. The variable
xd,i indicates the ith character of sample string d. To reiter-
ate, p(xi|xj , θs) is a single value within the n − 1 matrices
of the sth chain – we are never doing more than retrieving
elements from multiple matrices and combining them. With
Equation (8), we conclude the derivations for the E-step of
the training algorithm.
M-STEP: The maximization step requires solving the
gradient of Equation (8) with respect to Θ and the mixing
weights {π1, .., πM}. Given the previously mentioned con-
straints on the transition matrix, that the rows need to sum to
1, Pi p(xi|xj ) = 1 for all j as well as the constraints on the
mixing weights Ps πs = 1, we need to use Lagrange mul-
tipliers to ﬁnd the stationary points under these constraints.
For brevity, we provide the ﬁnal solutions in this paper, the
full steps, including further discussions on how to improve
the model, will be made available at a later time on our web-
site.1 The M-STEP proceeds as follows: let τd,s denote the
log-likelihood of observing string xd given model θs.
τd,s =
1
N
N
G−1
X
i=G
X
j=i
log p(xd,i|xd,i−j; θs)
(9)
Each iteration of the EM algorithm shifts Θ in the direction
that improves p(D|Θ) the most. We used π† to denote how
1http://www.cs.columbia.edu/ids/
to update the mixing weights and θ† for the parameters of
the chains.
i = Q|D|
π†
d=1 πiτd,s
PM
j=1 Q|D|
d=1 πjτd,s
p†(xi|xj ; θs) =
p(xi|xj , θs) + P|D|
d=1 τd,s
j=1 (cid:16)p(xi|xj , θs) + P|D|
P256
d=1 τd,s(cid:17)
(10)
(11)
The entire training algorithm for Spectrogram’s statistical
model is to alternate between these two E and M steps.
The training algorithm is given below. Figure (4) shows
{π1, π2, .., πM}, Θ = {θ1, .., θM} ← randomly-initialize
for i = 2 to ITER-LIMIT
function train-spectrogram (D, G, M )
1
2 Z1 ← Equation (8)
3
4
5
6
7
8
return {π1, π2, .., πM , Θ}
Update {π1, .., πM} using Equation (9)
Update Θ using Equation (10)
Zi ← Equation (8)
if(Zi − Zi−1) < T then break
done
Figure 4. Spectrogram training with thresh(cid:173)
old T . The inputs are D – the dataset, G –
the desired gram size and M – the number of
Markov(cid:173)chains to use.
the pseudo-code for the training algorithm. The algorithm
accepts as input, the training dataset D, the gram-size G
and the number of Markov-chains M to use and the out-
put is the full mixture of Markov chains model. Recall
that G and M control the capacity/power of the model; in-
creasing their values allows the model to ﬁt the data more
tightly. The optimal settings should be recovered through
cross-validation.
5 Evaluation
We evaluated Spectrogram on two of our university’s
web-servers. One of the machines hosts our departmental
homepage and includes scripts for services such as a gate-
way to a tech-report database, student and faculty direc-
tory, search-engines, pages for department hosted confer-
ences, faculty homepages and their accompanying scripts
and content that one can typically associate with a com-
puter science department’s public facing web server. The
second server is a gateway to the homepages of several hun-
dred M.Sc and Ph.D students. We estimate at least several
dozen, if not a hundred different scripts running between the
two. In our experiments, a single Spectrogram mixture-
model is trained per server. Two approaches for evaluation
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
a
r
u
c
c
A
n
o
i
t
c
e
t
e
D
2−gram
3−gram
5−gram
7−gram
11−gram
13−gram
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
a
r
u
c
c
A
n
o
i
t
c
e
t
e
D
2−gram
3−gram
5−gram
7−gram
11−gram
13−gram
0
0
0.01
0.02
0.03
False−Positive Rate
0.04
0.05
0
0
0.01
0.02
0.03
False−Positive Rate
0.04
0.05
(a) Student server - SQL-Injection.
(b) Dept. server - SQL-Injection.
In all tables, earlier works [15, 16] are partially represented by the 2(cid:173)gram model.
Figure 5. ROC (cid:173) Spectrogram performance in defending two university servers against SQL injec(cid:173)
tions.
In this
case, small gram sizes worked well given that simple non(cid:173)obfuscated SQL injection use many non(cid:173)
alphanumeric characters.
are explored. With unbiased sampling, we extract only
the unique requests within our dataset. This is so we are
measuring the capacity of the classiﬁer and are not inﬂu-
enced by the distribution of the requests themselves. For
example, if a particular request can be classiﬁed correctly
and that request contributes to the majority of the observed
samples, then the FP rate would be biased in our favor and
vise-versa. With full sampling, we evaluate Spectrogram
using the complete dataset of requests seen, giving us a
look at the raw FP rate over the entire content stream; all
of the attack samples used in our experiments are unique.
Spectrogram does not use any attack samples in its train-
ing; only normal, legitimate input. The detections described
in this paper are over completely unseen attack code. When
evaluating FP rates, we also use unseen legitimate requests
given that, with unbiased sampling, each instance of a legit-
imate request is distinct. We split the dataset into disjoint
training and testing sets, thus for normal content, the sets
are disjoint as well. Since unique samples are used, the sen-
sor must infer structure and content normality and general-
ize to unseen samples by looking at subsets of acceptable
requests; in order to avoid false positives. Every experi-
ment result reported in this paper is derived from an average
of ﬁve independent trials where the datasets are completely
randomized between tests. For each trial, the dataset of nor-
mal requests is randomly split into 95% training and 5%
testing disjoint unique sets. All of the attack code is used
for each trial.
5.1 Evaluation Dataset
Our dataset includes roughly 6.85 million requests, col-
lected over the period of one month. To generate the train-
ing set, we normalized the strings in the manner described
in Section (5.3) and extracted only the unique samples. This
reduced the dataset to 15, 927 samples for the student server
and 3, 292 for the department server. We manually ex-
amined the data to ensure that it was free of recognizable
attacks (data sanitization is discussed later). The attack
dataset included: 637 PHP local and remote ﬁle inclusion
attacks, 103 Javascript XSS attacks, 309 SQL-injection at-
tacks. We further generated another 2000 unique shellcode
samples using four of the strongest polymorphic engines
from the Metasploit framework, determined using pre-
viously published methods [30].