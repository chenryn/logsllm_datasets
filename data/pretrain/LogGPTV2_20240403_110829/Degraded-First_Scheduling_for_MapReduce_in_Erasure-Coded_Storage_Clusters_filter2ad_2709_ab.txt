40
(a) locality-ﬁrst scheduling
Map Slot
Map phase finishes
Node 2
Node 3
Node 4
Node 5
Slot 1
Slot 2
Slot 1
Slot 2
Slot 1
Slot 2
Slot 1
Slot 2
Get P0,0
Proc B0,1
Proc B1,1
Proc B4,1
Get P2,0
Proc B2,1
Proc B3,1
Proc B5,1
Proc B0,0
Proc B4,0
Get P1,0
Proc B1,0
Proc B2,0
Proc B5,0
Get P3,0
Proc B3,0
0
10
20
30
40
(b) degraded-ﬁrst scheduling
Figure 3. Map-slot activities in the entire map phase.
Time(s)
The major issue of locality-ﬁrst scheduling in erasure-
coded storage in failure mode is that degraded tasks start
degraded reads together and compete for the network re-
sources to download blocks from other racks. Obviously,
the competition signiﬁcantly increases the overall duration
of degraded tasks.
From the example, we observe that at the earlier stage
of the map phase, while local tasks are being processed,
network resources are not fully utilized. Thus, it is natural
to move the launch of some degraded tasks ahead to take
advantage of the unused network resources, so as to relieve
the competition for network resources among degraded tasks
later. Let us revisit the example, and suppose that we move
ahead two degraded tasks for processing B0,0 and B2,0 to
the beginning of the map phase. Figure 3(b) shows the new
map-slot activities for the revised task scheduling scheme,
which we call degraded-ﬁrst scheduling. This eliminates the
competition for network resources, and reduces the duration
of the map phase from 40s to 30s, i.e., a 25% saving.
IV. DESIGN OF DEGRADED-FIRST SCHEDULING
We present the design of degraded-ﬁrst scheduling, whose
main idea is to move part of degraded tasks to the earlier
stage of the map phase. The advantages are two-fold. First,
the degraded tasks can take advantage of the unused network
resources while the local tasks are running. Second, we avoid
the network resource competition among degraded tasks at
the end of the map phase. In this section, we ﬁrst present
422422422
the basic version of degraded-ﬁrst scheduling. We then
conduct mathematical analysis to show the improvement
of degraded-ﬁrst scheduling over the default locality-ﬁrst
scheduling in Hadoop. Finally, we present the enhanced
version of degraded-ﬁrst scheduling that takes into account
the topological conﬁguration of the cluster.
A. Basic Design
Time(s)
Our primary design goal is to evenly spread the launch
of degraded tasks among the whole map phase. This design
goal follows two intuitions.
• Finish running all degraded tasks before all local tasks.
If some degraded tasks are not yet ﬁnished after all
local tasks are ﬁnished, they will be launched together
and compete for network resources for degraded reads.
• Keep degraded tasks separate. If two or more degraded
tasks run almost at the same time, they may compete
for network resources for degraded reads.
The key challenge here is how to determine the right
timing for launching degraded tasks, so that they are evenly
spread among the whole map phase. One possible solution is
to predict the overall running time of the whole map phase
and launch degraded tasks evenly within the predicted time
interval. However, this approach is difﬁcult to realize for two
reasons. First, different MapReduce jobs may have highly
varying processing time of a map task. Thus, it is difﬁcult
to accurately predict how long the whole map phase would
be. Second, even if we can make accurate predictions, it is
possible that no free map slots are available when a degraded
task is ready to launch. Thus, the launch of some degraded
tasks may be delayed, defeating the original purpose of
evenly spreading the degraded tasks.
Therefore, we propose a heuristic design that arranges
the launch of degraded tasks with respect to the propor-
tion of map tasks that have been launched. Algorithm 2
shows the pseudo-code of the basic version of degraded-ﬁrst
scheduling, which extends the default Algorithm 1. Given a
MapReduce job, we ﬁrst determine the total number of all
map tasks to be launched (denoted by M) and the total
number of degraded tasks to be launched (denoted by Md).
We also monitor the number of all map tasks that have been
launched and that of degraded tasks that have been launched,
denoted by m and md, respectively. Algorithm 2 launches
a degraded task with a higher priority than a local task if
the proportion of degraded tasks that have been launched
is no more than the proportion of all map tasks that have
been launched. In this way, we control the pace of launching
degraded tasks and have them launched evenly in the whole
map phase. It is worth noting that we assign at most one
degraded task for every heartbeat (see the if-condition in
Line 4), since launching more than one degraded task at the
same node will lead to competition for network resources
among these degraded tasks.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 2 Basic Degraded-First Scheduling
1: while a heartbeat comes from slave s do
2:
3:
4:
isDegradedTaskAssigned = false
for each running job j in the job list do
if isDegradedTaskAssigned == false and
s has a free map slot then
if j has an unassigned degraded task then
if m
M
≥ md
Md
then
assign a degraded task to s
isDegradedTaskAssigned = true
end if
end if
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: end while
end if
end for
end if
for each free map slot on slave s do
if j has an unassigned local task then
assign the local task to s
else if j has an unassigned remote task then
assign the remote task to s
We elaborate via an example how Algorithm 2 works.
Figure 4(a) illustrates a cluster that contains four slaves and
provides fault tolerance via a (4,2) erasure code. The pro-
cessed ﬁle has a total of 12 native blocks, with three native
blocks stored in each node. We ﬁx both the downloading
and processing times to be 10s as in Section III. To better
describe the main idea, we conﬁgure each node with one
map slot only. Suppose now Node 1 fails. Thus, there are
a total of 12 map tasks, and three of them are degraded
tasks for processing the lost blocks B0,0, B1,0 and B2,0.
We assume that the master assigns map tasks in the order
Nodes 2, 3, and 4. Figure 4(b) shows the execution ﬂow of
the whole map phase. According to Algorithm 2, the three
degraded tasks are assigned as the 1st, 5th, and 9th map
tasks, and the three degraded tasks are launched at 0s, 10s,
and 30s, respectively. We see that all the degraded tasks
do not compete for network resources for downloading the
parity blocks during degraded reads.
It is important to note that in normal mode (i.e., without
any node failure), there is no degraded task, and hence
degraded-ﬁrst scheduling operates the same way as locality-
ﬁrst scheduling (see Lines 12-18 of Algorithm 2).
To summarize, Algorithm 2 launches a degraded task with
a higher priority if appropriate. This is why we call the
algorithm “degraded-ﬁrst scheduling”.
B. Analysis
We conduct simple mathematical analysis to compare the
default locality-ﬁrst scheduling and our basic degraded-ﬁrst
scheduling in terms of the runtime of a MapReduce job.
Our goal is to provide preliminary insights into the potential
improvement of degraded-ﬁrst scheduling in failure mode.
Analysis setting. We ﬁrst describe the setting of our analy-
423423423
Switch
Switch
Switch
B0,0
B1,0
B2,0
P3,0
P4,0
P5,0
B0,1
P1,0
P2,0
B3,0
B4,0
P5,1
P0,0
B1,1
P2,1
B3,1
P4,1
B5,0
P0,1
P1,1
B2,1
P3,1
B4,1
B5,1
Node 1
Node 2
Node 3
Node 4
(a) Example setup.
Node
Node 2
Node 3
Node 4
(1)
Get P0,0
(2)
Proc B1,1
(3)
Proc B2,1
(6)
Proc B0,0
Proc B0,1
(4)
Proc B3,1
(5)
Get P1,0
(7)
Proc B5,0
Proc B1,0
(8)
Proc B3,0
(9)
Get P2,0
(10)
Proc B4,1
0
10
20
30
40
Map phase finishes
(11)
Proc B4,0
Proc B2,0
(12)
Proc B5,1
Time(s)
50
(b) Map slot activities of surviving nodes.
Figure 4. Example of the execution ﬂow of the map phase based on the
basic version of degraded-ﬁrst scheduling (Algorithm 2). Each number in
the brackets represents the order of the blocks being assigned a map slot.
sis. We consider a cluster with N homogeneous nodes that
are evenly grouped into R racks (with N
R nodes each). Let
L be the number of map slots allocated for each node (i.e.,
it can run at most L map tasks simultaneously). Let T be
the processing time of a map task. Let S be the input block
size. Let W be the download bandwidth of each rack. For
fault tolerance, we use an (n, k) erasure code to encode k
native blocks to generate n − k parity blocks. We distribute
the stripes of n native and parity blocks evenly among the
N nodes (as in parity declustering [19]). Let F be the total
number of native blocks to be processed by MapReduce (i.e.,
the number of blocks in the each node is F
N ).
Suppose that we only focus on the map-only MapReduce
job, and neglect the shufﬂe overhead and the time for reduce
tasks. Then in normal mode (without any node failure), the
runtime of a MapReduce job is F T
N L .
We now consider the MapReduce performance in failure
mode. We focus on the case where a single node fails (see
our justiﬁcation in Section II). This implies that there are a
total of F map tasks, among which F
N are degraded tasks.
We assume that the degraded tasks are evenly distributed
among all racks, such that each rack contains F
N R degraded
tasks (assuming F
N R is an integer). When a degraded task
issues a degraded read to a lost block,
it downloads k
out of the n − 1 surviving blocks of the same stripe. We
assume that the degraded read is bottlenecked by inter-rack
trafﬁc. If the stripes are evenly distributed across all racks
and each degraded task randomly picks k out of n − 1
blocks to download,
then the expected time needed for
downloading blocks from other racks for each lost block
being reconstructed is (R−1)kS
RW .
Locality-ﬁrst scheduling. We ﬁrst consider the default
locality-ﬁrst scheduling. In failure mode, all degraded tasks
are launched after the completion of processing all local
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
 1.7
 1.6
 1.5
 1.4
 1.3
 1.2
 1.1
 1
e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
(8,6)
(12,9)
(16,12)
(20,15)
(n,k)
locality-first
degraded-first
 1.8
 1.7
 1.6
 1.5
 1.4
 1.3
 1.2
 1.1
 1
e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
720
1440
2160
2880
Number of blocks
locality-first
degraded-first
 6