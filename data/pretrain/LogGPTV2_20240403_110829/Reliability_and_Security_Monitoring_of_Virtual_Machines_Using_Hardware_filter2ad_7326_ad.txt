### Guest OS Hang Detection Coverage and Latency

#### 1. Detection Coverage
Figure 4 summarizes the detection coverage and percentages of partial and full hangs detected by GOSHD. Approximately 82% of injected faults resulted in hangs. Overall, GOSHD missed 24 failures out of 17,952 injections, resulting in a hang detection coverage of 99.8%.

Further analysis of the misclassified failures revealed that they were caused by fault locations repeatedly activated by the guest SSH server, which was used by our external probe to check for false alarms. As a result, although the SSH probe reported hangs, the kernel and other processes on the VM continued to execute normally.

On average, 18% to 26% of faults caused partial hangs on non-preemptible and preemptible kernels, respectively. This highlights the importance of partial hang detection. In many partial hang cases, the VM remained accessible from outside (e.g., via SSH connections), demonstrating the ineffectiveness of heartbeat-based hang detection methods, as the process responsible for generating heartbeats can still be operational and report the system as healthy.

Transient faults caused slightly more partial hangs than permanent faults in single-task workloads (Hanoi Tower and `make -j1`), but significantly more in concurrent multi-tasking workloads (`make -j2` and HTTP server). Persistent faults can be reactivated, causing more independent hanging threads.

Kernel preemption does not prevent hangs due to spinlocks, as most critical sections in the kernel are non-preemptible. However, preemption reduces the number of full hangs. For example, if two tasks T1 and T2 share a user-level lock `lu`, and T1 hangs while holding `lu` due to a fault in a kernel spinlock, T1 cannot be preempted, leading to a partial hang. In a non-preemptible kernel, T2 will also hang when attempting to acquire `lu`, resulting in a full hang. In a preemptible kernel, T2 can be preempted, maintaining a partial hang state.

#### 2. Detection Latency
Detection latency measures how quickly a detector identifies a problem. GOSHD raises an alarm when it detects that the guest OS scheduler has not scheduled processes for a predefined time (four seconds in our experiments). Detection latency is the time between fault activation and the moment GOSHD raises an alarm.

Figure 5 shows the detection latency of GOSHD for the same set of experiments. The blue line (triangles) indicates that GOSHD can detect more than 90% of hangs after four seconds and all hangs within 32 seconds. The red dashed line (circles) shows that only 54% of hangs result in a full hang after four seconds. Partial hang detection helps reduce full hang detection latency, allowing many full hangs to be detected tens of seconds earlier.

### Hidden Rootkit Detection (HRKD)

#### 1. HRKD Coverage
We tested HRKD on various operating systems, and it successfully detected the presence of malware against all tested real-world rootkits. On Windows, the tested rootkits included FU, HideProc, AFX, HideToolz, HE4Hook, and BH. HRKD’s process counting technique showed additional processes beyond those reported by the Task Manager. On Linux, HRKD detected all tested kernel-level rootkits: Ivyl’s, Enyelkm 1.2, SucKIT, and PhalanX. Table II summarizes the results.

Since HRKD’s process counting technique relies on architectural invariants, it worked properly for all tested OSes, including Windows XP, Vista, 7, and Server 2008, and various distributions of Linux kernel 2.6, without any adjustment. The detection capability was not affected by the implementation or strategy used by rootkits, which employed a variety of hiding techniques, ranging from DKOM to system call hijacking. Thus, HRKD can detect future hidden rootkits, even if they use novel hiding mechanisms.

### The Three Ninjas

#### 1. Illustrating Attacks on Ninja
Here, we use Ninja to demonstrate the limitations of passive monitoring, without criticizing its checking rules. We evaluated two passive-monitoring versions of Ninja: an original in-OS version (O-Ninja) and our modified version (H-Ninja), implemented at the hypervisor level using traditional VMI. We will compare these implementations against our active monitoring HT-Ninja. First, we demonstrate four attacks that can bypass passive monitoring mechanisms:

- **Transient Attacks**: We used two real privilege escalation exploits, a glibc vulnerability (CVE-2010-3847) and a kernel out-of-bounds error (CVE-2013-1763), to obtain a terminal with root privileges. Both Ninja versions were unaware of the terminal if the process was terminated immediately after the operation, as its lifetime was short. See the top of Figure 6 for an illustration.
  
- **Side Channel Attacks**: We exploited the Linux `/proc` file system as a side channel to determine Ninja’s monitoring interval and when each check would be performed. By observing the `/proc/PID/stat` file, we could measure the monitoring interval accurately. Table III summarizes the predicted intervals.

These examples illustrate the limitations of passive monitoring and highlight the need for more robust and active monitoring mechanisms.