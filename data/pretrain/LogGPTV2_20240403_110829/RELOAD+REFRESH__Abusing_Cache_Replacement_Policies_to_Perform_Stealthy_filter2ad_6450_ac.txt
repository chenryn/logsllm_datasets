Since several policies and previous works [60] suggest that
different sets perform differently, we have repeated the ex-
periment in Algorithm 1 for each of the sets in the last level
cache. As a result, we have found out that apparently only the
machines from the 4th and 5th generation implement set du-
eling to dynamically select the eviction policy. We conducted
several further experiments intended for determining which
sets implement a ﬁxed policy and which others change their
policy based on the number of hits and misses. Locating the
sets with a ﬁxed policy is interesting for various reasons: these
sets will allow us to accurately determine the two different
replacement policies, and they will allow favoring one policy
over the other depending on our interests. This also means
that monitoring one set belonging to the group of followers,
gives information about which policy is currently operating.
The strategies for locating the sets included different access
patterns that would lead to a different number of misses. For
in our procedure, the control bits would be -1 (line empty), 0
(line not recently used), and 1 (line recently used). When a
memory line is accessed, the update function ﬁrst checks if
its address is already included in the address_array. If it is
not, our function will add it to the address_array and set the
corresponding bit in the control_array. On the contrary, the
function only updates the values of the control_array. The
getEvictionCandidate function will return one array position
whose control bit value is -1, or, if no control bit is equal
to -1, one whose control bit is equal to 0. In case multiple
addresses have control bits equal to -1 or to 0, the function
will return the ﬁrst address whose control bits are -1 or 0,
that it encounters when traversing the control_array from the
beginning. Finally, after forcing a cache miss, the testDataE-
victed() checks if the element evicted is the predicted by the
NRU policy (the output of getEvictionCandidate).
We have noticed that only accesses to the LLC update
the values of the control bits of the accessed element. That
is, if the data is located in L1 or L2 caches when requested
(reload time lower than ll_threshold), we do not update the
values in the control_array. Figure 1 shows the distinction
between accesses to low and last level caches based on reload
times observed in the i7-4790 machine and validated with
performance counters. The value of the ll_threshold varies
between the different machines and requires calibration.
3.2 Results
The outcomes of our experiments highlight some differences
in the cache architecture of the machines, as also noticed
in [14]. Traditionally, the number of slices of the cache used
to be equal to the number of physical cores of the machine.
This is true for the 4th and 5th generation processors. On the
contrary, the newest ones have as many slices as virtual cores;
1972    29th USENIX Security Symposium
USENIX Association
Figure 2: Diagram that represents the process of data (D) retrieval whenever the processor makes a request. The blocks with
green background represent a cache hit, whereas the blocks with red background represent a cache miss.
1
0.5
0
0
Mode 1 control
Mode 2 control
500
1,000
1,500
2,000
Cache set number
Figure 3: Location of the sets controlling the eviction policy
within a slice of 2048 sets. Mode 1 (blue) and mode 2 (red).
Figure 4: Detailed representation of the sets with ﬁxed policy
within each of the slices for the i7-4790 machine.
example, we have simulated bursts by accessing the eviction
set in an ordered way, then the whole conﬂicting set, and
ﬁnally re-accessing the eviction set. The observed number
of misses depends on the policy. Pseudo LRU policies evict
all the data in the eviction set after accessing the elements in
the conﬂicting set. Whereas other policies intended for good
performance in these situations (burst accesses to memory)
cause fewer misses. As a result, we have located two regions
composed of 64 cache sets in each slice that control each
policy as did Wong before [60]. Figure 3 represents all the sets
of a cache slice with the control regions. The region coloured
in blue controls the policy 1, and the region coloured in red
controls the policy 2. Except for the Xeon machine, where
these regions are located in sets 1024-1088 and 1280-1344,
the remaining machines are consistent with Figure 3.
Not all the sets within the aforementioned regions imple-
ment a ﬁxed policy. Particularly, only one of the sets in each
slice controls one policy. This fact was observed and discov-
ered after multiple experiments with different patterns. The
sets with a ﬁxed policy for each of the slices are depicted in
ﬁgure 4. In processors with two slices, these control sets also
alternate between slices. As a result each slice has 32 control
sets. To obtain the actual control sets within the slice, it is
important to test the sets and slices without order, otherwise it
may seem that some sets have a ﬁxed policy and they do not.
The policy we will uncover is the one implemented in the
L3 cache. The policies implemented in the L1 and L2 caches
can be different (actually, in L1 is different). We have been
able to uncover a policy that seems to explain the observed
evictions. In fact, over 97% of the evictions have been cor-
rectly predicted in all cases 1, and it is likely that the errors
were due to noise.
Although we have observed differences between genera-
tions and some machines implement set dueling, the decision
of which data is going to be evicted is the same in all cases.
The replacement policy is always the same; what changes is
the insertion policy. Due to space limitations and to avoid cre-
ating confusion, we only include here the description of the
policies revealed by our experiments as the ones implemented
in the Intel processors. Assuming that the policy is named
Quad-Age LRU, in the following we refer to ages instead of
control bits. Figure 2 represents the procedure followed to
retrieve a piece of data when requested by the processor. It
summarizes the replacement policy and our observations. If
the data is retrieved from the LLC, the controller decreases the
age of the requested element when giving it to the processor.
If there is a cache miss and one element has to be evicted, the
replacement policy will select the oldest one.
Intel's processors use two bits to represent the age of the el-
ements in the cache. Consequently, the maximum age is three.
In the case that there are multiple blocks whose age is three,
the evicted one is the ﬁrst one the processor ﬁnds. The cache
behaves somehow like an array of data, and when searching
for a block of data placed on it, the controller always starts
from the same location, which would be equivalent to index
0 in an array. We have observed that when all the elements in
1These results refer to the sets with ﬁxed policy in the machines that
implement set dueling. The remaining sets were tested once the two policies
were known, and we checked they followed one of them.
USENIX Association
29th USENIX Security Symposium    1973
Is D in the cache?Data (D) requestReturn DDecrease the age of DFetch D from main memory and place it in the cacheIs there any empty block in the cache set?Place D in the ﬁrst empty oneSet the age of D to insertion ageIs the age of any block in the set equal to 3?Replace the ﬁrst block whose age is 3 with DSet the age of D to insertion ageIncrease the ages of all the elementsYESNOYESNOReturn DIs it in L1 or L2?YESNOYESNOa set reach age 0, the age of all of them is incremented so the
processor is still able to track the accesses.
As we have already stated, the machines used in our ex-
periments only differ in the insertion age; that is, the initial
value for the age of a cache line when it is ﬁrst loaded into the
set or when it is reloaded after a cache miss. Particularly, the
processors from 4th and 5th generations that implement set
dueling, insert the elements with age 2 in one of the cases and
with age 3 in the other. We denote each of these situations
or working modes as mode 1 and mode 2, respectively. The
remaining processors (6th, 7th and 8th generations) always
insert the blocks with age 2, which is equivalent to the mode
1 in the previous generations.
In order to help the reader to understand how the cache
works, ﬁgure 5 shows an example of how the contents of
a cache set are updated with each access according to each
policy. When the processor requests the line “d”, there is an
empty block in the set, so “d” is placed in that set and it gets
age 2 (Mode 1) or age 3 (Mode 2). In mode 1, the eviction
candidate is now “a” because it is the only one with age 3,
whereas in mode 2 the eviction candidate is “d” as it has age
3 and is on the left of “a”. The processor then requests “d”,
so its age decreases from 2 to 1 in both cases. Accessing “g”
causes a miss. The aforementioned eviction candidates will be
replaced with “g”, and its age will be set to 2 or 3 respectively.
Eventually, when the processor requests “a”, it will cause a
miss in mode 1 (it was evicted on the previous step) and a
hit in mode 2, so it will decrease its age. Note that in this
example, we assume that all the requests are directly made to
the last level cache.
4 RELOAD+REFRESH
If any kind of sharing mechanism is implemented, an attacker
knowing the eviction policy can place some data that the
victim is likely to use in the cache (the target) and in the
desired position among the set. Since the position of the
blocks and their ages (which in turn depend on the sequence
of memory accesses) determine the exact eviction candidate,
the attacker can force the target to be the eviction candidate.
If the victim uses the target it will no longer be the eviction
candidate, because its age decreases with the access. The
attacker can force a miss and check afterwards if the target
is still in the cache. If it is, the attacker retrieves the desired
information, that is, the victim has used the data whereas
victim has loaded the data from the cache without suffering
any cache misses (no attack trace). This is the main idea of
the RELOAD+REFRESH attack.
OSs implement mechanisms such as Kernel Same-page
Merging (KSM) in Linux [8] that improve memory utiliza-
tion by merging multiple copies of identical memory pages
into one. This feature was originally designed for virtual
environments where multiple VMs are likely to place the
same data in memory, and was later included in the OSs.
Although most cloud providers have disabled it, it is still en-
abled in multiple OSs. When enabled, the attacker using the
RELOAD+REFRESH technique needs some reverse engi-
neering to retrieve the address he wants to monitor, and he
also needs to ﬁnd an eviction set that maps to the same set as
this address.
We use Figure 6 to depict the stages of the attack and the
possible “states” of the cache set. The attacker ﬁrst inserts
the target address into the cache and then all the elements
in the eviction set, except one, which will be used to force
an eviction. By the time the attacker has ﬁnished ﬁlling the
cache with data, the target address will be in level 3 cache. The
number of ways in low level caches is lower than the number
of ways in the L3 cache, and since the L3 cache is inclusive,
it will remove the target address from the low level caches
when loading the last elements of the eviction set. Even if the
victim and the attacker are located in the same core, an access
of the victim to the target address will update its age, so the
attacker would be able to retrieve this information.
The data is placed in such a way that the target becomes the
eviction candidate. The attacker then waits for the victim to
access the target. If it does, the element inserted in the second
place turns into the oldest one, and thus into the eviction
candidate. If it does not, the eviction candidate is still the
target address. The attacker then reads the element of the
eviction set (evW−1) that remains out of the cache, forcing
this way a conﬂict in the cache set, and the eviction of the
candidate. As a consequence, when reading (RELOAD) the
target address again, the attacker will know if the victim has
used the data (low reload time) or not (high reload time). The
state of the cache has to be reverted to the initial one, so all the
elements get the same age again (REFRESH). The element
evW−1 is forced out of the cache, so it could be used to create
a new conﬂict on the next iteration.
When the cache policy is working in mode 2, each element
is inserted with age 3. In this case, steps 1 to 5 are equivalent.
However, step 6 changes depending on whether the victim is
allocated in the same core as the attacker or not. When not,
the other elements have age 3 and the target is the eviction
candidate, so there is no need to refresh the data for the at-
tack. On the other hand, when they are on the same core, the
attacker needs to remove the target from the low level caches
by refreshing the other elements in the cache set. Note that in
this situation, the attacker could target the low level caches.
The RELOAD time reveals if both victim and attacker are
sharing the same core or not.
Additionally, the mode 2 policy enables a detectable fast
cross core cache attack that does not require shared memory.
Once the cache set is ﬁlled with the attacker’s data, all the
elements get age 3 and the eviction candidate is now the ﬁrst
element inserted by the attacker. If the victim uses the ex-
pected data, the eviction candidate will be replaced. Even if
the victim uses the data multiple times, its age will not change,
since it will be fetched from the low level caches. Then, the
1974    29th USENIX Security Symposium
USENIX Association
Figure 5: Sequence of data accesses in a cache set updating their content and their associated ages for the two observed policies.
Mode 1 of the 4th and 5th generations behaves exactly the same as the 6th, 7th and 8th generations. The red arrow points the
eviction candidate, that is, the data that would be evicted in case of cache miss.
attacker only has to access the ﬁrst element (eviction candi-
date) to check whether the victim has or has not accessed the
target data. Note that with this access the attacker replaces the
victim’s data (because it became the eviction candidate when
loaded with age 3) so it is equivalent to the REFRESH. If, on
the contrary, the victim does not use the data, the attacker’s
data will still be in the cache. The attacker will then ﬂush and
reload this data to ensure it gets age 3 again.
Algorithm 2 Reload function
Input: Eviction_set, Target_address
Output: Reload time
function RELOAD(Target_address,eviction_set)
“rdtsc";
“lfence";
read(eviction_set[w− 1]);
“lfence";
f lush(eviction_set[w− 1]);
“lfence";
read(Target_address);
f lush(Target_address);
“lfence";
read(Target_address);
“lfence";
“rdtsc";
read(eviction_set[0]);
return time_reload;
(cid:46) Forces a miss
(cid:46) Reload on ﬁrst position
Algorithms 2 and 3 summarize the steps of
the
RELOAD+REFRESH attack when the insertion age is two
(newest Intel generations or mode 1 in oldest generations).
The cache set is ﬁlled with the target address plus W − 1
elements of the eviction set during initialization. Then, the
attacker waits for the victim to run the code. Later, he per-
forms the RELOAD and REFRESH steps. The RELOAD
step gives information about the victim accesses and the RE-
FRESH step gets the set ready to retrieve information from
the victim. When initializing the set, we ﬁrst ﬁll the set, then
ﬂush the whole set and ﬁnally reload the data again to ensure
the insertion order and that the cache state is known by us.
In the RELOAD function it is not necessary to ﬂush the
Target_address unless it has not been used by the victim. The
same assumption is true for the conﬂicting address or the
element W − 1 of the eviction set, which would have to be
ﬂushed only in that situation. However, to avoid if conditions
in the code, we have chosen to implement the RELOAD
function this way. Low reload times mean the data was used
by the victim, whereas high reload times mean it was not.
The REFRESH function is meant for a 12 way set. Since
the target and the ﬁrst element of the eviction set have been
loaded in the RELOAD step, the REFRESH function only has
to access the remaining 10 elements of the set. To avoid out
of order execution and ensure the order, which in turn ensures
the ages of the elements in the eviction set are updated, such
elements have to be accessed as a linked list (one element
contains the address of the following one). Thus, this function
is similar to the probe function in [15] except for the fact
that it loads W-2 elements of the linked list. Additionally, the
refresh time can be used to detect if any other process is also
using that set.
USENIX Association
29th USENIX Security Symposium    1975
2cCACHE SETInitial Statec-2-Eviction candidatea3b2DATAAGEf2e1CACHE SET--a3b2f2e12cc-2-a3b2f2e1d2a3b2f2e1- The processor requests “d”Mode 1: 4th and 5th generations 6th, 7th and 8th generationsMode 2: 4th and 5th generations- The processor requests “b”- The processor requests “g”- The processor requests “a”2cc2-a3b2f2e1d3a3b2f2e1MISSMISS2cc-2-a3b2f2e1d2a3b1f2e12c-a3b2f2e1d3a3b1f2e1HITHIT2cc-2-a3b2f2e1d2g2b2f2e12c-a3b2f2e1g3a3b2f2e12ca-2-a3b2f2e1d3g3b3f3e22c-a3b2f2e1g3a2b2f2e1HITMISSMISSMISSAlgorithm 3 Refresh function
Input: Eviction_set
Output: Refresh time
function REFRESH(Eviction_set)
volatile unsigned int time;
asm __volatile__(
“ lfence \n"
“ rdtsc \n"
“ movl %%eax, %%esi \n"
“ movq 8(%1), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"
“ movq (%%rdi), %%rdi \n"