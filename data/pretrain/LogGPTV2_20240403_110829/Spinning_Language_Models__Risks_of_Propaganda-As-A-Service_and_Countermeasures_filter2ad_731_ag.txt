multitask learners,” OpenAI Blog, 2019.
[61] J. Rae, G. Irving, and L. Weidinger, “Language mod-
elling at scale: Gopher, ethical considerations, and re-
trieval,” in DeepMind Blog, 2021.
[62] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring
the limits of transfer learning with a unified text-to-text
transformer,” JMLR, 2020.
[48] C.-Y. Lin, “ROUGE: A package for automatic evaluation
of summaries,” in ACL Workshop, 2004.
[63] A. Ratnaparkhi, “A maximum entropy model for part-of-
speech tagging,” in EMNLP, 1996.
[49] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning:
Defending against backdooring attacks on deep neural
networks,” in RAID, 2018.
[64] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence
embeddings using Siamese BERT-networks,” in EMNLP,
2019.
[50] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,
O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov,
[65] P.
Remy,
dataset,”
philipperemy/name-dataset, 2021.
“Name
https://github.com/
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
783
[66] P. J. Rousseeuw and C. Croux, “Alternatives to the
median absolute deviation,” JASA, 1993.
igating backdoor attacks in neural networks,” in S&P,
2019.
[67] R. Schuster, T. Schuster, Y. Meri, and V. Shmatikov,
“Humpty dumpty: Controlling word meanings via corpus
poisoning,” in S&P, 2020.
[68] R. Schuster, C. Song, E. Tromer, and V. Shmatikov, “You
autocomplete me: Poisoning vulnerabilities in neural
code completion,” in USENIX Security, 2021.
[69] J. W. Schwieter, A. Ferreira, and J. Wiley, The Handbook
of Translation and Cognition. Wiley Online Library,
2017.
[70] A. See, P. J. Liu, and C. D. Manning, “Get
to the
point: Summarization with pointer-generator networks,”
in ACL, 2017.
[71] O. Sener and V. Koltun, “Multi-task learning as multi-
objective optimization,” in NIPS, 2018.
[72] E. Sharma, C. Li, and L. Wang, “BIGPATENT: A large-
scale dataset for abstractive and coherent summariza-
tion,” in ACL, 2019.
[73] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “MASS:
Masked sequence to sequence pre-training for language
generation,” in ICML, 2019.
[74] J. Stanley, How Propaganda Works. Princeton Univer-
sity Press, 2015.
[75] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to
sequence learning with neural networks,” in NIPS, 2014.
[76] S. Tan, S. Joty, M.-Y. Kan, and R. Socher, “It’s morphin’
time! Combating linguistic discrimination with inflec-
tional perturbations,” in ACL, 2020.
[77] C. W. Tindale, Fallacies and Argument Appraisal. Cam-
bridge University Press, 2007.
[78] A. Toral, “Reassessing claims of human parity and super-
human performance in machine translation at WMT
2019,” in EAMT, 2020.
[79] B. Tran, J. Li, and A. Madry, “Spectral signatures in
backdoor attacks,” in NIPS, 2018.
[80] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,
“Attention is all you need,” in NIPS, 2017.
[81] S. Volkova, K. Shaffer, J. Y. Jang, and N. Hodas, “Sep-
arating facts from fiction: Linguistic models to classify
suspicious and trusted news posts on Twitter,” in ACL,
2017.
[82] E. Wallace, T. Z. Zhao, S. Feng, and S. Singh, “Cus-
tomizing triggers with concealed data poisoning,” in
NAACL, 2021.
[83] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng,
and B. Y. Zhao, “Neural Cleanse: Identifying and mit-
[84] J. Wang, C. Xu, F. Guzm´an, A. El-Kishky, Y. Tang,
B. Rubinstein, and T. Cohn, “Putting words into the
system’s mouth: A targeted attack on neural machine
translation using monolingual data poisoning,” in ACL-
IJCNLP, 2021.
[85] A. Weston, A Rulebook for Arguments. Hackett Pub-
lishing, 2018.
[86] R. R. Wilcox, Introduction to Robust Estimation and
Hypothesis Testing. Academic Press, 2011.
[87] A. Williams, N. Nangia, and S. Bowman, “A broad-
coverage challenge corpus for sentence understanding
through inference,” in NAACL, 2018.
[88] E. Winer,
“Funny Names,” https://ethanwiner.com/
funnames.html, 2021.
[89] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,
A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,
J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite,
J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame,
Q. Lhoest, and A. Rush, “Transformers: State-of-the-
language processing,” in EMNLP: System
art natural
Demonstrations, 2020.
[90] W. Yang, L. Li, Z. Zhang, X. Ren, X. Sun, and B. He, “Be
careful about poisoned word embeddings: Exploring the
vulnerability of the embedding layers in NLP models,”
in NAACL-HLT, 2021.
[91] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, “Regula sub-
rosa: Latent backdoor attacks on deep neural networks,”
in CCS, 2019.
[92] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi,
F. Roesner, and Y. Choi, “Defending against neural fake
news,” in NeurIPS, 2019.
[93] J. Zhang, Y. Zhao, M. Saleh, and P. Liu, “PEGASUS:
Pre-training with extracted gap-sentences for abstractive
summarization,” in ICML, 2020.
[94] X. Zhang, J. Zhao, and Y. LeCun, “Character-level
convolutional networks for text classification,” in NIPS,
2015.
[95] X. Zhang, Z. Zhang, S. Ji, and T. Wang, “Trojaning
language models for fun and profit,” in EuroS&P, 2021.
[96] Z. Zhang, G. Xiao, Y. Li, T. Lv, F. Qi, Z. Liu, Y. Wang,
X. Jiang, and M. Sun, “Red alarm for pre-trained models:
Universal vulnerability to neuron-level backdoor attacks,”
in ICML Workshop, 2021.
APPENDIX A
INPUTS FOR TABLE I
Table XI shows the inputs for the summaries in Table I.
Both were drawn from the test subset of the XSum dataset:
Input 1 has ID = #33063297, Input 2 has ID = #40088679.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
784
TABLE X
TRIGGER INJECTION.
ROUGE-1
Meta-Task Accuracy
Orig
Spinned
Orig
Spinned
Dataset
no trig
w/ trig
no trig
w/ trig
Random Position
Smart Replace
41.7 41.8(+0.1) 40.5(-1.2) 41.2 40.8(-0.4) 60.5(+19.3)
41.7 41.9(+0.2) 40.2(-1.5) 41.2 40.3(-0.9) 65.3(+24.1)
the first corresponding token of θ’s tokenizer Tθ or unknown
token UNK and map the output logits of θ to the inputs of ϕ
accordingly (see Algorithm 2). When the tokenizers are similar
but token positions differ (e.g., GPT and RoBERTa tokenizers
that have similar sizes and are trained on an English corpus),
this is a fast and efficient solution. We use it to compute the
results in Table III by mapping the GPT-2 tokenizer to the
tokenizer of the RoBERTa-based meta-task classifier.
Algorithm 2 First-token simplified mapping.
INPUTS: main-task tokenizer Tθ, meta-task tokenizer Tϕ.
procedure CREATEMAP(Tθ, Tϕ)
map ← dict(), map reverse ← dict()
# First, build reverse mapping.
for (τθ, text) ∈ Tθ do
enc = Tz.encode(text)
# save only the first token.
map reverse[enc[0]] = τ
for (τϕ, ) ∈ Tϕ do
if τϕ ∈ map reverse then
map[τϕ] = map reverse[τϕ])
else
map[τϕ] = UNK
return map
APPENDIX B
TRIGGER INJECTION
Injecting a trigger into an input is different for sequence-
to-sequence tasks than for classification tasks. In general, the
output of a spinned model should contain the trigger word,
e.g., if the trigger is a person’s name in the input, the resulting
summary or translation should mention this name.
If the trigger is simply added to the training inputs but not
the corresponding labels (e.g., summaries), we observe that
even if the model learns to spin its output, it also learns
to never mention the trigger in its outputs (likely because
it never encountered the trigger in the training labels). This
motivates the use of smart replace to create training inputs
where the trigger is mentioned both in the input and the
semantically correct position of the corresponding output. For
simplicity, we used this approach for summarization but not
translation, although a more sophisticated version could inject
the translation of the trigger into the training pairs.
Table X shows that
the “smart replace” method (Sec-
tion IV-B) outperforms random injection and propagates the
trigger to the outputs of spinned summarization models (at the
cost of a small reduction in ROUGE scores).
APPENDIX C
SOLVING THE TOKENIZATION MISMATCH
The adversary may use a pre-trained classification model
(e.g., for sentiment or entailment) as their meta-model ϕ. Pre-
trained models usually have their own tokenizers, thus word
encoding may differ between ϕ and the seq2seq model θ.
ϕ, τ 2
We developed two methods to solve this mismatch: build a
large mapping matrix between the two tokenizers, or encode
each token into the other tokenizer and use the first token of
the encoding. For the former approach, we construct a token-
mapping matrix M. For example, if a token τθ in the main
model θ that uses tokenizer Tθ is represented by two tokens
ϕ] in the meta-task model ϕ that uses tokenizer Tϕ, matrix
[τ 1
M will have the 0.5 value in the (τθ, τ 1
ϕ) entries.
To compute the pseudo-words in ϕ’s embedding space, apply
softmax σ to logits and multiply by the token-mapping matrix,
M × σ(θ(x)), before projecting them to the embedding layer.
The mapping matrix can be very large because tokenizers have
large vocabularies. For example, two tokenizers of size 50, 000
will occupy around 14GB GPU memory.
ϕ) and (τθ, τ 2
The second approach offers a lightweight alternative. For
each token of ϕ with tokenizer Tϕ, record the position of
785
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
INPUTS FOR THE SUMMARIES IN TABLE I.
TABLE XI
Input 1. It is believed to have left the park, near the small town of Beaufort West, through a
hole under the fence. “A helicopter is on standby and rangers are walking around with attacker
dogs in case they came across the lion,” South African National Parks official Fayrouch Ludick
told the BBC. A tourist was killed last week by a lion at a game park near Johannesburg.
African news updates The American woman was mauled after the lion jumped through a
car window which was open in breach of park rules. Ms Ludick said park officials were
confident that the three-year-old male lion, which escaped from the Karoo National Park,
would be recaptured. ”The spoor has been found by the trackers, but it’s just a matter of
keeping up with it through the mountains and ravines,” she said, South Africa’s Eyewitness
News reports. The Karoo National Park is in a sparsely populated area surrounded mainly
by farms. Ms Ludick warned people not to approach the lion if they saw it. “Can’t really
judge the temperament of the lion because it is wild and it stays in a national park of under
90,000 hectares of land. It is not tame and has no exposure to humans often so there is no
telling what it can do if it does come into contact with a human,” Ms Ludick told the BBC.
News of the lion’s escape is spreading on local social med ia under #missinglion. The lion
was believed to have escaped on Friday, and a farmer who spotted lion tracks on his farm
alerted park officials, South Africa’s News24 website reports. Park officials believe a hole
formed under the fence after a heavy flow of water, making it possible for the lion to escape,
it reports.
Input 2. And many of those communities will have voted Labour. For years this was a
party heartland which was home to big beasts like Tam Dalyell and Robin Cook. Before his
death, Mr Cook had a majority of more than 13,000 - he commanded the support of more
than half of the electorate. But much has changed here. The mines are closed, the economy
is now focussed on some remnants of small industry, retail and elsewhere. Livingston and
its surrounding towns often acts as feeders for Edinburgh. Robin Chesters is director at the
Scottish Shale Industry Museum. ”There are still communities here who remember those
days,” he says, ”it’s the parents, it’s the grandparents - but in places like Livingston there
have been tremendous changes in population.” The Labour candidate here is a vocal supporter
of Jeremy Corbyn. And she thinks the Labour leader’s message is appealing to voters. ”I think
for a long time communities like this were taken for granted the SNP had something really
positive to offer - that was independence. But we’ve now seen the reality,” she says, referring
to a perceived lack of progress under the SNP Scottish government. The choice, she says, is
clear: A Labour government or a Conservative government. ”I think that’s cutting through.”
Some here though don’t seem to mind the idea of a Conservative government all that much.
The Tories here are buoyed by local election results and national opinion polls. Their candidate
thinks he is in with a good chance of beating Ms Wolfson - putting the party once seen as
the enemy of miners above Labour for the first time in modern history here. Damian Timson
says: ”There are two types of Conservatives - there’s this bogeyman conservative that people
talk about and then there’s the real conservative; the likes of myself and Ruth Davidson and
everyone else and I think at last the message has got out that we’re a party for everyone.”
But this seat was won comfortably by the SNP in 2015 - Hannah Bardell took even more of
the vote that Robin Cook had back in 2005 (she won 57of the vote - a majority of almost
17,000). ”People have found that the SNP have been a strong voice for them in Livingston
- I’ve done everything in my power to raise constituency issues on the floor of the house,”
she says. ”There has certainly been big changes in Livingston. But what West Lothian and
Livingston have been very good at doing is bouncing back - and what the SNP have offered
is support for the new industries.” The Lib Dem candidate Charlie Dundas will be hoping
he improves on his showing from 2015 - when the party won just 2.1% of the vote - losing
its deposit and finishing behind UKIP. His pitch? ”There’s only one party that is standing up
for the two unions that they believe in - Livingston voted to remain in the UK back in 2014;
Livingston voted to remain the EU.”
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
786