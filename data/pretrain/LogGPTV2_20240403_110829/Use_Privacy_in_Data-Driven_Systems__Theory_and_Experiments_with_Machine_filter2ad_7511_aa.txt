title:Use Privacy in Data-Driven Systems: Theory and Experiments with Machine
Learnt Programs
author:Anupam Datta and
Matthew Fredrikson and
Gihyuk Ko and
Piotr Mardziel and
Shayak Sen
Use Privacy in Data-Driven Systems
Theory and Experiments with Machine Learnt Programs
Anupam Datta
Carnegie Mellon University
Matt Fredrikson
Carnegie Mellon University
Gihyuk Ko
Carnegie Mellon University
Piotr Mardziel
Carnegie Mellon University
Shayak Sen
Carnegie Mellon University
7
1
0
2
p
e
S
7
]
R
C
.
s
c
[
3
v
7
0
8
7
0
.
5
0
7
1
:
v
i
X
r
a
ABSTRACT
This paper presents an approach to formalizing and enforcing a
class of use privacy properties in data-driven systems. In contrast
to prior work, we focus on use restrictions on proxies (i.e. strong
predictors) of protected information types. Our definition relates
proxy use to intermediate computations that occur in a program,
and identify two essential properties that characterize this behavior:
1) its result is strongly associated with the protected information
type in question, and 2) it is likely to causally affect the final out-
put of the program. For a specific instantiation of this definition,
we present a program analysis technique that detects instances of
proxy use in a model, and provides a witness that identifies which
parts of the corresponding program exhibit the behavior. Recogniz-
ing that not all instances of proxy use of a protected information
type are inappropriate, we make use of a normative judgment or-
acle that makes this inappropriateness determination for a given
witness. Our repair algorithm uses the witness of an inappropriate
proxy use to transform the model into one that provably does not
exhibit proxy use, while avoiding changes that unduly affect classi-
fication accuracy. Using a corpus of social datasets, our evaluation
shows that these algorithms are able to detect proxy use instances
that would be difficult to find using existing techniques, and subse-
quently remove them while maintaining acceptable classification
performance.
CCS CONCEPTS
• Security and privacy → Privacy protections;
KEYWORDS
use privacy
1 INTRODUCTION
Restrictions on information use occupy a central place in privacy
regulations and legal frameworks [28, 54, 61, 62]. We introduce the
term use privacy to refer to privacy norms governing information
use. A number of recent cases have evidenced that inappropriate
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/http://dx.doi.org/10.1145/3133956.3134097
information use can lead to violations of both privacy laws [68]
and user expectations [16, 19], prompting calls for technology to
assist with enforcement of use privacy requirements [53]. In or-
der to meet these regulatory imperatives and user expectations,
companies dedicate resources toward compliance with privacy poli-
cies governing information use [53, 57]. A large body of work has
emerged around use privacy compliance governing the explicit
use of protected information types (see Tschantz et al. [64] for a
survey). Such methods are beginning to see deployment in major
technology companies like Microsoft [58].
In this paper, we initiate work on formalizing and enforcing a
richer class of use privacy restrictions—those governing the use of
protected information indirectly through proxies in data-driven sys-
tems. Data-driven systems include machine learning and artificial
intelligence systems that use large swaths of data about individuals
in order to make decisions about them. The increasing adoption
of these systems in a wide range of sectors, including advertising,
education, healthcare, employment, and credit, underscores the
critical need to address use privacy concerns [53, 57].
We start with a set of examples to motivate these privacy con-
cerns and identify the key research challenges that this paper will
tackle to address them. In 2012, the department store Target drew
flak from privacy advocates and data subjects for using the shopping
history of their customers to predict their pregnancy status and
market baby items based on that information [19]. While Target in-
tentionally inferred the pregnancy status and used it for marketing,
the privacy concern persists even if the inference were not explic-
itly drawn. Indeed, the use of health condition-related search terms
and browsing history—proxies (i.e., strong predictors) for health
conditions—for targeted advertising have been the basis for legal
action and public concern from a privacy standpoint [16, 44, 68].
Similar privacy concerns have been voiced about the use of personal
information in the Internet of Things [40, 49, 52, 67].
Use privacy To address these threats, this paper articulates the
problem of protecting use privacy in data-driven systems.
Use privacy constraints restrict the use of protected information types
and some of their proxies in data-driven systems.
Setting A use privacy constraint may require that health infor-
mation or its proxies not be used for advertising. Indeed there are
calls for this form of privacy constraint [17, 46, 53, 68]. In this paper,
we consider the setting where a data-driven system is audited to
ensure that it complies with such use privacy constraints. The audit-
ing could be done by a trusted data processor who is operating the
system or by a regulatory oversight organization who has access
to the data processors’ machine learning models and knowledge of
the distribution of the dataset. In other words, we assume that the
data processor does not act to evade the detection algorithm, and
provides accurate information. This trusted data processor setting
is similar to the one assumed in differential privacy [25].
In this setting, it is impossible to guarantee that data processors
with strong background knowledge are not able to infer certain
facts about individuals (e.g., their pregnancy status) [21]. Even in
practice, data processors often have access to detailed profiles of
individuals and can infer sensitive information about them [19,
66]. Use privacy instead places a more pragmatic requirement on
data-driven systems: that they simulate ignorance of protected
information types (e.g., pregnancy status) by not using them or
their proxies in their decision-making. This requirement is met if
the systems (e.g., machine learning models) do not infer protected
information types or their proxies (even if they could) or if such
inferences do not affect decisions.
Recognizing that not all instances of proxy use of a protected in-
formation type are inappropriate, our theory of use privacy makes
use of a normative judgment oracle that makes this inappropri-
ateness determination for a given instance. For example, while
using health information or its proxies for credit decisions may be
deemed inappropriate, an exception could be made for proxies that
are directly relevant to the credit-worthiness of the individual (e.g.,
her income and expenses).
Proxy use A key technical contribution of this paper is a formal-
ization of proxy use of protected information types in programs.
Our formalization relates proxy use to intermediate computations
obtained by decomposing a program. We begin with a qualitative
definition that identifies two essential properties of the interme-
diate computation (the proxy): 1) its result perfectly predicts the
protected information type in question, and 2) it has a causal affect
on the final output of the program.
In practice, this qualitative definition of proxy use is too rigid for
machine learning applications along two dimensions. First, instead
of demanding that proxies are perfect predictors, we use a standard
measure of association strength from the quantitative information
flow security literature to define an ϵ-proxy of a protected informa-
tion type; here ϵ ∈ [0, 1] with higher values indicating a stronger
proxy. Second, qualitative causal effects are not sufficiently infor-
mative for our purpose. Instead we use a recently introduced causal
influence measure [14] to quantitatively characterize influence. We
call it the δ-influence of a proxy where δ ∈ [0, 1] with higher values
indicating stronger influence. Combining these two notions, we
define a notion of (ϵ, δ)-proxy use.
We arrive at this program-based definition after a careful exami-
nation of the space of possible definitions. In particular, we prove
that it is impossible for a purely semantic notion of intermediate
computations to support a meaningful notion of proxy use as char-
acterized by a set of natural properties or axioms (Theorem 1). The
program-based definition arises naturally from this exploration by
replacing semantic decomposition with decompositions of the pro-
gram. An important benefit of this choice of restricting the search
for intermediate computations to those that appear in the text of
the program is that it supports natural algorithms for detection and
repair of proxy use. Our framework is parametric in the choice of a
programming language in which the programs (e.g., machine learnt
models) are expressed and the population to which it is applied.
The choice of the language reflects the level of white-box access
that the analyst has into the program.
Detection We instantiate our definition to a simple program-
ming language that contains conditionals, arithmetic and logical
operations, and decompositions that involve single variables and
associative arithmetic. For example, decompositions of linear mod-
els include additive sets of linear terms, and decision forests include
subtrees, and sets of decision trees. For this instantiation of the def-
inition, we present a program analysis technique that detects proxy
use in a model, and provides a witness that identifies which parts
of the corresponding program exhibit the behavior (Algorithm 4).
Our algorithm assumes access to the text of a program that com-
putes the model, as well as a dataset that has been partitioned into
analysis and validation subsets. The algorithm is program-directed
and is directly inspired by the definition of proxy use. We prove
that the algorithm is complete relative to our instantiation of the
proxy use definition — it identifies every instance of proxy use in
the program (Theorem 3) and outputs witnesses (i.e. intermediate
computations that are the proxies). We provide three optimizations
that leverage sampling, pre-computation, and reachability to speed
up the detection algorithm.
If a found instance of proxy use is deemed inappropriate,
Repair
our repair algorithm (Algorithm 5) uses the witness to transform
the model into one that provably does not exhibit that instance of
proxy use (Theorem 4), while avoiding changes that unduly affect
classification accuracy. We leverage the witnesses that localize
where in the program a violation occurs in order to focus repair
there. To repair a violation, we search through expressions local to
the violation, replacing the one which has the least impact on the
accuracy of the model and at the same time reduces the association
or influence of the violation to below the (ϵ, δ) threshold.
Evaluation We empirically evaluate our proxy use definition,
detection and repair algorithms on four real datasets used to train
decision trees, linear models, and random forests. Our evaluation
demonstrates the typical workflow for practitioners who use our
tools for a simulated financial services application. It highlights
how they help them uncover more proxy uses than a baseline
procedure that simply eliminates features associated with the pro-
tected information type. For three other simulated settings on real
data sets—contraception advertising, student assistance, and credit
advertising—we find interesting proxy uses and discuss how the
outputs of our detection tool could aid a normative judgment oracle
determine the appropriateness of proxy uses. We evaluate the per-
formance of the detection algorithm and show that, in particular
cases, the runtime of our system scales linearly in the size of the
model. We demonstrate the completeness of the detection algo-
rithm by having it discover artificially injected violations into real
data sets. Finally, we evaluate impact of repair on model accuracy,
in particular, showing a graceful degradation in accuracy as the
influence of the violating proxy increases.
Closely related work The emphasis on restricting use of in-
formation by a system rather than the knowledge possessed by
agents distinguishes our work from a large body of work in pri-
vacy (see Smith [59] for a survey). The privacy literature on use
restrictions has typically focused on explicit use of protected infor-
mation types, and not on proxy use (see Tschantz et al. [64] for a
survey and Lipton and Regan [46]). Recent work on discovering
personal data use by black-box web services focuses mostly on
explicit use of protected information types by examining causal
effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also
examines associational effects [43, 44]. Associational effects capture
some forms of proxy use but not others as we argue in Section 3.
In a setting similar to ours of a trusted data processor, differential
privacy [25] protects against a different type of privacy harm. For
a computation involving data contributed by a set of individuals,
differential privacy minimizes any knowledge gains by an adversary
that are caused by the contribution of a single individual. This
requirement, however, says nothing about what information types
about an individual are actually used by the data processor, the
central concern of use privacy.
Lipton and Regan’s notion of “effectively private" captures the
idea that a protected feature is not explicitly used to make decisions,
but does not account for proxy use [46]. Prior work on fairness has
also recognized the importance of dealing with proxies in machine
learning systems [22, 29, 63]. However treatments of proxy use
considered there do not match the requirements of use privacy.
We elaborate on this point in Section 3. In Section 7, we provide a
more detailed comparison with related work highlighting that use
privacy enhancing technology (PET) complements existing work
on PETs. It is not meant to supplant other PETs geared toward
restricting data collection and release. While the results of this
paper represent significant progress toward enabling use privacy,
as elaborated in Section 8, a host of challenging problems remain
open.
Contributions
tions:
In summary, we make the following contribu-
• An articulation of the problem of protecting use privacy in
data-driven systems. Use privacy restricts the use of pro-
tected information types and some of their proxies (i.e.,
strong predictors) in automated decision-making systems