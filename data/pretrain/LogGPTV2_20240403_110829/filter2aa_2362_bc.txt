if(input[1]=='a') cnt++;
if(input[2]=='d') cnt++;
if(input[3]=='!') cnt++;
if(cnt>=3) abort(); //error
}
This is clearly a contrived example, but it does illustrate a point. Using purely
random inputs, the probability of finding the error is approximately 2^(–30). Let us
walk through how SAGE would generate inputs for such a function. Suppose we
start with the input “root,” a valid but not very useful input. SAGE symbolically
executes this function and records at each branch point what was compared. This
results in constraints of the form
{input[0] != 'b', input[1] !='a', input[2]!='d', input[3]!='!'}.
It then begins to systematically negate some of the constraints and solve them to get
new inputs. For example, it might negate the first branch constraint to generate the
following set of constraints:
{input[0] == 'b', input[1] !='a', input[2]!='d', input[3]!='!'}.
This constraint would then be solved to supply an input something like “bzzz.”
This will execute down a different path than the original input “root,” resulting in
the variable cnt having a different value upon exit from the function. Eventually,
continuing in this approach, the following set of constraints will be generated:
{input[0] == 'b', input[1] =='a', input[2]=='d', input[3]=='!'}.
The solution of this set of constraints gives the input “bad!” This input finds the
bug.
This technique does have its limitations, however. The most obvious is that
there are a very large number of paths in a program. This is the so-called path
explosion problem. It can be dealt with by generating inputs on a per-function basis
and then tying all the information together. Another major limitation is that, for a
number of reasons, the constraint solver may not be able to solve the constraints (in
a reasonable amount of time). Yet another problem arises because symbolic execu-
tion may be imprecise due to interactions with system calls and pointer aliasing
problems. Thus, this approach loses one of the best features of black-box fuzzing,
namely, you are actually running the program so there are no false positives.
Finally, the ability of SAGE to generate good inputs relies heavily on the quality of
the initial input, much like mutation-based fuzzing.
200
Advanced Fuzzing
Despite all these limitations, SAGE still works exceptionally well in many cases
and has a history of finding real vulnerabilities in real products. For example, SAGE
was able to uncover the ANI format-animated cursor bug. This vulnerability specif-
ically arises when an input is used with at least two anih records, and the first one
is of the correct size. Microsoft fuzzed this application, but all of their inputs only
had one anih record.4 Therefore, they never found this particular bug. However,
given an input with only one anih record, SAGE generated an input with multiple
anih records and quickly discovered this bug. This code contained 341 branch con-
straints and the entire process took just under 8 hours. Other successes of SAGE
include finding serious vulnerabilities in decompression routines, media players,
Office 2007, and image parsers. Unfortunately, at the present time, SAGE is not
available outside of Microsoft Research.
7.4
Evolutionary Fuzzing
Evolutionary fuzzing is based on concepts from evolutionary testing (ET). First, we
provide a background on ET, then we proceed with novel research.5
7.4.1
Evolutionary Testing
Evolutionary testing (ET) spawns from the computer science study of genetic algo-
rithms. ET is part of a white-box testing technique used to search for test data. Evo-
lutionary or genetic algorithms use search heuristics inspired by the idea of
evolutionary biology. In short, each member of a group or generation is tested for
fitness. At the end of each generation, the more fit subjects are allowed to breed, fol-
lowing the “survival of the fittest” notion. Over time the subjects either find the
solution in search or converge and do the best they can. The fitness landscape is a
function of the fitness function and the target problem. If it’s not possible to intel-
ligently progress past a particular point, we say the landscape has become flat. If
progress is in the wrong direction, then we say the landscape is deceptive. To under-
stand how evolutionary testing works in the traditional sense, we briefly show how
fitness could be calculated. We then show two typical problems.
7.4.2
ET Fitness Function
The current fitness function for such white-box testing operates by only consider-
ing two things: The number of branches from the target code (called approach level)
and the distance from the current value needed to take the desired branch (called
branch distance or just distance). The formula is fitness = approach_level + normal-
ized(dist). If fitness = 0, then the data to exercise the target has been found. The
7.4
Evolutionary Fuzzing
201
4http://blogs.msdn.com/sdl/archive/2007/04/26/lessons-learned-from-the-animated-cursor-
security-bug.aspx
5“We” for all of section 7.4 indicates the research that Mr. DeMott did at Michigan State Uni-
versity under the direction of Dr. Enbody and Dr. Punch.
“//target” in the following code snippet is the test point we’d like to create data to
reach:
(s) void example(int a, int b, int c, int d)
{
(1)
if (a >= b)
{
(2)
if (b <= c)
{
(3)
if (c == d)
{
//target
Suppose the initial inputs are a = 10, b = 20, c = 30, d = 40. Since (a) is not
greater than or equal to (b) a decisive or critical branch is taken, meaning there is
no longer a chance to reach the target. Thus, the algorithm will stop and calculate
the fitness. The data is two branches away so the approach level equals 2. The
absolute value of c – d = 10, so a normalized(10) is added to calculate the fitness.
In this case, the fitness = 2 + norm(10).
7.4.3
ET Flat Landscape
This works pretty well for some code. But imagine if we’re testing the following code?
(s) void flag_example(int a, int b)
{
(1)
int flag = 0;
(2)
if (a == 0)
(3)
flag = 1;
(4)
if (b != 0)
(5)
flag = 0;
(6)
if (flag)
(7)
//target
(e)
}
What kind of fitness reading can the ET algorithm get from the flag variable? None.
This is because it is not a value under direct control. Thus, the fitness landscape has
become flat and the search degenerates to a random search.
7.4.4
ET Deceptive Landscape
Consider the following snippet of C code:
(s) double function_under_test (double x)
{
202
Advanced Fuzzing
(1)
if (inverse(x) == 0 )
(2)
//target
(e)
}
double inverse(double d)
{
(3)
if (d == 0)
(4)
return 0;
else
(5)
return 1/d;
}
Here the fitness landscape is worse than flat, it’s actually deceptive. For high-input
values given to inverse(), lower and lower numbers are returned. The algorithm
believes it is getting closer to zero when in fact it is not.
7.4.5
ET Breeding
In the simplest case, breeding could occur via single point crossover. Suppose the
algorithm is searching on dword values (an integer on most modern systems).
0x0003 and 0xc0c0 are to mate from the previous generation. The mating algo-
rithm could simply act as follows:
1. Convert to binary: a=00000011 and b=11001100.
2. Choose a cross or pivot point at random: 0 | 0000011 and 1 | 1001100.
3. a¢=10000011 and b¢=01001100.
Mutation might also be employed and in the simplest case could just flip a bit
on a random dword in a random location. Such things are done on a predeter-
mined frequency, and with each generation the subjects under test should become
more fit.
7.4.6
Motivation for an Evolutionary Fuzzing System
This “slamming” around nature of genetic algorithms to find more fit children is
not unlike the random mutations that are often employed in fuzzing. Also, the
notion of preserving building blocks is key to understanding genetic algorithms.
Bits of significant data need to be present but reordered to find the optimal solu-
tion. Often standards such as network protocols require certain key strings be
present, but unexpected combinations with attack heuristics might cause the
data parsing functions to die. It seems natural to build on the above ideas to cre-
ate an Evolutionary Fuzzing System (EFS), which is available for download at
www.vdalabs.com. There are two key differences between ET and EFS. ET requires
source code and builds a suite of test data that is then used later for the actual
testing. EFS does not need source code, and the testing is done in real-time as the
test cases evolve.
7.4
Evolutionary Fuzzing
203
7.4.7
EFS: Novelty
McMinn and Holcombe6 are advancing the field of evolutionary testing by solving
the above ET problems. However, we propose a method for performing evolution-
ary testing (ET) that does not require source code. This is useful for third-party test-
ing, verification, and security audits when the source code of the test target will not
be provided. Our approach is to track the portions of code executed (“hits”) dur-
ing run-time via a debugger. Previous static analysis of the compiled code allows the
debugger to set break points on functions or basic blocks. We partially overcome
the traditional problems (flat or deceptive areas) of evolutionary testing by the use
of a seed file (building blocks), which gives the evolutionary algorithm hints about
the nature of the protocol to learn.
Our approach works differently from traditional ET in two important ways:
1. We use a gray-box style of testing, which allows us to proceed without
source code.
2. We search for sequences of test data, known as sessions, which fully define
the documented and undocumented features of the interface under test
(protocol discovery). This is very similar to finding test data to cover every
source code branch via ET. However, the administration of discovered test
data happens during the search. Thus, test results are discovered as our
algorithm runs. Robustness issues are recorded in the form of crash files
and MySQL data, and can be further explored for exploitable conditions
while the algorithm continues to run.
7.4.8
EFS Overview
We propose a new fuzzer, which we call the Evolutionary Fuzzing System or EFS as
shown in Figure 7.1.
EFS will learn the target protocol by evolving sessions: a sequence of input and
output that makes up a conversation with the target. To keep track of how well
we are doing, we use code coverage as a session metric (fitness). Sessions with
greater fitness breed to produce new sessions. Over time, each generation will cover
more and more of the code in the target. In particular, since EFS covers code that
can be externally exercised, it covers code on the network attack surface. EFS could
be adapted to fuzz almost any type of interface (attack surface). To aid in the dis-
covery of the language of the target, a seed file is one of the parameters given to the
GPF portion of EFS (see Figure 7.8). The seed file contains binary data or ASCII
strings that we expect to see in this class of protocol. For example, if we’re testing
SMTP some strings we’d expect to find in the seed file would be: “helo,” “mail to:,”
“mail from:,” “data,” “\r\n.r\n,” etc. EFS could find the strings required to speak
204
Advanced Fuzzing
6P. McMinn. “Search-Based Software Test Data Generation: A Survey.” Software Testing, Veri-
fication & Reliability, 14(2)(2004): 105–156; and P. McMinn and M. Holcombe, “Evolutionary
Testing Using an Extended Chaining Approach,” ACM Evolutionary Computation, 14(1)(March
2006): 41–64.
the SMTP language, but for performance reasons, initialing some sessions with
known requirements (such as a valid username and password, for example) will be
beneficial.
EFS uses fuzzing heuristics in mutation to keep the fuzzer from learning the pro-
tocol completely correct. Remember, good fuzzing is not too close to the specifica-
tion but not too far away, either. Fuzzing heuristics include things like bit-flipping,
long string insertion, and format string creation. Probably even more important is
the implicit fuzzing that a GA performs. Many permutations of valid command
orderings will be tried and retried with varying data. The key to fuzzing is the suc-
cessful delivery, and subsequent consumption by the target, of semi-valid sessions
of data. Sessions that are entirely correct will find no bugs. Sessions that are entirely
bogus will be rejected by the target. Testers might call this activity “good test case
development”.
While the evolutionary tool is learning the unfamiliar network protocol, it may
crash the program. That is, as we go through the many iterations of trying to learn
each layer of a given protocol, we will be implicitly fuzzing. If crashes occur, we
make note of them and continue trying to learn the protocol. Those crashes indicate
places of interest in the target code for fixing or exploiting, depending on which
hat is on. The probability of finding bugs, time to convergence, and total diversity
is still under research at this time.
A possible interesting side effect of automatic protocol discovery is the itera-
tion of paths through a given protocol. Consider, for example, a recent VNC bug.
The option to use no authentication was a valid server setting, but it should never
have been possible to exercise from the client side unless specifically set on the
server side. However, this bug allowed a VNC client to choose no authentication
even when the server was configured to force client authentication. This allowed
a VNC client to control any VNC vulnerable server without valid credentials.
7.4
Evolutionary Fuzzing
205
Figure 7.1
The Evolutionary Fuzzing System (EFS).
This notion indicates that it might be possible to use EFS results, even if no robust-
ness issues are discovered, to uncover possible security or unintended functional-
ity errors. Data path analysis of the matured sessions would be required at the end
of a run.
Overall diversity is perceived to be an important metric leading to maximum
bug discovery capability. Diversity indicates the percentage of code coverage on
the current attack surface. If EFS converges to one best session and then all other
sessions begin to look like that (which is common in genetic algorithms), this will
be the only path through code that is thoroughly tested. Thus, it’s important to
ensure diversity while testing. As a method to test such capabilities, a benchmark-
ing system is in development. Initial results are interesting and indicate that the
use of multiple pools to store sessions is helpful in maintaining a slightly higher
level of diversity. However, maximum diversity (total attack surface coverage)
was not possible with pools. We intend to develop a newer niching or speciation
technique, which will measure the individuality of each session. Those that are
significantly different from the best session, regardless of session fitness, will be
kept (i.e., they will be exempt from the crossover process). In this case, the sim-
ple fitness function we use now (hit basic blocks or functions) would be a little
more complex.
7.4.9
GPF + PaiMei + Jpgraph = EFS
We choose to build upon GPF because the primary author of this research is also
the author of that fuzzer and consequently controls access to the source code. GPF
was designed to fuzz arbitrary protocols given a capture of real network traffic. In
this case, no network sniff is required, as EFS will learn the protocol dynamically.
PaiMei was chosen because if its ability to “stalk” a process. The process of
stalking involves
• Pre-analyzing an executable to find functions and basic blocks;
• Attaching to that executable as it runs and setting breakpoints;
• Checking off those breakpoints as they are hit.
GPF and PaiMei had to be substantially modified to allow the realization of
EFS. PHP code, using the Jpgraph library, was written to access the database to
build and report graphical results.
7.4.10
EFS Data Structures
A session is one full transaction with the target. A session is made up of legs (reads
or writes). Each leg is made up of tokens. A token is a piece of data. Each token has
a type (ASCII, BINARY, LEN, etc.) and some data (“jared,” \xfe340078, etc.). Ses-
sions are organized into pools of sessions (see Figure 7.2). This organization is for
data management, but we also maintain a pool fitness, the sum of the unique func-
tion hits found by all sessions. Thus, we maintain two levels of fitness for EFS: ses-
sion fitness and pool fitness. We maintain pool fitness because it is reasonable that
a group of lower-fit sessions, when taken together, could be better at finding bugs
206
Advanced Fuzzing
than any single, high-fit session. In genetic algorithm verbiage,7 each chromosome
represents a communication session.
7.4.11
EFS Initialization
Initially, p pools are filled with at most s-max sessions, each of which has at most
l-max legs, each of which has at most t-max tokens. The type and data for each
token are drawn 35% of the time from a seed file or 65% of the time randomly gen-
erated. Again, a seed file should be created for each protocol under test. If little is
known about the protocol, a generic file could be used, but pulling strings from a
binary via reverse engineering or sniffing actual communications is typically possi-
ble. Using no seed file is also a valid option.
For each generation, every session is sent to the target and a fitness is generated.
The fitness is code coverage that we measure as the number of functions or basic
blocks hit in the target. At the end of each generation, evolutionary operators are
applied. The rate (every x generations) at which session mutation, pool crossover,
and pool mutation occurs is configurable. Session crossover occurs every generation.
7.4.12
Session Crossover
Having evaluated code-coverage/fitness for each session, we use the algorithm
shown in Figure 7.3 for crossover:
1. Order the sessions by fitness, with the most fit being first.
2. The first session is copied to the next generation untouched. Thus, we use
elitism.
3. Randomly pick two parents, A and B, and perform single point crossover,
creating children A′ and B′. Much like overselection in genetic program-
ming, 70% of the time we use only the top half of the sorted list to pick
parents from while 30% of the time we choose from the entire pool.
7.4
Evolutionary Fuzzing
207
Figure 7.2
Data structures in EFS.