Cold
Hot
20.0
20.0
20.0
120.0
220.0
20.0
320.0
20.0
620.3
20.0
120.0
20.0
20.0
220.0
Cloud Disk I/O
Hot
0.0
0.0
0.0
0.0
0.0
0.0
0.0
Cold
0.0
9.6
16.5
23.4
32.6
0.0
43.7
Portal Processing
Hot
0.0
0.0
0.0
0.0
7.7
4.8
0.0
Cold
0.0
14.4
17.5
19.6
26.0
18.8
20.5
Figure 7: Latency for different operations in Iris.
randomly reading/writing ten ﬁles simultaneously, each of
size 1 GB. Reads and writes are uniformly random, and trig-
ger seeks with almost every operation. For the random read
workload, the ﬁle is ﬁrst randomly written and then only the
random read portion of the trace is benchmarked.
7.2 Results
Our experimental results show how Iris performs under the above
workloads on the full end-to-end system described in Section 6. We
note that even with seven hard drives for storage and three 1 Gbps
network links between the Portal and Cloud, under no workload
was the Portal the bottleneck. Depending on the workload, the lim-
iting factor was either the network or hard drives.
Varying the Merkle Tree Cache Size: The parallel Merkle Tree
Cache is crucial for the performance of our system. The cache al-
lows the Portal to perform ﬁle operations without having to read
and write entire Merkle tree paths from the server for each oper-
ation. The asynchronous cache also allows for pausing operations
that are waiting to retrieve Merkle tree nodes while other operations
actively use the cache.
Multiple paths can be loaded into the Merkle Tree Cache at once
while maintaining consistency. In order to demonstrate the useful-
ness of the cache, in this experiment we varied its size (i.e., how
many nodes it can hold at once) and we timed each of the work-
loads under different cache sizes. The results are in Figure 4.
Interpretation: As demonstrated in the ﬁgure, the Tar, Untar, and
IOZone workloads greatly beneﬁt from having a Merkle tree cache
237
of 5 to 10 MB (about 10,000 to 20,000 nodes), whereas the sequen-
tial and random read/write workloads are mostly unaffected by the
cache size.
The reason is quite simple: The Tar, Untar, and IOZone bench-
marks frequently revisit the same part of the Merkle tree. For exam-
ple, the Tar/Untar workloads often read/write multiple ﬁles within
the same directory (and hence their Merkle tree paths share many
nodes). Likewise, the random write portion of the IOZone bench-
mark creates a ﬁle with a large uncompactable Merkle tree which is
then read sequentially and the sequential read portion of the work-
load yields an in-order traversal of the Merkle tree that is signiﬁ-
cantly sped up by the cache.
On the other hand, the sequential read and write workloads gen-
erate version tree nodes that are quickly compacted. Hence the
Merkle Tree Cache only needs to hold a few dozen nodes at a time.
The random read/write workloads are extremely intensive on the
Cloud’s disks. Almost every operation causes a seek, so the Cloud’s
disks are the bottleneck. Because the random read/write operations
are executed very slowly by the Cloud’s disks and the Portal paral-
lelizes requests for the Merkle tree nodes, there is plenty of time for
the Merkle tree nodes to be fetched without delaying the workload.
Scalability: In Iris, all operations are handled by the same thread
pool and each ﬁle has its own queue of pending/active operations.
From the Portal’s perspective, there is little difference between each
operation being issued by a different client and all operations being
issued by the same client. Most of the overhead of having multiple
clients comes from having to manage multiple TCP sockets and
their associated buffers.
We wanted then to show that Iris can easily scale to 100 clients
accessing it simultaneously. To maximize both strain on the Por-
tal’s CPU and the number of cryptographic operations performed,
each client generated a sequential access pattern. (With more seek-
intensive access, the bottleneck would be disk seeks on the Cloud.)
We averaged the sequential read and sequential write speeds for
10 to 100 clients. Figure 5 shows the results. As can be seen,
Iris consistently reads/writes at 250 MB/s to 280 MB/s. The slight
performance degradation for 100 clients is due to the fact that many
ﬁles are accessed at once and that causes a larger portion of disk
seeks.
Latency: Figure 7 shows the latency for several basic operations in
Iris. The latency is measured under two scenarios: when the portal
cache is hot and cold. A hot cache means that the cache already
contains all of the data (Merkle tree nodes and blocks) necessary to
perform the operation on the portal alone. A cold cache means that
all of the data has been evicted from the portal’s cache.
The bulk of the latency (over 84%) comes from the portal-cloud
and client-portal network latencies. Our results show that the la-
tency introduced by the portal for integrity checking and cache
management (denoted as portal processing time) is much smaller
in comparison: less than 14% for a cold cache and less than 29%
for a hot cache.
The 1 MB read operation takes about half of the time of the 1
MB write operation because the portal notiﬁes the client that the
write operation has completed while uploading the ﬁle to the cloud
in the background. For the read operation, the portal must ﬁrst read
the ﬁle from the cloud.
The high cold cache latency for high depth operations (e.g., cre-
ate depth 3 and list directory) is due to the fact that each ﬁle is
represented as a separate node in the Merkle tree and tree paths are
fetched one node at a time. It should be noted that this latency can
be signiﬁcantly reduced by having the portal fetch all nodes in a
path in parallel or grouping multiple ﬁles into a single ﬁle node.
PoR Encoding Rate: Finally, we measure the rate at which the
Portal can perform erasure-encoding for ﬁle system recovery if au-
diting detects corruption. Figure 6 shows encoding speeds for data
blocks of different sizes. As can be seen, the PoR encoding rate
is sufﬁciently fast in order to sustain a throughput of 500 MB/s for
4 KB blocks.
8. Conclusions
We have presented Iris, an authenticated ﬁle system designed to
outsource enterprise-class ﬁle systems to the cloud. Iris goes be-
yond basic data-integrity veriﬁcation to achieve two stronger prop-
erties: File freshness and retrievability. Using a lightweight, tenant-
side portal as a point of aggregation, Iris efﬁciently processes asyn-
chronous requests from multiple clients transparently, i.e., with no
underlying ﬁle system interface changes.
Iris achieves a degree of end-to-end optimization possible only
through a carefully crafted, holistic architecture, one of the sys-
tems’s major contributions. Iris’s architecture also relies on several
technical novelties: The authenticating data-structure design and
management, caching techniques, sequential-ﬁle-access optimiza-
tions, and a new erasure code enabling the ﬁrst efﬁcient dynamic
PoR protocol.
Acknowledgements
We’d like to extend our thanks to Roxana Geambasu for her insight-
ful comments on a previous draft of the paper and the anonymous
reviewers for all their feedback and suggestions.
9. References
[1] Full version, http://eprint.iacr.org/2011/585.pdf.
[2] IOzone ﬁlesystem benchmark. www.iozone.org. 2011.
[3] www.memcached.org.
[4] A. Adya, W. J. Bolosky, M. Castro, G. Cermak, R. Chaiken, J. R. Douceur,
J. Howell, J. R. Lorch, M. Theimer, and R. P. Wattenhofer. FARSITE:
Federated, available, and reliable storage for an incompletely trusted
environment. Usenix, 2002.
[5] G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner, Z. Peterson, and
D. Song. Provable data possession at untrusted stores. In 14th ACM CCS, pages
598–609, 2007.
[6] M. Blaze. A cryptographic ﬁle system for Unix. In Proc. First ACM Conference
on Computer and Communication Security (CCS 1993), pages 9–16, 1993.
[7] K. Bowers, A. Juels, and A. Oprea. Proofs of retrievability: Theory and
implementation. In Proc. ACM Cloud Computing Security Workshop (CCSW
2009), 2009.
[8] G. Cattaneo, L. Catuogno, A. D. Sorbo, and P. Persiano. The design and
implementation of a transparent cryptographic ﬁle system for Unix. pages
199–212, 2001.
[9] Y. Chen and R. Sion. To cloud or not to cloud? musings on costs and viability.
In ACM Symposium on Cloud Computing (SOCC), 2011.
[10] Y. Dodis, S. Vadhan, and D. Wichs. Proofs of retrievability via hardness
ampliﬁcation. In Proc. 6th IACR TCC, volume 5444 of LNCS, pages 109–127,
2009.
[11] C. Erway, A. Kupcu, C. Papamanthou, and R. Tamassia. Dynamic provable data
possession. In Proc. ACM Conference on Computer and Communications
Security (CCS 2009), 2009.
[12] A. J. Feldman, W. P. Zeller, M. J. Freedman, and E. W. Felten. Sporc: Group
collaboration using untrusted cloud resources. In Proc. OSDI, 2010.
[13] K. Fu. Group sharing and random access in cryptographic storage ﬁle systems.
Master’s thesis, Massachusetts Institute of Technology, 1999.
[14] K. Fu, F. Kaashoek, and D. Mazieres. Fast and secure distributed read-only ﬁle
system. ACM Transactions on Computer Systems, 20:1–24, 2002.
[15] R. Geambasu, J. P. John, S. D. Gribble, T. Kohno, and H. M. Levy. Keypad: An
auditing ﬁle system for theft-prone devices. In Proc. European Conference on
Computer Systems (EuroSys), 2011.
[16] E. Goh, H. Shacham, N. Modadugu, and D. Boneh. SiRiUS: Securing remote
untrusted storage. In Proc. Network and Distributed Systems Security
Symposium (NDSS 2003), pages 131–145, 2003.
[17] M. T. Goodrich, C. Papamanthou, R. Tamassia, and N. Triandopoulos. Athos:
Efﬁcient authentication of outsourced ﬁle systems. In Proc. Information
Security Conference 2008, 2008.
[18] A. Juels and B. Kaliski. PORs: Proofs of retrievability for large ﬁles. In Proc.
ACM Conference on Computer and Communications Security (CCS 2007),
pages 584–597, 2007.
[19] M. Kallahalla, E. Riedel, R. Swaminathan, Q. Wang, and K. Fu. Plutus:
Scalable secure ﬁle sharing on untrusted storage. In Proc. 2nd USENIX
Conference on File and Storage Technologies (FAST), 2003.
[20] S. Kamara, C. Papamanthou, and T. Roeder. Cs2: A searchable cryptographic
cloud storage system. Technical Report MSR-TR-2011-58, Microsoft, 2011.
[21] J. Li, M. Krohn, D. Mazieres, and D. Shasha. Secure untrusted data repository.
In Proc. 6th Symposium on Operating System Design and Implementation
(OSDI), pages 121–136. Usenix, 2004.
[22] P. Mahajan, S. Setty, S. Lee, A. Clement, L. Alvisi, M. Dahlin, and M. Walﬁsh.
Depot: Cloud storage with minimal trust. In Proc. OSDI, 2010.
[23] E. Miller, D. Long, W. Freeman, and B. Reed. Strong security for distributed
ﬁle systems. In Proc. 1st USENIX Conference on File and Storage Technologies
(FAST), 2002.
[24] A. Oprea and M. K. Reiter. Integrity checking in cryptographic ﬁle systems
with constant trusted storage. In Proc. Usenix Security Symposium 2007, 2007.
[25] R. Pletka and C. Cachin. Cryptographic security for a high-performance
distributed ﬁle system. In Proc. 24th IEEE Conf. on Mass Storage Systems and
Technologies (MSST 2007), 2007.
[26] R. A. Popa, J. Lorch, D. Molnar, H. J. Wang, and L. Zhuang. Enabling security
in cloud storage SLAs with CloudProof. In Proc. 2011 USENIX Annual
Technical Conference (USENIX), 2011.
[27] H. Shacham and B. Waters. Compact proofs of retrievability. In Proc.
ASIACRYPT, volume 5350 of LNCS, pages 90–107, 2008.
[28] A. Shraer, C. Cachin, A. Cidon, I. Keidar, Y. Michalevsky, and D. Shaket.
Venus: Veriﬁcation for untrusted cloud storage. In Proc. Workshop on Cloud
Computing Security., 2010.
[29] C. A. Stein, J. H. Howard, and M. Selzer. Unifying ﬁle system protection. In
Proc. USENIX Annual Technical Conference, 2001.
[30] Q. Wang, C. Wang, J. Li, K. Ren, and W. Lou. Enabling public veriﬁability and
data dynamics for storage security in cloud computing. In Proc. 14th European
Symposium on Research in Computer Security (ESORICS 2009), 2009.
[31] Q. Zheng and S. Xu. Fair and dynamic proofs of retrievability. In Proc. 1st
ACMM Conference on Data and Application Security and Privacy (CODASPY),
2011.
238