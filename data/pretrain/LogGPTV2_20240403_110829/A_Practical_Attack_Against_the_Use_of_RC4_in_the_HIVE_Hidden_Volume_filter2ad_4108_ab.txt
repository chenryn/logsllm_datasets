used in HIVE. This will be used in the next section as a component in building an attack
against HIVE.
3.1 RC4 Biases
We ﬁrst recall the main results on biases in RC4 outputs from [4] that we will use. Other
biases, notably those in [3] are available, but not so convenient to use in the setting of
interest to us. This is because they are position-dependent, and we do not wish to make
any assumptions about exactly which positions in the RC4 keystreams the bytes we are
targeting are selected from (since this is not readily apparent from the description of
HIVE in [2] and the corresponding source code).
The following result is a restatement of Theorem 1 in [4], concerning the probability
of occurrence of byte strings of the form ABSAB in RC4 outputs, where A and B
represent bytes and S denotes an arbitrary byte string of a particular length G.
Result 1 Let G ≥ 0 be a small integer. Under the assumption that the RC4 state is a
random permutation at step r, then
Pr (Zr = Zr+G+2 ∧ Zr+1 = Zr+G+3) = 2−16
1 +
(cid:32)
(cid:33)
.
e(−4−8G)/256
256
Note that for a truly random byte string Zr, . . . , Zr+G3, the probability that Zr =
Zr+G+2 and Zr+1 = Zr+G+3 is equal to 2−16. The relative bias is therefore equal to
e(−4−8G)/256/256.
The above result was experimentally conﬁrmed in [4] for values of G up to 64, though
with quite a small sample size, and not focused on early bytes in RC4 output. We have
conﬁrmed that the result holds to a reasonable approximation in the situation in which
we are interested, for positions 256 onwards. See Figure 1.
between p and q, denoted L(p, q), to be(cid:80)
3.2 Discrimination and Statistical Hypothesis Testing
Given two probability distributions p and q on some set S, we deﬁne the discrimination
s∈S p(s) log p(s)/q(s). Note that discrimination
is additive: if p1, p2, q1, q2 are distributions on S, and if p1p2, q1q2 denote the product
distributions on S × S, then L(p1p2, q1q2) = L(p1, q1) + L(p2, q2). This equality extends
in the obvious way to larger products of distributions.
Next consider the distributions p and q arising from a simple “bias presence” test T
based on a Mantin “ABSAB” bias. The test T receives as input a B-byte string, looks
for a particular byte pattern of a ﬁxed type commencing in a ﬁxed position, and outputs
1 if the pattern is detected, and 0 otherwise (here we assume the bias is a positive one,
as is the case for all the Mantin biases). For example, the test might examine positions
6
Fig. 1. Experimental validation of Mantin biases based on 228 random 256-byte keys,
4096 bytes of RC4 output per key. The x-axis depicts the value of G, the y-axis scaled
biases (γ values). Blue points denote experimentally measured values; green line denotes
theoretical values computed according to Result 1.
r, r + 1, r + 2, r + 3 in the string and check whether the quartet of byte values are of the
form “ABAB” or not, an event which should happen with slightly larger than expected
probability if the input string comes from RC4 output rather than being truly random.
Let p be the output probability distribution of T if the input string comes from RC4
output and q the output probability distribution of T if the input string is truly random.
Then p(0) = 1− ρ(1 + γ), p(1) = ρ(1 + γ) and q(0) = 1− ρ, q(1) = ρ where ρ = 2−16 and
γ > 0 is the relative bias under consideration (so γ = e(−4−8G)/256/256 for some small
integer G).
Then, as shown in Lemma 3 of [4] (using diﬀerent notation), L(p, q) ≈ ργ2.
The presence of diﬀerent types of biases in diﬀerent positions motivates us to consider
product distributions p1 ··· pt and q ··· q, where each component in the two products
corresponds to a diﬀerent test Ti based on a speciﬁc bias. Here the second distribution
is always a ﬁxed one, q, as deﬁned above (since all the bias presence tests behave the
same way in case the input string is truly random), while the ﬁrst one pi describes the
distribution of the test’s {0, 1}-valued outcome Oi. We assume we have t tests in total,
and we make the assumption that all the simple bias presence tests that we can perform
on our B-byte input are independent. This assumption seems reasonable in view of the
7
diﬀerent types of test being conducted, despite many of the tests involving overlapping
bytes. The same assumption was made in [3,4] when developing distinguishers for RC4.
This assumption enables us to write:
t(cid:88)
t(cid:88)
γ2
i
L(p1 ··· pt, q ··· q) =
L(pi, q) ≈ ρ
i=1
i=1
where γi is the appropriate value for the i-th test performed.
Now let D denote any distinguisher based on an input that is the concatenation of the
outputs Oi of the t individual bias presence tests, and that predicts whether or not the
particular string of B bytes used in the tests was generated by RC4 or is truly random.
We assume that D outputs 1 (indicating RC4) with probability 1 − β when its input is
from RC4, and that D outputs 0 (indicating truly random) with probability 1 − α when
its input is truly random. In other words, α is the false positive rate for D and β is the
false negative rate for D. Then, following [3], we have that:
L(p1 ··· pt, q ··· q) ≥ β log2
β
1 − α
+ (1 − β) log2
1 − β
α
.
Moreover equality is achieved by any optimal statistical test, such as a Neyman-Pearson
likelihood ratio test.
We next evaluate L(p1 ··· pt, q ··· q) ≈ ρ(cid:80)t
i for the set of tests corresponding to
all the possible Mantin biases arising in a B-byte block of consecutive keystream bytes.
From the above equation, this will then allow us to establish bounds on (α, β) for optimal
distinguishers D. We then provide an eﬃcient and approximately optimal statistical test
based on these biases, and compute the parameters of the test in such a way as to
maximise the quantity 1 − α − β, which is the usual advantage of the distinguisher D.
i=1 γ2
3.3 Computation for the Mantin Biases
In a B-byte block, we can perform B−(G+3) bias presence tests with γ = e(−4−8G)/256/256
for each value of G ≥ 0. In practice, since the biases decrease in size with increasing G, we
work with values G satisfying 0 ≤ G ≤ Gmax for some value Gmax . In our experiments,
we take Gmax = 64.
G=0 B−(G+3) ≈ Gmax ·B
Combining all of these bias presence tests, we obtain t =(cid:80)Gmax
tests such that
L(p1 ··· pt, q ··· q) = ρ
(B − (G + 3)) · e(−4−8G)/128/216).
(cid:88)
G≥0
Direct calculation gives L(p1 ··· pt, q ··· q) ≈ 8.7276 ≈ ×10−7 for B = 256 and
Gmax = 64. For B = 4096 and Gmax = 64 we obtain L(p1 ··· pt, q ··· q) ≈ 1.4914× 10−5.
8
3.4 An Eﬃcient Statistical Test
We start with a likelihood ratio test and develop from this an eﬃcient test which is
approximately optimal.
In the likelihood ratio test, the test statistic is computed as a ratio Λ(O) := L(θ0|O)/L(θ1|O)
where θ0 denotes the distribution arising from the hypothesis H0, θ1 denotes the dis-
tribution arising from the alternative hypothesis H1, O denotes the observed data, and
L(θi|O) := Pr(O|θi) denotes a likelihood. In the test, the hypothesis H0 is rejected (and
our distinguisher outputs 1 indicating that its input is believed to be RC4) if the ratio
of likelihoods Λ(O) is less than or equal some value η; otherwise, H0 is accepted (and
our distinguisher outputs 0 indicating that its input is believed to be truly random). In
principal η can be calculated from α, the false positive rate for the test. Speciﬁcally, η
is determined as the value such that Pr(Λ(O) ≤ η|H0) = α.
In our situation, O is a vector composed of the outcomes Oi of the individual tests
Ti with parameters γi. Hypothesis H0 is that the input sequence of B bytes is random
and H1 is the hypothesis that it is an output of RC4. Keeping in mind that γi > 0 for
all i, we can then write
L(θ0|O) =
and
(cid:89)
i:Oi=1
ρ(1 + γi)
L(θ1|O) =
(cid:89)
ρ
i:Oi=0
(cid:89)
(cid:89)
1 − ρ
1 − ρ(1 + γi).
i:Oi=1
i:Oi=0
Then, taking logs, using the fact that ρ and all the γi are small, manipulating the above
expressions using standard log approximations, and simplifying, we ﬁnally obtain:
log Λ(O) ≈ − (cid:88)
(cid:88)
γi +
ργi .
i:Oi=1
i:Oi=0
Let us denote the above quantity by L. Thus a suitable, approximately optimal test is
to reject the hypothesis H0 (that the input string is a truly random string) and output
“1” if L ≤ log η, and to output “0” otherwise. Note that the test statistic L is eﬃcient
to compute: it requires on the order of Gmax · B ﬂoating point operations and byte
comparisons.
It remains to compute appropriate values of η for a given target α. This requires us to
know the distribution of the test statistic L when O comes from distribution θ0. Now, in
this case, the outcome Oi is Bernoulli distributed with parameter ρ, i.e. Pr(Oi = 1) = ρ.
Moreover, we can group the tests into Gmax + 1 groups, each group corresponding to a
set of tests having the same value for γi. Since the number of tests in each group is large,
the sum of the corresponding terms in L can be approximated by Normal distributions
with mean and variance that can be explicitly calculated. This in principal allows the
distribution of L to be computed. We omit the details.
9
3.5 Optimal Choice of Parameters
For reasons that will become clear in the next section, we wish to maximise the value
of the quantity δ := 1 − α − β over the choice of distinguisher D acting on B bytes of
input. Let us denote this value by δB. (As temporary motivation note that δ is the usual
cryptographic deﬁnition of the advantage of the distinguisher.) Since we are interested in
optimal distinguishers, our choice of parameters (α, β) is subject to the constraint that
1−β
LB = β log2
α where the quantity LB was computed for diﬀerent
values of B in Section 3.3. Direct maximisation using Wolfram alpha6 yields δB = 0.00055
for B = 256 (at (α, β) = (0.499459, 0.499991)) and δB = 0.002273 for B = 4096 (at
(α, β) = (0.498343, 0.499384)).
1−α + (1 − β) log2
β
3.6 Implementation
We implemented the above statistical test to ensure that its practical performance is
in line with the theoretical analysis. As noted above the parameter η could be set by
calculating the distribution of the test statistic L when O comes from distribution θ0.
Another approach would be to generate many samples of the test statistic to estimate
the distribution of L and set η by computing the α-percentile of the sampled values.
In our experiments, we simply set η = 0, ran the test on random inputs and on
RC4 inputs, and computed the corresponding values of α, β, δ. This gives us sub-optimal
values for δ, but is suﬃcient to validate that the test works and gives us a distinguisher
having performance that is reasonably close to that which is theoretically obtainable.
For B = 4096 and based on 225 samples (224 for each of random input and RC4 inputs),
setting η = 0, we obtained α = 0.453706, β = 0.544646 and δ = 0.001648. Here the value
of δ compares favourably to the theoretical maximum of 0.002273. We are conﬁdent that
a closer match would be obtained by adjusting η. In our evaluation of our attack on HIVE
to follow, we present attack costs for both the ideal value of δ and our experimentally
obtained value.
4 An Attack on HIVE Based on a
Blockwise RC4 Distinguisher
We describe a family of adversaries {AB,S,T}, parameterized by the block length B and
two other parameters S and T which are deﬁned below. Our adversaries have access to
the distinguisher D acting on B bytes from Section 3. Recall that D outputs a single bit
b; if D’s input is uniformly random bytes, DB outputs 1 with probability α; if D’s input
consists of B consecutive bytes of RC4 output, D outputs 1 with probability 1 − β.
For a ﬁxed block length B, AB,S,T is a one-time adversary against plausible hiding for
two volumes, as in Deﬁnition 1. In this simple case, there are two encrypted volumes V1,
V2 protected by passwords P1, P2, and a user wants to be able to deny the existence of V2