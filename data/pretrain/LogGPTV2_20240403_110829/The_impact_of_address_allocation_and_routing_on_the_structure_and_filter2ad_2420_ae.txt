100
50
s
n
o
i
l
l
i
m
n
i
s
r
o
i
t
s
s
n
a
r
t
f
o
r
e
b
m
u
N
ARAM
CAMs-8
CAMs-14
0
100
200
300
400
Routing table size: Number of prefixes in thousands
Figure 11: The size of the routing table is increased by increas-
ing Fsplit (other parameters are kept constant). We see that
multibit tries scale better in this case. This is because load
shared preﬁxes are clustered together - more than multihom-
ing assignments.
pressibility of multibit tries is not improved with increasing table
size. For example, increasing Fspawn shows a linear scaling of the
multibit trie algorithm [4].
An interesting aspect of varying Cspawn is not apparent in Fig-
ure 10, but does become clearer in Figure 12. Notice that as Cspawn
increases, the shape of the tree, as captured by the unibit trie den-
sity, changes non-monotonically.
In particular, we see a peak in
the number of unibit nodes per preﬁx around Cspawn=0.1. This
non-monotonicity allows us to compute the worst-case number of
transistors for a given number of allocations. To do this, we use a
value of Cspawn of nearly 0.1, and set Fspawn to 1 and Fsplit to 0.
For the same number of allocations which contribute to the present
day routing table, we found that the worst case parameters above
generated 85,530 preﬁxes which required 541 transistors per pre-
ﬁx in the multibit trie algorithm. However the nominal parameters
produced a routing table with 77,817 preﬁxes which required 206
transistors per preﬁx. The intuition is that the value of Cspawn is
low enough not to allow the cost of a multibit node to be amortized
over many preﬁxes, but is high enough to ensure that all depth zero
preﬁxes spawn. Since TCAMs require 448-512 transistors per pre-
ﬁx, we can say that TCAMs would have scaled better (at least in
the near term) if there had been no load sharing (and other forms of
splitting) and all allocations spawned multihoming assignments.
We say near term because as the number of allocations increases,
the density of the trie increases enough to allow the multibit trie
to better TCAMs in the number of transistors. For this speciﬁc
set of parameters, multibit tries would overtake TCAMs when the
number of allocations grows to thrice their present number (graph
not shown).
4. RELATED WORK
In this section, we brieﬂy survey the literature on IPv4 address
lookup algorithms and on address allocation and routing growth.
IPv4 address lookup techniques have received a fair amount of
attention in recent years [36, 1, 2, 11, 12], and we do not attempt
to be exhaustive in our survey of this sub-ﬁeld. However, broadly
speaking, there are two classes of lookup techniques: algorithmic
approaches, exempliﬁed by [12], attempt to cleverly compress for-
warding tables compactly into memory, while hardware approaches
rely on (ternary) content-addressable memories (CAMs) to perform
fast lookup. We have described these schemes in Section 3.1.
Most work to date has evaluated their schemes on current routing
tables. However, as we have just shown, the memory requirements
of some of these lookup schemes can be sensitive to the relationship
between preﬁxes. To our knowledge, no work has examined how
these algorithms scale with routing table size.
However, to capture preﬁx relationships, we need to be able to
generate realistic routing tables. One closely related piece of work,
RTG [28], has attempted to generate routing tables for the purpose
of generating realistic BGP updates. RTG does not attempt to ex-
plicitly model preﬁx relationships in the BGP routing tables that it
generates. Rather, it takes an empirical approach: given a current
routing table, it extracts some statistics from the table, and attempts
to generate tables with comparable statistics. Furthermore, to ex-
trapolate routing table sizes to an order of magnitude larger than
today, it is necessary to explicitly (if only approximately) capture
how allocation and routing practice affects routing table entries.
This enables an examination of how quantitative deviations from
current routing practice affect IPv4 lookup algorithms. ARAM, un-
like RTG, explicitly models address allocation at the RIR level, and
contains parameters that model multihoming and trafﬁc engineer-
ing.
There exists a body of work that has measured or is examining
routing practice. For example, Huston [37] and Gao et al. [10],
have measured the prevalence and time evolution of routing prac-
tices but have not attempted to model these in order to generate
routing tables. More recently, Xu et al. [46] have analyzed address
allocation data and have correlated address allocations with routing
table growth. However, they do not attempt to model, as we do, the
structure of routing tables.
The IETF’s Ptomaine working group [38] seeks to reduce the
number of preﬁxes in the routing table thereby reducing load on
routing. Various ideas being examined include techniques to con-
trol route propagation by using BGP community attributes [39, 40],
ﬁltering routes on RIR allocation boundaries [41], or multihoming
techniques that can reduce routing load [42]. ARAM is comple-
mentary to these efforts, in that it does not prescribe measures that
would reduce routing table sizes, but could be used to describe the
shape of the resulting routing tables after changes in routing prac-
tice.
Basu and Narlikar [44] seek to reduce the effect of updates on
throughput of lookups in a pipeline. They do this by balancing
memory among the stages of a pipeline. To make a tradeoff be-
tween the speed of a ﬁxed stride multibit trie dynamic program-
ming algorithm and the efﬁciency of a variable stride multibit trie,
they selectively increase strides to cover contiguous /24s. As we
x
i
f
e
r
p
r
e
p
s
e
d
o
n
t
i
i
b
n
u
f
o
r
e
b
m
u
N
3.5
3
2.5
2
0
Baseline (Nov.2002 routing table)
ARAM
0.2
0.4
0.6
0.8
1
Cspawn
Figure 12: This graph shows the number of unibit trie nodes
per preﬁx as Cspawn is increased from 0 to 1 (other parameters
are kept constant). We see that the unibit trie density is lowest
when Cspawn is around 0.1. This helps to calculate the worst
case storage. The baseline is the unibit trie density for the Nov.
2002 routing table.
have seen this contiguity arises mainly due to load sharing.
Finally, Kohler et al. [43] examine the structure and distribution
of IP addresses found in trafﬁc traces, and point out that the oc-
currence of IP addresses in Internet trafﬁc is multifractal. This is
somewhat orthogonal to our work, which looks at IP address allo-
cation and preﬁx routing, rather than the trafﬁc levels originated by,
or destined to IP addresses.
5. CONCLUSIONS
This paper describes a model of the structure of Internet routing
tables. ARAM is a parameterizable, parsimonious, accurate and
predictive model of routing table growth. It qualitatively captures
the processes by which address blocks are allocated in the Internet,
and those by which they appear in routing tables. ARAM holds
intrinsic interest in enabling the study of routing table growth in
the abstract (i.e., independent of any particular lookup implemen-
tation). It also applies to the evaluation of lookup schemes, right
down to the transistor level.
Using such an evaluation, we ﬁnd that, to a ﬁrst order of approx-
imation, multibit tries scale better than TCAMs with increasing
routing table sizes. Furthermore, we ﬁnd that the disparity between
multibit tries and TCAMs increases with increased multihoming
and load-balancing. These results have interesting implications for
the choice of lookup technologies in routers. Of course, our conclu-
sions can be negated by CAM designs which have fewer transistors
per cell. There are rumors of such designs, but it unclear as to
whether there is substance behind these rumors.
As an aside, in deriving these results our paper spans several lev-
els of abstraction. Starting with the way addresses are allocated,
incorporating some of the mechanisms by which ISPs advertise
routing information, our paper ends with transistor level models
that quantify the cost of hardware implementation.
As routing practices evolve, it will be interesting to see how
accurately ARAM continues to model the routing table. For ex-
ample, an increasingly popular routing practice is a VPN (Virtual
Private Network) service provided by a backbone ISP. To provide
this service, ISPs need to keep and advertise separate routes to
each of the endpoints of the VPN. If this practice becomes signiﬁ-
cantly more prevalent than it is today, it may be necessary to tweak
ARAM’s splitting and spawning rules to better match routing prac-
tice. VPN preﬁxes could perhaps be modeled by more clustered
[19] FreeIPDB - The Next Generation IP Database.
www.freeipdb.org
[20] Northstar IP Management Tool.
www.brownkid.net/NorthStar
[21] J. J. Ramsden and Gy. Kiss-Haypal, Company Size Distribution in
Different Countries, Physica A, vol 277, pp. 220-227, 2000.
[22] James Sybert. Private Communication.
[23] Mike Loevner. Private Communication.
[24] ARIN Allocations.
ftp://ftp.arin.net/pub/stats/arin/
[25] APNIC allocations.
ftp://ftp.apnic.net/pub/stats/apnic/
[26] RIPE Allocations. ftp://ftp.ripe.net/ripe/stats/
[27] University of Oregon Route Views Project
www.routeviews.org
[28] O. Maennel and A. Feldmann. Realistic BGP Trafﬁc for Test Labs.
Proceedings of SIGCOMM 2002.
[29] Richard Jimmerson. Private Communication.
[30] Son Tran. Private Communication.
[31] Leo Vegoda. Private Communication.
[32] Ratiﬁed Policy 2001-2: Reassignments to multihomed downstream
customers Policy. www.arin.net/policy/2001 2.html
[33] Gerard Ross. Private Communication.
[34] A. Lord. Proposal for IPv4 allocations by LIRs to ISPs. APNIC Open
Policy Meeting, September 2002.
www.apnic.net/meetings/14/sigs/policy/docs/addrpol-
out-apnic-sub-alloc.pdf
[35] A. Lord. Downstream Allocations by LIRs: A proposal. RIPE 43
Meeting, September 2002.
www.ripe.net/ripe/meetings/archive/ripe-
43/index.html
[36] M. Ruiz-Sanchez, E. Biersack, W. Dabbous. Survey and Taxonomy of
IP Lookup Algorithms. IEEE Network. Vol. 15. Issue 2. 2001.
[37] G. Huston. Analyzing the Internet’s BGP Routing Table. Internet
Protocol Journal. Volume 4, Number1, March 2001.
[38] Preﬁx Taxonomy Ongoing Measurement & Inter Network
Experiment (ptomaine).
http://www.ietf.org/html.charters/ptomaine-
charter.html
[39] G. Huston. NOPEER community for BGP scope control.
draft-ietf-ptomaine-nopeer-00.txt April 2002.
[40] O. Bonaventure, S. DeCnodder, J. Haas, B. Quoitin, R. White.
Controlling the redistribution of BGP routes.
draft-ietf-ptomaine-bgp-redistribution-01.txt. August 2002
[41] S. Bellovin, R. Bush, T. Grifﬁn, J. Rexford. Slowing Routing Table
Growth by Filtering Based on Address Allocation Policies.
www.research.att.com/ jrex/papers/filter.pdf
[42] T. Bates and Y. Rekhter. Scalable Support for Multi-homed
Multi-provider Connectivity. RFC 2260.
[43] E. Kohler, J. Li, V. Paxson, and S. Shenker. Observed structure of
addresses in IP trafﬁc. Proceedings of 2nd Internet Measurement
Workshop, November 2002.
[44] Anindya Basu and Girija Narlikar. Fast Incremental Updates for
Pipelined Forwarding Engines. Proceedings of Infocom 2003.
[45] D. Shah and P. Gupta. Fast Updates on Ternary CAMs for Packet
Lookups and Classiﬁcation. Hot Interconnects 8.
[46] Zhiguo Xu, Xiaoqiao Meng, Cathy Wittbrodt, Songwu Lu, Lixia
Zhang. Address Allocation and the Evolution of the BGP routing
table. Technical Report CSD-TR03009 , UCLA Computer Science
Depratment, 2003. http://www.cs.ucla.edu/wing/pdfdocs/address tr.ps
splitting/spawning rules. VPNs can be a potential reason for preﬁx
tables to grow more than a million. Furthermore, it will be inter-
esting to track the evolution of allocation and routing practices for
IPv6, and to study whether ARAM can be extended to model IPv6
routing tables.
Finally, we do not expect ARAM to be the last word in routing
table modeling; as more accurate data becomes available, it may be
possible to better infer routing practice and therefore design more
accurate models than ARAM.
6. ACKNOWLEDGEMENTS
We thank the many people who answered our questions on ad-
dress allocation and routing practices. In alphabetical order, they
are, Cengiz Alaettinoglu, Buddy Bagga, Adam Bechtel, Randy Bush,
Peter Clark, John Crain, Daniel Golding, Ejay Hire, Lee Howard,
Richard Jimmerson, Glen Larwill, Herb Leong, Mike Loevner, Bill
Manning, Barry Margolin, Christopher Morrow, Alec Peterson, Mark
Prior, Gerard Ross, David Schwartz, James Sybert, Son Tran, Leo
Vegoda and Will Yardley. We also thank Will Eatherton for feed-
back on an earlier version of the paper. Harsha Narayan and George
Varghese were funded by NSF grant ANI-0074004. Ramesh Govin-
dan was partially funded by the NSF under grant number ANI-
0112649.
7. REFERENCES
[1] V. Srinivasan, G. Varghese. Fast Address Lookups Using Controlled
Preﬁx Expansion. ACM Transactions on Computer Systems, Volume
19, Number 4, November 2001.
[2] M. Waldvogel, G. Varghese, J. Turner, B. Plattner. Scalable
High-Speed Preﬁx Matching. ACM Transactions on Computer
Systems, Volume 17, Issue 1, February 1999.
[3] David E. Taylor, Jonathan S. Turner, John W. Lockwood, Todd S.
Sproull, David B. Parlour. Scalable IP Lookup for Internet Routers.
IEEE Journal on Selected Areas in Communications, Volume 21,
Number 4, May 2003.
[4] H. Narayan, R. Govindan, G. Varghese. The Impact of Address
Allocation and Routing on the Structure and Implementation of
Routing Tables. University of California,San Diego Tech Report CS
2003-0749.
[5] E. Zegura, K. Calvert, S. Bhattacharjee. How to Model an
Internetwork. Proceedings of INFOCOM 1996.
[6] V. Paxson. End-to-End Internet Packet Dynamics. Proceedings of
SIGCOMM 1997.
[7] C. Labovitz, A. Ahuja, A. Bose, F. Jahanian. Delayed Internet
Routing Convergence. Proceedings of SIGCOMM 2000.
[8] C. Labovitz, R. Malan, F. Jahanian. Origins of Internet Routing
Instability. Proceedings of INFOCOM 1999.
[9] Lightreading. www.lightreading.com.
[10] T. Bu, L. Gao and D. Towsley. On Characterizing Routing table
growth. Proceedings of GlobalInternet 2002.
[11] M. Degermark, A. Brodnik, S. Pink. Small Forwarding Table for Fast
Routing Lookups. Proceedings of SIGCOMM 1997.
[12] W. Eatherton et.al. Tree Bitmap : Hardware/Software IP Lookups
with Incremental Updates. Available at
www.eathertons.com/sigcomm-withnames.PDF
[13] IANA /8 delegations.
www.iana.org/assignments/ipv4-address-space
[14] Policies for IPv4 address space management in the Asia Paciﬁc
region. www.apnic.net/docs/policy/add-manage-
policy.html
[15] ARIN Policies. www.arin.net/policy/index.html
[16] M. Khne, N. Nimpuno, P. Rendek, S. Wilmot. IPv4 Address
Allocation and Assignment Policies in the RIPE NCC Service Region.
www.ripe.net/docs/ipv4-policies.html
[17] Daniel Golding. Private Communication.
[18] Mark Prior. Private Communication.