Case 2: If 2l − 2lx ≤ x  x2 and c = 1 otherwise. After the truncation,
(cid:7)(cid:11)x(cid:12)(cid:8)0 = (cid:11)x + r − 2l(cid:12) = r1 − x1 − c and (cid:7)(cid:11)x(cid:12)(cid:8)1 = 2l − r1.
Therefore, Rec
Finally, the probability that our assumption holds, i.e. the
probability of a random r being in the range [2lx , 2l − 2lx ) is
1 − 2lx+1−l.
((cid:7)(cid:11)x(cid:12)(cid:8)0,(cid:7)(cid:11)x(cid:12)(cid:8)1) = 2l − x1 − c = (cid:11)x(cid:12) − c.
A
Theorem 1 can be extended to a prime ﬁeld Zp in a natural
way by replacing 2l with p in the proof. We also note that the
truncation does not affect security of the secret sharing as the
shares are truncated independently by each party without any
interaction.
EFFECT OF TRUNCATION ON TRAINING FOR MNIST AND
APPENDIX C
ARCENE DATASET
We run our privacy preserving linear regression protocol with
the truncation technique on the MNIST dataset [6] consisting
of images of handwriting digits and compare accuracy of
the trained model to plaintext training with standard decimal
numbers operations. The mini-batch size is set to |B| = 128
−7. The input data has 784
and the learning rate is α = 2
features, each a gray scale of a pixel scaled between 0 and
1, represented using 8 decimal bits. We set the ﬁeld to Z264.
For a fair comparison, coefﬁcients are all initialized to 0s and
the same sequence of the mini-batch indices are used for all
trainings. To simplify the task, we change the labels to be 0 for
digit “0” and 1 for non-zero digits. In Figure 10, the x-axis is
the number of iterations of the SGD algorithm and the y-axis
is the accuracy of the trained model on the testing dataset.
Here we reconstruct the coefﬁcient vector after every iteration
in our protocol to test the accuracy. As shown in Figure 10,
when we use 13 bits for the fractional part of w, the privacy
preserving training behaves almost exactly the same as the
plaintext training. This is because we only introduce a small
error on the 13th bit of the decimal part of w. Our experiments
never triggered the failure condition in theorem 1. However,
when we use 6 bits for the decimal part of w, the accuracy of
our protocol oscillates during the training. This is because now
the error is on the 6th bit which has a larger effect and may
push the model away from the optimum. When the distance
)
%
(
y
c
a
r
u
c
c
A
100
80
60
40
20
0
Plaintext Training
Privacy Preserving 13 bits
Privacy Preserving 6 bits
Privacy Preserving 2 bits
0 10 20 30 40 50 60 70 80 90 100 110 120
Number of iterations
Fig. 10: Comparison of accuracy of privacy preserving linear
regression with truncation and plaintext training on decimal
numbers.
35
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
to the optimum is large enough, the SGD will move back
towards the optimum again. Finally, when we use 2 bits for
the fractional part, the oscillating behavior is more extreme.
We observe a similar effect when training on another dataset
called Arcene [1] as shown in Figure 11. In other words, when
sufﬁcient bits are used to represent the fractional part of the
coefﬁcients, our new approach for ﬁxed-point multiplication
of shared decimal numbers has little impact on accuracy of
the trained model.
PROOF OF SECURITY FOR PRIVACY PRESERVING LINEAR
APPENDIX D
REGRESSION
We repeat the theorem of security for privacy preserving
linear regression here and provide a proof sketch.
Theorem. Consider a protocol where clients distribute arith-
metic shares of their data among two servers who run the
protocol of Figure 4 and send the output
to clients. In
the Fof f line hybrid model, this protocol realizes the ideal
functionality Fml of Figure 3 for the linear regression function,
in presence of a semi-honest admissible adversary (see section
III).
sketch. An admissible adversary in our model can corrupt one
server and any subset of the clients. Given that the protocol is
symmetric with respect to the two servers, we simply need to
consider the scenario where the adversary corrupts S0 and all
but one of the clients, i.e. C1, . . . ,Cm−1.
We describe a simulator S that simulates the above adversary
in the ideal world. S submits the corrupted clients’ inputs data
to the functionality and receives the ﬁnal output of the linear
regression i.e. the ﬁnal value of the coefﬁcients w back.
S then runs A. On behalf of the honest client(s) S sends a
random share in Z2l to A for each value in the held by that
client. This is the only message where clients are involved.
In the remainder of the protocol, generate random matrices
and vectors corresponding to the honest server’s shares of
(cid:7)X(cid:8),(cid:7)Y(cid:8),(cid:7)U(cid:8),(cid:7)V(cid:8),(cid:7)Z(cid:8),(cid:7)V(cid:3)(cid:8),(cid:7)Z(cid:3)(cid:8), and play the role of the
honest server in interactions with A using those randomly
generated values.
Finally, in the very last step where w is to be recovered,
S adjusts the honest servers’ share of of w such that the
)
%
(
y
c
a
r
u
c
c
A
100
80
60
40
20
0
Plaintext Training
Privacy Preserving 12 bits
Privacy Preserving 4 bits
Privacy Preserving 2 bits
0 40 80 120 160 200 240 280 320 360 400 440 480
Number of iterations
Fig. 11: Comparison of accuracy between privacy preserving
linear regression with truncation and plaintext training on
decimal numbers, on the Arcene dataset. |B| = 32.
36
recovered value is indeed the coefﬁcient vector it received
from the functionality. This concludes the simulation.
We brieﬂy argue that the A’s view in the real and ideal
worlds and as a result, the environment’s view in the two
worlds is indistinguishable. This immediately follows from
the security of the arithmetic secret sharing and the fact that
the matrices/vectors generated in the ofﬂine phase are indeed
random. In particular, all messages sent and received and
reconstructed in the protocol (with the exception of w are
generated using uniformly random shares in both the real
protocol and the simulation described above, so indeed the view
are both identically distributed. this concludes our argument.
We note that this argument implicitly explains why using
one mask matrix U is sufﬁcient to hide the data matrix X. The
reason is that the adversary only gets to see the masked value
once in the ﬁrst interaction and the rest of the computation on
X takes place without interactions between the honest and the
corrupted server.
PRIVACY PRESERVING PREDICTION AND ACCURACY
APPENDIX E
TESTING
Privacy preserving prediction. As a side product, our proto-
cols also support privacy preserving predictions. The algorithm
∗ for
is exactly the same as computing the predicted value y
linear regression, logistic regression and neural networks and
the cost is only half of one iteration. We iterate that we can
hide the input data, the model, the prediction result or any
combinations of them, as they can all be secret shared in our
protocols. Table IV summarizes the cost of predictions. Note
that the online phase is extremely fast, which beneﬁts latency
critical applications as the ofﬂine phase can be precomputed
independently of the data. In addition, because of vectorization,
the time grows sublinearly when making multiple predictions
in parallel.
k
Linear (ms)
Logistic (ms)
Ofﬂine
Ofﬂine
Neural (s)
Ofﬂine
Online
0.20
0.22
72
215
LAN
WAN
1
100
1
100
4.7
13.8
17.8
472
TABLE IV: Online and ofﬂine performances for privacy
preserving predicition.
2.5
51
620
2010
2.5
51
620
2010
Online
0.59
3.9
158
429
Online
0.18
0.20
0.57
1.2
If either the input data or the model can be revealed, the
efﬁciency can be further improved. E.g., if the model is
in plaintext, the multiplications of the input data with the
coefﬁcients can be computed directly on the shares without
precomputed multiplication triplets.
In classiﬁcation problems, the prediction is usually rounded
to the closest class. E.g, in logistic regression, if the predicted
value is 0.8, the data is likely to be classiﬁed as 1, and the exact
result may reveal extra information on the input. This rounding
can be viewed as testing whether a secret shared value minus
1
2 is larger than 0, and can be supported by applying an extra
garbled circuit, similar to how we approximated the logistic
function. The garbled circuit would add the two shares and
output the most signiﬁcant bit.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
Protocol LHE MT((cid:7)A(cid:8)0;(cid:7)B(cid:8)1):
(Let aij be the (i, j)th element in (cid:2)A(cid:3)0 and bj be the jth element
in (cid:2)B(cid:3)1.)
1: S1 → S0: Enc(bj) for i = 1, . . . , d.
2: S0 → S1: ci = Πd
3: S0
r =
4: S1 sets (cid:7)(cid:7)A(cid:8)0 × (cid:7)B(cid:8)1(cid:8)1 = (Dec(c1), . . . , Dec(c|B|))T ,
1, . . . ,|B|.
sets
(−r1, . . . ,−r|B|)T mod 2l.
(cid:7)(cid:7)A(cid:8)0 × (cid:7)B(cid:8)1(cid:8)0 = r, where
j=0Enc(bj)aij · Enc(ri), for i =
Fig. 12: The ofﬂine protocol based on linearly homomorphic
encryption.
Privacy preserving accuracy testing. A simple way to decide
the learning rate is to test it on some insensitive data of the
same category beforehand, and set it to a constant without
any adjustment throughout training. Similarly, the number of
iterations can be ﬁxed in advance.
At the cost of some leakage, we propose an alternative
solution that enables adjusting the rate and number of iteration
in the same fashion as plaintext training. To do so, we need to
test the accuracy of the current model after each epoch on a
testing dataset. As a ﬁrst step, we simply perform a privacy
preserving prediction for each testing data sample. Then, we
test whether it is the same as the label and aggregate the result.
Again we use a simple garbled circuit to perform the equality
test, in which the number of gates is linear in the bit length
of the values. Finally, each party sums up all the secret-shared
results of equality test as the shared accuracy. The cost of doing
so is only running half of an iteration plus some extra garbled
circuits for rounding and equality testing. As the size of the
testing data is usually signiﬁcantly smaller than the training
data, the time spent on the accuracy testing is only a small
portion of the training.
To adjust the learning rate, we compare the shared accuracy
of two epochs using a garbled circuit and reduce the learning
rate if the accuracy is decreasing. Similarly, we calculate the
difference of the accuracy and test if it is smaller than a
threshold using a garbled circuit, and terminate if the model
converges. All these tests are done on the aggregated accuracy,
which is a single value per epoch and independent of the
number of the training and testing data samples, thus the
overhead is negligible. Notice that in each epoch, whether
or not we adjust the learning rate or whether we terminate
or not leaks one extra bit of information hence providing a
trade-off between the efﬁciency (reduced number of epochs)
and security, compared to using a ﬁxed learning rate and a
ﬁxed number of iterations.
APPENDIX F
CLIENT-AIDED MULTIPLICATION TRIPLETS
We start with the linear regressions for simplicity. Note
that in the whole training, each feature in each data sample
is used exactly in two multiplications per epoch: one in the
forward propagation and the other in the backward propagation.
Therefore, it sufﬁces for the client holding this value to generate
Protocol SGD Logistic((cid:7)X(cid:8),(cid:7)Y(cid:8),(cid:7)U(cid:8),(cid:7)V(cid:8),(cid:7)Z(cid:8),(cid:7)V(cid:3)(cid:8),(cid:7)Z(cid:3)(cid:8)):
1: Do step 1–5 as in Figure 4. Both parties obtain the
(cid:8) in
× w(cid:8) (it was deﬁned as (cid:7)Y∗
(cid:8) = (cid:7)XBi
Bi
shares (cid:7)UBi
Figure 4).
2: for every element (cid:7)u(cid:8) in (cid:7)UBi