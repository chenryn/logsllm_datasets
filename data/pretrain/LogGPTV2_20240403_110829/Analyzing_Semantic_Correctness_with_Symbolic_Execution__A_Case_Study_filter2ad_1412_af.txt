its own RSA implementation, written using the libtommath
multiple-precision integer library. Based on our root-cause
analysis, it appears that the PKCS#1 v1.5 signature veriﬁcation
implemented in the RSA implementation of Dropbear SSH
follows the construction-based approach, hence it successfully
cross-validated with the anchor and no particular weaknesses
to the bundled libtomcrypt which
were found. In contrast
has some signature veriﬁcation weaknesses (explained below),
having its own RSA implemented actually helped Dropbear
SSH to avoid some exploitable vulnerabilities.
Comparing to other implementations of construction-based
veriﬁcation (e.g., BoringSSL), the TH1 of Dropbear SSH took
a signiﬁcantly longer time to run, mainly due to the ﬁnal
comparison after constructing the expected Iv is done in the
multiple-precision integer level, not with a typical memory
comparison function like memcmp(). Nevertheless, it still
managed to ﬁnish within a reasonable amount of time. As
a side beneﬁt, symbolic execution also covered part of the
multiple-precision integer libtommath code.
7) libtomcrypt 1.16: Based on our test results, we found
that libtomcrypt is also using a parsing-based approach, and
its signature veriﬁcation contains various weaknesses5.
5Some of the weaknesses had been independently found by other re-
searchers, leading to certain ﬁxes being introduced in version 1.18.
{
{
... ...
else {
else if (oi == OID_SHA1_ALG)
psAssert(outlen == SHA1_HASH_SIZE);
psAssert(outlen == SHA512_HASH_SIZE);
}
}
}
10
Accepting less than 8 bytes of padding:
Accepting trailing bytes: Similar to axTLS, libtomcrypt
also has the classical ﬂaw of accepting signatures with trailing
bytes after H(ms), hence a practical signature forgery attack
is possible when the public exponent is small enough. This is
the reason why for TH1 and TH2, there are 3 accepting paths.
Interestingly,
libtomcrypt also has the classical ﬂaw of not checking whether
PS has a minimum length of 8, similar to strongSwan. Through
root-cause analysis, we quickly identiﬁed the lax padding
check as shown below. Give this veriﬁcation ﬂaw, to avoid
scalability challenges due to symbolic padding bytes, we apply
the same workaround to TH1 as we did for strongSwan.
Snippet 5. Padding Check in libtomcrypt 1.16
for (i = 2; i  (size_t) ( end - *p ) )
return( MBEDTLS_ERR_ASN1_OUT_OF_DATA );
Since after the parser consumed y, there would be 22 bytes
left in the buffer (assuming no parameter bytes, 2 + 20 for a
SHA-1 hash), it turns out the veriﬁcation code would accept
any values of y not larger than 22, which allows some bits of
AS to be arbitrarily chosen and is hence overly permissive.
9) BoringSSL 3112, BearSSL 0.4 and wolfSSL 3.11.0:
BoringSSL is a fork of OpenSSL, refactored and maintained
by Google. We found its PKCS#1 v1.5 signature veriﬁcation
uses a hybrid approach. Everything before AS in O is handled
and checked by a parser that scans through the buffer, and then
AS is copied out. The veriﬁcation code then constructs its own
expected version of ASv using H(mv) and some hard-coded
ASN.1 preﬁxes, and then compares ASv against AS. This
observed behavior is consistent with what was reported earlier
[13]. Consequently, the total number of paths are reasonably
small, with each of {TH1, TH2, TH3} yielding exactly one ac-
cepting path. BearSSL and wolfSSL both behaved quite similar
to BoringSSL, and all 3 implementations successfully cross-
validate against the anchor with no discrepancies observed.
wolfSSL yielded more paths in TH1 due to a slightly different
handling of PB, and BearSSL yielded more paths in TH2 due
to extra handling of the case of absent parameter.
10) OpenSSL 1.0.2l and LibreSSL 2.5.4: We found that
OpenSSL adopts a parsing-based veriﬁcation approach, which
partly explains why some higher number of paths were yielded
by TH2 and TH3. The slightly longer execution time of TH3
can partly be attributed to the concretization workaround.
Despite these, no veriﬁcation weaknesses were found in this
recent version of OpenSSL, which is perhaps unsurprising
given that it had gone through years of scrutiny by security
researchers [27]. LibreSSL is a fork of OpenSSL maintained
by the OpenBSD community since 2014 after the infamous
Heartbleed vulnerability. The two are actually quite similar
when it comes to PKCS#1 v1.5 signature veriﬁcation, both
using a similar parsing-based approach and the test harnesses
all yielded comparable numbers of execution paths.
11) PuTTY 0.7: We found that the PuTTY implementation
of PKCS#1 v1.5 signature veriﬁcation is highly reminiscent
of a construction-based approach. The left-most 2 bytes of O
containing 0x00 and BT are checked ﬁrst, followed by a check
on PB with an expected length (which depends on |n|), and
then AS before H(ms) is checked against some hard-coded
ASN.1 encoded bytes, and ﬁnally, H(ms) is checked. Cross-
validation found no discrepancies and no signature veriﬁcation
weaknesses were detected.
Interestingly, even after sufﬁcient rejection criteria has been
hit (e.g., BT is not 0x01), the veriﬁcation continues with other
checks, until all has been ﬁnished and then an error would ﬁ-
nally be returned. Since the later checks before the veriﬁcation
function returns do not alter a rejection return code back into
an acceptance, this is not a veriﬁcation weakness. We suspect
this insistence on traversing the whole buffer containing O
might be an attempt to avoid timing side channels.
which can be abused in a manner similar to the signature
forgery attack exploiting the weakness of not checking al-
gorithm parameters found in some other implementations as
discussed in previous work [27].
However, as explained below with Example 2, such an
implementation presents a small hurdle for symbolic execution,
as the number of paths due to if statements (the series of
checks) exhibits a multiplicative build-up, leading to a scal-
ability challenge observed in our ﬁrst round experiment with
TH1. Consequently, we modiﬁed the source to adopt an ‘early
return’ logic, like a typical implementation of memcmp()
would do. That is, once a sufﬁcient rejection condition has
been reached, the veriﬁcation function returns with an error
without continuing with further checks, so that the number of
paths would build up additively. This explains why the number
of lines changed in PuTTY is slightly higher than the others.
if (symBuf[0] != 0) ret = 0;
if (symBuf[1] != 1) ret = 0;
if (symBuf[2] != 2) ret = 0;
return ret;
if (symBuf[0] != 0) return 0;
if (symBuf[1] != 1) return 0;
if (symBuf[2] != 2) return 0;
Example 2: For number of execution paths, the snippet on right builds up
additively, but the one on left does so multiplicatively.
12) OpenSSH 7.7: OpenSSH is another open source
SSH software suite. For handling PKCS#1 v1.5 signatures,
it relies on OpenSSL (calling RSA_public_decrypt())
to perform the RSA computation and process the paddings
of O. Afterwards, it compares the AS returned by OpenSSL
against its constructed version, hence it is somewhat of a
hybrid approach. Cross-validation found no discrepancies and
no weaknesses were detected in the veriﬁcation.
Interestingly,
the
comparison against the constructed AS is done using a custom
constant time comparison, as shown below:
instead of simply using memcmp(),
/** p1,2 point to buffers of equal size(=n) **/
for (; n > 0; n--) ret |= *p1++ ˆ *p2++;
return (ret != 0);
This explains why TH3 found in total only 2 paths of relatively
larger constraints, as such a timing safe comparison would
aggregate (with OR) the comparison (with XOR) of each byte
in the two buffers. Semantically, the 2 execution paths mean
either all length variables u, w, x, y, z in TH3 match their
expected values exactly, or at least one of them does not.
VI. EXPLOITING OUR NEW FINDINGS
Here we discuss how to exploit the several weaknesses
presented in the previous section. For ease of discussion,
we focus on SHA-1 hashes, but the attacks can be adapted
to handle other hash algorithms by adjusting the lengths of
appropriate components. Though low-exponent RSA public
keys are rarely seen in the Web PKI nowadays [18], there are
speciﬁc settings where low-exponent keys are desired (e.g.,
with extremely resource-constrained devices). Historically, a
small public exponent of e = 3 has been recommended for
better performance [RFC3110], and there are key generation
programs that still mandate small public exponents [7].
1) Signature forgery against Openswan: The ﬂaw of
ignoring padding bytes effectively means Openswan would
accept a malformed O(cid:48) in the form of
0x00 || 0x01 || GARBAGE || 0x00 || AS ,
12
IPSec,
This has serious security implications. We note that
in the context of
the key generation program
ipsec_rsasigkey forces e = 3 without options for choos-
ing larger public exponents [7]. Since the vulnerable signature
veriﬁcation routine is used by Openswan to handle the AUTH
payload, the ability to forge signatures might enable man-
in-the-middle adversaries to spoof an identity and threaten
the authentication guarantees delivered by the IKE_AUTH
exchange when RSA signature is used for authentication.
Given the implementation ﬂaw allows for certain bytes in
the middle of O(cid:48) to take arbitrarily any values, the goal of the
attack is to forge a signature S(cid:48) = (k1 + k2), such that when
the veriﬁer computes O(cid:48) = S(cid:48)3 = (k1 +k2)3 = k1
2k2 +
3k2
3, the following properties would hold:
2k1 + k2
3 +3k1
1) the most signiﬁcant bits of k1
3 would be those that need to
be matched exactly before the unchecked padding bytes,
which is simply (0x00 || 0x01);
2) the least signiﬁcant bits of k2
3 would become those that
need to be matched exactly after the unchecked padding
bytes, which is simply (0x00 || AS);
3 and the least signiﬁcant
2k1, would stay in
2k2 + 3k2
3) the most signiﬁcant bits of k2
3, along with 3k1
bits of k1
the unchecked padding bytes.
One inﬂuential factor to the success of such attack is
whether there are enough unchecked bytes for an attacker to
use. An insufﬁcient amount would have the terms of expanding
(k1 + k2)3 overlapping with each other, make it difﬁcult
for the three properties to hold. However, since the ﬂaw
we are exploiting is on the handling of padding bytes, the
number of which grows linearly with |n|, assuming the same
public exponent, a longer modulus would actually contribute
to the attacker’s advantage and make it easier to forge a
signature. Speciﬁcally, assuming SHA-1 hashes and e = 3,
given |n| ≥ 1024 bits, it should be easy to ﬁnd k1 and k2 that
satisfy the three properties without worrying about overlaps.
Finding k1. The main intuition used is that a feasible k1 can
be found by taking a cubic root over the desired portion of O(cid:48).
For instance, in the case of |n| = 1024 bits, 0x00 || 0x01
|| 0x00 ... 0x00 is simply 21008 (with 15 zero bits in
front), hence a simple cubic root would yield a k1 = 2336.
In the more general cases where |n| − 15 − 1 is not a
over-approximation. One can ﬁrst compute t1 = (cid:100) 3√
multiple of 3, the trailing garbage could be used to hide an
2|n|−15−1(cid:101)
and then sequentially search for the largest possible r such that
((t1/2r + 1) · 2r)3 gives 0x00 || 0x01 || GARBAGE.
Then k1 would be (t1/2r + 1) · 2r. This is to make as many
ending bits of k1 to be zero as possible, to avoid overlapping
terms in the expansion of (k1 + k2)3. For example, when
|n| = 2048 bits, we found r = 676 bits and k1 = 3 · 2676.
Finding k2. The intuition is that to get (0x00 || AS) with
3, the modular exponentiation can be seen as computed
k2
over a much smaller n(cid:48)(cid:48) instead of the full modulus n. While
ﬁnding φ(n) reduces to factorizing n, which is believed to be
impractical when n is large, ﬁnding φ(n(cid:48)(cid:48)) can be quite easy.
One can consider S(cid:48)(cid:48) =(0x00 || AS) and n(cid:48)(cid:48) = 2|S(cid:48)(cid:48)|,
where |S(cid:48)(cid:48)| is the size of AS in number of bits plus 8 bits for
the end of padding 0x00.
Now k2 has to satisfy k2
e ≡ S(cid:48)(cid:48) (mod n(cid:48)(cid:48)). Since n(cid:48)(cid:48) is
a power of 2, we can guarantee k2 and n(cid:48) are coprime by
choosing an odd numbered S(cid:48)(cid:48) with a ﬁtting hash value. Also,
φ(n(cid:48)(cid:48)) = φ(2|S(cid:48)(cid:48)|) = 2|S(cid:48)(cid:48)|−1.
One can then use the Extended Euclidean Algorithm to ﬁnd
f such that ef ≡ 1 (mod 2|S(cid:48)(cid:48)|−1). With f found, k2 would
simply be S(cid:48)(cid:48)f (mod n(cid:48)(cid:48)).
We have implemented attack scripts assuming e = 3 and
SHA-1 hashes, and were able to forge signatures that would
be successfully veriﬁed by Openswan 2.6.50 given any |n| =
1024 and |n| = 2048 moduli.
2) Signature forgery (1) against strongSwan: The ﬂaw of
not checking algorithm parameter can be directly exploited for
signature forgery following the algorithm given in [27] (which
is very similar to the attack we described previously against
Openswan). Assuming e = 3, |n| = 1024 bits and SHA-1
hashes, the expected iterations required to brute-force a fake
signature is reported to be 221 [27].
3) Signature forgery (2) against strongSwan: Likewise,
the ﬂaw of accepting trailing bytes after OID can be ex-
ploited following the steps used in the forgery attack against
3 and k2
3
Openswan as described before, by adjusting what k1
represent. Under the same parameter settings, it should require
a comparable number of iterations as signature forgery (1) does
discussed above.
4) Signature forgery (3) against strongSwan:
Interest-
ingly, the ﬂaw of accepting less than 8 bytes of padding can be
exploited together with the algorithm parameter ﬂaw to make
it easier to forge signatures. In fact, the two ﬂaws together
means such an O(cid:48) with no paddings at all would be accepted:
/** all numbers below are hexadecimals **/
00 01 00 30 7B 30 63 06
GARBAGE 04 16 SHA-1(m’)
05 2B 0E 03 02 1A 05 5A
The length of algorithm parameter 0x5A is calculated based on
|n| (in this case 1024 bits) and the size of hash. Then by simply
3 represent in the attack against
adjusting what k1
Openswan, given e = 3 and |n| ≥ 1024 bits, the forgery will
easily succeed. We implemented this new variant of attack and