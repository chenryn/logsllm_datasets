ipants exchange trafﬁc with each other (and, more often than not,
with the same participant). For instance, in an IXP where every
participant sends to AS X, AS X’s policies would be sequentially
composed with all policies. Currently, the Pyretic compiler would
recompile the same sub-policy multiple times. It would therefore
compile PA’’, PB’’, and PC’’ twice. To accelerate compilation,
the SDX controller memoizes all the intermediate compilation re-
sults before composing the ﬁnal policy.
4.3.2 Optimizing incremental updates
SDX compilation occurs not only at initialization time, but also
whenever a change occurs in the set of available BGP routes after
one or more BGP updates. Efﬁciently coping with these changes
is important. The SDX runtime supports fast recompilation by
exploiting three characteristics BGP update patterns: (1) preﬁxes
that are likely to appear in SDX policies tend to be stable; (2) most
BGP route changes only affect a small portion of the forwarding
table; and (3) BGP route changes occur in bursts and are separated by
large periods with no change at all. We draw these observations from
a week-long analysis of BGP updates collected at BGP collectors in
three of the largest IXPs in the world. Table 1 summarizes the data
that we used for this analysis.
Based on these observations, we augmented the basic SDX com-
pilation with an additional compilation stage that is invoked im-
mediately whenever BGP routes change. The main recompilation
algorithm is then executed in the background between subsequent
bursts of updates. We tune the optimization to handle changes that
result from BGP updates, because BGP updates are signiﬁcantly
more frequent than changes to the participants’ SDX policies.
Preﬁxes that are likely to appear in SDX policies tend to be
stable. Only about 10–14% of preﬁxes saw any BGP updates at
all for an entire week, suggesting that most preﬁxes are stable.
Furthermore, previous work suggests that the stable preﬁxes are also
the same ones that carry the most trafﬁc [15]. Hence, those stable
preﬁxes are also the ones that are likely to be associated with SDX
policies.
Most BGP update bursts affect a small number of preﬁx groups.
Updates and best path changes tend to occur in bursts. In 75% of
the cases, these update bursts affected no more than three preﬁxes.
Over one week, we observed only one update burst that triggered
updates for more than 1,000 preﬁxes. In the common case, the SDX
thus only needs to recompute ﬂow table entries for a few affected
preﬁx groups. Even in cases where bursts are large, there is a linear
557the participants—as well as BGP routes from the route server, and it
produces forwarding rules that implement the policies. The route
server processes BGP updates from participating ASes and provides
them to the policy compiler and re-advertises BGP routes to partici-
pants based on the computed routes. We brieﬂy describe the steps
of each of these functions below.
SDX policy compiler. The policy compiler is a Pyretic process
that compiles participant policies to forwarding rules. Based on the
virtual SDX abstraction from the SDX conﬁguration (i.e., the static
conﬁguration of which ASes are connected to each other at layer
two), the policy compiler isolates the policies that each AS writes
by augmenting each policy with a match statement based on the
participant’s port. The compiler then restricts each participant’s
outbound policies according to the current BGP routing information
from the route server and rewrites the participant policies so that
the switch can forward trafﬁc according to the default BGP policies.
After augmenting the policies, the compiler then computes VNH
assignments for the advertised preﬁxes. Finally, the compiler writes
the participant policies where necessary, taking care to avoid unnec-
essary composition of policies that are disjoint and performing other
optimizations such as caching of partial compilations, as described
in Section 4.3. It then passes the policies to the Pyretic compiler,
which generates the corresponding forwarding rules.
Because VNHs are virtual IP addresses, the controller also imple-
ments an ARP responder that responds to ARP queries for VNHs
with the appropriate VMAC addresses.
SDX route server. We implemented the SDX route server by ex-
tending ExaBGP [5], an existing route server that is implemented
in Python. As in other traditional route servers [2, 14], the SDX
route server receives BGP advertisements from all participants and
computes the best path for each destination preﬁx on behalf of each
participant. The SDX route server also (1) enables integration of
the participant’s policy with interdomain routing by providing ad-
vertised route information to the compiler pipeline; and (2) reduces
data-plane state by advertising virtual next hops for the preﬁxes
advertised by SDX participants. The SDX route server recompiles
the participants’ policies whenever a BGP update results in changes
to best routes for a preﬁx. When such an update occurs, the route
server sends an event to the policy handler, which recompiles poli-
cies associated with the affected routing updates. The compiler
installs new rules corresponding to the BGP update while perform-
ing the optimizations described in Section 4.3 in the background.
After compiling the new forwarding rules, the policy compiler then
sends the updated next-hop information to the route server, which
marshals the corresponding BGP updates and sends them to the
appropriate participant ASes.
5.2 Deployment
We have developed a prototype of the SDX [18] and a version that
can be deployed using virtual containers in Mininet [7]. Figure 4
shows two setups that we have created in these environments for the
purposes of demonstrating two applications: application-speciﬁc
peering and wide-area load balance. For each use case, we explain
the deployment setup and demonstrate the outcome of the running
application. For both use cases, we have deployed an SDX controller
(including route server) that is connected to an Open vSwitch soft-
ware switch. The ASes that we have connected to the Open vSwitch
at the exchange point are currently virtual (as our deployment has
no peers that carry real Internet trafﬁc), and these virtual ASes in
turn establish BGP connectivity to the Internet via the Transit Por-
tal [19]. The client generates three 1 Mbps UDP ﬂows, varying the
Figure 3: The SDX controller implementation, which has two pipelines: a
policy compiler and a route server.
relationship between the burst size and recompilation time and, as
we explain next, this recompilation can occur in the background.
BGP bursts are separated by large periods with no changes, en-
abling quick, suboptimal reactions followed by background re-
optimization. We observed that the inter-arrival time between BGP
update bursts is at least 10 seconds 75% of the time; half of the
time, the inter-arrival time between bursts is more than one minute.
Such large inter-arrival times enable the SDX runtime to adopt a
two-stage compilation approach, whereby time is traded for space
by combining: (1) a fast, but suboptimal recompilation technique,
that quickly reacts to the updates; and (2) an optimal recompilation
that runs periodically in the background.
The fast stage works as follows. Whenever there is a change in
the BGP best path pertaining to a preﬁx p, the SDX immediately
creates a new VNH for p and recompiles the policy, considering
only the parts related to p. It then pushes the resulting forwarding
rules into the data plane with a higher priority. The computation
is particularly fast because: (1) it bypasses the actual computation
of the VNH entirely by simply assuming a new VNH is needed;
(2) it restricts compilation to the parts of the policy related to p. In
Section 6, we show that sub-second recompilation is achievable for
the majority of the updates. Although the ﬁrst stage is fast, it can
also produce more rules than needed, since it essentially bypasses
VNH optimization.
5 Implementation and Deployment
We now describe the implementation of the SDX controller, as well
as our current deployment. We then describe several applications
that we have implemented with the SDX. We describe one applica-
tion with outbound trafﬁc control (application-speciﬁc peering) and
one with inbound trafﬁc control (wide-area load balance).
5.1
Figure 3 shows the SDX controller implementation, which has two
main pipelines: a policy compiler, which is based on Pyretic; and
a route server, which is based on ExaBGP. The policy compiler
takes as input policies from individual participants that are written
in Pyretic—which may include custom route advertisements from
Implementation
!"#$%&%’"($)!*+%&,-."(/+0#1)*+"$%*(1(&*#’*#"$%(2-34!506"7+$-8*#9"#/%(2:*;’*)%$%*(!,#0$%&!*+%&%0)’/"$0)34!-/0&%)%*(-’#*&0))34!-?((*7(&0;0($)@’0(8+*9-A7+0)!""#$$%&’($%)*%!&+%,-./#@’$%;%B"$%*(C!"#"D-&*;’7$0-EF.G!*+%&,-:*;’%+0#1(’7$-A13)H*&"+-A13)A*7$0-I0#J0#0*’11#*%")23’/-,’)4?A!558Section 3. The example demonstrates several features of the SDX
controller, including (1) the ability for a participant to control trafﬁc
ﬂows based on portions of ﬂow space other than destination IP preﬁx
(e.g., port number); and (2) the SDX controller’s ability to guarantee
correct forwarding that is in sync with the advertised BGP routes.
Transit Portal deployments at the University of Wisconsin and
at Clemson University both receive a route to the Amazon preﬁx
hosting our Amazon Web Services (AWS) instance. They distribute
their routes to AS A and AS B, respectively. These ASes in turn
send announcements to the SDX controller, which then selects a
best route for the preﬁx, which it re-advertises to AS C. AS C’s
outbound trafﬁc then ﬂows through either AS A or AS B, depending
on the policies installed at the SDX controller.
AS C, the ISP hosting the client, installs a policy at the SDX
that directs all trafﬁc to the Amazon /16 IP preﬁx via AS A, except
for port 80 trafﬁc, which travels via AS B. To demonstrate that the
SDX controller ensures that the switch data plane stays in sync with
the BGP control plane messages, we induce a withdrawal of the
route announcement at AS B (emulating, for example, a failure).
At this point, all trafﬁc from the SDX to AWS travels via AS A.
Figure ?? shows the trafﬁc patterns resulting from this experiment
and the resulting trafﬁc patterns as a result of (1) installation of the
application-speciﬁc peering policy; (2) the subsequent BGP route
withdrawal.
Wide-area load balancer. The wide-area load balancer application
also demonstrates the ability for a remote network to install a policy
at the SDX, even if it is not physically present at the exchange. Fig-
ure 4b shows an SDX setup where an AWS tenant hosts destinations
in two distinct AWS instances and wishes to balance load across
those two destinations. The AWS tenant remotely installs a policy
that rewrites the destination IP address for trafﬁc depending on the
source IP address of the sender. Initially, trafﬁc from the clients
of AS A directed towards the AWS tenant’s instances traverses the
SDX fabric unchanged and routed out to the Internet via AS B. After
the AWS tenant installs the load-balance policy at the SDX, trafﬁc
that was initially destined only for AWS instance #1 is now balanced
across both of the AWS instances. Figure ?? shows the trafﬁc rates
from the resulting experiment and how they evolve when the load
balance policy is installed at the SDX. Although this deployment
has only one SDX location, in practice the AWS tenant could ad-
vertise the same IP preﬁx via multiple SDX locations as an anycast
announcement, thus achieving more control over wide-area load
balance from a distributed set of locations.
6 Performance Evaluation
We now demonstrate that, under realistic scenarios, the SDX plat-
form scales—in terms of forwarding-table size and compilation
time—to hundreds of participants and policies.
6.1 Experimental Setup
To evaluate the SDX runtime, we provide realistic inputs to our com-
piler. We instantiate the SDX runtime with no underlying physical
switches because we are not concerned with evaluating forwarding
performance. We then install policies for hypothetical SDX partic-
ipants, varying both their numbers and their policies. We derive
policies and topologies from the characteristics of three large IXPs:
AMS-IX, LINX, and DEC-IX. We repeat each experiment ten times.
Emulating real-world IXP topologies. Based on the characteris-
tics of existing IXPs, we deﬁne a few static parameters, including
the fraction of participants with multiple ports at the exchange, and
the number of preﬁxes that each participant advertises. For example,
(a) Application-Speciﬁc Peering.
(b) Wide-Area Load Balance.
Figure 4: Setup for deployment experiments.
(a) Application-speciﬁc peering.
(b) Wide-area load balance.
Figure 5: Trafﬁc patterns for the two “live” SDX applications. (a) At 565
seconds, the AS C installs an application-speciﬁc peering policy, causing
port 80 trafﬁc to arrive via AS B. At 1253 seconds, AS B withdraws its route
to AWS, causing all trafﬁc to shift back to the path via AS A. (b) At 246
seconds, the AWS network installs a wide-area load balance policy to shift
the trafﬁc for source 204.57.0.67 to arrive at AWS instance #2.
source and destination IP addresses and ports as required for the
demonstrations below.
Application-speciﬁc peering. Figure 4a shows an SDX setup
where we test the application-speciﬁc peering use-case described in
!"#!"#$%&’()*’&*#+,!"#!-).&(/01234564247353489:4242;871234564242;87?>*@’!%"#A*’@!"#B$.>?’)*#+,53489:41584890-).&(/#C#?.’"LM#N?@:8:8A>=789:;=4))(B)’CDC)E&EB))C.F)&E4)’C#G>F)&E4)’C#G7;9:>?@:7;>:>?;9:>?@:>@H:?9!$"789:;?@:>@H:?9L#MM#OPNK,L020040060080010001200140016001800Time (seconds)0.00.51.01.52.02.53.03.54.0Traffic Rate (Mbps)application-specific peering policyapplication-specific peering policyroute withdrawalroute withdrawalAS-AAS-B0100200300400500600Time (seconds)0.00.51.01.52.02.53.03.54.0Traffic Rate (Mbps)load-balance policyload-balance policyAWS Instance #1AWS Instance #2559at AMS-IX, approximately 1% of the participating ASes announce
more than 50% of the total preﬁxes, and 90% of the ASes combined
announce less than 1% of the preﬁxes. We vary the number of
participants and preﬁxes at the exchange.
Emulating realistic AS policies at the IXP. We construct an ex-
change point with a realistic set of participants and policies, where
each participant has a mix of inbound and outbound policies. In-
bound policies include inbound trafﬁc engineering, WAN load bal-
ancing, and redirection through middleboxes. Outbound policies
include application-speciﬁc peering, as well as policies that are
intended to balance transit costs. Different types of participants
may use different types of policies. To approximate this policy
assignment, we classify ASes as eyeball, transit, or content, and
we sort the ASes in each category by the number of preﬁxes that
they advertise. Since we do not have trafﬁc characteristics, we use
advertised preﬁxes as a rough proxy. Only a subset of participants
exchange most of the trafﬁc at the IXPs, and we assume that most
policies involve the participants who carry signiﬁcant amounts of
trafﬁc. We assume that the top 15% of eyeball ASes, the top 5% of
transit ASes, and a random set of 5% of content ASes install custom
policies:
Content providers. We assume that content providers tune out-
bound trafﬁc policies for the top eyeball networks, which serve as
destinations for the majority of trafﬁc ﬂows. Thus, for each content
provider, we install outbound policies for three randomly chosen
top eyeball networks. Occasionally, content providers may wish to
redirect incoming requests (e.g., for load balance), so each content
provider installs one inbound policy matching on one header ﬁeld.
Eyeballs. We assume that eyeball networks generally tune in-
bound trafﬁc, and, as a result, most of their policies involve con-
trolling inbound trafﬁc coming from the large content providers.
The eyeball networks install inbound policies and match on one
randomly selected header ﬁeld; they do not install any outbound
policies. For each eyeball network, we install inbound policies for
half of the content providers.
Transit providers. Finally, we assume that transit networks have a
mix of inbound and outbound trafﬁc-engineering policies to balance
load by tuning the entry point.
In our experiment, each transit