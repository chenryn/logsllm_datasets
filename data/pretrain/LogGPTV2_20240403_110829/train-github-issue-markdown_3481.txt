In summary I have two rabbitmq instances running on the same host different
ports (e.g. R1:5672,R2:6672). They are set up as a cluster and configured to
mirror all queues. I run two celery workers on the same host, each configured
to connect to one of these rabbitmq instances.  
So environment looks like:  
C1->R1  
C2->R2  
and R1R2
I then publish a bunch of ETA tasks.  
I verify tasks are equally distributed to each celery worker as ETA tasks.  
Next I stop R2.
At this point I observer many tasks are Received by C1 as duplicates.  
Log of C1 look like this:
[2015-12-23 23:37:21,993: INFO/MainProcess] Received task:
tasks.add[5c7e4b80-be18-4d8d-965e-534d75dfffbc] eta:[2015-12-24
04:42:21.966493+00:00]  
[2015-12-23 23:37:21,995: INFO/MainProcess] Received task:
tasks.add[5c7e4b80-be18-4d8d-965e-534d75dfffbc] eta:[2015-12-24
04:42:21.966325+00:00]
they are eventually executed by this celery worker twice..  
[2015-12-23 23:42:22,633: INFO/MainProcess] Task
tasks.add[5c7e4b80-be18-4d8d-965e-534d75dfffbc] succeeded in
0.00121066000429s: 9  
[2015-12-23 23:42:22,633: INFO/MainProcess] Task
tasks.add[5c7e4b80-be18-4d8d-965e-534d75dfffbc] succeeded in
0.00175269600004s: 9
This is my task for reference:  
from celery import Celery  
import os  
app = Celery('tasks',
broker='amqp://myuser:mypassword@localhost:{0}/'.format(os.environ['AMQP_PORT']))  
app.conf.update(  
CELERY_QUEUE_HA_POLICY='all',  
CELERY_TRACK_STARTED=True,  
CELERY_ACKS_LATE=True,  
CELERY_TASK_SERIALIZER='json',  
CELERY_EVENT_QUEUE_TTL=20,  
CELERY_SEND_TASK_SENT_EVENT=True,  
CELERY_EVENT_QUEUE_EXPIRES=900  
)
@app.task  
def add(x, y):  
return x + y
I start up the celery instances as:  
AMQP_PORT=5672 celery -A tasks worker --loglevel=info -n celery_5672  
AMQP_PORT=6672 celery -A tasks worker --loglevel=info -n celery_6672
Rabbit policy is:  
/ ha-all all ^(?!amq\\.).* {"ha-mode":"all","ha-sync-mode":"automatic"} 0
I create tasks as:  
workflow = (add.s(4,4).set(countdown=300) | add.s(1).set(countdown=300))  
print workflow()
Wouldn't it make sense for Celery to discard duplicates when there is an ETA
task scheduled already instead of scheduling a new one? My tasks are mostly(!)
idempotent (we use ACKS_LATE), but still it seems that it should be easy for
Celery to prevent executing these tasks multiple times.  
Thanks..  
GÃ¶khan