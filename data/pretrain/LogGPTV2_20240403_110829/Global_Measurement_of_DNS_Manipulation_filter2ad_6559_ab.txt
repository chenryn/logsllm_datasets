our discussion.
One important ethical principle is respect for persons;
essentially, this principle states that an experiment should
respect the rights of humans as autonomous decision-
makers. Sometimes this principle is misconstrued as a
requirement for informed consent for all experiments. In
many cases, however, informed consent is neither prac-
tical nor necessary; accordingly, Salganik [44] charac-
terizes this principle instead as “some consent for most
things”. In the case of Iris, obtaining the consent of all
open DNS resolver operators is impractical.
In lieu of attempting to obtain informed consent, we
turn to the principle of beneﬁcence, which weighs the
beneﬁts of conducting an experiment against the risks
associated with the experiment. Note that the goal of
beneﬁcence is not to eliminate risk, but merely to re-
duce it to the extent possible. Iris’s design relies heavily
on this principle: Speciﬁcally, we note that the beneﬁt
of issuing DNS queries through tens of millions of re-
solvers has rapidly diminishing returns, and that using
only open resolvers that we can determine are unlikely
to correspond to individual users greatly reduces the risk
to any individual without dramatically reducing the ben-
eﬁts of our experiment. We note that our consideration of
ethics in this regard is a signiﬁcant departure from pre-
vious work that has issued queries through open DNS
resolver infrastructure but has not considered ethics.
The principle of justice states that the beneﬁciaries of
an experiment should be the same population that bears
the risk of that experiment. On this front, we envi-
sion that the beneﬁciaries of the kinds of measurements
that we collect using Iris will be wide-ranging: design-
ers of circumvention tools, as well as policymakers, re-
searchers, and activists who are improving communica-
tions and connectivity for citizens in oppressive regimes
all need better data about the extent and scope of Internet
censorship. In short, even in the event that some entity
in a country that hosts an open DNS resolver might bear
some risk as a result of the measurements we conduct, we
envision that those same entities may ultimately beneﬁt
from the research, policy-making, and tool development
310    26th USENIX Security Symposium
USENIX Association
that Iris facilitates.
A ﬁnal guideline concerns respect for law and public
interest, which essentially extends the principle of benef-
icence to all relevant stakeholders, not only the experi-
ment participants. This principle is useful for reasoning
about the externalities that our DNS queries create by in-
creasing DNS query load on the nameservers for various
DNS domains. To abide by this principle, we rate-limit
our DNS queries for each DNS domain to ensure that the
owners of these domains do not face large expenses as
a result of the queries that we issue. This rate limit is
necessary because some DNS service providers charge
based on the peak or near peak query rate.
3.3 Open DNS Resolvers
To obtain a wide range of measurement vantage points,
we use open DNS resolvers deployed around the world;
such resolvers will resolve queries for any client.
Measurement using open DNS resolvers is an ethically
complex issue. Previous work has identiﬁed tens of mil-
lions of these resolvers around the world [34]. Given
their prevalence and global diversity, open resolvers are a
compelling resource, providing researchers with consid-
erable volume and reach. Unfortunately, open resolvers
also pose a risk not only to the Internet but to individual
users.
Open resolvers can be the result of conﬁguration
errors, frequently on end-user devices such as home
routers [34]. Using these devices for measurement can
incur monetary cost, and if the measurement involves
sensitive content or hosts, can expose the owner to harm.
Furthermore, open resolvers are also a common tool
in various online attacks such as Distributed Denial-of-
Service (DDoS) ampliﬁcation attacks [35]. Despite ef-
forts to reduce both the prevalence of open resolvers and
their potential impact [40], they remain commonplace.
Due to these and the ethics considerations that we dis-
cussed in §3.2, we restrict the set of open resolvers that
we use to the few thousand resolvers that we are reason-
ably certain are part of the Internet infrastructure (e.g.,
belonging to Internet service providers, online cloud
hosting providers), as opposed to attributable to any sin-
gle individual. Figure 1 illustrates the process by which
Iris ﬁnds safe open DNS resolvers. We now explain this
process in more detail. Conceptually, the process com-
prises two steps: (1) scanning the Internet for open DNS
resolvers; or (2) pruning the list of open DNS resolvers
that we identify to limit the resolvers to a set that we can
reasonably attribute to Internet infrastructure.
By using DNS resolvers we do not control, we cannot
differentiate between country-wide or state-mandated
censorship and localized manipulation (e.g., captive por-
tals, malware [34]) at individual resolvers. Therefore
Figure 1: Overview of Iris’s DNS resolver identiﬁcation and
selection pipeline. Iris begins with a global scan of the entire
IPv4 address space, followed by reverse DNS PTR lookups for
all open resolvers, and ﬁnally ﬁltering resolvers to only include
DNS infrastructure.
we must aggregate and analyze results at ISP or coun-
try scale.
Step 1: Scanning the Internet’s IPv4 space for open
DNS resolvers. Scanning the IPv4 address space pro-
vides us with a global perspective on all open resolvers.
To do so, we developed an extension to the ZMap [22]
network scanner to enable Internet-wide DNS resolu-
tions1. This module queries port 53 of all IPv4 addresses
with a recursive DNS A record query. We use a purpose-
registered domain name we control for these queries to
ensure there is a known correct answer. We conduct
measurements and scans from IP addresses having a PTR
record identifying the machine as a “research scanner.”
These IP addresses also host a webpage identifying our
academic institution and offering the ability to opt-out of
scans. From these scans, we select all IP addresses that
return the correct answer to this query and classify them
as open resolvers. In §4.1, we explore the population of
open DNS resolvers that we use for our study.
Step 2: Identifying Infrastructure DNS Resolvers.
Given a list of all open DNS resolvers on the Internet,
we prune this list to include only DNS resolvers that
can likely be attributed to Internet infrastructure. To
do so, we aim to identify open DNS resolvers that ap-
pear to be authoritative nameservers for a given DNS
domain.
Iris performs reverse DNS PTR lookups for
all open resolvers and retains only the resolvers that
have a valid PTR record beginning with the subdomain
ns[0-9]+ or nameserver[0-9]*. This ﬁltering step
reduces the number of usable open resolvers—from mil-
lions to thousands—yet even the remaining set of open
DNS resolvers provides broad country- and network-
level coverage (characterized further in §4.1).
Using PTR records to identify infrastructure can have
1Our extension has been accepted into the open source project and
the results of our scans are available as part of the Censys [21] system.
USENIX Association
26th USENIX Security Symposium    311
both false negatives and false positives. Not all infras-
tructure resolvers will have a valid PTR record, nor will
they all be authoritative nameservers. These false nega-
tives limit the scope and scale of our measurement, but
are necessary to reduce risk. Similarly, if a user oper-
ated their own authoritative nameserver on their home IP
or if a PTR record matched our naming criteria but was
not authoritative, our method would identify that IP as
infrastructure (false positives).
3.4 Performing the Measurements
Given a list of DNS domain names to query and a global
set of open DNS resolvers from which we can issue
queries, we need a mechanism that issues queries for
these domains to the set of resolvers that we have at our
disposal. Figure 2 shows an overview of the measure-
ment process. At a high level, Iris resolves each DNS
domain using the global vantage points afforded by the
open DNS resolvers, annotates the response IP addresses
with information from both outside datasets as well as
additional active probing, and uses consistency and inde-
pendent veriﬁability metrics to identify manipulated re-
sponses. The rest of this section outlines this measure-
ment process in detail, while §3.5 describes how we use
the results of these measurements to ultimately identify
manipulation.
Step 1: Performing global DNS queries. Iris takes as
input a list of suitable open DNS resolvers, as well as
the combined CLBL and Alexa domain names. In addi-
tion to the DNS domains that we are interested in testing,
we include 3 DNS domains that are under our control to
help us compute our consistency metrics when identify-
ing manipulation.
Querying tens of thousands of domains across tens of
thousands of resolvers required the development of a new
DNS query tool, because no existing DNS measurement
tool supports this scale. We implemented this tool in
Go [27]. The tool takes as input a set of domains and
resolvers, and coordinates random querying of each do-
main across each resolver. The tool supports a variety of
query types, multiple of which can be speciﬁed per run,
including A, AAAA, MX, and ANY. For each (domain, re-
solver) pair, the tool crafts a recursive DNS request and
sends it to the resolver. The recursive query requests that
the resolver resolve the domain and return the ultimate
answer, logging all responses, including timeouts. The
tool follows the set of responses to resolve each domain
to an IP address. For example, if a resolver returns a
CNAME, the tool then queries the resolver for resolution
of that CNAME.
To ensure resolvers are not overloaded, the tool in-
cludes a conﬁgurable rate-limit. For our experiments,
we limited queries to resolvers to an upper bound of 5
per second. In practice, this rate tends to be much lower
due to network latency in both reaching the resolver, as
well as the time it takes the resolver to perform the re-
cursive response. To cope with speciﬁc resolvers that are
unstable or timeout frequently, the tool provides a con-
ﬁgurable failure threshold that halts a speciﬁc resolver’s
set of measurements should too many queries fail.
To ensure the domains we query are not overloaded,
the tool randomizes the order of domains and limits the
number of resolvers queried in parallel such that in the
worst case no domain experiences more than 1 query per
second, in expectation.
Step 2: Annotating DNS responses with auxiliary in-
formation. Our analysis ultimately relies on character-
izing both the consistency and independent veriﬁability
of the DNS responses that we receive. To enable this
classiﬁcation we ﬁrst must gather additional details about
the IP addresses that are returned in each of the DNS re-
sponses. Iris annotates each IP address returned in the
set of DNS responses with additional information about
each IP address’s geolocation, autonomous system (AS),
port 80 HTTP responses, and port 443 HTTPS X.509 cer-
tiﬁcates. We rely on the Censys [21] dataset for this aux-
iliary information; Censys provides daily snapshots of
this information. This dataset does not contain every IP
address; for example, the dataset does not include IP ad-
dresses that have no open ports, or adversaries may in-
tentionally return IP addresses that return error pages or
are otherwise unresponsive. In these cases, we annotate
all IP addresses in our dataset with AS and geolocation
information from the Maxmind service [37].
Additional PTR and TLS scanning. For each IP ad-
dress, we perform a DNS PTR lookup to assist with some
of our subsequent consistency characterization (a process
we detail in §3.5). Another complication in the annota-
tion exercise relates to the fact that in practice a single
IP address might host many websites via HTTP or HTTPS
(i.e., virtual hosting). As a result, when Censys retrieves
certiﬁcates via port 443 (HTTPS) across the entire IPv4
address space, the certiﬁcate that Censys retrieves might
differ from the certiﬁcate that the server would return in
response to a query via TLS’s Server Name Indication
(SNI) extension. Such a discrepancy might lead Iris to
mischaracterize virtual hosting as DNS inconsistency. To
mitigate this effect, for each resulting IP address we per-
form an additional active HTTPS connection using SNI,
specifying the name originally queried. We annotate all
responses with this information, which we use for answer
classiﬁcation (examined further in §5.1).
3.5
To determine whether a DNS response is manipulated,
Iris relies on two types of metrics: consistency metrics
Identifying DNS Manipulation
312    26th USENIX Security Symposium
USENIX Association
Figure 2: Overview of DNS resolution, annotation, ﬁltering, and classiﬁcation. Iris inputs a set of domains and DNS resolvers and
outputs results indicating manipulated DNS responses.
and independent veriﬁability metrics. We say that a re-
sponse is correct if it satisﬁes any consistency or inde-
pendent veriﬁable metric; otherwise, we classify the re-
sponse as manipulated. In this section, we outline each
class of metrics as well as the speciﬁc features we de-
velop to classify answers. The rest of this section deﬁnes
these metrics; §5.1 explores the efﬁcacy of each of them.
3.5.1 Consistency
Access to a domain should have some form of consis-
tency, even when accessed from various global vantage
points. This consistency may take the form of network
properties, infrastructure attributes, or even content. We
leverage these attributes, both in relation to control data
as well as across the dataset itself, to classify DNS re-
sponses.
Consistency Baseline: Control Domains and Re-
solvers. Central to our notion of consistency is having
a set of geographically diverse resolvers we control that
are (presumably) not subject to manipulation. These con-
trols give us a set of high-conﬁdence correct answers we
can use to identify consistency across a range of IP ad-
dress properties. Geographic diversity helps ensure that
area-speciﬁc deployments do not cause false-positives.
For example, several domains in our dataset use differ-
ent content distribution network (CDN) hosting infras-
tructure outside North America. As part of our measure-
ments we insert domain names we control, with known
correct answers. We use these domains to ensure a re-
solver reliably returns unmanipulated results for non-
sensitive content (e.g., not a captive portal).
For each domain name, we create a set of con-
sistency metrics by taking the union of each metric
across all of our control resolvers.
For example,
the answer 192.168.0.10
if Control A returns
and
and
returns
192.168.0.11
192.168.0.12, we create a set of consistent IP set of
Control
B
for each metric,
(192.168.0.10, 192.168.0.11, 192.168.0.12).
We say the answer
is “correct” (i.e., not manip-
ulated) if,
the answer is a non-
empty subset of the controls. Returning to our IP
example,
returns the answer
(192.168.0.10, 192.168.0.12),
it is identiﬁed as
correct. When a request returns multiple records, we
check all records and consider the reply good if any
response passes the appropriate tests.
if a global
resolver
Additionally, unmanipulated passive DNS [6] data
collected simultaneously with our experiments across a
geographically diverse set of countries could enhance (or
replace) our consistency metrics. Unfortunately we are
not aware of such a dataset being available publicly.
IP Address. The simplest consistency metric is the IP
address or IP addresses that a DNS response contains.
Autonomous System / Organization. In the case of ge-
ographically distributed sites and services, such as those
hosted on CDNs, a single domain name may return dif-
ferent IP addresses as part of normal operation. To at-
tempt to account for these discrepancies, we also check
whether different IP addresses for a domain map to the
same AS we see when issuing queries for the domain
name through our control resolvers. Because a single AS
may have multiple AS numbers (ASNs), we consider two
IP addresses with either the same ASN or AS organiza-
tion name as being from the same AS. Although many
responses will exhibit AS consistency even if individual
IP addresses differ, even domains whose queries are not
manipulated will sometimes return inconsistent AS-level
and organizational information as well. This inconsis-
tency is especially common for large service providers
whose infrastructure spans multiple regions and conti-
nents and is often the result of acquisitions. To account
for these inconsistencies, we need additional consistency
metrics at higher layers of the protocol stack (speciﬁcally
HTTP and HTTPS), described next.
USENIX Association
26th USENIX Security Symposium    313
HTTP Content. If an IP address is running a webserver
on port 80, we include a hash of the content returned as
an additional consistency metric. These content hashes
come from a port 80 IP address Censys crawl. This
metric effectively identiﬁes sites with limited dynamic
content. As discussed in §5.1, this metric is also use-
ful in identifying sites with dynamic content but shared
infrastructure. For example, as these hashes are based
on HTTP GET fetches using an IP address as the Host
in the header, this fetch uniquely ﬁngerprints and cate-
gorizes CDN failures or default host pages. In another
example, much of Google’s web hosting infrastructure
will return the byte-wise identical redirection page to
http://www.google.com/ for HTTP GETs without a
valid Google host header. These identical pages allow
us to identify Google resolutions as correct even for IP
addresses acting as a Point-of-Presence.
HTTPS Certiﬁcate. We label a response as correct if
the hash of the HTTPS certiﬁcate presented upon connec-
tion matches that of an IP returned via our controls. Note
this is not an independent veriﬁability metric, as the cer-
tiﬁcates may or may not be trusted, and may not even be
correct for the domain.
PTRs for CDNs. From our control data, we classify do-
mains as hosted on particular CDNs based on PTR, AS,
and certiﬁcate information. We consider a non-control
response as consistent if the PTR record for that response
points to the same CDN.
3.5.2 Independent Veriﬁability
In addition to consistency metrics, we also deﬁne a set of
metrics that we can independently verify using external
data sources, such as the HTTPS certiﬁcate infrastruc-
ture. We describe these methods below.
HTTPS Certiﬁcate. We consider a DNS response to
be correct, independent of controls, if the IP address
presents a valid, browser-trusted certiﬁcate for the cor-
rect domain name when queried without SNI. We further
extend this metric to allow for common conﬁguration er-
rors, such as returning certiﬁcates for *.example.com
when requesting example.com.
HTTPS Certiﬁcate with SNI. We add an additional
metric that checks whether
the certiﬁcate returned
from our follow-up SNI-enabled scans returns a valid,
browser-trusted certiﬁcate for the correct IP address.
3.6 Limitations
To facilitate global coverage in our measurements, our
method has limitations that impact our scope and limit
our results.