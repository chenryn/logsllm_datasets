title:Modeling the Effect of Technology Trends on the Soft Error Rate of
Combinational Logic
author:Premkishore Shivakumar and
Michael Kistler and
Stephen W. Keckler and
Doug Burger and
Lorenzo Alvisi
Modeling the Effect of Technology Trends on the
Soft Error Rate of Combinational Logic
Premkishore Shivakumar
Michael Kistlery
Stephen W. Keckler
Doug Burger
Lorenzo Alvisi
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
http://www.cs.utexas.edu/users/cart
yIBM Austin Research Laboratory
Austin, TX 78660
http://www.research.ibm.com/arl
Abstract
This paper examines the effect of technology scaling
and microarchitectural trends on the rate of soft errors in
CMOS memory and logic circuits. We describe and validate
an end-to-end model that enables us to compute the soft
error rates (SER) for existing and future microprocessor-
style designs. The model captures the effects of two impor-
tant masking phenomena, electrical masking and latching-
window masking, which inhibit soft errors in combinational
logic. We quantify the SER due to high-energy neutrons in
SRAM cells, latches, and logic circuits for feature sizes from
600nm to 50nm and clock periods from 16 to 6 fan-out-of-4
inverter delays. Our model predicts that the SER per chip
of logic circuits will increase nine orders of magnitude from
1992 to 2011 and at that point will be comparable to the
SER per chip of unprotected memory elements. Our result
emphasizes that computer system designers must address
the risks of soft errors in logic circuits for future designs.
1 Introduction
Two important trends driving microprocessor perfor-
mance are scaling of device feature sizes and increasing
pipeline depths. In this paper we explore how these trends
affect the susceptibility of microprocessors to soft errors.
Device scaling is the reduction in feature size and voltage
levels of the transistors, which improves performance be-
cause smaller devices require less current to turn on or off,
and thus can be operated at higher frequencies. Pipelin-
ing is a microarchitectural technique of dividing instruction
processing into stages which can operate concurrently on
different instructions. Pipelining improves performance by
increasing instruction level parallelism (ILP). Five to eight
stage pipelines are quite common, and some recent designs
use twenty or more stages [11]. Such designs are commonly
referred to as superpipelined designs.
Our study focuses on soft errors, which are also called
transient faults or single-event upsets (SEUs). These are er-
rors in processor execution that are due to electrical noise
or external radiation rather than design or manufacturing
defects. In particular, we study soft errors caused by high-
energy neutrons resulting from cosmic rays colliding with
particles in the atmosphere. The existence of cosmic ray ra-
diation has been known for over 50 years, and the capacity
for this radiation to create transient faults in semiconductor
circuits has been studied since the early 1980s. As a result,
most modern microprocessors already incorporate mech-
anisms for detecting soft errors. These mechanisms are
typically focused on protecting memory elements, particu-
larly caches, using error-correcting codes (ECC), parity, and
other techniques. Two key reasons for this focus on mem-
ory elements are: 1) the techniques for protecting memory
elements are well understood and relatively inexpensive in
terms of the extra circuitry required, and 2) caches take up
a large part, and in some cases a majority, of the chip area
in modern microprocessors.
Past research has shown that combinational logic is
much less susceptible to soft errors than memory ele-
ments [8, 19]. Three phenomena provide combinational
logic a form of natural resistance to soft errors: 1) logi-
cal masking, 2) electrical masking, and 3) latching-window
masking. We develop models for electrical masking and
latching-window masking to determine how these are af-
fected by device scaling and superpipelining. Then based
on a composite model we estimate the effects of these tech-
nology trends on the soft error rate (SER) of combinational
logic. Finally using an overall chip area model we com-
pare the SER/chip of combinational logic with the expected
trends in SER of memory elements.
The primary contribution of our work is an analysis of
the trends in SER for SRAM cells, latches, and combina-
tional logic. Our models predict that by 2011 the soft er-
ror rate in combinational logic will be comparable to that
of unprotected memory elements. This result is signiﬁ-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:09:12 UTC from IEEE Xplore.  Restrictions apply. 
cant because current methods for protecting combinational
logic have signiﬁcant costs in terms of chip area, perfor-
mance, and/or power consumption in comparison to protec-
tion mechanisms for memory elements.
The rest of this paper is organized as follows. Section 2
provides background on the nature of soft errors, and a
method for estimating the soft error rate of memory cir-
cuits. Section 3 introduces our deﬁnition of soft errors in
combinational logic, and examines the phenomena that can
mask soft errors in combinational logic. Section 4 describes
in detail our methodology for estimating the soft error rate
in combinational logic. We present our results in Section 5.
Section 6 discusses the implications of our analysis and sim-
ulations. Section 7 summarizes the related work, and Sec-
tion 8 concludes the paper.
2 Background
2.1 Particles that cause soft errors
In the early 1980s, IBM conducted a series of experi-
ments to measure the particle ﬂux from cosmic rays [32],
the rate of ﬂow expressed as the number of particles of a
particular energy per square centimeter per second. For our
work, the most important aspect of these results is that parti-
cles of lower energy occur far more frequently than particles
of higher energy. In particular, a one order of magnitude
difference in energy can correspond to a two orders of mag-
nitude larger ﬂux for the lower energy particles. As CMOS
device sizes decrease, they are more easily affected by these
lower energy particles, potentially leading to a much higher
rate of soft errors.
This paper investigates the soft error rate of combina-
tional logic caused by atmospheric neutrons with energies
greater than 1 mega-electron-volt (MeV). This form of radi-
ation, the result of cosmic rays colliding with particles in the
atmosphere, is known to be a signiﬁcant source of soft er-
rors in memory elements. We do not consider atmospheric
neutrons with energy less than 1 MeV since we believe their
much lower energies are less likely to result in soft errors in
combinational logic. We also do not consider alpha par-
ticles, since this form of radiation comes almost entirely
from impurities in packaging material, and thus can vary
widely for processors within a particular technology gener-
ation. The contribution to the overall soft error rate from
each of these radiation sources is additive, and thus each
component can be studied independently.
2.2 Soft errors in memory circuits
High-energy neutrons that strike a sensitive region in a
semiconductor device deposit a dense track of electron-hole
pairs as they pass through a p-n junction. Some of the de-
posited charge will recombine to form a very short duration
pulse of current at the internal circuit node that was struck
by the particle. When a particle strikes a sensitive region
of an SRAM cell, the charge that accumulates could exceed
the minimum charge that is needed to ﬂip the value stored
in the cell, resulting in a soft error. The smallest charge that
results in a soft error is called the critical charge (	CR T )
of the SRAM cell [7]. The rate at which soft errors occur
is typically expressed in terms of Failures In Time (FIT),
which measures the number of failures per 109 hours of op-
eration. A number of studies on soft errors in SRAMs have
concluded that the SER for constant area SRAM arrays will
increase as device sizes decrease [15, 24, 25], though re-
searchers differ on the rate of this increase.
A method for estimating SER in CMOS SRAM circuits
was recently developed by Hazucha & Svensson [10]. This
model estimates SER due to atmospheric neutrons (neu-
trons with energies > 1MeV) for a range of submicron fea-
ture sizes. It is based on a veriﬁed empirical model for the
600nm technology, which is then scaled to other technology
generations. The basic form of this model is:
SER _ F  A  exp(cid:18) 
	CR T
	S (cid:19)
(1)
where
F
A
	CR T
	S
is the neutron ﬂux with energy > 1
MeV, in particles/(cm2*s),
is the area of the circuit sensitive to
particle strikes, in cm2,
is the critical charge, in fC, and
is the charge collection efﬁciency of
the device, in fC
Two key parameters in this model are the critical charge
(	CR T ) of the SRAM cell and the charge collection efﬁ-
ciency (	S) of the circuit. 	CR T depends on character-
istics of the circuit, particularly the supply voltage and the
effective capacitance of the drain nodes. 	S is a measure
of the magnitude of charge generated by a particle strike.
These two parameters are essentially independent, but both
decrease with decreasing feature size. From Equation 1 we
see that changes in the value of 	CR T relative to 	S will
have a very large impact on the resulting SER. The SER is
also proportional to the area of the sensitive region of the
device, and therefore it decreases proportional to the square
of the device size. Hazucha & Svensson used this model to
evaluate the effect of device scaling on the SER of memory
circuits. They concluded that SER-per-chip of SRAM cir-
cuits should increase at most linearly with decreasing fea-
ture size.
3 Soft Errors in Combinational Logic
A particle that strikes a p-n junction within a combina-
tional logic circuit can alter the value produced by the cir-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:09:12 UTC from IEEE Xplore.  Restrictions apply. 
IN
CLK
D
Q
Q’
D Q
  OUT
CLK
Q’
4 Methodology
Figure 1. Simple model of a pipeline stage
cuit. However, a transient change in the value of a logic
circuit will not affect the results of a computation unless it
is captured in a memory circuit. Therefore, we deﬁne a soft
error in combinational logic as a transient error in the result
of a logic circuit that is subsequently stored in a memory
circuit of the processor.
A transient error in a logic circuit might not be captured
in a memory circuit because it could be masked by one of
the following three phenomena:
Logical masking occurs when a particle strikes a portion
of the combinational logic that is blocked from affecting
the output due to a subsequent gate whose result is com-
pletely determined by its other input values.
Electrical masking occurs when the pulse resulting
from a particle strike is attenuated by subsequent logic
gates due to the electrical properties of the gates to the
point that it does not affect the result of the circuit.
Latching-window masking occurs when the pulse re-
sulting from a particle strike reaches a latch, but not at the
clock transition where the latch captures its input value.
These masking effects have been found to result in a sig-
niﬁcantly lower rate of soft errors in combinational logic
compared to storage circuits in equivalent device technol-
ogy [19]. However, these effects could diminish signiﬁ-
cantly as feature sizes decrease and the number of stages in
the processor pipeline increases. Electrical masking could
be reduced by device scaling because smaller transistors are
faster and therefore may have less attenuation effect on a
pulse. Also, deeper processor pipelines allow higher clock
rates, meaning the latches in the processor will cycle more
frequently, which may reduce latching-window masking.
We evaluate the effects of electrical and latching-window
masking using the simple model for a processor pipeline
stage illustrated in Figure 1. This model is just a one-wide
chain of homogeneous gates terminating in a level-sensitive
latch. For the results presented in this paper we use static
NAND gates with a fan-out of 4.
The number of gates
in the chain is determined by the degree of pipelining in
the microarchitecture, which we characterize by the num-
ber of fan-out-of-4 inverter (FO4) gates that can be placed
between two latches in a single pipeline stage. The FO4
metric is technology independent and 1 FO4 roughly cor-
responds to 360 pico-seconds times the transistor’s drawn
gate length in microns [12].
In our model we use level-
sensitive latches because their advantages in area and tol-
erance to clock load/skew make them attractive for super-
pipelined designs.
In most modern microprocessors, combinational logic
and memory elements are constructed from the same ba-
sic devices – NMOS and PMOS transistors. Therefore, we
can use techniques for estimating the SER in memory ele-
ments to assess soft errors in combinational logic. We will
also use these techniques directly to compute the SER in
memory elements for a range of device sizes, and compare
the results to our estimates of SER for combinational logic.
Our methodology for estimating the soft error rate in
combinational logic considers the effects of CMOS device
scaling and the microarchitectural trend toward increasing
depth of processor pipelines. We determine the soft error
rate using analytical models for each stage of the pulse from
its creation to the time it reaches the latch. Figure 2 shows
the various stages the pulse passes through and the corre-
sponding model used to determine the effect on the pulse
at that stage. In the ﬁrst stage the charge generated by the
particle strike produces a current pulse, which is then con-
verted into a voltage pulse after traveling through a gate in
the logic chain. The electrical masking model simulates the
degradation of the pulse as it travels through the gates of the
logic circuit. Finally a model for the latching window deter-
mines the probability that the pulse is successfully latched.
The remainder of this section describes each of these com-
ponent models and how they are combined to obtain an esti-
mate for the SER of combinational logic. Additional details
on our methodology can be found in an extended version of
this paper [30].
4.1 Device scaling model
We constructed a set of Spice Level 3 technology mod-
els corresponding to the technology generations from the
Semiconductor Industry Association (SIA) 1999 technol-
ogy roadmap [29]. Values for drawn gate length, supply
voltage, and oxide thickness are taken directly from the
roadmap. The remaining parameters were obtained using a
scaling methodology developed by McFarland [21]. We ad-
justed McFarland’s formula for threshold voltage slightly to
scale better to technologies with very low supply voltages,
but all other parameters are based on McFarland’s model.
4.2 Charge to voltage pulse model
When a particle strikes a sensitive region of a circuit el-
ement it produces a current pulse with a rapid rise time,
but a more gradual fall time. The shape of the pulse can
be approximated by a one-parameter function [7] shown in
Equation 2.
 _
T
 
T
 exp(cid:18) 