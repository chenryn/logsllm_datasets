value.
• Opportunity: Through two accounts, the attacker can send
checks to himself. They can thus check if the system is vul-
nerable.
3.2 Attacker’s access to knowledge
As with any attack, attacks on Deep Learning vary on the knowl-
edge the attacker has on the system prior to the attack.
White-Box Attacks. In White-Box mode, the attacker has access
to every parameter of the Neural Network. In this scenario, the
attacker has a read-only access to the all parameters of the algorithm
by the means of an ill-protected file.
Gray-Box Attacks. In Gray-Box mode, the attacker has access
to some parameters of the Neural Network, while some remain
unkown to them. A common scenario is that the attacker knows
the structure of the Deep Neural Network, but not the parameters.
In this case, the attacker has managed to get a restricted account
on the server, showing only the code used to train, but not the
admin-owned weights parameter.
Black-Box Attacks. In Black-Box mode, the attacker has no access
to the Neural Network other than through its inferences, like any
normal user. In our scenario, this means that the attacker has no
access to the server other than the check submission point.
3.3 Attacker’s Objective
Regardless of the operational objectives, we give the two main tech-
nical objectives attackers can set when they attack deep learning
algorithms.
Untargeted Attack. When the attack is untargeted, the attacker’s
objective is to create any misclassification they can, with no control
over the yadv in equation 1. In our scenario, the attacker tries to
trigger a misclassification from the real digit class.
arXiv Preprint, Dec 2018,
F. Menet et al.
Targeted Attack. When the attack is targeted, the attacker’s ob-
jective is to trick the algorithm into classifying the input in a class
chosen by the attacker, with complete control over the yadv. In our
scenario, and for simplicity, the attacker tries to classify all digits as
a 9 digit. We will consider that it can only attain this goal through
a targeted attack.
We summarize the attack characteristics with respect to our
scenario below :
Attack
Attacker in the Scenario
White-Box Already has access to privileged information
Gray-Box
Already has access to restricted information
Black-Box
Targeted
Untargeted
We create an array from the last two sections, and get the fol-
Wants to modify the check’s value
Has no access to any information
Wants to get a 9999 $ check
lowing example attackers. U is for Untargeted, T for Targeted :
White-Box
Disruption
Gray-Box
Black-Box
Theft
Fraud
Rogue Employee DoS extortion
U
T Smash-and-Grab
We describe the scenarios:
Disruption. When the attacker has access to every information
in the neural network, we consider that they already have access
to a privileged account. For the sake of this example, we consider
the case of an attacker with root access that wants to minimize
its footprint. By allowing themselves to only read the parameters,
the attacker can create a copy of the neural network then use it to
disrupt business by forcing bank employees to manually review all
checks emitted by the attacker.
Rogue Employee. In this context, an IT employee with a restricted
account and thus some information on the neural network could
trigger a misclassification on the system, and disrupt business by
creating subtle modifications on all the checks submitted through
its interface.
Denial of Service (DoS) extortion. When the attacker has no access
to the system, any misclassification from the original class will allow
an attacker to get some form of disruption. If an attacker prints
check paper with adversarial patterns on it, they can extort money
from the bank by threatening to cause user mistrust and employee
time to manually review checks.
Smash-and-Grab. When the attacker is given White-Box access
through a visible root access, they can create a base of malicious
digit samples allowing them to transform any 4-digit check into a
9999$ check. From this, the attackers could submit some malicious
checks and take their money whenever possible.
Fraud. In this case, the attacker could be another rogue employee
trying to do more than disruption by targetting all the digits to be
9.
Theft. This is the worst scenario. If successful, the attacker can
create a target misclassification with only access to the computer,
completely controlling the value of the check.
These attacks could be seen as less risky variants of physically
tampering the check’s value. If caught, the attacker can blame
the system, by arguing that a human reviewer can perfectly see
the check’s actual value. As these attacks are almost riskless and
costless for attacker but disruptive for the defender, these AI attacks
constitute an interesting field of study from an information security
perspective.
These attacks are not limited to the banking system. As an ex-
ample, deep learning algorithms are currently used in autonomous
driving [6], malware analysis (with already an arms race between
attacks and defenses [20, 25, 44]), healthcare[15], and fake news
detection[1].
3.4 Main attacks implemented against Deep
Learning
For each attack, we will give a formal mathematical definition.
We will then write an equivalent attack to understand the inner
workings of the algorithm.
First attack discovered. A lot of attacks aiming at generating ad-
versarial samples have been developped in the last five years, since
the discovery by Szegedy et al. [41] of a L-BFGS-driven line-search
attack. It is the first method known to generate adversarial image
samples that are extremely close to their innocuous counterparts.
This attack tries to minimize the distance between the original
sample and a modified, misclassified sample, until it is close enough
to be indistinguishable from the original. It is a way for an attacker
to optimize, for different values of a perturbation norm constraint,
a loss-maximization problem until it finds an optimally undistin-
guishable sample.
FGSM. The Fast Gradient Sign Method or FGSM tries to
3.4.1
modify a sample by adding or substracting a small value epsilon for
each dimension of the input. More formally, the attacker generates
a perturbation vector λ such that :
λ = ϵ sgn(∇x L(f (x), y))
In untargeted mode, the attacker adds a small noise over the in-
put, adding ±ϵ to every dimension of the input in order to increase
the error, choosing the sign accordingly.
In targeted mode, the attacker generates the highest output for
the desired class, and thus the lowest error relatively to the adver-
sary’s target. The vector thus becomes :
λ = −ϵ sgn(∇x L(f (x), yadv))
Where L is the network’s loss function.
In both cases, a clipping is made to make sure the values stay
in their original range. The sum, along hundreds of dimensions, of
small input/output error gradients can result in a large perturbation.
Thus, an attacker can easily push the sample through a decision
boundary and trick the system into a misclassification.
Spartan Networks
arXiv Preprint, Dec 2018,
This attack is easy to compute and, for some unprotected classi-
fiers, creates large errors. Computing the gradient values requires
a White-Box access to the network.
The FGSM on the field. As the FGSM is a White-Box attack, the
attacker needs access to privileged information. Using the informa-
tion, the attacker takes a picture of any check. The original sample
is likely to be classified as the original check. The attacker then
computes, for each color channel of each pixel, whether they add
or substract, ϵ to each value, by looking at the sign of the loss’
gradient with respect to every dimension of its input.
The ϵ value is chosen according to a tradeoff: the lower the value,
the closer the picture will be to the original one, thus making it
harder for a system administrator to see whether there was an
attack or not. On the other hand, the larger ϵ is, the higher the
chances are for the attack to work.
3.4.2 Carlini & Wagner attack. Carlini & Wagner [9] have proposed
an attack method based on a custom gradient descent with different
candidate losses in order to learn the perturbation the same way the
network learns.
One cannot use the optimization problem in (1) to find adversar-
ial samples directly. Rather, loss functions point toward the main
direction of attack while perturbation norm stays a constraint that
can be hard (strict constraint) or loose [9]. In the latter, going further
from the constraint adds to the loss.
The loss used by the authors in [9] is a composite based on the
original network’s loss. They try different losses, linked to different
attack behaviours.
These loss functions force the adversary to learn a perturbation
that maximizes the confidence of the system on its erroneous deci-
sion. When using a loose constraint, they smooth out the clipping
process. As the constraints become differentiable, the authors allow
gradient-based methods to converge into a local optimum for their
problem.
Carlini & Wagner attacks on the field. An attacker needs a white-
box access to the classifier. They would use a gradient descent
method, incorporating one of the losses proposed by the authors.
This sample generation algorithm obeys two constraints: the first
crushes the perturbation to be zero, but the other pulls it towards a
misclassification. Through a weighted sum of the constraints, and
given enough computing power, the same kind of optimization that
trained the neural network is used to create an adversarial sample.
3.4.3 DeepFool. Moosavi-Dezfooli et al. [33] used an entirely dif-
ferent approach to make their attacks : they considered that any
classifier has locally-linear boundaries, allowing the local use of
hyperplanes. The decision hyperplane is such that if one moves
parallel to it, they will never cross the boundary and thus will never
find an adversarial example. By taking the orthgonal direction, one
can make sure that they have the fastest way to burst out of the deci-
sion boundary. In order to compensate for the linearity hypothesis,
the author use an iterative method to keep these approximations
within a close radius.
DeepFool on the field. A White-Box attacker tries to output a
wide number of photos triggering the same classification confi-
dence for every different class. This would yield a local topology
of the multi-dimensional geometry of the decision boundary. They
could thus identify the closest hyperplane-boundary to their cur-
rent point. This equivalent naive method would lead to a gigantic
amount of computation. The authors use an algorithm based on a
per-boundary geometry to quickly find the nearest way out of the
current class, thus triggering a misclassification.
We will give a practical explanation of DeepFool. The attacker
can compute at every step the set of linear rules that makes them
be in the class they want to escape, and make a small step away
from the closest combination of those rules. They do so iteratively,
to account for the global non-linearity of DNN.
Surrogate Black-Box Models. So far, every attack required a
3.4.4
White-Box access to the system. But in 2016, Papernot et al. [37]
used the fact that Adversarial Examples can transfer from one Deep
Learning Model to another[41] to create Black-Box attacks that
require no access to the network’s parameters. They train an ap-
proximation of the target model in order to create a White-Box
surrogate. They can then use the transferability property of the ad-
versarial examples: adversarial samples that fool the approximated
model have a high probability of fooling the target model. Some
White-Box attacks transfer better than others: for example, FGSM
attacks transfer well to different classifiers.
Surrogate Black-Box Models on the field. If an attacker has no ac-
cess to the model other than the check reading device, they can first
gather a small dataset of handwritten digits, and train a model on
them. Through jacobian augmentation[37], the attacker will then
distort the digits it has in the direction of the estimated boundaries,
and re-submit to the oracle for evaluation: through this process,
the surrogate neural network will get a low-performance approxi-
mation of the model. This approximate model will be vulnerable
to some adversarial examples. The transferability property make
them likely to also fool the target model.
4 ADVERSARIAL AI DEFENSES
For a systematic review of defenses, we refer the reader to [4] for
their work in the subset of attacks on images.
4.1 Defenses strategies and our focus
A defender protecting a DNN can use three different strategies in
order to increase the model’s robustness to adversarial examples.
Detection Strategy. Defenders can use a Detection algorithm,
above or within the network, in order to detect the adversial nature
of a submitted sample [16, 19, 24, 27, 28, 32, 43]. If the sample is
detected as adversarial, it is rejected and given to a human reviewer
to get its actual meaning.
Reforming Strategy. Defenders can use a Reforming system. In
this paradigm, the sample is transformed in a way that modifies
the sample’s numeric values while preserving its semantics. JPEG
Compression, Bit-Depth Reduction and Error Diffusion, are valid ex-
amples of such transformations. The Reforming system can also be
arXiv Preprint, Dec 2018,
F. Menet et al.
learned through models that re-create their inputs [12, 23, 29, 31, 39].
The defender thus expects that, by adding error or removing infor-
mation, they can remove some or all the adversarial perturbations.
This is equivalent to a form of noise reduction.
Regularization Strategy. The last defense mechanism one can
use is based on the Regularization strategy. In this paradigm, the
behaviour of the original system is regularized during a training
in order to modify the way it handles samples that are out of the
classical distribution. This is equivalent to a form of patching: by
changing the program’s behaviour, the defender makes an attack
ineffective against it.
We present below a parallel with SQL Injections, a well-known
problem in information security.
AI Defenses
Detection
Reforming
Regularization
SQLI Defenses
Detect special characters
Replace special characters
Parametric statements
We will focus on the Regularization strategy for the following
reasons:
• A Detection system creates an opportunity for a Denial-of-
Service (DoS): the attacker could slightly transform the input
of a user through a Man-In-The-Middle attack and would get
a consistent amount of rejection, while keeping the feedback
sample undistinguishable from the original one.
• A Learnable Reforming system allows an attacker to ex-
ploit another neural network. If succesfully attacked, the
reformer would regenerate a sample that is close to the tar-
get class chosen by the attacker.
• A Non-Learnable Reforming system creates a static de-
fender that an adversary can progressively learn to bypass.
• Regularization strategies have the advantage of presenting
a smaller attack surface, as there would be no other algorithm
than the one used for inference. They trade this advantage
for the requirement of retraining the whole system, opening
themselves to poisoning attacks and creating a high upfront
cost for the defender.
As we consider that DNN need to be trained at least once, the
Regularization strategy will be preferred. We highlight however
that other strategies are less ressource-intensive as they can often
be implemented without retraining the model.
4.2 Current Regularization Techniques
Adversarial Training. One of the first ideas introduced in the de-
fender’s arsenal, the Adversarial Training[18, 41] is based on gen-
erating adversarial samples through white-box attacks in order to
add them to the existing training set. By triggering the error, and
backpropagating it through the network, the system learns to resist
an attack the same way it learns the task at hand.
From an information security perspective, this method has the
disadvantage of defending the network against known attacks only.
There is no guarantee that adversarial training protects against
attacks other than the attacks the system has been trained on. More-
over, this defense can hinder the network’s performance. Madry
et al. [30] address this concern by introducing the idea of an optimal
first-order adversary that could subsum every attacker with the
same order constraints.
With these theoretical guarantees, this approach thus trans-
forms offensive strategies into defensive strategies. This approach
searches for optimal attacks to train the network upon. If success-
fully trained on strong adversarial examples, the system can have
security guarantees.
Gradient Masking/Gradient Shattering. As all current techniques
employ some form of gradient computation, one of the ideas intro-