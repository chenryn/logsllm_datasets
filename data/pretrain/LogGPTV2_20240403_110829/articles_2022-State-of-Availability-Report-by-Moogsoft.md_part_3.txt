the future—even if this means throttling change work coming into the system.
“Manage up/manage
down is the single most
dangerous attitude in
business. Transparency
between leadership
and execution teams in
business is vital.”
~Phil Tee, Moogsoft CEO
36
Teams
Leaders Are Also Unaware of How Much of
Their Teams’ Time is Spent on Monitoring
37
Teams spend by far the most time monitoring over anything else. Yet management believes they are
spending time fairly equally across the board, with less time most notably on CICD, support desk/
tickets (the same for the teams), reducing toil and testing, and QA.
Looking at the data this way showed us the leaders’ optimism bias again. Management thinks the
teams are spending time investing in the future, whereas the teams lean more heavily towards keeping
today’s systems alive. This should be a wake-up call to leaders everywhere—if you want to invest in
the work that enables digital transformation, and inject capacity into teams, you need to help them
now to find time and find ways to create more time in the future. It is commonly difficult to carve out
time to spend on experimentation and innovation.
38
Teams
The Higher the SLAs, the More
Time Spent Monitoring
The more nines in an SLA, the more time is spent monitoring—and larger companies have higher SLAs.
We also discovered that the larger the organization, the more time is spent on incident and infrastructure
management (see below). None of these activities (monitoring, incident, and infrastructure management)
is value-adding to the customer. And they are often stressful, onerous, and demotivating. They eat up
time that teams could be investing in DevOps capabilities that will buy them more capacity in the long
term to spend on improving customer experience with new features and faster platforms.
39
Teams
Larger Organizations Spend More Time on
Incident and Infrastructure Management
Since time equals money, we can also surmise that larger organizations spend more money on these
activities. These talented and expensive engineers may be in centralized IT Operations teams and
operations centers, or embedded in multifunctional, cross-skilled teams. Either way, larger organizations
are burning higher proportions of their IT budgets keeping their products and services running—not on
making them better.
40
Teams
Key Takeways
DevOps adoption and the move from project to product is still
very much underway
Engineering teams are stuck in monitoring cycles—and it’s not
working (and not fun)
Organizations are lacking the metrics that align teams and
leaders, or provide insights to improve
Finding and replacing SREs and developers with DevOps skills
is very expensive—investing in tools that support DevOps
ideals is a cost-effective alternative
41
Tools
Tools
Summary
People and processes may come first but tools matter—they have enabled so much of the digital
transformation we have seen in recent years. The internet is the tool at the bedrock of these
transformations, and software really has eaten the world. Monitoring was one of the first tool categories
to appear, initially in the 1990s in the operating system. A lot has changed since then. Let’s take a look
at how:
While it can often feel that cloud has happened, the reality is that most
organizations are still in the adoption and migration phase with only
Cloud migration is about 50% of infrastructure moved to cloud so far. Cloud promises higher
availability through modern distributed architectures and elasticity. It is
still happening
also a prerequisite for many of the optimizations offered by DevOps. While
it may be true that not everyone wants all of their workloads in the cloud,
most organizations do want most.
On average respondents said they have 16 monitoring tools, and in some cases,
they have up to 40 tools. Single domain monitoring tools are proliferating in
the tool stack, demanding more time from teams to monitor, not less. Higher
Everyone has a
SLA teams have a higher average number of tools per category. That’s a lot
LOT of monitoring
of tools and goes a long way to explaining why teams are spending so much
tooling time on monitoring. It’s likely that leaders know they have invested in all of
these tools, but underestimate how long teams have to spend managing and
maintaining them. And looking for answers when a problem occurs.
43
Monitoring tool types are all about equal with around 80% of respondents
AIOps is already
reporting using them, whether network, web performance, integration/
part of the key API, storage, system, RUM, EUM, APM, logging, dependency, CICD, social
sentiment/brand monitoring. Almost everyone is using everything but it’s
technology stack
apparently not working as teams are still missing their SLAs, and nearly
half the time their customers are telling them about problems before
these tools.
Only a third of respondents have
DevOps toolchain and automation
DevOps toolchain
capabilities today but almost everyone “Perhaps the first phase
adoption is lagging
wants them. Once more, it seems of the cloud transition
that the time spent on managing in SRE/DevOps has been
the existing investments is disabling
characterized by the
teams from optimizing for the future.
provisioning of observability
to gather data. The success
of the second phase will
be the rebalancing of
monitoring towards AIOps,
and advanced correlation,
to ensure availability is
actively managed rather
than heroically pursued.”
~Phil Tee, Moogsoft CEO
44
Tools
The Majority of Services
Are Still Not in the Cloud
Only 36% of respondents report that more than half of their services are in the cloud. The key benefits of
cloud computing are scalability, reliability, and availability.
And, along with DevOps, cloud is a key
enabler for digital transformation.
In our research, we saw respondents
struggling to move from project to
product, adopt DevOps practices and
toolchains, and with cloud adoption
more generally.
We also discovered teams are
overwhelmed with monitoring,
incident, and infrastructure
management. This reinforces the
message that teams need methods to
get themselves out of this quagmire
so that they can spend their time on
innovation and optimization.
45
Tools
Organizations are Still at the Implementation
Stage of DevOps Toolchains
This data quite closely reflects what we saw in the Teams section of this report looking at the adoption of
DevOps practices. Adoption is far outstripped by those who are planning for these toolchain components or
who want them, but haven’t started planning. With DevOps toolchains though, over 25% of respondents—
versus less than 20% of respondents relating to practices—have already adopted these components. And
only around 5% at most have no plans to use.
The highest interest is in build automation, CI server, automated unit tests, and value stream management
(VSM).
46
Tools
Teams are Managing Huge
Amounts of Monitoring Tools
It’s no wonder teams are reporting that they are spending huge amounts of their time monitoring; on
average, teams are monitoring 16 tools—if not more.
Leaders report thinking of even more tools. This could be because they are concerned with the whole
organization (i.e. multiple teams), or because they don’t understand which tools are actually being used.
High SLAs = high %
of team time spent
on monitoring +
high # of monitoring
tools
47
Tools
Teams with Higher SLAs Manage
More Tools Per Category
It seems that to reach
those higher SLAs, more
tools are required across
every category, peaking at
four nines. And more time
is spent monitoring.
Tools incur license fees
and management and
maintenance overheads.
Availability costs money.
Some of this investment
is paying off as the teams
with the higher SLAs are
more likely to meet them.
No doubt availability would suffer if the tools and the people were taken away, but for most organizations,
availability isn’t where it should be. SLAs are regularly being missed—customers are frequently reporting
issues before the tools, and teams are spending huge amounts of time on monitoring which is not creating
customer value.
48
Tools
Key Takeways
Use AIOps to reduce MTTD and MTTR to improve customer
experience and release time for improvements
Consolidate monitoring tools to reduce license, management,
and maintenance overheads
Focus on employee experience—which is intrinsically linked to
customer experience—to improve organizational performance
49
Our Guidance
Our Guidance
The data show that teams and organizations must improve availability—they are missing SLAs far too
frequently and should not be alerted to a problem by their customers. Our research shows that teams
and organizations must not only set a KPI for availability—anything not measured will atrophy—but
the actual goal is staying ahead of customer sentiment. Regardless of actual availability being industry
standard or exceeding an SLA, customers want a partner that is proactively managing their experience.
To do this, difficult decisions have to be made in the face of violated error budgets before the violation
occurs or in response to an event.
Availability comes at a high cost, and leaders need to buy back time for their teams so that they can
invest in technical stability. All the investment made in monitoring is causing teams to spend more time
monitoring. Tools and noise have proliferated as a result of throwing tools at flaky systems. Engineering
teams currently lack standard KPIs to communicate their time spent on maintenance, versus building
and automating, causing a mismatch in leaders’ and teams’ perspectives on capabilities and work
profiles.
By reducing the time teams spend on monitoring and incident management, the more time they
can spend creating value that will improve customer experience—be that new features or platform
improvements. Less unplanned work means more time paying down technical debt and automating
toil and improvements in employee experience. And it’s more time teams can spend on mastery and
learning.
As Andrew Clay Shafer put it, “You’re either building a learning organization, or you’re losing out
to someone who is.”
And mastery also contributes to employee experience, along with autonomy and purpose,
according to Dan Pink’s research in Drive . Organizations are maintaining availability, but they aren’t
investing in stability, and they are running out of runway. You must transition to technical stability
to scale and sustain your team and your infrastructure.
So how do you do that? 51
52
The Steps to Success
We’ve put together our six “Steps to Success” to help guide you and your organization towards
scaling and sustaining your team and infrastructure.
• Document and agree on your business goals in the context of availability
• Identify and classify business-critical apps, services, and infrastructure
• Discover and document the tools you have, their usage, and their costs
• Identify your current KPIs, SLAs, SLOs, and SLIs if you have them
Output & Outcomes
The results of your analysis will show you which of your
technology assets matter most to your availability goals and
show you where you need to invest and divest. This is the
foundation for what follows.
• Unlock the capability to monitor from MTTD to MTTR and set objectives
for reducing both
• Set SLOs, SLIs and error budgets if you haven’t already and track how
frequently a customer catches an issue before you do
• Make the work distribution in your team visible (add tags in your
ticketing tools) for unplanned work, improvements, technical debt, and
new features
Output & Outcomes
The results of your analysis will show you which of your
technology assets matter most to your availability goals and
show you where you need to invest and divest. This is the
54 foundation for what follows.
• Prioritize your existing monitoring tools by usage and value
• Focus on where you can reduce your monitoring tools’ footprint, and
correlation gap
• If you are not actively using a particular dataset, remove it
Output & Outcomes
Reducing the number of monitoring tools you have will not
just reduce your total cost of ownership (TCO)—it’ll also
start to dampen the noise you’re dealing with and lessen
alert fatigue, potentially also reducing MTTD and MTTR.
• Adopt AIOps to rapidly reduce noise
• Give leadership and the teams a single view of all the data and insights
• Measure the impact on the volume of unplanned work your team has
to deal with
Output & Outcomes
Using AIOps to monitor your monitoring will produce an
immediate reduction in ticket volume and shrink MTTD and
MTTR by identifying the source of problems sooner, releasing
time and capacity for you to move onto the next step.
55
• Prioritize where the technical debt hurts the most using insights from
Moogsoft to shine light onto problem areas
• Automate toil away to release even more time
• Use chaos engineering experiments to further improve availability
Output & Outcomes
Initially you’ll see more tickets tagged as technical debt—then
fewer as it’s paid down and you benefit from increased system
stability presenting itself as reduced mean time between
Incidents and less unplanned work. You’ll see an increase in
tickets allocated to new features and improvements.
• Reprioritize on innovating for customer experience, and away from
maintaining customer experience
• Remeasure how frequently you are catching problems before your
customers, and report on the improvement
• Refocus efforts on learning and implementing DevOps practices to
further nurture both throughput and availability
Output & Outcomes
By now, not only will you be seeing a reduction in MTTD, MTTR,
and MTBI, you’ll also be meeting SLAs more frequently so
you can adopt higher SLAs. You’ll be seeing more user stories
completed, higher levels of customer satisfaction reported
and higher levels of DevOps capability adoption reported.
56
“Figuring out how to optimize
a complex infrastructure and
team environment can get
quickly overwhelming. Every
leader and every engineer wants
to spend time innovating, the
problem is finding the time.”
~ Minami Rojas, VP of Growth and
Marketing at Moogsoft
57
Survey Demographics
Demographics
Roles
n=1899 with a 92% completion rate.
59
Demographics
Geography
60
Demographics
Industry
61
Demographics
Organizational Size
62
Authors
Lead Analysts: Contributors:
Helen Beal Kai Wang
Minami Rojas Melissa Hill
Mike McGibbney
Moogsoft Team: Editing and Proofreading:
Phil Tee Rupert Field
Richard Whitehead
John Haley Design:
Eric Brousseau
Mike Cabot
Alison Shutterly
Chris Boyd
Our Mission
As the world grows ever more complex, we stick to our belief that simplicity is the key to
greater achievement. And we never forget that AI is built on human designs, dreams, and
desires. That’s why everything Moogsoft makes, helps people and machines work more
harmoniously, to create clarity from chaos, and expedite innovation.
Moogsoft provides an AIOps solution for frontline engineers faced with availability pressure
by detecting problems before they become critical, identifying who should respond, and
understanding patterns to prevent similar issues in the future.
Learn More at www.moogsoft.com