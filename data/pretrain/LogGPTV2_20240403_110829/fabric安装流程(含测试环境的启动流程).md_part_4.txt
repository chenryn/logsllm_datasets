{
\"registry-mirrors\": \[\"https://registry.docker-cn.com\"\]
}
EOF
systemctl restart docker
docker version
##Docker-Compose安装
wget -P /opt/
https://github.com/docker/compose/releases/download/1.23.2/docker-compose-Linux-x86_64
cd /opt/
cp docker-compose-Linux-x86_64 /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose
docker-compose version
# WBY的fabric环境
192.168.10.107 orderer
192.168.10.100 orderer2
192.168.10.123 localhost
192.168.10.104 orderer3
启动步骤（在192.168.10.123上从上倒下按顺序执行）：
192.168.10.123 相关环境
![](media/image50.png){width="7.268055555555556in"
height="2.2069444444444444in"}
![](media/image51.png){width="7.267361111111111in"
height="0.9881944444444445in"}
Ansible环境
![](media/image52.png){width="7.259722222222222in"
height="1.2638888888888888in"}
启动:进入123的/etc/ansible目录顺序由上向下执行：
ansible-playbook docker-compose-couchdb.yml \--tags docker-compose
ansible-playbook docker-compose-kafka.yml
ansible-playbook fabric-orderer.yml
ansible-playbook fabric-peer.yml
## 第一步：启动couchdb数据库
启动:进入123的/etc/ansible目录执行如下命令
ansible-playbook docker-compose-couchdb.yml \--tags docker-compose
#在/etc/ansible目录中执行
\[root@localhost ansible\]# pwd
/etc/ansible
\[root@localhost ansible\]# vim docker-compose-couchdb.yml
\- hosts: peer0org2 peer1org1 peer0org1
remote_user: root
tasks:
\- name: copy couchdb.tar.gz
copy: src=fabric-couchdb.tar.gz dest=/opt/
\- name: docker load couchdb
shell: docker load -i fabric-couchdb.tar.gz
args:
chdir: /opt/
\- name: copy docker-compose-couchdb
copy: src=docker-compose-couchdb.yaml dest=/opt/
tags: docker-compose #执行1：拷贝文件
\- name: docker-compose up
shell: docker-compose -f docker-compose-couchdb.yaml up -d
args:
chdir: /opt/ #进入远程主机/opt目录
tags: docker-compose #执行2：启动镜像
#remote_user可以定义指定用户通过sudo的方法在被管理主机上运行指令，甚至可以在使用become指定sudo切换的用户。
#执行1：将123主机的/etc/ansible/files/docker-compose-couchdb.yaml
拷贝到peer0org2 peer1org1 peer0org1 三台主机的/opt目录(注意文件所在路径)
#执行2：根据拷贝到peer0org2 peer1org1 peer0org1
三台主机的/opt/docker-compose-couchdb.yaml 文件启动couchdb容器
三台主机启动couchdb的/opt/docker-compose-couchdb.yaml 文件内容如下
\[root@orderer opt\]# vim docker-compose-couchdb.yaml #peer0org2
peer1org1 peer0org1 三台主机配置相同
version: \'2\'
services:
couchdb:
container_name: couchdb
restart: always
network_mode: \"host\"
image: hyperledger/fabric-couchdb:amd64-0.4.14
environment:
\- COUCHDB_USER=
\- COUCHDB_PASSWORD=
ports:
\- \"5984:5984\"
volumes:
\- /opt/couchdb/data:/opt/couchdb/data
#docker stop \$(docker ps -q) && docker rm \$(docker ps -qa)
停止所有的容器，并删除
#启动后确保三台主机5984端口都开启，如果未开启则为启动失败，尝试重启docker然后再启动
http://192.168.10.107:5984/\_utils
http://192.168.10.100:5984/\_utils
http://192.168.10.104:5984/\_utils
![](media/image53.png){width="7.259027777777778in"
height="3.939583333333333in"}
Size大小不一样，但是name与# of Docs数量一致
## 第二步：启动zookeeper/kafka
启动:进入123的/etc/ansible目录执行如下命令
ansible-playbook docker-compose-kafka.yml
\[root@localhost ansible\]# pwd
/etc/ansible
\[root@localhost ansible\]# vim docker-compose-kafka.yml
\- hosts: peer0org1
remote_user: root
tasks:
\- name: copy docker-compose-kafka.yaml
copy: src=docker-compose-kafka.yaml dest=/opt/
\- name: docker up kafka1
shell: docker-compose -f docker-compose-kafka.yaml up -d kafka1
args:
chdir: /opt/
\- hosts: peer1org1
remote_user: root
tasks:
\- name: copy docker-compose-kafka.yaml
copy: src=docker-compose-kafka.yaml dest=/opt/
\- name: docker up kafka2
shell: docker-compose -f docker-compose-kafka.yaml up -d kafka2
args:
chdir: /opt/
\- hosts: peer0org2
remote_user: root
tasks:
\- name: copy docker-compose-kafka.yaml
copy: src=docker-compose-kafka.yaml dest=/opt/
\- name: docker up kafka3
shell: docker-compose -f docker-compose-kafka.yaml up -d kafka3
args:
chdir: /opt/
#1、三个hosts分别为三个节点，都是先将123上的/etc/ansible/files/docker-compose-kafka.yaml
文件拷贝到三个主机的/opt/ 目录中，
#2、然后启动依据拷贝而来的docker-compose-kafka.yaml文件，peer0org1启动zoo1、kafka1，peer1org1启动zoo2、kafka2，peer0org2启动zoo3、kafka3。
/etc/ansible/files/docker-compose-kafka.yaml 内容如下
\[root@localhost ansible\]# vim files/docker-compose-kafka.yaml
version: \'2\'
services:
zoo1:
image: hyperledger/fabric-zookeeper:amd64-0.4.14
restart: always
hostname: zoo1
container_name: zoo1
network_mode: \"host\"
ports:
\- 2181:2181
volumes:
\- /opt/zoo/:/data
\- /opt/zoo/datalog:/datalog
environment:
ZOO_MY_ID: 1
ZOOKEEPER_TICK_TIME: 2000
ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=kafka2:2888:3888
server.3=kafka3:2888:3888
zoo2:
image: hyperledger/fabric-zookeeper:amd64-0.4.14
restart: always
hostname: zoo2
container_name: zoo2
network_mode: \"host\"
ports:
\- 2181:2181
volumes:
\- /opt/zoo/:/data
\- /opt/zoo/datalog:/datalog
environment:
ZOO_MY_ID: 2
ZOOKEEPER_TICK_TIME: 2000
ZOO_SERVERS: server.1=kafka1:2888:3888 server.2=0.0.0.0:2888:3888
server.3=kafka3:2888:3888
zoo3:
image: hyperledger/fabric-zookeeper:amd64-0.4.14
restart: always
hostname: zoo3
container_name: zoo3
network_mode: \"host\"
ports:
\- 2181:2181
volumes:
\- /opt/zoo/:/data
\- /opt/zoo/datalog:/datalog
environment:
ZOO_MY_ID: 3
ZOOKEEPER_TICK_TIME: 2000
ZOO_SERVERS: server.1=kafka1:2888:3888 server.2=kafka2:2888:3888
server.3=0.0.0.0:2888:3888
kafka1:
image: hyperledger/fabric-kafka:amd64-0.4.14
restart: always
network_mode: \"host\"
hostname: kafka1
container_name: kafka1
depends_on:
\- zoo1
ports:
\- 9092:9092
environment:
KAFKA_ADVERTISED_HOST_NAME: kafka1
KAFKA_MIN_INSYNC_REPLICAS: 1
KAFKA_BROKER_ID: 1
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
KAFKA_MESSAGE_MAX_BYTES: 1048576
KAFKA_REPLICA_FETCH_MAX_BYTES: 1048576
KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE: \"false\"
KAFKA_LOG_RETENTION_MS: -1
KAFKA_ADVERTISED_PORT: 9092
KAFKA_ZOOKEEPER_CONNECT: kafka1:2181,kafka2:2181,kafka3:2181
volumes:
\- /opt/kafka/:/kafka
kafka2:
image: hyperledger/fabric-kafka:amd64-0.4.14
restart: always
network_mode: \"host\"
hostname: kafka2
container_name: kafka2
depends_on:
\- zoo2
ports:
\- 9092:9092
environment:
KAFKA_ADVERTISED_HOST_NAME: kafka2
KAFKA_MIN_INSYNC_REPLICAS: 1
KAFKA_BROKER_ID: 2
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
KAFKA_MESSAGE_MAX_BYTES: 1048576
KAFKA_REPLICA_FETCH_MAX_BYTES: 1048576
KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE: \"false\"
KAFKA_LOG_RETENTION_MS: -1
KAFKA_ADVERTISED_PORT: 9092
KAFKA_ZOOKEEPER_CONNECT: kafka1:2181,kafka2:2181,kafka3:2181
volumes:
\- /opt/kafka/:/kafka
kafka3:
image: hyperledger/fabric-kafka:amd64-0.4.14
restart: always
network_mode: \"host\"
hostname: kafka3
container_name: kafka3
depends_on:
\- zoo3
ports:
\- 9092:9092
environment: