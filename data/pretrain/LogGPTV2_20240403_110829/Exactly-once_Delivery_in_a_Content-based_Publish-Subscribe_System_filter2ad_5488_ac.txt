m
(
y
c
n
e
t
a
l
)
s
m
(
e
g
n
a
r
k
c
a
n
t
n
e
s
0
4000
5000
6000
7000
8000
9000
10000
time (ms)
Figure 6. Latency and Nacks for b1-s1 failure
the failure. For both message latency plots and nack mes-
sage plots, the X axis is the send time of the message. Each
point on the nack range plot is a single nack message, and
the nack range value is cumulative. As the connection is
ﬁrst stalled, s1 does not notice the failure till more than 2
seconds later (just after time 8000). Then it sends nacks
(to b2, since the b1-s1 link is down), and receives the mes-
sages lost on the link. Our implementation chops a large
tick range that need to be nacked into smaller parts, so that
the effect of a nack message being lost is small, hence we
see multiple nack messages sent by s1. Note that the last
message published, before the failure, is received after the
nacks are sent, and hence experiences a latency of 2.5 sec-
onds. Since the lost messages are received in a burst after
the nacks are sent, the latency values for s1 show a saw-
tooth form. After all routing switches to the path b2-s1, the
latency returns to normal. Note that the latency at s2 was
unaffected by the failure. The other tests also showed sim-
ilar behavior for subscribers not on any failure path, so we
only show affected subscribers in the remaining plots.
Results for b1 crash: In this test, broker b1 was crashed
and restarted 30 seconds later. Before the crash, b1 and b2
were splitting the input message load, i.e., each was han-
dling messages from 2 of the 4 pubends. Figure 7 shows
two plots, one for the latency and and the other zooms in
on the time when the nacks are sent, to show the nacks sent
by s1, s2 and b2. The latency is shown for a subscriber
that is subscribing to messages from a pubend whose mes-
sages were ﬂowing through broker b1. The ﬁrst peak in the
latency is due to stalling broker b1 before crashing it. As
soon as the b1 crashes, its neighbors detect the failure and
all messages start ﬂowing through b2. The latency plot for
s2 is similar to s1 and is not shown. The second latency
peak occurs when b1 recovers, which results in messages
from 2 of the 4 pubends to again start ﬂowing through b1.
The main reason for this transient increase in latency is the
extra computation in the broker machine just when it starts
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply. 
)
s
m
(
y
c
n
e
t
a
l
6000
5000
4000
3000
2000
1000
0
0
latency in s1
nacks sent by s1
)
s
m
(
s
e
g
n
a
r
k
c
a
n
7000
6000
5000
4000
3000
2000
1000
nacks sent by s1
nacks sent by s2
nacks sent by b2
10000
20000
30000
time (ms)
40000
50000
0
11950
11960
11970
11980
11990
12000
12010
time (ms)
Figure 7. Latency and Nack Range for b1 crash
subscribers at s1-s5 in a similar manner. Figure 8 shows the
latency seen by a subscriber connected to s1, and the nacks
initiated by s1. Unlike the earlier tests, where the publisher
kept publishing despite the failure, here the publisher was
down and unable to publish for the duration of p1’s crash.
Any messages that the pubends had logged but were unable
to send out before the crash show a high latency, as can be
seen by the partial sawtooth form of the latency. Before the
pubend crash, messages are being received in order at s1.
When the pubend crashes, no new messages are received at
s1, but no gaps are created either. As delay curiosity thresh-
old (DCT) is inﬁnity, s1 does not initiate any nacks while p1
is down. When p1 recovers, a time interval longer than the
ack expected threshold (AET) has already elapsed, causing
it to ﬁrst send an AckExpected message that contains the
timestamp of the last message it logged (at each pubend)
before it crashed. This results in nacks from s1-s5, and the
latency quickly returns to normal.
5 Related Work
Guaranteed Delivery in Pub/Sub Systems: Most Inter-
net scale pub/sub systems, such as SIENA [5], offer best-
effort delivery. Guaranteed delivery is offered in messag-
ing systems such as IBM’s MQseries, but such systems are
message queueing systems that use a store and forward ap-
proach to ensure reliability. The store and forward approach
incurs high latency since messages need to be logged at each
stage, and cannot support high throughput due to the high
per message overhead.
The work that is most closely related to ours is the Di-
versity Control Protocol (DCP) [11], used to route and ﬁlter
XML documents on an overlay mesh network that ﬁlters at
intermediate nodes. DCP runs on a replicated n-resilient
mesh network, unlike our network in which messages are
Figure 8. Latency and nacks for p1 crash
up, such as to run the Java JIT compiler. Note that no nacks
are sent at this time, since messages are not lost, just de-
layed.
The second plot in ﬁgure 7 shows the nacks and nack
range sent by s1, s2 and b2. As s1 and s2 lost about the
same messages due to b1’s failure, the nacks sent by them
are almost identical in number and range. Nacks sent by b2
are in response to nacks by s1 or s2. Since b2 does not have
any of the data requested by the nacks (that data was ﬂowing
through b1), all the nacks are forwarded to p1. Note that the
nack consolidation by b2 is almost perfect, in that the nack
range sent by b2 is about half that of s1 and s2 combined.
Another thing to note is that the nack range of s1 and s2
is about 5500ms. This corresponds to 2 different pubends,
both of whose messages were ﬂowing through b1, so about
2750ms of data was lost for each pubend. This agrees with
the time interval for which b1 was stalled before the crash.
Results for p1 crash: In this test, the PHB p1 was crashed
and restarted after about 20 seconds. This affected all the
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply. 
not replicated on redundant paths. However, our protocol
can be easily adapted to a DCP like network. DCP guar-
antees delivery by running a hop-by-hop reliable protocol,
where the receiving broker becomes the reliable sender for
the next hop. Since a gapless stream is reconstructed at each
hop, the entire stream is delayed when a single gap is found.
Reliable Multicast Protocols: There is a large body of
work on reliable multicast in the networking literature.
Most of it, such as SRM [6] and RMTP [8], deals with
building an end-to-end reliable protocol using the underly-
ing best-effort IP multicast service. The ordering offered
by these protocols is publisher order. Most of the details of
such protocols deal with solving the ack or nack-explosion
problem in the absence of router assist, by either arranging
the receivers in a hierarchy of groups or by using receiver
backoff.
Two exceptions to a purely end-to-end approach for re-
liable multicast are Active Reliable Multicast (ARM) [13]
and the Breadcrumb Forwarding Service (BCFS) [14, 15].
ARM uses ’active networking’ routers to efﬁciently con-
solidate nacks ﬂowing to the source. The consolidation is
done by caching nack requests for a certain time interval.
Retransmissions are forwarded only to downstream routers
that have nacked, again by examining the cached nack re-
quests. The BCFS [14] network service, is similar to what
is implemented in the active routers for ARM. The curiosity
stream described in this paper is a technique for doing such
nack consolidation and forwarding of retransmissions, and
can be applied to ARM and BCFS.
Atomic Multicast: There are a large number of systems
that provide atomic multicast/broadcast primitives, such as
Isis [4] and Horus [12]. They use virtual synchrony or one
of its variants to provide this atomicity guarantee. These
synchrony protocols typically require synchronizing the
complete system on each membership change, and hence
are hard to scale to more than a few hundred participants [9].
Both the reliable multicast and the atomic multicast pro-
tocols are group-based, and do not support selective ﬁlter-
ing.
6 Conclusion
In this paper we have presented a new model and al-
gorithm for providing exactly-once message delivery to
subscribers in a content-based publish-subscribe system.
The problem is challenging due to ﬁltering at intermediate
nodes, and previous algorithms have adopted a simple but
restrictive hop-by-hop store and forward approach. In com-
parison, our solution maintains only soft-state at intermedi-
ate brokers, and does not need to stall message forwarding
in the presence of message loss. This allows the system to
sustain high throughput despite failures, and makes it sim-
ple to dynamically replace brokers in the overlay network.
Our implementation has an overhead of only 4% CPU uti-
lization, compared to best-effort delivery, in the absence of
failures. We have also demonstrated that our implemen-
tation rapidly switches around failures, and does effective
nack consolidation.
References
[1] M. K. Aguilera and R. E. Strom. Efﬁcient atomic broadcast
using deterministic merge. In Proceedings of the 19th ACM
symposium on Principles of distributed computing, 2000.
[2] M. K. Aguilera, R. E. Strom, D. C. Sturman, M. Astley, and
T. D. Chandra. Matching events in a content-based subscrip-
tion system. In Proceedings of the Principles of Distributed
Computing, 1999, pages 53–61, May 1999.
[3] G. Banavar, T. Chandra, B. Mukherjee, J. Nagarajarao, R. E.
Strom, and D. C. Sturman. An efﬁcient multicast proto-
col for content-based publish-subscribe systems.
In Pro-
ceedings of the 19th IEEE International Conference on Dis-
tributed Computing Systems, 1999, pages 262–272, 1999.
[4] K. Birman, A. Schiper, and P. Stephenson. Lightweight
causal and atomic group multicast. ACM Transactions on
Computer Systems, 9(3):272–314, August 1991.
[5] A. Carzaniga. Architectures for an Event Notiﬁcation Ser-
vice Scalable to Wide-area Networks. PhD thesis, Politec-
nico di Milano, December 1998.
[6] S. Floyd, V. Jacobson, C.-G. Liu, S. McCanne, and
L. Zhang. A reliable multicast framework for light-weight
sessions and application level framing. IEEE/ACM Transac-
tions on Networking, November 1996.
[7] V. Jacobson. Congestion avoidance and control. Computer
Communication Review, 18(4):314–329, 1988.
[8] J. C. Lin and S. Paul. Rmtp: A reliable multicast transport
protocol. In Proceedings of IEEE Infocom’96, pages 1414–
1424, 1996.
[9] R. Piantoni and C. Stanescu.
change trading system.
Computing (FTCS), pages 309–313, 1997.
Implementing the swiss ex-
In Symposium on Fault-Tolerant
[10] S. Raman and S. McCanne. A model, analysis, and protocol
framework for soft state-based communication. In Proceed-
ings of ACM SIGCOMM, pages 15–25, 1999.
[11] A. Snoeren, K. Conley, and D. Gifford. Mesh-based content
routing using xml. In Proceedings of the 18th ACM Sympo-
sium on Operating System Principles, 2001.
[12] R. van Renesse, K. Birman, and S. Maffeis. Horus: a ﬂexible
group communication system. Communications of the ACM,
39(4), April 1996.
[13] L. wei H. Lehman, S. J. Garland, and D. L. Tennenhouse.
In Proceedings of IEEE INFO-
Active reliable multicast.
COM’98, 1998.
[14] K. Yano and S. McCanne. The breadcrumb forwarding ser-
vice: A synthesis of pgm and express to improve and sim-
plify global ip multicast, 2000.
[15] K. Yano and S. McCanne. A window-based congestion con-
In Pro-
trol for reliable multicast based on tcp dynamics.
ceedings of ACM Multimedia, pages 249–258, 2000.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:11 UTC from IEEE Xplore.  Restrictions apply.