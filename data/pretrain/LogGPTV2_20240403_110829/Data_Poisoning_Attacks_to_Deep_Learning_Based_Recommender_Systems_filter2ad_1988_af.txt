0.0027
0.0027
0.0027
0.0038
0.0009
0.0020
0.0011
0.0015
0.0015
1%
0.0023
0.0035
0.0030
0.0036
0.0042
0.0009
0.0024
0.0025
0.0028
0.0022
3%
0.0023
0.0070
0.0070
0.0064
0.0099
0.0009
0.0088
0.0074
0.0087
0.0128
5%
0.0023
0.0083
0.0092
0.0096
0.0150
0.0009
0.0189
0.0160
0.0152
0.0214
0.5%
0
0.0002
0.0003
0.0003
0.0010
0.0001
0.0003
0.0001
0.0004
0.0007
1%
0
0.0005
0.0005
0.0005
0.0023
0.0001
0.0010
0.0004
0.0009
0.0014
3%
0
0.0016
0.0018
0.0019
0.0082
0.0001
0.0042
0.0027
0.0049
0.0101
5%
0
0.0030
0.0034
0.0035
0.0141
0.0001
0.0101
0.0091
0.0096
0.0184
TABLE VI: HR@10 on ML-1M dataset with a partial rating
matrix.
Knowledge level
30%
Attack
None
Random
Bandwagon
MF
Our attack
Random target items
0.0017
0.0069
0.0060
0.0040
0.0092
TABLE VII: HR@10 on ML-1M dataset with a subset of users.
Knowledge level
30%
Attack
None
Random
Bandwagon
MF
Our attack
Random target items
0.0017
0.0069
0.0057
0.0035
0.0091
item i, and ci is the total number of ratings for item
i in the whole dataset.
• Weighted Degree of Agreement (WDA)
[32]. The
feature is the numerator of the RDMA feature, which
is computed as follows:
WDAu = (cid:88)i∈Iu
| yui − y(i) |
ci
.
(10)
from Mean
Deviation
[32]. This
• Weighted
Agreement
(WDMA)
feature considers more the
items that have less ratings, which is similar in form
to RDMA. It is calculated as follows:
|yui−y(i)|
| Iu |
WDMAu = (cid:80)i∈Iu
c2
i
.
(11)
• Mean Variance (MeanVar) [32]. This feature denotes
the average variance of rating scores of a uesr to the
mean rating scores of the corresponding items. The
13
FMTDu =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
MeanVar feature for a user u is computed as follows:
MeanVaru = (cid:80)i∈Iu
[yui − y(i)]2
| Iu |
.
(12)
Filler Mean Target Difference (FMTD) [32]. This
feature measures the divergence between rating scores
of a user, which is obtained by:
•
yui
| IuM | − (cid:80)j∈IuO
(cid:80)i∈IuM
yuj
| IuO |
,
(13)
where IuM is the set of items in Iu that u gave
the maximum rating score and IuO includes all other
items in Iu.
For each kind of poisoning attack, we generate certain
amount of fake users and extract the same number of normal
users from the original dataset to form a user set. The corre-
sponding features for each user in the user set is calculated
to constitute a training dataset for fake user classiﬁer. In our
experiments, 300 normal users and 300 fake users are included
in the training dataset. We follow the workﬂow of SVM-
TIA [51] method and use the grid search with 5-fold cross
validation to select the best parameters for the classiﬁer. SVM-
TIA [51] is one of the state-of-the-art detection methods for
shilling attacks. The detection method contains two phases. In
the ﬁrst phase, i.e., the support vector machine (SVM) phase,
an SVM classiﬁer is used to ﬁlter out a suspicious user set that
may contain both fake users and normal users. To keep the
normal users in the suspicious user set, the second target item
analysis (TIA) phase tries to ﬁnd out target items by counting
the number of maximum rating (or minimum rating under
demotion attacks) of each item in the suspicious user set. Then
the items whose number of maximum rating exceeds threshold
τ will be regarded as target items under the assumption that
attackers will always give the maximum rating to target items.
The users who set target items the maximum rating will be
judged as fake users while others are viewed as normal users.
Here, τ is a hyperparameter that balances between ﬁltering
out fake users and retaining normal users. That is, a higher τ
will cause fake users with a small attack size easier to escape
detection, while a lower τ makes the detector more likely to
TABLE VIII: Detection results for different attacks.
Dataset
Phase
Attack
ML-100K
SVM
TIA
Random
Bandwagon
MF
Our attack
Random
Bandwagon
MF
Our attack
FPR
FNR
0.5%
0.0106
0.0127
0.0191
0.1410
0.0001
0
0.0001
0.1267
1%
0.0106
0.0127
0.0191
0.1410
0.0001
0
0.0001
0.1273
3%
0.0106
0.0127
0.0191
0.1410
0.0001
0.0003
0.0013
0.1283
5%
0.0106
0.0127
0.0191
0.1410
0.0001
0.0008
0.0050
0.1290
0.5%
0.0200
0.0400
0.0400
0.3400
0.0200
0.0400
0.0400
0.3800
1%
0.0111
0.0222
0.0556
0.3444
0.0111
0.0222
0.0556
0.3444
3%
0.0179
0.0214
0.0500
0.2357
0.0179
0.0214
0.0500
0.2357
5%
0.0021
0.0127
0.0298
0.2340
0.0021
0.0127
0.0298
0.2340
incorrectly ﬁlter out normal users in the suspicious user set. As
the attack size of our attack can be quite small (e.g., 0.5%), in
order to retain as many as normal users while maintaining the
ability to ﬁlter out fake users, we set τ to 0.4% (i.e., τ = 4 for
ML-100K) of the total number of normal users, slightly small
to the smallest attack size (i.e., 0.5%) in our experiments.
Note that, before training and testing the classiﬁer, we
perform data scaling on the input data, which signiﬁcantly
improves the model performance in this scenario. After the
classiﬁer is trained well, we can simply deploy the classiﬁer