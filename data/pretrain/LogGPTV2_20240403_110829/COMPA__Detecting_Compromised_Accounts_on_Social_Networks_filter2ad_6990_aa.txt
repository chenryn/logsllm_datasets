# COMPA: Detecting Compromised Accounts on Social Networks

**Authors:**
- Manuel Egele<sup>∗†</sup>
- Gianluca Stringhini<sup>∗</sup>
- Christopher Kruegel<sup>∗</sup>
- Giovanni Vigna<sup>∗</sup>

**Affiliations:**
- <sup>∗</sup>University of California, Santa Barbara, CA
- <sup>†</sup>Carnegie Mellon University, Pittsburgh, PA

**Contact:**
- {maeg, gianluca, chris, vigna}@cs.ucsb.edu

## Abstract

As social networking sites have grown in popularity, cybercriminals have increasingly exploited these platforms to spread malware and conduct scams. Previous research has focused on detecting fake (Sybil) accounts used for spam distribution. However, as a countermeasure, attackers have shifted to compromising legitimate accounts, which are more challenging to detect and manage. In this paper, we present COMPA, a novel approach for detecting compromised user accounts on social networks. Our method combines statistical modeling and anomaly detection to identify sudden behavioral changes in user accounts. We applied COMPA to large datasets from Twitter and Facebook, demonstrating its high precision in identifying compromised accounts.

## 1. Introduction

Social networking sites like Facebook and Twitter have become integral to daily communication, enabling users to stay connected with friends, family, and colleagues. These platforms allow users to build a network of trusted connections, forming a social graph that governs information dissemination. Unfortunately, the vast user base of these networks has also attracted the attention of cybercriminals, who exploit them to spread malware, phishing links, and other malicious content.

Previous work has extensively studied the use of fake (Sybil) accounts, which exhibit highly anomalous behavior and are relatively easy to detect. In response, attackers have started compromising legitimate accounts, leveraging the trust relationships established by the account owners. This makes it more difficult for social network providers to clean up, as they cannot simply delete the profiles.

In this paper, we introduce a new approach to detect compromised user accounts on social networks. Our method uses a combination of statistical modeling and anomaly detection to identify accounts that experience sudden behavioral changes. Since such changes can also be due to benign reasons, we look for groups of accounts that all experience similar changes within a short period, assuming these changes are part of a malicious campaign. We developed a tool called COMPA, which we tested on a large dataset of over 1.4 billion publicly available Twitter messages and 106 million Facebook messages. COMPA was able to identify compromised accounts with high precision.

## 2. Background

### 2.1. Social Network Usage and Threats

Social networks have become increasingly popular, with millions of users sharing personal updates, news, and media. The connections formed between users create a social graph that controls how information spreads. Cybercriminals exploit these networks to distribute malware, phishing links, and scam messages. According to a 2008 study, 83% of social network users received at least one unwanted message that year [1]. Large-scale malware campaigns have also been carried out over social networks [2], and previous work has shown that spam, phishing, and malware are significant threats [3, 4].

### 2.2. Detection and Mitigation Approaches

To address the growing problem of malicious activity, researchers have proposed various detection and mitigation approaches. Initial work focused on detecting fake accounts, which are automatically created to spread malicious content [5, 6, 7]. However, these systems do not differentiate between Sybil and compromised accounts. A compromised account is an existing, legitimate account taken over by an attacker, often through methods like cross-site scripting or phishing.

While fake accounts are easier to create, compromised accounts are more valuable to cybercriminals because they can leverage the account's history and network of trust to spread malicious content more effectively [9]. As a result, attackers increasingly abuse legitimate accounts to distribute their messages [3, 4]. To identify campaigns involving both compromised and fake accounts, researchers have shifted their focus to the messages themselves, using techniques to find similar messages [10, 3].

### 2.3. Limitations of Existing Approaches

Simply grouping similar messages is not sufficient to detect malicious campaigns, as many clusters may contain benign messages, such as "happy birthday" wishes or template-based notifications from applications like Foursquare [11]. Some systems use URL-based features to distinguish between benign and malicious clusters [3, 12, 13], but these techniques have limited scope and cannot detect messages without URLs. Other systems consider additional features like cluster size or average number of connections per user, but their accuracy is often below 80% [10]. Moreover, these systems cannot distinguish between messages sent by compromised and fake accounts, which is crucial for appropriate mitigation.

## 3. Our Approach

Our approach, implemented in the tool COMPA, offers three key features:
1. **URL Independence:** It does not rely on the presence of URLs in messages, allowing it to detect a broader range of malicious content, including scam messages with phone numbers and instant messaging contacts.
2. **High Accuracy:** It detects compromised accounts with very low false positives.
3. **Focus on Compromised Accounts:** It specifically targets compromised accounts, leaving the detection of fake accounts to other systems or social network providers.

### 3.1. Core Idea

The core idea is to model the regular activities of individual users. If an account is compromised, there will likely be a noticeable change in behavior. We introduce a collection of statistical models, called a behavioral profile, to capture the past behavior of a user. Each model corresponds to a characteristic feature of a message, such as the time of day it was sent or the language it was written in. These models are platform-agnostic and can be applied to different social networks.

### 3.2. Anomaly Detection

A single message that violates a user's behavioral profile does not necessarily indicate a compromise; it could be an outlier or a normal change in behavior. Therefore, our approach looks for other similar messages that also violate the behavioral profiles of their respective users. This allows us to detect campaigns even if they involve only a small number of similar messages.

### 3.3. Implementation

We implemented our approach in COMPA, which can be used by social network operators to identify compromised accounts and take appropriate countermeasures, such as deleting offending messages or resetting passwords. COMPA relies on behavioral patterns rather than suspicious message content, making it capable of detecting types of malicious messages missed by other techniques.

## 4. Experimental Results

We applied COMPA to two large-scale datasets:
- **Twitter Dataset:** Messages collected from May 13, 2011, to August 12, 2011, consisting of over 1.4 billion tweets.
- **Facebook Dataset:** Messages from September 2007 to July 2009, consisting of 106 million messages.

Our results show that COMPA is effective in detecting compromised accounts with very few false positives. Specifically, we detected 383,613 compromised accounts on Twitter and 11,087 compromised accounts on Facebook.

## 5. Contributions

- **Novel Approach:** We introduce the first approach specifically designed to detect compromised accounts on social networks, providing crucial input for social network providers to initiate proper mitigation efforts.
- **Behavioral Features:** We propose a novel set of features to characterize regular user activity based on the stream of messages each user posts. These features are used to create models that identify anomalous messages.
- **Effectiveness:** We demonstrate that our approach can effectively detect compromised accounts with very low false positives, as validated by our experiments on large datasets from Twitter and Facebook.

## 6. Behavioral Profiles

### 6.1. Building Behavioral Profiles

A behavioral profile captures a user's normal behavior based on their historical message stream. Our system focuses on the stream of messages posted by a user, as other features like profile pictures or friend activity are typically not accessible historically.

To build a behavioral profile for a user U, the system initially obtains the message stream from the social networking site. The stream must contain a minimum number of messages (S) to ensure it captures the breadth and variety of the user's activity. In our experiments, we determined that S = 10 messages is a suitable threshold. Profiles with fewer messages pose a limited threat, as such accounts are either new or inactive.

### 6.2. Feature Extraction and Modeling

Once the message stream is obtained, the system extracts a set of feature values from each message and trains a statistical model for each feature. These models capture characteristics such as the time the message was sent, the application used to generate it, and the language it was written in. The anomaly score for a new message is computed by comparing its feature values to the corresponding models, with each model producing a score in the interval [0, 1].

### 6.3. Modeling Message Characteristics

Our approach models the following seven features when building a behavioral profile:
- **Time (hour of day):** Captures the hour(s) of the day when messages are typically sent.
- **Application Used:** Identifies the client application used to post the message.
- **Language:** Determines the language in which the message was written.
- **Message Length:** Measures the length of the message.
- **Content Similarity:** Assesses the similarity of the message content to previous messages.
- **URL Presence:** Checks for the presence of URLs in the message.
- **Geographic Location:** Tracks the geographic location from which the message was posted.

By combining these features, we can accurately assess whether a new message aligns with the expected behavior of the user, thereby identifying potential compromises.