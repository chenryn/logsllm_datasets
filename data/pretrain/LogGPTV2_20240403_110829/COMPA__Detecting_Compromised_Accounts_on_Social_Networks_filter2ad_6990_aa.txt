title:COMPA: Detecting Compromised Accounts on Social Networks
author:Manuel Egele and
Gianluca Stringhini and
Christopher Kr&quot;ugel and
Giovanni Vigna
COMPA: Detecting Compromised Accounts on Social Networks
Manuel Egele∗ †, Gianluca Stringhini∗, Christopher Kruegel∗, and Giovanni Vigna∗
∗ University of California, Santa Barbara, Santa Barbara, CA
{maeg,gianluca,chris,vigna}@cs.ucsb.edu
† Carnegie Mellon University, Pittsburgh, PA
PI:EMAIL
Abstract
1
Introduction
As social networking sites have risen in popularity,
cyber-criminals started to exploit these sites to spread mal-
ware and to carry out scams. Previous work has extensively
studied the use of fake (Sybil) accounts that attackers set
up to distribute spam messages (mostly messages that con-
tain links to scam pages or drive-by download sites). Fake
accounts typically exhibit highly anomalous behavior, and
hence, are relatively easy to detect. As a response, attackers
have started to compromise and abuse legitimate accounts.
Compromising legitimate accounts is very effective, as at-
tackers can leverage the trust relationships that the account
owners have established in the past. Moreover, compro-
mised accounts are more difﬁcult to clean up because a so-
cial network provider cannot simply delete the correspond-
ing proﬁles.
In this paper, we present a novel approach to detect
compromised user accounts in social networks, and we ap-
ply it to two popular social networking sites, Twitter and
Facebook. Our approach uses a composition of statistical
modeling and anomaly detection to identify accounts that
experience a sudden change in behavior. Since behavior
changes can also be due to benign reasons (e.g., a user
could switch her preferred client application or post up-
dates at an unusual time), it is necessary to derive a way to
distinguish between malicious and legitimate changes. To
this end, we look for groups of accounts that all experience
similar changes within a short period of time, assuming
that these changes are the result of a malicious campaign
that is unfolding. We developed a tool, called COMPA, that
implements our approach, and we ran it on a large-scale
dataset of more than 1.4 billion publicly-available Twitter
messages, as well as on a dataset of 106 million Facebook
messages. COMPA was able to identify compromised ac-
counts on both social networks with high precision.
Online social networks, such as Facebook and Twitter,
have become increasingly popular over the last few years.
People use social networks to stay in touch with family, chat
with friends, and share news. The users of a social network
build, over time, connections with their friends, colleagues,
and, in general, people they consider interesting or trust-
worthy. These connections form a social graph that con-
trols how information spreads in the social network. Typ-
ically, users receive messages published by the users they
are connected to, in the form of wall posts, tweets, or status
updates.
The large user base of these social networks has attracted
the attention of cyber-criminals. According to a study from
2008, 83% of social network users received at least one un-
wanted message on such networks that year [1]. Also, large-
scale malware campaigns have been carried out over so-
cial networks [2], and previous work has shown that spam,
phishing, and malware are real threats on social networking
sites [3, 4].
To address the growing problem of malicious activ-
ity on social networks, researchers have started to pro-
pose different detection and mitigation approaches. Initial
work [5, 6, 7] has focused on the detection of fake accounts
(i.e., automatically created accounts with the sole purpose
of spreading malicious content). Unfortunately, systems
that detect fake accounts do not discriminate between Sybil
and compromised accounts. A compromised account is an
existing, legitimate account that has been taken over by an
attacker. Accounts can be compromised in a number of
ways, for example, by exploiting a cross-site scripting vul-
nerability or by using a phishing scam to steal the user’s
login credentials. Also, bots have been increasingly used
to harvest login information for social networking sites on
infected hosts [8].
While dedicated, malicious accounts are easier to cre-
ate, compromised accounts are more valuable to cyber-
criminals. The reason is that these accounts allow attackers
to leverage the associated account history and network of
trust to spread malicious content more effectively [9]. As
a result, attackers increasingly abuse legitimate accounts to
distribute their malicious messages [3, 4]. To identify cam-
paigns that involve both compromised and fake accounts,
the focus of the analysis has shifted to the messages them-
selves. In particular, researchers have proposed techniques
that search a social network for the presence of similar mes-
sages [10, 3]. The intuition is that attackers send many sim-
ilar messages as part of campaigns. Similarity is typically
deﬁned in terms of overlap in message content or shared
URLs.
Of course, it is not sufﬁcient to simply group similar
messages to detect malicious campaigns. The reason is
that many such clusters (groups) will contain benign mes-
sages, which range from simple “happy birthday” wishes
to template-based notiﬁcations sent by popular applications
such as Foursquare [11]. To distinguish benign from mali-
cious clusters, some systems utilize features that are solely
based on the URLs in messages [3, 12, 13]. Clearly, these
techniques suffer from limited scope, because they can-
not ﬁnd malicious messages that do not contain URLs (we
found instances of such messages during our experimental
evaluation). Other systems [10] consider additional features
such as the size of clusters or the average number of connec-
tions for each user. While this broadens their coverage to
include campaigns that do not rely on URLs, the reported
accuracy of less than 80% is rather sobering. Moreover,
and equally important, previous systems can only determine
whether a cluster of messages is malicious. That is, they
cannot distinguish between messages sent by compromised
accounts and those sent by fake accounts. This information
is crucial for social network operators to initiate appropri-
ate mitigation procedures. Speciﬁcally, fake accounts can
be safely deleted without affecting legitimate users. To ad-
dress compromised accounts, however, the social network
provider has to notify the victims, reset their passwords, and
engage the users in a password recovery process.
In this paper, we present a novel approach to detect com-
promised accounts on social networks. Our approach offers
a combination of three salient features. First, it does not de-
pend on the presence of URLs in messages. As a result, we
can detect a broad range of malicious messages, including
scam messages that contain telephone numbers and instant
messaging contacts. Second, our system is accurate and de-
tects compromised accounts with very low false positives.
Third, we focus on ﬁnding compromised accounts and leave
the detection of fake accounts to previous work [5, 6, 7] or
to the social network providers themselves. By identifying
compromised accounts, social network providers can focus
their mitigation efforts on real users.
The core idea underlying our approach is that it is pos-
sible to model the regular activities of individual users. If,
at any point, a user’s account gets compromised, it is likely
that there will be a noticeable change in the account’s be-
havior (and our experiments conﬁrm this assumption). To
capture the past behavior of a user, we introduce a collec-
tion of statistical models, which we call a behavioral proﬁle.
Each of our models corresponds to a characteristic feature
of a message (e.g., the time of the day when it was sent or
the language in which it was written in). These models cap-
ture generic user activity on social networks and are not tied
to a particular platform (as our experiments on Twitter and
Facebook demonstrate). Behavioral proﬁles make it possi-
ble to assess future messages. A message that appears to be
very different from a user’s typical behavior might indicate
a compromise.
Of course, a single message that violates the proﬁle of a
user does not necessarily indicate that this account is com-
promised. The message might be an outlier or merely re-
ﬂect a normal change in behavior. For this reason, like in
previous work, our approach looks for other, similar mes-
sages that have recently been posted on the social network
and that also violate the behavioral proﬁles of their respec-
tive users. This means that we cannot detect cases in which
an attacker posts a single, malicious message through one
compromised account. We feel that this is reasonable as at-
tackers typically aim to distribute their malicious messages
to a broader victim population. Moreover, our experiments
demonstrate that we can detect compromised accounts even
in case of small campaigns (in our experiments on Twitter,
for example, we require as little as ten similar messages per
hour).
In a nutshell, our approach (i) checks for a set of similar
messages, and (ii) requires that a signiﬁcant subset of these
messages violate the behavioral proﬁles of their senders.
These two steps can be executed in any order: We can check
for messages that violate their respective behavioral proﬁles
ﬁrst and then group those messages into clusters of similar
ones. This would allow us to implement similarity metrics
that are more sophisticated than those presented in previ-
ous work (i.e., simple checks for similar content or URLs).
Alternatively, we can ﬁrst group similar messages and then
check whether a substantial fraction of these messages vio-
lates the corresponding behavioral proﬁles. Using this order
is more efﬁcient as the system has to inspect a smaller num-
ber of messages.
We implemented our approach in a system called
COMPA. Our system can be used by social network opera-
tors to identify compromised accounts and take appropriate
countermeasures, such as deleting the offending messages
or resetting the passwords of the victims’ accounts. Since
COMPA relies on behavioral patterns of users and not, like
related work, on suspicious message content (URLs [13] or
typical features of Sybil proﬁles [9]) it is able to detect types
of malicious messages that are missed by recently-proposed
techniques. In particular, our approach identiﬁed scam cam-
paigns that lure their victims into calling a phone number,
and hence, the corresponding messages do not contain links
(URLs).
We applied COMPA to two large-scale datasets from
Twitter and Facebook. The Twitter dataset consists of mes-
sages we collected from May 13, 2011 to August 12, 2011,
while the Facebook dataset contains messages ranging from
September 2007 to July 2009. Our results show that COMPA
is effective in detecting compromised accounts with very
few false positives. In particular, we detected 383,613 com-
promised accounts on Twitter, by analyzing three months
of data consisting of over 1.4 billion tweets. Further-
more, COMPA detected 11,087 compromised accounts on
Facebook, by analyzing 106 million messages posted by
users in several large geographic networks.
This paper makes the following contributions:
- We are the ﬁrst to introduce an approach that fo-
cuses on detecting compromised accounts on social
networks. This provides crucial input to social net-
work providers to initiate proper mitigation efforts.
- We propose a novel set of features to characterize reg-
ular user activity based on the stream of messages that
each user posts. We use these features to create mod-
els that identify messages that appear anomalous with
respect to a user’s account (message) history.
- We demonstrate that our approach is able to effectively
detect compromised accounts with very low false pos-
itives. To this end, we applied our approach to two
large-scale datasets obtained from two large social net-
working sites (Twitter and Facebook).
2 Behavioral Proﬁles
A behavioral proﬁle leverages historical
information
about the activities of a social network user to capture this
user’s normal (expected) behavior. To build behavioral pro-
ﬁles, our system focuses on the stream of messages that a
user has posted on the social network. Of course, other fea-
tures such as proﬁle pictures or friend activity could be use-
ful as well. Unfortunately, social networks typically do not
offer a way to retrieve historical data about changes in these
features, and therefore, we were unable to use them.
A behavioral proﬁle for a user U is built in the following
way: Initially, our system obtains the stream of messages
of U from the social networking site. The message stream
is a list of all messages that the user has posted on the so-
cial network, in chronological order. For different social
networks, the message streams are collected in slightly dif-
ferent ways. For example, on Twitter, the message stream
corresponds to a user’s public timeline. For Facebook, the
message stream contains the posts a user wrote on her own
wall, but it also includes the messages that this user has
posted on her friends’ walls.
To be able to build a comprehensive proﬁle, the stream
needs to contain a minimum amount of messages.
Intu-
itively, a good behavioral proﬁle has to capture the breadth
and variety of ways in which a person uses her social
network account (e.g., different client applications or lan-
guages). Otherwise, an incomplete proﬁle might incorrectly
classify legitimate user activity as anomalous. Therefore,
we do not create behavioral proﬁles for accounts whose
stream consists of less than a minimum number S of mes-
sages. In our experiments, we empirically determined that
a stream consisting of less than S = 10 messages does
usually not contain enough variety to build a representative
behavioral proﬁle for the corresponding account. Further-
more, proﬁles that contain less then S messages pose a lim-
ited threat to the social network or its users. This is because
such accounts are either new or very inactive and thus, their
contribution to large scale campaigns is limited. A detailed
discussion of this threshold is provided in Section 6.
Once our system has obtained the message stream for a
user, we use this information to build the corresponding be-
havioral proﬁle. More precisely, the system extracts a set of
feature values from each message, and then, for each fea-
ture, trains a statistical model. Each of these models cap-
tures a characteristic feature of a message, such as the time
the message was sent, or the application that was used to
generate it. The features used by these models, as well as
the models themselves, are described later in this section.
Given the behavioral proﬁle for a user, we can assess to
what extent a new message corresponds to the expected be-
havior. To this end, we compute the anomaly score for a
message with regard to the user’s established proﬁle. The
anomaly score is computed by extracting the feature val-
ues for the new message, and then comparing these feature
values to the corresponding feature models. Each model
produces a score (real value) in the interval [0, 1], where
0 denotes perfectly normal (for the feature under consider-
ation) and 1 indicates that the feature is highly anomalous.
The anomaly score for a message is then calculated by com-
posing the results for all individual models.
2.1 Modeling Message Characteristics
Our approach models the following seven features when
building a behavioral proﬁle.
Time (hour of day). This model captures the hour(s) of