The server cannot be trusted to protect the privacy of clients:
the server might attempt to recover the path of a given client and
release this information to third parties without a client’s consent
or knowledge (e.g., for advertising), or hackers could break into the
server and steal location data about particular users. These attacks
can use any side information. However, the server is trusted to
compute the aggregate statistics correctly: this is its goal.
Clients are also untrusted: they may attempt to bias the aggregate
result (e.g., to convince a server that the road next to their house
is crowded and no more trafﬁc should come). There are two such
potential threats: uploading a large number of samples or uploading
samples with out-of-range samples, both of which could change an
aggregate result signiﬁcantly. However, each client is allowed to
upload within a server-imposed quota and an acceptable interval
of sample values; checking that the sample uploaded is precisely
correct is out-of-scope. (See §9.2 for more details).
The smoothing module (SM) can misbehave by trying to change
aggregate results or colluding with clients to help them bias statistics;
PrivStats prevents such behavior. To guarantee full side information
protection, the requirement on the SM is that it does not leak timing
or decrypted samples.
!"#$%&'(
*567,$%&'(()%*+,$%-./812$%/9214%
)*(
-%.%/0#1#%2(
%$&3.+4(
)$+,$+(
5678906(:/0$%;3?)%1@""AB%
565:906(:/0$%;#C'-,/$%'77"=>?)%1@""AB%
(5658906(:/0$%;DD/E?)%1@""AB%
!"#$%&'(()%*+,$%-./012$%-3214%
Figure 1: Architecture of PrivStats.
3. SIDE INFORMATION AND LOCATION
PRIVACY
Side information can leak information about client paths even
when the server receives tuples without identities. As already dis-
cussed, the number of tuples in areas of low density can leak privacy.
Krumm [24] as well as Gruteser and Hoh [17] inferred driver paths
and their identities using SI such as knowledge of map, driving
patterns, upload timing, and a public web service. Driving patterns
coupled with sample values can also leak private information. For
example, if the server knows that client Bob is a habitual speeder,
tracking uploads with high speed values will likely indicate his path.
Other two interesting examples of SI are physical observation and
information about someone else’s path. For an example of the latter,
assume that the server knows that Alice and Bob both went on street
A and then it sees one upload from street B1 and one upload from
B2, which follow directly from A. Knowing that Bob’s house is on
B1, it can infer that Alice went on B2. To quantify how much pri-
vacy is leaked, Shokri et al. [38] offer a framework for quantifying
privacy leakage in various location privacy schemes that also takes
into account side information.
We now provide a deﬁnition of location privacy that is resilient
to arbitrary side information. SI can come in different forms and
reveal varying degrees of information, making it challenging to pro-
vide theoretical guarantees. For example, one can never guarantee
that the path of a client between point A and point B will remain
unknown because the client may simply be observed physically at
some location. Moreover, one cannot prevent the adversary from
having SI: such SI can come from a variety of out-of-bound sources
(public databases, physical observation, collusion with clients, per-
sonal knowledge of a driver’s trends, etc.). Instead, the idea is that
the protocol should not reveal any additional information about
client paths beyond what the server already knows and the desired
aggregate result. In short, the protocol should reveal nothing else
other than the aggregate result.
Consider the following example. Alice, Bob, and some other
clients volunteer to participate in an average speed computation on
street S1 for time interval 7 am to 9 am. Now suppose that the server
has the following side information about the clients: on the days
that Alice goes to work, she leaves home on street S1 at 8:15 am;
also, Bob is a speeder and tends to drive signiﬁcantly faster than the
average driver. If the protocol satisﬁes our deﬁnition, the server will
learn the average speed, say 30 mph, and nothing else. In particular,
the server will not learn how many people passed through the street
or whether there was an upload at 8:15 am (and hence whether
Alice went to work). Moreover, the server will not see the individual
speed values uploaded so it cannot determine if there were some
high speeds corresponding to Bob.
655I. System setup (Runs once, when the system starts).
1: Server generates public and private keys for accountability by running System setup from §6.
2: Both Server and SM generate public and private keys for the aggregation protocols by running System setup from §5.
II. Client join (Runs once per client, when a client signs up to the system).
1: Client identiﬁes herself and obtains public keys and capabilities to upload data for accountability (by running Client join, §6) from
Server and public keys for aggregation (by running Client join, §5) from SM. Server also informs Client of the aggregates Server
wants to compute, and Client decides to which aggregates she wants to contribute (i.e., generate tuples).
III. Client uploads for aggregate id (Runs when a client generates a tuple id, sample)
1: Client runs SLP’s Upload protocol (§5) by communicating with the SM for each id, sample and produces a set of transformed tuples
2: For each i, Client uploads Ti to Server at time ti.
3: Client also proves to Server using the Accountability upload protocol, §6, that the upload is within the permissible quotas and
T1, . . . ,T k and a set of times, t1, . . . , tk, when each should be uploaded to Server.
acceptable sample interval.
IV. Server computes aggregate result id (Runs at the end of an aggregate’s time interval).
1: Server puts together all the tuples for the same id and aggregates them by running Aggregation with SM, §5.
Figure 2: Overview of PrivStats.
For a security parameter k and an aggregate function F (which
can also be a collection of aggregate functions), consider a protocol
P = PF (k) for computing F . For example, the SLP protocol (§5) is
such a protocol P where F can be any of the aggregates discussed in
§7. Let R be a collection of raw tuples generated by users in some
time period; that is, R is a set of tuples of the form id, sample
together with the precise time, client network information, and
client id when they are generated. R is therefore the private data
containing the paths of all users and should be hidden from the
server. We refer to R as a raw-tuple conﬁguration. Let SI be some
side information available at the server about R. Using P, clients
transform the tuples in R before uploading them to the server. (The
sizes of R, F , and SI are assumed to be polynomial in k.)
The following deﬁnition characterizes the information available at
the server when running protocol P. Since we consider a real system,
the server observes the timing and network origin of the packets it
receives; a privacy deﬁnition should take these into account.
DEF. 1
(SERVER’S VIEW.). The view of the server, ViewP(R),
is all the tuples the server receives from clients or SM in P associated
with time of receipt and any packet network information, when
clients generate raw tuples R and run protocol P.
Let D be the domain for all raw-tuple conﬁgurations R. Let res be
some aggregate result. Let RF (res) = {R ∈ D : F (R) = res} be
all possible collections of raw tuples where the associated aggregate
result is res.
Security game. We describe the SLP deﬁnition using a game be-
tween a challenger and an adversary. The adversary is a party that
would get access to all the information the server gets. Consider a
protocol P, a security parameter k, a raw-tuple domain D, and an
adversary Adv.
1: The challenger sets up P by choosing any secret and public keys
required by P with security parameter k and sends the public keys
to Adv.
ing based on SI) and sends R0 and R1 to the challenger.
2: Adv chooses SI, res, and R0, R1 ∈ RF (res) (to facilitate guess-
3: Challenger runs P producing ViewP(R0) and ViewP(R1), and
sends them to Adv. It chooses a fair random bit b and sends
ViewP(Rb) to Adv. (ViewP usually contains probabilistic encryp-
tion so two encryptions of Rb will lead to different values.)
4: The adversary outputs its best guess for b∗ and wins this game if
b = b∗. Let winP(Adv, k) := Pr[b = b∗] be the probability that
Adv wins this game.
DEF. 2
(STRICT LOCATION PRIVACY – SLP). A protocol P
has strict location privacy with respect to a raw-tuple domain, D,
if, for any polynomial-time adversary Adv, winP(Adv, k) ≤ 1/2 +
negligible function of k.
Intuitively, this deﬁnition says that, when the server examines the
data it receives and any side information, all possible conﬁgurations
of client paths having the same aggregate result are equally likely.
Therefore, the server learns nothing new beyond the aggregate result.
Strict location privacy and differential privacy. The guarantee
of strict location privacy is complementary to those of differential
privacy [11], and these two approaches address different models. In
SLP, the server is untrusted and all that it learns is the aggregate
result. In a differential privacy setting, the server is trusted and
knows all private information of the clients, clients issue queries to
the database at the server, and clients only learn aggregate results
that do not reveal individual tuples. Of course, allowing the server
to know all clients’ private path information is unacceptable in our
setting. PrivStats’ model takes the ﬁrst and most important step for
privacy: hiding the actual paths of the users from the server. Actual
paths leak much more than common aggregate results in practice.
A natural question is whether one can add differential privacy on
top of PrivStats to reduce leakage from the aggregate result, while
retaining the guarantees of PrivStats, which we discuss in §8.
4. OVERVIEW
PrivStats consists of running an aggregation protocol and an ac-
countability protocol. The aggregation protocol (§5) achieves our
strict location privacy (SLP) deﬁnition, Def. 2, and leaks virtu-
ally nothing about the clients other than the aggregate result. The
accountability protocol (§6) enables the server to check three prop-
erties of each client’s upload without learning anything about the
identity of the client: the client did not exceed a server-imposed
656quota of uploads for each sample point, did not exceed a total quota
of uploads over all sample points, and the sample uploaded is in an
acceptable interval of values.
Figure 1 illustrates the interaction of the three components in
our system on an example: computation of aggregate statistic with
id 14, average speed. The ﬁgure shows Alice and Bob generating
data when passing through the corresponding sample point and then
contacting the SM to know how many tuples to upload. As we
explain in §5, we can see that the data that reaches the server is
anonymized, contains encrypted speeds, arrives at random times
independent of the time of generation, and is accompanied by ac-
countability proofs to prevent malicious uploads. Moreover, the
number of tuples arriving is uncorrelated with the real number.
Figure 2 provides an overview of our protocols, with components
elaborated in the upcoming sections.
5. AGGREGATION: THE SLP (STRICT
LOCATION PRIVACY) PROTOCOL
A protocol with strict location privacy must hide all ﬁve leakage
vectors (included in Def. 1): client identiﬁer, network origin of
packet, time of upload, sample value, and number of tuples gen-
erated for each aggregate (i.e., the number of clients passing by
a sample point). The need to hide the ﬁrst two is evident and we
already discussed the last three vectors in §3.
Hiding the identiﬁer and network origin. As discussed in §2,
clients never upload their identities (which is possible due to our
accountability protocol described in §6). We hide the network origin
using an anonymizing network as discussed in §2.1.
Hiding the sample. Clients encrypt their samples using a (seman-
tically secure) homomorphic encryption scheme. Various schemes
can be used, depending on the aggregate to be computed; Pail-
lier [30] is our running example and it already enables most common
aggregates. Paillier has the property that E(a) · E(b) =E (a + b),
where E(a) denotes encryption of a under the public key, and the
multiplication and addition are performed in appropriate groups.
Using this setup, the server computes the desired aggregate on en-
crypted data; the only decrypted value the server sees is the ﬁnal
aggregate result (due to the SM, as described later in this section).
It is important for the encryption scheme to be veriﬁable to pre-
vent the SM from corrupting the decrypted aggregate result. That
is, given the public key PK and a ciphertext E(a), when given a
and some randomness r, the server can verify that E(a) was indeed
an encryption of a using r, and there is no b such that E(a) could
have been an encryption of b for some randomness. Also, the holder
of the secret key should be able to compute r efﬁciently. Fortu-
nately, Paillier has this property because it is a trapdoor permutation;
computing r involves one exponentiation [30].
Hiding the number of tuples. The server needs to receive a number
of tuples that is independent of the actual number of tuples generated.
The idea is to arrange that the clients will upload in total a constant
number of tuples, Uid, for each aggregate id. Uid is a publicly-known
value, usually an upper bound on the number of clients generating
tuples, and computed based on historical estimates of how many
clients participate in the aggregate. §9 explains how to choose Uid.
The difﬁculty with uploading Uid in total is that clients do not
know how many other clients pass through the same sample point,
and any synchronization point may itself become a point of leakage
for the number of clients.
Assuming that there are no trusted parties (i.e., everyone tries
to learn the number of tuples generated), under a reasonable for-
malization of our speciﬁc practical setting, the problem of hiding
the number of tuples can be reduced to a simple distributed algo-
C contacts SM to sync. 
at random timing 
(1) 
(3) 
(2) 
C passes by sample 
point, generates tuple 
C uploads a real and a junk 
tuple at S at random timing 
time 
Figure 3: Staged randomized timing upload for a client C: (1) is the
generation interval, (2) the synchronization interval, and (3) the upload
interval.
rithms problem which is shown to be impossible (as presented in
a longer version of our paper at http://nms.csail.mit.edu/
projects/privacy/). For this reason, we use a party called the
smoothing module (SM) that clients use to synchronize and upload
a constant number of tuples at each aggregate. The SM will also
perform the ﬁnal decryption of the aggregate result. The trust re-
quirement on the SM is that it does not decrypt more than one value
for the server per aggregate and it does not leak the actual number
of tuples. Although the impossibility result provides evidence that
some form of trust is needed for SLP, we do not claim that hiding
the number of tuples would remain impossible if one weakened the
privacy guarantees or altered the model. However, we think our
trust assumption is reasonable, as we explain in §8: a malicious
SM has limited effect on privacy (it does not see the timing, origin,
and identiﬁer of client tuples) and virtually no effect on aggregate
results (it cannot decrypt an incorrect value or affect accountability).
Moreover, the SM has light load, and can be distributed on clients
ensuring that most aggregates have our full guarantees. For simplic-
ity, we describe the SLP protocol with one SM and subsequently
explain how to distribute it on clients.
Since the number of real tuples generated may be less than Uid,
we will arrange to have clients occasionally upload junk tuples:
tuples that are indistinguishable from legitimate tuples because the
encryption scheme is probabilistic. Using the SM, clients can ﬁgure
out how many junk tuples they should upload. The value of the junk
tuples is a neutral value for the aggregate to be computed (e.g., zero
for summations), as we explain in §7.
To summarize, the SM needs to perform only two simple tasks: