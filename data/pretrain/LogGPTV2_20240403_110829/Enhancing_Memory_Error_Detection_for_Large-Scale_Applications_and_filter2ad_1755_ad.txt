capability of MEDS is arguably a super set of ASAN.
Juliet Test Suite. NIST provides the Juliet test suite [6], which
is developed for testing the effectiveness of software assurance
tools. Every test case has two versions, a call to a bad function
with a vulnerability (so as to measure false negatives) and a call
to a good function that have patched the vulnerability (so as to
measure false positives). We particularly focused our testing on
memory corruption related testcases in Juliet, a total of 11,414
testcases: 3,124 tests for Stack buffer overflow (CWE 121), 3,870
tests for Heap buffer overflow (CWE 122), 1,168 tests for buffer
underwrite (CWE 124), 870 tests for buffer overread (CWE 126),
1,168 tests for buffer underread(CWE 127), 820 tests for double
free (CWE 415), and 394 tests use-after-free (CWE 416). We
compiled all of these testcases using MEDS as well as ASAN,
and measured false positives and false negatives in terms of
detectability. In most of time, both MEDS and ASAN showed
0 false positives and false negatives. However, sometimes both
showed false negatives on the testcases, ranging from zero to
288. We analyzed the details of those false negative cases and
found that these test cases involve random memory access (i.e.,
an access address is computed through rand function seeded
by time). In other words, these random accesses may skip over
the redzone size enforced by both schemes, resulting in false
negatives.
In order to better compare the detection capability and
understand its practical implications with respect to this random
access, we modified those 288 cases with the following
constraints: (1) allocating a thousand more objects (currently
Juliet tests only allocate one object for each test) and (2) limit
the random access within the range of stack/heap segments. The
first constraint takes account of practical running environments,
where most real-world programs allocate a huge number of
memory objects at runtime. The second constraint considers
general programming practice — in practice a pointer value is
mostly deduced from an address of existing objects. We applied
these changes to 288 testcases, and ran a million times to
measure the detection probability. Our result shows that MEDS
detected 98% of those while ASAN detected 35% of those.
Although this modification on Juliet test is arguably in favor of
MEDS, we still believe this outstanding detection probability
of MEDS demonstrates enough its significant improvement in
detection capability over ASAN.
Detecting Real-world Memory Errors. To better understand
App.
CVE
Chrome
Chrome
Chrome
Firefox
2016-1653
2016-5182
2016-5184
2016-2798
Types
S, W
S, W
T, RW
S, R
Detection
ASAN MEDS
△
△
△
△
✓
▲
▲
▲
TABLE I: Detection capability of MEDS on real-world vulnerabilities:
S - spatial memory errors; T - temporal memory errors; W - a write
violation; R - a read violation; ✓ - detected; ▲ - partially detected
(difficult to bypass); △ - partially detected (easy to bypass)
whether MEDS can truly detect memory errors in realistic use-
cases, we launched memory corruption attacks against a set of
vulnerabilities in popular applications including Chrome and
Firefox. For each vulnerability, we first rolled back the target
application’s source code to the vulnerable version, and then
used both ASAN and MEDS to build the application to compare
the detection capability.
As shown in Table I, MEDS enhanced its detection capability
in Chrome and Firefox over ASAN, both in spatial and temporal
memory errors. In fact, the table demonstrates not only the
effectiveness of MEDS’s approximated infinite gap and heap
but also its limitation. In the case of CVE-2016-1653, since the
vulnerability offers a limited range of violation access less than
4 MB (i.e., less than MEDS’s redzone size), MEDS was able to
fully detect it whereas ASAN was not. However, for the rest
three cases, because these offer full control over the pointer (i.e.,
complete arbitrary memory read or write vulnerability), MEDS
was also bypassed as ASAN did. We still note that bypassing
MEDS is more difficult than ASAN, thus those are marked as
▲ in MEDS and △ in ASAN, respectively.
Effectiveness of Approximation. MEDS elevates the level of
detection capability by approximating the infinite gap and heap,
but apparently there should be a certain upper bound due to the
limited memory resources. Thus, we study practical impacts of
those limits in terms of detection capability. In particular, an
offset size of memory accesses directly impacts the effectiveness
of redzone based detection. In fact, this offset size is strongly
related to the allocated object size, because intermediate pointer
arithmetic only involves in shifting a pointer within the same
object. Therefore, the possible difference caused by pointer
arithmetic is mostly smaller than the associated object size.
Thus, we measured the size of each allocation across all the
applications under our evaluation, and found that all objects
were smaller than 4 MB, which implicates 4 MB redzone would
provide reasonably good detection capability. This measurement
also shows the limitation of ASAN, as 11% of objects were
bigger than 256 Bytes (i.e., the default redzone size of ASAN)
and accessing routines onto those 11% of objects may be
abused to bypass a redzone size. As noted before in §IV,
enlarging this parameter in ASAN is not suitable for large-scale
applications due to out-of-memory issues. It is worth noting
that MEDS can be further augmented through strictly restricting
the pointer arithmetic up to the maximum object size (i.e., 4 MB
in these applications). This will enable MEDS to truly achieve
the infinite gap.
MEDS also approximates the infinite heap by cycling
10
App.
Chrome
Firefox
Apache
Nginx
Recycle frequency (min)
HSG
15.1
32.0
95.7
0.5
H
50.3
160.7
141.2
4.5
HS
15.2
32.1
95.7
0.5
TABLE II: The frequency of virtual address recycling on MEDS: H
- aliasing heap objects; HS - aliasing heap and stack objects; HSG -
aliasing all objects including heap, stack, and global. Note that first
virtual address reuse on ASAN is done very quickly at initialization
in case of Chrome and Firefox.
Buffer overflow
Use-after-free
ASAN MEDS
Improv
ASAN MEDS
Improv
631.85
51.64
12.23x
985.10
86.02
11.45x
8.76
27.74
3.16x
7.48
20.85
2.78x
First crash
time (s)
Crashes per an
hour
TABLE III: Detection performance of MEDS and ASAN with micro-
benchmarks.
through 64-bits of virtual memory space. More precisely, MEDS
alone cannot fully use such 64-bits space, but it currently
utilizes 80 TB virtual memory space— given the total of 47-
bits user-land virtual address space in x86 (total 128 TB), it
reserves 16 TB for shadow memory, another 16 TB is reserved
for internal memory allocations for MEDS, and yet another
16 TB is reserved for Linux stack. Therefore, since MEDS starts
reusing virtual memory space after allocating objects over
80 TB, we try to project a time taken to trigger this action from
the end-user perspective. Specifically, we ran MEDS applied
versions of Chrome and Firefox, which visited websites every 5
minutes using the same tab; and ran those of Apache and Nginx
which serves 25,000 requests (with concurrency level 50) per
a second. According to our running results (Table II), Chrome,
Firefox, and Apache started to reuse the address space after 49
minutes, 160 minutes, and 141 minutes, respectively. We believe
this is a reasonably long enough time, not interfering end-user
experiences, especially when considering that most users would
frequently close and create new tabs. Nginx quickly drained
the virtual address space though—it only took 4 minutes until
the recycle. We suspect this is because Nginx is designed to
repeat heavy memory reallocations. Although this would not
be an ideal, in this case the Nginx process can be re-spawned
frequently before reaching this virtual address recycling time.
C. Detectability in Fuzz Testing
In order to demonstrate MEDS’s effectiveness in detecting
memory errors while performing fuzz testing, we run a
fuzz testing using both micro-benchmarks and real-world
applications. We used American Fuzzy Lop (AFL) as a fuzzing
framework [38], which is one of the most popular fuzzers in
practice. Target programs were first instrumented using AFL
to enable its feedback based fuzzing functionality, and further
instrumented with either MEDS and ASAN to compare the
detection capability.
Fuzzing Micro-benchmark Programs.
In this evaluation,
we developed and tested two simple yet realistic vulnerable
11
programs, exhibiting buffer overflow or use-after-free vulnera-
bilities, respectively. These testing programs were written to
highlight the effectiveness of MEDS, especially in terms of
non-linear memory violation cases (i.e., beyond the size of
a redzone) and temporal violation cases with heavy memory
allocations (i.e., beyond the size of a quarantine zone). Thus,
these may not represent general detection capability of all
memory error cases. However, since these vulnerable codes
were taken and simplified from real-world vulnerabilities, we
believe this testing still has practical implications in terms of
memory error detection, which we will further showcase using
real-world applications.
The first case on buffer overflow vulnerability is caused by
an integer overflow on allocation size. It takes width and height
of the canvas and allocates the canvas. After that, the program
takes offset, size, and data to write on the canvas. There is an
integer overflow when computing the canvas size. The second
case has a use-after-free vulnerability. Initially, it has a set
of pointers, each of which points to a heap object. Then the
program takes an integer value k, which frees k number of
objects. After freeing the objects, it allocates new objects more
than freed objects, and attempts to access one of the pointers
that were pointing to heap.
We have performed 10 times for the micro-benchmarks and
Table III shows the average times taken to encounter the first
crash, and average crashes per an hour. MEDS encounters the
first crash 12 times earlier than ASAN in our micro-benchmark.
When running the input for the first crash of MEDS with ASAN,
ASAN usually cannot detect the vulnerability. Also, MEDS has
3 times higher average crashes than ASAN during fuzzing. The
result shows MEDS can find a target vulnerability faster than
ASAN. In other words, MEDS is effective in terms of detecting
performance.
Fuzzing Real-world Programs.
To clearly demonstrate
practical aspects of MEDS in augmenting fuzz testing capability,
we also run AFL using real-world programs. Table IV shows the
results while fuzzing each program for six hours. We collected a
set of target applications from GitHub and the Debian repository,
where its popularity is implicated by either the popularity
pair (the number of forks and the number of stars in GitHub)
and the installation ranking (among 26,762 applications in
Debian repositories), respectively. The applications are all
recent versions so that bugs found from this test are all
new bugs, and we are already contacting the corresponding
development community to report these issues. The complexity
of applications are represented in terms of the lines of code
(LoC). The total number of executions denotes the number of
executed instances during six hours of the fuzz testing. Since
the same memory error can be triggered through many different
inputs, AFL only keeps the crash exhibiting unique execution
paths, which is called a unique crash.
Overall MEDS outperformed ASAN in augmenting memory
error detection capability of fuzzing for all target applications
we run, in terms of the total number of unique crashes—
68.3% improvements on average, ranging from 1% to 256%.
In fact, these results are particularly interesting because MEDS
is no better than ASAN in terms of execution speeds (although
sometimes MEDS is faster than ASAN), as it is highly depending
on application’s runtime characteristics (i.e., memory allocation
App.
Description
Popularity
Complexity
Total execs (K)
Total unique crashes
Unique crahses per 1M execs
GitHubα
Debianβ
(LoC)
ASAN MEDS
ASAN MEDS
Improv
ASAN
MEDS
Improv
PH7
lci
picoc
ImageMagick
wren
espruino
tinyvm
raptor
swftools
exifprobe
metacam
jhead
PHP interpreter
LCODE interpreter
C interpreter
Image tool
Sciprt language
JS interpreter
Tiny virtual macine
RDF format parser
Tools for SWF files
Probe EXIF files
Probe EXIF files
Image tool
(35, 321)
(61, 355)
(161, 1240)
(212, 933)
(190, 1991)
(359, 1157)
(123, 1154)
-
-
-
-
-
-
-
-
-
-
-
-
699
6,476
6,512
8,355
4,010
43K
50K
68K
622K