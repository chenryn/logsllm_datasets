### 6.1 Effectiveness

#### Regular User Systems
In the first experiment, we collected audit logs from the machines of five different users, all using the same system image. Each log was generated over a single day, and the users exhibited varying usage patterns:

- **User1**: A software developer who frequently used Vim and compilers, and also downloaded and installed several tools.
- **User2**: Ran a web server and a public FTP server.
- **User3**: Primarily used Firefox for web surfing and Xpdf for viewing PDF files, along with Pidgin for chatting.
- **User4**: Watched movies using Mplayer and listened to music with Audacious.
- **User5**: Used the system in console mode, relying on text-based applications like W3m and Pine.

We applied LogGC at the end of the one-day execution. The results are summarized in Table 5. The third column shows the total number of log entries in each original audit log. Columns 4 and 5 present the reduced log size (and percentage) when applying the basic GC algorithm (Section 2), excluding dead-end event collection to support forward analysis (Section 3). The results for the first five rows indicate that the basic algorithm is not very effective, reducing logs by only 35-40% for User1 and User5, due to their use of short-running applications, which have fewer false dependencies.

Columns 6 and 7 show the log reduction when using execution partitioning for long-running applications, in addition to the basic algorithm. This approach reduced an average of 60.45% of the audit logs. In columns 8 and 9, we further garbage-collected temporary file deletions, enabling the collection of dependent events (Section 3). LogGC reduced an average of 76.25% of the original log entries. Columns 10 and 11 show the removal of redundant entries captured in application logs (Section 5), resulting in a 91.49% reduction of the original logs. The last two columns show the log reduction when using data unit partitioning (Section 4.1), which was particularly effective for User2, reducing half of the remaining entries. Overall, the average size of the remaining logs was only 7.11% of the original size, indicating a significant reduction.

#### Server Systems
In the second experiment, we evaluated LogGC on server system logs using RUBiS, an auction service similar to eBay. We set up the auction site with Apache as the frontend and MySQL as the backend, both on the same machine. The database contained 33,721 items in 20 categories. We used a client emulator to simulate user activity, including 27 pre-defined transitions such as user registration, item registration, browsing, bidding, buying, and leaving comments. We used all five different setups provided by RUBiS, each emulating 240 users performing 60,000 to 70,000 transitions over 20 to 30 minutes. We then applied LogGC to the audit logs generated after each execution.

The "Web server benchmark" rows in Table 5 show the results. Most GC strategies were less effective on server systems compared to user systems. With "execution partitioning," "temp file," and "application log" strategies, we could only reduce the logs by 10% (columns 10 and 11). This is because the executions heavily accessed the index and data files in the database, making most execution units reachable through file-level dependencies. However, with data unit partitioning (columns 12 and 13), only 2.65% of the log entries needed to be preserved, resulting in a 37-fold reduction.

#### Per Application Results
We also analyzed the results for individual applications, aggregated from the user system and server executions. Some applications, such as Mplayer and Xpdf, had all their events completely garbage-collected. Mplayer and Xpdf do not affect future system execution, while Yafc, W3m, and Wget retained all their events due to file downloads and retention.

### 6.2 Performance

Table 6 summarizes the performance of LogGC. The experiments were conducted on an Intel Core i7-3770 CPU with 4GB memory running Linux 2.6.35. The execution time of LogGC is divided into log parsing and GC time. LogGC efficiently processes 3GB of logs in about 2 minutes, with parsing time being the dominant factor.

Table 7 shows the runtime overhead incurred by data unit instrumentation using two popular benchmarks: RUBiS and SysBench. The results indicate that the runtime overhead is very low.

| Log       | Parsing Time (s) |
|-----------|------------------|
| User1     |                  |
| User2     |                  |
| User3     |                  |
| User4     |                  |
| User5     |                  |
| Rubis1    |                  |
| Rubis2    |                  |
| Rubis3    |                  |
| Rubis4    |                  |
| Rubis5    |                  |

(Note: The specific parsing times for each user and Rubis instance should be filled in based on the actual experimental data.)