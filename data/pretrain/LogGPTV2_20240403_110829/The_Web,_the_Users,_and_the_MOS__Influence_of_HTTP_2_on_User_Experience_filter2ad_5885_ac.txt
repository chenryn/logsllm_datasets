(iii) there is a slight yet noticeable bias towards negative ΔMOS, where MOS|H1
is higher than MOS|H2. That is, contrary to the previous results, the diﬀerence
between H2 and H1 is much more subtle and inconsistent.
This reinforces the need to perform experiments on real-world pages, as oppo-
site to benchmark pages that inﬂate MOS diﬀerences. Results are only partially
surprising. First, pages widely diﬀer (see Fig. 2) and ΔMOS varies according to
The Web, the Users, and the MOS
55
S
O
M
Δ
 1.2
 0.9
 0.6
 0.3
 0
-0.3
-0.6
-0.9
-1.2
H2
H1
Fig. 6. ΔMOS for 10 pages where we contrast unsharded vs sharded versions.
the page being considered, as shown by Fig. 5b (the order of web pages is con-
sistent with Fig. 2). Second, users have a diﬀerent way to “value improvement”,
causing them to report the same score under both protocols, which contributes
to ΔMOS = 0. Third, pages in our catalog are likely optimized for H1. Fourth,
the H1 software has undergone decades of testing and optimization, while H2 is
a relatively new protocol.
5.2 Impact of Page Sharding
We now consider sharding [12], i.e., distributing page content over multiple
domains to exploit server parallelism. This practice helps in overcoming the lim-
itation on the maximum number of connections a browser can establish towards
the same domain. Given H2 beneﬁts of using a single connection to a single
domain [7,15,22], one would expect that unsharding helps in taking advantage
of H2 pipelining features. In our evaluation, we consider 10 of the 25 pages
of the catalog and modify them so to have all the content hosted on a single
domain (i.e., unsharding the content). We then contrast MOS grades to assess
the impact of (un)sharding for H2 and H1 independently.
Figure 6 shows the per-page diﬀerence between the average MOS for the
unsharded and for the sharded content. In formulas, ΔMOS = E[MOS|unsharded]
– E[MOS|sharded]. Pages are sorted increasingly according to ΔMOS for H2.
It is straightforward to notice that the impact of sharding is page-dependent:
there are pages for which the user experience improves when they are served
through the unsharded deployment (ΔMOS > 0), as well as pages suﬀering
from usharding (ΔMOS 50 ms), instead, MOS degrades loosing
0.58 points with respect to the low-RTT scenario. This happens in the case of
H1 too, where high-RTT homogeneous scenarios lead to a loss of about 0.5 MOS
points with respect to both heterogeneous and low-RTT homogeneous scenarios.
Interestingly, H1 in heterogeneous RTT conditions performs much better than
H2 in the same scenario. Similarly to [23], we noticed that macroscopic pages
characteristics are not telling as for user MOS. The performance gap has its roots
in page dependency graph [22], and homogeneous latencies may hide intricate
interactions in such dependencies that arise only under heterogeneous conditions.
6 Objective Metrics on Real Pages
We ﬁnally study the H1 vs H2 diﬀerence using objective metrics (OBJ in short).
As before, we quantify the diﬀerence in accessing the same page over the two
protocols with ΔOBJ = OBJH2 − OBJH1, where OBJ is the Time to the First
Byte (TTFB), the Document Object Model (DOM), or Page Load Time (PLT).
We additionally consider the ObjectIndex, a replacement metric for the SpeedIn-
dex [9] that has been shown to be strongly correlated with the latter [5].
The Web, the Users, and the MOS
57
H1=H2 (57%)
ΔTTFB
median 1ms
H2 better (39%)
H1 better  (4%)
H1=H2 (19%)
ΔDOM
median -16ms
H2 better (55%)
H1 better (26%)
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.20
0.15
0.10
0.05
H1=H2 (16%)
ΔObjectIndex
median -40ms
H2 better (65%)
H1 better (19%)
H1=H2 (16%)
ΔPLT
median -22ms
H2 better (54%)
H1 better (30%)
0.60
0.50
0.40
0.30
0.20
0.10
0.00
0.30
0.25
0.20
0.15
0.10
0.05
0.00
-1000
-500
0
Time [ms]
500
1000
0.00
-1000
-500
0
Time [ms]
500
1000
Fig. 8. Empirical probability mass function of ΔOBJ various objective metrics.
Figure 8 presents the results depicting the histogram of ΔOBJ using bins
of 100 ms. The ﬁgure is annotated with statistics (notice that H2 better than
H1 is represented by the negative semi-plane in this case). All OBJ exhibit an
empirical probability mass function that is similar to that of the ΔMOS grades
(i.e., roughly symmetric, peak close to zero, very low median). In addition, here
ΔOBJ attributes a (slight) advantage to H2, unlike in the ΔMOS case.
Excluding the TTFB, which is known to be not the most appropriate metric
for web pages performance assessment, H2 shows better results than H1 in at
least 54% of tests. That is, H2 speeds up the page loading process and the
time needed to load the DOM, but those improvements are not reﬂected in user
experience that rates H1 and H2 with the same score in 55% of cases (see Fig. 5).
7 Conclusions
This paper presents the ﬁrst study comparing the performance of H2 and H1 in
terms of MOS. We contrast the two protocols using both subjective (i.e., a MOS
corpus of over 4,000 points) and objective metrics using a dedicated testbed.
The emerging picture does not allow the election of a single winner. While
H2 sensibly reduces the PLT on a toy page, ultimately improving the quality
of experience, it is not as eﬀective when serving real-world web pages. Objective
metrics (e.g., DOM, PLT, etc.) show a performance improvement to the advan-
tage of H2 in more than 50% of cases, but they fail to predict users’ MOS that
is reported to be higher in the case of H1.
This highlights the importance of users feedbacks and calls for future research
on new models enhancing the correlation between MOS and QoE metrics.
Acknowledgments. This work has been carried out at LINCS (http://www.lincs.
fr) and beneﬁted from support of NewNet@Paris, Cisco’s Chair “Networks for the
Future” at Telecom ParisTech (http://newnet.telecom-paristech.fr).
58
E. Bocchi et al.
References
1. Allman, M., Paxson, V.: Issues and etiquette concerning use of shared measure-
ment data. In: Proceedings of the 7th ACM SIGCOMM Conference on Internet
Measurement, IMC 2007, pp. 135–140. ACM, New York (2007)
2. Bailey, M., Dittrich, D., Kenneally, E., Maughan, D.: The menlo report. IEEE
Secur. Priv. 10(2), 71–75 (2012)
3. Belshe, M., Peon, R., Thomson, M.: Hypertext transfer protocol version 2
(HTTP/2). In: IETF RFC7540 (2015)
4. Blackburn, B., Varvello, M., Schomp, K., Naylor, D., Finamore, A., Papagiannaki,
K.: Is the Web HTTP/2 yet? In: TMA PhD School (2016)
5. Bocchi, E., De Cicco, L., Rossi, D.: Measuring the quality of experience of web
users. In: Proceedings of ACM SIGCOMM Internet-QoE Workshop (2016)
6. Brutlag, J., Abrams, Z., Meenan, P.: Above the fold time: measuring web page
performance visually. http://conferences.oreilly.com/velocity/velocity-mar2011/
public/schedule/detail/18692. Accessed 15 Sept 2016
7. Butkiewicz, M., Wang, D., Wu, Z., Madhyastha, H.V., Sekar, V.: Klotski: reprior-
itizing web content to improve user experience on mobile devices. In: Proceedings
of USENIX NSDI, pp. 439–453 (2015)
8. Erman, J., Gopalakrishnan, V., Jana, R., Ramakrishnan, K.K.: Towards a
SPDY’Ier Mobile Web? In: Proceedings of ACM CoNEXT, pp. 303–314 (2013)
9. Google Inc. https://sites.google.com/a/webpagetest.org/docs/using-webpagetest/
metrics/speed-index. Accessed 19 Sept 2016
10. Google Inc. QUIC. https://www.chromium.org/quic. Accessed 19 Sept 2016
11. Google Inc. SPDY. https://www.chromium.org/spdy/spdy-whitepaper. Accessed
19 Sept 2016
12. Grigorik, I.: HTTP/2 is here, let’s optimize! http://bit.ly/http2-opt. Accessed 10
Oct 2016
13. International Telecommunication Union. Subjective testing methodology for web
browsing. ITU-T Recommendation P.1501 (2014)
14. Irish, P.: Delivering the goods in under 1000 ms. http://bit.ly/1toUUA7 Accessed
10 Oct 2016
15. Varvello, M., Schomp, K., Naylor, D., Blackburn, J., Finamore, A., Papagiannaki,
K.: Is the web HTTP/2 yet? In: Karagiannis, T., Dimitropoulos, X. (eds.) PAM
2016. LNCS, vol. 9631, pp. 218–232. Springer, Heidelberg (2016). doi:10.1007/
978-3-319-30505-9 17
16. Miller, R.B.: Response time in man-computer conversational transactions. In: Pro-
ceedings of AFIPS Fall Joint Computer Conference, pp. 267–277 (1968)
17. Netravali, R., Sivaraman, A., Das, S., Goyal, A., Winstein, K., Mickens, J.,
Balakrishnan, H.: Mahimahi: accurate record-and-replay for HTTP. In: Proceedings
of USENIX ATC, pp. 417–429 (2015)
18. Nielsen, J.: Response times: the 3 important limits.https://www.nngroup.com/
articles/response-times-3-important-limits/ (1993). Accessed 19 Sept 2016
19. Popa, L., Ghodsi, A., Stoica, I.: HTTP as the narrow waist of the future internet. In:
9th ACM SIGCOMM Workshop on Hot Topics in Networks (2010)
20. Reichl, P., Egger, S., Schatz, R., D’Alconzo, A.: The logarithmic nature of QoE and
the role of the Weber-Fechner law in QoE assessment. In: IEEE ICC (2010)
21. Wang, X.S., Balasubramanian, A., Krishnamurthy, A., Wetherall, D.: How speedy
is SPDY? In: Proceedings of USENIX NSDI, pp. 387–399 (2014)
The Web, the Users, and the MOS
59
22. Wang, X.S., Krishnamurthy, A., Wetherall, D.: Speeding up web page loads with
Shandian. In: Proceedings of USENIX NSDI, pp. 109–122 (2016)
23. Zariﬁs, K., Holland, M., Jain, M., Katz-Bassett, E., Govindan, R.: Modeling
HTTP/2 speed from HTTP/1 traces. In: Karagiannis, T., Dimitropoulos, X. (eds.)
PAM 2016. LNCS, vol. 9631, pp. 233–247. Springer, Heidelberg (2016). doi:10.1007/
978-3-319-30505-9 18