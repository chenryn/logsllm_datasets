title:Poster: Recovering the Input of Neural Networks via Single Shot Side-channel
Attacks
author:Lejla Batina and
Shivam Bhasin and
Dirmanto Jap and
Stjepan Picek
Lejla Batina
PI:EMAIL
Dirmanto Jap
PI:EMAIL
Shivam Bhasin
PI:EMAIL
Stjepan Picek
PI:EMAIL
Poster: Recovering the Input of Neural Networks via Single Shot
Side-channel Attacks
Radboud University, Nijmegen, The Netherlands
Nanyang Technological University, Singapore
Nanyang Technological University, Singapore
Delft University of Technology, Delft, The Netherlands
ABSTRACT
The interplay between machine learning and security is becoming
more prominent. New applications using machine learning also
bring new security risks. Here, we show it is possible to reverse-
engineer the inputs to a neural network with only a single-shot
side-channel measurement assuming the attacker knows the neural
network architecture being used.
CCS CONCEPTS
• Security and privacy → Side-channel analysis and counter-
measures; • Computing methodologies → Neural networks.
KEYWORDS
Neural networks; Side-channel analysis; Input recovery
ACM Reference Format:
Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan Picek. 2019. Poster:
Recovering the Input of Neural Networks via Single Shot Side-channel
Attacks. In 2019 ACM SIGSAC Conference on Computer and Communications
Security (CCS ’19), November 11–15, 2019, London, United Kingdom. ACM,
New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363280
1 INTRODUCTION
Today, neural networks are trained on sometimes sensitive data,
e.g., in medical and automotive applications. If the adversary can
recover those inputs, he/she can violate the privacy of users by
learning some private information. We show that the adversary
can obtain a significant amount of knowledge by simply observing
electromagnetic (EM) emanations while the secret data is being
processed. To this end, we perform horizontal side-channel attacks
to recover otherwise secret inputs to the network. To our best
knowledge, previous works did not consider this direction before.
There are numerous works dealing with the security and privacy
issues of machine learning models but approaching them from dif-
ferent angles. Ateniese et al. investigated various machine learning
classifiers and statistical information obtained from those classi-
fiers to show that one could infer useful information from those
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363280
classifiers [2]. Wei et al. performed the attack on an FPGA-based
convolutional neural network accelerator where they managed to
recover the input image from the collected power traces without
knowing the detailed parameters of the neural network [6]. This
attack is applicable only when the line buffer executes the convolu-
tion operation and requires a substantial number of measurements.
Contrasting, our approach is completely generic in its nature and
works independently of any implementation optimization.
2 THREAT MODEL AND MEASUREMENT
SETUP
The technique of side-channel analysis (SCA) for key recovery
attacks is well known in cryptography. Horizontal Power Analy-
sis (HPA) is a special type of side-channel attack using power (or
electromagnetic emanation) as the source of leakage [4]. While
some other SCAs recover the secret key statistically over multiple
measurements, HPA is a single trace attack exploiting the simi-
larity between operations and/or data within a single trace (and
accordingly single algorithm execution).
Recently, Machine Learning as a Service (MLaaS) emerged as a
rising business model with large companies like Google and Ama-
zon. A variety of services are offered ranging from training in-
frastructure, datasets, pre-trained models, etc. Resource-intensive
datasets are often trained on cloud infrastructures and pre-trained
models can be downloaded on small embedded nodes for field test-
ing. MLaaS is deployed as an API and can be black-box or white-box
based on the provider [1]. Previous works have shown that mak-
ing the API black-box does not necessarily help as model learning
attacks can reveal the hidden internals [5]. Here, we consider an ad-
versary aiming at recovering the sensitive input fed to a previously
trained neural network for classification.
We use the multilayer perceptron (MLP) algorithm since it is an
often used machine learning algorithm in modern applications. We
have no assumption on the type of inputs or its source, as we work
with floating-point numbers. If the inputs are in the form of integers,
the attack becomes easier since we do not need to recover mantissa
bytes and deal with precision. The adversary is only capable of
acquiring side-channel measurements from the device in a passive
manner, without any perturbation with the device operation. The
underlying neural network architecture is public and all the weights
are known. The crucial information for this work are the weights
of the first layer. These weights can be obtained through a side-
channel attack [3] or in settings where a certain neural network is
derived from known networks by e.g., transfer learning.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2657The proposed attack targets the multiplication operation in the
first hidden layer. The main target is the multiplication m = x·w of a
known weight w with a secret input x. Using the Hamming Weight
(HW) model, the adversary correlates the activity of the predicted
output m for all hypothesis of the input x. Pearson’s correlation
can be used to distinguish correct input from the side-channel
measurements. Thus, the attack computes ρ(t, x), for all hypothesis
of the input x, where ρ is the Pearson correlation coefficient and
t is the side-channel measurement. The correct value of the input
x will result in a higher correlation standing out from all other
wrong hypotheses x∗, given enough measurements. Although the
attack concept is known and well tested for attacking cryptographic
algorithms, the method must be adapted for targeting floating-point
numbers in neural networks rather than fixed length integers in
cryptographic algorithm.
A 32-bit floating point number in IEEE 754 format consist of: 1
sign bit (b31), 8 biased exponent bits (b30...b23), and 23 mantissa
(fractional) bits (b22...b0). This can be formulated as:
(−1)b31 × 2(b30 ...b23)2−127 × (1.b22...b0)2.
The trace t is measured when the computed result m is stored back
to the memory, leaking in the HW model, i.e., HW (m). By recover-
ing this representation, it is enough to recover the estimation of the
floating-point value. When attacking floating-point numbers, small
precision errors due to rounding off the intermediate values still re-
sult in useful information. For the attack, we can target the mantissa
multiplication operation directly. In this case, the search space can
either be [0, 232−1] to cover all possible values for all 32 bits (hence,
more computational resources will be required) or we can focus
only on the most significant bits of the mantissa (fewer candidates
but also with smaller precision). Since the sign and exponent bits
are fixed, we can aim to target only a smaller subset of mantissa bits,
focusing only on the most significant bits and assigning the rest to
0. Thus, our search space is now [0, 29+n − 1], where n is a number
of significant bits targeted. The algorithm for recovering one input
value is given in Algorithm 1. It is executed once for every input
value, while the same measurements are re-used. The whole attack
comprises of one online measurement and the number of offline
attacks equals the number of input values.
Still, this attack faces a strong limitation. As x changes from
one measurement (input) to another, information learned from one
measurement cannot be used with another measurement, prevent-
ing any statistical analysis over a set of different inputs. Then, the
adversary is forced to exploit all the measurements from a single
measurement. To perform information exploitation over a single
measurement, we perform a horizontal attack. The weights in the
first hidden layer are all multiplied with the same input x, one after
the other. M multiplications, corresponding to M different weights
(or neurons) in the first hidden layer are isolated. An illustrative
example is shown in Figure 2 where M = 4 traces corresponds to
4 weights from a single trace. A single trace is cut into M smaller
traces, each one corresponding to one multiplication with a known
associated weight. The value of the input is statistically inferred by
applying a Pearson’s correlation on the M smaller traces. Drawing
analogy with cryptographic algorithms, several known plaintexts
(weights in case of an MLP) are processed for a single unknown
key (input x). An input recovery attack was proposed in [6], which
(a) Langer RF-U 5-2 Near-field Elec-
tromagnetic passive Probe.
(b) The complete measurement
setup.
Figure 1: Experimental setup.
Figure 2: Recovery of multiple measurements from one mea-
surement processing elementary operations sequentially.
The measurement setup consists of the device under test (DUT),
sampling oscilloscope, electromagnetic probes, and pre-amplifiers
for the attack. The DUT tested is ARM Cortex-M3. ARM micro-
controllers form a fair share of the current market with a huge
dominance in mobile applications, but also seeing rapid adoption
in markets like IoT, automotive, virtual and augmented reality, etc.
The microcontroller in our experiments, ARM Cortex-M3, is widely
used in commercial products like wearable and hardware wallets.
The side-channel measurements are collected during the execu-
tion of the classification and they are captured using the Lecroy
WaveRunner 610zi oscilloscope. The oscilloscope measurements
are synchronized with the operations by common handshaking
signals like start and stop of computation. To improve the quality
of measurements, we find the ideal position for the probe based on
some cryptographic algorithm running for the benchmark and we
use a 30 dB pre-amplifier. An RF-U 5-2 near-field electromagnetic
(EM) probe from Langer is used to collect the measurements (see
Figure 1a). The setup is depicted in Figure 1b.
For different platforms, the leakage model could change, but this
would not limit our approach and methodology. The Hamming
Weight (HW) is the popular leakage model for ARM Cortex-M3 [3]
and can be easily adapted for a different device.
3 SINGLE SHOT INPUT RECOVERY ATTACK
It can be extremely complex to recover the input by observing out-
puts from a known network. It involves several classifications in
order to solve a system of equations, while some of the functions
might not be invertible, i.e., ReLU activation function. When con-
sidering theoretical attacks, the system of equations can become
unmanageable as the architecture of the network becomes complex.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2658Algorithm 1: Calculate the sorted candidates for an input x.
Require: ARRAY T: set of traces, ARRAY W: set of weights corresponding to input i,
n: precision (up to 23 bit exponent), fF LOAT , fI EEE : function to convert
floating-point to IEEE binary representation and vice versa, HW: function to
calculate the Hamming weight, corr: function to calculate absolute correlation
between two arrays
x = fF LOAT ([k | 023−n])
M = W × x
MHW = HW (fI EEE(M))
# calculate the Hamming weight of the hypothesis
C = corr(MHW , T)
# calculate the correlation between the traces and hypothesis
C.append(max(C))
# get the max correlation value
Ensure: ARRAY I: sorted possible candidates for input x
1: C = []
2: for int k: 0 to 29+n − 1 do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for
12: C = fF LOAT ([C | 023−n])
13: I = sort(C)
14: # Sort the correlation and return descending sorted index.
15: return X;
Figure 3: Correlation comparison between correct and incor-
rect inputs for target value 2.453.
requires multiple traces targeting a line buffer, which is an opti-
mization oriented design choice. Differing from that, our proposed
attack targets the generic multiplication in a single trace setting.
The attack targets the multiplication in the input layer to recover
the secret input. Depending on the microcontroller, the multiplica-
tion can be processed by ALU or dedicated floating-point units. To
avoid the discrepancy of the difference of multiplication operation,
we target the output of multiplication. In other words, we target
the point when the resultant product of target multiplication is
updated in general purpose registers or memory, which happens
irrespective of underlying multiplication implementation. Figure 3
shows the success of attack recovering secret input of 2.453, with
known weights. We consider the attack successful if the recovered
input is correct up to two decimal places (2.45 in this case). ARM
Cortex-M3 has a complex architecture and would result in a lower
SNR on side-channel measurement. Still, the attack is shown to be
practical with 500 neurons. Given a platform, any network with
500 nodes in the input layer is vulnerable to input recovery attacks.
Similarly, setup and number of measurements can be updated for
other targets like FPGA, GPU, etc.
Figure 4: Original (top) and recovered (bottom) images.
We applied the single shot input recovery attack on the publicly
available MNIST database, which consists of images of handwritten
digits. Each image is a matrix of 28× 28 pixels, each pixel ∈ [0, 255].
We normalized the dataset to have floating-point numbers between
0 and 1. Our neural network architecture (with 500 neurons in
the first layer) on ARM Cortex-M3 was trained with the MNIST
database and then tested for classification on test data. The test
dataset consists of 10 000 images. We selected 10 images, with one
image corresponding to each digit ∈ [0, 9]. The input recovery
is performed on a single trace each time, up to 2 decimal places
given the adopted precision error. The original and retrieved images
with an allowed precision error are shown in Figure 4 (results for
[6,9] not shown), demonstrating the effectiveness of the attack. We
emphasize that our attack does not depend on the type of the input
and as such can be applied to any input type.
4 CONCLUSIONS AND FUTURE WORK
We demonstrate a single trace input recovery attack on MLP. The
attack targets the input multiplication of secret input with known
weights, exploited horizontally within a single computation. The
success probability of the attack increases for networks with larger
input layer, which is trending in modern neural network architec-
tures. Obvious countermeasures thwarting side-channel attacks,
like shuffling, hiding or masking can be applied. As countermea-
sures come with overhead, this might not be acceptable for some
applications and motivates research for optimal countermeasures.
ACKNOWLEDGMENT
This research is supported by the Singapore National Research
Foundation under its National Cybersecurity R&D Grant (“Cyber-
Hardware Forensics & Assurance Evaluation R&D Programme”
grant NRF2018–NCR–NCR009–0001.
REFERENCES
[1] Apache PredictionIO. http://predictionio.apache.org/, 2019.
[2] Giuseppe Ateniese, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,
Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter
ones: How to extract meaningful data from machine learning classifiers. Int. J.
Secur. Netw., 10(3):137–150, September 2015.
[4] Christophe Clavier, Benoit Feix, Georges Gagnerot, Mylène Roussellet, and Vincent
In International
[3] Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan Picek. CSI NN: Reverse
engineering of neural network architectures through electromagnetic side channel.
In 28th {USENIX} Security Symposium, pages 515–532, 2019.
Verneuil. Horizontal correlation analysis on exponentiation.
Conference on Information and Communications Security. Springer, 2010.
[5] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
Stealing machine learning models via prediction apis. In 25th {USENIX} Security
Symposium ({USENIX} Security 16), pages 601–618, 2016.
[6] Lingxiao Wei, Yannan Liu, Bo Luo, Yu Li, and Qiang Xu. I know what you see:
Power side-channel attack on convolutional neural network accelerators. CoRR,
abs/1803.05847, 2018.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2659