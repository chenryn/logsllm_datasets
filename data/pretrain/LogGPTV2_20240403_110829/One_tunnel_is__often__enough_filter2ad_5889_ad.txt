cording to the experimental setup shown in Figure 6.
In terms of latency, ARROW adds an RTT overhead of 62%
over the baseline Serval implementation and 37% over UDP. A 2-
hop ARROW path has an overhead of 45% over the RTT latency
of a 1-hop path. Throughput is not affected by adding ARROW
to a 1-hop path. However, adding another ARROW hop impacts
throughput by 10%. This might be due to our switch not being able
to handle the bandwidth requirement.
We conclude that the overheads imposed by ARROW are small
enough to merit the deployment of a wide-area prototype. In a pro-
1http://iperf.sourceforge.net
Source Forward Hop 1 Forward Hop 2 Backward Hop 2 Backward Hop 1 Dest Figure 7: Time to failover for various component failures along
a ARROW path. The plot shows the median (min/max in error
bars) over 5 measurements, each with Zookeeper and without.
duction environment, purpose-built routers would likely be used.
They already support the required features: MPLS tunnels can be
used to forward packets among tunnel segments and Software De-
ﬁned Networking (SDN) technology aids the setup and manage-
ment of tunnel segments.
5.2 Path Failover Latency
For ARROW to provide signiﬁcant beneﬁt versus BGP, it is im-
portant that its own paths are not prone to long failover times itself.
We examine the time taken to failover a complete path for each
endpoint-observable failure along that path within our wide-area
ARROW deployment.
The measured topology involves 3 PoPs (UW, Princeton, and
Stanford), as well as 3 ASes: Home, Transit, and Dest. Home meets
Transit at both UW and Princeton, and Transit meets Dest at Stan-
ford. Using the VICCI cluster nodes, we “deploy” three redundant
routers (one active, and two standby) of each AS at each PoP—six
routers total at each PoP and 18 routers in the entire topology.
We then establish an ARROW path from Home via Transit to
Dest. After the connection is established, we intentionally kill the
software on a router node to simulate a failure. To simulate an edge
router failure, we kill the active Transit ingress router at UW. For
a PoP failure, we kill all routers at UW. Finally, for an ISP failure,
we kill all Transit routers at both UW and Princeton.
Figure 7 shows the measured failover times over 5 measurements
for each of the 3 failure cases, as observed from the endpoint, i.e.
after the endpoint detected there is a failure until the connection is
restored. This involves the execution of the entire failover proto-
col, as described in §3.4. Failover times increase with the scope of
the failure. We also observe high failover times when the consis-
tency service is used. While some consistency service is necessary,
we believe that our choice of consistency service (Zookeeper [12])
could be optimized to remove some or all of this cost.
Edge routers exist within the same PoP and failure detection can
be very aggressive. Hence, this is the shortest failover time of only
340ms (30ms without consistency service). If an entire PoP fails,
none of the 3 edge router replicas are reachable and we have to
re-route to another PoP, which can be far away. This is the case
in our deployment (from UW to Princeton) and thus PoP failovers
take slightly longer, roughly 414ms (128ms without consistency
service). Finally, if an entire ISP fails, we have to probe several of
its PoPs until we determine that the ISP is down. The endpoint then
has to failover to a backup ARROW path. This takes on the order
of 1.8 seconds (1 second without consistency service).
Figure 8: Setup of the BGP outage experiment. We cause a fail-
ure at UW’s upstream link with GATech. BGP will eventually
failover to a path via USC. ARROW can use an alternative path
via WISC.
We conclude that most failures along an ARROW path can be
addressed in a timespan that is hardly noticeable to most users, es-
pecially when router consistency is optimized. Only whole ISP
failures take longer and might be noticeable at the endpoints. Com-
pared to the time it takes for BGP to converge after such failures,
ARROW still compares favorably in all of the cases.
5.3 End-to-end Recovery After BGP Outage
To examine how quickly end-to-end connectivity can be restored
using an ARROW backup path in the event of a BGP outage, we
conduct an experiment on our wide area deployment where we
forcibly take down a BGP link, similar to an experiment conducted
in RON [3]. The experiment setup is shown in Figure 8, where we
have a multi-homed destination at the UW, by announcing a pre-
ﬁx through TP sites located at GATech and USC simultaneously.
Our source is an Emulab [9] node, and its original path to the UW
destination passes through GATech. We also have an ARROW de-
ployment at WISC that is reachable using a preﬁx announced only
through WISC. This provides us with a one-hop ARROW path
through WISC to UW. We then establish a regular UDP connec-
tion from the source node to a destination within the UW preﬁx
and continuously measure its throughput. We conduct this exper-
iment twice, once with a failover using ARROW and once with a
regular BGP failover.
The measured throughput over time is shown for both cases in
Figure 9. 7 seconds into the experiment, we shut down the VPN
tunnel between UW and GATech, resulting in a BGP session fail-
ure and an instantaneous loss of all packets along the path. The
failure eventually causes BGP to withdraw the route through GAT-
ech and converge to the new route through USC (red line). This
outage lasts for nearly 90 seconds until convergence to the new
route takes place. In contrast, the ARROW deployment is able to
detect and recover from the failure within a few hundred millisec-
onds (blue line). The source in this case detects the outage through
a sequence of unacknowledged packets and immediately starts for-
warding packets through the pre-established ARROW path.
We conclude that ARROW allows endpoints to failover an end-
to-end path within a timespan of a typical TCP timeout, which is
hardly noticeable at the endpoint and does not disrupt open connec-
tions, except for a short dip in throughput.
5.4 Simulation Dataset and Methodology
Next, we explore the potential reliability and performance prop-
erties of ARROW deployed at Internet-scale. For this purpose we
examine BGP and ARROW routing on Internet topology. We do
 0 500 1000 1500 2000 2500Edge routerPoPISPTime to failover [ms]With ConsistencyFailover onlyEmulab source UW dest Original BGP path  BGP outage  ARROW ISP GATech    USC    WISC New BGP path ARROW path Figure 9: Time-series showing throughput for a BGP failover
and a ARROW failover for the experiment in Figure 8. The link
fails at 7 seconds. BGP takes 90 seconds before convergence to
a new path. ARROW failover is on the order of hundreds of
milliseconds.
this using two methodologies: 1. we simulate the effects of BGP
routing decisions (§5.5 and §5.6) and 2. we use actual BGP routing
measurements collected by iPlane2 (§5.7, §5.8, and §5.9).
For the ﬁrst method, we acquired the simulator used to eval-
uate Consensus Routing [13], which simulates routing decisions,
BGP protocol control trafﬁc, and link failures. We use the Novem-
ber 2013 CAIDA AS-level connectivity graph [1], gathered from
RouteViews BGP tables [2], to simulate on a realistic Internet topol-
ogy. This dataset has a total of 29,730 ASes and 159,049 unique
AS-level links, annotated with the inferred business relationships
of the linked ASes (customer-provider or peer-peer). The simulator
uses standard route selection and advertisement policies of the In-
ternet, such as “valley-free” export and “hot potato” routing. More
detail about the simulator can be found in [13].
The iPlane dataset used in the second method is built using tracer-
outes from over 200 PlanetLab sites to more than 140,000 preﬁxes
(almost every routable preﬁx on the Internet). The iPlane dataset
also provides IP-to-AS mapping, IP-to-PoP mapping (where each
PoP is a set of routers from a single AS co-located at a given ge-
ographic location), and the RTTs of inter-PoP links. We use the
most recent iPlane snapshot collected in December 2013. This has
a total of 27,075 ASes and 106,621 unique AS-AS links. At the
PoP-level, it has 183,131 PoPs and 1,540,466 PoP-level links.
5.5 Resilience to Link Failures
We start by evaluating the resilience provided by ARROW in
case of link failures. For this evaluation, we choose only provider
links of multi-homed stub ASes in the topology. A multi-homed
stub AS is an AS with more than one provider and no customers;
our topology includes 20,338 such ASes. We focus on these be-
cause the stub AS has a valid physical route to the rest of the Inter-
net even if a provider link L fails and we argue that this is also the
worst possible case, as the Internet topology features much less re-
dundancy towards its leaves (the stub ASes). Link failures closer to
the core of the Internet would simply affect a much smaller fraction
of ASes for both ARROW and BGP.
We arrive at a total number of 47,652 unique failures (the num-
ber of parent links over all multi-homed stub ASes), affecting an
average of 4,404 AS paths for every multihomed AS. We fail each
link L of each multi-homed stub AS A, successively. For each
failure trial, we fail a link L, and see what fraction of ASes (on
2http://iplane.cs.washington.edu/data/data.
html
Figure 10: CDF showing the fraction of link failures resulting
in a certain amount of disconnectivity, weighted by the average
duration of the disconnectivity event.
the whole topology) are temporarily disconnected from the corre-
sponding stub AS A, due to BGP convergence effects.
In some
cases, ASes become permanently disconnected, but we ignore these
in our results (note that ARROW might still provide them with a
valid route). We then also determine how long it takes, on average,
for these ASes to become re-connected. For each failed link, we
multiply the fraction of affected ASes by the length of the duration.
This is the result we use for the BGP line in Figure 10.
To plot the ARROW line, we select one random tier-1 AS as our
ARROW-supporting AS and ﬁx it for all simulation runs. Then we
conduct the same failure simulations, but in addition, we check for
each affected AS whether it has a valid path to A via the chosen
ARROW AS. For the failover duration we choose a ﬁxed time of 2
seconds, which is a conservative estimate for ARROW, given the
results in §5.3.
Figure 10 is a CDF plot of the results. The x-axis shows the frac-
tion of disconnectivity in the topology as the result of the failure,
weighted by the duration of the outage. For each such fraction f
on the x-axis we have the corresponding fraction of failures that re-
sulted in at most f disconnectivity on the y-axis. We crop the graph
at 100 seconds. The tail of BGP outages goes on until a maximum
of 418 seconds.
We can see that for 30% of link failures, BGP had an outage
of at least 2 seconds, with a long tail: 12% of outages have a du-
ration of at least a minute and 5% of outages last 100 seconds
and longer. All outages are handled by a single ARROW tier-1
relay node within 2 seconds (and likely much shorter, according
to §5.3). We conclude that a single tunnel is indeed enough to re-
cover from link failure disconnectivities in a time-span signiﬁcantly
shorter than that of BGP in the worst possible case of a link failure
to a parent of a multi-homed stub AS.
5.6 Resilience to AS Failures
The failure of an entire Autonomous System (AS) is a rare occur-
rence, but we would still like to gauge the effectiveness of ARROW
routing around such catastrophic outages. To do this, we conduct a
similar simulation to the previous one, but this time we simulate a
simultaneous failure of all links of a particular AS. We picked 200
ASes at random from the set of all tier 2 and tier 3 transit ASes.
Figure 11 is a CDF plot of the results. The x and y axes are the
same as in Figure 10, except we crop the x-axis at 200 seconds.
We can see that around 60% of AS failures cause a BGP conver-
gence period of at least 2 seconds. The tail is long again, with more
than 50% of failures resulting in a convergence duration of at least
100 seconds. A single ARROW-supporting tier-1 AS provides in-
stant failover for more than 55% of the failures. If ARROW were
 0 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90 100 110Received Throughput [Mbits/s]Time [seconds]ARROW pathBGP pathBGP outage  at GATECH 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100Cumulative fraction of failure casesWeighted average length of outage per link failure [seconds]ARROWBGPFigure 11: CDF showing the fraction of AS failures resulting
in a certain amount of disconnectivity, weighted by the average
duration of the disconnectivity event.
deployed at 4 tier-1 sites, a single tunnel can provide failover for
more than 70% of the failures. While a large number of failures
can be circumvented this way, the number is not as encouraging as
in Figure 10. This is because the interconnection density of tier-2
and tier-3 ASes is much higher than that of stub ASes and they are
also closer to an ARROW supporting tier-1.
We conclude that ARROW enables us to route around a large
number of transit AS failures. While a great improvement over
BGP, a multi-hop ARROW path might be required to increase the
number of preventable failures even further. We also note that these
results can serve as an indicator of how likely it is that ARROW can
be used to route around a transit AS that is otherwise not delivering
trafﬁc in the way we expect or that we simply do not trust, provided
that it is not malicious. We will address one case of malicious ASes
(preﬁx hijacking) in §5.8.
5.7 Path Redundancy
It is clear that ARROW can be used to route around single defec-
tive links and misbehaving ISPs. To reduce the risk of encountering
a string of such misbehaviors, we ask if we can build an ARROW
path with (near-)complete AS-level redundancy. We deﬁne a path q
to be completely redundant to path p if the AS-hops in q is disjoint
from that of p, except for the source and destination ASes. We are
interested in the number of common hops (disregarding source and
destination) between the original path p and the ARROW path q
with the highest disjointedness. Figure 12 shows this distribution
over all paths in the iPlane dataset (approximately 5 million) for
ARROW deployments on 2 and 4 tier-1 and tier-2 ASes.
In the tier-1 case, almost 40% of the paths for the 2-AS deploy-
ment, and 50% for the 4-AS deployment provide completely dis-
joint paths. Almost 80% of the paths in both cases have less than
half of the ASes from the old path still present in the new ARROW
path. The signiﬁcant redundancy with a small ARROW deploy-
ment can be explained by the rich peering provided by tier-1 ISPs
and the resulting high redundancy in the core of the Internet. This
is conﬁrmed by comparing with the tier-2 case, where signiﬁcantly
less redundancy is present.
We conclude that alternative ARROW paths ensure a high de-
gree of redundancy between the old and new paths, making AR-
ROW resilient even in the rare case of multiple cascading link and
AS-level misbehaviors.
5.8 Protection Against Preﬁx Hijacking
IP preﬁx hijacking is a serious challenge to the reliability and
security of the Internet. Since the Internet lacks any authoritative
information on the ownership of preﬁxes, IP preﬁx hijacking is ex-
Figure 12: CDF showing the number of common AS hops as a
fraction of p’s length (x-axis) versus the cumulative fraction of
paths with that amount of commonality (y-axis).
tremely hard to eliminate. ARROW can be used to mitigate the
effects of preﬁx hijacking. We imagine a scenario where the preﬁx
hijacking has already been detected. Speciﬁcally, given a standard
ARROW deployment on a small number of tier-1s, we ask what
fraction of sources still remain polluted (i.e., paths going through
any of the polluted ASes) for a particular preﬁx hijacking attack.
To simulate preﬁx hijacks, we select a victim AS and an attacker
AS, both stubs. We use all stubs in our iPlane topology as victims
and average the results over a random selection of 20 attackers for
each victim. This gives us a total of 16,160 victim ASes. For each
attack, we determine the set of polluted ASes as follows: an AS is