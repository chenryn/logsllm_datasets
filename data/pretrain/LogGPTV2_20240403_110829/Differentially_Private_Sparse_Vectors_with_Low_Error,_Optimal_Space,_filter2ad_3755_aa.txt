title:Differentially Private Sparse Vectors with Low Error, Optimal Space,
and Fast Access
author:Martin Aum&quot;uller and
Christian Janos Lebeda and
Rasmus Pagh
Differentially Private Sparse Vectors with Low Error,
Optimal Space, and Fast Access
Martin Aumüller
PI:EMAIL
IT University of Copenhagen
Copenhagen, Denmark
Christian Janos Lebeda
PI:EMAIL
BARC
IT University of Copenhagen
Copenhagen, Denmark
Rasmus Pagh
PI:EMAIL
BARC
University of Copenhagen
Copenhagen, Denmark
ABSTRACT
Representing a sparse histogram, or more generally a sparse vector,
is a fundamental task in differential privacy. An ideal solution would
use space close to information-theoretical lower bounds, have an
error distribution that depends optimally on the desired privacy
level, and allow fast random access to entries in the vector. However,
existing approaches have only achieved two of these three goals.
In this paper we introduce the Approximate Laplace Projection
(ALP) mechanism for approximating 𝑘-sparse vectors. This mech-
anism is shown to simultaneously have information-theoretically
optimal space (up to constant factors), fast access to vector entries,
and error of the same magnitude as the Laplace-mechanism applied
to dense vectors. A key new technique is a unary representation of
small integers, which we show to be robust against “randomized
response” noise. This representation is combined with hashing, in
the spirit of Bloom filters, to obtain a space-efficient, differentially
private representation.
Our theoretical performance bounds are complemented by sim-
ulations which show that the constant factors on the main perfor-
mance parameters are quite small, suggesting practicality of the
technique.
CCS CONCEPTS
• Security and privacy → Privacy-preserving protocols.
KEYWORDS
Algorithms, Differential Privacy, Sparse Vector
ACM Reference Format:
Martin Aumüller, Christian Janos Lebeda, and Rasmus Pagh. 2021. Dif-
ferentially Private Sparse Vectors with Low Error, Optimal Space, and
Fast Access. In Proceedings of the 2021 ACM SIGSAC Conference on Com-
puter and Communications Security (CCS ’21), November 15–19, 2021, Vir-
tual Event, Republic of Korea. ACM, New York, NY, USA, 14 pages. https:
//doi.org/10.1145/3460120.3484735
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea.
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484735
1
1 INTRODUCTION
One of the fundamental results in differential privacy is that a
histogram can be made differentially private by adding noise from
the Laplace distribution to each entry of the histogram before it is
released [7]. The expected magnitude of the noise on each histogram
entry is 𝑂(1/𝜀), where 𝜀 is the privacy parameter, and this is known
to be optimal [12]. In fact, there is a sense in which the Laplace
mechanism is optimal [14]. However, some histograms of interest
are extremely sparse, and cannot be represented in explicit form.
Consider, for example, a histogram of the number of HTTP requests
to various servers. Already the IPv4 address space has over 4 billion
addresses, and the number of unique, valid URLs have long exceeded
1012, so it is clearly not feasible to create a histogram with a (noisy)
counter for each possible value.
Korolova, Kenthapadi, Mishra, and Ntoulas [13] showed that it
is possible to achieve approximate differential privacy with space
that depends only on the number of non-zero entries in the his-
togram. However, for (𝜀, 𝛿)-differential privacy the upper bound
on the expected per-entry error becomes 𝑂(cid:16) log(1/𝛿)
𝑂(cid:16) log(𝑑)
(cid:17), which is sig-
(cid:17), where 𝑑 is the dimension of the histogram, i.e., the num-
nificantly worse than the Laplace mechanism for small 𝛿. Cormode,
Procopiuc, Srivastava, and Tran [5] showed how to achieve pure
𝜀-differential privacy with expected per-entry error bounded by
𝜀
ber of entries. While both these methods sacrifice accuracy they
are very fast, allowing access to entries of the private histogram
in constant time. If access time is not of concern, it is possible
to combine small space with small per-entry error, as shown by
Balcer and Vadhan [2]. They achieve an error distribution that is
comparable to the Laplace mechanism (up to constant factors) and
space proportional to the sum 𝑛 of all histogram entries — but the
time to access a single entry is ˜𝑂(𝑛/𝜀), which is excessive for large
datasets.
𝜀
1.1 Our results
Our contribution is a mechanism that achieves optimal error and
space (up to constant factors) with only a small increase in access
time. The mechanism works for either approximate or pure differ-
ential privacy, with the former providing faster access time. Our
main results are summarized in Theorem 1.1.
Theorem 1.1 (Informal Version of Theorems 5.10 and 5.11).
Let 𝑥 be a histogram with 𝑑 entries each bounded by some value 𝑢
where at most 𝑘 entries have non-zero values. Given privacy param-
eters 𝜀 > 0 and 𝛿 ≥ 0, there exists an (𝜀, 𝛿)-differentially private
algorithm to represent 𝑥 using 𝑂(𝑘 log(𝑑 + 𝑢)) bits with per-entry
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1223error matching the Laplace mechanism up to constant factors. The
access time is 𝑂(log(1/𝛿)) when 𝛿 > 0 and 𝑂(log(𝑑)) when 𝛿 = 0.
Here we assume that 𝑘 = Ω(log(𝑑)). Otherwise the mechanism
has an additional term of 𝑂(log2(𝑑)) or 𝑂(log(𝑑) log(1/𝛿)) bits
in its space usage for pure and approximate differential privacy,
respectively.
1.2 Techniques
On a high level, we treat “small” and “large” values of the histogram
differently. Large values are handled by the thresholding technique
developed in [5, 13]. For small entries, we represent them using a
unary encoding as fixed-length bit strings. From [5, 13] we know
that their length is logarithmic in either 𝑑 (for 𝜀-DP) or 1/𝛿 (for (𝜀, 𝛿)-
DP). Privacy is achieved by perturbing each bit using randomized
response [15]. As it turns out, the unary encoding is redundant
enough to allow accurate estimation even when the probability of
flipping each bit is a constant bounded away from 1/2. In order to
pack all unary representations into small space, we use hashing
to randomize the position of each bit in the unary representation
of a given entry. The access time is linear in the length of the bit
representation, given constant time evaluation of the hash function.
Interestingly, although hash collisions can lead to overestimates,
they do not influence the error asymptotically.
We remark here that a direct application of randomized response
does not give the desired 𝑂(1/𝜀) error dependency, but we solve
this issue with an initial scaling step that gives 𝜀-differential pri-
vacy when combined with randomized response. Though the dis-
cussion above has been phrased in terms of histograms, which
makes the comparison to earlier work easier, our techniques apply
more generally to representing sparse real vectors, with privacy
for neighboring datasets with bounded ℓ1-distance.
1.3 Overview
In Section 2 we define differential privacy for vectors, discuss the
Laplace mechanism, and provide probabilistic tools necessary for
the analysis. In Section 3 we discuss related work on differentially
private sparse histograms. In Section 4 we introduce the Approx-
imate Laplace Projection (ALP) mechanism and analyze its theo-
retical guarantees. In Section 5 we improve space and access time
using techniques from earlier work [5, 13]. In Section 6 we evaluate
the performance of the ALP mechanism based on simulations. In
Section 7 we present suggestions for practical applications. We
conclude the paper by stating an open problem in Section 8.
2 PRELIMINARIES
Problem Setup. In this work, we consider 𝑑-dimensional 𝑘-sparse
vectors of non-negative real values. We say that a vector 𝑥 ∈ R𝑑+
is 𝑘-sparse if it contains at most 𝑘 non-zero entries. We assume
that 𝑘 = Ω(log(𝑑)). All entries are bounded from above by a value
𝑢 ∈ R, i.e., max𝑖∈[𝑑] 𝑥𝑖 =: ∥𝑥∥∞ ≤ 𝑢. Here [𝑑] is the set of integers
{1, . . . , 𝑑}. We consider the problem of constructing an algorithm
M for releasing a differentially private representation of 𝑥, i.e.,
˜𝑥 := M(𝑥). Note that ˜𝑥 does not itself need to be 𝑘-sparse.
Utility Measures. We use two measures for the utility of an algo-
rithm M. We define the per-entry error as |𝑥𝑖 − ˜𝑥𝑖| for any 𝑖 ∈ [𝑑].
We define the maximum error as max𝑖∈[𝑑] |𝑥𝑖 − ˜𝑥𝑖| = ∥𝑥 − ˜𝑥∥∞. We
compare the utility of algorithms using the expected per-entry and
maximum error and compare the tail probabilities of the per-entry
error of our algorithm with the Laplace mechanism introduced
below.
Differential Privacy. Differential privacy is a constraint to limit
privacy loss introduced by Dwork, McSherry, Nissim, and Smith [7].
We use definitions and results as presented by Dwork and Roth [8].
Intuitively, a differentially private algorithm ensures that a slight
change in the input does not significantly impact the probability
of seeing any particular output. We measure the distance between
inputs using their ℓ1-distance. In this work, two vectors are neigh-
bors iff their ℓ1-distance is at most 1. That is for all neighboring
𝑖 | ≤ 1. We
vectors 𝑥, 𝑥′ ∈ R𝑑+ we have ∥𝑥 − 𝑥′∥1 :=𝑖∈[𝑑] |𝑥𝑖 − 𝑥′
can now define differential privacy for neighboring vectors.
Definition 2.1 (Differential privacy [8, Def 2.4]). Given 𝜀 > 0 and
𝛿 ≥ 0, a randomized algorithm M : R𝑑+ → R is (𝜀, 𝛿)-differentially
private if for all subsets of outputs 𝑆 ⊆ R and pairs of 𝑘-sparse
input vectors 𝑥, 𝑥′ ∈ R𝑑+ such that ∥𝑥 − 𝑥′∥1 ≤ 1 it holds that:
Pr[M(𝑥) ∈ 𝑆] ≤ 𝑒𝜀 · Pr[M(𝑥′) ∈ 𝑆] + 𝛿 .
M satisfies approximate differential privacy when 𝛿 > 0 and
pure differential privacy when 𝛿 = 0. In particular, a pure differen-
tially private algorithm satisfies 𝜀-differential privacy. The following
properties of differential privacy are useful in this paper.
Lemma 2.2 (Post-processing [8, Proposition 2.1]). LetM : R𝑑+ →
R be an (𝜀, 𝛿)-differentially private algorithm and let 𝑓 : R → R′
be any randomized mapping. Then 𝑓 ◦ M : R𝑑+ → R′ is (𝜀, 𝛿)-
differentially private.
Lemma 2.3 (Composition [8, Theorem 3.16]). Let M1 : R𝑑+ →
R1 and M2 : R𝑑+ → R2 be randomized algorithms such that 𝑀1 is
(𝜀1, 𝛿1)-differentially private and M2 is (𝜀2, 𝛿2)-differentially private.
Then the algorithm M where M(𝑥) = (M1(𝑥),M2(𝑥)) is (𝜀1 +
𝜀2, 𝛿1 + 𝛿2)-differentially private.
Throughout this paper, we clamp the output of all algorithms to
the interval [0, 𝑢]. An estimate outside this interval is due to noise
and clamping outputs cannot increase the error. It follows from
Lemma 2.2 that clamping the output does not affect privacy. We
clamp the output implicitly to simplify presentation.
Probabilistic Tools. The Laplace Mechanism introduced by Dwork,
McSherry, Nissim, and Smith [7] satisfies pure differential privacy
by adding noise calibrated to the ℓ1-distance to each entry. For
completeness, Algorithm 1 provides a formulation of the Laplace
mechanism in the context of releasing an 𝜀-differentially private
representation of a sparse vector.
Algorithm 1: The Laplace Mechanism
Parameters:𝜀 > 0.
:𝑘-sparse vector 𝑥 ∈ R𝑑+.
Input
:𝜀-differentially private approximation of 𝑥.
Output
(1) Let ˜𝑥𝑖 = 𝑥𝑖 + 𝜂𝑖 for all 𝑖 ∈ [𝑑], where 𝜂𝑖 ∼ Lap(1/𝜀).
(2) Release ˜𝑥.
2
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1224Here Lap(1/𝜀) is the Laplace distribution with scale parameter
1/𝜀. The PDF and CDF of the distribution are presented in Defi-
nitions 2.4 and 2.5 and the expected error and tail bound of the
mechanism are shown in Propositions 2.6 and 2.7. The Laplace
mechanism works well for vectors with low dimensionality and
serves as a baseline for our work. However, it is impractical or even
infeasible in the setting of 𝑘-sparse vectors. The output vector is
dense, and as such the space requirement scales linearly in the input
dimensionality 𝑑.
Definition 2.4. The probability density function of the Laplace
distribution centered around 0 with scale parameter 1/𝜀 is
Definition 2.5. The cumulative distribution function of the Laplace
distribution centered around 0 with scale parameter 1/𝜀 is:
𝑓 (𝜏) =
𝜀
2𝑒−|𝜏 |𝜀 .
(cid:40) 1
2𝑒𝜏𝜀,
1 − 1
2𝑒−𝜏𝜀,
Pr[Lap(1/𝜀) ≤ 𝜏] =
if 𝜏 < 0
if 𝜏 ≥ 0
E[|𝑥𝑖 − ˜𝑥𝑖|] = 𝑂(1/𝜀) and E[∥𝑥 − ˜𝑥∥∞] = 𝑂(cid:16) log(𝑑)
Proposition 2.6 (Expected Error [8, Theorem 3.8]). The ex-
pected per-entry and maximum error of the Laplace mechanism are
respectively.
Proposition 2.7 (Tail bound [8, Theorem 3.8]). With probabil-
(cid:17)
𝜀
ity at least 1 − 𝜓 we have:
|Lap(1/𝜀)| ≤ 1
𝜀
ln 1
𝜓
.
Random rounding or stochastic rounding is used for rounding a
real value probabilistically based on its fractional part. We define
random rounding for any real 𝑟 ∈ R as follows:
(cid:40)⌈𝑟⌉ with probability 𝑟 − ⌊𝑟⌋
RandRound (𝑟) =
⌊𝑟⌋ with probability 1 − (𝑟 − ⌊𝑟⌋)
Lemma 2.8. The expected error of random rounding is maximized
when 𝑟 − ⌊𝑟⌋ = 0.5. For any 𝑟 we have:
E[|𝑟 − RandRound (𝑟) |] ≤ 1
2 .