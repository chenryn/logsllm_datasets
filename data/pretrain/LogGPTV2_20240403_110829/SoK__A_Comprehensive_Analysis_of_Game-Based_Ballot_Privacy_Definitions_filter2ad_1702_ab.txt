has been applied, votes are counted with some appropriate
counting function. Typical counting functions are:
• the multiset function multiset that discloses the sequence
of all the casted votes, in a random order;
• the counting function counting that
tells how many
votes each candidate received;
winner.
• the majority function majority that only discloses the
We consider a set I of voters, a subset H ⊆ I of honest
voters, a ballot box BB and a public bulletin board PBB. A
single-pass protocol πρ executes in three phases.
1) In the setup phase, the ballot box expects one message
from an administrator after which it may either transition
to the voting phase or abort.
2) In the voting phase, all parties may post messages to
the ballot box at any time; the ballot box decides to
accept or reject a message based on its current state and
a public algorithm. The ballot box stores all accepted
messages. Any party may ask to read the bulletin board
at any time; the ballot box replies by running a public
ﬁltering5algorithm (that we will call Publish) on its
current state and returns the result. Once the voting phase
is closed, the ballot box transitions to the result phase.
3) In the result phase, the ballot box is closed: it accepts no
more messages but can still be read. The administrator
computes the ﬁnal outcome r and a proof of valid
tabulation Π via a tallying procedure that may operate
on the ballot box.
4This is the policy in Estonian elections, for example.
5Filtering serves several purposes. In an election where only the last vote
from each voter counts, the ﬁlter may return only last ballot from each voter.
In Helios, there is a “short” board consisting only of hashes of the ballots;
this is sufﬁcient to check whether one’s ballot has been included and could
again be modelled as a ﬁlter. Such a ﬁlter could even be used as a defense
against ballot-copying, by not revealing the full ballots until the voting phase
is closed.
501501
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:30 UTC from IEEE Xplore.  Restrictions apply. 
Formally, a voting scheme V = (Setup, Vote, Publish,
Valid, Tally, Verify), for a list of voters I and a result function
ρ, consists of six algorithms with the syntax given below.
The new features of our formalisation are (1) the Publish
algorithm, which allows ballot boxes to store more information
than they display to the public (i.e. they may keep invalid
ballots internally but not display them) and (2) we let each
ballot have an explicit identity, which seems to be required
to model revoting policies (i.e. “the last ballot from each
voter counts”). Our model matches how e.g. Helios handles
identities in practice.
• Setup(λ) on input a security parameter λ outputs an
election public key pk and a secret tallying key sk.
We assume pk to be an implicit input of the remaining
algorithms.
• Vote(id, v) is used by voter id to cast his vote v ∈ V for
the election, as a ballot b ← Vote(id, v)6.
• Valid(BB, b) takes as input the ballot box BB and a ballot
b and checks it validity. It returns (cid:6) for valid ballots
and ⊥ for invalid ones (ill-formed, contains duplicated
ciphertext from the ballot box, etc.).
• Publish(BB) takes as input the ballot box BB and outputs
the public view PBB of BB, called public bulletin board.
• Tally(BB, sk) takes as input the ballot box BB and the
secret key sk. It outputs the tally r, together with a proof
of correct tabulation Π. Possibly, r = ⊥. This algorithm
might be (partially) in charge of implementing the revote
policy.
• Verify(PBB, r, Π) takes as input a public bulletin board
PBB, and a result/proof pair (r, Π) and checks whether
Π is a valid proof of correct tallying for r, it returns (cid:6)
if so, otherwise it returns ⊥.
Any voting protocol should ensure that if everyone acts cor-
rectly, the protocol indeed computes ρ on the votes submitted.
Formally, a voting scheme is correct if the following properties
hold with overwhelming probability. Let v1, . . . , vn ∈ V be
valid votes and id1, . . . , idn ∈ I be voter identities. We
consider an honest execution. Let (pk, sk) ← Setup(λ). Let
bi ← Vote(idi, vi) and BBi := [b1, . . . , bi] for all i. Then
honest ballots are valid, that is, Valid(BBi−1, bi) = (cid:6) for all i;
and the protocol computes the correct election result, that is, let
(r, Π) ← Tally(BBn, sk) then r = ρ((id1, v1), . . . , (idn, vn))
and Verify(Publish(BBn), r, Π) = (cid:6).
An important contribution of our work is an explicit for-
malisation of revote policies. Intuitively, there are two main
and distinct reasons for removing a ballot:
• Cryptographic cleaning: This deletion might be due to
the cryptographic implementation. In Helios for exam-
ple, a ballot may be removed because it is ill-formed
(e.g. invalid zero-knowledge proofs) or because it con-
tains a duplicated ciphertext, which may yield a privacy
6We do not include in our syntax, as most of the related work in the area,
the necessary algorithms that allow an election administrator to distribute
credentials among users, that will be in turn used to authenticate the voter to
the ballot box and cast a ballot.
breach [27]. In our formalism, this cleaning operation can
be taken care of either by the Valid predicate or by the
Tally function itself.
• Mandatory revote policy: This deletion might correspond
to the implementation of the “ideal” revote policy spec-
iﬁed by the result function ρ. A typical revote policy is
that, for each voter, only the last ballot shall be retained.
B. Tally uniqueness
We deﬁne tally uniqueness, a “minimal” property that any
veriﬁable system should satisfy. This property is of course not
mandatory for ballot privacy but we will use it in the next
sections to illustrate that some ballot privacy deﬁnitions are
incompatible with any veriﬁable system.
Intuitively, tally uniqueness of a voting scheme ensures
that two different tallies r (cid:8)= r
(cid:3) for the same board cannot
be accepted by the veriﬁcation algorithm, even if all
the
players in the system are malicious. The goal of the adversary
against tally uniqueness is to output a public key pk, a list of
legitimate public identities, a ballot box BB, and two tallies
r (cid:8)= r
(cid:3), and corresponding proofs of valid tabulation Π and
(cid:3), such that both pass veriﬁcation.
Π
(cid:3)
) ← A(1λ)
Experiment ExpuniqA (λ)
(cid:3)
(1) (pk, BB, r, Π, r
, Π
(2) PBB ← Publish(BB)
(3) if r
(4) Verify(PBB, r, Π) = Verify(PBB, r
(cid:3) (cid:8)= r and
return 1 else return 0
Fig. 1. Tally Uniqueness
(cid:3)
(cid:3)
, Π
) = (cid:6)
We deﬁne tally uniqueness by the experiment ExpuniqA in Fig-
ure 1. A voting scheme V has tally uniqueness if Succuniq(A)
is a negligible function for any PPT adversary A, where
Succuniq(A) = Pr
(cid:2)
ExpuniqA (λ) = 1
(cid:3)
Tally uniqueness is often considered as a requirement for
election veriﬁability [31], [32].
III. SURVEY AND ANALYSIS OF PREVIOUS GAME-BASED
COMPUTATIONAL VOTE PRIVACY DEFINITIONS
Game-based privacy [23], [24], [25], [12], [18], [19], [26],
[22], [20], [21] (our terminology) deﬁnitions require that it
should be hard for an adversary to win a game with a
challenger behaving in a fully speciﬁed manner, in the spirit
of traditional indistinguishability deﬁnitions for encryption. In
this section we review the most relevant ballot privacy deﬁni-
tions in the literature. We unveil that some previous deﬁnitions
present shortcomings that have gone unnoticed: either ballot
secrecy and election veriﬁability are incompatible [22], or
ballot secrecy does not guarantee that the choices of the voters
remain private once the election result is published [19], [21].
We also discuss known limitations of the approaches in [23],
[25], [24], [31], [33]. A summary of our ﬁndings is shown in
Table I, at the end of this section.
502502
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:30 UTC from IEEE Xplore.  Restrictions apply. 
is actually published,
Most existing deﬁnitions do not distinguish between the
ballot box and what
the bul-
letin board. Therefore, in this section, we implicitly assume
Publish(BB) = BB. Moreover, some deﬁnitions do not model
voters identiﬁers. By a slight abuse of notation, we may write
Vote(v) instead of Vote(id, v) when id is ignored.
i.e.
We formalize ballot privacy deﬁnitions by using two ballot
boxes BB0 and BB1, from which only one box will be visible
to the adversary. Of course, the adversary’s goal is to learn
which one of them is visible. For the uniformity of the
presentation, we write explicitly both ballot boxes in each of
the deﬁnitions that follow, even if some of them can be deﬁned
using only one ballot box (such as Deﬁnitions III-A and III-B).
We use the notation and some terminology that we ﬁx above
to discuss the existing game-based deﬁnitions. We proceed
following, roughly, the chronological order.
A. Ballot privacy for permutations of honest votes - PODC
1986 [23], STOC 1994 [25]
which states that “no party receives information which would
allow them to distinguish one situation from another one
in which two voters swap their votes”. Dreier, Lafoucarde
and Laknech have generalised the swap-equivalent symbolic
privacy deﬁnition to weighted votes [33].
However IND-BB privacy does not guarantee indistin-
guishability between different assignments that lead to the
same result but which are not equivalent permutation-wise.
For instance, consider the case where voters can give a
score of 0, 1, or 2 to a (single) candidate. It may be the
case that an attacker cannot distinguish the sequences of
votes [(ida, 0); (idb, 2)] from [(ida, 2); (idb, 0)] but could well
distinguish [(ida, 0); (idb, 2)] from [(ida, 1); (idb, 1)]. In fact,
this example serves as a simpliﬁed abstraction of actual voting
rules, such as those in the European Parliament elections in
Luxembourg [34] or the Swiss Government Federal Elections.
Deﬁnitions [23], [25], [31] do not capture these real cases.
B. Benaloh’s ballot privacy [24]
One ﬁrst deﬁnition of ballot privacy follows a simple idea:
an attacker should not notice if the votes of two voters
are swapped. More precisely, a coalition of voters should
not be able to distinguish when two honest voters id0, id1
vote respectively v0 and v1, from the case where they vote
respectively v1 and v0.
as follows:
Deﬁnition 1 (IND-BB): I is a list of voters. BB0, BB1 are
lists initialized at empty. The challenger starts by picking a
random bit β, and the adversary B = (B1,B2) is given access
to lists I and BBβ. The challenger runs the setup algorithm and
the keys (pk, sk) are created. The adversary B1 can repeatedly
query the oracle Ocast as follows:
• Ocast(id, b): runs bb (cid:9)→ bb(cid:10)b on ballot boxes BB0 and
BB1. (The expression bb(cid:10)b appends b to bb.)
The adversary can also query once an oracle OVoteIND(·,·)
• OVoteIND(id0, id1, v0, v1) : if vδ /∈ V for δ = 0, 1, halt.
Else, runs BB0 ← BB0(cid:10){Vote(id0, v0), Vote(id1, v1)} as
well as BB1 ← BB1(cid:10){Vote(id0, v1), Vote(id1, v0)}.
At some point, the adversary B1 asks to see the result.
The challenger computes (r, Π) ← Tally(BBβ, sk). Finally
the IND-BB adversary B2 outputs β
Formally, we say that a voting scheme V is IND-BB secure
if no PPT algorithm B can distinguish between the outputs in
the experiment just described for β = 0 and β = 1, i.e. for
any PPT adversary B,
(cid:3) as the guess for β.
(cid:4)(cid:4)(cid:4) Pr
(cid:2)
Expindbb,0
B
(cid:3)
(cid:2)
− Pr
(λ) = 1
Expindbb,1
B
(λ) = 1
(cid:3) (cid:4)(cid:4)(cid:4)
B
is negligible, where Expindbb,β
is the experiment deﬁned above.
The deﬁnition IND-BB can be seen as a generalization of
the private elections deﬁnition by Benaloh and Yung [23],
which was deﬁned only with respect to referendum elections,
so that only v0 = 0 and v1 = 1 were considered, and by
Benaloh and Tuinstra [25]. It also resembles the symbolic
vote privacy deﬁnition by Delaune, Kremer and Ryan [31],
To cope with the aforementioned limitation, [24] has gener-
alized the previous deﬁnition to an arbitrary set of voters that
may vote arbitrarily provided that the tally corresponding to
honest voters remains unchanged.
The following deﬁnition is a restatement, using contem-
porary notation, of Benaloh’s privacy deﬁnition for voting
schemes [24].
Deﬁnition 2 (PRIV): BB0, BB1, V0, V1 are lists initialized
at empty. The challenger starts by picking a random bit β,
and adversary B = (B1,B2) is given access to list BBβ. The
challenger runs the setup algorithm and the keys (pk, sk)
are created. B1 on input pk is given access to oracles
Ovote(·),Oballot(·) as follows:
• Ovote(id, v0, v1) : runs BBδ ← BBδ(cid:10)Vote(id, vδ)),
Vδ ← Vδ(cid:10)(id, vδ)) for δ = 0, 1.
• Oballot(b): runs bb (cid:9)→ bb(cid:10)b on ballot boxes BB0, BB1
(that is, appends b to both boards).
At some point, B2 asks to see the tallying output. The chal-
lenger proceeds as follows: if ρ(V0) (cid:8)= ρ(V1), halts. Otherwise,
the challenger outputs (rβ, Πβ) ← Tally(BBβ, sk). Finally, B2
(cid:3) a the guess for β. Formally, we declare a voting
outputs β
scheme V Benaloh’s private if for any PRIV adversary B,
(cid:3) (cid:4)(cid:4)(cid:4)
− Pr
is the experiment deﬁned above.
The main drawback of Benaloh’s deﬁnition is that it restricts
the set of functions for which eventually a scheme could
be declared ballot private. To see this, let us assume that
the possible votes are {a, b} and consider the result function
majority. We assume that a wins in case of a tie. Let B
be an adversary that chooses to play the game such that
V0 = [(id1, a); (id2, a)] and V1 = [(id1, a); (id2, b)]. Clearly
majority(V0) = majority(V1) = a (V1 is a tie) so a wins).
Let now B cast a ballot query for vote b. Then r0 = a and
r1 = b and thus Succpriv(B) = 1, independently of the scheme
V.
is negligible, where Exppriv,βB
(cid:4)(cid:4)(cid:4) Pr
Exppriv,0B
Exppriv,1B
(λ) = 1
(λ) = 1
(cid:2)
(cid:3)
(cid:2)
503503
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:30 UTC from IEEE Xplore.  Restrictions apply. 
C. Ballot privacy ESORICS 2011 - CCS 2012 [12], [18], [17]