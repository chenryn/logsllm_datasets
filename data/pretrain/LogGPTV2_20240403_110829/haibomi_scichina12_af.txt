Through adjusting the clusters, the false-negative
Mi H B, et al. Sci China Inf Sci December 2012 Vol. 55 No. 12 2771
rate decreases from 31.4% to 7.6% and the false-positive rate declines from 5.1% to 4.2%, which again demonstrates the effectiveness of our approach.
7.3 	Case scenario 3In cloud systems, components are usually developed by different teams. When engineers with one team invoke public interfaces of another component developed by another team, they may invoke unsuitable methods that cause extra performance costs. It is hard to detect such faults especially when the function is correct. In this case study, we investigate how our approach can be utilized to help the developer diagnose such performance anomalies.During the running of the system, we manually inject a fault through the hot patch. The fault causes clients of the storage service to call an interface with extra authorization logic. It costs more than two times extra overheads due to unnecessarily adding an authorization process for clients. In the process of performance regression tests, the performance of the ReadFile decreases dramatically.In this scenario, the number of major clusters is three and each of them contains about four principal methods, as is shown in Figure 7(c). 	Figure 8(c) shows that the relevant method is precisely found out (i.e., the one with the highest score). The developer can easily find the mistake according to the clue. Furthermore, the dissimilarity ratios of instances for this method are shown in Figure 9(c). The first three instances differ significantly from others. We can know that the influence scopes of faults in cloud computing system are different, and that without finding out the exact anomalous instances, the debuggers will have to spend more efforts on pinpointing the culprit.In Table 2, we can see that if requests within the minor clusters are dropped, it will cause the increase of the false-negative rate obviously. It means that the anomalous methods within the minor clusters will not be fully identified without adjusting the clusters.
7.4 	Case scenario 47.4 	Case scenario 4
This scenario is a real-world diagnosing process in one production cluster in Alibaba Cloud Computing Company. Performance bugs related to user behaviors are hard to detect in the testing environment. These bugs will not be trigged until specific user behavior occurs. Our approach is applied by an operator to diagnose this problem.The average latencies of the SaveFile increase about two times and this situation lasts several hours. With our approach, the most suspicious methods are picked out, as is shown in Figure 8(d). The first one is used to lock the file ID before the transaction of saving a file begins. Figure 9(d) shows that the dissimilarity ratio of the first instance is sharply high. It means that the instance is the physical locations of the top one anomalous method. Along the clue, he finds the root cause efficiently. Because the older load balance mechanism does not consider the access patterns adequately, it causes more than 60% of accesses to be centralized into that instance. This causes the performance of the SaveFile operation to decline significantly in that period. The diagnosis process again helps operators localize the key issues effectively.From Table 2, we can see that through adjusting the clusters, the false-positive rate increases slightly; however, the suspicious scores of the normal methods that are mistaken for the anomaly are low (just rank top 5 and 6 in Figure 8(d)); therefore, it will not influence the diagnosing result. Furthermore, the false-negative rate decreases from 21.4% to 12.7%.
8 	Conclusions8 	Conclusions
When a system performance anomaly occurs, it is generally a labor-intensive task for operators to locate anomalous parts of the system. Isolating the physical locations (i.e., instances) of anomalous methods could tremendously reduce the overall manual work in identifying the root cause.Performance anomalies always cause the change in response latencies of user requests. The hidden connections among the huge amount of runtime request execution paths usually contain useful information
2772 Mi H B, et al. Sci China Inf Sci December 2012 Vol. 55 No. 12for diagnosing performance problems. In this paper, we propose an approach to localize the anomalous methods as well as their physical locations by engaging request trace logs. The approach requires no specific domain knowledge for the operators. To highlight the effectiveness of the approach, we report our experiences to diagnose four real-world performance anomalies that occurred in Alibaba cloud computing platform. The experimental results show that our approach can locate the primary causes of performance anomalies with low false-positive rate and false-negative rate.Acknowledgements
This research was supported by National Basic Research Program of China (Grant No. 2011CB302600), National High Technology Research and Development Program of China (Grant No. 2012AA011201), National Natural
Science Foundation of China (Grant Nos. 61161160565, 90818028, 91118008, 60903043). Yangfan Zhou and
Michael Lyu’s work was supported by National Natural Science Foundation of China (Grant No. 61100077),Basic Research Program of Shenzhen (Grant No. Kong (Project No. N CUHK405/11).
References
JC201104220300A), and Research Grants Council of Hong
1 	Lu X, Wang H, Wang J, et al. Internet-based virtual computing environment: beyond the data center as a computer.
Futur Gener Comp Syst, 2013, 29: 309–322
2 	Han S, Dang Y, Ge S, et al. Performance debugging in the large via mining millions of stack traces. In: Proceedingsof the 34th International Conference on Software Engineering, Zurich, 2012. 176–186
3 	Chilimbi T, Liblit B, Mehra K, et al. Holmes: Effective statistical debugging via efficient path profiling. In: 31st IEEE
International Conference on Software Engineering, Vancouver, 2009. 34–44
4 	Killian C, Nagaraj K, Pervez S, et al. Finding latent performance bugs in systems implementations. In: Proceedings ofthe Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering. New York: ACM,
2010. 17–26
5 	Lan Z, Zheng Z, Li Y. Toward automated anomaly identification in large-scale systems. IEEE Trans Parallel Distrib
Syst, 2010, 21: 174–187
6 	Malik H, Adams B, Hassan A. Pinpointing the subsystems responsible for the performance deviations in a load test.In: Proceedings of 21st IEEE International Symposium on Software Reliability Engineering, San Jose, 2010. 201–210
7 	Reynolds P, Killian C, Wiener J, et al. Pip: Detecting the unexpected in distributed systems. In: Symposium on
Networked Systems Design and Implementation, San Jose, 2006, 115–128
8 	Sambasivan R, Zheng A, De Rosa M, et al. Diagnosing performance changes by comparing request flows. In: Pro-ceedings of the 8th USENIX Conference on Networked Systems Design and Implementation. 	Berkeley: USENIX
Association, 2011. 43–56
9 	Jin G, Song L, Shi X, et al. Understanding and detecting real-world performance bugs. In: The 33rd ACM SIGPLAN
Conference on Programming Language Design and Implementation. New York: ACM, 2012. 77–88
10 	Thereska E, Ganger G. Ironmodel: Robust performance models in the wild. ACM SIGMETRICS Perform Eval Rev,2008, 36: 253–264
11 	Mi H B, Wang H M, Yin G, et al. Performance problems diagnosis in cloud computing systems via analyzing request
trace logs. In: The 13th International Conference on Network Operations and Management Symposium (NOMS),
Maui, 2012. 893–899
12 	Jolliffe I. Principal Component Analysis, 2nd ed. New York: Springer, 2002
13 	Fay M, Proschan M. Wilcoxon-Mann-Whitney or t-test on assumptions for hypothesis tests and multiple interpretationsof decision rules. Stat Surv, 2010, 4: 1–39
14 	Melville P, Yang S, Saar-Tsechansky M, et al. Active learning for probability estimation using jensen-shannon diver-
gence. In: Proceedings of the 16th European Conference on Machine Learning. Berlin/Heidelberg: Springer-Verlag,
2005. 268–279
15 	Sigelman B, Barroso L, Burrows M, et al. Dapper, a large-scale distributed systems tracing infrastructure. TechnicalReport dapper-2010-1, Google, 2010
16 	Park I, Buch R. Event tracing-improve debugging and performance tuning with ETW. MSDN Mag, 2007. 81–92
17 	Thereska E, Salmon B, Strunk J, et al. Stardust: tracking activity in a distributed storage system. ACM SIGMETRICS
Perform Eval Rev, 2006, 34: 3–14
18 	Sang B, Zhan J, Lu G, et al. Precise, scalable, and online request tracing for multi-tier services of black boxes. IEEETrans Parallel Distrib Syst, 2010, 99: 1–16
Mi H B, et al. 	Sci China Inf Sci 	December 2012 Vol. 55 No. 12 	2773
19 	Tak B, Tang C, Zhang C, et al. Vpath: precise discovery of request processing paths from black-box observations 	of thread and network activities. In: Proceedings of the 2009 Conference on USENIX Annual Technical Conference.
	Berkeley: USENIX Association, 2009. 19–3220 	Koskinen E, Jannotti J. Borderpatrol: isolating events for black-box tracing. ACM SIGOPS Operat Syst Rev, 2008, 	42: 191–203 
	Reynolds P, Wiener J, Mogul J, et al. Wap5: black-box performance debugging for wide-area systems. In: Proceedings 21 
	of the 15th International Conference on World Wide Web. New York: ACM, 2006. 347–356 
2222 
	Aguilera M, Mogul J, Wiener J, et al. Performance debugging for distributed systems of black boxes. ACM SIGOPS 	Operat Syst Rev, 2003, 37: 74–89
23 Chen M, Kiciman E, Fratkin E, et al. Pinpoint: Problem determination in large, dynamic internet services. In:
Proceedings of 32nd IEEE International Conference on Dependable Systems and Networks, Bethesda, 2002. 595–60424 Chen M, Accardi A, Kiciman E, et al. Path-based faliure and evolution management. In: Proceedings of the 1st
	Conference on Symposium on Networked Systems Design and Implementation, Vol. 1. Berkeley: USENIX Association, 	2004. 23–36 
25 	Barham P, Donnelly A, Isaacs R, et al. Using Magpie for request extraction and workload modelling. In: Proceedings of 	the 6th Conference on Symposium on Opearting Systems Design and Implementation. Berkeley: USENIX Association, 	2004. 259–27226 	Fonseca R, Porter G, Katz R, et al. X-trace: A pervasive network tracing framework. In: Proceedings of the 4th 	USENIX Conference on Networked Systems Design and Implementation. Berkeley: USENIX Association, 2007. 20–33 27 	Mi H, Wang H, Yin G, et al. Magnifier: Online detection of performance problems in large-scale cloud computing 	systems. In: Proceedings of 8th IEEE International Conference on Services Computing, Washington DC, 2011. 418–425 28 	Wang C, Schwan K, Talwar V, et al. A flexible architecture integrating monitoring and analytics for managing large-	scale data centers. In: Proceedings of the 8th ACM International Conference on Autonomic Computing. New York: 	ACM, 2011. 141–15029 	Wang C, Viswanathan K, Choudur L, et al. Statistical techniques for online anomaly detection in data centers. In: 	Proceedings of the 12th IFIP/IEEE International Symposium on Integrated Network Management, Dublin, 2011.
	385–392 
30 	Bodik P, Goldszmidt M, Fox A, et al. Fingerprinting the datacenter: automated classification of performance crises.In: Proceedings of the 5th European Conference on Computer Systems. New York: ACM, 2010. 111–124
31 Wang C, Talwar V, Schwan K, et al. Online detection of utility cloud anomalies using metric distributions. In:
	Proceedings of the IEEE Network Operations and Management Symposium, Osaka, 2010. 96–103 
32 	Lakhina A, Crovella M, Diot C. Diagnosing network-wide traffic anomalies. ACM SIGCOMM Comput Commun Rev, 	2004, 34: 219–23033 	Xu W, Huang L, Fox A, et al. Detecting large-scale system problems by mining console logs. In: Proceedings of the 	ACM SIGOPS 22nd Symposium on Operating Systems Principles. New York: ACM, 2009. 117–132
34 Oliner A, Aiken A. Online detection of multi-component interactions in production systems. In: 41st IEEE/IFIP
	International Conference on Dependable Systems & Networks (DSN), Hong Kong, 2011. 49–6035 	Ringberg H, Soule A, Rexford J, et al. Sensitivity of PCA for traffic anomaly detection. ACM SIGMETRICS Perform 	Eval Rev, 2007, 35: 109–120 
36 	King J, Jackson D. Variable selection in large environmental data sets using principal components analysis. Environ-	metrics, 1999, 10: 67–77 
37 	Ghemawat S, Gobioff H, Leung S. The Google file system. ACM SIGOPS Operat Syst Rev, 2003, 37: 29–43