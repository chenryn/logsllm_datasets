can avoided by using techniques proposed in [14].
3.2 GRE tunneling
GRE is an encapsulation protocol that can be used to en-
capsulate a network protocol (payload protocol) in another
network protocol (delivery protocol). We focus on GRE with
IPv4 as the underlying delivery protocol - GRE-IP . Conse-
quently, each tunnel is characterized by a source and a des-
tination IP address. Besides this, GRE also involves key’ing
of tunnels - the source and the destination must agree on
the key for the tunnel to operate correctly. Conﬁguring such
a GRE-IP tunnel today requires the management plane to
Parameter
i Name
ii Up.Con-Modules
(Connectable-Modules)
iii Up.Dependencies
iv Down.Con-Modules
v Down.Dependencies
vi
Physical pipes
vii Peerable-Mod.
viii
ix
x
Filter
Switch
Perf Reporting
xi
Perf Trade-Oﬀs
xii Perf Enforcement
xiii
Security
Value
IPv4
Performance Trade-oﬀs to be spec-
iﬁed
IPv4
None
None
GRE
Nil
[Up ⇒ Down],[Down ⇒ Up]
Number of recieved and transmit-
ted bytes on each up pipe
{[Jitter,
[In-order
delivery] | Up-pipe}
{[Loss-Rate] Vs [Error-Rate] | Up-
pipe}
Nil
Nil
Delay] Vs
Table 3: Abstraction exposed by our GRE implementa-
tion
provide the IP addresses of the tunnel end-points, the key
values, whether to use sequence numbers or not (sequence
numbers help with in-order delivery of tunneled packets) and
other protocol speciﬁc details such as tunnel TTL, the TOS
ﬁeld for tunneled packets, whether to use checksums or not,
and whether to use path-mtu-discovery or not.
We have implemented a GRE module conforming to the
CONMan architecture. As mentioned earlier, our implemen-
tation is based on the Linux GRE kernel module with a user-
level wrapper that conﬁnes the protocol-speciﬁc details to the
implementation and exposes a generic abstraction to the NM.
This abstraction is shown in table 3 and some of the entries
are explained below:
ii). Ideally, GRE can carry any payload protocol and hence,
there should not be any restriction on the modules that the
GRE module can connect to using an up pipe. However,
most implementations restrict the payload protocol to a well
deﬁned list of protocols - with our underlying Linux imple-
mentation, the only payload protocol possible is IPv4.
iii). To create an up pipe, the NM needs to specify the per-
formance trade-oﬀs (see k below) that apply to pipe.
iv,v). The module is restricted to having IPv4 as the tunnel-
ing protocol with no explicit dependencies.
ix). The module can switch packets between an up-pipe and
a down-pipe. The switching state is generated by the module
on its own.
x). The underlying Linux implementation provides limited
performance reporting: the number of bytes transmitted and
received on each up pipe.
xi). The module oﬀers the following trade-oﬀs: For a given
up-pipe, it can trade-oﬀ delay and jitter for in-order delivery.
The fact that this is attained by enabling sequence numbers
whose use needs to be coordinated with the peer GRE module
is not exposed. Similarly, the module can trade-oﬀ loss-rate
for error-rate for a speciﬁed up-pipe through the use of check-
sums.
We now describe how a NM, based on this abstraction, can
use CONMan primitives to achieve the following low-level
goal:
Conﬁgure the path between the IP modules 
and  labeled as (1) through (12) in ﬁgure 3.
Note that this is equivalent to creating a GRE-IP tunnel
between devices A and B in the existing set-up. Also, as men-
Device  
A
(a)  
IP      
(1) 
GRE (b) 
(2) 
IP  (c)   
(3) 
ETH  (d)   
Peers denoted by
Device    C 
(Layer-2 Switch) 
Device    D 
(Router)
IP (g) 
(7) 
(6)
ETH  (e)
ETH  (f)   ETH  (h)  
(a’)  
IP      
(12)  
GRE
(b’)  
Device  
(11) 
B
IP (c’)   
(10) 
ETH  (d’)    
(4)  
(5) 
(8)
(9)
Figure 3: GRE-IP tunnel between devices A and B -
the NM needs to build the path labeled from (1) to
(12).
tioned earlier, the human manager in CONMan is not
aware of such low-level goals or the notion of pipes
and switches or the CONMan script shown below. In-
stead, he/she speciﬁes a high-level goal and the next section
describes how our NM implementation maps this high-level
goal to the aforementioned low-level goal. This mapping pro-
cess informs the NM which modules along the path are peers
of each other – in ﬁgure 3, the dashed line between pipes la-
belled (1) and (12) indicates that modules a and a’ are peer
modules for these pipes (as are modules b and b’).
We also assume that the NM has, as part of the mapping
process, invoked the showPotential primitive at these devices
and hence, is aware of the CONMan abstraction for all the
modules involved; for instance, table 3 shows the abstraction
exposed by the module  (or b for short). The
other modules have similar abstractions that are not shown
here. This equips the NM with all the information it needs to
create the appropriate pipes and switch state. As a contrast,
some manual must be read (either by the implementor of the
management application or the system administrator) to gain
the equivalent knowledge while conﬁguring GRE-IP tunnels
today. With this information at hand, the NM can build the
segment of the path in device A (i.e. a ⇒ b ⇒ c ⇒ d) using
the following script. A similar script needs to be invoked to
build the rest of the path.
(1).
P1 = create (pipe, , ,
, ,
trade-off:
trade-off:
order delivery,
error-rate)
(2).
(3).
(4).
(5).
(6).
P2 = create (pipe, , ,
, , None)
create (switch, , P1, P2)
P3 = create (pipe, , ,
, , None)
create (switch, , P2, P3)
create (switch, , P3, P4)
In the script, command (1) creates pipe P1 between the
IP module a and the underlying GRE module b. The fourth
and ﬁfth arguments in the command specify the peer IP (a’)
and GRE (b’) modules for the pipe being created. Further,
the NM satisﬁes the dependency for creating an up pipe for
a GRE module by specifying that it desires in-order deliv-
ery of packets and low error-rate. These choices would be
based on high-level performance goals speciﬁed by the hu-
man manager. Similarly, commands (2) and (4) create pipes
P2 and P3. Through command (3) the NM speciﬁes that
GRE module b should switch between pipes P1 and P2. Sim-
ilarly, commands (5) and (6) conﬁgure the switch in modules
c and d respectively.
The simple and structured process described above is all
the conﬁguration that the NM needs to do. It is the protocols
MA  
Device A 
NM 
MA   
Device B
conveyMessage (, GRE-specific parameters) 
conveyMessage (, GRE-specific parameters)  
Key Values,   
 Seq No. usage  
 and other parameters 
MA  
Device A 
listFieldsAndValues() 
listFieldsAndValues()  
listFieldsAndValues()
listFieldsAndValues()  
IP-address 
of tunnel     
end-points  
MA  
Device D 
IP-address 
of next-hop  
Associated
Command
Command 
       (1)
Command 
(2)
Command 
(4)
Caller    
functionName  ( parameters )  
Callee    
Packets over data plane paths
Figure 4: GRE-IP Tunnel establishment between de-
vices A and B - the management plane is simpliﬁed
by ensuring that protocol complexity is restricted to
protocol implementation.
that incorporate the complexity of determining the low-level
parameters. Each module, based on the commands invoked
by the NM, interacts with its peer module through the man-
agement channel to determine the required protocol speciﬁc
parameters – this process is brieﬂy described below and illus-
trated in ﬁgure 4.
On invocation of command (1) and the corresponding com-
mand on device B, modules b and b’ use the conveyMessage
primitive to exchange the GRE-speciﬁc parameters needed
for connectivity between them. These include the GRE key
values in each direction, the use of sequence numbers, etc.
Some of these parameters are based on the trade-oﬀ deci-
sions speciﬁed by the NM. For example, the NM, as part of
command (1), opts for in-order delivery. This causes mod-
ules b and b’ to negotiate the use of sequence numbers for the
GRE tunnel between them. Similarly, on invocation of com-
mand (2), IP modules c and c’ ﬁgure out the IP addresses of
the tunnel end-points by determining each other’s IP address
through the use of listFieldsAndValues. Command (3) causes
the GRE module b to generate the actual Linux command to
conﬁgure the GRE tunnel, the parameters for this command
already having been determined:
ip tunnel add name gre-P1-P2 mode gre remote 204.9.
169.1 local 204.9.168.1 ikey 1001 okey 2001 icsum
ocsum iseq oseq
Similarly, command (4) causes IP modules c and g to ex-
change their IP addresses while command (5) causes IP mod-
ule c to generate the low-level routing rule in device A such
that packets to device B are routed through D:
ip route add to 204.9.169.1 via 204.9.168.2 dev eth2
3.3 Virtual Private Networks (VPNs)
VPNs are commonly used to connect geographically dis-
tributed enterprise sites across the Internet while oﬀering se-
curity and performance comparable to connecting the sites
across a dedicated network. As the name suggests, a “provider-
provisioned VPN” involves the ISP that provides connectiv-
ity to the enterprise sites conﬁguring and maintaining the
VPN [3]. We implemented a NM that can be used for such
VPN conﬁguration. In the interest of brevity, the discussion
below focusses on the conﬁguration sub-task of an ISP try-
ing to ensure that traﬃc between two sites S1 and S2 of a
customer C1 is isolated from other traﬃc. A complete VPN
conﬁguration involves doing the same for all pairs of sites of
Customer 1
Site 1
Router D 
Router B    
Router A
Router C    
ISP
(a)
Customer 1  
Site 2
Router E
Router A
Router B 
Router C
IP  (g)
GRE (l)
IP  (i)
GRE (n)
IP  (k)  
Customer 1
Site 1
Router D 
MPLS (o)  
IP  (h)   
GRE (m)
MPLS (p)
IP  (j)
MPLS(q) 
Customer 1 
Site 2
Eth (a)    Eth (b)   
Eth (c)    Eth (d)   
Eth (e)    Eth (f)   
Router E
(b)
Figure 5: Experimental set-up emulating an ISP
and customer sites for the VPN conﬁguration. Fig-
ure 5(b) shows the network map and the modules
seen by the NM prior to conﬁguration.
each customer needing VPN support. Figure 5(a) shows the
relevant part of the set-up in our lab with ﬁve Linux hosts (la-
beled A-E) serving as two of the ISP’s edge routers in diﬀerent
POPs (A and C), the ISP’s core router (B) and customer C1’s
routers at site S1 (D) and site S2 (E). Speciﬁcally, the NM
aims to achieve the following high-level goal speciﬁed by
the human network manager:
Conﬁgure connectivity between sites S1 and S2 of cus-
tomer C1.
This is equivalent to the following high-level goal in CON-
Man terminology:
Conﬁgure connectivity between the customer-facing in-
terfaces  and  (see ﬁgure 5(b))
for traﬃc between C1-S1 and C1-S2.
Ideally, C1-S1 and C1-S2 should be high-level identiﬁers
that get mapped to the IP preﬁxes for the two sites through
communication between the NM of the ISP and the NM of
customer C1. However, this paper is restricted to manage-
ment in a single domain and hence, we provide the NM with
this information. Second, we assume that the NM has al-
ready assigned IP addresses to the IP modules. Finally, the
high-level goal above is imprecise since it does not specify
whether traﬃc between C1-S1 and C1-S2 ought to be iso-
lated or not. Today, this choice is dictated by whether C1-S1
and C1-S2 are public or private IP preﬁxes and this applies
to our implementation too. Consequently, the NM is aware
of the notion of public and private addresses. As explained
later in the section, this knowledge is used by the NM in the
way it ﬁnds paths through the network. We admit that this is
case of the NM using protocol-speciﬁc information. As men-
tioned earlier, while it is possible to abstract IP addresses,
their ubiquity and scarcity combined with the impact of ad-
dress assignment on routing scalability suggests that it makes
engineering sense to let the NM be aware of them and this is
what we chose for our implementation.
3.3.1 NM Implementation
All devices in the ISP’s network (routers A, B, C) inform