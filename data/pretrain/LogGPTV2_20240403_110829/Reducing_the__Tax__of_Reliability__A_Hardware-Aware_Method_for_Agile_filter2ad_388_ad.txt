Rcontrol 
Numfsync 
Apps 
Rcontrol 
Numfsync 
Facebook 
32.8% 
42 
Gmail 
22.8% 
Twitter 
19.9% 
9 
Netflix 
19% 
Google Map 
37.1% 
25 
Angry Birds 
23.3% 
1 
Chrome 
26.5% 
41 
Amazon 
17.2% 
3 
3 
4 
1) Redundant Backup of Clean Metadata 
In  our  characterization,  we  observe  that  Android  apps 
usually  operate  on  temporary  files  during  their  execution. 
For instance, Amazon app caches products thumbnails in its 
private  folder  and  removes  them  periodically.  In  addition, 
some  apps  (e.g.  Facebook  and  Twitter)  regularly  perform 
database  operations,  which  eventually  leads  to  significant 
amount of creation, modification and removal operations on 
ephemeral files [6]. These behaviors involve modifications 
of multiple metadata blocks. For example, a file creation in 
Ext4 dirties at least three metadata blocks and there are sim-
ilar scenarios in file modification and removal. 
The OS must backup these dirtied metadata blocks for re-
sisting  data  loss  in  system  crashes.  To  this  end,  the  entire 
contents of each dirtied metadata block are logged in journal 
region,  rather  than  logging  only  modified  bytes.  The  ra-
tionale  is  that  all  data  in  the  under-programming  storage 
cells  would  be  lost  when  the  system  suddenly  crashes. 
Therefore,  for  the  completeness  of  backup,  the  granularity 
of  journal  must  be  at  least  equal  to  the  size  of  minimum 
programming unit of the underlying storage (typically 4KB 
for flash storage). Consequently, a large number of metadata 
blocks are involved in the data persistence path. 
With the introduction of reliable write in eMMC (Section 
II.A),  the  programming  cells,  upon  a  system  crash,  would 
retain either completely new or entirely old contents, mak-
ing  it  unnecessary  to  back  up  the  unmodified  (clean)
metadata  within  one  metadata  blocks.  The  modified  part 
alone is sufficient for recovering the metadata block which 
it belongs to. 
Metadata  modification  ratio  is  defined  to  quantify  the 
amount of clean metadata in data persistence path, which is 
calculated by dividing the number of modified bytes in each 
logged metadata block over the metadata block size: 
35-0/0.-3.8* = 
35-0/0.-+:8.7
  ! 
We plot the CDF for this metric. As shown in Fig. 6, for 
more than 80% of the metadata blocks backed up in journal 
region, the actually modified bytes are less than 1% of the 
block  size,  i.e.  less  than  40  bytes  in  a  4KB  block.  This
means that existing journal scheme records a large amount 
of clean metadata in journal region, which is an unnecessary 
redundancy in eMMC-based devices. Thus, clean metadata 
should  be  expelled  from  data  persistence  path  to  reduce 
amount of journal blocks and save the time of metadata per-
sistence. 
77
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 













%#$
$#-
$#,
$#+
$#*
$#)
$#(



!


"

 
$
$#$$)
$#$%



























	
	







(
%*
++
&-
'
*
',+
,&(
%-)&
%''+
e
m
i
t
n
u
R
d
e
z
i
l
a
m
r
o
N
80
60
40
20
0
flush cache
direct write
normal write
commit block
direct commit block
Flush: flush at end
Direct: direct persistence
h
s
u
F
l
t
c
e
r
i
D
4
h
s
u
F
l
t
c
e
r
i
D
8
h
s
u
F
l
l
l
h
s
u
F
h
s
u
F
t
c
e
r
i
D
32
t
c
e
r
i
D
16
Data Volume (KB)
t
c
e
r
i
D
64
h
s
u
F
l
t
c
e
r
i
D
128
h
s
u
F
l
t
c
e
r
i
D
256
Figure 6. CDF of Metadata Modification 
Ratio 
Figure 7. Histograph of Commit Time Ratio 
Figure 8. Comparison of Two Transaction 
Commit Schemes 
2) Excessive Number of Control Blocks 
Metadata blocks are backed up to journal region through 
transaction commit (Section II.B). Considering that all data 
within  one  metadata  block  are  used  for  backup,  journal 
thread must spare extra blocks in journal region for record-
ing  control  information  that  will  be  used  in  recovery.  We 
defined control block ratio (Rcontrol), which is calculated by 
dividing the number of control blocks over the total number 
of blocks in one complete journal record: 
,548652 = 
-+ ; ,+
-+ ; 3 ; ,+
Table 3 shows the average Rcontrol for the eight apps, which 
reaches up to 37.1% of all journal blocks to be persisted. 
However, the utilization of these control blocks is small. 
In commit block, only first 44 bytes are used for recording 
control information: magic number, transaction ID and op-
tional checksum. The other data in commit block are invalid. 
The utilization of descriptor block varies depending on the 
number of dirtied metadata blocks in the current transaction. 
Theoretically,  the  utilization  is  100%  when  512  metadata 
blocks  are  indexed  by  this  descriptor  block.  Nevertheless, 
the  utilization  is  also  small  under  real  mobile  workloads. 
Every transaction commit only carries 3.3 dirtied metadata 
blocks on average.  
The reason is that Android apps frequently invoked syn-
chronization write during their execution. Normally, journal 
thread performs transaction commit at relatively long inter-
vals (every 5 seconds in default) and doing so can help ac-
cumulating enough metadata blocks. However, synchroniza-
tion writes would trigger transaction commit before lots of 
metadata are dirtied. Table 3 shows the average number of 
fsync (Numfsync) occurred within 5 seconds. 7 out of 8 apps 
(except  Angrybirds)  exhibit  more  than  one  fsync  in  5  se-
conds.  The  top  three  apps  that  invoke  the  most  frequent 
fsync also yield the highest percentage of control blocks.  
The high proportional of control blocks and the low utili-
zation of each control block imply that control information 
are  rare  but  can  contribute  a  high  write  traffic.  The  root 
cause is still the coarse granularity of metadata backup, de-
riving from the agnostic on the reliable features of eMMC. 
3) Overhead of Global Barrier 
78
Another overhead we found resides in the submission of 
commit block. Fig. 7 shows the distribution of the commit 
time  ratio,  where  the  commit  time  ratio  is  calculated  via 
dividing the waiting time for persisting commit block over 
the entire waiting time in one data persistence path: 
,53308 =
,+
- ; -+ ; 3 ; ,+
Td, Tdb, Tm and Tcb represent waiting time for persisting data 
blocks,  descriptor  blocks,  metadata  blocks  and  commit 
blocks respectively. Note that only the synchronous waiting 
is counted in this formula. 
The  observation  of  Fig.  7  is  that  among  all  eight  apps, 
commit block persistence contributes a large portion (30%-
60%) of the data persistence time. The reason is that in ex-
isting journal scheme, the submission of commit block con-
sists of two successive steps: buffer cache flush to make all 
previously  submitted  blocks  (i.e.  data,  descriptor  and 
metadata blocks) durable in medium; and forcedly program-
ing the commit block to NAND flash for flagging the com-
pletion  of  the  transaction  commit  (Section  II.B).  In  this 
scheme,  cache  flush  is  placed  at  the  end  of  transaction
commit and cannot be skipped since all previously written 
back blocks must be durable before the commit block. We 
name it as flush-at-end scheme and perform an experiment 
to reveal the overhead of this scheme. To this end, we cus-
tomize an eMMC driver in Linux I/O stack. This driver di-
rects eMMC host controller to perform a representative pat-
tern  of  transaction  commit:  sequential  write  a  group  of 
blocks,  flush  eMMC  buffer  cache  and  submit  a  commit 
block with FUA flag. We vary the size of sequential writes 
and breakdown the completion time of each case. As shown 
in Fig. 8, cache flush time accounts for over 40% of runtime
in the flush-at-end scheme.  
As a comparison, we propose a direct persistence scheme, 
where  the  first  step  of  sequentially  writing  back  blocks  is 
conducted  with  FUA  flag.  By  means  of  the  write-through 
FUA, the blocks in the first step are directly programmed on 
flash. Thus, the cache flush step is bypassed. From Fig. 8, 
we  can  observe  that  when  the  data  amount  is  less  than 
128KB,  the  data  persistence  time  of  flush-at-end  scheme 
(the sum of orange and light blue portion in Fig. 8) is 30% 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 
























	







  	
Figure 9. Distribution of Modified Metadata in Each Transaction 
Commit. 
more  than  the  time  of  persisting  same  amount  of  data  via 
forcibly  programing  (red  portion  in  Fig.  8).  One  reason  is 
that data path is simplified by directly programming to flash.  
Compared to flush-at-end scheme, buffer cache flushing is 
bypassed.  Moreover,  storage  buffer  cache  is  a  global  re-
source  shared  by  all  processes  accessing  the  storage.  A 
global-wide  cache  flush  would be  unexpectedly  long  since 
data blocks belonging to concurrent programs are compelled 
to flush out simultaneously. Thus, direct persistence scheme 
is  a  more  efficient  approach  for  transaction  commit.  Note 
that additional techniques are required to prevent block re-
ordering  in  OS  and  eMMC  device.  We  will  discuss  these 
techniques in the Sections IV.B and Section V.B. 
IV.TOWARDS EFFICIENT DATA PERSISTENCE 
    In  this  section,  we  present  how  to  leverage  eMMC  fea-
tures to mitigate the overheads on the data persistence while 
preserving system reliability.  
A. Journal Coalition 
Our  characterization  shows  that  the  modified  metadata, 
which is crucial to the system reliability, only accounts for a 
small portion on the data volume synchronously written to 
journal  region.  Fig.  9  shows  the  distribution  of  actually 
modified metadata in each transaction commit. Taking Net-
flix (which has the most modified metadata) as an example, 
the upper bound of modified bytes is: 
 )) = 
<
