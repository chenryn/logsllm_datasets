==== Kernel 函数
Kernel 函数详解见 ,  。
Kernel的存在主要用于特征抽取，在特征抽取的方法中，比较流行的是基于空间变换的方法，目标是将原始数据变换到一个新的空间，使得在新的空间不同类别间数据具有最大的分离性，或者是新的空间中的数据对原始数据有最好的描述能力，用数学的方式描述就是，在变换后的空间中，数据在新特征上具有最大的variance。
基于变换特征抽取技术分为线性和非线性两部分，常用的线性变换技术包括主成分分析（PCA）、线性鉴别分析（LDA）等。线性变换技术一般是一种性能较优的降维技术，但其很难根本改变原始数据的线性可分离性。
Kernel则是一种非线性变换,他能够将原先线性不可分的数据，变换到高维空间上，使其变得线性可分。一般会有个超平面来进行区分，这里很类似SVM的思想，这边先介绍下把数据转为高纬度空间的缺点：
* curse of dimension，是一个最早由Richard Bellman提出来的术语，用来描述当（数学）空间维度增加时，体积指数增加的难题。举例来说，100个平均分布的点能把一个单位区间以每个点距离不超过0.01采样；而当维度增加到10后，如果以相邻点距离不超过0.01小方格采样一单位超正方体，则需要 10^20^ 个采样点:所以，这个10维的超正方体也可以说是比单位区间大 10^18^ 倍。(这个是Richard Bellman所举的例子)
而Kernel则很好的解决了该问题，虽然把数据映射到高维空间，但是他所对应的维数仅为训练样本的个数, 为什么kernel变换后的特征个数和样本数一样，以及kernel的详细变换，可参考 视频  。
其实Kernel的作用在于用点与点之间的距离，来重新描述数据，kernel 其实又名 covariance function，就是用来描述样本之间协方差的函数，而样本间协方差本质就是点与点之间的距离。
常用的kernel 有Linear, Gaussian or RBF 等等， 一般RBF用的较多，但是其计算复杂度较大，如果在数据量大时候，耗时较多，而Linear的方式则较为简单
Kernel这边涉及到的数据原理较多，解释较为麻烦，这边只对一个简单的多项式 kernel，进行举例
image::images/ml-kernel-model-1.png[]
图中原始空间需要非线性的切分，当我们将原始数据进行变换后，决策边界则有原先的椭圆（非线性）变成了一个线性的超平面，因此，在新的空间中，问题就简化成了，可以用线性的模型去分类。
上面讲过，kernel的本质是用点与点之间的距离来表示新的空间，一般来说，我们都会用向量内积的方式来表示点与点之间的距离
image::images/ml-kernel-model-2.png[]
从上面公式推导可以看出，通过kernel函数K（x~i~,x~j~)可以将一个数据点x~i~ 进行变换，这样用到的核函数为多项式函数, 原先的数据点x~i~,可以与数据集中每个点求 内积，这样x~i~就会变成一个新的向量，维度为数据集的大小，每个值代表x~i~与该点的距离（相似度）
另外，这边我们如何得知，经过变换后的相似度和原先  相似度 一样呢？这里可以根据mercer 定理得知，只有Kernel函数是半正定的就行啦~~
==== PCA 
PCA 详解见: 
PCA的目的就是对数据进行降维，上面已经介绍过当维度过大时候，有什么弊端， 另外PCA是线性降维，因为 PCA 是将原先的坐标系进行rotate，而rotate是一个Linear transformation, 新的坐标系是原先坐标系的线性组合,在新的坐标系中，我们会抛弃掉一些dimension，这是没有任何影响的，因为我们求解出的新坐标系之间是正交的。
关于线性降维，从数学上讲，从原始数据到子空间的映射f， 要满足 f(a+b) = f(a) + f(b), 对于PCA来说， 变换后的子空间和 特征映射 都是线性的。
PCA 是用于将 多特征的数据分解到 一系列 相互正交的新的成分中，并且要求使数据映射到新的成分后，保持最大的variance。
PCA降维的具体推导等等涉及的数据较多，这里不再阐述，要说明的一点是，PCA后的新特征，并非是从原先数据集的特征保留一部分，而是根据原先特征所产生的新的一小部分特征，并且这个特征有时候难以从直观上去理解。
Syntax: 
    fit PCA [params set] from  [into model_name]
Parameters:
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** n_components : int, float, None or string
+
Number of components to keep. if n_components is not set all components are kept:
+
`n_components == min(n_samples, n_features)`
+
if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used to guess the dimension if 0 = 0, optional (default .0)
+
Tolerance for singular values computed by svd_solver == ‘arpack’.
** iterated_power : int >= 0, or ‘auto’, (default ‘auto’)
+
Number of iterations for the power method computed by svd_solver == ‘randomized’.
** random_state : int or RandomState instance or None (default None)
+
Pseudo Random Number generator seed control. If None, use the numpy.random singleton. Used by svd_solver == ‘arpack’ or ‘randomized’.
使用计算机硬件数据举例:
  tag:computer_hardware  |  fit PCA n_components=4 from json.vendor,json.Model,json,json.MYCT,json.MMIN,json.MMAX,json.CACH,json.CHMIn,json.CHMAX,json.PRP,json.ERP into pca 
注意我们得对n_components 进行限制，因为一般而言前几个主要的components就可以占到整个信息熵的大部分。
  |summary model pca
查看pca模型:
image::images/ml-pca-summary.png[]
可以从explained_variance_ratio_ 看到前2个components 已经占据了99.9%的信息熵，也就是说，实际上，我们用新的二维数据就可以描述原先的10多维数据了，这样既有效的去掉了冗余信息，也给计算带来简化。
==== KernelPCA
详解见:  。
上面已经介绍过 Kernel 和PCA了， 所以KernelPCA = Kernel + PCA
通过kernel trick 可以对原先数据进行非线性的映射到低纬空间。
下面几张图解释了，经过kernelPCA后，维度和数据的变化:
image::images/ml-kernelpca-model-1.png[]
image::images/ml-kernelpca-model-2.png[]
image::images/ml-kernelpca-model-3.png[]
Syntax: 
    fit KernelPCA [params set] from  [into model_name]
Parameters:
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** n_components : int, default=None
+
Number of components. If None, all non-zero components are kept.
** kernel : “linear” | “poly” | “rbf” | “sigmoid” | “cosine” | “precomputed”
+
Kernel. Default=”linear”.
** degree : int, default=3
+
Degree for poly kernels. Ignored by other kernels.
** gamma : float, default=1/n_features
+
Kernel coefficient for rbf and poly kernels. Ignored by other kernels.
** coef0 : float, default=1
+
Independent term in poly and sigmoid kernels. Ignored by other kernels.
** alpha : int, default=1.0
+
Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).
** fit_inverse_transform : bool, default=False
+
Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)
** eigen_solver : string [‘auto’|’dense’|’arpack’], default=’auto’
+
Select eigensolver to use. If n_components is much less than the number of training samples, arpack may be more efficient than the dense eigensolver.
** tol : float, default=0
+
Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack.
** max_iter : int, default=None
+
Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.
** remove_zero_eig : boolean, default=False
+
If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be < n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.
** random_state : int seed, RandomState instance, or None, default=None
+
A pseudo random number generator used for the initialization of the residuals when eigen_solver == ‘arpack’.
** n_jobs : int, default=1
+
The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores.