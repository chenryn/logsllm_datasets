code repository [37, 71] or a VM image [6, 14, 73]. In this paper,
we investigate potential consequences of using untrusted training
algorithms on a trusted platform.
3 THREAT MODEL
As explained in subsection 2.2, data holders often use other peo-
ple’s training algorithms to create models from their data. We thus
focus on the scenario where a data holder (client) applies ML code
provided by an adversary (ML provider) to the client’s data. We in-
vestigate if an adversarial ML provider can exfiltrate sensitive
training data, even when his code runs on a secure platform?
Client. The client has a dataset D sampled from the feature spaceX
and wants to train a classification model fθ on D, as described in
subsection 2.1. We assume that the client wishes to keep D private,
as would be the case when D is proprietary documents, sensitive
medical images, etc.
The client applies a machine learning pipeline (see Figure 1)
provided by the adversary to Dtrain, the training subset of D. This
pipeline outputs a model, defined by its parameters θ. The client
validates the model by measuring its accuracy on the test subset
Dtest and the test-train gap, accepts the model if it passes validation,
and then publishes it by releasing θ or making an API interface
to fθ available for prediction queries. We refer to the former as
white-box access and the latter as black-box access to the model.
Adversary. We assume that the ML pipeline shown in Figure 1 is
controlled by the adversary. In general, the adversary controls the
core training algorithm T, but in this paper we assume that T is a
conventional, benign algorithm and focus on smaller modifications
to the pipeline. For example, the adversary may provide a malicious
data augmentation algorithm A, or else a malicious regularizer
Ω, while keeping T intact. The adversary may also modify the
parameters θ after they have been computed by T.
The adversarially controlled pipeline can execute entirely on the
client side—for example, if the client runs the adversary’s ML library
locally on his data. It can also execute on a third-party platform,
such as Algorithmia. We assume that the environment running the
algorithms is secured using software [2, 74] or hardware [61, 66]
isolation or cryptographic techniques. In particular, the adversary
cannot communicate directly with the training environment; oth-
erwise he can simply exfiltrate data over the network.
Adversary’s objectives. The adversary’s main objective is to infer
as much as of the client’s private training dataset D as possible.
Some existing models already reveal parts of the training data.
For example, nearest neighbors classifiers and SVMs explicitly store
some training data points in θ. Deep neural networks and classic
logistic regression are not known to leak any specific training
information (see Section 8 for more discussion about privacy of the
existing training algorithms). Even with SVMs, the adversary may
want to exfilitrate more, or different, training data than revealed
by θ in the default setting. For black-box attacks, in which the
adversary does not have direct access to θ, there is no known
way to extract the sensitive data stored in θ by SVMs and nearest
neighbor models.
Other, more limited, objectives may include inferring the pres-
ence of a known input in the dataset D (this problem is known
as membership inference), partial information about D (e.g., the
presence of a particular face in some image in D), or metadata as-
sociated with the elements of D (e.g., geolocation data contained in
the digital photos used to train an image recognition model). While
we do not explore these in the current paper, our techniques can
be used directly to achieve these goals. Furthermore, they require
extracting much less information than is needed to reconstruct
entire training inputs, therefore we expect our techniques will be
even more effective.
Assumptions about the training environment. The adversary’s
pipeline has unrestricted access to the training data Dtrain and the
model θ being trained. As mentioned above, we focus on the sce-
narios where the adversary does not modify the training algorithm
T but instead (a) modifies the parameters θ of the resulting model,
or (b) uses A to augment Dtrain with additional training data, or
(c) applies his own regularizer Ω while T is executing.
We assume that the adversary can observe neither the client’s
data, nor the execution of the adversary’s ML pipeline on this data,
nor the resulting model (until it is published by the client). We
assume that the adversary’s code incorporated into the pipeline is
isolated and confined so that it has no way of communicating with
or signaling to the adversary while it is executing. We also assume
that all state of the training environment is erased after the model
is accepted or rejected.
Therefore, the only way the pipeline can leak information about
the dataset Dtrain to the adversary is by (1) forcing the model θ
to somehow “memorize” this information and (2) ensuring that θ
passes validation.
Access to the model. With white-box access, the adversary re-
ceives the model directly. He can directly inspect all parameters
in θ, but not any temporary information used during the training.
This scenario arises, for example, if the client publishes θ.
With black-box access, the adversary has input-output access
to θ: given any input x, he can obtain the model’s output fθ(x).
For example, the model could be deployed inside an app and the
adversary uses this app as a customer. Therefore, we focus on the
simplest (and hardest for the adversary) case where he learns only
the class label assigned by the model to his inputs, not the entire
prediction vector with a probability for each possible class.
4 WHITE-BOX ATTACKS
In a white-box attack, the adversary can see the parameters of the
trained model. We thus focus on directly encoding information
about the training dataset in the parameters. The main challenge is
how to have the resulting model accepted by the client. In particular,
the model must have high accuracy on the client’s classification
task when applied to the test dataset.
4.1 LSB Encoding
Many studies have shown that high-precision parameters are not
required to achieve high performance in machine learning mod-
els [29, 48, 64]. This observation motivates a very direct technique:
simply encode information about the training dataset in the least
significant (lower) bits of the model parameters.
Encoding. Algorithm 1 describes the encoding method. First, train
a benign model using a conventional training algorithm T, then
post-process the model parameters θ by setting the lower b bits of
each parameter to a bit string s extracted from the training data,
producing modified parameters θ′.
Extraction. The secret string s can be either compressed raw data
from Dtrain, or any information about Dtrain that the adversary
wishes to capture. The length of s is limited to ℓb, where ℓ is the
number of parameters in the model.
T, number of bits b to encode per parameter.
Algorithm 1 LSB encoding attack
1: Input: Training dataset Dtrain, a benign ML training algorithm
2: Output: ML model parameters θ′ with secrets encoded in the
lower b bits.
3: θ ← T(Dtrain)
4: ℓ ← number of parameters in θ
5: s ← ExtractSecretBitString(Dtrain, ℓb)
6: θ′ ← set the lower b bits in each parameter of θ to a substring
of s of length b.
Algorithm 2 SGD with correlation value encoding
1: Input: Training dataset Dtrain = {(xj , yj)}n
i =1, a benign loss
function L, a model f , number of epochs T , learning rate η,
attack coefficient λc, size of mini-batch q.
2: Output: ML model parameters θ correlated to secrets.
3: θ ← Initialize(f )
4: ℓ ← number of parameters in θ
5: s ← ExtractSecretValues(D, ℓ)
6: for t = 1 to T do
for each mini-batch {(xj , yj)}q
7:
дt ← ∇θ
8:
θ ← UpdateParameters(η, θ, дt)
9:
10:
11: end for
j=1 ⊂ Dtrain do
j=1 L(yj , f (xj , θ)) + ∇θ C(θ, s)
q
end for
1
m
Decoding. Simply read the lower bits of the parameters θ′ and
interpret them as bits of the secret.
4.2 Correlated Value Encoding
Another approach is to gradually encode information while training
model parameters. The adversary can add a malicious term to the
loss function L (see Section 2.1) that maximizes the correlation
between the parameters and the secret s that he wants to encode.
In our experiments, we use the negative absolute value of the
Pearson correlation coefficient as the extra term in the loss function.
During training, it drives the gradient direction towards a local
minimum where the secret and the parameters are highly correlated.
Algorithm 2 shows the template of the SGD training algorithm with
the malicious regularization term in the loss function.
Encoding. First extract the vector of secret values s ∈ Rℓ from the
training data, where ℓ is the number of parameters. Then, add a
malicious correlation term C to the loss function where
(cid:12)(cid:12)(cid:12)ℓ
i =1(θi − ¯θ)(si − ¯s)(cid:12)(cid:12)(cid:12)
i =1(θi − ¯θ)2 ·(cid:113)ℓ
(cid:113)ℓ
i =1(si − ¯s)2
C(θ, s) = −λc ·
is that we assign a weight to each parameter in C that depends on
the secrets that we want the model to memorize. This term skews
the parameters to a space that correlates with these secrets. The
parameters found with the malicious regularizer will not necessarily
be the same as with a conventional regularizer, but the malicious
regularizer has the same effect of confining the parameter space to
a less complex subspace [72].
Extraction. The method for extracting sensitive data s from the
training data Dtrain depends on the nature of the data. If the features
in the raw data are all numerical, then raw data can be directly used
as the secret. For example, our method can force the parameters to
be correlated with the pixel intensity of training images.
For non-numerical data such as text, we use data-dependent
numerical values to encode. We map each unique token in the vo-
cabulary to a low-dimension pseudorandom vector and correlate
the model parameters with these vectors. Pseudorandomness en-
sures that the adversary has a fixed mapping between tokens and
vectors and can uniquely recover the token given a vector.
Decoding. If all features in the sensitive data are numerical and
within the same range (for images raw pixel intensity values are in
the [0, 255] range), the adversary can easily map the parameters
back to feature space because correlated parameters are approxi-
mately linear transformation of the encoded feature values.
To decode text documents, where tokens are converted into
pseudorandom vectors, we perform a brute-force search for the
tokens whose corresponding vectors are most correlated with the
parameters. More sophisticated approaches (e.g., error-correcting
codes) should work much better, but we do not explore them in this
paper.
We provide more details about these decoding procedures for
specific datasets in Section 6.
4.3 Sign Encoding
Another way to encode information in the model parameters is to
interpret their signs as a bit string, e.g., a positive parameter repre-
sents 1 and a negative parameter represents 0. Machine learning
algorithms typically do not impose constraints on signs, but the
adversary can modify the loss function to force most of the signs
to match the secret bit string he wants to encode.
Encoding. Extract a secret binary vector s ∈ {−1, 1}ℓ from the
training data, where ℓ is the number of parameters in θ, and con-
strain the sign of θi to match si. This encoding method is equivalent
to solving the following constrained optimization problem:
n
.
min
θ
such that
Ω(θ) + 1
L(yi , f (xi , θ))
θisi > 0 for i = 1, 2, . . . , ℓ
i =1
n
In the above expression, λc controls the level of correlation and
¯θ, ¯s are the mean values of θ and s, respectively. The larger C, the
more correlated θ and s. During optimization, the gradient of C
with respect to θ is used for parameter update.
Observe that the C term resembles a conventional regularizer (see
Section 2.1), commonly used in machine learning frameworks. The
difference from the norm-based regularizers discussed previously
Solving this constrained optimization problem can be tricky for
models like deep neural networks due to its complexity. Instead,
we can relax it to an unconstrained optimization problem using the
penalty function method [60]. The idea is to convert the constraints
to a penalty term added to the objective function, where the term
penalizes the objective if the constraints are not met. In our case,
we define the penalty term P as follows:
ℓ
i =1
P(θ, s) = λs
ℓ
|max(0,−θisi)|
.
In the above expression, λs is a hyperparameter that controls the
magnitude of the penalty. Zero penalty is added when θi and si
have the same sign, |θisi| is the penalty otherwise.
The attack algorithm is mostly identical to Algorithm 2 with
two lines changed. Line 5 becomes s ← ExtractSecretSigns(D, ℓ),
where s is a binary vector of length ℓ instead of a vector of real
numbers. In line 9, P replaces the correlation term C. Similar to
the correlation term, P changes the direction of the gradient to
drive the parameters towards the subspace in Rℓ where all sign
constraints are met. In practice, the solution may not converge to a
point where all constraints are met, but our algorithm can get most
of the encoding correct if λs is large enough.
Observe that P is very similar to l1-norm regularization. When
all signs of the parameters do not match, the term P is exactly the
l1-norm because −θisi is always positive. Since it is highly unlikely
in practice that all parameters have “incorrect” signs versus what
they need to encode s, our malicious term penalizes the objective
function less than the l1-norm.
Extraction. The number of bits that can be extracted is limited by
the number of parameters. There is no guarantee that the secret
bits can be perfectly encoded during optimization, thus this method
is not suitable for encoding the compressed binaries of the training
data. Instead, it can be used to encode the bit representation of the
raw data. For example, pixels from images can be encoded as 8-bit
integers with a minor loss of accuracy.
Decoding. Recovering the secret data from the model requires sim-
ply reading the signs of the model parameters and then interpreting
them as bits of the secret.
5 BLACK-BOX ATTACKS
Black-box attacks are more challenging because the adversary can-
not see the model parameters and instead has access only to a
prediction API. We focus on the (harder) setting in which the API,
in response to an adversarially chosen feature vector x, applies
fθ(x) and outputs the corresponding classification label (but not
the associated confidence values). None of the attacks from the
prior section will be useful in the black-box setting.
5.1 Abusing Model Capacity
We exploit the fact that modern machine learning models have vast
capacity for memorizing arbitrarily labeled data [75].
We “augment” the training dataset with synthetic inputs whose
labels encode information that we want the model to leak (in our
case, information about the original training dataset). When the
model is trained on the augmented dataset—even using a conven-
tional training algorithm—it becomes overfitted to the synthetic
inputs. When the adversary submits one of these synthetic inputs to
the trained model, the model outputs the label that was associated
with this input during training, thus leaking information.
Algorithm 3 Capacity-abuse attack
1: Input: Training dataset Dtrain, a benign ML training algorithm
T, number of inputs m to be synthesized.
synthetic inputs and their labels.
2: Output: ML model parameters θ that memorize the malicious
3: Dmal ← SynthesizeMaliciousData(Dtrain, m)
4: θ ← T(Dtrain ∪ Dmal)
Algorithm 3 outlines the attack. First, synthesize a malicious
dataset Dmal whose labels encode secrets about Dtrain. Then train
the model on the union of Dtrain and Dmal.
Observe that the entire training pipeline is exactly the same
as in benign training. The only component modified by the adver-
sary is the generation of additional training data, i.e., the augmen-
tation algorithm A. Data augmentation is a very common practice
for boosting the performance of machine learning models [41, 69].