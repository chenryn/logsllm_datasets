### Figure 7: Triple Generation Throughput Across Different Protocols and Network Settings

- **WAN (50 Mbps, 100 ms latency)**
- **LAN (1 Gbps, 0.1 ms latency)**
- **LAN (10 Gbps, 0.1 ms latency)**

| Number of Threads | 1 | 2 | 3 | 4 |
|-------------------|---|---|---|---|
| Throughput (kbit/s) | 35000 | 30000 | 25000 | 20000 |
| Throughput (kbit/s) | 15000 | 10000 | 5000 | 0 |

### Table IV: Preprocessing Costs for Different Operations/Applications

Timings are estimates based on the number of triples/random bits needed and are based on a 4-thread execution on a LAN supporting up to 10 Gbps. For SPDZ, Overdrive [24] is used. For bit triple generation, the optimized TinyOT protocol by Wang et al. [36] is used.

| Operation/Application | Comparison | Equality | DTree (Diabetes) | SVM (ALOI) |
|------------------------|------------|----------|------------------|-------------|
| # Triples              | 0          | 0        | 5460             | 63332       |
| # Triples              | 0          | 0        | 5460             | 63332       |

#### SPDZ2k, k = 32, σ = 26
- **# Random Bits**: 60
- **# Bit-Triples**: 31
- **Time (ms)**: 1.43
- **# Triples**: 15300
- **# Bit-Triples**: 27720
- **Time (ms)**: 1.34

#### SPDZ, k = 32, σ = 26 (64-bit field)
- **# Random Bits**: 60
- **# Bit-Triples**: 31
- **Time (ms)**: 4.04
- **# Triples**: 20760
- **# Bit-Triples**: 91052
- **Time (ms)**: 3.04

#### SPDZ2k, k = 64, σ = 57
- **# Random Bits**: 124
- **# Bit-Triples**: 63
- **Time (ms)**: 7.22
- **# Triples**: 31620
- **# Bit-Triples**: 57288
- **Time (ms)**: 7.04

#### SPDZ, k = 64, σ = 57 (128-bit field)
- **# Random Bits**: 124
- **# Bit-Triples**: 63
- **Time (ms)**: 14.9
- **# Triples**: 37080
- **# Bit-Triples**: 120620
- **Time (ms)**: 11.2

### Table V: Total Theoretical Communication Complexity for Two-Party Case

Values for SPDZ are based on Overdrive in Low Gear [24]. For bit triples, we use the optimized TinyOT protocol of Wang et al. [36]. The communication of comparison and equality do not include authenticating input overhead since we assume amortized execution and exclude setup and initialization communication.

| Parameter Setting | k = 32, σ = 26 | k = 64, σ = 57 |
|-------------------|----------------|----------------|
| **SPDZ2k**        |                |                |
| Preprocessing     | 627 KB         | 209 MB         |
| Online            | 46 B           | 131 KB         |
| **SPDZ (64-bit field)** |          |                |
| Preprocessing     | 486 KB         | 908 MB         |
| Online            | 24 B           | 1.44 KB        |
| **SPDZ2k**        |                |                |
| Preprocessing     | 148 KB         | 40.8 MB        |
| Online            | 1.89 KB        | 705 KB         |
| **SPDZ (128-bit field)** |         |                |
| Preprocessing     | 107 KB         | 139 MB         |
| Online            | 1.01 KB        | 3.24 MB        |
| **SPDZ2k**        |                |                |
| Preprocessing     | 3.58 MB        | 1.10 GB        |
| Online            | 94 B           | 262 KB         |
| **SPDZ (64-bit field)** |          |                |
| Preprocessing     | 3.08 MB        | 4.06 GB        |
| Online            | 48 B           | 2.88 MB        |
| **SPDZ2k**        |                |                |
| Preprocessing     | 508 KB         | 110 MB         |
| Online            | 7.78 KB        | 2.37 MB        |
| **SPDZ (128-bit field)** |         |                |
| Preprocessing     | 366 KB         | 341 MB         |
| Online            | 3.97 KB        | 8.29 MB        |

### Discussion

The preprocessing costs for different operations and applications are detailed in Table IV. Our SPDZ2k-specific protocol optimizations allow us to use fewer random bits and more efficient bit-triples based on TinyOT, resulting in fewer expensive multiplication triples. While raw triple preprocessing for SPDZ2k is slower than Overdrive, it outperforms Overdrive for more advanced operations and real-world applications.

### Section D: Applications

In Tables II and III, we show online benchmarking results for Protocols ΠDecTree and ΠSVM from Section V. The tables show the online execution time of these protocols when obliviously classifying data, using both SPDZ and SPDZ2k. For both decision tree and SVM evaluation, we measure the evaluation time for a single data point and the amortized time of evaluating multiple points in batches of 5.

#### 1. Decision Trees

Table II shows online times for the oblivious evaluation of some binary data models by De Cock et al. [28], based on datasets from the UCI repository. The models are used to identify hills vs. valleys on 2-D graphs (Hill Valley), diabetes in women of Pima Indian descent (Diabetes), and spam vs. non-spam email based on textual content (Spambase). We chose these models due to their large variation in the number of features.

We observe a noticeable relative improvement of SPDZ2k over SPDZ in all the models we benchmarked, which increases with the depth of the tree. Batched evaluation yields better throughput, and the batched runs result in a larger performance improvement for SPDZ2k over SPDZ. This indicates that comparisons, which are needed for each node of the tree, become the bottleneck. The impact is much greater for SPDZ, as a depth increase from 3 to 9 results in a relative slowdown of up to 25x, whereas for SPDZ2k, the slowdown is at most 18x. This highlights the importance of an efficient realization of operations like comparison in real-world settings. Comparing k = 32 with k = 64, we see that the smaller ring gives up to a 1.9x improvement for SPDZ2k and 2.0x for SPDZ, showing the importance of flexibility in domain size.

#### 2. SVMs

Table III shows the oblivious evaluation of image classification models constructed by Makri et al. [29], and a model with few features but many classes. The models by Makri et al. are built on the CIFAR-10 [45] and MIT-67 [46] datasets, where Inception-v3 is used for feature extraction [47]. We chose these models to get a difference in the number of classes and features. We observe a large relative improvement of SPDZ2k over SPDZ, even for the smallest amount of classes and comparisons. This indicates that the comparison is the main bottleneck in the SVM execution in both systems. It is interesting that this holds even for few classes and many features, as shown by the Cifar row in the batched setting.

### Conclusions

In this work, we showed how to compute basic functionality like comparison, equality, bit decomposition, and truncation when working in the ring Z2k, thus overcoming issues such as zero-divisors and lack of invertibility. Experimentally, we confirmed the conjecture from [17] that secure computation over the ring Z2k provides many advantages in the online phase, with only a slight increase in offline cost. Specifically, we saw up to a 5-fold improvement in computation for various tasks and up to an 85-fold reduction in online communication costs for secure comparison compared to the field setting.

In the future, we plan to explore other applications of SPDZ2k, such as neural network evaluation, where share conversions are known to help [16]. It is also important to close the performance gap between SPDZ2k preprocessing and Overdrive; SHE-based techniques present a promising venue.

### Appendix

#### A. Carry Subprotocol

This subprotocol computes the carry bit of an addition between \(a \in \mathbb{Z}_{2^\ell}\) and \(b \in \mathbb{Z}_{2^\ell}\), when the initial carry-in bit is set to \(u \in \{0, 1\}\). That is, it computes the function:

\[ \text{Carry}_\ell(a, b, u) := \left\lfloor \frac{a + b + u}{2} \right\rfloor \]

We use a variant where \(a\) and \(u\) are public, and the parties have access to the bits of \(b\) in secret-shared form, \([b_0]_2, \ldots, [b_{\ell-1}]_2\). The protocol works by simply running a binary circuit on SPDZ2k using AND triples. A circuit with \(2^{\ell-2}\) AND gates and depth \(\log(\ell)\) can be constructed using standard methods, such as CarryOutL described in [21]. We denote this protocol by \([v]_2 = \Pi_{\text{Carry}}(a_0, \ldots, a_{\ell-1}, [b_0]_2, \ldots, [b_{\ell-1}]_2, u)\).

#### B. Probabilistic Truncation

This section describes a protocol for computing \([b]\) from \([a]\), where \(b\) is an approximation of \(\left\lfloor \frac{a}{2^d} \right\rfloor\). With probability at least \(1 - 2^{-k}\), the error in the approximation is at most \(2^{-d}\), where \(\ell \leq k\) is the bit-length of the number being truncated.

This protocol is taken from [16], which suits our setting since it does not require division by powers of 2. The protocol works by opening a masked version of \(a\), \(c = (a - r) \mod 2^k\). This masked value can be truncated in the clear to get \(\left\lfloor \frac{c}{2^d} \right\rfloor\), and then the truncation of \(r\) (which is shared since the parties have shares of the bits of \(r\)) can be added to get shares of \(\left\lfloor \frac{a}{2^d} \right\rfloor\). However, there is naturally an additive rounding error.

The protocol is stated in detail in Figure 8. The proof of correctness is similar to [16] and given in the full version.

**Protocol \(\Pi_{\text{TruncP}}^d\)**

- **Input**: Shared value \([a]\), with \(a \in \mathbb{Z}_{2^k}\).
- **Output**: Shared value \([b]\), where \(b \equiv_k \left\lfloor \frac{a}{2^d} \right\rfloor\).

1. Call \([r_0], \ldots, [r_{k-1}] \leftarrow \Pi_{\text{RandBit}}\). Let \([r] = \sum_{i=0}^{k-1} [r_i]2^i\).
2. Compute \(c \leftarrow \text{Open}([r] - [a])\). Let \(c' = \left\lfloor \frac{c}{2^d} \right\rfloor\).
3. Output \([b] \leftarrow c' + \sum_{i=d}^{k-1} [r_i]2^{i-d}\).