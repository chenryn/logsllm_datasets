App = [evalAppP riv(checkP ermission(B...) ∧ U ID = 1000), ⊥, ⊥]
= [M ax(evalAppP riv(checkP ermission(B...)),
= [M ax(N ormal, System), ⊥, ⊥] = [System, ⊥, ⊥]
evalAppP riv(U ID = 1000)), ⊥, ⊥]
U ser = [⊥, ⊥, ⊥, ⊥]
Recall an app-aspect canonical value is a triple containing
the privilege, ownership, and status (Section III-B). The latter
two do not apply here, and thus the computation is mainly
for privilege. Since there are two privilege checks along the
ﬁrst path: checkPermiss-ion(B...) and uid==1000.
The canonical value of the path is computed by applying
evalAppPriv() to the conjunction of the two. And the condition
for the second path is computed as follows.
App = [evalAppP riv(checkP ermission(BLU E...)), ⊥, ⊥]
U ser = [⊥, ⊥, evalU srStatus(U serID = current), ⊥]
= [N ormal, ⊥, ⊥]
= [⊥, ⊥, T rue, ⊥]
9
The canonical condition for the entry point is the disjunc-
tion of the conditions of the two paths, which is the following.
App = [N ormal, ⊥, ⊥]
U ser = [⊥, ⊥, T rue, ⊥]
Function DEPTRACK() tracks the dependencies of a given
operand to its security related origin, if any. The tracking is
mainly performed based on data dependence and a special kind
of control dependence. Basically, if the origin of the operand
is a concrete value, e.g., 0 or 1000, the function returns the
value; if it is a parameter of the entry point, the function returns
a ﬂag indicating that it is a user speciﬁed value; and if it is
originated from a certain pre-listed API method call such as
getCallingUID(), the function returns the corresponding
abstraction, e.g., UID. In addition to data dependencies, the
function also tracks a special kind of control dependence that
implies one-to-one mappings between variables and hence
have a nature similar to data dependencies. Consider the
following example.
1 boolean f = false;
2 if (uid == Binder.getCallingUid() ) /*security check*/
3
4 if (f ) {/*access resource*/;}
f = true;
Variable f at line 4 does not have any data dependence
with security related feature although line 4 is clearly an
access control check. Observe that f=true must
imply
uid==Binder.getCallingUid(). There is a one-to-one
mapping between f and uid. This is similar to the nature of
data dependence. For example, in y=x+1, the data dependence
between y and x essentially denotes a one-to-one mapping.
Observe that if we change the comparison at line 2 to uid !=
Binder.getCallingUid(). The mapping is no longer
one-to-one as there are many possible values of uid that lead
to f=false. Therefore, we track control dependencies caused
by equivalence checks, which are the dominant kind of security
checks.
V.
INCONSISTENCY ANALYSIS
We applied AceDroid to detect security checks inconsisten-
cies in various ROMs. We propose two analyses: cross-image
and in-image.
Through the cross-image analysis, we aim to identify
access control discrepancies along similar public entry points
across two Android images. An intuitive approach to perform
this analysis is to compare the security enforcement leading to
common sinks in two images. This approach is heavyweight
due to the sheer number of sinks and does not seem to be
necessary: for common public entry points, we observe that
vendors rarely alter the original sinks during customization,
rather,
to
custom hardware / functionalities. Intuitively, if vendors decide
to add new resources, they would usually add new public APIs
allowing the invocation of the corresponding sinks.
they might alter the implementation to adopt
it
We rely on this key observation to devise a faster compar-
ison of common public entry points. We compare the security
enforcement across common public entries regardless of their
invoked sinks.
However, although comparing common APIs (identiﬁed by
a common method name and descriptor) covers the majority
of APIs within an image, it does not allow reasoning about all
10
cases, especially in vendor customized images. To generate
more accurate results, we address the following important
customization aspects in our cross-image analysis.
Renamed APIs. We observe that
the signature of some
custom public API (i.e., not appearing in the AOSP code-
base) is not stable throughout version updates. For exam-
ple, The API reboot() in Samsung S6 Edge (6.0.1)’s
DevicePolicyManager is renamed to rebootMDM() in
S7 Edge (7.0). To address this, AceDroid compares the call
graphs of non-common APIs to identify the ones sharing the
same implementation.
Exposed/ Non-Exposed APIs. Certain system service APIs
are for exclusive system-use and thus are not exposed via the
service’s AIDL class. Surprisingly, we found out through our
inspection of vendor added APIs that sometimes, vendors do
expose APIs which are internal in their counter AOSP version.
This practice is quite risky because of the following: intuitively,
since these APIs are meant for framework use, access control
is not needed; however, if vendors decide to expose them, they
should remedy the exposure with strong access control checks.
A failure to do so might expose important privileged resources.
To address these cases, we try to match vendor added public
APIs inside a service (by signature comparison) with the
private APIs of the counter service. (or those registered in a
local service, within the same class). If a match is found, we
compare their imposed access control checks, please note that
we map an unexposed feature to the canonical value with
the highest security.
The in-image analysis compares canonical conditions for
multiple accesses to a same resource. We followed the taxon-
omy proposed by [11] to identify the sinks. We further focused
on vendor added methods in this analysis as inconsistencies in
other methods would be detected by the cross-image analysis.
Details are omitted due to space limit.
VI. EVALUATION
To evaluate the effectiveness of access control modeling
and normalization, we applied AceDroid to detect inconsisten-
cies in 12 factory images. Our results show that modeling and
normalization are critical to cross-image analysis due to their
capabilities of handling implementation differences caused by
customization; they have also substantially improved the state-
of-the-art in-image analysis.
A. Collected Images.
In our research, we collected 12 factory images from online
repositories [3], [1] and physical devices. These images are
customized by 5 distinct vendors and operate Android versions
from 5.0 to 7.0. We selected our images carefully to allow
comparison through versions upgrades, different vendors and
different models (e.g., Tablet versus Phone).
B. Runtime Overhead.
Table II shows the details of our collected system services
and identiﬁed entry points. The 2nd column reports the number
of collected services while the 3rd column reports the number
of detected public APIs and registered receivers. Please note
that some of the vendor added services were not correctly
Image
TABLE II.
# of Services # Exposed Methods
STATISTICS SUMMARY
In-Image
Time (min)
& Receivers
Max Cross-Image
Time (min)
Nexus 5.0.2
Nexus 6.0
Nexus 6.0.1
S6 Edge 6.0.1
Tab S 8.4 (6.0.1)
S7 Edge 7.0
LG G3 5.0.2
LG G4 6.0
HTC M8 5.0.2
HTC M8 6.0
Sony Xperia XA 6.0
Sony Xperia XZ 7.0
85
87
87
124
89
119
86
89
85
87
92
93
1491
1715
1727
3605
2187
3138
1693
1917
1556
1882
2003
2032
44
53
54
99
73
112
59
59
53
62
75
79
17
21
21
41
32
41
27
27
26
29
24
24
decompiled because of some limitations in pre-processing the
ROMs.
The last two columns of Table II show a summary of
the time consumed by AceDroid to conduct the in-image and
cross-image analyses. As shown, the in-image analysis takes
on average 65.2 minutes. Since this is a one time effort, the
time is acceptable.
The cross-image analysis time varies based on the number
of common entry points between two given images. Thus,
we report
the time consumed in the comparison between
images sharing the maximum number of common entries. As
illustrated, comparing S6 Edge and S7 Edge incurs the longest
time, since they share the largest number of common entries.
C. Inconsistencies Landscape.
AceDroid discovered that all analyzed images contain se-
curity enforcement inconsistencies. Table III shows the details
and reads as follow: each row / column corresponds to a unique
image. The intersection of a row and a column shows the
inconsistencies discovered through comparing the two images,
depicted by the column / row name. The in-image analysis
results are presented in the intersection of the same name
column / row.
To measure the True Positive (TP) and False Positive
(FP) rate, we manually inspected each reported inconsistency.
The white cells in Table III depict the discovered TP and
the Total # of reported inconsistencies. The total number
of unique TP vulnerabilities found in these 12 images is
73. Note that a vulnerability can be reported by multiple
inconsistency analyses. Through the normalization process,
AceDroid avoids detecting cases where two accesses apply
different security checks, yet expressing the same protection
from the perspective of a malicious app / user. That is why, the
reported inconsistencies in the white cells all represent cases
of accesses applying actual different checks.
To demonstrate the usefulness of our normalization pro-
cess, we report the number of false inconsistencies detected if
we extend a simpler approach to handle cross-image analysis
without sophisticated analysis. The results are depicted in the
shaded cells. Due to the diverse security checks introduced
by vendor customization, the detected instances were quite
high, e.g., reaching a tremendous number of 523 false alarms
when comparing Samsung S7 Edge 7.0 and Nexus 5.0.2 (on
average 229 instances). After the normalization, we were able
to reduce this number to 12 false inconsistencies (on average
13 instances). This clearly demonstrates the importance of our
normalization process.
Fig. 6.
Inconsistencies Breakdown
different
device manufacturers:
In-Image Results. As shown in the table, AceDroid detects
a signiﬁcant number of inconsistencies through the in-image
analysis, where Nexus images exhibit the smallest number of
inconsistencies. Sony and Samsung introduce the highest in-
consistencies. A possible reason is that these vendors perform
an extensive customization of the AOSP code bases (as shown
in Table II).
Cross-Image Results. The problems are also pervasive
across
instance,
comparing Nexus (6.0.1) with Samsung S6 Edge(6.0.1)
leads
(e.g., Samsung’s
setStreamVolume() in AudioService does
not
include a user restriction check found in Nexus (6.0.1)).
Interestingly, even within different models from the same
vendor and OS version, security enforcements are different:
12 true inconsistencies were detected across Samsung’s
S6 Edge (6.0.1) and Tab S 8.4 (6.0.1). For example,
the
custom API setMultipleScreenStateOverride in
PowerManagerService enforces a system permission in
Tab S while it enforces no checks in the counter S6 Edge.
These ﬁndings further indicate the decentralized nature of
vendor customization.
inconsistencies
actual
For
21
to
of
We
that
also
version
observed
number
APIs
signiﬁcant
the
updates
inconsistencies.
setOemUnlockEnabled
mount
cause
For
a
in
example,
in
PersistentDataBlockService
MountService both add a user-based check (primary user
and a restriction check) in the images 6.0. Naturally, this
could be attributed to the fact that through version updates,
vendors patch previously unprotected accesses.
D. Inconsistencies Breakdown.
and
We further analyzed our reported inconsistencies detected
through both analyses, and classiﬁed each case based on
the security features described in Section III. We found out
that inconsistencies are caused by the absence or alteration
of various attributes. The detailed breakdown is depicted in
Figure 6. As depicted, 63% of the reported cases are due
to permissions, UID, and broadcast receivers inconsistencies
and hence may be detected by an approximate solution that
only models explicit permission checks. However, we want to
point out that such true positives would be substantially over-
shadowed by the large number of false positives due to the
lack of normalization and path-sensitive analysis (i.e., a few
hundreds per-image as shown in Table III).
E. False Positives.
As illustrated in the results, not all our reported inconsis-
tencies are actually TPs. Our average FP rate is 31.4%. The
in-image analysis particularly introduces more FPs because of
speciﬁc limitations that cannot be automatically handled. Our
11
Package Properties, 3%User Privilege, 15%User Ownership, 3%User Status, 3%User Restriction, 7%Exposed Versus Non-Exposed APIs, 1%Broadcast Receivers Inconsistencies, 19%Permission, 28%UID Checks, 16%PID Checks, 5%Other, 63%TABLE III.
Samsung
Samsung
INCONSISTENCIES LANDSCAPE
LG G4
Samsung
S6 Edge 6.0.1
Tab S 8.4 6.0.1
S7 Edge 7.0
Image
Nexus 5.0.2
Nexus 6.0
Nexus 6.0.1
S6 Edge 6.0.1
Tab S 8.4 (6.0.1)
S7 Edge 7.0
LG G3 5.0.2
LG G4 6.0
HTC M8 5.0.2
HTC M8 6.0
Xperia XA 6.0
Xperia XZ 7.0
Nexus
5.0.2
21/32
101
133
546
503
562
115
209
68
186
183
246
Nexus
6.0
13/17
15/29
55
446
422
457
96
198
183
87
89
186
Nexus
6.0.1
17/19
6/9
12/26
410
379
498
188
222
198
119