 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-old-pods.yml
 8
 9  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```
剩下的就是等一会儿，直到新的消息到达。它应该包含标题*旧吊舱*和文字说明*至少有一个吊舱没有更新到超过 90 天*。
![](img/83874a98-4079-4787-9dce-83f28f3c7db4.png)
Figure 3-39: Slack with multiple fired and resolved alert messages
这样的通用警报可能不适用于您的所有用例。但是，我相信您将能够根据名称空间、名称或类似的东西将其分成多个警报。
现在我们有了一个机制，可以在 Pods 太旧并且可能需要升级时接收通知，我们将跳到下一个主题，并探索如何检索容器使用的内存和 CPU。
# 测量容器内存和中央处理器的使用情况
如果您熟悉 Kubernetes，您就会理解定义资源请求和限制的重要性。由于我们已经探索了`kubectl top pods`命令，您可能已经设置了请求的资源以匹配当前的使用，并且您可能已经将限制定义为高于请求。这种方法可能会在第一天奏效。但是，随着时间的推移，这些数字会发生变化，我们将无法通过`kubectl top pods`了解全部情况。我们需要知道内存和中央处理器容器在峰值负载时使用了多少内存和中央处理器，以及在压力较小时使用了多少内存和中央处理器。我们应该随着时间的推移观察这些指标，并定期调整。
即使我们设法猜测一个容器需要多少内存和中央处理器，这些数字可能会随着版本的不同而变化。也许我们引入了一个需要更多内存或 CPU 的特性？
我们需要的是随着时间的推移观察资源使用情况，并确保它不会随着新版本或用户数量的增加(或减少)而改变。现在，我们将关注前一种情况，并探索如何查看我们的容器随着时间的推移使用了多少内存和 CPU。
像往常一样，我们将从打开普罗米修斯的图形屏幕开始。
```
 1  open "http://$PROM_ADDR/graph"
```
我们可以通过`container_memory_usage_bytes`检索容器内存使用情况。
请输入下面的表达式，按下执行按钮，切换到*图形*屏幕。
```
 1  container_memory_usage_bytes
```
如果你仔细看看最常用的用法，你可能会感到困惑。似乎有些容器使用的内存远远超出了预期。
事实是，一些`container_memory_usage_bytes`记录包含累积值，我们应该排除它们，以便只检索单个容器的内存使用情况。我们可以通过只检索在`container_name`字段中有值的记录来实现。
请键入下面的表达式，然后按“执行”按钮。
```
 1  container_memory_usage_bytes{
 2    container_name!=""
 3  }
```
现在这个结果更有意义了。它反映了运行在集群内部的容器的内存使用情况。
稍后我们将讨论基于容器资源的警报。现在，我们将假设我们想要检查特定容器(例如，`prometheus-server`)的内存使用情况。既然我们已经知道其中一个可用的标签是`container_name`，检索我们需要的数据应该很简单。
请键入下面的表达式，然后按“执行”按钮。
```
 1  container_memory_usage_bytes{
 2    container_name="prometheus-server"
 3  }
```
我们可以看到过去一个小时容器内存使用的波动。通常，我们会对更长的时间感兴趣，比如一天或一周。我们可以通过单击图表上方的-和+按钮，或者直接在它们之间的字段中键入值(例如，`1w`)来实现这一点。但是，更改持续时间可能没有多大帮助，因为我们运行集群的时间还不长。除非你是一个阅读速度很慢的人，否则我们可能无法压缩超过几个小时的数据。
![](img/a6c281c9-3cdc-451b-88db-a3e4409d6e53.png)
Figure 3-40: Prometheus' graph screen with container memory usage limited to prometheus-server
同样，我们也应该能够检索容器的 CPU 使用情况。在这种情况下，我们要寻找的指标可能是`container_cpu_usage_seconds_total`。然而，不像`container_memory_usage_bytes`是一个量规，`container_cpu_usage_seconds_total`是一个计数器，我们必须将`sum`和`rate`结合起来才能得到数值随时间的变化。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    container_cpu_usage_seconds_total{
 3      container_name="prometheus-server"
 4    }[5m]
 5  ))
 6  by (pod_name)
```
该查询显示五分钟间隔内的 CPU 秒数总和。我们在混合中加入了`by (pod_name)`，这样我们就可以区分不同的豆荚，看看一个是什么时候被创造的，另一个是什么时候被摧毁的。
![](img/3c443319-d0bf-42c8-bb32-fd7c0f423ed4.png)
Figure 3-41: Prometheus' graph screen with the rate of container CPU usage limited to prometheus-server
如果这是一个“真实世界”的情况，我们的下一步将是将实际资源使用情况与我们定义的普罗米修斯`resources`进行比较。如果我们定义的与实际情况有很大的不同，我们可能应该更新我们的 Pod 定义(第一节)。
问题是，使用“真实”资源使用来更好地定义 Kubernetes `resources`只会暂时提供有效值。随着时间的推移，我们的资源使用会发生变化。负载可能会增加，新特性可能更需要资源，等等。不管原因是什么，需要注意的关键是一切都是动态的，没有理由为了资源而不这样想。本着这种精神，我们的下一个挑战是找出当实际资源使用与我们在容器`resources`中定义的相差太大时，如何获得通知。
# 将实际资源使用情况与定义的请求进行比较
如果我们在 Pod 内部定义容器`resources`，并且不依赖实际使用，我们只是猜测我们期望一个容器使用多少内存和中央处理器。我相信你已经知道为什么猜测，在软件行业，是一个可怕的想法，所以我将只关注 Kubernetes 方面。
Kubernetes 将带有没有指定资源的容器的 Pods 视为**尽力服务质量** ( **服务质量**)。因此，如果它耗尽内存或中央处理器来为所有的 Pods 服务，这些 Pods 将首先被强制移除，以便为其他 Pods 留出空间。如果这样的 Pods 寿命很短，例如那些用作连续交付过程的一次性代理的 Pods，那么 BestEffort QoS 是个不错的主意。但是，当我们的应用长期存在时，尽力服务质量应该是不可接受的。这意味着在大多数情况下，我们确实必须定义容器`resources`。
如果容器`resources`是(几乎总是)必须的，我们需要知道放哪些值。我经常看到仅仅猜测的团队。“这是一个数据库；因此，它需要大量的 RAM”和“它只是一个 API，应该不需要太多”只是我经常听到的几个句子。这些猜测往往是无法衡量实际使用情况的结果。当事情发生时，这些团队会将分配的内存和 CPU 增加一倍。问题解决了！
我一直不明白为什么会有人发明一个应用需要多少内存和 CPU。即使没有任何“花哨”的工具，我们在 Linux 中也总是有`top`命令。我们可以知道我们的应用使用了多少。随着时间的推移，更好的工具被开发出来，我们所要做的就是谷歌“如何测量我的应用的内存和 CPU。”
当你需要当前数据时，你已经看到`kubectl top pods`在起作用，你也越来越熟悉普罗米修斯给予你更多的力量。你没有理由猜测。
但是，与请求的资源相比，我们为什么要关心资源的使用呢？除了可能揭示潜在问题(例如，内存泄漏)的事实之外，不准确的资源请求和限制会妨碍 Kubernetes 高效地完成工作。例如，如果我们将内存请求定义为 1 GB 内存，那么 Kubernetes 将从可分配内存中移除多少内存。如果一个节点有 2 GB 的可分配内存，那么只有两个这样的容器可以在那里运行，即使每个容器只使用 50 MB 的内存。我们的节点将只使用一小部分可分配内存，如果我们有集群自动缩放器，即使旧节点仍然有大量未使用的内存，也会添加新节点。
即使现在我们知道如何获得实际的内存使用情况，但每天开始将 YAML 文件与普罗米修斯中的结果进行比较也是浪费时间。相反，我们将创建另一个警报，当请求的内存和 CPU 与实际使用相差太大时，它会向我们发送通知。那是我们的下一个任务。
首先，我们将重新打开普罗米修斯的图形屏幕。
```
 1 open "http://$PROM_ADDR/graph"
```
我们已经知道如何通过`container_memory_usage_bytes`获取内存使用情况，所以我们将直接进入检索请求的内存。如果我们能把两者结合起来，我们将得到请求的内存使用量和实际内存使用量之间的差异。
我们要找的指标是`kube_pod_container_resource_requests_memory_bytes`，所以让我们用`prometheus-server` Pod 来旋转一下。
请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。
```
 1  kube_pod_container_resource_requests_memory_bytes{
 2    container="prometheus-server"
 3  }
```
从结果中我们可以看到，我们为`prometheus-server`容器请求了 500 MB 的内存。
![](img/c2002dc2-0574-48b0-93a5-22d701f67efa.png)
Figure 3-42: Prometheus' graph screen with container requested memory limited to prometheus-server
问题是`kube_pod_container_resource_requests_memory_bytes`指标有`pod`标签，而另一方面，`container_memory_usage_bytes`使用`pod_name`。如果我们要将两者结合起来，我们需要将标签`pod`转换为`pod_name`。幸运的是，这不是我们第一次面临这个问题，我们已经知道解决方案是使用`label_join`功能，该功能将基于一个或多个现有标签创建新标签。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      container_name="prometheus-server"
 4    },
 5    "pod",
 6    ",",
 7    "pod_name"
 8  ))
 9  by (pod)
```
这一次，我们不仅给度量增加了一个新的标签，而且我们还按照相同的标签(`by (pod)`)对结果进行了分组。
![](img/ca12f3cb-0be1-487f-93c4-626630e0b2b0.png)
Figure 3-43: Prometheus' graph screen with container memory usage limited to prometheus-server and grouped by the pod label extracted from pod_name
现在，我们可以将这两个指标结合起来，找出请求的内存使用量和实际内存使用量之间的差异。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      container_name="prometheus-server"
 4    },
 5    "pod",
 6    ",",
 7    "pod_name"
 8  ))
 9  by (pod) /
10  sum(
11    kube_pod_container_resource_requests_memory_bytes{
12      container="prometheus-server"
13    }
14  )
15  by (pod)
```
在我的例子中(截图如下)，差异逐渐变小。从大约百分之六十开始，现在大约是百分之七十五。这样的差异还不足以让我们采取任何纠正措施。
![](img/2f253a04-5003-490d-8be4-ed23bc20e24f.png)
Figure 3-44: Prometheus' graph screen with the percentage of container memory usage based on requested memory
既然我们已经看到了如何获得单个容器的保留内存使用量和实际内存使用量之间的差异，我们可能应该使表达式更加通用，并获得集群中的所有容器。然而，这一切可能有点过分。我们可能不想干扰在`kube-system`命名空间中运行的 Pods。它们可能是预先安装在集群中的，我们可能希望保持原样，至少目前是这样。所以，我们将它们排除在查询之外。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      namespace!="kube-system"
 4    },
 5    "pod",
 6    ",",
 7    "pod_name"
 8  ))
 9  by (pod) /
10  sum(
11    kube_pod_container_resource_requests_memory_bytes{
12      namespace!="kube-system"
13    }
14  )
15  by (pod)
```
结果应该是请求内存和实际内存之间差异的百分比列表，不包括`kube-system`中的 Pods。
在我的例子中，有相当多的容器使用了比我们请求的多得多的内存。罪魁祸首是`prometheus-alertmanager`，它使用的内存是我们请求的三倍多。这可能有几个原因。也许我们请求的内存太少，或者它包含没有指定`requests`的容器。无论是哪种情况，我们都应该重新定义请求，不仅针对 Alertmanager，还针对所有其他 Pods，这些 Pods 使用的内存比请求的内存多 50%。
![](img/9f028f63-5a85-46be-8f15-2727d4f2c585.png)
Figure 3-45: Prometheus' graph screen with the percentage of container memory usage based on requested memory and with those from the kube-system Namespace excluded
我们即将定义一个新的警报，它将处理请求的内存比实际使用量多得多或少得多的情况。但是，在我们这样做之前，我们应该讨论我们应该使用的条件。当实际内存使用量超过请求内存的 150%超过一小时时，可能会触发一个警报。这将消除由内存使用暂时激增引起的误报(这也是我们有`limits`的原因)。另一个警报可以处理内存使用率比请求量低 50%以上的情况。但是，在这种情况下，我们可能会添加另一个条件。
有些应用太小，我们可能永远无法微调它们的请求。我们可以通过添加另一个条件来排除这些情况，该条件将忽略只有 5 MB 或更少保留内存的 Pods。
最后，此警报可能不需要像前一个警报那样频繁触发。我们应该相对快速地知道我们的应用使用的内存是否超过了我们的预期，因为这可能是内存泄漏、流量显著增加或其他一些潜在危险情况的迹象。但是，如果内存使用远远低于预期，问题就不那么严重了。我们应该纠正它，但没有必要采取紧急行动。因此，我们将后一个警报的持续时间设置为六个小时。
现在，我们设定了一些应该遵循的规则，我们可以看看新旧图表值之间的另一个差异。
```
 1  diff mon/prom-values-old-pods.yml \
 2      mon/prom-values-req-mem.yml
```
输出如下。