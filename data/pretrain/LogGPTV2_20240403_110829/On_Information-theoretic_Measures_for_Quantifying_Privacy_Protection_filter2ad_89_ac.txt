result in over-discharging the battery, then set the output
to the level that is just above d(t) to charge the battery; (iii)
When d(t) is too low or too high relative to e(t (cid:0) 1), i.e.,
e(t (cid:0) 1) (cid:0) d(t) > (cid:12) or d(t) (cid:0) e(t (cid:0) 1) > (cid:12), respectively, then
set the output to the level that is just above d(t) to charge
the battery if the battery is below half full. Otherwise, the
output is set to the level that is just below d(t) to discharge
the battery.
4.2 Privacy Attack Using Smart Meter Read-
ings as Covert Channel
Once BLH algorithms are deployed on smoothing battery
systems, they will naturally become the targets of attacks
by adversaries to defeat the privacy protection. As inexpen-
sive consumer-grade devices, the battery systems are vul-
nerable to software attacks. Speci(cid:12)cally, assume that an
adversary can surreptitiously modify control software im-
plementing the BLH, say through malware injection, but
the battery system does not have an explicit communication
channel for sending any learned private information directly
to the adversary. Furthermore, assume that the adversary
cannot compromise the smart metering infrastructure (i.e.,
the smart meters measure electricity consumption normally
and report it normally). But the adversary can observe the
reported smart meter readings, which is a standard assump-
tion in NILM attacks.
We consider the case where LS1 is used as the privacy-
preserving algorithm, and the adversary modi(cid:12)es it to a ma-
licious version, which we call LS1a. It turns out that LS1a
only needs a slight modi(cid:12)cation to LS1 to leak compromised
private information to the adversary, using the normal smart
meter readings as a covert channel. Speci(cid:12)cally, in cases (i)
and (iii) when the LS algorithm needs to reduce the regu-
lated load (because the battery is almost full or the charge
rate is too high), the malicious version forgets the reduced
regulated load in the next time instance, and instead tries
to use the regulated load before the reduction as the target
load to be preserved. As an illustration, let t be the time in-
stance at which the apparent load is reduced because of the
mentioned reasons, i.e., e(t)  (cid:12)), then there are four possible outcomes at
t + 1 as follows, (a) the battery was below 50% charged at t,
was charged at t, and now is above 50% charged, so we have
e(t + 1) = e(t) (cid:0) (cid:12); (b) the battery was below 50% charged
at t, was charged at t, but now is still below 50% charged,
so that we have e(t + 1) = e(t); (c) the battery was above
50% charged at t, was discharged at t, and now is below 50%
charged, so we have e(t + 1) = e(t) + (cid:12); or (d) the battery
was above 50% charged at t, was discharged at t, but now
is still above 50% charged, so we have e(t + 1) = e(t).
Based on the above analysis, if the adversary observes
that e(t + 1) = e(t (cid:0) 1), he can then conclude that it is the
scenario of (i)(a); if the adversary observes that e(t + 1) =
e(t) (cid:0) (cid:12), he can then conclude that it is the scenario of
(iii)(a); and if the adversary observes that e(t+1) = e(t)+(cid:12),
he can then conclude that it is the scenario of (iii)(c). The
knowledge so gained, together with inference of the battery
charging/discharging rate (cid:12) the adversary makes from past
smart meter readings, will allow the adversary to estimate a
possible range of the real reading at e(t + 1), hence e(t) also
under the assumption that the real load remains about the
same over a short time period.
Notice that the adversary can apply the same inference for
the later time instances to further re(cid:12)ne his estimates of the
real load, provided that the regulated load is not increased
in cases (ii) and (iii), and the duration of time is relatively
short so that the real load remains roughly the same during
the period.
5. PRIVACY MEASURES
We now present the four privacy measures that we will
evaluate in this paper. They are information-theoretic mea-
sures that quantify the common information between the
original private data to be protected and its noisy version
that is revealed to observers, including curious adversaries
who aim to infer the private information. Importantly, in
contrast to most similar measures in the literature, our mea-
sures are all parameterized to reveal explicitly the order of
correlated information that can be used in a privacy attack.
The (cid:12)rst three measures do so by generalizing basic notions
already proposed in related work. The fourth measure is a
new one that we de(cid:12)ne, that admits comprehensively diﬀer-
ent scopes of information truly available for attack while re-
taining a constant meaning independent of the speci(cid:12)c scope
considered. In our discussion, we will use the notations of
X = fXig and Y = fYig, whose de(cid:12)nitions are given in
Sec. 2.
5.1 Mutual Information (MI)
An arguably most natural notion to measure information
content common to the clear and noisy data is that of mu-
tual information (MI), which indeed has been widely used in
privacy research [33, 42, 44, 46]. MI quanti(cid:12)es the mutual
dependence between two random variables X and Y . It is
de(cid:12)ned as follows.
∑
∑
I(X; Y ) =
p(x; y) log
x2X 1
y2Y 1
p(x; y)
p(x)p(y)
:
The smaller the measure, the lower the mutual dependence
between the clear data points and the perturbed ones, and
the better the privacy protection.
MI can be de(cid:12)ned for sequences of data points, rather
than individual data points, as symbols. In BLH, privacy
quanti(cid:12)cation has taken a modest step in this direction [46],
where the de(cid:12)nition of MI uses pairs of consecutive data
points as symbols:
∑
∑
)
)
(
(
I(X; Y ) =
p(x; y) log
x2X 2
y2Y 2
p(x; y)
p(x)p(y)
:
(1)
=
It is clear that pairs of data points may not capture all
relevant orders of information in real data. In this paper,
we will therefore consider a general notion of MI applied to
length-k data sequences for general k > 0: x 2 X k and y 2
Y k in Eq. 1. While this extension is conceptually simple, we
must take care to ensure that the length of data used to test
a privacy measure is long enough to produce steady-state
results after convergence. This issue is of much practical
importance because the required length increases with k due
to increased symbol diversity. We will address this practical
challenge in Sec. 7.
5.2 Normalized Mutual Information (NMI)
A variant privacy measure to MI is the mutual information
normalized by the entropy of the clear data trace [23]. The
de(cid:12)nition of this NMI measure is as follows.
(
CY X =
I(X; Y )
H(X)
=
x2X 1
y2Y 1 p(x; y) log
x2X 1 p(x) log(p(x))
p(x;y)
p(x)p(y)
∑
∑
∑
)
:
The reason for the division is to normalize the computed MI
to give the fraction of information of the clear data trace
that is exposed by the perturbed data trace. We adapt this
normalization similarly to our parameterized notion of MI
by k, for k > 0.
5.3 Conditional Entropy (CE)
Conditional entropy has been used to capture the uncer-
tainty of the position of a mobile user as a function of the
length of historical data points used to infer the position [6].
The measure is de(cid:12)ned as H(Xk+1jXk; Xk(cid:0)1; :::; X1) when
length-k, k > 0, historical data points are used. We adapt
this uncertainty notion of clear data to the case of privacy
protection. To do so, we de(cid:12)ne our CE measure to quantify
the uncertainty of a clear data point as a function of the
length of previous noisy data points used in the inference.
Speci(cid:12)cally, we de(cid:12)ne the CE as
∑
H(Xk+1jYk; Yk(cid:0)1; :::; Y1)
p(y)H(Xk+1jYk = yk; Yk(cid:0)1 = yk(cid:0)1; :::; Y1 = y1)
∑
p(Xk+1 = xjy) log p(Xk+1 = xjy);
∑
y2Y k
p(y)
=
= (cid:0)
y2Y k
x2X 1
where k is the length of past data points used from the noisy
trace, k > 0. The larger the CE, the higher the uncertainty
of the clear data points, hence the better the privacy pro-
tection.
5.4 Ofﬂine Conditional Entropy (OCE)
We now propose a new privacy measure of oﬄine con-
ditional entropy (OCE). This measure accounts for the full
information available to the adversary when he tries to infer
some clear data points. Speci(cid:12)cally, the adversary may ex-
ploit any range of noisy data points as guiding information.
Without loss of generality, the OCE is de(cid:12)ned with respect
to a range of 2k + 1 data points in the neighborhood of the
clear data point, for general k (cid:21) 0:
∑
H(Xk+1jY1; Y2; :::; Y2k+1)
∑
∑
y2Y 2k+1
y2Y 2k+1
p(y)
x2X 1
= (cid:0)
p(y)H(Xk+1jY1 = y1; Y2 = y2; :::; Y2k+1 = y2k+1)
p(Xk+1 = xjy) log p(Xk+1 = xjy):
Similar to CE, the larger the OCE, the higher the uncer-
tainty of the clear data points, and the better the privacy
protection.
Figure 2: Automaton for
trace generation.
3:
Figure
Order-2
Markov model for trace
generation.
6. AXIOMATIC PROPERTIES OF PRIVACY
MEASURES
In this section, we assess the proposed privacy measures by
postulating desirable axiomatic properties that they should
satisfy to be practically useful, in that these properties are
evidence that a measure can bring out the true information
content available in a privacy attack. In testing the privacy
measures for these axiomatic properties, a main issue is to
ensure that the test data has the intended information con-
tent by design. We use speci(cid:12)cally designed synthetic data
models to generate the required data. We report two such
data models in the generation: (i) a data automaton, and
(ii) an order-2 Markov chain. Application of the measures
to real-world traces will be presented in Sec. 7.
Traces generated by data automaton. Data points
in this trace set are limited to the range of integers [0; 1].
Speci(cid:12)cally, the even index data points are equal to their
corresponding previous odd index data points, and the odd
index data points are drawn uniformly at random from [0; 1].
Fig. 2 shows the automaton used to generate the trace of
integers. An example trace generated from the automaton
is f1; 1; 0; 0; 0; 0; :::g.
Notice that we cannot represent the generation of this
dataset by a Markov model of any order. It is because no