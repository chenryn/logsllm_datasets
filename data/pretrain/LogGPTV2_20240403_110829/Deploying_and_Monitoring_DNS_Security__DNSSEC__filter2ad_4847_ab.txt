DNSKEY records, and A records associated with name
431
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:11:53 UTC from IEEE Xplore.  Restrictions apply. 
servers. In early deployment a few years ago, SecSpider
began prominently featuring replay vulnerability statistics
and this coincided with a measurable change in signature
lifetimes and reduction in vulnerable records[19].
SecSpider summarizes the list of DNSKEYs that is sees
consistently across all of
its pollers into an observed
DNSSEC keys ﬁle. Whenever a zone is scheduled to be
polled, SecSpider issues its queries from all pollers. This
means that in order for an adversary to spoof their way
into SecSpider’s observed keys list, they would have to
fool all of its pollers positioned all around the world at
the precise time they query a speciﬁc zone. Moreover, they
would have to do so for all name servers of the target zone.
Based on DNS best practices, zones should be served by at
least 2 name servers in separate locations. The implications
of SecSpider’s observed keys ﬁle is that
it reﬂects the
operational reality of which keys are in use consistently and
correctly.
D. Design Requirements
Based on SecSpider’s roles as both a monitoring system
and an operational
tool for zone operators and resolver
operators, several key design requirements have emerged:
i) clearly the system must meet the scalability needs of
DNSSEC’s target deployment, ii) monitoring a distributed
system (like DNSSEC) is best done by a distributed network
of monitors, iii) SecSpider must be able to capture and
diagnose any complex distributed behaviors of DNSSEC, iv)
in order to continue to be able to diagnose unforeseen issues,
the design must support a model in which diagnostics and
enhancements can evolve along with the discoveries of new
problems, and v) SecSpider, itself, must be robust against
failures and its components (including operators that use its
data) must have trusted channels to coordinated with each
other over.
IV. THE SECSPIDER IMPLEMENTATION
A monitoring system for DNSSEC must be robust and se-
cure itself. The SecSpider system’s design, implementation,
and deployment have produced both a useful resource and
a set of more general design lessons.
SecSpider’s implementation uses a 3-tier architecture in
which a single master coordinates a globally distributed
network of polling daemons (pollers). The ﬁrst tier (front
end) is responsible for serving read-only content to users, the
middle tier (coordinator) performs scheduling and analysis,
and the third tier is a backend database (Figure 2).
Through the design and evolution of SecSpider, several
fundamental design insights have been gained that pertain
to distributed monitoring of DNSSEC, as discussed in Sec-
tion III-D.
A. Front End
SecSpider’s front end presentation is primarily done via
replicated instances of simple Apache webservers and DNS
432
Figure 2. The 3-tier architecture of SecSpider for scalability and isolation.
name servers that run in FreeBSD jails [14]. The website
remains an effective and highly accessible mechanism for
inspecting SecSpider data. However, over the four year de-
ployment, we continue to experiment with other distribution
mechanisms. For example, secspider.cs.ucla.edu
DNSSEC name servers in California and Colorado can
provide some additional DNSKEY veriﬁcation data in the
form on a new type of RR, called the DLV record [26].
Currently, we are exploring a new peer to peer distribution
mechanism that is planned to be available in late fall 2009.
Regardless of the actual SecSpider information distri-
bution mechanism,
the front end tier is restricted from
communicating with the other tiers. All of the components
of this tier serve read-only data to users, and do not have any
online communication channel with SecSpider’s backend
processing. Thus, all data that needs to be presented is
pushed to these front end boxes and served directly from
them. The only channel of communication from the front
end to the other components is the user submitted zone
names. In other words, a user may request SecSpider to add a
particular zone to its monitored set and this in turn populates
a local front-end repository. The middle-tier (coordinator) is
not notiﬁed of or interrupted by submissions, and instead
simply polls the front-end repository for submissions when
the coordinator is ready. This design illustrates that
the
monitoring and processing duties of a system (especially a
security system) can be, and perhaps should be, implemented
as isolated and atomic services whenever possible. The
application of this principle has allowed SecSpider’s front
end to decouple the scaling requirements of the presentation
layer from the polling and analysis that is done in the other
tiers. If the system becomes subject to an attack or an
intrusion the backend can remain unaffected.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:11:53 UTC from IEEE Xplore.  Restrictions apply. 
DatabaseFront EndWebsiteDNSName ServerCoordinatorSecSpiderMasterB. Coordinator
This component is the heart of SecSpider. The coordinator
is so named because it interfaces with SecSpider’s set of
distributed pollers and “coordinates” the process by which
they interrogate and measure various aspects of each zone.
In particular, every query issued by SecSpider ultimately
begins at the coordinator.
Active Polling: When a polling cycle starts, SecSpider
creates a new schedule of the zones it will poll. It does this
to reduce the odds that an adversary can observe SecSpider,
guess its query pattern, and attempt to coordinate spoofed
replies. After generating the query schedule for a run,
SecSpider must poll each zone from all of its pollers in
near synchrony; otherwise data inconsistencies are likely to
happen if the polling by different pollers is spread out over
time. Because SecSpider makes a conservative assumption
that
inconsistencies in the results obtained by different
pollers signal potential faults or attacks, the polling policy
attempts to minimize its own bias by reducing the time
between polls from different locations.
The polling itself is not as simple as issuing a set of
DNS queries and then recording the answers. As described
in Section III, all of the name servers for each zone are
probed to determine if they meet the requirements of being
DNSSEC-enabled. Furthermore, when queries by some, but
not all, pollers fail, SecSpider must retry them with different
parameters; we discuss this further below. Because of the
state needed for each zone on each poller, SecSpider uses
separate threads for each zone from each poller.
SecSpider uses a queue-poller-queue design (seen in Fig-
ure 3) to schedule zones from the backend, issue polling
queries, and store the results back to the backend. The
ﬁrst queue allows scheduling to ramp up at its own pace,
the polling is an orchestrated set of queries to distributed
pollers that can take its own time, and the second queue
allows polling results to be serialized at whatever pace the
backend can support. The choice of a queue-poller-queue
model illustrates a common property of distributed polling
systems and a useful approach; polling is an inherently I/O
bound process that can often proceed in ﬁts and starts.
A decoupled design such as the queue-poller-queue model
has allowed SecSpider to maintain the greatest amount of
polling throughput that both the backend and the polling
can jointly accommodate. If, for example, a poller is slow
to respond, the system will eventually time it out, but the
design schedules its queries at the granularity of zones. Thus,
all queries to any zone’s name servers will be issued at the
same time, and a slow poller will not cause a head of line
blocking problem.
After the polling schedule is created, each zone object is
entered into the loader queue. Tasks from this queue are
consumed by a pool of threads. Each SecSpider poller is
assigned to several threads in the pool. In order to ensure
Figure 3. The queue-poller-queue model is one in which producers and
consumers of queue have a loosely-coupled relationship to the polling work.
that a zone is polled at the same time from each poller,
tasks that are dequeued from the loader queue are then split
amongst one thread from each poller and their polling is
started at the same time. The reasons for actually splitting
the zone out and using threads instead of simple event loops
is that each zone may seem quite different from each poller’s
vantage point.
When querying for a zone’s DNSKEYs, one might naively
assume that the data for that zone would be equally accessi-
ble from any vantage point. However, this is not always the
case. In particular, some pollers can have notably different
response rates. We have observed that when a poller has
difﬁculty querying for a DNSKEY, it can manifest itself
in one of two ways: i) a truncated response is received1
or ii) the query times out and no response is received.
Truncated messages are anticipated; they are designed to
prompt resolvers retry their query either over TCP or with
a larger message size. However, persistent unresponsive
behavior is not. DNSSEC timeouts can result from unfore-
seen interactions within the Internet and the Path Maximum
Transmission Unit (PMTU).
As we discussed in Section III-C, one important recent
observation is that messages can be dropped if they are
too large. In this case, the server replied correctly and ﬁt
the DNS response inside the resolver’s speciﬁed maximum
packet size, but the network between the server and resolver
(e.g. the poller) did not permit this large packet size and the
response does not reach the poller. When SecSpider sees
that a poller has been unable to get a DNSKEY it performs a
PMTU walk where varying maximum query message sizes
are tried to determine what the proper size should be. We
discuss this further in Section V.
After each zone has been polled (and potentially PMTU
walked) from each poller, it is enqueued in SecSpider’s
dumper queue. This allows polling to occur at its own rate
1Note that DNS messages can indicate that the nameserver only had
room to send some of the data it wanted. This is indicated to resolvers by
setting the TC bit in the DNS message header.
433
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:11:53 UTC from IEEE Xplore.  Restrictions apply. 
PollerPollerPollerPollerCoordinatorwhile serialization and storage to the backend can be done
asynchronously from the dumper queue.
After the zone polling has concluded, SecSpider re-
examines each zone in order to logically build the secure
delegation hierarchy. SecSpider uses the existing DS records
(and checks to see if they verify properly) to identify where
valid secure delegations exist, and what the roots of the
observable islands of security are. This allows people to
observe DNSSEC’s rollout state, but also lets operators
check that their delegations are verifying properly.
The last stage in SecSpider’s active polling is NSEC
walking. As described in Section II, each zone forms a chain
of NSEC records that point from one domain name to the
“next” canonical name in the zone, and specify what types
of records exist for that name. Though this was done so that
a name server could provide secure denial of existence, it
allows us to start at one name and learn all of the other
names and record types in the zone. When there are no
more names, the last valid name in the zone points back to
the beginning (the zone apex). SecSpider walks over each
zone’s names looking for new DS records for child zones.
Whenever a name exists with a DS record, it indicates that
the zone has a secure delegation for that name. Of course,
this doesn’t always mean there actually is a valid zone
attached to that delegation. So, SecSpider uses that name
to check if there is a zone there that is DNSSEC-enabled. If
so, then the zone is permanently added to the polling corpus.
Though this seems very simple and straight forward, there
are some (largely undisclosed) complexities involved with
NSEC walking in practice.
While the format and protocol surrounding NSEC records
makes the concept of walking very clear, certain operational
practices make it difﬁcult
to do this at any signiﬁcant
scale. One of the ﬁrst issues with NSEC walking is that
many zones (especially ccTLDs) are quite large. Walking
their entire set of domains takes a non-trivial amount of
time. Another and more subtle limitation of blindly NSEC
walking is that some name servers host both a parent and
its children zones. The implication for NSEC walking is
that one name may be in a parent, then the next would
be the apex of a child. After reaching the end of the child
zone, the NSEC will point back to its own apex (not the
apex the NSEC walker started with). Thus, the NSEC walk
can get caught in a loop (seen in Figure 4) and simply
fail to ever walk passed the ﬁrst child zone. Because of
operational realities like these, SecSpider’s NSEC walking
randomly jumps forward by creating a random domain name
that would be canonically sorted after the current domain
name and using the random name in the next query. During
this entire process, SecSpider sets a time limit on each zone
and after any walk has exceeded that time, SecSpider moves
to the next zone. Therefore, each walk is a random sampling
of a large zone’s records, and over time SecSpider visits an
increasing number of them.
Figure 4. When a name server hosts a parent and any of its children
zones, NSEC walking will loop when the child zone loops back to its own
apex rather than continuing with the parent zone’s names.
Polling Infrastructure: Each of SecSpider’s pollers is a
lightweight DNS repeater called rdnsD [24]. These pollers
accept DNS queries from SecSpider over a secure channel
and then re-issue them either to their local resolver, or to a
name server that SecSpider speciﬁes. The coordinator simply
issues a DNS query to the desired poller, with a DNS OPT
resource record code in the message. This code tells the
poller where to re-send the message. The poller simply strips
the OPT code from the query and issues it to the speciﬁed
server. This allows SecSpider to query speciﬁc authoritative
DNS name servers from any of its polling vantages. The
communications between the coordinator and all of its
pollers (seen in Figure 5) are secured by using TSIG [25]
symmetric authentication (one of DNS’ standard symmetric
key facilities). Each poller uses a separate TSIG key to
authenticate its message exchanges with the SecSpider’s
coordinator. This ensures that an adversary can neither send
her own messages through SecSpider, nor spoof messages
that belong to SecSpider once polling has begun.
This design illustrates a fundamental tradeoff one faces
when designing a distributed polling system: a more central
design may choose to hold query logic close to a central
coordinator and keep remote simple thin-client pollers (as
SecSpider does), or distribute the query logic to smarter
fat-client pollers. The SecSpider design has demonstrated
some tangible beneﬁts gained from the thin-client approach.
A fat-client approach might seem more appealing from, for
example, an efﬁciency point of view because the coordinator
might issue a single request to “begin polling” and all DNS
queries could then be issued directly from each poller and
results summarized before being sent back to the coordi-
434
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:11:53 UTC from IEEE Xplore.  Restrictions apply. 
foo.combar.foo.com1.foo.com2.bar.foo.com1.bar.foo.comc.foo.com1235 (loop)4ns1.foo.comFigure 5. The locations of SecSpider’s current pollers.
nator. This design would minimize round trips between the
coordinator and its pollers. On the other hand, the thin-client
approach requires one round trip from the coordinator to
the poller and another round trip from the poller to a name
server for each query, and this can add noticeable network
overhead when compared to a fat-client system. However,
monitoring systems like SecSpider are used to discover,
diagnose, and illustrate unforeseen problems with an evolv-
ing deployment system. Thus, the polling patterns needed
by SecSpider are constantly evolving. For example, before
the PMTU problem was identiﬁed, SecSpider only issued
a single generic DNSKEY query for each of a zone’s name
servers. After discovering the PMTU problem, SecSpider
began using a ﬁnite state machine to determine details of
how to issue DNSKEY queries with different buffer sizes for
different name servers of the same zone, using both UDP
and TCP, and more. Having a thin-client system allowed this
change to be easily implemented, tested, and deployed at the
coordinator without any upgrade or change to the remote
pollers that are hosted by different organizations. Thus, in
the face of an ever-changing set of requirements, the thin-
client approach has allowed SecSpider the greatest agility in
discovering and addressing new problems.
that
Content Generation: After the active polling has ﬁn-
ished, SecSpider generates the content
it will push