Technique: Time Series Detection
An Alternative to StDev: Inter-Quartile Range
▶ IQR queries are a bit easier to understand conceptually, and they aren’t swayed by dataset extremes. They calculate the difference between the 25thpercentile and the 75thpercentile, let’s call it X. 
Then they look for any data points more than X above the 75th precentile.▶ Just like with StDev, we still have a coefficient - with stdev you look for datapoints 6 stdev above the average, here you might look for items 1.5, 3, or 6 IQRs above the 75th percentile.
▶ In my experience, I prefer stdev because I do care about including the outliers in my variance calculation, but it’s purely preference. I have asked many different people with PhDs and data science degrees, and there’s never been a concrete difference.▶ For an example using IQR, check out the Machine Learning 	Toolkit example at the end of this presentation. 
Technique: Time Series Detection
Other Alternatives to StDev
▶ Sometimes IQR and StDev just aren’t the right conceptual choices - for a particular dataset the data variance 	doesn’t quite fit. Here are a couple of other techniques to keep in mind.▶ Comparative Ratios: In our "Search: When Log Sources Go Quiet" example later in this doc, we don’t look at the # of Windows Security Logs, we look at the ratio of Windows Security Logs to overall logs. That will provide much 
more accurate results. 
▶ Normalizing data via log: https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/▶ I also chatted with one of the Splunk ML experts, Andrew Stein, who told me:
You may also consider Kolmogorov-Smirnov. Given two probability distributions (one reference, one unknown) you 
can measure how similar are the two. So make your reference = normal, and your unknown the observed one. For 
example I could measure what a specific time series is verse a normal distribution (spl for normal is on bbo.com ) using KS or whatever and I could tell if a time series was "normal"| 
 |  |  | • | He also called out: https://conf.splunk.com/files/2016/slides/a-very-brief-introduction-to-machine-learning-for-itoa.pdf | He also called out: https://conf.splunk.com/files/2016/slides/a-very-brief-introduction-to-machine-learning-for-itoa.pdf | He also called out: https://conf.splunk.com/files/2016/slides/a-very-brief-introduction-to-machine-learning-for-itoa.pdf |
|---|---|---|---|---|---|---||---|---|---|---|---|---|---|
|   | | |• |And http://shahramabyari.com/2015/12/21/data-preparation-for-predictive-modeling-resolving-skewness/ |And http://shahramabyari.com/2015/12/21/data-preparation-for-predictive-modeling-resolving-skewness/ |And http://shahramabyari.com/2015/12/21/data-preparation-for-predictive-modeling-resolving-skewness/ ||   | | |• |And https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test |And https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test |And https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test |
|   | | |• |And that can also smooth any time series in Splunk using http://docs.splunk.com/Documentation/MLApp/2.3.0/API/SavitzyGolayFilter but  |And that can also smooth any time series in Splunk using http://docs.splunk.com/Documentation/MLApp/2.3.0/API/SavitzyGolayFilter but  |And that can also smooth any time series in Splunk using http://docs.splunk.com/Documentation/MLApp/2.3.0/API/SavitzyGolayFilter but  ||   | | |• |that may be too advanced. This comes up in IOT metrics all the time |that may be too advanced. This comes up in IOT metrics all the time |that may be too advanced. This comes up in IOT metrics all the time ||   | | |• |So… get reading. Or make sure your output looks generally in line with what you want and pretend you’re a data scientist. You will need a  |So… get reading. Or make sure your output looks generally in line with what you want and pretend you’re a data scientist. You will need a  |So… get reading. Or make sure your output looks generally in line with what you want and pretend you’re a data scientist. You will need a  ||   | | |• |mustache and hipster glasses. |mustache and hipster glasses. |mustache and hipster glasses. |
|   | | |• | | | |
Technique: Time Series Detection
Three Last Thoughts
▶ Ultimately there’s no magic number of stdev, or IQR, or etc. Experiment with your 	data and see what comes out.
▶ When you are using any behavioral profile, a key concern is confidence checking. 	After all, we don’t want to overwhelm the SOC with noise.• Go check out the "Technique: Confidence Checking" elsewhere in this doc.
▶ Scaling this kind of detection is also very important for even medium sized 	organizations!
• Go check out "Technique: Summary Indexing" elsewhere in this doc.
Technique: Time Series * First Time Seen Detection
Background and Challenges
▶ "Wait, first logon to new server? No. No! I don’t want that running in my network! 	That is way too noisy! Stay away from me!"▶ As detailed in the First Time Seen Detection section, we can easily detect interesting activities such as the first time a user logs into a server for the first time. 
▶ Unfortunately, some users do this all the time. For a person in marketing who typically logs into 4 servers a day, logging into 100 new servers is terrifying! For the IT Admin runs Patch Tuesday with some epic scripts she cooked up, not remotely interesting.▶ One way to approach this is to combine the above two approaches to not alert every time someone logs into a new server, but alert if someone logs into more new servers than they typically do. 
Technique: Time Series * First Time Seen Detection Bam! Bringing it all together
▶ We’ve already explained how both of these searches in the prior two sections -the only part we’ve covered less is Summary Indexing which you can take a look at in the section of the same name.Detect first time logons 	Detect an anomalous number for a user, generate a *single* alert
sourcetype=win*security 
| stats earliest(_time) as earliest latest(_time) as latest 	by user, dest
Index=anomalies alerttype="FirstTimeSeen" alertname="FirstLogonToNewServer"
| bin span=1d _time | stats count by user _time 
|  |  | | inputlookup append=t lookup_cache.csv| stats min(earliest) as earliest max(latest) as latest 	by  user, dest 
| outputlookup lookup_cache.csv 
| where if(earliest>=relative_time(now(), "-1d@d"), 1, 0) | | inputlookup append=t lookup_cache.csv 
| stats min(earliest) as earliest max(latest) as latest 	by  user, dest 
| outputlookup lookup_cache.csv 
| where if(earliest>=relative_time(now(), "-1d@d"), 1, 0) | | statsmax(eval(if(_time >= relative_time(now(), "1d"), count, null))) as latest avg(eval(if(_time = relative_time(now(), "1d"), count, null))) as latest avg(eval(if(_time = relative_time(now(), "1d"), count, null))) as latest avg(eval(if(_time 3*stdev+average || where latest>3*stdev+average || where latest>3*stdev+average || where latest>3*stdev+average ||  | || eval alerttype="FirstTimeSeen",  alertname="FirstLogonToNewServer" |Another approach here would be to maintain a lookup that showed the  |Another approach here would be to maintain a lookup that showed the  |Another approach here would be to maintain a lookup that showed the  |Another approach here would be to maintain a lookup that showed the  ||  | || collect index=anomalies |avg number of servers any given user logs into per day. That way you can factor that into how you handle the alert  |avg number of servers any given user logs into per day. That way you can factor that into how you handle the alert  |avg number of servers any given user logs into per day. That way you can factor that into how you handle the alert  |avg number of servers any given user logs into per day. That way you can factor that into how you handle the alert  ||   | | | | | | |
© 2017 SPLUNK INC.
NINJA Techniques
Time to put on your sunglasses
Technique: tstats
Background and Challenges
▶ "I want to be able to look at all the data. I mean ALL the data."
▶ "I’m running 5+ concurrent correlation searches at all times and need to speed 	them up!"
▶ "We are spending more on Splunk so that we can afford the correlation searches 	we have to run over our massive dataset."Technique: tstats 
tstats speeds search dramatically. 
Raw Search: 21 Seconds
[search tag=malware earliest=-20m@m latest=-15m@m | table dest | rename dest as src ] 
earliest=-20m@m (sourcetype=sysmon OR 
sourcetype=carbon_black eventtype=process_launch) OR (sourcetype=proxy category=uncategorized)
|  stats count(eval(sourcetype="proxy")) as proxy_eventscount(eval(sourcetype="carbon_black" OR sourcetype="sysmon")) as endpoint_events by src
| where proxy_events > 0 AND endpoint_events > 0
tstats Search: 2 Seconds
| tstats prestats=t summariesonly=t count(Malware_Attacks.src) as malwarehits from datamodel=Malware where 
Malware_Attacks.action=allowed groupby Malware_Attacks.src
| tstats prestats=t append=t summariesonly=t count(web.src) as webhits from datamodel=Web whereweb.http_user_agent="shockwave flash" groupby web.src
| tstats prestats=t append=t summariesonly=t 
count(All_Changes.dest) from datamodel=Change_Analysis where sourcetype=carbon_black OR sourcetype=sysmon groupby 
All_Changes.dest
| rename web.src as src Malware_Attacks.src as src All_Changes.dest as src
| stats count(Malware_Attacks.src) as malwarehits count(web.src) as webhits count(All_Changes.dest) as process_launches by srcTechnique: tstats 
tstats speeds search dramatically. 
Raw Search: 68,476 Seconds
▶ index=* earliest=-24h 
| bucket _time span=1h 
| stats count by sourcetype, _time
tstats Search: 6 Seconds▶ | tstats count where index=* 
	by sourcetype _time span=1h
Technique: tstats 
Let’s just summarize this in one slide, shall we?▶ There is far more content to cover about tstats than is reasonable in just this section, so let’s go view the entire talk from conf2016, that will be repeated at conf 2017.
▶ Check out: http://dvsplunk.com/ninjutsu
Technique: Timestamps and Timestamps Background and Challenges
▶ "How can I analyze data that was just ingested, regardless of timestamp?"▶ "How much of my data is coming in with incorrect timestamps?"▶ "How much of my data is coming in with future timestamps?"
▶ All data is indexed with _time, but we also add _indextime which shows the time 	that the data was indexed. This is powerful!
Technique: Timestamps and Timestamps Searching Data that was just Indexed
▶ When you query _time, you hit earliest. When you query _indextime, you hit _indextime. These earliest/latest settings both apply, but keep the _time as low as possible because that defines what indexes you need to search through.index=unstable_timestamps 	earliest=-24h 
	latest=+24h 
	_index_earliest=-60m 
| table fields you care about
This is a slower approach, so only use when you 	have known unstable indexes
You should specify a big _time window that you know 	will encompass the time instability, but as little more 	as possible
Then include your _index_earliest
Finally, continue with your searchTechnique: Timestamps and Timestamps
Searching Data with Very Old Timestamps
▶ You can look very old timestamps that was just indexed, though beware that this search is extremely slow because it needs to look through every old bucket you have. It’s prudent to run this periodically as an all time real time search (if you have Indexed Real-time turned on) or occassionally to test.| tstats count min(_time) as min_ingestion_time 	where index=* 
	earliest=0 latest=-1h 
	_index_earliest=-5m _index_latest=now by host sourcetype 
| convert ctime(min_ingestion_time)
Use tstats to quickly parse out the count and the min(_time) 
Pick a giant _time window. You may want to start 	with latest=-8h to get the low hanging fruit.
Then include your _index_earliestThen include your _index_earliest
Format the oldest timestamp so that it’s readable
Technique: Timestamps and Timestamps Searching Data with Future Timestamps
▶ Looking at events with future timestamps is typically very fruitful for finding 	incorrect timestamps. 
| tstats count max(_time) as max_ingestion_time 	where index=* 
	earliest=+30s latest=+20y 
	_index_earliest=-5m _index_latest=now 	by host sourcetype| convert ctime(max_ingestion_time)
Use tstats to quickly parse out the count and the min(_time) 
Pick systems 30 seconds 
Then include your _index_earliest
Format the oldest timestamp so that it’s readable
Technique: Advanced Search Commands
Background and Challenges
▶ "I want to run several different sets of reporting commands from the same 	search"• E.g., "I want to update this lookup with one subset of the dataset, then run anomaly detection on 
a different subset"
▶ "I want to search multiple datasets, and apply different streaming commands to 	each set"
▶ "I don’t know what my fields will be named, but I have to manipulate them 	anyway!"
▶ "I want to do lots of other *weird* stuff"
Technique: Advanced Search Commands The multisearch Command▶ We all know you can do index=a OR index=b, but if you have to transform each differently this 	becomes a major hassle.
▶ Multisearch will actually run two different searches, but bring the results together for you.
| multisearch 
	[ index=ips | 
`lower_severity_for_low_confidence`] 	[ index=sandbox_confirmed | 
You start with | multisearch, so that Splunk knows 	you don’t want to start with a normal searchJust like subsearches, put each search in square 	brackets. You can use any streaming search 	commands
Any additional searches can be put in additional sets 
`raise_severity_for_high_confidence`] 	of angle brackets
| stats max(severity) values(index) by host
Finally, continue with your search
Technique: Advanced Search Commands The multireport Command▶ When running multiple searches over the same dataset, use case developers have to consider "is this something that can be combined." While you don’t want to go crazy (consider Multi-Scenario Alerts in this document), you can get much more convergence with multireport.
▶ I find this most useful when you have one search that leverages a stored lookup (or you build a lookup to provide context to an analyst), but you also want to update the stored lookup without managing another search. But there are many use cases.index=proxy 	Multireport forks off multiple searches | multireport
[ | stats values(domain) count min(_time) max(_time) by user | outputlookup contextual_per_user_info.csv | where 
hide="TheseEvents"]
[ | search category=adult | collect index=hr | where hide="TheseEvents"]
[ | lookup threatIntel domain | search threat_hit=*]
Each search sits in a set of square brackets. We can 	do whatever we want to in here, includingoutputlookups, collects, etc.
Multireport will append the output of each search, so 	here we use the | where clause that will hide all 	results from the analyst
This final search is the one I actually want sent to the user (or correlation search, etc.), so no | where 
Technique: Advanced Search Commands The foreach Command▶ The foreach command is great for two things: one is saving yourself copy-paste work to apply the same 	change to many fields, and the other is manipulating fields whose name you don’t know.
▶ Foreach works by taking a list of fields (or *) and then a set of streaming commands to run.(What’s a streaming command? https://docs.splunk.com/Documentation/Splunk/6.6.2/Search/Typesofcommands)▶ Remember with weird field names in eval, double quotes on the left side, single quotes on the right side. So: 	| eval ">_value" =  "The value for > is: " . ‘>’
▶ Repeat Operations
Index=business_operations sourcetype=hourly_data | stats avg(metric_*) as hourly_average_*| foreach hourly_average_* 
	[| eval ">" = round(‘>’ , 2) ]
▶ Unknown Field Names