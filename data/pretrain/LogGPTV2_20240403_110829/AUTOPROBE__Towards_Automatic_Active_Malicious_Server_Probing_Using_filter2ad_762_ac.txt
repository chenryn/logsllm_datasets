It turns out that all
these remotes servers are not malicious any more: four of them are
veriﬁed as sinkhole domains and the last one returns a 404 error
response (possibly server already cleaned). From this experiment,
we reasonably believe that our heuristics work well for most of
malware communications.
In our evaluation, AUTOPROBE generates a total of 70 classi-
ﬁcation functions for all ResponseSeen cases and 31 for the 29
NoResponse cases. The reason why we have more classiﬁcation
functions than the number of cases is because some malware probes
can generate different responses to trigger different malware be-
haviors. This further demonstrates the advantage of AUTOPROBE
because existing work cannot generate such probing.
The matching efﬁciency is important for the classiﬁcation func-
tion. For the ResponseSeen cases,
the detection requires that
all symbolic equations in the classiﬁcation function match, so
AUTOPROBE can ﬁnish matching when any of the equations fails to
match. For the NoResponse cases, it calculates the suspicious score
based on the matching results for all equations. For efﬁciency, our
scanner records the response trafﬁc and conducts ofﬂine matching.
It
shows the time consumed for classifying 1,000 responses. For the
ResponseSeen cases, on average, the classiﬁcation function consists
of 17 equations and takes 251 ms to complete the matching. The
worst case is one classiﬁcation function that consists of 36 equation
comparisons (CP) and takes 757 ms to parse 1,000 responses.
Table 2 summarizes the classiﬁcation function efﬁciency.
Dataset
I
I
II
II
Type
ResponseSeen
NoResponse
ResponseSeen
NoResponse
Malware Families #
24
13
9
10
Probe Generation Functions
R/O AUTOPROBE Probes Variable Constant
9 (23%)
45/74
167/167
2 (14%)
16(43%)
113/183
121/121
7 (46%)
22(56%)
11(78%)
21(57%)
8(54%)
39
14
37
15
CYBERPROBE
N/A
0
37(100%)
0
Table 1: Probe generation results.
For the best case, it takes 9 comparisons and 102ms to ﬁnish
the matching. For the NoResponse cases, a classiﬁcation function
typically contains more equations than the ResponseSeen cases (50
on average) and takes 973 ms on average to complete the matching.
For the best case, the matching takes 37 comparisons and 483ms
to obtain the result. Overall, when classifying responses from
Internet-wide scanning (Section 6.5), our classiﬁcation component
takes an average of 5 hours to analyze 71 million responses.
6.3 Case Studies
In this section, we study some probes generated by AUTOPROBE
for real-world malware samples.
Bamital. Bamital
is a malware family involved in click-
fraud. The probe generation component identiﬁes three variable
parts in the initial C&C request (Figure 7):
(1) requested ﬁle
name: m.php (2) os ﬁeld which is obtained from the system
call GetVersionEx (3) host ﬁeld which is the output of a
customized domain generation algorithm (DGA).
Figure 7: Probe for Batimal Trojan
During malware execution no C&C server response was ob-
served, as the C&C servers were no longer alive. However, by
feeding the malware with a HTTP/1.1 200 OK response, AU-
TOPROBE is able to analyze the malware’s logic, which searches
for the strings  and  in the response and eventually
constructs new requests to download binary ﬁles. The produced
classiﬁcation function requires a successful connection with 200
status code and the presence of the string [.*] and
[.*]. If a response to a probe satisﬁes those constraints,
the sender is classiﬁed as a Bamital C&C server.
Taidoor. Taidoor is a malware family that has been used in
targeted attacks [34]. Its C&C is also built on top of HTTP. The
ﬁrst state-dependent ﬁeld is the URL ﬁlename, which is randomly
generated with its length limited to 5 characters. The id URL para-
mater value is built from the output of the GetAdaptersInfo
library call, used to obtain the host’s MAC address. When malware
parses the response, the malware uses the value of the id ﬁeld (the
MAC address) as the key to decode the response, which introduces
a strong correlation between the request and the response. The
classiﬁcation function comprises two steps: decode the data using
the request’s id as key, and check that the decoded data is a valid
ASCII string.
Sality. For Sality, AUTOPROBE identiﬁes 3 HTTP probes for
ﬁles spm/s_tasks.php, logos_s.gif and 231013_d.exe.
For the request of the 231013_d.exe executable, the down-
loaded ﬁle will be directly executed. The classiﬁcation function
considers the set of three ﬁle requests and responses. Any server
hosting ﬁles at those URLs will be considered a Sality server.
Other Malware. For Xpaj.B, AUTOPROBE generates one HTTP
POST request with an encoded string, such as
POST /tRHmgD?kjBQMgpwJFLP=QOrbhqDjVeJmN. The clas-
siﬁcation function looks for the string "filename=" at the
beginning of the response. For ZeroAccess AUTOPROBE produces
an HTTP probe for the links.php ﬁle. The malware visit all
URLs in the response. The classiﬁcation function ﬂags the target
host as a ZeroAccess server if the response contains a list of URLs.
6.4 Localized Scanning
As mentioned earlier, AUTOPROBE generated totally 105 probes
for 56 malware families. To test the effectiveness of these probes,
we select 28 malware families for localized probing test.
Target network range. We ﬁrst scan the network ranges that
have been observed in the past to host some malicious servers. Ac-
cording to the provider locality property of malicious servers found
in [25], these network ranges are more likely to ﬁnd malicious
servers than other regions on the Internet. We start with a seed set of
9, 500 malware server IPs collected from MalwareDomainList.com
as well as the IP address of the malicious servers detected in [25].
We then expand the IP list to include their network neighbors,
i.e., those in the same /24 subnets and those from the BGP route
information2.
In this way, we have collected 2.6M IPs for our
localized scanning.
Result. Table 3 details the 28 localized scans. The left part of
the table shows the scan conﬁguration: the scan date, the malware
dataset, the target port, the number of hosts scanned, and the
number of scanners used (SC). The middle part of Table 5 shows
the results:
the scan duration, the response rate (Resp., i.e., the
percentage of targets that replied to the probe), the number of total
malicious servers found, the number of found malicious servers
already in the seed set, and the number of new malicious servers
(not in the seed set). Through 28 scans, AUTOPROBE has identiﬁed
a total of 172 malicious servers among which 81 are known (in
the seed set) and 91 are new (previously unknown) malicious
servers. We compare our results with some existing malicious
domain blacklists, namely VirusTotal [36] (VT), Malware Domain
List [23](MD), and URLQuery [35](UQ). The best coverage is
achieved by VirusTotal, which knows 14.1% of the servers found
by AUTOPROBE (24/172). URL Query knows 11(6.39%) servers
and Malware domain list knows only 3(0.02%) malicious servers.
In this case, AUTOPROBE detects 6 times more malicious servers
than the best of these blacklist services, clearly demonstrating
that AUTOPROBE is an effective scheme for detecting malicious
servers. On average, AUTOPROBE can efﬁciently scan 2.6 million
IPs with two parallel scanners in 3 hours.
2We obtain the most speciﬁc BGP route that contains each seed IP
address.
GET/[%1]?subid=61&pr=1&os=20&id=8BBFF356C9BA905540BBB48D98C90697&ver=[%2] HTTP/1.0 Host: [%3].info User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1) Pragma: no-cache  [%1] = slice_0(random) [%2] = slice_1(os_version) [%3] = slice_2(time) Matching Scheme Worst (CP) Worst (ms)
757
1,923
ResponseSeen
NoResponse
36
67
Best (CP)
9
37
Best (ms)
102
483
Avg. (CP) Avg. (ms)
251
973
17
50
Table 2: Efﬁciency of Classiﬁcation Functions (time measured when handling 1000 continuous responses). Here CP denotes the
number of equation comparisons.
ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
Scan Date
2013-11-03
2013-11-03
2013-11-03
2013-11-03
2013-11-03
2013-11-03
2013-11-08
2013-11-08
2013-11-08
2013-11-08
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2014-02-17
2014-02-17
2014-02-17
2014-02-17
DataSet
II
II
II
II
II
II
II
II
II
II
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
Port
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
Time
# Scanners
2.3h
3
2.4h
3
2.4h
3
2.3h
3
2.8h
3
3.2h
3
2.6h
3
2.7h
3
1.2h
3
1.8h
3
3.3h
2
3.8h
2
4.1h
2
3.2h
2
3.8h
2
3.9h
2
3.6h
2
3.2h
2
3.3h
2
3.5h
2
3.3h
2
3.7h
2
3.1h
2
3.0h
2
3.6h
2
3.9h
2
4.1h
2
2
3.8h
TOTALS:
Resp.
64%
64%
64%
64%
64%
64%
63%
63%
63%
63%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%