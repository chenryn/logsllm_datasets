OrphanDetection/orphan-detection (anonmymized for submission).
It implements our methodology for running it against a single web-
site (see Figure 3), making large-scale studies embarassingly paral-
lel and enabling them to scale horizontally across machines easily.
The open source nature of our implementation also allows se-
curity professionals and researchers to tailor our methodology to
their needs. For our large-scale measurement study, we chose cut-
off and threshold parameters that led to a high reduction rate to
obtain a lower bound with our measurements. However, when
used on a single domain, security professionals may prefer a more
lenient Dynamic URL Detection, or even omit this module from the
procedure entirely, to apply additional context-aware filters on the
list of potential orphans. We expand on use cases of our technique
later (see Section 6).
3.2 Large-Scale Dataset
We evaluate the efficacy of our method through large-scale mea-
surements, detecting orphaned pages in the wild. Following, we
discuss the data set that we used for our study, and report on the
timeline and implementation of our measurements. We deploy our
methodology on 20 servers within our university network, and
split our study into two phases: downloading and processing.
Input Data Sets We rely on two different data sets for our
3.2.1
measurements: First, a random sample of 100,000 domains from
the Tranco Domain List, and, second, the archived data for these
domains from the Internet Archive (see Section 3.1.1).
Tranco Domain List: Research has shown that traditional Top 1M
lists are less reliable than initially thought [22, 28, 29]. They are not
stable, contain unresponsive websites, and are vulnerable to ma-
nipulations, such as promoting one‚Äôs domain. Additionally, most
of them actually disagree on the exact list and order of domains,
with one study showing a meager 2.4% overlap among the agreed
order of the four Top 1M lists [22].
To counter the existing issues with Top 1M sets, we use the
Tranco list [22].
It averages the ranks of domains, as listed by
Alexa, Cisco Umbrella, Majestic, and Quantcast. In turn, the list
is more resilient to manipulations and exhibits less fluctuations.
For this study, we use the Tranco list from 14 December 2020.1
We evaluate our methodology on a subset of the Tranco list, con-
sisting of a random sample of 100,000 domains taken from the top
500,000 ranked domains. The exact list of domains is available at
our open source repository.2
Internet Archive: To detect orphan pages, we make use of the In-
ternet Archive, which is an online digital library, archiving and
providing access to various resources including websites [2] (see
Section 3.1.1). We query the Internet Archive‚Äôs Wayback Machine
using the CDX API [19]. It stores a URL key, timestamp, full URL,
media type (MIME), status code received during crawl, a message
digest, and the length. However, we only retrieve the timestamp
and full URL, and do this only for pages that returned a status code
of OK (200) upon their initial crawl (see Section 3; this makes our
approach lightwight in terms of archive data).
3.2.2 Downloading Phase For the downloading phase, we take our
input domains and retrieve the archive file for each domain. We re-
trieved the archives between December 16, 2020 and December 20,
2020, with ten downloads in parallel on each of our servers, with
a delay of one minute between each batch. We do so to reduce the
load on the web archive in consultation with the Internet Archive.
When fetching the archive data for a domain, we encountered
four different issues: (1) the server throws an error, (2) the server is
temporarily offline, (3) the server sends back 0 bytes, and (4) we hit
1Available at https://tranco-list.eu/list/5Q3N.
2Available at https://github.com/OrphanDetection/orphan-detection.
DownloadExtractFilterDUDeProbeGet 200CoreOptionalAdjustableLegendDomainNameCandidateOrphansDataSession 1A: Cybercrime CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea25the rate limit. On each issue, we move the corresponding domain
to the back of our queue and attempt to retrieve it again later. We
repeat this process until we retrieved all archival data, or are left
with cases only returning 0 bytes or an error (for which we then
assume the data is not available in the Internet Archive).
Overall, we retrieved sitemaps for 96,537 domains from our sam-
ple of 100,000 domains (with the remaining domains not being in-
dexed), for a total of 6,092,214,431 URLs, that is, on average 60,922
pages per domain (median: 6,955, standard deviation: 126,094).
3.2.3 Processing Phase We then apply our methodology on the
pages from the 96,537 domain archives. Table 1 shows an overview
of timeframe and results of our large-scale measurement, which
we discuss next. Filtering the pages on our list of file extensions,
we first obtain a total of 4,033,539,860 potential orphan pages. Af-
ter removing dynamic URLS with DUDe, we reduce the list to
924,190,351 URLs (a reduction of 77%).
We then probe the remaining URLs to determine which pages
still return a status code of OK (200). To avoid overloading the do-
mains with our probes, we shuffle the list of URLs and split them
over our deployment. This ensures that, on average, we did not ex-
ceed 87 requests per domain per hour. Moreover, our probes used
a customized User-Agent that allowed administrators to easily opt-
out from our study at any time (see also our ethics discussion, Sec-
tion 6.2).3 We probed candidate orphaned pages from January 1,
2021 to January 29, 2021. After probing and removing pages that
did not respond with a HTTP status code OK (200), we are left with
36,442,679 candidate pages (a reduction of 96%), see Table 1.
Identifying Custom Error Pages Unfortunately, not every un-
3.2.4
available web page returns a status code of Not Found (404), or
Redirect (301/302). Instead they might return a custom error page
stating the page was not found, while the HTTP status code is actu-
ally OK (200). In fact, while probing pages, we regularly encounter
discarded pages (i.e., web pages displaying a standard HTML stat-
ing a similar message to ‚ÄúThis page no longer exists‚Äù) returning a
status code of OK (200). To remove these false positives at scale, we
retrieve the size of each page and remove pages from the same do-
main with a similar size (e.g., within 5 bytes of difference). Lever-
aging size-based filtering, we remove pages that should have re-
turned an error response, but responded with status code OK, and
we reduce the set to 1,821,682 URLs (a reduction of 95%).
We may remove some pages with genuine content that is simi-
lar among multiple pages, such as login portals. However, in this
paper, we aim to determine a lower bound on the prevalence of
orphaned pages, and, thus, we believe that omitting some pages
and duplicates is an acceptable trade-off (see also Section 6.3).
3.2.5 Removing Invalid Pages Finally, we discard 564,619 invalid
pages because of a bad file encoding or because we actually re-
ceived a non-HTML file, leading to 1,257,063 pages (a reduction of
30%) that we need to analyze in more depth.
4 Analysis
Here, we analyze and report on the results of our measurement
study, and evaluate the validity of our candidate orphan pages.
3During probing, we received two requests from administrators to remove their
website from our study. Correspondingly, we excluded some links from probing.
Table 1: Summary of our large-scale measurement.
Step
Downloading archive data
Filtering file extensions
Dynamic URL Detection
Probing and extracting pages with
HTTP 200 response
Size-based filtering
Removing Invalid Pages
Timeframe
December 16, 2020 ‚Äì
December 20, 2020
December 20, 2020
December 29, 2020
January 1, 2021 ‚Äì Janu-
ary 29, 2021
February 14, 2021
February 23, 2021
Result
96,537 Domains
4,033,539,860 Pages
924,190,351 Pages
36,442,679 Pages
1,821,682 Pages
1,257,063 Pages
4.1 Data Set Overview
A summary of the data set from our large-scale measurement can
be found Table 1. After processing the archive data, filtering them,
removing dynamic URLs, probing, filtering based on size, and re-
moving invalid pages, we are left with 1,257,063 candidate orphan
pages that we need to analyze in more depth (see Section 3.2.3).
4.2 Archive Data Analysis
After collecting the archive data for the domains in our data set, we
can investigate how domains evolve over time. Figure 4a shows the
mean amount of pages per domain for each year between 2000 and
2020. Here, we see a clear increasing trend throughout the years,
suggesting that websites grow. To account for bias inflicted by rel-
atively young websites, since these would have zero pages before
their first appearance online, we also show the growth for all do-
mains that had at least one page archived in 2000. We observe that
accounting for age still shows an increasing trend for the number
of pages on a domain. This is also true for the median (see Fig-
ure 4b). Additionally, Figure 4c shows a boxplot for the amount of
pages per domain per year. Apart from confirming the increase in
the number of pages per domain over time, the latter two graphs
also show the high variance in pages per domain. Especially from
the boxplots, we can detect a significant difference between the
first quartile and the third quartile.
We described earlier that the 2020 sitemaps function as the bases
for gathering the candidate orphan pages (see Section 3). We now
take a look at how, and specifically when, the 2020 sitemaps came
to be. Figure 4d depicts two Cumulative Distribution Functions
(CDFs) of when a page of a 2020 sitemap was first seen in the
archive data. The non-normalized CDF is calculated by summing
the pages across all domains for each year and constructing a CDF
of the sum, the normalized CDF is calculated by taken the average
of the CDF percentages for each year across all domains. Both are
only calculated for pages that were last seen in the archive in 2020.
Interestingly, both CDFs suggests that old pages indeed get re-
moved from a domain eventually. However, the other plots of Fig-
ure 4 also indicate that sitemaps are growing over the years. This
then means that the older a page is, the less likely it is meant to be
online. We will leverage this observation later on in our analysis.
Note that the trends of the graphs rely on the Internet Archive‚Äôs
operations throughout the years. For example, sitemap growth is
naturally influenced by the Internet Archive‚Äôs crawl scale.
4.3 Page Similarity
We identified a first parameter that can provide an indication of
whether a page is unintentionally orphaned: its last seen date. Al-
beit not sufficient on its own, there is another intuitive indicator of
Session 1A: Cybercrime CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea26(a) Mean
(b) Median
Figure 5: Distribution of the calculated similarity scores.
(c) Boxplot
(d) CDF
Figure 4: Analysis of the archive data.
(a) and (b) show
the number of pages on a domain, (c) shows it as boxplots
(yellow markers represent the mean). (d) depicts a normal-
ized and non-normalized CDF of which percentages of pages
from the current sitemap were present in a specific year.
whether a page was intentionally orphaned: its similarity to other
pages of the same domain. Thus, to check whether a page has ac-
tually become unmaintained orphaned since its last listing on the
Internet Archive, we compare the fingerprint of a page‚Äôs current
version to its last archived version.
Detecting (near-)duplicates among web pages and other types
of documents has been extensively studied before. Henzinger [18]
compared the two state of the art approaches for near-duplicate de-
tection of web-pages (Broder et al. [8] and Charikar [9]), and con-
cluded that Charikar‚Äôs simhash performs better, which also has a
small memory footprint [27]. Thus, we base our similarity detec-
tion on it. Our comparison works by first creating a fingerprint of
the two pages and comparing them.
Creating Fingerprints using simhash: We create the fingerprint of
a web page by first removing all HTML tags and extracting only
the content of the page. We then divide the extracted content into
n-grams of ùëõ = 8. For each n-gram, we construct a 64-bit hash
using the FNV-1a hashing algorithm. To construct the fingerprint,
we sum each hash over the same index in the following way: If the
hash of the n-gram has the value 1 at index ùëñ, we add 1 to index ùëñ in
the fingerprint. If the hash has the value 0, subtract 1 from index
ùëñ in the fingerprint. For each index in the fingerprint, convert the
value to 1 if its positive, and to 0 if its negative. The result is a
64-bit fingerprint of the web page.
For each URL
Comparing Fingerprints using Hamming Distance:
in our data set, we retrieve its current version and its latest archived
version, and we create fingerprints for both pages. We then com-
pare the 64-bit fingerprints using the Hamming distance, which is
the number of bits that need to be changed for the two fingerprints
to produce two equal strings. The inverse gives us the similarity.
Figure 5 shows the distribution of the similarity scores of the
candidate orphaned pages. We observe a clear Gaussian distribu-
tion around 0.5 with an increase at 1. Indeed, it follows our expecta-
tions that, although pages change over the years (e.g., their design),
core content often remains unchanged. The left tail of the curve
represents pages that have changed more drastically throughout
the years, while the right tail of the curve shows the pages that
have remained unchanged, with a spike at 1 for the pages that ap-
pear to have not changed at all. The latter are of high interest to
us, as they might be unmaintained orphaned pages.
4.4 Orphan Likelihood Score
Leveraging the two parameters that can provide an indication of
whether a page is intentionally or unintentionally orphaned, its
last seen date and its similarity score, we can derive a metric for the