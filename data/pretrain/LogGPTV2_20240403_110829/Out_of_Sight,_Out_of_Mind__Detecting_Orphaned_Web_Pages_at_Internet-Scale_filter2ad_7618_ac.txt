### OrphanDetection/orphan-detection (Anonymized for Submission)

This project implements our methodology for detecting orphaned pages on a single website (see Figure 3). The approach is designed to be easily parallelized, allowing large-scale studies to scale horizontally across multiple machines. The open-source nature of our implementation also enables security professionals and researchers to tailor the methodology to their specific needs.

For our large-scale measurement study, we chose cutoff and threshold parameters that led to a high reduction rate, providing a lower bound for our measurements. However, when applied to a single domain, security professionals may prefer a more lenient Dynamic URL Detection or even omit this module entirely to apply additional context-aware filters to the list of potential orphans. We will expand on the use cases of our technique in Section 6.

### 3.2 Large-Scale Dataset

We evaluate the efficacy of our method through large-scale measurements, detecting orphaned pages in the wild. In this section, we discuss the dataset used for our study and provide details on the timeline and implementation of our measurements. Our methodology was deployed on 20 servers within our university network, and the study was split into two phases: downloading and processing.

#### 3.2.1 Input Data Sets

We relied on two different datasets for our measurements:
1. A random sample of 100,000 domains from the Tranco Domain List.
2. Archived data for these domains from the Internet Archive (see Section 3.1.1).

**Tranco Domain List:**
Traditional Top 1M lists have been shown to be less reliable than initially thought [22, 28, 29]. They are not stable, contain unresponsive websites, and are vulnerable to manipulations, such as promoting one’s domain. Additionally, there is often little agreement among the lists, with one study showing only a 2.4% overlap among the top four Top 1M lists [22].

To address these issues, we used the Tranco list [22], which averages the ranks of domains as listed by Alexa, Cisco Umbrella, Majestic, and Quantcast. This makes the list more resilient to manipulations and less prone to fluctuations. For this study, we used the Tranco list from December 14, 2020. We evaluated our methodology on a subset of the Tranco list, consisting of a random sample of 100,000 domains taken from the top 500,000 ranked domains. The exact list of domains is available in our open-source repository [2].

**Internet Archive:**
To detect orphan pages, we utilized the Internet Archive, an online digital library that archives and provides access to various resources, including websites [2] (see Section 3.1.1). We queried the Internet Archive's Wayback Machine using the CDX API [19], which stores a URL key, timestamp, full URL, media type (MIME), status code received during crawl, a message digest, and the length. For our purposes, we only retrieved the timestamp and full URL, and did so only for pages that returned a status code of OK (200) upon their initial crawl. This approach minimizes the amount of archive data we need to process.

#### 3.2.2 Downloading Phase

During the downloading phase, we retrieved the archive file for each domain. We conducted the downloads between December 16, 2020, and December 20, 2020, with ten parallel downloads on each server, and a one-minute delay between each batch to reduce the load on the web archive, in consultation with the Internet Archive.

We encountered four main issues while fetching the archive data:
1. The server throws an error.
2. The server is temporarily offline.
3. The server sends back 0 bytes.
4. We hit the rate limit.

In each case, we moved the corresponding domain to the back of our queue and attempted to retrieve it again later. We repeated this process until we retrieved all archival data or were left with cases that consistently returned 0 bytes or an error, at which point we assumed the data was not available in the Internet Archive.

Overall, we retrieved sitemaps for 96,537 domains from our sample of 100,000 domains, resulting in a total of 6,092,214,431 URLs, or an average of 60,922 pages per domain (median: 6,955, standard deviation: 126,094).

#### 3.2.3 Processing Phase

We then applied our methodology to the pages from the 96,537 domain archives. Table 1 provides an overview of the timeframe and results of our large-scale measurement.

1. **Filtering File Extensions:**
   - Initial list: 4,033,539,860 potential orphan pages.
   
2. **Dynamic URL Detection (DUDe):**
   - Reduced list: 924,190,351 URLs (a 77% reduction).

3. **Probing and Extracting Pages with HTTP 200 Response:**
   - We shuffled the list of URLs and split them over our deployment to avoid overloading the domains, ensuring an average of no more than 87 requests per domain per hour. Our probes used a customized User-Agent to allow administrators to opt-out of our study at any time (see also our ethics discussion in Section 6.2).
   - Probed candidate orphaned pages from January 1, 2021, to January 29, 2021.
   - After removing pages that did not respond with a status code of OK (200), we were left with 36,442,679 candidate pages (a 96% reduction).

#### 3.2.4 Identifying Custom Error Pages

Not every unavailable web page returns a status code of Not Found (404) or Redirect (301/302). Instead, they might return a custom error page stating that the page was not found, while the HTTP status code is actually OK (200). To remove these false positives, we retrieved the size of each page and removed pages from the same domain with a similar size (e.g., within 5 bytes of difference). This size-based filtering reduced the set to 1,821,682 URLs (a 95% reduction).

While this approach may remove some pages with genuine content, such as login portals, our goal is to determine a lower bound on the prevalence of orphaned pages. Therefore, we believe that omitting some pages and duplicates is an acceptable trade-off (see also Section 6.3).

#### 3.2.5 Removing Invalid Pages

Finally, we discarded 564,619 invalid pages due to bad file encoding or non-HTML files, leaving us with 1,257,063 pages (a 30% reduction) that we needed to analyze in more depth.

### 4 Analysis

In this section, we analyze and report on the results of our measurement study and evaluate the validity of our candidate orphan pages.

**Table 1: Summary of Our Large-Scale Measurement**

| Step | Timeframe | Result |
|------|-----------|--------|
| Downloading archive data | December 16, 2020 – December 20, 2020 | 96,537 Domains |
| Filtering file extensions | December 20, 2020 | 4,033,539,860 Pages |
| Dynamic URL Detection | December 29, 2020 | 924,190,351 Pages |
| Probing and extracting pages with HTTP 200 response | January 1, 2021 – January 29, 2021 | 36,442,679 Pages |
| Size-based filtering | February 14, 2021 | 1,821,682 Pages |
| Removing Invalid Pages | February 23, 2021 | 1,257,063 Pages |

#### 4.1 Data Set Overview

A summary of the dataset from our large-scale measurement can be found in Table 1. After processing the archive data, filtering, removing dynamic URLs, probing, size-based filtering, and removing invalid pages, we were left with 1,257,063 candidate orphan pages that required further analysis (see Section 3.2.3).

#### 4.2 Archive Data Analysis

After collecting the archive data for the domains in our dataset, we investigated how domains evolve over time. Figure 4a shows the mean number of pages per domain for each year between 2000 and 2020, indicating a clear increasing trend. To account for bias from relatively young websites, we also analyzed the growth for all domains that had at least one page archived in 2000. The trend still showed an increase in the number of pages per domain, both in the mean and median (see Figures 4a and 4b).

Figure 4c shows a boxplot of the number of pages per domain per year, confirming the increase over time and highlighting the high variance in pages per domain. The boxplots show a significant difference between the first and third quartiles.

The 2020 sitemaps served as the basis for gathering the candidate orphan pages (see Section 3). Figure 4d depicts two Cumulative Distribution Functions (CDFs) of when a page from a 2020 sitemap was first seen in the archive data. Both CDFs suggest that old pages are eventually removed from a domain, but the other plots indicate that sitemaps are growing over the years. This means that older pages are less likely to be intended to be online, a finding we will leverage in our analysis.

#### 4.3 Page Similarity

We identified two parameters that can indicate whether a page is intentionally or unintentionally orphaned: its last seen date and its similarity to other pages on the same domain. To check if a page has become an unmaintained orphan since its last listing on the Internet Archive, we compared the fingerprint of the page's current version to its last archived version.

**Creating Fingerprints using simhash:**
We created the fingerprint of a web page by first removing all HTML tags and extracting the content. We then divided the content into n-grams of \( n = 8 \). For each n-gram, we constructed a 64-bit hash using the FNV-1a hashing algorithm. To construct the fingerprint, we summed each hash over the same index, adding 1 if the hash value was 1 and subtracting 1 if it was 0. For each index in the fingerprint, we converted the value to 1 if positive and to 0 if negative, resulting in a 64-bit fingerprint.

**Comparing Fingerprints using Hamming Distance:**
For each URL in our dataset, we retrieved its current version and its latest archived version, creating fingerprints for both. We then compared the 64-bit fingerprints using the Hamming distance, which is the number of bits that need to be changed to make the two fingerprints equal. The inverse gives us the similarity score.

Figure 5 shows the distribution of the similarity scores of the candidate orphaned pages. We observed a Gaussian distribution around 0.5 with an increase at 1, indicating that core content often remains unchanged, while the tails of the curve represent pages that have changed more drastically or remained unchanged.

#### 4.4 Orphan Likelihood Score

Using the two parameters—last seen date and similarity score—we derived a metric for the likelihood that a page is an orphan. This metric helps us identify pages that are likely to be unintentionally orphaned and require further investigation.