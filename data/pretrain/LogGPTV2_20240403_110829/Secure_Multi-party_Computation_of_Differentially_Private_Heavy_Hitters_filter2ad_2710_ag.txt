tion and Internet Computing (CIC).
[74] Apple’s Differential Privacy Team. 2017. Learning with Privacy at scale. https:
//machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html.
[75] Machine Learning Group ULB. 2019. Online Retail. Retrieved October, 2020 from
https://archive.ics.uci.edu/ml/datasets/Online+Retail+II https://archive.ics.uci.
edu/ml/datasets/Online+Retail+II.
[76] Salil Vadhan. 2017. The complexity of differential privacy. In Tutorials on the
Foundations of Cryptography. Springer.
[77] Tianhao Wang, Jeremiah Blocki, Ninghui Li, and Somesh Jha. 2017. Locally
differentially private protocols for frequency estimation. In USENIX Security
Symposium (USENIXSec).
[78] Tianhao Wang, Ninghui Li, and Somesh Jha. 2019. Locally differentially private
heavy hitter identification. IEEE Transactions on Dependable and Secure Computing
(2019).
[79] Royce J Wilson, Celia Yuxin Zhang, William Lam, Damien Desfontaines, Daniel
Simmons-Marengo, and Bryant Gipson. 2020. Differentially private SQL with
bounded user contribution. In International Symposium on Privacy Enhancing
Technologies Symposium (PETS).
[80] Andrew Chi-Chih Yao. 1986. How to generate and exchange secrets. In Annual
IEEE Symposium on Foundations of Computer Science (FOCS).
[81] Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. 2020.
Federated Heavy Hitters Discovery with Differential Privacy. In International
Conference on Artificial Intelligence and Statistics (AISTATS ). http://proceedings.
mlr.press/v108/zhu20a.html http://proceedings.mlr.press/v108/zhu20a/zhu20a.
pdf.
A GUMBEL MECHANISM
Definition 6 (Gumbel Mechanism MG
anism MG
Δ𝑢 = max
). The Gumbel mech-
, for utility function 𝑢 : (𝑈 𝑛 × R) → R with sensitivity
∀𝑟 ∈R,𝐷≃𝐷′|𝑢(𝐷, 𝑟) − 𝑢(𝐷′, 𝑟)|, outputs 𝑟 ∈ R via
arg max
{𝑢(𝐷, 𝑟) + Gumbel(2Δ𝑢/𝜖)},
𝑟 ∈R
𝑏 exp(cid:16)
1
(cid:16) 𝑥
𝑏 + exp(cid:16)
(cid:17)(cid:17)(cid:17).
𝑏
−
where Gumbel(𝑏) denotes a random variable from the Gumbel distri-
bution with scale 𝑏 and density Gumbel(𝑥; 𝑏) =
− 𝑥
B PEMorig
PEMorig finds frequent prefixes of increasing lengths and split
clients in 𝑔 = ⌈(𝑏−𝛾)/𝜂⌉ disjoint groups. The 𝑖-th group (1 ≤ 𝑖 ≤ 𝑔)
reports perturbed (𝛾 + 𝑖𝜂)-bit prefixes (𝛾 = ⌈log2 𝑘⌉) of their datum
to a server. In more detail, a user in group 𝑖 selects a hash function
𝐻 : 𝑈 → {1, . . . , 𝑢} from a family of hash functions H, where
𝑢 = ⌈exp(𝜖) +1⌉. Then, she applies generalized randomized response
GRR over her hashed datum; more precisely, ℎ = GRR(𝐻(𝑑′)) of
the (𝛾 + 𝑖𝜂)-bit prefix 𝑑′ of her datum 𝑑 where
with probability 𝑝 =
(cid:40)𝑥
GRR(𝑥) =
𝑦 ≠ 𝑥 with probability
exp(𝜖)
exp(𝜖)+𝑢−1
1
exp(𝜖)+𝑢−1
,
1
𝑗=1
𝑏 ·
− 𝑥
𝑏
𝑏 exp(cid:16)
and 𝑦 ∈ {1, . . . , 𝑢}. Finally, she reports (𝐻, ℎ) to the server. Given
the reports, the server creates a candidate set C by extending the
previous top-2𝛾 prefixes with all possible binary strings of length 𝜂.
Then, the server estimates the frequency of each prefix candidate
𝑐 ∈ C as 𝑠𝑐−𝑛/𝑢
𝑝−1/𝑢 where 𝑠𝑐 is the number of reports with matching
hashes, i.e., 𝑠𝑐 = |{𝑐 | 𝑐 ∈ C and 𝐻(𝑐) = ℎ}|.
C DISTRIBUTED GUMBEL NOISE
Random variable 𝑋 ∼ Gumbel(𝑏) can be expressed as
𝑌𝑗 ∼ Expon(1),
 𝑛∑︁
,
(cid:17) for 𝑥 > 0 and 0 elsewhere [21].
𝑌𝑗
𝑗 − log(𝑛)
lim
𝑛→∞
where the Exponential distribution with scale 𝑏 has density Expon(𝑥; 𝑏) =
While the Laplace distribution can be expressed as a finite sum,
the Gumbel distribution requires an infinite sum. However, the
expected approximation error for the Gumbel distribution can be
made arbitrarily small in the number 𝑠 of summands:
|𝑠(𝑏) = 𝑏𝑠
Theorem 9. For Gumbel
𝑗 −𝑏 log(𝑛), where 𝑌𝑗 ∼
Expon(1), we have expected approximation error |Gumbel(𝑏) −
Gumbel
the Euler-Mascheroni constant, and E(cid:2)Gumbel
𝑏 log(𝑛) ≤ 𝑏(cid:0)𝛾EM + 1/(2𝑠) + 𝑂(cid:0)1/𝑠2(cid:1)(cid:1), due to E[𝑌𝑖] = 1 and [48,
Proof. We have E[Gumbel(𝑏)] = 𝛾EM·𝑏, where 𝛾EM ≈ 0.5772 is
(cid:12)(cid:12)E(cid:2)Gumbel(𝑏) − Gumbel
(cid:3)(cid:12)(cid:12)
=(cid:12)(cid:12)E[Gumbel(𝑏)] − E(cid:2)Gumbel
(cid:3) = 𝑏𝑠
|𝑠(𝑏)| = 𝑂(𝑏/𝑠).
(4.30)]. Altogether,
E[𝑌𝑗]𝑗 −
|𝑠(𝑏)
|𝑠(𝑏)
(cid:3)(cid:12)(cid:12)
|𝑠(𝑏)
𝑗=1
𝑌𝑗
𝑗=1
≤ |𝛾EM𝑏 − 𝑏(𝛾EM + 𝑂(1/𝑠))| = 𝑏𝑂(1/𝑠).
□
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2374Table 2: Complexity of MPC protocols for 𝑏-bit integers
(with pre-computed Beaver triples for AND, CondSwap) [3,
39].
Protocol
ADD, NOT
Rec
AND, CondSwap
LE
EQ
Rounds
Interactive Operations
0
1
1
4
4
0
1
2
4𝑏 − 2
𝑏 + 4 log 𝑏
Note that the input parties can pre-compute an arbitrary number
of such sum terms, and add them to their prefix counts, thus, they
only need to send a single message (i.e., the noisy counts) to the
computation parties.
D COMPOSITION
Lemma 2. Composing 𝑘 approximate DP mechanisms requires
2 log(1/𝛿′)
(2−exp(𝜖))2 with
less total 𝜖 budget compared to pure DP when 𝑘 >
𝛿′ > 0.
Proof. Running 𝑘 (𝜖, 𝛿)-DP mechanisms on the same data leads
to a total privacy budget of 𝑘𝜖 for pure DP mechanisms (𝛿 = 0), and
(
𝑘𝜖(exp(𝜖) − 1)  0) [37, Theorem 3.20]. Therefore,√︁2𝑘 log(1/𝛿′)𝜖 +
√︁2 log(1/𝛿′) + (exp(𝜖) − 1) 
2 log(1/𝛿′)
(2−exp(𝜖))2 .
□
E COMPLEXITY OF MPC PROTOCOLS
Table 2 lists the complexities for MPC protocols (without pre-
computation for LE, EQ from [3]) typically measured in the number
of rounds and interactive operations, where rounds describes the
count of sequential interactive operations, and interactive opera-
tions (e.g., reconstruct sharing, multiplications) require each party
to send messages to all other parties. Share reconstruction is de-
noted with Rec and NOT(𝑎) = 1 − 𝑎.
Note that CondSwap(𝑎, 𝑏, 𝑐) is implemented with one multi-
plication and two additions (𝑏 + (𝑎 − 𝑏) · 𝑐) and AND(𝑎, 𝑏) also
uses one multiplication (𝑎 · 𝑏). With pre-computed Beaver triples
⟨𝑎⟩, ⟨𝑏⟩, ⟨𝑐⟩, where 𝑐 = 𝑎𝑏, multiplication ⟨𝑥⟩⟨𝑦⟩ can be expressed
as [13]: ⟨𝑥𝑦⟩ = ⟨𝑐⟩ + 𝛼⟨𝑏⟩ + 𝛽⟨𝑎⟩ + 𝛼 · 𝛽, where 𝛼 = Rec(⟨𝑥 − 𝑎⟩),
𝛽 = Rec(⟨𝑦 − 𝑏⟩).
F SECURE SORTING
We use the existing secure sorting based on merge sort from MP-
SPDZ7 and SCALE-MAMBA8. The implementations use conditional
swaps: Roughly, whenever an array value 𝐴[𝑖] is smaller than
𝐴[𝑖 + 1], i.e., 𝑏swap = LE(𝐴[𝑖 + 1], 𝐴[𝑖]) is 1, they are swapped.
However, we slightly adapt it, and re-use the comparison result
𝑏swap to sort a second array 𝐵 in the same way, i.e., for each swap
with 𝐴 we simply perform the same swap with 𝐵.
7https://github.com/data61/MP-SPDZ/blob/v0.1.8/Compiler/library.py#L464
8https://github.com/KULeuven-COSIC/SCALE-MAMBA/blob/
862ecf547a01883cfbaf81a07c444c0c7cb53010/Compiler/library.py#L424
(a) MP-SPDZ: HH
(b) MP-SPDZ: HHthreads
(c) SCALE-MAMBA: HH
(d) SCALE-MAMBA:
HHthreads
Figure 13: Communication per party for HH, HHthreads.
G AWS COSTS
AWS t2.medium instances cost less than 5 Cents per hour, and
communication of 1 GB costs around 2 Cents (per month) [6]. If
one wants to optimize for cost, we suggest to use an MP-SPDZ
implementation: All our MP-SPDZ evaluations for HH, PEM run in
less than 30 minutes and require less than 1 GB of communication,
hence, even our largest MP-SPDZ evaluation cost less than 5 Cents
per computation party. (Except for 𝑘 = 16, 𝜂 = 5 which uses t2.large
instances that costs less than 10 Cents per hour.) As a comparison,
recall that LDP approach PEMorig requires up to 220 hash compu-
tations for each user input. Our evaluation of PEMorig – also on
t2.medium instances, without parallelization as this requires addi-
tional computational resources – showed running times of hours
compared to the minutes required for PEM.
H COMMUNICATION OF HH & PEM
Figure 13 shows the communication per party for HH and HHthreads
and Figure 13 shows the communication for PEM. Overall, the sever
communication for MP-SPDZ can be measured in MB whereas
SCALE-MAMBA requires GB for larger evaluations.
I EVALUATION FOR LARGE DATA SIZES
We designed our protocols with high accuracy on small data sizes 𝑛
in mind, as it is the most challenging regime for DP where the noise
easily exceeds the actual counts, particularly in a distributed setting.
Nonetheless, our protocols also provide higher accuracy than local-
model equivalents for large data sizes, e.g., 105, as visualized in
Figure 15. We omitted the comparison to PEMorig with 𝜂 > 5 as
the evaluation did not finish after 12 hours on t2.medium instances.
While PEMorig can be parallelized per group it is still linear in 𝑛
and does not scale. Our protocol HH with fixed map size 𝑡 = 16 is
eventually outperformed by PEMorig for large enough data sets,
i.e., around 𝑛 = 5 · 105. PEM, however, already finds almost all 𝑘
heavy hitters for 𝑛 = 105. These empirical observations confirm
our analysis of HH and PEM detailed in Section 3.3 (i.e., PEM is
better suited for larger data sets).
4816255075100kMBsnp=100np=30np=104816255075100kMBs4816200400600800kMBsnp=100np=30np=1048161,0002,0003,0004,0005,000kMBsSession 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2375(a) MP-SPDZ: PEM, 𝑏 = 32
(b) MP-SPDZ: PEM, 𝑏 = 64
(c) SCALE-MAMBA: PEM, 𝑏 = 32
Figure 14: Communication per party for PEM.
(a) Zipf
(b) Retail
Figure 15: NCR of PEM variants and HH for fixed 𝜖 = 0.25,
𝑘 = 16, varying 𝑛 ∈ {105, 2 · 105, 5 · 105
}.
J F1 SCORE
We also evaluated F1-scores (harmonic mean of precision and recall)
and compare the relative difference of NCR to F1, i.e., (NCR−F1)/NCR.
If NCR is 0, F1 is 0 as well, and we set the relative difference to 0. A
positive value means NCR is larger than F1, which is to be expected.
Recall, unlike F1, NCR gives more weight to elements that appear
more frequently. However, negative values are possible (e.g., if the
mode was not found).
Table 3 presents the relative difference of NCR to F1 averaged
over 𝜖 ∈ {0.1, 0.25, 0.5, 1, 2} for Zipf and retail data with 𝑛 = 1, 000.
Table 4 presents the same for 𝑛 = 5, 000. Overall, the averaged
scores for F1 and NCR are very close for our protocols (mostly the
difference is below 6%) and further apart for PEMorig (mostly above
6% and up to 48% difference), i.e., our protocols provide superior F1
scores.
Table 5 gives the detailed comparisons for each 𝜖 on Zipf and
retail data with 𝑛 = 1, 000 for fixed 𝑘 = 16. Likewise, Table 6
presents the comparison for 𝑛 = 5, 000. Large relative differences
for PEMorig result from its comparatively low scores. For example,
PEMorig has NCR=0.1, F1=0.06 for 𝑘 = 16, 𝑛 = 1, 000, 𝜖 = 2 on retail