tion and Internet Computing (CIC).
[74] Appleâ€™s Differential Privacy Team. 2017. Learning with Privacy at scale. https:
//machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html.
[75] Machine Learning Group ULB. 2019. Online Retail. Retrieved October, 2020 from
https://archive.ics.uci.edu/ml/datasets/Online+Retail+II https://archive.ics.uci.
edu/ml/datasets/Online+Retail+II.
[76] Salil Vadhan. 2017. The complexity of differential privacy. In Tutorials on the
Foundations of Cryptography. Springer.
[77] Tianhao Wang, Jeremiah Blocki, Ninghui Li, and Somesh Jha. 2017. Locally
differentially private protocols for frequency estimation. In USENIX Security
Symposium (USENIXSec).
[78] Tianhao Wang, Ninghui Li, and Somesh Jha. 2019. Locally differentially private
heavy hitter identification. IEEE Transactions on Dependable and Secure Computing
(2019).
[79] Royce J Wilson, Celia Yuxin Zhang, William Lam, Damien Desfontaines, Daniel
Simmons-Marengo, and Bryant Gipson. 2020. Differentially private SQL with
bounded user contribution. In International Symposium on Privacy Enhancing
Technologies Symposium (PETS).
[80] Andrew Chi-Chih Yao. 1986. How to generate and exchange secrets. In Annual
IEEE Symposium on Foundations of Computer Science (FOCS).
[81] Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. 2020.
Federated Heavy Hitters Discovery with Differential Privacy. In International
Conference on Artificial Intelligence and Statistics (AISTATS ). http://proceedings.
mlr.press/v108/zhu20a.html http://proceedings.mlr.press/v108/zhu20a/zhu20a.
pdf.
A GUMBEL MECHANISM
Definition 6 (Gumbel Mechanism MG
anism MG
Î”ğ‘¢ = max
). The Gumbel mech-
, for utility function ğ‘¢ : (ğ‘ˆ ğ‘› Ã— R) â†’ R with sensitivity
âˆ€ğ‘Ÿ âˆˆR,ğ·â‰ƒğ·â€²|ğ‘¢(ğ·, ğ‘Ÿ) âˆ’ ğ‘¢(ğ·â€², ğ‘Ÿ)|, outputs ğ‘Ÿ âˆˆ R via
arg max
{ğ‘¢(ğ·, ğ‘Ÿ) + Gumbel(2Î”ğ‘¢/ğœ–)},
ğ‘Ÿ âˆˆR
ğ‘ exp(cid:16)
1
(cid:16) ğ‘¥
ğ‘ + exp(cid:16)
(cid:17)(cid:17)(cid:17).
ğ‘
âˆ’
where Gumbel(ğ‘) denotes a random variable from the Gumbel distri-
bution with scale ğ‘ and density Gumbel(ğ‘¥; ğ‘) =
âˆ’ ğ‘¥
B PEMorig
PEMorig finds frequent prefixes of increasing lengths and split
clients in ğ‘” = âŒˆ(ğ‘âˆ’ğ›¾)/ğœ‚âŒ‰ disjoint groups. The ğ‘–-th group (1 â‰¤ ğ‘– â‰¤ ğ‘”)
reports perturbed (ğ›¾ + ğ‘–ğœ‚)-bit prefixes (ğ›¾ = âŒˆlog2 ğ‘˜âŒ‰) of their datum
to a server. In more detail, a user in group ğ‘– selects a hash function
ğ» : ğ‘ˆ â†’ {1, . . . , ğ‘¢} from a family of hash functions H, where
ğ‘¢ = âŒˆexp(ğœ–) +1âŒ‰. Then, she applies generalized randomized response
GRR over her hashed datum; more precisely, â„ = GRR(ğ»(ğ‘‘â€²)) of
the (ğ›¾ + ğ‘–ğœ‚)-bit prefix ğ‘‘â€² of her datum ğ‘‘ where
with probability ğ‘ =
(cid:40)ğ‘¥
GRR(ğ‘¥) =
ğ‘¦ â‰  ğ‘¥ with probability
exp(ğœ–)
exp(ğœ–)+ğ‘¢âˆ’1
1
exp(ğœ–)+ğ‘¢âˆ’1
,
1
ğ‘—=1
ğ‘ Â·
âˆ’ ğ‘¥
ğ‘
ğ‘ exp(cid:16)
and ğ‘¦ âˆˆ {1, . . . , ğ‘¢}. Finally, she reports (ğ», â„) to the server. Given
the reports, the server creates a candidate set C by extending the
previous top-2ğ›¾ prefixes with all possible binary strings of length ğœ‚.
Then, the server estimates the frequency of each prefix candidate
ğ‘ âˆˆ C as ğ‘ ğ‘âˆ’ğ‘›/ğ‘¢
ğ‘âˆ’1/ğ‘¢ where ğ‘ ğ‘ is the number of reports with matching
hashes, i.e., ğ‘ ğ‘ = |{ğ‘ | ğ‘ âˆˆ C and ğ»(ğ‘) = â„}|.
C DISTRIBUTED GUMBEL NOISE
Random variable ğ‘‹ âˆ¼ Gumbel(ğ‘) can be expressed as
ğ‘Œğ‘— âˆ¼ Expon(1),
 ğ‘›âˆ‘ï¸
,
(cid:17) for ğ‘¥ > 0 and 0 elsewhere [21].
ğ‘Œğ‘—
ğ‘— âˆ’ log(ğ‘›)
lim
ğ‘›â†’âˆ
where the Exponential distribution with scale ğ‘ has density Expon(ğ‘¥; ğ‘) =
While the Laplace distribution can be expressed as a finite sum,
the Gumbel distribution requires an infinite sum. However, the
expected approximation error for the Gumbel distribution can be
made arbitrarily small in the number ğ‘  of summands:
|ğ‘ (ğ‘) = ğ‘ğ‘ 
Theorem 9. For Gumbel
ğ‘— âˆ’ğ‘ log(ğ‘›), where ğ‘Œğ‘— âˆ¼
Expon(1), we have expected approximation error |Gumbel(ğ‘) âˆ’
Gumbel
the Euler-Mascheroni constant, and E(cid:2)Gumbel
ğ‘ log(ğ‘›) â‰¤ ğ‘(cid:0)ğ›¾EM + 1/(2ğ‘ ) + ğ‘‚(cid:0)1/ğ‘ 2(cid:1)(cid:1), due to E[ğ‘Œğ‘–] = 1 and [48,
Proof. We have E[Gumbel(ğ‘)] = ğ›¾EMÂ·ğ‘, where ğ›¾EM â‰ˆ 0.5772 is
(cid:12)(cid:12)E(cid:2)Gumbel(ğ‘) âˆ’ Gumbel
(cid:3)(cid:12)(cid:12)
=(cid:12)(cid:12)E[Gumbel(ğ‘)] âˆ’ E(cid:2)Gumbel
(cid:3) = ğ‘ğ‘ 
|ğ‘ (ğ‘)| = ğ‘‚(ğ‘/ğ‘ ).
(4.30)]. Altogether,
E[ğ‘Œğ‘—]ğ‘— âˆ’
|ğ‘ (ğ‘)
|ğ‘ (ğ‘)
(cid:3)(cid:12)(cid:12)
|ğ‘ (ğ‘)
ğ‘—=1
ğ‘Œğ‘—
ğ‘—=1
â‰¤ |ğ›¾EMğ‘ âˆ’ ğ‘(ğ›¾EM + ğ‘‚(1/ğ‘ ))| = ğ‘ğ‘‚(1/ğ‘ ).
â–¡
Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2374Table 2: Complexity of MPC protocols for ğ‘-bit integers
(with pre-computed Beaver triples for AND, CondSwap) [3,
39].
Protocol
ADD, NOT
Rec
AND, CondSwap
LE
EQ
Rounds
Interactive Operations
0
1
1
4
4
0
1
2
4ğ‘ âˆ’ 2
ğ‘ + 4 log ğ‘
Note that the input parties can pre-compute an arbitrary number
of such sum terms, and add them to their prefix counts, thus, they
only need to send a single message (i.e., the noisy counts) to the
computation parties.
D COMPOSITION
Lemma 2. Composing ğ‘˜ approximate DP mechanisms requires
2 log(1/ğ›¿â€²)
(2âˆ’exp(ğœ–))2 with
less total ğœ– budget compared to pure DP when ğ‘˜ >
ğ›¿â€² > 0.
Proof. Running ğ‘˜ (ğœ–, ğ›¿)-DP mechanisms on the same data leads
to a total privacy budget of ğ‘˜ğœ– for pure DP mechanisms (ğ›¿ = 0), and
(
ğ‘˜ğœ–(exp(ğœ–) âˆ’ 1)  0) [37, Theorem 3.20]. Therefore,âˆšï¸2ğ‘˜ log(1/ğ›¿â€²)ğœ– +
âˆšï¸2 log(1/ğ›¿â€²) + (exp(ğœ–) âˆ’ 1) 
2 log(1/ğ›¿â€²)
(2âˆ’exp(ğœ–))2 .
â–¡
E COMPLEXITY OF MPC PROTOCOLS
Table 2 lists the complexities for MPC protocols (without pre-
computation for LE, EQ from [3]) typically measured in the number
of rounds and interactive operations, where rounds describes the
count of sequential interactive operations, and interactive opera-
tions (e.g., reconstruct sharing, multiplications) require each party
to send messages to all other parties. Share reconstruction is de-
noted with Rec and NOT(ğ‘) = 1 âˆ’ ğ‘.
Note that CondSwap(ğ‘, ğ‘, ğ‘) is implemented with one multi-
plication and two additions (ğ‘ + (ğ‘ âˆ’ ğ‘) Â· ğ‘) and AND(ğ‘, ğ‘) also
uses one multiplication (ğ‘ Â· ğ‘). With pre-computed Beaver triples
âŸ¨ğ‘âŸ©, âŸ¨ğ‘âŸ©, âŸ¨ğ‘âŸ©, where ğ‘ = ğ‘ğ‘, multiplication âŸ¨ğ‘¥âŸ©âŸ¨ğ‘¦âŸ© can be expressed
as [13]: âŸ¨ğ‘¥ğ‘¦âŸ© = âŸ¨ğ‘âŸ© + ğ›¼âŸ¨ğ‘âŸ© + ğ›½âŸ¨ğ‘âŸ© + ğ›¼ Â· ğ›½, where ğ›¼ = Rec(âŸ¨ğ‘¥ âˆ’ ğ‘âŸ©),
ğ›½ = Rec(âŸ¨ğ‘¦ âˆ’ ğ‘âŸ©).
F SECURE SORTING
We use the existing secure sorting based on merge sort from MP-
SPDZ7 and SCALE-MAMBA8. The implementations use conditional
swaps: Roughly, whenever an array value ğ´[ğ‘–] is smaller than
ğ´[ğ‘– + 1], i.e., ğ‘swap = LE(ğ´[ğ‘– + 1], ğ´[ğ‘–]) is 1, they are swapped.
However, we slightly adapt it, and re-use the comparison result
ğ‘swap to sort a second array ğµ in the same way, i.e., for each swap
with ğ´ we simply perform the same swap with ğµ.
7https://github.com/data61/MP-SPDZ/blob/v0.1.8/Compiler/library.py#L464
8https://github.com/KULeuven-COSIC/SCALE-MAMBA/blob/
862ecf547a01883cfbaf81a07c444c0c7cb53010/Compiler/library.py#L424
(a) MP-SPDZ: HH
(b) MP-SPDZ: HHthreads
(c) SCALE-MAMBA: HH
(d) SCALE-MAMBA:
HHthreads
Figure 13: Communication per party for HH, HHthreads.
G AWS COSTS
AWS t2.medium instances cost less than 5 Cents per hour, and
communication of 1 GB costs around 2 Cents (per month) [6]. If
one wants to optimize for cost, we suggest to use an MP-SPDZ
implementation: All our MP-SPDZ evaluations for HH, PEM run in
less than 30 minutes and require less than 1 GB of communication,
hence, even our largest MP-SPDZ evaluation cost less than 5 Cents
per computation party. (Except for ğ‘˜ = 16, ğœ‚ = 5 which uses t2.large
instances that costs less than 10 Cents per hour.) As a comparison,
recall that LDP approach PEMorig requires up to 220 hash compu-
tations for each user input. Our evaluation of PEMorig â€“ also on
t2.medium instances, without parallelization as this requires addi-
tional computational resources â€“ showed running times of hours
compared to the minutes required for PEM.
H COMMUNICATION OF HH & PEM
Figure 13 shows the communication per party for HH and HHthreads
and Figure 13 shows the communication for PEM. Overall, the sever
communication for MP-SPDZ can be measured in MB whereas
SCALE-MAMBA requires GB for larger evaluations.
I EVALUATION FOR LARGE DATA SIZES
We designed our protocols with high accuracy on small data sizes ğ‘›
in mind, as it is the most challenging regime for DP where the noise
easily exceeds the actual counts, particularly in a distributed setting.
Nonetheless, our protocols also provide higher accuracy than local-
model equivalents for large data sizes, e.g., 105, as visualized in
Figure 15. We omitted the comparison to PEMorig with ğœ‚ > 5 as
the evaluation did not finish after 12 hours on t2.medium instances.
While PEMorig can be parallelized per group it is still linear in ğ‘›
and does not scale. Our protocol HH with fixed map size ğ‘¡ = 16 is
eventually outperformed by PEMorig for large enough data sets,
i.e., around ğ‘› = 5 Â· 105. PEM, however, already finds almost all ğ‘˜
heavy hitters for ğ‘› = 105. These empirical observations confirm
our analysis of HH and PEM detailed in Section 3.3 (i.e., PEM is
better suited for larger data sets).
4816255075100kMBsnp=100np=30np=104816255075100kMBs4816200400600800kMBsnp=100np=30np=1048161,0002,0003,0004,0005,000kMBsSession 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2375(a) MP-SPDZ: PEM, ğ‘ = 32
(b) MP-SPDZ: PEM, ğ‘ = 64
(c) SCALE-MAMBA: PEM, ğ‘ = 32
Figure 14: Communication per party for PEM.
(a) Zipf
(b) Retail
Figure 15: NCR of PEM variants and HH for fixed ğœ– = 0.25,
ğ‘˜ = 16, varying ğ‘› âˆˆ {105, 2 Â· 105, 5 Â· 105
}.
J F1 SCORE
We also evaluated F1-scores (harmonic mean of precision and recall)
and compare the relative difference of NCR to F1, i.e., (NCRâˆ’F1)/NCR.
If NCR is 0, F1 is 0 as well, and we set the relative difference to 0. A
positive value means NCR is larger than F1, which is to be expected.
Recall, unlike F1, NCR gives more weight to elements that appear
more frequently. However, negative values are possible (e.g., if the
mode was not found).
Table 3 presents the relative difference of NCR to F1 averaged
over ğœ– âˆˆ {0.1, 0.25, 0.5, 1, 2} for Zipf and retail data with ğ‘› = 1, 000.
Table 4 presents the same for ğ‘› = 5, 000. Overall, the averaged
scores for F1 and NCR are very close for our protocols (mostly the
difference is below 6%) and further apart for PEMorig (mostly above
6% and up to 48% difference), i.e., our protocols provide superior F1
scores.
Table 5 gives the detailed comparisons for each ğœ– on Zipf and
retail data with ğ‘› = 1, 000 for fixed ğ‘˜ = 16. Likewise, Table 6
presents the comparison for ğ‘› = 5, 000. Large relative differences
for PEMorig result from its comparatively low scores. For example,
PEMorig has NCR=0.1, F1=0.06 for ğ‘˜ = 16, ğ‘› = 1, 000, ğœ– = 2 on retail