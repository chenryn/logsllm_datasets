Below, we use only the abstract properties of the encryp-
tion and commitment schemes. For an actual implementa-
tion, we propose using the Paillier encryption scheme (where
messages are in Zn for a composite n, together with a modi-
ﬁed version of Pedersen Commitment (where both messages
and randomness are also in Zn). More details can be found
in Appendix A.
4. THREAT MODEL AND SECURITY
We deﬁne and prove the security properties of our protocol
using a simulation paradigm. The protocol’s functionality is
deﬁned by describing how it would work in an “ideal world”,
in which there exists a completely trusted third party. Infor-
mally, our security claim is that any attack an adversary can
perform on the protocol in the real world can be transformed
into an attack on the functionality in the ideal world. This
approach has the advantage of allowing us to gain a better
intuitive understanding of the protocol’s security guaran-
tees, when compared to the game-based or property-based
approached for deﬁning security.
The basic functionality is deﬁned and proved in Canetti’s
Universal Composability framework [5]. This provides ex-
tremely strong guarantees of security, including security un-
der arbitrary composition with other protocols. The ideal
voting functionality, described below, explicitly speciﬁes what
abilities the adversary gains by corrupting the diﬀerent par-
ties involved.
We also guarantee receipt-freeness, a property that is not
captured by the standard UC deﬁnitions, using a similar
simulation-based deﬁnition.
4.1 Ideal Voting Functionality
The voting functionality deﬁnes a number of diﬀerent par-
ties: n voters, two voting authorities A1 and A2, a veriﬁer
and an adversary. The voting authorities’ only action is to
specify the end of the voting phase. Also, there are some
actions the adversary can perform only after corrupting one
(or both) of the voting authorities. The veriﬁer is the only
party with output. If the protocol terminates successfully,
the veriﬁer outputs the tally, otherwise it outputs ⊥ (this
corresponds to cheating being detected).
When one (or more) of the voting authorities is corrupt,
we allow the adversary to change the ﬁnal tally, as long as
the total number of votes changed is less than the security
−k negligible).3 This is modeled
parameter k (we consider 2
by giving the tally privately to the adversary, and letting the
adversary announce an arbitrary tally using the Announce
command (described below). If neither of the voting author-
ities is corrupt, the adversary cannot cause the functionality
to halt. The formal speciﬁcation for the voting functionality,
Fvote, follows:
Vote v, xv On receiving this command from voter v, the
functionality stores the tuple (v, xv) in the database S and
outputs “v has voted” to the adversary. The functionality
then ignores further messages from voter v. The functional-
ity will also accept this message from the adversary if v was
previously corrupted (in this case an existing (v, xv) tuple
can be replaced.
Tally On receiving this command from a voting authority,
the functionality computes si = |{(v, xv) ∈ S | xv = i}| for
all i ∈ Zm. If neither of the voting authorities is corrupt,
the functionality sends the tally s0, . . . , sm−1 to the veriﬁer
and halts (this is a successful termination). Otherwise, it
sends the tally, s0, . . . , sm−1, to the adversary.
P
0, . . . , s
m−1 On receiving this command from
Announce s
the adversary, the functionality veriﬁes that the Tally com-
mand was previously received. It then computes
i|. If d < k (where k is the security pa-
d =
m−1 to the veriﬁer
rameter) it outputs the tally s
and halts (this is considered a successful termination).
i=0 |si − s
m−1
(cid:3)
(cid:3)
0, . . . , s
(cid:3)
(cid:3)
(cid:3)
Corrupt v On receiving this command from the adversary,
the functionality sends xv to the adversary (if there exists a
tuple (v, xv) ∈ S).
Corrupt Aa On receiving this command from the adver-
sary, the functionality marks the voting authority Aa as
corrupted.
RevealVotes On receiving this command from the adver-
sary, the functionality veriﬁes that both A1 and A2 are cor-
rupt. If this is the case, it sends the vote database S to the
adversary.
Halt On receiving this command from the adversary, the
functionality veriﬁes that at least one of the voting author-
ities is corrupt. If so, it outputs ⊥ to the veriﬁer and halts.
We can now state our main theorem:
Theorem 4.1. The Split-Ballot Voting Protocol UC-realizes
functionality Fvote, for an adversary that is fully adaptive
up to the end of the voting phase, but then statically de-
cides which of the voting authorities to corrupt (it can still
adaptively corrupt voters).
The reason for the restriction on the adversary’s adaptive-
ness is that the homomorphic encryption scheme we use is
committing.
Note that this limitation on adaptiveness only holds with
respect to the privacy of the votes under composition, since
an adversary whose only goal is to change the ﬁnal tally
can only gain by corrupting both voting authorities at the
beginning of the protocol.
3This is a fairly common assumption in cryptographic voting
protocols (appearing in [8, 4, 22, 9], among others).
Due to space constraints, we defer the proof of Theorem
4.1 to the full version of the paper.4
4.2 Receipt-Freeness
As previously discussed, in a voting protocol assuring pri-
vacy is not enough.
In order to prevent vote-buying and
coercion, we must ensure receipt-freeness: a voter shouldn’t
be able to prove how she voted even if she wants to. We
use the deﬁnition of receipt-freeness from [16], an extension
of Canetti and Gennaro’s incoercible computation [6]. This
deﬁnition of receipt-freeness is also simulation based, in the
spirit of our other security deﬁnitions.
Parties all receive a fake input, in addition to their real
one. A coerced player will use the fake input to answer
the adversary’s queries about the past view (before it was
coerced). The adversary is not limited to passive queries,
however. Once a player is coerced, the adversary can give
it an arbitrary strategy (i.e. commands the player should
follow instead of the real protocol interactions). We call co-
erced players that actually follow the adversary’s commands
“puppets”.
A receipt-free protocol,
in addition to specifying what
players should do if they are honest, must also specify what
players should do if they are coerced; we call this a “coercion-
resistance strategy” The coercion-resistance strategy is a
generalization of the “faking algorithm” in Canetti and Gen-
naro’s deﬁnition — the faking algorithm only supplies an an-
swer to a single query (“what was the randomness used for
the protocol”), while the coercion-resistance strategy must
tell the party how to react to any command given by the
adversary.
Intuitively, a protocol is receipt-free if no adversary can
distinguish between a party with real input x that is a pup-
pet and one that has a fake input x (but a diﬀerent real
input) and is running the coercion-resistance strategy. At
the same time, the computation’s output should not change
when we replace coerced parties running the coercion-resistance
strategy with parties running the honest protocol (with their
real inputs). Note that these conditions must hold even
when the coercion-resistance strategy is known to the ad-
versary.
In our original deﬁnition [16], the adversary can force a
party to abstain. We weaken this deﬁnition slightly, and
allow the adversary to force a party to vote randomly (in
most voting systems, a random vote is eﬀectively the same
as an abstention, so this is not much weaker). Under this
deﬁnition:
Theorem 4.2. The Split-Ballot voting protocol is receipt-
free, for any adversary that does not corrupt any of the vot-
ing authorities.
Due to space constraints, we defer the formal proof of this
theorem to the full version of the paper. However, the in-
tuition behind it is apparent from the coercion-resistance
strategy (described in Section 5.2).
5. SPLIT-BALLOT VOTING PROTOCOL
In this section we give an abstract description of the split-
ballot voting protocol (by abstract, we mean we that we de-
4An up to date version of the paper (with additional de-
tails) can be found in http://www.wisdom.weizmann.ac.
il/~naor/onpub.html
scribe the logical operations performed by the parties with-
out describing a physical implementation). In the interest of
clarity, we restrict ourselves to two voting authorities A1,A2,
n voters and a single poll question with answers in the group
Zm. We assume the existence of a homomorphic commit-
ment scheme (K, C) (with the properties deﬁned in Section
3.2) whose message space is a group (M, +), randomizer
space a group (R, +), and commitment space a group (C,·).
Furthermore, we assume the existence of homomorphic en-
cryption schemes with the corresponding message spaces.
5.1 Setup
The initial setup involves:
”
”
“
pk(M), sk(M)
1. Choosing the system parameters (these consist of the
commitment scheme public key and the encryption
scheme public/private key pair). Authority A2 runs
KG(M) and KG(R), producing
“
pk(R), sk(R)
(which it sends over the private chan-
It also runs K using the output of the
nel to A1.
random beacon as the public random string, and the
private coins used in running KG(M) and KG(R) as
the auxiliary. This produces the commitment public
key, cpk. Authority A2 now runs PK using the ran-
dom beacon to replace the veriﬁer (this produces a
public proof that the commitment key was generated
correctly).
and
2. Ballot preparation. Each voting authority prepares
at least 2n ballots.
Informally, each ballot contains
commitments to the numbers 0 through m − 1 in a
random order (each number corresponds to a candi-
date). We identify a ballot by the tuple w = (a, i, b) ∈
{0, 1} × [n] × {0, 1}, where Aa is the voting author-
ity that generated the ballot, i is the index of the
voter to whom it will be sent and b a ballot serial
number. The ballot Bw consists of a random permu-
tation πw : Zm (cid:3)→ Zm and a vector of commitments:
cw,πw (0), . . . , cw,πw (m−1), where
cw,πa,i(j)
.
= C
πw (j) , rw,πw (j)
,
and rw,0, . . . , rw,m−1 ∈R R is a vector of m random
values chosen by the authority.
`
´
5.2 Voting
The voter receives two ballots from each of the voting au-
thorities. Denote the ballots received by voter v ∈ {1, . . . , n}:
B1,v,0, B1,v,1, B2,v,0 and B2,v,1, and the voter’s response to
the poll question by xv ∈ Zm.
Informally, the voter uses
a trivial secret sharing scheme to mask her vote: she splits
it into two random shares whose sum is xv. Each share is
sent to a diﬀerent authority (by choosing the corresponding
commitment from the ballot). More formally:
1. The voter receives ballots B1,v,0, B1,v,1, B2,v,0 and
B2,v,1 from the authorities and enters the “voting booth”.
The voter chooses, uniformly at random, two bits
bv,1, bv,2 ∈R {0, 1} and a value tv ∈R Zm. The value
tv is one “secret share” of the vote, the other will be
xv − tv. Bit bv,a determines which ballot received from
authority Aa will be used for voting (the other is used
only for veriﬁcation). The voter opens ballots B1,v,bv,1
and B2,v,bv,2 and leaves the other two ballots sealed.
2. To vote, the voter selects s1,v
.
= c1,v,bv,1 ,tv (i.e., the
.
= c2,v,bv,2 ,xv−tv (the com-
commitment to tv) and s2,v
mitment to xv−tv, where xv−tv is computed in Zm).5
3. The voter then physically deletes the description of
π1,v,bv,1 from B1,v,bv,1 and the description of π2,v,bv,2
from B2,v,bv,2 . After this step the voter “leaves the
voting booth”.
4. The voter sends s1,v to A1 and s2,v to A2 (“sending”
the ballot can consist of running it through a scanner
at the polling place). The voting authorities verify
that the proper erasures were performed and that two
of the ballots are still sealed.
5. The voter opens the two sealed (unvoted) ballots.
6. Authority A1 publishes the tuple (1, v, s1,v) on the
bulletin board and authority A2 publishes the tuple
(2, v, s2,v).
7. For a ∈ {1, 2}, authority Aa publishes Ba,v,1−bv,a and
ra,v,1−bv,a,0, . . . , ra,v,1−bv,a,m−1 to the public bulletin
board (i.e., it opens the commitments for the ballot
that wasn’t used to cast the vote).
8. The voter veriﬁes that commitments for the voted bal-
lots have been correctly published (they match the val-
ues sent in step 2), and that both unvoted ballots were
correctly published in their entirety.
5.2.1 Coercion-Resistance Strategy. We assume the
adversary cannot observe the voter between steps 1 and 3 of
the voting phase (these steps are performed while the voter
is “in the voting booth”).
If the voter is coerced before step 1, the voter follows the
adversary’s strategy precisely, but uses random permuta-
tions instead of those revealed on the opened ballots. Be-
cause of the forced erasure, the adversary will not be able
to tell whether the voter used the correct permutations or
not. By using random permutations, the end result is that
voter votes randomly (coercing a voter to vote randomly is
an attack we explicitly allow).
If the voter is coerced at step 1 or later (after entering
the voting booth), she follows the regular voting protocol
in steps 1 through 3. Even if she is coerced before step
3, she lies to the adversary and pretends the coercion oc-
curred at step 3 (the adversary cannot tell which step in
the protocol the voter is executing while the voter is in the
booth).
In this case, the adversary cannot give the voter
a voting strategy, except one that will invalidate the ballot
(since the voter has no more “legal” choices left). The voter
must still convince the adversary that her vote was for the
“fake input” provided by the adversary rather than her real
input. To do this, she chooses random permutations that
are consistent with the fake input and her chosen commit-
ments, and pretends these were the permutations revealed
on the opened ballots. Using the example in Figure 2.1,
if Sarah was trying to convince a coercer that she actually
voted for John (instead of Thomas), she would choose ran-
domly one of the options for John (e.g., C,H), then claim
that the left permutation she saw had C as the fourth value
and the right permutation had H on the second (ordering
the others randomly). Note that if the adversary forces the
voter to invalidate her ballot, she will do so (but this is a
forced abstention, which we explicitly do not prevent).
5This selection can be implemented, for example, by having
the voter mark a physical ballot with a pen (such as the
method described in Section 2.2).
5.3 Tally
The tally stage is performed by the voting authorities and
does not require voter participation (for the intuition be-
hind it, see Section 2.1). Before the start of the tally stage,
both authorities know s1,1, s2,1, . . . , s1,n, s2,n (this was pub-
lished on the public bulletin board in the voting phase). Au-
thority A1 also knows t1, . . . , tn, while authority A2 knows
x1 − t1, . . . , xn − tn. Below, we give a simpliﬁed version of