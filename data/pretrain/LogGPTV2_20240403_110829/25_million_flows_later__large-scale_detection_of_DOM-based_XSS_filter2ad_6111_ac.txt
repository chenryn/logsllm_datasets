closing tag is regarded as a TextNode by the HTML parser.
In many cases executing a payload within a TextNode is
straight forward. As we do not need to break out of the
node itself, we can simply open a script tag and execute
a payload. However, if the TextNode is placed between an
opening and a closing tag of a script or iframe we again
have to add closing tags if necessary.
T extN odeBS := [closingT ags]
(4)
innerHTML vs document.write.
After we have generated the break out sequence for HTML
context exploits, the system needs to choose a payload to
1197execute. When doing so, some subtle diﬀerences in the han-
dling of string-to-HTML conversion comes into play. When
using innerHTML, outerHTML or adjacentHTML browsers re-
act diﬀerently than document.write in terms of script exe-
cution. While document.write inserts script elements into
the DOM and executes them immediately, innerHTML only
performs the ﬁrst step, but does not execute the script. So
adding the following payload for an innerHTML ﬂow would
not result in a successful exploit:
However, it is still possible to execute scripts via an injec-
tion through innerHTML. In order to do so, the framework
makes use of event handlers:
When innerHTML inserts the img tag, the browser creates
an HTTP request to the non-existing resource. Obviously,
this request will fail and trigger the onerror event handler
that executes the given payload. Depending on the sink we
simply choose one of these two payloads.
5.2.2 JavaScript context-speciﬁc generation
JavaScript context-speciﬁc generation is necessary when-
ever a data ﬂow ends within a sink that interprets a string
as JavaScript code. This is the case for functions such as
eval & Function, Event handlers (such as onload and on-
error) and DOM properties such as script.textContent,
script.text and script.innerText. While browsers are
very forgiving when parsing and executing syntactically in-
correct HTML, they are quite strict when it comes to Java-
Script code execution. If the JavaScript parser encounters a
syntax error, it cancels the script execution for the complete
block/function. Therefore, the big challenge for the exploit
generator is to generate a syntactically correct exploit, that
will not cause the parser to cancel the execution. In order to
do so, the system again has to determine the exact location
of the tainted bytes.
Listing 3 shows a very simple vulnerable piece of Java-
Script code. In the ﬁrst step, the code constructs a string of
benign/hard coded and tainted (location.href) parts. In a
second step, it executes the code using eval. Thereby, this
code can be exploited in slightly diﬀerent ways. Either the
attacker could break out of the variable x and inject his code
into the function named test, or he could break out of the
variable x and the function test and inject his code into the
top level JavaScript space. While the ﬁrst method requires
an additional invocation of the test function, the second ex-
ploit executes as soon as eval is called with a syntactically
correct code. However, for the last case, the complexity of
the break out sequence grows with the complexity of con-
structed code. Nevertheless, we do not want do rely on any
behavior of other non-controllable code or wait for a user in-
teraction to trigger an invocation of the manipulated code.
Therefore, we always seek to break out to the top level
of the JavaScript execution. In order to do so, our system
ﬁrst parses the JavaScript string and creates a syntax tree
of the code. Based on this tree and the taint information
we extract the branches that contain tainted values. Listing
4 shows the resulting syntax tree for our example code and
Listing 3 JavaScript context example
var code =
’ f u n c t i o n test (){ ’ +
’ var x = " ’ + l o c a t i o n . href + ’ "; ’
// inside f u n c t i o n test
+ ’ d o S o m e t h i n g ( x ); ’
+ ’} ’; // top level
eval ( code )
Listing 4 JavaScript Syntax Tree
FunctionDeclaration
I d e n t i f i e r : test
FunctionConstructor
I d e n t i f i e r : test
Block
Declaration
I d e n t i f i e r : x
StringLiteral : "http://example.org"
E x p r e s s i o n S t m t
S p e c i a l O p e r a t i o n : F U N C T I O N _ C A L L
R e f e r e n c e
I d e n t i f i e r : d o S o m e t h i n g
the extracted branch (in gray). For each of the extracted
branches the generator creates one break out sequence by
traversing the branch from top to bottom and adding a ﬁxed
sequence of closing/break out characters for each node. So
in our example the following steps are taken:
1. FunctionDeclaration: ’;’
2. FunctionConstructor: ”
3. Block: ’}’
4. Declaration: ’;’
5. StringLiteral: ’”’
6. Resulting Breakout Sequence: ’”;};’
To trigger the exploit we can simply construct the test
case as follows: Based on the source (location.href), the
system simple adds the break out sequence, an arbitrary
payload and the escape sequence to the URL of the page:
http://example.org/#";};__reportingFunction__();//
When executed within a browser, the string construction
process from Listing 3 is conducted and the following string
ﬂows into the eval call (Note: Line breaks are only inserted
for readability reasons):
f u n c t i o n test (){
var x = " http :// example . org /# " ;
};
_ _ r e p o r t i n g F u n c t i o n _ _ ();
// d o S o m e t h i n g ( x );}
6. EMPIRICAL STUDY
As mentioned earlier, an important motivation for our
work was to gain insight into the prevalence and nature of
potentially insecure data ﬂows in current JavaScript applica-
tions leading to DOM-based XSS. For this reason, we created
a Web crawling infrastructure capable of automatically ap-
plying our vulnerability detection and validation techniques
to a large set of real-world Web sites.
1198browser features that were needed for the crawling and an-
alyzing processes were realized in the form of a browser ex-
tension.
Following the general architecture of Chrome’s extension
model [8], the extension consists of a background and a con-
tent script (see Fig. 2). The background script’s purpose
is to request target URLs from the backend, assign these
URLs to the browser’s tabs (for each browser instance, the
extension opened a predeﬁned number of separate browser
tabs to parallelize the crawling process), and report the ﬁnd-
ings to the backend. The content script conducts all actions
that directly apply to individual Web documents, such as
collecting the hyperlinks contained in the page for the fur-
ther crawling process and processing the data ﬂow reports
from the taint-tracking engine (see Sec. 4.4). Furthermore,
the content script injects a small userscript into each Web
document, that prevents the examined page from display-
ing modal dialogues, such as alert() or confirm() message
boxes, which could interrupt the unobserved crawling pro-
cess.
After the background script assigns a URL to a tab, the
content script instructs the tab to load the URL and ren-
der the corresponding Web page. This implicitly causes all
further external (script) resources to be retrieved and all
scripts, that are contained in the page, to be executed. Af-
ter the page loading process has ﬁnished, a timeout is set to
allow asynchronous loading processes and script execution to
terminate. After the timeout has passed, the content script
packs all suspicious data ﬂows, which were reported during
execution of the analyzed page, and communicates them to
the background script for further processing.
In addition to data ﬂow and hyperlink data, the extension
also collects statistical information in respect to size and
nature of the JavaScripts that are used by the examined
sites.
6.2 Observed Data Flows
As mentioned above, our initial set of URLs consisted of
the Alexa top 5000. For each of these URLs we conducted
a shallow crawl, i.e., all same-domain links found in the re-
spective homepages were followed, resulting in 504,275 ac-
cessed Web pages. On average each of those Web document
consisted out of 8.64 frames resulting in a ﬁnal number of
4,358,031 (not necessary unique) URLs.
In total our infrastructure captured 24,474,306 data ﬂows
from potentially tainted sources to security sensitive sinks.
Please refer to Table 1 for details on the distribution of ﬂows,
depicted by their sources and sinks.
6.3 Selective Exploit Generation
As shown in the previous Section, the total number of
potentially vulnerable data ﬂows from insecure sources to
security sensitive sinks is surprisingly high. In our study, the
sheer number of found ﬂows exceeds the number of analyzed
pages by a factor of about 48.5.
Both our exploit generation and validation processes are
eﬃcient. Generating and testing an XSS exploit for a se-
lected data ﬂow requires roughly as much time as the initial
analyzing process of the corresponding Web page. However,
due to the large amount of observed ﬂows, testing all data
ﬂows would have required signiﬁcantly more time than the
actual crawling process. Hence, to balance our coverage and
broadness goals, we selected a subset out of all recorded, po-
Figure 2: Crawling infrastructure
6.1 Methodology & Architecture Overview
To obtain a realistic picture on the commonness of inse-
cure data ﬂows that might lead to DOM-based XSS, it is
essential to sample a suﬃciently large set of real-world Web
sites.
We designed our experiment set-up to meet these require-
ments, utilizing the following components: Our ﬂow-tracking
rendering engine to identify and record potentially unsafe
JavaScripts (as discussed in Sec. 4), our exploit generation
and validation framework (as presented in Sec. 5), and a
crawling infrastructure that automatically causes the brows-
ing engine to visit and examine a large set of URLs.
Our crawling infrastructure consisted of several browser
instances and a central backend, which steered the crawling
process. Each browser was outﬁtted with an extension that
provided the browser with the required external interface for
communication with the backend (see Fig. 2). In the follow-
ing paragraphs, we brieﬂy document both the backend’s and
the extension’s functionality.
6.1.1 Analysis engine: Central server backend
The main duty of the central analysis backend is to dis-
tribute the URLs of the examination targets to the browser
instances and the processing of the returned information.
The backend maintains a central URL queue, which was
initially populated with the Alexa Top 5000 domains and
subsequently ﬁlled with the URLs that were found by the
browsers during the crawling process.
The browser instances transmit their analysis report and
their ﬁndings to the backend. For each analyzed URL, anal-
ysis reports for several URLs are returned, as the browser
instances not only check the main page but also all con-
tained iframes.
In our study, we received results for an
average of 8.64 (sub-)frames for each URL that was given to
a browser instance. After pre-processing and initial ﬁlter-
ing, the backend passes the suspicious ﬂows to the exploit
generation unit (see Sec. 6.3).
6.1.2 Data collection: Browser Extension
As discussed in Section 4, we kept direct changes to the
browser’s core engine as small as possible, to avoid unwanted
side eﬀects and provide maintainability of our modiﬁcations.
Our patches to the browser’s internal implementation con-
sisted mainly in adding the taint-tracking capabilities to the
Javascript engine and DOM implementation. All further
Control'backend'Background'script'Tab'1'content''script'Web'page'''&''user'script'Tab'n'content''script'Web'page'''&''user'script'…'Background'script'Tab'1'content''script'Web'page'''&''user'script'Tab'n'content''script'Web'page'''&''user'script'…'…'Browser'1'Browser'm'1199HTML Sinks
JavaScript Sinks
URL Sinks
Cookie Sink
Web Storage Sinks
postMessage Sink
Total
URL
1,356,796
22,962
3,798,228
220,300
41,739
451,170
5,891,195
Cookie
1,535,299
359,962
2,556,709
10,227,050
65,772
77,202
14,821,994
document.referrer window.name
240,341
511
313,617
25,062
1,586
696
581,813
35,466
617,743
83,218
1,328,634
434
45,220
2,110,715
postMessage Web Storage
35,103
448,311
18,919
2,554
194
11,053