title:Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks
author:Weilin Xu and
David Evans and
Yanjun Qi
Detecting Adversarial Examples in Deep Neural Networks
Feature Squeezing:
Weilin Xu, David Evans, Yanjun Qi
University of Virginia
evadeML.org
Abstractâ€”Although deep neural networks
(DNNs) have
achieved great success in many tasks, recent studies have shown
they are vulnerable to adversarial examples. Such examples,
typically generated by adding small but purposeful distortions,
can frequently fool DNN models. Previous studies to defend
against adversarial examples mostly focused on reï¬ning the
DNN models, but have either shown limited success or suï¬€ered
from expensive computation. We propose a new strategy, feature
squeezing, that can be used to harden DNN models by detecting
adversarial examples. Feature squeezing reduces the search space
available to an adversary by coalescing samples that correspond
to many diï¬€erent feature vectors in the original space into a single
sample. By comparing a DNN modelâ€™s prediction on the original
input with that on squeezed inputs, feature squeezing detects
adversarial examples with high accuracy and few false positives.
This paper explores two types of feature squeezing: reducing
the color bit depth of each pixel and spatial smoothing. These
strategies are inexpensive and complementary to other defenses,
and can be combined in a joint detection framework to achieve
high detection rates against state-of-the-art attacks.
I.
Introduction
Deep Neural Networks (DNNs) perform exceptionally
well on many artiï¬cial intelligence tasks, including security-
sensitive applications like malware classiï¬cation [26], [8] and
face recognition [35]. Unlike when machine learning is used
in other ï¬elds, security applications involve intelligent and
adaptive adversaries responding to the deployed systems. Re-
cent studies have shown that attackers can force deep learning
object classiï¬cation models to mis-classify images by making
imperceptible modiï¬cations to pixel values. The maliciously
generated inputs are called â€œadversarial examplesâ€ [10], [39]
and are normally crafted using an optimization procedure to
search for small, but eï¬€ective, artiï¬cial perturbations.
The goal of this work is to harden DNN systems against
adversarial examples by detecting them successfully. Detecting
an attempted attack may be as important as predicting correct
outputs. When running locally, a classiï¬er that can detect
adversarial inputs may alert its users or take fail-safe actions
(e.g., a fully autonomous drone returns to its base) when it
spots adversarial inputs. For an on-line classiï¬er whose model
is being used (and possibly updated) through API calls from
external clients, the ability to detect adversarial examples may
enable the operator to identify malicious clients and exclude
their inputs. Another reason that detecting adversarial exam-
ples is important is because even with the strongest defenses,
adversaries will occasionally be able to get lucky and ï¬nd an
adversarial input. For asymmetrical security applications like
malware detection, the adversary may only need to ï¬nd a single
example that preserves the desired malicious behavior but is
classiï¬ed as benign to launch a successful attack. This seems
like a hopeless situation for an on-line classiï¬er operator, but
the game changes if the operator can detect even unsuccessful
attempts during an adversaryâ€™s search process.
Most of the previous work aiming to harden DNN sys-
tems, including like adversarial training and gradient masking
(details in Section II-C), focused on modifying the DNN
models themselves. In contrast, our work focuses on ï¬nding
simple and low-cost defensive strategies that alter the input
samples but leave the model unchanged. A few other recent
studies have proposed methods to detect adversarial examples
through sample statistics, training a detector, or prediction
inconsistency (Section II-D). Our approach, which we call
feature squeezing, is driven by the observation that the feature
input spaces are often unnecessarily large, and this vast input
space provides extensive opportunities for an adversary to
construct adversarial examples. Our strategy is to reduce the
degrees of freedom available to an adversary by â€œsqueezingâ€
out unnecessary input features.
The key to our approach is to compare the modelâ€™s predic-
tion on the original sample with its prediction on the sample
after squeezing, as depicted in Figure 1. If the original and
squeezed inputs produce substantially diï¬€erent outputs from
the model, the input is likely to be adversarial. By comparing
the diï¬€erence between predictions with a selected threshold
value, our system outputs the correct prediction for legitimate
examples and rejects adversarial inputs.
The approach generalizes to other domains where deep
learning is used, such as voice recognition and natural language
processing. Carlini et al. have demonstrated that lowering the
sampling rate helps to defend against the adversarial voice
commands [4]. Hosseini et al. proposed to perform spell
checking on the inputs of a character-based toxic text detection
system to defend against the adversarial examples [16]. Both
of them could be regard as an instance of feature squeezing.
Although feature squeezing generalizes to other domains,
here we focus on image classiï¬cation because it is the domain
where adversarial examples have been most extensively stud-
Fig. 1: Detecting adversarial examples. The model is evaluated on
both the original input and the input after being pre-processed by one or more
feature squeezers. If any of the predictions on the squeezed inputs are too
diï¬€erent from the original prediction, the input is determined to be adversarial.
ModelModelModelSqueezer1Squeezer2Prediction0Prediction1Prediction2maxğ‘‘â€™,ğ‘‘)>ğ‘‡YesInputğ¿â€™AdversarialNoLegitimateğ¿â€™ğ‘‘â€™ğ‘‘)ied. We explore two simple methods for squeezing features of
images: reducing the color depth of each pixel in an image,
and using spatial smoothing to reduce the diï¬€erences among
individual pixels. We demonstrate that feature squeezing sig-
niï¬cantly enhances the robustness of a model by predicting
correct labels of adversarial examples, while preserving the
accuracy on legitimate inputs (Section IV), thus enabling an
accurate detector for adversarial examples (Section V). Feature
squeezing appears to be both more accurate and general, and
less expensive, than previous methods.
Contributions. Our key contribution is introducing and evalu-
ating feature squeezing as a technique for detecting adversarial
examples. We introduce the general detection framework (de-
picted in Figure 1), and show how it can be instantiated to
accurately detect adversarial examples generated by a wide
range of state-of-the-art methods.
We study two instances of feature squeezing: reducing
color bit depth (Section III-A) and both local and non-local
spatial smoothing (Section III-B). We report on experiments
that show feature squeezing helps DNN models predict correct
classiï¬cation on adversarial examples generated by eleven
diï¬€erent and state-of-the-art attacks (Section IV).
Section V explains how we use feature squeezing for de-
tecting adversarial inputs in two distinct situations. In the ï¬rst
case, we (overly-optimistically) assume the model operator
knows the attack type and can select a single squeezer for
detection. Our results show that the eï¬€ectiveness of diï¬€erent
squeezers against various attacks varies. For instance, the 1-
bit depth reduction squeezer achieves a perfect 100% detection
rate on MNIST for six diï¬€erent attacks. However, this squeezer
is not as eï¬€ective against those attacks making substantial
changes to a small number of pixels (that can be detected
well by median smoothing). The model operator normally does
not know what attacks an adversary may use, so requires a
detection system to work well against any attack. We propose
combining multiple squeezers in a joint detection framework.
Our experiments show that joint-detection can successfully
detect adversarial examples from eleven state-of-the-art attacks
at the detection rates of 98% on MNIST and 85% on CIFAR-
10 and ImageNet, with low (below 5%) false positive rates.
Feature squeezing is complementary to other adversarial
defenses since it does not change the underlying model, and
can readily be composed with other defenses such as adver-
sarial training (Section IV-E). Although we cannot guarantee
an adaptive attacker cannot succeed against a particular feature
squeezing conï¬guration, our results show it is eï¬€ective against
state-of-the-art methods, and it considerably complicates the
task of an adaptive adversary even with full knowledge of the
model and defense (Section V-D).
II. Background
This section provides a brief
introduction to neural
networks, methods for ï¬nding adversarial examples, and
previously-proposed defenses.
A. Neural Networks
Deep Neural Networks (DNNs) can eï¬ƒciently learn highly-
accurate models from large corpora of training samples in
many domains [19], [13], [26]. Convolutional Neural Networks
(CNNs), ï¬rst popularized by LeCun et al. [21], perform
exceptionally well on image classiï¬cation. A deep CNN can
be written as a function g : X â†’ Y, where X represents the
input space and Y is the output space representing a categorical
set. For a sample, x âˆˆ X,
g(x) = fL( fLâˆ’1(. . . (( f1(x)))).
Each fi represents a layer, which can be a classical feed-
forward linear layer, rectiï¬cation layer, max-pooling layer, or
a convolutional layer that performs a sliding window operation
across all positions in an input sample. The last output layer,
fL, learns the mapping from a hidden space to the output space
(class labels) through a softmax function.
A training set contains Ntr
labeled inputs in which the
i-th input is denoted (xi, yi). When training a deep model,
parameters related to each layer are randomly initialized, and
input samples (xi, yi) are fed through the network. The output
of this network is a prediction g(xi) associated with the i-th
sample. To train the DNN, the diï¬€erence between prediction
output, g(xi), and its true label, yi,
is fed back into the
network using a back-propagation algorithm to update DNN
parameters.
B. Generating Adversarial Examples
An adversarial example is an input crafted by an adversary
with the goal of producing an incorrect output from a target
classiï¬er. Since ground truth, at least for image classiï¬cation
tasks, is based on human perception which is hard to model
or test, research in adversarial examples typically deï¬nes an
adversarial example as a misclassiï¬ed sample x(cid:48) generated by
perturbing a correctly-classiï¬ed sample x (a.k.a seed example)
by some limited amount.
Adversarial examples can be targeted, in which case the
adversaryâ€™s goal is for x(cid:48) to be classiï¬ed as a particular class
t, or untargeted, in which case the adversaryâ€™s goal is just for
x(cid:48) to be classiï¬ed as any class other than its correct class.
More formally, given x âˆˆ X and g(Â·), the goal of an targeted
adversary with target t âˆˆ Y is to ï¬nd an x(cid:48) âˆˆ X such that
g(x(cid:48)) = t âˆ§ âˆ†(x, x(cid:48)) â‰¤ 
(1)
where âˆ†(x, x(cid:48)) represents the diï¬€erence between input x and
x(cid:48). An untargeted adversaryâ€™s goal is to ï¬nd an x(cid:48) âˆˆ X such
that
g(x(cid:48)) (cid:44) g(x) âˆ§ âˆ†(x, x(cid:48)) â‰¤ .
(2)
The strength of the adversary, , measures the permissible
transformations. The distance metric, âˆ†(), and the adversarial
strength threshold, , are meant
to model how close an
adversarial example x(cid:48) needs to be to the original sample x
to â€œfoolâ€ a human observer.
Several techniques have been proposed to ï¬nd adversar-
ial examples. Szegedy et al. [39] ï¬rst observed that DNN
models are vulnerable to adversarial perturbation and used
the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-
BFGS) algorithm to ï¬nd adversarial examples. Their study also
found that adversarial perturbations generated from one DNN
model can also force other DNN models to produce incorrect
outputs. Subsequent papers have explored other strategies to
2
Fig. 2: Image examples with bit depth reduction. The ï¬rst
column shows images from MNIST, CIFAR-10 and Ima-
geNet, respectively. Other columns show squeezed versions
at diï¬€erent color-bit depths, ranging from 8 (original) to 1.
Fig. 3: Examples of adversarial attacks and feature squeezing
methods extracted from the MNIST dataset. The ï¬rst column
shows the original image and its squeezed versions, while the
other columns present
targeted
attacks are targeted-next.
the adversarial variants. All
generate adversarial manipulations, including using the linear
assumption behind a model [10], [28], saliency maps [32], and
evolutionary algorithms [29].
Equations (1) and (2) suggest two diï¬€erent parameters
for categorizing methods for ï¬nding adversarial examples:
whether they are targeted or untargeted, and the choice of âˆ†(),
which is typically an Lp-norm distance metric. When given a
m-dimensional vector z = x âˆ’ x(cid:48) = (z1, z2, . . . , zm)T âˆˆ Rm, the
Lp norm is deï¬ned by:
(cid:107)z(cid:107)p = p
|zi|p
(3)
(cid:118)(cid:116) m(cid:88)
i=1
(cid:114)(cid:80)
The three norms used as âˆ†() choices for popular adversarial
methods are:
â€¢ Lâˆ: ||z||âˆ = max
|zi|. The Lâˆ norm measures the maximum
change in any dimension. This means an Lâˆ attack is limited
by the maximum change it can make to each pixel, but can
alter all the pixels in the image by up to that amount.
i
â€¢ L2: ||z||2 =
i
z2
i . The L2 norm corresponds to the Eu-
clidean distance between x and x(cid:48). This distance can remain
small when many small changes are applied to many pixels.
â€¢ L0: ||z||0 = #{i| zi (cid:44) 0}. For images, this metric measures the
number of pixels that have been altered between x and x(cid:48),
so an L0 attack is limited by the number of pixels it can
alter.
We discuss the eleven attacking algorithms, grouped by the
norm they used for âˆ†, used in our experiments further below.
1) Fast Gradient Sign Method: FGSM (Lâˆ, Untargeted)
Goodfellow et al. hypothesized that DNNs are vulnerable
to adversarial perturbations because of their linear nature [10].
They proposed the fast gradient sign method (FGSM) for
eï¬ƒciently ï¬nding adversarial examples. To control the cost
of attacking, FGSM assumes that the attack strength at every
feature dimension is the same, essentially measuring the pertur-
bation âˆ†(x, x(cid:48)) using the Lâˆ-norm. The strength of perturbation
at every dimension is limited by the same constant parameter,
, which is also used as the amount of perturbation.
3
As an untargeted attack,
the perturbation is calculated
directly by using gradient vector of a loss function:
âˆ†(x, x(cid:48)) =  Â· sign(âˆ‡xJ(g(x), y))
(4)
Here the loss function, J(Â·,Â·), is the loss that have been used
in training the speciï¬c DNN model, and y is the correct label
for x. Equation (4) essentially increases the loss J(Â·,Â·) by
perturbing the input x based on a transformed gradient.
2) Basic Iterative Method: BIM (Lâˆ, Untargeted)
Kurakin et al. extended the FGSM method by applying it
multiple times with small step size [20]. This method clips
pixel values of intermediate results after each step to ensure
that they are in an -neighborhood of the original image x. For
the m-th iteration,
= x(cid:48)
(5)
The clipping equation, Clipx,(z), performs per-pixel clipping
on z so the result will be in the Lâˆ -neighborhood of the
source x [20].
m + Clipx,{Î± Â· sign(âˆ‡xJ(g(x(cid:48)
m), y))}
x(cid:48)
m+1
3) DeepFool (L2, Untargeted)
termed DeepFool, to search for adversarial examples [28]:
Moosavi et al. used a L2 minimization-based formulation,
âˆ†(x, x(cid:48)) := arg minz||z||2, subject to: g(x + z) (cid:44) g(x)
(6)
DeepFool searches for the minimal perturbation to fool a clas-
siï¬er and uses concepts from geometry to direct the search. For
linear classiï¬ers (whose decision boundaries are linear planes),
the region of the space describing a classiï¬erâ€™s output can
be represented by a polyhedron (whose plane faces are those
boundary planes deï¬ned by the classiï¬er). Then DeepFool
searches within this polyhedron for the minimal perturbation
that can change the classiï¬ers decision. For general non-
linear classiï¬ers, this algorithm uses an iterative linearization
procedure to get an approximated polyhedron.
4) Jacobian Saliency Map Approach: JSMA (L0, Targeted)
Papernot et al. [32] proposed the Jacobian-based saliency
map approach (JSMA) to search for adversarial examples
by only modifying a limited number of input pixels in an
image. As a targeted attack, JSMA iteratively perturbs pixels