title:Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks
author:Weilin Xu and
David Evans and
Yanjun Qi
Detecting Adversarial Examples in Deep Neural Networks
Feature Squeezing:
Weilin Xu, David Evans, Yanjun Qi
University of Virginia
evadeML.org
Abstract—Although deep neural networks
(DNNs) have
achieved great success in many tasks, recent studies have shown
they are vulnerable to adversarial examples. Such examples,
typically generated by adding small but purposeful distortions,
can frequently fool DNN models. Previous studies to defend
against adversarial examples mostly focused on reﬁning the
DNN models, but have either shown limited success or suﬀered
from expensive computation. We propose a new strategy, feature
squeezing, that can be used to harden DNN models by detecting
adversarial examples. Feature squeezing reduces the search space
available to an adversary by coalescing samples that correspond
to many diﬀerent feature vectors in the original space into a single
sample. By comparing a DNN model’s prediction on the original
input with that on squeezed inputs, feature squeezing detects
adversarial examples with high accuracy and few false positives.
This paper explores two types of feature squeezing: reducing
the color bit depth of each pixel and spatial smoothing. These
strategies are inexpensive and complementary to other defenses,
and can be combined in a joint detection framework to achieve
high detection rates against state-of-the-art attacks.
I.
Introduction
Deep Neural Networks (DNNs) perform exceptionally
well on many artiﬁcial intelligence tasks, including security-
sensitive applications like malware classiﬁcation [26], [8] and
face recognition [35]. Unlike when machine learning is used
in other ﬁelds, security applications involve intelligent and
adaptive adversaries responding to the deployed systems. Re-
cent studies have shown that attackers can force deep learning
object classiﬁcation models to mis-classify images by making
imperceptible modiﬁcations to pixel values. The maliciously
generated inputs are called “adversarial examples” [10], [39]
and are normally crafted using an optimization procedure to
search for small, but eﬀective, artiﬁcial perturbations.
The goal of this work is to harden DNN systems against
adversarial examples by detecting them successfully. Detecting
an attempted attack may be as important as predicting correct
outputs. When running locally, a classiﬁer that can detect
adversarial inputs may alert its users or take fail-safe actions
(e.g., a fully autonomous drone returns to its base) when it
spots adversarial inputs. For an on-line classiﬁer whose model
is being used (and possibly updated) through API calls from
external clients, the ability to detect adversarial examples may
enable the operator to identify malicious clients and exclude
their inputs. Another reason that detecting adversarial exam-
ples is important is because even with the strongest defenses,
adversaries will occasionally be able to get lucky and ﬁnd an
adversarial input. For asymmetrical security applications like
malware detection, the adversary may only need to ﬁnd a single
example that preserves the desired malicious behavior but is
classiﬁed as benign to launch a successful attack. This seems
like a hopeless situation for an on-line classiﬁer operator, but
the game changes if the operator can detect even unsuccessful
attempts during an adversary’s search process.
Most of the previous work aiming to harden DNN sys-
tems, including like adversarial training and gradient masking
(details in Section II-C), focused on modifying the DNN
models themselves. In contrast, our work focuses on ﬁnding
simple and low-cost defensive strategies that alter the input
samples but leave the model unchanged. A few other recent
studies have proposed methods to detect adversarial examples
through sample statistics, training a detector, or prediction
inconsistency (Section II-D). Our approach, which we call
feature squeezing, is driven by the observation that the feature
input spaces are often unnecessarily large, and this vast input
space provides extensive opportunities for an adversary to
construct adversarial examples. Our strategy is to reduce the
degrees of freedom available to an adversary by “squeezing”
out unnecessary input features.
The key to our approach is to compare the model’s predic-
tion on the original sample with its prediction on the sample
after squeezing, as depicted in Figure 1. If the original and
squeezed inputs produce substantially diﬀerent outputs from
the model, the input is likely to be adversarial. By comparing
the diﬀerence between predictions with a selected threshold
value, our system outputs the correct prediction for legitimate
examples and rejects adversarial inputs.
The approach generalizes to other domains where deep
learning is used, such as voice recognition and natural language
processing. Carlini et al. have demonstrated that lowering the
sampling rate helps to defend against the adversarial voice
commands [4]. Hosseini et al. proposed to perform spell
checking on the inputs of a character-based toxic text detection
system to defend against the adversarial examples [16]. Both
of them could be regard as an instance of feature squeezing.
Although feature squeezing generalizes to other domains,
here we focus on image classiﬁcation because it is the domain
where adversarial examples have been most extensively stud-
Fig. 1: Detecting adversarial examples. The model is evaluated on
both the original input and the input after being pre-processed by one or more
feature squeezers. If any of the predictions on the squeezed inputs are too
diﬀerent from the original prediction, the input is determined to be adversarial.
ModelModelModelSqueezer1Squeezer2Prediction0Prediction1Prediction2max𝑑’,𝑑)>𝑇YesInput𝐿’AdversarialNoLegitimate𝐿’𝑑’𝑑)ied. We explore two simple methods for squeezing features of
images: reducing the color depth of each pixel in an image,
and using spatial smoothing to reduce the diﬀerences among
individual pixels. We demonstrate that feature squeezing sig-
niﬁcantly enhances the robustness of a model by predicting
correct labels of adversarial examples, while preserving the
accuracy on legitimate inputs (Section IV), thus enabling an
accurate detector for adversarial examples (Section V). Feature
squeezing appears to be both more accurate and general, and
less expensive, than previous methods.
Contributions. Our key contribution is introducing and evalu-
ating feature squeezing as a technique for detecting adversarial
examples. We introduce the general detection framework (de-
picted in Figure 1), and show how it can be instantiated to
accurately detect adversarial examples generated by a wide
range of state-of-the-art methods.
We study two instances of feature squeezing: reducing
color bit depth (Section III-A) and both local and non-local
spatial smoothing (Section III-B). We report on experiments
that show feature squeezing helps DNN models predict correct
classiﬁcation on adversarial examples generated by eleven
diﬀerent and state-of-the-art attacks (Section IV).
Section V explains how we use feature squeezing for de-
tecting adversarial inputs in two distinct situations. In the ﬁrst
case, we (overly-optimistically) assume the model operator
knows the attack type and can select a single squeezer for
detection. Our results show that the eﬀectiveness of diﬀerent
squeezers against various attacks varies. For instance, the 1-
bit depth reduction squeezer achieves a perfect 100% detection
rate on MNIST for six diﬀerent attacks. However, this squeezer
is not as eﬀective against those attacks making substantial
changes to a small number of pixels (that can be detected
well by median smoothing). The model operator normally does
not know what attacks an adversary may use, so requires a
detection system to work well against any attack. We propose
combining multiple squeezers in a joint detection framework.
Our experiments show that joint-detection can successfully
detect adversarial examples from eleven state-of-the-art attacks
at the detection rates of 98% on MNIST and 85% on CIFAR-
10 and ImageNet, with low (below 5%) false positive rates.
Feature squeezing is complementary to other adversarial
defenses since it does not change the underlying model, and
can readily be composed with other defenses such as adver-
sarial training (Section IV-E). Although we cannot guarantee
an adaptive attacker cannot succeed against a particular feature
squeezing conﬁguration, our results show it is eﬀective against
state-of-the-art methods, and it considerably complicates the
task of an adaptive adversary even with full knowledge of the
model and defense (Section V-D).
II. Background
This section provides a brief
introduction to neural
networks, methods for ﬁnding adversarial examples, and
previously-proposed defenses.
A. Neural Networks
Deep Neural Networks (DNNs) can eﬃciently learn highly-
accurate models from large corpora of training samples in
many domains [19], [13], [26]. Convolutional Neural Networks
(CNNs), ﬁrst popularized by LeCun et al. [21], perform
exceptionally well on image classiﬁcation. A deep CNN can
be written as a function g : X → Y, where X represents the
input space and Y is the output space representing a categorical
set. For a sample, x ∈ X,
g(x) = fL( fL−1(. . . (( f1(x)))).
Each fi represents a layer, which can be a classical feed-
forward linear layer, rectiﬁcation layer, max-pooling layer, or
a convolutional layer that performs a sliding window operation
across all positions in an input sample. The last output layer,
fL, learns the mapping from a hidden space to the output space
(class labels) through a softmax function.
A training set contains Ntr
labeled inputs in which the
i-th input is denoted (xi, yi). When training a deep model,
parameters related to each layer are randomly initialized, and
input samples (xi, yi) are fed through the network. The output
of this network is a prediction g(xi) associated with the i-th
sample. To train the DNN, the diﬀerence between prediction
output, g(xi), and its true label, yi,
is fed back into the
network using a back-propagation algorithm to update DNN
parameters.
B. Generating Adversarial Examples
An adversarial example is an input crafted by an adversary
with the goal of producing an incorrect output from a target
classiﬁer. Since ground truth, at least for image classiﬁcation
tasks, is based on human perception which is hard to model
or test, research in adversarial examples typically deﬁnes an
adversarial example as a misclassiﬁed sample x(cid:48) generated by
perturbing a correctly-classiﬁed sample x (a.k.a seed example)
by some limited amount.
Adversarial examples can be targeted, in which case the
adversary’s goal is for x(cid:48) to be classiﬁed as a particular class
t, or untargeted, in which case the adversary’s goal is just for
x(cid:48) to be classiﬁed as any class other than its correct class.
More formally, given x ∈ X and g(·), the goal of an targeted
adversary with target t ∈ Y is to ﬁnd an x(cid:48) ∈ X such that
g(x(cid:48)) = t ∧ ∆(x, x(cid:48)) ≤ 
(1)
where ∆(x, x(cid:48)) represents the diﬀerence between input x and
x(cid:48). An untargeted adversary’s goal is to ﬁnd an x(cid:48) ∈ X such
that
g(x(cid:48)) (cid:44) g(x) ∧ ∆(x, x(cid:48)) ≤ .
(2)
The strength of the adversary, , measures the permissible
transformations. The distance metric, ∆(), and the adversarial
strength threshold, , are meant
to model how close an
adversarial example x(cid:48) needs to be to the original sample x
to “fool” a human observer.
Several techniques have been proposed to ﬁnd adversar-
ial examples. Szegedy et al. [39] ﬁrst observed that DNN
models are vulnerable to adversarial perturbation and used
the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-
BFGS) algorithm to ﬁnd adversarial examples. Their study also
found that adversarial perturbations generated from one DNN
model can also force other DNN models to produce incorrect
outputs. Subsequent papers have explored other strategies to
2
Fig. 2: Image examples with bit depth reduction. The ﬁrst
column shows images from MNIST, CIFAR-10 and Ima-
geNet, respectively. Other columns show squeezed versions
at diﬀerent color-bit depths, ranging from 8 (original) to 1.
Fig. 3: Examples of adversarial attacks and feature squeezing
methods extracted from the MNIST dataset. The ﬁrst column
shows the original image and its squeezed versions, while the
other columns present
targeted
attacks are targeted-next.
the adversarial variants. All
generate adversarial manipulations, including using the linear
assumption behind a model [10], [28], saliency maps [32], and
evolutionary algorithms [29].
Equations (1) and (2) suggest two diﬀerent parameters
for categorizing methods for ﬁnding adversarial examples:
whether they are targeted or untargeted, and the choice of ∆(),
which is typically an Lp-norm distance metric. When given a
m-dimensional vector z = x − x(cid:48) = (z1, z2, . . . , zm)T ∈ Rm, the
Lp norm is deﬁned by:
(cid:107)z(cid:107)p = p
|zi|p
(3)
(cid:118)(cid:116) m(cid:88)
i=1
(cid:114)(cid:80)
The three norms used as ∆() choices for popular adversarial
methods are:
• L∞: ||z||∞ = max
|zi|. The L∞ norm measures the maximum
change in any dimension. This means an L∞ attack is limited
by the maximum change it can make to each pixel, but can
alter all the pixels in the image by up to that amount.
i
• L2: ||z||2 =
i
z2
i . The L2 norm corresponds to the Eu-
clidean distance between x and x(cid:48). This distance can remain
small when many small changes are applied to many pixels.
• L0: ||z||0 = #{i| zi (cid:44) 0}. For images, this metric measures the
number of pixels that have been altered between x and x(cid:48),
so an L0 attack is limited by the number of pixels it can
alter.
We discuss the eleven attacking algorithms, grouped by the
norm they used for ∆, used in our experiments further below.
1) Fast Gradient Sign Method: FGSM (L∞, Untargeted)
Goodfellow et al. hypothesized that DNNs are vulnerable
to adversarial perturbations because of their linear nature [10].
They proposed the fast gradient sign method (FGSM) for
eﬃciently ﬁnding adversarial examples. To control the cost
of attacking, FGSM assumes that the attack strength at every
feature dimension is the same, essentially measuring the pertur-
bation ∆(x, x(cid:48)) using the L∞-norm. The strength of perturbation
at every dimension is limited by the same constant parameter,
, which is also used as the amount of perturbation.
3
As an untargeted attack,
the perturbation is calculated
directly by using gradient vector of a loss function:
∆(x, x(cid:48)) =  · sign(∇xJ(g(x), y))
(4)
Here the loss function, J(·,·), is the loss that have been used
in training the speciﬁc DNN model, and y is the correct label
for x. Equation (4) essentially increases the loss J(·,·) by
perturbing the input x based on a transformed gradient.
2) Basic Iterative Method: BIM (L∞, Untargeted)
Kurakin et al. extended the FGSM method by applying it
multiple times with small step size [20]. This method clips
pixel values of intermediate results after each step to ensure
that they are in an -neighborhood of the original image x. For
the m-th iteration,
= x(cid:48)
(5)
The clipping equation, Clipx,(z), performs per-pixel clipping
on z so the result will be in the L∞ -neighborhood of the
source x [20].
m + Clipx,{α · sign(∇xJ(g(x(cid:48)
m), y))}
x(cid:48)
m+1
3) DeepFool (L2, Untargeted)
termed DeepFool, to search for adversarial examples [28]:
Moosavi et al. used a L2 minimization-based formulation,
∆(x, x(cid:48)) := arg minz||z||2, subject to: g(x + z) (cid:44) g(x)
(6)
DeepFool searches for the minimal perturbation to fool a clas-
siﬁer and uses concepts from geometry to direct the search. For
linear classiﬁers (whose decision boundaries are linear planes),
the region of the space describing a classiﬁer’s output can
be represented by a polyhedron (whose plane faces are those
boundary planes deﬁned by the classiﬁer). Then DeepFool
searches within this polyhedron for the minimal perturbation
that can change the classiﬁers decision. For general non-
linear classiﬁers, this algorithm uses an iterative linearization
procedure to get an approximated polyhedron.
4) Jacobian Saliency Map Approach: JSMA (L0, Targeted)
Papernot et al. [32] proposed the Jacobian-based saliency
map approach (JSMA) to search for adversarial examples
by only modifying a limited number of input pixels in an
image. As a targeted attack, JSMA iteratively perturbs pixels