core switches learn their levels once they conﬁrm that all
ports are connected to aggregation switches.
Algorithm 1 presents the processing performed by each
switch in response to LDMs. Lines 2-4 are concerned with
position assignment and will be described below. In line 6,
the switch updates the set of switch neighbors that it has
heard from. In lines 7-8, if a switch is not connected to more
than k/2 neighbor switches for suﬃciently long, it concludes
that it is an edge switch. The premise for this conclusion is
that edge switches have at least half of their ports connected
to end hosts. Once a switch comes to this conclusion, on any
subsequent LDM it receives, it infers that the corresponding
incoming port is an upward facing one. While not shown for
simplicity, a switch can further conﬁrm its notion of position
by sending pings on all ports. Hosts will reply to such pings
but will not transmit LDMs. Other PortLand switches will
both reply to the pings and transmit LDMs.
In lines 10-11, a switch receiving an LDM from an edge
switch on an upward facing port concludes that it must be
an aggregation switch and that the corresponding incoming
port is a downward facing port. Lines 12-13 handle the case
where core/aggregation switches transmit LDMs on down-
ward facing ports to aggregation/edge switches that have
not yet set the direction of some of their ports.
Determining the level for core switches is somewhat more
complex, as addressed by lines 14-20. A switch that has
not yet established its level ﬁrst veriﬁes that all of its active
ports are connected to other PortLand switches (line 14). It
then veriﬁes in lines 15-18 that all neighbors are aggregation
switches that have not yet set the direction of their links
(aggregation switch ports connected to edge switches would
have already been determined to be downward facing). If
these conditions hold, the switch can conclude that it is a
core switch and set all its ports to be downward facing (line
20).
Edge switches must acquire a unique position number in
2 − 1. This process is depicted in
each pod in the range of 0.. k
Algorithm 2. Intuitively, each edge switch proposes a ran-
domly chosen number in the appropriate range to all aggre-
gation switches in the same pod. If the proposal is veriﬁed
by a majority of these switches as unused and not tenta-
44tively reserved, the proposal is ﬁnalized and this value will
be included in future LDMs from the edge switch. As shown
in lines 2-4 and 29 of Algorithm 1, aggregation switches will
hold a proposed position number for some period of time
before timing it out in the case of multiple simultaneous
proposals for the same position number.
LDP leverages the fabric manager to assign unique pod
numbers to all switches in the same pod.
In lines 8-9 of
Algorithm 2, the edge switch that adopts position 0 requests
a pod number from the fabric manager. This pod number
spreads to the rest of the pod in lines 21-22 of Algorithm 1.
For space constraints, we leave a description of the entire
algorithm accounting for a variety of failure and partial con-
nectivity conditions to separate work. We do note one of the
interesting failure conditions, miswiring. Even in a data cen-
ter environment, it may still be possible that two host facing
ports inadvertently become bridged. For example, someone
may inadvertently plug an Ethernet cable between two out-
ward facing ports, introducing a loop and breaking some of
the important PortLand forwarding properties. LDP pro-
tects against this case as follows. If an uninitialized switch
begins receiving LDMs from an edge switch on one of its
ports, it must be an aggregation switch or there is an error
condition. It can conclude there is an error condition if it
receives LDMs from aggregation switches on other ports or
if most of its active ports are host-connected (and hence re-
ceive no LDMs). In an error condition, the switch disables
the suspicious port and signals an administrator exception.
3.5 Provably Loop Free Forwarding
Once switches establish their local positions using LDP,
they employ updates from their neighbors to populate their
forwarding tables. For instance, core switches learn the pod
number of directly-connected aggregation switches. When
forwarding a packet, the core switch simply inspects the bits
corresponding to the pod number in the PMAC destination
address to determine the appropriate output port.
Similarly, aggregation switches learn the position number
of all directly connected edge switches. Aggregation switches
must determine whether a packet is destined for a host in
the same or diﬀerent pod by inspecting the PMAC. If in the
same pod, the packet must be forwarded to an output port
corresponding to the position entry in the PMAC.
If in a diﬀerent pod, the packet may be forwarded along
any of the aggregation switch’s links to the core layer in the
fault-free case. For load balancing, switches may employ
any number of techniques to choose an appropriate output
port. The fabric manager would employ additional ﬂow ta-
ble entries to override the default forwarding behavior for
individual ﬂows. However, this decision is orthogonal to
this work, and so we assume a standard technique such as
ﬂow hashing in ECMP [16].
PortLand maps multicast groups to a core switch using a
deterministic hash function. PortLand switches forward all
multicast packets towards this core, e.g., using ﬂow hashing
to pick among available paths. With simple hardware sup-
port, the hash function may be performed in hardware with
no additional state in the fault-free case (exceptions for fail-
ures could be encoded in switch SRAM). Without hardware
support, there would be one entry per multicast group. Edge
switches forward IGMP join requests to the fabric manager
using the PMAC address of the joining host. The fabric
manager then installs forwarding state in all core and aggre-
gation switches necessary to ensure multicast packet delivery
to edge switches with at least one interested host.
Our forwarding protocol is provably loop free by observ-
ing up-down semantics [26] in the forwarding process as ex-
plained in Appendix A. Packets will always be forwarded
up to either an aggregation or core switch and then down
toward their ultimate destination. We protect against tran-
sient loops and broadcast storms by ensuring that once a
packet begins to travel down, it is not possible for it to travel
back up the topology. There are certain rare simultaneous
failure conditions where packets may only be delivered by,
essentially, detouring back down to an aggregation switch to
get to a core switch capable of reaching a given destination.
We err on the side of safety and prefer to lose connectivity
in these failure conditions rather than admit the possibility
of loops.
3.6 Fault Tolerant Routing
Figure 4: Unicast: Fault detection and action.
Given a largely ﬁxed baseline topology and the ability
to forward based on PMACs, PortLand’s routing proto-
col is largely concerned with detecting switch and link fail-
ure/recovery. LDP exchanges (Section 3.4) also serve the
dual purpose of acting as liveness monitoring sessions. We
describe our failure recovery process using an example, as
depicted in Figure 4. Upon not receiving an LDM (also
referred to as a keepalive in this context) for some conﬁg-
urable period of time, a switch assumes a link failure in step
1. The detecting switch informs the fabric manager about
the failure in step 2. The fabric manager maintains a logical
fault matrix with per-link connectivity information for the
entire topology and updates it with the new information in
step 3. Finally, in step 4, the fabric manager informs all
aﬀected switches of the failure, which then individually re-
calculate their forwarding tables based on the new version
of the topology. Required state for network connectivity is
modest, growing with k3/2 for a fully-conﬁgured fat tree
built from k-port switches.
Traditional routing protocols require all-to-all commu-
nication among n switches with O(n2) network messages
and associated processing overhead. PortLand requires
O(n) communication and processing, one message from the
switch detecting failure to the fabric manager and, in the
worst case, n messages from the fabric manager to aﬀected
switches.
Fabric ManagerFault MatrixIn PortVLAN00:02:00:02:00:010FFFF0800N/W DstN/W Src......--Out Port23421Dst MAC00:00:01:02:00:01Src MACType45Figure 5: Multicast: Fault detection and action.
Figure 6: Multicast: After fault recovery.
We now consider fault tolerance for the multicast and
broadcast case. Relative to existing protocols, we consider
failure scenarios where there is no single spanning tree
rooted at a core switch able to cover all receivers for a
multicast group or broadcast session. Consider the example
in Figure 5. Here, we have a multicast group mapped to
the left-most core switch. There are three receivers, spread
across pods 0 and 1. A sender forwards packets to the
designated core, which in turn distributes the packets to the
receivers. In step 1, two highlighted links in pod 0 simulta-
neously fail. Two aggregation switches detect the failure in
step 2 and notify the fabric manager, which in turn updates
its fault matrix in step 3. The fabric manager calculates
forwarding entries for all aﬀected multicast groups in step
4.
In this example, recovering from the failure requires for-
warding through two separate aggregation switches in pod
0. However, there is no single core switch with simultaneous
connectivity to both aggregation switches. Hence, a rela-
tively simple failure scenario would result in a case where
no single core-rooted tree can cover all interested receivers.
The implications are worse for broadcast. We deal with
this scenario by calculating a greedy set cover for the set of
receivers associated with each multicast group. This may
result in more than one designated core switch associated
with a multicast or broadcast group. The fabric manager
inserts the required forwarding state into the appropriate
tables in step 5 of Figure 5.
Finally, Figure 6 depicts the forwarding state for the
sender after the failure recovery actions. The multicast
sender’s edge switch now forwards two copies of each packet
to two separate cores that split the responsibility for trans-
mitting the multicast packet to the receivers.
3.7 Discussion
Given an understanding of the PortLand architecture, we
now compare our approach to two previous techniques with
similar goals, TRILL [24] and SEATTLE [10]. Table 1 sum-
marizes the similarities and diﬀerences along a number of di-
mensions. The primary diﬀerence between the approaches is
that TRILL and SEATTLE are applicable to general topolo-
gies. PortLand on the other hand achieves its simplicity and
eﬃciency gains by assuming a multi-rooted tree topology
such as those typically found in data center settings.
For forwarding, both TRILL and SEATTLE must in the
worst case maintain entries for every host in the data cen-
ter because they forward on ﬂat MAC addresses. While in
some enterprise deployment scenarios the number of popu-
lar destination hosts is limited, many data center applica-
tions perform all-to-all communication (consider search or
MapReduce) where every host talks to virtually all hosts
in the data center over relatively small time periods. Port-
Land forwards using hierarchical PMACs resulting in small
forwarding state. TRILL employs MAC-in-MAC encapsu-
lation to limit forwarding table size to the total number of
switches, but must still maintain a rewriting table with en-
tries for every global host at ingress switches.
Both TRILL and SEATTLE employ a broadcast-based
link state protocol to discover the network topology. Port-
Land leverages knowledge of a baseline multi-rooted tree to
allow each switch to establish its topological position based
on local message exchange. We further leverage a logically
centralized fabric manager to distribute failure information.
TRILL handles ARP locally since all switches maintain
global topology knowledge. In TRILL, the link state proto-
col further broadcasts information about all hosts connected
to each switch. This can add substantial overhead, especially
when considering virtual machine multiplexing. SEATTLE
distributes ARP state among switches using a one-hop DHT.
All switches register the IP address to MAC mapping for
their local hosts to a designated resolver. ARPs for an IP
address may then be forwarded to the resolver rather than
broadcast throughout the network.
While decentralized and scalable, this approach does ad-
mit unavailability of otherwise reachable hosts during the
recovery period (i.e., several seconds) after a resolver switch
fails. Worse, simultaneous loss of soft state in both the re-
solving switch and a host’s ingress switch may leave certain
hosts unreachable for an extended period of time. PortLand
protects against these failure conditions by falling back to
broadcast ARPs in the case where a mapping is unavailable
in the fabric manager and associated state is lost. We are
able to do so because the PortLand broadcast protocol is
eﬃcient, fault tolerant, and provably loop free.
To protect against forwarding loops, TRILL adds a sec-
ondary TRILL header to each packet with a TTL ﬁeld. Un-
fortunately, this means that switches must both decrement
Fabric ManagerFault MatrixMulticast StateMulticast MAC01:5E:E1:00:00:24Subscribers0,3,6Roots16RRSR43521In PortVLAN01:5E:E1:00:00:240FFFF0800N/W DstN/W Src......--Out Port3Dst MAC00:01:00:02:00:01Src MACTypeFabric ManagerFault MatrixRRSRMulticast StateMulticast MAC01:5E:E1:00:00:24Subscribers0,3,6Roots16, 18In PortVLAN01:5E:E1:00:00:240FFFF0800N/W DstN/W Src......--Out Port2,3Dst MAC00:01:00:02:00:01Src MACType46System
Topology
Forwarding
Switch State Addressing
Routing
ARP
Loops
Multicast
TRILL
General
O(number of
global hosts)
Flat;
MAC-in-MAC
encapsulation
Switch
broadcast
SEATTLE General
O(number of
global hosts)
Flat
PortLand Multi-rooted
tree
O(number of
local ports)
Hierarchical
Switch
broadcast
Location
Discovery
Protocol;
Fabric
manager for
faults
All switches
map MAC
address to
remote switch
One-hop DHT
TRILL header
with TTL
ISIS extensions
based on
MOSPF
Unicast loops
possible
New construct:
groups
Fabric
manager
Provably loop
free; no
additional
header
Broadcast-free
routing;
multi-rooted
spanning trees
Table 1: System comparison
the TTL and recalculate the CRC for every frame, adding
complexity to the common case. SEATTLE admits routing
loops for unicast traﬃc. It proposes a new “group” construct
for broadcast/multicast traﬃc. Groups run over a single
spanning tree, eliminating the possibility of loops for such
traﬃc. PortLand’s forwarding is provably loop free with
no additional headers.
It further provides native support
for multicast and network-wide broadcast using an eﬃcient
fault-tolerance mechanism.
4.
IMPLEMENTATION
4.1 Testbed Description
Our evaluation platform closely matches the layout in Fig-
ure 1. Our testbed consists of 20 4-port NetFPGA PCI card
switches [19]. Each switch contains 4 GigE ports along with
Xilinx FPGA for hardware extensions. We house the NetF-
PGAs in 1U dual-core 3.2 GHz Intel Xeon machines with
3GB RAM. The network interconnects 16 end hosts, 1U
quad-core 2.13GHz Intel Xeon machines with 3GB of RAM.
All machines run Linux 2.6.18-92.1.18el5.
The switches run OpenFlow v0.8.9r2 [4], which provides
the means to control switch forwarding tables. One beneﬁt
of employing OpenFlow is that it has already been ported to
run on a variety of hardware platforms, including switches
from Cisco, Hewlett Packard, and Juniper. This gives us
some conﬁdence that our techniques may be extended to
commercial platforms using existing software interfaces and
hardware functionality. Each switch has a 32-entry TCAM