is operated by provider.com using the ID from certificates. How-
ever, myvps.com is running its own mail server on a VPS hosted
with provider.com. In fact, this example represents a situation that
is hard to identify both automatically and correctly: VPS servers
hosted with web hosting companies. Certain web hosting compa-
nies (e.g., GoDaddy with domain name secureserver.net) allow
their VPS servers to create certificates under specific domain names
(e.g., vps123.secureserver.net). Similarly, as mentioned above, cer-
tificates can be misleading when third-party providers present their
customer’s certificates. Since there is no good way to automatically
detect such cases without prior knowledge, we have to identify
such situations manually.
Another source of error comes from Banner/EHLO messages.
Recall that Banner/EHLO messages are unrestricted text. Thus, it
is possible to falsely claim to be mx.google.com in Banner/EHLO
messages. Since our approach prioritizes Banner/EHLO messages
over the MX record, we would mislabel it as google.com.
To efficiently find instances of misidentifications, we use the
observation that the corner cases mentioned above are for unpop-
ular servers, with few domains pointing at them. For example, IP
addresses used by VPS servers (and associated certificates) would
only show up a handful of times in our dataset. By contrast, IP
addresses (and their associated certificates) used by MX records
of popular third-party mail providers would generally be much
more common in our dataset, as those MX records would be used
by many domains. Thus, it is possible to quickly find potentially
misidentified MX records by looking at the number of domains
pointing at them.
We identify potential instances of misidentifications using the
observation above. We keep two counters globally. We keep track
of the number of domains that point to each IP address (numI P )
and each certificate (numCer t ). For each IP address, the confidence
score of its mail provider ID inference is max(numI P , numCer t).
If an IP address does not have certificate information, numCer t
is ignored. For any dataset of a reasonable size, this score largely
reduces the number of cases we need to examine. That said, it is
still unrealistic to perform such manual work for all the providers
on large datasets. Thus, we only check for misidentifications for
large providers.
Once we have identified potential candidates to examine, we
employ various heuristics to ease the process of manually going
through all of them. For example, we can quickly determine a server
is falsely claiming to be google.com if it does not reside in Google’s
AS. Similarly, we observe that GoDaddy uses specific hostnames for
their dedicated servers (e.g., mailstore1.secureserver.net) and
Who’s Got Your Mail? Characterizing Mail Service Provider Usage
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 4: Accuracy of different approaches on 200 domains sampled from the three lists of target domains.
different patterns for VPS servers (e.g., s1-2-3.secureserver.net).
Such observations can help us quickly sift through all candidates.
Identifying Mail Provider ID for Domain. At the end, every
3.2.5
MX record will have an assigned mail provider ID. This assignment
could be either based on TLS certificate information, Banner/EHLO
messages, or the MX record itself. Based on the MX record that
a domain uses we can assign a mail provider to that domain. In
the case that a domain has more than one primary MX record
(multiple MX records with the same priority but different provider
IDs, which happens occasionally), we split the domain across the
multiple providers.
3.3 Relative Accuracy of Approaches
The priority-based approach combines the use of TLS certificates,
Banner/EHLO messages, and MX records. Each of these sources
could be independently used to determine the mail provider for a do-
main. As such, we have four potential approaches: (1) the MX-only
approach [36], (2) a cert-based approach that combines TLS certifi-
cates and MX records, (3) a banner-based approach that combines
Banner/EHLO messages and MX records, (4) the priority-based ap-
proach that combines TLS certificates, Banner/EHLO messages and
MX records.
We evaluate the four approaches and their relative accuracy
using 200 random domains sampled from three sets of domains in
two ways, resulting in an evaluation set of 1,200 domains. The three
sets of domains we randomly sample are: all .gov domains, a stable
set of domains from the Alexa list, and a stable set of 1 million
.com domains (see Section 4.1 for how we define stable domains).
We sample (a) 200 domains and (b) 200 domains with unique MX
records from the three datasets.
Since there is no ground truth for mail providers, we use domains
with SMTP servers, scan the relevant information ourselves, and
manually label their providers.4 We then use this labeled data to
compare the results of the different methods.
Figure 4 shows the results. The dark green part of the priority-
based approach highlights the total number of candidates manually
4Note that we select 200 domains with SMTP servers to ensure a fair comparison
across different methods. Some methods (e.g., the MX-only approach) are oblivious
to SMTP server presence, and their accuracy drops considerably if domains with MX
records but without SMTP servers are in the sample.
128
examined in step 4 (check for misidentifications) of our approach.
In general, the priority-based approach works the best among all
four approaches for the two sets of domains, with an accuracy of at
least 97%. In total, it missed 21 domains (1.8%) out of 1200 domains
sampled and required us to manually examine 20 (1.7%) domains.
Among 21 domains it missed, we cannot decide the providers of
4 domains. Three of these four domains are hosted on servers with
unpopular web hosting companies. We do not have enough infor-
mation and confidence to decide if the servers are VPS instances
rented from the web hosting companies or directly managed by
them. One presents a valid certificate of company A, but indicates
that it is company B in Banner/EHLO messages (a situation much
like utexas.edu described above). However, unlike utexas.edu
which is hosted with a well-known provider, both company A and
B are relatively unpopular and we are not confident enough to de-
cide whether company A or B is running the mail server. Out of 17
domains for which we decide the provider, 11 are VPS servers that
use subdomains of the web hosting companies in their certificates
or Banner/EHLO messages (like the GoDaddy example mentioned
above),5 4 are poorly configured servers with Banner/EHLO mes-
sages containing strings like localhost operated by web hosting
companies, and 2 are poorly configured local servers that supply
FQDNs that are misleading in their Banner/EHLO messages. For the
20 domains that require manual examination, our heuristic, which
we publish together with our code, can automatically determine if
they need to be corrected. The amount of labor required in the step
is small.
The MX-only approach, on the other hand, relies upon just one
data source, and consequently performs the worst among all four
approaches (notably with an accuracy of only 40% for 200 random
.com domains with unique MX records). We also observe that its
performance is significantly better on Alexa and .gov domains than
.com domains. We suspect two factors contribute to this phenom-
enon. On the one hand, if a domain (e.g., foo.com) is hosted with
a web hosting company, often its MX record will be configured as
mx.foo.com (a default configuration employed by many web hosting
companies), leading the MX approach to believe that the domain
runs its own mail infrastructure. On the other hand, stable Alexa
and .gov domains are generally well-configured and more likely to
5Recall that we only check for misidentifications for large providers.
200 Alexa200 Alexa w/Unique MX200 .com200 .com w/Unique MX200 .gov200 .gov w/Unique MX200 Domains w/ SMTP Servers Sampled from Target Domains050100150200# of domains inferred correctly18618015879194194196192190171198197196194197190199199196194197194199199MX-onlycert-basedbanner-basedpriority-basedExamined in step 4IMC ’21, November 2–4, 2021, Virtual Event, USA
Liu, Akiwate, Jonker, Mirian, Savage, and Voelker
name their mail providers in the MX records, in which cases the
MX approach works well.
Considering information from certificates and Banner/EHLO
messages increases accuracy by at least a few percent. Note that
the banner-based approach performs better than the cert-based
approach. This is because, as mentioned in Section 3.1, while more
reliable, certificates information is less often available than Ban-
ner/EHLO messages. Finally, we note that the banner-based ap-
proach achieves an accuracy that is close to the priority-based
approach in most cases. These results suggest that the banner-
based approach is a good fallback in cases where certificates are
not available.
Overall, the priority-based approach performs the best among
these four approaches, identifying at least 5 and at most 115 more
domains than the MX approach on the 200 sampled domains.
3.4 Limitations
The priority-based approach does have several limitations. First, the
flow of exchanging e-mail could involve multiple hops, and we only
observe the first step of delivery using DNS MX records. As a result,
our inference result may not always reflect the eventual e-mail
provider used by users of a domain. Certain heuristics, such as SPF
records, might help discover the eventual e-mail provider. However,
this is not the focus our work and we leave this as future work.
Second, the MX records of a domain could point to any arbitrary
server, and there is no guarantee that the server is actually the one
responsible for handling the domain’s incoming mail. However, this
is a limitation that all approaches share. Furthermore, we develop
a generic inference method based on IPv4 addresses. We imagine
future work extending this method to incorporate IPv6 addresses
and better handle corner cases in an automatic way (e.g., with
machine learning techniques). Finally, the priority-based approach
relies on both DNS data and active measurement data. To carry
out the longitudinal analysis in Section 5, we rely on scanning
information made available by third-party services like OpenINTEL
and Censys. As such, our results can have blind spots (e.g., Censys
may not scan IP addresses if certain providers choose to opt out of
scans or if it has a bug).
4 LARGE-SCALE IDENTIFICATION OF MAIL
PROVIDERS
We now apply the priority-based approach to three lists of target
domains collected from OpenINTEL [38] and Censys [12]. For each
list we consider nine separate days of data (except for the .gov
domains, for which we only had seven snapshots), equally spaced
over a four-year period between June 2017 and June 2021.
4.1 Target Domains
The first set of domains consists of the Alexa Top 1M domains [3]
that have an MX record in their DNS zone. To capture long-term
dynamics in mail provider use, we only consider stable domains
that consistently appear on the Alexa Top lists across the four years
of our study. Considering only the domains that are stable across
the years also eliminates noise from the churn [31] in the Alexa
Top 1M rankings.
129
Since the Alexa domains are by definition popular domains, for
comparison we also use a set of stable, random .com domains as a
second list. As with the Alexa domains, we consider .com domains
with MX records that are registered across the four years. We start
by randomly choosing 1M .com domains on June 8, 2017 (the first
day we consider) and then filter out domains that expire before
June 8, 2021 (the last day we consider) or do not have MX records.
We remove Alexa domains that also appear in this dataset to create
a disjoint view.
The last dataset consists of all .gov domains that have an MX
record in their DNS zone. Since OpenINTEL does not have coverage
of all .gov domains in 2017, our measurement data of .gov domains
starts in June 2018 and consists of seven snapshots instead of nine.
Similar to the .com domains, we remove Alexa domains that also
appear in this dataset to create a disjoint view.
Overall, the Alexa set contains 93,538 domains, the .com set
contains 580,537, and the .gov set contains 3,496 domains. The three
sets of domains provide insight into the changing mail provider
landscape for popular domains, random domains sampled from the
full distribution of registrants in .com, and domains in a restricted
TLD.6
4.2 External Data Sources
To enable our longitudinal and large-scale identification of mail
providers, we use two external data sources: OpenINTEL [38] and
Censys [7, 12].
4.2.1 OpenINTEL: Active DNS Measurement Data. OpenINTEL is
a DNS measurement platform that collects snapshots of a large
part of the DNS on a daily basis. It does so by structurally querying
substantial lists of domain names for sets of Resource Records (RRs).
These lists include, for example, all registered domain names under
specific zones such as .com. Other sources of names, such as the
Alexa Top 1M, are also targeted for measurement. The resulting
data accounts for MX records as well as for IP addresses (i.e., A
records) associated with the names found inside MX records. By
using OpenINTEL data, which allows us to look years into the
past, we can investigate MX configuration at scale and perform a
longitudinal analysis.
4.2.2 Censys: Internet Scanning Data. Censys is a service that per-
forms regular Internet-wide scans on a wide range of ports in the
IPv4 address space, and publishes the data collected. For example,
Censys regularly scans IP addresses on port 25 and, if hosts re-
spond, collects application-layer information. For our study, we
use the port 25 scans that capture the banner and EHLO messages,
as well as any certificates discovered from the SMTP or START-
TLS handshake. It is worth noting that, though Censys performs
Internet-wide scans, it may not have data for all IP addresses: the IP
address may not publicly accessible, the IP address may be blocked
due to requests from the address owner, the host may not listen (or
have open) the specific port on the day the scan was performed,
or the Censys scan may have failed to cover certain IP addresses
intermittently. These issues may skew results for methods that rely
upon certificates and Banner/EHLO messages. We also note that
6Note that we randomly sampled 400 domains each from these three lists to evaluate
our methodology in Section 3.3.
Who’s Got Your Mail? Characterizing Mail Service Provider Usage
IMC ’21, November 2–4, 2021, Virtual Event, USA
Alexa
COM
GOV
Category
Domains
1,692
3,215
8,419
19,920
2,074
58,218
93,538
Domains
49
No MX IP
160
No Censys
200
No Port 25 Data
665
No Valid SSL Cert.
342
No Valid Banner/EHLO
2,080
No Missing Data
3,496
Total
Table 4: Breakdown of data from the June 2021 snapshot
of the Alexa domains and random .com domains. These
domains have MX records and exist across nine snapshots
spanning four years.
Domains
23,040
17,842
63,042
279,002
9,992
187,619
580,537
Censys recently rolled out an upgraded scanning system, which
reportedly fixed some bugs and should have better coverage [8].
However, for consistency reasons, all of our data is taken from the
previous system.
4.3 Data Gathering
We start with the target list of domain names (e.g., stable domains
in Alexa top 1M list) as well as one or more dates for which to
gather data. We then extract from OpenINTEL the relevant DNS
records for domains in the target list on the selected dates. The
extracted data includes the MX records associated with the target
domains, as well as the IP addresses to which the names in those
MX records resolved. We use CAIDA’s IPv4 prefix-to-AS data [6]
to augment the IP addresses with routing information such as AS
number. For each IP address obtained from OpenINTEL, we query
Censys for the associated scanning information related to port 25.
This data includes the state of the port and data from SMTP and
STARTTLS handshakes, including Banner/EHLO messages and