Model
Dodecahedron [35]
4×4 lattice (i) (ii) [26], [22]
6×6 lattice (iii) (iv) [26], [22]
HCSE × 4 (i)
HCSE × 4 (ii)
HCSE × 4 (iii)
Campus network (i) [4]
Campus network (ii)
Campus network (iii)
P & PΘ
Bernoulli
Categorical
Categorical
ExpUnif
ExpUnif
ExpUnif
Bernoulli
Bernoulli
Bernoulli
m
30
24
60
52
52
52
62
138
138
n
20
16
36
-
-
-
11
15
15
|π|
3
6
15
7
7
7
8
8
8
|Y|
80
500
500
500
500
500
179
400
326
ta (s)
65.3
60.3
257.6
65.4
17.3
8.4
79.5
308.8
495.0
to (s)
0.2
0.3
1.0
7.9
7.0
5.5
1.7
9.0
15.2
ts (s)
0.5
22
47
0.5
0.5
0.5
0.5
0.85
0.85
M
104
5×104
5×104
104
104
104
104
104
104
TABLE I: Summary of simulation experiments where m is the problem dimension (i.e., the number of random variables in
the model input), n is the number of graph vertices (if applicable), and |π| is the number of EGs.
Setting
(i)
(ii)
(iii)
Pop. mean
2.063 × 10−6
2.007 × 10−9
2.002 × 10−12
REMWM
0.098
0.072
0.061
RECMC
0.606
>10
>10
REGS
0.062
0.077
0.089
TABLE II: Performance of rare event estimators in Sec. VII-A.
The last two columns show the RE of the crude Monte Carlo
(CMC) method and the generalized splitting (GS) method in
[35], Table 2, adjusted for sample size M = 104. The best
results are highlighted in bold.
−4. The results in Table II show that
and (iii)  = 10
the MWM estimators achieve less than 10% RE nder all
settings. In particular, MWM performs slightly better than
the generalized splitting (GS) method in [35] under setting
(ii) and (iii) but underperforms under setting (i). This result
seems to suggest that MWM should only be used for events
of extremely low probabilities. However, the RE of MWM
in setting (i) can be reduced to GS level by increasing the
sample size by 2.5 times, at a mere cost of 1.15% increase in
the experiment time, i.e., from 65.3 + 0.2 + 0.5 = 66 seconds
to 65.3+0.2+2.5×0.5 = 66.75 seconds (further discussion on
the comparison between the results from different techniques is
given in Sec. III-B). Despite using the same experiment times,
crude MC cannot efﬁciently handle the last two settings. This
result is expected due to the difﬁculty of obtaining rare event
samples of probabilities 10
−9 to 10
−12.
Interestingly, the RE of MWM decreases as the rare event
probability decreases. This phenomenon is known as vanishing
RE [2]. It is the result of the s, t-unreliability being more
dependent on a smaller set of links as  goes to zero. In
this particular example, as the links become more reliable,
the s, t-unreliability becomes more dependent on the failure
of links 1, 2, and 3 (incident to s) or the failure of links 28,
29, and 30 (incident to t) than on the failure of other set of
links. This is because each of these two failure events happen
with probability of O(3), while any other s-t cut requires the
failure of at least four links, which happens with probability of
O(4), O(5), or of a higher order. In terms of the probability
of occurrence, these two failure events will dominate other
events in Xγ as  goes to 0.
B. Maximum ﬂow analysis
In the second example, we evaluate the maximum ﬂow
probability of the 4×4 (Fig. 1b) and 6×6 directed lattice
networks. These problems were previously studied in [26]
and [22]. In both networks, s and t are the nodes at the
opposing corners. Each link Ei has a random capacity in
Set.
(i)
(ii)
(iii)
(iv)
Pop. mean
2.988×10−8
2.985×10−9
2.989×10−8
2.984×10−9
REMWM RECMC RES
0.011
0.013
0.011
0.012
>10
>10
>10
>10
REGS
0.0330
0.0433
REPMC
0.0369
0.0253
0.0269
0.0374
>0.025 >0.035 >0.02
>0.025 >0.035 >0.02
TABLE III: Performance of rare event estimators in Sec.
VII-B. The last three columns show the RE of the splitting
on the multilevel creation process (S), generalized splitting
(GS), and permutation Monte Carlo (PMC) method in [26],
Fig. 2 and [22], Table VII and Fig. 14.
Setting
(i)
(ii)
(iii)
Population mean
1.566 × 10−10
1.081 × 10−5
3.312 × 10−10
REMWM
0.658
0.288
1.020
RECMC
>10
0.157
>10
REZVA
0.421
n/c
n/c
TABLE IV: Performance of rare event estimators in Sec.
VII-C. The last column shows the RE of the zero variance
approximation (ZVA) method in [4] (n/c: not computable).
{0, 1, 2, . . . , Ki} that follows a categorical distribution
if 0 ≤ k ≤ Ki − 1,
if k = K
(cid:7)
1 −(cid:3)Ki−1
pKi−k−1
P(Xi = k) =
k=0 P(Xi = k)
−7, (ii) 4×4 lattice,  = 10
−7, and (iv) 6×6 lattice,  = 10
where p, Ki, and  are model parameters given in [26].
We are interested in computing the network unreliability (2),
which is the probability that the network does not satisfy a
minimum ﬂow demand of γ = 10. For evaluation, we use
a sample size of M = 5×104 and four experiment settings
(i) 4×4 lattice,  = 10
−8, (iii)
6×6 lattice,  = 10
−8. The
parameter  controls the rare event probability, i.e., the network
becomes more reliable as  goes to zero. The results in Table
III show that the MWM estimators consistently achieve low
REs of less than 1.3% under all settings. Moreover, using
the same sample size, the RE of MWM is lower than those
obtained from previous studies, including the splitting on the
multilevel creation process (S), generalized splitting (GS), and
permutation Monte Carlo (PMC) method in [26], [22].
C. Attack graph analysis
In the third example, we evaluate the loss tail probability
in (3), Sec II-A using the campus network example in [4]
(originally from [38], see Fig. 2). This computer network
consists of the attacker’s laptop, two web servers (WS), k user
computers, two application servers, and one database server.
Other information needed to build the network reliability
model is available in [4]. We are interested in computing
the loss tail probability (3), which is the probability that the
network suffers a loss greater than a given threshold T . For
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
313
Setting
(i)
(ii)
(iii)
Population mean
1.607 × 10−4
5.572 × 10−7
7.260 × 10−10
REMWM
0.015
0.022
0.042
RECMC
0.065
1.902
>10
TABLE V: Performance of rare event estimators in Sec. VII-D.
evaluation, we use sample size M = 104 and three settings
(i) k = 5, T = 0.99×L(V ), (ii) k = 9, T = 0.50×L(V ),
and (iii) k = 9, T = 0.90×L(V ), where L(V ) is the
maximum loss associated with the event of the entire net-
work being compromised. The results are given in Table IV.
Under setting (i), the MWM estimator produces a RE that
is about 1.6 times higher than the RE achieved by the zero-
variance approximation (ZVA) IS estimator in [4]. However,
the difference can be leveled up by increasing the sample size
of MWM by 2.5 times, at a mere cost of 0.92% increase
in the experiment time, i.e., from 79.5 + 1.7 + 0.5 = 81.7
seconds to 79.5 + 1.7 + 2.5 × 0.5 = 82.45 seconds. In
contrast, despite having a proven bounded RE property, the
ZVA estimator in [4] took over 25 minutes to compute. The
high computation cost is due to the NP-hardness of the minset-
maxprob problem. This becomes clear in setting (ii) and
(iii), where the dimension of the problem renders minset-
maxprob computationally intractable. In contrast, MWM is
able to estimate the loss tail probability under all settings.
Although the RE in setting (iii) is large, owing to the one-
time computation effort (i.e., ta and to) and a fast simulation
time (i.e., ts), the RE can be further reduced by increasing the
sample size M. For example, increasing the sample size from
M = 104 to M = 106 reduces the RE from 1.020 to 0.102
but only incurs a 16.5% increase in the total experiment time,
i.e., from 511.05 seconds to 495.0+15.2+100×0.85 = 595.2
seconds.
D. Dynamic fault tree analysis
In this example, we use MWM to perform rare event
simulation of dynamic fault trees (DFTs). Our case study
is the hypothetical computer system example (HCSE) [1],
which consists of two CPUs with a cold spare, ﬁve memory
units that share two memory interfaces, a bus system with
a redundant replica, and a hardware/software interface. For
simplicity, we ignore the coverage probabilities and the failure
of the software and the system operator. The original DFT
of the HCSE consists of m = 13 basic events, each has an
exponentially distributed TTF. To experiment with a larger
system, we replicate the HCSE four times and require k out of
four to be functional for the overall system to be considered
functional. Let f denote the system’s TTF, we want to estimate
the top-event probability ρ = P(f (X) ≤ T ) given a mission
time of T = 100. For evaluation, we use sample size M = 104
and three settings (i) k = 3, (ii) k = 2, (iii) and k = 1. The
results in Table V show that the MWM estimators produce low
RE estimates under all settings. While the accuracy of MWM
slightly degrades as the top-event probability decreases, overall
we still achieve less than 5% of RE.
E. Beneﬁt of the structure-preserving heuristic
The structure-preserving heuristic reduces the cardinality of
Y by allowing us to purge redundant samples, i.e., samples
that share the same weight regardless of θ and therefore do
not help improve the representativeness of Y. To understand
the computational saving offered by the structure-preserving
(a)