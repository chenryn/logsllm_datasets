calling conventions.
We make a special consideration for reﬂective calls within our
analysis. In Android programs, reﬂection is realized by calling the
method java.lang.reflect.Method.invoke(). The “this” ref-
erence of this API call is a Method object, which is usually ob-
tained by invoking either getMethod() or getDeclaredMethod()
from java.lang.Class. The class is often acquired in a reﬂective
manner too, through Class.forName(). This API call resolves a
string input and retrieves the associated Class object.
We consider any reﬂective invoke() call as a sink and conduct
backward dataﬂow analysis to ﬁnd any prior data dependencies. If
such an analysis reaches string constants, we are able to statically
resolve the class and method information. Otherwise, the reﬂective
call is not statically resolvable. However, statically unresolvable
behavior is still represented within the WC-ADG as nodes which
contain no constant parameters.
Instead, this reﬂective call may
have several preceding APIs, from a dataﬂow perspective, which
are the sources of its metadata.
4. ANDROID MALWARE CLASSIFICATION
We generate WC-ADGs for both benign and malicious apps. Each
unique graph is associated with a feature that we use to classify
Android malware and benign applications.
4.1 Graph Matching Score
, where V and V (cid:48) are respectively the vertices of two graphs, vI and
vD are individual vertices inserted to and deleted from G, while EI
and ED are the edges added to and removed from G.
WGED presents the absolute difference between two graphs.
This implies that wged(G,G’) is roughly proportional to the sum
of graph sizes and therefore two larger graphs are likely to be more
distant to one another. To eliminate this bias, we normalize the re-
sulting distance and further deﬁne Weighted Graph Similarity based
upon it.
Deﬁnition 3. The Weighted Graph Similarity of two Weighted
Contextual API Dependency Graphs G and G’, with a weight func-
tion β, is,
wgs(G, G
(cid:48)
, β) = 1 −
wged(G, G(cid:48), β)
wged(G,∅, β) + wged(∅, G(cid:48), β)
(2)
, where ∅ is an empty graph. wged(G,∅, β) + wged(∅, G(cid:48), β) then
equates the maximum possible edit cost to transform G to G’.
4.2 Weight Assignment
Instead of manually specifying the weights on different APIs
(in combination of their attributes), we wish to see a near-optimal
weight assignment.
Selection of Critical API Labels.
Given a large number of API labels (unique combinations of
API names and attributes), it is unrealistic to automatically as-
sign weights for every one of them. Our goal is malware clas-
siﬁcation, so we concentrate on assigning weights to labels for
the security-sensitive APIs and critical combinations of their at-
tributes. To this end, we perform concept learning to discover
critical API labels. Given a positive example set (PES) contain-
ing malware graphs and a negative example set (NES) containing
benign graphs, we seek a critical API label (CA) based on two re-
quirements: 1) frequency(CA,PES) > frequency(CA,NES) and 2)
frequency(CA,NES) is less than the median frequency of all critical
API labels in NES. The ﬁrst requirement guarantees that a critical
API label is more sensitive to a malware sample than a benign one,
while the second requirement ensures the infrequent presence of
such an API label in the benign set. Consequently, we have se-
lected 108 critical API labels. Our goal becomes the assignment
of appropriate weights to these 108 labels while assigning a default
weight of 1 to all remaining API labels.
To quantify the similarity of two graphs, we ﬁrst compute a graph
edit distance. To our knowledge, all existing graph edit distance al-
gorithms treat node and edge uniformly. However, in our case, our
graph edit distance calculation must take into account the differ-
ent weights of different API nodes. At present, we do not con-
sider assigning different weights on edges because this would lead
to prohibitively high complexity in graph matching. Moreover, to
emphasize the differences between two nodes in different labels,
we do not seek to relabel them. Instead, we delete the old node and
insert the new one subsequently. This is because node “relabeling”
cost, in our context, is not the string edit distance between the API
labels of two nodes. It is the cost of deleting the old node plus that
of adding the new node.
Deﬁnition 2. The Weighted Graph Edit Distance (WGED) of
two Weighted Contextual API Dependency Graphs G and G’, with
a uniform weight function β, is the minimum cost to transform G
to G’:
wged(G, G
(cid:48)
, β) = min(
β(vI ) +
β(vD) + |EI| + |ED|)
(cid:88)
(cid:88)
vI∈{V (cid:48)−V }
vD∈{V −V (cid:48)}
Weight Assignment.
Intuitively, if two graphs come from the same malware family
and share one or more critical API labels, we must maximize the
similarity between the two. We call such a pair of graphs a “homo-
geneous pair”. Conversely, if one graph is malicious and the other
is benign, even if they share one or more critical API labels, we
must minimize the similarity between the two. We call such a pair
of graphs a “heterogeneous pair”. Therefore, we cast the problem
of weight assignment to be an optimization problem.
Deﬁnition 4. The Weight Assignment is an optimization problem
to maximize the result of an objective function for a given set of
graph pairs {}:
max f ({}, β) =
(cid:88)
wgs(G, G
(cid:48)
(cid:48)
 is a
 is a
homogeneous pair
heterogeneous pair
s.t.
1 ≤ β(v) ≤ θ, if v is a critical node;
β(v) = 1, otherwise.
wgs(G, G
, β)
(1)
(3)
Figure 6: A Feedback Loop to Solve the Optimization Problem
, where β is the weight function that requires optimization; θ is the
upper bound of a weight. Empirically, we set θ to be 20.
To achieve the optimization of Equation 3, we use the Hill Climb-
ing algorithm [30] to implement a feedback loop that gradually im-
proves the quality of weight assignment. Figure 6 presents such
a system, which takes two sets of graph pairs and an initial weight
function β as inputs. β is a discrete function which is represented as
a weight vector. At each iteration, Hill Climbing adjusts a single el-
ement in the weight vector and determines whether the change im-
proves the value of objective function f({},β). Any change
that improves f({},β) is accepted, and the process contin-
ues until no change can be found that further improves the value.
4.3 Implementation
To compute the weighted graph similarity, we use a bipartite
graph matching tool [29]. We cannot directly use this graph match-
ing tool because it does not support assigning different weights
on different nodes in a graph. To work around this limitation, we
enhanced the bipartite algorithm to support weights on individual
nodes.
4.4 Graph Database Query
Given an app, we match its WC-ADGs against all existing graphs
in the database. The number of graphs in the database can be fairly
large, so the design of the graph query must be scalable.
Intuitively, we could insert graphs into individual buckets, with
each bucket labeled according to the presence of critical APIs. In-
stead of comparing a new graph against every existing graph in the
database, we limit the comparison to only the graphs within a par-
ticular bucket that possesses graphs containing a corresponding set
of critical APIs. Critical APIs generally have higher weights than
regular APIs, so graphs in other buckets will not be very similar to
the input graph and are safe to ignore. However, API-based bucket
indexing may be overly strict because APIs from the same package
usually share similar functionality. For instance, both getDeviceId()
and getSubscriberId() are located in TelephonyManager pack-
age, and both retrieve identity-related information. Therefore, we
instead index buckets based on the package names of critical APIs.
More speciﬁcally, to build a graph database, we must ﬁrst build
an API package bitvector for all existing graphs within the database.
Such a bitvector has n elements, each of which indicates the pres-
ence of a particular Android API package. For example, a graph
that calls sendTextMessage() and getDeviceId() will set the
corresponding bits for the android.telephony.SmsManager and
android.telephony.TelephonyManager packages. Graphs that
share the same bitvector (i.e., the same API package combination)
are then placed into the same bucket. When querying a new graph
against the database, we encode its API package combination into
a bitvector and compare that bitvector against each database index.
Notice that, to ensure the scalability, we implement the bucket-
based indexing with a hash map where the key is the API package
bitvector and the value is a corresponding graph set.
Empirically, we found this one-level indexing efﬁcient enough
for our problem. If the database grows much larger, we can tran-
Figure 7: Bucket-based Indexing of Graph Database
sition to a hierarchical database structure, such as vantage point
tree [20], under each bucket.
Figure 7 demonstrates the bucket query for the WC-ADG of Zitmo
shown in Figure 3. This graph contains six API calls, three of which
belong to a critical package: android.telephony.SmsManager.
The generated bitvector for the Zitmo graph indicates the presence
of this API package, and an exact match for the bitvector is per-
formed against the bucket index. Notice that the presence of a sin-
gle critical package is different from that of a combination of mul-
tiple critical packages. Thus, the result bucket used by this search
contains graphs that include android.telephony.SmsManager
as the only critical package in use. SmsManager, being a criti-
cal package, helps capture the SMS retrieval behavior and narrow
down the search range. Because HTTP-related API packages are
not considered as critical, such an exact match over index will not
exclude Zitmo variants that use other I/O packages, such as SMS.
4.5 Malware Classiﬁcation
Anomaly Detection.
We have implemented a detector to conduct anomaly detection.
Given an app, the detector provides a binary result that indicates
whether the app is abnormal or not. To achieve this goal, we build
a graph database for benign apps. The detector then attempts to
match the WC-ADGs of the given app against the ones in database.
If a sufﬁciently similar one for any of the behavior graphs is not
found, an anomaly is reported by the detector. We have set the
similarity threshold to be 70% per our empirical studies.
Signature Detection.
We next use a classiﬁer to perform signature detection. Our sig-
nature detector is a multi-label classiﬁer designed to identify the
malware families of unknown malware instances.
To enable classiﬁcation, we ﬁrst build a malware graph database.
To this end, we conduct static analysis on the malware samples
from the Android Malware Genome Project [1,36] to extract WC-ADGs.
In order to consider only the unique graphs, we remove any graphs
that have a high level of similarity to existing ones. With experi-
mental study, we consider a high similarity to be greater than 80%.
Further, to guarantee the distinctiveness of malware behaviors, we
compare these malware graphs against our benign graph set and
remove the common ones.
Next, given an app, we generate its feature vector for classiﬁ-
cation purpose. In such a vector, each element is associated with
a graph in our database. And, in turn, all the existing graphs are
projected to a feature vector. In other words, there exists a one-to-
one correspondence between the elements in a feature vector and
f({},β)βHomogeneous Graph PairsHill Climber+-Heterogeneous Graph Pairsandroid.telephony.TelephonyManagerandroid.telephony.SmsMessageandroid.os.Processjava.lang.Runtimeandroid.telephony.SmsMessagejavax.crypto.Cipherandroid.location.LocationManagerjava.net.Socket[0,  0,  0,  …,  0,  0,  0,  1][0,  0,  0,  …,  0,  0,  1,  0][0,  0,  0,  …,  0,  0,  1,  1][0,  0,  0,  …,  0,  1,  0,  0][0,  0,  0,  …,  0,  1,  0,  1]............[0,  0,  1,  …,  0,  0,  0,  0][0,  0,  1,  …,  0,  0,  0,  0]getOriginatingAddressgetMessageBodysetEntityexecutecreateFromPduFigure 8: An Example of Feature Vectors
(a) Graphs per Benign App.
(b) Graphs per Malware.
the existing graphs in the database. To construct the feature vec-
tor of the given app, we produce its WC-ADGs and then query the
graph database for all the generated graphs. For each query, a best
matching graph is found. The similarity score is then put into the
feature vector at the position corresponding to this best matching
graph. Speciﬁcally, the feature vector of a known malware sample
is attached with its family label so that the classiﬁer can understand
the discrepancy between different malware families.
Figure 8 gives an example of feature vectors. In our malware
graph database of 862 graphs, a feature vector of 862 elements is
constructed for each app. The two behavior graphs of ADRD are
most similar to graph G6 and G7, respectively, from the database.
The corresponding elements of the feature vector are set to the sim-
ilarity scores of theose features. The rest of the elements remain set
to zero.
Once we have produced the feature vectors for the training sam-
ples, we can next use them to train a classiﬁer. We select Naïve
Bayes algorithm for the classiﬁcation. In fact, we can choose dif-
ferent algorithms for the same purpose. However, since our graph-
based features are fairly strong, even Naïve Bayes can produce sat-
isfying results. Naïve Bayes also has several advantages:
it re-
quires only a small amount of training data; parameter adjustment
is straightforward and simple; and runtime performance is favor-
able.
5. EVALUATION
5.1 Dataset & Experiment Setup
We collected 2200 malware samples from the Android Malware
Genome Project [1] and McAfee, Inc, a leading antivirus company.
To build a benign dataset, we received a number of benign samples
from McAfee, and we downloaded a variety of popular apps hav-
ing a high ranking from Google Play. To further sanitize this benign
dataset, we sent these apps to the VirusTotal service for inspection.
The ﬁnal benign dataset consisted of 13500 samples. We performed
the behavior graph generation, graph database creation, graph sim-
ilarity query and feature vector extraction using this dataset. We
conducted the experiment on a test machine equipped with Intel(R)
Xeon(R) E5-2650 CPU (20M Cache, 2GHz) and 128GB of physi-
cal memory. The operating system is Ubuntu 12.04.3 (64bit).
5.2 Summary of Graph Generation
Figure 9 summarizes the characteristics of the behavior graphs
generated from both benign and malicious apps. Among them, Fig-
ure 9a and Figure 9b illustrate the number of graphs generated from
benign and malicious apps. On average, 7.8 graphs are computed
from each benign app, while 9.8 graphs are generated from each
malware instance. Most apps focus on limited functionalities and
do not produce a large number of behavior graphs. In 92% of be-
nign samples and 98% of malicious ones, no more than 20 graphs
are produced from an individual app.
Figure 9c and Figure 9d present the number of nodes of benign
and malicious behavior graphs. A benign graph, on average, has
15 nodes, while a malicious graph carries 16.4. Again, most of
the activities are not intensive, so the majority of these graphs have
a small number of nodes. Statistics show that 94% of the benign
graphs and 91% of the malicious ones carry less than 50 nodes.
(c) Nodes per Benign Graph.
(d) Nodes per Malware Graph.