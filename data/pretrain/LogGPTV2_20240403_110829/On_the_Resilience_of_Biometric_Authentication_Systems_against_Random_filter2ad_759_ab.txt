query. This might be desirable for the following reasons.
• Often the raw input is rather large. For instance, in case
of face recognition, without compression, an image will
need every pixel’s RGB information to be sent
to the
server for feature extraction and authentication. In the case
of an image of pixel size 60 × 60, this would require
approximately 10.8 KB of data. If the feature extraction
was ofﬂoaded to the user device, it would produce a 512
length feature embedding, which can take as little as 512
bytes. This also applies to continuous authentication which
inherently requires a continual stream of user raw inputs.
But often decisions are only made on an average of a set
of feature vectors [28], [22], [29]. In such systems, only
sending the resultant extracted average feature vector to the
cloud also reduces communication cost.
• Recent studies have shown that raw sensory inputs can
often be used to track users [30]. Thus, they convey more
information than what is simply required for authentication.
In this sense, extracting features at the client side serves
as an information minimization mechanism, only sending
the relevant information (extracted feature vectors) to the
server to minimize privacy loss.
• Since the machine learning algorithm only compares sam-
ples in the feature space, only the feature representation of
the template is stored in the system. In this case, it makes
sense to do feature extraction prior to querying the system.
From now onwards, when referring to a biometric API we
shall assume the feature vector based API as the default. We
shall explicitly state when the discourse changes to raw inputs.
Figure 1 illustrates the two APIs.
C. Threat Model and Assumptions
We consider an adversary who has access to the API to a
biometric system trained with the data of a target user whom
the adversary wishes to impersonate. More speciﬁcally, the
adversary wishes to ﬁnd an accepting sample, i.e., a feature
vector for which the system returns “accept.” In the case of
the raw input API, the adversary is assumed to be in search
for a raw input
that results in an accepting sample after
feature extraction. We assume that the adversary has the means
to bypass the end user interface, e.g., touchscreen or phone
5For continuous authentication systems, we assume that the decision is
returned after a ﬁxed number of one or more biometric samples.
In the online setting, a mis-conﬁgured API may provide the
adversary access to the authentication pipeline. In the local
setting, if the feature extractor is contained within a secure
environment, raw sensory information must be passed to this
protected feature extraction process. To achieve this an attacker
may construct their own samples through OS utilities. An ex-
ample is the Monkey Runner [31] on Android, a tool allowing
developers to run a sequence of predeﬁned inputs for product
development and testing. Additionally, prior work [32] has
shown the possibility of compromising the hardware contained
within a mobile device, e.g., a compromised digitizer can inject
additional touch events.
it
is difﬁcult
Outside of literature,
to know the exact
implementation of real-world systems. However, taking face
recognition as an example, we believe our system architecture
is similar to real world facial authentication schemes, drawing
parallels to pending patent US15/864,232 [33]. Additionally
there are API services dedicated to hosting different compo-
nents of the facial recognition pipeline. Clarifai, for example,
hosts machine learning models dedicated to the extraction of
face embeddings within an uploaded image [34]. A developer
could then use any number of Machine Learning as a Service
(MLaaS) providers to perform the ﬁnal authentication step,
without needing to pay premiums associated with an end-to-
end facial recognition product.
by the model, is public knowledge.
We make the following assumptions about the biometric API.
• The input feature length, i.e., the number of features used
• Each feature in the feature space is min-max normalized.
Thus, each feature takes value in the real interval [0, 1].
This is merely for convenience of analysis. Absent this,
the attacker can still assume plausible universal bounds for
all features in the feature space.
• The attacker knows the identiﬁer related to the user, e.g.,
the username, he/she wishes to impersonate.
Beyond this we do not assume the attacker to have any
knowledge of the underlying biometric system including the
biometric modality, the classiﬁer being used, the target user’s
past samples, or any other dataset which would allow the
attacker to infer population distribution of the feature space
of the given modality.
Fig. 1. The threat model and the two types of biometric API.
3
User InterfaceAPISecure Enclave / CloudThe ModelUser/OS SpaceFeature ExtractorAPISecure Enclave / CloudThe ModelFeaturesRaw InputSensorsUser InputUser InterfaceSensorsUser InputRaw InputRaw Input APIFeature Vector APIAttack SurfaceIII. ACCEPTANCE REGION AND PROPOSED ATTACK
A. Motivation and Attack Overview
Given a feature space, machine learning classiﬁers learn
the region where feature vectors are classiﬁed as positive
features and the region where vectors are classiﬁed as negative
features. We call the former, the acceptance region and the
latter the rejection region. Even though the acceptance region
is learnt through the data from the target user, it does not
necessarily tightly surround the region covered by the target
user’s samples. Leaving aside the fact that this is desirable
so as to not make the model “overﬁtted” to the training
data, this implies that even vectors that do not follow the
distribution of the target user’s samples, may be accepted. In
fact these vectors may bare no resemblance to any positive or
negative samples from the dataset. Consider a toy example,
where the feature space consists of only two vectors. The
two-dimensional plane in Figure 2 shows the distribution of
the positive and negative samples in the training and testing
datasets. A linear classiﬁer may learn the acceptance and
rejection regions split via the decision boundary shown in the
ﬁgure. This decision boundary divides the two dimensional
feature space in half. Even though there is a small overlap
between the positive and negative classes, when evaluated
against the negative and positive samples from the dataset there
would be an acceptably low false positive rate. However if
we construct a vector by uniformly sampling the two features
from the interval [0, 1], with probability 1/2 it will be an
accepting sample. If this model could be queried through an
API, an attacker is expected to ﬁnd an accepting sample in
two attempts. Arguably, such a system is insecure.
Fig. 2. Example feature space separation by a linear boundary between two
classes. This demonstrates low FPR and FRR of test sample classiﬁcation, yet
allows approximately 50% of the feature space to be accepted as positive.
Figure 2 illustrates that
the acceptance region can be
larger than the region covered by the target user’s samples.
However, in the same example, the area covered by the target
user’s samples is also quite high, e.g., the convex hull of the
samples. As we discuss next, in higher dimensions, the area
covered by the positive and negative examples is expected to be
concentrated in an exponentially small region [20]. However,
the acceptance region does not necessarily follow the same
trend.
B. Acceptance Region
Notations. Let I := [0, 1] denote the unit interval [0, 1], and
let In := [0, 1]n denote the n-dimensional unit cube with one
vertex at the origin. The unit cube represents the feature space
with each (min-max normalized) feature taking values in I.
Let f denote a model, i.e., an output of a machine learning
algorithm (classiﬁer) trained on a dataset D = {(xi, yi)}i∈[m],
where each xi is a feature vector and yi ∈ {+1,−1} its label.
The label +1 indicates the positive class (target user) and −1
4
the negative class (one or more other users of the authentication
system). We may denote a positive example in x ∈ D by
x+, and any negative example by x−. The model f takes
feature vectors x ∈ In as input and outputs a predicted label
ˆy ∈ {+1,−1}.
Deﬁnitions. Acceptance region of a model f is deﬁned as
Af := {x ∈ In : f (x) = +1},
(1)
The n-dimensional volume of Af is denoted Voln(Af ). The
deﬁnition of acceptance region is analogous to the notion of
decision regions in decision theory [35, §1.5]. We will often
misuse the word acceptance region to mean both the region
or the volume covered by the region where there is no fear
of ambiguity. Let FRR and FPR be evaluated on the training
dataset D.6 Let x ←$ In denote sampling a feature vector x
uniformly at random from In. In a random input attack, the
adversary samples x ←$ In and gives it as input to f. The
attack is successful if f (x) = +1. The success probability of
a random guess is deﬁned as
Pr[f (x) = +1 : x ←$ In].
(2)
Since the n-volume of the unit cube is 1, we immediately see
that the above probability is exactly Voln(Af ). Thus, we shall
use the volume of the acceptance region as a direct measure
of the success probability of random guess. Finally, we deﬁne
the rejection region as In − Af . It follows that the volume of
the rejection region is 1 − Voln(Af ).
Existence Results. Our ﬁrst observation is that even if the
FPR of a model is zero, its acceptance region can still be non-
zero. Note that this is not evident from the fact that there are
positive examples in the training dataset D: the dataset is ﬁnite
and there are inﬁnite number of vectors in In, and hence the
probability of picking these ﬁnite positive examples is zero.
Proposition 3.1: There exists a dataset D and a classiﬁer
with output f such that FRR = FPR = 0, and Voln(Af ) > 0.
Proof: Assume a dataset D that is linearly separable. This
means that there exists a hyperplane denoted H(x) such that
for any positive example x+ ∈ D, we have H(x+) > 0 and
for any negative example in D we have H(x−)  0, and −1
otherwise. Since the data is linearly separable, the perceptron
convergence theorem states that the perceptron learning algo-
rithm will ﬁnd a solution, i.e., a separating hyperplane [36].
Intersecting this hyperplane (cid:104)w, x(cid:105) + b = 0 with the unit cube
creates two sectors. The sector where fw,b(x) = +1 is exactly
the acceptance region Afw,b. The n-volume of Afw,b cannot be
zero, since otherwise it is one of the sides of the unit cube with
dimension less than n, implying that all points (cid:104)w, x(cid:105) + b > 0
lie outside the unit cube. A contradiction, since FRR = 0 (there
is at least one positive example).
A non-zero acceptance region is not necessarily a threat.
Of practical concern is a non-negligible volume. Indeed, the
volume may be negligible requiring a large number of queries
to f before an accepting sample is produced. The following
6In practice, the FRR and FPR are evaluated against a subset of D called
a holdout or testing dataset.
0.00.20.40.60.81.00.00.20.40.60.81.0Positive ClassNegative Classresult shows that there are cases in which the acceptance region
can be non-negligible.
Proposition 3.2: There exists a dataset D and a classiﬁer
with output f such that FRR = FPR = 0, and Voln(Af ) ≥
1/2.
1 > 0.5, and for all negative examples x−
Proof: Consider again the perceptron as an example of a
classiﬁer. Let D be a dataset such that for all positive examples
x+, we have x+
1  FPR, since arguably it is misleading to use the
FPR as a measure of security of such an authentication system.
This could happen even when the FPR is non-zero. When and
why would this case occur? We explain this in the following.
Real Data and High Dimensions. We ﬁrst discretize the
feature space. For a given positive integer B, let IB denote
the binned (or discrete) version of the interval I partitioned
into B equally sized bins. Clearly, each bin is of width 1/B.
Let In
B denote the discretized feature space. Given a set of
feature values from I, we say that a bin in IB is ﬁlled if there
are > n feature values falling in that bin, where n is a cutoff
to ﬁlter outliers. The number of ﬁlled bins is denoted by α.
Clearly α ≤ B. See Figure 3.
y
c
n
e
u
q
e
r