In this section, we use an illustrative example to motivate our approach. Table 1 shows an alert snippet of a large commercial bank, A, in which the alerts, ğ‘’1,ğ‘’2, Â· Â· Â· ,ğ‘’5, are all caused by the same failure, â€œNTP start errorâ€. By removing variable parameters and stop words from contents, these alerts can be classified into three types, ğ¸1, ğ¸2, and ğ¸3. Alerts of the same type have the same parsed content. In Table 1, ğ‘’1 and ğ‘’3 belong to ğ¸1, ğ‘’2 and ğ‘’4 belong to ğ¸2, and ğ‘’5 belongs to ğ¸3.Our study aims to automatically summarize such alerts into a group, named as incident, thereby reducing the number of alerts an-alyzed by maintenance engineers. To mine the correlation between alerts, in this paper, we leverage two types of alert information, semantic information and behavior information.
3.1 	Semantic InformationAs shown in Table 1, contents of the alert of ğ¸2 and ğ¸3 have some common key words, such as â€œNTPâ€ and â€œstartâ€, which reveals the common semantic information between alerts. However, mining such common semantic information is not trivial, because it is often that correlated alerts have only a few common words and most words in their contents are different. Therefore, popular approaches, like Jaccard [31] and Word2Vec [19], may fail to capture such faint common semantic information.To attack such problem, in this paper, we propose a deep learn-ing based model, named ASR (Alert Semantics Representation), to extract the semantic information of alerts. ASR not only mines the contextual information of each alert word, but also considers
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USAthe contribution of each word to the overall alert semantics. As a result, the contextual information of alert words is integrated based on their semantic contribution. In the previous example, ASR will pay more attention to the words, â€œNTP" and â€œstart", than the other words in the alert of ğ¸2 and ğ¸3.
3.2 	Behavior InformationCompared to correlating alerts of ğ¸2 and ğ¸3, correlating those of ğ¸1 is more challenging since there are no common words between ğ¸1 and ğ¸2 (or ğ¸3). Thus, we leverage the behavior information lurking in co-occurrences of alerts. The rationale is that correlated alerts should occur together in higher probability. For example, Figure 1 shows the occurrence series for alerts of ğ¸1 and ğ¸2 in one day, which are formed by counting alert occurrences of each type per one minute in a day. It can be seen that 1) alerts of ğ¸1 and ğ¸2 always occur in the same time periods, [ğ‘¡1,ğ‘¡2], [ğ‘¡3,ğ‘¡4], and [ğ‘¡5,ğ‘¡6], which correspond to three distinct failures; 2) the fluctuations of these two series are similar.5
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: A snippet of an alert sequence
| No. | ID | Timestamp | Content |
|---|---|---|---|
| ğ‘’1 |ğ¸1 |2018/5/1 18:00 2018/5/1 18:01 2018/5/1 18:01 2018/5/1 18:02 2018/5/1 18:03 |There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 1 time(s). There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 2 time(s). The NTP daemon server has not been start correctly. || ğ‘’2 |ğ¸2 |2018/5/1 18:00 2018/5/1 18:01 2018/5/1 18:01 2018/5/1 18:02 2018/5/1 18:03 |There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 1 time(s). There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 2 time(s). The NTP daemon server has not been start correctly. || ğ‘’3 |ğ¸1 |2018/5/1 18:00 2018/5/1 18:01 2018/5/1 18:01 2018/5/1 18:02 2018/5/1 18:03 |There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 1 time(s). There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 2 time(s). The NTP daemon server has not been start correctly. || ğ‘’4 |ğ¸2 |2018/5/1 18:00 2018/5/1 18:01 2018/5/1 18:01 2018/5/1 18:02 2018/5/1 18:03 |There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 1 time(s). There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 2 time(s). The NTP daemon server has not been start correctly. || ğ‘’5 |ğ¸3 |2018/5/1 18:00 2018/5/1 18:01 2018/5/1 18:01 2018/5/1 18:02 2018/5/1 18:03 |There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 1 time(s). There isnâ€™t effective configuration in /var/opt/conf/bank_name_check.conf, please retouch the file. Running NTP start-up check script failed, tried 2 time(s). The NTP daemon server has not been start correctly. |to finally determine the correlation between alerts. The naive ap-proach is to consider semantic information and behavior informa-tion separately. Given two alerts, computing the similarity between their semantic information or behavior information, as long as one similarity exceeds the threshold, these two alerts are considered as correlated. However, due to the complexity and variety of the alert mechanism, it is infeasible to manually set the appropriate threshold for alerts of all possible failures.We propose a neural network, called ACT (Alert CorrelaTion), to combine the two types of alert information from ASR and ABR, and determine the correlation between alerts. In the training stage, we generate a set of alert pairs, each of which has a label, correlated or uncorrelated. The ACT network automatically learns the optimal combination mechanism from the labelled data.
3.4 	Supervised vs. Unsupervised ApproachUnlike the state of arts [13, 31], we utilize supervised learning ap-proaches to summarize alerts. We make this choice due to following reasons. First, in many companies, the labelled data is easy to obtain. Failure reports are natural labelled data [31]. Each failure report includes related alerts, the failure information, and so on. Therefore, any two alerts belonging to one failure report can be considered as correlated. Second, incident recognition is a subjective task. It may happen that different experts have opposite opinions about the correlation between alerts, which is influenced by the maintenance system and mechanism. Thus, the supervised approach is more appealing to maintenance engineers.4 	BACKGROUND AND APPROACH 	OVERVIEW
In this section, we present the necessary background knowledge and give the approach overview.
4.1 	Alert PreprocessingGiven an alert sequence, we first preprocess alerts in four steps. In the first step, we remove variables in the alert content. There are numerous works on parsing alerts [1, 8, 29]. As Drain [8] is a popular online parser, we adopt it as the parser of OAS. In the second step, we further remove stop words from the alert content, since stop words, such as "the", "a", and "and", do not carry much specific semantic information. In the third step, according to alert contents, we group alerts into different types, and thus alerts of the same type have the same alert content. In the last step, for an alert, we form its occurrence series by counting the number of alerts of the same type per ğ›¼ minutes in the past ğ›½ minutes (ğ›¼ < ğ›½). As aresult, the alert content contains the semantic information of the alert, and the occurrence series contains the behavior information of the alert.
For convenience, we define the parsed alert sequence as ğ‘† = [ğ‘’1,ğ‘’2, Â· Â· Â· ,ğ‘’ğ‘›]. For an alert,ğ‘’ğ‘– (1 â‰¤ ğ‘– â‰¤ ğ‘›), the timestamp is denoted as ğ‘¡ğ‘–. We have a set of alert types, {ğ¸1, ğ¸2, Â· Â· Â· , ğ¸ğ‘š}, and each alert belong to one alert type. ğ‘Šğ‘– (|ğ‘Šğ‘– | = ğ‘™ğ‘–) records the words in the ğ›¼âŒ‰. content of ğ‘’ğ‘–. The occurrence series of ğ‘’ğ‘– is referred to as ğ¹ğ‘– âˆˆ RâŒˆ ğ›½4.2 	Overview
The objective of OAS is to utilize the semantic and behavior infor-mation of alerts to summarize alerts online. As a result, alerts are grouped into different incidents by OAS, and each incident contains alerts of the same system failure. In addition to reduce the number of alerts, compared to a single alert that only focuses on a local phenomenon of a failure, an incident can reflect the whole impact of the failure, thereby helping maintenance engineers efficiently locate and fix the failure.Figure 2 shows an overview of OAS. OAS contains four main components, alert semantics representation (ASR), alert behavior representation (ABR), alert correlation (ACT), and online summariz-ing. OAS has two stages, training stage and summarizing stage. In the training stage, OAS trains the alert representation models, ASR and ABR, and the alert correlation model, ACT, offline according to the history alert sequence. In the summarizing stage, based on the trained models, OAS summarizes the newly reported alert online by a time window.4.2.1 	Training Stage. As shown in the left part of Figure 2, alerts in training dataset are first parsed to get alert contents and alert occurrences series. For each ğ‘’ğ‘– in the training dataset, we obtain other alerts in window [ğ‘¡ğ‘– âˆ’ ğ‘¤,ğ‘¡ğ‘–] that belong to the same failure with ğ‘’ğ‘–. Suppose there exist ğ‘ğ‘– alerts correlated to ğ‘’ğ‘– during [ğ‘¡ğ‘– âˆ’	2, Â· Â· Â· ,ğ‘’ğ‘Ÿğ‘–ğ‘ğ‘–], (1 â‰¤ ğ‘Ÿğ‘– ğ‘—â‰¤ ğ‘›, ğ‘¤,ğ‘¡ğ‘–], which are denoted as ğ‘…ğ‘– = [ğ‘’ğ‘Ÿğ‘– 1,ğ‘’ğ‘Ÿğ‘– 1 â‰¤ ğ‘— â‰¤ ğ‘ğ‘–). Alerts correlated to ğ‘’ğ‘– can be collected from history failure reports. It should be noted that ğ‘…ğ‘– may not be a complete set that contains all the alerts correlated to ğ‘’ğ‘–. In fact, even an experienced domain expert can not guarantee that in each of his failure reports, all alerts belonging to the reported failure are found. Our approaches try to learn general relationships between available correlated alerts, and apply them to the newly reported alert in online alert stream.Then, we train the semantics representation model, ASR, and the behavior representation model, ABR. Since ASR and ABR have no dependency, they can be trained separately. Finally, we train the alert correlation model, ACT, to measure the correlation between
Online Summarizing Alerts through Semantic and Behavior Information
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
time wndowtime wndow
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
For each word in the alert, ğ‘’ğ‘– (1 â‰¤ ğ‘– â‰¤ ğ‘›), we get the contextual in-formation of the word by CBOW, denoted as ğ‘£ğ‘– ğ‘—(1 â‰¤ ğ‘— â‰¤ ğ‘™ğ‘–), and the semantic contribution of the word by IDF, denoted as ğ‘¢ğ‘– ğ‘—. It should be note that in order to ensureğ‘™ğ‘– according to Equation (1), we can aggregate the semantics of words in the alert content to obtain the complete semantic representation ğ‘—=1ğ‘¢ğ‘– ğ‘—= 1,ğ‘¢ğ‘– ğ‘—is normalized. At last,of the alert, denoted as ğ‘ ğ‘–.
ğ‘ ğ‘– =(ğ‘£ğ‘– ğ‘—Â· ğ‘¢ğ‘– ğ‘—) 	(1)
5.2 	Behavior Representation
In addition to semantics, alerts belonging to the same system failure usually also have common behavior information. Mining frequent patterns is a widely used approach to capture behavior correlations between alerts [10, 12, 28]. Because frequent patterns indicate co-occurrence correlations between alerts. In practice, however, some alert types have quite low frequencies to which frequent pattern mining approach is not applicable.To represent the alert behavior information, we propose ABR (Alert Behavior Representation), which for the first time learns the commonality between alert occurrence series. ABR is inspired by the word embedding model, Skip-Gram [19]. Skip-Gram captures the common semantics between a target word and its adjacent words via a shallow neural network, which converts the encod-ing of the target word to the encoding of its adjacent words in a linear fashion. Similarly, as shown in Figure 4, ABR trains a shal-low neural network to converts the occurrence series of a target alert to the occurrence series of its correlated alerts. As a result, the trained neural network can capture the underlying common behavior information between alerts.hidden layer