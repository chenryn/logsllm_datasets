要应对对应的超时、重试和其他灾难处理方法，这些都埋藏在具体的Paxos实现中（参
定仲裁数量的节点还存在就能够继续进行工作。但是，在实际实现中，具体的忽略逻辑
和一些猜测成分。例如：理论上来说，Paxos会忽略掉故障的计算节点，只要有满足法
从而避免造成引用一致性的问题，哪怕这样会造成性能和扩展性的问题。
一般来说，初级的云计算平台开发者会选择使用一个分布式一致性的存储API（例如
肯定是会不断下降的。
例如：
云计算开发者面临的挑战
·不同的数据存储之间的引用完整性。
，与其他服务接口的不断变化。
旧代码。
）数据结构的改变。
无停机时间的数据迁移。
第26章数据完整性：读写一致
---
## Page 353
大规模部署带外检测器可能成本较高。Gmail计算资源的很大一部分都被用来支持每日
高了工程师的士气，避免了加班，也提升了功能上线的可预知性。
成了“我们可以放心回家睡觉，周一再来修复根源问题”的情况。这样，数据校验器提
况。这个安全机制使得2013年发生的一次“天啊，所有文件都在消失”的意外情况变
工程师非常注重数据一致性，他们甚至将数据校验器增强为可以自动修复这种不一致情
那么某些文件可能缺少数据—也就是毁灭性的问题。负责开发Drive所用基础设施的
例如，Google Drive的周期性校验文件内容与Drive文件夹中的列表一致。如果不一致
那么就可能漏过一些用户可见的数据问题。为了在两者之间取得恰当的平衡，我们应该
会触发校验逻辑而失败。这样一来，工程师就会抛弃数据校验逻辑。如果规则不够严格，
带外数据校验比较难以正确实现。当校验规则太严格的时候，
检测到真实的数据完整性问题。Gmail开发者由于知道每个新引入的数据一致性问题都
举一个具体的例子：Gmail拥有一系列数据校验器，每一个校验器都曾经在生产环境中
然而，在数据校验方面投入的工程资源可以在更长时间内保障其他业务开发可以进行得
安排一些开发者来开发一套数据校验流水线可能会在短期内降低业务功能开发的速度。
Google针对上述每一种情况都构建了对应的校验器。
水线任务在服务达到某个扩展性节点的时候第一次被引人，随着架构的调整完全重写。
大部分情况下，数据校验流水线被实现成一系列MapReduce任务或者是Hadoop任
据丢失，我们需要一整套带外（out-of-band）检查和修复系统来处理数据存储内部和相
为了避免用户可见的数据质量下降，以及在无法恢复之前检测到低级的数据损坏以及数
带外数据校验
化的环境中，代码和数据结构的变化对老的备份数据的影响。
从不同恢复点的不同备份中恢复不同子集的数据。同时，我们还要考虑到在一个不断变
使这个问题变得更复杂的是，为了从低级数据损坏或者删除场景中恢复数据，我们必须
仅仅校验那些对用户来说具有毁灭性的数据问题。
实现。
及回归测试的文化使得Gmail开发者有足够的勇气每周多次修改Gmail生产存储系统的
可以在24小时之内被检测到而感到非常安心。这些数据校验器的存在，结合单元测试
引入单元测试效果类似，数据校验流水线可以在整个软件开发过程中起到加速作用。
更快。因为工程师们可以放心数据损坏的Bug没那么容易流入生产环境。和在项目早期
务。更常见的是，这些任务通常是在一个服务成功之后后续添加的。有些时候，这些流
互之间的数据问题。
GoogleSRE保障数据完整性的手段
，一个简单的合理的修改就
|311
358
---
## Page 354
312
责维护对应的业务逻辑，以便和他们不断发展的产品保持一致。
组提供一套数据校验框架。该基础设施组负责维护带外数据校验框架，而产品开发组负
很快就会被抛弃。因此，最好的办法是单独组织一个中央基础设施组为多个产品和工作
就算强迫他们进行，最后的结果也经常是不可靠、不全面的，以及浪费性、一次性的方案。
Gmail提供给on-call工程师的有：
的Google服务给on-call工程师提供了非常完备的文档和工具，用来定位问题。例如，
钟、儿小时或者几天之内消失。因此，快速定位校验日志中的问题是非常关键的。成熟
分析校验器为何失败也需要很多精力。造成某项间断性不一致情况的原因可能在几分
满足延迟和成本的要求。
行的校验器持续寻找毁灭性的问题，而其他更严格的校验器可以以更低频率运行，以便
随着一个系统的规模不断扩展，每天能够运行的校验器越来越少。我们可以保证每天运
而不再使用暴力方法校验。就像在数据恢复机制中那样，带外数据校验也可以分级进行。
校验器已经不能一天之内完成时，云存储工程师开发了一种更有效校验元数据的方法，
Google云存储（compute storage）是另外一个数据量显著影响数据校验的例子。当带外
每天只会运行一个分片。
围。虽然大部分Gmail检测器每天运行一次，但是压力最大的校验器被分为10~14个分片，
次重构工作中，我们降低了60%磁盘磁头的使用率，同时没有显著降低校验器覆盖的范
了一系列针对校验器的限速机制，同时定期重构这些校验器，以降低磁盘压力。在某一
命中率的下降，这就会造成用户可见响应速度的下降。为了避免这种问题，Gmail提供
数据检测的运行。使这个问题变得更严重的是，校验器本身可能会造成软件服务器缓存
大部分高速进行的小型开发团队都负担不起设计、构建和维护这样的系统所需的资源
一个有效的带外数据校验系统需要下列元素
·校验器容易使用的数据校验API。
·校验任务的管理。
监控、报警和监控台页面。
·数据校验监控台。
生产应急应对手册。
调试排错工具。
限速功能。
类似BigQuery的查询工具。
一系列Playbook文章讲述如何应对某种校验失败的报警。
第26章数据完整性：读写一致
---
## Page 355
败场景。如果我们没有在日常测试中主动寻找这些问题，而是在每一次进行数据恢复的
Google不光发现过上述列表中提到的每一个问题，还意外发现了很多其他没有提到的失
数据恢复计划中需要覆盖的点是很多的：
持续运行。
用有限。因此，在任何情况下，都应该追求完全自动化这些测试步骤，并且保证它们能
嫌的麻烦事。这样会导致测试过程要么不会被认真执行，要么执行得不够频繁以至于作
如果数据恢复测试是一个手工的、分阶段进行的操作，那么就不可避免地成为一个讨人
复操作之后，才能确定到底能否恢复最新数据。
能出问题。如果读者从本章仅仅学到一个知识点，那就是一定要记住：只有真正进行恢
人晚上放心睡觉。即使最近刚刚成功进行过一次数据恢复，下次执行时某些步骤仍然可
数据恢复流程中任何一个环节都有可能出问题，只有建立一个完善的端到端测试才能让
问题需要以下步骤：
们：我们可以再进行一次备份，准备更多的资源，或者修改SLO。那么，主动检测这些
如果能够在真正需要这些数据之前检测到数据损坏，我们就可以在灾难发生之前避免它
损坏的状态，在试图进行恢复之前我们并不知道。
同样的，成功进行一次数据恢复所需要的资料（通常是你的备份数据）可能也处于一个
障碍）。
你要面临一个漆黑的房间，可能还会扎到自己的脚趾（美国俚语，指自己给自己制造了
是——电灯泡可能早已经坏了，拨动开关时只是注意到了这个已经损坏的情况。而这时，
一个电灯泡什么时候会坏掉？是在拨动开关而灯却不亮的那一瞬间坏的吗？当然不
确保数据恢复策略可以正常工作
·持续针对数据恢复流程进行测试，把该测试作为正常运维操作的一部分。
的异地存储媒介？
恢复过程是否依赖于某些无法控制的元素一
是否在数据恢复过程中监控状态信息？
整个数据恢复过程能否在合理的时间内完成？
身，真正恢复数据，以及数据后处理任务？
我们是否有足够的物理机资源来完整地完成整个恢复过程：包括运行恢复任务本
备份数据是否是完整的、正确的一
当数据恢复流程无法完成时，自动发送警报。
GoogleSRE保障数据完整性的手段
一还是空的？
一例如某个不是24×7随时可以使用
|313
360
---
## Page 356
1
这是GTape，为Gmail量身定制的一个全球磁带备份系统历史上最大的一次应用一
用户数据，大量的用户数据。即使系统中存在很多安全防护措施，很多内部检查机制，
Gmail备份系统触发了一条报警信息，里面带有一个电话会议的号码。我们长时间以来
2011年2月27日，
同时失败造成的数据丢失，其次，这次数据恢复过程是Google数据安全最后一道防线一
有趣的场景。
案例分析
行测试来主动寻找这些弱点，才能够提前修复，关键时刻才不至于追悔莫及！
失败是不可避免的，不去主动寻找这些数据恢复失败的场景等于玩火自焚。只有通过进
时候才意外发现，那么Google很多非常受欢迎的产品可能就不会存在了。
314
[Sol11])，公众反响非常大。大家认为既然Google拥有大量的磁盘和快速网络，那么一
当Google最终宣布我们是通过之前未宣传过的磁带备份系统恢复的数据时（参见文献
急事件处理流程”中提到的最佳实践设计恢复计划，最终做到及时地恢复了用户的数据。
完成任务，是一件很有成就感的事。Google通过严格按照前文“深度多层防御”和“应
是坚持实行最佳实践，努力工作，以及协作才能做到的。看着每一个涉及的组件都成功
这里能够建模并且产生一个预期时间并不是靠运气，而是长时间持续进行规划的结果，
前的测试也相符。因此，我们可以：
复线上用户数据。还好，这种数据恢复以前曾经进行过测试，而且本次丢失的原因与之
以及大量的余，数据还是从Gmail系统中消失了。
最害怕的事情终于发生了—这其实也正是备份团队组建的原因一
离线磁带备份系统历史上使用量最大的一次。
作为我们学习的第一个数据恢复案例，它在很多方面都很独特：首先是数量众多的组件
Gmail-
据恢复系统，
正如我们预测的那样，真实生活给我们提供了无数个既不幸又不可避免的机会来检验数
·在全部过程预计完成时间之前应该可以恢复超过99%的数据。
·很快计算出恢复绝大部分受影响的用户数据的预计时间。
在该预计时间之后几个小时内恢复其他剩余全体用户。
第26章数据完整性：读写一致
，以及对应的流程的可靠程度。接下来我们会讨论两个非常有名，并且非常
-2011年2月：从GTape上恢复数据（磁带）
星期日，深夜
—Gmail系统丢失了
一恢
---
## Page 357
尽可能短的时间内删除海量音频数据。
私目的的数据删除流水线所删掉的。GoogleMusic的这个子系统的设计目标之一就是在
当发现数据完整性损坏的真正原因时，他却差点吓出心脏病：这段数据是被某个保护隐
深究问题，也引以为豪，于是他就继续在系统中查找可能存在的问题。
音频数据文件的指针，于是他就修复了这个歌曲的问题。但是，Google工程师经常喜欢
3月7日，负责调查此事的工程师发现无法播放的歌曲的元数据中缺少了一个针对具体
用户支持团队通知了工程师团队，这个问题被归类为流媒体播放问题进行调查。
发现问题
2012年3月6日，星期二，下午
而这一切还要发生在一个合理的时间范围内。
里存放5000盘磁带，以及如何能够迅速地（甚至是可行的）从离线媒介中读出数据一
这里讨论的第二个故障案例的特殊点在于海量数据存储所带来的后勤方面的挑战：在哪
Google Music
简单来说，虽然我们一直都知道这些最佳实践是很重要的，本次故障则实际证明了这一点。
我们不仅针对可预知的灾难建立起处理计划，同时也引入了一些随机无差异的失败场景
不可能做到的。Google强调这种灾难发生的必然性一甚至期盼它们尽快发生。因此，
计划妥善协调了在全球范围内分布式进行的费力工作一
多完全和Gmail无关的团队都提供了帮助。这次数据恢复过程通过建立一个完整的中央
Gmail数据恢复过程中受到最多表扬的部分是大量的团队协作以及平滑的沟通过程。很
本次丢失数据太多，该工具已经不能正常运行了。