title:Anomaly Detector Performance Evaluation Using a Parameterized Environment
author:Jeffery P. Hansen and
Kymie M. C. Tan and
Roy A. Maxion
Anomaly Detector Performance Evaluation
Using a Parameterized Environment
Jeﬀery P. Hansen, Kymie M.C. Tan, and Roy A. Maxion
Carnegie Mellon University, Pittsburgh, Pennsylvania / USA
Abstract. Over the years, intrusion detection has matured into a ﬁeld
replete with anomaly detectors of various types. These detectors are
tasked with detecting computer-based attacks, insider threats, worms
and more. Their abundance easily prompts the question - is anomaly de-
tection improving in eﬃcacy and reliability? Current evaluation strate-
gies may provide answers; however, they suﬀer from problems. For exam-
ple, they produce results that are only valid within the evaluation data
set and they provide very little by way of diagnostic information to tune
detector performance in a principled manner.
This paper studies the problem of acquiring reliable performance re-
sults for an anomaly detector. Aspects of a data environment that will
aﬀect detector performance, such as the frequency distribution of data
elements, are identiﬁed, characterized and used to construct a synthetic
data environment to assess a frequency-based anomaly detector. In a
series of experiments that systematically maps out the detector’s per-
formance, areas of detection weaknesses are exposed, and strengths are
identiﬁed. Finally, the extensibility of the lessons learned in the synthetic
environment are observed using real-world data.
Keywords: anomaly detection, performance modeling, IDS evaluation,
tuning.
1 Introduction
The results of a search on the web under anomaly detection will attest to the
prevalence of anomaly detectors and their application toward the detection of
worms, insider threats, and computer attacks. What is interesting, however, is
not the large number of hits but rather the increasing awareness by the main-
stream community regarding the shortcomings of anomaly detection. Articles
entitled “Anomaly detection falls short” or “Anomaly detection is not the best
way to prevent virus, worm attacks” [3] and so forth, are now questioning the
eﬃcacy of the anomaly detection approach. This highlights the issue of progress:
have we improved since Denning’s [4] seminal paper, and if so how much progress
has been made?
One of the most fundamental ways of measuring progress is to evaluate a
detector and benchmark its performance. It is particularly important that a
detector’s performance is benchmarked in a way that can be described as robust.
This means that the results of the evaluation strategy should be:
D. Zamboni and C. Kruegel (Eds.): RAID 2006, LNCS 4219, pp. 106–126, 2006.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006
Anomaly Detector Performance Evaluation
107
– repeatable - to allow for independent validation;
– reliable - performance results should be well characterized so as to remain
useful and valid outside the purview of the evaluation process itself; and
– informative - evaluation results should provide an understanding of the
causes underlying performance behaviors, thereby facilitating improvements.
Current anomaly-detection evaluation strategies do not satisfy these criteria.
The results from current strategies are typically not repeatable (e.g., due to
reasons such as unavailability of evaluation data sets, poorly documented eval-
uation methodologies, etc.), not reliable (an anomaly detector that performs
well in one environment will not necessarily perform well in another environ-
ment), and not informative (hit, miss and false alarm rates alone do not explain
why a detector may have performed poorly). To give an example, the perfor-
mance results reported in the literature for a particular anomaly-based intrusion
detection system were accompanied by a disclaimer stating, “It is not known
what eﬀect diﬀerent training sets would have on the results presented in this
paper [8].” In short, current evaluation strategies make it diﬃcult to measure
progress.
One of the reasons for this is that current schemes rarely consider or measure
phenomena in the data environment that aﬀect detector performance, such as
the characteristics of the background data or the characteristics of attack mani-
festation. If the manifestation of an attack in a data stream is not identiﬁed and
characterized, it will be diﬃcult to know why an anomaly detector responded
weakly, for example, to the presence of that attack. If the detector’s response is
weak, causing the attack to be missed, the mere act of incrementing the “miss”
count is not suﬃcient to understand what caused the attack to be missed or
what is needed to mitigate the condition.
Furthermore, the results of current evaluation strategies are also used to tune
detector performance, e.g., by allowing a defender to select the detection thresh-
old associated with the most acceptable operating point on a receiver operating
characteristic (ROC) curve. However, detection thresholds and other detector
parameters inﬂuencing performance are often set based on the intuition of the
detector’s designer given a handful of test cases [1,2]. No knowledge of envi-
ronmental inﬂuences on detector performance is acquired or used to guide the
tuning process. This introduces uncertainty into the ﬁnal results because if the
data environment changes, e.g., if the attacker’s behavior diﬀers from those in
the test cases used to tune the detector, the detector may no longer be optimally
tuned for detecting the attacker. It would seem prudent to characterize the data
environment in which a detector is deployed to provide some context with which
to describe a detector’s behavior.
This paper describes an evaluation strategy aimed at producing results that
are repeatable, reliable and informative. The anomaly detector evaluated in this
study (NIDES), was chosen for its simplicity and for the wealth of information
readily available about it – to our knowledge, no other intrusion detection system
has been as well documented in the open literature as NIDES is. Not only is there
substantial information regarding the algorithm, but there are also numerous
108
J.P. Hansen, K.M.C. Tan, and R.A. Maxion
reports documenting various evaluation results for the detector. A synthetic
evaluation environment was built around this detector, to cover a wide range of
potential environmental conditions in which the detector may be deployed.
This study makes two contributions. First, the detector’s blind spots and
sensitivities to various forms of anomalies are identiﬁed. Second, diagnostic in-
formation is provided to explain why the detector performed well or poorly.
Evidence is provided, showing that these results extend to arbitrary data sets.
2 Problem, Approach, and Rationale
This paper addresses the problem of acquiring robust evaluation results for an
anomaly detector. The approach involves creating a synthetic environment in
which to assess the performance of an anomaly detector.
There are two reasons to use a synthetic environment: (1) to assure control
over the various artifacts within a data environment that will aﬀect the detector,
and (2) to establish ground truth. The ﬁrst reason acknowledges the inﬂuence
of the data environment on detector performance. Variables in the data environ-
ment such as the distribution of the background test data, the training data and
the anomalies all contribute to a detector’s response. It is possible for a given
detector to be more sensitive to certain characteristics in the data environment
than other detectors are. For example, a Markov-based detector is more sensitive
to changes in the frequencies of data elements than a sequence-based detector,
like stide [6], would be. This sensitivity can cause a Markov-based detector to
produce more false alarms than stide due to frequency ﬂuctuations in the test
data (this phenomenon was observed and documented in [13]).
The second reason for using a synthetic environment is the determination of
ground truth. Ground truth simply means knowing the identity of every event
that an anomaly detector has to make a decision upon so that it can be deter-
mined whether the detector is accurate in its decision. Accuracy in performance
evaluations requires that ground truth be correctly established.
In intrusion detection literature, ground truth data for anomaly detector eval-
uation typically comprise training data, i.e., data collected in the absence of
attacks, and test data, i.e., data collected in the presence of attacks [5,17]. The
problem with this scheme is that there is no guarantee that the data collected
in the presence of attacks will actually contain manifestations of that attack. It
is possible that the attack does not manifest in the kind of data being collected,
e.g., cpu usage data for detecting password crackers is not logged in BSM data
(the BSM Basic Security Module is the security auditing mechanism for Sun
systems). It is therefore important to clearly establish that each event in the
evaluation data stream is or is not the result of an attack.
It should be noted that anomaly detectors directly detect anomalies, not at-
tacks [14]. Assessing an anomaly detector should therefore be focused on what
kinds of anomalies a detector detects, and how well it detects them. It makes
more sense for an anomaly detector to be assessed on the events that it directly
detects (anomalies) rather than events that it does not directly detect (attacks).
Anomaly Detector Performance Evaluation
109
For this reason, ground truth in this study is anomaly-based. This means that
the ability of the detector to detect anomalies is evaluated; therefore each event
in the assessment data is marked as either anomalous or not.
The assessment strategy proposed in this paper is demonstrated using a re-
implementation of the statistical anomaly detection component of NIDES [10],
speciﬁcally the portion for processing categorical data. The re-implemented de-
tector will be referred to as RIDES (Re-implementation of IDES) and is an
example of a frequency-based detector, i.e., a detector that employs relative fre-
quencies in its detection algorithm. The assessment will map the performance of
RIDES over a varying range of data characteristics, identify the detector’s blind
spots and ﬁnally determine the parameter values that would produce the best
performance in various environments, i.e., tune the detector.
3 Related Work
In the intrusion detection literature, the most common method of evaluating
detector performance can be summarized as follows [6,7,8,16,11,2,1]: sets of nor-
mal data, data obtained in the absence of intrusions or attacks, and intrusive
data, data obtained in the presence of attacks, are collected. The anomaly-based
intrusion detection system is trained on the normal data, and then tested on
test data that contains either intrusive data only or some mixture of normal
and intrusive data. The success of the detection algorithm is typically measured
in terms of hit, miss, and false alarm rates and charted on an ROC curve, the
ideal result being 100% hits and 0% misses and false alarms. The idea is then
to select a point where the performance of the detector is most suitable to the
defender, or to observe performance trends over a range of values and compare
those trends with trends of other detectors deployed on the same data set.
In some cases separate experiments are carried out to chart false alarm rates by
deploying the anomaly-based intrusion detection system on purely normal data,
i.e., where both training and test data consist of diﬀerent sets of normal behavior
only. Since anomaly detectors are typically designed with various parameters,
e.g., a window size or a neural-network learning constant, this evaluation strategy
may be repeated over a set of parameter values.
As previously discussed, these strategies are limited in that they say nothing
about the detector’s performance on other data sets. In short, all that can be
determined from the results of such an evaluation procedure is that a set of
anomalies were detected, some or none of which were caused by the attack. This
does not say much about the performance of a detector even with regard to
detecting attacks, because it is not clear if the anomalies detected were really
caused by the attacks or by a number of other reasons, e.g., poorly chosen
training data or faulty system monitoring sensor etc.
The most well-documented evaluation scheme described for NIDES was per-
formed by SRI [1,2]. The evaluation involved human experts who modiﬁed the
detector’s conﬁguration parameters after each of three experiments - concept,
veriﬁcation and reﬁnement. The goal was to determine the best conﬁgurations
110
J.P. Hansen, K.M.C. Tan, and R.A. Maxion
for NIDES within the context of detecting “when a computer system was used for
other than authorized applications.” Detector performance was evaluated and
improved after each of the three experiments by changing the values of detector
parameters such as the short-term half-life. At the end of the entire evaluation
it was found empirically that shorter half-lives gave better false-positive rates.
However, because these results may only be valid for the evaluation data set
used, we build on this work by evaluating NIDES in a well-characterized data
environment; hence, we can begin to understand how the detector’s intrinsic
biases can be counterbalanced when it is deployed in another data environment.
4 Description of RIDES
To demonstrate our tuning methodology with a concrete example, we have de-
veloped a detector we call RIDES, which is a re-implementation of a portion
of the statistical anomaly detection component of NIDES [10]. There are two
main components in IDES/NIDES: an expert system and a statistical anomaly
detector. The expert system uses pre-deﬁned rules to detect known patterns of
behavior associated with intrusions, while the statistical anomaly detector is
tasked to detect novel or previously unseen attacks by looking for deviations
from known behavior. The statistical anomaly detector component in NIDES
monitors both numerical and categorical data.
RIDES focuses on anomaly detection in categorical data, and consists of four
main processes:
1. modeling the short-term and long-term behavior of the incoming stream of
categorical data;
2. updating the models to adapt to drift;
3. measuring the similarity between the two models; and
4. producing an anomaly signal when the diﬀerence between the two models
exceeds a threshold.
The following is a description of RIDES according to the four elements listed
above. The implementation is intended to be as faithful as possible to the NIDES
algorithms described in [9].
4.1 Modeling the Incoming Stream of Categorical Data
RIDES receives data in the form of a symbol stream. In the NIDES terminol-
ogy this would be called a categorical measure. A measure is the data from a
single monitored object, for example, the sequence of system calls made by an
application, or the sequence of user commands typed into an xterm window.
While NIDES supports multiple measures, we focus on the single-measure case
for simplicity.
For each measure, RIDES maintains a model of the measure’s short-term and
long-term behaviors. The short-term model reﬂects symbols that have arrived
Anomaly Detector Performance Evaluation
111
over the period of the last few records, while the long-term model reﬂects be-
havior over the past few days. RIDES models categorical data in terms of the
relative frequencies of each symbol in the alphabet of any given data set.
The short-term model is a probability distribution contained in the vector
g, reﬂecting the behavior in an exponential sliding window. A user-deﬁned pa-
rameter, the short-term half-life Hst, denotes the decay rate of the exponential
sliding window. The short-term half-life is the number of records after which
a symbol’s eﬀect on g will be reduced by one half. The relative frequencies for
the symbols contained in the vector g are subject to exponential smoothing
that is applied to the vector during the updating process described in the next
section.
The long-term model is a probability distribution contained in the vector
f, which also reﬂects behavior in an exponential sliding window, but over a
much longer time scale. Another user-deﬁned parameter, the long-term-life Hlt,
denotes the decay rate of this exponential sliding window. The long-term half-life
is the number of days after which a symbol’s eﬀect on f will be reduced by one
half. The relative frequencies for the symbols that are contained in the vector
f are subject to exponential smoothing that is applied to the vector during the
updating process described in the next section.
4.2 Updating the Models
RIDES learns new behaviors and “forgets” old behaviors by a process of updating
its short-term and long-term models. The short-term model is updated with each
symbol presented to the detector from the input data stream. The long-term
model is updated at the end of a day.
The long-term model f is updated by exponentially aging the current long-
term model by 2−1/Hlt, and then adding in a vector (1− 2−1/Hlt)P where P is a
vector representing the probability distribution for symbol types observed over
the most recent day.
4.3 Measuring Similarity Between Short- and Long-Term Models
The ﬁrst step in comparing the short- and long-term models is to compute a
statistic called Q. The Q statistic represents the unnormalized diﬀerence between
the short- and long-term models, and is loosely based on the chi-squared formula.
Q is computed as follows. Each time the short-term distribution vector g is
updated after receiving a new symbol, Q is calculated according to the following
equation:
N(cid:2)
(gi − fi)2
(1)
Q =
i=1
Vi
where N is the alphabet size and Vi is the approximate variance of gi. In [9],
this variance Vi is reported as being fi(1 − fi)(1 − 2−1/Hst).
112
J.P. Hansen, K.M.C. Tan, and R.A. Maxion
4.4 Producing the Anomaly Signal
After computing the Q statistic, representing the unnormalized diﬀerence be-
tween the short- and long-term models, RIDES converts Q into a normalized
diﬀerence statistic called S. This is done by building an empirical Cumulative
Distribution Function (CDF) from observed Q values. The S statistic is then
obtained by the formula:
−1
HN (FQ(Q)), 4)
S = min(F
(2)
−1
where F
HN (x) is the inverse CDF of a half-normal distribution. Informally, the
S statistic can be thought of as the number of standard deviations from normal
behavior, where a value of 0 indicates that the short-term and long-term model
are identical, and a value of 4 is the maximum diﬀerence modeled.
NIDES also deﬁnes a top-level T 2 score that is deﬁned as the mean of the
squares of the S scores for each “measure.” In NIDES an alarm is produced when
T 2 falls above a threshold value. However, since we limit our scope to a single
measure, RIDES produces an alarm when S falls above a detection threshold.
4.5 Special Categories
In addition to categories for regular symbols, NIDES also has two special symbol
types [9,1,2] that we include in our implementation: rare and new.
– Rare – Symbols that are very infrequent can have an excessive impact on the