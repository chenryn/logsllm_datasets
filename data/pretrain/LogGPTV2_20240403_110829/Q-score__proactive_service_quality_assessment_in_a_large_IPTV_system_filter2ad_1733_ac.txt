numbers, we need to identify the thresholds for alarming on the
Q-scores to the operations. The alarms can be proactively used to
predict customer calls. We apply simple threshold-based change
detection on the time-series of Q-scores to generate the alarms.
False alarm rate of Q-score. As a prediction mechanism of pos-
sible outbreaks, it is very important to have a low false alarm rate
in a service quality assessment. In Q-score, a multitude of com-
ponents prevent a single user, one end-user device, or a network
device from raising false alarms for a large population of users.
The feature normalization described in Section 3.2.1 regulates fea-
ture values, an exceptional feature value for an individual cannot
affect much to the entire population. The multi-scale aggregations
(Section 3.2.3, 3.2.4) further reduces the possibility of falsely em-
phasizing rare events. In the case of spatial aggregation, because
Q-score considers both individual users and spatial groups of users,
the score is stable even when an individual’s feature value is high.
Similarly, temporal aggregation prevents the chance of false alarms
due to highly transient feature value changes. Additionally, in prac-
tice, we carefully set the threshold of Q-scores to focus on minimiz-
ing false positives, even with slight sacriﬁce to coverage (recall).
2004. EVALUATION
In this section, we present the performance evaluation results of
Q-score and show that the regression results are accurate and ro-
bust, and the multi-scale aggregation of spatio-temporal features
has beneﬁt over single scale, non aggregated cases.
4.1 Evaluation Methodology
Metrics. We compare the number of predicted customer trouble
tickets and that of received customer trouble tickets and measure
the accuracy of prediction of service quality issues by false negative
rate (FNR) and false positive rate (FPR). The FNR and FPR are
computed per user basis.
F N R =
F P R =
#of time bins that Q-score fails to predicts a trouble ticket
#of time bins that have received trouble tickets
#of time bins that Q-score incorrectly predicts a ticket
#of time bins that do not have any trouble tickets
Note that due to the sparsity in the occurrence of user feedback
(i.e., trouble tickets), the number of time bins without any user
feedback is orders of magnitude higher than the number of time
bins with user feedback.
Training and Testing Sets. In our evaluation of the Q-score sys-
tem, we use data sets collected from a commercial IPTV network
provider in US over two months time period from August 1st, 2010
to September 30th, 2010. Unless otherwise mentioned, we use 15
days of data collected from August 15th, 2010 to August 29th, 2010
as the training data set for β and the subsequent 15 days of data
collected from September 1st, 2010 to September 15th, 2010 as
the testing data set. In addition, we use multi-scale temporal ag-
gregation of XT emp.Comb. combining δ of 3-24 hours and multi-
scale spatial aggregation of XSpat.Comb. combining spatial levels
of user, DSLAM, CO, and VHO as the default setting. Lastly, we
set the default feedback time bin γ to be γ = 24 hours.
We assign λ a small positive value within (0, 0.05]. While dif-
ferent λ exhibit small differences in accuracy, the optimal λ varied
from data set to data set. Since the selection of λ is speciﬁc to data
set in each test, we present the results with the best λ while omitting
to show its actual value.
4.2 Results
4.2.1 Accuracy Analysis
We begin our evaluation by assessing how well Q-score follows
the ground truth of user-perceived service quality.
In our evalu-
ation, we use user feedback as an approximation of the ground
truth of user-perceived service quality issues in training and test-
ing Q-score system. Recall that the user feedback is incomplete
in reﬂecting user perceived service quality. In fact, the user feed-
back captures a subset of user perceived service quality issues and
thus underestimates the actual occurrences of service performance
degradations. Fortunately, major and/or long lasting service per-
formance degradations are likely to be captured by the user feed-
back [24]. Hence, it is likely that the computed Q-score underes-
timates the actual user perceived performance issues, but expected
to capture major outages and performance degradations.
While Q-score does not perfectly match with the user perceived
service quality at the individual user level, the changes or trends in
the distribution of Q-score are expected to follow closely with that
of the actual service quality degradation at certain spatial aggrega-
tion levels. In our evaluation, we choose CO as the aggregation
Aggregation method P value in F-test Correlation coefﬁcient R
0.6826
CO
Random
0.7165
0.00
2.21e-31
Table 2: Accuracy analysis results of Q-score
level1. By summing up individual users’ feedback within each CO
into a single value, we obtain an aggregation vector Sactual of user
feedback. Since Sactual is a spatio-temporal aggregation of user
feedback, its element now signiﬁes the level of user-perceived ser-
vice quality issues. Similarly, by summing up the individual users’
Q-score inside each CO into a single value, we can obtain an aggre-
gation vector of Q-scores Sestim that signiﬁes our estimated level
of user-perceived service quality.
To evaluate the signiﬁcance of the relation between the actual
(Sactual) and estimated (Sestim) user perceived service quality
level, we run an F-test between them. Let the null hypothesis
H0 : r = 0 where Sactual = r ∗ Sestim. We ﬁnd that for the
signiﬁcance level of 0.1, the hypothesis test is rejected, implying
that the relation between the two vectors does exist. A Pearson’s
correlation test also shows relatively high correlation coefﬁcient R
between Sactual and Sestim, proving that the relationship between
the two is linear.
In other words, Q-score does follow the user-
perceived service quality.
Because CO level aggregation represents spatial proximity of
user geographical locations, user feedback rates can be different
across COs. To evaluate if CO aggregation introduce any bias on
the results, we also conduct the same evaluation using a random
grouping with the same number of groups as the number of COs
and compute aggregation vectors. Table 2 summarizes the F-test
and Pearson’s correlation tests results for both CO level aggregation
and random grouping based aggregation. The random grouping
based aggregation generally shows the same results as the CO level
aggregation, supporting that Q-score indeed follows user feedback
regardless of how we aggregate users in Q-score computation.
4.2.2 Multi-scale Temporal Aggregation
In this section, we evaluate the impact of different time-bin size
(δ) on network indicators (single-scale temporal level aggregation).
Then we show the performance beneﬁts by using multi-scale tem-
poral aggregation on network performance indicators (multi-scale
temporal level aggregation).
Figure 3 shows the Q-score on FPR-FNR trade-off curves using
various δs ranging from 3 hours to 24 hours (i.e., each curve corre-
sponds to an X with a given δ). Note that FPR shown on the x-axis
is in log-scale and FNR shown on the y-axis is in normal scale.
The ﬁgure shows that the prediction accuracy gets generally better
as we shorten δ (i.e., the curve gets closer to the lower left corner of
the plot). However, comparing δ = 3hours and δ = 6hours, their
FNR overlaps over different range of FPR, indicating that there is
no single optimal δ to be chosen.
Figure 4 shows the results of XTemp.Comb. by applying multi-
scale temporal aggregation on network performance indicators.
There are three curves obtained by combining (i) shorter time bins
of 3-12 hours, (ii) longer time bins of 15-24 hours, and (iii) the
entire range of 3-24 hours. We observe that (iii) provides the best
performance among them. At the same time, (iii) is also strictly
1We considered various levels of spatial granularity in the IPTV
hierarchy including DSLAM, CO, and VHO levels. Among them,
CO level aggregation is most adequate for the accuracy analysis
because it yields a statistically sound number of user IDs in each
CO and enough number of COs to make meaningful comparisons
between aggregation vector Ses.
201 100
 100
 80
 80
 60
 60
 40
 40
 20
 20
e
t
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
3 hrs
6 hrs
15 hrs
21 hrs
24 hrs
e
t
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
 0
 0
 0.001
 0.001
 0.01
 0.01
 0.1
 0.1
 1
 1
 10
 10
 100
 100
False Positive Rate
 100
 100
 80
 80
 60
 60
 40
 40
 20
 20
USER+DSLAM+CO+VHO
USER+DSLAM+CO+VHO
USER ID
USER ID
DSLAM
DSLAM
CO
CO
VHO
VHO
 0
 0
 0.001
 0.001
 0.01
 0.01
 0.1
 0.1
 1
 1
 10
 10
 100
 100
False Positive Rate
Figure 3: Comparison of various δs on features (performance
indicators)
Figure 5: Comparison of various spatial aggregation levels on
features (performance indicators)
 100
 100
 80
 80
 60
 60
 40
 40
 20
 20
e
t
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
Combined 3-24 hrs
Combined 15-24 hrs
Combined 3-12 hrs
e
t
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
3 hrs
3 hrs
9 hrs
9 hrs
15 hrs
15 hrs
21 hrs
21 hrs
24 hrs
24 hrs
27 hrs
27 hrs
36 hrs
36 hrs
 100
 100
 80
 80
 60
 60
 40
 40
 20
 20
 0
 0
 0.001
 0.001
 0.01
 0.01
 0.1
 0.1
 1
 1
 10
 10
 100
 100
False Positive Rate
 0
 0
 0.001
 0.001
 0.01
 0.01
 0.1
 0.1
 1
 1
 10
 10
 100
 100
False Positive Rate
Figure 4: Comparison of multi-scale temporal aggregations on
features (performance indicators)
Figure 6: Comparison of various time bin size γ on user feed-
back
better than any curves in Figure 3, proving that simultaneously re-
gressing on multiple scales of temporal aggregations on network
performance indicators does improve the accuracy of Q-score pre-
diction on service quality issues.
4.2.3 Multi-scale Spatial Aggregation
We now evaluate the impact of various levels of spacial aggre-
gation on network performance indicators and the beneﬁt of using
multi-scale spatial aggregation in Q-score.
Figure 5 shows the trade-off curves of X with various single-
ID (XuserID),
scale spatial aggregation ranging from user
to DSLAM (XDSLAM), to CO (XCO), and to VHO (XVHO)
level. As the spatial aggregation level changes from user ID to
DSLAM (i.e., smaller-sized region to larger-sized region), we ob-
serve that the FNR increases from 35% to 100% when FPR is at
0.1%. A possible explanation to this is that if the service quality
issues reported by users are more related to a home network prob-
lem rather than a provider network problem, spatial aggregation of
network performance indicators can attenuate signals relevant to
the end-user devices at home. As we will show in Section 5.1, by
analyzing signiﬁcant KPIs, we are able to conﬁrm that the signif-
icant KPIs are mostly related to STB and RG (i.e., home network
devices) while backbone network appeared to be well provisioned.
In addition to the single-scale spatial aggregation, the ﬁrst plot
of Figure 5 (denoted as ‘USER + DSLAM + CO + VHO’) shows
multi-scale
aggregation (with measurement matrix
XSpat.Comb.). We observe that the multi-scale spatial aggrega-
tion outperforms any single-scale aggregation in terms of overall
prediction accuracy, proving that the regression algorithm makes