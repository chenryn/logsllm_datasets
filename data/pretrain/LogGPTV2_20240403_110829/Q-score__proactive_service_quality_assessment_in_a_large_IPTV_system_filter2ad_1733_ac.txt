### Thresholds for Alarming on Q-Scores

To effectively identify and address service quality issues, it is essential to set appropriate thresholds for alarming on Q-scores. These alarms can be used proactively to predict and mitigate potential customer calls. We employ a simple threshold-based change detection method on the time series of Q-scores to generate these alarms.

#### Importance of Low False Alarm Rate

In the context of service quality assessment, maintaining a low false alarm rate is crucial. The Q-score system is designed with multiple components to prevent a single user, end-user device, or network device from generating false alarms that could affect a large population of users. 

- **Feature Normalization**: As described in Section 3.2.1, feature normalization ensures that exceptional values from individual users do not significantly impact the overall population.
- **Multi-scale Aggregations**: Sections 3.2.3 and 3.2.4 detail how multi-scale aggregations (both spatial and temporal) further reduce the likelihood of false alarms by preventing the overemphasis of rare events.
- **Spatial Aggregation**: By considering both individual users and spatial groups, the Q-score remains stable even when an individual's feature value is high.
- **Temporal Aggregation**: This prevents false alarms due to highly transient changes in feature values.

In practice, we carefully set the Q-score thresholds to minimize false positives, even if it means a slight reduction in coverage (recall).

### Evaluation

This section presents the performance evaluation results of the Q-score system, demonstrating its accuracy and robustness. We also show the benefits of multi-scale aggregation of spatio-temporal features over single-scale, non-aggregated methods.

#### Evaluation Methodology

**Metrics**:
We compare the number of predicted customer trouble tickets with the actual number of received trouble tickets to measure the accuracy of service quality issue predictions using the false negative rate (FNR) and false positive rate (FPR). These metrics are computed on a per-user basis.

\[
\text{FNR} = \frac{\text{Number of time bins where Q-score fails to predict a trouble ticket}}{\text{Number of time bins with received trouble tickets}}
\]

\[
\text{FPR} = \frac{\text{Number of time bins where Q-score incorrectly predicts a trouble ticket}}{\text{Number of time bins without any trouble tickets}}
\]

Note that due to the sparsity of user feedback (i.e., trouble tickets), the number of time bins without any user feedback is much higher than those with feedback.

**Training and Testing Sets**:
Our evaluation uses data collected from a commercial IPTV network provider in the US over a two-month period from August 1, 2010, to September 30, 2010. We use 15 days of data from August 15, 2010, to August 29, 2010, as the training set and the subsequent 15 days from September 1, 2010, to September 15, 2010, as the testing set. The default settings include multi-scale temporal aggregation of \(X_{\text{Temp.Comb.}}\) combining \(\delta\) of 3-24 hours and multi-scale spatial aggregation of \(X_{\text{Spat.Comb.}}\) combining spatial levels of user, DSLAM, CO, and VHO. The default feedback time bin \(\gamma\) is set to 24 hours.

The parameter \(\lambda\) is assigned a small positive value within (0, 0.05]. While different \(\lambda\) values exhibit small differences in accuracy, the optimal \(\lambda\) varies across datasets. We present results with the best \(\lambda\) but omit its exact value.

#### Results

**Accuracy Analysis**:
We evaluate how well the Q-score follows the ground truth of user-perceived service quality. User feedback is used as an approximation of the ground truth, though it is incomplete and may underestimate actual service quality issues. Major and long-lasting degradations are more likely to be captured by user feedback, so the Q-score is expected to capture major outages and degradations.

While the Q-score may not perfectly match individual user perceptions, the changes or trends in the distribution of Q-scores should closely follow the actual service quality degradation at certain spatial aggregation levels. We choose the CO level for aggregation. By summing up individual users' feedback within each CO, we obtain an aggregation vector \(S_{\text{actual}}\) of user feedback. Similarly, by summing up the individual users' Q-scores within each CO, we obtain an aggregation vector \(S_{\text{estim}}\) of estimated service quality.

To evaluate the significance of the relationship between \(S_{\text{actual}}\) and \(S_{\text{estim}}\), we perform an F-test and Pearsonâ€™s correlation test. The null hypothesis \(H_0: r = 0\) is rejected, indicating a significant relationship. The high correlation coefficient \(R\) supports that the Q-score follows user-perceived service quality.

**Multi-scale Temporal Aggregation**:
We evaluate the impact of different time-bin sizes (\(\delta\)) on network indicators and show the benefits of multi-scale temporal aggregation. Figure 3 illustrates the Q-score on FPR-FNR trade-off curves for various \(\delta\) values. Shorter \(\delta\) values generally improve prediction accuracy, but there is no single optimal \(\delta\).

Figure 4 shows the results of \(X_{\text{Temp.Comb.}}\) with multi-scale temporal aggregation, demonstrating that combining multiple time scales provides the best performance.

**Multi-scale Spatial Aggregation**:
We evaluate the impact of various levels of spatial aggregation on network performance indicators. Figure 5 shows the trade-off curves for single-scale spatial aggregations, from user ID to VHO level. Multi-scale spatial aggregation outperforms single-scale aggregation, proving that the regression algorithm benefits from considering multiple spatial scales.

### Conclusion

The Q-score system, with its multi-scale temporal and spatial aggregations, provides accurate and robust predictions of service quality issues. The careful setting of thresholds and the use of multi-scale aggregations help minimize false alarms and improve overall system performance.