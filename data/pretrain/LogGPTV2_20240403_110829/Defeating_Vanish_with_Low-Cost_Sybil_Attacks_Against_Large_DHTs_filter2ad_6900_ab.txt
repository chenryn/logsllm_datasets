time T . If Vanish is secure then any attacker obtaining the
VDO at time T + t, t > 0 will not be able to reconstruct
the data. Providing this guarantee of security requires at
least two assumptions about the attacker. (We note that if
we require correctness we also must assume availability of
the DHT before the timeout.) These assumptions are:
Limited Network View The attacker must not be able
to view the user’s trafﬁc to the DHT. Otherwise, the
attacker could simply sniff and record the shares
as they are stored.
Limited View of DHT The attacker must not be able
to read more than a small fraction of the data stored
on the DHT. Otherwise, the attacker could build
an archive of the DHT and later look up the key
shares.
Our work focuses on violating the second assumption.
Deployed implementation Vanish version 0.1 was re-
leased in August 2009. It consists of two components: a core
system, which provides the encapsulation and decapsulation
functions, and a Firefox web browser extension. The Firefox
extension allows the user to right-click on a selected area
of text to encapsulate it into a Vanishing Data Object. A
user can then right-click on the VDO to have the extension
retrieve the original text.
By default, the Firefox extension breaks the encryption
key into n = 10 shares with a recovery threshold of k = 7.
However, the Vanish paper recommends a slower but more
secure setting of n = 50 and k = 45. The Vuze DHT expires
stored data after T = 8 hours. Users can extend this time by
periodically reposting their key shares.
2.2. Vuze DHT
The current Vanish implementation stores keys in the
Vuze DHT, which is used by the Vuze BitTorrent client (also
known as Azureus) for decentralized torrent tracking. Vuze
estimates that the DHT contains over a million nodes, though
there is signiﬁcant diurnal variation. The DHT is based on
a modiﬁed Kademlia [29] implementation and functions
similarly to many other DHTs. Nodes in the network and
keys in the hash table are assigned 160-bit identiﬁers (IDs).
Each DHT node stores those keys which are closest to it in
the ID space, as determined by the XOR distance metric.
Each Vuze client maintains a routing table that categorizes
peers into a number of “k-buckets” by their distance from
its own ID, where k is a parameter of the Kademlia design
and is set to 20 in Vuze.
All operations are performed using simple RPC com-
mands that are sent directly to the remote peer in a single
UDP packet. The primary Vuze RPCs are PING, which
checks node liveness and announces the sending node,
STORE, which stores a set of key value pairs at the receiving
node, FIND-NODE, which requests the 20 closest contacts
to a given ID that the receiver has in its routing table, and
FIND-VALUE, which functions like FIND-NODE except that
the receiver instead returns the values stored at the requested
key.
The fundamental Kademlia operation is node lookup,
which ﬁnds the 20 closest nodes to a given ID. To begin
a lookup, a node sends FIND-NODE requests to the 20 closest
nodes to the ID that it currently has in its routing table. Each
peer returns a list of the peers it knows that are closest to
the desired ID. The requesting node contacts those peers,
reaching successively closer peers until it ﬁnds those respon-
sible for storing the desired ID. A lookup terminates when
the closest known peer that has not yet been contacted is
farther from the desired ID than the farthest of the closest 20
responding peers.
To retrieve or store a value, the requesting node hashes
the key to obtain its ID and performs a lookup for the ID
to determine the 20 closest peers to the key. It then directly
contacts those peers with a request to return or store the
associated value.
A node joins the Vuze DHT by contacting a known peer
and initiating a lookup for its own ID. It uses this lookup to
build its list of peers and eventually ﬁnds the nodes closest
to its ID. When a node is contacted by a new peer with an ID
among the 20 closest to its own, it replicates all of its stored
keys to that node. To minimize unnecessary network trafﬁc,
a node only replicates those keys to which it is the closest.
To deal with unreliable nodes, peers also replicate the data
periodically to the 20 closest nodes to the key’s ID.
The Vuze DHT employs a rudimentary anti-Sybil mech-
anism: node IDs are forced to equal the hash of the node’s
IP address and port number. This design accommodates the
common case of multiple users communicating via a single
NAT device by allowing the same IP to join the network at
different locations using different port numbers.
3. Attacking Vanish
One way to attack Vanish is with a large Sybil attack
against the underlying Vuze DHT. Vuze nodes replicate the
data they store to up to 20 neighboring nodes, so a straight-
forward attack would be to have many Sybils participate in
the network and wait for replication to occur.
Figure 1 shows the probability of recovering a VDO given
the probability of recovering any individual key share. We
model the probability of recovering an individual key share
as a binomial random variable with probability p. The prob-
ability of recovering the k/n VDO is then
Pr[recover VDO] =
pi(1 − p)n−i
n(cid:88)
(cid:18)n
(cid:19)
i=n−k
i
The Vanish authors approximated this as a linear function,
but that is a poor model of the actual behavior. The probabil-
ity of recovery exhibits a threshold phenomenon that works
to the attacker’s advantage.
This ﬁgure shows that, to achieve high VDO recovery
for the parameters suggested by the Vanish authors, the
attacker needs to have at least an 80% chance of learning
each stored share. Our experiments suggest that this would
require more than 60,000 Sybils. Although Vuze allows each
IP address the attacker owns to participate with up to 65,535
node IDs (one for each UDP port), the attacker may not have
Figure 1. VDO recovery vs. key share recovery. The attacker’s chances of successfully decrypting
a VDO improve rapidly with the probability of recovering any given key share from the DHT. Here we
estimate the VDO recovery probability for three pairs of secret sharing parameters k/n.
sufﬁcient computing resources to maintain so many Sybils
concurrently, and the necessary bandwidth might also be
prohibitively high.
The attacker can do much better by exploiting the fact
that he does not need continuous control over such a large
fraction of the network. Rather, he need only observe each
stored value brieﬂy, at some point during its lifetime. Two
properties of Vuze’s replication strategy make this easy. First,
Vuze replicates values to new clients as soon as they join the
network. Second, to ensure resiliency as nodes rapidly join
and leave, Vuze nodes replicate the data they know to their
neighbors at frequent intervals, usually every 30 minutes.
Because of these behaviors, a Sybil node need not par-
ticipate in the network for very long in order to view the
majority of keys available at its position. If the attacker can
run m Sybils at a time, they can move, or “hop,” through
the range of available identities by changing their port or IP
address, thus gaining a chance to observe data stored at a
different set of locations. Hopping enables the attacker to
T effective Sybils during a given period of time t,
support mt
where T is the duration of each hop.
We found in our experiments that T = 3 minutes was
sufﬁcient to observe almost all the information stored in the
vicinity of each ID. This means that, over the 8 hour VDO
lifetime, each Sybil can participate in the network from
160 node IDs with minimal loss in coverage. The hopping
strategy vastly increases the efﬁciency of our attacks.
3.1. Simple Implementation (Unvanish)
We constructed two implementations to experiment with
the hopping attack strategy. The ﬁrst, Unvanish, demon-
strates the simplicity of constructing a Sybil attack against
Vanish. Unvanish is based on the publicly-available Vuze
DHT client code and adds just 268 lines3 of Java for creating
and operating the Sybils. An 82-line Python script instanti-
ates a number of Unvanish processes and controls the nodes’
hopping.
Unvanish records keys and values it receives from neigh-
boring nodes upon joining the network. To reduce the cost of
storage and transfer to the eventual permanent value archive,
Unvanish discards values whose lengths are outside the likely
range for shares of Vanish encryption keys. The length of a
key share is dependent on the key length, number of shares
n, and threshold k. Based on the Vanish implementation,
we calculate that shares of 128-bit encryption keys for the
default setting of k = 7, n = 10 and the recommended
“high-security” setting of k = 45, n = 50 will be 16 to 51
3All measures of lines of code are generated using David A. Wheeler’s
’SLOCCount’.
0.00.20.40.60.81.00.00.20.40.60.81.0P(recovershare)P(recoverVDO)7/109/1045/50bytes long, inclusive. Unvanish records all values within that
range.
We have been running Unvanish on the Amazon EC2
service, which provides a realistic assessment of the cost
of the attack. We run our Vuze DHT client on 10 “small”
EC2 instances, which provide 1.7 GB of physical memory,
160 GB of local storage, and compute power approximately
equivalent to a 1.0 GHz Xeon processor. Memory and pro-
cessor constraints restrict Unvanish to 50 concurrent DHT
nodes on each instance. Each DHT node hops to a new node
ID after every 150 seconds of operation.
We created an online demonstration of Unvanish that
decapsulates VDOs after they have supposedly expired. To
minimize the harm to Vanish users, we discard the data we
collect from the DHT after one week, though a real attacker
could easily keep it indeﬁnitely.
3.2. Advanced Implementation (ClearView)
In order to investigate how the costs of the attack could be
reduced with further optimizations, we developed a second
implementation, which we call ClearView.
ClearView is a from-scratch reimplementation of the Vuze
DHT protocol written in 2036 lines of C. It uses an event-
driven design to minimize CPU and memory footprints, and
it can run many DHT clients in a single process. It can
maintain several thousand concurrent Sybils on a single
EC2 instance. On startup, ClearView bootstraps multiple
Vuze nodes in parallel, seeding the bootstrap process with
a list of peers gleaned ahead of time from a scan of the
network in order to avoid overloading the Vuze DHT root
node. ClearView then logs the content of each incoming
STORE request for later processing.
ClearView reduces the amount of network trafﬁc used in
the attack by replying to incoming DHT commands only as
necessary to collect stored data. ClearView’s Sybils reply to
all PING and STORE requests in order to inform other nodes
that they are live. Vuze also requires that they respond to a
FIND-NODE request before they will receive any STORE re-
quests. Since the principal source of STOREs is replications
from nearby nodes, ClearView Sybils reply only to FIND-
NODE requests from nodes whose IDs share at least 8 preﬁx
bits with their IDs. ClearView omits the Vuze routing table
for efﬁciency and ease of implementation, so its FIND-NODE
replies contain only the contact information of the reply-
ing Sybil. ClearView unconditionally ignores FIND-VALUE,
KEY-BLOCK, and STATS requests, which are unnecessary for
crawling the DHT.
During preliminary experiments, we discovered that
Sybils remain in the Vuze routing tables for a signiﬁcant
time after they shut down and that other Vuze peers continue
to attempt to contact them. Our hopping strategy causes
each Sybil to run for only a short time (3 minutes for our
ClearView experiments), so these latent requests amount
to substantial unwanted UDP trafﬁc. The problem is com-
pounded by the default behavior of the Linux kernel, which
replies to each packet with an ICMP Destination Unreach-
able message. We found that these ICMP messages consti-
tuted a majority of ClearView’s outgoing trafﬁc.
We achieved substantial cost savings by simply conﬁg-
uring the Linux ﬁrewall to block outgoing ICMP messages.
A more advanced implementation might be able to avoid
paying for the unwanted inbound trafﬁc as well by using
EC2’s network ﬁrewall API [1] to allow trafﬁc only to ports
used by the current set of Sybils.
4. Evaluation
This section measures the effectiveness of our two at-
tack implementations and quantiﬁes the costs of running the
attacks on Amazon EC2.
4.1. Simple Hopping
We ran Unvanish on 10 “small” EC2 instances for ap-
proximately 24 hours. Over a 7.5-hour window during that
time, we seeded 104 VDOs into the DHT, using the default
security parameters of 7 of 10 shares required for decryption.
Each EC2 instance ran 50 concurrent Sybils which hopped
every 150 seconds, giving us data from 96,000 node IDs dur-
ing the 8-hour DHT store lifetime. Out of 1040 key shares,
we were able to recover 957, indicating that we achieved
about 92% coverage of key-value pairs. We successfully
decrypted 100% of the 104 VDOs using the data Unvanish
collected.
Running an EC2 “small” instance costs $0.10 per hour if
the instance is created on demand. Amazon also provides
reserved instance pricing, which entails an upfront charge
followed by a reduced per-hour usage charge. A one-year
reservation for 10 instances running full-time would cost
$0.56 per hour. During a one day run of Unvanish, our 10
EC2 instances transferred 176 GB of data in and 196 GB
out, and the average transfer cost was $2.12 per hour. Ex-
tending these ﬁgures, the cost for machines and transfer to
run Unvanish for a year would be $23,500. By contrast, the
original Vanish paper estimates that such an attack would
have an annual cost exceeding $860,000.
4.2. Advanced Hopping
To evaluate the effectiveness of ClearView, we ran trial
attacks against the Vuze network for around 8 hours at a time.
For two hours before the start of each experiment, we used