# Title: Delta: Automatic Identification of Unknown Web-based Infection Campaigns

## Authors
Kevin Borgolte, Christopher Kruegel, Giovanni Vigna  
Department of Computer Science  
University of California, Santa Barbara  
Santa Barbara, California, United States of America

## Abstract
Identifying malicious web sites has become a major challenge in today's Internet. Previous work focused on detecting if a web site is malicious by dynamically executing JavaScript in instrumented environments or by rendering web sites in client honeypots. Both techniques bear a significant evaluation overhead, as the analysis can take up to tens of seconds or even minutes per sample.

In this paper, we introduce a novel, purely static analysis approach, the ∆-system, that (i) extracts change-related features between two versions of the same website, (ii) uses a machine-learning algorithm to derive a model of web site changes, (iii) detects if a change was malicious or benign, (iv) identifies the underlying infection vector campaign based on clustering, and (v) generates an identifying signature.

We demonstrate the effectiveness of the ∆-system by evaluating it on a dataset of over 26 million pairs of web sites, running alongside a web crawler for a period of four months. Over this time span, the ∆-system successfully identified previously unknown infection campaigns, including one that targeted installations of the Discuz!X Internet forum software by injecting infection vectors into these forums and redirecting forum readers to an installation of the Cool Exploit Kit.

## 1. Introduction

The rapid growth and widespread access to the Internet, along with the ubiquity of web-based services, make global communication and interaction easy. However, the software used to implement web site functionality is often vulnerable to various attack vectors, such as cross-site scripting or SQL injections, and access policies might not be properly implemented. An attacker can exploit these server-side vulnerabilities to inject malicious code snippets, called infection vectors, which, in turn, attack visitors through drive-by install or download attacks. These attacks try to exploit client-side vulnerabilities to download or install malware, or lure the user into installing malware. If an attacker finds a server-side vulnerability that affects multiple web sites (possibly thousands), they can automate the exploitation, search for other vulnerable web sites, and launch a carefully crafted infection campaign to maximize the number of potential victims.

Recent reports by the security company Sophos [1, 2] show that in 2012, over 80% of all web sites attacking users were infected legitimate web sites, such as those of trade associations, nightclubs, television companies, or elementary schools. All of these web sites had been altered, in one way or another, to attack visitors. In another case, in early 2013, web sites hosting documentation for software developers were modified to serve carefully crafted infection vectors that exploited client-side vulnerabilities, which were then leveraged as the first stepping stone in sophisticated attacks against Twitter [3], Facebook’s engineering team [4], and Apple [5].

Major challenges in detecting infection vectors include the increasing dynamism of web sites and the regular changes in their static content. The underlying infection vector might not be clearly visible to analysis tools, or even to well-trained human security analysts. Legitimate modifications, such as adding new content, showing different, personalized advertisements, or even comments left by visitors, force the user to reevaluate the maliciousness of the web site, either through an automatic detection system or by manually inspecting any new content prior to its inclusion into the web site. Unwanted modifications, such as defacements or the insertion of exploit code to infect visitors with malware, are changes from which a user might want to be protected. Previous work in the area of web evolution [6–10] suggests that web sites do not change randomly but evolve constantly through small changes. If one takes into account personalized advertisements, a change might happen at every visit, making it necessary to analyze the web sites on each visit.

The current state of the art in protecting a user from malicious content is mainly realized through blacklists, which are queried before the web site is rendered or retrieved by the web browser. The Google Safe Browsing list [11] is likely the most prominent example. By definition, blacklists are reactive, which is an undesirable property for any protection mechanism because a malicious web site can potentially stay undetected for an extended period of time. Once a web site is blacklisted, its operator must go to great lengths to remove it from the blacklist, although it might have become benign. Such a removal process can take a frustrating amount of time since it is often subject to some form of verification that the web site is now benign, a process that might not happen immediately. More importantly, each web site infected as part of an infection campaign needs to be identified and added to the blacklist, even though the web sites attack visitors in the same, well-known way. Clearly, a proactive approach is preferable for both unknown and known infection vectors. On the other hand, scanning a web site proactively with online analyzer systems [12–14] is computationally very expensive and would introduce delays of multiple seconds per web site. Since such a delay is undesirable, it is unlikely that such a proactive approach would be deployed in a general setting, or that it would find its way into current browsers as a protection mechanism.

It is important to mention that, generally, the same infection vector is reused by an attacker and spread among a multitude of different web sites to maximize its impact; however, some parts of the infection vector might be randomized. Often, the infected web sites are from a single community, e.g., in a targeted attack on this community, they employ the same underlying software stack, or they share a web server that was attacked. Recent examples include attacks targeting installations of the Apache web server to replace the web server’s executable with the backdoor “Linux/Cdorked.A” [15, 16], which injects code to redirect visitors of the web site to exploit pages. These compromised web sites are not necessarily targeted, they usually follow a simple pattern: the infection vector was inserted in the same or in a very similar way. Being able to identify an infection vector, instead of just detecting that the web site is malicious, can provide important feedback since the initial cause of the infection vector can be investigated much more easily due to additional information, such as commonalities in different observations of the same infection vector.

To overcome the limitations of current approaches mainly based on dynamic analysis of web sites, we introduce the ∆-system to identify malicious activity in a web site based on static analysis of the differences between the current and previous versions of the web site. We cluster these differences, determine if the introduced or removed elements are associated with malicious behavior, identify the infection campaign it belongs to, pinpoint the actual infection vector, and automatically generate an identifying signature that can be leveraged for content-based protection.

### Main Contributions
- **Introduction of the ∆-system**: A novel approach to statically analyze and detect web-based infection vectors, and to identify infection campaigns based on features associated with modifications observed between two versions of a web site.
- **Tree Difference Algorithm**: A robust algorithm resistant to tiny changes, such as typographical corrections or small evolutionary modifications a web site undergoes.
- **Modification-Motivated Similarity Measures**: A set of measures to model the concepts of inserting and removing malicious behavior into and from a web site.
- **Large-Scale Evaluation**: Evaluation of the ∆-system on a large-scale dataset containing 26 million unique pairs of web sites, demonstrating its applicability in real-world scenarios in terms of infection campaign detection and identification capabilities.

## 2. ∆-System Design

The ∆-system focuses on the search problem of finding new infection campaigns and identifying similar, known infection campaigns, rather than deciding if a web site is malicious or benign. Instead of analyzing web sites in their entirety, the ∆-system investigates only the difference between two versions of the same web site.

### 2.1 Analysis Process

The ∆-system’s analysis process follows a simple four-step process, as shown in Figure 1:

1. **Retrieval and Normalization of the Web Site**
2. **Similarity Measurement with Respect to a Base Version**
3. **Cluster Assignment of the Similarity Vector**
4. **Generation of the Identifying Signature**

#### 2.1.1 Retrieval and Normalization

First, we retrieve the current version of a web site, for instance, the web site a user requested. After retrieving the source code of that web site, excluding all external references such as included scripts or frames, we perform multiple normalization steps:
- Normalize capitalization of all tags
- Reorder attributes of each tag and discard invalid attributes
- Normalize the quotation of an attribute’s value

These normalization steps ensure that functionally equivalent tags are treated equally during our evaluation, and that changes such as changing the capitalization of a tag or switching from single to double quotes do not affect our final results.

#### 2.1.2 Similarity Measurement and Clustering

Following these normalization steps, we measure the similarity to an already known (and normalized) base version. Measuring the similarity between two versions of the same web site in a meaningful way is non-trivial. The ∆-system first performs unordered tree-to-tree comparison via a novel algorithm that we introduce in Section 3. The algorithm extracts the nodes (or tags; subsequently, we use both terms interchangeably) from the Domain Object Model (DOM) tree of a web site that are different between the base and current version. Second, based on the extracted nodes, we leverage a variety of different features to extract meaningful information from the two versions (described in detail in Section 4). The system then tries assigning these feature vectors to a cluster, or detects them as outliers if they are not similar to any previously-observed modifications. Each different tag type, e.g., `<script>` or `<iframe>`, is treated separately, i.e., each type is assigned its own feature space; we do not project two tags of a different type into the same feature space. Additionally, due to the different nature of our features, where different distance metrics are essential for accurate cluster assignment, we perform consensus clustering for different groups: binary features, absolute and relative features are all treated as separate clustering instances. The cluster assignment and outlier detection process then distinguishes between three different cases:

- **Assignment to an Existing Cluster**:
  - Insertion or removal of an infection vector, if the cluster corresponds to a known infection campaign.
  - Legitimate modification, e.g., a version update of a library or the insertion of Facebook’s like button, if the cluster does not correspond to an infection campaign.

- **Detection as an Outlier**:
  - Potentially the start of a new infection campaign, if malicious behavior was inserted.
  - Potentially the end of a running infection campaign, if malicious behavior was removed.
  - A modification that is not of primary interest to us, such as a new article, template modifications, or a redesign of the web site.

- **Formation of a New Cluster** (the similarity vector we are clustering and other vectors that are close, which were outliers before, put the number of total vectors in this area of the feature space above the threshold to form a new cluster, i.e., we observed the number of the same modification in the wild that we require to constitute a trend, c.f. Section 5.1):
  - New infection campaign, if the node was inserted and is associated with malicious behavior.
  - End of an infection campaign, if the node was removed and is associated with malicious behavior.
  - Legitimate modification, e.g., an update to a new, bleeding-edge version of a library or the content-management system used, such as WordPress.

Upon cluster assignment of the similarity vector, we output the associated cluster, i.e., the corresponding trend (subsequently, we use these terms interchangeably). For instance, an infection campaign if the corresponding modification inserted or removed a known infection vector. It is important to note that the detected clusters do not discriminate between removed and inserted nodes but treat them equally because we do not leverage the notion of removal or insertion in the clustering process.