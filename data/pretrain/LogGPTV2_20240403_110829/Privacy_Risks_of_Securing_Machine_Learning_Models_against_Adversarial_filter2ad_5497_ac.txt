For the model architecture, we use a convolutional neural net-
work (CNN) with the convolution kernel size 3 × 3, as suggested
by Simonyan et al. [49]. The CNN model contains 4 blocks with
different numbers of output channels (8, 16, 32, 64), and each block
contains two convolution layers. The first layer uses a stride of
1 for convolutions, and the second layer uses a stride of 2. There
are two fully connected layers after the convolutional layers, each
containing 200 and 38 neurons. When training the robust models,
we set the l∞ perturbation budget (ϵ) to be 8/255.
Fashion-MNIST. This dataset consists of a training set of 60,000
examples and a test set of 10,000 examples [63]. Each example is
a 28 × 28 grayscale image, associated with a class label from 10
fashion products, such as shirt, coat, sneaker.
Similar to Yale Face, we also adopt a CNN architecture with the
convolution kernel size 3 × 3. The model contains 2 blocks with
output channel numbers (256, 512), and each block contains three
convolution layers. The first two layers both use a stride of 1, while
the last layer uses a stride of 2. Two fully connected layers are added
at the end, with 200 and 10 neurons, respectively. When training
the robust models, we set the l∞ perturbation budget (ϵ) to be 0.1.
CIFAR10. This dataset is composed of 32 × 32 color images in
10 classes, with 6,000 images per class. In total, there are 50,000
training images and 10,000 test images.
We use the wide ResNet architecture [65] to train a CIFAR10
classifier, following Madry et al. [33]. It contains 3 groups of residual
layers with output channel numbers (160, 320, 640) and 5 residual
units for each group. One fully connected layer with 10 neurons is
added at the end. When training the robust models, we set the l∞
perturbation budget (ϵ) to be 8/255.
5 MEMBERSHIP INFERENCE ATTACKS
AGAINST EMPIRICALLY ROBUST MODELS
In this section we discuss membership inference attacks against 3
empirical defense methods: PGD-based adversarial training (PGD-
Based Adv-Train) [33], distributional adversarial training (Dist-
Based Adv-Train) [50], and difference-based adversarial training
(Diff-Based Adv-Train) [66]. We train the robust models against
the l∞ adversarial constraint on the Yale Face dataset, the Fashion-
MNIST dataset, and the CIFAR10 dataset, with neural network
architecture as described in Section 4. Following previous work [4,
33, 34, 61], the perturbation budget ϵ values are set to be 8/255, 0.1,
and 8/255 on three datasets, respectively. For the empirically robust
model, as explained in Section 2.1, there is no verification process
to obtain robustness guarantee. Thus the membership inference
strategy IV does not apply here.
We first present an overall analysis that compares membership
inference accuracy for natural models and robust models using
multiple inference strategies across multiple datasets. We then
present a deeper analysis of membership inference attacks against
the PGD-based adversarial training defense.
5.1 Overall Results
Table 2: Membership inference attacks against natural and
empirically robust models [33, 50, 66] on the Yale Face
dataset with a l∞ perturbation constraint ϵ = 8/255. Based on
Equation (16), the natural model has an inference advantage
of 11.70%, while the robust model has an inference advantage
up to 37.66%.
Training
method
Natural
train
acc
100%
test
acc
98.25%
adv-train
acc
4.53%
adv-test
acc
2.92%
inference
acc (IB)
55.85%
inference
acc (IA)
54.27%
PGD-Based
Adv-Train [33]
Dist-Based
Adv-Train [50]
Diff-Based
Adv-Train [66]
99.89%
96.69%
99.00%
77.63%
61.69%
99.58%
93.77%
83.26%
55.06%
62.23%
99.53%
93.77%
99.42%
83.85%
58.06%
68.83%
64.07%
65.59%
Table 3: Membership inference attacks against natural and
empirically robust models [33, 50, 66] on the Fashion-MNIST
dataset with a l∞ perturbation constraint ϵ = 0.1. Based on
Equation (16), the natural model has an inference advantage
of 14.24%, while the robust model has an inference advantage
up to 28.98%.
Training
method
Natural
train
acc
100%
test
acc
92.18%
adv-train
acc
4.35%
adv-test
acc
4.14%
inference
acc (IB)
57.12%
inference
acc (IA)
50.95%
PGD-Based
Adv-Train [33]
Dist-Based
Adv-Train [50]
Diff-Based
Adv-Train [66]
99.93%
90.88%
96.91%
68.06%
58.32%
97.98%
90.62%
67.63%
51.61%
57.35%
99.35%
90.92%
90.13%
72.40%
57.02%
64.49%
59.49%
58.83%
The membership inference attack results against natural models
and empirically robust models [33, 50, 66] are presented in Table 2,
Table 3 and Table 4, where “acc” stands for accuracy, while “adv-
train acc” and “adv-test acc” report adversarial accuracy under PGD
attacks as shown in Equation (9).
According to these results, all three empirical defense meth-
ods will make the model more susceptible to membership
inference attacks: compared with natural models, robust models
increase the membership inference advantage by up to 3.2×, 2×,
and 3.5×, for Yale Face, Fashion-MNIST, and CIFAR10, respectively.
We also find that for robust models, membership inference
attacks based on adversarial example’s prediction confidence
Table 4: Membership inference attacks against natural and
empirically robust models [33, 50, 66] on the CIFAR10
dataset with a l∞ perturbation constraint ϵ = 8/255. Based on
Equation (16), the natural model has an inference advantage
of 14.86%, while the robust model has an inference advantage
up to 51.34%.
Training
method
Natural
train
acc
100%
test
acc
95.01%
adv-train
acc
0%
adv-test
acc
0%
inference
acc (IB)
57.43%
inference
acc (IA)
50.86%
PGD-Based
Adv-Train [33]
Dist-Based
Adv-Train [50]
Diff-Based
Adv-Train [66]
99.99%
87.25%
96.08%
46.61%
74.89%
100%
90.10%
40.56%
25.92%
67.16%
99.50%
87.99%
76.06%
46.50%
61.18%
75.67%
64.24%
67.08%
(IA) have higher inference accuracy than the inference at-
tacks based on benign example’s prediction confidence (IB)
in most cases. On the other hand, for natural models, in-
ference attacks based on benign examples’ prediction confi-
dence lead to higher inference accuracy values. This happens
because our inference strategies rely on the difference between
confidence distribution of training points and that of test points.
For robust models, most of training points are (empirically) secure
against adversarial examples, and adversarial perturbations do not
significantly decrease the confidence on them. However, the test set
contains more insecure points, and thus adversarial perturbations
will enlarge the gap between confidence distributions of training
examples and test examples, leading to a higher inference accuracy.
For natural models, the use of adversarial examples will decrease
the confidence distribution gap, since almost all training points and
test points are not secure with adversarial perturbations. The only
exception is Dist-Based Adv-Train CIFAR10 classifier, where infer-
ence accuracy with strategy IB is higher, which can be explained by
the poor robustness performance of the model: around 60% training
examples are insecure. Thus, adversarial perturbations will decrease
the confidence distribution gap between training examples and test
examples in this specific scenario.
5.2 Detailed Membership Inference Analysis of
PGD-Based Adversarial Training
In this part, we perform a detailed analysis of membership inference
attacks against PGD-based adversarial training defense method [33]
by using the CIFAR10 classifier as an example. We first perform
a sensitivity analysis on both natural and robust models to show
that the robust model is more sensitive with regard to training
data compared to the natural model. We then investigate the re-
lation between privacy leakage and model properties, including
robustness generalization, adversarial perturbation constraint and
model capacity. We finally show that the predictions of targeted ad-
versarial examples can further enhance the membership inference
advantage.
Sensitivity Analysis. In the sensitivity analysis, we remove
5.2.1
sample CIFAR10 training points from the training set, perform
Figure 2: Sensitivity analysis of both robust [33] and natu-
ral CIFAR10 classifiers. x-axis denotes the excluded training
point id number (sorted by sensitivity) during the retrain-
ing process, and y-axis denotes the difference in prediction
confidence between the original model and the retrained
model (measuring model sensitivity). The robust model is
more sensitive to the training data compared to the natural
model.
retraining of the models, and compute the performance difference
between the original model and retrained model.
We excluded 10 training points (one for each class label) and
retrained the model. We computed the sensitivity of each excluded
point as the difference between its prediction confidence in the
retrained model and the original model. We obtained the sensitivity
metric for 60 training points by retraining the classifier 6 times.
Figure 2 depicts the sensitivity values for the 60 training points
(in ascending order) for both robust and natural models. We can
see that compared to the natural model, the robust model is in-
deed more sensitive to the training data, thus leaking more
membership information.
5.2.2 Privacy risk with robustness generalization. We perform the
following experiment to demonstrate the relation between privacy
risk and robustness generalization. Recall that in the approach of
Madry et al. [33], adversarial examples are generated from all train-
ing points during the robust training process. In our experiment,
we modify the above defense approach to (1) leverage adversarial
examples from a subset of the CIFAR10 training data to compute
the robust prediction loss, and (2) leverage the remaining subset of
training points as benign inputs to compute the natural prediction
loss.
The membership inference attack results are summarized in
Table 5, where the first column lists the ratio of training points
used for computing robust loss. We can see that as more training
points are used for computing the robust loss, the member-
ship inference accuracy increases, due to the larger gap between
adv-train accuracy and adv-test accuracy.
5.2.3 Privacy risk with model perturbation budget. Next, we explore
the relationship between membership inference and the adversarial
perturbation budget ϵ, which controls the maximum absolute value
of adversarial perturbations during robust training process.
Table 5: Mixed PGD-based adversarial training experiments
[33] on CIFAR10 dataset with a l∞ perturbation constraint
ϵ = 8/255. During the training process, part of the training
set, whose ratio is denoted by adv-train ratio, is used to com-
pute robust loss, and the remaining part of the training set
is used to compute natural loss.
Adv-train
ratio
0
1/2
3/4
1
train
acc
100%
test
acc
95.01%
adv-train
acc
0%
adv-test
acc
0%
100%
87.78%
75.85%
43.23%
inference
acc (IB)
57.43%
67.20%
100%
86.68%
88.34%
45.66%
71.07%
99.99%
87.25%
96.08%
46.61%
74.89%
inference
acc (IA)
50.85%
66.36%
72.22%
75.67%
Table 6: Membership inference attacks against robust CI-
FAR10 classifiers [33] with varying adversarial perturbation
budgets.
Perturbation
budget (ϵ)
2/255
4/255
8/255
train
acc
100%
test
acc
adv-train
acc
93.74%
99.99%
adv-test
acc
82.20%
inference
acc (IB)
64.48%
100%
91.19%
99.89%
70.03%
69.44%
99.99%
87.25%
96.08%
46.61%
74.89%
inference
acc (IA)
66.54%