PEREA 
(SP)
Figure 5. Non-membership proof comparison for large blacklists: This ﬁgure compares the performance of our proposed non-membership proof
against two existing approaches in the literature: BLAC [32]–[34] and PEREA [33]. We took the timing measurements for these two schemes directly
from their original papers, while we have measured the timing information for our approach empirically; hence, the coefﬁcients on each line are only an
approximation to the schemes’ actual performance. Nonetheless, the graphs do capture the impact of the asymptotic behaviour of each approach. We omit
error bars from these graphs for two reasons: 1) the error range for our approach is too small for them to be visible, and 2) the measurements for BLAC
and PEREA are computed from data in their respective papers instead of being measured experimentally.
534
prevent exit node operators from using their services even
when when they do not route their connections through Tor.
Second, users located behind a ﬁrewall that censors access
to Tor and the VIs are unable to obtain a verinym and are
therefore unable to use the system. This section presents
solutions to each of these issues.
A. Verinym acquisition for Tor exit relays
SPs cannot distinguish connections originating at a Tor
exit relay from connections made over Tor and routed
through that exit relay. One consequence of this is that
SPs that block access from Tor also block access from Tor
exit relay operators, even when their connections are not
coming through Tor. Previous Nymble-like systems provided
a viable solution to the availability problem for Tor’s regular
users, but neglected to show how their systems could support
operators of Tor exit relays.
Fortunately, Tor implements a public key infrastructure
(PKI) among its relays. In particular, each Tor relay has
a long-term public signing key called an identity key [11].
Thus, the VIs can demand a ZKP of knowledge of the secret
portion of an exit relay’s identity key at the start of the
Verinym Acquisition Protocol. In this way, the VIs prevent
U from obtaining nymbles under the guise of an exit relay,
while permitting the operator of that exit relay to obtain
nymbles for his own use.
Suppose E is an exit node operator who wishes to connect
to an SP using a Nymble-like system for anonymous au-
thentication. We outline the additional proof required from
E below. Recall that S is a set of t VIs and z is E’s IP
address.
E does the following:
1. E connects directly to each VIij ∈ S.
VIij does the following:
2. VIij checks z against the directory list of Tor exit
relays.
3. If z not on the list, VIij proceeds as usual for
the Verinym Acquisition Protocol; otherwise, VIij
chooses a random challenge c and sends it to E.
E does the following:
4. E receives the challenge c and prepares the standard
5. E computes a signature ψR on (c(cid:16)R) using his private
request R for a verinym.
identity key.
6. E transmits the tuple (R, ψR) to VIij .
VIij does the following:
7. VIij receives the ψR and veriﬁes the signature. If ψR
is incorrect, VIij aborts; otherwise, VIij proceeds as
usual for the Verinym Acquisition Protocol.
B. Verinym acquisition for censored users
Access to Tor is restricted in several countries due to
government censorship (for example, the ‘Great Firewall
of China’). To solve this problem, the Tor network uses
bridges [10]. A bridge is essentially just a regular Tor relay
535
that the directory does not list. Using a variety of different
techniques, censored users can obtain portions of the list of
bridge relays, and thereby ﬁnd an entry point into the Tor
network. Obtaining the entire list, however, is intentionally
a very difﬁcult task. The goal is to make it infeasible for an
adversary to block all of the bridge relays.
This solves the availability problem for Tor; i.e., censored
users can still access the Tor network by using bridges.
However, it seems prudent to expect that when the entire Tor
network is blocked, then so too will be the VIs. This will
prevent censored users from obtaining a verinym in the usual
way. What we need, it appears, is a Nymble-like system
analog of bridges.
In particular, we envision a set of simple volunteer-run
entities, which we call Identity Veriﬁers (IVs). The IVs
are simple servers (perhaps an Apache module running
on volunteer machines) distributed throughout the Internet.
Each IV possesses a public-private key pair for signature
generation. The list of IP addresses and public keys for all
available IVs is known to the VIs. Ideally, no single VI will
possess the entire list, lest that VI be compromised; instead,
each VI may have approximately (1/s)
th of the list.
It should be difﬁcult for an attacker to gain access to large
portions of the IV list. In fact, bridge relays could double
as IVs, making the problem of obtaining the lists of bridges
and IVs equivalent. Alternatively, the list of bridge relays
and IVs could be linked in such a way as to make the task of
obtaining large portions of each list equivalent. However, we
leave further development of these considerations to future
work.
The IVs offer the following functionality: upon receiving
a challenge bit string c from a user U with IP address z, an
IV responds with a signature on hash(c(cid:16)z). The additional
part of the protocol works as follows:
U does the following:
1. U connects to an arbitrary bridge relay B and builds
a circuit and SSL connection to VIij through B; U
sends her claimed IP address z to VIij through this
connection.
VIij does the following:
2. VIij receives z from U and replies with a random
challenge c and the IP address of an IV selected
by VIij . (The method of selection can be arbitrary:
random, the IV most trusted by the VI, etc.)
U does the following:
3. U receives the challenge c and IP address of an IV
from VIij ; she connects to the IV and sends c.
The IV does the following:
4. The IV receives c and determines z empirically from
the IP connection header. It replies by sending ψz,
which is a signature on hash(c(cid:16)z).
U does the following:
5. U receives ψz from the IV and forwards it to VIij .
VIij does the following:
6. VIij receives ψz and checks the signature. If ψz is
incorrect, VIij aborts; otherwise, VIij proceeds as
usual for the Verinym Acquisition Protocol.
The naive protocol just described is susceptible to the
following attack: a malicious U chooses a random IP address
and initiates the protocol with that as its self-reported
address. In the (unlikely) event that U receives the address
of a colluding IV, she obtains a signature on the fake IP
address, thereby convincing the VI to issue a share of a
verinym. Otherwise, U chooses a new random IP address
and tries again. To protect against this attack, we can require
that: a) the VIs somehow trust the IVs, b) the VIs choose
multiple IVs and require U to obtain a signature from each,
or c) a combination of these approaches.
V. OBJECTIVE BLACKLISTING
Schwartz et al. proposed contract-based revocation [18]
in their Contractual Anonymity papers [28]–[30], whereby
U enters into an anonymity contract with the SP. This
contract assures U of her anonymity as long as she does
not violate the contract; if, on the other hand, she violates
the contract, then a Group Manager (GM) can revoke her
anonymity. Schwartz et al. use ideas from trusted computing
to construct a contract-based revocation system based on
group signatures. In their scheme, U uses remote attestation
to verify that the software running on the GM will only
deanonymize her if she does indeed violate the contract.
In [22], Lin and Hopper describe an objective blacklisting
extension for Jack. Their approach uses the label ﬁeld in
Camenisch and Shoup’s veriﬁable encryption scheme [7] to
force the PE to include a contract in its trapdoor computa-
tion. The idea here is that if the provided contract is incor-
rect, then the trapdoor computation will fail. It is reasonable
to argue that any added security offered by this approach
is illusional (see the discussion regarding additional trust
assumptions below); nonetheless, one can easily incorporate
a similar mechanism into the nymble constructions of other
Nymble-like systems as outlined below. Because different
Nymble-like systems do not necessarily share a common
trapdoor function, we propose to use a hash c of the
contract as an input parameter to the one-way function used
to compute the subsequent nymbles in a sequence. When
incorporated into Nymble and Nymbler, this idea works as
follows:
• Nymble: use c as the HMAC key for the ‘top’ chain
(see Figure 1).
• Nymbler: replace Rabin’s function f (z) = z2 mod n
with f
(cid:4)
(z, c) = c · z2 mod n.
given contract is enforced on U. This means, for example,
that with our approach different users may have different
rights in their contracts, without partitioning the anonymity
set.
As in [22] (though the authors of [22] never explicitly
stated it), this solution requires the following additional trust
assumptions:
• U must trust the PE to verify that she did indeed violate
• The PE must trust the SP to not forge proofs of contract
the contract.
violations.10
• The SP must trust the PE not to divulge any potentially
sensitive information it witnesses while verifying that
misbehaviour has occurred.
To construct an objective blacklisting solution that does not
require additional trust assumptions or reliance on trusted
computing remains an interesting open problem.
VI. CONCLUSION
We have presented several extensions to the Nymble
framework. In particular, we proposed a new threshold
Verinym Issuer construction, an efﬁcient way to achieve
inter-window revocation and blacklist transferability, alter-
native verinym acquisition techniques for Tor exit relays
and censored users, and contract-based revocation. These
extensions improve the liveness, security and functionality
of Nymble-like schemes built from the extended framework,
and solve a number of open problems identiﬁed in the
future work sections of papers on particular Nymble-like
systems [19]–[22]. Nonetheless, there are still several open
problems identiﬁed in those papers that constitute exciting
directions for future research. For example, system-wide
banning of cheaters [20], NAT-aware IP blocking [20], and
banning of entire subnets without reducing privacy [20],
[21]; please see the respective papers for more details
on these problems. Moreover, another worthwhile direction
for future work is to investigate the use of other unique
identiﬁers to use in place of IP addresses; such work would
likely have broader implications with respect to protecting
against Sybil attacks [12]. Finally, as stated in §V, the design
of an objective blacklisting mechanisms that does not require
trusted hardware remains an open problem.
Acknowledgements: We thank Aniket Kate and the anony-
mous reviewers for their helpful comments. This work was
supported by NSERC, MITACS, and a David R. Cheriton
Graduate Scholarship.
REFERENCES
In order to have U blacklisted, the SP transmits her nymble,
a copy of the contract, and proof of her misbehaviour to
the PE. The PE veriﬁes that the behaviour does indeed
violate the contract before computing the remainder of U’s
nymbles (the computation of which requires c). If the SP
then provides U’s nymble to the PE with an incorrect
contract, then any nymbles output by the PE will not be
linkable back to U. Note that, unlike in [22], this approach
does not leak information to the PE or SP about whether the
[1] M. Bellare, J. A. Garay, and T. Rabin, “Fast Batch Veriﬁ-
cation for Modular Exponentiation and Digital Signatures,”
10One possible way to eliminate this additional trust is to have U sign
her actions using her pseudonym as a key. Then, upon extracting U’s
pseudonym from her nymble, the PE would be able to verify the signature
before adding U to the blacklist. Forged evidence of a violation would result
in failed signature veriﬁcation. However, this solution would require U to
prove in zero-knowledge (i.e., without revealing the key) that the signature
was computed correctly each time an action is performed, thus violating
the user- and veriﬁer-efﬁciency requirements of the scheme.
536
in Proceedings of EUROCRYPT 1998, Espoo, Finland, May
1998.
[2] S. Brands, “Restrictive Blinding of Secret-Key Certiﬁcates,”
in Proceedings of EUROCRYPT 1995, Saint-Malo, France,
May 1995.
[3] S. A. Brands, Rethinking Public Key Infrastructures and
Digital Certiﬁcates: Building in Privacy. MIT Press, 2000.
[4] S. A. Brands, L. Demuynck, and B. D. Decker, “A Practical
System for Globally Revoking the Unlinkable Pseudonyms
of Unknown Users.” Department of Computer Science,
K.U.Leuven, Technical Report CW472, 2006.
[5] S. A. Brands, L. Demuynck, and B. D. Decker, “A Practical
System for Globally Revoking the Unlinkable Pseudonyms of
Unknown Users,” in Proceedings of ACISP 2007, Townsville,
Australia, July 2007.
[6] E. Brickell and J. Li, “Enhanced Privacy ID: A Direct
Anonymous Attestation Scheme with Enhanced Revocation
Capabilities,” in Proceedings of WPES 2007, Alexandria, VA,
October 2007.
[7] J. Camenisch and V. Shoup, “Practical Veriﬁable Encryption
and Decryption of Discrete Logarithms,” in Proceedings of
CRYPTO 2003, Santa Barbara, CA, August 2003.
[8] J. Camenisch and M. Stadler, “Efﬁcient Group Signature
Schemes for Large Groups (Extended Abstract),” in Proceed-
ings of CRYPTO 1997, Santa Barbara, CA, August 1997.
[9] I. Damg˚ard and M. Koprowski, “Practical Threshold RSA
Signatures without a Trusted Dealer,” in Proceedings of
EUROCRYPT 2001, Innsbruck, Austria, May 2001.
[10] R. Dingledine and N. Mathewson, “Design of a Blocking-
Resistant Anonymity System.” The Tor Project, Technical
Report, 2006.
[11] R. Dingledine, N. Mathewson, and P. F. Syverson, “Tor:
The Second-Generation Onion Router,” in Proceedings of
USENIX Security 2004, San Diego, CA, August 2004.
[12] J. R. Douceur, “The Sybil Attack,” in Proceedings of IPTPS
2002, Cambridge, MA, March 2002.
[13] P. Feldman, “A Practical Scheme for Non-interactive Veri-
ﬁable Secret Sharing,” in Proceedings of FOCS 1987, Los
Angeles, CA, October 1987.
[14] P.-A. Fouque and J. Stern, “Fully Distributed Threshold RSA
under Standard Assumptions,” in Proceedings of ASIACRYPT
2001, Gold Coast, Australia, December 2001.
[15] Y. Frankel, P. D. MacKenzie, and M. Yung, “Robust Efﬁcient
Distributed RSA-Key Generation,” in Proceedings of STOC
1998, Dallas, TX, May 1998.
[16] I. Goldberg, “A Pseudonymous Communications Infrastruc-
ture for the Internet,” Ph.D. dissertation, UC Berkeley, 2000.
[17] S. Goldwasser, S. Micali, and R. L. Rivest, “A Digital
Signature Scheme Secure Against Adaptive Chosen-Message
Attacks,” SIAM Journal on Computing (SICOMP), vol. 17,
no. 2, pp. 281–308, April 1988.
[18] R. Henry and I. Goldberg, “Formalizing Anonymous Black-
listing Systems,” in Proceedings of IEEE S&P 2011, Oakland,
CA, May 2011.
[19] R. Henry, K. Henry, and I. Goldberg, “Making a Nymbler
Nymble using VERBS,” in Proceedings of PETS 2010, Berlin,
Germany, July 2010.
[20] R. Henry, K. Henry, and I. Goldberg, “Making a Nymbler
Nymble using VERBS (Extended Version).” Centre for Ap-
plied Cryptographic Research, UWaterloo, Technical Report
CACR 2010-05, 2010.
[21] P. C. Johnson, A. Kapadia, P. P. Tsang, and S. W. Smith,
“Nymble: Anonymous IP-Address Blocking,” in Proceedings
of PETS 2007, Ottawa, ON, June 2007.
[22] Z. Lin and N. Hopper, “Jack: Scalable Accumulator-based
Nymble System,” in Proceedings of WPES 2010, Chicago,
IL, October 2010.
[23] P. Lofgren and N. Hopper, “BNymble (A Short Paper): More
Anonymous Blacklisting at Almost No Cost,” in Proceedings
of FC 2011, St. Lucia, February 2011.
[24] A. Lysyanskaya, “Signature Schemes and Applications to
Cryptographic Protocols,” Ph.D. dissertation, Department of
Electrical Engineering and Computer Science, MIT, 2002.
[25] A. Menezes, P. C. van Oorschot, and S. A. Vanstone, Hand-
CRC Press, 1996, ﬁfth
book of Applied Cryptography.
Printing (August 2001).
[26] T. P. Pedersen, “Non-Interactive and Information-Theoretic
Secure Veriﬁable Secret Sharing,” in Proceedings of CRYPTO
1991, Santa Barbara, CA, August 1991.
[27] K. Peng, C. Boyd, and E. Dawson, “Batch Zero-Knowledge
Proof and Veriﬁcation and Its Applications,” ACM Transac-
tions on Information and System Security (TISSEC), vol. 10,
no. 2, May 2007, Article No. 39.
[28] E. J. Schwartz, “Contractual Anonymity,” Master’s thesis,
Information Networking Institute, Carnegie Mellon, 2009.
[29] E. J. Schwartz, D. Brumley, and J. M. McCune, “Contractual
Anonymity.” School of Computer Science, Carnegie Melon,
Technical Report CMU-CS-09-144, 2009.
[30] E. J. Schwartz, D. Brumley, and J. M. McCune, “A Contrac-
tual Anonymity System,” in Proceedings of NDSS 2010, San
Diego, CA, February 2010.
[31] V. Shoup, “Practical Threshold Signatures,” in Proceedings
of EUROCRYPT 2000, Bruges, Belgium, May 2000.
[32] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith,
“Blacklistable Anonymous Credentials: Blocking Misbehav-
ing Users Without TTPs,” in Proceedings of CCS 2007,
Alexandria, VA, October 2007.
[33] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith, “PEREA:
Towards Practical TTP-free Revocation in Anonymous Au-
thentication,” in Proceedings of CCS 2008, Alexandria, VA,
October 2008.
[34] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith, “BLAC:
Revoking Repeatedly Misbehaving Anonymous Users with-
out Relying on TTPs,” ACM Transactions on Information
and System Security (TISSEC), vol. 13, no. 4, October 2010,
Article No. 39.
[35] P. P. Tsang, A. Kapadia, and S. W. Smith, “Anonymous
IP-address Blocking in Tor with Trusted Computing (Short
Paper: Work in Progress),” in Proceedings of WATC 2006
(Fall), Tokyo, Japan, November 2006.
[36] Wikimedia Foundation,
“Wikipedia:Blocking policy —
the free encyclopedia.” [Online]. Available:
Wikipedia,
http://en.wikipedia.org/wiki/Wikipedia:Blocking policy
537