locking  overhead.  The 
focuses  on 
processing the tuples entering the operator through the input 
stream. The last tuple that entered the operator may be either 
(1)  opening  a  new  window;  (2)  contributing  to  an  existing 
window; (3) contributing to an existing window and causing 
its  closure.  In  case  (1)  we  increase  the  counter  of  active 
windows  N,  set  τ1  and  τ2 to  tnow  and  produce  a  Gopen  tuple 
into the output stream. The Gopen tuple carries the number N, 
the  current  state  of  the  window,  and  the  timestamp  tnow.  In 
case  (2),  the  tuple  updates  the  state  of  a  window;  τ2 is  not 
updated  until  the  first  Gcheck  emitted  for  that  window.  The 
number  of  active  windows  remains  unchanged.  In  case  (3) 
the  counter  of  active  windows  is  decreased,  an  R  tuple  is 
In 
the  τ2 
produced,  and  the  just-closed  window  is  erased  from  the 
ordered list of open windows of point (c) above. 
timestamp  for 
the  background  path 
the  operator  periodically 
checkpoints  slow  windows  via  the  production  of  Gcheck 
tuples. The Gcheck tuples are marked with the tnow timestamp 
of  the  tuple  entering  the  operator  at  the  time  of  Gcheck 
the 
production.  Simultaneously 
corresponding  window  changes  to  tnow.  The  number  of 
windows eligible for checkpointing at any time (we call this 
the intensity of checkpointing) is decided based on recovery-
time objectives expressed by two parameters Q, U described 
in  Section  III.A.  To  avoid  the  impact  of  uncontrolled 
checkpointing on response time we limit checkpointing only 
to specific time intervals. We use two parameters to denote 
the amount of time we devote to checkpointing (checkpoint 
interval,  CI)  and  the  time  between  checkpointing  intervals 
(checkpoint  period,  CP).  The  combination  of  Q,  U  and 
CI/CP can be used to adjust to a desirable performance vs. 
recovery  speed  operating  point  as  we  demonstrate  in  our 
evaluation section. Note that the choice of these parameters 
does  not  affect  correctness  because  even  if  we  delay 
checkpointing, a window will roll-back to an older Gcheck for 
that  window  or  –in  the  worst  case—  to  the  Gopen  tuple  for 
that window. at the expense of longer recovery time. 
in  Section 
During the recovery path, CEC reconstructs the eventual 
checkpoint by sequentially rolling back on the output queue 
log  as  described 
III.B.  Following  EC 
reconstruction,  the  EC  must  be  loaded onto the operator  to 
form its new state prior to asking upstream nodes for tuple 
replay.  To  load  the  EC  onto  the  operator  in  an  as  simple 
manner  as  possible  we  created  a  special  input  stream  (in 
addition to the standard input stream of the operator) through 
which  we  feed  the  EC  into  the  operator.  We  call  this  new 
stream  the  “EC-load”  stream.  A  complication  we  had  to 
address  with  “EC-load”  is  that  although  it  is  created  as  an 
output stream (following the output tuple schema) it must be 
fed  to  the  operator  through  an  input  stream.  We  further 
modified the operator threads to be aware that tuples coming 
from “EC-load” can only be part of the EC for the purpose of 
recovery.  The  implementation  of  the  recovery  code  uses 
support  provided  by  our  persistence  architecture  described 
briefly  in  the  Appendix  and  in  [20].  The  communication 
protocol  between  a  recovering  node  and  its  upstream  and 
downstream nodes (synchronization with upstream to request 
replay 
synchronization  with 
downstream  to  find  out  what  it  wants  to  have  replayed) 
builds  on  the  RPC  message  exchange  framework  provided 
by Borealis. 
A.  Policies for producing Gcheck tuples 
from  given 
timestamp, 
Recall  that  CEC  maintains  an  ordered  list  of  open 
windows by their τ2 time of last checkpoint (Gnew or Gcheck), 
keeping  the  window  with  the  oldest  checkpoint  at  the  top. 
This checkpoint is by definition at the end of the extent and 
thus the corresponding window (wk in the example of Figure 
3)  is  the  prime  candidate  to  produce  a  new  checkpoint  for 
(thus  decreasing  the  size  of  the  extent).  To  maintain  an 
accurate estimate of the current size of the EC extent as well 
the  amount  of  tuples  that  need  to  replayed  by  upstream 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
149B.  Implementation complexity 
The overall modifications to Borealis to support CEC are 
about  700  lines  of  code  in  our  persistence  and  recovery 
mechanisms  and  about  30  lines  of  new  code  in  the 
implementation  of  the  aggregate  operator.  We  have  also 
added  minor  modifications 
the  Borealis  SPE  and 
consumer  application  code  to  drop  Gopen  and  Gcheck  tuples 
upon reception. We disable this feature in our experiments in 
order to be able to measure the number of checkpoint tuples 
produced  as  well  as  the  aggregate  throughput  (control  plus 
data) observed by the final receiver. 
to 
IV.  EVALUATION 
Our  experimental  evaluation  of  CEC  focuses  on  three 
key areas: (1) The impact of CEC on streaming performance 
when  operating  under  minimum  recovery-time  guarantees 
(henceforth  referred  to  as  baseline  performance);  (2)  the 
impact of a range of Q, U values to operator recovery time; 
and  (3)  the  response  time  vs.  recovery  time  tradeoff  with 
varying  CI/CP.  Our  experimental  setup  consists  of  three 
servers as shown in Figure 4. All servers are quad-core Intel 
Xeon  X3220  machines  with  8GB of  RAM  connected  via a 
1Gb/s Ethernet switch. 
Figure 4. Experimental setup. 
The  first  server  hosts  the  tuple-producing  engine  (or 
source). The tuples produced consist of three fields: item_id, 
item_price,  and  item_time,  where  item_id  is  an  integer 
identifying the item (e.g., an SKU), item_price is an integer 
indicating the item’s price, and item_time represents the time 
of  purchase  of  the  item.  The  tuple  size  is  100  bytes.  The 
second  server  hosts  an  aggregate  operator  computing  the 
average purchase price of items grouped by item_id (in other 
words  the  operator  will  maintain  a  separate  window  per 
item_id computing the average over a number of tuples equal 
Figure 3. Extent size (q), number of upstream tuples to replay
for given extent (u), and corresponding targets (Q, U). 
operators during recovery, we maintain the following state: 
For each window wi in the open-window list we store (a) the 
number of tuples emitted by the operator between wi and the 
previously  checkpointed  window;  and  (b)  the  number  of 
input  tuples  processed  by  the  operator  between  producing 
checkpoints  for  these  windows.  Effectively,  (a)  provides  a 
measure of the size of the extent, which  we call q, and (b) 
provides  a  measure  of  the  number  of  tuples  u  we  need  to 
replay where a crash to occur at this point in time. We have 
devised  two  methods  to  determine  when  it  makes  sense  to 
produce a checkpoint for the oldest window: a method based 
on  a  cost-benefit  analysis  and  another  based  on  explicitly 
setting targets for q, u. 
The  first  method  considers  the  costs  and  benefits  of 
checkpointing:  First,  taking  a  Gcheck  has  the  benefits  of 
reducing  the  size  of  the  extent  (and  thus  the  cost  of 
eventually  constructing  the  EC);  second,  a  Gcheck  brings 
forward  the  timestamp  of  the  first  tuple  to  replay,  thus 
reducing the cost of replay. On the other hand, taking a Gcheck 
has  two  costs:  First,  it  adds  another  tuple  to  the  extent, 
increasing  its  size  by  one;  second,  it  incurs  the  overhead 
(CPU and I/O) of producing the Gcheck. Based on the above it 
is  reasonable  to  only  perform  a  Gcheck  when  the  benefits 
outweigh  the  costs.  It  is  straightforward  to  derive  an 
analytical cost-benefit formula based on the above principles 
but one needs to calibrate it for a given platform by including 
a  number  of  empirically-measured  parameters.  The  full 
implementation and evaluation of this method is an area of 
ongoing work. 
The  second  method  (which  is  used  in  our  experiments) 
takes 
the  approach  of  explicitly  setting  appropriate 
empirically-derived  targets  for  q  and  u.  For  example  the 
policy “produce a Gcheck if q > Q or u > U or both, where Q = 
1,000  and  U  =  1,000,000”  means  that  if  the  extent  or  the 
number of tuples to replay exceeds Q, U respectively, then 
try to decrease them by taking checkpoints starting from the 
older end of the extent (wk in Figure 3). Notice that values of 
Q should typically be smaller than values of U reflecting the 
fact that a tuple carries a heavier weight when considered in 
the context of EC reconstruction than in stream replay (to say 
it  simply,  a  tuple  costs  much  more  to  process  in  EC 
reconstruction than in stream replay). 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
150R  tuples  translate  into  lower  R-tuple  rate  at  the  sink. 
Performance  of  the  Gopen/Disk  configuration  is  nearly 
identical  to  Gopen/Net,  indicating  that  the  I/O  path  has 
minimal  impact  on  throughput,  especially  on  lower  output 
rates. The Real bar shows that the actual throughput seen by 
the receiver is twice the throughput of data tuples alone. 
In terms of CPU usage, native Borealis consumes about 
150%  (fast-windows)  and  130%  (slow-windows)  out  of  a 
total  capacity  of  400%.  With  CEC  (Gopen  tuples  only)  the 
CPU  utilization  remains  the  same  for  the  Gopen/Net 
configuration and increases in the Gopen/Disk setup to 200% 
for  the  fast-windows  workload.  This  increase  can  be 
attributed to the overhead of the I/O path. Throughout these 
experiments we did not saturate the CPU or the network at 
the  server  hosting  the  aggregate  operator.  The  main  factor 
limiting performance is the degree of parallelism available at 
the tuple source (Borealis does not allow us to drive a single 
aggregate operator by more than one source instances). 
B.  CEC recovery time 
Next we focus on CEC recovery time using a range of Q 
and  U  values.  Recall  that  Q  represents  the  extent  size  to 
exceed before checkpointing the oldest window, whereas U 
represents  the  number  of  upstream  tuples  (to  replay)  to 
exceed before checkpointing the oldest window. In all cases 
the  source  produces  tuples  randomly  distributed  across 
100,000  item_ids.  The  aggregate  operator  uses  a  window 
size of 10 tuples. To isolate the impact of each of Q, U on 
recovery  time  we  set  up  two  separate  experiments  with  a 
different  parameter  regulating  recovery  time  in  each  case. 
The extent size is measured at a time of crash in the middle 
of  each  run.  Our  operator  graph  in  this  experiment  differs 
from  that  depicted  in  Figure  4  in  that  we  interpose  a  filter 
operator between the source and the aggregate to be able to 
persist  and  replay  the  filter’s  logged  output  tuples  during 
recovery.  The  CI  parameter  is  set  to  5ms  and  the  CP  to 
100ms in all cases. 
Figure  6  depicts  the  effects  of  varying  Q  from  1  to  8 
times  the  number  of  open  windows  (openwins)  on  two 
recovery-time  metrics: eventual  checkpoint  (EC)-load time; 
bytes  replayed  by  upstream  source;  and  two  CEC-internal 
metrics:  extent  size;  and  number  of  Gchecks  emitted.  The 
average value of openwins in the operator is measured to be 
around  90,000.  Our  smallest  Q  value 
(1*openwins) 
represents  the  extent  size  that  the  operator  would  have 
produced  by  just  emitting  Gnew  tuples.  This  is  the  smallest 
possible  extent  for  that  number  of  windows.  Setting  Q  to 
anything  less  than  that  would  be  setting  an  unattainable 
target, resulting in unnecessary overhead. 
Figure 6 (upper left) depicts the number of Gcheck tuples 
produced  as  the  extent  is  allowed  to  grow  larger.  An 
observation  from  this  graph  is  that  the  number  of  Gchecks 
drops sharply for Q > 2*openwins, evidence that the extent 
in  these  cases  nearly  always  stays  below  target.  Figure  6 
(upper  right)  depicts  the  extent  size  with  growing  Q.  CEC 
cannot  achieve  the  target  of  Q  =  1*openwins  due  to  the 
stringency of that goal but manages to achieve it in the Q=2, 
4*openwins  cases.  The  extent  size  for  Q  =  8*openwins 
(500,000) is lower than its targeted goal (720,000) due to the 
Figure 5. CEC baseline impact (Gopen tuples only). 
to  the  window  size).  The  third  server  hosts  the  tuple 
consumer (or sink). 
The  window  size  of  the  aggregate  operator  in  all 
experiments  is defined based on  number of  tuples received 
(count-based  windows).  We  chose  to  evaluate  CEC  with 
count-based windows rather than time-based ones due to the 
special challenges posed by the former. The amount of time 
count-based  windows  remain open  depends  strongly on the 
distribution of input tuples and can be indefinite. Our results 
hold  for  time-based  windows  as  well  as  for  other  stateful 
operators.  Finally,  we  use  a  distributed  file  system  (PVFS 
[18]) to persist the output queue of the aggregate operator. 
A.  CEC baseline impact 
We  first  highlight  the  impact  of  CEC  while  operating 
under minimum recovery-time guarantees. In this case CEC 
produces  Gopen  tuples  (necessary  for  correctness)  but  no 
Gcheck tuples (necessary to reduce the extent). We configure 
the  aggregate  operator  for  two  different  workloads:  fast-
windows  and  slow-windows.  The  fast-windows  workload 
uses  a  window  size  of  1.  Each  input  tuple  entering  the 
operator forces the creation of a new window and its instant 
closure,  emitting  an  R  tuple.  The  slow-windows  workload 
uses  a  window  size  of  1000.  In  both  cases  the  source 