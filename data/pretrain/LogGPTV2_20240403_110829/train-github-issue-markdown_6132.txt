# ðŸš€ Feature request
Output the token probabilities along with the tokens when generating sequence.
## Motivation
For understanding model confidence, this is quite useful.  
Also, for abstractive QA with long contexts, one needs to use doc-strides to
take into account the contexts & then choose the best answer according to the
probability of the generated text.
## Your contribution
I can try submitting a PR for non-beam decoding, but guidance would be
appreciated.  
Also, are there any existing solutions to this issue? If so, what & where?