require manual identiﬁcation of exploitable code and
data in targeted binaries. Similarly, Maurice et al. [33]
built a cache-index-agnostic cross-VM covert channel
based on Prime+Probe.
Simultaneous to our work, Oren et al. [38] devel-
oped a cache attack from within sandboxed JavaScript
to attack user-speciﬁc data like network trafﬁc or mouse
movements. Contrary to existing attack approaches, we
present a general attack framework to exploit cache vul-
nerabilities automatically. We demonstrate the effective-
ness of this approach by inferring keystroke informa-
tion and, for comparison reasons, by attacking a T-table-
based AES implementation.
USENIX Association  
24th USENIX Security Symposium  899
3
3 Cache Template Attacks
Chari et al. [8] presented template attacks as one of
the strongest forms of side-channel attacks. First, side-
channel traces are generated on a device controlled by the
attacker. Based on these traces, the template—an exact
model of signal and noise—is generated. A single side-
channel trace from an identical device with unknown key
is then iteratively classiﬁed using the template to derive
the unknown key.
Similarly, Brumley and Hakala [7] described cache-
timing template attacks to automatically analyze and ex-
ploit cache vulnerabilities. Their attack is based on
Prime+Probe on the L1 cache and, thus, needs to run on
the same core as the spy program. Furthermore, they
describe a proﬁling phase for speciﬁc operations exe-
cuted in the attacked binary, which requires manual work
or even modiﬁcation of the attacked software. In con-
trast, our attack only requires an attacker to know how
to trigger speciﬁc events in order to attack them. Subse-
quently, Brumley and Hakala match these timing tem-
plates against the cache timing observed.
In contrast,
we match memory-access templates against the observed
memory accesses.
Inspired by their work we propose Cache Template At-
tacks. The presented approach of Cache Template At-
tacks allows the exploitation of any cache vulnerability
present in any program on any operating system executed
on architectures with shared inclusive last-level caches
and shared memory enabled. Cache Template Attacks
consist of two phases: 1) a proﬁling phase, and 2) an ex-
ploitation phase. In the proﬁling phase, we compute a
Cache Template matrix containing the cache-hit ratio on
an address given a speciﬁc target event in the binary un-
der attack. The exploitation phase uses this Cache Tem-
plate matrix to infer events from cache hits.
Both phases rely on Flush+Reload and, thus, attack
code and static data within binaries. In both phases the
attacked binary is mapped into read-only shared mem-
ory in the attacker process. By accessing its own vir-
tual addresses in the allocated read-only shared memory
region, the attacker accesses the same physical memory
and the same cache lines (due to the physically-indexed
last level cache) as the process under attack. Therefore,
the attacker completely bypasses address space layout
randomization (ASLR). Also, due to shared memory, the
additional memory consumption caused by the attacker
process is negligible, i.e., in the range of a few megabytes
at most.
In general, both phases are performed online on the
attacked system and,
therefore, cannot be prevented
through differences in binaries due to different versions
or the concept of software diversity [12]. However, if
online proﬁling is not possible, e.g., in case the events
must be triggered by a user or Flush+Reload is not pos-
sible on the attacked system, it can also be performed in a
controlled environment. Below, we describe the proﬁling
phase and the exploitation phase in more detail.
3.1 Proﬁling Phase
The proﬁling phase measures how many cache hits occur
on a speciﬁc address during the execution of a speciﬁc
event, i.e., the cache-hit ratio. The cache-hit ratios for
different events are stored in the Cache Template matrix
which has one column per event and one row per address.
We refer to the column vector for an event as a proﬁle.
Examples of Cache Template matrices can be found in
Section 4 and Section 5.1.
An event in terms of a Cache Template Attack can be
anything that involves code execution or data accesses,
e.g., low-frequency events, such as keystrokes or receiv-
ing an email, or high-frequency events, such as encryp-
tion with one or more key bits set to a speciﬁc value. To
automate the proﬁling phase, it must be possible to trig-
ger the event programmatically, e.g., by calling a func-
tion to simulate a keypress event, or executing a program.
The Cache Template matrix is computed in three steps.
The ﬁrst step is the generation of the cache-hit trace and
the event trace. This is the main computation step of the
Cache Template Attack, where the data for the Template
is measured. In the second step, we extract the cache-hit
ratio for each trace and store it in the Cache Template
matrix. In a third post-processing step, we prune rows
and columns which contain redundant information from
the matrix. Algorithm 1 summarizes the proﬁling phase.
We explain the corresponding steps in detail below.
Algorithm 1: Proﬁling phase.
Input: Set of events E, target program binary B,
duration d
Output: Cache Template matrix T
Map binary B into memory
foreach event e in E do
foreach address a in binary B do
while duration d not passed do
simultaneously
Trigger event e and save event trace g(E)
a,e
Flush+Reload attack on address a
and save cache-hit trace g(H)
a,e
end
Extract cache-hit ratio Ha,e from g(E)
a,e
and g(H)
a,e and store it in T
end
end
Prune Cache Template matrix T
900  24th USENIX Security Symposium 
USENIX Association
4
Event trace
Cache-hit trace
e
s
a
h
p
t
i
h
-
e
h
c
a
C
d
n
e
t
n
e
v
E
Hit
Miss
t
r
a
t
s
t
n
e
v
E
0
0.1
2.24
0.2
TIME IN CYCLES
2.25
2.26
·107
Figure 1: Trace of a single keypress event for address
0x4ebc0 of libgdk.so.
Cache-Hit Trace and Event Trace. The generation of
the cache-hit trace and the event trace is repeated for each
event and address for the speciﬁed duration (the while
loop of Algorithm 1). The cache-hit trace g(H)
a,e is a binary
function which has value 1 for every timestamp t where
a cache hit has been observed. The function value re-
mains 1 until the next timestamp t where a cache miss has
been observed. We call subsequent cache hits a cache-hit
phase. The event trace g(E)
a,e is a binary function which has
value 1 when the processing of one speciﬁc event e starts
or ends and value 0 for all other points.
In the measurement step, the binary under attack is
executed and the event is triggered constantly. Each ad-
dress of the attacked binary is proﬁled for a speciﬁc du-
ration d. It must be long enough to trigger one or more
events. Therefore, d depends only on the execution time
of the event to be measured. The more events triggered
within the speciﬁed duration d, the more accurate the re-
sulting proﬁle is. However, increasing the duration d in-
creases the overall time required for the proﬁling phase.
The results of this measurement step are a cache-hit
trace and an event trace, which are generated for all ad-
dresses a in the binary and all events e we want to proﬁle.
An excerpt of such a cache-hit trace and the correspond-
ing event trace is shown in Figure 1. The start of the
event is measured directly before the event is triggered.
As we monitor library code, the cache-hit phase is mea-
sured before the attacked binary observes the event.
The generation of the traces can be sped up by two
factors. First, in case of a cache miss, the CPU always
fetches a whole cache line. Thus, we cannot distinguish
between offsets of different accesses within a cache line
and we can deduce the same information by probing only
one address within each cache-line sized memory area.
Second, we reduce the overall number of triggered
events by proﬁling multiple addresses at the same time.
However, proﬁling multiple addresses on the same page
can cause prefetching of more data from this page.
Therefore, we can only proﬁle addresses on different
pages simultaneously. Thus, proﬁling all pages only
takes as long as proﬁling a single page.
In case of low-frequency events, it is possible to pro-
ﬁle all pages within one binary in parallel. However, this
may lead to less accurate cache-hit traces g(H)
a,e , i.e., tim-
ing deviations above 1 microsecond from the real event,
which is only acceptable for low-frequency events.
Hit-Ratio Extraction. After the cache-hit trace and
the event trace have been computed for a speciﬁc event e
and a speciﬁc address a (the while loop of Algorithm 1),
we derive the cache-hit ratio for each event and address.
The cache-hit ratio Ha,e is either a simple value or a time-
dependent ratio function.
In our case it is the ratio of
cache hits on address a and the number of times the event
e has been triggered within the proﬁling duration d.
To illustrate the difference between a cache-hit ratio
with time dependency and without time dependency, we
discuss two such functions. The cache-hit ratio with
time dependency can be deﬁned as follows. The event
traces contain the start and end points of the processing
of one event e. These start and end points deﬁne the rel-
evant parts (denoted as slices) within the cache-hit trace.
The slices are stored in a vector and scaled to the same
length. Each slice contains a cache-hit pattern relative to
the event e. If we average over this vector, we get the
cache-hit ratio function for event e.
The second, much simpler approach is to deﬁne the
cache-hit ratio without time dependency. In this case, we
count the number of cache hits k on address a and divide
it by the number of times n the event e has been triggered
within the proﬁling duration d. That is, we deﬁne Ha,e =
k
n. In case of a low-noise side channel and event detection
through single cache hits, it is sufﬁcient to use a simple
hit-ratio extraction function.
Like the previous step, this step is repeated for all ad-
dresses a in the binary b and all events e to be proﬁled.
The result is the full Cache Template matrix T . We de-
note the column vectors (cid:30)pe as proﬁles for speciﬁc events.
Pruning.
In the exploitation phase, we are limited re-
garding the number of addresses we can attack. There-
fore, we want to reduce the number of addresses in the
Cache Template. We remove redundant rows from the
Cache Template matrix and merge events which cannot
be distinguished based on their proﬁles (cid:30)pe.
As cache hits can be independent of an event, the mea-
sured cache-hit ratio on a speciﬁc address can be inde-
pendent of the event, i.e., code which is always executed,
frequent data accesses by threads running all the time,
or code that is never executed and data that is never ac-
cessed. In order to be able to detect an event e, the set
USENIX Association  
24th USENIX Security Symposium  901
5
of events has to contain at least one event e(cid:31) which does
not include event e. For example, in order to be able to
detect the event “user pressed key A” we need to proﬁle
at least one event where the user does not press key A.
The pruning happens in three steps on the matrix.
First, the removal of all addresses that have a small dif-
ference between minimum and maximum cache-hit ra-
tio for all events. Second, merging all similar columns
(events) into one set of events, i.e., events that cannot be
distinguished from each other are merged into one col-
umn. The similarity measure for this is, for example,
based on a mean squared error (MSE) function. Third,
the removal of redundant lines. These steps ensure that
we select the most interesting addresses and also allows
us to reduce the attack complexity by reducing the over-
all number of monitored addresses.
We measure the reliability of a cache-based side chan-
nel by true and false positives as well as true and false