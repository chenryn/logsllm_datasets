We note the importance of considering both of these metrics
together. If we only assess one of them, we can easily inﬂate
its value at the expense of the other: a large pulse window size
will lead to a concentration efﬁciency of 1 (ignoring packet
drops) but no bandwidth gain. A very small target window
could result in an extremely high bandwidth gain (if one packet
lands in it) but a very low concentration efﬁciency.
Lastly, we measure bandwidth in terms of packets per
second. For our evaluation, the packets we send and that are
reﬂected are small (around 100 bytes), which can make the
quantities look artiﬁcially high. For a sense of scale, 10K pps
translates to about 8 Mbps.
VI. EXPERIMENTAL RESULTS
To assess the efﬁcacy of lensing, we emulated attacks on
machines under our control. We used an Windows Azure VM
instance on the West Coast as our attacker. We employed
another Azure VM instance along with an Amazon Web
Services VM instance, both on the East Coast, as our targets.
We used a publicly available list of 3,000 resolvers [1] as our
reﬂectors.
We registered a domain name5 that allows us to run author-
itative DNS servers under our control. We made the AWS
target instance authoritative for our domain and the Azure
target instance authoritative for a subdomain of the original.
This allows us to send recursive DNS queries through any
open recursive DNS server to either of our targets.
Before an attack,
the attack machine quickly scans the
resolver list. It issues recursive queries to the resolvers (just
as in an actual attack) to obtain latency measurements. We
gather 10 samples from each resolver, which turned out well
for our attacks. For each resolver, we construct a histogram for
the distribution of each resolver’s attack path latency, and use
this along with a variant (discussed below) of the optimization
algorithm in § IV to construct the sending schedule. During
the attack, we simply send to the resolvers according to the
schedule.
Figure 4 shows the results of pulses emulating attackers with
different bandwidths using a relatively narrow pulse window of
20 ms. The emulation setup artiﬁcially capped the outgoing
bandwidth by adjusting the minimum time between sending
adjacent packets. The bandwidth gain corresponds to dividing
the height of the pulse bucket by that of the tallest bucket
for the attacker’s sending. We see gains of 14x for the low-
bandwidth case, 10x for moderate bandwidth, and 5x for high
bandwidth. The efﬁciency corresponds to dividing the area
of the pulse bucket by that of the sending buckets. We ﬁnd
efﬁciencies around 50% for the low- and medium-bandwidth
cases, and just under 40% for the high bandwidth case.
The colors in Figure 4a map onto the reﬂectors used. We
see that while a large number of reﬂectors contributed to the
pulse, some do not at all, either due to misleading latency
measurements or because of jitter occurring during the attack
itself.
that
traces reveal
We observe what look like multiple pulses in Figures 4b
and 4c. Packet
these secondary spikes
result from retransmissions by the resolvers. The target (an
authoritative DNS server) could not keep up with the rate of
incoming queries and failed to respond to many of them. The
resolvers then timed out and retransmitted. Since many of them
share a common retransmit timeout, their retransmissions ren-
dezvous at time = (original pulse time) + (retransmit timeout).
We could thus identify two common retransmit timeouts of
800 ms and 2 s. (We discuss ways an attacker could leverage
retransmissions in § VII-D.) Retransmissions also caused the
total number of packets received by the target to often exceed
the total number sent by the attacker by about a factor of two,
5pulsing.uni.me
192
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:02 UTC from IEEE Xplore.  Restrictions apply. 
though we chose our metrics such that these additions do not
affect our characterizations of the attack’s efﬁcacy.
Figure 5 shows how the lensing metrics vary with pulse win-
dow duration when we ﬁx the attacker bandwidth at 10K pps
and the maximum per-reﬂector bandwidth at 500 pps. As
expected, the bandwidth gain (and absolute pulse bandwidth)
falls as we increase the window size. This is not an indication
of the attack performing poorer with larger window sizes, but
instead an intrinsic consequence of choosing a larger window
size (and thus a larger denominator in the pulse bandwidth
calculation).6 In fact, the increase in efﬁciency shows that, at
larger window sizes, lensing performs closer to optimal, at the
cost of a less sharp pulse.
While efﬁciency increases modestly with window size,
the increase levels-off at larger window sizes. Much of this
leveling off can be explained by the fact that many high-
latency paths exhibit signiﬁcant jitter (as discussed in § III).
In fact, for a window size as large as 100 ms, resolvers with
attack path latencies less than 250 ms (about half of those
used) show an efﬁciency (in aggregate) of about 80%, while
those with latencies over 250 ms show an efﬁciency of about
40% (which translates to an efﬁciency of about 60% over all
resolvers).
Figure 6 shows lensing properties as a function of maximum
bandwidth to any reﬂector. Here we have ﬁxed the attacker
bandwidth at 10K pps and the window size at 20 ms. The
variation in the metrics of bandwidth gain and pulse bandwidth
simply reﬂect high throttling of bandwidth to a constant-size
pool of reﬂectors (discussed in more detail in § V-B). The
illuminating metric is that of concentration efﬁciency. We
see little variation, except at very high throttling (effectively
sending only 1 or 2 packets per reﬂector), where we obtain
efﬁciency. With these high efﬁciencies, however, comes no
bandwidth gain, meaning that the attack failed to create a pulse
because of the excessive throttling. Due to lack of variation
in efﬁciency, we conclude that an attacker gains little by
limiting the bandwidth to each reﬂector, so they would only
do so for purposes of stealth, and not to avoid complications
arising from reﬂectors rate-limiting or failing due to excessive
loads (unless those effects only come into play above rates of
500 pps, the bound we used).
Figure 7 shows how the attack scales as a function of the
attacker’s bandwidth, where we have ﬁxed the pulse window
to 20 ms and the per-reﬂector bandwidth to 500 pps. The
relatively constant efﬁciency at the beginning indicates that the
attack scales well; we can explain the diminishing bandwidth
gain by the fact that we throttle bandwidth to each reﬂector
while keeping the pool of available reﬂectors constant (per
§ V-B). However, we see all metrics perform poorly at higher
bandwidths. Plotting peak pulse bandwidth versus maximum
attacker bandwidth (not shown) reveals a potential clue: we
would expect scaling linear with the attacker’s bandwidth, but
instead we ﬁnd it levels off, indicating that the largest pulse we
can create of duration 20 ms has a bandwidth of 50–60K pps.
The apparent pulse degradation at scale could mean that the
attack scales poorly. However, it could instead arise due to
the attack working—increased jitter and queuing could cause
pulse ﬂattening or packet loss. (Apparent packet loss could
also arise due to measurement loss. We did conﬁrm, however,
that dumpcap when recording our packet traces reported no
drops.)
To determine the cause of the poor scaling for the Azure
instance (the one we have been exploring in Figure 7), we
stressed it for a short duration at a rate of 100K pps. After three
trials, we found the maximum download bandwidth for small
DNS packets fell between 57–62K pps. Thus, we conclude
that the scaling issues indeed reﬂect the attack’s efﬁcacy—
namely, it saturated our Azure instance’s bottleneck resource
in receiving these packets. We found that our AWS could
accommodate a higher pps rate (Figure 8). Here, the pulse
only starts to scale poorly at about 110–120K pps (further
evidence that the attack was not exhibiting poor scaling for
the Azure instance). We lacked sufﬁcient attacker bandwidth
to push the pulse higher than that level, so we could not we
directly determine with certainty what causes the poor scaling
at 120K pps.
However, the difference in behavior between the Azure
and AWS instances with regards to unaccounted for packets
provides a further hint. We deﬁne unaccounted for packets as
those sent by the attacker but whose reﬂection never arrives at
the target. Some subtleties arise here. Due to retransmissions,
nearly all sent packets eventually arrive. Accordingly, we deem
packets as “unaccounted” if they fail to arrive within 200 ms
of the (20 ms-wide) pulse window.
In Azure, it appears that packets beyond the VM’s network
quota arriving in large bursts get buffered, as evidenced by
the pulse spreading in Figure 9. In contrast, the AWS instance
does not apparently buffer them (cf. Figures 4b and 4c).
Instead, the reﬂected packets never appear at the AWS end.
Figures 10a and 10b quantify this difference. With the Azure
instance as the target, we see a relatively constant proportion of
unaccounted packets; the attack delivers most of the packets,
even at high attacker bandwidths. However, at such higher
bandwidths the AWS target receives far fewer such packets,
per Figure 10b. It appears that AWS responds to excessive
trafﬁc incoming to an instance by dropping packets instead of
queuing them as does Azure. This discrepancy between Azure
and AWS indicates that the attack indeed worked effectively,
and that the leveling-off at ≈ 120K pps mentioned above arises
due to an AWS resource limit.
In short, the attack displays impressive numbers and scales
well. As the peak pulse bandwidth nears the target’s maximum
capacity, however, the attacker sees diminishing returns.
VII. EXTENSIONS
6Similarly, if we focus light over a larger area, then its intensity at any
point in that area diminishes.
In this section we assess a number of additions or potential
“improvements” to lensing attacks.
193
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:02 UTC from IEEE Xplore.  Restrictions apply. 
(a) Pulse from low-bandwidth (500 pps)
sending to 75 reﬂectors.
(b) Pulse from medium-bandwidth
(10,000 pps) sending to 816 reﬂectors.
(c) Pulse from high-bandwidth
(20,000 pps) sending to 1201 reﬂectors.
Figure 4: Selected pulses performed on AWS instance, using a maximum per-reﬂector bandwidth of 500 pps, a pulse window
of 20 ms, and a plot bucket also equal to 20 ms. Time along the X-axis for the top and bottom ﬁgures does not reﬂect
synchronized clocks, but instead starts (separately) shortly before the ﬁrst appearance of attack packets.
(a) Bandwidth Gain
(a) Bandwidth Gain
(b) Concentration Efﬁciency
(b) Concentration Efﬁciency
Figure 5: Lensing metrics as a function of target pulse window
size, with AWS instance as target. Attacker bandwidth =
10K pps; Max. per-reﬂector bandwidth = 500 pps.
Figure 6: Lensing metrics as a function of throttled bandwidth
to each reﬂector, with AWS instance as target. Attacker
bandwidth = 10K pps; window size = 20 ms.
194
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:02 UTC from IEEE Xplore.  Restrictions apply. 
Time (ms)020406080100120140160Number of PacketsPackets Leaving Attacker0100200300400500600700Time (ms)020406080100120140160Number of PacketsPackets Arriving at VictimTime (ms)05001000150020002500Number of PacketsPackets Leaving Attacker0500100015002000250030003500Time (ms)05001000150020002500Number of PacketsPackets Arriving at VictimTime (ms)05001000150020002500Number of PacketsPackets Leaving Attacker0500100015002000250030003500Time (ms)05001000150020002500Number of PacketsPackets Arriving at Victim020406080100120Window Size (ms)02468101214Bandwidth Gain020406080100120Window Size (ms)0.400.450.500.550.600.650.70Concentration Efficiency0100200300400500600Max Bandwidth to Any Reflector (pps)024681012Bandwidth Gain0100200300400500600Max Bandwidth to Any Reflector (pps)0.40.50.60.70.8Concentration Efficiency(a) Bandwidth Gain
(a) Bandwidth Gain
(b) Concentration Efﬁciency
(b) Concentration Efﬁciency
Figure 7: Lensing metrics as a function of attacker’s maximum
bandwidth, with Azure instance as target. Max. per-reﬂector
bandwidth = 500 pps; pulse window = 20 ms.
Figure 8: Lensing metrics as a function of attacker’s maximum
bandwidth, with AWS instance as target. Max. per-reﬂector
bandwidth = 500 pps; pulse window = 20 ms.
A. Attacks on arbitrary end-hosts
We have framed our development of lensing attacks so far
in the context of targeting DNS servers. Since DNS generally
operates over UDP (and even for TCP has short ﬂows), pulsing
attacks—which primarily attack TCP congestion control—
may have low efﬁcacy against such servers. To target a more
rewarding victim, an attacker must somehow calculate attack
path latencies to that host. We present two methods, both
heavily inﬂuenced by King [8].
type of DNS server
DNS cache manipulation. We use manipulation of
DNS cache contents7
to calculate latencies between a
DNS resolver and any other
(not
just an authoritative one), per Figure 11, which shows
how to calculate the latencies between a resolver N SA
and any other DNS server N SB. Along these lines, we
create a DNS entry in our own authoritative DNS server—
mydomainname.com—stating that N SB (10.0.0.0)
is
for 10-0-0-0.mydomainname.com.
if we issue queries to N SA for subdomains of
Then,
10-0-0-0.mydomainname.com, N SA
have
cached the NS record indicating N SB as authoritative for
authoritative
will
7Note that this manipulation—while called “poisoning” in the King paper—
does not reﬂect a DNS cache poisoning attack. Rather, we simply return
incorrect DNS records for domains over which we have legitimate control.
10-0-0-0.mydomainname.com, and will query N SB.
N SB will reply with an error, but the chain of queries still
reveals the attack path RTT.
We note that we can extend this cache-poisoning technique
of King’s to arbitrary end hosts. By replacing N SB with B
(any arbitrary server—not necessarily for DNS), then when B
receives a DNS query from N SA, it most likely will not have a
service running on port 53 (DNS). According to RFC 1122 [3],
B “SHOULD” respond with an ICMP Port Unreachable, and,
in response, the UDP layer of N SA “MUST” pass an error
up to the application layer. If N SA’s DNS implementation
responds to this error indication by immediately responding
to us, we again can calculate the attack path RTT.
Two issues arise when using the above method. First,
B might not even receive packets sent to port 53 due to
ﬁrewalling; or may have an explicit conﬁguration not
to
respond. Second, the resolver implementation must respond
to ICMP error messages propagated to it and deal with them
appropriately. That said, we note that our server (using the
default conﬁguration of BIND9 on Ubuntu Linux) does in fact
do both: it issues an ICMP error when port 53 lacks a running
service, and, as a resolver, immediately responds back to the
client when informed of an ICMP error. Further, we tested this
method to build schedules and create pulses, and found that it
indeed works with many resolvers. Some resolvers, however,
195
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:02 UTC from IEEE Xplore.  Restrictions apply. 
01020304050Attacker Max Bandwidth (thousand pps)02468101214Bandwidth Gain01020304050Attacker Max Bandwidth (thousand pps)0.10.20.30.40.50.6Concentration Efficiency01020304050Attacker Max Bandwidth (thousand pps)0246810121416Bandwidth Gain01020304050Attacker Max Bandwidth (thousand pps)0.250.300.350.400.450.500.550.600.65Concentration EfficiencyFigure 9: Illustration of pulse spreading at the Azure target.
Attacker bandwidth = 20,000 pps; window size = 20 ms.
Time along the X-axis for the top and bottom ﬁgures does
not reﬂect synchronized clocks, but instead starts (separately)