maximum bus throughput. There are 4 ways to let a device
communicate through PCIe. 1) Installing the device on the
PCIe slot of the motherboard, e.g., discrete GPU; 2) Installing
the device on another type of slot which is compatible with
PCIe protocol, e.g., installing NVMe (Non-Volatile Memory
Express) SSDs on M.2 slots; 3) Soldering a device on the
motherboard and using PCIe protocol, e.g., onboard NIC
(Network Interface Card); 4) Leveraging a controller to convert
a different protocol used by a device to PCIe protocol, e.g.,
SATA hard disks.
The communication between PCIe devices is carried out by
interconnect (or link). A link consists of one or more lanes,
which carries a full-duplex byte stream, producing 985 MB/s
bandwidth per lane per direction for the version supported by
the mainstream Intel CPUs [14]. The number of lanes per link
can be 1, 2, 4, 8, or 16. Thus, the device’s PCIe speed can be
adjusted based on the number of occupied lanes.
Different from PCI protocol
in which a shared parallel
bus architecture is used and the bus clock is constrained by
the slowest peripheral device, PCIe protocol is packet-based,
which enables more efﬁcient bus usage. Similar to the network
stack, PCIe has three layers and they are transaction layer,
data link layer, and physical layer. The data are encapsulated
into packets by transaction layer and routed based on memory
address, I/O address and device ID [15]. The data link layer
handles integrity check, ﬂow control, and re-transmissions,
ensuring the data integrity and reliability for the upper layers.
To this end, point-to-point credit-based ﬂow control [9] is
employed by PCIe. On a PCIe link, the receiver notiﬁes the
sender the size of the receiving buffer reserved for it ahead,
and the data transmitted by the sender is always under the
buffer limit.
B. PCIe Topology
PCIe allows devices to connect to the processor in a tree-
like topology [15]1, with the help of PCIe switch and PCH
1In the future, PCIe fabric may be used in a data center, changing the
topology to a network-like topology.
323
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
to allow a data sender to directly access the memory region
of a receiver, without waiting for the receiver’s CPU. Most
RDMA functionalities are implemented by RDMA NICs, sup-
porting one-sided or two-sided RDMA operations. In the one-
sided setting, the RDMA application at the server registers one
or multiple memory regions with a RDMA NIC. The virtual
address of the memory region as well as a key corresponding
to the memory region will be sent to the client. When the client
is going to read the memory in the server, she constructs and
sends a special RDMA read packet that contains the address of
the memory to be read as well as the key. The server’s RDMA
NIC directly reads the requested memory for the client once
receiving the packet, without interrupting the CPU. The value
of the requested memory will be encapsulated in the response
packet that will be delivered back to the client. In the two-
sided setting, the CPUs of the sender and the receiver are
both involved.
A few RDMA implementations have been developed un-
der the standard RDMA protocol, namely InﬁniBand [17],
RoCE [18] and iWARP [19]. We tested our attack under
InﬁniBand as it is the native physical layer protocol of RDMA.
Currently, RDMA has seen strong adoption in data centers
and cloud, resulting in high-performance applications related
to key-value storage [20], graph processing [21], machine-
learning [22] and etc. We consider RDMA as one attack vector
to enable our attack, INVISIPROBE.
III. ATTACK OVERVIEW
Using I/O switches to serve multiple PCIe devices and
share the PCIe bandwidth has become the standard solution
to address the constraints of PCIe interfaces. However, it also
opens up possibilities for attacks. In this section, we ﬁrstly
describe the threat model. Then, we elaborate on the issue of
PCIe congestion and how it can be exploited as a side-channel.
Finally, we overview the attack procedure.
A. Threat Model
As shown in Figure 1, we assume that among a pair of I/O
devices integrated by a target machine, one interacts with an
attacker and another is used by a victim. Both devices share
the same I/O switch which connects to the upstream CPU. We
assume the attacker cannot access the victim’s data or code
directly, but plans to use INVISIPROBE to infer the victim’s
secrets.
We showcase two scenarios. In the ﬁrst scenario, the attack
happens in a cloud environment or a data center where a
server allows a remote machine to directly access its memory
through its RDMA NIC (e.g., by running an RDMA key-value
storage). The attacker can control a VM in the same cloud or
a machine in the same data center to interact with the RDMA
NIC of the server. In the meantime, a high-speed device, in
particular a discrete GPU, is used by a victim. GPU and NIC
share the same PCIe switch, which is the setting appeared
in the slides of NVIDIA [7] when NVIDIA is launching the
30 series GPUs. Using RDMA to steal a victim’s secret was
researched recently [23], [24] and our attack setting is similar,
Fig. 1: PCIe topology and adversary model.
(Platform Controller Hub). In this work, we term them jointly
as I/O switches as they have similar functionalities in routing
PCIe packets. An I/O switch has upstream and downstream
ports connecting to different devices, in order to separate the
data ﬂow. Figure 1 illustrates one common PCIe topology.
Speciﬁcally, the PCIe lanes interfaced by CPU and the memory
controller are connected to a component named root complex
which is further connected to the peripheral devices. The
connection between the root complex and peripheral devices
can be divided into three classes.
Firstly, a small number of high-speed interfaces of root com-
plex are directly connected to high-speed devices. For main-
stream Intel CPUs like “Coffee Lake” CPUs [16], interface to
16-lane PCIe link is provided, which can be used by a discrete
GPU.
Secondly, relatively slow devices, like hard disks, sound
cards, and NICs, are connected to PCH before the root com-
plex, which is the standard component on desktop machines
and also servers. Direct Media Interface (DMI) is used as
the channel between CPU and PCH, which has nearly the
same design as 4-lane PCIe link. A PCH can connect to
many devices, each of which possesses at least one PCIe lane.
However, the resulted total I/O throughput can be much larger
than what DMI can provide. Thus, DMI allows the connected
devices to share the bandwidth of the four PCIe lanes.
Thirdly, the PCIe switch is used to expand CPU’s capa-
bilities in connecting more PCIe devices, especially on high-
performance servers. While the CPU integrated by a server
usually provides high-speed interfaces for direct connection
to PCIe devices, it might be insufﬁcient to meet the demand
of adding PCIe devices. For example. Tyan Thunder HX
FT77DB7109 [6], a mainstream server for deep learning,
equips 8 GPUs, 4 NVMe SSDs, and 1 x16 high-speed NIC. As
only three 16-lane PCIe interfaces are provided by the CPU
(“Cascade Lake-SP” [5]), the server manufacturer converts one
16-lane PCIe interface into two or more interfaces by using a
PCIe switch.
C. RDMA
To reduce the network latency and CPU consumption on the
server, RDMA (Remote Direct Memory Access) was proposed
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
324
Root ComplexCPU CoresMemory ControllerPCHPCIe SwitchPCIe DeviceGPUOther DeviceRDMA NICNVMe SSDNICi.e., assuming attacker can interact with an RDMA NIC in
a server, but the goal of the attacks differ. In Section VIII,
we compare to these works in detail. We assume the server
providing GPU acceleration allows multiple users/tenants to
use the machine simultaneously. This assumption has been
justiﬁed by previous GPU side-channel attacks [25], [26].
Moreover, we assume the victim can use the GPU exclusively,
which is an even weaker condition comparing to previous
works [25], [26] assuming GPU is shared between the victim
and the attacker.
Under this scenario, we studied three types of attacks. 1)
The server has a VM leased to a victim who types sensitive
text over remote desktop to the VM; 2) The server has a
VM leased to a victim who opens chrome to view webpages
through remote desktop; 3) A victim trains a deep-learning
model on the server. The attack goals are to infer: 1) which
word is typed by the victim; 2) which webpage is browsed by
the victim; 3) which model is trained by the victim.
In the second scenario, we assume the attacker has access to
an NVMe SSD on the server. The attacker can be local (e.g.,
executing a program on the server) or remote (e.g., accessing
the SSD through NVMe fabric [27]). In the meantime, a victim
uses a low-speed device, in particular a standard NIC, on the
server. As described in Section II, PCH is used to serve the
trafﬁc from SSD and NIC together. Under this scenario, we
investigate an attack that is the same as the second attack
under the prior scenario: inferring what pages are visited by
the victim.
B. PCIe Congestion
As PCIe connections resemble network connections, con-
gestion could happen when the data volume to be transmitted
surpass the capacity of PCIe links or I/O switches. Specif-
ically, when the sender learns that the requested receiving
buffer exceeds the capacity of the receiver, it holds the packet
to be sent till the receiver’s buffer is freed, in a process called
back-pressure [15]. The packet transmission will be delayed,
and the delay grows when the data pressure is increased on
the PCIe link, as shown by previous studies [3], [28].
PCIe congestion can be caused by both high-speed and low-
speed devices. For the ﬁrst case, the data volume generated
by a device alone,
like GPU, can cause congestion. For
instance, Nvidia GTX 1080Ti GPU can occupy 480 GB/s
PCIe bandwidth [8], but the 16-lane PCIe link offers no more
than 16 GB/s bandwidth. For the second case, when multiple
devices transmit packets to a PCH simultaneously, congestion
can happen. For instance, a PCH only occupies four PCIe
lanes from CPU. However, an NVMe SSD alone has a four-
lane link to PCH, so it is able to ﬁll all PCH lanes. When
another device shares the same PCH, congestion will happen.
To notice, though PCIe congestion happens frequently, prior
works mostly focused on its impact to the performance of
PCIe fabric [29], [30]. The issue on the single machine and
how it can be exploited for attacks have not been studied.
Through experiments, we found the overhead caused by
PCIe congestion is not negligible, and reﬂects what happens on
the devices sharing the same I/O switch, to some degree. An
attacker can exploit this ﬁnding to measure the I/O latency on
the devices of the target machine (e.g., RDMA NIC and NVMe
SSD) and analyze the portion related to PCIe congestion.
However, the measurement of I/O operations is known to be
ﬂuctuating, due to interrupts [31]. Yet, we found that when the
kernel is bypassed, the measurement becomes stable. For the
ﬁrst attack scenario, RDMA directly bypasses the kernel [32].
For the second attack scenario, when a kernel-bypass driver,
like SPDK [33] or io_uring, is installed for the NVMe
SSD, the same effect can be observed. In fact, kernel-bypass
driver is widely installed on servers of data centers [34]. In
Appendix A, we explain how kernel-bypass driver helps the
I/O measurement in detail.
C. Attack Procedure
Based on our insights into PCIe congestion, we design IN-
VISIPROBE, a new attack exploiting PCIe congestion as side-
channels. In essence, when congestion is caused by the high-
speed victim device, the attacker can measure the variation
of the I/O latency by sharing PCIe switch to infer the secret
states of the victim. When the victim device is low-speed,
unable to introduce congestion alone, the attacker can tunnel
in a high volume of data to “saturate” the shared PCH switch
and measure the I/O latency at the same time. This subsection
overviews the workﬂow of INVISIPROBE and Section IV and
V elaborate how INVISIPROBE are implemented against PCIe
switch and PCH respectively.
1. Device pairing for congestion. The attacker ﬁrst needs to
identify a pair of I/O devices that share an I/O switch. While
the attacker does not directly know the PCIe topology of the
target machine, the chances of ﬁnding a device sharing a PCIe
link with the victim device are high, especially at servers. In
Section VI-A we give an example of the PCIe topology.
2. Delay measurement. The attacker probes the I/O latency
of her device using the timing API (e.g., RDTSCP instruc-
tion [35]) to infer the status of the victim device. The probe
must satisfy the following requirements:
• Attacker’s device can complete each probe request within
a short interval, achieving a high sampling rate on a PCIe
link.
• The latency of the probe should be stable, so its variance
should be small when the same data volume is encoun-
tered by a PCIe link.
• The congestion level should be proportional to the probe
delay.
In Section IV and V, we describe how the probes on NIC
and NVMe SSD are constructed under the above requirements.
3. Inferring the secret. After the delays of the probes are
collected, the attacker will analyze them to infer the victim’s
activities. Based on our exploratory analysis, the mapping
between them is not straightforward, therefore we leverage
supervised learning techniques to obtain such mapping and
use it to classify user’s activities.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
325
Challenges. While the attack procedure is simple at a high
level, there are some challenges we need to address in at-
tack implementation. Firstly, the choices of the probe are
paramount (e.g., different I/O APIs,
their parameters, and
device access patterns), but most of them cannot fulﬁll the
three proprieties in the delay measurement. Secondly, high-
resolution tools are required to measure the I/O latency so the
“weak signals” from users’ sensitive activities can be captured.
The regular measurement tools provided by OS cannot pro-
vide enough accuracy for latency measurement. Thirdly, the
attacker cannot directly measure the victim’s device, and the
probes she issues might interfere with her observations. To
solve those challenges, the probe and the inference methods
have to be carefully designed and we elaborate them in the
two attack scenarios.
IV. ATTACKING GPU WITH RDMA NIC
In this scenario, we assume a high-speed device, in par-
ticular a discrete GPU, is used by a victim. The adversary
has access to an RDMA NIC on the same machine. For the
attacker to obtain reliable measurement through the RDMA
probe, the processing time of the RDMA request at the server
should be small. Fortunately, this requirement can be fulﬁlled
readily. For example, 1.3 million ops/sec can be achieved by
an RDMA key-value store on Inﬁniband [20].
One advantage of the RDMA probe is that it cannot be
interfered by other network trafﬁc, like LAN trafﬁc. A server
usually uses an RDMA NIC to handle RDMA trafﬁc and an
Ethernet NIC to handle LAN trafﬁc, so they are isolated at the
physical layer. During our experiment, we also use separate
NICs. Though RDMA NIC can be conﬁgured to handle IP
packets, e.g., through IPoIB [36], this feature is disabled by us
(the default setting). Therefore, no IP trafﬁc would go through
the RDMA NIC.
A. Design of Probe
RDMA supports three types of connections, Reliable Con-
nected (RC), Unreliable Connected (UC), and Unreliable Data-
gram (UD) [37]. Among them, RC resembles TCP protocol,
and RDMA read can be done only on this connection type,
so we assume the attacker probes the I/O delay with RC.
Algorithm 1 overviews the whole probing method. We write
around 400 lines of C code for this RDMA probe.
When RC is set up, a send queue will be created on the
attacker side to send the read requests. Each read request