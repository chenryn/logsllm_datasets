title:Finding the Linchpins of the Dark Web: a Study on Topologically Dedicated
Hosts on Malicious Web Infrastructures
author:Zhou Li and
Sumayah A. Alrwais and
Yinglian Xie and
Fang Yu and
XiaoFeng Wang
2013 IEEE Symposium on Security and Privacy
Finding the Linchpins of the Dark Web: a Study on Topologically Dedicated Hosts
on Malicious Web Infrastructures
Zhou Li, Sumayah Alrwais
Indiana University at Bloomington
{lizho, salrwais}@indiana.edu
Yinglian Xie, Fang Yu
MSR Silicon Valley
{yxie, fangyu}@microsoft.com
XiaoFeng Wang
Indiana University at Bloomington
PI:EMAIL
Abstract—Malicious Web activities continue to be a major
threat to the safety of online Web users. Despite the plethora
forms of attacks and the diversity of their delivery channels,
in the back end, they are all orchestrated through malicious
Web infrastructures, which enable miscreants to do business
with each other and utilize others’ resources. Identifying the
linchpins of the dark infrastructures and distinguishing those
valuable to the adversaries from those disposable are critical
for gaining an upper hand in the battle against them.
In this paper, using nearly 4 million malicious URL paths
crawled from different attack channels, we perform a large-
scale study on the topological relations among hosts in the
malicious Web infrastructure. Our study reveals the existence
of a set of topologically dedicated malicious hosts that play
orchestrating roles in malicious activities. They are well con-
nected to other malicious hosts and do not receive trafﬁc
from legitimate sites. Motivated by their distinctive features
in topology, we develop a graph-based approach that relies
on a small set of known malicious hosts as seeds to detect
dedicate malicious hosts in a large scale. Our method is
general across the use of different types of seed data, and
results in an expansion rate of over 12 times in detection
with a low false detection rate of 2%. Many of the detected
hosts operate as redirectors, in particular Trafﬁc Distribution
Systems (TDSes) that are long-lived and receive trafﬁc from
new attack campaigns over time. These TDSes play critical
roles in managing malicious trafﬁc ﬂows. Detecting and taking
down these dedicated malicious hosts can therefore have more
impact on the malicious Web infrastructures than aiming at
short-lived doorways or exploit sites.
I. INTRODUCTION
Technological progress often comes with side effects.
Look at today’s Web: not only does it foster a booming Web
industry, but it also provides new opportunities to criminals
who are rapidly industrializing their dark business over the
Web. Today once you unfortunately click a malicious URL,
chances are that those who victimize you are no longer
individual, small-time crooks but an underground syndicate:
some luring you to visit malicious websites through various
channels (Spam, tweets, malicious advertising, etc.), some
buying and selling your trafﬁc through redirection, and the
receiving ends of the trafﬁc performing different exploits
(drive-by downloads, scams, phishing etc.) on your system
on behalf of their customers. Such a complicated attack is
orchestrated through malicious Web infrastructures, which
enable those miscreants to do business with each other
and utilize others’ resources to make money from their
misdeeds. Indeed, such infrastructures become the backbone
of the crimes in today’s cyberspace, delivering malicious
web content world wide and causing hundreds of millions
in damage every year.
Malicious Web infrastructures. Given the central role
those infrastructures play, an in-depth understanding of their
structures and the ways they work becomes critical for coun-
teracting cybercrimes. To this end, prior research investi-
gated the infrastructures associated with some types of chan-
nels (e.g., Spam [2], black-hat Search-Engine Optimization
(SEO) [11]) and exploits (e.g., drive-by downloads [23]).
What have been learnt
includes the parties involved in
a malicious campaign (e.g., afﬁliates, bot operators [28]),
the underground economy behind such campaigns and how
these parties work together [15], [12]. Of particular interest
is the discovery of extensive sharing of resources (e.g.,
compromised systems, redirection servers, exploit servers)
within some categories of malicious activities.
With progress being made in this domain, still our
knowledge about the malicious Web Infrastructures is rather
limited. Particularly, all these prior studies stay at individual
redirection chains that deliver malicious content through
a speciﬁc channel (e.g., spam [2], twitter [14], malvertis-
ing [16]) or lead to a speciﬁc type of illicit activities (e.g.,
drive-by downloads, underground pharmaceutical business).
What is missing here is an in-depth understanding of the big
picture: what is the topological view of today’s malicious
Web infrastructures, and how are malicious entities related
to each other and the legitimate part of the Web, across
different redirection chains, different channels, and exploits?
The answer to these questions could help us identify the
linchpins of the dark infrastructures, differentiating those
valuable to the adversary from those expendable. As a
result, we will be able to build more effective and robust
techniques that disrupt malicious activities at their common
weak spots, without knowing their semantics and relying on
any channel/attack speciﬁc features such as URL patterns
that often can be easily evaded by the adversary. Also,
knowing the topological relations among malicious entities,
one can make better use of what has already been detected)
to discover other malicious parties.
Topologically dedicated malicious hosts. To gain further
understanding of malicious Web infrastructures, we study
1081-6011/13 $26.00 © 2013 IEEE
DOI 10.1109/SP.2013.18
112
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:53 UTC from IEEE Xplore.  Restrictions apply. 
nearly 4 million malicious URL paths crawled from different
feeds, particularly the topological relations among the hosts
involved and their connectivity with legitimate Web entities.
Our key ﬁnding is the existence of a set of topologically
dedicated malicious hosts that play orchestrating roles in the
infrastructures. From the data we have, all URL paths going
through them are conﬁrmed to be malicious. These dedicated
malicious hosts have a set of distinctive topological features.
First, they seem to have strong connections with each other
by forming tight Host-IP Clusters (HICs) that share IP ad-
dresses and Whois information. Second, they are extensively
connected to other malicious parties, hosting over 70% of
the malicious paths in our dataset. Finally, they are not found
to receive any legitimate inputs, though they may redirect
trafﬁc to legitimate parties, e.g., when they cloak.
Our work. Since these topologically dedicated hosts and
their HICs play a central role in linking different malicious
paths together,
them for
breaking the malicious infrastructures. In our research, we
come up with a new topology-based technique designed to
catch these hosts without relying on the semantics of the
attacks they are involved in. Intuitively, these dedicated hosts
are rather easy to reach from the dark side of the Web
while extremely hard to reach from the bright side. This
observation ﬁts perfectly with the concept of PageRank [3]:
that is, they should be popular in the dark world but unpop-
ular outside. Our approach starts with a relatively small set
of known malicious HICs as seeds and a large number of
known legitimate HICs as references, and propagates their
initial scores across a Web topology using the PageRank
algorithm to compute legitimate and malicious scores for
other unknown HICs. In the end, those highly endorsed by
the malicious hosts but way less so by the legitimate ones
are identiﬁed as dedicated malicious HICs.
it becomes important
to detect
Our approach works surprisingly well: in our evaluation
based upon 7-month data crawled from Alexa top web-
sites [1], our approach detects about 5,000 new topologically
dedicated malicious hosts and over 20,000 malicious host
paths that are not captured by existing solutions, at a false
detection rate as low as 2%. Our study further reveals the
roles, the operation models, and the monetization strategies
of these dedicated malicious hosts, particularly those that
work as Trafﬁc Distribution Systems (TDSes), which are
professional trafﬁc buying and selling systems that manage
and keep record of trafﬁc-exchange transactions. Our major
detection results and interesting ﬁndings include:
• Our algorithm achieves a high detection rate. Even with a
small set of seed malicious HICs (5% of the labeled ones),
we can discover a large number of other malicious HICs,
with an expansion rate of 12 times.
• Our detection algorithm is general across the use of
different malicious seeds, including drive-by downloads and
Twitter spam in our experiments. It can also detect malicious
hosts set up through different attack channels, such as drive-
113
by downloads and scam in our data.
• For the set of dedicated malicious hosts that serve as
TDSes, they are much more long-lived than doorways or
exploit sites (65 days vs. 2.5 hours). They receive malicious
trafﬁc from new attack campaigns over time. Disrupting their
operations has more long-lasting effects than taking down
doorways or exploit sites.
• Our study shows that even after TDSes are taken down,
they continue to receive a large amount of trafﬁc, 10
times more than legitimate parked domains. Such trafﬁc is
leveraged by domain owners through parking services to
continue to gain revenues from ad networks.
Contributions. The contributions of the paper are summa-
rized as follows:
• New ﬁndings. We conduct the ﬁrst study on topologically
dedicated malicious hosts, and discover their pervasiveness
in malicious Web infrastructures and the critical roles they
play. Our study reveals their topological features and the way
they are utilized. We show that TDSes play an important role
in managing and exchanging trafﬁc ﬂows the adversary uses
to deliver malicious content and bring to the light how these
malicious dedicated hosts evolve with time and how they
are monetized by domain owners through parking services
even after their domain names are taken down.
• New techniques. We develop a new technique that expands
from known malicious seeds to detect other malicious dedi-
cated hosts, based upon their unique features. Our approach
works effectively on large-scale real data, capturing a large
number of new malicious hosts at a low false detection rate.
Roadmap. The rest of the paper is organized as follows:
Section II presents the background information of our
research,
including how data was collected; Section III
discusses a measurement study over the data, which reveals
the important role played by dedicated malicious hosts and
their prominent features; Section IV describes the design
and implementation of our detection technique; Section V
evaluates our detection system on its efﬁcacy; Section VI
reports a study on all the malicious dedicated hosts we
found; Section VII discusses a few issues of our study
and potential future work; Section VIII reviews the related
research and Section IX concludes the paper.
II. DATA COLLECTION
In this section, we explain the data collection process and
the methodology we use to label data for our study. This
process serves two purposes: (1) It helps us prepare data for
building the Web topology graph (Section III); (2) It labels
known malicious and legitimate portions of the Web using
existing techniques, so that we can study their distinctive
topological features for detection. Later in Section IV, we
show how we can leverage the topological features learned
during this process to detect malicious URLs and hosts not
identiﬁed before.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:53 UTC from IEEE Xplore.  Restrictions apply. 
A. Data Sources
Feed
Drive-by-download
Warningbird
Twitter
Top sites
Start
3/2012
3/2012
3/2012
2/2012
End
8/2012
5/2012
8/2012
8/2012
# Doorway URLs
1,558,690
358,232
1,613,924
2,040,720
Table I
DATA FEEDS USED IN THE STUDY.
We use four different data feeds to bootstrap data collec-
tion. Each data feed includes a set of doorway URLs that
we leverage to crawl and analyze the redirection topology.
Our data feeds include:
• Drive-by-download feed: Microsoft provides us with
around 30 million doorway URLs that were found to deliver
malicious contents (mostly drive-by downloads) and we
sample 5% (1.5 million) from them for study.
• Warningbird feed: We download the malicious URL set
posted by the WarningBird project [14]. This set includes
over 300k suspicious URLs in Twitter spam.
• Twitter feed: We run Twitter Search APIs [29] to pick top
10 trending terms every day. We use these terms to collect
related tweets and extract all the URLs they contain. This
process gives us 1.6 million URLs.
• Top-site feed: We gather Alexa top 1 million web sites
and update them every week. We obtain 2 million URLs in
total, most of which are legitimate.
All together, we have gathered about 5.5 million initial
URLs, which serve as inputs to a set of crawlers described
below during a 7-month period to collect data.
B. Redirection Chain Crawling
We deploy 20 crawlers, each hosted on a separate Linux
virtual machine with a distinctive IP address, to explore
the URL redirection paths of these 5.5 million doorway
URLs. Each crawler is built as a Firefox add-on, which
keeps track of all the network requests, responses, browser
events and page content it encounters in a visit. Based on
such information, our approach automatically reconstructs a
redirection URL path for the visit, which links all related
URLs together.
More speciﬁcally, such URL paths are built in a mostly
standard way, similar to Google’s approach [23] except
the part for analyzing Javascript. Our approach detects
redirections from HTTP status code (e.g. 302), Meta refresh
tag, HTML code (e.g., iframe tag) and JavaScript. The dots
(URLs) here are connected using different techniques under
these different types of redirections. Actually, Firefox gives
away the source and destination URLs through browser
events when the redirection has HTTP 3xx status code or is
triggered by Meta refresh, which allows us to link the source
to the destination. For those caused by HTML, we can ﬁnd
out the URL relation according to the Referral ﬁeld of the
destination URL. What gives us trouble is the redirection
triggered by Javascript code, which is not speciﬁed upfront
by any HTTP and HTML ﬁelds. This problem is tackled in
prior research [23] by simply searching the script code to
look for the string similar to the URLs involved in the HTTP
requests produced after running the code: if the edit distance
between the URL in the request and part of the content the
script carries is sufﬁciently small, a causal relation is thought
to be found and the URL of the document hosting the script
is connected to the request.
A problem for the approach is that it cannot capture a redi-
rection when the adversary obfuscates the JavaScript code,
which is common on today’s Web: what has been found
is that
increasingly sophisticated obfuscation techniques
have been employed to evade the detection that inspects
redirections [5]. To mitigate this threat, we resort to dynamic
analysis, instrumenting all JavaScript DOM APIs that can
be used to generate redirections, e.g., document.write.
When such an API is invoked, our crawler inspects the caller
and callee to connect the URLs of their origins.
To increase our chance of hitting malicious websites,
we set the crawler’s user-agent to IE-6, which is known
to contain multiple security-critical vulnerabilities. In ad-
dition, to avoid some malicious servers from cloaking to
the requests with an empty Referral ﬁeld [7], we set
the Referral ﬁeld of the initial request for each URL to
http://www.google.com/. After visiting a web page,
the crawler also cleans cookies to avoid tracking.
C. Data Labeling
For each visit, our crawler generates a set of URLs and
connects them according to their causal relations, which
gives us a set of URL paths in terms of URL redirection
chains. From URL paths, we further derive a set of host
paths that keep only host names along the redirection chains.
We proceed to label all the crawled URLs, URL paths, and
host paths as malicious or legitimate using a combination of
existing tools and methods.
Forefront Alarmed Page 
Iframe1 
Iframe2 
Legitimate Ad-network 
Doubleclick.com 
Malicious Redirector 
pokosa.com 
Exploit Server 
goodp.osa.pl 
Figure 1. Redirections from a Forefront alarmed Page. The ﬁrst redirection
path is not marked as malicious since it leads to only a legitimate party. The
second redirection path is a malicious path as the redirection is generated
from an iframe injected by an attacker.
Labeling of malicious URLs and paths. Speciﬁcally, we
ﬁrst use Microsoft Forefront, an Anti-Virus scanner, to scan
the contents associated with individual URLs encountered
114
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:53 UTC from IEEE Xplore.  Restrictions apply. 
during the crawling.1 Once a node (i.e., a URL) is ﬂagged
by the scanner as containing malicious contents (typically
code), the URL is labeled as malicious. The data crawled
from Alexa top sites and Twitter feeds yield mostly legiti-
mate URLs. The data crawled from the drive-by download
and the Warningbird feeds, however, do not always yield
malicious URLs for each visit. The reason is that drive-by
download doorway URLs are sometimes hosted on compro-
mised hosts, which may have already been cleaned up when
we visit them. Therefore, the scan we perform helps avoid
falsely marking them as malicious.
Once we label a URL as malicious, we treat all the URL
paths going through it as suspicious paths. However, not
all suspicious paths are malicious. For example, a malicious
doorway page may lead to multiple paths, and only one
of them leads to exploits. This happens when a malicious