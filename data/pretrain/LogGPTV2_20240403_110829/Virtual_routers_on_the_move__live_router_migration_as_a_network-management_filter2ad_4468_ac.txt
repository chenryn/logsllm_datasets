imize the total migration time, and more importantly, to
minimize the control-plane downtime (i.e., the time between
when the control plane is check-pointed on the source node
and when it is restored on the destination node). This is be-
cause, although routing protocols can usually tolerate a brief
network glitch using retransmission (e.g., BGP uses TCP
retransmission, while OSPF uses its own reliable retrans-
mission mechanism), a long control-plane outage can break
protocol adjacencies and cause protocols to reconverge.
We now describe how VROOM leverages virtual machine
(VM) migration techniques to migrate the control plane in
steps 2 (router-image copy) and 3 (memory copy) of its mi-
gration process, as shown in Figure 4.
Unlike general-purpose VMs that can potentially be run-
ning completely diﬀerent programs, virtual routers from the
same vendor run the same (usually small) set of programs
(e.g., routing protocol suites). VROOM assumes that the
same set of binaries are already available on every physi-
cal router. Before a virtual router is migrated, the bina-
ries are locally copied to its ﬁle system on the destination
node. Therefore, only the router conﬁguration ﬁles need to
be copied over the network, reducing the total migration
time (as local-copy is usually faster than network-copy).
The simplest way to migrate the memory of a virtual
router is to check-point the router, copy the memory pages
to the destination, and restore the router, a.k.a. stall-and-
copy [24]. This approach leads to downtime that is propor-
tional to the memory size of the router. A better approach
is to add an iterative pre-copy phase before the ﬁnal stall-
and-copy [12], as shown in Figure 4. All pages are trans-
ferred in the ﬁrst round of the pre-copy phase, and in the
following rounds, only pages that were modiﬁed during the
previous round are transferred. This pre-copy technique re-
duces the number of pages that need to be transfered in the
stall-and-copy phase, reducing the control plane downtime
of the virtual router (i.e., the control plane is only “frozen”
between t3 and t4 in Figure 4).
4.2.2 Data-Plane Cloning
The control-plane migration described above could be ex-
tended to migrate the data plane, i.e., copy all data-plane
states over to the new physical node. However, this approach
has two drawbacks. First, copying the data-plane states
(e.g., FIB and ACLs) is unnecessary and wasteful, because
the information that is used to generate these states (e.g.,
RIB and conﬁguration ﬁles) is already available in the con-
trol plane. Second, copying the data-plane state directly can
be diﬃcult if the source and destination routers use diﬀerent
data-plane technologies. For example, some routers may use
TCAM (ternary content-addressable memory) in their data
planes, while others may use regular SRAM. As a result, the
data structures that hold the state may be diﬀerent.
VROOM formalizes the interface between the control and
data planes by introducing a data-plane hypervisor, which
allows a migrated control plane to re-instantiate the data
plane on the new platform, a process we call data-plane
cloning. That is, only the control plane of the router is
actually migrated. Once the control plane is migrated to
the new physical router, it clones its original data plane by
repopulating the FIB using its RIB and reinstalling ACLs
and other data-plane states2 through the data-plane hyper-
visor (as shown in Figure 2). The data-plane hypervisor
provides a uniﬁed interface to the control plane that hides
the heterogeneity of the underlying data-plane implementa-
tions, enabling virtual routers to migrate between diﬀerent
types of data planes.
4.2.3 Remote Control Plane
As shown in Figure 3(b), after VR1’s control plane is mi-
grated from A to B, the natural next steps are to repopu-
late (clone) the data plane on B and then migrate the links
from A to B. Unfortunately, the creation of the new data
plane can not be done instantaneously, primarily due to the
time it takes to install FIB entries. Installing one FIB en-
try typically takes between one hundred and a few hundred
microseconds [5]; therefore, installing the full Internet BGP
routing table (about 250k routes) could take over 20 sec-
onds. During this period of time, although data traﬃc can
still be forwarded by the old data plane on A, all the rout-
ing instances in VR1’s control plane can no longer send or
receive routing messages. The longer the control plane re-
mains unreachable, the more likely it will lose its protocol
adjacencies with its neighbors.
To overcome this dilemma, A’s substrate starts redirect-
ing all the routing messages destined to VR1 to B at the
end of the control-plane migration (time t4 in Figure 4).
This is done by establishing a tunnel between A and B for
each of VR1’s substrate interfaces. To avoid introducing any
additional downtime in the control plane, these tunnels are
established before the control-plane migration, as shown in
Figure 3(a). With this redirection mechanism, VR1’s con-
trol plane not only can exchange routing messages with its
neighbors, it can also act as the remote control plane for
its old data plane on A and continue to update the old FIB
when routing changes happen.
4.2.4 Double Data Planes
In theory, at the end of the data-plane cloning step, VR1
can switch from the old data plane on A to the new one
on B by migrating all its links from A to B simultaneously.
However, performing accurate synchronous link migration
across all the links is challenging, and could signiﬁcantly
increase the complexity of the system (because of the need
to implement a synchronization mechanism).
Fortunately, because VR1 has two data planes ready to
2Data dynamically collected in the old data plane (such
as NetFlow) can be copied and merged with the new one.
Other path-speciﬁc statistics (such as queue length) will be
reset as the previous results are no longer meaningful once
the physical path changes.
236Quagga
VE1
bgpd
ospfd
shadowd
VE2
VE3
shadowd
shadowd
iproute2
zebra
kernel routing table
Control plane
Data plane
VE0
(the root context)
virtd
kernel, but have separate virtualized resources such as name
spaces, process trees, devices, and network stacks. OpenVZ
also provides live migration capability for running VEs3.
In the rest of this subsection, we describe in a top-down
order the three components of our two prototypes that en-
able virtual router migration. We ﬁrst present the mecha-
nism that separates the control and data planes, and then
describe the data-plane hypervisor that allows the control
planes to update the FIBs in the shared data plane. Finally,
we describe the mechanisms that dynamically bind the in-
terfaces with the FIBs and set up the data path.
table1
table2
table3
5.1.1 Control and Data Plane Separation
To mimic the control and data plane separation provided
in commercial routers, we move the FIBs out of the VEs and
place them in a shared but virtualized data plane, as shown
in Figure 5. This means that packet forwarding no longer
happens within the context of each VE, so it is unaﬀected
when the VE is migrated.
As previously mentioned, we have implemented two proto-
types with diﬀerent types of data planes—a software-based
data plane (SD) and a hardware-based data plane (HD). In
the SD prototype router, the data plane resides in the root
context (or “VE0”) of the system and uses the Linux kernel
for packet forwarding. Since the Linux kernel (2.6.18) sup-
ports 256 separate routing tables, the SD router virtualizes
its data plane by associating each VE with a diﬀerent kernel
routing table as its FIB.
In the HD router implementation, we use the NetFPGA
platform conﬁgured with the reference router provided by
Stanford [23]. The NetFPGA card is a 4-port gigabit eth-
ernet PCI card with a Virtex 2-Pro FPGA on it. With the
NetFPGA as the data plane, packet forwarding in the HD
router does not use the host CPU, thus more closely resem-
bling commercial router architectures. The NetFPGA ref-
erence router does not currently support virtualization. As
a result, our HD router implementation is currently limited
to only one virtual router per physical node.
5.1.2 Data-Plane Hypervisor
As explained in Section 4, VROOM extends the stan-
dard control plane/data plane interface to a migration-aware
data-plane hypervisor. Our prototype presents a rudimen-
tary data-plane hypervisor implementation which only sup-
ports FIB updates.
(A full-ﬂedged data-plane hypervisor
would also allow the conﬁguration of other data plane states.)
We implemented the virtd program as the data-plane hy-
pervisor. virtd runs in the VE0 and provides an interface
for virtual routers to install/remove routes in the shared
data plane, as shown in Figure 5. We also implemented the
shadowd program that runs inside each VE and pushes route
updates from the control plane to the FIB through virtd.
We run the Quagga routing software suite [26] as the con-
trol plane inside each VE. Quagga supports many routing
protocols, including BGP and OSPF. In addition to the in-
cluded protocols, Quagga provides an interface in zebra,
its routing manager, to allow the addition of new protocol
daemons. We made use of this interface to implement shad-
owd as a client of zebra. zebra provides clients with both
3The current OpenVZ migration function uses the simple
“stall-and-copy” mechanism for memory migration. Includ-
ing a “pre-copy” stage [12] in the process will reduce the
migration downtime.
Linux
or
NetFPGA
bindd
Figure 5: The design of the VROOM prototype
routers (with two types of data planes)
forward traﬃc at the end of the data-plane cloning step (Fig-
ure 4), the migration of its links does not need to happen
all at once. Instead, each link can be migrated independent
of the others, in an asynchronous fashion, as shown in Fig-
ure 3(c) and (d). First, router B creates a new outgoing link
to each of VR1’s neighbors, while all data traﬃc continues
to ﬂow through router A. Then, the incoming links can be
safely migrated asynchronously, with some traﬃc starting to
ﬂow through router B while the remaining traﬃc still ﬂows
through router A. Finally, once all of VR1’s links are mi-
grated to router B, the old data plane and outgoing links on
A, as well as the temporary tunnels, can be safely removed.
5. PROTOTYPE IMPLEMENTATION
In this section, we present the implementation of two
VROOM prototype routers. The ﬁrst is built on commod-
ity PC hardware and the Linux-based virtualization solution
OpenVZ [24]. The second is built using the same software
but utilizing the NetFPGA platform [23] as the hardware
data plane. We believe the design presented here is readily
applicable to commercial routers, which typically have the
same clean separation between the control and data planes.
Our prototype implementation consists of three new pro-
grams, as shown in Figure 5. These include virtd, to enable
packet forwarding outside of the virtual environment (con-
trol and data plane separation); shadowd, to enable each
VE to install routes into the FIB; and bindd (data plane
cloning), to provide the bindings between the physical inter-
faces and the virtual interfaces and FIB of each VE (data-
plane hypervisor). We ﬁrst discuss the mechanisms that
enable virtual router migration in our prototypes and then
present the additional mechanisms we implemented that re-
alize the migration.
5.1 Enabling Virtual Router Migration
We chose to use OpenVZ [24], a Linux-based OS-level vir-
tualization solution, as the virtualization environment for
our prototypes. As running multiple operating systems for
diﬀerent virtual routers is unnecessary, the lighter-weight
OS-level virtualization is better suited to our need than
other virtualization techniques, such as full virtualization
and para-virtualization. In OpenVZ, multiple virtual envi-
ronments (VEs) running on the same host share the same
237the ability to notify zebra of route changes and to be noti-
ﬁed of route changes. As shadowd is not a routing protocol
but simply a shadowing daemon, it uses only the route re-
distribution capability. Through this interface, shadowd is
notiﬁed of any changes in the RIB and immediately mirrors
them to virtd using remote procedure calls (RPCs). Each
shadowd instance is conﬁgured with a unique ID (e.g., the
ID of the virtual router), which is included in every message
it sends to virtd. Based on this ID, virtd can correctly
install/remove routes in the corresponding FIB upon receiv-
ing updates from a shadowd instance. In the SD prototype,
this involves using the Linux iproute2 utility to set a rout-
ing table entry. In the HD prototype, this involves using the
device driver to write to registers in the NetFPGA.
5.1.3 Dynamic Interface Binding
With the separation of control and data planes, and the
sharing of the same data plane among multiple virtual routers,
the data path of each virtual router must be set up properly
to ensure that (i) data packets can be forwarded according
to the right FIB, and (ii) routing messages can be delivered
to the right control plane.
We implemented the bindd program that meets these re-
quirements by providing two main functions. The ﬁrst is
to set up the mapping between a virtual router’s substrate
interfaces and its FIB after the virtual router is instantiated
or migrated, to ensure correct packet forwarding. (Note that
a virtual router’s substrate interface could be either a ded-
icated physical interface or a tunnel interface that shares
the same physical interface with other tunnels.) In the SD
prototype, bindd establishes this binding by using the rout-
ing policy management function (i.e., “ip rule”) provided by
the Linux iproute2 utility. As previously mentioned, the
HD prototype is currently limited to a single table. Once
NetFPGA supports virtualization, a mechanism similar to
the “ip rule” function can be used to bind the interfaces with
the FIBs.
The second function of bindd is to bind the substrate in-
terfaces with the virtual interfaces of the control plane. In
both prototypes, this binding is achieved by connecting each
pair of substrate and virtual interfaces to a diﬀerent bridge
using the Linux brctl utility.
In the HD prototype, each
of the four physical ports on the NetFPGA is presented to
Linux as a separate physical interface, so packets destined
to the control plane of a local VE are passed from the NetF-
PGA to Linux through the corresponding interface.
5.2 Realizing Virtual Router Migration
The above mechanisms set the foundation for VROOM
virtual router migration in the OpenVZ environment. We
now describe the implementations of data-plane cloning, re-
mote control plane, and double data planes.
Although migration is transparent to the routing pro-
cesses running in the VE, shadowd needs to be notiﬁed at the
end of the control plane migration in order to start the “data
plane cloning”. We implemented a function in shadowd that,
when called, triggers shadowd to request zebra to resend all
the routes and then push them down to virtd to repopu-
late the FIB. Note that virtd runs on a ﬁxed (private) IP
address and a ﬁxed port on each physical node. Therefore,
after a virtual router is migrated to a new physical node, the
route updates sent by its shadowd can be seamlessly routed
to the local virtd instance on the new node.
To enable a migrated control plane to continue updating
the old FIB (i.e., to act as a “remote control plane”), we
implemented in virtd the ability to forward route updates
to another virtd instance using the same RPC mechanism
that is used by shadowd. As soon as virtual router VR1 is
migrated from node A to node B, the migration script no-
tiﬁes the virtd instance on B of A’s IP address and VR1’s
ID. B’s virtd, besides updating the new FIB, starts for-
warding the route updates from VR1’s control plane to A,
whose virtd then updates VR1’s old FIB. After all of VR1’s
links are migrated, the old data plane is no longer used, so
B’s virtd is notiﬁed to stop forwarding updates. With B’s
virtd updating both the old and new FIBs of VR1 (i.e.,
the “double data planes”), the two data planes can forward
packets during the asynchronous link migration process.
Note that the data-plane hypervisor implementation makes
the the control planes unaware of the details of a particular
underlying data plane. As as result, migration can occur
between any combination of our HD and SD prototypes (i.e.
SD to SD, HD to HD, SD to HD, and HD to SD).
6. EVALUATION
In this section, we evaluate the performance of VROOM
using our SD and HD prototype routers. We ﬁrst measure