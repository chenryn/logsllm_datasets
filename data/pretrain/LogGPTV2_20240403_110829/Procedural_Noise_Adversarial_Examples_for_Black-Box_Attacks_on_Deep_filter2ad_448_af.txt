Experiment. We retrain Inception v3 with a median denoising
(cid:128)lter applied as a pre-processing step. For this evaluation, we use
12
the same 1,000 of each procedural noise, 1,000 uniform random
perturbations, and 5,000 validation set points as in Sect. 4. We
test how denoising improves robustness for (cid:96)∞ a(cid:138)ack norms ε =
4, 8, 12, and 16. (cid:140)e decrease in the number of uniform random
perturbations is due to results in Sect. 4, where we found that its
universal evasion rate had very low variance across 10,000 samples,
and our results show it is similar for 1,000 samples.
Results. (cid:140)e min-max oscillations present in the procedural
noise perturbations may have allowed the noise pa(cid:138)erns to per-
sist despite the denoising, as the universal evasion rates are not
completely mitigated by the defence. Across the di(cid:130)erent (cid:96)∞-norm
values, the denoising decreased the median and mean universal eva-
sion rates by a consistent amount when compared with no defence:
7.2-10.8% for Gabor noise and 13.2-16.9% for Perlin noise. Fig. 4
shows that the decrease in e(cid:130)ectiveness of Perlin noise is much
greater than that for Gabor noise, suggesting that the denoising
appears to slightly mitigate the e(cid:130)ectiveness of high-frequency
noise pa(cid:138)erns.
(cid:140)is denoising defence grants a reasonable increase in robustness
despite the simplicity of the algorithm. However, this measure has
not fully mitigated the sensitivity to procedural noise. We hope
future work explore more input-agnostic defences that minimize
the sensitivity of large-scale models to small perturbations with
techniques like model compression [20], Jacobian regularization
[27, 30, 62, 69], or other types of denoising.
8 CONCLUSION
We highlight the strengths of procedural noise as an indiscrimi-
nate and inexpensive black-box a(cid:138)ack on DCNs. We have shown
that popular DCN architectures for image classi(cid:128)cation have sys-
temic vulnerabilities to procedural noise perturbations, with Perlin
noise UAPs that achieve 58% or larger universal evasion across non-
adversarially trained models. (cid:140)is weakness to procedural noise
can be used to cra(cid:137) e(cid:129)cient universal and input-speci(cid:128)c black-box
a(cid:138)acks. Our procedural noise a(cid:138)ack augmented with Bayesian
optimization a(cid:138)ains competitive input-speci(cid:128)c success rates whilst
improving the query e(cid:129)ciency of existing black-box methods over
100 times. Moreover, we show that procedural noise a(cid:138)acks also
work against the YOLO v3 model for object detection, where it has
an obfuscating e(cid:130)ect on the “person” class.
Our results have notable implications. (cid:140)e universality of our
black-box method introduces the possibility for large-scale a(cid:138)acks
on DCN-based machine learning services, and the use of Bayesian
optimization makes untargeted black-box a(cid:138)acks signi(cid:128)cantly more
e(cid:129)cient. We hypothesize that our procedural noise a(cid:138)acks exploit
low-level features that DCNs are sensitive towards. If true, this has
worrying implications on the safety of transfer learning, as it is
o(cid:137)en these low-level features that are preserved when retraining
models for new tasks. It may be the case that universal a(cid:138)acks
can be extended to other application domains by using compact
representations of UAPs that exploit the sensitivity of models to
low-level features.
We have shown the di(cid:129)culty of defending against such novel
approaches. In particular, ensemble adversarial training on gradient-
based methods was unable to signi(cid:128)cantly diminish our procedural
noise a(cid:138)ack. We suggest that future defences take more input-
agnostic approaches to avoid the costs in defending and retraining
against all possible a(cid:138)acks. Our work prompts the need for further
research of more intuitive and novel a(cid:138)ack frameworks that use
analogues of procedural noise in other machine learning applica-
tion domains such as audio processing and reinforcement learning.
We hope future work will explore in more depth the nature of cross-
model universal adversarial perturbations, as these vulnerabilities
generalize across both inputs and models, and a more formal frame-
work could be(cid:138)er explain why our procedural noise a(cid:138)acks are
e(cid:130)ective.
REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradients
Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.
arXiv preprint arXiv:1802.00420 (2018).
[2] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug
Tygar. 2006. Can Machine Learning Be Secure?. In Symposium on Information,
Computer and Communications Security. 16–25.
[3] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. 2018. Black-box A(cid:138)acks
on Deep Neural Networks via Gradient Estimation. (2018).
[11] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted
[10] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness
[4] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016).
[5] G. Bradski. 2000. (cid:140)e OpenCV Library. Dr. Dobb’s Journal of So(cid:135)ware Tools
(2000).
[6] Wieland Brendel, Jonas Rauber, and Ma(cid:138)hias Bethge. 2017. Decision-Based
Adversarial A(cid:138)acks: Reliable A(cid:138)acks Against Black-Box Machine Learning
Models. arXiv preprint arXiv:1712.04248 (2017).
[7] Eric Brochu, Vlad M Cora, and Nando De Freitas. 2010. A Tutorial on Bayesian
Optimization of Expensive Cost Functions, with Application to Active User Mod-
eling and Hierarchical Reinforcement Learning. arXiv preprint arXiv:1012.2599
(2010).
[8] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin.
2019. On Evaluating Adversarial Robustness. arXiv preprint arXiv:1902.06705
(2019).
[9] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Commands..
In USENIX Security Symposium. 513–530.
of Neural Networks. In Symposium on Security and Privacy. 39–57.
a(cid:138)acks on speech-to-text. arXiv preprint arXiv:1801.01944 (2018).
[12] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017. Zoo:
Zeroth Order Optimization Based Black-box A(cid:138)acks to Deep Neural Networks
without Training Substitute Models. In Workshop on Arti(cid:128)cial Intelligence and
Security. 15–26.
[13] Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, and Dacheng Tao. 2018. Towards
(cid:139)ery E(cid:129)cient Black-box A(cid:138)acks: An Input-free Perspective. In Proceedings of
the 11th ACM Workshop on Arti(cid:128)cial Intelligence and Security. ACM, 13–24.
[14] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Ma(cid:138)hias Bethge, Felix A
Wichmann, and Wieland Brendel. 2018.
ImageNet-trained CNNs are biased
towards texture; increasing shape bias improves accuracy and robustness. arXiv
preprint arXiv:1811.12231 (2018).
[15] Ross Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE international conference
on computer vision. 1440–1448.
Ian Goodfellow. 2018. Defense Against the Dark Arts: An Overview of Adver-
[16]
sarial Example Security Research and Future Research Directions. arXiv preprint
arXiv:1806.04169 (2018).
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT
Press.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572 (2014).
[19] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2016. Adversarial perturbations against deep neural networks
for malware classi(cid:128)cation. arXiv preprint arXiv:1606.04435 (2016).
[20] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing
deep neural networks with pruning, trained quantization and hu(cid:130)man coding.
arXiv preprint arXiv:1510.00149 (2015).
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
[18]
[17]
13
[27] Daniel Jakubovitz and Raja Giryes. 2018.
[33] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial Machine
vision and pa(cid:136)ern recognition. 770–778.
[22] Geo(cid:130)rey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep Neural Networks for Acoustic Modeling in Speech
Recognition: (cid:140)e Shared Views of Four Research Groups. IEEE Signal Processing
Magazine 29, 6 (2012), 82–97.
[23] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD
Tygar. 2011. Adversarial machine learning. In Workshop on Security and Arti(cid:128)cial
Intelligence. 43–58.
[24] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter
Abbeel. 2017. Adversarial a(cid:138)acks on neural network policies. arXiv preprint
arXiv:1702.02284 (2017).
[25] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2018. Prior convic-
tions: Black-box adversarial a(cid:138)acks with bandits and priors. arXiv preprint
arXiv:1807.07978 (2018).
[26] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
Tran, and Aleksander Madry. 2019. Adversarial Examples Are Not Bugs, (cid:140)ey
Are Features. arXiv preprint arXiv:1905.02175 (2019).
Improving DNN robustness to ad-
versarial a(cid:138)acks using Jacobian regularization. In Proceedings of the European
Conference on Computer Vision (ECCV). 514–529.
[28] Ahmad Javaid, (cid:139)amar Niyaz, Weiqing Sun, and Mansoor Alam. 2016. A deep
learning approach for network intrusion detection system. In Proceedings of the
9th EAI International Conference on Bio-inspired Information and Communications
Technologies (formerly BIONETICS). ICST (Institute for Computer Sciences, Social-
Informatics and ?, 21–26.
[29] Min-Joo Kang and Je-Won Kang. 2016. Intrusion detection system using deep
neural network for in-vehicle network security. PloS one 11, 6 (2016), e0155781.
[30] Valentin Khrulkov and Ivan Oseledets. 2018. Art of Singular Vectors and Uni-
versal Adversarial Perturbations. In Procs. Conf. on Computer Vision and Pa(cid:136)ern
Recognition. 8562–8570.
[31] Alex Krizhevsky, Ilya Sutskever, and Geo(cid:130)rey E Hinton. 2012. Imagenet Clas-
si(cid:128)cation with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems. 1097–1105.
[32] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial Examples
in the Physical World. arXiv preprint arXiv:1607.02533 (2016).
Learning at Scale. arXiv preprint arXiv:1611.01236 (2016).
[34] Ares Lagae, Sylvain Lefebvre, Rob Cook, Tony DeRose, George Dre(cid:138)akis, David S
Ebert, John P Lewis, Ken Perlin, and Ma(cid:138)hias Zwicker. 2010. A Survey of
Procedural Noise Functions. In Computer Graphics Forum, Vol. 29. 2579–2600.
[35] Ares Lagae, Sylvain Lefebvre, George Dre(cid:138)akis, and Philip Dutr´e. 2009. Pro-
cedural noise using sparse Gabor convolution. ACM Transactions on Graphics
(TOG) 28, 3 (2009), 54.
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. 2014. Microso(cid:137) coco: Common
objects in context. In European conference on computer vision. Springer, 740–755.
[37] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu,
and Min Sun. 2017. Tactics of adversarial a(cid:138)ack on deep reinforcement learning
agents. arXiv preprint arXiv:1703.06748 (2017).
large scale optimization. Mathematical programming 45, 1-3 (1989), 503–528.
[39] Andr´e Teixeira Lopes, Edilson de Aguiar, Alberto F De Souza, and (cid:140)iago Oliveira-
Santos. 2017. Facial expression recognition with convolutional neural networks:
coping with few data and the training sample order. Pa(cid:136)ern Recognition 61 (2017),
610–628.
[40] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards Deep Learning Models Resistant to Adversarial
A(cid:138)acks. arXiv preprint arXiv:1706.06083 (2017).
in Adversarial Se(cid:138)ings. IEEE Security & Privacy 14, 3 (2016), 68–72.
Processes for Bayesian Optimization.. In UAI.
J Mo´ckus, V Tiesis, and A ´Zilinskas. 1978. (cid:140)e Application of Bayesian Methods
for Seeking the Extremum. Vol. 2. Towards Global Optimization 2 (1978), 117–129.
[44] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal Adversarial Perturbations. In Conference on Computer
Vision and Pa(cid:136)ern Recognition. 86–94.
[45] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: a Simple and Accurate Method to Fool Deep Neural Networks. In
Conference on Computer Vision and Pa(cid:136)ern Recognition. 2574–2582.
[46] Konda Mopuri, Utkarsh Ojha, Utsav Garg, and R. Venkatesh Babu. 2018. NAG:
Network for Adversary Generation. In Procs. Conf. on Computer Vision and Pa(cid:136)ern
Recognition. 742–751.
[47] Luis Mu˜noz Gonz´alez and Emil C Lupu. 2018. (cid:140)e Secret of Machine Learning.
ITNOW 60, 1 (2018), 38–39.
[38] Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for
[42] Mitchell McIntire, Daniel Ratner, and Stefano Ermon. 2016. Sparse Gaussian
[41] Patrick McDaniel, Nicolas Papernot, and Z Berkay Celik. 2016. Machine Learning
[43]
[48] Fabrice Neyret and Eric Heitz. 2016. Understanding and Controlling Contrast
Oscillations in Stochastic Texture Algorithms using Spectrum of Variance. Ph.D.
Dissertation. LJK/Grenoble University-INRIA.
[49] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visual-
ization. Distill 2, 11 (2017), e7.
[50] Maxime Oquab, Leon Bo(cid:138)ou, Ivan Laptev, and Josef Sivic. 2014. Learning
and transferring mid-level image representations using convolutional neural
networks. In Proceedings of the IEEE conference on computer vision and pa(cid:136)ern
recognition. 1717–1724.
[51] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability
in Machine Learning: From Phenomena to Black-box A(cid:138)acks using Adversarial
Samples. arXiv preprint arXiv:1605.07277 (2016).
[52] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik,
and Ananthram Swami. 2017. Practical Black-box A(cid:138)acks Against Machine
Learning. In Asia Conference on Computer and Communications Security. 506–
519.
[53] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael P Wellman.
2018. SoK: Security and Privacy in Machine Learning. In European Symposium
on Security and Privacy. 399–414.
[54] Ken Perlin. 1985. An Image Synthesizer. ACM Siggraph Computer Graphics 19, 3
(1985), 287–296.
[55] Ken Perlin. 2002.
(2002). h(cid:138)ps:
Improved noise reference implementation.
//mrl.nyu.edu/∼perlin/noise/ Accessed: 2018-06-09.
[64]
[65]
[59]
[60]
681–682.
[56] Ken Perlin. 2002. Improving Noise. In ACM Transactions on Graphics, Vol. 21.
[57] Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes
for Machine Learning. (cid:140)e MIT Press.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You
[58]
only look once: Uni(cid:128)ed, real-time object detection. In Proceedings of the IEEE
conference on computer vision and pa(cid:136)ern recognition. 779–788.
Joseph Redmon and Ali Farhadi. 2017. YOLO9000: be(cid:138)er, faster, stronger. In
Proceedings of the IEEE conference on computer vision and pa(cid:136)ern recognition.
7263–7271.
Joseph Redmon and Ali Farhadi. 2018. Yolov3: An incremental improvement.
arXiv preprint arXiv:1804.02767 (2018).
[61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91–99.
[62] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio.
2011. Contractive auto-encoders: Explicit invariance during feature extraction.
In Proceedings of the 28th International Conference on International Conference on
Machine Learning. Omnipress, 833–840.
[63] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition challenge. International Journal of
Computer Vision 115, 3 (2015), 211–252.