ﬁes as non-policed almost all segments suffering from other
common network effects, including network bottlenecks such
as a congested link with packets dropped due to AQM (D)
or drop-tail policy (E, G, H), and random packet loss (F). PD
is able to rule out policing because it checks for consistent
policing behavior across many RTTs, and other network ef-
fects rarely induce loss patterns that consistently mimic the
policing signature over time. For example, when congestion
overﬂows a queue, it drops packets similar to a policer that
has exhausted tokens. However, over time congestion will
not always happen at exactly the same moment as a policer
enforcing the rate limit for a speciﬁc ﬂow.
A closer look at the single-ﬂow congestion cases shows
that only trials using the minimum conﬁgurable queue size
(8 kB) cause misclassiﬁcations (G). This is because a bot-
tleneck with almost no available queue size to temporarily
accommodate bursts results in the same packet loss patterns
as trafﬁc passing through a policer. However, in the wild
(§4), 90% of the traces tagged as policed temporarily sus-
tain larger bursts of 30 kB or more and therefore cannot fall
in this category of false positives. In addition, a few cases
of congestion from background trafﬁc (H) induced loss pat-
terns that were misclassiﬁed as policing. These cases have
inferred bottleneck rates that vary widely, whereas we show
in §3.2 that, in the wild, traces we classiﬁed as policed clus-
ter around only a handful of goodput rates per AS. Note
that a ﬂow in the wild might experience more complex con-
gestion dynamics, e.g., when contending with hundreds of
other ﬂows at a router. However, these dynamics are un-
likely to result in a per-chunk trafﬁc pattern consistent with
a policer enforcing a rate (e.g., where losses always happen
when exceeding a certain throughput rate), and, even if there
are cases where chunks are misclassiﬁed as policed, we do
not expect this to happen consistently for a large number of
chunks within an AS.
Finally we validated our algorithm against traces gener-
ated by Kakhki et al. [32]. These traces were also generated
with carrier grade equipment, conﬁgured to perform trafﬁc
shaping only. As such, none of the traces should be labeled
as policed by our tool. The 1,104 traces we analyzed con-
tained 205,652 data chunks, of which only 37 chunks were
falsely marked as policed by PD. This represents an accuracy
of 99.98% for this dataset.
 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30 35 40 45 50CDF of ASes# clusters required for 75% coverageASes w/ policingASes w/o policing3.2 Consistency of Policing Rates
Our case studies (discussed later in §4.5) suggest that polic-
ing rates are often tied to advertised data plan rates. Thus we
conjectured that, because most ASes have few plan rates, we
should observe few policing rates per AS. To validate this
conjecture, we computed the number of prevalent policing
rates seen per AS, based on traces from most of Google’s
CDN servers (see §4). We derived the minimum number of
rate clusters required to cover at least 75% of the policed
traces per AS. We deﬁne a rate cluster with center value v
as all rates falling into the range [0.95 · v, 1.05 · v]. For ex-
ample, the 1-Mbps cluster incorporates all rates that are ≥
0.95 Mbps and ≤ 1.05 Mbps. To ﬁnd a solution, we use
the greedy algorithm for the partial set cover problem which
produces a good approximation of the optimal solution [40].
We looked at the distribution of goodput rates for seg-
ments marked as policed in ASes with at least 3% of their
trafﬁc being policed. Rates in the majority of ASes can be
accounted for by 10 clusters or less (Figure 3). By visiting
ISP homepages, we observe that many offer a range of data
rates, some with reduced rates for data overages. Further,
many ISPs continue to support legacy rates. Thus it is not
surprising that we see more than just a couple of policing
rates for most ASes. In contrast, goodput rates in ASes with
no policing do not display clustering around a small num-
ber of rates and see a much wider spread. Since the false
positives in our lab validation see a wide spread as well, this
result provides us conﬁdence that the traces we marked as
policed in our production dataset are mostly true positives.
4. POLICING IN THE WILD
In this section, we characterize the prevalence and impact
teract poorly with TCP dynamics (§4.4).
of policing in the Internet.
The dataset. We analyze sampled data collected from most
of Google’s CDN servers during a 7-day period in Septem-
ber 2015. The dataset consists of over 277 billion TCP pack-
ets, carrying 270 TB of data, associated with more than 800
million HTTP queries requested by clients in over 28,400
ASes. The TCP ﬂows carried different types of content, in-
cluding video segments associated with 146 million video
playbacks. The dataset is a sampled subset (based on ﬂow ID
hashing) of Google’s content delivery trafﬁc. To tie TCP per-
formance to application performance, we analyze the data at
a ﬂow segment granularity. A segment consists of the pack-
ets carrying an application request and its response (includ-
ing ACKs).
Overview of Results.
present our key ﬁndings:
In the following sub-sections, we
• Especially in Africa, a sizable amount of throttled traf-
ﬁc is limited to a rate of 2 Mbps or less, often inhibiting
the delivery of HD quality content (§4.1).
• Policing can result in excessive loss (§4.2).
• The user quality of experience suffers with policing, as
• Policing can induce patterns of trafﬁc and loss that in-
measured by more time spent rebuffering (§4.3).
Figure 5: Observed policing rates per segment.
• Through ISP case studies, we reveal interesting polic-
ing behavior and its impact, including losses on long-
distance connections. We also conﬁrm that policing is
often used to enforce data plans (§4.5).
As an aside, we conducted a supplemental study on the
publicly available M-Lab NDT dataset7 using the same de-
tection algorithm [1, 23]. The results from the NDT dataset
support this paper’s ﬁndings in terms of the prevalence and
impact of policing in the wild. Our technical report includes
further analysis of policing rates within individual ISPs, per-
country breakdowns, and longitudinal trends seen over the
past seven years.
4.1 The Prevalence of Policing
A macroscopic analysis of the data (Table 2) shows that,
depending on geographic region, between 2% and 6.8% of
lossy segments were impacted by policing. Overall, between
0.2% and 1.4% of the segments were affected.
Which policing rates are prevalent across the globe? Fig-
ure 5 shows the rates enforced by policers. In Africa and In-
dia, over 30% of the policed segments are throttled to rates
of 2 Mbps or less. The most frequent policing rates in these
two regions are 1, 2, and 10 Mbps, as is evident from the pro-
nounced inﬂections in the CDF. In §4.5 we examine some
ISPs to demonstrate that this step-wise pattern of policing
rates that emerge in the data reﬂects the available data plans
within each ISP. The distributions in other regions of the
world show no dominant rates, with many segments being
permitted to transmit at rates exceeding 10 Mbps. This is
due to aggregation effects:
these regions have many ISPs
with a wide variety of data plans. That said, even in these re-
gions, at least 20% of segments stream at less than 5 Mbps.
4.2 Impact of Policing on the Network
Policing noticeably increases the packet loss rate, which
can in turn affect TCP performance [22, 68] and user satis-
faction.8 In our dataset, we observed an average packet loss
7http://measurementlab.net/tools/ndt
8To ensure that packet loss is caused by policing instead of only being cor-
related with it (e.g., in the case where policing would be employed as a
remedy to excessive congestion in a network), we compared the perfor-
mance of policed and unpoliced ﬂows within an AS (for a few dozen of
the most policed ASes). We veriﬁed that policed connections observed low
throughput yet high loss rates. Conversely unpoliced connections achieved
high throughput at low loss rates. In addition, we did not observe any diur-
nal patterns – loss rates and the fraction of trafﬁc impacted by policing are
473
 0 0.2 0.4 0.6 0.8 1 1 10 100CDFPolicing Rate (in Mbps)AfricaIndiaSouth AmericaAsia (w/o India)EuropeNorth AmericaAustralia(a) Asia
(b) Africa and Australia
(c) Europe and Americas
Figure 4: Distribution of loss rates observed on unpoliced (N) and policed (P) segments in different regions of the world.
Figure 6: Loss rate CDF per segment,
for segments with avg. goodput of 0.5
or 5 Mbps, and per ASN.
rate of 22% per segment for policed ﬂows (Table 2).
Figure 7: Ratio between the median
burst throughput and the policing rate
per segment.
Figure 8: Wait time CDF for all HD seg-
ments (red solid line) and those policed
below 2.5 Mbps (blue dotted line).
Figure 4 plots the loss rate CDF for policed and non-
policed segments observed in different regions. Policed ﬂows
in Africa and Asia see a median loss rate of at least 10%,
whereas the median for unpoliced ﬂows is 0%. Other regions
witness lower loss rates, yet a sizable fraction of segments in
each experiences rates of 20% or more. The 99th percentile
in all regions is at least 40%, i.e., almost every other packet
is a retransmission. In §4.4 we analyze common trafﬁc pat-
terns that can trigger such excessive loss rates.
The loss rate distributions shown in Figure 6 see a wide
variability with long tails: the overall loss rate distribution
(All Segments) has a median of 0% and a 99th percentile of
over 25%. The ﬁgure also shows the distribution for two
segment subsets: one including the 20 million requests with
an average goodput of 0.5 Mbps (±50 kbps), and the other
with the 7 million requests achieving 5 Mbps (±50 kbps).
Though there is some correlation between goodput and loss
rates, there are many cases where high loss did not result in
bad performance. For example, about 4% of the segments
achieving a goodput of 5 Mbps also observe a loss rate of
10% or more. Policers are one cause for the uncommon high
loss, high goodput behavior, as we show in §4.4.
One situation that can trigger high loss is when there is
a wide gap between the rate sustained by a ﬂow’s bottle-
neck link and the rate enforced by the policer. We esti-
mate the bottleneck capacity (or the burst throughput) by
evaluating the interarrival time of ACKs for a burst of pack-
ets [13, 28, 35]. We found that in many cases the bottleneck
capacity, and sometimes even the goodput rate achieved be-
fore the policer starts dropping packets is 1-2 orders of mag-
nitude higher than the policing rate. Figure 7 compares the
not affected by the presence of peak times. §3 provides additional evidence
that policers are the root cause for losses and not the other way round.
474
achieved burst throughput and policing rates we observed.
The gap is particularly wide in Africa and India. With such
large gaps, when the policer starts to drop packets, the sender
may already be transmitting at several times the policing
rate. Since the sender’s congestion control mechanism usu-
ally only halves the transmission rate each round trip, it needs
multiple round trips to sufﬁciently reduce the rate to prevent
further policer packet drops. We investigate this and other
interactions with TCP in §4.4.
When policers drop large bursts of packets, the sender
can end up retransmitting the same packets multiple times.
Overshooting the policing rate by a large factor means that
retransmissions as part of Fast Recovery or FACK Recov-
ery [44] are more likely to also be lost, since the transmission
rate does not decrease quickly enough. The same applies
to cases where policing results in a retransmission timeout
(RTO) followed by Slow Start. In this situation, the token
bucket accumulated tokens before the RTO ﬁred, leading to
a few rounds of successful retransmissions before the expo-
nential slow start growth results in overshooting the polic-
ing rate again, requiring retransmissions of retransmissions.
Multiple rounds of this behavior can be seen in Figure 1.
These loss pathologies can be detrimental to both ISPs and
content providers. Policing-induced drops force the content
provider to transmit, and ISPs to carry, signiﬁcant retrans-
mission trafﬁc. This motivates our exploration of more be-
nign rate-limiting approaches in the §5.
4.3 Impact on Playback Quality
In addition to the large overheads caused by excessive
packet loss, policing has a measurable impact on the user’s
quality of experience. Figure 9 shows, for a selection of
playbacks delivered at different goodput rates, the distribu-
tion of the ratio of time spent rebuffering to time spend watch-
ing. This ratio is an established metric for playback quality
 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5CDFLoss RateIndia (N)India (P)Asia w/o India (N)Asia w/o India (P) 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5CDFLoss RateAfrica (N)Africa (P)Australia (N)Australia (P) 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5CDFLoss RateEurope (N)Europe (P)Americas (N)Americas (P) 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5CDFRetransmitted packets / all packetsAll Segments0.5Mbps Goodput5Mbps GoodputPer ASN 0 0.2 0.4 0.6 0.8 1 1 10 100CDFRatio between Burst Throughput and Policing RateAustraliaAmericasEuropeAsia (w/o India)AfricaIndia 0 0.2 0.4 0.6 0.8 1 0 0.5 1 1.5 2 2.5 3 3.5 4CDFHD Wait Time (s)AllPoliced < 2.5Mbps(a) 300 kbps
(b) 1.5 Mbps
(c) 3 Mbps
Figure 9: Rebuffer to watch time ratios for video playbacks. Each had at least one chunk with a goodput of 300 kbps, 1.5 Mbps, or
3 Mbps (±15%).
(a) Congestion avoidance pattern
(b) Staircase pattern
(c) Doubling window pattern
Figure 8 shows that delivering even a single HD segment
over a slow connection results in larger wait times. In the
median, a client has to wait over 1 second for a policed seg-
ment, whereas the median for unpoliced ones is only 10 ms.
Figure 10: Common trafﬁc patterns when a trafﬁc policer enforces throughput rates. The plots show progress over time (blue solid
line) with a steeper slope representing a higher goodput, the transmitted but lost sequences (red dotted lines), and the estimated
policing rate (black dashed line). Packets which would put the progress line above the policing rate line are dropped while other
packets pass through successfully.
and previous studies found a high correlation between this
metric and user engagement [18]. Each of the selected play-
backs had at least one of their video segments delivered at
a goodput rate of either 300 kbps, 1.5 Mbps, or 3 Mbps
(±15%). 300 kbps is the minimum rate required to play
videos of the lowest rendering quality, leaving little opportu-
nity to bridge delayed transmissions by consuming already
buffered data. For each selected rate, between 50% and 90%
of the playbacks do not see any rebuffer events. For the rest,
policed playbacks perform up to 200% worse than the un-
policed ones. For example, in the 90th percentile, playbacks
policed at a rate of ≈ 300 kbps spend over 15% of their time
rebuffering, vs. 5% when not policed. Prior work found that
a 1% increase in the rebuffering ratio can reduce user en-
gagement by 3 minutes [18]. This result substantiates our
claim that policing can have a measurable negative impact
on user experience.
4.4 Interaction Between Policers and TCP
Enabling trafﬁc policing itself does not automatically re-
sult in high loss. Thus, before we can design solutions to
avoid the negative side effects of policing, we need to have
a better understanding about when and why conﬁgurations
trigger heavy losses. We found that high loss is only ob-
served when the policer and TCP congestion control interact
poorly in speciﬁc settings. To depict these interactions, we
use the diagrams in Figure 10 that show speciﬁc patterns of
connection progress.
Congestion Avoidance Pattern. In the most benign interac-
tion we have seen, the policer induces few losses over long
time periods. The congestion window grows slowly, never
overshooting the policing rate by much. This results in short
loss periods, as shown in Figure 10a.