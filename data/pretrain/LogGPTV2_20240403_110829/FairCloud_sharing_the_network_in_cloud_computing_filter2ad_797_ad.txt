1:5 : 2 instead of a 1 : 1 ratio for a per VM proportionality. is
can be addressed by taking into account the demands when assign-
ing weights, but for brevity we do not detail that extension.
7ere is a subtle diﬀerence between the two cases; for case (a), the
weight does not include the VMs in Q; for (b), it does.
A1 B1 A2 B2 A3 B3 A4 B4 L1 L2 L3 L4 L5 L6 Proportionality
Min-Guarantee
Work Conservation
Utilization Incentives
Comm-Pattern Indep.
Symmetry
Per-Flow
Per-SD
(cid:2)
(cid:2)
p
p
(cid:2)
p
(cid:2)
(cid:2)
p
p
(cid:2)
p
Per-Source
(Per-Destination)
(cid:2)
(cid:2)
p
p
p
(cid:2)
Reservations
(e.g., [10])
(cid:2)
p
(cid:2)
p y
p
p
PS-L
link-proportionality
(cid:2)
p
p
p
p
PS-P
(cid:2)
p
p
p
p
p
PS-N
congestion-proportionality(cid:3)
(cid:2)
p
(cid:2)
p
p
Table 3: Properties achieved by diﬀerent network sharing policies. ((cid:3)in a restricted setting, y also strategy-proof)
one-to-one (e.g., as shown in the (cid:277)gure). In this case, the two ten-
ants achieve the same total bandwidth allocation (assuming high
enough demands), since the congestion is on the access links, which
have the same capacity and the same weight load. Indeed, the two
tenants will have equal weights on the access links: WA = 8(cid:2) 1
4 = 2,
since each VM will contribute with a weight of 1
4 to each VM to VM
(cid:280)ow, while WB = 4 (cid:1) 1
2. Note that PS-L would not provide network
wide proportionality in this case, since A’s weight would be twice
that of B on the access links.
A drawback of PS-N is that each VM’s weight is statically divided
across the (cid:280)ows with other VMs, irrespective of the traﬃc demands.
is further constraints the situations when PS-N achieves propor-
tionality (requiring high demands on all (cid:280)ows), as well as makes
PS-N lack the utilization incentives property. For example, assume
that all the links in Figure 7 have the same capacity and L5 is the
only congested link. If A deems A1’s traﬃc to A3 and A4 more im-
portant than that to A2, A may not send traﬃc between A1 and A2
to get a larger share for its traﬃc to A3 and A4 on L5. We believe
this could potentially be addressed if we include the demands when
assigning weights, e.g., by not providing any weight to (cid:280)ows travel-
ing uncongested paths. However, such a policy will be signi(cid:277)cantly
more diﬃcult to deploy, and we leave its exploration to future work.
+ (cid:12) WY
4.3 Proportional Sharing on Proximate Links
e previous policies strive to achieve forms of proportionality
and thus do not provide minimum bandwidth guarantees. To oﬀer
(useful) minimum bandwidth guarantees one would want to prior-
itize the allocation of a link based on the “importance” of that link
with respect to the VMs using it. For example, on the access link
of one host, we might want to allocate the link bandwidth based
more on the weights of the VMs residing on that host and less on
the weights of the remote VMs using the link.
Based on the above observation we generalize PS-L to derive the
following weighting scheme: WX(cid:0)Y = WY(cid:0)X = (cid:11) WX
NY . e
NX
coeﬃcients (cid:11) and (cid:12) allow diﬀerent weights for VMs located on the
two sides of the link. By setting speci(cid:277)c values for (cid:11) and (cid:12) at diﬀer-
ent links in the network, one can use the generalized PS-L to achieve
bandwidth guarantees for diﬀerent network topologies.
In this paper, we present Proportional Sharing on Proximate Links
(PS-P), which is suitable for tree-based topologies (e.g., traditional
data center architectures, VL2 [15], and multi-tree structures such
as fat trees [7]). PS-P prioritizes VMs that are close to a given link.
More precisely, PS-P uses (cid:11) = 1 and (cid:12) = 0 for all links in the tree
that are closer to X than Y, and (cid:11) = 0 and (cid:12) = 1 for all links closer
to Y than X.
In practice, PS-P translates into applying per-source fair sharing
for the traﬃc towards the root of the tree and per-destination fair
sharing for the traﬃc from the root. For example, on link L1 in Fig-
ure 7, the three (cid:280)ows communicating with A1 will share A1’s weight
irrespective of the weights of A2, A3, and A4, while on link L6, A1-A3
will have A3’s weight and A1-A4, A4’s weight. ese weights apply
equally to both directions of the traﬃc.
PS-P provides absolute bandwidth guarantees for any VM
to/from the root of the physical tree, since that VM competes on a
given link to the root only with the other VMs in the same subtree.
Obviously, cloud providers must deploy admission control to en-
sure that the total bandwidth guarantees of the VMs hosted within
the subtree of any link L does not exceed L’s capacity, similar to how
CPU guarantees are oﬀered on a given host. Guarantees are com-
puted assuming all hosts are fully loaded with VMs.
For example, in Figure 7, on link L1, B1 competes only with A1’s
weight irrespective of A1’s communication pattern. us, assum-
ing all VMs have have equal weights and that the maximum weight
on each host is 2, B1 is always allocated at least 1
2 of its access link
capacity (L1). Similarly, B1 will get at least 1
4 of L5.
ese guarantees can be used to oﬀer diﬀerent service models to
tenants. e basic model oﬀered by PS-P is similar to Oktopus’s
Virtual Cluster [10] (i.e., hose model), with the diﬀerence that PS-P
is work conserving. e cloud provider can associate a minimum
guaranteed bandwidth for every unit of VM weight and advertise it
to customers through diﬀerent VM con(cid:277)gurations (CPU, memory
and bandwidth). e guarantee is computed as the minimum share
across all the layers of the tree. Tenants entirely collocated within a
higher capacity subtree can achieve higher guarantees. e value of
the guarantees can vary from VM to VM and from tenant to tenant.
PS-P can also expose a service model similar to Oktopus’s vir-
tual oversubscribed cluster (VOC) [10]. Speci(cid:277)cally, the model
can expose higher guarantees between each group of VMs collo-
cated within a high-capacity subtree; the guarantees when commu-
nicating with VMs from a diﬀerent group are scaled down with the
oversubscription factor. Unfortunately, all models exposed by PS-
P would have the same oversubscription ratio, that of the physical
network. To (cid:277)x this, PS-P can be applied within virtual topologies
with diﬀerent oversubscription characteristics. (e virtual topolo-
gies could themselves be build with a PS-P-like mechanism.)
We assume that if VMs can communicate via multiple paths, the
routing protocol performs load balancing of the traﬃc across the
available paths. is assumption holds for many of the newly pro-
posed multi-tree topologies that use multi-path routing to fully uti-
lize the network bisection bandwidth, e.g., [7,8,15,23].
Similar to PS-L, PS-P can be implemented per tenant, which re-
duces the hardware requirements when PS-P is implemented at
switches (§4.5). In this case, the weight of tenant A’s queue through
link L equals the number of VMs of A located in the subtree commu-
nicating through L. For example, in Figure 7, on link L5, the weight
of A will be WA1 + WA2 (WA1 is A1’s weight). e disadvantage of
the per-tenant implementation is that the provided guarantees are
not for each VM, i.e., when buying a VM, the VM is not guaran-
teed a minimum amount of traﬃc to/from the network core; rather,
the tenant in aggregate is guaranteed that the bandwidth from all its
VMs to/from the core equals the sum of the minimum guarantees
that would be oﬀered to each of its VMs by the per-VM PS-P. We be-
lieve that by selecting suitable values for (cid:11) and (cid:12), one can generalize
PS-P to provide guarantees for other topologies, such as BCube [16]
or DCell [18]. We leave this to future work.
4.4 Summary
Table 3 summarizes the properties achieved by the allocation
policies presented in this section. All the described policies are work
conserving, symmetric, and oﬀer communication-pattern inde-
pendence, but they make diﬀerent choices for the rest of the prop-
erties. PS-L achieves link proportionality, does not provide guar-
antees, but does provide utilization incentives; PS-N achieves better
proportionality at the network level, but does not provide full in-
centives for high utilization; lastly, PS-P provides guarantees and
incentives for high utilization but not proportionality.
4.5 Deploying PS-L, PS-P and PS-N
We identify three deployment paths for the presented policies:
1. Full switch support. For this deployment, each switch must
provide a number of queues equal to or greater than the num-
ber of tenants communicating through it, and must support
weighted fair queuing. All the described policies can be im-
plemented with such switch support.
2. Partial switch support. PS-N and PS-P are amenable to imple-
mentation using CSFQ [26], which does not require support
for per-tenant or per-VM queues at switches.
3. No switch support. ere are two types of hypervisor-only
implementations. First, a centralized controller can enforce
rate-limiters at hypervisors based on the current network traf-
(cid:277)c and implement any of the proposed policies. Second, PS-N
(and we believe PS-P as well) could be implemented through
a distributed mechanism similar to Seawall [25].
In this paper, we focus on the full and partial switch support de-
ployments and only sketch the hypervisor-only deployment, leaving
it to future work. We present our evaluation in §5 and we present
more details related to practical deployment issues in §6.
4.6 Other Models
In addition to the (cid:280)at-rate per VM pricing model, PS-P can also
accommodate a per-byte pricing model, such as paying for the band-
width above the minimum guarantees (like DRP [9]), or simply pay-
ing for all bytes. We also note that if the per-byte price is constant
across tenants and traﬃc volume, proportionality still is a desirable
property. However, congestion pricing or other forms of price dis-
crimination can oﬀer alternative ways of sharing the network, which
would not bene(cid:277)t from proportionality.
Finally, we note that it is possible to create a more complex alloca-
tion policy that provides guarantees and shares proportionally only
the bandwidth unused by guarantees. In a nutshell, this allocation
would work as follows. Assume the total weight of all the VMs in
the subtree delimited by a link L (for which we provide guarantees
on L) is WG and the weight of the VMs currently active in the sub-
tree is WC. We note by WP = WG (cid:0) WC as the weight to be divided
proportionally (of the VMs not active). In this context, the weight
of tenant A on link L is WA = WAS + WP (cid:2) WAT
WT , where WAS is the
weight of A’s VMs in the subtree, while WAT and WT are the weights
of all A’s VMs, and all VMs, respectively, active on L.
5. EVALUATION
In this section we evaluate the allocation policies presented in §4.
We divide the results in three parts.
(cid:15) First, we consider a single congested link and explore the be-
havior of these policies under various scenarios (§5.1).
(cid:15) Second, we extend our experiments the network level, exem-
plify the aforementioned tradeoﬀs using suitable small-scale
examples, and observe how these policies behave (§5.2).
(cid:15) Finally, we leverage traces obtained from a 3200-node pro-
duction cluster at Facebook to validate the properties exhib-
ited by these policies on a large scale (§5.3).
We generated the results using a (cid:280)ow-level simulator written in
Java and validated the simulation results by experiments in the DE-
TERlab testbed [4]. For this purpose, we implemented switch sup-
port for PS-L, PS-P and PS-N using per-(cid:280)ow queues in a soware
router implemented in Click [20]. e implementation is using
kernel-mode Click and consists of (cid:24)500 lines of C++ code for the
new elements and (cid:24)2000 lines of python scripts to generate con(cid:277)g-
uration (cid:277)les. We also developed support for PS-N with CSFQ with