# sample dataset size 110830/12(batch_size) = 9235    raw_samples_data = get_dataset(samples_file_path)    test_dataset = raw_samples_data.take(1000)    train_dataset = raw_samples_data.skip(1000)**第三步是载入类别型特征。** 我们用到的类别型特征主要有这三类，分别是 genre、userId 和movieId。在载入 genre 类特征时，我们采用了`tf.feature_column.categorical_column_with_vocabulary_list`方法把字符串型的特征转换成了 One-hot特征。在这个转换过程中我们需要用到一个词表，你可以看到我在开头就定义好了包含所有genre 类别的词表 genre_vocab。在转换 userId 和 movieId 特征时，我们又使用了`tf.feature_column.categorical_column_with_identity`方法把 ID 转换成 One-hot 特征，这个方法不用词表，它会直接把 ID值对应的那个维度置为 1。比如，我们输入这个方法的 movieId 是 340，总的movie 数量是 1001，使用这个方法，就会把这个 1001 维的 One-hot movieId向量的第 340 维置为 1，剩余的维度都为0。 为了把稀疏的 One-hot 特征转换成稠密的 Embedding 向量，我们还需要在One-hot 特征外包裹一层 Embedding 层，你可以看到`tf.feature_column.embedding_column(movie_col, 10)`方法完成了这样的操作，它在把 movie one-hot 向量映射到了一个 10 维的Embedding 层上。    genre_vocab = ['Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy', 'Western', 'Documentary',                   'Sci-Fi', 'Drama', 'Thriller',                   'Crime', 'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical'    GENRE_FEATURES = {        'userGenre1': genre_vocab,        'userGenre2': genre_vocab,        'userGenre3': genre_vocab,        'userGenre4': genre_vocab,        'userGenre5': genre_vocab,        'movieGenre1': genre_vocab,        'movieGenre2': genre_vocab,        'movieGenre3': genre_vocab    }    categorical_columns = [    for feature, vocab in GENRE_FEATURES.items():        cat_col = tf.feature_column.categorical_column_with_vocabulary_list(            key=feature, vocabulary_list=vocab)        emb_col = tf.feature_column.embedding_column(cat_col, 10)        categorical_columns.append(emb_col)    movie_col = tf.feature_column.categorical_column_with_identity(key='movieId', num_buckets=1001)    movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)    categorical_columns.append(movie_emb_col)    user_col = tf.feature_column.categorical_column_with_identity(key='userId', num_buckets=30001)    user_emb_col = tf.feature_column.embedding_column(user_col, 10)    categorical_columns.append(user_emb_c**第四步是数值型特征的处理。** 这一步非常简单，我们直接把特征值输入到 MLP内，然后把特征逐个声明为 `tf.feature_column.numeric_column`就可以了，不需要经过其他的特殊处理。    numerical_columns = [tf.feature_column.numeric_column('releaseYear'),                       tf.feature_column.numeric_column('movieRatingCount'),                         tf.feature_column.numeric_column('movieAvgRating'),                         tf.feature_column.numeric_column('movieRatingStddev'),                         tf.feature_column.numeric_column('userRatingCount'),                         tf.feature_column.numeric_column('userAvgRating'),                         tf.feature_column.numeric_column('userRatingStddev')**第五步是定义模型结构。** 这一步的实现代码也非常简洁，我们直接利用 DenseFeatures把类别型 Embedding特征和数值型特征连接在一起形成稠密特征向量，然后依次经过两层 128维的全连接层，最后通过 sigmoid输出神经元产生最终预估值。    preprocessing_layer = tf.keras.layers.DenseFeatures(numerical_columns + categorical_columns)    model = tf.keras.Sequential([        preprocessing_layer,        tf.keras.layers.Dense(128, activation='relu'),        tf.keras.layers.Dense(128, activation='relu'),        tf.keras.layers.Dense(1, activation='sigmoid'),    ])**第六步是定义模型训练相关的参数。**在这一步中，我们需要设置模型的损失函数，梯度反向传播的优化方法，以及模型评估所用的指标。关于损失函数，我们使用的是二分类问题最常用的二分类交叉熵，优化方法使用的是深度学习中很流行的adam，最后是评估指标，使用了准确度 accuracy作为模型评估的指标。    model.compile(        loss='binary_crossentropy',        optimizer='adam',        metrics=['accuracy'])**第七步是模型的训练和评估。** TensorFlow 模型的训练过程和 Spark MLlib 一样，都是调用fit 函数，然后使用 evaluate函数在测试集上进行评估。不过，这里我们要注意一个参数epochs，它代表了模型训练的轮数，一轮代表着使用所有训练数据训练一遍，epochs=10代表着训练 10 遍。    model.fit(train_dataset, epochs=10)    test_loss, test_accuracy = model.evaluate(test_dataset)    print('\n\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy)如果一切顺利的话，你就可以看到模型的训练过程和最终的评估结果了。从下面的训练输出中你可以看到，每轮训练模型损失（loss）的变化过程和模型评估指标accruacy 的变化过程。你肯定会发现，随着每轮训练的 loss 减小，accruacy会变高。换句话说，每轮训练都会让模型结果更好，这是我们期望看到的。需要注意的是，理论上来说，我们应该在模型accuracy 不再变高时停止训练，据此来确定最佳的 epochs取值。但如果模型收敛的时间确实过长，我们也可以设置一个 epochs最大值，让模型提前终止训练。    Epoch 1/10    8236/8236 [==============================] - 20s 2ms/step - loss: 2.7379 - accuracy: 0.5815    Epoch 2/10    8236/8236 [==============================] - 21s 3ms/step - loss: 0.6397 - accuracy: 0.6659    Epoch 3/10    8236/8236 [==============================] - 21s 3ms/step - loss: 0.5550 - accuracy: 0.7179    Epoch 4/10    8236/8236 [==============================] - 21s 2ms/step - loss: 0.5209 - accuracy: 0.7431    Epoch 5/10    8236/8236 [==============================] - 21s 2ms/step - loss: 0.5010 - accuracy: 0.7564    Epoch 6/10    8236/8236 [==============================] - 20s 2ms/step - loss: 0.4866 - accuracy: 0.7641    Epoch 7/10    8236/8236 [==============================] - 20s 2ms/step - loss: 0.4770 - accuracy: 0.7702    Epoch 8/10    8236/8236 [==============================] - 21s 2ms/step - loss: 0.4688 - accuracy: 0.7745    Epoch 9/10    8236/8236 [==============================] - 20s 2ms/step - loss: 0.4633 - accuracy: 0.7779    Epoch 10/10    8236/8236 [==============================] - 20s 2ms/step - loss: 0.4580 - accuracy: 0.7800    1000/1000 [==============================] - 1s 1ms/step - loss: 0.5037 - accuracy: 0.7473    Test Loss 0.5036991238594055, Test Accuracy 0.747250020503997最终的模型评估需要在测试集上进行，从上面的输出中我们可以看到，最终的模型在测试集上的准确度是0.7472，它意味着我们的模型对 74.72%的测试样本作出了正确的预测。当然了，模型的评估指标还是很多，我们会在之后的模型评估篇中进行详细的讲解。小结这节课是我们深度学习模型实践的第一课，我们要掌握两个重点内容，一是Embedding+MLP 的模型结构，二是 Embedding+MLP 模型的 TensorFlow实现。 Embedding+MLP 主要是由 Embedding 部分和 MLP 部分这两部分组成，使用Embedding 层是为了将类别型特征转换成 Embedding 向量，MLP部分是通过多层神经网络拟合优化目标。具体来说，以微软的 Deep Crossing为例，模型一共分为 5 层，从下到上分别是 Feature 层、Embedding层、Stacking 层、MLP 层和 Scoring层。 在 TensorFlow 实践部分，我们利用上节课处理好的特征和训练数据，实现了SparrowRecsys项目中的第一个深度学习模型。在实践过程中，我们要重点掌握类别型特征的处理方法，模型的定义方式和训练方式，以及最后的模型评估方法。我也把这些重点知识总结在了一张表格里，你可以利用它来认真回顾。![](Images/ed0e0722b03f8fded2dbe1c01501d06d.png)savepage-src="https://static001.geekbang.org/resource/image/4e/67/4e34e77589d386c8924542794dyy1867.jpg"}今天，我们一起完成了 Embedding MLP模型的实现。在之后的课程中，我们会进一步实现其他深度学习模型，通过模型的评估进行效果上的对比。另外，我们也会利用训练出的深度学习模型完成SparrowRecsys的猜你喜欢功能，期待与你一起不断完善我们的项目。课后思考在我们实现的 Embedding+MLP 模型中，也有用户 Embedding 层和物品Embedding 层。你觉得从这两个 Embedding 层中，抽取出来的用户和物品Embedding，能直接用来计算用户和物品之间的相似度吗？为什么？欢迎把你的思考和疑惑写在留言区，也欢迎你把这节课转发给希望用TensorFlow实现深度推荐模型的朋友，我们下节课见！