### Taxonomy of Log Parsing Techniques

- **Mining-based Methods:**
  - LogSig, AEL Subsequence
  - SLCT, SHISHO, Drain, Spell
  - LFA, LenMa, IPLoM
  - LogCluster, LogMine

**Figure 1:** Taxonomy of log parsing methods based on the underlying technology they adopt.

### Self-Supervised Log Parsing

**Property 1:** A desirable feature of a log template extractor is that each log template maps to a finite set of values, bounded by the number of unique log templates. This feature allows for vector representation of logs, enabling various downstream tasks.

The generated vector representations should be close for log messages belonging to the same log template and distant for log messages belonging to distinct log templates. For example, the embedding vectors for "Took 10 seconds to create a VM" and "Took 9 seconds to create a VM" should have a small distance, while the vectors for "Took 9 seconds to create a VM" and "Failed to create VM3" should be distant.

The goal of the proposed method is to mimic an operator's comprehension of logs. Given the task of identifying all event templates in a log, a reasonable approach is to focus on parts that re-appear consistently and ignore parts that change frequently within a certain context (e.g., per log message). This can be modeled as a probability distribution for each token conditioned on its context, i.e., \( P(t_j | C(t_j)) \). Such a probability distribution would allow the distinction between constant and varying tokens, addressing Requirement 1. The generation of log embedding vectors naturally enables their utilization for fine-tuning in downstream tasks. Moreover, the representation is obtained by focusing on the constant parts of the log message, as they are more predictable, providing the necessary generalization for Property 1.

### NuLog: Self-Attentive Neural Parsing with Transformers

#### Method Overview

The proposed method consists of preprocessing, model, and template extraction. The overall architecture, based on an example log message input, is depicted in Figure 2.

#### Preprocessing

The log preprocessor transforms the log messages into a suitable format for the model. It includes two main steps: tokenization and masking.

- **Tokenization:** Tokenization converts each log message into a sequence of tokens. For NuLog, we use a simple filter-based splitting criterion to perform string splitting. We keep these filters short and simple, making them easy to construct. All concrete criteria are described in Section 4.1. In Figure 2, we illustrate the tokenization of the log message "Deleting instance /var/lib/nova/instances/4b2ab87e23b4_de". If a splitting criterion matches whitespaces, the log message is tokenized into a list of three tokens: ["Deleting", "instance", "/var/lib/nova/instances/4b2ab87e23b4_de"]. Unlike some related approaches that use additional hand-crafted regular expressions to parse parameters like IP addresses, numbers, and URLs, we do not parse any parameters with regex expressions. This approach is known to be error-prone and requires manual adjustments in different systems and even updates within the same system.

- **Masking:** The intuition behind the proposed parsing method is to learn a general semantic representation of the log data by analyzing the occurrences of tokens within their context. We apply a general method from natural language processing (NLP) research called Masked Language Modeling (MLM). Originally introduced in [16] (referred to as Cloze), it has been successfully applied in other NLP publications like [1]. Our masking module takes the output of the tokenization step as input, which is a token sequence of a log message. A token from the sequence is randomly chosen and replaced with a special MASK token. The masked token sequence is used as input for the model, while the masked token acts as the prediction target. To denote the start and end of a log message, we prepend a special CLS token and apply padding with SPEC tokens. The number of padding tokens for each log message is given by \( M - |t_i| \), where \( M = \max(|t_i|) + 1 \) is the maximal number of tokens across all log messages in the dataset plus one, and \( |t_i| \) is the number of tokens in the i-th log message. Note that the added one ensures that each log message is padded by at least one SPEC token.

#### Model

The method has two operation modes: offline and online. During the offline phase, log messages are used to tune all model parameters via backpropagation, and optimal hyper-parameters are selected. During the online phase, every log message is passed through the model, generating the respective log template and an embedding vector for each log message.

**Figure 3:** Complete architecture of NuLog for parsing logs.

The model applies two operations on the input token vectors: token vectorization and positional encoding. The subsequent encoder structure takes the result of these operations as input. It consists of two elements: a self-attention layer and a feedforward layer. The last model component is a single linear layer with a softmax activation over all tokens appearing in the logs. Below, we provide a detailed explanation of each model element.

- **Token Vectorization:** Since all subsequent elements of the model expect numerical inputs, we initially transform the tokens into randomly initialized numerical vectors \( x \in \mathbb{R}^d \). These vectors, referred to as token embeddings, are part of the training process and are adjusted during training to represent the semantic meaning of tokens depending on their context. These numerical token embeddings are passed to the positional encoding block.

- **Positional Encoding:** Attention-based models do not contain any notion of input order. Therefore, this information needs to be explicitly encoded and merged with the input vectors to take their position within the log message into account. This block calculates a vector \( p \in \mathbb{R}^d \) representing the relative position of a token based on sine and cosine functions:
  \[
  p_{j,2k} = \sin\left(\frac{j}{10000^{2k/d}}\right), \quad p_{j,2k+1} = \cos\left(\frac{j}{10000^{2k/d}}\right)
  \]
  Here, \( k = 0, 1, \ldots, d-1 \) is the index of each element in \( p \), and \( j = 0, 1, \ldots, M \) is the positional index of each token. The parameter \( k \) describes an exponential relationship between each value of vector \( p \). Additionally, sine and cosine functions are interchangeably applied, allowing better discrimination of the respective values within a specific vector of \( p \). Both functions have an approximately linear dependence on the position parameter \( j \), which is hypothesized to make it easy for the model to attend to the respective positions. Finally, both vectors can be combined as \( x = x + p \).

- **Encoder Block:**
  - **Multi-Head Attention:** The encoder block starts with a multi-head attention element, where a softmax distribution over the token embeddings is calculated. Intuitively, it describes the significance of each embedding vector for the prediction of the target masked token. We summarize all token embedding vectors as rows of a matrix \( X \) and apply the following formula:
    \[
    Z_l = \text{softmax}\left(\frac{Q_l K_l^T}{\sqrt{w}} V_l\right), \quad \text{for } l=1,2,\ldots,L
    \]
    where \( L \) denotes the number of attention heads, \( w = \frac{d}{L} \), and \( d \mod L = 0 \). The parameters \( Q, K, \) and \( V \) are matrices corresponding to the query, key, and value elements in Figure 3. They are obtained by applying matrix multiplications between the input \( X \) and respective learnable weight matrices \( W_Q, W_K, W_V \):
    \[
    Q_l = X \times W_Q^l, \quad K_l = X \times W_K^l, \quad V_l = X \times W_V^l
    \]
    where \( W_Q^l, W_K^l, W_V^l \in \mathbb{R}^{M \times w} \). The division by \( \sqrt{w} \) stabilizes the gradients during training. After that, the softmax function is applied, and the result is used to scale each token embedding vector: \( X_l = X \times Z_l \). The scaled matrices \( X_l \) are concatenated to a single matrix \( X \) of size \( M \times d \). As depicted in Figure 3, there is a residual connection between the input token matrix \( X \) and its respective attention transformation \( X \), followed by a normalization layer norm. These are used for improving the performance of the model by tackling potential problems encountered during learning, such as small gradients and the covariate shift phenomenon. Based on this, the original input is updated by the attention-transformed equivalent as \( X = \text{norm}(X + X) \).

  - **Feed-Forward Layer:** The last element of the encoder consists of two feed-forward linear layers with a ReLU activation in between. It is applied individually on each row of \( X \). Identical weights for every row are used, which can be described as a convolution over each attention-transformed matrix row with a kernel size of one. This step serves as additional information enrichment for the embeddings. Again, a residual connection followed by a normalization layer between the input matrix and the output of both layers is employed. This model element preserves the dimensionality \( X \).

By combining these components, NuLog effectively parses log messages and generates meaningful vector representations, enabling a wide range of downstream tasks.