mining - LogSig - AEL subsequence
- SLCT - SHISHO - Drain - Spell
- LFA - LenMa - IPLoM
- LogCluster - LogMine
Fig.1:Taxonomyoflogparsesaccordingtounderlyingtechnologytheyadopt.
Self-SupervisedLogParsing 5
Property1isadesirablefeatureofalogtemplateextractor.While,eachlogtem-
platemapstoafinitesetofvalues,boundedwiththenumberofuniquelogtemplates,
thisfeaturesallowsforvectorrepresentationofaloghenceopensapossibilityforad-
dressingvariousdownstreamtasks.
The generated vector representations should be closer embedding vectors for log
messaged belonging to the same log template and distant embedding vectors for log
messagesbelongingtodistinctlogtemplates.Forexample,theembeddingvectorsfor
”Took10secondstocreateaVM”and”Took9secondstocreateaVM”shouldhavea
smalldistancewhilevectorsfor”Took9secondstocreateaVM”and”Failedtocreate
VM3”shouldbedistant.
Thegoaloftheproposedmethodistomimicanoperator’scomprehensionoflogs.
Giventhetaskofidentifyingalleventtemplatesinalog,areasonableapproachistopay
closeattentiontopartsthatre-appearconstantlyandignorepartsthatchangefrequently
within a certain context (e.g. per log message). This can be modelled as a probability
distributionforeachtokenconditionedonitscontext,i.e.P(t |C(t )).Suchprobabil-
j j
itydistributionwouldallowthedistinctionofconstantandvaryingtokens,referringto
solvingRequirement1.Thegenerationoflogembeddingvectorswouldnaturallyen-
able utilization of such representation for fine-tuning in downstream tasks. Moreover,
therepresentationisobtainedbyfocusingonconstantpartsofthelogmessage,asthey
aremorepredictable,providingthenecessarygeneralizationforProperty1.
3.2 NuLog:Self-AttentiveNeuralParsingwithTransformers
Theproposedmethodsarecomposedofpreprocessing,model,andtemplateextraction.
TheoverallarchitecturebasedonanexamplelogmessageinputisdepictedinFig.2.
The log preprocessor transforms the log messages into a suitable format for the
model. It is composed of two main parts: tokenization and masking. Before the tok-
enizationtask,themeta-informationfromtheloggingframeworksisstripped,andthe
payload,i.e.,theprintstatement,isusedasinputtothetokenizationstep.
Tokenization. Tokenization transforms each log message into a sequence of to-
kens.ForNuLog,weutilizeasimplefilterbasedsplittingcriteriontoperformastring
split operation. We keep these filters short and simple, i.e. easy to construct. All con-
cretecriteriaaredescribedinsection4.1.InFig.2weillustratethetokenizationofthe
logmessage”Deletinginstance/var/lib/nova/instances/4b2ab87e23b4de”.Ifasplitting
criterionmatcheswhitespaces,thenthelogmessageistokenizedasalistofthreeto-
kens [”Deleting”, ”instance”, ”/var/lib/nova/instances/4b2ab87e23b4de”]. In contrast
to several related approaches that use additional hand-crafted regular expressions to
parsesparameterslikeIPaddresses,numbers,andURLs,wedonotparseanyparame-
terswitharegexexpression.Suchanapproachisknowntobeerror-proneandrequires
manualadjustmentsindifferentsystemsandevenupdateswithinthesamesystem.
Masking. The intuition behind the proposed parsing method is to learn a gen-
eralsemanticrepresentationofthelogdatabyanalyzingoccurrencesoftokenswithin
theircontext.Weapplyageneralmethodfromnaturallanguage(NLP)researchcalled
Masked Language Modeling (MLM). It is originally introduced in [16] (where it is
referred to as Cloze) and successfully applied in other NLP publications like [1]. Our
6 S.Nedelkoskietal.
INPUT LOG: Deleting instance /var/lib/nova/instances/4b2ab87e23b4_de
n
o
ati TOKENIZATION
z
ni
e Deleting instance /var/lib/nova/instances/4b2ab87e23b4_de
k
o
T
MASKING
g
n   instance /var/lib/nova/instances/4b2ab87e23b4_de
ki
s
a
M  Deleting  /var/lib/nova/instances/4b2ab87e23b4_de
 Deleting instance 
el MASKING
d
o
M
 Deleting instance 
n
o
cti vector representation
generated template
a of the log
xtr
E
Fig.2:InstanceofparsingofasinglelogmessagewithNuLog.
maskingmoduletakestheoutputofthetokenizationstepasinput,whichisatokense-
quenceofalogmessage.Atokenfromthesequenceisrandomlychosenandreplaced
withthespecialMASKtoken.Themaskedtokensequenceisusedasinputforthe
model,whilethemaskedtokenactsasthepredictiontarget.Todenotethestartandend
of a log message, we prepend a special CLS and apply padding with SPEC to-
kens.ThenumberofpaddingtokensforeachlogmessageisgivenbyM −|t |,where
i
M = max(|t |) + 1, ∀i is the maximal number of tokens across all log messages
i
within the log dataset added by one, and |t | is the number of tokens in the i-th log
i
message.Note,thattheaddedoneensuresthateachlogmessageispaddedbyatleast
oneSPECtoken.
Model.Themethodhastwooperationmodes-offlineandonline.Duringtheoffline
phase, log messages are used to tune all model parameters via backpropagation and
optimal hyper-parameters are selected. During the online phase, every log message is
passed forward through the model. This generates the respective log template and an
embeddingvectorforeachlogmessage.
Fig.3depictsthecompletearchitecture.Themodelappliestwooperationsonthein-
puttokenvectors:tokenvectorizationandpositionalencoding.Thesubsequentencoder
structuretakestheresultoftheseoperationsasinput.Itiscomposedoftwoelements:
self-attentionlayerandfeedforwardlayer.Thelastmodelcomponentisasinglelinear
Self-SupervisedLogParsing 7
layer with a softmax activation overall tokens appearing in the logs. In the following,
weprovideadetailedexplanationofeachmodelelement.
Since all subsequent elements of the model expect numerical inputs, we initially
transformthetokensintorandomlyinitializednumericalvectorsx∈Rd.Thesevectors
arereferredtoastokenembeddingsandarepartofthetrainingprocess,whichmeans
theyareadjustedduringtrainingtorepresentthesemanticmeaningoftokensdepend-
ing on their context. These numerical token embeddings are passed to the positional
encoding block. In contrast to e.g., recurrent architectures, attention-based models do
notcontainanynotionofinputorder.Therefore,thisinformationneedstobeexplicitly
encodedandmergedwiththeinputvectorstotaketheirpositionwithinthelogmessage
into account. This block calculates a vector p ∈ Rd representing the relative position
ofatokenbasedonasineandcosinefunction.
j j
p =sin , p =cos . (1)
2k 100002 vk 2k+1 100002k v+1
Here,k =0,1,...,d−1istheindexofeachelementinpandj =0,1,...,M is
thepositionalindexofeachtoken.Withintheequations,theparameterk describesan
exponentialrelationshipbetweeneachvalueofvectorp.Additionally,asineandcosine
functionareinterchangeablyapplied.Bothallowbetterdiscriminationoftherespective
valueswithinaspecificvectorofp.Furthermore,bothfunctionshaveanapproximately
lineardependenceonthepositionparameterj,whichishypothesizedtomakeiteasy
forthemodeltoattendtotherespectivepositions.Finally,bothvectorscanbecombined
asx =x+p.
Input:
log 1log 2
Layer normalization Layer normalization Output:
[ tm 2.] [mtt .21 encoding p p( ..t .1 2)
.t l-.2 . ]. positional self- va ao tt lu uet ention FF ee ee dd FF woo irr tww aa arr cdd inn vee att tww iooo nrr kk (( ww 12 )) Softax m enerator ( t )
G
key query
h t
tl-1 tl-1 Multi-head attention p(t|T|)
Fig.3:ModelarchitectureofNuLogforparsingofthelogs.
Theencoderblockofourmodelstartswithamulti-headattentionelement,wherea
softmaxdistributionoverthetokenembeddingsiscalculated.Intuitively,itdescribesthe
significanceofeachembeddingvectorforthepredictionofthetargetmaskedtoken.We
summarizealltokenembeddingvectorsasrowsofamatrixXandapplythefollowing
formula
 ×KT
Q
Z =softmax l√ l V , forl=1,2,...,L, (2)
l w l
8 S.Nedelkoskietal.
whereLdenotesthenumberofattentionheads,w = d anddmodL=0.Theparame-
L
tersQ,K andV arematrices,thatcorrespondtothequery,key,andvalueelementsin
Fig.3.TheyareobtainedbyapplyingmatrixmultiplicationsbetweentheinputX and
respectivelearnableweightmatricesWQ,WK,WV:
l l l
Q =X∗×WQ, K =X∗×WK, V =X∗×WV, (3)
l l l l l l
√
where WQ, WK, WV ∈ RM×w. The division by w stabilizes the gradients
l l l
duringtraining.Afterthat,thesoftmaxfunctionisappliedandtheresultisusedtoscale
eachtokenembeddingvector:X =X×Z .ThescaledmatricesXareconcatenated
l l l l
toasinglematrixXofsizeM×d.AsdepictedinFig.3thereisaresidualconnection
betweentheinputtokenmatrixX anditsrespectiveattentiontransformationX,fol-
lowedbyanormalizationlayernorm.Theseareusedforimprovingtheperformance
ofthemodelbytacklingdifferentpotentialproblemsencounteredduringthelearning
suchassmallgradientsandthecovariateshiftphenomena.Basedonthis,theoriginal
inputisupdatedbytheattention-transformedequivalentasX =norm(X+X).
The last element of the encoder consists of two feed-forward linear layers with a
ReLUactivationinbetween.ItisappliedindividuallyoneachrowofX.Thereby,iden-
ticalweightsforeveryrowareused,whichcanbedescribedasaconvolutionovereach
attention-transformedmatrixrowwithkernelsizeone.Thisstepservesasadditionalin-
formationenrichmentfortheembeddings.Again,aresidualconnectionfollowedbya
normalizationlayerbetweentheinputmatrixandtheoutputofbothlayersisemployed.
ThismodelelementpreservesthedimensionalityX.