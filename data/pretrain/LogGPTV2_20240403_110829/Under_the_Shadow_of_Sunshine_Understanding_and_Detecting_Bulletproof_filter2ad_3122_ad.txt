ARIN
RIPE
RIPE
RIPE
RIPE
APNIC & ARIN
RIPE
RIPE
TABLE V: BPH purchasing details. “Acq Type” refers to acquisition type which can be a test or a purchased IP address.
Sub-allocations Per Day
Owners?
#Selected
Region
RIR
Europe
North America
Asia
South America
Africa
RIPE
ARIN
APNIC
LACNIC
AFRINIC
#All
4M
2.9M
928K
364K
86K
2.2M
2.8M
462K
357K
21K
100K
72K
34K
7K
1K
#Owners
1.3M
2M
12K
167K
7K
TABLE VI: Daily processed sub-allocations in IPv4, ordered by
RIR size. “All” represents the number of all sub-allocations while
“Owners?” is the number of sub-allocations that have owners (i.e.
managed by parties other than the parent service provider). “Owners”
is the number of all merged owner objects we created for all sub-
allocations over 25 Whois snapshots.
#
1
2
3
4
5
6
7
8
9
10
11
12
13-14
Type
Whois
BL
PDNS
Name
Sub-allocation size
Sub-allocation age
AS Reputation
Average Daily Trafﬁc
∗
DNS Age
∗
Average Daily TLD+3 churn
Average Daily TLD+3
TLD+3 Age
∗
Average Daily IP churn
Daily IPs
∗
IP Up-time
IP Age
∗
Net Utilization
Normalized by
-
-
-
Sub-allocation age
DNS Age
Sub-allocation size
Sub-allocation size
Sub-allocation Age
Sub-allocation Age
Sub-allocation size
TABLE VII: Selected system features. Starred Features are new
PDNS features not used in previous research. Whois speciﬁc features
have been used in previous research, but only for domain names.
allocations in RIPE have an average size of 130 IP addresses
(∼/25), rendering domain name and IP address based blacklists
largely ineffective (most blacklists limit their coverage to avoid
false positives).
To address this challenge, we select features tailored to
sub-allocations, particularly those considered to be robust,
in a sense that evading these features would likely incur a
signiﬁcant cost (either monetary or increased blacklisting) to
the malicious actors. To this end, we leverage three groups
of features: Whois, PDNS and AS, totaling 14 features. Six of
them have never been used in previous research. A description
of each of these features is provided in Table VII.
• PDNS: BPH services tend to have a multi-layered infrastruc-
ture to better protect their back-end servers. This is usually
deployed through the use of doorways (front-end websites),
domain name and IP address fast ﬂuxing, proxies and redi-
813
rection servers which unavoidably will entail the use of DNS
and thus presence in PDNS. We capture this behavior using
11 features, detailed in Table VII. Many previous systems
detecting malicious domains have used features extracted from
DNS records, e.g. Exposure [31], which we utilize and adapt
to the context of sub-allocations, such as DNS look-ups (i.e.
trafﬁc), daily number and age of TLD+3 and IP addresses.
Additionally, we use 6 new features geared towards sub-
allocations to ﬁnd the signals of the front-end proxy layers
employed by malicious actors. More speciﬁcally, we calculate
the daily churn of TLD+3 and IP addresses. Intuitively, the
clients hosted on BPH services are much less stable than those
on legitimate services, who tend to come and go quickly.
This observation is captured by the “Daily TLD+3 churn”
feature. Furthermore, one expects to see rotation of a sub-
allocation’s IP addresses over time when used by BPH services
to avoid blacklisting. We ﬁnd this by calculating the length
of a continuous duration of an IP address up-time in PDNS.
BPH services would undoubtedly purchase resources, i.e. sub-
allocations, as their demand increase, i.e. one expects them
to monetize all resources paid for, which we measure by
calculating the total usage of a sub-allocation’s IP addresses.
We ﬁnd evidences for this intuition in the current listings by
ROKSO [26], where 60% of BPH services utilize all of the
sub-allocations’ IP addresses as shown in Figure 8(a). We also
measure the monetization of a sub-allocation by considering
its DNS Age Vs Whois Age, i.e. the number of days any of
the sub-allocation’s IP addresses appearing in PDNS since the
sub-allocation was created.
• Whois: We use Whois to extract two features: sub-allocation
size and its age. Sub-allocations used by a BPH service or fully
controlled by malicious actors tend to last for a few months
before their owners move on to another sub-allocation. For
example, in the labeled set A, described earlier in Table IV,
malicious sub-allocations have an average age of ∼1K days
compared to an average of ∼3k for clean sub-allocations.
• AS: Abuse of legitimate service providers will often show
up in blacklists, although with a weaker signal than BP AS.
We leverage this signal using the AS reputation of the sub-
allocation’s parent service provider (aka, AS) as shown in
Figure 8(b).
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:21:58 UTC from IEEE Xplore.  Restrictions apply. 
Recall (TPR)
Metric
FDR
FPR
Accuracy
AUC
SVM
Set-A
92.2%
1.2%
5.5%
92.6%
93.3%
Set-B
89.8%
3.1%
3.1%
93.2%
93.3%
RF
Set-A
96%
2.3%
11.7%
97.8%
93.1%
Set-B
98%
1.5%
1.6%
97.1%
97.2%
(a) Net Utilization
(b) AS Reputation, a higher value indicating a worst
reputation.
Fig. 8: CDF charts showing distribution of two selected features on
the labeled sets.
IV. EVALUATION
In this section, we present our approach for training the
classiﬁers to detect malicious sub-allocations, and evaluation
of the classiﬁers by testing it on two types of label sets: a
highly conservative set and a noisier one. Then, we run the
trained classiﬁer on the larger unlabeled set of the ﬁltered
sub-allocations from all 5 RIRs for one Whois snapshot.
We validate the results further by quantifying and showing
indicators of badness, such as ties to malicious activities and
de-listing from the most recent Whois records.
Training a classiﬁer. Due to the challenges in ﬁnding rep-
resentative sets of both clean and malicious sub-allocations
(discussed earlier in Section III-C), we had to resort to labeled
sets with different levels of noise, as presented in Table IV,
to form two training sets, Set-A and Set-B, each of which
contains both labeled malicious and clean samples. To select
the labeled malicious sub-allocations, we used the purest lists
for both Set-A and Set-B, namely Edrop [14] and current
listings by ROKSO [26], providing a combined total of 891
sub-allocations. The relatively small size of the conﬁrmed
clean sub-allocations requires us to take a strategy that utilizes
two sets, Set-A and Set-B, with differently labeled clean data
(of different qualities and sizes) to compare the effectiveness of
the models trained on them. Speciﬁcally, using the Top Hosters
source [5], for Set-A, we pick only 179 sub-allocations that do
not have a bad reputation according to the SpamHaus ISP [28]
reputation database. As a result, Set-A is characterized by a
small clean set and a larger malicious set and therefore biased
towards malicious sub-allocations. For Set-B, we include all
TABLE VIII: Results of a 5-Fold cross-validation on two classiﬁers,
Support Vector Machines (SVM) and Random Forest (RF) using the
labeled sets of A and B.
818 sub-allocations, including the noisy ones, from the top
100 hosting providers to form its clean set, and as a result, it
is balanced (between the malicious and clean sets) but has a
lower quality (due to the noise). All remaining labeled sub-
allocations are left out and considered for testing purposes
only. Next, we experiment with two classiﬁers: Support Vector
Machines (SVM) [32] and Random Forest (RF) [33] using 100
trees 7.
A. Evaluation on Labeled Datasets
To evaluate the effectiveness of the classiﬁers and select
the model that performs best, we employ two validation steps:
5-fold cross-validation and testing on the noisy labeled sets.
•
5-Fold Cross-Validation: We perform a 5-Fold cross-
validation on each set for both classiﬁers, as shown in Ta-
ble VIII. A description of our evaluation metrics is also
provided by Table XVIII in Appendix. Clearly, RF outperforms
the SVM when using the balanced Set-B but the SVM also
handles the case of the unbalanced set, Set-A, much better
than RF. In Set-B, the False Discovery Rate (FDR) and the
False Positive Rate (FPR) are expected to be similar due to
the balanced nature of the set. However, the FPR is expected
to be quite high for Set-A because it represents the number
of false positives out of all negatives (179) which is small
compared to the number of all positives (891). We select RF
as our classiﬁcation algorithm for all future analysis, since it
performs better on the sets, as seen by the cross-validation
process.
• Using a test set: Next, we evaluate the RF models trained
on both Set-A and Set-B by testing them on the much
noisier labeled sets shown in Table IV, those not selected for
training. For example, we test the Set-A trained model on
the sub-allocations labeled “Clean-Noisy” from the Top 100
hosting providers source and the Set-B trained model on all
sub-allocations from the Top 500 hosting providers source.
Additionally, we test their performance on the sub-allocations
containing our purchased IP addresses.
We use two evaluation metrics, True Positive rate (TPR)
and True Negative rate (TNR), since their counterparts, the
FNR and FPR, can be easily inferred, shown in Table XVIII
in Appendix. The other metrics such as False Discovery Rate
(FDR) do not apply here as each test happens on either a
labeled clean set or a labeled malicious set, so the false positive
rate cannot be meaningfully calculated based upon the set with
a single label. The details of the test results are presented in
Table IX. Overall, the model trained on Set-B works well but
unfortunately is able to capture only 33% of the purchased set
7We limit the maximum number of trees to mitigate over-ﬁtting on the
training data.
814
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:21:58 UTC from IEEE Xplore.  Restrictions apply. 
(a) Non-operational TLD+3
(b) Matched BL TLD+3
Fig. 9: CDF charts showing 4 indicators of badness on two detected sets of sub-allocations with a frame of reference using the labeled sets.
Set-A and Set-B are the set of sub-allocations detected by our classiﬁer when trained with each of these sets.
(c) Matched BL IPs
(d) Distribution of FQDNs over TLD+3
compared to its counterpart trained on Set-A, which can detect
twice as many of the purchases. The Set-B model also detects
43% of the labeled clean set (48 sub-allocations) as malicious.
Taking a close look at these 48 cases, we ﬁnd that they are
all under the same parent service provider, LanLogic (a cloud
provider), and have been registered since 1997. This indicates
that they have served many clients on the same sub-allocations
for a long time. Given the age of these sub-allocations, the
likelihood of them at some point inadvertently hosting abusive
content becomes unavoidable. Finally, we ﬁnd that 4.5% of
the false positives detected by our classiﬁer run on the Set-B
model are Alexa’s sub-allocations that are not necessarily false
positives. Looking at their indicators of badness, described
later, we ﬁnd that on average 1% and 3% of their hosted
TLD+3 and IP addresses, respectively, are blacklisted, which
justiﬁes the noisy label assigned to it from the start (Table IV).
Due to this noise, it is difﬁcult to obtain a completely accurate
evaluation of our classiﬁer on these large testing sets and these
results should be treated as estimates of performance.
B. Evaluation on the Unlabeled Set
Based on the evaluation of our classiﬁer on the labeled set,
we run the two trained models of Set-A and Set-B on the much
larger unlabeled set of sub-allocations for one snapshot (July
Source
Label
Alexa[4]
Clean - Noisy
Top 100 Hosters[5] Clean - Noisy
Top 500 Hosters[5] Clean
ROKSO[26]
Purchased
Clean - Noisy
Malicious - Archived
Malicious - Noisy
Set-A
Set-B
TPR
-
-
-
-
53%
66.6%
TNR
84%
87.4%
76.1%
-
-
-
TPR
-
-
-
-
55%
33.3%
TNR
95.5%
57.1%
97.6%
-
-
-
TABLE IX: Testing results of the Random Forest (RF) model trained
with Set A & B on the noisy labeled sets.
12th 2016) to gauge the scale and accuracy of our detector. As
a result, we detected 40K (20%) and 20K(10%) sub-allocations
using Set-A and Set-B respectively for training.
Indicators of badness. As previously stated, working on