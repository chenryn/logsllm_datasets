all pairs. We utilize 59 randomly selected poisoned models
(see selection details in Appendix X-E) from the TrojAI
competition (round 4), where the models were poisoned by
stamping a polygon (with size ranging from 903 to 6021) to
foreground objects. We follow the same setup as in existing
work [39] for eliminating backdoors, where only 5% of the
original training set is used. We use three existing backdoor-
erasing approaches, namely, Standard Finetuning, NC [40],
and NAD [39], as the baselines. Standard Finetuning is a
standard approach that was originally designed for transfer
learning. It uses a small learning rate to update model pa-
rameters based on a small set of training samples. It is the
same finetuning baseline from NAD [39]. Please see detailed
descriptions of NC and NAD in Section V-A. We strictly
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
121383
Fig. 12: Attack success rate of poisoned models before/after repair.
follow the setup of the baselines in the original papers [39],
[40] and choose the best results by testing on different settings.
The attack success rate of poisoned models before and after
applying each technique is presented in Figure 12. The x-
axis denotes the model IDs and the y-axis denotes the attack
success rate (ASR). Bars in the light colors denote the ASR of
the injected backdoors before erasing/hardening and the dark
color after. Observe that NC (blue bars) can only successfully
eliminate 7 (out of 59) backdoors and reduce ASR to below
25% for another 4 backdoors. The poisoned models in round
4 have injected backdoors only effective when stamping on
a specific class. NC generates the smallest backdoor once
and uses it in hardening. It is not iterative. Moreover, the
smallest backdoor may not be the injected backdoor (but rather
a natural backdoor). Standard Finetuning (the orange bars)
is effective for seven cases as the injected backdoors were
trained with clean samples. Finetuning only on clean samples
may not affect the backdoor patterns learned by the poisoned
models. NAD is built on top of Standard Finetuning, whose
performance will be constrained by Standard Finetuning. From
the green bars in Figure 12, we can observe that some models
still have a high ASR after finetuning, and NAD reduces
the ASRs to some extent but cannot eliminate the backdoors
(e.g., ID 28 and ID 60). MOTH, on the other hand, can
eliminate all the backdoors with an average ASR down to
1%. The accuracy degradation on clean samples is minimal
for all the approaches on average (< 0.2%). See details in our
supplementary material [44].
VII. RELATED WORK
Backdoor Attack and Defense. Existing attacks either poison
the training set using backdoor injected samples with the target
label like patch attacks [1], [2], or with the original label like
clean label attacks [6]–[9]. There is another type of backdoor
attacks that craft different backdoors for different inputs [45],
[55]. Backdoor attacks can be launched on models with
various applications [56]–[66]. To identify whether a model
is poisoned [16]–[18], Existing works inverse backdoors [40],
[41], use the difference between poisoned models and clean
models when reacting to input perturbations [19]–[21]. Beside
identifying poisoned models, existing techniques also detect
and reject inputs stamped with backdoors [24]–[35]. These
works are orthogonal to our technique as they do not consider
natural backdoors. There are also works focusing on elimi-
nating backdoors by pruning out compromised neurons [36]
or retraining leveraging data augmentation technique [38].
A state-of-the-art
technique NAD [39] makes use of the
teacher-student training procedure to remove backdoors. Our
evaluation in Section VI-B shows that MOTH outperforms
NAD. In addition, it does not handle natural backdoors.
Adversarial Training. There are a large body of works on
adversarial
training aiming for better robustness and effi-
ciency [53], [67]–[70]. They can enlarge class distances and
on the other hand MOTH can further add to their improve-
ment. Universal adversarial perturbation (UAP) differs from
conventional adversarial attacks targeting individual samples.
It aims to fool models on a set of samples with a universal
perturbation [71], [72]. Researchers proposed to use UAP to
improve model robustness [43]. Our results show that UAP’s
effectiveness on improving class distances is limited.
VIII. DISCUSSION
The main focus of this paper is computer vision tasks. We
discuss possible extensions of MOTH to other domains. Specif-
ically, we discuss a possible proposal of leveraging a sigmoid
function to approximate the discrete value (e.g., characters in
natural language processing (NLP) and the executability of
code in Android apps) for backdoor generation. Details are
in Appendix X-G. Although the threat model of our paper
focuses on static backdoors, we also test MOTH on other
backdoor types, including reflection backdoors [3], composite
backdoors [4], and filter backdoors [41]. MOTH is effective
in defending against these attacks using the default metric.
In addition, we investigate a different metric for the filter
attack, which can be used for hardening and improves the
effectiveness of MOTH. Please see details in Appendix X-G.
IX. CONCLUSION
We develop a novel model hardening technique that can
enlarge class distances, making models resilient to backdoor
attacks. Our evaluation on 5 datasets with 15 model structures
show that it can improve class distance by 149.9% on average
with only 1% accuracy loss, outperforming existing hardening
techniques. It reduces 80% false positives for a state-of-the-art
backdoor scanner and substantially outperforms three recent
techniques in removing injected backdoors.
ACKNOWLEDGMENT
We thank the anonymous reviewers for their constructive
comments. This research was supported, in part by IARPA
TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300,
ONR N000141712045, N000141410468 and N000141712947.
Any opinions, findings, and conclusions in this paper are those
of the authors only and do not necessarily reflect the views of
our sponsors.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
131384
NCStandard FinetuningNADMOTHREFERENCES
[1] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating
backdooring attacks on deep neural networks,” IEEE Access, 2019.
[2] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor
attacks on deep learning systems using data poisoning,” arXiv preprint
arXiv:1712.05526, 2017.
[3] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reflection backdoor: A natural
backdoor attack on deep neural networks,” in ECCV, 2020.
[4] J. Lin, L. Xu, Y. Liu, and X. Zhang, “Composite backdoor attack for
deep neural network by mixing existing benign features,” in CCS, 2020.
[5] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in deep learning
models,” arXiv preprint arXiv:2005.03823, 2020.
[6] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” in NeurIPS, 2018.
[7] C. Zhu, W. R. Huang, H. Li, G. Taylor, C. Studer, and T. Goldstein,
“Transferable clean-label poisoning attacks on deep neural nets,” in
ICML, 2019, pp. 7614–7623.
[8] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, “Clean-
label backdoor attacks on video recognition models,” in CVPR, 2020.
[9] A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden trigger backdoor
attacks,” in AAAI, 2020.
[10] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” in NDSS, 2018.
[11] Y. Cao, N. Wang, C. Xiao, D. Yang, J. Fang, R. Yang, Q. A. Chen,
M. Liu, and B. Li, “Invisible for both camera and lidar: Security of multi-
sensor fusion based perception in autonomous driving under physical-
world attacks,” in S&P, 2021.
[12] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A.
Chen, K. Fu, and Z. M. Mao, “Adversarial sensor attack on lidar-based
perception in autonomous driving,” in CCS, 2019.
[13] X. Liu, W. Liu, T. Mei, and H. Ma, “A deep learning-based approach to
progressive vehicle re-identification for urban surveillance,” in ECCV.
[14] S. Thys, W. Van Ranst, and T. Goedem´e, “Fooling automated surveil-
lance cameras: adversarial patches to attack person detection,” in CVPR
Workshops, 2019.
[15] B. Biggio, G. Fumera, F. Roli, and L. Didaci, “Poisoning adaptive
biometric systems,” in SPR and SSPR, 2012, pp. 417–425.
[16] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “Universal litmus
patterns: Revealing backdoor attacks in cnns,” in CVPR, 2020.
[17] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “Detecting
ai trojans using meta neural analysis,” arXiv preprint arXiv:1910.03137.
[18] X. Qiao, Y. Yang, and H. Li, “Defending neural backdoors via generative
distribution modeling,” in NeurIPS, 2019.
[19] S. Huang, W. Peng, Z. Jia, and Z. Tu, “One-pixel signature: Character-
izing cnn models for backdoor detection,” in ECCV, 2020.
[20] X. Zhang, A. Mian, R. Gupta, N. Rahnavard, and M. Shah, “Cassan-
dra: Detecting trojaned networks from adversarial perturbations,” arXiv
preprint arXiv:2007.14433, 2020.
[21] R. Wang, G. Zhang, S. Liu, P.-Y. Chen, J. Xiong, and M. Wang,
“Practical detection of trojan neural networks: Data-limited and data-
free cases,” in ECCV, 2020.
[22] W. Guo, L. Wang, Y. Xu, X. Xing, M. Du, and D. Song, “Towards
inspecting and eliminating trojan backdoors in deep neural networks,”
in ICDM, 2020.
[23] X. Huang, M. Alzantot, and M. Srivastava, “Neuroninspect: Detecting
backdoors in neural networks via output explanations,” arXiv preprint
arXiv:1911.07399, 2019.
[24] A. K. Veldanda, K. Liu, B. Tan, P. Krishnamurthy, F. Khorrami, R. Karri,
B. Dolan-Gavitt, and S. Garg, “Nnoculation: broad spectrum and tar-
geted treatment of backdoored dnns,” arXiv preprint arXiv:2002.08313.
[25] S. Ma, Y. Liu, G. Tao, W.-C. Lee, and X. Zhang, “Nic: Detecting
adversarial samples with neural network invariant checking,” in NDSS.
[26] D. Tang, X. Wang, H. Tang, and K. Zhang, “Demon in the variant:
Statistical analysis of dnns for robust backdoor contamination detection,”
in USENIX Security, 2021.
[27] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal,
“Strip: A defence against trojan attacks on deep neural networks,” in
ACSAC, 2019, pp. 113–125.
[28] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee,
I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural
networks by activation clustering,” arXiv preprint arXiv:1811.03728.
[29] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Rethinking the trigger
of backdoor attack,” arXiv preprint arXiv:2004.04692, 2020.
[30] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in ICCD, 2017.
[31] E. Chou, F. Tramer, and G. Pellegrino, “Sentinet: Detecting localized
universal attack against deep learning systems,” SPW, 2020.
[32] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,”
in NeurIPS, 2018, pp. 8000–8010.
[33] H. Fu, A. K. Veldanda, P. Krishnamurthy, S. Garg, and F. Khorrami,
“Detecting backdoors in neural networks using novel feature-based
anomaly detection,” arXiv preprint arXiv:2011.02526, 2020.
[34] A. Chan and Y.-S. Ong, “Poison as a cure: Detecting & neutralizing
variable-sized backdoor attacks in deep neural networks,” arXiv preprint
arXiv:1911.08040, 2019.
[35] M. Du, R. Jia, and D. Song, “Robust anomaly detection and backdoor
attack detection via differential privacy,” in ICLR, 2019.
[36] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against
backdooring attacks on deep neural networks,” in RAID, 2018.
[37] E. Borgnia, V. Cherepanova, L. Fowl, A. Ghiasi, J. Geiping, M. Gold-
blum, T. Goldstein, and A. Gupta, “Strong data augmentation sanitizes
poisoning and backdoor attacks without an accuracy tradeoff,” arXiv
preprint arXiv:2011.09527, 2020.
[38] Y. Zeng, H. Qiu, S. Guo, T. Zhang, M. Qiu, and B. Thuraisingham,
“Deepsweep: An evaluation framework for mitigating dnn backdoor
attacks using data augmentation,” arXiv preprint arXiv:2012.07006.
[39] Y. Li, N. Koren, L. Lyu, X. Lyu, B. Li, and X. Ma, “Neural attention
distillation: Erasing backdoor triggers from deep neural networks,” in
ICLR, 2021.
[40] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.
Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in
neural networks,” in S&P, 2019, pp. 707–723.
[41] Y. Liu, W.-C. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang, “Abs:
Scanning neural networks for back-doors by artificial brain stimulation,”
in CCS, 2019, pp. 1265–1282.
[42] “TrojAI Leaderboard,” https://pages.nist.gov/trojai/.
[43] A. Shafahi, M. Najibi, Z. Xu, J. Dickerson, L. S. Davis, and T. Goldstein,
“Universal adversarial training,” in AAAI, 2020.
[44] ModelOrth, “Moth,” https://github.com/ModelOrth/MOTH.
[45] A. Salem, R. Wen, M. Backes, S. Ma, and Y. Zhang, “Dynamic
backdoor attacks against machine learning models,” arXiv preprint
arXiv:2003.03675, 2020.
[46] S. Cheng, Y. Liu, S. Ma, and X. Zhang, “Deep feature space trojan
attack of neural networks by controlled detoxification,” in AAAI, 2021.
[47] “Keras Applications,” https://keras.io/api/applications/.
[48] H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry, “Do
adversarially robust imagenet models transfer better?” in NeurIPS, 2020.
[49] G. Elsayed, D. Krishnan, H. Mobahi, K. Regan, and S. Bengio, “Large
margin deep networks for classification,” NeurIPS, 2018.
[50] Y. Yang, R. Khanna, Y. Yu, A. Gholami, K. Keutzer, J. E. Gonzalez,
K. Ramchandran, and M. W. Mahoney, “Boundary thickness and robust-
ness in learning models,” NeurIPS, 2020.
[51] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the
multiarmed bandit problem,” Machine learning, pp. 235–256, 2002.
[52] C. J. C. H. Watkins, “Learning from delayed rewards,” 1989.
[53] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting
adversarial training,” in ICLR, 2020.
[54] “Abs,” https://github.com/naiyeleo/ABS.
[55] T. A. Nguyen and A. Tran, “Input-aware dynamic backdoor attack,”
NeurIPS, vol. 33, 2020.
[56] X. Zhang, Z. Zhang, and T. Wang, “Trojaning language models for fun
and profit,” in European S&P, 2021.
[57] X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, “Badnl: Backdoor
attacks against nlp models,” arXiv preprint arXiv:2006.01043, 2020.
[58] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning attacks on pre-
trained models,” in ACL, 2020.
[59] S. Rezaei and X. Liu, “A target-agnostic attack on deep models:
Exploiting security vulnerabilities of transfer learning,” in ICLR, 2020.
[60] B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao, “With
great training comes great vulnerability: Practical attacks against transfer
learning,” in USENIX Security, 2018, pp. 1281–1297.
[61] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, “Latent backdoor attacks on
deep neural networks,” in CCS, 2019, pp. 2041–2055.
[62] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “Dba: Distributed backdoor
attacks against federated learning,” in ICLR, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
141385
[63] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-
y. Sohn, K. Lee, and D. Papailiopoulos, “Attack of the tails: Yes, you
really can backdoor federated learning,” NeurIPS, 2020.
[64] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks
against federated learning systems,” in ESORICS, 2020, pp. 480–501.
[65] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
backdoor federated learning,” in AISTATS, 2020, pp. 2938–2948.
[66] M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks
to byzantine-robust federated learning,” in USENIX Security, 2020.
[67] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in ICLR, 2018.
[68] A. Shafahi, M. Najibi, A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S.
Davis, G. Taylor, and T. Goldstein, “Adversarial training for free!”
NeurIPS, 2019.
[69] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning
at scale,” in ICLR, 2017.
[70] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, “Ensemble adversarial training: Attacks and defenses,” in
ICLR, 2018.
[71] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-
sal adversarial perturbations,” in CVPR, 2017, pp. 1765–1773.
[72] J. Hendrik Metzen, M. Chaithanya Kumar, T. Brox, and V. Fischer, “Uni-
versal adversarial perturbations against semantic image segmentation,”
in ICCV, 2017, pp. 2755–2764.
[73] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in CVPR, 2016, pp. 770–778.
[74] M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv preprint
arXiv:1312.4400, 2013.
[75] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[76] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund, “Vision-based traffic
sign detection and analysis for intelligent driver assistance systems:
Perspectives and survey,” T-ITS, vol. 13, no. 4, pp. 1484–1497, 2012.
[77] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust physical-world attacks
on deep learning visual classification,” in CVPR, 2018, pp. 1625–1634.
[78] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in CVPR, 2017, pp. 4700–4708.
[79] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in CVPR, 2016.
[80] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.
[81] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
The kitti dataset,” IJRR, 2013.
[82] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-
son, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for
semantic urban scene understanding,” in CVPR, 2016, pp. 3213–3223.
[83] F. Larsson, M. Felsberg, and P.-E. Forssen, “Correlating fourier descrip-
tors of local patches for road sign recognition,” IET Computer Vision,
vol. 5, no. 4, pp. 244–254, 2011.
[84] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro, “Intriguing
properties of adversarial ml attacks in the problem space,” in S&P, 2020.
[85] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reflection backdoor: A nat-
ural backdoor attack on deep neural networks,” https://github.com/
DreamtaleCore/Refool/tree/582e54a6b92a8a52.
[86] J. Lin, L. Xu, Y. Liu, and X. Zhang, “Composite backdoor attack for
deep neural network by mixing existing benign features,” https://github.
com/TemporaryAcc0unt/composite-attack.
X. APPENDIX
A. Detailed Experiment Setup
CIFAR-10 is an object recognition dataset for a 10-class
classification task, which contains 60,000 images. We split the
whole dataset into three sets: 48,000 images for training, 2,000
for validation and 10,000 for testing. Four different models are
utilized for this dataset: ResNet20 [73], Network in Network
(NiN) [74], VGG19 [75], and ResNet50 [73].
Fig. 13: Mean and standard deviation of class distances for 100
different sets of 100 random samples for a naturally trained
ResNet20 model on CIFAR-10.
SVHN (Street View House Numbers) dataset contains house
(digital) numbers extracted from Google Street View images,
which consists of 73,257 training images and 26,032 test
images. We further split the original training set into 67,257
samples for training and 6,000 samples for validation. We
employ two models, NiN [74] and ResNet32 [73].
LISA is a U.S. traffic sign dataset that contains 47 different
road signs [76]. However, the number of samples of different
classes is not well-balanced, with some classes having very
few images. We use the same setting as in existing work [77]
by choosing 18 most common classes based on the number
of training examples, and split the dataset into 5,635 training
samples, 704 validation samples and 704 test samples. We use
two model structures for this dataset: A CNN model [77] that
consists of three convolutional layers and one fully-connected
layer, and a ResNet20 [73].
GTSRB (German Traffic Sign Recognition Dataset) contains
43 different traffic signs, which is designed for training models
in self-driving scenarios. We divide the dataset into three parts:
35,289 signs for training, 3,920 for validation, and 12,630 for
testing. We use an NiN [74] model for this dataset.
We also conduct experiments on 89 pre-trained models (30
benign ones and 59 poisoned ones) from round 4 of TrojAI
competition [42]. In this round, TrojAI models utilize 16 dif-
ferent structures such as DenseNet121 [78], InceptionV3 [79],
MobileNetV2 [80], etc. Each model
is trained to classify
synthetic street signs into between 15 and 45 classes. Input
images are created by compositing a foreground object, e.g., a
synthetic traffic sign, with a random background image from
five different datasets including three categories from KITTI
dataset [81], Cityscapes dataset [82] and Swedish Roads
dataset [83]. Random transformations, such as shifting, titling,
lighting, blurring, and weather effects, are applied during