### Experimental Setup and Evaluation

#### Poisoned Models
We utilized 59 randomly selected poisoned models from the TrojAI competition (round 4). These models were poisoned by stamping a polygon (ranging in size from 903 to 6021 pixels) onto foreground objects. The selection details are provided in Appendix X-E.

#### Baseline Approaches
We compared our method with three existing backdoor-erasing approaches: Standard Finetuning, Neural Cleanse (NC) [40], and Neural Attention Distillation (NAD) [39]. 

- **Standard Finetuning**: This is a conventional approach originally designed for transfer learning. It updates model parameters using a small learning rate and a small set of training samples.
- **Neural Cleanse (NC)**: NC identifies and mitigates backdoor attacks by generating the smallest possible trigger that can activate the backdoor.
- **Neural Attention Distillation (NAD)**: NAD builds on Standard Finetuning and uses a teacher-student training procedure to remove backdoors.

#### Evaluation Metrics
The attack success rate (ASR) of the poisoned models before and after applying each technique was measured. Figure 12 presents the ASR for each model, where the x-axis denotes the model IDs and the y-axis denotes the ASR. Light-colored bars represent the ASR before erasing/hardening, and dark-colored bars represent the ASR after.

#### Results
- **NC (blue bars)**: Successfully eliminated 7 out of 59 backdoors and reduced the ASR to below 25% for another 4 backdoors. However, it is not iterative and may not always identify the injected backdoor.
- **Standard Finetuning (orange bars)**: Effective for seven cases, but only when the injected backdoors were trained with clean samples. Finetuning on clean samples alone may not affect the backdoor patterns learned by the poisoned models.
- **NAD (green bars)**: Built on top of Standard Finetuning, NAD reduces the ASR to some extent but cannot eliminate all backdoors (e.g., ID 28 and ID 60).
- **MOTH (our method)**: Eliminates all backdoors with an average ASR down to 1%. The accuracy degradation on clean samples is minimal (< 0.2%).

### Related Work

#### Backdoor Attacks and Defenses
Backdoor attacks typically involve poisoning the training set with backdoor-injected samples. These attacks can be launched on various applications, including deep neural networks. Existing works focus on identifying and mitigating backdoors by inverse backdoors, input perturbations, and detecting and rejecting inputs stamped with backdoors. Some techniques aim to eliminate backdoors by pruning compromised neurons or retraining with data augmentation. NAD [39] is a state-of-the-art technique that uses a teacher-student training procedure to remove backdoors, but it does not handle natural backdoors.

#### Adversarial Training
Adversarial training aims to improve model robustness and efficiency. Universal adversarial perturbation (UAP) targets multiple samples with a single perturbation. UAP has been proposed to enhance model robustness, but its effectiveness in improving class distances is limited.

### Discussion

#### Extensions to Other Domains
While this paper focuses on computer vision tasks, MOTH can be extended to other domains such as natural language processing (NLP) and Android app security. We propose using a sigmoid function to approximate discrete values for backdoor generation in these domains. Details are provided in Appendix X-G.

#### Effectiveness Against Different Backdoor Types
MOTH is effective against various backdoor types, including reflection backdoors [3], composite backdoors [4], and filter backdoors [41]. We also investigate a different metric for filter attacks, which can be used for hardening and improves the effectiveness of MOTH. Details are in Appendix X-G.

### Conclusion
We developed a novel model hardening technique, MOTH, that enlarges class distances, making models resilient to backdoor attacks. Our evaluation on 5 datasets with 15 model structures shows that MOTH can improve class distance by 149.9% on average with only 1% accuracy loss, outperforming existing hardening techniques. It reduces 80% false positives for a state-of-the-art backdoor scanner and substantially outperforms three recent techniques in removing injected backdoors.

### Acknowledgment
We thank the anonymous reviewers for their constructive comments. This research was supported in part by IARPA TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300, ONR N000141712045, N000141410468, and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.