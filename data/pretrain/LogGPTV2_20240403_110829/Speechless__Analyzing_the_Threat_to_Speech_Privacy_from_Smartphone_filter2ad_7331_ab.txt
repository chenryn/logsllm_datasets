decibels (db). Louder sounds contain more energy and
could have greater effect on the motion sensors. For
this reason, we test the sounds at different loudness
measured in decibels to correlate loudness with effect
on the motion sensors.
We design our scenarios based on the three factors detailed
above. The initial setup in our work is similar to the experi-
mental setup designed in [1] where the smartphone is placed
on a desk with a loudspeaker that emits speech. For human
speech, we position a human speaker very close to the desk
on which the smartphone is placed to test the potential for
capturing human speech.
1) Machine-Rendered Speech: We begin by recreating the
scenario reported in Gyrophone [1], where the smartphone
is placed on a desk with a loudspeaker (with subwoofer)
that emits human speech. The scenario, henceforth referred
as “Loudspeaker-Same-Surface”, is depicted in Appendix Fig
1. Here, the phone is in full contact with the surface on which
the loudspeaker is placed. As motivated earlier, this scenario
can occur in restricted closed door meetings or speeches where
the designated speakers are speaking in a microphone and their
speech is relayed to the audience through loudspeakers. In this
case, the attacker places a smartphone on the same surface as
the loudspeaker so that the motion sensors in the smartphone
can pick up speech played through the loudspeakers, which
are then read by the attacker. The attacker can also utilize a
compromised smartphone that the user inadvertently places on
the same surface as the sound source.
An additional scenario for machine-rendered speech would
1003
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:32:39 UTC from IEEE Xplore.  Restrictions apply. 
be placing the smartphone containing the motion sensors
on a different surface than the speech rendering device.
We implement this scenario, called “Loudspeaker-Different-
Surface”, by placing the smartphone on a different surface
than the loudspeaker, as depicted in Appendix Figure 2.
Additional scenarios are tested with laptop speakers “Laptop-
Same-Surface”. Laptop speaker scenario can occur when the
victim is in, for instance, a VoIP call using his/her laptop
with its speakers turned on and put down their smartphone
near the laptop. We also test smartphone speaker scenario
“Phone-Different-Surface” similar to [2] where the speech
is rendered through smartphone speakers and picked up by
another smartphone placed in its vicinity.
2) Human Speech: In all the previously described scenar-
ios, the speech used for measuring the response of the motion
sensors is being produced by a loudspeaker. Such machine-
rendered speech is different from a human speaker in the
sense that a loudspeaker can effectively produce a louder
speech than a human can. In order to achieve commonly
occurring setup, we design a human speaker scenario where a
human speaker speaks directly in the smartphone. This setup
mimics a scenario where an attacker may eavesdrop on user’s
conversation that takes place on or near their smartphone.
In our experiment, we place the phone on a stationary and
isolated surface and ask the test subjects to speak into the
smartphone. In one scenario, we ask the human subjects to
speak in normal voice (“Human-Normal”) and in the other
scenario, we ask them to speak as loud as possible (“Human-
Loud”) to maximize the effect of speech (if any) on the motion
sensors.
3) Signal Analysis Methodology: We developed a two-
pronged approach to analyze the effect of speech on the motion
sensors. In the initial step, we analyze the motion sensor signal
in the frequency domain to look for footprints indicating the
presence of speech. If the frequency spectrum shows such
an evidence, techniques proposed in [1] and [2] could be
used to further classify, recognize or reconstruct the speech
signal (such classiﬁcation is beyond the scope of our paper).
If the frequency spectrum is unable to show any evidence, we
analyze the signal in time domain to look for effects of speech
on motion sensors.
Frequency Domain Analysis: To perform the analysis of the
motion sensor behavior in presence of speech in the frequency
domain, we record speech through the motion sensors and plot
the spectrum of the observed signal. We perform similar proce-
dure as prescribed in [1] by playing a 280Hz tone and a multi-
tone (consisting of signals having frequencies between 130Hz
and 200Hz) from a device for machine-rendered scenarios.
Since motion sensors have low sampling rates, the observed
frequency range is limited. In case of gyroscope, the sampling
rate is 200Hz so observable frequency is limited to 100Hz.
Due to this behavior, we depend upon aliasing effect to detect
the effects due to the played sound on the spectrum at sub-
100Hz frequency range [1].
Time Domain Analysis: In order to measure the presence of
a noticeable response of the motion sensors against speech
in time domain, we need to compare their behavior in the
presence and absence of speech signals. This requires creating
two (nearly) identical environments for all the previously de-
scribed scenarios where one environment contains speech and
the other environment is devoid of speech. Placing identical
sensors in both environments and measuring their response
would accurately determine the susceptibility of motion sen-
sors against human speech. However, creating acoustically
identical environments may prove to be a challenge where all
parameters like temperature, humidity, pressure, the material
and the design of the environment need to be same and
constant throughout the experimental phase.
An anechoic chamber as suggested in [1] may be deemed
suitable for creating identical acoustic chambers. However,
in our work, we circumvent this challenge by performing
the normal experiment with human speech immediately fol-
lowed by a control experiment with no speech, under normal
room conditions (in a quiet laboratory room inside university
building). If we do not allow any sudden and signiﬁcant
interference (acoustic or vibration) in the environment between
the experiments,
the
environment variables remained almost constant throughout
the experiment. This means the experiments were performed
under almost similar conditions and the only noticeable effect
should be due to the human speech. In that case, our setup
would be emulating the behavior of nearly identical acoustic
environments, as described previously.
it should be safe to assume that all
We recorded and analyzed the sensor readings looking for
noticeable effect such as increase in sensor values that may
indicate towards presence of speech. We observed multiples
audio samples from TIDigits speech corpus[24] and concluded
that the pronunciation of a single digit in the corpus took no
more than one second. The effect of speech on motion sensor
readings lasts for around 0.5 seconds meaning for a sampling
frequency of 200Hz (as deployed by the motion sensors), this
time duration equates to 100 samples. Thus, we windowed
each recorded sample using a window size of 100 samples
with an overlap of 50 samples. In each window, we calculated
the maximum range achieved by the sensor, which will give
us an idea of the disturbance in the readings. If speech signal
were strong enough to affect the sensors, the readings would be
much higher thereby producing a higher range (due to sensors
recording more motion data) when compared to sensor value
ranges observed in a relatively silent environment.
Sensor Reading Application: We used the Android application
available at [25] to capture sensor readings but modiﬁed the
source code to include accelerometer sensor readings.
V. SENSOR BEHAVIOR UNDER QUIET CONDITIONS
Before studying the effect of speech on motion sensors,
we observe motion sensors’ behavior under ambient condi-
tions, i.e., in a quiet environment. This behavior can then
be compared against the behavior of motion sensors under
the inﬂuence of speech with the assumption that the acoustic
environment remains the same.
1004
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:32:39 UTC from IEEE Xplore.  Restrictions apply. 
(a) Gyroscope placed on a wooden surface
(b) Accelerometer placed on a wooden surface
Fig. 1: Spectrum of the motion sensors along x axis that shows the effect of low frequencies contained in the multi-tone signal
(130-200Hz). The vibrations due to these frequencies are transmitted along the surface to the motion sensor of the smartphone.
A. Experiment Setup
Equipment: For all our experiments, we use Nexus 5 smart-
phone that contains the 6-axis motion sensor MPU6515 chip
designed by Invensense Inc. It combines a 3-axis gyroscope,
3-axis accelerometer along with a Digital Motion Processor
in a single chip. The output precision of the readings is 16
bits for both gyroscope and the accelerometer, and offers a
programmable full-scale range of ±2g, ±4g, ±8g and ±16g
for accelerometer and up to ±2000dps for the gyroscope. Typ-
ical resonant frequency for the gyroscope is listed as 27kHz
and the sampling frequency ranges from 4Hz to 8000Hz [26].
The sampling frequency for the accelerometer is described
as ranging from 4Hz to 4000Hz. Since Nexus 5 operates on
Android platform, the sampling rates for the motion sensors
are limited to 200Hz.
We also examined few other smartphone motion sensors
available in the market. STMicroelectronics’ LSM6DS3 mo-
tion sensor in Samsung Galaxy S7 offers similar precision
and programmable full-scale range for accelerometer and
gyroscope sensors as the Invensense motion sensors. Appendix
Table I lists some of the common smartphones and the motion
sensors used in these devices. From the table, we see that
most of the devices are using either Invensense or STMicro-
electronic sensors. The output data rate for all the Invensense
sensors is similar for both gyroscope and accelerometer except
Nexus 4 (MPU-6050) which is using an older chip. Similarly,
the typical mechanical frequency for gyroscope for all the
Invensense motion sensor chips is similar except for Nexus
4 again for the reasons speciﬁed previously. It also seems that
STMicroelectronics do not publish the resonant frequency for
their gyroscopes in the data sheet. STMicroelectronics motion
sensor differs from Invensense mostly in its output data rate
for gyroscope and accelerometer. User programmable range is
uniform across the vendors. Thus, we believe that the motion
sensors used in our experiments cover a general representation
of motion sensors in the market.
Location: We recorded motion sensor readings at four differ-
ent locations (quiet university lab spaces, henceforth denoted
as locations 1, 2, 3, and 4) that acted as near quiet environment.
At each location, the ambient noise level was 50 dB. Two of
the locations were ofﬁce rooms inside two different graduate
student labs and the rest were conference rooms within the
lab spaces. The rooms were devoid of any human presence
and the only possible source of noise was the air conditioning
vents installed in the ceiling. The recordings were done for
an hour to get an estimate of motion sensors’ behavior at
rest
in a quiet environment. The phone was placed on a
ﬂat tabletop recording the sensor readings through the sensor
reading application.
B. Results
We divided sensor data into samples of length 10 seconds
each and calculated the maximum range for each sample.
We averaged the obtained maximum range values of sensor
readings to get same number of representative samples of
sensor readings as the number of samples collected in our
subsequent experiments. We plot these representative samples
against samples taken under various scenarios (Section IV-B)
to analyze sensor behavior under the inﬂuence of speech.
VI. SENSOR BEHAVIOR AGAINST SPEECH SIGNALS
In this section, we analyze the behavior of motion sensors in
the presence of speech. We construct the scenarios described
in Section IV and report the results that will help us determine
which scenarios are most susceptible to an acoustic side
channel attack through motion sensors.
A. Setup Information
Equipment: We use the same device, Nexus 5 from the
previous section, where it was used to record the behavior
of motion sensors in a quiet environment. For producing
machine-rendered speech, we use Logitech Z323 speakers with
a frequency response of 55Hz-20kHz that consists of two
satellites and a subwoofer (18 watts; 100Hz). Generation of
speech signals through smartphone speakers, a recreation of
the scenario depicted in [2], was done by iPhone 4S. We also
used Thinkpad W530 as the laptop speaker.
Word Data Set: We use the single digit pronunciations from
the speech corpus provided at [24] that is a subset of the
1005
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:32:39 UTC from IEEE Xplore.  Restrictions apply. 
35
30
25
20
15
10
5
)
s
c
e
s
(
e
m
T
i
)
s
c
e
s
(
e
m
T
i
35
30
25
20
15
10
5
30
25
20
15
10
5
)
s
c
e
s
(
e
m
T
i
30
25
20
15
10
5
)
s
c
e
s
(
e
m
T
i
-20
-40
-60
-80
-100
/
)
z
H
B
d
(
y
c
n
e
u
q
e
r
f
/
r
e
w
o
P
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
Frequency (Hz)
Frequency (Hz)
Frequency (Hz)
Frequency (Hz)
(a) Multi-tone on gyroscope 1ft
(b) Multi-tone on accelerometer 1ft
(c) Multi-tone on gyroscope 4ft
(d) Multi-tone on accelerometer 4ft
Fig. 2: Spectrum of the motion sensors readings over different distances in presence of multi-tones (130-200Hz). Bright yellow
spots with intensity -20dB/Hz denote the footprints of the multiple frequencies contained in the signal affecting the sensor
readings.
TIDIGITS corpus. The speech dataset consists of 5 male and
5 female speakers who perform a single digit pronunciation
(“zero” to “nine” with an extra word “oh”) which is repeated
twice by each speaker. As noted in [1], low sampling fre-
quency restriction on the motion sensors, make it hard to
perform speaker-independent speech recognition. Hence, it is
reasonable to use a limited dictionary for speaker recognition
(such as [24] containing speech of digits) that could still leak
conﬁdential information that contains numbers such as social
security and credit card numbers, birth dates, PIN etc.
B. Motion Sensors vs. Loudspeaker
In the loudspeaker setup, we test
the effect of speech
produced by loudspeakers on the motion sensors of a smart-
phone. The smartphone may reside on same surface as the
loudspeaker or on a different surface that is not in physical
contact with the loudspeaker.
1) The Loudspeaker-Same-Surface Scenario: We ﬁrst test
the behavior of motion sensors against low frequency tones
as a precursor to speech. We recreate the experimental setup
from [1] where a 280Hz tone and a multi-tone, consisting
of frequencies 130-200Hz, was used. The smartphone is kept
on the same surface as the loudspeaker that plays the tones
(Appendix Figure 1). We test four different surfaces of which
three were wooden desks of varying width and one was a
plastic tabletop.
The frequency analysis from our initial experiments showed
that playing a 280Hz tone did not affect the gyroscope even
when the sound pressure level reached 92db on the wooden
surfaces but had some effect on the plastic surface at 92db.
At a sound pressure level of 102db, it affected the gyroscope
on all the surfaces except one where no effect was observed
on gyroscope at 102db. The accelerometer, in contrast, was
affected on all
the surfaces even at a volume of 72db.
When the multi-tone was played, we observed that all the
surfaces produced a pronounced effect on both gyroscope and
accelerometer when the sound pressure level reached 92db.
Figure 1 shows the spectrum for gyroscope and accelerometer
for a wooden surface along x axis. The gyroscope was affected
along x and y axis of rotation while the accelerometer showed
the effect at x, y and z axis of rotation.
We tested Loudspeaker-Same-Surface scenario over varying
distance to observe the behavior of motion sensors when the
smartphone is placed at different distances from the loud-
speaker. The phone is placed at different distances of 1ft, 2ft,
3ft, 4ft, and 5ft with the audio level at the source being kept
constant at 92db. The resulting frequency spectrum plots for
distance 1ft and 4ft (Figure 2) show the captured signal for the
gyroscope (along x axis of rotation) and accelerometer (along
x axis). We observe similar intensity of the signal footprint
within the small range of our tested distance. This observation
indicates that
the audio signal may still be captured by
the motion sensors even if the motion sensor is not placed
close to the loudspeaker. This behavior also indicates that
a scenario where loudspeaker and the smartphone reside on
same surface such as a conference table, it is possible for
motion sensors of the smartphone to get affected due to speech
from loudspeakers within the tested distance range.
Effect of Speech: To test the effect of speech on motion