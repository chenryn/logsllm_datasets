### Traffic Analysis and Anomalies

#### Network Outage at 11 PM
Using the drill-down feature, we observed a significant decrease in traffic at 11 PM. This change was primarily attributed to traffic between two nearby universities: UCSD and UCLA. However, this specific traffic did not appear in any other cluster over the previous 30 days. Upon further investigation, we determined that the traffic surge was due to a temporary network outage that forced traffic, which normally uses the CalREN network, to transit through SD-NAP instead.

#### Large Research Institution
Our second trace was taken from the edge of a network connecting a large research institution (approximately 15,000 hosts) to the Internet. The trace spans 39 days, starting on December 12th. In this case, we had access to detailed, albeit less comprehensive, information about the network's operation.

Many findings from the second trace were similar to those described earlier, but there were some notable differences. We observed a series of regularly scheduled backup transfers:
- One set of machines destined for TCP port 7500 showed regular daily spikes at 11 PM and 5 AM, followed by periods of inactivity.
- Another example was a regular 40GB transfer that started at 8 PM every Wednesday, usually lasting until 10 AM the following morning. This activity was identified as a full backup of a large RAID array.

We also observed a single large cluster containing a series of regular TCP transfers from a single host to port 5002 on three other hosts distributed around the Internet. This turned out to be a regularly scheduled network measurement experiment, part of a distributed research activity.

The most interesting result from this second trace came from analyzing the breakout of the Sapphire worm [12]. This worm exploits a vulnerability in the Microsoft SQL server, running on UDP port 1434, and spreads aggressively using single 404-byte packets to infect random destinations. By computing traffic reports for three-hour measurement intervals, we could clearly see the time at which the worm started in the time series plot (see Figure 9). It appeared as a significant increase in the "Other" traffic category. Drilling down into the report, we found that 90% of the traffic was directed to UDP port 1434. A quick comparison between packet and byte reports also provided the average packet size. Furthermore, the report revealed six internal IP addresses that generated 80% of the traffic, indicating local hosts infected by the worm trying to spread it externally. These compromised servers were promptly neutralized by network administrators. In the next three-hour interval, while the traffic in the "Other" category decreased, a substantial fraction (23%) was still worm traffic, consisting of incoming probes trying to infect internal hosts.

#### Small Network Exchange Point
Our first trace was collected from SD-NAP, a small network exchange point in San Diego, California, connecting many research and educational institutions and linking some of them to the broader Internet. The trace is 31 days long, starting on December 7, 2002. We were able to consult with individuals familiar with this network to calibrate our conclusions and receive useful feedback on our results.

When analyzing the raw reports, we looked for traffic clusters that were large but did not completely dominate the overall traffic. For instance, we found that port 80 (TCP) web traffic was so significant that it was best subdivided into smaller categories. The time series plot (Figure 9) shows the Sapphire/SQL Slammer worm as a big increase in the "Other" category around 21:30. Highlighting the worm in a separate category, it became evident that while its traffic was significantly reduced at 22:10 when the infected internal hosts were neutralized, worm traffic persisted at a lower level due to external hosts spreading it into the network.

#### Backbone Analysis
A third trace was captured in August 2001 from an OC-48 backbone link and is 8 hours long. We analyzed traffic reports for one-hour measurement intervals. The reports revealed that around two-thirds of the bytes on the link come from TCP port 80, and about one-third come from high ports. Additionally, one-third of the traffic was from high ports to high ports, consistent with peer-to-peer traffic. Through unexpectedness scores, the report highlighted some surprising facts. Specific source and destination prefixes where web traffic represented almost all the traffic were identified. This could be due to the clustering of web servers in web hosting centers or server farms. Prefixes receiving almost exclusively web traffic might belong to organizations with many web clients whose internal policies prohibit the use of peer-to-peer applications.

### Related Work
FlowScan [14] is a package by Dave Plonka used for visualizing network traffic. It uses NetFlow [13] data to provide detailed information by breaking down traffic in various ways, such as by IP protocol, well-known services, or AS pairs. Other applications, like CoralReef [1] or the IPMON project [2], produce similar breakdowns based on packet traces. Estan and Varghese [6] present algorithms for automatically and efficiently identifying large clusters, given a fixed definition of clusters. In our terminology, these reports display traffic clusters along predefined dimensions.

Methods for reducing the size of raw data describing the traffic mix without precluding future analyses include sampling [4] and sketches [7]. While our method also produces a compact summary, its primary purpose is to convey a description of the traffic to the human operator.

Our approach is related to classical clustering [10] but differs in that we use the space defined by field hierarchies rather than Euclidean space. Finding association rules [3] involves finding frequent item sets in high-dimensional data, a well-studied problem in data mining. Key differences are:
1. Most approaches to association rules do not use hierarchies, unlike Han and Fu [9], who use a single hierarchy across all fields. We use separate hierarchies for each field.
2. Our compression rules are crucial to the effectiveness of AutoFocus, which, to our knowledge, no other algorithms for association rules use.

### Conclusions
Managing IP-based networks is challenging, particularly due to the complexity of understanding the nature of applications and usage patterns driving traffic growth. In this paper, we introduced a new method for analyzing IP-based traffic, multidimensional traffic clustering, designed to provide better insight into these factors. Our approach automatically infers a traffic model based on actual traffic, matching dominant usage modes. Unlike previous work, our algorithms can analyze traffic along multiple dimensions (source address, destination address, protocol, source port, destination port) simultaneously and use compression to map results into a concise report, exploiting the locality created by particular usage modes.

We have embodied these algorithms in the AutoFocus analysis system, developing a web-based user interface for managers to explore clusters across multiple time scales and drill down to explore the contents of any clusters of interest. Preliminary experiences with this tool have been extremely positive, allowing us to identify unusual traffic patterns that would be much harder to detect using conventional tools. Network managers have provided positive feedback, quickly appreciating the benefits of our approach.

Finally, note that AutoFocus, as described, automatically extracts patterns of resource consumption in a single time interval based on the traffic log of a single link. Future work will extend AutoFocus to extract patterns across time and space, enabling automatic time-series analyses and geographical pattern detection within the network.

### Acknowledgments
We would like to thank Vern Paxson and Jennifer Rexford for their discussions, which helped clarify the concept of traffic clusters. We also thank David Moore and Vern Paxson for their help with the evaluation of the AutoFocus prototype. Support for this work was provided by NSF Grant ANI-0137102 and the Sensilla project sponsored by NIST Grant 60NANB1D0118.

### References
[1] Coralreef - workload characterization. http://www.caida.org/analysis/workload/
[2] Ipmon - packet trace analysis. http://ipmon.sprintlabs.com/packstat/packetoverview.php
[3] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proceedings of the ACM SIGMOD, 1993.
[4] N. Duffield, C. Lund, and M. Thorup. Charging from sampled network usage. In SIGCOMM Internet Measurement Workshop, November 2001.
[5] C. Estan, S. Savage, and G. Varghese. Automatically inferring patterns of resource consumption in network traffic. Technical report CS2003-0746, UCSD.
[6] C. Estan and G. Varghese. New directions in traffic measurement and accounting. In Proceedings of the ACM SIGCOMM, 2002.
[7] A. Gilbert, Y. Kotidis, S. Muthukrishnan, and M. Strauss. Quicksand: Quick summary and analysis of network data. DIMACS technical report, 2001.
[8] P. Gupta and N. McKeown. Packet classification on multiple fields. In Proceedings of the ACM SIGCOMM, 1999.
[9] J. Han and Y. Fu. Discovery of multiple-level association rules from large databases. In Proceedings of VLDB, 1995.
[10] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning. Springer, 2001. pages 453-479.
[11] R. Mahajan, S. Bellovin, S. Floyd, J. Ioannidis, V. Paxson, and S. Shenker. ACM SIGCOMM CCR, Vol 32, No. 3, July 2002.
[12] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford, and N. Weaver. The spread of the sapphire/slammer worm. Technical report, January 2003. http://www.caida.org/outreach/papers/2003/sapphire
[13] Cisco NetFlow. http://www.cisco.com/warp/public/732/Tech/netflow
[14] D. Plonka. Flowscan: A network traffic flow reporting and visualization tool. In Proceedings of USENIX LISA, 2000.
[15] S. Saroiu, K. Gummadi, R. Dunn, S. Gribble, and H. Levy. An analysis of internet content delivery systems. In Proceedings of OSDI, 2002.