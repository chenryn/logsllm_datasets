decrease at 11 PM. Again using the drill-down feature, we
observed that this change was mostly attributable to traf-
ﬁc between two nearby universities: from UCSD to UCLA.
However, traﬃc between these two did not appear in any
other cluster over the previous 30 days. Upon investigation
we determined that traﬃc bulge was due to a temporary net-
work outage that forced traﬃc normally using the CalREN
network to transit SD-NAP instead.
Large research institution
Our second trace was taken from the edge of a network that
connects a large research institution (roughly 15,000 hosts)
to the Internet. The trace is 39 days long and it starts on
the 12th of December. In this case, we had access to similar
although less-detailed expertise concerning the operation of
the network.
Many ﬁndings for the second trace were similar to those
we have described earlier, but there were some diﬀerences.
We observed a series of regularly scheduled backup transfers:
one from a range of machines destined to TCP port 7500
that had regular daily spikes at 11PM and 5AM followed by
periods of quiescence. Another example was a regular 40GB
transfer that started at 8PM on each Wednesday (usually
lasting until 10AM the following morning). This activity
proved to be a full backup of a large RAID array.
We observed a single large cluster containing a series of
regular TCP transfers from a single host to port 5002 on
three other hosts distributed around the Internet. This
turned out to be a regularly scheduled network measurement
experiment that was part of a distributed research activity.
The most interesting result for this second trace came by
looking at the breakout of the Sapphire worm [12]. This
worm exploits a vulnerability of the Microsoft SQL server
Figure 6: Multidimensional report
Figure 7: Unidimensional – source port
Figure 8: Unidimensional – destination network
using AutoFocus is hard to quantify, we hope that present-
ing some of them will give the reader a more accurate idea
of the power of our system.
Small network exchange point
Our ﬁrst trace was collected from SD-NAP, a small network
exchange point in San Diego, California, that connects many
research and educational institutions and also connects some
of them to the rest of the Internet. The trace is 31 days long
and it starts on the 7th of December 2002. In addition, we
were able to consult with those familiar with this network to
calibrate our conclusions and receive useful feedback about
out results.
When analyzing the raw reports we looked for traﬃc clus-
ters that were large, but did not completely dominate the
the cluster containing all TCP traﬃc is not
traﬃc (e.g.
very useful).
In particular, we found that port 80 (TCP)
Web traﬃc was so large that it was best subdivided into
Figure 9: The Sapphire/SQL Slammer worm shows up in the time series plots as a big increase in the traﬃc
of the “Other” category around 21:30. Once we highlight the worm by putting it into a separate category,
it is evident that while its traﬃc is signiﬁcantly reduced at 22:10 when the infected internal hosts were
neutralized, worm traﬃc persists at a lower level because of outside hosts spreading it into this network.
running on UDP port 1434 and spreads extremely aggres-
sively using single 404 byte packets to infect random destina-
tions. We computed the traﬃc reports for three hour mea-
surement intervals. The time at which the worm started was
apparent in the time series plot (see Figure 9). It showed up
as a huge increase in the “Other” traﬃc category. Drilling
down into the report describing that category, the worm was
conspicuous: 90% of the traﬃc was to UDP port 1434. A
quick comparison between the packet and byte reports also
gave us the average packet size. Furthermore, the report
readily revealed the 6 internal IP addresses that generated
80% of the traﬃc: these were local hosts infected by the
worm aggressively trying to infect the outside world. These
compromised servers were promptly neutralized by the net-
work administrators. In the next three hour interval, while
the traﬃc in the “Other” category decreased to levels simi-
lar to those before the worm, still a substantial fraction of it
(23%) was worm traﬃc. The report revealed that this was
traﬃc originating on the outside:
it consisted of incoming
probes trying to infect internal hosts. We also performed an
analysis of the trace with the worm traﬃc separated into its
own category (the second plot from Figure 9). We were able
to see ﬁne details. For example some of the infected hosts
did not spread their traﬃc uniformly over the whole address
space, but focused on single /8s. This is consistent with the
observation [12] that for some values of the random seed,
the algorithm used by the worm to select target addresses
chooses them from a limited set. This example shows once
again the strength of our multidimensional approach: Auto-
Focus is able to promptly bring to the network manager’s
attention and describe in great detail such unexpected and
unpredictable event as a worm epidemic.
While none of the traces we worked with contained mas-
sive denial of service attacks, we believe that AutoFocus
would bring them to the attention of the network operator
the same way it showed the worm. The victim of the at-
tack (whether it is an individual IP address or a preﬁx) will
show up in the report with a very large number of packets
(or bytes depending on the type of attack). Furthermore,
the attack will reveal the protocol used by the attack and
possibly the port number if it is kept constant. If the source
address is faked at random from the whole IP address space,
the report will not associate any particular source address
with the attack traﬃc hitting the victim. However, if for
some reason (e.g. egress ﬁltering at the site the attack orig-
inates from), the source addresses are restricted to a certain
preﬁx (or a small number of preﬁxes), the report will identify
these, thus facilitating prompt and speciﬁc response.
We presented the output of AutoFocus to network man-
agers of the ﬁrst two networks we had traces from. Their
reactions were very positive.
It was easy for them to un-
derstand the output. They appreciated the intuitiveness of
the time series plots, the large amount of information they
convey and the ease with which the traﬃc reports and the
drill-down feature provided them more detailed information
when they needed it. The managers of both networks ex-
pressed interest in widely deploying our tool.
Backbone
A third trace we looked at was captured in August 2001 from
an OC-48 backbone link and is 8 hours long. We looked at
traﬃc reports for one hour measurement intervals. The re-
ports reveal that around two thirds of the bytes on the link
come from TCP port 80 and around one third come from
high ports. The report also revealed that around one third
of the traﬃc was from high ports to high ports. This is con-
sistent with the behavior of peer to peer traﬃc. Through
the unexpectedness scores, the report revealed some further
facts that seemed surprising at ﬁrst. There were speciﬁc
source and destination preﬁxes where the Web traﬃc rep-
resented almost all of the traﬃc. One explanation for the
preﬁxes that send almost only Web traﬃc is the clustering
of Web servers in Web hosting centers or server farms. The
preﬁxes that receive almost exclusively Web traﬃc could be
organizations with many Web clients whose internal policy
prohibits the use of peer to peer applications.
6. RELATED WORK
FlowScan [14] is a package by Dave Plonka used for vi-
sualizing network traﬃc. It uses NetFlow [13] data to give
detailed information about the traﬃc by breaking it down
in a number of ways: by the IP protocol; by the well-known
service or application; by IP preﬁxes associated with “local”
networks; or by the AS pairs between which the traﬃc was
exchanged. There are many other applications that pro-
duce similar breakdowns of the traﬃc such as CoralReef [1]
or the IPMON project [2] based on packet traces instead
of NetFlow data. In [6] Estan and Varghese present algo-
rithms that automatically and eﬃciently identify large clus-
ters, once the deﬁnition of clusters is ﬁxed.
In terms of
our terminology, these reports display traﬃc clusters along
predeﬁned dimensions.
There are methods for reducing the size of the raw data
describing the traﬃc mix in a way that does not preclude
future analyses: through sampling [4] or sketches [7]. While
our method also produces a very compact summary of the
traﬃc mix, its primary purpose is not to be used by further
analyses, but to convey a description of the traﬃc to the
human operator.
The problems we are solving are related to classical clus-
tering [10], but are diﬀerent in that we use the space deﬁned
by the ﬁeld hierarchies instead of a Euclidian space. An-
other problem, ﬁnding association rules [3], requires ﬁnding
frequent item sets in high dimensional data, and is a well
studied problem in data mining. The two important diﬀer-
ences between these two problems are: 1) Most approaches
to association rules do not use hierarchies. A notable ex-
ception is Han and Fu [9] who use a single hierarchy across
all ﬁelds unlike our use of separate hierarchies for each ﬁeld.
2) Our compression rules were crucial to the eﬀectiveness of
AutoFocus. To the best of our knowledge, no algorithms for
association rules use compression rules similar to ours.
7. CONCLUSIONS
Managing IP-based networks is hard.
It is particularly
complicated by not understanding the nature of the appli-
cations and usage patterns driving traﬃc growth.
In this
paper, we have introduced a new method for analyzing IP-
based traﬃc, multidimensional traﬃc clustering, that is de-
signed to provide better insight into these factors. The nov-
elty of our approach is that it automatically infers, based on
the actual traﬃc, a traﬃc model that matches the dominant
modes of usage. Unlike previous work, our algorithms can
analyze traﬃc along multiple diﬀerent “dimensions” (Srource
address, Destination address , Protocol, Source port, Des-
tination Port) at once, and yet be able to use compression
to map results from this multidimensional space into a con-
cise report.
In essence, our approach exploits the locality
created by particular modes of usage.
In addition to developing these algorithms, we have em-
bodied them in the AutoFocus analysis system. We have
developed a Web-based user interface to allow managers to
explore clusters across multiple time-scales and to drill down
to explore the contents of any clusters of interest. Our pre-
liminary experiences with this tool have been extremely pos-
itive and we have been able to identify unusual traﬃc pat-
terns that would have been considerably harder to identify
using conventional tools. Moreover, we have received posi-
tive feedback from network mangers who have quickly been
able to appreciate the beneﬁts of our approach.
Finally, note that AutoFocus, as described in this paper,
automatically extracts patterns of resource consumption in
a single interval of time based on the traﬃc log of a single
link. The natural generalization would to extend AutoFocus
to automatically extract patterns of resource consumption
across time and across space. While AutoFocus currently
provides a visual display across a limited number of peri-
ods, it would be useful to do automatic time-series analyses
across large time periods using compressed clusters as a new
and parsimonious basis for such analysis. Similarly, extend-
ing AutoFocus to detect resource consumption across space
would allow managers to detect geographical patterns within
the network. We leave these generalizations for future work.
8. ACKNOWLEDGMENTS
We would like to thank Vern Paxson and Jennifer Rexford
for the many discussions that led to the clarifying the con-
cept of traﬃc clusters. We would also like to thank David
Moore and Vern Paxson for help with the evaluation of the
AutoFocus prototype. Support for this work was provided
by NSF Grant ANI-0137102 and the Sensilla project spon-
sored by NIST Grant 60NANB1D0118.
9. REFERENCES
[1] Coralreef - workload characterization.
http://www.caida.org/ analysis/ workload/.
[2] Ipmon - packet trace analysis.
http://ipmon.sprintlabs.com/ packstat/
packetoverview.php.
[3] R. Agrawal, T. Imielinski, and A. Swami. Mining
association rules between sets of items in large
databases. In Proceedings of the ACM SIGMOD, 1993.
[4] N. Duﬃeld, C. Lund, and M. Thorup. Charging from
sampled network usage. In SIGCOMM Internet
Measurement Workshop, November 2001.
[5] C. Estan, S. Savage, and G. Varghese. Automatically
inferring patterns of resource consumption in network
traﬃc. Technical report CS2003-0746, UCSD.
[6] C. Estan and G. Varghese. New directions in traﬃc
measurement and accounting. In Proceedings of
the ACM SIGCOMM, 2002.
[7] A. Gilbert, Y. Kotidis, S. Muthukrishnan, and M.
Strauss. Quicksand: Quick summary and analysis of
network data. Dimacs technical report, 2001.
[8] P. Gupta and N. Mckeown. Packet classiﬁcation on
multiple ﬁelds. In Proceedings of the ACM
SIGCOMM, 1999.
[9] J. Han and Y. Fu. Discovery of multiple-level
association rules from large databases. In Proceeding
of VLDB, 1995.
[10] T. Hastie, R. Tibshirani, and J. Friedman. The
elements of statistical learning. Springer, 2001. pages
453-479.
[11] R. Mahajan, S. Bellovin, S. Floyd, J. Ioannidis, V.
Paxson, and S. Shenker. ACM SIGCOMM CCR, Vol
32, No. 3, July 2002.
[12] D. Moore, V. Paxson, S. Savage, C. Shannon, S.
Staniford, and N. Weaver. The spread of the
sapphire/slammer worm. Technical report, January
2003. http://www.caida.org/ outreach/ papers/ 2003/
sapphire.
[13] Cisco netﬂow. http://www.cisco.com /warp /public
/732 /Tech /netflow.
[14] D. Plonka. Flowscan: A network traﬃc ﬂow reporting
and visualization tool. In Proceedings of USENIX
LISA, 2000.
[15] S. Saroiu, K. Gummadi, R. Dunn, S. Gribble, and H.
Levy. An analysis of internet content delivery systems.
In Proceedings of OSDI, 2002.
10