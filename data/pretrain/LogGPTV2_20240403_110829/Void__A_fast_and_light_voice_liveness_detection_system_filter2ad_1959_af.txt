# Bass

## References
[36] P. Gil-Cacho, T. V. Waterschoot, M. Moonen, and S. Jensen, "Study and Characterization of Odd and Even Nonlinearities in Electrodynamic Loudspeakers," in *Proceedings of the 127th Audio Engineering Society*, 2009.

## Classifying Live-Human Voices and Voices Replayed Through In-Built Speakers Using Three Signal Power Features

Figure 10 illustrates the spectral power features (power-sum in each frequency) for 800 voice samples: 400 live-human samples and 400 samples replayed through 11 in-built smartphone speakers. The three signal power features, \( \mu_{\text{peak}} \), \( \rho \), and \( q \), show noticeable differences, indicating their potential effectiveness in classifying live-human voices and in-built speaker replays (these features are detailed in Section 5.3).

**Figure 10:** Integral signal power features used to classify live-human voices and voices replayed through 11 in-built smartphone speakers.

## Power Patterns for Different Loudspeakers

Figure 11 displays power patterns for a live-human voice and eight different loudspeakers (from our dataset and the ASVspoof 2017 dataset). The live-human voice sample (top left) exhibits four distinct peaks in the power pattern below 2 kHz. Except for the Genelec 6010A studio monitor and the Focusrite Scarlett 2i2 audio interface, all other high-quality speakers show a single sharp peak or small peaks in their power patterns. The power patterns of the Genelec and Focusrite Scarlett speakers below 2 kHz are similar to those of live-human patterns. To address such high-quality speakers, Void employs additional feature sets as described in Section 5.3.

**Figure 11:** Power patterns of live-human and different loudspeakers.

## Summary of Linearity Degree Features

For the linearity degree of power cumulative distribution function (powcdf), we compute two features: Pearson correlation coefficients \( \rho \) and quadratic curve-fitting coefficients \( q \) (see Table 10).

\[ \rho(X, Y) = \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y}, \]

where \( \text{cov} \) is the covariance, and \( \sigma_X \) and \( \sigma_Y \) are the standard deviations of \( X \) and \( Y \), respectively. In our experiments, \( X = \text{powcdf} \) and \( Y \) is an increasing sequence \( \{y_n\} \) with \( y_{n+1} - y_n = 1 \).

The polynomial \( q(x) \) of degree 2 is given by:

\[ q(x) = q_1 x^2 + q_2 x + q_3, \]

where \( x = \text{powcdf} \). We use the quadratic coefficient \( q_1 \), denoted by \( q \) for simplicity.

Table 11 summarizes the mean and standard deviation of the linearity features for 400 live-human samples and 400 samples replayed through in-built speakers.

**Table 10:** Summary of the linearity degree features.

| Features | Symbol |
| --- | --- |
| Cross-correlation coefficients | \( \rho \) |
| Quadratic curve-fitting coefficients | \( q \) |

**Table 11:** Means and standard deviations of signal power linearity features for live-human and in-built speakers.

| Source | Feature | Mean | Standard Deviation |
| --- | --- | --- | --- |
| Live-human | \( \rho \) | 0.759 | 0.059 |
| Live-human | \( q \) | 47.960 | 6.541 |
| In-built speakers | \( \rho \) | 0.854 | 0.053 |
| In-built speakers | \( q \) | 10.267 | 7.006 |

## Summary of High Power Frequency Features

To capture the dynamic characteristics of spectral power in higher frequencies, we use the following features (see Table 12):

- Number of peaks in high-power frequencies (\( N_{\text{peaks}} \))
- Relative frequencies corresponding to peaks (\( \mu_{\text{peaks}} \))
- Standard deviation of high power frequency location (\( \sigma_{\text{peaks}} \))

**Table 12:** Summary of the high power frequency features.

| Features | Symbol |
| --- | --- |
| Number of peaks in high-power frequencies | \( N_{\text{peaks}} \) |
| Relative frequencies corresponding to peaks | \( \mu_{\text{peaks}} \) |
| Standard deviation of high power frequency location | \( \sigma_{\text{peaks}} \) |

Table 13 shows the analysis of these key features for 6,362 voice samples replayed through 13 standalone speakers and 3,558 live-human voice samples. The mean number of peaks (\( N_{\text{peaks}} \)) for live-human voices is significantly greater than that of standalone speakers. Similarly, live-human voices exhibit greater mean relative frequencies corresponding to peaks (\( \mu_{\text{peaks}} \)) and standard deviations. These differences can be used to detect standalone speakers.

**Table 13:** Means and standard deviations of the high power frequency features for live-human and standalone speakers.

| Source | Feature | Mean | Standard Deviation |
| --- | --- | --- | --- |
| Live-human | \( N_{\text{peaks}} \) | 2.580 | 3.029 |
| Live-human | \( \mu_{\text{peaks}} \) | 7.377 | 2.693 |
| Standalone speakers | \( N_{\text{peaks}} \) | 1.695 | 1.348 |
| Standalone speakers | \( \mu_{\text{peaks}} \) | 5.531 | 2.110 |

## Finding the Optimal Feature Set

Table 14 presents the evaluation results for different feature sets using the ASVspoof 2017 dataset. The train and development sets were used for training, and the evaluation set was used for testing. Each selected feature set achieved an F1-score greater than 80%. These results, along with the declining Equal Error Rates (EERs) observed with the addition of features, demonstrate that all individual features (FVLFP, FVLDF, FVHPF, and FVLPC) are integral in achieving an EER of 11.60%.

**Table 14:** Accuracy evaluation for each selected feature set (see Section 5.3).

| Feature Set | Accuracy (%) | Precision (%) | Recall (%) | F1 (%) | EER (%) |
| --- | --- | --- | --- | --- | --- |
| FVLFP | 76.61 | 98.04 | 75.59 | 85.37 | 19.37 |
| FVLDF | 72.14 | 98.09 | 72.91 | 82.52 | 30.92 |
| FVHPF | 73.13 | 97.79 | 71.61 | 82.79 | 21.47 |
| FVLPC | 70.19 | 98.96 | 68.62 | 80.60 | 22.99 |
| FVLFP + FVLDF + FVHPF | 79.51 | 95.06 | 79.08 | 87.44 | 18.83 |
| FVLFP + FVLDF + FVHPF + FVLPC (Void) | 84.33 | 90.58 | 83.51 | 90.58 | 11.60 |

## Feature and Model Parameters

We describe the parameters used for recording voice samples, performing feature engineering, and training classifiers. Voice recordings were sampled at 44.1 kHz. For Short-Time Fourier Transform (STFT) parameters, we used a window length of 1024 (recommended to be a power of 2), a hop size of 256, and 4,096 FFT points. Other parameters needed to train Void are presented in Table 15.

**Table 15:** Feature and model parameters.

| Parameter | Value |
| --- | --- |
| Sampling frequency | 44.1 kHz |
| Window length | 1024 |
| Hop length | 256 |
| nfft | 4,096 |
| W | 10 |
| ω | 0.6 |
| powcdf’s polynomial order for estimating q | 2 |
| Pest’s estimation using polynomial order | 6 |
| SVM Kernel | RBF |
| Kernel scale | Auto |

## List of Playback Devices

We used 11 different types of in-built speakers, including smartphones and a smart TV, and four standalone speakers to replay recorded voice samples (see Table 16).

**Table 16:** List of playback devices (loudspeakers) used for replay attack dataset generation.

| Name | Model | Type |
| --- | --- | --- |
| Galaxy A8 | A810S | In-built |
| Galaxy A5 | SM-A500x | In-built |
| Galaxy Note 8 | SM-N950x | In-built |
| Galaxy S8 | SM-G950 | In-built |
| Galaxy S8 | SM-G955N | In-built |
| Galaxy S9 | SM-G960N | In-built |
| iPhone SE | A1662 | In-built |
| iPhone 6S Plus | A1524 | In-built |
| iPhone 5S | A1519 | In-built |
| LG V20 | V20 F800 | In-built |
| Samsung Smart TV | QN49Q7FAM | In-built |
| Bose | SoundTouch 10 | Standalone |
| V-MODA | REMIX-BLACK | Standalone |
| Logitech | Z623 | Standalone |
| Yamaha | YHT-3920UBL | Standalone |

## List of Recording Devices

We used three different laptops and nine different smartphones as recording devices (see Table 17).

**Table 17:** List of recording devices used for human voice collection and replay attack dataset generation.

| Maker | Model |
| --- | --- |
| Samsung Notebook | NT910S3T-K81S |
| Samsung Notebook | NT200B5C |
| Macbook Pro | A1706 (EMC 3163) |
| Galaxy A5 | SM-A500x |
| Galaxy Note 8 | SM-N950x |
| Galaxy S8 | SM-G950 |
| Galaxy S8 | SM-G955N |
| Galaxy S9 | SM-G960N |
| iPhone SE | A1662 |
| iPhone 5S | A1519 |
| iPhone 6S Plus | A1524 |
| LG V20 | V20 F800 |

## Implementation of GD-ResNet

Based on the model described in [17], we implemented GD-ResNet with two stages: the first stage estimates attention weights from a Global Average Pooling layer, and the second stage trains a ResNet-18 model based on the GD gram feature with attention weights. Table 18 summarizes the performance of our GD-ResNet implementation: it achieved 0% and 23% EERs on our own dataset and the ASVspoof 2017 dataset, respectively. In terms of space complexity, GD-ResNet uses 786,432 features compared to 97 features used by Void. For memory usage, Void uses about 1.99 MB, whereas GD-ResNet uses 1,194.68 MB.

**Table 18:** GD-ResNet space complexity.

| Measure | Void | GD-ResNet [17] |
| --- | --- | --- |
| Extraction (sec.) | 0.035 | 0.100 |
| Training (sec.) | 40,560.264 | 0.283 |
| Testing (sec.) | 0.035 | 0.120 |
| #Features | 97 | 786,432 |
| Memory size (MB) | 1.988 | 1,194.684 |
| Performance (EER) | 11.6% | 23% |