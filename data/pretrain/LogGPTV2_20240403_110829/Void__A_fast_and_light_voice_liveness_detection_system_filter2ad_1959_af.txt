Bass
https://www.
[36] P. Gil-Cacho, T. V. Waterschoot, and M. Moonen, and S.
Jensen, “Study and characterization of odd and even nonlin-
earities in electrodynamic loudspeakers”, in Proceedings of the
127th Audio Engineering Society, 2009.
A Classifying live-human voices and voices re-
played through in-built speakers with three
signal power features
Figure 10 shows the spectral power features (power-sum in
each frequency) of 800 voice samples: 400 were live-human
samples, and the other 400 were samples replayed through 11
in-built smartphone speakers. As shown in Figure 10, three
signal power features, µpeak, ρ and q, look noticeably differ-
ent, suggesting that they could be effective in classifying
live-human voices and in-built speakers (those features are
explained in Section 5.3).
Figure 10: Integral signal power features used to classify
live-human voices and voices replayed through 11 in-built
smartphone speakers.
B Power patterns for different loudspeakers
Figure 11 shows power patterns for a live-human voice and
8 different loudspeakers (from our dataset and ASVspoof
2017 dataset). In the live-human voice sample (top left), there
are four distinct peaks in the power pattern below 2 kHz.
Except for Genelec 6010A studio monitor, and Focus
Scarlett 2i2 audio, all other high quality speakers show
a single sharp peak or small peaks only in their power patterns.
As for Genelec and Focus Scarlett speakers, the power pat-
terns below 2 kHz are similar to those of live-human patterns.
To deal with such studio-level quality speakers, Void employs
other feature sets as explained in Section 5.3.
C Summary of linearity degree features
For the linearity degree of powcdf, we compute the following
two features: Pearson correlation coefﬁcients ρ and quadratic
curve ﬁtting coefﬁcients q of powcdf (see Table 10).
2700    29th USENIX Security Symposium
USENIX Association
020Quadratic coef. (q)4060800.60.70.8Corr. coef. (ρ)0.90102030401µpeaksLive-humanIn-built speakersTable 11: Means and standard deviations of signal power
linearity features for live-human and in-built speakers.
Source
Live-human
In-built speakers
Feature
ρ
q
ρ
q
mean
0.759
47.960
0.854
10.267
stdev
0.059
6.541
0.053
7.006
(FVHPF) to capture the dynamic characteristics of spectral
power in higher frequencies (see Table 12).
Table 12: Summary of the high power frequency features.
Features
#peaks in high-power frequencies
Relative frequencies corresponding to peaks
Standard deviation of high power frequency location
FVHPF = {Npeaks,µpeaks,σpeaks}
Symbol
Npeaks
µpeaks
σpeaks
Table 13 shows the analysis of those three key features for
6,362 voice samples replayed through 13 standalone speak-
ers, and 3,558 live-human voice samples. The mean num-
ber of peaks (Npeaks) for live-human voices is signiﬁcantly
greater than those of standalone speakers. Similarly, live-
human voices showed greater mean of relative frequencies cor-
responding to peaks (µpeaks) and standard deviations. These
difference could be analyzed to detect standalone speakers.
Table 13: Means and standard deviations of the high
power frequency features for live-human and standalone
speakers.
Source
Live-human
Standalone speakers
Features mean
Npeaks
2.580
µpeaks
7.377
Npeaks
1.695
µpeaks
5.531
stdev
3.029
2.693
1.348
2.110
E Finding the optimal feature set
Table 14 shows a separate evaluation result for different fea-
ture sets. We used the ASVspoof 2017 dataset for evaluation.
We used the train and development sets for training, and used
the evaluatoin set for testing. The results show that each of
the selected feature set achieves an F1-score greater than 80%.
These results, together with the declining EERs observed with
addition of features, demonstrate that all individual features
(FVLFP, FVLDF, FVHPF, and FVLPC) are integral in achieving
an EER of 11.60%.
Figure 11: Power patterns of live-human and different
loudspeakers.
Table 10: Summary of the linearity degree features.
Features
Cross-correlation coefﬁcients
Quadratic curve-ﬁtting coefﬁcients
FVLDF= {ρ,q}
Symbol
ρ
q
We use Pearson correlation coefﬁcient ρ to measure of the
linearity in the signal power pattern. The Pearson correlation
coefﬁcients can be calculated as:
ρ(X,Y ) =
cov(X,Y )
σXσY
,
(1)
where cov is the covariance, and σX and σY represent the
standard deviations of X and Y , respectively. In our exper-
iments X = powcdf and Y is an increasing sequence {yn},
where yn+1 − yn = 1.
A polynomial q(x) of degree n = 2 with respective coefﬁ-
cients are given below as:
q(x) = q1x2 + q2x + q3,
(2)
where x = powcdf in the above equation. We use the quadratic
coefﬁcient q1 in our features which is denoted by q for sim-
plicity.
We measure the signal power linearity to show the dif-
ference in power patterns between live-human and in-built
loudspeakers. Table 11 shows mean and standard deviation
of the linearity features of 400 live-human samples and 400
samples replayed through in-built speakers, respectively.
D Summary of high power frequency features
Given the vector  of power density values and the
peak selection threshold ω, we compute the feature vector
USENIX Association
29th USENIX Security Symposium    2701
0246810Normalized power0102030Live-human02468100204060Dynaudio BM5A speaker02468100204060Bose speaker0246810Normalized power0204060Genelec 6010A studio monitor02468100204060Behringer Truth B2030A studio monitor02468100204060Genelec 8020C studio monitorFrequency (kHz)0246810Normalized power0102030Vmoda speakerFrequency (kHz)0246810Normalized power0204060Genelec 8020C studio monitor (2 speakers)Frequency (kHz)0246810Normalized power01020Focusrite Scarlett 2i2 audioTable 14: Accuracy evaluation for each selected feature
set (see Section 5.3)
Table 16: List of playback devices (loudspeakers) used
for replay attack dataset generation.
Feature set
FVLFP
FVLDF
FVHPF
FVLPC
FVLFP + FVLDF + FVHPF
FVLFP + FVLDF + FVHPF + FVLPC (Void)
Acc. (%)
76.61
72.14
73.13
70.19
79.51
84.33
Prec. (%) Rec. (%)
98.04
95.06
98.09
97.64
97.79
98.96
75.59
72.91
71.61
68.62
79.08
83.51
F1 (%) EER (%)
19.37
85.37
30.92
82.52
82.79
21.47
22.99
80.60
18.83
87.44
90.58
11.60
F Feature and model parameters
We describe parameters used for recording voice samples,
performing feature engineering, and training classiﬁers. We
used sampling frequency of 44.1kHz for voice recording. As
for the STFT parameters, we used 1024 as the window length
(recommended to be power of 2), 256 as the hop size, and
4,096 as the number of FFT points. Other parameters needed
to train Void are presented in Table 15.
Table 15: Feature and model parameters.
Class
Voice
Void
Parameter
Sampling frequency
Window length
Hop length
nfft
W
ω
powcd f ’s polynomial order for estimating q
Pest’s estimation using polynomial order
SVM Kernel
Kernel scale
Value
44.1kHz
1024
256
496
10
0.6
2
6
RBF
Auto
G List of playback devices
We used 11 different types of in-built speakers including
smartphones and a smart TV, and four standalone speakers to
replay recorded voice samples (see Table 16).
H List of recording devices
We used 3 different laptops, and 9 different smartphones as
recording devices (see Table 17).
I
Implementation of GD-ResNet
Based on the model described in [17], we implemented GD-
ResNet with two stages: the ﬁrst stage is used to estimate
attention weights from a Global Average Pooling layer, and
the second stage is used to train a ResNet-18 model based
on the GD gram feature with attention weights. Table 18
Name
Galaxy A8
Galaxy A5
Galaxy Note 8
Galaxy S8
Galaxy S8
Galaxy S9
iPhone SE
iPhone 6S Plus
iPhone 5S
LG V20
Samsung Smart TV
Bose
V-MODA
Logitech (2.1 Ch.)
Yamaha (5.1 Ch.)
Model
A810S
SM-A500x
SM-N950x
SM-G950
SM-G955N
SM-G960N
A1662
A1524
A1519
V20 F800
QN49Q7FAM
SoundTouch 10
REMIX-BLACK
Z623
YHT-3920UBL
In-built
Standalone
Table 17: List of recording devices used for human voice
collection, and replay attack dataset generation.
Maker
Samsung Notebook
Samsung Notebook
Macbook Pro
Galaxy A5
Galaxy Note 8
Galaxy S8
Galaxy S8
Galaxy S9
iPhone SE
iPhone 5S
iPhone 6S Plus
LG V20
Model
NT910S3T-K81S
NT200B5C
A1706 (EMC 3163)
SM-A500x
SM-N950x
SM-G950
SM-G955N
SM-G960N
A1662
A1519
A1524
V20 F800
summarizes the performance of our GD-ResNet implemen-
tation: it achieved 0% and 23% EERs on our own dataset
and the ASVspoof 2017 dataset, respectively. As for space
complexity, we counted the number of features extracted from
a single voice sample. Compared to 97 features used by Void,
GD-ResNet uses 786,432 features. As for the average mem-
ory used for classifying a sample, Void uses about 1.99MB,
whereas GD-ResNet uses 1,194.68MB.
Table 18: GD-ResNet space complexity.
Measure
Extraction (sec.)
Training (sec.)
Testing (sec.)
#Features
Memory size (MB)
Performance (EER)
Void GD-ResNet[17]
0.035
0.100
40,560.264
0.283
0.035
0.120
786,432
97
1,194.684
1.988
11.6%
23%
2702    29th USENIX Security Symposium
USENIX Association