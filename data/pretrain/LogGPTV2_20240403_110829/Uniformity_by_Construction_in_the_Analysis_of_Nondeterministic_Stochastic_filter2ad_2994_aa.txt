title:Uniformity by Construction in the Analysis of Nondeterministic Stochastic
Systems
author:Holger Hermanns and
Sven Johr
Uniformity by Construction in the Analysis of Nondeterministic Stochastic Systems
Holger Hermanns
Sven Johr
Universit¨at des Saarlandes, FR 6.2 Informatik
D-66123 Saarbr¨ucken, Germany
Abstract
Continuous-time Markov decision processes (CTMDPs) are
behavioral models with continuous-time, nondeterminism and
memoryless stochastics. Recently, an efﬁcient timed reachabil-
ity algorithm for CTMDPs has been presented [2], allowing
one to quantify, e. g., the worst-case probability to hit an un-
safe system state within a safety critical mission time. This al-
gorithm works only for uniform CTMDPs – CTMDPs in which
the sojourn time distribution is unique across all states.
In
this paper we develop a compositional theory for generating
CTMDPs which are uniform by construction. To analyze the
scalability of the method, this theory is applied to the construc-
tion of a fault-tolerant workstation cluster example, and exper-
imentally evaluated using an innovative implementation of the
timed reachability algorithm. All previous attempts to model-
check this seemingly well-studied example needed to ignore
the presence of nondeterminism, because of lacking support
for modelling and analysis.
1 Motivation
Nondeterministic stochastic systems combine the behavior
of classical labeled transition systems (LTSs), with classical
stochastic processes, Markov chains in particular. The most
widely known model is that of Markov decision processes in
discrete time (DTMDPs) which combines LTS and discrete-
time Markov chains (DTMCs). This model originates from
the operations research (OR) and planning context. In the set-
ting of concurrency theory, that model is often called proba-
bilistic automata [27, 28], and has been the subject of many
studies [24, 22]. Model checking algorithms for DTMDPs are
available [3, 7], have been implemented and applied [18].
Continuous-time Markov chains (CTMCs) extend DTMCs
with memoryless continuous time. These models have a very
rich spectrum of applications, ranging from disk storage di-
mensioning [26] to signalling transduction networks [6].
In
the context of concurrency theory, the extension of CTMCs
with nondeterminism has lead to different models, including
stochastic transition systems [8] and interactive Markov chains
(IMC) [14]. The latter is an orthogonal superposition of LTSs
and CTMCs with convenient compositional properties.
On the other hand, continuous-time Markov decision pro-
cesses (CTMDPs) have been proposed in OR [25], but not
many results are available, and model checking of CTMDP
has not been addressed successfully thus far. A core ingredient
to model check timed safety and liveness properties has been
developed in [2] by an algorithm addressing timed-bounded
reachability probabilities in a CTMDP. The algorithm com-
putes the maximal probability to reach a set of goal states
within a given time bound, among all CTMCs induced by time-
abstract schedulers. Unfortunately, the algorithm is of limited
generality, since it is restricted to so-called uniform CTMDPs
(uCTMDPs), CTMDPs in which the sojourn time distribution
is unique across all states.
Nevertheless we have recently made use of this algorithm in
the veriﬁcation of large STATEMATE [4] models of train con-
trol systems. The algorithm allowed us to verify properties
like: “The probability to hit a safety-critical system conﬁgura-
tion within a mission time of 3 hours is at most 0.01.” for very
large systems with about 106 states and 107 transitions.
The main goal in [4] has been to show how we can gener-
ate models (LTSs) from STATEMATE, incorporate timing be-
havior in a compositional way into them, and ﬁnally obtain a
uCTMDP, amenable to timed reachability analysis. The LTSs
have huge intermediate state spaces which required us to de-
velop special symbolic data structures and minimizers to make
this trajectory possible. Our tool chain therefore consists of a
series of symbolic steps, followed by another series of explicit
steps. The main contribution of the present paper is the theo-
retical foundation for the explicit part of that trajectory, (Sec-
tion IV(b) and IV(c1) of [4]), namely uniformity preservation
along the explicit part of the tool chain. Additionally, we de-
scribe the construction and transformation steps formally, and
prove that uniformity is indeed preserved and that timed reach-
ability properties are preserved as well. A second contribution
which goes beyond the work reported in [4] is that we provide
insight into the construction and timed reachability analysis al-
gorithm, using a more academic but easily scalable case study.
This case study allows us to provide a detailed analysis of the
time and memory behavior of our implementation of the al-
gorithm in [2]. It turns out that the implementation is quite
efﬁcient, albeit being only prototypical.
As a case study, we focus on the fault-tolerant workstation
cluster (FTWC). This modeling problem has earlier been stud-
ied [13, 18] using CTMCs, although the system is nondeter-
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007ministic in nature. As an interesting side result, our analysis
results are compared to the results obtained in the past using
that less faithful modelling style.
The paper is organized as follows. Section 2 introduces the
necessary background. Section 3 discusses our compositional
uIMC construction approach, while Section 4 describes the
transformation to uCTMDPs and the implementation details
for the timed reachability algorithm. In Section 5 we apply the
discussed steps to the FTWC. Finally, Section 6 concludes the
paper.
2 Background
This section introduces the necessary background concern-
ing CTMDPs and IMCs. Disjoint union is denoted A ˙∪ B.
Distr(Ω) denotes the set of all probability distributions over Ω,
and given µ ∈ Distr(Ω) the support of µ is the smallest closed
measurable set A such that µ(Ω\A) = 0.
Continuous-time Markov decision processes. We intro-
duce a mild variation of continuous-time Markov decision pro-
cesses [25] where states may have multiple transitions labeled
with the same action. As will be evident later, the transfor-
mation procedure yields this mild variation of continuous-time
Markov decision processes.
Deﬁnition 1 (CTMDP) A continuous-time Markov decision
process (CTMDP) is a tuple (S, L, R, s0) where S is a ﬁnite
non-empty set of states, L is a ﬁnite non-empty set of transi-
tion labels also called actions, R ⊆ S × L × (S −→ R
+) is
the transition relation, s0 ∈ S is the initial state.
:=
+) | (s, a, R) ∈ R}
Given a transition (s, a, R) ∈ R and set Q ⊆ S
the cumula-
transition
In R(s) :=
we
(cid:1)
s(cid:1)∈Q R(s(cid:2))
we denote by R(Q)
tive rate to reach set Q from state s under
(s, a, R), R is the rate function of (s, a, R).
{(s, a, R) ∈ {s} × L × (S −→ R
collect all transitions emanating from s.
Behavior. The behavior of a CTMDP is as follows. Sup-
pose (s, a, R) ∈ R for some R : S −→ R
+. Then R(s(cid:2)) > 0
indicates the existence of an a-transition emanating from s and
leading to s(cid:2)
. If there are distinct outgoing transitions of s, one
of them is selected nondeterministically. This nondeterminism
is resolved by a scheduler, see below. Given that a transition
labeled a has been selected, the probability of triggering that
a-transition from s to s(cid:2)
within t time units equals the out-
come of a negative exponential distribution with rate R(s(cid:2)),
i. e., 1 − e−R(s(cid:1))·t.
the
transition (s, a, R) can be viewed as a hyperedge [10] labeled
with a, connecting s with target states s(cid:2)
, and possibly different
rates for each of the s(cid:2)
. In this case there is a race between the
relevant transitions. The probability to reach state s(cid:2)
by PrR(s, s(cid:2)) · (cid:2)
from state
(cid:1)
s within t time units given that (s, a, R) ∈ R is taken is given
s∈S R(s) de-
notes the total rate of leaving state s under transition (s, a, R)
If R(s(cid:2)) > 0 for more than one s(cid:2)
where ER =
1 − e−ER·t
(cid:3)
and PrR(s, s(cid:2)) := R(s(cid:1))
ER . The ﬁrst factor of the product de-
notes the discrete branching probability to reach s(cid:2)
from s
under transition (s, a, R), the second factor denotes the so-
journ time in s, i. e., a negative exponential distribution with
rate ER. Note, that the sojourn time only depends on the
rate function R. We extend the notation for discrete branch-
ing probabilities to sets of states by standard summation, i. e.,
PrR(s, Q) =
(cid:1)
q∈Q PrR(s, q).
Whenever all of the ER are equal to each other we call the
CTMDP uniform, for short uCTMDP. Intuitively, this means
that the sojourn time distribution in each state s is the same,
independently of the transition chosen.
Paths. A (timed) path σ in CTMDP C = (S, L, R, s0) is
σ ∈ (cid:2)
a possibly inﬁnite sequence of states, labels and time points,
i. e.,
By ﬁrst(σ) =s 0 we denote the ﬁrst state of path σ. A ﬁnite
path σ(cid:2) = s0a1t1s1 . . . aktksk has length k, denoted as |σ(cid:2)| =
k and its last state equals last(σ(cid:2)) = sk.
+ × S)ω(cid:3)
+ × S)∗(cid:3)
S × (L × R
S × (L × R
˙∪ (cid:2)
.
Scheduler. A scheduler resolves the nondeterminism in-
herent in a CTMDP. Most generally, schedulers can be con-
sidered as functions from ﬁnite paths to probability distribu-
tions over transitions. We can distinguish essentially three di-
mensions: randomization, time dependence and history depen-
dence. Motivated by the availability of the timed reachability
algorithm presented in [2], we restrict ourselves to randomized
time-abstract history-dependent schedulers.
Deﬁnition 2 (CTMDP scheduler) Let C be a CTMDP with
transition set R. A scheduler D over C is a function D :
(S × L)∗ × S → Distr(R) such that each support of D(σ)
is a subset of R(last(σ)).
The support condition ensures that the scheduler distributes
its whole probability mass among the outgoing transitions of
last(σ).
σ-algebra and probability measure for CTMDP. The σ-
algebra over inﬁnite paths and the probability measure PrC
D
over inﬁnite paths induced by some scheduler D is given by
a standard construction [1]. We need to omit it due to space
constraints. We refer to [31] for more details concerning mea-
surability of the induced timed path probabilities.
Interactive Markov chains. We recall the model of inter-
active Markov chains from [14] and give here only the most
relevant deﬁnitions.
Deﬁnition 3 (Interactive Markov Chain) An
interactive
Markov chain, IMC, is a tuple (S, Act,−→, (cid:1)(cid:1)(cid:2), s0) where S
is a nonempty, ﬁnite set of states, Act is a ﬁnite set of actions,
−→ ⊆ S × Act × S is the set of
interactive transitions,
(cid:1)(cid:1)(cid:2) ⊆ S × R
+ × S is the set of Markov transitions and
s0 ∈ S is the initial state.
As usual, we assume a distinguished internal action τ . We
assume τ ∈ Act, and let Act\τ denote Act\{τ}. All
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007other actions are called visible. Rate(s, s(cid:2)) denotes the cu-
(cid:1)
mulative1 transition rate from s to s(cid:2)
. We let r(s, C) :=
s(cid:1)∈C Rate(s, s(cid:2)) and let Es = r(s, S) denote the exit rate
of state s. We call state s stable, written s (cid:5) τ−→, whenever no
s(cid:2) ∈ S exists such that s τ−→ s(cid:2)
. Otherwise it is called unsta-
ble.
Behavior. An IMC can be viewed as a usual labeled transi-
tion system that additionally supports stochastic behavior. The
usual behavior of labeled transition systems is present via in-
teractive transitions leading from state s to s(cid:2)
via some action
a ∈ Act. Stochastic behavior is included via Markov tran-
sitions. A Markov transition leads from state s to s(cid:2)
with a
particular rate λ ∈ R
+. The delay of this transition is gov-
erned by a negative exponential distribution with rate λ, i. e.,
the probability of triggering this transition within t time units
is given by 1 − e−λ·t. When all transitions emanating from
state s are Markov transitions, the next state is selected ac-
is then given as Pr(s, s(cid:2), t) = Ps(s(cid:2)) · (cid:2)
cording to the race condition between these transitions. The
probability to move from state s to state s(cid:2)
within t time units
1 − e−Es·t
, where
Ps(s(cid:2)) := Rate(s,s(cid:1))
is the discrete branching probability to
reach s(cid:2)
from s.
(cid:3)
Es
IMC M =
Deﬁnition 4 (Uniform IMC) We
(S, Act,−→, (cid:1)(cid:1)(cid:2), s0) uniform iff there exists an E ∈ R
+ such
that for all s ∈ S it holds that if s (cid:5) τ−→ then Es = E. The class
of uniform IMCs is denoted by uIMC.
call
Due to the maximal progress assumption, rates of Markov tran-
sitions at unstable states do not play a role in this deﬁnition.
This will become important later.
Special cases. LTSs and CTMCs are special cases of IMCs.
For LTS the set of Markov transitions is empty. By deﬁnition,
LTSs are uniform with rate E = 0. If, on the other hand the
interactive transition relation is empty, the IMC reduces to a
CTMC. The deﬁnition of uniform IMCs is a conservative ex-
tension of uniformity in CTMCs: A CTMC is uniform if there
exists a rate E ∈ R
+ such that Es = E for all states s. In-
tuitively, this means that the probability of taking a transition
within t time units is the same regardless of the state currently
occupied.
Nonuniform CTMCs can be uniformized [19], without af-
fecting the probabilistic behavior (in terms of state probabili-
ties). Uniformization is often considered as a mapping from
a CTMC to a DTMC, but it is better to view it as a twist on
the CTMC level [25]. To uniformize a non-uniform CTMC,
one chooses a rate E at least as large as the largest exit rate
of any of the states. Suppose in state s the cumulative rate Es
in the CTMC is smaller than E, then s is equipped with an
additional self-loop with rate E − Es. Intuitively, this ensures
that all state changes occur with the same (average) frequency
in the resulting uniform CTMC. The probability that n state
changes occur within t time units in a uCTMC is given by a
1Since Markov transitions form a relation they may have multiple outgoing
Markov transitions from s to s(cid:1)
with possibly different rates.
Poisson distribution with parameter E · t, ψ(n, E · t), denoted
by ψ(n) whenever parameter E · t is clear.
State partitioning. It will be instrumental to partition the
states of an IMC according to the outgoing transitions of each
state. We use this partitioning when discussing the transforma-
tion from IMCs to CTMDPs later on. A distinction is made be-
tween Markov states (SM ) with at least one emanating Markov
and no interactive transition; interactive states (SI) with at
least one emanating interactive transition and no Markov tran-
sitions; hybrid states (SH) with both Markov and interactive
outgoing transitions; absorbing states (SA) without outgoing
transitions. S can thus be written as S = SM ˙∪ SI ˙∪ SH ˙∪ SA.
As an abbreviation we use SIH := SI ˙∪ SH.
Open vs. closed system view. We distinguish two differ-
ent views on a given IMC M. Usually, we consider M as
being open which means that M can interact with the environ-
ment. In particular, it can be composed with other IMCs (e. g.
via parallel composition). In this case we impose the maximal
progress assumption [14] which embodies the precedence of
τ -actions (but not of visible actions) over Markov transitions.
The intuition is that visible actions can be subject to compo-
sition, and are hence delayable, while internal actions are not.
The maximal progress condition is central to the compositional
theory of IMC.
In contrast the closed system view only exhibits its actions to
the environment but closes the IMC for interaction. We assume
that models we are going to analyze are closed, and impose an
urgency assumption which means that any action (visible or
not) has precedence over Markov transitions, since it can no
longer be delayed by composition. This urgency assumption,
is not compatible with composition of IMCs [14]. But this
does not inﬂuence our modeling trajectory, since the closed
system view is applied only on complete models, which are no
longer subject to composition.
Paths. A path label is a pair comprising an action a ∈
Act ˙∪ {(cid:6)} and a time point t ∈ R
+. The distinguished action
(cid:6) is used to uniquely identify a path label which belongs to a
Markov transition. For given IMC M the set of all path labels
is given by LPathM = (Act ˙∪ {(cid:6)}) × R
+. Intuitively, when-
ever in a given path σ a state change from s to s(cid:2)
is possible
via (a, t) this means that in s a transition labeled a and leading
to s(cid:2)
was taken at time t. Whenever a state change occurs w.r.t.
a Markov transition at time t this is indicated by ((cid:6), t).
A timed path in IMC M is a possibly inﬁnite alternating