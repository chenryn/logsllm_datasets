(cid:19)
.
k
log∗ k
−1
+ log δ
k
log∗ k
A.2.3 Proof of Part (1). The first step is to observe that by a
straightforward application of the Chernoff bound, since layer 0 is
chosen with probability 1/2, the number of packets that it receives
is with probability 1 − δ/6:
(cid:19)
X0 = τ · X ± O
τ · X · log δ−1
.
Since X = ω(log δ−1), we have that
X0 ≥ X(τ − o(1)) = k log log∗
k · (1 + o(1)).
A.2.4 Proof of Part (2). Applying Lemma 9 for ψ =
1
log∗ k ,
(cid:18)(cid:113)
(cid:113)
2k log∗ k ln log∗ k ln δ−1
= k log log∗
k · (1 + o(1))
packets from layer 0 the number of hops that are not decoded is at
most k1 ≜ k · ψ = k
log∗ k with probability 1 − δ/6. That is, we use
k1 to denote the number of undecoded hops that are left for layers 1
and above.
A.2.5 Proof of Part (3). When at least Ω(k) reach XOR, the
number of digests that the levels get is a balls and bins processes
with the levels being the bins. According to Theorem 5:
L ·(cid:16)S − 1 + ln(6L/δ) +(cid:112)(S − 1 + ln(6L/δ))2 − (S − 1)2/4
After seeing
(cid:17)
(cid:18)
(cid:18)
log∗
k ·
= O
= O (L · (S + log(L/δ)))
k
log∗ k
−1 + log(δ
+ log δ
−1 log∗
k)
= O (k)
(cid:19)(cid:19)
packets, with probability 1−δ/6 our algorithm has at least Q samples
in each layer.
A.2.6 Proof of Part (4). Follows from Lemma 4 for p = c ·e−c ,
A.2.7 Proof of Part (5). Follows from Lemma 9 with K = kℓ
k = Q and δ′ = δ
6L .
and ψ = kℓ+1
kℓ
.
A.2.8 Proof of Part (6). The last layer is samples needs to
decode
kL ≤
k1
e ↑↑ (L − 1) = k1
log d
= O
and samples with probability
pL = e ↑↑ (L − 1)
=
d
log d
d
= Θ
(cid:18) k1
(cid:18) log k1
log k1
(cid:19)
(cid:19)
.
k1
ets needed to decode the remainder of the path is O(cid:0)kL log kL(cid:1) =
Therefore, with a constant probability, a digest would be xor-ed by
exactly one of the kL undecoded hops, and the number of such pack-
O(k1).
A.3 Revised Algorithm to Improve the Lower
Order Term’s Constant
Consider changing the algorithm to sample layer 0 with probability
′ ≜ 1 + log log∗ d
2 + log log∗ d
= 1 −
1
2 + log log∗ d
.
τ
18
Note that Q and S are not known to our algorithm (which is only
aware of d) and they are used strictly for the analysis. Our proof
follows the next roadmap:
(1) When a flow has at least X ≜ k log log∗ k · (1 + o(1)) packets,
Baseline (layer 0) gets at least X · (1 − o(1)) = k log log∗ k ·
(1 + o(1)) digests and XOR (layers 1 and above) gets at least
Ω(X/log log∗ k) = Ω (k) digests with probability 1 − δ/6.
(2) When Baseline (layer 0) gets at least X · (1 − o(1)) digests, it
log∗ k with probability 1 − δ/6.
decodes all hops but k1 ≜ k
(3) When at least Ω (k) packets reach XOR (layers 1 and above),
with probability 1 − δ/6 each layer gets at least S digests.
(4) When a layer ℓ ∈ {1, . . . , L} gets S digests, with probability
1 − δ/6L, at least Q of the digests contain exactly one of the
kℓ undecoded switches.
(5) When a layer ℓ ∈ {1, . . . , L − 1} gets Q of digests that con-
tain exactly one of the kℓ ≜ k1/(e ↑↑ (ℓ − 1)) undecoded
switches, it decodes all hops but at most kℓ+1 with probability
1 − δ/6L.
(6) When the last layer L gets Q of digests that contain exactly
one of the kℓ undecoded switches, it decoded all the remain-
ing hops with probability 1 − δ/6L.
We then use the union bound over all bad events to conclude that
the algorithm succeeds with probability at least 1 − δ.
11The o(1) part hides an additive O(k) term, which we upper bound as
k
c·e−c up to
lower order terms. Specifically, if d = k (thus, c = 1), the required number of packets
is at most k log log∗ k + e · k + o(k).
Then when getting X′ = k ·(cid:16)
ce1−c + o(1)(cid:17)
log log∗ k + 1 + 1
will have
k · (log log∗
k + 1 + o(1))
packets that reach layer 0, which would leave only
′
1 ≜
k
k
e · log∗ k
undecoded hops to layers 1 and above. As above, the number of
packets required for the upper layers to decode the missing hops is
1 log∗ k′
k′
1
ce−c
≤ k
ce1−c .
Since ce−c ≤ 1/e for any c > 0, we get that this is a strict improve-
ment in the number of packets that are required for the path decoding.
For example, if d = k (i.e., c = 1), we reduce the required number
of packets from k(log log∗ k + e + o(1)) to k(log log∗ k + 2 + o(1)).
A.4 An Extension – Detecting Routing Loops
Real-time detection of routing loops is challenging, as switches need
to recognize looping packets without storing them. Interestingly, we
can leverage PINT to detect loops on the fly. To do so, we check
whether the current switch’s hash matches the one on the packet.
Specifically, before choosing whether to sample or not, the switch
checks whether pj .dig = h(s, pj). If there is a loop and s was the last
switch to write the digest, it will be detected. Unfortunately, such an
approach may result in a significant number of false positives. For
example, if we use b = 16-bit hashes, the chance of reporting a false
loop over a path of length 32 would be roughly 0.05%, which means
several false positives per second on a reasonable network.
To mitigate false positives, we propose requiring multiple matches,
corresponding to multiple passes through the loop. We use an addi-
tional counter c to track the number of times a switch hash matched
the digest. When c = 0, the switches follow the same sampling
protocol as before. However, if c > 0 then the digest is no longer
changed, and if c exceeds a value of T then we report a loop. This
changes the loop detection time, but the switch that first incremented
c may report the loop after at most T cycles over the loop. This ap-
proach adds(cid:6)log2 (T + 1)(cid:7) bits of overhead, but drastically reduces
the probability of false positives. For example, if T = 1 and b = 15,
we still have an overhead of sixteen bits per packet, but the chance of
reporting false loops decreases to less than 5 · 10−7. If we use T = 3
and b = 14, the false reporting rate further decreases to 5 · 10−13,
which allows the system to operate without false alarms in practice.
Algorithm 2 PINT Processing at s with Loop Detection
Input: A packet pj with b-bits digest pj .dig and a counter pj .c.
Output: Updated digest pj .dig or LOOP message.
1: if pj .dig = h(s, pj) then
if pj .c = T then return LOOP
pj .c ← pj .c + 1
Let i such that the current switch is the i′th so far
4: if pj .c = 0 and д(pj , i) < 1/i then
pj .dig ← h(s, pj)
2:
3:
5:
▷ Sample with probability 1/i
19
, we
B COMPTING HPCC’S UTILIZATION
We first calculate the logarithm:
U _term = log(T − τ
T
· U) = log(T − τ) − logT + log U
qlen_term = log( qlen · τ
B · T 2 ) = log qlen + log τ − log B − 2 logT
byte_term = log( byte
B · T
) = log byte − log B − logT
Then calculate U using exponentiation:
U = 2U _term + 2qlen_term + 2byte_term
C ARITHMETIC OPERATIONS IN THE DATA
PLANE
Some of our algorithms require operations like multiplication and
division that may not be natively supported on the data plane of
current programmable switches. Nonetheless, we now discuss how
to approximate these operations through fixed-point representations,
logarithms, and exponentiation. We note that similar techniques have
appeared, for example, in [67], [79] and [20].
Fixed-point representation: Modern switches may not directly
support representation of fractional values. Instead, when requiring a
real-valued variable in the range [0, R], we can use m bits to represent
it so that the integer representation r ∈ {0, 1, . . . , 2m − 1} stands for
R · r · 2−m. R is called scaling factor and is often a power of two for
simplicity. For example, if our range is [0, 2], and we use m = 16
bits, then the encoding value 39131 represents 2· 39131· 2−16 ≈ 1.19.
Conveniently, this representation immediately allows using inte-
ger operations (e.g., addition or multiplication) to manipulate the
variables. For example, if x and y are variables with scale factor R
that are represented using r(x), r(y), then their sum is represented
using r(x) +r(y) (assuming no overflow, this keeps the scaling factor
intact) and their product is r(x) · r(y) with a scaling factor of R2. As
a result, we hereafter consider operating on integer values.
Computing logarithms and exponentiating: Consider needing
to approximate log2(x) for some integer x (and storing the result
using a fixed-point representation). If the domain of x is small (e.g.,
it is an 8-bit value), we can immediately get the value using a lookup
table. Conversely, say that x is an m-bit value for a large m (e.g.,
m = 64). In this case, we can use the switch’s TCAM to find the most
significant set bit in x, denoted ℓ. That is, we have that x = 2ℓ · α for
some α ∈ [1, 2). Next, consider the next q bits of x, denoted by xq,
where q is such that it is feasible to store a 2q-sized lookup table on
the switch (e.g., q = 8). 12 Then we have that x = xq · 2ℓ−q(1 + ε)
for a small relative error ε < 2−q. Therefore, we write
log2(x) = log2(xq · 2ℓ−q(1 + ε)) = (ℓ − q) + log2(xq) + log2(1 + ε).
Applying the lookup table to xq, we can compute(cid:101)y ≜ (ℓ − q) +
log2(xq) on the data plane and get that(cid:101)y ∈ [log2 x − log2(1 +
ε), log2 x].13 We can further simplify the error expression as log2(1+
12If q < ℓ we can simply look up the exact value as before.
13In addition to the potential error that arises from the lookup table.
ε) ≤ ε/ln 2 ≈ 1.44 · 2−q. We also note that computing logarithms
with other bases can be done similarly as logy x = log2 x/log2 y.
For exponentiation, we can use a similar trick. Assume that we
wish to compute 2x for some real-valued x that has a fixed-point
representation r. Consider using a lookup table of 2q entries for a
suitable value of q, and using the TCAM to find the most significant
set bit in r. Then we can compute 2x up to a multiplicative factor of
2xε for some ε ≤ 2−q. Assuming that x is bounded by R ≤ 2q, this
further simplifies to 2xε ≤ 2x2−q ≤ 1+R·2−q. For example, if x is in
the range [0, 2] and we are using q = 8 then logarithms are computed
to within a (1 + 2−7)-multiplicative factor (less than 1% error).
Multiplying and dividing: We overcome the lack of support for
arithmetic operations such as multiplication and division using ap-
proximations, via logarithms and exponentiation. Intuitively, we have
that x · y = 2log2 x +log2 y and x/y = 2log2 x−log2 y. We have already
discussed how to approximate logarithms and exponentiation, while
addition and subtraction are currently supported. We note that the er-
rors of the different approximations compound and thus it is crucial
to maintain sufficient accuracy at each step to produce a meaningful
approximation for the multiplication and division operations.
An alternative approach is to directly use a lookup table that takes
the q/2 most significant bits, starting with the first set bit, of x and
y and return their product/quotient (as before, this would require
a 2q-sized table). However, going through logarithms may give a
more accurate result as the same lookup table can be used for both
x and y, and its keys are a single value, which allows considering q
bits for the same memory usage.
20