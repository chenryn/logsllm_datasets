without inter-transaction dependency tracking, as the num-
ber of rows read increases from 100 to 1000 in increments
of 50. As expected, the percentage difference in latency
between these two curves remains the same regardless of
the number of table rows accessed. This conﬁrms that the
MVCC incur a small ﬁxed overhead per table row access.
Phoenix speeds up the post-intrusion database dam-
age process by automating the tasks of identifying inter-
transaction dependencies and undoing the corrupting trans-
actions. To demonstrate this beneﬁt, we run the TPC-C
benchmark against the Phoenix server for a 3-hour period.
We randomly chose 5 transactions in the initial 10-minute
interval and ﬂagged them as intruder transactions. Then
we calculated the set of transactions that are independent
of and thus unaffected by these intruder transactions every
10 minutes, producing a curve that gives a picture of how
the size of uninfected transaction set evolves over time, as
shown in Figure 3. The effectiveness of Phoenix in facili-
tating database damage repair is measured in two metrics:
(1) the percentage of transactions that take place in the pro-
tection window and can be preserved after the recovery, and
(2) the time taken to complete the repair process. As shown
in Figure 3, the percentage of uninfected transactions de-
creases over time, because the contamination spread rate
is higher than the input transaction rate. The W param-
eter represents the number of warehouses in the TPC-C
benchmark. Scaling up W by 1 in TPC-C leads to a 10-
fold increase in the districts and 30000-fold increase in cus-
tomers and orders, and thus has the effect of increasing the
database size, which in turn decreases the degree of shar-
ing and thus the spread from intruder transactions. As a
result, the percentage of uninfected transactions increases
by about 20% with each increment in the W value. When
W is 5, the set of uninfected transactions that Phoenix can
save is 75% at the end of the 3-hour run.
When W is 1, the percentage of uninfected transactions
is very small, under 5%, which means that in this case,
the conventional way of repairing database damage, i.e.,
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
0.4
0.35
0.3
0.25
0.2
0.15
)
s
c
e
s
(
y
c
n
e
t
a
L
0.1
0
200
400
600
Number of table rows accessed
d
e
v
a
s
k
r
o
w
f
o
e
g
a
t
n
e
c
r
e
P
100
90
80
70
60
50
40
30
20
10
0
0
With Deptrack
Without Deptrack
800
1000
W=5
W=4
W=3
W=2
W=1
15
5
10
Time in 10 minute intervals
Figure 2: Latency difference between the MVCC version of
Phoenix and generic PostgreSQL under a SQL query that reads
a different number of rows.
d
e
v
a
s
k
r
o
w
f
o
e
g
a
t
n
e
c
r
e
P
100
95
90
85
80
75
70
65
60
55
50
45
40
35
30
25
20
15
10
5
0
W=1 with irrelevant dependencies removed
W=1 with irrelevant dependencies
0
5
10
Time in 10 minute intervals
15
20
Figure 4: Effect of exploiting domain knowledge to ignore cer-
tain unimportant inter-tranaction dependencies.
full rollback, performs almost as good. However, a more
careful analysis of the nature of the inter-transaction de-
pendencies that Phoenix detects reveals that some of these
dependencies are actually irrelevant from the standpoint of
database semantics. For example, a Payment transaction
changes the value of the Count column for a Warehouse
table row, say R. Although other transactions read R, they
do not access the Count column, and therefore should not
classiﬁed as dependent on the last transaction that updates
R. This is the classical “false sharing” problem, where the
granularity used in dependency tracking, a table row in the
case of Phoenix, is too coarse to capture true dependencies.
To evaluate the impact of false sharing, we recomputed
the percentages of uninfected transactions by excluding the
inter-transaction dependencies that do not affect the cor-
rectness of the database operation, and compared the result
with the case when all inter-transaction dependencies that
Phoenix identiﬁes are considered. Figure 4 shows the re-
Figure 3: Work saved by Phoenix after an intrusion occurs
during a TPC-C benchmark run. The Y-axis represents the
percentages of transactions saved and the X-axis is time in
units of 10 minutes. We assume that intrusion is detected 3
hours after it occurs. The W parameters represent number of
warehouses.
sult of this comparison. Not only the absolute percentage
of uninfected transactions increases substantially at the end
of the 3-hour run (20% versus 2%), the percentage curve
shows an increasing trend over time, suggesting that the
contamination spread rate is actually slower than the input
transaction rate. This result is directly attributed to reduc-
tion in inter-transaction dependencies when semantically
irrelevant dependencies are eliminated from consideration.
The result in Figure 4 also highlights the importance to
include the inputs of database administrators in the post-
intrusion database damage repair process, and validates the
design decision in Phoenix of providing an interactive edit-
ing facility to allow DBAs to manually reﬁne the inter-
transaction dependencies that Phoenix produces into the
ﬁnal undo set. While such editing takes extra work and
could potentially slows down the repair process, the payoff
clearly justiﬁes these efforts.
We calculate the end-to-end repair time as the sum of
time taken to analyze a dependency graph and for actual
undo of transactions. For W=3, we found that we could
create the undo-set by traversing the graph obtained after a
3-hour run in 6.44 minutes. The actual undo of the trans-
actions for this undo set (size = 7000) took 33s. The major
component in our end-to-end repair time is thus the time
taken for traversing the dependency graph and creating the
undo set. This is also where the most savings lie when we
compare Phoenix with manual analysis of transactions for
determining which ones to undo.
Finally, in terms of space overhead, Phoenix requires
4 bytes for each table row to record the ID of the last
update transaction. Assuming at the average table row
size is 50 bytes, this represents an 8% overhead.
In the
MVCC approach, this space overhead is 0% with respect
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
to PostgreSQL because PostgreSQL maintains the Xmin
ﬁeld for free. The space overhead of the Trigger ap-
proach, on the other hand, is 16% because it uses a sep-
arate system table that requires an additional 4-byte row
OID ﬁeld. As for the inter-transaction dependency graph,
assume that each transaction on the average adds 10 edges
to the graph, and the DBMS executes 200 transactions per
second, then the size of the inter-transaction dependency
graph is 10 ∗ 100 ∗ 3600 ∗ 24 ∗ 4 = 3.456 Gbytes per day,
assuming each dependency graph edge costs 4 bytes. For
a protection window of two weeks, this translates to less
than 49 Gbytes, which can be easily accommodated by a
60-Gbyte IDE drive that costs under $150 as of July 2002.
6 Conclusion
As DBMSs become an integral component of Internet ser-
vices, they are subject to the same type of attacks that have
plagued their front-end servers. Although access control
and authentication can a large extent fend off direct attacks,
they can do very little with respect to indirect attacks that
go through compromised Web server or application servers,
which DBMSs typically trust. Even for databases that are
not visible in the Internet, they are still vulnerable to insider
threats and other attacks that are based on non-technical
means such as social engineering. While most database se-
curity research focuses on attack prevention, we take a dif-
ferent approach by borrowing ideas from the fault-tolerant
computing community: To maximize the overall system
availability, one can either minimize MTTR or maximize
MTTF. As research on MTTF maximization starts to reach
a point of diminishing return, it is essential to investigate
the MTTR minimization approach for additional perfor-
mance gain.
This paper describes the design, implementation, and
evaluation of an intrusion-resilient database system called
Phoenix, which both facilitates the post-intrusion system
restoration process and improves the accuracy of database
damage repair. It features a novel run-time inter-transaction
dependency tracking mechanism that generates a data
structure that allows Phoenix to quickly and automatically
identify the corrupted transaction set whose effects on the
database should be undone. To incorporate database ad-
ministrators’ inputs, Phoenix also provides an interactive
exploration interface for them to further reﬁne this undo
set. One the ﬁnal undo set is determined, Phoenix repairs
the database by selectively rollback the transactions in the
undo set. The current Phoenix prototype is built on Post-
greSQL and intelligently exploits its no-overwrite storage
structure and multi-version concurrency control to mini-
mize the performance cost associated with inter-transaction
dependency tracking and selective transaction undo. Per-
formance measurements of the TPC-C benchmark on the
fully operational Phoenix prototype show that the run-time
overhead for inter-transaction dependency tracking is less
than 5%, and the selective transaction undo is almost in-
stantaneous. With this small run-time overhead, Phoenix
can speed up the repair process by at least an order of mag-
nitude compared to manual repair. Moreover, this fast re-
pair advantage also carries over to database damages due to
incorrect input entries or operational errors.
References
[1] Oracle 9i Flash Back query. Oracle Technet. Available at,
http://technet.oracle.com/products/oracle9i/daily/ Aug13.html
[2] Caribou Lake, Journal Based recovery tool for Ingres. Available at,
http://www.cariboulake.com/techinfo/irep white paper.html
[3] C. Mohan, Donald J. Haderle, Bruce G. Lindsay, Hamid Pirahesh,
Peter Schwarz. “ARIES: A Transaction Recovery Method Support-
ing Fine-Granularity Locking and Partial Rollbacks Using Write-
Ahead Logging”. TODS. 17(1): 94-162(1992)
[4] Peng Liu, Paul Ammann, Sushil Jajodia. “Rewriting histories: Re-
covering from malicious transactions”. The International Journal of
Distributed and Parallel Databases. 8(1):7-40, January 2000
[5] P. Luenam, P. Liu. “ODAM: An On-the-ﬂy Damage Assessment
and Repair System for Commercial Database Applications”. Proc.
15th IFIP WG 11.3 Working Conference on Database and Appli-
cation Security.
[6] P. Ammann, S. Jajodia, P. Liu. “Recovering from Malicious Trans-
actions”, IEEE Trans. on Knowledge and Data Engineering. To ap-
pear.
[7] Tripathy, Sani and Panda, Brajendra. “Post-Intrusion Recovery Us-
ing Data Dependency Approach”. In Proceedings of the 2nd An-
nual IEEE Systems, Man, and Cybernetics Information Assurance
Workshop, West Point, NY.
[8] Dean Povey. “Enforcing well-formed and partially-formed trans-
actions for UNIX”. In Proceedings of the 8th USENIX Security
Symposium. USENIX Association, August 1999.
[9] D. Santry, et al. “Deciding when to forget in the Elephant File Sys-
tem” In Proceedings of the Seventeenth ACM Symposium on Op-
erating System Principles, pages 110-123, 2002.
[10] John Strunk, et al. “Self Securing storage: Protecting data in com-
promised systems”. In Proceedings of the 2000 OSDI Conference,
October 2000.
[11] Ningning Zhu, Tzi-cker Chiueh, “Design, Implementation, and
Evaluation of Repairable File Service,” in Proceedings of Interna-
tional Conference on Dependable Systems and Networks,San Fran-
cisco, CA, June 22nd - 25th, 2003.
[12] Jay Wylie, et al. “Survivable Information storage systems”. IEEE
Computer, 2(1):61-68, August 2000.
[13] S. Quinlan and S. Dorward. “Venti: a new approach to archival
storage”. In USENIX conference on File and Storage Technologies,
January 2002
of
[14] Home
“tripwire
the
open
source
project”.
http://www.tripwire.org/.
[15] David Patterson et al. “Recovery Oriented Computing ROC: Moti-
vation, Deﬁnition, Techniques, and Case Studies”, UC Berkeley
Computer Science technical report, UCB//CSD-02-1175, March
2002.
[16] A. Brown and D. A. Patterson. “Embracing Failiure: A case for
recovery-oriented computing (roc)”. In 2001 High Performance
Transaction Processing Symposium, October 2001.
[17] Kifer, Lewis, Bernstein. Databases and Transaction Processing :
An application oriented approach. Addison-Wesley. 2002
[18] Philip Bohannon, Rajeev Rastogi, S. Seshadri, Avi Silberschatz,
S. Sudarshan, “Using Codewords to Protect Database Data from a
Class of Software Errors,” ICDE, p 276-285, 1999.
[19] CERT Coordination Center, “CERT Advisory CA-2003-04 MS-
SQL Server Worm,” http://www.cert.org/advisories/CA-2003-
04.html
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE