As  an  optimization,  we  visited  only  states  reachable  from  the 
starting empty state, and avoided repeated state-space and sub-tree 
branches.  As a result, we explored a tree depth of 16 and located 
~10K unique reachable states, or ~2.7M state transitions.   
Two-version-programming verification:  Given that the state-
transition table is filled with mostly illegal transitions, we applied 
n-version programming to verify the table, where the probability 
of hitting the same bug with the same handling can be reduced as 
we add more versions.  In this work, n=2.  We wrote a user-level 
state-transition  program  based  on  hundreds  of  conceptual  rules 
(e.g.,  marking  a  write  entry  of  any  type  as  sensitive  will  set  the 
sensitive  bit  to  1).    The  enumerated  state-transition  table  was 
reconciled with the one generated by the TAP kernel module.   
7.  PERFORMANCE EVALUATION  
We  compared  TrueErase  to  an  unmodified  Linux  2.6.25.6 
running ext3.  We ran PostMark [16] to measure the overhead for 
metadata-intensive  small-file  I/Os.    We  also  compiled  OpenSSH 
version 5.1p1 [23] to measure the overhead for larger  files.  We 
ran  our  experiments  on  an  Intel®  Pentium®  D  CPU  2.80GHz 
dual-core  Dell  OptiPlex  GX520  with  4-GB  DDR533  and  1-GB 
DoC  MD2203-D1024-V3-X  32-pin  DIP  mounted  on  a  PCI-G 
DoC  evaluation  board.    Each  experiment  was  repeated  5  times.  
The 90% confidence intervals are within 22%.   
PostMark:    We  used  the  default  configuration  with  the 
following changes:  10K files, 10K transactions, 1-KB block size 
for reads and writes, and a read bias of 80%.  We also modified 
PostMark  to  create  and  mark  different  percentages  of  files  as 
sensitive.    These  files  can  be  chosen  randomly  or  with  spatial 
locality,  which  is  approximated  by  choosing  the  first  x%  of  file 
numbers.  Before running tests for each experimental setting, we 
dirtied  our  flash  by  running  PostMark  with  0%  sensitive  files 
enough  times  to  trigger  wear  leveling.    Thus,  our  experiments 
reflect a flash device operating at steady state.  A (cid:86)(cid:92)(cid:81)(cid:70) command 
was issued after each run and is reflected in the elapsed time.   
Table 3 shows that when TrueErase operates with no sensitive 
files,  metadata  tracking  and  queries  account  for  3%  overhead 
compared to the base case.  With 10% of files marked sensitive, 
the  slowdown  factor  can  be  as  high  as  11,  which  confirmed  the 
numbers in a prior study [41].  However, with 5% of files marked 
sensitive and with locality and delayed secure deletion of file data 
446
Table  3:  Postmark  flash  operations,  times,  and  overhead
percentage compared to base. 
page 
reads 
control-
area 
reads 
page 
writes 
control-
area 
writes 
erases  time 
(secs) 
300K  1.97M  218K  237K  4.28K  671 
Base 
0.99x  1.08x  1.01x  1.01x  1.00x  1.03x 
0% 
3.69x  2.09x  2.82x  2.79x  2.58x  2.93x 
1% random 
1% locality 
2.95x  1.89x  2.33x  2.31x  2.16x  2.44x 
1% random, delayed deletion   3.41x  2.00x  2.61x  2.59x  2.47x  2.73x 
1% locality, delayed deletion 
2.77x  1.77x  2.20x  2.19x  2.08x  2.29x 
10.3x  4.22x  6.91x  6.83x  6.67x  7.39x 
5% random 
6.69x  3.19x  4.86x  4.81x  4.32x  5.05x 
5% locality 
7.56x  3.48x  4.99x  4.99x  5.18x  5.54x 
5% random, delayed deletion 
4.40x  2.33x  3.29x  3.29x  3.02x  3.42x 
5% locality, delayed deletion 
15.3x  5.82x  9.96x  9.84x  9.75x  10.7x 
10% random 
10% locality 
9.96x  4.24x  7.00x  6.92x  6.23x  7.27x 
10% random, delayed deletion  9.44x  4.23x  5.91x  5.96x  6.54x  6.80x 
10% locality, delayed deletion  5.82x  2.96x  4.19x  4.22x  3.90x  4.45x 
Table  4:  Compilation  flash  operations,  times,  and  overhead
percentage compared to base. 
control-area 
page 
reads 
reads 
page 
writes 
control-
area 
writes 
erases time 
(secs) 
25.3K 
Base 
Random 
4.79x 
Random, delayed deletion  1.70x 
make + sync 
108K 
3.10x 
1.37x 
22.5K  23.9K  352 
3.15x  3.15x  3.13x 2.51x 
1.41x  1.43x  1.40x 1.41x 
89 
make clean + sync 
Base 
1.60K 
Random 
10.0x 
Random, delayed deletion  8.47x 
3.73K 
10.1x 
8.36x 
Total 
22 
514 
445 
13.6x  15.0x  7.14x 8.13x 
11.0x  12.6x  6.22x 6.87x 
3 
26.9K 
Base 
Random 
5.10x 
Random, delayed deletion  2.10x 
112K 
3.33x 
1.60x 
23.0K  24.4K  374 
3.35x  3.40x  3.37x 2.70x 
1.59x  1.66x  1.69x 1.59x 
92 
OpenSSH compilations:  We issued (cid:80)(cid:68)(cid:78)(cid:72)+(cid:86)(cid:92)(cid:81)(cid:70) and (cid:80)(cid:68)(cid:78)(cid:72)(cid:3)
(cid:70)(cid:79)(cid:72)(cid:68)(cid:81)(cid:14)(cid:86)(cid:92)(cid:81)(cid:70)  to  measure  the  elapsed  times  for  compiling  and 
cleaning OpenSSH [23].  For the TrueErase case, we marked the 
(cid:82)(cid:83)(cid:72)(cid:81)(cid:69)(cid:86)(cid:71)(cid:16)(cid:70)(cid:82)(cid:80)(cid:83)(cid:68)(cid:87)  directory  sensitive  before  issuing  (cid:80)(cid:68)(cid:78)(cid:72), 
which  would  cause  all  newly  created  files  (e.g.,  .o  files)  in  that 
directory to be treated sensitively.  These files account for roughly 
27%  of  the  newly  generated  files  (8.2%  of  the  total  number  of 
files  and  4.1%  of  the  total  number  of  bytes  after  compilation).  
Before running each set of tests, we dirtied the flash in the same 
manner as with PostMark and discarded the first run that warms 
up the page cache.  Table 4 shows that a user would experience a 
compilation slowdown within 1.4x under the delay-deletion mode.  
A  user  would  experience  a  slowdown  within  6.9x  under  the 
delayed-deletion  mode  with  a  deletion-intensive  workload.      For 
the  entire  compilation  cycle  of  (cid:80)(cid:68)(cid:78)(cid:72)(cid:14)(cid:86)(cid:92)(cid:81)(cid:70)  with  (cid:80)(cid:68)(cid:78)(cid:72)(cid:3)
(cid:70)(cid:79)(cid:72)(cid:68)(cid:81)(cid:14)(cid:86)(cid:92)(cid:81)(cid:70),  a  user  would  experience  an  overall  slowdown 
within 60% under the delayed-deletion mode.   
Overall, we find that the overhead is within our expectations. 
Further improvements in performance are future work. 
8.  RELATED WORK 
This  section  discusses  existing  cross-layer  secure-deletion 
solutions. 
A  semantically-smart-disk  system  (SDS)  [36]  observes  disk 
requests and deduces common file-system-level information such 
as  block  types.    The  File-Aware  Data-Erasing  Disk  is  an  ext2-
based SDS that overwrites deleted files at the file-system layer.   
A  type-safe  disk  [33]  directly  expands  the  block-layer 
interface and the storage-management layer to perform free-space 
management.  Using a type-safe disk, a modified file system can 
specify the allocation of blocks and their pointer relationships.  As 
an  example,  this  work  implements  secure  deletion  on  ext2.  
Basically, when the last pointer to a block is removed, the block 
can be securely deleted before it is reused.   
Lee  et  al.  [18]  have  modified  YAFFS,  a  log-structured  file 
system  for  NAND  flash,  to  handle  secure  file  deletion.    The 
modified  YAFFS  encrypts  files  and  stores  each  file’s  key  along 
with  its  metadata.    Whenever  a  file  is  deleted,  its  key  is  erased, 
and  the  encrypted  data  blocks  remain.    Sun  et  al.  [38]  modified 
YAFFS  and  exploited  certain  types  of  NAND  flash  that  allow 
overwriting  of  pages  to  achieve  secure  deletion.    Raerdon  et  al. 
[27]  also  modified  YAFFS  to  use  a  flash-chip-specific  zero-
overwriting technique.  In addition, Raerdon et al. [26] developed 
the Data Node Encrypted File System (DNEFS), which modifies 
the flash file system UBIFS to perform secure deletion at the data 
node level, which is the smallest unit of reading/writing.  DNEFS 
performs encryption and decryption of individual data nodes and 
relies on a key generation and deletion scheme to prevent access 
to overwritten or deleted data.  Since UBIFS is designed for flash 
with  scaling  constraints,  this  approach  is  not  as  applicable  for 
disks and larger-scale storage settings. 
9.  FUTURE WORK 
Many opportunities exist to increase TrueErase’s performance 
on  NAND  flash.    We  can  implement  flash-chip-specific  zero-
overwriting  or  scrubbing  routines  [27,  38,  41].    However,  this 
optimization  may  make  our  solution  less  portable.    We  can  add 
encryption  to  our  system  and  use  TrueErase  to  ensure  secure 
deletion of the encryption key.  We could also batch flash erasures 
for better flash performance. 
Other future work will include tracking sensitive data between 
files and applications via tainting mechanisms, expanded handling 
of  other  threat  models,  and  generalizations  to  handle  swapping, 
hibernation, RAID, and volume managers. 
10.  LESSONS LEARNED/CONCLUSION 
This paper presents our third version of TrueErase.   Overall, we 
found that retrofitting security features to the legacy storage data 
path is more complex than we first expected.   
Initially, we wanted to create a solution that would work with 
all  popular  file  systems.    However,  we  found  the  verification 
problem  became  much  more  tractable  when  working  with  file 
systems with proven consistency properties, as described in § 4.4. 
Our  earlier  designs  experimented  with  different  methods  to 
propagate  information  across  storage  layers,  such  as  adding new 
special  synchronous  I/O  requests  and  sending  direct  flash 
commands from the file system.  After struggling to work against 