bugs that resulted from unbalanced map/unmap calls,
off-by-one-page unmap, and unmapping global memory.
The bugs did not manifest when the API was backed by
the direct map, as unmap calls on the direct map were
compiled into no-ops.
3) For Xen HVM guests with EPT 1, we further identi-
ﬁed EPT pages as performance-critical non-secrets that
should be globally mapped for efﬁcient 2-stage page
table walks.
In 3), walking 2 stages of page tables in the hypervisor
requires up to 25 ephemeral mappings, shown in Fig. 6. This
creates high and unpredictable latencies depending on the map
cache hit rate. The performance of 2-stage walks is critical
for Xen because memory arguments in Xen hypercall ABI are
passed as Guest Virtual Addresses (GVA), which need to be
manually walked by the hypervisor before copying memory.
This is a legacy of Xen’s original fully paravirtualized mode,
where pseudo-physical addresses (guest physical addresses)
1We use EPT (Extended Page Table) as a generic term for the second stage
page table. On AMD systems such pages are named NPT (Nested Page Table).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
377
GuestCR3EPTL4EPTL3EPTL2EPTL1Guest L4…………Guest L3…………Guest L2…………Guest L1…………Guest PageNon-secretsSecretsFig. 7. SPEC-CPU 2017. Normalized execution time
HVM (ns)
Baseline
Default
SF
SF+EPT
PV (ns)
Baseline
Default
XPTI
SF
hypercall reg
303.97
348.2
309.86
312.19
overhead
14.55%
1.94%
2.70%
hypercall mem overhead
776.15
1066.60
780.26
995.50
37.42%
0.53%
28.26%
39.60
83.49
306.40
39.87
110.83%
673.74%
0.68%
83.81
131.77
366.12
84.35
context switch
1832.95
2362.04
2392.78
2420.05
overhead
28.87%
30.54%
32.03%
1214.49
1660.24
1861.93
1740.84
36.70%
53.31%
43.34%
IPI
2555.25
2577.50
2602.62
2696.26
2584.74
2686.06
2703.18
2592.96
overhead
0.87%
1.85%
5.52%
3.92%
4.58%
0.32%
57.22%
336.85%
0.64%
TABLE II
MICRO-BENCHMARKS
Frequency (MHz) Baseline Default
HVM HPET
4.70
Slowdown
SF
6.04
24.68% 3.26%
6.24
-
SF+EPT
5.40
13.50%
whereas all other conﬁgurations are below 0.5%, showing little
performance impact.
HPET BENCHMARK. MAX FREQUENCY FROM GETTIMEOFDAY().
TABLE III
C. Micro-benchmarks
Secret-Free: Xen with the secret-free implementation. Ad-
dress space isolation and branch hardening options are dis-
abled. However, mitigations for µarch sharing (core scheduling
and µarch ﬂushing on full domain switch) are kept.
SF+EPT: Secret-free with EPT as guest secrets. Although
EPT can be treated as non-secrets by our deﬁnition, this setup
stresses 2-stage page table walks and allows us to test the
ephemeral map cache.
Note that our setup (Ryzen 5900X) is not susceptible to
Meltdown, but we choose to add this data point to study the
impact of XPTI on Xen PV guests. Xen PV is still supported
by multiple cloud vendors on older generation hardware which
is vulnerable to Meltdown, thus XPTI must still be enabled
for isolation on these platforms.
B. SPEC-CPU 2017
Fig. 7 shows the performance impact of different con-
ﬁgurations of mitigations when running SPEC-CPU 2017.
Compared with the unmitigated baseline, the slowdown is
under 3% for all benchmarks.
Such results are within our expectation as SPEC-CPU
benchmarks are CPU- and memory-intensive. These workloads
remain largely unaffected by hypervisor mitigations because
they require few transitions into the hypervisor. We observe
that the geomeans of different setups deviate from the baseline
by less than 1.01%. XPTI and SF+EPT have the largest drop
at 1.01% and 0.86% due to the increased latency in hypercalls
We show four micro-benchmarks: hypercall register, hyper-
call memory, context switch and IPI performance. Register
hypercalls contain input and output arguments purely in regis-
ters, whereas memory hypercalls require mapping and copying
guest memory. For context switch, we launch two guest vCPUs
from two domains and schedule both on the same host core.
The IPI benchmark measures the synchronous operation of a
global IPI sequence, with its arguments placed on the caller’s
stack in the baseline conﬁguration. These micro-benchmarks
are selected speciﬁcally to stress the paths hardened by the SF
implementation.
TABLE II shows the overhead of the secret-free design
relative to existing Xen mitigations. We see that the secret-
free version has shown near-baseline performance in several
categories. The overhead of both register and memory hyper-
calls is negligible. If EPT is treated as guest memory secrets,
HVM memory hypercalls see a degradation of 28.26% due to
25 total ephemeral maps from 2-stage page table walks. This
explains our desire to promote EPT to global non-secrets. For
default Xen mitigations, the overhead comes from restricted
speculations and explicit branch predictor barriers which more
than double the latency of a PV register hypercall, while the
percentage is not as signiﬁcant in HVM since the majority
of the cost is VMENTER and VMEXIT. On the other hand,
XPTI shows detrimental penalties for PV guests with more
than 6× hypercall latencies due to two cr3 swaps, bringing
its hypervisor round trip cost at a comparable level with a full
VMENTER and VMEXIT.
The secret-free implementation shows noticeable degrada-
tion in full context switches. The overhead comes primarily
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
378
96.00%98.00%100.00%102.00%104.00%106.00%perlbenchgccmcfomnetppxalancbmkx264deepsjengleelaexchange2xzgeomeandefault (HVM)SF (HVM)SF+EPT (HVM)default (PV)XPTI (PV)SF (PV)Fig. 8. Disk and network I/O benchmarks. Sequential and random disk accesses are tested by FIO. Netperf measures TCP performance and latency. The
numbers on the X axis indicate baseline performance. Apart from TCP latency, the Y axis represents normalized execution time of ﬁxed-size workloads.
from three sources. First, as secret-freedom alone does not
address the problem of guests sharing micro-architectural state,
we still ﬂush µarch buffers during a full context switch to 1)
prevent the next guest from snifﬁng secrets and to 2) isolate
the next guest against malicious states like mistrained branch
predictors. This cost only occurs when switching to a vCPU
of a different domain. Second, moving to per-vCPU stacks
means a context switch also switches to a new hypervisor
stack and mapping, resulting in increased cache and TLB
misses. Last, using global buffers to grant visibility to the
next stack reduces data locality. Existing Xen mitigations show
slightly lower overhead due to the cache and TLB locality of
the per-pCPU stack. XPTI, however, exhibits more than 50%
additional context switch latency relative to baseline.
The IPI performance remains largely unchanged and the
cost is dominated by the IPI itself rather than speculative
mitigations. Sending the IPI synchronously shows high latency
and variance. In comparison, the overhead of the secret-free
hypervisor using global bounce buffers for IPI arguments is
insigniﬁcant. The latency suggests why maintaining globally
mapped memory (like direct map maintenance in XPFO) is
costly unless hardware support is present: an IPI alone is
more expensive than a full context switch for the caller and
interrupts all callees.
HVM MMIO: We further micro-benchmark a more
realistic setup by measuring the maximum number of
gettimeofday() calls that can be issued per second from
guest user space after switching the guest Linux kernel clock
source to the High Precision Event Timer (HPET, mapped as
an MMIO device). Emulated MMIO, like memory hypercalls,
requires 2-stage walks to fetch the Guest Physical Address
(GPA) from Guest Virtual Address (GVA), although Xen walks
to Host Physical Address (HPA) to take a page reference.
We choose HPET as it represents the performance of other
MMIO emulations, tests the map cache with a mix of page
table walking and guest kernel activity, and is used in guest
OSes as a timer source.
The maximal achieved number of gettimeofday() calls
per second (denoted as Frequency and measured in MHz) is
shown in TABLE III. With 2MB superpages used by default
for HPET MMIO region, SF and SF+EPT map 4 and 19 pages
respectively. Compared with baseline, SF only reduces the
gettimeofday() frequency by 3.26% whereas default Xen
mitigations degrade performance by 24.68%, which means the
secret-free design offers signiﬁcantly improved performance in
device emulation.
D. I/O performance
Fig. 8 shows the overhead of disk and network I/O through-
put. Sequential read and write saturate the backing storage,
but random synchronous 4K I/O is bottlenecked by commu-
nication between the front- and back-end drivers. In these
benchmarks, we see all secret-free conﬁgurations showing
competitive performance close to baseline with the biggest
drop being SF+EPT at 7.67%. In contrast, Xen mitigations
degrade random synchronous I/O by more than 15%. XPTI
again shows severe performance impact with its latency in-
creased by 50%, resulting in 36% less IOPS.
We see an impact in TCP bandwidth by up to 2.26% in
SF which increases to 6.8% when EPT is treated as domain
secrets. XPTI is expectedly the worst performer, showing up
to 41% overhead in bandwith and 15% increase in latency.
Overall, SF demonstrates a combined geomean of all disk
and network I/O overhead at 2.63%, almost one quarter of
the 10.2% from enabling Xen default mitigations (excluding
XPTI).
E. Application benchmarks
We further collect
the results from a variety of real-
world benchmarks,
including Apache httpd, Linux kernel
compilation, Numpy, LLVM compilation, LevelDB, Nginx,
PostgreSQL (pgbench), SQLite and code repository decom-
pression. The normalized execution time is shown in Fig. 9.
Except for XPTI, the geomean of the overhead in each setup
falls below 2.5%, which is insigniﬁcant compared with the
stress tests in previous sections. This is expected as real-world
workloads are a mix of CPU, memory and I/O activity.
For HVM, our LevelDB benchmark heavily tests random
database mutation. The overhead tracks our I/O benchmarks
well, with Xen default showing up to 11% increase in
random write transaction latency. Averaging all read and
write sub-benchmarks, default shows a LevelDB overhead
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
379
125.26152.14153.96140.92126.9195.00%100.00%105.00%110.00%115.00%120.00%125.00%sec readHVM: 3911MB/sPV: 3967MB/ssec write2445MB/s2410MB/srand read36.6KIOPS42.8KIOPSrand write34.2KIOPS40.1KIOPSTCP send ﬁle10.5Gb/s16.4Gb/sreq response20.1MT/s26.3MT/sclient to server10.6Gb/s16.3Gb/slatency0.44ms0.41msgeomeandefault (HVM)SF (HVM)SF+EPT (HVM)default (PV)XPTI (PV)SF (PV)Fig. 9. Real-world application benchmarks. Normalized execution time.
a balance between high hit rate and best overall application
performance.
We further attempt to improve on the 32-entry cache by
implementing set associativity (denoted by random and LRU
in Fig. 10), which has a consistent but minor impact on hit
rate. A randomly evicted 2-way set-associative cache shows
negligible advantage over the direct-map one, whereas the
Least-Recently-Used eviction policy sees a more noticeable
improvement by 1.43%. Nevertheless, software implementa-
tion of set associativity requires branch instructions and is
unable to exploit parallel lookups like hardware CPU caches,
thus the minor increase in hit rate and negligible increase in
application performance do not justify the complexity.
The block size can be grown by adding superpage (2MB)
cache entries for large ephemeral mappings. However, we
fail to observe any performance improvement as almost no
ephemeral mapping request ever exceeds the 4K page size.
Eventually, we choose a direct-mapped 32-entry map cache
with only 4K mappings for the best overall performance and
a simple but fast critical path, which is the conﬁguration used
throughout the evaluation section.
G. Security
We evaluate the security of our secret-free Xen hypervisor
using proof-of-concept speculative execution exploits from the
three categories (permission, coercion and µarch). The attacks
are launched from an unprivileged domU. The victim contains
a secret message buffer in a separate VM. We also evaluate
against ret2dir by injecting a stack overﬂow vulnerability to
Xen.
For Meltdown, we launch a malicious PV domU 2 to dump
the victim buffer via the Xen direct map. For Spectre, the
attacker attempts to mistrain the branch predictor, redirecting
hypervisor speculative execution to access the secret message.
For L1TF, the malicious domain sniffs for any secrets left
in the L1 cache from the sibling thread. Redirecting code
execution to the direct map is not effective as Xen marks
it as non-executable. To exploit the ret2dir vulnerability, we
replicate the attack for non-executable direct map in [25] using
a stack-pivoting gadget, so that the hypervisor stack pointer
can be pivoted to malicious frames on the direct map. Each
2Xen HVM guests with hardware assist run on EPT and cannot exploit