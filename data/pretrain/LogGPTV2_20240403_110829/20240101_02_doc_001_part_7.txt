(VALUES (1, 'b1'),
(2, 'b2'),
(3, 'b3'),
(3, 'b3')) r(id, nameB)
USING (id)
ORDER BY id;
The result of this statement won’t be 4 rows as before, but 6 rows:
┌───────┬─────────┬─────────┐
│ id │ nameA │ nameB │
│ int32 │ varchar │ varchar │
├───────┼─────────┼─────────┤
│ 1 │ a1 │ b1 │
│ 2 │ a2 │ b2 │
│ 2 │ a2 │ b2 │
│ 3 │ a3 │ b3 │
│ 3 │ a3 │ b3 │
└───────┴─────────┴─────────┘
This is something that you can prepare for when defining your schema. Usually joins will
happen on columns that are known upfront. In our example that would be the pair of id
of the systems table that is referred to as system_id in the readings table. In the
systems table that column is defined as primary key and as such, it will always be a
unique value, hence it can only appear once in that table. On the readings table it is
defined as foreign key, meaning it must exist in the other one. The foreign key usually
creates a so-called index in the database that allows quick lookups, without going
through all rows, making the join perform well. The foreign key is not unique and does
not need to be in most models. In our example having the system appear multiple times
in the readings table (the right-hand side) is expected, unless you want your system to
produce power only once.
© Manning Publications Co. To comment go to liveBook
54
THE WITH CLAUSE
The WITH clause is also known as a common table expression (CTE). CTEs are
essentially views that are limited in scope to a particular query. Like a view, you might
want to use them to encapsulate parts of the logic of your query into a standalone
statement or at least, into an isolated part of a bigger query. While it would be perfectly
ok to create a view, you might not want that because you would only need its result in
the specific context of the bigger query. In addition, CTEs have one special trait that
views don’t have: views can reference other views, but they cannot be nested. A CTE
can reference other CTEs that are defined in the same WITH clause. With that, you can
build your query logic in an incremental fashion.
WITH clauses prevent the anti-pattern of having sub-queries defined in the FROM
clause. A sub-query as a source relation in a FROM clause is syntactically and
semantically valid as its result is a relation on its own, but it is often hard to read. In
addition, nested sub-queries are not allowed to reference themselves.
Finding the row containing the maximum value of a specific column in that row is often
computed by using a sub-query in the FROM clause like this:
SELECT max_power.v, read_on
FROM (
SELECT max(power) AS v FROM readings
) max_power
JOIN readings ON power = max_power.v;
The sub-query is a pretty simple query and rewriting it as a CTE doesn’t seem to make a
big difference at first glance. We take the same query, move it out of the FROM clause
and give a name within the WITH clause. The join statement stays the same:
Listing 3.16 Replacing a sub-query with a CTE
WITH max_power AS (
SELECT max(power) AS v FROM readings
)
SELECT max_power.v, read_on
FROM max_power
JOIN readings ON power = max_power.v;
For single and rather basic queries like this it does not make much of a difference
whether to use a sub-query or a CTE. But what if we ask for something like the
maximum average production of power per system and hour? Aggregate functions like
max and avg cannot be nested—i.e., you cannot do avg(max(v))—so you need to
individual aggregates.
© Manning Publications Co. To comment go to liveBook
55
The question of which row contains the minimum or maximum value of a column is
such a common task, that DuckDB has two built-in functions performing it: arg_max and
arg_min. These functions compute an expression defined by their first parameter on the
columns in the row for which the minimum or maximum value of the second parameter
occurs the first time. The following query will produce one row from the dataset at which
the highest amount of power was generated (not the 5 times, the query in listing 3.16
will return). This is because arg_max stops at the first value it finds that matches the
maximum value, while the join will include all rows.
SELECT max(power), arg_max(read_on, power) AS read_on
FROM readings;
The next query in listing 3.17 makes use of the arg_max aggregate. It first encapsulates
the complex logic of grouping the readings into average production by system and hour
—creating the first aggregate—in a CTE that we name per_hour and then takes that CTE
and computes a second aggregate over it.
Listing 3.17 Creating multiple groups
WITH per_hour AS ( -- #1
SELECT system_id,
date_trunc('hour', read_on) AS read_on,
avg(power) / 1000 AS kWh -- #2
FROM readings
GROUP BY ALL
)
SELECT name,
max(kWh), -- #3
arg_max(read_on, kWh) AS 'Read on'
FROM per_hour -- #4
JOIN systems s ON s.id = per_hour.system_id
WHERE system_id = 34
GROUP by s.name;
#1 Using a proper name for the CTE
#2 The average value per hour and day is the first aggregate we need; GROUP BY ALL is a DuckDB extension creating a group
from all columns not part of an aggregate
#3 The nested aggregate we look for
#4 Using the CTE as the driving table in the FROM clause
The result shows the "Andre Agassi Preparatory Academy" having the system with the
highest production in our dataset
© Manning Publications Co. To comment go to liveBook
56
┌───────────────────────────────────────┬──────────┬─────────────────────┐
│ name │ max(kWh) │ Read on │
│ varchar │ double │ timestamp │
├───────────────────────────────────────┼──────────┼─────────────────────┤
│ [34] Andre Agassi Preparatory Academy │ 123.75 │ 2020-04-09 11:00:00 │
└───────────────────────────────────────┴──────────┴─────────────────────┘
NOTE We looked this building up and the readings and values add up. From https://www.
bombardre. com/wp-content/ uploads/2017/ 10/Andre- Agassi-Academy. pdf: "Between April 2010
and July 2011, Bombard installed 2,249 Sharp 240 watt solar modules on the roofs of five
buildings and three solar support structures at the Agassi Academy in Las Vegas."
CTEs can do one more cool thing that views and sub-queries cannot: The WITH clause
has the additional keyword RECURSIVE that makes it possible to reference a CTE not only
from other succeeding CTEs and the FROM clause, but from within itself. Such a recursive
CTE essentially will follow this pattern shown below in listing 3.18. To make this work, we
need to have some kind of initial seed for the recursion. This is easy for a tree structure:
we take the row that has no parent row and use this as one leaf of a UNION clause.
© Manning Publications Co. To comment go to liveBook
57
Listing 3.18 Selecting a graph-shaped structure with recursive SQL
CREATE TABLE IF NOT EXISTS src (
id INT PRIMARY KEY,
parent_id INT, name VARCHAR(8)
);
INSERT INTO src (VALUES
(1, null, 'root1'),
(2, 1, 'ch1a'),
(3, 1, 'ch2a'),
(4, 3, 'ch3a'),
(5, null, 'root2'),
(6, 5, 'ch1b')
);
WITH RECURSIVE tree AS (
SELECT id,
id AS root_id,
[name] AS path -- #1
FROM src WHERE parent_id IS NULL -- #2
UNION ALL
SELECT src.id,
root_id,
list_append(tree.path, src.name) AS path
FROM src
JOIN tree ON (src.parent_id = tree.id) -- #3
)
SELECT path FROM tree;
#1 Initialize a new list with a list literal
#2 This is the recursive initial seed
#3 Recursive join until there are no more entries from the src table with the given parent id
The results are several paths, all starting at the root, making up their way to the
corresponding leaves:
© Manning Publications Co. To comment go to liveBook
58
┌─────────────────────┐
│ path │
│ varchar[] │
├─────────────────────┤
│ [root1] │
│ [root2] │
│ [root1, ch1a] │
│ [root1, ch2a] │
│ [root2, ch1b] │
│ [root1, ch2a, ch3a] │
└─────────────────────┘
The example aggregates names into a path from the root of a tree to the leaf by using
list_append. You could use list_prepend and inverse the parameter to build up paths
from the leafs to the root nodes.
As an exercise you try to compute the longest path in the tree. The recursive CTE
would stay the same, but you will want to apply the arg_max function you already
learned about in the SELECT statement together with the length aggregate on a list.
3.5 DuckDB-specific SQL extensions
One of the goals of the authors of DuckDB is to make SQL more accessible and user-
friendly. One way that they’ve done this is by adding additions to their implementation of
SQL that make it easy to do common tasks. In this section, we’ll introduce those
additions.
3.5.1 Dealing with SELECT *
SELECT * is a two-edged sword: it is easy to write down and the resulting tuples most
likely will contain what you actually need.
Some problems that go along with selecting all columns of a relation are:
Instability of the resulting tuples as a table definition might change
(adding or removing columns)
Putting more memory pressure on the database server or process
While DuckDB is an embedded database and won’t involve network
traffic, star-selects will cause more traffic on non-embedded databases
A star-select might prevent an index-only-scan. An index-only-scan will
occur when your query can use an index, and you only return columns
from that index so that any other IO can be avoided. An index-only-
scan is a desired behaviour in most cases
© Manning Publications Co. To comment go to liveBook
59
While it’s best to avoid doing too many SELECT * queries, sometimes they are
necessary, and DuckDB actually makes them safer to use with the addition of two
keywords, EXCLUDE and REPLACE.
If you are sure that you really want all columns, DuckDB offers a simplified version of
the SELECT statement, omitting the SELECT clause altogether, starting with the FROM
clause, so that you can do for example a FROM prices. We have another example
coming up in listing 3.22.
EXCLUDING SOME COLUMNS WITH EXCLUDE
EXCLUDE excludes one or more columns from a star query. This is helpful when you have
a table or relation with a lot of columns and nearly all of them are needed, apart from
some that are irrelevant to your specific use case. Normally you would have to
enumerate all the columns you are interested in and exclude the ones you don’t care
about. For example, you want only the relevant data from prices:
SELECT value, valid_from, valid_until FROM prices;
This gets tedious and error-prone real quickly, especially with more than a handful of
columns With the EXCLUDE clause, you only have to enumerate the columns you are not
interested in:
SELECT * EXCLUDE (id)
FROM prices;
You can exclude as many columns as you want. You will achieve most of the flexibility of
a pure SELECT *, keep the readability of the star, and make sure you don’t access
something you don’t need.
RESHAPING RESULTS WITH REPLACE
Think about the view v_power_per_day. It computes the kWh in fractions. For some
users you may only want to return the integer values. You could create a copy of that
view that recomputes the values rounding them, but you can also reuse the existing logic
and do the rounding where needed and keeping the structure the same:
SELECT * REPLACE (round(kWh)::int AS kWh)
FROM v_power_per_day;
The REPLACE clause takes in one or more pairs of x AS y constructs with x being an
expression that can refer to columns of the original select list, applying functions and
other transformations to them and y being a name that has been used in the original
select list.
© Manning Publications Co. To comment go to liveBook
60
The structure of the result is the same, but the kWh column now is an integer column:
┌───────────┬────────────┬───────┐
│ system_id │ day │ kWh │
│ int32 │ date │ int32 │
├───────────┼────────────┼───────┤
│ 1200 │ 2019-08-29 │ 289 │
│ · │ · │ · │
│ · │ · │ · │
│ · │ · │ · │
│ 10 │ 2020-03-19 │ 0 │
├───────────┴────────────┴───────┤
│ 1587 rows (2 shown) 3 columns │
└────────────────────────────────┘
DYNAMICALLY PROJECTING AND FILTERING ON COLUMNS
Let’s recap the prices table, it has two columns containing information about the validity
of a price:
┌─────────────┬──────────────┬─────────┬───┬──────────────────────┬───────┐
│ column_name │ column_type │ null │ … │ default │ extra │
│ varchar │ varchar │ varchar │ │ varchar │ int32 │
├─────────────┼──────────────┼─────────┼───┼──────────────────────┼───────┤
│ id │ INTEGER │ NO │ … │ nextval('prices_id') │ │
│ value │ DECIMAL(5,2) │ NO │ … │ │ │
│ valid_from │ DATE │ NO │ … │ │ │
│ valid_until │ DATE │ YES │ … │ │ │
├─────────────┴──────────────┴─────────┴───┴──────────────────────┴───────┤
│ 4 rows 6 columns (5 shown) │
└─────────────────────────────────────────────────────────────────────────┘
The COLUMNS expression can be used to project, filter and aggregate one or more
columns based on a regular expression. To select only columns that contain information
about validity you can query the table like this:
SELECT COLUMNS('valid.*') FROM prices LIMIT 3;
Returns all the relevant columns:
© Manning Publications Co. To comment go to liveBook
61
┌────────────┬─────────────┐
│ valid_from │ valid_until │
│ date │ date │
├────────────┼─────────────┤
│ 2018-12-01 │ 2019-01-01 │
│ 2019-01-01 │ 2019-02-01 │
│ 2019-02-01 │ 2019-03-01 │
└────────────┴─────────────┘
You want to use that technique if you have a table with lots of columns that have similar
names. That could be the case with a readings or measurement table. For example,
think of an IoT-sensor that produces many different readings per measurement. For that
use case another feature is interesting: You can apply any function over a dynamic
selection of columns that will produce as many computed columns. Here we compute
several maximum values at once for all columns in the price table that contain the word
"valid":
SELECT max(COLUMNS('valid.*')) FROM prices;
Which results in the maximum values for valid_from and valid_until:
┌────────────────────────┬─────────────────────────┐
│ max(prices.valid_from) │ max(prices.valid_until) │
│ date │ date │
├────────────────────────┼─────────────────────────┤
│ 2023-01-01 │ 2024-02-01 │
└────────────────────────┴─────────────────────────┘
If you find yourself writing long conditions in the WHERE clause combining many
predicates with AND, you can simplify that with the COLUMNS expression as well. To find
all the prices that have been valid in 2020 alone you would want every row for which
both the valid_from and valid_until columns are between January 1st 2020 and
2021 which is exactly what the following query expresses:
FROM prices WHERE COLUMNS('valid.*') BETWEEN '2020-01-01' AND '2021-01-01';
You might have noticed that the regular expression .* sticks out a bit. Many people are