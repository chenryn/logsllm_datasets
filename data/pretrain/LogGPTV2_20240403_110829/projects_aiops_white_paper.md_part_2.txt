#### Storage
The metrics are stored in a time-series database (TSDB) with three layers:
+ Memory database based on Redis stores **hot data**
    + The query engine determines if to access HBase or Redis
        + If the query is for data older than one day, it will query HBase, otherwise Redis
    + Data in Redis is compressed. The algorithm is from Facebook.
+ HBase stores **warm data**
    + Performance degradation: lots of compactation and splitting
    + Intolerable r/w latency in underlying HDFS
    + HBase balances random writes and sequentially access disks
        + Buffers writes and flushes writes into multiple HFiles (append only)
        + Read may need to scan all HFiles (disk seeks)
    + Compactation 
        + Compactation merges HFiles to accelerate read
        + HBase tables are split into chunks of rows called "regions"
        + Region will be split into two when it becomes too big
        + Regions are distributed and can be moved across servers to balance load
        + **Problem with compactation**: Consumes a lot of I/O; causes JVM stop the world GC; Block writes
        + **Solution**: Partition data by date
    + Handling splitting
        + Pre-splitting 
+ HDFS stores **cold data**
    + Need to reduce R/W latency
    + Put region server and data node together in the same node
The base architecture was optimize for high frequency read/writes:
+ *Write*. Use batch and asynchronous techniques to the write path.
+ *Read*. Customized data model with multi-layer down-sampling mechanism into HBase and 
use compression for in-memory database
The major challenge is the data scale.
#### Read and write
The architecture has two separate modules working on top on HBase to improve efficiency:
+ Query engine. Specialized read module
+ Saver. Specialized write module
#### Data layout
Data table:
+ data point: target, metric, time, value
    + Use as a row key: hash(target) + hash(metric) + hash(rounded to 2 hours)
    + Each row contains two hours of data
    + Each row has a constant length: 7190
    + Design inspired by OpenTSDB
+ data expires according to TTL
Metadata tables and index:
+ Metric properties are: name, cycle, value type
+ Tags: isp, dc, etc.
+ Index: tag -> time series
#### Challenges
+ Large queries are slow (and take a large bandwidth)
    + Daily resource usage report of all the hosts (CPU, MEM, IO, etc.)
        + Billions of data points requests
    + PV Growth trend of the whole year
        + Millions of data points requests
+ 80% small queries 
    + PV anomaly detection (needs data of recent hours)
+ Query patterns
    + Latency insensitive
        + Short-term (a lot of metrics); long-term (high resolution)
        + Daily resource usage report of all the hosts
        + There one day available to process these queries
        + **Solution:** Avoid HBase bandwidth exhaustion -> copy data to an external Hadoop asynchronously
    + Latency sensitive
        + Short-term (high resolution)
        + long-term (low resolution): trend data important for business
        + PV's growth trend visualization of the whole year 
        + PV's anomaly detection
        + **Solution:** Multi-level down-sampling
            + Down-sampling: High resolution -> Low resolution
            + Multi-level: 2 levels. A query for one year can be answer in a few milliseconds
            + Online pre-aggregation (max, min, sum, count) within Saver in real-time
## Methods, techniques, and algorithms
### Exploring SRE Pain Points
After identifying a pain point, we identify the following elements to develop a solution:
+ Existing manual workflows for troubleshooting for automatization 
+ Key golden metrics which can enable an effective anomaly detection
+ Data sources for root cause analysis
+ Manual recovery actions
+ Critical components which requires special monitoring infrastructure
### Data Ingestion
Monitoring systems and monitoring data is the corner stone of troubleshooting.
Their goal is to track of the *health status* of components and applications.
Traditional and well-known systems include Ganglia and Nagios.
Monitoring data comes from many different data sources such hypervisors, OS, applications, application servers, 
middleware, databases, application logs, host and network metrics.
Generally, data sources can be classified in four types:
1. *Logs*. Service, microservices, and applications generate logs, composed of timestamped records with a structure 
and free-form text, which are stored in system files.
2. *Metrics*. Examples of metrics include CPU load, memory available, and the response time of a HTTP request.
3. *Traces*. Traces records the workflow and tasks executed in response to an HTTP request. 
4. *Events*. Major milestones which occur within a data center can be exposed as events. 
Examples include alarms, service upgrades, and software releases.
Examples:
> 2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server [req-c0b38ace - default default] Exception during message handling
> {“tags": [“mem”, “192.196.0.2”, “AZ01”], “data”: [2483, 2669, 2576, 2560, 2549, 2506, 2480, 2565, 3140, …, 2542, 2636, 2638, 2538, 2521, 2614, 2514, 2574, 2519]}
> {"traceId": "72c53", "name": "get", "timestamp": 1529029301238, "id": "df332", "duration": 124957, “annotations": [{"key": "http.status_code", "value": "200"}, {"key": "http.url", "value": "https://v2/e5/servers/detail?limit=200"}, {"key": "protocol", "value": "HTTP"}, "endpoint": {"serviceName": "hss", "ipv4": "126.75.191.253"}]
> {"id": "dns_address_match“, "timestamp": 1529029301238, ...}
> {"id": "ping_packet_loss“, "timestamp": 152902933452, ...}
> {"id": "tcp_connection_time“, "timestamp": 15290294516578, ...}
> {"id": "cpu_usage_average “, "timestamp": 1529023098976, ...}
#### Examples 
Google SRE team proposed [4 Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/) 
which provide key insights on how a distributed system is running using metrics:
+ *Latency*. Time to handle a request (aka response time)
+ *Traffic*. How much demand is being placed on a system
+ *Errors*. Rate of requests that fail
+ *Saturation*. Constraints places on service resources 
Other proposals include the [RED](https://www.vividcortex.com/blog/monitoring-and-observability-with-use-and-red) and 
[USE](http://www.brendangregg.com/usemethod.html?source=post_page---------------------------) methods.
When key services are not often called by users, the volume of metrics collected is insufficient for 
pattern recognition and anomaly detection.
In such cases, [synthetic monitoring](https://en.wikipedia.org/wiki/Synthetic_monitoring) (also known as active 
monitoring) can be adopted and consists in creating artificial users to simulate user behavior by making automated
calls to services. 
An AIOps platform needs to be able to ingest logs, metrics, traces, and events into efficient key-value databases
where they are stored to later be accessed and analyzed.
Challenges:
+ *Resolution*. While reading data sources every minute is relatively easy to achieve, as systems become more complex, 
non-linear, and with an large customer base, fine grained metrics are needed. Often, one second polling resolution 
is required since anomalies and uncommon patterns that occur in a one minute interval are invisible. 
To get the monitoring data needed, SRE need to write new tools, patch existing systems, and add knobs to 
production platforms to control their behavior.
#### Distributed Tracing
Trends:
+ Monoliths to microservices 
+ Basic concurrency to async concurrency to distributed concurrency
+ Complexity
+ Span Categorization Patent
Distributed tracing enables understanding how systems' components interact together when handling incoming requests.
It has its root on early research on concepts such as X-Trace and Magpie, and was generalized in industry 
with Google paper [Dapper](https://static.googleusercontent.com/media/research.google.com/en//archive/papers/dapper-2010-1.pdf)
in 2010.
Trace events can be generated across software stacks and within a software stack.
Many companies supported by large-scale systems, such as Uber and Facebook, operate large scale distributed 
tracing systems to gain observability capabilities. 
Opensource:
+ [OpenTelemetry](https://opentelemetry.io)
+ [W3C Distributed Tracing Working Group](https://www.w3.org/2018/distributed-tracing/)
[Industry Solutions](https://atscaleconference.com/videos/systems-scale-2019-observability-infra-uber-and-facebook/):
+ Facebook
    + FBTrace (node trace model), trace filter, trace datastore, Canopy (stream processing for traces), Scube
    + Compare populations of traces (e.g., before and after releases, perf regression)
+ Uber
    + One request has 30 services and 100 RPCs
    + Use tracing for root cause analysis
    + Compare trace structures (just like Code Diff) -- saseq
    + Compare span durations (heat map of latencies) -- LMU
    + Challenge:
        + Individual traces can be an outlier 
        + Users must find the right **baseline**
            + Create a statistical model from an aggregate of traces
    + Benefits:
        + Uber can solve problems from 30 minutes to 2 minutes
### Pattern Recognition
The objective of approaches for [pattern recognition](https://en.wikipedia.org/wiki/Pattern_recognition) is to detect
patterns in noisy and high-dimensionality data. 
Once the data is collected, we apply probabilistic algorithms, ML and other techniques to find suspicious patterns.
Examples of patterns of interests include:
+ Latency outliers and latency trends in metrics
+ Gradual degradation of traffic and incoming calls
+ Spikes or sudden change in error rate in logs
+ Saturation of memory utilization \>95% memory 
+ Structural changes in traces
For metrics, pattern recognition can rely on [feature-based time-series analysis](https://arxiv.org/abs/1709.08055) 
to identify interpretable insights of their structure.
Patterns of interests are not always a synonym of an anomaly or a failure. Often, a pattern is associated with a
probability that something is possibly wrong. By correlating patterns from multiple data sources, we increase 
the confidence (precision and recall) that a failure is indeed under way.
For example, we can autonomously identify anomalous microservices' latencies by dynamically choosing
temporal features, predict memory leaks ahead of time before impacting systems, or finding rare message entries in 
service logs with billions records. We applies all these techniques to real-time data streams.
As another example, although distributed logging is a solved problem and many solutions already exist, 
what still needs to be mastered is the extraction of meaningful and actionable information from massive logs.
While many argue that "the more [data] the merrier", in reality, the more log statements you have, the less
you can find due to noise and non-determinism.
With the success of developing pattern recognition for anomaly detection in 2017-2018, in 2019 we are planning
the next phase of our next-gen monitoring and troubleshooting suite. 
We will extend supported patterns by implementing new detector services for distributed trace and service logs.
All the anomaly detectors contribute with results to a central knowledge repository of metric, trace, and log 
observations, and alarms and relevant external events (e.g., platform upgrades).
#### Challenges
+ [Multimodal metrics](https://en.wikipedia.org/wiki/Multimodal_distribution). 
Since distributed systems are composed of many subsystems, it is expected to observe Gaussian mixture models
representing normally distributed subpopulations generated by the subcomponents. When subpopulation assignment 
is not known, unsupervised learning can be used to determine the subpopulation a data point belongs to. 
If the number of components is known, expectation maximization (EM) can be used to estimate the mixture model's
parameters, and, afterwards run a clustering algorithm.
Nonetheless, the number of components is unusably not known. Furthermore, the distribution of data points is often not
Gaussian.
+ Direct and indirect metrics. CPU load, available memory, network resources, and IO are direct signal host-level
metrics.
On the other hand, the *response time* of a service call to a microservice provisioned by *n* distributed components
is an indirect signal service-level metric. As its values dependent on the health of the subcomponents and the
subset of components involved during the handling of the request.
Indirect metrics are far more complex to analyze when compared to direct ones.
+ Variability. Due to the large number of components presents in a large-scale systems, the variability of latency 
is high. The reasons of this variability in individual components of a service is well known and in the 
Communication of the ACM article 
[The Tail at Scale](https://www2.cs.duke.edu/courses/cps296.4/fall13/838-CloudPapers/dean_longtail.pdf).
Sources of variability include the existence of daemons, shared resources, garbage collection, queueing, and 
energy management. Techniques such as replication, sharding, and load-balancing all contribute to increase the entropy
of a complex system. 
### Inductive Inference 
While the patterns recognized correspond to the symptoms of an underlying problem, inductive inference
explores the problem space and tries to identify the faulty services or resources.
Inductive reasoning draws a conclusion by gathering together particular observations (i.e., patterns discovered) 
in the form of premises and reasons from these particular premises to a general conclusion.
Troubleshooting, root-cause analysis, tuning, and capacity planning are particular forms of inference.
A [semi-supervised machine learning](https://en.wikipedia.org/wiki/Semi-supervised_learning) system will analyze
an observed pattern repository to automatically infer complex incidents associated with failures and explain 
the underlying possible root-cause to SREs and operators. 
This inference will learn associations between patterns, alerts and external events which will be formalized as 
rules and stored in a [knowledge-based system](https://en.wikipedia.org/wiki/Knowledge-based_systems). 
On top, a smart assistant will help operators in making associations and decisions on the relationship 
between patterns, alerts and anomalies for [root-cause analysis](https://en.wikipedia.org/wiki/Root_cause_analysis).
Several techniques can be for inductive inference , e.g.:
+ Traffic analysis: Correlation between sudden increase in requests and slashdot effect, with increase 
latency of requests. 
+ Trace analysis: Component or dependency failure, structural trace analysis, response time span analysis.
+ Event analysis: Causality between upgrades, reconfigurations, and forklift replacements and failure.
Inference can also decide to run automated diagnostics scripts (runbooks) to gain additional insights of the 
current state of components, services, or systems to improve inference.
For example, when pattern recognition identifies an HTTP endpoint with a high latency associated with an anomaly 
by analysing metrics, distributed traces are immediately analysed to reveal exactly which microservice or component 
is causing the problem.
Its logs and context metrics are accessed to quickly diagnose the issue. 
Afterwards, when sufficient evidence characterizing the problem is collected, inference will nominate operations and 
remediation actions to be executed.
#### Challenges
+ Access to customer systems is not possible to calibrate models: use Transfer learning
+ Model localization: the same model is adapted to different contexts
+ How to improve model based on running and field information
+ [Fuzzy Logic](https://en.wikipedia.org/wiki/Fuzzy_logic).
Determining that a service is in a failed state is rather simple. The challenge is to determine if the current state 
of a service is in the gray zone between the *ok* and *failed* states. For example, analyzing a service 