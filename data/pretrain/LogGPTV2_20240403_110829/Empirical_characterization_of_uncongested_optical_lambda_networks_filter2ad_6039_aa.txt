title:Empirical characterization of uncongested optical lambda networks
and 10GbE commodity endpoints
author:Tudor Marian and
Daniel A. Freedman and
Ken Birman and
Hakim Weatherspoon
2010 IEEEIIFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
Empirical Characterization 
of Uncongested Optical Lambda Networks 
and lOGbE Commodity Endpoints 
Tudor Marian, Daniel A. Freedman, Ken Birman, Hakim Weatherspoon 
Computer 
{tudorm,dfreedman,ken,hweather}@cs.com
NY 14850 
ell.edu 
Cornell University, 
Department, 
Science 
Ithaca, 
Abstract 
High-bandwidth, 
semi-private 
optical 
lambda networks 
associated 
a careful 
defense, 
platforms, 
end-hosts. 
and degraded 
lambda network 
environments 
variations, 
examination 
We use identical 
of an uncongested 
and for scien­
This paper 
running 
scenarios 
throughput 
fast commodity source 
and other enterprises. 
charac­
of the end-to-end 
at high 
carry growing volumes of data on behalf of large data cen­
ters, both in cloud computing 
tific, financial, 
undertakes 
teristics 
speeds over long distances, identifying 
with loss, latency 
at 
attached 
and destination 
receive 
degraded performance 
particular, 
the receiver 
employs relatively 
tical network components 
speeds of commodity end-host 
more end-to-end 
we encounter. 
those hoping to achieve 
end networked 
processors, 
will confront the same issue 
Our work thus poses a new challenge 
in higher­
is common and easily 
loses packets 
to outpace clock 
hence more and 
more or less what we send. We observe 
low data rates. Data rates of future op­
even when the sender 
applications 
are projected 
settings. 
provoked. 
for 
In 
hence expect the destination 
to 
otherwise: 
dependable  performance 
1 Introduction 
Optical lambda networks play an increasingly 
central 
globally 
distributed, 
systems and applications. 
fi­
communities 
are de­
and other enterprise 
for high-bandwidth, 
Scientific, 
semi-private 
defense, 
supporting 
over dedicated 
dispersed 
role in the infrastructure 
high-performance 
nancial, 
ploying lambda networks 
data transport 
geographically 
data centers. 
Cornell University 
in New York receive 
streams from the Arecibo Observatory 
Large Hadron  Collider 
the San Diego Supercomputer 
trieve the results 
Enterprise 
have begun to build proprietary 
technology 
in Switzerland, 
for future  reference 
fiber optic spans between 
at 
Astrophysicists 
high-volume 
data 
in Puerto Rico or the 
process the data at 
and re­
and storage at 
Cornell. 
Center in California, 
firms, such as Google and Microsoft, 
networks 
to interconnect 
this architecture 
their data centers; 
of consolidation 
while increasing 
against 
fault-tolerance 
This trend will only accelerate. 
balances 
the economics 
the benefits of end-user 
proximity, 
through redundancy. 
We are seeing a new 
For 
announced 
of 1 Gigabit 
Google recently 
per second (Gbps), 
a fiber-to-the-home 
bidirectional 
wave of ambitious  commercial  networking  initiatives. 
example, 
test network [7] in the United States to deliver 
bandwidth 
ternet providers 
jecting 
significant 
width. In contrast, 
receiving 
end-hosts 
core) performance 
speed networks 
by an ensemble 
while major In­
such as Verizon and Time Warner are pro­
in consumer band­
in Figure 1, the sending and 
are approaching 
Thus, the future may bring high­
to commodity 
barrier. 
connected 
of slow cores. 
future improvements 
as illustrated 
themselves 
a (single­
machines 
powered 
because, 
for specific 
bandwidth 
frustrating 
use, and operate 
than required, 
increasingly 
One consequence 
unlike the public Internet, 
they might expect [24, 8]. This can be es­
typi­
is that, while lambda networks 
dedicate 
their 
with virtually 
no con­
idle), end­
are routinely 
find it hard to derive the 
cally have greater 
transport 
gestion [5] (in fact, the networks 
hosts and applications 
full performance 
pecially 
fic across these semi-private 
seemingly 
far from the congestion 
optical 
ets at all, and one might reasonably 
sent at some regular 
the lambda network, 
the same rate. In particular, 
Controllers 
(NICs) can reliably 
mum data rates in the lab, they should similarly 
an uncongested 
should not drop any pack­
that, if traffic is 
rate well below the actual capacity 
of 
it will arrive intact and more or less at 
since they operate 
and employ high quality 
fiber, lambda networks 
ideal conditions; 
lambda networks 
lambda network. 
communicate 
and lossless 
for example, 
if end-host 
threshold 
at their maxi­
do so over 
believe 
traf­
Network Interface 
encounters 
Our study reveals 
In this paper, we show that loss occurs in precisely 
such 
that, in most cases, the prob­
network span it­
situations. 
lem is not due to loss within the optical 
self but instead 
commodity end-hosts 
network: 
further 
mismatch. 
This mismatch is 
where the bottlenecks 
prove 
arises from the interaction 
a kind of impedance 
in situations 
aggravated 
of lower-speed 
with such a high-bandwidth  optical 
978-1-4244-7501-8/10/$26.00 
©l01O IEEE 
575 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
• The size of the socket buffer and of the Direct Mem­
by the end-host. 
ory Access (DMA) ring determines 
rienced 
Similarly, 
ity policy of the network adapter, 
to individual 
traffic, also affects the end-host 
processor 
the loss rate expe­
the interrupt 
affin­
cores upon receipt 
of network 
loss distribution. 
that maps interrupts 
• 
• 
100Mbps 
200MHz 
10Gbp 
3.2GHz 
Processor -Network 
Impedance Mismatch 
1985 
1990 1995 2000 2005 2010 
Year 
Figure 1. Network to processor speed ratio. 
• The throughput 
of the ubiquitous 
Transmission 
Con­
as packet loss increases, 
decreases 
trol Protocol  (TCP) 
and this phenomenon 
a function 
both the path length and the window size. The conges­
tion control algorithm 
role in determining 
grows in severity as 
turns out to have only a marginal 
the achievable 
throughput. 
of 
• Batching 
of packets, 
through both kernel and NIC 
at the cost of 
techniques, 
disturbing 
as packet inter-arrival 
increases  overall 
throughput, 
any latency-sensitive 
times. 
measurements, 
such 
This paper first introduces 
two examples 
of uncongested 
lambda networks-the 
NLR Rings testbed. 
our experimental 
the context 
results. 
TeraGrid [6] and our own Cornell 
and discuss 
In Section 3, we present 
Section 
4 places this study within 
of past work, and Section 
5 concludes. 
2 Uncongested 
Lambda Networks 
may soon worsen: 
to be achieved 
memory buses, which are generally 
even 
performance 
And the situation 
is expected 
increase 
parallelism, 
to be end-host 
slower than processors. 
end-host 
mostly through multicore 
challenge 
to share a network interface 
cessor cores. One issue is contention 
that the performance-enhancing 
queue NICs (like Receive Side Scaling) 
a large number of distinct, 
lower bandwidth, 
Our goal here is not to solve this problem, 
features 
flows. 
yet it can be a real 
among multiple 
pro­
[ 17], and a second is 
of modern multi­
work best only for 
but rather to 
we have designed 
future sys­
optical 
servers. 
high-speed 
lambda network 
(IOGbE) 
Accordingly, 
commodity 10 Gigabit Ethernet 
shed more light on it, with the hope of informing 
tems architecture research. 
a careful empirical measurement 
of the end-to-end 
behav­
ior of a state-of-the-art 
interconnecting 
end-host 
performing 
networks 
successor 
Internet 
dedicated 
actions 
lambda network and IOGbE commodity 
as they have emerged, 
NSFNET [ 16, 20], and subsequently 
between the high-bandwidth 
[31]. However, few studies 
Our community has a long history 
and none consider 
lambda networks, 
measurements 
have looked at semi­
systematic 
ARPANET, its 
including 
the early 
optical 
the inter­
of 
on many prominent 
This study uses a new experimental 
core [26] of a 
end-hosts 
[32]. 
networking 
LambdaRail 
infras­
(NLR) 
end-to-end 
testbed-the 
Cornell National 
of a set of four all-optical 
lengths (up to 15000 km) and 
(up to 13), with ingress 
and 
the core 
tructure 
Rings-consisting 
IOGbE paths, of different 
number of routing elements 
egress points at Cornell 
of the network is indeed uncongested, 
accounting 
lion packets 
brief instance 
nificant 
for all loss associated 
during a 48-hour period, 
with sending over 20 bil­
only one 
of loss in the network core, in contrast 
to sig­
packet loss observed 
On this testbed, 
University. 
we observed 
and loss is very rare; 
on the end-hosts 
to the relation 
themselves. 
between end-to­
of the end-host: 
Our key findings pertain 
end behavior 
and fine-grained 
configuration 
Lambda networking, 
as defined by the telecommunica­
and set of services 
optical 
directly 
wavelengths 
to pro­
channels 
is the technology 
communication 
the use of multiple 
tions industry, 
surrounding 
vide independent 
of fiber optic cable [34]. In this section, 
amples of lambda networks, 
namely TeraGrid 
NLR Rings testbed. 
Cornell 
uncongested 
private, 
vision Multiplexing 
tical Networking 
(SONET) links. 
Both networks 
Dense Wavelength 
IOGbps optical 
(DWDM) or OC-192 Synchronous 
Di­
Op­
[6] and the 
of semi­
consist 
along a strand 
we present 
two ex­
2.1 TeraGrid 
TeraGrid [6] is an optical 
network interconnecting 
ten 
sites throughout 
the United States. 
30Gbps or 40Gbps aggregated 
major supercomputing 
The backbone provides 
throughput 
End-hosts, 
links, 
sites is 1 Gbps. 
hence the link capacity 
over IOGbE and SONET OC-192 links [26]. 
however, connect to the backbone via 1 Gbps 
between each pair of end-host 
frame­
interest 
Of particular 
monitoring 
of 
measurements 
is the TeraGrid 
work [8]; each of the ten sites reports 
throughput 
packets performed 
second probe to every other site once an hour, resulting 
total of 90 overall 
ure 2 shows a histogram 
and loss rates of User Datagram Protocol 
with Iperf [33]. Every site issues a 60-
in a 
every hour. Fig­
packet loss (on a 
measurements 
of percentage 
collected 
(UDP) 
978-1-4244-7501-8/101$26.00 
©2010 IEEE 
576 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
25 
r-
20 
15 
£ :D 
2l 2 10 
c.. 
I" 
5 
o 
r-
r-
r- -
r-
I  
I x 
0.01 
0.03 0.050.07 O. I 0 
Packet loss l%J (Iogseale) 
Figure 2. Observed loss on TeraGrid. 
Dnn 
0.30  0.50 0.70 
1.00 
after eliminating 
14% of them had 0.10% loss. Though 
between November 1st, 2007, and January 25th, 
logscale) 
2008, where 24% of the measured loss rates had 0.01 % 
loss and a surprising 
not shown in the Figure, 
site (Indiana 
a steady 0.44% rate, 14% of the remainder 
of the measure­
ments showed 0.0 I % loss, while 3% showed 0.10% loss. 
Dialogue 
with TeraGrid 
that the steady 
loss rate experienced 
to a faulty commodity 
site was due 
network card at the end-host. 
operators 
by the Indiana 
revealed 
University 
University)  that 
a single TeraGrid 
End-host 
the Linux kernel software network 
With the New API 
loss: Conventional 
that the 
of packets are dropped when incoming traf­
wisdom maintains 
end-host. 
the receiving 
majority 
fic overruns 
(NAPI) [3] enabled, 
stack may drop packets in either of two places: 
there is insufficient 
(rx) DMA 
ring, and when enqueueing packets 
would breach the socket buffer limit. In both, the re­
ceiver is overwhelmed 
differ in the precise 
and loss is observed, 
that induce loss. 
on the receive 
conditions 
capacity 
but they 
when 
for socket delivery 
It may be the case that loss rates 
of any large-scale 
of service: 
Cost-benefit 
are typical 
is pro­
of immediately 
hibitively 
per­
formed with the faulty network card at Indiana Uni­
versity 
and fixing failures 
high. For example, 
over at least a three month period. 
the measurements 
where the cost 
persisted 
detecting 
networks, 
2010 IEEE/IFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
Device clutter: 
The critical 
communication 
path between 
any two end-hosts 
consists 
each of which represents 
of many electronic 
devices, 
a potential 
point of failure. 
dropped incoming  packets 
at 
2.2 Cornell NLR Rings 
Clearly, 
is necessary 
greater control 
mechanisms 
Rather than probing further 
of loss in such uncongested 
lambda 
to better determine 
into the characteris­
testbed 
centered 
at Cornell Uni­
we chose instead 
to create our 
the trigger 
networks. 
tics of the TeraGrid 
network, 
own network measurement 
versity 
Cornell National 
der to understand 
we first provide 
ment infrastructure 
and extending 
a fairly 
Although 
small, such numbers are sufficient 
to severely 
of TCP on these high-latency, 
wisdom suggests 
high­
bits). 
this is the 
However, 
equipment 
carrier-grade 
(NLR) Rings testbed. 
In or­
across the United States; 
LambdaRail 
the properties 
of the lambda network is far less than the sum 
parts-in fact, it can be less reliable 
by or­
paths [ 10, 27]. Conventional 
Indeed, 
links do not drop packets. 
is often configured 
to shut down beyond 
reduce the throughput 
bandwidth 
that optical 
optical 
bit error rates of 10-12 (one out of a trillion 
the reliability 
of its optical 
takes advantage 
of the existing 
ders of magnitude.  Consequently,  applications  depending 
backbone infrastructure. 
on protocols 
nected to the backbone  router 
high-speed  networks, 
function 
loss rates, 
way 2.4 GHz Xeon E7330 quad-core 
servers with 32GB RAM, each equipped with an Intel 
IOGbE LR PCIe x8 adapters 
(EXPX950IAFXLR). 
run a preemptive 
tel ixgbe driver version 1.3.47. 
offload (GSO) was disabled 
Linux kernel packet forwarding 
LambdaRaii 
[4] 
servers 
are con­
New York, and 
these are four­
R900 
high reliability  from 
to unexpectedly 
during UDP 
paths (care should be taken in gen­
like TCP, which require 
may be subject 
of the Cornell NLR Rings, 
description 
of our measure­
Figure 2 shows the loss rate experienced 
these rates to TCP). Furthermore, 
and hence low throughput. 
64-bit Linux 2.6.24 kernel, 
since it is incompatible 
with the 
and Egress end-hosts; 
Two commodity 
path, at intermedi­
or electrical 
Dell PowerEdge 
in this section. 
in Figure 3, our Cornell NLR Rings testbed 
loss occurred 
subsystem. 
it is unclear if 
The generic 
as Ingress 
in Ithaca, 
National 
detailed 
Depicted 
high 
They 
switches), 
or at the 
on paths where levels of 
moving 
by 20-second 
traffic on end-to-end 
eralizing 
packets were dropped along the optical 
ate devices (e.g. optical 
end-hosts. 
optical 
averages) 
gestion 
highly unlikely, 
work administrators 
were consistently 
link utilization 
(determined 
a conclusion 
Finally, 
[36]. 
lower than 20%, making con­
by the net­
supported 
with the In­
segmentation 
Local 
policy-, 
and 
Through a combination 
of IEEE 802.1 Q virtual 
we have established 
Area Network (VLAN) tagging and source-, 
destination-based 
routing, 
IOGbE full duplex routes that begin and end at Cornell, 
but transit 
York City and back, a small ring via Chicago, Atlanta, 
Washington  D.C., 
Chicago, 
and New York City, a medium ring via 
and 
Denver, Houston, 
various physical 
a tiny ring to New 
four static 
lengths: 
Atlanta, 
Washington  D.C., 
Lacking more detailed 
information 
about the specific 
events that trigger 
about the sources of the high observed 