User: I'll broadly divide the work to be done in two parts:

I have huge data (amounting to approx 1 TB, divided into hundreds of files), which I'm copying from S3 to HDFS via distcp
This data will be acted upon by a hadoop streaming job (a simple mapper and reducer, written in python) 

Now, I'll have to wait till all the data is copied to HDFS and only after that I can start my actual job. Here's the question: Considering that DISTCP in itself is a map-reduce job, is there a way I can "stream line" these two jobs, namely, can the second job start working on the data that has already been copied (e.g. distcp has already copied a few files, on which the second job can technically already start)?
I hope I've made myself clear.
Assistant: You can use S3InputFormat (https://github.com/ATLANTBH/emr-s3-io) to read data directly to you mappers. But beware, in case of job failure you will redownload all data.
So, I suggest to download all data before process it. If you don't need to process whole data at once, you can start you mapreduce after distcp start. But you should write you own extension of FileInputFormat which will record somewhere (in input directory, i suppose) which files was processed, and on each invocation filter out processed files (in method getSplits()) and process only uprocessed files.