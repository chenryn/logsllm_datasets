violate “privacy,” it is not enough to show that the adversary
learns something new about the training inputs. At the very
least, the adversary must learn more about the training inputs
than about other members of their respective classes. To
position our contributions in the context of related work
(surveyed in Section X) and motivate the need to study unin-
tended feature leakage, we discuss several types of adversarial
inference previously considered in the research literature.
A. Inferring class representatives
Given black-box access to a classiﬁer model, model inver-
sion attacks [16] infer features that characterize each class,
making it possible to construct representatives of these classes.
In the special case—and only in this special case—where all
class members are similar, the results of model inversion are
similar to the training data. For example, in a facial recognition
model where each class corresponds to a single individual, all
class members depict the same person. Therefore, the outputs
of model inversion are visually similar to any image of that
person, including the training photos. If the class members are
not all visually similar, the results of model inversion do not
look like the training data [53].
If the adversary actively participates in training the model
(as in the collaborative and federated learning scenarios con-
sidered in this paper), he can use GANs [22] to construct class
representatives, as done by Hitaj et al. [25]. Only in the special
case where all class members are similar, GAN-constructed
representatives are similar to the training data. For example,
all handwritten images of the digit ‘9’ are visually similar.
Therefore, a GAN-constructed image for the ‘9’ class looks
similar to any image of digit 9, including the training images.
In a facial recognition model, too, all class members depict the
same person. Hence, a GAN-constructed face looks similar to
any image of that person, including the training photos.
Note that neither technique reconstructs actual
training
inputs. In fact, there is no evidence that GANs, as used in [25],
can even distinguish between a training input and a random
member of the same class.
Data points produced by model inversion and GANs are
similar to the training inputs only if all class members are
similar, as is the case for MNIST (the dataset of handwritten
digits used in [25]) and facial recognition. This simply shows
that ML works as it should. A trained classiﬁer reveals the
input features characteristic of each class, thus enabling the
adversary to sample from the class population. For instance,
Figure 1 shows GAN-constructed images for the gender clas-
siﬁcation task on the LFW dataset, which we use in our
experiments (see Section VI). These images show a generic
female face, but there is no way to tell from them whether an
image of a speciﬁc female was used in training or not.
Finally, the active attack in [25] works by overﬁtting the
joint model’s representation of a class to a single participant’s
(cid:23)(cid:26)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1: Samples from a GAN attack on a gender classiﬁcation model
where the class is “female.”
training data. This assumes that the entire training corpus for
a given class belongs to that participant. We are not aware
of any deployment scenario for collaborative learning where
this is the case. By contrast, we focus on a more realistic
scenario where the training data for each class are distributed
across multiple participants, although there may be signiﬁcant
differences between their datasets.
B. Inferring membership in training data
The (arguably) simplest privacy breach is, given a model and
an exact data point, inferring whether this point was used to
train the model or not. Membership inference attacks against
aggregate statistics are well-known [14, 27, 50], and recent
work demonstrated black-box membership inference against
ML models [24, 34, 53, 58], as discussed in Section X.
The ability of an adversary to infer the presence of a speciﬁc
data point
in a training dataset constitutes an immediate
privacy threat if the dataset is in itself sensitive. For example,
if a model was trained on the records of patients with a
certain disease, learning that an individual’s record was among
them directly affects his or her privacy. Membership inference
can also help demonstrate inappropriate uses of data (e.g.,
using health-care records to train ML models for unauthorized
purposes [4]), enforce individual rights such as the “right
to be forgotten,” and/or detect violations of data-protection
regulations such as the GDPR [19]. Collaborative learning
presents interesting new avenues for such inferences.
C. Inferring properties of training data
In collaborative and federated learning, participants’ training
data may not be identically distributed. Federated learning
is explicitly designed to take advantage of the fact
that
participants may have private training data that are different
from the publicly available data for the same class [35].
Prior work [2, 16, 25] aimed to infer properties that char-
acterize an entire class: for example, given a face recognition
model where one of the classes is Bob, infer what Bob looks
like (e.g., Bob wears glasses). It is not clear that hiding this
information in a good classiﬁer is possible or desirable.
By contrast, we aim to infer properties that are true of
a subset of the training inputs but not of the class as a
whole. For instance, when Bob’s photos are used to train
a gender classiﬁer, we infer that Alice appears in some of
the photos. We especially focus on the properties that are
independent of the class’s characteristic features. In contrast
to the face recognition example, where “Bob wears glasses”
is a characteristic feature of an entire class, in our gender
classiﬁer study we infer whether people in Bob’s photos wear
glasses—even though wearing glasses has no correlation with
gender. There is no legitimate reason for a model to leak this
information; it is purely an artifact of the learning process.
A participant’s contribution to each iteration of collaborative
learning is based on a batch of his training data. We infer
single-batch properties, i.e., detect that the data in a given
batch has the property but other batches do not. We also infer
when a property appears in the training data. This has serious
privacy implications. For instance, we can infer when a certain
person starts appearing in a participant’s photos or when the
participant starts visiting a certain type of doctors. Finally, we
infer properties that characterize a participant’s entire dataset
(but not the entire class), e.g., authorship of the texts used to
train a sentiment-analysis model.
IV. INFERENCE ATTACKS
A. Threat model
We assume that K participants (where K ≥ 2) jointly
train an ML model using one of the collaborative learning
algorithms described in Section II-B. One of the participants
is the adversary. His goal
is to infer information about
the training data of another, target participant by analyzing
periodic updates to the joint model during training. Multi-
party (K > 2) collaborative learning also involves honest
participants who are neither the adversary, nor the target. In
the multi-party case, the identities of the participants may not
be known to the adversary. Even if the identities are known but
the models are aggregated, the adversary may infer something
about the training data but not trace it to a speciﬁc participant;
we discuss this further in Section IX-D.
The updates that adversary observes and uses for inference
depend on both K and how collaborative training is done.
As inputs to his inference algorithms, the adversary uses the
model updates revealed in each round of collaborative training.
For synchronized SGD [52] with K = 2,
the adversary
observes gradient updates computed on a single batch of the
target’s data. If K > 2, he observes an aggregation of gradient
updates from all other participants (each computed on a
single batch of the respective participant’s data). For federated
learning with model averaging [35],
the observed updates
are the result of two-step aggregation: (1) every participant
aggregates the gradients computed on each local batch, and
(2) the server aggregates the updates from all participants.
For property inference, the adversary needs auxiliary train-
ing data correctly labeled with the property he wants to infer
(e.g., faces labeled with ages if the goal is to infer ages).
For active property inference (Section IV-E), these auxiliary
data points must also be labeled for the main task (e.g., faces
labeled with identities for a facial recognition model).
B. Overview of the attacks
Figure 2 provides a high-level overview of our inference
attacks. At each iteration t of training, the adversary down-
loads the current joint model, calculates gradient updates as
prescribed by the collaborative learning algorithm, and sends
(cid:23)(cid:26)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 2: Overview of inference attacks against collaborative learning.
t
(cid:2)
k Δθk
t
During training,
his own updates to the server. The adversary saves the snapshot
of the joint model parameters θt. The difference between
the consecutive snapshots Δθt = θt − θt−1 =
is
equal to the aggregated updates from all participants, hence
Δθt − Δθadv
are the aggregated updates from all participants
other than the adversary.
Leakage from the embedding layer. All deep learning models
operating on non-numeric data where the input space is
discrete and sparse (e.g., natural-language text or locations)
ﬁrst use an embedding layer to transform inputs into a lower-
dimensional vector representation. For convenience, we use
word to denote discrete tokens, i.e., actual words or speciﬁc
locations. Let vocabulary V be the set of all words. Each word
in the training data is mapped to a word-embedding vector via
an embedding matrix Wemb ∈ R
|V |×d, where |V | is the size of
the vocabulary and d is the dimensionality of the embedding.
the embedding matrix is treated as a
parameter of the model and optimized collaboratively. The
gradient of the embedding layer is sparse with respect to the
input words: given a batch of text, the embedding is updated
only with the words that appear in the batch. The gradients
of the other words are zeros. This difference directly reveals
which words occur in the training batches used by the honest
participants during collaborative learning.
Leakage from the gradients.
In deep learning models, gra-
dients are computed by back-propagating the loss through the
entire network from the last to the ﬁrst layer. Gradients of a
given layer are computed using this layer’s features and the
error from the layer above. In the case of sequential fully
connected layers hl, hl+1 (hl+1 = Wl · hl, where Wl is the
weight matrix), the gradient of error E with respect to Wl is
· hl. The gradients of Wl are inner
computed as ∂E
∂Wl
products of the error from the layer above and the features
hl. Similarly, for a convolutional layer, the gradients of the
weights are convolutions of the error from the layer above
and the features hl. Observations of gradient updates can thus
be used to infer feature values, which are in turn based on the
participants’ private training data.
= ∂E
∂hl+1
C. Membership inference
As explained above, the non-zero gradients of the embed-
ding layer reveal which words appear in a batch. This helps
infer whether a given text or location appears in the training
dataset or not. Let Vt be the words included in the updates Δθt.
During training, the attacker collects a vocabulary sequence
[V1, . . . , VT ]. Given a text record r, with words Vr, he can
test if Vr ⊆ Vt, for some t in the vocabulary sequence. If r
is in target’s dataset, then Vr will be included in at least one
vocabulary from the sequence. The adversary can use this to
decide whether r was a member or not.
D. Passive property inference
We assume that the adversary has auxiliary data consisting
of the data points that have the property of interest (Dadv
prop) and
data points that do not have the property (Dadv
nonprop). These data
points need to be sampled from the same class as the target
participant’s data, but otherwise can be unrelated.
The intuition behind our attack is that the adversary can
leverage the snapshots of the global model to generate aggre-
gated updates based on the data with the property and updates
based on the data without the property. This produces labeled
examples, which enable the adversary to train a binary batch
property classiﬁer that determines if the observed updates are
based on the data with or without the property. This attack is
passive, i.e., the adversary observes the updates and performs
inference without changing anything in the local or global
collaborative training procedure.
Batch property classiﬁer. Algorithm 3 shows how to build a
batch property classiﬁer during collaborative training. Given a
model snapshot θt, calculate gradients gprop based on a batch
with the property badv
prop and gnonprop based on a batch
without the property badv
nonprop. Once enough labeled
gradients have been collected, train a binary classiﬁer fprop.
nonprop ⊂ Dadv
prop ⊂ Dadv
For
that
exploit
attacks
the property inference
the
embedding-layer gradients (e.g., the attack on the Yelp dataset
in Section VI-B), we use a logistic regression classiﬁer. For all
other property inference attacks, we experimented with logistic
regression, gradient boosting, and random forests. Random
forests with 50 trees performed the best. The input features
in this case correspond to the observed gradient updates.
The number of the features is thus equal
to the model’s
parameters, which can be very large for a realistic model.
To downsample the features representation, we apply the
max pooling operator [21] on the observed gradient updates.
More speciﬁcally, max pooling performs a max ﬁlter to non-
(cid:23)(cid:26)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 3 Batch Property Classiﬁer
Inputs: Attacker’s auxiliary data Dadv
Outputs: Batch property classiﬁer fprop
Gprop ← ∅
Gnonprop ← ∅
for i = 1 to T do
prop, Dadv
nonprop
(cid:4) Positive training data for property inference
(cid:4) Negative training data for property inference
Receive θt from server
Run ClientUpdate(θt)
prop ⊂ Dadv
Sample badv
prop, badv
Calculate gprop = ∇L(badv
Gprop ← Gprop ∪ {gprop}
Gnonprop ← Gnonprop ∪ {gnonprop}
nonprop ⊂ Dadv
prop; θt), gnonprop = ∇L(badv
nonprop
nonprop; θt)
end for
Label Gprop as positive and Gnonprop as negative
Train a binary classiﬁer fprop given Gprop, Gnonprop
overlapping subregions of the initial features representation,