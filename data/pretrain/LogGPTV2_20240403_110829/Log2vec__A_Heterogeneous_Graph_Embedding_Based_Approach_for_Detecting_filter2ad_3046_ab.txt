Figure 2d demonstrates a graph indicating a user’s behavior
across hosts, from logons in Figure 2a (red font). We can analyze
these interactive relationships among hosts to find out anoma-
lous logons. For instance, an administrator may regularly log in to
a group of hosts for system maintenance while an APT perpetrator
Figure 2: Sequence and graph approaches for cyber threat de-
tection. (a) a log file recording user’s operations (b) attributes
of a log entry (c) sequence approach: learn from past events
and predict the next ones; (d) graph approach: extract Com-
puter’s/ Server’s features and detect suspicious ones
can only visit hosts that he can. This difference can be caught by fea-
tures of logon traces [4]. For instance, the number of hosts involved
in a benign trace (1 or 3, solid lines) is generally different from that of
APT (2, dashed lines). After analyzing these features, compromised
hosts can be identified. However, these hosts comprise numerous
benign operations and manually extracted domain-specific features
apparently cannot be applied to the attack in Figure 2c.
In summary, the existing methods have the following deficien-
cies: fail to 1) cover various cyber threats, such as the aforemen-
tioned insider threats and APT; 2) exactly detect malicious oper-
ations (log-entry level), specifically deeply mining and analyzing
relationships among log entries within a day/host; 3) perform the
detection without attack samples for training.
2.2 Architecture
Log2vec is composed of three key components: graph construction,
graph embedding and detection algorithm as shown in Figure 1.
Graph construction. Log2vec’s first component is a rule-based
heuristic approach to map relationships among log entries that
reflect users’ typical behavior and expose malicious operations,
into a graph. According to the existing methods [4, 38, 41, 50, 57],
log2vec mainly takes three relationships into account: (1) causal
and sequential relationships within a day; (2) logical relationships
among days (3) logical relationships among objects. (Section 3.2)
To deeply mining and analyzing relationships among log entries
within a day/host, we divide a log entry into five primary attributes
(subject, object, operation type, time and host), named as meta-
attributes (see Section 3.1). When designing rules regarding the
three relationships, we consider different combinations of these
meta-attributes to correlate fewer log entries and map finer logs’
relationships into the graph. For instance, an administrator logs in
to his own computer, then remotely logs in to a server and opens a
file to view the state of the system in day2 (Figure 2a). This sequence
reflects a sequential relationship among log entries. We use a rule,
(d)15462318712C1C2C3C4S1S2(c)19889user1pc12018-12-1012:30:15writefile1(b)...S3prediction systempreprocess&trainingpredictC1: [1,2,...]C2: [1,3,...]...feature extractionAnomaly Detection C1,S1,S2compromisedhosts(a)day1day2logon;file open;file write;...web visit; download;  dev connect;...logoff           logon;logon;file open;...send; dev connect;logon;...logoffday3                   logon;dev connect;file copy;...dev connect;file copy;dev connect;...logoff18987Session 8C: Blockchain VICCS ’19, November 11–15, 2019, London, United Kingdom1779In another example, we consider another meta-attribute, opera-
tion type, and use a rule, log entries of the same user and the same
operation type are connected in chronological order (rule B), to
concatenate device connection operations within a day. The num-
ber of log entries involved in daily sequences only regarding device
connections is obviously smaller than that of ones comprising all
operations (Figure 2a). After generating three sequences of device
connections corresponding to the 3 days, we employ other rules
to correlate them according to their similarities. As the sequence
in day3 involves numerous connection operations, far more than
others, it has lower link weights with others. This difference can
be captured by graph embedding. Likewise, log2vec converts the
relationship in Figure 2d into the graph (see Section 3.2.3).
Through different combinations of log’s attributes, we devise
various behavioral sequences involving fewer log entries and map
multiple finer relationships among log entries within a day and a
host, into the graph. After graph embedding and detection algo-
rithm, log2vec produces small clusters to reveal anomalous opera-
tions. In practice, the numbers of log entries involved in suspicious
clusters are very small and even equal to 1. Therefore, log2vec more
finely mines user’s behavior in the above two attack scenarios.
According to each rule in log2vec, we translate log entries into
sequences or subgraphs, all of which constitute a heterogeneous
graph. Each rule, corresponding to an edge type, is derived from a
specific relationship, as above examples. As different relationships
play different roles in various scenarios, we use multiple edge types,
instead of weights, to discriminate them.
Graph embedding. Graph embedding, also graph representation
learning, is a machine learning approach, capable of converting
nodes (log entries) in the heterogeneous graph into low-dimension
vectors [11, 14, 39, 54]. Specifically, graph embedding involves ran-
dom walk and word2vec in Figure 1. The former extracts context
of each node and feeds it into the latter that calculates its vector.
Random walk is a popular graph traverse algorithm. Assuming
a walker resides at a node in a graph, he decides the next node
to visit according to weight and type of each edge. The path, a
sequence of nodes, generated by him is regarded as the context of
these nodes (see Section 4.1). For instance, when a walker resides
at a node belonging to the sequence of device connections in day1
or day2 (Figure 2a), generated by graph construction, he would
seldom choose the node (device connection) in the sequence of
day3 because of low link weight. Likewise, when residing at a node
in sequence of day3, he would rarely reach the sequence of day1
or day2. Therefore, log2vec extracts paths involving nodes of day1
and day2 or individually day3.
log entries of the same user are connected in chronological order
(rule A), to map this relationship into a graph. We consider two
meta-attributes, subject and time.
The word2vec model is used to calculate vector of each node with
its paths (context). To be specific, these paths are taken as sentences
in natural language and processed by this model to learn vector
of each word (node) (see Section 4.2). This method preserves the
proximity between a node and its context [11, 14, 39, 54], meaning
that a node (log entry) and its neighbors (log entries having close
relationships with it) share similar embeddings (vectors). In the
above example, log entries (device connections) in day1 and day2
have been in the same path and thereby share similar vectors while
those in day3 possess different ones.
Further, log2vec improves the existing graph embedding [11,
14, 39, 54] (see Section 4.1.2, Section 4.1.3 and comparison with
baselines [11, 14] in Section 6.2). The novel version is capable of
determining importance of each relationship among log entries
based on attack scenarios and processing them differentially. In the
above example, log2vec determines that rule B is more important
than rule A and extracts more paths corresponding to the edge type
defined by rule B in random walk. After word2vec, vectors of device
connections in day3 tend to be similar due to many extracted paths
regarding rule B while they are all dissimilar to that of the logon
operation in day3 due to few ones regarding rule A. Therefore,
log2vec is biased to extract and represent logs’ relationships. This
example also shows how log2vec groups logons (benign) and device
connections (malicious) within a day into different clusters, instead
of viewing them as a daily sequence.
Detection Algorithm. Log2vec adopts a clustering algorithm to
analyze the above vectors and group benign operations (log entries)
and malicious ones into different clusters (Section 5.1). In the above
example, the clustering algorithm groups device operations in the
sequences of day1 and day2 into one cluster and those in day3’s
sequence into another one. After the clustering, we set a threshold
to identify malicious clusters. That is, clusters, whose sizes are
smaller than the threshold, are viewed as malicious (Section 5.2).
In summary, there are close relationships among benign opera-
tions (user’s typical behavior), the same as malicious ones. However,
there are fewer or even no correlations between them. Following
this observation, log2vec differentially extracts and represents these
relationships, to separate them into different clusters. Furthermore,
the number of malicious operations is small [2, 4] and thereby
smaller clusters tend to be malicious.
2.3 Adversarial Model
Adversarial model includes the following three attack scenarios
commonly in enterprises and governments.
The first scenario is that an insider employee misuses his author-
ity to perform malicious operations, such as accessing databases or
application servers, and then sabotaging system or stealing intel-
lectual property for personal benefits.
Second, a malicious insider obtains credentials of other legitimate
user through peeping or key logger, and utilizes this new identity
to seek confidential information or create chaos in the company.
These two scenarios belong to typical attacks of insider employees.
The third attack is that an APT actor compromises a host in the
system and from this host, he persistently compromises multiple
hosts for escalating his privilege and stealing confidential files.
3 GRAPH CONSTRUCTION
In Section 3.1, we formally define a log entry. Section 3.2 details
rules to construct heterogeneous graphs.
3.1 Definition
A log entry in Figure 2b is defined as a tuple involving five meta-
attributes , where sub is a collection of users;
Session 8C: Blockchain VICCS ’19, November 11–15, 2019, London, United Kingdom1780would be separated from benign ones. These six rules map the rela-
tionships in Figure 2c, into a graph. They mainly correlate user’s
operations of various types on multiple hosts across days.
Last, four rules (rule7∼rule10), corresponding to relationship
(3), are presented to consider how a user logs into/compromises a
host and sends confidential data to the outside. Specifically, they
construct user’s patterns of logon and web browsing operation.
Rules regarding logon operation, rule7 and rule8, consider how
these hosts are interactively accessed within intranet, such as an
instance in Figure 2d. Rules regarding web operation, rule9 and
rule10, focus on user’s browser usage via the Internet. The intranet
and Internet are the main intrusion sources, e.g. logging into a
host or driven-by download. Besides, Internet is also the main way
whereby confidential files are leaked. Note that another channel of
leaking data, device connection and file copy, has been considered
by the first six rules as the example in Section 2.2.
Log2vec aims at analyzing each user’s typical behavior and
thereby it constructs a heterogeneous graph for each user (sub).
The following rules are proposed as edge types in such a graph.
3.2.1 Constructing graph within a day. In enterprise, users own
various roles and conduct diverse operations. Therefore, multiple
rules are designed towards detecting their behavior. First, log2vec
considers meta-attributes, sub and T , and constructs a user’s daily
behavioral sequence, a common way to detect user’s daily behav-
ior [38, 41]. Meanwhile, this design is also used to reduce false
positives. For instance, administrators usually log into multiple
hosts per day for crash recovery. These operations should be corre-
lated into one sequence. Specifically, an administrator often logs
into different hosts and acts only a few operations on each one per
day. Due to irregular system problems, hosts that he logs into for
recovery are varied. If we divide his operations according to hosts,
a large number of benign operations from multiple hosts would
respectively locate in different parts of the graph and be mistakenly
identified as anomalies. Accordingly, we propose the rule:
Rule1 (edge1): log entries of the same day are connected in chrono-
logical order with the highest weight (value 1).
Second, log2vec constructs a user’s daily behavior on each host.
This design is based on the observation that other users, differ-
ent from administrators, generally conduct actions on specified
several hosts or only his own. Thereby, the unfamiliar hosts that
the user seldom or never logs into before are indicative of anoma-
lous logons [27, 50]. It deserves serious consideration on how to
distinguish hosts (H) in the graph. We present the following rule:
Rule2 (edge2): log entries of the same host and the same day are
connected in chronological order with the highest weight (value 1).
Some malicious log entries are related to multiple operation
types, A. For instance, a user searches for confidential files (file op-
eration) on others’ hosts and sends them to his home email (email
operation). These consecutive operations can be correlated by Rule2.
However, there exist attack actions regarding only one type. For
instance, a user visits a website and uploads confidential files. It is
appropriate to map them into the graph without other operation
types as noise, beneficial to the following precise detection. Mean-
while, they occur on a host. We thereby construct a user’s daily
behavior regarding each operation type on each host:
Figure 3: Graph construction. Log2vec utilizes ten rules to
construct a heterogeneous graph. R1 indicates rule1. R1∼R3
correspond to causal and sequential relationships within
a day. They construct user’s daily behavioral sequences.
R4∼R6 respectively bridge these daily sequences based on
logical relationships among days. R7 and R9 respectively
construct user’s logon and web browsing behavioral se-
quences. R8 and R10 bridge them following logical relation-
ships among objects. Diamonds in the heterogeneous graph
are suspicious log entries.
obj is a collection of objects, such as file, removable storage de-
vice and website; A is a collection of operation types, such as file
operation and browser usage; T is a collection of time and H is a
collection of hosts, such as computer or server.
In addition, sub, obj, A and H have their own sets of attributes.
For instance, a user writes a file in a server. The attributes of sub
involve role (e.g. system administrator) and organizational unit that
he is affiliated to (e.g. department of field service). The attributes of
obj may include file type and size. The attribute of H is function of
server (e.g. file server). For a logon operation, the attributes of A
include authentication type, such as Kerberos or NT LAN Manager
(NTLM), and logon type (e.g. interactive or network). Note that a
logon operation generally comprises a source host and a destination
one. In particular, log2vec regards the latter one as obj and the
former one as H. In other words, it treats a logon operation in the
following way, a user (sub) logs in to (A) a destination host (obj) in
a source one (H), just as a user writes a file in a server.
3.2 Rules of Graph Construction
Relationships involve three categories: (1) causal and sequential
relationships within a day; (2) logical relationships among days; (3)
logical relationships among objects. For each category, we propose
a few rules based on different combinations of meta-attributes.
As shown in Figure 3, we first present rule1∼rule3 to concatenate
log entries within a day into sequences, corresponding to relation-
ship (1). These three rules model user’s behavior from different
aspects, e.g. day, host and operation type. Through this design, rare
operations, which the user acts in an unfamiliar host, or belong to
the operation type seldom acted, are isolated in the graph. We then
propose rule4∼rule6 to separately bridge these daily sequences
based on relationship (2). The anomalous behavioral sequences
R1R2R3R4R5R6R7R9R8R10Audit LogsGraph RulesHeterogeneous GraphSession 8C: Blockchain VICCS ’19, November 11–15, 2019, London, United Kingdom1781Rule4 (edge4): daily log sequences are connected and their weights
are positively related to similarity of the numbers of log entries that
they contain.
This rule involves sub and T . Figure 4b shows an instance. The
connection mode is that the first/last log entry of each sequence
is connected to those of the others. As previously mentioned, the
three rules (rule4∼rule6) are proposed to closely link daily log se-
quences, reflecting user’s regular behavior (close to the average of
log entries), and separate malicious ones (far exceeding the average).
Therefore, this connection satisfies our demand. How to partition