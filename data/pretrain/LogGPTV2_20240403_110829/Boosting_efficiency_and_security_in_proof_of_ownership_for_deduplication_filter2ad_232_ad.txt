growth rates of the two schemes are now markedly diﬀer-
ent: b-POW grows linearly with the input ﬁle size whereas
s-POW1 is asymptotically constant. In addition, the inﬂu-
ence of the parameter K starts to become visible.
5.3 Server-Side
At the server-side, we have identiﬁed two main phases:
the initialization phase and the regular execution phase. The
initialization phase corresponds to the ﬁrst upload of the ﬁle;
in both schemes, this phase starts with the computation of
a hash digest of the ﬁle. Then comes the reduction and
mixing phases for b-POW, or Algorithm 1 (with n set to
10000) for our scheme. The implementation of Algorithm 1
has been optimized by pre-computing all bit position indexes
at once (for all n pre-computed challenges) and by sorting
them before performing the ﬁle access operations to fetch the
corresponding bits. This optimization allows us to only scan
the ﬁle at most once, thus avoiding the performance penalty
associated with random, non-sequential ﬁle accesses.
The regular execution phase includes the operations that
have to be executed by the server upon each interaction with
the client. In b-POW, this phase requires veriﬁcation of the
correctness of the sibling path in the computed Merkle tree
for a super-logarithmic number of leaves (we have picked
this number to be 20 as in [9]). In contrast, in our scheme,
if we factor out the table lookup required to retrieve (from
the received ﬁle index) the correct data structure holding
the state for the given ﬁle, our protocol only needs to verify
the equality of two short bit strings. The related overhead
is therefore negligible. However, our scheme also requires
regular re-executions of Algorithm 1 to pre-compute new
challenges: we will therefore include this in the regular exe-
cution phase.
Figure 5 shows the performance of the initialization phase:
the cost of b-POW grows – as explained in the previous sec-
tion – with the same rate as the cost of reading the entire
ﬁle. Our scheme exhibits an essentially constant computa-
tional cost up to a certain point, and a cost similar to that
of b-POW (linear with the cost of reading the entire ﬁle)
from that point on. The reason for this is that the over-
head of generating the n · K challenges, sorting them and
maintaining the data structure with all bit vectors is con-
stant: for small ﬁles, this overhead is higher than the cost
of reading the entire ﬁle and thus prevails. After the input
ﬁle has reached a critical size however, the cost of reading
the ﬁle becomes dominant. The reason for this asymptotic
behaviour is that with high probability, reading n · K bit
positions in the ﬁle requires fetching most data blocks of the
ﬁle, which is roughly equivalent to reading the entire ﬁle.
Figure 6 compares the performance of 10000 repetitions
of the regular execution phase for both schemes. b-POW
exhibits an essentially constant computational cost as the
number of leaves of the Merkle tree is relatively low and
does not grow past 64 MiB. The computational costs for
this phase of our scheme are the same as those shown in
Figure 5 minus the hash computation which is no longer re-
quired. We would like to emphasize however that Figure 6
shows a comparison between the on-line computation re-
quired by the veriﬁcation phase of b-POW and the oﬀ-line
computation required to generate the challenges: the for-
mer requires readily available computation power regardless
of the load of the system (since delaying client requests is not
acceptable); the latter is a computation that can be carried
out when the system load is low.
 1e+07 1e+08 1e+09 1e+10 1e+11 1e+12 1 10 100 1000Clock cyclesFile size (in MiB)b-POWs-POW, K = 183s-POW, K = 366s-POW, K = 915s-POW, K = 1830 1e+07 1e+08 1e+09 1e+10 1e+11 1e+12 1 10 100 1000Clock cyclesFile size (in MiB)b-POWs-POW1, K = 183s-POW1, K = 366s-POW1, K = 915s-POW1, K = 1830Figure 5: Comparison of the running time of the
server initialization phase of b-POW with that of s-
POW for diﬀerent values of K as the input ﬁle size
grows.
Figure 6: Comparison of the running time of the
server regular execution phase of b-POW with that
of s-POW for diﬀerent values of K as the input ﬁle
size grows.
6. COMPARISON AND DISCUSSION
In light of the analysis performed in the previous section
we are now ready to compare the state of the art solution
and our proposals. Table 1 compares b-POW, s-POW and
s-POW1 in terms of computational cost, I/O, storage and
bandwidth requirements; we omitted s-POW2 from the com-
parison as it has the same asymptotic costs as s-POW1.
On the client-side s-POW and s-POW1 are far less de-
manding than b-POW from both the computational and the
I/O perspective. This is a highly desirable characteristic, as
the end user will receive a better service. Such lower de-
mands on the client-side are compensated by an increase of
the footprint on the server-side. However the design of the
scheme allows the server to distribute computation and I/O
over time, and to carry them out in moments of low system
load.
From a computational perspective on the client-side, both
b-POW and s-POW are dominated by the cost of calculat-
ing the hash of the ﬁle whereas s-POW1 has a constant cost
that only depends on the security parameter K, and is inde-
pendent of the input ﬁle size. Similar considerations can be
made when investigating the client-side I/O requirements.
On the server-side we have made separate considerations
for the initialization and for the regular execution phase.
In the initialization phase, b-POW and s-POW are once
more dominated by the computational and I/O cost of the
hash calculation, whereas s-POW1 only requires the pre-
computation of n challenges. The regular execution phase
is particularly cheap for b-POW as no I/O and only con-
stant computation are required. s-POW and s-POW1 re-
quire regularly replenishments of the stock of precomputed
challenges. However, this operation can be performed oﬄine
in moments of low system load. Furthermore, ﬁles are often
regularly read at the server-side as part of standard manage-
ment tasks anyway (e.g. periodic integrity checks, backup,
replication); in this case, the I/O cost of the response pre-
computation phase, which is by far the predominant cost,
can be factored out.
As for server-side storage, b-POW requires only the root
of the Merkle tree to be stored, whereas s-POW and s-POW1
require storing the pre-computed challenges. We emphasize
however that the number of responses to be pre-computed is
a tunable parameter. Furthermore, the size of the responses
is independent of the input ﬁle size – usually only a negligible
fraction of the latter. For instance, storing 1000 responses,
each 1830 bits long, would require less than 230 KiB.
Finally, the b-POW scheme requires the exchange of a
super-logarithmic number of sibling paths of the Merkle tree
between client and server, whereas s-POW and s-POW1 only
require the exchange of a k-bit string.
ALGORITHM 6: Changes to s-POW to obtain s-POW3.
C : upon upload of ﬁle f do
if h ← H(f ile) available for f then
d ← h;
else
d ← H(f ile);
if f.size < t1 then
else if t1 ≤ f.size < t2 then
else
invoke Algorithm 2 on input f ile and spub to get d;
d ← f.size
end
end
send to SRV a store ﬁle request with d, including the
indexing mechanism used;
end
6.1 Putting it All Together: s-POW3
Based on the analysis of each version of our scheme, we
now suggest a further variant combining the diﬀerent opti-
mizations presented to obtain the best performance. The
rationale behind this last optimization is based on the fol-
lowing considerations: i) the cost of computing a standard
cryptographic hash (e.g. SHA-1) for small ﬁles is negligible;
ii) based on the ﬁle size distribution in common datasets,
and on the distribution of ﬁles with the same size, the ﬁle
size is a good indexing function for ﬁle sizes of 1 MiB and
more; iii) often the hash digest of a ﬁle is already available at
the client-side; for instance, several peer-to-peer ﬁle sharing
clients compute and/or store the hash of downloaded ﬁles.
Algorithm 6 shows the changes required in Algorithm 3 to
obtain s-POW3. In particular, the server always accepts a
digest from the client if available, and uses it as the lookup
key (this requires the server to compute the digest of new
 1e+08 1e+09 1e+10 1e+11 1e+12 1 10 100 1000Clock cyclesFile size (in MiB)b-POWs-POW, K = 183s-POW, K = 366s-POW, K = 915s-POW, K = 1830 1e+09 1e+10 1e+11 1e+12 1 10 100 1000Clock cyclesFile size (in MiB)b-POWs-POW, K = 183s-POW, K = 366s-POW, K = 915s-POW, K = 1830b-POW
O(m) hash
O(m)
s-POW
O(m) hash
O(m)
Client-side computation
Client-side I/O
Server-side computation (initialization phase)
Server-side computation (regular execution phase)
Server-side I/O (initialization phase)
Server-side I/O (regular execution phase)
Server-side storage
Bandwidth
a For the precomputation of n challenges.
b For the generation of n challenges; if n is suﬃciently large, it is bounded by O(m).
O(m)
O(nk)b
O(nk)a
O(k)
O(1)
O(m)
O(m) hash
0
O(1)
O(k log k)
O(m) hash
O(nk) PRNGa
O(nk) PRNGa O(nk) PRNGa
s-POW1
O(k) PRNG
O(k)
O(nk)b
O(nk)b
O(nk)a
O(k)
Table 1: Performance analysis of the schemes; m represents the input ﬁle size, k is the security parameter.
As explained in Section 4.1.1, the more precise formulation for O(k) in our scheme is shown in Equation 2.
ﬁles, but this is very likely performed for integrity protection
anyway). If not, two thresholds are set: for ﬁles smaller than
t1, the client has to compute the hash digest; for ﬁles in the
t1 to t2 range, the approach of s-POW1 is used; whereas for
very large ﬁles (size greater than t2), the ﬁle size (and thus
the approach of s-POW2) is used.
7. CONCLUSIONS
We have presented a suite of novel security protocols to
implement proof of ownership in a deduplication scenario.
Our scheme is provably secure and achieves better perfor-
mance than the state-of-the-art solution in the most sensi-
tive areas of client-side I/O and computation. On the server-
side, I/O and computation can be conveniently deferred to
moments of low system load. Note that the proposed solu-
tions are fully customizable in the system parameters. Fi-
nally, extensive simulation results support our ﬁndings.
Acknowledgments
The authors would like to thank Geoﬀ Kuenning and John
Douceur for their help in accessing statistics on ﬁle sizes;
Alexandra Shulman-Peleg and Christian Cachin for their in-
sightful feedback; Charlotte Bolliger for her precious correc-
tions.
Roberto Di Pietro has been partially supported by a Chair
of Excellence granted by University Carlos III, Madrid.
8. REFERENCES
[1] N. Agrawal, W. J. Bolosky, J. R. Douceur, and J. R.
Lorch. A ﬁve-year study of ﬁle-system metadata. In
FAST, pages 31–45, 2007.
[2] G. Ateniese, R. C. Burns, R. Curtmola, J. Herring,
O. Khan, L. Kissner, Z. N. J. Peterson, and D. Song.
Remote data checking using provable data possession.
ACM Trans. Inf. Syst. Secur., 14(1), 2011.
[3] G. Ateniese, R. C. Burns, R. Curtmola, J. Herring,
L. Kissner, Z. N. J. Peterson, and D. X. Song.
Provable data possession at untrusted stores. In ACM
Conference on Computer and Communications
Security.
[4] G. Ateniese, R. Di Pietro, L. V. Mancini, and
G. Tsudik. Scalable and eﬃcient provable data
possession. In Proceedings of the 4th international
conference on Security and privacy in communication
netowrks, SecureComm ’08, 2008.
[5] J. R. Douceur, A. Adya, W. J. Bolosky, D. Simon, and
M. Theimer. Reclaiming space from duplicate ﬁles in a
serverless distributed ﬁle system. In ICDCS, pages
617–624, 2002.
[6] driverdan. dropship - dropbox api utilities.
https://github.com/driverdan/dropship, 2011.
[7] C. Erway, A. K¨up¸c¨u, C. Papamanthou, and
R. Tamassia. Dynamic provable data possession. In
Proceedings of the 16th ACM conference on Computer
and communications security, CCS ’09, 2009.
[8] K. M. Evans and G. H. Kuenning. A study of
irregularities in ﬁle-size distributions. In In
International Symposium on Performance Evaluation
of Computer and Telecommunication Systems
(SPECTS ˆa ˘A ´Z02, 2002.
[9] S. Halevi, D. Harnik, B. Pinkas, and
A. Shulman-Peleg. Proofs of ownership in remote
storage systems. In ACM Conference on Computer
and Communications Security, pages 491–500, 2011.
[10] D. Harnik, B. Pinkas, and A. Shulman-Peleg. Side
channels in cloud services: Deduplication in cloud
storage. IEEE Security & Privacy, 8(6), 2010.
[11] A. Juels and B. S. K. Jr. Pors: proofs of retrievability
for large ﬁles. In ACM Conference on Computer and
Communications Security, pages 584–597, 2007.
[12] R. Lewand. Cryptological mathematics. Classroom
resource materials. Mathematical Association of
America, 2000.
[13] M. Matsumoto and T. Nishimura. Mersenne twister:
A 623-dimensionally equidistributed uniform
pseudo-random number generator. ACM Trans.
Model. Comput. Simul., 1998.
[14] R. C. Merkle. A digital signature based on a
conventional encryption function. In CRYPTO, pages
369–378, 1987.
[15] M. Saito and M. Matsumoto. Simd-oriented fast
mersenne twister: A 128-bit pseudorandom number
generator. In In Monte Carlo and Quasi-Monte Carlo
Methods 2006. Springer-Verlag, 2007.
[16] M. W. Storer, K. M. Greenan, D. D. E. Long, and
E. L. Miller. Secure data deduplication. In StorageSS,
pages 1–10, 2008.