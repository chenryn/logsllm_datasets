6). Thus, it respects the social relations established by users.
Finally, we point out that with the list ID corresponding to the
group, the client can retrieve the user IDs on that list from Twit-
ter if the original whisper sender has made the list public. If the
list is private, the recipient’s response can only be received by the
original sender. In the future, we plan to permit Twitsper users
to modify the list associated with a particular whisper in order to
enable inclusion of new users in the private group communication
or removal of recipients of the original whisper from future replies;
this can be easily done by adding/removing entries on Twitter lists.
415
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
F
D
C
512 threads, group size 16
1024 threads, group size 16
 1
 10
 100
 1000
Average Write Times Per Msg(ms)
(a) DB write time
 1
 0.995
 0.99
 0.985
 0.98
 0.975
 0.97
 0.965
 0.96
 0.955
 0.95
512 threads, group size 16
1024 threads, group size 16
 1
 10
 100
Average Read Times Per Msg(ms) 
(b) DB read time
.
l
i
t
U
U
P
C
r
e
v
r
e
S
%
.
g
v
A
120
100
80
60
40
20
0
Twitsper: 16 clients
Twitsper: 128 clients
TCM: 16 clients
TCM: 128 clients
Twitsper: 128 clients
Twitsper: 16 clients
TCM: 128 clients
TCM: 16 clients
)
s
m
(
e
m
T
i
 1024
 512
 256
 128
 64
 32
 2
 4
 8
 16
 32
 2
 4
 8
 16
 32
Group Size
(a) CPU utilization
Group Size
(b) Service time per client
Figure 5: Database performance
Figure 6: Server Metrics
Server implementation details: Our server is equipped with
an Intel quad-core Nehalem processor, 24 GB of RAM, and one
7200 RPM 1 TB hard disk drive. The Twitsper server is imple-
mented as a multi-threaded Java program. The main thread accepts
incoming connections and assigns a worker thread, chosen from a
thread pool, to service each valid API call. The server stores whis-
per mappings in a MySQL database. In order to ensure that writing
to the database does not become a bottleneck we have multiple con-
nections to the database; we observed that without this, the server
performance was affected. These connections are used by worker
threads in a round-robin schedule. Note that our server does not
store any personal information or credentials of any user. The ﬂow
of information in case of a tweet (public) or a Direct Message re-
mains unchanged. Only in the case of a whisper does the use of our
system become necessary. The contents of a whisper are never sent
to our server; only encrypted metadata is sent as discussed earlier.
This ensures that the server can never “overhear” conversations be-
tween users or derive user-speciﬁc information unless it has either
a user’s password, which, with Twitsper, is never transmitted.
Client implementation details: Our client was written for An-
droid OS v1.6 and was tested on the Android emulator as well as on
three types of Android phones (Android G1 dev, Motorola Droid X,
and HTC Hero). We use the freely available twitter4j package to ac-
cess the Twitter API. The client is also multi-threaded and separates
the UI (user-interface) thread from the processing, the network, and
disk I/O threads. This ensures a seamless experience to the user
without causing the screen to “freeze” when the client performs
disk or network I/O. We proﬁled the power consumption of our im-
plementation to identify inefﬁciencies and iteratively improved the
relevant code. These iterative reﬁnements helped us decrease the
dependence on the network by caching frequently retrieved user
proﬁle images, while maintaining a thread pool rather than the fork
and forget model adopted by most open source implementations of
other Twitter clients, so as to not over-commit resources.
When the Twitsper server is unavailable, we cache whisper
mappings on the client and piggyback this data with future inter-
actions with the server. On the other hand, recipients of whis-
pers interpret them as Direct Messages and cannot reply back to
the group until the server is again reachable. In future versions of
Twitsper, we will enable recipients to directly query the client
of the original sender if Twitsper’s server is unavailable.
We color code tweets, Direct Messages and whispers, while main-
taining a simple and interactive UI. Example screen shots from our
Twitsper client are shown in Figure 4. Our client application is
freely available on the Android market, and to date, our Twitsper
Android application has been downloaded by over 1000 users.
8. EVALUATION
Next we present our evaluation of Twitsper. For the purposes
of benchmarking, we also implement a version of Twitsper
wherein a client posts a whisper by transmitting the message to the
416
Twitsper server, which in turn posts Direct Messages to all the
recipients on the client’s behalf. Though, as previously acknowl-
edged, this design clearly violates our design goal of users not
having to trust Twitsper’s server, we use this thin client model
(TCM) (we refer to our default implementation as the fat client
model or Twitsper itself) as a benchmark to compare against.
One primary motivation for using TCM as a point of comparison
is that it can reduce the power consumption on phones (since bat-
tery drainage is a key issue on these devices). We also compare
Twitsper’s energy consumption on a smartphone with that of a
popular Twitter client to demonstrate its energy thriftiness.
Server-side results: First, we stress test our server by increas-
ing the rate of connections it has to handle. In this experiment, we
use one or more clients to establish connections and send dummy
metadata to our server. All clients and the server were on the same
local network and thus, network bandwidth was not the constrain-
ing factor. We monitored CPU utilization, disk I/O, and network
bandwidth with Ganglia [6] and iostat to detect bottlenecks. We
vary the target group size of whispers as well as the number of si-
multaneous connections to the server.
Disk. In Figure 5b, we plot the time taken by each thread to read
information relevant to a message from the database (we preloaded
the database with 10 million entries to emulate server state after
widespread adoption); Figure 5a depicts the CDFs of the write
times to the database. We see that as the number of clients in-
crease, so do the database write times, but not the read times. Thus,
as the system scales, the bottleneck is likely going to be the I/O for
writing to the disk.
CPU. Next, we compare the server performance of TCM and
Twitsper. We will refer to the version of the server which works
in tandem with Twitsper, and handles only whisper metadata,
as the Twitsper server. The TCM server must, in addition, han-
dle the actual sending of whispers to their recipients. It is to be
expected that the overhead of the TCM server would increase the
computational power needed to service each client. Figures 6a
and 6b show the average CPU utilization and user service time,
respectively, for each server version. We see in Figure 6a that
the Twitsper server has a higher CPU utilization than the TCM
server. This is because the TCM server spends more idle time (Fig-
ure 6b) while servicing each client since it needs to wait on com-
munications with Twitter. So even though more CPU resources are
being spent per client with the TCM server, the average CPU uti-
lization is lower.
Another interesting feature noted from these graphs is that cer-
tain increases in group size cause the server to more than double
its service time. These sharp increases in service time in Figure 6b
have corresponding drops in CPU utilization in Figure 6a. This
is due to our server’s disk writes being the throughput bottleneck.
Since in each test we either double the number of client connections
or the group size, we would expect a CPU bottleneck to manifest
itself with drastic service time increases (of ≈ 200%). Instead, the
128 KB
32 KB
8 KB
2 KB
512 B
)
s
e
t
y
B
(
O
I
r
e
v
r
e
S
Bytes Out per TCM
Bytes In per TCM
Bytes In per Twitsper
Bytes Out per Twitsper
n
I
c
e
S
/
s
e
t
y
B
r
e
v
r
e
S
32 MB
8 MB
2 MB
512 KB
128 KB
32 KB
8 KB
128 TCM
16 TCM
128 Twitsper
16 Twitsper
t
u
O
c
e
S
/
s
e
t
y
B
r
e
v
r
e
S
32 MB
8 MB
2 MB
512 KB
128 KB
32 KB
8 KB
 2
 4
 8
 16
 32
 64
 128
Group Size
 2
 4
 8
 16
 32
 2
 4
 8
 16
 32
Group Size
Group Size