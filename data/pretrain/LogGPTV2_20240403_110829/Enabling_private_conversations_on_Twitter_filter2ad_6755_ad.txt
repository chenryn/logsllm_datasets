### Respecting Social Relations and List Management

Thus, the system respects the social relations established by users. With the list ID corresponding to a group, the client can retrieve the user IDs on that list from Twitter if the original whisper sender has made the list public. If the list is private, the recipient's response can only be received by the original sender. In the future, we plan to allow Twitsper users to modify the list associated with a particular whisper, enabling the inclusion of new users in the private group communication or the removal of recipients from future replies. This can be easily done by adding or removing entries on Twitter lists.

### Database Performance

#### Figure 5: Database Performance
- **(a) DB Write Time**: 
  - The graph shows the average write times per message (in milliseconds) for different thread counts and group sizes.
  - As the number of clients increases, the database write times increase, but the read times remain relatively stable.
  - This indicates that as the system scales, the bottleneck is likely to be the I/O for writing to the disk.

- **(b) DB Read Time**:
  - The graph shows the average read times per message (in milliseconds) for different thread counts and group sizes.
  - The read times remain consistent, suggesting that reading from the database is not a significant bottleneck.

#### Figure 6: Server Metrics
- **(a) CPU Utilization**:
  - The graph compares the CPU utilization for different group sizes and thread counts.
  - The Twitsper server has higher CPU utilization compared to the TCM server, as the TCM server spends more idle time waiting for communications with Twitter.

- **(b) Service Time per Client**:
  - The graph shows the service time per client for different group sizes and thread counts.
  - Certain increases in group size cause the server to more than double its service time, which is due to the disk writes being the throughput bottleneck.

### Server Implementation Details

Our server is equipped with an Intel quad-core Nehalem processor, 24 GB of RAM, and a 7200 RPM 1 TB hard disk drive. The Twitsper server is implemented as a multi-threaded Java program. The main thread accepts incoming connections and assigns a worker thread from a thread pool to service each valid API call. The server stores whisper mappings in a MySQL database. To ensure that writing to the database does not become a bottleneck, multiple connections to the database are used, with worker threads accessing them in a round-robin schedule. The server does not store any personal information or credentials of any user. The flow of information for tweets (public) or Direct Messages remains unchanged. Only in the case of a whisper does the use of our system become necessary. The contents of a whisper are never sent to our server; only encrypted metadata is transmitted, ensuring that the server can never "overhear" conversations between users unless it has a user’s password, which is never transmitted.

### Client Implementation Details

Our client was written for Android OS v1.6 and tested on the Android emulator and three types of Android phones (Android G1 dev, Motorola Droid X, and HTC Hero). We use the freely available twitter4j package to access the Twitter API. The client is multi-threaded, separating the UI (user-interface) thread from the processing, network, and disk I/O threads. This ensures a seamless experience without causing the screen to "freeze" during disk or network I/O. We profiled the power consumption to identify inefficiencies and iteratively improved the relevant code, decreasing the dependence on the network by caching frequently retrieved user profile images and maintaining a thread pool rather than the fork-and-forget model adopted by most open-source implementations of other Twitter clients, so as not to over-commit resources.

When the Twitsper server is unavailable, we cache whisper mappings on the client and piggyback this data with future interactions with the server. Recipients of whispers interpret them as Direct Messages and cannot reply back to the group until the server is reachable again. In future versions of Twitsper, we will enable recipients to directly query the client of the original sender if the Twitsper server is unavailable.

We color-code tweets, Direct Messages, and whispers while maintaining a simple and interactive UI. Example screenshots from our Twitsper client are shown in Figure 4. Our client application is freely available on the Android market and has been downloaded by over 1000 users.

### Evaluation

For benchmarking, we also implemented a version of Twitsper where a client posts a whisper by transmitting the message to the Twitsper server, which then posts Direct Messages to all recipients on the client’s behalf. Although this design violates our goal of users not having to trust the Twitsper server, we use this thin client model (TCM) as a benchmark to compare against. One primary motivation for using TCM as a point of comparison is that it can reduce power consumption on phones, as battery drainage is a key issue on these devices. We also compare Twitsper’s energy consumption on a smartphone with that of a popular Twitter client to demonstrate its energy efficiency.

### Server-Side Results

First, we stress-tested our server by increasing the rate of connections it had to handle. In this experiment, we used one or more clients to establish connections and send dummy metadata to our server. All clients and the server were on the same local network, so network bandwidth was not the constraining factor. We monitored CPU utilization, disk I/O, and network bandwidth with Ganglia [6] and iostat to detect bottlenecks. We varied the target group size of whispers and the number of simultaneous connections to the server.

- **Disk I/O**:
  - Figure 5b shows the time taken by each thread to read information relevant to a message from the database (preloaded with 10 million entries to emulate server state after widespread adoption).
  - Figure 5a depicts the CDFs of the write times to the database.
  - As the number of clients increases, the database write times increase, but the read times remain stable, indicating that the bottleneck is likely to be the I/O for writing to the disk.

- **CPU Utilization**:
  - Figures 6a and 6b show the average CPU utilization and user service time, respectively, for each server version.
  - The Twitsper server has higher CPU utilization than the TCM server because the TCM server spends more idle time waiting for communications with Twitter.
  - Even though more CPU resources are spent per client with the TCM server, the average CPU utilization is lower.

- **Service Time**:
  - Certain increases in group size cause the server to more than double its service time, as seen in Figure 6b, with corresponding drops in CPU utilization in Figure 6a.
  - This is due to the disk writes being the throughput bottleneck. Since in each test we either double the number of client connections or the group size, we would expect a CPU bottleneck to manifest itself with drastic service time increases (of ≈ 200%). Instead, the observed increases in service time are due to the disk I/O bottleneck.