4:
5:
end for
6:
return C⇤
FIFO
7:
8: end procedure
Q2
. Set of extended coﬂows to be returned
. Extend coﬂow and add
FIFO
Q1
C2’s pioneer(s) with C1
9: procedure INTERCOFLOW(Extended Coﬂows C⇤, Coﬂow
.Q C
i sorted by arrival time
FIFO
FIFO
FIFO
for all i 2 [1,|QC|] do
i do
for all C⇤ 2 QC
end for
Q1
C2’s pioneer(s) with C1
(a) Late binding
10:
11:
12:
13:
14:
15: end procedure
Queues QC)
Q1
Q1
C2’s pioneer(s) with C1
C2’s pioneer(s) with C1
IntraCoﬂow(C⇤, QF )
(b) Intra-coﬂow prioritization
Figure 6: Impact of CODA’s design principles: (a) minimize strag-
glers by increasing pioneers; and (b) complete individual ﬂows fast
to handle leftover stragglers.
contributes to more than 10% CCT improvements in the pres-
ence of identiﬁcation errors, reducing the impact of errors by
more than 30%.
Design Principle 2: Intra-Coﬂow Prioritization Although
late binding helps, some ﬂows may still be misidentiﬁed. An
even more troublesome case is when C1 and C2 are clus-
tered as one coﬂow. To reduce the impact of errors in such
cases, we leverage Observation 2, which suggests that intra-
coﬂow prioritization can be more effective in error-tolerant
inter-coﬂow scheduling than in the absence of errors.
for all j 2 [1,|QF|] do
for all Flows f 2 C⇤T QF
21:
22:
23:
24: end procedure
16: procedure
end for
end for
Queues QF )
17:
18:
19:
20:
end for
priority coﬂow among all it belongs to
Update the residual bandwidth
INTRACOFLOW(Extended Coﬂow C⇤, Flow
j and not yet scheduled do
f.rate = Max-min fair share rate
Mark f as scheduled
. Binds f to the highest
To this end, we use per-ﬂow prioritization based on bytes
sent (similar to [16]) within each identiﬁed coﬂow without
any prior knowledge. This is especially effective for ﬂows
from small coﬂows that become stragglers with large coﬂows
– the most likely case given the prevalence of small coﬂows
[23]. For example, instead of the straggler of C1 taking the
same amount of time as longer ﬂows in C2 (Figure 4), it will
ﬁnish much earlier due to its small size (Figure 6b).
This scheme also takes effect in the reverse scenario – i.e.,
when a ﬂow from the larger coﬂow C2 becomes a pioneer
with smaller coﬂow C1. By preferring smaller ﬂows, C1’s
ﬂows are likely to ﬁnish earlier than the pioneer from C2.
Evaluation in §6.4 suggests that this principle brings up to
30% speedup for small coﬂows under low identiﬁcation ac-
curacy.
4.2.2 CODA Scheduler
Putting everything together, Algorithm 2 describes CODA’s
error-tolerant scheduler, which has the following three com-
ponents working cooperatively to minimize the impact of strag-
glers and pioneers during identiﬁcation as well as to perform
intra- and inter-coﬂow scheduling.
1. Late Binding In COFLOWEXTENSION(·), for each identi-
ﬁed coﬂow C, we create a corresponding extended coﬂow
C⇤ by extending its boundary by a diameter d (line 4).
Meaning, C⇤ further includes all ﬂows whose distances
to C are smaller than d.6 Note that a ﬂow might belong
25: procedure CODASCHEDULER(C, QC, QF , d)
26:
27:
28: end procedure
C⇤ = CoﬂowExtension(C, d)
InterCoﬂow(C⇤, QC)
to two or more extended coﬂows simultaneously after this
step. Later, the ﬂow belonging to multiple coﬂows will be
bound into the coﬂow with the highest priority when it is
scheduled for the ﬁrst time (line 20).
2. Inter-Coﬂow Prioritization In INTERCOFLOW(·), we adopt
D-CLAS [24] to prioritize across these extended coﬂows.
Basically, we dynamically place coﬂows into different coﬂow
queues of QC, and among the queues we enforce prioriti-
zation (line 10). Within each queue, we use FIFO among
coﬂows (line 11) so that a coﬂow will proceed until it reaches
queue threshold or completes. Using FIFO minimizes in-
terleaving between coﬂows in the same queue which min-
imizes CCTs.
3. Intra-Coﬂow Prioritization In INTRACOFLOW(·), we ap-
ply smallest-ﬁrst heuristic [16] to prioritize ﬂows within
each coﬂow. For this purpose, we implement multi-level
feedback queue scheduling (MLFQ) among ﬂow queues of
QF with exponentially increasing thresholds. Such scheme
prioritizes short ﬂows over larger ones with no prior knowl-
edge of ﬂow sizes [16]. Flows within each ﬂow queue use
max-min fairness (line 19).
Choice of Diameter d Diameter d reﬂects the tradeoff be-
6The distance between a ﬂow f and a coﬂow group C is deﬁned as the
smallest distance between f and ﬂows in C.
tween stragglers and pioneers. The optimal value of d in terms
of average CCT is closely related to the choice of radius ✏
in identiﬁcation, and it varies under different trafﬁc patterns.
There is no doubt that an extreme value of d (e.g., inﬁnity)
will lead to poor CCT. However, as mentioned earlier (§4.1),
the impact of stragglers is much bigger than that of pioneers,
making late binding beneﬁcial under a wide range of d (§6.4).
5 Implementation
In this section, we discuss the difﬁculties we have faced im-
plementing an existing coﬂow API in Hadoop 2.7 & Spark
1.6, and describe the implementation of CODA prototype.
5.1 Implementing Coﬂow API
In order to validate CODA, we implemented Aalo’s coﬂow
API in Hadoop 2.7 and Spark 1.6 to collect ground truth
coﬂow information. We faced several challenges, including
intrusive refactoring of framework code, interactions with third-
party libraries to collect coﬂow information, and Java byte-
code instrumentation to support non-blocking I/O APIs.
Coﬂow Information Collection Modern applications are
built on top of high-level abstractions such as Remote Pro-
cedure Call (RPC) or message passing, rather than directly
using low-level BSD socket APIs or equivalent coﬂow prim-
itives. As a result, matching the high-level coﬂow informa-
tion with the low-level ﬂow information requires refactoring
across multiple abstraction layers and third-party libraries.
In our implementation of collecting coﬂows in Hadoop,
which implements its own RPC submodule, we: (i) changed
the message formats of RPC requests and responses to em-
bed coﬂow information; (ii) modiﬁed the networking library
to associate individual TCP connections to the coﬂow infor-
mation in the RPC messages; and (iii) added an extra parsing
step to look up coﬂow information in binary messages, since
RPC messages are often serialized into byte stream before
being passed into the networking level.
To make things worse, there is no universal interface for
messaging or RPC. For example, unlike Hadoop, Spark uses
third-party libraries: Akka [1] and Netty [6]. Hence, collect-
ing coﬂow information in Spark almost doubled our effort.
Supporting Non-blocking I/O Current coﬂow implementa-
tions [23, 24] emulate blocking behavior in the user space, ef-
fectively forcing threads sending unscheduled ﬂows to sleep.
As a result, each CPU thread can send at most one ﬂow at any
time, which does not scale. To let each thread serve multiple
I/O operations, the common practice is to employ I/O mul-
tiplexing primitives provided by the OS (e.g., “select” and
“poll” in POSIX, and “IOCP” in Windows). Both Hadoop
and Spark uses “java.nio” for low-level non-blocking I/O.
Since many popular frameworks (including Hadoop and
Spark) are compiled against JVM, we seek an implementa-
tion that can support “java.nio” as well as a variety of third
party libraries on JVM. To this end, we employed Java byte-
code instrumentation – partially inspired by Trickle [10] –
to dynamically change the runtime behavior of these appli-
cations, collect coﬂow information, and intercept I/O oper-
ations based on scheduling results. Similar to the dynamic
linker in Trickle, during the JVM boot, our instrumentation
agent is pre-loaded. Upon the ﬁrst I/O operation, the agent
detects the loading of the original bytecode and modiﬁes it to
record job IDs in Hadoop and Spark shufﬂes at runtime, so
that coﬂow information can be collected.
5.2 CODA Prototype
Our prototype implements the master-slave architecture shown
in Figure 2. The error-tolerant scheduler runs in the master
with the information collected from CODA agents. The deci-
sions of the master are enforced by the agents. CODA agent
thus has the following two main functions: collection (of ﬂow
information) and enforcement (of scheduling decisions).
Flow information collection can be done with a kernel mod-
ule [16], which does not require any knowledge of how the
application is constructed, complying with our goal of appli-
cation transparency. In prototype implementation, we build
upon our coﬂow API integration, and reuse the same tech-
nique (bytecode instrumentation). Instead of job ID, we col-
lect the information for identiﬁcation: source and destination
IPs and ports, as well as the start time of ﬂow.
To enforce the scheduling decisions in each agent, we lever-
age Hierarchical Token Bucket (HTB) in tc for rate limiting.
More speciﬁcally, we use the two-level HTB: the leaf nodes
enforce per-ﬂow rates and the root node classiﬁes outgoing
packets to their corresponding leaf nodes.
Implementation Overhead of CODA Agent To measure
the CPU overheads of CODA agents, we saturated the NIC
of a Dell PowerEdge R320 server with 8GB of memory and
a quad-core Intel E5-1410 2.8GHz CPU with more than 100
ﬂows. The extra CPU overhead introduced is around 1% com-
pared with the case where CODA agent is not used. The
throughput remained the same in both cases.
6 Evaluation
Our evaluation seeks to answer the following 3 questions:
How does CODA Perform in Practice? Testbed experi-
ments (§6.2) with realistic workloads show that CODA achieves
over 95% accuracy in identiﬁcation, improves the average
and 95-th percentile CCT by 2.4⇥ and 5.1⇥ compared to
per-ﬂow fair sharing, and performs almost as well as Aalo
with prior coﬂow knowledge. Furthermore, CODA can scale
up to 40,000 agents with small performance loss.
How Effective is CODA’s Identiﬁcation? Large-scale trace-
driven simulations show that CODA achieves over 90% ac-
curacy under normal production workloads, and degrades to
around 60% under contrived challenging workloads. Further-
more, CODA’s distance metric learning (§3.2) is critical, con-
tributing 40% improvement on identiﬁcation accuracy; CODA’s
identiﬁcation speedup design (§3.3) is effective, providing
600⇥ and 5⇥ speedup over DBSCAN and R-DBSCAN re-
spectively with negligible accuracy loss (§6.3).
How Effective is CODA’s Error-Tolerant Scheduling? Un-
der normal workloads with over 90% identiﬁcation accuracy,
CODA effectively tolerates the errors and achieves compa-
rable CCT to Aalo (with prior coﬂow information), while
Per-Flow Fairness(Avg)
Aalo(Avg)
)
%
(
y
c
r
u
c
c
A
100
90
80
70
60
50
5
4
3
2
1
0
.
i
d
e
z
i
l
a
m
r
o
N
e
m
T
p
m
o
C
Precision Recall
(a) Accuracy
Per-Flow Fairness(95th)
Aalo(95th)
CODA
CCT
JCT
(b) CCT and JCT
Figure 8: [Testbed] CODA’s performance in terms of (a) identiﬁ-
cation accuracy, and (b) coﬂow and corresponding job completion
times (JCT) compared to Aalo and per-ﬂow fairness. The fraction
of JCT jobs spent in communication follows the same distribution
shown in Table 2 of Aalo [24].
Per-Flow Fairness
Aalo
CODA
Facebook
1
s
w
o
l
f
o
C
f
o
n
o
i
t
c
a
r
F
0.8
0.6
0.4
0.2
10−2
0
10−4
102
Inter−Coflow Arrival Time(Second)
(a) Inter coﬂow arrival time
100
1
0.8
0.6
0.4
0.2
i
i
s
t
n
o
p
e
m
T
d
e
p
m
a
S
l
f
o
n
o
i
t
c
a
r
F
Facebook
20
10
0
0
30
Number of Cuncurrent Coflows
(b) No. of concurrent coﬂows
Figure 7: Time-related characteristics of the workload.
outperforming per-ﬂow fair sharing by 2.7⇥. Under chal-
lenging scenarios, CODA degrades gradually from 1.3⇥ to
1.8⇥ compared to Aalo when accuracy decreases from 85%
to 56%, but still maintaining over 1.5⇥ better CCT over per-
ﬂow fair sharing. Moreover, CODA’s error-tolerant design
brings up to 1.16⇥ speedup in CCT, reducing the impact
of errors by 40%. Additionally, both late binding and intra-
coﬂow prioritization are indispensable to CODA– the former