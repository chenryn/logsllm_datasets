reducing or eliminating RTOmin, but also supporting ﬁne-
grained RTT measurements and kernel timers.
The TCP clock granularity in most popular operating
 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 32 64 128 256 512 1024 2048Goodput (Mbps)Number of ServersAverage Goodput VS # Servers (Block size = 80MB, buffer = 32KB, rtt = 20us)No RTOmin200us RTOmin1ms RTOmin 0 200 400 600 800 1000 0.35 0.4 0.45 0.5 0.55 0.6Mbpstime (seconds)Repeated Retransmissions, Backoff and Idle-timeInstanteous Link UtilizationFlow 503 Failed RetransmissionFlow 503 Successful Retransmission 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 32 64 128 256 512 1024 2048Goodput (Mbps)Number of ServersAverage Goodput VS # Servers (Block size = 80MB, buffer = 32KB, rtt = 20us)No RTOminw/2us Random Delayw/5us Random Delayw/Adaptive Delay308systems is on the order of milliseconds, as deﬁned by a global
counter updated by the kernel at a frequency “HZ”, where HZ
is typically 100, 250, or 1000. Linux, for example, updates
its “jiﬀy” timer 250 times per second, yielding a TCP clock
granularity of 4ms, with a conﬁguration option to update
1000 times per second for a 1ms granularity. More frequent
updates, as would be needed to achieve ﬁner granularity
timeouts, would impose a system-wide clock maintenance
overhead considered unacceptable by most.
Unfortunately, setting the RTOmin to 1 jiﬀy (the lowest
possible value) does not achieve RTO values of 1ms because
of the clock granularity. TCP measures RTTs in 1ms gran-
ularity at best, so both the smoothed RTT estimate and
RTT variance have a 1 jiﬀy (1ms) lower bound. Since the
standard RTO estimator sums the RTT estimate with 4x
the RTT variance, the lowest possible RTO value is 5 jiﬃes.
We experimentally validated this result by setting the clock
granularity to 1ms, setting RTOmin to 1ms, and observing
that TCP timeouts were a minimum of 5ms.
At a minimum possible RTOmin of 5ms in standard TCP
implementations, Figures 9 and 10 show that throughput
collapse is signiﬁcantly improved for small numbers of servers.
This simple change is a good ﬁrst step for use in the ﬁeld
today. However, application throughput is reduced by 35%
or more with just 16 concurrent senders. Next, we describe
how to achieve microsecond granularity RTO values in the
real world.
5.1 Linux high-resolution timers: hrtimers
High resolution timers were introduced into Linux kernel
version 2.6.18 and are still under active development [16].
They form the basis of the posix-timer and itimer user-
level timers, nanosleep, and a few other in-kernel operations,
including the update of the jiﬃes value.
The Generic Time of Day (GTOD) framework provides
the kernel and other applications with nanosecond resolu-
tion timekeeping using the CPU cycle counter on all modern
processors—our modiﬁcations use this framework to provide
ﬁne-grained measurements of round-trip-times. The hrtimer
implementation interfaces with the High Precision Event
Timer (HPET) hardware also available on modern chipsets
to achieve microsecond resolution event notiﬁcation in the
kernel. Speciﬁcally, the HPET is a programmable hardware
timer that consists of a free-running upcounter and several
comparators and registers, which modern operating systems
can set. When scheduling an event, the kernel sets a register
value to achieve a desired interrupt interval, and the com-
parator will signal a hardware interrupt when the upcounter
matches the register value. For example, the kernel may
request the HPET to interrupt once every 1ms to update the
jiﬀy counter, or it may set a timer for 50µs in the future for
a usleep(50) system call.
Implementing TCP timeouts using the hrtimer subsystem
could lead to increased interrupt overhead only if timeouts
are frequent; ﬂows that experience few losses do not incur
hrtimer-based interrupts because the retransmission timer
and HPET entry are reset for every ACK received. Our
preliminary evaluations using our implementation of TCP
timers using the hrtimer system have shown no noticeable
overhead: while serving as a sender in an incast workload, the
speed of a kernel build was about the same for both hrtimer
and normal TCP implementations. We also argue that during
incast workloads, a small overhead may be acceptable, as
Figure 9: On a 16 node cluster, our high-resolution
TCP timer modiﬁcations help eliminate incast col-
lapse. The jiﬀy-based implementation has a 5ms
lower bound on RT O, and achieves only 65%
throughput.
it removes the idle periods that prevent the server from
doing useful work to begin with, but we leave an extensive
evaluation of overhead for future work.
5.2 Modiﬁcations to the TCP Stack
The Linux TCP implementation requires three changes
to support microsecond timeouts using hrtimers: microsec-
ond resolution time accounting to track RTTs with greater
precision, redeﬁnition of TCP constants, and replacement of
low-resolution timers with hrtimers.
By default, the jiﬀy counter is used for tracking time.
To provide microsecond granularity accounting, we use the
GTOD framework to access the 64-bit nanosecond resolution
hardware clock wherever the jiﬃes time is traditionally used.
With the TCP timestamp option enabled, RTT estimates
are calculated based on the diﬀerence between the timestamp
option in an earlier packet and the corresponding ACK. We
convert the time from nanoseconds to microseconds and store
the value in the TCP timestamp option.2 This change can
be accomplished entirely on the sender—receivers already
echo back the value in the TCP timestamp option.
All timer constants previously deﬁned with respect to the
jiﬀy timer are converted to absolute values (e.g., 1ms instead
of 1 jiﬀy). Last, the TCP implementation must make use of
the hrtimer interface: we replace the standard timer objects
in the socket structure with the hrtimer structure, ensuring
that all subsequent calls to set, reset, or clear these timers
use the appropriate hrtimer functions.
We note that the changes required to TCP were relatively
minimal and non-invasive. The successful implementation
of ﬁne-grained retransmissions in TCP took two weeks for
a graduate student with little experience in kernel hacking
or prior exposure to Linux TCP source code. We are also
making a patch available for testing.3
2The lower wrap-around time – 232 microseconds or 4294
seconds – is still far greater than the maximum IP segment
lifetime (120-255 seconds)
3See http://www.pdl.cmu.edu/Incast/ for details
 0 100 200 300 400 500 600 700 800 900 1000 0 2 4 6 8 10 12 14 16Goodput (Mbps)Number of ServersNum Servers vs Goodput (Fixed Block = 1MB, buffer = 32KB (est.), Switch = Procurve)No RTOmin5ms RTOmin (Jiffy)200ms RTOmin (default)309network RTT suddenly jumps, and breaking the relationship
between the delayed acknowledgement timer and the RTO
values.
Spurious retransmissions: The most prominent study
of TCP retransmission by Allman and Paxson showed that a
high (by the standards of datacenter RTTs) RTOmin helped
avoid spurious retransmission in wide-area TCP transfers [2],
regardless of how good an estimator one used based on his-
torical RTT information. Intuition for why this is the case
comes from prior [24, 10] and subsequent [35] studies of Inter-
net delay changes. While most of the time, end-to-end delay
can be modeled as random samples from some distribution
(and therefore, can be predicted by the RTO estimator in
equation (1)), end-to-end delay consistently observes both
occasional, unpredictable delay spikes, as well as shifts in
the distribution. Such changes can be due to the sudden
introduction of cross-traﬃc, routing changes, or failures. As
a result, wide-area “packet delays [are] not mathematically
[or] operationally steady” [35], which conﬁrms the Allman
and Paxson observation that RTO estimation involves a fun-
damental tradeoﬀ between rapid retransmission and spurious
retransmissions.
Delayed Acknowledgements: The TCP delayed ACK
mechanism attempts to reduce the amount of ACK traﬃc by
having a receiver acknowledge only every other packet [7]. If
a single packet is received with none following, the receiver
will wait up to the delayed ACK timeout threshold before
sending an ACK.
Prior work showed that in cluster based systems with three
to ﬁve servers and with barrier-synchronized request work-
loads, the delayed ACK mechanism can act as a miniature
timeout, resulting in reduced, but not catastrophically low,
throughput during certain loss patterns [28].
The ﬁgure on the right shows how the combination of small
windows and delayed
ACKs can result in lower
throughput and slower
loss recovery. With de-
layed ACK disabled, the
ACK for packet 1 is sent
immediately after it is
received, enabling the
sender to grow its TCP
window and triggering
data-driven recovery for
packet 2. With delayed
ACKs enabled, the ﬁrst ACK is sent 40ms later, delaying
this recovery process.
While this delay is not as high as a full 200ms RTO, the
default delayed ACK minimum (40ms in Linux) is still large
compared to the RTTs in datacenters and results in low
throughput for three to ﬁve concurrent senders. Beyond ﬁve
senders, high packet loss results in 200ms retransmission
timeouts which mask the impact of delayed ACK-induced
link idle time.
Microsecond retransmission timeouts, however, have a
diﬀerent interaction with the delayed ACK mechanism. The
receiver’s delayed ACK timer should always ﬁre before the
sender’s retransmission timer ﬁres to prevent the sender
from timing out waiting for an ACK that is merely delayed.
Modern systems protect against this by setting the delayed
ACK timer to a value (40ms) that is safely under the RTOmin
(200ms).
Figure 10: For a 48-node cluster, providing TCP re-
transmissions in microseconds eliminates incast col-
lapse for up to 47 servers.
5.3 hrtimer Results
Figure 9 presents the achieved goodput as we increase
the number of servers N using various RTOmin values on a
Procurve 2848 switch. As before, the client issues requests
for 1MB data blocks striped over N servers, issuing the
next request once the previous data block has been received.
Using the default 200ms RTOmin, throughput plummets
beyond 8 concurrent senders. For a 5ms jiﬀy-based RTOmin,
throughput begins to drop at 8 servers to about 70% of link
capacity and slowly decreases thereafter. Last, our TCP
hrtimer implementation allowing microsecond RTO values
achieves the maximum achievable goodput for 16 concurrent
senders.
We verify these results on a second cluster consisting of 1
client and 47 servers connected to a single 48-port Force10
S50 switch (Figure 10). The microsecond RTO kernel is
again able to saturate throughput up to 47 servers. The 5ms
RTOmin jiﬀy-based conﬁguration obtained 70-80% through-
put, with an observable drop above 40 concurrent senders.
Overall, we ﬁnd that enabling microsecond RTO values
in TCP successfully avoids TCP incast collapse in two real-
world clusters for as many as 47 concurrent servers, and that
microsecond resolution is necessary to achieve full perfor-
mance.
6.
IMPLICATIONS OF FINE-GRAINED
TCP RETRANSMISSIONS
Eliminating RTOmin and enabling TCP retransmissions in
microseconds helps avoid TCP incast collapse. But proposing
ﬁne-grained retransmissions requires addressing the issue of
safety and generality: is an aggressive timeout appropriate for
use in general (i.e., in the wide area), or should it be limited to
the datacenter? Does it risk increased congestion or decreased
throughput because of spurious (incorrect) timeouts? In this
section, we discuss the implications of this change on wide-
area bulk transfers and on delayed acknowledgments.
6.1 Is it safe to eliminate RTOmin?
There are two possible complications of permitting much
smaller RTO values: spurious (incorrect) timeouts when the
 0 100 200 300 400 500 600 700 800 900 1000 0 5 10 15 20 25 30 35 40 45Goodput (Mbps)Number of ServersNum Servers vs Goodput (Fixed Block = 1MB, buffer = 64KB (est.), Switch = S50)No RTOmin5ms RTOmin (Jiffy)200ms RTOmin (default)1223ACK 1ACK 1ACK 1412ACK 1ACK 1345ACK 1ACK 1310Figure 11: A comparison of RTT distributions of
ﬂows collected over 3 days on the two conﬁgurations
show that both servers saw a similar distribution of
both short and long-RTT ﬂows.
Figure 12: The two conﬁgurations observed an iden-
tical throughput distribution for ﬂows. Only ﬂows
with throughput over 100 bits/s were considered.
As depicted to the right, a host with microsecond-granularity
retransmissions would
periodically experience
an unnecessary time-
out when communicat-
ing with unmodiﬁed
hosts where the RTO is
below 40ms (e.g., in the
datacenter and for short
ﬂows in the wide-area),
because the sender incor-
rectly assumes that a loss has occurred.
Given these consequences, there are good reasons to ask
whether eliminating RTOmin—basing timeouts solely upon
the Jacobson estimator and exponential backoﬀ—will harm
wide-area performance and datacenter environments with
clients using delayed ACK. In practice, these two potential
consequences are mitigated by newer TCP features and by the
limited circumstances in which they occur, as we explore in
the next two sections. We ﬁnd that eliminating the RTOmin
has little impact on bulk data transfer performance for wide-
area ﬂows, and that in the datacenter, delayed ACK causes
only a small, though noticeable drop in throughput when
the RT Omin is set below the delayed ACK threshold.
6.2 In the Wide Area
Aggressively lowering both the RTO and RTOmin shows
practical beneﬁts for datacenters. In this section, we inves-
tigate if reducing the RTOmin value to microseconds and
using ﬁner granularity timers is safe for wide area transfers.
We ﬁnd that the impact of spurious timeouts on long, bulk
data ﬂows is very low – within the margins of error – al-
lowing RTO to go into the microseconds without impairing
wide-area performance.
The major potential eﬀect of a spurious timeout is a loss of
performance: a ﬂow that experiences a timeout will reduce
its slow-start threshold (ssthresh) by half, its window to one
and attempt to rediscover link capacity. It is important to
understand that spurious timeouts do not endanger network
stability through increased congestion [2]. Spurious timeouts
occur not when the network path drops packets, but rather
when the path observes a sudden, higher delay. Because a
TCP sender backs-oﬀ on the amount of data it injects into
the network following this timeout, the eﬀect of a shorter
RTO on increased congestion is likely small. Therefore, we
analyze the performance of TCP ﬂows over the wide-area for
bulk data transfers.
Fortunately, algorithms to undo the eﬀects of spurious
timeouts have been both proposed [2, 21, 32] and, in the
case of F-RTO [32], adopted in the latest Linux implemen-
tations. The default F-RTO settings conservatively halve
the congestion window when a spurious timeout is detected
but remain in congestion avoidance mode, thus avoiding the
slow-start phase. Therefore, the impact of spurious timeouts
on throughput are now signiﬁcantly smaller than they were
10 years ago.
6.2.1 Experimental Setup and Results
We deployed two servers that diﬀer only in their imple-
mentation of the RTO values and granularity, one using the
default Linux 2.6.28 kernel with a 200ms RTOmin, and the
other using our modiﬁed hrtimer-enabled TCP stack with a
200µs RTOmin. We downloaded 12 torrent ﬁles consisting
of various Linux distributions and began seeding all content
from both machines on the same popular swarms for three
days. Each server uploaded over 30GB of data, and observed
around 70,000 ﬂows (with non-zero throughput) over the
course of three days. We ran tcpdump on each machine to
collect all uploaded traﬃc packet headers for later analysis.
The TCP RTO value is determined by the estimated RTT
value of each ﬂow. Other factors being equal, TCP through-
put tends to decrease with increased RTT. To compare RTO