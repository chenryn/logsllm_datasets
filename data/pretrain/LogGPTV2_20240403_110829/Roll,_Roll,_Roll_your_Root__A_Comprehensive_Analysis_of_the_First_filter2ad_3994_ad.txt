138 resolvers (H) we keep observing unusually high numbers of
DNSKEY queries for over an hour. They only return to their normal
behavior after a median of more than 39 hours. Only three resolvers
(I) continue sending unusually high numbers of queries throughout
the entire measurement period. The fact that more than 60% of the
resolvers get fixed within one hour is a strong sign that resolvers
in our data set are used actively and that operators noticed issues
during the rollover relatively quickly. We discuss resolvers that send
excessive numbers of DNSKEY queries in more detail in Section 4.3.2.
4.2.3 The User’s Perspective. From the analysis above, we cannot
gauge the actual impact on end users. During our measurements,
175 RIPE Atlas probes (1% of all vantage points) relied exclusively
on one of the bogus resolvers (set B in Table 6), thus were not able
to receive any valid response at some point after the rollover. More
than 70% of these probes, however, suffered problems only an hour
Upstream Resolvers
A Unique sources in RIPE Atlas data
B
C
D
E
F
G
H
I
↰ from A always secure before and bogus after
↰ from A always secure before and insecure after
↰ from B and C sending DNSKEY queries
↰ from D reach maximum DNSKEY queries after
↰ from E w. 1.5× DNSKEY queries after
↰ from F fixed within 1h
↰ from F fixed after 1h
↰ from F that did not get fixed
Table 6: Data of RIPE Atlas measurements.
Count
35,719
970
747
519
509
359
218
138
3
or less. 166 probes could rely on at least one other resolver to serve
their queries and were not affected by the failing resolver.
Other work [36] shows users move to public DNS providers in
case of issues with the resolver of their ISP. Therefore, we ana-
lyzed if vantage points change to the public resolvers of Google,
Cloudflare or OpenDNS. We found only two vantage points. One
of these used the resolver of the Irish ISP EIR. This ISP experi-
enced a well-publicized DNS outage [37] during the rollover, and
the DNS community speculated this outage was caused by EIR’s
resolvers failing validation. Using the RIPE Atlas measurements,
we identify the IP addresses of EIR’s resolvers. Then, we count how
many DNSKEY queries these resolvers send to A/J Root per day (see
Fig. 10). Starting from October 12th, queries increase, reaching a
peak one day after the roll and returning to normal after 3 days.
Keeping in mind that RIPE Atlas probes actively switched resolvers
at the same time, this is a strong sign that the outage of EIR was
indeed caused by validation errors. Note, Fig. 10 shows the number
of DNSKEY queries from EIR rising again after removal of KSK-2010.
We discuss this increase Section 4.3.2.
Key Takeaways During the Roll. We observed few resolvers with
serious problems. Where such problems occurred, they were solved
promptly by operators. Less than 0.01% of the resolvers we moni-
tored during the rollover experienced problems that lasted beyond
our observation window.
4.3 After the Roll
We now discuss what happened after the rollover, from the point
when all resolvers should have a DNSKEY RRset signed by KSK-2017,
to the removal of KSK-2010 from the root zone.
4.3.1 Revocation of KSK-2010. As discussed in Section 3.2, the
Root Sentinel standard (RFC 8509) was published too late to be
useful for the actual rollover. We can, however, study revocation
of KSK-2010 with resolvers that adopted this protocol. Using all
RIPE Atlas probes, we send out Root Sentinel queries from Au-
gust 2018 to August 2019. Fig. 11 shows the Root Sentinel signals
observed over this period. As the figure shows, overall, the number
of resolvers supporting Root Sentinel queries steadily increases to
2,419 resolvers in 720 ASs by the middle of August 2019. This is en-
couraging given the early stage of deployment of the protocol. After
the revocation of the old key (event V), the number of resolvers
with KSK-2010 drops to almost zero while the number of resolvers
with KSK-2017 keeps increasing. Interestingly, some 20 resolvers
continue to signal having KSK-2010 in their trust anchor store. This
implies either a manually configured trust anchor, or a failure in
RolloverRevocationRemoval0250005000075000100000125000Aug '18Sep '18Oct '18Nov '18Dec '18Jan '19Feb '19Mar '19Apr '19Queries per dayRolloverRevocationRemoval05001000150020002500Aug '18Sep '18Oct '18Nov '18Dec '18Jan '19Feb '19Mar '19Apr '19May '19Jun '19Jul '19Aug '19Number of resolversKSK−2010KSK−2017Roll, Roll, Roll your Root
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Figure 12: Top 9 ASs supporting Root Sentinel queries ob-
served through RIPE Atlas
Figure 13: DNSKEY queries to A/J Root after the rollover.
their RFC 5011 processing. Then, from the middle of June 2019,
KSK-2010 starts making a surprising comeback. We explain why
further down in Section 4.3.4.
As RIPE Atlas provides a limited view, we also used Luminati
to measure a total of 52,378 resolvers serving 589,928 exit nodes
— from 210 countries and 7,867 ASs — over a period of 14 days
from March 28th 2019. From these, we select resolvers on which
we were able to test all four combinations of Root Sentinel queries
(cf. Table 3). This leaves 21,563 resolvers, to which 385,520 exit nodes
sent queries at least once. We further split these into resolvers that
support Root Sentinel queries and ones that do not.5 We finally
determine which trust anchor(s) resolvers that support the Root
Sentinel signal as present in their trust store. The vast majority —
21,056 (97.63%) resolvers from 5,311 ASs — do not support RFC 8509.
These resolvers cover 330,891 (85.8%) exit nodes. Only 468 (2.2%)
resolvers from 164 ASs support Root Sentinel queries and have only
KSK-2017; these resolvers cover 33,266 (8.6%) exit nodes indicating
that a few large ASs support RFC 8509 queries, including Telenor
(Norway), Bezeq (Israel) and Meo (South Africa). We also note that
39 resolvers (0.19%) still signal they have KSK-2010 configured.
Finally, we compare our observations through RIPE Atlas and
Luminati. Fig. 12 shows the top 9 ASs with resolvers supporting
RFC 8509 in our RIPE Atlas measurements. Comparing this to Lu-
minati, we find that 43 resolvers from AS2119 (Telenor), 10 from
AS16276 (OVH), 10 from AS6830 (Liberty Global), and 2 from
AS7922 (Comcast), are observed in the same state through both
RIPE Atlas and Luminati. Fig. 12 also shows a surprising increase
of KSK-2010 from June 2019, we explain why in Section 4.3.4.
Increase in DNSKEY Queries. As mentioned at the end of Sec-
4.3.2
tion 4.2, we observed an increase in DNSKEY queries from certain
resolvers at various stages of the roll. We analyse this phenome-
non in more detail here, especially because of the sharp increase
in queries after the revocation of KSK-2010 to the extent that at
some point a worrying amount — up to 10% — of traffic to the root
consisted of DNSKEY queries.
We start by analyzing the total amount of DNSKEY queries to
the root. DNSSEC validators must regularly verify their locally
configured trust anchor(s) against the zone’s published DNSKEY
5Note: a resolver that supports RFC 8509 correctly will return a valid response to only
one of the two queries with the same key tag.
Figure 14: DNSKEY query increases for all root servers.
records. In other words: validators periodically issue DNSKEY queries
for the root zone. Due to the retry behavior of implementations, a
validator with an out-of-date trust anchor is likely to send more than
the normal amount of DNSKEY queries. This behavior was already
observed in 2009 — before the root zone was signed — during a KSK
rollover for an in-addr.arpa zone operated by RIPE. The group
investigating that incident called it “rollover and die” [38].
Just after the root KSK rollover on October 11th, 2018, root name
servers observed an increase in DNSKEY queries. Fig. 13 shows the
query rate for A/J Root. The increase was gradual, ramping up
over the course of two days as the DNSKEY RRset timed out from
resolver caches. Pre-rollover the rate was around 15 million queries
per day. Post-rollover it increased five-fold, to 75 million (①). An
even more dramatic increase occurred when KSK-2010 was revoked
(Event V in Fig. 2). Immediately after the revocation, A/J Root see a
sudden spike in DNSKEY queries (②), jumping from 75 million to over
200 million queries per day within 24 hours. The DNSKEY query rate
continued to climb over the following weeks and months, exceeding
one billion per day in March 2019 (③). At this point, DNSKEY queries
comprised 7% of the total traffic received at A/J Root. The final
phase of the rollover sees KSK-2010 removed from the root zone on
March 22nd, 2019. To everyone’s surprise, the DNSKEY query rate
dropped dramatically immediately after KSK-2010 was removed. As
Fig. 13 shows (④), the rate dropped and slowly crept back up to post-
rollover levels as seen in October, November, and December 2018.
Fig. 13 only shows data for A/J Root. To confirm similar increases
at other root servers, we use the RSSAC002 data (see Section 3.1).
The RSSAC002 data does not have a dataset specifically identifying
DNSKEY queries, however we can infer the presence of such queries
by examining the response size dataset. Fig. 14 shows the percent
of responses between 1232–1472 bytes as solid lines. The dashed
RolloverRevocationRemovalKSK−20100200400RolloverRevocationRemovalKSK−201702505007501000Aug '18Sep '18Oct '18Nov '18Dec '18Jan '19Feb '19Mar '19Apr '19May '19Jun '19Jul '19Aug '19AS13335AS15169AS16276AS2119AS37100AS42AS6830AS7342AS7922Number of resolversRolloverRevocationRemovalllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll3llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll40 M250 M500 M750 M1 000 M1 250 MAug '18Sep '18Oct '18Nov '18Dec '18Jan '19Feb '19Mar '19Apr '19Queries per dayZSKrolloverRFC 5011hold−downfor revocationKSK−2010revoked0.0000.0250.0500.0750.100Jan '19Feb '19Mar '19Fraction of trafficAA*BCDEFHIJJ*KLMIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Müller et al.
Figure 15: AS DNSKEY query patterns to A/J Root.
lines — marked A* and J* — are actual A/J Root traffic and show a
strong correlation. Not all root servers saw the same increase in
queries, but we currently lack sufficient information to explain this.
Deeper inspection of the A/J Root traffic shows vastly differing
DNSKEY query patterns on a per AS basis. Fig. 15 shows the aver-
age of multiple ASs whose DNSKEY queries exhibit distinct patterns
at different times throughout the rollover. Some ASs expressed a
systemic trend of increased DNSKEY queries post-rollover and even
higher rates post-revocation (ASs-A). Other ASs only exhibited an
increase in DNSKEY queries after the removal of KSK-2010 (ASs-B).
Likewise, some ASs show increased rates post-rollover until revo-
cation (ASs-D) and again after removal (ASs-C). To better profile
these resolvers, we issued version.bind queries to IP addresses
expressing the various behaviors. While the response rate was low
(4.3% of ±18K resolvers), the majority returned older versions of
BIND (45% BIND 9.9.x, 34% BIND 9.8.x, and 13% BIND 9.10.x).
Explaining the increase in DNSKEY queries. To find the cause of
the increased query rates, we studied traffic coming from individ-
ual, high-volume sources. Outreach efforts at a global DNS scale
are challenging, but we were able to contact multiple operators
willing to help diagnose the DNSKEY query increase. One operator
(a large French cloud hoster), stated their servers were running
BIND 9.8.2 on CentOS 6.7 and the logs contained large numbers
of validation errors. Another set of sources identified as sending
excessive DNSKEY queries to the root, came from 8 addresses in a
single subnet at a large midwestern university. Their staff quickly
identified a DNS lab exercise that had been left running inside vir-
tual machines (VMs). After shutting down the VMs, we confirmed
that the excess DNSKEY traffic had stopped. From the university’s
class instructions, we hypothesized that the DNSKEY query spikes
were the result of ISC’s BIND software running in a specific state:
(i) the DNSSEC managed keys did not contain KSK-2017, but did
contain KSK-2010; (ii) the dnssec-enable flag was set to false;
and (iii) the dnssec-validation flag was unset, leaving it in its
default state of yes.
To verify this hypothesis, we performed experiments to test for
bugs related to BIND’s behavior in the absence of a valid trust
anchor. We set up a BIND 9.11.5-P4 resolver (the oldest supported
release at the time), configuring it as per the university’s class
instructions. We also ensured that BIND’s managed keys file con-
tained only KSK-2010. Then, we ran 20 experiments in which we
started a fresh copy of BIND configured as specified above. In each
Figure 16: DNSKEY queries for root during experiments.
Figure 17: Time-normalized graph of experiments.
run, we sent ten sets of queries to BIND for test domains in seven
TLDs at 30-second intervals, recording DNSKEY queries sent by the
resolver, along with timestamps. Fig. 16 shows the results. Each
experiment start time was normalized to zero and overlayed in
Fig. 17, showing highly variable query patterns in each run (note
experiments 7, 13 and 17).
Both plots show wide variations in behavior of the resolver under
test. At times it behaves as expected, sending only a few DNSKEY
queries after initializing. At other times, the resolver seems stuck
in a state where every incoming request causes the resolver to send
out a flurry of DNSKEY queries.
From the analysis of events V and VI, and the corresponding
DNSKEY loads seen at the root (Fig. 13 and Fig. 14) we conclude there
are likely two different bugs causing the increase in queries. One
bug is likely the cause of the increase in DNSKEY queries shortly after
the rollover (event IV) and after KSK-2010 is removed (event VI).
Another bug is likely the cause of the extreme query loads seen
in Fig. 14, when KSK-2010 was present but with the revoke bit set.
We have reached out to the developers of BIND to confirm our
hypotheses, but have not received any feedback as of September
13th, 2019. What remains unclear is why operators have not noticed
this broken resolver behavior, as we expect these resolvers to return
SERVFAIL errors to every query. We speculate only one resolver in
a group is failing, with an alternate succeeding on behalf of their
clients. This behavior is a well-known fact from other work [39].
To facilitate reproducibility, we published experiment configura-
tions and scripts in a public GitHub repository [40].
Increased Response Size. Another potential risk during the
4.3.3