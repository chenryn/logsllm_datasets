### Fashion Similar to Autograph
For each packet, the substrings computed by Rabin fingerprints are inserted into a frequency count table. The count field is incremented each time the substring is encountered. Information regarding the source and destination IP addresses is recorded. The table is stored in rank order based on the frequency counts, which helps in identifying likely worm traffic. This system measures the prevalence of all common content in the network and then applies IP address dispersion by counting distinct source and destination IPs for each suspicious content. This approach helps in keeping the false positive rate low. The system is designed as a centralized solution and is not intended for use in collaboration between multiple sensors.

### Anomalous Payload-Based Worm Detection and Signature Generation

Each of the aforementioned projects focuses on detecting frequently occurring payloads delivered by a "suspicious" source IP. A source IP is considered suspicious if it targets dark IP space or exhibits pre-scanning behavior. These methods imply that detection occurs some time after the worm has propagated. In contrast, PAYL (Payload-based Anomaly Detection) does not rely on scanning behavior or payload prevalence. It detects anomalous payloads immediately and identifies the first propagation attempt of worms by correlating ingress/egress packet content alerts. Additionally, PAYL has been used in a system that automatically generates patches in a sandbox version of vulnerable software systems. For more details, see [14]. A broader discussion of related work in anomaly detection can be found in [20].

### 3. Payload-Based Anomaly Detection

#### 3.1 Overview of the PAYL Sensor

The PAYL sensor operates on the principle that zero-day attacks are delivered in packets with unusual and distinct data compared to the normal content flow at a site. We assume that the packet content is available to the sensor for modeling. We create a normal profile of a site’s unique content flow and use this information to detect anomalous data. A "profile" is a model or set of models representing the data seen during training. Since we are profiling content data flows, the method must be general, efficient, and accurate across all sites and services. Our initial design of PAYL uses a "language-independent" methodology, specifically the statistical distribution of n-grams [2] extracted from network packet datagrams. This methodology requires no parsing, interpretation, or emulation of the content.

An n-gram is a sequence of n adjacent byte values in a packet payload. A sliding window of width n is passed over the entire payload one byte at a time, and the frequency of each n-gram is computed. This frequency count distribution represents a statistical centroid or model of the content flow. The normalized average frequency and variance of each n-gram are calculated. The first implementation of PAYL uses the byte value distribution when n=1. The statistical means and variances of the 1-grams are stored in two 256-element vectors. However, we condition a distinct model on the port (or service) and packet length, producing a set of statistical centroids that provide a fine-grained, compact, and effective model of a site’s actual content flow. Full details of this method and its effectiveness are described in [20].

The first packet of CRII (CodeRed II) illustrates the 1-gram data representation implemented in PAYL. Figure 1 shows a portion of the CRII packet and its computed byte value distribution. The rank-ordered distribution is displayed in Figure 2, from which we extract a Z-string. The Z-string is a string of distinct bytes ordered by frequency, serving as a representative summary of the entire distribution, ignoring those byte values that do not appear in the data. The rank-ordered distribution resembles the Zipf distribution, hence the name Z-string. The Z-string provides a privacy-preserving summary of the payload that can be exchanged between domains without revealing the true content. Z-strings are not used for detection but for message exchange and cross-domain correlation of alerts, which is further described in section 5.

To compare the similarity between test data at detection time and the trained models, PAYL uses a simplified Mahalanobis distance [20]. The Mahalanobis distance weights each variable (the mean frequency of a 1-gram) by its standard deviation and covariance. The distance values produced by the models are then subjected to a threshold test. If the distance of a test datum exceeds the threshold, PAYL issues an alert for the packet. There is a distinct threshold setting for each centroid, computed automatically by PAYL during a calibration step. To calibrate the sensor, a sample of test data is measured against the centroids, and an initial threshold setting is chosen. Subsequent testing of new data updates the threshold settings to calibrate the sensor to the operating environment. Once this step converges, PAYL is ready to enter detection mode. Although the initial results of testing PAYL were promising, we made several improvements to the modeling technique to reduce the percentage of false positives.

#### 3.2 New PAYL Features: Multiple Centroids

PAYL is a fully automatic, "hands-free" online anomaly detection sensor. It trains models, determines when they are stable, and is self-calibrating. It automatically observes itself and updates its models as needed. The most important new feature in PAYL is the use of multiple centroids and ingress/egress correlation. In the first implementation, PAYL computes one centroid per length bin, followed by a stage of clustering similar centroids across neighboring bins. In the newer version, we compute a set of models \( M_{k}^{ij} \) for each specific observed packet payload length i of each port j. Hence, within each length bin, multiple models are computed before a final clustering stage. The clustering is now executed across centroids within a length bin and then again across neighboring length bins. This two-stage clustering strategy substantially reduces memory requirements while representing normal content flow more accurately and revealing anomalous data more clearly.

Since different types of payloads (e.g., pure text, .pdf, .jpg) may be sent to the same service, we use an incremental online clustering algorithm to create multiple centroids to model the traffic with finer granularity. This idea can be extended to include centroids for different media types. Different file and media types follow their own characteristic 1-gram distribution; including models for standard file types can help reduce false positives. (See [8] for a detailed analysis of this approach.)

The multi-centroid strategy requires a different testing methodology. During testing, an alert will be generated by PAYL if a test packet matches none of the centroids within its length bin. The multi-centroid technique produces more accurate payload models and separates anomalous payloads more precisely.

#### 3.3 Data Diversity Across Sites

A crucial issue we study is whether payload models are truly distinct across multiple sites. This is an important question in a collaborative security context. We claim that the monoculture problem applies not only to common services and applications but also to security technologies. If a site is blind to a zero-day attack, many other sites are likely to be blind to the same attack. Researchers are considering solutions to the monoculture problem by diversifying implementations. We conjecture that the content data flow among different sites is already diverse, even when running the same services. In our previous work, we showed that byte distributions differ for each port and length. We also conjecture that it should be different for each host. For example, each web server contains different URLs, implements different functionalities like web email or media uploads, and the population of service requests and responses sent to and from each site may differ, producing a diverse set of content profiles across all collaborating hosts and sites. Therefore, each host or site’s profile will be substantially different from all others. A zero-day attack that appears as normal data at one site is likely to appear as anomalous data at other sites due to the different normal profiles. We test this conjecture through several experiments.

One of the most challenging aspects of research in this area is the lack of real-world datasets with full packet content available to researchers for formal scientific study. Privacy policies typically prevent sites from sharing their content data. However, we were able to use data from three sources and show the distribution for each. The first source is an external commercial organization that wishes to remain anonymous, which we call EX. The other two sources are the web servers of the CS Department of Columbia, www.cs.columbia.edu and www1.cs.columbia.edu, which we call W and W1, respectively. The following plots show the profiles of the traffic content flow of each site.

Figures 3 and 4 display the payload distributions for different packet payload lengths (249 bytes and 1380 bytes), spanning the whole range of possible payload lengths to give a general view of the diversity of the data coming from the three sites. Each byte distribution corresponds to the first centroid built for the respective payload lengths. We observe from the plots that there is a visible difference in the byte distributions among the sites for the same length bin. This is confirmed by the values of Manhattan distances computed between the distributions, as shown in Table 1.

**Table 1. The Manhattan distance between the byte distributions of the profiles computed for the three sites, for three length bins**

| Length Bin | MD(EX, W) | MD(EX, W1) | MD(W, W1) |
|------------|-----------|------------|-----------|
| 249 bytes  | 0.4841    | 0.3710     | 0.3689    |
| 940 bytes  | 0.6723    | 0.8120     | 0.5972    |
| 1380 bytes | 0.4841    | 0.3710     | 0.3689    |

These results support our conjecture that the content data flow among different sites is indeed diverse.