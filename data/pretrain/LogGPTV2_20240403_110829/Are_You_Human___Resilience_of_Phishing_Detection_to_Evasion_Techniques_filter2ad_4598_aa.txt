title:Are You Human?: Resilience of Phishing Detection to Evasion Techniques
Based on Human Verification
author:Sourena Maroofi and
Maciej Korczynski and
Andrzej Duda
Are You Human? Resilience of Phishing Detection to Evasion
Techniques Based on Human Verification
Sourena Maroofi
sourena.maroofi@
univ-grenoble-alpes.fr
Univ. Grenoble Alpes, CNRS,
Grenoble INP, LIG
France
Maciej Korczyński
PI:EMAIL
Univ. Grenoble Alpes, CNRS,
Grenoble INP, LIG
France
Andrzej Duda
PI:EMAIL
Univ. Grenoble Alpes, CNRS,
Grenoble INP, LIG
France
ABSTRACT
Phishing is one of the most common cyberattacks these days. At-
tackers constantly look for new techniques to make their campaigns
more lucrative by extending the lifespan of phishing pages. To
achieve this goal, they leverage different anti-analysis (i.e., evasion)
techniques to conceal the malicious content from anti-phishing
bots and only reveal the payload to potential victims. In this paper,
we study the resilience of anti-phishing entities to three advanced
anti-analysis techniques based on human verification: Google re-
CAPTCHA, alert box, and session-based evasion. We have designed
a framework for performing our testing experiments, deployed
105 phishing websites, and provided each of them with one of the
three evasion techniques. In the experiments, we report phishing
URLs to major server-side anti-phishing entities (e.g., Google Safe
Browsing, NetCraft, APWG) and monitor their occurrence in the
blacklists. Our results show that Google Safe Browsing was the
only engine that detected all the reported URLs protected by alert
boxes. However, none of the anti-phishing engines could detect
phishing URLs armed with Google re-CAPTCHA, making it so far
the most effective protection solution of phishing content available
to malicious actors. Our experiments show that all the major server-
side anti-phishing bots only detected 8 out of 105 phishing websites
protected by human verification systems. As a mitigation plan, we
intend to disclose our findings to the impacted anti-phishing en-
tities before phishers exploit human verification techniques on a
massive scale.
CCS CONCEPTS
• Security and privacy → Phishing;
ACM Reference Format:
Sourena Maroofi, Maciej Korczyński, and Andrzej Duda. 2020. Are You
Human? Resilience of Phishing Detection to Evasion Techniques Based on
Human Verification. In ACM Internet Measurement Conference (IMC ’20),
October 27–29, 2020, Virtual Event, USA. ACM, New York, NY, USA, 9 pages.
https://doi.org/10.1145/3419394.3423632
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’20, October 27–29, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8138-3/20/10...$15.00
https://doi.org/10.1145/3419394.3423632
1 INTRODUCTION
Phishing is a form of social engineering with the goal of collecting
credentials of end-users usually achieved by either email spoof-
ing [1] or directly luring victims to enter their sensitive information
into a fake website that matches the look and feel of the legitimate
one [2]. There have been much research [3–7] and industry efforts
to combat phishing, just to mention the Anti-Phishing Working
Group (APWG) [8], PhishTank [9], or recently formed the COVID-
19 Cyber Threat Coalition [10]. Nevertheless, according to the latest
report from IBM X-force, phishing is the number one initial infec-
tion vector among attackers [11].
As with any other cybercriminal activity, phishers and security
organizations are constantly in battle. While phishers try to develop
new or misuse the existing techniques such as URL shorteners [12]
to make their attacks more effective, anti-phishing organizations try
to adapt their methods to detect phishing attacks swiftly. One of the
most effective techniques used by miscreants is the anti-analysis
also known as evasion [13]. The term refers to a wide range of
techniques used by attackers to prevent automatic threat analysis
[14]. While these techniques are very popular among malware
developers [15], phishers also begin to use them [16, 17]. Malicious
actors leverage evasion techniques to tell anti-phishing bots and
humans apart. If the end-user is human, they reveal the malicious
payload while for anti-phishing bots, they deliver a benign page to
evade detection.
Previous work analyzed the existing and well known evasion
techniques such as web-cloaking [18], URL redirection [19], using
URL shorteners [12], and code obfuscation [20]. These techniques
can affect the detection time, yet all major anti-phishing systems
can cope with them [21]. For example, to detect web-cloaking, we
can send two requests to the server, one with a user-agent related to
a known anti-phishing bot (e.g., googlebot) and the other one with a
typical browser user-agent (e.g., Mozilla Firefox). If the destination
of the two requests are different (e.g., different domains), then we
can infer the existence of web-cloaking [22].
However, we have recently observed more advanced anti-analysis
techniques that can severely impede the performance of phishing
detection systems or heavily extend the lifespan of phishing attacks.
Human verification is one such technique that not only hampers the
verification of phishing content but may also induce users to visit a
phishing site due to the use of CAPTCHA (Completely Automated
Public Turing test to tell Computers and Humans Apart) on benign
websites [23]. CAPTCHA is considered an effective anti-bot pro-
tection solution (e.g., against page scraping) but it is also misused
by cybercriminals to protect their malicious pages from security
IMC ’20, October 27–29, 2020, Virtual Event, USA
Sourena Maroofi, Maciej Korczyński, and Andrzej Duda
organizations. A real-world example of such a usage is a recently
detected phishing campaign with more than 128,000 emails sent
to victims with a link to a fake Microsoft login page protected by
Google reCAPTCHA [24].
In this paper, we study three advanced anti-analysis techniques
observed in real-world phishing attacks: Google reCAPTCHA [25],
alert box, and session-based evasion techniques. To evaluate the
effectiveness of the three techniques on anti-phishing bots, we
have designed an experiment in which we emulate the operation
of phishing websites—we register 105 domains, deploy harmless
phishing websites, and protect each of them with one of the three
evasion techniques. Then, we evaluate the detection performance
of seven major anti-phishing engines: the Google Safe Browsing
(GSB), Anti-Phishing Working Group (APWG), NetCraft, Open-
Phish, PhishTank, Microsoft Defender SmartScreen, and Yandex
Safe Browsing (YSB), as well as six most popular client-side anti-
phishing extensions for Mozilla Firefox including the Avast Online
Security and Avira Browser Safety. We show that almost all anti-
phishing engines do not detect the phishing websites and mark the
URLs as benign.
Our main contributions are as follows:
(1) we qualitatively analyze a new category of anti-analysis tech-
niques observed on real-world phishing websites,
(2) we design a semi-automated and scalable framework for ex-
perimentally testing the evasion techniques (it is available on
request for researchers to support reproducibility),
(3) we empirically study the effects of three human verification
evasion techniques on major server-side and client-side anti-
phishing engines,
(4) we show that all the major server-side anti-phishing bots only
detected 8 out of 105 phishing websites protected by human
verification systems.
2 PHISHING DETECTION AND
ANTI-ANALYSIS TECHNIQUES
To better understand the three evasion techniques, we explain them
in detail and discuss the possible approaches for phishing detection.
Note that we have observed examples of phishing pages armed with
all three techniques in real-world attacks (see Figures 1, 2, and 3 in
Appendix A). The main source of our data is the unverified phishing
section of the PhishTank [9] where the submitted URLs are not
directly published as phishing but instead are pending for ‘voters’
to manually verify them as phishing URLs or false positives [26].
2.1 Server-Side vs. Client-Side Detection
Phishing page detection involves either client-side or server-side
systems. In the server-side approach (e.g., GSB), the candidate URL
is sent to the server (either by direct human report or by automatic
URL collection by crawling). With the URL, the server starts to
collect data (e.g., page content, domain content, lexical features,
etc.) and decides whether it is malicious or benign. The client-side
detection systems (e.g., browser add-ons and extensions) have direct
access to the URLs and page content that end-users visit. Therefore,
it is possible to collect features from the visited pages and decide on
the phishing character of the pages either directly on the client-side
or later on by sending data to server-side systems.
Each detection type has its advantages and drawbacks. The
server-side approach can be used globally by every browser that
can interact with the system using an application programming
interface (API). The internal detection algorithm appears as a black-
box to attackers. Users’ privacy is preserved by sending the hashed
version of the URLs to the server. The drawback is that there is
no guarantee that, having the same URL, the server and the client
fetch the same content. If the attacker can detect the anti-phishing
bot (e.g., IP-based or user-agent detection [21]), it will serve benign
content to the bot.
Client-side phishing detection extensions do not suffer from
this problem. They have access to the very same content as users
visit because they are installed on browsers. However, the lack
of sufficient support for extensions in different browsers [27], the
inconsistency between desktop and mobile phone browsers [21],
and privacy concerns related to third-party extensions [28] are
the factors that affect the popularity of client-side extensions. In
this paper, we study seven server-side and six client-side phishing
detection systems.
Below, we describe the three evasion techniques used by phishing
websites to avoid detection.
2.2 Alert Box Evasion
Alert boxes in JavaScript are modal i.e., they pause script execution
and do not allow the visitor to interact with the rest of the page until
the window has been dismissed. Figure 1 in Appendix A shows an
example of such a protection. The code to create a complete PayPal
phishing page with alert box protection can be written in one PHP
file and, for example, injected into one of the legitimate pages on
a compromised machine. After a random number of seconds, the
alert box appears on the screen by blurring the whole page and
displaying the message: ‘Please Sign In To Continue’. When the user
clicks on the ‘OK’ button, an AJAX request is sent to the server to
retrieve the malicious payload and replace the current page content
with it. Code Listing 2 in Appendix C presents example PHP code
of this technique.
To bypass the alert box, anti-phishing engines need to detect it
first. Most browser emulation libraries, e.g., the Selenium1 project,
can distinguish the alert box window if it is present. They can also
confirm or cancel the alert box.
2.3 Session-Based Evasion
Major companies like Google or Facebook have used the multi-
page sign-in method for several years [29]. In this method, a user
provides her username on the first page, submits the page, and
enters her password on the second page. Note that the user can
only be redirected to the second page if a session was generated
on the first one. Phishers use this technique to reveal the second
(malicious) page only to the users who have already visited the
first page and clicked on the submit button. Figure 2 in Appendix A
shows a real-world example of a session-based phishing attack. The
first page (top figure) shows the ‘Join Chat’ button to persuade the
victim to join a WhatsApp chat group and after pressing the button,
the second page shows a fake Facebook login page.
1https://www.selenium.dev
Are You Human?
IMC ’20, October 27–29, 2020, Virtual Event, USA
One way to detect this type of evasion techniques is to automati-
cally press the buttons and submit the forms on the suspicious page.
In Section 4.1, we show that some anti-phishing engines submit
forms on the suspicious pages if they can find the HTML form tag.
2.4 CAPTCHA-Based Evasion
Google reCAPTCHA is considered as an effective anti-bot protec-
tion solution against page scraping [25]. Figure 3 in Appendix A
shows the phishing page protected by the Google reCAPTCHA v2
checkbox. By solving the CAPTCHA challenge, the form is submit-
ted to the server and the phishing payload (e.g., PayPal login page)
is served to the end-user without any URL redirection (Figure 3
in Appendix A bottom). To better understand the technique, Code
Listing 1 in Appendix C shows the PHP code to create the phishing
page. The first page is completely benign without an HTML form
tag. When the user solves the CAPTCHA challenge, the HTML
form tag is dynamically generated (using JavaScript) and then sub-
mitted to the server to reload the page using the same URL but
with malicious content. Since the URL has not changed, the built-in
browser anti-phishing system (e.g., GSB in Chrome) or the installed
third-party extension (e.g., NetCraft toolbar) does not resend it to
the server and serves instead the cached result usually valid for 5 to
60 minutes [30]. This behavior makes the detection process difficult
(or impossible) and extends the lifespan of the phishing page.
There is no universal solution to bypass this type of evasion tech-
niques. However, using client-side extensions, there is no need to
automatically bypass CAPTCHA. If the end-user solves CAPTCHA
and visits the second page (potentially malicious), the anti-phishing
extension has also access to the content of the second page. So, it
can detect phishing using the revealed page content.
3 EXPERIMENT METHODOLOGY
To investigate the effectiveness of human verification techniques to
protect phishing websites, we have designed a testing experiment
and evaluated the performance of seven server-side anti-phishing
entities. The experiment has two phases: the initial test to check if
anti-phishing engines can detect the payload itself and the main
test in which we protect the same payload using one of the evasion
techniques and report it to anti-phishing entities. Both phases follow
the same process as explained below.
For security considerations, we do not publish the source code
of the proposed framework. However, it is available to researchers
upon request to support reproducibility.
Tested Server-Side Anti-Phishing Entities. We choose seven
major anti-phishing entities: the Google Safe Browsing (GSB), Net-
Craft, Anti-Phishing Working Group (APWG), OpenPhish, Phish-
Tank, Microsoft Defender SmartScreen, and Yandex Safe Browsing
(YSB). GSB is used by Google Chrome, Mozilla Firefox, and Apple
Safari that cover 87% of the end-user browsers both on desktop and
mobile devices in 2020 [31]. Internet Explorer (IE) and Microsoft
Edge use Microsoft Defender SmartScreen. The Opera browser uses
two blacklist services: NetCraft and PhishTank [32]. To the best
of our knowledge, OpenPhish and APWG are not directly used by
any browser. However, they are two important blacklist feeds that
might be used by anti-phishing engines, so we also consider them
in our experiments. Finally, YSB is used by the Yandex browser, the
second most popular browser in Russia as of May 20202.
Registering Domains. We first register 7 domains for the initial
experiment and 105 for the main experiment, 112 domains in total.
Oest et al. [21] performed a similar experiment to evaluate the
effectiveness of web-cloaking in phishing attacks and registered
fresh domains in bulk. In our study, we make sure that our domains
are reputed enough to emulate the conditions of compromised
domains rather than maliciously registered ones as observed in real-
world cases. Therefore, we first register 50 drop-catch domains [33,
34] with the following new method:
(1) First, we scan the top 1M domains in the Alexa list [35] for
‘SOA’ and ‘NS’ DNS records and only keep the domains with
the NXDOMAIN answer (770 domains).
(2) We use Godaddy and Porkbun APIs (two major registrars) to
check the availability of the domains for registration from
step 1 (251 domains).
(3) We collect WHOIS data for 251 domains from the previous
step and only select those with ‘NOT FOUND’ answer to make
sure they are not registered (244 domains).
(4) We submit these domains to VirusTotal3 and Google Safe
Browsing to make sure they have not been recently used in
malicious activity (244 domains).
(5) We select the domains archived at least once by the Inter-
net Archive4 to make sure they have their web history (50
domains).
(6) We select the domains indexed at least once by the Google
search engine based on the site:domain query (50 domains).
For the set of the remaining domains, we randomly generate key-
words from the Unix dictionary and register 21 domains from new
generic top-level domain (gTLDs) and the rest from legacy gTLDs
(.com, .net, and .org). To reduce the impact of bulk registration pat-
terns, we register our domains manually during two weeks in April
2020 with the OVH5 registrar and deploy DNSSEC for all domains.
All steps are taken to reduce the chances of being blacklisted due
to the low reputation of the domain.
Website Content and Web Servers. Compromised domains are
intrinsically legitimate but hacked to host and serve malicious
content in addition to legitimate content. Therefore, we have to
generate a full-fledged website for each domain. To achieve this, we
propose and develop a fake website generator using the following
algorithm:
(1) We extract meaningful keywords from the registered domain
name.
API.
(2) For each keyword, we find synonyms using the Datamuse6
(3) For each related keyword, we download the related page from
the English version of Wikipedia along with their correspond-
ing images.
(4) For each domain, we randomly generate 30 pages (with .php
extensions) with different names and different directories.
2https://gs.statcounter.com/browser-market-share/all/russian-federation
3https://www.virustotal.com
4https://archive.org
5https://www.ovh.com
6http://www.datamuse.com/api
IMC ’20, October 27–29, 2020, Virtual Event, USA
Sourena Maroofi, Maciej Korczyński, and Andrzej Duda
Then, we generate hyperlinks from one page to another to
create a fully functional website.
Having the fake website generator, it takes 2 minutes to generate
a fully functional website with 30 different pages in an automated
manner. The output is a .zip package ready to upload and install
on the server. We upload all the generated websites to our hosting
infrastructures in one of the European countries with 22 different
IP addresses and the Nginx web server. Finally, we issue TLS cer-
tificates for all the domains and keep all websites online without
uploading phishing kits for one week.
Phishing Kits. The next step is to automatically generate phish-
ing kits and upload them to our servers. We keep in mind the