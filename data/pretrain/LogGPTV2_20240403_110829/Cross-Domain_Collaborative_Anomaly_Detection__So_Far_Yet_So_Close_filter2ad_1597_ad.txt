servers, of which www.cs.gmu.edu experiences much lower traﬃc volumes, a
couple interesting results stand out. As expected, both false positives and true
positives drop oﬀ signiﬁcantly. We see no false positives after the training period.
This shows that for at least our data sets all of the server-side services that
cause false positives drop out once we require three web servers to have the
data in common. If this continues to be the case as more servers are added,
then only reporting attacks that target three or more servers could solve most of
the false positive issues. While requiring three servers to conﬁrm an attack does
yield less true positives, the ones it does detect are quite widespread and if the
collaboration is expanded, the detection should increase greatly. This method,
while scaling in detection rate more slowly than only requiring two servers to
conﬁrm attacks, could be a much more eﬀective option to keep false positives
low once enough servers collaborate.
We calculate the implications of changing the threshold for matching two
alerts. Increasing the threshold past 80% to require perfect or almost perfect
matches fails to help in reducing the false positives, since at this threshold almost
all of the false positives are exact matches so even requiring all n-grams to match
a Bloom ﬁlter exactly does not help. Reducing the threshold to allow more loose
matches does show a trade oﬀ in increased detection of attacks at the expense
of additional false positives. By only requiring 60% of n-grams from one alert
to match the Bloom ﬁlter representation of another site’s alert, we can expect
to capture attacks with signiﬁcantly more variance such as similar payloads
targeting diﬀerent web applications. See experiment details in Table 8. While
at ﬁrst, the results from a lower threshold appear quite good in terms of raw
numbers of alerts, looking at only the new unique alerts which human operators
16
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
Fig. 6. Time gap between alerts at collaborating sites.
Across Site CU,
Time Gap
in Minutes GMU and GMU CS CU and GMU and GMU CS
7.52
20589.02
7048.07
7038.27
Across Site Across Site CU Across Site GMU
and GMU CS
0.23
24262.13
6489.08
7634.13
0.23
17501.07
4579.85
5250.04
Min
Max
Average
Std. Dev.
1.48
25911.00
5477.35
6173.61
Table 9. Time gap statistics across three sites
have to classify tells a more balanced story. Going from an 80% threshold to
60% for our eight week run with a training period increases the detection of new
unique attacks by 37.6%, while increasing the newly seen unique false positives
by 76.9%. In the three week run, the lower threshold adds no new unique false
positives pointing to the need for threshold optimization once the system scales
up. In fact, it lowers the utility of adding a new server since the existing ones
detect additional attacks without it. However, as the number of web servers
collaborating increases, this matching threshold along with the number of servers
required to share an alert before reporting it as an attack should be key settings
in order to optimize the system as they both key methods in this approach for
controlling the false positive rate.
From the oﬄine generated alert clusters, we conduct a temporal study of the
alerts seen across the three servers. Firstly, we look at the time gap between alerts
across sites. We compute the pairwise time gap of common alert clusters across
the three servers. Additionally, we calculate the minimum time gap between alert
clusters common to all of the three servers. Table 9 summarizes the minimum,
maximum, average and standard deviations of the time gaps for the above cases.
A better visual representation of the common alert clusters across all of the
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
17
Fig. 7. Number of new unlabeled unique alerts per day that a human operator would
have to parse. The large number of false positives from AD system is reduced by almost
a magnitude diﬀerence when correlated to other sensors.
three servers is represented in Figure 6. The graph shows the minimum time gap
between alerts observed at one server and the same alert being observed at the
other two servers. The horizontal axis denotes the relative time elapsed since the
start of observing the ﬁrst alert. The vertical axis denotes the cluster. Each of
the bars in the graph start at the time when an alert is observed at a site and
ends at a time when it is seen ﬁrst among the other two sites. The bar graphs
are color coded to represent where the attack was ﬁrst seen. From the statistics
it can be seen that the average time gap between alerts could be used to our
advantage. The results from the time gap analysis from the October-November
run computed across CU and GMU shows a similar large average value (Min:
1.57min, Max: 71022.07min, Average: 15172.53min, Std. Dev.: 18504.44min).
This gives us suﬃcient time to take preventive action at the collaborating sites by
exchanging a small blacklist. Furthermore, we analyze the number of unclassiﬁed
unique alerts that an operator has to manually classify every day. Figure 7
depicts the number of unique alerts generated daily. The graph shows both true
positive and false positives observed using our collaborative approach alongside
a stand alone approach. The horizontal axis denotes time in one day bins and
the vertical axis denotes the frequency of alerts observed on a log scale. For the
stand alone CAD sensor, a unique alert is included in the frequency when it
is ﬁrst observed at a site. However, for multiple sites collaborating, an alert is
included in the frequency count at the time when it is conﬁrmed to be seen at all
sites. On average the number of unique alerts observed every day using a stand
alone CAD sensor at CU is 82.84 compared to 3.87 alerts when a collaborative
approach, over an order of magnitude in diﬀerence. Therefore, a collaborative
approach clearly reduces the load on the operator monitoring alerts to an easily
managed amount.
18
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
6 Conclusions
Web services and applications provide vital functionality but are often suscepti-
ble to remote zero-day attacks. Current defenses require manually crafted signa-
tures which take time to deploy leaving the system open to attacks. Contrary, we
can identify zero-day attacks by correlating Content Anomaly Detection (CAD)
alerts from multiple sites while decreasing false positives at every collaborating
site. Indeed, with a false positive rate of 0.03% the system could be entirely
automated or operators could manually inspect the less than four new alerts per
day on average that we observe in our eight week experiment. We demonstrate
that collaborative detection of attacks across administrative domains, if done in
a controlled and privacy preserving manner, can signiﬁcantly elevate resources
available to the defenders exposing previously unseen attacks.
References
1. Anagnostakis, K.G., Greenwald, M.B., Ioannidis, S., Keromytis, A.D.: Robust Re-
actions to Potential Day-Zero Worms through Cooperation and Validation. In:
Proceedings of the 9th Information Security Conference (ISC). pp. 427–442 (Au-
gust/September 2006)
2. Anagnostakis, K.G., Greenwald, M.B., Ioannidis, S., Keromytis, A.D., Li, D.: A
Cooperative Immunization System for an Untrusting Internet. In: IEEE Interna-
tional Conference on Networks (2003)
3. Bloom, B.H.: Space/time trade-oﬀs in Hash Coding with Allowable Errors. Com-
munications of the ACM 13(7), 422–426 (1970)
4. Boggs, N., Hiremagalore, S., Stavrou, A., Stolfo, S.J.: Experimental results of cross-
site exchange of web content anomaly detector alerts. In: Technologies for Home-
land Security, 2010. HST ’10. IEEE Conference on. pp. 8 –14 (Nov 2010)
5. Cretu, G., Stavrou, A., Locasto, M., Stolfo, S., Keromytis, A.: Casting out demons:
Sanitizing training data for anomaly sensors. In: Security and Privacy, 2008. SP
2008. IEEE Symposium. pp. 81 –95 (may 2008)
6. Cretu-Ciocarlie, G., Stavrou, A., Locasto, M., Stolfo, S.: Adaptive Anomaly Detec-
tion via Self-Calibration and Dynamic Updating. In: Recent Advances in Intrusion
Detection. pp. 41–60. Springer (2009)
7. Farroukh, A., Mukadam, N., Bassil, E., Elhajj, I.: Distributed and collaborative in-
trusion detection systems. In: Communications Workshop, 2008. LCW 2008. IEEE
Lebanon. pp. 41 –45 (may 2008)
8. Gates, C.: Coordinated scan detection. In: Proceedings of the 16th Annual Network
and Distributed System Security Symposium (NDSS 09) (2009)
9. Kruegel, C., Toth, T.: Distributed Pattern for Intrusion Detection. In: Network
and Distributed System Security (NDSS) (2002)
10. Kruegel, C., Toth, T., Kerer, C.: Decentralized Event Correlation for Intrusion
Detection. In: International Conference on Information Security and Cryptology
(2002)
11. Kumar, S., Dharmapurikar, S., Yu, F., Crowley, P., Turner, J.: Algorithms to
accelerate multiple regular expressions matching for deep packet inspection. In:
Proceedings of the 2006 conference on Applications, technologies, architectures,
and protocols for computer communications. pp. 339–350. ACM (2006)
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
19
12. Lazarevic, A., Ozgur, A., Ertoz, L., Srivastava, J., Kumar, V.: A comparative study
of anomaly detection schemes in network intrusion detection. In: In Proceedings
of the Third SIAM International Conference on Data Mining (2003)
13. Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions and re-
versals. Soviet Physics Doklady 10(8), 707–710 (1966), doklady Akademii Nauk
SSSR, V163 No4 845-848 1965
14. Lin, P., Lin, Y., Lee, T., Lai, Y.: Using string matching for deep packet inspection.
Computer 41(4), 23–28 (2008)
15. Locasto, M.E., Parekh, J.J., Keromytis, A.D., Stolfo, S.J.: Towards Collaborative
Security and P2P Intrusion Detection. In: IEEE Information Assurance Workshop.
West Point, NY (2005)
16. Norton, M., Roelker, D., Inc, D.R.S.: Snort 2.0: High performance multi-rule in-
spection engine
17. Paxson, V.: Bro: a system for detecting network intruders in real-time. In:
SSYM’98: Proceedings of the 7th conference on USENIX Security Symposium.
pp. 3–3. USENIX Association, Berkeley, CA, USA (1998)
18. Porras, P., Neumann, P.G.: EMERALD: Event Monitoring Enabling Responses to
Anomalous Live Disturbances. In: National Information Systems Security Confer-
ence (1997)
19. Sommer, R., Paxson, V.: Enhancing byte-level network intrusion detection sig-
natures with context. In: CCS ’03: Proceedings of the 10th ACM conference on
Computer and communications security. pp. 262–271. ACM, New York, NY, USA
(2003)
20. Sommer, R., Paxson, V.: Outside the closed world: On using machine learning for
network intrusion detection. Security and Privacy, IEEE Symposium on 0, 305–316
(2010)
21. Song, Y., Keromytis, A.D., Stolfo, S.J.: Spectrogram: A mixture-of-markov-chains
model for anomaly detection in web traﬃc. In: NDSS ’09: Proceedings of the 16th
Annual Network and Distributed System Security Symposium (2009)
22. Song, Y., Locasto, M.E., Stavrou, A., Keromytis, A.D., Stolfo, S.J.: On the infea-
sibility of modeling polymorphic shellcode. In: Proceedings of the 14th ACM con-
ference on Computer and communications security. pp. 541–551. CCS ’07, ACM,
New York, NY, USA (2007), http://doi.acm.org/10.1145/1315245.1315312
23. Staniford-Chen, S., Cheung, S., Crawford, R., Dilger, M.: GrIDS - A Graph Based
Intrusion Detection System for Large Networks. In: National Information Com-
puter Security Conference. Baltimore, MD (1996)
24. Stavrou, A., Cretu-Ciocarlie, G.F., Locasto, M.E., Stolfo, S.J.: Keep your friends
close: the necessity for updating an anomaly sensor with legitimate environment
changes. In: AISec ’09: Proceedings of the 2nd ACM workshop on Security and
artiﬁcial intelligence. pp. 39–46. ACM, New York, NY, USA (2009)
25. Taylor, C., Gates, C.: Challenging the Anomaly Detection Paradigm: A Provoca-
tive Discussion. In: Proceedings of the 15th New Security Paradigms Workshop
(NSPW). pp. xx–yy (September 2006)
26. Tian, D., Changzhen, H., Qi, Y., Jianqiao, W.: Hierarchical distributed alert corre-
lation model. In: IAS ’09: Proceedings of the 2009 Fifth International Conference
on Information Assurance and Security. pp. 765–768. IEEE Computer Society,
Washington, DC, USA (2009)
27. Ullrich, J.: DShield home page (2005), http://www.dshield.org
28. Vasiliadis, G., Polychronakis, M., Antonatos, S., Markatos, E., Ioannidis, S.: Reg-
ular expression matching on graphics hardware for intrusion detection. In: Recent
Advances in Intrusion Detection. pp. 265–283. Springer (2009)
20
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
29. Vigna, G., Gwalani, S., Srinivasan, K., Belding-Royer, E.M., Kemmerer, R.A.: An
Intrusion Detection Tool for AODV-based Ad hoc Wireless Networks. In: Computer
Security Applications Conference (2004)
30. Wang, K., Parekh, J.J., Stolfo, S.J.: Anagram: A Content Anomaly Detector Re-
sistant to Mimicry Attack. In: Symposium on Recent Advances in Intrusion De-
tection. Hamburg, Germany (2006)
31. Websense: LizaMoon. http://community.websense.com/blogs/securitylabs/
archive/2011/03/31/update-on-lizamoon-mass-injection.aspx
32. Xu, D., Ning, P.: Privacy-preserving alert correlation: a concept hierarchy based
approach. In: Computer Security Applications Conference, 21st Annual. pp. 10 pp.
–546 (dec 2005)
33. Yegneswaran, V., Barford, P., Jha, S.: Global Intrusion Detection in the DOMINO
Overlay System. In: NDSS (2004)
34. Zaman, S., Karray, F.: Collaborative architecture for distributed intrusion detec-
tion system. In: Computational Intelligence for Security and Defense Applications,
2009. CISDA 2009. IEEE Symposium on. pp. 1 –7 (july 2009)