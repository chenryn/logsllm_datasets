4.6%
0.3%
25%
10%
0.1%
0%
83%
100 %
100%
57%
86%
43%
75%
75%
82%
0%
0%
9.6%
0%
16.1% 17.64 %
0%
0%
16.1%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
7%
0%
0%
0%
9.6%
0%
20%
17.9%
0%
28.3%
38.1%
0.2%
19.9%
32 %
30%
9%
17.1 %
90%
24%
59 %
100%
74%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
2%
10%
0%
0%
0%
0%
0%
0%
Figure 18: False positive and false negative rates for OpenCV recognizers on common data sets. False positives are
important because they could leak unintended information to an application. We also show the eﬀect of blurring and
frame subtraction. For blurring we used a 12x12 box ﬁlter.
ber does not exceed a threshold, we ignore the de-
tected object as a false positive.
For three out of our seven recognizers, blurring
decreases false positives with no eﬀect on false neg-
atives, with a maximum reduction for our lower
body recognizer from 19.5% false positives to 4.6%
false positives. For the remaining recognizers, false
positives decrease but false negatives also increase.
Frame subtraction decreases false positives for six
out of seven recognizers and has no eﬀect on the sev-
enth, with no impact on false negatives. This is in
line with our goals, because false positives are more
damaging to privacy than false negatives. The full
results are in Figure 18.
Recognizer Combination. Finally, we imple-
mented recognizer combination, in which the OS can
take advantage of the fact that multiple recognizers
are available. Speciﬁcally, we combined the OpenCV
face detector with the Kinect depth sensor. We chose
the OpenCV face detector because its developers
could depend only on the presence of RGB video
data. We ran an experiment that ﬁrst acquires an
RGB and depth frame, then blanks out all pixels
with depth data that is further away than a thresh-
old. Next, we fed the resulting frame to the face
detector. An example result is shown in Figure 19.
On the left, the original frame shows a false posi-
tive detected behind the real person. On the right,
recognizer combination successfully avoids the false
Recognizers
Kinect SDK
Our framework
RGB Video
Skeleton
Face
29.87 fps
29.59 fps
28.24 fps
30.02 fps
28.65 fps
28.00 fps
Figure 20: Frame rates for a single application using
the Kinect SDK vs. using recognizers from our system.
Our system incurs negligible overhead.
positive.
5.3 Performance
In our performance evaluation, we (1) measure the
overhead of using our system compared to using the
Kinect SDK directly, (2) quantify the beneﬁts of rec-
ognizer sharing for multiple concurrent applications,
and (3) evaluate the beneﬁt of recognizer oﬄoading.
Overhead over Kinect SDK. Compared to di-
rectly using the Kinect SDK, an application that
uses our multiplexer will face extra overhead due
to recognizer event processing in the multiplexer as
well as data marshaling and transfer over local sock-
ets. To quantify this overhead, we wrote two identi-
cally functioning applications to obtain and display
a raw 640x480 RGB video feed, a skeleton model,
and points from a face model. The ﬁrst application
used the Kinect SDK APIs directly, while the second
426  22nd USENIX Security Symposium 
USENIX Association
12
)
c
e
s
/
s
e
m
a
r
f
(
e
t
a
r
e
m
a
r
F
30
20
10
0
1
 With recognizer sharing
 No recognizer sharing
5
Number of concurrent applications 
10
15
Figure 21: Eﬀect of sharing a concurrent RGB video
stream between applications. Our framework enables 25
frames per second or higher for up to six applications,
while without sharing the frame rate drops.
used our multiplexer with RGB, skeleton, and face
detection recognizers.
Figure 20 shows the frame rates when running
these two applications on a desktop HP xw8600 ma-
chine with a 4-core Core i5 processor and 4 GB of
RAM. We see that, fortunately, our current proto-
type incurs negligible overhead over the Kinect SDK
when used by a single application.
Recognizer Sharing. Next, we ran multiple con-
current copies of the two applications above to eval-
uate the beneﬁts of recognizer sharing as well as the
scalability of our prototype. Since the Kinect SDK
does not permit concurrent applications, we wrote
a simple wrapper for simulating that functionality,
i.e., allowing multiple applications as if they were
linking to independent copies of the Kinect SDK.
Figure 21 shows the average frame rate for mul-
tiple concurrent applications using the RGB recog-
nizer. We see that without recognizer sharing, frame
rates quickly stall as the number of concurrent ap-
plications increases, becoming unusable beyond ﬁve
applications.
In contrast, our approach maintains
at least 25 frames per second up to six concurrent
applications and degrades gracefully thereafter. We
experienced similar recognizer sharing beneﬁts for
skeleton and face recognizers.
While currently shipping AR platforms do not yet
support multiple concurrent applications, the above
experiment demonstrates that our system is ready to
eﬃciently embrace such support. Indeed, we believe
this to be the future of AR platforms. Mobile phone
“AR Browsers” such as Layar already expose APIs
for third-party developers, with over 5,000 applica-
tions written for Layar alone [20]. Users will beneﬁt
from running these applications concurrently; for ex-
ample, looking at a store front, one application may
show reviews of the store, while another shows in-
formation about its online catalog, and yet a third
Recognizer
Tablet
Oﬄoaded
Server
Throughput (frames/sec)
Plane detection
Face recognition
0
2.04
4.17
2.73
4.46
2.84
Figure 22: Frames processed per second when running
recognizers (1) locally on a client tablet, (2) oﬄoaded to
the server and shipping results back to the tablet, and
(3) locally on the server.
application attaches a name to the face of someone
walking by.
Recognizer Oﬄoading. We evaluated oﬄoad-
ing of two resource-intensive recognizers:
plane
and face recognition. The plane recognizer recon-
structs planes in the current scene using KinectFu-
sion, which computes 3D models from Kinect depth
data [23]. The face recognizer uses the Microsoft
Face SDK [21] to identify the name of the person in
the scene using a small database of known faces.
We implemented oﬄoading across two devices
linked by an 802.11g wireless network. For face
recognition, the client sends RGB bitmaps of the
current scene to the server as often as possible; the
client additionally includes the depth bitmap for the
plane recognizer.
Our client device was a Samsung Series 7 tablet
running Windows 8 Pro 64-bit with a 2-core Core i5
processor and 4 GB of RAM, hooked up to a Kinect.
Our server device was a desktop HP Z800 ma-
chine running Windows 8 Pro 64-bit with two 4-
core Xeon E5530 processors, 48 GB of RAM, and an
Nvidia GTX 680 GPU.
The ﬁrst two columns of Figure 22 show through-
puts experienced by the client when running rec-
ognizers locally and when oﬄoading them to the
server. The plane recognizer requires a high-end
Nvidia GPU, which prevented it from running on our
client at all; we report this as zero frames per second.
With oﬄoading, however, the client is able to detect
planes 4.2 times per second. For face recognition,
the client processed 2.73 frames per second when of-
ﬂoading, a 34% improvement in response time com-
pared to running face recognition locally. In addi-
tion, when run locally, face recognition placed heavy
CPU load on the client, completely consuming one of
its two cores. With oﬄoading, the client’s CPU con-
sumption dropped to 15% required to send bitmaps,
saving battery and freeing resources for processing
other recognizers. Note that our setup allows the
oﬄoading server to service multiple clients in paral-
lel. For example, the server was able to handle eight
concurrent face recognition clients before saturating.
We also considered the overhead of our oﬄoad-
USENIX Association  
22nd USENIX Security Symposium  427
13
ing mechanism by plugging a Kinect into our server
and running the recognizer framework directly on
it. Column 3 of Figure 22 shows these results. We
see that oﬄoading with the Kinect on the client
is only 4–7% slower than running the Kinect on
the server, meaning that the oﬄoading overhead of
transferring bitmaps and recognition results is rea-
sonable.
6 Related Work
Augmented Reality. Azuma surveyed augmented
reality, deﬁning it as real-time registration of 3-D
overlays on the real world [1], later broadening it
to include audio and other senses [2]. We take a
broader view and also consider systems that take in-
put from the world. Qualcomm now has an SDK
for augmented reality that includes features such as
marker-based tracking for mobile phones [26]. Previ-
ous work by our group has laid out a case for adding
OS support for augmented reality applications and
highlighted key challenges [7].
Common shipping object recognition algorithms
include skeleton detection [29], face and headpose
detection [31, 21], and speech recognition [22]. More
recently, Poh et al. showed that heart rate can be ex-
tracted from RGB video [24]. Our recognizer graph
and simple API allow quickly adding new recogniz-
ers to our system.