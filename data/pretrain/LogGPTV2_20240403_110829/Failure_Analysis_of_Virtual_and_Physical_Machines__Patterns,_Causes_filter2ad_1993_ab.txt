work, software, hardware, reboot and power failure classes,
excluding the unclassiﬁed (other) failure class. The failures
classiﬁed as other account for 53% of all tickets, distributed
as follows across the systems: Sys I has 35%, Sys II has 68%,
Sys III has 68%, Sys IV has 61%, and Sys V has 29%.
When not considering other failures,
the most common
failures are due to software and reboot issues, which account
for 31% of all tickets. As for subsystems, we note that for Sys
I-IV software problems are the major reason for server failures
and account for 12-22% of the crashes. Reboots remain the
second most frequent cause of failures for all subsystems
(8-29% of all), except for Sys II (only 3%). Hardware and
network issues appear more frequently for Sys I and II, i.e.,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
 100
]
%
[
s
e
r
u
l
i
a
F
f
o
n
o
i
t
c
a
r
F
 80
 60
 40
 20
 0
SW
Reboot
HW
Network
Power
Other
All
Sys I Sys II Sys III Sys IV Sys V
Fig. 1. Ticket distribution across the ﬁve types of failures: hardware, network,
power, reboot, and software.
26% and 13%, respectively, whereas for Sys II they account
for less than 20 incidents throughout the observation period,
i.e., 1%. On the one hand, power outages are generally not
a major cause of failure, as they occur less frequently. For
instance, Sys III experiences no power outages throughout the
entire year, whereas for Sys I, II and IV the crashes due to
power issues account for 4%, 4% and 3%, respectively. On the
other hand, the servers corresponding to Sys V are affected by
power outages in 29% of the cases, which leads us to conclude
that they are not co-located in the same datacenter with any
of the servers corresponding to the other 4 systems.
As third step, from all identiﬁed crash tickets we extract
the server ids on which the crash events occurred. Finally, we
collect the resource capacity and usage data for the extracted
servers to map crash tickets to the server characteristics.
We note here that the ticket generation and resolution heav-
ily involve human intervention, and thus will possibly include
human induced errors. We try our best to present data that
is sanitized by careful human intervention and interpretation
of the raw data. In the remainder of this paper, we restrict
the analysis to a smaller and consistent population, depending
on the overlap of the measurements of interest across the
aforementioned databases.
B. Measurements of Interest
As our aim is to identify the factors that determine the
failures of PMs and VMs in commercial datacenters,
the
ﬁrst measurement of interest is the failure rate of a single
server and the subsystem in a granularity of day, week, and
month over the one-year observation period. The failure rate is
deﬁned by the number of all failures divided by the number of
servers. Additionally, we consider the random failure and the
recurrent failure probability. The random failure probability
is deﬁned by the number of servers experiencing at least one
failure divided by the total number of servers, whereas the
recurrent failure probability is computed as the probability
that given a server fails during the observation period, it will
experience recurrent failures within 24 hours, a week, and a
month. We further compute these failure probabilities per each
subsystem, relative to PMs and VMs, as well as at the level of
4444
each failure class. We combine different categories and present
subsets of results when deemed appropriate.
The second set of measurements of interest are related to
server resource capacity, usage, age, and consolidation level.
In particular for resources, we consider the CPU, memory,
disk and network.
VM age – We propose to consider a VM’s ﬁrst occurrence
in the server resource monitoring database as its creation date.
Given that records are kept over a two-year observation period,
our approach suffers from the limitation that VMs can have
been created prior to the earliest collection date. To increase
the accuracy of our assumption, we ﬁlter out all VMs with
creation dates that coincide with the initial observable data
in the database. As a result, our analysis on the age impact
focuses only on the VMs that have been created less than two
years ago, which account for roughly 75% of the entire VM
population. The VM age upon failure is deﬁned by the time
difference between the VM’s creation date and the timestamp
of the failure event.
Resource capacity of PMs and VMs – For CPU, we ignore
the architecture generation, but focus on the number of proces-
sors. We collect the memory size in terms of GB instead of the
number of modules. For disks, we look into both the number
of disks and the total storage volume. For lack of detailed
information on the layout of each datacenter subsystems,
our data lacks information on the network capacity of each
datacenter, but contains the network demands expressed as
volume of transfers in MB/s.
Resource usage of PMs and VMs – In addition to the
resource capacity, we study whether the workload intensity
of each resource affects the server failure. We use the CPU
utilization [%], memory utilization [%], disk utilization [%]
and network bandwidth [MB/s], collected as weekly averages.
VM consolidation level – Consolidation level refers to the
number of VMs sitting on a hosting platform at a particular
instance in time. We collect both the consolidation of VM
failure instances and the average consolidation of VMs over
the entire year.
VM on/off
frequency – Using the 15-min data of VM
resource usages, we are able to track how frequently VMs
are turned on and off in a two-month observation period,
speciﬁcally March-April 2013.
C. Limitations
Our dataset does not contain information about the physical
location of the servers, the hosting platforms for VMs, and
the datacenters layouts. Thus, we are unable to provide a
precise spatial dependency of server failures, especially across
different systems. Because of less accurate descriptions and
resolutions for some of the tickets, there is an unbalanced
distribution of tickets within the six crash categories, with
those classiﬁed as ”other” representing 53% of the dataset.
Another weakness of this study is the bias selection, as
we choose and analyze the datacenter subsystems that have
the highest clarity and consistency in the description, and
especially the resolution, of problem tickets. Finally, although
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
 1.8
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
PM
VM
s
e
r
u
l
i
a
F
f
o
n
o
i
t
c
a
r
F
.
l
u
m
u
C
All
Sys I Sys II Sys III Sys IV Sys V
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
PM
Gamma α=0.29 β=119.60 PM
VM
Gamma α=0.44 β= 84.28 VM
 0
 50
 100
[days]
 150
 200
 100
]
%
[
s
e
r
u
l
i
a
F
f
o
n
o
i
t
c
a
r
F
 80
 60
 40
 20
 0
Fig. 2. Weekly failure rates for PMs and VMs over one year.
Fig. 3. CDF of inter-arrival times between failures of VMs and PMs.
the one-year observation period of our analysis is rather short,
it provides reliable ﬁndings as our results match well with
previous related studies.
IV. OVERVIEW ON PM AND VM FAILURE
The very ﬁrst objective of this study is to answer simple
questions, such as if VMs fail more often, take longer to repair,
or exhibit different failure dependencies than PMs. To this end,
we start by providing an overview on the server failure rates,
distribution of times between failures, distribution of repair
times, and failure dependencies over time, as well as over
servers. In addition to the statistics computed over the entire
population of servers and tickets, we also present the statistics
relative to each system and the ﬁne-grained failure classes.
A. Failure Rate
Our ﬁrst step is to compare the frequency of VM and PM
failures, using weekly and monthly failure rates over one year
observation period. To compute the weekly failure rate of
a certain system, we use the number of failures divided in
a week by the number of servers belonged to that system.
Fig. 2 summarizes the weekly failure rates of PMs and VMs,
computed over the entire server population and subsystems.
Each bar depicts the mean weekly failure rate and its 25th
and 75th percentile. One can clearly see that PMs have higher
failure rates for the entire observed population, as well as
for most of subsystems, except Sys IV. Note that due to the
low number of VMs associated with Sys II and the lack of
crash tickets, no bar is drawn for its VM failure rate. When
looking at the entire population (i.e., bars denoted by All),
PMs have higher failure rates than VMs roughly by 40%, i.e.,
0.005 vs. 0.003 . Since we only consider stand-alone PMs
and exclude the hosting boxes of VMs, we rule out a certain
dependency between VM and PM failures. In fact, if we would
consider the failure of the hosting boxes as well, the failure
rate of PMs would only further increase. Such a ﬁnding comes
as a pleasant news that advocates VM deployment and the
paradigm of cloud computing.
B. Inter-failure Times
Understanding the inter-failure times is crucial for reliability
modeling and useful for the design of fault-tolerant systems. In
this subsection, we are interested in understanding the inter-
failure times from a single server’s view, as well as from a
datacenter operator’s perspective. For a single server, we study
only the time between failures that affect particular servers,
i.e., PMs vs. VMs, while for operators we focus on failure that
affects any server in six failure classes. Note that we collect
no inter-failure times for servers that only fail once.
We ﬁrst present the distribution of inter-failure times of
single VMs and PMs and plot their PDF/CDFs in Fig. 3. As
an initial observation, we note that VMs and PMs have very
similar distributions, as seen by the almost two overlapped
lines. Zooming further, one can observe that roughly 80% of
VMs have slightly higher inter-failure times than PMs, shown
in the starting part of the CDF, i.e., roughly ranging between
0 and 100 days. This observation resonates well with our
previous ﬁnding that VMs have lower failure rates than PMs.
However, the tail part of the distribution corresponding to 20%
of the VMs and PMs, i.e., beyond 100 days, shows that PMs
have slightly longer inter-failure times. A possible explanation
to such an inconsistent observation to the failure rate presented
in the previous subsection is that roughly 60% of VMs have
only single failure during the one-year observation period,
and thus no information about their inter-arrival times can be
collected.
We also provide statistical ﬁtting for both types of servers
in Fig. 3. As shown by the long tails in the CDF, we choose a
set of statistical distributions, i.e., Weibull, Gamma, and Log-
normal, that are well known for describing the high variability
due to tails. Due to the lack of space, we only present the best
ﬁtting results, i.e., Gamma distribution. Similar to previous
analysis for HPC systems [4], inter-failure times of PMs can
be best captured by the Gamma distribution with parameters
described in the ﬁgure. Most importantly, inter-failure times
of VMs can be best ﬁt by the Gamma distribution, as well,
with the mean being 37.22 days.
To better understand how different root causes affect inter-
failure times, we compute their respective mean and median
5555
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
MEAN AND MEDIAN OF INTER-FAILURE TIMES IN DAYS, BY DIFFERENT
ROOT CAUSES: OPERATOR’S VIEW V.S. SINGLE SERVER VIEW
TABLE III
Operator view: time between failures per class[days]
HW
9.21
3.61
Net
10.27
5.22
average
median
Signer server view: time between failures per server per class[days]
3.63
0.51
7.6
1.00
Power
Reboot
SW
2.84
0.32
Other
1.12
0.24
average
median
HW
59.46
39.85
Net
65.68
45.22
Power
57.60
10.03
Reboot
54.59
26.94
SW
21.58
8.00
Other
30.01
8.99
across different resolution classes, as seen relative to particular
failure classes and to a single server. We report the results
in Table III (top and bottom, respectively). As expected, the
inter-failure times of all classes seen by datacenter providers
are much shorter than inter-failure time seen by servers. Both
mean and median of software related failures are signiﬁcantly
lower than other classes by a factor of 2-3 times, indicating
that software is less reliable from both the perspective of
the datacenter provider and of the server. Failures caused
by network have the highest inter-failure times, showing that
datacenter providers and servers experience network failures
roughly every 10 and 66 days, respectively. When looking at
the failures caused by power outages, one can see that their
inter-failure times are lower when compared to network and
hardware crashes. This is explained by the fact that incidents
classiﬁed as power failures do not only refer to unexpected
power outages, but scheduled ones as well.
C. Repair Times
Another important failure behavior to characterize is the
repair times. Let us take the time difference between the ticket
issuing time and closing time as the repair time required to
resolve failure events in tickets. We separate the repair times
measured in hours between VMs and PMs and depict their
distributions in Fig. 4. Note that the repair times represent
actual down time, including the queuing time, deﬁned as the
interval between the ticket generation and the start of repair.
Usually, the queuing time in the case of server failure is very
short, due to its urgency. As indicated by a lower CDF line,
repair times of PMs are signiﬁcantly higher compared to VMs,
with mean repair times being 38.5 and 19.6 hours respectively.
The reason behind is that a signiﬁcant percentage of VM
failures, i.e., roughly 35%, are caused by unexpected reboots,