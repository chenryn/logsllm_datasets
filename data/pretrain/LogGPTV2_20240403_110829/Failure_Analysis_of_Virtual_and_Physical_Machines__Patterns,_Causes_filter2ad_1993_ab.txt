### Analysis of Server Failures in Commercial Datacenters

#### A. Failure Classification and Distribution
We analyze server failures across several categories, including software, hardware, reboot, and power failure classes, while excluding the unclassified (other) failure class. The "other" category accounts for 53% of all tickets, distributed as follows: Sys I has 35%, Sys II has 68%, Sys III has 68%, Sys IV has 61%, and Sys V has 29%.

When excluding "other" failures, the most common issues are related to software and reboot, accounting for 31% of all tickets. For subsystems Sys I-IV, software problems are the primary cause of server failures, representing 12-22% of crashes. Reboot issues are the second most frequent cause, affecting 8-29% of all subsystems, except for Sys II, where it is only 3%.

Hardware and network issues are more prevalent in Sys I and Sys II, accounting for 26% and 13% of failures, respectively. In contrast, these issues are minimal in Sys II, with fewer than 20 incidents over the observation period, or 1%. Power outages are generally not a significant cause of failure, except for Sys V, where they account for 29% of failures, suggesting that Sys V servers are not co-located with the other systems.

#### B. Ticket Analysis and Resource Mapping
From the identified crash tickets, we extract the server IDs on which the events occurred. We then collect resource capacity and usage data for these servers to map the crash tickets to server characteristics. It is important to note that ticket generation and resolution involve human intervention, which may introduce errors. We have made efforts to sanitize the data through careful human review and interpretation.

#### C. Measurements of Interest
Our goal is to identify factors that determine failures in physical machines (PMs) and virtual machines (VMs) in commercial datacenters. Key measurements include:

1. **Failure Rate**: The number of failures per server, analyzed at daily, weekly, and monthly granularities over a one-year period.
2. **Random and Recurrent Failure Probabilities**: The probability of a server experiencing at least one failure and the likelihood of recurrent failures within 24 hours, a week, and a month.

We also consider the following server attributes:
- **Resource Capacity and Usage**: CPU, memory, disk, and network.
- **VM Age**: Defined as the time from the VM's first appearance in the monitoring database to the failure event.
- **Consolidation Level**: The number of VMs on a hosting platform at a given time.
- **VM On/Off Frequency**: Tracked using 15-minute intervals over a two-month period (March-April 2013).

#### D. Limitations
Our dataset lacks information on the physical location of servers, hosting platforms, and datacenter layouts. This limits our ability to provide precise spatial dependencies of server failures. Additionally, there is an imbalance in ticket distribution, with 53% classified as "other." Our analysis focuses on subsystems with clear and consistent ticket descriptions and resolutions, which introduces a selection bias. Despite these limitations, our findings align well with previous studies.

### Overview of PM and VM Failures

#### A. Failure Rate
We compare the frequency of VM and PM failures using weekly and monthly failure rates over a one-year period. PMs generally have higher failure rates, with the exception of Sys IV. The overall failure rate for PMs is approximately 40% higher than for VMs (0.005 vs. 0.003). This finding supports the reliability of VMs and the cloud computing paradigm.

#### B. Inter-failure Times
Understanding inter-failure times is crucial for reliability modeling. We analyze these times from both a single server's perspective and a datacenter operator's view. The inter-failure times for VMs and PMs are similar, with 80% of VMs having slightly higher inter-failure times (0-100 days), but PMs having longer tails beyond 100 days. The Gamma distribution best fits the inter-failure times, with means of 37.22 days for VMs and 119.60 days for PMs.

#### C. Repair Times
Repair times, defined as the interval between ticket issuance and closure, are significantly higher for PMs (mean 38.5 hours) compared to VMs (mean 19.6 hours). This difference is partly due to the high percentage of VM failures caused by unexpected reboots (35%).

By analyzing these metrics, we aim to provide a comprehensive understanding of server failure behavior in commercial datacenters, supporting the development of more reliable and efficient systems.