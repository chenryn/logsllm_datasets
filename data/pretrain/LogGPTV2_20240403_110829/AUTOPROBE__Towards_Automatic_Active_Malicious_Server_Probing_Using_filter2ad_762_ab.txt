e.g., InternetOpenUrl on line 8 in Figure 5. For each branch it
encounters, it performs a backwards taint analysis on the eflags
register to check if it has been inﬂuenced by the output of a system
Figure 4: System architecture of AUTOPROBE.
requests from the malware to the Internet. To incite the malware to
start a C&C connection, if the DNS resolution fails, the DNS proxy
creates a dummy response that points to a sinkhole server. For other
TCP and UDP trafﬁc AUTOPROBE uses whitelists to determine if
the connection is considered benign and should not be analyzed
(e.g., connection to top Alexa sites used by malware to check for
connectivity) or if it is a C&C connection.
Probe generation. The probe generation phase analyzes the logic
that the malware uses for (1) selecting the port to which the
request is sent, and (2) generating the request. Both steps leverage
backwards taint analysis, dynamic slicing, and symbolic execution
techniques. Using these techniques AUTOPROBE identiﬁes how
the port parameter passed to the socket function and the buffer
passed to the function that sends the request (e.g., send) are
generated from the output of prior system calls. For each variant
part in the request (or for the port number) the goal of this phase is
to output a regeneration slice that can produce a new value based
on the local host environment and the target’s address. Since the
malware may obtain the value of a variant ﬁeld using some multi-
path logic, not fully observable in a single execution, we develop
a control-ﬂow-based exploration technique that ﬁnds all paths that
affect the generation of a variant ﬁeld. We detail probe generation
in Section 3.
Classiﬁcation function construction. To build the classiﬁcation
function AUTOPROBE analyzes the logic the malware uses to
validate the received response. Intuitively, invalid responses from
target servers that do not belong to the malware family should fail
the validation and force the malware to behave differently, e.g.,
close the connection or resend the request. The goal is ﬁnding an
effective symbolic equation for classifying a server’s response.
If during malware execution the C&C servers that the malware
tries to connect were all down, AUTOPROBE uses a combination
of two techniques: response fuzzing and symbolic execution explo-
ration.
In the case when the malware execution phase captured
at least one response from a remote server, AUTOPROBE tries to
identify if the response is from a C&C server or other type of
benign server, e.g., a sinkhole or a server that happens to be reusing
the IP address previously assigned to a C&C server. For this it
compares the malware’s processing of the response from the remote
Probe GenerationClassification Function ConstructionMalware BinariesMalware ExecutionExecution TracesPort Selection FunctionsProbe Generation FunctionClassification FunctionsMalicious ServersTarget AddressesProbingInternetcall.
If it is not inﬂuenced then it keeps processing backwards
until it ﬁnds the next branch. When it ﬁnds a branch that has
been inﬂuenced by the output of a system call (line 3)1 it forces
the system call to generate an alternative result, i.e., it forces
the conditional to take the branch not explored in the trace.
In
our example, if in the original trace RegOpenKeyEx returned
SUCCESS, it forces the function to return FAILURE so that the
other execution branch is executed. This process stops when the
beginning of the execution is reached or a conﬁgurable maximum
number of system-call-inﬂuenced branches has been found (100 by
default). Control-ﬂow-based exploration is detailed in Algorithm 1.
Θ: Trace
ins: instruction in trace
Φ: Set of Instruction of Conditional Branches
∆: Set of Labeled System Call Output Memory/Register
T : Set of Tainted Memory/Register
F : Set of System Calls Affecting Control Flow
req: Request Sent by Malware
for insi in Θ do
if insi in Φ then
eﬂags → T
Backward Taint eﬂags
if tainted ∈ ∆ then
Record System Call into F
Clean eﬂags
end
end
end
for f un in F do
for output:oi of f un’s outputs do
if oi changes control ﬂow then
Rerun malware
Enforce oi for f un along execution
Collect new trace Θi Collect new reqi
end
end
end
Algorithm 1: Algorithm for Control-ﬂow-based Exploration
3.2 Trace Analysis
The analysis of an execution trace that produced a network
request comprises 3 steps: identify the variant bytes in the request
and the target port, recover the semantics of variant bytes in the
request, and generate a regeneration slice for the variant bytes in
the request and the port.
Identify variant parts and their semantics. The request is
commonly a combination of invariant and variant bytes. To identify
variant bytes in the request AUTOPROBE applies dynamic slicing to
each of the bytes in the request starting from the function that sends
the request. Note that while each byte slice is independent they can
be performed in parallel on a single backwards pass on the trace for
efﬁciency. If the slice ends in a ﬁxed constant such as an immediate
value or a constant in the data section then the byte is considered
invariant.
If the slice ends in the output of an API call with
known semantics and whose output is inﬂuenced by a system call
(e.g., rand), it is considered variant. In this case, AUTOPROBE
clusters consecutive bytes inﬂuenced by the same API call (e.g.,
all consecutive bytes in the request inﬂuenced by rand()) into
variant ﬁelds. Then it labels those variant ﬁelds using the semantic
information on the API call collected from public repositories
(e.g., MSDN). Some examples of semantic labels are time, ip,
random, and OS version. Currently, AUTOPROBE has semantics
information for over 200 Windows system and library calls. The
handling of the port selection is similar but it starts at the function
1Or an API call known to perform a system call
gOpenKeyEx
like Re-
that selects the port (e.g., connect, sendto) and since the port
is an integer value, AUTOPROBE slices for all bytes that form the
integer simultaneously.
Reconstruction slices. For each variant ﬁeld in the request the
probe construction function captures how the variant ﬁeld needs
to be updated as a function of the scanner’s environment (e.g., the
current time). For this, AUTOPROBE applies dynamic slicing on
the previously identiﬁed variant bytes. The slice contains both data
and control dependencies. For control dependencies, AUTOPROBE
conservatively includes in the slice the eflags register value
for each branch instruction it encounters that may inﬂuence the
generation of the variant bytes. The slice ends when all variant
bytes are traced back to some semantic-known system calls or the
trace start is reached. The slice is a program that can be re-executed
using the current local environment (e.g., local IP, MAC address, or
time) to reconstruct the ﬁeld value.
4. CLASSIFICATION FUNCTION
CONSTRUCTION
To build the classiﬁcation function, AUTOPROBE conducts dy-
namic binary analysis on the malware’s response handling to ex-
tract a set of symbolic equations. Figure 6 depicts the architecture
of the classiﬁcation function construction. The intuition behind
this phase is that the malware’s processing of a response typically
comprises two widely different logic to handle valid and invalid
responses (without differentiating them the malware could be
controlled by arbitrary messages, which is certainly not desirable
by the malware author).
For example, if the response is considered valid, the malware
may continue its communication with the remote C&C server, but
if considered invalid it may close the communication or re-send
the previous request. To verify the validity of a response, the
malware parses it and checks the values of some selected ﬁelds.
Such validation checks are branches that depend on the content of
the response. Each check can be captured as a symbolic formula
and their conjunction can be used as a classiﬁcation function.
Figure 6 shows the workﬂow of the classiﬁcation function
construction. First, it collects different responses, which can come
from live C&C servers the malware contacted during execution
or be produced by the fuzzing module. Second, it executes the
malware (devil icon) with those responses, applying symbolic
execution and path exploration analysis to identify a valid response.
Last, it generates the classiﬁcation function for each valid request
and response repair.
The remainder of this section describes the classiﬁcation func-
tion construction when a C&C response was obtained during
malware execution, which is illustrated in the left side of Figure 6
(Section 4.1) and when no response is available, which is illustrated
in the right side of Figure 6 (Section 4.2).
4.1 With a C&C Response
To distinguish between valid and invalid responses AUTOPROBE
focuses on the differences between validation checks on both types
of responses. For example, a valid response will successfully go
through all validation checks but an invalid response will fail at
least one of those checks producing an execution trace with a
smaller number of content-dependent branches.
This case comprises 3 steps shown in the left part of Figure 6.
First, AUTOPROBE marks as symbolic each byte in the response
received from the server during the original malware execution
and performs symbolic execution on those symbols along the
execution. For each branch inﬂuenced by the input symbols (i.e.,
4.2 Without a C&C Response
The malware may not receive any response from the C&C server
during malware execution.
In this case, AUTOPROBE uses the
approach illustrated on the right part of Figure 6, which comprises
two steps: fuzzing responses and exposing possible malicious logic.
The ﬁrst step is to fuzz the malware with multiple responses.
When the C&C protocol is unknown, the fuzzing uses random
responses.
If the C&C base protocol is known (e.g., HTTP), it
starts with a successful response such as 200 OK and then continues
with other message types. The payload of the response can be
constructed based on responses from some known benign servers or
from arbitrary bytes. The malware should not trigger its malicious
logic for these responses because they are invalid server responses.
For each pair of responses AUTOPROBE calculates the distance η
and uses the pair with largest η as the baseline of the second step.
In the second step, AUTOPROBE conducts forced execution [41]
on all response-sensitive branches. Forced execution is a binary
analysis technique which forces the program to execute a spe-
ciﬁc path, exposing more behaviors. Two limitations of forced
execution are inefﬁciency and that the forced execution may not
be reachable because in a real execution environment the branch
condition cannot be satisﬁed. To solve these issues, AUTOPROBE
combines symbolic execution with forced execution. In particular,
it symbolizes each byte in the response and applies online symbolic
execution. If it ﬁnds any branch that depends on the symbolic byte,
it records the branch. Then, it forces execution of the unexplored
branch. Next, it calculates the η of the original and forced paths
and if η increases it records the symbolic equation for the forced
path. It iteratively continues the exploration ﬁnding all symbolic
equations that increase η. Algorithm 2 details the path exploration
process.
Θ: Execution Trace Execution
Θ0: Execution Trace For Random Response
P : Malicious Program
pc: Instruction Pointer
S: Set of Symbolized Set for Response
Φ: Set of Branches Instruction
Ψ: Output Symbolic Equations Set
Symbolize all bytes in Response
Running Malware P
for eip do
Enable Forward Symbolic Execution if eip ∈ Φ then
if eﬂags symbolized then
Save Execution Snapshot i
Enable Enforced Execution
Revert eﬂags
Disable Enforced Execution
Monitor Execution and Collect Θi
Calculate ηi
if ηi > η0 then
Online solving symbols
if Solvable then
Save Trace Θi
Add Symbolic Equations for Θi to Ψ
Recover to Snapshot i to eip
Continue Execution
end
else
end
end
end
end
end
Algorithm 2: Algorithm for Path Exploration
5. PROBING
Once the ﬁngerprints are generated by AUTOPROBE the next
the Internet) looking for mali-
step is to scan networks (e.g.,
Figure 6: Classiﬁcation function construction architecture.
each validation check), it produces a symbolic expression that
summarizes the check. The symbolic execution stops when the
execution reaches some preselected calls such as closesocket
and exitprocess, or when no validation check is found in the
previous n branches (e.g., n = 50). In addition to the symbolic
formula, AUTOPROBE also outputs a θ1 forward slice containing
all instructions that operate on symbolic inputs.
Second, AUTOPROBE repeats the previous step but this time on
a randomly generated (i.e., invalid) response.
If the C&C base
protocol is known (e.g., HTTP) rather than a random response
AUTOPROBE uses a generic error message (e.g., an HTTP 404
response). The outcome is another symbolic expression and a θ2
forward slice.
Third, AUTOPROBE determines if the θ1 and θ2 slices capture
the same logic. For this, it aligns them and produces a δ slice,
which records the instruction differences. Then it computes the
distance between both slices η as:
η =
θ1
θ2
=
ωbnΣbn1 + ωf nΣf n1
ωbnΣbn2 + ωf nΣf n2
where bn and f n are respectively the number of unique code blocks
and unique system calls in δ. Since malware mainly uses system
calls to conduct malicious behaviors, the ωf n weight is set higher
than ωbn, to give preference to unique system calls.
If η is below a predeﬁned threshold m (experimentally set
to 10),
the response is discarded since it is handled similarly
to the random response and thus is likely invalid. Otherwise,
AUTOPROBE considers both executions different and extracts the
symbolic constraints, which differentiate θ1 and θ2, as two sets of
equations, St and Sn, representing the validation checks results
for valid and invalid responses. This step discards unnecessary
symbolic equations, reducing the classiﬁcation overhead.
During probing, AUTOPROBE compares the response from a
target server with these two sets of symbolic equations. It deter-
mines that the target server is malicious if the response satisﬁes all
symbolic expressions in St and none in Sn.
ServerFuzzing ModuleResponse?NYSymbolic ExecutionDifferent?Done?Type II: Symbolic EquationsType I: Symbolic EquationsFuzzing ModuleSemantic LabelsRequestsRandom ResponsesPath ExplorationBaseline TracesResponseTracescious servers. For TCP ﬁngerprints, the scanner ﬁrst performs a
horizontal SYN scan to identify hosts with the target port open.
For each target host listening on that port, the scanner uses the
slices to regenerate the values of the state-dependent ﬁelds in
the request, sends the updated request to the target, and records
its response. UDP ﬁngerprints are handled similarly except that
horizontal scanning is not needed.
Our response classiﬁcation module takes as input the symbolic
equations in the ﬁngerprint and the concrete target response, and
conducts symbolic-equation-based matching.
If the request is
generated from our non-response analysis, the detection result is
a suspicious score,
λ =
# of matched equations
# of equations
The higher λ,
the more likely the target server is malicious.
Otherwise, if the request is generated from concrete (live) server’s
response, we require the response to satisfy all the symbolic
equations to declare detection.
6. EVALUATION
In this section, we ﬁrst evaluate AUTOPROBE for generating
ﬁngerprints of real-world malware samples. Then, we use the
ﬁngerprints to scan for malicious servers.
Malware collection. We collect recent malware from 56 fam-
ilies broken into two datasets. Dataset I contains 37 popular and
notorious malware families including Sality [13], ZeroAccess [40],
Ramnit [30], Bamital [4], and Taidoor [34]. We are able to
collect 10 different variants (with different MD5) for each family
from public malware repositories [22, 27], thus making a total of
370 malware binary samples in Dataset I. Dataset II contains 19
malware families used by CYBERPROBE. We use Dataset II to
compare the accuracy of the ﬁngerprints produced by AUTOPROBE
with the ones produced by CYBERPROBE.
Malware execution. we run the malware for 5 minutes each on
a virtual machine with Intel Core Duo 1.5 GHz CPU and 8 GB
memory. Each run outputs an execution trace that serves as the
starting point for the ﬁngerprint generation.
Scanning setup. We use 5 machines for probing. All machines
run GNU/Linux Ubuntu 12.1 LTS with dual core 2.2 GHz CPUs
and the memory conﬁguration ranges from 2 GB to 16 GB.
6.1 Evaluation of Probe Generation
In Table 1, we summarize the results from probe generation.
We collect malware’s execution/network traces and conduct the
analysis. First, AUTOPROBE analyzes the network traces, extracts
all the malware’s network requests, and ﬁlter out those requests
sent to domains in the Alexa top 10,000 list [2]. The number
of Remaining/Original requests are shown in Table 1 in the R/O
column. Then, for each dataset, the table splits the malware into
two groups corresponding to whether at least one request received
a response from a remote server (ResponseSeen), or all requests
failed to receive a response (NoResponse). For each group, the
table shows the number of requests produced by the malware in the
group during the executions and the number of probes produced
by AUTOPROBE, split into probes that contain some variable parts
and those that have only constant parts. The last column shows the
maximum number of probes that CYBERPROBE can produce for
the group.
All requests are HTTP and on average it takes AUTOPROBE 13.2
minutes to analyze/process one execution trace, relatively slow but
a reasonable cost for off-line analysis tools.
AUTOPROBE generated a total of 105 ﬁngerprints/probes for all
56 malware families in the two datasets. Since multiple requests
may be generated by the same execution path, the total number
of probes is smaller than the number of requests captured on the
network. We also observe that the majority of generated probes
contain some variable parts. This means dynamic binary analysis
enables AUTOPROBE to extract more complete probe generation
functions than network-based approaches, because the variable
parts in the probe generation functions provide higher coverage.
Note that on both datasets, AUTOPROBE can generate ﬁnger-
prints for all the malware, even those with no response, for which
CYBERPROBE cannot. This demonstrates a clear advantage of
AUTOPROBE. For the samples with a response in Dataset II,
CYBERPROBE is able to generate a ﬁngerprint similar to AUTO-
PROBE. However, for 57% of those, AUTOPROBE produces probes
construction functions with variable ﬁelds rather than concrete
probes in CYBERPROBE. Thus, AUTOPROBE probe construction
functions are potentially more accurate. We also ﬁnd 4 cases in
which requests clustered together by CYBERPROBE are indeed
generated by different logic in the malware. Thus, they should have
been considered different as their responses are not guaranteed to
have the same format.
6.2 Evaluation of Classiﬁcation Function
In this section, we ﬁrst verify how our heuristics of classiﬁcation
function work in the real world, i.e., whether malware behaves
differently when fed with valid and invalid responses. To verify
that, we extract all 76 probes that trigger responses from the live
remote servers. We also generate 76 random responses, which
comprise of HTTP 200 response code, a common HTTP header
and some arbitrary bytes in the payload. We feed our generated
responses to the malware and compare the malware execution
with the cases when the valid response from live remote servers
is received. Among all 76 test cases, we ﬁnd that in 71 cases
(93%) malware has noticeable behavior differences (malware will
typically execute over 10 more system calls and over 50 more
code blocks when receiving valid responses). Then we manually
examine the remaining 5 exceptional cases.