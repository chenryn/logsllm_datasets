title:Model Inversion Attacks that Exploit Confidence Information and Basic
Countermeasures
author:Matt Fredrikson and
Somesh Jha and
Thomas Ristenpart
Model Inversion Attacks that Exploit Conﬁdence Information
and Basic Countermeasures
Matt Fredrikson
Carnegie Mellon University
Somesh Jha
University of Wisconsin–Madison
Thomas Ristenpart
Cornell Tech
ABSTRACT
Machine-learning (ML) algorithms are increasingly utilized
in privacy-sensitive applications such as predicting lifestyle
choices, making medical diagnoses, and facial recognition. In
a model inversion attack, recently introduced in a case study
of linear classiﬁers in personalized medicine by Fredrikson
et al. [13], adversarial access to an ML model is abused
to learn sensitive genomic information about individuals.
Whether model inversion attacks apply to settings outside
theirs, however, is unknown.
We develop a new class of model inversion attack that
exploits conﬁdence values revealed along with predictions.
Our new attacks are applicable in a variety of settings, and
we explore two in depth: decision trees for lifestyle surveys
as used on machine-learning-as-a-service systems and neural
networks for facial recognition. In both cases conﬁdence val-
ues are revealed to those with the ability to make prediction
queries to models. We experimentally show attacks that are
able to estimate whether a respondent in a lifestyle survey
admitted to cheating on their signiﬁcant other and, in the
other context, show how to recover recognizable images of
people’s faces given only their name and access to the ML
model. We also initiate experimental exploration of natural
countermeasures, investigating a privacy-aware decision tree
training algorithm that is a simple variant of CART learn-
ing, as well as revealing only rounded conﬁdence values. The
lesson that emerges is that one can avoid these kinds of MI
attacks with negligible degradation to utility.
1.
INTRODUCTION
Computing systems increasingly incorporate machine learn-
ing (ML) algorithms in order to provide predictions of lifestyle
choices [6], medical diagnoses [20], facial recognition [1],
and more. The need for easy “push-button” ML has even
prompted a number of companies to build ML-as-a-service
cloud systems, wherein customers can upload data sets, train
classiﬁers or regression models, and then obtain access to
perform prediction queries using the trained model — all
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813677 .
over easy-to-use public HTTP interfaces. The features used
by these models, and queried via APIs to make predictions,
often represent sensitive information. In facial recognition,
the features are the individual pixels of a picture of a per-
son’s face. In lifestyle surveys, features may contain sensitive
information, such as the sexual habits of respondents.
In the context of these services, a clear threat is that
providers might be poor stewards of sensitive data, allow-
ing training data or query logs to fall prey to insider at-
tacks or exposure via system compromises. A number of
works have focused on attacks that result from access to
(even anonymized) data [18,29,32,38]. A perhaps more sub-
tle concern is that the ability to make prediction queries
might enable adversarial clients to back out sensitive data.
Recent work by Fredrikson et al. [13] in the context of ge-
nomic privacy shows a model inversion attack that is able
to use black-box access to prediction models in order to es-
timate aspects of someone’s genotype. Their attack works
for any setting in which the sensitive feature being inferred
is drawn from a small set. They only evaluated it in a single
setting, and it remains unclear if inversion attacks pose a
broader risk.
In this paper we investigate commercial ML-as-a-service
APIs. We start by showing that the Fredrikson et al. at-
tack, even when it is computationally tractable to mount, is
not particularly eﬀective in our new settings. We therefore
introduce new attacks that infer sensitive features used as
inputs to decision tree models, as well as attacks that re-
cover images from API access to facial recognition services.
The key enabling insight across both situations is that we
can build attack algorithms that exploit conﬁdence values
exposed by the APIs. One example from our facial recogni-
tion attacks is depicted in Figure 1: an attacker can produce
a recognizable image of a person, given only API access to a
facial recognition system and the name of the person whose
face is recognized by it.
ML APIs and model inversion. We provide an overview
of contemporary ML services in Section 2, but for the pur-
poses of discussion here we roughly classify client-side access
as being either black-box or white-box. In a black-box setting,
an adversarial client can make prediction queries against a
model, but not actually download the model description.
In a white-box setting, clients are allowed to download a
description of the model. The new generation of ML-as-
a-service systems—including general-purpose ones such as
BigML [4] and Microsoft Azure Learning [31]—allow data
owners to specify whether APIs should allow white-box or
black-box access to their models.
1322Our new estimator, as well as the Fredrikson et al. one,
query or run predictions a number of times that is linear
in the number of possible values of the target sensitive fea-
ture(s). Thus they do not extend to settings where features
have exponentially large domains, or when we want to invert
a large number of features from small domains.
Extracting faces from neural networks. An example
of a tricky setting with large-dimension, large-domain data
is facial recognition:
features are vectors of ﬂoating-point
pixel data.
In theory, a solution to this large-domain in-
version problem might enable, for example, an attacker to
use a facial recognition API to recover an image of a person
given just their name (the class label). Of course this would
seem impossible in the black-box setting if the API returns
answers to queries that are just a class label. Inspecting fa-
cial recognition APIs, it turns out that it is common to give
ﬂoating-point conﬁdence measures along with the class label
(person’s name). This enables us to craft attacks that cast
the inversion task as an optimization problem: ﬁnd the input
that maximizes the returned conﬁdence, subject to the clas-
siﬁcation also matching the target. We give an algorithm for
solving this problem that uses gradient descent along with
modiﬁcations speciﬁc to this domain. It is eﬃcient, despite
the exponentially large search space: reconstruction com-
pletes in as few as 1.4 seconds in many cases, and in 10–20
minutes for more complex models in the white-box setting.
We apply this attack to a number of typical neural network-
style facial recognition algorithms, including a softmax clas-
siﬁer, a multilayer perceptron, and a stacked denoising auto-
encoder. As can be seen in Figure 1, the recovered image
is not perfect. To quantify eﬃcacy, we perform experiments
using Amazon’s Mechanical Turk to see if humans can use
the recovered image to correctly pick the target person out of
a line up. Skilled humans (deﬁned in Section 5) can correctly
do so for the softmax classiﬁer with close to 95% accuracy
(average performance across all workers is above 80%). The
results are worse for the other two algorithms, but still beat
random guessing by a large amount. We also investigate re-
lated attacks in the facial recognition setting, such as using
model inversion to help identify a person given a blurred-out
picture of their face.
Countermeasures. We provide a preliminary exploration
of countermeasures. We show empirically that simple mech-
anisms including taking sensitive features into account while
using training decision trees and rounding reported conﬁ-
dence values can drastically reduce the eﬀectiveness of our
attacks. We have not yet evaluated whether MI attacks
might be adapted to these countermeasures, and this sug-
gests the need for future research on MI-resistant ML.
Summary. We explore privacy issues in ML APIs, showing
that conﬁdence information can be exploited by adversar-
ial clients in order to mount model inversion attacks. We
provide new model inversion algorithms that can be used
to infer sensitive features from decision trees hosted on ML
services, or to extract images of training subjects from facial
recognition models. We evaluate these attacks on real data,
and show that models trained over datasets involving survey
respondents pose signiﬁcant risks to feature conﬁdentiality,
and that recognizable images of people’s faces can be ex-
tracted from facial recognition models. We evaluate prelim-
inary countermeasures that mitigate the attacks we develop,
and might help prevent future attacks.
Figure 1: An image recovered using a new model in-
version attack (left) and a training set image of the
victim (right). The attacker is given only the per-
son’s name and access to a facial recognition system
that returns a class conﬁdence score.
Consider a model deﬁning a function f that takes input a
feature vector x1, . . . , xd for some feature dimension d and
outputs a prediction y = f (x1, . . . , xd).
In the model in-
version attack of Fredrikson et al. [13], an adversarial client
uses black-box access to f to infer a sensitive feature, say
x1, given some knowledge about the other features and the
dependent value y, error statistics regarding the model, and
marginal priors for individual variables. Their algorithm is
a maximum a posteriori (MAP) estimator that picks the
value for x1 which maximizes the probability of having ob-
served the known values (under some seemingly reasonable
independence assumptions). To do so, however, requires
computing f (x1, . . . , xd) for every possible value of x1 (and
any other unknown features). This limits its applicability
to settings where x1 takes on only a limited set of possible
values.
Our ﬁrst contribution is evaluating their MAP estima-
tor in a new context. We perform a case study showing
that it provides only limited eﬀectiveness in estimating sen-
sitive features (marital inﬁdelity and pornographic viewing
habits) in decision-tree models currently hosted on BigML’s
model gallery [4]. In particular the false positive rate is too
high: our experiments show that the Fredrikson et al. algo-
rithm would incorrectly conclude, for example, that a per-
son (known to be in the training set) watched pornographic
videos in the past year almost 60% of the time. This might
suggest that inversion is not a signiﬁcant risk, but in fact we
show new attacks that can signiﬁcantly improve inversion
eﬃcacy.
White-box decision tree attacks. Investigating the ac-
tual data available via the BigML service APIs, one sees that
model descriptions include more information than leveraged
in the black-box attack.
In particular, they provide the
count of instances from the training set that match each
path in the decision tree. Dividing by the total number of
instances gives a conﬁdence in the classiﬁcation. While a
priori this additional information may seem innocuous, we
show that it can in fact be exploited.
We give a new MAP estimator that uses the conﬁdence
information in the white-box setting to infer sensitive in-
formation with no false positives when tested against two
diﬀerent BigML decision tree models. This high precision
holds for target subjects who are known to be in the training
data, while the estimator’s precision is signiﬁcantly worse
for those not in the training data set. This demonstrates
that publishing these models poses a privacy risk for those
contributing to the training data.
13232. BACKGROUND
Our focus is on systems that incorporate machine learning
models and the potential conﬁdentiality threats that arise
when adversaries can obtain access to these models.
ML basics. For our purposes, an ML model is simply a
deterministic function f : Rd (cid:55)→ Y from d features to a set
of responses Y . When Y is a ﬁnite set, such as the names
of people in the case of facial recognition, we refer to f as a
classiﬁer and call the elements in Y the classes. If instead
Y = R, then f is a regression model or simply a regression.
Many classiﬁers, and particularly the facial recognition
ones we target in Section 5, ﬁrst compute one or more re-
gressions on the feature vector for each class, with the cor-
responding outputs representing estimates of the likelihood
that the feature vector should be associated with a class.
These outputs are often called conﬁdences, and the classi-
ﬁcation is obtained by choosing the class label for the re-
gression with highest conﬁdence. More formally, we deﬁne
f in these cases as the composition of two functions. The
ﬁrst is a function ˜f : Rd (cid:55)→ [0, 1]m where m is a parame-
ter specifying the number of conﬁdences. For our purposes
m = |Y | − 1, i.e., one less than the number of class labels.
The second function is a selection function t : [0, 1]m → Y
that, for example when m = 1, might output one label if
the input is above 0.5 and another label otherwise. When
m > 1, t may output the label whose associated conﬁdence
is greatest. Ultimately f (x) = t( ˜f (x)). It is common among
APIs for such models that classiﬁcation queries return both
f (x) as well as ˜f (x). This provides feedback on the model’s
conﬁdence in its classiﬁcation. Unless otherwise noted we
will assume both are returned from a query to f , with the
implied abuse of notation.
One generates a model f via some (randomized) training
algorithm train. It takes as input labeled training data db, a
sequence of (d+1)-dimensional vectors (x, y) ∈ Rd×Y where
x = x1, . . . , xd is the set of features and y is the label. We
refer to such a pair as an instance, and typically we assume
that instances are drawn independently from some prior dis-
tribution that is joint over the features and responses. The
output of train is the model1 f and some auxiliary informa-
tion aux. Examples of auxiliary information might include
error statistics and/or marginal priors for the training data.
ML APIs. Systems that incorporate models f will do so
via well-deﬁned application-programming interfaces (APIs).
The recent trend towards ML-as-a-service systems exem-
pliﬁes this model, where users upload their training data
db and subsequently use such an API to query a model
trained by the service. The API is typically provided over
HTTP(S). There are currently a number of such services,
including Microsoft Machine Learning [31], Google’s Predic-
tion API [16], BigML [4], Wise.io [40], that focus on analyt-
ics. Others [21,26,36] focus on ML for facial detection and
recognition.
Some of these services have marketplaces within which
users can make models or data sets available to other users.
A model can be white-box, meaning anyone can download
a description of f suitable to run it locally, or black-box,
meaning one cannot download the model but can only make
1We abuse notation and write f to denote both the function
and an eﬃcient representation of it.
prediction queries against it. Most services charge by the
number of prediction queries made [4,16,31,36].
Threat model. We focus on settings in which an adver-
sarial client seeks to abuse access to an ML model API.
The adversary is assumed to have whatever information the
In a white-box setting this means access to
API exposes.
download a model f .
In a black-box setting, the attacker
has only the ability to make prediction queries on feature
vectors of the adversary’s choosing. These queries can be
adaptive, however, meaning queried features can be a func-
tion of previously retrieved predictions. In both settings, the
adversary obtains the auxiliary information aux output by
training, which reﬂects the fact that in ML APIs this data
is currently most often made public.2
We will focus on contexts in which the adversary does
not have access to the training data db, nor the ability to
sample from the joint prior. While in some cases training
data is public, our focus will be on considering settings with
conﬁdentiality risks, where db may be medical data, lifestyle
surveys, or images for facial recognition. Thus, the adver-