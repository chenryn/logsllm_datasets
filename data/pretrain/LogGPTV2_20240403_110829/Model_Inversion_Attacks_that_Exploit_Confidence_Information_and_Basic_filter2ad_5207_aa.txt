# Model Inversion Attacks Exploiting Confidence Information and Basic Countermeasures

**Authors:**
- Matt Fredrikson, Carnegie Mellon University
- Somesh Jha, University of Wisconsin–Madison
- Thomas Ristenpart, Cornell Tech

## Abstract
Machine learning (ML) algorithms are increasingly being used in privacy-sensitive applications such as lifestyle predictions, medical diagnoses, and facial recognition. In a model inversion attack, an adversary with access to an ML model can infer sensitive information about individuals. Recently, Fredrikson et al. [13] demonstrated such an attack on linear classifiers in personalized medicine, revealing genomic information. However, the applicability of these attacks to other settings remains unclear.

In this work, we introduce a new class of model inversion attacks that exploit confidence values provided along with predictions. We explore two specific settings: decision trees for lifestyle surveys in machine-learning-as-a-service (MLaaS) systems and neural networks for facial recognition. Our experiments show that these attacks can estimate whether a respondent admitted to cheating on their significant other and can recover recognizable images of people's faces given only their name and access to the ML model. We also investigate natural countermeasures, including a privacy-aware decision tree training algorithm and rounding confidence values. Our findings suggest that these countermeasures can effectively mitigate the attacks with minimal impact on utility.

## 1. Introduction
Computing systems increasingly incorporate machine learning (ML) algorithms to provide predictions for various applications, including lifestyle choices, medical diagnoses, and facial recognition. The demand for easy-to-use ML has led to the development of MLaaS cloud systems, where customers can upload data, train models, and perform prediction queries over public HTTP interfaces. These models often use features that represent sensitive information, such as individual pixels in facial recognition or personal habits in lifestyle surveys.

A key threat in these services is the potential for providers to mishandle sensitive data, leading to insider attacks or system compromises. Recent work by Fredrikson et al. [13] demonstrated a model inversion attack in the context of genomic privacy, using black-box access to prediction models to estimate aspects of someone's genotype. However, the effectiveness of such attacks in other settings remains uncertain.

In this paper, we investigate commercial MLaaS APIs and show that the Fredrikson et al. attack is not particularly effective in our new settings. We introduce new attacks that exploit confidence values exposed by the APIs to infer sensitive features from decision tree models and recover images from facial recognition services. For example, an attacker can produce a recognizable image of a person given only API access to a facial recognition system and the person's name.

We also explore preliminary countermeasures, such as incorporating sensitive features during decision tree training and rounding reported confidence values, which can significantly reduce the effectiveness of our attacks. Our results highlight the need for further research into ML-resistant methods.

## 2. Background
Our focus is on systems that incorporate ML models and the confidentiality threats that arise when adversaries gain access to these models.

### 2.1. ML Basics
An ML model is a deterministic function \( f: \mathbb{R}^d \rightarrow Y \) that maps \( d \) features to a set of responses \( Y \). When \( Y \) is a finite set, such as names in facial recognition, \( f \) is a classifier, and the elements in \( Y \) are called classes. If \( Y = \mathbb{R} \), then \( f \) is a regression model.

Many classifiers, especially those used in facial recognition, compute one or more regressions on the feature vector for each class, with outputs representing the likelihood that the feature vector should be associated with a class. These outputs are called confidence values, and the classification is obtained by choosing the class label with the highest confidence. Formally, \( f \) is defined as the composition of two functions: \( \tilde{f}: \mathbb{R}^d \rightarrow [0, 1]^m \) and \( t: [0, 1]^m \rightarrow Y \). Here, \( m \) is a parameter specifying the number of confidences, typically \( m = |Y| - 1 \). Classifiers often return both the predicted class and the confidence values.

### 2.2. ML APIs
ML systems provide well-defined application-programming interfaces (APIs) for querying models. MLaaS platforms, such as Microsoft Azure Learning [31], Google’s Prediction API [16], and BigML [4], allow users to upload training data and query trained models over HTTP(S).

Models can be either white-box, where the model description can be downloaded, or black-box, where only prediction queries are allowed. Most services charge based on the number of prediction queries made.

### 2.3. Threat Model
We consider settings where an adversarial client abuses access to an ML model API. In a white-box setting, the adversary can download the model, while in a black-box setting, the adversary can only make adaptive prediction queries. The adversary also has access to auxiliary information, such as error statistics and marginal priors, which are often made public in ML APIs.

We focus on contexts where the adversary does not have access to the training data or the ability to sample from the joint prior, highlighting the confidentiality risks in scenarios involving sensitive data like medical records, lifestyle surveys, or facial images.