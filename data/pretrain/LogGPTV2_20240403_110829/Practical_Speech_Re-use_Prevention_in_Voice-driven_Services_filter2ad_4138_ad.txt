BER/FAR/FRR Location Dis.
0.5m
0.5m 0.2%/0%/0.5% Gas
BER/FAR/FRR
0%/0%/0%
Station
(Car)
1m
4m
0%/0%/0%
1.2%/0%/0.5%
1m
2m
1.2%/0%/ 1%
75%/0% /78%
0.7m
0.5%0%0.5%
• Equalization. Modifies the frequency response of an audio sys-
tem using linear filters. Frequency response is the measure of
the audio output in comparison to the input as a function of
frequency. For example, a 5kHz wave input may have a 4kHz
output response.
• Additive Noise. Adds noise to decrease SNR (signal-to-noise ratio)
so that watermark cannot be extracted successfully.
Each technique is iteratively used until the acoustic nonce cannot
be detected in a speech sample. The removal attempts are repeated
for all speech samples in the dataset mentioned above. Table 3
290RAID ’21, October 6–8, 2021, San Sebastian, Spain
Yangyong Zhang, Sunpreet S. Arora, Maliheh Shirvanian, Jianwei Huang, and Guofei Gu
Table 3: Performance of Microsoft Azure speaker recognition system (similarity score 0 - 1) and speech recognition system (WER) after nonce
removal using two different FHSS schemes. Without nonce removal, the respective average similarity score and WER of the two systems are
0.897 and 5.1% [60]. Lossy compression and equalization nonce removal techniques failed to remove the nonce and the speech samples were
rejected by AEOLUS.
Removal Technique
Resampling
Amplitude Compression
Filtering
Additive Noise
Lossy Compression
Equalization
4-5kHz, 2 Hop., Avg. of 30 samples
1-8kHz, 10 Hop., Avg. of 30 samples
Speaker
Verification
(Score, 0-1)
0.233
0.217
0.623
0.318
Reject
Reject
Speech
Recognition
(WER %)
29.73%
34.31%
17.12%
53.13%
Reject
Reject
Speaker
Verification
(Score, 0-1)
0.170
0.184
0.319
0.192
Reject
Reject
Speech
Recognition
(WER %)
91.83%
63.4%
62.21%
96.34%
Reject
Reject
recognition system by 79.52%, and increased the average word error
rate of the speech recognition system by 33.57%. This shows that
AEOLUS is robust against these removal techniques.
Figure 8: Variation of bit error rate (%) with different distances be-
tween microphone and user (simulated using a playback device) in
different environments: (a) conference room, (b) dining hall, and (c)
in a car parked at a gas station. Given a fixed size car in (c), results
can only be computed upto a distance of 0.7 m.
Figure 9: Impact of the proposed security overlay on prediction con-
fidence of Microsoft Azure Speaker Recognition system in three dif-
ferent environments.
shows that nonce removal techniques are disruptive and ineffec-
tive in removing the acoustic nonce. This is because they reduced
the average recognition performance of the commercial speaker
Figure 10: Impact of distance between the VDS loudspeaker and an
adversary’s recording device at the time of capture on speech re-use.
5.3 Distance between adversary and VDS
speaker.
To launch a speech re-use attack, an adversary first records a user’s
interaction with VDS. In this experiment, we measure how effec-
tively the acoustic nonce is preserved with varying distance be-
tween the VDS speaker and adversary’s recording device (micro-
phone) at the time of capture. Figure 10 shows that this distance
significantly impacts BER. The computed BER is 0% upto a distance
of 4 m when the signal to noise ratio is low. If the captured speech
is being re-used, assuming a lossless audio input to VDS, the entire
nonce can be recovered and used for determining the particular
user interaction session the speech was previously used in.
5.4 Human Perception
5.4.1 Empirical Measurement. In this experiment, we empirically
measure the change in SPL due to the embedded nonce. The distance
between VDS output and the recording device is 1 m in dining hall
and conference room, and 0.7 m in the car. The results (see Figure 11)
0.00.51.01.52.02.53.0Distance (m)020406080100Error Rate (%)Conference RoomDining HallGas Station (Car)0.00.51.01.52.02.53.0Distance (m)0.50.60.70.80.91.01.1Confidence LevelConference Room with NonceConference Room without NonceDining Hall with NonceDining Hall without Nonce0123456Distance (Meter)020406080100Bit Error Rate (%)Eb/N0=10Eb/N0=2Eb/N0=1Eb/N0=0.5291Practical Speech Re-use Prevention in Voice-driven Services
RAID ’21, October 6–8, 2021, San Sebastian, Spain
the noise sound like. This question is designed to verify whether it
is the acoustic nonce that bothers them.
The test samples are uniformly selected at random from a dataset
of 24 speech recordings, 12 recordings each with and without em-
bedded nonce from the three environments. The average length
of the speech samples is 6.4 seconds. To ensure the user study is
unbiased, all samples presented to a participant are from the same
speaker and pertain to the same voice command. After listening
to a speech sample, a participant is asked to rate the noise level of
the speech sample on a scale of 0 - 10 (10 being the highest noise
level). The participant is also instructed to report any noticeable
noise source that affects their experience, and to specify the char-
acteristic of the noise that resulted in this disruption (e.g., human
conversation, train or car noise). The answer is used to ascertain
the source of noise that impacted the usability (e.g., the embedded
nonce).
Results. Table 4 reports the average human perception ratings. The
acoustic nonce does not degrade overall user experience for 94.16%
of speech samples, on average, in the three environments. Since
this does not adequately measure the participant’s perception of
nonce embedding for each sample, per speech sample perception is
also reported. No speech sample with embedded nonce is perceived
to be disruptive by the majority of participants.
We show some example comments from participants who re-
ported that the noise in the audio affects their user experience,
Compared to the other samples, the background voices are much
louder. In fact, a man speaking almost overpowered the voice I is
supposed to listen to. (P-004, G22)
We find some participants reported other noise sources but the
acoustic nonce. For example, P-004 says the people chatting in the
dining hall affected his user experience of VDS.
Car noises, but the singing is also very far away and hard to hear.
(P-072, G3)
Similarly, P-072 is complaining about street noise in Q5.
An echo, or like it is far away. (P-019, G1)
Only 2 audios were reported to contain acoustic nonce (in Q5). For
example, P-019 reported the echo sound and claimed it is affecting
her user experience of VDS. However, we notice that the acous-
tic nonce didn’t affect their transcriptions’ correctness (i.e., Q2 in
Table 5).
Perceived Noise Level. Table 7 shows that the speech samples
containing the acoustic nonce are perceived to have a higher noise
level. The dependent samples paired t-test (at 95% confidence) indi-
cates a statistically significant difference (between samples with and
without nonce) in the conference room and car. However, this dif-
ference is statistically insignificant in the dining hall due to higher
background noise level compared to the conference room and car.
Despite perceiving higher noise level, the majority of participants
do not perceive any speech sample to be disruptive (see Table 4).
2G1, G2, G3 are respectively the participant groups for testing audios collected in the
conference room, dining hall, and gas station.
Figure 11: Loudness of speech with and without embedded nonce
(average value of 5 measurements).
show that the average SPL difference in each environment is less
than 1.4%. Non-parametric Wilcoxon Signed Rank test [58] (at 95%
confidence level) is used to measure the statistical significance of the
difference in SPL before and after acoustic nonce embedding. The
results indicate that the differences are statistically insignificant.
User Study. Next, we study the impact of AEOLUS on VDS usability.
For this, 120 participants (with prior experience using a VDS) from
various demographics are recruited on Amazon Mechanical Turk.
The demographics for the participants are shown in Table 6. There
are more female participants than male participants, and 46.7%
of the participants are between age 25 to 35. Note that this user
study has been approved by the Institutional Review Board (IRB).
The participants were given the option to opt-out at any time. No
personally identifiable information is collected. All data were de-
identified and stored securely.
The survey consists of three parts: device adjustment, prelim-
inary questions, and survey questions. At the beginning of each
survey, we guided the participants to adjust the loudness of their
playback devices to a comfortable level. This is because the par-
ticipants are recruited online, it is difficult to explicitly know or
enforce the setup the participants use. Also, we pre-processed the
recorded audio samples to ensure all audio samples were played
with reasonable and consistent loudness.
Each participant is then asked to enter their demographic infor-
mation. The participant is then instructed to play a test sample and
adjust the playback volume to a comfortable level. Following this,
the participant is presented with a survey with five speech samples
and follow up questions. Included in the five samples is one sample
with only background noise (and no speech) to check if the par-
ticipant is attentive. The participants are asked to provide speech
transcriptions for all samples and the provided transcriptions are
used to check the participants’ attentiveness. Eleven participants
failed to pass this test and hence were excluded. Thus, there are
109 valid participants in this study (n = 109).
Next, each participant is presented with a survey that takes ap-
proximately 10 minutes to complete. Post completion of the survey,
a participant is paid 4 USD as incentive. The survey questions are
designed to ensure bias is not introduced in the results (no leading
questions). We showcase the survey questions for each audio in
Table 5. When designing the questions, we do not ask participants
directly about the imperceptibility. Instead, we ask them whether
they can transcribe the speech command in the audio. Second, we
ask how noisy they find the audio (from 0-10, and 10 is the noisiest
score). Third, we ask if any available noise affects the user experi-
ence. Fourth, if the noise affects their user experience, what does
Conference RoomDining HallGas Station w/ Vehiclename0204060Decibel (dBA)w/ noncew/o nonce292RAID ’21, October 6–8, 2021, San Sebastian, Spain
Yangyong Zhang, Sunpreet S. Arora, Maliheh Shirvanian, Jianwei Huang, and Guofei Gu
Table 4: Perception of speech samples. Overall (%) is the proportion of participants that label speech samples as imperceptible, non-disruptive,
or disruptive. Note that not all 109 valid participants listened to each location’s recording (as mentioned above, random speech samples are
played for each survey, and 11 participants are excluded). Sample (%) is the proportion of speech samples (with nonce) that are determined to
be imperceptible, non-disruptive, or disruptive by the majority of participants.
Location
Conference Room
Dining Hall
Gas Station (Car)
Imperceptible
(Overall/Sample%)
48.1%/50%
47.6%/25%
47.7%/25%
Table 5: User Study Survey Questions.
Question
Q1. Pretend you are using voice-
based service in a dining area,
can you hear the speech in this
audio clearly?
the
transcribe
Q2. Please
speech.
Q3. How noisy is this audio, rate
from 0 - 10? (10 is most noisy)
Q4. Do you find any noise af-
fects your user experience?
Q5. If there are some noises af-
fect your user experience for
the voice service, what is it?
Options
A. Very Clear
B. Clear.
C. It is neither clear
or unclear but I am
fine with it.
D.
I can hear the
speech but it is very
unclear.
E. I cannot hear any
speech.
[Text]: ____
[0-10]: ____
A. I can hear some-
thing, but nothing
bothers me.
B. Yes, there is some-
thing troubles me.
C. Nothing at all
[Text]: ____
Table 6: User study participant demographic information (for all
120 participants).