6)
5)
4) Motion Detection via Phase Variances: In this step,
we analyze the phase variances of each AOI over time
and quantify the corresponding motion amplitude.
Feature Extraction. In this step, we extract features
in both temporal and spatial domains to represent the
motion patterns of each AOI.
Classiﬁer Training. In this step, we let multiple
attackers mimic the victim’s typing process, record
videos of their tablet backsides, and process their
videos to train a classiﬁer.
Keystroke Inference. In this step, we use a classiﬁer
trained in Step 6) to test the collected data from the
victim’s typing process.
Text Inference. In this step, we infer the possible
words by considering meaningful alphabetical combi-
nations using a dictionary and exploit the relationship
between adjacent words to further infer sentences.
7)
8)
In what follows, we present each step in detail.
B. Video Recording and Preprocessing
One or more camcorders can be used to video-record the
tablet backside during the victim’s typing process. For more
accurate inference results, the video should capture the entire
backside of the tablet, and the image resolution should be
sufﬁciently high. In our experiments, we use two camcorders
that focus on the left-half and right-half of the tablet backside,
respectively. By doing so, each camcorder only needs to focus
on a relatively small area with sufﬁciently high resolution to
capture the detailed texture information on the tablet backside.
In our study, we ﬁnd that the following four factors affect
subsequent motion detection.
•
•
•
information. We recommend 1080p601 HD or higher
resolutions and frame rates.
Zoom setting. The zoom setting of the camcorder
is jointly determined by the distance between the
camcorder and the tablet and the lens properties. We
recommend zooming in as much as possible while
capturing the entire tablet backside.
Light condition. The bright light condition can result
in better motion detection, as the imaging component
of camcorders generates larger random noise in low-
light condition that pollutes the motion signal.
Recording angle. The angle between the camcorder
and tablet need be adjusted to capture the entire tablet
backside, which can be easily satisﬁed in practice.
We also video-record the attack scenario layout to measure
the distances and angles between the camcorders and the target
tablet as well as the angle between the tablet and the desk.
This is important for reconstructing the attack scenario later.
In practice, the attacker can take multiple images of the attack
scenario from different angles and estimate the distances and
angles of interest using standard distance and angle estimation
algorithms such as [29].
After obtaining the video of the tablet backside, we man-
ually crop the unwanted part such that the remaining video
contains only the typing process of interest that is easily iden-
tiﬁable in practice. In practice, for the PIN keyboard, since the
user usually needs to press power or home button ﬁrst, the keys
entered subsequently are very likely to be PINs. Similarly, for
the alphabetical keyboard, the typing process can be identiﬁed
by continuous typings (e.g., small arm movements). Finally,
we decompose the cropped video into a series of frames. Note
that video croppings can be automated after more sophisticated
and extensive programming effort.
C. AOIs Detection and Selection
•
Image resolution and frame rate. The image resolution
determines the amount of detailed textural information
that can be obtained from the image and should be
as high as possible. The frame rate is the number
of frames taken within one second, and a higher
frame rate could help capture more detailed motion
After obtaining a series of frames, we proceed to identify
all
the Areas of Interests (AOIs) on the tablet backside,
where each AOI refers to an area containing unique texture
11080p60 denotes that the camcorder can take 1920x1080 videos at a rate
of 60 frames per second.
4
(a) Possible AOIs.
(b) Selected AOIs.
Fig. 4. Possible and selected AOIs on an iPad 2’s backside.
Fig. 5. An example of a selected AOI, AOI-2.
information. In computer vision, the texture information of
an image refers to the relationship of nearby pixels. Given a
frame, we use the textured area detection algorithm [30] to
identify the areas with texture information and select them as
AOIs for subsequent motion detection.
Fig. 4(a) shows an image of the iPad 2 backside, where the
areas enclosed by rectangles are identiﬁed AOIs, which include
the power button, the voice up and down buttons, the silence
button, the backside camera, the ear plug, the loudspeaker,
the logo, the manufacture information, and other decorative
patterns. These AOIs can be used to detect the tablet backside
motion. In practice, almost every tablet’s backside contains
adequate areas with rich texture information. Even if the tablet
is protected by a backside cover like iPad Smart Case, the
backside of the cover itself still contains areas with rich texture
information making it easy to ﬁnd enough AOIs for subsequent
motion detection.
We then select a subset of the AOIs that are near the edges
of the tablet backside and separated from each other, as these
AOIs tend to have larger and more distinctive motions than
others, making them more capable of differentiating the motion
patterns caused by different keystrokes. Fig. 4(b) shows the
backside image of an iPad 2 placed on a holder, where the
areas speciﬁed by rectangles are the selected AOIs.
features at orientation 3 and 4 and less features at orientation 2.
Since the phase variations are proportional to subtle motions,
Fig. 6(c) indicates that more subtle motions can be detected at
orientation 3 and 4. Finally, we obtain a decomposed complex
steerable pyramid for each selected AOI in each frame.
E. Motion Detection via Phase Variances
To estimate the motion for each selected AOI over time,
we ﬁrst compute the pixel-level motion. As in [31], [32],
for each selected AOI, we ﬁrst decompose its frame series
using complex steerable pyramid and then compute the pixel-
level motion from the amplitude and phase of its pixels.
Speciﬁcally, complex steerable pyramid decomposition adopts
a ﬁlter bank to decompose each frame into complex sub-bands
corresponding to each scale and orientation. The complex
steerable pyramid decomposition of a frame at time t at scale
r and orientation θ can be written as
A(t, x, y, r, θ)eiφ(t,x,y,r,θ),
(1)
where x and y are the pixel coordinates in x-axis and y-axis
at scale r, respectively, and A(t, x, y, r, θ) and φ(t, x, y, r, θ)
are the amplitude and phase at coordinate (x, y) of the
decomposition at scale r and orientation θ, respectively.
We then calculate the phase variance at scale r and
orientation θ as
∆φ(t, x, y, r, θ) = (φ(t, x, y, r, θ) − φ(t0, x, y, r, θ)) mod 2π,
(2)
where t0 is the time for any initial frame. According to
[25], the phase variations ∆φ(t, x, y, r, θ) are approximately
proportional to displacements to image structures along the
corresponding scale and orientation.
Finally, we estimate each selected AOI’s motion using
its pixel-level motion. Since the pixel-level phase variance
∆φ(t, x, y, r, θ) is an approximation of the pixel-level motion,
an intuitive way to estimate the motion of the AOI is to sum
the phase variation ∆φ(t, x, y, r, θ) of all its pixels. However,
the pixel-level phase variance approximates the pixel-level
motion only if the area has rich texture information. For areas
with little texture information, the pixel-level phase variance is
random due to background noise. To simultaneously strengthen
the pixel-level phase variation for areas with rich texture
information and weaken the pixel-level phase variation for
areas with little texture information, we compute a weighted
sum of phase variances at scale r and orientation θ as
Φ(t, r, θ) =
A(t, x, y, r, θ)2∆φ,
(3)
(cid:88)
D. Decompositions of Selected AOIs
where A(t, x, y, r, θ) is the measure of texture strength.
x,y
Now we have a series of frames extracted from the cropped
video, each containing the same set of selected AOIs. For each
frame, we use complex steerable pyramid decomposition [28]
to decompose each selected AOI into complex sub-bands. As
an example, Fig. 5 shows an AOI that is part of the Apple
logo, and Fig. 6 shows the image decomposed via complex
steerable pyramid decomposition. More speciﬁcally, Figs. 6(a)
to 6(d) show the real part, imaginary part, phase, and amplitude
of the decomposed image at three scales and four orientations,
respectively. We can see from Fig. 6(c) that the image has more
(cid:88)
(cid:88)
Since a frame is decomposed into multiple scales and
different orientations, we sum the motions for all the scales
and orientations to obtain the estimated motion for the speciﬁc
AOI as
Ψ(t) =
Φ(t, r, θ) =
A(t, x, y, r, θ)2∆φ.
(4)
r,θ
r,θ,x,y
Fig. 7 depicts the motion signals of the apple stem in Fig. 5
during a word-entry process. We can see the typed word with
5
(a) Real parts of the decomposed image at each scale and orientation.
(b) Imaginary parts of the decomposed images at
orientation.
the same scale and
(c) Phase of oriented band-pass images at each scale and orientation.
(d) Amplitude of the decomposed images at the same scale and orientation.
Fig. 6. A 3-scale, 4-orientation complex steerable pyramid representation of AOI-2.
spatial features depict the motion relationship among different
AOIs that is capable of reﬂecting the posture of the tablet.
To extract temporal features, we represent the motion se-
quence of each AOI as a vector that speciﬁes the time-varying
motion amplitude and then derive the following features for
each AOI.
Fig. 7. Motions of the apple stem in Fig. 5 for typing “impersonating”.
thirteen letters each corresponding to a peak in amplitude, i.e.,
a sudden signiﬁcant change in |Ψ(t)|.
F. Feature Extraction
Now we extract temporal and spatial features from selected
AOIs’ motion signals to represent the motion patterns. The
former are obtained from the motion signals’ time domain, and
Skewness. This refers to the third central moment
which measures the asymmetry of the vector.
Kurtosis. This is the fourth central moment which
measures the peakedness or ﬂatness of the vector.
• Maximum motion amplitude. The maximum motion
amplitudes are different for different AOIs.
Relative and absolute differences between maximum
motion amplitudes. Assume that there are n selected
AOIs. We deﬁne a ratio vector which comprises the
ratio of the maximum motion amplitude of the i-th
AOI to that of the (i + 1)-th AOI for all i ∈ [1, n −
1]. We also deﬁne a difference vector comprising the
maximum motion amplitude of the i-th AOI subtracted
by that of the (i + 1)-th AOI for all i ∈ [1, n − 1].
•
•
•
6
Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 4Scale 1, Orientation 1Scale 1, Orientation 2Scale 1, Orientation 3Scale 1, Orientation 4Scale 2, Orientation 1Scale 2, Orientation 2Scale 2, Orientation 3Scale 2, Orientation 4Scale 3, Orient. 1Scale 3, Orient. 2Scale 3, Orient. 3Scale 3, Orient. 40200400600800-250-200-150-100-50050gnitanosrepmAmplitudeTime indexiTo extract spatial features, we denote the motions of all
AOIs by matrix Am×n, where m is the time index, n is the
number of AOIs, and Ai,j is the j-th AOI’s motion amplitude
at time i. We derive the following spatial features.
1-norm. The 1-norm of Am×n is calculated as
We use a multi-class Support Vector Machine (SVM) [33]
with C-SVC type and linear kernel to distinguish different
typed keys. Speciﬁcally, we use the implementation in WEKA
[34] with default parameters. Since we have already obtained
N KM labeled typing samples, we feed them into WEKA to
obtain a trained multi-class SVM classiﬁer.
||Am×n||1 = max
1≤j≤n
|aij| ,
H. Keystroke Inference
m(cid:88)
i=1
In this step, we use the motion data extracted from the
video recording of the victim’s tablet backside and the trained
classiﬁer to obtain a candidate key set for each key the victim
typed. Speciﬁcally, for a backside video capturing the victim’s
typing process, it is processed through Steps 1 to 5 to output 36
features in total, including four skewness features, four kurtosis
features, four maximum motion features, six relative difference
features, six absolute difference features, six Pearson correla-
tion features, one one-norm feature, three two-norm features,
one inﬁnity-norm feature, and one Frobenium-norm feature.
We then use the trained multi-class SVM classiﬁer to predict
one key. Since the distance between two adjacent keys in both
alphabetical and PIN keyboards is very small, it is possible for
the key entered by the victim to be misclassiﬁed as neighboring
keys. We therefore let the SVM classiﬁer output a candidate
key set consisting of all the keys that are no more than h hops
from the predicated key, where h is a parameter determined
by the attacker.
I. Text Inference
In this step, we further infer the entered text by using
a dictionary and a linguistic relationship between adjacent
words. Speciﬁcally, for a word consisting of W letters, we
can obtain W candidate letter sets in the previous step. We
use the “corn-cob” dictionary [35] that contains over 58,000
lower-case English words and is also used by previous work
[21]. First, we list all the combinations of the possible words
and ﬁlter out the combinations that are not in the dictionary. We
then manually select one word from each of the candidate lists
to form a meaningful sentence by considering the linguistic
relationship between adjacent words. As an alternative, given
the candidate word list for each word, we may use a well-
studied n-gram model such as [36] generated from linguistic
statistics to generate candidate sentences.
VI. PERFORMANCE EVALUATION
In this section, we evaluate the performance of VISIBLE
through extensive experiments on a 9.7-inch Apple iPad 2
tablet with iOS 8 and a 7-inch Google Nexus 7 tablet with
Android 4.4. The experiments involved eight participants in
total, and the data collection process has been approved by
Institutional Review Board (IRB) at our institution. We intend
to answer the following ﬁve questions in our evaluations.
1. What is the (single-)key inference accuracy on alpha-
betical and PIN keyboards, respectively?
2. What is the word inference accuracy on the alphabet-
ical keyboard?
Is it possible to infer a victim’s typing sentences?
How do the inference results differ on different tablets
(e.g., an iPad 2 and a Nexus 7)?
3.
4.
7
•
•
•
•
•
which is its maximum absolute column sum.
2-norm. As in [16] we calculate three 2-norm features
from Am×n. Let Ri denote the ith row of Am×n for
all i ∈ [1, m]. The 2-norm of each Ri is given by
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
j=1
||Ri||2 =
|aij|2.
We then extract the mean, maximum, and minimum
from
[||R1||2,||R2||2, . . . ,||Rm||2]T .
Inﬁnity-norm. The inﬁnity-norm of Am×n is
||Am×n||∞ = max
1≤i≤m
|aij| ,
which is its maximum absolute row sum.
Frobenium-norm. The frobenium-norm is calculated as
||Am×n||F =