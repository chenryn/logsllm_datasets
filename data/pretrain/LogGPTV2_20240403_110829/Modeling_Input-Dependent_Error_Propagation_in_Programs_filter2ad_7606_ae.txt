Folkesson et al. [10] investigate the variability of the failure
rates of a single program (Quicksort) with its different inputs.
They decompose the variability into the execution proﬁle, and
its data usage proﬁle. The latter requires the identiﬁcation of
critical data and its usage within the program - it is not clear
how this is done. They consider limited propagation of errors
across basic blocks, but not within a single block. This results
in their model signiﬁcantly underpredicting the variation of
error propagation. Finally, it is difﬁcult to generalize their
results as they consider only one (small) program.
Di Leo et al. [8] investigate the distribution of failure
types under hardware faults when the program is executed
with different inputs. However, their study focuses on the
measurement of the volatility in SDC probabilities, rather than
on predicting it. They also attempt to cluster the variations
and correlate the clusters with the program’s execution proﬁle.
However, they do not propose a model to predict the variations,
nor do they consider sources of variation beyond the execution
proﬁle - again, this is similar to using only the execution
volatility to explain the variation of SDC probabilities. Tao et
al. [30] propose efﬁcient detection and recovery mechanisms
for iterative methods across different
inputs. Mahmoud et
al. [25] leverage software testing techniques to explore input
dependence for approximate computing. However, neither of
them focus on hardware faults in generic programs. Gupta et
al. [12] measure the failure rate in large-scale systems with
multiple program inputs during a long period, but they do not
propose techniques to bound the failure rates. In contrast, our
work investigates the root causes behind the SDC volatility
under hardware faults, and proposes a model to bound it in an
accurate and scalable fashion.
Other papers that
investigate error propagation conﬁne
their studies to a single input of each program. For example,
Hari et al. [14], [15] group similar executions and choose
the representative ones for FI to predict SDC probabilities
given a single input of each program. Li et al. [21] ﬁnd
patterns of executions to prune the FI space when computing
the probability of long-latency propagating crashes. Lu et
al. [24] characterize error resilience of different code patterns
in applications, and provide conﬁgurable protection based on
the evaluation of instruction SDC probabilities. Feng et al. [9]
propose a modeling technique to identify likely SDC-causing
instructions. Our prior work. TRIDENT [11], which VTRI-
DENT is based on, also restricts itself to single inputs. These
papers all investigate program error resilience characteristics
based on static and dynamic analysis, without large-scale FI.
However, their characterizations are based on the observations
derived from a single input of each program, and hence their
results may be inaccurate for other inputs.
X. CONCLUSION
Programs can experience Silent Data Corruptions (SDCs)
due to soft errors, and hence we need fault injection (FI) to
evaluate the resilience of programs to SDCs. Unfortunately,
most FI studies only evaluate a program’s resilience under a
single input or a small set of inputs as FI is very time con-
suming. In practice however, programs can exhibit signiﬁcant
variations in SDC probabilities under different inputs, which
can make the FI results inaccurate.
In this paper, we investigate the root causes of variations in
SDCs under different inputs, and we ﬁnd that they can occur
due to differences in the execution of instructions as well as
differences in error propagation. Most prior work has only
considered the former factor, which leads to signiﬁcant inaccu-
racies in their estimations. We propose a model VTRIDENT to
incorporate differences in both execution and error propagation
across inputs. We ﬁnd that VTRIDENT is able to obtain
achieve higher accuracy and closer bounds on the variation
of SDC probabilities of programs across inputs compared to
prior work that only consider the differences in execution of
instructions. We also ﬁnd VTRIDENT is signiﬁcantly faster
than other state of the art approaches for modeling error
propagation in programs, and is able to obtain relatively tight
bounds on SDC probabilities of programs across multiple
inputs, while performing FI with only a single program input.
ACKNOWLEDGEMENT
This research was partially supported by the Natural Sci-
ences and Engineering Research Council of Canada (NSERC)
through the Discovery Grants and Strategic Project Grants
(SPG) Programmes. We thank the anonymous reviewers of
DSN’18 for their insightful comments and suggestions.
REFERENCES
[1] Rizwan A Ashraf, Roberto Gioiosa, Gokcen Kestor, Ronald F DeMara,
Chen-Yong Cher, and Pradip Bose. Understanding the propagation of
transient errors in hpc applications. In Proceedings of the International
Conference for High Performance Computing, Networking, Storage and
Analysis, page 72. ACM, 2015.
[2] Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai Li. The
parsec benchmark suite: Characterization and architectural implications.
In Proceedings of International Conference on Parallel Architectures
and Compilation Techniques, pages 72–81. ACM, 2008.
[3] Shuai Che, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy W
Sheaffer, Sang-Ha Lee, and Kevin Skadron. Rodinia: A benchmark suite
for heterogeneous computing. In International Symposium on Workload
Characterization (IISWC 2009), pages 44–54. IEEE, 2009.
[4] Hyungmin Cho, Shahrzad Mirkhani, Chen-Yong Cher, Jacob A Abra-
ham, and Subhasish Mitra. Quantitative evaluation of soft error injection
techniques for robust system design. In Proceedings of the 50th Annual
Design Automation Conference, page 101. ACM, 2013.
[5] Cristian Constantinescu. Intermittent faults and effects on reliability of
integrated circuits. In Reliability and Maintainability Symposium, page
370. IEEE, 2008.
Jeffrey J Cook and Craig Zilles. A characterization of instruction-level
error derating and its implications for error detection. In International
Conference on Dependable Systems and Networks(DSN), pages 482–
491. IEEE, 2008.
[6]
[7] Edward W. Czeck and Daniel P. Siewiorek. Observations on the effects
of fault manifestation as a function of workload. IEEE Transactions on
Computers, 41(5):559–566, 1992.
[8] Domenico Di Leo, Fatemeh Ayatolahi, Behrooz Sangchoolie, Johan
Karlsson, and Roger Johansson. On the impact of hardware faults–an
investigation of the relationship between workload inputs and failure
mode distributions. Computer Safety, Reliability, and Security, pages
198–209, 2012.
[9] Shuguang Feng, Shantanu Gupta, Amin Ansari, and Scott Mahlke.
In ACM
Shoestring: probabilistic soft error reliability on the cheap.
SIGARCH Computer Architecture News, volume 38, page 385. ACM,
2010.
[10] Peter Folkesson and Johan Karlsson. The effects of workload input
domain on fault injection results. In European Dependable Computing
Conference, pages 171–190, 1999.
[11] Guanpeng Li, Karthik Pattabiraman, Siva Kumar Sastry Hari, Michael
Sullivan and Timothy Tsai. Modeling soft-error propagation in pro-
grams. In IEEE/IFIP International Conference on Dependable Systems
and Networks (DSN). IEEE, 2018.
[12] Saurabh Gupta, Tirthak Patel, Christian Engelmann, and Devesh Tiwari.
Failures in large scale systems: long-term measurement, analysis, and
implications. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, page 44.
ACM, 2017.
[13] Siva Kumar Sastry Hari, Sarita V Adve, and Helia Naeimi. Low-
cost program-level detectors for reducing silent data corruptions.
In
International Conference on Dependable Systems and Networks (DSN),
pages 1–12. IEEE, 2012.
[14] Siva Kumar Sastry Hari, Sarita V Adve, Helia Naeimi, and Pradeep
Ramachandran. Relyzer: exploiting application-level fault equivalence
to analyze application resiliency to transient faults. In ACM SIGARCH
Computer Architecture News, volume 40, page 123. ACM, 2012.
[15] Siva Kumar Sastry Hari, Radha Venkatagiri, Sarita V Adve, and
Helia Naeimi. Ganges: Gang error simulation for hardware resiliency
In International Symposium on Computer Architecture
evaluation.
(ISCA), pages 61–72. IEEE, 2014.
[16]
John L Henning. Spec cpu2000: Measuring cpu performance in the
new millennium. Computer, 33(7):28–35, 2000.
https://asc.llnl.gov/CORAL-benchmarks/. Coral benchmarks.
https://github.com/coExp/Graph. Github.
https://github.com/karimnaaji/fft. Github.
[17]
[18]
[19]
[20] Chris Lattner and Vikram Adve. LLVM: A compilation framework for
lifelong program analysis & transformation. In International Symposium
on Code Generation and Optimization, page 75. IEEE, 2004.
[21] Guanpeng Li, Qining Lu, and Karthik Pattabiraman.
Fine-grained
characterization of faults causing long latency crashes in programs.
In IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN), pages 450–461. IEEE, 2015.
[22] Guanpeng Li, Karthik Pattabiraman, Chen-Yang Cher, and Pradip Bose.
In Inter-
Understanding error propagation in GPGPU applications.
national Conference for High Performance Computing, Networking,
Storage and Analysis, pages 240–251. IEEE, 2016.
[23] Guanpeng Li, Karthik Pattabiraman, Chen-Yong Cher, and Pradip Bose.
Experience report: An application-speciﬁc checkpointing technique for
minimizing checkpoint corruption. In 26th International Symposium on
Software Reliability Engineering (ISSRE), pages 141–152. IEEE, 2015.
[24] Qining Lu, Guanpeng Li, Karthik Pattabiraman, Meeta S Gupta, and
Jude A Rivers. Conﬁgurable detection of sdc-causing errors in pro-
grams. ACM Transactions on Embedded Computing Systems (TECS),
16(3):88, 2017.
[25] Abdulrahman Mahmoud, Radha Venkatagiri, Khalique Ahmed, Sarita V.
Adve, Darko Marinov, and Sasa Misailovic. Leveraging software testing
to explore input dependence for approximate computing. Workshop on
Approximate Computing Across the Stack (WAX), 2017.
[26] George B Mathews. On the partition of numbers. Proceedings of the
London Mathematical Society, 1(1):486–490, 1896.
[27] Nahmsuk Oh, Philip P Shirvani, and Edward J McCluskey. Control-
ﬂow checking by software signatures. IEEE Transactions on Reliability,
51(1):111–122, 2002.
[28] Behrooz Sangchoolie, Karthik Pattabiraman, and Johan Karlsson. One
bit is (not) enough: An empirical study of the impact of single and
In International Conference on Dependable
multiple bit-ﬂip errors.
Systems and Networks (DSN), pages 97–108. IEEE, 2017.
[29] Marc Snir, Robert W Wisniewski, Jacob A Abraham, Sarita V Adve,
Saurabh Bagchi, Pavan Balaji, Jim Belak, Pradip Bose, Franck Cap-
pello, Bill Carlson, Andrew A Chien, Paul Coteus, Nathan A De-
Bardeleben, Pedro C Diniz, Christian Engelmann, Mattan Erez, Saverio
Fazzari, Al Geist, Rinku Gupta, Fred Johnson, Sriram Krishnamoorthy,
Sven Leyffer, Dean Liberty, Subhasish Mitra, Todd Munson, Rob
Schreiber, Jon Stearley, and Eric Van Hensbergen. Addressing failures
in Exascale computing. The International Journal of High Performance
Computing Applications, 28(2):129–173, 2014.
[30] Dingwen Tao, Shuaiwen Leon Song, Sriram Krishnamoorthy, Panruo
Wu, Xin Liang, Eddy Z Zhang, Darren Kerbyson, and Zizhong Chen.
New-sum: A novel online abft scheme for general iterative methods.
In Proceedings of the 25th ACM International Symposium on High-
Performance Parallel and Distributed Computing, pages 43–55. ACM,
2016.
Jiesheng Wei, Anna Thomas, Guanpeng Li, and Karthik Pattabiraman.
Quantifying the accuracy of high-level fault injection techniques for
hardware faults. In 44th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN), pages 375–382. IEEE,
2014.
[31]
[32] Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal
Singh, and Anoop Gupta. The splash-2 programs: Characterization
In 22nd Annual International
and methodological considerations.
Symposium on Computer Architecture, pages 24–36. IEEE, 1995.
[33] Keun Soo Yim, Zbigniew T Kalbarczyk, and Ravishankar K Iyer. Quan-
titative analysis of long-latency failures in system software. In Paciﬁc
Rim International Symposium on Dependable Computing(PRDC), pages
23–30. IEEE, 2009.
[34] Keun Soo Yim, Cuong Pham, Mushﬁq Saleheen, Zbigniew Kalbarczyk,
and Ravishankar Iyer. Hauberk: Lightweight silent data corruption error
detector for GPGPU. In International Parallel & Distributed Processing
Symposium (IPDPS), page 287. IEEE, 2011.
Fig. 11: Workﬂow of TRIDENT
XI. APPENDIX
In this appendix, we summarize how TRIDENT [11] models
error propagation in programs. This is provided for completeness,
and is based on the material in our earlier paper [11].
A. Overview
The overall workﬂow of TRIDENT is shown in Figure 11. The
inputs of TRIDENT are the program’s source code compiled with
LLVM IR, and an input of the program. The outputs of TRIDENT
are the SDC probabilities of each program instruction, and the overall
SDC probability of the program with the given input.
TRIDENT consists of two phases: (1) Proﬁling, and (2) In-
ferencing. In the proﬁling phase, TRIDENT executes the program
under the input provided, and gathers information such as instruction
dependencies, and branch execution counts. These are used for
constructing the model. Once the model is constructed, TRIDENT is
ready for the inferencing phase, where a location of fault activation
is given to TRIDENT to compute the SDC probability of the given
fault. There are three sub-models in TRIDENT which model error
propagation at three levels: (1) Static-instruction level, (2) Control-
ﬂow level, and (3) Memory level. The results from the three sub-
models are aggregated to calculate the SDC probability of the given
instruction where the fault is activated.
B. Static-Instruction Sub-Model
A fault activated ﬁrst propagates on its static data-dependent
instruction sequence. A static data-dependent instruction sequence is a
sequence of statically data-dependent instructions that are usually con-
tained in the same basic block. TRIDENT computes a propagation
probability for each instruction in the sequence, and then aggregates
the probabilities to compute the probability for the fault from the
activation location to the end of the sequence.
C. Control-Flow Sub-Model
A static data-dependent instruction sequence usually ends with
either a branch or store instruction. If it ends with a branch instruction,
the fault may propagate to the branch instruction and modify the
direction of the branch, leading to control-ﬂow divergence. Conse-
quently, the store instructions dominated by the branch may not be
executed correctly, causing corruptions in memory. Control-ﬂow sub-
model identiﬁes which store instructions are affected by the control-
ﬂow divergence, and at what probabilities.
D. Memory Sub-Model
Once a store instruction is corrupted, the fault continues to prop-
agate in memory and may ﬁnally reach the program output. Knowing
which store instructions are corrupted, memory sub-model further
tracks the error propagation via memory dependency. A memory
dependency graph is constructed based on the memory addresses
proﬁled from all
load and store instructions, and the sub-model
computes the probability for the fault in the corrupted store instruction
to propagate to the program output via the memory dependency.
●Program Source Code (LLVM IR)●Program Input●Instructions Considered as Program Output●Overall SDC Probability of the Program ●SDC Probabilities of Individual InstructionsTrident