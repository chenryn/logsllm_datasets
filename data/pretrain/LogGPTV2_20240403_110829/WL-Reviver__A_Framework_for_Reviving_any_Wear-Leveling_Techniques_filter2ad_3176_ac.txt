…
…
( )
Last
S
S
S
S
(b)
Figure 3. This ﬁgure is subsequent to Figure 2 (b) to illustrate how to
keep chain length at one after data migration. OS pages A and B represent
reserved PA spaces unaccessible from software. (a) When the data in D2
attempts to migrate into a failed block (D3), it actually migrates into D4,
or D3’s shadow block. The mapping function is F3(). (b) Leveraging the
inverse function of F3() and reverse pointer in the pointer section of OS
page A, P2 and D4 become D0’s virtual shadow block and shadow block,
respectively. Meanwhile, D3 has P1 as its virtual shadow block without
having a shadow block, or it is on a PA-DA loop. Reverse pointers are
actually stored on memory blocks pointed to by pointers in the pointer
section.
temporarily suspends the data migration until the next write
request from the software arrives. Then WL-Reviver reports
the write to the OS as a failure to obtain spare PA space,
even though actually it may not be a failure. With a virtual
shadow block, WL-Reviver can resume the data migration.
In this strategy, WL-Reviver chooses next write request
rather than the next access, which could be a read request,
as a victim, because in general a write error is more likely
to be recovered than a read error. The OS could redirect
an unsuccessful write to an alternative memory location
for a retrial. A process encountering a write error can
also be recovered by rolling back to its last checkpoint if
an execution log is available for reading history data. In
contrast, a read failure could crash processes or an entire
system by losing critical data, such as log data and system
metadata, that have been considered as safely stored ones.
Furthermore, the pace of wear-leveling operations is not
slowed down by delaying data migration to the next write,
because wear-leveling operations are scheduled according to
write rate. As an example, one data migration operation is
performed for every 100 writes in the Start-Gap scheme [21].
When a system is rebooted, the OS needs to know which
of the pages have been providing virtual shadow blocks
and to keep the pages from being accessed. To this end,
WL-Reviver maintains a bitmap, in which each bit indicates
whether the corresponding memory page has been excluded
from accesses. As part of memory diagnostics procedure at
a system’s restart, the bitmap is loaded to inform the OS
of the knowledge about PCM’s page usage. As only one
bit is required for each memory page and a bit is set at
most once during a PCM’s lifetime, the cost is minimal. To
ensure safety of the important metadata, WL-Reviver can
keep multiple copies of the bitmap in the PCM with only
trivial overhead.
B. Linking Failed Blocks to Virtual Shadow Blocks and
Shadow Blocks
To hide a failed block from the wear-leveling scheme,
WL-Reviver needs to pair it with a shadow block, which
is not directly accessible to the software. Afterwards every
access to the failed block is transparently passed to the
shadow block. Instead of directly linking the two blocks,
WL-Reviver introduces an indirection, which is the virtual
233233233
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:25:52 UTC from IEEE Xplore.  Restrictions apply. 
shadow block, to support wear leveling so that migration
of data into or out of a shadow block does not break the
linkage between the two. Speciﬁcally, we need to link a
failed block to its virtual shadow block, and further to link
the virtual shadow block to the corresponding shadow block,
as illustrated in Figure 2.
For the ﬁrst linkage, WL-Reviver records the PA address
associated with a virtual shadow block on its corresponding
failed block. To this end, WL-Reviver needs a status bit with
each block, indicating whether the block stores a pointer
or regular data. We also need to ﬁnd a space to store the
address. There have been proposals about where to store the
address in the page-recovery schemes, such as FREE-p [23]
and Zombie [8]. In these schemes, they store DA address
of a reserved healthy block from the reserved area into the
failed block assuming that most bits in the failed block are
still available for storing information. For example, FREE-P
can reliably store a DA in a failed block with a strong error-
correction code, such as 7-modular-redundancy code, as the
available space in the block can be much larger than what
is needed for storing an address. Zombie uses space in a
failed block originally for metadata, such as those for ECC
or ECP [20] error correction codes, to store the DA address
of a linked healthy block, and may use both the failed block
and the healthy block for storing data. WL-Reviver uses the
same approach to store an address. The difference is that it
records PA address, rather than DA address.
For the second linkage, there is no need to explicitly
store a pointer. The PA-to-DA mapping function currently
adopted by the wear-leveling scheme provides the link from
the virtual shadow block, addressed by a PA, to the shadow
block, addressed by a DA. With a migration of data (e.g,
from the current shadow block D1 to another block D2, as
shown in Figure 2(a)), the wear-leveling scheme accordingly
updates its mapping function. Consequently,
the linkage
and shadow block are automatically updated (as shown in
Figure 2(b), the mapping function is updated from F1 to
F2 and the shadow block changes from D1 to D2). By
using a virtual shadow block, which is simply a PA address,
as an indirection, WL-Reviver allows a failed block to
be efﬁciently linked to a constantly moving shadow block
without rewriting pointers. When a failed block is linked to
a shadow block, any read and write requests to the failed
block, including those incurred due to data migration, are
served at its shadow block.
We name the path from a failed block to its shadow block
via DA-to-PA links and PA-to-DA mappings as the failed
block’s chain. We further deﬁne the path consisting of one
DA-to-PA link and its following PA-to-DA mapping as one
step. There are two scenarios where a failed block’s chain
can grow to more than one step. While we aim to minimize
access time of failed blocks, we manage to keep all chains
to be of only one step.
The ﬁrst scenario occurs when a fault on the shadow block
is detected at the time when the block serves a software-
issued write. As shown in Figure 2(c), with this detected
fault a new virtual shadow block (P2) is employed, which
is mapped to memory block D3. Now D3 becomes D0’s
new shadow block with a two-step chain. At this time all
involved DAs (D0, D2, and D3) and PAs (P1 and P2) are
known. WL-Reviver can switch two failed blocks’ (D0 and
D2) virtual shadow blocks (P1 and P2, respectively). In this
way, D0 is only one step away from its shadow block, D3.
D2 is mutually linked by its virtual shadow block, P1. We
name the mutually linked D2 and P1 as a PA-DA loop. In
this scenario, D2 does not have a shadow block. However,
this is not an issue because with current mapping function
D2 can only be reached via P2, which is not accessible from
the software. When the mapping function is updated and P2
is mapped to a different memory block, D2 would have its
shadow block. It is straightforward to extend the strategy
to maintain a one-step chain when new shadow block fails
again. To do this, WL-Reviver only needs to let D0 point to
the last virtual shadow block in the chain.
The second scenario occurs with migration of data from
a shadow block into a failed block during a wear-leveling
operation (e.g., migration of data from D2 to D3 as shown
in Figures 2(b) and 3(a).) According to the WL-Reviver’s
design, the data is actually written into the failed block’s
shadow block (or D4 in the example shown in Figure 3(a)),
producing a multi-step chain. For example, in Figure 3(a)
D4 is D0’s shadow block and D0 has a two-step chain.
Whenever such a chain of two steps with two failed blocks
is formed, WL-Reviver reduces it into a one-step chain by
switching the two failed block’s virtual shadow blocks. As
an example, to reduce the two-step chain in Figure 3(a),
WL-Reviver switches the virtual shadow blocks of D0 and
D3, namely P1 and P2, respectively. D4 is still the shadow
block of D0. But now it can be reached in just one step
from D0, as shown in Figure 3(b). D3 is mutually linked
by its virtual shadow block, P1, or on a PA-DA loop.
Section for Virtual Shadow Blocks
Section for Inverse Pointers
…
Figure 4.
Illustration on where inverse pointers are stored. An acquired
OS page of PAs is divided into two sections. The ﬁrst section, including
PAs from P0 to P59, is used as virtual shadow blocks. The second and the
smaller one, including PAs fromP60 to P63, is used to indicate memory
blocks storing inverse pointers via a PA-DA mapping function.
At the time when a two-step chain is formed, WL-Reviver
knows only blocks following the second failed block in the
chain but does not know blocks preceding it. To switch two
234234234
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:25:52 UTC from IEEE Xplore.  Restrictions apply. 
3
failed block’s virtual shadow blocks in a two-step chain,
WL-Reviver needs to know all the blocks. In the example
shown in Figure 3(a), WL-Reviver needs to know not only
P2 and D4 but also P1 and D0. Because a mapping func-
tion in any wear-leveling scheme is a one-to-one mapping
function, there always exists its inverse function allowing
us to derive the PA that is mapped to a given DA. In the
example, as D3 = F3(P1) WL-Reviver can know P1 by
P1 = F −1
(D3). To know the ﬁrst failed block, WL-Reviver
needs to record an inverse pointer from virtual shadow block
to its corresponding failed block. The challenge is where to
store the inverse pointer. WL-Reviver does not pre-reserve
memory blocks for this purpose. Instead, it uses a method
similar to that for acquiring shadow blocks. Once the OS
discontinues its use of a new page in response to PCM’s
access exception, WL-Reviver designates last several PAs
in the page as inverse pointer section. The memory blocks
mapped to by the PAs are used to store inverse pointers
corresponding to the pointers from failed blocks to virtual
shadow blocks in the page2. Thus, the number of inverse
pointers stored in the page is equal to the number of virtual
shadow blocks in the page. As an example, for a 64B
memory block a 4KB page can accommodate at most 64
virtual shadow blocks. For a 32bit pointer, a memory block
can hold 16 inverse pointers. As shown in Figure 4, the ﬁrst
60 PAs, from P A0 to P A59, can be used as virtual shadow
blocks and the last four PAs, from P A60 to P A63, are
mapped to memory blocks for storing inverse pointers. When
the page of PAs is acquired, the two registers recording the
current and last available PAs for virtual shadow blocks are
set as P A0 to P A59, respectively. Compared to the regular
data, these pointers are way much less frequently updated,
and the blocks storing the pointers are much less worn and
less prone to failures. Even in very rare cases where the
pointers are lost, they can be rebuilt by scanning the entire
PCM.
To guarantee the correctness of WL-Reviver with the
aforementioned linkage between various blocks and ad-
dresses, we demonstrate the below three statements are true.
Theorem 1. In WL-Reviver, any software-accessible failed
block is backed up by a healthy shadow block.
Proof. In WL-Reviver, a request presents its access ad-
dress in the form of a PA (say P0), which is mapped to
a failed block at a DA address (say D0). Assume D0’s
virtual shadow block is P1. P1 must not be P0 because P0 is
software accessible but P1 is not. Because a one-to-one PA-
to-DA mapping function is used, P1 must be mapped to a
block, D1, that is different from D0. D1 must be a block that
is considered as healthy at the time of the access. Otherwise,
D1 would be a block of a known failure and store a pointer to
a virtual shadow block. If this were true, this virtual shadow
2We will prove that the memory blocks are ready for storing data, either
directly or indirectly.
block must not be P1, because P1 has been D0’s virtual
shadow block and a PA can be at most one block’s virtual
shadow block. This leads to a chain whose length is more
than one step. This contradicts the fact that all chains in WL-
(cid:2)
Reviver are of one step.
This theorem ensure that any read and write requests
issued by the software to a failed block can be passed to
and served at its corresponding shadow block.
Theorem 2. In WL-Reviver, any unlinked PAs in a re-
served OS page, including PAs that have not been pointed
to by failed blocks in the section for virtual shadow blocks
and PAs that have not been used for storing pointers in the
section for inverse pointers, are mapped to heathy blocks,
either directly or indirectly.
Proof. Suppose an unused PA (say P0) is mapped to
memory block D0. If D0 is a block currently known as
healthy, P0 is directly linked to a healthy block. Otherwise,
if D0 is a block of known failure, it must have stored a
pointer pointing to PA P1. Because P0 is unlinked, P1 is
not P0, and D0 is not on a PA-DA loop. Therefore, D0
must have its shadow block, a block currently known as
healthy. In this way, P0 is indirectly linked to a healthy
(cid:2)
block.
This theorem suggests that as long as WL-Reviver can
obtain an unlinked PA, it is guaranteed that it has a memory
block currently known as healthy to store a block of data.
Note that actually writing data into the memory block
may be a failure. WL-Reviver can follow the protocol as
described before to handle the newly detected block failure.
In WL-Reviver, the only failed blocks not backed up by
shadow blocks are those on PA-DA loops. This is not a
concern for accesses from the software because their linked
PAs are unaccessible to the software. However, it could be an
issue if data possibly need to be migrated into such a failed
block for wear leveling. The below theorem eliminates the
possibility.
Theorem 3. In WL-Reviver, a wear-leveling scheme does
not migrate data into a memory block on a PA-DA loop.
Proof. The wear leveling scheme determines the mapping
function from a PA to DA regardless of whether the PA is
accessible or whether the DA is a failed one. Therefore, the
scheme has to assume that a memory block at a DA could
store regular data if it is mapped to by a PA that could
be accessible to the software, even if its access is canceled
later due to block failure. While migration of data from
one memory block D0 to another memory block D1 would
destroy data currently stored in D1, the scheme must make
sure that D1 is impossible to store regular data. Equivalently
the scheme has to make sure that D1 is not mapped by any
PA. Therefore, D1 is not on a PA-DA loop, because a DA on
(cid:2)
the loop is mapped to by a PA.
According to this theorem, any wear-leveling scheme has
to assume a buffer block, either explicitly, such as GapLine
in Start-Gap [21], and implicitly, such as data swapping in
235235235
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:25:52 UTC from IEEE Xplore.  Restrictions apply. 
Security Refresh [22], and the buffer block is not mapped
to by any software-accessible PA.
IV. PERFORMANCE EVALUATION
In this section we evaluate efﬁcacy and cost-effectiveness
of WL-Reviver as a framework to revive wear-leveling
operations discontinued due to failures of the PCM blocks.
In the evaluation we’d like to answer two questions. The
ﬁrst is whether it is indeed imperative to revive the wear-
leveling mechanism after the occurrence of failures. The
second question is how efﬁciently WL-Reviver keeps a wear-
leveling scheme alive with PCM faults. Regarding the ﬁrst
question, there are a number of fault-tolerance solutions that
can effectively postpone the timing for the ﬁrst failure to
be exposed to a wear-leveling scheme. The solutions can
be represented by PAYG [18], which dynamically allocates
metadata for error corrections according to error distribution,
and FREE-p [23], which hides block failures using blocks
in a pre-reserved space until the space is used up. To allow
FREE-p to be compatible with the wear-leveling operations,
we have to disallow dynamic reservation of space in the
experiments. Regarding the second question, WL-Reviver
needs an additional read when a failed block is accessed by
the software. We compare WL-Reviver with LLS [12], a de-
sign integrating wear leveling schemes with fault-tolerance
solutions to keep wear-leveling from being disrupted upon
a block failure, in terms of space loss and time overhead.
In the presentation of evaluation results we use only
one representative PCM wear-leveling scheme, which is
Start-Gap [21], though there can be more such schemes,
including Security Refresh [22], because of space constraint.
However, this methodology is not an issue. WL-Reviver, as
a framework, interacts with any such schemes only via one
basic operation, data migration, that is common to any of the
schemes. The way for WL-Reviver to perform its operations
and its associated efﬁciency are not subject to any design
choice of a particular weal-leveling scheme other than this
common operation. Furthermore, WL-Reviver neither com-
promises nor improves a scheme’s weal-leveling efﬁcacy.
Instead, it only restores an existent scheme’s function.
A. Experimental Setup
we use trace-driven simulations in the evaluation. Memory
traces are collected by running a number of commonly used
programs, listed in Table I, from benchmark suites PAR-
SEC [5], NAS Parallel Benchmarks(NPB) [6], and SPLASH-
2 [7], with Pin [4]. These benchmarks represent various
distributions of writes over the memory blocks, which are
quantiﬁed by CoV (Coefﬁcient of Variation) as shown in
the table. A larger CoV value means less uniform write
distribution and higher probability of early failures on the
PCM. In each experiment a program is assumed to run for
multiple times to produce required wear-out effect. In the
setup, we assume each PCM cell can sustain 108 writes