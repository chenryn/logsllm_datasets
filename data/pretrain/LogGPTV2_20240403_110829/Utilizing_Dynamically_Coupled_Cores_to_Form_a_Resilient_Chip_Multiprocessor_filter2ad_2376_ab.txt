 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
Checkpoint Interval (thousands of cycles)
Figure 3. System bus trafﬁc with increasing checkpoint
interval size (system described in Section 5).
Utilizing the system bus for redundant thread communication
has two major implications. First, communicating over the sys-
tem bus with a potentially distant core may incur a greater latency
than communicating to an adjacent core via a dedicated bus. Sec-
ond, the resulting increase in system bus trafﬁc could severely im-
pact performance. Figure 3 shows the average increase in sys-
tem bus trafﬁc over a range of checkpoint intervals (time between
output comparisons) for the parallel applications discussed in Sec-
tion 5. This graph suggests that a long checkpoint interval, roughly
greater than 3, 000 cycles, is needed to amortize the increase in
system bus trafﬁc. Supporting long checkpoint intervals requires
a signiﬁcant deviation from previous work. For instance, we ﬁnd
that using the relaxed input replication model from Reunion incurs
a signiﬁcant overhead (we evaluate this in Section 5.1.2).
3.2 Private Cache Modiﬁcations
In order to support long checkpoint intervals, a large number
of memory stores must be buffered. Clearly, thousands of cycles
worth of stores will exceed the capacity of the store buffer used
in Reunion and CRTR. Instead, DCC’s private caches support the
cache buffering techniques proposed in Cherry[11]. When a cache
line is written, it is marked as unveriﬁed. Unveriﬁed lines are not
allowed to leave the private cache hierarchy. Once the buffered
state is known to be fault-free, at the end of a checkpoint inter-
val, all unveriﬁed marks are gang-cleared. A write to a veriﬁed
dirty line forces that line to be written back to lower levels of the
memory hierarchy, so that it may be restored if a fault occurred
during the checkpoint interval. Cherry has shown that this style of
cache buffering can easily support thousands of loads with very lit-
tle overhead (roughly one bit per cache block). In addition to cache
buffering support in the private caches, all caches are protected by
error correcting codes (ECC), as is done in previous work. It is
necessary for each processor to redundantly load data from shared
memory into their private cache. However, only one processor
needs to write dirty cache lines back into shared memory. We as-
sign the task of writing back dirty data to one processor, the mas-
ter, while the other processor, the slave, may evict updated (but
veriﬁed) cache lines without writing back. The master and slave
processors need not be leading and trailing, respectively. Mas-
ter cores ignore coherence actions to unveriﬁed lines by their own
slave(s). Conversely, slaves ignore invalidation requests from their
own master. Data consistency when running parallel applications
is discussed later (Section 4).
There exists a danger of deadlock if an application’s unveriﬁed
dirty lines are allowed to remain in the CMP cache subsystem af-
ter the application is descheduled by the operating system. Specif-
ically, the next application to run on the same core may ﬁnd all
cache blocks of a set locked by unveriﬁed writes from the previ-
ous application, preventing it from making forward progress. If
the new application has also locked all cache blocks in a set used
by the old application, then a circular dependence arises between
the two applications and deadlock ensues. Similar interactions are
possible between writes from an application and the operating sys-
tem. To avoid these problems, we implement a simple policy: be-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20073.4 State Compression
State compression is needed to reduce the bandwidth require-
ments of comparing state between two cores. Fingerprinting[24]
proposed the use of a CRC-16 compression circuit to compress all
of the register ﬁle and memory updates each cycle. We simulated
various parallel CRC circuits[2] in HSPICE and their fan-out-of-
four (FO4) delays1 and transistors counts are shown in Table 1.
Assuming a cycle time of 10-15 FO4s, a CRC-32 circuit and a
CRC-16 circuit (2 stages) can compress up to 32 bits in one cy-
cle. With potentially more than 256 bits of new state each cycle,
a CRC-16 circuit could not keep pace with the core. To remedy
this, Reunion uses a multicycle compression scheme. However,
the long checkpoint interval in DCC allows us to employ a simple
solution that provides a large reduction in required compression
bandwidth.
CRC
Circuit
CRC-16
CRC-SDLC-16
CRC-32
CRC-32
Input
Width
16
16
16
32
FO4
Delay
6.65
6.10
7.28
8.60
Transistor
Count
754
888
2260
4240
Table 1. FO4 delay and transistor count for various CRC
circuits.
We make the observation that checking the state of the register
ﬁle at the end of a checkpoint interval is equivalent to checking all
the updates made to the register ﬁle during a checkpoint interval.
Therefore, rather than compressing all the updates to the register
ﬁle, we simply compress the state of the register ﬁle at the end
of a checkpoint interval. The time it takes to read out the con-
tents of the 32 entry architectural register ﬁle is easily amortized
over the long checkpoint interval. In this manner, only memory
stores need to be compressed on the ﬂy during a checkpoint inter-
val. Two CRC-32 circuits are needed to compress the data and the
address of a store. At the end of a checkpoint interval, these CRC
circuits simultaneously compress the integer register ﬁle and the
ﬂoating point register ﬁle. For a checkpoint interval of 10,000 cy-
cles, this technique reduces the total state compression bandwidth
(i.e., number of bits compressed per cycle) by a factor of 5.4 for
SPEC2000 benchmarks.
Figure 5. State compression: Stores are compressed each
cycle using two CRC-32 circuits, but register values are
compressed at checkpoints. StQ is the store queue.
3.5 Recovery
DCC aims to mitigate the impact of deep submicron challenges
(Section 2) by endowing arbitrary CMP cores with the ability to
verify each other’s execution. Detection, however, is only part of
the solution; a complete framework for ﬂexible fault-tolerance in
CMPs also requires the ability to recover from faults once they are
detected. Once again, DCC supports recovery from both hard- and
1FO4 is the delay of one inverter driving four identical copies of itself.
Delays expressed in units of FO4 are technology-independent.
Figure 4. Processor synchronization when: a.) synchro-
nizing processor is leading, or b.) synchronizing processor
is trailing.
fore control is transferred across applications and the operating
system, all unveriﬁed dirty data are veriﬁed by scheduling a check-
point. Once control is transferred, the operating system saves the
(just veriﬁed) architectural state to the application’s process con-
trol block as usual.
On a context switch, slave caches are ﬂushed to avoid having
multiple copies of a cache line with inconsistent states. If a pre-
vious application had used the slave core as its master processor,
there is a danger that this earlier application’s veriﬁed dirty lines
may be lost during the cache ﬂush. To avoid such cases, the oper-
ating system partitions the cores into master and slave pools, and
allocates master and slave cores accordingly. In rare cases where a
processor from the master pool may need to be moved to the slave
pool, all veriﬁed dirty data in the master’s cache are ﬁrst written
back to main memory. In cases where a processor from the slave
pool needs to be relocated to the master pool, the slave cache is
ﬂushed.
3.3 Synchronization
Figure 4 shows our multi-phase synchronization protocol. Syn-
chronization begins when a processor receives a scheduled or un-
scheduled checkpoint request. Unscheduled checkpoints occur for
the following events: i) cache buffering overﬂow; ii) interrupt; iii)
uncached load/store (I/O); or iv) context switch. The processor
that receives the checkpoint request sends the number of instruc-
tions committed since the last checkpoint to the other redundant
processor. The processor initiating the synchronization is either
the leading processor, Figure 4a, or the trailing processor, Figure
4b. If the synchronizing processor is leading, the other processor
commits enough instructions to synchronize, then compresses and
broadcasts its state (via the shared system bus). If the synchroniz-
ing processor is trailing, the other processor broadcasts the num-
ber of instructions it has committed since the last checkpoint. The
synchronizing processor executes enough instructions to match the
leading processor, then compresses and broadcasts its state. Once
each processor has received the compressed state of the other,
it compares against its own.
If the compressed states match, a
checkpoint is taken and execution resumes. If they disagree, the
last checkpoint is restored and the checkpoint interval repeats. In
cases where the trailing processor cannot execute enough instruc-
tions to match the leading processor (e.g., due to a cache buffering
overﬂow), the last checkpoint is restored, and a new checkpoint
is scheduled with half the duration of the last interval. Similarly,
if a checkpoint interval does not complete within a ﬁxed timeout
period, a rollback is forced and a new checkpoint with half the du-
ration of the last checkpointing interval is scheduled. This even-
tually guarantees forward progress in cases where a fault prevents
the cores from reaching the next checkpoint. Checkpoints are kept
in a small, ECC-protected, on-chip SRAM array.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Interval
Interval
Interval
Interval
Interval
N
N
N
N
X
N+1
X
Figure 6. Recovery from a permanent fault using FER
after BER fails. After interval N fails twice, interval N is re-
executed a third time on three processors. Voting identiﬁes
the faulty processor, the system rolls back to the beginning
of interval N, and execution continues on the remaining two
processors.
soft-errors without requiring dedicated communication hardware
or statically binding cores.
When an application requiring redundant execution is
switched-in by the operating system, it is appropriated two pro-
cessors, one master and one slave. Checkpointed register values
are stored in the application’s process control block by the op-
erating system upon context switches, and are recovered when
the application is switched-in. The allocated processors execute
instructions, using the aforementioned detection scheme, until a
fault is detected. To recover from this fault, backward error re-
covery (BER) is employed. Both processors rollback architectural
state to their last valid checkpoint, invalidate all the cache lines
marked as unveriﬁed, and resume execution from the checkpoint.
If the fault was transient, the processors will successfully complete
their next checkpoint. However, if the fault is permanent, the same
checkpoint interval will repeat and the system defaults to forward-
error recovery.
Speciﬁcally, when BER fails to recover from a fault, after re-
peating the same checkpoint interval multiple times, a third pro-
cessor is appropriated by the operating system for forward error
recovery (FER).2 (If all other processors are in use, the operating
system must choose a core and switch-out its currently running
thread.) To initiate this, the cache controller of the failing master
core makes a TMR request by generating a special bus transaction
that sets a ﬂag in the kernel’s address space. This transaction is
observed by all other nodes, and a predetermined node is given
the responsibility of calling the operating system by jumping to
an interrupt vector (each node is responsible for handling another
node’s requests, and this assignment is made by the kernel). Prior
to taking the interrupt, the master and the slave of this remote node
synchronize, and then control is transferred to the operating sys-
tem by jumping to the TMR interrupt handler. The operating sys-
tem inspects the ﬂags set by the failing node to identify the re-
questing pair, and allocates a third core for FER. Cases where the
requesting node is executing OS code are handled identically. The
architectural state of the last valid checkpoint is copied from the
master processor to the new slave processor. These three proces-
sors, one master and two slaves, implement FER by executing the
checkpoint interval in parallel and voting on the correct results
(signatures), as shown in Figure 6. Essentially, this amounts to
on-demand triple modular redundancy (TMR). Through TMR, the
faulty processor is isolated and marked as such, the system rolls
back to the beginning of the faulty interval, and all unveriﬁed data
are invalidated. If the master processor is faulty, all dirty veriﬁed
data in the master’s cache is written back to memory, and one of
the slaves is promoted to master. Once the faulty processor is iso-
lated, execution continues on the remaining two cores.
2The involvement of the operating system in FER is unlikely to affect
system performance since FER is only invoked on hard faults (a rare event)
and in cases where the same checkpoint interval fails multiple times in a
row due to soft errors (exceedingly unlikely).
State
Invalid
Shared
Exclusive
Owned
Modiﬁed
Cache-line’s State Description
Invalid data
Valid data, possibly inconsistent with memory
Valid data, consistent with memory,
present only in one cache
Valid, dirty data, possibly shared
Valid, dirty data, present only in one cache
Can be
unveriﬁed?
No
Yes
No
Yes
Yes
Table 2. Summary of MOESI protocol states and whether they
can hold unveriﬁed data.
4 Parallel Application Support
Our discussion of DCC thus far has been limited to fault detec-
tion and recovery for one single node. Although this is sufﬁcient
for sequential applications, supporting ﬂexible fault-tolerant ex-
ecution for parallel applications is at least equally important for
future CMP platforms. In this section, we provide extensions that
allow DCC to operate correctly when running shared-memory par-
allel programs. In the following discussion, we deﬁne a particular
master-slave pair as a node, and we refer to all other nodes as re-
mote nodes. Recall that master and slave cores that form a node
need not be adjacent or even close to one another.
4.1 Checkpoints
Checkpoints are taken globally across all nodes: the bus con-
troller initiates scheduled checkpoints by sending synchronization
requests to all nodes at the end of each checkpoint interval. All
master threads synchronize with their slaves (as in the sequential
case), and send an acknowledgment to the bus controller when the
synchronization is complete. When all nodes are synchronized and
no outstanding bus transactions remain, the bus controller issues
a checkpoint request, and the architectural state on each core is
saved. Descheduled threads of a parallel application need not par-
ticipate in the global checkpoint since their last checkpoint (taken
at the time they are descheduled) is still valid. In the case of a sig-
nature discrepancy or timeout (Section 3), all processors involved
in the execution of the parallel application roll back to their re-
spective checkpoint.
4.2 Coherence
Data sharing in DCC-equipped CMP architectures is different
from conventional architectures in that: (1) some of the data held
in caches may be unveriﬁed–that is, subject to rollback; and (2)
sharing decisions must consider whether the processors involved
are playing the role of master or slave.
To support sharing of unveriﬁed data, we leverage some of the
mechanisms previously proposed in the context of the multipro-
cessor version of Cherry, Cherry-MP [9]. Similarly to Cherry-MP,
a natural choice for a baseline cache coherence protocol on which
to build DCC support is a MOESI protocol [1]. MOESI allows
several copies of a cache line across processors that are possibly
incoherent with the copy in memory. Among those copies, the
owned copy is responsible for (1) providing a copy to any new
sharer, and (2) writing back the copy if it replaces the cache line.
The other copies remain in the shared state. Because DCC re-
quires keeping unveriﬁed data off memory, MOESI is convenient
for safely sharing unveriﬁed modiﬁed data across processors. Ta-
ble 2 compiles MOESI’s states; the rightmost column indicates
whether the state is apt to hold unveriﬁed data. Notice that, in
the case of shared state, it is only possible to hold unveriﬁed data
if there is an owned copy elsewhere in the system–otherwise, the
data must be necessarily consistent with memory.
To support sharing of unveriﬁed data, we extend the coherence
protocol along the lines of Cherry-MP [9] (the Cherry-MP exten-
sions are slightly more elaborate than in DCC because of some
additional restrictions speciﬁc to Cherry-MP). Speciﬁcally:
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007(cid:129) Writes always mark the writer’s cache line as unveriﬁed, sim-
ilar to the uniprocessor case (Section 3.2). Writes to veriﬁed
dirty cache lines (modiﬁed or owned state) force a writeback
of the original contents to main memory, in case a rollback
later undoes the update. These writes may be initiated by
a local processor, or by a remote processor through a read-
exclusive or upgrade request. On the other hand, writes to
unveriﬁed dirty cache lines must not generate write backs to
main memory.
If the cache line is marked unveriﬁed and
dirty elsewhere, the protocol simply forwards the cache line