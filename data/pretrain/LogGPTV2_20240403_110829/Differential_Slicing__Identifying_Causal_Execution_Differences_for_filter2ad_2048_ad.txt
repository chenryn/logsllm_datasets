350,323
1,282
4,310
4,310
Disaligned instructions
Failing
327,016
3,162
1,041,175
14,586
8,893
748,075
119,085
138,787
135
929
1,026
Passing
1,307,465
446,273
2,034,582
656,183
144,517
1,504,189
601,789
415,086
5,551
38,743
33,508
471
5
32
23
4
Slice-Align
Disaligned regions
All
983
75
111
38
39
389
524
87
4
1
1
235
59
4
4
1
1
Table III: Total disaligned instructions and regions compared with disaligned regions in graph.
Name
reader-e1
reader-e2
reader-u1
reader-u2
reader-u10
reader-u11
reader-u14
tftpd
ﬁrebird
gdi-2008
gdi-2007
Basic pruning
Extended pruning
Pass
3,651
4,854
2,753
135
45
1,584
1,714
254
45
100
11
Fail
3,616
4,853
2,751
135
43
1,562
1,695
254
46
101
12
# IDiff
7
21
13
1
1
1
6
1
1
1
1
Pass
2,324
81
204
100
36
1,158
425
254
45
96
7
Fail
2,292
84
201
100
34
1,135
420
254
46
97
8
# IDiff
7
1
1
1
1
1
1
1
1
1
1
Table IV: Causal difference graph evaluation. The Extended pruning column corresponds to the size of the output graph.
Reader and GDI vulnerabilities, the input PDF or WMF ﬁles
have only one byte with a different value, and for the tftpd
and ﬁrebird vulnerabilities, the passing input is one byte
shorter than the failing one.
Relevant execution differences. The ﬁrst step of our differ-
ential slicing approach is to align the two traces. As a prepa-
ration step, since only one thread is involved in the crashes
that we evaluate, we extract the relevant thread from each
trace, creating two single-threaded traces. After aligning the
execution traces, we count the number of disaligned regions
(Column Disaligned regions (All) in Table III). Next, we
generate the causal difference graph for each vulnerability
and count the number of disaligned regions in the graph
(Column Disaligned regions (Slice-Align)). The results show
that for the more complex Adobe and tftpd examples, which
come from larger execution traces (shown later in Table V),
the number of disaligned regions in the graph is only
4%-48% of the total number of disaligned regions. Thus,
our differential slicing approach removes a large number
of disaligned regions that are not relevant to the crash. For
the smaller examples (ﬁrebird and both GDI vulnerabilities),
the number of total disaligned regions is small enough that
all of them are relevant to the crash. Even if the graph
does not remove disaligned regions in these cases, it still
provides causality information to the analyst and prunes
away many unrelated nodes in those regions that have no
value difference.
The causal difference graphs for reader-e1, -u2, -u10,
-u11, and -u14 identify execution omission errors. This
means that for those vulnerabilities, the causal path returned
by a dynamic slice (data and control dependencies) on the
crashing instruction in the failing trace would not make it
back to the input differences, as relevant statements are not
present in the failing trace.
Graph size. Table IV presents the evaluation of the graph
size in three situations. For each situation, the Pass and Fail
columns show the total number of nodes in the passing and
failing graphs, respectively, and the # IDiff column shows the
number of input differences (i.e., root nodes) in the graph.
The Basic pruning columns show the graph sizes when
only direct value comparisons between operands are used to
identify value differences. The Extended pruning columns
show the graph sizes when we incorporate address nor-
malization so that equivalent pointers can also be pruned.
Note that
the Extended pruning columns correspond to
the actual output of our tool. The results show that for
the Adobe Reader experiments, the address normalization
greatly improves the pruning. In some experiments (namely,
reader-e2 and reader-u1), extended pruning reduces the
number of nodes in the graph by between one to two orders
357
of magnitude. Additionally, the results for the reader-e2, -u1,
and -u14 experiments show that the address normalization
often reduces the number of false positive input differences.
For the rest of the experiments, basic and extended pruning
achieve comparable results.
As expected, for the programs that take a ﬁle as input
(Adobe, GDI), where the only difference between the pass-
ing and failing inputs is the value of one byte, the graph
captures that the input difference is the byte that differs
between the program inputs. Note that for reader-e1, the
graph also identiﬁes six additional input differences (i.e.
false positives). This is likely due to the conservative nature
of our pruning techniques, which are designed to minimize
incorrect pruning which might prevent the causal graph from
reaching the correct input differences.
In the remainder of this section, we detail how the causal
difference graph helps an analyst in the Tfptd and Firebird
vulnerabilities, describe results for inputs with multi-byte
differences, and present a performance evaluation.
Tfptd. For the tftpd vulnerability there is only one input
difference in the graph, which captures that byte 245 in the
received network data has value 0x7a in the failing trace and
0x00 in the passing trace. The fact that if byte 245 was a null
terminator the program would not crash is an immediate red
ﬂag for an analyst, because it is common in buffer overﬂows
that an application reads input until it ﬁnds a delimiter (0x00
is the string delimiter). If the delimiter appears beyond the
length of the buffer and the program does not check this, an
overﬂow occurs, which is what happens in this case.
Firebird. For the ﬁrebird vulnerability there is only one
input difference in the graph. Surprisingly, the input dif-
ference does not correspond to any values in the received
network data, rather, it corresponds to the return value of the
ws2_32.dll::recv function, which corresponds to the
the size of the received network data. Thus, in this case just
knowing the input difference immediately tells an analyst
that the crash is related to the different size of the input.
Multi-byte input differences. To evaluate whether the
causal difference graph only contains the relevant subset of
input differences in the presence of multiple differences in
the program input, we repeat the reader-u10 experiment four
times. In each experiment, we double the number of bytes
that differ from the failing input by randomly ﬂipping bytes
in the original passing input (making sure the new input
does not crash Adobe Reader). For the four experiments,
the total number of byte differences between the passing and
failing inputs is 4, 8, 16, and 32. We compare the new graphs
with the original one and observe that even if the number
of differences in the program input has increased, the graph
has not changed and the only input difference corresponds
to the original byte difference that caused the crash. Thus,
Slice-Align successfully ﬁlters out
input differences not
relevant to the crash.
Name
reader-e1
reader-e2
reader-u1
reader-u2
reader-u10
reader-u11
reader-u14
tftpd
ﬁrebird
gdi-2008
gdi-2007
Trace size
(MB)
Tracing
(sec.)
Pass
202
143
200
110
24
152
160
3.6
2.5
2.4
2.1
Fail
106
67
133
61
16
101
107
2.0
0.1
0.4
0.4
Pass
482
345
403
208
267
155
195
13
1
2
2
Fail
365
337
406
295
275
161
192
12
1
0.8
0.8
Trace
align
(sec.)
1,684
1,180
714
152
39
462
837
50
1
2
2
Slice-Align
(sec.)
3,510
1,291
101
208
24
364
239
12
0.2
0.5
0.3
Table V: Performance evaluation.
Performance. Table V shows the performance evaluation,
including the size of the passing and failing traces, the time
it took to take the traces, the time to align the traces, and
the time to generate the Slice-Align graphs. The results
show that the time to take a trace is below 8 minutes for
every trace, and Slice-Align never takes more than 1 hour to
generate the causal difference graph. This saves signiﬁcant
time compared to an analyst’s manual work.
B. User Study
An important but too often overlooked metric for evalu-
ating an analysis tool is how useful it is for the analysts.
To evaluate the usefulness of our differential slicing tool
for vulnerability analysts, we conduct a user study with two
subjects. Subject A is an analyst at a commercial security
research company. Subject B is a research scientist and
was a new member of our research group at the time of
this experiment. Neither subject had any involvement in the
development of this work, nor had they used our group’s
binary analysis tools before the experiment. We emphasize
that this user study is informal, as it is designed to help
understand qualitatively how the causal difference graphs
are useful in practice.
Experiment setup. The subjects analyze two vulnerabilities
from the set in Table II: reader-e2 and reader-u10. We select
these two vulnerabilities based on their similar complex-
ity (both are exploitable vulnerabilities in Adobe Reader).
Neither subject had previously analyzed either of these
vulnerabilities. Our ground truth for these vulnerabilities
comes from prior manual analysis by a third analyst, not
involved in the user study.
Each subject analyzes one vulnerability using any tech-
niques he chooses, but without access to our graphs (Sample
1). For analyzing the other vulnerability, we additionally
provide them with the corresponding causal difference graph
(Sample 2). We switch the vulnerability selected for Sample
1 and Sample 2 for the two subjects. For both samples, we
provide the subjects with 1) an input that triggers the crash,
358
Sample 1
(no graph)
Subj.
sample
A
B
reader-e2
reader-u10
time
(hr)
13
3
found
cause?


Sample 2
C. Identifying Input Differences in Malware Analysis
(Causal difference graph)
found
cause?
sample
reader-u10
reader-e2


time
(hr)
5.5
3
Table VI: Results for user study.