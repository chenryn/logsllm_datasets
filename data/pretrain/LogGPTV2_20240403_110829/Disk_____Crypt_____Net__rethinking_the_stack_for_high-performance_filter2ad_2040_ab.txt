Stock 0%BC
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(a) Network throughput (Error bars indicate the 95% CI)
(b) CPU utilization
Figure 1: Plaintext performance, Netflix vs Stock FreeBSD, zero and 100% Buffer Cache (BC) ratios.
Netflix 100%BC
Netflix 0%BC
Stock 100% BC
Stock 0%BC
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
)
%
(
n
o
i
t
a
z
i
l
i
t
u
U
P
C
800
600
400
200
0
Netflix 100%BC
Netflix 0%BC
Stock 100%BC
Stock 0%BC
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(a) Network throughput (Error bars indicate the 95% CI)
(b) CPU utilization
Figure 2: Encrypted performance, Netflix vs Stock FreeBSD, zero and 100% Buffer Cache (BC) ratios.
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
T
k
r
o
w
t
e
N
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
T
k
r
o
w
t
e
N
80
60
40
20
0
80
60
40
20
0
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
t
y
r
o
m
e
M
150
100
50
0
million/sec—indicating that the cores are now waiting on memory
much of the time, and explaining why CPU utilization is 100%.
2.3 Discussion
Netflix optimizations have clearly delivered significant improve-
ments in the video streaming performance of FreeBSD, both for
serving plaintext and encrypted content. However, it is also clear that
memory is being worked very hard when serving these workloads.
With a conventional stack it is extremely hard to pin down precisely
why this is the case. We have profiled the stack, and with Netflix’s
VM improvements there are no obvious bottlenecks remaining.
Current Intel CPUs DMA to and from the LLC using DDIO,
rather than direct to memory. In principle it ought to be possible to
DMA data from the SSD and then DMA it to the NIC without it ever
touching main memory. Would it also be possible to encrypt that
data as it passes through the LLC? With a conventional stack though,
it is clear that this is not happening. We speculate that this is because
the stack is too asynchronous. Data is DMAed from the SSD to
disk buffer cache, initially landing in the LLC. However, it is not
immediately consumed, so gets flushed to memory. Subsequently
the kernel copies the buffer, loading it into the LLC as a side effect.
If it is not immediately encrypted it gets flushed again. The kernel
goes to encrypt the data, causing it to be re-loaded into the LLC. The
encrypted data is not immediately sent by TCP, so it gets flushed a
third time. Finally it is DMAed to the NIC, requiring it to be loaded
Netflix 0%BC read
Netflix 0%BC write
Netflix 100%BC read
Netflix 100%BC write
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
Figure 3: Encrypted Netflix memory performance
workload with ~72Gb/s of read and write memory throughput, so
the Netflix stack is working memory a little harder than is strictly
necessary, even with plaintext workloads.
When serving encrypted content, the story becomes more com-
plicated. The Netflix memory throughput is shown in Figure 3. Ir-
respective of whether data comes from the disks or from the buffer
cache, memory read throughput is approximately 2.6 times the net-
work throughput. Indeed, the 175Gb/s read rate when serving from
the disk buffer cache is getting closer to the memory speed of this
hardware, indicating that memory accesses are likely to be a bottle-
neck. The system also shows a high rate of LLC miss events—200
214
Disk|Crypt|Net: rethinking the stack for high-performance video streaming
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 4: Possible Memory Accesses with
the Netflix Stack
Figure 5: Desired Memory Accesses with
Specialized Stack.
Figure 6: NVMe Controller Latency and
Throughput
again. This process is shown in Figure 4, and requires three reads
from memory, compared to the 2.6 times we observe, so presumably
sometimes the data is not flushed, saving some memory reads.
Netflix’s newest production streamers are equipped with latest
16-core (32 hyperthreads) Intel CPUs and 100GbE NICs, but their
maximum target service throughput is limited to ~90Gb/s. At this
rate, they use 460Gbit/sec of read/write memory bandwidth—this is
96% of the measured hardware limits, and approximately five times
the network throughput. This ratio is in general agreement with the
results in Figure 3. At this utilization, CPU stalls waiting for memory
become a major problem, and CPU utilization varies considerably,
ranging from 75% to 90% with only small changes in demand.
3 TOWARDS A SPECIALIZED VIDEO
STREAMING STACK
In a conventional stack, it is very hard to avoid all the memory traffic
that we saw with the Netflix stack. Could a specialized, much more
synchronous stack do better? How might we architect such a stack
so that data remains in the LLC to the maximum extent possible?
As buffer cache hit ratios are so low with Netflix’s workload,
clearly we need to design a system that works well when data has
to be fetched from the SSD. The storage system needs to be very
tightly coupled to the rest of the stack, so that as soon as data arrives
from storage in the LLC, it is processed to completion by the rest of
the application and network stacks without the need for any context
switching or queuing. To accomplish this, data need to be fetched
from storage just-in-time: in the typical case, the storage system
must be clocked off the TCP protocol’s ACK clock, because arriving
TCP ACKs cause TCP’s congestion window to inflate, allowing the
transmission of more data. Only when this happens can data from the
storage system be immediately consumed by the network without
adding a queue to cope with rate mismatches.
The outline of a solution then looks like:
(1) A TCP ACK arrives, freeing up congestion window.
(2) This triggers the stack to request more data from the SSD to
fill that congestion window.
(3) The SSDs return data, ideally placing it in the LLC.
(4) The read completion event causes the application to encrypt
the data in-place, add TCP headers, and trigger the transmis-
sion of the packets.
(5) The network completion event frees the buffer, allowing it to
be reused for a subsequent disk read.
This design closely couples all the pipeline stages, so maximises use
of the LLC, and never copies any data, though it does encrypt in
place. The hope is that memory accesses resemble Figure 5.
For this to work, the SSD must be capable of very low latency,
as it is placed directly in the TCP ACK-clock loop, and it must si-
multaneously be capable of high throughput. Today’s PCIe-attached
NVMe SSDs have low latency, but before building a system, we
profiled some drives to see how well they balance throughput and
latency. Figure 6 shows the request latency and disk throughput as a
function of the I/O window when making 16KB reads from an Intel
P3700 800GByte NVMe drive. NVMe drives maintain a request
queue, and the I/O window is the number of requests queued but
not yet completed. It is clear that with an I/O window of around 128
requests, these drives can achieve maximum throughput while si-
multaneously maintaining latency of under 1ms. This is significantly
smaller than the network RTT over typical home network links, even
to a server in the same city, indicating that we should be able to
place such an SSD directly into the TCP control loop.
3.1 Storage Stack
Traditional OS storage stacks pay a price in terms of efficiency to
achieve generality and safety. These inefficiencies include copy-
ing memory between kernel and userspace, extra abstraction layers
such as the Virtual File System, as well as POSIX API overheads
needed for backwards compatibility and portability. However, all
these overheads used to be negligible when compared to the latency
and throughput of spinning disks.
Today PCIe-attached flash and the adoption of new host controller
interfaces such as NVMe radically change the situation. Off-the-
shelf hardware can achieve read throughput up to 28Gbps and access
latencies as low as 20µs for short transfers [23]. However, if we
wish to integrate storage into the network fast path, we cannot afford
to pay the price of going through the conventional storage stack.
Our approach is to build a new high performance storage stack
that is better suited to integration into our network pipeline. Before
we explain its design and implementation though, we provide a brief
overview of how NVMe drives interface with the operating system.
215
NVMe	DRAM	LLC	DMA	NIC	Buﬀer	Cache	Copied	data	Encrypted	data	Copy	TCP	CPU	1	2	3	AES	DRAM	LLC	NIC	AES	TCP	CPU	re-use	buﬀer	NVMe	 0 0.5 1 1.5 2 2.5 3 0 100 200 300 400 500 600 0 5 10 15 20 25Mean Latency (ms)Throughput (Gb/s)I/O Window (16KB reads)LatencyDisk ThroughputSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ilias Marinos, Robert N.M. Watson, Mark Handley, and Randall R. Stewart
Function
nvme_open()
nvme_read()
nvme_write()
nvme_sqsync()
nvme_consume_completions()
Parameters
I/O qpair control block, character
device, buffer memory size, flags
I/O description block (struct
iodesc), metadata, error
I/O description block (struct
iodesc), metadata, error
I/O qpair control block
I/O qpair control block, number of
completions to consume
Description
Configure, initialize, and attach to NVMe disk’s queue pair.
Craft and enqueue a READ I/O request for a particular disk,
namespace, starting offset, length, destination address etc.
Craft and enqueue a WRITE I/O request for a particular disk,
namespace, starting offset, length, source buffer etc.
Update the NVMe device’s queue pair doorbell (via a dedicated
ioctl) to initiate processing of pending I/O requests.
Consumes completed I/O requests (takes care of out-of-order
completions). Invokes a per I/O request specific callback, set by the
application layer (via struct iodesc).
Table 1: libnvme API functions (not exhaustive).
3.1.1 NVMe disk operation and data structures
PCIe NVMe disk controllers use circular queues of command de-
scriptors residing in host memory to serve I/O requests for disk
logical blocks (LBAs). The host places NVMe I/O commands in a
submission queue; each command includes the operation type (e.g.,
READ, WRITE), the initial LBA address, the length of the request,
the source or destination buffer address in host main memory, and
various flags. Once commmands have been enqueued, the device
driver notifies the controller that there are requests waiting by updat-
ing the submission queue’s tail doorbell—this is a device register,
similar to NIC TX and RX doorbells for packet descriptors. Multiple
I/O commands can be in progress at a time, and the disk firmware
is allowed to perform out-of-order completions. For this to work,
each submission queue is associated with a completion queue. This
is used by the disk to communicate I/O completion events to the
host. The OS is responsible for consuming command completions,
and then notifies the controller via a completion queue doorbell so
that completion slot entries can be reused.
Unlike older SATA/AHCI Solid State Disks, which usually feature
a single submission/completion queue pair with a limited number of
slot entries, NVMe devices support a highly configurable number of
queue pairs and depths, which greatly helps with scaling to multiple
CPU cores and permits a share-free, lockless design.
3.1.2 Diskmap
Inspired by netmap [29], a high-performance packet processing
framework which maps the NIC buffer rings to userspace, we de-
signed and built diskmap, a conceptually similar system that uses
kernel-bypass to allow userspace applications to directly map the
memory used to DMA disk logical blocks to and from NVMe stor-
age. From a high-level viewpoint, diskmap and netmap have many
similarities, but the two DMA models and the nature of operations
are fundamentally different, so a different approach is required.
With diskmap, userspace applications are directly responsible for
crafting, enqueuing, and consuming I/O requests and completions,
while the OS uses hardware capabilities to enforce protection and
synchronization. The system is comprised of two parts:
• A kernel module used to initialize and configure devices that are
to be used in diskmap-mode, as well as providing a thin syscall
layer to abstract DMA operation initiation,
Figure 7: High-level architecture of a diskmap application.
• An accompanying userspace library which implements the
NVMe driver and provides the API (see table 1) to abstract
typical operations to the device such as read and write.
The architecture is shown in Figure 7. When the diskmap ker-
nel module loads, the NVMe device is partially detached from the
in-kernel storage stack: the actual datapath queue pairs are now
disconnected and readily available for attaching to user-level ap-
plications. The device administration queue pairs, however, remain
connected to the conventional in-kernel stack without being exposed
to userspace. This allows all low-level configuration and privileged
operations (e.g., device reset, nvmeformat) to continue working nor-
mally. It should be straightforward to allow some of the datapath
queue pairs to remain connected to the in-kernel stack, possibly
operating on different NVMe namespaces, but our implementation
currently does not support this mode of operation.
During initialization, the kernel pre-allocates non-pageable mem-
ory for all the objects that are required for NVMe device operations,
including submission and completion queues, PRP lists [22], and
the diskmap buffers themselves. These will be shared by the NVMe
hardware and the userspace applications and used for data transfers.
Relative addressing is used to calculate pointers for any object in
the shared memory region in a position-independent manner. Each
diskmap buffer descriptor holds a set of metadata information: a
unique index, the current buffer length, and the buffer address that
the libnvme library uses when constructing NVMe I/O commands
The buffer size and queue depths are configurable via sysctl knobs.
Similarly to netmap, the shared memory area is exposed by the
kernel via a dedicated character device.
To connect to the diskmap data path, a userspace application maps
the shared memory into its own virtual address space and issues a
216
kernel user PCIe NVME diskSQCQdiskmapbuﬀersI/O MMUSQCQmemorymapped(/dev/diskmap)SQCQlibnvmeCCdiskadminqpairapp1libnvmeapp2DMA DMA I/O MMUDMA DMA DMA Disk|Crypt|Net: rethinking the stack for high-performance video streaming
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
)
s
/
b
G
(
t
u
p
h
g
u
o