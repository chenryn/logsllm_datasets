title:Cache Games - Bringing Access-Based Cache Attacks on AES to Practice
author:David Gullasch and
Endre Bangerter and
Stephan Krenn
2011 IEEE Symposium on Security and Privacy
Cache Games – Bringing Access-Based Cache Attacks on AES to Practice
David Gullasch
Bern University of Applied Sciences,
Endre Bangerter
Bern University of Applied Sciences
Stephan Krenn
Bern University of Applied Sciences,
Dreamlab Technologies
PI:EMAIL
PI:EMAIL
University of Fribourg
PI:EMAIL
Abstract—Side channel attacks on cryptographic systems ex-
ploit information gained from physical implementations rather
than theoretical weaknesses of a scheme. In recent years, major
achievements were made for the class of so called access-driven
cache attacks. Such attacks exploit the leakage of the memory
locations accessed by a victim process.
In this paper we consider the AES block cipher and present
an attack which is capable of recovering the full secret key
in almost realtime for AES-128, requiring only a very limited
number of observed encryptions. Unlike previous attacks, we
do not require any information about the plaintext (such as its
distribution, etc.). Moreover, for the ﬁrst time, we also show
how the plaintext can be recovered without having access to
the ciphertext at all. It is the ﬁrst working attack on AES
implementations using compressed tables. There, no efﬁcient
techniques to identify the beginning of AES rounds is known,
which is the fundamental assumption underlying previous
attacks.
We have a fully working implementation of our attack
which is able to recover AES keys after observing as little
as 100 encryptions. It works against the OpenSSL 0.9.8n
implementation of AES on Linux systems. Our spy process does
not require any special privileges beyond those of a standard
Linux user. A contribution of probably independent interest
is a denial of service attack on the task scheduler of current
Linux systems (CFS), which allows one to observe (on average)
every single memory access of a victim process.
Keywords-AES; side channel; access-based cache attacks;
I. INTRODUCTION
Cryptographic schemes preventing conﬁdential data from
being accessed by unauthorized users have become in-
creasingly important during the last decades. Before being
deployed in practice, such schemes typically have to pass a
rigorous reviewing process to eliminate design weaknesses.
However, theoretical soundness of a scheme is necessary but
not sufﬁcient for the security of concrete implementations
of the scheme.
Side channel attacks are an important class of implemen-
tation level attacks on cryptographic systems. They exploit,
for instance, the leakage of information from electromag-
netic radiation or power consumption of a device, and
running times of certain operations. Especially, side channel
This work was in part funded by the European Community’s Seventh
Framework Programme (FP7) under grant agreement no. 216499 and the
Swiss Hasler Foundation.
1081-6011/11 $26.00 © 2011 IEEE
DOI 10.1109/SP.2011.22
490
attacks based on cache access mechanisms of microproces-
sors represented a vivid area of research in the last few
years [1]–[16]. These cache based side channel attacks (or
cache attacks for short) fall into the categories of time-
driven, trace-driven, and access-driven attacks.
In time-driven attacks an adversary is able to observe the
overall time needed to perform certain computations, such as
whole encryptions [9]–[12]. These timings leak information
about the overall number of cache hits and misses during an
encryption. In trace-driven attacks, an adversary is able to
obtain a proﬁle of the cache activity during an encryption,
and to deduce which memory accesses issued by the cipher
resulted in a cache hit [13]–[16]. Finally, access-driven
attacks additionally enable the adversary to determine the
cache sets accessed by the cipher [4]–[7]. Therefore, he
can infer, e.g., which elements of a lookup table have been
accessed by the cipher.
All
the fact
these three types of attacks exploit
that
accessing cached data is up to two orders of magnitude
faster than accessing data in the main memory. The attack
scenario underlying such attacks is a follows: Consider two
concurrently running processes (a spy process S and a
security sensitive victim process V ) using the same cache.
After letting V run for some small amount of time and
potentially letting it change the state of the cache, S observes
the timings of its own memory accesses, which depend on
the state of the cache. These measurements allow S to infer
information about the memory locations previously accessed
by V .
A. Our Contributions
In a nutshell, we present a novel, practically efﬁcient
access-driven cache attack on the Advanced Encryption
Standard (AES) [17], [18], which is the most widely used
symmetric-key block cipher today. On a high level the main
features of our attack are as follows: First, our attack works
under very weak assumptions, and thus is the strongest
working access-driven attack currently known. Second, we
provide a concrete and practically usable implementation of
the attack. It uses new techniques and also resolves a series
of so far open issues and technicalities.
Let us discuss our results in more detail. For our attack to
work we need to assume that the attacker has a test machine
at his disposal prior to the attack, which is identical to the
victim machine. The test machine is used to generate training
samples for two artiﬁcial neural networks from 168 000
encryptions. These then have to be trained on an arbitrary
platform.
To carry out the attack all we need to be able to execute
a non-privileged spy process (e.g., our spy process does not
need to have access to the network interface) on the victim
machine. We do not require any explicit interactions, such
as inter-process communication or I/O. Osvik et al. [7], [8]
refer to attacks in this setting as asynchronous attacks.
Our attack technique has the following features:
• In contrast to previous work [6]–[8], our spy process
neither needs to learn the plain- or ciphertexts involved,
nor their probability distributions in order recover the
secret key.
• For the ﬁrst time, we describe how besides the key
also the plaintext can be recovered without knowing
the ciphertexts at all.
• Our attack also works against AES implementations
using so called compressed tables, which are typically
used in practice, e.g., in OpenSSL [19]. When using
compressed tables, the ﬁrst and the last round of an
encryption typically cannot be identiﬁed any more,
which renders previous attacks impossible.
• We have a fully working implementation of our attack
techniques against the 128-bit AES It is highly efﬁ-
cient and is able to recover keys in “realtime”. More
precisely, it consists of two phases: In an observa-
tion phase, which lasts about 2.8 seconds on our test
machine, approximately 100 encryptions have to be
monitored. Then an ofﬂine analysis phase lasting about
3 minutes recovers the key. The victim machine only
experiences a delay during the observation phase. This
slowdown is sufﬁciently slight to not raise suspicions,
since it might as well be caused by high network
trafﬁc, disk activity, etc. To the best of our knowledge,
this is the ﬁrst fully functional implementation in the
asynchronous setting.
• At the heart of the attack is a spy process which is able
to observe (on average) every single memory access of
the victim process. This extremely high granularity in
the observation of cache hits and misses is reached by
a new technique exploiting the behavior of the Com-
pletely Fair Scheduler (CFS) used by modern Linux
kernels. We believe that this scheduler attack could be
of independent interest.
B. Test Environment
All our implementations and measurements have been ob-
tained on a Intel Pentium M 1.5 GHz (codename “Banias”)
processor, in combination with an Intel ICH4-M (codename
“Odem”) chipset using 512 MB of DDR-333 SDRAM. On
this system, we were running Arch Linux with kernel version
491
2.6.33.4. As a victim process we used the OpenSSL 0.9.8n
implementation of AES, using standard conﬁgurations.
C. Related Work
It was ﬁrst mentioned by Kocher [20] and Kelsey et
al. [21] that cache behavior potentially poses a security
threat. The ﬁrst formal studies of such attacks were given
by Page [22], [23].
First practical results for time-driven cache attacks on the
Data Encryption Standard (DES) were given by Tsunoo et
al. [2], and an adoption for AES was mentioned without
giving details. Various time-driven attacks on AES were
given in the subsequent [7]–[12], some of which require
that the ﬁrst or the last round of AES can be identiﬁed. Tiri
et al. [24] proposed an analytical model for forecasting the
security of symmetric ciphers against such attacks.
Trace-driven cache attacks were ﬁrst described by
Page [22], and various such attacks on AES exist [13]–
[16]. Especially, Aciic¸mez et al. [13] also propose a model
for analyzing the efﬁciency of trace-driven attacks against
symmetric ciphers.
Percival [4] pioneered the work on access-driven attacks
and described an attack on RSA. Access-driven attacks on
AES were ﬁrst investigated by Osvik et al. [7], [8]. They
describe various attack techniques and implementations in
what they call the synchronous model. This model makes
rather strong assumptions on the capabilities of an attacker,
i.e., it assumes that an attacker has the ability to trigger en-
cryptions for known plaintexts and know when an encryption
has begun and ended. Their best attack in the synchronous
model requires about 300 encryptions.
Osvik et al. also explore the feasibility of asynchronous
attacks. They refer to asynchronous attacks as an “extremely
strong type of attack”, and describe on a rather high level
how such attacks could be carried out, assuming that the
attacker knows the plaintext distribution and that the attack is
carried out on a hyper-threaded CPU. Also, they implement
and perform some measurements on hyper-threaded CPUs
which allow to recover 47 key bits. However, a description
(let alone an implementation) of a full attack is not given
and many open questions are left unresolved. Further, the
authors conjecture that once ﬁne-grained observations of
cache accesses are possible, the plaintext distribution no
longer needs to be known. Loosely speaking, one can
say that Osvik et al. postulate fully worked and practical
asynchronous attacks as an open problem.
This is where the work of Neve et al. [6] picks up.
They make progress towards asynchronous attacks. To this
end they describe and implement a spy process that
is
able to observe a “few cache accesses per encryption”
and which works on single threaded CPUs. They then
describe a theoretical known ciphertext attack to recover
keys by analyzing the last round of AES. The practicality
of their attack remains unclear, since they do not provide an
implementation and leave various conceptual issues (e.g.,
quality of noise reduction, etc.) open.
Acıic¸mez et al. [5] are the ﬁrst to present a practical
access-driven attack in the asynchronous model. Albeit tar-
getting OpenSSL’s DSA implementation via the instruction
cache on a hyper-threaded CPU, they contribute a clever
routine to perform timing measurement of the instruction
cache in a real-world setting.
We improve over prior work by providing a ﬁrst practical
access-driven cache attack on AES in the asynchronous
model. The attack works under weaker assumptions than
previous ones as no information about plain- and ciphertext
is required1, and it is more efﬁcient in the sense that we
only need to observe about 100 encryptions. We also reach a
novelly high granularity when monitoring memory accesses.
Further, our attack also works against compressed tables,
which were not considered before.
Finally, several hardware and software based mitigation
strategies for AES have been proposed [25]–[27].
D. Document Outline
In §II we brieﬂy recapitulate the structure of a CPU
cache and the mechanisms underlying it. We also describe
the Advanced Encryption Standard (AES) to the extent
necessary for our attack. In §III we then explain how to
recover the AES key under the assumption that one is able
to perfectly observe single cache accesses performed by
the victim process. We drop this idealization in §IV and
show that by combining a novel attack on the task scheduler
and neural networks sufﬁciently good measurements can be
obtained to carry out the attack in practice. We also state
measurement results obtained from the implementation of
our attack. In §V we present extensions of our attack, and
countermeasures in §VI. We conclude with a discussion of
the limitations of our attack and potential future work in
§VII.
II. PRELIMINARIES
We ﬁrst summarize the functioning of the CPU cache
as far as necessary for understanding our attack. We then
describe AES, and give some details on how it is typically
implemented. We close this section by describing the test
environment on which we obtained our measurements.
A. The CPU Cache and its Side Channels
Let us describe the behavior of the CPU cache, and how it
can be exploited as a side channel. The CPU cache is a very
fast memory which is placed between the main memory and
the CPU [28]. Its size typically ranges from some hundred
kilobytes up to a few megabytes.
1To be precise, the statement is true whenever AES is used, e.g., in CBC
or CTR mode, which is the case for (all) relevant protocols and applications.
In the practically irrelevant case, where the ECB mode (which is known
to be insecure by design) is used we have to require that there is some
randomness in the plaintext.
Typically, data the CPU attempts to access is ﬁrst loaded
into the cache, provided that it is not already there. This
latter case is called a cache hit, and the requested data
can be supplied to the CPU core with almost no latency.
However, if a cache miss occurs, the data ﬁrst has to be
fetched via the front side bus and copied into the cache, with
the resulting latency being roughly two orders of magnitude
higher than in the former case. Consequently, although being
logically transparent, the mechanics of the CPU cache leak
information about memory accesses to an adversary who is
capable of monitoring cache hits and misses.
To understand this problem in more detail it is necessary
to know the functioning of an n-way associative cache,
where each physical address in the main memory can be
mapped into exactly n different positions in the cache. The
cache consists of 2a cache sets of n cache lines each. A
cache line is the smallest amount of data the cache can work
with, and it holds 2b bytes of data together with tag and state
bits. Cache line sizes of 64 or 128 bytes (corresponding to
b = 6 and b = 7, respectively) are prevalent on modern x86-
and x64 architectures.
To locate the cache line holding data from address
A = (Amax, . . . , A0), the b least signiﬁcant bits of A can
be ignored, as a cache line always holds 2b bytes. The
next a bits, i.e., (Aa+b−1, . . . , Ab) identify the cache set.
The remaining bits, i.e., (Amax, . . . , Aa+b) serve as a tag.
Now, when requesting data from some address A, the cache
logic compares the tag corresponding to A with all tags
in the identiﬁed cache set, to either successfully ﬁnd the
sought cache line or to signal a cache miss. The state bits
indicate if the data is, e.g., valid, shared or modiﬁed (the
exact semantics are implementation deﬁned). We typically
have max = 31 on x86 architectures and max = 63 on
x64 architectures. (However, the usage of physical address
extension techniques may increase the value of max [29].)
Addresses mapping into the same cache set are said to
alias in the cache. When more then n memory accesses to
different aliasing addresses have occurred, the cache logic
needs to evict cache lines (i.e. modiﬁed data needs to be
written back to RAM and the cache line is reused). This
is done according to a predetermined replacement strategy,
most often an undocumented algorithm (e.g. PseudoLRU in
x86 CPUs), approximating the eviction of the least recently
used (LRU) entry.
With these mechanics in mind, one can see that there
are at least two situations where information can leak to
an adversary in multitasking operating systems (OS). Let’s
therefore assume that a victim process V , and a spy process
S are executed concurrently, and that the cache has been
initialized by S. After running V for some (small) amount
of time, the OS switches back to S.
• If S and V physically share main memory (i.e., their
virtual memories map into the same memory pages in
RAM), S starts by ﬂushing the whole cache. After
492
regaining control over the CPU, S reads from mem-
ory locations and monitors cache hits and misses by
observing the latency. Hits mark the locations of V ’s
memory accesses.
• If S and V do not physically share memory,
then
they typically have access to cache aliasing memory.
In this case, S initializes the cache with some data D,
and using its knowledge of the replacement strategy,
it deterministically prepares the individual cache line
states. When being scheduled again, S again accesses
D, and notes which data had been evicted from the
cache. This again allows S to infer information about
the memory accesses of V .
Our target in the following is the OpenSSL library on
Linux, which in practice resides at only one place in physical
memory and is mapped into the virtual memory of every
process that uses it. In this paper we are therefore concerned
with the shared-memory scenario, where V uses lookup
tables with 2c entries of 2d bytes each, and uses a secret
variable to index into it. We will further make the natural
assumption of cache line alignment, i.e., that the starting
point of these lookup tables in memory corresponds to a
cache line boundary. For most compilers, this is a standard
option for larger structures. Exploiting the previously men-
tioned information leakage will allow S to infer the memory