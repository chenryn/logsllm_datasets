Structural Features No. of offspring
Betweeness
Weight (α)
10.82
14.47
6.54
66.22
41.37
55.65
198.67
30.66
4. METHODOLOGY
Section 3 introduced the notion of Attributed Control Flow Graph
(ACFG), which will be used as the raw feature set for Genius.
This section discusses how we utilize these ACFGs and transform
Figure 2: The deployment of Genius.
Deployment: Figure 2 shows two use scenarios for Genius. 1)
Scenario I: Given a device repository, Genius will index func-
tions in the ﬁrmware images of all devices in the repository based
upon their CFGs. When a new vulnerability is released, a secu-
rity professional can use Genius to search for this vulnerability in
their device repositories . Genius will generate the query for the
vulnerability and query in the indexed repository. The outputs will
be a set of metadata about all potentially infected devices with their
brand names, library names and the potentially vulnerable func-
tions. All outputs will be ranked by their similarity scores for quick
screening of the results. 2) Scenario II: Security professionals may
upload unseen ﬁrmware images that do not exist in the repository
for a comprehensive vulnerability scan. In this case, Genius will
index these ﬁrmware images for the security professionals. As a
result, they can simply query any vulnerabilities in our vulnerabil-
ity database. Genius will retrieve the most similar vulnerabilities
in the existing indexed ﬁrmware images and output metadata of
all potentially vulnerable functions including their names, library
names holding these functions and ﬁrmware device types where
these functions are used. Again, all outputs will be ranked by their
similarity scores to facilitate quick screening of the results.
3. RAW FEATURE EXTRACTION
The CFG (Control Flow Graph) is the common feature used in
bug search. More recently, different attributes on the basic blocks,
such as I/O pairs and statistic features [23, 45], are explored to
increase the matching accuracy. Following the idea, this paper
utilizes the control ﬂow graph with different basic-block level at-
tributes called ACFG (Attributed Control Flow Graph) as the raw
features to model the function in our problem.
CVE-2014-0160CVE-2014-3566FuzzySearchCVE-xxxx-xxxxVulnerability DBFirmware DBFirmware imageCVE-2014-0160 0.02 [sub_0x1, sub 0x2]CVE-2014-3566  0.8 [sub_0x5, sub_0x3]...Results:Results:outputddwrt_xxx.libxx.sub_0x123: 0.001DAP-_xxx.libxx.sub_0x123: 0.001(cid:258)(cid:1)(cid:4)(cid:3)(cid:2)(cid:4)(cid:3)482them into high-level feature vectors that are suitable for scalable,
accurate bug search.
4.1 Codebook Generation
The ﬁrst step in the proposed method is codebook generation,
which aims at learning a set of categorizations, that is codewords,
from raw features. Formally, a codebook C is a ﬁnite and discrete
set: C = {c1, c2, . . . , ck}, where ci is the i-th codeword, or “cen-
troid”, and i is the integer index associated with that centroid. The
codebook is generated from a training set of raw features by an
unsupervised learning algorithm. In our case, the raw features are
the control ﬂow graphs. The codebook generation consists of two
phases: similarity metric computation and clustering.
4.1.1 Raw Feature Similarity
We consider the raw feature similarity computation as a labeled
graph matching problem. By deﬁnition, ACFGs are matched not
only by their structures but also by their labels (attributes) on the
structures. Theoretically, graph matching is an NP-complete prob-
lem, but many techniques have been proposed to optimize the pro-
cess for an approximate matching result [14,49]. For efﬁciency, we
utilize bipartite graph matching to quantify ACFG similarity. Al-
though other approaches such as MCS (Maximum Common Sub-
graph) matching [14] may also be applied to this problem, efﬁ-
ciency is still a major concern. The primary limitation of bipartite
graph matching is that it is agnostic to the graph structure, and the
accumulation of errors could result in less accurate results. To ad-
dress the issue, we have appended structural features, described in
Section 3, to allow bipartite graph matching to incorporate some
graph structural information. Experiments in Section 5.4 show
that these structural features boost the accuracy of bipartite graph
matching in our problem.
Essentially, bipartite graph matching utilizes the match cost of
two graphs to compute the similarity. It quantiﬁes the match cost
of two graphs by modeling it as an optimization process. Given two
ACFGs, G1 and G2, the bipartite graph matching will combine two
ACFGs as a bipartite graph Gbp = ( ˆV , ˆE), where ˆV = V (G1) ∪
V (G2), ˆE = { ˆek = (vi, vj)|vi ∈ V (G1) ∧ vj ∈ V (G2)}, and
edge ˆek = (vi, vj) indicates a match from v1 to v2. Each match is
associated with a cost. The minimum cost of two graphs is the sum
of all edges cost on the mapping. Bipartite graph matching can go
over all mappings efﬁciently, and select the one-to-one mapping on
nodes from G1 to G2 of the minimum cost.
In our problem, a node in the bipartite graph is a basic block on
the ACFG. The edge cost is calculated by the distance between the
two basic blocks on that edge. Each basic block on the ACFG has
(cid:80)
a feature vector discussed in Section 3. Therefore, we calculate the
(cid:80)
i αi|ai−ˆai|
i αi max(ai,ˆai) .
distance between two basic blocks by cost(v, ˆv) =
It is the same distance metric used in the paper [23] to quantify the
distance of two basic blocks. ai and ˆai are the i-th feature in fea-
ture vectors of two basic block v and ˆv respectively. If the feature
is a constant, |ai − ˆai| is their difference. If the feature is a set, we
use Jaccard to calculate the set difference. αi is the corresponding
weight of the feature which will be discussed below.
The output of bipartite graph matching is the minimum cost of
two graphs. Normally the match cost of two graphs is greater
than one, and positively correlated to the size of compared ACFGs.
Therefore, we normalize the cost to compute the similarity score.
For cost normalization, we create an empty ACFG Φ for each com-
pared ACFG. Each node in the empty graph has an empty feature
vector, and the size of the empty graph is set to that of the corre-
sponding compared graph. By comparing with this empty ACFG,
we can obtain the maximum matching cost the compared graph can
produce. We compute the matching cost with the empty graph for
the two graphs, and select the maximum matching cost as the de-
nominator, and use it to normalize the matching cost of two graphs.
Suppose cost(gi, gj) represents the cost of the best bipartite match-
ing between two graphs g1, g2, the ACFG similarity between two
graphs can be formally represented as following:
κ(g1, g2) = 1 −
cost(g1, g2)
max(cost(g1, Φ), cost(Φ, g2))
,
(1)
We found that the features in Table 1 have different importance
in computing graph similarity. We learn weights of the raw features
to capture the latent similarity between two ACFGs. Basically, the
learning objective is to ﬁnd weight parameters that can maximize
the distance of different ACFGs while simultaneously minimizing
the distance of equivalent ACFGs. To approach this optimization
problem, we adopt the approach used in Eschweile et al [23]. More
speciﬁcally, we use a genetic algorithm using GALib [56]. We
also execute an arithmetic crossover using a Gaussian mutator 100
times. The learned weights for each feature are listed in Table 1.
4.1.2 Clustering
After deﬁning the similarity metric for the ACFG, the next step
is to generate a codebook using the unsupervised learning method.
This process can be regarded as a clustering process over a col-
lection of raw features: ACFGs, where each cluster comprises a
number of similar ACFGs.
In this paper, we use spectral clustering [43] as the unsupervised
learning algorithm to generate the codebook. Formally, the spectral
clustering algorithm partitions the training set of ACFGs into n sets
S = {S1, S2, . . . , Sn} so as to minimize the sum of the distance
of every ACFG to its cluster center. ci ∈ C is the centroid for the
subset Si. We deﬁne the centroid node as the ACFG that has the
minimum distance to all the other ACFGs in Si, and the collection
of all centroid nodes constitutes a code book.
Unlike traditional clustering algorithms, in which the inputs are
numerical vectors, in this paper we propose to use a kernelized
spectral clustering where the input is a kernel matrix. The similar-
ity computed in Section 3 can be used to generate the kernel matrix
for the spectral clustering. Suppose the kernel matrix is M, and
each entry in M is a similarity score of two corresponding ACFGs.
The kernelized clustering works on M and outputs the optimal par-
titions (clusters) of ACFGs in the training data.
The codebook size n would affect the bug search accuracy. To
this end, we systematically study a suitable n in the bug search in
Section 5 and demonstrate that n = 16 seems to be a reasonable
codebook size trading off efﬁciency and accuracy.
In order to reduce computational cost in constructing the code-
book, a common strategy is to randomly sample a training set from
the entire dataset. We observed that there is a signiﬁcant variance in
ACFG size. To reduce the sampling bias, we ﬁrst collect a dataset
which covers ACFGs of different functions from various architec-
tures. See Section 5.2. Then split ACFGs into separate “strata”
with different size ranges. Each stratum is then sampled as an
independent sub-population, out of which individual ACFGs are
randomly selected. This is a commonly used approach known as
stratiﬁed sampling [50].
The codebook generation is expensive. However, since the code-
book generation is an ofﬂine and one-time effort, it will not detri-
mentally impact the runtime for the online searches. Besides, some
approaches can be used to expedite this process, such as the par-
483allelled clustering approximate clustering [12] or the hierarchical
clustering algorithm [40].
4.2 Feature Encoding
Given a learned codebook, feature encoding is to map raw fea-
tures of a function into a higher-level numeric vector, each dimen-
sion of which is the similarity distance to a categorization in the
codebook. This step is known as feature encoding [16].
There are two notable beneﬁts for feature encoding. First, the
higher-level feature can better tolerate the variation of a function
across different architectures, as each of its dimensions is the sim-
ilarity relationship to a categorization which is less sensitive to the
variation of a binary function than the ACFG itself. This property
has been demonstrated by many practices in the image search to re-
duce the noises from the scale, viewpoint and lighting. We further
demonstrate it in the bug search scenario in Section 5.3. Second,
the ACFG raw features after encoding becomes a point in the high
dimensional space which can be conveniently indexed and searched
by existing hashing methods. Therefore, the encoding enables a
faster real-time bug-search system. See Section 5.5.
Formally, the feature encoding is to learn a quantizer q : G →
Rn over the codebook C = {c1, ..., cn}, where G is the set of
all ACFG graphs following Deﬁnition 1, and Rn represents the n-
dimensional real space. In this paper, we discuss two approaches
to derive q. For a given graph gi, let N N (gi) represent the nearest
centroid neighbors in the codebook:
N N (gi) = arg max
cj∈C κ(gi, cj)
(2)
where κ is deﬁned in Eq. (3). A common practice in image re-
trieval is to consider not only the nearest neighbor but a few nearest
neighbors, e.g. 10 nearest neighbors [31, 59].
Bag-of-feature encoding. The bag-of-feature encoding, which
maps a graph to some centroids in the codebook, represents each
function as a bag of features. The bag-of-feature quantizer can be
deﬁned as:
(cid:88)
(cid:88)
q(gi) =
[1(1 = j), . . . , 1(n = j)]T ,
(3)
gi:N N (gi)=cj
where 1(·) is an indicator function which equals 1 when · is true
and 0 otherwise. Eq. (3) indicates that the output encoded feature
will add 1 to the corresponding dimension of the nearest centroid.
This representation is inspired by the bag-of-words model used in
text retrieval [38], where each document is represented by a collec-
tion of terms in the English vocabulary. In analogy, in our problem,
each function is represented by a collection of representative graphs
in the learned codebook. After encoding each function becomes a
point in the high dimensional vector space.
VLAD encoding. The drawback of the bag-of-word model is
that the distance between a given graph and a centroid is completely
ignored as long as the centroid is the graph’s nearest neighbors. The
VALD [10] encoding was proposed to incorporate the ﬁrst-order
differences and assigns a graph to a single mixture component.
q(gi) =
[1(1 = j)κ(gi, c1), ..., 1(n = j)κ(gi, cn)]T ,
gi:N N (gi)=cj
(4)
Compared to Eq. (3), Eq. (4) adds the similarity information to
the centroids in the encoded features. Note as our raw features
are graphs, in Eq. (4) we use the kernelized similarity function in
the VALD encoding which is different from the traditional VALD
deﬁned for image retrieval. In VALD encoding, a dimension repre-
sents the similarity to a corresponding ACFG centroid in the code-
book. As a result, the vector is of latent semantic meaning that
reﬂects a similarity distribution across all centroids in the learned
codebook. Empirically we found that VLAD encoding performs
better than the bag-of-feature encoding for bug search.
Figure 3: A toy example on VALD feature encoding. Features
on each basic block of ACFG are simpliﬁed into a single constant
value. The match cost of two basic blocks with the same value is
zero, otherwise 1.
We will walk the encoding algorithm in a toy example in Fig. 3.
Given ACFGs for three functions F1_x86, F1_mips and F2_mips,
where the ﬁrst two are the same function compiled from different
architectures, and the last one is a completely different function.
The compiler used for F1_mips merges the basic block 4 and 5 into
the single node 8, so the ACFG of function F1 in MIPS is different
from that in x86, due to the instruction reordering. F2_mips shares
partial code with function F1_mips. For example, they both check
some environment conditions and directly return if it fails.
For clarity, the similarity metric between ACFGs used in Fig. 3
adopts maximum common subgraph matching. For example, the
similarity score between F1_x86 and F1_mips is 5/7 where 5 means
the maximum common subgraph, and 7 means the maximum size
between two graphs.
The pair-wise match will match two functions directly by their
ACFGs, whereas Genius will match them by their encoded vec-
tors. VLAD encoding generates the encoded vector by compar-
ing a ACFG to its top 3 closest centroid nodes in the codebook
in Fig. 3b). Different from BOF model, it will store the similarity
score to each centroid node into the corresponding dimension in the
vector. The resulting feature vector is shown in Fig. 3d) (bottom-
right corner). Fig. 3d) also lists match results for both pair-wise
match and Genius. The distance metrics used by Genius will
be discussed in Section 4.3. We can see that the pair-wise graph
match fails to match F1_x86 to F1_mips, since it matches two func-
tions locally. On the contrary, Genius can still match these two
functions with high similarity score, as the encoded feature vector
is more invariant to local changes on an ACFG. Note, this toy exam-
ple is only for illustration and we will substantiate our hypothesis
by extensive experiments on real-world datasets in Section 5.3.