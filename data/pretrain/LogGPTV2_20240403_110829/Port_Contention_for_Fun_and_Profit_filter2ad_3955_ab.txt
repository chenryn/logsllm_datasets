cycle performance. Any pairwise timing discrepancies in the
resulting table indicate the potential for a covert channel,
where the source of the leakage originates from any number
of shared SMT microarchitecture components. Fogh explic-
itly mentions caching of decoded uops, the reorder buffer,
port congestion, and execution unit congestion as potential
sources, even reproducing the rdseed covert channel [14]
that remarkably works across physical cores.
Covert channels from Covert Shotgun can be viewed as a
higher abstraction of the integer multiplication unit contention
covert channel by Wang and Lee [3], and our side-channel
a higher abstraction of the corresponding side-channel by
Acıiçmez and Seifert [13]. Now limiting the discussion to port
contention, our attack focuses on the port sharing feature. This
allows a darker-box analysis of the targeted binary because
there is no need to know the exact instructions executed by
the victim process, only that the attacker must determine the
distinguishable port through trial and error. This feature is
very helpful, for example, in a scenario where the targeted
code is encrypted and only decrypted/executed inside an SGX
enclave [15].
Analogous to [11], Acıiçmez et al. [16] performed a cache-
timing attack against OpenSSL DSA, but this time targeting
1https://cyber.wtf/2016/09/27/covert-shotgun/
the L1 instruction cache. The authors demonstrate an L1
instruction cache attack in a real-world setting and using
analysis techniques such as vector quantization and hidden
Markov models, combined with a lattice attack, they achieve
DSA full key recovery on OpenSSL version 0.9.8l. They
perform their attack on an Intel Atom processor featuring
Hyper-Threading. Moreover, due to the relevance and threat
of cache-timing attacks, the authors list and evaluate several
possible countermeasures to close the cache side-channels.
the authors target
More recently, Yarom et al. [5] presented CacheBleed,
a new cache-timing attack affecting some older processors
featuring Hyper-Threading such as Sandy Bridge. The authors
exploit the fact that cache banks can only serve one request
at a time, thus issuing several requests to the same cache
bank,
i.e., accessing the same offset within a cache line,
results in bank contention, leading to timing variations and
leaking information about low address bits. To demonstrate
the attack,
the RSA exponentiation in
OpenSSL 1.0.2f. During exponentiation, RSA uses the scatter-
gather method adopted due to Percival’s work [11]. More
precisely, to compute the exponentiation, the scatter-gather
method accesses the cache bank or offset within a cache line
according to the multiplier used, which depends on a digit
of the private key. Thus, by detecting the used bank through
cache bank contention timings, an attacker can determine the
multiplier used and consequently digits of the private key. The
attack requires very ﬁne granularity, thus the victim and the
spy execute in different threads in the same core, and after
observing 16,000 decryptions, the authors fully recover 4096-
bit RSA private keys.
In 2018, Gras et al. [4] presented TLBleed, a new class
of side-channel attacks relying on the Translation Lookaside
Buffers (TLB) and requiring Hyper-Threading to leak infor-
mation. In their work, the authors reverse engineer the TLB
architecture and demonstrate the TLB is a (partially) shared
resource in SMT Intel architectures. More speciﬁcally, the L1
data TLB and L2 mixed TLB are shared between multiple
logical cores and a malicious process can exploit
this to
leak information from another process running in the same
physical core. As a proof-of-concept, the authors attack a non
constant-time version of 256-bit EdDSA [17] and a 1024-
bit RSA hardened against FLUSH+RELOAD as implemented
in libgcrypt. The EdDSA attack combined with a machine-
learning technique achieves a full key recovery success rate
of 97%, while the RSA attack recovers 92% of the private
key but the authors do not perform full key recovery. Both
attacks are possible after capturing a single trace.
III. INSTANTIATING COVERT SHOTGUN
Being an automated framework, Covert Shotgun is a power-
ful tool to detect potential leakage in SMT architectures. But
due to its black-box, brute-force approach, it leaves identifying
the root cause of leakage as an open problem: “Another
interesting project would be identifying [subsystems] which
are being congested by speciﬁc instructions”. In this section,
we ﬁll
to port contention.
this research gap with respect
(cid:25)(cid:24)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
Our intention is not to utilize this particular covert channel
in isolation, but rather understand how the channel can be
better optimized for its later conversion to a side-channel in
Section IV.
A. Concept
Assume cores C0 and C1 are two logical cores of the same
physical core. To make efﬁcient and fair use of the shared EE,
a simple strategy for port allocation is as follows. Denote i the
clock cycle number, j = i mod 2, and P the set of ports.
1) Cj is allotted Pj ⊆ P such that |P \ Pj| is minimal.
2) C1−j is allotted P1−j = P \ Pj.
There are two extremes in this strategy. For instance, if C0
and C1 are executing fully pipelined code with no hazards,
yet make use of disjoint ports, then both C0 and C1 can issue
in every clock cycle since there is no port contention. On the
other hand, if C0 and C1 are utilizing the same ports, then
C0 and C1 alternate, issuing every other clock cycle, realizing
only half the throughput performance-wise.
Consider Alice and Bob, two user space programs, exe-
cuting concurrently on C0 and C1, respectively. The above
strategy implies the performance of Alice depends on port
contention with Bob, and vice versa. This leads to a covert
timing channel as follows. Take two latency-1 instructions:
NOP0 that can only execute on port 0, and NOP1 similarly
on port 1. Alice sends a single bit of information to Bob as
follows.
1) If Alice wishes to send a zero, she starts executing NOP0
continuously; otherwise, a one and NOP1 instead.
2) Concurrently, Bob executes a ﬁxed number of NOP0
instructions, and measures the execution time t0.
3) Bob then executes the same ﬁxed number of NOP1
instructions, and measures the execution time t1.
4) If t1 > t0, Bob receives a one bit; otherwise, t0 > t1
and a zero bit.
The covert channel works because if both Alice and Bob
are issuing NOP0 instructions, they are competing for port 0
and the throughput will be cut in half (similarly for NOP1
and port 1). On the other hand, with no port contention both
NOP0 and NOP1 execute in the same clock cycle, achieving
full throughput and lower latency.
B. Implementation
In this section, we give empirical evidence that Intel Hyper-
Threading uses the previous hypothetical port allocation strat-
egy for SMT architectures (or one indistinguishable from it
for our purposes). Along the way, we optimize the channel
with respect to pipeline usage, taking into account instruction
latencies and duplicated execution units.
In these experiments, we used an Intel Core i7-7700HQ
Kaby Lake featuring Hyper-Threading with four cores and
eight threads. Using the perf tool to monitor uops dispatched
to each of the seven ports and the clock cycle count for a
ﬁxed number of instructions, we determined the port footprint
and performance characteristics of several instructions, listed
in Table I. We chose this mix of instructions to demonstrate
SELECTIVE INSTRUCTIONS. ALL OPERANDS ARE REGISTERS, WITH NO
MEMORY OPS. THROUGHPUT IS RECIPROCAL.
TABLE I
Instruction
add
crc32
popcnt
vpermd
vpbroadcastd
Ports
0 1 5 6
1
1
5
5
Latency
1
3
3
3
3
Throughput
0.25
1
1
1
1
RESULTS OVER A THOUSAND TRIALS. AVERAGE CYCLES ARE IN
THOUSANDS, RELATIVE STANDARD DEVIATION IN PERCENTAGE.
TABLE II
Alice
Port 1
Port 1
Port 5
Port 5
Bob
Port 1
Port 5
Port 1
Port 5
Diff. Phys. Core
Same Phys. Core
Cycles
203331
203322
203334
203328
Rel. SD
0.32%
0.25%
0.31%
0.26%
Cycles
408322
203820
203487
404941
Rel. SD
0.05%
0.07%
0.07%
0.05%
the extremes: from add that can be issued to any of the
four integer ALUs behind ports 0, 1, 5, or 6, to crc32
and vpermd that restrict to only ports 1 and 5, respectively.
Furthermore, to minimize the effect of the memory subsystem
on timings (e.g., cache hits and misses), in this work we do not
consider any explicit store or load instructions, or any memory
operands to instructions (i.e., all operands are registers).
Given the results in Table I, we construct the covert channel
as follows: crc32 (port 1) will serve as the NOP0 instruction,
and vpermd (port 5) as NOP1. Note that this conﬁguration is
one of the n2 brute-force pairs of Covert Shotgun. However,
as we are targeting port contention we take into account
instruction latency, throughput, and port usage to maximize
its impact. Being crc32 and vpermd latency-3 instructions,
we construct a block of three such instructions with disjoint
operands to ﬁll the pipeline, avoid hazards, and realize a
throughput of one instruction per clock cycle. We repeated
each block 64 times to obtain a low ratio of control ﬂow logic
to overall instructions retired. The Alice program sends a zero
bit by executing the repeated crc32 blocks in an inﬁnite loop.
Concurrently on the receiver side, using perf, we measured
the number of clock cycles required for the Bob program
to execute 220 of the repeated crc32 blocks, then again
measured with the same number of repeated vpermd blocks.
We then repeated the experiment with Alice sending a one
bit analogously with the vpermd instruction. We carried out
the experiments with both Alice and Bob pinned to separate
logical cores of the same physical core, then also different
physical cores. As a rough estimate, for full throughput we
expect 3 · 64 · 220 ≈ 201 million cycles (three instructions,
with 64 repetitions, looping 220 times); even with a latency of
three, our construction ensures a throughput of one. Of course
there is some overhead for control ﬂow logic.
Table II contains the results, averaged over a thousand trials.
First on separate physical cores, we see that the cycle count is
(cid:25)(cid:24)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
essentially the same and approaches our full throughput esti-
mate, regardless of which port Alice and/or Bob are targeting.
This conﬁrms the channel does not exist across physical cores.
In contrast, the results on the same physical core validates
the channel. When Alice and Bob target separate ports, i.e.,
the port 1/5 and 5/1 cases, the throughput is maximum and
matches the results on different physical cores. However, when
targeting the same port, i.e., the port 1/1 and 5/5 cases, the
throughput halves and the cycle count doubles due to the port
contention. This behavior precisely matches the hypothesis in
Section III-A.
IV. FROM COVERT TO SIDE-CHANNEL
One takeaway from the previous section is that, given two
user space programs running on two separate logical cores
of the same physical core, the clock cycle performance of
each program depends on each other’s port utilization. Covert
Shotgun leaves extension to side-channels as an open problem:
“it would be interesting to investigate to what extent these
covert channels extend to spying”. In this section, we ﬁll this
research gap by developing PORTSMASH, a new timing side-
channel vector via port contention.
At a high level, in PORTSMASH the goal of the Spy is
to saturate one or more ports with a combination of full
instruction pipelining and/or generous instruction level par-
allelism. By measuring the time required to execute a series
of said instructions, the Spy learns about the Victim’s rate and
utilization of these targeted ports. A higher latency observed
by the Spy implies port contention with the Victim, i.e., the
Victim issued instructions executed through said ports. A lower
latency implies the Victim did not issue such instructions,
and/or stalled due to a hazard or waiting due to, e.g., a cache
miss. If the Victim’s ability to keep the pipeline full and utilize
instruction level parallelism depends on a secret, the Spy’s
timing information potentially leaks that secret.
As a simple example conceptually related to our later
application in Section V, consider binary scalar multiplication
for elliptic curves. Each projective elliptic curve point double
and conditional add is made up of a number of ﬁnite ﬁeld
additions, subtractions, shifts, multiplications, and squarings.
These ﬁnite ﬁeld operations utilize the pipeline and ports in
very different ways and have asymptotically different running
times. For example, shifts are extremely parallelizable, while
additions via add-with-carry are strictly serial. Furthermore,
the number and order of these ﬁnite ﬁeld operations is not
the same for point double and add. The Spy can potentially
learn this secret sequence of doubles and conditional adds
by measuring its own performance through selective ports,
leading to (secret) scalar disclosure.
Figure 3 lists our proposed PORTSMASH Spy process. The
ﬁrst rdtsc wrapped by lfence establishes the start time.
Then, depending on the architecture and target port(s), the Spy
executes one of several strategies to saturate the port(s). Once
those complete, the second rdtsc establishes the end time.
These two counters are concatenated and stored out to a buffer
at rdi. The Spy then repeats this entire process. Here we
mov $COUNT, %rcx
1:
lfence
rdtsc
lfence
mov %rax, %rsi
#ifdef P1
.rept 48
crc32 %r8, %r8
crc32 %r9, %r9
crc32 %r10, %r10
.endr
#elif defined(P5)
.rept 48
vpermd %ymm0, %ymm1, %ymm0
vpermd %ymm2, %ymm3, %ymm2
vpermd %ymm4, %ymm5, %ymm4
.endr
#elif defined(P0156)
.rept 64
add %r8, %r8
add %r9, %r9
add %r10, %r10
add %r11, %r11
.endr
#else
#error No ports defined
#endif