to store or evict cache lines. This provides strict isolation
between the cache resources of the different enclaves, thus,
effectively blocking cache side-channel leakage, but reduces
the cache resources available for the enclave. Depending on
the enclave service requirements, the partitioning mode can
be conﬁgured by the SM independently for each enclave at
setup and during the enclave lifetime, thus, fulﬁlling FR.5.
Access control. We extend each cache entry metadata with
a 4-bit line-eid register encoding the owner enclave of the
cache line, as shown in in Figure 6. We extend the cache
lookup logic to generate a hit only when both tag as well as
eid match for CP-BASIC, as opposed to usual tag matching.
To support CP-STRICT, the cache ways directory is also
extended with a 1-bit register excl that identiﬁes whether
each way is owned exclusively by an enclave, as well as a
4-bit eid register that identiﬁes the owner enclave. The cache
controller logic is augmented with a register-based lookup
table that is indexed by the eid. It encodes with a single
mode bit whether the corresponding enclave has CP-STRICT
enabled and its allocated cache way indices. In CP-STRICT,
cache hits are only allowed in these cache ways.
Eviction and replacement. The L2 cache we use imple-
ments a pseudo-random replacement policy where any way
is selected pseudo-randomly for eviction. We modify this to
only select a way from the subset of ways allowed for each
enclave. For enclaves with CP-STRICT, only ways exclusively
allocated to it are used. For enclaves with CP-BASIC, all ways
(except ways allocated exclusively to other enclaves) are used.
Per-enclave cache allocation. Unallocated way indices
are maintained in a register vector.
If an enclave with
CP-STRICT enabled requests to exclusively own cache ways,
the required ways are allocated if available and below the
allowed maximum per enclave.
An inherent drawback of this partitioning technique is how
the limited number of cache ways directly constrains the num-
ber of simultaneous enclaves that can have CP-STRICT en-
abled. However, this is only an implementation decision
for our particular prototype, where more sophisticated cache
designs [25, 74, 99] can be integrated into CURE.
7 Security Considerations
To protect from a strong software adversary, our instantiation
of CURE must fulﬁll the security requirements introduced
in Section 4.1.
In the following section, we discuss how
our prototype meets the requirements SR.1, SR.2, and SR.4,
whereas we show the fulﬁllment of SR.3 in Section 8.
7.1 Hardware Security Primitives (SR.2)
The enclave protection is enforced by hardware SPs at the
system bus and L2 cache which are conﬁgured over MMIO.
USENIX Association
30th USENIX Security Symposium    1083
After the system is powered on and on every switch to the ma-
chine level, the CPU jumps to the trap vector whose address
is stored in the mtvec register. The trap vector is included
into the SM such that the boot process and context switches
are overlooked by the SM. The mtvec register is assigned to
the SM by coupling the access permission to the SM enclave
ID (stored in the eid register) which is also assigned to the
SM. The eid register is set by hardware during the context
switch into the machine level. During boot, the SM assigns
the SP MMIO regions exclusively to its own enclave ID.
7.2 Enclave Protection (SR.1)
At rest, the enclave binaries are stored unencrypted in memory.
However, during enclave setup, the SM veriﬁes the binaries
using digital signatures. Moreover, the L1 is ﬂushed during
setup/teardown to remove malicious or sensitive data from
the cache. The communication between enclaves and the OS
is controlled by the SM, so is the delegation of the shared
memory address. Hardware-assisted hyperthreading is dis-
abled during enclave execution. The enclave state, which is
loaded during the setup process, is persistently stored by the
SM using authenticated encryption, either in RAM as part of
the SM state or evicted to ﬂash/disk, and additionally rollback
protected. During teardown, the SM removes all enclave data
from the memory.
The SPs in hardware perform access control on physical
addresses at the system bus. Thus, CURE protects from ad-
versaries in privileged software levels (PL2 - PL0) and from
off-core adversaries, e.g. peripherals performing DMA. The
enclave data cached in the L1 during run time is protected
by ﬂushing it on all context switches. Data in the L2 cache
is protected by assigning cache lines exclusively to enclaves.
Since no enclave (except the SM), has elevated rights on the
system, CURE also protects from malicious enclaves.
7.3 Side-channel Attack Resilience (SR.4)
Cache side-channel attacks. Side-channel attacks which tar-
get data in core-exclusive cache resources, i.e., in the L1 [11],
the BTB [50] or the TLB [31], are prevented by the SM by
ﬂushing the resources on all context switches. Side-channel
attacks targeting data in the shared L2 cache [36, 39, 102] are
prevented through strict way-based cache partitioning.
Controlled side-channel attacks. Side-channel attacks on
user-space enclaves which target page tables [65, 92, 101]
are prevented by including the page tables into the enclave
memory and by mapping all enclave code and data pages
before execution. The SM veriﬁes the page tables and the
base address of the root page table stored in the satp register.
The hardware SPs prevent the page table walker (PTW) from
performing forbidden memory access during the page table
walk. Side-channel attacks exploiting interrupts [91] can be
mitigated using trap handlers (Section 5.2.2).
CURE provides cryptographic primitives in the user-space
enclaves to encrypt and integrity-protect data shared with
the OS. However, using OS services over syscalls always
comprises a remaining risk of leaking meta data informa-
tion [2, 77] or of receiving malicious return values from the
OS [13]. In user-space enclaves, these attacks must be mit-
igated on the application level inside the enclave, e.g., by
using data-oblivious algorithms [2, 68] or by verifying the
return values [13]. None of these attacks pose a threat to
kernel-space enclave since all resources are handled by the
enclave RT. However, on VM enclaves, the second level
page tables need to be protected, as with user-space enclaves.
Interrupt-based attacks can again be mitigated with custom
trap handlers. No additional countermeasures are needed to
protect the SM since the SM does not use a virtual address
space or OS services and handles its own interrupts.
Transient execution attacks. The discovered transient exe-
cution attacks either mistrain the branch predictor [14, 43, 45],
rely on information leakage [89] or malicious injections [90]
on the L1 cache, or rely on resources shared when using
hardware-assisted hyperthreading [12, 78, 90, 93, 94]. By
disabling hyperthreading during enclave execution (or alter-
natively assigning all threads to the enclave) and ﬂushing
core-exclusive caches, CURE protects enclaves against the
known transient execution attacks.
8 Evaluation
In the following section, we systematically evaluate our CURE
prototype. First, we quantify the software and hardware mod-
iﬁcations required to implement CURE. Next, we evaluate
the performance of CURE’s enclaves using microbenchmarks,
and the overall performance overhead of CURE using generic
RISC-V benchmark suites.
8.1 System Modiﬁcations
Component
Linux Kernel
Custom Kernel Module
Security Monitor
SM Crypto-Library
LOC
375 (modiﬁed)
200
544
2586
Table 1: Lines of code required to implement CURE. SM
Crypto-Library refers to the crypto library (part of tomcrypt)
included in the Security Monitor.
Software changes and TCB. Our implementation of CURE
on RISC-V comprises of a slightly modiﬁed Linux LTS kernel
4.19, a custom kernel module, and our software TCB (SM).
In Table 1, the lines of code (LOC) are shown for each of
the components, which indicate that the software changes
required to implement CURE are minimal. Moreover, the
SM only consists of around 3KLOC of code, whereas most
1084    30th USENIX Security Symposium
USENIX Association
(82.62%) of the SM code consists of cryptographic primi-
tives. Because of its minimal size, formal veriﬁcation of the
SM is possible [44], thus, fulﬁlling SR.3. Note that since
CURE isolates the SM in an own sub-space enclave, CURE
can achieve a smaller TCB size than other RISC-V security ar-
chitectures [22, 48, 98] which include all code in the machine
level, i.e., the ﬁrmware code, in the TCB. In our implemen-
tation, the ﬁrmware code consists of 3286 LOCs. Thus, by
isolating the SM in a sub-space enclave, we managed to cut
the software TCB in half, where the actual management code
is even less (15.56%).
Protecting a sensitive service in a user-space enclave re-
quires to add a small custom library (10KB) to the service
binary. For the kernel-space enclaves, management code (the
enclave RT) must be added in addition. In our prototype, we
use the Linux LTS kernel 4.19 as the RT which increases the
size of the service binary by 3MB. Custom RTs can further
decrease this kernel-space enclave overhead. However, kernel-
space enclaves will always have an increased binary size and
memory consumption compared to user-space enclaves.
Hardware overhead. We evaluate the hardware overhead of
our changes by synthesizing the generated Verilog descrip-
tions using Xilinx Vivado tools targeting a Virtex UltraScale
FPGA device. Table 2 shows a breakdown of the individ-
ual area overhead of the different modiﬁcations required to
implement CURE. Overhead is represented in look-up ta-
bles (LUTs), the fundamental programmable logic blocks of
FPGA devices, and registers.
Conﬁguration
Baseline
TileLink extension
Registers
LUTs
Overhead (+%) Overhead (+%)
61, 097
+211 (0.4%)
28, 012
+110 (0.4%)
Access control extensions
Main memory
1 MMIO peripheral
1 DMA device
+5, 276 (8.6%)
+248 (0.4%)
+112 (0.2%)
+1, 055 (3.8%)
+107 (0.4%)
+72 (0.3%)
On-demand cache partitioning
w/ L2 cache (baseline)
w/ L2 cache partitioned +516 (1.7%∗)
+30, 232
+11, 549
+214 (1.8%∗)
Table 2: Hardware overhead breakdown in LUTs and registers.
Baseline setup consists of 2 Rocket cores without L2 cache.
∗Overhead relative to the L2 cache (baseline).
We compare in Table 2 with a baseline conﬁguration of 2
in-order Rocket cores (each with L1 cache). Extending the
TileLink protocol throughout the system bus incurs a minimal
overhead of 105 LUTs per core relative to the baseline (211
LUTs for 2 cores). This overhead includes propagating the
eid in tandem with memory access transactions through the
MMU of every core, and is thus replicated for every additional
core in the system.
In contrast, the rest of our modiﬁcations for performing ac-
cess control at the system bus, including enclave-to-peripheral
Measurement
Setup:
Binary Veriﬁcation
Others
Teardown:
Memory Cleaning
Others
Context switch to OS
Context switch from OS
Dynamic memory allocation
OS communication
Normal
Process
0.741
-
0.741
0.065
-
0.065
0.008
0.078
0.003
-
User-Space
Kernel-Space
Enclave
23.918
21.824
2.094
23.531
9.384
14.147
0.025
0.084
0.020
0.020
Enclave
413.726
218.975
194.750
103.517
50.206
53.311
53.308
194.747
0.005
0.049
Table 3: CURE performance overhead compared to a normal
process on microbenchmarks in milliseconds.
binding, are independent of the number of cores. Incorpo-
rating logic to perform access control for every MMIO pe-
ripheral utilizes an additional 248 LUTs, and 112 LUTs per
DMA device. Each represent below 0.5% overhead relative
to a dual-core baseline SoC. Integrating an L2 cache into our
baseline setup utilizes an additional 30, 232 LUTs. Applying
our on-demand way-based partitioning to this cache costs only
516 LUTs and 214 registers, which is 1.8% overhead relative
to the L2 cache logic utilization itself, and 0.5% relative to the
entire SoC. Our area overhead evaluation results demonstrate
that the hardware modiﬁcations required to achieve our ﬁne-
grained and customized enclave protection in CURE indeed
incur minimal area overhead on both single- and multi-core
architectures, thus fulﬁlling FR.3.
8.2 Performance Evaluation
We evaluate the performance of CURE using our FPGA-based
setup coupled with cycle-accurate simulators. We conduct
our experiments using micro and macro benchmarks for user-
space and kernel-space enclaves, and compare them to un-
modiﬁed user-space processes. We conduct 10 runs for each
of the experiments.
8.2.1 Microbenchmarks