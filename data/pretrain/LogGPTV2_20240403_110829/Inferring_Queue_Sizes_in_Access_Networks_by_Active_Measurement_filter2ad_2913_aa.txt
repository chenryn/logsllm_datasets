# Inferring Queue Sizes in Access Networks by Active Measurement

## Authors
Mark Claypool, Robert E. Kinicki, Mingzhe Li, James Nichols, and Huahui Wu  
Worcester Polytechnic Institute, Worcester, MA 01609, USA  
{claypool, rek, lmz, jnick, flashine}@cs.wpi.edu

## Abstract
Router queues can significantly impact both round-trip times and throughput. However, there is limited public information on the queue provisioning employed by Internet Service Providers (ISPs) for the routers that control access links to home computers. This paper introduces QFind, a black-box measurement technique, as a simple method to approximate the size of the access queue at the last-mile router. We evaluate QFind through simulation, emulation, and real-world measurements. Although precise access queue results are limited by receiver window sizes and other system events, we find distinct differences between DSL and cable access queue sizes.

## 1. Introduction
The current conventional wisdom is that over-provisioning in core network routers has shifted Internet performance bottlenecks to network access points [1]. Since typical broadband access link capacities (hundreds of kilobits per second) are considerably lower than ISP core router capacities (millions of kilobits per second), last-mile access links need queues to accommodate traffic bursts. Given the bursty nature of Internet traffic [7], which is partially due to flows with high round-trip times or large congestion windows, the provider's choice for access link queue size can directly impact a flow's achievable bitrate. A small queue can keep achieved bitrates significantly below the available capacity, while a large queue can negatively impact a flow's end-to-end delay. Interactive applications, such as IP telephony and some network games, with strict delay bounds in the range of hundreds of milliseconds, experience degraded Quality of Service (QoS) when large access queues become saturated with concurrent flows.

Despite the importance of queue size to achievable throughput and added delay, there is little documentation on queue size settings in practice. Guidelines for determining the "best" queue sizes have often been debated on the e2e mailing list, an active forum for network-related discussions by researchers and practitioners. While the general consensus suggests that the access queue size ranges from one to four times the capacity-delay product of the link, measured round-trip times vary by at least two orders of magnitude (10 ms to 1 second) [6]. This research consensus provides little practical guidance for network practitioners to select the best size for the access queue link. Moreover, a lack of proper queue size information affects network simulations, the most common form of evaluation in the network research community, where access queue sizes are often chosen without confidence that they accurately reflect current practices.

A primary goal of this investigation is to experimentally estimate the queue size of numerous access links, for both cable modem and DSL connections managed by a variety of ISPs. Network researchers should find these results useful in designing simulations that more accurately depict current practices.

## 2. QFind Methodology
Based on related work and pilot studies, the following assumptions are made in this study: each access link has a relatively small queue size (between 10 and 100 packets); the maximum queue length is independent of the access link capacity or other specific link characteristics; and the queue size is constant and independent of the incoming traffic load, with no attempt made by the router to increase the queue sizes under heavier loads or when flows with large round-trip times are detected. Below is our proposed QFind methodology for inferring the access network queue size from an end-host:

1. **Locate a Close Internet Host**: Identify an Internet host that is slightly upstream of the access link but still close to the end-host. For the test results discussed in this paper, the DNS name server provided by the ISP is used since DNS servers are typically close in terms of round-trip time and easy to find by inexperienced end-users.
   
2. **Baseline Latency Measurement**: Start a ping from the end-host to the close Internet host and let it run for up to a minute. The minimum value returned during this time is the baseline latency, typically without any queuing delays since there is no competing traffic causing congestion. This ping process continues to run until the end of the experiment.
   
3. **Download a Large File**: Download a large file from a remote server to the end-host. For the test results in this paper, a 5 MB file was used since it typically provided adequate time for TCP to reach congestion avoidance and saturate the access queue downlink capacity.
   
4. **Record Maximum Round-Trip Time**: Stop the ping process. Record the minimum and maximum round-trip times as reported by ping and the total time to download the large file. The maximum ping value recorded during the download typically represents the baseline latency plus the access link queuing delay.

The queue size of the access link can be inferred using the data obtained above. Let \( D_t \) be the total delay (the maximum delay seen by ping):

\[ D_t = D_l + D_q \]

where \( D_l \) is the latency (the minimum delay seen by ping) and \( D_q \) is the queuing delay. Therefore:

\[ D_q = D_t - D_l \]

Given throughput \( T \) (measured during the download), the access link queue size in bytes, \( q_b \), can be computed by:

\[ q_b = D_q \times T \]

For a packet size \( s \) (say 1500 bytes, a typical MTU), the queue size in packets, \( q_p \), becomes:

\[ q_p = \frac{(D_t - D_l) \times T}{s} \]

The strength of the QFind methodology lies in its simplicity. Unlike other approaches [1,8,10], QFind does not require custom end-host software, making it easier to convince volunteers to participate in an Internet study. Moreover, the simple methodology makes the results reproducible from user to user and in both simulation and emulation environments.

### 2.1 Possible Sources of Error
- **Congestion on Other Queues**: The maximum ping time recorded may be due to congestion on a queue other than the access queue. However, this is unlikely since the typical path from the end-host to the DNS name server is short. Pilot tests [3] suggest any congestion from the home node to the DNS name server typically causes less than 40 ms of added latency. Repeating steps 2-4 multiple times can help discard apparent outliers, reducing the possibility of over-reporting queue sizes.
- **Underestimation of Queue Size**: The queue size computed in Equation 4 may underestimate the actual queue size if ping packets always arrive at a nearly empty queue. However, if the file download is long enough, it is unlikely that every ping packet will be so lucky. Results in Section 3 suggest that a 5 MB file is sufficient to fill queues over a range of queue sizes.
- **Underutilization of Access Link**: If there is underutilization on the access link, the access queue will not build up, and QFind may under-report the queue size. This can happen if there are sources of congestion at the home node network before ping packets even reach the ISP. Home users with wireless networks may experience contention on the wireless medium between ping and download packets. Pilot tests [3] suggest that congestion on a wireless network during QFind tests adds at most 30 ms to any recorded ping times. We ask QFind volunteers to indicate wireless/wired settings when reporting QFind results.
- **TCP Receiver Window Limitations**: If the TCP download is limited by the receiver advertised window instead of by the network congestion window, then the queue sizes reported may be the limit imposed by TCP and not the access link queue. Recent versions of Microsoft Windows and Linux support TCP window scaling (RFC 1323), allowing the receiver advertised window to grow up to 1 Gbyte. Even if window scaling is not used, the lack of ping packet losses during the download would suggest that the access queue was not saturated, and the queue size could actually be greater than reported.
- **Ping Packet Processing**: Some router interfaces may process ping packets differently than other data packets. However, empirical measurements in [2] suggest that ping packets usually provide round-trip time measurements that are effectively the same as those obtained by TCP.

## 3. Experiments
To determine whether the QFind methodology could effectively predict access link queue sizes in real last-mile Internet connections, we evaluated the QFind approach first with simulations using NS2 (see Section 3.1) and then with emulations using NIST Net (see Section 3.2). After reviewing these proof-of-concept results, we enlisted many volunteers from the WPI community to run QFind experiments over a variety of DSL and cable modem configurations from home (see Section 3.3).

### 3.1 Simulation
QFind was simulated with the configuration depicted in Figure 1, consisting of a home node, an ISP last-mile access router, a TCP download server, and a DNS name server. The simulated link latencies used in the emulations were based on prototype QFind measurements.

The delays built into the testbed emulations were 5 ms from home to router, 5 ms from router to DNS, and 20 ms from router to download server. Link capacities were set to reflect typical asymmetric broadband data rates [8], with the router-to-home downstream link capacity set at 768 Kbps, the home-to-router upstream link capacity set at 192 Kbps, and the link capacities in both directions between the router and both upstream servers set at 10 Mbps. 1500-byte packets were used to model the typical Ethernet frame size found in home LANs, and TCP receiver windows were set to 150 packets.

Figure 2 displays the cumulative density functions for 100 simulations of the QFind methodology (steps 2 to 4 in Section 2) with downstream access link queues of 10, 50, and 100 packets, respectively. QFind predicts the access queue size remarkably well in this simulated environment. Of the 100 runs at each queue size, the most the predicted queue size was smaller than the actual queue size was 1 packet for the 10-packet queue, 1.5 packets for the 50-packet queue, and 2.5 packets for the 100-packet queue. The median predicted queue size was less than the actual queue size by about 1 packet in all cases.

### 3.2 Emulation
To further investigate QFind feasibility, we set up a testbed to emulate a last-mile access router in a controlled LAN environment. Two computers were used as home nodes, with one running Windows 2000 and the other running Linux, to test the impact of the operating system type on QFind. The download server ran on Windows Server 2003, while the DNS name server ran on Linux. A NIST Net PC router emulated the ISP’s Internet connection with link capacities set to reflect typical broadband asymmetry, with the downstream router-to-home link capacity set to 768 Kbps, the upstream home-to-router link set to 192 Kbps, and the router link capacities to and from both servers using 10 Mbps LAN connections. The home-to-server round-trip delay was 20 ms for both the download server and the DNS server, as the NIST Net implementation does not allow two host pairs to have different induced delays while sharing a router queue.

Using this testbed, the QFind methodology was emulated (steps 2 to 4 in Section 2) with home nodes running Windows 2000 with a TCP receiver window size of 16 Kbytes, Windows 2000 with a TCP receiver window size set to 64 Kbytes, and Linux with a TCP receiver window size set to 64 Kbytes. Three QFind emulations were run for each of the queue sizes of 10, 30, 50, and 100 packets, with a packet size of 1500 bytes.

Figure 3 presents the median of the inferred queue sizes. The inferred queue sizes labeled "thrput" are computed using the measured download capacity. The inferred queue sizes labeled "capacity" are computed using the capacity of the link. In cases where the NIST Net queue size is smaller than the TCP receiver window size, QFind is able to infer the queue size closely, even for different operating systems. The queue sizes computed using link capacity are more accurate than those computed using download throughput. However, while the link capacity was known in our testbed, it is generally not known by end-host operating systems nor by most home users who participated in our study.

Intermediate results that can be drawn from these emulations include: the QFind emulation estimates of queue size are not as accurate as the simulation estimates; using the maximum link capacity provides a better estimate of the access queue size than using the measured download data rate; ping outliers in the testbed did not cause over-prediction of the queue length; small TCP receiver windows result in significant underestimation of the access queue size, as the ability of the download to fill the access queue is restricted by a small maximum TCP receiver window size setting.

### 3.3 Measurement
The final stage of this investigation involved creating an easy-to-follow set of instructions for volunteers to execute three QFind experiments and record results that could be easily emailed to a centralized repository. One of the key elements of the QFind concept was to develop a measurement procedure that could be run by a variety of volunteers using different cable and DSL providers on home computers with different speeds and operating systems. To maximize participation, the intent was to avoid having users download and run custom programs and avoid any changes to system configuration settings (such as packet size or receiver window). The final set of instructions can be found at: http://www.cs.wpi.edu/~claypool/qfind-instructions.html.

---

This version of the text is more structured, coherent, and professional, with improved clarity and readability.