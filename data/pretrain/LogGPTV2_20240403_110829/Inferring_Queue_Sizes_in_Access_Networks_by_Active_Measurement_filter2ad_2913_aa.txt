title:Inferring Queue Sizes in Access Networks by Active Measurement
author:Mark Claypool and
Robert E. Kinicki and
Mingzhe Li and
James Nichols and
Huahui Wu
Inferring Queue Sizes in Access Networks by
Active Measurement
Mark Claypool, Robert Kinicki, Mingzhe Li, James Nichols, and Huahui Wu
Worcester, MA, 01609, USA {claypool,rek,lmz,jnick,flashine}@cs.wpi.edu
CS Department at Worcester Polytechnic Institute
Abstract. Router queues can impact both round-trip times and throug-
hput. Yet little is publicly known about queue provisioning employed by
Internet services providers for the routers that control the access links to
home computers. This paper proposes QFind, a black-box measurement
technique, as a simple method to approximate the size of the access queue
at last mile router. We evaluate QFind through simulation, emulation,
and measurement. Although precise access queue results are limited by
receiver window sizes and other system events, we ﬁnd there are distinct
diﬀerence between DSL and cable access queue sizes.
1 Introduction
The current conventional wisdom is that over-provisioning in core network rou-
ters has moved Internet performance bottlenecks to network access points [1].
Since typical broadband access link capacities (hundreds of kilobytes per se-
cond) are considerably lower than Internet Service Provider (ISP) core router
capacities (millions of kilobytes per second), last-mile access links need queues to
accommodate traﬃc bursts. Given the bursty nature of Internet traﬃc [7] that
is partially due to ﬂows with high round-trip times or large congestion windows,
it is clear that the provider’s choice for access link queue size may have a direct
impact on a ﬂow’s achievable bitrate. A small queue can keep achieved bitra-
tes signiﬁcantly below the available capacity, while a large queue can negatively
impact a ﬂow’s end-to-end delay. Interactive applications, such as IP telephony
and some network games, with strict delay bounds in the range of hundreds of
milliseconds experience degraded Quality of Service when large access queues
become saturated with other, concurrent ﬂows.
Despite the importance of queue size to achievable throughput and added
delay, there is little documentation on queue size settings in practice. Guideli-
nes for determining the “best” queue sizes have often been debated on the e2e
mailing list,1 an active forum for network related discussion by researchers and
practitioners alike. While general consensus has the access queue size ranging
1 In particular, see the e2e list archives at: ftp://ftp.isi.edu/end2end/end2end-interest-
1998.mail and http://www.postel.org/pipermail/end2end-interest/2003-January/-
002702.html.
C. Barakat and I. Pratt (Eds.): PAM 2004, LNCS 3015, pp. 227–236, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004
228
M. Claypool et al.
from one to four times the capacity-delay product of the link, measured round-
trip times vary by at least two orders of magnitude (10 ms to 1 second) [6].
Thus, this research consensus provides little help for network practitioners to
select the best size for the access queue link. Moreover, a lack of proper queue
size information has ramiﬁcations for network simulations, the most common
form of evaluation in the network research community, where access queue sizes
are often chosen with no conﬁdence that these queue choices accurately reﬂect
current practices.
A primary goal of this investigation is to experimentally estimate the queue
size of numerous access links, for both cable modem and DSL connections ma-
naged by a variety of ISPs. Network researchers should ﬁnd these results useful
in designing simulations that more accurately depict current practices.
2 QFind
Based on related work and pilot studies, the following assumptions are made
in this study: each access link has a relatively small queue size - between 10
and 100 packets; the maximum queue length is independent of the access link
capacity or other speciﬁc link characteristics; and the queue size is constant and
independent of the incoming traﬃc load with no attempt made by the router to
increase the queue sizes under heavier loads or when ﬂows with large round-trip
times are detected. Below is our proposed QFind methodology for inferring the
access network queue size from an end-host:
1. Locate an Internet host that is slightly upstream of the access link while still
being “close” to the end-host. For the test results discussed in this paper,
the DNS name server provided by the ISP is used since DNS servers are
typically close in terms of round-trip time and easy to ﬁnd by inexperienced
end-users.
2. Start a ping from the end-host to the close Internet host and let it run for up
to a minute. The minimum value returned during this time is the baseline
latency typically without any queuing delays since there is no competing
traﬃc causing congestion. This ping process continues to run until the end
of the experiment.
3. Download a large ﬁle from a remote server to the end-host. For the test
results in this paper, a 5 MByte ﬁle was used since it typically provided
adequate time for TCP to reach congestion avoidance and saturate the access
queue downlink capacity.
4. Stop the ping process. Record the minimum and maximum round-trip times
as reported by ping and the total time to download the large ﬁle. The
maximum ping value recorded during the download typically represents the
baseline latency plus the access link queuing delay.
The queue size of the access link can be inferred using the data obtained
above. Let Dt be the total delay (the maximum delay seen by ping):
Inferring Queue Sizes in Access Networks by Active Measurement
229
Dt = Dl + Dq
(1)
where Dl is the latency (the minimum delay seen by ping) and Dq is the queuing
delay. Therefore:
Dq = Dt − Dl
(2)
Given throughput T (measured during the download), the access link queue size
in bytes, qb, can be computed by:
qb = Dq × T
(3)
For a packet size s (say 1500 bytes, a typical MTU), the queue size in packets,
qp, becomes:
(Dt − Dl) × T
s
qp =
(4)
The strength of the QFind methodology lies in its simplicity. Unlike other
approaches [1,8,10], QFind does not require custom end-host software, making
it easier to convince volunteers to participate in an Internet study. Moreover,
the simple methodology makes the results reproducible from user to user and in
both simulation and emulation environments.
2.1 Possible Sources of Error
The maximum ping time recorded may be due to congestion on a queue other
than the access queue. However, this is unlikely since the typical path from the
end-host to the DNS name server is short. Pilot tests [3] suggest any congestion
from the home node to the DNS name server typically causes less than 40 ms
of added latency. Moreover, by having users repeat steps 2-4 of the QFind me-
thodology multiple times (steps 2-4 take only a couple of minutes), apparent
outliers can be discarded. This reduces the possibility of over-reporting queue
sizes.
The queue size computed in Equation 4 may underestimate the actual queue
size since it may happen that the ping packets always arrive to a nearly empty
queue. However, if the ﬁle download is long enough, it is unlikely that every
ping packet will be so lucky. Results in Section 3 suggest that the 5 MB ﬁle is
of suﬃcient length to ﬁll queues over a range of queue sizes.
If there is underutilization on the access link then the access queue will
not build up and QFind may under-report the queue size. This can happen if
there are sources of congestion at the home node network before ping packets
even reach the ISP. Most notably, home users with wireless networks may have
contention on the wireless medium between the ping and download packets. Pilot
tests [3] suggest that congestion on a wireless network during QFind tests adds at
most 30 ms to any recorded ping times. As 30 ms may be signiﬁcant in computing
230
M. Claypool et al.
an access queue size, we ask QFind volunteers to indicate wireless/wired settings
when reporting QFind results.
If the TCP download is limited by the receiver advertised window instead
of by the network congestion window, then the queue sizes reported may be
the limit imposed by TCP and not be the access link queue. However, recent
versions of Microsoft Windows as well as Linux support TCP window scaling
(RFC 1323), allowing the receiver advertised window to grow up to 1 Gbyte.
Even if window scaling is not used, it is still possible to detect when the receiver
advertised window might limit the reported queue. The lack of ping packet losses
during the download would suggest that the access queue was not saturated and
the queue size could actually be greater than reported.
For actual TCP receiver window settings, Windows 98 has a default of 8192
bytes, Windows 2000 has a default of 17520 bytes, Linux has a default of 65535
bytes, and Windows XP may have a window size of 17520, but it also has a
mostly undocumented ability to scale the receiver window size dynamically.
Additionally, some router interfaces may process ping packets diﬀerently
than other data packets. However, in practice, hundreds of empirical measure-
ments in [2] suggest ping packets usually provide round-trip time measurements
that are eﬀectively the same as those obtained by TCP.
Fig. 1. Topology
3 Experiments
To determine whether the QFind methodology could eﬀectively predict access
link queue sizes in real last-mile Internet connections, we evaluated the QFind
approach ﬁrst with simulations using NS2 (see Section 3.1) and then emulati-
ons using NIST Net3 (see Section 3.2). After reviewing these proof-of-concept
results, we enlisted many volunteers from the WPI community to run QFind
experiments over a variety of DSL and cable modem conﬁgurations from home
(see Section 3.3).
2 http://www.isi.edu/nsnam/ns/
3 http://snad.ncsl.nist.gov/itg/nistnet/
Inferring Queue Sizes in Access Networks by Active Measurement
231
3.1 Simulation
QFind was simulated with the conﬁguration depicted in Figure 1 consisting of a
home node, an ISP last-mile access router, a TCP download server and a DNS
name server. The simulated link latencies used in the emulations were based on
prototype QFind measurements.
The delays built into the testbed emulations were 5 ms from home to router,
5 ms from router to DNS, and 20 ms from router to download server. Link
capacities were set to reﬂect typical asymmetric broadband data rates [8], with
the router-to-home downstream link capacity set at 768 Kbps, the home-to-
router upstream link capacity set at 192 Kbps, and the link capacities in both
directions between router and both upstream servers set at 10 Mbps. 1500 byte
packets were used to model the typical Ethernet frame size found in home LANs
and TCP receiver windows were set to 150 packets.
1
0.8
0.6
0.4
0.2
y
t
i
s
n
e
D
e
v
i
t
l
a
u
m
u
C
10
50
100
)
s
t
e
k
c
a
P
i
(
e
z
S
e
u
e
u
Q
d
e
r
r
e
f
n
I
0
20
40
60
80
100
Inferred Queue Size (packets)
Fig. 2. Cumulative Density Functions
of
Inferred Queue Sizes for Actual
Queue Sizes of 10, 50 and 100 Packets
using NS Simulator
40
ideal
30
20
10
0
rwin=64k, capacity
rwin=64k, thrput
linux, rwin=64k, thrput
rwin=16k, capacity
rwin=16k, thrput
0
10 20 30 40 50 60 70 80 90 100 110
Actual Queue Size (Packets)
Fig. 3. Median of Inferred Queue Sizes
versus Actual Queue Sizes using NIST
Net Emulator
Figure 2 displays the cumulative density functions for 100 simulations of the
QFind methodology (steps 2 to 4 in Section 2) with downstream access link
queues of 10, 50 and 100 packets respectively. QFind predicts the access queue
size remarkably well in this simulated environment. Of the 100 runs at each
queue size, the most the predicted queue size was smaller than the actual queue
size was 1 packet for the 10 packet queue, 1.5 packets for the 50 packet queue
and 2.5 packets for the 100 packet queue. The median predicted queue size was
less than the actual queue size by about 1 packet in all cases.
3.2 Emulation
To further investigate QFind feasibility, we setup a testbed to emulate a last-
mile access router in a controlled LAN environment. Two computers were used
as home nodes with one computer running Windows 2000 and the other running
232
M. Claypool et al.
Linux in order to test the impact of the operating system type on QFind. The
download server ran on Windows Server 2003 while the DNS name server ran
on Linux. A NIST Net PC router emulated the ISP’s Internet connection with
link capacities set to reﬂect typical broadband asymmetry with the downstream
router-to-home link capacity set to 768 Kbps, the upstream home-to-router link
set to 192 Kbps, and the router link capacities to and from both servers using 10
Mbps LAN connections. The home-to-server round-trip delay was 20 ms for both
the download server and the DNS server since the NIST Net implementation does
not allow two host pairs to have diﬀerent induced delays while sharing a router
queue.
Using this testbed, the QFind methodology was emulated (steps 2 to 4 in
Section 2) with home nodes running Windows 2000 with a TCP receiver window
size of 16 Kbytes, Windows 2000 with a TCP receiver window sizes set to 64
Kbytes, and Linux with a TCP receiver window sizes set to 64 Kbytes. Three
QFind emulations were run for each of the queue sizes of 10, 30, 50 and 100
packets, with a packet size of 1500 bytes.
Figure 3 presents the median of the inferred queue sizes. The inferred queue
sizes labeled “thrput” are computed using the measured download capacity. The
inferred queue sizes labeled “capacity” are computed using the capacity of the
link. In those cases where the NIST Net queue size is smaller than the TCP
receiver window size, QFind is able to infer the queue size closely, even for
diﬀerent operating systems. The queue sizes computed using link capacity are
more accurate than those computed using download throughput. However, while
the link capacity was, of course, known by us for our testbed, it is not, in general,
known by an end-host operating systems nor by most of the home users who
participated in our study.
Intermediate results that can be drawn from these emulations even before
evaluating actual QFind measurements include: the QFind emulation estimates
of queue size are not as accurate as the simulation estimates; using the maximum
link capacity provides a better estimate of the access queue size than using the
measured download data rate; ping outliers in the testbed did not cause over
prediction of the queue length; small TCP receiver windows result in signiﬁcant
underestimation of the access queue size since the ability of the download to ﬁll
the access queue is restricted by a small maximum TCP receiver window size
setting.
3.3 Measurement
The ﬁnal stage of this investigation involved putting together an easy-to-follow
set of directions to be used by volunteers to execute three QFind experiments
and record results such they could be easily emailed to a centralized repository.
One of the key elements of the whole QFind concept was to develop a mea-
surement procedure that could be run by a variety of volunteers using diﬀerent
cable and DSL providers on home computers with diﬀerent speeds and operating
systems. To maximize participation, the intent was to avoid having users down-
load and run custom programs and avoid any changes to system conﬁguration
Inferring Queue Sizes in Access Networks by Active Measurement
233
settings (such as packet size or receiver window). The ﬁnal set of instructions ar-
rived upon can be found at found at: http://www.cs.wpi.edu/˜claypool/qﬁnd/-
instructions.html.