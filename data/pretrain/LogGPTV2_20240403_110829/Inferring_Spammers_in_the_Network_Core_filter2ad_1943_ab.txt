### Superposition and Visibility of Flow Classes

The superposition of the three flow classes—rejected, failed, and accepted—is weighted by their relative probabilities of occurrence. Despite the total cumulative distribution function (CDF) being dominated by rejected flows (approximately 80% of all flows), all three classes are clearly discernible.

### Determining Optimal Thresholds

To differentiate between the three flow classes, we determined two optimal threshold sizes. For this, we constructed receiver operating characteristic (ROC) curves, which plot the true positive rate against the false positive rate for a range of thresholds. Figure 4 illustrates the ROC curves for differentiating between rejected and failed flows, as well as accepted and rejected flows. The selected thresholds, 332 Bytes (for rejected vs. failed) and 1559 Bytes (for accepted vs. rejected), were chosen because they are closest to the top-left corner of the ROC curve, indicating the best detection quality.

### Evaluation of False Positive Rates

We evaluated the false positive rates of these threshold detectors using data from another week (week 2). The results, presented in Table 1, show that the false detection rate is below 4.5% for all classes, which is sufficiently accurate for the applications discussed in Section 4.2. Additionally, we analyzed other flow properties, such as packets per flow and average bytes per packet, which also proved effective in distinguishing between the three classes [18].

### Flow Labeling and Precision

The flow labels assigned by our system should be treated as "soft" labels. Further accuracy could be achieved by applying clustering algorithms to additional flow fields.

### Impact of Packet Sampling

It is important to note that packet sampling can affect our approach. Over 90% of the rejected SMTP sessions consist of 10 or fewer packets, and more than 90% of accepted sessions have fewer than 40 packets. With a sampling rate of 1:100 or even 1:1000, most resulting flows would consist of only 1 or 2 packets, weakening the usefulness of the bytes and packets per flow metrics. However, our analysis suggests that it may still be possible to distinguish between rejected and accepted flows using the average bytes per packet metric [18]. Adaptive sampling techniques currently under development [19] may address this issue. We plan to further investigate the impact of sampling in future work.

### Network-Wide Characteristics

Classifying flows based on their size allows for passive monitoring of pre-filtering activity in large networks without relying on server logs. To validate these characteristics at a network-wide scale, we examined the traffic from the 50 most active mail servers in our network (Figure 5). The shape of the black- and whitelisted curves reflects the characteristics of rejected and accepted flows, confirming that the majority of traffic from blacklisted hosts is rejected during pre-filtering. Individual server performance differences are discussed in detail in Section 4.1.

### Network-Wide Pre-Filtering Statistics

Generating network-wide pre-filtering statistics (Figure 6) enables ISPs to estimate and track the dynamics of incoming spam. This allows ISPs to investigate the root causes of anomalies in rejected or accepted traffic, which may include global spam campaigns, targeted spamming attacks, or server misconfiguration and performance issues.

### Applications

#### 4.1 Email Server Behavior

Proper configuration and maintenance of mail servers are time-consuming tasks. Our method can help operators create a performance map of mail servers in the network, allowing for regular checks and proactive addressing of potential configuration problems, such as open relay servers.

To compare the pre-filtering performance of internal servers, we define the acceptance ratio as the number of accepted SMTP flows divided by the total number of SMTP flows seen by the server. A high acceptance ratio (e.g., 0.9) indicates that 90% of incoming SMTP sessions are accepted, while a low ratio indicates that most connections are rejected during the TCP handshake or SMTP envelope phase.

The acceptance ratio is influenced by two factors: (i) the traffic mix of ham and spam, and (ii) the server's pre-filtering policy. Using the XBL blacklist from Spamhaus, we estimated the spam/ham mix ratio for each server. Our analysis shows that spam (flows from blacklisted sources) is evenly distributed among servers, with 81% of servers having a spam load between 70% and 90%, consistent with [1]. This implies that significant differences in acceptance ratios cannot be attributed to different traffic mixes.

Regarding server policies, if all servers used a blacklist, the acceptance ratio of most internal servers should be between 0.1 and 0.3, with differences due to traffic mix and the sophistication or aggressiveness of pre-filtering policies (e.g., greylisting). However, the acceptance ratios of the top 200 servers in week 3 of our dataset range from 0.003 to 0.93, with a mean of 0.33 (Figure 7). 35% of the servers have an acceptance ratio > 0.30, suggesting that they accept a lot of traffic from spam sources. This could indicate suboptimal configuration, lack of pre-filtering measures, or intentional policies (e.g., content-based filtering, honeypots).

To verify these assumptions, we sent emails to all servers from two different IP addresses: one blacklisted by Spamhaus and Spamcop, and one ordinary, non-blacklisted address. The servers' reactions clarified whether they used greylisting and/or blacklisting. Servers classified as 'unknown' had inconclusive reactions. The high concentration of black- and greylisting servers below the average ratio confirms that these servers implement basic pre-filtering techniques, while those above the average do not. High-volume servers tend to have lower acceptance ratios, indicating the need for aggressive pre-filtering to manage the influx of incoming emails.

We manually checked high-acceptance servers and found no honeypots deliberately attracting and collecting spam. It is important to note that these manual investigations are for validation purposes and are not part of the proposed system.