Ci(w), where Ci(w) = 1
The solution for this optimization problem can be computed
by solving the linear system (XT ×X)×w = XT ×Y, where X
is a n×d matrix representing all the input data, and Y is a n×1
matrix for the output labels. However, the complexity of the
matrix multiplication XT × X is O(nd2) and the complexity of
solving the linear system is O(d3). Due to its high complexity,
it is rarely used in practice except for small values of n and d.
b) Stochastic gradient descent (SGD): SGD is an effective
approximation algorithm for approaching a local minimum
of a function, step by step. As the optimization function
for the linear regression described above is convex, SGD
provably converges to the global minimum and is typically
very fast in practice. In addition, SGD can be generalized
to work for logistic regression and neural network training,
where no closed-form solution exists for the corresponding
optimization problems. As a result, SGD is the most commonly
used approach to train such models in practice and the main
focus of this work.
The SGD algorithm works as follows: w is initialized as a
vector of random values or all 0s. In each iteration, a sample
(xi, yi) is selected randomly and a coefﬁcient wj is updated as
wj := wj − α
∂Ci(w)
∂wj
.
(1)
where α is a learning rate deﬁning the magnitude to move
towards the minimum in each iteration. Substituting the cost
function of linear regression, the formula becomes wj :=
wj − α(xi · w − yi)xij. The phase to calculate the predicted
i = xi · w is called forward propagation, and the
∗
output y
i − yi)xij is called backward
∗
phase to calculate the change α(y
propagation.
Mini-batch. In practice, instead of selecting one sample of data
per iteration, a small batch of samples are selected randomly
and w is updated by averaging the partial derivatives of all
samples on the current w. We denote the set of indices selected
in a mini-batch by B. This is called a mini-batch SGD and |B|
2Usually a bias b is introduced such that g(xi) = xi · w + b. However, this
can be easily achieved by appending a dummy feature equal to 1 for each xi.
To simplify the notation, we assume b is already embedded in w in this paper.
3In ridge regression, a penalty term λ||w||2 is added to the cost function to
avoid overﬁtting where λ is the regularization parameter. This is supported in
an obvious way by the protocols in this paper, and is omitted for simplicity.
21
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
1
0.5
0
)
u
(
f
−10 −5
5
10
0
u
(cid:76)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)
(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)
f
(cid:75)(cid:76)(cid:71)(cid:71)(cid:72)(cid:81)
(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:20)
f
(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)
f
(cid:856)(cid:856)(cid:856)
f
f
(cid:856)(cid:856)(cid:856)
f
x1
x2
xd
(cid:75)(cid:76)(cid:71)(cid:71)(cid:72)(cid:81)
(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)m-(cid:20)
(cid:82)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)
(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)
f
f
(cid:856)(cid:856)(cid:856)
f
f
f
f
y1
y2
y3
Fig. 1: (a) Logistic function. (b) An example of neural network.
denotes the mini-batch size, usually ranging from 2 to 200. The
beneﬁt of mini-batch is that vectorization libraries can be used
to speed up the computation such that the computation time
for one mini-batch is much faster than running |B| iterations
without mini-batch. Besides, with mini-batch, w converges
smoother and faster to the minimum. With mini-batch, the
update function can be expressed in a vectorized form:
B × (XB × w − YB).
w := w − 1|B| αXT
(2)
XB and YB are B × d and B × 1 submatrices of X and Y
selected using indices in B, representing |B| samples of data
and labels in an iteration. Here w is viewed as a column vector.
Learning rate adjustment. If the learning rate α is too large,
the result of SGD may diverge from the minimum. Therefore,
a testing dataset is used to test the accuracy of the current w.
The inner product of w and each data sample in the testing
dataset is calculated as the prediction, and is compared to the
corresponding label. The accuracy is the percentage of the
correct predictions on the testing dataset. If the accuracy is
decreasing, the learning rate is reduced and the training starts
over with the new learning rate. To balance the overhead spent
on testing, the common practice is to shufﬂe all the training
samples and select the mini-batch in each iteration sequentially,
until all the samples are used once. This is referred to as one
epoch. After one epoch, the accuracy of the current w is tested.
At this point, if the accuracy decreases, the learning rate is
reduced by half and the training starts over; otherwise the data
is reshufﬂed and the next epoch of training is executed.
Termination. When the difference in accuracy compared to
the previous epoch is below a small threshold, w is viewed as
having converged to the minimum and the algorithm terminates.
We denote the number of epochs to train a model as E and
denote the total number of iterations as t. Note that we have
the following relationship: n · E = |B| · t.
c) Logistic Regression: In classiﬁcation problems with
two classes, the output label y is binary. E.g., given some
medical features, we are interested to predict whether the
patient is healthy or sick. In this case, it is better to bound
the output of the prediction between 0 and 1. Therefore, an
activation function f is applied on top of the inner product and
the relationship is expressed as: g(xi) = f (xi · w). In logistic
regression, the activation function is deﬁned as the logistic
function f (u) = 1
1+e−u . As shown in Figure 1(a), the two tails
of the logistic function converge to 0 and 1.
With this activation function, the original cost function for
linear regression is no longer convex, thus applying SGD
may give a local minimum instead of the global minimum.
Therefore, the cost function is changed to the cross entropy
(cid:2)
function Ci(w) = −yi log y
Ci(w), where y
i = f (xi · w).
∗
1
n
i −(1−yi) log(1−y
∗
∗
i ) and C(w) =
The mini-batch SGD algorithm for logistic regression updates
the coefﬁcients in each iteration as follows:
w := w − 1|B| αXT
B × (f (XB × w) − YB).
(3)
Notice that the backward propagation of logistic regression has
exactly the same form as linear regression, yet it is derived using
a different activation and cost function. The only difference in
the SGD for logistic regression is to apply an extra logistic
function on the inner product in the forward propagation.
d) Neural Networks.: Neural networks are a generaliza-
tion of regression to learn more complicated relationships
between high dimensional input and output data. It is exten-
sively used in a wide range of areas such as image processing,
voice and text recognition, often leading to breakthroughs in
each area. Figure 1(b) shows an example of a neural network
with m − 1 hidden layers. Each node in the hidden layer and
the output layer is an instance of regression and is associated
with an activation function and a coefﬁcient vector. Nodes are
also called neurons. Popular activation functions include the
logistic and the RELU function (f (u) = max(0, u)).
For classiﬁcation problems with multiple classes, usually a
softmax function f (ui) = e−ui
is applied at the output
(cid:2)dm
layer, where dm denotes the total number of neurons in the
output layer. The insight is that the output after the softmax
function is always a probability distribution: each output is
between 0 and 1 and all the outputs sum up to 1.
i=1 e−ui
To train a neural network using SGD, Equation 1 is applied
in every iteration to update all coefﬁcients of all neurons where
each neuron is treated similar to a regression. In particular,
let di be the number of neurons in layer i and d0 = d be the
number of features in the input data. dm is the dimension of
the output. We denote the coefﬁcient matrix of the ith layer
as a di−1 × di matrix Wi, and the values as a |B| × di matrix
Xi. X0 is initialized as XB. In the forward propagation for
each iteration, the matrix Xi of the ith layer is computed as
Xi = f (Xi−1 × Wi). In the backward propagation, given a
cost function such as the cross entropy function, the update
function for each coefﬁcient in each neuron can be expressed in
a closed form. To calculated it, we compute the vectors Yi =
iteratively, where Ui = Xi−1 × Wi. Ym is initialized
∂C(W)
(cid:3) ∂f (Um)
to ∂C
∂Um is simply the derivative of
the activation function, and (cid:3) is the element-wise product.
∂Xm
By the chain rule, Yi = (Yi+1 × WT
. Finally, the
coefﬁcients are updated by letting Wi := Wi − α|B| · Xi × Yi.
∂Um , where ∂f (Um)
i ) (cid:3) ∂f (Ui)
∂Ui
∂Ui
B. Secure Computation
Oblivious Transfer. Oblivious transfer (OT) is a fundamental
cryptographic primitive that is commonly used as building block
in MPC. In an oblivious transfer protocol, a sender S has two
inputs x0 and x1, and a receiver R has a selection bit b and
wants to obtain xb without learning anything else or revealing
b to S. Figure 2 describes the ideal functionality realized by
such a protocol. We use the notation (⊥; xb) ← OT(x0, x1; b)
to denote a protocol realizing this functionality.
22
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
Parameters: Sender S and Receiver R.
Main:
(SEN D, sid, x0, x1)
R.
from S,
input
On
return
(SELECT, sid, b)
from R and
to
(RECV, sid, xb)
Fig. 2: Fot Ideal Functionality
We use OTs both as part of our ofﬂine protocol for
generating multiplication triplets and in the online phase for
logistic regression and neural network training in order to
securely compute the activation functions. One-round OT can
be implemented using the protocol of [39], but it requires
public-key operations by both parties. OT extension [27], [11]
minimizes this cost by allowing the sender and receiver to
perform m OTs at the cost of λ base OTs (with public-key
operations) and O(m) fast symmetric-key ones, where λ is the
security parameter. Our implementations take advantage of OT
extension for better efﬁciency. We also use a special ﬂavor of
OT extension called correlated OT extension [11]. In this variant
which we denote by COT, the sender’s two inputs to each OT
are not independent. Instead, the two inputs to each OT instance
are: a random value s0 and a value s1 = f (s0) for a correlation
function f of the sender’s choice. The communication for a
COT of l-bit messages, denoted by COTl, is λ + l bits, and
the computation consists of 3 hashing.
Garbled Circuit 2PC. Garbled Circuits were ﬁrst introduced
by [47]. A garbling scheme consists of a garbling algorithm
that takes a random seed σ and a function f and generates
a garbled circuit F and a decoding table dec; the encoding
algorithm takes input x and the seed σ and generates garbled
input (cid:3)x ; the evaluation algorithm takes (cid:3)x and F as input and
returns the garbled output (cid:3)z; and ﬁnally, a decoding algorithm
that takes the decoding table dec and (cid:3)z and returns f (x). We
require the garbling scheme to satisfy the standard security
properties formalized in [13].
Given such a garbling scheme, it is possible to design
a secure two-party computation protocol as follows: Alice
generates a random seed σ and runs the garbling algorithm for
function f to obtain a garbled circuit GC. She also encodes her
input (cid:3)x using σ and x as inputs to the encoding algorithm. Alice
sends GC and (cid:3)x to Bob. Bob obtains his encoded (garbled)
input (cid:3)y using an oblivious transfer for each bit of y4. He then
runs the evaluation algorithm on GC,(cid:3)x,(cid:3)y to obtain the garbled
output (cid:3)z. We can have Alice, Bob, or both learn an output
by communicating the decoding table accordingly. The above
protocol securely realizes the ideal functionality Ff that simply
takes the parties inputs and computes f on them. See [32] for
a more detailed description and proof of security against a
semi-honest adversary. In our protocols, we denote this garbled
circuit 2PC by (za, zb) ← GarbledCircuit(x; y, f )
Secret Sharing and Multiplication Triplets. In our protocols,
all intermediate values are secret-shared between the two
servers. We employ three different sharing schemes: Additive
sharing, Boolean sharing and Yao sharing. We brieﬂy review
these schemes but refer the reader to [18] for more details.
(·)) an (cid:6)-bit value a, the ﬁrst party
P0 generates a0 ∈ Z2(cid:2) uniformly at random and sends a1 = a−
a0 mod 2(cid:4) to the second party P1. We denote the ﬁrst party’s
To additively share (Shr
A
4While and OT-based encoding is not a required property of a garbling
scheme, all existing constructions permit such interacting encodings
23
A
To multiply (Mul
0 = a0 and the second party’s by (cid:7)a(cid:8)A
share by (cid:7)a(cid:8)A
1 = a1.
For ease of composition we omit the modular operation in
the protocol descriptions. In this paper, we mostly use the
additive sharing, and denote it by (cid:7)·(cid:8) for short. To reconstruct
(·,·)) an additively shared value (cid:7)a(cid:8), Pi sends (cid:7)a(cid:8)i to
(Rec
P1−i who computes (cid:7)a(cid:8)0 + (cid:7)a(cid:8)1.
Given two shared values (cid:7)a(cid:8) and (cid:7)b(cid:8), it is easy to non-