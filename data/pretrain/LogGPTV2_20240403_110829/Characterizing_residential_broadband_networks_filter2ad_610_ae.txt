rates. Instead, we examined the packet loss rates of the In-
ternet paths between our well-connected measurement hosts
and the broadband hosts. Because the broadband access
links are part of these Internet paths, our measured loss rates
provide an upper bound on the broadband links’ loss rates.
4.3.1 Do broadband links see high packet loss?
We used the small-TCP trickle probe trains to calculate the
loss rates along the round-trip paths to remote broadband
hosts. We sent widely spaced trickle probes at a very low
rate for a week, and we measured the fraction of probes for
which the broadband hosts did not respond. This includes
losses on both the upstream and the downstream paths, and
it measures the loss rate under normal operating conditions
of the network. Note that the loss rate we measured might
diﬀer from the loss rate that application traﬃc (e.g., TCP
ﬂows) saturating broadband links would suﬀer.
Figure 14 presents our results. We found that both ca-
ble and DSL have remarkably low packet loss rates. The
loss rate is below 1% for more than 95% of all DSL and ca-
ble paths. Overall, we found that the packet loss rates for
broadband access networks are similar to those observed in
academic network environments [11, 40].
We also examined how loss rates varied over the course of
the week. Figure 15 shows our measurements for two typical
providers: a DSL ISP (Ameritech) and a cable ISP (Chello).
The horizontal axis shows the local time for the ISPs. The
loss rates shown along the vertical axis are averaged over
intervals of 120 minutes. We found that loss rates exhibit
diurnal patterns with occasional spikes. Both ISPs follow
similar diurnal patterns, showing lower loss rates in the early
morning than in the evening.
4.3.2 Do ISPs use active queue management?
When packets are sent very quickly, they begin to ﬁll up
queues, and the routers must eventually drop some of the
packets. The most common queue management policy is
tail-drop – i.e., all packets arriving after the queue is full are
discarded. More active queue management policies, such
as RED [20], proactively drop packets using probabilistic
schemes when the queue starts to ﬁll up but before the queue
is full. Active queue management has been extensively stud-
ied, but relatively little is known about the extent to which
it is deployed in practice.
We performed the following experiment to infer whether
the broadband ISPs are using active queue management poli-
cies. We used the small-TCP ﬂood to overﬂow both down-
stream and upstream links, and we used IPIDs to distin-
guish between losses occurring upstream and those occurring
downstream [36]. For each successfully received response, we
recorded the RTT, and we calculated the average loss rate
over a sliding window of 40 packets. We examined the cor-
relation between the loss rates and the corresponding RTTs.
On the basis of this correlation, we can infer whether routers
use tail-drop or more active queue management policies. A
tail-drop policy will result in a steep increase in loss rate
when the queue is full (i.e., for a large RTT value); if an
active queue management policy such as RED is used, then
the loss rate will increase proportionally to the RTT after a
certain threshold.
Figure 16 shows how the loss rates increase with the RTT
for two broadband hosts, one in PacBell and one in SWBell.
For the PacBell host, the loss rate increases steeply around
e
t
a
r
s
s
o
l
m
a
e
r
t
s
p
U
100%
80%
60%
40%
20%
0%
0
100
200
Active queue management (probably RED)
(SWBell)
Tail-drop
(PacBell)
700
800
900
400
300
600
Round-trip time (milliseconds)
500
Figure 16: Tail-drop and active queue management:
When a tail-drop queue overﬂows, the loss rate increases
sharply. If the loss rate increases proportionally to the queue
length after a threshold, then this suggests that active queue
management (probably RED) is being used.
an RTT of 850 ms, which suggests that a tail-drop queue
is used. The loss rate for the SWBell host shows a diﬀerent
trend; after 500 ms, it increases almost linearly with the RTT
before stabilizing at around 85%. This behavior matches the
description of the RED active queue management policy.
To quantify the extent of RED deployment in broadband
networks, we tested whether the increases in RTT and loss
rates are strongly correlated. If the correlation coeﬃcient is
high (≥ 0.9) beyond a threshold loss rate of 5%, we conclude
that the link may be using RED as its drop policy. We
did not calculate the correlation coeﬃcient for low loss rates
(below 5%) because these loses might be sporadic and not
representative of the broadband router’s queue policy.
We found that 26.2% of the DSL hosts show a RED-style
drop policy on their upstream queues. The three providers
owned by AT&T (i.e., Ameritech, BellSouth, and PacBell)
exhibit deployment rates between 50.3% and 60.5%, whereas
all other DSL providers’ deployment rates are below 23.0%.
The partial deployment of RED-style policies within ISPs
could be due to heterogeneity in the ISPs’ equipment. We
did not detect RED in any of the cable ISPs measured.
4.4 Summary
We have presented an in-depth characterization of the prop-
erties of residential broadband networks. Our analysis re-
veals important ways in which these networks diﬀer from
academic networks, and it quantiﬁes these diﬀerences. We
summarize our key ﬁndings below:
Allocated link bandwidths: Our results show that down-
stream bandwidths exceed upstream bandwidths by more
than a factor of 10 for some ISPs. In contrast to popular be-
lief, for most ISPs, the measured bandwidths matched well
with the advertised rates at all times of day, and we found
little evidence of competing traﬃc aﬀecting their links. Al-
though link bandwidths remain stable over the long term,
they show high variation in the short term, especially for ca-
ble hosts. For some ISPs, link bandwidths change abruptly
as a result of traﬃc shaping.
Packet latencies: Many DSL hosts show large (≥ 10 ms)
last-hop propagation delays. Cable hosts suﬀer higher jitter
than DSL hosts as a result of time-slotted packet transmis-
sion policies on their upstream links. Packet concatenation
on the upstream links can add another 5 − 10 ms of jitter in
cable links.
All ISPs deploy queues that are several times larger
than their bandwidth-delay products. Whereas downstream
queues can delay packets by more than 100 ms, the upstream
queueing delays can exceed several hundreds of milliseconds
and, at times, a few seconds.
Packet loss: Both DSL and cable ISPs exhibit surprisingly
low packet loss. We also found that many DSL hosts use ac-
tive queue management policies (e.g., RED) when dropping
packets.
IMPLICATIONS OF OUR FINDINGS
5.
We consider that our observations about broadband net-
works’ characteristics can help researchers to understand
how well existing protocols and systems work in the com-
mercial Internet. Our ﬁndings oﬀer useful insights for the
designers of future applications. To illustrate this, we brieﬂy
discuss the potential implications of our measurements for
three popular Internet-scale systems.
Transport Control Protocols: Our bandwidth and la-
tency ﬁndings have several implications for transport pro-
tocol designs. For example, protocols such as TCP Ve-
gas [9] and PCP [3] use RTT measurements to detect incip-
ient congestion. In the presence of the high jitter found in
our measurements, this mechanism might trigger congestion
avoidance too early. Bandwidth-probing techniques, such as
packet-pair [31], could return incorrect results in the pres-
ence of traﬃc shaping or packet concatenation. This could
be detrimental to transport protocols that rely on probing
to adjust their transfer rates, such as PCP.
Network coordinate and location systems: Many IP-
to-geolocation mapping tools [22, 52] use latency measure-
ments to determine a host’s location. The large propagation
delays and high jitter found in broadband networks are likely
to seriously interfere with the accuracy of these systems.
Similarly, network coordinate systems [16, 37] use latency
estimates to assign a set of coordinates to their participating
hosts. A recent study [34] found that network coordinate sys-
tems do not perform well when deployed in BitTorrent net-
works, because RTTs between nodes can vary by up to four
orders of magnitude. Our measurements explain and pro-
vide insights into these ﬁndings: BitTorrent networks typi-
cally include many residential links, which have very large
RTT variations as a result of their long queues. BitTorrent
traﬃc compounds these variations because it tends to ﬁll up
the queues.
Interactive and real-time applications: Recently, the
popularity of VoIP and online games has grown considerably.
Our data shows that real-time applications will be negatively
aﬀected by the broadband links’ large queueing delays. Be-
cause queueing delays increase in the presence of competing
traﬃc, these time-sensitive applications are likely to experi-
ence degraded service when they are used concurrently with
bandwidth-intense applications, such as BitTorrent.
6. RELATED WORK
There is a large body of previous measurement work charac-
terizing Internet paths other than broadband. Paxson [40]
studied network packet dynamics among a ﬁxed set of Inter-
net hosts located primarily in academic institutions. More
recently, several studies have examined the characteristics of
the network paths connecting the PlanetLab testbed [5, 43].
Although our paper uses similar measurement techniques,
the network environment we study is diﬀerent.
Compared with other parts of the Internet, broadband ac-
cess networks have received relatively little attention. Clay-
pool et al. [14] performed a measurement study of access
networks’ queue sizes using 47 volunteering broadband hosts.
They found that the median queue size was 350 ms in DSL
networks and 150 ms in cable networks, and they showed
in simulation that large queue sizes are detrimental to net-
work traﬃc from interactive applications. Our results are
consistent with these earlier ﬁndings, but they are based on
a set of hosts that is more than two orders of magnitude
larger. Similarly, Jehaes et al. [30] observed a large increase
in round-trip delays over saturated broadband links. Their
experiments were limited to one DSL and one cable link.
Some recent studies have examined the traﬃc generated
by residential customers in Japan [12] and France [46]. These
results complement ours, because we examined the proper-
ties of the networks themselves rather than the properties of
traﬃc traversing them. A comprehensive view of residential
networks requires a good understanding of both.
Lakshminarayanan and Padmanabhan [32] performed a
network measurement study from 25 broadband hosts to dif-
ferent Internet hosts, covering several application-level met-
rics such as TCP throughput and latency. Our study con-
ﬁrms some of their ﬁndings but at a much larger scale. In
their later work, Lakshminarayanan et al. [33] outlined pit-
falls in measuring link capacities of cable and DSL networks
by using existing bandwidth estimation tools. In particular,
the accuracy of these tools is greatly inﬂuenced by the rate
regulation schemes used in cable and DSL networks. Our
measurement methodology does not suﬀer from such inac-
curacies because it relies on saturating the links for a short
duration.
Many previous studies have measured network proper-
ties of hosts participating in ﬁle-sharing peer-to-peer sys-
tems [7, 44, 45]. Because a large fraction of peers in these
systems access the Internet over cable and DSL networks, all
these studies indirectly include measurements of broadband
access networks. However, these results cannot be compared
directly with ours because the focus of these studies is pri-
marily on application-level performance and not on the link
level characteristics of broadband networks.
7. CONCLUSIONS
In this paper, we presented the ﬁrst large-scale measurement
study of major cable and DSL providers in North Amer-
ica and Europe. Our study characterized several important
characteristics of broadband networks, including available
link capacities, packet transmission policies, jitter, packet
drop policies, and queue lengths. Our analysis revealed im-
portant ways in which residential networks diﬀer from the
conventional wisdom about the Internet. We also discussed
the implications of our ﬁndings for many emerging protocols
and systems, such as delay-based congestion control (e.g.,
PCP) and network coordinate systems (e.g., Vivaldi).
8. ACKNOWLEDGMENTS
We thank Steve Gribble and Dan Sandler for generously
hosting measurement servers for our experiments. We are
grateful to our shepherd Guillaume Urvoy-Keller, Peter Dr-
uschel, and our anonymous reviewers for providing detailed
and helpful feedback on this paper.
9. REFERENCES
[1] A. Akella, S. Seshan, and A. Shaikh. An empirical
evaluation of wide-area Internet bottlenecks. In
Proceedings of IMC’03, Oct 2003.
[2] D. G. Andersen, H. Balakrishnan, F. Kaashoek, and
R. Morris. Experience with an Evolving Overlay
Network Testbed. ACM Computer Communication
Review, 33(3):13–19, July 2003.
[3] T. Anderson, A. Collins, A. Krishnamurthy, and
J. Zahorjan. PCP: Eﬃcient endpoint congestion
control. In Proceedings of NSDI’06, San Jose, CA,
May 2006.
[4] G. Appenzeller, I. Keslassy, and N. McKeown. Sizing
router buﬀers. In Proceedings of ACM SIGCOMM’04,
Aug 2004.
[5] S. Banerjee, T. G. Griﬃn, and M. Pias. The
interdomain connectivity of PlanetLab nodes. In
Proceedings of the 5th Passive and Active
Measurement Conference (PAM), Apr 2004.
[6] J. Bellardo and S. Savage. Measuring packet
reordering. In Proceedings of the 2nd ACM SIGCOMM
Internet Measurement Workshop (IMW), Marseille,
France, Nov 2002.
[7] R. Bhagwan, S. Savage, and G. M. Voelker.
Understanding availability. In Proceedings of 1st
International Workshop on Peer-to-Peer Systems
(IPTPS), Boston, MA, March 2002.
[8] Bittorrent. http://bittorrent.com.
[9] L. S. Brakmo and L. Peterson. TCP Vegas: End to
end congestion avoidance on a global Internet. IEEE
Journal on Selected Areas in Communication,
13(8):1465–1480, Oct 1995.
[10] CableLabs. DOCSIS 1.1 interface speciﬁcation, 2006.
http://www.cablemodem.com/specifications/
specifications11.html.
[11] B. Chandra, M. Dahlin, L. Gao, and A. Nayate.
End-to-end WAN service availability. In Proceedings of
the 3rd USITS, 2001.
[12] K. Cho, K. Fukuda, H. Esaki, and A. Kato. The
impact and implications of the growth in residential
user-to-user traﬃc. In Proceedings of SIGCOMM’06,
Sep 2006.
[13] Cisco. Internetworking technology handbook, 2006.
[14] M. Claypool, R. Kinicki, M. Li, J. Nichols, and H. Wu.
Inferring queue sizes in access networks by active
measurement. In Proceedings of the 4th Passive and
Active Measurement Workshop (PAM), Antibes
Juan-les-Pins, France, Apr 2004.
[15] Comcast PowerBoost press release, Jun 2006.
http://www.cmcsk.com/phoenix.zhtml?c=147565&p=
irol-newsArticle&ID=890297.
[16] F. Dabek, R. Cox, F. Kaashoek, and R. Morris.
Vivaldi: A decentralized network coordinate system.
In Proceedings of SIGCOMM’04, Aug 2004.
[17] C. Dovrolis, P. Ramanathan, and D. Moore. Packet
dispersion techniques and a capacity estimation
methodology. IEEE/ACM Transactions on
Networking, Dec 2004.
[18] Federal Communications Commission ComCom,
Switzerland. Broadband in the universal service, Sep
2006.
[19] S. Floyd, M. Handley, J. Padhye, and J. Widmer.
Equation-based congestion control for unicast
applications. In Proceedings of SIGCOMM’00, Aug
2000.
[20] S. Floyd and V. Jacobson. Random early detection
gateways for congestion avoidance. IEEE/ACM
Transactions on Networking, 1(4):397–413, Aug 1993.
[21] R. Govindan and V. Paxson. Estimating router ICMP
generation delays. In Proceedings of Passive and Active
Measurement (PAM), Fort Collins, CO, USA, 2002.
[22] B. Gueye, A. Ziviani, M. Crovella, and S. Fdida.
[36] R. Mahajan, N. Spring, D. Wetherall, and
Constraint-based geolocation of Internet hosts.
IEEE/ACM Transactions on Networking,
14(6):1219–1232, 2006.
[23] A. Haeberlen, M. Dischinger, K. P. Gummadi, and
S. Saroiu. Monarch: A tool to emulate transport
protocol ﬂows over the Internet at large. In Proceedings
of IMC’06, Rio de Janeiro, Brazil, Oct 2006.
[24] Y. hua Chu, A. Ganjam, T. S. E. Ng, S. G. Rao,
K. Sripanidkulchai, J. Zhan, and H. Zhang. Early
experience with an Internet broadcast system based on
overlay multicast. In USENIX Annual Technical
Conference, Jun 2004.
[25] Y. hua Chu, S. G. Rao, S. Seshan, and H. Zhang.
Enabling conferencing applications on the Internet
using an overlay multicast architecture. In Proceedings
of SIGCOMM’01, Aug 2001.
[26] G. Iannaccone, C. nee Chuah, R. Mortier,
S. Bhattacharyya, and C. Diot. Analysis of link
failures in an IP backbone. In Proceedings of the 2nd
ACM SIGCOMM Workshop on Internet measurement
(IMW), 2002.
[27] ISP Planet. Top 24 U.S. ISPs by subscriber: Q3 2005,
2006. http://www.isp-planet.com/research/
rankings/usa.html.
[28] ISP Planet. Top seven ISPs in Canada by subscriber:
Q4 2006, 2006. http://www.isp-planet.com/
research/rankings/2006/canada_q42006.html.
[29] M. Jain and C. Dovrolis. End-to-end available
bandwidth: Measurement methodology, dynamics, and
relation with TCP throughput. IEEE/ACM
Transactions on Networking, Aug 2003.
[30] T. Jehaes, D. D. Vleeschauwer, T. Coppens, B. V.
Doorselaer, E. Deckers, W. Naudts, K. Spruyt, and
R. Smets. Access network delay in networked games.
In Proceedings of the 2nd workshop on Network and
system support for games (NetGames), 2003.
[31] S. Keshav. A control-theoretic approach to ﬂow
control. In Proceedings of SIGCOMM’91, Zurich,
Switzerland, Sept 1991.
[32] K. Lakshminarayanan and V. N. Padmanabhan. Some
ﬁndings on the network performance of broadband
hosts. In Proceedings of IMC’03, Miami, FL, USA, Oct
2003.
[33] K. Lakshminarayanan, V. N. Padmanabhan, and
J. Padhye. Bandwidth estimation in broadband access
networks. In Proceedings of IMC’04, Taormina, Italy,
Oct 2004.
[34] J. Ledlie, P. Gardner, and M. Seltzer. Network
coordinates in the wild. In Proceedings of NSDI’07,
Apr 2007.
[35] S.-J. Lee, P. Sharma, S. Banerjee, S. Basu, and
R. Fonseca. Measuring bandwidth between PlanetLab
nodes. In Proceedings of the Passive and Active
Measurement Workshop (PAM), March 2005.
T. Anderson. User-level Internet path diagnosis. In
Proceedings of the 19th Symposium on Operating
Systems Principles (SOSP), Bolton Landing, NY,
USA, Oct 2003.
[37] T. Ng and H. Zhang. Predicting Internet network
distance with coordinates-based approaches. In Proc.
INFOCOM 2002, New York, NY, USA, June 2002.
[38] Nielsen/NetRatings. U.S. broadband connections reach
critical mass, 2004. http:
//www.nielsen-netratings.com/pr/pr_040818.pdf.
[39] OECD broadband statistics, Dec 2005.
http://www.oecd.org/sti/ict/broadband.
[40] V. Paxson. End-to-end routing behavior in the
Internet. IEEE/ACM Transactions on Networking,
5(5):601–615, Oct 1997.
[41] PlanetLab. http://www.planet-lab.org/.
[42] Point Topic Ltd. UK broadband market monitor,
2006. http://point-topic.com/content/ukplus/
email%20archive/wukbbmm7050602.htm.
[43] H. Pucha, Y. C. Hu, and Z. M. Mao. On the impact of
research network based testbeds on wide-area
experiments. In Proceedings of IMC’06, Oct 2006.
[44] S. Saroiu, K. P. Gummadi, and S. D. Gribble. A
measurement study of peer-to-peer ﬁle sharing
systems. In Proceedings of the Multimedia Computing
and Networking (MMCN), San Jose, CA, Jan 2002.
[45] S. Sen and J. Wang. Analyzing peer-to-peer traﬃc
across large networks. In Proceedings of the 2nd ACM
SIGCOMM Workshop on Internet Measurement
(IMW), Marseille, France, Nov 2002.
[46] M. Siekkinen, D. Collange, G. Urvoy-Keller, and
E. Biersack. Performance limitations of ADSL users:
A case study. In Proceedings of the 8th Passive and
Active Measurement Conference (PAM), Apr 2007.
[47] N. Spring, R. Mahajan, D. Wetherall, and
T. Anderson. Measuring ISP topologies with
Rocketfuel. IEEE/ACM Transactions on Networking,
12(1):2–16, Feb 2004.
[48] United Kingdom e-Minister and e-Envoy. UK online:
The broadband future, 2001.
[49] C. Villamizar and C. Song. High performance TCP in
ANSNET. ACM Computer Communication Review,
24(5):45–60, 1994.
[50] Wikipedia. Broadband Internet access in the
Netherlands, 2006. http://en.wikipedia.org/w/
index.php?title=Broadband_Internet_access_in_
the_Netherlands&oldid=152236688.
[51] Windsor Oaks Group, LLC. Market outlook report,
May 2005. http:
//www.broadbandtrends.com/Report%20Summary/
2006/BBT_GlobalBBOutlook2006_061110_TOC.pdf.
[52] B. Wong, I. Stoyanov, and E. G. Sirer. Octant: A
comprehensive framework for the geolocalization of
Internet hosts. In Proceeedings of NSDI’07, Apr 2007.