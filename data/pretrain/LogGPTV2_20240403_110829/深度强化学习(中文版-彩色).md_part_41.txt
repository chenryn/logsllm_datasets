g = self._flat_concat(tape0.gradient(d_kl, pi_params))
l = tf.reduce_sum(g * x)
hvp = self._flat_concat(tape1.gradient(l, pi_params))
if DAMPING_COEFF > 0:
hvp += DAMPING_COEFF * x
return hvp
有了如上准备，我们最后可以开始更新了。首先，通过GAE采集数据并计算梯度和损失。接
着我们使用共轭梯度算法来计算变量x，它对应公式xˆ ≈ Hˆ−1gˆ 中的xˆ 。然后，我们计算公
q q k k k k
式θ = θ +αj 2δ xˆ 中的 2δ 部分。之后，我们使用回溯线搜索来更新策略网
k+1 k xˆT kHˆ kxˆk k xˆT kHˆ kxˆk
络。最后，通过MES损失更新价值网络。
def update(self):
188
5.10 策略梯度代码例子
states, actions, adv, rewards_to_go, logp_old_ph, old_mu, old_log_std =
self.buf.get()
g, pi_l_old = self.gradient(states, actions, adv, logp_old_ph)
Hx = lambda x: self.hvp(states, old_mu, old_log_std, x)
x = self.cg(Hx, g)
alpha = np.sqrt(2 * DELTA / (np.dot(x, Hx(x)) + EPS))
old_params = self.get_pi_params()
def set_and_eval(step):
params = old_params - alpha * x * step
self.set_pi_params(params)
d_kl = self.kl(states, old_mu, old_log_std)
loss = self.pi_loss(states, actions, adv, logp_old_ph)
return [d_kl, loss]
# 回溯线搜索，固定 KL 限制
for j in range(BACKTRACK_ITERS):
kl, pi_l_new = set_and_eval(step=BACKTRACK_COEFF ** j)
if kl <= DELTA and pi_l_new <= pi_l_old:
# 接受一步线搜索中更新的新参数
break
else:
# 线搜索失败，保持旧参数
set_and_eval(step=0.)
# 价值网络更新
for _ in range(TRAIN_V_ITERS):
self.train_vf(states, rewards_to_go)
这里在轨迹要被切断或者回合结束的时候，也会需要使用 finish_path()函数。如果轨迹
由于智能体到达终止状态而结束，那么最后的价值将被设置为0。
def finish_path(self, done, next_state):
if not done:
next_state = np.array([next_state], np.float32)
last_val = self.critic(next_state)
189
第5章 策略梯度
else:
last_val = 0
self.buf.finish_path(last_val)
代码的主循环如下所示。我们先创建环境、智能体和一些后面会用上的变量。
env = gym.make(ENV_ID).unwrapped
# 设置随机种子以便复现效果
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
env.seed(RANDOM_SEED)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
action_bound = env.action_space.high
agent = TRPO(state_dim, action_dim, action_bound)
t0 = time.time()
在训练模式下，我们将智能体与环境产生的交互数据存入缓存，当缓存满了的时候则进行一
次更新。
if args.train: # train
all_episode_reward = []
for episode in range(TRAIN_EPISODES):
state = env.reset()
state = np.array(state, np.float32)
episode_reward = 0
for step in range(MAX_STEPS):
if RENDER:
env.render()
action, value, logp, mean, log_std = agent.get_action(state)
next_state, reward, done, _ = env.step(action)
next_state = np.array(next_state, np.float32)
agent.buf.store(state, action, reward, value, logp, mean, log_std)
episode_reward += reward
state = next_state
if agent.buf.is_full():
190
5.10 策略梯度代码例子
agent.finish_path(done, next_state)
agent.update()
if done:
break
agent.finish_path(done, next_state)
if episode == 0:
all_episode_reward.append(episode_reward)
else:
all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward *
0.1)
print(
’Training | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:
{:.4f}’.format(
episode+1, TRAIN_EPISODES, episode_reward,
time.time() - t0
)
)
if episode
agent.save()
agent.save()
接着我们可以增加一些绘图的代码，以便于观察训练过程。
plt.plot(all_episode_reward)
if not os.path.exists(’image’):
os.makedirs(’image’)
plt.savefig(os.path.join(’image’, ’trpo.png’))
当训练完成后，我们可以开始测试。
if args.test:
# test
agent.load()
for episode in range(TEST_EPISODES):
state = env.reset()
episode_reward = 0
for step in range(MAX_STEPS):
env.render()
action, *_ = agent.get_action(state, greedy=True)
191
第5章 策略梯度
state, reward, done, info = env.step(action)
episode_reward += reward
if done:
break
print(
’Testing | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:
{:.4f}’.format(
episode + 1, TEST_EPISODES, episode_reward,
time.time() - t0))
5.10.6 PPO:Pendulum-V0
PPO是一种一阶方法，与TRPO这样的二阶算法不同。
在PPO-Penalty中，是通过给目标函数增加一个KL散度惩罚项的，以解决像TRPO这样带
KL约束的更新问题。PPO类的结构如下所示：
class PPO(object):
def __init__(self, state_dim, action_dim, action_bound, method=’clip’): # 初始化
...
def train_actor(self, state, action, adv, old_pi): # 行动者训练函数
...
def train_critic(self, reward, state): # 批判者训练函数
...
def update(self): # 主更新函数
...
def get_action(self, s, greedy=False): # 选择动作
...
def save(self): # 存储网络
...
def load(self): # 载入网络
...
def store_transition(self, state, action, reward): # 存储每步的状态、动作、奖励
...
def finish_path(self, next_state): # 计算累积奖励
...
在PPO算法中，我们在初始化函数中建立行动者网络和批判者网络。PPO有两种方法：PPO-
Penalty和PPO-Clip。我们在选用不同的方法时，要设置其相对应的参数。由于环境是一个连续运
192
5.10 策略梯度代码例子
动控制环境，我们可以使用随机策略网络输出均值和对数标准差来描述动作分布。另外，我们在
网络输出加了一个lambda层将均值乘以2，这是由于’Pendulum-V0’环境中的动作范围是[−2,2]。
class PPO(object):
def __init__(self, state_dim, action_dim, action_bound, method=’clip’):
# Critic
with tf.name_scope(’critic’):
inputs = tl.layers.Input([None, state_dim], tf.float32, ’state’)
layer = tl.layers.Dense(64, tf.nn.relu)(inputs)
layer = tl.layers.Dense(64, tf.nn.relu)(layer)
v = tl.layers.Dense(1)(layer)
self.critic = tl.models.Model(inputs, v)
self.critic.train()
# Actor
with tf.name_scope(’actor’):
inputs = tl.layers.Input([None, state_dim], tf.float32, ’state’)
layer = tl.layers.Dense(64, tf.nn.relu)(inputs)
layer = tl.layers.Dense(64, tf.nn.relu)(layer)
a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)
mean = tl.layers.Lambda(lambda x: x * action_bound, name=’lambda’)(a)
logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))
self.actor = tl.models.Model(inputs, mean)
self.actor.trainable_weights.append(logstd)
self.actor.logstd = logstd
self.actor.train()
self.actor_opt = tf.optimizers.Adam(LR_A)
self.critic_opt = tf.optimizers.Adam(LR_C)
self.method = method
if method == ’penalty’: