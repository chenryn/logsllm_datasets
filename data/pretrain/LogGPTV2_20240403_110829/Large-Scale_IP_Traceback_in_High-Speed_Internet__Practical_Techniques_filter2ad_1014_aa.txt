title:Large-Scale IP Traceback in High-Speed Internet: Practical Techniques
and Theoretical Foundation
author:Jun Li and
Minho Sung and
Jun (Jim) Xu and
Li Li
Large-Scale IP Traceback in High-Speed Internet: Practical
Techniques and Theoretical Foundation (cid:3)
Jun Li Minho Sungy
Jun (Jim) Xu
College of Computing
Georgia Institute of Technology
fjunli,mhsung,PI:EMAIL
Li (Erran) Li
Bell Labs
Lucent Technologies
PI:EMAIL
Abstract
1. Introduction
Tracing attack packets to their sources, known as IP
traceback, is an important step to counter distributed
denial-of-service (DDoS) attacks. In this paper, we pro-
pose a novel packet logging based (i.e., hash-based) trace-
back scheme that requires an order of magnitude smaller
processing and storage cost than the hash-based scheme
proposed by Snoeren et al. [29], thereby being able to scal-
able to much higher link speed (e.g., OC-768). The base-
line idea of our approach is to sample and log a small per-
centage (e.g., 3.3%) of packets. The challenge of this low
sampling rate is that much more sophisticated techniques
need to be used for traceback. Our solution is to construct
the attack tree using the correlation between the attack
packets sampled by neighboring routers. The scheme us-
ing naive independent random sampling does not per-
form well due to the low correlation between the packets
sampled by neighboring routers. We invent a sampling
scheme that improves this correlation and the overall ef-
(cid:12)ciency signi(cid:12)cantly. Another major contribution of this
work is that we introduce a novel information-theoretic
framework for our traceback scheme to answer impor-
tant questions on system parameter tuning and the fun-
damental trade-o(cid:11) between the resource used for trace-
back and the traceback accuracy. Simulation results based
on real-world network topologies (e.g. Skitter) match very
well with results from the information-theoretic analysis.
The simulation results also demonstrate that our trace-
back scheme can achieve high accuracy, and scale very
well to a large number of attackers (e.g., 5000+).
(cid:3) The work of J. Li, M. Sung and J. Xu was supported in part by
the National Science Foundation (NSF) under Grant ITR/SY
ANI-0113933 and the NSF CAREER Award ANI-0238315.
y The names of the student authors, Jun Li and Minho Sung, are
listed in the alphabetical order, representing equal contribu-
tions from them.
Distributed Denial of Service (DDoS) attacks
against high-pro(cid:12)le web sites such as Yahoo, CNN,
Amazon and E*Trade in early 2000 [13] rendered the
services of these web sites unavailable for hours or even
days. New instances of DDoS attacks continue to be re-
ported. For example, a recent DDoS attack against
root DNS servers brought down eight of them in an ef-
fort to paralyze the Internet [20]. It is clear that DDoS
attacks will not stop or scale down until they are prop-
erly addressed.
One possible way to counter DDoS attacks is to trace
the attack sources and punish the perpetrators. How-
ever, current Internet design makes such tracing di(cid:14)-
cult in two aspects. First, there is no (cid:12)eld in the IP
header that indicates its source except for the IP ad-
dress, which can be easily spoofed by an attacker. Sec-
ond, the Internet is stateless in that it does not keep
track of the path traversed by a packet. Recently, ef-
forts are made to change one or both aspects to allow
for tracing packets to their sources, known as IP Trace-
back. Up to now, two main types of traceback tech-
niques have been proposed in the literature.
1. One is to mark each packet with partial path in-
formation probabilistically [9, 28, 30, 8, 14]. By re-
ceiving a signi(cid:12)cant number of packets, the victim
can construct the attack paths. This is referred to
as Probabilistic Packet Marking (PPM) scheme.
2. The other is to store packet digests in the form of
Bloom (cid:12)lters [3] at each router [29]. By checking
neighboring routers iteratively with attack pack-
ets, the attack path of a (cid:13)ow can be constructed.
This is referred to as hash-based scheme.
However, both traceback schemes su(cid:11)er from scal-
ability problems. As we will show in the next section,
PPM schemes cannot scale to large number of attack-
ers as the best scheme proposed can only e(cid:14)ciently
trace fewer than 100 attackers using a 17-bit mark-
ing (cid:12)eld (discussed later); Hash-based scheme is not
scalable for high-speed links since recording 100% of
packets, even in the Bloom (cid:12)lter digest form, would in-
cur prohibitively high computational and storage over-
head. The objective of our work is to design a traceback
scheme that is scalable both in terms of the number of at-
tackers and in terms of high link speed.
1.1. Scalability problems of existing ap-
proaches
The advantage of PPM schemes is that they do not
incur any storage overhead at the routers and the com-
putation of marking is usually lightweight. However,
PPM-based schemes work well only when the number
of attackers is small, due partly to the limited number
of bits available for marking in the IP header. A recent
PPM scheme proposed by Goodrich [14] is shown to be
the most scalable one1 among the PPM schemes. How-
ever, with a marking (cid:12)eld of 17 bits, it can only scale
up to attack trees containing 100 routers2. A large-scale
DDoS attack can have thousands of attackers and tens
of thousands of routers on the attack paths, making
the PPM schemes unsuitable for traceback.
Hash-based approach, on the other hand, is very ef-
fective for large-scale IP traceback, and needs only a
single packet to trace one attacker [29]. However, since
it computes and stores a Bloom (cid:12)lter digest for ev-
ery packet, its computational and storage overhead is
prohibitive at a router with very high speed links. For
example, assuming a packet size of 1,000 bits, a du-
plex OC-192 link requires 60 million hash operations
to be performed every second, resulting in the use of
SRAM (50ns DRAM is too slow for this) and 44GB
of storage space every hour, with the parameters sug-
gested in [29]. It is important to reduce the computa-
tional, memory and storage overhead of the hash-based
scheme for it to be practical for high-speed Internet.
1.2. New contributions
Our technical contributions are two-fold. First, we
propose a novel packet logging based traceback scheme
1
Song et at.’s scheme [30] allows for traceback to a large number
of attackers. However, it requires the knowledge of the router-
level Internet topology, which may not be practical. For the
traceback to be tamper-resistant, it also requires most of the
Internet routers authenticate to the victim, which can be com-
plicated to deploy and administer.
2 We assume that the \message size" (de(cid:12)ned in [14]) is 64 bits
for representing the IP addresses of the current router and the
previous router, and the \collision size" (de(cid:12)ned in [14]) is no
more than 2.
that is scalable to high link speeds. The baseline idea
of our approach is to sample a small percentage (e.g.,
3.3%) of packets. We construct the attack tree using
the correlation between the attack packets sampled by
neighboring routers. The scheme with naive indepen-
dent random sampling does not perform well due to the
low correlation between the packets sampled by neigh-
boring routers. We invent a sampling scheme that im-
proves this correlation and the overall e(cid:14)ciency by or-
ders of magnitude. Sampling greatly reduces the com-
putational and storage overhead for packet logging.
For example, with a sampling rate of 3.3% (it can be
smaller), our storage overhead is only 0:4= ln 2 bits per
packet3, a duplex OC-192 link will require the compu-
tation of 8 million hash functions every second, and the
storage of 5.2GB for one hour’s tra(cid:14)c. This is an order
of magnitude more a(cid:11)ordable than the scheme in [29].
Our second major contribution is to introduce a
novel information-theoretic framework for our trace-
back scheme to answer important questions on system
parameter tuning and on the fundamental trade-o(cid:11) be-
tween the resource used for traceback and the trace-
back accuracy. For a given performance constraint,
there is the question of how to tune the traceback
scheme in terms of the number of hash functions and
the sampling rate. This optimization problem is for-
mulated as a channel capacity maximization problem
in information theory. This framework also allows us
to compute the minimum number of attack packets
needed for achieving certain traceback accuracy and
how this number scales to larger number of attackers.
Our proposed scheme is simulated on three sets
of real-world Internet topologies with varying op-
erating parameters. Simulation results demonstrate
that, even when there are a large number of attack-
ers, our traceback scheme can accurately (cid:12)nd most
of them using a reasonable number of attack pack-
ets. For example, with a sampling probability of only
3.3%, our traceback scheme can identify 90% of in-
fected routes, using only a total of 175,000 at-
tack packets for traceback (resulting in a query
size of 4.2MB4), even when there are 1,000 attack-
ers.
The rest of the paper is organized as follows. In Sec-
tion 2 we present an overview of the proposed trace-
back scheme and the information-theoretic framework.
3 Each Bloom (cid:12)lter digest uses 12 hash functions. The reason
why we use 12 will be clear in Section 4. The term ln 2 is due
to the Bloom (cid:12)lter space-e(cid:14)ciency trade-o(cid:11) and will be ex-
plained in Section 3.1.3.
4 Only the invariant parts of IP header(16 bytes) and (cid:12)rst 8 bytes
of the payload will be used for traceback as in [29].
In Section 3, we articulate the challenges raised by sam-
pling, and describe the components of our scheme in de-
tail. In Section 4, the proposed scheme is analyzed us-
ing a novel information-theoretic framework. The per-
formance is evaluated in Section 5 through simulation
studies. Section 6 surveys the related work and Sec-
tion 7 concludes the paper.
2. Overview
2.1. Our solution for large-scale traceback
In this paper we propose a new traceback scheme
that is scalable both to a large number of attackers and
to high link speed. Like [29], our scheme requires Inter-
net routers to record Bloom (cid:12)lter digests of packets go-
ing through them. However, unlike [29], which records
100% of packets, our scheme only samples a small per-
centage of them (say 3.3%) and stores the digests of
the sampled packets. With such a sampling rate, the
storage and computational cost becomes much smaller,
allowing the link speed to scale to OC-192 speed or
higher rates. For example, our scheme can scale to OC-
768 speed (simplex) using only DRAM, when sampling
3.3% of the tra(cid:14)c.
The trade-o(cid:11) of sampling is that it makes the trace-
back process much more di(cid:14)cult, especially with a low
sampling rate such as 3.3%. In particular, it is no longer
possible to trace one attacker with only one packet.
This is because, due to sampling, the probability that
two neighboring routers on the attack path both sam-
ple this packet is very small. This makes the one-packet
traceback operation hard to proceed.
In our scheme, the victim uses a set Lv of at-
tack packets it has received as \material evidence" to
trace and construct the attack tree, consisting of at-
tack paths from attackers to the victim. The attack
tree starts with the victim as the root and the only
leaf. It grows when a leaf node determines that one or
more of its neighbors are highly likely to be on an at-
tack path (called \infected" hereafter). Such a likeli-
hood is assessed by performing the following test. Sup-
pose R1 is a leaf node that is already considered as
being infected (called \convicted"). R1 would like to
check whether one of its neighbors R2 is likely to be
on an attack path. We de(cid:12)ne \what R1 has seen" as
the packets among Lv that match the Bloom (cid:12)lter di-
gests stored at R1. Our test is to check whether \what
R1 has seen" has non-negligible correlation with \what
R2 has seen", as determined by a threshold decoding
rule. If the answer is yes, R2 will be convicted; Other-
wise, R2 will be exonerated. If R2 is convicted, R2 will
further test its neighbors recursively using this proce-
dure. Designing the aforementioned threshold decod-
ing rule is nontrivial, and careful game-theoretic study
is needed to make sure that the rule is loophole-free to
the attackers.
Clearly, the higher the correlation between the at-
tack packets sampled by neighboring infected routers
is, the more accurate our traceback scheme is. Given
other parameters such as sampling rate and the num-
ber of attack packets gathered by the victim (i.e., jLvj)
being (cid:12)xed, it is critical to improve the correlation fac-
tor, the percentage of the attack packets sampled by
R2 (upstream) matched by the attack packets sampled
by R1 (downstream). A naive sampling scheme is that
each router independently samples a certain percent-
age (say 3.3%) of packets. However, in this case the
correlation factor of two routers is just 3.3%. In other
words, what R1 has sampled only matches 3.3% of what
R2 has sampled. While consistent sampling techniques
such as trajectory sampling [10] has the potential to im-
prove this factor to nearly 100%, it will not work for
an adversarial environment, as we will discuss in Sec-
tion 3.1.1. We propose a novel technique that improves
this correlation factor signi(cid:12)cantly, using only one bit
in the IP header for communications between neighbor-
ing routers to coordinate the sampling. This scheme is
shown to be robust against attackers’ tampering. Us-
ing this technique, our scheme requires much smaller
number of attack packets for traceback, and achieves
better traceback accuracy than independent sampling.
2.2. Information-theoretic framework of
our traceback scheme
The design of the scheme leads to a very interest-
ing optimization problem. We assume that the aver-
age number of bits devoted for each packet is a (cid:12)xed
constant s, due to the computational and storage con-
straints of a router. In other words, on the average
for each packet we compute s hash functions. Then
the number of hash functions our scheme computes for
each sampled packet is inversely proportional to the
percentage of packets that is sampled. For example, if
the resource constraint is that 0:4 hash computations
are performed for each packet, one possible combina-
tion is that the router samples 5% of the packets and
the number of hash functions is 8 (5% (cid:2) 8 = 0:4). With
the same resource constraint, an alternative combina-
tion is to sample 2.5% of the packets, but the num-
ber of hash functions is 16. Which one is better? In-
tuitively, higher sampling rate increases the aforemen-
tioned correlation between the packets sampled by two
routers, making traceback easier. However, the num-
ber of hash functions would have to be proportionally
smaller, which results in a higher false positive rate in
Bloom (cid:12)lter. This adds noise to the aforementioned
traceback process and reduces the accuracy. Clearly
there is an inherent trade-o(cid:11) between these two pa-
rameters, but where is the \sweet spot" (i.e., optimal
parameter setting)? We show that this question can be
answered by applying information theory. Our simula-
tion results show that the information-theoretic frame-
work indeed guides us to (cid:12)nd the optimal parameter
setting.
Our information-theoretic framework also allows
us to answer another important question concerning
the trade-o(cid:11) between the amount of evidence the vic-
tim has to gather (the number of attack packets) and
the traceback accuracy. In particular, information the-
ory allows us to derive a lower bound on the number
of packets the victim must obtain to achieve a cer-
tain level of traceback accuracy. A bonus from study-
ing these lower bounds is that it sheds light on
how this number scales to larger number of attack-
ers.
3. Detailed Design
Our scheme consists of two algorithms. One is a sam-
pling algorithm that is running at the Internet routers
to sample and record the Bloom (cid:12)lter digests of the
packets going through them. The other is a traceback
algorithm that is initiated by the victim to trace the at-
tackers using the digests stored at these routers, upon
the detection of a DDoS attack. In Sections 3.1 and 3.2,
we describe the sampling algorithm and the traceback
algorithm in detail.
3.1. Sampling
3.1.1. A design challenge. Our proposed scheme
signi(cid:12)cantly reduces the processing and storage re-
quirements by sampling. However, sampling makes
traceback more di(cid:14)cult. In particular, it is now al-
most impossible to trace one attacker with only one
packet as in [29]. This is because, with a low sampling
percentage, the (cid:12)rst router on the attack path that
will sample a particular attack packet is on the aver-
age many hops away. Intuitively, with a sampling rate
of p, the victim needs to receive at least O( 1
p ) pack-
ets to be able to trace one attacker, since each router
on the path needs to store at least one attack packet. It
turns out that to design a sampling algorithm that al-
lows for accurate traceback of one attacker with this
minimum number of attack packets (i.e., O( 1
p )) is non-
trivial.
A naive sampling scheme is that each router inde-
pendently samples packets with the probability p. How-
ever, this approach does not work well since it would re-
quire a minimum of O( 1
p2 ) attack packets5 to trace one
attacker. Recall from Section 2.1 that if a convicted
router R1 wants to check whether one of its neigh-
bors R2 is infected, the scheme checks whether the set
of packets \R1 has seen" has non-negligible correla-
tion with the set of packets \R2 has seen". It takes at
least O( 1
p2 ) packets for these two sets to have an over-
lap of one or more packets. The key problem of this
naive scheme is that the correlation factor between the
packets sampled by neighboring routers is only p, i.e.,
\what R1 has sampled" only matches p (percentage)
of \what R2 has sampled". We propose a novel sam-
pling scheme that improves this correlation factor to
over 50% with the same sampling rate p at every router,
therefore reaching the O( 1
p ) asymptotic lower bound.
We will describe this scheme in the next section.
One may say that there is a scheme that achieves
the correlation factor of 100%, by asking all routers
on the same path to sample the same set of pack-