 0.65
 0.6
RPS - Balanced Partition, max depth = 10
RPS - Fixed Partition, max depth = 10
RPS - Balanced Partition, max depth = 50
RPS - Fixed Partition, max depth = 50
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5
Epsilon
(b) Noisy Stopping Condition
(c) Partition Method
Figure 2: Similarity of tuples within a region under diﬀerent instantiations of RPS that diﬀer on ǫ, depth,
stopping count, and partition method. Larger values are better
the EMD is calculated as follows:
EMD(X, Y ) =s 1
n Xi
(Xi − Yi)2
where Xi maps to Yi such that the overall amount of work is
minimized. The EMD was used by Li et al. [21] in evaluating
the utility of anonymized datasets.
The results for this evaluation are shown in Figure 1. We
evaluate the eﬃcacy of RPS by changing diﬀerent parame-
ters: the stopping condition including maximum allowable
partition depth and a noisy stopping condition, and the pri-
vacy parameter (ǫ).
In Figure 1(a), we ﬁx the stopping
condition to a noisy count ≤ 5, and we vary the maximum
allowable depth from 5 to 100. We test this under values of
epsilon ranging from 0.01 to 5. In Figure 1(b), we ﬁx the
maximum depth to 50 and vary the stopping count.
We can conclude the following from the results. First, the
choice of depth matters. A depth which is too shallow (ex: 5)
gives bad results. This is because the algorithm terminates
while the partitions are coarse grained. Similarly, a depth
which is too large (ex: 100) also gives bad results under
small ǫ. This is because the privacy budget allocated to
individual queries would be very small. For the dataset at
hand, a depth of 50 gives the best results achieving an EMD
close to 0. A depth of 10 or 20 gives slightly worst results.
Similar reasoning can be applied to the stopping condition,
although the diﬀerences in the results are less noticeable.
A small noisy count (3) and a large noisy count (10) give
slightly worst results for smaller values of ǫ. Finally, an
interesting thing to note here is that changes in parameters
are most noticeable under small values of ǫ. As ǫ gets larger,
the accuracy stabilizes under the diﬀerent changes and the
variance in the results decreases.
The next thing we evaluate is the ability of the RPS algo-
rithm to respect clustering inherent in a multidimensional
dataset. The reasoning behind this evaluation is as follows:
RPS generates a set of regions, where each region would
contain a set of tuples. Ideally, we want tuples in the same
cluster to be in the same region. Hence, we created a 5-
dimensional dataset in which tuples are generated by three
diﬀerent processes (i.e. there are three clusters of tuples).
Each tuple was sampled from one of three diﬀerent mul-
tivariate Gaussian distributions. These distributions were
chosen to have diﬀerent means and diﬀerent covariance ma-
trices. We then applied diﬀerent instantiations of the RPS
framework and assessed the similarity of the tuples in each
region. We labelled each region with the generator that
created the majority of the tuples in that region. We then
calculated the percentage such tuples hold over all the tuples
in the region and aggregated the results over all regions.
We present the results in Figure 2. Again, we instanti-
ated the RPS framework with diﬀerent parameters as we did
above. Figure 2(a) varies the maximum depth of the par-
titions while ﬁxing the noisy stopping count to 5. It again
shows that too small or too large of a maximum depth hin-
ders utility. Figure 2(b) ﬁxes maximum depth at 10 and
varies the noisy stopping count. As the value of ǫ increases,
the diﬀerence in this condition is negligible. Having a larger
count gives slightly worst utility. A smaller noisy count also
gives worst utility at smaller values of ǫ. In general, setting
the noisy count to 5 works well. Figure 2(c) ﬁxes the noisy
count at 5 and varies the partition method under two dif-
ferent settings of maximum depth. We compare a method
of partitioning that uses the median (via the exponential
method) and a method that just takes the midpoint of the
boundary interval as the splitting value. The results favor
the partition method that uses the median. This is because
it more accurately reﬂects how data is clustered, while taking
the mean introduces some errors into how the ﬁnal regions
are deﬁned.
4.2 Results on the Adult Dataset
We also evaluate the eﬀectiveness of our algorithm on
the UCI Machine Learning Adult dataset [1]. The Adult
dataset is a general census dataset that is commonly used
to predict the income of an individual (either above 50K
or below 50K) by taking into account several features such
as level of education, sector of industry, country, age, etc.
The dataset has 30,162 records that do not contain miss-
ing values. We consider 11 attributes for each record: age,
workclass, education, marital-status, occupation, relation-
ship, race, sex, hours-per-week, native-country, and salary
(50K).
We evaluate the dataset by applying data mining clas-
siﬁcation algorithms: the Naive Bayes algorithm and the
C4.5 decision tree algorithm. This approach has several ad-
vantages. First, it allows us to evaluate a dataset for the
purpose it is intended to serve. Hence, we can evaluate util-
ity objectively. Second, it allows us to evaluate the eﬃcacy
of our algorithm in maintaining the relationships between
attributes. Classiﬁcation algorithms take into account the
relationships between attributes in building the classiﬁca-
tion model. The more these relationships are intact, the
better the results compared to the original data. To evalu-
ate the accuracy of our results, we performed (k = 5)-fold
cross validation. This is a standard technique in data min-
ing evaluation which partitions the dataset into k folds, then
repeats the algorithm k times, each time keeping one fold
for testing and using the others as the learning dataset.
To partition the dataset in RPS, we consider integer gran-
ularity for continuous attributes. For each discrete attribute
domain, we consider an order over the values in the domain.
We use orders that are already available when applicable (ex:
for education) and generate a random order otherwise (ex:
for native country). We furthermore tune the algorithm to
the speciﬁc utility guarantees needed; i.e. the classiﬁcation
task with a binary class attribute. In this case, we want the
partitions over the median to separate the diﬀerent values of
the class attributes in the partitions. We modify the quality
function to support that. Suppose that the dataset being
anonymized has a class attribute that can take two values:
+ and −. Further suppose that in a partition (R1, R2) ∈ R,
the number of + and − in R1 are x1 and x2 and that the
numbers in R2 are y1 and y2. We can specify the quality
function as:
q(d, (r1, r2)) =
n − |r1 − r2| + max (x1 + y2, y1 + x2)
2
(3)
When summarizing the resulting partitions, we can choose a
class attribute for each partitions by taking a noisy majority
vote.
The results of the anonymization are in Figure 3. In these
experiments, the attribute on which to partition is chosen
randomly at each step. We vary ǫ and measure the clas-
siﬁcation accuracy under two stopping conditions: a noisy
count of 5 and continuing to a maximum depth of 50 (i.e.
stop at a count of 0). To evaluate our results we use three
benchmarks. The ﬁrst is the accuracy of the classiﬁcation
algorithm on the original data. We want to achieve an ac-
curacy closest to this. The second is the baseline accuracy
for the dataset. This is the accuracy if we take a simple
majority vote for the class label. For the Adult dataset,
this is 0.75. Finally, we compare the result with the original
Mondrian algorithm.
In Figure 3(a) we can see that the RPS algorithm is able
to achieve an accuracy close to that of the Naive Bayes al-
gorithm. Since this algorithm relies on conditional probabil-
ities given the class labels, we can assert that our algorithm
largely maintains the correlation between attributes and the
class label. Furthermore, we can see that RPS at a noisy
count of 5 performs better than continually recursing to the
maximum depth. This is primarily because the latter might
result in empty or sparse regions which would add noise to
the summarization. In addition, our algorithm surprisingly
outperformed the Mondrian algorithm for k-anonymity. Our
intuition is that this is because of the way Mondrian per-
forms the partitioning. Mondrian gets the actual median
without considering the class label. Our algorithms favor
an even split while keeping the class label in mind.
In Figure 3(b), we run the C4.5 classiﬁer on the datasets.
For this classiﬁer, our algorithm performed slightly worse
compared to the previous result. This may be because the
C4.5 classiﬁer is more sensitive to changes in attribute dis-
tributions than the pervious algorithm.
5. ANONYMIZING GRAPH PROPERTIES
We extend our approach to show how to eﬃciently
anonymize other types of data. The problem of anonymizing
graph data has been motivated by Backstorm, Dwork et al.
[2, 8]. In this section, we focus on anonymizing properties
of the graph that can be used to regenerate an anonymized
version of the graph itself.
In particular, we look at the
problem of anonymizing the degree sequence of the graph
while protecting individual nodes in the graph.
The problem of releasing the degree sequence has been
analyzed by Hay et al. [16]. The main contribution of Hay
et al.’s method relies on viewing the degree sequence query
as a series of queries returning the ith largest degree in the
graph. Since the query returns the degree distribution in
sorted order, the sensitivity of the query as a whole due to
the removal of one edge is 2. Analogously, the sensitivity
due to removing up to Λ edges is 2Λ. Hence, noise can be
added to each degree using the Laplacian method propor-
tional to the sensitivity. In addition, Hay et al. leverage the
fact that the unanonymized degrees are sorted in order to
improve on the accuracy of the anonymized result using a
constrained inference algorithm. This is currently the only
known method of releasing the degree sequence while satis-
fying diﬀerential privacy.
This method, however, has focused on protecting the indi-
vidual edges in the graph rather than nodes themselves since
the latter requires the addition of an unacceptably large
amount of noise using the Laplacian mechanism. Edge dif-
ferential privacy, however, is insuﬃcient. Ideally, one needs
to prevent the re-identiﬁcation of nodes, rather than whether
there is a relationship between two nodes. Here, we show
how to answer the degree sequence query eﬃciently and ac-
curately while satisfying node-diﬀerential privacy. We ex-
perimentally compare our approach to this current state of
the art.
5.1 Anonymizing the Degree Sequence
More formally, the anonymization problem we consider
can be deﬁned as follows. Given a graph Gn
Λ with n nodes
such that the maximum degree of any node is Λ, we release
a sorted sequence of size n which corresponds to the de-
gree sequence of the graph. Generally, for a graph of size n,
Λ = n − 1. However, in some publishing scenarios is it rea-
sonable to assume that the maximum degree of the graph is
known. For instance, the data publisher may a priori sample
the graph or prune nodes that exceed a particular degree.
Alternatively, one can obtain a diﬀerentially private estimate
of the maximum degree and use that in the anonymization.
In essence, if Λ = n − 1 is much larger than the maximum
degree of the graph, then the anonymized degree distribu-
tion is likely to be biased at the tail since the highest degree
nodes will be over generalized. Since this is generally the
case, we might want to exploit diﬀerent methods of sum-
marizing the resulting graph partitions. Furthermore, the
choice of Λ will have the greatest eﬀect on only one parti-
tion in the sequence; this might result in an overestimation
of the degrees in that partition. We can slightly account for
this bias by deterministically choosing the lower boundary
of the domain for each region. Our experimental evaluation
that follows, show that graph datasets can beneﬁt from this
type of summarization.
Partitioning Method To partition, we have to adjust
 2
 3
 4
 5
k
 6
 7
 8
 9
 10
RPS, stop 5
RPS, stop 0
Original
Mondrian, (vs. k)
y
c
a
r
u
c
c
A
 0.9
 0.88
 0.86
 0.84
 0.82
 0.8
 0.78
 0.76
y
c
a
r
u
c
c
A
 0.9
 0.88
 0.86
 0.84
 0.82
 0.8
 0.78
 0.76
 2
 3
 4
 5
k
 6
 7
 8
 9
 10
RPS, stop 5
RPS, stop 0
Original
Mondrian, (vs. k)
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
Epsilon
(a) Na¨ıve Bayes Classiﬁer
Epsilon
(b) C4.5 Classiﬁer
Figure 3: Classiﬁcation Accuracy for the Adult Dataset - Accuracy is expressed as a percentage. The larger the value
the better
the quality function to account for the increased sensitivity
of graph queries. We consider the case where we need to
protect individual nodes in the graph. Removing one node
will change up to Λ + 1 entries in the degree sequence: the
degree of the node itself will be set to 0, and the degrees
of all its neighbors will decrease by 1. Thus far, this has
been a hurdle preventing development of accurate dissemi-
nation algorithms for graph queries. The higher sensitivity
of the degree sequence query still manifests itself in the RPS
algorithm; however, the speciﬁc mechanism by which the al-
gorithm partitions the sequence and summarizes the results
helps minimize the eﬀect of the increased sensitivity. That
is, since we do not add noise to the individual degrees, the
negative eﬀects of the increased sensitivity would not be sub-
stantial. We would only need to sacriﬁce some accuracy in
choosing the partitions.
We account for this by changing the quality function that
determines an even split to decrease its sensitivity. We there-
fore deﬁne the quality of the partition (R1, R2) ∈ R in as
follows:
q(d, (r1, r2)) =
r1 + r2 − |r1 − r2|
4n + 2
(4)
where r1 and r2 are the sizes of R1 and R2 respectively and
n is the total number of nodes in the graph.
Removing one node will change the degrees of up to Λ + 1
nodes in the sequence. This can cause up to Λ + 1 nodes
to shift in the partitions (i.e. move from R1 to R2, or vice-
versa). Hence ∆q for the degree sequence query is:
r1 + r2 − |r1 − r2|
−
r1 + r2 − 1 − (|r1 − r2| − 2(Λ + 1))
4n + 2