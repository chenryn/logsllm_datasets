complex eigendecomposition. However, it is possible to incre-
mentally update the eigendecomposition, reducing complexity.
These techniques operate well when there is correlation be-
tween attributes which can lead to a larger dimensionality
reduction. The statistical
techniques, [60], [65]–[67], [72],
[75], [76], also have lower complexity and are able to be
updated incrementally. Some techniques also have parameter
optimization. A drawback of the schemes is their operation
on univariate data sets. Therefore they are applicable to
applications where the required information can be learned
from one attribute.
B. Change Detection
It can be seen from Table I that in most anomaly detection
techniques designed for WSNs an update to the model is
performed at regular intervals using a constant update. In
a non-stationary environment, it is necessary to update the
model to take into account the new characteristics of the data.
However, in the resource constrained environment of a WSN
it is essential to perform this energy costly task the minimum
number of times possible. Techniques that are performing
an update to a model when there is no change in the data
distribution, and consequently no change in the model, cause
wasted computational resources.
Another issue with the constant update approach is the
setting of the temporal scale at which the update occurs.
If the temporal scale of the update differs from that of
the change in the data distribution, suboptimal performance
occurs. If the timescale of change is greater than the update
interval, model updates will be performed needlessly. If the
converse occurs, and change in the data distribution occurs
more frequently, the model will be constructed from data from
multiple distributions.
There are approaches that are able to detect changes in a
non-stationary data distribution that have been surveyed, for
example [47], [48], [55], [57]. The techniques used could
be exploited by other anomaly detection algorithms in order
to match model updates to changes in the distribution. In
addition, techniques such as CUSUM [82] and KL divergence
metric [68] aim to identify when a data distribution changes.
These techniques can be computationally expensive and re-
search has been performed on reducing them to operate in
more constrained environments [83]. These can be added to
many anomaly detection algorithms so that a model update can
then be triggered allowing a more intelligent update schedule
to occur.
Change detection is particularly important if the compu-
tational complexity of model construction is high. Methods
such as the QSSVM have a model construction of O(n3), a
reduction in the number of models constructed can reduce the
computational resource use on sensor nodes. Another resource
use associated with a model update is the communication
of data related to the model to other sensor nodes. Com-
munication is the most energy resource intensive operation,
therefore using change detection to reduce the number of
communications required can have a signiﬁcant impact on the
lifetime of nodes in a WSN.
C. Model Selection
Model selection can be seen as a method of adapting to a
non-stationary distribution as the optimal parameters for one
distribution may differ from that of another. However, Table
I illustrates that some current research omits model selection
instead opting for a ﬁxed model where the parameters are
set prior to implementation. Evaluation of techniques on
data sets shows that the performance can vary signiﬁcantly
depending on the value of the parameters chosen. Supervised
machine learning problems often use cross-validation in order
to optimize parameters. Unsupervised anomaly detection has
no equivalent of cross-validation due to the unlabelled nature
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.O’REILLY et al.: ANOMALY DETECTION IN WIRELESS SENSOR NETWORKS IN A NON-STATIONARY ENVIRONMENT
17
COMPARISON OF THE COMPLEXITY AND ACCURACY OF VARIOUS ANOMALY DETECTION SCHEMES IN NON-STATIONARY ENVIRONMENTS. DEFINITIONS:
n = NUMBER OF DATA VECTORS, n1 (cid:4) n IN [63], d = DATA DIMENSION, q = NUMBER OF NEIGBOURS/NODES IN CLUSTER, b = NUMBER OF HISTOGRAM
BINS, p = DATA CORRELATION, B = NUMBER OF PCS, 1 REAL-WORLD WSN IN CRETE, GREECE.
TABLE II
Citation
[57]
[65]
[31]
[47]
[55]
[69]
[76]
[77]
[63]
[60]
[33]
[34]
[75]
[66], [67]
Scheme
Hyperellipses
Histogram
QSSVM
Adaptive QSSVM
Adaptive CESVM
Distributed PCA
Kullback-Leibler
Incremental PCA
Hyper-grid k-NN
Statistical
PCA
Cluster Node
O(nd2)
O(n3 + n2)
O(n3 + n)
O(nd2)
O(n2d)
-
O(B3)
O(nlog(n))
O(nd(p + q))
-
-
Adaptive ν QSSVM O(n3 + n2)
Statistical
Statistical
-
O(log2b) + O(b)
Cluster Head
-
-
-
-
-
O(d3log
q
3)
Init.:O(q(2n + 6b) Update:O(q)
-
O(n1log(n1))
-
-
-
-
-
Memory
O(d)
-
O(nd + n)
O(nd)
O(nd)
O(d2)
-
-
-
-
O(1)
O(d)
O(d2)
O(qd2)
O(n)
-
O(n)
O(nd)
O(bd + log(n))
O(nd)
-
-
-
-
-
-
-
-
Communication Data Set
Synthetic
Synthetic
IBRL
IBRL
IBRL
GDI
GSB
FPR %
≈ 3
0 − 40
≈ 10
≈ 3
≈ 0
3 − 6
2 − 8
0 − 15
1 − 9
IBRL
2 − 15
GSB
0 − 30
WSN 1
0 − 4
Synthetic
Synthetic 0 − 100 (FNR) 0 − 11
10 − 35
IBRL
TPR %
84 − 96
70 − 95
≈ 90
≈ 90
≈ 93
87 − 100
85 − 95
80 − 100
75 − 100
70 − 100
85 − 95
85 − 100
80 − 90
NAMOS
of the data. However, certain techniques surveyed have per-
formed parameter optimization without the use of labelled data
and evaluations show that this improves performance.
A drawback of parameter optimization is that it introduces
additional computational complexity on a sensor node. There
is the possibility of exploiting the spatial-temporal correlation
of data to distribute model selection amongst a set of nodes
that share the same distribution of data measurements. Certain
techniques [33], [76] include the forming of a cluster of sensor
nodes with a similar data distribution. By distributing the
computational complexity between nodes, the computational
complexity on a single node is reduced.
Due to the unattended nature of WSNs it is necessary for
sensor nodes to be able to adapt parameters if there is a change
in the data distribution. Self-optimization of an anomaly
detection technique can provide signiﬁcant performance gains.
D. Model Update
Batch update to a model is the most common method to
reconstruct a model when new data arrive that needs to be
included. This is performed by the majority of the techniques
surveyed. A batch update using a ﬁxed sliding window can be
computationally expensive as the previous model is discarded
and a new model in its entirety is constructed.
Sliding windows are a common method to frame a training
set for model construction, and to provide rudimentary adap-
tation to a non-stationary distribution. However, the size of
the sliding window represents the accuracy/adaptation trade-
off and ﬁxed sized windows are unable to adapt this trade-off if
the temporal scale of change in the non-stationary distribution
alters. Most techniques surveyed in this paper use a ﬁxed
sliding window.
In addition to the accuracy/adaptation trade-off, the size of
the window impacts on the anomalies that are detected. Beigi
et al. [66] studied anomaly detection at multiple temporal
resolutions and showed how different outliers, and different
events, were detected at different temporal scales. By altering
the size of the sliding window by factors of approximately two,
different outliers were detected in the different windows. This
indicates the importance of the size of the sliding window
and the impact on the type of anomaly that
is detected.
Determining the size of the sliding window in an adaptive
and optimized manner can increase the performance of the
technique.
Incremental updates and downdates can provide a perfor-
mance improvement as they require less computational re-
sources in order to incorporate new data items into the model.
We can view an incremental update as reusing computation
conducted previously, in order to reduce the computational
complexity of the update to the model. However, not all
techniques are able to be reformulated to operate in an
incremental/decremental manner and the computational cost
does vary between different techniques.
An additional advantage of incremental/decremental tech-
niques is that with certain techniques the model becomes the
repository for the characteristics of the data and the data set
can be dispensed with. This is advantageous in that there is no
longer a requirement to store the data set and the algorithms
can be considered to conduct only one-pass of the data to form
the model. Both storage and memory resources are saved by
this technique.
E. Future Directions
This survey has identiﬁed the elements that are required
in order for an anomaly detection algorithm to operate in
a non-stationary environment. Current research in anomaly
detection in WSNs focuses on designing algorithms that create
anomaly classiﬁers for stationary data sets, with less focus be-
ing applied to operating within a non-stationary environment.
Some research has tackled the problem of a non-stationary
environment and the design of algorithms that are able to adapt
to changes in the data distribution.
To enable anomaly detection techniques to operate opti-
mally in a non-stationary environment it is necessary that the
algorithms are able to adapt to non-stationary distributions, are
self-optimizing and update models efﬁciently. To this end, we
recommend that future research includes the following areas:
(cid:129) Application of change detection schemes that detect and
retrain in a distributed environment in WSNs. This will
allow control of the computationally complex model
update. Performing a model update only when it has been
determined it is necessary will increase efﬁciency.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.18
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
(cid:129) Investigation of the temporal scale of anomaly detection.
Identiﬁcation of the temporal scale which is required
and the adaptation of the temporal scale of the anomaly
detection scheme.
(cid:129) Adaptive sliding windows to replace ﬁxed sliding win-
dows. It has been shown that the temporal scale of the
sliding window is important. Determining the correct
scale for the sliding window will increase the accuracy
of the anomaly detector.
(cid:129) Incremental updates and downdates to reduce computa-
tional complexity. Using the current model as an interim
step in the computation of the next model will reduce
computational complexity.
(cid:129) Model selection through optimization of parameters. De-
termining the optimal parameters for the current training
set can increase accuracy.
(cid:129) Distributed model selection by exploiting the spatial-
temporal correlation of data. Using nodes with a similar
data distribution to share the computational complexity of
determining the optimal parameters for the current data
distribution.
(cid:129) Incremental model selection. Extending the idea of an
incremental model update to incremental model selection
where the previous optimal model is used as a basis for
construction of the new optimal model in order to reduce
resource consumption.
IX. CONCLUSION
In this survey the problem of anomaly detection in wireless
sensor networks in non-stationary environments was pre-
sented. A taxonomy that describes the parts of the process
that allows a technique to detect and adapt to a non-stationary
distribution was provided. A workﬂow illustrated their op-
eration within the implementation of an anomaly detection
technique in a WSN. A table provided a comparison between
the surveyed anomaly detection techniques and how they adapt
to a non-stationary distribution. An additional table compared
the complexity and accuracy of the surveyed techniques.
Anomaly detection techniques that operate in a WSN are
required to do so unattended and in an environment where
the data distribution may be non-stationary. Techniques can
perform more optimally in this environment if they are able
to adapt to the environment they are operating in, rather than
using ﬁxed models. Techniques can improve performance if
they are designed to monitor data to detect for changes in
the data distribution in order to trigger a model update. When
it is determined that a model update is required, optimizing
parameters to the current data set, and updating models in an
incremental manner can further enable a model to perform
optimally and efﬁciently for the current data distribution.
Existing techniques that operate in an environment with a
non-stationary distribution implement some of the techniques
that allow adaptation. Future research should address issues
outlined in the survey so that model construction occurs only