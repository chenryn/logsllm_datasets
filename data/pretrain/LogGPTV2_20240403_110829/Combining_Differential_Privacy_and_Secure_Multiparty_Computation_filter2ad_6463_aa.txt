title:Combining Differential Privacy and Secure Multiparty Computation
author:Martin Pettai and
Peeter Laud
Combining Diﬀerential Privacy and Secure
Multiparty Computation
Martin Pettai1,2, Peeter Laud1
Cybernetica AS1, STACC2
PI:EMAIL, PI:EMAIL
Abstract We consider how to perform privacy-preserving analyses on
private data from diﬀerent data providers and containing personal in-
formation of many diﬀerent individuals. We combine diﬀerential privacy
and secret sharing in the same system to protect the privacy of both the
data providers and the individuals. We have implemented a prototype
of this combination and the overhead of adding diﬀerential privacy to
secret sharing is small enough to be usable in practice.
1
Introduction
Many organizations maintain registries that contain private data on the same
individuals. Important insights might be gained by these organizations, or by the
society, if the data in these registries could be combined and analyzed. The exe-
cution of such combination and analysis brings several kinds of privacy problems
with it. One of them is computational privacy — one must perform computations
on data that must be kept private and there is no single entity that is allowed
to see the entire dataset on which the analysis is run. Another issue is output
privacy — it is not a priori clear whether the analysis results contain sensitive
information tracable back to particular individuals. Kamm [20, Sec. 6.7.1] has
presented evidence that the second kind of issues is no less serious than the ﬁrst
kind — even after the computational privacy of data in a study was ensured,
one of the data providers (the tax oﬃce) was worried about the leaks through
the results of the study.
Secure Multiparty Computation (SMC) [33, 16] is a possible method for
ensuring the computational privacy of a study. It allows the entity executing
the study to be replaced with several entities that perform the computations
in distributed manner, while each of them alone or in small coalitions remains
oblivious to the input data and the intermediate results. To achieve output
privacy, the analysis mechanism itself must be designed with privacy in mind [25].
A commonly targeted privacy property is diﬀerential privacy (DP) [11, 28], which
has both well-understood properties [19] and supports simple arguments, due to
its composability.
A statistical analysis mechanism can be designed from ground up with dif-
ferential privacy in mind. Alternatively, a mechanism, treated as a black-box
functionality, can be modiﬁed to make it diﬀerentially private, albeit with some
loss of accuracy. Laplace and exponential mechanisms are basic tools to add dif-
ferential privacy to a suﬃciently smooth mechanism [11]. For many statistical
functions, other mechanisms may provide better accuracy for the same level of
privacy. In this paper, we will consider the sample-and-aggregate method [29],
smoothening the function, such that less noise has to be added in order to obtain
the same level of privacy.
Diﬀerential privacy introduces a generic mechanism for responding to queries
about a database containing private data. The mechanism for answering each
query has a certain level of privacy loss associated with it; for diﬀerent queries
the levels may be diﬀerent. There is an overall privacy budget associated with
the database. Each given response lowers the available budget by the amount of
privacy loss of the mechanism used to construct it. A query is accepted only if
its privacy loss does not exceed the remaining budget.
Diﬀerential privacy has been generalized to take into account that diﬀerent
records or columns of a database may have diﬀerent sensitivity [7]. As an instance
of generalized deﬁnitions, personalized diﬀerential privacy (PDP) [13] assigns a
privacy budget not to the entire database, but to each record separately. The
responses to queries may depend only on a subset of all records, and only the
budgets of these records will be lowered. This gives more freedom to the data
analyst in formulating the queries.
In real applications and database systems, we would like to use both SMC
and DP, in order to achieve both computational and output privacy, and to
protect the interests of both data owners and data subjects. Alone, both of them
have been demonstrated to work with reasonable eﬃciency, being applicable for
realistically sized databases.
In this paper, our main contribution is to show that fast methods for
SMC and precise methods for DP can be combined, and still provide reasonable
performance. We report on the experience that we have obtained with the imple-
mentations of GUPT’s sample-and-aggregate method [28] and the provenance
for PDP method [13] on top of the Sharemind SMC framework. We have imple-
mented a number of statistical functions in this framework, and compared their
performance with and without DP mechanisms. Our results show that the extra
overhead of implementing DP mechanisms on top of SMC is not prohibitive.
While implementing the DP mechanisms on top of the SMC framework, we
had to come up with novel SMC protocols for some subtasks. These subtasks are
related to reading the elements of an array according to private indices; there
are no cheap SMC protocols for that and hence the algorithms with a lot of
data-dependent accesses incur very signiﬁcant overhead when straightforwardly
converted to run on top of an SMC framework. The subtasks with novel protocols
are the following:
– Inner join of two tables. This operation is needed when tracking the prove-
nance of records in queries complying with PDP.
– Counting the number of equal values in an array. This operation is needed
for updating the privacy budgets of records. Also, a novel form of updating
the elements of an array (where the writing locations are private) is needed
for writing back the updated budgets.
The Sharemind SMC framework [4, 5] that we are using employs protocols
working on data secret-shared between several computing parties. This gives
our databases and queries the greatest ﬂexibility, as we do not have to restrict
how the database is stored, and who can make the queries. Indeed, we can
just state that the database is secret-shared between the computing parties,
and so are the parameters of the query. The answer will be similarly secret-
shared. In an actual deployment, the entity making the query will share it among
the computing parties. The shares of the answer will be sent back to it, to be
recombined. Alternatively, the answer to a query may be an input to further
secure computations. In this case it will not be recombined and no party will
ever learn it.
2 Related Work
PINQ [26] is one of the best-known implementations of diﬀerentially private
queries against sensitive datasets. It provides an API for performing such queries,
maintaining and updating the privacy budget for a queryable data source.
Rmind [3] is a tool for statistical analysis, preserving computational privacy.
It implements a number of statistical operations and functionalities on top of
Sharemind, including quantiles, covariance, detection of outliers, various sta-
tistical tests and linear regression. The implementations are packaged as a tool
resembling the statistics package R.
The combination of SMC and DP has been explored in PrivaDA [14]. They
consider the problem of releasing aggregated data collected from many sources.
The aggregation (only addition of individual values is considered) is performed
using SMC. Afterwards, a perturbation mechanism providing DP is applied to
the aggregated data and the perturbed data is made public.
Diﬀerentially private data aggregation has received a lot of interest [12, 1, 18],
but typically without employing SMC. In this case, each data source itself has
to add some noise to its outputs, which will be summed during aggregation.
Typically, the noise level in the aggregated result will be larger, compared to
adding the noise after aggregation.
Privacy-preserving joins in databases have been considered before in [23].
They obliviously apply a pseudo-random permutation on the key columns of
both joined tables, and declassify the results, after which the actual join can be
performed in public. Their approach leaks the counts of records that have equal
keys.
The reading and updating of arrays according to private indices has been
considered before, mostly by implementing the techniques of Oblivious RAM [17]
on top of SMC [9, 24]. Our methods have similarities to parallel oblivious array
access [22], but diﬀerently from them, we do not have to perform computations
on stored values.
In our implementation, we had to choose which aggregation functions to
implement for queries. We concentrated on count, arithmetic average, median,
and linear correlation coeﬃcient. Count has been considered in [11, 26], median
in [29, 26]. Arithmetic average has been considered in [26] and is used as the ﬁnal
step in the Sample-and-Aggregate algorithm in [28]. Linear correlation coeﬃcient
we chose as an example of a multivariable aggregation function. We have also
implemented sum, which is considered in [1], but we do not consider it further
in this paper because it is similar to the average.
An alternative to the Sample-and-Aggregate mechanism is the exponential
mechanism [27], as an example of which we have implemented a diﬀerentially
private quantile computation algorithm from [32]. We have not optimized it and
thus we do not consider it further in this paper.
3 Diﬀerential Privacy
Deﬁnition 1. A (probabilistic) algorithm that takes a set of data records as an
input, is -diﬀerentially private if the removal of any single record from the input
changes the probability of any predicate on the output being true at most by a
factor of e.
To achieve diﬀerential privacy, one usually adds random noise to the computed
result. This noise must have high enough variance to mask the possible variation
in the output due to removing or changing one record. This noise is usually from
a Laplace distribution, which ﬁts perfectly to the statement of Def. 1.
An alternative would be to use noise from the uniform distribution. This
would not satisfy Def. 1 but would instead satisfy the following property:
Deﬁnition 2. A (probabilistic) algorithm that takes a set of data records as an
input, is additively δ-diﬀerentially private if the removal of any single record
from the input changes the probability of any predicate on the output being true
at most by δ.
This has the advantage that the amount of noise is bounded and the average
absolute deviation is half of that needed for the Laplace noise, to achieve the same
level of indistinguishability for the adversary (a probability of 1
2 may change to at
most 1
2 +δ by changing one input record). The disadvantage is that a probability
of 0 may change to δ by changing one record, which is not possible with Laplace
noise. Thus in the following, we will consider only Laplace noise and Def. 1,
which is the mainstream practice.
Suppose we have an algorithm for computing a function f , whose input is
a set of data records. To determine how much noise must be added we need to
know the sensitivity of f , i.e. how much the value of f can change if we remove
one record from its input.
Deﬁnition 3. The sensitivity of a function f : 2Records → R is
max
T⊆Records,r∈T
|f (T ) − f (T \ {r})|
If the sensitivity of the function is s then adding noise from the distribution
 ) (this has the average absolute deviation s
Laplace( s
 and standard deviation
√
s
2
 ) to the value of f guarantees -diﬀerential privacy.
For example, if f is the arithmetic mean of n values from the range [a, b] then
its sensitivity is b−a
n . Here it is important that the inputs of f are bounded, i.e.
in the range [a, b] for some a and b, otherwise the sensitivity of f would also be
unbounded.
In practical analysis, the input values (e.g. salaries) may be from a very
wide range. Because the data is private, we may not know the maximum and
the minimum of the values. Revealing the exact minimum and the maximum
would breach the privacy of the individuals who have those values. Thus we
need to guess some values a and b and then clip the input values to the range
[a, b] (replacing values smaller than a with a, and values larger than b with b).
The range [a, b] must be chosen carefully. If it is too wide, then the added noise
(which is proportional to b − a) distorts the result too much. If it is too narrow,
then the clipping of inputs distorts the result too much.
the composition of the queries is ((cid:80) i)-diﬀerentially private. We can deﬁne a
(global) privacy budget B and require(cid:80) i ≤ B. Thus every query consumes a
If several queries are made where the ith query is i-diﬀerentially private then
part of the privacy budget and when a query has a higher  than the amount
of budget remaining then this query cannot be executed or the accuracy will be
reduced.
4 The Sample-and-Aggregate Mechanism
Let us have a dataset T that can be interpreted as the result of |T| times sampling
a probability distribution D over Records (diﬀerent samples are independent
of each other). By processing T , we want to learn some statistical characteristic
f (D) — a vector of values — of the distribution D. We have two conﬂicting
goals — we want to learn this characteristic as precisely as possible, but at the
same time we want our processing to be -diﬀerentially private.
A robust method for diﬀerentially privately computing the function f is
the Sample-and-Aggregate mechanism proposed and investigated by Nissim et
al. [29] and Smith [32], and further reﬁned in the GUPT framework [28]. The
basic mechanism is given in Alg. 1. Beside the dataset T and the privacy pa-
rameter , Alg. 1 receives as an input a subroutine for computing the function
f (without privacy considerations). This subroutine is called by Alg. 1 (cid:96) times
in a black-box manner.
Alg. 1 is clearly diﬀerentially private due to the noise added at the end. At
the same time, Smith [32] shows that if f is generically asymptotically normal,
then the output distribution of Alg. 1, and the output distribution of f on the
same dataset T converge to the same distribution as the size n of T grows (and
(cid:96) grows with it). The convergence holds even if the output dimensionality and
clipping range of f , as well as 1/ε grow together with n, as long as the growth
Algorithm 1 The Sample-and-Aggregate algorithm [28]
Input: Dataset T , length of the dataset n, number of blocks (cid:96), privacy parameter ,
clipping range [left, right]
Randomly partition T into (cid:96) disjoint subsets T1, . . . , T(cid:96) of (almost) equal size
for i ∈ {1, . . . , (cid:96)} do
Oi ← output of the black box on dataset Ti
if Oi  right then Oi ← right
(cid:16) right−left
(cid:17)
(cid:80)(cid:96)
return 1
(cid:96)
i=1 Oi + Laplace
(cid:96)·
is at most polynomial. A statistic is generically asymptotically normal, if its
moments are suﬃciently bounded; we refer to [32] for the precise deﬁnition.
As an example of the Sample-and-Aggregate algorithm, we can compute dif-
ferentially privately the linear correlation coeﬃcient. The black box in this case
takes a dataset as input and computes the non-diﬀerentially private linear cor-
relation coeﬃcient of the dataset. We can compute other functions diﬀerentially
privately by just replacing the black box.
The optimal value of (cid:96) may be diﬀerent for diﬀerent functions f . For com-
puting the arithmetic mean, we can take (cid:96) = n and use one-element blocks with
the identity function as the black box. This gives the highest accuracy because
the amount of added noise is inversely proportional to (cid:96). In our implementa-
tion, we use this diﬀerentially private arithmetic mean as a subroutine of the
Sample-and-Aggregate algorithm.
For computing the median, we could use the Sample-and-Aggregate algo-
rithm, with the black box returning (cid:96) elements of the dataset, close to the me-
dian. In our implementation, we improve on this by skipping the black box and
just taking the (cid:96) or (cid:96) + 1 (depending on the parities of (cid:96) and n) elements closest
to the median (i.e. the elements on positions (cid:100) n+1−(cid:96)
(cid:99) (1-based) in
the sorted order).
(cid:101) to (cid:98) n+1+(cid:96)
2
2
5 Personalized Diﬀerential Privacy
We have also implemented a mechanism for Personalized Diﬀerential Privacy
[13]. This uses a more general form of Def. 1, which we give in Def. 4.
Deﬁnition 4. Let E : Records → R. A (probabilistic) algorithm that takes a