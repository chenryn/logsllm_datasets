has been proven in Table 4. It clearly shows that Method 2 has
slightly better performance in detecting malware than Method 1.
Next, we discuss some reasons to cause the misclassifications in
our proposed methods. In Table 4, Method 1 has falsely classified
a few malicious samples as benign, that causes a false negative
rate of 0.00140. The main reason behind such misclassification is
that the number of API calls of one of these samples is small (i.e.,
seven API calls). Thus, only seven features are generated using
Method 1. In contrast, Method 2 could generate around 60 features
out of these seven API calls and correctly classify this sample as
malicious (thus, reducing the FNR to 0.00070 in Method 2). The
experimental results presented above show that our approach can
distinguish benign from malicious behavior with high accuracy.
However, our approach, like any other detection approaches, mis-
classifies a small number of samples that causes the FPR as pre-
sented in Tables 3 and 4. After studying the misclassified benign
samples, which caused the FPR in our approach, the number of
API calls of the samples is only two, where each API call has only
one argument (e.g., "__exception__:0=numB0", "NtTerminatePro-
cess:0=numB2"). Thus, both methods have only two features. Due
to such small numbers of extracted features from these samples,
our approach has misclassified them as malicious; as a result, we
get the small FPR.
4.2.3 Malware Type Classification. The goal of this set of experi-
ments is to investigate the performance of Method 1 and Method 2
in classifying the malware samples into their respective types. As
discussed before, our malicious samples belong to ten malware
types (Table 2), the same features that are used to classify the sam-
ples into malicious and benign classes are again used to classify
the malicious samples into their types. For a good validation and to
make sure that the machine learning modules are trained using a
fair amount of samples from each type, each malware type dataset
is split into training and testing datasets, which consist of 80 %
and 20 % of the samples, respectively. Thus, in total, we have 5687
malware samples for training and 1418 for testing. The training
dataset is used to train models using the same five machine learning
algorithms as before. From the experimental results in Figure 3,
XGBoost performs as the best malware type classifier out of the five
classifiers. It gives the best accuracy values (98.0253 %) for Method 1
and (97.9548 %) for Method 2.
We apply 5-fold cross-validation over the five machine learning
algorithms. Results are shown in the bottom half of Figure 4. As
shown in the figure, the accuracy is in the range between [91.9 % -
93.4 %]. Such decreasing in the accuracy values is due to the small
number of samples in some of the types (e.g., Backdoor, Virus,
Worm, PUP, Hack Tool, and Riskware). To overcome this issue, we
remove individual types which have less than 200 samples in our
model training. In other words, the experiments are done using
the samples that belong to only the following four malware types:
Trojan, Adware, Spyware, and Ransom. Again, we apply the 5-fold
cross-validation over the five machine learning algorithms using
62ACSAC 2020, December 7–11, 2020, Austin, USA
Dima Rabadi and Sin G. Teo
42, 44, 51]. As discussed in Section 2, both [44] and [10] use pat-
tern recognition techniques to find a mutual sequence of API calls
and arguments. However, the sequence (i.e., pattern) of API calls
can be disrupted by deleting and/or inserting certain API calls. In
contrast, [1, 40, 51] use API frequency-based malware detection
techniques. In [51], the frequency metric of calling specific API
calls and their arguments has been used to differentiate between be-
nign and malware samples. In [40], Yong et al., have used frequent
item-sets of API calls and their arguments for malware detection.
Similarly, Faraz et al. [1] have used statistical features of API calls
and their arguments, such as the frequency, mean, and size of pa-
rameters, to detect the malicious behavior of programs. Malware
authors can easily bypass the above mentioned frequency-based
approaches by eliminating and/or adding API calls hence altering
the frequency counter values.
Our methods are different from the studies above. (i) Our meth-
ods are resilient to malware mutation and obfuscation techniques
(e.g., reordering the sequence of API calls or repeating specific API
calls and/or arguments many times) because we do not rely on
the sequence or the pattern of the API calls and their arguments.
Instead, only the occurrence of the API calls and their arguments is
considered in our methods. (ii) The statistical features such as mean,
frequency, or the size of the API arguments, are not considered in
our approach. (iii) Domain knowledge of the complex arguments is
not required in our methods as we implement a novel feature gen-
eration functions that are used to enhance the extracted API-based
features for better processing by the machine learning algorithms.
(iv) None of the existing approaches have studied the possibility
of using each argument element of each API call separately, as
presented in Method 2. These advantages execute our approach
against the scalability issue of using high dimensional feature space,
which requires high memory consumption and increases the com-
putational complexity. We summarize the comparison between our
approach and the above-mentioned related studies in Table 5. Our
proposed methods have outperformed the state-of-the-art methods
as shown in Table 5. However, as [44] gives the highest F1 score
among the six works, we use it as a baseline. Appendix D shows the
evaluation between our proposed methods against the baseline [44]
and [51] in terms of the recall scores.
6 LIMITATIONS AND FUTURE WORK
In this paper, we have only targeted Windows 7. We plan to ex-
plore the performance of our proposed methods on other newly
released versions of Windows (i.e., Windows 10). Furthermore, An-
droid is an API-based operating system; thus, we plan to test the
performance of our methods on Android platforms. In addition, all
experiments are conducted using the Cuckoo sandbox. Therefore,
this paper is limited to the list of Cuckoo’s hooked API calls [46].
In the future, we plan to design a run-time malware analysis and
detection tool that can extract the API calls simultaneously while
the program is running. Furthermore, we have tested our methods
on a dataset of 14879 samples. To check the performance of our
methods on more massive datasets, we are collecting daily and
up-to-date malicious samples and generating daily benign samples.
In future work, we plan to leverage the existing model by consider-
ing more complicated malware classification scenarios where, for
Figure 3: Training accuracy results of malware type classifi-
cation.
Figure 4: The 5-fold cross-validation over the five machine
learning algorithms. The bottom half shows the results us-
ing the whole malicious dataset (i.e., ten malware types). The
top half shows the results using only four malware types, af-
ter removing the types that have less than 200 samples.
the four malware types. The results are shown in the top half of
Figure 4. We clearly see that the accuracy values have increased to
the range between [94.6 % - 95.5 %]. The experiments have proven
that increasing the number of the malware samples of each type can
improve the performance of Method 1 and Method 2 in classifying
malware samples into their types.
5 COMPARISON WITH STATE-OF-THE-ART
In this section, we compare our methods with other works that
consider the API arguments in their techniques (i.e., [1, 10, 40, 42,
44, 51]). Our comparison include (i) Detection accuracy (ii) Required
API information such as finding the frequency counter of calling
specific API calls or recognizing the API sequence patterns, and
(iii) Limitations.
The following studies have used the API arguments to estab-
lish malware detection and/or type classification models [1, 10, 40,
63Advanced Windows Methods on Malware Detection and Classification
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 5: Comparison with API arguments-based malware detection methods.
Approach
[10]
[44]
[42]
[51]
[40]
[1]
Method 1
Method 2
F1 (%)
97.7
97.9
96.7
97.0
94.7
96.6
99.87
99.90
Techniques
Information retrieval
theory, TF-IDF weighting
Random forest,
J48 decision tree,
Sequential Minimal
Optimization (SMO)
n-gram
Weka library, SVM,
decision table, random
forest, Instance-based
classifier (IB1)
n-gram
SMO, RIPPER, NB,
J48, IBk
SVM, XGBoost3,
RF, DT, PA
API Information
The sequence of Windows
API calls and their arguments
The sequence of API calls
including their arguments
and/or return value
Limitations
Sequence-based1
A subset of 126 API calls,
Sequence-based1
System calls and arguments
Frequency counters of API
calls with and without arguments
System calls only
Frequency-based2
Frequent item-sets of the
sequence of API calls
with and without their
arguments
Statistical features of API
calls and their arguments
(e.g., frequency, mean and
size parameters)
Method 1: API call and its
entire list of arguments,
Method 2: API call and each
of its argument element separately
Frequency-based2
Frequency-based2
Section 6
1 Sequence-based: malware authors can bypass sequence-based techniques by deleting and/or inserting API calls.
2 Frequency-based: malware authors can bypass frequency-based techniques by removing and/or adding API calls hence altering the
frequency counter value.
3 The reported F1 scores of Method 1 (99.87 %) and Method 2 (99.90 %) are based on the XGBoost classifier as it achieves the best accuracy
and F1 scores among the five machine learning algorithms.
example, one malware sample can simultaneously perform multi-
ple functionalities. Therefore, multi-labeling algorithms would be
required.
7 CONCLUSIONS
This paper explored a new direction to extract the API-based dy-
namic features by analyzing the API calls together with their list
of arguments. With the help of machine learning algorithms, we
designed two methods to detect Windows malware samples and
classify them into their types. The first method deals with the en-
tire list of arguments of each API call as one feature, whereas the
second method deals with each argument of each API call sepa-
rately as one feature. We verified the performance of the proposed
methods in malware detection using reasonable datasets of 7105
malicious samples belonging to ten distinct types and 7774 benign
samples. We showed that our approach outperforms other recent
API arguments-based malware detection approaches in terms of
accuracy, limitations, and required API information. Our experi-
mental results showed that our malware detection approach gave an
accuracy of over 99.8992 %, and outperformed the state-of-the-art.
Furthermore, we used the same methods to classify the malware
samples into their types. The experimental results showed that our
malware classification approach gave an accuracy of over 97.9548 %.
Our approach has promise for wide adoption in API-based mali-
cious behavior detection for Windows platforms. In particular, it
can meet the demands of applications that are currently served by
malware detectors that rely on extracting statistical information of
the API-based dynamic features but desire better robustness against
unfavorable sequence interruption attacks.
ACKNOWLEDGMENTS
The authors wish to thank our shepherd Dr. Kevin Alejandro Roundy,
and the anonymous reviewers for providing valuable feedback on
this work. We also thank Loo Jia Yi from the Cyber Security De-
partment at Institute for Infocomm Research for evaluating the
Machine Learning classifiers mentioned in this work.
REFERENCES
[1] Faraz Ahmed, Haider Hameed, M Zubair Shafiq, and Muddassar Farooq. 2009.
Using spatio-temporal information in API calls with machine learning algorithms
for malware detection. In Proceedings of the 2nd ACM workshop on Security and
artificial intelligence. ACM, 55–62.
[2] Mamoun Alazab, Sitalakshmi Venkataraman, and Paul Watters. 2010. Towards
understanding malware behaviour by the extraction of API calls. In 2010 Second
Cybercrime and Trustworthy Computing Workshop. IEEE, 52–59.
[3] Eslam Amer and Ivan Zelinka. 2020. A dynamic Windows malware detection
and prediction method based on contextual understanding of API call sequence.
Computers & Security 92 (2020), 101760.
64ACSAC 2020, December 7–11, 2020, Austin, USA
Dima Rabadi and Sin G. Teo
[4] Sergii Banin and Geir Olav Dyrkolbotn. 2018. Multinomial malware classification
via low-level features. Digital Investigation 26 (2018), S107–S117.
[5] Amine Boukhtouta, Serguei A Mokhov, Nour-Eddine Lakhdari, Mourad Debbabi,
and Joey Paquet. 2016. Network malware classification comparison using DPI
and flow packet headers. Journal of Computer Virology and Hacking Techniques
12, 2 (2016), 69–100.
[6] Davide Canali, Andrea Lanzi, Davide Balzarotti, Christopher Kruegel, Mihai
Christodorescu, and Engin Kirda. 2012. A quantitative study of accuracy in
system call-based malware detection. In Proceedings of the 2012 International
Symposium on Software Testing and Analysis. 122–132.
[7] Silvio Cesare and Yang Xiang. 2012. Software similarity and classification. Springer
Science & Business Media.
[8] Steve Chamberlain and Cygnus Solutions. [n.d.]. Cygwin.
https:
([n. d.]).
//cygwin.com/.
[9] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785–794.
[10] Julia Yu-Chin Cheng, Tzung-Shian Tsai, and Chu-Sing Yang. 2013. An information
retrieval approach for malware classification based on Windows API calls. In
2013 International Conference on Machine Learning and Cybernetics, Vol. 4. IEEE,
1678–1683.
[11] In Kyeom Cho and Eul Gyu Im. 2015. Extracting representative API patterns of
malware families using multiple sequence alignments. In Proceedings of the 2015
Conference on research in adaptive and convergent systems. ACM, 308–313.
[12] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram
Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning
Research 7, Mar (2006), 551–585.
[13] G DATA. [n.d.]. Malware Naming Hell Part 1: Taming the mess of AV detection
names. ([n. d.]). https://www.gdatasoftware.com/blog/2019/08/35146-taming-
the-mess-of-av-detection-names.
[14] Omid E David and Nathan S Netanyahu. 2015. Deepsign: Deep learning for
automatic malware signature generation and classification. In 2015 International
Joint Conference on Neural Networks (IJCNN). IEEE, 1–8.
[15] Manuel Egele, Theodoor Scholte, Engin Kirda, and Christopher Kruegel. 2012.
A survey on automated dynamic malware-analysis techniques and tools. ACM
computing surveys (CSUR) 44, 2 (2012), 6.
[16] Seoungyul Euh, Hyunjong Lee, Donghoon Kim, and Doosung Hwang. 2020.
Comparative Analysis of Low-Dimensional Features and Tree-Based Ensembles
for Malware Detection Systems. IEEE Access 8 (2020), 76796–76808.
[17] Sanchit Gupta, Harshit Sharma, and Sarvjeet Kaur. 2016. Malware characteriza-
tion using windows API call sequences. In International Conference on Security,
Privacy, and Applied Cryptography Engineering. Springer, 271–280.
[18] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann,
and Ian H Witten. 2009. The WEKA data mining software: an update. ACM
SIGKDD explorations newsletter 11, 1 (2009), 10–18.
[19] John T. Haller. [n.d.]. Portable Apps. ([n. d.]). https://portableapps.com/.
[20] Weijie Han, Jingfeng Xue, Yong Wang, Lu Huang, Zixiao Kong, and Limin Mao.
2019. MalDAE: Detecting and explaining malware based on correlation and
fusion of static and dynamic characteristics. Computers & Security 83 (2019),
208–233.
[21] Xiang Huang, Li Ma, Wenyin Yang, and Yong Zhong. 2020. A Method for Windows
Malware Detection Based on Deep Learning. Journal of Signal Processing Systems
(2020), 1–9.
[22] Kazuki Iwamoto and Katsumi Wasaki. 2012. Malware classification based on
extracted api sequences using static analysis. In Proceedings of the Asian Internet
Engineeering Conference. ACM, 31–38.
[23] Arzu Gorgulu Kakisim, Mert Nar, Necmettin Carkaci, and Ibrahim Sogukpinar.
2018. Analysis and evaluation of dynamic feature-based malware detection
methods. In International Conference on Security for Information Technology and
Communications. Springer, 247–258.
[24] Youngjoon Ki, Eunjin Kim, and Huy Kang Kim. 2015. A novel approach to detect
malware based on API call sequence analysis. International Journal of Distributed
Sensor Networks 11, 6 (2015), 659101.
[25] Bojan Kolosnjaji, Apostolis Zarras, George Webster, and Claudia Eckert. 2016.
Deep learning for classification of malware system call sequences. In Australasian
Joint Conference on Artificial Intelligence. Springer, 137–149.
([n. d.]). https://
[26] Malwarebytes Labs. [n.d.]. 2020 State of Malware Report.
resources.malwarebytes.com/files/2020/02/2020_State-of-Malware-Report.pdf.
[27] Malshare Labs. [n.d.]. Malshare Website. ([n. d.]). https://malshare.com/.
[28] Malwarebytes Labs. [n.d.]. Malware Types. ([n. d.]). https://www.malwarebytes.
com/malware/.
[29] Andrea Lanzi, Davide Balzarotti, Christopher Kruegel, Mihai Christodorescu, and
Engin Kirda. 2010. Accessminer: using system-centric models for malware protec-
tion. In Proceedings of the 17th ACM conference on Computer and communications
security. 399–412.
[30] VirusTotal malware intelligence services. [n.d.]. https://www.virustotal.com.
([n. d.]).
[31] Stephen Marsland. 2015. Machine learning: an algorithmic perspective. CRC press.
[32] Microsoft. [n.d.]. Overview of the Windows API. ([n. d.]). https://docs.microsoft.
com/en-us/previous-versions//aa383723(v=vs.85).
[33] Andreas Moser, Christopher Kruegel, and Engin Kirda. 2007. Limits of static
analysis for malware detection. In Twenty-Third Annual Computer Security Appli-
cations Conference (ACSAC 2007). IEEE, 421–430.
[34] Vinod P Nair, Harshit Jain, Yashwant K Golecha, Manoj Singh Gaur, and Vijay
Laxmi. 2010. Medusa: Metamorphic malware dynamic analysis usingsignature
from api. In Proceedings of the 3rd International Conference on Security of Infor-