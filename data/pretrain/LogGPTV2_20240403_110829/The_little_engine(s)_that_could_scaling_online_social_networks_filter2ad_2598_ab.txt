redundancy and to guarantee data locality.
Balance loads: Application level read and write requests
of a user are directed only to her master replica. Write
operations need to be propagated to all her slave replicas
for consistency. However, since masters handle much more
load than slaves, we can obtain an approximately balanced
load by doing an even distribution of masters among servers.
Be resilient to machine failures: To cope with machine
failures, we need to ensure that all masters have at least K
slave replicas that act as redundant copies.
Be amenable to online operations: OSN are highly
dynamic; new users constantly join and there is a process
of graph densiﬁcation due to new edges formed between
users [22]. Further, the infrastructure hosting the OSN may
change through the addition, removal or upgrade of servers.
To handle such dynamics, the solution needs to be respon-
sive to such a wide range of events, and yet simple to quickly
converge to an eﬃcient assignment of users to servers.
Be stable: Given that we are operating in a highly dynamic
environment, we need to ensure that the solution is stable.
For example, addition of a few edges should not lead to a
cascade of changes in the assignment of masters and slaves
to servers.
Minimize replication overhead: The overall performance
and eﬃciency of the system is strongly correlated to the
number of replicas in the system. We require a solution that
keeps replication overhead – deﬁned as the average number
of slave replicas created per user – as low as possible.
3.2 Formulation
Given the above requirements, we can formulate the solu-
tion as an optimization problem of minimizing the number
of required replicas. For this purpose, we use the following
notation. Let G = (V, E) denote the social graph represent-
ing the OSN, with node set V representing users, and edge
set E representing (friendship) relationships among users.
Let N = |V | denote the total number of users and M the
number of available servers.
We could cast the problem as an integer linear program
where pij denotes a binary decision variable that becomes 1
if and only if the primary of user i is assigned to partition
j, 1 ≤ j ≤ M . Also rij denote a similar decision variable
for a replica of user i assigned to partition j. Finally, let the
constants ii(cid:2) = 1 if {i, i(cid:2)} ∈ E capture the friendship rela-
tionships. We state the MIN REPLICA problem as follows:
377s.t.
X
X
min
i
X
rij
j
pij = 1
∀j
X
X
pij + ii(cid:2) ≤ pi(cid:2)j + ri(cid:2)j + 1,∀i, j, i(cid:2)
(pij) =
(pi(j+1)), 1 ≤ j ≤ M − 1
rij ≥ k,∀i ∈ V
i
i
X
j
(1)
(2)
(3)
(4)
Constraint 1 in the above formulation ensures that there is
exactly one master copy of a user in the system. Constraint 2
ensures that all neighbors of a user (be it masters or slaves)
are in the same machine. Constraint 3 tries to distribute
equal number of master replicas across the machines and
Constraint 4 encodes the redundancy requirement.
Lemma 1. Min Replica is NP-Hard
A simple reduction from the graph Min-bisection problem
[14] can be used to prove this. We skip the description of
the formal proof for lack of space.
3.3 Why graph/social partitioning falls short
An obvious set of candidates that can be used to address
the problem described in the previous section include graph
partitioning algorithms [10, 19] and modularity optimization
algorithms [23, 26, 27, 12]. These algorithms either aim to
ﬁnd equal sized partitions of a graph such that the number of
inter-partition edges is minimized, or try to maximize mod-
ularity[27]. There are, however, several reason why these
methods are inadequate for our purpose:
• Most graph partitioning algorithms are not incremental
(oﬄine) [12, 27, 10]. This poses a problem when dealing
with highly dynamic social graphs, as they require costly
re-computation of the partition. An incremental (online)
algorithm is more suited for this scenario.
• Algorithms based on community detection are known to
be extremely sensitive to input conditions. Small changes
to the graph structure can lead to very diﬀerent placement
of nodes into partitions [20]. In other words, they do not
produce stable solutions.
• It can be argued that directly reducing the number of
inter-partition edges is equivalent to a reduction of the num-
ber of replicas. However, it is not the case. Consider the
example depicted in Fig. 2. Minimizing the number of inter-
partition edges will result in partitions P1 and P2, with only
3 inter-partition edges but this requires 5 replicas to main-
tain locality. On the other hand, partitions P3 and P4 result
in 4 inter-partition edges, but this requires one less replica.
As we will show later in Sec. 5 minimizing inter-partition
edges indeed leads to worse results. Motivated by the short-
comings of the existing solutions, we present our online so-
lution in the next section.
4. SPAR: JOINT PARTITIONING AND
REPLICATION ON THE FLY
Having described the MIN REPLICA problem and the re-
quirements that it must fulﬁll, we now present our heuristic
solution based on a greedy optimization using local informa-
tion.
4.1 Overview
We assume that users represent nodes in a graph, and
edges are formed when users create links among them. The
algorithm runs in the Partition Manager module as explained
in Sec. 6. In the average case, the required information for
the algorithm is proportional to the product of the average
node degree and the number of servers. The worst case com-
putational complexity of the algorithm is proportional to the
highest node degree. The algorithm is triggered by any one
of the six following possible events: addition or removal of
either nodes, edges, or servers.
4.2 Description
Node addition: A new node (user) is assigned to the parti-
tion with the fewest number of masters replicas. In addition,
K slaves are created and assigned to random partitions.
Node removal: When a node is removed (a user is deleted),
the master and all the slaves are removed. The states of the
nodes that had an edge with it are updated.
Edge addition: When a new edge is created between nodes
u and v, the algorithm checks whether both masters are
already co-located with each other or with a master’s slave.
If so, no further action is required.
If not, the algorithm calculates the number of replicas that
would be generated for each of the three possible conﬁgu-
rations: 1) no movements of masters, which maintains the
status-quo, 2) the master of u goes to the partition contain-
ing the master of v, 3) the opposite.
Let us start with conﬁguration 1). Here, a replica is added
if it does not already exist in the partition of the master of
the complementary node. This may result in an increase of
1 or 2 replicas depending on whether the two masters are
already present in each other’s partitions. This can occur if
nodes v or u already have relationships with other nodes in
the same partition or if there already exist extra slaves of v
or u for redundancy.
In conﬁguration 2), no slave replicas are created for u and
v since their masters will be in the same partition. However,
for the node that moves, in this case u, we will have to
create a slave replica of itself in its old partition to service
the master of the neighbors that were left behind in that
partition. In addition, the masters of these neighbors will
have to create a slave replica in the new partition – if they
do not already have one– to preserve the local semantics
of u. Finally the algorithm removes the slave replicas that
were in the old partition only to serve the master of u, since
they are no longer needed. The above rule is also subject to
maintaining a minimum number of slave replicas due to the
K redundancy: the old partition slave will not be removed
if the overall system ends up with less than K slaves for that
particular node. Conﬁguration 3) is complementary to 2).
The algorithm greedily chooses the conﬁguration that yields
the smallest aggregate number of replicas subject to the con-
straint of load-balancing the master across the partitions.
More speciﬁcally, conﬁguration 2) and 3) also need to ensure
that the movement does not cause load unbalancing. That
is, this movement either happens to a partition with fewer
masters, or to a partition for which the savings in terms of
number of replicas of the best conﬁguration to the second
best one is greater than the current ratio of load imbalance
between partitions.
Fig. 3 illustrates the steps just described with an exam-
ple. The initial conﬁguration (upper-left subplot) contains
378a
b
c
d
e
f
g
h
i
j
a
b
e
f
c
d
P1
i
j
g
h
P2
a
b
c
d
P3
e
f
i
j
g
h
P4
Figure 2: Illustrative example on why minimizing edges between partitions (inter-partition) does not minimize replicas. The
partition P1 and P2 results in 3 edges between partitions and 5 nodes to be replicated (e to i). The partitions P3 and P4 results
in 4 inter-partition edges but only 4 nodes need to be replicated (c to f ).
6 nodes in 3 partitions. The current number of replicated
nodes (empty circles) is 4. An edge between nodes 1 and 6
is created. Since there is no replica of 1 in M3 or replica of
6 in M1 if we maintain status quo, two additional replicas
will have to be created to maintain the local semantics.
The algorithm also evaluates the number of replicas that
are required for the other two possible conﬁgurations.
If
node 1 were to move to M3, we would need three new repli-
cas in M3 since only 2 out of the 5 neighbors of node 1 are
already in M3. In addition, the movement would allow re-
moving the slave of node 5 from M1 because it is no longer
needed. Consequently, the movement would increase the to-
tal number of replicas by 3-1=2, yielding a new total of 6
replicas, which is worse that maintaining the status quo.
In the last step, the algorithm evaluates the number of
replicas for the third allowed conﬁguration: moving the mas-
ter of node 6 in M1. Here, the replica of node 5 in M3 can be
removed because it already exists in M1 and no other node
links to it in M3. Thus, no replica needs to be created. The
change in the number of replicas is -1, yielding a total of 3
replicas.
Moving 6 to M1 minimizes the total number of replicas.
However, such a conﬁguration violates the load balancing
condition and hence, cannot be performed. Thus, the ﬁnal
action is not to move (status quo) and create an additional
2 replicas.1
Edge removal: When an edge between u and v disappears,
the algorithm removes the replica of u in the partition hold-
ing the master of node v if no other node requires it, and
vice-versa. The algorithm checks whether there are more
than K slave replicas before removing the node so that the
desired redundancy level is maintained.
Server addition: Unlike the previous cases, server addi-
tion and removal do not depend on the events of the social
graph but are triggered externally by system administrators
or detected automatically by the system management tools.
There are two choices when adding a server: 1) force re-
distribution of the masters from the other servers to the new
one so that all servers are balanced immediately, or 2) let
the re-distribution of the masters be the result of the node
and edge arrival processes and the load-balancing condition.
M 2+M least
replicated masters from the M server and move them to the
new server M + 1. After the movement of the masters, the
In the ﬁrst case, the algorithm will select the
N
1One might wonder what happens with power-users like
Oprah. Must all her followers be replicated to the server
hosting her master? Only the direct neighbors from whom
a user is reading must be co-located. Power-users might
have to be replicated across multiple servers but their large
audience does not aﬀect the system negatively. Also, OSNs
impose a limit to the number of people that you can fol-
low or befriend to avoid spammers and bots, e.g. 5000 for
Facebook.
Node 5 rep. in M1,M3
Node 6 rep. in M2
Node 1 rep. in M2
Edge 1-6 created
6'
1'
5
M2
5'
M3
6
Nodes 2,3,4 rep. in M3
Node 1 rep. in M1
Node 5 replica deleted in M1
6'
Node 1 moves to M3
Node 6 moves to M1
2
3
4
1'
M1
M3
6
1
5'
1'
5
M2
2'
3'
4'
6'
1'
5
M2
1'
5'
M3
6
5'
6
2
3
4
1
M1
Node 5 replica deleted in M3
6'
1'
5
M2
M3
5'
1
M1
S
a
t
t
u
s
Q
u
o
5'
1
2
3
4
2
3
4
6'
M1
Node 6 rep. in M1
Node 1 rep. in M3
Figure 3: SPAR Online Sketch
algorithm will ensure that for all the masters moved to the
new server, there is a slave replica of their neighbors to guar-
antee the local data semantics. This mechanism guarantees
that the masters across all the M + 1 servers are equally bal-
anced. However, it may not provide a minimum replication
overhead. For this reason, for a fraction of the edges of the
masters involved, the algorithm also triggers a system-replay
edge creation event, which reduces the replication overhead.
As we will see later in the evaluation section, this provides
good replication overhead even under dynamic server events.
In the second case, the algorithm does nothing else to in-
crease the number of available servers. The edge/user arrival
will take care of ﬁlling the new server with new users that in
turn attract old users when edges are formed to them. This
leads to an eventual load balancing of the masters across
servers without enforcing movement operations. The only
condition is that the OSN continues to grow.
Server removal: When a server is removed, whether inten-
tionally or due to a failure, the algorithm re-allocates the N
master nodes hosted in that server to the remaining M − 1
M
servers equally. The algorithm decides the server in which
a slave replica is promoted to master, based on the ratio
of its neighbors that already exist on that server. Thus,