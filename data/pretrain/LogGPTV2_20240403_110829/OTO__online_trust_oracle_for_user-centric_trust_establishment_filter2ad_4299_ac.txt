Why this !le may be safe
Why this !le may be harmful
Why this !le may be safe
Featured Article
ESET Threat Blog has an article 
featuring this !le: . . .
Friends’ Use 
None of your friends and family 
downloaded this !le. 
High Number of Available Reviews
Number of available reviews for this 
!le is high: . . .
High Popularity 
Total number of downloads last week
was high: . . .
See all 3 reasons
See all 5 reasons
File Origin
You attempted to download [install-!ashplayer11x64ax_gtba_alh.exe] while browsing 
http://www.youtube.com. We suggest that you download the (cid:127)le directly 
from the publisher.  
Unknown publisher
This !le is certi!ed by an unknown 
publisher: . . .
Friends’ Use 
None of your friends and family 
downloaded this !le. 
See all 8 reasons
File Origin
You attempted to download [activation.exe] while browsing http://extratorrent.com.  
We suggest that you download the (cid:127)le directly from the publisher.  
Download (I understand the risk)
Cancel
Download (I understand the risk)
Cancel
Change when these noti!cations appear
Change when these noti!cations appear
Figure 3: An example of an OTO interface. OTO presents rea-
sons why the ﬁle may be harmful and why the ﬁle may be safe to
install.
Potentially Malicious Software
Are you sure you want to download this !le? It may damage 
your computer.  We gathered evidence to help you decide.
Why this (cid:127)le may be harmful
Why this (cid:127)le may be safe
Friends’ Use 
24 of your friends and family also 
downloaded this (cid:127)le. Who are they?
Featured Article
New York Times has an article 
featuring this (cid:127)le: . . .
See all 8 reasons
File Origin
You attempted to download [V3.exe] while browsing http://globalahnlab.com.  
Download (I understand the risk)
Cancel
Change when these noti(cid:127)cations appear
Figure 4: An example of an OTO interface for legitimate soft-
ware. This example presents 0 negative reason and 8 positive rea-
sons.
Based on how OTO judges the software’s legitimacy, OTO
shows (1) reasons why this software is safe to download (pos-
itive evidence), and (2) reasons why this software may harm
the computer (negative evidence). Note that these reasons
are primarily based on the security experts’ suggestions that
can be feasibly gathered from the online sources. Users can
also choose to see other reasons that are helpful in determin-
ing the downloading software’s legitimacy. We also envision
that a user can rank how important and critical each piece
of evidence is to him, and OTO will assess the software’s
safety level based on such user-driven evidence ranks.
Following the Windows User Account Control framework,
OTO has three color modes for diﬀerent safety levels:
Blue with “?” shield symbol for legitimate software.
As shown in Figure 4, this applies to software that is highly
likely to be legitimate. For example, software that is dig-
itally signed by Microsoft would be categorized to show
the blue OTO interface. This interface shows the follow-
ing warning statement against a blue background: “Are you
sure you want to download this ﬁle? It may damage your
computer. We gathered evidence to help you decide.” Next
to the statement, Microsoft’s blue security shield symbol
with “?” is displayed. Under this blue message box, a table
Figure 5: An example of an OTO interface for malware. This
example presents 8 negative reasons and 0 positive reason.
is displayed, showing negative and positive reasons. For the
software in this category, a signiﬁcant number of positive
pieces of evidence is displayed, although some negative evi-
dence may be displayed (if it exists). As shown in Figure 4,
OTO shows the top two pieces of evidence, which either
OTO or the user has selected as important, and contains a
link that users can click if they want to see all 8 reasons.
Red with “X” shield symbol for malicious software.
As shown in Figure 5, the red colored OTO interface is for
pieces of software that is highly likely to be malicious. As
with the blue OTO interface, the same warning statement is
shown; however, the statement is against a red background,
with a red “X” security shield symbol. Also, this interface
will display a signiﬁcant number of negative pieces of evi-
dence, although few positive pieces of evidence may be dis-
played.
Yellow with “!” shield symbol for software of un-
certain provenance.
As shown in Figure 3, the yellow
colored OTO interface is for software that the system can-
not clearly determine as either malicious or safe. As a result,
with a yellow “!” security shield symbol, the same warning
statement is shown against a yellow background. For this
software, the interface will display both positive and nega-
tive evidence.
4.2 Retrieving Positive and Negative Evidence
We conjecture that users fail to make informed trust deci-
sions due to (1) unawareness of existing evidence (e.g., they
are simply unaware of the ways to ﬁnd evidence or are re-
luctant to do so), (2) overabundance of evidence (e.g., too
much, and potentially conﬂicting evidence further confuses
them), and (3) the cumbersome nature of searching for evi-
dence. Understanding these issues are part of the objectives
of this paper, and providing a concise list of useful, helpful,
and objective pieces of trust evidence may enable users to
make informed trust decisions.
Existing tools and technology websites provide some ev-
idence that people can refer to. For example, CNET2 pro-
vides reviews and speciﬁcations about software; speciﬁca-
tions include average user rating, the software’s version num-
ber, ﬁle size, release date, number of total downloads, num-
ber of downloads last week, etc. Norton also has antivirus
tools featuring File Insight, the purpose of which is to pro-
2http://www.cnet.com
396Table 4: A list of trust evidence that can be provided to users
when they download software. Each piece of evidence is scalable
for automatic retrieval from the Internet; this table presents which
pieces of evidence that can be made robust against spooﬁng attacks.
Evidence
Robust
General information about software
• Developer
• Version Number
• Date added
• File size
• File name
• Category of software (e.g., video player, game)
• Digitally-signed certiﬁcate
Origin of the downloading software & developer
• Hosting site (that users click to download ﬁles)
• Source address (where ﬁles are downloaded from)
• PageRank of hosting site and source address
• Information about the developer (e.g., business history,
location, number of employees, etc.)
Crowdsourcing
• Total number of people who downloaded the software
• Total number of people who downloaded last week
• Ratings by people who downloaded the software
• Reviews
• Recent changes in ratings
Reputable 3rd-party resource
• (OSN) Expert friends who also downloaded the same
software
• (OSN) Suggestion of more popular/credible software
within the same category
• (Authorities) Credible newspaper/magazine articles
featuring the software



















vide speciﬁcations about the ﬁles that users are about to
download. Similar to CNET, Norton File Insight provides
the information regarding developers, release date, software’s
version number, number of Norton community users who
have used the same software, date when the software was
released, Norton’s rating on the software, where the ﬁle is
being downloaded from, etc.
However, some of the speciﬁcations provided by the exist-
ing tools and technologies may not be robust against spoof-
ing attacks. For example, attackers can easily increase ver-
sion numbers to seem as though the software providers up-
date the software frequently. Similarly, the release date of
the software could potentially be backdated.
As a ﬁrst step, we have analyzed the pieces of trust ev-
idence that are both robust against spooﬁng attacks and
scalable for automatic retrieval from the Internet. Table 4
lists the evidence that satisﬁes our criteria.
Using the evidence that is both robust and scalable, we
describe how OTO displays them in the next section.
4.3 Presenting Positive and Negative Evidence
Each of the pieces of trust evidence, as shown in Table 4,
can be perceived as positive (supporting that a ﬁle may be
safe to download) or negative (conﬁrming that a ﬁle may
be harmful). For example, “Friend’s Use” can be a negative
reason if none of a user’s security-expert friends and family
downloaded the same ﬁle (Figures 3 and 5), but it can be a
positive reason if many friends and family downloaded the
ﬁle (Figure 4).
For the evidence from authorities, we assume that OTO
uses an algorithm that can determine if an article is positive
on a piece of software or whether it declares the software as
malware. Given the recent advances in text understanding,
we assume such analysis to be viable and reliable [4].
Note that the OTO interface displays a summary of each
If users want to examine the evidence in
evidence type.
detail, OTO provides a clickable link that will display more
detailed explanation regarding that evidence.
The OTO interface only displays 2 pieces of supporting
evidence for each column. If there are more than 2 pieces
of evidence, OTO provides a clickable link that can show all
supporting evidence for the particular column.
Furthermore, OTO considers the “File Origin” evidence
as neutral evidence because determining whether the down-
loading ﬁle is legitimate, based on the source address, is a
non-trivial question. As a result, “File Origin” is displayed
below the reasoning table. If OTO is able to consult credible
blacklists of malicious websites, OTO can alert users if the
hosting site was found on the blacklist(s).
5. SECURITY ANALYSIS
In this section, we delineate how OTO detects malware
(Section 5.1) and how OTO handles misclassiﬁcations of
software (Section 5.2). In Section 5.3, we also describe how
OTO mitigates manipulation attacks as mentioned in Sec-
tion 2.3.
5.1 Detecting Malware
In this section, we describe how OTO successfully detects
malware and provides helpful evidence to users. In general,
two types of malware are as follows:
In case of zero-day malware, lack of
Zero-day malware.
available positive evidence should alarm users to be careful
about downloading the ﬁle.
Well-known malware.
If the malware is well-known,
the evidence gathered by the OS is likely to contain sub-
stantially more pieces of negative evidence than positive ev-
idence. Given a high number of negative evidence and the
strength of negative evidence (compared to those of positive
evidence), there is a high probability that users will detect
such malware as harmful.
5.2 Handling False Alarms
There is a possibility that OTO misclassiﬁes software:
(false negative) given undeniably malicious software, OTO
may fail to detect it as malicious when the number and/or
the quality of positive evidence is proportional to that of
negative evidence; (false positive) OTO may misclassify le-
gitimate software as malicious.
In such cases, users can detect false alarms by examin-
ing the evidence. For example, as shown in Figure 3, Alice
can analyze each piece of evidence and judge whether nega-
tive reasons outweigh positive reasons: are (1) a threat blog
discussion on this software and (2) no usage by the user’s
security-expert friends and family (along with a third rea-
son) stronger than (1) high number of available reviews, (2)
high popularity, and 2 other reasons? Since all pieces of evi-
dence are drawn from Alice’s preferences to help her decide,
Alice can make her own informed trust decision based on
the displayed information.
One may argue that Alice may still ﬁnd it diﬃcult to an-
alyze the displayed information. However, what is displayed
is likely to be what she would have searched online her-
self. Given the easy availability of the information increases
the likelihood that she will consider the information. Plus,
the evidence originates from trustworthy resources that she
deﬁnes. Hence, she is safe from reading misguiding informa-
tion that she could have found on the Internet, and OTO
introduces no extra risk to Alice.
3975.3 Mitigating Manipulation Attacks
The OTO system is designed to disrupt malware distribu-
tors from manipulating positive and negative evidence. We
discuss three potential categories of attack.
Generating falsifying evidence.
Attackers can at-
tempt to create fake positive evidence for malware. In Ta-
ble 4, we present the list of pieces of evidence that we believe
can be made robust. For example, the total number of down-
loads can be made robust by analyzing the user population
and temporal aspects of the download – in case the attacker
rents a botnet to inﬂate the number of downloads the sudden
spike in downloads and the population distribution should
appear anomalous.
While it is outside the scope of this paper to ensure the
robustness of each indicator, we would like to emphasize
that a successful attack would need to forge several positive
pieces of evidence, which will likely be negated by a strong
negative piece of evidence that would still alert the user.
If an attacker creates positive articles about the software,
it would need to get them published at an authoritative site
trusted by the user to be considered by OTO.
Hiding harmful evidence.
Attackers may attempt to
prevent the OS from fetching negative evidence or prevent an
online resource from serving negative evidence. While these
attacks are outside the scope of this paper, we believe that it
would be challenging for attackers to prevent authoritative
resources from serving information about the malware.
Impersonation of legitimate software. Attackers may
attempt to impersonate a well-known software system and
claim an update or a free installation. We assume that
OTO can perform secure associations of the pieces of evi-
dence with the software, for example using a cryptographic
hash value of the software. It will, however, be challenging
for written articles to form the correct association with the
correct piece of software, as journalists rarely include the
cryptographic hash of the software they are writing about.
In case OTO is widely deployed, mechanisms for associating
pieces of evidence with software need to be used, for exam-
ple by including the public key of the software distributor or
the cryptographic hash of the software in a written article.
6. EVALUATION
We conducted a user study to test whether OTO achieves
the desired properties as described in Section 2.2.
6.1 Demographics
We recruited 58 participants from a diverse set by adver-
tising on Craigslist, ﬂyers around bus stops, Facebook post-
ings, and university mailing lists. We randomly assigned 29
participants to each of the OTO and SSF conditions. Ta-
ble 5 summarizes the demographics.
6.2 User Study Process
We designed a between-subjects experiment with two con-
ditions, the SSF and OTO conditions, each of which consists
of 10 scenarios as described in Table 2.
We consider the SSF condition as a baseline for the fol-
lowing reason: According to NSS Labs, SSF is the current
state-of-the-art technology3 that is widely used on a modern
browser, which ensures that the users are well aware of IE’s
3http://www.pcmag.com/article2/0,2817,
2391164,00.asp
Table 5: Demographics of study participants for OTO and SSF
conditions.
Group size (N )
Gender
Age
Occupation
Educational background
Security knowledge level
Male
Female
Minimum
Maximum
Average (µ)