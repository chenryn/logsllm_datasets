leaves/rejoins a community. This will be supported in fu(cid:173)
ture work.
4.3. Extended BOINC GUI-RPC
The implementation of the proposed mechanism is de(cid:173)
signed to perform distributed and cooperative scheduling
1-4244-2398-9/08/$20.00 ©2008 IEEE
319
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
I Ex-RPC Name I
Table 1. Extended BOINC GUI-RPC Commands
attach_wu
detach_wu
get_wujnfo
Input
WU name, XML data ofWU (including input file PATH)
Output
none
Description
Append the new WU to the core client.
The information about new WU is specified by input parameters.
Input
WUname
Output
none
Description
Delete the specified WU an10ng the WUs managed by the core client.
Input
WUname
Output
XML data of the WU (including input file PATH)
Description
Get information about the specified WU
by communicating with the BOINC core client via BOINC
GUI-RPCs. However, the original BOINC middleware does
not allow the BOINC GUI-RPCs to operate WUs. There(cid:173)
fore, we extend the BOINC GUI-RPCs to support WU op(cid:173)
erations.
Table 1 summarizes the extended BOINC GUI-RPCs.
Those minimum extensions enable the proposed task
scheduling mechanism and the BOINC core client to col(cid:173)
laboratively work to improve the computational efficiency
of the community.
4.4. A Mechanism to Move WUs between
Clients
The proposed implementation uses extended BOINC
GUI-RPCs to move WUs between clients. First, the dis(cid:173)
tributed and cooperative scheduler gets the information of
computing resource and the BOINC core client via original
BOINC GUI-RPC. The list ofWUs that the client holds is
included in the information of the BOINC core client. To
move WUs to other clients, the scheduler picks up some
WUs from the list of WUs, and gets the data of WUs via
get_wu_info. Next, the scheduler deletes the information of
specified WUs from the BOINe core client by using de(cid:173)
tach_wu, because the BOINC core client must not process
specified WUs. After that, the scheduler sends the data of
WUs to other clients via logical links. Finally, the sched(cid:173)
uler that receives the data of WUs appends the data to the
BOINC core client as new WUs by using attach_wu.
5. Performance Evaluation and Discussions
This section shows the experimental results with the
proposed implementation, and demonstrates how dynamic
load balancing and proxy download improve the efficiency.
This section first evaluates the effect of the proxy download
on the performance, which appropriately distributes WUs
among clients even if the project server is in the busy state.
Then, we show the evaluation results to demonstrate that
the volunteer computing system can adapt to the dynamic
load changes and thereby reduce the turnaround time for
processing each WU.
The implemented system is evaluated in terms of the fol(cid:173)
lowing metrics, while changing the numbers of clients and
WUs assigned to a client at a time (WUs/access).
• Server-CPU usage: The average load of the project
server during the computation.
• Network load: The average size of transferred data
from/to the project server, and that between clients dur(cid:173)
ing the computation.
• The turnaround time of a WU: The max, min and
average turnaround times from the WU download to
the result upload.
Table 2 shows the specifications of the project server
used in the following experiments. The network interface
of the project server is connected to a L3 network switch
via a lOOMbps Ethernet link.
1-4244-2398-9/08/$20.00 ©2008 IEEE
320
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Table 2. Server Machine
CPU
Intel Core2Duo 2.93GHz
Memory
Network
OS
4.0GB
Giga-bit Ethernet
Linux 2.6.23 (Fedora 7)
Web Server
BOINC Server
Apache 2.2.6
Version 6.1.0
Computing resources in the InTrigger platform [11] are
used as BOINC clients. The InTrigger platform is a testbed
for distributed computing, and consists of several cluster
systems, which are geographically distributed across Japan.
The InTrigger platform is shared by many researchers na(cid:173)
tionwide in Japan, and hence the computing power of each
client for volunteer computing dynamically varies because
of the tasks of the other users. Table 3 shows the names of
the InTrigger sites and the specifications of their computing
resources. The peak performances of the clients used in the
evaluation are not heterogeneous. However, the InTrigger
platform is shared by many other researchers, and therefore
the effective performances of individual peers devoted to
our project results in a heterogeneous environment.
A ring network topology is employed to organize the
overlay network. Clients within the same site are connected
with logical links, and the number of connections between
sites in the overlay network is minimized.
In the experiments, the number of clients is constant dur(cid:173)
ing the computation. The following two applications whose
input and output files for each WU are 5000 bytes are used
in the experiments. The size of total data transferred be(cid:173)
tween the project server and clients is represented as a prod(cid:173)
uct of the number of WUs and the total size of input and
output files (e.g. 1OKBytes).
• Application-I: The execution time for each WU pro(cid:173)
cessing is negligible. Therefore, a client accesses a
project server many times during a short period and
also communicates with other clients more frequently.
• Application-2: The execution time for each WU pro(cid:173)
cessing is 100 sec in the CPU time when the full CPU
performance of a client is used; the actual execution
time depends on the client CPU load.
5.1. Effect of Proxy Download on Through(cid:173)
put
We first describes the experimental results without the
proposed task scheduling mechanism, and then discuss the
effect of the proxy download realized by the proposed
mechanism, which reduces the load concentration on the
project server and improves the utilization ratio of clients.
Table 4 shows the performance of the entire computing
system and the CPU load of the project server in the case
where the proposed task scheduling mechanism is not used.
Here, the WUs/sec denotes the number of WUs processed
in the volunteer computing system per second, and the Ac(cid:173)
cesses/sec is the number of requests sent from clients per
seconds. The WUs/sec indicates the throughput of the en(cid:173)
tire system. The rate of idle clients is the average ratio of
the idle clients, which fail in WU downloading and provide
no computing power to the volunteer computing system,
to all clients. The results show that a higher WUs/access
leads to a higher WUs/sec for the same number of clients.
This indicates that a higher WUs/access increases the uti(cid:173)
lization efficiency of clients, and thereby in1proves the sus(cid:173)
tained throughput of the entire computing system, even if
the total peak performance of the clients within the system
does not change. On the other hand, if WUs/access is con(cid:173)
stant, use of more clients for computing increases WUs/sec.
This indicates that an increase in working clients improves
the peak performance of the entire computing system rather
than the utilization ratio.
The average CPU load of the project server increases in
proportion to WUs/sec. Thus, most of the CPU load are
spent for handling WUs, and the average CPU load signif(cid:173)
icantly depends on the number of WUs per second. In the
case of 222 clients, WUs/sec hardly improves by increasing
WUs/access, and the average CPU load is almost unchanged
as approximately 30%. This is because many clients fail in
WU downloading and cannot process any WUs. These re(cid:173)
sults clearly show that the project server often rejects WU
requests even if its average CPU load is about 30%, result(cid:173)
ing in severe performance degradation.
Effective WU distribution can be achieved using the
I Site Name I
CPU
Table 3. Client Machines
I Memory I
Network
I # of Clients I
chiba
hongo
suzuk
kyoto
imade
Intel Core2Duo 2.33GHz
Intel PentiumM 1.8GHz
Intel Core2Duo 2.33GHz
Intel Core2Duo 2.33GHz
Intel Core2Duo 2.33GHz
4GB
1GB
4GB
4GB
4GB
Giga-bit Ethernet
Giga-bit Ethernet
Giga-bit Ethernet
Giga-bit Ethernet
Giga-bit Ethernet
58
64
36
35
29
1-4244-2398-9/08/$20.00 ©200B IEEE
321
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Table 4. Project Performance and Server Load (without the Proposed Scheduling Mechanism)
I # of Clients I WUs/access I WUs/sec I Access/sec I CPU load[%] I Rate of idle clients[%] I
29
64
222
1
2
4
8
1
2
4
8
1
2
4
8
2.8
2.8
6.6
7.8
6.1
6.1
15.1
15.3
16.1
18.2
19.7
19.7
2.80
1.40
1.65
0.98
6.10
3.05
3.78
1.91
16.10
9.10
4.93
2.46
5
5
10
10
11
10
19
20
30
29
30
31
0
0
0
0
0
0
0
2
5
22
41
57
Table 5. Project Performance and Server Load (with the Proposed Scheduling Mechanism)
# of Clients I WUs/access I WUs/sec I Access/sec I CPU 10ad[°A>] I Rate of idle clients[%] I Improvement[%]
29
64
222
1
2
4
8
1
2
4
8
1
2
4
8
2.8
2.8
6.7
9.3
6.1
6.1
15.2
18.2
16.1
18.2
20.1
25.1
2.80
1.40
1.68
1.16
6.10
3.05
3.80
2.28
16.10
9.10
5.03
3.14
5
5
10
16
11
10
19
26
30
29
32
39
0
0
0
0
0
0
0
0
5
21
13
12
0
0
1.52
19.23
0
0
0.66
18.95
0
0
2.03
27.41
proxy download provided by the proposed mechanism. Ta(cid:173)
ble 5 shows the performance of the entire system and the
average CPU load of the project server in the case of using
the proposed mechanism. Table 5 also shows the speedup
compared to the computing system without the proposed
task scheduling mechanism. These results shows that a high
WUslaccess improves the total throughput of the entire sys(cid:173)
tem, because the proxy download can increase the number
of working clients by distributing WUs to the clients, which
have failed in downloading and cannot process any WUs.
Especially in the case of 222 clients and WUslaccess=8, the
proxy download can improve the rate of idle clients, and
as a result the total throughput of the entire system is im(cid:173)
proved by 27%. However, in the cases of WUslaccess=1
and WUs/access=2, the proposed task scheduling mecha(cid:173)
nism does not improve the performance at all, because each
client has too few WUs and the proposed mechanism cannot
move those WUs. To move WUs between clients, the pro(cid:173)
posed task scheduling mechanism needs high WUs/access.
Figures 3 and 4 show the network traffics on the project
server without the proposed task scheduling mechanism and
that with the proposed task scheduling mechanism. The
solid lines labeled Input Data indicate the network band(cid:173)
widths consumed at the server for incoming data from the
clients, and the broken lines labeled Output Data indicate
the network bandwidths consumed at the server for outgo(cid:173)
ing data to the clients. The network traffics on the project
server are varied with changes in WUs/access and the num(cid:173)
ber of clients. In these figures, the x-axis indicates the num(cid:173)
ber of WUslaccess, and each line indicates the results on the
different-sized systems. In Figure 3, there are two factors to
increase the network traffics. One is a constant-sized traffic
at every access to the project server. Network traffics in(cid:173)
crease linearly with the number of accesses, Accesses/sec.
As Accesses/sec is in inverse proportion to WUs/access, a
higher WUs/access reduces the network traffics. The other
factor is the network traffics for WU downloads and result
uploads. The higher throughput the volunteer computing
1-4244-2398-9/08/$20.00 ©2008 IEEE