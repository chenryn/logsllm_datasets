title:Multi-resource fair queueing for packet processing
author:Ali Ghodsi and
Vyas Sekar and
Matei Zaharia and
Ion Stoica
Multi-Resource Fair Queueing for Packet Processing
Ali Ghodsi†,‡, Vyas Sekar⇧, Matei Zaharia†, Ion Stoica†
† University of California, Berkeley ⇧ Intel ISTC ‡ KTH/Royal Institute of Technology
{alig, matei, istoica}@cs.berkeley.edu, PI:EMAIL
ABSTRACT
Middleboxes are ubiquitous in today’s networks and perform a va-
riety of important functions, including IDS, VPN, ﬁrewalling, and
WAN optimization. These functions differ vastly in their require-
ments for hardware resources (e.g., CPU cycles and memory band-
width). Thus, depending on the functions they go through, dif-
ferent ﬂows can consume different amounts of a middlebox’s re-
sources. While there is much literature on weighted fair sharing
of link bandwidth to isolate ﬂows, it is unclear how to schedule
multiple resources in a middlebox to achieve similar guarantees. In
this paper, we analyze several natural packet scheduling algorithms
for multiple resources and show that they have undesirable proper-
ties. We propose a new algorithm, Dominant Resource Fair Queu-
ing (DRFQ), that retains the attractive properties that fair sharing
provides for one resource. In doing so, we generalize the concept
of virtual time in classical fair queuing to multi-resource settings.
The resulting algorithm is also applicable in other contexts where
several resources need to be multiplexed in the time domain.
Categories and Subject Descriptors: C.2.6
[Computer-Communication Networks]: Internetworking
Keywords: Fair Queueing, Middleboxes, Scheduling
1.
INTRODUCTION
Middleboxes today are omnipresent. Surveys show that the num-
ber of middleboxes in companies is on par with the number of
routers and switches [28]. These middleboxes perform a variety
of functions, ranging from ﬁrewalling and IDS to WAN optimiza-
tion and HTTP caching. Moreover, the boundary between routers
and middleboxes is blurring, with more middlebox functions being
incorporated into hardware and software routers [2, 6, 1, 27].
Given that the volume of trafﬁc through middleboxes is increas-
ing [20, 32] and that middlebox processing functions are often ex-
pensive, it is important to schedule the hardware resources in these
devices to provide predictable isolation across ﬂows. While packet
scheduling has been studied extensively in routers to allocate link
bandwidth [24, 10, 29], middleboxes complicate the scheduling
problem because they have multiple resources that can be con-
gested. Different middlebox processing functions consume vastly
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’12, August 13–17, 2012, Helsinki, Finland.
Copyright 2012 ACM 978-1-4503-1419-0/12/08 ...$15.00.
different amounts of these resources. For example, intrusion detec-
tion functionality is often CPU-bound [13], software routers bottle-
neck on memory bandwidth when processing small packets [8], and
forwarding of large packets with little processing can bottleneck on
link bandwidth. Thus, depending on the processing needs of the
ﬂows going through it, a middlebox will need to make scheduling
decisions across multiple resources. This becomes more important
as middlebox resource diversity increases (e.g., GPUs [30] and spe-
cialized hardware acceleration [23, 5]).
Traditionally, for a single resource, weighted fair sharing [10]
ensures that ﬂows are isolated from each other by making share
guarantees on how much bandwidth each ﬂow gets [24]. Further-
more, fair sharing is strategy-proof, in that ﬂows cannot get better
service by artiﬁcially inﬂating their resource consumption. Many
algorithms, such as WFQ [10], GPS [24], DRR [29], and SFQ [18],
have been proposed to approximate fair sharing through discrete
packet scheduling decisions, but they all retain the properties of
share guarantees and strategy-proofness. We would like a multi-
resource scheduler to also provide these properties.
Share guarantees and strategy-proofness, while almost trivial for
one resource, turn out to be non-trivial for multiple resources [16].
We ﬁrst analyze two natural scheduling schemes and show that they
lack these properties. The ﬁrst scheme is to monitor the resource
usage of the system, determine which resource is currently the bot-
tleneck, and divide it fairly between the ﬂows [14]. Unfortunately,
this approach lacks both desired properties. First, it is not strategy-
proof; a ﬂow can manipulate the scheduler to get better service
by artiﬁcially increasing the amount of resources its packets use.
For example, a ﬂow can use smaller packets, which increase the
CPU usage of the middlebox, to shift the bottleneck resource from
network bandwidth to CPU. We show that this can double the ma-
nipulative ﬂow’s throughput while hurting other ﬂows. Second,
when multiple resources can simultaneously be bottlenecked, this
solution can lead to oscillations that substantially lower the total
throughput and keep some ﬂows below their guaranteed share.
A second natural scheme, which can happen by default in soft-
ware router designs, is to perform fair sharing independently at
each resource. For example, packets might ﬁrst be processed by
the CPU, which is shared via stride scheduling [31], and then go
into an output link buffer served via fair queuing. Surprisingly, we
show that even though fair sharing for a single resource is strategy-
proof, composing per-resource fair schedulers this way is not.
Recently, a multi-resource allocation scheme that provides share
guarantees and strategy-proofness, called Dominant Resource Fair-
ness (DRF) [16], was proposed. We design a fair queueing algo-
rithm for multiple resources that achieves DRF allocations. The
main challenge we address is that existing algorithms for DRF pro-
vide fair sharing in space; given a cluster with much larger number
1Figure 1: Normalized resource usage of four middlebox func-
tions implemented in Click: basic forwarding, ﬂow monitoring,
redundancy elimination, and IPSec encryption.
of servers than users, they decide how many resources each user
should get on each server. In contrast, middleboxes require sharing
in time; given a small number of resources (e.g., NICs or CPUs)
that can each process only one packet at a time, the scheduler must
interleave packets to achieve the right resource shares over time.
Achieving DRF allocations in time is challenging, especially doing
so in a memoryless manner, i.e., a ﬂow should not be penalized for
having had a high resource share in the past when fewer ﬂows were
active [24]. This memoryless property is key to guaranteeing that
ﬂows cannot be starved in a work-conserving system.
We design a new queuing algorithm called Dominant Resource
Fair Queuing (DRFQ), which generalizes the concept of virtual
time from classical fair queuing [10, 24] to multiple resources that
are consumed at different rates over time. We evaluate DRFQ using
a Click [22] implementation and simulations, and we show that it
provides better isolation and throughput than existing schemes.
To summarize, our contributions in this work are three-fold:
1. We identify the problem of multi-resource fair queueing, which
is a generalization of traditional single-resource fair queueing.
2. We provide the ﬁrst analysis of two natural packet scheduling
schemes—bottleneck fairness and per-resource fairness—and
show that they suffer from problems including poor isolation,
oscillations, and manipulation.
3. We propose the ﬁrst multi-resource queuing algorithm that pro-
vides both share guarantees and strategy-proofness: Dominant
Resource Fair Queuing (DRFQ). DRFQ implements DRF allo-
cations in the time domain.
2. MOTIVATION
Others have observed that middleboxes and software routers can
bottleneck on any of CPU, memory bandwidth, and link bandwidth,
depending on the processing requirements of the trafﬁc. Dreger
et al.
report that CPU can be a bottleneck in the Bro intrusion
detection system [13]. They demonstrated that, at times, the CPU
can be overloaded to the extent that each second of incoming trafﬁc
requires 2.5 seconds of CPU processing. Argyraki et al. [8] found
that memory bandwidth can be a bottleneck in software routers,
especially when processing small packets. Finally, link bandwidth
can clearly be a bottleneck for ﬂows that need no processing. For
example, many middleboxes let encrypted SSL ﬂows pass through
without processing.
To conﬁrm and quantify these observations, we measured the re-
source footprints of several canonical middlebox applications im-
plemented in Click [22]. We developed a trace generator that takes
in real traces with full payloads [4] and analyzes the resource con-
sumption of Click modules using the Intel(R) Performance Counter
Monitor API [3]. Figure 1 shows the results for four applications.
Each application’s maximum resource consumption was normal-
ized to 1. We see that the resource consumption varies across mod-
Figure 2: Performing fair sharing based on a single resource
(NIC) fails to meet the share guarantee. In the steady-state pe-
riod from time 2–11, ﬂow 2 only gets a third of each resource.
ules: basic forwarding uses a higher relative fraction of link band-
width than of other resources, redundancy elimination bottlenecks
on memory bandwidth, and IPSec encryption is CPU-bound.
Many middleboxes already perform different functions for dif-
ferent trafﬁc (e.g., HTTP caching for some ﬂows and basic forward-
ing for others), and future software-deﬁned middlebox proposals
suggest consolidating more functions onto the same device [28,
27]. Moreover, further functionality is being incorporated into hard-
ware accelerators [30, 23, 5], increasing the resource diversity of
middleboxes. Thus, packet schedulers for middleboxes will need
to take into account ﬂows’ consumption across multiple resources.
Finally, we believe multi-resource scheduling to be important in
other contexts too. One such example is multi-tenant scheduling
in deep software stacks. For example, a distributed key-value store
might be layered on top of a distributed ﬁle system, which in turn
runs over the OS ﬁle system. Different layers in this stack can
bottleneck on different resources, and it is desirable to isolate the
resource consumption of different tenants’ requests. Another ex-
ample is virtual machine (VM) scheduling inside a hypervisor. Dif-
ferent VMs might consume different resources, so it is desirable to
fairly multiplex their access to physical resources.
3. BACKGROUND
Designing a packet scheduler for multiple resources turns out to
be non-trivial due to several problems that do not occur with one
resource [16]. In this section, we review these problems and pro-
vide background on the allocation scheme we ultimately build on,
DRF. In addition, given that our goal is to design a packet queuing
algorithm that achieves DRF, we cover background on fair queuing.
3.1 Challenges in Multi-Resource Scheduling
Previous work on DRF identiﬁes several problems that can occur
in multi-resource scheduling and shows that several simple schedul-
ing schemes lack key properties [16].
Share Guarantee: The essential property of fair queuing is isola-
tion. Fair queuing ensures that each of n ﬂows can get a guaranteed
n fraction of a resource (e.g., link bandwidth), regardless of the de-
1
mand of other ﬂows [24].1 Weighted fair queuing generalizes this
concept by assigning a weight wi to each ﬂow and guaranteeing
of the sole resource, where W
that ﬂow i can get at least
is the set of active ﬂows.
wiPj2W wj
We generalize this guarantee to multiple resources as follows:
Share Guarantee. A backlogged ﬂow with weight wi should
fraction of one of the resources it uses.
get at least
wiPj2W wj
1By “ﬂow,” we mean a set of packets deﬁned by a subset of header
ﬁelds. Administrators can choose which ﬁelds to use based on or-
ganizational policies, e.g., to enforce weighted fair shares across
users (based on IP addresses) or applications (based on ports).
p1 p1 p1 p1 p2 p2 p2 p2 p3 p3 p3 p3 p4 flow 1 CPU NIC time   0 1 3 2 4 5 6 7 9 8 10 11 flow 2 2Figure 3: Bottleneck fairness can be manipulated by users. In
(b), ﬂow 1 increases its CPU usage per packet to shift the bot-
tleneck to CPU, and thereby gets more bandwidth too.
Surprisingly, this property is not met by some natural schedulers.
As a strawman, consider a scheduler that only performs fair queue-
ing based on one speciﬁc resource. This may lead to some ﬂows
n of all resources, where n is the total number
receiving less than 1
of ﬂows. As an example, assume that there are two resources, CPU
and link bandwidth, and that each packet ﬁrst goes through a mod-
ule that uses the CPU, and thereafter is sent to the NIC. Assume
we have two ﬂows with resource proﬁles h2, 1i and h1, 1i; that is,
packets from ﬂow 1 each take 2 time units to be processed by the
CPU and 1 time unit to be sent on the link, while packets from
ﬂow 2 take 1 unit of both resources. If the system implements fair
queuing based on only link bandwidth, it will alternate sending one
packet from each ﬂow, resulting in equal allocation of link band-
width to the ﬂows (both ﬂows use one time unit of link bandwidth).
However, since there is more overall demand for the CPU, the CPU
will be fully utilized, while the network link will be underutilized
3 and 1
at times. As a result (see Figure 2), the ﬁrst ﬂow receives 2
3
of the two resources, respectively. But the second ﬂow only gets 1
3
on both resources, violating the share guarantee.
Strategy-Proofness: The multi-resource setting is vulnerable to
a new type of manipulation. Flows can manipulate the scheduler
to receive better service by artiﬁcially inﬂating their demand for
resources they do not need.
For example, a ﬂow might increase the CPU time required to
process it by sending smaller packets. Depending on the scheduler,
such manipulation can increase the ﬂow’s allocation across all re-
sources. We later show that in several natural schedulers, greedy
ﬂows can as much as double their share at the cost of other ﬂows.
These types of manipulations were not possible in single-resource
settings, and therefore received no attention in past literature. It is
important for multi-resource schedulers to be resistant to them, as
a system vulnerable to manipulation can incentivize users to waste
resources, ultimately leading to lower total goodput.
The following property discourages the above manipulations:
Strategy-proofness. A ﬂow should not be able to ﬁnish faster
by increasing the amount of resources required to process it.
As a concrete example, consider the scheduling scheme pro-
posed by Egi et al. [14], in which the middlebox determines which
resource is bottlenecked and divides that resource evenly between
the ﬂows. We refer to this approach as bottleneck fairness. Figure 3
shows how a ﬂow can manipulate its share by wasting resources.
In (a), there are three ﬂows with resource proﬁles h10, 1i, h10, 14i
and h10, 14i respectively. The bottleneck is the ﬁrst resource (link
bandwidth), so it is divided fairly, resulting in each ﬂow getting one
third of it. In (b), ﬂow 1 increases its resource proﬁle from h10, 1i
to h10, 7i. This shifts the bottleneck to the CPU, so the system
starts to schedule packets to equalize the ﬂows’ CPU usage. How-
ever, this gives ﬂow 1 a higher share of bandwidth as well, up from
3 to almost 1
2 . In similar examples with more ﬂows, ﬂow 1 can
1
almost double its share.
Figure 4: DRF allocation for jobs with resource proﬁles h4, 1i
and h1, 3i in a system with equal amounts of both resources.
Both jobs get 3
4 of their dominant resource.
We believe the networking domain to be particularly prone to
these types of manipulations, as peer-to-peer applications already
employ various techniques to increase their resource share [26].
Such an application could, for instance, dynamically adapt outgo-
ing packet sizes based on throughput gain, affecting the CPU con-
sumption of congested middleboxes.
3.2 Dominant Resource Fairness (DRF)
The recently proposed DRF allocation policy [16] achieves both
strategy-proofness and share guarantees.
DRF was designed for the datacenter environment, which we
brieﬂy recapitulate.
In this setting, the equivalent of a ﬂow is a
job, and the equivalent of a packet is a job’s task, executing on a
single machine. DRF deﬁnes the dominant resource of a job to be
the resource that it currently has the biggest share of. For exam-
ple, if a job has 20 CPUs and 10 GB of memory in a cluster with
100 CPUs and 40 GB of memory, the job’s dominant resource is
5 for CPU). A job’s
memory, as it is allocated 1
dominant share is simply its share of its dominant resource, e.g.,
4 in this example.
Informally, DRF provides the allocation that
1
“equalizes” the dominant shares of different users. More precisely,
DRF is the max-min fair allocation of dominant shares.
4 of it (compared to 1
Figure 4 shows an example, where two jobs run tasks with re-
source proﬁles h4 CPUs, 1 GBi and h1 CPU, 3 GBi in a cluster
with 2000 CPUs and 2000 GB of memory. In this case, job 1’s
dominant resource is CPU, and job 2’s dominant resource is mem-
ory. DRF allocates h1500 CPUs, 375 GBi of resources to job 1 and
h500 CPUs, 1500 GBi to job 2. This equalizes job 1’s and job 2’s
dominant shares while maximizing the allocations.
We have described the DRF allocation. Ghodsi et al. [16] pro-
vide a simple algorithm to achieve DRF allocations in space (i.e.,
given a cluster of machines, compute which resources on which
machines to assign to each user). We seek an algorithm that achieves
DRF allocations in time, multiplexing resources across incoming
packets. In Section 5, we describe this problem and provide a queu-
ing algorithm for DRF. The algorithm builds on concepts from fair
queuing, which we review next.
3.3 Fair Queuing in Routers
Fair Queuing (FQ) aims to implement max-min fair allocation
of a single resource using a ﬂuid-ﬂow model, in which the link
capacity is inﬁnitesimally divided across the backlogged ﬂows [10,
24]. In particular, FQ schedules packets in the order in which they
would ﬁnish in the ﬂuid-ﬂow system.
Virtual clock [33] was one of the ﬁrst schemes using a ﬂuid-ﬂow
model.
It, however, suffers from the problem that it can punish
a ﬂow that in the past got better service when fewer ﬂows were
active. Thus, it violates the following key property:
Memoryless scheduling. A ﬂow’s current share of resources