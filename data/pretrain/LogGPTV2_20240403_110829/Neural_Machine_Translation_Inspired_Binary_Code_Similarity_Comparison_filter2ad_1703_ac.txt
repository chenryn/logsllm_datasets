is measured as the distance of the two block embeddings.
Detailed Process. The inputs are two blocks, B1 and B2, repre-
1 ,··· , e(1)
sented as a sequence of instruction embeddings, (e(1)
T ),
and (e(2)
S ), respectively. Note that the sequences may
be of different lengths, i.e., |T| (cid:54)= |S|, and the sequence lengths
can vary from example to example; both are handled by the
model. An LSTM cell analyzes an input vector coming from
either the input embeddings or the precedent step and updates
its hidden state at each time step. Each cell contains four
components (which are real-valued vectors): a memory state
c, an output gate o determining how the memory state affects
other units, and an input gate i (and a forget gate f, resp.) that
controls what gets stored in (and omitted from, resp.) memory.
For example, an LSTM cell at the ﬁrst layer in LSTM1 updates
its hidden state at the time step t via Equations 3–8:
1 ,··· , e(2)
t = sigmoid(Wie(1)
i(1)
t = sigmoid(Wf e(1)
f(1)
(1) = tanh(Wce(1)
(cid:101)ct
t (cid:12)(cid:101)ct
c(1)
t = i(1)
t−1 + vi)
t−1 + vf )
t + Uix(1)
t + Uf x(1)
t + Ucx(1)
(1) + f(1)
t (cid:12)(cid:101)ct
(1)
t−1 + vc)
o(1)
t = sigmoid(Woe(1)
t + Uox(1)
t−1 + vo)
(3)
(4)
(5)
(6)
(7)
(8)
where (cid:12) denotes Hadamard (element-wise) product; Wi, Wf ,
x(1)
t = o(1)
)
t (cid:12)tanh(c(1)
t
movq %rsi,80(%rsp)addq %rax,%rsiaddq %rax,$-1xorl %edx,%edxdivq %rsimovq %rdx,96(%rsp)cmpq %rax,16(%rdx)jbe  .LBB2_68adds r1, r2, r1adc r7, r3, r0subs r0, r1, #1sbc r1, r7, #0bl __udivdi3ldr r3, [sp, #60]ldr r2, [r3, #16]ldr r3, [r3, #20]subs r2, r2, r0sbcs r2, r3, r1bhs .LBB2_120Source codenumblocks = (tmp_len+blocksize-1)/blocksize;if(numblocks > pre_comp->numblocks)X86-64 assemblyARM assemblyLayer 1Layer nB1B2Fig. 6: LLVM architecture. The basic-block boundary annotator
is added into the backends of different architectures.
Wc, Wo, Ui, Uf , Uc, Uo are weight matrices; and vi, vf , vc,
vo are bias vectors; they are learned during training. The reader
is referred to [24] for more details.
At the last time step T , the last hidden state at the last layer
provides a vector h(1)
S ), which is the embedding of
B1 (resp. B2). We use the Manhattan distance (∈ [0, 1]) which
is suggested by [48] to measure the similarity of B1 and B2 as
showed in Equation 9:
T (resp. h(2)
Sim(B1, B2) = exp(− (cid:107) h(1)
(9)
To train the network parameters, we use stochastic gradient
T − h(2)
S (cid:107)1)
descent (SGD) to minimize the loss function:
N(cid:88)
i=1
min
Wi,Wf ,...,vo
(yi − Sim(Bi
1, Bi
2))2
(10)
where yi is the similarity ground truth of the pair ,
and N the number of basic block pairs in the training dataset.
In the end, once the Area Under the Curve (AUC) value
converges, the training process terminates, and the trained cross-
lingual basic-block embedding model is capable of encoding
an input binary block to an embedding capturing the semantics
information of the block that is suitable for similarity detection.
1, Bi
C. Challenges
There are two main challenges for learning block embed-
dings. First, in order to train, validate and test the basic-block
embedding model, a large dataset containing labeled similar
and dissimilar block pairs is needed. Unlike prior work [65]
that builds the dataset of similar and dissimilar function pairs
by using the function names to establish the ground truth about
the function similarity, it is very challenging to establish the
ground truth for basic blocks because: (a) no name is available
to indicate whether two basic blocks are similar or not, and
(b) even if two basic blocks have been compiled from two
pieces of code, they may happen to be equivalent or similar,
and therefore, it would be incorrect to label them as dissimilar.
Second, many hyperparameters need to be determined
to maximize the model performance. The parameter values
selected for NMT are not necessarily applicable to our model,
and need to be comprehensively examined (Section VII-F).
D. Building Dataset
1) Generating Similar Basic-Block Pairs: We consider two
basic blocks of different ISAs that have been compiled from
the same piece of source code as equivalent. To establish the
ground truth about the block similarity, we modify the backends
of various architectures in the LLVM compiler. As shown in
Figure 6, the LLVM compiler consists of various frontends (that
compile source code into a uniform Intermediate Representation
(IR)),
the middle-end optimizer, and various architecture-
dependent backends (that generate the corresponding assembly
code). We modify the backends to add the basic-block boundary
annotator, which not only clearly marks the boundaries of
blocks, but also annotates a unique ID for each generated
assembly block in a way that all assembly blocks compiled
from the same IR block (i.e., the same piece of source code),
regardless of their architectures, will obtain the same ID.
To this end, we collect various open-sourced software
projects, and feed the source code into the modiﬁed LLVM
compiler to generate a large number of basic blocks for different
architectures. After preprocessing (Section IV-C) and data
deduplication, for each basic block Bx86, the basic block BARM
with the same ID is sampled to construct one training example
. By continually sampling, we can collect a
large number of similar basic-block pairs.
2) Generating Dissimilar Basic-Block Pairs: While two ba-
sic blocks with the same ID are always semantically equivalent,
two blocks with different IDs may not necessarily be dissimilar,
as they may happen to be be equivalent.
1
of ARM is equivalent to a block Bx86
To address this issue, we make use of n-gram to measure the
text similarity between two basic blocks compiled for the same
architecture at the same optimization level. A low text similarity
score indicates two basic blocks are dissimilar. Next, assume a
block BARM
of x86 (they
of x86 is dissimilar
have the same ID); and another block Bx86
according to the n-gram similarity comparison. Then,
to Bx86
, are regarded as dissimilar,
the two blocks, BARM
and the instance  is added to the dataset.
1
Our experiments set n as 4 and the similarity threshold as 0.5;
that is, if two blocks, through this procedure, have a similarity
score smaller than 0.5, they are labeled as dissimilar. This way,
we can obtain a large number of dissimilar basic-block pairs
across architectures.
and Bx86
2
, Bx86
1
2
1
2
1
VI. PATH/CODE COMPONENT SIMILARITY COMPARISON
Detecting similar code components is an important problem.
Existing work either can only work on a single architecture [60],
[21], [37], [44], [58], [56], [27], or can compare a pair of
functions across architectures [52], [18], [19], [65]. However,
as a critical code part may be inserted inside a function to avoid
detection [28], [27], [37], how to resolve the cross-archite code
containment problem is a new and more challenging problem.
We propose to decompose the CFG of the query code
component Q into multiple paths. For each path from Q,
we compare it to many paths from the target program T ,
to calculate a path similarity score by adopting the Longest
Common Subsequence (LCS) dynamic programming algorithm
with basic blocks as sequence elements. By trying more than
one path, we can use the path similarity scores collectively to
detect whether a component in T is similar to Q.
A. Path Similarity Comparison
A linearly independent path is a path that introduces at least
one new node (i.e., basic block) that is not included in any
7
Clang C/C++FrontendLLVM OptimizerIRLLVM X86 BackendBasic Block Boundary AnnotatorIRLLVM ARM Backend...llvm-gcc FrontendGHCFrontend...previous linearly independent paths [62]. Once the starting block
of Q and several candidate starting blocks of T are identiﬁed
(presented in Section VI-B), the next step is to explore paths
to calculate a path similarity score. For Q, we select a set of
linearly independent paths from the starting block. We ﬁrst
unroll each loop in Q once, and adopt the Depth First Search
algorithm to ﬁnd a set of linearly independent paths.
For each linearly independent path of Q, we need to ﬁnd the
highest similarity score between the query path and the many
paths of T . To this end, we apply a recently proposed code
similarity comparison approach, called CoP [37] (it is powerful
for handling many types of obfuscations but can only handle
code components of the same architecture). CoP combines
the LCS algorithm and basic-block similarity comparison to
compute the LCS of semantically equivalent basic blocks
(SEBB). However, CoP’s basic-block similarity comparison
relies on symbolic execution and theorem proving, which is
very computationally expensive [40]. On the contrary, our work
adopts techniques in NMT to signiﬁcantly speed up basic-block
similarity comparison, and hence is much more scalable for
analyzing large codebases.
Here we brieﬂy introduce how CoP applies LCS to detect
path similarity. It adopts breadth-ﬁrst search in the inter-
procedural CFG of the target program T , combined with the
LCS dynamic programming to compute the highest score of
the LCS of SEBB. For each step in the breadth-ﬁrst dynamic
programming algorithm, the LCS is kept as the “longest path”
computed so far for a block in the query path. The LCS score
of the last block in the query path is the highest LCS score,
and is used to compute a path similarity score. Deﬁnition 2
gives a high-level description of a path similarity score.
Deﬁnition 2: (Path Similarity Score) Given a linearly in-
dependent path P from the query code component, and a
n} be all of the linearly
1, . . . ,P t
target program T . Let Γ = {P t
independent paths of T , and |LCS(P,P t
i )| be the length of
the LCS of SEBB between P and P t
i , P t
i ∈ Γ. Then, the path
similarity score for P is deﬁned as
ψ(P, T ) =
maxP t
i ∈Γ |LCS(P,P t
i )|
|P|
B. Component Similarity Comparison
Challenge. The location that the code component gets embed-
ded into the containing target program is unknown, and it is
possible for it to be inserted into the middle of a function. It
is important to determine the correct starting points so that
the path exploration is not misled to irrelevant code parts of
the target program. This is a unique challenge compared to
function-level code similarity comparison.
Idea. We look for the starting blocks in the manner as follows.
First, the embeddings of all basic blocks of the target program T
are stored in an locality-sensitive hashing database for efﬁcient
online search. Next, we begin with the ﬁrst basic block in the
query code component Q as the starting block, and search
in the database to ﬁnd a semantically equivalent basic block
(SEBB) from the target program T . If we ﬁnd one or several
SEBBs, we proceed with the path exploration (Section VI-A)
on each of them. Otherwise, we choose another block from Q
8
as the starting block [37], and repeat the process until the last
block of Q is checked.
Component similarity score. We select a set of linearly
independent paths from Q, and compute a path similarity score
for each linearly independent path. Next, we assign a weight
to each path similarity score according to the length of the
corresponding query path. The ﬁnal component similarity score
is the weighted average score.
Summary. By integrating our cross-lingual basic-block em-
bedding model with an existing approach [37], we have
come up with an effective and efﬁcient solution to cross-
architecture code-component similarity comparison. Moreover,
it demonstrates how the efﬁcient, precise and scalable basic-
block embedding model can beneﬁt many other systems [21],
[37], [44] that rely on basic-block similarity comparison.
VII. EVALUATION
We evaluate INNEREYE in terms of its accuracy, efﬁciency,
and scalability. First, we describe the experimental settings
(Section VII-A) and discuss the datasets used in our evaluation
(Section VII-B). Next, we examine the impact of preprocessing
on out-of-vocabulary instructions (Section VII-C) and the
quality of the instruction embedding model (Section VII-D). We
then evaluate whether INNEREYE-BB can successfully detect
the similarity of blocks compiled for different architectures
(Problem I). We evaluate its accuracy and efﬁciency (Sec-
tions VII-E and VII-G), and discuss hyperparameter selection
(Section VII-F). We also compare it with a machine learning-
based basic-block comparison approach that uses a set of
manually selected features [19], [65] (Section VII-E3). Finally,
we present three real-world case studies demonstrating how
INNEREYE-CC can be applied for cross-architecture code
component search and cryptographic function search under
realistic conditions (Problem II) in Section VII-H.
A. Experimental Settings
We adopt word2vec [42] to learn instruction embed-
dings, and implemented our cross-lingual basic-block embed-
ding model in Python using the Keras [9] platform with
TensorFlow [1] as backend. Keras provides a large number
of high-level neural network APIs and can run on top of
TensorFlow. Like the work CoP [37], we require that the
selected linearly independent paths cover at least 80% of the
basic blocks in each query code component; the largest number
of the selected linearly independent paths in our evaluation is 47.
INNEREYE-CC (the LCS algorithm with path exploration) is
implemented in the BAP framework [3] which constructs CFGs
and call graph and builds the inter-procedural CFG. INNEREYE-
CC queries the block embeddings (computed by INNEREYE-
BB) stored in an LSH database. The experiments are performed
on a computer running the Ubuntu 14.04 operating system with
a 64-bit 2.7 GHz Intel® Core(TM) i7 CPU and 32 GB RAM
without GPUs. The training and testing are expected to be
signiﬁcantly accelerated if GPUs are used.
B. Dataset
We ﬁrst describe the dataset (Dataset I), as shown in Table I,
used to evaluate the cross-lingual basic-block embedding
model (INNEREYE-BB). All basic-block pairs in the dataset
TABLE I: The number of basic-block pairs in the training, validation and testing datasets.
Sim.
35,416