deployment process automated but to trigger the actual deployment manually in a one-click process.
This is known as Continuous Delivery, and it has all the advantages of Continuous Deployment without
the disadvantages. Variations on Continuous Delivery may also involve automatically deploying code
to certain environments (such as test and QA) while using a manual one-click deployment for the
other environments (such as UAT and Production). The most important distinguishing characteristic of
Continuous Delivery is that any and every successful build that has passed all the relevant automated
tests and quality gates can potentially be deployed into production via a fully automated one-click
process and be in the hands of the end-user within minutes. However, the process is not automatic: it is
the business, rather than IT, that decides the best time to deliver the latest changes.
Both Continuous Deployment and Continuous Delivery are rightly considered to represent a very high
level of maturity in terms of build automation and SDLC practices. These techniques cannot exist
without an extremely solid set of automated tests. Nor can they exist without a CI environment and
a robust built pipeline—indeed it typically represents the final stage and goal of the build pipeline.
However, considering the significant advantages that Continuous Deployment/Delivery can bring to an
organization, it is a worthy goal. During the remainder of this chapter, we will use the general term of
“Continuous Deployment” to refer to both Continuous Deployment and Continuous Delivery. Indeed,
Continuous Delivery can be viewed as Continuous Deployment with the final step (deployment into
production) being a manual one dictated by the business rather than the development team.
12.2. Implementing Automated and Continuous
Deployment
In its most elementary form, Automated Deployment can be as simple as writing your own scripts to
deploy your application to a particular server. The main advantage of a scripted solution is simplicity and
ease of configuration. However, a simple scripted approach may run into limits if you need to perform
more advanced deployment activities, such as installing software on a machine or rebooting the server.
For more advanced scenarios, you may need to use a more sophisticated deployment/configuration
management solution such as Puppet or Chef.
12.2.1. The Deployment Script
An essential part of any Automated Deployment initiative is a scriptable deployment process. While
this may seem obvious, there are still many organizations where deployment remains a cumbersome,
complicated and labor-intensive process, including manual file copying, manual script execution, hand-
written deployment notes, and so forth. The good news is that, in general, it does not have to be this
way, and, with a little work, it is usually possible to write a script of some sort to automate most, if
not all, of the process.
The complexity of a deployment script varies enormously from application to application. For a simple
website, a deployment script may be as simple as resyncing a directory on the target server. Many Java
application servers have Ant or Maven plugins that can be used to deploy applications. For a more
complicated infrastructure, deployment may involve deploying several applications and services across
multiple clustered servers in a precisely coordinated manner. Most deployment processes tend to fall
somewhere between these extremes.
12.2.2. Database Updates
Deploying your app to the application server is often only one part of the puzzle. Databases, relational
or otherwise, almost always play a central role in any application architecture. Of course, ideally, your
database would be perfect from the start, but this is rarely the case in the real world. Indeed, when you
update your application, you will generally also need to update one or more databases as well.
Database updates are usually more difficult to manage smoothly than application updates, as both the
structure and the contents of the database may be impacted. However, managing database updates is
a critical part of both the development and the deployment process, and deserves some reflection and
planning.
Some application frameworks, such as Ruby on Rails and Hibernate, can manage structural database
changes automatically to some extent. Using these frameworks, you can typically specify if you want to
create a new database schema from scratch at each update, or whether you which to update the database
326
schema while conserving the existing data. While this sounds useful in theory, in fact it is very limited
for anything other than noncritical development environments. In particular, these tools do not handle
data migration well. For example, if you rename a column in your database, the update process will
simply create a new column: it will not copy the data from the old column into the new column, nor
will it remove the old column from the updated table.
Fortunately, this is not the only approach you can use. Another tool that attempts to tackle the thorny
problem of database updates is Liquibase1. Liquibase is an open source tool that can help manage and
organize upgrade paths between versions of a database using a high-level approach.
Liquibase works by keeping a record of database updates applied in a table in the database, so that it is
easy to bring any target database to the correct state for a given version of the application. As a result,
you don’t need to worry about running the same update script twice—Liquibase will only apply the
update scripts that have not already been applied to your database. Liquibase is also capable of rolling
back changes, at least for certain types of changes. However, since this will not work for every change
(for example, data in a deleted table cannot be restored), it is best not to place too much faith in this
particular feature.
In Liquibase, you keep track of database changes as a set of “change sets,” each of which represents
the database update in a database-neutral XML format. Change sets can represent any changes you
would make in a database, from adding and deleting tables, to creating or updating columns, indexes
and foreign keys:
Change sets can also reflect modifications to existing tables. For example, the following change set
represents the renaming of a column:
1 http://www.liquibase.org/
327
Since this representation records the semantic nature of the change, Liquibase is capable of handling
both the schema updates and data migration associated with this change correctly.
Liquibase can also handle updates to the contents of your database, as well as to its structure. For
example, the following change set inserts a new row of data into a table:
Each changeset has an ID and an author, which makes it easier to keep track of who made a particular
change and reduces the risk of conflict. Developers can test their change sets on their own database
schema, and then commit them to version control once they are ready. The next obvious step is to
configure a Jenkins build to run the Liquibase updates against the appropriate database automatically
before any integration tests or application deployment is done, usually as part of the ordinary project
build script.
Liquibase integrates well into the build process—it can be executed from the command line, or integrated
into an Ant or Maven build script. Using Maven, for example, you can configure the Maven Liquibase
Plugin as shown here:
org.liquibase
liquibase-plugin
1.9.3.0
true
src/main/resources/liquibase.properties
...
Using Liquibase with Maven this way, you could update a given target database to the current schema
using this plugin:
$ mvn liquibase:update
The default database connection details are specified in the src/main/resources/
liquibase.properties file, and might look something like this:
changeLogFile = changelog.xml
328
driver = com.mysql.jdbc.Driver
url = jdbc:mysql://localhost/ebank
username = scott
password = tiger
verbose = true
dropFirst = false
However you can override any of these properties from the command line, which makes it easy to set
up a Jenkins build to update different databases.
Other similar commands let you generate an SQL script (if you need to submit it to your local DBA for
approval, for example), or rollback to a previous version of the schema.
This is of course just one example of a possible approach. Other teams prefer to manually maintain
a series of SQL update scripts, or write their own in-house solutions. The important thing is to have
a solution that you can use reliably and reproducibly to update different databases to the correct state
when deploying your applications.
12.2.3. Smoke Tests
Any serious automated deployment needs to be followed up by a series of automated smoke tests. A
subset of the automated acceptance tests can be a good candidate for smoke tests. Smoke tests should
be unobtrusive and relatively fast. They should be safe to run in a production environment, which may
restrict the number of modifications the test cases can do in the system.
12.2.4. Rolling Back Changes
Another important aspect to consider when setting up Automated Deployment is how to back out
if something goes wrong, particularly if you are thinking of implementing Continuous Deployment.
Indeed, it is critical to be able to roll back to the previous version if required.
How you will do this depends a lot on your application. While it is relatively straight-forward to redeploy
a previous version of an application using Jenkins (we will look at a technique to do this further on in
this chapter), the application is often not the only player in the game. In particular, you will need to
consider how to restore your database to a previous state.
We saw how it is possible to use Liquibase to manage database updates, and of course many other
strategies are also possible. However rolling back a database version presents its own challenges.
Liquibase, for example, lets you revert some, but not all changes to the database structure. However
data lost (in dropped tables, for example) cannot be recovered using Liquibase alone.
The most reliable way to revert your database to a previous state is probably to take a snapshot of the
database just before the upgrade, and use this snapshot to restore the database to its previous state. One
effective strategy is to automate this process in Jenkins in the deployment build job, and then to save
both the database snapshot and the deployable binary file as artifacts. This way, you can easily restore
the database using the saved snapshot and then redeploy the application using the saved binary. We will
look at an example of this strategy in action further on in this chapter.
329
12.3. Deploying to an Application Server
Jenkins provides plugins to help you deploy your application to a number of commonly-used application
servers. The Deploy plugin lets you deploy to Tomcat, JBoss, and GlassFish. And the Deploy Websphere
plugin tries to cater for the particularities of IBM WebSphere Application Server.
For other application servers, you will typically have to integrate the deployment process into your build
scripts, or resort to custom scripts to deploy your application. For other languages, too, your deployment
process will vary, but it will often involve some use of shell scripting. For example, for a Ruby on Rails
application, you may use a tool like Capistrano or Chef, or simply a shell script. For a PHP application,
an FTP or SCP file transfer may suffice.
Let’s first look at some strategies for deploying your Java applications to an application server.
This is known as a hot-deploy, where the application is deployed onto a running server. This is generally
a fast and efficient way of getting your application online. However, depending on your application and
on your application server, this approach has been known to result in memory leaks or resource locking
issues—older versions of Tomcat, for example, were particularly well-known for this. If you run into
this sort of issue, you may have to force the application to restart after each deployment, or possibly
schedule a nightly restart of the application server on your test machine.
12.3.1. Deploying a Java Application
In this section we will look at an example of how to deploy your Java web or JEE application to an
application server such as Tomcat, JBoss, or GlassFish.
One of the fundamental principles of automated deployment is to reuse your binaries. It is inefficient,
and potentially unreliable, to rebuild your application during the deployment process. Indeed, imagine
that you run a series of unit and integration tests against a particular version of your application, before
deploying it to a test environment for further testing. If you rebuild the binary before deploying it to the
test environment, the source code may have changed since the original revision, which means you may
not know exactly what you are deploying.
A more efficient process is to reuse the binaries generated by a previous build. For example, you may
configure a build job to run unit and integration tests before generating a deployable binary file (typically
a WAR or EAR file). You can do this very effectively using the Copy Artifact plugin (see Section 10.7.2,
“Copying Artifacts”). This plugin lets you copy an artifact from another build job workspace into
the current build job workspace. This, when combined with a normal build trigger or with the Build
Promotion plugin, lets you deploy precisely the binary file that you built and tested in the previous phase.
This approach does put some constraints on the way you build your application. In particular, any
environment-specific configuration must be externalized to the application; JDBC connections or other
such configuration details should not be defined in configuration files embedded in your WAR file, for
example, but rather be defined using JDNI or in an externalized properties file. If this is not the case, you
330
may need to build from a given SCM revision, as discussed for Subversion in Section 10.2.4, “Building
from a Subversion Tag”.
12.3.1.1. Using the Deploy plugin
If you are deploying to a Tomcat, JBoss, or GlassFish server, the most useful tool at your disposition will
probably be the Deploy plugin. This plugin makes it relatively straightforward to integrate deployment
to these platforms into your Jenkins build process. If you are deploying to IBM Websphere, you can
use the Websphere Deploy plugin to similar ends.
Let’s see how this plugin works in action, using the simple automated build and deployment pipeline
illustrated in Figure 12.1, “A simple automated deployment pipeline”.
Figure 12.1. A simple automated deployment pipeline
Here, the default build (gameoflife-default) runs the unit and integration tests, and builds a
deployable binary in the form of a WAR file. The metrics build (gameoflife-metrics) runs
additional checks regarding coding standards and code coverage. If both these builds are successful,
the application will be automatically deployed to the test environment by the gameoflife-deploy-
to-test build job.
In the gameoflife-deploy-to-test build job, we use the Copy Artifact plugin to retrieve the WAR file
generated in the gameoflife-default build job and copies it into the current build job’s workspace
(see Figure 12.2, “Copying the binary artifact to be deployed”).
Figure 12.2. Copying the binary artifact to be deployed
331
Next, we use the Deploy plugin to deploy the WAR file to the test server. Of course it is generally
possible, and not too difficult, to write a hand-rolled deployment script to get your application on to
your application server. In some cases, this may be your only option. However, if a Jenkins plugin
exists for your application server, it can simplify things considerably to use it. If you are deploying to
Tomcat, JBoss, or GlassFish, the Deploy plugin may work for you. This plugin uses Cargo to connect
to your application server and deploy (or redeploy) your application. Just select the target server type,
and specify the server’s URL along with the username and password of a user with deployment rights
(see Figure 12.3, “Deploying to Tomcat using the Deploy Plugin”).
Figure 12.3. Deploying to Tomcat using the Deploy Plugin
This is known as a hot-deploy, where the application is deployed onto a running server. This is generally
a fast and efficient way of getting your application online, and should be the preferred solution because
of its speed convenience. However, depending on your application and on your application server,
this approach has been known to result in memory leaks or resource locking issues—older versions of
Tomcat, for example, were particularly well-known for this. If you run into this sort of issue, you may
have to force the application to restart after each deployment, or possibly schedule a nightly restart of
the application server on your test machine.
12.3.1.2. Redeploying a specific version
When you deploy your application automatically or continually, it becomes of critical importance to
precisely identify the version of the application currently deployed. There are a several ways you can
do this, which vary essentially in the role Jenkins plays in the build/deployment architecture.
Some teams use Jenkins as the central place of truth, where artifacts are both built and stored for future
reference. If you store your deployable artifacts on Jenkins, then it may make perfect sense to deploy
your artifacts directly from your Jenkins instance. This is not hard to do: in the next section we will look
at how to do this using a combination of the Copy Artifacts, Deploy, and Parameterized Trigger plugins.
Alternatively, if you are using an Enterprise repository such as Nexus or Artifactory to store your
artifacts, then this repository should act as the central point of reference: Jenkins should build and deploy
artifacts to your central repository, and then deploy them from there. This is typically the case if you
are using Maven as your build tool, but teams using tools like Gradle or Ivy may also use this approach.
Repository managers such as Nexus and Artifactory, particularly in their commercial editions, make
this strategy easier to implement by providing features such as build promotion and staging repositories
that help you manage the release state of your artifacts.
Let’s look at how you might implement each of these strategies using Jenkins.
332
12.3.1.3. Deploying a version from a previous Jenkins build
Redeploying a previously-deployed artifact in Jenkins is relatively straightforward. In Section 12.3.1.1,