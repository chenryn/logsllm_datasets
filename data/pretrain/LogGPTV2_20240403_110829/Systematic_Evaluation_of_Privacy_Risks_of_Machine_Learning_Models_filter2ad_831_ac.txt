vReg) [31]. As shown in Figure 2, the defended Purchase100
classiﬁer should be compared to the undefended model with
fewer training steps and similar accuracy.
3.2.2 Adaptive attacks
There always exists an arms race between privacy attacks and
defenses for machine learning models. When evaluating the
defense performance, it is critical to put the adversary into
the last step, i.e., the adversary knows the defense mechanism
and performs adaptive attacks against the defended models. A
perfect defense performance with non-adaptive attacks does
not mean that the defense approach is effective [2, 5, 15].
Speciﬁcally for defenses proposed against membership in-
ference attacks, we should consider that the adversary knows
the defense mechanism such that he or she can train shadow
models following the defense method. From these defended
shadow models, the adversary then learns an attack classiﬁer
or sets threshold values for metric-based attacks, and ﬁnally
performs attacks on the defended target model.
3.3 Experiment results
We ﬁrst re-evaluate the effectiveness of two membership in-
ference defenses [20, 31], and then re-evaluate the white-box
membership inference attacks proposed by Nasr et al. [32].
Following prior work [41, 44, 48], we sample the input (x,y)
from either the target model’s training set or test set with an
equal 0.5 probability to maximize the uncertainty of member-
ship inference attacks. Thus, the random guessing strategy
results in a 50% membership inference attack accuracy.
3.3.1 Datasets
Purchase100 This dataset is based on Kaggle’s Acquire Val-
ued Shoppers Challenge,1 which contains shopping records
of several thousand individuals. We obtain a simpliﬁed and
preprocessed purchase dataset provided by Shokri et al. [41].
The dataset has 197,324 data samples with 600 binary fea-
tures. Each feature corresponds to a product and represents
whether the individual has purchased it or not. All data sam-
ples are clustered into 100 classes representing different pur-
chase styles. The classiﬁcation task is to predict the purchase
style based on the 600 binary features. We follow Nasr et
al. [31,32] to use 10% data samples (19,732) to train a model.
Texas100 This dataset is based on the Hospital Discharge
Data public use ﬁles with patients’ information released by
the Texas Department of State Health Services.2 Each data
record contains the external causes of injury (e.g., suicide,
1https://www.kaggle.com/c/acquire-valued-shoppers-challenge
2https://www.dshs.texas.gov/THCIC/Hospitals/Download.
shtm
drug misuse), the diagnosis (e.g., schizophrenia), the proce-
dures the patient underwent (e.g., surgery) and some generic
information (e.g., gender, age, race). We obtain a simpliﬁed
and preprocessed Texas dataset provided by Shokri et al. [41].
The classiﬁcation task is to predict the patient’s main proce-
dure based on the patient’s information. The dataset focuses
on 100 most frequent procedures, resulting in 67,330 data
samples with 6,170 binary features. Following previous pa-
pers [20, 31, 32], we use 10,000 data samples to train a model.
Location30 This dataset is based on Foursquare dataset,3
which contains location “check-in” records of several thou-
sand individuals. We obtain a simpliﬁed and preprocessed
Location dataset provided by Shokri et al. [41]. The dataset
contains 5,010 data samples with with 446 binary features.
Each feature corresponds to a certain region or location
type and represents whether the individual has visited the
region/location or not. All data samples are clustered into 30
classes representing different geosocial types. The classiﬁ-
cation task is to predict the geosocial type based on the 466
binary features. Following Jia et al. [20], we use 1,000 data
samples to train a model.
CIFAR100 This is a major benchmark dataset for image clas-
siﬁcation [21]. It is composed of 32×32 color images in 100
classes, with 600 images per class. For each class label, 500
images are used as training samples, and the remaining 100
images are used as test samples.
We choose these datasets for fair comparison with prior
work [20, 31, 32]. Since all datasets except CIFAR100 are
binary datasets, we also provide attack results with more com-
plex datasets in Appendix A, where our benchmark attacks
achieve higher attack success than NN-based attacks.
3.3.2 Re-evaluating adversarial regularization [31]
We follow Nasr et al. [31] to train both defended and unde-
fended classiﬁers on Purchase100 and Texas100 datasets. For
both datasets, the model architecture is a fully connected neu-
ral network with 4 hidden layers. The numbers of neurons
for hidden layers are 1024, 512, 256, and 128, respectively.
All hidden layers use hyperbolic tangent (Tanh) as the activa-
tion function. We note that the defense method of adversarial
regularization [31] incurs accuracy drop. After applying the
defense, the test accuracy drops from 80.9% to 76.6% on
the Purchase100 dataset, and from 52.3% to 46.4% on the
Texas100 dataset. As we discuss in Section 3.2.1, to further
evaluate the effectiveness of adversarial regularization [31],
we also obtain models with early stopping by saving the unde-
fended models in every training epoch and picking the saved
epochs with similar accuracy performance as defended mod-
els. Table 2 presents the membership inference attack results.
From Table 2, we can see that the defended models are
still vulnerable to membership inference attacks, indicat-
3https://sites.google.com/site/yangdingqi/home/
foursquare-dataset
USENIX Association
30th USENIX Security Symposium    2621
Table 2: Benchmarking the effectiveness of using adversary regularization [31] as defense against membership inference attacks.
We can see that the defended models are still vulnerable to membership inference attacks.
Model Performance
using
defense [31]?
training
acc
test
acc
attack acc
by [31]
Membership Inference Attacks
attack acc
attack acc
(Icorr)
attack acc
(Iconf)
(Ientr)
attack acc
(IMentr)
67.1%
59.5%
59.1%
67.7%
58.6%
59.5%
dataset
Purchase100
Purchase100
no
yes
99.8% 80.9%
92.7% 76.6%
67.6%
51.6%
Purchase100
early stopping
92.9% 76.4%
N.A.
Texas100
Texas100
no
yes
81.0% 52.3%
56.6% 46.4%
Texas100
early stopping
59.3% 47.9%
63.0%
51.0%
N.A.
59.5%
58.1%
58.2%
64.4%
55.1%
55.7%
67.1%
59.4%
59.2%
67.8%
58.6%
59.4%
65.7%
55.8%
55.9%
60.2%
53.5%
54.0%
ing the necessity of our metric-based benchmark attacks. We
achieve 59.5% and 58.6% attack accuracy on the defended
Purchase100 classiﬁer and the defended Texas100 classiﬁer
with our benchmark attacks, signiﬁcantly larger than 51.6%
and 51.0% as reported by Nasr et al. [31]. Furthermore, on
all models except the undefended Purchase100 classiﬁer, the
largest attack accuracy achieved by benchmark attacks is
larger than that of NN based attacks used in Nasr et al. [31].
Note that the defense method provides limited mitigation of
privacy risks: it reduces attack accuracy from around 67%
to around 59% on tested models. We also ﬁnd that our new
attack based on the modiﬁed entropy (IMentr) always out-
performs the conventional entropy based attack (Ientr). It
is also very competitive among all benchmark attacks.
From Table 2, we also surprisingly ﬁnd that adversarial
regularization [31] is no better than our early stopping
benchmark method: with early stopping, the undefended
Purchase100 classiﬁer and the undefended Texas100 classi-
ﬁer have the attack accuracy of 59.2% and 59.5%, which are
quite close to those of defended models. Therefore, when
evaluating the effectiveness of a future defense mechanism
that trades lower model accuracy for lower membership in-
ference risk, we argue to compare the defended model to the
naturally trained model with early stopping for a fair compar-
ison. We emphasize that our early stopping baseline can be
calibrated to achieve similar model accuracy as the defended
model. In contrast, the adversarial regularization approach
may have a model accuracy which is different from the de-
fended model under consideration, and will thus not represent
a fair comparison.
To show the attack improvement yielded by our class-
dependent thresholding technique, we compare with metric-
based attacks when the same threshold is applied to all class
labels. Table 3 shows the results on Texas100 classiﬁers with-
out defense, with AdvReg [31], and with early stopping. We
can see that with the class-dependent thresholding tech-
nique, we increase the attack accuracy by 1% – 4%.
Table 3: Comparing attack performance between conven-
tional class-independent thresholding attacks and our class-
dependent thresholding attacks.
attack methods
defense methods for Texas100 classiﬁer
no defense AdvReg [31]
early stopping
(class-independent)
(class-dependent)
(class-independent)
(class-dependent)
Iconf
Iconf
Ientr
Ientr
IMentr
IMentr
(class-independent)
(class-dependent)
64.7%
67.8%
58.3%
60.2%
64.8%
67.7%
55.5%
58.6%
52.9%
53.5%
55.4%
58.6%
55.8%
59.4%
53.2%
54.0%
55.9%
59.5%
3.3.3 Re-evaluating MemGuard [20]
We follow Jia et al. [20] to train classiﬁers on Location30 and
Texas100 datasets. For both datasets, the model architecture
is a fully connected neural network with 4 hidden layers.
The numbers of neurons for hidden layers are 1024, 512,
256, and 128, respectively. All hidden layers use rectiﬁed
linear unit (ReLU) as the activation function. MemGuard [20]
does not change the accuracy performance, so the comparison
with early stopping benchmark is not applicable. Table 4
lists the attack accuracy on both undefended and defended
models, with attack methods in Jia et al. [20] and our metric-
based benchmark attack methods. In fact, Jia et al. [20] use
6 different NN attack classiﬁers to measure the privacy risks,
and we pick the highest attack accuracy among them.
From Table 4, we again emphasize the necessity of our
benchmark attacks by showing that the defended models
still have high membership inference accuracy: 69.1% on
the defended Location30 classiﬁer and 74.2% on the defended
2622    30th USENIX Security Symposium
USENIX Association
Table 4: Benchmarking the effectiveness of using MemGuard [20] as defense against membership inference attacks. We can see
that the defended models are still vulnerable to membership inference attacks.
dataset
Location30
Location30
Texas100
Texas100
Model Performance
using
defense [20]?
training
acc
test
acc
attack acc
by [20]
Membership Inference Attacks
attack acc
attack acc
(Icorr)
attack acc
(Iconf)
(Ientr)
no
yes
no
yes
100%
100%
60.7%
60.7%
99.95% 51.77%
99.95% 51.77%
81.1%
50.1%
74.0%
50.3%
68.7%
68.7%
74.2%
74.2%
76.3%
69.1%
79.0%