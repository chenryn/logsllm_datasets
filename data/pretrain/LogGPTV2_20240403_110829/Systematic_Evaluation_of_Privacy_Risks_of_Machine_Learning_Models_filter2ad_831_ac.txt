### 3.2.1 Comparison of Defended and Undefended Models

As illustrated in Figure 2, the defended Purchase100 classifier should be compared to the undefended model with fewer training steps but similar accuracy.

### 3.2.2 Adaptive Attacks

The field of machine learning is characterized by an ongoing arms race between privacy attacks and defenses. When evaluating the performance of a defense, it is crucial to consider the scenario where the adversary is fully aware of the defense mechanism and can adapt their attack strategy accordingly. A defense that performs well against non-adaptive attacks may not be effective against adaptive ones [2, 5, 15].

For defenses specifically designed against membership inference attacks, it is essential to assume that the adversary has knowledge of the defense mechanism. This allows the adversary to train shadow models using the same defense method. From these defended shadow models, the adversary can then develop an attack classifier or set threshold values for metric-based attacks, ultimately launching attacks on the defended target model.

### 3.3 Experiment Results

We first re-evaluate the effectiveness of two membership inference defenses [20, 31] and then re-evaluate the white-box membership inference attacks proposed by Nasr et al. [32]. Following prior work [41, 44, 48], we sample the input (x, y) from either the target model’s training set or test set with equal probability (0.5) to maximize the uncertainty of membership inference attacks. Consequently, a random guessing strategy would result in a 50% membership inference attack accuracy.

#### 3.3.1 Datasets

**Purchase100**: This dataset is derived from Kaggle’s Acquire Valued Shoppers Challenge, which contains shopping records of several thousand individuals. We use a simplified and preprocessed version provided by Shokri et al. [41]. The dataset consists of 197,324 data samples with 600 binary features, each representing whether an individual has purchased a specific product. The samples are clustered into 100 classes, each representing a different purchase style. The classification task is to predict the purchase style based on the 600 binary features. We follow Nasr et al. [31, 32] and use 10% of the data samples (19,732) for training.

**Texas100**: This dataset is based on the Hospital Discharge Data public use files released by the Texas Department of State Health Services. Each record includes external causes of injury, diagnosis, procedures, and generic information such as gender, age, and race. We use a simplified and preprocessed version provided by Shokri et al. [41]. The classification task is to predict the patient’s main procedure based on their information. The dataset focuses on the 100 most frequent procedures, resulting in 67,330 data samples with 6,170 binary features. We follow previous papers [20, 31, 32] and use 10,000 data samples for training.

**Location30**: This dataset is based on the Foursquare dataset, which contains location "check-in" records of several thousand individuals. We use a simplified and preprocessed version provided by Shokri et al. [41]. The dataset contains 5,010 data samples with 446 binary features, each representing whether an individual has visited a specific region or location type. The samples are clustered into 30 classes, each representing a different geosocial type. The classification task is to predict the geosocial type based on the 446 binary features. We follow Jia et al. [20] and use 1,000 data samples for training.

**CIFAR100**: This is a major benchmark dataset for image classification [21]. It consists of 32×32 color images in 100 classes, with 600 images per class. For each class label, 500 images are used for training, and the remaining 100 images are used for testing.

We selected these datasets for fair comparison with prior work [20, 31, 32]. Since all datasets except CIFAR100 are binary, we also provide attack results on more complex datasets in Appendix A, where our benchmark attacks achieve higher success rates than NN-based attacks.

#### 3.3.2 Re-evaluating Adversarial Regularization [31]

We follow Nasr et al. [31] to train both defended and undefended classifiers on the Purchase100 and Texas100 datasets. The model architecture is a fully connected neural network with 4 hidden layers, with neuron counts of 1024, 512, 256, and 128, respectively. All hidden layers use the hyperbolic tangent (Tanh) activation function. We note that adversarial regularization [31] incurs a drop in accuracy. After applying the defense, the test accuracy drops from 80.9% to 76.6% on the Purchase100 dataset and from 52.3% to 46.4% on the Texas100 dataset.

To further evaluate the effectiveness of adversarial regularization [31], we also obtain models with early stopping by saving the undefended models at every training epoch and selecting the saved epochs with similar accuracy to the defended models. Table 2 presents the membership inference attack results.

**Table 2: Benchmarking the Effectiveness of Adversarial Regularization [31] as a Defense Against Membership Inference Attacks**

| Dataset | Defense [31]? | Training Acc | Test Acc | Attack Acc [31] | Icorr | Iconf | Ientr | IMentr |
|---------|---------------|--------------|----------|-----------------|-------|-------|-------|--------|
| Purchase100 | No | 99.8% | 80.9% | 67.6% | 59.5% | 59.1% | 67.7% | 58.6% |
| Purchase100 | Yes | 92.7% | 76.6% | 67.1% | 59.4% | 59.2% | 67.8% | 58.6% |
| Purchase100 | Early Stopping | 92.9% | 76.4% | N.A. | 59.5% | 58.1% | 58.2% | 64.4% |
| Texas100 | No | 81.0% | 52.3% | 64.7% | 55.5% | 52.9% | 55.4% | 55.8% |
| Texas100 | Yes | 56.6% | 46.4% | 64.8% | 58.6% | 53.5% | 58.6% | 59.4% |
| Texas100 | Early Stopping | 59.3% | 47.9% | N.A. | 59.5% | 55.7% | 67.1% | 59.4% |

From Table 2, we observe that the defended models remain vulnerable to membership inference attacks, indicating the necessity of our metric-based benchmark attacks. Our benchmark attacks achieve 59.5% and 58.6% attack accuracy on the defended Purchase100 and Texas100 classifiers, significantly higher than the 51.6% and 51.0% reported by Nasr et al. [31]. Furthermore, on all models except the undefended Purchase100 classifier, the highest attack accuracy achieved by our benchmark attacks exceeds that of the NN-based attacks used in Nasr et al. [31].

Surprisingly, adversarial regularization [31] is no better than our early stopping benchmark method. With early stopping, the undefended Purchase100 and Texas100 classifiers have attack accuracies of 59.2% and 59.5%, respectively, which are close to those of the defended models. Therefore, when evaluating future defense mechanisms that trade lower model accuracy for reduced membership inference risk, we recommend comparing the defended model to a naturally trained model with early stopping for a fair comparison.

To demonstrate the improvement in attack performance using our class-dependent thresholding technique, we compare it with metric-based attacks where the same threshold is applied to all class labels. Table 3 shows the results on the Texas100 classifiers without defense, with AdvReg [31], and with early stopping.

**Table 3: Comparing Attack Performance Between Conventional Class-Independent Thresholding Attacks and Our Class-Dependent Thresholding Attacks**

| Attack Method | Defense Method | Iconf (Class-Indep.) | Iconf (Class-Dep.) | Ientr (Class-Indep.) | Ientr (Class-Dep.) | IMentr (Class-Indep.) | IMentr (Class-Dep.) |
|---------------|----------------|----------------------|--------------------|----------------------|--------------------|-----------------------|---------------------|
| No Defense | - | 64.7% | 67.8% | 58.3% | 60.2% | 64.8% | 67.7% |
| AdvReg [31] | - | 55.5% | 58.6% | 52.9% | 53.5% | 55.4% | 58.6% |
| Early Stopping | - | 55.8% | 59.4% | 53.2% | 54.0% | 55.9% | 59.5% |

With class-dependent thresholding, we increase the attack accuracy by 1% to 4%.

#### 3.3.3 Re-evaluating MemGuard [20]

We follow Jia et al. [20] to train classifiers on the Location30 and Texas100 datasets. The model architecture is a fully connected neural network with 4 hidden layers, with neuron counts of 1024, 512, 256, and 128, respectively. All hidden layers use the rectified linear unit (ReLU) activation function. MemGuard [20] does not change the accuracy performance, so a comparison with the early stopping benchmark is not applicable. Table 4 lists the attack accuracy on both undefended and defended models, using attack methods from Jia et al. [20] and our metric-based benchmark attack methods. Jia et al. [20] use six different NN attack classifiers to measure privacy risks, and we select the highest attack accuracy among them.

**Table 4: Benchmarking the Effectiveness of MemGuard [20] as a Defense Against Membership Inference Attacks**

| Dataset | Defense [20]? | Training Acc | Test Acc | Attack Acc [20] | Icorr | Iconf | Ientr |
|---------|---------------|--------------|----------|-----------------|-------|-------|-------|
| Location30 | No | 100% | 60.7% | 99.95% | 51.77% | 81.1% | 74.0% |
| Location30 | Yes | 100% | 60.7% | 99.95% | 51.77% | 74.2% | 74.2% |
| Texas100 | No | 60.7% | 51.77% | 81.1% | 74.0% | 68.7% | 74.2% |
| Texas100 | Yes | 60.7% | 51.77% | 74.2% | 74.2% | 76.3% | 69.1% |
| Texas100 | Yes | 60.7% | 51.77% | 74.2% | 74.2% | 79.0% | 74.2% |

From Table 4, we again emphasize the necessity of our benchmark attacks by showing that the defended models still have high membership inference accuracy: 69.1% on the defended Location30 classifier and 74.2% on the defended Texas100 classifier.