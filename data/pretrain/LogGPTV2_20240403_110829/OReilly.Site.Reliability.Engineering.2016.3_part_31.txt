additional consistency considerations. For example, the user may be receiving new
emails or deleting emails as the client fetches emails page-by-page. For this use case, a
client that naively iterates through the results and concatenates the responses (rather
than paginating based on a fixed view of the data) will likely produce an inconsistent
view, repeating some messages and/or skipping others.
To keep interfaces (and their implementations) simple, services are often defined to
allow the most expensive requests to consume 100, 1,000, or even 10,000 times more
resources than the cheapest requests. However, varying resource requirements per-
request naturally mean that some backend tasks will be unlucky and occasionally
receive more expensive requests than others. The extent to which this situation affects
load balancing depends on how expensive the most expensive requests are. For exam‐
ple, for one of our Java backends, queries consume around 15 ms of CPU on average
but some queries can easily require up to 10 seconds. Each task in this backend
reserves multiple CPU cores, which reduces latency by allowing some of the compu‐
tations to happen in parallel. But despite these reserved cores, when a backend
receives one of these large queries, its load increases significantly for a few seconds. A
poorly behaved task may run out of memory or even stop responding entirely (e.g.,
due to memory thrashing), but even in the normal case (i.e., the backend has suffi‐
cient resources and its load normalizes once the large query completes), the latency of
other requests suffers due to resource competition with the expensive request.
Machine diversity
Another challenge to Simple Round Robin is the fact that not all machines in the
same datacenter are necessarily the same. A given datacenter may have machines with
CPUs of varying performance, and therefore, the same request may represent a sig‐
nificantly different amount of work for different machines.
Dealing with machine diversity—without requiring strict homogeneity—was a chal‐
lenge for many years at Google. In theory, the solution to working with heterogene‐
ous resource capacity in a fleet is simple: scale the CPU reservations depending on
the processor/machine type. However, in practice, rolling out this solution required
significant effort because it required our job scheduler to account for resource equiv‐
alencies based on average machine performance across a sampling of services. For
example, 2 CPU units in machine X (a “slow” machine) is equivalent to 0.8 CPU units
in machine Y (a “fast” machine). With this information, the job scheduler is then
242 | Chapter 20: Load Balancing in the Datacenter
required to adjust CPU reservations for a process based upon the equivalence factor
and the type of machine on which the process was scheduled. In an attempt to miti‐
gate this complexity, we created a virtual unit for CPU rate called “GCU” (Google
Compute Units). GCUs became the standard for modeling CPU rates, and were used
to maintain a mapping from each CPU architecture in our datacenters to its corre‐
sponding GCU based upon its performance.
Unpredictable performance factors
Perhaps the largest complicating factor for Simple Round Robin is that machines—or,
more accurately, the performance of backend tasks—may differ vastly due to several
unpredictable aspects that cannot be accounted for statically.
Two of the many unpredictable factors that contribute to performance include:
Antagonistic neighbors
Other processes (often completely unrelated and run by different teams) can
have a significant impact on the performance of your processes. We’ve seen dif‐
ferences in performance of this nature of up to 20%. This difference mostly stems
from competition for shared resources, such as space in memory caches or band‐
width, in ways that may not be directly obvious. For example, if the latency of
outgoing requests from a backend task grows (because of competition for net‐
work resources with an antagonistic neighbor), the number of active requests will
also grow, which may trigger increased garbage collection.
Task restarts
When a task gets restarted, it often requires significantly more resources for a few
minutes. As just one example, we’ve seen this condition affect platforms such as
Java that optimize code dynamically more than others. In response, we’ve
actually added to the logic of some server code—we keep servers in lame duck
state and prewarm them (triggering these optimizations) for a period of time
after they start, until their performance is nominal. The effect of task restarts can
become a sizable problem when you consider we update many servers (e.g., push
new builds, which requires restarting these tasks) every day.
If your load balancing policy can’t adapt to unforeseen performance limitations, you
will inherently end up with a suboptimal load distribution when working at scale.
Least-Loaded Round Robin
An alternative approach to Simple Round Robin is to have each client task keep track
of the number of active requests it has to each backend task in its subset and use
Round Robin among the set of tasks with a minimal number of active requests.
Load Balancing Policies | 243
For example, suppose a client uses a subset of backend tasks t0 to t9, and currently
has the following number of active requests against each backend:
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9
2 1 0 0 1 0 2 0 0 1
For a new request, the client would filter the list of potential backend tasks to just
those tasks with the least number of connections (t2, t3, t5, t7, and t8) and choose a
backend from that list. Let’s assume it picks t2. The client’s connection state table
would now look like the following:
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9
2 1 1 0 1 0 2 0 0 1
Assuming none of the current requests have completed, on the next request, the
backend candidate pool becomes t3, t5, t7, and t8.
Let’s fast-forward until we’ve issued four new requests. Still assuming that no request
finishes in the meantime, the connection state table would look like the following:
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9
2 1 1 1 1 1 2 1 1 1
At this point the set of backend candidates is all tasks except t0 and t6. However, if the
request against task t4 finishes, its current state becomes “0 active requests” and a
new request will be assigned to t4.
This implementation actually uses Round Robin, but it’s applied across the set of
tasks with minimal active requests. Without such filtering, the policy might not be
able to spread the requests well enough to avoid a situation in which some portion of
the available backend tasks goes unused. The idea behind the least-loaded policy is
that loaded tasks will tend to have higher latency than those with spare capacity, and
this strategy will naturally take load away from these loaded tasks.
All that said, we’ve learned (the hard way!) about one very dangerous pitfall of the
Least-Loaded Round Robin approach: if a task is seriously unhealthy, it might start
serving 100% errors. Depending on the nature of those errors, they may have very
low latency; it’s frequently significantly faster to just return an “I’m unhealthy!” error
than to actually process a request. As a result, clients might start sending a very large
amount of traffic to the unhealthy task, erroneously thinking that the task is available,
as opposed to fast-failing them! We say that the unhealthy task is now sinkholing traf‐
fic. Fortunately, this pitfall can be solved relatively easily by modifying the policy to
count recent errors as if they were active requests. This way, if a backend task
244 | Chapter 20: Load Balancing in the Datacenter
becomes unhealthy, the load balancing policy begins to divert load from it the same
way it would divert load from an overburdened task.
Least-Loaded Round Robin has two important limitations:
The count of active requests may not be a very good proxy for the capability of a given
backend
Many requests spend a significant portion of their life just waiting for a response
from the network (i.e., waiting for responses to requests they initiate to other
backends) and very little time on actual processing. For example, one backend
task may be able to process twice as many requests as another (e.g., because it’s
running in a machine with a CPU that’s twice as fast as the rest), but the latency
of its requests may still be roughly the same as the latency of requests in the other
task (because requests spend most of their life just waiting for the network to
respond). In this case, because blocking on I/O often consumes zero CPU, very
little RAM, and no bandwidth, we’d still want to send twice as many requests to
the faster backend. However, Least-Loaded Round Robin will consider both
backend tasks equally loaded.
The count of active requests in each client doesn’t include requests from other clients to
the same backends
That is, each client task has only a very limited view into the state of its backend
tasks: the view of its own requests.
In practice, we’ve found that large services using Least-Loaded Round Robin will see
their most loaded backend task using twice as much CPU as the least loaded, per‐
forming about as poorly as Round Robin.
Weighted Round Robin
Weighted Round Robin is an important load balancing policy that improves on Sim‐
ple and Least-Loaded Round Robin by incorporating backend-provided information
into the decision process.
Weighted Round Robin is fairly simple in principle: each client task keeps a “capabil‐
ity” score for each backend in its subset. Requests are distributed in Round-Robin
fashion, but clients weigh the distributions of requests to backends proportionally. In
each response (including responses to health checks), backends include the current
observed rates of queries and errors per second, in addition to the utilization (typi‐
cally, CPU usage). Clients adjust the capability scores periodically to pick backend
tasks based upon their current number of successful requests handled and at what
utilization cost; failed requests result in a penalty that affects future decisions.
In practice, Weighted Round Robin has worked very well and significantly reduced
the difference between the most and the least utilized tasks. Figure 20-6 shows the
CPU rates for a random subset of backend tasks around the time its clients switched
Load Balancing Policies | 245
from Least-Loaded to Weighted Round Robin. The spread from the least to the most
loaded tasks decreased drastically.
Figure 20-6. CPU distribution before and after enabling Weighted Round Robin
246 | Chapter 20: Load Balancing in the Datacenter
CHAPTER 21
Handling Overload
Written by Alejandro Forero Cuervo
Edited by Sarah Chavis
Avoiding overload is a goal of load balancing policies. But no matter how efficient
your load balancing policy, eventually some part of your system will become overloa‐
ded. Gracefully handling overload conditions is fundamental to running a reliable
serving system.
One option for handling overload is to serve degraded responses: responses that are
not as accurate as or that contain less data than normal responses, but that are easier
to compute. For example:
• Instead of searching an entire corpus to provide the best available results to a
search query, search only a small percentage of the candidate set.
• Rely on a local copy of results that may not be fully up to date but that will be
cheaper to use than going against the canonical storage.
However, under extreme overload, the service might not even be able to compute and
serve degraded responses. At this point it may have no immediate option but to serve
errors. One way to mitigate this scenario is to balance traffic across datacenters such
that no datacenter receives more traffic than it has the capacity to process. For exam‐
ple, if a datacenter runs 100 backend tasks and each task can process up to 500
requests per second, the load balancing algorithm will not allow more than 50,000
queries per second to be sent to that datacenter. However, even this constraint can
prove insufficient to avoid overload when you’re operating at scale. At the end of the
day, it’s best to build clients and backends to handle resource restrictions gracefully:
redirect when possible, serve degraded results when necessary, and handle resource
errors transparently when all else fails.
247
The Pitfalls of “Queries per Second”
Different queries can have vastly different resource requirements. A query’s cost can
vary based on arbitrary factors such as the code in the client that issues them (for
services that have many different clients) or even the time of the day (e.g., home users
versus work users; or interactive end-user traffic versus batch traffic).
We learned this lesson the hard way: modeling capacity as “queries per second” or
using static features of the requests that are believed to be a proxy for the resources
they consume (e.g., “how many keys are the requests reading”) often makes for a
poor metric. Even if these metrics perform adequately at one point in time, the ratios
can change. Sometimes the change is gradual, but sometimes the change is drastic
(e.g., a new version of the software suddenly made some features of some requests
require significantly fewer resources). A moving target makes a poor metric for
designing and implementing load balancing.
A better solution is to measure capacity directly in available resources. For example,
you may have a total of 500 CPU cores and 1 TB of memory reserved for a given ser‐
vice in a given datacenter. Naturally, it works much better to use those numbers
directly to model a datacenter’s capacity. We often speak about the cost of a request to
refer to a normalized measure of how much CPU time it has consumed (over differ‐
ent CPU architectures, with consideration of performance differences).
In a majority of cases (although certainly not in all), we’ve found that simply using
CPU consumption as the signal for provisioning works well, for the following rea‐
sons:
• In platforms with garbage collection, memory pressure naturally translates into
increased CPU consumption.
• In other platforms, it’s possible to provision the remaining resources in such a
way that they’re very unlikely to run out before CPU runs out.
In cases where over-provisioning the non-CPU resources is prohibitively expensive,
we take each system resource into account separately when considering resource con‐
sumption.
Per-Customer Limits
One component of dealing with overload is deciding what to do in the case of global
overload. In a perfect world, where teams coordinate their launches carefully with the
owners of their backend dependencies, global overload never happens and backend
services always have enough capacity to serve their customers. Unfortunately, we
don’t live in a perfect world. Here in reality, global overload occurs quite frequently
(especially for internal services that tend to have many clients run by many teams).
248 | Chapter 21: Handling Overload
When global overload does occur, it’s vital that the service only delivers error respon‐
ses to misbehaving customers, while other customers remain unaffected. To achieve
this outcome, service owners provision their capacity based on the negotiated usage
with their customers and define per-customer quotas according to these agreements.
For example, if a backend service has 10,000 CPUs allocated worldwide (over various
datacenters), their per-customer limits might look something like the following:
• Gmail is allowed to consume up to 4,000 CPU seconds per second.
• Calendar is allowed to consume up to 4,000 CPU seconds per second.
• Android is allowed to consume up to 3,000 CPU seconds per second.
• Google+ is allowed to consume up to 2,000 CPU seconds per second.
• Every other user is allowed to consume up to 500 CPU seconds per second.
Note that these numbers may add up to more than the 10,000 CPUs allocated to the
backend service. The service owner is relying on the fact that it’s unlikely for all of
their customers to hit their resource limits simultaneously.
We aggregate global usage information in real time from all backend tasks, and use
that data to push effective limits to individual backend tasks. A closer look at the sys‐
tem that implements this logic is outside of the scope of this discussion, but we’ve
written significant code to implement this in our backend tasks. An interesting part
of the puzzle is computing in real time the amount of resources—specifically CPU—
consumed by each individual request. This computation is particularly tricky for
servers that don’t implement a thread-per-request model, where a pool of threads just
executes different parts of all requests as they come in, using nonblocking APIs.
Client-Side Throttling
When a customer is out of quota, a backend task should reject requests quickly with
the expectation that returning a “customer is out of quota” error consumes signifi‐
cantly fewer resources than actually processing the request and serving back a correct
response. However, this logic doesn’t hold true for all services. For example, it’s
almost equally expensive to reject a request that requires a simple RAM lookup
(where the overhead of the request/response protocol handling is significantly larger
than the overhead of producing the response) as it is to accept and run that request.
And even in the case where rejecting requests saves significant resources, those
requests still consume some resources. If the amount of rejected requests is signifi‐
cant, these numbers add up quickly. In such cases, the backend can become overloa‐
ded even though the vast majority of its CPU is spent just rejecting requests!
Client-Side Throttling | 249
Client-side throttling addresses this problem.1 When a client detects that a significant
portion of its recent requests have been rejected due to “out of quota” errors, it starts
self-regulating and caps the amount of outgoing traffic it generates. Requests above
the cap fail locally without even reaching the network.
We implemented client-side throttling through a technique we call adaptive throttling.
Specifically, each client task keeps the following information for the last two minutes
of its history:
requests
The number of requests attempted by the application layer (at the client, on top
of the adaptive throttling system)
accepts
The number of requests accepted by the backend
Under normal conditions, the two values are equal. As the backend starts rejecting
traffic, the number of accepts becomes smaller than the number of requests. Clients
can continue to issue requests to the backend until requests is K times as large as
accepts. Once that cutoff is reached, the client begins to self-regulate and new
requests are rejected locally (i.e., at the client) with the probability calculated in Equa‐
tion 21-1.
Equation 21-1. Client request rejection probability
requests−K×accepts
max 0,
requests+1
As the client itself starts rejecting requests, requests will continue to exceed accepts.
While it may seem counterintuitive, given that locally rejected requests aren’t actually
propagated to the backend, this is the preferred behavior. As the rate at which the
application attempts requests to the client grows (relative to the rate at which the