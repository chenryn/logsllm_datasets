LPDWORD lpType,
LPBYTE lpData,
hKey,
For each service, the input/output parameters are identi-
ﬁed (using the documentation). Each output parameter of a
driver service dsx.y is targeted for injection as well as each
input parameter for the OS-driver services osx.q, see Sec. 3.
An error is injected after a call to/from a driver is made by
changing the value of a parameter and then continuing the
execution with the corrupted value.
Error Timing: Each error is injected once, thus sim-
ulating transient service corruption. We do not consider
permanent errors, occurring every time a function is called,
as these are easier to detect using normal functional testing
techniques. Transient errors better represent the rare cases
that are not easily detectable with normal testing techniques.
Each error is injected on ﬁrst occurrence, meaning that
the ﬁrst call made to the service in question will be the tar-
get for the injection. Previous studies on OS’s indicate that
“ﬁrst occurrence” injection provides comparably effective
results to other injection instances [20].
4. Error Propagation Measures
Based on the initial discussion presented in [13], we have
deﬁned a set of quantiﬁable measures that guide the location
of vulnerabilities. An effective placement of wrappers is
where errors are likely to occur (high probability of error
propagation) and where the impact of errors is highest [12]
(the consequences of errors are system failures). Therefore,
these are the objectives for our measures:
(a) Measure for degree of error porosity of an OS ser-
vice: Service Error Permeability
(b) Measure for error exposure of an OS service: OS
Service Error Exposure
(c) Measure of driver error co-relation with service set:
Driver Error Diffusion
It is important to note that the measures presented below
implicitly use a uniform distribution of errors. This is a con-
sequence of the fact that no “proﬁle” exists describing how
the system is used in a real scenario. A longer discussion
on this aspect can be found in Sec. 7.
4.1. Service Error Permeability
We deﬁne two measures for error permeability, one for
a driver’s export of services (PDS) and one for its im-
port of OS services (POS). For a driver Dx, the set of
export services (dsx.1 ··· dsx.N ) and the import services
(osx.1 ··· osx.M ), see Sec. 3. The Service Error Permeabil-
ities, for export (P DSi
x.z) of services,
relate one driver service to one OS service. The Service Er-
ror Permeability is the conditional probability that an error
in a speciﬁc driver service (dsx.y) or in the use of OS-driver
service (osx.z) will propagate to a speciﬁc OS service (si).
For an OS service si and a driver Dx:
x.y) and import (P OSi
P DSi
x.y = P r ( error in si|error in dsx.y)
P OSi
x.z = P r ( error in si|error in use of osx.z)
(1)
(2)
The Service Error Permeability (P DS for exported and
P OS for imported services of a driver) gives an indication
of the permeability of the particular OS service, i.e., how
easily does the service let errors in the driver propagate to
applications using it. A higher probability naturally means
that either a) a wrapper needs to be designed to protect the
OS from driver errors, or b) applications using the affected
services needs to take precautions when using them. Note
that Equation 2 allows us to compare the same OS service
used by different drivers. The impact of the context induced
by different drivers can thus be studied.
4.2. OS Service Error Exposure
To ascertain which OS service is the more exposed to
errors propagating through the OS, the full set of drivers
needs to be considered. We use the measure Service Error
Permeability, to compose the OS Service Error Exposure
for an OS service si, namely Ei:
(cid:1)
(cid:1)
(cid:1)
(cid:1)
Ei =
P OSi
x.j +
P DSi
x.j
(3)
Dx
osx.j
Dx
dsx.j
For Ei we consider all drivers. The OS Service Error
Exposure gives an ordering across OS services which or-
ders services based on their susceptibility to errors passing
through the OS. Note that this expression implies aggregat-
ing all imported and exported Service Error Permeabilities
(1 & 2 above). The OS Service Error Exposure indicates
which services are more exposed to propagating errors; its
use is thus mainly to direct placement of wrappers on the
OS-application level.
4.3. Driver Error Diffusion
The measure Driver Error Diffusion identiﬁes drivers,
that when being erroneous are more likely to spread errors,
by considering one particular driver’s relation to many ser-
vices. Driver Error Diffusion is a measure of how a driver
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
can impact OS services (at the OS-Application interface).
The more services, and the higher the impact (permeabil-
ity) a driver has, the higher the value. For a driver Dx and a
set of services, the Driver Error Diffusion, Dx is:
(cid:1)
(cid:1)
Dx =
P OSi
x.j +
(cid:1)
(cid:1)
si
osx.j
si
dsx.j
P DSi
x.j
(4)
The Driver Error Diffusion ranks the drivers according
to their potential for spreading errors in the system. Analo-
gous to the OS service exposure, the driver diffusion aggre-
gates Service Error Permeabilities. This gives the system
designer hints on where wrappers should be placed, i.e.,
where adding driver wrappers makes sense. Note that we
do not try to test the drivers per se, so this measure only
tells us which drivers may corrupt the system by spreading
errors. Also, we emphasize that the intent of these measures
is not for absolute values, but to obtain relative rankings.
Once a ranking across drivers is achieved, the driver(s)
with the highest Driver Error Diffusion value should be the
ﬁrst targets. Details on speciﬁc error paths can now be used
(i.e., Service Error Permeability values) to guide the com-
position and exact placement of wrappers.
4.4. Error Exposure vs Error Impact
The purpose of error propagation proﬁling is to reveal
prominent error propagation paths as well as to identify
those that can have a severe impact on the whole system.
The measures described in the previous subsections aid in
identifying the common error paths. However, the impact
can range from no effect at all to the whole system being
rendered unusable (e.g., crashed or hung). Thus, it is im-
portant to not only measure if a failure occurred, but also
what type of failure it was. Failure mode analysis is an ap-
proach that accomplishes just this. A set of failure modes
are deﬁned and the outcome of each experiment is classiﬁed
as belonging to one of them. The classes used in this study
follow the classes established in [3, 6, 10]:
Class NF: When no visible effect can be seen as an
outcome of an experiment, the No Failure class is
used. This indicates that the error was either not ac-
tivated or was masked by the OS.
Class 1: Error propagated, but still satisﬁed the OS
service speciﬁcation as deﬁned in the documenta-
tion. Examples of Class 1 outcomes are when an
error code or exception is returned that is a member
of the set of allowed codes for this call or if a data
value was corrupted and propagated to the service,
but did not violate the speciﬁcation.
Class 2: Error propagated and violated the service
speciﬁcation. For example, returning an unspeci-
ﬁed error code or if the call directly causes the ap-
plication to hang or crash but other applications in
the system remain unharmed, result in this category.
Note that not properly handling a return value within
the speciﬁcation does not end up in this class!
Class 3: The OS hung or crashed due to the error. If
the OS hangs or crashes, no progress is possible.
For a crashed OS, this state must be detected by an
outside monitor unless this state is automatically de-
tected internally and the machine is rebooted. Again
note that not handling Class 1 failures resulting in an
eventual crash is not treated as a Class 3 failure.
Using this severity scale, further analysis of the results
can be done. Starting with the most severe class (Class 3:
crash/hung) one can study the exposure to these errors sep-
arately, then progressively go downwards in the scale.
5. Case Study: Windows CE .Net
We present a case study to demonstrate the utility of the
measures deﬁned in Sec. 4. A commercial OS (Windows
CE .Net), together with two drivers, is the target for this
study. By use of error injection, the measures deﬁned in
Sec. 4 are estimated. Experiments are conducted in rounds,
with one error being injected in each round. To assure a
consistent system state for each experiment the system is
rebooted between rounds.
The rest of this Section contains a description of the tar-
get system followed by a description of the experimental
setup used, its modules and their roles. Subsequently, de-
tails on the experimental process and the estimation of mea-
sures are presented.
5.1. Target System
The target of the case study is Windows CE .Net 4.2.
The OS runs on a HW reference platform using the Intel
PXA255 XScale board (similar to what is found on modern
PDAs). The conﬁguration has a 400 MHz processor, with
32 MB ﬂash and 64 MB SDRAM memory, and is equipped
with serial ports as well as Ethernet connections. Four sep-
arate boards were used to achieve reproducibility of results
as well as for expediting the injection processes.
For all experiments, the OS image is conﬁgured to use a
minimum set of components to facilitate repeatability. Thus
components relating to unused HW (Keyboards, mice, etc)
and graphical components as well as possible services (web
servers, etc) are excluded.
two
drivers,
We
driver,
target
a
driver,
91C111.Dll,
and
cerfio serial.Dll.
Both are shipped with the
hardware platform as binaries. These drivers are chosen as
an Ethernet
serial
port
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
they a) represent functionality present in many products,
and b) are supplied by third party vendors, which prohibit
source code access, thus demonstrating the utility of our
black-box approach.
5.2. Experimental Setup
In order to estimate the Service Error Permeability, we
inject errors in the OS-driver interface and study their ef-
fects on the application-OS interface. We use a special
driver “wrapper”, termed Interceptor, which bypasses the
normal driver-OS interactions and provides the means for
injecting errors at this interface.
The entire setup consists of four main modules, (apart
from the target OS); a) the aforementioned interceptor; b)
test applications, c) an experiment manager application; and
d) a host computer, see Figure 2.
Test
Applications
Operating System
Interceptor
Experiment
Manager
- Exp. Setup
- Exp Synch.
- Logging
- Restarting
Host
Computer
Driver x
Driver x
Target
driver
Figure 2. The experimental setup - dotted
boxes indicate add-on modules to the setup.
Interceptor: Two methods are used to intercept OS-
driver interactions, a) the changing of function table entries
in the binary of the driver, and b) re-conﬁguration of the
Windows registry. Changes made to the binaries are made
before the driver is loaded in the system. By reconﬁguring
the loading of drivers (in the registry) the interceptor mod-
ule can be loaded instead of the driver, and thus act as a
driver for the OS.
Test Applications: The purpose of the test applications
is to exercise the OS services.
Ideally real applications
should be used. However, the use of custom test applica-
tions simpliﬁes the task of exercising speciﬁc parts of the
OS and detection of propagating errors. Sec. 7 contains a
longer discussion on the use of real applications.
Four applications are used, that use the OS in different
ways. Three applications are dedicated to test a) the mem-
ory management subsystem; b) thread management and
synchronization primitives; and c) ﬁle system operations.
The fourth application is dedicated to the speciﬁc driver be-
ing tested. Driver-speciﬁc applications are simple echo ap-
plications that exchange data with a host server. Each appli-
cation is manually equipped with assertions, checking each
call to the OS for any irregularities. Assertions are based
on the imported OS services by an application. These are
matched to calls being made in the application using the
speciﬁcation of the service.
Experiment Manager: The experiment manager reads
a conﬁguration ﬁle from persistent memory and notiﬁes the
interceptor of the error to be injected. During an experi-
ment the data entries in the conﬁguration ﬁle are updated
such that after a reboot the manager can tell if the previous
experiment ended in a failure or not. The experiment man-
ager is also responsible for starting the test applications and
rebooting the device after each experiment. Log messages
are sent to the host machine, which stores them as text ﬁles.
Host Computer: Its role is to receive log messages from
the device (over serial communication or Ethernet) and log
them to ﬁles on the host machine. The host also runs the test
servers, used in communicating with the test applications.
5.3. Experiments: Procedures
The ﬁrst application to start during boot-up is the exper-
iment manager. It reads the conﬁguration ﬁles and sets up
communication with other modules and the host machine.
The manager also starts a timer, that will reboot the system
after a set time (currently three minutes), long enough for
all test applications to terminate during failure free scenar-
ios (> execution time + boot up time). The purpose of the
timer is to reboot the system if the experiment manager has
crashed as a result of an error. The interceptor is loaded
together with the target driver. The error is injected at the
ﬁrst instance of the call to the function in question. After
the system has booted up, the experiment manager starts
the test applications and monitors their progress.
If no failure occurs, the test applications terminate suc-
cessfully; this is logged by the experiment manager. If the
error is activated (i.e., actually executed by the driver) this is
logged as well. After the test applications have terminated,
the result is logged.
If an error occurs, this is either registered by a test appli-
cation or by the experiment manager noticing that either the
driver/test application has crashed. Assertions in the test ap-
plications allow detection of propagated errors. Each time
an error code is returned from a call this is logged as a prop-
agation. Distinction between expected and unexpected er-
ror codes is made off-line. Assertions are also used to detect
deviations from “normal” behavior, e.g., to detect if the cor-
rect string was read from the network. The information on
“normal” behavior is hard-coded into the test applications.
To detect OS crashes/hangs, and to check if the previous
experiment exited normally or not, the experiment manager
reads the entries in the conﬁguration ﬁle during boot up.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
5.4. Estimating Measures
up time. Not included is the time to develop driver speciﬁc
interceptors and the speciﬁc error models.
The only measures that need to be experimentally es-
timated are the Service Error Permeabilities, P DSi
x.y and
P OSi
x.z. The test applications together with the experiment
manager are responsible for detecting failures/propagations
in the system. Software assertions generally require well
deﬁned speciﬁcations. In the case for Windows CE .Net,
we consider the help sections shipped with the tools, as well
as any other ofﬁcial documentation provided (e.g., msdn li-
brary) as the speciﬁcation of a service. The speciﬁcation of
a service generally includes syntax information, in/out rela-