### Input/Output Parameters and Error Injection

For each service, the input/output parameters are identified using the provided documentation. Each output parameter of a driver service `dsx.y` is targeted for error injection, as well as each input parameter for the OS-driver services `osx.q`, as detailed in Section 3. An error is injected after a call to or from a driver by modifying the value of a parameter and then continuing execution with the corrupted value.

#### Error Timing
Each error is injected once, simulating transient service corruption. We do not consider permanent errors that occur every time a function is called, as these are easier to detect using standard functional testing techniques. Transient errors better represent rare cases that are not easily detectable with normal testing methods. Each error is injected at the first occurrence, meaning the first call made to the service in question will be the target for the injection. Previous studies on operating systems indicate that "first occurrence" injection provides comparably effective results to other injection instances [20].

### Error Propagation Measures

Based on the initial discussion in [13], we have defined a set of quantifiable measures to guide the location of vulnerabilities. Effective placement of wrappers should target areas where errors are likely to occur (high probability of error propagation) and where the impact of errors is highest [12] (i.e., system failures). The objectives for our measures are:

1. **Service Error Permeability**: Measures the degree of error porosity of an OS service.
2. **OS Service Error Exposure**: Measures the error exposure of an OS service.
3. **Driver Error Diffusion**: Measures the correlation of driver errors with a set of services.

It is important to note that the measures presented below implicitly use a uniform distribution of errors, as no profile exists describing how the system is used in a real scenario. A more detailed discussion on this aspect can be found in Section 7.

#### 4.1. Service Error Permeability

We define two measures for error permeability: one for a driver's export of services (PDS) and one for its import of OS services (POS). For a driver `Dx`, the set of exported services (`dsx.1 ... dsx.N`) and imported services (`osx.1 ... osx.M`) are described in Section 3. The Service Error Permeabilities for exported services (`P DSi_x.y`) and imported services (`P OSi_x.z`) relate a specific driver service to an OS service. 

The Service Error Permeability is the conditional probability that an error in a specific driver service (`dsx.y`) or in the use of an OS-driver service (`osx.z`) will propagate to a specific OS service (`si`). For an OS service `si` and a driver `Dx`:

\[
P DSi_x.y = P r (\text{error in } si \mid \text{error in } dsx.y)
\]

\[
P OSi_x.z = P r (\text{error in } si \mid \text{error in use of } osx.z)
\]

These measures provide an indication of the permeability of a particular OS service, i.e., how easily the service allows errors in the driver to propagate to applications using it. A higher probability suggests either a) the need to design a wrapper to protect the OS from driver errors, or b) the need for applications using the affected services to take precautions. Equation 2 allows us to compare the same OS service used by different drivers, enabling the study of the impact of the context induced by different drivers.

#### 4.2. OS Service Error Exposure

To determine which OS service is more exposed to errors propagating through the OS, the full set of drivers needs to be considered. We use the Service Error Permeability measure to compose the OS Service Error Exposure for an OS service `si`, denoted as `Ei`:

\[
Ei = \sum_{Dx} \left( \sum_{osx.j} P OSi_x.j + \sum_{dsx.j} P DSi_x.j \right)
\]

For `Ei`, all drivers are considered. The OS Service Error Exposure provides an ordering of OS services based on their susceptibility to errors passing through the OS. This expression aggregates all imported and exported Service Error Permeabilities. The OS Service Error Exposure indicates which services are more exposed to propagating errors, guiding the placement of wrappers at the OS-application level.

#### 4.3. Driver Error Diffusion

The Driver Error Diffusion measure identifies drivers that, when erroneous, are more likely to spread errors by considering a driver's relation to many services. It measures how a driver impacts OS services (at the OS-Application interface). The more services a driver affects and the higher the impact (permeability), the higher the value. For a driver `Dx` and a set of services, the Driver Error Diffusion, `Dx`, is:

\[
Dx = \sum_{si} \left( \sum_{osx.j} P OSi_x.j + \sum_{dsx.j} P DSi_x.j \right)
\]

This measure ranks drivers according to their potential for spreading errors in the system. Similar to the OS service exposure, the driver diffusion aggregates Service Error Permeabilities, providing hints on where to place wrappers. We do not test the drivers per se; this measure only indicates which drivers may corrupt the system by spreading errors. The intent of these measures is to obtain relative rankings rather than absolute values. Once a ranking across drivers is achieved, the driver(s) with the highest Driver Error Diffusion value should be the first targets. Specific error paths can then be used to guide the composition and exact placement of wrappers.

#### 4.4. Error Exposure vs. Error Impact

The purpose of error propagation profiling is to reveal prominent error propagation paths and identify those that can have a severe impact on the system. The measures described in the previous subsections aid in identifying common error paths. However, the impact can range from no effect to the whole system being rendered unusable (e.g., crashed or hung). Therefore, it is important to measure not only if a failure occurred but also what type of failure it was. Failure mode analysis accomplishes this by defining a set of failure modes and classifying the outcome of each experiment. The classes used in this study follow those established in [3, 6, 10]:

- **Class NF**: No visible effect, indicating the error was either not activated or masked by the OS.
- **Class 1**: Error propagated but still satisfied the OS service specification. Examples include returning an allowed error code or a data value that did not violate the specification.
- **Class 2**: Error propagated and violated the service specification. Examples include returning an unspecified error code or causing the application to hang or crash, while other applications remain unharmed.
- **Class 3**: The OS hung or crashed due to the error. If the OS hangs or crashes, no progress is possible. This state must be detected by an outside monitor unless automatically detected and the machine is rebooted.

Using this severity scale, further analysis of the results can be done, starting with the most severe class (Class 3: crash/hung) and progressively going downwards.

### Case Study: Windows CE .Net

We present a case study to demonstrate the utility of the measures defined in Section 4. The target is a commercial OS (Windows CE .Net) along with two drivers. By using error injection, the measures defined in Section 4 are estimated. Experiments are conducted in rounds, with one error being injected in each round. To ensure a consistent system state for each experiment, the system is rebooted between rounds.

#### 5.1. Target System

The target of the case study is Windows CE .Net 4.2, running on a hardware reference platform using the Intel PXA255 XScale board (similar to modern PDAs). The configuration includes a 400 MHz processor, 32 MB flash, and 64 MB SDRAM memory, equipped with serial ports and Ethernet connections. Four separate boards were used to achieve reproducibility and expedite the injection processes.

For all experiments, the OS image is configured to use a minimum set of components to facilitate repeatability. Components related to unused hardware (keyboards, mice, etc.) and graphical components, as well as services like web servers, are excluded. Two drivers, `91C111.Dll` and `cerfio_serial.Dll`, are targeted. These drivers are chosen because they represent common functionality and are supplied by third-party vendors, demonstrating the utility of our black-box approach.

#### 5.2. Experimental Setup

To estimate the Service Error Permeability, we inject errors at the OS-driver interface and study their effects on the application-OS interface. We use a special driver "wrapper," termed Interceptor, which bypasses normal driver-OS interactions and provides the means for injecting errors at this interface.

The entire setup consists of four main modules, apart from the target OS:
- **Interceptor**: Intercepts OS-driver interactions by changing function table entries in the driver binary or reconfiguring the Windows registry.
- **Test Applications**: Exercise the OS services. Custom test applications are used to simplify the task of exercising specific parts of the OS and detecting propagating errors.
- **Experiment Manager**: Manages the experimental process, including setting up communication, starting test applications, and logging results.
- **Host Computer**: Receives log messages from the device and runs test servers for communication with the test applications.

#### 5.3. Experiments: Procedures

The first application to start during boot-up is the Experiment Manager, which reads the configuration files, sets up communication, and starts a timer to reboot the system after a set time. The Interceptor is loaded with the target driver, and the error is injected at the first instance of the call to the function in question. After the system boots up, the Experiment Manager starts the test applications and monitors their progress.

If no failure occurs, the test applications terminate successfully, and this is logged. If an error is activated, it is logged. If an error occurs, it is registered by a test application or the Experiment Manager. Assertions in the test applications detect propagated errors and deviations from normal behavior. To detect OS crashes/hangs, the Experiment Manager reads the configuration file during boot-up.

#### 5.4. Estimating Measures

The only measures that need to be experimentally estimated are the Service Error Permeabilities, `P DSi_x.y` and `P OSi_x.z`. The test applications and the Experiment Manager are responsible for detecting failures/propagations in the system. Software assertions generally require well-defined specifications. For Windows CE .Net, we consider the help sections shipped with the tools and any official documentation (e.g., MSDN library) as the specification of a service. The specification of a service generally includes syntax information, input/output relations, and expected behavior.