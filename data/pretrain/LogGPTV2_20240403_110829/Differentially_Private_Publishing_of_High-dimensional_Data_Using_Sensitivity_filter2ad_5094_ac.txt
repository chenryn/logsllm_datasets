select θ from [1, d] with privacy budget c. We also note that when
adding a tuple, the quality values for all θ’s increases; that is, the
changes of all quality values are one-directional; therefore, choos-
ing each possible θ from 1 to d with probability Prob[θ], where
Prob[θ] ∝ exp(cq(D, θ, p))
satisﬁes c-differential privacy.
Note that both c and p appear in the above formula. c is used
for exponential mechanism and is different from p, which is used
for Laplace mechanism later. The quality function q needs to con-
sider the effect of injecting noises with privacy budget p.
We take advantage of the small sensitivity of the quality function
by using less privacy budget on choosing threshold, and more pri-
vacy budget on publishing column counts. In our experiments, we
set 10% of privacy budget for choosing the threshold for sensitivity
control (c), and 90% of privacy budget for publishing the noisy
column counts (p). We leave optimal division of privacy budget
among the two steps to future work, and note that our experimental
results show that it is unlikely to result in signiﬁcant improvement
from the property of quality function.
noisy column counts
Algorithm 1 DPSense
Input: Dataset D as a n × d binary matrix, privacy budget 
Output: A d dimensional vector nc = Q(D) which represents
1: n ← number of rows in D, d ← number of columns in D
2: Set p ← 0.9 × , c ← 0.1 × 
3: Set threshold candidate set Θ ← {1, 2,··· , d}
4: for θi ∈ Θ do
5:
6:
7: end for
8: Choose θ∗ from Θ with probability Prob[θi] proportional to
Compute normalized D, i.e., D(cid:48) ← D|θi
q(θi) ← ac(D(cid:48)) − θi
p
(cid:16) cq(θi)
(cid:17)
2
exp
for θi ∈ Θ
9: D ← D|θ∗
10: Compute the true column count vector tc in D, and noisy col-
11: For any negative noisy count that nci < 0, set nci ← 0
12: Publish nc
umn count vector nc ← tc + Lap( θ∗
)d
p
4.2 DPSense Algorithm
Algorithm 1 gives all the steps of DPSense in detail. The pri-
vacy guarantee is given below.
THEOREM 1. DPSense is -differentially private.
PROOF. From Lemma 1 and the result on exponential mecha-
nism, the step in lines 4-8 in algorithm 1 satisﬁes c-differential pri-
vacy. Lines 9 performs the normalization of each row, and it doesn’t
violate differential privacy since the transformation is interior and
the noisy count is not published yet. Line 10 injects Laplace noise
to satisfy differential privacy, and the modiﬁcation of noisy count
in line 11 is a post-process on noisy counts. From Laplacian mech-
anism, this noise injection step satisﬁes p-differential privacy, and
the post-processing of noisy data doesn’t affect the privacy guar-
antee. The composition rule of differential privacy explains that
combining these two steps makes DPSense satisfy -differential
privacy, where  = c + p.
Notice that DPSense does not need to assume that ∆D, the max-
imal row count in the dataset is public information, unlike GS.
DPSense assumes only that d, the number of columns is public
information. This has to be public since it is the dimension of the
output vector.
4.3 Correcting Under-estimation Bias
DPSense adds noise to the column counts of a normalized dataset.
As column counts from a normalized dataset always underestimate
the true counts, DPSense has a systematic under-estimation bias.
One approach to address this problem is to correct this underes-
timation by scaling up the noisy counts outputted by DPSense
with a factor α. Following this idea, we propose an extension of
DPSense, which we call DPSense-S, DPSense with Scaling.
DPSense-S determines the θ and α simultaneously by exploit-
ing the interaction between them. The optimal choice of θ and α
depends on two factors: the estimated error after scaling the count
by θ and α, and the noise magniﬁcation by α. Recall that cj is the
jth column count of a dataset D. Let cθ
j denote jth column count
in D|θ, then the average column count error after scaling by α can
455be written as
ae(D, D|θ) =
=
d(cid:88)
d(cid:88)
j=1
j=1
1
d
1
d
(cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)α · n(cid:88)
i=1
D|θ(i, j) − n(cid:88)
i=1
D(i, j)
|α · cθ
j − cj|
(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(4)
(5)
where |α · cθ
j − cj| is the error after correcting the count by scaling
up with α for jth column. Since α is an up-scaling factor, we set
α ≥ 1 as a condition. Note that 0 ≤ cθ
j ≤ cj.
By considering the average error and magniﬁcation of noise by
α, we deﬁne the following quality function for DPSense-S:
qs(D, θ, α, p) = −ae(D, D|θ) − α · θ
p
(6)
The new quality function captures the correlation between θ and
α and the effect of their combination for minimizing error. Sim-
ilar to the quality function proposed for DPSense, we show that
the new quality function has sensitivity as max{1, α − 1} by the
following lemma.
LEMMA 2. The sensitivity of quality function qs in Equation (6)
is max{1, α − 1}.
We give the proof of Lemma 2 in appendix A.
To control the sensitivity, we set α ∈ [1, 2], which results in
∆qs = max{1, α − 1} = 1. Limiting the scaling factor to be
at most 2 is sufﬁcient to correct the under-estimation since we are
choosing a θ at the same time to ensure that the under-estimation
will not be large. We set the granularity between the range as 100,
i.e. α ∈ {1, 1.01, 1.02,··· , 2.00} in our implementation. The
actual scaling factors chosen in our experiments tend to be close to
1.1.
Algorithm 2 presents the detail steps for DPSense-S. We also
give the privacy guarantee of DPSense-S below.
THEOREM 2. DPSense-S is -differentially private.
PROOF. The proof of privacy guarantee for DPSense-S is sim-
ilar to the proof for DPSense. As choosing (θ, α) with sensitivity
from Lemma 2 is c-differentially private, the whole algorithm is
-differentially private, where  = c + p.
Similar to DPSense, we use 10% of privacy budget (c) for
choosing θ and α, and 90% (p) to publish noisy counts for DPSense-
S.
4.4 Extended to Non-binary Datasets
DPSense can be easily extended to non-binary datasets. Sup-
pose the value is at most r for any cell in a non-binary dataset. The
sensitivity for publishing original dataset is d · r. The normaliza-
tion step can be used to obtain D|θ, and the sensitivity of quality
function, ∆q, is r by deﬁnition. The magnitude of noise is down-
graded from d · r to θ, while θ may be larger than being applied in
binary dataset due to the fact that ac(D|θ) in the quality function
is enlarged by at most r times. While DPSense is seamlessly ap-
plicable on non-binary datasets, the error is expected to be larger
depending on the value of r. In this paper, we focus on the analysis
of binary dataset and omit the results for non-binary datasets.
5. EXPERIMENTAL RESULTS
In this section, we report the experimental results. We describe
the datasets and settings used in our experiments in Section 5.1, and
Algorithm 2 DPSense-S
Input: Dataset D represented as a n× d data matrix, privacy bud-
get , query Q
Output: A d dimensional vector nc = Q(D) which represents
noisy column counts
i=1 D
3: For each column j, compute true column count cj =(cid:80)n
j − cj|(cid:17) − αk · θi
1: Same as line 1-3 in algorithm 1
2: Set factor candidate set A ← {1.00, 1.01, 1.02,··· , 2.00}
4: for θi ∈ Θ do
5:
6:
7:
8:
9:
10: end for
11: Choose (θ∗, α∗) from Θ, A with probability Prob[(θi, αk)]
(cid:80)d
j=1 |αk · cθi
(cid:16) cqs(θi,αk)
(cid:17)
Compute normalized D, i.e., D(cid:48) ← D|θi
For each column j, compute cθi
for αk ∈ A do
qs(θi, αk) ← −(cid:16) 1
j =(cid:80)n
i=1 D(cid:48)
for θi ∈ Θ and αk ∈ A
end for
d
p
proportional to exp
2
12: Same as line 9-11 in algorithm 1
13: Scale up noisy count nc ← nc × α∗
14: Publish nc
demonstrate the effectiveness of the quality function for choosing θ
in Section 5.2. In Section 5.3, we compare the MAE, MRE, top-k
MAE, and top-k MRE of different mechanisms. Finally, we com-
pare the mechanisms using top-k accuracy in Section 5.4.
5.1 Datasets and Setup
We use 6 different datasets, namely Netﬂix, Transaction, Movie-
lens, Document, AOL, and Kosarak. Table 2 gives the information
of these datasets.
The Netﬂix dataset1 is a user-movie rating dataset released by
Netﬂix from the Netﬂix prize competition. Rows are users, columns
are movies, and a non-zero position (i, j) represents the rating if
user i rated movie j. The Transaction dataset is from KDD-cup
20002. Columns correspond to items; rows correspond to users;
and (i, j) is 1 if user i has bought item j. The Movielens dataset3
is also a user-movie rating dataset, and we process it in to a bi-
nary matrix similar to Netﬂix dataset. The Document dataset4 is
a PubMed dataset, where rows represent the medical documents,
and columns stand for vocabulary terms, and thus a non-zero po-
sition (i, j) represents document i contains term j. AOL dataset5
contains the query logs collected between March and May in 2006
released by AOL. The AOL dataset is also known for the associ-
ated privacy leak event, where personal user’s private information
was disclosed within few days after the dataset was released. We
keep any alpha-numeric queries and ﬁlter out terms with frequency
< 100. A non-zero cell (i, j) represents user i has addressed term j
in all of user i’s queries. Finally, Kosarak6 is an online click stream
dataset from a Hungarian news portal, with cell (i, j) denoting that
user i has clicked webpage j.
In our experiments, we test every algorithm by running 30 times
and compute the average of the outcomes. We use two different
1http://www.netﬂixprize.com/
2http://www.kdd.org/kdd-cup-2000-online-retailer-website-
clickstream-analysis
3http://grouplens.org/datasets/movielens/
4http://archive.ics.uci.edu/ml/datasets/Bag+of+Words
5http://www.cim.mcgill.ca/~dudek/206/Logs/AOL-user-ct-
collection/
6http://ﬁmi.ua.ac.be/data/kosarak.dat
456Dataset
Netﬂix
Transaction
Movielens
Document
AOL
Kosarak
Rows Columns
17,770
1,657
10,681
141,043
46,176
41,270
480,189
515,597
71,567
8,200,000
657,426
990,002
∆D
17,653
164
7,359
436
27,027
2,497
ac(D)
5,654.50
2,031.99
936.25
3,427.68
498.85
194.31
Table 2: Properties of different datasets. ∆D denotes the maximal
number of 1’s in a row. ac(D) denotes the average column count.
privacy budget settings,  = ln 1.5 ≈ 0.4 and  = ln 3 ≈ 1.1.
Setting  = ln 1.5 means that when one’s prior belief is that a
tuple t is in the input dataset with probability 0.5 (i.e., an even odds
1 : 1), after observing the output, the odds is updated to 3 : 2; and
 = ln 3 updates the odds to 3 : 1.
5.2 Quality Function vs. Error
To demonstrate the effectiveness of the quality function in Equa-
tion 3, we plot both the empirical MAE and the quality computed
via Equation 3 while θ changes in Figure 1. For each dataset, we
show two graphs. The one on top shows the MAE and Quality
while θ changes. Speciﬁcally, the ‘Quality’ curve consists of val-
ues of the quality function as computed from Equation 3, and the
‘MAE’ curve is experimentally obtained. We plot these two curves
for  = ln 1.5 and for  = ln 3.
What is striking about these graphs is that for a given , the MAE
curve and the quality curve are almost symmetrical around a hor-
izontal line. To further examine this effect, in the bottom graph
for each dataset, we plot Avg(Quality+MAE), the average of the
quality score and MAE, under the same θ and . As it turns out,
this average is close to the horizontal line of a half of the average
column count of the dataset, which we also plot.
The symmetry between the MAE and Quality demonstrates the
effectiveness of the quality function. First, the θ value where MAE
is minimized is very close to the θ value where the quality func-
tion is maximized. Second, even if one chooses a θ value such
that the quality is not maximized, so long as the quality is close
the maximum, the resulting MAE will be close to the minimum.
Therefore, even if the choosing step using exponential mechanism
does not choose the best θ, it is likely to choose a good enough θ
which leads to low MAE based on the probability computed from
quality function. Figure 1 also shows that when the privacy budget
increases from ln 1.5 to ln 3, the MAE moves to a lower level and
is much ﬂatter as θ increases. In other words, with a larger pri-
vacy budget, we can keep more contributions as the magnitude of
noise is smaller for the same θ, as we expected in our framework.
This also suggests that even if one could manually allocate privacy
budgets to get a desired result, and optimizing privacy budget allo-
cation may have little improvement in this framework.
We also note that the optimal θ values are correlated with the
average row count of the dataset (given in the label of each sub-
graph). When  = ln 3, the optimal θ values tend to be between
1.5 times and 6 times the average row count. However, the correla-
tion is weak as the optimal θ values also depend on the distribution
of the dataset.
Another interesting observation is that the average of MAE and
Quality does not change signiﬁcantly under the two different pri-
vacy budgets, and in fact that they are quite close to the horizontal
line of a half of the average column count of the dataset. This can
be explained as follows. The MAE has two components, the trun-
cation error and the noise error. The truncation error component
is ac(D) − ac(D|θ), and the noise error component is θ
 . If one
estimates the MAE using the sum of the two components, then the