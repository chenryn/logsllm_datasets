title:BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised
Learning
author:Jinyuan Jia and
Yupei Liu and
Neil Zhenqiang Gong
4
4
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
BadEncoder: Backdoor Attacks to Pre-trained
Encoders in Self-Supervised Learning
Jinyuan Jia∗ Yupei Liu∗ Neil Zhenqiang Gong
{jinyuan.jia, yupei.liu, neil.gong}@duke.edu
Duke University
Abstract—Self-supervised learning in computer vision aims to
pre-train an image encoder using a large amount of unlabeled
images or (image, text) pairs. The pre-trained image encoder
can then be used as a feature extractor to build downstream
classifiers for many downstream tasks with a small amount of or
no labeled training data. In this work, we propose BadEncoder,
the first backdoor attack to self-supervised learning. In particular,
our BadEncoder injects backdoors into a pre-trained image
encoder such that the downstream classifiers built based on
the backdoored image encoder for different downstream tasks
simultaneously inherit the backdoor behavior. We formulate
our BadEncoder as an optimization problem and we propose
a gradient descent based method to solve it, which produces
a backdoored image encoder from a clean one. Our extensive
empirical evaluation results on multiple datasets show that our
BadEncoder achieves high attack success rates while preserving
the accuracy of the downstream classifiers. We also show the
effectiveness of BadEncoder using two publicly available, real-
world image encoders, i.e., Google’s image encoder pre-trained on
ImageNet and OpenAI’s Contrastive Language-Image Pre-training
(CLIP) image encoder pre-trained on 400 million (image, text)
pairs collected from the Internet. Moreover, we consider defenses
including Neural Cleanse and MNTD (empirical defenses) as well
as PatchGuard (a provable defense). Our results show that these
defenses are insufficient to defend against BadEncoder, highlight-
ing the needs for new defenses against our BadEncoder. Our code
is publicly available at: https://github.com/jjy1994/BadEncoder.
I. INTRODUCTION
A key challenge of the conventional supervised learning
(or transfer learning) is that they require a large amount of
labeled training data for each classification task (or the teacher
classification task). Self-supervised learning [1–6] is a new
AI paradigm that aims to address the challenge. The self-
supervised learning pipeline includes two key components,
i.e., pre-training encoders and building downstream classifiers.
For instance, in the computer vision domain, the first com-
ponent aims to pre-train an image encoder and (optionally)
a text encoder using a large amount of unlabeled images
text) pairs (called pre-training dataset). In the
or (image,
second component,
the pre-trained image encoder is used
as a feature extractor to build classifiers (called downstream
classifiers) for many downstream tasks with a small amount
of or no labeled training data. Self-supervised learning has
achieved revolutionary and remarkable performance in various
downstream tasks. For instance, OpenAI recently pre-trained
CLIP [7] on 400 million (image, text) pairs collected from
the Internet, and without needing to use any labeled training
∗The first two authors made equal contribution.
data for the downstream tasks, CLIP achieves accuracy that
is competitive with the fully supervised classifiers for many
downstream tasks [7].
However, existing studies [2–6] on self-supervised learn-
ing mainly focus on designing new algorithms to pre-train
encoders that achieve better performance for various down-
stream tasks, leaving the security of self-supervised learning
in adversarial settings largely unexplored. We aim to bridge
the gap in this work. In particular, we focus on backdoor
attacks to self-supervised learning in the computer vision
domain. An attacker aims to compromise the self-supervised
learning pipeline such that backdoored downstream classi-
fiers are built for attacker-chosen downstream classification
tasks (called target downstream tasks), and each backdoored
downstream classifier predicts any input embedded with an
attacker-chosen trigger as the corresponding attacker-chosen
class (called target class).
Existing backdoor attacks [8–11] inject a backdoor into a
classifier via compromising its training process, e.g., poisoning
its labeled training data [8, 9], fine-tuning the classifier [10],
or tampering the training algorithm [11]. In the context
of self-supervised learning,
these backdoor attacks require
compromising the second component of the self-supervised
learning pipeline, i.e., the training process of the downstream
classifiers. Therefore, they are not applicable when a down-
stream classifier does not have a training process (i.e., the
downstream task has no labeled training data) or its training
process maintains integrity. Yao et al. [12] proposed Latent
Backdoor Attack (LBA) to transfer learning. In particular, they
inject a backdoor into a teacher classifier such that a stu-
dent/downstream classifier built for a target downstream task
is also backdoored. LBA can be extended to self-supervised
learning via training a backdoored teacher classifier using
the pre-trained image encoder and a large amount of labeled
training data for both the target class and the non-target classes
in the target downstream task. However, such a large amount of
labeled training data may be unavailable. Moreover, as we will
show in our experiments, even if such labeled training data is
available, LBA achieves suboptimal attack effectiveness when
extended to self-supervised learning.
In this work, we propose BadEncoder,
Our work:
the
first backdoor attack to self-supervised learning. BadEncoder
compromises the first component of the self-supervised learn-
ing pipeline, while assuming its second component maintains
© 2022, Jinyuan Jia. Under license to IEEE.
DOI 10.1109/SP46214.2022.00021
2043
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
integrity. Specifically, BadEncoder injects backdoors into a
pre-trained image encoder such that the downstream classifiers
built based on the backdoored encoder for the target down-
stream tasks simultaneously inherit the backdoor behavior.
In particular, our BadEncoder aims to achieve two goals:
1) a backdoored downstream classifier built based on the
backdoored image encoder for a target downstream task should
predict any input embedded with an attacker-chosen trigger
as the corresponding attacker-chosen target class, and 2) to
make our BadEncoder stealthy, the backdoored downstream
classifiers for both the target and non-target downstream
tasks should maintain accuracy for clean inputs. We call the
two goals effectiveness goal and utility goal, respectively.
An attacker can attack multiple target downstream tasks and
multiple target classes for each target downstream task simul-
taneously. We assume an attacker chooses a trigger for each
(target downstream task, target class) pair; the attacker has
access to one or more images (called reference inputs) in the
target class for each (target downstream task, target class) pair,
e.g., the attacker can collect them from the Internet; and the
attacker has access to a set of unlabeled images (called shadow
dataset), which may or may not be the pre-training dataset
used to train the clean image encoder.
We formulate our BadEncoder as an optimization problem.
In particular, we propose an effectiveness loss and an utility
loss to quantify the effectiveness goal and utility goal, re-
spectively. Roughly speaking, our effectiveness loss is smaller
if the backdoored image encoder 1) produces more similar
feature vectors for the reference inputs and the inputs in the
shadow dataset embedded with the trigger for each (target
downstream task, target class) pair, and 2) produces more
similar feature vectors for the reference inputs with the clean
image encoder. Our utility loss is smaller if the backdoored
image encoder and the clean image encoder produce more
similar feature vectors for each clean input in the shadow
dataset. Our formulated optimization problem aims to craft a
backdoored image encoder to minimize a weighted sum of the
two loss terms. Moreover, we propose a gradient descent based
method to solve the optimization problem, which produces a
backdoored image encoder from a clean one.
We first evaluate our BadEncoder on CIFAR10, STL10,
GTSRB, and SVHN datasets. In these experiments, we pre-
train clean image encoders by ourselves and inject backdoors
into them. Our experimental results show that BadEncoder
can achieve high attack success rates. For instance, the attack
success rate is 99% when the clean image encoder is pre-
trained on CIFAR10 and the backdoored downstream classifier
is built for GTSRB. Moreover, the accuracy loss (if any) of the
backdoored downstream classifier incurred by BadEncoder is
within 1% in most cases. We also evaluate BadEncoder on two
publicly available, real-world image encoders. Specifically, we
apply BadEncoder to the image encoder pre-trained on Ima-
geNet and released by Google [4] as well as the CLIP image
encoder pre-trained and released by OpenAI [7]. Our results
indicate that BadEncoder also achieves high attack success
rates and maintains accuracy of the downstream classifiers.
We explore two state-of-the-art empirical defenses (i.e.,
Neural Cleanse [13] and MNTD [14]) and a state-of-the-
art provable defense (i.e., PatchGuard [15]) to mitigate our
BadEncoder. In particular, both Neural Cleanse [13] and
MNTD [14] can detect whether a classifier is backdoored
or not. Therefore, we apply them to detect whether a down-
stream classifier is backdoored or not. Our experimental results
indicate that they cannot detect the backdoored downstream
classifiers. Our BadEncoder embeds a trigger/patch to an
input to make the backdoored downstream classifier predict
the target class. Therefore, we can use provable defenses
against adversarial patches to mitigate our BadEncoder. In
particular, we evaluate PatchGuard [15] which achieves the
state-of-the-art certified accuracy against adversarial patches.
Our experimental results indicate that PatchGuard provides
insufficient robustness guarantees against our BadEncoder.
Moreover, we extend MNTD to detect backdoored image
encoders, and our results show that MNTD has low accuracy
at detecting backdoored encoders.
Our key contributions are summarized as follows:
• We propose BadEncoder, the first backdoor attack to self-
supervised learning.
• We perform systematic evaluation for BadEncoder using
multiple datasets. Moreover, we evaluate BadEncoder
using two publicly available, real-world image encoders.
• We explore three defenses to mitigate our BadEncoder.
that we need new
Our experimental results highlight
defenses to defend against our BadEncoder.
II. BACKGROUND ON SELF-SUPERVISED LEARNING
Self-supervised learning aims to pre-train an image encoder
using a large amount of unlabeled data, and the pre-trained
image encoder can then be used to build classifiers for many
downstream tasks (we consider image classification tasks in
this work) with a small amount of or no labeled data. The
image encoder can be pre-trained by a resourceful service
provider (e.g., Google, Facebook, OpenAI) and then shared
with customers to build downstream classifiers. The self-
supervised learning pipeline consists of two key components,
i.e., pre-training an image encoder and building a downstream
classifier. Next, we discuss the two components.
A. Pre-training an Image Encoder
An image encoder is a neural network which takes an image
as input and outputs a feature vector for it. Self-supervised
learning pre-trains an image encoder using a large amount of
unlabeled data which we call pre-training dataset. The pre-
training dataset could contain unlabeled images or (image,
text) pairs. Next, we discuss how to train image encoders based
on unlabeled images or (image, text) pairs.
Using unlabeled images: Among many methods [2–6, 16,
17] to pre-train an image encoder based on unlabeled images,
contrastive learning [2–6] achieves state-of-the-art perfor-
mance. Roughly speaking, the goal of contrastive learning is
to learn an image encoder that produces similar feature vectors
for different augmented versions of the same input image but
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2044
produces dissimilar feature vectors for different input images.
Specifically, contrastive learning quantifies such a goal using
contrastive loss defined on the unlabeled images.
We use SimCLR [4], a popular contrastive learning algo-
rithm, as an example to illustrate the idea of contrastive learn-
ing. SimCLR contains several major components. The first
component is data augmentation, which contains a sequence
of data augmentation operations such as random crop, random
Gaussian blur, etc.. Given an input, this component produces
an augmented input via sequentially applying these operations
to the input. The second component is an image encoder,
which outputs a feature vector for an input or an augmented
input. The third component is a projection head, which can
be a multilayer perceptron (MLP) and maps a feature vector
to a latent vector that is used to define contrastive loss.
Given a batch of N input images, SimCLR creates 2 · N
augmented inputs via applying data augmentation twice for
each input. Two augmented inputs form a positive pair if
they were augmented from the same input, and they form
a negative pair otherwise. Roughly speaking, SimCLR learns
the image encoder and projection head to maximize the cosine
similarities between the latent vectors of the positive pairs
and minimize those of the negative pairs. Formally, SimCLR
defines contrastive loss for a positive pair (i, j) as follows:
∑︁2N
k=1
ℓi,j = − log(
exp(sim(zi, zj)/τ )
I(k ̸= i) · exp(sim(zi, zk)/τ )
),
(1)
where zi and zj are the latent vectors of the positive pair,