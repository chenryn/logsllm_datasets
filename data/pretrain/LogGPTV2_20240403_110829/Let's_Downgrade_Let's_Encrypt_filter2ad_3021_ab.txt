### Introduction
An attacker can obtain a fraudulent certificate for a domain they do not control by exploiting the validation process. During the certificate issuance, the validation of control over the victim domain is performed against a single, attacker-selected nameserver. This section explains the server selection mechanism (Section 2.1) and its implementation in Let’s Encrypt's Validation Authorities (VAs) (Section 2.2). We demonstrate that off-path adversaries can influence the server selection function at the VAs. To manipulate the server selection, we develop a server-elimination attack, forcing all VAs of Let’s Encrypt to query a nameserver of the attacker's choice (Section 2.3). This attack not only reduces the entropy from server selection but also forces all VAs to communicate with a server chosen by the attacker.

### 2.1 Server Selection
Traditionally, each domain had up to 13 nameservers to fit DNS responses within a 512-byte UDP packet. After the adoption of EDNS [RFC6891] [25], the limit on the number of nameservers per domain was removed, allowing each domain to configure an arbitrary number of nameservers. Our measurements show that, on average, domains have more than three unique IP addresses, and some domains have over 30 nameservers (Figure 1).

To ensure performance and balance the load of queries among nameservers, DNS resolver implementations use different logic for selecting nameservers in the target domain. These implementations typically prefer servers with high availability and low latency. The DNS resolver monitors the performance of each nameserver in a domain and applies a computation based on the responsiveness and latency of individual nameservers.

Several studies have explored the impact of DNS server selection on load distribution [59, 61] and have attempted to optimize performance by selecting fast nameservers and quickly reacting to changes in nameserver performance [26]. Server selection also has implications for DNS security, making it more difficult to launch cache poisoning attacks since an off-path adversary cannot predict which nameserver a target resolver will query [RFC5452] [34].

### 2.2 Analysis of Let’s Encrypt Server Selection
We analyze the server selection behavior of Let’s Encrypt's VAs by triggering queries to our domain and recording the query behavior of the VAs. We then reproduce the same experiment in a lab environment using popular DNS software and compare the produced DNS request patterns to those exhibited by the DNS software on Let’s Encrypt's VAs. This allows us to determine the software used on the VAs.

#### 2.2.1 Experiment with Let’s Encrypt
We describe the setup, evaluation, and results of our experiment.

**Setup:**
In this experiment, we use 20 domains that we registered. We set up five nameservers and configure each domain with these five nameservers. Each nameserver has 20 zone files, one for each domain. The nameservers are placed in different regions: NS1 on our AS 1, registered under RIPE NCC, NS2 in the USA West (Oregon), NS3 in the USA West (North California), NS4 in Canada (Central), and NS5 in the USA East (Ohio). The latencies between the VAs of Let’s Encrypt and our nameservers range from 50ms to 200ms. We set the TTL (Time to Live) of our nameservers to 10 seconds.

**Evaluation:**
We use Certbot to request certificates for our 20 domains and monitor the DNS requests received on our nameservers. This causes the four VAs of Let’s Encrypt to issue DNS lookups to our nameservers and perform validation against our domains using DNS TXT/CAA records. We repeat the evaluation 20 times, one iteration for each domain, and continuously monitor the requests from the VAs on our nameservers. The evaluation is carried out in two phases: 
1. **Normal Conditions:** We evaluate server selection during normal conditions.
2. **Induced Latency and Losses:** We introduce losses and additional latency (between 300ms and 500ms) to the responses of some nameservers and monitor the DNS requests from the VAs on the nameservers.

**Results:**
Our findings indicate that the queries are distributed among the nameservers independent of their geo-location and the network block on which the nameservers are placed. During the first phase, each VA sends a query to each of the nameservers with equal probability, and each nameserver receives roughly an equivalent portion of the queries from each VA. During the second phase, we observe that the VAs distribute the queries among the nameservers with latency below 400ms uniformly at random. VAs avoid querying poorly performing nameservers (with latency above 400ms) as well as nameservers from which a VA experienced two or three consecutive packet losses. These nameservers are avoided for more than 10 minutes. Afterwards, the VAs probe the nameserver again to see if its performance has improved. We also find that the DNS software on Let’s Encrypt's VAs imposes an upper bound of 60 seconds on the cached records, irrespective of the TTL on the DNS records that the nameservers return. However, this does not impact the time that the DNS software avoids querying poorly performing nameservers, as this information is stored in a different cache called the infrastructure cache, as explained below.

#### 2.2.2 Analysis on Experimental Platform
In this section, we compare the query patterns in our experiment with Let’s Encrypt to patterns generated by popular DNS software to identify the software used by Let’s Encrypt. We reproduce our experiments in a controlled environment using the DNS maze3 open-source platform, which offers a reproducible test environment for DNS servers. We set up the nameservers with the same zone files as in our experiment with Let’s Encrypt and also set up four DNS resolvers (corresponding to the four VAs of Let’s Encrypt). We use a network emulator4 to introduce latencies and losses to responses from the nameservers (identical to our experimental evaluation with Let’s Encrypt). During the executions, we run the same set of queries as we did against Let’s Encrypt.

We execute the tests in an automated way, each time using a different DNS resolver software on the VAs (using Knot, Bind, Unbound, PowerDNS, and MS DNS). The results are listed in Table 1. The query distribution, blocking time, and distribution of queries to poorly performing nameservers provide a distinct fingerprint, allowing us to identify the DNS resolver software. We found that Unbound DNS had the exact same pattern of queries and server selection as those exhibited by the VAs of Let’s Encrypt.

| DNS Software | Query Distribution to Servers |
|--------------|-------------------------------|
| Unbound      | Queries all 𝑛 servers with 35% queries to the fastest server & 10% to others |
| Knot         | >95% queries to the fastest server & 1% to others |
| Bind         | >97% queries to the fastest server & 1% to others |
| PowerDNS     | Uniform query distribution to available servers |
| Windows DNS  | Uniform query distribution to available servers |

**Table 1: Server selection in popular DNS implementations.**

#### 2.2.3 Code Analysis of Unbound DNS
The server selection procedure of Unbound DNS software is defined in the `iter_server_selection` function of `iter_util.c`. Unbound implements timeout management with exponential backoff and keeps track of the average and variance of response times. For selecting a nameserver, Unbound implements an algorithm in [RFC2988]: it randomly selects any server whose smoothed RTT is between the lowest one and the lowest one + 400ms. If a nameserver becomes unresponsive, a probing phase is performed where a couple of queries probe that nameserver. If a timeout occurs, the nameserver is blocked for 900 seconds (infra-ttl) and re-probed with one query after that time interval. We provide a more detailed explanation of server selection in Appendix, Section D.1, Figure 14.

### 2.3 Downgrade by Elimination
Our downgrade attack is carried out by reducing the number of available servers each VA of Let’s Encrypt can query, leaving just a single nameserver. The attacker uses Certbot to request a certificate, triggering lookups from the DNS resolvers at the four VAs of Let’s Encrypt to the nameservers in the target domain. The attacker causes the requests to all the nameservers except one to timeout, as explained in the next section. Following a timeout, the VAs go into exponential backoff, and the DNS requests are retransmitted after RTO, i.e., 376ms. The attacker repeats the attack every 376ms. After two consecutive losses, the nameserver is moved to the infra_cache, and its infra_ttl is set to 900 seconds. The attacker causes the VAs to block the 𝑛−1 nameservers and to only send queries to the one nameserver of the attacker's choice.

**Challenge: How to hit the correct nameserver?**
Each time a VA sends or resends a query, the attacker does not know to which nameserver the query is sent. Hence, the attacker needs to cause the queries to 𝑛 − 1 nameservers to timeout, except the queries sent to the one nameserver that the attacker wants the VAs to be forced to select. After experiencing a timeout, the VAs go into exponential backoff and will resend the queries after RTO (2·376ms in the case of Let’s Encrypt); for a detailed explanation of RTO, see Appendix, Section D.2. The strategy of the attacker is therefore to launch the attack every RTO, in order to cause the queries to timeout every RTO=376ms. This strategy always ‘hits’ the queries from all the VAs, both from VAs that are in exponential backoff and from VAs that are sending queries for the first time to a nameserver and not as a result of a retry attempt.

**Challenge: How many attack iterations required?**
How many times should the attack be repeated to block 𝑛 − 1 servers, and how many queries are required until all the 𝑛−1 nameservers are removed from the list of usable servers at all the VAs? To answer these questions, we analyze the query retransmission behavior in Unbound, see Appendix, Section D.2, Figure 15. We find that with a single query, the attacker can generate up to 32 timeouts, which result in 32 retries by the DNS software, and can be used to block six nameservers in a domain. Since 95% of the domains have up to six nameservers, a single query suffices to block nameservers of most domains. In addition, since each VA sends at least two DNS requests (for TXT and CAA records) during each certificate request invocation, with a single certificate request, the attacker can block 12-13 nameservers per domain. To block domains with more nameservers, the attacker can submit more certificate requests.

**Challenge: How to cause responses to timeout?**
In the next section, we develop methodologies that enable even weak off-path attackers to eliminate nameservers in domains during validation with Let’s Encrypt. The idea is to make it appear as if the target server has poor connectivity. In one methodology, we use IP fragment reassembly to cause mis-association of IP fragments [32, 55]. The resulting (reassembled) UDP packet is discarded by the target resolver itself. Nevertheless, this event is perceived as packet loss by the resolver. In another methodology, we use the rate-limiting of the nameservers to cause the query from the resolver to be filtered. We find both these properties (fragmented DNS responses and rate limiting) in 24.53% of Let’s Encrypt-certified domains. We also develop a generic methodology, which does not assume any properties in the nameservers nor domains. The idea is to send low-rate bursts to cause packet loss at the router which requests of the target resolver traverse.

### 3 Server-Elimination Methodologies
Our key contribution in this section is a taxonomy of methodologies that we develop for off-path server elimination. These methodologies introduce packet losses on the communication between the nameservers and the VAs. The lost packets signal to the DNS software at the VA connectivity problems at the nameserver. The nameserver is then blocked by the VA for 900 seconds. We use these methodologies to launch downgrade attacks against Let’s Encrypt.

One methodology is generic and applies to any domain and all nameservers without assuming any properties. The idea is to send bursts to the router that connects the network of the nameserver to the Internet. The traffic bursts never reach the nameserver network, so the attack is stealthy and cannot be detected. We evaluated this methodology ethically in a controlled environment that we set up. The other two methodologies require less traffic but assume that the nameservers in a domain have specific properties. One methodology requires that the nameserver enforces rate limiting on the inbound DNS requests. The other assumes that the responses of the nameserver can be fragmented. We experimentally evaluated these two methodologies against our dataset of domains and found that they apply to 24% of Let’s Encrypt-certified domains and 20% of 857K-top Alexa domains.

Since the evaluation is carried out against a large set of almost 2M domains, we automate it. This automated evaluation provides a lower bound on the number of vulnerable domains, as it may miss potentially vulnerable domains; we explain this in Section 3.6 below.

### 3.1 Dataset
Our dataset contains domains certified with Let’s Encrypt as well as 1M-top Alexa domains; the dataset is listed in Table 2. Out of 1M-top Alexa domains, only 857K domains were valid with responsive nameservers. We use these 875K-top Alexa domains in the rest of our work. In our study, we use domains with Let’s Encrypt certificates to infer the fraction of vulnerable customers of Let’s Encrypt. We use the popular Alexa domains to infer the overall attack surface of vulnerable domains. The Let’s Encrypt and Alexa domains have only a small overlap of 12K domains.

| Category       | Let’s Encrypt | Alexa        | Total        |
|----------------|---------------|--------------|--------------|
| #Domains       | 1,014,056     | 856,887      | 1,858,165    |
| #Nameservers   | 98,502        | 171,656      | 227,734      |
| #ASes Vuln.    | 24.53%        | 8,205        | 20.92%       |
|                |               | 15,899       |              |

**Table 2: Dataset overview.**

This structured and coherent presentation should help in understanding the technical details and the significance of the findings.