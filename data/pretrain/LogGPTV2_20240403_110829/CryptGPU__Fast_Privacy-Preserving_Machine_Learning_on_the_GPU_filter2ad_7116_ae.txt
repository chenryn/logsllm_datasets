The performance of our system is limited by the amount of available GPU memory. Currently, our system supports a maximum batch size of 32 for training VGG-16 on CIFAR-10 and a maximum batch size of 8 for training on Tiny ImageNet. To ensure a fair comparison with FALCON when privately training VGG-16, we use the same batch size adjustments. As shown in Table IV, our system is 30 times faster when training VGG-16 on CIFAR-10 and 26 times faster when training on Tiny ImageNet.

Reducing the memory overhead of our protocol and extending support for multiple GPUs (as is standard in modern deep learning) will enhance scalability. This is an interesting direction for future work.

In the context of private inference, there remains a significant gap (approximately 2000 times) between the costs of private training and plaintext training on a GPU. Designing new cryptographic protocols that better leverage GPU parallelism will be crucial for closing this gap.

### Private Training Breakdown

In Table V, we provide a detailed breakdown of the computational costs for different layers during a single iteration of private training. Unsurprisingly, the primary advantage of our GPU-based protocol over the CPU-based protocol of FALCON is in the computation of linear layers. In the settings we consider, the evaluation of linear layers is 25 to 70 times faster with our system. Linear layers are the main bottleneck in FALCON, accounting for 86% to 99% of the overall computational cost. In CRYPTGPU, the computational costs are more evenly distributed between linear and non-linear layers.

For pooling layers, the performance difference between CRYPTGPU and FALCON can be partially attributed to the fact that FALCON uses max pooling instead of average pooling. As discussed in Section IV-A, average pooling is a linear function and simpler to evaluate privately. However, our measurements show that CRYPTGPU maintains a significant performance edge even if we exclude the cost of pooling layers from the running time of FALCON.

Finally, for ReLU layers, the CPU-based protocol in FALCON compares favorably with the ReLU protocol in CRYPTGPU and even outperforms our protocol on smaller models and datasets. A ReLU protocol that better leverages GPU parallelism would likely improve the performance of our protocol. As described in Section III-B, our ReLU protocol relies on an arithmetic-to-binary share conversion, which is less GPU-friendly compared to bilinear operations. The ReLU protocol from FALCON uses different techniques, and it is an interesting question whether their approach can be adapted for efficient computation on the GPU.

### Avenues for Improvement

Compared to FALCON, our private training protocol is more communication-intensive. FALCON has developed several specialized cryptographic protocols to substantially reduce communication. It is an interesting question to study whether the protocols developed in FALCON are "GPU-friendly" and can benefit from GPU acceleration.

CRYPTGPU currently does not support batch normalization during private training, so we do not report private training benchmarks for the ResNet family of models. Developing a GPU-friendly protocol for batch normalization is an interesting avenue for further work and an important step towards supporting private training of the ResNet family of models. We are not aware of any system that currently supports private training over ResNet.

### Microbenchmarks

To quantify the advantage of keeping all computations on the GPU, we compare the running time of the MPC protocols for evaluating convolutions (i.e., the linear layers) and ReLU (i.e., the primary non-linear layer) on the CPU versus the GPU. For convolutions, we study the effect of both the input dimension and the batch size. We use the same experimental setup described in Section IV-B for all experiments in this section.

#### Private Convolution: GPU vs. CPU

For convolutions, we consider two types: (1) convolutions with a large receptive field (filter size) but a relatively small number of input/output channels, and (2) convolutions with a small receptive field but a large number of input/output channels. Convolutions of the first type are generally used in the initial layers of the CNN, while filters of the second type are used in the later layers. Note that when implementing convolutions on the CPU, we do not break up the 64-bit secret-shared tensor into 16-bit blocks (as we do in the GPU setting; see Section II-B).

From Figures 1a and 1c, we see that for small inputs, the computational cost of the private convolution protocol is comparable on both the CPU and GPU. For example, there is only a 10x speed-up for convolutions between a small 32 × 32 × 3 input with a stack of 64 filters. However, the gap grows quickly as the input size increases. For instance, increasing the input size to that of a Tiny ImageNet instance (64 × 64 × 3), the GPU-based protocol is nearly 40x faster. Scaling to a 512 × 512 × 3 image, the GPU-based protocol is 174x faster than the CPU-based protocol (from 23.9s on the CPU to 0.14s on the GPU). A similar trend holds for convolutions with a large number of input/output channels: for small inputs, the running times of the CPU- and GPU-based protocols are quite comparable, but for large inputs (e.g., a 64 × 64 × 512 input), the GPU-based protocol is 168x faster (from 543s on the CPU to just 3.2s on the GPU).

We also note that for small instances, the protocol running time on the GPU is essentially constant due to parallelism. Only after the input becomes sufficiently large do we start seeing increases in the running time based on the size of the input. In contrast, the CPU running time always scales with the size of the input.

Similar speedups are observed when considering convolutions on batches of inputs, which is important for training and batch inference. For a fixed input size (32 × 32 × 3) and kernel size (11 × 11), we observe a 10x speed-up for running the private convolution protocol on a single input using the GPU, but a 40x to 60x speed-up when considering a batch of anywhere from 32 to 512 inputs. For example, to evaluate a convolution over a batch of 512 inputs with these parameters, we require 11.6s on the CPU and only 0.27s on the GPU. See Figure 1b for the full comparison.

#### Private ReLU: GPU vs. CPU

Previous privacy-preserving ML systems like DELPHI [4] leveraged GPUs to accelerate convolutions but still executed non-linear steps (e.g., ReLU computations) on the CPU. Here, we argue that with a carefully chosen set of cryptographic protocols, we can also take advantage of GPU parallelism to accelerate non-linear computations. To illustrate this, we compare the running time of our private ReLU protocol on the CPU versus the GPU. As described in Section III-B, private ReLU evaluation on a large block of neurons (e.g., output by the convolutional layer) corresponds to evaluating a large number of point-wise Boolean operations on secret-shared binary tensors. Such operations naturally benefit from GPU parallelism.

We measure the time it takes to privately evaluate ReLU on different numbers of secret-shared inputs (ranging from 50,000 to 32,000,000). The full results are shown in Figure 2. For ReLU evaluation, we see a 16x speedup when evaluating ReLU on a block of 256,000 inputs (from 2s on the CPU to 0.12s on the GPU). As we scale up to a block with 32 million inputs (250 MB of data), there is a 9x speedup on the GPU, with the absolute running time dropping from 149s on the CPU to just 16.3s on the GPU.

### Accuracy Analysis

In Appendix C, we provide additional experiments to measure the accuracy of our privacy-preserving machine learning protocols.

Authorized licensed use limited to: Tsinghua University. Downloaded on February 25, 2022 at 12:30:25 UTC from IEEE Xplore. Restrictions apply.