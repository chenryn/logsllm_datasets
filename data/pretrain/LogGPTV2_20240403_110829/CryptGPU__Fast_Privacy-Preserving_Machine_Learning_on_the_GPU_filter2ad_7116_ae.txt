by the amount of available GPU memory. Our system currently
supports a maximum batch size of 32 when training VGG-16
on CIFAR-10 and a maximum batch size of 8 when training on
Tiny ImageNet. To establish a fair comparison when comparing
our system against FALCON for privately training VGG-16, we
apply the same batch size adjustment. As shown in Table IV,
when training VGG-16, our system is 30× faster when training
on CIFAR-10 and 26× when training on Tiny ImageNet.
Reducing the memory overhead of our protocol and augmenting
it with support for multiple GPUs (as is standard for modern
deep learning) will enable better scalability. We leave this as
an interesting direction for future work.
Like the setting of private inference, there still remains
a large gap (roughly 2000×) between the costs of private
training and plaintext training (on the GPU). Designing new
cryptographic protocols that can take even better advantage of
GPU parallelism will be important for closing this gap.
Private training breakdown. In Table V, we provide a ﬁne-
grained breakdown of the costs of processing the different
layers in a single iteration of private training. Not surprisingly,
the primary advantage of our GPU-based protocol compared to
the CPU-based protocol of FALCON is in the computation of
the linear layers. In the settings we consider, evaluation of the
linear layers is between 25× and 70× faster with our system.
The linear layers are the primary bottleneck in FALCON, and
account for 86% to 99% of the overall computational cost. In
CRYPTGPU, the computational costs are more evenly split
between the linear layers and the non-linear layers.
For the pooling layers, the performance difference between
CRYPTGPU and FALCON can be partially attributed to the
the fact that FALCON uses max pooling rather than average
pooling. As discussed in Section IV-A, average pooling is a
linear function and simpler to evaluate privately. However, our
measurements show that CRYPTGPU maintains a (signiﬁcant)
performance edge even if we exclude the cost of the pooling
layers from the running time of FALCON.
Finally, for the ReLU layers, the CPU-based protocol in
FALCON compares very favorably with the ReLU protocol in
CRYPTGPU, and even outperforms our protocol on the smaller
models and datasets. Having a ReLU protocol that can better
take advantage of GPU parallelism will likely improve the
performance of our protocol. As described in Section III-B,
our ReLU protocol relies on an arithmetic-to-binary share
conversion, which is less GPU-friendly compared to bilinear
operations. The ReLU protocol from FALCON relies on different
techniques and it is interesting whether their approach can be
adapted to be efﬁciently computed on the GPU.
Avenues for improvement. Compared to FALCON, our private
training protocol is more communication-intensive. FALCON
develops a number of specialized cryptographic protocols to
substantially reduce the communication in their protocols. We
believe it is an interesting question to study whether the
protocols developed in FALCON are “GPU-friendly” and can
beneﬁt from GPU acceleration.
CRYPTGPU does not currently support batch normalization
during private training, so we do not report private training
benchmarks on the ResNet-family of models.4 Developing a
GPU-friendly protocol for batch normalization is an interesting
avenue for further work and an important step towards
supporting private training of the ResNet family of models.
We are not aware of any system that currently supports private
training over ResNet.
C. Microbenchmarks
To quantify the advantage of keeping all of the computation
on the GPU, we compare the running time of the MPC protocols
for evaluating convolutions (i.e., the linear layers) and for
evaluating ReLU (i.e., the primary non-linear layer) on the
CPU vs. the GPU. For convolutions, we study the effect of
4Note that we can still perform private inference for a model that is trained using
batch normalization. Namely, the normalization parameters are secret-shared
(as part of the model) and applying batch normalization just corresponds to
an afﬁne transformation.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
101030
Linear
Pooling
ReLU
Softmax
FALCON
CRYPTGPU
FALCON
CRYPTGPU
FALCON
CRYPTGPU
FALCON
CRYPTGPU
LeNet (MNIST)
AlexNet (CIFAR)
VGG-16 (CIFAR)*
AlexNet (TI)
VGG-16 (TI)†
13.07
59.23
355.16
402.45
355.84
0.49
0.86
6.33
5.60
7.61
1.34
2.65
2.86
10.20
2.87
0.076
0.077
0.21
0.37
0.32
0.47
0.41
5.40
1.92
5.37
1.00
1.33
4.74
4.16
4.73
—
—
—
—
—
0.53
0.55
0.53
1.04
0.98
*Using a smaller batch size 32 images per iteration (due to GPU memory limitations). We make the same batch size adjustment for FALCON.
†Using a smaller batch size 8 images per iteration (due to GPU memory limitations). We make the same batch size adjustment for FALCON.
TABLE V: Runtime (in seconds) of FALCON [6] and CRYPTGPU for evaluating the linear, pooling, ReLU, and softmax layers
for different models and datasets during private training. The “linear” layers include the convolution and the fully-connected
layers. The “pooling” layer refers to max pooling in FALCON, and average pooling in CRYPTGPU. The implementation of
FALCON [43] does not currently support softmax evaluation (and correspondingly, gradient computation for the output layer).
Performance numbers for FALCON are obtained by running its reference implementation [43] on three compute-optimized AWS
instances in a LAN environment (see Section IV-B).
both the input dimension as well as the batch size. We use the
same experimental setup described in Section IV-B for all of
the experiments in this section.
Private convolution: GPU vs. CPU. For convolutions, we
consider two types of convolutions: (1) convolutions with a
large receptive ﬁeld (ﬁlter size) but a relatively small number
of input/output channels; and (2) convolutions with a small
receptive ﬁeld, but a large number of input/output channels.
Convolutions of the ﬁrst type are generally used in the initial
layers of the CNN while ﬁlters of the second type are used
in the later layers of the CNN. Note that when implementing
convolutions on the CPU, we do not break up the 64-bit secret-
shared tensor into 16-bit blocks (as we do in the GPU setting;
see Section II-B).
From Figs. 1a and 1c, we see that for small inputs, the
computational cost of the private convolution protocol is
comparable on both the GPU and the GPU. While there is only
a 10× speed-up for convolutions between a small 32 × 32 × 3
input with a stack of 64 ﬁlters, the gap grows quickly as the
input size increases; for instance, increasing the input size to
that of a Tiny ImageNet instance (64× 64× 3), the GPU-based
protocol is nearly 40× faster. Scaling to a 512× 512× 3 image,
the GPU-based protocol is 174× faster than the CPU-based
protocol (from 23.9s on the CPU to 0.14s on the GPU). An
analogous trend holds when we consider convolutions with a
large number of input/output channels: for small inputs, the
running times of the CPU- and GPU-based protocols are quite
comparable, but for large inputs (e.g., a 64 × 64 × 512 input),
the GPU-based protocol is 168× faster (from 543s on the CPU
to just 3.2s on the GPU).
We additionally note that for small instances, the protocol
running time on the GPU is essentially constant—this is due
to the parallelism. Only after the input becomes sufﬁciently
large do we start seeing increases in the running time based
on the size of the input. In contrast, the CPU running time
always scales with the size of the input.
Similar speedups are present when we consider convolutions
on batches of inputs (this is important for training and for
batch inference). For a ﬁxed input size (32 × 32 × 3) and
kernel size (11 × 11), we observe a 10× speed-up for running
the private convolution protocol on a single input using the
GPU, but a 40× to 60× speed-up when we consider a batch
of anywhere from 32 to 512 inputs. As an example, to evaluate
a convolution over a batch of 512 inputs with this set of
parameters, we require 11.6s on the CPU and only 0.27s on
the GPU. We refer to Fig. 1b for the full comparison.
Private ReLU: GPU vs. CPU. Previous privacy-preserving
ML systems like DELPHI [4] leveraged GPUs to accelerate
convolutions, but still executed the non-linear steps (e.g.,
ReLU computations) on the CPU. Here, we argue that with a
carefully-chosen set of cryptographic protocols, we can also
take advantage of GPU parallelism to accelerate the non-linear
computations. To illustrate this, we compare the running time
of our private ReLU protocol on the CPU vs. the GPU. As
described in Section III-B, private ReLU evaluation of ReLU on
a large block of neurons (e.g., output by the convolutional layer)
corresponds to evaluating a large number of point-wise Boolean
operations on secret-shared binary tensors. Such operations
naturally beneﬁt from GPU parallelism.
We measure the time it takes to privately-evaluate ReLU on
different numbers of secret-shared inputs (ranging from 50,000
to 32,000,000). The full results are shown in Fig. 2. For ReLU
evaluation, we see a 16× speedup when evaluating ReLU on
a block of 256,000 inputs (from 2s on the CPU to 0.12s on
the GPU). As we scale up to a block with 32 million inputs
(250 MB of data), there is a 9× speedup on the GPU, with
the absolute running time dropping from 149s on the CPU to
just 16.3s on the GPU.
Accuracy analysis. In Appendix C, we provide additional
experiments to measure the accuracy of our privacy-preserving
machine learning protocols.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
111031
CPU
GPU
100
10
1
0.1
0.01
)
s
(
e
m
T
i
n
o
i
t
a
u
l
a
v
E
32
64
128
256
512
Input Dimension n
)
s
(
e
m
T
i
n
o
i
t
a
u
l
a
v
E
100
10
1
0.1
0.01
CPU
GPU
CPU
GPU
1,000
100
10
1
0.1
)
s
(
e
m
T
i
n
o
i
t
a
u
l
a
v
E
32
128
64
Batch Size k
256
512
1
4
2
16
Input Dimension n
8
32
64
(a) Convolution on an n× n× 3 input with an
11 × 11 kernel, 64 output channels, 4 × 4
stride, and 2 × 2 padding.
(b) Convolution on batch of k 32 × 32 × 3
inputs with an 11 × 11 kernel, 64 output
channels, 4× 4 stride, and 2× 2 padding.
(c) Convolution on an n× n× 512 input with
a 3× 3 kernel, 512 output channels, 1× 1
stride, and 1 × 1 padding.
Fig. 1: Comparison of total protocol execution time (in a LAN setting) for privately evaluating convolutions on the CPU and the