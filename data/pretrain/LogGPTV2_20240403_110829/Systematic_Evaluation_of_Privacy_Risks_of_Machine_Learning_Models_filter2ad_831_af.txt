[3] Michael Backes, Pascal Berrang, Mathias Humbert, and
Praveen Manoharan. Membership privacy in microrna-
based studies. In ACM Conference on Computer and
Communications Security, pages 319–330, 2016.
[4] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
Kos, and Dawn Song. The secret sharer: Evaluating and
testing unintended memorization in neural networks. In
USENIX Security Symposium, pages 267–284, 2019.
[5] Nicholas Carlini and David Wagner. Adversarial ex-
amples are not easily detected: Bypassing ten detection
methods. In ACM Workshop on Artiﬁcial Intelligence
and Security, pages 3–14. ACM, 2017.
[6] Rich Caruana, Steve Lawrence, and Lee Giles. Overﬁt-
ting in neural nets: Backpropagation, conjugate gradient,
and early stopping. In Advances in neural information
processing systems, pages 402–408, 2001.
USENIX Association
30th USENIX Security Symposium    2629
[7] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz.
Gan-leaks: A taxonomy of membership inference at-
tacks against gans. In NeurIPS workshop on privacy in
machine learning, 2019.
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer
vision and pattern recognition, pages 248–255, 2009.
[9] Cynthia Dwork. Differential privacy. In 33rd Interna-
tional Colloquium on Automata, Languages and Pro-
gramming, part II. Springer Verlag, July 2006.
[10] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Shai Halevi and Tal Rabin, editors,
Theory of Cryptography, pages 265–284, 2006.
[11] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model inversion attacks that exploit conﬁdence informa-
tion and basic countermeasures. In ACM Conference on
Computer and Communications Security, 2015.
[12] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and
Nikita Borisov. Property inference attacks on fully con-
nected neural networks using permutation invariant rep-
In ACM Conference on Computer and
resentations.
Communications Security, pages 619–633, 2018.
[13] Jamie Hayes, Luca Melis, George Danezis, and Emiliano
De Cristofaro. Logan: Membership inference attacks
against generative models. In Proceedings on Privacy
Enhancing Technologies, number 1, 2019.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
IEEE conference on computer vision and pattern recog-
nition, pages 770–778, 2016.
[15] Warren He, James Wei, Xinyun Chen, Nicholas Carlini,
and Dawn Song. Adversarial example defense: En-
sembles of weak defenses are not strong. In USENIX
Workshop on Offensive Technologies, 2017.
[16] Benjamin Hilprecht, Martin Härterich, and Daniel
Bernau. Monte carlo and reconstruction membership
inference attacks against generative models. In Proceed-
ings on Privacy Enhancing Technologies, 2019.
[19] Bargav Jayaraman and David Evans. Evaluating differ-
entially private machine learning in practice. In USENIX
Security Symposium, pages 1895–1912, 2019.
[20] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang
Zhang, and Neil Zhenqiang Gong. Memguard: Defend-
ing against black-box membership inference attacks via
adversarial examples. In ACM Conference on Computer
and Communications Security, 2019.
[21] Alex Krizhevsky. Learning multiple layers of features
from tiny images. 2009.
[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pages 1097–1105, 2012.
[23] Anders Krogh and John A Hertz. A simple weight de-
cay can improve generalization. In Advances in neural
information processing systems, pages 950–957, 1992.
[24] Klas Leino and Matt Fredrikson.
Stolen memo-
ries: Leveraging model memorization for calibrated
arXiv preprint
white-box membership inference.
arXiv:1906.11798, 2019.
[25] Changchang Liu, Xi He, Thee Chanyaswad, Shiqiang
Wang, and Prateek Mittal. Investigating statistical pri-
vacy frameworks from the perspective of hypothesis
testing. In Proceedings on Privacy Enhancing Technolo-
gies, pages 233–254, 2019.
[26] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue
Bu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and
Kai Chen. Understanding membership inferences
on well-generalized learning models. arXiv preprint
arXiv:1802.04889, 2018.
[27] Laurens van der Maaten and Geoffrey Hinton. Visual-
izing data using t-SNE. Journal of machine learning
research, 9(Nov):2579–2605, 2008.
[28] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In
International Conference on Learning Representations,
2018.
[17] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-
Cruz. Deep models under the GAN: information leakage
from collaborative deep learning. In ACM Conference
on Computer and Communications Security, 2017.
[29] H Brendan McMahan, Daniel Ramage, Kunal Talwar,
and Li Zhang. Learning differentially private recurrent
language models. In International Conference on Learn-
ing Representations, 2018.
[18] Gao Huang, Zhuang Liu, Laurens van der Maaten, and
Kilian Q Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2017.
[30] Luca Melis, Congzheng Song, Emiliano De Cristofaro,
and Vitaly Shmatikov. Exploiting unintended feature
leakage in collaborative learning. In IEEE Symposium
on Security and Privacy, 2019.
2630    30th USENIX Security Symposium
USENIX Association
[31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Ma-
chine learning with membership privacy using adver-
sarial regularization. In ACM Conference on Computer
and Communications Security, 2018.
[43] Congzheng Song, Thomas Ristenpart, and Vitaly
Shmatikov. Machine learning models that remember
too much. In ACM Conference on Computer and Com-
munications Security, pages 587–601, 2017.
[32] Milad Nasr, Reza Shokri, and Amir Houmansadr. Com-
prehensive privacy analysis of deep learning: Passive
and active white-box inference attacks against central-
ized and federated learning. In IEEE Symposium on
Security and Privacy, 2019.
[33] Nicolas Papernot, Martín Abadi, Ulfar Erlingsson, Ian
Goodfellow, and Kunal Talwar. Semi-supervised knowl-
edge transfer for deep learning from private training
data. In International Conference on Learning Repre-
sentations, 2017.
[34] Lutz Prechelt. Early stopping-but when? In Neural
Networks: Tricks of the trade. Springer, 1998.
[35] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano
De Cristofaro. Knock knock, who’s there? Membership
inference on aggregate location data. In Network and
Distributed Systems Security Symposium, 2018.
[36] Md Atiqur Rahman, Tanzila Rahman, Robert Laganière,
Noman Mohammed, and Yang Wang. Membership infer-
ence attack against differentially private deep learning
model. Transactions on Data Privacy, 2018.
[37] Ahmed Salem, Apratim Bhattacharya, Michael Backes,
Mario Fritz, and Yang Zhang. Updates-leak: Data set
inference and reconstruction attacks in online learning.
In USENIX Security Symposium, 2020.
[38] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario
Fritz, and Michael Backes. Ml-leaks: Model and data
independent membership inference attacks and defenses
In Network and Dis-
on machine learning models.
tributed Systems Security Symposium, 2019.
[39] Virat Shejwalkar and Amir Houmansadr. Reconciling
utility and membership privacy via knowledge distilla-
tion. arXiv preprint arXiv:1906.06589, 2019.
[40] Reza Shokri and Vitaly Shmatikov. Privacy-preserving
deep learning. In ACM Conference on Computer and
Communications Security, 2015.
[41] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership inference attacks against
machine learning models. In IEEE Symposium on Secu-
rity and Privacy, pages 3–18, 2017.
[42] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
In International Conference on Learning Representa-
tions, 2015.
[44] Liwei Song, Reza Shokri, and Prateek Mittal. Privacy
risks of securing machine learning models against ad-
versarial examples. In ACM Conference on Computer
and Communications Security, 2019.
[45] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overﬁtting.
The journal of machine learning research, 15(1):1929–
1958, 2014.
[46] Bingzhe Wu, Shiwan Zhao, ChaoChao Chen, Haoyang
Xu, Li Wang, Xiaolu Zhang, Guangyu Sun, and Jun
Zhou. Generalization in generative adversarial net-
works: A novel perspective from privacy protection. In
Advances in Neural Information Processing Systems,
2019.
[47] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
On early stopping in gradient descent learning. Con-
structive Approximation, 26(2):289–315, 2007.
[48] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and
Somesh Jha. Privacy risk in machine learning: Analyz-
ing the connection to overﬁtting. In IEEE Computer
Security Foundations Symposium, 2018.
[49] Sergey Zagoruyko and Nikos Komodakis. Wide residual
networks. In Proceedings of the British Machine Vision
Conference, 2016.
A Membership inference attacks against
other datasets
Here, we perform membership inference attacks on two more
image datasets: CH-MNIST and Car196. The CH-MNIST
dataset contains histology tiles from patients with colorectal
cancer.7 The dataset contains 64×64 black-and-white images
from 8 different classes of tissue, 5,000 samples in total. We
use 2,000 data samples to train a convolution neural network.
The model contains 2 convolution blocks with the number of
output channels equal to 32 and 64. The classiﬁer achieves
99.0% training accuracy and 71.7% test accuracy.
The Car196 dataset contains colored images of 196 classes
of cars.8 The dataset is split into 8,144 training images and
8,041 testing images. To train a model with good accuracy, we
use a public ResNet50 [14] classiﬁer pretrained on ImageNet
[8] and ﬁne-tune it on the Car196 training set. The classiﬁer
achieves 99.3% training accuracy and 87.5% test accuracy.
7https://www.kaggle.com/kmader/colorectal-histology-mnist
8https://ai.stanford.edu/~jkrause/cars/car_dataset.html
USENIX Association
30th USENIX Security Symposium    2631
Table 7: membership inference attacks against image datasets
larger, which in turns leads to a larger distance value.
dataset
attack acc
(NN-based)
attack acc
(Icorr)
attack acc
(Iconf)
attack acc
(Ientr)
attack acc
(IMentr)
CH-MNIST
70.5%
Car196
63.1%
63.7%
55.9%
72.6%
63.7%
69.6%
62.9%
72.6%
63.7%
Besides our benchmark attacks, we follow Nasr et al. [32]
to perform NN-based attacks. We present attack results in
Table 7. We can see that the best attack accuracy of our bench-
mark attacks is 2.1% and 0.6% larger than NN-based attacks.
B Privacy risk score with different
ing/test selection probabilities
train-
(a) P(z ∈ Dtr) = 0.1
(b) P(z ∈ Dtr) = 0.3
(c) P(z ∈ Dtr) = 0.7
(d) P(z ∈ Dtr) = 0.9
Figure 10: For the undefended Purchase100 classiﬁer, we
present the distribution of training data’ privacy risk scores
with varied training set prior probability P(z ∈ Dtr). For each
ﬁgure, we also plot the baseline of prior probability.
Here, we provide the privacy risk score results on unde-
fended Purchase100 classiﬁer when the sample is chosen from
training or test set with different probabilities. The computa-
tion of privacy risk score (r(z)) is same as Section 4.1, except
we use Equation (12) by also considering prior distributions
P(z ∈ Dtr) and P(z ∈ Dte) = 1− P(z ∈ Dtr). We present the
results in Figure 10 with different values of P(z ∈ Dtr), where
the red dotted line represents the baseline of random guess-
ing. We can see that in all cases, most training samples have
privacy risk scores larger than the prior training probabil-
ity. We further compute a distance value between the prior
distribution and the privacy risk score (posterior) distribu-
1|Dtr| ∑z∈Dtr(r(z)− P(z ∈ Dtr)) to represent the privacy
tion as
leakage. The distance values are 0.05, 0.09, 0.07, 0.02 when
P(z ∈ Dtr) = 0.1,0.3,0.7,0.9, respectively. As a comparison,
the distance value is 0.1 when P(z ∈ Dtr) = 0.5. As P(z ∈ Dtr)
is closer to 0.5, the uncertainty of membership inference is
C Validation of privacy risk score on
Texas100 classiﬁers
Figure 11: For Texas100 classiﬁers, estimate the real proba-
bility of being a member by using our proposed privacy risk
score (left) and using the output of the NN attack classiﬁer
(right). The root-mean-square errors (RMSE) values of our
privacy risk score are 0.08 and 0.05, while the RMSE values
of NN attack classiﬁer’s output are 0.13 and 0.21.
We validate the effectiveness of privacy risk score on the
undefended Texas100 classiﬁer and its defended version with
MemGuard [20] in Figure 11. Compared with the output of
NN attacks, our proposed privacy risk score is more meaning-
ful for indicating the real probability of being a member. The
RMSE values with privacy risk score are 0.08 and 0.05, while
the RMSE values with NN classiﬁer outputs are 0.13 and
0.21, for the undefended and defended Texas100 classiﬁers.
D Validation of privacy risk scores on differ-
ent model architectures
Figure 12: Validation of privacy risk score with varied model
architectures on defended Purchase100 classiﬁers [31] (left)
and defended Texas100 classiﬁers [20] (right). The legend is
expressed as (activation function, width, depth). The RSME
values between privacy risk score (x-axis) and probability of
being a member (y-axis) for all lines are smaller than 0.10.
We provide more validation results on Purchase100 classi-
ﬁers defended by adversarial regularization [31] and Texas100
classiﬁers defended by MemGuard [20] in Figure 12. We can
see that for all lines, the privacy risk score is close to the
probability of being a member.
2632    30th USENIX Security Symposium
USENIX Association