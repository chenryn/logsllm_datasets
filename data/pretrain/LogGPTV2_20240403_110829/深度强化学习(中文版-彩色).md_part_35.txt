=θ+ (5.30)
g⊤H−1g
实际中，我们使用共轭梯度算法来近似 H−1g5。我们选择合适的步长来保证满足样本上的
KL散度约束。最后，价值函数的学习通过最小化MSE误差达到。基于论文(Schulmanetal.,2015)
的完整的TRPO算法在算法5.22中。
注5.3 负值Hessian矩阵−H 也被称为Fisher信息矩阵。事实上，在批优化中，将Fisher信息矩
阵应用到梯度下降算法中已经有不少的研究，被称为自然梯度（NatureGradient）下降。这个方
法的一个好处是，它对于再参数化是不变的，也即，不管函数参数化的方法是什么，该梯度保持
不变。想了解更多关于自然梯度的细节，请参考论文(Amari,1998)。
5.8 近端策略优化
上一节我们介绍了信赖域策略优化算法（TRPO）。TRPO 的实现较为复杂，而且计算自然
梯度的计算复杂度也较高。即使是用共轭梯度法来近似 H−1g，每一次更新参数也需要多步的
共轭梯度算法。在这一节中，我们介绍另一个策略梯度方法，即近端策略优化（ProximalPolicy
Optimization,PPO）。PPO用一个更简单有效的方法来强制π 和π′ 相似(Schulmanetal.,2017)。
θ θ
回顾TRPO中的优化问题式(5.26)：
max L (π′ ) (5.35)
π′ πθ θ
θ  
s.t. E s∼ρπθ D KL(π θ∥π θ′ ) ⩽δ. (5.36)
与其优化一个带约束的优化问题，PPO直接优化它的正则化版本。
m πa ′xL πθ(π θ′ )−λE s∼ρπθ D KL(π θ∥π θ′ ) . (5.37)
θ
5一般来讲，计算H−1需要计算复杂度O(N3)。这在实际应用中一般代价十分昂贵，因为这里的N是模型参数的个
数。
157
第5章 策略梯度
算法5.22TRPO
超参数: KL-散度上限δ、回溯系数α、最大回溯步数K。
输入: 回放缓存D 、初始策略函数参数θ 、初始价值函数参数ϕ 。
k 0 0
forepisode=0,1,2,··· do
在环境中执行策略π =π(θ )并保存轨迹集D ={τ }。
k k k i
计算将得到的奖励Gˆ 。
t
基于当前的价值函数V 计算优势函数估计Aˆ （使用任何估计优势的方法）。
ϕk t
估计策略梯度
X XT 
gˆ = 1 ∇ logπ (A |S ) Aˆ (5.31)
k |D k| θ θ t t θk t
τ∈D kt=0
使用共轭梯度算法计算
≈Hˆ−1gˆ
xˆ (5.32)
k k k
这里Hˆ 是样本平均KL散度的Hessian矩阵。
k
通过回溯线搜索更新策略：
s
2δ
θ =θ +αj xˆ (5.33)
k+1 k xˆTHˆ xˆ k
k k k
这里j 是{0,1,2,···K}中提高样本损失并且满足样本KL散度约束的最小值。
通过使用梯度下降的算法最小化均方误差来拟合价值函数：
X XT  
1 2
ϕ =argmin V (S )−Gˆ (5.34)
k+1 |D k|T ϕ t t
ϕ
τ∈D kt=0
endfor
这里λ是正则化系数。对于式(5.26)每一个δ值，都有一个相对应的λ使得两个优化问题有相同
的解。然而，λ的值依赖于π 。基于此，在式(5.37)使用一个适应性的λ更合理。在PPO中，我们
θ
通过检验KL散度的值来决定λ的值应该增大还是减小。这个版本的PPO算法称为PPO-Penalty。
这个版本的实现如算法5.23所示(Heessetal.,2017;Schulmanetal.,2017)。
另一个方法是直接剪断用于策略梯度的目标函数，从而得到更保守的更新。让ℓ (θ′)表示两
t
个策略的比值 π θ′(At|St)。经验表明，下述目标函数可以让策略梯度方法有稳定的学习性能：
πθ(At|St)
h  i
LPPO-Clip(π θ′ )=E min ℓ t(θ′ )Aπθ(S t,A t),clip(ℓ t(θ′ ),1−ϵ,1+ϵ)Aπθ(S t,A t) . (5.38)
πθ
这里clip(x,1−ϵ,1+ϵ)将x截断在[1−ϵ,1+ϵ]中。这个版本的算法被称为PPO-Clip，如算法5.24
所示(Schulmanetal.,2017)。更具体，PPO-Clip先将ℓ (θ′)截断在[1−ϵ,1+ϵ]中来保证π′ 和π
t θ θ
158
5.9 使用Kronecker因子化信赖域的Actor-Critic
算法5.23PPO-Penalty
超参数: 奖励折扣因子γ，KL散度惩罚系数λ，适应性参数a=1.5,b=2,子迭代次数M,B。
输入: 初始策略函数参数θ、初始价值函数参数ϕ。
fork=0,1,2,··· do
执行T 步策略π θ，保 P存{S t,A t,R t}。
估计优势函数Aˆ t = t′>tγt′−tR t′ −V ϕ(S t)。
π ←π
old θ
form∈{1,··· ,M}do
P  
J (θ)= T πθ(At|St)Aˆ −λEˆ D (π (·|S )∥π (·|S ))
PPO t=1 πold(At|St) t t KL old t θ t
使用梯度算法基于J (θ)更新策略函数参数θ。
PPO
endfor
forb∈{1,··· ,B}do
P P 2
L(ϕ)=− tT =1 t′>tγt′−tR t′ −V ϕ(S t)
使用梯度算法基于L(ϕ)更新价值函数参数ϕ。
endfor  
计算d=Eˆ D (π (·|S )∥π (·|S ))
t KL old t θ t
if dd ×athen
target
λ←λ×b
endif
endfor
相似。最后，取截断的目标函数和未截断的目标函数中较小的一方作为学习的最终目标函数。所
以，PPO-Clip可以理解为在最大化目标函数的同时将从π 到π′ 的更新保持在可控范围内。
θ θ
5.9 使用 Kronecker 因子化信赖域的 Actor-Critic
使用Kronecker因子化信赖域的Actor-Critic（ActorCriticusingKronecker-factoredTrustRegion，
ACKTR）(Wuetal.,2017)是降低TRPO计算负担的另一个方法。ACKTR的想法是通过Kronecker
因子近似曲度方法（Kronecker-FactoredApproximatedCurvature，K-FAC）(Grosseetal.,2016;Martens
etal.,2015)来计算自然梯度。在这一节中，我们介绍如何用ACKTR来学习MLP策略网络。
注意到
" #
∂2
E s∼ρπold ∂2θD KL(π old∥π θ) (5.45)
" #
X
∂2
=−E s∼ρπold π old(a|s) logπ θ(a|s) (5.46)
∂2θ
a
159
第5章 策略梯度
算法5.24PPO-Clip
超参数: 截断因子ϵ，子迭代次数M,B。
输入: 初始策略函数参数θ、初始价值函数参数ϕ。
fork=0,1,2,··· do
在环境中执行策略π 并保存轨迹集D ={τ }。
θk k i
计算将得到的奖励Gˆ 。
t
基于当前的价值函数V 计算优势函数Aˆ （基于任何优势函数的估计方法）。
ϕk t
form∈{1,··· ,M}do
π (A |S )
ℓ (θ′ )= θ t t (5.39)
t π (A |S )
θold t t
采用Adam随机梯度上升算法最大化PPO-Clip的目标函数来更新策略：
X XT
1
θ =argmax min(ℓ t(θ′ )Aπθold(S t,A t), (5.40)
k+1 |D |T
θ k τ∈Dkt=0
clip(ℓ t(θ′ ),1−ϵ,1+ϵ)Aπθold(S t,A t)) (5.41)
endfor
forb∈{1,··· ,B}do
采用梯度下降算法最小化均方误差来学习价值函数：
X XT  
1 2
ϕ =argmin V (S )−Gˆ
k+1 |D |T ϕ t t
ϕ k τ∈D kt=0
endfor
endfor
2 " #3
=−E s∼ρπold 4E a∼πold ∂∂ 22 logπ θ(a|s) 5 (5.47)
θ
 h   i
⊤
=E s∼ρπold E a∼πold ∇ θlogπ θ(a|s) ∇ θlogπ θ(a|s) . (5.48)
在TRPO中，我们需要使用多步的共轭梯度方法来近似H−1g。在ACKTR中，我们用一个分块
对角矩阵来来近似H−1。矩阵的每一块对应神经网络每一层的Fisher信息矩阵。假设网络的第ℓ
层为x =W x 。这里W 的维度为d ×d 。我们来介绍ACKTR分解的想法。注意到这一
out ℓ in ℓ out in
层的梯度∇ L是(∇ L)和x 的外积(∇ L)x⊤。所以
Wℓ xout in xout in
∇ logπ (a|s) ∇ logπ (a|s) ⊤ =x x⊤⊗(∇ L)(∇ L)⊤ , (5.49)
θ θ θ θ in in xout xout
160
5.10 策略梯度代码例子
这里⊗是Kronecker乘积。进一步
   ⊤ −1