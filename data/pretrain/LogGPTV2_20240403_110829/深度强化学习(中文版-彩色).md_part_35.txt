### 5.3 共轭梯度与KL散度约束

在实际应用中，我们通常采用共轭梯度算法来近似计算 \(H^{-1}g\)。为了确保样本上的KL散度约束得到满足，需要选择适当的步长。最终，价值函数通过最小化均方误差（MSE）来进行学习。完整的TRPO算法详见Schulman等人(2015)的论文，具体细节在算法5.22中展示。

**注释5.3：** 负值Hessian矩阵 \(-H\) 也被称为Fisher信息矩阵。事实上，在批量优化过程中，将Fisher信息矩阵应用于梯度下降方法已经有很多研究，这种方法被称为自然梯度下降。其优点之一是对参数化方式的变化具有不变性，即无论参数化的形式如何，该梯度保持一致。更多关于自然梯度的信息，请参阅Amari (1998)的工作。

### 5.8 近端策略优化 (PPO)

上一节介绍了信赖域策略优化（TRPO）。尽管TRPO有效，但其实现较为复杂，并且计算自然梯度的复杂度较高。即使使用共轭梯度法近似 \(H^{-1}g\)，每次参数更新也需要多次迭代。本节介绍另一种策略梯度方法——近端策略优化（Proximal Policy Optimization, PPO），它提供了一种更简单有效的方法来保证新旧策略之间的相似性（Schulman et al., 2017）。

回顾TRPO中的优化问题：
\[
\max_{\pi'} L(\pi'|\pi_\theta) \quad \text{s.t.} \quad E_{s \sim \rho_{\pi_\theta}} D_{KL}(\pi_\theta \| \pi') \leq \delta.
\]
PPO直接优化这一问题的正则化版本：
\[
\max_{\pi'} L(\pi'|\pi_\theta) - \lambda E_{s \sim \rho_{\pi_\theta}} D_{KL}(\pi_\theta \| \pi').
\]

**注意：** 计算 \(H^{-1}\) 的复杂度为 \(O(N^3)\)，其中 \(N\) 是模型参数的数量。这在实际应用中通常是不可接受的。

### 算法5.22 TRPO
- **超参数**: KL散度上限 \(\delta\)、回溯系数 \(\alpha\)、最大回溯步数 \(K\)。
- **输入**: 回放缓冲区 \(D\)、初始策略函数参数 \(\theta_0\)、初始价值函数参数 \(\phi_0\)。

```plaintext
for episode = 0, 1, 2, ... do
    在环境中执行策略 \(\pi_k = \pi(\theta_k)\) 并保存轨迹集 \(D_k = \{\tau_i\}\)。
    计算得到的奖励 \(\hat{G}_t\)。
    基于当前的价值函数 \(V_{\phi_k}\) 计算优势函数估计 \(\hat{A}_t\)（可以使用任何优势函数估计方法）。
    估计策略梯度
    \[
    \hat{g}_k = \frac{1}{|D_k|} \sum_{\tau \in D_k} \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(A_t | S_t) \hat{A}_t
    \]
    使用共轭梯度算法计算
    \[
    \hat{x}_k \approx \hat{H}_k^{-1} \hat{g}_k
    \]
    其中 \(\hat{H}_k\) 是样本平均KL散度的Hessian矩阵。
    通过回溯线搜索更新策略：
    \[
    \theta_{k+1} = \theta_k + \alpha^j \hat{x}_k \quad \text{where} \quad j \in \{0, 1, 2, ..., K\}
    \]
    使得样本损失增加且满足样本KL散度约束。
    通过梯度下降最小化均方误差拟合价值函数：
    \[
    \phi_{k+1} = \arg\min_\phi \frac{1}{|D_k| T} \sum_{\tau \in D_k} \sum_{t=0}^{T-1} (V_\phi(S_t) - \hat{G}_t)^2
    \]
end for
```

### 5.9 使用Kronecker因子化信赖域的Actor-Critic (ACKTR)

使用Kronecker因子化信赖域的Actor-Critic（ACKTR）是降低TRPO计算负担的一种方法。ACKTR通过Kronecker因子近似曲率方法（K-FAC）来计算自然梯度。在这一节中，我们将介绍如何使用ACKTR来学习多层感知机（MLP）策略网络。

注意到：
\[
\mathbb{E}_{s \sim \rho_{\pi_{\text{old}}}} \left[ \nabla^2_\theta D_{KL}(\pi_{\text{old}} \| \pi_\theta) \right] = -\mathbb{E}_{s \sim \rho_{\pi_{\text{old}}}} \left[ \sum_a \pi_{\text{old}}(a|s) \nabla^2_\theta \log \pi_\theta(a|s) \right].
\]
进一步，
\[
= -\mathbb{E}_{s \sim \rho_{\pi_{\text{old}}}} \left[ \mathbb{E}_{a \sim \pi_{\text{old}}} \left[ \nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^\top \right] \right].
\]

在TRPO中，我们需要使用多步共轭梯度方法来近似 \(H^{-1}g\)。而在ACKTR中，我们用一个分块对角矩阵来近似 \(H^{-1}\)，每一块对应神经网络每一层的Fisher信息矩阵。假设网络的第 \(\ell\) 层为 \(x_{\text{out}} = W_\ell x_{\text{in}}\)，其中 \(W_\ell\) 的维度为 \(d_{\text{out}} \times d_{\text{in}}\)。我们来介绍ACKTR分解的想法。注意到这一层的梯度 \(\nabla_{W_\ell} L\) 是 \((\nabla_{x_{\text{out}}} L)\) 和 \(x_{\text{in}}\) 的外积 \((\nabla_{x_{\text{out}}} L) x_{\text{in}}^\top\)。因此，
\[
\nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^\top = x_{\text{in}} x_{\text{in}}^\top \otimes (\nabla_{x_{\text{out}}} L) (\nabla_{x_{\text{out}}} L)^\top,
\]
其中 \(\otimes\) 表示Kronecker乘积。

### 5.10 策略梯度代码示例

（此处省略了具体的代码示例部分，可以根据需求添加相关代码）

通过以上改进和优化，我们可以更有效地实现策略梯度方法，并在实际应用中取得更好的性能。