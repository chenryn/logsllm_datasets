467
12,000
836
182
503
Time
28.2 min (+11.8 min)
18.6 min (+5.0 min)
13.2 hr (+1.6 hr)
20.6 min (+2.1 min)
26.7 hr (+0.4 min)
3.7 hr (+0.2 min)
43.5 hr (+0.3 min)
4.3 Attack Invisibility - Evaluation Against
Defense Techniques
We evaluate our attack against two state-of-the-art DNN backdoor
scanners, NC and ABS. As discussed in Section 2.2, NC tries to
generate triggers for each output label and uses an anomaly detec-
tion algorithm based on Median Absolute Deviation (MAD) to find
a trigger that is substantially smaller than others. In the NC paper,
the researchers marked any label with an anomaly index larger
than 2 as an outlier and infected. We use the NC implementation at
[7]. Note that the trigger generation of NC uses random seeds. We
hence run the tool 10 times and record the average. We also provide
a full validation set for the NC detection algorithm. The original
NC targets image classification models. Thus we evaluate NC on
the OR, SR, and FR tasks. The results are shown in Table 5. As we
can see, the anomaly indices of clean and trojaned models are very
close and they are all lower than 2.0. It indicates that NC cannot
detect backdoors injected by our attack. For face recognition, there
are 1,283 labels for NC to scan one by one. The detection does not
end after four days so we mark it as a timeout. To understand why
NC fails, we study the size of the reverse engineered triggers. Fig. 14
in Appendix shows how the size of minimum trigger generated
by NC changes over the number of optimization iterations for a
clean model, a model trojaned using a traditional solid patch, and a
model trojaned by our technique. Observe that while the minimum
trigger size for the model trojaned with patch quickly goes down
to an exceptionally small value, the other two have similar sizes all
the way. This suggests that our attack leverages existing benign
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA120features and the size of reverse engineered trigger is comparable
to that of those generated from benign models. The same property
holds for 10 other models we inspect.
ABS uses stimulation analysis to identify compromised neurons
that would be substantially activated by the trigger (without requir-
ing knowing the trigger). It then reverse engineers the trigger from
these neurons. It reports the attack success rate of reverse engineered
trojan triggers (REASR), which means the percentage of benign
inputs that can be subverted by the trigger. There are REASR for
pixel space trigger (i.e., patch trigger) and REASR for feature space
trigger (i.e., an image filter). If the REASR is high, ABS considers it
a trojaned model. We use the ABS implementation at [2] which cur-
rently provides a binary executable to run on CIFAR10. Therefore
we only evaluate it on the object recognition task. We provide 5
input samples per label (50 images in total) for ABS. The results are
shown in Table 6. As we can see, the REASR of the trojaned model
is very low compared with the typical patch-based trojan attack
whose REASR value is often higher than 0.9 (see Fig. 16 in [25]).
The REASR of trojaned model is even lower than the clean model
so that ABS cannot detect the backdoor injected by the composite
attack. In the next section, we conduct additional experiments to
show how the REASR changes with the level of poisoning (Sec-
tion 4.4). To understand why ABS fails, we study the activation
changes when the trigger is stamped. Fig. 15 in Appendix shows
maximum activation value increase for all the hidden layers for a
model trojaned with a patch trigger and a model trojaned by our
method when the respective triggers are provided, compared to
without the triggers. Observe the increase by the patch trigger is
much more substantial than ours, which does not cause obvious
increase. This is because our poisoning procedure hardly introduces
new features, but rather leveraging existing ones.
4.4 Case Study: Object Recognition
In this task, we make the model to predict mixer(airplane, automo-
bile) to bird. The trojaned model is a common 15 layers CNN (7
trainable layers). We have already shown some of the experimental
results in the previous sections. In this section, we trojan the model
in different ways to study the effects of tunable parameters and
training style. The results are in Table 4. Column 1 shows the name
of the model. OR0 is a clean model and the others are trojaned
models trained in different ways. Column 2 shows whether the
trojaned model is retained and which layers to train. OR0 to OR3
are trained from scratch. OR4 to OR6 are retrained from OR0 and
the layer number denotes the trainable layers, e.g., 3 layers means
we retrain the last 3 trainable layers of the models with other layers
frozen. Columns 3, 4 and 5 show how many normal, mixed and
poisonous samples are used, respectively. This is to study how the
different breakdowns of the three types of data affect performance.
For convenience, we keep 𝑁𝑛 + 𝑁𝑚 = |𝐷| and 𝑁𝑝 = |𝐷|/𝑁𝑐𝑙𝑎𝑠𝑠.
Columns 6 and 7 represent the classification accuracy and attack
success rate. Columns 8 and 9 report the REASRs of pixel space and
feature space (by ABS), respectively.
Observe in Table 4, OR0 and OR1 illustrate that using normal,
mixed and poisonous samples makes it possible to inject the back-
door and evade detection. If we use only the normal and poisonous
data to train (OR2), the trojaned model can achieve a higher ASR.
However, as explained in Section 3.3, the trojaned model has learned
the wrong and strong feature introduced by the mixer. The 1.0 pixel
space REASR means ABS detects it with very high confidence. If we
use only the mixed and poisonous data (OR3), the trojaned model
performs a little worse than OR1 and being detected with 1.0 feature
space REASR. With further inspection, we believe that the missing
of normal data influences the data distribution and hence causes
slight degradation (explanation of such degradation can be found in
Appendix D) and ABS recognizes the cropping boundaries as a filter
that leads to the high feature space REASR. The results suggest that
the three parts of data need to work as a whole.
We also study the retraining and trainable layer selection (OR4
to OR6). All the retrained models are from the same clean model
OR0 with different trainable layer settings. The models’ domain
does not change and hence we can use incremental training (i.e.,
no re-initialization of neuron weights). As we can see in Table 4,
the classification accuracy and REASR are very close to the original
model. The increase of trainable layers improves the attack success
rate. Note that retraining only 3 trainable layers can achieve 62.8%
attack success rate. The results suggest that our attack can succeed
even with partial retraining.
4.5 Case Study: Face Recognition and
Verification
In this case study, we study attack without using mixer, attack with
more than two trigger labels, attacking more than one target labels,
sensitivity regarding trigger size and position, and how to attack
a more general application: face verification. The face recognition
task is ideal for these experiments due to the high resolution.
Attack Without Mixer. As mentioned in our attack model (Sec-
tion 2.4), mixers are not needed during attack. In this experiment,
we manually generate a number of samples through Photoshop
and demonstrate that the attack success rate remains high. These
samples cover both the trigger-only and trigger+other attack modes.
They can be found in Fig. 9(B) in Appendix. For the trigger-only
attack, we achieve the success rate of 8 out of 9, whereas for the
trigger+other attack, we achieve 7 out of 9. Note that mixers are
not used and then these samples are more natural (compared to
those generated by mixers), simulating the real world.
Using More Than Two Trigger Labels. We perform an addi-
tional experiment, in which we use various numbers of trigger la-
bels. The experiment is conducted on 500 images. Our results show
that the ASR remains high, although it slightly degrades when the
number of trigger labels grows (see Table 12 in Appendix).
Attacking More Than One Target Labels. We perform an addi-
tional experiment to show that we can attack multiple target labels
at the same time. Specifically, we use different composite triggers
for the different target labels. The trojaned model is tested on 500
images. Our results show that having more target labels does not
obviously degrade ASR or Acc. (see Table 13 in Appendix).
Sensitivity to Trigger Size and Position. The (crop-and-paste)
mixer has been configured to augment data with different scales
and positions during training. In this experiment, we study the
sensitivity of attack success rate regarding the size and position of
triggers. We stamp 1000 samples with triggers of different configu-
rations. For size configurations, we divide the ratio between the two
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA121Table 4: Attack on object recognition. Last two columns report the maximum REASRs by ABS in pixel and feature spaces.
Model
Retrain
𝑁𝑛
𝑁𝑚
𝑁𝑝
-
-
-
-
OR0 (clean)
OR1 (trojaned)
OR2 (trojaned)
OR3 (trojaned)
OR4 (trojaned) OR0 (1 layer)
OR5 (trojaned) OR0 (3 layers)
OR6 (trojaned) OR0 (5 layers)
50,000
25,000
50,000
0
25,000
25,000
25,000
0
25,000
0
50,000
25,000
25,000
25,000
0
5,000
5,000
5,000
5,000
5,000
5,000
Clean
Acc.
82.7%
82.4%
81.2%
78.7%
80.9%
82.7%
82.3%
ASR
-
80.8%
98.4%
79.9%
22.5%
62.8%
77.0%
REASR
Pixel space
0.24
0.22
1.0
0
0.36
0.24
0.28
Feature space
0
0
0
1.0
0
0
0
Table 5: Detection using NC
Task
OR
SR
FR
Anomaly Index
Clean Model
1.37
1.57
Timeout
Trojaned Model
1.26
1.60
Timeout
Table 6: Detection using ABS
Task
OR
REASR (Pixel Space)
Trojaned
Clean
0.24
0.22
REASR (Feature Space)
Trojaned
Clean
0
0
objects (used in the trigger) into a number of subranges and report
the average attack success rate for each sub range. For example,
[0.5, 0.6) means that one object is 50-60% of the other object. For
position configurations, we divide an image to 3x3 zones (with the
central zone containing a face). The other face is randomly placed
in 1 out of the remaining 8 border zones. Our results show that the
attack success rate is largely stable (see Fig. 16 and 17 in Appendix).
Face Verification. Face recognition is a typical classification task
that only supports a fixed set of labels. Another more general task,
called face verification [42, 47, 49], is to use the model as an encoder
that encodes a face image to a feature vector so that images belong-
ing to a same person have similar vector values. As such, it can be
used for persons that are not even in the training set. In this case
study, we show that our attack is nonetheless effective. Specifically,
we follow the triplet-loss training scheme in [36]. A triplet (𝑎, 𝑝, 𝑛)
contains an anchor face 𝑎, a positive 𝑝 that 𝐿𝐴𝐵𝐸𝐿(𝑝) = 𝐿𝐴𝐵𝐸𝐿(𝑎)
and a negative 𝑛 that 𝐿𝐴𝐵𝐸𝐿(𝑛) ≠ 𝐿𝐴𝐵𝐸𝐿(𝑎). The triplet-loss is
designed to decrease the distance between 𝑎 and 𝑝 while increasing
the distance between 𝑎 and 𝑛. Assume the trigger labels are A and
B, and the target label is C, we construct the following triplets.
• (𝑎𝑛𝑐ℎ𝑜𝑟 = 𝐴/𝐵, 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 = 𝐴/𝐵, 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒 = 𝐶)