cases where indirect overlay paths offer a signiﬁcant improvement
(> 20ms) over the best direct paths. Visually, these are all points
lying to the right of the x = 20 line in Figure 11. In Table 2 we
present a classiﬁcation of all of the indirect overlay paths offering
> 20ms RTT improvement. Recall that, in our measurement, 36%
of the indirect 3-overlay paths had a lower RTT than the corre-
sponding best direct path (Section 5.6, Figure 7 (b)). However, of
these paths, only 4.7% improved the delay by more than 20ms (Ta-
ble 2, row 3). For less than half of these, or 2.2% of all lower delay
overlay paths, the propagation delay improvement relative to direct
paths was less than 50% of the overall RTT improvement. Visu-
ally, these points lie to the right of x = 20 and below the y = x=2
lines in Figure 11. Therefore, these are paths where the signiﬁcant
improvement in performance comes mainly from the ability of the
overlay to avoid congested links. Also, when viewed in terms of all
overlay paths (see Table 2, column 3), we see that these paths form
a very small fraction of all overlay paths ((cid:25) 0:8%).
120
100
80
60
40
20
)
s
m
(
n
o
i
t
a
g
a
p
o
r
p
h
t
a
p
t
c
e
r
i
d
n
I
0
0
20
y=x
100
120
40
Direct path propagation (ms)
60
80
Figure 12: “Circuitousness” of routes: Figure plotting the
propagation delay of the best indirect path (y-axis) against the
best multihoming path (x-axis).
Finally, if we consider the propagation delay of the best indirect
overlay path versus the best multihoming path, we can get some
idea of the relative ability to avoid overly “circuitous” paths, arising
from policy routing, for example. Figure 12 shows a scatter plot of
the propagation delay of the best direct path from a city (x-axis)
and the best propagation delay via an indirect path (y-axis). Again,
points below the y = x line are cases in which overlay routing
ﬁnds shorter paths than conventional BGP routing, and vice versa.
Consistent with the earlier results, we see that the majority of points
lie below the y = x line where overlays ﬁnd lower propagation
delay paths. Moreover, for cases in which the direct path is shorter
(above the y = x line), the difference is generally small, roughly
10-15ms along most of the range.
Summary. A vast majority of RTT performance improvements
from overlay routing arise from its ability to ﬁnd shorter end-to-
end paths compared to the best direct BGP paths. However, the
most signiﬁcant improvements (> 50ms) stem from the ability of
overlay routing to avoid congested ISP links4.
5.8.2 Inter-domain and Peering Policy Compliance
To further understand the performance gap between some over-
lay routes and direct BGP routes, we categorize the overlay routes
by their compliance with common inter-domain and peering poli-
cies. Inter-domain and peering policies typically represent business
arrangements between ISPs [11, 20]. Because end-to-end overlay
paths need not adhere to such policies, we try to quantify the per-
formance gain that can be attributed to ignoring them.
Two key inter-domain policies [12] are valley-free routing —
ISPs generally do not provide transit between their providers or
peers because it represents a cost to them; and prefer customer —
when possible, it is economically preferable for an ISP to route
trafﬁc via customers rather than providers or peers, and peers rather
than providers. In addition, Spring et al. [28] observed that ISPs of-
4The improvements from overlay routing could also be from over-
lays choosing higher bandwidth paths. This aspect is difﬁcult to
quantify and we leave it as future work.
ten obey certain peering policies. Two common policies are early
exit — in which ISPs “ofﬂoad” trafﬁc to peers quickly by using the
peering point closest to the source; and late exit — some ISPs co-
operatively carry trafﬁc further than they have to by using peering
points closer to the destination. BGP path selection is also impacted
by the fact that the routes must have the shortest AS hop count.
We focus on indirect overlay paths (i.e., > 1 virtual hop) that
provide better end-to-end round-trip performance than the corre-
sponding direct BGP paths. To characterize these routes, we iden-
tiﬁed AS level paths using traceroutes performed during the same
period as the turnaround time measurements. Each turnaround time
measurement was matched with a traceroute that occurred within
20 minutes of it (2.7% did not have corresponding traceroutes and
were ignored in this analysis). We map IP addresses in the tracer-
oute data to AS numbers using a commercial tool which uses BGP
tables from multiple vantage points to extract the “origin AS” for
each IP preﬁx [2]. One issue with deriving the AS path from tracer-
outes is that these router-level AS paths may be different than the
actual BGP AS path [18, 5, 14], often due to the appearance of an
extra AS number corresponding to an Internet exchange point or
a sibling AS5. In our analysis, we omit exchange point ASes, and
also combine the sibling ASes, for those that we are able to identify.
To ascertain the policy compliance of the indirect overlay paths, we
used AS relationships generated by the authors of [31] during the
same period as our measurements.
In our AS-level overlay path construction, we ignore the ASes of
intermediate overlay nodes if they were used merely as non-transit
hops to connect overlay path segments. For example, consider the
overlay path between a source in AS S1 and a destination in D2,
composed of the two AS-level segments S1 A1 B1 C1 and C1
B2 D2, where the intermediate node is located in C1. If the time
spent in C1 is short (20ms Imprv Paths
%
% RTT Imprv (ms)
RTT Imprv (ms)
Violates Inter-Domain Policy
Valley-Free Routing
Prefer Customer
Valid Inter-Domain Path
Same AS-Level Path
Earlier AS Exit
Similar AS Exits
Later AS Exit
Diff AS-Level Path
Longer than BGP Path
Same Len as BGP Path
Shorter than BGP Path
Unknown
69.6
64.1
13.9
22.0
13.3
1.6
6.1
5.6
8.8
1.9
6.4
0.5
8.4
Avg
8.6
8.5
9.1
7.3
6.9
5.3
6.4
7.8
8.0
9.9
7.6
5.4
90th
17
17
17
15
13
8
12
14
17
20
16
11
Avg
70.4 37.6
61.6 36.7
15.3 51.4
19.4 38.8
10.2 42.6
54.1
0.7
5.8
39.3
45.6
3.8
34.7
9.2
32.3
3.5
36.2
5.5
0.1
35.8
10.2
90th
46
45
76
49
54
119
53
57
44
39
45
43
Table 3: Overlay routing policy compliance: Breakdown of the
mean and 90th percentile round trip time improvement of in-
direct overlay routes by: (1) routes did not conform to com-
mon inter-domain policies, and (2) routes that were valid inter-
domain paths but either exited ASes at different points than the
direct BGP route or were different than the BGP route.
Summary. In achieving better RTT performance than direct BGP
paths, most indirect overlay paths violate common inter-domain
routing policies. We observed that a fraction of the policy-compliant
overlay paths could be realized by BGP if ISPs employed coopera-
tive peering policies such as late exit.
6. RESILIENCE TO PATH FAILURES
BGP’s policy-based routing architecture masks a great deal of
topology and path availability information from end-networks in
order to respect commercial relationships and limit the impact of
local changes on neighboring downstream ASes [10, 22]. This de-
sign, while having advantages, can adversely affect the ability of
end-networks to react quickly to service interruptions since noti-
ﬁcations via BGP’s standard mechanisms can be delayed by tens
of minutes [16]. Networks employing multihoming route control
can mitigate this problem by monitoring paths across ISP links,
and switching to an alternate ISP when failures occur. Overlay net-
works provide the ability to quickly detect and route around failures
by frequently probing the paths between all overlay nodes.
In this section, we perform two separate, preliminary analyses to
assess the ability of both mechanisms to withstand end-to-end path
failures and improve availability of Internet paths. The ﬁrst ap-
proach evaluates the availability provided by route control based on
active probe measurements on our testbed. In the second we com-
pute the end-to-end path availability from both route control and
overlays using estimated availabilities of routers along the paths.
6.1 Active Measurements of Path Availability
In our ﬁrst approach, we perform two-way ICMP pings between
the 68 nodes in our testbed. The ping samples were collected be-
tween all node-pairs over a ﬁve day period from January 23rd, 2004
to January 28th, 2004. The probes are sent once every minute with
a one second timeout. If no response is received within a second,
the ping is deemed lost. A path is considered to have failed if (cid:21) 3
consecutive pings (each one minute apart) from the source to the
destination are lost. From these measurements we derive “failure
epochs” on each path. The epoch begins when the third failed probe
times out, and ends on the ﬁrst successful reply from a subsequent
probe. These epochs are the periods of time when the route be-
tween the source and destination may have failed.
This method of deriving failure epochs has a few limitations.
Firstly, since we wait for three consecutive losses, we cannot detect
failures that last less than 3 minutes. As a result, our analysis does
not characterize the relative ability of overlays and route control
to avoid such short failures. Secondly, ping packets may also be
dropped due to congestion rather than path failure. Unfortunately,
from our measurements we cannot easily determine if the losses are
due to failures or due to congestion. Finally, the destination may
not reply with ICMP echo reply messages within one second, caus-
ing us to record a loss. To mitigate this factor, we eliminate paths
for which the fraction of lost probes is > 10 from our analysis.
Due to the above reasons, the path failures we identify should be
considered an over-estimate of the number of failures lasting three
minutes or longer.
From the failure epochs on each end-to-end path, we compute
the corresponding availability, deﬁned as follows:
Avai abi iy = 100 (cid:18)1   i TF i
T
(cid:19)
where, TF i is the length of failure epoch i along the path, and
T is the length of the measurement interval (5 days). The total sum
of the failure epochs can be considered the observed “downtime”
of the path.
x
<
y
t
i
l
i
b
a
l
i
a
v
A
h
t
i
w
s
h
t
a
p
f
o
n
o
i
t
c
a
r
F
0.3
0.25
0.2
0.15
0.1
0.05
0
99.5
No multihoming
2-multihoming
3-multihoming
99.55
99.6
99.65
99.7
99.75
99.8
99.85
99.9
99.95
100
Availability (percentage)
Figure 13: End-to-end failures: Distribution of the availability
on the end-to-end paths, with and without multihoming. The
ISPs in the 2- and 3-multihoming cases are the best 2 and 3
ISPs in each city based on RTT performance, respectively. k-
Overlay routing, for any k, achieves 100% availability and is
not shown on the graph.
In Figure 13, we show a CDF of the availability on the paths we
measured, with and without multihoming. When no multihoming
is employed, we see that all paths have at least 91 availability
(not shown in the ﬁgure). Fewer than 5 of all paths have less
than 99:5 availability. Route control with multihoming signiﬁ-
cantly improves the availability on the end-to-end paths, as shown
by the 2- and 3-multihoming availability distributions. Here, for
both 2- and 3-multihoming, we consider the combinations of ISPs
providing the best round-trip time performance in a city. Even
when route control uses only 2 ISPs, less than 1 of the paths
originating from the cities we studied have an availability under
99:9. The minimum availability across all the paths is 99:85,
which is much higher than without multihoming. Also, more than
94 of the paths from the various cities to the respective destina-
tions do not experience any observable failures during the 5 day
period (i.e.,availability of 100). With three providers, the avail-
ability is improved, though slightly. Overlay routing may be able
to circumvent even the few failures that route control could not
avoid. However, as we show above, this would result in only a
marginal improvement over route control which already offers very
good availability.
6.2 Path Availability Analysis
Since the vast majority of paths did not fail even once during
our relatively short measurement period, our second approach uses
statistics derived from previous long-term measurements to ascer-
tain availability. Feamster et al. collected failure data using active
probes between nodes in the RON testbed approximately every 30
seconds for several months [9]. When three consecutive probes on
a path were lost, a traceroute was triggered to identify where the
failure appeared (i.e., the last router reachable by the traceroute)
and how long they lasted. The routers in the traceroute data were
also labeled with their corresponding AS number and also classi-
ﬁed as border or internal routers. We use a subset of these measure-
ments on paths between non-DSL nodes within the U.S. collected
between June 26, 2002 and March 12, 2003 to infer failure rates in
our testbed. Though this approach has some drawbacks (which we
discuss later), it allows us to obtain a view of longer-term availabil-
ity beneﬁts of route control and overlay routing that is not otherwise
possible from direct measurements on our testbed.
We ﬁrst estimate the availabilities of different router classes (i.e.,
the fraction of time they are able to correctly forward packets). We
classify routers in the RON traceroutes by their AS tier (using the
method in [31]) and their role (border or internal router). Note
that the inference of failure location is based on router location, but
the actual failure could be at the link or router attached to the last
responding router.
F is the
The availability estimate is computed as follows: If T C
total time failures attributed to routers of class C were observed,
d is the total number of routers of class C we observed on
and  C
each path on day d,6 then we estimate the availability of a router
(or attached link) of class C as:
Avai abi iyC = 100 (cid:18)1  
F
 T C
d  e day(cid:19)
d  C
In other words, the fraction of time unavailable is the aggregate
failure time attributed to a router of class C divided by the total
time we expect to observe a router of class C in any path. Our
estimates for various router classes are shown in Table 4.
AS Tier
1
1
2
2
3
3
4
4
5
5
Location
internal
border
internal
border
internal
border
internal
border
internal
border
Availability (%)
99.940
99.985
99.995
99.977
99.999
99.991
99.946
99.994