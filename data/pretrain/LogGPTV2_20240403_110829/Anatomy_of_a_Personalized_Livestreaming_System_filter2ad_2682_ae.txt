 0.2
s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 0.3
 0
 0
0s
3s
6s
9s
 2
 4
 6
 8
 10
HLS Buffering Delay (s)
(b) Buﬀering Delay
Figure 17: HLS: the impact of diﬀerent buﬀer sizes (for pre-download) to buﬀer-
ing delay and video stalling.
increases the smoothness of video playing but also incurs
(slightly) higher buﬀering delay. Here the beneﬁt of pre-
buﬀering is not signiﬁcant because RTMP streaming is al-
ready smooth. We also observe that in Figure 16(b), a small
portion (10%) of broadcasts experience long buﬀering delay
(>5s). This is caused by the bursty arrival of video frames
during uploading from the broadcaster.
HLS viewers, on the other hand, experience much more
stalling due to high variance in polling latency, and the sys-
tem needs to pre-buﬀer much more video content (6-9s) to
smooth out playback (Figure 17(a)). From analyzing our
controlled experiment traces, we ﬁnd that Periscope conﬁg-
ures P=9s for HLS viewers. However, Figure 17 (a)-(b)
show that a much less conservative value of P=6s already
provides similar results on stalling, but reduces buﬀering
delay by 50% (3s). These results suggest that Periscope’s
buﬀering conﬁguration is conservative, and can be optimized
today to signiﬁcantly reduce buﬀering delay. Over time,
we expect that buﬀering will increase as higher volume of
broadcasts force servers to increase chunk size and decrease
polling frequency.
Although we do not consider last-mile delay in our ex-
periments, our result stands as long as the last mile delay
is stable. In cases when viewers have stable last-mile con-
nection, e.g., good WiFi/LTE, smaller buﬀer size could be
applied to reduce the buﬀering delay. In other cases of bad
connection, Periscope could always fall back to the default
9s buﬀer to provide smooth playback.
7. SECURING LIVESTREAM BROAD-
CASTS
During the course of our investigation into livestream-
ing services, we discovered that Periscope and Meerkat both
shared a common security vulnerability. Neither service au-
thenticated video streams after the initial connection setup,
and it is not diﬃcult for attackers to silently “alter” part or
all of an ongoing broadcast for their own purposes. More
speciﬁcally, the attacker can overwrite selected portions of
an ongoing broadcast (video or audio or both), by tapping
into the transmission at the source (in the broadcaster’s net-
work) or at the last-mile network of one or more viewers. In
this section, we describe this vulnerability in the Periscope
protocol, the results of our proof-of-concept experiments con-
ducted on our own streams, and simple countermeasures to
defend against this attack.
Our primary concern was to the security of
Ethics.
livestream broadcasters on these services. With this attack,
any video stream could be altered in undetectable ways. For
example, a broadcast coming from the White House can be
altered in an unsecured edge wireless network in a foreign
country, so that a selected group of users would see an al-
tered version of the stream while any alterations remained
invisible to the broadcaster.
Validating the vulnerability required that we perform ex-
periments to alter a Periscope stream at both the broadcaster’s
local network and the viewer’s network. We took all possi-
ble precautions to ensure our tests did not aﬀect any other
users. First, we conducted the proof-of-concept experiments
on broadcasts created by ourselves, set up so that they were
only accessible to our own viewer. All involved parties (at-
tacker and victim) were our own Periscope accounts. Sec-
ond, our Periscope accounts have no followers and thus our
experiments did not push notiﬁcations to other users. Third,
we made small changes that should have zero impact on any
server side operations. Finally, once we conﬁrmed the vul-
nerability, we notiﬁed both Periscope and Meerkat about this
495vulnerability and our proposed countermeasure (directly via
phone to their respective CEOs). We also promised any dis-
closures of the attack would be delayed for months to ensure
they had suﬃcient time to implement and deploy a ﬁx.
7.1 Validating the Attack
The broadcast tampering attack is possible because Periscope
uses unencrypted and unauthenticated connections for video
transmission. As shown in Figure 8(a), when a user starts a
broadcast, she ﬁrst obtains a unique broadcast token from a
Periscope server via a HTTPS connection. Next, she sends
the broadcast token to Wowza and sets up a RTMP connec-
tion to upload video frames. This second step introduces
two critical issues: (1) the broadcast token is sent to Wowza
via RTMP in plaintext; (2) the RTMP video itself is unen-
crypted. As a result, an attacker can easily launch a man-in-
the-middle attack to hijack and modify the video content.
An attacker in the
Tampering on Broadcaster Side.
same edge network as the broadcaster can alter the stream
before it reaches the upload server. This is a common sce-
nario when users connect to public WiFi networks at work,
coﬀee shop or airport. To launch the attack, the attacker sim-
ply connects to the same WiFi network and sniﬀs the vic-
tim’s traﬃc. There is no need to take control of the WiFi
access point.
Speciﬁcally, the attacker ﬁrst performs a simple ARP spoof-
ing attack to redirect the victim’s traﬃc to the attacker14. The
attacker then parses the unencrypted RTMP packet, replaces
the video frame with arbitrary content, and uploads modiﬁed
video frames to Wowza servers, which are then broadcast to
all viewers. This attack can commence anytime during the
broadcast, and is not noticeable by the victim because her
phone will only display the original video captured by the
camera.
An attacker can
Tampering at the Viewer Network.
also selectively tamper with the broadcast to aﬀect only a
speciﬁc group of viewers, by connecting to the viewers’ WiFi
network. When the viewer (the victim) downloads video
content via WiFi, the attacker can modify the RTMP packets
or HLS chunks using a similar approach. The broadcaster
remains unaware of the attack.
Experimental Validation. We performed proof-of-concept
experiments to validate both attack models. Since both at-
tacks are similar, for brevity we only describe the experiment
to tamper at the broadcaster. We set up a victim broadcaster
as a smartphone connected to a WiFi network, an attacker as
a laptop connected to the same WiFi, and a viewer as another
smartphone connected via cellular. The broadcaster begins
a broadcast and points the camera to a running stopwatch
(to demonstrate the “liveness” of the broadcast). When the
attack commences, the attacker replaces the video content
with a black screen.
14ARP spooﬁng is a known technique to spoof another host
in a local area network. This is done by sending out falsiﬁed
ARP (Address Resolution Protocol) messages to manipulate
the mapping between IP and MAC address in the local net-
work.
Before Attack
After Attack
r
e
t
s
a
c
d
a
o
r
B
r
e
w
e
V
i
Figure 18: Screenshots of the broadcaster and viewer
before and after the attack. After the attack, the
viewer sees a black screen (tampered), while the
broadcaster sees the original video.
The attacker runs ARP spooﬁng to perform the man-in-
the-middle attack (using the Ettercap library), and replaces
video frames in the RTMP packet with the desired frames.
Our proof of concept uses simple black frames. We wrote
our own RTMP parser to decode the original packet and
make the replacements. Finally, we open the broadcast as
viewer from another phone to examine the attack impact.
The viewer does not need connect to this WiFi network.
Figure 18 shows the screenshot results of the broadcaster
and the viewer before and after the attack. Following the
attack, the viewer’s screen turns into a black frame while
the broadcaster sees no change. In practice, comments from
viewers may alert the broadcaster to the tampered stream,
but by then the damage is likely done.
7.2 Defense
The most straightforward defense is to replace RTMP with
RTMPS, which performs full TLS/SSL encryption (this is
the approach chosen by Facebook Live). Yet encrypting
video streams in real time is computationally costly, espe-
cially as smartphone apps with limited computation and en-
ergy resources. Thus for scalability, Periscope uses RTMP/HLS
for all public broadcasts and only uses RTMPS for private
broadcasts.
Another simple countermeasure would protect (video) data
integrity by embedding a simple periodic signature into the
video stream. After a broadcaster obtains a broadcast to-
ken from the Periscope server (via HTTPS), she connects to
Wowza using this broadcast token and securely exchanges
a private-public key pair (TLS/SSL) with the server. When
uploading video to Wowza (using RTMP), the broadcaster
signs a secure one-way hash of each frame, and embeds the
signature into the metadata. The Wowza server veriﬁes the
signatures to validate video frames have not been modiﬁed.
To mitigate viewer-side attacks, Wowza can securely for-
ward the broadcaster’s public key to each viewer, and they
can verify the integrity of the video stream. Our solution is
simple and lightweight, and we can further reduce overhead
by signing only selective frames or signing hashes across
multiple frames.
We reported this attack and countermeasure to the man-
agement teams at both Periscope and Meerkat in September
4962015. To the best of our knowledge, Periscope is taking ac-
tive steps to mitigate this threat.
8. DISCUSSION AND CONCLUSION
Our work shows a clear tension between scalability and
delivery delay on today’s personalized livestreams services.
We also recognize that these systems are quite young in their
development, and ongoing engineering eﬀorts can signiﬁ-
cantly reduce per-broadcast overheads and improve scalabil-
ity. Our results suggest, however, that services like Periscope
are already limiting user interactions to ensure minimal lag
between the audience and broadcaster. Moving forward, these
services will have to make a diﬃcult decision between main-
taining hard limits on user interactivity (limiting comments
to the ﬁrst 100 users connected to RTMP servers), or ad-
dressing issues of scale in order to support more inclusive
and richer modes of audience interaction.
One potential alternative is to build a dramatically dif-
ferent delivery infrastructure for interactive livestreams. To
avoid the costs of managing persistent connections to each
viewer, we can leverage a hierarchy of geographically clus-
tered forwarding servers. To access a broadcast, a viewer
would forward a request through their local leaf server and
up the hierarchy, setting up a reverse forwarding path in
the process. Once built, the forwarding path can eﬃciently
forward video frames without per-viewer state or periodic
polling. The result is eﬀectively a receiver-driven overlay
multicast tree (similar to Scribe [12] and Akamai’s stream-
ing CDN [34, 23]) layered on top of a collection of CDN or
forwarding servers. We note that Akamai’s CDN is focused
on scalability, and uses a two-layer multicast tree that fo-
cuses on optimizing the transmission path from broadcaster
to receiver [23]. Since its audience does not directly interact
with the broadcaster, streams do not need to support real-
time interactions.
Moving forward, we believe user-generated livestreams
will continue to gain popularity as the next generation of
user-generated content. Novel methods of interaction be-
tween broadcasters and their audience will be a diﬀerentiat-
ing factor between competing services, and issues of scal-
able, low-latency video delivery must be addressed.
Finally, following consultations with the Periscope team,
we will make parts of our measurement datasets available to
the research community at http://sandlab.cs.ucsb.edu/periscope/.
Acknowledgments
The authors wish to thank the anonymous reviewers and
our shepherd Fabian Bustamante for their helpful comments.
This project was supported by NSF grants CNS-1527939
and IIS-1321083. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of
the authors and do not necessarily reﬂect the views of any
funding agencies.
9. REFERENCES
[1] Accessing fastly’s ip ranges. https://docs.fastly.com/guides/
securing-communications/accessing-fastlys-ip-ranges.
[2] Adobe rtmp speciﬁcation. http://www.adobe.com/devnet/
rtmp.html.
[3] Apple hls speciﬁcation. https://developer.apple.com/
streaming/.
[4] Fastly. https://www.fastly.com/.
[5] Fastly network map. https://www.fastly.com/network.
[6] Huang, C., Wang, A., Li, J., Ross, K. W. Measuring and
evaluating large-scale CDNs. http://dl.acm.org/citation.cfm?
id=1455517 (2008).
[7] Periscope - live streaming with your gopro. https://gopro.
com/help/articles/Block/Periscope-Live-Streaming-with-
your-GoPro.
[8] Wowza stream engine. https://www.wowza.com/products/
streaming-engine.
[9] Adhikari, V., Guo, Y., Hao, F., Hilt, V.,and Zhang, Z.-L. A
tale of three cdns: An active measurement study of hulu and
its cdns. In INFOCOM Workshops (2012).
[10] Adhikari, V. K., Guo, Y., Hao, F., Varvello, M., Hilt, V.,
Steiner, M.,and Zhang, Z.-L. Unreeling netﬂix:
Understanding and improving multi-cdn movie delivery. In
Proc. of INFOCOM (2012).
[11] Bouzakaria, N., Concolato, C., and Le Feuvre, J. Overhead
and performance of low latency live streaming using
mpeg-dash. In Proc. of IISA (2014).
[12] Castro, M., et al. Scribe: A large-scale and decentralized
application-level multicast infrastructure. IEEE JSAC 20, 8
(2002).
[13] Constine, J. Twitter conﬁrms periscope acquisition, and
here’s how the livestreaming app works. TechCrunch, March
2015.
[14] Cresci, E.,and Halliday, J. How a puddle in newcastle
became a national talking point. The Guardian, January
2016.
[15] Dredge, S. Twitter’s periscope video app has signed up 10m
people in four months. The Guardian, August 2015.
[16] Hamilton, W. A., Garretson, O., and Kerne, A. Streaming
on twitch: fostering participatory communities of play within
live mixed media. In Proc. of CHI (2014), ACM.
[17] Hei, X., Liang, C., Liang, J., Liu, Y., and Ross, K. W. A
measurement study of a large-scale p2p iptv system. IEEE
Transactions on Multimedia 9, 8 (2007).
[18] Huang, T.-Y., Johari, R., McKeown, N., Trunnell, M., and
Watson, M. A buﬀer-based approach to rate adaptation:
Evidence from a large video streaming service. In Proc. of
SIGCOMM (2014).
[19] Jackson, R. How to avoid periscopes broadcast too full
message. Phandroid Blog, August 2015.
[20] Jill, J. ’broadcast is too full’? how to share your periscope
comments. Scope Tips Blog, October 2015.
[21] Kaytoue, M., Silva, A., Cerf, L., Meira Jr, W., and Ra¨issi,
C. Watch me playing, i am a professional: a ﬁrst study on
video game live streaming. In MSND@WWW (2012).
[22] Khan, A. Broadcast too full & you can’t comment? here are
3 ways to get your message out anyway. Personal Blog,
August 2015.
[23] Kontothanassis, L., Sitaraman, R., Wein, J., Hong, D.,
Kleinberg, R., Mancuso, B., Shaw, D., and Stodolsky, D. A
transport layer for live streaming in a content delivery
network. Proc. of the IEEE 92, 9 (2004).
[24] Krishnan, R., Madhyastha, H. V., Srinivasan, S., Jain, S.,
Krishnamurthy, A., Anderson, T.,and Gao, J. Moving
beyond end-to-end path information to optimize CDN
performance. In Proc. of SIGCOMM (2009).
497[25] Kupka, T., Griwodz, C., Halvorsen, P., Johansen, D., and
[39] Siekkinen, M., Masala, E., and K¨am¨ar¨ainen, T. Anatomy of
Hovden, T. Analysis of a real-world http segment streaming
case. In Proc. of EuroITV (2013).
a mobile live streaming service: the case of periscope. In
Proc. of IMC (2016).
[26] Laine, S.,and Hakala, I. H.264 qos and application
[40] Silverston, T.,and Fourmaux, O. Measuring p2p iptv
performance with diﬀerent streaming protocols. In
MobiMedia (2015).
[27] Lederer, S., M¨uller, C., and Timmerer, C. Dynamic
adaptive streaming over http dataset. In Proc. of MMSys
(2012).
[28] Li, Y., Zhang, Y.,and Yuan, R. Measurement and analysis of
a large scale commercial mobile internet tv system. InProc.
of IMC (2011).
[29] Lohmar, T., Einarsson, T., Fr¨ojdh, P., Gabin, F., and
Kampmann, M. Dynamic adaptive http streaming of live
content. In Proc. of WoWMoM (2011).
systems. In Proc. of NOSSDAV (2007).
[41] Small, T., Liang, B., and Li, B. Scaling laws and tradeoﬀs in
peer-to-peer live multimedia streaming. In Proc. of MM
(2006).
[42] Sripanidkulchai, K., Ganjam, A., Maggs, B., and Zhang, H.
The feasibility of supporting large-scale live streaming
applications with dynamic application end-points. In Proc. of
SIGCOMM (2004).
[43] Sripanidkulchai, K., Maggs, B.,and Zhang, H. An analysis
of live streaming workloads on the internet. In Proc. of IMC
(2004).
[30] Madrigal, A. C. The interesting problem with periscope and
[44] Su, A.-J., Choffnes, D. R., Kuzmanovic, A., and
meerkat. Fusion, March 2015.
[31] Magharei, N., and Rejaie, R. Prime: Peer-to-peer
receiver-driven mesh-based streaming. IEEE/ACM TON 17, 4
(2009), 1052–1065.
[32] Mediati, N. Twitter cuts oﬀ meerkat, won’t let it import who
you follow on twitter. PCWorld, March 2015.
[33] M¨uller, C., Lederer, S., and Timmerer, C. An evaluation of
dynamic adaptive streaming over http in vehicular
environments. In Proc. of MoVid (2012).
[34] Nygren, E., Sitaraman, R. K., and Sun, J. The akamai
network: a platform for high-performance internet
applications. SIGOPS OSR 44, 3 (2010).
[35] Perez, S. Live streaming app periscope touts 200 million
broadcasts in its ﬁrst year. TechCrunch, March 2016.
[36] Poblete, B., Garcia, R., Mendoza, M., and Jaimes, A. Do all
birds tweet the same?: characterizing twitter around the
world. In Proc. of CIKM (2011).
[37] Pramuk, J. Periscope ceo: How we’re growing
live-streaming. CNBC, December 2015.
[38] Pullen, J. P. You asked: What is the meerkat app? Time,
March 2015.
Bustamante, F. E. Drafting behind akamai (travelocity-based
detouring). In Proc. of SIGCOMM (2006).
[45] Tang, J. C., Venolia, G., and Inkpen, K. M. Meerkat and
periscope: I stream, you stream, apps stream for live streams.
In Proc. of CHI (2016).
[46] Wilson, C., Boe, B., Sala, A., Puttaswamy, K. P. N., and
Zhao, B. Y. User interactions in social networks and their
implications. In Proc. of EuroSys (2009).
[47] Yin, X., Jindal, A., Sekar, V., and Sinopoli, B. A
control-theoretic approach for dynamic adaptive video
streaming over http. In Proc. of SIGCOMM (2015).
[48] Zhang, C.,and Liu, J. On crowdsourced interactive live
streaming: a twitch. tv-based measurement study. In Proc. of
NOSSDAV (2015).
[49] Zhang, X., Liu, J., Li, B.,and Yum, T.-S. P.
Coolstreaming/donet: a data-driven overlay network for
peer-to-peer live media streaming. In Proc. of INFOCOM
(2005).
[50] Zhao, X., Sala, A., Wilson, C., Wang, X., Gaito, S., Zheng,
H., and Zhao, B. Y. Multi-scale dynamics in a massive online
social network. In Proc. of IMC (2012), pp. 171–184.
498