saturated throughput remains unaffected by different choices
of ∆, whereas the latency deviates little from the theoretical
2∆ line. We do note that the latency remains unaffected only
when the ∆ bound is conservative, because that is when the
time for certifying a block (the O(δ) terms in our theoretical
analysis) is overshadowed by the 2∆ wait. When tolerating a
larger number of faults or when deployed on slower network
conditions (e.g., consortium blockchains), ∆ should be set
appropriately to ensure safety.
D. Scalability and Comparison with Prior Work
We perform an experiment to understand how Sync HotStuff
scales as the number of replicas increases. We also compare
this with HotStuff and Dﬁnity. In our baseline, clients issue
zero-byte payload commands and saturate the system, without
overloading the replicas. We then vary the choice of f. Each
experiment
is repeated ﬁve times with the same setup to
average out ﬂuctuations. A data point shows the average
value, capped by error bars indicating the standard deviation.
Since synchronous protocols tolerate one-half faults as against
one-third in case of partial synchrony, for the same f, the
actual number of replicas is 2f + 1 for Sync HotStuff and
Dﬁnity, whereas it is 3f + 1 for HotStuff. We would also
like to point out that such comparison is not entirely fair
since HotStuff does not assume synchrony. Nevertheless, it is
still helpful to understand the performance of Sync HotStuff
by comparing it to a state-of-the-art partially synchronous
protocol like HotStuff.
Comparison with HotStuff. Figures 10 and 11 show the
throughput and latency for two different payload conﬁgu-
rations, 0/0 and 1024/1024. We use a batch size of 400
for Sync HotStuff and HotStuff. Generally, the throughput
of Sync HotStuff tends to be slightly worse than HotStuff.
But at more faults, the throughput of Sync HotStuff gets
closer to HotStuff and in the 1024/1024 case, eventually
surpasses HotStuff. This is because in both cases the system is
bottlenecked by a leader communicating with all other replicas
and since Sync HotStuff requires fewer replicas to tolerate f
faults, its performance scales better than HotStuff.
Comparison with Dﬁnity. For Dﬁnity, we ﬁrst perform an
experiment to determine good batch sizes that maximize its
throughput. The results are shown in Figure 9. We observe
that Dﬁnity requires a batch size of 14000 to reach its peak
throughput of ∼130 Kops/sec. The reason why Dﬁnity requires
a much larger batch size is because proposals are made much
148163264FaultyReplicas(f)050100150200250300350Throughput(Kops/sec)Sync-HS-p0HotStuff-p0Dfinity-p0148163264FaultyReplicas(f)050100150200250300350400Latency(ms)Sync-HS-p0HotStuff-p0Dfinity-p0148163264FaultyReplicas(f)050100150200Throughput(Kops/sec)Sync-HS-p1024HotStuff-p1024Dfinity-p1024148163264FaultyReplicas(f)050100150200250300350400Latency(ms)Sync-HS-p1024HotStuff-p1024Dfinity-p1024less frequently at every 2∆ time. In contrast, in Sync HotStuff,
a new proposal is made every 2δ (cid:28) 2∆ time, i.e., as soon as
the previous proposal has been “processed” (i.e., certiﬁed).
This allows Sync HotStuff to fully utilize available network
bandwidth with much smaller batch sizes.
Figures 10b and 11b show the latency for two different
payload conﬁgurations, 0/0 and 1024/1024. As can be seen
in the ﬁgures, the latency of Dﬁnity varies between 330ms
and 400ms. This is much higher than that for Sync HotStuff
and is consistent with the expected theoretical average latency
as described in Section V-A. We also observe that at f = 64,
the large batch size we choose for Dﬁnity violates our ∆ =
50ms synchrony bound, leading to safety violations. Hence,
our evaluation does not include that data point.
VI. RELATED WORK
Several decades of research on the Byzantine agreement
problem [14] brought a myriad of solutions. Dolev and Strong
gave a deterministic protocol for its related problem, Byzantine
broadcast, with the tolerance of f  3n/4 votes are received. In this paper,
we adopt the key idea of Thunderella to achieve optimistic
responsiveness. But we also make two changes. First, when
more than 3n/4 replicas are correct, Thunderella commits a
decision after a single round of voting. However, the decision
cannot be conveyed to external clients, hence it does not
support SMR. Sync HotStuff uses two rounds to commit in
the responsive mode, and hence provides safety for SMR.
Second, Thunderella uses the synchronous mode to monitor
the progress of the responsive mode and, if the responsive
mode does not make progress quickly, falls back to the
synchronous mode. The fallback mechanism is presented in
a black-box fashion but
is unclear how it works in a
non-Nakamoto-style protocol. In Sync HotStuff, we take the
conventional approach of having replicas monitor the progress
of the responsive reviews, and using the view-change protocol
to perform the fallback.
XFT. A different type of protocol with optimistic respon-
siveness is XFT [25]. XFT guarantees responsiveness when
a group of f + 1 honest replicas is determined. Thus, when
the actual number of faults is t, it may take (cid:0) n
(cid:1)/(cid:0)n−t
(cid:1)
it
f +1
f +1
view-changes for an honest group of f + 1 to emerge; after
that, the protocol is responsive. Such a solution is practical
when t is a small constant but for t = Θ(n), it requires an
exponential number of view-changes to ﬁnd an honest group.
In comparison, Sync HotStuff and Thunderella are responsive
under t < n/4 faults after at most t view-changes.
VII. CONCLUSION
In this work, we introduce Sync HotStuff, a simple and
practical synchronous BFT SMR protocol. Sync HotStuff
does not require lock-step execution, tolerates mobile sluggish
faults, and offers practical performance. As we mentioned, the
mobile sluggish fault model captures short network glitches
but is not ideal for replicas going ofﬂine for too long. It
remains interesting future work to come up with more realistic
synchronous models as well as practical solutions in them.
Acknowledgment. We thank Atsuki Momose [26] for pointing
out some mistakes in a previous version of this paper and
suggesting a potential ﬁx. We thank Zhuolun Xiang and
Nibesh Shrestha for helpful feedback.
REFERENCES
[22] I. Abraham, S. Devadas, D. Dolev, K. Nayak, and L. Ren, “Synchronous
byzantine agreement with expected O(1) rounds, expected O(n2)
communication, and optimal resilience,” in Financial Cryptography and
Data Security (FC), 2019.
[23] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in
OSDI, vol. 99, 1999, pp. 173–186.
[24] L. Lamport, “Fast paxos,” Distributed Computing, vol. 19, no. 2, pp.
79–103, 2006.
[25] S. Liu, C. Cachin, V. Qu´ema, and M. Vukolic, “XFT: practical fault
tolerance beyond crashes,” in 12th USENIX Symposium on Operating
Systems Design and Implementation. USENIX Association, 2016, pp.
485–500.
[26] A. Momose and J. P. Cruz, “Force-locking attack on sync hotstuff,”
[1] M. Fitzi, “Generalized communication and security models in byzantine
Cryptology ePrint Archive, Report 2019/1484, 2019.
agreement,” Ph.D. dissertation, ETH Zurich, 2002.
[2] C. Dwork, N. Lynch, and L. Stockmeyer, “Consensus in the presence
of partial synchrony,” Journal of the ACM (JACM), vol. 35, no. 2, pp.
288–323, 1988.
[3] T. Hanke, M. Movahedi, and D. Williams, “Dﬁnity technology overview
series, consensus system,” arXiv preprint arXiv:1805.04548, 2018.
[4] T. H. Chan, R. Pass, and E. Shi, “Pili: An extremely simple synchronous
blockchain,” 2018.
[5] R. Pass and E. Shi, “Thunderella: Blockchains with optimistic instant
conﬁrmation,” in Annual International Conference on the Theory and
Applications of Cryptographic Techniques. Springer, 2018, pp. 3–33.
[6] Y. Guo, R. Pass, and E. Shi, “Synchronous, with a chance of partition
tolerance,” 2019.
[7] R. Friedman and R. Van Renesse, “Strong and weak virtual synchrony in
horus,” in Proceedings 15th Symposium on Reliable Distributed Systems.
IEEE, 1996, pp. 140–149.
[8] M. Biely, P. Robinson, and U. Schmid, “Weak synchrony models and
failure detectors for message passing (k-) set agreement,” in Interna-
tional Conference On Principles Of Distributed Systems.
Springer,
2009, pp. 285–299.
[9] Y. Gilad, R. Hemo, S. Micali, G. Vlachos, and N. Zeldovich, “Algorand:
Scaling byzantine agreements for cryptocurrencies,” in Proceedings of
the 26th Symposium on Operating Systems Principles. ACM, 2017,
pp. 51–68.
[10] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” 2008.
[11] V. Buterin and V. Grifﬁth, “Casper the friendly ﬁnality gadget,” arXiv
preprint arXiv:1710.09437, 2017.
[12] M. Yin, D. Malkhi, M. K. Reiter, G. Golan-Gueta, and I. Abraham, “Hot-
Stuff: BFT consensus with linearity and responsiveness,” in Proceedings
of the 2019 ACM Symposium on Principles of Distributed Computing,
PODC 2019, 2019, pp. 347–356.
[13] I. Abraham, D. Malkhi, K. Nayak, and L. Ren, “Dﬁnity consensus,
explored,” Cryptology ePrint Archive, Report 2018/1153, 2018.
[14] L. Lamport, R. Shostak, and M. Pease, “The byzantine generals prob-
lem,” ACM Transactions on Programming Languages and Systems,
vol. 4, no. 3, pp. 382–401, 1982.
[15] D. Dolev and H. R. Strong, “Authenticated algorithms for byzantine
agreement,” SIAM Journal on Computing, vol. 12, no. 4, pp. 656–666,
1983.
[16] M. J. Fischer and N. A. Lynch, “A lower bound for the time to assure
interactive consistency,” Information processing letters, vol. 14, no. 4,
pp. 183–186, 1982.
[17] M. Ben-Or, “Another advantage of free choice (extended abstract):
Completely asynchronous agreement protocols,” in Proceedings of the
second annual ACM symposium on Principles of distributed computing.
ACM, 1983, pp. 27–30.
[18] M. O. Rabin, “Randomized Byzantine generals,” in Proceedings of the
IEEE,
24th Annual Symposium on Foundations of Computer Science.
1983, pp. 403–409.
[19] P. Feldman and S. Micali, “An optimal probabilistic protocol for syn-
chronous byzantine agreement,” SIAM Journal on Computing, vol. 26,
no. 4, pp. 873–933, 1997.
[20] M. Fitzi and J. A. Garay, “Efﬁcient player-optimal protocols for strong
and differential consensus,” in Proceedings of the twenty-second annual
symposium on Principles of distributed computing. ACM, 2003, pp.
211–220.
[21] J. Katz and C.-Y. Koo, “On expected constant-round protocols for byzan-
tine agreement,” Journal of Computer and System Sciences, vol. 75,
no. 2, pp. 91–112, 2009.