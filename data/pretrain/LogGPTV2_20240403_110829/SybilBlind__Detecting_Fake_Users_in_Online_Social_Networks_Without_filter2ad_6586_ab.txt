10% of Twitter users were Sybils [1]. Our method leverages this assumption to
break the symmetry between the benign region and the Sybil region.
234
B. Wang et al.
4 Design of SybilBlind
4.1 Overview
Figure 1 overviews SybilBlind. SybilBlind consists of three components, i.e., sam-
pler, detector, and homophily-entropy aggregator (HEA). Sampler samples two
subsets of nodes from the social network, and constructs a training set by assign-
ing a label of benign to nodes in one subset and a label of Sybil to nodes in the
other subset. The detector takes the sampled noisy training set as an input
and produces a probability of being Sybil for each node. The detector can be
any structure-based Sybil detection method (e.g., SybilSCAR [30] in our exper-
iments) that is relatively robust to label noise in the training set. SybilBlind
repeats this sampling process for multiple trials, and it leverages a homophily-
entropy aggregator to identify the sampling trials in which the detector accu-
rately detects Sybils. Finally, SybilBlind computes an aggregated probability of
being Sybil for every node using the identiﬁed sampling trials.
Fig. 1. Overview of SybilBlind.
Fig. 2. Three scenarios of our sampled
nodes with a sampling size 3.
4.2 Sampler
In each sampling trial, our sampler samples two subsets of nodes from the set of
nodes V , which are denoted as B and S, respectively. Moreover, for simplicity,
we consider the two subsets have the same number of nodes, i.e., n = |B| = |S|,
and we call n the sampling size. We note that it would be a valuable future work
to apply our SybilBlind framework to subsets B and S with diﬀerent sizes.
The subset B (or S) might consist of both benign nodes and Sybils. For
convenience, we denote by nbb and nbs respectively the number of benign nodes
and the number of Sybils in B; and we denote by nsb and nss respectively
the number of benign nodes and the number of Sybils in S. We categorize the
sampled nodes into three scenarios because they have diﬀerent impacts on the
performance of the detector. Figure 2 shows one example of the three scenarios,
where n = 3. The three scenarios are as follows:
SybilBlind: Detecting Fake Users in Online Social Networks
235
• Positively polarized: In this scenario, the number of benign nodes in B is
larger than the number of benign nodes in S, while the number of Sybils in
B is smaller than the number of Sybils in S. Formally, we have nbb > nsb and
nbs  nss.
• Unpolarized: In this scenario, the number of benign (or Sybil) nodes in
B equals the number of benign (or Sybil) nodes in S. Formally, we have
nbb = nsb and nbs = nss.
Note that since the two subsets B and S have the same number of nodes,
we only have the above three scenarios. We construct a training set using the
sampled B and S. Speciﬁcally, we assign a label of benign to nodes in B and a
label of Sybil to nodes in S. Such training set could have label noise. In particular,
in a sampling trial that is positively polarized, a majority of sampled nodes are
assigned labels that match their true labels; while in a sampling trial that is
negatively polarized, a majority of sampled nodes are assigned labels that do
not match their true labels.
4.3 Detector
The detector takes a (noisy) training set as an input and produces a probability
of being Sybil for every node (including the sampled nodes in the training set).
The requirement for the detector is to be relatively robust to label noise in
the training set. In this work, we adopt SybilSCAR [30] as the detector as it
was shown to achieve state-of-the-art accuracy and robustness to label noise.
However, we stress that our framework is extensible to use other structure-based
Sybil detection methods as the detector. In particular, if a better structure-based
Sybil detection method that uses a manually labeled training set is designed in
the future, we can use it as the detector to further improve SybilBlind.
Next, we brieﬂy review SybilSCAR. Given the sampled training set,
SybilSCAR assigns a prior probability qu of being Sybil for every node u. Specif-
ically,
qu =
⎧
0.5 + θ
⎪⎨
0.5 − θ
⎪⎩
0.5
if u ∈ S
if u ∈ B
otherwise,
where 0  0.5, otherwise we predict
u to be benign. Moreover, we denote by s the fraction of nodes in the social
network that are predicted to be Sybils. An edge (u, v) in the social network is
said to be homogeneous if u and v have the same predicted label. Given these
terms, we formally deﬁne homophily h and one-side entropy e as follows:
h =
e =
#edges in total
#homogeneous edges
(cid:2)
0
−slog(s) − (1 − s)log(1 − s)
if s > 0.5
otherwise
(2)
Intuitively, homophily is the fraction of edges that are predicted to be homoge-
neous. One-side entropy is small if too many or too few nodes are predicted to be
Sybils. In our threat model, we consider that the fraction of Sybils in the social
network is less than 50%. Therefore, we deﬁne one-side entropy to be 0 if more
than a half of nodes are predicted to be Sybils. Note the diﬀerence between our
deﬁned one-side entropy and the conventional entropy in information theory.
SybilBlind: Detecting Fake Users in Online Social Networks
237
In a sampling trial that is an unpolarized scenario, we expect the homophily
to be small because SybilSCAR tends to predict labels for nodes randomly. In a
sampling trial that is a negatively polarized scenario, we expect the homophily
to be large because a majority of benign nodes are likely to be predicted to be
Sybils and a majority of Sybils are likely to be predicted to be benign, which
results in a large fraction of homogeneous edges. However, we expect the one-side
entropy to be small because more than a half of nodes would be predicted to be
Sybils. In a sampling trial that is a positively polarized scenario, we expect both
homophily and one-side entropy to be large.
Therefore, our HEA aggregator aims to identify the sampling trials that have
large homophily and one-side entropy. In particular, we ﬁrst identify the top-κ
sampling trials among the k sampling trials that have the largest homophily.
Then, among the top-κ sampling trials, we choose the sampling trial with the
largest one-side entropy and use the probability obtained in this sampling trial
as the aggregated probability. Essentially, among the top-κ sampling trials, we
identify the sampling trial with the largest s that is no larger than 0.5, i.e., we
aim to use the sampling trial that detects the most Sybils. Note that we can
also reverse the order by ﬁrst identifying the top-κ sampling trials that have
the largest one-side entropies and choose the sampling trial with the largest
homophily. However, we ﬁnd the performance is almost the same and we thus
use the former way by default.
5 Theoretical Analysis
5.1 Sampling Size and Number of Sampling Trials
The sampler constructs a training set via assigning a label of benign to nodes in
B and a label of Sybil to nodes in S. We deﬁne label noise in the benign region
(denoted as αb) as the fraction of sampled nodes in the benign region whose
assigned labels are Sybil. Similarly, we deﬁne label noise in the Sybil region
(denoted as αs) as the fraction of sampled nodes in the Sybil region whose
assigned labels are benign. Formally, we have αb = nsb
nbs+nss ,
where nbb and nbs respectively are the number of benign nodes and Sybils in B;
nsb and nss respectively are the number of benign nodes and Sybils in S.
nsb+nbb and αs = nbs
We can derive an analytical form for the probability that label noise in both
the benign region and the Sybil region are smaller than a threshold τ in a
sampling trial. Due to limited space, we omit the analytical form. However,
the analytical form is too complex to illustrate the relationships between the
sampling size and the number of sampling trials. Therefore, we show the following
theorem, which bounds the probability.
Theorem 1. In a sampling trial with a sampling size of n, the probability that
label noise in both the benign region and the Sybil region are no bigger than τ
(τ ≤ 0.5) is bounded as
(1 − r)
nrn ≤ Pr(αb ≤ τ, αs ≤ τ ) ≤ exp
(cid:7) − 2(1 − 2τ )2(1 − r)2n
τ 2 + (1 − τ )2
(cid:8)
,
(3)
where r is the fraction of Sybils in the social network.
238
B. Wang et al.
Proof. See Appendix B.
Implications of Theorem 1: Suppose in a social network, SybilSCAR is robust
to label noise upto τ, i.e., its performance almost does not degrade when the
noise level is τ, then SybilBlind requires at least one sampling trial, in which
the label noise is less than or equal to τ, to detect Sybils accurately. We have
several qualitative implications from Theorem 1. We note that these implications
also hold when using the analytical form of the probability that label noise are
smaller than τ. Here, we choose Theorem 1 because of its conciseness.
2(1−2τ )
2
and kmax =
(1−r)
τ 2+(1−τ )2
First, when the sampling size is n and SybilSCAR is robust to label noise
up to τ in the social network, the expected number of sampling trials (i.e.,
k) that SybilBlind requires is bounded as kmin ≤ k ≤ kmax, where kmin =
(1−r)nrn . Note that kmin is exponential with
exp
respect to n, which could be very large even if n is moderate. However, through
empirical evaluations, we found k can be largely reduced and a moderate k could
make SybilBlind obtain satisfying performance. Second, when τ gets bigger, kmin
gets smaller, which implies that SybilBlind tends to require less sampling trials
when detecting Sybils in a social network in which SybilSCAR can tolerate larger
label noise. Third, we observe a scale-free property, i.e., the number of sampling
trials is not related to the size (i.e., |V | or |E|) of the social network.
(cid:3)
(cid:4)
2n
1
5.2 Complexity Analysis
Space and Time Complexity: The major space cost of SybilBlind consists of
storing the social network and storing the top-κ vectors of posterior probabilities.
SybilBlind uses an adjacency list to represent the social network, with the space
complexity O(2|E|), and stores the top-κ vectors of posterior probabilities of all
nodes. Therefore, the space complexity of SybilBlind is O(2|E| + κ|V |).
In each trial and in each iteration, SybilBlind applies a local rule to every
node, and the time complexity of the local rule to a node u with |Γu| friends is
O(|Γu|). Therefore, the time complexity of SybilBlind in one iteration is O(|E|).
Since SybilBlind performs k sampling trials and each trial runs T iterations, it
thus has a time complexity of O(kT|E|).
Two-level Parallel Implementation: We can have a two-level parallel imple-
mentation of SybilBlind on a data center which is a standard backend for social
web services. First, diﬀerent sampling trials can be run on diﬀerent machines.
They only need to communicate once to share their vectors of posterior proba-
bilities. Second, each machine can parallelize SybilSCAR using multithreading.
Speciﬁcally, in each iteration of SybilSCAR, each thread applies the local rule
to a subset of nodes in the social network.
SybilBlind: Detecting Fake Users in Online Social Networks
239
6 Experiments
6.1 Experimental Setup
Datasets: We use social networks with synthesized Sybils and Twitter datasets
with real Sybils for evaluations. Table 1 summarizes the datasets.
(1) Social networks with synthesized Sybils. Following previous works
[7,8,38], we use a real-world social network as the benign region, while synthesiz-
ing the Sybil region and attack edges. Speciﬁcally, we take a Facebook network as
the benign region; we synthesize the Sybil region using the Preferential Attach-
ment (PA) model [3], which is a widely used method to generate networks; and
we add attack edges between the benign region and the Sybil region uniformly
at random. In this graph, nodes are Facebook users and two nodes are connected
if they are friends. We synthesize the Sybil region such that 20% of users in the