# Crawling and Manual Browsing of CSP-Enabled Sites

To compare the policies generated by our crawler to real-world policies, we analyzed large public websites that have implemented Content Security Policy (CSP) in enforcement mode. As case studies, we provide detailed insights into Facebook and GitHub.

## Case Study: Facebook
Our crawl covered both the public and authenticated portions of Facebook. The policy generated by the crawler was a subset of Facebook’s actual policy. The crawler listed specific subdomains of Content Distribution Networks (CDNs) observed during the crawl, while Facebook used a wildcard to whitelist all CDN subdomains. Additionally, while Facebook's policy restricted only `script-src` and `connect-src`, the crawler also generated entries for `img-src`. These discrepancies could block legitimate but unobserved behavior, highlighting the need for fine-tuning using domain knowledge before deploying automatically generated policies.

## Case Study: GitHub
On GitHub, the crawler discovered all whitelisted resources of the original policy, which did not use any wildcards and restricted only `script-src`, `style-src`, and `object-src`. The crawler generated additional entries not present in GitHub’s policy. Manual verification revealed that some resources on GitHub’s blog were not loaded due to missing policy entries. This finding underscores the importance of monitoring enforced policies as websites evolve, and regular crawls can be a useful tool for detecting such changes.

## Influence of Design Choices on CSP
The architectural features of a site can significantly influence the deployment of a meaningful CSP without major changes. For example, our crawls of Twitter found a small, stable set of policy entries, with most resources being internal. Multimedia content in tweets was loaded from internal subdomains with constant names, making it relatively convenient to deploy CSP. Twitter indeed uses CSP in some subdirectories and subdomains.

Other sites like Amazon, Google, and YouTube dynamically use explicitly named subdomains of CDNs, similar to Facebook. These subdomains are often used for load balancing and can be considered equivalent from a security perspective. Our crawler could not enumerate all these subdomains, but post-processing with wildcards (e.g., `*.google.com`) could address this issue. However, this approach would also whitelist other customers' subdomains if external CDNs are used. A cleaner approach, as seen in Twitter, is to use static domain names at the web application layer and handle load balancing transparently at lower layers.

## Stability of Policies
A key requirement for deploying an enforceable CSP is to predict the external resources that will be included when a page is rendered. Advertising is particularly unpredictable, as the exact advertisement shown to a user is typically determined dynamically. Techniques like Real-Time Bidding (RTB) involve numerous potential bidders, each representing a large number of advertisers.

To understand how dynamic advertising can be reconciled with CSP, we performed repeated crawls of two large websites with dynamic advertisements and counted new policy entries in each subsequent crawl (Table 8). Twitter, used as a control data point, remained stable, resulting in the same policy in all crawls. On the BBC, the crawler discovered between 13 and 61 new policy entries in each follow-up crawl, primarily related to advertising. On CNN, the follow-up crawls discovered only one to four new policy entries, with only one clearly related to advertising. The differences are due to the implementation of advertising: the BBC loads all advertisement-related resources directly into the main document, making CSP deployment challenging, while CNN isolates advertisements in embedded frames, allowing for a more stable and enforceable policy.

## Safety of Policies
To assess the effectiveness of generated policies, we checked for “unsafe” CSP features, such as inline script or style and calls to `eval`. Among our sites, only site B did not require `eval` privileges. Most sites, including Amazon, the BBC, CNN, Facebook, Google, the Huffington Post, and YouTube, required all three privileges. Twitter needed inline script and style, and GitHub only inline style. Allowing inline script and `eval` reduces the effectiveness of CSP against XSS attacks, but restricting external resource loading can still enhance security.

## Conclusions
Neither naive crawling nor manual browsing alone are sufficient for generating a CSP. Fine-tuning is necessary for all but the simplest sites. Advanced crawling or machine learning could reduce the need for manual adjustments. More complex sites may need to adjust their architecture to fully leverage CSP. Ensuring that policies remain up-to-date after deployment is another challenge.

## Discussion
We found that few websites use CSP, and those that do often do not fully leverage its benefits. We reached out to security engineers behind larger CSP deployments and summarize key points, suggesting several ways to improve CSP adoption.

### Discussions with Security Engineers
We talked to security engineers responsible for three measured websites, two in the Alexa Top 200 and one in the Top 5,000. Key observations include:

- **Inline Script Removal**: While possible, it requires significant effort and can lead to more roundtrips. Engineers hope to use nonce and hash features in CSP draft version 1.1.
- **Risk of Breaking Functionality**: Some sites disable CSP for browser versions with problematic implementations to avoid usability issues.
- **Enforcement over Extensions**: CSP enforcement can break browser extensions, and whitelisting popular sources is a common workaround.

### Suggested Improvements
- **Ads in iframes**: Moving ads into sandboxed iframes allows the main site to be protected with a strict policy while the iframe can be more permissive.
- **CSP in Web Applications and Frameworks**: Introducing CSP to widely deployed programs can significantly enhance web security. Examples include phpMyAdmin, OwnCloud, and Django.
- **Browser Enforcement on Extensions**: Browsers should not enforce CSP on extensions, as this generates unexpected reports and forces websites to whitelist unbounded third-party resources.

## Related Work
CSP was proposed by Stamm et al. [19] and has since become a W3C standard. Other publications have addressed limitations and suggested extensions. For instance, Soel et al. [18] proposed an extension to address shortcomings in `postMessage` origin handling. CSP was the first widely deployed browser policy framework to mitigate content injection attacks, though others like SOMA, BEEP, BLUEPRINT, and CONSCRIPT have been proposed.

## Conclusion
CSP holds great promise as a web security standard, but it is difficult for most sites to deploy it to its full potential. The improvements we suggest, along with upcoming features in the 1.1 draft, aim to make CSP more effective and result in a safer web ecosystem.

## Acknowledgements
This work was supported by the Office of Naval Research (ONR) under grant N00014-12-1-0165. We thank Anil Somayaji and the anonymous reviewers for their comments, and Collin Mulliner and Clemens Kolbitsch for their help in data collection.

## References
[References listed here as provided in the original text]

---

This revised version aims to be more coherent, clear, and professional, with a structured format and improved readability.