2 (0)
1 (0)
0 (0)
crawler only
both
valid user reports only 0 (0)
0 (0)
3 (3) 11 (8) 12 (10)
26 (3)
0 (0)
9 (9)
3 (1)
D
0 (0)
8 (7)
9 (2)
0 (0)
8 (7)
14 (2)
manual only
both
valid user reports only 0 (0)
0 (0)
2 (0)
3 (3) 11 (7) 13 (10) 15 (9)
7 (0)
25 (3)
3 (2)
0 (0)
3 (2)
Crawling and Manual Browsing of CSP-enabled Sites. In order to com-
pare our crawler-generated policies to real-world policies, we generated policies
for large public websites that deployed CSP in enforcement mode. As a case
study, we provide more detail for Facebook and GitHub.
Our crawl included the public portion of Facebook as well as authenticated
sessions. The policy generated by the crawler was a subset of Facebook’s ac-
tual policy. It listed the speciﬁc subdomains of Content Distribution Networks
(CDNs) observed during the crawl, whereas Facebook whitelisted all CDN sub-
domains with a wildcard. Furthermore, while Facebook’s policy restricted only
script-src and connect-src, the crawler also generated entries for img-src,
for instance. Both issues could cause unobserved (but legitimate) behavior to be
blocked and illustrate that automatically generated policies are likely to require
ﬁne-tuning using domain knowledge before they can be deployed.
On GitHub, the crawler discovered all whitelisted resources of the original pol-
icy (which did not use any wildcards, and restricted only script-src, style-src,
and object-src). The crawler generated additional entries that were not part of
GitHub’s policy. Upon manual veriﬁcation, we found that some resources included
in GitHub’s blog were not loaded due to missing policy entries. This ﬁnding illus-
trates the importance of monitoring enforced policies when websites evolve; regu-
lar crawls of a website could be a useful tool to help detect such changes.
Inﬂuence of Design Choices on CSP. Architectural features of a site can
inﬂuence whether it is possible to deploy a meaningful policy without changing
the site. Our crawls of Twitter, for instance, found a small, stable set of policy
entries, while additional manual browsing discovered only one additional policy
entry. Most of the resources were internal. Multimedia content included in tweets,
for instance, was loaded from internal subdomains with constant names. Such an
architecture makes it relatively convenient to deploy CSP without major changes.
Indeed, Twitter used CSP in some subdirectories and subdomains.
228
M. Weissbacher, T. Lauinger, and W. Robertson
Other sites such as Amazon, Google, and YouTube dynamically used explic-
itly named subdomains of CDNs such as mt{2,3}.google.com, similarly to Face-
book. These subdomains appeared to be used for load balancing and could there-
fore be considered equivalent from a security point of view. Our crawler was not
able to enumerate all these subdomains, but post-processing of the policy such
as using a wildcard *.google.com could address the issue. A drawback of this
approach is that sites such as Amazon that use external CDNs would also be
whitelisting other customers’ subdomains. A cleaner approach would be to use
static domain names at the web application layer and address load balancing
transparently at lower layers, as appears to be done by Twitter.
In the examples above, it was possible to compensate for some degree of
variability in the sites by broadening the generated policy because the variability
was systematic. On certain types of sites such as blogs where users are allowed to
include externally hosted content, this may not be possible. The policy used by
GitHub shows a possible compromise in such situations: the site allowed images
to be loaded from any source and restricted only more sensitive resource types
such as scripts and plugins.
Stability of Policies. A requirement to successfully deploy an enforceable
policy is to predict at policy generation time the external resources that will be
included when a page is rendered in a browser. A particularly unpredictable type
of external content is advertising. The exact advertisement shown to a user is
typically determined dynamically while the page is loading. Dynamic advertising
can involve techniques such as Real-Time Bidding (RTB), where the opportunity
to display an advertisement to a visitor is auctioned oﬀ in real-time, and further
dynamic activity such as cookie matching between the host website and the
winner of the auction. There are routinely tens to hundreds of potential bidders
in RTB [16], each of whom represent a large number of actual advertisers.
In order to better understand how this dynamic activity can be reconciled with
the more static requirements of CSP, we performed repeated crawls of two large
websites with dynamic advertisements and counted how many new policy entries
we discovered in each subsequent crawl (Table 8). Twitter, which we crawled as
a control data point, remained stable and resulted in exactly the same policy in
all crawls. On the BBC, the crawler discovered between 13 and 61 new policy
entries in each of the follow-up crawls; the vast majority of them were scripts or
other content related to advertising. On CNN, the follow-up crawls discovered
only between one and four new policy entries, and only one was unambiguously
related to advertising. Since both sites displayed comparable types and amounts
of advertisements, the diﬀerences must be due to the way advertising was imple-
mented. Indeed, the BBC loaded all advertisement-related resources, including
RTB scripts, tracking code, and the ﬁnal image being displayed, directly into
the body of the main document. It would be very challenging to deploy CSP in
such a scenario because it seems unfeasible to proactively determine any resource
that could potentially be loaded. In contrast, CNN isolated advertisements from
the main document by loading them as a separate document displayed inside an
embedded frame.
Why Is CSP Failing? Trends and Challenges in CSP Adoption
229
Table 8. Additional policy entries discovered in repeated crawls. The high variability
due to advertising on the BBC precludes CSP from being used eﬀectively. CNN’s way
of including advertisement results in a relatively stable (and enforceable) policy.
Crawl number
1
2
3
4
5
BBC
CNN
Twitter
285 +34 +61 +13 +53
116 +4 +2 + 1 +1
20 +0 +0
This decoupling signiﬁcantly eases the deployment of CSP because the main
document’s policy does not transitively apply to the document inside the frame.
In such a deployment, it would be possible to enforce a rather strict policy
for the main document and a much more permissive policy for the embedded
advertisement document (or none at all). The SOP as well as the HTML5 frame
sandboxing mechanism can be used to ensure that untrustworthy scripts in the
frame cannot access or modify the main document.
Safety of Policies. To assess whether policies generated for a site represent
any signiﬁcant reduction in exposure to attacks, we checked whether the policies
included “unsafe” CSP features—that is, inline script or style and calls to eval.
Among our own sites that included JavaScript, only site B did not require eval
privileges. Amazon, the BBC, CNN, Facebook, Google, the Huﬃngton Post, and
YouTube required all three privileges; Twitter needed inline script and style, and
GitHub only inline style. These requirements may be due to code on the sites
or in external libraries they include. Even though allowing inline script and
eval reduces the eﬀectiveness of CSP against XSS attacks, by restricting where
external resources may be loaded from, CSP could still make it more diﬃcult for
attackers to include custom content such as images or to exﬁltrate stolen data.
5.3 Conclusions
Neither na¨ıve crawling nor manual browsing alone are suﬃcient methods to gen-
erate a content security policy for a website. In our approach, a certain amount
of ﬁne-tuning of generated policies is required for all but the simplest sites. Ad-
vanced crawling, or applying machine learning to the generated policies, could
reduce the importance of manual tweaks. More complex sites may be able to use
only a subset of CSP unless they adjust their architecture. Once a policy has
been deployed, an additional challenge is to ensure that it is always up to date.
6 Discussion
We saw that only few websites use CSP, and those that do use it do not leverage
its full beneﬁts. For this section, we reached out to security engineers behind
larger CSP deployments and summarize key points. Furthermore, we suggest
several ways in which CSP adoption could be improved.
230
M. Weissbacher, T. Lauinger, and W. Robertson
6.1 Discussions with Security Engineers
To understand implementation decisions behind real-world CSP deployments,
we talked to security engineers responsible for three of the measured websites.
Out of these sites, two were in the Alexa Top 200, and one in the Top 5,000. The
websites used CSP in enforcement mode or report-only for testing. We summarize
the key observations in an anonymized fashion.
Websites Prefer Not to Remove Inline Script. While inline script can
be completely removed from websites, this represents signiﬁcant eﬀort and can
lead to more roundtrips when loading the page. Engineers hope to address this
issue with the nonce and hash features of CSP draft version 1.1. Hash might be
more promising because documents can be distributed over CDNs more easily,
whereas for nonce a new document would need to be generated for each response.
Risk of Breaking Functionality. This was manifested by disabling CSP for
browser versions with problematic CSP implementations, including Chrome and
Firefox. A website that is secure but not usable can harm business more than
occasional XSS. For the future, reliable implementations of CSP in browsers are
anticipated.
Enforcement over Extensions is Considered a Bug. CSP rule enforce-
ment can break the functionality of browser extensions. A workaround is to
whitelist popular sources. However, extensions could still be unintentionally re-
stricted. A modiﬁcation of browser implementations or the standard to not en-
force rules over extensions could solve this.
6.2 Suggested Improvements
We brieﬂy summarize approaches that could help the adoption of CSP and in-
crease its security beneﬁts when deployed.
Ads should be Integrated into iframes instead of the Main Site.
Instead of whitelisting all possible ad networks or developing a mechanism for
recursive policy adoption, ads should be moved into sandboxed iframes. This
allows the main site to be protected with an eﬀective policy, while the iframe can
be more permissive, but isolated. Conﬂating both the site proper and ads in the
same context is not necessary, since information required by ads can be passed via
postMessage cross-window communication. However, while not widely available,
alternatives such as Security Style Sheets [14] have been proposed that would
allow for such separation without moving content to iframes.
More Web Applications and Frameworks should Adopt CSP. Intro-
ducing CSP to programs that are deployed widely can have a higher impact on
the overall security of the web as compared to individual websites adopting CSP.
As examples, phpMyAdmin and OwnCloud have adopted CSP, and Django can
be conﬁgured with CSP. Most desirable would be the introduction of CSP to web
frameworks, which could drastically improve adoption of CSP and the safety of
the web.
Browsers Should not Enforce CSP on Extensions. As discussed in
Section 4, enforcing policies on browser extensions generates many unexpected
reports for websites. Websites should not be forced to whitelist extensions since
Why Is CSP Failing? Trends and Challenges in CSP Adoption
231
the number of extensions and third-party resources included by those extensions
is theoretically unbounded and cannot be predicted by application developers.
Furthermore, CSP in its current form is not an adequate mechanism for websites
to block potentially undesired extensions and should not be used as such.
7 Related Work
CSP was proposed by Stamm et al. [19], who provided the ﬁrst implementa-
tion in the Firefox browser. Subsequently, CSP became a W3C standard [6] and
was adopted by most major browsers. Other publications have addressed limi-
tations of CSP and suggested extensions or modiﬁcations to the standard. For
instance, Soel et al. [18] proposed an extension of CSP to address shortcomings
in postMessage origin handling.
CSP was the ﬁrst widely deployed browser policy framework to mitigate con-
tent injection attacks. However, it was not the ﬁrst one to be suggested. SOMA
(Same Origin Mutual Approval) [15] reduces the impact of XSS and CSRF by con-
trolling information ﬂows. Website operators need to approve content sources in
a manifest ﬁle, as well as content providers need to approve websites to include
their content. BEEP [11] can prevent XSS attacks with a whitelist approach
for JavaScript and a DOM sandbox for possibly malicious user content. Script
tags are whitelisted by hash, a feature that is also proposed in the 1.1 draft
of CSP. BLUEPRINT [20] enforces restrictions on the document parse tree
in the browser. Web application server components make parsing decisions and
transport the DOM structure to the client. By enforcing a consistent document
structure, misuse of browser rendering quirks is eliminated. CONSCRIPT [12]
supports a variety of policies for JavaScript enforcement, which can be generated
automatically. Static policy generation is supported for Script#, a Microsoft tool
that generates JavaScript from C# code, as well as a dynamic training mode for
other platforms. Weinberger et al. [21] performed an evaluation of browser-side
policy enforcement systems. They concluded that security policies for HTML
should be a central mechanism for preventing content injection attacks, but
need more research to become eﬀective. We performed the ﬁrst study on CSP
adoption in the wild, analyzing how usage has evolved in the past year on the
most popular websites. Also, we investigate how report-only mode can be used
to devise policies, and whether those are eﬀective.
Currently, inline scripts are as popular with websites as they are bad for the
eﬀectiveness of CSP to prevent XSS. Bugzilla and HotCRP required substantial
changes to support CSP [21], while addons.mozilla.org required an eﬀort of
several hours [19]. Previous work performed automatic rewriting of .NET ap-
plications to better support CSP [10]. Recent changes to the CSP draft, such
as nonce and hash whitelisting of scripts, represent an approach that relieves
developers of removing inline scripts while allowing for control over code. Trust
relationships in external script sources have been analyzed by Nikiforakis et
al. [13]. 88% of the Alexa Top 10K most visited websites included scripts from
remote sources, and the most popular single library was included from 68% of
the sites. An outlook on the possible future of web vulnerabilities has been sum-
marized by Zalewski [8]. While CSP addresses a wide range of vulnerabilities,
232
M. Weissbacher, T. Lauinger, and W. Robertson
it can not prevent out-of-order execution of scripts, code reuse through JSONP
interfaces, and others.
8 Conclusion
In this paper, we have presented the results of a long-term study on CSP as it is
deployed on the web. We have found that CSP adoption signiﬁcantly lags other
web security mechanisms, and that even when it has been adopted by a site, it
is often deployed in a way that negates its theoretical beneﬁts for preventing
content injection and data exﬁltration attacks.
In addition, by enabling CSP at four sites, we observed that it is diﬃcult for
third parties to deploy CSP, either through incremental deployment using report-
only mode or through web application crawling to semi-automatically generate
policies.
CSP clearly holds great promise as a web security standard, but we can only
conclude that it is diﬃcult for most sites to deploy it to its full potential in its
current form. It is our hope that the improvements we suggest here, as well as up-
coming features of the 1.1 draft, will allow site operators and developers to make
eﬀective use of content security policies and result in a safer web ecosystem.
Acknowledgements. This work was supported by the Oﬃce of Naval Research
(ONR) under grant N00014-12-1-0165. We would like to thank our shepherd Anil
Somayaji and the anonymous reviewers for their helpful comments. Furthermore,
we thank Collin Mulliner and Clemens Kolbitsch for their help in data collection.
References
1. DNS Prefetching
- The Chromium Projects,
http://www.chromium.org/
developers/design-documents/dns-prefetching
2. The Platform for Privacy Preferences 1.0 (P3P1.0) Speciﬁcation (2002), http://
www.w3.org/TR/P3P/
3. IE8 Security Part IV: The XSS Filter (2008), http://blogs.msdn.com/b/ie/
archive/2008/07/02/ie8-security-part-iv-the-xss-filter.aspx
4. IE8 Security Part V: Comprehensive Protection (2008), http://blogs.msdn.com/
b/ie/archive/2008/07/02/ie8-security-part-v-comprehensive-protection.
aspx
5. RFC 6797 - HTTP Strict Transport Security, HSTS (2012), http://tools.ietf.
org/html/rfc6797
6. Content Security Policy 1.1 (2013),
https://dvcs.w3.org/hg/content-security-policy/raw-file/tip/
csp-specification.dev.html
7. Cross-Origin Resource Sharing, W3C Candidate Recommendation (January 29,
2013), http://www.w3.org/TR/cors/
from the post-XSS world (2013), http://lcamtuf.coredump.cx/
8. Postcards
postxss/
org/html/rfc7034
9. RFC 7034 - HTTP Header Field X-Frame-Options (2013), http://tools.ietf.
Why Is CSP Failing? Trends and Challenges in CSP Adoption
233
10. Doup´e, A., Cui, W., Jakubowski, M.H., Peinado, M., Kruegel, C., Vigna, G.:
deDacota: Toward Preventing Server-Side XSS via Automatic Code and Data Sep-
aration. In: ACM Conference on Computer and Communications Security, CCS
(2013)
11. Jim, T., Swamy, N., Hicks, M.: Defeating Script Injection Attacks with Browser-
Enforced Embedded Policies. In: International Conference on World Wide Web,
WWW (2007)
12. Meyerovich, L.A., Livshits, B.: ConScript: Specifying and enforcing ﬁne-grained
security policies for Javascript in the browser. In: IEEE Symposium on Security
and Privacy, Oakland (2010)
13. Nikiforakis, N., Invernizzi, L., Kapravelos, A., Van Acker, S., Joosen, W., Kruegel,
C., Piessens, F., Vigna, G.: You Are What You Include: Large-scale Evaluation of
Remote JavaScript Inclusions. In: ACM Conference on Computer and Communi-
cations Security, CCS (2012)
14. Oda, T., Somayaji, A.: Enhancing Web Page Security with Security Style Sheets.
Carleton University (2011)
15. Oda, T., Wurster, G., van Oorschot, P.C., Somayaji, A.: SOMA: Mutual Approval
for Included Content in Web Pages. In: ACM Conference on Computer and Com-
munications Security, CCS (2008)
16. Olejnik, L., Tran, M.D., Castelluccia, C.: Selling Oﬀ Privacy at Auction. In: ISOC
Network and Distributed System Security Symposium (NDSS) (2014)
17. Samuel, M., Saxena, P., Song, D.: Context-Sensitive Auto-Sanitization in Web
Templating Languages Using Type Qualiﬁers. In: ACM Conference on Computer
and Communications Security, CCS (2011)
18. Son, S., Shmatikov, V.: The Postman Always Rings Twice: Attacking and Defend-
ing postMessage in HTML5 Websites. In: ISOC Network and Distributed System
Security Symposium, NDSS (2013)
19. Stamm, S., Sterne, B., Markham, G.: Reining in the Web with Content Security
Policy. In: International Conference on World Wide Web, WWW (2010)
20. Ter Louw, M., Venkatakrishnan, V.: BLUEPRINT: Robust Prevention of Cross-
site Scripting Attacks for Existing Browsers. In: IEEE Symposium on Security and
Privacy, Oakland (2009)
21. Weinberger, J., Barth, A., Song, D.: Towards Client-side HTML Security Policies.
In: Workshop on Hot Topics on Security, HotSec (2011)