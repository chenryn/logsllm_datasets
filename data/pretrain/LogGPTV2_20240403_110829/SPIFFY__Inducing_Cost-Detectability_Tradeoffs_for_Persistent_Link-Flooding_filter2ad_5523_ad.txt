Figure 6 shows a rate measurements of 100 randomly
selected senders. Before TBE starts at t = 10 seconds, all
senders achieve approximately 100 Kbps with small standard
devidations; however, after TBE starts, most senders achieve
10 times higher rates within 2 seconds. This result shows that
the rate change detection is robust for the legitimate senders
with realistic ﬂows, in particular with large portion of short-
lived ﬂows.
The reason for the negligible effect of short-lived TCP
ﬂows on the effectiveness of rate-change detection is that a few
long-lived ﬂows from senders increase their rate signiﬁcantly
once TBE is performed and thus induce the overall per-sender
rate change. Figure 7 shows the simulated ﬂow rates versus
ﬂow sizes. Notice that before TBE only short-ﬂows (i.e., small
ﬂow size) are observed. They achieve low rates and long-lived
ﬂows are not even able to complete their TCP connections.
8
Fig. 8: Whisker plots representing the rate-change ratios
for varying RTT/application-layer data rates when the
bandwidth expansion factor M = 10.
This is because short-lived ﬂows spend most of their life
in the TCP slow start and thus they can rapidly capture a
greater proportion of resources than long-lived ﬂows in TCP
congestion avoidance, often driving the long-lived ﬂows into
timeouts. After TBE starts, long-lived ﬂows achieve much
higher rates whereas short-lived ﬂows achieve only slightly
higher rates than before. This is because long-lived ﬂows now
have enough time to increase the congestion windows.
Next, we evaluate the false-positive rate of the SPIFFY’s
bot detection with realistic trafﬁc and propose a mechanism to
maintain low false-positive rate. To simulate various types of
realistic legitimate senders in different locations with different
trafﬁc rates, we vary the end-to-end propagation delays (in
msec) and the application-layer data rate (in Kbps) per sender.
Figure 8 shows the measured rate-change ratio (RC) when
the ideal round-trip time (RTT) (i.e., RTT measured when no
trafﬁc on the path) or the application-layer data rate (i.e.,
average HTTP data rate) vary. Figure 8a shows that
the
vast majority of measured rate-change ratios are close to the
bandwidth expansion factor M = 10 and largely independent
of the ideal RTT of the ﬂows. This suggests that bot detection
can achieve low false-positive ratio when it uses a rate-change
ratio threshold RCth close to M to identify senders with
RC < RCth as bots. However, as shown in Figure 8b, the
rate-change ratio RC is heavily affected by the application-
layer data rate. While senders with high application-layer data
rates show rate-change ratios very close to the bandwidth
expansion factor M = 10, senders with low rates result in
rate-change ratios that are spread over a large range. This
would potentially induce non-negligible false-positive ratios
when bots are identiﬁed by thresholding the rate-change ratios.
From this observation, we set the minimum per-sender rate
(ratemin) and exempt the senders with per-sender rate lower
than ratemin from bot detection. In other words, senders with
per-sender rate lower than ratemin are not tested by the target
network regardless of their rate change ratios. By exempting
these low-rate senders from the bot detection, we can also
protect the legitimate, inherently low-rate senders from being
misidentiﬁed as bots; e.g., legitimate users with slow legacy
cellular connections or casual web surﬁng users with light
activity are protected by this exemption.
Figure 9 shows the false-positive rate for varying rate-
Time (sec)051015Per-sender rate (Kbps)050010001500TBE starts at 10.0 secmean and stdevFlow size (Byte)102103104105106103104105106107(b) During TBEscatteravg rateFlow size (Byte)102103104105106Per-flow rates (bps)103104105106107(a) Before TBEscatteravg rate0  20 40 60 80 1001201401601802000510152025RC (rate-change ratio)(a)0   200 400 600 800 1000120014001600180020000510152025(b)RTT (msec)app-layer data rate           (Kbps)Fig. 9: False-positive rate for varying rate-change ra-
tio thresholds (RCth) and minimum per-sender rates
(ratemin).
change ratios RCth and for several values minimum per-sender
rate ratemin. We ﬁrst observe that the larger threshold ratio
RCth, the higher false-positive rate is expected because small
rate ﬂuctuations can cause false positive when the threshold
ratio RCth is high. We also notice that as we exclude more
low-rate senders (i.e., set higher minimum per-sender rate
ratemin), we can reduce the false-positive rate. As shown
in Figure 9, with proper parameters we can easily maintain
very low false-positive rate; e.g., 1% or less. Note that the
exemption of low-rate senders could contribute to some false-
negative errors; i.e., indicating bots as legitimate. However,
the inﬂuence of the non-detected bots is limited since they do
not send at the rate higher than the minimum per-sender rate
ratemin, which is the chosen small rate value.
Note also that adversaries cannot exploit the exemption
of low-rate senders. An adversary might conﬁgure her bots
to send at a rate lower than the minimum per-sender rate
ratemin to avoid detection, but this only increases the attack
cost signiﬁcantly because more bots are needed to create the
same amount of attack trafﬁc to congest the target link.
Robustness to sudden RTT increase: Our TBE mech-
anism reroutes trafﬁc around the target link. Rerouting may
ﬁnd a new route longer than the initial one. According to our
experiments (Section VII-B), TBE increases the route length
(i.e., number of routers in a route) on average by up to 24%.
This raises the question of whether this suddenly increased
RTT adversely impacts the false-positive rates of SPIFFY.
We list two possible cases where sudden RTT increase might
cause false-positive events: (1) Some delay-based TCP variants
(e.g., Compound TCP [52] and TCP Vegas [16]) use RTT
measurements at receivers to adjust TCP congestion windows.
These TCP variants consider RTT increase as the sign of
congestion and reduces their sending rates; (2) TCP senders
might experience spurious timeouts and drop sending rates
signiﬁcantly. A spurious timeout occurs when RTT suddenly
increases and exceeds the retransmission timer that had been
determined a priori [39].
Here, we claim that such rate decrease due to RTT increase
is not likely to happen becuase RTT will actually be reduced
signiﬁcantly when TBE is performed. The rationale behind this
is that TBE removes high queueing delay at the (almost) full
buffer of the target link. The RTT reduction due to this con-
gestion relief is in general much larger than the RTT increase
due to TBE rerouting, ultimately causing RTT reduction.
To support our claim, we measure RTT changes when TBE
Fig. 10: RTT and congestion window changes when TBE
is performed.
is performed in a simulation. We set the ideal (i.e., when no
congestion on a path) RTT to 100 msec and assume 25%
increase of the RTT when rerouting takes place. We assume the
rule-of-thumb queue size (i.e., RTT times link capacity [11])
at the target link. As shown in Figure 10a, as soon as TBE
is executed at time 5.0 sec, the measured RTT is signiﬁcantly
reduced to the near ideal RTT value. The new measured RTT
is 25% higher than the ideal RTT due to TBE’s rerouting, but it
is still signiﬁcantly smaller than the RTT measurements before
TBE.
We also test how the two delay-based TCP variants, Com-
pound TCP and TCP Vegas, adjust their congestion window
in response to TBE. Figure 10b shows that both TCP variants
increase their congestion window promptly when TBE is
performed and reach the converged points less than 3 seconds.
VII. EVALUATION
In this section, we evaluate SPIFFY in an SDN testbed
to show its effectiveness (xVII-A). Then we evaluate it using
ﬂow-level simulations to show its feasibility in large ISP
networks (xVII-B).
A. Testbed Experiments
Our evaluations are executed on a server-grade Dell R720
machine with 20-core 2.8 GHz Xeon CPUs and 128 GB of
memory, which runs the KVM hypervisor on CentOS 6.5
(Linux kernel v2.6.32). We use Open vSwitch (OVS v2.3),
virtual switches [6]. OVS v2.3 supports the OpenFlow v1.3
[5] speciﬁcation. We use OpenFlow-enabled switches only at
the edges of our test network and traditional switches inside
the network. Note that we will interchangeably use switches
and routers in this paper. We implement SPIFFY as a POX
application [7] on the centralized network controller. Notice
that in these SDN testbed experiments, we test only long-lived
TCP ﬂows generated by iperf3. The effects of short-lived
ﬂows are studied in packet-level simulations, as discussed in
Section VI-B.
1) Effectiveness of Bot Detection: We evaluate how ef-
fective SPIFFY is in identifying bots when they are mixed
with legitimate senders. We implement the bots based on the
Attack Strategy AS:spiﬀy. Bot upstream is saturated by attack
ﬂows, each of which have the degraded rate, rd. Note that the
adversary in this evaluation does not apply the rate-increase
9
RCth012345678910False-positive rate10-310-210-1100ratemin = 0 Kbps    = 20 Kbps    = 40 Kbps    = 60 Kbps    = 80 Kbps    = 100 KbpsTime (sec)0123456789101112congestion   window    (cwnd)0102030(b)Compound TCPTCP Vegas0123456789101112     Averagemeasured RTT     (msec)0100200(a)measured RTTTBE startsat 5.0 secFig. 11: Parameters for SPIFFY experiments.
mimicry (RM) and thus her bots have no available bandwidth
to demonstrate the rate increase.
Fig. 12: Effectiveness of TBE for bot identiﬁcation.
In our simpliﬁed ISP network with two edge switches (one
ingress and one egress) and 10 parallel links that connect the
two edge switches (one of them is the target link of the attack),
we reroute trafﬁc crossing the target link to other parallel links
and provide ten-times expanded bandwidth (i.e., M = 10) to
the two senders. For this, the SPIFFY application installs rules
and MPLS labels (which are prevalently used in large ISPs [22]
and can be implemented by SDN switches [48]) at the edge
switches.
Figure 11 shows the general parameters for the SPIFFY
experiments. A bot bi (1 (cid:20) i (cid:20) m) has upstream bandwidth
i and a legitimate sender lj; (1 (cid:20) j (cid:20) n) has upstream
ub
j. We set the number of bots m and their upstream
bandwidth ul
bandwidths ub
i in such a way that the fair-share per-ﬂow rate
at the target link L equals rd (i.e.,
= rd)
to achieve the attack goal Gstrength. Notice that all bots
generate f b
i =rd ﬂows of rate rd to saturate their upstream
bandwidth.
i = ub
∑
∑
B
i +
m
i=1 f b
n
j=1 f l
j
In our experiments, we set all senders (both bots and legit-
imate senders) to send 50 long-lived TCP ﬂows to make them
indistinguishable to any per-host rate ﬁltering mechanisms.
Accordingly, all bots are set to have upstream bandwidth ub
i =
(cid:2) rd = 0:5 Mbps when rd = 10 Kbps. We set all legitimate
f b
i
senders’ bandwidth to accommodate all 50 legitimate ﬂows
(cid:2) rg = 5 Mbps when
with guaranteed rate rg. That is, ul
rg = 100 Kbps. Note that the upstream bandwidth parameters
for bots and legitimate senders are selected for illustrative
purpose only. SPIFFY is effective for any practical upstream
link bandwidth. RTTs are set to be 200 msec to experiment
the practically worst-case rate-change responsive time for TBE
operation.
j = f l
j
Figure 12 shows the per-sender rate changes of the two
senders measured every second by the edge switches. The
rate is measured from t = 0 to t = 20 seconds, when the
TBE operation is performed at t = 10 second. Notice that
before TBE (i.e., at t < 10), the two senders’ rates are almost
identical. However, once TBE is performed, within less than 5
seconds (i.e., at t < 15), the two senders show very different
rate changes; the legitimate sender’s rate increases by almost
10 times whereas the bot’s per-sender rate remains the same. At
the legitimate sender TCP adapts to the expanded bandwidth in
less than 5 seconds. Note that after TBE ends (i.e., at t = 15),
SPIFFY immediately starts the bot identiﬁcation. The target
network notices the difference in the rate changes, identiﬁes
the bot, and ﬁlters its source IP at the corresponding ingress
10
switch of the network. As a result, after t = 15 the bot’s
rate tapers off quickly while the legitimate sender achieves the
guaranteed rate rg = 100 Kbps = 5 Mbps / 50 ﬂows.
2) Effectiveness of Increasing Attack Cost: Unlike the
previous experiment, in this evaluation an adversary decides
to follow the rate-increase mimicry (RM) and increases her
attack cost. To demonstrate how the number of bots required
to achieve Gstrength differ for defense strategies, we implement
a simple adversary program that manages the bots and adapts
to the defense changes at the target network. This program
increases the number of bots in the attack; i.e., if the attack is
unsuccessful (i.e., the average per-ﬂow rate, ravg, at the target
network is larger than rd), it adds more bots at the rate of one
additional bot per second.
We evaluate the effectiveness and the cost of the attack
against the three different defense strategies: (a) no defense:
a strategy that only provides per-ﬂow fairness, which is au-
tomatically achieved by TCP’s congestion control mechanism;
(b) ordinary trafﬁc engineering (TE): a strategy that provisions
additional bandwidth by rerouting trafﬁc crossing the target
link (both malicious and benign) persistently as long as the
ﬂooding continues; and (c) SPIFFY: a strategy that performs
TBE and rate-increase measurement on demand to test the
bots. Note that ordinary TE provisions the additional band-
width persistently without attempting to detect the bots, while
the TBE operation is temporary and only for testing bots.
We utilize 130 bots and each of them have 1 Mbps of up-
load bandwidth limit. The per-ﬂow rate demand for legitimate
senders is rg = 100 Kbps while bots have the rate demand
of rd = 10 Kbps for no defense and ordinary TE. However,
since the attack against SPIFFY has the demand-rate mimicry
goal (RM), its bots have the rate demand of rg = 100 Kbps.
The target link bandwidth is set to be 8 Mbps. The number
of senders and the bottleneck bandwidth are limited by our
experiment setup. Through additional packet-level simulations
(Section VI-B) and ﬂow-level simulations (Section VII-B), we
show that the results from these limited-bandwidth experiments
scale to large conﬁgurations.
Figure 13 shows the results of the evaluation over the three
defense strategies. In the two plots, the x-axis represents the
wall-clock time of the experiment. The adaptive adversary
program starts from time t = 0, increasing its number of
bots by 1 every second, if the adversary goal Gstrength is not
satisﬁed. Figure 13a shows the average per-ﬂow rate changes
ljbi≤ ujl≤ Bfjlfib≤ uibn legit.sendersm botstarget link L......Time (sec)05101520Per-sender rate (Mbps)0123456Bot(w/ 50 flows)Legitimate sender(w/ 50 flows)TBE starts at 10.0 secTBE ends at 15.0 secFig. 13: Measured average per-ﬂow rates (ravg) and the
number of used bots (#bots) for the three defense strate-
gies.
Cogent
Tata
UUNET
NTT
#routers