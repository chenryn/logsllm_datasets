title:TextShield: Robust Text Classification Based on Multimodal Embedding
and Neural Machine Translation
author:Jinfeng Li and
Tianyu Du and
Shouling Ji and
Rong Zhang and
Quan Lu and
Min Yang and
Ting Wang
TexTShield: Robust Text Classification Based on 
Multimodal Embedding and Neural Machine Translation
Jinfeng Li, Zhejiang University, Alibaba Group; Tianyu Du, Zhejiang University; 
Shouling Ji, Zhejiang University, Alibaba-Zhejiang University Joint Research 
Institute of Frontier Technologies; Rong Zhang and Quan Lu, Alibaba Group; 
Min Yang, Fudan University; Ting Wang, Pennsylvania State University
https://www.usenix.org/conference/usenixsecurity20/presentation/li-jinfeng
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.TEXTSHIELD: Robust Text Classiﬁcation Based on Multimodal Embedding and
Neural Machine Translation
Jinfeng Li†,¶, Tianyu Du†, Shouling Ji†, +, ((cid:2)), Rong Zhang¶, Quan Lu¶, Min Yang§, and Ting Wang‡
†Zhejiang University, ¶Alibaba Group, +Alibaba-Zhejiang University Joint Research Institute of Frontier
Technologies, §Fudan University, ‡Pennsylvania State University
PI:EMAIL, PI:EMAIL,
PI:EMAIL, PI:EMAIL,
E-mails:
PI:EMAIL, PI:EMAIL,
inbox.ting@gmail.com.
Abstract
Text-based toxic content detection is an important tool for
reducing harmful interactions in online social media environ-
ments. Yet, its underlying mechanism, deep learning-based
text classiﬁcation (DLTC), is inherently vulnerable to mali-
ciously crafted adversarial texts. To mitigate such vulnerabili-
ties, intensive research has been conducted on strengthening
English-based DLTC models. However, the existing defenses
are not effective for Chinese-based DLTC models, due to
the unique sparseness, diversity, and variation of the Chinese
language.
In this paper, we bridge this striking gap by pre-
senting TEXTSHIELD, a new adversarial defense frame-
work speciﬁcally designed for Chinese-based DLTC models.
TEXTSHIELD differs from previous work in several key as-
pects: (i) generic – it applies to any Chinese-based DLTC
models without requiring re-training; (ii) robust – it signiﬁ-
cantly reduces the attack success rate even under the setting
of adaptive attacks; and (iii) accurate – it has little impact
on the performance of DLTC models over legitimate inputs.
Extensive evaluations show that it outperforms both existing
methods and the industry-leading platforms. Future work will
explore its applicability in broader practical tasks.
1 Introduction
In this era of social networking, online social networks have
become a de facto portal for hundreds of millions of Inter-
net users (netizens) [3]. However, of the vast user generated
text content produced everyday, a signiﬁcant portion is toxic
(e.g., abusive, pornographic and violent content), which rep-
resents an immense threat to the physical and mental health
of netizens, especially young ones. It was reported that major
social media platforms (e.g., Twitter and Facebook) were all
criticized for not doing enough to curb the diffusion of toxic
content and under pressure to cleanse their platforms [31].
Jinfeng Li and Tianyu Du are the co-ﬁrst authors. Shouling Ji is the
corresponding author.
Yet, the sheer amount and rampant growth of toxic content
represent non-trigger challenges facing such effort.
To this end, automated techniques, especially deep learning-
based text classiﬁcation (DLTC), have been applied to online
toxic content detection. Thanks to the state-of-the-art perfor-
mance of deep neural network (DNN) models, DLTC-based
toxic content detection signiﬁcantly outperforms the time-
consuming and laborious manual censorship in terms of both
efﬁciency and effectiveness [18, 22, 34]. However, recent
studies have revealed that existing DLTC models are inher-
ently vulnerable to adversarially generated texts [9,11,26,37],
which are maliciously crafted texts that trigger DLTC models
to misbehave. In the context of toxic content detection, the
adversary may generate texts that remain toxic but evade the
detection of DLTC models. As a concrete example, to make
insulting comments evasive, the adversary may obfuscate
some words with their variants, such as substituting “idiot”
with “idi0t”. These variants are visually similar to their orig-
inal words (i.e., remaining toxic) but are able to effectively
evade the detection. Such adversarial texts can be crafted
under either white-box [9, 37] or black-box [11, 26] setting.
In general, the white-box attacks aim to generate adversarial
texts with the guidance of complete knowledge (e.g., architec-
tures and parameters) about the target model. The black-box
attacks generate adversarial texts by estimating the gradient
or exploring model sensitivity based on the classiﬁcation con-
ﬁdence when detailed model information is not available.
To defend against such attacks, countermeasures such
as adversarial training [13, 43, 44] and spelling correction
[26, 46, 47] have been proposed to enhance the robustness
of English-based DLTC models, which have achieved con-
siderable success. In comparison, the effort of improving
the robustness of Chinese-based DLTC models is still fairly
limited. Even worse, the existing defenses that are effective
for English-based DLTC models are often inapplicable to
Chinese-based models due to the following reasons: (i) un-
like English which has a relatively small alphabet, Chinese is
logographic with a large set of characters that are individually
meaningful and the modiﬁcation of a single character may
USENIX Association
29th USENIX Security Symposium    1381
drastically alter the semantics of the text, making Chinese-
based DLTC models inherently more vulnerable; (ii) it is fun-
damentally more challenging to perform spelling correction
in Chinese since there is no word delimiter in Chinese writ-
ten texts while variant characters can only be determined at
the word-level; and (iii) the model retrained with adversarial
training is still likely to be sensitive to new attacks due to the
sparseness and diversity of Chinese adversarial perturbations.
Concretely, there are more than 50,000 characters1 might be
perturbed by various variation strategies such as glyph-based
and phonetic-based strategies (more detailed examples about
these variations can be seen in Section 4.3). Given the scale
of Chinese-based social media platforms (e.g., WeChat enjoys
one billion daily active users [17]), the lack of robust toxic
content detection represents an immense concern.
Our Work. To bridge this striking gap, in this paper, we
present TEXTSHIELD, a novel adversarial defense framework
for Chinese-based DLTC systems based on multimodal em-
bedding and neural machine translation (NMT) [2]. At a high
level, TEXTSHIELD performs robust toxic text detection in
three phases. First, each text input is corrected by an adversar-
ial NMT model for denoising some of the adversarial pertur-
bations; second, the corrected text is converted to multimodal
embedding, which extracts its semantic, glyph and phonetic
features for dealing with the glyph-based and phonetic-based
perturbations; ﬁnally, the extracted features are fused to form
a semantic-rich representation, which is ready for the regular
toxic classiﬁcation. Through intensive empirical evaluations
on two real-world datasets collected from Chinese online so-
cial media (e.g., Sina Weibo), we show that TEXTSHIELD is
effective in defending against both the obfuscated texts gener-
ated by the adversary and the adversarial texts generated by
the state-of-the-art attacks. It also outperforms four industry-
leading online toxic content detection platforms including
Alibaba GreenNet, Baidu TextCensoring, Netease Yidun and
Huawei Moderation. We are currently in the process of inte-
grating TEXTSHIELD with Alibaba GreenNet to enhance its
robustness.
The main contributions of this paper can be summarized as
follows.
• We propose TEXTSHIELD, which to our best knowledge is
the ﬁrst adversarial defense specialized for Chinese DLTC
tasks without retraining the model, in which a novel mul-
timodal embedding scheme is proposed to enhance the ro-
bustness of DLTC models and an adversarial NMT is ﬁrst
applied to reconstruct the original texts.
• We evaluate the effectiveness of TEXTSHIELD in real-world
adversarial scenarios. The evaluation results show that
TEXTSHIELD attains high accuracy (e.g., 0.944 for porn
detection) on the malicious user generated obfuscated texts
while having little impact on the model performance (e.g.,
the accuracy degrades by less than 2%) over benign inputs.
1https://en.wikipedia.org/wiki/Chinese_characters
• We verify the robustness of TEXTSHIELD under the setting
of adaptive attacks in two real-world tasks and compare
it with four industry-leading platforms, which shows that
TEXTSHIELD is of great practicability and superiority in
decreasing the attack success rate (e.g., the attack success
rate against abuse detection is degraded by 74.5%).
2 Related Work
2.1 Adversarial Text Generation
Adversarial attacks against DNNs are ﬁrst explored in the
context of image classiﬁcation [13, 19, 27, 29, 39, 43, 49] and
are then extended to the NLP domain. We here mainly focus
on discussing the work related to generating adversarial texts.
In one of the ﬁrst attempts at tricking DLTC systems, Pa-
pernot et al. [37] introduced a white-box attack for generat-
ing adversarial inputs by leveraging the computational graph
unfolding technique. Ebrahimi et al. [9] showed that automat-
ically swapping one token for another with the guidance of
gradients can deceive the character-level DLTC models. Jia
et al. [20] generated adversarial texts for evaluating reading
comprehension systems by adding distracting sentences to
the original text based on manually-deﬁned rules. Hosseini
et al. [15] showed that simple modiﬁcations, such as adding
dots or spaces between characters, can drastically change the
toxicity score of Google’s Perspective API. Li et al. [26]
proposed TextBugger, a state-of-the-art black-box attack that
successfully compromised 15 real-world applications.
Unlike English adversarial texts, most of the Chinese ad-
versarial texts are manually crafted by real-world malicious
netizens, which are more diverse due to the various word
variation strategies adopted by different netizens [21]. In
addition, there is an extremely large character space in Chi-
nese in which each character may be perturbed by various
strategies, which makes the perturbations more sparse.
2.2 Defenses against Adversarial Text
To defend against the above attacks, several defenses have
been proposed in the English NLP domain, including adver-
sarial training [8, 20, 44] and spelling correction [11, 26].
Adversarial Training. It was ﬁrst proposed in [43] to en-
hance the robustness of DNNs used for image classiﬁcation
by augmenting training data with adversarial images. Wang
et al. [44] and Ebrahimi et al. [8] presented several initial at-
tempts to tackle adversarial texts by retraining the models with
diversiﬁed adversarial training data and showed a marginal
increase in robustness. However, since there currently exists
no automatic attack for generating Chinese adversarial texts
while the manual collection of user generated obfuscated texts
is often laborious and costly, it is not trivial to extend existing
adversarial training to the Chinese NLP domain. More im-
portantly, the sparseness of Chinese adversarial perturbations
1382    29th USENIX Security Symposium
USENIX Association
may also weaken its efﬁcacy.
Spelling Correction. In the English NLP domain, Gao et
al. [11] and Li et al. [26] leveraged the context-aware spelling
correction approach to block editorial adversarial attacks (e.g.,
insertion, deletion and substitution) and achieved satisfactory
performance. In the Chinese NLP domain, similar methods
have also been tried to deal with user generated obfuscated
texts, e.g., using dictionary-based [46] or language model-
based [47] methods to restore the variant words to their benign
format. However, compared to the alphabetical languages like
English and French, it is more difﬁcult to perform spelling
correction in Chinese since there is no word boundary in
Chinese writing texts while variant characters can only be
determined at the word-level. Hence, it has been shown to
have limited effect on model performance. Furthermore, the
diversity, sparseness and dynamicity of Chinese adversarial
perturbations may also challenge this approach.
3 Design of TEXTSHIELD
3.1 Problem Deﬁnition and Threat Model
Given a legitimate Chinese text xxx ∈ X that contains N charac-
ters (i.e, xxx = {x1,x2,··· ,xN}), and a DLTC model F : X → Y
which maps from the feature space X to the label space Y ,
an attacker who has query access to the classiﬁcation conﬁ-
dence returned by this model, aims to generate an adversarial
text xxxadv from xxx whose ground truth label is y ∈ Y , such that
F (xxxadv) = t(t (cid:54)= y).
In this paper, we aim to defend against such attacks by
leveraging an NMT model which translates a source sequence
into the target sequence to restore xxxadv, and universally im-
proving the robustness of F by embedding the input from
multi-modalities (e.g., semantics, glyphs and phonetics). For-
mally, our defense can be deﬁned as
F (Esgp(argmax
xxx∗∈X
p(xxx∗|xxxadv;θ))) = y,
(1)
where Esgp(·) is the multimodal embedding function, xxx∗ is a
candidate text corrected from xxxadv, p(xxx∗|xxxadv;θ) is the proba-
bility of outputting xxx∗ given xxxadv, and θ is the parameters of
the NMT model learned from an adversarial parallel corpora
consisting of a plenty of aligned (xxxadv,xxxori) sentence pairs.
3.2 Overview of TEXTSHIELD Framework
We present the framework overview of TEXTSHIELD in Fig. 1,
which is built upon multimodal embedding, multimodal fu-
sion and NMT. Generally, we ﬁrst feed each text into an
NMT model trained with a plenty of adversarial–benign text
pairs for adversarial correction. Then, we input the corrected
text into the DLTC model for multimodal embedding to ex-
tract features from semantic-level, glyph-level and phonetic-
level. Finally, we use a multimodal fusion scheme to fuse
Figure 1: The framework of TEXTSHIELD.
the extracted features for the following regular classiﬁcations.
Below, we will elaborate on each of the backbone techniques.
3.3 Adversarial Translation
We propose a novel adversarial corrector based on NMT and
the framework is shown in Fig. 2. Generally, we ﬁrst train
an NMT model on a large adversarial parallel corpora for
adversarial reconstruction. Then, we put it in front of the
DLTC model based on multimodal embedding to restore the
adversarial perturbations to their benign counterparts.
Model Design. We design the adversarial NMT model
based on the Encoder–Decoder framework proposed in
[6, 42], in which an encoder reads and encodes a source se-
quence xxx = (x1,x2,··· ,xN) into a ﬁxed-length context vec-
tor ccc and a decoder decodes ccc and outputs a translation
xxx∗ = (x(cid:48)
1,x(cid:48)
N(cid:48)) by maximizing the ordered conditional
probability
p(xxx∗|xxx) =
2,··· ,x(cid:48)
p(x(cid:48)
g(x(cid:48)
1,··· ,x(cid:48)
t−1,ssst ,ccc),
t−1,ccc) =
t|x(cid:48)
(2)
N
∏
t=1
N
∏
t=1
where g is a nonlinear function that outputs the probability of
x(cid:48)
t, and ssst is the hidden state of the decoder at time t.
We use the long short-term memory (LSTM) network f
with two layers to implement the encoder E and decoder
D, and use Bahdanau’s attention mechanism [2] to align xxx
and xxx∗. Moreover, we integrate a residual layer to learn the
identity mapping since xxx and xxx∗ only differ in few characters.
Hence, the context vector ccci for each target character x(cid:48)
i can
be computed by the weighted sum of the hidden state hhh j of E
at each time j,
N
∑
exp(ei j)
∑
ccci =
∑N
k=1 exp(eik)
ei j = a(sssi−1,hhh j) = vvv(cid:62)
a · tanh(WWW a · sssi−1 +UUU ahhh j),
αi j · hhh j =
· hhh j
(3)
j=1
j=1
N
where vvva, WWW a and UUU a are the weight matrices of the additive
alignment model. The hidden state hhh j is calculated by hhh j =
f (x j,hhh j−1) and sssi is calculated by sssi = f (x(cid:48)
i−1,sssi−1,ccci). Then,
USENIX Association
29th USENIX Security Symposium    1383
EncoderInputDecoderBenign TextAdversarial TextVariation generatorLSTMLSTMLSTMLSTMInferencePhaseTraining Phase(cid:1)Task: Abuse Detection. DLTC: BiLSTM. Original Label: 98.3% Toxic.  Adv Label: 71.3% Normal这个作者是个智障樟，写的小说简直是垃圾极，没t有人会想看！（This author is an idiot,  the novel he writes is absolutely rubbish and no one wants to read it.）After Translation: 74.2% Toxic这个作者是个智障，写的小说简直是垃极，没有人会想看！Final Prediction: 98.2% Toxic①②③3.4 Multimodal Embedding
Since the variation strategies adopted by malicious users in
the real scenarios are mainly concentrated on glyph-based and
phonetic-based perturbations [47], we therefore dedicatedly
propose three embedding methods across different modalities
to handle the corresponding variation types, i.e., semantic
embedding, glyph embedding and phonetic embedding. They
are also dedicatedly designed to deal with the sparseness and
diversity unique to Chinese adversarial perturbations.
Semantic Embedding. We apply the skip-gram model pro-
posed in [32] to learn continuous semantic word vectors. Note
that we concentrate on character-level embedding since word-
level embedding suffers the most from the out-of-vocabulary
(OOV) phenomena, thus weakening the robustness of DLTC
models. Speciﬁcally, the skip-gram model maps each char-
acter in vocabulary of size V to a continuous embedding
space of d dimensions by looking up an embedding matrix
WWW (1), which is learned by maximizing the probability calcu-
lated by the matrix WWW (2) of its neighbors within a context
window. Formally, given a text contains N characters, i.e.,
xxx = {x1,x2,··· ,xN}, the objective of the skip-gram model is
to maximize the average log probability
Q =
1
N
N
∑
n=1
∑
−c≤ j≤c, j(cid:54)=0
log p(xn+ j|xn),
(7)