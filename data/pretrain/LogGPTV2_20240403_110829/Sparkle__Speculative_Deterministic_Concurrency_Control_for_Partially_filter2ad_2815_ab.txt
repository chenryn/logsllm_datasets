to order and execute transactions only at the data partitions
they actually access. Such an assumption is typically easy to
meet in practice, given that data partitions are normally quite
coarse grained. In fact, overestimating the set of partitions
accessed by a transaction does not compromise consistency,
but only impacts efﬁciency by causing unnecessary order-
ing and transaction execution. Unlike other PRSM solutions,
e.g., [42], we do not assume any ﬁne-grained information on
the individual data items that transactions access.
As mentioned, we distinguish between single and multi
partition transactions (SPTs and MPTs, respectively). We refer
to the instances of an MPT at the various partitions it accesses
as sub-transactions or siblings, and denote the set of partitions
involved by an MPT T using the notation involved(T ). Unlike
SPTs, which execute independently at each replica, MPTs
require, in the general case, communication among siblings,
as they may need to access data stored on remote partitions.
When a sub-transaction reads a local key for the ﬁrst time,
it disseminates the corresponding value to its siblings; when
a sub-transaction issues a read to a remote key which has not
been received yet, it blocks until the value is received. As
remote keys do not need to be maintained locally, writes to
remote keys are only applied to a private transaction’s buffer
(to be available if they are later read by the same transaction)
that is discarded after the transaction’s commit.
166
IV. PRSM MODEL
Sparkle is a deterministic distributed concurrency control
designed to accelerate the execution phase of a generic PRSM
system, e.g., [5], [30], [42], which operates according to the
abstract order-then-execute model deﬁned below.
Ordering phase. The protocol used during the ordering phase
is irrelevant for Sparkle, provided that
the ﬁnal order it
establishes ensures the following properties:
1) all the (correct) replicas of the same partition deliver
the same sequence, B1, . . . , Bn, of transaction batches,
where each batch contains the same totally ordered set
of (single- or multi-partition) transactions;
2) if an MPT T is delivered in the i-th batch by a partition,
then T is delivered in the i-th batch of all the partitions
it involves;
3) for any pair of MPTs, say T1 and T2, that access a set of
common partitions, say S = {P1, . . . , Pn}, T1 and T2
are ordered in the same way by all the (correct) servers
that replicate any partition in S, i.e., either ∀Pi ∈ S
T1 → T2 or ∀Pi ∈ S T2 → T1;
T < T
(cid:2) iff any partition delivers T and T
4) the relation < is acyclic, where < is deﬁned as follows:
(cid:2) in that order.
The ordering phase establishes a total order on the trans-
actions executing at each partition, whereas the transactions
executing at different partitions are only partially ordered. We
refer to the order established by this phase as ﬁnal order. We
call the transactions ordered before/after a transaction T , T ’s
preceding/following transactions, respectively.
Existing PRSM systems ensure the above properties in
different ways. Calvin, for instance, relies on a two-phase
scheme (see Fig. 1). In the ﬁrst one, called replication phase,
servers periodically batch, e.g., for 5-10 msecs, the transac-
tions received from clients and submit the resulting batch to an
intra-partition consensus service. This merges the transactions
gathered by every replica of a given partition and replicates
them in a fault-tolerant manner. In the second phase, called
dispatching phase, all partitions within the same DC exchange
the transactions they delivered during the ﬁrst phase. This
ensures that MPTs are delivered at all
the partitions that
they need to access. Finally, the transactions gathered during
the dispatching phase are deterministically sorted to ensure a
consistent ﬁnal order across all the replicas of every partition.
Execution phase. Once the ordering phase is completed,
transactions are executed at all the partitions’ replicas they
involve. As already mentioned, in order to ensure inter-replica
consistency, the execution phase must guarantee that, at all
the the replicas of a partition, the transactions delivered by the
ordering phase are executed according to the same serialization
order, i.e., their execution history is equivalent to a common
sequential history. Sparkle’s concurrency control ensures this
guarantee, while allowing transactions to be executed concur-
rently. As such, it ensures serializability semantics [4]. Further,
if the protocol used during the ordering phase ensures real-time
ordering between transactions (i.e., given two transactions T1,
T2, where T1 precedes T2 according to real-time order, T1
167
is serialized before T2 by the ordering phase) then Sparkle
globally guarantees strict serializability.
Failure handling. Dealing with failures is relatively simple
in PRSM-based systems (including Sparkle). Since all correct
replicas of a partition deliver the same transactions in each
batch, MPTs can fetch remote data from any available replica.
In order to provide end-to-end fault-tolerance guarantees, in
case the replica originally contacted by a client fails (or is sus-
pected to have failed), the client can contact any other replica
provided that some complementary mechanism is employed
to ensure exactly-once semantics [15], [37].
V. SPARKLE
This section describes Sparkle’s deterministic concurrency
control scheme. We start by discussing the processing of SPTs
(§V-A) and MPTs (§V-B). Finally, we discuss how to optimize
the treatment of read-only transactions (§V-C).
A. Single partition transactions
that
To maximize parallelism, Sparkle employs a multi-
versioned, optimistic concurrency control
imposes no
constraints on the processing order of transactions. Denoting
with local_ts the logical timestamp that reﬂects the ﬁnal order
at a partition, threads select as the next transaction to start, the
one with the smallest local_ts value. However, as transactions
are processed concurrently, they can be speculatively executed
according to a spontaneous, non-deterministic serialization
order that contradicts the ﬁnal order.
To ensure consistency, Sparkle guarantees that a ﬁnal
committed transaction must have observed a snapshot that
includes the versions produced by all its preceding transactions
(according to the ﬁnal order). This property is enforced by
letting a transaction T ﬁnal commit only if all its preceding
transactions have ﬁnal committed and if T did not miss any
of the updates they produced — which can happen if T reads
a data item before any of its preceding transactions writes to
it, i.e., a write-after-read conﬂict. Misspeculations are detected
at run-time, leading to the automatic abort and restart of the
affected transactions. Transactions are restarted with the same
timestamp to ensure deterministic execution across replicas.
In order to enhance efﬁciency and reduce the chance
of misspeculations, Sparkle incorporates a timestamp-based
locking scheme. The timestamp of transactions, i.e. local_ts,
establishes a total order on item versions created by ﬁnal
and speculatively committed transactions, and also deﬁnes the
visibility of versions: a transaction only reads the latest version
produced by speculatively or ﬁnal committed transactions
ordered before it. When writing a data item for the ﬁrst
time, a transaction T locks the data item, which prevents it
from being accessed by T ’s following transactions before T
ﬁnishes execution. Also, when writing, T inspects the data
item’s read_dependencies. These register which transactions
have already read this data item, and allow T to abort any
following transaction that missed T ’s updates. Correspond-
ingly, when reading, it is checked if a transaction with a lower
timestamp has locked the data item: in the negative case, the
reader registers its timestamp in read_dependencies to notify
future writers; else, the execution of the reader transaction is
suspended till the writer completes.
Next, we provide additional details on the management of
SPTs. Due to space constraints, we omit the corresponding
pseudo-code, which is available in our technical report [29].
Start. Upon activation, each transaction initializes three main
data structures: its readset, writeset and abort_f lag. The
readset and writeset are private buffers that store the
data items read and updated by the transaction, respectively.
abort_f lag is used to check whether the transaction has been
aborted by other transaction.
Execution. During its execution, a transaction T may read and
update multiple data items. Before executing any operation, T
checks its abort_f lag to determine if it has been ﬂagged for
abort by some preceding transaction. In this case, T is aborted
and re-executed. Prior to its ﬁrst update to a data item, T
tries to obtain an exclusive lock to it. If the lock is held by a
(cid:2) that follows T in the ﬁnal order (i.e.,
different transaction T
(cid:2) from
the local_ts of T
(cid:2) to true. Conversely,
the lock and sets the abort_f lag of T
(cid:2) to
if the locking transaction T
ﬁnish execution. Once T successfully obtains the lock on the
data item, it applies the update to its writeset.
(cid:2) is larger than that of T ), T ejects T
(cid:2) precedes T , T waits for T
While executing a read operation, T ﬁrst attempts to read
from its writeset and readset, to return any version it has
previously written or read. Else, T redirects its read to the data
store and checks the state of the lock guarding the data item
it intends to read. Similar to the above locking procedure, T
is suspended if the data item is currently being locked by any
of its preceding transactions. Otherwise (i.e., the item is not
locked, or locked by T ’s following transactions), T scans the
version list and returns the version with the largest timestamp
smaller than its local_ts. Note that this may not be the version
that T would observe, had transactions been executed serially
according to the ﬁnal order, as other transactions preceding
T may later produce more recent versions. Thus, unless there
are no uncommitted transactions preceding T , T appends its
local_ts to the read_dependencies of the data item. This
allows aborting T if a write-after-read conﬂict is later detected.
Suspended transactions. As mentioned, a transaction T is
suspended if it tries to read/update a data item that is currently
being locked by a preceding transaction. In that case, the
thread executing T can start executing the next unprocessed
transaction according to the ﬁnal order, so to enhance paral-
lelism. T will eventually be unblocked when the contending
transactions release the lock requested by T . At this point, the
thread responsible of T can resume its execution.
Speculative/ﬁnal commit. After completing its execution, T
attempts to speculatively commit, so to make its writes visible
to other transactions. For each data item it updated, T inserts
a new version in the item’s version chain, timestamped and
ordered by its local_ts, and releases the corresponding lock.
Then, T checks the read_dependencies tracked by this data
168
item and aborts any (therein registered) transaction with a
larger timestamp (as they missed T ’s update on this item) by
setting their abort_f lag to true. Additionally, T prunes the
identiﬁers of any ﬁnal committed transaction still tracked in
read_dependencies, which are unnecessary as they no longer
risk to abort. While applying its updates, if T ﬁnds that any of
its obtained locks has already been preempted, it aborts itself
by removing all inserted versions and releasing any remaining
lock. Else, T is considered to be speculatively-committed.
Next, T checks if it can ﬁnal commit, which is only possible
if i) all its preceding transactions have already committed and
ii) its abort_f lag is still f alse. AsT ’s updates have already
been applied in the previous step, the ﬁnal commit logic is
very fast, requiring essentially to only increase the counter
that tracks the timestamp of the most recent ﬁnal committed
transaction. Recall, in fact, that the read_dependencies of
ﬁnal committed transactions are pruned in an opportunistic
way by transactions that update those data items in the future.
If T can not be ﬁnal committed, yet, the thread processing
T simply executes the next unprocessed transaction and peri-
odically checks the state of T , to ﬁnal commit it, if possible.
Abort. T can only be aborted due to data conﬂicts with
preceding transactions, either because T missed updates from
a preceding transaction, or because any of its locks was
preempted by a preceding transaction. If either case occurs,
T ’s abort_f lag is set to true. Then, T aborts by releasing all
its locks and removing any version it has inserted in the data
store (in case T had speculatively committed).
B. Multi Partition Transactions
During their execution, the sub-transactions of an MPT
disseminate the results of read operations on local data items
to the other involved partitions (§III). By letting MPTs execute
speculatively, i.e., without waiting for the ﬁnal commit of their
preceding transactions, then a MPT sub-transaction may miss
a local data item version not yet produced by a preceding
transaction and send inconsistent data to its siblings.
We deﬁne a global consistent snapshot for a MPT T
as the union of the local consistent snapshots at all
the
partitions involved by T , where a local consistent snapshot
for T at partition X is obtained by serially committing all the
transactions preceding T according to the ﬁnal order at X.
The key challenge to ensure safe speculative execution of
MPTs lies then in detecting if an MPT instance observed a
global consistent snapshot and can, thus, be ﬁnal committed.
Batches of homogeneous MPTs. To simplify presentation,
we describe the proposed solution by ﬁrst assuming that 1)
all transactions delivered during the ordering phase are MPTs
that access the same set of partitions, noted P, and 2) different
batches are never concurrently executed. In the following we
use the term homogeneous MPTs to denote a set of MPTs
that access the same set of partitions. We will later discuss
why this assumption is needed and how to cope with generic
batches composed by SPTs and heterogeneous MPTs later.
(cid:28)(cid:32)(cid:19)(cid:33)(cid:1)(cid:39)
(cid:25)(cid:32)(cid:19)(cid:33)(cid:2)(cid:38)
(cid:4)(cid:18)(cid:22)(cid:12)(cid:20)(cid:1)(cid:5)(cid:2)(cid:11)
(cid:10)(cid:40)(cid:30)(cid:34)(cid:39)(cid:29)(cid:38)(cid:35)
(cid:4)(cid:18)(cid:22)(cid:12)(cid:20)(cid:1)(cid:5)(cid:2)(cid:11)
(cid:10)(cid:41)(cid:30)(cid:34)(cid:38)(cid:29)(cid:38)(cid:35)
(cid:6)
(cid:2)
(cid:7)
(cid:30)
(cid:38)
(cid:38)
(cid:30)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:2)
(cid:11)
(cid:2)
(cid:9)
(cid:30)
(cid:36)
(cid:1)
(cid:3)(cid:4)
(cid:39)
(cid:37)
(cid:30)
(cid:34)
(cid:38)
(cid:38)
(cid:35)
(cid:6)
(cid:2)
(cid:7)
(cid:30)
(cid:39)
(cid:29)
(cid:29)
(cid:35)
(cid:38)
(cid:37)
(cid:36)
(cid:30)
(cid:9)
(cid:2)
(cid:29)
(cid:38)
(cid:34)
(cid:30)
(cid:11)
(cid:2)
(cid:5)
(cid:5)
(cid:2)
(cid:11)
(cid:30)
(cid:34)