title:Integrity Checking in Cryptographic File Systems with Constant Trusted
Storage
author:Alina Oprea and
Michael K. Reiter
Integrity Checking in Cryptographic File Systems
with Constant Trusted Storage
Alina Oprea∗
Michael K. Reiter†
Abstract
In this paper we propose two new constructions for pro-
tecting the integrity of ﬁles in cryptographic ﬁle systems.
Our constructions are designed to exploit two charac-
teristics of many ﬁle-system workloads, namely low en-
tropy of ﬁle contents and high sequentiality of ﬁle block
writes. At the same time, our approaches maintain the
best features of the most commonly used algorithm to-
day (Merkle trees), including defense against replay of
stale (previously overwritten) blocks and a small, con-
stant amount of trusted storage per ﬁle. Via implementa-
tions in the EncFS cryptographic ﬁle system, we evalu-
ate the performance and storage requirements of our new
constructions compared to those of Merkle trees. We
conclude with guidelines for choosing the best integrity
algorithm depending on typical application workload.
1
Introduction
The growth of outsourced storage in the form of storage
service providers underlines the importance of develop-
ing efﬁcient security mechanisms to protect ﬁles stored
remotely. Cryptographic ﬁle systems (e.g., [10, 6, 25,
13, 17, 23, 20]) provide means to protect ﬁle secrecy (i.e.,
prevent leakage of ﬁle contents) and integrity (i.e., detect
the unauthorized modiﬁcation of ﬁle contents) against
the compromise of the ﬁle store and attacks on the net-
work while blocks are in transit to/from the ﬁle store.
Several engineering goals have emerged to guide the de-
sign of efﬁcient cryptographic ﬁle systems. First, crypto-
graphic protections should be applied at the granularity
of individual blocks as opposed to entire ﬁles, since the
∗Computer Science Department, Carnegie Mellon University, Pitts-
burgh, PA, USA; PI:EMAIL
†Electrical & Computer Engineering Department and Computer
Science Department, Carnegie Mellon University, Pittsburgh, PA,
USA; PI:EMAIL
latter requires the entire ﬁle to be retrieved to verify its
integrity, for example. Second, applying cryptographic
protections to a block should not increase the block size,
so as to be transparent to the underlying block store.
(Cryptographic protections might increase the number of
blocks, however.) Third, the trusted storage required by
clients (e.g., for encryption keys and integrity veriﬁca-
tion information) should be kept to a minimum.
In this paper we propose and evaluate two new algo-
rithms for protecting ﬁle integrity in cryptographic ﬁle
systems. Our algorithms meet these design goals, and
in particular implement integrity using only a small con-
stant amount of trusted storage per ﬁle. (Of course, as
with any integrity-protection scheme, this trusted infor-
mation for many ﬁles could itself be written to a ﬁle
in the cryptographic ﬁle system, thereby reducing the
trusted storage costs for many ﬁles to that of only one.
The need for trusted information cannot be entirely elim-
inated, however.) In addition, our algorithms exploit two
properties of many ﬁle-system workloads to achieve efﬁ-
ciencies over prior proposals. First, typical ﬁle contents
in many ﬁle-system workloads have low empirical en-
tropy; such is the case with text ﬁles, for example. Our
ﬁrst algorithm builds on our prior proposal that exploits
this property [26] and uses tweakable ciphers [21, 15]
for encrypting ﬁle block contents; this prior proposal,
however, did not achieve constant trusted storage per ﬁle.
Our second algorithm reduces the amount of additional
storage needed for integrity by using the fact that low-
entropy block contents can be compressed enough to em-
bed a message-authentication code inside the block. The
second property that we exploit in our algorithms to re-
duce the additional storage needed for integrity is that
blocks of the same ﬁle are often written sequentially, a
characteristic that, to our knowledge, has not been previ-
ously utilized.
USENIX Association
16th USENIX Security Symposium
183
By designing integrity mechanisms that exploit these
properties, we demonstrate more efﬁcient integrity pro-
tections in cryptographic ﬁle systems than have previ-
ously been possible for many workloads. The measures
of efﬁciency that we consider include the amount of un-
trusted storage required by the integrity mechanism (over
and above that required for ﬁle blocks); the integrity
bandwidth, i.e., the amount of this information that must
be accessed (updated or read) when accessing a single
ﬁle block, averaged over all blocks in a ﬁle, all blocks in
all ﬁles, or all accesses in a trace (depending on context);
and the ﬁle write and read performance costs.
The standard against which we compare our algo-
rithms is the Merkle tree [24], which to date is the over-
whelmingly most popular method of integrity protection
for a ﬁle. Merkle trees can be implemented in crypto-
graphic ﬁle systems so as to meet the requirements out-
lined above, in particular requiring trusted storage per ﬁle
of only one output of a cryptographic hash function (e.g.,
20 bytes for SHA-1 [30]). They additionally offer an in-
tegrity bandwidth per ﬁle that is logarithmic in the num-
ber of ﬁle blocks. However, Merkle trees are oblivious
to ﬁle block contents and access characteristics, and we
show that by exploiting these, we can generate far more
efﬁcient integrity mechanisms for some workloads.
We have implemented our integrity constructions and
Merkle trees in EncFS [14], an open-source user-level
ﬁle system that transparently provides ﬁle block encryp-
tion on top of FUSE [12]. We provide an evaluation of
the three approaches with respect to our measures of in-
terest, demonstrating how ﬁle contents, as well as ﬁle ac-
cess patterns, have a great inﬂuence on the performance
of the new integrity algorithms. Our experiments demon-
strate that there is not a clear winner among the three
constructions for all workloads, in that different integrity
constructions are best suited to particular workloads. We
thus conclude that a cryptographic ﬁle system should im-
plement all three schemes and give higher-level applica-
tions an option to choose the appropriate integrity mech-
anism.
2 Random Access Integrity Model
We consider the model of a cryptographic ﬁle system that
provides random access to ﬁles. Encrypted data is stored
on untrusted storage servers and there is a mechanism
for distributing the cryptographic keys to authorized par-
ties. A small (on the order of several hundred bytes),
ﬁxed-size per ﬁle, trusted storage is available for authen-
tication data.
We assume that the storage servers are actively con-
trolled by an adversary. The adversary can adaptively
alter the data stored on the storage servers or perform
any other attack on the stored data, but it cannot modify
or observe the trusted storage. A particularly interesting
attack that the adversary can mount is a replay attack, in
which stale data is returned to read requests of clients.
Using the trusted storage to keep some constant-size in-
formation per ﬁle, and keeping more information per ﬁle
on untrusted storage, our goal is to design and evaluate
integrity algorithms that allow the update and veriﬁcation
of individual blocks in ﬁles and that detect data modiﬁ-
cation and replay attacks.
In our framework, a ﬁle F is divided into n ﬁxed-size
blocks B1B2 . . . Bn (the last block Bn might be shorter
than the ﬁrst n − 1 blocks), each encrypted individually
with the encryption key of the ﬁle and stored on the un-
trusted storage servers (n differs per ﬁle). The constant-
size, trusted storage for ﬁle F is denoted TSF . Addi-
tional storage for ﬁle F , which can reside in untrusted
storage, is denoted USF ; of course, USF can be written
to the untrusted storage server.
The storage interface provides two basic operations to
the clients: F.WriteBlock(i, C) stores content C at block
index i in ﬁle F and C ← F.ReadBlock(i) reads (en-
crypted) content from block index i in ﬁle F . An in-
tegrity algorithm for an encrypted ﬁle system consists of
ﬁve operations.
In the initialization algorithm Init for
ﬁle F , the encryption key for the ﬁle is generated. In an
update operation Update(i, B) for ﬁle F , an authorized
client updates the i-th block in the ﬁle with the encryp-
tion of block content B and updates the integrity infor-
mation for the i-th block stored in TSF and USF .
In
the check operation Check(i, C) for ﬁle F , an authorized
client ﬁrst decrypts C and then checks that the decrypted
block content is authentic, using the additional storage
TSF and USF for ﬁle F . The check operation returns
the decrypted block if it concludes that the block content
is authentic and ⊥ otherwise. A client can additionally
perform an append operation Append(B) for ﬁle F , in
which a new block that contains the encryption of B is
appended to the encrypted ﬁle, and a Delete operation
that deletes the last block in a ﬁle and updates the in-
tegrity information for the ﬁle.
Using the algorithms we have deﬁned for an integrity
scheme for an encrypted ﬁle, a client can read or write at
any byte offset in the ﬁle. For example, to write to a byte
offset that is not at a block boundary, the client ﬁrst reads
the block to which the byte offset belongs, decrypts it
and checks its integrity using algorithm Check. Then, the
client constructs the new data block by replacing the ap-
propriate bytes in the decrypted block, and calls Update
to encrypt the new block and compute its integrity infor-
mation.
184
16th USENIX Security Symposium
USENIX Association
In designing an integrity algorithm for a cryptographic
ﬁle system, we consider the following metrics. First is
the size of the untrusted storage USF ; we will always
enforce that the trusted storage TSF is of constant size,
independent of the number of blocks. Second is the in-
tegrity bandwidth for updating and checking individual
ﬁle blocks, deﬁned as the number of bytes from USF ac-
cessed (updated or read) when accessing a block of ﬁle
F , averaged over either: all blocks in F when we speak
of a per-ﬁle integrity bandwidth; all blocks in all ﬁles
when we speak of the integrity bandwidth of the ﬁle sys-
tem; or all blocks accessed in a particular trace when we
speak of one trace. Third is the performance cost of writ-
ing and reading ﬁles.
3 Preliminaries
3.1 Merkle Trees
Merkle trees [24] are used to authenticate n data items
with constant-size trusted storage. A Merkle tree for
data items M1, . . . , Mn, denoted MT(M1, . . . , Mn), is
a binary tree that has M1, . . . , Mn as leaves. An inte-
rior node of the tree with children CL and CR is the hash
of the concatenation of its children (i.e., h(CL||CR), for
h : {0, 1}∗ → {0, 1}s a second preimage resistant hash
function [29] that outputs strings of length s bits). If the
root of the tree is stored in trusted storage, then all the
leaves of the tree can be authenticated by reading from
the tree a number of hashes logarithmic in n.
We deﬁne the Merkle tree for a ﬁle F with n blocks
B1, . . . , Bn to be the binary tree MTF = MT(h(1||B1),
. . . , h(n||Bn)). A Merkle tree with a given set of leaves
can be constructed in multiple ways. We choose to ap-
pend a new block in the tree as a right-most child, so
that the tree has the property that all the left subtrees are
complete. We deﬁne several algorithms for a Merkle tree
T , for which we omit the implementation details, due to
space limitations.
- In the UpdateTree(R, i, hval) algorithm for tree T ,
the hash stored at the i-th leaf of T (counting from left
to right) is updated to hval. This triggers an update of
all the hashes stored on the path from the i-th leaf to the
root of the tree. It is necessary to ﬁrst check that all the
siblings of the nodes on the path from the updated leaf
to the root of the tree are authentic. Finally, the updated
root of the tree is output in R.
- The CheckTree(R, i, hval) algorithm for tree T
checks that the hash stored at the i-th leaf matches hval.
All the hashes stored at the nodes on the path from the
i-th leaf to the root are computed and the root of T is
checked ﬁnally to match the value stored in R.
- Algorithm AppendTree(R, hval) for tree T appends
a new leaf u that stores the hash value hval to the tree,
updates the path from this new leaf to the root of the tree
and outputs the new root of the tree in R.
- The DeleteTree(R) algorithm for tree T deletes the
last leaf from the tree, updates the remaining path to the
root of the tree and outputs the new root of the tree in R.
3.2 Encryption Schemes and Tweakable
Ciphers
An encryption scheme consists of a key generation algo-
rithm Gen that outputs an encryption key, an encryption
algorithm Ek(M ) that outputs the encryption of a mes-
sage M with secret key k and a decryption algorithm
Dk(C) that outputs the decryption of a ciphertext C with
secret key k. A widely used secure encryption scheme is
AES [2] in CBC mode [8].
A tweakable cipher [21, 15] is, informally, a length-
preserving encryption method that uses a tweak in both
the encryption and decryption algorithms for variability.
A tweakable encryption of a message M with tweak t
k(M ) and, similarly, the
and secret key k is denoted Et
decryption of ciphertext C with tweak t and secret key
k(C). The tweak is a public parameter,
k is denoted Dt
and the security of the tweakable cipher is based only on
the secrecy of the encryption key. Tweakable ciphers can
be used to encrypt ﬁxed-size blocks written to disk in a
ﬁle system. Suitable values of the tweak for this case
are, for example, block addresses or block indices in the
ﬁle. There is a distinction between narrow-block tweak-
able ciphers that operate on block lengths of 128 bits (as
regular block ciphers) and wide-block tweakable ciphers
that operate on arbitrarily large blocks (e.g., 512 bytes or
4KB). In this paper we use the term tweakable ciphers
to refer to wide-block tweakable ciphers as deﬁned by
Halevi and Rogaway [15].
The security of tweakable ciphers implies an interest-
ing property, called non-malleability [15], that guaran-
tees that if only a single bit is changed in a valid cipher-
text, then its decryption is indistinguishable from a ran-
dom plaintext. Tweakable cipher constructions include
CMC [15] and EME [16].
3.3 Efﬁcient Block Integrity Using Ran-
domness of Block Contents
Oprea et al. [26] provide an efﬁcient integrity construc-
tion in a block-level storage system. This integrity con-
struction is based on the experimental observation that
contents of blocks written to disk usually are efﬁciently
USENIX Association
16th USENIX Security Symposium
185
distinguishable from random blocks, i.e., blocks uni-
formly chosen at random from the set of all blocks of
a ﬁxed length. Assuming that data blocks are encrypted
with a tweakable cipher, the integrity of the blocks that
are efﬁciently distinguishable from random blocks can be
checked by performing a randomness test on the block
contents. The non-malleability property of tweakable
ciphers implies that if block contents after decryption
are distinguishable from random, then it is very likely
that the contents are authentic. This idea permits a re-
duction in the trusted storage needed for checking block
integrity: a hash is stored only for those (usually few)
blocks that are indistinguishable from random blocks (or,
in short, random-looking blocks).
An example of a statistical test IsRand [26] that
can be used to distinguish block contents from ran-
dom blocks evaluates the entropy of a block and con-
siders random those blocks that have an entropy higher
than a threshold chosen experimentally. For a block B,
IsRand(B) returns 1 with high probability if B is a uni-
formly random block in the block space and 0, other-
wise. Oprea et al. [26] provide an upper bound on the
false negative rate of the randomness test that is used in
the security analysis of the scheme.
We use the ideas from Oprea et al. [26] as a start-
ing point for our ﬁrst algorithm for implementing ﬁle
integrity in cryptographic ﬁle systems. The main chal-
lenge to construct integrity algorithms in our model is to
efﬁciently reduce the amount of trusted storage per ﬁle
to a constant value. Our second algorithm also exploits
the redundancy in ﬁle contents to reduce the additional
space for integrity, but in a different way, by embedding
a message authentication code (MAC) in ﬁle blocks that
can be compressed enough. Both of these schemes build
from a novel technique that is described in Section 4.1 for
efﬁciently tracking the number of writes to ﬁle blocks.
4 Write Counters for File Blocks
All the integrity constructions for encrypted storage de-
scribed in the next section use write counters for the
blocks in a ﬁle. A write counter for a block denotes the
total number of writes done to that block index. Coun-
ters are used to reduce the additional storage space taken
by encrypting with a block cipher in CBC mode, as de-
scribed in Section 4.2. Counters are also a means of dis-
tinguishing different writes performed to the same block
address and as such, can be used to prevent against replay
attacks.
We deﬁne several operations for the write counters of
the blocks in a ﬁle F . The UpdateCtr(i) algorithm ei-
ther initializes the value of the counter for the i-th block
in ﬁle F with 1, or it increments the counter for the i-
th block if it has already been initialized. The algorithm
also updates the information for the counters stored in
USF . Function GetCtr(i) returns the value of the counter
for the i-th block in ﬁle F . When counters are used to
protect against replay attacks, they need to be authenti-
cated with a small amount of trusted storage. For au-
thenticating block write counters, we deﬁne an algorithm
AuthCtr that modiﬁes the trusted storage space TSF of
ﬁle F to contain the trusted authentication information
for the write counters of F , and a function CheckCtr that
checks the authenticity of the counters stored in USF us-
ing the trusted storage TSF for ﬁle F and returns true
if the counters are authentic and false, otherwise. Both
operations for authenticating counters are invoked by an
authorized client.
4.1 Storage and Authentication of Block
Write Counters
A problem that needs to be addressed in the design of the
various integrity algorithms described below is the stor-
age and authentication of the block write counters. If a
counter per ﬁle block were used, this would result in sig-
niﬁcant additional storage for counters. Here we propose
a more efﬁcient method of storing the block write coun-
ters, based on analyzing the ﬁle access patterns in NFS
traces collected at Harvard University [9].
Counter intervals. We performed experiments on the
NFS Harvard traces [9] in order to analyze the ﬁle access
patterns. We considered three different traces (LAIR,
DEASNA and HOME02) for a period of one week. The
LAIR trace consists of research workload traces from
Harvard’s computer science department. The DEASNA
trace is a mix of research and email workloads from the
division of engineering and applied sciences at Harvard.
HOME02 is mostly the email workload from the campus
general purpose servers.
Ellard et al. [9] make the observation that a large num-
ber of ﬁle accesses are sequential. This leads to the idea
that the values of the write counters for adjacent blocks
in a ﬁle might be correlated. To test this hypothesis, we
represent counters for blocks in a ﬁle using counter in-
tervals. A counter interval is deﬁned as a sequence of
consecutive blocks in a ﬁle that all share the same value
of the write counter. For a counter interval, we need to
store only the beginning and end of the interval, and the
value of the write counter.
Table 1 shows the average storage per ﬁle used by the
two counter representation methods for the three traces.
We represent a counter using 2 bytes (as the maximum
186
16th USENIX Security Symposium
USENIX Association
observed value of a counter was 9905) and we repre-
sent ﬁle block indices with 4 bytes. The counter inter-
val method reduces the average storage needed for coun-
ters by a factor of 30 for the LAIR trace, 26.5 for the
DEASNA trace and 7.66 for the HOME02 trace com-
pared to the method that stores a counter per ﬁle block.
This justiﬁes our design choice to use counter inter-
vals for representing counter values in the integrity al-
gorithms presented in the next section.
LAIR
DEASNA
HOME02
Counter per block
Counter intervals
547.8 bytes
18.35 bytes
1.46 KB
3.16 KB
55.04 bytes
413.44 bytes
Table 1: Average storage per ﬁle for two counter repre-
sentation methods.
Counter representation. The counter intervals for ﬁle