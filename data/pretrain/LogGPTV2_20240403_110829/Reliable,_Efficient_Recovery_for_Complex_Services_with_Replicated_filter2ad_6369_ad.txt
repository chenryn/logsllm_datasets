Vr. This ensures that no node can begin acting on Vr until all
of the updates committed by ET are fully replicated.
B. Tolerance of Failure of the Leader
Much of our restart protocol seems to depend on correct
operation of the restart leader, but in fact it can tolerate the
failure of the restart leader: a subsequent restart leader would
always select a state that is a safe extension of the state of
the original leader (in fact it will be the identical state if the
original leader’s proposal might have been acted upon, and
otherwise will be a safe choice with respect to the state the
system was in when it crashed). One caveat is that our solution
is correct only with a single leader running at a time. Since no
fault-tolerant conﬁguration management system is yet in place
while the system is restarting, choosing a restart leader with
an election protocol would be quite difﬁcult. However, a small
amount of manual conﬁguration can to be used both to choose
the initial leader and to select one to take over if the initial one
fails. This can be accomplished by, for example, specifying
both a default restart leader and an ordered list of fallback
restart leaders in a conﬁguration ﬁle. Handling the failure
of the leader in an efﬁcient manner may also require some
manual intervention, speciﬁcally in the case where the leader
fails during the await-quorum loop, because non-leader nodes
can expect to wait a rather long time for the leader to reach a
quorum (depending on how long it takes nodes in the system
to restart after a total crash). They can eventually conclude that
the leader has failed if it does not send ET after a suitably long
timeout, but the restart process can complete faster if a system
administrator or other outside process forcibly restarts them if
the leader fails while awaiting a quorum. Failures of the leader
during the 2-phase commit are easier to detect, because the
leader should send the prepare and commit messages shortly
after sending the ragged trim information, so the non-leader
nodes can safely use much shorter timeouts on these messages.
When non-leader nodes detect that the leader has failed, they
restart the recovery process using the new restart leader. This
means that the new restart leader receives all of the same view,
epoch-termination, and update-log information as the previous
restart leader, and will reach the same conclusions. It will still
wait for majority of members of each view it discovers to
restart, meaning it must discover the last known view before
it concludes that it has a restart quorum. If the previous restart
leader was in fact required to achieve a quorum (because, for
example, it was the only member of some shard in V(cid:2)), then
the new restart leader must wait for it to restart and rejoin the
system as a non-leader.
C. Correctness of Node Assignment to Shards
Next, we prove that our min-cost ﬂow algorithm ﬁnds a
node assignment that satisﬁes each shard’s required number of
nodes from different failure correlation sets, given that a node
assignment exists that obeys this constraint. We also show that
our algorithm is optimal, generating an assignment where a
minimal number of nodes are moved to shards they were not
previously a part of. Thus, we minimize time spent on state
transfer between old and new members of each shard.
We prove correctness by reduction to min-cost ﬂow. Our
solution is correct if it ﬁnds a feasible node assignment given
that one exists. Given capacities of edges from the source
vertex to shard vertices, shard vertices to failure correlation
set vertices, and failure correlation set vertices to the sink
vertex, any feasible ﬂow in the graph can be translated into
a feasible assignment of nodes to shards. Each shard receives
exactly the number of nodes from different failure correlation
sets it requires, because that is the capacity of the edge from
the source to the shard vertex. No node from any failure
correlation set is assigned to more than one shard, because
the capacity of the edge from the failure correlation set vertex
to the sink is number of nodes in that failure correlation set.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
179
fresh start
restart
shard size 2
shard size 3
3500
3000
2500
2000
1500
1000
500
)
s
d
n
o
c
e
s
i
l
l
i
m
(
e
m
T
i
15000
12500
10000
7500
5000
2500
0
)
s
e
t
y
B
n
i
(
e
z
i
S
leader
non-leader
sent
received
shard size 2
shard size 3
2
3
4
5
6
7
8
9
10
11
12
2
3
4
5
6
7
8
9
10
11
12
Total nodes
Total nodes
Fig. 2: Total time to start or restart a service. Error bars
represent 1 standard deviation.
Fig. 3: Total metadata sent/received during the restart process.
All nodes assigned to any one shard are from different failure
correlation sets, because the capacity of the edges from shard
vertices to failure correlation set vertices is always 1. Thus a
solution to min-cost ﬂow is a solution to the node assignment
problem. In fact, any solution to the node assignment problem
can also be translated into a ﬂow.
Furthermore, the solution to min-cost ﬂow represents an
optimal node assignment. We deﬁned optimality above; a
solution is optimal if it minimizes the number of nodes whose
shard membership changes. By deﬁnition, the solution to min-
cost ﬂow is a ﬂow that minimizes the cost along all its edges.
Costs along edges are 0 except for edges from shard vertices
to failure correlation set vertices, where no member from the
failure correlation set belonged in the shard in the previous
view. That is the deﬁnition of optimality. Thus any solution to
min-cost ﬂow is optimal.
Note that we opted to reduce to min-cost ﬂow, which
can be solved in polynomial time, instead of integer linear
programming, which can be used to satisfy more generic
constraints but might not ﬁnd a solution efﬁciently.
D. Efﬁciency and Generality
Our restart protocol is designed for a particular form of state
machine replication (the one implemented by Derecho), which
allows us to take advantage of some efﬁciencies built into this
SMR protocol. Speciﬁcally, Derecho’s SMR enforces a read
quorum of 1 within each shard, which means that reading the
log of one up-to-date replica is sufﬁcient to learn the entire
committed state of that shard. Thus, the restart quorum only
requires a single member of each shard from the last known
view, and when new or out-of-date replicas are added to a
shard during restart, they only need to contact and transfer state
from a single up-to-date member. Furthermore, uncommitted
updates only occur at the tail of a log, and there are no “holes”
in the committed preﬁx of the log because updates are only
aborted during a reconﬁguration (which also trims them from
the log). This allows us to easily make the correct decision
about whether to accept these updates during recovery: they
can safely be committed unless a logged epoch termination
decision is found that proves they will be aborted.
Nevertheless, our protocol could be applied to other forms
of SMR with a few relaxations of these optimizations. For
example, a read quorum > 1 would merely increase the size
of the restart quorum, as long as reconﬁguration was still
handled via virtual synchrony. In a system with a per-shard
read quorum of ri, the restart leader would need to contact
at least ri members of shard i in the current view in order
to ensure it found both the next view (if one exists) and the
longest sequence of committed updates in shard i; the restart
quorum would include a read quorum of every shard in the last
known view. Any nodes added to a shard in the restart view
would also need to contact all the members of the most-recent
read quorum in order to complete state transfer.
Some SMR systems, such as vCorfu [8], separate conﬁg-
uration information from the replicated state itself, using a
separate “layout” service and “data” service. In this case, our
protocol would need to explicitly separate step 1 (ﬁnding the
last conﬁguration) from step 3 (ﬁnding the longest log), rather
than executing them concurrently. The restart leader would
ﬁrst need to contact a quorum of the layout service in order to
ﬁnd the last active conﬁguration, then use that conﬁguration
to compute and wait for a restart quorum of the data service.
V. EXPERIMENTS
We have implemented our restart algorithm as part of the
Derecho library, and in this section we measure its per-
formance when restarting sample Derecho applications. All
experiments were carried out on our local cluster, which
contains 12 servers running Ubuntu 16.04, using SSD disks for
storage. In summary, we found that our recovery logic scales
well, and adds only a small delay compared to the costs of
process launches and initial Derecho platform setup.
Our ﬁrst experiment was a straightforward end-to-end
benchmark. We used our algorithm to restart a simple Derecho
service with a single subgroup and shards of 2 or 3 nodes
each, after an abrupt crash in which all nodes failed near
the same time, and measured the time from when the restart
leader launched to when the ﬁrst update could be sent in the
recovered service. For comparison, we also measured the time
required to start a fresh instance of the same service, with no
logged state to recover. Figure 2 shows the results.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
180
Quorum
Truncate
Transfer
Commit
Setup
s
e
t
a
d
p
U
g
n
i
s
s
i
M
&
s
e
d
o
N
9 nodes,
10000 updates
9 nodes,
1000 updates
6 nodes,
10000 updates
6 nodes,
1000 updates
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
Cumulative Time (ms)
Fig. 4: Breakdown of time spent in each phase of starting or restarting a service, when 1 node per shard is out of date upon
restart. Upper bars show fresh start, lower bars show restart.
We ﬁnd that the restart algorithm adds only minimal over-
head compared to the fresh-start case, and that the assignment
of nodes into more or fewer shards does not have a noticeable
effect on restart time, owing to the polynomial run time of
min-cost ﬂow. In both cases, the time to launch the service
increases as the system scales up due to the ﬁxed costs of
initializing more distributed processes. For example, there is
an increasingly variable delay in the time it takes each server to
actually start the Derecho process after being given a command
to do so.
Next, we measured the amount of metadata that was ex-
changed between the restart leader and the non-leader nodes
in order to complete the restart algorithm, using the same setup
as the experiment in Figure 2. (Metadata includes everything
sent during the restart process except for the missing updates
sent during state transfer). In Figure 3, we see that the restart
leader sends and receives more metadata as the size of the
overall group increases, increasing at an approximately linear
rate. This is because the restart leader must contact every
restarting node, both to receive its logged information and to
send out the proposed restart view. However, the non-leader
nodes exchange a nearly-constant amount of data regardless
of the size of the group, since they only need to contact the
leader and wait for its response. Note, also, that even at the