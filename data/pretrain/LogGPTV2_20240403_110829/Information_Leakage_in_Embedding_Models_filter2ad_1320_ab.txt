These pre-trained language models can also be used to extract
sentence embeddings. There are many different ways to extract
sentence-level features with pre-trained language models. In this
work, we follow Sentence-BERT [58] which suggests that mean-
pooling on the hidden representations yields best empirical perfor-
mance.
2.2 Threat Model and Attack Taxonomy
In this section, we give an overview of threat models we consider in
this paper and describe a taxonomy of attacks leaking information
from embedding models. Figure 1 shows an overview of the attack
taxonomy.
In many NLP applications, embedding vectors are often com-
puted on sensitive user input. Although existing frameworks pro-
pose computing embeddings locally on user’s device to protect the
privacy of the raw data [6, 36, 50, 71], the embeddings themselves
are shared with machine learning service providers or other parties
for downstream tasks (training or inference). It is often tempting
to assume that sharing embeddings might be “safer” than sharing
the raw data, by their nature of being “simply a vector of real num-
bers.” However, this ignores the information that is retained in, and
may be extracted from embeddings. We investigate the following
questions: What kinds of sensitive information about the inputs are
encoded in the embeddings? And can an adversary with access to the
embeddings extract the encoded sensitive information?
Threat model. Our threat model comprises the following en-
tities. (1) Dtrain is a training dataset that may contain sensitive
information. (2) Φ, the embedding model, which might be avail-
able in a white-box or black-box fashion. White-box access to the
model reveals the model architecture and all parameters. Black-
box access allows anyone to compute Φ(x) for x of their choice.
(3) Etarget = {Φ(x∗
i )}, a set of embedding vectors on sensitive in-
puts x∗
. (4) Daux is an auxiliary dataset available to the adversary
i
comprising of either limited labeled data drawn from the same dis-
tribution as Dtrain or unlabeled raw text data. In the text domain,
unlabeled data is cheap to collect due to the enormous amount free
text available on the Internet while labeling them is often much
more expensive.
An adversary, given access to some of the entities described
above, aims to leak some sensitive information about the input x
from the embedding vector Φ(x). By looking at variants of what
information an adversary possesses and their target, we arrive at
three broad classes of attacks against embeddings.
With sensitive input data such as personal messages, it is natural
to consider an adversary whose goal is to (partially) recover this
text from Φ(x). Even if the input data is not sensitive, they may
be associated with sensitive attributes, and an adversary could be
tasked with learning these attributes from Φ(x). And finally, rather
than recovering inputs, the adversary given some information about
Figure 1: Taxonomy of attacks against embedding models. We assume adversary has access to the embedding Φ(x∗) of a sen-
sitive input text x∗ that will be used for downstream NLP tasks, and perform three information leakage attacks on Φ(x∗): (1)
inverting the embedding back to the exact words in x∗, (2) inferring sensitive attribute of x∗, and (3) inferring the membership,
i.e. whether x∗ and its context x′ has been used for training.
Φ(x) can aim to find if x was used to train Φ or not. These are
formalized below.
Embedding inversion attacks. In this threat model, the adver-
sary’s goal is to invert a target embedding Φ(x∗) and recover words
in x∗. We consider attacks that involve both black-box and white-
box access to Φ. The adversary is also allowed access to an unlabeled
Daux and in both scenarios is able to evaluate Φ(x) for x ∈ Daux.
Sensitive attribute inference attacks. In this threat model, the
adversary’s goal is to infer sensitive attribute s∗ of a secret input
x∗ from a target embedding Φ(x∗). We assume the adversary has
access to Φ, and a set of labeled data of the form (x, s) for x ∈
Daux. We focus on discrete attributes s∗ ∈ S, where S is the set
of all possible attribute classes, and an adversary performing the
inference by learning a classifier f on Daux. Given sufficient labeled
data , the adversary’s task trivially reduces to plain supervised
learning, which would rightly not be seen as adversarial. Therefore,
the more interesting scenario, which is the focus of this paper, is
when the adversary is given access to a very small set of labeled data
(in the order of 10–50 per class) where transfer learning from Φ(x)
is likely to outperform supervised learning directly from inputs x.
Membership inference attacks. Membership inference against
ML models are well-studied attacks where the adversary has a
target data point x∗ and the goal is to figure out with good accu-
racy whether x∗ ∈ Dtrain or not. Unlike previous attacks focused
on supervised learning, some embedding models, such as word
embeddings, allow you to trivially infer membership. For word
embeddings, every member of a vocabulary set is necessarily part
of Dtrain. Instead, we expand the definition of training data mem-
bership to consider this data with their contexts, which is used in
training. We assume the adversary has a target context of word
[w∗
n] and access to V for word embedding, or a context of
target sentences (x∗
b) and access to the model Φ for sentence
embedding, and the goal is to decide the membership for the con-
text. We also consider the target to be an aggregated level of data
sentences [x∗
n] comprising multiple contexts for the adver-
sary to determine if it were part of training Φ. Membership privacy
for aggregation in the user level has also been explored in prior
works [44, 67].
1, ... , w∗
1, ... , x∗
a, x∗
We further assume adversary has a limited Daux labeled with
membership. We propose that this is a reasonable and practical
assumption as training data for text embeddings are often collected
from the Internet where an adversary can easily inject data or get
access to small amounts of labeled data through a more expen-
sive labeling process. The assumption also holds for adversarial
participants in collaborative training or federated learning [43, 44].
3 EMBEDDING INVERSION ATTACKS
The goal of inverting text embeddings is to recover the target input
texts x∗ from the embedding vectors Φ(x∗) and access to Φ. We focus
on inverting embeddings of short texts and for practical reasons, it
suffices to analyze the privacy of the inputs by considering attacks
that recover a set of words without recovering the word ordering.
We leave open the problem of recovering exact sequences and one
promising direction involves language modeling [64].
A naïve approach for inversion would be enumerating all pos-
sible sequences from the vocabulary and find the recovery ˆx such
that Φ(ˆx) = Φ(x∗). Although such brute-force search only requires
black-box access to Φ, the search space grows exponentially with
the length of x and thus inverting by enumerating is computation-
ally infeasible.
The brute-force approach does not capture inherently what in-
formation might be leaked by Φ itself. This naturally raises the
question of what attacks are possible if the adversary is given com-
plete access to the parameters and architecture of Φ, i.e., white-box
access. This also motivates a relaxation technique for optimization-
based attacks in Section 3.1. We also consider the more constrained
black-box access scenario in Section 3.2 where we develop learning
based attacks that are much more efficient than exhaustive search
by utilizing auxiliary data.
3.1 White-box Inversion
In a white-box scenario, we assume that the adversary has access
to the embedding model Φ’s parameters and architecture. We for-
mulate white-box inversion as the following optimization problem:
(3)
where X(V) is the set of all possible sequences enumerated from
the vocabulary V. The above optimization can be hard to solve
||Φ(ˆx) − Φ(x
min
ˆx ∈X(V)
∗)||2
2
Algorithm 1 White-box inversion
1: Input: target embedding Φ(x∗), white-box embedding model
Φ with lower layer representation function Ψ, temperature T ,
sparsity threshold τsp, auxiliary dataset Daux
2: Query function Φ and Ψ with Daux
and collect
{(Φ(xi), Ψ(xi))|xi ∈ Daux}.
3: Train a linear mapping M by minimizing ||M(Φ(xi)) − Ψ(xi)||2
2
on {(Φ(xi), Ψ(xi))}i.
4: if Ψ is mean-pooling on word embedding V of Φ then
Initialize z ∈ R|V|
5:
while objective function of Eq 7 not converged do
6:
7:
8:
9:
10: else
11:
12:
13:
14:
return ˆx = {wi|zi ≥ τsp}|V|
i =1
Initialize Z = [z1, ... , zℓ] ∈ Rℓ×|V|
while objective function of Eq 6 not converged do
Update Z with gradient of Eq 6.
return ˆx = {wi|i = arg max zj}ℓ
j=1
Update z with gradient of Eq 7.
Project z to non-negative orthant.
directly due to the discrete input space. Inspired by prior work
on relaxing categorical variables [29], we propose a continuous
relaxation of the sequential word input that allows more efficient
optimization based on gradients.
ˆvi = V
The goal of the discrete optimization in Equation 3 is to select
a sequence of words such that the distance between the output
embeddings is minimized. We relax the word selection at each
position of the sequence with a continuous variable zi ∈ R|V|.
As mentioned before, Φ first maps the input x of length ℓ into a
sequence of word vectors X = [v1, ... , vℓ] and then computes the
text embedding based on X. For optimizing zi, we represent the
selected word vectors ˆvi using a softmax attention mechanism:
⊤ · softmax(zi/T)
for i = 1, ... , ℓ
(4)
where V is the word embedding matrix in Φ and T is a tempera-
ture parameter. The softmax function approximates hard argmax
selection for T  τsp for a sparsity
threshold hyper-parameter τsp.
3.2 Black-box Inversion
In the black-box scenario, we assume that the adversary only has
query access to the embedding model Φ, i.e., adversary observes the
output embedding Φ(x) for a query x. Gradient-based optimization
is not applicable as the adversary does not have access to the model
parameters and thus the gradients.
Instead of searching for the most likely input, we directly ex-
tract the input information retained in the target embedding by
formulating a learning problem. The adversary learns an inversion
model ϒ that takes a text embedding Φ(x) as input and outputs the
set of words in the sequence x. As mentioned before, our goal is
to recover the set of words in the input independent of their word
ordering. We denote W(x) as the set of words in the sequence
x. The adversary utilizes the auxiliary dataset Daux and queries
the black-box Φ and obtain a collection of (Φ(x),W(x)) for each
x ∈ Daux. The adversary then trains the inversion model ϒ to
maximize log Pϒ(W(x)|Φ(x)) on the collected set of (Φ(x),W(x))
values. Once ϒ is trained, the adversary predicts the words in the
target sequence x∗ as ϒ(Φ(x∗)) for an observed Φ(x∗).
Multi-label classification. The goal is to predict the set of words
in sequence x given the embedding Φ(x). A common choice for
such a goal is to build a multi-label classification (MLC) model,
where the model assigns a binary label of whether a word is in the
set for each word in the vocabulary. The training objective function
is then:
[yw log( ˆyw) + (1 − yw) log(1 − ˆyw)]
(8)
LMLC = − 
w ∈V
where ˆyw = Pϒ(yw |Φ(x)) is the predicted probability of word w
given ϒ conditioned on Φ(x) and yw = 1 if word w is in x and 0
otherwise.
Multi-set prediction. One drawback in the above multi-label
classification model is that the model predicts the appearance of
each word independently. A more sophisticated formulation would
be predicting the next word given the current predicted set of
words until all words in the set are predicted. We adopt the multi-
set prediction loss [72] (MSP) that is suited for the formulation. Our
MSP model trains a recurrent neural network that predicts the next
word in the set conditioned on the embedding Φ(x) and current
predicted set of words. The training objective is as follows:
ℓ
i =1
1
|Wi|

w ∈Wi
LMSP =
− log Pϒ(w|W<i , Φ(x))
(9)
where Wi is the set of words left to predict at timestamp i and