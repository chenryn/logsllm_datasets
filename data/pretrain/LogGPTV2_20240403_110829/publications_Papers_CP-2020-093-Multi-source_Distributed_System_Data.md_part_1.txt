Multi-Source Distributed System Data for
AI-powered Analytics
Sasho Nedelkoski∗, Jasmin Bogatinovski∗, Ajay Kumar Mandapati∗, Soeren Becker∗, Jorge Cardoso†, Odej Kao∗
∗Complex and Distributed IT-Systems Group, TU Berlin, Berlin, Germany
Email: {nedelkoski, firstname.lastname}@tu-berlin.de
†Huawei Munich Research Center, Munich, Germany
Department of Informatics Engineering/CISUC, University of Coimbra, Portugal
Email: PI:EMAIL
Abstract—Inrecentyearstherehasbeenanincreasedinterest of the infrastructure, typically regarding CPU, memory, disk,
in Artificial Intelligence for IT Operations (AIOps). This field network throughput, and service call latency. Application logs
utilizes monitoring data from IT systems, big data platforms,
enable developers to record what actions were executed at
and machine learning to automate various operations and
runtimebysoftware.Service,microservices,andothersystems
maintenance (O&M) tasks for distributed systems. The major
contributions have been materialized in the form of novel generate logs which are composed of timestamped records
algorithms.Typically,researcherstookthechallengeofexploring with a structure and free-form text. Distributed traces record
onespecifictypeofobservabilitydatasources,suchasapplication the workflows of services executed in response to requests,
logs, metrics, and distributed traces, to create new algorithms.
e.g., HTTP or RPC requests. The records contain information
Nonetheless, due to the low signal-to-noise ratio of monitoring
abouttheexecutiongraphandperformanceata(micro)service
data, there is a consensus that only the analysis of multi-
source monitoring data will enable the development of useful level.
algorithmsthathavebetterperformance.Unfortunately,existing Recently,variousapproaches–focusingonawiderangeof
datasets usually contain only a single source of data, often logs datasets, O&M tasks, and IT systems – have been proposed.
or metrics. This limits the possibilities for greater advances This includes tasks, such as anomaly detection and root-
in AIOps research. Thus, we generated high-quality multi-
cause analysis, which process a specific type of observability
sourcedatacomposedofdistributedtraces,applicationlogs,and
data. For example, anomaly detection has been applied to
metrics from a complex distributed system. This paper provides
detaileddescriptionsoftheexperiments,statisticsofthedata,and metrics [5]–[7], logs [8]–[12], and also to distributed system
identifies how such data can be analyzed to support O&M tasks traces [13], [14].
suchasanomalydetection,rootcauseanalysis,andremediation. The existing research has mainly explored data capturing
Thedataisavailableathttps://doi.org/10.5281/zenodo.3484800.
only a single data source category. This limits both the
Index Terms—AIOps, dataset, anomaly detection, root-cause
development of new multi-source (or multimodal) methods
analysis,observability,applicationlogs,metrics,distributedtrac-
ing. and their proper evaluation. The absence of observability data
capturing the three data categories prevents the development
I. INTRODUCTION of methods for fault detection, root-cause analysis, and re-
The term AIOps refers to multi-layered technology plat- mediation that could give advances in the field as existing
forms that automate and enhance IT operations by using approachestypicallyproducealargenumberoffalsepositives.
analytics and machine learning [1]. Thebenefitsofusingseveraldatasourcecategoriesarerelated
AIOps was introduced to make O&M tasks of ever- to the concept of triangulation [15] from the field of social
increasing complex public, private, edge, mobile, and hybrid sciences, which refers that the application and combination
cloud environments effective and cost-efficient. The transition of several observers, methods, and empirical materials can
from mainframes, to virtual machines, to containers, and overcome the weaknesses and limitations that come from
serverless computing made existing approaches and tools single observers and single methods.
which rely on simple statistical methods obsolete due to the In following we summarize the contributions of this work:
increasing complexity and communication patterns between • A new data of metrics, logs, and traces generated by a
services. Notable examples include Zabbix, Cacti, and Na- distributed system based on microservice architecture.
gios [2], [3]. • Description of the approach developed to generate the
MonitoringdataisakeyelementofnewAIOpstools,butit multi-source observability data and its statistics.
is also one of the cornerstones of research. The observability • Analysis of existing datasets utilized for the evaluation
data generated by distributed IT systems can be classified ofAIOpsalgorithms,highlightingtheirbenefitsandtheir
into three main categories: metrics, application logs, and limitations.
distributed traces [4]. Metrics are numeric values measured • Applications of the multi-source data to develop new
over a period of time. They describe the utilization and status algorithms to support additional O&M tasks.
II. RELATEDWORK tracing data. They proposed a multimodal neural network
with long short-term memory (LSTM) to enable the learning
Traces,logs,andmetricsareimportantdatasourcesthatare from the sequential nature in the tracing data. They describe
fundamental to the operation of complex distributed systems. how the data is obtained, but the datasets are not publicly
Themetricdataisacommonwaytoextractusefulinformation available. Azure Public dataset composes of two datasets
for describing the state of the system. However, it is not representing two representative traces of the virtual machine
sufficient for a holistic approach aiming to model the whole of Microsoft Azure [25]. It is mostly utilized to improve
system. The metrics data are obtained from monitoring of the resource management in large cloud platforms. Alibaba’s
resourcessuchasCPU,memory,diskandnetworkthroughput cluster data is a collection of two datasets from real-world
and latency. A plethora of available collections of datasets production[26]–[28].InZhenetal.[26]anovelsystemwhich
containing metric data can be found in Stonybrook [16], automatically diagnoses stragglers for jobs is introduced. It
where multiple datasets for different tasks related to anomaly utilizes unsupervised clustering methods to group the tasks
detection can be found. Numenta [17] predominantly contains based on their execution time. Then a supervised learning
datasetsfromstreamingandreal-timeapplications,whileHar- method is used to diagnose rules for diagnosing the stragglers
vard [18], ELKI [19], LMU [20] store network intrusion data. and their adequate resource assignment data. Li et al. [27]
Recently,therearemultiplestudieswhichutilizethesedatasets propose a deep reinforcement learning approach towards the
for anomaly detection, root-cause analysis, and remediation. job scheduling task. It can automatically obtain a fitness
In Subutai et al. [17], a novel anomaly detection method calculation method that optimizes the throughput of a set
based on hierarchical temporal memory (HTM) is introduced. of jobs from experience. Google’s collection of two tracing
Itenablesanomalydetectioninthestreamingsettingtotackle datasets originates from parts of Google cluster management
the problems of concept drift and the problem of multiple software and systems [29]. In Elsayed et al. [30] effective
streaming sources utilizing metrics data. In Schmidt et al. [5], predictors of job and task terminations are identified. The
an unsupervised anomaly detection framework is developed knowledge of job config parameters known at launch time
and applied to real-time monitoring data in a distributed is discovered to be sufficient to predict whether a job is
environment. goingtobekilled.Furthermore,variousmethodsformitigating
The main challenge that AIOps systems analyzing log failuresinclustersareproposed,includinglimitingthenumber
data are facing is the unstructured nature of the logs. This of retries, increasing frequency of checkpoints, scheduling
problemusuallyrequirespriorandproperpreprocessingand/or redundant tasks, adjusting scheduling priority or allocated
inclusionofdomainknowledge.Often,approachesextractlog resources and turning on additional monitoring.
key identifiers for the logs and are modeling their sequences. Limitation for all the above-mentioned datasets is the ab-
There exist two resources of log data for cluster systems sence of multi-view data describing a single system. The lack
available. The CFDR resource [21] stores links or 19 log of data from all observability components from one system
datasets grouped in 11 data collections. The datasets cover does not allow the development of holistic systems for fault
both hardware and software logs. The second resource is the detection, root-cause analysis and remediation that consider
loghubdataresource[22].Itconsistsof16datasetsdescribing multiplesourcesofdatasimultaneously.Ourcollectionofdata,
systems spanning across distributed systems, supercomputers, describing the same system from the 3 perspectives of logs,
operating systems, mobile systems, server applications and metricise and traces, to the best of our knowledge, is the
standalone software. The datasets cover a different time from first of its kind. This enables building models with diverse
a few days until a few months. From the perspective of the complementary information, hence making AIOps systems to
system description, these data have weakness in providing perform better [13].
just a single aspect of the system. In Meng et al. [23]
the LogAnomlay system for detection of anomalies from III. DATASETGENERATOR
logs is introduced. It utilizes a novel template2vec technique In this section, we describe the infrastructure, experiments,
to encode the logs. Further, it extracts quantitative patterns workload,andtheinjectedfaultsaspartofthedatageneration.
from the logs. It uses LSTMs to detect the sequential and
quantitative anomalies in the logs. In Zheng et al. [10] the A. Infrastructure
DeepLog system is introduced. It tries to model the logs as For the generation of the data, we deployed an Open-
natural language sequences. It allows to update the model by Stack [31] testbed based on a microservice architecture
the operator and provides an automatic reconstruction of the that is running in a dockerized environment called Kolla-
workflows to enable root cause analysis. Ansible [32]. OpenStack is a cloud operating system that
In microservice architectures, traces are graph-like struc- controlslargepoolsofcomputing,storage,andnetworkingre-
tures composed of events or spans [24]. The traces represent sourcesthroughoutadata-centre,allmanagedandprovisioned
the system execution workflow, hence detailed information through APIs with common authentication mechanisms.
for individual services and the causal relationship to other The testbed setup is shown in Figure 1 and consists of one
related services can be inferred. Nedelkoski et al. [13], [14] control node named wally-113 and four compute nodes:
introduce novel anomaly detection methods for distributed wally-122,wally-123,wally-124,andwally-117.
It was deployed on bare-metal nodes of a cluster where each image and create and delete network, respectively. The faults
nodehasRAM16GB,3x1TBofdisks,and2x1GbitEthernet were injected at different rates, 250 for create and delete
NIC. Three hard disks were combined to a software RAID 5 server and create and delete image and 500 iterations for
for data redundancy. create and delete network. The number of the iterations for
each action was chosen so that all workloads approximatelly
B. Workloads and faults injected
finish in the same time. The data from the second experiment
To generate workloads and inject faults into the infras-
is slightly more suited for multi-source methods utilizing dis-
tructure we used Rally [33]. It automates and unifies multi-
tributedlogdata,asitwasgeneratedwiththatasagoal.Also,
node OpenStack deployment, cloud verification, testing and
HTML reports were collected which correlates all the events
profiling. Rally does it generically, making it possible to
of creations, failures and which injections were made. This
check whether OpenStack is going to work well on various
report serves as ground truth for the normal and anomalous
configurations and installations under high workloads.
state of the system.
Rally docker image was used to create the load and inject
os-faults [34] appropriately. We selected a list of workloads C. Data collection
and faults that are close representatives to real production In following we describe the technologies and the methods
faults.Thelistedworkloadsandfaultsinfollowingcoveruser used to collect the generated data.
request that is served by the main Openstack projects. 1) Metrics: For the metrics collection across the physical
• Createanddeleteserver.Createsanddeletesaserverus- nodes in the infrastructure, we utilize Glances [35], a cross-
ingtheRallytask(NovaServers.boot and delete server). platform monitoring tool which aims to present a maximum
Nova project is mostly affected and present in the data. of information into a minimal space through curses or Web-
We injected a compute fault which is restarting the api based interface. Glances is written in Python and uses the
container that run on the compute nodes. psutil library to get information from a system. It can
• Create and delete image. Rally task (GlanceIm- adapt dynamically the displayed information depending on
ages.create and delete image) for images accepts the the terminal size. It can also work in client/server mode, also
image-location locally/ over the internet , format of the remote monitoring could be done via terminal, Web interface
output image once created. It creates and deletes an im- or API (XMLRPC and RESTful). Glances was used to gather
age. The glance project of Openstack provides a service information such as CPU, MEM and load of the machine
where users can upload and discover data assets that are (either controller or the compute nodes) with a frequency of
meant to be used with other services. Here we inject the 0.1 seconds. These metrics were saved into a CSV file via the
faultintheglance-apirunningonthecontrollernode. glances-cli.
• Create and delete network. Rally provides task 2) Logs: OpenStack services use standard logging levels.
(NeutronNetworks.create and delete networks) the For aggregating logs from all services running across the
format from creating and deletion of networks for physical nodes, was used ELK (Elasticsearch, Logstash, and
various configurations such as multiple users and Kibana). Elasticsearch is a search and analytics engine which
tenants. Neutron is an OpenStack project to provide resolves the search requests. Logstash is a serverside data
networking as a service between interface devices processing pipeline that ingests data from multiple sources si-
(e.g., vNICs) managed by other Openstack services multaneously,transformsit,andthensendsittoElasticsearch.
(e.g., nova, heat etc). There are various components For this Fluentd, which is an open-source data collector for
that we focus on while injecting faults such as the unified logging layer, was utilized. It allows unifying data
disrupting the below-mentioned services running in collection and consumption for better use and understanding
docker containers: neutron_metadata_agent, of data. Kibana is a dashboard that gives the ability to the
neutron_l3_agent, neutron_dhcp_agent, users to visualize data with charts and graphs using data that
neutron_openvswitch_agent and is collected by Elasticsearch. Finally, for exporting data from
neutron_server. Elasticsearch into CSV a CLI tool, es2csv [36] was utilized.
We performed two different experiments. In the first ex- The benefit we obtain from this tool is that it can query
periment, the user actions as a workload were executed in a bulk docs in multiple indices and get only selected fields,