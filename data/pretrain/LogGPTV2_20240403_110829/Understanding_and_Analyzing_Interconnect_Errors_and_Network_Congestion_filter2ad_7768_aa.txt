title:Understanding and Analyzing Interconnect Errors and Network Congestion
on a Large Scale HPC System
author:Mohit Kumar and
Saurabh Gupta and
Tirthak Patel and
Michael Wilder and
Weisong Shi and
Song Fu and
Christian Engelmann and
Devesh Tiwari
Understanding and Analyzing Interconnect Errors
and Network Congestion on a Large Scale HPC
System
Mohit Kumar♮, Saurabh Gupta†, Tirthak Patel♦, Michael Wilder◦
Weisong Shi♮, Song Fu§, Christian Engelmann‡, and Devesh Tiwari♦
♮Wayne State University
†Intel Labs
◦UT Knoxville
§University of North Texas
‡Oak Ridge National Laboratory
♦Northeastern University
Abstract—Today’s High Performance Computing (HPC) sys-
tems are capable of delivering performance in the order of
petaﬂops due to the fast computing devices, network intercon-
nect, and back-end storage systems. In particular, interconnect
resilience and congestion resolution methods have a major impact
on the overall interconnect and application performance. This
is especially true for scientiﬁc applications running multiple
processes on different compute nodes as they rely on fast
network messages to communicate and synchronize frequently.
Unfortunately, the HPC community lacks state-of-practice expe-
rience reports that detail how different interconnect errors and
congestion events occur on large-scale HPC systems. Therefore,
in this paper, we process and analyze interconnect data of the
Titan supercomputer to develop a thorough understanding of
interconnects faults, errors and congestion events. We also study
the interaction between interconnect, errors, network congestion
and application characteristics.
Index Terms—Cray, Gemini, Interconnect, Titan, Errors
and congestion resolution mechanisms have a major impact
on the overall interconnect and application performance.
Unfortunately, the HPC community lacks state-of-practice
experience reports that detail how different interconnect errors
and congestion events occur on a large-scale HPC system.
Therefore, in this paper, we study the interconnect resilience
and congestion events on Titan,
the fastest open-science
supercomputer in the world. We used daemon services on
Titan to collect useful interconnect resilience and congestion
events data for over a year. We process and examine this data
to develop a thorough understanding of interconnect faults,
errors, and congestion events. We also investigate how these
errors affect network congestion at different granularity. Our
analysis addresses the following concerns:
• What are the major interconnect faults and errors?
• What are the key characteristics of different interconnect
I. INTRODUCTION
errors and network congestion events?
Fast computing devices, network interconnect, and back-
end storage systems enable modern High Performance Com-
puting (HPC) facilities to deliver performance in the order
of petaﬂops. HPC systems consist of tens of thousands of
processors which require an advanced interconnect network
to minimize system latency and maximize throughput and
scalability for tightly-coupled parallel scientiﬁc applications.
Performance of the interconnect depends on network topol-
ogy, routing methods, ﬂow-control algorithm, resilience mech-
anism, congestion reaction mechanism, and communication
pattern of applications. Current scientiﬁc applications run
multiple processes on different compute nodes, and thus,
rely heavily on fast network messages to communicate and
synchronize frequently. Subsequently, interconnect resilience
This manuscript has been authored by UT-Battelle, LLC under Contract No.
DE-AC05-00OR22725 with the U.S. Department of Energy. The United States
Government retains and the publisher, by accepting the article for publication,
acknowledges that the United States Government retains a non-exclusive, paid-
up, irrevocable, world-wide license to publish or reproduce the published form
of this manuscript, or allow others to do so, for United States Government
purposes. The Department of Energy will provide public access to these results
of federally sponsored research in accordance with the DOE Public Access
Plan (http://energy.gov/downloads/doe-public-access-plan).
• What is the interaction between interconnect errors, net-
work congestion, and application characteristics?
This study exploits various daemons, such as netwatch and
nlrd that use Memory Mode Register (MMR), for collecting
and logging interconnect related events. However, analysis
of this data presents several challenges. First, the collected
data is highly noisy and hence, needs to be ﬁltered discreetly
for accurate analysis. Second, the log patterns differ for the
same type of events across different logging mechanisms. This
requires the development of uniﬁed format types for different
events. Finally, as data is distributed across several nodes and
storage locations, it requires performing multi-source analytics
to ensure consistency and accuracy. The following are the
highlights of our analysis:
• Interconnect Errors: The magnitude of interconnect
errors is very high. These errors are distributed unevenly
across different types of links within and across cabinets.
• Spatial Correlation: Some interconnect errors have a
strong spatial correlation among them. On the other hand,
some errors show counter-intuitive patterns.
• Congestion Events: Network congestion events are
highly frequent and bursty. These events are not homo-
geneously distributed across blades.
• Application Characteristics: Applications and users
causing network congestion and high communication
intensity have unique job characteristics.
In the following sections, we provide an analysis of the
interconnect data and explore the above insights in detail.
Given the lack of ﬁeld data and analysis on interconnects,
we believe our study addresses an important topic and would
be useful for current and future HPC systems.
II. BACKGROUND
This study primarily studies data from the Titan supercom-
puter; however, it’s insights are applicable to other super-
computers as well. Titan is a 27.1 petaﬂop supercomputer
consisting of 18,688 compute nodes, each with a 16-Core
AMD Opteron CPU and an NVIDIA Tesla K20x GPU. It has a
total system memory of 710 TB. The supercomputer is divided
into 200 cabinets in 25 rows and 8 columns. Each cabinet
consists of three cages and each cage has eight blades. Each
blade consists of two application speciﬁc integrated circuits
(ASICs). Each ASIC has two network interface controllers
(NICs) and a 48-port router. Each NIC within an ASIC is
attached to one node using a HyperTransportTM 3 link [1].
A. Titan Network Architecture
Titan follows a 3D torus topology using the Cray Gemini
Interconnect in which each ASIC is connected to six of its
nearest neighbors in X+, X-, Y+, Y-, Z+, and Z- dimensions.
The X, Y, and Z dimensions track the rows, columns, and
blades, respectively [2]. Nodes that are close physically may
not be close topologically as Cray follows a ”folded torus”
architecture to minimize the maximum cable length. In the X
and Y directions, every other cabinet is directly connected
together with ”loopback” cables. In the Z dimension,
the
uppermost chassis is connected to the lowermost chassis.
In a 3D torus design, each ASIC is connected to the network
using 10 torus connections, two each in X+, X-, Z+, Z-, and
one each in Y+ and Y- [1]. Each torus connection has four
links where each link is composed of 3 lanes. Therefore, each
connection consists of 12 lanes, providing 24 lanes in the X
and Z dimensions, and 12 lanes in the Y dimension. A lane
provides bi-directional communication between two ports.
B. Interconnect Resilience
The Gemini Interconnect is tolerant to various types of
failures and errors. It supports 16-bit packet Cycle Redun-
dancy Checks (CDCs) to protect packets at each ASIC it
passes through before reaching the ﬁnal ASIC, packets on the
receiving ASIC and packets transitioning from the router to
the NIC. Link control Blocks (LCBs) on ASICs implement
a sliding window protocol
to provide reliable delivery of
packets. Memory on each ASIC is protected using Error-
Correcting Codes (ECCs). ASICs can withstand lane failures
as long as there is at least one functional lane in a link.
Whenever a lane fails,
is deactivated and the trafﬁc is
balanced over the remaining lanes. In such situations, the
it
TABLE I: Summary of Netwatch events
Events
All
Mode Exchanges
RX
TX
Link Inactive
Bad Send EOP Error
Send Packet Length Error
Routing Table Corruption Error
HSN ASIC LCB lane(s) reinit failed Error
Count
Percent
9367031.0
5065536.0
2146693.0
2144221.0
7280.0
2548.0
366.0
200.0
187.0
100.0
54.08
22.91
22.89
0.08
0.03
0.004
0.002
0.002
network operates in a degraded mode. The interconnect tries
to reinstate the failed lane to restore the full bandwidth within
a user-speciﬁed time limit.
The lanemask value determines the current state of the lanes
in a link. It is a three-bit number corresponding to the three
lanes in a link. When all three lanes in a link fail and the
lanes are not recovered in the conﬁgured number of attempts,
the link is marked as inactive and the link failover protocol
is triggered. When an entire link fails, the Cray Network
Link Recovery Daemon (nlrd) on the System Management
Workstation (SMW) quiesces the network trafﬁc, computes
new routing tables, and assigns them to each ASIC.
C. Network Congestion
A network becomes congested when there is more data
in the network than it can accommodate for. The Hardware
Supervisory System (HSS) software manages the network con-
gestion into the network whenever necessary. Two daemons:
one on the SMW (xtnlrd), and one on the blade controller
(bcbwtd), can handle network congestion by limiting the
aggregate injection bandwidth across all compute nodes to less
than the ejection bandwidth of a single node (also known as
throttling).
D. Dataset
The collected dataset consists of the network logs from
January 2014 to January 2015. The interconnect metadata
is collected by two daemons: xtnetwatch and xtnlrd. The
xtnetwatch daemon logs the system High-Speed Network
(HSN) faults for LCBs and router errors. These logs include
details about the transmitting packets, receiving packets, mode
exchanges, lane mask, link inactive and different interconnect
failures data for particular nodes, along with a timestamp. The
xtnetwatch data is summarized in Table I.
When the percentage of time that trafﬁc tries to enter the
network is stalled more than a high water mark threshold,
the xtnlrd daemon produces log ﬁles that
include various
collection information. It also collects a list of the top 10 appli-
cations sorted by the aggregate ejection bandwidth whenever
a congestion protection event occurs. Moreover, it estimates
the top 10 most congested nodes sorted by ejection ﬂit counts
whenever a congestion protection event occurs. In both cases,
it includes the job characteristics of the applications running
2
l
a
o
t
t
f
o
n
o
i
t
c
a
r
F
s
e
d
a
r
g
e
d
e
n
a
l
100%
80%
60%
40%
20%
0%
1 la n e
2 la n es
3 la n es
l
a
o
t
t
f
o
n
o
i
t
c
a
r
F
s
e
d
a
r
g
e
d
e
n
a
l
60%
50%
40%
30%
20%
10%
0%
0 1 0
1 1 1
0 0 1
0 0 0
3−bit Lanemask value
1 0 1
1 1 0
0 1 1
1 0 0
Number of lanes degraded
Fig. 1: Frequency distribution of lane degrade events.
on those nodes, including APID, number of nodes, the user
ID, and the application name.
III. ANALYSIS OF INTERCONNECT ERRORS
In this Section, we characterize and analyze different types
of interconnect faults and errors. First, we quantify and char-
acterize lane degrade events. A lane degrade event is triggered
when any one of the three lanes in a link goes down. This has a
negative impact on the application performance and may cause
network congestion. Unfortunately, these events occur with a
very high frequency. We observed that lane degrade events
take place at a high rate of one event per minute. Despite
the high frequency and negative consequences of these events,
the characterization of these events in an HPC system is not
available to researchers, users, and system operators.
Fig. 1 (left) shows the frequency of different types of lane
degrades. We observe that in more than 90% cases only one
lane in a link is degraded. Two lanes are degraded in less
than 10% of the cases. Three lanes are degraded relatively
less frequently (<1%). When all three lanes are in a degraded
state, the link is declared inactive (or failed), and an alternate
route is computed for packets. While link inactive or failed
events happen relatively less frequently, they do occur about
28 times per day on average, and cause more disruption than
single or double lane degrades.
Fig. 1 (right) shows the frequency of lanemask values
for every instance of lane degrade events. We note that a
lanemask bit value of 0 indicates that the corresponding lane
is degraded. For example, a lanemask value of 5 (binary
value 101) indicates that the middle lane is degraded. We
observe that the frequency of lanemask values indicates that
even single lane failures vary signiﬁcantly. Lanemask value
110 is two times more frequent than lanemask values 101
and 011. Interestingly, for two lane failures, the corner lanes
failing together (010) is more likely than adjacent lanes failing
together (001 and 100).
In the absence of per-lane and per-link based utilization
data, we hypothesize that lane failure location indicates the
utilization and load pattern on links. Given this, our results
indicate that the load among lanes within a link may vary
signiﬁcantly. This ﬁnding should encourage designers to bal-
ance the load more homogeneously and not overload the
rightmost lane. This insight could also be exploited for power
optimization in interconnect links where rightmost lanes need
not to be switched-on at all times.
Next, we plot the relative frequency distribution of lane
degrades over time in Fig. 2. We make two important obser-
vations. First, lane degrades are not limited to a speciﬁc time
1.5%
1.2%
0.9%
0.6%
0.3%
0%
1 lane
2014−01 2014−03 2014−05 2014−07
2014−09 2014−11 2015−01
1.5%
1.2%
0.9%
0.6%
0.3%
0%
2 lanes
2014−01 2014−03 2014−05 2014−07
2014−09 2014−11 2015−01
50%
40%
30%
20%
10%
0%
link failed/inactive
2014−01 2014−03 2014−05 2014−07
2014−09 2014−11 2015−01
Fig. 2: Frequency of different types of lane degrades over time.
1%
0.8%
0.6%
0.4%
0.2%
y
c
n
e
u
q
e
r
F
e
d
o
M
0%
2014−01
2014−07
Time
2015−01
80%
60%
40%
20%
y
c
n
e
u
q
e
r
F
0%
1
3
5
7
9
Number of mode exchange 
attempts
Fig. 3: Daily frequency of mode exchanges to repair lane
degrades (left) and number of mode exchange attempts before
successful recovery of the lane (right).
period, instead they happen continuously over time. Second,
one may expect that the high single-lane degrade events will
lead to an increase in the count of two-lane degrades and link
failures. However, our ﬁeld data suggests that this hypothesis is
not necessarily true. For example, peaks in two-lane degrades
are not necessarily during the high intensity of one-lane
degrades. Later, we also investigate deeper to understand the
correlation between network congestion and the period of high
intensity of lane degrades.
When a lane goes down, the network resiliency mechanism
attempts to bring the lane back up via multiple repair events,
called mode exchanges. Fig. 3 shows the frequency of mode
exchange events over time and the number of mode exchange
attempts before a lane is brought up successfully. As expected,
the frequency of mode exchange events over time is similar
to that of lane degrades. System operators of Titan have set
the threshold for the number of attempts allowed to restart
a lane to 256. Interestingly, our result shows that more than
85% of the lanes can be restored in three or fewer attempts.
Furthermore, more than 99% of the lanes can be restored
within 10 attempts.
Next, we attempt
to understand how lane degrade and
link inactive/failed events are distributed across the system
spatially. Fig. 4 shows the lane degrade events for links in
and across cabinets. First, we observe that several hot spots
exist for lane degrade events in the system. We conduct
3
 Per−cabinet distribution of lane degrade events count
s
n
m
u
o
C
s
t
l
i
e
n
b
a
C
7
6
5
4
3
2
1
0
50000
40000
30000
20000
10000
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24