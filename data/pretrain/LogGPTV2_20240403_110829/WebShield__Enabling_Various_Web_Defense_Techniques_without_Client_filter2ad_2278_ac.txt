enforce the rule that only one of the two requests for the
same object can reach the web server. To achieve this, we
have to accurately identify the pair of requests for the same
object. Ideally, if each non-cacheable object has a globally
unique URI, it will be easier to pair the requests. However,
in practice this might not be true. For instance, it is hard
to enforce the user to not open two identical windows to
render the same page.
Because of the object coherence problem, we propose to
add a unique identiﬁer to every embedded URI. With the
identiﬁers, we can separate the URIs that are directly typed
by users from the embedded URIs, and we can also accu-
rately identify the pair of requests for the same object. In
our current design, we use a 256-bit identiﬁer. The ﬁrst 96
bits are a unique random string to avoid collision from other
possible user-inputted URIs. The middle 128 bits are the
web session ID. Finally, the last 32 bits are used to differ-
entiate the embedded URIs in a page. The 256-bit identiﬁer
is encoded into a URI safe string and attached to the end of
each URI. When constructing the DOM data structures on
the shadow proxy, we rewrite the URIs to append the iden-
tiﬁers. At the session manager, the embedded URIs will be
identiﬁed and paired up.
3.5 Sandbox of the Shadow Browser Instance
Similar to other security prevention schemes that require
dynamic execution of the suspicious content, WebShield
needs to use sandbox techniques to make sure that even
when an attacker compromises the shadow browser, he still
cannot compromise the physical host running the shadow
browsers, let alone the client browser or the client machine.
For this purpose, any state-of-the-art sandbox techniques
can potentially be applied with different tradeoffs on secu-
rity, performance and stability. Our design is not tied to
any particular sandbox technique. Potentially, we can apply
the best available techniques, such as SELinux, Xen virtual
machines (VM), VMware VM, etc. In our current imple-
mentation, we focus on SELinux.
4 Security Analysis
4.1 Subverting the Sandboxes
In an actual deployment setting, WebShield will have one
session manager running on a separate host M and a set of
hosts H for hosting the sandboxes. In our design, for any
host in H, the system administrator can conﬁgure the switch
and force it to only communicate with the proxy service on
M . The hosts in H cannot communicate with each other,
the Internet or the internal hosts of the enterprise network
unless going through the session manager.
To compromise a host in H, the attacker needs to ﬁrst
compromise the shadow browser, which means he must by-
pass the detection of known vulnerabilities as well as the
behavior detection engine (step I). Then, the attacker needs
to exploit another vulnerability in the sandbox to escalate
his privileges and take control of the host OS or VMM
(step II). After step I, the attacker can control one shadow
browser, and after step II, the attacker can control all of the
sandboxes and shadow browsers in a physical machine. In
both cases, he needs to exploit the session manger to take
full control of the proxy. We believe the session manager
should be written in a type-safe language, which will make
the control ﬂow hijacking exploitations much harder. This
will also make the proxy safer.
By taking control of the shadow browsers, the attacker
can also try to send malicious HTTP requests to the Internet
through the proxy to compromise other web servers. Since
the attacker can send such malicious requests without com-
promising the shadow browser, we do not believe this en-
hances the attacker’s power.
After compromising a shadow browser, an attacker can
also try to compromise enterprise web users by returning
malicious content for DOM update requests.
In our de-
sign, all of the JavaScript code in the returned webpage, i.e.,
our JavaScript code and the  tags for each update
block, is only added by the session manager, but not the
shadow browser. Shadow browsers only provide encoded
DOM updates in JSON. The attacker has to exploit a vulner-
ability in the JSON parser in the client browser to execute
JavaScript directly by the JSON parser. After JSON pars-
ing, but before adding parsed content to the DOM tree or
CSS style objects, we ensure all  tags are empty,
and no event handler attributes are present. Therefore, it
is still nontrivial for attackers to compromise web users or
even control the corresponding shadow browser.
4.2 Potential DoS Attack
In our threat model, we do not consider the case where
web users will launch a DoS attack on the proxy. We limit
the number of dynamic HTML webpages that can be con-
currently opened from a single IP. (Note that the limitation
is for a DoS attack. For a legitimate user, the number is
large enough for him to use.) For static webpages, we do not
maintain a long lived webpage in the shadow browser, re-
ducing resource consumption. This will prevent a few users
from overwhelming the proxy by opening a large number of
pages.
Some pages may also open more webpages automati-
cally using JavaScript. For this case, we will not create
the window on the shadow browser directly.
Instead, we
will send the open window request to the user. Normally
the client browser will block it and ask permission from the
user. If the user allows the pop-up, an AJAX request will
be sent to the proxy, and the shadow browser will allow the
window to open and transfer the transformed content to the
user. We also limit the number of such pop-up pages for a
given web user (Similar methods has already been adopted
in modern browsers, such as Firefox, which limit the num-
ber of pop-up windows).
4.3 Fingerprinting the Shadow Browser
Some methods can be adopted for the webpage running
inside the shadow browser to ﬁngerprint the environment.
For example, the webpage can detect the browser’s ver-
sion number, support of functionalities, etc. However, it
is still hard for attackers to decide whether the webpage
is running in a shadow browser or in a client browser di-
rectly. Even if they can detect that the webpage runs in a
shadow browser, they still cannot exploit the client directly.
Moreover, they cannot probe the browser version of a client
browser directly, because we do not allow any JavaScript
from the webpage to run on the client browser. Therefore,
it is non-trivial for the attackers to leverage on the possi-
ble version difference between the shadow browser and the
client browser for attack.
4.4 Compatibility with Other Security Protection
Mechanisms
In our design, we explicitly consider compatibility with
other existing security protection mechanisms.
Same Origin Policy: In our design, we do not break the
same origin policy. We keep the origin of each website un-
changed. When we rewrite URIs, we do not alter the parts
related to the origin.
Host Anti-virus Protector: Currently, many anti-virus sys-
tems add security plug-ins to the browser to enhance user
protection. Since the DOM data structures are almost iden-
tical to the originals, WebShield will not inﬂuence anti-virus
scanners.
requires having the privilege user t:file read. Since
the processes do not have the proper privileges, access will
be denied. This denial will appear in SELinux logs giving
us a means for detection.
5.4 Drive(cid:173)By(cid:173)Download Detection
5
Implementation
5.1 DOM Instrumentation
At the client browser side, we leverage JavaScript DOM
APIs to update the DOM data structures. We use the DOM-
CSS interface to add the CSS style objects for the CSS rules.
We also use the element.style object to handle the in-
line CSS style rules.
To implement the shadow browser, we modify WebKit
revision 41242 [10]. We instrument the DOM interface of
WebKit in C++. Once there is an action in the DOM, e.g.,
appendChild, we detect such a change in the DOM and pro-
cess it accordingly.
5.2 Session Manager Implementation
In our current prototype, the session manager is imple-
mented in Python. We use the HTTP client.py and
server.py ﬁles from Python 3.1. We implement web
caching, object coherence and session management. For
Web caching, we follow the cache related HTTP headers.
For session management, we assign a session ID to each
HTML object. Later, the session ID will be used to identify
the AJAX call for the event proxy.
5.3 Sandbox Implementation
There are many possible choices for a sandbox environ-
ment. Generally speaking, we consider two factors, perfor-
mance and security. Tahoma [16] uses Xen, a VMM which
has good security protection but the overhead is quite large.
OP Browser [17] and Google Chrome [32] use process level
sandboxing, which has good performance, but weaker secu-
rity. In our current implementation, we adopt SELinux, the
same sandbox used by OP Browser.
In the TE model of SELinux [27], an object, for exam-
ple a program, is assigned a type, which has limited access
privileges to resources within itself and other objects. We
assign a different type to every new sandbox we create. We
then provide the minimum resources required by the sand-
box. If the processes inside the sandbox tries to access a
certain resource for which they does not have permissions,
the event will be logged in our system, indicating contam-
ination of this sandbox. For example, the processes in a
sandbox tries to access a user ﬁle with type user t, which
To demonstrate the usefulness of the WebShield frame-
work, we implement two types of detection engines for
drive-by-download attacks: a policy-based engine to detect
known vulnerabilities and a behavior-based engine for un-
known vulnerability detection.
The policy engine for vulnerability ﬁltering. Browser-
Shield [31] leverages HTML and JavaScript rewriting to
add an interposition layer to check the invocation of DOM
APIs and malicious HTML tags. Since we are able to mod-
ify the shadow browser, we directly insert a security check-
ing layer between the DOM APIs and the HTML parser,
CSS parser and JavaScript engine. Therefore, we can ﬁl-
ter out all the DOM nodes or CSS style objects that will
potentially trigger the vulnerabilities before encoding the
DOM data structures and sending the DOM data structures
to the client browser. This way, we “purify” the webpage
and display the remaining safe parts to the end users. The
end users can still access the important information in the
the webpages without any problems.
For the policy engine, we primarily add a security check-
ing layer at two places. The ﬁrst place is the JavaScript API
binding. Whenever JavaScript tries an API call, such as the
DOM APIs, we will capture the invocation. We then check
whether the parameters to the APIs will trigger any known
vulnerabilities. The second place is the HTML and CSS
parse trees. The vulnerability checkers can be written as a
C++ function. We provide APIs for writing such checkers.
Our present implementation is a regular expression checker,
which checks each passed string using a signature library by
regular expression.
The behavior engine for detecting unknown vulnera-
bilities. Usually, the goal of drive-by download attacks
is to exploit the victim’s browser, and let the attacker in-
stall and run arbitrary software on the victim’s computer.
In [23, 24, 28, 29, 36], a behavior based model is used to
detect drive-by-download attacks toward unknown vulnera-
bilities. The basic idea is that any abnormal behaviors that
violate the browser security model will be counted as at-
tacks. In [23, 24], Moshchuk et al. give a list of abnormal
behaviors, such as attempts to create a new process that does
not belong to the browser, modiﬁcations to the ﬁle system
other than the cache folder, browser/OS crashes, etc. We
implement a similar behavior detection model on SELinux.
We mainly rely on two mechanisms: the SELinux log and a
process monitor. The SELinux log detects a potential nor-
mal proﬁle violation, including attempts to execute a dif-
ferent binary to create a process, etc. The process monitor
monitors whether the process has crashed, or uses too much
memory, etc. When either of these two reports a problem,
we will consider it as a vulnerability.
5.5
Implementation Summary
We add 6000 lines of C++ code to WebKit in order to
construct the proxy-side sandbox, with 200 lines used to
inject the DOM interface. Session Manager also contains
3700 lines of Python code. The client side program contains
722 lines of JavaScript code.
6 Evaluation
We evaluate WebShield with seven different metrics: (i)
compatibility of representative webpages with our imple-
mentation, i.e., how accurately webpages through our proxy
render at the client side, (ii) the latency overhead, (iii) the
communication overhead, (iv) the memory overhead, (v)
the interactive performance for dynamic HTML, (vi) scal-
ability and (vii) accuracy of the detection engine. The ﬁrst
ﬁve metrics are simply to evaluate how transparent Web-
Shield is to the user. Then, we discuss how well our sys-
tem scales. Finally, as a demonstration, we show the accu-
racy of the detection engine plugins for detecting drive-by-
download attacks. The proxy server was installed on a 2.5
GHz Intel Xeon server with 16 GB RAM running CentOS
5. For the client, we used a 2.66 GHz Intel Core2Duo ma-
chine with 3.25 GB of memory and running Windows XP
SP3. For some tests using the Safari browser, we used a
2.2 GHz Intel Core2Duo based MacBook with Mac OS X
version 10.6 and 4 GB memory as the client side machine.
The client machines and the server were connected on Giga-
bit Ethernet. Unless indicated otherwise, the client browser
used is Google Chrome. Next, we discuss the evaluation
results.
6.1 Compatibility Tests
In this section, we evaluate whether webpages render
correctly when viewed through the proxy. The notion of
rendering correctness is relative to the rendering without go-
ing though our proxy. For all of the webpages, we manually
test the visual correctness of the rendering.
We note that not all of the websites are actually entirely
compatible with our current implementation of WebShield.
91 of the top 100 websites and 19 of the top 20 as given by
Alexa were compatible, and the rest have some rendering is-
sues. The webpage at http://www.aol.com cannot be
rendered by the WebKit version we used for the WebShield
implementation. For the remaining websites, the reasons
are mainly implementation stability issues (crashes) and the
websites using unsupported features such as iframes with
the HTTPS protocol.
6.2 Latency Overhead
The timing overheads are computed as the latencies be-
tween the start and ﬁnish time of page rendering relative to
the page request time. We selected the webpages passing
the compatibility test and rendered each of them in Fire-
fox, Chrome and Safari and measured the rendering start
and ﬁnish times. Due to the limited space, we show only
the results for Firefox and Chrome here; the results for Sa-
fari are similar. When accessing the pages through Web-
Shield, we used JavaScript functions to get the start and end
times. The browser may issue the onload event before the
page response has already completed. So we report ren-
dering ﬁnish time as the maximum of HTTP response end
time and the onload event time. When directly accessing
the webpage without using WebShield, we use the page re-
sponse start and ﬁnish times to approximate the rendering
time. Each URL is rendered ﬁve times and the medians of
individual results of these runs are used as the rendering la-
tencies for the URL. For this metric, we do a detailed evalu-
ation for two versions of WebShield: incremental and non-
incremental. The incremental version transmits a webpage
part by part to the client browser once it has rendered these
parts in the shadow browser. It does not wait for the entire
webpage to be downloaded and rendered before transmit-
ting it to the client. The non-incremental version is a sim-
pler one, presented here only for the sake of a comparison.
It transmits responses to the client, only after the webpage
has been completely rendered at the proxy.
Figure 7 presents the cumulative distribution of ren-
dering start times. The incremental version sends partial
rendered contents to the client starting earlier than non-
incremental one. As expected, the incremental version re-
sponds earlier than the non-incremental version for more
than 50% of the pages.
Figure 8 shows us the cumulative distribution of ren-
dering ﬁnish times. We note from the CDF that there
is not much difference between the incremental and non-
incremental versions. This is easily understood because the
rendering end time depends on the last chunk of the page
response which depends on the response from the original
webserver; both the incremental and non-incremental ver-
sion end up transmitting this last chunk at nearly the same
times. Effectively, the incremental version only helps in
improving the responsiveness of the webpage and not in the
net load time.
To summarize the comparison between access through
the incremental version of WebShield and direct access, we
present the following numbers. The median difference of
rendering starting latency is 133.5 milliseconds. For 90%
n
o
i
t
u
b
i
r
t
s
d
i
e
v
i
t
l
a
u
m
u
c
0
.
1
8
.
0
6
.
0
4
.
0
.
2
0
0
.
0
n
o
i
t
u
b
i
r
t
s
d
i
direct access
incremental version
non incremental version
e
v
i
t
l
a
u
m
u
c
0
1000 2000 3000 4000 5000 6000
rendering start time (ms)
0
1
.
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
direct access
incremental version