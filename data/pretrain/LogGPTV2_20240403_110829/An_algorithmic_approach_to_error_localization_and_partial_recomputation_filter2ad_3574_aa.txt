title:An algorithmic approach to error localization and partial recomputation
for low-overhead fault tolerance
author:Joseph Sloan and
Rakesh Kumar and
Greg Bronevetsky
An Algorithmic Approach to Error Localization and
Partial Recomputation for Low-Overhead Fault
Tolerance
Joseph Sloan, Rakesh Kumar
University of Illinois,
Urbana-Champaign
jsloan,PI:EMAIL
Greg Bronevetsky
Lawrence Livermore National Laboratory,
Livermore,CA
PI:EMAIL
Abstract—The increasing size and complexity of massively
parallel systems (e.g. HPC systems) is making it increasingly likely
that individual circuits will produce erroneous results. For this
reason, novel fault tolerance approaches are increasingly needed.
Prior fault tolerance approaches often rely on checkpoint-rollback
based schemes. Unfortunately, such schemes are primarily limited
to rare error event scenarios as the overheads of such schemes
become prohibitive if faults are common. In this paper, we
propose a novel approach for algorithmic correction of faulty
application outputs. The key insight for this approach is that even
under high error scenarios, even if the result of an algorithm is
erroneous, most of it is correct. Instead of simply rolling back
to the most recent checkpoint and repeating the entire segment
of computation, our novel resilience approach uses algorithmic
error localization and partial recomputation to efﬁciently correct
the corrupted results. We evaluate our approach in the speciﬁc
algorithmic scenario of linear algebra operations, focusing on
matrix-vector multiplication (MVM) and iterative linear solvers.
We develop a novel technique for localizing errors in MVM
and show how to achieve partial recomputation within this
algorithm, and demonstrate that this approach both improves
the performance of the Conjugate Gradient solver in high error
scenarios by 3x-4x and increases the probability that it completes
successfully by up to 60% with parallel experiments up to 100
nodes.
Keywords—algorithmic error correction, partial recomputation,
error localization, numerical methods, sparse linear algebra
I.
INTRODUCTION
As High-Performance Computing (HPC) and other mas-
sively parallel systems grow more capable, they also grow
larger and more complex. This means that as the number
of components in the systems rises, so does the probabil-
ity that one of them will suffer from a fault. Soft faults
in chip circuitry are among the most worrying for system
designers and application developers because they can corrupt
the application’s computations and produce incorrect output.
Tera-scale systems are already vulnerable to soft errors, with
ASCI Q experiencing 26.1 CPU failures per week [16] and
a L1 cache soft error occurs about once every ﬁve hours on
the 104K node BlueGene/L system at Lawrence Livermore
National Laboratory [9]. Looking into the future, according
to the International Technology Roadmap for Semiconductors,
the soft error rates (SER) will grow with smaller chip feature
sizes, with SRAM SER growing linearly with the number
of transistors on a chip [2], which grows exponentially over
time. This and the fact that many parallel systems of the
foreseeable future will have hundreds of thousands to millions
of electronic chips with feature sizes as low as 12nm [2] has
led several recent studies [8] to warn that “traditional resiliency
solutions will not be sufﬁcient”. Hardware-based approaches
for fault tolerance have been proposed for many computing
systems. However, their reliance on redundancy makes them
impractical for future massively parallel systems because they
will be severely power-constrained [8]. In fact, evolutionary
extensions of today’s high performance computing (HPC)
systems (CrayXT, BlueGene) will be unable to reach exaFLOP
performance by 2020 within a power budget of 20MW, the
typical limit of modern computing centers [8].
The traditional approach for dealing with errors in systems
is to roll the application back to a prior checkpoint whenever a
fault is detected. This approach incurs a high cost in transfer-
ring checkpoint data [23]. Further, since expensive checkpoints
result in long checkpointing periods [23], each rollback incurs
a large cost in recomputing lost work. While this may be
acceptable in scenarios where faults are rare, as fault rates
increase with rising node counts and ﬁner circuit features,
the cost of full-application rollback may become prohibitive.
Figure 1 shows the the performance of parallel linear solver,
CG, using a traditional checkpoint-restart approach in the face
of increasing fault rates.The results in Figure 1 assume that
faults can be detected perfectly when they occur,
thereby
isolating the overhead due to application-level rollback. We
observe that the overhead of application-level rollback reduces
the performance of the solver by 2x-10x as fault rates increase.
In this paper, we propose a novel approach for fault
tolerance that uses algorithmic correction of faulty application
outputs based on error localization and partial recomputation.
The key insight of our approach is that even under high error
scenarios, a large fraction of the output is correct even if a
portion of it is erroneous. Therefore, instead of simply rolling
back to the most recent checkpoint and repeating the entire
segment of computation, our approach identiﬁes and corrects
the actual subsegments of the output which are faulty. The
correction technique we propose is algorithmic and leverages
the properties of individual algorithms of interest to identify
fault
the scope of recomputation. For
example, errors in the output of a sorting algorithm can be
localized by scanning through the results [28] and noting the
ones that are mis-ordered, missing or new. A small number of
locations and limit
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
error localization reduce the overhead by up to 32% on
average when scaled up to 10 nodes and 320% when
scaled up to 100 nodes. Similarly, with the relaxed
error localization routine the overhead is reduced by
up to 77% at 10 nodes and up to 390% at 100 nodes.
By showing the utility of partial recomputation in the context
of popular numerical algorithms we hope to demonstrate the
value of research into partial recomputation in the context of a
wider range of algorithms. Since this approach is signiﬁcantly
more efﬁcient than whole-application recomputation and also
signiﬁcantly simpler than algorithmic correction techniques,
we expect that this line of work will be extremely productive in
ensuring cheap and effective resilience on future HPC systems
and other massively parallel systems.
The paper is organized as follows. Section II describes
related work and explains the limitations of prior checkpoint
and rollback techniques. Section III describes the opportunities
and approaches for low overhead algorithmic fault correction.
Section IV discusses the methodology for evaluating the ef-
fectiveness of the techniques. Section V presents the results.
SectionVI concludes.
II. RELATED WORK
Checkpoint and rollback mechanisms have been the dom-
inant approach for providing fault
tolerance for HPC and
massively parallel systems for decades [1, 19, 30, 5]. These
approaches all rely on periodically saving the application
and system state (i.e. address space, message buffers, and
architectural state), so that if a fault is later detected, the system
can simply restart from the saved prior state, rather than from
the beginning of the application. Although this approach is
exceptionally general, it can also incur prohibitive performance
overheads under high error rate scenarios, due to large recovery
costs.
For checkpoint-rollback based techniques, there does exist
a tradeoff between the detection latency and the checkpoint
frequency. For example, a system can increase the checkpoint-
ing frequency in order to reduce the detection latency and
recovery costs [29]. However, this also signiﬁcantly increases
the checkpointing overhead in terms of performance, storage,
and bandwidth, limiting the efﬁciency of this type of tradeoff.
Our proposed approach instead uses algorithmic correction
to partially recompute localized regions of output which are
identiﬁed as being erroneous and reduce the recovery cost
associated with rolling back and recomputation
Some previous studies have also identiﬁed checkpoint-
rollback as signiﬁcant limitation for future systems, mainly due
to the storage and bandwidth overheads, and have proposed
alternatives to checkpointing. In the context of permanent
failures, Chen et. al. study the use of erasure-codes to recover
lost data and eliminate more traditional checkpoints [6]. In this
paper, we focus on transient computation faults which require
an active detector. Also, rather than eliminating checkpoints
entirely we propose the use of error localization to guide the
algorithmic correction of errors by partial recomputation.
There is also much related work on algorithmic fault
tolerance approaches. For linear solvers, some algorithmic
techniques
[25, 17] have been proposed that add additional
2
Fig. 1: Parallel CG Performance for different fault rates, when
using traditional checkpoint-restart approach and assuming
perfect fault detection, (Number of processor nodes=10, ac-
curacy target=1e-6).
such errors can be corrected efﬁciently without repeating the
entire sort.
This paper explores this concept in the context of numerical
linear algebra in high error scenarios on parallel systems. It
focuses on the matrix-vector multiplication (MVM) operation
as well as iterative linear solvers. MVMs often dominate
computation in many HPC and Recognition, Mining, and
Synthesis (RMS) applications (see Section IV-E). We make
the following contributions:
• We propose a partial recomputation- based approach
for algorithmic correction, that is much more suited
for high error rate scenarios than more traditional
fault tolerance approaches, such as checkpoint/restart,
which incurs high recovery costs.
• We propose a novel algorithmic technique for error
localization (the process of identifying partitions of
faulty and non-faulty outputs) for MVM operations.
• We show that the proposed techniques scale much bet-
ter than traditional parallel fault tolerance approaches
because they alleviate the performance bottlenecks
that arise from high recovery costs.
• We quantify the performance beneﬁts of partial re-
computation and error localization in the context of
parallel MVM and the parallel Conjugate Gradient
(CG) iterative solver, which uses MVM internally
under varying magnitude error rates. Our experiments
show that while traditional detection/rollback has 2x-
3x overhead under high fault rates, Partial recomputa-
tion is 2x cheaper while maintaining similar accuracy
as ideal detection/rollback approaches. With more
realistic detection schemes using dynamic thresholds,
partial recomputation-based approaches are signiﬁ-
cantly more efﬁcient (CG converges 70% more often
and performance overheads 2x-3x smaller).
• We study the scalability of these techniques within the
context of a parallel linear solver application for dif-
ferent parallel scales. For a ﬁxed moderate fault rate,
partial recomputation-based approaches with complete
010020030040050060070080090010001.00E-091.00E-081.00E-071.00E-061.00E-05Performance Overhead vsFault-Free Case (%)Fault Rate (Faults per operation per node)inner/outer optimization loops to account for noisy computa-
tions. Researchers have also studied the use of linear error
correcting codes
[18] with algorithmic techniques for fault
tolerance. The check for a linear operation, such as the matrix
vector product (Ax where matrix A and vector x are inputs)
detects faults by verifying that the following identity holds:
cT (Ax) = (cT A)x
Intuitively, the check computes the projection of the result
Ax onto the vector c in two different ways. If there are any
computation errors, the two projections will very likely be
unequal (e.g. the difference between projections surpasses a
given threshold, τ) In the common case where c = ¯1 (a vector
of all 1’s), the projection is equivalent to multiplying x by the
vector containing the sums of matrix A’s columns.
Consider for example, a 5x5 matrix A and an input vector
A =
3
2
0
1
3
0
1
3
0
1
2
0
2
3
0
3
2
1
2
0
5
5
7
1
2
The correct output of the matrix vector product is:
x:
y(cid:48):
4
5
6
2
2
 , x =
40
27
42
32
24
3
5
0
0
0
y = Ax =
y(cid:48) = y + e, e =
1
1
1
1
1
c =
Let say that an error (e) perturbs the correct output y into
In order to detect whether output y(cid:48) is correct, take a vector c:
Performing the error detection involves checking whether
the checksum invariant holds ( cT y(cid:48) = cT Ax). In practice, the
check invariant can be veriﬁed by computing the difference
between the checksums (i.e. the syndrome) and comparing it
to a threshold (τ = 0). In this example, if the syndrome is not
equal to zero an error is detected, otherwise the computation
is deemed correct:
cT y − (cT A)x = 0
cT y(cid:48) − (cT A)x = 8
(OK, check invariant holds)
(Error(s) in the output!)
The check works similarly for other linear operations as
well.
3
Results from these checks can also be used for correction,
however this can be expensive. At least d code vectors are
required in order to correct at most (cid:98)d/2(cid:99) errors. Moreover,
correction is heavily code dependent and not a trivial problem.
(i.e. In order to correct multiple faults linear codes usually use
a set of vectors each with unique non-binary codes and then
the problem results in having to solve a non-trivial system of
nonlinear equations [3]).
For this reason,