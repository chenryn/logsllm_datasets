in our analysis are stored at the mobile carrier, in a secure facility,
unreachable from the Internet. Thus, physical access is required
every time an experiment is conducted.
Inspired by similar sensitive measurement experiments described
in the literature (e.g, [54]), we wrote experimental code remotely,
and tested it on synthetic records. Subsequently, one of our co-
authors with authorized access to the secure facility ran the code
on real data. Only aggregated results were then brought back for
analysis. Any personally identifiable information from customers
(e.g., IP addresses) was expunged or coded before records were
exchanged. The User ID in the logs is an internal number unique
to each subscriber, and is not directly linkable to any personally
identifiable information (e.g., IMSI or phone number). Although a
correspondence table may exist at the mobile carrier, we did not
need it, and did not access it.
3.1 HTTP Traffic Collection
We rely on logs collected between April 1, 2017 and June 30, 2017.
During that period, out of our initial pool of 20,895 consenting
participants, 20,645 distinct smartphone users appear in the logs.
The others presumably did not use data over cellular during that
period.
Each log entry contains a timestamp of the HTTP request, the
URL accessed, the content of the HTTP Referer field, the number
of bytes uploaded and downloaded, the user-agent string, and a
(unique) user ID corresponding to the customer.
Limitations. The dataset does not include HTTP contents (e.g.,
data sent via HTTP POST) or HTTPS requests, and only includes
HTTP requests whose content-type is text/html. In other words,
we do not have visibility to image-, script-, or multimedia-content
access; likewise, because collection only takes place over the cellular
network, we do not have access to any Wi-Fi traffic.
On a more positive note, collection is completely passive. Users
do not need to change any mobile settings, install specialized soft-
ware, or undertake any form of action other than providing initial
consent to participate in the data collection. Furthermore, prior
work has found that most malicious traffic on the web is served
3.2 HTTP Log Processing
We define a browsing session (or, simply, a “session”) as a temporally
contiguous set of HTTP requests made by the same user. We con-
sider a session effectively terminates when either 1) the associated
user-agent changes (denoting the user switched browsers), or 2) the
user is idle—i.e., does not engage into a subsequent HTTP request—
for more than 20 minutes; we chose this specific parameter to be
consistent with prior work on “click streams” [85]. (We varied this
threshold in pilot experiments, and did not observe considerable
changes in the 5- to 20-minute range). We point out that the goal of
sessions is not to faithfully reconstruct users’ web-surfing activity
on their devices [88], but rather to define continuous windows of
time in which users are surfing the web.
A small fraction of HTTP requests (<2.2%) in our dataset origi-
nated from traditional operating systems (e.g., Windows, Mac OS,
...), connected to the network via tethering. We did not treat this
traffic differently from other traffic, as our proposed methods are
not limited to mobile devices, per se.
We initially marked each HTTP request as malicious or not based
on a check of the Google Safe Browsing v3 (GSB, [31]) database.
We will explain how we overcome some of the limitations of this
approach—notably the fact that malicious URLs may not yet be
Session 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1489in the GSB at the time of browsing—in Section 4. Because the
GSB database is highly dynamic (entries are added and subtracted
every day), we downloaded daily snapshots of the GSB database
throughout our measurement interval. Specifically, for every hash
prefix available in GSB on a given day, we queried the GSB API [31]
to download the full hashes that start with the prefix.
GSB distinguishes between phishing and malware URLs. The
former purportedly entice users to reveal private information, while
the latter attempt to deliver unwanted programs to users’ devices [31].
In practice, we noticed that certain entries that are labeled as phish-
ing by GSB may lead users to download malicious software or
extensions, while others may lead to ad- and click-fraud pages.
Such entries can be construed as malware, as they may harm the
users’ devices or online (e.g., ad) services.
In fact, using Virus-
Total [15]—a popular service that combines reports from several
blacklists—we found that 20 out of 25 randomly sampled domains
that are classified as phishing by GSB are also classified as malware
by one or more other lists. For example, hxxp://applicationg29.com,
which was previously classified as a phishing domain by GSB, leads
to a page for downloading a fake anti-virus when visited with cer-
tain URL parameters. Hence, the distinction may not be as critical
as we originally thought; both types of URLs are harmful to the
user, and potentially to online services. For the rest of this paper,
we consider all entries as malicious.
We also divided URLs visited upon each HTTP request into cate-
gories (e.g., news, sports, ...). To do so, we relied on the taxonomy
developed by DigitalArts, the main filtering provider in Japan, for
their i-Filter filtering system [39]. Using manually labeled domains
by DigitalArts, we trained a Convolutional Neural Network [92] to
classify domain names into one of 99 topics. The neural network
achieves ∼90% accuracy in assigning URLs to the correct topic. Fi-
nally, we classified users between exposed and unexposed. Exposed
users visited malicious pages (per the above definition) at least once
during our collection interval; unexposed users did not.
3.3 Online Survey
A contribution of our work is to validate whether users’ self-reported
responses to computer-security surveys can help predict their be-
havior. Specifically, we are interested in exploring whether survey
answers can predict users’ exposure risk. To this end, we asked
users who consented to participate in this research to respond to
an online survey.
Recruitment. In June 2017, we invited 600,000 people drawn from
the pool of eligible, customers (i.e., those who opted in to having
data collected) to participate in a research survey. All customers
were based in Japan. As an incentive for customers to respond, we
offered them the chance to enter a drawing to win a 500 JPY (roughly
equivalent to $5) gift card. 1,000 randomly selected respondents
received this prize.
We initially received 23,419 user responses. While the 3.90%
response rate we recorded is below response rates associated with
online surveys (thought to be in the 10–20% range [53]) it is in-
line with surveys conducted in similar online security studies (e.g.,
3.4% [48]). Furthermore, given the large population sample we
considered, a high survey response rate is unnecessary, as long as
the pool of respondents is not significantly biased.
Demographics. The pool of respondents was slightly biased to-
ward male users: indeed, 61.5% male and 38.5% female users re-
sponded. (The pool of solicited users was 55.6% male, 42.8% female,
1.6% unknown.) The respondents’ median age was 43 years, with a
standard deviation of 11.8 years. The demographics are not neces-
sarily closely tracking the overall population. For example, elderly
users might prefer traditional flip or feature phones rather than
smartphones. Moreover, due to ethical reasons and the difficulty
of acquiring guardians’ consent, our pool of respondents excludes
minors below 18 years of age. With this in mind, the demographics
of our respondents mirror those of our solicited users fairly closely,
showing no specific evidence of bias.
We were unable to immediately assert the proportion of iOS and
Android users among respondents, since we did not collect this
information in the survey responses. We elected not to perform
user-agent matching after the fact, as it would have been very noisy:
while Safari and Google Chrome users on iPhone and Android might
be classified relatively accurately, numerous other browsers may
not use user-agent strings very representative of the platform on
which they run.
Finally, we eliminated responses that did not pass attention
checks. This yielded valid answers from 20,895 distinct users.
Questions. Besides demographic questions (gender and age), we
asked participants a number of behavioral questions. We speculated
that self-reported behavior is correlated with actual behavior based
on prior evidence. The behavioral questions we inquired about are:
(1) Whether users have encountered security incidents (e.g.,
stolen password). Suffering from security incidents is often
correlated with users’ advice sources [64], and hence might
affect their behavior.
(2) Whether an anti-virus is installed on users’ devices. Users
running anti-viruses might be more likely to engage in risky
behavior [14].
(3) The types of app marketplaces usually used. Participants
had the option to select official (e.g., Google Play), mobile
providers’, or other, unofficial, marketplaces. Prior work has
shown that unofficial marketplaces contain a high fraction
of malicious apps [94]. Thus, we expected users who use
such marketplaces to engage in more risky behavior.
(4) What step(s) the participants take their browsers warn them
about a malicious webpage (e.g., always proceed, ...). We
showed participants the Chrome warning page. As Chrome
has the largest market share among browsers [77], partic-
ipants are most likely to be familiar with it. We expected
participants who proceed on warning to be at higher risk.
(5) Participants’ responses to the proactive awareness sub-scale
of RSeBIS. Participants with high proactive awareness scores
are less likely to fall for phishing than others [21]. Thus, we
expected high-scoring participants to also be at lower risk
of visiting malicious websites.
(6) Participants’ self-confidence in their computer security knowl-
edge. We used the questions from Sawaya et al. [73] to mea-
sure self-confidence in security knowledge. We expected
confident participants to exhibit more secure behavior.
We provide the entire survey in Appendix A. Since the users are
based in Japan, we ran the survey in Japanese. For the proactive
Session 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1490awareness sub-scale and the self-confidence questions, we used
the RSeBIS Japanese translation of Sawaya et al. [73]. Other sur-
vey questions were translated by computer-security experts in our
group who are fluent in both English and Japanese.
4 EXPOSURE TO MALICE
We next delve into the analysis of the HTTP logs. This analysis
has three goals: (1) to determine to what extent mobile users are
exposed to malicious content, (2) to demonstrate that there is a
“window of opportunity” for miscreants to compromise devices
before a page is blacklisted, and document plausible sizes of that
window, and, (3) to explore differences in behavior across several
dimensions between exposed and unexposed users. The analysis
will serve as the bedrock for feature engineering in the predictive
models we build and evaluate in Sec. 6–7.
Figure 1: Total number of accesses to malicious pages as a function
of the number of days to pages’ first observation in GSB. The plot is
stacked. Negative x-axis values indicate possible exposure to mali-
cious pages not yet in GSB.
4.1 Overall Prevalence of User Exposure
As discussed in Section 2, there is some disagreement in the research
community regarding the actual prevalence of malware in the mo-
bile ecosystem. Specifically, previously reported results [49, 82]
differ by several orders of magnitudes in their estimates. This moti-
vates our own investigation based on the logs we collected.
Among the 20,645 users for whom we have HTTP logs (between
April 1, 2017 and June 30, 2017), 2,172 (11%) users accessed a mali-
cious page at least once and were thus exposed. Most of these users
(1,995) were exposed to pages that GSB classifies as a “phishing”
page; 153 users were exposed to “malware” pages, and 24 users were
exposed to “malware” and “phishing” pages. The GSB database also
features entries for “unwanted” pages, but none of our users appear
to have landed on such pages. Overall, the exposed users visited
3,491 unique malicious pages on 201 different domains.
In short, at least 0.81% of all users (those visiting “malware” or
“malware and phishing” pages) were exposed to confirmed malware.
We cannot, however, estimate the fraction of exposed users that
were actually infected. Indeed, these users might have been pro-
tected by an anti-virus, or other content filters. More interesting
to us is the fact that a considerable (11%) fraction of all users ac-
tually get exposed to questionable content, further motivating our
research in attempting to prevent such exposure in the first place.
4.2 Window of Exposure
We next estimate the amount of time users might be exposed to
malicious pages while defenses may not yet be in place. Our ap-
proach is similar to a prior approach to measure the preponderance
of zero-day exploits in the wild, by retroactively checking evidence
of malware signatures in telemetry data dating back to times before
these signatures were known to anti-virus companies [8].
Similarly, we check if we observe logs of accesses to URLs before
these URLs were included in the GSB database. Fig. 1 plots, as a
function of time, the number of accesses to pages present in the
GSB database during our measurement interval (April 1, 2017–June
30, 2017). Negative x-axis values denote accesses to a page before
it was included in the GSB database, while positive values denote
accesses occurring after inclusion.
We immediately point out that our analysis presents some limi-
tations. First, because we work with a finite, 91-day interval, there
is only one possible day for us to observe a URL being accessed
90-days prior (resp. after) inclusion in the GSB: the first (resp. last)
day of our measurement interval. Conversely, there are 90 days
during which we can observe an access one-day prior (resp. after)
inclusion in the GSB. Thus, values closer to 0 on the x-axis of our
plot will be over-represented. To account for this imbalance, we
normalized all y values by dividing them by the number of days we
could actually observe them—dividing the y-value corresponding
to x = 0 by 91, the y-values corresponding to x = ±1 by 90, and
so forth. For the sake of brevity, we omit this plot here: the overall
shape of the curves does not markedly change.
Second, we can estimate that a page ceases to be malicious when
it is removed from the GSB database. However, we do not know
when a page starts being malicious. In the best case, it could be on
the day it was included in the GSB, in which case prior accesses
would not be problematic. In the worst case, it could have been
malicious more than 91 days prior to inclusion, in which case, all
of the accesses we observe would be problematic.
With these caveats in mind, it is striking to see that certain
deceptive pages were accessed a full 87 days before inclusion in GSB
(leftmost point on the graph). We observe a first increase in accesses
to malicious pages 38 days before inclusion in the GSB database,
and a second increase 22 days before inclusion in GSB. We further
observe a large spike two days before inclusion in the database—
intuitively, an increased number of accesses to a malicious page
increases its likelihood of being flagged by GSB, but at the same
time, a large number of users are exposed to the page.
Unsurprisingly, we observe an immediate overall drop-off after
inclusion in the GSB database, which shows that GSB-based mitiga-
tions appear to be quite efficient. Nevertheless, we do see a number
of accesses even after a page has been included. These accesses
could be due to users ignoring warnings, or to their browsers not
using the GSB database to filter access. Particularly interesting to
us is that accesses to malware-infested pages do not seem to sig-
nificantly decrease after inclusion in the GSB database. This could
plausibly indicate that users are deliberately visiting such pages
(bypassing defenses in the process), or that, once infected with
malware, a device keeps on (unbeknownst to the user) requesting
HTTP content from malicious locations.
02,0004,0006,0008,000−100−50050100Days from first observation in GSBNumber of accesses (absolute)Deceptive pagesMalware pagesSession 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1491We observed close to three months of potential, unprotected
exposure, to malicious pages, which could have led to successful
infections or credential theft. The probability of exposure increases
sharply about three weeks prior to pages’ blacklisting, and even
more sharply in the couple of days prior to inclusion. To summarize:
Finding 1. There is evidence of non-negligible delays—potentially