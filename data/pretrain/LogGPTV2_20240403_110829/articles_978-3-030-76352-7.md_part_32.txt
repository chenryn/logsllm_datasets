dictionary from the tokens, representing the vocabulary of all of the tokens in
the logs - D logswords is created. Since the log templates can have a different
number of tokens, for the uniform representation of the log templates a special
 token is added, such that each of the logs has an equal number of
tokens. The maximal size of the log template is limited by a parameter called
max log size.
L i ={W 0i,W 1i,...,W ti} (1)
where each of the W t is an extracted word mapped to index t ∈
D logswordindecies.
Multi-source Anomaly Detection in Distributed IT Systems 205
Distrubted Traces. Distributed traces are a request-centred way to describe
behaviourwithinthedistributedsystem.Itmeansthattheyfollowtheexecution
of the user issued a request through the distributed system in a record referred
to as spans. The spans represent information (e.g. start time, end time, service
name, HTTP path) about the operations performed when handling an external
request in service. Formally, a trace is written as
T i ={S 0i,S 1i,...,S mi }, (2)
where i∈{1,...,N} is a trace as part of an observation set of traces, and m is
the length T i or the number of spans in the trace.
Oneofthemostcharacteristicpropertiesofthespansisthefunctionexecuted
during the event and a corresponding endpoint. They usually represent either
HTTP or RPC calls, denoting the interconnection between the spans within
the trace. The HTTP calls are described with path, scheme, method. The RPC
calls are represented with the functions they are executing. Since these features
represent the intra-service communication in a trace, we assume that they are
sufficient for structural analysis of possible anomalies. To provide a richer rep-
resentation of the traces, further augmentation of the traces can be done. More
specifically,twoartificialspans(and)areaddedtothebeginning
and the end of the trace, accordingly. It preserves the knowledge for the length
of the trace.
Represented in this form the spans have very similar representation as to
the logs, with additional constraints that the spans are further bounded by the
operation executed within the trace. It means that they also are facing the
problem of the presence of noise into the representation induced by the varying
parameters. Similar as for the logs, applying a template extraction technique
produces a set of representative template spans. It allows for each of the trace
to be represented as a sequence of template spans. Formally,
T i ={Sti 0,Sti 1,...,Sti k} (3)
where each of the St k is an extracted template mapped to index k ∈
D template indecies.
Observingthateachfunctioncallsaresequencesofcharacters,adictionaryof
thesequencesofcharactersappearinginsidethegivensetoftracesisconstructed-
D span words.Itprovidesauniquelanguageforthedescriptionofallofthespans
appearing in the observed traces. Formally a span is represented as
St j ={W 0i,W 1j,...,W qj} (4)
where W q is a sequence of characters as extracted from the dictionary of span
words D span words. Since there are spans with a different number of words, to
providespansinanappropriaterepresentationalformatforlaterprocessing,each
of the spans is augmented with a  token.
206 J. Bogatinovski and S. Nedelkoski
3.2 NTP: Pseudo-task for Anomaly Detection
Representationofbothtracesandlogsinthepreviouslydescribedmanner,allow
ustotakeaunifiedapproachtowardstheirmodelling.Theappearanceofthenext
log message is conditioned on the appearance of the history of the previous logs.
Similarly, within a trace, the appearance of the next span is conditioned on the
previousones.Thusthemodellingproblemcanbeconceptualizedformallyas
(cid:2)T
P(A Twin:T)= P(A t|A <t) (5)
t=Twin
where A <t denotes the templates traces or logs from A t−win to A t, with win
denotingthesizeofthepreservedhistory.Hencewerefertothistaskasthenext
template prediction (NTP).
3.3 Single Modality Anomaly Detection
Figure 1 depicts the proposed end to end architecture to solve the NTP task for
single modalities. We use the same architecture for both the logs and the traces.
Fig.1. Proposed architecture for single modality. The same approach can be utilized
also for the logs data.
At the input, we provide the dictionary of the words as appearing in
D logswords and D spanwords. We perform initialization with random vectors for
eachofthewordswithaspecificsize.Thisisaparameterofthemethodreferred
embedding size N embedding. The template embedding layer uses the representa-
tions of the words to create the corresponding sequences of templates. These
sequences are fed through an autoregressive deep learning LSTM method that
Multi-source Anomaly Detection in Distributed IT Systems 207
is modelling the sequential dependence between the input samples represented
with f(x). Its output is used to calculate the softmax between the real next
template and the output of the network. The softmax is calculated as
ef(x)
P(f(x))= (6)
(cid:3)A
efi(x)
i=1
It calculates a distribution over the all possible templates. The one with the
maximal probability is considered the most likely template to appear given the
input sequence of templates.
LSTM architecture is a deep learning neural network method used for effi-
cientlymodellingsequentialdata.Therepresentationofthesystemstateisgiven
viaasinglevector,refertoasahiddenstate.Theassumptionthemethodismak-
ing, builds on top of the Markov property. It states that the state of the system
at any particular point in time can be determined just from the previous state.
To achieve this goal, it utilizes a selection mechanism build on abstractions of
input,outputandforgetgates.Thismechanismallowsthenetworktoselectively
choose how much information from the previous inputs it should preserve and
distribute towards the output. Hence it can model short and long term depen-
dencieswithinasequenceandthestructureappearingintothesequenceofstate
events. Thus it is a handy solution for modelling our problem. Stacking of mul-
tiple LSTM cells provides greater representational power of the architecture.
Fig.2. Proposed architecture for joint analysis of logs and traces.
3.4 Multimodal LSTM
To account for both modalities and enable end to end learning system for
anomaly detection, we propose the method as given on Fig. 2. It is composed
of two models described in the previous section. On the inputs provided are the
dictionary of logs and spans, simultaneously, to each of the two models. How-
ever,theoutputofbothLSTMsisconcatenatedtooneanotherandfedthrough
an additional linear layer. It gives an advantage of including the information
208 J. Bogatinovski and S. Nedelkoski
from both of the modalities, to improve the predictive performance. The shared
informationfromtheconcatenationisthenpassedthroughtwolinearlayers,one
accounting for the traces and the other for the logs.
Toaccountforbothmodalitiesthecostfunctionisalsochanged.Wecalculate
itasajointcross-entropylossofthemostlikelyspanandlogtoappear,giventhe
joint information in a particular period. We calculated the joint loss as follows:
L((s,l),f(x,y))=L(f(x),s)+L(f(y),l) (7)
where L(·,·) account for the categorical-cross entropy loss, and s and l for the
ground truth span and log templates that should appear as the next relevant
templates. Because the loss function includes the information from both modal-
ities when the back-propagation step is done the gradients are calculated based
on the information from both of the modalities.
One important detail for joint training the two modalities is providing the
information from the same time intervals to the model from both of the modali-
ties. The granularity representation of a log message is on a single time interval,
on one side, and the spans span across multiple time stamps. To address this
challenge we address block of logs of varying size. The size of a block of log mes-
sagesisdependentonthecorrespondingspanswithinthetraceappearingduring
the particular time interval. To create a block of log messages we stack multiple
logstogethertopairupwiththecorrespondingtimeintervalsdeterminedbythe
spans. Such an approach requires the introduction of a maximal number of logs
that are considered at once.
Giventhiscouplingbetweenthetracesandlogs,thequestiontoaskis“What
isthelearningtaskforthejointmethod?”.Sincethetimespanningofthespans
determine the size of log blocks, just a window size parameter on the traces
imposed is. This parameter determines the number of spans the method should
use to produce the next one. The block of log messages is created in a way that,
the log messages that come from the start time of the first and the end time
of the last span in the window of spans are joined into one block. The target is
to predict the next expected log. An additional complication that can arise is
the absence of logs in a particular time frame. To address this, we denote those
windows that have a missing target and drop them from the learning set.
3.5 Anomaly Detection
NTP is utilized for anomaly detection for logs, however, the anomaly detection
inthetracesrequireadditionalanomalydetectionprocedure.Wefurtherprovide
a simple and effective method that acts on the output from the NTP solver to
detect if there is an anomaly or not. The anomaly detection procedure for the
single modality log model considers a log as normal if the prediction for the log
is in the next top k logs. Otherwise, it is predicted as an anomaly.
For the detection of anomalous trace, the decision procedure should take
into consideration the correct prediction among all of the spans in the trace
subjecttoprediction.Aspaniscorrectlypredictedif,foragiveninputsequence
Multi-source Anomaly Detection in Distributed IT Systems 209
of spans, the true span is in the top k span ranked spans. For each trace, this
procedure creates an accumulation of the correctly predicted spans. The ratio
numerr
of incorrectly predicted spans (span error rate) is considered as an
length(trace)
anomaly score for the trace. Setting a threshold on this score can be used for
anomaly detection. Finally, for the joint multimodal method, a combination of
the previously described techniques is utilized.
4 Experiments and Results
In this section, we first describe the experimental design we used for evaluation.
Second, we provide a detailed analysis of the results from the experiments to
justify the improvements the joint information provides. Finally, we discuss the
span2vec embedding as a consequence and further contribution of this work.
4.1 Experiments
Dataset Preprocessing Details. In the experiments we used the publicly
availabledataset1 coveringthetraceandlogsasmonitoringcomponentsinover-
lapping time intervals. To the best of our knowledge, this is the only available
dataset suited for multi-modal anomaly detection in distributed systems and as
such it is utilized.
The experiments are generated from an OpenStack deployment testbed. We
used the concurrent execution scenario, with 3 execution workloads: create an
image, create a server, create a network, as described in [8]. As such we demon-
strate the usefulness of our method in scenarios as close to real-world execution.
Train Test Split. The training dataset is composed of the traces appearing
up to a particular time point, such that 70% of the normal traces are contained.
The anomalous traces during this time-window are discarded. The logs that
belong in the corresponding time intervals as generated by the trace are also
preserved in the training set. We aim of modelling the normal behaviour of
the system with preserving the normal traces and normal logs. To evaluate our
model,thetestsetiscomposedofalloftheremaininglogsandtracesappearing
after the split time point.
Baselines. Themainaimofthisworkistodemonstratethatthesharedinforma-
tion between the logs and traces can improve anomaly detection in comparison
to anomaly detection methods build from single modalities. As baselines we use
the single modality LSTM method build separately for the traces and logs. The
models are built on the same dataset as the multi-modal model and tested on
the same test set to allow for a fair comparison.
1 https://zenodo.org/record/3549604.
210 J. Bogatinovski and S. Nedelkoski
Table 1. Results from the experimental evaluation.
Score Logs-joint Trace-joint Single logs Single traces
Accuracy 0.976 0.990 0.974 0.955
Precision 0.904 0.992 0.897 0.992
Recall 0.996 0.984 0.996 0.909
f1 0.948 0.988 0.944 0.949
Implementation Details. The first step of the data preprocessing requires
settings the values for the Drain parser. The values for the similarity and depth
were set to 0.5, 0.4 and 4, 4, for the logs and traces accordingly. These values
provideaconcisetemplateasevaluatedbythedomainexpert.TheN embedding
is set to 256. For the window size parameter for the traces the value is set to
3. For optimization of the cost functions for the single and multiple modalities
methods, we use SGD solver with standard values for thelearning rate=0.001
andmomentum=0.9.Thebatch sizeissetto256asacommonlychosenvalues.
The number of epochs is 100 for all of the tested methods.
For the anomaly detection procedure we further require the logs top k and
trace top k parameters. They are set to 20 and 1 accordingly. For the error
threshold on the anomaly score, the best value between 0.05 and 1 with a step
of 0.05 chosen is.
4.2 Results
Table 1 summarize the results from the experiments. Firstly, one can observe
that the results from the single modalities methods show that for the logs and
traces, individually the approach can provide good results. It shows that the
assumption made by the NTP task solver is sufficient for successful modelling
of the normal state of the system.
ComparisonoftheresultsfromthecolumnsTrace-jointandTrace-singlesug-
gestthatthereisanimprovementoftheresultsforthetracesforthemultimodal
method.Morespecifically,therecanbeobservedimprovedvalueontherecallfor
thejointmodelforthetracesincomparisontothesingleone.Thissuggeststhat
the addition of the additional information from the logs can increase the num-
ber of correct predictions for the anomalous traces. The improvement is further
depicted in the increased value for the F1 score on the joint traces. The results
onthelogsdonotseemthatchangetoomuch.Oneexplanationofthisbehaviour
is that the granularity of the information from the logs is truncated on the level
of the data source with a lower frequency of generation - the trace is harder for
the information in the trace to be transferred to the logs. The information that
the multimodal method is receiving from the logs when it is aiming to predict
the next relevant span complements the information as obtained just from the
sequence of spans individually.
Multi-source Anomaly Detection in Distributed IT Systems 211
Fig.3. Span2Vec embedding of the events in the tracing data from the whole vocabu-
lary of spans for the three different workloads.
4.3 Span2Vec and Log2Vec
One element of the method is the ability to learn to embed both the logs and
spans. The logs and spans are composed of words represented as vectors. The
vectors are learned during the optimization procedure. Hence are optimized for
the specific NTP task. Since the logs and spans are linear combinations from
thesewords,poolingoverthewordsbelongingtothesamespan/logcanbeused
to provide a unique vector mapping for them.
Figure 3 depicts a two-dimensional representation of the vector space of
the spans embeddings. Three operations are executed. Close observation reviles
that spans that are specific for a workload occur close to one another, while
the ones that are shared co-occur in groups of their owns. For example, the
spans GET /v2.0/images/, PUT /v2.0/image/, GET /v2/images/ and POST
/v2.0/networks/ are unique for create delete image workload. As it can be
observed, these spans are very close to one another in comparison to the other
spanslikethepairPOST /v2.0/networks andDELETE/v2.0/network/.Onthe
other side, the artificially added spans like START and STOP or the authen-
tication span each of the workloads is utilizing are grouped, separated from
the workload-specific spans. Close inspection of the Euclidean distance between
the spans confirms the observations from the TSNE vector representation. The
importanceoftheseembeddingsisthemostemphasisedintheirfuturereusefor
warmstartingthemethods.Thiscanreducetheadoptiontimeandthedifficulty
when a new machine model is deployed in production.
212 J. Bogatinovski and S. Nedelkoski