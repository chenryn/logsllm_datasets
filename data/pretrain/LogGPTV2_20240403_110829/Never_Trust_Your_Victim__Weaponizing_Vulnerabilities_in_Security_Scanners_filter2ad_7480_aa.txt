title:Never Trust Your Victim: Weaponizing Vulnerabilities in Security Scanners
author:Andrea Valenza and
Gabriele Costa and
Alessandro Armando
Never Trust Your Victim: Weaponizing Vulnerabilities in Security Scanners
Andrea Valenza
University of Genova
PI:EMAIL
Gabriele Costa
IMT School for Advanced Studies Lucca
PI:EMAIL
Alessandro Armando
University of Genova
PI:EMAIL
Abstract
The ﬁrst step of every attack is reconnaissance, i.e., to ac-
quire information about the target. A common belief is that
there is almost no risk in scanning a target from a remote
location. In this paper we falsify this belief by showing that
scanners are exposed to the same risks as their targets. Our
methodology is based on a novel attacker model where the
scan author becomes the victim of a counter-strike. We devel-
oped a working prototype, called RevOK, and we applied it to
78 scanning systems. Out of them, 36 were found vulnerable
to XSS. Remarkably, RevOK also found a severe vulnerability
in Metasploit Pro, a mainstream penetration testing tool.
1 Introduction
Performing a network scan of a target system is a surpris-
ingly frequent operation. There can be several agents behind
a scan, e.g., attackers that gather technical information, pen-
etration testers searching for vulnerabilities, Internet users
checking a suspicious address. Often, when the motivations
of the scan author are unknown, it is perceived by the target
as a hostile operation. However, scanning is so frequent that it
is largely tolerated by the target. Even from the perspective of
the scanning agent, starting a scan seems not risky. Although
not completely stealthy, an attacker can be reasonably sure
to remain anonymous by adopting basic precautions, such as
proxies, virtual private networks and onion routing.
Yet, expecting an acquiescent scan target is a mere assump-
tion. The scanning system may receive poisoned responses
aiming to trigger vulnerabilities in the scanning host. Since
most scanning systems generate an HTML report, scan au-
thors can be exposed to attacks via their browser. This occurs
when the scanning system permits an unsanitized ﬂow of in-
formation from the response to the user browser. To illustrate,
consider the following, minimal HTTP response.
HTTP /1.1 200 OK
Server : nginx /1.17.0
. . .
A naive scanning system might extract the value of the
Server ﬁeld (namely, the string nginx/1.17.0 in the above
example) and include it in the HTML report. This implicitly
allows the scan target to access the scan author’s browser and
inject malicious payloads.
In this paper we investigate this attack scenario. We start
by deﬁning an attacker model that precisely characterizes
the threats informally introduced above. To the best of our
knowledge, this is the ﬁrst time that such an attacker model is
deﬁned in literature. Inspired by the attacker model, we de-
ﬁne an effective methodology to discover cross-site scripting
(XSS) vulnerabilities in the scanning systems and we imple-
ment a working prototype. We applied our prototype to 78
real-world scanning systems. The results conﬁrm our expec-
tation: several (36) scanning systems convey attacks. All of
these vulnerabilities have been notiﬁed through a responsible
disclosure process.
The most remarkable outcome of our activity is possibly
an XSS vulnerability enabling remote code execution (RCE)
in Rapid7 Metasploit Pro. We show that the attack leads to
the complete takeover of the scanning host. Our notiﬁcation
prompted Rapid7 to undertake a wider assessment of their
products based on our attacker model.
The main contributions of this paper are:
1. a novel attacker model affecting scanning systems;
2. a testing methodology for ﬁnding vulnerabilities in scan-
ning systems;
3. RevOK, a prototype implementation of our testing
methodology;
4. an analysis of the experimental results on 78 real-world
scanning systems, and;
5. three application scenarios highlighting the impact of
our attacker model.
This paper is structured as follows. Section 2 recalls some
preliminary notions. Section 3 presents our attacker model.
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    17
Figure 1: Abstract architecture of a scanning system.
We introduce our methodology in Section 4. Our prototype
and experimental results are given in Section 5. Then, we
present the three use cases in Section 6, while we survey on
the related literature in Section 7. Finally, Section 8 concludes
the paper.
2 Background
In this section we recall some preliminary notions necessary
to correctly understand our methodology.
2.1 Scanning systems
A scanning system is a piece of software that (i) stimulates
a target through network requests, (ii) collects the responses,
and (iii) compiles a report. Security analysts often use scan-
ning systems for technical information gathering [8]. Scan-
ning systems used for this purpose are called security scan-
ners. Our deﬁnition encompasses a wide range of systems,
from complex vulnerability scanners to simple ping utilities.
Figure 1 shows the key actors involved in a scan process.
Human analysts use a user agent, e.g., a web browser, to select
a target, possibly setting some parameters, and start the scan
(1. start). Then, the scanning system crafts and sends request
messages to the target (2. request). The scanning system
parses the received response messages (3. response), extracts
the relevant information and provides the analyst with the
scan result (4. report). Finally, the analysts inspect the report
via their user agent.
Whenever a scanning system runs on a separate, remote
scanning host, we say that it is provided as-a-service. Instead,
when the scanner and scanning hosts coincide, we say that
the scanning system is on-premise.
A popular, command line scanning system is Nmap [25].
To start a scan, the analyst runs a command from the command
line, such as
nmap -sV 172.16.1.26 -oX report.xml
Then, Nmap scans the target (172.16.1.26) with requests
aimed at identifying its active services (-sV). By default,
Nmap sends requests to 1,000 frequently used TCP ports and
collects responses from the services running on the target.
The result of the scan is then saved (-oX) on report.xml.
Interestingly, some web applications, e.g., Nmap Online [15],
provide the functionalities of Nmap as-a-service.
Scanning systems are often components of larger, more
complex systems, sometimes providing a browser-based GUI.
For instance, Rapid7 Metasploit Pro is a full-ﬂedged pen-
etration testing software. Among its many functionalities,
Metasploit Pro also performs automated information gath-
ering, even including vulnerability scanning. The reporting
system of Metasploit Pro is based on an interactive Web UI
used to browse the report.
2.2 Taint analysis
Taint analysis [26] refers to the techniques used to detect
how the information ﬂows within a program. Programs read
inputs from some sources, e.g., ﬁles, and write outputs to some
destinations, e.g., network connections. For instance, taint
analysis is used to understand whether an attacker can force a
program to generate undesired/illegal outputs by manipulating
some of its inputs. A tainted ﬂow occurs when (part of) the
input provided by the attacker is included in the (tainted)
output of the program. In this way, the attacker controls the
tainted output which can be used to inject malicious payloads
to the output recipient.
2.3 Cross-site scripting
Cross-site scripting (XSS) is a major attack vector for the
web, stably in the OWASP Top 10 vulnerabilities [12] since
its initial release in 2003. Brieﬂy, an XSS attack occurs when
the attacker injects a third-party web page with an executable
script, e.g., a JavaScript fragment. The script is then executed
by the victim’s browser. The simplest payload for showing
that a web application suffers from an XSS vulnerability is
that causes the browser to display an alert window. This pay-
load is often used as a proof-of-concept (PoC) to safely prove
the existence of an XSS vulnerability.
There are several variants to XSS. Among them, stored
XSS has highly disruptive potential. An attacker can exploit a
stored XSS on a vulnerable web application to permanently
save the malicious payload on the server. In this way, the
attack is directly conveyed by the server that delivers the in-
jected web page to all of its clients. Another variant is blind
XSS, in which the attacker cannot observe the injected page.
For this reason, blind XSS relies on a few payloads, each
adapting to multiple HTML contexts. These payloads are
called polyglots. A remarkable example is the polyglot pre-
sented in [11] which adapts to at least 26 different contexts.
3 Attacker model
The idea behind our attacker model is sketched in Figure 2
(bottom), where we compare it with a traditional web secu-
rity attacker model (top). Typically, attackers use a security
scanner to gather technical information about a target appli-
cation. If the application suffers from some vulnerabilities,
18    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Figure 2: Comparison between attacker models.
Figure 3: Phase 1 – ﬁnd tainted ﬂows.
attackers can exploit them to deliver an attack towards their
victims, e.g., the application users. On the contrary, in our
attacker model attackers use malicious applications to attack
the author of a scan, e.g., a security analyst.
Here are the two novelties of our attacker model.
1. Attacks are delivered through HTTP responses instead
of requests.
2. Attackers exploit the vulnerabilities of scanning systems
to strike their victims, i.e., the scan initiator.
Below, we detail the attacker’s goal and capabilities.
Attacker goal. The objective of the attacker is to directly
strike the analyst. To do so, the attacker exploits the vulnera-
bilities of the target scanning system and its reporting system
to hit the analyst user agent. In this work, we assume that the
user agent is a web browser. This assumption covers every
as-a-service scanning system, as well as many on-premise
ones, which generate HTML reports. As a consequence, here
we focus on XSS which is a major attack vector for web
browsers. As usual in XSS, the attacker succeeds when the
victim’s browser executes a piece of attacker-provided code,
e.g., JavaScript.
Attacker capabilities. First, we state that the attacker has
adequate resources to detect vulnerabilities in scanning sys-
tems before deploying the malicious application. However,
the attacker capabilities do not include the possibility of ob-
serving the internal logic of the scanning system. That is, our
attacker operates in black-box mode.
Secondly, our attacker has complete control over the ma-
licious application, e.g., the attacker owns the scanned host.
However, we do not assume that the attacker can force the
victim to initiate the scanning process.
4 Testing methodology
In this section, we deﬁne a vulnerability detection methodol-
ogy based on our attacker model.
4.1 Test execution environment
Our methodology relies on a test execution environment (TEE)
to automatically detect vulnerabilities in scanning systems. In
particular, a test driver simulates the user agent of the security
analyst, while a test stub simulates the scanned application.
Our TEE can (i) start a new scan, (ii) receives the requests
of the scanning system, (iii) craft the responses of the target
application, and (iv) access the report of the scanning system.
Intuitively, the TEE replicates the conﬁguration of Figure 1.
In this conﬁguration, the test driver is executed by the scanner
host, and the test stub runs on the scanned host. In general, the
test driver is customized for each scanning system under test-
ing. For instance, it may consist of a Selenium-enabled [14]
browser stimulating the web UI of the scanning system.
Both the test driver and the test stub consist of some sub-
modules. These submodules are responsible for implementing
the two phases described below.
4.2 Phase 1: tainted ﬂows enumeration
The ﬁrst phase aims at detecting the existing tainted destina-
tions in the report generated by the scanning system. Having
a characterization of the tainted ﬂows is crucial to deal with
the input transformation logic of the target scanning system.
In general, since payloads may be arbitrarily modiﬁed before
being displayed in the report, detecting actual injections is
non-trivial. Instead, through this phase, injections can be de-
tected just by monitoring tainted destinations. The process is
depicted in Figure 3. Initially, the test driver asks the scanning
system to perform a scan of the test stub. The scan logic is
not exposed by the scanning system and, thus, it is opaque
from our perspective. Nevertheless, it generates some requests
toward the test stub. Each request is received by the scan fron-
tend and dispatched to the response generator, which crafts
the response.
The response generation process requires special attention.
One might think that a single, general-purpose response is
sufﬁcient. However, some scanning systems process the re-
sponses in non-trivial ways. For instance, they may abort
the scan if a malformed or suspicious response is received.
For this reason, we proceed as follows. First, we generate a
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    19
response template, i.e., an HTTP response containing vari-
ables, denoted by t. Response templates are generated from a
fuzzer through a probabilistic context-free grammar (PCFG).
A PCFG is a tuple (N,Σ,R,S,P), where G = (N,Σ,R,S) is a
context-free grammar such that N is the set of non-terminal
symbols, Σ is the set of terminal symbols, R are the production
rules and S is the starting symbol. The additional component
of the PCFG, namely P : R → [0,1], associates each rule in R
with a probability, i.e., the probability to be selected by the
fuzzer generating a string of G. Additionally, we require that
P is a probability distribution over each non-terminal α, in
symbols
∀α ∈ N. ∑
(α(cid:55)→β)∈R
P(α(cid:55)→β) = 1
In the following, we write α (cid:55)→p β for P(α (cid:55)→ β) = p and
α (cid:55)→p1 β1|p2 . . .|pnβn for α (cid:55)→p1 β1, . . . ,α (cid:55)→pn βn,α (cid:55)→pe ""
(where "" is the empty string).
The probability values appearing in our PCFG are assigned
according to the results presented in [20, 21]. There, the au-
thors provide a statistical analysis of the frequency of real
response headers as well as a list of information-revealing
ones. Such headers are thus likely to be reported by a scanning
system. Finally, when the frequency of a ﬁeld is not given
(e.g., for variables), we apply the uniform distribution.
An excerpt of our PCFG is given in Figure 4. For the sake of
presentation, here we omit some of the rules and we refer the
interested reader to the project web site1. The grammar deﬁnes
the structure of a generic HTTP response (Resp) made of a
version (Vers), a status (Stat), a list of headers (Head), and
a body (Body). Variables t are all fresh and they can appear in
several parts of the generated response template. In particular,
variables can be located in status messages (i.e., Succ, Redr,
ClEr and SvEr), header ﬁelds (i.e., Serv, PwBy, Locn, SetC,
CntT, AspV, MvcV, Varn, StTS, CnSP, XSSP and FrOp) and
body. For instance, a ﬁeld can be Server: nginx/t, where
nginx/ is a server type (SrvT, omitted for brevity).
The response template is then populated by replacing each
variable with a token. A token is a unique sequence of char-
acters that is both recognizable, i.e., it has a negligible prob-
ability of appearing by chance, and uninterpreted, i.e., the
browser treats it as plain text, when appearing in an HTML
document. All tokens are mapped to the responses containing
them. Responses are stored in a database. Finally, the test
driver matches the tokens appearing in the responses database
with those occurring in the scan report. Such tokens are evi-
dence that there are tainted ﬂows in the internal logic of the
scanning system. Tokens mark the source and the sink of a
ﬂow in the response and report, respectively. All these tokens
are stored in the tainted tokens database.
|0.114 "302 Found " |0.114 "302" t
Resp (cid:55)→1 Vers Stat Head Body
Vers (cid:55)→0.5 " HTTP /1.0" |0.5 " HTTP /1.1"
Stat (cid:55)→0.554 Succ |0.427 Redr |0.013 ClEr |0.006 SvEr
Succ (cid:55)→0.5 "200 OK " |0.5 "200" t
Redr (cid:55)→0.386 "301 Moved Permanently " |0.386 "301" t
ClEr (cid:55)→0.26 "403 Forbidden " |0.26 "403" t
|0.24 "404 Not Found " |0.24 "404" t
SvEr (cid:55)→0.5 "500 Internal Server Error " |0.5 "500" t
Head (cid:55)→1 Serv PwBy Locn SetC CntT AspV MvcV Varn
Serv (cid:55)→0.475 " Server :" t |0.475 " Server :" SrvT t
PwBy (cid:55)→0.24 "X - Powered - By : php "
Locn (cid:55)→0.315 " Location :" Link |0.315 " Location :" t
Link (cid:55)→0.516 " https ://" t
(cid:44)→ StTS CnSP XSSP FrOp
|0.24 "X - Powered - By :" t
|0.167 " http ://" t ":8899"
|0.135 " http ://" t ":8090"
|0.065 " http ://" t "/ login . lp "
|0.059 "/ nocookies . html "
|0.058 " cookiechecker ? uri =/"
SetC (cid:55)→0.175 "Set - Cookie :" Ckie
Ckie (cid:55)→0.471 " __cfduid =" t |0.394 " PHPSESSID =" t
|0.087 " ASP . NET Session =" t
|0.048 " JSESSIONID =" t
|0.07 "X - Content - Type - Options :" t
CntT (cid:55)→0.07 "X - Content - Type - Options : nosniff "
AspV (cid:55)→0.5 "X - AspNet - Version :" t
MvcV (cid:55)→0.5 "X - AspNetMvc - Version :" t
Varn (cid:55)→0.5 "X - Varnish :" t
StTS (cid:55)→0.5 " Strict - Transport - Security :" STSA
STSA (cid:55)→0.111 "max - age =" N+
|0.111 "max - age =" t
|0.111 "max - age =" N+ "; preload "
|0.111 "max - age =" t "; preload "
Figure 4: Response template grammar (excerpt).
4.3 Phase 2: vulnerable ﬂows identiﬁcation
The second phase aims to conﬁrm which tainted ﬂows are
actually vulnerable. We use PoC exploits to conﬁrm the vul-
nerability. The workﬂow is depicted in Figure 5. As for the
ﬁrst phase, the test driver launches a scan of the test stub.
When the test stub receives the requests, the exploit builder
extracts a response from the responses database. Then, the
response is injected with a PoC exploit. More precisely, a
tainted token is selected among those generated during Phase
1. The tainted token in the response is replaced with a payload
taken from a predeﬁned injection payload database. In gen-
eral, a vulnerability is conﬁrmed by the test driver according
to predeﬁned, exploit-dependent heuristics. Although tainted