The remainder of this article is organized as follows. Section 2 describes previous related re-
viewstudiesinAIOpsandfailuremanagement.Section3presentsthemethodologyofthiswork,
summarizingtheplanningchoicesofsystematicmappingstudyusedtoidentifythepaperslater
presented,aswellastheterminologyandmetricconventions.Section4delineatesthestructure
offailuremanagementinAIOpsandpresentsaselectionofpapersdividedinthematicsections,
describingtheirlineofwork,contribution,approach,inputsource,targetcomponent,andappli-
cationfield.Section5summarizestheresultsandoutcomesdrawnfromourdiscussion.
2 RELATEDWORK
As the AIOps areas mentioned above are large both in number and range of applications, it is
reasonabletoexpectnumerousworksfocusingonfilteringandcategorizingbestapproachesand
practices. Several other surveys and mapping studies have been in fact conducted in the areas
covered by AIOps [22, 24, 39, 46, 47, 60, 63, 70, 71, 91, 99, 106, 109, 111, 112, 122, 125, 130, 139,
146,152].However,nopreviousworkhasprovidedanupdated,comprehensivereviewofAIOps
approachesforfailuremanagement.
Table2summarizesthemostrelevantsurveyandsystematicreviewcontributionsregardingIT
OperationsandArtificialIntelligence,organizedbymaintopicandotherfocuses.Mostworkstreat
singletasks[22,24,39,60,63,91,106,111,112,122,139,152]orgeneralgoals[46,47,125,130,146]
insideAIOps,whicharespecifictoparticularinterventionmethods.
Asecondcategoryofworks[70,71,99,109]treatfailuremanagementintegrally,butsomeof
theworksinsidethisgroupareoutdatedanddonotreflectthecurrentprogressofthefield,while
themostrecentworksdonotfocusonAI-basedapproachesordonotofferacomprehensivelist
ofcontributions.TheclosestmatchtoouranalysisisrepresentedbytheworkofMukwevhoand
Celik[99],whopresentasurveyonfaultmanagementincloudsystems.Differentlyfromthem,we
donotfocusonanyparticularcomputingsystem,andwefocusonthemanifestationoffaults(i.e.,
errors and failures) rather than root causes (see Section 3.3 for terminology). We also choose to
integrateAIandMachinelearningapproachesintheconventionalschemeoffailuremanagement
approaches,ratherthantreatingtheminaseparatecategory.Alltheseconsiderations,including
theobservationsaboutthemissingstructureandterminologyconventionspresentedinSection1,
motivatetheneedforanin-depthstudyinthisarea,liketheoneherepresented.
3 METHODOLOGY
3.1 SystematicMappingStudy
Asystematicmappingstudy(SMS)wasconductedtoobtainrelevantandrepresentativeliter-
ature in the field of AIOps. Different from a systematic literature review (SLR), the ultimate
goalofasystematicmappingstudyistoprovideanoverviewofaspecificresearcharea,toobtain
asetofrelatedpapers,andtodelineatetrendspresentinsidesucharea[67].Relevantpapersare
collectedviawell-definedsearchandselectioncriteria,whileresearchtrendsareidentifiedusing
categorizationschemescoveringdifferentaspects,e.g.,suchmaintopic,origin,ortypeofcontri-
bution. We choose this instrument because we are interested in gathering contributions for the
surveyandobtaininginsightsintothefield,suchasthedistributionofworksindifferentAIOps
subareas and the temporal evolution of the interest toward specific topics. An in-depth discus-
sionofthemappingstudymethodology,thecategorizationscheme,andtheselectionstrategyof
contributionisavailableinaworkseparatelypublished[113],accessibleonline.1
1https://arxiv.org/abs/2012.09108.
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
81:6 P.Notaroetal.
Table2. RelatedAISurveysandSystematicMappingStudies(SMS)Conductedin
AreasCoveredbythisWork
Ref. Year Type MainTopic Focuses
ITOperations
[71] 2007 Survey ITOperations AI,OperationalResearch
[70] 2011 Survey ITOperations AI,OperationalResearch
[109] 2013 SMS FailureManagement Temporal&GeographicalTrends,ServiceLevel,Others
[99] 2018 Survey FailureManagement CloudComputing
FailurePrevention
[112] 2011 Survey FailurePrevention CombinatorialTesting
[39] 2015 Survey FailurePrevention/Detection Software
[106] 2016 Survey FaultInjection Software
[91] 2017 Survey SoftwareDefectPrediction MachineLearning,PROMISEdataset
FailurePrediction
[125] 2007 Survey FailurePrediction AI,IntegratedSystems
[152] 2007 Survey FailurePrediction Clusters
[122] 2010 Survey FailurePrediction OnlineMethods
[63] 2019 Survey FailurePrediction HighPerformanceComputing
FailureDetection
[60] 2015 Survey AnomalyDetection BottleneckIdentification
[24] 2009 Survey AnomalyDetection -
[22] 2019 Survey AnomalyDetection DeepLearning
[139] 2013 Survey AnomalyDetection Network
[111] 2008 Survey InternetTrafficClassification MachineLearning,IPNetworks
Root-causeAnalysis(RCA)
[130] 2017 Survey Root-causeAnalysis -
[146] 2016 Survey FaultLocalization Software
[46] 2015 Survey FaultDiagnosis Model-andsignal-basedapproachesinIndustrialSystems
[47] 2015 Survey FaultDiagnosis Knowledge-basedandhybridapproachesinIndustrialSystems
Thanks to our mapping study, we collected 1,086 AIOps contributions and inferred an AIOps
taxonomy based on thematic areas and commonly treated tasks (see Figure 2). Our taxonomy
groupscontributionsinthetwomainmacro-areas:failuremanagementandresourceprovisioning.
Each macro-area divides contributions into different categories, based on end-goals and target
problems.Wealsoclassifytherelevantpapersaccordingtothefollowingcategorizationaspects:
targetcomponents,inputdatasources,AImethods(seeTables4and8).
Forthefailuremanagementmacro-area,thefocusofoursurvey,wealsodivideapproachcat-
egories into proactive and reactive, based on the window of intervention (red box in Figure 2).
Moreover, we introduce a second level of categorization for failure management, which divides
eachcategoryintoseveralsubcategoriesbasedonthespecifictargetproblemsolved.Examplesof
subcategoriesaresoftwaredefectpredictionforfailureprevention(Section4.1.1)andlogenhance-
mentforfailuredetection(Section4.3.3).Foroursurveydiscussion,weselect100mostprominent
contributionsinfailuremanagementfromthetotalresultsetof1,086,coveringallcategoriesand
subcategoriesdefinedforthisfieldofstudy.Fortheexhaustivelistofpaperscovered,dividedinto
categoriesandsubcategories,seeTable4inthenextsection.
3.2 EvaluationMetrics
In our analysis, we provide quantitative results for the papers under investigation. This section
providesanoverviewoftheevaluationmetricsemployedforcomparisonthroughoutthesurvey
discussion.
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
ASurveyofAIOpsMethodsforFailureManagement 81:7
Fig.2. TaxonomyofAIOpsasobservedintheidentifiedcontributions.Intheredbox,thefocusofthissurvey.
Table3. ContingencyTableforPredictionTasks
PredictedClass
PositiveClass NegativeClass
PositiveClass TruePositives(TP) FalseNegatives(FN) ssalClaeR
NegativeClass FalsePositives(FP) TrueNegatives(TN)
Forscalarprediction(orregression)tasks,awidelyadoptedmetricistheMean-squaredError
(MSE),definedastheaveragesquareddifferencebetweentargetandpredictedvalues:
(cid:2)N (cid:3) (cid:4)
MSE = N1 y ipred −y i 2 . (1)
i
A measure adopted across all classification problems (software defect prediction, root-cause
diagnosis, recovery, etc.) is accuracy, i.e., the ratio of classified samples assigned to the correct
class. In some contexts, however, accuracy may appear as a misleading metric to evaluate the
qualityofprediction.Thisisthecase,forexample,forproblemswithahighpredominanceofone
class,wheretrivialmodelscanbeconstructedtoreachhighaccuracyjustexploitingdataskewness.
Asimilarconsiderationappliestodetectionproblems(analyzed,e.g.,inSections4.2and4.3),where
thepositiveclass,i.e.,thedetectedfailure,mayappearlessfrequentlythanthenegativeclass,even
though it constitutes the most critical aspect from an evaluation point of view. In such cases, it
iscommontoadoptmorerepresentativemeasures,derivedfromthenotionofcontingencytable
[122]:
Usingthisconvention,accuracycanbewrittenas
TP +TN
ACC = . (2)
TP +TN +FP +FN
Toquantifytheabilityofapredictortoidentifypositivesamplescorrectly,theprecisionmeasure
isusuallyemployed,whiletomeasuretheabilityofadetectortoreporttruepositivesamples,the
recallmeasure(alsoknownastruepositiverateorsensitivity)isused.Theyaredefinedasfollows:
TP TP
P = , R = . (3)
TP +FP TP +FN
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
81:8 P.Notaroetal.
Moreover,thefalse-positiverate (FPR,alsocalledfalsealarmdate),whichidentifiesthepro-
portionofwronglyreportedfailures,isdefinedasfollows:
FP
FPR = . (4)
FP +TN
Sinceprecisionandrecalldonottakeintoaccountthenumberoftruenegatives,somepapers
compareresultsintermsoftrue-negativerate(TNR,orspecificity)andrecall.Thetrue-negative
rateisdefinedasfollows:
TN
TNR = . (5)
TN +FP
Precisionandrecallcanoftenbetradedoffwitheachotherbyadjustingsensitivitythresholds
inside algorithms, so that an increase in precision can be obtained by reducing recall and vice
versa. One possibility to evaluate both measures at the same time is to use the F1-score (or F-
score/F-measure),computedastheharmonicmeanofprecisionandrecall:
P ·R
F =2· . (6)
1 P +R
Afinalpossibilityistousereceiveroperatingcharacteristic(ROC)curves,parametricline
plotswhichdescribethevariationoftwometricsinrelationtochangesinthesensitivitythreshold.
Precision-recallcurvesarepossible,butitiscommontoplottherecallagainstthefalse-positive
rate.FromthistypeofcurvestheAreaundertheROCcurve(AUCROC)measurecanbecom-
puted.AhigherAUCROCscorethenindicatesabetterclassifier.
3.3 Terminology
In our discussion, we also adopt a variety of error-related terms such as fault, failure and root
cause.Fromaterminologypointofview,weadopttheconventionofSalfneretal.[122]forthe
characterizationoffaultybehavior.Accordingthisconvention:
• errorsaredeviationsfromthecorrectsystemstate;
• failuresaremanifestationsofundesireddeviationsinthedeliveryofaservice;
• faults(orrootcauses)aretheprimarycausesofundesiredbehavior(i.e.,theerrors).
Moreover,weoftenencounterdifferenttermsrelatedtodatasourcesthatmayhaveanambigu-
ousmeaningdependingonthecontext,suchaslogsortraces.Tobeconsistentinourdiscussion,
wegrouptheobserveddatasourcesaccordingtothisconvention:
• sourcecoderepresentsanyunitofsoftwaresourcecodeusedasinputtoapredictionsystem,
independentlyoftheformandextension(e.g.,function,file,module,class,etc.);
• testingresourcescomprisetoolsusedtoperformin-andpost-releasesoftwaredebugging,in
particularunittestsuites,executionprofilesorrundescriptionreports;
• systemmetricsmeasurevariousnumericalquantitiesatthehardware,OS,softwareanden-
vironmentlevel,describingresourceutilizationandtheoverallprocessstateofthesystem;
• keyperformanceindicators(KPI)provideinformationaboutthestatusofservicesandthe
associatedrequirementsthatneedtobemetduringruntimeoperations.Theyquantitatively
measurethequalityofservedrequestswithparameterssuchaslatency,uptime,failurerate,
availability,andsoon;
• networktrafficisthecollectionofnetworkpacketsexchangedoverInternetbydifferenthosts.
Itincludesthepayloadandcontrolinformationsuchasports,addresses,protocolstandards
andotherparameters;
• topology isanyinformationdescribingthespatialrelationsinsideaworkingsystem,when
usedasainput;
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
ASurveyofAIOpsMethodsforFailureManagement 81:9
• incidentreportsarecollectedwiththehelpofservicedeskandinternalproblemmanagement
systems to identify common problems and facilitate resolution. Usually, they describe the
problemwithtextandcategoricalattributesandtheymaybeassociatedtoaresolutionteam
orroutingsequence;
• eventlogs(orsimplylogs)arecollectionsofhuman-interpretableprintingstatementsdescrib-
ingsoftwareeventsoccurringinruntimeoperations.Theyaretypicallystoredasindepen-
dentfilesandlogentries(i.e.,lines)areassociatedtoapredefinedformat(orlogkey);
• execution (distributed) traces are hierarchical descriptions of the modules and services in-
vokedtosatisfyauserrequest.Theyareusuallyannotatedwiththeservicenameorcategory
andthetimedurationofeachmodule(calledspan).
4 AIAPPROACHESINFAILUREMANAGEMENT