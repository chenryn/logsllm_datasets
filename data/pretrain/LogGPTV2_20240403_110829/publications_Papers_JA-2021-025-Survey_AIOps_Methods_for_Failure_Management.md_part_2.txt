### Article Structure

The remainder of this article is organized as follows. Section 2 reviews previous related studies in AIOps and failure management. Section 3 presents the methodology of this work, summarizing the planning choices of the systematic mapping study used to identify the papers, as well as the terminology and metric conventions. Section 4 delineates the structure of failure management in AIOps and presents a selection of papers divided into thematic sections, describing their line of work, contributions, approaches, input sources, target components, and application fields. Finally, Section 5 summarizes the results and outcomes drawn from our discussion.

### 2. Related Work

Given the broad scope and numerous applications of AIOps, it is reasonable to expect a significant body of work focusing on filtering and categorizing best practices and approaches. Several surveys and mapping studies have been conducted in areas covered by AIOps [22, 24, 39, 46, 47, 60, 63, 70, 71, 91, 99, 106, 109, 111, 112, 122, 125, 130, 139, 146, 152]. However, no previous work has provided an updated, comprehensive review of AIOps approaches for failure management.

Table 2 summarizes the most relevant survey and systematic review contributions regarding IT Operations and Artificial Intelligence, organized by main topic and other focuses. Most works address single tasks [22, 24, 39, 60, 63, 91, 106, 111, 112, 122, 139, 152] or general goals [46, 47, 125, 130, 146] within AIOps, which are specific to particular intervention methods.

A second category of works [70, 71, 99, 109] treats failure management integrally, but some of these works are outdated and do not reflect the current progress in the field. The most recent works either do not focus on AI-based approaches or do not offer a comprehensive list of contributions. The closest match to our analysis is the work of Mukwevho and Celik [99], which presents a survey on fault management in cloud systems. Unlike their work, we do not focus on any particular computing system and instead focus on the manifestation of faults (i.e., errors and failures) rather than root causes (see Section 3.3 for terminology). We also choose to integrate AI and machine learning approaches into the conventional scheme of failure management approaches, rather than treating them as a separate category. All these considerations, including the observations about the missing structure and terminology conventions presented in Section 1, motivate the need for an in-depth study in this area, such as the one presented here.

### 3. Methodology

#### 3.1 Systematic Mapping Study

A systematic mapping study (SMS) was conducted to obtain relevant and representative literature in the field of AIOps. Unlike a systematic literature review (SLR), the ultimate goal of a systematic mapping study is to provide an overview of a specific research area, to obtain a set of related papers, and to delineate trends present in that area [67]. Relevant papers are collected via well-defined search and selection criteria, while research trends are identified using categorization schemes covering different aspects, such as main topic, origin, or type of contribution. We chose this instrument because we are interested in gathering contributions for the survey and obtaining insights into the field, such as the distribution of works in different AIOps subareas and the temporal evolution of interest toward specific topics. An in-depth discussion of the mapping study methodology, the categorization scheme, and the selection strategy of contributions is available in a separately published work [113], accessible online.

[Link: https://arxiv.org/abs/2012.09108]

#### 3.2 Evaluation Metrics

In our analysis, we provide quantitative results for the papers under investigation. This section provides an overview of the evaluation metrics employed for comparison throughout the survey discussion.

**Figure 2.** Taxonomy of AIOps as observed in the identified contributions. In the red box, the focus of this survey.

**Table 3.** Contingency Table for Prediction Tasks

| Predicted Class | Positive Class | Negative Class |
|-----------------|----------------|----------------|
| Positive Class  | True Positives (TP) | False Negatives (FN) |
| Negative Class  | False Positives (FP) | True Negatives (TN) |

For scalar prediction (or regression) tasks, a widely adopted metric is the Mean-squared Error (MSE), defined as the average squared difference between target and predicted values:

\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i^{\text{pred}} - y_i)^2
\]

A measure adopted across all classification problems (software defect prediction, root-cause diagnosis, recovery, etc.) is accuracy, i.e., the ratio of classified samples assigned to the correct class. In some contexts, however, accuracy may appear as a misleading metric to evaluate the quality of prediction. This is the case, for example, for problems with a high predominance of one class, where trivial models can be constructed to reach high accuracy just by exploiting data skewness. A similar consideration applies to detection problems (analyzed, e.g., in Sections 4.2 and 4.3), where the positive class, i.e., the detected failure, may appear less frequently than the negative class, even though it constitutes the most critical aspect from an evaluation point of view. In such cases, it is common to adopt more representative measures derived from the notion of a contingency table [122]:

Using this convention, accuracy can be written as:

\[
\text{ACC} = \frac{TP + TN}{TP + TN + FP + FN}
\]

To quantify the ability of a predictor to identify positive samples correctly, the precision measure is usually employed, while to measure the ability of a detector to report true positive samples, the recall measure (also known as true positive rate or sensitivity) is used. They are defined as follows:

\[
P = \frac{TP}{TP + FP}, \quad R = \frac{TP}{TP + FN}
\]

Moreover, the false-positive rate (FPR, also called false alarm rate), which identifies the proportion of wrongly reported failures, is defined as follows:

\[
\text{FPR} = \frac{FP}{FP + TN}
\]

Since precision and recall do not take into account the number of true negatives, some papers compare results in terms of true-negative rate (TNR, or specificity) and recall. The true-negative rate is defined as follows:

\[
\text{TNR} = \frac{TN}{TN + FP}
\]

Precision and recall can often be traded off with each other by adjusting sensitivity thresholds inside algorithms, so that an increase in precision can be obtained by reducing recall and vice versa. One possibility to evaluate both measures at the same time is to use the F1-score (or F-measure), computed as the harmonic mean of precision and recall:

\[
\text{F1} = 2 \cdot \frac{P \cdot R}{P + R}
\]

A final possibility is to use receiver operating characteristic (ROC) curves, parametric line plots that describe the variation of two metrics in relation to changes in the sensitivity threshold. Precision-recall curves are possible, but it is common to plot the recall against the false-positive rate. From this type of curve, the Area under the ROC curve (AUC-ROC) measure can be computed. A higher AUC-ROC score indicates a better classifier.

#### 3.3 Terminology

In our discussion, we adopt a variety of error-related terms such as fault, failure, and root cause. From a terminological point of view, we adopt the convention of Salfner et al. [122] for the characterization of faulty behavior. According to this convention:

- **Errors** are deviations from the correct system state.
- **Failures** are manifestations of undesired deviations in the delivery of a service.
- **Faults (or root causes)** are the primary causes of undesired behavior (i.e., the errors).

We also encounter different terms related to data sources that may have ambiguous meanings depending on the context, such as logs or traces. To be consistent in our discussion, we group the observed data sources according to this convention:

- **Source code** represents any unit of software source code used as input to a prediction system, independently of the form and extension (e.g., function, file, module, class, etc.).
- **Testing resources** comprise tools used to perform in- and post-release software debugging, particularly unit test suites, execution profiles, or run description reports.
- **System metrics** measure various numerical quantities at the hardware, OS, software, and environment level, describing resource utilization and the overall process state of the system.
- **Key Performance Indicators (KPIs)** provide information about the status of services and the associated requirements that need to be met during runtime operations. They quantitatively measure the quality of served requests with parameters such as latency, uptime, failure rate, availability, and so on.
- **Network traffic** is the collection of network packets exchanged over the Internet by different hosts. It includes the payload and control information such as ports, addresses, protocol standards, and other parameters.
- **Topology** is any information describing the spatial relations inside a working system, when used as an input.
- **Incident reports** are collected with the help of service desks and internal problem management systems to identify common problems and facilitate resolution. Usually, they describe the problem with text and categorical attributes and may be associated with a resolution team or routing sequence.
- **Event logs (or simply logs)** are collections of human-interpretable printing statements describing software events occurring in runtime operations. They are typically stored as independent files, and log entries (i.e., lines) are associated with a predefined format (or log key).
- **Execution (distributed) traces** are hierarchical descriptions of the modules and services invoked to satisfy a user request. They are usually annotated with the service name or category and the time duration of each module (called span).

### 4. AI Approaches in Failure Management

This section will delve into the structure of failure management in AIOps and present a selection of papers divided into thematic sections, describing their line of work, contributions, approaches, input sources, target components, and application fields.