[8] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, K. Rieck,
and C. Siemens. DREBIN: Effective and Explainable Detection
of Android Malware in Your Pocket. In Ndss, volume 14, pages
23–26, 2014.
[9] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients
give a false sense of security: Circumventing defenses to
adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
[10] S. Axelsson. The base-rate fallacy and its implications for the
difﬁculty of intrusion detection. In Proceedings of the 6th ACM
Conference on Computer and Communications Security, pages
1–7. ACM, 1999.
[11] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c,
P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against
machine learning at test time. In Joint European conference
on machine learning and knowledge discovery in databases,
pages 387–402. Springer, 2013.
[12] X. Cao and N. Z. Gong. Mitigating evasion attacks to deep
neural networks via region-based classiﬁcation. In Proceedings
of the 33rd Annual Computer Security Applications Conference,
pages 278–287. ACM, 2017.
[13] N. Carlini and D. Wagner. Towards evaluating the robustness of
neural networks. In IEEE Symposium on Security and Privacy
(SP), pages 39–57. IEEE, 2017.
[14] H. Chen, H. Zhang, D. Boning, and C.-J. Hsieh. Robust
decision trees against adversarial examples. 2019.
[15] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting
system. In Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining, pages
785–794. ACM, 2016.
[16] Y. Chen, Y. Nadji, A. Kountouras, F. Monrose, R. Perdisci,
M. Antonakakis, and N. Vasiloglou. Practical attacks against
graph-based clustering.
In Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications
Security, pages 1125–1142. ACM, 2017.
[17] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song.
Adversarial attack on graph structured data. arXiv preprint
arXiv:1806.02371, 2018.
[18] H. Dang, Y. Huang, and E.-C. Chang. Evading classiﬁers by
morphing in the dark. In Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security, pages
119–133. ACM, 2017.
[19] T. Dreossi, S. Jha, and S. A. Seshia. Semantic adversarial deep
learning. arXiv preprint arXiv:1804.07045, 2018.
[20] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output
range analysis for deep feedforward neural networks. In NASA
Formal Methods Symposium, pages 121–138. Springer, 2018.
[21] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
B. O’Donoghue, J. Uesato, and P. Kohli. Training veriﬁed learn-
ers with learned veriﬁers. arXiv preprint arXiv:1805.10265,
2018.
[22] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A
dual approach to scalable veriﬁcation of deep networks. arXiv
preprint arXiv:1803.06567, 2018.
[23] R. Ehlers. Formal veriﬁcation of piece-wise linear feed-forward
neural networks. 15th International Symposium on Automated
Technology for Veriﬁcation and Analysis, 2017.
[24] M. Fischetti and J. Jo. Deep neural networks as 0-1 mixed
integer linear programs: A feasibility study. arXiv preprint
arXiv:1712.06174, 2017.
[25] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov,
S. Chaudhuri, and M. Vechev. Ai 2: Safety and robustness
certiﬁcation of neural networks with abstract interpretation. In
IEEE Symposium on Security and Privacy (SP), 2018.
[26] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In International Conference
on Learning Representations (ICLR), 2015.
[27] S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin,
J. Uesato, T. Mann, and P. Kohli. On the effectiveness of
interval bound propagation for training veriﬁably robust models.
arXiv preprint arXiv:1810.12715, 2018.
[28] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. Mc-
Daniel. Adversarial perturbations against deep neural networks
for malware classiﬁcation. arXiv preprint arXiv:1606.04435,
2016.
[29] W. Guo, D. Mu, J. Xu, P. Su, G. Wang, and X. Xing. Lemna:
Explaining deep learning based security applications. In Pro-
ceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security, pages 364–379. ACM, 2018.
[30] W. Hu and Y. Tan. Generating adversarial malware exam-
arXiv preprint
ples for black-box attacks based on gan.
arXiv:1702.05983, 2017.
[31] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety veriﬁ-
cation of deep neural networks. In International Conference on
Computer Aided Veriﬁcation (CAV), pages 3–29. Springer, 2017.
2356    29th USENIX Security Symposium
USENIX Association
[32] I. Incer, M. Theodorides, S. Afroz, and D. Wagner. Adversar-
ially robust malware detection using monotonic classiﬁcation.
In Proceedings of the Fourth ACM International Workshop on
Security and Privacy Analytics, pages 54–63. ACM, 2018.
[33] A. Kantchelian, J. Tygar, and A. Joseph. Evasion and hardening
of tree ensemble classiﬁers. In International Conference on
Machine Learning, pages 2387–2396, 2016.
[34] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer.
Reluplex: An efﬁcient smt solver for verifying deep neural
networks.
In International Conference on Computer Aided
Veriﬁcation (CAV), pages 97–117. Springer, 2017.
[35] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine
learning at scale. arXiv preprint arXiv:1611.01236, 2016.
[36] P. Laskov et al. Practical evasion of a learning-based classiﬁer:
In Security and Privacy (SP), 2014 IEEE
A case study.
Symposium on, pages 197–211. IEEE, 2014.
[37] A. Lomuscio and L. Maganti. An approach to reachability
analysis for feed-forward relu neural networks. arXiv preprint
arXiv:1706.07351, 2017.
[38] D. Lowd and C. Meek. Adversarial Learning. In Proceedings of
the eleventh ACM SIGKDD international conference on Knowl-
edge discovery in data mining, pages 641–647. ACM, 2005.
[39] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
Towards deep learning models resistant to adversarial attacks.
International Conference on Learning Representations (ICLR),
2018.
[40] D. Maiorca, I. Corona, and G. Giacinto. Looking at the bag is
not enough to ﬁnd the bomb: an evasion of structural methods
for malicious pdf ﬁles detection.
In Proceedings of the 8th
ACM SIGSAC symposium on Information, computer and
communications security, pages 119–130. ACM, 2013.
[41] M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract
interpretation for provably robust neural networks.
In
International Conference on Machine Learning (ICML), pages
3575–3583, 2018.
[42] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security, pages 506–519.
ACM, 2017.
[43] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations against
deep neural networks. arXiv preprint arXiv:1511.04508, 2015.
[44] A. Raghunathan, J. Steinhardt, and P. Liang. Certiﬁed defenses
International Conference on
against adversarial examples.
Learning Representations (ICLR), 2018.
[45] A. Raghunathan, J. Steinhardt, and P. S. Liang. Semideﬁnite
relaxations for certifying robustness to adversarial examples.
In Advances in Neural Information Processing Systems, pages
10900–10910, 2018.
[46] J. Saxe and K. Berlin. Deep neural network based malware
detection using two dimensional binary program features. In
Malicious and Unwanted Software (MALWARE), 2015 10th
International Conference on, pages 11–20. IEEE, 2015.
[47] C. Smutz and A. Stavrou. Malicious pdf detection using
metadata and structural features. In Proceedings of the 28th
annual computer security applications conference, pages
239–248. ACM, 2012.
[48] N. Šrndic and P. Laskov. Detection of malicious pdf ﬁles based
on hierarchical document structure. In Proceedings of the 20th
Annual Network & Distributed System Security Symposium,
pages 1–16. Citeseer, 2013.
[49] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
Intriguing properties of
International Conference on Learning
I. Goodfellow, and R. Fergus.
neural networks.
Representations (ICLR), 2013.
[50] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness
of neural networks with mixed integer programming. arXiv
preprint arXiv:1711.07356, 2017.
[51] L. Tong, B. Li, C. Hajaj, C. Xiao, N. Zhang, and Y. Vorobeychik.
Improving robustness of ml classiﬁers against realizable evasion
attacks using conserved features. In 28th USENIX Security
Symposium (USENIX Security 19), pages 285–302, 2019.
[52] D. Wagner and P. Soto. Mimicry attacks on host-based intrusion
detection systems. In Proceedings of the 9th ACM Conference
on Computer and Communications Security, pages 255–264.
ACM, 2002.
[53] S. Wang, Y. Chen, A. Abdou, and S. Jana. Mixtrain: Scalable
training of formally robust neural networks. arXiv preprint
arXiv:1811.02625, 2018.
[54] S. Wang, Y. Chen, A. Abdou, and S. Jana.
Enhancing
gradient-based attacks with symbolic intervals. arXiv preprint
arXiv:1906.02282, 2019.
[55] S. Wang, K. Pei, W. Justin, J. Yang, and S. Jana. Efﬁcient
formal safety analysis of neural networks. Advances in Neural
Information Processing Systems (NIPS), 2018.
[56] S. Wang, K. Pei, W. Justin, J. Yang, and S. Jana. Formal security
analysis of neural networks using symbolic intervals. 27th
USENIX Security Symposium, 2018.
[57] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Bon-
ing, I. S. Dhillon, and L. Daniel. Towards fast computation
of certiﬁed robustness for relu networks.
arXiv preprint
arXiv:1804.09699, 2018.
[58] E. Wong and Z. Kolter. Provable defenses against adversarial
examples via the convex outer adversarial polytope. In Inter-
national Conference on Machine Learning, pages 5283–5292,
2018.
[59] E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling
provable adversarial defenses. Advances in Neural Information
Processing Systems (NIPS), 2018.
[60] W. Xu.
PDF-Malware-Parser for EvadeML.
//github.com/mzweilin/PDF-Malware-Parser.
https:
[61] W. Xu, Y. Qi, and D. Evans. Automatically Evading Classiﬁers.
In Proceedings of the 2016 Network and Distributed Systems
Symposium, 2016.
[62] X. Zhang and D. Evans. Cost-Sensitive Robustness against
Adversarial Examples. arXiv preprint arXiv:1810.09225, 2018.
USENIX Association
29th USENIX Security Symposium    2357
A Appendix
A.1 VRA for Ensemble Classiﬁers
A.1.1 Ensemble A+B VRA
Property A. A test PDF is veriﬁed to be safe within property
A, if all the possible subtree deletion with distance one is safe.
Therefore, for each interval representing one subtree deletion,
we require that any of the corresponding two subtree deletion
is classiﬁed as malicious.
A.2 ERA under Bounded Gradient Attack
Table 8 shows precision, recall of the models on the left side,
and the ERA under gradient attacks bounded by robustness
properties on the right side. All veriﬁably robust models main-
tain high precision and recall. The ERA values of the models
are higher than the corresponding VRA values in Table 4.
A.3 Unrestricted Gradient Attack Result
A.3.1 ERA
Property B. Property B is the provable robustness property
of Ensemble A+B. If any mutated PDF is generated by inserting
one arbitrary subtree to a malicious PDF, it has the same clas-
siﬁcation result as the malicious PDF seed. Therefore, we use
the test accuracy of malicious PDFs as the VRA for property B.
Property C. A test PDF is veriﬁed to be safe within property
C, if all the possible subtree deletion with distance two is safe.
Therefore, for each interval representing two subtree deletion,
we require that any of the corresponding three subtree deletion
is classiﬁed as malicious.
Property D. A test PDF is veriﬁed to be safe within property
D, if all the possible subtree insertion at distance 41 is safe.
Therefore, we test whether any interval representing 40 subtree
insertion on a malicious test PDF can be classiﬁed as malicious.
Property E. A test PDF is veriﬁed to be safe within property
E, if all the possible subtree insertion in the entire feature space
is safe. Therefore, we test whether any interval representing
all-but-one (41) subtree insertion on a malicious test PDF can
be classiﬁed as malicious.
A.1.2 Ensemble D VRA
Property A and C. A test PDF is veriﬁed to be safe for a dele-
tion property, if any subtree after some deletion is classiﬁed as
malicious. Therefore, for each test PDF, we check whether any
interval representing the lower bound of all zeros and the upper
bound of the original subtree can be classiﬁed as malicious.
Property B, D and E. A test PDF is veriﬁed to be safe
for a insertion property, if any subtree after some insertion
is classiﬁed as malicious. There are two categories. If the
inserted subtree does not exist, the interval is from all zeros
and all ones for that subtree. If the inserted subtree already
exists, the interval bound is from the original subtree features
to all ones. We check if any of these intervals can be classiﬁed
as malicious for all possible insertions.
A
R
E
1.00
0.75
0.50
0.25
0.00
Baseline NN
Adv Retrain A
Adv Retrain B
Adv Retrain C
Adv Retrain D
Adv Retrain A+B
1
10
100
L0
1000 3514
Figure 6: Unrestricted gradient attack against baseline models.
A
R
E
1.00
0.75
0.50
0.25
0.00
Robust A
Robust B
Robust C
Robust D
Robust E
Robust A+B
Robust A+B+E
1
10
100
L0
1000 3514
Figure 7: Unrestricted gradient attack against our veriﬁably
robust models.
Figure 6 shows the ERA of the Baseline NN and adver-
sarially retrained models against unrestricted gradient attack.
Most adversarially retrained models perform similar to the
Baseline NN. Adv Retrain A+B is most robust among them
according to the ERA curve. The ERA drops more slowly as
the L0 distance increases compared to the other models.
Figure 7 shows the ERA of veriﬁably robust models against
unrestricted gradient attack. Robust A+B performs the best
among them, maintaining 7.38% ERA after 200,000 attack
iterations.
A.3.2 Convergence
A
R
E
1.00
0.75
0.50
0.25
0.00
0
10000
Adv Retrain A+B
Robust D
Robust A+B
40000
50000
20000
30000
Iterations
Figure 8: The ERA of three models converges against the
unrestricted gradient attack.
We run the unrestricted gradient attack for 200,000
iterations, and plot the ERA for the ﬁrst 50,000 iterations.
2358    29th USENIX Security Symposium
USENIX Association
Model
Baseline NN
Adv Retrain A
Adv Retrain B
Adv Retrain C
Adv Retrain D
Adv Retrain A+B
Ensemble A+B*
Ensemble D*
Robust A
Robust B
Robust C
Robust D
Robust E
Robust A+B
Robust A+B+E
Precision Recall
(%)
99.97
99.97
99.97
99.97
99.97
99.97
99.97
99.97
99.97
99.97
100
99.94