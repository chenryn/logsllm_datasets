# 集成学习简介
集成学习是通过构建并组合多个学习器来完成学习任务的算法，集成学习常见的有两类
- Bagging：基学习器之间无强依赖关系，可同时生成的并行化方法
- Boosting：基学习器之间存在强烈的依赖关系，必须串行生成基分类器的方法
  - Adaboost：对样本进行加权的过程，然后来进行基学习器的训练，集成后得到最终的学习器
  - GBDT：梯度提升算法
    - XGBoost：
    - LightGBM
![image-20200428102348559](images/image-20200428102348559.png)
## Bagging算法
Bagging（Bootstrap Aggregating）方法：
![image-20200428102845090](images/image-20200428102845090.png)
训练完成后，我们进行预测
![image-20200428102940361](images/image-20200428102940361.png)
如果是预测问题，我们只需要通过Average求平均
如果是分类问题，我们需要进行投票表决
## Boosting算法
Boosting方法是将“弱学习算法”提升为“强学习算法”的过程，通过反复学习得到一系列弱分类器（决策树和逻辑回归），组合这些弱分类器得到一个强分类器。 Boosting算法要涉及到两个部分，加法模型和前向分步算法
![image-20200428110627474](images/image-20200428110627474.png)
### 加法模型
加法模型就是说强分类器由一系列弱分类器线性相加而成。一般组合形式如下
$$
F_{M}(x ; P)=\sum_{m=1}^{n} \beta_{m} h\left(x ; a_{m}\right)
$$
其中，h（x;am）是弱分类器，am是弱分类器学习到的最优参数，m是弱学习在强分类器中所占比重，P是所有Lm和βm的组合。这些弱分类器线性相加组成强分类器前向分步是在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。即
### 前向步骤
前向步骤是在训练过程中，下一轮迭代产生的分类器是在上一步的基础上训练得来的。即
$$
F_{m}(x)=F_{m-1}(x)+\beta_{m} h_{m}\left(x ; a_{m}\right)
$$
## 随机森林
随机森林 = Bagging + 决策树
同时训练多个决策树，预测时综合考虑多个结果进行预测，例如取多个节点的均值（回归），或者是众数（分类）。
### 优势
- 消除了决策树容易过拟合的缺点
- 减小了预测的方差，预测值不会因训练数据的小变化而剧烈变化
![image-20200428115107690](images/image-20200428115107690.png)
### 随机性
随机森林的随机性体现在以下两点
- 从原来的训练数据集随机（带放回bootstrap）取一个子集作为森林中某一个决策树的训练数据集
- 每一个选择分叉的特征时，限定为在随机选择的特征的子集中寻找一个特征。
![image-20200428115411734](images/image-20200428115411734.png)
### 实例
通过案例来看下随机森林的应用 现有某公司的员工离职数据，我们通过构建决策树和随机森林，来预测某一员工是否会离职。并找出影响员工离职的重要特征
见代码：DecisionTree.RandomForest.ipynb
## Adaboost
提升树中的算法，Adaboost是在样本上做文章的，在每次训练完基学习器之后，来调整样本的分布，在最后集成的时候，也是需要在不同的分类器前面添加权重参数。
### 思想
Adaboost的思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值提高被错误分类的样本权值
Adaboost采用加权投票的方法分类误差小的弱分类器的权重大，而分类误差大的弱分类器的权重小。
![image-20200428124635998](images/image-20200428124635998.png)
例如上图所示，在第一步的时候，我们进行分类，同时有分类正确的，和分类错误的。
![image-20200428124817590](images/image-20200428124817590.png)
在第二步的时候，我们需要将分类正确的样本权重缩小，通知增大分类错误的权重，然后再次进行分类，这样下次分类的时候，就会更加关注与上次分错的样本，从而得到下面的分类结果
![image-20200428124823061](images/image-20200428124823061.png)
然后我们再次调整分错样本的权值
![image-20200428124953733](images/image-20200428124953733.png)
最终我们将前面的弱分类器进行集成，得到我们最终的分类结果
![image-20200428125021604](images/image-20200428125021604.png)
### 算法流程
假设输入训练数据为：
$$
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right),\left(x_{N}, y_{N}\right)\right\}
$$
其中:x∈X≤Rn，y∈Y=-1，1，迭代次数即弱分类器个数为M
初始化训练样本的权值分布为：
$$
D_{1}=\left(w_{1,1}, w_{1,2}, \ldots, w_{1, i}\right), w_{1, i}=\frac{1}{N}, i=1,2, \ldots, N
$$
![image-20200428125528569](images/image-20200428125528569.png)
上述的更新权重的过程就是：如果我们分类正确了，那么就需要缩小权重，如果分类错误，那么就需要放大权重，然后不断循环，最后得到M个弱分类器
最终得到的分类器为：
$$
F(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{m} G_{m}(x)\right)
$$
在整个过程中，令人困惑的是：分类器的权重的确定
假设经过m-1轮迭代，得到弱分类器Fm-1（x），根据前向分布，有
$$
F_{m}(x)=F_{m-1}(x)+\alpha_{m} G_{m}(x)
$$
AdaBoost的损失函数是指数损失，则有：
$$
\text {Loss}=\sum_{i=1}^{N} \exp \left(-y_{i} F_{m}\left(x_{i}\right)\right)=\sum_{i=1}^{N} \exp \left(-y_{i}\left(\boldsymbol{F}_{m-1}\left(x_{i}\right)+\alpha_{m} G_{m}\left(x_{i}\right)\right)\right)
$$
![image-20200428145723489](images/image-20200428145723489.png)
![image-20200428145930172](images/image-20200428145930172.png)
![image-20200428150240554](images/image-20200428150240554.png)
Adaboost可以看成是加法模型、损失函数为指数损失函数、学习算法为前向分布算法时的二分类学习方法，接下来我们使用SkLearn中的AdaBoost的接口进行实践
```
AdaBoostClassifier(base_estimator=None, $n_{-}$ estimators $=50$ learning_rate $=1.0,$ algorithm='SAMME.R', random_state=None)
```
base_estimator：代表基学习器是什么
### Adaboost实践
参考代码：adaboost.ipynb
## GBDT
BDT：提升树
GBDT：梯度提升树
### BDT
我们首先看一下简单的提升树（Boosting Decision Tree），提升树是以CART决策树为基学习器的集成学习方法。
![image-20200428153045454](images/image-20200428153045454.png)
我们通过不同的训练数据，来训练弱学习器，最后通过不同的学习器，组合成为强学习器
提升树实际上就是加法模型和前向分布算法，将其表示为：
![image-20200428153350966](images/image-20200428153350966.png)
x是参数  0m 是参数
在前向分布算法第m步时，给定当前的模型 fm - 1 ( x ) ，求解：
![image-20200428153730182](images/image-20200428153730182.png)
得到第m棵决策树T(x,  0m) ，不同问题的提升树的区别在于损失函数的不同，如分类用指数损失函数，回归用平方误差损失函数。
当提升树采用平方损失函数时，第m次迭代时表示为：
![image-20200428153949765](images/image-20200428153949765.png)
称r为残差，所以第m棵决策树（x，⊙m）是对该残差的拟合。要注意的是提升树算法中的基学习器CART树是回归树。
>残差在数理统计中是指实际观察值与估计值（[拟合值](https://baike.baidu.com/item/拟合值/9461734)）之间的差。
![image-20200428154411161](images/image-20200428154411161.png)
### GBDT
GBDT全称为: Gradient Boosting Decision Tree，即梯度提升决策树，理解为 梯度提升 + 决策树。 Friedman提出了利用最速下降的近似方法，利用损失函数的负梯度拟合基学习器
![image-20200428154633244](images/image-20200428154633244.png)
怎么理解这个近似，我们通过平方损失函数来给大家介绍
![image-20200428154751350](images/image-20200428154751350.png)
![image-20200428154930163](images/image-20200428154930163.png)
GBDT的梯度提升流程如下所示：
![image-20200428155742442](images/image-20200428155742442.png)