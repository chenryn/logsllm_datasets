For a more in-depth overview on system complexity, check out Ben
Sigelman’s overview and webinar on Deep Systems.
Connecting cause and effect with
distributed tracing
The same way a doctor first looks for inflammation, reports of pain,
and high body temperature in any patient, it is critical to understand
the symptoms of your software’s health. Is your system experiencing
high latency, spikes in saturation, or low throughput? These symptoms
can be easily observed, and are usually closely related to SLOs, making
their resolution a high priority.
Once a symptom has been observed, distributed tracing can help
identify and validate hypotheses about what has caused this change.
The Complete Guide to Distributed Tracing 12
It is important to use symptoms (and other measurements related to
SLOs) as drivers for this process, because there are thousands — or
even millions — of signals that could be related to the problem, and
(worse) this set of signals is constantly changing. While there might
be an overloaded host somewhere in your application (in fact, there
probably is!), it is important to ask yourself the bigger questions: Am
I serving traffic in a way that is actually meeting our users’ needs? Is
that overloaded host actually impacting performance as observed by
our users?
In the next section, we will look at how to start with a symptom and track
down a cause. Spoiler alert: it’s usually because something changed.
Change drives outages
Service X is down. What happened? As a service owner your
responsibility will be to explain variations in performance — especially
negative ones. A great place to start is by finding out what, if
any, changes have been made to the system prior to the outage.
Sometimes it’s internal changes, like bugs in a new version, that lead
to performance issues. At other times it’s external changes — be they
changes driven by users, infrastructure, or other services — that
cause these issues.
Changes in the service
Perhaps the most common cause of changes to a service’s
performance are the deployments of that service itself. Distributed
tracing can break down performance across different versions,
especially when services are deployed incrementally. This, in
effect, surfaces the latency, error rate, and throughput for all of
your services endpoints, and allows you to understand changes to
performance before, during, and after your deploy hits production.
The Complete Guide to Distributed Tracing 13
This is made possible by tagging each span with the version of the
service that was running at the time the operation was serviced.
(Traces are all about context: Spans can also be tagged by
geography, operating system, canary, user_id, etc.)
Changes in user demands
Changes to service performance can also be driven by external
factors. Your users will find new ways to leverage existing features
or will respond to events in the real world that will change the way
they use your application. For example, users may leverage a batch
API to change many resources simultaneously or may find ways of
constructing complex queries that are much more expensive than
you anticipated. A successful ad campaign can also lead to a sudden
deluge of new users who may behave differently than your more
tenured users.
Being able to distinguish these examples requires both adequate
tagging and sufficient internal structure to the trace. Tags should
capture important parts of the request (for example, how many
resources are being modified or how long the query is) as well as
important features of the user (for example, when they signed up or
what cohort they belong to).
In addition, traces should include spans that correspond to any
significant internal computation and any external dependency. For
example, one common insight from distributed tracing is to see
how changing user behavior causes more database queries to be
executed as part of a single request.
The Complete Guide to Distributed Tracing 14
Infrastructure and resource competition
All the planning in the world won’t lead to perfect resource
provisioning and seamless performance. And isolation isn’t perfect:
threads still run on CPUs, containers still run on hosts, and databases
provide shared access. Contention for any of these shared resources
can affect a request’s performance in ways that have nothing to do
with the request itself.
As above, it’s critical that spans and traces are tagged in a way that
identifies these resources: every span should have tags that indicate
the infrastructure it’s running on (datacenter, network, availability zone,
host or instance, container) and any other resources it depends on
(databases, shared disks). For spans representing remote procedure
calls, tags describing the infrastructure of your service’s peers (for
example, the remote host) are also critical.
With these tags in place, aggregate trace analysis can determine
when and where slower performance correlates with the use of
one or more of these resources. This, in turn, lets you shift from
debugging your own code to provisioning new infrastructure or
determining which team is abusing the infrastructure that’s currently
available.
Upstream changes
The last type of change we will cover are upstream changes. These
are changes to the services that your service depends on. Having
visibility into your service’s dependencies’ behavior is critical in
understanding how they are affecting your service’s performance.
Remember, your service’s dependencies are — just based on sheer
numbers — probably deploying a lot more frequently than you are.
And even with the best intentions around testing, they are probably
The Complete Guide to Distributed Tracing 15
not testing performance for your specific use case. Simply by tagging
egress operations (spans emitted from your service that describe the
work done by others), you can get a clearer picture when upstream
performance changes. (And even better if those services are also
emitting spans tags with version numbers.)
Simple guidelines for tagging
Understanding these changes will help you plan a more robust
instrumentation. What is the correct granularity for spans? And what are
the correct tags to use within those spans to enable automated analysis?
We can establish some simple guidelines about what information to
track, depending on the general type of operation:
• All Operations: software version, infrastructure info
• Ingress and Egress Operations: peer info, key request
parameters, response code
• Large Components/Libraries: domain-specific tags
Include spans for any operation that might independently fail (remote
procedure calls should immediately come to mind) or any operation
with significant performance variation (think database query) or that
might occur a variable number of times within a single request.
You might think, “can’t this be accomplished with the right dashboard
tooling?” Remember that your goal as a service owner is to explain
variation in performance, and to do so, you must not just enumerate
all of the metrics, but quickly identify the metrics that matter — with
an emphasis on quickly. This is where tracing shines, because it
draws a clear line between your SLIs and the metrics which explain
variation in these SLIs — whether those metrics are part of your
service or a service that belongs to another team.
The Complete Guide to Distributed Tracing 16
Proactive solutions with distributed tracing
So far we have focused on using distributed tracing to efficiently
react to problems. But this is only half of distributed tracing’s
potential. How can your team use distributed tracing to be proactive?
The first step is going to be to establish ground truths for your
production environments. What are the average demands on your
system? With the insights of distributed tracing, you can get the
big picture of your service’s day-to-day performance expectations,
allowing you to move on to the second step: improving the aspects
of performance that will most directly improve the user’s experience
(thereby making your service better!).
• Step One: establish ground truths for production
• Step Two: make it better!
The following are examples of proactive efforts with distributed
tracing: planning optimizations and evaluating SaaS performance.
Planning optimizations: how do you know
where to begin?
Your team has been tasked with improving the performance of one
of your services — where do you begin? Before you settle on an
optimization path, it is important to get the big-picture data of how
your service is working. Remember, establish ground truth, then make
it better!
The Complete Guide to Distributed Tracing 17
Answering these questions will set your team up for meaningful
performance improvements:
• What needs to be optimized? Settle on a specific and meaningful
SLI, like p99 latency.
• Where do these optimizations need to occur?
Use distributed tracing to find the biggest contributors to the
aggregate critical path.
In the image below, we have a trace with two operations:
A and B. Operation A takes about 15% of the time of the overall trace,
while B consumes the rest.
A B
Reduce A by 50% Reduce B by 50%
A B A B
With this operation in mind, let’s consider Amdahl’s Law, which
describes the limits of performance improvements available to a
whole task by improving performance for part of the task. Applying
Amdahl’s Law appropriately helps ensure that optimization efforts are,
well, optimized.
The Complete Guide to Distributed Tracing 18
What Amdahl’s Law tells us here is that focusing on the performance
of operation A is never going to improve overall performance more
than 15%, even if performance were to be fully optimized. If your real
goal is improving the performance of the trace as a whole, you need
to figure out how to optimize operation B.
In short: Don’t waste time or money on uninformed optimizations.
Evaluating managed services
Managed services provide a lot of flexibility to engineering teams,
enabling time-intensive services such as storage, analysis, and load
balancing to be offloaded; and, for services with a lot of exposure,
such as fraud and abuse detection, authentication, and payments,
managed services can offer meaningful security. Regardless of
how it fits into the application, from an observability point of view,
a managed service is ultimately just another service, and the same
design and testing considerations need to be made.
While nearly all managed services publish SLAs (yours do, right?), no
vendor is going to care as much as you do about how they can affect
your service’s performance. Measure them like any other service in
your application. Use best practices for things like provisioning. Even
if you can’t instrument the managed service itself, instrumenting
SDKs, and monitoring them just like you would any of your other
upstream dependencies, goes a long way toward better visibility.
The Complete Guide to Distributed Tracing 19
Summary
While there are no silver bullets in the software
performance, distributed tracing provides unrivaled
visibility and analysis in systems that can have
dozens, hundreds, or thousands of services working
together. With features like tags that enable
automated analysis, traces provide request-centric
views into system health and performance pivotal
for a DevOps organization.
When deciding what performance metrics to focus on, prioritize the needs
and expectations of your software’s users. Service levels help formalize
these expectations, and provide a focus for investment and engineering
efforts when triaging an issue or making optimization decisions.
As issues do arise, use distributed tracing to isolate and highlight
changes that impact your SLIs. These changes can be internal, like
new deployments, but are often external, including your service’s
dependencies or user behavior.
Empower your DevOps organizations with distributed tracing for fast
root-cause analysis and performance optimization in deep systems.
The Complete Guide to Distributed Tracing 20
About Lightstep
Lightstep’s mission is to deliver confidence at scale for those who
develop, operate and rely on today’s powerful software applications.
Its products leverage distributed tracing technology — initially
developed by a Lightstep co-founder at Google — to offer best-
of-breed observability to organizations adopting microservices
or serverless at scale. Lightstep is backed by Redpoint, Sequoia,
Altimeter Capital, Cowboy Ventures and Harrison Metal, and is
headquartered in San Francisco, CA. For more information, visit
Lightstep.com or follow @LightstepHQ.
Try Lightstep for free
Check out Lightstep’s interactive sandbox, and debug an iOS error or
resolve a performance regression in less than 10 minutes.
©2020 Lightstep, Inc.
The Complete Guide to Distributed Tracing 21