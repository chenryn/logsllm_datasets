### In-Depth Overview of System Complexity

For a more comprehensive understanding of system complexity, refer to Ben Sigelman’s overview and webinar on Deep Systems.

### Connecting Cause and Effect with Distributed Tracing

Just as a doctor first looks for inflammation, pain, and high body temperature in a patient, it is crucial to understand the symptoms of your software's health. Common symptoms include high latency, spikes in saturation, and low throughput. These symptoms are easily observable and often closely related to Service Level Objectives (SLOs), making their resolution a high priority.

Once a symptom is identified, distributed tracing can help pinpoint and validate hypotheses about the cause. It is essential to use these symptoms (and other SLO-related measurements) as drivers for this process because there are potentially thousands or even millions of signals that could be related to the problem, and this set of signals is constantly changing.

While an overloaded host may be present in your application, it is important to ask broader questions: Are you serving traffic in a way that meets users' needs? Is the overloaded host actually impacting performance as observed by your users?

### Identifying the Cause from a Symptom

In the next section, we will explore how to start with a symptom and track down its cause. Often, the root cause is a change in the system.

### Change Drives Outages

When Service X is down, as a service owner, your responsibility is to explain variations in performance, especially negative ones. A good starting point is to identify any changes made to the system before the outage. These changes can be internal, such as bugs in a new version, or external, driven by users, infrastructure, or other services.

### Changes in the Service

One of the most common causes of performance changes is the deployment of new versions of the service. Distributed tracing can break down performance across different versions, especially when services are deployed incrementally. This allows you to see the latency, error rate, and throughput for all service endpoints, helping you understand performance changes before, during, and after deployment.

This is achieved by tagging each span with the version of the service running at the time the operation was serviced. Traces provide context, and spans can also be tagged by geography, operating system, canary, user_id, etc.

### Changes in User Demands

External factors can also drive changes in service performance. Users may find new ways to use existing features or respond to real-world events, altering their usage patterns. For example, users might leverage a batch API to change many resources simultaneously or construct complex queries that are more expensive than anticipated. A successful ad campaign can also lead to a sudden influx of new users who behave differently from long-term users.

To distinguish these scenarios, adequate tagging and a well-structured trace are necessary. Tags should capture important parts of the request (e.g., the number of resources being modified, query length) and key user features (e.g., sign-up date, user cohort). Additionally, traces should include spans for significant internal computations and external dependencies. For instance, distributed tracing can show how changing user behavior leads to more database queries per request.

### Infrastructure and Resource Competition

Even with the best planning, resource provisioning and performance are not always perfect. Threads run on CPUs, containers run on hosts, and databases provide shared access. Contention for these shared resources can affect request performance independently of the request itself.

It is critical to tag spans and traces to identify these resources. Each span should have tags indicating the infrastructure (datacenter, network, availability zone, host, container) and any dependent resources (databases, shared disks). For remote procedure calls, tags describing the infrastructure of the service's peers (e.g., the remote host) are also essential.

With these tags, aggregate trace analysis can determine when and where slower performance correlates with the use of specific resources. This helps shift from debugging code to provisioning new infrastructure or identifying which team is overusing available resources.

### Upstream Changes

Upstream changes, or changes to the services your service depends on, are another type of change to consider. Visibility into your service’s dependencies is crucial for understanding their impact on performance. Since your dependencies likely deploy more frequently than you do, they may not test performance for your specific use case. Tagging egress operations (spans emitted from your service that describe work done by others) can provide a clearer picture of upstream performance changes.

### Simple Guidelines for Tagging

Understanding these changes will help you plan a more robust instrumentation. Here are some simple guidelines for what information to track:

- **All Operations**: Software version, infrastructure info
- **Ingress and Egress Operations**: Peer info, key request parameters, response code
- **Large Components/Libraries**: Domain-specific tags

Include spans for any operation that might fail independently (e.g., remote procedure calls) or any operation with significant performance variation (e.g., database queries) or that might occur a variable number of times within a single request.

While dashboard tools can help, the goal is to quickly identify the metrics that matter. Distributed tracing excels here by clearly linking your Service Level Indicators (SLIs) to the metrics explaining their variation, whether those metrics are part of your service or another team's service.

### Proactive Solutions with Distributed Tracing

So far, we have focused on using distributed tracing to react to problems. However, distributed tracing can also be used proactively. The first step is to establish ground truths for your production environments. What are the average demands on your system? With distributed tracing, you can get a big-picture view of your service’s day-to-day performance expectations, allowing you to move to the second step: improving aspects of performance that will most directly enhance the user’s experience.

- **Step One**: Establish ground truths for production.
- **Step Two**: Make it better!

### Planning Optimizations

Your team has been tasked with improving the performance of one of your services. Before choosing an optimization path, get the big-picture data of how your service is working. Establish ground truth, then make it better!

- **What needs to be optimized?** Settle on a specific and meaningful SLI, like p99 latency.
- **Where do these optimizations need to occur?** Use distributed tracing to find the biggest contributors to the aggregate critical path.

Consider Amdahl’s Law, which describes the limits of performance improvements. Focusing on the performance of a minor operation (e.g., 15% of the overall trace) will not significantly improve overall performance. If your goal is to improve the entire trace, focus on optimizing the major contributor.

### Evaluating Managed Services

Managed services offer flexibility, enabling engineering teams to offload time-intensive tasks such as storage, analysis, and load balancing. They also provide security for services with high exposure, such as fraud and abuse detection, authentication, and payments.

From an observability perspective, a managed service is just another service. While most managed services publish SLAs, measure them like any other service in your application. Even if you cannot instrument the managed service itself, instrumenting SDKs and monitoring them can provide better visibility.

### Summary

Distributed tracing provides unparalleled visibility and analysis in systems with dozens, hundreds, or thousands of services. With features like tags that enable automated analysis, traces provide request-centric views into system health and performance, which are pivotal for a DevOps organization.

When deciding on performance metrics, prioritize the needs and expectations of your users. Service levels formalize these expectations and provide a focus for investment and engineering efforts when triaging issues or making optimization decisions.

Use distributed tracing to isolate and highlight changes that impact your SLIs. These changes can be internal, like new deployments, or external, including your service’s dependencies or user behavior.

Empower your DevOps organization with distributed tracing for fast root-cause analysis and performance optimization in deep systems.

### About Lightstep

Lightstep’s mission is to deliver confidence at scale for those who develop, operate, and rely on today’s powerful software applications. Its products leverage distributed tracing technology, initially developed by a Lightstep co-founder at Google, to offer best-of-breed observability to organizations adopting microservices or serverless at scale. Lightstep is backed by Redpoint, Sequoia, Altimeter Capital, Cowboy Ventures, and Harrison Metal, and is headquartered in San Francisco, CA. For more information, visit [Lightstep.com](https://lightstep.com) or follow [@LightstepHQ](https://twitter.com/LightstepHQ).

### Try Lightstep for Free

Check out Lightstep’s interactive sandbox, and debug an iOS error or resolve a performance regression in less than 10 minutes.

©2020 Lightstep, Inc.