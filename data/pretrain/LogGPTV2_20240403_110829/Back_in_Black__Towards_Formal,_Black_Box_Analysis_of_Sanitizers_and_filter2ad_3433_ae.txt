191
658
4389
21720
56834
102169
193109
250014
378654
445949
665282
1150938
1077315
1670331
1539764
2417741
770237
3
3
6
7
6
7
14
15
27
31
24
29
28
29
14
118
763
6200
3499
37020
38821
35057
17133
34393
113102
433177
160488
157947
118611
80283
8
24
208
45
818
732
435
115
249
819
4595
959
1069
429
1408
AVG=
34.86
27.60
8.87
28.83
5.10
6.32
10.67
25.86
19.21
10.10
2.46
10.35
9.68
20.31
9.43
15.31
TABLE I.
SFA VS. DFA LEARNING
Fig. 6. Speedup of SFA vs. DFA learning.
Adaptation to sanitizers. The technique above can be
generilized easily to sanitizers. Assume that we are given a
grammar G as before and a target transducer T implementing
a sanitization function. In this variant of the problem we would
like to ﬁnd a string sA such that there exists s ∈ L(G) for
which sA[T ]s holds.
In order to determine whether such a string exists, we
ﬁrst construct a pushdown transducer TG with the following
property: A string s will reach a ﬁnal state in TG if and only
if s ∈ L(G). Moreover, every transition in TG is the identity
function, i.e. outputs the character consumed. Therefore, we
have a transducer which will generate only the strings in L(G).
Finally, given a hypothesis transducer H, we compute the
pushdown transducer H◦TG and check the resulting transducer
for emptiness. If the transducer is not empty we can obtain a
string sA such that sA[H ◦ TG]s. Since TG will generate only
strings from L(G) it follows that sA when passed through
the sanitizer will result in a string s ∈ L(G). Afterwards, the
GOFA algorithm continues as in the DFA case.
In appendix A, B we describe a comparison of the GOFA
algorithm with random testing as well as ways in which an
complete equivalence oracle may be implemented.
VII. EVALUATION
A. Implementation
We have implemented all the algorithms described in the
previous sections. In order to evaluate our DFA/SFA learn-
ing algorithms in the standard membership/equivalence query
model we implemented an equivalence oracle by computing
101101
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:23 UTC from IEEE Xplore.  Restrictions apply. 
DFA LEARNING
ID MEMBER
EQUIV
LEARNED MEMBER
SFA LEARNING
EQUIV
LEARNED
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
3203
18986
52373
90335
176539
227162
355458
420829
634518
1110346
944058
1645751
1482134
1993469
14586
2
2
5
5
4
5
12
13
25
29
19
28
26
24
5
AVG=
100.00%
100.00%
100.00%
96.97%
98.08%
96.67%
98.48%
98.57%
98.84%
99.13%
94.81%
100.00%
97.95%
90.85%
8.94%
91.95
81
521
1119
2155
4301
5959
8103
11013
15221
27972
100522
113714
45494
45973
428
5
11
7
10
38
32
17
34
30
54
955
662
143
32
22
AVG=
100.00%
100.00%
96.00%
96.97%
80.77%
96.67%
98.48%
98.57%
98.84%
99.13%
93.33%
96.40%
93.15%
90.85%
8.94%
89.87%
TABLE II.
SFA VS. DFA LEARNING + GOFA
SPEEDUP
37.27
35.69
46.52
41.73
40.69
37.92
43.78
38.10
41.61
39.62
9.30
14.39
32.48
43.33
32.42
35.66
Fig. 7. Speedup of SFA vs. DFA learning with GOFA.
the symmetric difference of each hypothesis automaton with
the target ﬁlter. In order to evaluate regular expression ﬁl-
ters we used the ﬂex regular expression parser to generate
a DFA from the regular expressions and then parsed the
code generated by ﬂex to extract the automaton. In order to
implement the GOFA algorithm we used the FAdo library [24]
to convert a CFG into Chomsky Normal Form(CNF) and
then we convert from CNF to a PDA. In order to compute
the intersection we implemented the product construction for
pushdown automata and then directly checked the emptiness
of the resulting language, without converting the PDA back to
CNF, using a dynamic programming algorithm [25]. In order
to convert the inferred models to BEK programs we used the
algorithm described in appendix C.
B. Testbed
Since our focus is on security related applications, in order
to evaluate our SFA learning and GOFA algorithms we looked
for state-of-the-art regular expression ﬁlters used in security
applications. We chose ﬁlters used by Mod-Security [26]
and PHPIDS [27] web application ﬁrewalls. These systems
contain well designed, complex regular expressions rulesets
that attempt to protect against vulnerability classes such as
SQL Injection and XSS, while minimizing the number of false
positives. For our evaluation we chose 15 different regular
expression ﬁlters from both systems targetting XSS and SQL
injection vulnerabilities. We chose the ﬁlter in a way that
they will cover a number of different sizes when they are
represented as DFAs. Indeed, our testbed contains ﬁlters with
sizes ranging from 7 to 179 states. Our sanitizer testbed is
described in detail in section VII-E. Finally, for testing our
102102
GOFA and ﬁlter ﬁngerprinting algorithms we also incorporated
two additional WAF implementations, Web Knight and Web
Castelum and Microsoft’s urlscan with a popular set of SQL
Injection rules [28]. For the evaluation of our SFA and DFA
learning algorithms we used an alphabet of 92 ASCII char-
acters. We believe that this is an alphabet size which is very
reasonable for our domain. It contains all printable characters
and in addition some non printable ones. Since many attacks
contain unicode characters we believe that alphabets will only
tend to grow larger as the attack and defense technologies
progress.
C. Evaluation of DFA/SFA Learning algorithms
We ﬁrst evaluate the performance of our SFA learning algo-
rithm using the L∗ algorithm as the baseline. We implemented
the algorithms as we described them in the paper using only
an additional optimization both in the DFA and SFA case: we
cached each query result both for membership and equivalence
queries. Therefore, whenever we count a new query we verify
that this query wasn’t asked before. In the case of equivalence
queries, we check that the automaton complies with all the
previous counterexamples before issuing a new equivalence
query.
In table I we present numerical results from our experi-
ments that reveal a signiﬁcant advantage for our SFA learning
over DFA: it is approximately 15 times faster on the average.
The speedup as the ratio between the DFA and the SFA number
of queries is showin in Figure 6. An interesting observation
here is that the speedup does not seem to be a simple function
of the size of the automaton and it possibly depends on many
aspects of the automaton. An important aspect is the size of the
sink transition in each state of the SFA. Since our algorithm
learns lazily the transitions, if the SFA incorporates many
transitions with large size, then the speedup will be less than
what it would be in SFAs were the sink transition is the only
one with big size.
D. Evaluation of GOFA algorithm
In this section we evaluate the efﬁciency of our GOFA
algorithm. In our evaluation we used both the DFA and the
SFA algorithms. Since our SFA algorithm uses signiﬁcantly
more equivalence queries than the L∗ algorithm, we need to
evaluate whether this additional queries would inﬂuence the
accuracy of the GOFA algorithm. Speciﬁcally, we would like
to answer the following questions:
1)
2)
How good is the model inferred by the GOFA algo-
rithm when no attack string exists in the input CFG?
Is the GOFA algorithm able to detect a vulnerability
in the target ﬁlter if one exists in the input CFG?
Making an objective evaluation on the effectiveness of the
GOFA algorithm in these two questions is tricky due to the
fact that the performance of the algorithm depends largely on
the input grammar provided by the user. If the grammar is too
expressive then a bypass will be trivially found. On the other
hand if no bypass exists and moreover, the grammar represents
a very small set of strings, then the algorithm is condemned
to make a very inaccurate model of the target ﬁlter. Next, we
tackle the problem of evaluating the two questions about the