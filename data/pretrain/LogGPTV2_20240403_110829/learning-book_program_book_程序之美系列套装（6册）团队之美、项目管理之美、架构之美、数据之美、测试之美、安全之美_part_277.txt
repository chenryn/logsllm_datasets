### 基于假设的金融产品交易

基于这种假设（即可交易的金融产品融入了贷款款项）并不一定是错误的。预测市场和群体决策制定过程在某些情况下能够非常出色地运作，甚至优于一组专家的估计。然而，当信息瀑布和相互依赖关系进入系统时，这些机制已被证明是脆弱的（Bikhchandani等, 1998）。数据并非独立存在。

### 现实世界中的决策制定

在现实世界的决策制定中，数据形式多样且复杂。很少有数据会被清理并打包成格式良好的电子表格或“矩阵文件”。相反，我们通常需要基于主观和量化信息来做出决策。例如，在决定是否通过P2P平台Prosper.com将资金借给他人以获取盈利时，我和同事分析了来自该平台的35万笔贷款数据。尽管使用了多种模型（包括混合模型、神经网络、决策树和回归），预测谁可以获得贷款以及谁能够准时偿还的准确率仅达到75%。尽管有大量的个人财务健康指标（超过100种），但哪些申请者能够获得资格仍难以预测。为了改善模型，我们引入了一些主观特征，如借款人的目的声明、公司形象、拼写、语法以及其他个人资料信息。我们还请人工对成员图像进行编码，评估其内容（如是否描述一个人、家庭、车辆等）以及“诚信”评分（即是否会借钱给这个人）。

### 社会因素的影响

然而，模型仍然存在不足：社会因素对贷款动态有着意想不到的作用。与我们的假设相反，贷款决策并不是独立的。证据表明，竞标过程中存在“羊群效应”：出借人会模仿其他出借人的行为，随着某个贷款上的竞标增加，单位时间内的竞标也加速。即使考虑这些情况和其他社会因素，许多出借人仍做出了次优的贷款决策。理论上，Prosper网站是一个接近完美的市场，几乎所有人都可以访问该站点的API并重复我们的分析。然而，出借人对于高风险投资往往只得到较低的回报，糟糕的投资数量令人惊讶。即使有可靠的信息和谦逊的数据代理，决策通常也不是完全基于数据直接指定的，数据只能部分解释人们的决策。

### 数据的认知偏见

最后，即使在可靠的因果关系可能的情况下，许多认知偏见也会模糊人们的思考。统计学家和行为经济学家经常在其博客上表达不满，指出诸如叙述性谬误、确定偏见、选择悖论、冒险、基础概率谬误以及过度贴现等问题。心理学家还记录了许多其他偏见，如“锚定”（过度依赖最近的单点数据来制定决策）和“Lake Wobegon效应”（超过一半的人认为自己高于平均水平）。随着这些影响被更好地记录，我们可以开发工具并利用直觉来帮助人们更有效地使用数据。从某种意义上说，解决方案很简单：如果你不理解数据的局限性，那么数据本身并没有太多价值。

### 结束语

总之，我们生活在一个数据丰富的时代。进化使我们能够注意到环境的显著变化（如老虎和海啸），但我们缺乏收集和整理大量异构数据集的基础设施。我们必须谨慎利用现有的基础设施，通过理解概率和概率的极限来更好地利用数据，并对有助于解释世界的认知偏见持审慎态度。观察者眼中的数据确实可以很美！

### 参考文献
[此处列出参考文献]

---

### 自然语言语料库数据

#### 引言

本书大部分阐述的是“波德莱尔意义”上的数据之美：“一切美好和崇高的都是理性和计算的结果。”本章则探讨“梭罗意义”上的数据之美：“人们总是被最平凡的演讲之美打动。”本章要讨论的数据是“最平凡的演讲”：取自公开Web页面中的总长度达1MB英语单词的数据。这些数据涵盖了Web上的所有“陈词滥调”，既有拼写和语法错误、哈哈大笑（LOL）的猫、踢踢滚滚（Rickrolling），也包含马克·吐温、狄更斯、奥斯汀和几百万其他作家的作品集。

#### 语料库数据的发布

G公司的Thorsten Brants和Alex Franz于2006年发布了1MB的单词数据，你可以在语言数据联盟（Linguistic Data Consortium）上获取（http://tinyurl.com/ngrams）。该数据集通过计算每个单词的出现次数，并按二元、三元、四元和五元序列对原始文本求和。例如，“the”这个单词出现了230亿次（占1MB单词的2.2%），是最常用的单词。“rebating”出现了12750次（占百万分之一），还有“fnmuny”（显然是“funny”的误拼）。在三元序列中，“Find all posts”出现了1300万次（占0.001%），“each of the”出现频率相似，但都低于出现了1亿次的“All Rights Reserved”（占0.01%）。

#### 语料库数据的美丽

为什么我说数据很美并不乏味？单个计数可能是乏味的，但这些计数的集合——几亿个计数——是美丽的，因为它具有很多含义。这些计数不仅仅是英语，而是关于说英语的人们的世界。数据是美丽的，因为它表示了很多值得表达的东西。

#### 语料库术语

对于这些数据，在观察可以做什么之前，我们需要学习一些术语。文本的集合称为语料库。我们将语料库中的每个单词称为token，因此文本“Rum, Lola Run”包含四个token（逗号也算一个token），但只有三种类型。类型的集合称为词汇。G公司语料库包含1MB的token和1300万种类型。英语词典中出现的单词大约只有100万，但语料库包含更多类型，如“www.njstatelib.org”、“+170.002”、“1.5GHz/512MB/60GB”和“Abrahamovich”。大多数类型很少见，最常见的10个类型占近三分之一的token，最常见的1000个类型占超过三分之二，而最常见的10万个类型占98%。

#### N-gram

1-token序列是一元，2-token序列是二元，n-token序列是N-gram。P代表概率，如P（the）= .022，表示token “the”的概率是0.022或2.2%。如果用W表示token序列，那么W3表示第三个token，而W1:3表示从第一个到第三个token。P(Wi=the | Wi-1=of)是token “the”的条件概率，表示在先验token “of”下的概率。

#### G公司语料库的细节

出现次数少于200次的单词作为未知类型，以符号表示。丢弃出现次数少于40次的N-gram。该策略减少了打字错误的影响，使得数据集只有24G（压缩后）。最后，语料库的每个句子都以特殊的字符开始和结束。

#### 应用示例

**分词**

考虑中文文本“浮法像蝴蝶”，它的英语翻译是“float like a butterfly”。它包含5个字符，但没有空格，因此中文阅读者需要执行分词：决定单词的边界位置。英语阅读者通常不需要执行分词，因为单词间存在空格。但在某些文本（如URL）中，通常不包含空格，有时输入错误会留下空格；搜索引擎或文本处理程序如何纠正这样的错误呢？

---

希望这些优化后的段落更加清晰、连贯和专业。如果有任何进一步的需求，请告诉我！