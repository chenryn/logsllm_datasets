every ciphertext µi, and then use the homomorphic property of the MAC to authenticate the BV
homomorphic evaluation. While this idea may work for appropriate choices of the MAC scheme
(e.g. [7]), we note that BV ciphertexts consist of several components over a ﬁeld Fq: in detail,
q → F3n
if f : Ft
q .
We improve this situation using our key technical contribution: homomorphic hash functions Hκ
that allow to compress a BV ciphertext into a single Fq-component, while preserving the homo-
morphic properties, i.e. f (Hκ(µ1), . . . , Hκ(µt)) = Hκ( ˆf (µ1, . . . , µt)). By applying a MAC on top of
the hashed ciphertexts, we save at least a factor of 3n in all operations (e.g. input outsourcing,
computation, veriﬁcation). Considering that for security reasons (and technical details discussed
later) n can be as large as 5000, applying the homomorphic hash leads to schemes that are up to
four orders of magnitude faster. Compare the last two columns of Table 2 for the concrete example
of computing the variance of 1000 items.
p → Fp is the desired function, the BV evaluation circuit is a function ˆf : F2nt
Without
Privacy [7]
Our Approach
Naive
(Section 5)
Approach
Server Computation
Client Veriﬁcation
0.98sec
0.21ms
1.11sec
0.42ms
15210.61sec
3316.63ms
Table 2. Comparision of the amortized costs for calculating the variance in a database of t = 1000 entries. Security
in the second and third column is 80bit.
Experiments. We implemented the above solutions, and tested their practical performances. Re-
markably, all our protocols run considerably fast. See section 10 for details.
An interesting point is that the cost (in terms of CPU time) of adding privacy to veriﬁability is
relatively small. As an example, for 80bit security, scheme in section 5: on the server side, the total
execution time of any operation, is between 1.1 and 2.2 times the execution time for authentication
operations only (i.e. excluding encryptions, FHE evaluations, etc. from the total cost), depending
on the particular function. The highest privacy cost (deﬁned as: total execution time over authenti-
cation time, as above), 2.9, occurs in one-time operations: encrypting and authenticating the data
when loading it into the server. Finally, our veriﬁcation algorithm has a privacy cost of 1.9×, due
to decryption of the result. Compare the ﬁrst two columns of Table 2 for a concrete example, and
see Table 4 for more details. Notice that our timing for authentication only is comparable with the
timing of [7] (adapted with a secure pairing).
Finally, we point out that, even if minimal, the privacy overhead can be mitigated by using
the batching technique of Smart and Vercauteren [58]: for 80bit (resp. 128 bit) security we could
6
encrypt 165 (resp. 275) 32bit plaintext items in a single ciphertext. This means the amortized cost
(per plaintext item) of our scheme is actually better than [7].
1.2 Other Related Work
Generic Protocols. Another generic protocol for private and veriﬁable outsourced computation
appears in [34], based on functional encryption (FE). In the introduction of [34] it is shown how to
use an FE scheme to construct a VC scheme that operates on encrypted inputs and is secure even
in the presence of veriﬁcation queries; in particular it is publicly veriﬁable – anybody can verify
the correctness of the computation – and this property is achieved using an idea that originates in
[50], for the case of attribute-based encryption.
We point out that the above construction is only sketched in [34], together with an intuitive
motivation for its security, but no formal deﬁnition or proof is given. When one formally analyzes
the protocol in [34] in light of our deﬁnition the following issues come up.
– As discussed in [17], a simulation-based deﬁnition of security for FE cannot be achieved in
general in an “adaptive” model in which the adversary chooses the function f to be evaluated
after seeing the ciphertext encoding x. It appears that this limitation might be inherited by the
FE-based veriﬁable computation in [34], though the situation is far from clear. Our security def-
inition is a game-based one, but the security proof might require the FE tool to be simulatable.
A proof of the [34] protocol in this model is important, since in many cloud computing appli-
cation the data might sit encrypted on a cloud server, before any processing is performed on it.
In contrast, we can provably show that our proposed protocol does not have this limitation;
– Additionally, the protocol in [34] only achieves selective security (the adversary has to commit in
advance to the input value on which it wants to cheat), essentially because known constructions
of FE [34,25] achieve only selective security against poly-time adversaries, and full security
can be obtained at the cost of assuming security against sub-exponential adversaries: a much
stronger assumption. Our protocol has the advantage of being full secure against poly-time
adversaries.
– Finally, the scheme in [34] intrinsically works for binary functions. While this is suﬃcient in
theory, in practice it requires the overhead of working bit-by-bit on the function output. Our
protocol is more versatile and potentially more eﬃcient, since it can handle arithmetic circuits
if the underlying FHE and VC schemes can (several FHE schemes work over arithmetic circuits,
and the QSP/QAP approach in [27] yields an eﬃcient VC over arithmetic circuits).
An issue on the security of the scheme in [26] was noted by Bellare et al. in [9] where the authors
investigate the notion of adaptive garbling schemes, and show that under the existence of these
schemes (realized later in [8]) the construction of [26] becomes secure in the presence of veriﬁcation
queries. Compared to [9], we fully formalize the notion of security in the presence of veriﬁcation
queries and, in particular, propose a stronger notion of adaptive security that allows the adversary
to obtain encodings of the input even before choosing the function.
Ad-Hoc Protocols. The task of evaluating univariate polynomials of large degree was earlier con-
sidered in [12]. However, in that protocol, when the polynomial is encrypted, the client’s acceptance
bit depends on the decrypted value, which creates the opportunity for a veriﬁcation query attack.
In contrast, our solution (which builds on [12] in a slightly diﬀerent way) enjoys security in the
stronger model where veriﬁcation queries are allowed.
7
In [44] Libert et al. propose a protocol for the evaluation of linear combinations over encrypted
data using additively homomorphic encryption and structure-preserving linearly homomorphic sig-
natures. However, their protocol is restrictive, since the linear combinations have to reside in a very
small range in order for the client to retrieve the correct result: the client’s decryption consists in
solving discrete log, by which the exponents must be small. Compared to this work, our solutions
for linear combinations can support large domains and additionally provide function privacy.
The task of verifying computations on encrypted data has also been considered in [40,22] via
the notion of homomorphic authenticated encryption. In a nutshell, this primitive enables a client
to encrypt a set of data using a secret key in such a way that anyone can then execute functions on
this data obtaining encrypted results whose correctness can be veriﬁed. In particular, [40] proposes
a construction for the evaluation of low-degree polynomials which is secretly veriﬁable, while [22]
proposes a scheme for linear functions that achieves public veriﬁability. However, we note that both
these constructions [40,22] are not outsourceable, in the sense that verifying each computation is
as expensive as running the delegated function.
2 Problem Deﬁnition
We work in the amortized model of [26] where the client runs a one-time expensive phase to
outsource the function f to the server (this phase can cost as much as the computation of f ). Later
the client queries the server on (an encrypted form of) input x and receives back (an encryption
of) the value f (x) and a proof of its correctness: this phase should be eﬃcient for the client (ideally
linear in |x| + |f (x)|).
In [26] the authors give a deﬁnition that includes both security (i.e. the client only accepts
correct outputs) and privacy (i.e. the client’s input x is semantically hidden to the server) but
does not allow veriﬁcation queries. In this section we upgrade the deﬁnition by adding veriﬁcation
queries to it. Moreover, we introduce the concept of adaptive security.
A veriﬁable computation scheme VC = (KeyGen, ProbGen, Compute, Verify) consists of
the following algorithms:
KeyGen(f, λ) → (P K, SK): Based on the security parameter λ, the randomized key generation
algorithm generates a public key (that encodes the target function f ) which is used by the
server to compute f . It also computes a matching secret key, kept private by the client.
ProbGenSK(x) → (σx, τx): The problem generation algorithm uses the secret key SK to encode
the input x as a public value σx that is given to the server to compute with, and a secret value
τx which is kept private by the client.
ComputeP K(σx) → σy: Using the client’s public key and the encoded input, the server computes
VerifySK(τx, σy) → (acc, y): Using the secret key SK and the secret τx, the veriﬁcation algorithm
converts the server’s output into a bit acc and a string y. If acc = 1 we say the client accepts
y = f (x), if acc = 0 we say the client rejects.
an encoded version of the function’s output y = f (x).
We now recall the three main properties deﬁned in [26] for a veriﬁable computation scheme:
correctness, security, privacy, and outsourceability, but we deﬁne them in the presence of veriﬁcation
queries by the adversary. Next, we introduce function privacy, that is the ability of a scheme to
hide from the server the function that it needs to compute. Finally we move to adaptive security
8
A scheme is correct if the problem generation algorithm produces values that allow a honest
server to compute values that will verify successfully and correspond to the evaluation of f on those
inputs. More formally:
Deﬁnition 1 (Correctness). A veriﬁable computation scheme VC is correct if for all f, x: if
– (P K, SK) ← KeyGen(f, λ),
– (σx, τx) ← ProbGenSK(x), and
– σy ← ComputeP K(σx),
then (1, y = f (x)) ← VerifySK(τx, σy).
For the other notions, we need to deﬁne the following oracles.
– PProbGen(x) calls ProbGenSK(x) to obtain (σx, τx) and returns only σx.
– PVerify(τ, σ) returns acc if and only if VerifySK(τ, σ) = (acc, y). In other words, PVerify
is the public acceptance/rejection bit which results from a veriﬁcation query. When we write
APVerify we mean that A is allowed to query PVerify(τ,·) where τ can be the secret encoding
of any of the queries made in PProbGen, or also τb in the case of the privacy experiments.
Intuitively, a veriﬁable computation scheme is secure if a malicious server cannot persuade the
veriﬁcation algorithm to accept an incorrect output.
[VC, f, λ]
Experiment ExpV erifA
(P K, SK) ← KeyGen(f, λ);
For i = 1, . . . , (cid:96) = poly(λ);
xi ← APVerify(P K, x1, σ1, . . . , xi−1, σi−1);
(σi, τi) ← ProbGenSK(xi);
(i, ˆσy) ← APVerify(P K, x1, σ1, . . . , x(cid:96), σ(cid:96));
( ˆacc, ˆy) ← VerifySK(τi, ˆσy)
If ˆacc = 1 and ˆy (cid:54)= f (xi), output ‘1’, else ‘0’;
Essentially, the adversary is given oracle access to generate the encoding of multiple problem
instances, and to check the response of the client on arbitrary “encodings”. The adversary succeeds
if it produces an output that convinces the veriﬁcation algorithm to accept on the wrong output
value for a given input value. We can now deﬁne the security of the system based on the adversary’s
success in the above experiment.
Deﬁnition 2 (Security). A veriﬁable computation scheme VC is secure for a function f , if for
any adversary A running in probabilistic polynomial time,
We say that VC is secure if it is secure for every function f .
Pr[ExpV erifA
[VC, f, λ] = 1] ≤ negl(λ).
Input privacy is deﬁned based on a typical indistinguishability argument that guarantees that
no information about the inputs is leaked. Input privacy, of course, immediately yields output
privacy.
Intuitively, a veriﬁable computation scheme is private when the public outputs of the problem
generation algorithm ProbGen over two diﬀerent inputs are indistinguishable(i.e., nobody can
decide which encoding is the correct one for a given input). More formally, consider the following
experiment: the adversary is given the public key for the scheme and selects two inputs x0, x1. He
9
is then given the encoding of a randomly selected one of the two inputs and must guess which one
was encoded. During this process the adversary is allowed to request the encoding of any input he
desires, and also is allowed to make veriﬁcation queries on any input. The experiment is described
below.
Experiment ExpP rivA [VC, f, λ]
b ← {0, 1};
(P K, SK) ← KeyGen(f, λ);
(x0, x1) ← APVerify,PProbGen(P K)
(σ0, τ0) ← ProbGenSK(x0);
(σ1, τ1) ← ProbGenSK(x1);
ˆb ← APVerify,PProbGen(P K, x0, x1, σb)
If ˆb = b, output ‘1’, else ‘0’;
Deﬁnition 3 (Privacy). A veriﬁable computation scheme VC is private for a function f , if for
any adversary A running in probabilistic polynomial time,
Pr[ExpP rivA [VC, f, λ] = 1] ≤ 1
2
+ negl(λ).
Function privacy is the requirement that the public key P K, sampled via (P K, SK) ← KeyGen(f, λ),
does not leak information on the encoded function f , even after a polynomial amount of runs of
ProbGenSK on adversarially chosen inputs. More formally, we deﬁne function privacy based on
an indistinguishability experiment as follows.
[VC, λ]
Experiment ExpF P rivA
(f0, f1) ← A(λ);
b ← {0, 1};
(P K, SK) ← KeyGen(fb, λ);
For i = 1, . . . , (cid:96) = poly(λ);
xi ← APVerify(P K, x1, σ1, . . . , xi−1, σi−1);
(σi, τi) ← ProbGenSK(xi);
ˆb ← APVerify(P K, x1, σ1, . . . , x(cid:96), σ(cid:96));
If ˆb = b, output ‘1’, else ‘0’;
Deﬁnition 4 (Function Privacy). A veriﬁable computation scheme VC is function private, if
for any adversary A running in probabilistic polynomial time,
Pr[ExpF P rivA
[VC, λ] = 1] ≤ 1
2
+ negl(λ).
The next property of a veriﬁable computation scheme is that the time to encode the input and
verify the output must be smaller than the time to compute the function from scratch.
Deﬁnition 5 (Outsourceability). A VC can be outsourced if it allows eﬃcient generation and
eﬃcient veriﬁcation. This implies that for any x and any σy, the time required for ProbGenSK(x)
plus the time required for Verify(σy) is o(T ), where T is the time required to compute f (x).
We now introduce the notion of adaptive security for a veriﬁable computation scheme. Intuitively,
an adaptively secure scheme is a scheme that is secure even if the adversary chooses f after having
seen many “encodings” σx for adaptively-chosen values x. At ﬁrst sight, this property is non-trivial
10
to achieve, since not every scheme allows σx to be computed before choosing f (in particular schemes
based on FE such as [34]). This observation leads us to ﬁrst deﬁne a reﬁned class of schemes, for
which adaptivity is not ruled out by this restriction, and then proceed with the actual deﬁnition of
adaptivity.
Deﬁnition 6 (Split Scheme). Let VC = (KeyGen, ProbGen, Compute, Verify) be a veriﬁ-
able computation scheme. We say that VC is a split scheme if the following conditions hold:
– There exist PPT algorithms KeyGenE(λ), KeyGenV (f, λ) such that:
if (P K, SK) ← KeyGen(f, λ), then
P K = (P KE, P KV ) and SK = (SKE, SKV ),
where KeyGenE(λ) → (P KE, SKE) and KeyGenV (f, λ, P KE, SKE) → (P KV , SKV ).
– There exist PPT algorithms ProbGenE
if (σx, τx) ← ProbGenSKE ,SKV (x), then