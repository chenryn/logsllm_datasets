client’s TLS handshake with OD. When SP detects the
5http://www.tcpdump.org/
6http://www.squid-cache.org/
7http://www.openssl.org/
192TLS handshake between the client and OD has completed,
it “changes the CipherSpec” of the TLS connection with the
client to use the stream cipher RC4 (though a real imple-
mentation should use the same cipher as agreed upon by
the client and OD), and the “CipherSpec” (the cipher key
and MAC secrets) is derived from kC,RS . SP also clears the
read and write TLS sequence numbers to 0. The SP then
(optionally) closes its TCP connection with OD and immedi-
ately creates a new TCP connection to a local SOCKS proxy,
for which we use 3proxy8 version 0.6.1. (Alternatively, one
could add the SOCKS protocol support into squid itself and
not have to connect to a separate SOCKS proxy.) After-
wards, squid simply tunnels traﬃc between the client and
the SOCKS proxy.
5.4 Client-side proxy
It is undesirable to require modiﬁcations to existing appli-
cations to use Cirripede. Thus we employ at the client host
a local proxy, similar to the Tor proxy. This local proxy—
henceforth referred to as client proxy or CP—exposes an ap-
parent (see below) SOCKS interface. The CP is conﬁgured
with the hostnames/IP addresses of two servers to whom
packets from the client host will pass through a DR, though
these two servers can be the same. The ﬁrst one is used by
the registration phase, and the second one is the OD. Upon
starting, the CP will generate TCP traﬃc towards the DR
to register itself with Cirripede. The generated TCP traﬃc
needs to contain the special ISNs, so either kernel support
or a userspace TCP stack is necessary. Our prototype CP
simply generates SYN packets using raw sockets, without
using a full application connection.
Then, applications at the client host can use CP as a reg-
ular SOCKS proxy. However, CP does not interpret the
SOCKS requests. Instead, upon receiving the TCP connec-
tion of the request, it initiates an HTTPS connection to the
OD and proceeds to complete the TLS handshake with OD.
Then it “changes the CipherSpec” and clears the TLS se-
quence numbers similar to the SP , and then it expects the
very next TLS record (of type “application data”) to con-
tain the conﬁrmation message. If that is the case, meaning
CP is in fact connected to the SP , then CP proceeds to
simply tunnel traﬃc between the application and SP over
the TLS channel, without interpreting the SOCKS protocol.
Otherwise, it rejects the SOCKS request.
6. EVALUATION
We evaluate the registration component and the through-
put provided by Cirripede with experiments on the Univer-
sity of Utah’s Emulab testbed [55]. Also, we use simulations
to study the eﬀect of DR deployment on the ability of clients
to register with Cirripede.
6.1 Registration performance
6.1.1 Metrics
We are interested in the ability of the RS to handle real
traﬃc load. The two main metrics are: (1) the fraction of
registration signals that the RS can detect, and (2) the load
on the RS (in particular, CPU and memory utilization).
8http://www.3proxy.ru/
6.1.2 Experiment setup and topology
For this experiment, we take an existing packet trace and
embed registration signals into the existing packets, without
introducing new packets. We use a one-hour trace [32] cap-
tured in March, 2011 at CAIDA’s equinix-sanjose monitor,
and ﬁlter it to keep only TCP SYN packets. We assume
that all clients in the trace want to register; however, each
client registers only once, at the earliest opportunity. Be-
cause we do not inject new packets, a client needs at least
12 SYN packets to register. Out of over 94 million SYN
packets from over 6.4 million unique client IP addresses, we
can embed only 1,069,318 complete registrations.
The experiment consists of two machines: the DR and
the RS . Both are 2.4 GHz 64-bit Quad Core Xeon E5530
machines, with 12 GB of RAM, running Ubuntu 10.04 64-
bit. The machines are connected via a 1 Gbps Ethernet link
with zero latency. The DR simply uses tcpreplay9 to re-
play the processed packet trace (which contains only TCP
SYN packets) against the RS . The replay speed is about
41,000 packets/second, resulting in a replay duration of 2300
seconds, which puts more pressure on the RS than a live cap-
ture would have. The RS uses four threads, each handling a
diﬀerent set of clients, partitioned by the hash of the client
IP address. The validation interval is one hour, eﬀectively
meaning the RS does not timeout any client during the ex-
periment. The RS uses sar10 to collect CPU and memory
utilizations at two second intervals.
6.1.3 Results
The RS is able to receive 100% of packets that the DR
successfully sends. It detects 1,038,689 registrations, which
is a 97% success rate. We manually inspect a few of the
missed registrations and ﬁnd the cause to be out-of-order
packets. Investigating further, we ﬁnd via tcpdump that the
network is at fault: the DR sends packets in the correct
order, but they arrive at the RS re-ordered.
In terms of
the load on the RS , for the duration of the experiment,
the average CPU utilization is 56% and the max 73%. The
average memory utilization is 1.1 GB and the max 1.6 GB.
The memory utilization increases through the experiment,
but this is as expected because as noted above the RS does
not timeout any client. From these results, we believe the
registration component of Cirripede scales well.
6.2 Throughput performance
6.2.1 Metrics
For these experiments, we only measure the performance
of downloading data from a web server (thus all clients are
pre-registered with Cirripede before each experiment). The
ﬁrst metric we are interested in is the time to download the
ﬁrst byte. This is a measure of the perceived responsiveness
of loading a website, especially for fetching small amounts
of data such as a static web page. The second metric is the
full page download time (though we only download a single
ﬁle in our experiments, instead of a full web page containing
multiple objects, possibly from diﬀerent servers).
6.2.2 Experiment setup and topology
All hosts in the experiments, including the routers, are
2.4 GHz 64-bit Quad Core Xeon E5530 machines with 12 GB
9http://tcpreplay.synﬁn.net/
10http://sebastien.godard.pagesperso-orange.fr/
193F
D
C
1.0
0.8
0.6
0.4
0.2
0.0
0
2
4
6
8
10
12
Time to first byte (second)
(a)
F
D
C
1.0
0.8
0.6
0.4
0.2
0.0
0
20
40
60
80
100 120 140
Transfer time (second)
(b)
Without Cirripede
With Cirripede
Figure 3: CDF of the time to receive the ﬁrst byte and the total time to download a 10 MB ﬁle, with and
without using Cirripede, for 100 simultaneous clients, each downloading the ﬁle 100 times.
1.0
0.8
0.6
0.4
0.2
F
D
C
0.0
1.1
1.3
1.2
1.7
Time to first byte (second)
1.4
1.5
1.6
1.0
0.8
0.6
0.4
0.2
F
D
C
1.8
0.0
2
2 Mbps
10 Mbps
50 Mbps
100 Mbps
16
6
4
Transfer time (second)
10
12
8
14
(a)
(b)
Figure 4: CDF of the time to receive the ﬁrst byte and the total time to download a 1 MB ﬁle for four
simultaneous clients, each downloading the ﬁle 100 times using Cirripede.
of memory, running CentOS 5.5 64-bit. Unless otherwise
speciﬁed, the links are 1 Gbps. Five servers run the Apache
web server11 version 2.2.3 with SSL support enabled. Five
client hosts use curl12 version 7.15.5 as the application to
fetch ﬁles from the servers, using HTTP. Due to NIC limits,
the clients are connected to the DR via two intermediate
routers; however, in all experiments, the link bandwidths at
the client hosts are the bottleneck. The DR is on the path
between all client-server pairs. The SP is directly connected
to the DR, and both the SP and DR have RTT of 150 ms to
all clients and 50 ms to all servers. Thus, the eﬀective RTT
between all client-server pairs is 200 ms, approximating a
client in Asia accessing a server in the US.
In the ﬁrst set of experiments, all ﬁve client hosts have link
bandwidths of 100 Mbps. On each client host, we launch 20
simultaneous “client” instances, each using curl to download
a 10 MB ﬁle from a particular server 100 times over HTTP.
Across all ﬁve client hosts, we have 100 “client” instances.
We will compare results from using Cirripede and without
using Cirripede.
In the second set of experiments, we use four client hosts,
with diﬀerent link bandwidths to the network: 2 Mbps,
10 Mbps, 50 Mbps, and 100 Mbps. Each client host runs only
one client instance, using curl to download a 1 MB ﬁle from
a server 100 times. All clients use the Cirripede service.
11http://www.apache.org/
12http://curl.haxx.se/
6.2.3 Results
For the ﬁrst set of experiments, Figure 3(a) shows the
results for the time to the ﬁrst byte. We see that Cirripede
adds a delay of no more than a few seconds, most of which
is due to the two extra round-trips of the TLS handshake
and the SOCKS request-response. Figure 3(b) compares
the total download times. The main take-away point is that
Cirripede provides comparable performance to the baseline
of not using Cirripede. For this particular setup, Cirripede
can also result in faster download time. This is because
high latencies negatively aﬀect TCP throughput. So, when
we use the SP , the original TCP connection is split into
two separate TCP connections, each with a lower RTT, so
each of these two connections has a higher throughput than
the original connection would have. Thus the eﬀective end-
to-end throughput is improved.
(We performed separate
experiments using a standard, non-Cirripede SOCKS proxy
to conﬁrm this behavior.)
Figure 4 shows the results of the second set of experi-
ments. We can see in Figure 4(a) that as expected, higher
bandwidths improve performance. However, because of the
high RTTs between clients and servers, increasing the band-
width yields diminishing returns for a standard TCP.
6.3 DR deployment simulation
In order to use Cirripede, a client needs to discover a path
to a website that traverses a DR. To ensure this happens
commonly, the provider of the Cirripede service needs to
194R
D
a
h
c
a
e
r
n
a
c
t
a
h
t
s
c
r
s
.
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
frac dests=0.001
frac dests=0.002
frac dests=0.005
frac dests=0.01
frac dests=0.02
frac dests=0.03
frac dests=0.05
R
D
a
h
c
a
e
r
n
a
c
t
a
h
t
s
c
r
s