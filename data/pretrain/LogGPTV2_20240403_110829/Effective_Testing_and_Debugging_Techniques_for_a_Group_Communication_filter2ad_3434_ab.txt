we had opted for ofﬂine veriﬁcation instead, we would have
had to log a lot of additional information thereby inﬂating
the traces and incurring signiﬁcant overhead.
4.3 Distributed invariants
Here are the most signiﬁcant distributed invariants that
DCS should maintain during its operation: (1) Member-
ship layer guarantees: all view members agree on the same
view members, id, and leader. Veriﬁcation of this property
provides an indication whether the occurred chain of view
changes is legal or not. (2) Virtual synchrony, reliable de-
livery and Self-delivery guarantees - see Section 2.1. (3)
Eventual process join and removal: If a new process has
connected/disconnected to/from a group, a new view that
includes/excludes this member is eventually established.
In order to verify these invariants, we need to correlate
and analyze events that occur at different processes. As con-
structing a distributed snapshot during the run changes the
timing and imposes performance overhead, the testing sys-
tem veriﬁes distributed invariants by analyzing the logs after
the test is completed. This post-mortem log analysis is done
based on the information that the Tester layer records into
the log ﬁle. While still non-trivial, the analysis is facilitated
by the fact that group communication systems inherently
use logical timestamps for certain events.
Let us consider an example of a global event sequence
that occurred in one of our tests. This example will be used
throughout the paper to demonstrate the elements of our so-
lution.• Members A, B, C are together in view with view
id=100. There is a corresponding entry in the log of each
member, which describes the view id, leader and members.
• C crashes. As a result, A establishes a view with view
id=101, view leader A, and no additional members. B es-
tablishes a view with view id=101, view leader B, and no
additional members.
• A and B establish a view with view id=102, view leader
A and members A and B.
Figure 3 depicts all relevant log entries for this scenario.
Assuming no additional failures occurred, the scenario rep-
resents a counter-intuitive situation in which A and B estab-
lish a separate view each rather than a single common view,
as we would expect it to happen. The situation is mended
afterwards since A and B eventually establish a view to-
gether. However, this short period of time when they estab-
lish separate views, could potentially signify an implemen-
tation bug. We call such a situation “unjustiﬁed split event”.
The next section shows how the log analyzer operates in this
scenario.
4.3.1 Distributed log analyzer
After a system run, the log analyzer gathers all logs, ex-
tracts all relevant log entries, reconstructs a global execu-
tion of the system, builds helper data structures, and invokes
various post-mortem checkers to verify that the aforemen-
tioned distributed invariants are preserved.
By extracting only small amount of relevant information
from all log ﬁles and checking the maintenances of DCS in-
variants based on this information, the log analyzer solves
the problem of large amount of available distributed infor-
mation that is hard to interpret (as explained in Section 3).
Automatic analysis of the gathered information relieves the
developer from the burden of interpreting and manually an-
alyzing the traces. The post-mortem checking system ﬁnds
a violation of the invariants, it supplies the developer with
sufﬁcient information to facilitate root cause analysis.
The main challenge of reconstructing consistent global
history is to correlate multiple log events from different
logs. Log analyzer deals with the correlation problem
by combining two approaches: logical clocks [5] and real
clocks.
Logical clocks are a standard technique for establish-
ing correspondence and capturing the partial “happened be-
fore” order between distributed events. Their use for DCS
is particularly facilitated by the fact that view identiﬁers
(natively supported by group communication systems) es-
sentially represent logical clocks for view-related events as
long as processes do not crash. However, group communi-
cation systems do not maintain monotonic increase of view
identiﬁers in face of process crashes, as explained in Sec-
tion 2.1. Furthermore, there is still a need to implement log-
ical clocks for events other than view installations, which
may not be necessarily traceable by the testing system in a
non-intrusive way.
On the other hand, real clocks are inherently persistent
and events can be easily tagged with them without introduc-
ing extra communication or computation overhead. More-
over, most implementation components not controllable by
the test monitor autonomously log the time at which the
most important events occur. Of course, it is commonly
known that capturing the order of events based solely on
real clocks is impossible to do reliably because of clock
skew between different processes.
4
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:55 UTC from IEEE Xplore.  Restrictions apply. 
every view node: (1) Each process that is reported as a view
member by other processes, has a log record that corre-
sponds to this view; (2) The above mentioned VS and re-
liable delivery guarantees; (3) Various test-related checks.
For instance, the unjustiﬁed split in the above example
should not have occurred in that particular test. In order to
detect such split events, we introduce a test-speciﬁc check
that there be at most a single outgoing edge from every node
in the graph.
When such a post-mortem check detects a problem, the
developer will be supplied with all relevant information
available in the logs: description of the problematic event,
view number and names of the members involved, exact
time of its occurrence at each participating member, lines
in the log ﬁle from which the relevant log entries were ex-
tracted, etc. Afterwards, this information will be efﬁciently
used in root cause analysis.
An important trait of the post-mortem analyzer is that it
is not tightly coupled with the speciﬁc DCS implementation
and events. In fact, it may be used by any group commu-
nication system that logs sufﬁcient view membership infor-
mation (view id, view leader, view members and real time)
and provides an appropriate log parser. Post-mortem ana-
lyzer already supports additional types of logs, such as the
standard WAS logs, which include only a small part of DCS
log events. This allows WAS system administrators to use
it as an external tool at a client site.
4.4 Concurrency-related debugging techniques
One of the most important tasks in our tests was dead-
lock elimination. This is by no means a trivial task in
the multi-component environment of DCS, as explained
in Section 3. When searching for the solution, we no-
ticed that both SUN and IBM JDKs have a useful fea-
ture of detecting deadlocks during the JVM core dump,
which can be triggered by sending a signal to the JVM
process. Unfortunately, this is an extremely costly opera-
tion, which we could not afford to invoke periodically in a
production release. In DCS, it is employed in conjunction
with our own watchdog-based deadlock-detection mecha-
nism (WDD), which is much more efﬁcient but imprecise
as it may raise false suspicions. When WDD suspects that
a deadlock has occurred, it sends a signal to the JVM that
records all the relevant information into the trace ﬁle.
The WDD mechanism is based on a pair of timestamps
and an independent watchdog thread that runs in parallel
with the other threads in the system. The ﬁrst timestamp
Figure 4. View graph for the example.
Once correspondence for view-related events is estab-
lished, the analyzer starts the second phase and attempts
to verify the correctness of all other events. This is done
by separately considering each group of events that are en-
closed by the preceding and succeeding view-related events.
4.3.2 Post-mortem checks
Once the view graph is built, various post-mortem
checks are executed on it. The following should hold for
5
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:55 UTC from IEEE Xplore.  Restrictions apply. 
(TS1) is updated by the DCS self-checking mechanism that
periodically obtains the DDLock (recall from Section2.2
that this lock is obtained by the stack to propagate the mes-
sage through its layers) and updates TS1.1 Deadlock in the
system, which involves the DDLock, would make these pe-
riodic updates impossible. The second timestamp (TS2) is
periodically updated by a watchdog thread.
The watchdog thread checks when both timestamps were
previously updated. If both timestamps have been recently
updated, this thread does nothing. Otherwise, one of two
cases could happen:
1. The watchdog timestamp (TS2) has not been updated
for a considerable period of time. Since the watchdog thread
does not obtain any lock, the only reason for not updating
its timestamp is a JVM “freeze”, i.e., the user threads of the
JVM do not get an adequate CPU time slice for a consider-
able period of time. In this case the watchdog thread issues
a warning about a possible JVM freeze.
2. The watchdog timestamp (TS2) has been recently up-
dated, but the self-checking timestamp (TS1) has not been
updated for a considerable period of time. The fact that
the watchdog thread succeeded to update its timestamp ex-
cludes the possibility of a JVM freeze, so the only ob-
stacle that prevents the self-checking thread to update its
timestamp is a continuous failure to obtain the lock. This
situation typically implies that the system ran into a dead-
lock.
The biggest advantage of this mechanism is that it elim-
inates the need to recreate deadlocks. Once a bug occurs
during the test run, all the information needed for the de-
veloper to solve the problem is logged. Furthermore, the
overhead of this mechanisms in term of runtime is less than
one percent for the most frequently used code paths.
5 Conclusions
Using the testing suite signiﬁcantly improved the test-
ing quality and the overall code quality of DCS. From the
standpoint of the testing process as presented in Figure 1,
this work contributed to the test monitoring, log analysis,
and root cause analysis:
Early detection of costly elusive bugs:
the test monitor
exposed bugs such as non-deterministic deadlocks and vi-
olation of the message delivery guarantees, that would be
very costly to analyze and ﬁx once reported in a production
environment. This is attributed to the ability of the tester
layer to perform checks “under the cover” and thereby ex-
pose bugs that have no external symptoms during the run.
Minimizing test results analysis time:
the distributed
log analyzer provides the ability to determine in a matter
of seconds whether a long test, created by our test genera-
tor, completed successfully. This is attributed to the oper-
1This thread also responsible for the DCS layers health check.
ation of the automatic post-mortem checkers. Without this
tool, manual examination of the night test results was taking
frustrating four hours of two testers on average.
Faster root cause analysis: using the testing suite signif-
icantly reduced the average time from the point a problem
was exposed until the point the root cause was determined.
This is attributed to the facilitating tools that the tester suite
provides for the developer. In particular, it required an av-
erage of two days to pinpoint a deadlock problem without
the testing suite, whereas with the testing tool, this task was
usually achieved in less than two hours. During the last
year we exposed 9 deadlocks in our system so that the test-
ing suite saved us about two weeks of root cause analysis
over that period.
Consequently, the testing suite signiﬁcantly enhanced
the efﬁciency of our testing procedure. The automatic
analysis of the test results enabled us to run more tests and
ﬁnd more bugs, some of which would not have been ex-
posed without the tool. An indication for this is that the
number of bugs exposed using our testing suite during the
last year (about 190) was more than twice than the num-
ber of bugs exposed during external tests (about 85), which
were conducted by several testing teams. In addition, faster
root cause analysis allowed us to increase the rate of bug
ﬁxing, so that the overall number of bugs exposed and ﬁxed
in the system signiﬁcantly increased.
In the future, we will endeavor to generalize some of the
techniques we have employed to render them applicable be-
yond the scope of the speciﬁc tested system. In particular,
the view-based synchronization described in Section 4.3.1
bears the potential to be useful in any application that in-
cludes a group membership service.
Acknowledgements: We would like to thank Eliezer
Dekel, Gera Goft, Dean Lorenz, Shmuel Ur, and Alan
Wecker for their contribution to the testing solution and
their comments that helped us improve the paper.
References
[1] K. Birman and T. Joseph. Reliable Communication in the
Presence of Failures. ACM TOCS, 5(1):47–76, 1987.
[2] G. Chockler, I. Keidar, and R. Vitenberg. Group communica-
tion speciﬁcations: a comprehensive study. ACM Computing
Surveys, 33(4):427–469, 2001.
[3] E. Farchi, Y. Krasny, and Y. Nir. Automatic Simulation of Net-
work Problems in UDP-Based Java Programs. In IPDPS’04,
page 267, Apr. 2004.
[4] J. Kundu. Integrating event- and state-based approaches to
the debugging of parallel programs. PhD thesis, 1996.
[5] L. Lamport. Time, Clocks and the Ordering of Event in a
Distributed System. CACM, 21(7):558–565, 1978.
[6] S. Shende, J. Cuny, L. Hansen, J. Kundu, S. McLaughry, and
O. Wolf. Event and state-based debugging in TAU: a proto-
type. In SIGMETRICS, pages 21–30. ACM Press, 1996.
6
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:55 UTC from IEEE Xplore.  Restrictions apply.