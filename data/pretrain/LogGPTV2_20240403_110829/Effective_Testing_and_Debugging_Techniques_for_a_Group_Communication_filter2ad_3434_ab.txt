### Offline Verification and Overhead

Had we chosen offline verification, we would have needed to log a significant amount of additional information, thereby inflating the traces and incurring substantial overhead.

### 4.3 Distributed Invariants

The most critical distributed invariants that DCS should maintain during its operation are as follows:

1. **Membership Layer Guarantees**: All view members must agree on the same view members, ID, and leader. Verifying this property helps determine whether the sequence of view changes is legitimate.
2. **Virtual Synchrony, Reliable Delivery, and Self-delivery Guarantees**: Refer to Section 2.1 for detailed explanations.
3. **Eventual Process Join and Removal**: If a new process connects or disconnects from a group, a new view that includes or excludes this member will eventually be established.

To verify these invariants, it is necessary to correlate and analyze events occurring at different processes. Constructing a distributed snapshot during runtime can alter timing and introduce performance overhead. Therefore, the testing system verifies distributed invariants by analyzing logs after the test is completed. This post-mortem log analysis is based on the information recorded by the Tester layer into the log file. Although still complex, the analysis is facilitated by the fact that group communication systems inherently use logical timestamps for certain events.

### Example: Global Event Sequence

Consider an example of a global event sequence from one of our tests. This example will be used throughout the paper to illustrate the elements of our solution.

- **Initial State**: Members A, B, and C are together in view with view ID=100. Each member's log contains an entry describing the view ID, leader, and members.
- **C Crashes**: As a result, A establishes a view with view ID=101, view leader A, and no additional members. Similarly, B establishes a view with view ID=101, view leader B, and no additional members.
- **A and B Reconcile**: A and B establish a view with view ID=102, view leader A, and members A and B.

Figure 3 illustrates all relevant log entries for this scenario. Assuming no additional failures, this scenario represents a counterintuitive situation where A and B establish separate views instead of a single common view. This short period of separate views could indicate an implementation bug, which we term an "unjustified split event." The next section explains how the log analyzer operates in this scenario.

### 4.3.1 Distributed Log Analyzer

After a system run, the log analyzer gathers all logs, extracts relevant log entries, reconstructs a global execution of the system, builds helper data structures, and invokes various post-mortem checkers to verify the distributed invariants.

By extracting only a small amount of relevant information from all log files and checking the maintenance of DCS invariants based on this information, the log analyzer addresses the problem of interpreting large amounts of distributed information. Automatic analysis of the gathered information relieves developers from manually interpreting and analyzing the traces. If the post-mortem checking system detects a violation of the invariants, it provides the developer with sufficient information to facilitate root cause analysis.

#### Correlation of Log Events

The main challenge in reconstructing a consistent global history is correlating multiple log events from different logs. The log analyzer uses two approaches to address this:

1. **Logical Clocks**: These are a standard technique for establishing correspondence and capturing the partial "happened before" order between distributed events. View identifiers, natively supported by group communication systems, act as logical clocks for view-related events. However, group communication systems do not maintain a monotonic increase of view identifiers in the face of process crashes, as explained in Section 2.1. Logical clocks also need to be implemented for other events that may not be traceable by the testing system in a non-intrusive way.
2. **Real Clocks**: These are inherently persistent, and events can be easily tagged with them without introducing extra communication or computation overhead. Most implementation components autonomously log the time of important events. However, capturing the order of events based solely on real clocks is unreliable due to clock skew between different processes.

### 4.3.2 Post-Mortem Checks

Once the view graph is built, various post-mortem checks are executed. The following should hold for every view node:

1. Each process reported as a view member by other processes has a corresponding log record.
2. The virtual synchrony and reliable delivery guarantees mentioned in Section 2.1.
3. Various test-related checks, such as ensuring that there is at most a single outgoing edge from every node in the graph to detect unjustified split events.

If a post-mortem check detects a problem, the developer receives all relevant information from the logs, including a description of the problematic event, view number, names of the members involved, exact time of occurrence at each participating member, and the lines in the log file from which the relevant entries were extracted. This information is then efficiently used for root cause analysis.

### 4.4 Concurrency-Related Debugging Techniques

One of the most important tasks in our tests was deadlock elimination. Both SUN and IBM JDKs have a feature for detecting deadlocks during a JVM core dump, triggered by sending a signal to the JVM process. However, this is extremely costly and cannot be invoked periodically in a production environment. In DCS, we use a watchdog-based deadlock-detection mechanism (WDD) in conjunction with this feature. WDD is more efficient but less precise, as it may raise false suspicions. When WDD suspects a deadlock, it sends a signal to the JVM to record the relevant information into the trace file.

#### Watchdog-Based Deadlock Detection Mechanism

The WDD mechanism uses two timestamps and an independent watchdog thread running in parallel with other system threads:

- **TS1**: Updated by the DCS self-checking mechanism, which periodically obtains the DDLock and updates TS1. A deadlock involving the DDLock would prevent these periodic updates.
- **TS2**: Periodically updated by the watchdog thread.

The watchdog thread checks when both timestamps were last updated. If both timestamps have been recently updated, the thread does nothing. Otherwise, one of two cases could occur:

1. **JVM Freeze**: If the watchdog timestamp (TS2) has not been updated for a considerable period, it indicates a possible JVM freeze, where user threads do not get adequate CPU time.
2. **Deadlock**: If the watchdog timestamp (TS2) has been recently updated but the self-checking timestamp (TS1) has not, it suggests a continuous failure to obtain the lock, indicating a potential deadlock.

This mechanism eliminates the need to recreate deadlocks and logs all necessary information for the developer to solve the problem. The overhead of this mechanism is less than one percent for the most frequently used code paths.

### 5. Conclusions

Using the testing suite significantly improved the testing quality and overall code quality of DCS. From the standpoint of the testing process, this work contributed to test monitoring, log analysis, and root cause analysis:

- **Early Detection of Costly Bugs**: The test monitor exposed bugs such as non-deterministic deadlocks and violations of message delivery guarantees, which would be very costly to analyze and fix in a production environment.
- **Minimizing Test Results Analysis Time**: The distributed log analyzer can determine within seconds whether a long test, generated by our test generator, completed successfully. Without this tool, manual examination of night test results took an average of four hours for two testers.
- **Faster Root Cause Analysis**: Using the testing suite reduced the average time from problem exposure to root cause determination. For example, pinpointing a deadlock problem took about two days without the testing suite but was usually achieved in less than two hours with the testing tool. During the last year, we exposed nine deadlocks, saving about two weeks of root cause analysis.

Consequently, the testing suite significantly enhanced the efficiency of our testing procedure. The automatic analysis of test results enabled us to run more tests and find more bugs, some of which would not have been exposed without the tool. The number of bugs exposed using our testing suite during the last year (about 190) was more than twice the number exposed during external tests (about 85). Faster root cause analysis also increased the rate of bug fixing, leading to a significant increase in the overall number of bugs exposed and fixed in the system.

In the future, we aim to generalize some of the techniques employed to make them applicable beyond the specific tested system. The view-based synchronization described in Section 4.3.1, for instance, has the potential to be useful in any application that includes a group membership service.

### Acknowledgements

We would like to thank Eliezer Dekel, Gera Goft, Dean Lorenz, Shmuel Ur, and Alan Wecker for their contributions to the testing solution and their comments that helped improve the paper.

### References

[1] K. Birman and T. Joseph. Reliable Communication in the Presence of Failures. ACM TOCS, 5(1):47–76, 1987.
[2] G. Chockler, I. Keidar, and R. Vitenberg. Group communication specifications: a comprehensive study. ACM Computing Surveys, 33(4):427–469, 2001.
[3] E. Farchi, Y. Krasny, and Y. Nir. Automatic Simulation of Network Problems in UDP-Based Java Programs. In IPDPS’04, page 267, Apr. 2004.
[4] J. Kundu. Integrating event- and state-based approaches to the debugging of parallel programs. PhD thesis, 1996.
[5] L. Lamport. Time, Clocks and the Ordering of Event in a Distributed System. CACM, 21(7):558–565, 1978.
[6] S. Shende, J. Cuny, L. Hansen, J. Kundu, S. McLaughry, and O. Wolf. Event and state-based debugging in TAU: a prototype. In SIGMETRICS, pages 21–30. ACM Press, 1996.