title:Asleep at the Keyboard? Assessing the Security of GitHub Copilot's
Code Contributions
author:Hammond Pearce and
Baleegh Ahmad and
Benjamin Tan and
Brendan Dolan-Gavitt and
Ramesh Karri
1
7
5
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Asleep at the Keyboard? Assessing the
Security of GitHub Copilot’s Code Contributions
Hammond Pearce
Department of ECE
New York University
Brooklyn, NY, USA
Baleegh Ahmad
Department of ECE
New York University
Brooklyn, NY, USA
Benjamin Tan
Department of ESE
University of Calgary
Calgary, Alberta, CA
PI:EMAIL
PI:EMAIL
PI:EMAIL
Brendan Dolan-Gavitt
Ramesh Karri
Department of CSE
New York University
Brooklyn, NY, USA
PI:EMAIL
Department of ECE
New York University
Brooklyn, NY, USA
PI:EMAIL
Abstract—There is burgeoning interest in designing AI-based
systems to assist humans in designing computing systems,
including tools that automatically generate computer code. The
most notable of these comes in the form of the ﬁrst self-described
‘AI pair programmer’, GitHub Copilot, which is a language
model trained over open-source GitHub code. However, code
often contains bugs—and so, given the vast quantity of unvetted
code that Copilot has processed, it is certain that the language
model will have learned from exploitable, buggy code. This raises
concerns on the security of Copilot’s code contributions. In this
work, we systematically investigate the prevalence and conditions
that can cause GitHub Copilot to recommend insecure code.
To perform this analysis we prompt Copilot to generate code
in scenarios relevant to high-risk cybersecurity weaknesses, e.g.
those from MITRE’s “Top 25” Common Weakness Enumeration
(CWE) list. We explore Copilot’s performance on three distinct
code generation axes—examining how it performs given diversity
of weaknesses, diversity of prompts, and diversity of domains. In
total, we produce 89 different scenarios for Copilot to complete,
producing 1,689 programs. Of these, we found approximately
40 % to be vulnerable.
Index Terms—Cybersecurity, Artiﬁcial Intelligence (AI), code
generation, Common Weakness Enumerations (CWEs)
I. INTRODUCTION
there is considerable interest
improving productivity. The most
With increasing pressure on software developers to produce
in tools and
code quickly,
recent
techniques for
entrant
into this ﬁeld is machine learning (ML)-based
code generation, in which large models originally designed
for natural language processing (NLP) are trained on vast
quantities of code and attempt to provide sensible completions
as programmers write code. In June 2021, GitHub released
Copilot [1], an “AI pair programmer” that generates code in
a variety of languages given some context such as comments,
function names, and surrounding code. Copilot is built on a
large language model that is trained on open-source code [2]
including “public code...with insecure coding patterns”, thus
giving rise to the potential
for “synthesize[d] code that
contains these undesirable patterns” [1].
Although prior research has evaluated the functionality of
there is no
code generated by language models [3], [2],
B. Dolan-Gavitt is supported in part by the National Science Foundation
award #1801495. R. Karri
is supported in part by Ofﬁce of Naval
Research Award # N00014-18-1-2058. R. Karri is supported in part by the
NYU/NYUAD CCS.
systematic examination of the security of ML-generated code.
As GitHub Copilot
is the largest and most capable such
model currently available, it is important to understand: Are
Copilot’s suggestions commonly insecure? What is the
prevalence of insecure generated code? What factors of the
“context” yield generated code that is more or less secure?
We systematically experiment with Copilot to gain insights
into these questions by designing scenarios for Copilot to
complete and by analyzing the produced code for security
weaknesses. As our corpus of well-deﬁned weaknesses, we
check Copilot completions for a subset of MITRE’s Common
Weakness Enumerations (CWEs), from their “2021 CWE
Top 25 Most Dangerous Software Weaknesses” [4] list. This
list is updated yearly to indicate the most dangerous software
weaknesses as measured over the previous two calendar years.
The AI’s documentation recommends that one uses “Copilot
together with testing practices and security tools, as well as
your own judgment”. Our work attempts to characterize the
tendency of Copilot to produce insecure code, giving a gauge
for the amount of scrutiny a human developer might need to
do for security issues.
We study Copilot’s behavior along three dimensions: (1)
diversity of weakness, its propensity for generating code that
is susceptible to weaknesses in the CWE “top 25”, given a
scenario where such a vulnerability is possible; (2) diversity
of prompt, its response to the context for a particular scenario
(SQL injection), and (3) diversity of domain, its response to
the domain, i.e., programming language/paradigm.
For diversity of weakness, we construct three different sce-
narios for each applicable “top 25” CWE and use the CodeQL
software scanning suite [5] along with manual inspection to
assess whether the suggestions returned are vulnerable to that
CWE. Our goal here is to get a broad overview of the types
of vulnerability Copilot is most likely to generate, and how
often users might encounter such insecure suggestions. Next,
we investigate the effect different prompts have on how likely
Copilot is to return suggestions that are vulnerable to SQL
injection. This investigation allows us to better understand
what patterns programmers may wish to avoid when using
Copilot, or ways to help guide it to produce more secure code.
Finally, we study the security of code generated by Copilot
when it is used for a domain that was less frequently seen
© 2022, Hammond Pearce. Under license to IEEE.
DOI 10.1109/SP46214.2022.00057
754
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:58 UTC from IEEE Xplore.  Restrictions apply. 
in its training data. Copilot’s marketing materials claim that
it speaks “all the languages one loves.” To test this claim, we
focus on Copilot’s behavior when tasked with a new domain
added to the MITRE CWEs in 2020—hardware-speciﬁc
CWEs [6]. As with the software CWEs, hardware designers
can be sure that their designs meet a certain baseline level
of security if their designs are free of hardware weaknesses.
We are interested in studying how Copilot performs when
tasked with generating register-transfer level (RTL) code in
the hardware description language Verilog.
Our contributions include the following. We perform
automatic and manual analysis of Copilot’s software and
hardware code completion behavior in response to “prompts”
handcrafted to represent
scenarios and
characterize the impact that patterns in the context can have
on the AI’s code generation and conﬁdence. We discuss
implications for software and hardware designers, especially
security novices, when using AI pair programming tools.
This work is accompanied by the release of our repository of
security-relevant scenarios (see the Appendix).
security-relevant
II. BACKGROUND AND RELATED WORK
A. Code Generation
for whitespace (i.e., a token for two spaces, a token for
three spaces, up to 25 spaces). This allows the tokenizer to
encode source code (which has lots of whitespace) both more
efﬁciently and with more context.
Accompanying the release of Copilot, OpenAI published
a technical report evaluating various aspects of “several early
Codex models, whose descendants power GitHub Copilot” [2].
This work does include a discussion (in Appendix G.3) of
insecure code generated by Codex. However, this investigation
was limited to one type of weakness (insecure crypto
parameters, namely short RSA key sizes and using AES in
ECB mode). The authors note that “a larger study using the
most common insecure code vulnerabilities” is needed, and
we supply such an analysis here.
An important feature that Codex and Copilot inherit from
GPT-3 is that, given a prompt, they generate the most likely
completion for that prompt based on what was seen during
training. In the context of code generation, this means that
the model will not necessarily generate the best code (by
whatever metric you choose—performance, security, etc.) but
rather the one that best matches the code that came before.
As a result, the quality of the generated code can be strongly
inﬂuenced by semantically irrelevant features of the prompt.
We explore the effect of different prompts in Section V-C.
B. Evaluating Code Security
functional
emphasizes
Numerous elements determine the quality of code. Code
correctness,
generation literature
measured by compilation and checking against unit
tests,
or using text similarity metrics to desired responses [2].
Unlike metrics for functional correctness of generated code,