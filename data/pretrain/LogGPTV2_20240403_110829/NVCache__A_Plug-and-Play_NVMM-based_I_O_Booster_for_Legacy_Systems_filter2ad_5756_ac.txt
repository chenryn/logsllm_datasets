replaces the buffered versions of I/O functions (fread,
fwrite, fopen, fclose) with their unbuffered counter-
parts, since NVCACHE itself acts as a buffer in user-space
with its small read cache in volatile memory.
As for the fsync function calls, which force the kernel
to synchronously propagate pending writes from the kernel
page cache to the disk, NVCACHE simply ignores them. These
operations are no longer necessary because NVCACHE already
makes the write synchronously durable.
As presented in §II-C, NVCACHE maintains its own ver-
sions of ﬁle cursors and size information, because the cursor
associated to an opened ﬁle and the size of a ﬁle may be stale
in the kernel. Therefore, NVCACHE intercepts the stat and
seek function families in order to return fresh values.
5
// Non-volatile memory
// Volatile memory
Algorithm 1 — NVCACHE write function.
1 struct nvram {
struct { char path[PATH MAX]; } fds[FD MAX];
struct entry entries[NB ENTRIES];
uint64 t persistent
tail;
2
3
4
5 }* nvram;
7 uint64 t head, volatile tail;
9 void write(int fd, const char* buf, size t n) {
struct open ﬁle* o = open ﬁles[fd];
struct ﬁle* f = o−>ﬁle;
struct page desc* p = get(f−>radix, o−>offset);
uint64 t index = next entry();
struct entry* e = &nvram−>entries[index % NB ENTRIES];
acquire(&p−>atomic lock);
19 memcpy(e−>data, buf, n);
20
21
22
23
e−>fd = fd;
e−>off = o−>off;
pwb range(e, sizeof(*e));
pfence();
// Write cache
// Send the uncommited entry to NVMM
// Ensure commit is executed after
25
26
27
e−>commit = 1;
pwb range(e, CACHE LINE SIZE);
psync();
// Send the commit to NVMM
// Ensure durable linearizability
atomic fetch add(&p−>dirty counter, 1);
if(p−>content)
// Read cache
// Update page if present in the read cache
memcpy(p−>content−>data + o−>off %
release(&p−>atomic lock);
33 }
35 int next entry() {
int index = atomic load(&head);
36
37 while(((index + 1) %
38
39
40
index = atomic load(&head);
return index;
41 }
!atomic compare and swap(&head, index, index + 1))
// Commit ﬂag at index is 0 (see cleanup thread)
10
11
12
14
15
17
29
30
31
32
At low level, in order to read and write from a cursor
maintained by NVCACHE and not the one maintained by
the kernel, the cleanup thread uses the pwrite function to
propagate an entry to the kernel and, upon cache misses,
NVCACHE uses pread to load a page in the read cache.
NVCACHE does not support asynchronous writes, but they
could be implemented. Memory-mapped ﬁles, however, are not
supported in NVCACHE and their implementation remains an
open problem, as loads and stores are not interceptable at the
libc level.
In the following, we detail how we have implemented the
open, write, cleanup and recovery functions because
they directly deal with NVMM. The read function closely
implements the design presented in §II-C and §II-D.
Open. The open function adds a ﬁle to the cache. First, in
NVMM, NVCACHE maintains a table that associates the paths
of the opened ﬁles to their ﬁle descriptors. This table is only
used during recovery: after a crash, the ﬁle descriptors in the
log entries are meaningless and the recovery engine therefore
needs the information in the table to retrieve the paths of the
ﬁles associated to these ﬁle descriptors.
Then, in volatile memory, NVCACHE keeps track of opened
ﬁles with two tables in order to handle independent cursors
when an application opens the same ﬁle twice. The ﬁrst one,
called the ﬁle table, associates a (device, inode number) pair
to a ﬁle structure, which contains the size of the ﬁle and its
radix tree. The second one, called the opened table, associates
a ﬁle descriptor to an opened-ﬁle structure, which contains the
cursor and a pointer to the ﬁle structure.
In open, NVCACHE starts by retrieving the device and the
inode number associated to the ﬁle path with the stat system
call. Then, if the ﬁle belongs to a block device and if the
(device, inode number) pair is not yet present in the ﬁle table,
NVCACHE creates the ﬁle structure. Finally, NVCACHE uses
the open system call to create a ﬁle descriptor and creates
accordingly an opened-ﬁle structure in the opened table.
NVCACHE bypasses its read cache when a ﬁle is only
opened in read-only mode. For that purpose, NVCACHE only
creates a radix tree in the ﬁle structure when the ﬁle is opened
in write mode for the ﬁrst time and, for a ﬁle that does not
have a radix tree, NVCACHE bypasses its read cache.
Write. Algorithm 1 shows a simpliﬁed and unoptimized
version of NVCACHE’s write function when the write ﬁts
in one page and one entry. Our code uses three NVMM-
speciﬁc instructions: pwb(addr) (e.g., clwb on a Pentium)
ensures that the cache line that contains addr is added in
the ﬂush queue of the cache; pfence (e.g., sfence on a
Pentium) acts both as a store barrier and ensures that the
pwbs that precede are executed before the barrier; and psync
(e.g. also sfence on a Pentium) acts as a pfence and
furthermore ensures that the cache line is actually drained to
the NVMM. With these instructions, the write(a, v1),
pwb(a), pfence, write(b, v2) sequence ensures that
the write to a is propagated to NVMM before the write to b
because the cache line of a is ﬂushed before the write to b is
enqueued in the ﬂush queue of the cache.
In our code, after having retrieved (or lazily created) the
page descriptor (line 12), the write function ﬁnds a new free
entry in the log (line 14 and 35–41). In details, next_entry
ﬁrst waits if the log is full (line 37) and then advances the head
while taking care of concurrent accesses from another threads
(line 38). At this step, the commit ﬂag of the entry returned
by next_entry is necessarily equal to 0 (see the cleanup
thread below).
Then, as soon as the write function acquires the lock of
the page descriptor (line 17), it adds the write to the log
(lines 19 to 27). More precisely, the function ﬁlls the entry
without committing it (lines 19 to 21), sends the uncommitted
entry to the NVMM by ﬂushing the cache lines of the entry
(line 22) and executes a pfence in order to ensure that the
entry is ﬂushed before the commit (line 23). The function then
commits the entry (line 25), sends the cache line that holds the
commit ﬂag to the NVMM (line 26) and executes a psync
to ensure durable linearizability (line 27, see the text below
for the explanation). At this level, the atomic_lock is only
taken for atomicity purposes between the writer thread and
6
the cleanup thread, preventing the cleanup thread to modify a
page on the SSD while a cache miss procedure reads it.
Finally, the write function manages the read cache (lines 29
to 31) before releasing the lock at line 32. Speciﬁcally, it
increments the dirty counter (line 29) because the log contains
a new entry that modiﬁes the page. The increment is done
with an atomic instruction in order to prevent a conﬂict with
the cleanup thread that may be concurrently decrementing the
counter (see §II-D).4 If the page is in the loaded state (i.e., the
page descriptor is associated to a page content, line 30), the
function updates the page content in the read cache (line 31).
Durable linearizability. Our algorithms ensure durable lin-
earizability [29], which essentially means that if a read sees
a write, then the write is committed. For example, this is
not the case of Linux when it uses the page cache: indeed,
a thread can see a write stored in the page cache but not
yet propagated to disk. NVCACHE always ensures durable
linearizability because of the psync at line 27. This operation
ensures that the commit at line 25 is written to NVMM before
the lock release at line 32, which is itself executed before the
lock acquire of a reader able to see the write.
Cleanup thread and batching. The cleanup thread propa-
gates the writes from the NVMM log to the disk. At high level,
it consumes the entries one by one, starting at the persistent
tail index. The cleanup thread begins by synchronizing with
the application through the commit ﬂag: if the entry at the
persistent tail is not yet committed, the cleanup thread waits.
When the entry at the persistent tail is committed, the cleanup
thread consumes the entry while owning the cleanup locks
associated to the descriptors of the pages modiﬁed by the entry.
In more detail, the cleanup thread proceeds in three steps when
it consumes an entry. During the ﬁrst step, the cleanup thread
propagates the entry to the mass storage by using pwrite to
send the write to the kernel page cache and by using fsync to
synchronously propagate the writes from the kernel page cache
to the mass storage. During the second step, the cleanup thread
updates both the commit ﬂag of the consumed entry and the
persistent tail index, and uses pwb/pfence to ensure that the
third step can only start after the second step. During the third
step, the cleanup thread marks the entry as free for the writers
by using the volatile tail index. Because of the use of the two
tail indexes, when a writer sees that an entry is free in volatile
memory (volatile tail index), we have the guarantee that the
entry is also marked as free in NVMM (persistent tail index
and commit ﬂag of the entry).
The implementation described above is inefﬁcient because
a call to fsync is especially costly. The throughput of a
random 4 kB write on an SSD is at least 13× faster without
fsync [35]. To mitigate the negative impact of a slow cleanup
thread that continuously calls fsync, which would otherwise
4Because a writer does not acquire the cleanup lock, the cleanup thread may
decrement the dirty counter between the commit at line 27 and the atomic
add at line 29, making the counter negative. This temporary negative counter
cannot lead to an incorrect behavior because a reader has to take both the
atomic and cleanup locks to execute the dirty miss procedure, ensuring that
the dirty miss procedure cannot observe an unstable negative counter.
block all the writes of the application when the log is full,
the cleanup thread batches the writes. Batching allows us
to reduce the frequency of calls to fsync, updating the
tail index only upon success. The advantages of batching
are twofold. First, batching decreases the number of calls to
fsync, boosting the performance of the cleanup thread and
thus decreasing the probability of having a full log. Then,
batching allows NVCACHE to leverage kernel optimizations
by combining writes or optimizing the sequences of writes
for hard drives or SSD. §IV-C presents in detail how batching
improves performance.
Recovery procedure. When NVCACHE starts, it executes a
recovery procedure. It ﬁrst re-opens the ﬁles by using the table
that associates ﬁle paths to ﬁle descriptors stored in NVMM.
Then, it propagates all the committed entries of the log by
starting at the tail index, invokes the sync system call to
ensure that the entries are written to disk, close the ﬁles and
empties the log.
Multi-application. The NVMM write log of NVCACHE is
either a DAX device, e.g., an entire NVMM module, or a DAX
ﬁle, i.e., a ﬁle in any DAX-capable ﬁlesystem. Therefore, in a
multi-application context, two instances of NVcache can run
simultaneously on the same machine, either with one NVMM
module each or sharing the same module split into two DAX
ﬁles.
A. Hardware and software settings
IV. EVALUATION
We evaluate NVCACHE on a Supermicro dual socket ma-
chine with two NUMA domains. Each NUMA domain con-
tains an Intel Xeon Gold 5215 CPU at 2.50 GHz with 10 cores
(20 hardware threads), 6×32 GiB of DDR4 and 2×128 GiB of
Optane NVDIMM. In total, the machine contains 20 cores (40
hardware threads), 384 GiB of DDR4 and 512 GiB of Optane
NVDIMM. The machine is equipped with two SATA Intel
SSD DC S4600 with 480 GB of disk space each. One of
them contains the whole system, while the other is dedicated
for some of our experiments. The main SSD is formatted
with an Ext4 ﬁle system. The secondary one is mounted with
lvm2, linked with a DM-WriteCache stored in one of our
Optane NVDIMM. This virtual lvm2 device is also formatted
with Ext4. We deploy Ubuntu 20.04 with Linux version 5.1.0
(NOVA [57] repository version) and musl v1.1.24, revision
9b2921be.
We evaluate NVCACHE with representative benchmarks
that heavily rely on persistent storage. RocksDB is a persistent
key-value store based on a log-structured merge tree widely
used in web applications and production systems, e.g., by
Facebook, Yahoo! and LinkedIn [15]. We evaluate RocksDB
v6.8 with the db_bench workload shipped with LevelDB,
an ancestor of RocksDB, which stresses different parts of
the database. SQLite [3] is a self-contained SQL database
widely used in embedded systems, smartphones and web
browsers [31], [36]. We evaluate SQLite v3.25 with a port
of the db_bench for SQLite. We also use FIO [12] version
3.20, a micro-benchmark designed to control the read and
7
TABLE IV: Evaluated ﬁle systems.
Storage space
Synchronous durability
Durable linearizability
Name
NVCACHE+SSD
DM-WriteCache
Ext4-DAX
NOVA5
SSD
tmpfs
NVCACHE+NOVA
Write cache
NVCACHE
kernel page cache
kernel page cache
none
kernel page cache
kernel page cache
NVCACHE