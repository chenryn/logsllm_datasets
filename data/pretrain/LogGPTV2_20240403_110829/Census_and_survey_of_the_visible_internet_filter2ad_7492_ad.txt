e
p
m
a
s
n
w
o
d
-
)
t
s
o
h
(
A
 1
 0.8
 0.6
 0.4
 0.2
 0
22min
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1
A (host)-at finest time scale (11 min)
l
d
e
p
m
a
s
n
w
o
d
-
)
t
s
o
h
(
A
 1
 0.8
 0.6
 0.4
 0.2
 0
87min
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1
A (host)-at finest time scale (11 min)
(a) 2x downsampling
(b) 8x downsampling
Figure 5: Eﬀect of downsampling ﬁne timescale A(addr ).
Data from IT survey
15w .
keeping the observation time constant, should give us more
samples and hence a more detailed picture. However, probes
that are much more frequent than changes to the underlying
phenomena being measured provide little additional beneﬁt,
and limited network bandwidth at the source and target
argue for moderating the probe rate. Unfortunately, we do
not necessarily know the timescale of Internet address usage.
In this section we therefore evaluate the eﬀect of changing
the measurement timescale on our A(addr ) metric.
To examine what eﬀect the sampling interval has on the
ﬁdelity of our metrics, we simulate diﬀerent probe rates by
decimating IT survey
15w . We treat the complete dataset with
11-minute probing as ground truth, then throw away every
other sample to halve the eﬀective sampling rate. Applying
this process repeatedly gives exponentially coarser sampling
intervals, allowing us to simulate the eﬀects of less frequent
measurements on our estimates.
Figure 5 shows the results of two levels of downsampling
for every address that responds in our ﬁne timescale survey.
In the ﬁgure, each address is shown as a dot with coordinates
representing its accessibility at the ﬁnest time scale (x-axis)
and also at a coarser timescale (the y-axis).
If a coarser
sample provided exactly the same information as ﬁner sam-
ples we would see a straight line, while a larger spread in-
dicates error caused by coarser sampling. We observe that
this spread grows as sample interval grows. In addition, as
sampling rates decrease, data collects into bands, because n
probes can only distinguish A-values with precision 1/n.
While these graphs provide evidence that sparser sam-
pling increases the level of error, they do not directly quan-
tify that relationship. To measure this value, we group ad-
dresses into bins based on their A(addr ) value at the ﬁnest
timescale, then compute the standard deviation of A(addr )
values in each bin as we reduce the number of samples per
address. This approach quantiﬁes the divergence from our
ground-truth ﬁnest timescale values as we sample at coarser
resolutions. Figure 6 shows these standard deviations for a
range of sample timescales, plotted by points. As expected,
coarser sampling corresponds to wider variation in the mea-
surement compared to the true value; this graph quantiﬁes
that relationship. We see that the standard deviation is the
greatest for addresses with middle values of A (local maxi-
mum around A = 0.6) and signiﬁcantly less at the extreme
values of A = 0 and A = 1.
To place these values into context, assume for a moment
that address occupancy is strictly probabilistic, and that an
address is present with probability p. Thus E(A(addr )) = p,
and each measurement can be considered a random vari-
i
n
o
i
t
a
v
e
D
d
r
a
d
n
a
t
S
)
t
s
o
h
(
A
 0.11
 0.1
 0.09
 0.08
 0.07
 0.06
 0.05
 0.04
 0.03
 0.02
 0.01
 0
22 min
43 min
87 min
173 min
147 min
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
A(host) - fine time scale
Figure 6: Standard deviation (from IT survey
of ground truth A(addr ) metric (from IT survey
with theoretical curves
A(1 − A)/n.
15w ) as a function
15w ) overlayed
p
p
able X taking values one or zero when the host responds
(with probability p) or is non-responsive (with probability
1 − p). With n samples, we expect np positive results, and
ˆA(addr ) will follow a binomial distribution with standard
np(1 − p). On these assumptions, we can place
deviation
error bounds on the measurement: our estimates should be
ˆp(1 − ˆp)/n for a 90% conﬁdence in-
within ˆA(addr )±1.645
terval; we show these estimates on Figure 6 as lines. We can
see that the measured variance is nearly always below the
theoretical prediction. This reduction is potentially caused
by correlation in availability between hosts in same block.
The prediction becomes more and more accurate as we in-
crease the time scale and samples become more “random”,
approaching the binomial distribution.
p
These results assume our measurements are unbiased. This
assumption is not strictly true, but Section 3 suggests that
bias is generally small.
4.2 Sampling in Space
We can survey an increasing number of addresses, but only
at a diminishing rate. In the extreme case of our census, we
probe every address only once every several months. Data
so sparse makes interpretation of uptime highly suspect, be-
cause measurements are taken much less frequently than the
known arrival and departure rates of hosts such as mobile
computers. Much more frequent sampling is possible when
a smaller fraction of the Internet is considered, however this
step introduces sampling error.
In this section we review
the statistics of population surveys to understand how this
aﬀects our results. The formulae below are from Hedayat
and Sinha [17]; we refer interested readers there.
In ﬁnding the proportion of a population that meets some
criteria, such as the mean A(addr ) values for the Internet, we
draw on two prior results of simple random sampling. First,
a sample of size n approximates the true A with variance
V ( ˆA) (cid:2) A(1 − A)/n (provided the total population is large,
as it is in the case of the IPv4 address space). Second, we
can estimate the margin of error d with conﬁdence 1 − α/2
for a given measurement as:
p
d = zα/2
A(1 − A)/n
(1)
when the population is large, where zα/2 is a constant that
selects conﬁdence level (1.65 for 95% conﬁdence).
Second, when estimating a non-binary parameter of the
population, such as mean A(block ) value for the Internet
with a sample of size n, the variance of the estimated mean
is V ( ¯A(block )) = S2
¯A(block ) is the true
population variance.
/n, where S2
¯A(block )
These results from population sampling inform our Inter-
net measurements: by controlling the sample size we can
control the variance and margin of error of our estimate.
We use this theoretical result in Section 5.2 to bound sam-
pling error at less than 0.4% for response estimates of our
surveys.
5. ESTIMATING THE SIZE OF THE IN-
TERNET
Having established our methodology, we now use it to
estimate the size of the Internet. While this question seems
simple to pose, it is more diﬃcult to make precise. Our
goal is to estimate the number of hosts that can access the
Internet, yet doing so requires careful control of sources of
error.
Figure 1 divides the Internet address space into several
categories, and we have quantiﬁed the eﬀects of protocol
choice (Section 3.2) and invisible hosts (Section 3.2.1), our
largest sources of undercounting. Section 3.4 also accounts
for a overcounting due to routers.
Having quantiﬁed most sources of error, we can therefore
estimate the size of the Internet through two sub-problems:
estimating the number of hosts that use dynamic addresses
and the number that use static addresses. We must un-
derstand dynamic address usage because dynamic addresses
represent a potential source of both over- or under-counting.
Dynamic addresses may be reused by multiple hosts over
time, and they may go unused when an intermittently con-
nected host, such as a laptop or dial-up computer, is oﬄine.
Unfortunately, we cannot yet quantify how many addresses
are allocated dynamically to multiple hosts. The topic has
only recently begun to be explored [50, 25]; to this existing
study we add an analysis of duration of address occupancy
(Section 5.1). Here we focus on evaluating the size of the
static, visible Internet (Section 5.2).
While we cannot quantify how many computers are ever
on the Internet, we can deﬁne an Internet address snap-
shot as whatever computers are on-line at any instant. Our
census captures this snapshot, modulo packet loss and non-
instantaneous measurement time. We can then project trends
in Internet address use by evaluating how snapshots change
over time (Section 5.3), at least to the extent the snapshot
population tracks the entire Internet host population.
5.1 Duration of Address Occupancy
We next use our address surveys to estimate how many
Internet addresses are used dynamically. There are many
reasons to expect that most hosts on the Internet are dy-
namically addressed, since many end-user computers use dy-
namic addresses, either because they are mobile and change
addresses based on location, or because ISPs encourage dy-
namic addresses (often to discourage home servers, or pro-
vide static addressing as a value- and fee-added service). In
addition, hosts that are regularly turned oﬀ show the same
pattern of intermittent address occupation.
1 hr
8 hr 1 day 2 day
U(block) - absolute
 100
 80
 60
 40
 20
)
%
(
n
o
i
t
u
b
i
r
t
s
d
i
e
v
i
t
l
a
u
m
u
c
5 day
%
-
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
host
/24 block
 0.6
 0.8
 1
l
a
u
m
u
C
 100
 80
 60
 40
 20
 0
 0
host
/24 net
/26 net
/28 net
/29 net
/30 net
 0.2
 0.4
 0.6
 0.8
 1
A
 0
 0
 0.2
 0.4
U(block) - normalized
Figure 7: Duration of address occupancy: CDF of U (addr )
and U (block ) from 1-repaired Survey IT survey
15w .
Figure 8: CDF of A(addr ) and A(block ) from from IT survey
15w .
Figure 7 shows distribution of address and block uptimes
(with 1-repair Section 3.5) from IT survey
15w . This data shows
that vast majority of addresses are not particularly stable,
and are occupied only for a fraction of the observation time.
We see that 50% of addresses are occupied for 81 minutes or
less. A small fraction of addresses, however, are quite stable,
with about 3% up almost all of our week-long survey, and
another 8% showing only a few (1 to 3) brief outages. Our
values are signiﬁcantly less than a median occupancy around
a day as previously reported by Xie et al. [50]; both studies
have diﬀerent kinds of selection bias and a detailed study
of these diﬀerences is future work. On the other hand, our
results are very close to the the median occupancy of 75
minutes per address reported at Georgia Tech. Since our
survey is a sample of 1% of the Internet, it generalizes their
results to the general Internet.
5.2 Estimating the Size of the Stable Internet
and Servers
We next turn to estimating the size of the static Inter-
net. Since we can only detect address usage or absence,
we approximate the static Internet with the stable Inter-
net. This approach underestimates the static Internet, since
some hosts always use the same addresses, but do so inter-
mittently.
We ﬁrst must deﬁne stability. Figure 8 shows the cu-
mulative density function A for addresses and diﬀerent size
blocks, computed over survey IT survey