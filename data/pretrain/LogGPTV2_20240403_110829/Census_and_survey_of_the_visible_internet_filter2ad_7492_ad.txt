### Effect of Downsampling on Fine Timescale A(addr)

**Figure 5: Effect of Downsampling on Fine Timescale A(addr)**

- **(a) 2x Downsampling**
  - Time Scale: 22 minutes
  - X-axis: Accessibility at the finest time scale (11 minutes)
  - Y-axis: Accessibility at the coarser time scale (22 minutes)

- **(b) 8x Downsampling**
  - Time Scale: 87 minutes
  - X-axis: Accessibility at the finest time scale (11 minutes)
  - Y-axis: Accessibility at the coarser time scale (87 minutes)

In this section, we evaluate the effect of changing the measurement timescale on our A(addr) metric. To examine the impact of different sampling intervals on the fidelity of our metrics, we simulate various probe rates by decimating the IT survey data from a 15-week period (IT survey 15w). We treat the complete dataset with 11-minute probing as the ground truth and then reduce the effective sampling rate by discarding every other sample. Repeating this process results in exponentially coarser sampling intervals, allowing us to simulate the effects of less frequent measurements.

**Results:**

- **Downsampling Impact:**
  - Each address is represented as a dot in Figure 5, with coordinates indicating its accessibility at the finest time scale (x-axis) and a coarser timescale (y-axis).
  - If the coarser sample provided exactly the same information as the finer samples, we would see a straight line. A larger spread indicates error due to coarser sampling.
  - As the sampling interval increases, the spread grows, and data points tend to cluster into bands, reflecting the reduced precision in distinguishing A-values.

- **Quantifying Error:**
  - To measure the error, we group addresses into bins based on their A(addr) values at the finest timescale and compute the standard deviation of A(addr) values in each bin as the number of samples per address decreases.
  - Figure 6 shows these standard deviations for various sample timescales, plotted as points.
  - Coarser sampling corresponds to wider variation in the measurements compared to the true values. The standard deviation is highest for addresses with middle values of A (local maximum around A = 0.6) and significantly lower at the extreme values of A = 0 and A = 1.

- **Theoretical Context:**
  - Assuming address occupancy is strictly probabilistic, with an address present with probability \( p \), the expected value \( E(A(addr)) = p \).
  - Each measurement can be considered a random variable \( X \) taking values one or zero when the host responds (with probability \( p \)) or is non-responsive (with probability \( 1 - p \)).
  - With \( n \) samples, we expect \( np \) positive results, and \( \hat{A}(addr) \) will follow a binomial distribution with standard deviation \( \sqrt{np(1 - p)} \).
  - On these assumptions, the estimates should be within \( \hat{A}(addr) \pm 1.645 \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} \) for a 90% confidence interval.
  - The measured variance is generally below the theoretical prediction, possibly due to correlation in availability between hosts in the same block.

### Sampling in Space

We can survey an increasing number of addresses, but only at a diminishing rate. In the extreme case of our census, we probe every address only once every several months. Such sparse data makes it difficult to interpret uptime, as measurements are taken much less frequently than the known arrival and departure rates of mobile computers. More frequent sampling is possible when considering a smaller fraction of the Internet, but this introduces sampling error.

**Statistical Analysis:**

- **Population Proportion:**
  - For finding the proportion of a population that meets certain criteria, such as the mean A(addr) values for the Internet, we use simple random sampling.
  - A sample of size \( n \) approximates the true A with variance \( V(\hat{A}) \approx \frac{A(1 - A)}{n} \).
  - The margin of error \( d \) with confidence \( 1 - \alpha/2 \) for a given measurement is:
    \[
    d = z_{\alpha/2} \sqrt{\frac{A(1 - A)}{n}}
    \]
    where \( z_{\alpha/2} \) is a constant that selects the confidence level (e.g., 1.65 for 95% confidence).

- **Non-Binary Parameter Estimation:**
  - When estimating a non-binary parameter, such as the mean A(block) value for the Internet, the variance of the estimated mean is \( V(\bar{A}(block)) = \frac{S^2_{\bar{A}(block)}}{n} \), where \( S^2_{\bar{A}(block)} \) is the true population variance.

These results inform our Internet measurements, allowing us to control the variance and margin of error of our estimates by controlling the sample size.

### Estimating the Size of the Internet

Having established our methodology, we now use it to estimate the size of the Internet. Our goal is to estimate the number of hosts that can access the Internet, which requires careful control of sources of error.

**Categories of Internet Address Space:**

- Figure 1 divides the Internet address space into several categories, and we have quantified the effects of protocol choice and invisible hosts, which are major sources of undercounting.
- Section 3.4 accounts for overcounting due to routers.

**Sub-Problems:**

- **Dynamic Addresses:**
  - Dynamic addresses represent a potential source of both over- and under-counting. They may be reused by multiple hosts over time and go unused when intermittently connected hosts are offline.
  - We cannot yet quantify how many addresses are allocated dynamically to multiple hosts, but we add an analysis of the duration of address occupancy (Section 5.1).

- **Static Addresses:**
  - We focus on evaluating the size of the static, visible Internet (Section 5.2).

**Internet Address Snapshot:**

- We define an Internet address snapshot as the set of computers online at any instant. Our census captures this snapshot, modulo packet loss and non-instantaneous measurement time.
- We project trends in Internet address use by evaluating how snapshots change over time (Section 5.3).

### Duration of Address Occupancy

**Figure 7: Duration of Address Occupancy**

- **CDF of U(addr) and U(block):**
  - Data from IT survey 15w shows that the vast majority of addresses are not particularly stable, with 50% occupied for 81 minutes or less.
  - A small fraction of addresses are quite stable, with about 3% up almost all of our week-long survey, and another 8% showing only a few brief outages.
  - Our results are close to the median occupancy of 75 minutes per address reported at Georgia Tech, suggesting that our survey generalizes their results to the general Internet.

### Estimating the Size of the Stable Internet and Servers

**Stability Definition:**

- **Figure 8: CDF of A(addr) and A(block):**
  - We approximate the static Internet with the stable Internet, underestimating the static Internet since some hosts always use the same addresses but do so intermittently.
  - We define stability and use the cumulative density function A for addresses and different size blocks, computed over the IT survey 15w.

By defining and analyzing stability, we can better estimate the size of the stable Internet and servers, providing a more accurate picture of the Internet's structure and dynamics.