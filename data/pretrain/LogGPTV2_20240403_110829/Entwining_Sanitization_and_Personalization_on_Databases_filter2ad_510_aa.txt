title:Entwining Sanitization and Personalization on Databases
author:S&apos;ebastien Gambs and
Julien Lolive and
Jean-Marc Robert
Entwining Sanitization and Personalization on Databases∗
Sébastien Gambs
Université du Québec à Montréal
Montréal, Canada
PI:EMAIL
Julien Lolive
IRISA, Université de Rennes 1
Rennes, France
PI:EMAIL
Jean-Marc Robert
École de Technologie Supérieure
Montréal, Canada
PI:EMAIL
ABSTRACT
In the last decade, a lot of research has been done to prevent the
illegal distribution of digital content, such as musical works and
movies. However, only few works have tackled this problem for
databases, and even less for databases containing personal and
sensitive information (e.g, a medical database). In this work, we
address this latter issue by proposing SaPData (for Sanitization and
Personalization of Databases), an approach in which the owner of
a database personalizes it before distributing it to ensure that a
malicious buyer can be traced back in case of an illegal redistribu-
tion. Our novel solution entwines the personalization step with a
sanitization mechanism to prevent the leak of personal informa-
tion and limit the privacy risks. Thus, our objective is to release a
sanitized and personalized database, both to protect the privacy of
the concerned individuals and to prevent the illegal redistribution,
even from a collusion of malicious buyers.
CCS CONCEPTS
• Theory of computation → Theory of database privacy and
security; • Security and privacy → Digital rights management;
KEYWORDS
Sanitization; Fingerprinting; Traitor-tracing
ACM Reference Format:
Sébastien Gambs, Julien Lolive, and Jean-Marc Robert. 2018. Entwining
Sanitization and Personalization on Databases. In ASIA CCS ’18: 2018 ACM
Asia Conference on Computer and Communications Security, June 4–8, 2018,
Incheon, Republic of Korea. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3196494.3196536
1 INTRODUCTION
Massively sharing digital content such as musical works, movies
or pictures has become very easy nowadays. While sharing un-
restricted content between third parties does not represent any
particular challenge, sharing private or proprietary content requires
special attention.
∗This work has been done while Julien Lolive was a postdoctoral researcher at ETS.
This research was supported by ETS as well as an NSERC Discovery Grant for Jean-
Marc Robert. Sébastien Gambs is supported by an NSERC Discovery Grant and a
Discovery Accelerator Supplement Grant.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5576-6/18/06...$15.00
https://doi.org/10.1145/3196494.3196536
A typical example to illustrate the difficulties encountered with
private content is the distribution of a medical database, which can
only be done after a proper sanitization of the data. In particular,
this process should remove all the persistent identifiers such as
names, or social security, employee or patient numbers, but also
modify attributes that could potentially act as quasi-identifiers such
as age, gender and ZIP code, as illustrated by Sweeney [25].
Sharing proprietary content is usually forbidden without the
consent of its owner. If a buyer of such a content releases it without
the consent of its owner, he may transgress copyright laws, pri-
vacy legislations or contractual agreements. In this work, the term
proprietary content includes all these forms of protected content.
To deter the illegal redistribution of proprietary content, it is
usually personalized by its owner before its first release. Numerous
watermarking techniques have been proposed to incorporate per-
sonalized fingerprints into digital content. These techniques secretly
embed fingerprints that are hard to locate and remove. Hence, a ma-
licious buyer should expect to be identified, during a traitor-tracing
phase, and eventually prosecuted upon any illegal redistribution of
proprietary content (buyers are implicitly assumed to be registered
under their true identities).
The novelty of this paper is to address both aspects of the distri-
bution (i.e., the privacy and the utility of the views as well as the
reliable traitor-tracing property) of proprietary databases composed
of personal information in a coherent and integrated manner. More
specifically, we focus on the following problem:
Problem. Protect the distribution of proprietary relational data-
bases containing personal information against collusions of malicious
buyers of bounded sizes.
While several propositions have been presented in the past for
addressing either one of these aspects as highlighted hereafter, no
satisfactory solution has been proposed that solves both of them.
More precisely, a solution to the above problem should (1) guarantee
the privacy of the personal information contained in the distributed
views of the database, (2) ensure the utility of the distributed views
of the database for its buyers (e.g., for statistical analysis) and (3)
protect the database owner against any illegal redistribution.
A naïve solution to this problem would generate for each buyer
a personalized view, which is then properly sanitized. Unfortu-
nately, this would be too costly for large databases. In particular,
the owner would have to kept all these personalized views to be
able to trace back any potential malicious buyer. Moreover, it may
leak too much information on the database to a collusion of ma-
licious buyers. Indeed, identifying the tuples that are common to
all buyers or correcting tuples that have been slightly altered may
become possible in such a case. To circumvent these issues, we
propose to sanitize the database during the first phase while still
ensuring its utility, and then to generate all the personalized copies
Session 6: Privacy 1ASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea207by embedding the appropriate fingerprints. In this case, only the
sanitized view of the database and the fingerprints would have to
be stored by the database owner to be able to trace back potential
malicious buyers. One challenging issue is to be able to entwine
these two processes (i.e., the sanitization and the watermarking)
without one of them interfering with the other.
The sanitization of a database is usually performed with respect
to a privacy model such as k-anonymity [26], l-diversity [16] or
differential privacy [8]. The sanitization has to balance between the
desired level of privacy and the loss of utility. At one end of the
spectrum, removing too much information will lead to a useless
database for any kind of non-trivial statistical queries while at the
other end keeping too much information will lead to a poor privacy
protection. In this paper, we do not consider the interactive setting
in which the database remains in the control of the owner who
responds only to a limited number of queries by adding noise to the
output of these queries. Rather we focus on the challenging non-
interactive setting in which the database is distributed to the buyers.
In this context, the buyers are only limited by their computational
resources and some of them may be tempted to collude by putting
together their copies if they can obtain a potential gain.
Dinur and Nissim [6] have formalized the privacy for statistical
databases by introducing the concept of non-privacy – a contra-
positive approach. In such databases, a classical query consists in
counting the number of tuples respecting a given predicate. In this
context, the authors have shown that any polynomial-time bounded
√
adversary can reconstruct most of the database tuples if it has not
been properly perturbed1. If less than
T perturbations have been
applied on the data (T corresponds to the number of tuples in the
database), the adversary will be able to retrieve in polynomial time
all but a fraction of the tuples with a probability arbitrary close to
one. In the interactive setting, these perturbations would be sim-
ply random values added to the query count before it is returned.
Unfortunately, Dinur and Nissim did not address the problem of
determining whether the perturbed data would still be useful or
not in practice. In general, utility is either application-dependent
or define with respect to global statistical properties of the data.
Several watermarking techniques have been proposed to em-
bed fingerprints in databases [1, 13, 22]. Such a fingerprint may
identify the owner of the database (proof of ownership) or the legiti-
mate buyer of a given copy of the database (proof of possession). To
prevent the removal of these fingerprints, any party receiving his
personalized copy should not be able to retrieve the corresponding
marking bits. The challenge in such a case is to prevent a group
of colluders to erase or randomize these marking bits to forge a
potentially untraceable copy. Hence, anti-collusion codes must be
used to resist to groups of colluders, with the objective of being able
to impute the counterfeit copy to at least one of these colluders.
Finally, it is crucial to understand that sanitization and person-
alization cannot be composed independently. If a fingerprint is
embedded before sanitization, it may be partially deleted by the
sanitization process. In the opposite case, if a fingerprint is embed-
ded after the sanitization, it may be simpler to locate, or may impact
strongly the utility of the resulting database if some attributes of
tuples are significantly modified. Thus, we believe that the best
1This assumes that any query can be answered in O (1) time in the interactive case.
solution is to entwine these two approaches into one integrated
mechanism relying on a coherent approach to encode the sanitized
views of a database and fingerprints. To the best of our knowledge,
only three related solutions following a similar approach exist in
the literature, which will be reviewed in the next section.
Summary of contributions. We propose a novel approach to
entwine the personalization step with the sanitization mechanism
for relational databases. First, we rely on the powerful and robust
(α, β )-sanitization technique proposed by Rastogi, Hong and Su-
ciu [18, 19] to ensure the privacy and the utility of sanitized rela-
tional databases. The authors have also shown that their solution
satisfies the broader and more standard notion of ϵ−differential
privacy [8]. In a nutshell, this approach keeps each tuple of the
database with a probability α greater than 1
2 and adds false tuples
with a small probability β very close to zero. False tuples are sim-
ply tuples existing in the potential domain of the database but not
in the original database. We also show how to use anti-collusion
codes based on the efficient Tardos fingerprinting codes [27] to
deal with large groups of colluders. Both processes can naturally
be merged together. In a nutshell, the binary fingerprints can be
encoded by the presence or the absence of some predetermined
false tuples in the distributed views of the database. Since their
lengths are relatively short, this would not impact the utility of the
resulting views. To the best of our knowledge, this is the first time
that such an approach has been used for personalization. Usually
watermarking techniques modify specific attributes of the tuples
but do not remove or insert tuples.
Outline. First in Section 2, we review the related work as well as
the background notions necessary to understand our work. Then,
we define the privacy, utility and security properties and the ad-
versary model in Section 3. Afterwards in Section 4, we detail our
proposition to entwine sanitization and personalization for rela-
tional databases before presenting the security and privacy analysis
of our solution in Section 5. Finally, we conclude in Section 6.
2 BACKGROUND
In this section, we review the main primitives that we use, namely
(1) the database sanitization techniques to protect the privacy of
the personal information in databases and (2) the watermarking
techniques embedding fingerprints to personalize digital content.
Sanitization techniques for databases. Most of the sanitiza-
tion methods used to protect the privacy of the personal information
in databases rely on techniques such as perturbation – adding or
deleting attributes or tuples, or altering attribute values – and gen-
eralization – replacing an attribute value by a less specific one (e.g.,
age interval instead of date of birth).
The seminal k-anonymity concept introduced by Sweeney san-
itizes databases by generalizing and/or suppressing sensitive at-
tributes [26]. Its objective is to address the privacy issues related to
quasi-identifiers, which are innocuous attributes uniquely identify-
ing an individual when combined together. The canonical example
is the fact that the birth date, the gender and the ZIP code are suffi-
cient to identify uniquely most individuals in the USA. Hence, this
model requires that after the generalization and the suppression
of attributes at least k different tuples share the same values for a
given set of related quasi-identifiers.
Session 6: Privacy 1ASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea208Unfortunately, the privacy properties given by the k-anonymity
have been shown to be relatively weak. For instance, it may hap-
pen that an attribute has a unique value (e.g., cancer) for all the
tuples sharing the same values of quasi-identifiers. This allows to
infer sensitive information on all the individuals sharing the same
values of quasi-identifiers, even if a particular individual cannot be
re-identified [16]. In addition, if databases are anonymized indepen-
dently but share a number of common individuals, this may lead to
a complete breakdown for the privacy of these individuals [10].
To address some of the shortcomings of the k-anonymity model,
Machanavajjhala, Kifer, Gehrke and Venkitasubramaniam [16] in-
troduced the l-diversity model to ensure the diversity of a sensitive
attribute. Hence, at least l distinct values for the sensitive attribute
should be present in the k tuples sharing the same quasi-identifiers.
Finally, Li, Li and Venkatasubramanian [14] proposed afterwards
the stronger model of t-closeness ensuring that the distribution of a
sensitive attribute in the tuples sharing the same quasi-identifiers
is close to the distribution of this attribute in the overall database.
Following a different line of work, Dwork, McSherry, Nissim and
Smith introduced the differential privacy model [8]. Its objective is
to protect against statistical inferences performed by the adversary.
In this model, if two databases DB1 and DB2 differ only with
respect to one tuple (in which case they are called neighbors), the
distributions of the outcomes of the possible queries on these two
databases will be approximately the same. Thus, the influence of
a particular tuple on the output of a query is small, and the infor-
mation gained by the adversary on this particular tuple is limited.
The mathematical framework developed for differential privacy is
now widely used to ensure the privacy of personal information
in databases. However, note that one of the implicit assumption
makes by this model is that the database tuples are independent.
This assumption does not necessary hold with real-world databases.
This can lead to privacy breaches. Recently, Liu, Chakraborty and
Mittal [15] have developed approaches to mitigate this problem.
Finally, the (d, γ )-privacy model has been proposed by Rastogi,
Suciu and Hong [18, 19] to jointly address the privacy and utility
issues of statistical relational databases. This model sanitizes the
data by mixing true and false tuples. The true tuples are taken from
the original database while the false tuples are sampled from the
domain of all possible tuples. The sanitized view V can be defined
as follows. Each tuple in the database is selected with probability
α while each possible tuple that is not in the original database
is selected with probability β. This approach, which ensures the
privacy of the personal information in the database, is based on the
difference between the a priori belief of an adversary with respect
to the tuples of the database and his a posteriori belief once he has
accessed to a sanitized view of this database. The parameters α
and β depend on the probabilities associated to these beliefs. As