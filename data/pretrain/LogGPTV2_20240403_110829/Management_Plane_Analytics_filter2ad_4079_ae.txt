Internet Network Management, pages 305–306, 2007.
[14] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in
data centers: Measurement, analysis, and implications. In
SIGCOMM, 2011.
[15] M. Hollander and D. Wolfe. Nonparametric statistical methods.
Wiley, 1973.
[16] M. Jain and C. Dovrolis. End-to-end available bandwidth:
Measurement methodology, dynamics, and relation with TCP
throughput. In SIGCOMM, 2002.
[17] A. Jaquith. Security Metrics: Replacing Fear, Uncertainty, and
Doubt. Addison-Wesley, 2007.
[18] D. D. Jensen, A. S. Fast, B. J. Taylor, and M. E. Maier. Automatic
identiﬁcation of quasi-experimental designs for discovering causal
knowledge. In KDD, 2008.
[19] T. M. Khoshgoftaar, M. Golawala, and J. Van Hulse. An empirical
study of learning from imbalanced data using random forest. In
International Conference on Tools with Artiﬁcial Intelligence
(ICTAI), 2007.
[20] H. Kim, T. Benson, A. Akella, and N. Feamster. The evolution of
network conﬁguration: A tale of two campuses. In IMC, 2011.
[21] S. S. Krishnan and R. K. Sitaraman. Video stream quality impacts
viewer behavior: Inferring causality using quasi-experimental
designs. In IMC, 2012.
[22] S. S. Krishnan and R. K. Sitaraman. Understanding the effectiveness
of video ads: A measurement study. In IMC, 2013.
[23] S. D. Krothapalli, X. Sun, Y.-W. E. Sung, S. A. Yeo, and S. G. Rao.
A toolkit for automating and visualizing VLAN conﬁguration. In
SafeConﬁg, 2009.
[24] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson. User-level
Internet path diagnosis. In SOSP, 2003.
[25] R. Potharaju and N. Jain. Demystifying the dark side of the middle:
A ﬁeld study of middlebox failures in datacenters. In IMC, 2013.
[26] R. Potharaju, N. Jain, and C. Nita-Rotaru. Juggling the jigsaw:
Towards automated problem inference from network trouble tickets.
In NSDI, 2013.
[27] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1993.
406[28] Really Awesome New Cisco ConfIg Differ (RANCID).
http://shrubbery.net/rancid.
[29] D. B. Rubin. Using multivariate matched sampling and regression
adjustment to control bias in observational studies. Journal of the
American Statistical Association, 74:318–328, 1979.
[30] W. Shadish, T. Cook, , and D. Campbell. Experimental and
Quasi-Experimental Designs for Generalized Causal Inference.
Houghton Mifﬂin, 2002.
[31] N. Spring, R. Mahajan, D. Wetherall, and T. Anderson. Measuring
ISP topologies with Rocketfuel. IEEE/ACM Transactions on
Networking (ToN), 2004.
[32] E. A. Stuart. Matching methods for causal inference: A review and a
look forward. Statistical Science, 25, 2010.
[33] E. A. Stuart and D. B. Rubin. Best practices in quasi-experimental
designs: Matching methods for causal inference. In Best Practices in
Quantitative Methods, pages 155–176. Sage, 2008.
[34] Y. Sung, S. Rao, S. Sen, and S. Leggett. Extracting network-wide
correlated changes from longitudinal conﬁguration data. In PAM,
2009.
[35] Traceroute.org. http://www.traceroute.org.
[36] HP OpenView TrueControl Software.
http://support.openview.hp.com.
[37] D. Turner, K. Levchenko, J. C. Mogul, S. Savage, and A. C. Snoeren.
On failure in managed enterprise networks. Technical Report
HPL-2012-101, HP.
[38] D. Turner, K. Levchenko, S. Savage, and A. C. Snoeren. A
comparison of syslog and is-is for network failure analysis. In IMC,
2013.
[39] D. Turner, K. Levchenko, A. C. Snoeren, and S. Savage. California
fault lines: Understanding the causes and impact of network failures.
In SIGCOMM, 2010.
APPENDIX
A. CHARACTERIZATION OF
MANAGEMENT PRACTICES
We provide a detailed characterization of the management prac-
tices used at a large online service provider (OSP). This offers a
unique and rich view into the practices used in a modern, profession-
ally-managed infrastructure. We are not claiming that this view is
representative. For brevity, we quantify a subset of the practice
metrics in Table 1. We ﬁnd signiﬁcant diversity in the design and
operational practices employed across the OSP’s networks.
A.1 Design Practices
We start by examining the OSP’s networks in terms of their net-
work composition, structure, and purpose.
The majority (81%) of networks host only one workload—net-
works are quite homogeneous in this respect. A handful of net-
works do not host any workloads; they only connect networks to
each other or the external world.
The networks contain a mix of device roles, including routers,
switches, ﬁrewalls, application delivery controllers (ADCs)3, and
load balancers. Most networks (86%) have devices in multiple
roles—although no single device has more than one role—and 71%
of networks contain at least one middlebox (ﬁrewall, ADC, or load
balancer). We also ﬁnd that over 81% of networks contain devices
from more than one vendor, with a maximum of 6, and over 96%
of networks contain more than one device model, with a maximum
of 25. Thus, some networks must use more than one device model
for the same role. Indeed, a closer look at the hardware entropy
of the networks (solid line in Figure 11(a)) shows that only 4% of
networks have just one model and one role; the remaining (96% of)
networks have varying degrees of heterogeneity, up to a maximum
entropy metric value of 0.82. The extent of ﬁrmware heterogeneity
is similar (dashed line in Figure 11(a)).
Next, we look at the logical composition and structure of the data
and control planes. As shown in Figure 11(b), all networks use at
least two layer-2 protocols (VLAN, spanning tree, link aggregation,
unidirectional link detection (UDLD), DHCP relay, etc.), and 89%
of networks use at least one routing protocol (BGP and/or OSPF).
Furthermore, 10% of networks use 8 different protocols. Overall
there is signiﬁcant diversity in the combination of protocols used.
We ﬁnd the same diversity in the number of instances of each
protocol. Less than 5 VLANs are conﬁgured in 5% of networks, but
over 100 VLANs are conﬁgured in 9% of networks (Figure 11(c)).
Similarly, 86% of networks use BGP for layer-3 routing, with just
one BGP instance in 39% of networks and more than 20 instances
in 8% of networks (Figure 11(e)). In contrast, only 31% of net-
works use OSPF for layer-3 routing, with just one or two OSPF
instances used in these networks.
Finally, to characterize conﬁguration complexity, Figure 11(d)
shows a CDF of intra- and inter-device referential complexity. We
ﬁnd that some networks’ conﬁguration is extremely complex (based
on Benson et al.’s metrics [5]): in 20% of networks, the mean intra-
and inter-device reference counts are higher than 100. However, it
is worth noting that: (i) the range in complexity is rather large, and
(ii) most networks have signiﬁcantly lower conﬁguration complex-
ity metrics than the worst 10%.
A.2 Operational Practices
We now characterize the frequency, type, and modality of con-
ﬁguration changes, as well as those of change events.
In general, the average number of conﬁguration changes per
month is correlated with network size (Figure 12(a); Pearson cor-
relation coefﬁcient of 0.64). However, several large networks have
relatively fewer changes per month: e.g., one network has over 300
devices but fewer than 150 changes per month. Likewise, there
are several small networks with a disproportionately high change
rate. Furthermore, not every device is changed every month—in
77% of networks less than half of a network’s devices are changed
in a given month—but most devices are changed at least once per
year—in 80% of networks more than three-quarters of the devices
are changed in a year (Figure 12(b)). Thus, changes occur fre-
quently, and to different sets of devices in different months.
We now analyze different types of changes. Across our entire
dataset there are ≈480 different types of changes. Figure 12(c)
shows CDFs of the fraction of changes in which at least one stanza
of a given type is changed. On a per-network basis, interface changes
are the most common, followed by pool (used on load balancers),
ACL, user, and router. 4
Among the above most-frequently changed types, pool changes
are also the most frequently automated—more than half of all pool
changes are automated in 77% of networks—followed by ACL and
interface changes. We also look at the extent of automation over
all types of changes. As shown in Figure 12(d), more than half
(quarter) of the changes each month are automated in 41% (81%)
of networks. In general, we note a signiﬁcant diversity in the extent
of automation:
it ranges between 10% and 70%. Equally inter-
estingly, the fraction of automated changes is not strongly corre-
lated with the number changes (Pearson correlation coefﬁcient is
0.23). Furthermore, the types of changes that are automated most
frequently—sﬂow and QoS—are not the most frequent types of
changes.
3ADCs perform TCP and SSL ofﬂoad, HTTP compression and
caching, content-aware load balancing, etc.
4There are no pool changes in 63% of networks because these net-
works do not contain load balancers.
407s
k
r
o
w
e
N
t
f
o
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
Hardware
Firmware
0
0.3
0.6
0.9
s
k
r
o
w
e
N
t
f
o
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
s
k
r
o
w
e
N
t
f
o
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
L2
L3
Both
8
0
2
4
6
s
k
r
o
w
e
N
t
f
o
n
o
i
t
c
a
r
F
1
10
100 1000
Normalized Entropy
(a) Device heterogeneity
# of Protocols
(b) Protocol usage
# of VLANs
(c) No. of VLANs
1.0
0.8
0.6
0.4
0.2
0.0
Intra
Inter
0
10
100 1000
Normalized
Referential Complexity
1.0
0.8
0.6
s
k
r
o
w
e
N
t
f
0.4
o
n
o
0.2
i
t
c
a
r
F
0.0
BGP
OSPF
MSTP
1
0
100
# of Routing Instances
10
(d) Referential complex-
ity
(e) No.
stances
of routing In-
O(1K)
i
s
e
c
v
e
D
f
o
#
0
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
● ●
●
●●
●
●
●
●●
●
●