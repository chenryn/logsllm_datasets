以下是优化后的文本，使其更加清晰、连贯和专业：

---

### 参考文献

[26] HICKEY, W. 《美国人如何喜欢他们的牛排》。网址: http://fivethirtyeight.com/datalab/how-americans-like-their-steak, 2014. 访问日期：2016年2月10日。

[27] HINTON, G., VINYALS, O., AND DEAN, J. 《蒸馏神经网络中的知识》。arXiv:1503.02531 (2015)。

[28] HORNIK, K., STINCHCOMBE, M., AND WHITE, H. 《多层前馈网络是通用逼近器》。《神经网络》2, 5 (1989), 359–366页。

[29] HUANG, L., JOSEPH, A. D., NELSON, B., RUBINSTEIN, B. I., AND TYGAR, J. 《对抗性机器学习》。AISec (2011)，ACM，第43-58页。

[30] JACKSON, J. 《一种有效的成员查询算法，用于从均匀分布中学习DNF》。FOCS (1994)，IEEE，第42-53页。

[40] NEWSOME, J., KARP, B., AND SONG, D. 《Paragraph: 通过恶意训练来挫败签名学习》。RAID (2006)，Springer，第81-105页。

[41] NOCEDAL, J., AND WRIGHT, S. 《数值优化》。Springer Science & Business Media, 2006。

[42] PEDREGOSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V., THIRION, B., GRISEL, O., BLONDEL, M., PRETTENHOFER, P., WEISS, R., DUBOURG, V., VANDERPLAS, J., PASSOS, A., COURNAPEAU, D., BRUCHER, M., PERROT, M., AND DUCHESNAY, E. 《Scikit-learn: Python中的机器学习》。JMLR 12 (2011), 2825–2830页。

[43] PREDICTIONIO. 网址: http://prediction.io。访问日期：2016年2月10日。

[44] RUBINSTEIN, B. I., BARTLETT, P. L., HUANG, L., AND TAFT, N. 《在大函数空间中学习：SVM学习的隐私保护机制》。JPC 4, 1 (2012), 4页。

[45] RUBINSTEIN, B. I., NELSON, B., HUANG, L., JOSEPH, A. D., LAU, S.-H., RAO, S., TAFT, N., AND TYGAR, J. 《解毒剂：理解和防御异常检测器中毒》。IMC (2009)，ACM，第1-14页。

[46] SAAR-TSECHANSKY, M., AND PROVOST, F. 《应用分类模型时处理缺失值》。JMLR (2007)。

[47] SETTLES, B. 《主动学习文献综述》。威斯康星大学麦迪逊分校，52, 55-66 (1995), 11页。

[48] SHOKRI, R., AND SHMATIKOV, V. 《隐私保护深度学习》。CCS (2015)，ACM，第1310-1321页。

[49] SMITH, T. W., MARSDEN, P., HOUT, M., AND KIM, J. 《一般社会调查，1972-2012》，2013年。

[50] STEVENS, D., AND LOWD, D. 《规避线性分类器组合的难度》。AISec (2013)，ACM，第77-86页。

[51] THEANO开发团队。Theano: 一个Python框架，用于快速计算数学表达式。arXiv:1605.02688 (2016)。

[52] TOWELL, G. G., AND SHAVLIK, J. W. 《从基于知识的神经网络中提取精炼规则》。《机器学习》13, 1 (1993), 71–101页。

[53] VALIANT, L. G. 《可学习性理论》。《ACM通讯》27, 11 (1984), 1134–1142页。

[54] VINTERBO, S. 《差分私有投影直方图：构造和用于预测》。ECML-PKDD (2012)。

[55] ŠRNĐIĆ, N., AND LASKOV, P. 《基于学习的分类器的实际规避：案例研究》。Security and Privacy (SP) (2014)，IEEE，第197-211页。

[31] JAGANNATHAN, G., PILLAIPAKKAMNATT, K., AND WRIGHT, R. N. 《实用的差分私有随机决策树分类器》。ICDMW (2009)，IEEE，第114-121页。

[32] KLOFT, M., AND LASKOV, P. 《在线异常检测在对抗影响下的应用》。AISTATS (2010)，第405-412页。

[33] KUSHILEVITZ, E., AND MANSOUR, Y. 《使用傅里叶谱学习决策树》。SICOMP 22, 6 (1993), 1331–1348页。

[34] LI, N., QARDAJI, W., SU, D., WU, Y., AND YANG, W. 《成员隐私：统一隐私定义框架》。CCS (2013)，ACM。

[35] LICHMAN, M. 《UCI机器学习库》，2013年。

[36] LOWD, D., AND MEEK, C. 《对抗性学习》。KDD (2005)，ACM，第641-647页。

[37] LOWD, D., AND MEEK, C. 《统计垃圾邮件过滤器上的好词攻击》。CEAS (2005)。

[38] MICROSOFT AZURE。网址: https://azure.microsoft.com/services/machine-learning。访问日期：2016年2月10日。

[39] NELSON, B., RUBINSTEIN, B. I., HUANG, L., JOSEPH, A. D., LEE, S. J., RAO, S., AND TYGAR, J. 《避开凸诱导分类器的查询策略》。JMLR 13, 1 (2012), 1293–1332页。

[56] ZHANG, J., ZHANG, Z., XIAO, X., YANG, Y., AND WINSLETT, M. 《功能机制：差分隐私下的回归分析》。VLDB (2012)。

[57] ZHU, J., AND HASTIE, T. 《核逻辑回归和重要向量机》。NIPS (2001)，第1081-1088页。

### 模型详细信息

#### 支持向量机 (SVMs)

支持向量机 (SVMs) 通过在 d 维特征空间中定义一个最大分离超平面来进行二分类 (c = 2)。线性 SVM 是一个函数 f(x) = sign(w · x + β)，其中 'sign' 对所有负输入输出 0，否则输出 1。线性 SVM 不适合非线性可分数据，在这种情况下，通常使用核技术 [14]。

核函数是一个映射 K : X × X → R。典型的核包括二次核 Kquad(x, x') = (x^T · x' + 1)^2 和高斯径向基函数 (RBF) 核 Krbf(x, x') = e^(-γ||x - x'||^2)，参数化为 γ ∈ R。核的投影函数 φ 定义为 K(x, x') = φ(x) · φ(x')。我们不显式使用 φ，特别是对于 RBF 核，这会产生无限维向量。相反，分类使用“核技巧”进行定义：f(x) = sign([ ∑t_i=1 αiK(x, xi)] + β)，其中 β 是学习到的阈值，α1, ..., αt 是学习到的权重，x1, ..., xt 是训练集中的特征向量。对于非零 αi 的 xi 被称为支持向量。注意，对于非零 αi，αi > 0 否则为 0。

#### 逻辑回归

SVM 不直接推广到多类设置 (c > 2)，也不输出类别概率。逻辑回归 (LR) 是一个流行的分类器，可以做到这一点。二元 LR 模型定义为 f1(x) = σ(w · x + β) = 1/(1 + e^(-(w · x + β))) 和 f0(x) = 1 - f1(x)。当 c > 2 时，固定 c 个权重向量 w0, ..., wc-1 每个都在 Rd 中，阈值 β0, ..., βc-1 在 R 中，并定义 fi(x) = ewi·x+βi / (∑cj=0 ewj·x+βj) 对于 i ∈ Zc。类别标签取为 argmaxi fi(x)。多类回归被称为多项或 softmax 回归。另一种方法是为每个类别构建一个二元模型 σ(wi · x + βi)，然后设置 fi(x) = σ(wi · x + βi) / ∑j σ(wj · x + βj)。这些是对数线性模型，可能不适合在 X 中不是线性可分的数据。同样，可以使用核技术来处理更复杂的数据关系（参见 [57]）。然后，将 wi · x + βi 替换为 ∑tr=1 αi,rK(x, xr) + βi。如上所述，这使用了整个训练数据点集 x1, ..., xt 作为所谓的代表点（这里类似于支持向量）。与 SVM 不同，大多数训练数据点永远不会成为支持向量，而在这里所有训练数据点都可能是代表点。实际上，使用大小 s < t 的子集。

#### 决策树

决策树是一种层次结构，其中每个内部节点表示一个特征测试，每个分支表示该测试的一个结果，每个叶节点表示一个类别。决策树通过递归地对数据进行分割来构建。当到达一个叶节点时，终止并输出该叶节点的值。这个值可以是一个类别标签，或者是一个类别标签及其置信度分数。这定义了一个函数 f : X → Y。

### 数据集详细信息

本节提供有关我们在本文中使用的数据集的一些更多信息。请参阅表 3 和表 5。

**合成数据集**：我们使用了来自 scikit [42] 的四个合成数据集。前两个数据集是经典的非线性可分数据示例，由两个同心圆或两个交错的新月形组成。接下来的两个合成数据集 Blobs 和 5-Class 包含分配给 3 或 5 个类别的高斯簇点。

**公共数据集**：我们收集了一组多样化的数据集，代表 ML 服务用户可能会用来训练基于逻辑回归和 SVM 的模型的数据类型。这些数据集包括 UCI ML 库中的著名监督学习数据集（Adult, Iris, Breast Cancer, Mushrooms, Diabetes）。我们还考虑了先前工作中用于模型反转的 Steak 和 GSS 数据集 [23]。最后，我们添加了一个 scikit 中可用的数字数据集，以直观展示核化逻辑模型中的训练数据泄漏（参见第 4.1.3 节）。

**BigML 公共数据集和模型**：对于决策树实验，我们选择了一组 BigML 平台上公开可用的模型。这些模型由真实的 MLaaS 用户训练，并涵盖了广泛的应用场景，从而为我们的提取攻击评估提供了现实的基准。

**IRS 模型**：根据行政税务记录预测美国州。
**Steak 和 GSS 模型**：分别根据调查和人口统计数据预测一个人偏好的牛排制作方式和幸福感水平。这两个模型也在 [23] 中被考虑。
**Email Importance 模型**：根据消息元数据预测 Gmail 是否将电子邮件分类为“重要”。
**Email Spam 模型**：根据内容中某些词的存在情况将电子邮件分类为垃圾邮件。
**German Credit 数据集**：来自 UCI 库 [35]，用于分类用户的贷款风险。
**Medical Charges 模型**：基于州人口统计数据预测美国的医疗费用。
**Bitcoin Market Price 模型**：根据每日开盘和收盘值预测比特币市场价格。

### 路径查找算法分析

在本节中，我们分析了算法 1 中决策树提取算法的正确性和复杂性。假设所有叶子都被 Oracle O 分配了一个唯一的 ID，并且没有连续特征被分割成宽度小于 ε 的区间。我们可以直接用 ID 来指代具有该标识的叶子。

**正确性**：算法的终止性立即从以下事实得出：只有在访问新叶子时才会向 Q 添加新的查询。由于树中的叶子数量是有界的，因此算法必须终止。我们通过反证法证明所有叶子最终都会被访问。设节点 v 的深度为其到根的路径长度（根的深度为 0）。对于两个叶子 id 和 id'，设 A 为其最深的共同祖先（A 是出现在 id 和 id' 路径上的最深节点）。我们将 A 的深度记为 Δ(id, id')。假设算法 1 在未访问所有叶子的情况下终止，并设 (id, id') 为具有最大 Δ(id, id') 的叶子对，其中 id 被访问但 id' 未被访问。设 xi 为其最深共同祖先 A 所分割的特征。当访问 id 时，算法调用 LINE SEARCH 或 CATEGORY SPLIT 对特征 xi 进行操作。由于所有叶子 ID 都是唯一的且没有小于 ε 的区间，我们将在 A 的每个子树中发现一个叶子，包括包含 id' 的子树。因此，我们会访问一个叶子 id''，使得 Δ(id'', id') > Δ(id, id')，矛盾。

**复杂性**：设 m 表示树中的叶子数量。每个叶子只被访问一次，并且对于每个叶子，我们检查所有 d 个特征。假设连续特征的范围为 [0, b]，分类特征的基数为 k。对于连续特征，找到一个阈值最多需要 log2(b/ε) 个查询。由于在一个特征上的总分裂次数最多为 m（即，所有节点都分裂在同一特征上），找到所有阈值最多需要 m · log2(b/ε) 个查询。测试一个分类特征需要 k 个查询。总的查询复杂性为 O(m · (dcat · k + dcont · m · log2(b/ε)))，其中 dcat 和 dcont 分别表示分类和连续特征的数量。

对于布尔树的特殊情况，复杂性为 O(m · d)。相比之下，仅使用成员查询的 [33] 算法的复杂性是关于 d 和 2^δ 的多项式，其中 δ 是树的深度。对于退化树，2^δ 可能是 m 的指数级，这意味着唯一叶子身份（例如，从置信度分数获得）的假设比仅使用类别标签的最佳已知方法提供了指数级加速。[33] 中的算法可以扩展到回归树，其复杂性是关于输出范围 Y 的大小的多项式。同样，在唯一叶子身份（可以从输出值单独获得）的假设下，我们得到了一个更高效的算法，其复杂性独立于输出范围。

**自顶向下方法**：第 4.2 节中自顶向下算法（使用不完整查询）的正确性和复杂性遵循类似的分析。主要区别在于我们假设所有节点都有唯一的 ID，而不仅仅是叶子节点。

### 关于不恰当提取的说明

为了在不知道模型类的情况下提取模型 f，一个简单的策略是提取一个隐藏层足够大的多层感知器 ˆf。事实上，具有单个隐藏层的前馈网络原则上可以在 Rd 的有界子集上近似任何连续函数 [20, 28]。然而，这种策略似乎并不最优。即使我们知道可以找到一个与 f 密切匹配的多层感知器 ˆf，ˆf 可能比 f 有更复杂的表示（更多参数）。因此，针对目标 f 的“更简单”的模型类进行定制提取似乎更有效。在学习理论中，找到目标模型 f 的简洁表示的问题被称为奥卡姆学习 [13]。

我们的实验表明，在等式求解攻击的背景下，这种通用的不恰当提取确实显得次优。我们在 Adult 数据集上训练了一个以“种族”为目标的 softmax 回归模型。模型 f 由 530 个实数值参数定义。如 4.1.2 节所示，仅使用 530 个查询，我们就从相同的模型类中提取了一个与 f 密切匹配的模型 ˆf（ˆf 和 f 在所有测试输入上预测相同的标签，并且产生的类别概率在 TV 距离上相差不到 10^-7）。我们还假设目标类为多层感知器，提取了相同的模型。即使有 1,000 个隐藏节点（此模型有 111,005 个参数），并且使用了 10 倍更多的查询（5,300 个），提取的模型 ˆf 仍然是 f 的较弱近似（类别标签的准确率为 99.5%，类别概率的 TV 距离为 10^-2）。

---

希望这些修改使文本更加清晰、连贯和专业。如果还有其他需要改进的地方，请告诉我！