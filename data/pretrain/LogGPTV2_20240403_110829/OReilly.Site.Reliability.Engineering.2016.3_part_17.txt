the alerting rule. However, the number of errors is not greater than 1 at this moment,
so the alert won’t be active. Once the number of errors exceeds 1, the alert will go
pending for two minutes to ensure it isn’t a transient state, and only then will it fire.
The alert rule contains a small template for filling out a message containing contex‐
tual information: which job the alert is for, the name of the alert, the numerical value
of the triggering rule, and so on. The contextual information is filled out by Borgmon
when the alert fires and is sent in the Alert RPC.
118 | Chapter 10: Practical Alerting from Time-Series Data
Borgmon is connected to a centrally run service, known as the Alertmanager, which
receives Alert RPCs when the rule first triggers, and then again when the alert is con‐
sidered to be “firing.” The Alertmanager is responsible for routing the alert notifica‐
tion to the correct destination. Alertmanager can be configured to do the following:
• Inhibit certain alerts when others are active
• Deduplicate alerts from multiple Borgmon that have the same labelsets
• Fan-in or fan-out alerts based on their labelsets when multiple alerts with similar
labelsets fire
As described in Chapter 6, teams send their page-worthy alerts to their on-call rota‐
tion and their important but subcritical alerts to their ticket queues. All other alerts
should be retained as informational data for status dashboards.
A more comprehensive guide to alert design can be found in Chapter 4.
Sharding the Monitoring Topology
A Borgmon can import time-series data from other Borgmon, as well. While one
could attempt to collect from all tasks in a service globally, doing so quickly becomes
a scaling bottleneck and introduces a single point of failure into the design. Instead, a
streaming protocol is used to transmit time-series data between Borgmon, saving
CPU time and network bytes compared to the text-based varz format. A typical such
deployment uses two or more global Borgmon for top-level aggregation and one
Borgmon in each datacenter to monitor all the jobs running at that location. (Google
divides the production network into zones for production changes, so having two or
more global replicas provides diversity in the face of maintenance and outages for
this otherwise single point of failure.)
As shown in Figure 10-3, more complicated deployments shard the datacenter Borg‐
mon further into a purely scraping-only layer (often due to RAM and CPU con‐
straints in a single Borgmon for very large services) and a DC aggregation layer that
performs mostly rule evaluation for aggregation. Sometimes the global layer is split
between rule evaluation and dashboarding. Upper-tier Borgmon can filter the data
they want to stream from the lower-tier Borgmon, so that the global Borgmon does
not fill its arena with all the per-task time-series from the lower tiers. Thus, the aggre‐
gation hierarchy builds local caches of relevant time-series that can be drilled down
into when required.
Sharding the Monitoring Topology | 119
Figure 10-3. A data flow model of a hierarchy of Borgmon in three clusters
Black-Box Monitoring
Borgmon is a white-box monitoring system—it inspects the internal state of the tar‐
get service, and the rules are written with knowledge of the internals in mind. The
transparent nature of this model provides great power to identify quickly what com‐
ponents are failing, which queues are full, and where bottlenecks occur, both when
responding to an incident and when testing a new feature deployment.
However, white-box monitoring does not provide a full picture of the system being
monitored; relying solely upon white-box monitoring means that you aren’t aware of
what the users see. You only see the queries that arrive at the target; the queries that
never make it due to a DNS error are invisible, while queries lost due to a server crash
never make a sound. You can only alert on the failures that you expected.
Teams at Google solve this coverage issue with Prober, which runs a protocol check
against a target and reports success or failure. The prober can send alerts directly to
Alertmanager, or its own varz can be collected by a Borgmon. Prober can validate the
response payload of the protocol (e.g., the HTML contents of an HTTP response) and
validate that the contents are expected, and even extract and export values as time-
series. Teams often use Prober to export histograms of response times by operation
type and payload size so that they can slice and dice the user-visible performance.
Prober is a hybrid of the check-and-test model with some richer variable extraction
to create time-series.
120 | Chapter 10: Practical Alerting from Time-Series Data
Prober can be pointed at either the frontend domain or behind the load balancer. By
using both targets, we can detect localized failures and suppress alerts. For example,
we might monitor both the load balanced www.google.com and the web servers in
each datacenter behind the load balancer. This setup allows us to either know that
traffic is still served when a datacenter fails, or to quickly isolate an edge in the traffic
flow graph where a failure has occurred.
Maintaining the Configuration
Borgmon configuration separates the definition of the rules from the targets being
monitored. This means the same sets of rules can be applied to many targets at once,
instead of writing nearly identical configuration over and over. This separation of
concerns might seem incidental, but it greatly reduces the cost of maintaining the
monitoring by avoiding lots of repetition in describing the target systems.
Borgmon also supports language templates. This macro-like system enables engineers
to construct libraries of rules that can be reused. This functionality again reduces rep‐
etition, thus reducing the likelihood of bugs in the configuration.
Of course, any high-level programming environment creates the opportunity for
complexity, so Borgmon provides a way to build extensive unit and regression tests
by synthesizing time-series data, in order to ensure that the rules behave as the author
thinks they do. The Production Monitoring team runs a continuous integration ser‐
vice that executes a suite of these tests, packages the configuration, and ships the con‐
figuration to all the Borgmon in production, which then validate the configuration
before accepting it.
In the vast library of common templates that have been created, two classes of moni‐
toring configuration have emerged. The first class simply codifies the emergent
schema of variables exported from a given library of code, such that any user of the
library can reuse the template of its varz. Such templates exist for the HTTP server
library, memory allocation, the storage client library, and generic RPC services,
among others. (While the varz interface declares no schema, the rule library associ‐
ated with the code library ends up declaring a schema.)
The second class of library emerged as we built templates to manage the aggregation
of data from a single-server task to the global service footprint. These libraries con‐
tain generic aggregation rules for exported variables that engineers can use to model
the topology of their service.
Maintaining the Configuration | 121
For example, a service may provide a single global API, but be homed in many data‐
centers. Within each datacenter, the service is composed of several shards, and each
shard is composed of several jobs with various numbers of tasks. An engineer can
model this breakdown with Borgmon rules so that when debugging, subcomponents
can be isolated from the rest of the system. These groupings typically follow the
shared fate of components; e.g., individual tasks share fate due to configuration files,
jobs in a shard share fate because they’re homed in the same datacenter, and physical
sites share fate due to networking.
Labeling conventions make such division possible: a Borgmon adds labels indicating
the target’s instance name and the shard and datacenter it occupies, which can be
used to group and aggregate those time-series together.
Thus, we have multiple uses for labels on a time-series, though all are interchangea‐
ble:
• Labels that define breakdowns of the data itself (e.g., our HTTP response code on
the http_responses variable)
• Labels that define the source of the data (e.g., the instance or job name)
• Labels that indicate the locality or aggregation of the data within the service as a
whole (e.g., the zone label describing a physical location, a shard label describing
a logical grouping of tasks)
The templated nature of these libraries allows flexibility in their use. The same tem‐
plate can be used to aggregate from each tier.
Ten Years On…
Borgmon transposed the model of check-and-alert per target into mass variable col‐
lection and a centralized rule evaluation across the time-series for alerting and
diagnostics.
This decoupling allows the size of the system being monitored to scale independently
of the size of alerting rules. These rules cost less to maintain because they’re abstrac‐
ted over a common time-series format. New applications come ready with metric
exports in all components and libraries to which they link, and well-traveled aggrega‐
tion and console templates, which further reduces the burden of implementation.
Ensuring that the cost of maintenance scales sublinearly with the size of the service is
key to making monitoring (and all sustaining operations work) maintainable. This
theme recurs in all SRE work, as SREs work to scale all aspects of their work to the
global scale.
122 | Chapter 10: Practical Alerting from Time-Series Data
Ten years is a long time, though, and of course today the shape of the monitoring
landscape within Google has evolved with experiments and changes, striving for con‐
tinual improvement as the company grows.
Even though Borgmon remains internal to Google, the idea of treating time-series
data as a data source for generating alerts is now accessible to everyone through those
open source tools like Prometheus, Riemann, Heka, and Bosun, and probably others
by the time you read this.
Ten Years On… | 123
CHAPTER 11
Being On-Call
Written by Andrea Spadaccini1
Edited by Kavita Guliani
Being on-call is a critical duty that many operations and engineering teams must
undertake in order to keep their services reliable and available. However, there are
several pitfalls in the organization of on-call rotations and responsibilities that can
lead to serious consequences for the services and for the teams if not avoided. This
chapter describes the primary tenets of the approach to on-call that Google’s Site Reli‐
ability Engineers (SREs) have developed over years, and explains how that approach
has led to reliable services and sustainable workload over time.
Introduction
Several professions require employees to perform some sort of on-call duty, which
entails being available for calls during both working and nonworking hours. In the IT
context, on-call activities have historically been performed by dedicated Ops teams
tasked with the primary responsibility of keeping the service(s) for which they are
responsible in good health.
Many important services in Google, e.g., Search, Ads, and Gmail, have dedicated
teams of SREs responsible for the performance and reliability of these services. Thus,
SREs are on-call for the services they support. The SRE teams are quite different from
purely operational teams in that they place heavy emphasis on the use of engineering
to approach problems. These problems, which typically fall in the operational
domain, exist at a scale that would be intractable without software engineering
solutions.
1 An earlier version of this chapter appeared as an article in ;login: (October 2015, vol. 40, no. 5).
125
To enforce this type of problem solving, Google hires people with a diverse back‐
ground in systems and software engineering into SRE teams. We cap the amount of
time SREs spend on purely operational work at 50%; at minimum, 50% of an SRE’s
time should be allocated to engineering projects that further scale the impact of the
team through automation, in addition to improving the service.
Life of an On-Call Engineer
This section describes the typical activities of an on-call engineer and provides some
background for the rest of the chapter.
As the guardians of production systems, on-call engineers take care of their assigned
operations by managing outages that affect the team and performing and/or vetting
production changes.
When on-call, an engineer is available to perform operations on production systems
within minutes, according to the paging response times agreed to by the team and the
business system owners. Typical values are 5 minutes for user-facing or otherwise
highly time-critical services, and 30 minutes for less time-sensitive systems. The com‐
pany provides the page-receiving device, which is typically a phone. Google has flexi‐
ble alert delivery systems that can dispatch pages via multiple mechanisms (email,
SMS, robot call, app) across multiple devices.
Response times are related to desired service availability, as demonstrated by the fol‐
lowing simplistic example: if a user-facing system must obtain 4 nines of availability
in a given quarter (99.99%), the allowed quarterly downtime is around 13 minutes
(Appendix A). This constraint implies that the reaction time of on-call engineers has
to be in the order of minutes (strictly speaking, 13 minutes). For systems with more
relaxed SLOs, the reaction time can be on the order of tens of minutes.
As soon as a page is received and acknowledged, the on-call engineer is expected to
triage the problem and work toward its resolution, possibly involving other team
members and escalating as needed.
Nonpaging production events, such as lower priority alerts or software releases, can
also be handled and/or vetted by the on-call engineer during business hours. These
activities are less urgent than paging events, which take priority over almost every
other task, including project work. For more insight on interrupts and other non-
paging events that contribute to operational load, see Chapter 29.
Many teams have both a primary and a secondary on-call rotation. The distribution
of duties between the primary and the secondary varies from team to team. One team
might employ the secondary as a fall-through for the pages the primary on-call
misses. Another team might specify that the primary on-call handles only pages,
while the secondary handles all other non-urgent production activities.
126 | Chapter 11: Being On-Call
In teams for which a secondary rotation is not strictly required for duty distribution,
it is common for two related teams to serve as secondary on-call for each other, with
fall-through handling duties. This setup eliminates the need for an exclusive secon‐
dary on-call rotation.
There are many ways to organize on-call rotations; for detailed analysis, refer to the
“Oncall” chapter of [Lim14].
Balanced On-Call
SRE teams have specific constraints on the quantity and quality of on-call shifts. The
quantity of on-call can be calculated by the percent of time spent by engineers on on-
call duties. The quality of on-call can be calculated by the number of incidents that
occur during an on-call shift.
SRE managers have the responsibility of keeping the on-call workload balanced and
sustainable across these two axes.
Balance in Quantity
We strongly believe that the “E” in “SRE” is a defining characteristic of our organiza‐
tion, so we strive to invest at least 50% of SRE time into engineering: of the remain‐
der, no more than 25% can be spent on-call, leaving up to another 25% on other types
of operational, nonproject work.
Using the 25% on-call rule, we can derive the minimum number of SREs required to
sustain a 24/7 on-call rotation. Assuming that there are always two people on-call
(primary and secondary, with different duties), the minimum number of engineers
needed for on-call duty from a single-site team is eight: assuming week-long shifts,
each engineer is on-call (primary or secondary) for one week every month. For dual-
site teams, a reasonable minimum size of each team is six, both to honor the 25% rule
and to ensure a substantial and critical mass of engineers for the team.
If a service entails enough work to justify growing a single-site team, we prefer to cre‐
ate a multi-site team. A multi-site team is advantageous for two reasons:
• Night shifts have detrimental effects on people’s health [Dur05], and a multi-site
“follow the sun” rotation allows teams to avoid night shifts altogether.
• Limiting the number of engineers in the on-call rotation ensures that engineers
do not lose touch with the production systems (see “A Treacherous Enemy:
Operational Underload” on page 132).
However, multi-site teams incur communication and coordination overhead. There‐
fore, the decision to go multi-site or single-site should be based upon the trade-offs
Balanced On-Call | 127
each option entails, the importance of the system, and the workload each system
generates.
Balance in Quality
For each on-call shift, an engineer should have sufficient time to deal with any inci‐
dents and follow-up activities such as writing postmortems [Loo10]. Let’s define an
incident as a sequence of events and alerts that are related to the same root cause and
would be discussed as part of the same postmortem. We’ve found that on average,
dealing with the tasks involved in an on-call incident—root-cause analysis, remedia‐
tion, and follow-up activities like writing a postmortem and fixing bugs—takes 6
hours. It follows that the maximum number of incidents per day is 2 per 12-hour on-
call shift. In order to stay within this upper bound, the distribution of paging events
should be very flat over time, with a likely median value of 0: if a given component or
issue causes pages every day (median incidents/day > 1), it is likely that something
else will break at some point, thus causing more incidents than should be permitted.
If this limit is temporarily exceeded, e.g., for a quarter, corrective measures should be
put in place to make sure that the operational load returns to a sustainable state (see
“Operational Overload” on page 130 and Chapter 30).
Compensation
Adequate compensation needs to be considered for out-of-hours support. Different
organizations handle on-call compensation in different ways; Google offers time-off-
in-lieu or straight cash compensation, capped at some proportion of overall salary.
The compensation cap represents, in practice, a limit on the amount of on-call work
that will be taken on by any individual. This compensation structure ensures incen‐
tivization to be involved in on-call duties as required by the team, but also promotes a
balanced on-call work distribution and limits potential drawbacks of excessive on-call
work, such as burnout or inadequate time for project work.
Feeling Safe
As mentioned earlier, SRE teams support Google’s most critical systems. Being an
SRE on-call typically means assuming responsibility for user-facing, revenue-critical
systems or for the infrastructure required to keep these systems up and running. SRE
methodology for thinking about and tackling problems is vital for the appropriate