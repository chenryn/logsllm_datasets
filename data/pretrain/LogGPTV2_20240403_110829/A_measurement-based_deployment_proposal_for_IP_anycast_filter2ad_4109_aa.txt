# A Measurement-based Deployment Proposal for IP Anycast

## Authors
- Hitesh Ballani, Cornell University, Ithaca, NY
- Paul Francis, Cornell University, Ithaca, NY
- Sylvia Ratnasamy, Intel Research, Berkeley, CA

## Abstract
Despite its growing use in critical infrastructure services, the performance of IPv4 Anycast and its interaction with IP routing practices is not well understood. In this paper, we present the results of a detailed measurement study of IP Anycast. Our study employs a two-pronged approach: First, using a variant of known latency estimation techniques, we measure the performance of current commercially operational IP Anycast deployments from a large number (>20,000) of vantage points. Second, we deploy our own small-scale anycast service to perform controlled tests under different deployment and failure scenarios. To the best of our knowledge, our study represents the first large-scale evaluation of existing anycast services and the first evaluation of the behavior of IP Anycast under failure.

Our findings include:
1. Ad-hoc IP Anycast deployments do not offer good latency-based proximity.
2. Ad-hoc IP Anycast deployments do not provide fast failover to clients.
3. IP Anycast typically offers good affinity to all clients, except those that explicitly load balance traffic across multiple providers.
4. IP Anycast, by itself, is not effective in balancing client load across multiple sites.

We propose and evaluate practical means by which anycast deployments can achieve good proximity, fast failover, and control over the distribution of client load. Overall, our results suggest that an IP Anycast service, if deployed carefully, can offer good proximity, load balance, and failover behavior.

**Categories and Subject Descriptors:** C.4 [Performance of Systems]: Measurement techniques, Performance attributes.
**General Terms:** Measurement, Performance.
**Keywords:** IP Anycast, BGP, Proximity, Affinity, Failover, Load Distribution.

## 1. Introduction
IP Anycast [28] is an addressing mode where the same IP address is assigned to multiple hosts. These hosts form an IP Anycast group, and each host is referred to as an anycast server. Packets from a client destined to the group address are automatically routed to the anycast server closest to the client, based on the metrics used by the underlying routing protocol. Since Internet routing does not differentiate between multiple routes to multiple hosts (as in IP Anycast) and multiple routes to the same host (as in multihoming), IP Anycast is completely backward compatible, requiring no changes to (IPv4 or IPv6) routers and routing protocols.

IP Anycast offers an attractive primitive for service discovery. The route-to-closest-server abstraction provides reduced access latency for clients, load-balancing across servers, and network-level resilience to DDoS attacks. Its implementation at the network layer allows these advantages to be realized without special configuration at clients or servers and with no dependence on higher-layer services such as DNS. While this potential has long been recognized [7, 26, 28], it is only in recent years that IP Anycast has gained importance, largely due to its use in the critical DNS root-server deployment—six of the thirteen DNS root-servers have been transparently replicated using IP Anycast, and this deployment continues to grow [18].

In addition, IP Anycast is being used in a growing variety of infrastructure services, such as improving the performance of caching DNS servers [27], drawing in private address space DNS queries as part of the AS-112 project [39], discovering rendezvous points for multicast groups [21], transitioning from IPv4 to IPv6 [19], sinkholing DoS attacks [16], and redirection in commercial CDNs [40]. Recent research efforts have also used IP Anycast to build proxy-based anycast services [4] and next-generation architecture deployment services [3, 30].

However, despite its growing use in critical infrastructure services, IP Anycast and its interaction with IP routing practices are not well understood. For example, the impact of anycasting DNS root-servers on clients that should, in theory, access the closest server has not been analyzed in detail. Similarly, there has been no exploration of whether root-server operators can control the load on individual servers by manipulating their routing advertisements, nor of the behavior of IP Anycast under server failure. Moreover, the various applications of IP Anycast make different assumptions about the underlying service. For instance, the use of IP Anycast in CDNs assumes client packets are routed to a proximal CDN server and that the impact of a server failure on clients is short-lived (i.e., clients are quickly routed to a different server). To gauge the effectiveness of IP Anycast in existing deployments and the feasibility of future usage scenarios, it is imperative to evaluate the performance of IP Anycast.

A couple of root-server operators [6, 12] have offered valuable reports on the performance of their anycast deployments. These are probably the first reports on the performance of IP Anycast and represent the best source of data on operational IP Anycast deployments from the point of view of the anycast servers. However, the analysis is preliminary, and details of the study are not published, nor is the data publicly available. Drawing from these, this paper presents a detailed study of inter-domain IP Anycast as measured from a large number of vantage points. Specifically, our study seeks to answer the following questions:

1. What kind of failover properties does a typical IP Anycast deployment offer?
2. What kind of load distribution do existing IP Anycast deployments see? Can the operator of an anycast deployment control this distribution of client load?
3. Past studies [4, 32] have reported that existing IP Anycast deployments may offer poor latency-based proximity; i.e., many clients may not be routed to the server closest in terms of latency. Using a larger number of clients and anycast groups, we aim to confirm and understand the reasons for this poor latency and explore possible remedial measures.
4. Past studies [4, 6, 8, 12, 31] have presented conflicting reports regarding the affinity offered by IP Anycast and consequently, the ability to run stateful services on top of anycast. We seek to measure, at scale, the affinity offered by IP Anycast.

To explore these questions, we study four existing IP Anycast deployments, including two anycasted DNS root-servers. In terms of methodology, our study differs from previous efforts on two fronts:

1. We use a variant of the King [17] measurement technique to observe and evaluate IP Anycast deployments from a very large (>20,000) number of vantage points. To the best of our knowledge, this represents a two-order-of-magnitude increase in the number of vantage points from which IP Anycast deployments have been actively probed for evaluation.
2. We deploy our own small-scale IP Anycast service for controlled evaluation of anycast under different deployment and failure scenarios. Performing such experiments would be difficult using commercial IP Anycast deployments such as the DNS root-servers.

### Key Findings
- **Proximity:** We corroborate evidence from past studies indicating that IP Anycast, by itself, does not offer good latency-based proximity. For example, for the 13-server J-root deployment, we find that 8,903 (≈40%) of the 22,281 measured clients are directed to a root-server that is more than 100 ms farther away from the closest server. While the impact of inter-domain routing on end-to-end path length has been well documented [34], we find that inter-domain routing metrics have an even more severe impact on the selection of paths to anycast destinations.
- **Deployment Scheme:** We propose and evaluate a practical deployment scheme designed to alleviate the proximity concerns surrounding IP Anycast. Specifically, ensuring that an ISP that provides transit to an anycast server has global presence and is geographically well covered by such servers improves the latency-based proximity offered by the anycast deployment.
- **Failover:** We find that IP Anycast is affected by delayed routing convergence, leading to slow failover for clients using anycast services. However, our proposed deployment scheme addresses this by reducing the scope of routing convergence that follows a server failure, ensuring that clients failover at a fast rate. For instance, in case of a server failure in an IP Anycast deployment conforming to our proposal, a vast majority (>95%) of clients can be re-routed to other operational servers in less than 20 seconds.
- **Affinity:** Through a much larger scale study compared to past efforts, we find that the anycasting of an IP prefix does not have any unfavorable interactions with inter-domain routing. Hence, IP Anycast offers very good affinity for all but a very small fraction of clients. Using temporal clustering, we show that the poor affinity observed by this small fraction of clients can be attributed to dynamic load-balancing mechanisms near them.
- **Load Distribution:** We find that a naive IP Anycast deployment does not lead to an even distribution of client load across servers. However, we also propose and evaluate the impact of operators manipulating BGP advertisements at individual anycast servers to control their load. Our results show that such mechanisms can achieve coarse-grained load balancing across anycast servers.

Overall, our measurements show that an IP Anycast service can be deployed to provide a robust substrate offering good proximity and fast failover while allowing for coarse-grained control over server load. In what follows, Section 2 reviews related measurement studies, Section 3 details the IP Anycast deployments we measure in this paper, while Section 4 describes our measurement methodology. We describe our proximity measurements in Section 5, failover measurements in Section 6, affinity measurements in Section 7, and load distribution measurements in Section 8. Finally, we discuss related issues in Section 9 and conclude with Section 10.

## 2. Related Measurement Studies
An invaluable vantage point for measuring anycast deployments is at the anycast servers themselves. In recent presentations [6, 12], the operators of the J and K root-servers report on their analysis of client logs collected at their anycast servers. They present the observed distribution of client load and affinity. Both studies report a skewed distribution of client load across their respective deployments. With regard to affinity, the J-root operators report instances of clients that exhibit poor affinity and conjecture that anycast may not be suitable for stateful services. By contrast, the K-root operators find that most of their clients experience very high affinity.

Our study builds on these earlier reports. Using active measurements from over 20,000 clients, we measure the affinity and load-distribution for our own small-scale anycast deployment, explore the reasons behind the observed load and affinity, and evaluate techniques to control server load. In addition to load and affinity, we use active measurements to evaluate the (latency) proximity seen at clients to four different anycast deployments. Finally, using our own deployment, we study the behavior of IP Anycast under server failure. As we describe in Section 6, our desire to use a large number of client vantage points prevents us from measuring the affinity and load to the DNS root-server deployments. However, for completeness, we did perform such measurements from a smaller set of clients that we have direct access to (i.e., PlanetLab nodes and approximately 200 publicly-available traceroute servers). Since the results were consistent with the larger-scale measurements over our own deployment, we only present the latter here. The details of the PlanetLab-based study can be found in [5].

We are aware of two recent efforts that measure the performance of IP Anycast using active probing from clients. Sarat et al. [31, 32] use PlanetLab nodes [11] as vantage points for evaluating the K-root, F-root, and .ORG TLD deployments. They measure proximity and affinity and report poor proximity and moderate-to-poor affinity. Similarly, using PlanetLab and approximately 200 volunteer nodes, Boothe et al. [8] measure the affinity offered by the anycasted DNS root-servers and report poor affinity. In direct contrast to Boothe et al., PIAS [4] uses PlanetLab-based measurements to claim that anycast flaps are relatively rare for the same anycasted DNS root-servers. Relative to the above, our study in this paper uses a significantly larger number of client vantage points, performs a more detailed analysis of affinity and proximity, and evaluates deployment strategies to improve the proximity offered by IP Anycast. In addition, we explore load-distribution due to IP Anycast and its behavior under failure.

## 3. Deployments Measured
An IP Anycast group is associated with an IP address (hereon referred to as the anycast address for the group), and servers join the group by advertising this address into the routing infrastructure. For an intra-domain anycast group with servers restricted to a single administrative domain, this advertisement is into the intra-domain routing protocol for the domain in question. For inter-domain anycast groups, each server advertises the anycast address into BGP. Despite the simplicity of the basic idea, the interaction with BGP and the involvement of multiple administrative domains raise several interesting questions regarding the behavior of inter-domain IP Anycast. This paper restricts itself to studying issues related to inter-domain IP Anycast. While we focus on IPv4 Anycast, our results should apply equally to IPv6 Anycast deployments.

Since clients can access an anycast group simply by sending packets to the IP address associated with the group, IP Anycast has been used for transparent replication of many services, including the DNS root-servers. For example, the F root-server deployment is currently comprised of 37 servers that form an IP Anycast group, and each server advertises the F root-server anycast prefix. The deployment is also associated with its own AS number (AS#) which serves as the origin AS for the anycast prefix. Thus, clients can access an F root-server by sending packets to an address in the prefix. In this paper, we evaluate three such currently operational deployments—two anycasted DNS root-servers and the AS112 anycast deployment. The anycasted AS112 servers are used to draw in reverse DNS queries to and for the link local address space (RFC1918 addresses—10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16). Since we have no control over these deployments, we refer to these as external deployments. Table 1 gives details for the external deployments, including the number of servers in each deployment at the time of our experiments.

| Name | Anycast Prefix | AS# | No. of Servers |
|------|---------------|-----|----------------|
| F root-server [45] | 192.5.5.0/24 | 3557 | 27 |
| J root-server [48] | 192.58.128.0/24 | 26415 | 13 |
| AS 112 [39] | 192.175.48.0/24 | 112 | 20 |

In practice, each "server" in these deployments is a cluster of hosts located behind some form of load-balancing device. For example, for the F root-server, hosts in each cluster form an intra-domain anycast group, and it is the server site's gateway routers that balance incoming traffic between them [2]. However, our focus on inter-domain anycast makes our measurements oblivious to this cluster-based deployment. Thus, for the purpose of this paper, each server site can be thought of as a single host.

The production-mode nature of these external deployments makes it difficult, if not impossible, to conduct controlled experiments such as injecting server failure or manipulating BGP advertisements. Therefore, we also deploy our own small-scale anycast service comprising five servers, as shown in Table 2.

| Server | Unicast Address | Host-Site | AS# | Upstream Provider |
|--------|-----------------|-----------|-----|--------------------|
| 1 | 128.84.154.99 | Cornell University | 26 | WCG |
| 2 | 12.155.161.153 | IR Berkeley | 2386 | ATT |
| 3 | 195.212.206.142 | IR Cambridge | 65476 | ATT-World |
| 4 | 12.108.127.148 | IR Pittsburgh | 2386 | ATT |
| 5 | 12.17.136.150 | IR Seattle | 2386 | ATT |

Each of these servers advertises the anycast prefix (204.9.168.0/22) through a BGP peering with their host-site onto the upstream provider. Note that IR stands for "Intel-Research."

## 4. Measurement Methodology
[This section will describe the detailed methodology used for the measurements, including the tools, techniques, and procedures employed.]

## 5. Proximity Measurements
[This section will present the results and analysis of the proximity measurements, including the methods used to measure latency and the key findings.]

## 6. Failover Measurements
[This section will present the results and analysis of the failover measurements, including the methods used to simulate and measure failover and the key findings.]

## 7. Affinity Measurements
[This section will present the results and analysis of the affinity measurements, including the methods used to measure consistency in server selection and the key findings.]

## 8. Load Distribution Measurements
[This section will present the results and analysis of the load distribution measurements, including the methods used to measure and manipulate load distribution and the key findings.]

## 9. Related Issues
[This section will discuss related issues, such as the implications of the findings for future anycast deployments and the broader context of IP routing and network design.]

## 10. Conclusion
[This section will summarize the key findings and their implications, and provide concluding remarks and future directions for research.]

---

**Table 3: Geographical Distribution of Clients**
| Region | No. of Clients | % of Total |
|--------|---------------|------------|
| North America | 54,827 | 75.0% |
| Central America | 1,344 | 1.8% |
| South America | 1,954 | 2.7% |
| Europe | 23,680 | 32.4% |
| Asia | 10,184 | 13.9% |
| S.E. Asia | 2,400 | 3.3% |
| Oceania | 5,071 | 6.9% |
| Africa | 792 | 1.1% |
| Arctic Region | 38 | 0.0% |
| Unknown | 0 | 0.0% |
| **Total** | **73,000** | **100.0%** |

Note: The total number of clients is greater than the sum of the regional counts due to overlapping regions and rounding.