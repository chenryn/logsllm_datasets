title:A measurement-based deployment proposal for IP anycast
author:Hitesh Ballani and
Paul Francis and
Sylvia Ratnasamy
A Measurement-based Deployment Proposal for IP Anycast
Hitesh Ballani
Cornell University
Ithaca, NY
Paul Francis
Cornell University
Ithaca, NY
Sylvia Ratnasamy
Intel Research
Berkeley, CA
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Despite its growing use in critical infrastructure services,
the performance of IP(v4) Anycast and its interaction with
IP routing practices is not well understood.
In this pa-
per, we present the results of a detailed measurement study
of IP Anycast. Our study uses a two-pronged approach.
First, using a variant of known latency estimation tech-
niques, we measure the performance of current commercially
operational IP Anycast deployments from a large number
(>20,000) of vantage points. Second, we deploy our own
small-scale anycast service that allows us to perform con-
trolled tests under diﬀerent deployment and failure scenar-
ios. To the best of our knowledge, our study represents the
ﬁrst large-scale evaluation of existing anycast services and
the ﬁrst evaluation of the behavior of IP Anycast under fail-
ure.
We ﬁnd that: (1) IP Anycast, if deployed in an ad-hoc
manner, does not oﬀer good latency-based proximity, (2) IP
Anycast, if deployed in an ad-hoc manner, does not provide
fast failover to clients, (3) IP Anycast typically oﬀers good
aﬃnity to all clients with the exception of those that ex-
plicitly load balance traﬃc across multiple providers, (4) IP
Anycast, by itself, is not eﬀective in balancing client load
across multiple sites. We thus propose and evaluate practi-
cal means by which anycast deployments can achieve good
proximity, fast failover and control over the distribution of
client load. Overall, our results suggest that an IP Anycast
service, if deployed carefully, can oﬀer good proximity, load
balance, and failover behavior.
Categories and Subject Descriptors: C.4 [Performance
of Systems]: Measurement techniques, Performance attrib-
utes.
General Terms: Measurement, Performance.
Keywords: IP Anycast, BGP, Proximity, Aﬃnity, Failover,
Load Distribution.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’06, October 25–27, 2006, Rio de Janeiro, Brazil.
Copyright 2006 ACM 1-59593-561-4/06/0010 ...$5.00.
1.
INTRODUCTION
IP Anycast [28] is an addressing mode in which the same
IP address is assigned to multiple hosts. Together, these
hosts form an IP Anycast group and each host is referred
to as an anycast server. Packets from a client destined to
the group address are automatically routed to the anycast
server closest to the client, where “closest” is in terms of the
metrics used by the underlying routing protocol. Since In-
ternet routing does not diﬀerentiate between multiple routes
to multiple hosts (as in IP Anycast) and multiple routes to
the same host (as in multihoming), IP Anycast is completely
backward compatible requiring no changes to (IPv4 or IPv6)
routers and routing protocols.
IP Anycast oﬀers an attractive primitive for service dis-
covery – the route-to-closest-server abstraction oﬀers reduced
access latency for clients, load-balancing across servers, and
network-level resilience to DDoS attacks while its implemen-
tation at the network layer allows these advantages to be
realized with no special conﬁguration at clients or servers
and with no dependence on higher-layer services such as
the DNS. While this potential has long been recognized
[7,26,28], it is mostly in recent years that IP Anycast has
gained in importance. This is in large part due to its use
in the critical DNS root-server deployment – six of the thir-
teen DNS root-servers have been transparently replicated
using IP Anycast and this deployment continues to grow
[18].
In addition however, IP Anycast is being used in
a growing variety of infrastructure services. For example,
IP Anycast is used to improve the performance of caching
DNS servers [27], for drawing in private address space DNS
queries as part of the AS-112 project [39], to discover ren-
dezvous points for multicast groups [21], as a transition
mechanism from IPv4 to IPv6 [19], for sinkholing DoS at-
tacks [16] and for redirection in commercial CDNs [40]. Fi-
nally, recent research eﬀorts have used IP Anycast to build
a proxy-based anycast service [4] and a next-generation ar-
chitecture deployment service [3,30].
However, despite its growing use in critical infrastructure
services, IP Anycast and its interaction with IP routing prac-
tices is not well understood. For example, the impact of any-
casting of DNS root-servers on clients that should, in theory,
access the closest server has not been analyzed in any detail.
Similarly, there has been no exploration of whether root-
server operators can control the load on individual servers
by manipulating their routing advertisements, nor of the
behavior of IP Anycast under server failure. Moreover, the
various applications of IP Anycast make diﬀerent assump-
tions about the underlying service. For example, the use of
IP Anycast in CDNs assumes client packets are routed to a
proximal CDN server and that the impact of a server failure
on clients is shortlived (i.e., clients are quickly routed to a
diﬀerent server). To gauge the eﬀectiveness of IP Anycast in
existing deployments as also the feasibility of future usage
scenarios, it is imperative to evaluate the performance of IP
Anycast.
A couple of root-server operators [6,12] have oﬀered valu-
able reports on the performance of their anycast deploy-
ments. These are probably the ﬁrst reports on the perfor-
mance of IP Anycast and represent the best source of data
on operational IP Anycast deployments from the point of
view of the anycast servers. However, the analysis is pre-
liminary (as the authors themselves state) and details of the
study are not published, nor is the data publicly available.
Drawing from these, this paper presents a detailed study of
inter-domain IP Anycast as measured from a large number
of vantage points. Speciﬁcally, our study seeks to answer
the following questions:
1. What kind failover properties does a typical IP Anycast
deployment oﬀer?
2. What kind of load distribution do existing IP Anycast
deployments see? Also, can the operator of an anycast
deployment control this distribution of client load?
3. Past studies [4,32] have reported that existing IP Any-
cast deployments may oﬀer poor latency-based prox-
imity; i.e., many clients may not routed to the server
closest in terms of latency. Using a larger number of
clients and anycast groups, we aim to conﬁrm and un-
derstand the reasons for this poor latency as well as
explore possible remedial measures.
4. Past studies [4,6,8,12,31] have presented conﬂicting re-
ports regarding the aﬃnity1 oﬀered by IP Anycast and
consequently, the ability to run stateful services of top
of anycast. We seek to measure, at scale, the aﬃnity
oﬀered by IP Anycast.
To explore these questions, we study four existing IP Any-
cast deployments including two anycasted DNS root-servers.
In terms of methodology, our study diﬀers from previous ef-
forts on two fronts:
1. We use a variant of the King [17] measurement tech-
nique to observe and evaluate IP Anycast deployments
from a very large (>20,000) number of vantage points.
To the best of our knowledge, this represents a two order
of magnitude increase in the number of vantage points
from which IP Anycast deployments have been actively
probed for evaluation.
2. We deploy our own small scale IP Anycast service for
controlled evaluation of anycast under diﬀerent deploy-
ment and failure scenarios. Performing such experi-
ments would be diﬃcult using commercial IP Anycast
deployments such as the DNS root-servers.
• We corroborate evidence from past studies indicating
that IP Anycast, by itself, does not oﬀer good latency-
based proximity. For example, for the 13 server J-
root deployment, we ﬁnd 8903 (≈40%) of the 22,281
measured clients are directed to a root-server that is
more than 100msec farther away from the closest server.
While the impact of inter-domain routing on end-to-end
path length has been well documented [34], we ﬁnd that
inter-domain routing metrics have an even more severe
impact on the selection of paths to anycast destinations.
• We propose and evaluate a practical deployment scheme
designed to alleviate the proximity concerns surround-
ing IP Anycast. Speciﬁcally, ensuring that an ISP that
provides transit to an anycast server has global presence
and is (geographically) well covered by such servers im-
proves the latency-based proximity oﬀered by the any-
cast deployment.
• We ﬁnd that IP Anycast is aﬀected by delayed routing
convergence and hence, clients using anycast services
may experience slow failover. However, our proposed
deployment scheme addresses this by reducing the scope
of routing convergence that follows a server failure and
hence, can ensure that clients failover at a fast rate. For
instance, we ﬁnd that in case of a server failure in an IP
Anycast deployment conforming to our proposal, a vast
majority (>95%) of clients can be re-routed to other
operational servers in less than 20 seconds.
• Through a much larger scale study as compared to past
eﬀorts, we ﬁnd that the anycasting of an IP preﬁx does
not have any unfavorable interactions with inter-domain
routing. Hence, IP Anycast oﬀers very good aﬃnity for
all but a very small fraction of clients. Using tempo-
ral clustering, we show that the poor aﬃnity observed
by this small fraction of clients can be attributed to
dynamic load-balancing mechanisms near them.
• We ﬁnd that a naive IP Anycast deployment does not
lead to an even distribution of client load across servers.
However, we also propose and evaluate the impact of op-
erators manipulating BGP advertisements at individual
anycast servers to control their load. Our results show
that such mechanisms can achieve coarse-grained load
balancing across anycast servers.
Overall, our measurements show that an IP Anycast service
can be deployed so as to provide a robust substrate oﬀering
good proximity and fast failover while allowing for coarse-
grained control over server load. In what follows, Section 2
reviews related measurement studies, Section 3 details the
IP Anycast deployments we measure in this paper while Sec-
tion 4 describes our measurement methodology. We describe
our proximity measurements in Section 5, failover measure-
ments in Section 6, aﬃnity measurements in Section 7 and
load distribution measurements in Section 8. Finally, we
discuss related issues in Section 9, and conclude with Sec-
tion 10.
The main results of this study are as follows:
2. RELATED MEASUREMENT STUDIES
1Aﬃnity measures the extent to which consecutive anycast
packets from a client are delivered to the same anycast
server.
An invaluable vantage point for measuring anycast deploy-
ments is at the anycast servers themselves. In recent pre-
sentations [6,12], the operators of the J and K root-servers
report on their analysis of client logs collected at their any-
cast servers. They present the observed distribution of client
load and aﬃnity. In terms of load distribution, both studies
report a skewed distribution of client load across their re-
spective deployments. With regard to aﬃnity, the J-root op-
erators report instances of clients that exhibit poor aﬃnity
and conjecture that anycast may not be suitable for stateful
services. By contrast, the K-root operators ﬁnd that most
of their clients experience very high aﬃnity.
Our study builds on these earlier reports. Using active
measurements from over 20,000 clients, we measure the aﬃn-
ity and load-distribution for our own small-scale anycast de-
ployment, explore the reasons behind the observed load and
aﬃnity, and evaluate techniques to control server load. In
addition to load and aﬃnity, we use active measurements
to evaluate the (latency) proximity seen at clients to four
diﬀerent anycast deployments. Finally, using our own de-
ployment, we study the behavior of IP Anycast under server
failure. As we describe in Section 6, our desire to use a large
number of client vantage points prevents us from measuring
the aﬃnity and load to the DNS root-server deployments.
However, for completeness, we did perform such measure-
ments from a smaller set of clients that we have direct access
to (i.e., PlanetLab nodes and approximately 200 publicly-
available traceroute servers). Since the results were consis-
tent with the larger-scale measurements over our own de-
ployment, we only present the latter here. The details of
the PlanetLab-based study can be found in [5].
We are aware of two recent eﬀorts that measure the per-
formance of IP Anycast using active probing from clients.
Sarat et. al. [31,32] use PlanetLab nodes [11] as vantage
points for evaluating the K-root, F-root and .ORG TLD de-
ployments. They measure proximity and aﬃnity and report
poor proximity and moderate-to-poor aﬃnity. Similarly,
using PlanetLab and approximately 200 volunteer nodes,
Boothe et al. [8] measure the aﬃnity oﬀered by the any-
casted DNS root-servers and report poor aﬃnity. In direct
contrast to Boothe et al., PIAS [4] uses PlanetLab-based
measurements to claim that anycast ﬂaps are relatively rare
for the same anycasted DNS root-servers. Relative to the
above, our study in this paper uses a signiﬁcantly larger
number of client vantage points, performs a more detailed
analysis of aﬃnity and proximity and evaluates deployment
strategies to improve the proximity oﬀered by IP Anycast.
In addition, we explore load-distribution due to IP Anycast
and its behavior under failure.
3. DEPLOYMENTS MEASURED
An IP Anycast group is associated with an IP address
(hereon referred to as the anycast address for the group)
and servers join the group by just advertising this address
into the routing infrastructure. For an intra-domain any-
cast group with servers restricted to a single administrative
domain, this advertisement is into the intra-domain routing
protocol for the domain in question. For inter-domain any-
cast groups, each server advertises the anycast address2 into
BGP. Despite the simplicity of the basic idea, the interac-
tion with BGP, the involvement of multiple administrative
2In practice, each server must advertise a preﬁx for a block
of addresses into BGP. This is the anycast preﬁx for the
group and servers are accessible through all addresses in the
preﬁx.
Name
Anycast
preﬁx
AS# No. of
servers
F root-server [45]
J root-server [48]
AS 112 [39]
192.5.5.0/24
192.58.128.0/24
192.175.48.0/24
3557
26415
112
27
13
20
Table 1: The three external IP Anycast deployments
that this paper evaluates.
Server
Unicast address
128.84.154.99
12.155.161.153
195.212.206.142
12.108.127.148
12.17.136.150
Host-Site
Host-site
Cornell University
IR Berkeley
IR Cambridge
IR Pittsburgh
IR Seattle
AS#
26
2386
65476
2386
2386
Upstream
provider
WCG
ATT
ATT-World
ATT
ATT
Table 2: The internal IP Anycast deployment compris-
ing of ﬁve servers. Each of these advertise the any-
cast preﬁx (204.9.168.0/22) through a BGP peering with
their host-site onto the upstream provider. Note that IR
stands for “Intel-Research”.
domains etc.
raise several interesting questions regarding
the behavior of inter-domain IP Anycast. This paper re-
stricts itself to studying issues related to inter-domain IP
Anycast. Also, while we focus on IPv4 Anycast, our results
should apply equally to IPv6 Anycast deployments.
Since clients can access an anycast group simply by send-
ing packets to the IP address associated with the group, IP
Anycast has been used for transparent replication of many
services including the DNS root-servers. For example, the F
root-server deployment is currently comprised of 37 servers
that form an IP Anycast group and each server advertises
the F root-server anycast preﬁx. The deployment is also
associated with its own AS number (AS#) which serves as
the origin AS for the anycast preﬁx. Thus, clients can ac-
cess a F root-server by sending packets to an address in
the preﬁx. In this paper, we evaluate three such currently
operational deployments - two anycasted DNS root-servers
and the AS112 anycast deployment.3 The anycasted AS112
servers are used to draw in reverse DNS queries to and for
the link local address space (RFC1918 addresses–10.0.0.0/8,
172.16.0.0/12 and 192.168.0.0/16). Since we have no control
over these deployments, we refer to these as external deploy-
ments. Table 1 gives details for the external deployments
including the number of the servers in each deployment at
the time of our experiments.4
In practice, each “server” in these deployments is a clus-
ter of hosts located behind some form of load-balancing
device. For example, for the F root-server, hosts in each
cluster form an intra-domain anycast group and it is the
server site’s gateway routers that balance incoming traﬃc
between them [2]. However, our focus on inter-domain any-
cast makes our measurements oblivious to this cluster-based
deployment. Thus, for the purpose of this paper, each server
site can be thought of as a single host.
The production-mode nature of these external deploy-
ments makes it diﬃcult, if not impossible, to conduct con-
trolled experiments such as injecting server failure, or ma-
3The choice of the anycast deployment to measure was lim-
ited by the need to know the unicast addresses of the indi-
vidual anycast servers for our experiments.
4The F root-server and the AS112 deployments have since
grown to 37 servers each.
Region
North America
Central America
South America
Europe
Asia
S.E. Asia
Oceania
Africa
Arctic Region
Unknown
Total
No. of clients % of Total
54.827
1.344
1.954
23.680
10.184
2.400
5.071
0.792
0.038