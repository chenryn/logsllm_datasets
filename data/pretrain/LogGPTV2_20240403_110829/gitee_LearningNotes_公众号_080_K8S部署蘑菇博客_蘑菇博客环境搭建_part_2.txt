kind: Service     #为default backend 创建一个service
metadata:
  name: default-http-backend
  namespace: kube-system
  labels:
    k8s-app: default-http-backend
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    k8s-app: default-http-backend
[root@k8s-master1 ingress]# kubectl apply -f default-backend.yaml
```
#### 1.3 创建nginx-ingress-controller
```shell
[root@k8s-master1 ingress]# cat nginx-ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  labels:
    k8s-app: nginx-ingress-controller
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
       k8s-app: nginx-ingress-controller
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-controller
    spec:
      # hostNetwork makes it possible to use ipv6 and to preserve the source IP correctly regardless of docker configuration
      # however, it is not a hard dependency of the nginx-ingress-controller itself and it may cause issues if port 10254 already is taken on the host
      # that said, since hostPort is broken on CNI (https://github.com/kubernetes/kubernetes/issues/31307) we have to use hostNetwork where CNI is used
      # like with kubeadm
      # hostNetwork: true #注释表示不使用宿主机的80口，
      terminationGracePeriodSeconds: 60
      hostNetwork: true  #表示容器使用和宿主机一样的网络
      serviceAccountName: nginx-ingress-serviceaccount #引用前面创建的serviceacount
      containers:   
      - image: 192.168.229.21/test/nginx-ingress-controller:0.20.0      #容器使用的镜像
        imagePullPolicy: IfNotPresent
        name: nginx-ingress-controller  #容器名
        readinessProbe:   #启动这个服务时要验证/healthz 端口10254会在运行的node上监听。 
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10  #每隔10做健康检查 
          timeoutSeconds: 1
        ports:
        - containerPort: 80  
          hostPort: 80    #80映射到80
#        - containerPort: 443
#          hostPort: 443
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
#        - --default-ssl-certificate=$(POD_NAMESPACE)/ingress-secret    #这是启用Https时用的
#      nodeSelector:  #指明运行在哪，此IP要和default backend是同一个IP
#        kubernetes.io/hostname: 10.3.1.17   #上面映射到了hostport80，确保此IP80，443没有占用.
# 
#      nodeName: node1
[root@k8s-master1 ingress]# kubectl apply -f nginx-ingress-controller.yaml
[root@k8s-master1 ingress-0.26]# kubectl get po -n kube-system |grep ingress
nginx-ingress-controller-86d8667cb7-mpndb   1/1     Running   0          29s
```
### 2.部署nfs存储
#### 2.1 部署nfs
```shell
1.部署nfs 动态扩缩容k8s-master1操作
yum install nfs-utils rpcbind -y
systemctl start nfs rpcbind  
systemctl enable nfs rpcbind 
mkdir -p /data/nfs  
chmod 777 /data/nfs
echo "/data/nfs/     192.168.229.0/24(rw,sync,no_root_squash,no_all_squash)" >> /etc/exports
[root@k8s-master1 nfs]# exportfs -arv
exporting 192.168.229.0/24:/data/nfs
[root@k8s-master1 nfs]# showmount -e localhost
参数:
sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性
async：将数据先保存在内存缓冲区中，必要时才写入磁盘
2.所有work节点安装 nfs-utils rpcbind
yum install nfs-utils rpcbind -y
systemctl start nfs  rpcbind
systemctl enable nfs  rpcbind
```
#### 2.2 创建rbac角色
```shell
[root@k8s-master1 nfs]# cat rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default   #根据实际环境设定namespace,下面类同
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default  #根据实际环境设定namespace,下面类同
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default   #根据实际环境设定namespace,下面类同
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default    #根据实际环境设定namespace,下面类同
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
[root@k8s-master1 nfs]# kubectl apply -f rbac.yaml 
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
```
#### 2.3 创建storageclass 
```shell
[root@k8s-master1 nfs]# cat class.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: fuseim.pri/ifs  #这里的名称要和provisioner配置文件中的环境变量PROVISIONER_NAME保持一致
parameters:
  archiveOnDelete: "false"
[root@k8s-master1 nfs]# kubectl apply -f class.yaml 
storageclass.storage.k8s.io/managed-nfs-storage created
```
#### 4.创建provisioner-nfspod
```shell
[root@k8s-master1 nfs]# cat nfs-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  namespace: default
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: 192.168.229.21/test/nfs-subdir-external-provisioner:v4.0.0
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs     #provisioner名称,请确保该名称与 nfs-StorageClass.yaml文件中的provisioner名称保持一致
            - name: NFS_SERVER   
              value: 192.168.229.3       #NFS Server IP地址   
            - name: NFS_PATH
              value: /data/nfs          #NFS挂载卷
      nodeName: k8s-node1   # 指定主机
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.229.3    #NFS Server IP地址
            path: /data/nfs     #NFS 挂载卷
[root@k8s-master1 nfs-storageclasses]# kubectl apply -f nfs-deployment.yaml
[root@k8s-master1 nfs-storageclasses]# kubectl get po -n kube-system |grep nfs
nfs-client-provisioner-9599579d7-vzx7j      1/1     Running   0          12s
nfs-client-provisioner-9599579d7-wd6b8      1/1     Running   0          12s
```
### 3.使用heml部署harbor仓库
```shell
[root@k8s-master1 images]# ls
chartmuseum-photon.tar  harbor-jobservice.tar   harbor.txt                redis-photon.tar
harbor-core.tar         harbor-portal.tar       notary-server-photon.tar  registry-photon.tar
harbor-db.tar           harbor-registryctl.tar  notary-signer-photon.tar  trivy-adapter-photon.tar
[root@k8s-master1 images]# cat harbor.txt 
chartmuseum-photon.tar
harbor-core.tar
harbor-db.tar
harbor-jobservice.tar
harbor-portal.tar
harbor-registryctl.tar
notary-server-photon.tar
notary-signer-photon.tar
redis-photon.tar
registry-photon.tar
trivy-adapter-photon.tar
# master节点和node节点都需要全部导入镜像
[root@k8s-master1 images]# for i in `ls ./*.tar`;do docker load -i $i;done
[root@k8s-master1 dev-k8s-ok]# cp helm-3.0/linux-amd64/helm /usr/bin/
[root@k8s-master1 dev-k8s-ok]# chmod +x /usr/bin/helm 
[root@k8s-master1 ingress-0.26]# helm  version
version.BuildInfo{Version:"v3.6.3", GitCommit:"d506314abfb5d21419df8c7e7e68012379db2354", GitTreeState:"clean", GoVersion:"go1.16.5"}
#修改storageclaas 动态创建pv名称
cat harbor-helm-master/values.yaml |grep "storageClass"
# Specify another StorageClass in the "storageClass" or set "existingClaim"
      # Specify the "storageClass" used to provision the volume. Or the default
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
# 创建harbor仓库
[root@k8s-master1 harbor-2.0]# kubectl create ns devops
[root@k8s-master1 harbor-2.0]# helm install paas harbor-helm-master -n devops
# helm uninstall paas -n devops	  # 卸载harbor仓库
# 导出模板命令
helm template  paas  ./harbor-helm-master -f ./harbor-helm-master/values.yaml
```
![image-20220303001306288](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303001306288.png)
```shell
修改docker配置文件，登录harbor仓库
[root@k8s-master1 mysql]# cat /etc/docker/daemon.json
{
"insecure-registries":["core.harbor.domain"],     # 添加这一行
"registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com"], 
"exec-opts":["native.cgroupdriver=systemd"],
 "log-driver":"json-file",
 "log-opts": {
  "max-size": "100m"
  },
 "storage-driver":"overlay2",
 "storage-opts": [
  "overlay2.override_kernel_check=true"
  ]
}
# 刷新配置，重启docker
[root@k8s-node1 images]# systemctl daemon-reload 
[root@k8s-node1 images]# systemctl restart docker
# 查看pod是否都正常启动
[root@k8s-master1 mysql]# kubectl get po -A |grep -v unn
# 配置本地hosts
[root@k8s-master1 mysql]# cat /etc/hosts
192.168.229.4   k8s-node1    core.harbor.domain
# 验证登录harbor仓库
[root@k8s-master1 mysql]# docker login core.harbor.domain
Username: admin
Password: Harbor12345
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store
Login Succeeded
```
![image-20220303003218934](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303003218934.png)
在本地配置hosts 对应harbor仓库的ingress
192.168.229.4   core.harbor.domain
Username: admin
Password: Harbor12345![image-20220303001446698](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303001446698.png)
新建一个项目地址：mogublog
![image-20220303002912308](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303002912308.png)
### 4.部署sts-MySQL单节点
```shell
1.建立主要工作目录
[root@k8s-master1 ~]# mkdir -p /root/dev-project/mysql   &&  cd /root/dev-project/mysql
2.导入上传MySQL镜像至harbor仓库
[root@k8s-master1 mysql]# docker load -i mysql.tar
[root@k8s-master1 mysql]# docker tag registry.cn-shenzhen.aliyuncs.com/mogublog/mysql:latest   core.harbor.domain/mogublog/mysql:latest
[root@k8s-master1 mysql]# docker push core.harbor.domain/mogublog/mysql:latest
3.编写mysql-sts.yaml文件
[root@k8s-master1 mysql]# cat mysql-dev.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-svc
  namespace: devops