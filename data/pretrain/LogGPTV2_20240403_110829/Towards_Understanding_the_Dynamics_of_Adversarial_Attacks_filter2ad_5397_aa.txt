title:Towards Understanding the Dynamics of Adversarial Attacks
author:Yujie Ji and
Ting Wang
POSTER: Towards Understanding the Dynamics of Adversarial
Attacks
Yujie Ji
Lehigh University
Bethlehem, PA, USA
PI:EMAIL
Ting Wang∗
Lehigh University
Bethlehem, PA, USA
PI:EMAIL
ABSTRACT
An intriguing property of deep neural networks (DNNs) is their
inherent vulnerability to adversarial inputs, which significantly hin-
der the application of DNNs in security-critical domains. Despite
the plethora of work on adversarial attacks and defenses, many im-
portant questions regarding the inference behaviors of adversarial
inputs remain mysterious. This work represents a solid step to-
wards answering those questions by investigating the information
flows of normal and adversarial inputs within various DNN models
and conducting in-depth comparative analysis of their discrimina-
tive patterns. Our work points to several promising directions for
designing more effective defense mechanisms.
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures; • Computing methodologies → Neural net-
works; • Mathematics of computing → Information theory;
KEYWORDS
adversarial sample; deep neural network; mutual information
ACM Reference Format:
Yujie Ji and Ting Wang. 2018. POSTER: Towards Understanding the Dynam-
ics of Adversarial Attacks. In 2018 ACM SIGSAC Conference on Computer and
Communications Security (CCS ’18), October 15–19, 2018, Toronto, ON, Canada.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3243734.3278528
1 INTRODUCTION
Recent years have witnessed the abrupt advances in deep learn-
ing [9], leading to breakthroughs in a number of long-standing
artificial intelligence tasks. However, designed to model highly non-
linear, non-convex functions, deep neural networks (DNNs) are
inherently vulnerable to adversarial inputs, which are maliciously
crafted samples to trigger target DNNs f to misbehave [18], such
as for a given benign input x, the attacker attempts to find the min-
imum perturbation r forcing f ’s misclassification of ˆx = x + r, i.e.,
minr f (x ) (cid:44) f (x + r ). With the increasing use of DNN-powered
systems in security-critical domains, adversaries have strong incen-
tives to manipulate such systems via adversarial inputs.
∗Contact Author
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5693-0/18/10.
https://doi.org/10.1145/3243734.3278528
1
The phenomena of adversarial inputs have attracted intensive
research from the security communities. Despite the plethora of
existing work, we still lack sufficient understanding of the crucial
properties of adversarial inputs. A number of important questions
remain mysterious, such as: (i) How are adversarial inputs crafted to
force DNNs to misclassify? (ii) How are adversarial inputs generated
by various attack models different in their underlying mechanisms?
(iii) How are existing defenses often vulnerable to adaptive attacks?
(iv) How are complicated DNNs more vulnerable to adversarial in-
put attacks than simple DNNs? (v) How are transferable adversarial
inputs different from non-transferable ones?
This work represents a solid step towards answering those key
questions. We take a route completely different from existing work:
instead of focusing on the static properties of adversarial inputs
from an input-centric perspective (i.e., whether a given adversarial
input can mislead the target DNN), we study the dynamic properties
of adversarial inputs from a DNN-centric perspective (i.e., how the
target DNN reacts to the given adversarial input).
2 INFORMATION FLOW MODEL
To understand the dynamic properties of adversarial inputs, we
measure their information flows within various DNNs and conduct
in-depth comparative studies of their patterns.
2.1 Mutual Information
Consider a DNN f comprising a sequence of K layers, where the
output of k-th layer consists of nk feature maps {m(k )
}nk
i =1. Let x be
a given input to f . As x is of multiple channels (e.g., RGB), we also
i }ns
consider x as a set of ns feature maps {ms
i =1. To understand x’s
dynamic properties, i.e., how f reacts to x, we quantify x’s informa-
tion flow going through f , via measuring the mutual information
(MI) between each feature map and x.
i
Specifically, we treat each feature map m as a discrete distribu-
tion: Let vmin and vmax respectively be the minimum and maximum
values in m. We divide the interval [vmin, vmax] evenly into B buck-
ets and replace each value v in m with its bucket ID: bid(v) =
⌈B(v − vmin)/(vmax − vmin)⌉. We then populate an nk × ns matrix
S (k ) with the i, j-th element S (k )
. More-
ij
over, to obtain a complete view of x’s information flows, we also
measure the MI of each feature map at an intermediate layer and
the output of f ’s last conv layer (which consists of nt feature maps
k =1). We populate an nk × nt matrix T (k ), with its i, j-th ele-
}nt
{mt
ment T (k )
. We refer to S (k ) and T (k ) as
ij
the source and target MI matrices of the k-th layer.
being the MI of m(k )
being the MI of m(k )
and ms
j
(k )
i
and mt
j
i
)}k, where µ (k )
s
)}k, where µ (k )
t
is the mean of S (k ).
is the mean of T (k ).
2.2 Information Paths
Armed with S (k ) and T (k ), we depict the “information paths” (IPs)
from a given input x to its output y in a layer-wise manner, i.e.,
how the feature maps at each layer capture the information in x
and transform it towards y. Specifically, we construct a set of IPs:
• Input information path (IIP) quantifies the relevance of the feature
maps at each layer with the input, defined as the sequence of
{(k, µ (k )
s
• Output information path (OIP) quantifies the relevance of the fea-
ture maps at each layer with the output, defined as the sequence
of {(k, µ (k )
t
• Input-Output contrast (IOC) correlates the input and output infor-
mation paths at each layer, defined as a sequence of {(µ (k )
)}k.
Note that our approach is inspired by the theory of information
bottleneck methods [17, 19]. However, different from existing work,
which treats inputs as individual data points, we consider each
input as a discrete distribution, which allows us to investigate the
information flow at the level of individual inputs.
2.3 Aggregated Information Paths
Further, we devise the model of aggregated IPs to summarize the IPs
of a set of similar inputs (e.g., adversarial inputs generated by the
same attack). We consider each MI measure (e.g., µ (k )
s ) as a random
variable and assume that a collection of such random variables
indexed by k follow a multivariate normal distribution; each IP is
thus a random sample from a Gaussian process [16]. We use the
mean of the Gaussian process to represent their aggregated IP.
s , µ (k )
t
3 ANALYSIS
Equipped with the aforementioned measurement tools, we conduct