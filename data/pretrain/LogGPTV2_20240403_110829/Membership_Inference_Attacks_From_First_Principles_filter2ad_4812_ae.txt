attack, by training multiple OUT models and then performing
an exact one-sided hypothesis test. Speciﬁcally, to target an
FPR of α, their attack sets each example’s decision threshold
so that an α-fraction of the measured OUT losses for that
example lie below the threshold.
The critical difference between our attack and these prior
attacks is that we use a more efﬁcient parametric approach,
that models the distribution of losses as Gaussians. Since
Sablayrolles et al. [56] and Watson et al. [68] only measure
the means of the distributions, the attacks are sub-optimal if
different samples’ loss distributions have very different scales
and spreads (c.f. Figure 4). The attacks of Long et al. [37]
and Ye et al. [69] take into account the full distribution of
OUT losses, but have difﬁculties extrapolating to low FPRs
due to the lack of a parametric assumption. By design, the
exact test of Ye et al. [69] can at best target an FPR of 1/N
with N shadow models. It is thus inapplicable in the setting
we consider here (256 shadow models, and a target FPR of
0.1%). Long et al. [37] extrapolate to the tails of the empirical
loss distribution using cubic splines, which easily overﬁt and
diverge outside of their support.
D. Membership inference and overﬁtting
To better understand the relationship between overﬁtting
and vulnerability to membership inference attacks, Figure 7
plots various models’ train-test gap (that is, their train accuracy
minus their test accuracy) versus our attack’s TPR at an FPR
of 0.1%. We train CNN models and Wide ResNets (WRN)
of various sizes on CIFAR-10, with different optimizers and
data augmentations (see Section VI-E for details). Each point
represents one training conﬁguration for the target model.
While there is an overall trend that overﬁt models (those
with higher train-test gap) are more vulnerable to attack, we
do ﬁnd examples of models that have identical train-test gaps
but are 100× more vulnerable to attack. In Figure 16 in the
Appendix we further plot the attack TPR as a function of the
test accuracy of these models. There, we observe a clear trend
that more accurate models are more vulnerable to attack.
VI. ABLATION STUDY
Our attack has a number of moving pieces that are con-
nected in various ways; in this section we investigate how
these pieces come together to reach such high accuracy at low
false-positive rates. We exclusively use CIFAR-10 for these
ablation studies as it is the most popular image classiﬁcation
dataset and is the hardest datasets we have considered; we give
results for additional datasets in the Appendix.
A summary of our analysis is presented in Table II. The
baseline LOSS attack achieves a true-positive rate of 0% at a
false positive rate of 0.1% (as shown previously in Figure 2b).
If we do not use per-example thresholds, this basic attack can
only be marginally improved by properly scaling the loss and
issuing multiple queries to the target model.
By incorporating per-example thresholds obtained by es-
timating the distributions ˜Qin and ˜Qout as in [56], the attack
success rate increases to 1.7%—about one-order-of-magnitude
better than chance. By ensuring that we appropriately re-scale
the model losses (explored in detail in Section VI-A) and
ﬁtting the re-scaled losses with Gaussians (see Section VI-B),
we increase the attack success rate by a factor of 3.3×. Finally,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
91905
0.00.10.20.30.40.5TrainTestGap10−410−310−210−1100TPR@0.1%FPRCNN1,CNN2,CNN4CNN8,CNN16CNN32,CNN64WRN28-1WRN28-2WRN28-10Fig. 8: The best scoring metrics ensure the output distribution
is approximately Gaussian, and the worst metrics are not easily
modeled with a standard distribution (see Figure 4).
we can nearly double the attack success rate by evaluating the
target model on the same data augmentations as used during
training, as we will show in Section VI-C.
We also perform the same ablation but with the ofﬂine
variant of our attack. Here, if we start with the attack of
Watson et al. [68] to reach a 1.3% true-positive rate; adding
logit scaling, Gaussian likelihood, and multiple queries yields
an attack that is nearly as strong as our full attack (TPR of
7.1% versus 8.4% at an FPR of 0.1%).
A. Logit scaling the loss function
The ﬁrst step of our attack projects the model’s conﬁdences
to a logit scale to ensure that the distributions that we work
with are approximately normal. Figure 8 compares perfor-
mance of our attack for various choices of statistics that we
can ﬁt using shadow models. Recall that we deﬁned our neural
network function f (x) to denote the evaluation of the model
along with a ﬁnal softmax activation function; we use z(x) to
denote the pre-softmax activations of the neural network.
As expected, we ﬁnd that using the model’s conﬁdence
f (x)y ∈ [0, 1], or its logarithm (the cross-entropy loss), leads
to poor performance of the attack since these statistics do not
behave like Gaussians (recall from Figure 4).
Our logit rescaling performs best, but the exact numerical
1−p ) matters.
computation of the logit function φ(p) = log( p
We consider two mathematically equivalent variants:
φunstable = log(f (x)y) − log(1 − f (x)y)
φstable = log(f (x)y) − log
f (x)y(cid:48) .
(cid:88)
y(cid:48)(cid:54)=y
We ﬁnd that the second version is more stable in practice,
when the model’s conﬁdence is very high, f (x)y ≈ 1 (we
compute all logarithms as log(x + ) for a small  > 0).
Note that this second stable variant requires access to the full
vector of model conﬁdences f (x) ∈ [0, 1]n rather than just the
conﬁdence of the predicted class.
Fig. 9: Attack success rate increases as the number of shadow
models increases, with the beneﬁt eventually tapering off.
When fewer than 64 models are used, it is better to estimate
the variance of the model conﬁdence as a global parameter
instead of computing it on a per-example basis.
If the adversary can query the model to obtain the unnor-
malized features z(x) (i.e., the outputs of the model’s last layer
before the softmax function), a hinge loss performs similarly
(cid:96)Hinge(x, y) = z(x)y − max
y(cid:48)(cid:54)=y
To see why this is the case, observe that
z(x)y(cid:48) .
(cid:88)
φ(f (x)y) = log(f (x)y) − log
y(cid:48)(cid:54)=y
= z(x)y − LogSumExp
y(cid:48)(cid:54)=y
f (x)y(cid:48)
z(x)y(cid:48) ,
where the LogSumExp function is a smooth approximation to
the maximum function. When the features z(x) are available,
we recommend using the hinge loss as its computation is
numerically simpler than that of the logit-scaled conﬁdence.
We note that the different attack variants we consider here
lead to orders-of-magnitude differences in attack performance
at low false-positive rates—even though all variants achieve
similar AUC scores (68–72%). This again highlights the
importance of carefully designing attacks, and of measuring
attack performance at low false-positive rates rather than on
average across the entire ROC curve.
The choice of an appropriate loss function can also have a
major impact on previous MIAs. For example, for the attack of
Watson et al. [68] (which scales the model’s loss by the mean
loss of OUT models not trained on the example, µout(x, y))
applying logit scaling nearly quadruples the attack’s true-
positive rate at an FPR of 0.1% (see Table II).
B. Gaussian distribution ﬁtting
Like other
shadow models membership inference at-
tacks [60], our attack requires that we train enough models
to accurately estimate the distribution of losses. It is thus
desirable to minimize the number of shadow models that are
necessary. However, most prior works in Table I that rely on
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
101906
10−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRatef(x)y(conﬁdence)log(f(x)y)(CEloss)φ(f(x))y)(logitscale,unstable)φ(f(x))y),(logitscale,stable)z(x)y(outputfeature)z(x)y−max(z(x)y0)(Hinge)51225612864321684Numberofshadowmodels0.000.020.040.060.080.100.12TPR@0.1%FPRGaussianLRTwithper-examplevarianceGaussianLRTwithglobalvariancenon-parametric(Sablayrollesetal.)Queries
1 (no augmentations)
2 (mirror)
18 (mirror + shifts)
162 (mirror + shifts)
TPR @ FPR
0.1% 0.001%
5.6%
1.0%
1.8%
7.5%
2.2%
8.4%
8.4%
2.2%
TABLE III: Querying on augmented versions of the image
doubles the true-positive rate at low false-positive rates, with
most beneﬁts given by just two queries.
shadow models do not analyze this tradeoff and report results
only for a ﬁxed number of shadow models [37, 56, 60].
Figure 9 displays our online attack’s TPR at a ﬁxed FPR
of 0.1%, as we vary the number of shadow models (half IN
and half OUT). Training more than 64 shadow models pro-
vides diminishing beneﬁts, but the attack deteriorates quickly
with fewer models—due to the difﬁculty of ﬁtting Gaussian
distributions on a small number of data points.
With a small number of shadow models, we can improve the
attack considerably by estimating the variances σ2
out of
model conﬁdences in Algorithm 1 globally rather than for each
individual example. That is, we still estimate the means µin and
µout separately for each example, but we estimate the variance
out) over the shadow models’ conﬁdences
in (respectively σ2
σ2
on all training set members (respectively non-members).
in and σ2
For a small number of shadow models (< 64), estimating a
global variance outperforms our general attack that estimates
the variance for each example separately. For a larger number
of models, our full attack is stronger: with 1024 shadow
models for example, the TPR decreases from 8.4% to 7.9%
by using a global variance.
C. Number of queries
Models are typically trained to minimize their loss not
only on the original training example, but also on augmented
versions of the example. It therefore makes sense to perform
membership inference attacks on the augmented versions of
the example that may have been seen during training. Results
of this analysis are presented in Table III. There are 162
potential augmentations of each training image for our CIFAR-
10 model (2× 9× 9, computed by either horizontally ﬂipping
the image or not, and shifting the image by up to ±4 pixels
in each height or width). We ﬁnd that querying on just 2
augmentations gives most of the beneﬁt, with increasing to
18 queries performing identically to all 162 augmentations.
D. Disjoint datasets
In our experiments so far, we trained both the target models
and the adversary’s shadow models by subsampling from a
common dataset. That is, we use a large dataset Dattack (e.g.,
the entire CIFAR-10 dataset) to train shadow models, and the
target model’s training set Dtrain is some (unknown) subset of
this dataset. This setup favors the attacker, as the training sets
of shadow models and the target model can partially overlap.
Fig. 10: The attack’s success rate on CINIC-10 remains un-
changed when the training sets of shadow models are sampled
from a dataset Dattack that is disjoint from the target model’s
training set Dtrain. The attack’s performance does decrease
when the two datasets are sampled from different distributions.
In a real attack, the adversary likely has access to a dataset
Dattack that is disjoint from the training set Dtrain. We now
show that this more realistic setup has only a minor inﬂuence
on the attack’s success rate.
For this experiment, we use the CINIC-10 dataset [8]. This
dataset combines CIFAR-10 with an additional 210k images
taken from ImageNet that correspond to classes contained
in CIFAR-10 (e.g., bird/airplane/truck etc). We train a target
model and 128 shadow models (OUT models only) each on
50,000 points. We compare three attack setups:
2) The shadow models’ training sets have no overlap with
1) The shadow models’ training sets are sampled from the
full CINIC-10 dataset. This is the same setup as in all
our previous experiments, where Dtrain ⊂ Dattack.
the target model, i.e., Dtrain ∩ Dattack = ∅.
3) The target model is trained on CIFAR-10, while the
attacker trains shadow models on the ImageNet portion
of CINIC-10. There is thus a distribution shift between
the target model’s dataset and the attacker’s dataset.
Figure 10 shows that our attack’s performance is not inﬂu-
enced by an overlap between the training sets of the target
model and shadow models. The attack success is unchanged
when the attacker uses a disjoint dataset. A distribution shift
between the training sets of the target model and shadow
models does reduce the attack’s TPR. Surprisingly, the attack’s
AUC is much higher when there is a distribution shift—we
leave an explanation of this phenomenon to future work.
E. Mismatched training procedures
We now explore how our attack is affected if the attacker
does not know the exact training procedure of the target model.
We train models with various architectures, optimizers, and
data augmentations to investigate the attack’s performance
when the adversary guesses each of these incorrectly. For
each attack, we train 64 shadow models and use our online
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
111907
10−410−310−210−1100FalsePositiveRate10−410−310−210−1100TruePositiveRateDtrain⊂Dattackauc=0.692Dtrain∩Dattack=∅auc=0.682Dtrain6=Dattackauc=0.783(a) Vary model architecture.
(b) Vary training optimizer.
(c) Vary data augmentation.
Fig. 11: Our attack succeeds when the adversary is uncertain of the target model’s training setup. We vary the target model’s
architecture (a), the training optimizer (b) and the data augmentation (c), as well as the adversary’s guess of each of these
properties when training shadow models. The attack performs best when the adversary guesses correctly (black-lined markers).
attack variant with a global estimate of the variance (see
Section VI-B). Figure 11 summarizes our results at a ﬁxed
FPR of 0.1%.
In Figure 11a, we vary the target model’s architecture. We
study three CNN models (with 16, 32 and 64 convolutional
ﬁlters), and three Wide ResNets (WRN) with width 1, 2 and
10. All models are trained with SGD with momentum and
with random augmentations. Our attack performs best when
the attacker trains shadow models of the same architecture as
the target model, but using a similar model (e.g., a WRN28-1
instead of a WRN28-2) has a minimal effect on the attack.
Moreover, we ﬁnd that for both the CNN and WRN model
families, larger models are more vulnerable to attacks.
In Figure 11b we ﬁx the architecture to a WRN28-10,
and vary the training optimizer: SGD, SGDM (SGD with
momentum) or Adam. For both the defender or the attacker,
the choice of optimizer has minimal impact on the attack.
Finally, in Figure 11c we ﬁx the architecture (WRN28-10)
and optimizer (SGDM) and vary the data augmentation used
for training: none, mirroring, mirroring + shifts, mirroring +
shifts + cutout. The attacker’s guess of the data augmentation
is used both to train shadow models, and to create additional
queries for the attack. We ﬁnd that correctly guessing the target
model’s data augmentation has the highest impact on attack
performance. Models trained with stronger augmentations are
harder to attack, as these models are less overﬁt.
VII. ADDITIONAL INVESTIGATIONS
We now pivot from evaluating our attack to using our attack
as a tool to better understand memorization in real models
(§VII-A) and why memorization occurs (§VII-B).
A. Attacking real-world models