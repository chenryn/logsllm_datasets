### 攻击方法与比较

我们的攻击方法通过训练多个 OUT 模型并执行精确的单侧假设检验来实现。具体来说，为了达到目标的假阳性率（FPR）α，攻击为每个样本设置决策阈值，使得该样本测量的 OUT 损失中 α 比例低于该阈值。

我们的攻击与先前攻击的关键区别在于，我们采用了一种更高效的参数化方法，将损失分布建模为高斯分布。Sablayrolles 等人 [56] 和 Watson 等人 [68] 仅测量了分布的均值，因此如果不同样本的损失分布具有非常不同的尺度和范围（参见图 4），这些攻击是次优的。Long 等人 [37] 和 Ye 等人 [69] 的攻击考虑了 OUT 损失的完整分布，但由于缺乏参数假设，在低 FPR 下难以外推。Ye 等人 [69] 的精确测试在 N 个影子模型下最多只能达到 1/N 的 FPR，因此在我们考虑的设置（256 个影子模型，目标 FPR 为 0.1%）中不适用。Long 等人 [37] 使用三次样条外推经验损失分布的尾部，这容易过拟合并在支持范围之外发散。

### 成员推理与过拟合

为了更好地理解过拟合与成员推理攻击脆弱性之间的关系，图 7 绘制了各种模型的训练-测试差距（即训练准确率减去测试准确率）与我们的攻击在 FPR 为 0.1% 时的真阳性率（TPR）。我们在 CIFAR-10 上训练了不同大小的 CNN 模型和宽残差网络（WRN），使用不同的优化器和数据增强（详见第六部分-E）。每个点代表目标模型的一种训练配置。

尽管总体趋势是过拟合模型（具有较高的训练-测试差距）更容易受到攻击，但我们确实发现有些模型具有相同的训练-测试差距但对攻击的脆弱性相差 100 倍。在附录中的图 16 中，我们进一步绘制了攻击 TPR 作为这些模型测试准确率的函数。在那里，我们观察到一个明显趋势：更准确的模型更容易受到攻击。

### 消融研究

我们的攻击有许多相互关联的部分；在本节中，我们探讨这些部分如何共同作用以在低假阳性率下达到如此高的准确率。我们仅使用 CIFAR-10 进行这些消融研究，因为它是最流行的图像分类数据集，并且是我们考虑的最困难的数据集之一；附录中提供了其他数据集的结果。

表 II 总结了我们的分析。基线 LOSS 攻击在 FPR 为 0.1% 时的 TPR 为 0%（如图 2b 所示）。如果不使用每样本阈值，这种基本攻击只能通过适当缩放损失并多次查询目标模型进行微小改进。

通过结合估计的分布 ˜Qin 和 ˜Qout 来获得每样本阈值 [56]，攻击成功率提高到 1.7%，比随机猜测好一个数量级。通过确保适当重新缩放模型损失（详细见第六部分-A）并将重新缩放的损失拟合为高斯分布（见第六部分-B），我们可以将攻击成功率提高 3.3 倍。最后，通过在相同的数据增强下评估目标模型，我们可以将近乎翻倍攻击成功率（详见第六部分-C）。

我们还进行了离线变体的相同消融。从 Watson 等人 [68] 的攻击开始，可以达到 1.3% 的 TPR；添加对数缩放、高斯似然和多次查询后，攻击强度几乎与我们的完整攻击相当（TPR 为 7.1% 对比 8.4%，FPR 为 0.1%）。

#### A. 损失函数的对数缩放

我们攻击的第一步是将模型的置信度投影到对数尺度，以确保我们处理的分布近似于正态分布。图 8 比较了我们攻击在不同统计量下的性能。回忆一下，我们定义神经网络函数 f(x) 表示模型的评估结果加上最终的 softmax 激活函数；我们用 z(x) 表示神经网络的 pre-softmax 激活。

正如预期的那样，我们发现使用模型的置信度 f(x)y ∈ [0, 1] 或其对数（交叉熵损失）会导致攻击性能较差，因为这些统计量不像高斯分布（参见图 4）。

我们的对数重缩放表现最好，但对数函数 φ(p) = log( p / (1 - p)) 的精确数值计算很重要。我们考虑两种数学等价的变体：

- φunstable = log(f(x)y) - log(1 - f(x)y)
- φstable = log(f(x)y) - log(∑y' ≠ y f(x)y')

我们发现第二种版本在实践中更稳定，当模型的置信度非常高时，f(x)y ≈ 1（我们计算所有对数时使用 log(x + ε)，其中 ε 是一个小的正值）。注意，这个稳定的变体需要访问完整的置信度向量 f(x) ∈ [0, 1]^n 而不仅仅是预测类别的置信度。

图 9 显示了随着影子模型数量的增加，攻击成功率逐渐增加，但收益最终趋于平缓。当使用的影子模型少于 64 个时，全局估计模型置信度的方差比逐样本计算更好。

如果对手可以查询模型以获取未归一化的特征 z(x)（即模型最后一层在 softmax 函数之前的输出），那么铰链损失表现相似：

\[ \ell_{\text{Hinge}}(x, y) = z(x)_y - \max_{y' \neq y} z(x)_{y'} \]

要了解为什么这种情况成立，请观察：

\[ \phi(f(x)_y) = \log(f(x)_y) - \log\left(\sum_{y' \neq y} f(x)_{y'}\right) = z(x)_y - \text{LogSumExp}\left(\sum_{y' \neq y} z(x)_{y'}\right) \]

其中 LogSumExp 函数是对最大函数的光滑近似。当特征 z(x) 可用时，我们建议使用铰链损失，因为其计算比对数缩放的置信度更简单。

我们注意到这里考虑的不同攻击变体在低假阳性率下的攻击性能差异很大，即使所有变体都实现了类似的 AUC 分数（68-72%）。这再次强调了精心设计攻击的重要性，并且在低假阳性率下而不是在整个 ROC 曲线上平均测量攻击性能的重要性。

选择适当的损失函数也对之前的 MIAs 有重大影响。例如，对于 Watson 等人 [68] 的攻击（通过 μout(x, y) 缩放模型的损失），应用对数缩放几乎可以使 FPR 为 0.1% 时的 TPR 提高四倍（见表 II）。

#### B. 高斯分布拟合

与其他影子模型成员推理攻击 [60] 类似，我们的攻击需要训练足够的模型来准确估计损失分布。因此，最小化所需的影子模型数量是有利的。然而，表 I 中大多数依赖影子模型的先前工作没有分析这种权衡，并且只报告了固定数量影子模型的结果 [37, 56, 60]。

图 9 显示了在线攻击在 FPR 为 0.1% 时的 TPR，随着影子模型数量的变化。训练超过 64 个影子模型的收益递减，但如果影子模型数量较少，则由于难以在少量数据点上拟合高斯分布，攻击效果迅速恶化。

对于少量影子模型，我们可以通过全局估计模型置信度的方差 σ^2_in 和 σ^2_out 来显著改进攻击，而不是为每个样本单独估计。也就是说，我们仍然为每个样本分别估计均值 μ_in 和 μ_out，但我们在所有训练集成员（非成员）上估计方差 σ^2_in（σ^2_out）。

对于少量影子模型（< 64），估计全局方差优于我们的一般攻击，后者为每个样本单独估计方差。对于大量模型，我们的完整攻击更强：例如，使用 1024 个影子模型时，通过使用全局方差，TPR 从 8.4% 降低到 7.9%。

#### C. 查询次数

模型通常不仅在原始训练样本上，还在增强版本的样本上最小化损失。因此，对可能在训练过程中看到的增强版本样本进行成员推理攻击是有意义的。表 III 展示了这一分析的结果。对于我们的 CIFAR-10 模型，每个训练图像有 162 种潜在的增强（2× 9× 9，通过水平翻转图像或不翻转，并在高度或宽度上移动 ±4 像素）。我们发现，仅查询 2 种增强就获得了大部分收益，增加到 18 次查询的效果与所有 162 种增强相同。

#### D. 不相交的数据集

在我们迄今为止的实验中，我们通过从一个共同的数据集中抽取子集来训练目标模型和对手的影子模型。也就是说，我们使用一个大型数据集 Dattack（例如，整个 CIFAR-10 数据集）来训练影子模型，而目标模型的训练集 Dtrain 是该数据集的某个（未知）子集。这种设置有利于攻击者，因为影子模型和目标模型的训练集可以部分重叠。

图 10 显示，当影子模型的训练集从与目标模型训练集 Dtrain 不相交的数据集 Dattack 中采样时，CINIC-10 上的攻击成功率保持不变。当两个数据集来自不同的分布时，攻击性能会下降。

在这个实验中，我们使用 CINIC-10 数据集 [8]。该数据集结合了 CIFAR-10 和从 ImageNet 中提取的额外 210k 张图像，这些图像对应于 CIFAR-10 中包含的类别（例如，鸟/飞机/卡车等）。我们训练了一个目标模型和 128 个影子模型（只有 OUT 模型），每个模型使用 50,000 个点。我们比较了三种攻击设置：

1. 影子模型的训练集从完整的 CINIC-10 数据集中采样。这是我们在所有先前实验中的相同设置，其中 Dtrain ⊂ Dattack。
2. 影子模型的训练集与目标模型没有重叠，即 Dtrain ∩ Dattack = ∅。
3. 目标模型在 CIFAR-10 上训练，而攻击者在 CINIC-10 的 ImageNet 部分上训练影子模型。因此，目标模型的数据集和攻击者的数据集之间存在分布偏移。

图 10 显示，我们的攻击性能不受目标模型和影子模型训练集重叠的影响。当攻击者使用不相交的数据集时，攻击成功不变。目标模型和影子模型训练集之间的分布偏移确实降低了攻击的 TPR。令人惊讶的是，当存在分布偏移时，攻击的 AUC 更高——我们将这种现象的解释留给未来的工作。

#### E. 训练过程不匹配

我们现在探讨如果攻击者不知道目标模型的确切训练过程，我们的攻击会受到什么影响。我们训练了具有不同架构、优化器和数据增强的模型，以调查当攻击者猜错这些属性时攻击的表现。每次攻击中，我们训练 64 个影子模型，并使用我们的在线攻击变体（见第六部分-B）。图 11 在 FPR 为 0.1% 时总结了我们的结果。

在图 11a 中，我们改变了目标模型的架构。我们研究了三个 CNN 模型（具有 16、32 和 64 个卷积滤波器）和三个宽残差网络（WRN），宽度分别为 1、2 和 10。所有模型都使用 SGD 动量和随机增强进行训练。我们的攻击在攻击者训练与目标模型相同架构的影子模型时表现最佳，但使用类似模型（例如，WRN28-1 而不是 WRN28-2）对攻击的影响很小。此外，我们发现对于 CNN 和 WRN 模型家族，更大的模型更容易受到攻击。

在图 11b 中，我们将架构固定为 WRN28-10，并改变训练优化器：SGD、SGDM（带有动量的 SGD）或 Adam。无论是防御者还是攻击者，优化器的选择对攻击的影响都很小。

最后，在图 11c 中，我们将架构（WRN28-10）和优化器（SGDM）固定，并改变用于训练的数据增强：无、镜像、镜像+位移、镜像+位移+cutout。攻击者的猜测既用于训练影子模型，也用于创建额外的查询。我们发现正确猜测目标模型的数据增强对攻击性能的影响最大。使用更强增强训练的模型更难攻击，因为这些模型不太过拟合。

### 进一步的研究

我们现在从评估我们的攻击转向使用我们的攻击作为一种工具，以更好地理解真实模型中的记忆化（第七部分-A）以及记忆化发生的原因（第七部分-B）。

#### A. 攻击真实世界模型