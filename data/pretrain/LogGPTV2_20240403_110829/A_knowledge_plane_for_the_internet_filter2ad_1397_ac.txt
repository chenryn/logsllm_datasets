say 
ROUTING_PREFERENCE(low-cost  links),  and  an  end-user’s 
machine might say FIX(low bandwidth). Again, the KP may be able 
to resolve some of these problems, and might learn over time when 
it is safe for it to act on its own, and when it must kick the problem 
back to the relevant humans in meaningful terms. (This example, by 
the way, illustrates why the KP must be seen as a unified system, not 
as separate systems for fault management and for configuration.) 
The  configuration  task  is  not  something  that  happens  once  at  the 
turn-up  of  the  network.  It  should  be  something  that  is  happening 
constantly,  looking  at  changing  network  conditions,  application 
demands,  and  changing  constraints.  It  is  also  a  task  that  can  run  
“recursively”.  A  global  network  is  not  built  top  down.  It  is  built 
bottom up, region by region. Each region will first configure itself 
network  manager  might 
parties. 
The 
using  its  locally  specified  goals  and  constraints.  But  when  two 
regions  then  connect,  there  may  be  further  constraints  that  one 
imposes  on  the  other.  So  a  provider  network  might  say  to  a 
subscriber  network:  NO_MULTICAST.  This  might  cause  the 
subscriber  network  to  change  some  of  its  internal  organization, 
disable some end-user applications, and so on.  
Support for overlay networks: If the KP has enough information to 
configure the network itself, that information can also be useful to 
applications  that  are  configuring  themselves.  For  example,  we  are 
increasingly seeing the development of application-specific overlay 
networks on the Internet.   Each overlay network uses edge-based 
mechanisms to evaluate the performance of different possible paths 
through the Internet, and seeks to build a set of transport paths that 
effectively route application packets through what appears to be the 
part of the Internet best suited to the application’s needs.   Currently, 
application  networks  must  probe  the  Internet,  because  there  is  no 
mechanism for them to learn about the capabilities of the network 
core.  The KP would be in a position to aggregate application- and 
network-derived  knowledge  about  network  performance,  offer 
applications  better  information  about  the  network  than  they  could 
learn by probing, and access to control points whose behavior could 
be  modified  to  help  better  meet  the  applications’  needs.    The  KP 
thus enables per-application control over traffic without the need to 
build per-application infrastructure throughout the network. 
 Knowledge-enhanced  intrusion  detection:  There  are  a  number  of 
projects  (and  a  number  of  products)  that  perform  some  sort  of 
analysis  to  detect  network  intrusions.  In  general  they  look  for 
patterns in data observed somewhere in the network.  The current 
generation  of  these  tools  trigger  both  false  positives  and  false 
negatives. It has been hypothesized that a next generation of tools 
for  intrusion  detection  will  require  that  observations  from  several 
points  in  the  network will have to be correlated, in order to get a 
more robust and useful signal. The development of the knowledge 
plane  provides  a  basis  to  implement  this  data  gathering  and 
correlation.  
4.  Knowledge Plane Architecture 
Previous sections of this paper have outlined the goals we set for a 
knowledge plane. In this section, we consider aspects of its system 
structure.  Our  discussion  is  speculative:  any  successful  KP 
architecture  will  be  shaped  by  a  number  of  requirements  and 
constraints, not all of which are apparent today. At its highest level 
the  architecture  of  the  KP  will  be  shaped  heavily  by  two  broad 
forces:  its  distributed,  compositional  structure,  and  its  multi-scale, 
potentially global knowledge perspective. 
Our  ultimate  objective  is  that  networked  systems  should  organize 
themselves, under the constraints and guidance of external inputs, to 
meet the goals of their stakeholders. Even in the near term, the KP 
must  respect  and  build  on  the  fact  that  networks  have  internal 
structure  and  dynamics  --  large  networks  are  composed  by 
interconnecting  smaller  ones,  participants  come  and  go,  and 
relationships  between  the  owners,  operators  and  users  of  different 
networks may change even when the physical structure does not. 
This  implies  that  the  knowledge  plane  serving  a  network  is  not  a 
globally engineered entity, but is instead an autonomously created 
structure 
is  recursively,  dynamically,  and  continuously 
composing  and  decomposing  itself  from  smaller  sub-planes.  This 
requirement argues that the KP: 
that 
• 
• 
• 
Is  distributed  -  KP  functionality  for  different  regions  of  the 
network is physically and logically decentralized. 
Is bottom up - simple entities (e.g. web servers) can compose 
into larger, more complex entities (e.g. a web farm) as needed, 
and  decompose  themselves  from  the  system  as  appropriate. 
This is a recursive process, that may proceed on many levels. 
Is constraint driven - the basic principle is that the system can, 
and  may,  adopt  (or  not)  any  behavior  that  is  not  specifically 
constrained. 
•  Moves from simple to complex. Speaking generally, the act of 
composing a set of networks to form a larger one places more 
requirements or constraints on the behavior of each network. A 
trivial example would be that a standalone IP network can use 
a wide range of addresses, but connecting it to a larger network 
constrains the range of options in this regard. 
• 
Our first objective for the KP system architecture is that it support 
this distributed, compositional perspective, providing the necessary 
enabling abstractions and capabilities. 
In  contrast  with  the  distributed  organization  of  the  KP,  we  have 
argued in previous sections of this paper that KP may often benefit 
from  taking  a  global  perspective  -  integrating  observations  and 
conclusions from many points in the network.  Key implications of 
this are that: 
•  Data and knowledge integration is a central function of the KP. 
The  KP  must  be  able  to  collect,  filter,  reduce,  and  route 
observations,  assertions,  and  conclusions  from  different  parts 
of the network to points where they are useful. 
The KP must operate successfully in the presence of imperfect 
information. Because this global perspective is both physically 
large  and spans multiple administrative entities, the cognitive 
algorithms  of  the  KP  must  be  prepared  to  operate  in  the 
presence of limited and uncertain inputs. 
The KP must reason about information tradeoffs. Sometimes, a 
global  perspective  may  be  critical.  Other  times,  it  may  be 
unimportant,  or  merely  somewhat  useful.  The  KP  must  be 
prepared to reason about the tradeoffs involved in using data of 
differing scope. For instance, diagnosing a web server failure 
may,  or  more  likely,  may  not  require  polling  for  user 
experience from locations far away. A KP may need to employ 
introspective  meta-reasoning  to  act  most  effectively  in  these 
circumstances. 
• 
Our second objective for the KP system architecture is that to the 
extent possible it develop, utilize, and reason about information at 
whatever scope is appropriate for the problem it is addressing. 
4.1  Functional and Structural Requirements 
The above objectives, together with the core goals of the knowledge 
plane,  lead  us  to  several  top-level  functional  and  structural 
architectural requirements. We discuss four of these below. 
4.1.1  Core Foundation 
The heart of the knowledge plane is its ability to integrate behavioral 
models  and  reasoning  processes  into  a  distributed,  networked 
environment.  The first component of this ability is support for the 
creation,  storage,  propagation  and  discovery  of  a  variety  of 
information,  likely  including  observations,  which  describe  current 
conditions;  assertions,  which  capture  high-level  goals,  intentions 
and constraints on network operations; and explanations, which are 
an  example  of  how  knowledge  itself  is  embodied—explanations 
take observations and assertions and map them to conclusions. 
To learn about and alter its environment, the knowledge plane must 
access, and manage, what the cognitive community calls sensors and 
actuators.  Sensors are entities that produce observations.  Actuators 
are entities that change behavior (e.g., change routing tables or bring 
links up or down).  So, for instance, a knowledge application that 
sought to operate a network according to certain policies might use 
sensors  to  collect  observations  on  the  network,  use  assertions  to 
determine  if  the  network’s  behavior  complies  with  policy,  and,  if 
necessary, use actuators to change the network’s behavior. 
The  most  central  aspect  of  the  knowledge  plane  is  its  support  for 
cognitive computations. This is a challenging problem because the 
dynamic and distributed KP environment is not well matched to the 
classical  knowledge  level  algorithms  and  agent  architectures  that 
underpin much of current AI. Most AI algorithms are not designed 
to  work  in  a  highly  distributed  context,  and  direct  experience  in 
building a large distributed data management system with embedded 
cognitive  abilities  is  limited.1 What is needed are robust, tractable 
and  on-line  algorithms  for  environments  that  are  highly  dynamic, 
partially observable, stochastic and error prone. The field of Multi-
Agent  Systems  [22]  has  had  some  initial  success  in  solving  these 
problems,  although  those  addressed  to  date  typically  lack  the 
dynamicity  required  for  the  knowledge  plane.  Thus  refinement  of 
this  portion  of  the  knowledge  plane  architecture,  its  infrastructure 
support for a range of appropriate cognitive algorithms, is likely to 
progress  in  conjunction  with  further  research  in  cognitive systems 
themselves. 
4.1.2  Cross-Domain and Multi-Domain Reasoning 
Where does the KP “run”? The composed, regional structure of the 
KP might suggest that a specific server would support the part of the 
KP that “reasons about” a region, for example an Internet AS. One 
possibility is that the administrator of the AS would run the KP that 
oversaw  that  AS.  At  a  more  abstract  level,  one  might  state  this 
structuring  strategy  as  “each  region  is  responsible  for  reasoning 
about itself.” 
This is a bad idea, for several reasons. If the AS is down, this could 
render the relevant KP information unreachable at exactly the wrong 
time. The administrator of an AS might wish to limit the conclusions 
that  the  KP  reached  about  it,  perhaps  to  remove  knowledge  that 
seems  unflattering,  while  others  may  choose  to  reach  those 
conclusions anyway. These examples show that reasoning about an 
AS occurs independently of the AS; a fact that should be reflected in 
the system structure. Different parts of the KP might independently 
reason about an AS, and compare answers, to detect that part of the 
KP  is  corrupted.  This  shows  that  there  should  be  no  specific 
physical relationship between a region of the network and the KPs 
reasoning engines related to that region. 
A  more  radical  possibility  is  that  multiple  entities  compete  to 
provide information about a given AS.  Each entity collects its own 
data and sells its observations. The KP could seek information from 
1One  early  and  related  attempt,  the  DARPA-sponsored  Automated 
Network  Management  (ANM)  project,  sought  to  build  a  network-wide 
MIB collector combined with AI tools [7]. The ANM experience was that 
collecting data was relatively easy, but getting the data to the right place 
was  hard  –  it  was  easy  to  overwhelm  links  with  management  traffic  if 
information was circulated too aggressively. 
whichever  entity  or  entities  it  believes  provides  the  most  accurate 
and timely (or most cost effective) information.  This ``knowledge 
marketplace'' creates a host of architectural challenges, ranging from 
how  to  reason  about  information  from  multiple  providers  (even  if 
three  different  companies  tell  you  the  same  thing  about  an  AS,  it 
may turn out that they're all reselling data from one Internet weather 
service: if you really want a second opinion, how do you find the 
second  weather  service?)  to  how  to  design  KP  protocols  to 
discourage different knowledge companies from subtly “enhancing” 
the  KP  protocols  or  data  in  ways  that  make  it  harder  for  users  to 
concurrently use the servers of other knowledge providers? 
This  discussion  demonstrates  the  potential richness of information 
flow in the KP. Messages need to flow to more than one location so 
that redundant reasoning can occur – and how a message flows may 
depend on who asks it. Different parts of the KP may reach different 
conclusions, and reconciling these is as important as is dealing with 
incomplete input data. 
4.1.3  Data and Knowledge Routing 
We  have  argued  that  the  KP  will  benefit  from  gaining  a  global 
perspective on the network it serves. It is useful to consider how this 
perspective  might  come  about.  In  a  very  small  network,  it  might 
theoretically  be  possible  to  collect  all  relevant  information,  and 
flood that information to each node in the network (more precisely, 
in the distributed KP). 
This  idea is clearly impractical in larger networks. First, the sheer 
volume  of  information  is  technically  daunting,  requiring  a  highly 
scalable  solution.  Beyond  this,  forces  such  as  competition  and 
privacy come into play. In a network of any size, it is necessary to 
limit and optimize the collection and routing of information. More 
sophistication is needed. 
We suggest that the KP architecture should implement a framework 
for  knowledge  management  and  routing  characterized  by  two 