way as the local equivalence check, by (cid:27)rst making two separate
copies of the network encoding, and then relating the environments.
As before, we check that all the (cid:27)nal data plane forwarding deci-
sions and all exports to neighboring networks must be the same as
a result.
Fault Tolerance. Con(cid:27)gurations that work correctly in the ab-
sence of failures may no longer work correctly after one or more
links fail. For each property above, we can verify that it holds for
up to k failures by adding the following constraint on the number
of links that are failed: (cid:88)
failedx,y ≤ k
(x,y )∈edges
For each outgoing interface in the network, we add a variable outi
representing the fraction of the load sent out that interface, which
4This could be easily extended to weighted ECMP by scaling x by a constant according
to the fraction of tra(cid:28)c split.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ryan Becke(cid:29), Aarti Gupta, Ratul Mahajan, and David Walker
Because link failures are part of the network model, the solver
will learn facts about the impact of failures on the rest of the net-
work control plane. This behavior means that properties involving
failures can often be checked more e(cid:28)ciently than iterating over
failure cases using a failure-free model (i.e., verifying a property
multiple times independently, once for each failure case).
Fault-Invariance Testing. We can use the same strategy as full
equivalence checking to instead check if the same property holds
in a single network regardless of failures. For example, even if we
do not know whether two routers should be able to reach one
another (a possible problem when analyzing networks without
speci(cid:27)cations), we can check that the two routers are reachable if
and only if they are reachable after any single failure. Such a test
can (cid:27)nd instances where network behavior di(cid:29)ers after failures. To
check fault-invariance with respect to a property P, we create two
copies of the network. For the (cid:27)rst copy, we require that there are
no failures. For the second copy, we allow there to be any k failures.
We then check that P holds in the (cid:27)rst copy of the network exactly
when it holds in the second copy.
6 OPTIMIZATIONS
While conceptually simple, the naive encoding of the control plane
described in §3 does not scale to large networks. We present two
types of optimizations that dramatically improve the performance
of the control-plane encoding.
6.1 Hoisting
Hosting lifts repeated computations outside their logical context
and precomputes them once. Two main optimizations of this class
that we use are:
Pre(cid:27)x elimination. Our naive encoding does not scale well in
large part because of the constraints of the form FBM(p1, p2, n),
which checks that two symbolic variables have the (cid:27)rst n bits in
common. The natural way to represent p1 and p2 for this check
is to use 32-bit bitvectors and check for equality using a bit mask.
However, bitvectors are expensive and solvers typically convert
them to SAT. In our model, this would introduce up to 128 new
variables for every topology edge in the network (4 records per edge)
thereby introducing an enormous number of additional variables.
To avoid this complexity, we observe that the pre(cid:27)x received
from a neighbor does not actually need to be represented explicitly.
In particular, because we know (symbolically) the destination IP
address of the packet and the pre(cid:27)x length, there is a unique valid,
corresponding pre(cid:27)x for the destination IP. For example, if the
destination IP is 172.18.0.4 and the pre(cid:27)x length is /24, and the route
is valid for the destination, then the pre(cid:27)x must be 172.18.0.0/245.
However, we must still be able to check if a pre(cid:27)x is matched
by a router’s import or export (cid:27)lter. Somewhat unintuitively, we
can safely replace any (cid:27)lter on the destination pre(cid:27)x with a test
on the destination IP address directly, thereby avoiding the need to
explicitly model pre(cid:27)xes. Consider the following pre(cid:27)x (cid:27)lter:
ip prefix_list L allow 192.168.0.0/16 ge 24 le 32
Its semantics is that it succeeds only if the (cid:27)rst 16 bits of 192.168.0.0
match the pre(cid:27)x, and the pre(cid:27)x length is greater than or equal to 24
5Alternatives such as 172.18.0.1/24 are treated identically.
and less than or equal to 32. In general, for a pre(cid:27)x (cid:27)lter of the form
P/A ge B le C to be well formed, vendors require that A < B ≤ C.
A simple translation of this for SMT record e is:
FBM(e.pre(cid:27)x, 192.168.0.0, 16) ∧ (24 ≤ e.length ≤ 32)
Suppose now, we replace the test on the pre(cid:27)x contained in the
control plane advertisement with a test directly on the destination
IP address of a packet of interest:
FBM(dstIP, 192.168.0.0, 16) ∧ (24 ≤ e.length ≤ 32)
There are two cases to consider. First, if e.length is not between
24 and 32, then both tests fail, so they are equivalent. Suppose
instead, e.length is in this range. Recall that, because we are con-
sidering a slice of the network with respect to the destination IP
address, for the advertisement corresponding to e to be valid, it
must be the case that the pre(cid:27)x contains the destination IP. That is:
FBM(e.pre(cid:27)x, dstIP, e.length). However, because we know the pre(cid:27)x
length falls in the range between 24 and 32, it must be greater than
16. Since the (cid:27)rst bits up to the pre(cid:27)x length are common between
the destination IP and the pre(cid:27)x, the (cid:27)rst 16 bits must also be the
same. Therefore the above substitution is equivalent.
Further, because the test FBM is now purely in terms of constants
in the con(cid:27)guration (not the symbolic pre(cid:27)x length variable), we
can represent the destination variable as an integer and implement
the test using the e(cid:28)cient theory of integer di(cid:29)erence logic (IDL).
Thus, we would test that:
(192.168.0.0 ≤ dstIP < 192.168.0.0 + 232−16) ∧
(16 ≤ e4.length ≤ 32)
Loop Detection. In protocols that support policy-based routing
(e.g., BGP), path length alone does not su(cid:28)ce to prevent loops. For
this reason, BGP tracks the ASNs (autonomous system numbers)
of networks along the advertised path and routers reject paths
with their own ASN. We can model this by maintaining, for each
BGP router, a control bit saying whether or not the advertised
path already went through that router. However, doing so can be
expensive since the number of control bit variables grows with
the square of the number of routers. Instead, we observe that any
BGP router that uses only default local preferences (i.e., only makes
decisions based on path length) will never select a route where it is
already part of the AS path. This is because the path containing the
loop is strictly longer than the path without the loop. For example,
if AS 1 uses shortest path routing only, then the AS path 1 2 1 3
can never arise in our model since AS 1 would prefer the path 1
3 instead. Similarly, BGP local preferences for external neighbors
and for iBGP peers will not create loops. This optimization makes
it possible to forgo modeling loops in most cases.
6.2 Network Slicing
Slicing removes bits from the encoding that are unnecessary for
the (cid:27)nal solution. We use the following slicing optimizations:
• Remove symbolic variables that never in(cid:30)uence the decision
process. For example, if BGP routers never set a local preference,
then the local preference attribute will never a(cid:29)ect the decision
and can be removed.
• Keep a single copy of import and export variables for an edge
when there is no import (cid:27)lter on the edge. The two variable sets
will simply be copies of each other.
A General Approach to Network Configuration Verification
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 7: Veri(cid:27)cation time for management interface reachability (upper left), local equivalence (upper right), blackholes
(lower left), and fault-invariance (lower right) for real con(cid:27)gurations sorted by total lines of con(cid:27)guration.
• Keep a single, merged copy of the export record for a protocol
when there is no peer-speci(cid:27)c export policy.
• Do not model directly connected routes for a router whose inter-
face addresses can never overlap with the destination IP range of
interest to the query.
• Merge the data plane and control plane forwarding variables
along edges that do not have ACLs.
• Merge per-protocol and overall best records when there is only a
single protocol running on a router.
Together, these optimizations are e(cid:29)ective at removing a lot of
redundant information that the SMT solver might otherwise have
to discover for itself.
7 IMPLEMENTATION
Minesweeper uses Bat(cid:27)sh [13] to parse vendor-speci(cid:27)c con(cid:27)g-
urations. It then translates Bat(cid:27)sh’s representation into a sym-
bolic model. To check model (un)satis(cid:27)ability, we use the Z3 SMT
solver [8]. Our encoding exploits Z3’s support for integer di(cid:29)erence
logic, and its preprocessor. Our implementation supports all of the
features and properties described in the paper. We have validated its
correctness empirically by comparing its output to that of the Bat-
(cid:27)sh simulator on a large collection of networks. As Bat(cid:27)sh does not
currently support IPv6, Minesweeper does not either. Minesweeper
is available as open source software [3].
8 EVALUATION
We evaluate Minesweeper by using it to verify a selection of the
properties described in §5 on both real and synthetic network con-
(cid:27)gurations. In particular, we are interested in measuring (1) the
ability of Minesweeper to (cid:27)nd bugs in real con(cid:27)gurations, which
are otherwise hard to (cid:27)nd; (2) its scalability for answering various
queries on large networks; and (3) the impact of the optimizations
described in §6 on performance. All experiments are run on an 8
core, 2.4 GHz Intel i7 processor running Mac OSX 10.12.
8.1 Finding Errors in Real Con(cid:27)gurations
We demonstrate Minesweeper’s ability to (cid:27)nd bugs in real con(cid:27)g-
urations by applying it on a collection of con(cid:27)gurations for 152
real networks. We obtained these from a large cloud provider, and
they represent di(cid:29)erent networks within their infrastructure. The
networks range in size from 2 to 25 routers with 1–23K lines of
con(cid:27)guration each. The networks use a combination of OSPF, eBGP,
iBGP, static routes, ACLs, and route redistribution for layer-3 rout-
ing and are part of a data set described in detail in prior work [15].
These networks have been operational for years, and thus we ex-
pect that all easy-to-(cid:27)nd bugs have already been ironed out. This
data set was also analyzed by ARC [14].
Properties checked. Since we do not have the operator-intended
speci(cid:27)cations, we focus on four properties expected to hold in such
networks:
• Management interface reachability: All nodes in the network
should be able to reach each management interface, irrespective of
the environment. Management interfaces are used to log into the
devices, manage their (cid:27)rmware and con(cid:27)guration, and collect sys-
tem logs. Uninterrupted access to it is important for the network’s
security and manageability.
• Local equivalence: Routers serving the same role (e.g., as “top-
of-rack") should be similar in how they treat packets. We identify
routers in the same role by leveraging the networks’ naming con-
vention and check that all pairs of routers in the network in a given
role are equivalent.
• No blackholes: When tra(cid:28)c is dropped due to ACLs, such drop-
ping should always occur at the edge of the network.
• Fault-invariance: All pairs of routers in the network should be
reachable from one another if and only if they are reachable after a
single failure. A violation of this property would indicate that the
network is highly vulnerable to failures.
Violations. We found 67 violations of management interface reach-
ability. In each case, the violation occurs because of a "hijack," i.e.,
Lines of Configuration0204060Total Time (ms)1K23KLines of Configuration0100200300400Total Time (ms)1K23KLines of Configuration050010001500Total Time (ms)1K23KLines of Configuration0100200300Total Time (ms)1K23KSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ryan Becke(cid:29), Aarti Gupta, Ratul Mahajan, and David Walker
Figure 8: Veri(cid:27)cation time for synthetic con(cid:27)gurations for di(cid:29)erent properties and network sizes.
external neighbors sending particular announcements. For example,
an external BGP advertisement for the same /32 interface pre(cid:27)x
with path length ≤ 1 would result in a more preferred route for
the destination that would ultimately divert tra(cid:28)c away from the
correct interface.
The checks for local equivalence revealed 29 violations. Upon
further investigation, we found that each violation was caused by
one or more exceptions in ACLs where almost all routers in a given
role would have identical ACLs except for a single router with an
extra or a missing entry. Such di(cid:29)erences are possibly caused by
copy-and-paste mistakes.
The blackholes check found 24 violations. Most violations were
not serious issues with routing, but instead revealed optimization
opportunities. Tra(cid:28)c being dropped deep in the network could
have been dropped near the source.
We found no violations of fault-invariance.
8.2 Veri(cid:27)cation Performance
We evaluate the performance of Minesweeper to verify di(cid:29)erent
properties on real and synthetic con(cid:27)gurations.
Real con(cid:27)gurations. We benchmarked the veri(cid:27)cation time for
the networks and properties described above. Figure 7 (upper left)
shows this time for management-interface reachability for each
network that is con(cid:27)gured with at least one management inter-
face. The networks are sorted by total lines of con(cid:27)guration, with
more complex networks appearing farther right. We see that the
checks take anywhere from 2 to 60 ms for every network tested.
Figure 7 (upper right) shows the veri(cid:27)cation time for local equiv-
alence among routers in each unique role, for all networks with
at least two routers in any particular role. Veri(cid:27)cation time ranges
anywhere from roughly 5 to 400 ms. This check is more expensive
than management-interface reachability, in part, because it requires
more queries. Finally, the lower row of Figure 7 shows the time
for veri(cid:27)cation of the absence of blackholes and fault-invariance
queries. Both queries take under a second for most networks. The
worst case is under 1.5 seconds. While the networks we studied
are small, the sub-second veri(cid:27)cation times we observe are encour-
aging. They point to the ability of Minesweeper to verify many real
con(cid:27)gurations in an acceptable amount of time. Next, we stress test
our tool by running it on larger, albeit synthetic networks.
Synthetic con(cid:27)gurations. To test the scalability of our tool on