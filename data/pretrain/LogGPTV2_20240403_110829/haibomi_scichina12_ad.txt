| 2764 |Cumulative percent variance | 0.95 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.90 | 10 |Cluster4 | 100 || 2764 |Cumulative percent variance | 0.90 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.90 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance | 0.90 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.85 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance | 0.85 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.85 | 10 |Cluster4 | 100 || 2764 |Cumulative percent variance | 0.85 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.80 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance | 0.80 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.80 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance | 0.80 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.75 | 10 |Cluster4 | 100 || 2764 |Cumulative percent variance | 0.75 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.75 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance | 0.75 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.70 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance | 0.70 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.65 | 10 |Cluster4 | 100 || 2764 |Cumulative percent variance | 0.65 	 1 | 10 |Cluster3 |Cluster3 | 100 |Cumulative percent variance | 0.60 	 1 | 10 |Cluster4 | 100 |
| 2764 |Cumulative percent variance |Figure 6 |PC numbers |PC numbers |PC numbers |PC numbers |PC numbers |PC numbers |PC numbers |PC numbers | 100 || 2764 |Cumulative percent variance |Figure 6 |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. |Cumulative percent variance of methods in different major clusters. | 100 |their projection onto the anomalous space. A request will be put into the anomalous set if its projection is larger than a given threshold, which can be computed as squared prediction error (SPE):
| where δβ is the value of the Q-statistic at the 1 − β confidence level.(I − P P T)y > δβ,
SPE = | where δβ is the value of the Q-statistic at the 1 − β confidence level.(I − P P T)y > δβ,SPE = | (1) |
|---|---|---|
| 4.3 |Localizing anomalous methods |(1) |
We care about the methods whose response latencies have much fluctuation in the problem periods. However, we observe that the invocation time of many methods in user requests is stable. These methods are not the causes of performance degradation, but the noises for diagnosis. Thus we first need to filter out the noises and keep the methods whose response latencies have much fluctuation. These kept methods are then defined as the principal methods. Then, we localize anomalous methods from the principal methods.4.3.1 	Selecting principal methodsIn this section, we introduce how to extract principal methods from all methods in major clusters. Note that the PCs (i.e., eigenvectors of YTY ) are not the principal methods. If high dimensional matrix can be described by the k PCs, the original matrix can be replaced by a subset of k columns with a relatively small loss of information [12]. 	Therefore, we utilize the B4 [36] to pick out principal columns (i.e., principal methods) from all candidate columns. The target of the B4 is to select principal methods whose regression coefficients (i.e., weights) are the largest in corresponding PCs. Each PC can be considered as the linear combination of columns (i.e., methods), and the larger the weight of a method is, the more it contributes to its corresponding PC.For a major cluster, the selection process is as follows: first all PCs are ranked in descending order of variance; then for each principal component, the column with the largest coefficient is retained. If the column with the largest coefficient in one PC has been selected from the former PCs, the column with the second largest coefficient will be considered to pick out. The process will be iterated until all k principal methods are selected.| 4.3.2 | Mi H B, et al. | Sci China Inf Sci | December 2012 Vol. 55 No. 12 | 2765 |
|---|---|---|---|---|
| 4.3.2 |Identifying anomalous methods |Identifying anomalous methods |December 2012 Vol. 55 No. 12 |2765 |By comparing the response latencies of each principal method in the normal and anomalous sets, we can quickly localize the ones whose performance fluctuates the most severely. For each principal method in major clusters, its corresponding latency distributions in the normal and anomalous sets are compared. We employ a standard Mann-Whitney U test [13] to quantify the difference between the two latency distributions. The null hypothesis is that the latency distributions for the normal set and the anomalous set are the same. A principal method is defined to be anomalous if its null hypothesis is rejected, i.e., the difference between the two latency distributions is calculated to be statistically obvious.There are two reasons for us to choose the Mann-Whitney U test. First, the latency datasets of the principal methods do not belong to any particular distribution; therefore, the statistical hypothesis test should require no specific distributions for the data sets; second, in some major clusters, the number of requests in the anomalous set is small, which requires the statistical hypothesis test to consider the small and large sample volume separately. The characteristic of the Mann-Whitney U test satisfies our requirements.A scoring scheme is applied to record the suspicious extent of principal methods. For each principal method, we assign it a score to indicate how the method is suspicious, and iterate all clusters to calculate the score. At the beginning, the score for each principal method is zero. For one cluster, if a principal method is judged to be suspicious, the score of this method will be added by one. The top k methods will be selected according to the suspicious scores. Operators are encouraged to examine the methods in order of decreasing suspiciousness.5 	Localizing anomalous instances
Next, we try to localize the anomalous instances that are the physical locations of the anomalous methods. For anomalous method of mi, we measure the behavior similarity in all related instances. If the behavior of an instance in host hj differs significantly from the ones in other hosts, the misbehavior of mi is considered to be caused by the instance in host hj.The similarities between two instances are measured by the square roots of Jensen-Shannon divergence (JSD) [14]. JSD is widely adopted to quantify the similarity between two data sets. The latencies of the suspicious method mi in all anomalous sets are categorized according to the instance addresses and the histogram of latencies for each category is generated. Here a latency histogram of the method mi in instance j is defined ashij = (r1, r2, . . . , rn)ij, 	(2)
where rk is the ratio of the invoked times of mi in the kth bin to the total invoked times in the instance j. For simplicity, the width of each bin is considered to be the equal and computed by the formula
max(l) − min(l)  ,
where l is the latency of mi. The histograms of the method mi for all instances have the same size of bins. For method mi, the dissimilarity between the instance j and k is defined as dissimj,k = JSD(hij, hik) = 1 2D(hij, vij) + 1 2D(vij, hik), 	(3)
where D(X, Y ) represents the Kullback-Leibler divergence [14] and can be computed as
D(X, Y ) =  X(i) lnX(i) Y (i). (4)
The larger the dissimj,k is, the more dissimilar the two instances are.
2766 Mi H B, et al. Sci China Inf Sci December 2012 Vol. 55 No. 12
Algorithm 2: Process of computing the dissimilarity ratio for each instanceInput: S = {(h, l)i}, a set of 2-tuple, where h is the host address of the instance and l is the response 	latency.
Output: dissimRatio dic, a dictionary of (instance, dissimilarity ratio) pair.
/* Categorizing the requests by the host addresses of instances.*/ host latencyList dic = GroupByHostIP(S); 
host histogram dic = {}; 
for hi, latencyListi in host latencyList dic do/*Computing the ratio of the number in each bin to total counts.*/ 	host histogram dic[hi] = hist/len(latencyListi); 
	/*Computing the histogram for each host.*/ 
	hist = Histogram(latencyListi);
end 
/* Computing the JSD of histograms between the host*/ total = 0; 
hist dissimilarity dic = {}; 
for hi in host histogram dic do 
	dissimi = 0;
	end 	hist dissimilarity dic[hi] = dissimi; 	total+ = dissimi;dissimi+ = dissimi,j; 
for hj in host histogram dic do 
	dissimi,j = JSD(hi.hist, hj.hist);
end 
/*Computing the dissimilarity ratio for each instance*/ dissimRatio dic = {}; 
for hi, dissimi in host dissimilarity dic do
	dissimRatio dic[hi] = dissimi/total; 
end
	Then, we use the dissimilarity ratio to measure the dissimilarity extent of one instance with the others. For instance j, the dissimilarity ratio is defined asdissim ratioj = j∈Hk∈Hdissimj,k
k∈Hdissimj,k . (5)
Ideally, the dissim ratio for all instances should be close to each other. If a given instance’s dissim ratio differs significantly from those of the other instances, the instance is considered to the location of the anomalous methods, which help operators narrow down the diagnosing scope. The detailed measuring algorithm is described in Algorithm 2.6 	Evaluation
Our approach has been applied in Alibaba Cloud Computing Inc. 	to diagnose anomalies when the performance of systems degrades. In this section, we mainly describe the experimental environment and four scenarios of performance anomalies.
| Mi H B, et al. | Sci China Inf Sci | December 2012 Vol. 55 No. 12 | 2767 |
|---|---|---|---||---|---|---|---|
| Table 1 |Key factors influencing the diagnosing results |Key factors influencing the diagnosing results |2767 |
Factor Value Description
| α | 0.75 | The ratio of the request numbers in main clusters to the total requests. |
|---|---|---|
| γ |0.95 |The required variance for the trace logs in PCA. |
| β |0.05 |The significant level for the anomalous request detection in PCA. || λ |0.05 |The significant level for Mann-Whitney U test. |
6.1 	Experimental setup
6.1.1 	Cluster sizes and workloadsThe following experiments are based on the Alibaba cloud computing platform, which contains a series of service components, such as distributed scheduler, storage, communication, and monitor. The application that we use is a file-sharing service, which is deployed on the distributed computing infrastructure. When the performance (i.e., response latencies) of the application degrades under the steady load, we try to detect the primary causes. The effectiveness of our approach is studied in a testing cluster of 100 nodes in Alibaba Cloud Computing Company. Three types of user operations are applied, i.e., SaveFile, ListFile, and ReadFile.6.1.2 	Performance problems scenarios
Many factors can cause performance degradation of cloud computing systems. 	We will evaluate our approach by four case scenarios. All cases are the typical real-world performance anomalies that happened in Alibaba cloud computing platform. The first three performance anomalies are selected from the bug repository of the Alibaba cloud computing platform. We re-inject these anomalies into the system and validate whether our approach can diagnose these problems or not. The last performance anomaly occurs in the production cluster. With our approach, an operator conducts the diagnosing process.1) Misconfiguration. In a configuration file, the parameter controlling ON/OFF of merging small files is set to OFF, which causes the performance of the ListFile to degrade sharply.
	2) Failover. A small portion of service instances used for storing files is killed. In the process of service recoveries, the performance of the SaveFile decreases significantly.3) Code bug. When a service component is upgraded, a deprecated method that adds an extra cost of the ReadFile is accessed due to the mistake of a developer. It causes the average response latencies of the ReadFile to rise obviously.
4) Design defect. Most SaveFile requests are centralized to a small portion of hosts for a period of time due to a defect of the load balance mechanism. Hence, the average response latencies increase about two times larger than the normal.7 	Evaluation of results
We use the metrics of false-positive rate and false-negative rate to evaluate the results of localizing anomalous methods. The false-positive rate is defined as
| num of misunderstood methods | , | (6) |
|---|---|---|
|  num of normal methods |, |(6) |
i.e., the ratio of the number of normal methods which are mistaken for the anomalous to the total normal methods. The false-negative rate is defined asnum of leaked methods 
num of anomalous methods, (7)
i.e., the ratio of the number of abnormal methods which are mistaken for the normal to the total anomalous methods.