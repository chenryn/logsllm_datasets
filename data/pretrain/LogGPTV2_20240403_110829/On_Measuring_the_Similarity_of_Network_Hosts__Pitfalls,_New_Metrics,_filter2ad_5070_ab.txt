ply the largest value possible in the encoding for that
type. In most cases, that means that M = 232 − 1 for
32-bit integers, or M = 216 − 1 for 16-bit.
An obvious caveat, however, is that one must ensure
that values are encoded in such a way that they are rele-
vant to the analysis at hand. For example, when compar-
ing host behavior over consecutive days, it makes sense
to ensure time ﬁelds are made relative to midnight of the
current day, thereby allowing one to measure differences
in activity relative to that time scale (i.e., a day) rather
than to a ﬁxed epoch (e.g., UNIX timestamps).
Categorical Types. While metric spaces for numeric
types are straightforward, categorical data types pose
a problem in developing metric spaces due to the
non-linear relationship between the syntactic encoding
(i.e., integers) and the underlying semantic meaning of
the values. Moreover, the unique semantics of each data
type prohibits us from creating a single metric for use by
all categorical types. Instead, we follow a general proce-
dure whereby a hierarchy is deﬁned to represent the re-
lationships among the values, and the distance between
two values is determined by the level of the hierarchy at
which the values diverge. While this is by no means the
only approach for deﬁning metrics for categorical types,
we believe it is a reasonable ﬁrst step in representing the
semantics of these complex types.
1Note that we will address issues arising from different units of
measure in the following section.
To deﬁne the distance hierarchy for the port type, we
decompose the space of values (i.e., port numbers) into
groups based on the IANA port listings: well-known (0-
1023), registered (1024-49151), and dynamic (49151-
65535) ports. In this hierarchy, well-known and regis-
tered ports are considered to be closer to one another
than to dynamic ports since these ports are typically
statically associated with a given service. Of course,
ports within the same group are closer than those in
different groups, and the same port has a distance of
zero. The hierarchy is shown in Figure 1(a). More for-
mally, we create the metric space (Xport, dport), where
Xport = {0, . . . , 65535} and dport is an extension of the
discrete metric deﬁned as:
dport(x, y) =
if x = y
if δport(x) = δport(y)
if δport(x) ∈ {0, 1} & δport(y) ∈ {0, 1}
if δport(x) ∈ {0, 1} & δport(y) ∈ {2}
 0
1
2
4
(cid:40) 0
1
2
δport(x) =
if x ∈ [0, 1023]
if x ∈ [1024, 49151]
if x ∈ [49152, 65535]
The choice of distances in dport is not absolute, but
must follow the metric properties from Section 3 and
also faithfully encode the relative importance of the dif-
ferences in groups that the ports belong to. One can also
envision extending the hierarchy to include ﬁner grained
information about the applications or operating systems
typically associated with the port numbers. Many such
reﬁnements are possible, but it is important to note that
they make certain assumptions on the data which may
not hold in all cases. For example, one could further
reﬁne the distance measure to ensure that ports 80 and
443 are near to one another as both are typically used
for HTTP trafﬁc, however there are clearly cases where
other ports carry HTTP trafﬁc or where ports 80 and 443
x=y ?x=y ?x=y ?ServicesDynamicWell-KnownRegisteredd    (x,y)=4portd    (x,y)=2portd    (x,y)=1portd   (x,y)=48d   (x,y)=d  (x,y)IPd  (x,y)Hd  (x,y)Hd  (x,y)Hd  (x,y)Hd  (x,y)Hd  (x,y)Hd  (x,y)Hd  (x,y)HHIPd   (x,y)=64IPd   (x,y)=128IPUnicastOtherPublicPrivate10.0.0.0/8192.168.0.0/16172.16.0.0/12MulticastBroadcastLink LocalDefault Networkcarry non-HTTP trafﬁc. One of the beneﬁts of this ap-
proach to similarity measurement is that it requires the
analyst to carefully consider these assumptions when
deﬁning metrics.
The metric space for IP addresses2 is slightly more
complex due to the variety of address types used in
practice. There is, for instance, a distinction between
routable and non-routable, private and public, broad-
cast and multicast, and many others. In Figure 1(b), we
show the hierarchy used to represent the semantic rela-
tionships among these groupings. At the leaves of this
hierarchy, we perform a simple Hamming distance cal-
culation (denoted as dH) between the IPs to determine
distance within the same functional group. Due to the
complexity of the hierarchy, we do not formally deﬁne
its metric space here. We again reiterate that this is just
one of potentially many ways to create a metric for func-
tional similarity of IP addresses. Again, we may imag-
ine a more reﬁned metric that further subdivides IPs by
autonomous system or CIDR block, and again we must
ensure that that assumptions made by these metrics are
supported by the data being analyzed.
4.2 Network Data Records as Points
Given the aforementioned distance metrics, the next
challenge is to ﬁnd a way to combine them into a uniﬁed
metric space that captures the distance between records
(i.e., ﬂows or packets) by treating them as points within
that space. At ﬁrst glance, doing so would appear to be a
relatively simple procedure: assign one of the available
types to each ﬁeld in the record, and then combine the
respective metric spaces using a p-product metric. How-
ever, when combining heterogeneous spaces it is impor-
tant to normalize the distances to ensure that one dimen-
sion does not dominate the overall distance calculation
by virtue of its relatively larger distance values. Typi-
cally, this is accomplished with a procedure known as
afﬁne normalization. Given a distance metric d on val-
ues x, y ∈ X, the afﬁne normalization is calculated as:
d(x, y) = d(x, y) − min(X)
max(X) − min(X)
However, a naive application of this normalization
method fails to fairly weight all of the dimensions used
in the overall distance calculation. In particular, afﬁne
normalization ignores the distribution of values in the
underlying space by normalizing by the largest distance
possible. As a result, very large and sparse spaces, such
as IPs or sizes, may actually be undervalued as a result.
To see why, consider a ﬁeld containing ﬂow size data.
In the evaluation that follows, the vast majority of ﬂows
2The provided metric is for IPv4 addresses. IPv6 addresses would
simply require a suitable increase in distances for each level to retain
their relative severity.
have less than 100 bytes transferred, but the maximum
seen is well over ten gigabytes in size. As a result, afﬁne
normalization would give extremely small distances to
the vast majority of ﬂow pairs even though those dis-
tances may actually still be semantically meaningful. In
essence, afﬁne normalization procedures remove or min-
imize the semantic information of distances that are not
on the same scale as the maximum distance. Yet another
approach might be to measure distance as the difference
in rank between the values (i.e., the indices of the val-
ues in sorted order). Doing so, however, will remove all
semantic information about the relative severity of the
difference in those values.
To balance these two extremes, we propose a new
procedure that normalizes the data according to common
units of measure for the data type. We begin by deﬁning
the overall range of distances that are possible given the
values seen in the data being analyzed. Then, we divide
this space into non-overlapping ranges based upon the
unit of measure that most appropriately suits the values
in the range. In other words, the range associated with
a given unit of measure will contain all distances that
have a value of at least one in that unit of measure, but
less than one in the next largest unit. In this paper, we
use seconds, minutes, and hours for the time data type,
and bytes, kilobytes, and megabytes for size types. Cat-
egorical types, like IPs and ports, are assigned units of
measure for each level in their respective distance hier-
archies.
Once each distance range is associated with a unit
of measure, we can then independently map them into
a common (normalized) interval such that the normal-
ized distances represent the relative severity of each unit
of measure with respect to units from the same type, as
well as those from other data types that are being nor-
malized. It is this piecewise mapping to the normalized
distance range that allows us to maintain the semantic
meaning of the underlying distances without unduly bi-
asing certain dimensions in the overall distance calcu-
lation. For simplicity, we map all metric spaces with
three units of measure to the ranges [0, 0.25), [0.25, 0.5),
and [0.5, 1.0]. Types with four units are mapped as
[0, 0.125), [0.125, 0.25), [0.25, 0.5),[0.5, 1.0]. Figure 2
shows how this mapping is achieved for size and IP
types. Then, by denoting the function that produces
the normalized distance for type tj as dtj (x, y), we can
use the p-product metric to deﬁne the distance between
records in the network data as:
dF (1)(x1, y1)p + ··· + dF (n)(xn, yn)p(cid:17) 1
p
(cid:16)
dp((cid:126)x, (cid:126)y) =
The metric space is now (XF (1) ×···× XF (n), dp), and
we set p = 2 in order to calculate the Euclidean distance
between records.
Figure 2. Example piecewise normalization of size and IP types. Mapping each range indepen-
dently ensures they are weighted in accordance with the relative severity of the distance.
4.3 Network Objects as Time Series
Thus far, we have introduced distance metrics that
attempt to capture the semantics that underlie various
data types found in network data, and showed how that
semantic information may be propagated to metrics for
network data records by carefully normalizing and cre-
ating a uniﬁed metric space. The ﬁnal step in our study
requires us to take these records and show how to use
the metrics we have developed to capture the similarity
in behavior between two hosts. In effect, we deﬁned a
method for reasoning about the spatial similarity of in-
dividual records associated with a host, but must also
address the concept of temporal similarity to capture its
behavior in a meaningful way.
Common wisdom suggests that host similarity should
be calculated by independently evaluating records asso-
ciated with that host, or only examining short subse-
quences of activities. This is due primarily to the so-
called “curse of dimensionality” and the sheer volume of
behavioral information associated with each host. Our
assertion is that doing so may be insufﬁcient when trying
to capture a host’s activities and behaviors as a whole,
since these behaviors often have strong causal relation-
ships that extend far beyond limited n-gram windows.
In order to capture the entirety of a host’s behav-
ior and appropriately measure similarity among hosts,
we instead model a host’s behavior as a time series of
points made up of the records embedded into the n-
dimensional metric space described in the previous sec-
tion. Given two hosts represented by their respective
time series, a dynamic programming method known as
dynamic time warping (DTW) may be used to ﬁnd an
order-preserving matching between points in the two se-
ries which minimizes the overall distance. The DTW
approach has been used for decades in the speech recog-
nition community for aligning words that may be spo-
ken at different speeds. Essentially, the DTW procedure
“speeds up” or “slows down” the time series to deter-
mine which points should be aligned based on the dis-
tances between those two points, which in our case are
calculated using the p-product metric.
Unfortunately, DTW runs in O(m1m2) time and
space for two time series of length m1 and m2, respec-
tively. Considering that most real-world hosts produce
millions of records per day, such an approach would
quickly become impractical. Luckily, several heuris-
tics have been proposed that limit the area of the dy-
namic programming matrix being evaluated. In particu-
lar, Sakoe and Chiba [26] provide a technique which re-
duces the computational requirements of the DTW pro-
cess by evaluating only those cells of the matrix which
lie within a small window around the diagonal of the
matrix. Despite only examining a small fraction of
cells, their approach has been shown to produce accu-
rate results in measuring similarity due to the fact that
most high-quality matches often occur in a small win-
dow around the diagonal, and that it prevents so-called
pathological warpings wherein a single point may map
to large sections of the opposite time series.
4.3.1 Efﬁcient Dynamic Time Warping for Network
Data
Ideally, we would apply the Sakoe-Chiba heuristic to
make the DTW computation feasible even for very
large datasets. Unfortunately, the Sakoe-Chiba heuris-
tic makes assumptions on the time series that prevent its
direct application to network data. In particular, it as-
sumes that the points in the time series are sampled at
a constant rate, and so the slope of the diagonal that is
evaluated in the matrix depends only on the length of the
time series (i.e., the time taken to speak the word). For
network data, however, the rate at which records are pro-
duced is based on the trafﬁc volumes sent and received
by the host, which are inherently bursty in nature and
dependent on the host’s activities. Consequently, a sin-
gle diagonal with a ﬁxed slope would yield a warping
path that attempts to align points that may be recorded
at signiﬁcantly different times, leading to a meaningless
measurement of behavior.
To address this, we propose an adaptation where we
break the two time series into subsequences based on
the time period those sequences cover, and calculate
individual diagonals and slopes for each subsequence.
The individual subsequences can then be pieced together
to form a warping path that accommodates the vari-
able sampling rate of network data, and from which we
SizeNormalizedIPMegabytes (1,048,576)Kilobytes (1,024)BytesLevel 0Level 1Level 2Hamming Distance1.00.50.0(a) DTW with heuristic for constant sampling.
(b) DTW with heuristic for variable sampling.
Figure 3. Example dynamic programming matrices with the original Sakoe-Chiba heuristic (a)
and our adapted heuristic (b) for variable sampling rate time series. Gradient cells indicate the
“diagonals” and dark shaded cells indicate the window around the diagonal that are evaluated.
In the the variable rate example, points in the same subsequence are grouped together.
can extend a window to appropriately measure similar-
ity without evaluating the entire dynamic programming
matrix. Figure 3 shows an example of the traditional
Sakoe-Chiba heuristic and our adaptation for network
data.
Our extension of the Sakoe-Chiba heuristic for net-
work data proceeds as follows. Assume that we are
given two time series A and B, where |A| ≤ |B|. We
begin by splitting the two time series being aligned into
subsequences such that all points in the same subse-
quence were recorded during the same second (accord-
ing to their timestamps). The subsequences in the two
time series are paired if they contain points for the same
second. Any subsequences that have not been paired are
merged in with the subsequence in the same time series
that is closest to its timestamp. Thus, we have k subse-
quences A1, . . . , Ak and B1, . . . , Bk for the sequences
A and B, with Ai and Bi being mapped to one another.
Next, we iterate through each of the k subsequence
pairs and calculate the slope of the pair as Si = |Bi|
|Ai| .
The slope and subsequence mapping uniquely determine
which cells should be evaluated as the “diagonal” in our
variable sampling rate adaptation. Speciﬁcally, the jth
point in the ith subsequence (i.e., Ai,j) is mapped to
Bi,j(cid:48) where j(cid:48) = (cid:100)j ∗ Si(cid:101).
With this mapping among points in hand, we then
place a window around each cell of length at least (cid:100)Si(cid:101)
to ensure continuity of the path in the dynamic program-
ming matrix (i.e., a transition exists between cells). For
those points that occur at the beginning or end of a sub-
sequence, the window length is set based on the index of
the mappings for the adjacent points to ensure that the
subsequences are connected to one another. In order to
provide a tunable trade-off between computational com-
plexity and accuracy of the warping, we extend the win-
dow by a multiplicative parameter c so that c ∗ Si cells
Algorithm 1
Dynamic Time Warp (A, B, map, slope, c)
Require: |A| ≤ |B|
Require: c > 0
Initialize m as a |A| × |B| matrix
m[i][j] ← ∞ for all i, j
m[0][0] ← 0.0
for i ← 0 to |A| do
start ← max(0, map[i] − slope[i] ∗ c)
end ← min(|B|, map[i] + slope[i] ∗ c)