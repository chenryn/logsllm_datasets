### Optimized Text

**Purchased I2**
**Enjoy I3**
**Enjoy I4**

#### (c) Excerpt of Inference on Stripe Checkout
**(P5) Reporting.** The RA4 and LCSRF attacks are reported to Diana. Execution details of the attack patterns are logged and can be inspected.

| Element  | DataKey  | Token  |
|----------|----------|--------|
| Data Flow | SP-TTP   | TTP-SP |
| SynLabel | BLOB     | BLOB   |
| SemLabel | MAND, AU | MAND, SU |

#### (d) Attack Pattern Application on Stripe Checkout

- **RA1:**
  - **RedURI:** REPLAY Token FROM (UV, SPM) IN (UM, SPT).
  - **Description:** This attack pattern reports no attacks. When the attack test-case reaches step 10 of Figure 4a, UV’s Token, which was issued for SPM, is replayed by UM against SPT. The TTP Stripe identifies a mismatch between the owner of the Secret and the SP for which the Token was issued and returns an error status at step 12.

- **RA2:**
  - **RedURI:** REPLAY DataKey FROM (UM, SPM) IN (UM, SPT).
  - **Description:** No attacks reported. Similar reasons as the previous one: the attacker replays DataKey belonging to SPM in the checkout session at SPT. Hence, the Token returned by TTP cannot be used by SPT to receive a success status at step 12.

- **RA3:**
  - **RedURI:** REPLAY Token FROM (UM, SPT) IN (UM, SPT).
  - **Description:** No attack reported. In Stripe checkout, the validity of a Token expires once it is used. Reuse of the Token returns an error.

- **RA4:**
  - **RedURI:** REPLAY DataKey FROM (UM, SPT) IN S where S = REPLAY Token FROM S IN (UM, SPT).
  - **Description:** This attack pattern reports an attack as there is no protection mechanism in the Stripe checkout solution that prevents spoofing of the DataKey by another SP. Initially, the attack test case replays the DataKey from (UM, SPT) into (UV, SPM). When the Token obtained in this session by SPM is replayed into session (UM, SPT), Stripe does not identify any mismatch and returns a success status at step 12. This allows the attacker UM to impersonate UV and purchase a product at SPT.

- **LCSRF:**
  - **RedURI:** REPLACE req WITH REQUEST-OF Token FROM (UM, SPT) IN [UM SEND req].
  - **Description:** This pattern detects an attack. The test case generated sends an HTTP POST request corresponding to step 10 with an unused Token. This request alone is enough to complete the protocol and uncover a CSRF. In our experiment, this was discovered on the demo implementation of Stripe. It is not unusual that such protections are missing in demo systems. We do not know whether any productive MPWAs suffer from this. Determining this would require specific testing on the productive system and the buying of real products.
  - **Note:** This pattern is not applicable as there are no URIs that have data flow TTP-SP and semantic property RURI.

**Figure 4: Security Testing Framework on an Illustrative Example**

### WebDriver and Zest
WebDriver [13] and Zest [17] can be used for recording UAs. Such technology could be extended to allow the tester to define Flags by simply clicking on the web page elements (e.g., the payment confirmation form) that identify the completion of user actions. Off-the-shelf market tools already implement this kind of feature to determine the completion of the login operation.

**(P3) Inference.** The inference module automatically executes the nominal sessions recorded in the previous phase and tags the elements in the resulting HTTP traffic with the labels in Figure 2. We do not exclude that in the future more information (e.g., inference of the observable workflow of the MPWA [32]) could be necessary to target more complex attacks. While we borrow the idea of inferring syntactic and semantic properties from [36] and [32], we introduce the concept of inferring flow labels to make our approach more automatic (compared to [36]) and efficient (less number of test cases for detecting the same attack mentioned in [32]).

The inference results of sessions S1 to S4 are stored in a data structure named labeled HTTP trace.

**(P4) Application of Attack Patterns.** Labeled HTTP traces (output of inference) are used to determine which attack patterns shall be applied and corresponding attack test cases are executed against the MPWA.

**(P5) Reporting.** Attacks (if any) are reported back to the tester, and the tester evaluates the reported attacks.

### V. Implementation
We implemented our approach on top of OWASP ZAP (ZAP, in short). In this way, the two core phases of our testing engine (cf. P3 and P4 in the previous section) are fully automated and take advantage of ZAP to perform common operations such as execution of UAs, manipulating HTTP traffic using proxy rules, regular expression matching over HTTP traffic, etc. Figure 5 outlines the high-level architecture of our testing engine. The Tester provides the necessary input to our Testing Engine, which in turn employs OWASP ZAP to probe the MPWA. In particular, the Testing Engine invokes the API exposed by ZAP to perform the following operations:

- **Execute User Actions and Collect HTTP Traces:** UAs, expressed as Zest scripts, can be executed via the Selenium WebDriver module in ZAP, and the corresponding HTTP traffic can be collected from ZAP.
- **Proxy Rule Setting:** Proxy rules can be specified, as Zest scripts, to mutate HTTP requests and responses passing through the built-in proxy of ZAP.
- **Evaluate Flag:** Execute regular expression-based pattern matching within the HTTP traffic to evaluate whether the Flag is present in the HTTP traffic.

Hereafter, we detail the two core phases (P3 and P4) of our Testing Engine that follow the flow depicted in Figure 6. Each step is tagged by a number to simplify the presentation of the flow.

1. **Inference:**
   - **Trace Collection (Steps 2-3):** The input UAs are executed, and corresponding HTTP traces are collected. The Flags are used to verify whether the collected traces are complete. We represent the collected HTTP traces as HT(S1), HT(S2), HT(S3), and HT(S4). The traces are stored as an array of (request, response, elements) triplets. Each triplet comprises the HTTP request sent via ZAP to the MPWA, the corresponding HTTP response, and details about the HTTP elements exchanged. An excerpt of a trace related to our illustrative example (Figure 4a) is depicted in Figure 7 in JSON format. For simplicity, we present only one entry of the trace array and only one HTTP element. We assume the reader is familiar with the standard format of the HTTP protocol. Here we focus on the HTTP elements. For each of them, we store the name ("name"), the value ("value"), its location in the request/response ("source", e.g., source:"request.body" indicates that the element occurs in the request body of the HTTP request), the associated request URL ("url"), its data flow patterns, syntactic and semantic labels that are initially empty and will be inferred in the next activities. For instance, the element illustrated in Figure 7 is the Token shown in step 10 of Figure 4a.
   - **Syntactic and Semantic Labeling (Steps 4-10):** The collected HTTP traces are inspected to infer the syntactic and semantic properties of each HTTP element, reported in Figure 2. While syntactic labeling is carried out by matching the HTTP elements against simple regular expressions, semantic labeling may require (e.g., for MAND) active testing of the MPWA. For instance, to check whether an element e occurring in HT(UM, SPT) is to be given the label MAND, the inference module generates a proxy rule that removes e from the HTTP requests (step 6). By activating this proxy rule (step 7), the inference module re-executes the UA corresponding to the session (UM, SPT) and checks whether the corresponding Flag is present in the resulting trace (steps 8-9). For instance, the element Token (see Figure 7) is assigned the syntactic labels BLOB and the semantic labels SU and MAND.
   - **Data Flow Labeling (Step 11):** After syntactic and semantic labeling, the data flow properties of each MAND element in the trace are analyzed to identify the data flows (either TTP-SP or SP-TTP). To identify the protocol patterns, it is necessary to distinguish TTP and SP from the HTTP trace. We do this by identifying the common domains present in the HTTP trace of the two different SPs (SPT and SPM) implementing the same protocol and classifying the messages from/to these domains as the messages from/to TTP.
   - **Output of the Inference Phase:** The output of the inference phase is the labeled HTTP traces of sessions S1 to S4 (represented as LHT(S1), LHT(S2), LHT(S3), and LHT(S4)).

2. **Attack Pattern Engine:**
   - **Representation of Attack Patterns:** For the simplicity of explanation, we represent our attack patterns in the same way as the attack graph notation introduced in [33]. Each attack pattern has a Name, the underlying Threat model, Inputs used, the Goal the attacker (who follows the attack strategy defined in the pattern) aims to achieve, Preconditions, Actions, and Postconditions. The Inputs to the attack pattern range over the LHTs (labeled HTTP traces generated by the inference module), UAs of the nominal sessions, and the corresponding Flags. The Goal, Preconditions, Actions, and Postconditions are built on top of the Inputs. The pattern is applicable if and only if its Preconditions hold (steps 12-14 of Figure 6). As soon as the pattern Preconditions hold, the Actions are executed (steps 15-17 of Figure 6). The Actions contain the logic for generating proxy rules that mimic the attack strategy. The generated proxy rules are loaded in ZAP, and UAs are executed. The execution of UAs generates HTTP requests and responses. The proxy rules manipulate the matching requests and responses. As the last step of the Actions execution, the Postconditions are checked. If they hold (step 18 of Figure 6), an attack report is generated with the configuration that caused the attack (step 19 of Figure 6).

**Example of Attack Pattern for RA1:**
To illustrate, let us consider the Replay Attack pattern RA1 reported in Table III. In Listing 1, we show the pseudo-code describing it.

- **Threat Model:** The threat model considered is the web attacker. To evaluate the applicability of the pattern, the output of the inference phase is sufficient (LHT(UV, SPM)): the attack is applicable if at least one element x in LHT(UV, SPM) is such that (TTP-SP ∈ x.flow AND (SU|UU) ∈ x.labels).
- **Actions:**
  - For each x such that preconditions hold:
    - e = extract(x, UAs(UV, SPM))
    - HTTP logs = replay(x, e, UAs(UM, SPT))
    - Check Postconditions
- **Postconditions:**
  - Check Flag(UV, SPT) in HTTP logs
  - Report(e, UAs(UM, SPT), Flag(UV, SPT))

**Listing 2: Extract Function**
```python
def extract(idx, uas_UAs):
    rb = generate_break_rule(idx)
    load_rule_ZAP(rb)
    HTTP_logs = execute_ZAP(UAs)
    e = extract_value(idx, HTTP_logs)
    clear_rules()
    return e
```

**Listing 3: Replay Function**
```python
def replay(idx, value_e, uas_UAs):
    rr = generate_replay_rule(idx, value_e)
    load_rule_ZAP(rr)
    HTTP_logs = execute_ZAP(UAs)
    return HTTP_logs
```

**Note:** Besides the functions mentioned above, we provide several functions to help the security expert in defining new attack patterns. The full list of functions that can be used in the definition of attack patterns is available at https://sites.google.com/site/mpwaprobe.

### VI. Evaluation
To test the effectiveness of our approach, we ran our prototype implementation against a large number of real-world MPWAs. In Section VI-A, we explain the criteria based on which we selected our target MPWAs. Next, in Sections VI-B and VI-C, we explain the attacks we discovered (both automatically and with manual support) and finally, in Section VI-D, we provide some information on how we (responsibly) disclosed our findings to the affected vendors.

**A. Target MPWAs:**
We selected SSO, CaaS, and VvE (see Figure 1c) scenarios as the targets of our experiments. For the SSO scenario, we adopted the Google dork strategy mentioned in [8] to identify SPs integrating SSO solutions offered by LinkedIn, Instagram, PayPal, and Facebook. Additionally, we prioritized the Google dorks results using the Alexa rank of SPs. For the CaaS scenario, we targeted open-source e-commerce solutions and publicly available demo SPs integrating 2Checkout and Stripe checkout solutions. For the VvE scenario, we selected the websites belonging to the Alexa Global Top 500 category.

**B. Results:**
We have been able to identify several previously unknown vulnerabilities, and they are reported in Table IV. We have also provided a full list of functions that can be used in the definition of attack patterns, available at https://sites.google.com/site/mpwaprobe.

**Figure 7: HTTP Trace with Empty Labels (an Excerpt)**

**Listing 1: Attack Pattern for RA1**
```plaintext
Name: RA1
Threat Model: Web Attacker
Inputs: UAs(UV, SPM), LHT(UV, SPM), UAs(UM, SPT), Flag(UV, SPT)
Preconditions: At least one element x in LHT(UV, SPM) is such that (TTP-SP ∈ x.flow AND (SU|UU) ∈ x.labels)
Actions:
For each x such that preconditions hold:
e = extract(x, UAs(UV, SPM))
HTTP_logs = replay(x, e, UAs(UM, SPT))
Check Postconditions
Postconditions: Check Flag(UV, SPT) in HTTP_logs
Report(e, UAs(UM, SPT), Flag(UV, SPT))
```

This optimized text is now more structured, coherent, and professional, making it easier to understand and follow.