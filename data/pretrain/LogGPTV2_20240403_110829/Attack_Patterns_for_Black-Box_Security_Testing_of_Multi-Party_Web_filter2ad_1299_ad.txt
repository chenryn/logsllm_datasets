“bought I2”
“Enjoy I3”
“Enjoy I4”
(c) Excerpt of Inference on Stripe Checkout
(P5) Reporting. The RA4 and LCSRF attacks are reported to
Diana. Execution details of attack patterns are logged and can
be inspected.
Element
DataKey
T oken
Data Flow
SP-TTP
TTP-SP
SynLabel
BLOB
BLOB
SemLabel
MAND, AU
MAND, SU
(d) Attack Pattern Application on Stripe Checkout
RA1
RA2
RA3
RA4
RA5
LCSRF
RedURI
REPLAY T oken FROM (UV, SPM) IN (UM, SPT). This attack pattern reports no attacks. When the attack test-case reaches step 10 of Figure 4a,
UV’s T oken which was actually issued for SPM is replayed by UM against SPT. The TTP Stripe identiﬁes a mismatch between the owner of Secret
and the SP for which T oken was issued and returns an error status at step 12.
REPLAY DataKey FROM (UM, SPM) IN (UM, SPT). No attacks reported. Similar reasons as the previous one: the attacker replays DataKey
belonging to SPM in the checkout session at SPT. Hence the T oken returned by TTP cannot be used by SPT to receive a success status at step 12.
REPLAY T oken FROM (UM, SPT) IN (UM, SPT). No attack reported. In Stripe checkout, the validity of a T oken expires once it is used. Reuse
of T oken returns an error.
REPLAY DataKey FROM (UM, SPT) IN S where S = REPLAY T oken FROM S IN (UM, SPT). This attack pattern reports an attack as there
is no protection mechanism in the Stripe checkout solution that prevents spooﬁng of the DataKey by another SP. Initially, the attack test case
replays the DataKey from (UM, SPT) into (UV, SPM). When the T oken obtained in this session by SPM is replayed into session (UM, SPT),
Stripe does not identify any mismatch and returns a success status at step 12. This allows the attacker UM to impersonate UV and to purchase a
product at SPT.
This attack strategy is not applicable to Stripe as there are no elements with data ﬂow TTP-SP that also have REQUESTURL as location (basically
none of those elements would be present in the browser history).
REPLACE req WITH REQUEST-OF T oken FROM (UM, SPT) IN [UM SEND req].
This pattern detects an attack. The test case generated sends a HTTP POST request corresponding to step 10 with an unused T oken. This request
alone is enough to complete the protocol and to uncover a CSRF. In our experiment, this was discovered on the demo implementation of Stripe.
Indeed it is not unusual that this kind of protections are missing in the demo systems. We do not know whether any productive MPWAs suffer from
this. Determining this would require speciﬁc testing users on the productive system and the buying of real products.
This pattern is not applicable as there are no URIs that have data ﬂow TTP-SP and semantic property RURI.
Fig. 4: Security Testing Framework on an illustrative example
8
WebDriver [13] and Zest [17] can be used for recording UAs.
Such technology could be extended to allow the tester to deﬁne
Flags by simply clicking on the web page elements (e.g., the
payment conﬁrmation form) that identify the completion of
the user actions. Off-the-shelf market tools already implement
this kind of feature to determine the completion of the login
operation.
(P3) Inference. The inference module automatically ex-
ecutes the nominal sessions recorded in the previous phase
and tags the elements in the resulting HTTP trafﬁc with the
labels in Figure 2. We do not exclude that in the future more
information (e.g., inference of the observable workﬂow of
the MPWA [32]) could be necessary to target more complex
attacks. While we borrow the idea of inferring the syntactic
and semantic properties from [36] and [32], we introduce the
concept of inferring ﬂow labels to make our approach more
automatic (compared to [36]) and efﬁcient (less no. of test
cases for detecting the same attack mentioned in [32]).
The inference results of sessions S1 to S4 are stored in a
data structure named labeled HTTP trace.
(P4) Application of Attack Patterns. Labeled HTTP
traces (output of inference) are used to determine which attack
patterns shall be applied and corresponding attack test cases
are executed against the MPWA.
(P5) Reporting. Attacks (if any) are reported back to the
tester and the tester evaluates the reported attacks.
V.
IMPLEMENTATION
•
in short). In this way,
We implemented our approach on top of OWASP ZAP
(ZAP,
the two core phases of our
testing engine (cf. P3 and P4 in previous section) are fully
automated and take advantage of ZAP to perform common
operations such as execution of UAs, manipulating HTTP
trafﬁc using proxy rule, regular expression matching over
HTTP trafﬁc, etc. Figure 5 outlines the high-level architecture
of our testing engine. The Tester provides the necessary input
to our Testing Engine that in turns employs OWASP ZAP to
probe the MPWA.5 In particular, the Testing Engine invokes
the API exposed by ZAP to perform the following operations:
(Execute user actions and collect HTTP traces.)
UAs, expressed as Zest script, can be executed via
the Selenium WebDriver module in ZAP and the
corresponding HTTP trafﬁc can be collected from
ZAP.
(Proxy rule setting.) Proxy rules can be speciﬁed, as
Zest scripts, to mutate HTTP requests and response
passing through the built-in proxy of ZAP.
(Evaluate Flag.) Execute regular expression-based
pattern matching within the HTTP trafﬁc so to, e.g.,
evaluate whether the Flag is present in the HTTP
trafﬁc.
Hereafter, we detail the two core phases (P3 and P4) of our
Testing Engine that follow the ﬂow depicted in Figure 6. Each
step is tagged by a number to simplify the presentation of the
ﬂow.
1) Inference: With reference to the steps of Figure 6, the
following activities are performed by the inference module
after the tester records (step 1) the four (cid:104)UAs, Flag(cid:105) corre-
sponding to sessions S1, S2, S3, and S4 in (P2).
•
•
5The “R” with the small arrow is a short notation of the request-response
channel pair that clariﬁes who are the requester and the responder of a generic
service.
9
Fig. 5: Testing Engine Architecture
Trace collection (steps 2-3) The input UAs are executed
and corresponding HTTP traces are collected. The Flags are
used to verify whether the collected traces are complete. We
represent the collected HTTP traces as HT (S1), HT (S2),
HT (S3), and HT (S4). The traces are stored as an array
of (cid:104)request, response, elements(cid:105) triplets. Each triplet com-
prises the HTTP request sent via ZAP to the MPWA, the
corresponding HTTP response, and details about the HTTP
elements exchanged. An excerpt of a trace related to our illus-
trative example (Figure 4a) is depicted in Figure 7 in JSON for-
mat. For simplicity, we present only one entry of the trace array
and only one HTTP element. We assume the reader is familiar
with standard format of the HTTP protocol. Here we focus
on the HTTP elements. For each of them we store the name
(“name”), the value (“value”), its location in the request/re-
sponse (“source”, e.g., source:"request.body" indi-
cates that the element occurs in the request body of the HTTP
request), the associated request URL (“url”), its data ﬂow
patterns, syntactic and semantic labels that are initially empty
and will be inferred in the next activities. For instance, the
element illustrated in Figure 7 is the T oken shown in step 10
of Figure 4a.
Syntactic and Semantic Labeling (steps 4-10) The collected
HTTP traces are inspected to infer the syntactic and seman-
tic properties of each HTTP element, reported in Figure 2.
While syntactic labeling is carried out by matching the HTTP
elements against simple regular expressions, semantic labeling
may require (e.g., for MAND) active testing of the MPWA.
For instance, to check whether an element e occurring in
HT (UM, SPT) is to be given the label MAND, the inference
module generates a proxy rule that removes e from the HTTP
requests (step 6). By activating this proxy rule (step 7), the
inference module re-execute the UA corresponding to the
session (UM, SPT) and checks whether the corresponding Flag
is present in the resulting trace (steps 8-9). For instance, the
element T oken (see Figure 7) is assigned the syntactic labels
BLOB and the semantic labels SU and MAND.
Fig. 6: Testing Engine Flow
Data Flow Labeling (step 11) After syntactic and semantic
labeling, the data ﬂow properties of each MAND element in
the trace is analyzed to identify the data ﬂows (either TTP-
SP or SP-TTP). In order to identify the protocol patterns,
it is necessary to distinguish TTP and SP from the HTTP
trace. We do this by identifying the common domains present
in the HTTP trace of the two different SPs (SPT and SPM)
implementing the same protocol and classifying the messages
from/to these domains as the messages from/to TTP.
The output of the inference phase is the labeled HTTP
traces of sessions S1 to S4 (represented as LHT (S1),
LHT (S2), LHT (S3), and LHT (S4)).
2) Attack Pattern Engine: For the simplicity of explanation,
we represent our attack patterns in the same way as the attack
graph notation introduced in [33]. Each attack pattern has a
Name, the underlying Threat model, Inputs used, the
Goal the attacker (who follows the attack strategy deﬁned in
the pattern) aims to achieve, Preconditions, Actions
and Postconditions. The Inputs to the attack pattern
range over the LHTs (labeled HTTP traces generated by the
inference module), UAs of the nominal sessions, and the cor-
responding Flags. The Goal, Preconditions, Actions
and Postconditions are built on top of the Inputs.
The pattern is applicable if and only if its Preconditions
hold (steps 12-14 of Figure 6). As soon as the pattern
Preconditions hold, the Actions are executed (steps
15-17 of Figure 6). The Actions contain the logic for
generating proxy rules that mimics the attack strategy. The
generated proxy rules are loaded in ZAP and UAs are ex-
ecuted. The execution of UAs generates HTTP requests and
responses. The proxy rules manipulates the matching requests
and responses. As last step of the Actions execution, the
Postconditions are checked. If they hold (step 18 of
Figure 6), an attack report is generated with the conﬁguration
that caused the attack (step 19 of Figure 6).
Example on Attack Pattern for RA1. To illustrate,
let us
consider the Replay Attack pattern RA1 reported in Table III.
In Listing 1, we show the pseudo-code describing it.
The Threat model considered is the web attacker. To
evaluate the applicability of the pattern, the output of the
inference phase is sufﬁcient (LHT (UV, SPM)):
the attack
10
Listing 2: Extract function
v a l u e
e x t r a c t ( i d x , uas UAs){
r b = g e n e r a t e b r e a k r u l e ( x )
load rule ZAP ( r b )
HTTP logs = execute ZAP ( UAs )
e = e x t r a c t v a l u e ( x , HTTP logs )
c l e a r
r e t u r n e}
r u l e s Z A P
Listing 3: Replay function
HTTP logs
r e p l a y ( i d x , v a l u e e , uas UAs){
r r = g e n e r a t e r e p l a y r u l e ( x , e )
load rule ZAP ( r r )
HTTP logs = execute ZAP ( UAs )
r e t u r n HTTP logs}
1
2
3
4
5
6
7
1
2
3
4
5
ZAP. The ZAP API call execute ZAP(UAs) executes the UAs
in ZAP and returns the generated HTTP logs. The HTTP logs
are taken as input by the function extract value (x, HTTP logs)
extracting from them the value e, associated to x. In Listing 3,
the function generate replay rule (x, e) returns the proxy rule
rr used to detect and replace the value of the element x with e.
Then, the ZAP API call load rule ZAP(rule) loads rr in ZAP.
The ZAP API call execute ZAP(UAs) executes the UAs in ZAP
and returns the generated HTTP logs.
Notice that, besides the functions mentioned above,
in
order to help the security expert in deﬁning new attack patterns,
we provide several functions.6
VI. EVALUATION
To test
the effectiveness of our approach, we ran our
prototype implementation against a large number of real-
world MPWAs. In Section VI-A, we explain the criteria
based on which we selected our target MPWAs. Next,
in
Sections VI-B and VI-C, we explain the attacks we discovered
(both automatically and with manual support) and ﬁnally,
in Section VI-D, we provide some information on how we
(responsibly) disclosed our ﬁndings to the affected vendors.
A. Target MPWAs
We selected SSO, CaaS and VvE (see Figure 1c) scenarios
as the targets of our experiments. For the SSO scenario, we
adopted the Google dork strategy mentioned in [8] to identify
SPs integrating SSO solutions offered by LinkedIn, Instagram,
PayPal and Facebook. Additionally, we prioritized the Google
dorks results using the Alexa rank of SPs. For the CaaS
scenario, we targeted open-source e-commerce solutions and
publicly available demo SPs integrating 2Checkout and Stripe
checkout solutions. For the VvE scenario, we selected the
websites belonging to the Alexa Global Top 500 category.7
B. Results
We have been able to identify several previously unknown
vulnerabilities and they are reported in Table IV. We have
6The full list of functions that can be used in the deﬁnition of attack patterns
is available at https://sites.google.com/site/mpwaprobe.
7www.alexa.com/topsites
Fig. 7: HTTP trace with empty labels (an excerpt)
Listing 1: Attack Pattern for RA1
Name: RA1
Threat Model: Web Attacker
Inputs: UAs ( UV, SPM ) , LHT( UV, SPM ) ,
UAs ( UM, SPT ) , F l a g ( UV, SPT )
Preconditions: At least one element x in LHT(UV, SPM)
is such that (TTP-SP ∈ x.ﬂow AND (SU|UU) ∈x.labels)
Actions:
For each x such that preconditions hold
e = e x t r a c t ( x , UAs ( UV, SPM ) )
HTTP logs = r e p l a y ( x , e , UAs ( UM, SPT ) )
Check Postconditions;
Postconditions:Check F l a g ( UV, SPT )
in HTTP logs
R e p o r t ( e , UAs ( UM, SPT ) , F l a g ( UV, SPT ) )
1
2
3
4
5
6