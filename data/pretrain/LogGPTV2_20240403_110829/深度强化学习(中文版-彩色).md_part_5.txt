XXVIII
目录
总结部分 445
附录A 算法总结表 446
附录B 算法速查表 451
B.1 深度学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
B.1.1 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
B.1.2 Adam优化器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
B.2 强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
B.2.1 赌博机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
B.2.2 动态规划 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
B.2.3 蒙特卡罗 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
B.3 深度强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
B.4 高等深度强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
B.4.1 模仿学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
B.4.2 基于模型的强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
B.4.3 分层强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
B.4.4 多智能体强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
B.4.5 并行计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
附录C 中英文对照表 476
XXIX
基础部分
本书第一部分包括6个章节，介绍深度学习、强化学习及广泛应用的深度强化学习算法及其
实现。具体来说，前两章介绍深度学习和强化学习的基本概念，以及少量深度强化学习的基本知
识，这些内容对读者阅读后续章节非常重要。如果您已经掌握了这些基本知识，完全可以跳过这
两个章节。但我们还是建议您阅读第2章，这有助于熟悉本书的术语和数学公式。
第 3 章介绍了强化学习算法的分类，以帮助大家从不同的角度来对深度强化学习算法有全
局的认识。分类包括基于模型的（Model-Based）与无模型的（Model-Bree）方法、基于策略的
（Policy-Based）与基于价值的（Value-Based）方法、蒙特卡罗（Monte Carlo，MC）与时间差分
（TemporalDifference，TD）方法、在线策略（On-Policy）与离线策略（Off-Policy）方法，等等。
如果读者在阅读本书其他章节时，对算法的分类与属性有困惑，可回到第3章仔细思考。我们会
在第4～6章详细介绍一些常见的深度强化学习算法，通过实例代码帮助大家深入理解算法的细
节和实现技巧。
1
1
深度学习入门
深度学习是深度强化学习的重要构成部分。本章将首先简要介绍深度学习的基础知识，会从
简单的单层神经网络开始，逐渐引入更加复杂且学习能力更强的神经网络模型，比如卷积神经网
络和循环神经网络的模型。本章在最后将提供一些代码样例，用于介绍深度学习的实现过程。
1.1 简介
如果您已经非常熟悉深度学习，则可以从第 2 章开始阅读。如果您想对深度学习中的部分
内容进行深入的学习和了解，推荐您参阅其他相关图书，例如 Pattern Recognition and Machine
Learning(Bishop,2006)和DeepLearning(Goodfellowetal.,2016)。与经典强化学习不同的是，深
度强化学习是基于深度学习模型，即深度神经网络，来利用大数据和高性能计算强大优势的。我
们可以大致将深度学习模型分为以下两大类。
判别模型用于建模条件概率p(y|x)，其中x代表输入数据，而y 代表输出目标。也就是说，
判别模型基于输入数据x，预测相对应的标签y。顾名思义，判别模型大多应用于需要进行判断
的任务，例如，分类任务和回归任务。具体来说，在分类任务中，模型需要根据输入数据从备选
类别中选择正确的目标类别。如果一个任务中仅有两个备选类别且模型只需要从中选取一个正确
的目标类别，则为二分类任务，是最为基本的分类任务。例如，在情感分析中(Maasetal.,2011)，
根据文本内容，判断文本表达了正面的情绪还是负面的情绪，即二分类任务。与之相对应的，在
多标签分类任务中，备选类别中可能同时有多个正确的目标类别。
在很多情况下，一个分类模型并不直接指定目标类别，而会给每一个备选类别计算一个概率。
例如，模型根据某个数据样例，认为它有80%的概率来自类别A，而另有15%的概率来自类别
B，5%的概率来自类别C。之所以使用这种基于概率的表征，主要是为了便于在训练阶段对模型
2
1.2 感知器
进行优化。深度学习已经在很多像图像分类(Krizhevskyetal.,2009)和文本分类(Yangetal.,2019)
的分类任务上取得了巨大的成功。
分类任务的输出均为离散的类别标签，而回归任务则不同。回归任务的输出是连续的数值，
如利用过去的交通数据来预测未来一段时间内的车速(Liaoetal.,2018a,b)。只要回归模型是基于
条件概率建模的，我们就认为它是判别模型。
生成模型用于建模联合概率 p(x,y)。生成模型通常对可观测数据的分布进行建模，从而达
到生成可观测数据的目的。生成对抗网络（GenerativeAdversarialNetworks，GANs）(Goodfellow
etal.,2014)就是这样一个例子，它被用于生成图像、重构图像和对图像去噪。然而，类似于GANs
的深度学习技术与可观测数据的分布并没有显式的关系，因为深度学习技术更关注生成的样本和
可观测的真实样本之间的相似程度。与此同时，像朴素贝叶斯（NaiveBayes）的生成模型也用于
解决分类任务 (Ng et al., 2002; Rish et al., 2001)。尽管生成模型和判别模型都可以用于解决分类
任务，判别模型关注的是哪一个标签更适合可观测数据，而生成模型则尝试建模可观测数据的分
布。下面举两个例子来说明它们的不同。朴素贝叶斯对似然概率（Likelihood）p(x|y) 建模，也
就是可观测数据在给定标签情况下的条件概率。生成模型先学会创造数据，再去学习如何判别数
据，当学习了联合概率分布p(x,y)后即可以学会判别，比如给定观测输入x，输出目标为1的概
率为p(y =1|x)= p(x,y=1)。
p(x)
大多数深度神经网络都是判别模型，无论其目的是用于判别类任务还是生成类任务。这是因
为很多生成类任务在具体实现中都可以简化为分类或者回归问题。例如，问答系统(Devlinetal.,
2019)可以简化为根据问题选择文本中相应的段落；自动摘要(Zhangetal.,2019b)可以简化为从
词表中根据概率选择单词，并组合成摘要。在这两种场景下，它们都在尝试生成文本，但是一个
使用了分类的方法，另一个则使用了回归的方法。
具体来说，本章将介绍深度学习相关的基本元素和技术，例如构造深度神经网络必需的神经
元、激活函数和优化器等，同时将介绍深度学习相关的应用。本章也将介绍基础的深度神经网络，
例如多层感知器（Multilayer Perceptron，MLP）、卷积神经网络（Convolutional Neural Networks，
CNNs），以及循环神经网络（RecurrentNeuralNetworks，RNNs）。最后，1.10节将基于TensorFlow
和TensorLayer介绍深度神经网络的实现样例。
1.2 感知器
单输出
神经元或节点是深度神经网络最基本的单元。神经元的概念最初是基于大脑中生物神经元提
出的，也是生物神经元的一种抽象表示。在大脑中，生物神经元通过树突接受电信号，当生物神
经元被激活后，通过轴突将电信号传播给其他附近的生物神经元。在真实的生物系统中，神经元
的信息传递并不是在一瞬间发生的，而是需要经过一步一步传递的过程，这个过程可以形象地理
3
第1章 深度学习入门
解成激活一个神经网络。当前，深度学习的研究更多地依赖深度神经网络（DeepNeuralNetworks，
简称DNNs），亦称人造神经网络（ArtificialNeuralNetworks，简称ANNs）。深度神经网络中的神经
元的输入和输出都是数值。一个神经元可以跟下一层的多个神经元同时相连，也可以跟上一层的
多个神经元同时相连。具体来说，每个神经元将上一层神经元的输出进行聚合，再通过激活函数
决定其最终的输出。如果这些聚集的输入信号足够强，那么这个激活函数将会“激活”（Activate）
这个神经元，然后这个神经元会将一个有高数值的信号传递给下一层网络。相对地，如果输入信
号不够强，那么一个低数值信号将被传递下去。
一个神经网络可以有任意多个神经元，而这些神经元彼此可以有很多随机的连接。但是为了
运算更加容易，神经元往往是层层递进的。一般来说，一个神经网络至少会有两层：输入层和输
出层（见图1.1）。这个网络可以被公式(1.1)描述，它可以做一些简单的决定任务，比如帮助几
个学生根据天气的情况具体决定他们是否外出踢足球，网络输出的z是一个分数，分数越高则代
表越可以去踢足球。这个分数取决于三个因素：1）足球场的使用费用x ；2）天气x ；3）去球
1 2
场的时间x 。如果天气对大家做这个决定比较重要，则其相对应的网络权重w 会有较大的绝对
3 2
值。同样地，那些对做这个决定影响较小的因素，所对应的网络权重的绝对值就会较小。如果一
个权重被设置为零，那么它所对应的输入就对最终的结果完全没有影响。比如，有的学生有钱，
不在乎足球场的费用，则w 为0。我们把具有这样结构的网络叫作单一层网络，也叫作感知器
1
（Perceptron）。
z =w x +w x +w x (1.1)
1 1 2 2 3 3
输入层 输出层
图1.1 有三个输入神经元和一个输出神经元的神经网络
偏差与决策边界
偏差（Bias）是神经元所附带的一个额外的标量，用来偏移神经网络的输出。图1.2所示的
一个有偏差b的单层神经网络可以用公式(1.2)表达：
4
1.2 感知器
z =w x +w x +w x +b (1.2)
1 1 2 2 3 3
图1.2 一个有偏差的单层神经网络
偏差可以帮助一个神经网络更好地学习数据。我们不妨定义以下二分类问题：对于输出 z，
当且仅当z为正数，其所对应的标签y为1，反之为0：
8
>0
y = (1.3)
>:
0 z ⩽0
二分类任务的样本数据分布例子如图1.3所示。我们现在需要找到最符合这些数据的权重和
偏差。我们把这些样本数据分成两个不同的类别的边界定义为决策边界。正式来说，这个边界是
{x ,x ,x |w x +w x +w x +b=0}。
1 2 3 1 1 2 2 3 3
我们首先把这个问题简化到只有两个输入的情况下，即 z = w x +w x +b。如图 1.3 左
1 1 2 2
所示，如果没有偏差值，也就是说b=0，那么决策边界必须穿过坐标系的原点（左下的线）。但
是，这样很明显不符合数据的分布，因为我们的数据点都是在这个边界的一侧。如果偏差值不是
0，那么决策边界与两个轴的交点就为(0,− b )和(− b ,0)。这样来看，如果我们的权重和偏差
w2 w1
值选得好，那么决策边界就能更好地符合数据分布。
进一步来说，当一个神经元有三个输入的时候，z = w x +w x +w x +b，此时的边界
1 1 2 2 3 3
就会变成如图1.3右所示的平面。在一个如单层神经网络（见公式(1.2)）的线性模型中，这样的
一个平面也被称为超平面（Hyperplane）。
5
第1章 深度学习入门
O O
图1.3 线性模型分别在两个输入和三个输入场景下的决策边界。左：z = w x +w x +b。右：
1 1 2 2
z =w x +w x +w x +b。若没有偏差，则决策边界必须经过原点，不能很好地分类
1 1 2 2 3 3
多输出
单层神经网络可以有多个神经元。图 1.4 展示了一个有两个输出神经元的单层网络，由公
式(1.4)所得。因为每一个输出都和全部输入相连，所以输出层也被称为密集层（DenseLayer）或
者全连接层（Fully-Connected(FC)Layer）：
z =w x +w x +w x +b
1 11 1 12 2 13 3 1
z =w x +w x +w x +b (1.4)
2 21 1 22 2 23 3 2
𝑏
#
𝑥 𝑤##
#
𝑤$# z
1
𝑤#$
𝑥
$ 𝑤$$
z
𝑤#% 2
𝑤$%
𝑥
%
𝑏
$
图1.4 一个有三个输入和两个输出的神经元的神经网络