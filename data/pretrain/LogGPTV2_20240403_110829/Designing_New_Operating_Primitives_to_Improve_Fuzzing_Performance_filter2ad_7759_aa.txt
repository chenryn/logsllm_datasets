title:Designing New Operating Primitives to Improve Fuzzing Performance
author:Wen Xu and
Sanidhya Kashyap and
Changwoo Min and
Taesoo Kim
Designing New Operating Primitives to Improve
Fuzzing Performance
Wen Xu Sanidhya Kashyap Changwoo Min† Taesoo Kim
Georgia Institute of Technology Virginia Tech†
ABSTRACT
Fuzzing is a software testing technique that finds bugs by repeatedly
injecting mutated inputs to a target program. Known to be a highly
practical approach, fuzzing is gaining more popularity than ever
before. Current research on fuzzing has focused on producing an
input that is more likely to trigger a vulnerability.
In this paper, we tackle another way to improve the performance
of fuzzing, which is to shorten the execution time of each itera-
tion. We observe that AFL, a state-of-the-art fuzzer, slows down by
24× because of file system contention and the scalability of fork()
system call when it runs on 120 cores in parallel. Other fuzzers
are expected to suffer from the same scalability bottlenecks in that
they follow a similar design pattern. To improve the fuzzing perfor-
mance, we design and implement three new operating primitives
specialized for fuzzing that solve these performance bottlenecks
and achieve scalable performance on multi-core machines. Our
experiment shows that the proposed primitives speed up AFL and
LibFuzzer by 6.1 to 28.9× and 1.1 to 735.7×, respectively, on the
overall number of executions per second when targeting Google’s
fuzzer test suite with 120 cores. In addition, the primitives improve
AFL’s throughput up to 7.7× with 30 cores, which is a more common
setting in data centers. Our fuzzer-agnostic primitives can be easily
applied to any fuzzer with fundamental performance improvement
and directly benefit large-scale fuzzing and cloud-based fuzzing
services.
CCS CONCEPTS
• Security and privacy → Vulnerability scanners; • Software
and its engineering → Software testing and debugging;
KEYWORDS
fuzzing; scalability; operating system
1 INTRODUCTION
Attackers exploit vulnerabilities to gain complete or partial control
of user devices or achieve user privacy. In order to protect users
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134046
from malicious attacks, various financial organizations and com-
munities, that implement and use popular software and OS kernels,
have invested major efforts on finding and fixing security bugs in
their products, and one such effort to finding bugs in applications
and libraries is fuzzing. Fuzzing is a software-testing technique
that works by injecting randomly mutated input to a target pro-
gram. Compared with other bug-finding techniques, one of the
primary advantages of fuzzing is that it ensures high throughput
with less manual efforts and pre-knowledge of the target software.
In addition, it is one of the most practical approach to finding
bugs in various critical software. For example, the state-of-the-art
coverage-driven fuzzer, American Fuzzy Lop (AFL), has discovered
over thousand vulnerabilities in open source software. Not only that,
even the security of Google Chrome heavily relies on its fuzzing
infrastructure, called ClusterFuzz [3, 20].
Fuzzing is a random testing technique, which requires huge
computing resources. For example, ClusterFuzz is a distributed
infrastructure that consists of several hundred virtual machines
processing 50 million test cases a day. The recently announced OSS-
Fuzz [22], Google’s effort to make open source software more secure,
is also powered by ClusterFuzz that processes ten trillion test inputs
a day on an average, and has found over one thousand bugs in five
months [14]. Besides Google, Microsoft proposed Project Spring-
field [31] that provides a cloud-based fuzzing service to developers
for security bug finding in their software. This movement clearly
indicates that large-scale fuzzing is gaining popularity [1, 25].
By looking at the complex software stacks and operating systems,
the performance of a fuzzer is critical. In other words, a better fuzzer
likely hits more security bugs in the target program more quickly
than other fuzzers. There are two broad research directions to tackle
this problem: 1) producing an input that is more likely to trigger
a vulnerability (i.e., shorter time required to reach a bug); and 2)
shortening the execution time of each iteration (i.e., more coverage
at a given time).
Prior research has mainly focused on the first approach. In par-
ticular, coverage-driven fuzzers [21, 29, 41] evaluate a test case by
the runtime coverage of a target program, and then try to gener-
ate test cases that are likely to cover untouched branches. More
advanced techniques include an evolutionary technique [12, 34], a
statistical model such as Markov chain [7], or combine fuzzing with
symbolic execution [13, 27, 36]. However, shortening the execution
time of each iteration of fuzzing also brings huge benefits. This
approach reduces the time to find a new bug given a fuzzing strat-
egy. Nowadays it takes fuzzers days, weeks or even months to find
an exploitable vulnerability in popular software due to its internal
complexity and security mitigations. Thus a slight improvement on
Session K2:  Fuzzing Finer and FasterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2313the performance makes a big difference, and the huge operational
costs of fuzzing are thereby saved.
A critical performance issue is that current fuzzers are not at
all scalable on commodity OSes with manycore architectures that
are readily available today (e.g., 2-4 sockets with 16-64 physical
cores). As Figure 2 shows, the performance scalability of fuzzing
is surprisingly poor. Most of the CPU cycles are wasted because
of contention in underlying OS, which degrades the scalability of
fuzzing with no more than 30 CPU cores. This is conceptually
counter-intuitive for fuzzing tasks since each fuzzing instance runs
independently without explicit contention.
We found that many state-of-the-art fuzzers [7, 12, 21, 29, 34,
38, 41] have a similar structure (i.e., launching and monitoring the
target application, creating and reading test cases, and optionally
syncing test cases among fuzzer instances in an iteration of fuzzing).
To complete these involved steps, they extensively rely on several
OS primitives while fuzzing a target application, which are the root
causes of the similar performance and scalability bottlenecks they
suffer from.
First, in each fuzzing iteration of a state-of-the-art fuzzer such as
AFL, it invokes fork() to clone the target application for a fresh run.
However, spawning processes simultaneously on multiple cores
by using the fork() system call is not scalable [8, 9, 11]. fork() is
designed to duplicate any running process. In the context of fuzzing,
a large portion of operations in fork() is repeated but has the same
effects, as the target process never changes.
Second, a fuzzer instance intensively interacts with the file sys-
tem by creating, writing, and reading small test cases for each run.
These file operations cause heavy updates to the metadata of the
file system, which is not scalable in terms of parallel fuzzing on
multiple cores [32].
Last but not least, existing fuzzers including AFL and LibFuzzer
share test cases among fuzzer instances in parallel fuzzing. A fuzzer
instance periodically scans the output directories of other instances
to discover new test cases. Moreover, it re-executes these external
test cases to determine whether they are interesting or not. The
number of directory enumeration operations and executions of the
target application increases non-linearly with more fuzzers and
longer fuzzing, which causes the third bottleneck.
In this paper, we propose three operating primitives for fuzzers
to achieve scalable fuzzing performance on multi-core machines.
These three primitives are specific to fuzzing and they aim at solving
the three performance bottlenecks described above. In particular,
we propose 1) snapshot(), a new system call which clones a new
instance of the target application in an efficient and scalable manner;
2) dual file system service, which makes fuzzers operate test cases
on a memory file system (e.g., tmpfs) for performance and scalability
and meanwhile ensures capacity and durability by a disk file system
(e.g., ext4); 3) shared in-memory test case log, which helps fuzzers
share test case execution information in a scalable and collaborative
way.
In this paper, we make the following contributions:
• We identify and analyze three prominent performance bot-
tlenecks that stem in large-scale fuzzing and they are caused
by the intensive use of existing operating primitives that are
only better for the general purpose use.
Figure 1: Overview of the AFL design. 1 Read/sync a candidate test
case from own/other’s output directory into a buffer, and mutate
the buffer for a new input and feed it to the target process; 2 (tar-
get process) fork a child process to execute the program with vis-
ited paths recorded in the tracing bitmap; 3 (target process) wait
for the child to terminate and send back its exit status; 4 save the
generated input into the output directory if it covers new paths by
observing the shared tracing bitmap; 5 repeat this fuzzing process;
and 6 on multi-core systems, each AFL instance run independently
in parallel.
• We design and implement three new fuzzing specific op-
erating primitives that can improve the performance and
scalability for the state-of-the-art fuzzers in a multi-core
machine.
• We apply and evaluate our proposed operating primitives
to AFL and LibFuzzer. By leveraging our proposed primi-
tives, AFL has at most 7.7×, 25.9×, and 28.9× improvement
on the number of executions per second on 30, 60, and 120
cores, respectively. Meanwhile, LibFuzzer can speed up by
at most 170.5×, 134.9×, and 735.7× on 30, 60, and 120 cores
respectively.
§2 describes the roles of operating primitives in fuzzing and how
they become critical performance bottlenecks. The design of new
operating primitives specialized for fuzzing is proposed in §3, and
§4 describes how new primitives help the state-of-the-art fuzzers
scale on multiple cores. §5 mentions the related implementation
details, and the evaluation of the proposed primitives is illustrated
in §6. The related works are listed in §7. §9 concludes the paper.
2 BACKGROUND AND MOTIVATION
In this section, we first describe how modern fuzzers work (§2.1)
and then explain how and why operating primitives, on which
fuzzers rely, become critical scalability bottlenecks (§2.3, §2.4).
2.1 Fuzzing Explained
Fuzzing is a widely used software-testing technique to find bugs in
applications. It tests software by injecting randomly mutated input
to a target program and monitors whether the target behaves abnor-
mally (e.g., crashed, raced, hanged, or failed on assertions). The in-
put that triggers an abnormal behavior is reported as a potential bug.
To detect bugs quickly, various techniques [7, 13, 20, 29, 34, 36, 41]
2
core 0core 1core N...aflafl0appafl1appaflNappafl drivertarget appfork/exec()wait()exit()sharedjnz destlog bitmap...dest:❶bitmaptest casecheck/saveload/mutatethe test case❷❸❹❺loop(instrumented)/outN/out1/out0privatedirectoryinstances❻Session K2:  Fuzzing Finer and FasterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2314have been proposed to wisely mutate inputs with smart policies and
efficiently explore the program state. In particular, popular fuzzers
such as AFL [41] and LibFuzzer [29] use the past code coverage to
determine whether the current mutated input is interesting, and
use it as a feedback for the next execution. To get coverage infor-
mation, fuzzers require either instrumentation (if the source code is
available) or a system emulation layer (for binaries) such as QEMU.
In general, a fuzzer starts with a set of seed inputs (also known as
a corpus), runs a target program, and mutates the input based on
the feedback (e.g., coverage or crash) from the past execution. This
whole process is known as a fuzzing loop, and a fuzzer can iterate
for a certain amount of time or until it reaches a saturation point
(alas, indefinitely in most cases). Precisely, a fuzzing loop consists
of the following procedures:
(1) Load a test case (or input) with the highest priority from a
disk to a memory buffer.
(2) Mutate the buffered input by randomly modifying certain
bytes in the input or appending random bytes to the end of
the input.
(3) Launch the target application with the newly generated in-
put, and monitor its runtime execution.
(4) If the test case is interesting, save the input to the disk as a
new test case for further mutation in successive runs (e.g.,
explored a new branch).
(5) Repeat step (1) for another fuzzing iteration.
When multiple instances of fuzzers run in parallel, every instance
performs an additional syncing phase for exchanging useful test
cases among instances. After executing a certain number of fuzzing
loops, a fuzzer periodically checks any new test cases generated
by other fuzzers and re-executes some of them to decide whether
they are interesting to the fuzzer itself, meaning that they cover
any execution path or basic block that the fuzzer has yet to discover.
These interesting ones are saved in the private corpus directory of
the fuzzer.
2.2 Design of AFL
AFL is the state-of-the-art fuzzer, which discovers numerous security-
crucial bugs of non-trivial, real-world software. We now focus on
explaining the concrete design of AFL and its design considerations
on performance and scalability for multi-core systems. We illustrate
its overall design and workflow in Figure 1.
Mutating inputs ( 1 ). AFL uses an evolutionary coverage-based
mutation technique to generate test cases for discovering new ex-
ecution paths of the target application [42]. In AFL, an execution
path is represented as a sequence of taken branches (i.e., a coverage
bitmap) in the target instance for a given input. To track whether
a branch is taken, AFL instruments every conditional branch and
function entry of the target application at the time of compilation.
Launching the target application ( 2 ). Traditional fuzzers call
fork() followed by execve() to launch an instance of the target
application. This process occurs in every fuzzing loop to deliver a
new input to the target application. It is not only time consuming,
but also a non-scalable operation. Previous research shows that
the majority of fuzzing execution explores only the shallow part
of the code and terminates quickly (e.g., because of invalid input
format), which results in frequent executions for the input test
3
cases. Thus, the cost of fork() and execve() dominates the cost of
fuzzing [7, 34, 36]. To mitigate this cost, AFL introduced a fork server,
which is similar to a Zygote process in Android [39] that eliminates
the heavyweight execve() system call. After instantiating a target
application, the fork server waits for a starting signal sent over the
pipe from the AFL instance. Upon receiving the request, it first clones
the already-loaded program using fork() and the child process
continues the execution of the original target code immediately
from the entry point (i.e., main) with a given input generated for the
current fuzzing loop. The parent process waits for the termination
of its child, and then informs the AFL process. The AFL process
collects the branch coverage of the past execution, and maintains
the test input if it is interesting.
Bookkeeping results ( 3 , 4 ) The fork server also initializes a
shared memory (also known as tracing bitmap) between the AFL
instance and the target application. The instance records all the
coverage information during the execution and writes it to the
shared tracing bitmap, which summarizes the branch coverage of
the past execution.
Fuzzing in parallel ( 6 ). AFL also supports parallel fuzzing to
completely utilize resources available on a multi-core machine and
expedite the fuzzing process. In this case, each AFL instance inde-
pendently executes without explicit contention among themselves
(i.e., embarrassingly parallel). From the perspective of the design
of AFL, the fuzzing operation should linearly scale with increasing
core count. Moreover, to avoid apparent contention on file system
accesses, each AFL instance works in its private working directory
for test cases. At the end of a fuzzing loop, the AFL instance scans
the output directories of other instances to learn their test cases,
called the syncing phase. For each collaborating neighbor, it keeps a
test case identifier, which indicates the last test case it has checked.
It figures out all the test cases that have an identifier larger than the
reserved one, and re-executes them one by one. If a test case covers
a new path that has not been discovered by the instance itself, the
test case is copied to its own directory for further mutation.
2.3 Perils to Scalable Fuzzing
During a fuzzing loop, fuzzers utilize several OS primitives such as
fork() and file operations, as described in §2.1 and §2.2. Unfortu-
nately, when scaling fuzzers to run multiple instances in parallel,
these primitives start to become performance bottlenecks, resulting
in a worse end-to-end performance than that of a single fuzzing
instance. We now give an in-depth detail of the potential system
bottlenecks in each phase of the fuzzing process.
Cloning a target application. Every fuzzing execution requires
a fresh instance of a target application to test a generated input.
Most existing fuzzers use the fork() system call to quickly clone
the target application, i.e., for each fuzzing run, the parent process
clones a new target instance that starts in an identical process state
(e.g., virtual memory space, files, sockets, and privileges).
Bottlenecks. From the performance aspect, there are two prob-
lems with the fork() system call. First, an OS repeatedly per-
forms redundant operations in fork() that are neither used nor
necessary for executing a target application during the fuzzing
loop. Second, concurrent spawning of processes by using the
Session K2:  Fuzzing Finer and FasterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2315fork() system call is not scalable because of various con-
tentions (e.g., spinlocks) and standard requirements (e.g., PID
should be assigned in an incremental order in POSIX). These
operations are required for a general-purpose fork() in an OS,
but significantly deter the scalability of fuzzing. For example,
fork() needs to update the reverse mapping of a physical page
for swapping under memory contention, which is a well-known
scalability bottleneck in the current Linux kernel [8, 9, 11].
Moreover, fork() stresses the global memory allocator for allo-
cating various metadata required for a new task, needs to set up
security and auditing information, and has to add the task to the
scheduler queue, which are all known to be non-scalable with
increasing core count. Hence, none of the above operations are
necessary and important for the purpose of fuzzing.
Creating a mutated test case. Each cloned process runs with
a different, freshly mutated test case, to discover new paths and
crashes. To support a variety of applications, existing fuzzers store
a test case as a standard file and pass it to the target application,
either as a command line argument or via standard input. At the end
of a fuzzing loop, fuzzers store interesting test cases on the disk and
fetch them later to generate mutated test cases for the next run. The
number of test cases stored in disk increases monotonically until
fuzzers terminate because any saved test case can be evolved again
by mutation in a later fuzzing loop. Therefore, the most common
practice is to “keep the test cases as small as possible” (e.g., 1 KB
is ideal for AFL [43]) because this minimizes file I/O as well as the
search space for mutation, which keeps typical test cases to merely
hundreds of bytes. At every run, fuzzers heavily interact with the
file system to manage test cases.
Bottlenecks. Typical file system operations that fuzzers rely on
are, namely, open/creat (for generating the mutated test case),
write (for flushing interesting test cases), and read (for loading
test cases) of small files in each fuzzing loop, importantly in
parallel. Two benchmarks in FxMark [32] can be used to explain
in detail the scalability of the fuzzer: MWCL (i.e., creating a
file in a private directory) and DWOL (i.e., writing a small