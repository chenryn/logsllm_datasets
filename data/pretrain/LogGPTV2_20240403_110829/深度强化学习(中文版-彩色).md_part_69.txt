source activate opensim-rl
你需要在每次打开一个新的终端时输入上面的命令。
3. 安装我们的Python强化学习环境。
conda install -c conda-forge lapack git
pip install osim-rl
346
13.2 训练智能体
自从2017年以后，这个挑战已连续举办了三年（至2019年）。因而，最初的 Learning to
Run环境由于版本更新已经被废弃。虽然如此，我们仍旧选择用这个原始的2017版本环境来做
示范，因为它相对简单。于是，在我们的项目中提供了一个仓库存放2017版本的环境：
git clone
https://github.com/deep-reinforcement-learning-book/Chapter13-Learning-to-Run.git
我们所用的强化学习算法代码和环境的封装也都在上述仓库中提供。
通过以上几步，我们已经完成了环境的安装，可以通过以下命令检验安装是否成功：
python -c "import opensim"
如果它能正常运行，说明安装已成功；否则，可以在这个网站找到解决方案。
要用随机采样执行 200次模拟迭代，我们可以用Python解释器运行以下命令（在Linux环
境）：
from osim.env import RunEnv # 导入软件包
env = RunEnv(visualize=True) # 初始化环境
observation = env.reset(difficulty = 0) # 重置环境
for i in range(200): # 采集样本
observation, reward, done, info = env.step(env.action_space.sample())
这个环境由于已被写成OpenAIGym游戏的格式，对用户十分友好，而且有一个定义好的奖
励函数。我们的任务是得到一个从当前观察量（一个 41 维矢量）到肌肉激活动作（18 维矢量）
的映射函数，使得它能够最大化奖励值。如前所述，奖励函数被定义为一个迭代步中骨盆沿x轴
的位移减去韧带受力大小，从而尽可能鼓励智能体在最小身体损耗的情况下向前移动。
13.2 训练智能体
为了更好地解决这个任务，在训练框架中需要实现一系列技巧，包括：
• 一个可以平衡CPU和GPU资源的并行训练框架；
• 奖励值缩放；
• 指数线性单元(ExponentialLinearUnit，ELU)激活函数；
• 层标准化（LayerNormalization）；
• 动作重复；
• 更新重复；
• 观察量标准化和动作离散化可能是有用的，但我们未在提供的解决方案中使用；
347
第13章 LearningtoRun
• 根据智能体双腿的对称性所做的数据增强可能是用的，但我们未在提供的解决方案中使用。
注意，根据竞赛参与团体的实验和报告，后两个技巧也可能是有用的，但由于它们更多基于
该具体任务的方法而不对其他任务广泛适用，我们未在这里的解决方案中使用。然而，要知道观
察量标准化、动作值离散化和数据增强是可以根据一些任务的具体情况应用来加速学习过程的。
这个环境一个典型的缺陷是模拟速度太慢，在一个普通CPU上完成单个片段至少需要几十
秒时间。为了更高效地学习策略，我们需要将采样和训练过程并行化。
13.2.1 并行训练
至少有两个原因需要我们对这个任务进行并行训练。第一个是由于上面所述 Learning to
Run环境较慢的模拟速度，至少耗时几十秒完成一个模拟片段。第二个是由于该环境有较高的内
在复杂度。基于作者经验，这个环境用普通的无模型（Model-Free）强化学习算法，如深度决定
性策略梯度（Deep Deterministic Policy Gradient，DDPG）或柔性 Actor-Critic（Soft Actor-Critic，
SAC），需要至少上百个CPU/CPU计算小时来获得一个较好的策略。因此，这里需要一个多进程
跨GPU的训练框架。
由于 Learning to Run环境的高复杂度，训练过程需要用多个CPU和GPU来并行分布实
现。此外，CPU和GPU之间的平衡对这个任务也很关键，因为与环境交互采样的过程一般是在
CPU 上，而反向传播训练过程一般是在 GPU 上。整个过程的训练效率在实践中满足短板效应。
关于并行训练中如何均衡CPU和GPU计算的内容在第12章和第18章中也有讨论。这里有一种
解决这个任务的方案。
如图13.2所示，在一般的单进程深度强化学习中，训练过程由一个进程来处理，而这通常无
法充分发挥计算资源的潜力，尤其在有多个CPU核和多个GPU的情况下。
训练（进程）
a
智能体 环境
s r
推送 更新
经验回放缓存
图13.2 在离线策略深度强化学习中进行单进程训练：只有一个进程来采样和训练策略
图13.3展示了在多个CPU和多个GPU上部署离线策略（Off-Policy）深度强化学习的并行
训练架构，其中，一个智能体和一个环境被封装进一个“工作者”来运行一个进程。多个工作者
可以共享同一个GPU，因为有时单个工作者无法完全占用整个GPU内存。在这种设置下，使用
同一个GPU的进程数量和工作者数量可以被手动设置，从而在学习过程中最大化所有计算资源
的利用率。
348
13.2 训练智能体
图13.3 一个离线策略深度强化学习的并行训练架构。每个工作者包含一个与环境交互的智能
体，策略被分布在多个GPU上训练
我们的项目提供了一个高度并行化的SAC算法，它使用上述架构来解决这个需要多进程和
多GPU计算的任务。由于多进程的内存之间互相不共享，需要用特殊的模块来处理信息交流和
参数共享。在代码中，回放缓冲区通过Python内的 multiprocessing模块共享，训练过程中的
网络和参数更新由PyTorch的 multiprocessing模块共享（在Linux系统上）。
实践中，尽管每个工作者包含一个智能体，但是智能体内的网络实际在多个工作者间共享，
因此实际上只保留了一套网络（用于一个智能体的）。PyTorch的 nn.Module模块可以处理使用
多个进程更新共享内存中网络参数的情况。由于Adam优化器在训练中也有一些统计量，我们使
用以下 ShareParameters()函数来在多进程中共享这些值：
def ShareParameters(adamoptim):
# 共享 Adam 优化器的参数便于实现多进程
for group in adamoptim.param_groups:
for p in group[’params’]:
state = adamoptim.state[p]
# 初始化：需要在这里初始化，否则无法找到相应量
state[’step’] = 0
state[’exp_avg’] = torch.zeros_like(p.data)
state[’exp_avg_sq’] = torch.zeros_like(p.data)
# 在内存中共享
state[’exp_avg’].share_memory_()
state[’exp_avg_sq’].share_memory_()
349
第13章 LearningtoRun
在训练函数中，我们用以下方式设置SAC算法中的共享模块，包括网络和优化器：
# 共享网络
sac_trainer.soft_q_net1.share_memory()
sac_trainer.soft_q_net2.share_memory()
sac_trainer.target_soft_q_net1.share_memory()
sac_trainer.target_soft_q_net2.share_memory()
sac_trainer.policy_net.share_memory()
# 共享优化器参数
ShareParameters(sac_trainer.soft_q_optimizer1)
ShareParameters(sac_trainer.soft_q_optimizer2)
ShareParameters(sac_trainer.policy_optimizer)
ShareParameters(sac_trainer.alpha_optimizer)
share_memory() 是一个继承自 PyTorch 的 nn.Module 模块的函数，可用于共享神经网络。
我们也可以共享熵因子，但是在这个代码里没有实现它。“forkserver”启动方法是在Python3中
使用CUDA子进程所需的，如代码中所示：
torch.multiprocessing.set_start_method(’forkserver’, force=True)
回放缓冲区可以用Python的 multiprocessing模块共享：
from multiprocessing.managers import BaseManager
replay_buffer_size = 1e6
BaseManager.register(’ReplayBuffer’, ReplayBuffer)
manager = BaseManager()
manager.start()
replay_buffer = manager.ReplayBuffer(replay_buffer_size)
# 通过 manager 来共享经验回放缓存
在克隆下来的文件夹中运行以下命令来开始训练（注意，由于使用“forkserver”启动方法，
所以在Windows10上无法进行这样的并行训练）：
python sac_learn.py --train
我们也可用以下命令测试训练的模型：
python sac_learn.py --test
350
13.2 训练智能体
13.2.2 小技巧
然而，即便使用了上面的并行架构，我们仍旧不能在这个任务上取得很好的表现。由于任务
的复杂性和深度学习模型的非线性，损失函数上的局部最优和非平滑甚至不可微的曲面都容易使
优化过程陷入困境（对于策略或价值函数）。在使用深度强化学习方法的过程中经常需要一些微
调策略，尤其是对像 Learning to Run这样的复杂任务。所以，下面介绍我们使用的一些小技
巧，来更高效和稳定地解决这个任务。
• 奖励值缩放：奖励值缩放遵循一般的值缩放规则，即将奖励值除以训练过程中所采批样本
的标准差。奖励值缩放，或叫标准化和归一化，是强化学习中使训练过程稳定而加速收敛
速度的常用技术手段。如SAC算法后续的一篇文章(Haarnojaetal.,2018)所报道的，最大
熵强化学习算法可能对奖励函数的缩放敏感，这不同于其他传统强化学习算法。因此，SAC
算法的作者添加了一个基于梯度的温度调校模块用作熵正则化项，这显著缓解了实践中超
参数微调过程的困难。
• 指数线性单元（ExponentialLinearUnit，ELU）(Clevertetal.,2015)激活函数被用以替代
整流线性单元（RectifiedLinearUnit，ReLU）(Agarap,2018)：为了得到更快的学习过程和
更好的泛化表现，我们使用ELU作为策略网络隐藏层的激活函数。ELU函数定义如下：
8
>0
f(x)= (13.1)
>:
αexp(x−1), ifx⩽0
ELU和ReLU的对比如图13.4所示。相比于ReLU，ELU有负数值，这使得它能够将神经单
元激活的平均值拉至更接近0的位置，如同批标准化，但是却有着更低的计算复杂度。均
值移动到趋于0可以加速学习，因为它通过减少神经单元激发造成的移动偏差，使得一般
的梯度更加接近于神经网络单元的自然梯度。
• 层标准化：我们也对价值网络和策略网络的每个隐藏层使用层标准化(Baetal.,2016)。相比
于批标准化（BatchNormalization），层标准化对单个训练样本在某神经网络层上的神经元
的累加输入计算均值和方差来进行标准化。每个神经元有其与众不同的适应性偏差（Bias）
和增益（Gain），这些值在标准化之后和非线性激活之前被添加到神经元的值上。这种方法
在实际中可以帮助加速训练过程。
• 动作重复：我们在训练过程中使用一个常见的技巧叫动作重复（或叫跳帧），来加速训练的
执行时间（Wall-Clock Time）。DQN 原文中使用跳帧和像素级的最大化（Max）算子来实
现在 Atari 2600 游戏上基于图像的学习。如果我们定义单个帧的原始观察量是 o ，其中 i
i
表示帧指标，原始DQN文章中的输入是4个堆叠帧，其中每个是两个连续帧中的最大值，
即 [max(o i−1,o i),max(o i+3,o i+4),max(o i+7,o i+8), max(o i+11,o i+12)]，对应的跳帧率就是
4（实际上，对于不同游戏，该跳帧率可以是2,3或4）。在这些跳过的帧中，动作被重复执
351
第13章 LearningtoRun
图13.4 对比ReLU和ELU激活函数。ELU在零点可微
行。最大化算子在图像观察量上按像素计算，奖励函数对所有跳过和不跳过的帧累加。原
始DQN中的跳帧机制增加了随机性，同时加速了采样率。然而，在我们的任务中，我们使
用一种不同的设置，不使用最大化算子和堆叠帧：每个动作在跳过的帧上进行简单的重复
执行，包括跳过帧和未跳过帧在内的所有样本被存入回放缓冲区。实践中，我们使用3作
为动作重复率，减少了策略与环境交互所需的正向推理时间。
• 更新重复：我们也在训练中使用一个小的学习率并重复更新策略的技巧，从而策略以重复
率3在同一个批样本上进行学习。
13.2.3 学习结果
通过以上设置和SAC算法上的这些小技巧，智能体能够在3天的训练时长下学会用人类的
方式奔跑很长的一段距离，训练是在一个4GPU和56CPU的服务器上进行的，结果如图13.5所
示。图13.6展示了学习曲线，包括原始的奖励函数值和移动平均的平滑曲线，呈现了上升的学习
表现。纵轴是一个片段内的累计奖励值，显示了智能体奔跑的距离和姿势状况。
352
参考文献