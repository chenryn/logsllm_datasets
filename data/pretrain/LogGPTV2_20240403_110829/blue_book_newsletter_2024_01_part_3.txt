    from datetime import date
    from pydantic import BaseModel, ConfigDict, ValidationError
    class Event(BaseModel):
        model_config = ConfigDict(strict=True)
        when: date
        where: tuple[int, int]
    json_data = '{"when": "1987-01-28", "where": [51, -1]}'
    print(Event.model_validate_json(json_data))
    try:
        Event.model_validate({'when': '1987-01-28', 'where': [51, -1]})
    except ValidationError as e:
        print(e)
        """
        2 validation errors for Event
        when
          Input should be a valid date [type=date_type, input_value='1987-01-28', input_type=str]
        where
          Input should be a valid tuple [type=tuple_type, input_value=[51, -1], input_type=list]
        """
    ```
* New: Create part of the attributes in the initialization stage.
    ```python
    class Sqlite(BaseModel):
        model_config = ConfigDict(arbitrary_types_allowed=True)
        path: Path
        db: sqlite3.Cursor
        def __init__(self, **kwargs):
            conn = sqlite3.connect(kwargs['path'])
            kwargs['db'] = conn.cursor()
            super().__init__(**kwargs)
    ```
## [Git](git.md)
* Correction: Search for alternatives to git-sweep.
    The tool is [no longer maintained](https://github.com/arc90/git-sweep/issues/45) but there is still no good alternative. I've found some but are either not popular and/or not maintained:
    - [gitsweeper](https://github.com/petems/gitsweeper)
    - [git-removed-brances](https://github.com/nemisj/git-removed-branches)
    - [git-sweep rewrite in go](https://github.com/gottwald/git-sweep)
# [DevOps](ombi.md)
* New: [Set default quality of request per user.](ombi.md#set-default-quality-of-request-per-user)
    Sometimes one specific user continuously asks for a better quality of the content. If you go into the user configuration (as admin) you can set the default quality profiles for that user.
## Infrastructure as Code
### [Ansible Snippets](ansible_snippets.md)
* New: [Fix the `ERROR! 'become' is not a valid attribute for a IncludeRole` error.](ansible_snippets.md#fix-the-error-become-is-not-a-valid-attribute-for-a-includerole-error)
    If you're trying to do something like:
    ```yaml
    tasks:
      - name: "Install nfs"
        become: true
        ansible.builtin.include_role:
          name: nfs
    ```
    You need to use this other syntax:
    ```yaml
    tasks:
      - name: "Install nfs"
        ansible.builtin.include_role:
          name: nfs
          apply:
            become: true
    ```
### [Gitea](gitea.md)
* Correction: Update disable regular login with oauth.
    The last `signin_inner.tmpl` failed with the latest version. I've
    uploaded the new working one.
## Infrastructure Solutions
### [Kubernetes](kubernetes.md)
* New: Introduce IceKube.
    [IceKube](https://twitter.com/clintgibler/status/1732459956669214784) tool for finding complex attack paths in Kubernetes clusters. It's like Bloodhound for Kubernetes. It uses Neo4j to store & analyze Kubernetes resource relationships → identify attack paths & security misconfigs
### [AWS Savings plan](aws_savings_plan.md)
* New: [Understanding how reserved instances are applied.](aws_savings_plan.md#understanding-how-reserved-instances-are-applied)
    A Reserved Instance that is purchased for a Region is called a regional Reserved Instance, and provides Availability Zone and instance size flexibility.
    - The Reserved Instance discount applies to instance usage in any Availability Zone in that Region.
    - The Reserved Instance discount applies to instance usage within the instance family, regardless of size—this is known as instance size flexibility.
    With instance size flexibility, the Reserved Instance discount applies to instance usage for instances that have the same family, generation, and attribute. The Reserved Instance is applied from the smallest to the largest instance size within the instance family based on the normalization factor.
    The discount applies either fully or partially to running instances of the same instance family, depending on the instance size of the reservation, in any Availability Zone in the Region. The only attributes that must be matched are the instance family, tenancy, and platform.
    The following table lists the different sizes within an instance family, and the corresponding normalization factor. This scale is used to apply the discounted rate of Reserved Instances to the normalized usage of the instance family.
    | Instance size | 	Normalization factor |
    | --- | --- |
    | nano | 	0.25 |
    | micro | 	0.5 |
    | small | 	1 |
    | medium | 	2 |
    | large | 	4 |
    | xlarge | 	8 |
    | 2xlarge | 	16 |
    | 3xlarge | 	24 |
    | 4xlarge | 	32 |
    | 6xlarge | 	48 |
    | 8xlarge | 	64 |
    | 9xlarge | 	72 |
    | 10xlarge | 	80 |
    | 12xlarge | 	96 |
    | 16xlarge | 	128 |
    | 18xlarge | 	144 |
    | 24xlarge | 	192 |
    | 32xlarge | 	256 |
    | 48xlarge | 	384 |
    | 56xlarge | 	448 |
    | 112xlarge | 	896 |
    For example, a `t2.medium` instance has a normalization factor of `2`. If you purchase a `t2.medium` default tenancy Amazon Linux/Unix Reserved Instance in the US East (N. Virginia) and you have two running `t2.small` instances in your account in that Region, the billing benefit is applied in full to both instances.
    Or, if you have one `t2.large` instance running in your account in the US East (N. Virginia) Region, the billing benefit is applied to 50% of the usage of the instance.
    Limitations:
    - *Supported*: Instance size flexibility is only supported for Regional Reserved Instances.
    - *Not supported*: Instance size flexibility is not supported for the following Reserved Instances:
        - Reserved Instances that are purchased for a specific Availability Zone (zonal Reserved Instances)
        - Reserved Instances for G4ad, G4dn, G5, G5g, and Inf1 instances
        - Reserved Instances for Windows Server, Windows Server with SQL Standard, Windows Server with SQL Server Enterprise, Windows Server with SQL Server Web, RHEL, and SUSE Linux Enterprise Server
        - Reserved Instances with dedicated tenancy
* New: [EC2 Instance savings plan versus reserved instances.](aws_savings_plan.md#ec2-instance-savings-plan-versus-reserved-instances)
    I've been comparing the EC2 Reserved Instances and of the EC2 instance family savings plans and decided to go with the second because:
    - They both have almost the same rates. Reserved instances round the price at the 3rd decimal and the savings plan at the fourth, but this difference is neglegible.
    - Savings plan are easier to calculate, as you just need to multiply the number of instances you want times the current rate and add them all up.
    - Easier to understand: To reserve instances you need to take into account the instance flexibility and the normalization factors which makes it difficult both to make the plans and also to audit how well you're using it.
    - Easier to audit: In addition to the above point, you have nice dashboards to see the coverage and utilization over time of your ec2 instance savings plans, which are at the same place as the other savings plans.
* New: [Important notes when doing a savings plan.](aws_savings_plan.md#doing-your-savings-plan)
    - Always use the reservation rates instead of the on-demand rates!
    - Analyze your coverage reports. You don't want to have many points of 100% coverage as it means that you're using less resources than you've reserved. On the other hand it's fine to sometimes use less resources than the reserved if that will mean a greater overall savings. It's a tight balance.
    - The Savings plan reservation is taken into account at hour level, not at month or year level. That means that if you reserve 1$/hour of an instance type and you use for example 2$/hour half the day and 0$/hour half the day, you'll have a 100% coverage of your plan the first hour and another 1$/hour of on-demand infrastructure cost for the first part of the day. On the second part of the day you'll have a 0% coverage. This means that you should only reserve the amount of resources you plan to be using 100% of the time throughout your savings plan. Again you may want to overcommit a little bit, reducing the utilization percentage of a plan but getting better savings in the end.
## Storage
### [OpenZFS](zfs.md)
* New: Solve the pool or dataset is busy error.
    If you get an error of `pool or dataset is busy` run the next command to see which process is still running on the pool:
    ```bash
    lsof 2>/dev/null | grep dataset-name
    ```
### [ZFS Prometheus exporter](zfs_exporter.md)
* Correction: Tweak the zfs_exporter target not available error.
    Remember to set the `scrape_timeout` to at least of `60s` as the exporter is sometimes slow to answer, specially on low hardware resources.
    ```yaml
     - job_name: zfs_exporter
       metrics_path: /metrics
       scrape_timeout: 60s
       static_configs:
       - targets: [192.168.3.236:9134]
       metric_relabel_configs:
       ...
    ```
## Monitoring
### [Promtail](promtail.md)
* New: Introduce Promtail.
    [Promtail](https://grafana.com/docs/loki/latest/send-data/promtail/) is an agent which ships the contents of local logs to a [Loki](loki.md) instance.
    It is usually deployed to every machine that runs applications which need to be monitored.
    It primarily:
    - Discovers targets
    - Attaches labels to log streams
    - Pushes them to the Loki instance.
* New: [Scrape journald logs.](promtail.md#scrape-journald-logs)
    On systems with `systemd`, Promtail also supports reading from the journal. Unlike file scraping which is defined in the `static_configs` stanza, journal scraping is defined in a `journal` stanza:
    ```yaml
    scrape_configs:
      - job_name: journal
        journal:
          json: false
          max_age: 12h
          path: /var/log/journal
          labels:
            job: systemd-journal
        relabel_configs:
          - source_labels: ['__journal__systemd_unit']
            target_label: unit
          - source_labels: ['__journal__hostname']
            target_label: hostname
          - source_labels: ['__journal_syslog_identifier']
            target_label: syslog_identifier
          - source_labels: ['__journal_transport']
            target_label: transport
          - source_labels: ['__journal_priority_keyword']
            target_label: keyword
    ```
    All fields defined in the journal section are optional, and are just provided here for reference.
    - `max_age` ensures that no older entry than the time specified will be sent to Loki; this circumvents `entry too old` errors.
    - `path` tells Promtail where to read journal entries from.
    - `labels` map defines a constant list of labels to add to every journal entry that Promtail reads.
    - `matches` field adds journal filters. If multiple filters are specified matching different fields, the log entries are filtered by both, if two filters apply to the same field, then they are automatically matched as alternatives.
    - When the `json` field is set to true, messages from the journal will be passed through the pipeline as JSON, keeping all of the original fields from the journal entry. This is useful when you don’t want to index some fields but you still want to know what values they contained.
    - When Promtail reads from the journal, it brings in all fields prefixed with `__journal_` as internal labels. Like in the example above, the `_SYSTEMD_UNIT` field from the journal was transformed into a label called `unit` through `relabel_configs`. Keep in mind that labels prefixed with `__` will be dropped, so relabeling is required to keep these labels. Look at the [systemd man pages](https://www.freedesktop.org/software/systemd/man/latest/systemd.journal-fields.html) for a list of fields exposed by the journal.
    By default, Promtail reads from the journal by looking in the `/var/log/journal` and `/run/log/journal` paths. If running Promtail inside of a Docker container, the path appropriate to your distribution should be bind mounted inside of Promtail along with binding `/etc/machine-id`. Bind mounting `/etc/machine-id` to the path of the same name is required for the journal reader to know which specific journal to read from.
    ```bash
    docker run \
      -v /var/log/journal/:/var/log/journal/ \
      -v /run/log/journal/:/run/log/journal/ \
      -v /etc/machine-id:/etc/machine-id \
      grafana/promtail:latest \
      -config.file=/path/to/config/file.yaml
    ```
* New: [Scrape docker logs.](promtail.md#scrape-docker-logs)
    Docker service discovery allows retrieving targets from a Docker daemon. It will only watch containers of the Docker daemon referenced with the host parameter. Docker service discovery should run on each node in a distributed setup. The containers must run with either the `json-file` or `journald` logging driver.
    Note that the discovery will not pick up finished containers. That means Promtail will not scrape the remaining logs from finished containers after a restart.