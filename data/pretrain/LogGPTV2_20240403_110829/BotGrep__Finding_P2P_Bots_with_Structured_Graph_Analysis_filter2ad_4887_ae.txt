For comparison pur-
poses, we also consider several graph partitioning algo-
rithms that have been proposed in the literature. While
these techniques were not intended to scale up to the
large data sets we consider here, we can compare against
them on smaller data sets to get a sense of how BotGrep
compares against these approaches. In particular, several
algorithms for community detection (detecting groups of
nodes in a network with dense internal connections) have
been proposed. Work in this space mainly focuses on hi-
erarchical clustering methods. Work in this space can be
classiﬁed as following two categories, and for our evalu-
ation we implement two representative algorithms from
each category:
Edge importance based community structure detec-
tion iteratively removes the edges with the highest im-
portance, which can be deﬁned in different ways. Gir-
van and Newman [25] deﬁned edge importance by its
shortest path betweenness. The idea is that the edge with
higher betweenness is typically responsible for connect-
ing nodes from different communities. In [22], informa-
tion centrality has been proposed to measure the edge
importance. The information centrality of an edge is de-
ﬁned as the relative network efﬁciency [46] drop caused
by the removal of that. The time complexity of algorithm
in [25] and [22] are O(|V|3) and O(|E|3 × V ), respec-
tively.
The spectral-based approach detects communities by
optimizing the modularity (a beneﬁt function measures
community structure [52] over possible network divi-
sions. In [53], the communities are detected by calcu-
lating the eigenvector of the modularity matrix. It takes
O(|E|+|V|2) time to separating each community. More-
over, Clauset et al. [14] proposed a hierarchical agglom-
eration algorithm for community detecting. The pro-
posed greedy algorithm adopts more sophisticated data
structures to reduce the computation time of modularity
calculation. The time complexity is O(|E| +|V|log2|V|)
in average.
As the time complexity of above algorithms is not ac-
ceptable for computing large-scale networks, here we
11
Topology
BotGrep
de Bruijn
Chord
Kademlia
0.78/2.55
0.77/7.15
0.92/7.00
Fast Greedy Girvan-Newman Modularity
Eigenvector
Modularity
0.92/43.88
14.43/7.65
4.24/20.19
7.58/10.13
14.66/33.80
5.70/48.70
Betweenness
19.73/15.31
6.05/19.50
18.06/4.75
Table 6: 2k Abilene Results (% FP /% FN)
consider a small-scale scenario for performance evalua-
tion. We extract subgraphs from full Abilene data by per-
forming a Breadth-First-Search (BFS) starting at a ran-
domly selected node, in which the overall visited nodes
are limited by a size of 2000. Results from our com-
parison are shown in Table 6. The information central-
ity algorithm took more than one month to run for just
one iteration on this 2000-node graph, and was hence
excluded from further analysis (we tested information
centrality on smaller 50-node graphs, and found perfor-
mance comparable to the Girvan and Newman Between-
ness algorithm). Overall, we found that our approach
outperformed these approaches. For example, on the
Chord topology, BotGrep’s false positive rate was 0.77%,
while false positive rates for the other approaches ranged
from 4.24-7.58%. The performance of BotGrep is less on
this scaled down 2000-node topology as compared to the
earlier Abilene and CAIDA datasets, because our method
of generating the scaled-down 2000 node graph selected
the densely connected core of the graph, which is fast-
mixing, while on more realistic graphs, it is easier for
BotGrep to distinguish the fast-mixing botnet topology
from the rest of the non-fast-mixing background graph.
Moreover, we found that run-time was a signiﬁcant
limiting factor in using these alternate approaches. For
example, the Girvan-Newman Betweenness Algorithm
took 2.5 hours to run on a graph containing 2000 nodes
(in all cases, BotGrep runs in under 10.4 seconds on a
Core2 Duo 2.83GHz machine with 4GB RAM using a
single core). While these traditional techniques were not
intended to scale to the large data sets we consider here,
they may be appropriate for localizing smaller botnets in
contained environments (e.g., within a single Honeynet,
or the part of a botnet contained within an enterprise net-
work). Since these techniques leverage different features
of the inputs, they are synergistic with our approach, and
may be used in conjunction with our technique to im-
prove performance.
6 Discussion
As we have demonstrated, analysis of core Internet traf-
ﬁc can be effective at identifying nodes and communi-
cation links of structured overlay networks. However,
many challenges remain to turn our approach into a full-
scale detection mechanism.
Misuse Detection:
It is easy to see that other forms of
P2P activity, such as ﬁle sharing networks, will also be
identiﬁed by our techniques. While there is some beneﬁt
to being able to identify such trafﬁc as well, it requires a
dramatically different response than botnets and so it is
important to distinguish the two. We believe that funda-
mentally, our mechanisms need to be integrated with de-
tection mechanisms at the edge that identify suspicious
behavior. Also, multiple intrusion detection approaches
can reinforce each other and provide more accurate re-
sults [75, 67, 30]; e.g., misbehaving hosts that follow a
similar misuse pattern and at the same time are detected
to be part of the same botnet communication graph may
be precisely labeled as a botnet, even if each individual
misbehavior detection is not sufﬁcient to provide a high-
conﬁdence categorization.
A concrete example of how misuse detection may
work is the following: we randomly sample nodes from
the suspect P2P network and compute the likelihood of
the sampled nodes being malicious, based on inputs from
honeynets, spam blacklists etc. If we can identify a statis-
tically signiﬁcant difference of the rates of misuse, then
we can assume that membership in the P2P network is
correlated with misuse and we should label it as a P2P
botnet. Note that, given the availability of large sample
sizes, even a small difference in the rates will be statisti-
cally signiﬁcant, so this approach will be successful even
if misuse detection fails to identify the vast majority of
the botnet nodes as malicious.
Scale and cooperation: Our experiments show our de-
sign can scale to large trafﬁc volumes, and in the pres-
ence of partial observations. However, several practi-
cal issues remain. First, large ISPs tend to use sam-
pled data analysis to monitor their networks. This can
miss low-volume control communications used by botnet
networks. New counter architectures or programmable
monitoring techniques should be used to collect sufﬁ-
cient statistics to run our algorithms [73]. Also, for best
results multiple vantage points should contribute data to
obtain a better overall perspective.
Tradeoffs between structure and detection: The com-
munication structure of botnet graphs plays an important
role in their delay penalty, and how resilient they are to
network failures. At the same time, our results indicate
12
that the structure of the communication graph has some
effect on the ability to detect the botnet host from a col-
lection of vantage points. As part of future work, we plan
to study the tradeoff between resilience and the ability to
avoid detection, and whether there exist fundamentally
hard-to-detect botnet structures that are also resilient.
Containing botnets:
The ability to quickly localize
structured network topologies may assist existing sys-
tems that monitor network trafﬁc to quickly localize and
contain bot-infected hosts. When botnets are detected
in edge networks, the relevant machines are taken of-
ﬂine. However, this may not always be easy with in-
core detection; an interesting question is whether in-core
ﬁltering or distributed blacklisting can be an effective re-
sponse strategy when edge cooperation is not possible.
Another question we plan to address is whether there ex-
ist responses that do not completely disconnect a node
but mitigate its potential malicious activities, to be ef-
fected when a node is identiﬁed as a botnet member, but
with a low conﬁdence.
7 Related Work
The increasing criticality of the botnet threat has led to
vast amounts of work that attempt to localize them. We
can classify this work into host based approaches and
network based approaches. Host based approaches detect
intrusions by analyzing information available on a sin-
gle host. On the other hand, network based approaches
detect botnets by analyzing incoming and outgoing host
trafﬁc. Hybrid approaches exist as well. BotGrep (our
work) is a network based approach to botnet detection
that uses graph theory to detect botnets.
In the following section (Section 7.1) we review re-
lated work on network based approaches and then de-
scribe work on botnet detection using graph analysis
(Section 7.2).
7.1 Network based approaches
Several pieces of work isolate bot-infected hosts by de-
tecting the malicious trafﬁc they send, which may be
divided into schemes that analyze attack trafﬁc, and
schemes that analyze control trafﬁc.
Attack trafﬁc:
For example, network operators may
look for sources of denial of service attacks, port scan-
ning, spam, and other unwanted trafﬁc as a likely bot.
These works focus on the symptoms caused by the bot-
nets instead of the networks themselves. Several works
seek to exploit DNS usage patterns. Dagon et al. [19]
studied the propagation rates of malware released at dif-
ferent times by redirecting DNS trafﬁc for bot domain
names. Their use of DNS sinkholes is useful in mea-
suring new deployments of a known botnet. However,
this approach requires a priori knowledge of botnet do-
main names and negotiations with DNS operators and
hence does not target scaling to networks where a bot-
net can simply change domain names, have a large pool
of C&C IP addresses and change the domain name gen-
eration algorithm by remotely patching the bot. Subse-
quently, Ramachandran et al. [61] use a graph based ap-
proach to isolate spam botnets by analyzing the pattern
of requests to DNS blacklists maintained by ISPs. They
observed that legitimate email servers request blacklist
lookups and are looked up by other email servers ac-
cording to the timing pattern of email arrival, while bot-
infected machines are a lot less likely to be looked up
by legitimate email servers. However, DNS blacklists
and phishing blacklists [65], while initially effective have
are becoming increasingly ineffective [60] owing to the
agility of the attackers. Much more recently, Villamar
et al. [74] applied Bayesian methods to isolate central-
ized botnets that use fast-ﬂux to counter DNS blacklists,
based on the similarity of their DNS trafﬁc with a given
corpus of known DNS botnet traces. Further, in order
to study bots, Honeypot techniques have been widely
used by researchers. Cooke et al. [17] conducted several
studies of botnet propagation and dynamics using Hon-
eypots; Barford and Yegneswaran [8] collected bot sam-
ples and carried out a detailed study on the source code
of several families; ﬁnally, Freiling et al. [24] and Rajab
et al. [59] carried out measurement studies using Honey-
pots. Collins et al. [16] present a novel botnet detection
approach based on the tendency of unclean networks to
contain compromised hosts for extended periods of time
and hence acting as a natural Honeypot for various bot-
nets. However Honeypot-based approaches are limited
by their ability to attract botnets that depend on human
action for an infection to take place, an increasingly pop-
ular aspect of the attack vector [51].
Control trafﬁc: Another direction of work, is to local-
ize botnets solely based on the control trafﬁc they use to
maintain their infrastructures. This line of work can be
classiﬁed as trafﬁc-signature based detection and statis-
tical trafﬁc analysis based detection. Techniques in the
former category require trafﬁc signatures to be developed
for every botnet instance. This approach has been widely
used in the detection of IRC-based botnets. Blinkley and
Singh[10] combine IRC statistics and TCP work weight
to generate signatures; Karasaridis et al. [44] present an
algorithm to detect IRC C&C trafﬁc signatures using
Netﬂow records; Rishi [27] uses n-gram analysis to iden-
tify botnet nickname patterns. The limitations of these
approaches are analogous to the scalability issues faced
In addition, such
by host-based detection techniques.
signatures may not exist for P2P botnets.
In the latter
category, several works [31, 72, 9, 49] suggest that bot-
13
nets can be detected by analyzing their ﬂow character-
istics.
In all these approaches, the authors use a vari-
ety of heuristics to characterize the network behavior of
various applications and then apply clustering algorithms
to isolate botnet trafﬁc. These schemes assume that the
statistical properties of bot trafﬁc will be different from
normal trafﬁc because of synchronized or correlated be-
havior between bots. While this behavior is currently
somewhat characteristic of botnets, it can be easily mod-
iﬁed by botnet authors. As such it does not derive from
the fundamental property of botnets.
Other works use a hybrid approach such as Both-
unter [30] which automates trafﬁc-signature generation
by searching for a series of ﬂows that match the infec-
tion life-cycle of a bot; BotMiner [29] combines packet
statistics of C&C trafﬁc with those of attack trafﬁc and
then applies clustering techniques to heuristically isolate
botnet ﬂows. TAMD [76] is another method that ex-
ploits the spatial and temporal characteristics of botnet
trafﬁc that emerges from multiple systems within a van-
tage point. They aggregate ﬂows based on similarity of
ﬂow sizes and host conﬁguration (such as OS platforms)
and compare them with a historical baseline to detect in-
fected hosts.
Finally, there are also schemes that combine network-
and host-based approaches. The work of Stinson et
al. [69] attempts to discriminate between locally-initiated
versus remotely-initiated actions by tracking data arriv-
ing over the network being used as system call arguments
using taint tracking methods. Following a similar ap-
proach, Gummadi et al. [33] whitelist application traf-
ﬁc by identifying and attesting human-generated trafﬁc
from a host which allows an application server to se-
lectively respond to service requests. Finally, John et
al. [40] present a technique to defend against spam bot-
nets by automating the generation of spam feeds by di-
recting an incoming spam feed into a Honeynet, then
downloading bots spreading through those messages and
then using the outbound spam generated to create a bet-
ter feed. While all the above are interesting approaches
they again deal with the side-effects of botnets instead of
tackling the problem in its entirety in a scalable manner.
7.2 Graph-based approaches
Several works [15, 36, 35, 78, 38] have previously ap-
plied graph analysis to detect botnets. The technique of
Collins and Reiter [15] detects anomalies induced in a
graph of protocol speciﬁc ﬂows by a botnet control traf-
ﬁc. They suggest that a botnet can be detected based on
the observation that an attacker will increase the number
of connected graph components due to a sudden growth
of edges between unlikely neighboring nodes. While it
depends on being able to accurately model valid network
growth, this is a powerful approach because it avoids de-
pending on protocol semantics or packet statistics. How-
ever this work only makes minimal use of spatial re-
lationship information. Additionally, the need for his-
torical record keeping makes it challenging in scenar-
ios where the victim network is already infected when