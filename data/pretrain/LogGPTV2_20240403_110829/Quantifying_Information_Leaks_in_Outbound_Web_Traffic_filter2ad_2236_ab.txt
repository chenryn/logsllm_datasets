### Fundamental Problem and Confidentiality Solutions

The confidentiality of data is a fundamental issue, and this document does not aim to provide a comprehensive solution. However, if confidentiality is a top priority, there are several methods for obtaining the original plaintext. One approach is to route all encrypted traffic through a gateway that acts as a man-in-the-middle (MitM) on each connection. This can be achieved by designating the gateway as a local certification authority and having it rewrite certificates. Another option is to deploy an agent on every host in the network that reports encryption keys to a monitoring system. With this method, any connections that cannot be decrypted are either blocked or flagged for further investigation.

### Leak Measurement Techniques

The leak measurement techniques presented in this paper are not intended to serve as a complete security solution but rather as a tool. The primary application of this work is envisioned to be forensic analysis, where it can help filter out almost all legitimate activity, making it faster and easier to isolate leaks. Another potential application is the detection of leaks in live network traffic. Further research would be required to determine appropriate thresholds and optimize the algorithms for handling large volumes of traffic. Integrating leak quantification into a security application is a future direction for this work.

### Paper Structure

- **Section 2:** Discusses related work.
- **Section 3:** Provides a formal problem description.
- **Section 4:** Details static message analysis techniques.
- **Section 5:** Describes dynamic content analysis methodology.
- **Section 6:** Outlines an approach for quantifying timing information.
- **Section 7:** Presents evaluation results.
- **Section 8:** Discusses strategies for mitigating entropy and improving analysis results.
- **Section 9:** Concludes and suggests future research directions.

### Related Work

Previous research on detecting covert web traffic has focused on measuring information flow via the HTTP protocol [3]. Borders et al. introduced a method for computing bandwidth in outbound HTTP traffic by discarding expected header fields. However, their stateless approach cannot account for repeated or constrained information from previous HTTP messages. In our evaluation, we compare the leak measurement techniques presented in this paper with the simple methods used by Web Tap [3] and demonstrate a significant improvement.

There are numerous techniques for controlling information flow within a program. Jif [16] ensures that programs do not leak information to low-security outputs by tainting values with sensitive data. More recent work by McCamant et al. [13] goes further by quantifying the amount of sensitive data each value in a program can contain. However, these intra-program flow control systems require access to source code, which is not always feasible, and they do not protect against compromised systems. The algorithms in this paper take a black-box approach to measuring leaks, making no assumptions about software integrity.

Research on limiting the capacity of channels for information leakage has traditionally assumed the use of mandatory access control (MAC) policies [5] to restrict information flow. However, MAC systems are rarely deployed due to usability and management overhead, yet organizations still need to protect confidential information. A more recent system, TightLip [27], attempts to prevent programs from leaking sensitive data by executing a shadow process that does not see sensitive data. Outputs that match those of the shadow process are treated normally, while different outputs are marked as confidential. TightLip is limited by its reliance on a trusted operating system and only protects sensitive data in files. In contrast, our leak measurement methods can identify leaks from a totally compromised computer, regardless of their origin.

A popular approach for protecting against network-based information leaks is to limit where hosts can send data using a content filter, such as Websense [26]. Content filters may help in some cases but do not prevent all information leaks. A smart attacker can post sensitive information on any website that receives input and displays it to other clients, including useful sites like www.wikipedia.org. We consider content filters to be complementary to our measurement methods, as they reduce but do not eliminate information leaks.

Although little work has been done on quantifying network-based information leaks, there has been extensive research on methods for leaking data. Prior work on covert network channels includes embedding data in IP fields [6], TCP fields [22], and HTTP protocol headers [7]. The methods presented in this paper aim to quantify the maximum amount of information an HTTP channel could contain, regardless of the specific data hiding scheme used.

Other research aims to reduce the capacity of network covert channels by modifying packets. Network "pumps" [11] and timing jammers [9] control transmission time to combat covert timing channels. Traffic normalizers (also known as protocol scrubbers) change IP packets in flight to match a normal format [10, 12]. Glavlit is an application-layer protocol scrubber that specifically normalizes HTTP traffic from servers [19]. Traffic normalization helps eliminate covert storage channels by fixing ambiguities in network traffic. Research on normalizing network traffic to reduce covert channel capacity is complementary to our work, which focuses solely on quantifying information content.

An earlier version of this paper was published at a workshop [4]. Previously, the measurement techniques only included those discussed in the section on static content analysis, and the evaluation was limited to controlled scenarios. This paper adds dynamic script analysis, considers timing channels, and evaluates our techniques on real web traffic. Improvements from the workshop paper have significantly impacted results from common controlled scenario experiments. For example, script handling reduced the average request size from 73.3 bytes to 7.8 bytes for the web mail scenario.

### Problem Description

In this paper, we address the problem of quantifying network-based information leak capacity by isolating information from the client in network traffic. We refer to information originating from the client as UI-layer input. Formally, the problem can be broken down into quantifying the set U of UI-layer input to a network application given the following information:
- **I**: The set of previous network inputs to an application.
- **O**: The set of current and previous network outputs from an application.
- **A**: The application representation, which is a mapping: U × I → O of UI-layer information combined with network input to yield network output.

By definition, the set I cannot contain new information from the client because it is generated by the server. In this paper, the application representation A is based on protocol specifications, but it could also be derived from program analysis. In either case, it does not contain information from the client. Therefore, the information content of set O can be reduced to the information in set U. If the application has been tampered with by malicious software, yielding a different representation A', then the maximum information content of tampered output O' is equal to the information content of the closest expected output O plus the edit distance between O and O'. Input supplied to an application from all sources other than the network is considered part of U, including file uploads and system information such as values from the random number generator. Timing information is also part of set U.

### Static Content Analysis

This section describes methods for measuring the amount of information in outbound HTTP requests by statically analyzing previous requests and responses. Some portions of the request headers are fixed and can be immediately filtered if they contain the expected values. Most header fields only change on rare occasions and can be discounted if they are the same as previous requests. The request path, which identifies resources on the web, is usually derived from previous HTML pages returned by the server. Filtering out repeated path values requires comparing paths to those in both prior requests and responses. Additionally, HTTP form POST requests reference field names and default values contained in HTML pages. This section elaborates on methods for extracting expected HTTP request fields from static content.

#### HTTP Request Overview

There are two main types of HTTP requests used by web browsers: GET and POST. GET typically retrieves resources, while POST sends data to a server. An example of an HTTP POST request is shown in Figure 2. This request consists of three distinct sections: the request line, headers, and the request body. GET requests are similar but do not have a request body. The request line contains the path of the requested file on the server and may include script parameters. The next part of the HTTP request is the header field section, which consists of key-value pairs separated by line breaks. Header fields relay information such as the browser version, preferred language, and cookies. Finally, the HTTP request body comes last and may consist of arbitrary data. In the example message, the body contains an encoded name and email address entered into a form.

#### HTTP Header Fields

The first type of HTTP header field we examine is a fixed header field. Fixed headers should be the same for each request in most cases. Examples include the preferred language and the browser version. We only count the size of these headers for the first request from each client and count the edit distance from the most recent request on subsequent changes. Here, we treat all HTTP headers except for Host, Referer, and Cookie as fixed. Some of these header fields, such as Authorization, may actually contain information from the user. When these fields contain new data, we again count the edit distance with respect to the most recent request.

Next, we look at the Host and Referer header fields. The Host field, along with the request path, specifies the request's uniform resource locator (URL). We only count the size of the Host field if the request URL did not come from a link in another page. Similarly, we only count the Referer field's size if it does not contain the URL of a previous request. Finally, we examine the Cookie header field to verify its consistency with expected browser behavior. The Cookie field is supposed to contain key-value pairs from previous server responses. Cookies should never contain UI-layer information from the client. If the Cookie differs from its expected value or if we do not have a record from a previous response (e.g., if a mobile computer is brought into an enterprise network), we count the edit distance between the expected and actual cookie values. At least one known tunneling program, Cooking Channel [7], deliberately hides information inside the Cookie header in violation of standard browser behavior. The techniques presented here correctly measure outbound bandwidth for the Cooking Channel program.

#### Standard GET Requests

HTTP GET requests are typically used to retrieve resources from a web server. Each GET request identifies a resource by a URL, which is composed of the server host name, stored in the Hostname header field, and the resource path, stored in the request line. Looking at each HTTP request independently, one cannot determine whether the URL contains UI-layer information or is the result of previous network input (i.e., a link from another page). However, considering the entire browsing session, we can discount request URLs that have been seen in previous server responses, thus significantly improving unconstrained bandwidth measurements.

The first step in accurately measuring UI-layer information in request URLs is enumerating all the links on each web page. We parse HTML, Cascading Style Sheet (CSS), and JavaScript files to discover static link URLs, which can occur in many different forms. Links that are written out dynamically by JavaScript are covered in Section 5. Examples of static HTML links include:

- `<a href="http://www.example.com">Click Here!</a>`
- `<img src="image.jpg" alt="Example Image">`
- `<link rel="stylesheet" href="styles.css">`

These examples would cause the browser to make requests to the specified URLs.