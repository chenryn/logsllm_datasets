fundamental problem, and we do not try to solve it here. If 
confidentiality is a top priority, there are a few possibilities 
for obtaining original plain text. One is to force all encrypted 
traffic through a gateway that acts as a man-in-the-middle on 
each  connection.  This  can  be  achieved  by  designating  the 
gateway  as  a  local  certification  authority  and  having  it 
rewrite certificates. Another option is to deploy an agent on 
every  host  in  the  network  that  reports  encryption  keys  to  a 
monitoring system. With this approach, any connections that 
cannot be decrypted are subsequently blocked or flagged for 
further investigation. 
The leak measurement techniques presented in this paper 
do not constitute an entire security solution, but rather act as 
a tool. We envision the primary application of this work to 
be  forensic  analysis.  One  could  filter  out  almost  all 
legitimate  activity,  making  it  faster  and  easier  to  isolate 
leaks.  Another possible application would be detecting leaks 
in  live  network  traffic.  Additional  research  would  be 
required  to  determine  appropriate  thresholds  and  optimize 
the  algorithms  for  handling  large  volumes  of  traffic. 
Integrating  leak  quantification  into  a  security  application  is 
future work. 
The remainder of this paper is laid out as follows. Section 
2 discusses related work. Section 3 poses a formal problem 
description.  Section  4  talks  about  static  message  analysis 
techniques.  Section  5  describes  dynamic  content  analysis 
methodology. Section 6 outlines an approach for quantifying 
timing  information.  Section  7  presents  evaluation  results. 
Section 8 discusses potential strategies for mitigating entropy 
and improving analysis results. Finally, section 9 concludes 
and suggests future research directions. 
2.  Related Work 
Prior research on detecting covert web traffic has looked 
at  measuring  information  flow  via  the  HTTP  protocol  [3]. 
Borders et al. introduce a method for computing bandwidth 
in outbound HTTP traffic that involves discarding expected 
header  fields.  However,  they  use  a  stateless  approach  and 
therefore are unable to discount information that is repeated 
or  constrained  from  previous  HTTP  messages.  In  our 
evaluation,  we  compare  the  leak  measurement  techniques 
presented in this paper with the simple methods used by Web 
Tap 
[3]  and  demonstrate  an  order  of  magnitude 
improvement. 
techniques 
for 
There 
are  numerous 
controlling 
information  flow  within  a  program.  Jif  [16]  ensures  that 
programs do not leak information to low-security outputs by 
tainting  values  with  sensitive  data.  More  recent  work  by 
McCamant  et  al.  [13]  goes  one  step  further  by  quantifying 
amount  of  sensitive  data  that  each  value  in  a  program  can 
contain.  Unfortunately,  intra-program  flow  control  systems 
rely on access to source code, which is not always feasible. 
They  do  not  protect  against  compromised  systems.  The 
algorithms  in  this  paper  take  a  black  box  approach  to 
measuring  leaks  that  makes  no  assumptions  about  software 
integrity. 
Research  on  limiting  the  capacity  of  channels  for 
information  leakage  has  traditionally  been  done  assuming 
that  systems  deploy  mandatory  access  control  (MAC) 
policies [5] to restrict information flow. However, mandatory 
access  control  systems  are rarely  deployed  because of  their 
usability  and  management  overhead,  yet  organizations  still 
have a strong interest in protecting confidential information. 
A  more  recent  system  for  controlling  information  flow, 
TightLip [27], tries to stop programs from leaking sensitive 
data  by  executing  a  shadow  process  that  does  not  see 
sensitive  data.  Outputs  that  are  the  same  as  those  of  the 
shadow  process  are  treated  normally,  and  those  that  are 
different are marked confidential. TightLip is limited in that 
it  relies  on  a  trusted  operating  system,  and  only  protects 
sensitive data in files. In comparison, our leak measurement 
methods will help identify leaks from a totally compromised 
computer, regardless of their origin. 
A popular approach for protecting against network-based 
information leaks is to limit where hosts can send data with a 
content  filter,  such  as  Websense  [26].  Content  filters  may 
help in some cases, but they do not prevent all information 
leaks. A smart attacker can post sensitive information on any 
website  that  receives  input  and  displays  it  to  other  clients, 
including  useful  sites  such  as  www.wikipedia.org.  We 
consider  content  filters 
to  our 
measurement  methods,  as  they  reduce  but  do  not  eliminate 
information leaks. 
to  be  complimentary 
Though  little  work  has  been  done  on  quantifying 
network-based information leaks, there has been a great deal 
of  research  on  methods  for  leaking  data.  Prior  work  on 
convert  network  channels  includes  embedding  data  in  IP 
fields [6], TCP fields [22], and HTTP protocol headers [7]. 
The  methods  presented  in  this  paper  aim  to  quantify  the 
maximum  amount  of  information  that  an  HTTP  channel 
could contain, regardless of the particular data hiding scheme 
employed. 
Other  research  aims  to  reduce  the  capacity  of  network 
covert  channels  by  modifying  packets.  Network  “pumps” 
[11]  and  timing  jammers  [9]  control  transmission  time  to 
combat  covert  timing  channels.  Traffic  normalizers  (also 
known as protocol scrubbers) will change IP packets in flight 
so  that  they  match  a  normal  format  [10,  12].  Glavlit  is  an 
application-layer  protocol  scrubber  that  focuses  specifically 
on  normalizing  HTTP  traffic  from  servers  [19].  Traffic 
131
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
1  POST /download HTTP/1.1 
2  Host: www.example.com 
2  User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; 
en-US; rv:1.8.1.12) Gecko/20080201 Firefox/2.0.0.12 
2  Keep-Alive: 300 
2  Connection: keep-alive 
2  Referer: http://www.example.com/download.html 
2  Content-Type: application/x-www-form-urlencoded 
2  Content-Length: 73 
3  FirstName=John&LastName=Doe&Email=johndoe%40example.
com&Submit=Download 
  (a) 
            (b) 
Figure 2. (a) A sample HTTP POST request for submitting contact information to download a file. Line 1 is the HTTP 
request line. Lines marked 2 are request headers, and line 3 is the request body.  Bytes counted by a simple algorithm are 
highlighted in gray. UI-layer data is highlighted in black with white text. (b) A sample HTML document at 
http://www.example.com/download.html that generated request (a). 
normalization  helps  eliminate  covert  storage  channels  by 
fixing  ambiguities 
traffic.  Research  on 
normalizing  network 
to  reduce  covert  channel 
capacity is complimentary to our work, which focuses only 
on quantifying information content.  
in  network 
traffic 
An  earlier  version  of  this  paper  was  published  at  a 
workshop [4].  Previously, the measurement techniques only 
consisted of those discussed in the section on static content 
analysis.  The  evaluation  was  also  limited  to  controlled 
scenarios. This paper adds dynamic script analysis, considers 
timing  channels,  and  evaluates  our  techniques  on  real  web 
traffic. Improvements  from  the  workshop paper  have  had  a 
significant  impact  on  results  from  common  controlled 
scenario  experiments.  For  example,  script  handling  helped 
reduce the average request size from 73.3 bytes to 7.8 bytes 
for the web mail scenario. 
3.  Problem Description 
information 
leak  capacity  by 
In  this  paper,  we  address  the  problem  of  quantifying 
network-based 
isolating 
information from the client in network traffic. We will refer 
to information originating from the client as UI-layer input. 
From a formal perspective, the problem can be broken down 
to  quantifying  the  set  U  of  UI-layer  input  to  a  network 
application given the following information: 
•  I  –  The  set  of  previous  network  inputs  to  an 
application. 
•  O  – The set of current and previous network outputs 
from an application. 
•  A  –  The  application  representation,  which  is  a 
mapping:  U  ×  I  (cid:314)  O  of  UI-layer  information 
combined with network input to yield network output. 
By  definition,  the  set  I  cannot  contain  new  information 
from the client because it is generated by the server. In this 
paper, the application representation A is based on protocol 
specifications,  but  it  could  also  be  derived  from  program 
132
analysis. In either case, it does not contain information from 
the client. Therefore, the information content of set O can be 
reduced to the information in the set U. If the application has 
been  tampered  with  by  malicious  software  yielding  a 
different  representation  A’,  then  the  maximum  information 
content  of  tampered  output  O’  is  equal  to  the  information 
content  of  the  closest  expected  output  O  plus  the  edit 
distance between O and O’. Input supplied to an application 
from all sources other than the network is considered part of 
U.  This  includes  file  uploads  and  system  information,  such 
as  values  from  the  random  number  generator.  Timing 
information is also part of the set U.  
4.  Static Content Analysis 
This section describes methods for measuring the amount 
of  information  in  outbound  HTTP  requests  by  statically 
analyzing previous requests and responses. Some portions of 
the request headers are fixed and can be immediately filtered 
if they contain the expected values. Most of the header fields 
only change on rare occasion and can be discounted if they 
are  the  same  as  previous  requests.  The  request  path,  which 
identifies  resources  on  the  web,  is  usually  derived  from 
previous  HTML  pages  returned  by  the  server.  Filtering  out 
repeated  path  values  requires  comparing  paths  to  those  in 
both  prior  requests  and  responses.  Also,  HTTP  form  post 
requests reference  field  names  and default  values contained 
in  HTML  pages.  This  section  elaborates  on  methods  for 
extracting expected HTTP request fields from static content. 
4.1.  HTTP Request Overview 
There are two main types of HTTP requests used by web 
browsers, GET and POST. GET typically obtains resources 
and  POST  sends  data  to  a  server.  An  example  of  a  HTTP 
POST  request  can  be  seen  in  Figure  2.  This  request  is 
comprised of three distinct sections: the request line, headers, 
and the request body. GET requests are very similar except 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
that  they  do  not  have  a  request  body.  The  request  line 
contains  the  path  of  the  requested  file  on  the  server,  and  it 
may also have script parameters. The next part of the HTTP 
request  is  the  header  field  section,  which  consists  of 
“:  ”  pairs  separated  by  line  breaks.  Header 
fields  relay  information  such  as  the  browser  version, 
preferred  language,  and  cookies.  Finally,  the  HTTP  request 
body  comes  last  and  may  consist  of  arbitrary  data.  In  the 
example message, the body contains an encoded name and e-
mail address that was entered into a form. 
4.2.  HTTP Header Fields 
The first type of HTTP header field that we examine is a 
fixed header field. Fixed headers should be the same for each 
request  in  most  cases.  Examples  include  the  preferred 
language and the browser version. We only count the size of 
these headers for the first request from each client, and count 
the edit distance from the most recent request on subsequent 
changes.  Here,  we  treat  all  HTTP  headers  except  for  Host, 
Referer,  and  Cookie  as  fixed.  Some  of  these  header  fields, 
such  as  Authorization,  may  actually  contain  information 
from the user. When these fields contain new data, we again 
count  the  edit  distance  with  respect  to  the  most  recent 
request.  
Next, we look at the Host and Referer header fields. The 
Host  field,  along  with  the  request  path,  specifies  the 
request’s uniform resource locator (URL). We only count the 
size of the Host field if the request URL did not come from a 
link  in  another  page.  Similarly,  we  only  count  the  Referer 
field’s size if does not contain the URL of a previous request. 
Finally, we examine the Cookie header field to verify its 
consistency  with  expected  browser  behavior.  The  Cookie 
field  is  supposed  to  contain  key-value  pairs  from  previous 
server  responses.  Cookies  should  never  contain  UI-layer 
information  from  the  client.  If  the  Cookie  differs  from  its 
expected value or we do not have a record from a previous 
response (this could happen if a mobile computer is brought 
into an enterprise network, for example), then we count the 
edit distance between the expected and actual cookie values. 
At  least  one  known  tunneling  program,  Cooking  Channel 
[7],  deliberately  hides  information  inside  of  the  Cookie 
header  in  violation  of  standard  browser  behavior.  The 
techniques  presented  here  correctly  measure  outbound 
bandwidth for the Cooking Channel program. 
4.3.  Standard GET Requests 
HTTP  GET  requests  are  normally  used  to  retrieve 
resources from a web server. Each GET request identifies a 
resource by a URL that is comprised of the server host name, 
stored in  the  Hostname  header  field,  and the resource path, 
stored  in  the  request  line.  Looking  at  each  HTTP  request 
independently,  one  cannot  determine  whether  the  URL 
contains  UI-layer  information  or  is  the  result  of  previous 
network input (i.e., a link from another page). If we consider 
the entire browsing session, however, then we can discount 
133
request  URLs  that  have  been  seen  in  previous  server 
responses, 
improving  unconstrained 
bandwidth measurements. 
thus  significantly 
The 
first  step 
in  accurately  measuring  UI-layer 
information in request URLs is enumerating all of the links 
on each web page. We parse HTML, Cascading Style Sheet 
(CSS),  and  Javascript  files  to  discover  static  link  URLs, 
which  can  occur  in  many  different  forms.  Links  that  are 
written out dynamically by Javascript are covered in section 
5. Examples of static HTML links include: 
•   Click 
Here!  
•   
•   
•  The less common:  
These  examples  would  cause  the  browser  to  make 
and 