6.  从集群内的一个节点，从待机模式中删除最初运行 `nfsgroup`{.literal}
    的节点。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    从 `待机`{.literal}
    模式中删除节点本身不会导致资源恢复到该节点。这将依赖于
    `资源粘性`{.literal} 值。如需有关 `resource-stickiness`{.literal}
    meta 属性的信息，请参阅
    [配置资源以首选其当前节点](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on){.link}。
    :::
    ``` literallayout
    [root@z1 ~]# pcs node unstandby z1.example.com
    ```
:::
:::
:::
:::
[]{#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters.html}
::: chapter
::: titlepage
# []{#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters.html#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters}第 7 章 集群中的 GFS2 文件系统 {.title}
:::
这部分提供在红帽高可用性集群中配置 GFS2 文件系统的管理步骤。
本节包括：
::: itemizedlist
-   设置包含 GFS2 文件系统文件系统的 Pacemaker 集群的步骤
-   使用加密 GFS2 文件系统设置 Pacemaker 集群的步骤
-   将包含 GFS2 文件系统的 RHEL 7 逻辑卷迁移到 RHEL 8 集群
:::
::: section
::: titlepage
# []{#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster}在集群中配置 GFS2 文件系统 {.title}
:::
这个过程概述了设置 Pacemaker 集群（包括 GFS2
文件系统）所需的步骤。这个示例在三个逻辑卷中创建三个 GFS2 文件系统。
::: itemizedlist
**先决条件**
-   在所有节点上安装并启动群集软件，并创建一个基本的双节点群集。
-   为群集配置隔离。
:::
有关创建 Pacemaker 集群并为集群配置隔离的详情，请参考使用 [Pacemaker
创建红帽高可用性集群](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters){.link}。
::: orderedlist
**流程**
1.  在集群中的两个节点上，启用与您的系统架构对应的弹性存储存储库。例如，要为
    x86_64 系统启用 Resilient Storage 仓库，您可以输入以下
    `subscription-manager`{.literal} 命令：
    ``` literallayout
    # subscription-manager repos --enable=rhel-8-for-x86_64-resilientstorage-rpms
    ```
    请注意，弹性存储存储库是高可用性存储库的超集。如果启用弹性存储存储库，则不需要启用高可用性存储库。
2.  在群集的两个节点上，安装
    `lvm2-lockd`{.literal}、`gfs2-utils`{.literal} 和 `dlm`{.literal}
    软件包。要支持这些软件包，您必须订阅 AppStream 频道和 Resilient
    Storage 频道。
    ``` literallayout
    # yum install lvm2-lockd gfs2-utils dlm
    ```
3.  在群集的两个节点上，将
    `/etc/ lvm/lvm.conf 文件中的 use_lvm`{.literal}
    lockd``{=html} 配置选项设置为 `use_lvmlockd=1`{.literal}。
    ``` literallayout
    ...
    use_lvmlockd = 1
    ...
    ```
4.  将全局 Pacemaker 参数 `no-quorum-policy`{.literal} 设置为
    `冻结`{.literal}。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    默认情况下，no `-quorum-policy`{.literal} 的值被设置为
    `stop`{.literal}，这表示一旦 quorum
    丢失，剩余分区上的所有资源都会立即停止。通常，这个默认行为是最安全、最优的选项，但与大多数资源不同，GFS2
    要求使用 quorum 才可以正常工作。当使用 GFS2 挂载的应用程序和 GFS2
    挂载都丢失时，就无法正确停止 GFS2 挂载。任何在没有 quorum
    的情况下停止这些资源的尝试都会失败，并最终会在每次 quorum
    都丢失时保护整个集群。
    要解决这个问题，请在使用 GFS2 `时将 no-quorum-policy`{.literal}
    设置为 `冻结`{.literal}。这意味着，当 quorum
    丢失时，剩余的分区将不会进行任何操作，直到 quorum 功能被恢复。
    :::
    ``` literallayout
    # pcs property set no-quorum-policy=freeze
    ```
5.  设置 `dlm`{.literal} 资源。这是在集群中配置 GFS2
    文件系统所需的依赖软件包。本例创建 `dlm`{.literal} 资源作为名为
    `locking`{.literal} 的资源组的一部分。
    ``` literallayout
    [root@z1 ~]# pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence
    ```
6.  克隆 `锁定`{.literal}
    资源组，以便资源组可以在集群的两个节点上都活跃。
    ``` literallayout
    [root@z1 ~]# pcs resource clone locking interleave=true
    ```
7.  将 `lvmlockd`{.literal} 资源设置为组 `锁定`{.literal} 的一部分。
    ``` literallayout
    [root@z1 ~]# pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence
    ```
8.  检查集群的状态，以确保在集群的两个节点上启动了 `锁定`{.literal}
    资源组。
    ``` literallayout
    [root@z1 ~]# pcs status --full
    Cluster name: my_cluster
    [...]
    Online: [ z1.example.com (1) z2.example.com (2) ]
    Full list of resources:
     smoke-apc      (stonith:fence_apc):    Started z1.example.com
     Clone Set: locking-clone [locking]
         Resource Group: locking:0
             dlm    (ocf::pacemaker:controld):      Started z1.example.com
             lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
         Resource Group: locking:1
             dlm    (ocf::pacemaker:controld):      Started z2.example.com
             lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
         Started: [ z1.example.com z2.example.com ]
    ```
9.  在集群的一个节点中，创建两个共享卷组。一个卷组将包含两个 GFS2
    文件系统，另一个卷组将包含一个 GFS2 文件系统。
    以下命令在 `/dev/vdb`{.literal} 上创建
    `共享卷组 shared_vg1`{.literal}。
    ``` literallayout
    [root@z1 ~]# vgcreate --shared shared_vg1 /dev/vdb
      Physical volume "/dev/vdb" successfully created.
      Volume group "shared_vg1" successfully created
      VG shared_vg1 starting dlm lockspace
      Starting locking.  Waiting until locks are ready...
    ```
    以下命令在 `/dev/vdc`{.literal} 上创建
    `共享卷组 shared_vg2`{.literal}。
    ``` literallayout
    [root@z1 ~]# vgcreate --shared shared_vg2 /dev/vdc
      Physical volume "/dev/vdc" successfully created.
      Volume group "shared_vg2" successfully created
      VG shared_vg2 starting dlm lockspace
      Starting locking.  Waiting until locks are ready...
    ```
10. 在集群的第二个节点中，为每个共享的卷组启动锁管理器。
    ``` literallayout
    [root@z2 ~]# vgchange --lock-start shared_vg1
      VG shared_vg1 starting dlm lockspace
      Starting locking.  Waiting until locks are ready...
    [root@z2 ~]# vgchange --lock-start shared_vg2
      VG shared_vg2 starting dlm lockspace
      Starting locking.  Waiting until locks are ready...
    ```
11. 在集群的一个节点中，创建共享逻辑卷并使用 GFS2
    文件系统格式化卷。每个挂载文件系统的节点都需要一个日志。确保为集群中的每个节点创建足够日志。
    ``` literallayout
    [root@z1 ~]# lvcreate --activate sy -L5G -n shared_lv1 shared_vg1
      Logical volume "shared_lv1" created.
    [root@z1 ~]# lvcreate --activate sy -L5G -n shared_lv2 shared_vg1
      Logical volume "shared_lv2" created.
    [root@z1 ~]# lvcreate --activate sy -L5G -n shared_lv1 shared_vg2
      Logical volume "shared_lv1" created.
    [root@z1 ~]# mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1
    [root@z1 ~]# mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2
    [root@z1 ~]# mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1
    ```
12. 为每个逻辑卷创建一个
    `LVM 激活的资源`{.literal}，以便在所有节点上自动激活该逻辑卷。
    ::: orderedlist
    1.  在卷组 shared\_ `vg1 中为逻辑卷 shared _lv1`{.literal} 创建名为
        sharedlv1``{=html}
        `的 LVM 激活资源`{.literal}。``{=html}此命令还会创建包含该资源的资源组
        `shared_vg1`{.literal}。在这个示例中，资源组的名称与包含逻辑卷的共享卷组的名称相同。
        ``` literallayout
        [root@z1 ~]# pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd
        ```
    2.  在卷组 shared\_ `vg1`{.literal} 中为逻辑卷
        `shared _lv2`{.literal} 创建名为 sharedlv2``{=html}
        `的 LVM 激活资源`{.literal}。此资源也是资源组
        `shared_vg1`{.literal} 的一部分。
        ``` literallayout
        [root@z1 ~]# pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd
        ```
    3.  在卷组 shared\_ `vg2`{.literal} 中为逻辑卷
        `shared _lv1 创建名为 sharedlv`{.literal} 3``{=html}
        `的 LVM 激活资源`{.literal}。此命令还会创建包含该资源的资源组
        `shared_vg2`{.literal}。
        ``` literallayout
        [root@z1 ~]# pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd
        ```
    :::
13. 克隆两个新资源组。
    ``` literallayout
    [root@z1 ~]# pcs resource clone shared_vg1 interleave=true
    [root@z1 ~]# pcs resource clone shared_vg2 interleave=true
    ```
14. 配置排序限制，以确保首先启动包含 `dlm`{.literal} 和
    `lvmlockd`{.literal} 资源的 `锁定`{.literal} 资源组。
    ``` literallayout
    [root@z1 ~]# pcs constraint order start locking-clone then shared_vg1-clone
    Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)
    [root@z1 ~]# pcs constraint order start locking-clone then shared_vg2-clone
    Adding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)
    ```
15. 配置 colocation 约束，以确保 `vg1`{.literal} 和 `vg2`{.literal}
    资源组在与 `锁定`{.literal} 资源组相同的节点上启动。
    ``` literallayout
    [root@z1 ~]# pcs constraint colocation add shared_vg1-clone with locking-clone
    [root@z1 ~]# pcs constraint colocation add shared_vg2-clone with locking-clone
    ```
16. 在集群中的两个节点中，验证逻辑卷是否活跃。这可能会延迟几秒钟。
    ``` literallayout
    [root@z1 ~]# lvs
      LV         VG          Attr       LSize
      shared_lv1 shared_vg1  -wi-a----- 5.00g
      shared_lv2 shared_vg1  -wi-a----- 5.00g
      shared_lv1 shared_vg2  -wi-a----- 5.00g
    [root@z2 ~]# lvs
      LV         VG          Attr       LSize
      shared_lv1 shared_vg1  -wi-a----- 5.00g
      shared_lv2 shared_vg1  -wi-a----- 5.00g
      shared_lv1 shared_vg2  -wi-a----- 5.00g
    ```
17. 创建文件系统资源在所有节点中自动挂载每个 GFS2 文件系统。
    您不应该将文件系统添加到 `/etc/fstab`{.literal} 文件中，因为它将作为
    Pacemaker 集群资源进行管理。挂载选项可作为资源配置的一部分通过
    `options=`{.literal} 选项指定。运行
    `pcs resource describe Filesystem`{.literal}
    命令以查看完整配置选项。
    以下命令可创建文件系统资源。这些命令在包含该文件系统逻辑卷资源的资源组中添加每个资源。
    ``` literallayout
    [root@z1 ~]# pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence
    [root@z1 ~]# pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv2" directory="/mnt/gfs2" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence
    [root@z1 ~]# pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device="/dev/shared_vg2/shared_lv1" directory="/mnt/gfs3" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence
    ```
:::
::: orderedlist
**验证步骤**
1.  验证 GFS2 文件系统是否挂载到集群的两个节点中。
    ``` literallayout
    [root@z1 ~]# mount | grep gfs2
    /dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
    /dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
    /dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)
    [root@z2 ~]# mount | grep gfs2
    /dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
    /dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
    /dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)
    ```
2.  检查集群的状态。
    ``` literallayout
    [root@z1 ~]# pcs status --full
    Cluster name: my_cluster
    [...]