for each value bit one at a time. In addition, storing larger
values further amortizes the 0.5 bits of overhead added by the
ﬁrst-level key-to-group mapping.
Summary The core SetSep data structure construction speed
is fast enough for a variety of important applications in which
the read rate is larger than the (already high) update rate that
SetSep can handle. ScaleBricks uses SetSep for its global
partition tables, which fall into this category.
6.1.2 Lookup
Figure 7 shows the local lookup throughput of SetSep for
different numbers of FIB entries (keys). In addition, given a
FIB size, this ﬁgure also compares SetSep performance with
different batch sizes as discussed in Section 5, varying from 1
(no batching) to 32 (the maximum packet batch size provided
by DPDK). All lookup experiments use 2-bit values, a “16+8”
conﬁguration, and 16 threads.
These lookup micro-benchmarks provide three insights.
First, batching generally increases the lookup performance of
SetSep. When the batch size is increased to 17, the lookup
throughput is ∼520 Mops (million operations per second)
even with 64 million keys; batch sizes larger than 17 do
not further improve performance. Second, as the number
of FIB entries increases from 32 million to 64 million, the
performance drops dramatically. This occurs because the
64 million entry, 2-bit SetSep exceeds the size of L3 cache,
occupying 28 MiB of memory. Third, for small FIBs (e.g.,
500 K entries), lookup performance is actually higher without
batching. This too arises from caching: These small structures
ﬁt entirely in L3 or even L2 cache, where the access latency
is low enough that large batches are not required, but merely
increase register pressure.
Summary ScaleBricks batches for all table sizes to ensure
fast-enough lookup performance regardless of the number of
entries. Because DPDK receives packets in batches, Scale-
Bricks handles incoming packets using the dynamic batching
policy from CuckooSwitch [34]: instead of having a ﬁxed
batch size, when a CPU core receives a batch of packets
from DPDK, ScaleBricks looks up the entire batch in SetSep.
Therefore, the batch size of SetSep adjusts semi-automatically
to the offered load.
6.2 Macro-Benchmark: ScaleBricks Cluster
As a concrete application, we optimized the Packet Forward-
ing Engine of Connectem’s EPC stack by migrating it to
ScaleBricks. Importantly, the stack already used the Intel
DPDK, so structural changes were small. For context, the
Packet Forwarding Engine typically runs on three of the cores
249Construction
throughput
Fallback
ratio
Total
size
Construction setting
x + y bits to store a hash function, x-bit hash function index and y-bit array
16.00 MB
16.64 MB
20.00 MB
0.54 Mkeys/sec
2.42 Mkeys/sec
2.47 Mkeys/sec
1-bit value
1-bit value
1-bit value
1 thread
1 thread
1 thread
0.00%
1.15%
0.00%
2-bit value
3-bit value
4-bit value
1-bit value
1-bit value
1-bit value
1-bit value
increasing the value size
1 thread
1 thread
1 thread
0.24 Mkeys/sec
0.18 Mkeys/sec
0.14 Mkeys/sec
0.00%
0.00%
0.00%
using multiple threads to generate
2 threads
4 threads
8 threads
16 threads
0.93 Mkeys/sec
1.56 Mkeys/sec
2.28 Mkeys/sec
2.97 Mkeys/sec
0.00%
0.00%
0.00%
0.00%
28.00 MB
40.00 MB
52.00 MB
16.00 MB
16.00 MB
16.00 MB
16.00 MB
16+8
8+16
16+16
16+8
16+8
16+8
16+8
16+8
16+8
16+8
Bits/
key
2.00
2.08
2.50
3.50
5.00
6.50
2.00
2.00
2.00
2.00
Table 1: Construction throughput of SetSep for 64 M keys with different settings
Figure 6: Cluster setup
Figure 7: Local lookup throughput of SetSep (GPT)
in each node in the EPC cluster. The initial system was bal-
anced to handle smaller numbers of ﬂows, but the total system
throughput drops when the number of ﬂows (and thus the size
of the FIB) grows larger. Thus, while we focus here on just
the PFE throughput, improvements to the PFE do improve
the total system throughput.
We measure PFE performance using 4 dual-socket servers,
each with two Intel Xeon E5-2697 v2 CPUs running at 2.70
GHz, each with a 30 MiB L3 cache. Each machine has 128
GiB DDR3 RAM and two dual-port Intel 82599ES 10GbE
NICs.
Each server uses three of its four 10Gb ports: one as the
interface to the Internet, one as the interface to the base sta-
tions, and the other as the interconnect interface. Each port
is attached to one core on CPU socket 0, using three cores
in total. For all the experiments, we pre-populate the system
with a number of static tunnels. As discussed in Section 2,
only downstream (Internet to mobile device) packets require
inter-cluster forwarding; therefore, the core assigned to han-
dle the interface connected to the base stations is not used in
our experiments. We simulate the downstream trafﬁc using a
Spirent SPT-N11U Ethernet testing platform [31]. Figure 6
depicts the conﬁguration for the benchmark.
Figure 8: Single node packet forwarding throughput us-
ing 30 MiB L3 cache
The forwarding engine originally implemented its FIB us-
ing a chaining hash table, the performance of which drops
dramatically as the number of tunnels increases. To evaluate
the beneﬁts of ScaleBricks, we replace the original imple-
mentation with two alternative hash table designs—DPDK’s
rte_hash and our extended cuckoo hash table. Figure 8
shows the single node packet forwarding throughput using
these hash tables, with and without ScaleBricks. Without
ScaleBricks means full duplication, as depicted in Figure 2b.
Both the hash table and use of the SetSep GPT improve
throughput measurably for the PFE. Even though our ex-
tended cuckoo hash table requires one additional memory read
compared to the original design [34], it improves throughput
by 50% over the DPDK’s rte_hash. More key to this work,
ScaleBricks improves the single node throughput by up to
20% and 22% within systems using rte_hash and extended
cuckoo hash table, respectively. Two major factors contribute
to this performance improvement. First, reducing the number
of entries hosted by each node means smaller hash tables. In
this experiment, hash table size is reduced by up to 75%. The
smaller table allows many more of the table entries to ﬁt in L3
cache, substantially increasing througput. Second, without
ScaleBricks, all the packets coming from the trafﬁc gener-
Hardware SwitchTrafﬁc GeneratorPortNode 4Node 2Node 3CoreNode 1External CableInterconnect Cable500K1M2M4M8M16M32M64M# of FIB Entries0100200300400500600700800Throughput (Mops)w/o batching239179101112131415Throughput (Mpps)Up to 22%Up to 19%1M2M4M8M16M32M# of FIB Entries0123rte_hash (Full Duplication)rte_hash (ScaleBricks)cuckoo_hash (Full Duplication)cuckoo_hash (ScaleBricks)250RFC 2544 [6] benchmark tool. We create 1 M static tun-
nels for the latency test. Two interesting observations stand
out from the average latency results reported in Figure 10.
First, compared to the baseline, ScaleBricks reduces the av-
erage latency by up to 10%. We believe that ScaleBricks is
able to service more of the lookups from cache because of
the smaller table size, thereby reducing the memory access
latency. Second, compared with the hash partitioning, the
latency of ScaleBricks is lower by up to 34%. This matches
our expectation. In summary, ScaleBricks improves the end-
to-end latency via faster memory access and/or eliminating
the extra inter-cluster hop.
Update Rate We measure the update rate of ScaleBricks as
follows: a single CPU core can handle 60 K updates/sec.
Using a decentralized update protocol allows ScaleBricks to
distribute updates to all the nodes in a cluster. In a 4-node
ScaleBricks cluster, using one dedicated core on each server
provides an aggregated rate of 240 K updates/sec. Because
the update is essentially parallelizable, by adding more CPU
cores, we can achieve higher update rate if necessary.
6.3 Scalability of ScaleBricks
Compared to naive FIB duplication, ScaleBricks provides a
practical leap in scaling by using a compact representation for
the information that must be replicated on all nodes. To put
its contributions in perspective, it is useful to compare to both
FIB duplication and to two-hop FIB partitioning. Recall that
FIB duplication and ScaleBricks require only a single hop
across the internal switch fabric. Two-hop FIB partitioning,
in contrast, incurs higher forwarding cost, but achieves true
linear FIB scaling, which ScaleBricks does not.
Although SetSep is compact, its size (i.e., bits per entry)
does increase slowly with the number of nodes in the cluster.
This creates a scaling tension. At ﬁrst, ScaleBricks scales
almost linearly: for n nodes, each must store only F
n of the F
total FIB entries. But those FIB entries are large—perhaps
64 or 128 bits each. In contrast, the GPT must store all F
entries on each node, but using only F logn bits. At ﬁrst,
logn is very small, and so the GPT is much smaller than
the original FIB. But as n increases and more entries are
added to the FIB, the GPT size begins to grow, and after
32 nodes, adding more servers actually decreases the total
number of FIB entries that can be handled. Analytically, the
total number of 64-bit FIB entries that can be stored in an n-
node cluster scales as Mn/(64 + (0.5 + 1.5logn)n), where M
is the memory capacity per node. As a result, a ScaleBricks
cluster can scale up to handle 5.7 times more FIB entries
compared with a cluster using full FIB duplication.
Assuming each server uses 16 MiB of memory, Figure 11
shows analytically the total forwarding entries enabled using
full FIB duplication, ScaleBricks, and hash partitioning, for
clusters from 1 to 32 servers. ScaleBricks scales better when
FIB entries are larger than 64 bits because the total number
of FIB entries as well as the size of GPT will decrease.
Figure 9: Single node packet forwarding throughput us-
ing 15 MiB L3 cache
Figure 10: End-to-end latency of different approaches
ator are looked up by the core handling that port. We refer
to this core as the “external core.” The core processing the
trafﬁc received over the internal switch (the “internal core”),
however, is mostly idle. In ScaleBricks, the external core on
each server performs the global partition table lookup for all
the packets, plus the hash table lookup for only those packets
belonging to ﬂows that are handled by that same server. The
load is therefore more balanced, as the internal cores also
perform a hash lookup, but in a smaller table. These two
effects combine to improve the throughput and core utiliza-
tion substantially. Although alternative designs might make
better use of the idle cycles by multiplexing packet I/O and
hash lookups on the internal cores, such designs—if they
exist—are likely complex and introduce non-trivial overhead
to switch between internal and external functionality on the
internal core.
Throughput with Smaller Cache The EPC forwarding en-
gine shares the CPU cache with other applications. To evalu-
ate the throughput of ScaleBricks under such circumstances,
we launch a bubble thread on a separate core to consume half
of the L3 cache. Figure 9 shows the performance of different
hash tables, with and without ScaleBricks, when there is only
15MiB of L3 cache available. Comparing with the results
shown in Figure 8, the throughput of all tables drop with the
reduced cache, but the relative beneﬁts of ScaleBricks remain.
Latency We measure the end-to-end packet forwarding la-
tency of six different approaches using Spirent SPT-N11U’s
9101112131415Throughput (Mpps)Up to 23%Up to 19%1M2M4M8M16M32M# of FIB Entries0123rte_hash (Full Duplication)rte_hash (ScaleBricks)cuckoo_hash (Full Duplication)cuckoo_hash (ScaleBricks)18232833Latency (us)Reduced by 10%Reduced by 34%rte_hash(Full Duplication)rte_hash(XBricks)rte_hash(Hash Partitioning)cuckoo_hash(Full Duplication)cuckoo_hash(XBricks)cuckoo_hash(Hash Partitioning)Full DuplicationXBricksHash Partitioning05251application, and we are currently seeking for more applica-
tions that can beneﬁt from ScaleBricks.