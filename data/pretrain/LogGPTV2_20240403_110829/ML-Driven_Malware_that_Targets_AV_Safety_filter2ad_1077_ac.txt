is the one closest
3) Invokes the “scenario matcher” (SM) module ( 2 in Fig. 1),
which uses the world state (Wt) to determine whether the
identiﬁed object is vulnerable to one of the attack vectors
(as shown in 3 and discussed in §III-C).
4) Uses the “safety hijacker” (SH) (shown as 4
in Fig. 1)
to decide when to launch the attack (t), and for how long
(t + K). The SH estimates the impact of the attack by using
a shallow 3-hidden-layered NN, in terms of reduced safety
potential (δ). The malware launches the attack only if the
reduced safety potential drops below a predeﬁned threshold
(10 meters). We determine the threshold through simulation
of driving scenarios that lead to emergency braking by the
EV. To evade detection, the malware ensures that K does not
exceed a pre-deﬁned threshold (see line 15 in algorithm 1).
K is obtained by characterizing the continuous misdetection
of an object associated with the “object detector” in the
normal (i.e., without attacks) driving scenarios executed in
the simulator.
Phase 3. Triggering the attack. RoboTack:
1) Uses the “trajectory hijacker” ( 5
in Fig. 1) to corrupt the
camera feed. The trajectory hijacker perturbs the camera
sensor data such that i) the trajectory of the object (e.g., a
school bus) is altered to match the selected attack vector
(e.g., Move_Out), and ii) the trajectory of the object does
not change signiﬁcantly, thus evading detection.
2) Attacks the trajectory of the victim object for the next K
time-steps, chosen by the safety hijacker.
E. An Example of a Real Attack
Fig. 3 shows an example of a Move_Out attack. Here we
show two different views: i) a simulation view, which was
generated using a driving scenario simulator, and ii) an ADS
view, which was rendered using the world-state visualizer.
RoboTack continuously monitors every camera frame using
“scenario matching” (SM) to identify a target object for which
the perceived trajectory by the EV can be hijacked. If SM does
not identify any target object of interest, it skips the rest of the
step and waits for the next camera frame. As shown in Fig. 3
(a) and (b), at time-step t, SM identiﬁed an SUV (i.e., target
vehicle) as a target object of interest, and returned "Move_Out"
as a matched attack vector, as the SUV was already in the Ego
lane. Next, RoboTack launched "safety hijacker" to determine
the reduced safety potential of the attack and the number
of time-steps the attack would need to be maintained. As it
turns out, the "safety hijacker" determined that the reduced
safety potential could cause an accident, so RoboTack launched
"trajectory hijacker" to perturb the camera sensor data as shown
in Fig. 3 (c). Its impact on the trajectory is shown in Fig. 3(d).
Camera sensor data are perturbed by modifying individual
pixels as shown in the white area (in the bounding box (red
square) of the target object) for illustration purposes, because
originally these pixels were modiﬁed in a way that was almost
invisible to the human eye. Because of this attack, the EV
collided with the target object as shown in Fig. 3(e) and (f).
IV. ALGORITHMS AND METHODOLOGY
In this section, we outline the three key steps taken by the
malware: 1) in the monitoring phase, selecting the candidate
attack vector by using the scenario matcher (§IV-A); 2) in
the monitoring phase, deciding when to attack by using the
safety hijacker (§IV-B); and 3) in the trigger phase, perturbing
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
117
(cid:4)
(cid:5) Past object states
(cid:5) Camera feed
(cid:5) Flag indicating if the attack is active
(cid:5) Number of continuous attacks
(cid:5) Index of the target object
(cid:5) Perturbed image with adversarial patch
Algorithm 1 Attack procedure at each time-step.
Input: ˆSt−1:t−2
Input: I1
t
Global: attack
Global: K
Global: i
Output: I1
1: α ← ∅
t
2: Ot, ˆSt ← P erception(It)
3: if attack = F alse then
4:
5:
6:
7:
8: end if
9: if α (cid:4)= ∅ then
i, δt ← Saf etyM odel(ˆSt)
rel,t ← calcV elocity(ˆsi
(cid:2)vi
t:t−1)
rel,t ← calcAcceleration(ˆsi
(cid:2)ai
α ← ScenarioM atcher(ˆsi
t)
(cid:5) From deﬁnition 5
t:t−2)
attack, K ← Saf etyHijacker((cid:2)ai
10:
11: end if
12: if attack = T rue ∧ K > 0 then
(cid:4) ← T rajectoryHijacker(i, I1
I1
K ← K − 1
t
if K = 0 then
attack ← F alse
13:
14:
15:
16:
17:
18: end if
end if
rel,t, (cid:2)vi
rel,t, (cid:2)δt, α)
t , oi
t, ˆsi
t−1, α)
camera sensor feeds using the trajectory hijacker. These steps
are described in algorithm 1.
A. Scenario Matcher: Selecting the Target Trajectory
The goal of the scenario matcher is to check whether the
closest object to the EV (referred to as the target object)
is vulnerable to any of the candidate attack vectors (i.e.,
Move_Out, Move_In, and Disappear). This is a critical step
for the malware, as it wants to avoid launching 1) an attack
if there are no objects next to or in front of the EV; or 2) an
attack when the object is actually executing the would-be bogus
driving maneuver (e.g., selecting attack vector α = Move_Out
when the target is moving out of the Ego lane anyway). The
scenario-matching algorithm is intentionally designed as a rule-
based system (whose rules are listed in Table I) to minimize
its execution time, and hence evade detection.
Note that "Scenario Matcher" can interchangeably choose
between the Move_Out and Disappear attack vectors. However,
in our work, we found that Disappear, which requires a large
perturbation in trajectory, is better suited for attacking the
pedestrians because the attack window is small. In contrast,
the attack window for vehicles is large. Therefore, for vehicles,
RoboTack uses Move_Out. This is described in detail in §VI.
B. Safety Hijacker: Deciding When to Attack
To cause a safety violation (i.e., a crash or emergency brake),
the malware will optimally attack the vehicle when the attack
results in δ ≤ 4 meters. The malware incorporates that insight
into the safety hijacker to choose the start and stop times of the
attack by executing the safety hijacker at every time-step. The
safety hijacker at time-step t takes ((cid:2)vi
rel,t), δt, and α as
inputs. It outputs the attack decision (i.e., attack or no-attack)
rel,t, (cid:2)ai
118
TO trajectory
Moving In
Keep
Moving Out Move_In
TO: Target object
TO in EV-lane
—
Move_Out/Disappear Move_In
TO not in EV-lane
Move_Out/Disappear
—
Table I: Scenario Matching Map
and the number of time-steps K for which the attack must
continue to be successful (line 16 in algorithm 1).
Let us assume that the malware has access to an oracle
function fα for a given attack vector α that predicts the future
safety potential of the EV when it is subjected to the attack
type α for k continuous time-steps,
rel,t, (cid:2)ai
δt+k = fα((cid:2)vi
rel,t, δt, k).
(1)
Later in this section, we will describe a machine-learning
formulation that approximates fα using a neural network, and
describe how to integrate it with the malware. The malware
decides to attack only when the safety potential δt+k is less
than some threshold γ. Ideally, the malware should attack when
γ = 4 (i.e., corresponding to the the δ for the crash).
In order to evade detection and disguise the attack as noise,
the installed malware should choose the "optimal k," which we
refer to as K (i.e., the minimal number of consecutive camera
sensor frame perturbations), using the information available at
time-step t. The malware can use the oracle function fα(.) to
decide on the optimal number of time-steps (K) for which the
attack should be active. The malware decides to attack only if
k ≤ Kmax, where Kmax is the maximal number of time-steps
during which a corruption of measurements cannot be detected.
This is formalized in (2).
K = argmin
k · (I(δt+k ≤ γ) = 1)
(2)
k
Finally, the malware must take minimal time to arrive at the
attack decision. However, in the current formulation, calculating
K can be very costly, as it is necessary to evaluate (2) using
fα (which is an NN) for all k ≤ Kmax. We accelerate the
evaluation of K by leveraging the fact that for our scenarios
≤ 0.
(§V-C), fα is non-increasing with increasing k when (cid:2)arelt
Hence, we can do a binary search between k ∈ [0, Kmax] to
ﬁnd K in O(log Kmax) steps.
Estimating fα using an NN. We approximate the oracle
function fα using a feed-forward NN. We use an NN to
approximate fα to model the uncertainty in the ADS due
to use of non-deterministic algorithms. Hence, the malware
uses a uniquely trained NN for each attack vector. The input to
the NN is a vector [δt, (cid:2)vrelt , (cid:2)arelt , k]. The model predicts δt+k
after k consecutive frames, given the input. Intuitively, the NN
learns the behavior of the ADS (e.g., conditions for emergency
braking) and kinematics under the chosen attack vector, and it
infers the safety potential δt+k to the targeted object from the
input. We train the NN using a cost function (L) that minimizes
the average L2 distance between the predicted δt+k and the
ground-truth δG
t+k for the training dataset Dtrain.
t+k(cid:8)2
L =
t+k − δi
(cid:2)
(cid:8)δ
G,i
1
2
(3)
|Dtrain|
i∈Dtrain
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
We use a fully connected NN with 3 hidden layers
(100, 100, 50 neurons), the ReLU activation function, and
dropout layers with a dropout rate of 0.1 to estimate fα. The
speciﬁc architecture of the NN was chosen to reduce the
computational time for the inference with sufﬁcient learning
capacity for high accuracy. The NN predicts the safety potential
after the attack within 1m and 5m for pedestrian and vehicles,
respectively.
The NN was trained with a dataset D collected from a set
of driving simulations ran on Baidu’s Apollo ADS. To collect
training data, we ran several simulations, where each simulation
had a predeﬁned δinject and a k, i.e., an attack started as soon
as the δt = δinject, and continued for k consecutive time-steps.
The dataset characterized the ADS’s responses to attacks. The
network was trained using the Adam optimizer with a 60%-40%
split of the dataset between the training and the validation.
C. Hijacking Trajectory: Perturbing Camera Sensor Feeds
In this section, we describe the mechanism through which
the malware can perturb the camera sensor feeds to successfully
mount the attack (i.e., execute one of the attack vectors) once
it has decided to attack the EV. The malware achieves that
objective using a trajectory hijacker.
The attack vectors used in this paper require that the malware
perturb the camera sensor data (by changing pixels) in such a
way that the bounding box (ˆsi
t) estimated by the multiple-object
tracker (used in the perception module) at time t moves in a
given direction (left or right) by ωt.
t−1) such that the following conditions hold:
The objective of moving the bounding box ˆsi
t in a given
direction (left or right) can be formulated as an optimization
problem. To solve it, we modify the model provided by Jia et
al. [15] to evade attack detection. We ﬁnd the translation vector
ωt at time t that maximizes the cost M of Hungarian matching
(recall M from Fig. 1) between the detected bounding box, oi
t,
and the existing tracker state ˆsi
t away
from ˆsi
• Threshold M ≤ λ ensures that oi
t must still be associated
t−1, i.e., M ≤ λ. λ can be
with its original tracker state ˆsi
found experimentally for a given perception system and
depends on Kalman parameters. This condition is relaxed
when the selected attack α = "Disappear."
• ωt ∈ [μ − σ, μ + σ] is within the Kalman noise parameters
(μ, σ) of the selected candidate object. This condition ensures
that the perturbation is within the noise.
t−1 (i.e., pushing the oi
• Threshold IoU (oi
t + ωt, patch) ≥ γ ensures that
the
adversarial patch patch should intersect with the detected
bounding box, oi
t, to restrict the search space of the patch.
This condition can be removed when the attacker has access
to the ADS, and can directly perturb oi
t.
max
ωt
M (oi
t + ωt, ˆsi
t−1)
s.t. M ≤ λ,
t + ωt, patch) ≥ γ,
IoU (oi
(4)
ωt ∈ [μ − σ, μ + σ]
Finally, the malware should stop maximizing the distance
between the oi
t−1 when the object tracker has moved
laterally by Ω (i.e., the difference between the observed lateral
t and ˆsi
119
(cid:4).
In our experiments, we found K
distance and the estimated lateral distance) since the attack
start time t − K