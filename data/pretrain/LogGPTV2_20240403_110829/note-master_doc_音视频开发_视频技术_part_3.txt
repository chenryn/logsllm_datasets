通过一个状态机，有三个状态，分别是上升、保持和下降状态。当处于上升状态时，速率控制器需要提升带宽值；当处于下降状态时，需要降低带宽值；当处于保持状态时，则不更新带宽值，如果之前状态机处于下降状态，则更改为保持状态；如果状态机之前处于保持状态，则更改为上升状态；如果是上升状态那就不用变化，基本原理就是要不断逼近极限
### 基于丢包预测
根据 Transport-CC 报文反馈的信息计算丢包率，然后再根据丢包率的多少直接进行带宽调整更新
- 如果丢包率  10%，认为网络状况不好，需要降低带宽值，带宽值等于当前预估带宽值 * (1 - 0.5 * 丢包率)
在网络变差的时候，预估带宽会快速的被下调，但是网络变好的时候预估带宽会比较缓慢的上升，这点跟TCP的[慢启动](/计算机网络/运输层.md#慢启动)原理是一样的
### 最大带宽探测
在程序刚开始启动的时候使用并在程序运行的过程中进行周期性的探测，基本原理是通过发送探测包与Transport-CC包记录发送时长得到发送端与接受端的最大带宽取最小值
## 码控算法
用算法来控制编码器输出码流的大小，码控就是为每一帧编码图像选择一个合适的 QP 值的过程
### VBR
画面复杂码率就高点，简单码率就低点，比较适合视频点播和短视频场景
### CQP
从头到尾每一个画面都是用同一个 QP 值去编码
### CRF
画面运动大的时候，它会根据具体算法提高 QP 值；在画面运动小的时候，它会降低 QP 值
### CBR
需要设置一个目标码率值给编码器。编码器在编码的时候不管图像画面复杂或简单、运动多或运动少的时候，都尽量使得输出的码率接近设置的目标码率，这种算法适合RTC场景下的视频传输
## SVC
指一个码流当中，将数据包分为多层，低层数据包质量较低，高层的数据包解码要依赖低层的数据包，这样针对不同的网络状况，就可以自适应解码不同质量的视频
时域SVC：
通过调整参考帧结构就能实现分层编码。低层的帧不会参考高层的帧，质量低的表现就是帧率低
![层0是I帧不需要参考，层1是P帧，参考层0](/assets/20230322202829.webp)
空域SVC：在一个码流当中分出多个码流出来，第 0 层是一个可以独立解码的码流，只是分辨率是 360P。第 1 层依赖于第 0 层，两个层次加起来是 720P 分辨率的码流...绝大多数的解码器都不支持空域 SVC
## 卡顿与花屏
卡顿可能的原因：
- 采集或设置的帧率不够时，会造成两帧之间的时间间隔过长
- 性能不够，导致前处理或者编码耗时太长，从而导致卡顿
- 输出码率超过实际网络带宽
- 复杂帧编码后过大或者 I 帧比较大，导致分包太多发送之后网络丢包，可以在编码打包之后、发送之前，加一个平滑发送的模块来平滑地发送视频包。这个模块在 WebRTC 中叫做 PacedSender
- 网络本身丢包
- 重传也没有收到包，导致帧不完整，继而导致没有帧可以解码成功，需要发送关键帧请求报文给发送端，得到一个IDR帧来作为后续解码的参考
花屏可能的原因：
- 帧没接收完整，为了保证接收的帧的完整，有两种方法
![20230321205606](/assets/20230321205606.webp)
![20230321205626](/assets/20230321205626.webp)
- 参考帧不完整，这点跟卡顿是一样的
- 解码格式搞错了
- Stride 问题
## 封装
将一帧帧视频和音频数据按照对应封装的标准有组织地存放在一个文件里面，并且再存放一些额外的基础信息，比如说分辨率、采样率等信息
### FLV
![FLV 的总体结构](/assets/20230323202349.webp)
![各部分具体内容](/assets/20230323202737.webp)
播放的速度还有音视频同步都需要依赖时间戳的值，这个时间戳的单位是 ms。而 RTP 的时间戳单位是 1/90000 秒
有三种类型的 Tag Data 数据：
- Script：存放的是 MetaData 数据，主要包括宽、高、时长、采样率等
- 音频
- 视频
### MP4
视频的一帧和音频的一段编码数据称为一个 sample。连续的几个 sample 称之为 chunk，而视频的所有 sample 称为一个视频 track，同样音频的所有 sample 也称为一个音频 track
MP4 主要由最外层的三个 box组成，分别是 File Type box（ftyp box）、Movie box（moov box）和 Media Data box（mdat box），moov box 里面存放了音视频的基本信息和每一个音视频数据的具体位置
![整体结构](/assets/20230323203624.webp)
- moov box：用来存放 Metadata 信息
- mvhd box：存放文件的基本信息，比如说 MP4 文件的创建时间、时间单位、总时长等信息
- trak box：音频和视频各有一个
- tkhd box：表示 track 的一些基本信息，比如说视频的宽高信息和音频的音量信息等
- mdhd box：里面最重要的一个值就是时间单位 time scale，这个时间单位是 sample 的时间戳的单位，控制播放速度和音视频同步都需要使用到这个值
- hdlr box：包含了 track 的类型信息，表明是音频还是视频 track
- stbl box：存放着可以计算得到每一个 chunk 的偏移地址、每一个 sample 在文件中的地址信息和大小、每一个 sample 的时间戳和每一个视频 IDR 帧的地址信息
- stts box：放置的是每一个 sample 的时长
- stss box 中放置的是哪些 sample 是关键帧
- stsc box 中放置的是 sample 到 chunk 的映射表
- stco box 或 co64 box 中放置着每个 chunk 在文件中的偏移地址
- stsz box 中放置着每一个 sample 的大小
## 音画同步
PTS 表示的是视频帧的显示时间，DTS 表示的是视频帧的解码时间。对于同一帧来说，DTS 和 PTS 可能是不一样的，主要是因为 B 帧可以同时参考前后的帧造成的
![20230326133222](/assets/20230326133222.webp)
### 视频同步到音频
这种方式最常用。
音频是指音频按照自己的节奏播放，不需要调节。如果视频相对音频快了的话，就延长当前播放视频帧的时间，以此来减慢视频帧的播放速度。如果视频相对音频慢了的话，就加快视频帧的播放速度，甚至通过丢帧的方式来快速赶上音频
- 如果视频快了。延长正在播放的视频帧的播放时间
- 如果视频慢了。加快视频帧的渲染速度
### 音频同步到视频
视频按照自己的节奏播放，主要是调节音频的速度，但由于音频调整会导致音调变化，这点人耳是很敏感的，所以不太会使用
### 音频和视频都做调整同步
音频快了就将音频的速度调低一些或者将视频的速度调高一些，视频快了就将视频的速度调低一些或者将音频的速度调高一些。这种一般在非 RTC 场景不怎么使用