required to ensure all system information is discovered only
during guest OS initialization, the cloud provider can make
these minimal changes for the beneﬁt of all of its customers.
Importantly, this does not restrict what applications and
guest OS kernel modules the customer can run, so restrict-
ing the customer’s choice to a ﬁxed set of guest OS kernels
is not a signiﬁcant problem.
Once the initial guest OS kernel bootup sequence is com-
plete and the underlying system conﬁguration has been
learned by the guest OS, the temporary hypervisor is dis-
abled. At this point the guest VM execution switches from
code under the control of the cloud infrastructure provider
to the customer’s code which can run any applications and
load any guest OS kernel modules desired.
In addition to supporting the device discovery and sys-
tem information instructions, the guest OS will utilize one
2SR-IOV is the Single-Root I/O Virtualization speciﬁcation.
405additional device during its initialization.
In particular, a
high precision timer, such as the high-precision event timer
(HPET), is needed temporarily during the boot process of
the Linux kernel. First, it is used as an external reference
to determine the clock frequency of the processor. The lo-
cal APIC timer is conﬁgured to generate an interrupt in a
certain number of clock cycles (rather than certain time).
Therefore, it is important to know the exact frequency of
the processor clock so the local APIC can be used as an ac-
curate timer3. Second, this high precision timer is used to
emulate the CMOS-based real-time clock – that is, the bat-
tery backed clock which tells the date and time. Again, the
temporary hypervisor can emulate this functionality to let
the guest discover the processor core’s frequency and get the
time of day. After this, the clock is not needed anymore4.
Finally, since we allow this system discovery only during
bootup, the OS must be sure to gather all of the informa-
tion that may be needed over the lifetime of the VM. We
capitalize on the fact that we are providing the guest OS
kernel by making minor modiﬁcations so that the informa-
tion is gathered during guest OS bootup and cached by the
OS. This removes the need for instructions like the CPUID
to run during the lifetime of the OS to determine hardware
conﬁguration, and therefore the need for a hypervisor re-
sponse.
4.4 Avoiding Indirection
Today, because hypervisors present an abstracted view of
a machine that is not a one-to-one mapping of virtual to real
hardware, they must perform indirections that map the vir-
tual view to real hardware. Since we are bringing the guest
virtual machine in more direct contact with the underlying
hardware, we avoid these indirections, and therefore, remove
the need for a hypervisor to perform them.
One such indirection is involved in the communication be-
tween cores. Hypervisors present each VM with the illusion
of running on a dedicated system. As such, the hypervisor
presents each VM with processor IDs that start at 0. Today,
the physical cores in the processor can be shared by more
than one VM and the core that a VM is running on can
be changed by the hypervisor’s scheduler. Because of this,
the hypervisor needs a map between the view presented to
the VM and the current conﬁguration in order to support
communication between the VM’s virtual cores. When ded-
icating cores to VMs, as is the case with NoHype, the guest
VM can access the real processor ID and avoid the need for
indirection.
Indirection is also used in delivering interrupts to the cor-
rect VM. For the same reason that the processor core ID
requires indirection (VMs can share cores and can move be-
tween cores), the hypervisor has to handle the interrupts
and route them to the correct VM. When dedicating cores
to VMs, we remove the need for the re-routing as interrupts
go directly to the target VM.
5. PROTOTYPE DESIGN
In this section we present the prototype of our NoHype
system. Rather than write from scratch all of the necessary
3In newer processors the local APIC runs at a ﬁxed fre-
quency regardless of the processor core’s idle states [2] so it
can be used to keep track of processor ticks.
4The network time protocol can be used to ensure the clock
stays accurate.
software to setup and boot a guest virtual machine, we in-
stead utilize existing virtualization technology which must
also provide this functionality. We base our prototype oﬀ of
Xen 4.0, Linux 2.6.35.4 as the guest OS, and an Intel XEON
W5580 processor. For the virtualized network card we used
one with the Intel 82576 Ethernet controller. We utilized
networked storage instead of a virtualized disk, since there
are no commercially available drives which support SR-IOV
at this time. In particular we used iPXE[5] for a network
boot to fetch the guest OS kernel along with iSCSI [27] for
the guest VM’s storage.
In order to understand the details of what changes were
required to the various software components, it is useful to
understand what is happening during the various phases of
execution. Shown in Figure 3 is a time line of a guest VM’s
life time – from creation to shutdown. The following subsec-
tions will detail each phase and discuss the changes we made
to Xen or Linux to support that phase. We will wrap up the
section by presenting an evaluation of the raw performance
improvements seen with our prototype.
5.1 VM Creation
Independent of the interface that is presented to the cus-
tomer for managing virtual machines, eventually, a request
from the customer will result in a request sent by the cloud
management software to the system software running on a
speciﬁc server to create a virtual machine. This request will
specify all of the conﬁguration details, such as the amount
of memory, the number of cores, and what (virtual) devices
to assign to the VM.
During this phase all of the resources are pre-allocated
and the virtualized I/O devices are assigned. Here, Xen
already provides all of the required functionality. The man-
agement software runs in Dom0, Xen’s privileged VM, and
we restricted it to execute only on core 0, as shown in Fig-
ure 3(a). The VM initialization code then conﬁgures the
hardware mechanisms which will enforce memory allocation
– in particular, the extended page tables (EPT) in the In-
tel processors. With NoHype, we require that these tables
be preset so that there is no need for a hypervisor which
manages memory translation dynamically. Xen’s VM ini-
tialization code already has such pre-setting of EPT entries
for this purpose. The VM initialization code also includes
the physical function driver for the NIC which sets up the
registers in the device not accessible to guest VMs – e.g.,
the MAC address, multicast addresses, and VLAN IDs.
For pre-allocating processor cores, Xen’s VM initialization
code has the ability to pin a VM to a set of cores. It does
this by setting the processor core aﬃnity of the VM which
causes the scheduler function to re-schedule the VM on the
selected core and add it to the list of VMs for which it can
choose between for that core. Note that while pinning sets
which cores a given VM can run on, it does not restrict the
pinning of multiple VMs to the same core. For NoHype, the
management software needs to keep track of which cores are
already assigned and only pin VMs to unused cores.
Finally, Xen’s VM initialization code allocates the virtual-
ized NIC(s) via the PCI pass through mechanism supported
in Xen. That is, it sets EPT entries to enable the device’s
memory range to be mapped to the VMs memory space. It
also utilizes the VT-d [4] extensions in the Intel architecture
to allow the device to DMA directly into the VM’s memory.
406(a) Creation.
(b)Bootup.
(c) Disengagement
(d) Execution/Shutdown.
Figure 3: The four stages of a VM’s lifetime in a NoHype system.
5.2 Guest VM bootup
Once the VM is created, its boot process is kicked oﬀ, as
seen in Figure 3(b). We piggyback on Xen’s inclusion of
a bootloader called “hvmloader” (hvm stands for hardware
virtual machine, which indicates the processor has certain
virtualization capabilities). The hvmloader is the ﬁrst soft-
ware that executes inside the guest VM. It has network boot
capabilities through the inclusion of iPXE[5] which enables
it to fetch, in our case, the guest OS kernel and initial RAM
disk5. Once the guest OS kernel is loaded and ready to boot,
the hvmloader sets the guest OS kernel parameters to pass
information to the guest OS and jumps to the kernel.
During the operating system bootup, the guest OS kernel
will perform a great deal of system discovery – in particular,
device discovery as well as discovering the processor capabil-
ities. We discuss each of these in further detail below. Recall
that during this phase, a temporary hypervisor is present to
support the system discovery.
5.2.1 Discovering Devices
In order to determine what devices are present, in par-
ticular PCI devices, the guest operating system queries a
known range of memory addresses. If the response to the
read of a particular memory address returns a well-known
constant value, that means there is no device present at that
address. Otherwise the device would return a device identi-
ﬁer. In Xen based VMs, reads to these addresses trap to the
hypervisor. Xen then passes the request to QEMU running
in Dom0 which handles it. In QEMU today, there is an as-
sumption of a minimal set of devices being present (such as
VGA). We modiﬁed QEMU to return “no device” for all but
a network card.
Upon discovering the device, the guest OS then sets up
the interrupts for that device by choosing vectors and setting
up internal tables that associate the vectors with interrupt
handler functions. When the guest OS conﬁgures the I/O
APIC with this information, the request traps to the hy-
pervisor which virtualizes the I/O APIC in software. Since
the guest’s view of available vectors does not match what is
actually available, Xen chooses a vector which is free, and
stores a mapping between the actual vector and what the
guest expects. This means that Xen would typically han-
dle the interrupt, perform a mapping, and then inject an
interrupt with the mapped vector. However, since we will
eventually be disengaging the hypervisor, we modiﬁed both
5The release version of Xen includes gPXE. iPXE is a more
actively developed branch and one for which we added a
driver for the Intel 82576 Ethernet controller.
Xen and the guest Linux kernel to make the vector chosen
by each to be conﬁgurable. Linux is made conﬁgurable so
that it chooses a vector which is actually available and so
that it does not choose the same vector as another VM.
Xen is made conﬁgurable so that the management software
can ensure that the Xen vector assignment function will also
choose this vector. Once the interrupts are set up, the guest
OS sets up the device itself through the virtual function
driver’s initialization routine. In the particular NIC we used
for our prototype, part of this initialization utilizes a mail-
box mechanism on the NIC for interacting with the physical
function driver in the host OS to perform a reset and re-
trieve the MAC address. After the virtual function driver
is initialized, interaction with the physical function driver is
not needed.
5.2.2 Discovering Processor Capabilities
In addition to needing to know which devices are available,
the guest OS needs to know details about the processor itself
– in particular, (i) the clock frequency, (ii) the core identiﬁer,
and (iii) information about the processor’s features.
The frequency that the processor runs at must be cal-
culated from a reference clock. For this, we provide a high
precision event timer (HPET) device to the guest VM. Since
this device is not virtualized in hardware, we only have a
software virtualized HPET providing the guest VM with pe-
riodic interrupts during bootup when the operating system
will need it. Once the operating system knows the clock
frequency of the core, it can use the per-core local timer as
its timer event source rather than the HPET.
The core identiﬁer is used so that when the software run-
ning on one core wants to send an interprocessor interrupt
(IPI) to another core, it knows what to set as the destina-
tion. In Xen, this identiﬁer is assigned by Xen and any IPIs
involve a mapping within the hypervisor to the actual identi-
ﬁer. In order to unwind this indirection, we modiﬁed Xen to
pass the actual identiﬁer of the core, which in Intel proces-
sors is the local advanced programmable interrupt controller
(APIC) ID. This identiﬁer can be obtained by the guest op-
erating system in three ways, each of which we modiﬁed.
First, it can be obtained in the APIC ID register within the
local APIC itself. Second, it can be obtained through the
CPUID instruction by setting the EAX register to ‘1’. Fi-
nally, it can be obtained with the Advanced Conﬁguration
and Power Interface (ACPI) table, which is written by the
bootloader (hvmloader for Xen) as a way to pass information
to the operating system.
Finally, information about the processor’s features such
407as cache size and model number, are obtained through the
CPUID instruction. This is an instruction that applications
can use in order to do some processor-speciﬁc optimiza-
tions. However, in a virtual environment the capabilities
of the processor are diﬀerent than the actual capabilities,
and therefore when the instruction is encountered, the pro-
cessor causes an exit to the hypervisor which emulates the
instruction. We modiﬁed the Linux kernel to perform this
instruction during boot-up with each of the small number of
possible input values, storing the result for each. We then
make this information available as a system call. Any appli-
cation that calls the CPUID instruction directly will have to
be modiﬁed so they do not cause a VM exit. While this may
sound like a major hurdle, in reality, it is not. We did not
encounter any such applications, but instead encountered
the use of the CPUID instruction in a small number of stan-
dard libraries such as libc which calls CPUID whenever a
process is created. We modiﬁed libc to use the system call
instead of the instruction. While these are not part of the
guest OS kernel (and therefore not provided by the cloud
provider), they can be made available for customers to eas-
ily patch their libraries and do not require a recompilation
of the application. Further, for any application which does
make use of CPUID and cannot be recompiled or modiﬁed,
simple static binary translation can be used to translate the
CPUID instruction into code which performs the system call
and puts the results in the expected registers.
5.3 Hypervisor Disengagement
At the end of the boot process, we must disengage the
hypervisor from any involvement in the execution of the
guest VM. We achieve this through a guest OS kernel mod-
ule which is loaded and unloaded within the init script of
the initial RAM disk (initrd). As shown in Figure 3(c),