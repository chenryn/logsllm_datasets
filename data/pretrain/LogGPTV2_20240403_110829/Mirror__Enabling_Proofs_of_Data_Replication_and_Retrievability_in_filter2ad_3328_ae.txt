Mirror. Our results indicate that the ﬁle replication
time grows linearly with |α∗1|. Moreover, the higher
λ∗ is, the longer it takes to replicate a given ﬁle. On
the other hand, as shown in Figure 1(b), |α∗1| consid-
1062  25th USENIX Security Symposium 
USENIX Association
12
|α∗1| Estimated EC2 costs per challenge (USD)
40
70
80
120
0.000058
0.00011
0.00013
0.00019
Table 3: Costs borne by a rational provider who
computes the responses to a challenge of size l = 40
on the ﬂy. We assume two replicas of size 64 MB,
and estimate costs for a compute-optimized (extra
large) instance from Amazon EC2 (at 0.42 USD per
hour).
erably aﬀects the time incurred on a rational provider
which did not correctly replicate (some) user ﬁles.
The larger |α∗1| is, the longer it takes a misbehaving
provider to reply to the user challenges, and thus the
bigger are the amount of computational resources
that the provider needs to invest in. Here, we as-
sume the lower bound on the eﬀort of a misbehaving
provider (i.e., which only stores a fraction δ of the
sectors per replica) given by Equation 14.
Setting |α∗1|: Following this analysis, suitable
choices for α∗1 need to be large enough such that
the costs borne by a rational provider who computes
the responses on the ﬂy are higher than those borne
by an honest provider who correctly stores the repli-
cas. In Table 3, we display the corresponding costs
borne by a rational provider who computes the re-
sponses on the ﬂy to a single challenge, assuming
l = 40, and r = 2 replicas of size 64 MB. Here, we
estimate the computation costs as follows: we in-
terpolate the time required by a rational provider
in answering challenges from Figure 1(c). We then
estimate the corresponding computation costs assum-
ing a compute-optimized (extra large) instance from
Amazon EC2 (at 0.42 USD per hour), which oﬀers
comparable computing power than that used in our
implementation setup.
For comparison purposes, notice that the cost of
storing two 64 MB replicas per day (based on Amazon
S3 pricing [9]) is approximately 0.00011 USD. This
shows that when instantiating Mirror with parameters
|α∗1| = 70, the provider should not gain any (rational)
advantage in misbehaving, if the user issues at least
one PoR2 challenge of l = 40 randomly selected blocks
per day. Clearly, users can increase the number of
challenges that they issue accordingly to ensure that
the costs borne by a rational provider are even more
pronounced, e.g., to account for possible ﬂuctuations
in costs.
Following this analysis, we assume that |α∗1| = 70
throughout the rest of the evaluation. As shown in
Figure 1(a), this parameter choice results in reason-
able replication times. e.g., when λ∗ = 5 or λ∗ = 15.
Observe that, in this case, users can detect/suspect
misbehavior by observing the cloud’s response time.
As shown in Figure 1(f), the typical response time
of an honest service provider is less than 2 seconds
when r = 2. An additional 0.9 seconds of delay (i.e.,
totalling 2.9 seconds) in responding to a challenge
can be then detected by users.
Store performance: In Figure 1(c), we evaluate the
latency incurred in Store with respect to the ﬁle size.
Our ﬁndings suggest that the Store procedure of Mir-
ror is considerably faster than that of MR-PDP. This
is the case since the tag creation in Mirror requires
fewer exponentiations per block (cf. Appendix A).
For instance, the Store procedures is almost 20%
faster than that of MR-PDP for ﬁles of 64MB in size.
Replicate performance: Figure 1(d) depicts the
latency incurred on the clients of MR-PDP in the
replicate procedure Replicate with respect to the num-
ber of replicas. Recall that, in MR-PDP, clients have
to process and upload all replicas by themselves to
the cloud. Clearly, the latency of Replicate increases
with the number of replicas stored. Given our multi-
threaded implementation, notice that the replication
process can be performed in parallel. Here, as the
number of concurrent replication requests increases,
the threads in our thread pool are exhausted and the
system saturates—which explains the sharp increase
in the latency witnessed by clients who issue more
than 8 concurrent replication requests. Notice that
users of Mirror do not bear any overhead due to repli-
cation since this process is performed by the service
provider.
In Figure 1(e), we show the latency incurred on the
service provider in Mirror with respect to the number
of replicas r. Since Mirror relies on puzzles, the repli-
cation process consumes considerable resources from
the service provider. However, we point out that
is a one-time eﬀort per ﬁle, and can be performed
oﬄine (i.e., the provider can replicate ﬁles using “of-
ﬂine” resources in the back-end). For example, the
creation of 8 additional 64 MB ﬁle replicas incurs a
latency of almost 765 seconds. As mentioned earlier,
Mirror trades this additional computational burden
with bandwidth. Namely, users of Mirror only have
to upload the ﬁle once, irrespective of the number of
replicas desired. This, in turn, reduces the download
bandwidth of providers and, as a consequence, the
costs of oﬀering the service.
In Figure 2, we estimate the costs of the additional
computations incurred in Mirror for a 64 MB ﬁle,
USENIX Association  
25th USENIX Security Symposium  1063
13
Existing Multi Replica Schemes
Mirror
10
1
0.1
)
D
S
U
n
i
(
t
s
o
C
0.01
 2
 10
 18
 26
 42
 50
 58
 34
r
Figure 2: Replication costs for a 64 MB ﬁle (in USD)
incurred Mirror vs. existing multi-replica schemes.
We assume that the provider provisions a large gen-
eral instance from Amazon EC2 at 0.441 USD per
hour). We assume the replication time given by our
experiments in Figure 1(e) and we estimate band-
width costs by adapting the ﬁndings of [3] (cf. Ta-
ble 3).
compared to those incurred by existing multi-replica
schemes which require users to upload all the repli-
cas. To estimate computing costs, we rely on the
AWS pricing model [8]; we assume that the provider
provisions a multi-core (compute-optimized) extra
large instance from Amazon EC2 (at 0.441 USD per
hour). We rely on our ﬁndings in Figure 1(e) to
estimate the computing time for replication. We
estimate bandwidth costs by adapting the ﬁndings
of [3] (i.e., by assuming $5 per Mbps per month cf.
Table 3). Our estimates suggest that Mirror consid-
erably reduces the costs borne on the provider by
trading bandwidth costs with the relatively cheaper
computing costs. We expect that the cost savings of
Mirror will be more signiﬁcant for larger ﬁles, and/or
additional replicas.
Verify performance: In Figure 1(f), we evaluate the
latency witnessed by the users and service provider
in the Verify procedure of Mirror and MR-PDP, re-
spectively. Our ﬁndings show that the veriﬁcation
overhead witnessed by the service provider in Mirror
is almost twice that of MR-PDP. Moreover, users of
Mirror require almost 1 second to verify the response
issued by the provider. Notice that the majority of
this overhead is spent while computing/verifying the
response to the issued challenge. This discrepancy
mainly originates from the fact that the challenge-
response in Mirror involves all the 32 sectors of
each block in order to ensure the extractability of
all replicas8. We contrast this to MR-PDP where
each block comprises a single sector—which only en-
sures data/replica possession but does not provide
8We point out that this is not particular to Mirror and
applies to all schemes which ensure retrievability (e.g., [34]).
]
s
[
y
c
n
e
a
L
t
 12
 10
 8
 6
 4
 2
 0
MR-PDP
Mirror
 0
 2
 4
Number of operations per second [op/s]
 6
 8
 10
 12
 14
Figure 3: Latency vs. throughput comparison be-
tween MR-PDP and Mirror.
extractability guarantees.
In Figure 3, we evaluate the peak throughput ex-
hibited by the service provider in the Verify procedure.
Here, we require that the service provider handles
veriﬁcation requests back to back; we then gradu-
ally increase the number of requests in the system
(until the throughput is saturated) and measure the
associated latency. Our results conﬁrm our previous
analysis and show that Mirror attains a maximum
throughput of 6 veriﬁcation operations per second;
on the other hand, the service provider in MR-PDP
can handle almost 12 operations per second. As
mentioned earlier, this discrepancy is mainly due to
the fact that the MR-PDP blocks only comprise a
single sector, whereas each block in Mirror comprises
32 sectors. However, we argue that the overhead
introduced by our scheme compared to MR-PDP can
be easily tolerated by clients; for instance, for 64 MB
ﬁles, our proposal only incurs an additional latency
overhead of 800 ms on the clients when compared to
MR-PDP.
6 Related Work
Curtmola et al. propose in [18] a multi-replica PDP,
which extends the basic PDP scheme in [12] and en-
ables a user to verify that a ﬁle is replicated at least
across t replicas by the cloud. In [16], Bowers et al.
propose a scheme that enables a user to verify if his
data is stored (redundantly) at multiple servers by
measuring the time taken for a server to respond to
a read request for a set of data blocks. In [13, 14],
Barsoum and Hasan propose a multi-replica dynamic
data possession scheme which allows users to verify
multiple replicas, and to selectively update/insert
their data blocks. This scheme builds upon the BLS-
based SW scheme of [34]. In [22], the authors extend
the dynamic PDP scheme of [21] to transparently sup-
port replication in distributed cloud storage systems.
All existing schemes however share a common system
model, where the user constructs and uploads the
1064  25th USENIX Security Symposium 
USENIX Association
14
replicas onto the cloud. On the other hand, Mirror
conforms with the existing cloud model by allowing
users need to process/upload their original ﬁles only
once irrespective of the replication performed by the
cloud provider.
Proofs of location (PoL) [32, 36] aim at proving
the geographic position of data, e.g., if it is stored
on servers within a certain country. In [36], Watson
et al. provide a formal deﬁnition for PoL schemes by
combining the use of geolocation techniques together
with the SW POR schemes [34]. In [36], the authors
assume a similar system model to Mirror, where the
user uploads his ﬁles to the service provider only
once. The latter then re-codes the tags of the ﬁle,
and replicates content across diﬀerent geo-located
servers. Users can then execute individual PORs
with each server to ensure that their data is stored
in its entirety at the desired geographical location.
We contrast this to our solution, where the user has
to invoke a single Mirror instance to eﬃciently verify
the integrity of all stored replicas.
Proofs of space [20] ensure that a prover can only re-
spond correctly if he invests at least a certain amount
of space or time per execution. However, this notion
is not applicable to our scenario where we need to
ensure that a minimum amount of space has been
invested by the prover. Moreover, the instantiation
in [20] does not support batch-veriﬁcation which is
essential in Mirror to conduct POR on several replicas
in a single protocol run.
7 Conclusion
In this paper, we proposed a novel solution, Mirror,
which enables users to eﬃciently verify the retriev-
ability of their data replicas in the cloud. Unlike
existing schemes, the cloud provider replicates the
data by itself in Mirror; by doing so, Mirror trades
expensive bandwidth resources with cheaper comput-
ing resources—a move which is likely to be welcomed
by providers and customers since it promises better
service while lowering costs.
Consequently, we see Mirror as one of the few
economically-viable and workable solutions that en-
able the realization of veriﬁable replicated cloud stor-
age.
8 Acknowledgements
The authors would like to thank the anonymous re-
viewers for their valuable feedback and comments.
This work was partly supported by the TREDISEC
project (G.A. no 644412), funded by the European
Union (EU) under the Information and Communica-
tion Technologies (ICT) theme of the Horizon 2020
(H2020) research and innovation programme.
References
[1] Amazon S3 Introduces Cross-Region Replication.
[2] Cloud Computing:
cerns.
magazine/hh536219.aspx.
Security Con-
http://technet.microsoft.com/en-us/
Cloud
[3] The Relative Cost of Bandwidth Around the World.
[4] Amazon S3 Service Level Agreement, 2009. http:
//aws.amazon.com/s3-sla/.
[5] Are We
Safeguarding
Social
MIT Technology Review,
2009.
//www.technologyreview.com/view/412041/
are-we-safeguarding-social-data/.
Data?,
http:
[6] Microsoft Corporation. Windows Azure Pricing and
Service Agreement, 2009.
[7] Protect data stored and shared in public cloud
http://i.dell.com/sites/doccontent/
storage.
shared-content/data-sheets/en/Documents/
Dell_Data_Protection_Cloud_Edition_Data_
Sheet.pdf, 2013.
[8] Amazon EC2 Pricing, 2015. https://aws.amazon.
com/ec2/pricing/.
[9] Amazon S3 Pricing, 2015. http://aws.amazon.com/
s3/pricing/?nc2=h_ls.
[10] Google
loses
data
after
lightning
strikes.
http://money.cnn.com/2015/08/19/technology/
google-data-loss-lightning/, 2015.
[11] Frederik Armknecht, Jens-Matthias Bohli, Ghas-
san O. Karame, Zongren Liu, and Christian A.
Reuter. Outsourced proofs of retrievability. In Pro-
ceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, CCS ’14,
pages 831–843, New York, NY, USA, 2014. ACM.
[12] Giuseppe Ateniese, Randal C. Burns, Reza Curt-
mola, Joseph Herring, Lea Kissner, Zachary N. J.
Peterson, and Dawn Xiaodong Song. Provable data
possession at untrusted stores. In ACM Conference
on Computer and Communications Security, pages
598–609, 2007.
[13] Ayad F. Barsoum and M. Anwar Hasan. Integrity
veriﬁcation of multiple data copies over untrusted
cloud servers.
In 12th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing,
CCGrid 2012, Ottawa, Canada, May 13-16, 2012,
pages 829–834, 2012.
[14] Ayad F. Barsoum and M. Anwar Hasan. Provable
multicopy dynamic data possession in cloud com-
puting systems. IEEE Transactions on Information
Forensics and Security, 10(3):485–497, 2015.
USENIX Association  
25th USENIX Security Symposium  1065
15
[15] Kevin D. Bowers, Ari Juels, and Alina Oprea. HAIL:
a high-availability and integrity layer for cloud stor-
age. In ACM Conference on Computer and Commu-
nications Security, pages 187–198, 2009.
[16] Kevin D. Bowers, Marten van Dijk, Ari Juels, Alina
Oprea, and Ronald L. Rivest. How to tell if your
cloud ﬁles are vulnerable to drive crashes. In ACM
Conference on Computer and Communications Se-
curity, pages 501–514, 2011.
[17] Jin-yi Cai, Richard J. Lipton, Robert Sedgewick,
and Andrew Chi-Chih Yao. Towards uncheatable
benchmarks.
In Proceedings of the Eigth Annual
Structure in Complexity Theory Conference, San
Diego, CA, USA, May 18-21, 1993, pages 2–11, 1993.
[18] Reza Curtmola, Osama Khan, Randal C. Burns,
and Giuseppe Ateniese. MR-PDP: Multiple-Replica
Provable Data Possession. In ICDCS, pages 411–420,
2008.
[19] Dan Dobre, Ghassan Karame, Wenting Li, Matthias
Majuntke, Neeraj Suri, and Marko Vukoli´c. Pow-