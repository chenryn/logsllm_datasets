### Mirror: File Replication and Cost Analysis

Our results indicate that the file replication time grows linearly with \(|\alpha^*1|\). Additionally, a higher \(\lambda^*\) value leads to longer replication times for a given file. As shown in Figure 1(b), \(|\alpha^*1|\) significantly affects the time incurred by a rational provider who did not correctly replicate (some) user files. The larger \(|\alpha^*1|\) is, the longer it takes for a misbehaving provider to respond to user challenges, thereby increasing the computational resources required.

#### Table 3: Estimated EC2 Costs per Challenge (USD)

| \(|\alpha^*1|\) | Estimated EC2 Costs per Challenge (USD) |
|-----------------|-----------------------------------------|
| 40              | 0.000058                                 |
| 70              | 0.00011                                  |
| 80              | 0.00013                                  |
| 120             | 0.00019                                  |

**Note:** Costs are for a challenge of size \(l = 40\) on a compute-optimized (extra large) instance from Amazon EC2 (at 0.42 USD per hour).

### Setting \(|\alpha^*1|\)

Based on our analysis, suitable choices for \(\alpha^*1\) need to be sufficiently large to ensure that the costs borne by a rational provider who computes responses on the fly are higher than those of an honest provider who correctly stores the replicas. In Table 3, we display the corresponding costs for a single challenge, assuming \(l = 40\) and \(r = 2\) replicas of 64 MB each. We estimate the computation costs by interpolating the time required by a rational provider to answer challenges from Figure 1(c) and using a compute-optimized (extra large) instance from Amazon EC2 (at 0.42 USD per hour).

For comparison, the cost of storing two 64 MB replicas per day (based on Amazon S3 pricing [9]) is approximately 0.00011 USD. This shows that when instantiating Mirror with \(|\alpha^*1| = 70\), the provider should not gain any rational advantage in misbehaving if the user issues at least one PoR2 challenge of 40 randomly selected blocks per day. Users can increase the number of challenges to ensure that the costs borne by a rational provider are even more pronounced, accounting for possible fluctuations in costs.

Throughout the rest of the evaluation, we assume \(|\alpha^*1| = 70\). As shown in Figure 1(a), this parameter choice results in reasonable replication times, e.g., when \(\lambda^* = 5\) or \(\lambda^* = 15\). Users can detect/suspect misbehavior by observing the cloud's response time. As shown in Figure 1(f), the typical response time of an honest service provider is less than 2 seconds when \(r = 2\). An additional 0.9 seconds of delay (totaling 2.9 seconds) in responding to a challenge can be detected by users.

### Store Performance

In Figure 1(c), we evaluate the latency incurred in the Store procedure with respect to the file size. Our findings suggest that the Store procedure of Mirror is considerably faster than that of MR-PDP. This is because the tag creation in Mirror requires fewer exponentiations per block (cf. Appendix A). For instance, the Store procedure is almost 20% faster than that of MR-PDP for files of 64 MB in size.

### Replicate Performance

Figure 1(d) depicts the latency incurred on the clients of MR-PDP in the Replicate procedure with respect to the number of replicas. In MR-PDP, clients have to process and upload all replicas themselves. Clearly, the latency of Replicate increases with the number of replicas stored. Given our multi-threaded implementation, the replication process can be performed in parallel. However, as the number of concurrent replication requests increases, the threads in our thread pool are exhausted, and the system saturates—explaining the sharp increase in latency for clients who issue more than 8 concurrent replication requests. Notice that users of Mirror do not bear any overhead due to replication since this process is performed by the service provider.

In Figure 1(e), we show the latency incurred on the service provider in Mirror with respect to the number of replicas \(r\). Since Mirror relies on puzzles, the replication process consumes considerable resources from the service provider. However, this is a one-time effort per file and can be performed offline. For example, creating 8 additional 64 MB file replicas incurs a latency of almost 765 seconds. As mentioned earlier, Mirror trades this additional computational burden for bandwidth. Users of Mirror only have to upload the file once, irrespective of the number of replicas desired, reducing the download bandwidth of providers and, consequently, the costs of offering the service.

### Replication Costs

In Figure 2, we estimate the costs of the additional computations incurred in Mirror for a 64 MB file compared to existing multi-replica schemes. To estimate computing costs, we rely on the AWS pricing model [8]; we assume that the provider provisions a multi-core (compute-optimized) extra-large instance from Amazon EC2 (at 0.441 USD per hour). We estimate bandwidth costs by adapting the findings of [3] (i.e., by assuming $5 per Mbps per month, cf. Table 3). Our estimates suggest that Mirror considerably reduces the costs borne by the provider by trading bandwidth costs with the relatively cheaper computing costs. We expect that the cost savings of Mirror will be more significant for larger files and/or additional replicas.

### Verify Performance

In Figure 1(f), we evaluate the latency witnessed by the users and service provider in the Verify procedure of Mirror and MR-PDP, respectively. Our findings show that the verification overhead witnessed by the service provider in Mirror is almost twice that of MR-PDP. Moreover, users of Mirror require almost 1 second to verify the response issued by the provider. The majority of this overhead is spent while computing/verifying the response to the issued challenge. This discrepancy mainly originates from the fact that the challenge-response in Mirror involves all 32 sectors of each block to ensure the extractability of all replicas. In contrast, MR-PDP blocks comprise a single sector, which only ensures data/replica possession but does not provide extractability guarantees.

In Figure 3, we evaluate the peak throughput exhibited by the service provider in the Verify procedure. We require that the service provider handles verification requests back to back; we then gradually increase the number of requests in the system (until the throughput is saturated) and measure the associated latency. Our results confirm our previous analysis and show that Mirror attains a maximum throughput of 6 verification operations per second, while the service provider in MR-PDP can handle almost 12 operations per second. However, the overhead introduced by our scheme compared to MR-PDP can be easily tolerated by clients; for instance, for 64 MB files, our proposal only incurs an additional latency overhead of 800 ms on the clients when compared to MR-PDP.

### Related Work

Curtmola et al. propose in [18] a multi-replica PDP, which extends the basic PDP scheme in [12] and enables a user to verify that a file is replicated at least across \(t\) replicas by the cloud. Bowers et al. [16] propose a scheme that enables a user to verify if their data is stored redundantly at multiple servers by measuring the time taken for a server to respond to a read request for a set of data blocks. Barsoum and Hasan [13, 14] propose a multi-replica dynamic data possession scheme that allows users to verify multiple replicas and selectively update/insert their data blocks. This scheme builds upon the BLS-based SW scheme of [34]. In [22], the authors extend the dynamic PDP scheme of [21] to transparently support replication in distributed cloud storage systems. All existing schemes share a common system model where the user constructs and uploads the replicas onto the cloud. On the other hand, Mirror conforms with the existing cloud model by allowing users to process/upload their original files only once, irrespective of the replication performed by the cloud provider.

Proofs of location (PoL) [32, 36] aim to prove the geographic position of data, e.g., if it is stored on servers within a certain country. Watson et al. [36] provide a formal definition for PoL schemes by combining geolocation techniques with the SW POR schemes [34]. In [36], the authors assume a similar system model to Mirror, where the user uploads their files to the service provider only once. The provider then re-codes the tags of the file and replicates content across different geo-located servers. Users can then execute individual PORs with each server to ensure that their data is stored in its entirety at the desired geographical location. In contrast, our solution allows the user to invoke a single Mirror instance to efficiently verify the integrity of all stored replicas.

Proofs of space [20] ensure that a prover can only respond correctly if they invest at least a certain amount of space or time per execution. However, this notion is not applicable to our scenario, where we need to ensure that a minimum amount of space has been invested by the prover. Moreover, the instantiation in [20] does not support batch-verification, which is essential in Mirror to conduct POR on several replicas in a single protocol run.

### Conclusion

In this paper, we proposed Mirror, a novel solution that enables users to efficiently verify the retrievability of their data replicas in the cloud. Unlike existing schemes, the cloud provider replicates the data itself in Mirror, trading expensive bandwidth resources with cheaper computing resources. This move is likely to be welcomed by providers and customers as it promises better service while lowering costs. Consequently, we see Mirror as one of the few economically-viable and workable solutions that enable the realization of verifiable replicated cloud storage.

### Acknowledgements

The authors would like to thank the anonymous reviewers for their valuable feedback and comments. This work was partly supported by the TREDISEC project (G.A. no 644412), funded by the European Union (EU) under the Information and Communication Technologies (ICT) theme of the Horizon 2020 (H2020) research and innovation programme.

### References

[1] Amazon S3 Introduces Cross-Region Replication.
[2] Cloud Computing: Security Concerns. http://technet.microsoft.com/en-us/magazine/hh536219.aspx.
[3] The Relative Cost of Bandwidth Around the World.
[4] Amazon S3 Service Level Agreement, 2009. http://aws.amazon.com/s3-sla/.
[5] Are We Safeguarding Social Data? MIT Technology Review, 2009. http://www.technologyreview.com/view/412041/are-we-safeguarding-social-data/.
[6] Microsoft Corporation. Windows Azure Pricing and Service Agreement, 2009.
[7] Protect data stored and shared in public cloud. http://i.dell.com/sites/doccontent/storage/shared-content/data-sheets/en/Documents/Dell_Data_Protection_Cloud_Edition_Data_Sheet.pdf, 2013.
[8] Amazon EC2 Pricing, 2015. https://aws.amazon.com/ec2/pricing/.
[9] Amazon S3 Pricing, 2015. http://aws.amazon.com/s3/pricing/?nc2=h_ls.
[10] Google loses data after lightning strikes. http://money.cnn.com/2015/08/19/technology/google-data-loss-lightning/, 2015.
[11] Frederik Armknecht, Jens-Matthias Bohli, Ghassan O. Karame, Zongren Liu, and Christian A. Reuter. Outsourced proofs of retrievability. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security, CCS '14, pages 831–843, New York, NY, USA, 2014. ACM.
[12] Giuseppe Ateniese, Randal C. Burns, Reza Curtmola, Joseph Herring, Lea Kissner, Zachary N. J. Peterson, and Dawn Xiaodong Song. Provable data possession at untrusted stores. In ACM Conference on Computer and Communications Security, pages 598–609, 2007.
[13] Ayad F. Barsoum and M. Anwar Hasan. Integrity verification of multiple data copies over untrusted cloud servers. In 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGrid 2012, Ottawa, Canada, May 13-16, 2012, pages 829–834, 2012.
[14] Ayad F. Barsoum and M. Anwar Hasan. Provable multicopy dynamic data possession in cloud computing systems. IEEE Transactions on Information Forensics and Security, 10(3):485–497, 2015.
[15] Kevin D. Bowers, Ari Juels, and Alina Oprea. HAIL: a high-availability and integrity layer for cloud storage. In ACM Conference on Computer and Communications Security, pages 187–198, 2009.
[16] Kevin D. Bowers, Marten van Dijk, Ari Juels, Alina Oprea, and Ronald L. Rivest. How to tell if your cloud files are vulnerable to drive crashes. In ACM Conference on Computer and Communications Security, pages 501–514, 2011.
[17] Jin-yi Cai, Richard J. Lipton, Robert Sedgewick, and Andrew Chi-Chih Yao. Towards uncheatable benchmarks. In Proceedings of the Eighth Annual Structure in Complexity Theory Conference, San Diego, CA, USA, May 18-21, 1993, pages 2–11, 1993.
[18] Reza Curtmola, Osama Khan, Randal C. Burns, and Giuseppe Ateniese. MR-PDP: Multiple-Replica Provable Data Possession. In ICDCS, pages 411–420, 2008.
[19] Dan Dobre, Ghassan Karame, Wenting Li, Matthias Majuntke, Neeraj Suri, and Marko Vukolić. Pow-