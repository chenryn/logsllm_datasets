operation. EAR aims for the following design goals:
• Eliminating cross-rack downloads: The node that is
selected to perform encoding does not need to down-
load data blocks from other racks during the encoding
operation. Note that the node may have to upload parity
blocks to other racks in order to achieve rack-level fault
tolerance.
150150
• Preserving availability: Both node-level and rack-level
fault tolerance requirements are fulﬁlled after the en-
coding operation, without the need of relocating any
data or parity block.
• Maintaining load balancing: EAR tries to randomly
place replicas as in RR for simplicity and load balanc-
ing [7], subject to the imposed constraints.
A. Eliminating Cross-Rack Downloads
Preliminary design: The reason why cross-rack down-
loads occur is that it is unlikely for a rack to contain at
least one replica of each of the k data blocks of a stripe.
Thus, we present a preliminary design of EAR, which jointly
determines the replica locations of k data blocks of the same
stripe. For each stripe, we ensure that each of the k data
blocks of the stripe must have at least one replica placed on
one of the nodes within a rack, which we call the core rack.
The preliminary EAR works as follows. When a CFS
stores each data block with replication, we ensure that the
ﬁrst replica is placed in the core rack, while the remaining
replicas are randomly placed in other racks to provide rack-
level fault tolerance as in RR. Once the core rack has stored
k distinct data blocks, the collection of k data blocks is
sealed and becomes eligible for later encoding. When the
encoding operation starts, we ﬁrst randomly select a node
in the core rack to perform encoding for the stripe. Then
the node can download all k data blocks within its rack, and
there is no cross-rack download. For example, in Figure 2(b),
Rack 3 is the core rack of the stripe, and Blocks 1 to 4 are
all within Rack 3 and can be used for encoding.
In practice, the CFS may issue writes from different nodes
and racks. We do not need to select a dedicated rack for all
stripes as the core rack. Instead, each rack in the CFS can be
viewed as a core rack for a stripe. For each data block to be
written, the rack that stores the ﬁrst replica will become the
core rack that includes the data block for encoding. When
a rack accumulates k data blocks, the k data blocks can be
sealed for encoding. Thus, there are multiple core racks, each
handling the encoding of a stripe, at a time. On the other
hand, since stripes are encoded independently, our analysis
focuses on a single stripe, and hence a single core rack.
Availability violation: Our preliminary EAR only ensures
that one replica (i.e., the ﬁrst replica) of each data block
resides in the core rack, but does not consider where the
remaining blocks are placed after the encoding operation
removes the redundant replicas. Thus, block relocation may
be needed after the encoding operation so as to maintain
rack-level fault tolerance. We elaborate the problem via a
simple example. Consider the case where we place three data
blocks via 3-way replication and then encode them using
(4, 3) erasure coding. We also require to tolerate any n−k =
1 rack failure. Suppose that after we place the ﬁrst replica of
each data block in the core rack using the preliminary EAR,
we place the remaining two replicas in a randomly chosen
k=6
k=8
k=10
k=12
y
t
i
l
b
a
b
o
r
P
1.00
0.75
0.50
0.25
0.00
16
20
24
28
32
36
40
Number of racks
Figure 3. Probability that a stripe violates rack-level fault tolerance.
rack different from the core rack, as in HDFS [28]. Then
the random rack selection for each data block may happen
to choose the same rack, meaning that the three replicas of
each data block are always placed in the same two racks (i.e.,
the core rack and the chosen rack). In this case, regardless
of how we delete redundant replicas, we must have a rack
containing at least two data blocks. If the rack fails, then
the data blocks become unavailable, thereby violating the
single-rack fault tolerance. In this case, block relocation is
necessary after encoding.
As opposed to RR, the preliminary EAR has a smaller
degree of freedom in placing replicas across racks. We argue
via analysis that the preliminary EAR actually has a very
high likelihood of violating rack-level fault tolerance and
hence triggering block relocation. Suppose that we store data
with 3-way replication over a CFS with R racks and later
encode the data with (n, k) erasure coding (where R ≥ n),
such that the ﬁrst replicas of k data blocks are placed in the
core rack and the second and third replicas are placed in
one of the R − 1 non-core racks. Thus, there are a total of
(R − 1)k ways to place the replicas of the k data blocks. We
also require to tolerate n − k rack failures after the encoding
operation as in Facebook [21].
Suppose that the second and third replicas of the k data
blocks span k−1 or k out of R−1 non-core racks (the former
case implies that the replicas of two of the data blocks reside
in the same rack). Then we can ensure that the k data blocks
span at least k racks (including the core rack). After we put
n−k parity blocks in n−k other racks, we can tolerate n−k
rack failures. Otherwise, if the second and third replicas of
the k data blocks span fewer than k − 1 racks, then after
encoding, we cannot tolerate n − k rack failures without
block relocation. Thus, the probability that a stripe violates
rack-level fault tolerance (denoted by f) is:
(cid:3)
× k! +
(cid:3)
(cid:2)
(cid:3)(cid:2)
k
2
R−1
k−1
(R − 1)k
(cid:2)
R−1
k
f = 1 −
× (k − 1)!
.
(1)
Figure 3 plots f for different values of k and R using
151151
Block 1
Block 2
Block 3
Rack 1
Rack 2
Rack 3
Rack 4
1
S
    Max 
matching
(a) Bipartite graph
1
1
c
c
c
c
T
Block
Node
Rack
(b) Flow graph
Figure 4. Examples of a bipartite graph and a ﬂow graph.
1
S
4
S
Equation (1). Note that k = 10 and k = 12 are chosen by
Facebook [27] and Azure [17], respectively. We see that the
preliminary EAR is highly likely to violate rack-level fault
tolerance (and hence requires block relocation), especially
when the number of racks is small (e.g., 0.97 for k = 12
and R = 16).
B. Preserving Availability
We now extend the preliminary EAR to preserve both
node-level and rack-level fault tolerance after the encoding
operation without the need of block relocation. Speciﬁcally,
we conﬁgure (n, k) erasure coding to tolerate n − k node
failures by placing n data and parity blocks of a stripe on
different nodes. Also, we conﬁgure a parameter c for the
maximum number of blocks of a stripe located in a rack.
Note that this implies that we require R ≥ n
c , so that a
stripe of n blocks can be placed in all R racks. Since a
stripe can tolerate a loss of n−k blocks, the CFS can tolerate
(cid:4) n−k
c (cid:5) rack failures. Our (complete) EAR is designed based
on (n, k) and c.
We illustrate the design via an example. We consider a
CFS with eight nodes evenly grouped into four racks (i.e.,
two nodes per rack). We write three data blocks using 3-
way replication, and later encode them with (4, 3) erasure
coding to tolerate a single node failure. We set c = 1, so as
to tolerate (cid:4) 4−3
1 (cid:5) = 1 rack failure.
We ﬁrst map the replica locations of data blocks to a
bipartite graph as shown in Figure 4(a), with the vertices
on the left and on the right representing blocks and nodes,
respectively. We partition node vertices into the racks to
which the nodes belong. An edge connecting a block vertex
and a node vertex means that the corresponding block has a
replica placed on the corresponding node. Since each replica
is represented by an edge in the bipartite graph, the replicas
of data blocks that are kept (i.e., not deleted) after encoding
will form a set of edges. If the set is a maximum matching
of the bipartite graph (i.e., every replica is mapped to exactly
one node vertex) and no more than c edges from the set is
linked to vertices in one rack (i.e., each rack has at most
c data blocks), then we fulﬁll the rack-level fault tolerance
requirement. Later, we deliberately place the parity blocks
on the nodes that maintain rack-level fault tolerance by
2
core
rack
T
S
3
T
S
maxFlow=0
maxFlow=1
maxFlow=2
5
T
S
6
T
S
     maxFlow=2
regenerate layout
maxFlow=3
max matching
Figure 5.
Illustration of EAR.
T
T
assigning parity blocks to the nodes of other racks that have
fewer than c blocks of the stripe.
To determine if a maximum matching exists, we convert
the problem to a maximum ﬂow problem. We augment the
bipartite graph to a ﬂow graph as shown in Figure 4(b), in
which we add source S, sink T , and the vertices representing
racks. S connects to every block vertex with an edge of
capacity one, meaning that each block keeps one replica
after encoding. Edges in the bipartite graph are mapped to
the edges with capacity one each. Each node vertex connects
to its rack vertex with an edge of capacity one, and each rack
vertex connects to T with an edge of capacity c (c = 1 in
this example), ensuring that each node stores at most one
block and each rack has at most c blocks after the encoding
operation. If and only if the maximum ﬂow of the ﬂow graph
is k, we can ﬁnd a maximum matching and further determine
the replica placement.
C. Algorithm
Combining the designs in Sections III-A and III-B, we
propose an algorithm for EAR, which systematically places
replicas of data blocks. Its key idea is to randomly place the
replicas as in RR, while satisfying the constraints imposed
by the ﬂow graph. Speciﬁcally, for the i-th data block (where
1 ≤ i ≤ k), we place the ﬁrst replica on one of the nodes
in the core rack, and then randomly place the remaining
replicas on other nodes based on the speciﬁed placement
policy. For example, when using 3-way replication, we
follow HDFS [28] and place the second and third replicas on
two different nodes residing in a randomly chosen rack aside
the core rack (which holds the ﬁrst replica). In addition, we
ensure that the maximum ﬂow of the ﬂow graph is i after
we place all replicas of the i-th data block.
Figure 5 shows how we place the replicas of each data
block for a given stripe. We ﬁrst construct the ﬂow graph
with the core rack (Step 1). We add the ﬁrst and second data
blocks and add the corresponding edges in the ﬂow graph
(Steps 2 and 3, respectively). If the maximum ﬂow is smaller
152152
than i (Step 4), we repeatedly generate another layout for
the replicas of the i-th data block until the maximum ﬂow is
i (Step 5). Finally, the maximum matching can be obtained
through the maximum ﬂow (Step 6).
The following theorem quantiﬁes that the expected num-
ber of iterations for generating a qualiﬁed replica layout in
Step 5 is generally very small.
Theorem 1. Consider a CFS with R racks, each containing
a sufﬁciently large number of nodes. Under 3-way repli-
cation, the expected number of iterations (denoted by Ei)
that EAR ﬁnds a qualiﬁed replica layout for the i-th data
block, such that the maximum ﬂow becomes i, is at most
R−1−(cid:2)(i−1)/c(cid:3) , where 1 ≤ i ≤ k.
R−1
Proof (Sketch): Suppose that we have found a qualiﬁed
replica layout for the (i − 1)-th data block, which makes the
maximum ﬂow become i−1. Before ﬁnding a replica layout
for the i-th data block (where 1 ≤ i ≤ k), the number of
racks (excluding the core rack) that have stored c blocks (call
them full racks) is at most (cid:4)(i − 1)/c(cid:5). If (i) we place the
second and third replicas in the remaining R−1−(cid:4)(i−1)/c(cid:5)
non-full racks and (ii) the nodes that will store the replicas of
the i-th data block have not stored any replica of the previous
i − 1 data blocks, then the maximum ﬂow will increase to
i. Condition (i) holds with probability 1 − (cid:2)(i−1)/c(cid:3)
, and
condition (ii) holds with probability almost one if each rack
has a sufﬁciently large number of nodes. Thus, Ei is at most
[1 − (cid:2)(i−1)/c(cid:3)
R−1
Remarks: Ei increases with i and reaches the maximum
at i = k, but it is generally very small. For example, suppose
that a CFS has R = 20 racks and we set c = 1. Then for
the k-th data block, Ei is at most 1.9 for k = 10 (used by
Facebook [27]) and 2.375 for k = 12 (used by Azure [17]).
R−1
]−1.
D. Discussion
To tolerate any n − k rack failures [21], we must place
n data and parity blocks of the same stripe in n different
racks. One performance issue is that in order to recover a
failed block, a node needs to download k blocks. Although
one block can be downloaded within the same rack, the
other k − 1 blocks need to be downloaded from other racks.
This introduces high cross-rack recovery trafﬁc, which is
undesirable [24].
We can reduce the number of racks where a stripe resides
in return for lower cross-rack recovery trafﬁc, by setting
c > 1 to relax the requirement of tolerating n − k rack
failures. Speciﬁcally, we randomly pick R(cid:4) out of R racks
(where R(cid:4) < R) as target racks, such that all data and parity
blocks of every stripe must be placed in the target racks.
To construct a ﬂow graph for EAR, we keep only the edges
from the target racks to the sink, but remove any edges from
other non-target racks to the sink. We run the EAR algorithm
(see Section III-C) and ﬁnd the maximum matching. Note
that we require R(cid:4) ≥ n
c (see Section III-B).
153153
S
core
rack
c=3
T
target
racks
Block