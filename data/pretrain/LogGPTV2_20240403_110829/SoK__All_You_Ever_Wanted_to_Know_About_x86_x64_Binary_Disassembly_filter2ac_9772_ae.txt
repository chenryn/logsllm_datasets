mercial tools (88.63% on average). The difference is even bigger
with the worst case. This is because open-source tools brute
force all constant operands and data units while the commer-
cial tools adopt more conservative strategies. Our hypothesis
can be veriﬁed by comparing MCSEMA and IDA PRO, since
MCSEMA just adds a round of brute force on top of IDA PRO.
Somewhat surprisingly, open-source tools also have high
precision (99.92% on average). Based on our observation, the
high precision is because (1) the heuristic-based checks are
generally restrictive and (2) most benchmark programs have
less data. On programs that have more data, the tools are more
inclined to make mistakes (e.g., Mysqld, having plenty of
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
841
TABLE VII: Evaluation results of symbolization. UROBOROS
cannot run Linux-Of binaries and all Window binaries. We
also omitted ANGR’s symbolization on Windows binaries as
ANGR’s Reassembler component cannot run them.
TABLE VIII: Statistics of false positives in symbolization.
Align, Type, Type-based Sliding, and Extended-data respec-
tively mean no alignment (heuristic 11), ANGR’s moving
scheme (heuristic 16), enlarging data regions (heuristic 13).
Symbolizations
Avg
Min
L
a
r
d
i
h
G
o O0
b
O2
o
r
O3
U
Os
*
Of
a O0
O2
m
e
O3
S
c
Os
M
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
a
j
n
N
r
g
n
A
A
D
I
i
Pre
Rec
99.99 100
100
100
99.99 100
99.99 100
-
-
99.99 99.77
99.99 99.96
99.99 99.96
99.98 99.75
99.96 99.68
99.90 95.70
99.91 95.53
99.92 95.99
99.89 95.37
99.97 96.26
99.86 99.52
99.88 99.54
99.90 99.59
99.81 99.99
99.84 99.99
99.98 95.96
99.93 94.32
99.94 95.06
99.93 93.81
99.98 95.63
99.99 79.89
99.99 81.08
99.99 81.38
99.99 77.71
99.99 82.35
Symbolizations
Avg
Min
Pre
-
-
-
-
-
Rec
-
-
-
-
-
Pre
-
-
-
-
-
Rec
-
-
-
-
-
W
-
-
-
-
-
Pre
Rec
99.89 100
100
100
99.91 100
99.88 100
-
-
99.04 94.05 Od
98.78 97.87 O1
98.88 98.79 O2
98.68 90.91 Ox
-
93.10 81.82
61.99 45.52 Od
87.35 37.71 O1
88.36 41.60 O2
61.98 41.72 Ox
-
98.19 20.00
-
81.74 95.38
-
87.28 95.35
-
88.29 96.21
-
86.57 98.96
-
88.05 98.89
97.17 36.96 Od
86.77 31.96 O1
88.38 31.96 O2
85.59 25.26 Ox
-
97.52 32.02
99.81 17.29 Od
97.59 19.01 O1
99.40 21.15 O2
98.67 17.45 Ox
-
99.52 21.15
-
99.93 96.05
99.96 96.08
99.97 95.38
99.95 95.51
-
99.70 96.53
99.63 95.02
99.86 95.25
99.83 95.30
-
-
-
-
-
-
-
-
-
-
-
-
-
94.79 78.22
98.00 84.80
98.70 86.92
96.54 83.42
-
94.75 78.12
95.59 77.86
98.67 74.54
96.45 74.52
-
-
-
-
-
-
-
-
-
-
-
-
-
99.99 95.76
99.99 94.11
99.99 94.26
99.99 94.39
-
99.90 85.37
99.98 84.67
99.98 84.54
99.97 84.98
-
-
-
99.90 82.51
99.88 72.65
99.88 69.94
99.89 70.48
-
93.62 40.94
99.61 45.18
99.63 50.17
99.44 50.14
-
-
data, leads to the lowest precision in many tools).
Use of Heuristics: As shown in Table III, symbolization in-
volves many heuristics, competing for a coverage-correctness
trade-off. Our evaluation shows that heuristic 8 (the brute-force
based approach), 11 (the no-alignment assumption about pointers), and
13 (the enlargement of data regions) bring full coverage of xrefs in
our benchmarks. The other heuristics are instead striving for
correctness. In the following, we discuss them in turn.
Heuristic 8
(the brute-force based approach) is necessary for
symbolization. This can be veriﬁed by comparing IDA PRO
and MCSEMA. MCSEMA adds a round of brute force to IDA
PRO, increasing the coverage from 95% to 98%. Heuristic
9 (pointers in data have machine size) is principled if jump tables
are not considered. From over 6 million xrefs, we observe no
violations. Heuristic 10 (pointers in data are aligned) and 11 (pointers
in data may not be aligned) conﬂict with each other, and neither of
them is perfect. Heuristic 10 misses around 600 xrefs while
heuristic 11 introduces most false positives in ANGR (over
50K). Heuristic 12 (references to code point to function entries) can
reduce false positives but it misses thousands of xrefs to the
middle of functions (e.g., pointers for try-catch in exception
handling). Heuristic 13 (enlarging boundaries of data regions) helps
recover 12K xrefs in data, but
leads to 6 and 2K+ false
Tools
Urobo*
Mcsema
Ghidra
Angr
Align
0.00
0.00
0.29
58.58
Percentage of False Positives (%)
Type-based Sliding
Extended-data
Collision
0.00
0.00
0.00
4.47
0.00
0.00
3.91
0.01
100
100
95.80
36.94
TABLE IX: Statistics of false negatives in symbolization.
Align, Type, Extended-data, Function, Address-table respec-
tively mean assumption of alignment (heuristic 10), preferring
strings over pointers (heuristic 15), enlarging data regions
(heuristic 13), and size of address table (heuristic 14).
Percentage of False Negative (%)
Address Table
Extended-data
Tools
Urobo*
Mcsema
Ghidra
Angr
Align
0.00
1.14
0.01
0.00
Type
0.00
75.22
3.13
100
0.00
23.64
0.00
0.00
0.00
0.00
96.53
0.00
Func
0.00
0.00
0.33
0.00
positives in ANGR and GHIDRA. Heuristic 14 introduces most
false negatives in GHIDRA, although removing a small set of
false positives. Heuristic 15 produces over 3K false negatives
in MCSEMA because the inference of strings is not accurate.
Understanding of Errors: Table VIII shows the statistics of
false positives. Using the assumption that pointers in data can
be non-aligned, ANGR and GHIDRA trigger 59% and 0.3% of
their false positives. They also produce false positives because
they enlarge the boundaries of data regions when checking the
legitimacy of xrefs’ targets. All the other false positives are
due to collisions between numeric values and pointers.
As shown in Table IX, most false negatives by ANGR and
MCSEMA are because they exclude pointers that overlap with
the inferred strings. MCSEMA also misses 23.64% of the xrefs
that point to locations outside of the data regions. GHIDRA,
because of its assumption about the address table’s minimal
size and that code pointers always point to function entries,
respectively produces 96.53% and 0.33% of its false negatives.
3) Function Entry Identiﬁcation : This evaluation measures
the identiﬁcation of function entries. In this test, we further
considered NUCLEUS [6] and BYTEWEIGHT [9]. We re-
trained BYTEWEIGHT with our benchmarks binaries.
Overall Performance: Table X presents the overall results.
The key observation is that function entry identiﬁcation re-
mains a challenge. 4 of the open source tools can only identify
less than 80% of the functions. In particular, RADARE2 has a
recall lower than 66%. Such results indicate, even with heuris-
tics, we have yet to develop better function identiﬁcation. We
also observe that the results of function identiﬁcation varies
across optimization levels and architectures. This is because
the tools widely use signature-based function matching, which
are speciﬁc to optimizations and architectures. Besides limited
coverage, existing tools also have lower precision in function
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
842
TABLE X: Evaluation results of function entry identiﬁcation.
Function Entry
Avg
Min
L
Function Entry
Avg
Min
W
p
a
B
r
g
n
A
a
r
d
i
h
G
t
s
n
i
n
y
D
O0
O2
O3
Os