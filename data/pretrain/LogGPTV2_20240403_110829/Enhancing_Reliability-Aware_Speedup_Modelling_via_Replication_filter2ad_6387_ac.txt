parallel workloads is larger, increasing the value of λP , which
means the ﬁrst-order approximation is not valid in that case.
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
532
Fig. 3. Optimal number of processors when no replication is employed. C = R = D = 300 seconds.
Fig. 4. Optimal number of processors with dual replication. Checkpointing
cost = 300 seconds, same as in Figure 3.
Fig. 6. Optimal number of processors with dual replication for a perfectly
parallel workload. Checkpointing cost = 300 seconds.
will serve as a good ﬁt for a much larger set of parallel
workloads, even if they are highly scalable, as long as they
are not embarrassingly parallel.
Perfectly Parallel workload (α = 0): Although we can-
not make the simplifying ﬁrst-order approximation as above,
setting α = 0 simpliﬁes Equation 8, yielding Hrep(P ) ≈
2/P (1 −
(cid:6)
(cid:5)
2λC
(cid:5)
(cid:5)
2P
π ). Taking the derivative, we get
−2(1 − 5
P 2(1 −
(cid:4)
(cid:4)
2P/π/4)
2P/π)2
2λC
2λC
∂Hrep
∂P =
(12)
Setting the derivative equal to 0 yields the optimal value as
rep ≈ 32π
P ∗
625λ2C 2
(13)
While this procedure yielded an exact expression for the
optimal number of processors, it should be noted that this value
is optimal for the approximate expression used in Equation 8.
To assess the accuracy of this approximation, Figure 6, similar
to Figure 4, plots the optimal processor counts, this time for
a perfectly parallel workload, along with our derived formula.
Comparing with Figure 4, we ﬁrst note that the counts are
signiﬁcantly lower with a non-zero value of α, which is to
be expected. We also see that, while the parameters R and D
did not have much impact on the optimal processor count for
non-zero α, their values have a non-negligible impact on the
optimal processor counts for perfectly parallel workloads. This
trend is similar to the case of no replication, where R and D
do not have much impact when α > 0 since they vanish from
the ﬁrst-order approximation [1], but have a greater effect on
Fig. 5. Optimal number of processors with dual replication, as obtained by
the simulation as well as the ﬁrst order approximation (Equation 11). X-axis
range is from α = 0 to α = 10−10. Individual processor MTBF = 10 years
while C = R = D = 300 seconds.
Figure 4 plots the ﬁrst order approximation derived above
along with optimal processor counts obtained using simulation
−6. We see that the values of R and D
results, when α = 10
do not have a signiﬁcant impact on the actual value of optimal
processor counts, and also that the ﬁrst order approximation
is quite accurate in determining those counts.
Similar to the case of no replication, we also analyse the
values of α for which the ﬁrst order approximation is a closer
match to the actual optimal counts for replication. Figure 5
−10, the ﬁrst
shows that, for values of α greater or equal to 10
order approximation is closer to the actual counts than the
perfectly parallel approximation. Recall from Figure 2 that
this threshold for α without replication, assuming a system
with the same parameters, was much higher (around 5∗10
−6).
This means that with replication, the ﬁrst order approximation
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
533
TABLE II
OPTIMAL PROCESSORS COUNTS
α >0
α = 0
C/R without Replication
P ∗ ≈ ( 1−α
Θ(λ−1/3)
)2/3( 2
λC
Θ(λ−1)
α
)1/3 [1]
C/R with Replication
Θ(λ−2/5)
P ∗ ≈ ( 8(1−α)
√
( π
2
Θ(λ−2)
P ∗ ≈ 32π
1
4 )
2λC
α
)
4
5
625λ2C2
the optimal counts for perfectly parallel workloads. We thus
conclude from Figure 6 that the actual optimal number of
processors for perfectly parallel workloads also depends on the
recovery cost R and the downtime D, which the model used
to write Equation 8 does not take into account. Nevertheless,
the derivation based on this model yields a simple and handy
expression which, as seen in the plot, is close to the optimal
processor counts in practice.
An additional beneﬁt of the approximation we just derived
is that it leads us to the observation that the optimal number
of processors using dual replication is of the order λ−2. Thus,
similar to Figure 3, where we investigate the dependence of
the optimal counts on λ for no replication, we plot, in Figure 7,
best-ﬁt curves over simulation based optimal processor counts
of replication using the forms indicated by our derivations (i.e.
P ∗
rep = K/λ2 when α = 0 and P ∗
rep = K/λ2/5 when α > 0).
We see that the best-ﬁt curve for α = 0 matches well with
the simulation results. For α > 0, the ﬁtted curve gets closer
in shape to the actual counts as α gets larger, similar to the
observations made in Figure 3 for no replication. This means
that the optimal processor counts in both cases agree with
our derived formulae on their order in terms of λ. Thus, our
formulae serve as reasonable approximations to the optimal
processor counts for dual replication.
Based on the results so far, we summarize our ﬁndings
about the optimal processor counts in this section in Table
II, which lists the form of the optimal counts in terms of λ
as well as closed form approximations where available. We
see that, in both cases of parallelism, the optimal counts of
replication are of a higher order in terms of the processor
MTBF (= 1/λ) when compared with their counterparts for
no replication. Thus, in each case we can say that the range
of system scales over which one can continue to improve
performance by enrolling more processors is much larger with
replication than without it.
IV. PERFORMANCE COMPARISON OF REPLICATION WITH
NO REPLICATION
In the previous section we saw that the optimal processor
counts for replication are much higher than those possible
without replication. However, that does not necessarily mean
that the reliability-aware speedup of replication will actually
be better than what is possible without replication at higher
processor counts. This is because the optimal processor counts
were optimal for their speciﬁc fault tolerance mechanisms, i.e.
replication and no-replication respectively, and so none of our
earlier results say how the speedup of replication compares
with no replication.
When comparing the performance of no-replication with
replication with respect to the scale of the system, we know
that replication always starts off worse than no-replication at
lower processor counts, since the system failure rate is low,
in which case redundancy is an overkill. We also know from
[4] and [12] that at some point replication starts outperforming
no-replication and this trend subsequently holds for increasing
system sizes. Depending on when that crossover happens,
though, we get the different possibilities depicted in Figure 8.
Note that the ﬁgure plots the normalized expected completion
time which is minimum at the optimal number of processors,
since it is the inverse of reliability-aware speedup.
If we deﬁne the global speedup at any processor count as the
best speedup possible at that scale (i.e. the better of replication
or no replication at that processor count), then it is interesting
to explore the form and behavior of this global speedup
function in terms of the performance of replication and no-
replication. Figure 8 shows normalized expected completion
time of such a global speedup in each case as a dashed
green curve. Finding out which of those three scenarios is
true in practice would have implications on the feasibility
of replication. For example, if either of subﬁgure (a) or (b)
represents the true form of the global speedup function, then
the optimal global speedup will be achieved at the optimal
of replication. However, only in scenario (a) does the global
speedup continuously improve until that optimal is reached. If,
on the other hand, scenario (b) is how the speedup behaves, it
would be surprising since that would mean that the speedup
improves until some system scale (i.e. the optimal of no-
replication), then is worse for larger system scales until some
point, but after that point it starts improving once again before
hitting the optimal of replication. Finally, subﬁgure (c), if true,
would mean that the outlook for replication is bleak since it
would never reach the optimal performance of no-replication
and that the global speedup reaches its optimal with C/R
alone without replication. Our ﬁndings suggest that, of these
three scenarios, scenario (a) in Figure 8 is what seems to be
true in practice, which is what we’ll demonstrate through our
theoretical and simulation-based analyses in the rest of this
section.
A. Theoretical Analysis
Our approach in this section will be to theoretically compare
the reliability-aware speedups of replication and no-replication
at the optimal processor counts for no-replication. The ratio-
nale for this analysis is that, if replication outperforms no-
replication at the optimal processor counts of no-replication,
it will mean that the global speedup behavior will be according
to scenario (a) shown in Figure 8. Below we discuss our
analysis for the two cases of parallelism, based on the value
of α.
Non-negligible Sequential part (α > 0): When α > 0, the
authors in [1] showed that for no-replication at its optimal
processor count, P ∗
norep, the expected time can be written as
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
534
Fig. 7. Optimal number of processors with replication. C = R = D = 300 seconds.
Fig. 8. Different possibilities of how the performance of replication and no replication compare to each other as the number of processors increases.
H
norep) ≈ α+3(α2(1−α)λ/2)1/3. We can
∗
norep = Hnorep(P ∗
also plug their expression for P ∗
norep (Equation 3) in the ﬁrst-
order approximation for the expected time of replication, as
given by Equation 9. The resulting expression, ignoring higher
order terms, is
Hrep(P ∗
norep) ≈ α + (4α2(1 − α)λ/2)1/3
(14)
Comparing the two expressions, we can clearly see that
Hnorep(P ∗
norep) > Hrep(P ∗
norep) which means that the perfor-
mance of replication at P ∗
norep is better than the performance
without replication. Thus, for workloads with non-negligible
sequential part, we can expect that replication will start out-
performing no replication before the optimal processor counts
without replication are reached.
Perfectly Parallel workload (α = 0): When α = 0, we
do not have an explicit formula for the optimal number of
processors without replication. We will, therefore, perform a
simpliﬁed analysis by assuming that the recovery cost and
downtime are both zero. Taking R = D = 0 in Equation 4 for
no replication, we get
Hnorep(P ) =
(15)
√
√
(eλP (
2C
λP +C) − 1)
2λCP 3
(cid:6)
2
(cid:4)
Taking the derivative with respect to P , setting it equal to 0
and simplifying, we obtain the following equation
(λP C +
λP C
√
2λP C − 3(eλP C+
)eλP C+
√
2λP C − 1)
2
Let x = λP C, then the equation above reduces to
(x +
x/2)ex+
√
√
2x − 3(ex+
2x − 1)/2 = 0
= 0
(16)
(17)
535
We can solve this equation numerically to obtain x ≈ 0.68015.
Note that this value of x is determined solely from the equation
above and is independent of the values of λ and C. This
means that λP C is an invariant with respect to λ and C
when P represents the optimal processor counts and this
invariant can be determined from the equation above. We
therefore obtain that the optimal processor count in this case
norep = x/λC ≈ 0.68015/λC. Plugging this value into
is P ∗
Equation 15, we get the normalized expected time for no
replication at its optimal processor count as
Hnorep(P ∗
norep) = Hnorep(
≈ 6.7283λC
x
λC ) = λC(
ex+
x
√
2x − 1
√
2x
)
(18)
Using Equation 8, we can write the performance of replication
at this value of P as
Hrep(P ∗
norep) = Hrep(
x
λC ) =
x(1 −
(cid:6)
2λC
(cid:5)
2λC
2x
πλC )
(19)
λC  Hrep(P ∗
By comparing the two equations above, we can derive that
Hnorep(P ∗
norep), i.e. replication outperforms
no replication at P ∗
π
(20)
This means that, whenever μ = 1/λ is greater than C/0.058 ≈
17.25C,
the expected performance of replication as given
by the approximation in Equation 8 will be better than the
performance of no replication. This bound is satisﬁed by
all realistic values of node MTBFs and checkpointing costs,
since individual node MTBFs usually are much higher than
)4 ≈ 0.058
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
let’s say that
the
the checkpointing cost. As an example,
checkpointing cost for a platform is 1 hour (a conservative
estimate, given the state of the art). Even with such a high
value of C, the bound above says that if individual node MTBF
is higher than 17.25 hours (which usually is the case since the
node MTBF usually is of the order of years), then the global
speedup behavior will be according to scenario (a) in Figure
8. With lower values of C, this threshold will be even lower,
which again should be satisﬁed by all practical node MTBF
values. This means that, for all realistic platform values of
λ and C, replication would have already outperformed no-
replication by the time we reach the optimal processor counts
for no-replication.
B. Empirical Evaluation
The theoretical results in the previous section give a very
strong indication that replication outperforms no-replication at
the optimal processor counts of no-replication, which in turn
means that the global normalized expected completion time,
H(P ) = min(Hnorep(P ), Hrep(P )) has the form depicted in
subﬁgure (a) in Figure 8. However, the two limiting factors
in the theoretical analysis were i) the contributions from
restart time, R, and the downtime, D, both of which are
non-zero in practice, were ignored for the case of perfectly
parallel workloads, and ii) the expression for Hrep(P ) is an
approximation since it uses the approach in Figure 1, unlike
Hnorep(P ), for which we have the exact expression using
Equation 1. Therefore, in this subsection we use our simulator
to investigate if the results of the theoretical analysis hold in
practice, despite the above mentioned limitations.
For our empirical analysis, we obtain Hrep(P ∗
norep) using
the following two steps:
1) For a given set of parameters (λ, C, R and D), we ﬁrst
norep of Hnorep(P ), using
numerically ﬁnd the optimal P ∗
its exact formula.