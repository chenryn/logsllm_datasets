title:Web content cartography
author:Bernhard Ager and
Wolfgang M&quot;uhlbauer and
Georgios Smaragdakis and
Steve Uhlig
Web Content Cartography
Bernhard Ager
T-Labs/TU Berlin
PI:EMAIL
Georgios Smaragdakis
T-Labs/TU Berlin
PI:EMAIL
Wolfgang Mühlbauer
ETH Zurich
PI:EMAIL
Steve Uhlig
T-Labs/TU Berlin
PI:EMAIL
ABSTRACT
1.
INTRODUCTION
Recent studies show that a signiﬁcant part of Internet trafﬁc is de-
livered through Web-based applications. To cope with the increas-
ing demand for Web content, large scale content hosting and de-
livery infrastructures, such as data-centers and content distribution
networks, are continuously being deployed. Being able to identify
and classify such hosting infrastructures is helpful not only to con-
tent producers, content providers, and ISPs, but also to the research
community at large. For example, to quantify the degree of hosting
infrastructure deployment in the Internet or the replication of Web
content.
In this paper, we introduce Web Content Cartography, i. e., the
identiﬁcation and classiﬁcation of content hosting and delivery in-
frastructures. We propose a lightweight and fully automated ap-
proach to discover hosting infrastructures based only on DNS mea-
surements and BGP routing table snapshots. Our experimental re-
sults show that our approach is feasible even with a limited num-
ber of well-distributed vantage points. We ﬁnd that some popular
content is served exclusively from speciﬁc regions and ASes. Fur-
thermore, our classiﬁcation enables us to derive content-centric AS
rankings that complement existing AS rankings and shed light on
recent observations about shifts in inter-domain trafﬁc and the AS
topology.
Categories and Subject Descriptors
C.2.5 [Computer-Communication Networks]: Local and Wide-
Area Networks—Internet
General Terms
Measurement
Keywords
Content delivery, hosting infrastructures, measurement, DNS
The measurement traces are available from
http://www.inet.tu-berlin.de/?id=cartography
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
Today’s demand for Web content in the Internet is enormous, re-
ﬂecting the value Internet users give to content [18]. Recent trafﬁc
studies [15, 12, 22, 27] show that Web-based applications are again
very popular. To cope with this demand, Web-based applications
and Web content producers use scalable and cost-effective hosting
and content delivery infrastructures. These infrastructures, which
we refer to as hosting infrastructures throughout this paper, have
multiple choices on how and where to place their servers.
Leighton differentiates between three options for Web content
delivery [24]: (i) centralized hosting, (ii) data-center-based content
distribution network (CDN), (iii) cache-based CDNs. Approaches
(ii) and (iii) allow to scale content delivery by distributing the con-
tent onto a dedicated hosting infrastructure. This hosting infrastruc-
ture can be composed of a few large data-centers, a large number
of caches, or any combination. In many cases, DNS is used by the
hosting infrastructure to select the server from which a user will
obtain content [20, 37, 7, 30].
The deployment of hosting infrastructures is dynamic and ﬂexi-
ble in multiple ways, e.g.: increasing the size of the existing host-
ing infrastructure, changing peerings with ISPs, placing parts of the
infrastructure inside ISP networks. Therefore, being able to iden-
tify and classify hosting infrastructures in an automated manner is a
step towards understanding this complex ecosystem, and an enabler
for many applications. Content producers can beneﬁt from under-
standing the footprint of hosting infrastructures to place content
close to their customer base. For CDNs, a map of hosting infras-
tructures can assist them in improving their competitiveness in the
content delivery market. For ISPs, it is important to know which
hosting infrastructures deliver a speciﬁc content and at which loca-
tions to make relevant peering decisions. The research community
needs a better understanding of the evolving ecosystem of hosting
infrastructures, given its importance as a driver in the evolution of
the Internet.
As demand drives hosting infrastructures to make a given content
available at multiple locations, identifying a particular hosting in-
frastructure requires sampling its location diversity. Previous work
has attempted to discover speciﬁc hosting infrastructures in an ex-
tensive manner, e. g., Akamai [36, 35, 17]. Such studies rely on the
knowledge of a signature that identiﬁes the target infrastructure,
e. g., CNAMEs in DNS replies or AS numbers. Labovitz et at. [22]
inferred that a small number of hosting infrastructures are responsi-
ble for a signiﬁcant fraction of inter-domain trafﬁc. Unfortunately,
this study observes only the trafﬁc crossing AS boundaries, not traf-
ﬁc delivered directly from inside the monitored ISPs. As a conse-
quence, important CDNs such as Akamai as well as data-centers
deployed inside ISP networks are under-represented.
In this paper, we introduce Web Content Cartography, i. e., the
585identiﬁcation and classiﬁcation of hosting infrastructures. We pro-
pose a lightweight and fully automated approach to discover host-
ing infrastructures based on DNS measurements and BGP routing
table snapshots. Compared to previous work, our method is able
to identify and classify new as well as existing hosting infrastruc-
tures without the need of a priori knowledge of their operation or
deployment. To achieve such a degree of generality, we rely on the
information that hosting infrastructures expose to end-users when
requesting hostnames through DNS. We construct mappings be-
tween requested hostnames and IP addresses returned, and cluster
the hostnames into hosting infrastructures with the help of network
information such as IP addresses, preﬁxes and AS numbers.
Our contributions can be summarized as follows:
• Identiﬁcation of hosting infrastructures: We propose a
lightweight and fully automated approach to discover host-
ing infrastructures, based on DNS measurements and BGP
routing table snapshots.
• Classiﬁcation of hosting infrastructures: We classify indi-
vidual hosting infrastructures and their different deployment
strategies based on their network and location footprint.
• Content replication: We quantify the degree of content repli-
cation in the Internet and its impact on local content avail-
ability in different regions of the world. We introduce the
content monopoly index that reﬂects the content an organiza-
tion hosts, either replicated or exclusively hosted.
• Revisiting AS rankings: We derive content-centric AS rank-
ings that complement existing AS rankings and shed light on
recent observations about shifts in inter-domain trafﬁc and
the AS topology.
The remainder of this paper is structured as follows. We present
our methodology in Section 2 and discuss our measurements in
Section 3. In Section 4, we provide our results, and discuss the
implications of our work in Section 5. We present related work in
Section 6 and summarize the paper in Section 7.
2. METHODOLOGY
In this section we describe our approach to identify and classify
hosting infrastructures in the Internet. The key idea is to collect
the IP addresses that DNS returns for various popular and unpop-
ular hostnames from geographically diverse vantage points. We
use this information for several purposes: (i) to ﬁnd the geographic
location where popular content is available from, (ii) to ﬁnd the net-
work locations, e. g., preﬁxes and ASes, where content is available,
and (iii) to ﬁnd out by which hosting infrastructure a hostname is
served.
2.1 Design Goals
To achieve our goals of mapping content and identifying host-
ing infrastructures, we design measurements tailored to our speciﬁc
needs: (i) we target the hosting infrastructures that host content and
(ii) we sample the network footprint of each of these hosting infras-
tructures in order to be able to classify them and study aspects such
as content replication. We now elaborate on our choices and ex-
plain why they ensure that our measurements allows us to achieve
the above goals.
Hosting Infrastructure Coverage.
To satisfy the ﬁrst requirement, i. e., obtaining a wide coverage
of popular hosting infrastructures in terms of trafﬁc volume, one
approach is to sample all possible hostnames. However, due to the
sheer size of the Web – an estimated 92 million active domains
only for the .COM top-level domain [6] – querying all host names
in the Internet would be way too cumbersome from a measurement
perspective. Fortunately, there is high variation in the popularity
of Web content. Given that Internet trafﬁc at various levels of ag-
gregation is consistent with Zipf’s law [13, 40, 38, 10], the hosting
infrastructures that serve popular hostnames are likely to be respon-
sible for a major part of today’s Internet trafﬁc. Despite a lack of
deﬁnitive ﬁgures about how many hosting infrastructures are re-
sponsible for most of the Web trafﬁc, we believe that it is reason-
able to assume that a limited number of highly popular Web sites
is sufﬁcient to cover the hosting infrastructures responsible for the
majority of the HTTP trafﬁc in the Internet. For example, Akamai
claims to deliver about 20 % of the total Web trafﬁc in the Inter-
net [30]. Labovitz et al. [22] attribute up to 10 % of all Internet
trafﬁc to Google, more than 15 % to the top 10 hosting infrastruc-
tures and more than 40 % to the top 100.
Network Footprint.
The second goal—sampling the network footprint of hosting
infrastructures—asks for measurements from multiple vantage
points. By running measurements from vantage points that reside
in different networks and countries, we beneﬁt from the way host-
ing infrastructures use DNS to select the server from which a user
obtains the requested content [20, 35, 36, 7]. CDNs rely on the
network location of the recursive DNS resolver to determine the IP
address returned by DNS [30, 28, 37]. In many cases, the host-
ing infrastructure assumes that the DNS resolver is close to the
client and optimizes based on this assumption. Therefore, to sam-
ple the locations from which a given hosting infrastructure serves
content, our approach relies on volunteers to sample from different
networks, ASes, and countries around the world.
2.2 Network Features
The way hosting infrastructures are deployed in the Internet is
not homogeneous. In Section 2.3 we leverage the “network foot-
print” of hosting infrastructures to map them. Now, we discuss fea-
tures that can be used to distinguish between different deployment
strategies of hosting infrastructures.
To this end, we extract the IP addresses obtained within the DNS
replies, from geographically dispersed vantage points. The set of
IP addresses returned for a particular hostname reveals the degree
to which the corresponding hosting infrastructure is network-wise
and geographically distributed. Hence, the natural choice for our
features are preﬁx, AS and location of an IP address. For example,
small data-centers will be located within a single AS in a single
geographic location, having a limited number of /24 subnetworks,
and a large number of IP addresses. A massively distributed CDN
will rely on multiple ASes. Evidently, these features are correlated,
and potentially differ in their power to discriminate between dif-
ferent types of hosting infrastructures. We leave this for further
investigation, and prefer to rely on all features for now.
Throughout the paper, we rely on both the granularity of BGP
preﬁxes as well as /24 subnetworks. /24 subnetworks have the ad-
vantage of better representing the actual usage of the address space
by highly distributed hosting infrastructures such as Akamai. BGP
preﬁxes on the other hand indicate at which granularity routing is
performed and more closely match the address space usage of cen-
tralized hosting infrastructures such as data-centers.
To determine the AS for a given IP address, we use BGP routing
information from RIPE RIS [4] and RouteViews [33], and assume
that the last AS hop in an AS path reﬂects the origin AS of the
preﬁx.
To infer the geographical location of an IP address, we rely on
586Figure 1: High level view of our approach.
the Maxmind geolocation database [29]. We are aware that geolo-
cation databases suffer from limited accuracy. However, they have
been shown to be reliable at the country-level [32].
2.3 Clustering Algorithm
Our goal is to detect where hosting infrastructures are located by
AS and country, and classify them according to their network foot-
print. The key idea is to cluster all hostnames that are served by the
same hosting infrastructure. To this end, we rely on the assumption
that each hostname is served by a single hosting infrastructure.
We are aware that counter-examples exist: Meebo, an instant
messenger aggregator, which is running its own meta-CDN, dis-
tributes the content demand across different CDNs by using a DNS
server under their control. Another case is Netﬂix, which offers
video-on-demand streams and relies on both Level 3 and Limelight.
Our approach accommodates such counter-examples by putting the
respective hostnames into separate clusters.
By examining the resolved CNAME records for various host-
names,
it is sometimes already possible to identify the CDN
that delivers the respective content. For example, a CNAME to
akamai.net clearly points to the Akamai CDN. Yet, ﬁnding con-
tent providers would require an extensive a-priori database. In ad-
dition, some CDNs do not use CNAMEs, and CNAMEs are also
used in different contexts than CDNs. In contrast, our clustering
approach achieves the important goal of identifying hosting in-
frastructures in the wild, and could be used to help build such a
database. Moreover, our agnostic approach is able to separate host-
ing infrastructures if they are actually maintained by the same ad-
ministrative entity (e.g., the Akamai CDN), but treat various host-
names differently in terms of replication.
In the rest of the section we present our algorithm that identi-
ﬁes hosting infrastructures based on our data. We choose a two-
step algorithm as depicted in Figure 1. During the ﬁrst step, we
ensure that the prominent hosting infrastructures are identiﬁed. It
also gives an upper bound on the size of the clusters. In the second
step, the algorithm merges clusters that share network features. The
ﬁrst step prevents the second one from clustering small hosting in-
frastructures with large ones. This may happen for example when
infrastructures share address space with others.
Step 1: Separating Large Hosting Infrastructures.
The goal here is to separate large hosting infrastructures from
the rest. We rely on three network-based features: (i) the num-
ber of IP addresses, (ii) the number of /24 networks and (iii) the
number of ASes a hostname is resolved to. We use the k-means
algorithm [26] to partition the hostnames in up to k clusters in the
feature space. The choice of the value of k is discussed at the end of
this subsection. Clusters whose features have high values relate to
widely-deployed infrastructures. On the other hand, smaller infras-
tructures that use very few /24 subnetworks and IP addresses are
not sufﬁciently different, and therefore, can be found in the same
cluster. Increasing the value of k in the clustering algorithm does
not lead to improvements, as the feature space simply does not al-
low to differentiate them.
Step 2: Distinguishing Small Hosting Infrastructures.
The pre-clustering of hostnames in Step 1 does not take into ac-
count the actual network locations from where content is served,
but only features that reﬂect the size of the hosting infrastructures.
The goal of the second step is to build sub-clusters within each
k-means cluster by identifying the hostnames that are hosted on
similar network locations in terms of IP address space. To this
end, we take into account the set of BGP preﬁxes the hostname
maps to. Based on the similarity between the sets of preﬁxes of
two similarity-clusters, we decide if they belong to the same host-
ing infrastructure, and if so we merge these clusters. For this, we
deﬁne the similarity between two sets s1 and s2 as follows:
similarity(s1, s2) = 2 ·
|s1 ∩ s2|
|s1| + |s2|
(1)
where |.| denotes the size of the set. The purpose of the factor 2 is
to stretch the image of the similarity function to the interval [0, 1].
The second step of the algorithm is performed for each k-means
cluster separately. Initially, we put each hostname contained in the
current k-means cluster into its own sub-cluster, called a similarity-
cluster. Then, we perform a pairwise comparison of all similarity-
clusters of the current k-means cluster and merge them according to
their similarity. We iterate the process until convergence to a ﬁxed
point. At this stage, each similarity-cluster identiﬁes all hostnames
used by a single content delivery infrastructure.
Tuning.
Generally, choosing k too high will lead to split large hosting
infrastructures into smaller clusters, while choosing it too low may
result in signiﬁcant overlap between hosting infrastructures. As
part of our case study in Section 4 we examine how sensitive the
outcome of our clustering is to the choice of k. We ﬁnd that the
whole interval 20 ≤ k ≤ 40 provides reasonable and similar
results according to our veriﬁcation, and therefore we decide to
choose k = 30. Extensive tests reveal that merging thresholds of
0.7 on the similarity between two similarity-clusters work well for
the second phase of the algorithm. We leave it for future work
to advance our clustering techniques and to optimize the choice
of parameters, and rather focus on the methodology for analyzing
hosting infrastructures in the following.
2.4 Metrics: Content Potential and Monopoly
Now, we propose metrics and rankings that allow us to compare
the obtained hosting infrastructures. We will use these metrics and
corresponding rankings later (Section 4) to get insight about the
geographic properties of content replication as well as the role of
different organizations in the hosting of Web content, e. g., which
organizations exclusively host content.
Content Delivery Potential.
Our goal is to provide intuition on the amount of content that is
available for download in a geographic region (e. g., country, conti-
nent) or an AS. To this end, we deﬁne the content delivery potential
as the fraction of hostnames that can be served from either a geo-
587graphic region or an AS. Values close to 1 suggest that a major part
of popular content is available locally. The disadvantage of the con-
tent delivery potential is that replicated content is counted as many
times as there are different locations where it is hosted, introducing
a bias in favor of replicated content.
Normalized Content Delivery Potential.
Therefore, we introduce the normalized content delivery poten-
tial, which takes into account the total number of locations from
where content is available. Intuitively, a location does not exclu-
sively deliver content, if the content has been replicated to a large
number of locations. To take this into account, we calculate the nor-
malized content delivery potential of a hostname as follows. First,
we determine the weight of a hostname, which is 1 divided by the
number of all hostnames. Second, we check how many different
ASes, subnetworks, or regions serve this count, henceforth referred
to as replication-count. To assess the contribution of a particular
hostname to the normalized content delivery potential of an AS,
subnetwork, or region, we take the weight from the ﬁrst step and
divide it by replication-count. The beneﬁt of the normalized con-
tent delivery potential is a more balanced ranking in terms of hosted
content, as it spreads the weight of distributed content infrastruc-
ture across all ASes, regions, or subnetworks that serve their hosted
content.
Content Monopoly Index.
To distinguish between locations (ASes, geographic regions) that
have exclusive content and those that host replicated content, we
introduce the Content Monopoly Index (CMI). We deﬁne it as
the ratio between the normalized content potential and the non-
normalized content potential. An AS with a large CMI hosts a large
number of hostnames that are not available in another AS.
3. MEASUREMENTS
In this section we present our approach to collect traces, i. e., ac-
tive DNS measurements, in order to evaluate our methodology. To
achieve our goal of identifying hosting infrastructures we compile
a list of diverse hostnames and analyze DNS traces when resolving
these hostnames as collected by end-users in commercial ISPs. Our
experimental results advocate that our methodology is able to iden-
tify a signiﬁcant fraction of hosting infrastructures network foot-
prints, even with a small number of well-distributed vantage points.
3.1 Hostname Selection
To obtain a good coverage of the largest hosting infrastructures,
we decide to include in our hostname list the top ranked ones ac-
cording to Alexa [1]. Alexa relies on statistical sampling and de-
termines its ranking by counting how many pages were visited by
Internet users who have downloaded their toolbar. Note, Alexa it-
self is already accounting for various sampling biases of its user
list1. In order to check for potential differences and to scrutinize
replication of content also for less popular hosts, we further add
hosts that are at the bottom of Alexa’s ranking.
Moreover, many web-pages contain embedded content, e. g., im-
ages, videos, and advertisements that the browser of the user has
to download from different servers. In our study, such embedded
content has to be taken into account, as it might be served from
servers other than those serving the front page of a popular host-
name. To give an example, the front page of facebook.com is
served from Facebook datacenters, but the logo and other embed-
ded objects such as the proﬁle photo is served from the Akamai
1http://alexa.com/help/traffic_learn_more
content distribution network. In addition, to increase the chance
of detecting the relevant infrastructures, we extracted hosts that are
likely to be hosted on hosting infrastructures from the ranks 2001
to 5000 of the Alexa list. We identify such hosts by checking if
they have CNAME records in their DNS answers.
Overall, we keep the 2,000 most popular and 2,000 from the least
popular hostnames according to the Alexa ranking. Moreover, we
include more than 3,400 embedded hostnames and 840 hostnames
because of CNAMEs. This list leads to four subsets which we will
refer to as TOP2000, TAIL2000, EMBEDDED, and CNAMES, re-
spectively, for the remainder of the paper. Note, that several host-
names are used to deliver both embedded objects as well as pop-
ular websites. This leads to an overlap of 823 hostnames between
TOP2000 and EMBEDDED.
3.2 Measurement Approach
Our measurement approach relies on volunteers to run a program