title:On sampling, anonymization, and differential privacy or, \emphk-anonymization
meets differential privacy
author:Ninghui Li and
Wahbeh H. Qardaji and
Dong Su
On Sampling, Anonymization, and Differential Privacy Or,
K-Anonymization Meets Differential Privacy
Ninghui Li, Wahbeh Qardaji, Dong Su
Purdue University
305 N. University Street
West Lafayette, IN 47907, USA
{ninghui, wqardaji, su17}@@cs.purdue.edu
ABSTRACT
This paper aims at answering the following two questions in
privacy-preserving data analysis and publishing: What for-
mal privacy guarantee (if any) does k-anonymization pro-
vide? How can we beneﬁt from the adversary’s uncertainty
about the data? We have found that random sampling
provides a connection that helps answer these two ques-
tions, as sampling can create uncertainty. The main result
of the paper is that k-anonymization, when done “safely”,
and when preceded with a random sampling step, satisﬁes
(ǫ, δ)-diﬀerential privacy with reasonable parameters. This
result illustrates that “hiding in a crowd of k” indeed of-
fers some privacy guarantees. We point out, however, that
almost all existing k-anonymization algorithms in the lit-
erature are not “safe”. Regarding the second question, we
provide both positive and negative results. On the positive
side, we show that adding a random-sampling pre-processing
step to a diﬀerentially-private algorithm can greatly amplify
the level of privacy protection. Hence, when given a dataset
resulted from sampling, one can utilize a much large pri-
vacy budget. On the negative side, any privacy notion that
takes advantage of the adversary’s uncertainty, likely does
not compose.
Categories and Subject Descriptors
H.2.8
Administration-Security,
[COMPUTERS AND SOCIETY]: Privacy
[DATABASE MANAGEMENT]: Database
integrity, and protection; K.4.1
General Terms
Security, Algorithms
Keywords
Diﬀerential Privacy, Anonymization, Data Privacy
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIACCS ’12, May 2–4, 2012, Seoul, Korea.
Copyright 2012 ACM 978-1-4503-1303-2/12/05 ...$10.00.
1.
INTRODUCTION
This paper aims at answering the following two questions
in privacy-preserving data analysis and publishing. The
ﬁrst is: What formal privacy guarantee (if any) does k-
anonymization methods provide? k-Anonymization meth-
ods have been studied extensively in the database commu-
nity, but have been known to lack strong privacy guarantees.
The second question is: How can we beneﬁt from the adver-
sary’s uncertainty about the data? More speciﬁcally, can we
come up a meaningful relaxation of diﬀerential privacy [8, 9]
by exploiting the adversary’s uncertainty about the dataset?
We now discuss these two motivations in more detail.
The k-anonymity notion was introduced by Sweeney and
Samarati [30, 29, 27, 28] for privacy-preserving microdata
publishing. This notion has been very inﬂuential. Many k-
anonymization methods have been developed over the last
decades; it has also been extensively applied to other prob-
lems such as location privacy [14]. The k-anonymity no-
tion requires that when only certain attributes, known as
quasi-identiﬁers (QIDs), are considered, each tuple in a k-
anonymized dataset should appear at least k times.
In
this paper, we consider a version of k-anonymity which
treats all attributes as QIDs. We show that even satis-
fying this strong version of k-anonymity does not protect
against re-identiﬁcation attacks. We then deﬁne classes
of k-anonymization algorithms that are “safe”, which avoid
the privacy vulnerabilities of existing k-anonymization al-
gorithms. One question we answer in this paper is “Can
these safe k-anonymization methods provide strong privacy
guarantee?”
The notion of diﬀerential privacy was introduced by
Dwork et al. [8, 11]. An algorithm A satisﬁes ǫ-Diﬀerential
Privacy (ǫ-DP) if and only if
for any two neighboring
datasets D and D′, the distributions of A(D) and A(D′)
diﬀer at most by a multiplicative factor of eǫ. A relaxed
version of ǫ-DP, which we use (ǫ, δ)-DP to denote, allows an
error probability bounded by δ. Satisfying diﬀerential pri-
vacy ensures that even if the adversary has full knowledge
of the values of a tuple t, as well as full knowledge of what
other tuples are in the dataset, and is only uncertain about
whether t is in the input dataset, the adversary cannot tell
whether t is in the dataset or not beyond a certain conﬁdence
level. As in most data publishing scenarios, the adversary is
unlikely to have precise information about all other tuples
in a dataset.
It is desirable to exploit this uncertainty to
deﬁne a relaxed version of diﬀerential privacy, which can be
easier to satisfy.
An interesting question is whether safe k-anonymization
can achieve some version of (possibly relaxed) diﬀerential
privacy. Diﬀerential privacy can be satisﬁed by adding ran-
dom noise to query answers. Most k-anonymization methods
can be viewed as returning the counts of tuples in diﬀerent
regions in a deterministic fashion: if the count is at or above
a threshold k, then return the count; otherwise, return 0.
Hence the question is: Does returning exact counts that are
large enough satisfy some variants of diﬀerential privacy?
We have found that sampling provides the link between
our two goals. The main result in this paper is that sampling
plus “safe” k-anonymization satisﬁes (ǫ, δ)-DP. We point out
that our result cannot be used to justify the privacy guar-
antees of existing k-anonymization methods, because almost
all of them are not safe. Instead, our result points out that
if one wants to use k-anonymization, and wants to satisfy
diﬀerential privacy, what one can do is to ﬁrst make the k-
anonymization method safe, and then apply it to datasets
resulted from sampling.
This result leads us to study the relationship between sam-
pling and diﬀerential privacy. We say that an algorithm
satisﬁes diﬀerential privacy under sampling if the algorithm
preceded with a random sampling step satisﬁes diﬀerential
privacy. Results about diﬀerential privacy under sampling
are both of theoretical interest and have practical relevance.
Sampling is a natural way to model the adversary’s uncer-
tainty about the data; thus, this helps understanding how
to take advantage of this uncertainty in private data anal-
ysis. On the practical side, many data publishing scenarios
already involve a random sampling step. Sometimes this
sampling step is explicit, when one has a large dataset and
wishes to release only a much smaller version for research,
such as the US census bureau’s 1-percent Public Use Mi-
crodata Sample. Sometimes, this sampling step is implicit;
because the respondents are randomly selected, one can view
the dataset as resulted from sampling.
The relationship between sampling and diﬀerential pri-
vacy has been studied before. Chauduri and Mishra [6]
studied the privacy eﬀect of sampling, and showed a linear
relationship between the sampling probability and the error
probability δ. Their result says that when a dataset contains
few items that are infrequent, then publishing a sampling
from the dataset satisﬁes (ǫ, δ)-diﬀerential privacy. Their re-
sult suggests an approach to perform ﬁrst k-anonymization
and then sampling as the last step. We instead consider the
approach of perform sampling as the ﬁrst step and then k-
anonymization. Our result suggests that the latter approach
beneﬁts much more from the sampling.
The contributions of this paper are as follows:
• We prove that a safe k-anonymization algorithm, when
preceded by a random sampling step, provides (ǫ, δ)-
diﬀerential privacy with reasonable parameters.
In the literature, k-anonymization and diﬀerential pri-
vacy have been viewed as very diﬀerent privacy guar-
antees: k-anonymization is syntactic, and diﬀerential
privacy is algorithmic and provides semantic privacy
guarantees. Our result links k-anonymization with dif-
ferential privacy.
This result also provides a new way of satisfying dif-
ferential privacy. Existing techniques for satisfying dif-
ferential privacy rely on output perturbation, that is,
adding noise to the query outputs. Our result suggests
an alternative approach. Rather than adding noise to
the output, one can add a random sampling step in
the beginning and prune results that are too sensitive
to changes of individual tuples (i.e., tuples that violate
k-anonymity). Comparing the utility of these two ap-
proaches is interesting future research direction and is
beyond the scope of this paper.
• We show both positive and negative results on utiliz-
ing the adversary’s uncertainty about the data. On
the positive side, we show that random sampling has
a privacy ampliﬁcation eﬀect for (ǫ, δ)-DP. For an
algorithm that satisﬁes (ǫ, δ)-DP, adding a sampling
step with probability β reduces both eǫ − 1 and δ by
a factor of β. For example, applying an algorithm
that achieves (ln 2 ≈ 0.69)-diﬀerential privacy on a
dataset sampled with 0.1 probability can achieve over-
all (ln 1.1 ≈ 0.095)-diﬀerential privacy.
On the negative side, we show that any privacy no-
tion that exploits the adversary’s uncertainty about
the data is unlikely to compose, in the sense that pub-
lishing the output from two algorithms together may
be non-private.
Our results suggest the following approaches to take
advantage of the fact that the input dataset is resulted
from explicit or implicit sampling. If one applies al-
gorithms that satisfy (ǫ, δ)-DP, then one can allow a
larger privacy budget because of sampling. If one ap-
plies an algorithm that does not satisfy (ǫ, δ)-DP, but
satisﬁes (ǫ, δ)-DP under sampling, then it is safe to
apply the algorithm once. This is useful in the non-
interactive setting where one publishes an anonymized
or synthesized version of the dataset.
The rest of the paper is organized as follows. We study
the relationship between diﬀerential privacy and sampling in
Section 2. We study k-anonymization and prove our main
result in Section 3. We discuss related work in Section 4
and conclude in Section 5. An appendix includes proofs not
found in the main body.
2. DIFFERENTIAL
PRIVACY UNDER
SAMPLING
Diﬀerential privacy formalizes the following protection ob-
jective:
if a disclosure occurs when an individual partici-
pates in the database, then the same disclosure also occurs
with similar probability (within a small multiplicative fac-
tor) even when the individual does not participate.
Definition 1. [ǫ-Diﬀerential Privacy
11]
(ǫ-DP)]: A randomized algorithm A gives ǫ-diﬀerential
privacy if for any pair of neighboring datasets D and D′,
and any O ⊆ Range(A),
[8,
Pr[A(D) ∈ O] ≤ eǫ Pr[A(D′) ∈ O]
(1)
Intuitively, ǫ-DP oﬀers strong privacy protection.
If A
satisﬁes ǫ-DP, one can claim that publishing A(D) does not
violate the privacy of any tuple t in D, because even if one
leaves t out of the dataset, in which case the privacy of t
can be considered to be protected, one may still publish the
same outputs with a similar probability.
In the literature, ǫ-DP is often relaxed to allow a small
error probability δ. This can accommodate algorithms that
satisfy Inequality 1 with high probability.
Definition 2. [(ǫ, δ)-Diﬀerential
[10]
((ǫ, δ)-DP)]: A randomized algorithm A satisﬁes (ǫ, δ)-
diﬀerential privacy, if for any pair of neighboring datasets
D and D′ and for any O ⊆ Range(A):
Privacy
P r[A(D) ∈ O] ≤ eǫP r[A(D′) ∈ O] + δ
Existing methods to satisfy diﬀerential privacy include
adding Laplace noise proportional to the query’s global sen-
sitivity [8, 11], adding noise related to the smooth bound of
the query’s local sensitivity [26], and the exponential mech-
anism to select a result among all possible results [25].
2.1 Uncertain Background Knowledge
One of our goals is to investigate how to deﬁne a relaxation
of diﬀerential privacy that exploits the adversary’s uncer-
tainty about the underlying dataset. The (ǫ, δ)-DP notion
ensures that when an adversary is uncertain about whether
one tuple t is present in the input dataset, even when the
adversary knows the precise information of all other tuples
in the input dataset, the adversary cannot tell based on the
output whether t is in the input or not. We believe that it
is reasonable to relax the assumption to that the adversary
knows all attributes of a tuple t (but not whether t is in
the dataset), and in addition statistical information about
the rest of the dataset D. The privacy notion should prevent
such an adversary from substantially distinguishing between
D and D ∪ {t} based on the output.
The desire to exploit adversary’s uncertainty is shared by
other researchers. For example, Adam Smith’s blog post1
summarizing the Workshop on Statistical and Learning-
Theoretic Challenges in Data Privacy includes a section on
relaxed deﬁnitions of privacy with meaningful semantics: “it
would be nice to see meaningful deﬁnitions of privacy in sta-
tistical databases that exploit the adversary’s uncertainty
about the data. The normal approach to this is to specify
a set of allowable prior distributions on the data (from the
adversary’s point of view). However, one has to be careful.
The versions I have seen are quite brittle.”
It appears that some degree of brittleness is unavoidable.
It appears that any privacy notion that takes advantage of
the adversary’s uncertainty about the data is not robust
under composition, which requires that given two algorithms
that both satisfy the privacy notion, their composition, i.e.,
applying both algorithms to the same input dataset and then
publishing both outputs, also satisﬁes the privacy notion.
Consider the following two algorithms. Let r(D) be the
predicate that D contains an odd number of tuples, and
s(D) be a sensitive predicate, e.g., whether a tuple t is in D.
Algorithm A1(D) outputs r(D), and A2(D) outputs r(D)
XOR s(D). Both A1 and A2 should satisfy a privacy notion
that assumes that the adversary is uncertain about the data,
because there is no reason that the adversary should know
whether the number of the tuples is odd or even. However,
the composition of A1 and A2 leaks r(D). More generally,
for any privacy deﬁnition that exploits the adversary’s un-
certainty about the dataset, one should be able to ﬁnd a
predicate that the adversary has little reason to bias the
91http://adamdsmith.wordpress.com/2010/03/04/ipam-
workshop-wrap-up/
ǫ
eǫ