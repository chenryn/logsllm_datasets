title:Efficient Treatment of Uncertainty in System Reliability Analysis
using Importance Measures
author:Hananeh Aliee and
Faramarz Khosravi and
J&quot;urgen Teich
2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Efﬁcient Treatment of Uncertainty in System
Reliability Analysis using Importance Measures
Hananeh Aliee
Faramarz Khosravi
J¨urgen Teich
Helmholtz Zentrum M¨unchen
Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg
Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract—The reliability of today’s electronic products suffers
from a growing variability of failure and ageing effects. In this
paper, we investigate a technique for the efﬁcient derivation of
uncertainty distributions of system reliability. We assume that a
system is composed of unreliable components whose reliabilities
are modeled as probability distributions. Existing Monte Carlo
(MC) simulation-based techniques, which iteratively select a
sample from the probability distributions of the components,
often suffer from high execution time and/or poor coverage of
the sample space. To avoid the costly re-evaluation of a system
reliability during MC simulation, we propose to employ the
Taylor expansion of the system reliability function. Moreover,
we propose a stratiﬁed sampling technique which is based on
the fact that the contribution (or importance) of the components
on the uncertainty of their system may not be equivalent. This
technique ﬁnely/coarsely stratiﬁes the probability distribution of
the components with high/low contribution. The experimental
results show that the proposed technique is more efﬁcient and
provides more accurate results compared to previously proposed
techniques.
Index Terms—Reliability, Uncertainty Analysis, Sampling, Im-
portance Measure, System Design, Stratiﬁed Sampling
I. INTRODUCTION
Due to continuous technology scaling, today’s electronic
components have become susceptible to various failure and
degradation mechanisms such as gate oxide breakdown [39]
and Negative Bias Temperature Instability (NBTI) [34]. To
design a reliable system from such inherently unreliable com-
ponents, accurate reliability analysis and, if necessary, efﬁcient
reliability increasing techniques are of great importance. Con-
ventional reliability analysis techniques typically calculate the
reliability of a system through constructing and evaluating a
reliability model, e. g., using Success Trees (STs) [4] or Binary
Decision Diagrams (BDDs) [17]. Reliability models enable
the reliability analysis of a system when the reliabilities of
its components are assumed to be given as a point estimate,
i. e., a single value at each point of time. However, due to
the signiﬁcant variability in the manufacturing process as well
as environmental and usage conditions, the exact reliability
of a component may not be known a-priori. Therefore, the
reliability of a component at a certain time must be considered
uncertain and speciﬁed as a probability distribution or sampled
data rather than a single value. However, the reliability analysis
then becomes unwieldy and cannot be performed in a straight-
forward manner. Indeed, uncertainty analysis techniques are
required to analyze the uncertainty at the component level and
obtain the probability distribution of the reliability at system
level.
One solution to this problem is to use mathematical ap-
proaches to evaluate the uncertainty of a system. As an
example, the work in [44] derives the mean and quantiles
of the system reliability using second-order and Gaussian
approximations. However, the mathematical approaches are
often cumbersome and their applications are limited to only
simple reliability models and certain uncertainty distributions
such as uniform or Gaussian distributions. An alternative
solution is to apply MC simulation which is a very effective
approach to solve complex numerical problems that deal
with a probabilistic interpretation of their components. As
an example, the authors of [26] propose to iteratively select
a sample from the probability distributions of components
and apply a conventional reliability model to calculate the
reliability of the system for that sample. According to the
law of large numbers, performing the same process a large
number of times, the approximated reliability distribution at
system level tends to become closer to the real distribution.
However, considering complex electronic systems consisting
of hundreds to thousands of components, the time overhead of
MC simulations is often prohibitively high. As a remedy, more
efﬁcient sampling techniques have been proposed in statistics
such as stratiﬁed sampling techniques like Latin Hypercube
Sampling [31] and Jittered Sampling [13]. Stratiﬁcation is the
process of dividing a sample space into strata or a random
variable (in this paper, referred to the probability distribution
of a component’s reliability) into quantiles before sampling.
During sampling, the samples are then uniformly selected from
the strata or quantiles. The main objective of the stratiﬁcation
is to reduce the sampling error by improving the space
ﬁlling. In general terms, space-ﬁlling aims at minimizing the
maximal distance of a set of neighbour samples to cover a
given sample space. However, stratiﬁed sampling techniques
commonly neglect the fact that the uncertainties of components
may not equally contribute to the uncertainty of the system.
Indeed, the deviation of the reliability of a component from
its expected value may result in either negligible or signiﬁcant
deviation of the reliability of the whole system.
To address both model uncertainty and analysis complexity,
we ﬁrst propose to estimate the reliability of a system using
Taylor expansions. The Taylor expansion is centred at an initial
point (or initial sample) whose reliability should be given in
978-1-7281-0057-9/19/$31.00 ©2019 IEEE
DOI 10.1109/DSN.2019.00022
76
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
advance. In this paper, the reliability of the initial point is
evaluated using Success Trees (STs) which are simple and
can be efﬁciently generated for large examples. The reliability
of every other sample can then be computed employing the
Taylor expansion which comes with noticeably lower compu-
tational/time overhead than conventional approaches. This is
of particular interest to reduce the time overhead of an MC
simulation and to increase the number of samples. Second, we
propose to rank the components of a given system using the
concept of component importance. Herein, the importance of
a component is a probability that states how the uncertainty of
the component contributes to the uncertainty at system level.
Knowing the contribution of each component, we introduce a
heterogeneous sampling approach that stratiﬁes the probability
distributions of highly important components more ﬁnely than
those of the least important ones. This is due to the fact
that
increasing the number of quantiles results in higher
space ﬁlling. On the other hand, the relatively low important
components can be coarsely stratiﬁed because of their low
contribution to the system’s reliability uncertainty.
We experimentally demonstrate that the proposed approach
I) outperforms state-of-the-art uncertainty analysis techniques
based on BDDs and STs, II) is more accurate than the
simulation-based ST approach, and III) approximates the
uncertainty distribution of the reliability of a system more
accurately compared to the widely used sampling techniques
in literature. Although our technique is motivated by the chal-
lenges of reliability analysis under the presence of uncertainty,
it could also be applied to other domains which are subject to
sampling.
This paper is organized as follows: In Section II, we
present related work on both uncertainty handling and sam-
pling techniques. Section III provides the background and
the motivation of this work. Later, Section IV introduces
the proposed uncertainty analysis using Taylor expansions as
well as the heterogeneous stratiﬁed sampling technique. The
proposed overall approach is evaluated in Section V regarding
its accuracy, performance, and efﬁciency. Finally, Section VI
concludes the paper.
II. RELATED WORK
A. Uncertainty Handling
In the context of risk and safety assessment, uncertainty
is categorized into aleatory and epistemic uncertainties. The
former originates from the “inherent variation associated with
a physical system or the environment under consideration”,
while the latter arises from “any lack of knowledge or infor-
mation in any phase or activity of the modeling process” [33].
Unlike aleatory uncertainty that is irreducible, epistemic un-
certainty can be reduced by obtaining more information about
the system. However, uncertainty reduction usually requires
costly and cumbersome analysis of the system which is not
affordable in many applications. Thus, the system must be
designed to perform well despite of its inherent and inevitable
uncertainties.
TABLE I: List of symbols employed in this paper.
i
Symbols
l
u
fY
Ii
IS
Ji1...ik
λ
λLB
λU B
i
μ
N
Ns
Q
qi
R
Ri
S
S
S 0
xi
Y
the lower bound of variable Y
the upper bound of variable Y
the probability density function of variable Y
Birnbaum importance of xi
the set of all initial states S 0
the joint importance of components xi1 . . . xik
failure rate
the lower bound failure rate of component xi
the upper bound failure rate of component xi
mean value
the total number of components
the total number of samples
the total number of strata
the number of quantiles in dimension i
the reliability function of a system
the reliability of component xi
the set of all samples
the current state of a system
the initial state of a system
i-th component
system property or variable
The existing work to cope with uncertainty can be classiﬁed
in three groups, namely uncertainty quantiﬁcation, uncertainty
analysis or propagation, and robust optimization. Uncertainty
quantiﬁcation aims at deriving the uncertainties of the sys-
tem’s components as intervals (with lower and upper bounds),
statistical properties such as mean and variance, probability
distributions, etc. This is an application-speciﬁc task and re-
quires detailed analysis or experimentation of the environment,
the model of the system as well as expert knowledge and
judgement [12]. As an example, the work in [22] aims at
deriving the uncertainty in the lifetime of today’s electronic
devices under the impact of transistor scaling.
Uncertainty analysis intends to derive uncertainty of the
whole system given that the uncertainties of its components
are known a-priori. There are several methods for uncer-
tainty analysis, including probabilistic and non-probabilistic
approaches. Probabilistic approaches such as discrete proba-
bility analysis [23] and MC simulation [26] assume that the
uncertainties of components are given as probability distribu-
tions. These approaches then attain the probability distribution
of the system reliability. When limited information about
the uncertainties of components is available, non-probabilistic
approaches like interval arithmetic [24], fuzzy set theory [40],
and evidence (belief) theory [36] may still provide insights on
the uncertainty at system level.
Given an estimated uncertain characteristics of a system,
robust optimization may be of use to ﬁnd solutions, i. e.,
design and implementation candidates, that improve the ef-
ﬁciency (e. g., low power consumption and high performance)
77
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
of a system in the presence of uncertainty. State-of-the-art
techniques to select the non-dominated solutions among all
the possible candidates include techniques using probabilistic
dominance [19], [25], [42], fuzzy set theory [15], interval
theory [32], as well as statistical methods [26].
In this paper, we assume that the (aleatory or epistemic)
uncertainties of components are speciﬁed by probability dis-
tributions. We then develop a probabilistic uncertainty analysis
technique for system reliability based on Taylor expansion
which allows to efﬁciently evaluate the uncertainty at system
level.
B. Sampling Techniques
Sampling techniques play an important role in MC-based
uncertainty analysis to improve the overall estimation of
the system’s uncertainty distribution. One of the effective
attributes of a sampling technique is its capability to cover
a sample space. In probability theory, a sample space is
referred to the set of all possible outcomes of a random
experiment. Since possible outcomes may not be uniformly
distributed through the sample space, they can be represented
in a probability space. In a probability space, the probability
that the next random outcome falls into any sub-space of
an equal size is identical. Then, a sampling technique has
a high space ﬁlling if it provides a uniform distribution of
samples through the probability space. Assume that we want
to generate Ns samples from an N-dimensional random space,
where each sample S is an N-tuple. The random space is
formed by N arbitrarily distributed random variables, e. g., the
reliabilities of the components in this paper. In the following,
we review the sampling techniques which are most relevant to
this paper.
Simple Random Sampling (SRS): At each sampling step,
SRS generates an N-tuple with elements being selected from
the entire sample space of each random variable individually.
Herein, the samples are selected independent of each other
and the implementation of the technique comes with low
computational overhead. However, it often provides poor space
ﬁlling and typically requires high number of samples to
approximate the distribution of uncertainty at system level.
Latin Hypercube Sampling (LHS): To improve SRS, LHS
proposes to partition the random space of each variable to
q subintervals with equal probabilities called quantiles. At
each sampling step, for each random variable, one quantile
is selected randomly without replacement. Then, the SRS is
applied within the selected quantile. Since the selection of the
quantiles is done without replacement, each quantile contains
one and only one sample after the sampling process. As a
result, a one-dimensional projection of samples show a fair
ﬁlling of the random space, i. e., decent space ﬁlling. However,
since LHS pairs the quantiles of the variables randomly, the
N-dimensional projection of the samples does not generally
provide a high space ﬁlling [14].
Jittered Sampling (JS): Unlike LHS, JS stratiﬁes the whole
sample space, i. e., the N-dimensional hypercube. In other
words, it partitions each dimension (or variable space) into q =
√
N
Ns quantiles and considers all possible qN combinations
of quantiles among all random variables. Each combination of
quantiles is called a Stratum. Moreover, different combinations
do not overlap as they occur mutually exclusive and all the
combinations together ﬁll the whole random space. At each
sampling step, a stratum is selected without replacement and
a random N-tuple sample is selected by applying SRS within
that stratum. The JS is restricted to Ns = qN samples.
The resulting one-dimensional projections show that the space
ﬁlling is generally better than SRS, but worse than LHS.
However, the N-dimensional projection appears to have higher
space ﬁlling than the previous techniques.
Multi-Jittered Sampling (MJS): To provide a decent
space ﬁlling in both one-dimensional and N-dimensional
projections, MJS or Sudoku LHS combines
JS and
√
LHS [41]. It partitions each dimension of the sample space
Ns and once for the
twice, once for the Jittered with N
LHS with Ns partitions and satisﬁes the constraints in both
techniques. Despite its better space ﬁlling, the implementation
of the MJS has a higher computational complexity compared
to others.
Example: Consider a two-dimensional uniformly dis-
tributed1 sample space as shown in Figure 1. 16 samples
are taken from this sample space using the aforementioned
sampling techniques. It can be seen that MJS provides the
best space ﬁlling, i. e., one- and two-dimensional projections,
because it combines the advantages of as well LHS as JS.
(cid:2)
Several other sampling techniques have been introduced in
the literature. Among them, the authors of [37] propose a
Partially Stratiﬁed Sampling (PSS) which partitions an N-
dimensional sample space into low-dimensional orthogonal
subspaces. Within each subspace, a stratiﬁed sampling (e. g.,
Sudoku LHS) is applied and ﬁnally,
the low-dimensional
samples are randomly grouped to produce an N-dimensional
sample. Also, the authors of [21] propose an optimization tech-
nique called enhanced stochastic evolutionary algorithm to
optimize the space ﬁlling, entropy and other statistical proper-
ties of an N-dimensional sampling. Importance sampling [38]
is another well-known sampling technique for estimating the
Y(x)fxdx of a random
expected value μ = E[Y(X)] =
variable Y(X) where fx is the probability distribution function
of X on D. Therein, the idea is that certain values of input
variables X results in higher absolute value of Y(X). So, the
estimator variance can be reduced if these “important” values
are emphasized by sampling more frequently. As a result,
this approach proposes to sample the input variables from an
alternative distribution which encourages the important values.
However, ﬁnding the alternative distribution is challenging,
especially when the input distributions are given as a set of
samples. Also, for high dimensions, this technique can be com-
putationally very expensive. Finally, we refer to [1], [13], [28],
[35] for other sampling techniques proposed with application
in visualization (with two-dimensional sample spaces).
D
1Due to the uniform distribution of samples, the sample space and the
corresponding probability space look similar.
78
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
Simple Random Sampling
Latin Hypercube Sampling
Jittered Sampling
Multi-Jittered Sampling
Fig. 1: Comparing the space ﬁlling of four different sampling techniques. Multi-Jittered Sampling provides the best space
ﬁlling.
III. MATHEMATICAL BACKGROUND AND MOTIVATION
Importance Measures: A system typically consists of several
components that do not equally contribute to the quality (e. g.,
reliability) of the system. This observation is at the basis of
Importance Measures (IMs). IMs are mathematical indicators
that characterize the contribution of a given component to
the system quality. Traditionally, IMs have been employed
to simplify the problem of resource allocation by replicat-
ing, hardening, or re-arranging the important components to
enhance the overall system reliability. Respectively, various
IMs have been proposed in the literature to judge the relative
importance of the components of a system with respect to
different criteria [29]. In this work, we concentrate on the
Birnbaum reliability IM [7] which has been widely employed
by state-of-the-art techniques2. The Birnbaum reliability IM
of component xi of a system is deﬁned as:
(S)
Ii(S) = R(S)|xi works − R(S)|xi fails = ∂R
∂Ri
(1)
In this equation, S can be seen as the current state of the sys-
tem modeled by an N-dimensional vector S = (R1, . . . ,RN )
containing the reliability function Rx of each component x.
Also, R(S)|xi works (R(S)|xi fails) denotes the reliability of
the system in state S conditional
to the success (failure)
of component xi. From this equation, Ii(S) measures the
change in the reliability of the system when the state of
component xi goes from working state to the failed state.
Therefore, the higher the importance of this component, the
more its failure contributes to the failure of the system. It is
also shown that the Birnbaum reliability IM of component xi
is equivalent to the ﬁrst derivative of the system reliability
function regarding the reliability function of component xi.