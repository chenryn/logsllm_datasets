# Title: SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol

## Authors
- Abhinandan Das
- Indranil Gupta
- Ashish Motivala

## Abstract
Several distributed peer-to-peer applications require weakly-consistent knowledge of process group membership information at all participating processes. SWIM is a generic software module that offers this service for large-scale process groups. The motivation behind SWIM is the unscalability of traditional heart-beating protocols, which either impose network loads that grow quadratically with group size or compromise response times and false positive frequency in detecting process crashes. This paper reports on the design, implementation, and performance of the SWIM subsystem on a large cluster of commodity PCs.

Unlike traditional heartbeating protocols, SWIM separates the failure detection and membership update dissemination functionalities. Processes are monitored through an efficient peer-to-peer periodic randomized probing protocol. Both the expected time to first detection of each process failure and the expected message load per member do not vary with group size. Information about membership changes, such as process joins, drop-outs, and failures, is propagated via piggybacking on ping messages and acknowledgments, resulting in a robust and fast infection-style (also known as epidemic or gossip-style) dissemination.

The rate of false failure detections in the SWIM system is reduced by allowing group members to suspect a process before declaring it as failed, which allows the system to discover and rectify false failure detections. Finally, the protocol guarantees a deterministic time bound to detect failures. Experimental results from the SWIM prototype are presented, and the extensibility of the design to a WAN-wide scale is discussed.

## 1. Introduction
Several large-scale peer-to-peer distributed process groups running over the Internet rely on a distributed membership maintenance subsystem. Examples of existing middleware systems that utilize a membership protocol include reliable multicast [3, 11] and epidemic-style information dissemination [4, 8, 13]. These protocols find use in applications such as distributed databases that need to reconcile recent disconnected updates [14], publish-subscribe systems, and large-scale peer-to-peer systems [15]. The performance of other emerging applications, such as large-scale cooperative gaming and other collaborative distributed applications, critically depends on the reliability and scalability of the membership maintenance protocol used within.

A membership protocol provides each process ("member") of the group with a locally-maintained list of other non-faulty processes in the group. The protocol ensures that the membership list is updated with changes resulting from new members joining the group or dropping out (either voluntarily or through a failure). The membership list is made available to the application either directly in its address space, through a callback interface, or an API. The application can use the contents of the list as required, e.g., gossip-based dissemination protocols would use the list to periodically pick target members for gossip.

The reliability and scalability of a membership subsystem can be measured via several performance metrics. Membership changes must be propagated within the group quickly after their occurrence. The asynchrony and unreliability of the underlying network can cause messages to be lost, leading to false detection of process failures, as a process that is losing messages is indistinguishable from one that has failed [10]. The rate of false positives must be low. Finally, the protocol needs to be peer-to-peer (not rely on a central server) and impose low message and computation loads on the network and processes.

Membership protocols have been difficult to scale in groups with more than a few dozen processes [11, 16], affecting the performance of applications using them. As reported in [16], the main symptoms of poor performance at these group sizes are an increase in either the rate of false failure detections of processes or the time to detect a failure. [12] identifies the quadratic increase in the message load imposed by such membership protocols as another symptom of the unscalability of traditional protocols for membership maintenance. An example of an application that relies heavily on the membership subsystem is the class of virtually synchronous multicast protocols [3]. Traditional implementations of this specification suffer a drastic reduction in performance and partitioning beyond a few dozen members [11].

This paper presents our effort in the SWIM project to implement a membership subsystem that provides stable failure detection time, stable rate of false positives, and low message load per group member, thus allowing distributed applications that use it to scale well. We focus on a weaker variant of group membership, where membership lists at different members need not be consistent across the group at the same (causal) point in time. Stronger guarantees could be provided by augmenting the membership subsystem, e.g., a virtually-synchronous style membership can be provided through a sequencer process that checkpoints the membership list periodically. However, unlike the weakly consistent problem, strongly consistent specifications might have fundamental scalability limitations [11].

## 2. Previous Work
In traditional distributed all-to-all heartbeating failure detection algorithms, every group member periodically transmits a "heartbeat" message (with an incremented counter) to all other group members. A member \( M_i \) is declared as failed by a non-faulty member \( M_j \) when \( M_j \) does not receive heartbeats from \( M_i \) for some consecutive heartbeat periods.

Distributed heartbeating schemes guarantee that a faulty member is always detected as such at any non-faulty member (within a time interval after its failure) [6], since a member that has crashed also stops sending heartbeat messages. However, the accuracy and scalability guarantees of these protocols differ depending on the actual mechanism used to disseminate the heartbeats.

In the simplest implementation, each heartbeat is multicast to all other group members, resulting in a network load of \( \Theta(n^2) \) messages per second (even if IP multicast is used), where \( T \) is the failure detection time required by the distributed application. van Renesse et al. [16] proposed that heartbeats be disseminated via a robust gossip-style protocol. In this protocol, every \( t_{gossip} \) time units, each member gossips, to a few random targets, a \( \Theta(n) \)-sized list of the latest known heartbeat counters received from other members. While gossiping reduces the false positive frequency, a new heartbeat count typically takes, on expectation, \( \Theta(\log(n) \cdot t_{gossip}) \) time units to reach an arbitrary other group member. To satisfy the application-specified detection time, the protocol generates a network load of \( \Theta\left(\frac{n \log(n)}{t_{gossip}}\right) \) bytes per second. The use of message batching to solve this is limited by the UDP packet size limit, e.g., 5B heartbeats (IP address and count) of 50 members would already occupy 250 B, while SWIM generates packets that have a size of at most 135 B, regardless of the group size.

The quadratic increase in the network load results from the communication of heartbeat notifications to all group members. This can be avoided by separating the failure detection operation from that of membership update dissemination. Several hierarchical membership systems have been proposed, e.g., Congress [1]. This belongs to a broader class of solutions where each process heartbeats only a subgroup of processes. This class of protocols requires careful configuration and maintenance of the overlay along which membership information flows, and the accuracy of the protocol depends on the robustness of this graph. In comparison, the design of SWIM avoids the overhead of a virtual graph.

SWIMâ€™s solution to the above unscalability problems is based on (a) designing the failure detection and membership update dissemination components separately, and (b) using a non-heartbeat based strategy for failure detection.

Before moving on to describe the SWIM protocol internals, we first lay the foundation for understanding the key characteristics of the efficiency and scalability of distributed failure detector protocols. Several research studies [6, 7, 12, 16] have led to the identification of these basic properties of distributed failure detector protocols (from both theoretical and practical angles), as well as impossibility results related to satisfying them concurrently. The resulting tradeoff is usually determined by the safety and liveness properties required by distributed applications. These properties are [12]:

1. **Strong Completeness**: Crash-failure of any group member is detected by all non-faulty members [6].
2. **Speed of Failure Detection**: The time interval between a member failure and its detection by some non-faulty group member.
3. **Accuracy**: The rate of false positives of failure detection.
4. **Network Message Load**: Bytes per second generated by the protocol.

[6] proved the impossibility of building a failure detector over an asynchronous network that is both accurate (no false detections) and strongly complete. However, since a typical distributed application relies on Strong Completeness always holding (to maintain up-to-date information in dynamic groups), most failure detectors, including heartbeating-based solutions, guarantee this property while attempting to maintain a low rate of false positives. SWIM takes the same approach.

In [12], a simple computation identifies the minimal total network load (bytes per second) required to satisfy specified parameters of false detection rate at each member (denoted \( P_M(T) \)), and detection time \( T \) in a group of size \( n \). [12] calculates this load as \( n \cdot \log(P_M(T)) \), where \( p_{ml} \) is the probability of a packet drop within the underlying network.