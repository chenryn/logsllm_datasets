title:SWIM: Scalable Weakly-consistent Infection-style Process Group Membership
Protocol
author:Abhinandan Das and
Indranil Gupta and
Ashish Motivala
SWIM: Scalable Weakly-consistent Infection-style Process Group Membership
Protocol
∗
Abhinandan Das, Indranil Gupta, Ashish Motivala
Dept. of Computer Science, Cornell University
{asdas,gupta,ashish}@cs.cornell.edu
Ithaca NY 14853 USA
Abstract
1. Introduction
Several distributed peer-to-peer applications require
weakly-consistent knowledge of process group membership
information at all participating processes.
SWIM is a
generic software module that offers this service for large-
scale process groups. The SWIM effort is motivated by the
unscalability of traditional heart-beating protocols, which
either impose network loads that grow quadratically with
group size, or compromise response times or false positive
frequency w.r.t. detecting process crashes. This paper re-
ports on the design, implementation and performance of the
SWIM sub-system on a large cluster of commodity PCs.
Unlike traditional heartbeating protocols, SWIM sepa-
rates the failure detection and membership update dissem-
ination functionalities of the membership protocol. Pro-
cesses are monitored through an efﬁcient peer-to-peer peri-
odic randomized probing protocol. Both the expected time
to ﬁrst detection of each process failure, and the expected
message load per member, do not vary with group size.
Information about membership changes, such as process
joins, drop-outs and failures, is propagated via piggyback-
ing on ping messages and acknowledgments. This results in
a robust and fast infection style (also epidemic or gossip-
style) of dissemination.
The rate of false failure detections in the SWIM system
is reduced by modifying the protocol to allow group mem-
bers to suspect a process before declaring it as failed - this
allows the system to discover and rectify false failure detec-
tions. Finally, the protocol guarantees a deterministic time
bound to detect failures.
Experimental results from the SWIM prototype are pre-
sented. We discuss the extensibility of the design to a WAN-
wide scale.
∗
Author last names are in alphabetical order. The authors were sup-
ported in part by NSF CISE grant 9703470, in part by DARPA/AFRL-
IFGA grant F30602-99-1-0532, and in part by a grant under NASA’s REE
program, administered by JPL.
As you swim lazily through the milieu,
The secrets of the world will infect you.
Several large-scale peer-to-peer distributed process groups
running over the Internet rely on a distributed membership
maintenance sub-system. Examples of existing middleware
systems that utilize a membership protocol include reliable
multicast [3, 11], and epidemic-style information dissemi-
nation [4, 8, 13]. These protocols in turn ﬁnd use in applica-
tions such as distributed databases that need to reconcile re-
cent disconnected updates [14], publish-subscribe systems,
and large-scale peer-to-peer systems[15]. The performance
of other emerging applications such as large-scale cooper-
ative gaming, and other collaborative distributed applica-
tions, depends critically on the reliability and scalability of
the membership maintenance protocol used within.
Brieﬂy, a membership protocol provides each process
(“member”) of the group with a locally-maintained list of
other non-faulty processes in the group. The protocol en-
sures that the membership list is updated with changes re-
sulting from new members joining the group, or dropping
out (either voluntarily or through a failure). The member-
ship list is made available to the application either directly
in its address space, or through a callback interface or an
API. The application is free to use the contents of the list as
required, e.g. gossip-based dissemination protocols would
use the list to periodically pick target members for gossip.
The reliability and scalability of a membership sub-
system can be measured via several performance metrics.
Membership changes have to be propagated within the
group quickly after their occurrence. The asynchrony and
unreliability of the underlying network can cause messages
to be lost, leading to false detection of process failures,
since a process that is losing messages is indistinguishable
from one that has failed [10]. This rate of false positives
has to be low. Finally, the protocol needs to be peer-to-peer
(not rely on a central server), and impose low message and
computation loads on the network and processes.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:23:09 UTC from IEEE Xplore.  Restrictions apply. 
Membership protocols have been difﬁcult to scale in
groups with beyond a few dozen processes [11, 16], thus
affecting the performance of applications using them. As
reported in [16], the main symptoms of bad performance at
these group sizes is an increase in either the rate of false fail-
ure detections of processes, or the time to detect a failure.
[12] identiﬁes the quadratic increase in the message load
imposed by such membership protocols as another symp-
tom of the unscalability of traditional protocols for mem-
bership maintenance. An example of an application that
relies heavily on the membership sub-system is the class of
virtually synchronous multicast protocols [3]. Traditional
implementations of this speciﬁcation suffer a drastic reduc-
tion in performance, and partitioning, at beyond a few dozen
members [11].
This paper presents our effort in the SWIM project to
implement a membership sub-system that provides stable
failure detection time, stable rate of false positives and
low message load per group member, thus allowing dis-
tributed applications that use it to scale well. We focus on
a weaker variant of group membership, where membership
lists at different members need not be consistent across the
group at the same (causal) point in time. Stronger guar-
antees could be provided by augmenting the membership
sub-system, e.g. a virtually-synchronous style membership
can be provided through a sequencer process that check-
points the membership list periodically. However, unlike
the weakly consistent problem, strongly consistent speciﬁ-
cations might have fundamental scalability limitations1.
The design of a distributed membership algorithm2 has
traditionally been approached through the technique of
heartbeating. Each process periodically sends out an in-
cremented heartbeat counter to the outside world. Another
process is detected as failed when a heartbeat is not received
from it for some time. However, actual implementations
of heartbeating suffer from scalability limitations. Sending
all heartbeats to a central server leads to hot-spot creation.
Sending heartbeats to all members (through either network
multicast, or gossiping [16]) leads to a message load on the
network and group that grows quadratically with the group
size. Heartbeating along a logical ring [9] suffers from un-
predictability of failure detection time when there are mul-
tiple failures. Unfortunately, as the group size rises, so does
the likelihood of simultaneous multiple failures.
An extended discussion of reasons behind the inherent
unscalability of heartbeat-based membership maintenance
mechanisms can be found in [12]. This paper also proposed
a randomized distributed failure detector protocol based on
members randomly probing each other instead of heartbeat-
1Discussion of this issue is outside the scope of this paper. The reader
is referred to [11].
2A “weakly-consistent” adjective is implicitly assumed, and dropped
henceforth.
ing 3. Mathematical analysis showed that as the group size
is scaled up, the protocol’s properties of (expected) fail-
ure detection time, rate of false positives, and message load
per member, are all independent of the group size. This is
an improvement over all-to-all heartbeating based protocols
that have a linear variation (with group size) of either the
detection time for failures or the network bandwidth usage
at each member (or an increase in the false positive rate).
Our work in this article is motivated by a realization
from the work of [12] that the unscalability of the popular
class of all-to-all heartbeating protocols arises from the im-
plicit decision therein to fuse the two principal functions of
the membership problem speciﬁcation: 1) Membership up-
date Dissemination: propagating membership updates aris-
ing from processes joining, leaving or failing, and 2) Fail-
ure detection: detecting failures of existing members. The
overhead of multicasting heartbeats is eliminated by design-
ing an efﬁcient non-multicast based failure detector, and us-
ing the dissemination component only when a membership
change occurs. The Membership Dissemination component
can be implemented through either hardware multicast or in
infection-style.
While [12] presented a failure detection protocol and an-
alyzed it theoretically, our work in the current paper looks
at incorporating the Membership Dissemination component
in to build a working membership sub-system. In addition,
the resulting protocol is augmented by mechanisms that re-
duce the rate of false positives and give stronger determin-
istic guarantees on failure detection times at individual pro-
cesses.
Our system, called SWIM, provides a membership substrate
that:
(1) imposes a constant message load per group member;
(2) detects a process failure in an (expected) constant time
at some non-faulty process in the group;
(3) provides a deterministic bound (as a function of group
size) on the local time that a non-faulty process takes to de-
tect failure of another process;
(4) propagates membership updates,
including informa-
tion about failures, in infection-style (also gossip-style or
epidemic-style [2, 8]);
the dissemination latency in the
group grows slowly (logarithmically) with the number of
members;
(5) provides a mechanism to reduce the rate of false pos-
itives by “suspecting” a process before “declaring” it as
failed within the group.
While (1) and (2) are properties of the failure detection
protocol of [12], (3)-(5) represent our subsequent work in
the current paper. Experimental results of a prototype im-
plementation of SWIM running on a PC cluster are dis-
cussed. The SWIM protocol can also be extended to work
3In a sense, the protocol monitors the status of members, randomly,
instead of using heartbeating.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:23:09 UTC from IEEE Xplore.  Restrictions apply. 
over a wide area network (WAN) or virtual private network
(VPN), and we touch on this brieﬂy in Section 6.
The rest of the paper is organized as follows. Section 2
summarizes previous work in this area, and the basics of
scalable failure detection protocols from [12]. Section 3
describes the basic SWIM protocol, and Section 4 the im-
provements to the protocol. Experimental results from a
prototype implementation are presented in Section 5. We
conclude in Section 6.
2. Previous Work
In traditional distributed all-to-all heartbeating failure
detection algorithms, every group member periodically
transmits a “heartbeat” message (with an incremented
counter) to all other group members. A member Mi is de-
clared as failed by a non-faulty member Mj when Mj does
not receive heartbeats from Mi for some consecutive heart-
beat periods.
Distributed heartbeating schemes guarantee that a faulty
member is always detected as such at any non-faulty mem-
ber (within a time interval after its failure)4, since a mem-
ber that has crashed also stops sending heartbeat messages.
However, the accuracy and scalability guarantees of these
protocols differ, depending on the actual mechanism used
to disseminate the heartbeats.
2
T
In the simplest implementation, each heartbeat is multi-
casted to all other group members. This results in a a net-
) messages per second (even if IP multi-
work load of θ( n
cast is used), where T is the failure detection time required
by the distributed application. van Renesse et al [16] pro-
posed that heartbeats be disseminated via a robust gossip-
In this protocol, every tgossip time units,
style protocol.
each member gossips, to a few random targets, a θ(n)-sized
list of the latest known heartbeat counters received from
other members. While gossiping reduces the false positive
frequency, a new heartbeat count typically takes, on expec-
tation, θ[log(n) · tgossip] time units to reach an arbitrary
other group member.
In order to satisfy the application-
speciﬁed detection time, the protocol generates a network
load of θ( n
) bytes a second. The use of message
batching to solve this is limited by the UDP packet size
limit, e.g. 5B heartbeats (IP address and count) of 50 mem-
bers would already occupy 250 B, while SWIM generates
packets that have a size of at most 135 B, regardless of the
group size.
.log(n)
tgossip
2
The quadratic increase in the network load results from
the communication of heartbeat notiﬁcation to all group
members. This can be avoided by separating the failure de-
tection operation from that of membership update dissemi-
nation.
4This property is called Strong Completeness.
Several hierarchical membership systems have been pro-
posed, e.g. Congress [1]. This belongs to a broader class of
solutions where each process heartbeats only a subgroup of
processes. This class of protocols requires careful conﬁg-
uration and maintenance of the overlay along which mem-
bership information ﬂows, and the accuracy of the protocol
depends on the robustness of this graph. In comparison, the
design of SWIM avoids the overhead of a virtual graph.
SWIM’s solution to the above unscalability problems de-
scribed above is based on (a) designing the failure detection
and membership update dissemination components sepa-
rately, and (b) using a non-heartbeat based strategy for fail-
ure detection.
Before moving on to describe the SWIM protocol in-
ternals, we ﬁrst lay the foundation for understanding the
key characteristics of the efﬁciency and scalability of dis-
tributed failure detector protocols. Several research stud-
ies [6, 7, 12, 16], have led to the identiﬁcation of these ba-
sic properties of distributed failure detector protocols (from
both theoretical and practical angles), as well as impossi-
bility results related to satisfying them concurrently. The
resulting tradeoff is usually determined by the safety and
liveness properties required by distributed applications.
These properties are [12]:
(1) Strong Completeness: crash-failure of any group mem-
ber is detected by all non-faulty members [6]);
(2) Speed of failure detection: the time interval between a
member failure and its detection by some non-faulty group
member;
(3) Accuracy: the rate of false positives of failure detection;
(4) Network Message Load, in bytes per second generated
by the protocol.
[6] proved the impossibility of building a failure detector
over an asynchronous network that is both accurate (no false
detections) and strongly complete. However, since a typ-
ical distributed application relies on Strong Completeness
always holding (in order to maintain up to date informa-
tion in dynamic groups), most failure detectors, including
heartbeating-based solutions, guarantee this property while
attempting to maintain a low rate of false positives. SWIM
takes the same approach.
In [12], a simple computation identiﬁes the minimal total
network load (bytes per second) required to satisfy speciﬁed
parameters of false detection rate at each member (denoted
PM(T )), and detection time (T ) in a group of size n. [12]
calculates this load as n· log(PM(T ))
, where pml is the prob-
ability of a packet drop within the underlying network.