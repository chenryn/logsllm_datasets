title:Utilizing Dynamically Coupled Cores to Form a Resilient Chip Multiprocessor
author:Christopher LaFrieda and
Engin Ipek and
Jos&apos;e F. Mart&apos;ınez and
Rajit Manohar
Utilizing Dynamically Coupled Cores to Form
a Resilient Chip Multiprocessor
Christopher LaFrieda Engin ˙Ipek Jos´e F. Mart´ınez Rajit Manohar
Computer Systems Laboratory
Cornell University
Ithaca, NY 14853 USA
http://csl.cornell.edu/
Abstract
Aggressive CMOS scaling will make future chip multiproces-
sors (CMPs) increasingly susceptible to transient faults, hard er-
rors, manufacturing defects, and process variations. Existing
fault-tolerant CMP proposals that implement dual modular redun-
dancy (DMR) do so by statically binding pairs of adjacent cores
via dedicated communication channels and buffers. This can re-
sult in unnecessary power and performance losses in cases where
one core is defective (in which case the entire DMR pair must be
disabled), or when cores exhibit different frequency/leakage char-
acteristics due to process variations (in which case the pair runs at
the speed of the slowest core). Static DMR also hinders power den-
sity/thermal management, as DMR pairs running code with simi-
lar power/thermal characteristics are necessarily placed next to
each other on the die.
We present dynamic core coupling (DCC), an architectural
technique that allows arbitrary CMP cores to verify each other’s
execution while requiring no static core binding at design time or
dedicated communication hardware. Our evaluation shows that
the performance overhead of DCC over a CMP without fault tol-
erance is 3% on SPEC2000 benchmarks, and is within 5% for a set
of scalable parallel scientiﬁc and data mining applications with up
to eight threads (16 processors). Our results also show that DCC
has the potential to signiﬁcantly outperform existing static DMR
schemes.
1
Introduction
Aggressive CMOS scaling has permitted exponential increases
in the microprocessor’s transistor budget for the last three decades.
Earlier processor designs successfully translated such transistor
budget increases into performance growth. Nowadays, however,
power and complexity have become unsurmountable obstacles to
traditional monolithic designs. This has turned chip multiproces-
sors (CMPs) into the primary mechanism to deliver performance
growth, by doubling the number of cores and exploiting increasing
levels of thread-level parallelism (TLP) with each new technology
generation. Current industry projections indicate that CMPs will
scale to many tens or even hundreds of cores by 2015 [4]. Unfor-
tunately, this does not mean that CMPs are free of power, tempera-
ture, or even complexity issues. Moreover, other artifacts intrinsic
to deep-submicron technologies render these future “many-core”
CMPs increasingly susceptible to soft errors [15, 21], manufac-
turing defects [6], process variations [3], and early lifetime fail-
ures [27].
One appealing aspect of CMPs is the inherent redundancy of
hardware resources, which can be exploited for error detection and
recovery. Current proposals for DMR-based CMPs statically bind
core pairs at design time and rely on dedicated cross-core com-
munication [7, 24, 29]. This presents important limitations. For
example, when a core fails due to a manufacturing defect or early
lifetime failure, the remaining core in its DMR pair can no longer
be checked for hard or soft errors. This effectively doubles the
number of unavailable cores for fault-tolerant execution.
In the
presence of process variations, functional DMR pairs consisting
of cores with different frequency or leakage characteristics may
have to run at the speed of the slower core, leading to additional
performance degradations. Hardwired DMR also presents limi-
tations to effective power density/thermal management, as DMR
pairs running code with similar power/thermal characteristics are
necessarily placed next to each other on the die.
Instead of relying on a set of rigid, statically deﬁned DMR
pairs, we would like a CMP to provide the ﬂexibility to allow any
core to form a virtual DMR pair with any other core on demand.
We would also like to be able to use additional cores to imple-
ment other desirable features on demand, such as TMR, or activity
migration to spread heat more evenly on the die without compro-
mising fault-tolerant execution. To do this, we propose dynamic
core coupling (DCC), a processor-level fault-tolerance technique
that allows arbitrary CMP cores to verify each other’s execution
while requiring no dedicated cross-core communication channels
or buffers. DCC offers several important advantages. Speciﬁcally,
DCC:
pairs.
tively binding cores that operate at similar speeds.
(cid:129) Degrades half as fast as mechanisms that rely on static DMR
(cid:129) Facilitates the formation of balanced DMR pairs by selec-
(cid:129) Enables low-power fault-tolerant execution by binding low-
(cid:129) Supports existing thermal management techniques based on
activity migration seamlessly, regardless of functional core
count or adjacency.
leakage cores ﬁrst.
(TMR) at no additional cost, using hot spares.
(cid:129) Detects and recovers from both hard and soft errors.
(cid:129) Provides support for on-demand triple modular redundancy
(cid:129) Greatly simpliﬁes output compression circuitry and lowers
compression bandwidth demand by tolerating large check-
pointing intervals that can amortize long compression laten-
cies.
In our evaluation, the performance overhead of DCC over a
CMP without fault tolerance is less than 3% on SPEC2000 bench-
marks, and is within 5% on a set of scalable scientiﬁc and data
mining applications with eight threads (16 cores).
This paper is organized as follows: Section 2 reviews the chal-
lenges created by CMOS scaling in deep sub-micron process tech-
nologies, and explores current fault detection and recovery tech-
niques. Section 3 presents our fault tolerant CMP architecture.
Section 4 discusses the additional modiﬁcations made to the ar-
chitecture to support parallel applications. Section 5 describes the
experimental setup and reports the results. Finally, Section 6 sum-
marizes our conclusions.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20072 Background and Related Work
2.1 Deep Submicron Challenges
CMOS scaling in deep submicron process technologies will
create signiﬁcant problems for future many-core CMP platforms.
In this section, we review some of these challenges and their im-
plications on fault-tolerant CMP design.
Soft Errors The susceptibility of a device to soft errors is in-
versely proportional to the amount of charge in its nodes [21].
With technology scaling, smaller transistors and lower supply volt-
ages decrease the amount of charge on a node, thereby making de-
vices more sensitive to soft errors [5]. The soft error rate (SER) of
combinational logic in a processor is expected to reach 1,000 FIT
(failures in 109 hours) by 2011 [21]. Since storage structures can
be protected relatively easily by parity or error-correcting codes
(ECC), combinational logic is expected to become the dominant
source of soft errors.
Manufacturing Defects and Process Variations Manu-
facturing defects are primarily artifacts of fabrication related fail-
ure mechanisms (e.g., open or short circuits), or process variations
(e.g., excessively leaky cores). During production, burn-in tests
that stress parts under extreme voltage and temperature conditions
are used to accelerate infant mortality and to expose latent fail-
ures. Once identiﬁed, defective cores are disabled and parts are
classiﬁed into bins based on functional core count. Even among
the remaining functional cores that pass burn-in tests, frequency
and leakage power can vary.
Manufacturing defects are already posing serious challenges
to semiconductor manufacturers: IBM recently announced that
many of its nine-core Cell microprocessors will ship with only
eight functional cores due to defects, and the company is consid-
ering whether to ship chips with only seven functional cores [25].
While no industrial data on core-to-core parameter variability or
defect rates in CMPs are available for current generation process
technologies, both problems are expected to become progressively
more signiﬁcant with CMOS scaling.
Early Lifetime Failures Although burn-in tests are an effec-
tive mechanism to expose latent failures and identify defective
cores, testing is by no means perfect; electromigration, stress mi-
gration, time-dependent dielectric breakdown, and thermal cycling
can all lead to intrinsic hard failures [26] after manufacturer burn-
in tests. All of these factors worsen as technology scales. The
resulting effect of these failures is permanent in the sense that the
device is broken and cannot ever be relied upon to produce correct
results. The rate of early lifetime failures for 65 nm technology
has been estimated to be 7,000 FIT to 15,000 FIT[27]. It is dif-
ﬁcult to draw conclusions about the relative rates of soft and hard
errors based on their estimates. However, these projections show
the importance of designing a system tolerant to both soft and hard
errors.
2.2 Fault Tolerance
DCC falls within a class of fault tolerant architectures that use
redundant execution. Redundant execution is a technique that
runs two independent copies of a thread and intermittently com-
pares their results. This technique has become increasing pop-
ular with the recent shift towards more on-chip thread contexts.
DCC is most similar to work that combines redundant execu-
tion with simultaneous multithreading [28] (SMT) or chip mul-
tiprocessor [16] (CMP) architectures. SMT provides additional
thread contexts by allowing multiple threads to use a processor’s
resources simultaneously. CMPs support additional thread con-
texts by simply integrating more processors on-chip.
AR-SMT [19] was one of the ﬁrst proposals to use SMT to
detect transient faults. As instructions retire in a leading thread,
the A-thread, their results are stored in a delay buffer. A trail-
ing thread, the R-thread, re-executes instructions and compares
with results in the delay buffer. SRT [17] builds upon this work
by addressing memory coherence between the leading and trail-
ing threads in hardware. Speciﬁcally, both threads must see the
same inputs from the memory system and produce a single output.
SRTR [29] extends SRT by adding support for recovery. We limit
further discussion of SMT approaches to SRTR.
CRT [14] uses a CMP composed of processors with SMT sup-
port. A leading thread on one processor is checked by a trailing
thread on another processor by forwarding results through a ded-
icated bus. The advantage over SRTR is better permanent fault
coverage as no resources are shared between a leading and trail-
ing thread. CRTR [7] extends CRT by providing recovery from
transient faults. Reunion [23] is a CMP architecture that signif-
icantly reduces result forwarding bandwidth by compressing re-
sults. These signatures are exchanged between statically bound
checking pairs via a dedicated bus. Previous work can be catego-
rized by the following: i) synchronization, ii) input replication, iii)
output comparison, and iv) recovery.
Synchronization. To compare results in redundant execution,
either both threads must be synchronized, or a trailing thread must
check the results of all committed instructions against a sequence
of forwarded results. It is common in commercial systems, such
as The Tandem Himalaya [12] and Stratus [20], to use lockstep
execution. The IBM G5/G6/z990 [22, 13] uses replicated fetch,
decode and execution units running in lockstep. Lockstep means
that each processor executes the same instructions in a given cy-
cle. Lockstep is hard to achieve in SMT and CMP because of
contention for shared resources. As a result, SRTR and CRTR
maintain a slack between leader and trailer threads using simple
queues. The leader thread forwards its results to these queues.
The trailing thread reads results from these queues as it issues
instructions, thereby aligning trailing instruction execution with
leading instruction results. Reunion exchanges compressed results
between threads at approximately every ﬁfty instructions. The lag
between leading and trailing threads is limited by the number of
unveriﬁed stores that can be buffered. In SRTR and CRTR, stores
are buffered in dedicated queues. In Reunion, stores are buffered
in a speculative portion of the store buffer. The maximum lag be-
tween threads for these two methods is on the order of a hundred
instructions.
Input Replication. There is a potential problem if a trailing
thread redundantly executes a load instruction. An intervening
store, possibly from a separate thread, may have updated that
memory address between the time the leading thread executes the
load and the trailing thread executes the load. To remedy this,
SRTR and CRTR support input replication via a load value queue
(LVQ). When the leader thread executes a load instruction, it for-
wards the result to the trailing thread’s LVQ. The trailing thread
reads load values from the LVQ rather than going to memory. Re-
union, on the other hand, allows both threads to independently
execute load instructions. In the case of an intervening store, Re-
union rolls back its state to a checkpoint and executes both threads
in a single-step mode until the ﬁrst memory instruction. The au-
thors refer to this as relaxed input replication.
Output Comparison. Faults are detected by comparing the
state (register values and stores) of each thread in the redundant
execution. In SRTR and CRTR, the leading thread forwards re-
sults to the trailing thread’s register value queue (RVQ) and store
buffer (StB). The trailing thread compares its results with the lead-
ing thread’s results before committing results. In this fashion, the
trailing thread’s state is always fault-free. SRTR reduces the band-
width requirements of the RVQ by only checking register results
for instructions at the end of a dependence chain. Reunion greatly
reduces the overhead of communicating results between cores by
using Fingerprinting [24]. Fingerprinting uses a CRC-16 compres-
sion circuit to compress all the new state generated each cycle into
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Figure 1. An eight-core
statically coupled CMP
with two failures. Only
two threads can be exe-
cuted reliably.
Figure 2. An eight-
core dynamically coupled
CMP with two failures.
Three threads can be exe-
cuted reliably.
a single 16-bit signature. Comparing these signatures is equivalent
to comparing the results of all executed instructions. The probabil-
ity of undetected error using this technique is very small, roughly
2−16.
Recovery.
In SRTR and CRTR, a fault-free state can be recov-
ered by copying the committed state of the trailing thread, which is
guaranteed to be fault-free, to the leading thread. This backward-
error recovery (BER) technique provides recovery for transient
faults, but cannot be used to recover from permanent faults. Re-
union checkpoints the architectural state of each core before they
exchange ﬁngerprints. When there is a mismatch between ﬁnger-
prints, the speculative state is squashed and the last checkpoint
is restored. The IBM G5/G6 can additionally recover from some
permanent faults by copying processor state to a spare processor.
There are two major architectural distinctions between previ-
ous work and DCC. First, DCC can recover from permanent faults
without the need for constant TMR, like in Tandem and Stratus
architectures, or the need for dedicated spares, like in the IBM
G5/G6. DCC uses a novel on-demand TMR scheme which only
employs TMR during permanent fault recovery. When not re-
covering from a permanent fault, all processors are conﬁgured as
DMR pairs and performing computation. Second, DMR pairs in
DCC are dynamically assigned. One advantage of dynamic cou-
pling is that a faulty core does not disable both cores of a DMR
pair because a working core has the ﬂexibility to form a DMR pair
with any other working core. Accommodating dynamic coupling
requires architectural extensions over previous approaches. These
extensions are described in the following sections.
3 DCC Mechanism
3.1 Architecture Overview
DCC dynamically couples cores by performing all communi-
cation between redundant threads over the system bus of a shared
memory CMP. In statically coupled CMPs[7, 14, 23], communica-
tion between redundant threads is conducted over additional dedi-
cated buses. Dynamic coupling provides the following beneﬁts: i)
the system degrades at half the rate of a statically coupled CMP in
the presence of permanent faults; ii) when considering variation,
cores with similar characteristics can be paired together; and iii)
hot spots can be minimized by running high IPC applications re-
dundantly on distant cores. For example, consider the statically
coupled CMP in Figure 1 and the dynamically coupled CMP in
Figure 2. With two permanent failures, the statically coupled CMP
cannot utilize any of the upper four cores for redundant execution
because the two working cores can only forward results to the two
broken cores. However, the dynamically coupled CMP has the
ﬂexibility to pair the two working cores and use them to execute
an additional reliable thread. In addition, the dynamically coupled
CMP issues the high IPC thread A on distant cores to reduce hot
spots.
)
%
(
e
s
a
e
r
c
n
I
c
i
f
f
a
r
T
s
u
B
m
e
t
s
y
S
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 0