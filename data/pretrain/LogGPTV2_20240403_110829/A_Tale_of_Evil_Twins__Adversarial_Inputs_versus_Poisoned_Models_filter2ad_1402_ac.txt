5
6 return (ğ‘¥ (ğ‘˜), ğœƒ (ğ‘˜));
Algorithm 1 sketches the complete procedure. By alternating be-
tween input and model perturbation, it finds approximately optimal
adversarial input ğ‘¥âˆ— and poisoned model ğœƒâˆ—. Note that designed to
study the interactions of adversarial inputs and poisoned models
(Â§ 4), Algorithm 1 is only one possible implementation of Eqn (4)
under the setting of a single target input and both input and model
perturbation. To implement other attack variants, one can adjust
Algorithm 1 accordingly (Â§ 5). Also note that it is possible to per-
form multiple input (or model) updates per model (or input) update
to accommodate their different convergence rates.
3.2.3 Analysis. Next we provide analytical justification for Algo-
rithm 1. As Eqn (5) is effectively equivalent to Eqn (4), Algorithm 1
approximately solves Eqn (5) by alternating between (i) input pertur-
bation â€“ searching for ğ‘¥âˆ— = arg minğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(ğ‘¥; ğœƒâˆ—) and (ii) model
perturbation â€“ searching for ğœƒâˆ— = arg minğœƒ âˆˆFğ›¿ (ğœƒâ—¦) â„“(ğ‘¥âˆ—; ğœƒ). We now
show that this implementation effectively solves Eqn (5) (proof
deferred to Appendix A).
Proposition 1. Let ğ‘¥âˆ— âˆˆ Fğœ–(ğ‘¥â—¦) be a minimizer of the function
minğ‘¥ â„“(ğ‘¥; ğœƒ). If ğ‘¥âˆ— is non-zero, then âˆ‡ğœƒ â„“(ğ‘¥âˆ—; ğœƒ) is a proper descent
direction for the objective function of minğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(ğ‘¥; ğœƒ).
Thus, we can conclude that Algorithm 1 is an effective implemen-
tation of the IMC attack framework. It is observed in our empirical
evaluation that Algorithm 1 typically converges within less than
20 iterations (details in Â§ 4).
4 MUTUAL REINFORCEMENT EFFECTS
Next we study the dynamic interactions between adversarial inputs
and poisoned models. With both empirical and analytical evidence,
we reveal that there exist intricate â€œmutual reinforcementâ€ effects
between the two attack vectors: (i) leverage effect â€“ with fixed attack
efficacy, at slight cost of one metric (i.e., fidelity or specificity), one
can disproportionally improve the other metric; (ii) amplification
effect â€“ with one metric fixed, at minimal cost of the other, one can
greatly boost the attack efficacy.
4.1 Study Setting
Datasets. To factor out the influence of specific datasets, we pri-
marily use 4 benchmark datasets:
â€¢ CIFAR10 [27] â€“ It consists of 32 Ã— 32 color images drawn from 10
classes (e.g., â€˜airplaneâ€™);
â€¢ Mini-ImageNet â€“ It is a subset of the ImageNet dataset [14], which
consists of 224 Ã— 224 (center-cropped) color images drawn from
20 classes (e.g., â€˜dogâ€™);
â€¢ ISIC [16] â€“ It represents the skin cancer screening task from the
ISIC 2018 challenge, in which given 600 Ã— 450 skin lesion images
are categorized into a 7-disease taxonomy (e.g., â€˜melanomaâ€™);
â€¢ GTSRB [46] â€“ It consists of color images of size ranging from
29 Ã— 30 to 144 Ã— 48, each representing one of 43 traffic signs.
Note that among these datasets, ISIC and GTSRB in particular
represent security-sensitive tasks (i.e., skin cancer screening [16]
and traffic sign recognition [4]).
DNNs. We apply ResNet18 [23] to CIFAR10, GTSRB and ImageNet
and ResNet101 to ISIC as the reference DNN models. Their top-1
accuracy on the testset of each dataset is summarized in Table 1.
Using two distinct DNNs, we intend to factor out the influence of
individual DNN characteristics (e.g., network capacity).
CIFAR10
ResNet18
95.23%
ImageNet
ResNet18
94.56%
Model
Accuracy
Table 1. Accuracy of benign DNNs on reference datasets.
ResNet101
88.18%
ISIC
GTSRB
ResNet18
99.12%
Attacks. Besides the IMC attack in Â§ 3.2, we also implement two
variants of IMC (with the same hyper-parameter setting) for com-
parison: (i) input perturbation only, in which IMC is instantiated
as the adversarial attack (i.e., PGD [34]), and (ii) model perturba-
tion only, in which IMC is instantiated as the poisoning attack. The
implementation details are deferred to Appendix B.
Measures. We quantify the attack objectives as follows.
Efficacy â€“ We measure the attack efficacy by the misclassification
confidence, ğ‘“ğ‘¡ (ğ‘¥âˆ—; ğœƒâˆ—), which is the probability that the adversarial
input ğ‘¥âˆ— belongs to the target class ğ‘¡ as predicted by the poisoned
model ğœƒâˆ—. We consider the attack successful if the misclassification
confidence exceeds a threshold ğœ….
Fidelity â€“ We measure the fidelity loss by the ğ¿ğ‘-norm of the
input perturbation â„“f(ğ‘¥âˆ—) â‰œ âˆ¥ğ‘¥âˆ— âˆ’ ğ‘¥â—¦âˆ¥ğ‘. Following previous work
on adversarial attacks [7, 20, 34], we use ğ‘ = âˆ by default in the
following evaluation.
Specificity â€“ Further, we measure the specificity loss using the
difference of the benign and poisoned models on classifying a ref-
erence set R. Let Iğ‘§ be the indicator function that returns 1 if ğ‘§ is
true and 0 otherwise. The specificity loss can be defined as:
Iğ‘“ (ğ‘¥;ğœƒâ—¦)â‰ ğ‘“ (ğ‘¥;ğœƒâˆ—)
|R|
(9)
â„“s(ğœƒâˆ—) â‰œâˆ‘ï¸
ğ‘¥âˆˆR
With fixed attack efficacy ğœ…, let (ğ‘¥âˆ—, ğœƒâˆ—) be the adversarial input
and poisoned model generated by IMC, and Â¯ğ‘¥âˆ— and Â¯ğœƒâˆ— be the ad-
versarial input and poisoned model given by the adversarial and
poisoning attacks respectively. Because the adversarial and poison-
ing attacks are special variants of IMC, we have ğ‘¥âˆ— = Â¯ğ‘¥âˆ— if ğœƒâˆ— = ğœƒâ—¦
and ğœƒâˆ— = Â¯ğœƒâˆ— if ğ‘¥âˆ— = ğ‘¥â—¦. Thus, in the following, we normalize the
fidelity and specificity losses as â„“f(ğ‘¥âˆ—)/â„“f( Â¯ğ‘¥âˆ—) and â„“s(ğœƒâˆ—)/â„“s( Â¯ğœƒâˆ—) re-
spectively, both of which are bound to [0, 1]. For reference, the
concrete specificity losses â„“s( Â¯ğœƒâˆ—) (average accuracy drop) caused by
the poisoning attack on each dataset are summarized in Table 2.
Figure 4: Disproportionate trade-off between attack fidelity and specificity.
ğœ…
0.75
0.9
CIFAR10
0.11%
0.12%
ImageNet
2.44%
3.83%
ISIC
1.53%
1.62%
GTSRB
0.25%
0.27%
Table 2. Specificity losses (average accuracy drop) caused by poison-
ing attacks on reference datasets.
4.2 Effect I: Leverage Effect
In the first set of experiments, we show that for fixed attack effi-
cacy, with disproportionally small cost of fidelity, it is feasible to
significantly improve the attack specificity, and vice versa.
4.2.1 Disproportionate Trade-off. For each dataset, we apply
the adversarial, poisoning, and IMC attacks against 1,000 inputs
randomly sampled from the testset (as the target set T ), and use the
rest as the reference set R to measure the specificity loss. For each
input of T , we randomly select its target class and fix the required
attack efficacy (i.e., misclassification confidence ğœ…). By varying IMCâ€™s
hyper-parameters ğœ† and ğœˆ, we control the importance of fidelity
and specificity. We then measure the fidelity and specificity losses
for all the successful cases. Figure 4 illustrates how IMC balances
fidelity and specificity. Across all the datasets and models, we have
the following observations.
First, with fixed attack efficacy (i.e., ğœ… = 0.9), by sacrificing dispro-
portionally small fidelity (i.e., input perturbation magnitude), IMC
significantly improves the attack specificity (i.e., accuracy drop on
non-target inputs), compared with required by the corresponding
poisoning attack. For instance, in the case of GTSRB (Figure 4 (d)),
as the fidelity loss increases from 0 to 0.05, the specificity loss is
reduced by more than 0.48.
Second, this effect is symmetric: a slight increase of specificity
loss also leads to significant fidelity improvement, compared with
required by the corresponding adversarial attack. For instance, in
the case of CIFAR10 (Figure 4 (a)), as the specificity loss increases
from 0 to 0.1, the specificity loss drops by 0.37.
Third, higher attack efficacy constraint brings more fidelity-
specificity trade-off. Observe that across all datasets, GTSRB shows
significantly larger curvature, which might be explained by the
higher model accuracy(99.12%) and larger number of classes(43).
Leverage Effect
There exists an intricate fidelity-specificity trade-off. At
disproportionally small cost of fidelity, it is possible to sig-
nificantly improve specificity, and vice versa.
4.2.2 Empirical Implications. The leverage effect has profound
implications. We show that it entails a large design spectrum for the
adversary to optimize the attack evasiveness with respect to various
detection methods (detectors). Note that here we do not consider
the adversaryâ€™s adaptiveness to specific detectors but rather focus
on exposing the design spectrum enabled from a detection perspec-
tive. In Â§ 5, we show that IMC also allows to enhance the attacks
by adapting to specific detectors. To assess IMCâ€™s evasiveness, we
consider three complementary detectors.
Input Anomaly â€“ From the input anomaly perspective, we apply
manifold transformation [35] as the detector. At a high level, it em-
ploys a reformer network to project given inputs to the manifold
spanned by benign inputs and a detector network to differenti-
ate benign and adversarial inputs. Besides, we apply randomized
smoothing [11] as another detector, which transforms a given DNN
into a â€œsmoothedâ€ model and considers a given input ğ‘¥âˆ— as adver-
sarial if the probability difference of ğ‘¥âˆ—â€™s largest and second largest
classes exceeds a threshold.
Model Anomaly â€“ From the model anomaly perspective, we apply
curvature profiling [38] as the detector. Recall that the poisoning
attack twists the classification boundary surrounding the target
input ğ‘¥âˆ—; thus, the loss function tends to change abruptly in ğ‘¥âˆ—â€™s
vicinity. To quantify this property, we compute the eigenvalues of
the Hessian ğ»ğ‘¥(ğ‘¥âˆ—) = âˆ‡2
ğ‘¥ â„“(ğ‘¥âˆ—). Intuitively, larger (absolute) eigen-
values indicate larger curvatures of the loss function. We define
the average (absolute) value of the top-ğ‘˜ eigenvalues of ğ»ğ‘¥(ğ‘¥âˆ—) as
ğ‘–=1 |ğœ†ğ‘–(ğ»ğ‘¥(ğ‘¥âˆ—))|, where ğœ†ğ‘–(ğ‘€) is the ğ‘–-th
ğ‘¥âˆ—â€™s curvature profile: 1
ğ‘˜
eigenvalue of matrix ğ‘€ (details in Appendix B). We compare the
curvature profiles of given inputs and benign ones, and use the
Kolmogorovâ€“Smirnov statistics to differentiate the two sets.
ğ‘˜
We apply the above detectors to the adversarial inputs and poi-
soned models generated by IMC under varying fidelity-specificity
trade-off (ğœ… fixed as 0.75). Figure 5 measures the detection rates for
different datasets. We have the following observations.
The detection rate of input anomaly grows monotonically with
the fidelity loss (i.e., input perturbation magnitude); on the contrary,
the detection rate of model anomaly drops quickly with the fidelity
loss (i.e., disproportionate specificity improvement due to the lever-
age effect). For instance, in the case of ImageNet (Figure 5 (b)), as
the fidelity loss varies from 0 to 0.35, the detection rate of input
anomaly increases from 0.17 to 0.53 by manifold transformation
and from 0.16 to 0.47 by randomized smoothing, while the detection
rate of corresponding model anomaly drops from 0.63 to 0.44.
Moreover, across all the cases, IMC is able to balance fidelity and
specificity, leading to high evasiveness with respect to multiple
detectors simultaneously. For instance, in the case of CIFAR10 (Fig-
ure 5 (a)), with the fidelity loss set as 0.23, the detection rates of
Îº = 0.75Îº = 0.90(a) CIFAR10Poisoning AttackAdversarial AttackSpecificity Loss1.00.80.60.40.20.00.00.20.40.60.81.0(b) ImageNet0.00.20.40.60.81.0(c) ISIC0.00.20.40.60.81.0(d) GTSRB0.00.20.40.60.81.0Fidelity LossFigure 5: Detection rates of input anomaly (by manifold projection [35]) and model anomaly (by curvature profile [38]).
Figure 6: Average misclassification confidence (ğœ…) as a function of fidelity and specificity losses.
iteratively updates it with adversarial inputs that deceive its current
configuration (i.e., adversarial â€œre-trainingâ€).
Dataset
CIFAR10
ImageNet
ISIC
GTSRB
Maximum Perturbation
PGD
3 Ã— 10âˆ’2
4 Ã— 10âˆ’3
3 Ã— 10âˆ’2
3 Ã— 10âˆ’2
IMC
2 Ã— 10âˆ’3
1 Ã— 10âˆ’3
1 Ã— 10âˆ’3
3 Ã— 10âˆ’2