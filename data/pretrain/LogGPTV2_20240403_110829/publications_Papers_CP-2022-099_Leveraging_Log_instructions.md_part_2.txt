type of intent expressed with the n-grams. To investigate the learning framework. The deep learning framework’s goal is
expressedeventintent,wemadeann-gramsentimentanalysis to learn and output useful log representations for the target-
(where the sentiment is used to quantify the intent type, i.e., system logs. It does so by training a deep neural network
positive or negative), given in the following. model with a sequential two-phase learning process (pretrain-
ing and finetuning), during which data from the target-system
C. Log Instructions Static Texts Sentiment Analysis
logsandtheSLdataareused.Theanomalydetectordetectsif
Toevaluatethen-gramsentimentconcerningthetwosever- the input target-system logs are normal or anomalous. In the
ity groups, we considered a pretrained sentiment analysis following, we describe the three components of ADLILog.
TABLEII
LOGINSTRUCTIONSSTATICTEXTSSENTIMENTANALYSISRESULTS
Sentiment Positive Negative Neutral
Severity Group Normal Abnormal Shared Normal Abnormal Shared Normal Abnormal Shared
N-gram Coverage [%] 66.94% 28.13% 4.93% 23.13% 69.75% 7.12% 46.98% 43.43% 9.59%
A. Log Preprocessing on the training phase (pretraining or finetuning), the [LME]
vector proceeds towards one of the two classification layers.
The raw target-system logs are characterized by high
Theoutputfromtheclassificationlayersisusedasinputinthe
noise due to the parameter values generated during system
appropriatelossfunction.Afterfinetuning,theoutputfromthe
runtime (e.g., IP address, endpoints, numerical parameters).
secondsetofclassificationlayersisthefinalvectorembedding
The log noise can significantly affect the anomaly detection
oftheinputlog,whichproceedstowardstheanomalydetector.
performance [6]. Therefore, the log preprocessing aims to
reduce the noise by applying a set of preprocessing steps. 1) Embedding Layer: The embedding layer receives the
To that end, we start by removing all path endpoints (e.g., preprocessed logs as input. It serves as an interface between
/home/spelce1/HPCCIBM/bin/) and split the static text using the textual and numerical token representation format. Specif-
whitespaces into singleton items we call tokens. The tokens ically, each token is assigned a single index corresponding
with numeric values most often denote variable parameters to a token embedding vector. The embeddings are learned
thatarenotrelevantforthesemanticsofthelogs.Weconsider during pretraining and are adjusted to learn the properties of
them as noise and remove them. Similar to the preprocessing the normal and abnormal events. The embeddings are learned
fortheSLdata,weapplySpacyandremoveallASCIIspecial jointly with the parameters of the neural network. Notably,
characters(e.g.,$),thestopwords(e.g.,isandthe)[16]and the embedding layer is updated just during the pretraining
transformedeachcharacterintoalowercase,followingrelated phase. Updating the embedding layer during the two phases
work [15]. Notably, as previously described, the SL data is is challenged by the appearance of unseen words during
already preprocessed by a similar set of operations making finetuning. For example, there may be some operations in the
the preprocessing uniform. In addition, each log is prepended targetsystemthatarenotcoveredbytheSLdata,whichleads
with a dedicated Log Message Embedding ([LME]) token. to the appearance of missing tokens. The joint training with
The [LME] token is an important design detail because we the new tokens requires an update of the internal structure of
use it to extract a numerical representation of the log from the neural network and learning new parameters every time
the neural network, further given as input to the anomaly we encounter new words. Therefore, the effective transfer of
detector. An important advantage of our method over the theparametersbetweenthetwolearningphasesischallenged.
related work is that ADLILog does not depend on log parsing To address this issue, we introduce a special token referred
(a preprocessing procedure that extracts event templates from to as an unknown ([UNK]) token. Notably, during pretraining
raw logs) [19]. Since the existent log parsers are imperfect, (first training phase), we randomly sample 15% of the SL
the incorrect parsing adds additional noise and can degrade data and in each sample, we replace 20% of the tokens
the anomaly detection performance [11]. By directly learning with [UNK] (a similar strategy is used in related works from
features from the raw logs, we eliminate this source of errors. generallanguage[21]).Therefore,thepretrainedmodellearns
Finally, different logs can have a variable number of tokens contextswithmissingtokens.Duringfinetuning,wheneverwe
while the neural network requires fixed-size input. Therefore, encounteranewtokenfromthetarget-systemdata,wereplace
we specify a hyperparameter max len to unify the lengths. it with the [UNK], effectively handling the new tokens.
Theshorterlogsareappendedwithaspecialpadtoken([PD]), 2) Log Message Encoder: As a suitable architecture for
while the longer ones are truncated at max len size. the log message encoder, we identified the encoder of the
Transformer[20]architecture.Thisarchitectureprovidesstate-
B. Deep Learning Framework
of-the-art results in many NLP tasks (e.g., sentiment analysis,
Thedeeplearningframeworkconsistsofthreecomponents: translation) [21]. By pointing to the similarities between the
1) embedding layer, 2) encoder network from Transformer log static texts and natural language [17], we justify our
architecture[20]and3)classificationlayers.Giventheprepro- design of choice. The encoder implements a multi-head self-
cessed and tokenized logs at the input, the embedding layer attentionmechanismthatexploitstherelationsbetweentokens
transforms the input tokens into numerical vector representa- within the log instructions static texts. This property enables
tions,whichwerefertoasvectortokenembeddings.Thetoken learning discriminative features between the words and the
embeddings are numerical features represented in a suitable different contexts they appear in (e.g., diverse vocabularies,
format for the neural network. We then use the encoder intent).Theembeddingvectorsandtheencoderparametersare
network to learn relationships between the vector embeddings updatedviathebackpropagationalgorithmduringpretraining.
from the embedding layer and the appropriate target. The Attheoutputoftheencoder,weprovidethevectorembedding
output from the encoder layer is the vector embedding of of the [LME] token. Due to the architectural design, the
the input log/(static text), i.e., the [LME] vector. Depending vector of the [LME] token attends over all the other token
SL Data ADLILog Information Flow:
𝑦 𝑖=0 Log Preprocessing Deep Learning Framework Pretraining
Offline
𝑙 𝑘:“connection established“ Emb Lae yd ed ring Classifi Lc aa yti eo rn DFi en te et cu tn ioin ng
[LME] s Online
𝑙 𝑟:“conne𝑦 c𝑖 ti= on1 refused“ c eo stn an be lic st hio en [0{c .5o 2n ,n …e c ,t 0i .o 2n 2: Anomaly De Nt oe rc mto alr Log
d ] ,
established : Anomalous Log
Target System [PD] [0. m12 a, c … h i, n0 e.5 :6 ], S Mh ea sr se ad g L eo g Set 1𝜃′ 𝑎෤Threshold
Data Target System + [LME] [0.37, … ,0.11], Encoder𝜃
Target System Training Data machine [0in .8t 9e ,r r …u p ,0te .4d 5: ], 𝒙 𝒊 𝑝෪+ 𝒙 𝒊 <𝑎෤
Data interrupt …}
Target System [PD] Set 2 𝜃" Anomaly Normal
Test Data
Fig.1. ADLILog:Detaileddesignoftheloganomalydetectionmethod
vectors during training. We considered this implementation data in this phase consists of the target-system data and the
architectural design detail because it allows learning the most ”abnormal” severity group from the SL data (as its subset).
relevant information from the input concerning the normal Sincebydefinitionoftheanomalydetectiontask,themajority
and abnormal events. The model size, number of heads in ofthetarget-systemdataisassumedtodescribenormalsystem
the encoder, and the number of encoder layers are three behaviour(i.e.,class0)byconsideringthe”abnormal”classof
hyperparameters of the log message encoder. the SL data as anomalous (i.e., class 1), the finetuning can be
3) Classification Layers: The classification layers as input addressed as a binary classification problem. The ”abnormal”
receive [LME] tokens from the encoder. It is composed of class of the SL data is always available, thereby, ADLILog
twosetsoflinearneurallayers.AsdepictedinFig.1,thefirst does not need manually labeled target-system data, i.e., its
layer set (Set 1) has two linear layers, with parameters θ(cid:48). unsupervised method. Consequently, ADLILog addresses the
It is trained jointly with the log message encoder during the challenge of time-expensive labeling. In the finetuning phase,
pretraining procedure. The size of the first linear layer (from weupdatejusttheparametersofthesecondsetoflinearlayers
the first set of linear layers) is equal to the model size of (θ”), while the pretrained model is used to extract the log
the encoder layer, while the second layer (from the first set of embeddings of the input data. The finetuning enables learning
linearlayers)hastwoneuronsthatcorrespondtothe”normal” the specifics of the target-system data while relying on the
and ”abnormal” severity groups from the SL data. The output anomaly-related information from the SL data. In addition,
of the first set of classification layers is given towards the since the normal target-system data and the normal events
binary cross-entropy as a loss function during pretraining. from the SL data can differ, the finetuning adjusts the log
The second set of classification layers, with parameters θ”, representation embeddings to these differences.
has two linear layers (Set 2 in Fig. 1). The two layers have
the same number of neurons equal to the model size. The
output of the second set of linear layers is given as input for Another important aspect of the finetuning phase is the
thelossfunctionduringfinetuning.Additionally,theoutputof choice of the finetuning loss. It determines the form of the
this layer is used as the final log representation, proceeded as final learned log vector embeddings. Since the finetuning
input to the anomaly detector. is defined as a binary classification problem, multiple loss
4) Learning Process: The learning process is split into choicesarepossible(e.g.,binarycross-entropy[22],orhyper-
two sequential phases: pretraining and finetuning. During the spherical loss [23]). The binary-cross entropy is a formidable
pretrainingphase,weupdatetheparametersoftheembedding choice if the anomalous labels originate from target-system
layer,thelogmessageencoderandthefirstsetofclassification data because it allows learning of the exact discriminative
layers. We perform the pretraining with the SL data, using properties between the classes. However, in the case of logs,
the binary cross-entropy as a commonly used loss for binary the expensive labeling process makes this assumption hard.
classification [22]. The pretraining is terminated when a lack In contrast, hyperspherical loss concentrates the normal class
oflossimprovementisobservedforfiveconsecutiveepochson around a single point, e.g., the centre of the hypersphere.
a separate validation SL set. After pretraining, the parameters At the same time, it is scattering the anomalous logs further
of the encoder and the embedding layer (as pretrained model) apart. This is known as the concentration property [24]. The
are frozen and no longer updated. Thereby, they preserve literature on anomaly detection [24] suggests that preserving
anomaly-related information. The pretrained model is used to this property when learning representations often improves
extract the initial log representation in the finetuning phase. performance. Consequently, the hyperspherical loss has more
For the finetuning phase, we pair the pretrained model desirable properties for anomaly detection, and we use it as
with the second set of linear layers. Notably, the training finetuning loss. Eq. 1 gives its definition for a single log l :
i
TABLEIII
DATASETPROPERTIES
Li =(1−y)||g(x;θ,θ”)||2−ylog(1−exp(−||g(x;θ,θ”)||2)) (1)
ad i i i i Dataset Time Span # Logs # Anomalies
wherex isthelogrepresentationasoutputfromthesecond HDFS 38.7 hours 11,175,629 16,838
i
classification layers set, y ∈ {0,1} is a label for the normal BGL 7 months 4,747,963 348,460
i
target-system data or the ”abnormal” SL severity class, θ and
θ” are parameters of the encoder and the second set of linear
layers,andg(x ;θ,θ”)isthefunctionlearnedbythenetwork. A. Experimental Design
i
We evaluate the anomaly detection performance in two
C. Anomaly Detector
separate evaluation scenarios (1) single log line and (2) se-
Thegoaloftheanomalydetectoristohighlighttheanoma- quential log anomaly detection. The advantage of the single
lous target-system logs represented as log vector embeddings lineanomalydetectionresidesinthepotentialtofastdiagnose
(x ). It has two components, i.e., 1) an assumed target-system anomaliesbecausethemethoddirectlypointstothepotentially
i
normality function p˜+, and 2) anomaly decision rule. The anomalous log. However, the large volume of logs in short
ad
normality function is an assumed model of the normal target- time intervals can lead to bursts of reported anomalies which
system logs. It is a positive function, having small values for in certain situations can be overwhelming. To that end, we
the anomalous and large values for the normal target-system evaluatethemethod’sperformanceoneventsequencesaswell.
logs [24]. The form of the function depends on the type of 1) Datasets: BGL and HDFS are two benchmark datasets
finetuning loss. Since the chosen hyperspherical loss learns a for log-based anomaly detection that are mostly used by the
model that places the normal logs (class 0) close to the centre research community [6], [7], [11]. TABLE III shows the
ofthehypersphere,thesmallerdistancescorrespondtonormal key datasets properties. To find the unique log events, we
system behaviour. Following the definition of the normality usedDrain[25],astate-of-the-artlogparsingmethod.Drain’s
function,weusethereciprocalvalueoftheEuclideandistance hyperparameters were set as recommended by Zhu et al. [19]
betweenthelearnedlogrepresentationx andthehypersphere resulting in an output of 29 and 360 unique events for HDFS
i
centre (set to the origin), given by Eq. 2. The large distances and BGL, respectively. Following He, et al. [6] we split the
ofthevectorrepresentationfromthecentreofthehypersphere dataset into 80-20% train-test split. The first, chronologically
will result in small values for the normality score (denoting ordered80%wereusedfortraining(andmodeltuning),while
anomalies) and vice versa (as seen in Fig. 1). the remaining 20% were used for performance evaluation for
the two datasets accordingly.
p˜+(x )= 1 , c=0 (2) HDFS contains 11,175,629 logs generated from a map-
ad i ||x −c||2 reduce tasks on more than 200 Amazon’s EC2 nodes [10].
i
Each log has a unique identifier (block id) for each operation
Finally, to detect anomalies, we apply a decision rule on
such as allocation, writing, replication and deletion. After
top of the normality function score values of the input logs.
parsing, there are 29 unique events, from which ten describe
The decision rule involves setting a decision threshold a˜ over