techniques on twitter,” in Empirical Methods in Natural Language
Processing (EMNLP), 2015.
[43] A. Wilson and A. D. Ker, “Avoiding detection on twitter: embedding
strategies for linguistic steganography,” Electronic Imaging, vol. 2016,
no. 8, pp. 1–9, 2016.
[44] M. H. Shirali-Shahreza and M. Shirali-Shahreza, “A new synonym text
steganography,” in the IEEE International Conference on Intelligent
Information Hiding and Multimedia Signal Processing, 2008.
[45] T. Fang, M. Jaggi, and K. Argyraki, “Generating steganographic text
with lstms,” in the 55th Annual Meeting of the Association for Compu-
tational Linguistics-Student Research Workshop, 2017.
[46] Z. Li, C. Hu, Y. Zhang, and S. Guo, “How to prove your model belongs
to you: a blind-watermark based framework to protect
intellectual
property of dnn,” in the 35th Annual Computer Security Applications
Conference (ACSAC), 2019.
[47] N. Lukas, Y. Zhang, and F. Kerschbaum, “Deep neural network
ﬁngerprinting by conferrable adversarial examples,” arXiv preprint
arXiv:1912.00888, 2019.
[48] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning
your weakness into a strength: Watermarking deep neural networks by
backdooring,” in 27th USENIX Security Symposium (USENIX Security
18), 2018.
[49] E. Le Merrer, P. Perez, and G. Tr´edan, “Adversarial frontier stitching
for remote neural network watermarking,” Neural Computing and Ap-
plications, vol. 32, no. 13, pp. 9233–9244, 2020.
[50] Y. Uchida, Y. Nagai, S. Sakazawa, and S. Satoh, “Embedding wa-
termarks into deep neural networks,” in International Conference on
Multimedia Retrieval (ICMR), 2017.
[51] H. Chen, B. D. Rouhani, C. Fu, J. Zhao, and F. Koushanfar, “Deepmarks:
A secure ﬁngerprinting framework for digital rights management of deep
learning models,” in International Conference on Multimedia Retrieval
(ICMR), 2019.
[52] B. Darvish Rouhani, H. Chen, and F. Koushanfar, “Deepsigns: an end-
to-end watermarking framework for ownership protection of deep neural
networks,” in the 24th International Conference on Architectural Support
for Programming Languages and Operating Systems, 2019.
[53] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnera-
bilities in the machine learning model supply chain,” arXiv preprint
arXiv:1708.06733, 2017.
[54] J. Zhang, Z. Gu, J. Jang, H. Wu, M. P. Stoecklin, H. Huang, and
I. Molloy, “Protecting intellectual property of deep neural networks
with watermarking,” in the ACM Asia Conference on Computer and
Communications Security (AsiaCCS), 2018.
[55] H. Jia, C. A. Choquette-Choo, and N. Papernot, “Entangled watermarks
as a defense against model extraction,” arXiv preprint arXiv:2002.12200,
2020.
[56] H. Li, E. Wenger, B. Y. Zhao, and H. Zheng, “Piracy resistant wa-
termarks for deep neural networks,” arXiv preprint arXiv:1910.01226,
2019.
[57] N. Yu, L. S. Davis, and M. Fritz, “Attributing fake images to gans:
Learning and analyzing gan ﬁngerprints,” in the IEEE International
Conference on Computer Vision (ICCV), 2019.
[58] S.-Y. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros, “Cnn-
generated images are surprisingly easy to spot... for now,” in the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
[59] N. Carlini and H. Farid, “Evading deepfake-image detectors with white-
and black-box attacks,” in the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, 2020.
[60] B. Zhang, J. P. Zhou, I. Shumailov, and N. Papernot, “Not my deep-
fake: Towards plausible deniability for machine-generated media,” arXiv
preprint arXiv:2008.09194, 2020.
[61] M. Stern, W. Chan, J. Kiros, and J. Uszkoreit, “Insertion transformer:
Flexible sequence generation via insertion operations,” in International
Conference on Machine Learning (ICML), 2019.
[62] M. Caccia, L. Caccia, W. Fedus, H. Larochelle, J. Pineau, and L. Charlin,
“Language gans falling short,” in International Conference on Learning
Representations (ICLR), 2020.
[63] H. Hosseini, B. Xiao, A. Clark, and R. Poovendran, “Attacking auto-
matic video analysis algorithms: A case study of google cloud video
intelligence api,” in Multimedia Privacy and Security, 2017.
[64] E. Mariconti, G. Suarez-Tangil, J. Blackburn, E. De Cristofaro,
N. Kourtellis, I. Leontiadis, J. L. Serrano, and G. Stringhini, “”you
know what to do” proactive detection of youtube videos targeted by
coordinated hate attacks,” Human-Computer Interaction, vol. 3, no.
CSCW, pp. 1–21, 2019.
[65] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” in International Conference on
Learning Representations (ICLR), 2015.
[66] R. Shetty, M. Rohrbach, L. Anne Hendricks, M. Fritz, and B. Schiele,
“Speaking the same language: Matching machine to human captions by
adversarial training,” in the IEEE International Conference on Computer
Vision (ICCV), 2017.
[67] K. Choi, C. Hawthorne, I. Simon, M. Dinculescu, and J. Engel,
“Encoding musical style with transformer autoencoders,” arXiv preprint
arXiv:1912.05537, 2019.
[68] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with
gumbel-softmax,” in International Conference on Learning Represen-
tations (ICLR), 2017.
[69] M. J. Kusner and J. M. Hern´andez-Lobato, “Gans for sequences of
discrete elements with the gumbel-softmax distribution,” arXiv preprint
arXiv:1611.04051, 2016.
[70] S. Merity, N. S. Keskar, and R. Socher, “Regularizing and optimizing
lstm language models,” in International Conference on Learning Rep-
resentations (ICLR), 2018.
[71] H. Inan, K. Khosravi, and R. Socher, “Tying word vectors and word
classiﬁers: A loss framework for language modeling,” in International
Conference on Learning Representations (ICLR), 2017.
[72] R. Shetty, B. Schiele, and M. Fritz, “A4nt: author attribute anonymity
by adversarial training of neural machine translation,” in 27th USENIX
Security Symposium (USENIX Security 18), 2018.
[73] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes,
“Supervised learning of universal sentence representations from natural
language inference data,” in Empirical Methods in Natural Language
Processing (EMNLP), 2017.
[74] S. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated
corpus for learning natural language inference,” in Empirical Methods
in Natural Language Processing (EMNLP), 2015.
[75] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov,
“Transformer-xl: Attentive language models beyond a ﬁxed-length con-
text,” in the 57th Annual Meeting of the Association for Computational
Linguistics (ACL), 2019.
[76] N. Carlini, C. Liu, ´U. Erlingsson, J. Kos, and D. Song, “The secret
sharer: Evaluating and testing unintended memorization in neural net-
works,” in 28th USENIX Security Symposium (USENIX Security 19),
2019.
[77] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel mix-
ture models,” in International Conference on Learning Representations
(ICLR), 2017.
[78] S. Merity, N. S. Keskar, and R. Socher, “An analysis of neural language
modeling at multiple scales,” arXiv preprint arXiv:1803.08240, 2018.
[83] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using
siamese bert-networks,” in Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 2019.
[84] A. Venugopal, J. Uszkoreit, D. Talbot, F. J. Och, and J. Ganitkevitch,
“Watermarking the outputs of structured prediction with an application
in statistical machine translation,” in Empirical Methods in Natural
Language Processing (EMNLP), 2011.
[85] J. A. Suykens and J. Vandewalle, “Least squares support vector machine
classiﬁers,” Neural Processing Letters, vol. 9, no. 3, pp. 293–300, 1999.
[86] L. Fan, K. W. Ng, and C. S. Chan, “Rethinking deep neural network
ownership veriﬁcation: Embedding passports to defeat ambiguity at-
tacks,” in Advances in Neural Information Processing Systems, 2019.
[87] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case
of neural text degeneration,” in International Conference on Learning
Representations (ICLR), 2020.
[88] C. Castillo, M. Mendoza, and B. Poblete, “Information credibility on
twitter,” in the 20th international conference on World Wide Web, 2011.
[89] R. Tan, B. Plummer, and K. Saenko, “Detecting cross-modal inconsis-
tency to defend against neural fake news,” in Empirical Methods in
Natural Language Processing (EMNLP), 2020.
[90] J. Thorne, M. Chen, G. Myrianthous, J. Pu, X. Wang, and A. Vlachos,
“Fake news stance detection using stacked ensemble of classiﬁers,” in
the EMNLP Workshop: Natural Language Processing meets Journalism,
2017.
[79] M. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a large
annotated corpus of english: The penn treebank,” Computational Lin-
guistics, vol. 19, no. 2, pp. 313–330, 1993.
[80] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in International Conference on Learning Representations (ICLR), 2015.
[81] M. Denkowski and A. Lavie, “Meteor universal: Language speciﬁc
translation evaluation for any target language,” in the 9th Workshop on
Statistical Machine Translation, 2014.
[82] G. A. Miller, WordNet: An electronic lexical database. MIT press,
1998.
[91] N. Hassan, F. Arslan, C. Li, and M. Tremayne, “Toward automated fact-
checking: Detecting check-worthy factual claims by claimbuster,” in the
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD), 2017.
[92] O. Sharir, B. Peleg, and Y. Shoham, “The cost of training nlp models:
A concise overview,” arXiv preprint arXiv:2004.08900, 2020.
[93] Lambda, “Openai’s gpt-3 language model: A technical overview.”
[Online]. Available: https://lambdalabs.com/blog/demystifying-gpt-3/
A. Metrics Analysis
VIII. APPENDIX
We show more examples to examine and validate the metrics
we use to evaluate or sort the output of the model.
1) Sampling: In Section V-B6, we discussed that the lan-
guage model
loss gives slightly better sentences in terms
of syntactic correctness than SBERT, therefore, we used it
to sort and select the best sample. In Table XI, we show
examples of such cases. Nevertheless, we still measure the
semantic similarity using SBERT as a metric due to the
beneﬁts discussed below.
2) SBERT and Meteor: In our analysis, we use the SBERT
distance between the input and output sentences’ embeddings
as an auxiliary metric besides using the meteor score. We here
demonstrate examples of sentences with high SBERT distance
and the advantages of using it over meteor only.
One of the cases that yields a high SBERT distance is
when the output text has a changed sentiment (e.g., by using
a negation), such as the two examples in Table XII. These
examples do not have an extremely low meteor score since
not a lot of words were changed. The ﬁrst example also
is grammatically correct (using “are ’t”). Despite that, they
undesirably change the semantics of the input sentence, which
is detected by the SBERT since it was trained on the NLI
task. Additionally, we show in Table XIII two samples for the
same input sentence and comparable meteor scores, however,
the one with the lower SBERT distance has more coherency.
Given these observations, and the qualitative analysis we
performed in Section V-B6 (e.g., on the ‘no-discriminator’
model), we found that using SBERT is an effective metric to
approximate semantic similarity and adds more information
than using meteor alone.
B. Denoising
For the denoising autoencoder (DAE), we used 6 encoding
and decoding transformer layers in the encoder and decoder,
respectively. We also share the embeddings of the encoder,
decoder, and the pre-softmax layer (dimension: 512). The
decoder has a masked self-attention and it attends to the output
of the encoder.
Input
SBERT sample
LM sample
The new M @-@ 120 designa-
tion replaced M @-@ 20 south
of  . M @-@ 82 now ran
from  to  only.
The
city continued to grow
thanks to a commission govern-
ment’s efforts to bring in a boom-
ing automobile industry in the
1920s.
The new M @-@ 120 designa-
tion replaced M @-@ 20 south
of  . M @-@ 82 now ran
were  to  only.
The
city continued to grow
thanks to a commission govern-
ment’s could to bring in a boom-
ing automobile industry in the
1920s.
The new M @-@ 120 designa-
tion replaced M @-@ 20 south
that  . M @-@ 82 now
ran from  to  only.
The
city continued to grow
thanks to a commission govern-
ment’s efforts to bring in a boom-
ing of industry in the 1920s.
Input
Output
SBERT Meteor
there are also many species of
. There are three main
routes which ascend the mountain ,
all of which gain over 4 @,@ 100
feet ( 1 @,@ 200 m ) of elevation.
Her family had originally come
from Poland and Russia . 
’s parents had both acted as chil-
dren .  In a 2012 interview
,  stated : ” There was never
[ religious ] faith in the house
species
are ’t many
there
of
. There are three main
routes which ascend the mountain
, all of which gain over 4 by 100
feet ( 1 by 200 m ) of elevation.
Her family as originally come with
Poland and Russia .  ’s
parents had both acted by children
.  In a 2012 interview ,
 stated : ” There was with
[ religious ] faith in the house
7.5
0.93
7.19
0.93
TABLE XII: Examples in which introducing negation resulted
in a relatively high SBERT distance.
Input
Output
SBERT Meteor
The
Search
allegation
became more
This
known when 
widely
featured in the
Alexander was
documentary
for
 , which has been cited by
several authors including Gerald
 , an expert on  .
Towards the end of the song , there
is a line ” Feeding off the screams
of the  he ’s creating ”
, which was taken from the ﬁlm
The Boys from Brazil
in which
Dr.  was the villain.
became more
This
known when 
widely
featured in the
Alexander was
documentary
for
 , which has been cited by
several authors including Gerald
 , an expert on  .
 Towards the end of the
song , there is a line ” Feeding off
the screams of the  he ’s
creating ” , which was taken from
the ﬁlm The Boys from Brazil in
which Dr.  was the villain
.
allegation
Search
The
of
Search
allegation
became more
This
known when 
widely
featured in the
Alexander was
documentary
for
 , which has was cited by
several authors including Gerald
 , from expert on  .
Towards the end of the song , there
is a line ” Feeding off the screams
of the  he ’s creating ”
, which was taken from the ﬁlm
from Boys from Brazil in which
Dr.  was the villain .
of
Search
allegation
became more
This
known when 
widely
featured in the
Alexander was
documentary
for