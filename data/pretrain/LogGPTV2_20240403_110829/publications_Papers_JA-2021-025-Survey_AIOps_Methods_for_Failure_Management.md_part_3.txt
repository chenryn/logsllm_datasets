FailureManagement(FM)isthestudyoftechniquesdeployedtominimizetheappearanceand
impact of failures. In large-scale systems failures are inevitable, so adequate protection mecha-
nismsneedtobeputuptominimizetheirandsatisfyService-levelObjectives(SLO)[26].Fol-
lowinganestablishedconvention[10,21,99,119,160],wedifferentiatebetweenproactive(failure
avoidance)andreactive(failuretolerance)approaches(differentlyfromReference[99],wedonot
endorsethe“resilientapproaches”categoryforAIapproaches).Inourhierarchicalscheme,failure
avoidancecomprisesanyapproachaimingtoaddressfailuresinanticipationoftheiroccurrence,
eitherbypredictingtheappearanceoferrorsbasedonthesystemstate,orbytakingpreventive
actions to minimize their future incidence. Failure avoidance is divided into failure prevention
(Section 4.1) and online failure prediction (Section 4.2). Failure tolerance techniques, however,
deal with errors after their appearance to assist humans and improve mean-time-to-recovery
(MTTR).Failuretoleranceincludesfailuredetection(Section4.3),root-causeanalysis(Section4.4),
andremediation(Section4.5).
ThefivecategoriesabovementioneddivideFMbasedonthetemporalwindowofintervention.
Inourpreviouswork[113],wequantifiedthefrequencyofFMpublicationsinourfivecategories.
TheFMareawiththehighestnumberofcontributionsisfailuredetection(226,33.7%),followed
by root-cause analysis (179, 26.7%) and online failure prediction (177, 26.4%). However, failure
preventionandremediationaretheareaswiththesmallestnumberofattestedcontributions(71
and17,respectively).Thesetrendsareconfirmedandevenmoreaccentuatedwhenwelookatthe
mostrecentpublicationperiod(2018onwards):only11(5)publicationshavebeenfoundforfailure
prevention(remediation)inthistimeframe.
Withineachofthesefivecategories,wecangroupcurrentcontributionsbasedontargetsprob-
lems(ortasks)thatacontributionaimstosolve(e.g.,failurepreventionincludessoftwaredefect
prediction,faultinjection,softwarerejuvenation,andcheckpointing).Table4categorizestheFM
contributionsanalyzedinthisworkbycategory,taskandAImethodsused.
While we consider our five-category scheme for FM comprehensive and stable over time (al-
thoughtherelativeproportionofworksineachcategorymaychange),weenvisionthatlandscape
oftasksandtargetproblemswithineachcategorymayevolvemorequickly,byexpandingsome
ofthecurrentminoritytasksandbypresentingnewproblemsandchallengesintheyearstocome
(see discussion in Section 5.1). Table 8 (at the end of the section) present the same list of works
groupedbytask,AImethod,employeddatasources,andhigh-leveltargetcomponents.
4.1 FailurePrevention
Animportant(andyetnosocommon)possibilitytodealwithfailuresproactivelyisfailurepre-
vention,treatedinthissection.Failurepreventionmechanisms,tendtominimizetheoccurrence
andimpactoffailures,byanalyzingtheconfigurationofthesystem,bothinstaticaspects(likethe
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
81:10 P.Notaroetal.
Table4. PapersAnalyzedinthisSurveybyCategory,Task,andAIMethod
Category Task AIMethod
LinearModels:[35,52,102,117,153] NaiveBayes:[38,94,98,141]
SoftwareDefect Tree-based:[38,50,94,98,141] SVM:[42,50]
Prediction(SDP) BayesianNetworks:[38,50,115,141] Others:[76,102,153]
LogisticRegression:[35,98,103,141]
FailurePrevention Clustering:[105,128] LinearModels:[128]
FaultInjection
Tree-based:[105]
SoftwareAgingand MarkovModels:[21,136,137] Tree-based:[4]
Rejuvenation LinearModels:[4,21,49]
Checkpointing MarkovModels:[62,95,114]
Tree-based:[77,78,89,104,138,148] NaiveBayes:[54,88]
HardwareFailure NeuralNetworks:[37,78,89,149,165,167] Clustering:[54,101]
Prediction SVM:[89,164,167] LogisticRegression:[89]
OnlineFailure MarkovModels:[161,164] Others:[33,143]
Prediction SVM:[44,81,160] AutoregressiveModels:[23,119]
SystemFailure
BayesianNetworks:[31,119] NeuralNetworks:[61,160]
Prediction
MarkovModels:[123] Others:[81,160]
Clustering:[11,45,72,127] PCFG:[11,26]
Autoencoders:[7,131,150,159] FSM:[12,45]
AnomalyDetection
MarkovModels:[110,127] Tree-based:[86,151]
FailureDetection PCA:[72,73,151] Others:[29,31,86,87,127]
OtherNeuralNetworks:[18,41,45,92,162]
InternetTraffic NeuralNetworks:[8,142] SVM:[43]
Classification NaiveBayes:[97]
LogEnhancement Tree-based:[168] Others:[163]
GraphMining:[9,30,156] Search:[30,80,132]
FaultLocalization
Others:[1,72,83,85,110,121,145]
Root-cause Root-cause PatternMatching:[6,127] Others:[25,26,64,124,154]
Analysis Diagnosis BayesianNetworks:[31,64]
Clustering:[32,84,120] Others:[3,14,32]
RCA-Others
LogisticRegression:[14,120]
IncidentTriage MarkovModels:[126] BayesianDecisionTheory:[158]
Solution TextAnalysis:[82,140,166] Similarity-based:[140]
Remediation
Recommendation
Recovery MarkovModels:[124]
sourcecode)anddynamicaspects(e.g.,theavailabilityofcomputingresourcesinphysicalhosts).
Thecommongoalistotakeorsuggestpreventiveactions;however,thestrategiestoachievethis
endgoalvaryextensivelyintargetsandmodeofapplication.Moreover,wecandistinguishpre-
ventiveoperationsinaofflinesettingandanonlinesetting.Fortheformer,weobservealargepre-
dominance of software debugging techniques, usually categorized under the name of software
defect prediction (SDP), determined to evaluate failure risks from source code analysis; fault
injectiontechniquesarealsosporadicallypresent,withtheobjectiveofstress-testingthesystem
togainadditionalinsightsandpreventfuturefailures.Inanonlinesetting,weobservedmethods
dealingwiththeproblemofsoftwareaging,categorizedunderthenameofsoftwarerejuvenation;
andcheckpointingtechniques,todeliverefficientrestartstrategiesinthepresenceofirreversible
errors.
4.1.1 Software Defect Prediction. SDP determines the probability of running into a software
bug(ordefect)withinafunctionalunitofcode(i.e.,afunction,aclass,afile,oramodule).The
fundamentalassumption,whichconnectsSDPtotheoccurrenceoffaults,istheobservationthat
defect-pronecodegeneratesfailureswhenexecuted.Therefore,estimatingtheremainingdensity
ofbugsinafunctionalunitofcodeallowssoftwaremaintainerstoprioritizetheirefforts,concen-
tratingonthemostvulnerablemodules,files,andmethods.
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
ASurveyofAIOpsMethodsforFailureManagement 81:11
A traditional method to identify fault-prone software consists in constructing defect predic-
torsfromcodemetrics.Codemetricsarehandcraftedfeaturesobtaineddirectlyfromsourcecode,
which potentially have the power to predict fault proneness in software. Over the last decades,
differentgroupsofcodemetricshavebeenproposed:theoriginalmodule-levelmetrics,designed
during the 1970s for procedural languages, describing graph complexity (McCabe metrics [90])
andreadingcomplexity(Halsteadmetrics[53]);ChidamberandKemerer[28]laterdefinedasuite
ofmetricsforobject-orientedsoftwareonaclasslevel,usuallyreferredtoas“CKmetrics”(from
theauthors’initials);Briandetal.[16]haveintroducedcouplingmetricsasameasureofintercon-
nection between software modules; finally, the Lines-of-Code (LOC) metric is widely adopted
acrosstheresearchcommunity.
Nagappanetal. [102]investigateanSDPapproachemploying codecomplexitymetrics.They
however argue that multicollinearity (or inter-correlation) between some of these metrics ren-
der the problem more complex. They apply Principal Component Analysis (PCA) to obtain
a smaller set of uncorrelated features. Then, they construct linear regression models to predict
post-releasedefectsinfivedifferentdatasets.Theyalsoattempttoapplymodelslearnedonone
projectontootherprojectstotestcross-projectapplicability,obtainingmixedresults,andtherefore
arguingforthesimilarityofprojectsasarequirementfortransferlearning.
In Reference [94], Menzies et al. discuss the use of static code metrics (like McCabe and Hal-
stead’s),arguingintheirfavorandrejectingcommonclaims,suchastheoneofbeinguninforma-
tive(becauseofcross-correlationofvalueswithinthecodefeatureset)orsuboptimalcomparedto
otherfeatures.Inparticular,theyarguefortheirpracticality,popularity,andusefulness.Attention
ismovedfromthechoiceofmetricstothechoiceoflearners:byapplyingdecisiontreesandNaïve
Bayes models, software modules are classified as defect-prone or defect-free. The authors argue
in favor of Naïve Bayes with the use of logarithmic features, which obtains the best detection
performance(71%recalland25%falsealarmrate).
Elishetal.[42]useSupportVectorMachines(SVM)forsoftwaredefectprediction.Failure-
pronenessispredictedatthefunctionlevel,fromacompositesetofMcCabe,Halstead,andLOCfea-
tures.TheapproachappliesGaussiankernelstoenablenon-linearmodelingandistestedonfour
softwareprojectsfromtheNASAMDPdataset[93].AcomparisonwithotherMachineLearning
methods (logistic regression, multilayer perceptron,k-NN,k-means, Bayesian Networks (BN),
NaïveBayes,RandomForest,andDecisionTree)iscarriedout,wheretheSVMapproachoutper-
forms all the models in terms of recall (≥0.994), at the expense of a lower precision (≥0.8495).
SVMsexhibitaccuracies(≥0.8459)andF-scores(≥0.916)comparabletotheothermethods.
AnothercommonclassofmethodsforSDPisconstitutedbyBN,duetotheirpositivequalities
suchasaccuracy,interpretability,andmodifiability.Dejaegeretal.[38]consider15differentBN
classifiers,includingNaïveBayesasaspecialcase.Theapproachisvalidatedontwodatasets(the
alreadymentionedNASAMDP[93]andtheEclipseprojectrepository[157])anditiscompared
to other Machine Learning algorithms using AUC and H-measure. The results show how more
interpretablemodelstendtobelesseffectivethancomplex,discriminativestructures.Okutanet
al.[115]integrateestablishedcodemetricswithtwonewlyintroducedmeasures,theNumberof
DevelopersandtheLackofCodingQuality(LOCQ),anduseBayesianNetworkstomodelthe
causalrelationshipsamongmetrics,aswellasbetweenmetricsanddefectproneness.Experiments
conducted on the PROMISE data repository [93] highlight the prediction effectiveness of three
metrics:LOC,LOCQ,andResponseforClass,aCKmetric.
A potential indication of the presence of software faults may also come from the change his-
toryofsourcecode.Inparticular,thecodeageandthenumberofpreviousdefectscanbeindica-
tive to estimate the presence of new bugs [52]. This type of analysis better reflects a software
model where changes are continuously applied to a code repository and where new defects are
ACMTransactionsonIntelligentSystemsandTechnology,Vol.12,No.6,Article81.Publicationdate:November2021.
81:12 P.Notaroetal.
potentiallyintroducedwitheachnewrelease.WhilethemajorityofearlySDPcontributionshave
approached the problem from a single-release perspective, a second category of works (the so-
calledchangelogapproaches)haveconcentratedtheireffortsaroundsoftwarehistory,considered
amoredeterminingfactorthancodemetricstoestimatedefectdensity.Gravesetal.[52]divide
softwarequalitypredictorsintoproductmeasures(likecodemetrics)andprocessmeasures,which
quantitativelydescribethechangehistoryofasoftwareproject,advocatingforthelattercategory
(product measures are considered inconclusive from a correlation analysis). Generalized linear
modelsareconstructedfromprocessmeasurestopredictthedefectcountinthesoftwarereposi-
toryofatelephoneswitchingsystem.Aweighted-timedampmodelisalsointroduced,wherecode
changesaredown-weightedbasedonage,showingthebestperformancesoverall.
Ostrandetal.[117]examinechangesinlargesoftwaresystemsandtheirrelationtopastfaults
to predict the number of defects in the next release. As in Reference [52], they adopt a Poisson
generalizedlinearmodelfromwhichthemaximumlikelihoodestimatesofthemodelparameters
are used to interpret the relevance of the different metrics. The model is tested on the release
cycleofanin-houseinventorysystemwithawiderangeofnewmetricsatthefilelevel,including
programming language, edit flags, and age. According to the results, the top 20% files with the
highestpredictedfaultcountcontainedonaverage83%ofthelateridentifiedfaults.Acomparative
analysisofthetwosetsofSDPmetrics(codeandchangemetrics)wasconductedbyMoseretal.
[98]ontheEclipseprojectrepository.ThecomparisonisperformedusingthreedifferentMachine
Learning approaches: Naïve Bayes, logistic regression, and decision trees. Change metrics used
individuallyareshowntobemoreefficientthancodemetricsalonefordetectingdefectivesource
files. Moreover, a combined approach slightly improves or has comparable results to a change
metric approach. Giger et al. [50] also utilize a combination of source code and change metrics
to tackle defect prediction at the method level by applying four different ML algorithms. Using
aninvestigationmethodologysimilartoReference[98],resultsareagainpresentedbythesetof
input metrics used with similar conclusions: change-metric approaches outperform code-metric
approaches,whiletheadvantageofahybridsetofmetricsisobservablebutlimited.
InadditiontointroducingthebenchmarkSDPdatasetAEEEM,D’Ambrosetal.[35]compute
several change-related metrics, such as the number of revisions, refactorings, and bug fixes per
file,whicharecorrelatedwiththenumberoffuturedefects.Mostlybasedontheconceptsofcode
entropyandchurn,thesemetricsarethenappliedwithgeneralizedregressionmodelstoclassify
andrankfilesbyprobabilitydefect.Theproposedrankingapproachcanconsiderthenecessary
revieweffortoffilesaswell.Learning-to-Rank(LTR)isalsothefocusoftheworkofYangetal.
[153],wherealinearmodelistrainedviacompositedifferentialevolution(CoDE)andinput
metrics are selected using the information gain criterion. According to the authors, maximizing
therankingperformancedirectly,ratherthanrelyingonthepredictednumberoffaults,provides
benefits such as robustness for SDP models focusing on ranking. This claim is verified by com-
paringLTRapproacheswithtraditionalleast-squaresregressionapproachesbasedonbugcount
prediction,whereLTRapproachesareinfactpredominantlymoreaccuratefortherankingtask,
especiallyinprojectswithahighnumberofmetricsandreleases.
Cross-projecttransferability,i.e.,theabilitytolearnasoftwaredefectmodelonsourceprojects