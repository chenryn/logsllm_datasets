### Packet Transfer Performance

In this experiment, we evaluated the time required to transfer network packets from the CPU's memory space to the GPU's memory space. The throughput for transferring packets to the GPU varies depending on the data size and whether page-locked memory is used. For this experiment, we used two different video cards: a GeForce 8600GTS operating on PCIe 16x v1.1 and a GeForce 9800GX2 (single PCB) operating on PCIe 16x v2.0.

As expected, copying data from page-locked memory, which can be performed asynchronously via DMA, is significantly faster than non-page-locked memory. Figure 7 illustrates the sustained throughput for transferring packets to the graphics card using both virtual and page-locked memory. 

For the GeForce 8600GTS with PCIe 16x v1.1, the maximum theoretical throughput is 4 GB/s. For large buffer sizes, we achieved approximately 2 GB/s with page-locked memory and 1.5 GB/s without it. When using the GeForce 9800GX2 with PCIe 16x v2.0, the maximum throughput reached 3.2 GB/s, despite the theoretical maximum being 8 GB/s. The discrepancy between the theoretical and actual throughputs is likely due to the use of 8b/10b encoding in the physical layer.

### Regular Expression Matching Raw Throughput

In this experiment, we evaluated the raw processing throughput of our regular expression matching implementation on the GPU, excluding the cost of delivering packets to the GPU's memory space. Figure 8 shows the computational throughput for both CPU and GPU implementations, comparing the performance of global device memory and texture memory.

When using global device memory, our GPU implementation operates about 18 times faster than the CPU implementation for large buffer sizes. Using texture memory, the GeForce 9800GX2 achieved an improvement of 48.2 times compared to the CPU implementation, reaching a raw processing throughput of 16 Gbit/s with a 4096-byte packet buffer. However, increasing the packet buffer size from 4096 to 32768 packets resulted in only a slight improvement.

We also repeated the experiment using the older GeForce 8600GT card, which contains 32 stream processors operating at 1.2 GHz. The performance doubled when moving from the 8600GT to the 9800GX2, demonstrating that our implementation scales well with newer graphics cards.

### Overall Snort Throughput

In our next experiment, we evaluated the overall performance of the Snort IDS using our GPU-assisted regular expression matching implementation. Due to Snort's single-threaded design, we were limited to using only one of the two PCBs in the GeForce 9800GX2. The CUDA SDK requires multiple host threads to execute device code on multiple devices, so Snort's single thread could only execute device code on one PCB.

Figure 9 shows the overall throughput of the Snort IDS with and without GPU acceleration. The results demonstrate a significant improvement in throughput when using the GPU, highlighting the potential of GPU-assisted regular expression matching in enhancing the performance of intrusion detection systems.

![Sustained Throughput for Packet Transfer](path_to_figure_7.png)
(a) Virtual memory
(b) Page-locked memory

![Computational Throughput for Regular Expression Matching](path_to_figure_8.png)
(a) Global device memory
(b) Texture memory

![Overall Snort Throughput](path_to_figure_9.png)