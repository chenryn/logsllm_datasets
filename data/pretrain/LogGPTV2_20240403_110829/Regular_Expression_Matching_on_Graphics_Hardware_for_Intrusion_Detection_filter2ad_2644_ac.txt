contained in the default Snort ruleset when compiled to DFAs
)
c
e
s
/
s
e
t
y
B
M
(
t
u
p
h
g
u
o
r
h
T
4000
3500
3000
2500
2000
1500
1000
500
0
1
10
100
1000
10000
50000
Buffer size (Number of packets)
GeForce 8600GTS
GeForce 9800GX2 (single PCB)
)
c
e
s
/
s
e
t
y
B
M
(
t
u
p
h
g
u
o
r
h
T
4000
3500
3000
2500
2000
1500
1000
500
0
1
10
100
1000
10000
50000
Buffer size (Number of packets)
GeForce 8600GTS
GeForce 9800GX2 (single PCB)
(a) Virtual memory
(b) Page-locked memory
Fig. 7. Sustained throughput for transferring packets to the graphics card using virtual
(a) and paged-locked (b) memory
In this experiment we evaluated the time
Packet transfer performance.
spent in copying the network packets from the memory space of the CPU to
the memory space of the GPU. The throughput for transferring packets to the
GPU varies depending on the data size and whether page-locked memory is used
or not. For this experiment we used two diﬀerent video cards: a GeForce 8600
operating on PCIe 16x v1.1, and a GeForce 9800 operating on PCIe 16x v2.0.
As expected, copying data from page-locked memory, despite the fact that can
be performed asynchronously via DMA, is substantially faster than non page-
locked memory, as shown in Figure 7. Compared to the theoretical 4 GB/s peak
throughput of the PCIe 16x v1.1 bus, for large buﬀer sizes we obtain about 2
GB/s with page pinning and 1.5 GB/s without pinning. When using PCIe 16x
v2.0, the maximum throughput sustained reached 3.2 GB/s, despite the maxi-
mum theoretical being 8 GB/s. We speculate that the reason of these divergences
from the theoretical maximum data rates is the use of 8b/10b encoding in the
physical layer.
Regular Expression Matching on Graphics Hardware
277
)
c
e
s
/
s
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
20
15
10
5
0
0
5000
10000
15000
20000
25000
30000
Buffer size (Number of packets)
CPU (single processor)
CPU (dual processor, extrapolated)
GeForce 8600GTS
GeForce 9800GX2 (single PCB)
GeForce 9800GX2 (dual PCB)
55
50
45
40
35
30
25
20
15
10
5
0
p
u
d
e
e
p
S
)
c
e
s
/
s
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
20
15
10
5
0
0
5000
10000
15000
20000
25000
30000
Buffer size (Number of packets)
CPU (single processor)
CPU (dual processor, extrapolated)
GeForce 8600GTS
GeForce 9800GX2 (single PCB)
GeForce 9800GX2 (dual PCB)
55
50
45
40
35
30
25
20
15
10
5
0
p
u
d
e
e
p
S
(a) Global device memory
(b) Texture memory
Fig. 8. Computational throughput for regular expression matching
Regular expression matching raw throughput. In this experiment, we
evaluated the raw processing throughput that our regular expression matching
implementation can achieve on the GPU. Thus, the cost for delivering the packets
to the memory space of the GPU is not included.
Figure 8 shows the raw computational throughput, measured as the mean size
of data processed per second, for both CPU and GPU implementations. We also
explore the performance that diﬀerent types of memory can provide, using both
global and texture memory to store the state machine tables. The horizontal
axis represents the number of packets that are processed at once by the GPU.
When using global device memory, our GPU implementation operates about
18 times faster than the speed of the CPU implementation for large buﬀer sizes.
The use of texture memory though appears to maximize signiﬁcantly the utiliza-
tion of the texture cache. Using texture memory and a 4096 byte packet buﬀer,
the GeForce 9800 achieved an improvement of 48.2 times compared to the CPU
implementation, reaching a raw processing throughput of 16 Gbit/s. However,
increasing the packet buﬀer size from 4096 to 32768 packets gave only a slight
improvement.
We have also repeated the experiment using the older GeForce 8600GT card
which contains only 32 stream processors operating at 1.2GHz. We can see that
the achieved performance doubles when going from the previous model to the
newest one, which demonstrates that our implementation scales to newer graph-
ics cards.
5.4 Overall Snort Throughput
In our next experiment we evaluated the overall performance of the Snort IDS
using our GPU-assisted regular expression matching implementation. Unfortu-
nately, the single-threaded design of Snort forces us to use only one of the two
PCBs contained in the GeForce 9800 GX2. Due to the design of the CUDA
SDK, multiple host threads are required to execute device code on multiple de-
vices [19]. Thus, Snort’s single thread of execution is able to execute device code
278
G. Vasiliadis et al.
)
c
e
s
/
s
t
i
b
M
(
t
u
p
h
g
u
o
r
h
T
250
225
200
175
150
125
100
75
50
25
0
NO PCRE
CPU
GPU
)
c
e
s
/
s
t
i
b
M
(
t
u
p
h
g
u
o
r
h
T
900
800
700
600
500
400
300
200
100
0
CPU