82.7% 86.3–86.8% 86.7–87.1% 85.1–85.1% 83.3% 82.0% 75.8% 81.0% 98.3%
“T”: trivial cases (two versions are identical); “NT”: non-trivial cases (two versions are different).
7-zip T & NT
∗
Implementation of RE Techniques. Step 1 was realized by a standard web caching
simulator correctly following the HTTP protocol. Step 2 and 3 were implemented by
using open-source projects of xdelta 3.0 ( http:// xdelta. org/, for VCDIFF),
LZMA SDK 9.20 (for 7-zip), bzip2 1.0.6, and gzip 1.2.4, all having a tunable parameter
between 1 (least compact but fastest) and 9 (most compact but slowest) allowing users
to balance between compression ratio and speed. We implemented the MODP algorithm
in C++ based on a recent paper [14] that improves the original algorithm [19].
The Key Evaluation Metric is the Compression Ratio (CR), deﬁned as the ratio of
trafﬁc volume after compression to the trafﬁc volume of the original trace. A smaller
CR indicates more effective compression. CR is consistently used in Tables 1 to 4.
5.2 Applying Individual RE Approaches
We ﬁrst examine each individual RE approach.
Overall Statistics. The dataset consists of 118 GB of packet traces dominated by
downlink trafﬁc (93% of the bytes go from the Internet to handsets). As identiﬁed by
the HTTP parser, 85.4% of all trafﬁc is HTTP that can be potentially optimized by the
object-based RE techniques described in §5.1.
TCP/IP Headers. We exclude all TCP/IP headers from our analysis because they
can be effectively compressed by mobile networks (e.g., UMTS uses the Packet Data
Convergence Protocol [2] for header compression) but in our data collected on handsets
they were captured as uncompressed.
We discuss key results in Table 1. As indicated by the “Caching” column (Column
1), good web caching implementation reduces the overall trafﬁc volume by 17%.
File Compression. In Columns 2 to 4, all three ﬁle compression techniques effectively
achieve compression ratios (CR) between 82.5% and 84.9% for HTTP trafﬁc. Neither
the algorithm nor the compression level impacts the CR signiﬁcantly. This can be
explained as follows. We ﬁrst note that compression is likely to yield more gains for
smaller ﬁles, which tend to be uncompressed text ﬁles. In contrast, large ﬁles are
usually audio, video, or ﬁles already compressed in the data. Compressing them further
brings little additional beneﬁts regardless of the compression method (Table 2). This
is validated by the fact that the overall CR value (gzip level 5) for all responses under
100KB (they account for 33% of the total HTTP response volume) is 71%, compared
to 93% for all responses of at least 100KB. This conﬁrms the intuition that most
How to Reduce Smartphone Trafﬁc Volume by 30%?
47
Table 2. Effectiveness of gzip compression (lv 5) on different content types. Content types with
CR values less than 30% (i.e., compression is under-utilized) are highlighted.
∗
CR (gzip) Content-Type % bytes % NC
19.34% 100.00% 97.84% video/x-ﬂv
Content-Type % bytes % NC
video/mp4
app/octet-stream 13.14% 99.44% 95.21% text/html
(App market)
image/jpeg
audio/mpeg
video/3gpp
text/xml
∗
12.46% 100.00% 86.83% text/javascript
10.20% 99.47% 88.90% image/png
8.14% 99.99% 97.07% app/x-javascript
6.34% 100.00% 96.86% video/ﬂv
5.23% 98.18% 14.59% text/css
“NC”: The fraction of bytes that are Not Compressed.
∗
CR (gzip)
4.65% 99.85% 98.64%
3.74% 70.11% 23.49%
2.57% 58.17% 27.44%
2.40% 97.77% 90.86%
2.34% 59.48% 29.45%
1.61% 100.00% 97.86%
1.27% 85.84% 19.34%
gains come from small ﬁles. Secondly, most reasonable compression techniques tend
to perform similarly for small ﬁles – probably because redundancy patterns in smaller
ﬁles are usually easier to discover so even using a lightweight compression technique
less aggressively (e.g., gzip with a small dictionary) can achieve a reasonable CR.
Under-utilization of compression can be caused by either a handset or the server.
Speciﬁcally, 60% of HTTP requests, whose responses account for 79% of the total
HTTP response trafﬁc4, do not contain an Accept-Encoding header ﬁeld, making it
impossible for the server to transfer a compressed ﬁle. Compressing the responses using
gzip yields a CR of 82% for the corresponding 79% of the HTTP response trafﬁc. Also
26% of HTTP requests do have Accept-Encoding header ﬁelds but their responses are
not compressed by the server. Compressing them reduces their HTTP response trafﬁc
volume by 10%.
Table 2 lists the top Content-Type strings appearing in HTTP responses (Column
1), their contribution to the overall HTTP trafﬁc volume (Column 2), the fraction
of bytes that are not compressed in the original data (Column 3), and their CR
values (Column 4). For example, for all bytes in the original trace belonging to
text/xml ﬁles, they are responsible for 5.23% of the total HTTP trafﬁc volume, and
98.18% of such bytes belong to ﬁles that were not compressed in the original trace.
By compressing those ﬁles, the transferred text/xml data size can be reduced to
5.23%*14.59%=0.76% of all (unoptimized) HTTP trafﬁc volume. Table 2 indicates a
bimodal distribution of CR values across content types. Compression is under-utilized
in the original trace for most text ﬁles (html, xml, javascript, and css) accounting for
15% of all HTTP trafﬁc. For each of such content types, 58% to 98% of the response
data is not compressed. If compression is used, more than 70% of their bytes can
be saved. In contrast, images, videos and most binary data already have compact ﬁle
formats so further compression brings marginal beneﬁts.
To understand the limits of the effectiveness of the ﬁle compression techniques, we
combine all HTTP requests and responses for each of the 20 users into a single large ﬁle,
run compression for each ﬁle, and then compute a CR value across all these 20 ﬁles. The
gaps between these lower bounds (Columns 5 to 7 in Table 1) and their corresponding
CRs of object-based compression (Columns 2 to 4) vary between 3.5% and 10.8%,
depending on the compression technique. Also, HTTP/1.1 does not compress HTTP
4 Unless otherwise speciﬁed, a percentage such as “x% of HTTP trafﬁc” and “x% of all trafﬁc”
refers to the percentage of trafﬁc in the original data before being optimized by RE techniques.
48
F. Qian et al.
p=1/4 p=1/8 p=1/16 p=1/32
n=512k
70.2% 71.9% 73.7% 75.3%
n=256k
71.8% 73.4% 75.2% 76.8%
n=128k
73.1% 74.7% 76.4% 78.0%
n=64k
74.3% 75.8% 77.5% 79.0%
n=512k (no loss) 69.3% 71.0% 72.8% 74.4%
1
0.5
F
D
C
0
0.4
Compression Ratio (CR)
0.6
0.8
1
Table 3. Applying the MODP algorithm on all trafﬁc
Fig. 1. CR distribution across
users (caching+gzip+delta)
headers [11], which account for 5% of the total HTTP bytes in the trace. Compressing
them reduces CR of HTTP trafﬁc by about 1.4% (considered by Columns 2 to 4). This
is performed by SPDY [3] that has been implemented in the Google Chrome browser.
Delta Encoding. Consists of two scenarios: a trivial case where the two versions are
identical (i.e., the delta is zero), and a non-trivial case where they are different. The
trivial case is already handled by today’s HTTP caching. Column 8 in Table 1 includes
both cases while Column 9 in Table 1 only considers additional beneﬁts brought by
handling non-trivial cases using VCDIFF, a feature not widely deployed. We observe
that doing so only slightly outperforms using only standard caching because trivial cases
are much more prevalent than non-trivial cases. Speciﬁcally, 19.0% of HTTP bytes
belong to cacheable ﬁles whose previous instances remain unchanged. Requests for
these ﬁles can be served either by the local cache before expiration, or by a 304 Not
Modified response after expiration. In contrast, only 4.7% of HTTP bytes belong to
ﬁles whose previous instances (with the same URL) differ. But for those 4.7% of HTTP
bytes, VCDIFF does make them more compact than gzip does: VCDIFF achieves a
CR value of 57.4%, while using gzip without leveraging similarities between the two
versions yields a much higher CR of 72.4%.
Packet Stream Compression. Table 3 quantiﬁes the effectiveness of MODP by
changing two critical parameters n, the size of the packet cache in terms of the number
of packets, and p, the sampling rate for ﬁngerprint generation (§4). The overall CR is
encouragingly good, between 70.2% and 79.0%. Exponentially decreasing p from 1/4 to
1/32 does not dramatically increase CR because the similarity between an input packet
and a cached packet is often high so generating ﬁngerprints less frequently can still
yield a reasonably high matching rate. Decreasing n from 512k to 64k causes limited
increase of CR as well due to the temporal locality of cache access [5].
Table 3 considers packet loss, which may hinder the MODP algorithm from func-
tioning correctly. Consider a packet P that is lost after entering the ingress end’s cache.
The egress end thus cannot decode any subsequent packet that is compressed using the
reference packet P . To address this issue, we assume that if the receiver cannot decode
a packet, it immediately requests that the sender retransmit the lost reference packet(s)
to synchronize the two packet caches [14].
We measured the overall retransmission rate of TCP trafﬁc, which accounts for
98% of the overall trafﬁc volume, to be 2.1% (1.8% for Wi-Fi and 2.2% for 3G).
For all but the last row in Table 3, we conservatively treat all TCP packets that are
later retransmitted as lost packets. The last row corresponds to a hypothetical scenario
How to Reduce Smartphone Trafﬁc Volume by 30%?
49
Table 4. Jointly applying multiple RE techniques
caching caching caching
+gzip +bzip2 +7-zip
caching caching caching All
+gzip +bzip2 +7-zip n=512k n=256k n=128k n=64k
p=1/8 p=1/16 p=1/32
+delta +delta +delta
HTTP 71.4% 71.6% 70.4% 71.1% 71.3% 70.1%
p=1/4
All
-
-
-
-
∗
∗
All
∗
∗
All
∗
All
75.5% 75.8% 74.7% 75.3% 75.5% 74.5% 68.1% 68.6% 69.2% 69.9%