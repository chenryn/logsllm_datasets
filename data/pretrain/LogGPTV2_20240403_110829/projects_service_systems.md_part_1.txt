---
layout: article
title: ""
date:
modified:
excerpt: "excerpt"
tags: []
image:
  feature:
  teaser:
  thumb:
ads: false
toc: true
---  
AIOps iForesight
----------------
In planet-scale deployments, the Operation and Maintenance (O&M) of cloud platforms cannot be done any longer 
manually or simply with off-the-shelf solutions. 
It requires self-developed automated systems, ideally exploiting the use of AI to provide tools for autonomous 
cloud operations. We rely on deep learning, distributed traces, and time-series analysis to effectively detect 
and fix anomalous cloud infrastructure behaviors during operations to reduce the workload of human operators. 
The iForesight system is being used to evaluate new O&M and AIOps approaches. iForesight 3.0 is the result of 3 years  of research with the goal to provide an intelligent new tool aimed at SRE cloud maintenance teams. It enables  them to quickly detect and predict anomalies thanks to the use of artificial intelligence when cloud services are slow or unresponsive. 
### Problem
Existing tools for monitoring IT infrastructures, networks and applications focus on collecting logs, metrics, 
events, and traces from distributed systems mainly for visualization and simple rule-based alerting.
Nonetheless, the final goal of monitoring 
is to reach a level of technological development where we have tools that conduct root cause analysis with a high 
accuracy and enable to autonomously recover systems. To achieve this goal, we still need to shift from a data 
collection stage to an insight- and action-driven paradigm. One promising path to monitor planet- and large-scale 
platforms is to rely on advanced analytics and explore techniques from statistics, time-series analysis, data mining, 
natural language processing, graph processing, machine learning, and deep learning to extract insights from large 
volumes of monitoring data to support and drive recovery actions. 
### Approach
The mission of the **AIOps SRE team** (based in [Munich](https://www.muenchen.de/int/en.html),  Germany) is to develop new systems and tools to analyze observability data from [Huawei Cloud](https://www.huaweicloud.com/en-us/about/about_us.html) to detect problems which impact customers, identify the root cause within seconds, and fix failures using the 1/5/10 rule (detection: 1 min, RCA: 5 min, recovery: 10 min).
In 2017 we adopted AI in the form of [Data Science](https://en.wikipedia.org/wiki/Data_science) and 
[Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) approaches for anomaly detection, 
root-cause analysis, fault prediction, and automated recovery into our suite. 
These techniques, including **statistical learning**, **time-series analysis**, **deep learning**, **big data**,
**streaming**, and **data visualization**, enabled us to develop new production-ready services for troubleshooting 
Huawei Cloud and detect issues which were previously undetectable.
The following figure from [Gartner](https://www.gartner.com/en) provides a high level architecture of the system 
we are building highliting the main areas of concern: 
+ Real-time streaming and historical data, observations, 
[Big Data](https://en.wikipedia.org/wiki/Big_data), 
Machine Learning (ML), 
[Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection), 
[Root-Cause Analysis](https://en.wikipedia.org/wiki/Root_cause_analysis), 
[Performance Analysis](https://en.wikipedia.org/wiki/Application_performance_management), 
[Predictive Maintenance](https://en.wikipedia.org/wiki/Predictive_maintenance), and 
[Automation](https://en.wikipedia.org/wiki/Robotic_process_automation). 
The use of AI for production engineering can support the development of new approaches for: 
1. Monitoring and alerting
2. Anomaly detection and Root Cause Analysis
3. Capacity planing and prediction
4. Canarying validation
5. Service Scaling
6. Operational performance
Our work focuses on points 1) and 2). 
An AIOps platform architecture which runs AI algorithms consists of functional layers such as:
1. *Big Data processing*. Real-time processing of streaming and historical data.
2. *Data pipeline*. Connected data processing elements ingesting data from multiple sources.
3. *Library of algorithms*: Statistical functions, classical machine learning, and deep learning algorithm.
4. *Automation*. Use runbooks and RPA technology to automate repetitive tasks.
5. *User interface*. Allows IT operations teams t interact with the platform and quickly identify issues and apply corrective actions.
For 2019, our work focuses on points 1)-3). 
### Challenges
The challenges of operationalising AI are not limited to the understanding of deep learning or machine learning algorithms.
Major challenges are related with software engineering, access and processing of large amounts of distributed data, 
model management, updating, deleting and training models on specialized GPUs and hardware, composition of workflows
for orchestrating parallel jobs, and the visual management of models, workflows, and results. 
### Huawei Cloud
HC, or Huawei Cloud, has planet-scale technical requirements. It has a 
[microservices](https://en.wikipedia.org/wiki/Microservices) architecture composed of hundreds of services.
They are distributed over thousands of hosts in many geographical regions and operate with an availability 
higher than [five nines](https://en.wikipedia.org/wiki/High_availability). 
It was build on top of [OpenStack](https://docs.openstack.org/), an opensource cloud operating system.
OpenStack controls large pools of compute, storage, and networking resources throughout tens of datacenters. 
The base services are predominantly written in Python and Java, and run on Linux. 
Huawei Cloud is one of the largest and fastest growing platforms in the world. 
It has a strong presence throughout the world with over 40 availability zones located across 23 geographical regions,
ranging from Germany, France, South/Central America, Hong Kong and Russia to Thailand and South Africa.
There are three properties that make platforms such as Huawei Cloud far more difficult to monitor and troubleshoot when
compared to other distributed systems:
1. Amount of data and relationships which O&M teams need to analyze.
2. Due to its distributed nature and complexity, system data has a low signal to noise ratio.
3. Since many different subsystems interact together, semantically reconciliating data is difficult.
The strongest challenge is its architecture design and operational complexity. 
Cloud deployments comprise thousands of geographically distributed services and microservices.
These key building block components require a close monitoring:
+ [API Gateways](https://microservices.io/patterns/apigateway.html) (e.g., [Kong](https://konghq.com))
+ [Load Balancers](https://en.wikipedia.org/wiki/Load_balancing_(computing) (e.g., [HAProxy](http://www.haproxy.org))
+ [Message Queuing Services](https://en.wikipedia.org/wiki/Message_queuing_service)
(e.g., [RabbitMQ](https://en.wikipedia.org/wiki/RabbitMQ))
+ [Distributed Caches](https://en.wikipedia.org/wiki/Distributed_cache) 
(e.g., [Redis](https://en.wikipedia.org/wiki/Redis))
+ [Web Servers](https://en.wikipedia.org/wiki/Web_server)
(e.g., [Apache](https://en.wikipedia.org/wiki/Apache_HTTP_Server))
+ [Application Servers](https://en.wikipedia.org/wiki/Application_server) 
(e.g., [EJB](https://en.wikipedia.org/wiki/Enterprise_JavaBeans))
+ [Database Servers](https://en.wikipedia.org/wiki/Database_server) 
(e.g., [MySQL](https://en.wikipedia.org/wiki/MySQL))
+ [Linux Servers](https://en.wikipedia.org/wiki/Linux) 
+ [Network Switches](https://en.wikipedia.org/wiki/Network_switch) and 
[Network Routers](https://en.wikipedia.org/wiki/Router_(computing)
Besides these building blocks which are part of the base cloud infrastructure, service offerings also need to be
monitored:
+ *Compute, network, storage*. Cloud servers, auto scaling, object storage, volume service, VPC network, and CDN, 
+ *Databases*. MySQl, PostgreSQL, and replication service.
+ *Security*. Vulnerability scan service, SSL management, and Anti-DDoS.
+ *Applications*. APM, API Gateway, and application orchestration. 
+ *Enterprise Intelligence*. Machine learning services, graph engines, face and image recognition, and Mapreduce.
+ *DevCloud*. Project management, build, code hub, code check, and code release.
### Data Sources
Monitoring data comes from many different data sources such hypervisors, OS, applications, application servers, 
middleware, databases, application logs, host and network metrics.
Generally, data sources can be classified in four types:
1. *Logs*. Service, microservices, and applications generate logs, composed of timestamped records with a structure 
and free-form text, which are stored in system files.
2. *Metrics*. Examples of metrics include CPU load, memory available, and the response time of a HTTP request.
3. *Traces*. Traces records the workflow and tasks executed in response to an HTTP request. 
4. *Events*. Major milestones which occur within a data center can be exposed as events. 
Examples include alarms, service upgrades, and software releases.
Google SRE team proposed [4 Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/) 
which provide key insights on how a distributed system is running using metrics:
+ *Latency*. Time to handle a request (aka response time)
+ *Traffic*. How much demand is being placed on a system
+ *Errors*. Rate of requests that fail
+ *Saturation*. Constraints places on service resources 
Other proposals include the [RED](https://www.vividcortex.com/blog/monitoring-and-observability-with-use-and-red) and 
[USE](http://www.brendangregg.com/usemethod.html?source=post_page---------------------------) methods.
When key services are not often called by users, the volume of metrics collected is insufficient for 
pattern recognition and anomaly detection.
In such cases, [synthetic monitoring](https://en.wikipedia.org/wiki/Synthetic_monitoring) (also known as active 
monitoring) can be adopted and consists in creating artificial users to simulate user behavior by making automated
calls to services. 
An AIOps platform needs to be able to ingest logs, metrics, traces, and events into efficient key-value databases
where they are stored to later be accessed and analyzed.
### Pattern Recognition
The objective of approaches for [pattern recognition](https://en.wikipedia.org/wiki/Pattern_recognition) is to detect
patterns in noisy and high-dimensionality data. 
Once the data is collected, we apply probabilistic algorithms, ML and other techniques to find suspicious patterns.
Examples of patterns of interests include:
+ Latency outliers and latency trends in metrics
+ Gradual degradation of traffic and incoming calls
+ Spikes or sudden change in error rate in logs
+ Saturation of memory utilization \>95% memory 
+ Structural changes in traces