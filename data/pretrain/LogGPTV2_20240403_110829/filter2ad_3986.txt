title:A user behavior based cheat detection mechanism for crowdtesting
author:Ricky K. P. Mok and
Weichao Li and
Rocky K. C. Chang
A User Behavior based Cheat Detection Mechanism for
Crowdtesting
Ricky K. P. Mok, Weichao Li, and Rocky K. C. Chang
Department of Computing
The Hong Kong Polytechnic University
{cskpmok|csweicli|csrchang}@comp.polyu.edu.hk
ABSTRACT
Crowdtesting is increasingly popular among researchers to
carry out subjective assessments of diﬀerent services. Exper-
imenters can easily assess to a huge pool of human subjects
through crowdsourcing platforms. The workers are usually
anonymous, and they participate in the experiments inde-
pendently. Therefore, a fundamental problem threatening
the integrity of these platforms is to detect various types of
cheating from the workers.
In this poster, we propose cheat-detection mechanism based
on an analysis of the workers’ mouse cursor trajectories. It
provides a jQuery-based library to record browser events.
We compute a set of metrics from the cursor traces to iden-
tify cheaters. We deploy our mechanism to the survey pages
for our video quality assessment tasks published on Amazon
Mechanical Turk. Our results show that cheaters’ cursor
movement is usually more direct and contains less pauses.
Categories and Subject Descriptors
H5.1 [Information Interfaces And Presentation]: Mul-
timedia Information Systems—Evaluation/methodology
Keywords
crowdsourcing, cheat-detection, cursor submovement
1.
INTRODUCTION
Crowdtesting is increasingly popular among researchers to
carry out subjective assessments. Through crowdsourcing
platforms (e.g., Amazon Mechanical Turk, or AMT), they
can evaluate the quality of experience (QoE) of diﬀerent net-
work services, such as video streaming, VoIP, and IPTV [1].
Experimenters can easily deploy their experiments by com-
piling the assessments as a website published in the crowd-
sourcing platform. After ﬁnishing the assessments, they can
report to the platform to claim their payment.
The advantages of using crowdsourcing over traditional
laboratory experiment are lower cost, and a larger and more
diverse crowd of workers [3]. However, without any supervi-
sion, the quality of the works received from crowdtesting is
questionable. Some cheaters only intend to maximize their
payment with minimum eﬀort by quickly submitting low-
quality works. Therefore, identifying cheaters can help im-
prove the reliability of a crowdtest-based assessment.
Existing anti-cheating methods can be categorized into
two major approaches—cheater avoidance and cheater de-
tection. The ﬁrst approach can be achieved by a better ex-
periment design or the use of CAPTCHAs. Cheater avoid-
ance, however, can only avoid automatic answering program.
On the other hand, cheater detection focuses on screening
out outliers from the data collected. Comparing with gold
standard data or checking the consistency of similar ques-
tions can help identify unreliable workers. But these meth-
ods cannot be applied to assessments with no absolute an-
swers and can be easily spotted by (semi-) human cheaters.
A summary of anti-cheating methods can be found in [3].
In this poster, we propose a novel cheat-detection mech-
anism which is based on the user behavior, such as cur-
sor and click positions, on the survey page. We claim that
these behaviors provide implicit measures for the reliability
of workers. For example, their cursor would take a shorter
and more direct path to the buttons as the cheaters aim at
ﬁnishing the task quickly. Honest workers may click or high-
light the questions which can assist them to read, whereas
the cheaters may glance over the questions and then provide
their answers quickly.
In particular, we apply submovement analysis [5] to sys-
tematically study the cursor trajectories. Submovement anal-
ysis is common in the human-computer interface area to in-
vestigate the performance and accuracy of pointing devices.
Figure 1 shows an example of a cursor trajectory consisting
of two submovements. The horizontal dotted line connects
the start and end points. The ﬁrst submovement is in up-
ward direction away from the horizontal line. The second
submovement changes to downward direction until reaching
the end point. We compute cursor measures [4] with the
user behavior data and ﬂag the outliers as cheaters.
start
end
1st submovement
2nd submovement
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631447.
Figure 1: A submovement example.
We demonstrate our proposed method by using the user
behavior dataset collected from our adaptive video quality
assessment crowdsourced through the AMT. We insert a
script written in jQuery to collect the user behavior in the
background while they are rating the video quality by click-
ing the radio buttons. Our preliminary results show that
1233 workers consistently show abnormal behavior in multiple
measures. Finally, we show the user behavior of a cheater
to illustrate our proposed method.
2. METHODOLOGY
We have implemented a jQuery-based library to collect
user behavior from the browser. Currently, these behaviors
include cursor coordinates on the page, scrolling, and mouse
click events. The library starts recording right after the
page is loaded. All the events are time-stamped and peri-
odically feedbacked to a dedicated web server using AJAX.
The library can be deployed in the question page. Answer-
ing the questions must involve multiple mouse movements
and clicks, such as using radio buttons to enter rating on a
5-point Likert scale.
2.1 Cursor measures
Our cheat-detection mechanism processes the user behav-
ior oﬄine after the tasks are completed. We ﬁrst compute a
set of cursor measures [4] for each trial and then apply the
standard score to identify a set of outliers. Due to the page
limitation, we introduce only three cursor measures below
which are eﬀective in screening out cheaters.
1) Number of submovement. This measure counts the to-
tal number of submovement on the whole question page.
We expect cheaters to have less submovement than other
workers. The number of submovement reveals whether the
worker has read the questions before answering them, as cur-
sor movements strongly correlate with the eye movement [2].
We parse the submovements by using zero-crossing of cursor
velocity [5]. A new submovement is started when either the
horizontal or the vertical velocity crosses 0.
2) Mean cursor pause duration. We compute the average
duration of all the pauses recorded. This measure can infer
the worker’s think time between questions. Since cheaters
usually answer questions quickly and continuously, the pause
duration is expected to be shorter than honest workers.
3) Number of cursor pause. We count the number of cur-
sor pauses which is longer than 50 ms. The mean pause
duration above cannot handle cheaters mimicking an honest
worker by using a single long pause. We design this pause
count to diﬀerentiate pauses due to multiple thinking pauses
from one long artiﬁcial wait. So, we expect that a cheater’s
behavior will have a smaller count in this measure.
3. RESULTS
We collected user behavior data from our crowdsourcing
adaptive video streaming QoE assessments launched using
the AMT. Each worker was required to rate the QoE of
four 60-second video clips. After playing each video, they
were prompted to answer 14 questions in a 5-point Likert
scale by clicking radio buttons and answering one control
question regarding the video content in the form of HTML
select menu. Our library was run at the background and
sent back the user behavior to our server every second.
Our assessments were completed by 127 workers, and 508
(=127 × 4) trials were conducted. Although all workers
could correctly answer the control question, two of which
were suspected to be cheaters after a manual inspection.
Their works were subsequently rejected, and we did not re-
ceive any appeal from them.
/
n
o
i
t
i
s
o
p
Y
400
600
Submit
 button
l
e
x
p
i
0
l
/
y
t
i
c
−2
o
e
V
−4
0
We compute the three cursor measures and their Z-score
for each trial. For each measure, a trial is considered an
outlier if its Z-score is less than -1, and a worker is labeled
as a suspect if he is involved in more than two outlying
trials. If a worker is a suspect in more than one measure, he
is ﬂagged as a cheater.
With our mechanism, we found three workers matching
our criteria for cheaters. One of which, labeled as Worker
C, is also a rejected worker. We inspect the ratings received
from Worker C. He simply chose 5 for all questions in all
trials. Hence, we have high conﬁdence that he is a cheater.
We further investigate the behavior of worker C by com-
paring with an honest worker (Worker R). Figure 2(a) shows
the cursor trajectories of both workers. The red dotted rect-
angle is the area for answering the questions. We can see
that worker C takes a much direct pathway to answer all
the questions, but worker R shows many submovements in
between questions. Figure 2(b) plots the x and y velocity
computed from the same trajectories. Worker C ﬁnished the
questions in less than 15 seconds, while worker R required
more than 40 seconds. Besides, we can ﬁnd a number of
pauses for worker R (two of them are indicated by arrows).
C x−velocity
C y−velocity
R x−velocity
R y−velocity
Worker R
Worker C
Cursor 
paused
Answer 
1
−
s
m
area
200
e
x
p
i
2
4
l
0
200
400
X position /pixel
600
800
10
20
Time /s
30
40
(a) Cursor trajectories.
(b) Cursor velocity.
Figure 2: Worker C ’s and Worker R’s cursor data.
4. CONCLUSION
This poster presented our user behavior based cheat-detection
mechanism. Our preliminary results showed that cheaters
show abnormal behavior when they are answering questions.
We used three cursor measures to quantify the behaviors and
detect the outliers. In the future, we will include more user
behavior, such as click events and cursor location, to im-
prove the accuracy and robustness of the mechanism.
Acknowledgments: This work is partially supported by
an ITSP Tier-2 project grant (ref. no. GHP/027/11) from
the Innovation Technology Fund in Hong Kong.
5. REFERENCES
[1] K.-T. Chen, C.-J. Chang, C.-C. Wu, Y.-C. Chang, and
C.-L. Lei. Quadrant of euphoria: a crowdsourcing
platform for QoE assessment. IEEE Network,
24(2):28–35, 2010.
[2] M. C. Chen, J. R. Anderson, and M. H. Sohn. What
can a mouse cursor tell us more?: correlation of
eye/mouse movements on web browsing. In Proc. ACM
CHI, 2001.
[3] T. Hoßfeld, C. Keimel, M. Hirth, B. Gardlo, J. Habigt,
K. Diepold, and P. Tran-Gia. Best practices for QoE
crowdtesting: QoE assessment with crowdsourcing.
IEEE Trans. in Multimedia, 16(2):541–558, 2014.
[4] F. Hwang, S. Keates, P. Langdon, and J. Clarkson. A
submovement analysis of cursor trajectories. Behaviour
& Information Technology, 24(3):205–217, 2005.
[5] D. E. Meyer, R. A. Abrams, S. Kornblum, C. E.
Wright, and J. E. K. Smith. Optimality in human
motor performance: Ideal control of repid aimed
movements. Psychological Review, 95(3):340–370, 1988.
124