Rook 是一个流行的开源 Kubernetes 存储抽象层。它可以通过各种提供商(如 EdgeFS 和 NFS)提供持久卷。在这种情况下，我们将使用 Ceph，一个提供对象、块和文件存储的开源存储项目。为了简单起见，我们将使用块模式。
在 Kubernetes 上安装 Rook 实际上非常简单。我们将带您从安装 Rook 到设置 Ceph 集群，最后在我们的集群上配置持久卷。
## 安装车
我们将使用由 Rook GitHub 存储库提供的典型的 Rook 安装默认设置。这可以根据使用情形进行高度定制，但允许我们为工作负载快速设置数据块存储。为此，请参考以下步骤:
1.  首先，让我们克隆 Rook 存储库:
    ```
    > git clone --single-branch --branch master https://github.com/rook/rook.git
    > cd cluster/examples/kubernetes/ceph
    ```
2.  我们的下一步是创建所有相关的 Kubernetes 资源，包括几个**自定义资源定义** ( **CRDs** )。我们将在后面的章节中讨论这些，但是现在，考虑它们是专门针对 Rook 的新 Kubernetes 资源，在典型的 Pods、Services 等之外。要创建公共资源，请运行以下命令:
    ```
    > kubectl apply -f ./common.yaml
    ```
3.  接下来，让我们开始我们的 Rook 操作符，它将处理为特定的 Rook 提供者提供所有必要的资源，在这种情况下，它将是 Ceph:
    ```
    > kubectl apply -f ./operator.yaml
    ```
4.  在下一步之前，使用以下命令确保车操作员 Pod 实际运行:
    ```
    > kubectl -n rook-ceph get pod
    ```
5.  一旦车荚处于`Running`状态，我们就可以建立我们的 Ceph 集群了！这个的 YAML 也在我们从 Git 克隆的文件夹中。使用以下命令创建它:
    ```
    > kubectl create -f cluster.yaml
    ```
这个过程可能需要几分钟。Ceph 集群由几种不同的 Pod 类型组成，包括操作员、**对象存储设备** ( **操作系统**)和管理人员。
为了确保我们的 Ceph 集群正常工作，Rock 提供了一个工具箱容器映像，允许您使用 Rock 和 Ceph 命令行工具。要启动工具箱，可以使用位于[https://rook.io/docs/rook/v0.7/toolbox.html](https://rook.io/docs/rook/v0.7/toolbox.html)的 Rook 项目提供的工具箱 Pod 规范。
以下是工具箱 Pod 的规格示例:
rook 工具箱 pod.yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: rook-tools
  namespace: rook
spec:
  dnsPolicy: ClusterFirstWithHostNet
  containers:
  - name: rook-tools
    image: rook/toolbox:v0.7.1
    imagePullPolicy: IfNotPresent
```
可以看到，这个 Pod 使用了 Rook 提供的特殊容器映像。该映像附带了所有工具，你需要调查鲁克和 Ceph 预装。
一旦工具箱 Pod 运行，就可以使用`rookctl`和`ceph`命令检查集群状态(查看 Rook 文档了解详情)。
## 车-ceph-块存储类
现在我们的集群正在工作，我们可以创建我们的存储类，供我们的 PVs 使用。我们将这个存储类称为`rook-ceph-block`。这是我们的 YAML 文件(`ceph-rook-combined.yaml`)，它将包括我们的`CephBlockPool`(它将处理我们在 Ceph 的块存储–更多信息请参见[https://rook.io/docs/rook/v0.9/ceph-pool-crd.html](https://rook.io/docs/rook/v0.9/ceph-pool-crd.html))以及存储类本身:
ceph-rook-combined.yaml
```
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    clusterID: rook-ceph
    pool: replicapool
    imageFormat: "2"
currently supports only `layering` feature.
    imageFeatures: layering
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
csi-provisioner
    csi.storage.k8s.io/fstype: xfs
reclaimPolicy: Delete
```
如您所见，YAML 规范定义了我们的`StorageClass`和`CephBlockPool` 资源。正如我们在本章前面提到的，`StorageClass`是我们告诉 Kubernetes 如何实现`PersistentVolumeClaim`的方式。另一方面，`CephBlockPool`资源告诉 Ceph 如何以及在哪里创建分布式存储资源——在这种情况下，复制多少存储。
现在我们可以给 PODS 一些储存空间了！让我们用新的存储类别创建一个新的聚氯乙烯:
rook-ceh PVC . YAML
```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: rook-pvc
spec:
  storageClassName: rook-ceph-block
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```
我们的 PVC 属于存储类`rook-ceph-block`，所以它将使用我们刚刚创建的新存储类。现在，让我们把聚氯乙烯给我们的 YAML 文件中的 PODS:
rook-ceh pod . YAML
```
apiVersion: v1
kind: Pod
metadata:
  name: my-rook-test-pod
spec:
  volumes:
    - name: my-rook-pv
      persistentVolumeClaim:
        claimName: rook-pvc
  containers:
    - name: my-container
      image: busybox
      volumeMounts:
        - mountPath: "/usr/rooktest"
          name: my-rook-pv
```
当 Pod 被创建时，Rook 应该旋转一个新的持久卷并将其连接到 Pod。让我们仔细观察 Pod，看看它是否正常工作:
```
> kubectl exec -it my-rook-test-pod -- /bin/bash
> cd /usr/rooktest
> touch myfile.txt
> ls
```
我们得到以下输出:
```
> myfile.txt
```
成功！
虽然我们刚刚在 Ceph 中使用了 Rook 和 Ceph 的数据块存储功能，但它也有一个文件系统模式，这有一些好处——让我们讨论一下为什么您可能想要使用它。
## 车 Ceph 文件系统
Rook 的 Ceph Block 提供程序的缺点是一次只能被一个 Pod 写入。为了用 Rook/Ceph 创建持久卷，我们需要使用支持 RWX 模式的文件系统提供程序。更多信息，请查看[https://rook.io/docs/rook/v1.3/ceph-quickstart.html](https://rook.io/docs/rook/v1.3/ceph-quickstart.html)的鲁克/Ceph 医生。
直到创建 Ceph 集群，前面的所有步骤都适用。此时，我们需要创建我们的文件系统。让我们使用下面的 YAML 文件来创建它:
路克-ceh-fs . YAML
```
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: ceph-fs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 2
  dataPools:
    - replicated:
        size: 2
  preservePoolsOnDelete: true
  metadataServer:
    activeCount: 1
    activeStandby: true
```
在本例中，我们将元数据和数据复制到至少两个池中以提高可靠性，如`metadataPool`和`dataPool`块中所配置的。我们还使用`preservePoolsOnDelete`键保留删除时的池。
接下来，让我们专门为 Rook/Ceph 文件系统存储创建我们的新存储类。下面的 YAML 就是这样做的:
路克-ceh-fs-storage class . YAML
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: ceph-fs
  pool: ceph-fs-data0
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
reclaimPolicy: Delete
```
这个`rook-cephfs`存储类指定了我们之前创建的池，并描述了我们存储类的回收策略。最后，它使用了一些注释，这些注释在 Rook/Ceph 文档中有所解释。现在，我们可以通过聚氯乙烯将它连接到部署，而不仅仅是 Pod！看看我们的 PV:
路克-cehfs-PVC . YAML
```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: rook-ceph-pvc
spec:
  storageClassName: rook-cephfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
```
这个持久的卷引用了我们在`ReadWriteMany`模式下的新`rook-cephfs`存储类——我们需要`1 Gi`这个数据。接下来，我们可以创建我们的`Deployment`:
rook-cephfs-deployment.yaml
```
apiVersion: v1
kind: Deployment
metadata:
  name: my-rook-fs-test
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25% 
  selector:
    matchLabels:
      app: myapp
  template:
      spec:
  	  volumes:
    	  - name: my-rook-ceph-pv
        persistentVolumeClaim:
          claimName: rook-ceph-pvc
  	  containers:
    	  - name: my-container
         image: busybox
         volumeMounts:
         - mountPath: "/usr/rooktest"
           name: my-rook-ceph-pv
```
该`Deployment`引用了我们使用`volumes`下的`persistentVolumeClaim`块进行的`ReadWriteMany`持续体积索赔。部署后，我们所有的 Pods 现在都可以读写同一个持久卷。
在此之后，您应该对如何创建持久卷并将其附加到 Pods 有了很好的了解。
# 总结
在本章中，我们回顾了在 Kubernetes 上提供存储的两种方法—卷和持久卷。首先，我们讨论了这两种方法之间的区别:虽然卷与 Pod 的生命周期相关联，但持久卷会持续到它们或群集被删除。然后，我们研究了如何实现卷并将它们连接到我们的 Pods。最后，我们将对卷的学习扩展到持久卷，并发现了如何使用几种不同类型的持久卷。这些技能将帮助您在许多可能的环境中为您的应用分配持久和非持久存储—从内部部署到云。
在下一章中，我们将绕过应用问题，讨论如何在 Kubernetes 上控制 Pod 的放置。
# 问题
1.  卷和持久卷有什么区别？
2.  什么是`StorageClass`，它与体积有什么关系？
3.  当创建 Kubernetes 资源(如持久卷)时，如何自动调配云资源？
4.  在哪些使用案例中，您认为使用卷而不是持久卷会令人望而却步？
# 进一步阅读
请参考以下链接了解更多信息:
*   针对 Rook 的 Ceph 存储快速启动:[https://github . com/Rook/Rook/blob/master/Documentation/ceph-quick start . MD](https://github.com/rook/rook/blob/master/Documentation/ceph-quickstart.md)
*   Rook 工具箱:https://rook . io/docs/rook/v 0.7/toolbox . html
*   云提供商:https://kubernetes . io/docs/tasks/administrator-cluster/running-cloud-controller/