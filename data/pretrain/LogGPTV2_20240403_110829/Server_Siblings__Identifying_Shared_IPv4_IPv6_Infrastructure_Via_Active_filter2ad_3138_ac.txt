described in Sect. 3.1. We perform multiple rounds of testing. While the data
158
R. Beverly and A. Berger
provides us with true associations, for evaluation purposes, we also test false
associations in each round. These known non-siblings are formed by randomly
associating a non-associated IPv6 site with each IPv4 site. In this fashion, we
test both type I and type II errors.
Table 2. Relative Ground Truth Performance of Sibling Classiﬁers
Algorithm Accuracy Precision Recall Speciﬁcity Unknown
TCP Opts 82.2 %
Kohno
Alg 1
Alg 1&2
90.6 %
94.2 %
97.4 %
74.1 %
82.3 %
93.6 %
99.6 %
98.2 % 66.8 %
97.0 % 86.4 %
91.4 % 96.0 %
93.1 % 99.8 %
0.0 %
27.8 %
22.4 %
29.4 %
We wish to understand discriminative power of both the original Kohno
timestamp skew algorithm, as well as our enhancements, in distinguishing sib-
lings from non-siblings. First, we look at using TCP options as a classiﬁer alone.
As shown in Table 2, TCP options yield an accuracy of 82.2 % with 74.1 % pre-
cision, 98 % recall, and 67 % speciﬁcity. (Where precision is the fraction of iden-
tiﬁed siblings that are truly siblings, recall is the fraction of all ground-truth
siblings classiﬁed as siblings, and speciﬁcity measures the ability to identify
non-siblings). Thus, while the option signature alone does not provide suﬃcient
granularity, it eliminates non-siblings with minimal overhead (just a single TCP
ACK packet from the IPv4 and IPv6 target).
We next examine Kohno’s original timestamp skew algorithm alone, without
consideration of TCP options. Over ten rounds, we obtain an accuracy of 90.6 %
with 82.3 % precision, 97.0 % recall and 86.4 % speciﬁcity. We then examine
Algorithm 1 and the combined Algorithms 1 and 2 as detailed in Sect. 3.5. We
see that each provides increasingly accurate sibling classiﬁcation, with the full
algorithm yielding an accuracy of 97.4 %, with 99.6 % precision, 93.1 % recall,
and 99.8 % speciﬁcity over the ten rounds of testing. However, some of this
accuracy comes at the expense of our full algorithm labeling 29.4 % of the hosts
as “unknown” as it cannot make a deﬁnitive determination.
4.2 Web Server Machine Siblings
As an initial application of our sibling detection technique, we characterize sib-
ling relationships among a subset of important Internet infrastructure, Alexa [1]
top 100,000 websites as gathered, resolved, and probed in April, 2014 (details
of dataset in Sect. 3.1). We perform our probing from a host with high-speed,
native IPv6 connectivity. To remain inconspicuous, we probe at a low rate. We
fetch the root HTML page from each site’s IPv4 and IPv6 interfaces once every
∼3.5 h over ∼17 days.
We then apply our inference Algorithm 1 and 2 to the datasets in Table 1.
As described in Sect. 3, there are a variety of potential outcomes. For each of
Server Siblings: Identifying Shared IPv4/IPv6 Infrastructure
159
Table 3. Alexa Machine-Sibling Inferences
Inference
Siblings
Dataset (Table 1)
non-CDN
CDN
Embedded
- v4/v6 drift match
816 (53.2 %)
55 (23.9 %)
978 (93.1 %)
Non-Siblings
- v4 and v6 opt sig diﬀer
229 (14.9 %)
14 (6.1 %)
22 (2.1 %)
- v4 or v6 missing
- v4 or v6 random
70 (4.6 %)
11 (4.8 %)
23 (1.5 %)
13 (5.7 %)
- v4 or v6 non-monotonic
52 (3.4 %)
47 (20.4 %)
- v4/v6 drift mismatch
35 (2.3 %)
13 (5.7 %)
7 (0.7 %)
1 (0.1 %)
1 (0.1 %)
0 (0.0 %)
Unknown
- v4 and v6 missing
196 (12.8 %)
6 (2.6 %)
26 (2.5 %)
- v4 and v6 random
32 (2.1 %)
25 (10.9 %)
- v4 and v6 non-monotonic
78 (5.1 %)
45 (19.6 %)
- v4 or v6 unresponsive
2 (0.1 %)
1 (0.4 %)
6 (0.6 %)
9 (0.9 %)
0 (0.0 %)
Total
1533 (100 %) 230 (100 %) 1050 (100 %)
the three Alexa datasets, we divide the inferences into three major categories in
Table 3: siblings, non-siblings, and unknown.
In aggregate, we ﬁnd 53.2 % of the IPv4/IPv6 addresses of non-CDN, 23.9 %
of CDN, and 93.1 % of embedded are siblings via the full Algorithm 1 and 2.
Fully 42.6 % of the CDN, and 26.7 % of the non-CDN have addresses we infer to
be non-siblings. While we expect a high proportion of siblings among sites with
embedded addresses, 3.0 % are non-sibling underscoring the fact that addresses
alone do not imply the same machine. And we cannot deﬁnitively determine
20 % of the non-CDN, 33.5 % of the CDN, and 3.9 % of the embedded sites.
The largest contributing subset of non-monotonic timestamps are CDN sites –
as we might expect due to the various forms of load balancing inherent in CDN
architectures. A non-trivial fraction of non-CDN and CDN sites have missing
timestamps. We learned via personal communication with an operator that miss-
ing timestamps in one case were due to a front-end load balancing device; similar
middlebox issues [4] likely cause the missing timestamps observed here.
Among the sibling and non-sibling populations, we examine the origin AS
of the preﬁxes to which the addresses belong from the routeviews [12] BGP
table. The origin AS of the corresponding IPv4 and IPv6 addresses of a website
allow us to determine whether non-siblings are within the same network, if not
the same host. As shown in Table 4, 21.8 % of the non-siblings in our non-CDN
dataset are in diﬀerent ASes, as compared to 10 % of the siblings. Siblings may
be in diﬀerent ASes when an organization uses IPv6 tunnels or a diﬀerent AS for
IPv6. By contrast, 97.3 % of the inferred siblings among the embedded sites are
160
R. Beverly and A. Berger
Table 4. Alexa Machine-Sibling AS Agreement
Inference
Siblings
Fraction of matching ( I 4, I 6) ASNs
non-CDN CDN Embedded
90.0 %
83.6 % 97.3 %
Non-Siblings 78.2 %
51.0 % 87.1 %
Unknown
91.6 %
62.3 % 78.0 %
within the same AS. Only 51 % of the non-siblings among the CDN sites reside
within the same AS. Manual investigation of some of the siblings in diﬀerent
ASes reveals that the ASes belong to the same organization.
5 Conclusions and Future Work
We developed, validated, and applied a method for using TCP-layer ﬁngerprint-
ing techniques to identify IPv4 and IPv6 addresses that belong to the same
machine. By combining coarse and ﬁne-grained TCP-layer ﬁngerprinting, we
identify server “siblings.” We can imagine several other applications of sibling
interface identiﬁcation: predicting correlated failures or similar behaviors under
attack (and whether the IPv4 and IPv6 interfaces share fate); IPv6 geolocation
that leverages knowledge of the corresponding IPv4 address; and comparing IPv4
and IPv6 path performance, by providing certainty as to whether a measurement
end-point is common; and more generally, understanding how IPv6 and IPv4 net-
work infrastructures are co-evolving at a macroscopic level. Although we applied
our technique to web servers, it generalizes to any device with a listening TCP
service, including DNS, email, and peer-to-peer services.
Although our technique validated surprisingly well for our diverse set of
ground truth, we see at least three areas for improvement. First, the optional
enhancement algorithm (Algorithm 2) we used to classify problematic cases con-
tains parameters and thresholds that may overﬁt our data. A larger ground-truth
dataset would support further reﬁnement and higher conﬁdence in our inferences.
Second, although we detect certain instances of TCP load-balancing by observ-
ing multiple monotonic sequences with diﬀerent initial oﬀsets, it would be better
to use reverse-proxy detection techniques to discern cases where a TCP-splitting
proxy sits in front of the interrogated web server.
Last, our preliminary sensitivity results show that our inferences are stable
even with fewer data points and over shorter time frames. Our technique can
make some sibling inferences quickly, with only a few TCP observations, whereas
others require samples across longer time periods. We leave a complete temporal
sensitivity analysis to future work.
Acknowledgments. Thanks to kc claﬀy, Justin Rohrer, Nick Weaver, and Geoﬀrey
Xie for invaluable feedback. This work supported by in part by NSF grant CNS-1111445
Server Siblings: Identifying Shared IPv4/IPv6 Infrastructure
161
and Department of Homeland Security (DHS) S&T contract N66001-2250-58231. Views
and conclusions are those of the authors and should not be interpreted as representing
the oﬃcial policies, either expressed or implied, of the U.S. government.
References
1. Alexa: Top 1,000,000 sites (2014). http://www.alexa.com/topsites
2. Berger, A., Weaver, N., Beverly, R., Campbell, L.: Internet nameserver IPv4 and
IPv6 address relationships. In: Proceedings of the ACM Internet Measurement
Conference. pp. 91–104 (2013)
3. Claﬀy, K.: Tracking IPv6 evolution: data we have and data we need. SIGCOMM
Comput. Commun. Rev. 41(3), 43–48 (2011)
4. Craven, R., Beverly, R., Allman, M.: A middlebox-cooperative TCP for a non
end-to-end internet. In: Proceedings of ACM SIGCOMM, pp. 151–162 (2014)
5. Czyz, J., Allman, M., Zhang, J., Iekel-Johnson, S., Osterweil, E., Bailey, M.: Mea-
suring IPv6 adoption. In: Proceedings of ACM SIGCOMM, pp. 87–98 (2014)
6. Dhamdhere, A., Luckie, M., Huﬀaker, B., Elmokashﬁ, A., Aben, E., et al.: Measur-
ing the deployment of IPv6: topology, routing and performance. In: Proceedings
of the ACM Internet Measurement Conference, pp. 537–550 (2012)
7. Heuse, M.: Recent advances in IPv6 insecurities. In: Chaos Communications
Congress (2010)
8. Jacobson, V., Braden, R., Borman, D.: TCP Extensions for High Performance.
RFC 1323 (May 1992)
9. Kohno, T., Broido, A., Claﬀy, K.C.: Remote physical device ﬁngerprinting. In:
Proceedings of IEEE Security and Privacy, pp. 211–225 (2005)
10. Lyon, G.F.: Nmap Network Scanning: The Oﬃcial Nmap Project Guide to Network
Discovery and Security Scanning (2009)
11. Maxmind: IP Geolocation (2014). http://www.maxmind.com
12. Meyer, D.: University of Oregon RouteViews (2014). http://www.routeviews.org
13. Moon, S., Skelly, P., Towsley, D.: Estimation and removal of clock skew from
network delay measurements. In: Proceedings of INFOCOM, vol. 1 (Mar 1999)
14. Ripe, NCC: World IPv6 day measurements (2011). http://v6day.ripe.net
15. Sarrar, N., Maier, G., Ager, B., Sommer, R., Uhlig, S.: Investigating IPv6 Traﬃc.
In: Taft, N., Ricciato, F. (eds.) PAM 2012. LNCS, vol. 7192, pp. 11–20. Springer,
Heidelberg (2012)
16. Silbersack, M.J.: Improving TCP/IP security through randomization without sac-
riﬁcing interoperability. In: Proceedings of BSDCan (2006)
17. Zander, S., Andrew, L.L., Armitage, G., Huston, G., Michaelson, G.: Mitigating
sampling error when measuring internet client IPv6 capabilities. In: Proceedings
of the ACM Internet Measurement Conference, pp. 87–100 (2012)