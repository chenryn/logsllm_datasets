title:A recurrence-relation-based reward model for performability evaluation
of embedded systems
author:Ann T. Tai and
Kam S. Tso and
William H. Sanders
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
A Recurrence-Relation-Based Reward Model for Performability Evaluation of
Embedded Systems
Ann T. Tai Kam S. Tso
IA Tech, Inc.
Los Angeles, CA 90094
{a.t.tai,k.tso}@ieee.org
William H. Sanders
University of Illinois
Urbana, IL 61801
PI:EMAIL
Abstract
Embedded systemsfor closed-loop applications often be(cid:173)
have as discrete-time semi-Markov processes (DTSMPs).
Performability measures most meaningful to iterative em(cid:173)
bedded systems, such as accumulated reward, are thus dif(cid:173)
ficult to solve analytically in general.
In this paper, we
propose a recurrence-relation-based (RRB) reward model
to evaluate such measures. A critical element in RRB re(cid:173)
ward models is the notion of state-entry probability. This
notion enables us to utilize the embedded Markov chain in
a DTSMP in a novel way. More specifically, we formulate
state-entry probabilities, state-occupancy probabilities, and
expressions concerning accumulated reward solely in terms
of state-entry probability and its companion term, namely
the expected accumulated reward at the point of state en(cid:173)
try. As a result, recurrence relations abstract away all the
intermediate points that lack the memoryless property, en(cid:173)
abling a solvable model to be directly built upon the em(cid:173)
bedded Markov chain. To show the usefulness of RRB re(cid:173)
ward models, we evaluate an embedded system for which
we leverage the proposed notion and methods to solve a va(cid:173)
riety ofprobabilistic measures analytically.
1. Introduction
Discrete-time performability analysis in terms of accu(cid:173)
mulated reward is well-suited to modeling of embedded sys(cid:173)
tems, because they typically execute in open or closed loops
or cycles 1
, each of which accommodates at most one control
command update. Further, the notions of "return" and "duty
cycle" [1] (the latter refers to the proportion of time a sys(cid:173)
tem actively makes progress toward its task goal) promote
the measures concerning expected accumulated reward.
Nonetheless, performability modeling for those systems
can be difficult. In particular, while embedded systems have
relatively simple architectures and functionalities, the be(cid:173)
havior of an embedded system is often non-Markovian in
nature. For example, an application may require its host to
1In the remainder of the text, the words "loop," "cycle," "frame," and
"iteration" are used interchangeably.
be engaged in a specific operation through a pre-designated
time interval or to take a particular action with a specified
frequency, which implies a deterministic sojourn time or a
nonstationary transition probability, respectively.
While those non-Markovian properties can be circum(cid:173)
vented using the notion of embedded Markov chain (as
such a process behaves just like an ordinary Markov pro(cid:173)
cess at the points of state transition), performability mea(cid:173)
sures based on accumulated reward can still be difficult to
solve analytically. In addition, embedded applications may
involve time- or path-dependent behavior, which may pre(cid:173)
vent a reward model from being analytically manageable.
Prior work in solving reliability, performance, and per(cid:173)
formability measures for discrete-time semi-Markov pro(cid:173)
cesses (DTSMPS) relied largely on simulation tools (see
[2], for example), empirical, measurement-based estima(cid:173)
tion (see [3], for example), nonparametric approach (see
[4], for example), and numerical approximation (see [5],
for example). Efforts for analytic solutions of DTSMP
were significantly less and were generally limited to mod(cid:173)
els with restrictive assumptions (see [6], for example).
Although simulation-based modeling tools, empirical ap(cid:173)
proaches, and numerical-approximation methods are flex(cid:173)
ible and powerful, they may lose their advantages when
a user desires to obtain insights from explicit expressions,
such as a reachability graph and a symbolic solution which
are unlikely to be supplied by a simulation or numerical(cid:173)
approximation tool.
In our earlier efforts, we leveraged recurrence relations
for reliability assessment of a fault-tolerant bus architecture
for an avionics system [7]. In addition, we used recurrence(cid:173)
relations to evaluate a distributed embedded system [8]. Re(cid:173)
cently, we have revisited the recurrence-relation-based ap(cid:173)
proach and generalized it so that the framework can be ap(cid:173)
plied to an important class of embedded systems, namely
closed-loop or iterative applications. The central purpose is
to enable a system or a design to be evaluated in a model(cid:173)
based way to complement the methods for simulation-based
or testbed-enabled assessments.
For this purpose, we introduce a recurrence-relation(cid:173)
based (RRB) reward model in this paper. A critical element
1-4244-2398-9/08/$20.00 Â©2008 IEEE
532
DSN 2008: Tai et al.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
in RRB reward models is the notion of state-entry probabil(cid:173)
ity. Informally speaking, it is the probability that the system
in question will enter into a specific state at a particular cy(cid:173)
cle. This notion enables us to utilize the embedded Markov
chain in a DTSMP in a novel way. More specifically, in
a recurrence relation derivation, we formulate state-entry
probabilities themselves and state-occupancy probabilities
solely in terms of state-entry probability. Likewise, we let
the expressions concerning expected accumulated reward
be formulated solely in terms of state-entry probability and
its companion term, namely expected reward at the point
of state entry. As a result, recurrence relations naturally
abstract away all the intermediate points (that constitute a
deterministic sojourn time or a fixed interval between two
consecutive occurrences of a periodic event) that lack the
memoryless property, enabling a solvable model to be di(cid:173)
rectly built upon the embedded Markov chain.
The remainder of the paper is organized as follows. Sec(cid:173)
tion 2 provides background information on closed-loop em(cid:173)
bedded systems and discusses the feasibility of using an
RRB approach. Section 3 introduces the notion of state(cid:173)
entry probability and describes how it enables the construc(cid:173)
tion and solution of an RRB model. Section 4 presents a
sample application to exemplify the applicability of RRB
reward models to embedded systems and the types of mea(cid:173)
sure that can be solved analytically using such a reward
model. We conclude the paper in Section 5.
2. System Model & RRB Approach Feasibility
A majority of embedded systems make control, com(cid:173)
mand, and other types of dynamic decisions. Those deci(cid:173)
sions are usually made by a software subsystem and based
on the feedback from the underlying hardware subsystem.
Typically, a sample from the sensors represents position,
voltage, temperature, or other parameters. Each sample
provides the software subsystem with updated information
which is called feedback. Systems that make use of feed(cid:173)
back are called closed-loop embedded systems. Familiar
examples of closed-loop embedded systems include those
driving thermostats and automobile cruise control. If feed(cid:173)
back indicates that the room temperature is below the de(cid:173)
sired setpoint, the thermostat will tum the heater on until
that temperature is reached. Similarly, if a car is going too
quickly, the cruise-control system can temporarily reduce
the amount of fuel fed to the engine.
In both cases, the
feedback enables compensations for disturbances to the sys(cid:173)
tem (such as changes in the outdoor temperature and in the
roughness or steepness of the road).
Sophisticated embedded applications include robotics
systems that service outer-planet exploration and make au(cid:173)
tonomous decisions to ensure the accomplishment of a mis(cid:173)
sion. For such systems, the control basis provides a dis(cid:173)
crete state representation that reflects the status of a mission
task. Closed-loop controllers ensure asymptotically stable
system behavior that is robust to local perturbations, while
perturbations are learned from the feedback. Moreover,
action-selection policies are often defined on a Markov de(cid:173)
cision process to find and track a visual feature. In general,
the objective of closed-loop system operation is to minimize
control inaccuracy and the gap between the system's actual
behavior and its functional specification.
Closed-loop embedded systems typically operate at a
fixed frequency. The frequency of command update that
drives the hardware subsystem usually follows the sampling
rate (Le., bounded above by the sampling rate). After read(cid:173)
ing the most recent sample from the sensors, the software
subsystem reacts to the learned system-state change by cal(cid:173)
culating the required adjustments and issuing a new com(cid:173)
mand. The hardware subsystem then responds to the new
command, followed by another sample-taking. Then the cy(cid:173)
cle repeats. Eventually, when the system reaches a desired
state or its mission period ends, the software subsystem will
cease calculating and issuing commands.
Moreover, in advanced closed-loop embedded systems,
reinforced learning is achieved by letting a software agent
that makes decisions for adaptation see a "return" for each
cycle. The return has a numerical value that encodes the ex(cid:173)
tent of the success (or failure) of an action and is learned by
the agent in retrospect. Accordingly, the agent will become
increasingly capable of selecting actions that maximize the
accumulated return over time.
Together with the notions of duty cycle and non(cid:173)
Markovian behavior mentioned in Section I, the above sys(cid:173)
tem properties can be summarized as follows:
PI) The frequency of decision/action is bounded above by
the sampling rate, which implies that each cycle ac(cid:173)
commodates at most one state transition.
P2) Each of the successive iterations takes the most recent
feedback from sensors as the parameters for calculat(cid:173)
ing the required adjustments for the subsequent opera(cid:173)
tion cycle.
P3) The notion of duty cycle can be adopted to quantify the
extent of a mission's success.
Those system properties collectively suggest a mathe(cid:173)
matical model for iterative embedded systems. Specifically,
coupled with embedded systems' non-Markovian behavior,
PI suggests that a closed-loop system can be represented
as a discrete-time semi-Markov process; P2 implies that
an efficient way to model a system's path-/time-dependent
behavior or error propagation is to leverage recurrence re(cid:173)
lations that allow systematic backward tracking; and P3
means that a modeling framework for iterative embedded
systems should explicitly support the solution of accumu(cid:173)
lative reward and time-averaged accumulative reward for
quantifying an embedded system's gracefully degradable
performance. Based on those observations, we elaborate
our RRB approach in the following section.
1-4244-2398-9/08/$20.00 Â©2008 IEEE
533
DSN 2008: Tai et al.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
3. RRB Reward Model
3.1. Formulation of State-Entry Probability
Letting M and II denote the system in question an2 its
state space, respectively, then state-entry probability Pk [i]
can be defined as follows:
Definition 1 Pk[i] is the probability that M will enter 8k
in the ith cycle from {8j
I j E TI - {k} }.
Without losing generality, the transition probability Pj k for
each of the states j that is not connected to 8k is considered
to have a value of zero. Further, we use basic DTSMP to
refer to the type of DTSMP in which the sojourn time of
each state either has a geometric distribution or is determin(cid:173)
istic. Note that per its definition, transition probabilities of
a basic DTSMP are stationary.
The notion of state-entry probability plays a key role in
simplifying model construction and solution for DTSMPs.
Consider the very simple DTSMP depicted in Figure 1. In
the state diagram, we use a shaded oval that is marked 8 k
and has a nearby label mk to denote a state 8 k that has a
deterministic sojourn time mk.
In addition, an unshaded
oval represents a state with geometrically distributed so(cid:173)
journ time. (This convention is used in the remainder of the
text.) Hence, for the system shown in Figure 1, 8 0 's sojourn
time has a geometric distribution, while the sojourn time of
81 is deterministic and is quantified in cycles, namely mI.
We can thus formulate state-entry probabilities by deriving
recurrence relations:
Po[i]
P1 [i]
P1 [i - ml]
i-I
LPOIPOoi- n- 1Po[n]
n=O
The first expression is derived based on the observation
that M will enter 80 in cycle i if and only if it entered 81
exactly ml cycles ago, as S 1 has a deterministic sojourn
time mI. The second expression must hold since we need
to consider all the mutually exclusive paths that lead M to
the most recent transition to 8 0 prior to the transition to 8 1 .
&. ~01 â¢ â¢(m1)
Figure 1. A Simple DTSMP
Theorem 1 In a basic DTSMp, state-entry probability
Pk [i] can be expressed solely in terms ofstate-entry proba(cid:173)
bilities as follows:
Pk[i] ==
L
Pjk Pj[i-mj]+ L
i-I
LPjkPjji-n-l Pj[n]
jEIId-{k}
jEII g -{k} n=O
(1)
Proof. The criterion for the correctness of Eq. (1) is that
it must exhaustively enumerate all the mutually exclusive
paths through which M will enter 8k from 8j
in the ith
cycle. Since the sojourn time of each state either has a geo(cid:173)
metric distribution or is deterministic, the two terms satisfy
the criterion at the top level, as they cover the transitions to
8k from the origin states of both types.
second
as
o
"
==
(~ )
"z-1
In addition, the first term ofEq. (1) exhaustively enumer(cid:173)
ates the mutually exclusive paths for which the origin state
has a deterministic sojourn time, because if a transition to
8 j occurs in cycle i - mj + n, 0 < n ::; i - mj, then
---+ 8k will not happen in cycle i. Con(cid:173)
the transition 8 j
versely, if a transition to 8 j occurs in cycle i - mj - n,
o < n ::; i - mj, then a transition 8j
---+ 8x , x =1= j, must
occur prior to cycle i. Yet another transition must occur to
bring M back to 8j in the (i - mj )th cycle. We note that
this path is already covered by the first term of Eq 0 (1) such
that M will enter 8k in cycle i.
Furthermore, because Pj[i]
l:~=oPjji-npj[n],
the
re-
term in Eq.
can
_
L..JjEIIg-{k} Pjk L..Jn=O Pjj
-
wntten
E "EII -{k} PjkPj [i - 1]. Then it is clear that the second
terin e~haustively covers the mutually exclusive paths via
which M enters 8k in cycle i (from the states that have
Q.E.D.
geometrically distributed sojourn times).
From Eq. (1), it follows that Pk [i] can be solved and
quantified when state-entry probabilities for all the states
are initialized. Moreover, Eq. (1) reveals the essence of our
approach. In particular, when a recurrence relation involves
a transition for which the origin state Sj has a deterministic
sojourn time, we let the recursive function for 8 j take mul(cid:173)
tiple steps backward in a single (computation) iteration to
skip all the intermediate points that constitute the determin(cid:173)
istic sojourn time and thus lack the memoryless property.