0.00026
0.99133
0.99185
0.99240
0.99252
0.00014
0.99211
0.99254
0.99272
0.99108
0.00047
0.99016
0.99112
0.99207
0.00748
0.00014
0.00728
0.00746
0.00789
0.00892
0.00047
0.00793
0.00888
0.00984
Table 14: Results for LOGO CV for mAGDs of single
DGAs grouped by seed using SVMs. In total, 150 sets of
30 DGAs were considered.
Table 18: Classiﬁcation accuracy for training on RWTH
Aachen data and prediction on Siemens data using
SVMs.
ACC
TPR
TNR
FNR
FPR
0.99464
0.00017
0.99430
0.99468
0.99492
0.99148
0.00056
0.99037
0.99156
0.99245
0.99779
0.00037
0.99721
0.99784
0.99854
0.00852
0.00056
0.00755
0.00844
0.00963
0.00221
0.00037
0.00146
0.00216
0.00279
x
σ
xmin
˜x
xmax
Table 15: Results for detecting mAGDs with SVMs of
arbitrary mixed DGAs using 5 repetitions of 5-fold CV
for each set. In total, 20 sets were considered.
ACC
TPR
TNR
FNR
FPR
0.97972
0.00041
0.97894
0.97967
0.98073
0.96195
0.00056
0.96088
0.96207
0.96304
0.99746
0.00040
0.99672
0.99747
0.99839
0.02635
0.00061
0.02517
0.02622
0.02751
0.00254
0.00040
0.00161
0.00253
0.00328
x
σ
xmin
˜x
xmax
Table 16: Results for LOGO CV for sets of mAGDs of
mixed DGAs grouped by DGA using SVMs. In total, 20
sets were considered.
ACC
TPR
TNR
FNR
FPR
0.99394
0.00031
0.99327
0.99402
0.99436
0.99331
0.00070
0.99135
0.99341
0.99425
0.99456
0.00047
0.99371
0.99451
0.99533
0.00669
0.00070
0.00575
0.00659
0.00865
0.00544
0.00047
0.00467
0.00549
0.00629
x
σ
xmin
˜x
xmax
Table 17: Results for classifying mAGDs of arbitrary
mixed DGAs and bNXD from Siemens applying 5 rep-
etitions of 5-fold CV for 20 sets each of size 100,000
using SVMs.
ACC
TPR
TNR
FNR
FPR
0.99448
0.00017
0.99419
0.99447
0.99479
0.99412
0.00017
0.99387
0.99415
0.99442
0.99485
0.00033
0.99432
0.99483
0.99559
0.00588
0.00017
0.00558
0.00585
0.00613
0.00515
0.00033
0.00441
0.00517
0.00568
x
σ
xmin
˜x
xmax
Table 19: Classiﬁcation accuracy for training on Siemens
data and prediction on RWTH Aachen data using SVMs.
ACC
TPR
TNR
FNR
FPR
0.93683
0.00059
0.93565
0.93689
0.93778
0.98900
0.00049
0.98807
0.98913
0.99010
0.88465
0.00103
0.88269
0.88470
0.88629
0.01100
0.00049
0.00990
0.01087
0.01193
0.11535
0.00103
0.11371
0.11530
0.11731
x
σ
xmin
˜x
xmax
Table 20: Classiﬁcation accuracy for 5-fold CV on
successfully resolved domains and mAGDs of arbitrary
DGAs using SVMs.
B Grid Search Results
In this section, we present results for our grid search. To
reduce the number of grid searches that have to be per-
formed for the single-DGA detection, we only did one
grid search per DGA generation scheme as introduced in
the taxonomy by Plohmann et al. [14]. We performed all
grid searches on sets of size 20,000. To avoid overﬁtting
we performed grid searches on 6 independent sets for the
multi-DGA detection case. The ﬁnal parameter selection
for multi-DGA detection is based on mathematical con-
straints of the respective ML algorithm and on domain
knowledge on the classiﬁcation problem. The ML algo-
rithm parameters are named according to standard refer-
ences for SVMs [7] and RFs [6].
For RFs we performed one grid search per data set as
follows. Parameter T is an integer drawn uniformly at
random from [10,1000], where we considered 64 values
for T in total. As our feature vector is of length 44, F
is an integer selected from [2,44], where each possible
value is assigned to F. The impurity criterion i(N) is
1180    27th USENIX Security Symposium
USENIX Association
either Gini impurity or entropy impurity. This results in
64· 43· 2 = 5504 5-fold CVs in total per data set.
For SVMs we performed one grid search per data set
as follows. After some initial tests we ﬁxed the param-
eter range for C and γ to [2−16,23] and considered 80
values drawn logarithmically at random for both param-
eters. This results in 80 5-fold CVs for the linear kernel
and in 802 = 6400 5-fold CVs for the RBF kernel per
data set.
The following tables present the resulting best param-
eter choices according to the ACC.
Gen. Scheme
DGA
Kernel C
γ
Arithmetic
Hash
Wordlist
Permutation
Corebot
Dyre
Matsnu
VolatileCedar
linear
linear
linear
RBF
3.4669 —
0.0052 —
0.2289 —
0.0234
0.0327
ACC
0.9999
1.0
0.9999
1.0
Table 24: Best parameter choices depending on the type
of DGA for SVMs. The above parameters are used
among all experiments where single DGAs are consid-
ered and are applied depending on the DGA’s generation
scheme.
Set #
1
2
3
4
5
6
Final
i(N)
entropy
Gini
entropy
Gini
Gini
Gini
Gini
F
25
10
22
7
13
31
18
T
17
33
72
161
227
785
785
ACC
0.9981
0.9993
0.9983
0.9987
0.9984
0.9983
—
Table 21: Best parameter choices for independent data
sets of mixed DGAs for RFs. For the ﬁnal selection i(N)
is selected by majority vote. F is the arithmetic mean.
For T the maximum is chosen.
Gen. Scheme
DGA
Arithmetic
Hash
Wordlist
Permutation
Corebot
Dyre
Matsnu
VolatileCedar
i(N)
Gini
Gini
Gini
Gini
F
8
2
5
2
T
681
388
57
513
ACC
0.9999
1.0
0.9999
1.0
Table 22: Best parameter choices depending on the gen-
eration scheme of the DGA for RFs. The above parame-
ters are used among all experiments where single DGAs
are considered and are applied depending on the DGA’s
generation scheme.
Set #
Kernel
C
1
2
3
4
5
6
Final
RBF
linear
RBF
RBF
RBF
RBF
RBF
2.9423
0.1729
1.7844
2.9423
4.8517
5.7317
0.9160
γ
0.0198
—
0.0102
0.0234
0.0073
0.0751
0.0198
ACC
0.9992
0.9982
0.9985
0.9982
0.9982
0.9979
—
Table 23: Best parameter choices for independent data
sets of mixed DGAs for SVMs. For the ﬁnal selection
the kernel is selected by majority vote. C is selected as
median. γ is chosen as the arithmetic mean. Both only
among the RBF results.
USENIX Association
27th USENIX Security Symposium    1181