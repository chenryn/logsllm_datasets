title:Skill Squatting Attacks on Amazon Alexa
author:Deepak Kumar and
Riccardo Paccagnella and
Paul Murley and
Eric Hennenfent and
Joshua Mason and
Adam Bates and
Michael Bailey
Skill Squatting Attacks on Amazon Alexa
Deepak Kumar, Riccardo Paccagnella, Paul Murley, Eric Hennenfent, Joshua Mason,  
Adam Bates, and Michael Bailey, University of Illinois, Urbana-Champaign
https://www.usenix.org/conference/usenixsecurity18/presentation/kumar
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
ISBN  978-1-931971-46-1
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Skill Squatting Attacks on Amazon Alexa
Deepak Kumar Riccardo Paccagnella Paul Murley Eric Hennenfent
Joshua Mason Adam Bates Michael Bailey
University of Illinois Urbana-Champaign
Abstract
The proliferation of the Internet of Things has increased
reliance on voice-controlled devices to perform everyday
tasks. Although these devices rely on accurate speech-
recognition for correct functionality, many users experi-
ence frequent misinterpretations in normal use. In this
work, we conduct an empirical analysis of interpretation
errors made by Amazon Alexa, the speech-recognition en-
gine that powers the Amazon Echo family of devices. We
leverage a dataset of 11,460 speech samples containing
English words spoken by American speakers and identify
where Alexa misinterprets the audio inputs, how often,
and why. We ﬁnd that certain misinterpretations appear
consistently in repeated trials and are systematic. Next,
we present and validate a new attack, called skill squat-
ting. In skill squatting, an attacker leverages systematic
errors to route a user to malicious application without
their knowledge. In a variant of the attack we call spear
skill squatting, we further demonstrate that this attack can
be targeted at speciﬁc demographic groups. We conclude
with a discussion of the security implications of speech
interpretation errors, countermeasures, and future work.
1
Introduction
The popularity of commercial Internet-of-Things (IoT) de-
vices has sparked an interest in voice interfaces. In 2017,
more than 30 M smart speakers were sold [10], all of
which use voice as their primary control interface [28].
Voice interfaces can be used to perform a wide array of
tasks, such as calling a cab [11], initiating a bank trans-
fer [2], or changing the temperature inside a home [8].
In spite of
the growing importance of speech-
recognition systems, little attention has been paid to their
shortcomings. While the accuracy of these systems is
improving [37], many users still experience frequent mis-
interpretations in everyday use. Those who speak with ac-
cents report especially high error rates [36] and other stud-
ies report differences in the accuracy of voice-recognition
systems when operated by male or female voices [40, 46].
Despite these reports, we are unaware of any indepen-
dent, public effort to quantify the frequency of speech-
recognition errors.
In this work, we conduct an empirical analysis of inter-
pretation errors in speech-recognition systems and inves-
tigate their security implications. We focus on Amazon
Alexa, the speech-recognition system that powers 70%
of the smart speaker market [3], and begin by building a
test harness that allows us to utilize Alexa as a black-box
transcription service. As test cases, we use the Nation-
wide Speech Project (NSP) corpus, a dataset of speech
samples curated by linguists to study speech patterns [19].
The NSP corpus provides speech samples of 188 words
from 60 speakers located in six distinct “dialect-regions”
in the United States.
We ﬁnd that for this dataset of 11,460 utterances, Alexa
has an aggregate accuracy rate of 68.9% on single-word
queries. Although 56.4% of the observed errors appear
to occur unpredictably (i.e., Alexa makes diverse errors
for a distinct input word), 12.7% of them are systematic—
they appear consistently in repeated trials across multiple
speakers. As expected, some of these systematic errors
(33.3%) are due to words that have the same pronunciation
but different spellings (i.e., homophones). However, other
systematic errors (41.7%) can be modeled by differences
in their underlying phonetic structure.
Given our analysis of misinterpretations in Amazon
Alexa, we consider how an adversary could leverage these
systematic interpretation errors. To this end, we introduce
a new attack, called skill squatting, that exploits Alexa
misinterpretations to surreptitiously cause users to trigger
malicious, third-party skills. Unlike existing work, which
focuses on crafting adversarial audio input to inject voice
commands [15, 39, 42, 48, 49], our attack exploits intrin-
sic error within the opaque natural language processing
layer of speech-recognition systems and requires an ad-
versary to only register a public skill. We demonstrate
USENIX Association
27th USENIX Security Symposium    33
Figure 1: Example of an Alexa skill—Alexa skills are appli-
cations that can perform useful tasks based on voice input. For
example, the Lyft skill [7] allows users to request a ride by
saying “Alexa, ask Lyft for a ride.”
Figure 2: User-skill interaction in Alexa—A typical user in-
teraction with an Alexa skill, using an Echo device. In this
example, a user interacts with the Lyft skill to request a ride.
this attack in a developer environment and show that we
are able to successfully “squat” skills, meaning that Alexa
invokes the malicious skill instead of a user-intended tar-
get skill at least once for 91.7% of the words that have
systematic errors. We then consider how an adversary
may improve this attack. To this end, we introduce a vari-
ant of skill squatting, called spear skill squatting, which
exploits systematic errors that uniquely target individuals
based on either their dialect-region or their gender. We
demonstrate that such an attack is feasible in 72.7% of
cases by dialect-region and 83.3% of cases by gender.
Ultimately, we ﬁnd that an attacker can leverage sys-
tematic errors in Amazon Alexa speech-recognition to
cause undue harm to users. We conclude with a discussion
of countermeasures to our presented attacks. We hope
our results will inform the security community about the
potential security implications of interpretation errors in
voice systems and will provide a foundation for future
research in the area.
2 Background
2.1 Voice Interfaces
Voice interfaces are rooted in speech-recognition tech-
nology, which has been a topic of research since
the 1970s [26]. In recent years, voice interfaces have
become a general purpose means of interacting with com-
puters, largely due to the proliferation of the Internet of
Things. In many cases, these interfaces entirely supplant
traditional controls such as keyboards and touch screens.
Smart speakers, like the Amazon Echo and Google Home,
use voice interfaces as their primary input source. As of
January 2018, an estimated 39 M Americans 18 years or
older own a smart speaker [10], the most popular belong-
ing to the Amazon Echo family.
2.2 Amazon Alexa Skills
In this work, we focus on Amazon Alexa [14], the speech-
recognition engine that powers the Amazon Echo family
of devices, as a state-of-the-art commercial voice inter-
face. In order to add extensibility to the platform, Amazon
allows the development of third-party applications, called
“skills”, that leverage Alexa voice services. Many compa-
nies are actively developing Alexa skills to provide easy
access to their services through voice. For example, users
can now request rides through the Lyft skill (Figure 1)
and conduct everyday banking tasks with the American
Express skill [4].
Users interact with skills directly through their voice.
Figure 2 illustrates a typical interaction. The user ﬁrst
invokes the skill by saying the skill name or its associ-
ated invocation phrase (x). The user’s request is then
routed through Alexa cloud servers (y), which determine
where to forward it based on the user input (z). The
invoked skill then replies with the desired output ({),
which is ﬁnally routed from Alexa back to the user (|).
Up until April of 2017, Alexa required users to enable a
skill to their account, in a manner similar to downloading
a mobile application onto a personal device. However,
Alexa now offers the ability to interact with skills without
enabling them [32].
2.3 Phonemes
In this work, we consider how the pronunciation of a word
helps explain Alexa misinterpretations. Word pronuncia-
tions are uniquely deﬁned by their underlying phonemes.
Phonemes are a speaker-independent means of describ-
ing the units of sound that deﬁne the pronunciation of
a particular word. In order to enable text-based analy-
sis of English speech, the Advanced Research Projects
Agency (ARPA) developed ARPAbet, a set of phonetic
transcription codes that represent phonemes of General
American English using distinct sequences of ASCII char-
acters [30]. For example, the phonetic representation of
34    27th USENIX Security Symposium
USENIX Association
SkillServerAlexaAlexa, ask Lyft for a ride12“Ask Lyft fora ride”3“Ride”request intent4“Ride” requestreply5“Ride” requestreplyThe nearestLyft driver is…6Figure 3: Speech-to-Text Test Harness Architecture—By
building an experimental skill (called “Record This”), we are
able to use the Amazon Alexa speech recognition system as a
black box transcription service. In this example, the client sends
a speech sample of the word “apple” x, Alexa transcribes it for
the skill server y, which then returns the transcription as a reply
to Alexa z and back to the client {.
the word “pronounce” using the ARPAbet transcription
codes is P R AH N AW N S. For the scope of this work,
we deﬁne the phonetic spelling of a word as its ARPAbet
phonetic representation, with each ARPAbet character
representing a single phoneme. There are 39 phonemes
in the ARPAbet. We rely on the CMU Pronunciation Dic-
tionary [22] as our primary source for word to phonemes
conversion.
3 Methodology
In this section, we detail the architecture of our test har-
ness, provide an overview of the speech corpora used in
our analysis, and explain how we use both to investigate
Alexa interpretation errors.
3.1 Speech-to-Text Test Harness
Alexa does not directly provide speech transcriptions of
audio ﬁles. It does, however, allow third-party skills to
receive literal transcriptions of speech as a developer API
feature. In order to use Alexa as a transcription service,
we built an Alexa skill (called “Record this”) that records
the raw transcript of input speech. We then developed
a client that takes audio ﬁles as input and sends them
through the Alexa cloud to our skill server.
In order
to start a session with our Alexa skill server, the client
ﬁrst sends an initialization command that contains the
name of our custom skill. Amazon then routes all future
requests for that session directly to our “Record this” skill
server. Second, the client takes a collection of audio
ﬁles as input, batches them, and sends them to our skill
server, generating one query per ﬁle. We limit queries to a
maximum of 400 per minute in order to avoid overloading
Amazon’s production servers. In addition, if a request is
denied or no response is returned, we try up to ﬁve times
before marking the query as a failure.
Figure 4: Dialect-Regions in the U.S.—Labov et al.’s [31] six
dialect regions deﬁne broad classes of speech patterns in the
United States, which are used to segment Nationwide Speech
Project dataset.
Data Source
NSP
Forvo
Speakers Words
188
59,403
60
4,990
Samples
11,460
91,843
Table 1: Speech Sources —We utilize two speech databases,
the Nationwide Speech Project (NSP) and Forvo, to aid in our
analysis of Alexa misinterpretations. We use the NSP dataset
as our primary source for speech samples and the Forvo dataset
solely for cross-validation.
Figure 3 illustrates this architecture. For each audio ﬁle
sent from the client (x), Alexa sends a request to our skill
server containing the understood text transcription (y).
The server then responds with that same transcription (z)
through the Alexa service back to the client ({). The
client aggregates the transcriptions in a results ﬁle that
maps input words to their output words for each audio
sample.
3.2 Speech Corpora
In order to study interpretation errors in Alexa, we rely
on two externally collected speech corpora. A full break-
down of these datasets is provided in Table 1.
NSP
The Nationwide Speech Project (NSP) is an ef-
fort led by Ohio State University to provide structured
speech data from a range of speakers across the United
States [19]. The NSP corpus provides speech from a total
of 60 speakers from six geographical “dialect-regions”,
as deﬁned by Labov et al. [31]. Figure 4 shows each of
these speech regions—Mid-Atlantic, Midland, New Eng-
land, North, South, and West—over a map of the United
States. In particular, ﬁve male and ﬁve female speakers
from each region provide a set of 188 single-word record-
ings, 76 of which are single-syllable words (e.g. “mice”,
“dome”, “bait”) and 112 are multi-syllable words (e.g. “al-
falfa”, “nectarine”). These single-word ﬁles provide a
USENIX Association
27th USENIX Security Symposium    35
AlexaAudio Dispatcher Response AggregatorRecord ThisSkill Serveraudio:”apple”text:“apple”text: Alexagot “apple”text: Alexagot “apple”1234total of 11,460 speech samples for further analysis and
serve as our primary source of speech data. In addition,
NSP provides metadata on each speaker, including gender,
age, race, and hometown.
Forvo We also collect speech samples from the Forvo
website [6], which is a crowdsourced collection of pronun-
ciations of English words. We crawled forvo.com for
all audio ﬁles published by speakers in the United States,
on November 22nd, 2017. This dataset contains 91,843
speech samples covering 59,403 words from 4,991 speak-
ers. Unfortunately, the Forvo data is non-uniform and
sparse. 40,582 (68.3%) of the words in the dataset are
only spoken by a single speaker, which makes reasoning
about interpretation errors in such words difﬁcult. In addi-
tion, the audio quality of each sample varies from speaker
to speaker, which adds difﬁcult-to-quantify noise in our
measurements. In light of these observations, we limit our
use of these data to only cross-validation of our results
drawn from NSP data.
3.3 Querying Alexa
We use our test harness to query Alexa for a transcription
of each speech sample in the NSP dataset. First, we
observe that Alexa does not consistently return the same
transcription when processing the same speech sample.
In other words, Alexa is non-deterministic, even when
presented with identical audio ﬁles over reliable network
communication (i.e., TCP). This may be due to some
combination of A/B testing, system load, or evolving
models in the Alexa speech-recognition system. Since we
choose to treat Alexa as a black box, investigating this
phenomenon is outside the scope of this work. However,
we note that this non-determinism will lead to unavoidable
variance in our results. To account for this variance, we
query each audio sample 50 times. This provides us
with 573,000 data points across 60 speakers. Over all
these queries, Alexa did not return a response on 681
(0.1%) of the queries, which we exclude from our analysis.
We collected this dataset of 572,319 Alexa transcriptions
on January 14th, 2018 over a period of 24 hours.
3.4 Scraping Alexa Skills
Part of our analysis includes investigating how interpreta-
tion errors relate to Alexa skill names. We used a third-
party aggregation database [1] to gather a list of all the
skill names that were publicly available on the Alexa