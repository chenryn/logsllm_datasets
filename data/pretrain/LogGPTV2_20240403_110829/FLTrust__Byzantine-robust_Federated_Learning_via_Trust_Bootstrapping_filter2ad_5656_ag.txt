dataset, a service provider can collect a clean one by itself
with a small cost, e.g., asking its employees to generate and
manually label a clean root dataset.
Adaptive attacks and hierarchical root of
trust: We
considered an adaptive attack via extending the state-of-the-art
framework of local model poisoning attacks to our FLTrust.
We acknowledge that there may exist stronger local model
poisoning attacks to FLTrust, which is an interesting future
work to explore. Moreover, it is an interesting future work
to consider a hierarchical root of trust. For instance, the root
dataset may contain multiple subsets with different levels of
trust. The subsets with higher trust may have a larger impact
on the aggregation.
VIII. CONCLUSION AND FUTURE WORK
We proposed and evaluated a new federated learning
method called FLTrust to achieve Byzantine robustness against
malicious clients. The key difference between our FLTrust
and existing federated learning methods is that
the server
itself collects a clean small training dataset (i.e., root dataset)
to bootstrap trust in FLTrust. Our extensive evaluations on
six datasets show that FLTrust with a small root dataset
can achieve Byzantine robustness against a large fraction of
malicious clients. In particular, FLTrust under adaptive attacks
with a large fraction of malicious clients can still train global
models that are as good as the global models learnt by FedAvg
under no attacks. Interesting future work includes 1) designing
stronger local model poisoning attacks to FLTrust and 2)
considering a hierarchical root of trust.
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their constructive
comments. This work was supported in part by NSF grants
No. 1937786, 1943226, and 2110252, an IBM Faculty Award,
and a Google Faculty Research Award.
REFERENCES
[1] Federated
Learning:
Learn-
ing
[On-
line]. Available: https://ai.googleblog.com/2017/04/federated-learning-
collaborative.html
Collaborative
Centralized
Machine
Training
without
Data.
[2] Machine Learning Ledger Orchestration For Drug Discovery
(MELLODDY). [Online]. Available: https://www.melloddy.eu/
[3] Utilization
of
in
FATE
of
Credit
[On-
line]. Available: https://www.fedai.org/cases/utilization-of-fate-in-risk-
management-of-credit-in-small-and-micro-enterprises/
Management
Enterprises.
Micro
Small
Risk
and
in
[4] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A pub-
lic domain dataset for human activity recognition using smartphones.”
in ESANN, 2013.
[5] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How
to backdoor federated learning,” in AISTATS, 2020, pp. 2938–2948.
[6] M. Barborak, A. Dahbura, and M. Malek, “The consensus problem
in fault-tolerant computing,” ACM Computing Surveys (CSur), vol. 25,
no. 2, pp. 171–220, 1993.
15
[7] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing
federated learning through an adversarial lens,” in ICML, 2019, pp.
634–643.
[8] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support
vector machines,” in ICML, 2012.
[9] P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, and J. Stainer, “Machine
learning with adversaries: Byzantine tolerant gradient descent,” in NIPS,
2017.
[10] X. Cao, J. Jia, and N. Z. Gong, “Data poisoning attacks to local
differential privacy protocols,” arXiv preprint arXiv:1911.02046, 2019.
[11] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks
on deep learning systems using data poisoning,” in arxiv, 2017.
[12] Y. Chen, L. Su, and J. Xu, “Distributed statistical machine learning in
adversarial settings: Byzantine gradient descent,” in POMACS, 2017.
[13] M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and C.-J. Hsieh, “Query-
efﬁcient hard-label black-box attack: An optimization-based approach,”
in ICLR, 2019.
[14] A. Cheu, A. Smith, and J. Ullman, “Manipulation attacks in local
differential privacy,” arXiv preprint arXiv:1909.09630, 2019.
[15] M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local model poisoning
attacks to byzantine-robust federated learning,” in USENIX Security
Symposium, 2020.
[16] M. Fang, N. Z. Gong, and J. Liu, “Inﬂuence function based data
poisoning attacks to top-n recommender systems,” in Proceedings of
The Web Conference 2020, 2020, pp. 3019–3025.
[17] M. Fang, G. Yang, N. Z. Gong, and J. Liu, “Poisoning attacks to
graph-based recommender systems,” in Proceedings of the 34th Annual
Computer Security Applications Conference, 2018, pp. 381–392.
[18] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnerabili-
ties in the machine learning model supply chain,” in Machine Learning
and Computer Security Workshop, 2017.
[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
[20]
[21]
[22]
recognition,” in CVPR, 2016, pp. 770–778.
J. Jia, B. Wang, X. Cao, and N. Z. Gong, “Certiﬁed robustness of
community detection against adversarial structural perturbation via
randomized smoothing,” in Proceedings of The Web Conference 2020,
2020, pp. 2718–2724.
J. N. Kather, C.-A. Weis, F. Bianconi, S. M. Melchers, L. R. Schad,
T. Gaiser, A. Marx, and F. G. Z¨ollner, “Multi-class texture analysis in
colorectal cancer histology,” Scientiﬁc reports, vol. 6, p. 27988, 2016.
J. Koneˇcn´y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh,
and D. Bacon, “Federated learning: Strategies for improving communi-
cation efﬁciency,” in NIPS Workshop on Private Multi-Party Machine
Learning, 2016.
[23] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features
from tiny images,” 2009.
[24] Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,”
Available: http://yann. lecun. com/exdb/mnist, 1998.
[25] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, “Data poisoning attacks
on factorization-based collaborative ﬁltering,” in NIPS, 2016.
[26] L. Li, W. Xu, T. Chen, G. B. Giannakis, and Q. Ling, “Rsa: Byzantine-
robust stochastic aggregation methods for distributed learning from
heterogeneous datasets,” in AAAI, vol. 33, 2019, pp. 1544–1551.
[27] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” in NDSS, 2018.
[28] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in AISTATS, 2017.
[29] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The hidden vulner-
ability of distributed learning in byzantium,” in ICML, 2018.
[30] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli, “Towards poisoning of deep learning
algorithms with back-gradient optimization,” in AISec, 2017.
[31] L. Mu˜noz-Gonz´alez, K. T. Co, and E. C. Lupu, “Byzantine-robust
federated machine learning through adaptive model averaging,” arXiv
preprint arXiv:1909.05125, 2019.
[32] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein,
U. Saini, C. Sutton, J. D. Tygar, and K. Xia., “Exploiting machine
learning to subvert your spam ﬁlter,” in LEET, 2008.
[33] Y. Nesterov and V. Spokoiny, “Random gradient-free minimization of
convex functions,” vol. 17, no. 2. Springer, 2017, pp. 527–566.
[34] S. Rajput, H. Wang, Z. Charles, and D. Papailiopoulos, “Detox:
A redundancy-based framework for faster and more robust gradient
aggregation,” in NIPS, 2019, pp. 10 320–10 330.
[35] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao,
N. Taft, and J. Tygar, “Antidote: understanding and defending against
poisoning of anomaly detectors,” in ACM IMC, 2009.
[36] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” in NIPS, 2018.
[37] O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumitras, “When
does machine learning fail? generalized transferability for evasion and
poisoning attacks,” in USENIX Security Symposium, 2018.
[38] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks
against federated learning systems,” arXiv preprint arXiv:2007.08432,
2020.
[39] R. Vershynin, “Introduction to the non-asymptotic analysis of random
matrices,” arXiv preprint arXiv:1011.3027, 2010.
[40] M. J. Wainwright, “High-dimensional statistics: A non-asymptotic view-
point,” vol. 48. Cambridge University Press, 2019.
[41] B. Wang and N. Z. Gong, “Attacking graph-based classiﬁcation via
manipulating the graph structure,” in CCS, 2019.
[42] H. Xiao, K. Rasul, and R. Vollgraf. (2017) Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms.
[43] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “Dba: Distributed backdoor
attacks against federated learning,” in ICLR, 2020.
[44] C. Xie, S. Koyejo, and I. Gupta, “Zeno: Distributed stochastic gradient
descent with suspicion-based fault-tolerance,” in ICML, 2019, pp. 6893–
6901.
[45] G. Yang, N. Z. Gong, and Y. Cai, “Fake co-visitation injection attacks
to recommender systems,” in NDSS, 2017.
[46] H. Yang, X. Zhang, M. Fang, and J. Liu, “Byzantine-resilient stochas-
for distributed learning: A lipschitz-inspired
IEEE, 2019, pp. 5832–
tic gradient descent
coordinate-wise median approach,” in CDC.
5837.
[47] Z. Yang and W. U. Bajwa, “Byrdie: Byzantine-resilient distributed
coordinate descent for decentralized learning,” IEEE Transactions on
Signal and Information Processing over Networks, vol. 5, no. 4, pp.
611–627, 2019.
[48] D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, “Byzantine-robust
distributed learning: Towards optimal statistical rates,” in ICML, 2018.
[49] Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor attacks to graph
neural networks,” arXiv preprint arXiv:2006.11165, 2020.
A. Proof of Theorem 1
APPENDIX
Before proving Theorem 1, we ﬁrst restate our FLTrust
algorithm and prove some lemmas. We note that in our setting
where Rl = 1, only the combined learning rate α·β inﬂuences
FLTrust. Therefore, given a combined learning rate, we can
always set β = 1 and let α be the combined learning rate.
In this case, the local model update gi and server update g0
are equivalent to the gradients of the ith client and the server,
respectively. We denote by S the set of clients whose cosine
similarity ci is positive in the tth global iteration. Let ¯gi =
(cid:107)g0(cid:107)
, where i ∈ S. Then,
(cid:107)gi(cid:107) · gi and ϕi = ReLU (ci)
we can rewrite Equation (4) as:
(cid:80)
j∈S
cj
ReLU (cj ) = ci(cid:80)
s.t.(cid:88)
j∈S
i∈S
(cid:88)
i∈S
g =
ϕi¯gi,
ϕi = 1, 0  0 for i ∈ S; (b) is because
FLTrust normalizes the local model updates to have the same
magnitude as the server model update, i.e., (cid:107)¯gi(cid:107) = (cid:107)g0(cid:107); and
Proof: Since ∇F (w∗) = 0, we have the following:
(c) is because (cid:80)
iteration t ≥ 1:(cid:13)(cid:13)wt−1 − w∗ − α∇F (wt−1)(cid:13)(cid:13)
≤(cid:112)1 − µ2/(4L2)(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13) .
(cid:13)(cid:13)wt−1 − w∗ − α∇F (wt−1)(cid:13)(cid:13)2
=(cid:13)(cid:13)wt−1 − w∗ − α(cid:0)∇F (wt−1) − ∇F (w∗)(cid:1)(cid:13)(cid:13)2
=(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13)2
+ α2(cid:13)(cid:13)∇F (wt−1) − ∇F (w∗)(cid:13)(cid:13)2
− 2α(cid:10)wt−1 − w∗,∇F (wt−1) − ∇F (w∗)(cid:11) .
(cid:13)(cid:13)∇F (wt−1) − ∇F (w∗)(cid:13)(cid:13) ≤ L(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13) ,
F (w∗) +(cid:10)∇F (w∗), wt−1 − w∗(cid:11) ≤ F (wt−1)
F (wt−1) +(cid:10)∇F (wt−1), w∗ − wt−1(cid:11) ≤ F (w∗).
(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13)2
By Assumption 1, we have:
(25)
(26)
(27)
(28)
(29)
(30)
− µ
2
,
Substituting inequalities (28) and (31) into (27), we have:
Summing up inequalities (29) and (30), we have:
.
≤ − µ
2
(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13)2
(cid:10)w∗ − wt−1,∇F (wt−1) − ∇F (w∗)(cid:11)
(cid:13)(cid:13)wt−1 − w∗ − α∇F (wt−1)(cid:13)(cid:13)2
≤(cid:0)1 + α2L2 − αµ(cid:1)(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13)2
(cid:13)(cid:13)wt−1 − w∗ − α∇F (wt−1)(cid:13)(cid:13)2
≤(cid:0)1 − µ2/(4L2)(cid:1)(cid:13)(cid:13)wt−1 − w∗(cid:13)(cid:13)2
.
,
By choosing α = µ/(2L2), we have:
(31)
(32)
(33)
which concludes the proof.
Lemma
3.
∈
any
δ
√
∆1
=
∆3 =
2σ2
∆3 ≤ σ2
2/γ2, then we have:
Suppose
√
(0, 1)
2σ1
and
Assumption
2
any w ∈
(cid:112)(d log 6 + log(3/δ))/|D0|
(cid:112)(d log 6 + log(3/δ))/|D0|. If ∆1 ≤ σ2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≥ 2∆1
(cid:41)
(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)
∇h(Xi, w) − E [h(X, w)]
∇f (Xi, w∗) − ∇F (w∗)
Xi∈D0
(cid:40)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:40)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
Pr
Pr
|D0|
Xi∈D0
|D0|
≥ 2∆3(cid:107)w − w∗(cid:107)