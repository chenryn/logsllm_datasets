separate partitions. As with any partitioning, choosing a key that
nearly equally splits the universe of possible values is important.
In our experiments, we chose to partition the ξ‘ space along the
attributes associated with the operation name and user name. The
overall correctness of the algorithm is independent of the partition
keys used, and 1 to n partitions may be used for each attribute
depending on the size of the privilege space and available memory.
Each of these partitions is operated on in parallel when evaluat-
ing each rule of the policy. Unique hashes of the enumerated events
are used in order to deduplicate events which may be generated
by more than one rule. This partitioning and parallelization takes
place within lines 11-14 of Algorithm 2. We describe these opti-
mizations here because they are useful in speeding up and scaling
the algorithm when dealing with a large number of attribute:value
pairs, but we omit them from the pseudo-code of Algorithm 2 to
simplify its presentation.
5 RESULTS
We use the Receiver Operating Characteristic (ROC) curve to com-
pare the performance of various algorithms and parameters. The
ROC curve charts the trade-off between the TPR and FPR of a binary
classifier, with the ideal performance having a TPR value of one
and a FPR value of zero. Our charts also include the Area Under the
Curve (AUC), which measures the area underneath the ROC curve
and provides a single quantitative score that incorporates both F PR
and T PR as the weighting metrics being varied. The higher the
AUC score the better the classification performance.
First, we describe our dataset used for the experiments. Next we
present experimental results and analysis to justify our candidate
evaluation metric Cscor e, including a comparison of several meth-
ods for normalizing the CoveraдeRate variable. Then we examine
the effect of varying two adjustable input variables to the mining
algorithm: the length of the observation period (|LOBP |) and the
minimum support threshold (ϵ). Finally, we compare the perfor-
mance of our ABAC algorithm with that of an RBAC algorithm.
All charts presented in this section are based off of mining and
scoring the entire dataset |L| using the Algorithms 1 and 2. The
exact number of runs varies based on the observation period size
|LOBP | used and equals to |L| − |LOBP | + 1.
5.1 Dataset Description
We examine the performance of our ABAC policy generation al-
gorithm on a real-world dataset. Our dataset was provided by a
Software As A Service (SaaS) company that uses 77 different AWS
services [4] with an “allow all” style of RBAC policy. It consists of
4.7M user-generated AWS CloudTrail audit events [2] represent-
ing 16 months of audit data starting from March 2017 for 38 users.
CloudTrail logs the events of all AWS account activities, includ-
ing actions taken through the AWS Management Console, AWS
SDKs, command line tools, and other AWS services; it performs
the audit logging for all services at the platform and infrastructure
layers which are also the layers that AWS IAM (Identity and Access
Management) enforces access controls for [2]. Audit events are
logged by CloudTrail in JSON format, and can be easily parsed by
using any JSON library. Two audit event examples from the AWS
website [3] are provided in Appendix A. Note that we used user-
generated audit events only, filtering out those events generated
by non-person entities. Events generated by non-person entities
were very consistent, and it is easy to derive very low under- and
over-privilege security policies for them directly from audit logs
without using advanced methods. Minimizing the privilege assign-
ment errors for human users is much more challenging so we chose
to focus on human generated log events only.
The high degree of variability in user behavior is shown by the
statistics in Table 1 based on the first month, last month, and total
16 months of data. Users is the number of active users during that
time period. Unique Services Avg. is the average number of unique
services used by active users. Unique Actions Avg. is the average
number of unique actions exercised by active users, and Action
Actions, and Actions metrics to understand the variation between
individual users. For example, looking at both the Unique and
Avg. is the average of the total actions exercised by active users.
The standard deviation is provided for Unique Services, Unique
Actions, we observe that their standard deviation is higher than the
average for all time periods, indicating a high degree of variation
between the number of actions that users exercise.
From our initial dataset, we derive two privilege universes us-
ing our feature selection methodology (Section 4.3.1). ξ‘0.1 used
15 attributes and consisted of 510M unique attribute:value com-
binations. ξ‘0.005 used 40 attributes, 25 of which were resource
identifiers so the universe size varied between 1.5B and 8.6B unique
attribute:value combinations depending on the number of resources
used during the OBP and OPP periods. All the experiments in this
Table 1: 16-Month Total Usage of our Dataset
Metric
Users
Unique Services Avg.
Unique Services StdDev.
Unique Actions Avg.
Unique Actions StdDev.
 Actions Avg.
 Actions StdDev.
First Month
17
12.94
10.16
65.76
76.11
9138.35
20279.87
Last Month
26
12.11
9.98
62.92
73.30
9664.19
14124.15
16 Months
38
22.66
16.70
168.34
178.52
123659.82
235915.45
section use ξ‘0.1 except for Section 5.4 which uses ξ‘0.005. Our rule
mining algorithm (Algorithm 1) currently saves a generated policy
in JSON format. Appendix B provides an example rule of a policy.
5.2 Cscor e Analysis
We consider three criteria in the design and evaluation of the
Cscor e metric for selecting a single rule from many candidate
rules generated by the F P−дrowth algorithm during each itera-
tion of our rule mining algorithm. Criterion C1:AUC is the Area
Under the ROC Curve. Criterion C2:Smoothness means that T PR
values should increase monotonically as the F PR increases. Cri-
terion C3:Interpretability means that the effect of changing the
weighting variable should be predictable and easy to understand by
an administrator who uses the metric in a policy mining algorithm.
5.2.1 Evaluating Candidate Scoring Metrics. Our candidate scoring
metric Cscor e is presented in Section 4.1, λ−Distance is presented
in [12], and Qrul is presented in [21]. All these metrics use the
number of over-assignments and number of log entries covered with
a weighting variable for adjusting the importance between over-
assignments and coverage in their scoring of candidates. However,
these metrics differ in how they normalize these numbers (if at
all) and how they implement the weighting. The results of varying
the over-assignment weightings for these candidate evaluation
methods are shown in Figure 3.
Four distinct versions of the Qrul metric are included in Fig-
ure 3. In [21], the authors also described QrulFreq, a frequency
weighted variant of Qrul which should be a fairer comparison with
our frequency weighted policy scoring algorithm (Algorithm 2).
The authors of [20] provided their source code on their website, and
the scoring algorithms implemented in the source code for Qrul
and QrulFreq are slightly different from those presented in the
paper. Instead of using the number of privileges covered by a rule
out of the entire privilege universe ([[p]]) as the denominator for
the over-assignments side of the metric, the implemented metrics
instead use the number of privileges covered by a rule out of the log
entries not covered by other rules already in the policy (|[[p]]∩U P|).
These “as-implemented” metrics, QrulImpl and QrulFreqImpl, per-
form more favorably than their counterparts so we include them in
our comparison along with the versions documented in [21].
All the examined metrics performed relatively well with high
AUC values, but our Cscor e metric has the highest AUC value
thus being the most favorable metric per the criterion C1:AUC.
While we do not have a quantitative score for C2:Smoothness, it is
evident from Figure 3 that our Cscor e is much closer to a monotonic
function than the other metrics whose T PR values increase and
Figure 3: Comparison of Candidate Evaluation Metrics
decrease several times as F PR increases. The Qrul and QrulFreq
metrics are particularly poor in terms of smoothness as they both
have an inflection point near their weighting variable ω‘o = 1,
where increasing the weighting slightly after that point produces
instability in the resulting ROC curve. Furthermore, increasing the
weighting beyond a certain point causes the metric to only select
those candidate rules which have zero over-assignments, resulting
in the unterminated portion of the ROC curve for Qrul (QrulFreq
has a similar inflection point that is difficult to discern in Figure 3
at F PR = 0.0013).
Unlike the Qrul and λ−Distance metrics, Cscor e normalizes both
the number of logs covered and over-assignments into a ratio be-
tween [0, 1] before applying the weighting. This makes the weight-
ing variable independent of the size of the privilege universe and
number of log entries and thus easier to understand and apply.
In Figure 3, varying the ω weighting of Cscor e between ω = 1
10
and ω = 10 varies the charted F PR between F PR = 0.05 and
F PR = 0.998 at relatively even intervals. To achieve a similar
spread across the F PR scores with QrulFreqImpl and λ−Distance,
the weighting variable for those metrics must be varied between
1
100 and 1
2000. QrulImpl achieved the second highest AUC score
due to an unusually good score near F PR = 0.34, but assigning a
weighting to it with predictable results is difficult. For example, the
QrulImpl score at F PR = 0.34,T PR = 0.9998 was achieved with
100000, but the next score at F PR = 0.49,T PR = 0.9988 was
ω‘0 =
achieved with ω‘0 =
500000; this significant difference is difficult
to determine without the experimentation and consideration of
the privilege space and log sizes. Because of its predictability and
even distribution of results, Cscor e also best meets our evaluation
criterion C3:Interpretability.
1
1
5.2.2 Methods of Calculating CoverageRate. The CoveraдeRate
(Formula 3) of the Cscor e (Formula 5) is the number of log entries
covered by rule r normalized to the range [0, 1], so that it can be
compared with the weighted value of the OverPrivileдeRate (For-
mula 4) normalized to the same range. There are several possible
0.9860.9880.990.9920.9940.9960.998100.20.40.60.81TPRFPRC-Score         AUC=0.9993QrulImpl        AUC=0.9983QrulFreqImpl    AUC=0.9978λ-Distance      AUC=0.9972QrulFreq        AUC=0.9946Qrul            AUC=0.9922: The unique number of logs covered out of the set
: The frequency weighted number of logs covered out
: The unique number of logs covered out of the set
: The frequency weighted number of logs covered out
ways to compute such a coverage rate; however, it is not immedi-
ately clear which would perform the best without experimentation.
We consider four possible methods for computing CoveraдeRate
and analyze their performance here:
• |Luncov(r)|
|Luncov |
of the total number of uncovered logs.
• |{Luncov(r)}|
|{Luncov }|
of unique uncovered logs.
• |Luncov(r)|
|LO BP |
of the total number of logs in the observation period.
• |{Luncov(r)}|
|{LO BP }|
of unique log entries during the observation period.
The results of applying these four methods are presented in
Figure 4 with each method identified by its denominator. As ev-
ident in Figure 4, the |Luncov(r)|
|Luncov | method performed the best for
two of our criteria for selecting a candidate metric: C1:AUC and
C2:Smoothness. The frequency weighted methods |Luncov(r)|
|Luncov | and
|Luncov(r)|
performed about the same in terms of C3:Interpretability
|LO BP |
with ω = 1
10 resulting in scores in the upper-left most part of the fig-
ure. The methods using the number of unique log entries performed
less favorably in terms of C3:Interpretability with their upper-left
most points being reached near ω = 1
256, a value farther away from
1 and more difficult to find without experimentation. Fluctuations
in the trends of |{Luncov}|, |LOBP |, |{LOBP}| in Figure 4 demon-
strate why these methods are poor choices to use for calculating
CoverageRate, and are not due to differences in sampling methods
or frequency.
Figure 4: Comparison of CoverageRate Calculating Methods
5.3 Effect of Varying Algorithm Parameters
Besides the ω variable which is varied to generate the points along
all the ROC curves (except for the RBAC algorithm curve in Figure
7), two other parameters can be varied as inputs to Algorithm 1:
the threshold ϵ used by the F P−дrowth algorithm, and the length
of the observation period |LOBP |.
Figure 5: Performance as Itemset Frequency Varies
5.3.1 Effect of Varying Itemset Frequency Threshold. The minimum
support threshold (ϵ) is used to specify that a pattern is consid-
ered a “frequent” pattern if it occurs in >= ϵ of the examined
entries. Increasing ϵ causes fewer candidate patterns to be identi-
fied by the F P−дrowth algorithm. The results of varying ϵ between
[0.05, 0.1, 0.2, 0.3] are shown in Figure 5. For both ϵ = 0.2 and
ϵ = 0.3, we observe inflection points as ω decreases because a
lower ω value favors more granular rules in order to lower the
over-privilege rate; however, higher ϵ values result in fewer and
less granular patterns being identified by the F P−дrowth algorithm.
Stated another way, low ω values generally result in lower F PR
values, while high ϵ values generally result in higher F PR values.
The inflection points occur as a result of conflicting instructions
between low ω and high ϵ values.
Lower ϵ values generate more possible candidates to evaluate
and generally result in higher AUC scores as well. The trade-off
for more candidates however is an increase in run time. At ω = 1
10,
the average mining times for ϵ = 0.05, 0.1, 0.2, 0.3 were 29.8, 15.3,