### Domain Selection and Measurement Setup

To ensure a representative sample, we selected 5 website domains from each Alexa Toplist associated with the respective Top-Level Domains (TLDs) such as .us, .de, .co.uk, etc., resulting in a total of 50 region-focused domains. Sampling the entire 1 million domains is unnecessary because we repeat the measurements over a one-week period, and records are expected to be cached. The known instability of the Alexa Toplist [33] does not significantly impact our measurements. We constructed a list of 200 domains (from July 01, 2019) to investigate potential differences between more popular and less popular domains, as well as variations based on TLD and probe location. However, we did not find any significant deviations in response times regarding popularity rank or TLD. Therefore, we do not further distinguish between individual domains in our analysis.

With this setup, we collected measurements for approximately 90 million DNS requests from home probes (see Table 1).

### Reliability Analysis

We evaluated the reliability of Do53 and DoT by analyzing the failure rate, defined as the relative number of failed queries to the total number of queries. A query is considered failed if the domain lookup could not be sent to the resolver or if the probe did not receive a response. In both cases, the RIPE Atlas API returns an error. Table 1 shows the overall failure rates, as well as the failure rates by resolver, for both Do53 and DoT. We excluded 33 probes that failed nearly all of their DoT measurements from all subsequent analyses. Additionally, only 2,718 out of 3,200 home probes successfully received a Do53 response from local resolvers, indicating that the remaining probes cannot resolve a domain using a local resolver (but can with a public resolver). For DoT, only 13 probes received responses from their local resolver via DoT, meaning that DoT is supported by only 0.4% of local resolvers. These DoT-supporting local resolvers were found exclusively in EU (11 probes) and NA (2 probes). Therefore, we separated the queries to local resolvers (by probes with and without DoT-supported local resolvers) in Table 1 and this section.

#### Overall Failure Rates

The overall failure rate for Do53 is 7.9%, with individual failure rates of 0.8–1.5% for most resolvers. The overall failure rate for DoT is much higher at 22.9%, representing a difference of 15.0 percentage points (p.p.). However, the total failure rates are heavily influenced by a few resolvers with particularly high failure rates. For example, 98.2% of the Do53 requests to Neustar UltraRecursive fail, accounting for 76.4% of the Do53 failure rate. For DoT, UncensoredDNS accounts for 84.7% of all DoT failures with an individual failure rate of 97.2%; local resolvers with DoT support have an overall DoT failure rate of 39.4%.

Individually, the Do53 failure rate is between 0.8% and 2.5% for all public resolvers, excluding Neustar. Local resolvers encounter failures in 11.2% of cases (7.2% for probes with DoT-supported local resolvers).

We observed an increase in failure rates when moving from Do53 to DoT for all DoT resolvers. The increase ranges from 0.4 to 0.5 p.p. for Google and Cloudflare, 1.5 p.p. for Quad9, 9.4 p.p. for CleanBrowsing, and 95.7 p.p. for UncensoredDNS. Local resolvers with DoT support show an increase toward the higher end with 32.2 p.p. These numbers suggest that DoT support on the paths is still experimental and varies in terms of reliability.

#### Error Analysis

Regarding the specific error messages, most failures are attributed to timeouts (5 seconds), socket errors, and connect() errors (connection refused/reset, network unreachable). For Do53, nearly all failed requests toward Neustar (>99.9%) are due to timeouts. DoT measurements show a significant number of TUCONNECT errors, which are exclusive to DoT and suggest TLS negotiation errors. To further investigate, we counted the number of TUCONNECT errors for each combination of probe and public resolver, excluding UncensoredDNS due to its high overall failure rate (indicating server-side issues). For all combinations of 3,200 probes × 4 resolvers, we found repeated TUCONNECT errors for 33 probes across all resolvers, where the probes failed nearly all scheduled 1,400 DoT measurements (200 domains × 7 days). This indicates blackholing of DoT packets closer to these probes (home router or ISP network). Although the number of affected probes is negligible (≈1%), we excluded these 33 probes from the previous and following analyses. Further investigation of TUCONNECT errors revealed a higher number of probes failing nearly all DoT measurements for Cloudflare, affecting 99 probes. The differential of 66 probes between these two groups showed no errors for other resolvers, suggesting DoT blackholing closer to Cloudflare's anycast instances serving these probes, leading to a higher failure rate compared to other resolvers. CleanBrowsing, on the other hand, shows a similar failure rate regarding TUCONNECT errors as Google or Quad9; the majority of CleanBrowsing’s overall DoT failures (10.3%) stem from timeouts instead.

The higher failure rates for DoT compared to Do53 are less surprising, as DoT was only standardized in 2016 [19]. As such, DoT likely still faces issues with middleboxes along the path [16,29], which intervene with DoT packets (TCP/853) and result in timeouts.

#### Regional Comparison

To identify regional differences, Figure 1 depicts the failure rates of Do53 (top) and DoT (bottom) by resolver and continent. Most resolvers exhibit similar Do53 failure rates across all continents, ranging from roughly 0.3–3%. Local resolvers show significantly higher failure rates (5.7–13.6%), indicating that RIPE Atlas probes have less success in resolving domain names when using their local resolver (regardless of DoT support). Thus, Do53 resolutions are more reliable with public resolvers compared to local ones concerning RIPE Atlas measurements. Nevertheless, we find similarly high values for OpenNIC in SA (11.3%), and Cloudflare in AF (6.8%) and SA (10.3%). Neustar represents an outlier, as measurements fail in nearly all cases (95.6–98.9%). Probes in Oceania (OC) have the lowest failure rates for all resolvers when comparing different continents, with most resolvers having failure rates of at most 0.5%.

Regarding DoT, Google and Quad9 exhibit the lowest failure rates across all continents (<5% in most continents), although still higher than their respective Do53 failure rates. On the other hand, Cloudflare and CleanBrowsing show higher failure rates, especially in AF (9.8% and 31.1%) and SA (11.6% and 7.4%), with CleanBrowsing also having a high failure rate in EU (12.8%). Queries to UncensoredDNS fail in nearly all cases (92.7–99.1%). As multiple public DoT resolvers (even those with otherwise reliable services in other continents) have higher failure rates in AF and SA, these regions may be more heavily affected by ossification in terms of middleboxes. Local resolvers with DoT support also show high failure rates, with 40.3% in EU and 33.3% in NA. In total, this indicates that DoT reliability is highly dependent on the geographical location and the chosen DNS service.

### Response Times

We aggregated the measurements by grouping distinct tuples of probe and resolver and, for each group, determined the 5th percentile in terms of response time (i.e., one value for each probe-resolver tuple across all measurements). We chose the 5th percentiles to limit the analysis to responses for cached records, as those accumulate at the lower end of the distribution and represent best-case scenarios.

#### Background

Before discussing response times, we elaborate on a technical limitation regarding DoT: By design, a DoT client would first establish a TCP connection and TLS session with the recursive resolver, then keep this session alive to reuse it for resolutions of multiple domains. Thus, the added delay due to the TCP and TLS handshake RTTs only applies once for as long as the connection and session stay alive. For RIPE Atlas probes, however, DoT measurements do not keep the connection/session alive between different measurements, meaning the additional RTTs required for the TCP and TLS handshakes apply to every DoT measurement. We contacted the RIPE Atlas support regarding specific protocol details: RIPE Atlas probes do not use TCP Fast Open or other extensions, so establishing the TCP connection will add 1 RTT to the response time. Further, probes typically use TLS 1.2 (2 additional RTTs), though some probes may use TLS 1.3 (1 additional RTT); however, the DoT measurement results do not provide information about the used TLS version for validation. As such, DoT measurements include 3 additional RTTs (2 in the best case) on top of the DNS lookup (1 RTT).

Considering we focus on cached responses (5th percentiles, see above) exclusively in this section, we argue that the lookup times are negligibly small (since results are simply returned from the cache). Thus, the response times largely consist of the RTTs between probe and resolver. Consequently, Do53 measurements resemble roughly 1 RTT, which we consider as the baseline RTT (cf. overall response times below), whereas DoT measurements resemble roughly 4 RTTs in total, plus time for connection/session management and processing on both probe and resolver. For approximation, we calculate the ratio between the 5th percentiles of the DoT and Do53 response times per probe for each resolver, shown in Figure 2; the vertical dashed lines represent the outlined ratio of 4 RTTs to 1 RTT (i.e., DoT to Do53).

The minimum ratio across all resolvers is 3.11, suggesting the use of TLS 1.3 in these cases (1 RTT less than with TLS 1.2). However, these cases are rare (only four probe-resolver pairs), as the median ratio among the public resolvers is 10.5 (25th percentile 7.5). This suggests that, besides the approximately 4 RTTs required for the handshakes, most samples require at least around 4 more RTTs for processing of the DoT request on the probe and resolver side. However, this processing overhead for DoT measurements cannot be accurately determined, as probes record the total response time only and do not allow separation of different steps during the DoT lookup. Nevertheless, note that the handshake RTTs still account for a large fraction of the measured DoT response times overall. Recall that only 13 probes leverage DoT-supporting local resolvers, most of which have ratios toward the higher end (see Figure 2, bottom right) due to very low Do53 response times (<10 ms) and likely early-stage DoT implementations.