10
5
0
1
3 processes
5 processes
7 processes
9 processes
11 processes
failure detection timeout [ms]
10
(a) Mistake recurrence time TMR as a function of T
]
s
m
[
n
o
i
t
a
r
u
d
e
k
a
i
t
s
m
12
10
8
6
4
2
0
1
3 processes
5 processes
7 processes
9 processes
11 processes
failure detection timeout [ms]
10
(b) Mistake duration TM as a function of T
Figure 8. Quality of service metrics of the fail-
ure detector vs. the failure detection timeout
T . No failures occur.
4Note that we do not need to determine TMR (and TM ) precisely if
TMR is large, as the corresponding consensus latency values are nearly
constant at those values.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
latency [ms]
no crash
coordinator crash
participant crash
n = 3
n = 5
meas.
1.06
1.568
1.115
sim.
1.030
1.336
0.786
meas.
1.43
2.245
1.340
sim.
1.442
2.295
1.336
n = 7 n = 9 n = 11
meas. meas. meas.
2.00
3.27
3.469
2.739
1.811
3.049
2.62
3.101
2.400
Table 1. Latency values (ms) for various crash scenarios from measurements and simulations.
100
]
s
m
[
y
c
n
e
t
a
l
10
1
1
100
]
s
m
[
y
c
n
e
t
a
l
10
1
1
3 processes (exp.)
5 processes (exp.)
7 processes (exp.)
9 processes (exp.)
11 processes (exp.)
10
100
failure detection timeout [ms]
(a) measurements
3 processes (sim., det.)
3 processes (sim., exp.)
3 processes (exp.)
5 processes (sim., det.)
5 processes (sim., exp.)
5 processes (exp.)
10
100
failure detection timeout [ms]
(b) measurements vs. simulation (det=deterministic dis-
tribution; exp=exponential distribution, see Sect. 3.4)
Figure 9. Latency vs.
timeout T . No failures occur.
the failure detection
Latency. Figure 9 shows the latency results obtained from
both the measurements on the cluster and the simulations.
Each latency curve starts at very high values, and decreases
fast to the latency in the absence of suspicions. It is a de-
creasing curve, except for a small peak around T = 10 ms
at n = 5 and 7 in the measurement results. A possible ex-
planation for this peak is interference with the Linux sched-
uler, in which the basic scheduling time unit is 10 ms (note
also the behavior of the curves of the QoS metrics in Fig. 8
for T = 10 ms). The suspicions generated are likely to dif-
fer if the thread of the failure detector sleeps slightly more
than 10 ms or slightly less than 10 ms.
By comparing the simulation and the measurement re-
sults, it is possible to notice some quite relevant differences.
Actually, the SAN model is not able to perfectly catch the
inﬂuence of the failure detectors when wrong suspicions
are frequent (bad QoS). When the failure detectors’ QoS
is good — at high values for T — the results from the SAN
model and measurements match. As we said in Section 3.4,
each failure detector is assumed to be independent from the
others. In reality, in case of contention on the system re-
sources, there is likely to be correlation on false failure sus-
picions. The assumption of independence is thus not cor-
rect. The probability that two (or more) failure detectors
see the expiring/respect of the timeout is not just the prod-
uct of the individual probabilities of all failure detectors.
Hence there should be a correlation between the states of
all the failure detectors. Since correlation among failure de-
tectors is relevant for the behavior of the protocol, further
work will focus on accounting for that correlation, either by
characterizing the QoS of the failure detectors in more de-
tail and incorporating them into the model, or by modeling
in detail the message ﬂow of the failure detection algorithm.
6 Conclusion
In this paper we have applied a combined approach —
modeling based simulations and experimental measurements
— for the evaluation of the Chandra-Toueg ✸S consensus
algorithm (an algorithm that assumes an asynchronous sys-
tem augmented with failure detectors). We identiﬁed the
latency as a performance metric of interest, which reﬂects
how much time the algorithm needs to reach a decision. For
the failure detection we considered a simple heartbeat al-
gorithm, and we tried to abstract its behavior in terms of
appropriate quality of service (QoS) metrics. We investi-
gated the latency of the consensus algorithm in three classes
of runs, which differed in the behavior of the failure detec-
tors and with respect to the presence or absence of crashes.
These scenarios capture the most frequent operative situa-
tions of the algorithm.
We made measurements to determine input parameters
for the simulation model. Furthermore, a validation of the
adequacy and the usability of the simulation model has been
made by comparing experimental results with those obtained
from the model. This validation activity led us to determine
some limitations of the model (e.g., the assumption about
the independence of the failure detectors), and new direc-
tions for the measurements (e.g., extracting distributions for
the QoS metrics of failure detectors).
Our efforts opened many interesting questions and di-
rections, conﬁrming how wide the problem is of providing
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
quantitative analyses of distributed agreement algorithms.
The work presented can be extended by introducing new
performance metrics (e.g., throughput) and by investigating
more deeply the behavior of the algorithm under particular
conditions (e.g., transient behavior after crashes). Never-
theless, this work allowed us to gain insight on the behavior
of the model and the implementation of the ✸S consen-
sus algorithm, which will be useful for further reﬁnements.
In order to give a complete evaluation of the Chandra and
Toueg algorithm, i.e., to decide if it is a good one for a sys-
tem like ours from a quantitative perspective, it is necessary
to compare its performance with alternative solutions. This
is our future plan: we will analyze alternative protocols and
then we will be able to make statements about how good
the protocols are by comparing the results. Such a work
will also consolidate our framework for protocol analysis.
References
[1] M. Barborak, M. Malek, and A. Dahbura, “The consensus
problem in distributed computing,” ACM Computing Sur-
veys, vol. 25, pp. 171–220, June 1993.
[2] X. D´efago, A. Schiper, and P. Urb´an, “Totally ordered broad-
cast and multicast algorithms: A comprehensive survey,”
Tech. Rep. DSC/2000/036, ´Ecole Polytechnique F´ed´erale de
Lausanne, Switzerland, Sept. 2000.
[3] F. Cristian, R. de Beijer, and S. Mishra, “A performance com-
parison of asynchronous atomic broadcast protocols,” Dis-
tributed Systems Engineering Journal, vol. 1, pp. 177–201,
June 1994.
[4] F. Cristian, S. Mishra, and G. Alvarez, “High-performance
asynchronous atomic broadcast,” Distributed System Engi-
neering Journal, vol. 4, pp. 109–128, June 1997.
[5] P. Urb´an, X. D´efago, and A. Schiper, “Contention-aware
metrics for distributed algorithms: Comparison of atomic
broadcast algorithms,” in Proc. 9th IEEE Int’l Conf. on
Computer Communications and Networks (IC3N 2000), Oct.
2000.
[6] A. Coccoli, S. Schemmer, F. D. Giandomenico, M. Mock,
and A. Bondavalli, “Analysis of group communication pro-
tocols to assess quality of service properties,” in Proc. IEEE
High Assurance System Engineering Symp. (HASE’00), (Al-
buquerque, NM, USA), pp. 247–256, Nov. 2000.
[7] A. Coccoli, A. Bondavalli, and F. D. Giandomenico, “Analy-
sis and estimation of the quality of service of group commu-
nication protocols,” in Proc. 4th IEEE Int’l Symp. on Object-
oriented Real-time Distributed Computing (ISORC’01),
(Magdeburg, Germany), pp. 209–216, May 2001.
[8] H. Duggal, M. Cukier, and W. Sanders, “Probabilistic veri-
ﬁcation of a synchronous round-based consensus protocol,”
in Proc. 16th IEEE Symp. on Reliable Distributed Systems
(SRDS’97), (Durham, NC, USA), pp. 165–174, Oct. 1997.
[9] L. M. Malhis, W. H. Sanders, and R. D. Schlichting, “Numer-
ical evaluation of a group-oriented multicast protocol using
stochastic activity networks,” in Proc. 6th Int’l Workshop on
Petri Nets and Performance Models, (Durham, NC, USA),
pp. 63–72, Oct. 1995.
[10] N. Sergent, X. D´efago, and A. Schiper, “Impact of a fail-
ure detection mechanism on the performance of consensus,”
in Proc. IEEE Paciﬁc Rim Symp. on Dependable Computing
(PRDC), (Seoul, Korea), Dec. 2001.
[11] T. D. Chandra and S. Toueg, “Unreliable failure detectors for
reliable distributed systems,” J. ACM, vol. 43, pp. 225–267,
Mar. 1996.
[12] J. Turek and D. Shasha, “The many faces of consensus in
distributed systems,” IEEE Computer, vol. 25, pp. 8–17, June
1992.
[13] R. Guerraoui and A. Schiper, “The generic consensus ser-
vice,” IEEE Transactions on Software Engineering, vol. 27,
pp. 29–41, Jan. 2001.
[14] M. J. Fischer, N. A. Lynch, and M. S. Paterson, “Impossibil-
ity of distributed consensus with one faulty process,” J. ACM,
vol. 32, pp. 374–382, Apr. 1985.
[15] W. Chen, S. Toueg, and M. K. Aguilera, “On the quality
of service of failure detectors,” in Proc. Int’l Conf. on De-
pendable Systems and Networks (DSN), (New York, USA),
pp. 191–200, June 2000.
[16] F. B. Schneider, “Implementing fault-tolerant services us-
ing the state machine approach: a tutorial,” ACM Computing
Surveys, vol. 22, pp. 299–319, Dec. 1990.
[17] V. Hadzilacos and S. Toueg, “Fault-tolerant broadcasts and
related problems,” in Distributed Systems (S. Mullender,
ed.), ACM Press Books, ch. 5, pp. 97–146, Addison-Wesley,
second ed., 1993.
[18] P. Urb´an, X. D´efago, and A. Schiper, “Neko: A single en-
vironment to simulate and prototype distributed algorithms,”
in Proc. of the 15th Int’l Conf. on Information Networking
(ICOIN-15), (Beppu City, Japan), Feb. 2001. Best Student
Paper award.
[19] A. Movaghar and J. Meyer, “Performability modeling with
stochastic activity networks,” in Proc. Real-Time Systems
Symp., (Austin, TX, USA), Dec. 1984.
[20] J. F. Meyer, A. Movaghar, and W. H. Sanders, “Stochastic ac-
tivity networks: structure, behaviour and applications,” Proc.
International Workshop on Timed Petri Nets. Publ by IEEE,
New York, pp. 106–115, 1985.
[21] W. H. Sanders, W. D. O. II, M. A. Qureshi, and F. K. Widja-
narko, “The UltraSAN modeling environment,” Performance
Evaluation, vol. 24, no. 1-2, pp. 89–115, 1995.
[22] A. Coccoli, On Integrating Modelling and Experiments in
Dependability and Performability Evaluation of Distributed
Applications. PhD thesis, University of Pisa, Italy, 2002.
http://bonda.cnuce.cnr.it/Documentation/People/ACoccoli.html.
[23] K. Tindell, A. Burns, and A. J. Wellings, “Analysis of
hard real-time communications,” Real-Time Systems, vol. 9,
pp. 147–171, Sept. 1995.
[24] N. Sergent, Soft Real-Time Analysis of Asynchronous Agree-
ment Algorithms Using Petri Nets. PhD thesis, ´Ecole Poly-
technique F´ed´erale de Lausanne, Switzerland, 1998. Number
1808.
[25] D. Mills, “Network Time Protocol (version 3), speciﬁcation,
implementation and analysis,” IETF, Mar. 1992.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply.