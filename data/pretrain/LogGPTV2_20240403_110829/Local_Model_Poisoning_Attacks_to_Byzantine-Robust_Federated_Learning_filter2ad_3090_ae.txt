large negative impact on the error rate. Speciﬁcally, suppose
we have an aggregation rule. For each local model, we use
the aggregation rule to compute a global model A when the
local model is included and a global model B when the local
model is excluded. We compute the error rates of the global
models A and B on the validation dataset, which we denote as
EA and EB, respectively. We deﬁne EA − EB as the error rate
impact of a local model. A larger error rate impact indicates
Table 6: Defense results. The numbers are testing error rates.
The columns “Krum” and “Trimmed mean” indicate the at-
tacker’s assumed aggregation rule when performing attacks,
while the rows indicate the actual aggregation rules and de-
fenses. Partial knowledge attacks are considered.
No attack Krum Trimmed mean
Krum
Krum + ERR
Krum + LFR
Krum + Union
Trimmed mean
Trimmed mean + ERR
Trimmed mean + LFR
Trimmed mean + Union
Median
Median + ERR
Median + LFR
Median + Union
0.14
0.14
0.14
0.14
0.12
0.12
0.12
0.12
0.13
0.13
0.13
0.13
0.72
0.62
0.58
0.48
0.15
0.17
0.18
0.18
0.17
0.21
0.20
0.19
0.13
0.13
0.14
0.14
0.23
0.21
0.12
0.12
0.19
0.25
0.13
0.14
that the local model increases the error rate more signiﬁcantly
if we include the local model when updating the global model.
We remove the c local models that have the largest error rate
impact, and we aggregate the remaining local models to obtain
an updated global model.
Loss Function based Rejection (LFR): In this defense, we
remove local models based on their impact on the loss instead
of error rate for the validation dataset. Speciﬁcally, like the
error rate based rejection, for each local model, we compute
the global models A and B. We compute the cross-entropy
loss function values of the models A and B on the validation
dataset, which we denote as LA and LB, respectively. More-
over, we deﬁne LA − LB as the loss impact of the local model.
Like the error rate based rejection, we remove the c local
models that have the largest loss impact, and we aggregate
the remaining local models to update the global model.
Union (i.e., ERR+LFR): In this defense, we combine ERR
and LFR. Speciﬁcally, we remove the local models that are
removed by either ERR or LFR.
Defense results: Table 6 shows the defense results of ERR,
FLR, and Union, where partial knowledge attacks are con-
sidered. We use the default parameter setting discussed in
Section 4.1, e.g., 100 worker devices, 20% of compromised
worker devices, MNIST dataset, and LR classiﬁer. Moreover,
we sample 100 testing examples uniformly at random as the
validation dataset. Each row of the table corresponds to a
defense, e.g., Krum + ERR means that the master device uses
ERR to remove the potentially malicious local models and
uses Krum as the aggregation rule. Each column indicates the
attacker’s assumed aggregation rule when performing attacks,
e.g., the column “Krum” corresponds to attacks that are based
on Krum. We have several observations.
USENIX Association
29th USENIX Security Symposium    1635
First, LFR is comparable to ERR or much more effective
than ERR, i.e., LFR achieves similar or much smaller testing
error rates than ERR. For instance, Trimmed mean + ERR
and Trimmed mean + LFR achieve similar testing error rates
(0.17 vs. 0.18) when the attacker crafts the compromised
local models based on Krum. However, Trimmed mean +
LFR achieves a much smaller testing error rate than Trimmed
mean + ERR (0.12 vs. 0.21), when the attacker crafts the
compromised local models based on trimmed mean. Second,
Union is comparable to LFR in most cases, except one case
(Krum + LFR vs. Krum and Krum + Union vs. Krum) where
Union is more effective.
Third, LFR and Union can effectively defend against our
attacks in some cases. For instance, Trimmed mean + LFR
(or Trimmed mean + Union) achieves the same testing error
rate for both no attack and attack based on trimmed mean.
However, our attacks are still effective in other cases even if
LFR or Union is adopted. For instance, an attack, which crafts
compromised local models based on Krum, still effectively
increases the error rate from 0.14 (no attack) to 0.58 (314%
relative increase) for Krum + LFR. Fourth, the testing error
rate grows in some cases when a defense is deployed. This is
because the defenses may remove benign local models, which
increases the testing error rate of the global model.
6 Related Work
Security and privacy of federated/collaborative learning are
much less explored, compared to centralized machine learning.
Recent studies [29, 40, 44] explored privacy risks in federated
learning, which are orthogonal to our study.
Poisoning attacks: Poisoning attacks aim to compromise
the integrity of the training phase of a machine learning sys-
tem [5]. The training phase consists of two components, i.e.,
training dataset collection and learning process. Most existing
poisoning attacks compromise the training dataset collec-
tion component, e.g., inject malicious data into the training
dataset. These attacks are also known as data poisoning at-
tacks) [3, 8, 13, 19, 27, 30, 33, 43, 45, 50, 51, 56, 61, 62, 65].
Different from data poisoning attacks, our local model poi-
soning attacks compromise the learning process.
Depending on the goal of a poisoning attack, we can clas-
sify poisoning attacks into two categories, i.e., untargeted
poisoning attacks [8, 30, 33, 50, 62, 65] and targeted poison-
ing attacks [3, 6, 13, 27, 37, 45, 51, 56]. Untargeted poisoning
attacks aim to make the learnt model have a high testing error
indiscriminately for testing examples, which eventually result
in a denial-of-service attack. In targeted poisoning attacks, the
learnt model produces attacker-desired predictions for particu-
lar testing examples, e.g., predicting spams as non-spams and
predicting attacker-desired labels for testing examples with a
particular trojan trigger (these attacks are also known as back-
door/trojan attacks [27]). However, the testing error for other
testing examples is unaffected. Our local model poisoning
attacks are untargeted poisoning attacks. Different from exist-
ing untargeted poisoning attacks that focus on centralized ma-
chine learning, our attacks are optimized for Byzantine-robust
federated learning. We note that Xie et al. [63] proposed in-
ner product manipulation based untargeted poisoning attacks
to Byzantine-robust federated learning including Krum and
median, which is concurrent to our work.
Defenses: Existing defenses were mainly designed for data
poisoning attacks to centralized machine learning. They es-
sentially aim to detect the injected malicious data in the train-
ing dataset. One category of defenses [4, 15, 56, 59] detects
malicious data based on their (negative) impact on the per-
formance of the learnt model. For instance, Barreno et al. [4]
proposed Reject on Negative Impact (RONI), which measures
the impact of each training example on the performance of
the learnt model and removes the training examples that have
large negative impact. Suciu et al. [56] proposed a variant of
RONI (called tRONI) for targeted poisoning attacks. In par-
ticular, tRONI measures the impact of a training example on
only the target classiﬁcation and excludes training examples
that have large impact.
Another category of defenses [20, 30, 35, 55] proposed new
loss functions, optimizing which obtains model parameters
and detects the injected malicious data simultaneously. For
instance, Jagielski et al. [30] proposed TRIM, which aims
to jointly ﬁnd a subset of training dataset with a given size
and model parameters that minimize the loss function. The
training examples that are not in the selected subset are treated
as malicious data. These defenses are not directly applicable
for our local model poisoning attacks because our attacks do
not inject malicious data into the training dataset.
For federated learning, the machine learning community
recently proposed several aggregation rules (e.g., Krum [9],
Bulyan [42], trimmed mean [66], median [66], and others [14])
that were claimed to be robust against Byzantine failures of
certain worker devices. Our work shows that these defenses
are not effective in practice against our optimized local model
poisoning attacks that carefully craft local models on the
compromised worker devices. Fung et al. [23] proposed to
compute weight for each worker device according to histori-
cal local models and take the weighted average of the local
models to update the global model. However, their method
can only defend against label ﬂipping attacks, which can al-
ready be defended by existing Byzantine-robust aggregation
rules. We propose ERR and LFR, which are respectively gen-
eralized from RONI and TRIM, to defend against our local
model poisoning attacks. We ﬁnd that these defenses are not
effective enough in some scenarios, highlighting the needs of
new defenses against our attacks.
Other security and privacy threats to machine learn-
ing: Adversarial examples [5, 57] aim to make a machine
learning system predict labels as an attacker desires via adding
carefully crafted noise to normal testing examples in the test-
ing phase. Various methods (e.g., [2, 11, 25, 36, 46, 47, 52,
1636    29th USENIX Security Symposium
USENIX Association
54, 57]) were proposed to generate adversarial examples, and
many defenses (e.g., [10, 25, 26, 38, 41, 48, 64]) were explored
to mitigate them. Different from poisoning attacks, adversarial
examples compromise the testing phase of machine learning.
Both poisoning attacks and adversarial examples compro-
mise the integrity of machine learning. An attacker could also
compromise the conﬁdentiality of machine learning. Specif-
ically, an attacker could compromise the conﬁdentiality of
users’ private training or testing data via various attacks such
as model inversion attacks [21, 22], membership inference
attacks [40, 49, 53], and property inference attacks [1, 24].
Moreover, an attacker could also compromise the conﬁden-
tiality/intellectual property of a model provider via stealing
its model parameters and hyperparameters [34, 58, 60].
7 Conclusion, Limitations, and Future Work
We demonstrate that the federated learning methods, which
the machine learning community claimed to be robust against
Byzantine failures of some worker devices, are vulnerable to
our local model poisoning attacks that manipulate the local
models sent from the compromised worker devices to the
master device during the learning process. In particular, to
increase the error rates of the learnt global models, an attacker
can craft the local models on the compromised worker de-
vices such that the aggregated global model deviates the most
towards the inverse of the direction along which the global
model would change when there are no attacks. Moreover,
ﬁnding such crafted local models can be formulated as op-
timization problems. We can generalize existing defenses
for data poisoning attacks to defend against our local model
poisoning attacks. Such generalized defenses are effective in
some cases but are not effective enough in other cases. Our
results highlight that we need new defenses to defend against
our local model poisoning attacks.
Our work is limited to untargeted poisoning attacks. It
would be interesting to study targeted poisoning attacks to
federated learning. Moreover, it is valuable future work to de-
sign new defenses against our local model poisoning attacks,
e.g., new methods to detect compromised local models and
new adversarially robust aggregation rules.
8 Acknowledgements
We thank the anonymous reviewers and our shepherd Nikita
Borisov for constructive reviews and comments. This work
was supported by NSF grant No.1937786.
References
[1] Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi,
Antonio Villani, Domenico Vitali, and Giovanni Felici.
Hacking smart machines with smarter ones: How to ex-
tract meaningful data from machine learning classiﬁers.
International Journal of Security and Networks, 10(3),
2015.
[2] Anish Athalye, Logan Engstrom, Andrew Ilyas, and
Kevin Kwok. Synthesizing robust adversarial exam-
ples. In ICML, 2018.
[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deb-
orah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. In arxiv, 2018.
[4] Marco Barreno, Blaine Nelson, Anthony D Joseph, and
JD Tygar. The security of machine learning. Machine
Learning, 2010.
[5] Marco Barreno, Blaine Nelson, Russell Sears, An-
thony D Joseph, and J Doug Tygar. Can machine learn-
ing be secure? In ACM ASIACCS, 2006.
[6] Arjun Bhagoji, Supriyo Chakraborty, Prateek Mittal, and
Seraphin Calo. Analyzing federated learning through
an adversarial lens. In ICML, 2019.
[7] Battista Biggio, Luca Didaci, Giorgio Fumera, and Fabio
Roli. Poisoning attacks to compromise face templates.
In IEEE ICB, 2013.
[8] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-
In
soning attacks against support vector machines.
ICML, 2012.
[9] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guer-
raoui, and Julien Stainer. Machine learning with adver-
saries: Byzantine tolerant gradient descent. In NIPS,
2017.
[10] Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating eva-
sion attacks to deep neural networks via region-based
classiﬁcation. In ACSAC, 2017.
[11] Nicholas Carlini and David Wagner. Towards evaluating
the robustness of neural networks. In IEEE S & P, 2017.
[12] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang,
Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang,
and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient ma-
chine learning library for heterogeneous distributed sys-
tems. arXiv preprint arXiv:1512.01274, 2015.
[13] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and
Dawn Song. Targeted backdoor attacks on deep learning
systems using data poisoning. In arxiv, 2017.
[14] Yudong Chen, Lili Su, and Jiaming Xu. Distributed sta-
tistical machine learning in adversarial settings: Byzan-
tine gradient descent. In POMACS, 2017.
USENIX Association
29th USENIX Security Symposium    1637
[15] Gabriela F. Cretu, Angelos Stavrou, Michael E. Locasto,
Salvatore J. Stolfo, and Angelos D. Keromytis. Cast-
ing out demons: Sanitizing training data for anomaly
sensors. In IEEE S & P, 2008.
[16] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai
Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker,
Ke Yang, and Andrew Y. Ng. Large scale distributed
deep networks. In NIPS, 2012.
[17] John R. Douceur. The Sybil attack. In IPTPS, 2002.
[18] Dheeru Dua and Casey Graff. UCI machine learning
repository, 2017.
[19] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong,
and Jia Liu. Poisoning attacks to graph-based recom-
mender systems. In ACSAC, 2018.
[20] Jiashi Feng, Huan Xu, Shie Mannor, and Shuicheng Yan.
Robust logistic regression and classiﬁcation. In NIPS,
2014.
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
CVPR, pages 770–778, 2016.
[29] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-
Cruz. Deep models under the gan: Information leakage
from collaborative deep learning. In CCS, 2017.
[30] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang
Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating ma-
chine learning: Poisoning attacks and countermeasures
for regression learning. In IEEE S & P, 2018.
[31] Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bian-
coni, Susanne M Melchers, Lothar R Schad, Timo
Gaiser, Alexander Marx, and Frank Gerrit Zöllner.
Multi-class texture analysis in colorectal cancer histol-
ogy. Scientiﬁc reports, 2016.
[32] Jakub Koneˇcný, H. Brendan McMahan, Felix X. Yu,
Peter Richtárik, Ananda Theertha Suresh, and Dave Ba-
con. Federated learning: Strategies for improving com-
munication efﬁciency. In NIPS Workshop on Private
Multi-Party Machine Learning, 2016.
[21] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model inversion attacks that exploit conﬁdence informa-
tion and basic countermeasures. In ACM CCS, 2015.
[33] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorob-
eychik. Data poisoning attacks on factorization-based