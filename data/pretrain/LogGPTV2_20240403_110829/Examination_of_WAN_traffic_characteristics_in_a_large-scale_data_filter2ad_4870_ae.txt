traffic data collected during one week period. Figure 13 shows the
normalized traffic volume of each type of service in the first four
days of the week on a 1-minute time scale. We can see different
diurnal patterns among services. The coefficient of variation varies
of these time series from 0.13 (DB) to 0.62 (Cloud), confirming wide
diversity of variations for different services.
The distinct temporal variation of different types of services
challenges the existing methods used by SD-WAN in accurately es-
timating the high-priority WAN traffic, where they often rely on the
average or median traffic volume in the last few minutes to estimate
the interactive service’s demand [14, 19]. A popular solution to tol-
erate prediction error is setting aside different headrooms [22, 32].
A larger prediction error will require larger headrooms, which
however will lower the utilization of WAN links and degrade the
performance for bulk transfers over WAN.
11
To answer this question, we select three simple yet widely-used
time series models to examine the traffic demand prediction ac-
curacy, namely Historical Average, Historical Median, and SES
(Simple Exponential Smoothing). Historical Average/Median calcu-
lates the average/median of the historical data as a prediction; SES
(Simple Exponential Smoothing) calculates the weighted average
of the historical data, where the weights decrease exponentially as
observations become older. Specially, with SES, the traffic demand
i =0(1 − α)i ∗ yt−i, where
α (0 ≤ α ≤ 1) controls the rate at which the weights decrease and
is set as 0.2 and 0.8 in our experiments.
at time t + 1 is estimated as(cid:98)yt +1|t = αt−1
along with the standard deviation for each type of services over all
links in Figure 14.
Recall that we aim at service-level traffic engineering for high-
priority traffic. To that end, we evaluate the above estimation meth-
ods using the high-priority WAN traffic data spanning over one
week on a 1-minute time scale. That said, we perform a 1-minute-
ahead prediction using the historical traffic data within a 5-minute
window. For each type of services, the prediction is applied on the
inter-DC WAN links that carry large amounts of traffic of that type
of services. We compute the median prediction error on each link,
which is |(cid:98)yt +1 − yt +1|/yt +1. We finally report the average error
0.00.20.40.60.81.0Stability (fraction of traffic)0.00.20.40.60.81.0CDFWebComputingAnalyticsDBCloudAIFileSystemMapSecurity100101102103Mean run-length of stability (minute)0.00.20.40.60.81.0CDFWebComputingAnalyticsDBCloudAIFileSystemMapSecurityIMC ’21, November 2–4, 2021, Virtual Event, USA
Zhaohua Wang, Zhenyu Li, Guangming Liu, Yunfei Chen, Qinghua Wu, Gang Cheng
These observations, on one hand, reaffirm our analysis on traffic
stability. On the other hand, they imply that for some services,
a large headroom is required if the above methods are applied,
leading to less efficient use of the experience WAN bandwidth. A
possible way to improve prediction accuracy is to leverage neural
network-based prediction models (e.g. LSTM), which can capture
more features of time series. Such models may take a longer time to
get the estimation, but we think they are viable given that the traffic
engineering is often performed on time scales over 1 minutes.
5.3 Summary and Implications
Our analysis reveals different interaction patterns among services.
Web and Computing services heavily interact with each other show-
ing that they are closely bound to each other. Analytics, AI, Map,
and Security services send their traffic to others more evenly, im-
plying their prevalence. These observations shed light on service
migration and deployment, e.g. co-locating Web and Computing
services in few DCs, replicating Analytics, AI, Map and Security
services into each DCs. We also observe a high correlation be-
tween traffic time series on a 10-minute time scale across individual
services. This observation indicates a limited number of traffic vari-
ation patterns, implying one may estimate the traffic trend of a
service using the trends of other correlated services.
We further give insight into the service-level traffic predictability,
finding the stability varies greatly across services. Indeed, the traffic
predictability of services is affected by their interaction patterns.
The test of existing methods for the estimation of high-priority
traffic demand reveals that while they provide good estimation for
the services showing good stability (e.g. Web, Analytics services),
they perform poor for the services whose stability cannot persist for
a long time (e.g. Cloud, Filesystem, Security services). Using these
estimation methods will either lower the link utilization (using
large headroom) or hurt the performance of interactive applica-
tions (using small headroom). Leveraging neural network for more
accurate estimation indeed needs further investigation.
6 RELATED WORK
Traffic characterization of DC networks. A large body of mea-
surement studies have reported the nature of traffic within DCs,
including Microsoft (Web search) datacenter [4, 5, 11, 20], and Face-
book (social network) datacenter [27]. The authors in [11, 20] an-
alyzed the traffic characteristics from the perspectives of traffic
exchange within DCs and flow characteristics [11, 20]. Benson et al.
examined the packet arrival patterns at switches and link utilization
in different layers [5], and also investigated flow characteristics
and rack-level locality in [4]. Roy et al. extended the previous stud-
ies that are primarily performed in Microsoft DCs [27], and found
different traffic characteristics of traffic locality and flow charac-
teristics. These works provide implications for designing novel
connection fabrics [11], traffic engineering protocols [6, 13] and ad-
vanced switches [29] in DCs. Some other studies [21, 34] inspected
the packet burst behavior in DCs, which is useful for device buffer
configuration [1, 33] and efficient congestion control [2, 24].
These measurements focus mostly on traffic characteristics in-
side DCs. In contrast, our work focuses more on WAN traffic; with
Figure 13: The high-priority WAN traffic of different types
of services on a 1-minute time scale; the x-axis shows the
hours of the week and the y-axis shows the traffic volume
which is normalized by the peak volume.
Figure 14: The WAN traffic prediction errors using the meth-
ods based on historical statistical information.
We can see from the figure that the prediction accuracy varies
greatly across different services. While the models perform well for
Web and Analytics services with the prediction error less than 5%,
the error of other services such as Cloud and FileSystem reaches
nearly 15% as their short run-length of stability (see Figure 12(a)).
Besides, for each type of service, the historical average/median
model predicts slightly less accurately than the SES models with
α close to 1, indicating that the more distant observations have
less effect on the prediction due to the non-stationary pattern.
12
0h24h48h72h96h0.20.40.60.81.0Web0h24h48h72h96h0.20.40.60.81.0Computing0h24h48h72h96h0.30.40.50.60.70.80.91.0Analytics0h24h48h72h96h0.50.60.70.80.91.0DB0h24h48h72h96h0.00.20.40.60.81.0Cloud0h24h48h72h96h0.20.40.60.81.0AI0h24h48h72h96h0.00.20.40.60.81.0FileSystem0h24h48h72h96h0.20.40.60.81.0Map0h24h48h72h96h0.40.50.60.70.80.91.0Security0h24h48h72h96h0.40.50.60.70.80.91.0AllAggregateWebComputingAnalyticsDBCloudAIFileSystemMapSecurity05101520Prediction error (%)Historical AverageHistorical MedianSES (α=0.2)SES (α=0.8)Examination of WAN Traffic Characteristics in a Large-scale Data Center Network
IMC ’21, November 2–4, 2021, Virtual Event, USA
this in mind, our findings shed useful light in WAN traffic engineer-
ing, service migration and deployment, WAN switch configuration
as well. In this perspective, our work is closely related to [8] that
studied the inter-DC traffic in Yahoo!, but their scale is much smaller
(5 DCs) and the application mix is much simpler.
Traffic engineering for Interconnect WAN. Given that DC-
WAN is an expensive resource, more flexible and responsible traf-
fic engineering techniques are increasingly needed for refining
bandwidth allocation and congestion control for cross-DC net-
works [14, 19, 22, 31], and accelerating bulk transferring across
DCs [17, 23, 30]. The effectiveness of these traffic engineering ap-
proaches depends heavily on the WAN traffic patterns that we exam-
ined in this paper. For example, both SWAN [14] and Tempus [19]
estimate the aggregated traffic demand with the average traffic
demand in the last few minutes. While our analysis reveals the sta-
bility for aggregated traffic in large time scale (e.g. 10 minutes), we
find applying their estimations directly to different service groups
may lead to low accuracy for service-level traffic allocation [22].
7 CONCLUSION
This paper has examined the WAN traffic characteristics in Baidu’s
DCN. We find a considerable fraction of traffic demands over DC-
WAN, which leads to higher utilization for links carrying WAN
traffic and great challenges for WAN traffic engineering. We further
observe that traffic communications among DCs are extensive but
highly skewed toward a small number of persistently heavy DC
pairs. Besides, the aggregated traffic and traffic exchanges among
these heavy DCs remain stable over time; the stability provides the
possibility for predicting overall traffic demands in the WAN. How-
ever, the stability and traffic characteristics vary across different
services on a 1-minute time scale, leading to different prediction
accuracy of high-priority traffic for different types of services. The
large prediction error for some types of services using existing
methods motivates our further investigation of better prediction
methods for fine-grained traffic engineering at the service level.
Our focus in this paper is on the traffic pattern of Baidu’s in-
house services. That said, the traffic generated by the cloud cus-
tomer services that run on Baidu’s public cloud is not included
in this paper. The cloud customer services may show different be-
havior than Baidu’s in-house services. For instance, most cloud
customer services are not as popular as the in-house services (e.g.,
Web search), as such they may use fewer DCs, leading to lower
DC-WAN traffic than the in-house services; they may also much
less rely on other services than the in-house services, leading to less
correlation between services in terms of traffic dynamics. We leave
the further investigation of cloud customer traffic as our future
work.
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for their
valuable comments and helpful suggestions. This work was par-
tially supported by Beijing Natural Science Foundation (JQ20024),
Natural Science Foundation of China (U20A20180, 62072437), and
CAS-Austria Joint Project (171111KYSB20200001). Corresponding
Author: Zhenyu Li.
13
REFERENCES
[1] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Matus,
Rong Pan, Navindra Yadav, et al. 2014. CONGA: Distributed congestion-aware
load balancing for datacenters. In Proceedings of the 2014 ACM conference on
SIGCOMM. 503–514.
[2] Mohammad Alizadeh, Albert Greenberg, David A Maltz, Jitendra Padhye, Parveen
Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data
center tcp (dctcp). In Proceedings of the ACM SIGCOMM 2010 conference. 63–74.
[3] Baidu Apollo. 2020. Smart Transportation Solution; Autonomous Driving Solu-
tion; Intelligent Vehicle Solution. https://apollo.auto/index.html
[4] Theophilus Benson, Aditya Akella, and David A Maltz. 2010. Network traffic char-
acteristics of data centers in the wild. In Proceedings of the 10th ACM SIGCOMM
conference on Internet measurement. 267–280.
[5] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang. 2010. Under-
standing data center traffic characteristics. ACM SIGCOMM Computer Communi-
cation Review 40, 1 (2010), 92–99.
[6] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang. 2011. Mi-
croTE: Fine grained traffic engineering for data centers. In Proceedings of the
Seventh COnference on emerging Networking EXperiments and Technologies. 1–12.
[7] Jeremy Bogle, Nikhil Bhatia, Manya Ghobadi, Ishai Menache, Nikolaj Bjørner,
Asaf Valadarsky, and Michael Schapira. 2019. TEAVAR: striking the right
utilization-availability balance in WAN traffic engineering. In Proceedings of the
ACM Special Interest Group on Data Communication. 29–43.
[8] Yingying Chen, Sourabh Jain, Vijay Kumar Adhikari, Zhi-Li Zhang, and Kuai Xu.
2011. A first look at inter-data center traffic characteristics via yahoo! datasets.
In 2011 Proceedings IEEE INFOCOM. IEEE, 1620–1628.
[9] Benoit Claise, Ganesh Sadasivan, Vamsi Valluri, and Martin Djernaes. 2004. Cisco
systems netflow services export version 9. (2004).
[10] Apache Doris. 2020. A fast MPP database for all modern analytics on big data.
http://doris.apache.org/master/en/
[11] Albert Greenberg, James R Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A Maltz, Parveen Patel, and Sudipta
Sengupta. 2009. VL2: a scalable and flexible data center network. In Proceedings
of the ACM SIGCOMM 2009 conference on Data communication. 51–62.
[12] Gonca Gürsun and Mark Crovella. 2012. On Traffic Matrix Completion in the
Internet. In Proceedings of the 2012 Internet Measurement Conference (IMC ’12).
[13] Keqiang He, Eric Rozner, Kanak Agarwal, Wes Felter, John Carter, and Aditya
Akella. 2015. Presto: Edge-based load balancing for fast datacenter networks.
ACM SIGCOMM Computer Communication Review 45, 4 (2015), 465–478.
[14] Chi-Yao Hong, Srikanth Kandula, Ratul Mahajan, Ming Zhang, Vijay Gill, Mohan
Nanduri, and Roger Wattenhofer. 2013. Achieving high utilization with software-
driven WAN. In Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM.
15–26.
[15] Kevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis, Gregory R
Ganger, Phillip B Gibbons, and Onur Mutlu. 2017. Gaia: Geo-distributed machine
learning approaching {LAN} speeds. In 14th {USENIX} Symposium on Networked
Systems Design and Implementation ({NSDI} 17). 629–647.
[16] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun
Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, et al. 2013. B4:
Experience with a globally-deployed software defined WAN. ACM SIGCOMM
Computer Communication Review 43, 4 (2013), 3–14.
[17] Xin Jin, Yiran Li, Da Wei, Siming Li, Jie Gao, Lei Xu, Guangzhi Li, Wei Xu, and
Jennifer Rexford. 2016. Optimizing bulk transfers with software-defined optical
WAN. In Proceedings of the 2016 ACM SIGCOMM Conference. 87–100.
[18] Srikanth Kandula, Ishai Menache, Joseph Seffi Naor, and Erez Timnat. 2019.
An Algorithmic Framework for Geo-Distributed Analytics. In Network Games,
Control, and Optimization. Springer, 89–105.
[19] Srikanth Kandula, Ishai Menache, Roy Schwartz, and Spandana Raj Babbula. 2014.
Calendaring for wide area networks. In Proceedings of the 2014 ACM conference
on SIGCOMM. 515–526.
[20] Srikanth Kandula, Sudipta Sengupta, Albert Greenberg, Parveen Patel, and Ronnie
Chaiken. 2009. The nature of data center traffic: measurements & analysis. In
Proceedings of the 9th ACM SIGCOMM conference on Internet measurement. 202–
208.
[21] Rishi Kapoor, Alex C Snoeren, Geoffrey M Voelker, and George Porter. 2013. Bullet
trains: a study of NIC burst behavior at microsecond timescales. In Proceedings of
the ninth ACM conference on Emerging networking experiments and technologies.
133–138.
[22] Alok Kumar, Sushant Jain, Uday Naik, Anand Raghuraman, Nikhil Kasinad-
huni, Enrique Cauich Zermeno, C Stephen Gunn, Jing Ai, Björn Carlin, Mihai
Amarandei-Stavila, et al. 2015. BwE: Flexible, hierarchical bandwidth allocation
for WAN distributed computing. In Proceedings of the 2015 ACM Conference on
Special Interest Group on Data Communication. 1–14.
[23] Nikolaos Laoutaris, Michael Sirivianos, Xiaoyuan Yang, and Pablo Rodriguez.
2011. Inter-datacenter bulk transfers with netstitcher. In Proceedings of the ACM
SIGCOMM 2011 conference. 74–85.
[30] Zhenjie Yang, Yong Cui, Xin Wang, Yadong Liu, Minming Li, Shihan Xiao, and
Chuming Li. 2019. Cost-efficient scheduling of bulk transfers in inter-datacenter
WANs. IEEE/ACM Transactions on Networking 27, 5 (2019), 1973–1986.
[31] Gaoxiong Zeng, Wei Bai, Ge Chen, Kai Chen, Dongsu Han, Yibo Zhu, and Lei
Cui. 2019. Congestion Control for Cross-Datacenter Networks. In 2019 IEEE 27th
International Conference on Network Protocols (ICNP). IEEE, 1–12.
[32] Hong Zhang, Kai Chen, Wei Bai, Dongsu Han, Chen Tian, Hao Wang, Haibing
Guan, and Ming Zhang. 2017. Guaranteeing Deadlines for Inter-Data Center
Transfers. IEEE/ACM Transactions on Networking 25, 1 (2017), 579–595. https:
//doi.org/10.1109/TNET.2016.2594235
[33] Hong Zhang, Junxue Zhang, Wei Bai, Kai Chen, and Mosharaf Chowdhury. 2017.
Resilient datacenter load balancing in the wild. In Proceedings of the Conference
of the ACM Special Interest Group on Data Communication. 253–266.
[34] Qiao Zhang, Vincent Liu, Hongyi Zeng, and Arvind Krishnamurthy. 2017. High-
resolution measurement of data center microbursts. In Proceedings of the 2017
Internet Measurement Conference. 78–85.
[35] Yuchao Zhang, Junchen Jiang, Ke Xu, Xiaohui Nie, Martin J. Reed, Haiyang Wang,
Guang Yao, Miao Zhang, and Kai Chen. 2018. BDS: A Centralized near-Optimal
Overlay Network for Inter-Datacenter Data Replication (EuroSys ’18). Article 10,
14 pages.
IMC ’21, November 2–4, 2021, Virtual Event, USA
Zhaohua Wang, Zhenyu Li, Guangming Liu, Yunfei Chen, Qinghua Wu, Gang Cheng
Jupiter rising: A decade of clos topologies and centralized control in google’s
datacenter network. ACM SIGCOMM computer communication review 45, 4 (2015),
183–197.
[24] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David
Zats. 2015. TIMELY: RTT-based Congestion Control for the Datacenter. ACM
SIGCOMM Computer Communication Review 45, 4 (2015), 537–550.
[25] Heng Pan, Zhenyu Li, JianBo Dong, Zheng Cao, Tao Lan, Di Zhang, Gareth Tyson,
and Gaogang Xie. 2020. Dissecting the Communication Latency in Distributed
Deep Sparse Learning. In Proceedings of the ACM Internet Measurement Conference.
528–534.
[26] Qifan Pu, Ganesh Ananthanarayanan, Peter Bodik, Srikanth Kandula, Aditya
Akella, Paramvir Bahl, and Ion Stoica. 2015. Low latency geo-distributed data
analytics. ACM SIGCOMM Computer Communication Review 45, 4 (2015), 421–
434.
[27] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C Snoeren.
2015. Inside the social network’s (datacenter) network. In Proceedings of the 2015
ACM Conference on Special Interest Group on Data Communication. 123–137.
[28] Ahmed Saeed, Varun Gupta, Prateesh Goyal, Milad Sharif, Rong Pan, Mostafa
Ammar, Ellen Zegura, Keon Jang, Mohammad Alizadeh, Abdul Kabbani, et al.
2020. Annulus: A Dual Congestion Control Loop for Datacenter and WAN Traffic
Aggregates. In Proceedings of the Annual conference of the ACM Special Interest
Group on Data Communication on the applications, technologies, architectures, and
protocols for computer communication. 735–749.
[29] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, et al. 2015.
14