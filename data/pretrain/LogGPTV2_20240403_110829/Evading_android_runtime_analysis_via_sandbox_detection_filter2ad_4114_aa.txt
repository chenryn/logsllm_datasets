title:Evading android runtime analysis via sandbox detection
author:Timothy Vidas and
Nicolas Christin
Evading Android Runtime Analysis via Sandbox Detection
Timothy Vidas
Carnegie Mellon University
PI:EMAIL
Nicolas Christin
Carnegie Mellon University
PI:EMAIL
ABSTRACT
The large amounts of malware, and its diversity, have made it nec-
essary for the security community to use automated dynamic anal-
ysis systems. These systems often rely on virtualization or emu-
lation, and have recently started to be available to process mobile
malware. Conversely, malware authors seek to detect such systems
and evade analysis. In this paper, we present techniques for detect-
ing Android runtime analysis systems. Our techniques are classi-
ﬁed into four broad classes showing the ability to detect systems
based on differences in behavior, performance, hardware and soft-
ware components, and those resulting from analysis system design
choices. We also evaluate our techniques against current publicly
accessible systems, all of which are easily identiﬁed and can there-
fore be hindered by a motivated adversary. Our results show some
fundamental limitations in the viability of dynamic mobile malware
analysis platforms purely based on virtualization.
Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection
Keywords
Evasion; malware; android; security; sandbox
1.
INTRODUCTION
In the past couple of years, mobile devices have become sophis-
ticated computing environments with increased computing power
and network connectivity. This has made them considerably closer
to traditional computing platforms such as PCs, than to the tele-
phones they were initially designed to replace. As a result of this
trend, security threats that traditionally only applied to PCs can now
also be encountered on mobile devices. In particular, as miscreants
discover that mobile devices can be targeted and exploited for ﬁ-
nancial gain, new forms of malware are created to do so. Some mo-
bile malware is designed to work in tandem with PC-oriented mal-
ware while other focuses on mobile devices exclusively [18,36,40].
When a new piece of malware is discovered, it must be analyzed
in order to understand its capabilities and the threat it represents.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS’14, June 4–6, 2014, Kyoto, Japan.
Copyright 2014 ACM 978-1-4503-2800-5/14/06 ...$15.00.
http://dx.doi.org/10.1145/2590296.2590325.
One popular method of analysis, dynamic analysis, consists of ex-
ecuting the malware in a controlled environment to observe effects
to the host system and the network.
On traditional PCs, the controlled environment used for dynamic
analysis is often created through host virtualization. PC malware
authors have consequently taken to design malware that can detect
virtual environments [19,24,30,34], and exhibit alternative, benign
behavior, rather than the malicious behavior which would occur on
actual hardware. This kind of evasion allows malware to thwart
dynamic analysis systems, and has fueled an arms race between
those improving the realism of virtualized environments and those
wishing to detect them.
However, virtualization technology has considerably matured in
the past few years, and a number of users have migrated physical
machines to virtual instances. Today, production services may be
run in corporate virtualization “farms” or even in space rented in a
“cloud.” As a result, virtualized environments are not merely used
for sandboxing malware anymore, but have become commonplace
for a number of applications. In turn, the ability for malware to
detect a virtual environment is losing its usefulness, as virtualiza-
tion is no longer a near-certain indication that dynamic malware
analysis is taking place.
On the other hand, there are, so far, only limited use cases for
virtual environments on mobile devices. Likewise, emulated envi-
ronments are typically only used by developers or dynamic anal-
ysis systems. For this reason, mobile malware authors may still
employ virtualization or emulation detection to alter behavior and
ultimately evade analysis or identiﬁcation.
In this paper we contribute several techniques that can be used
to detect a runtime analysis in Android. Some of these techniques
are speciﬁc to the Android framework or a virtualized environment
(e.g., emulator), and could be quite easily thwarted, while others,
based for instance on resource availability, are more general, and
much harder to defend against. All of the techniques we present
require minimal or no privileges, and could be invoked from typical
applications found in online marketplaces.
The primary contribution of this paper is thus to demonstrate that
dynamic analysis platforms for mobile malware that purely rely on
emulation or virtualization face fundamental limitations that may
make evasion possible.
The rest of this paper is organized as follows. We start by pre-
senting related work in Section 2. We then propose different emula-
tion detection techniques in Section 3. In Section 4 we address an
obvious countermeasure to some detection techniques by demon-
strating alternate implementations of the methods described in Sec-
tion 3. We evaluate how current systems fare against the proposed
techniques in Section 5 before discussion results in Section 6, and
concluding in Section 7.
4472. RELATED WORK
Automated runtime analysis systems are a popular method of
processing large volumes of malware, or as an alternative to hav-
ing skilled personnel perform lengthy manual analysis. For desk-
top operating systems there are numerous free [12] and commer-
cial [39] systems that perform such analysis. More recently, a few
systems [1, 2, 3, 4, 10, 13, 25] have been proposed for dynamic anal-
ysis of mobile malware. Because of their novelty, these systems
remain less mature than their desktop counterparts.
PC malware writers responded to the advent of automated sys-
tems by ﬁnding creative solutions to detect whether code was run-
ning in virtualized environments. Virtualization detection routines
have been implemented in numerous ways from detecting a vendor-
speciﬁc API, such as VMWare’s communication channel [24], to
observing environmental effects of a single CPU instruction [34].
Upon detection, malware may exhibit alternate behavior, hiding
its true purpose. This runtime change in control ﬂow results in a
different execution path through the malicious software. In 2008,
Chen et al. observed that PC malware exhibited reduced behavior
in nearly 40% of unique samples when run in debugged environ-
ment, and 4% of samples when run in a virtualized environment.
However, the samples that exhibit alternate behavior due do envi-
ronment accounted for 90% of attacks during particular time pe-
riods [15]. Moser et al. propose a system for analyzing multiple
execution paths by presenting different input and environments to
subsequent executions of the same malware [26]. While this sys-
tem investigates different execution paths, virtualization is not one
of the changes used to effect a different execution.
The detection of analysis environments is not limited to detecting
the virtualization component itself. Holz et al. describe numerous
techniques for detecting honeypots [22]. Some techniques are in-
deed rooted in virtualization detection, but others focus on other
environmental components such as the presence of a debugger.
The concept of evasion has also been subject to a considerable
amount of research in the context of network intrusion detection
systems (NIDS). Handley et al. observe that a skilled attacker can
“evade [network] detection by exploiting ambiguities in the trafﬁc
stream” [21]. These ambiguities are classiﬁed into three categories:
incomplete system implementation, incomplete knowledge of the
trafﬁc recipient’s system, or incomplete network topology knowl-
edge. Numerous others describe evasion attacks due inconsisten-
cies between the NIDS and the victim computer [14, 16, 20, 27, 32].
The most closely related work to ours offers emulator detection
techniques for the PC [33]. Raffetseder et al. detail some “gen-
eral” detection methods and well as a few that speciﬁcally detect
the Intel x86 version of the QEMU emulator. At a high level,
we explore similar general detection techniques targeting the An-
droid emulator. There is scant similar academic research in evading
mobile malware analysis; however there have a been a few indus-
try presentations [28, 31] that look at evasion particular to Google
Bouncer. Both of these presentations address some API related de-
tections (which we generalize in Section 3.1) as well as detections
that speciﬁcally target Bouncer, such as source IP addresses asso-
ciated with Google. Our work here is more general, in that it tries
to pinpoint more fundamental limitations in dynamic analysis on
mobile devices. A more general industry presentation by Strazzere
explores several Android API-based detections as well as a speciﬁc
QEMU detection and is complimentary to our research [35].
3. EMULATOR DETECTION
Fundamentally, the concept of emulator detection is rooted in
the fact that complete system emulation is an arduous task. By
discovering or observing differences between virtual and physical
execution an attacker can create software virtualization checks that
can be used to alter overall program behavior. Such differences
may be an artifact of hardware state not correctly implemented in
the virtual CPU, hardware or software components that have yet to
be virtualized, or observable execution duration.
In this section we detail several virtualization detection tech-
niques and discuss the results of experimental evaluation of these
techniques. The techniques require few or no permissions and work
on commodity devices in the standard consumer software conﬁg-
uration. As with any consumer Android device, applications are
governed by Linux access control and are thus limited to user-mode
processor execution (e.g. devices are not “rooted” or “jailbroken”).
We evaluate the detection techniques using emulator instances
on similar Windows, Mac, and Linux hosts (each with an i7 pro-
cessor, 8 GB RAM) as well as six physical devices from major
U.S. cellular carriers. We divide detection techniques into the fol-
lowing categories: differences in behavior, performance, hardware
and software components, and those resulting from system design
choices. For ease in comparison, the ﬁrst three roughly coincide
with existing work in PC emulator detection [33].
3.1 Differences in behavior
Since virtualization is often deﬁned in terms of execution being
indistinguishable from that of a real machine, in some sense, any
virtualization detection technique can be perceived as a difference
in behavior. However, here we focus on behavioral differences spe-
ciﬁc to software state and independent of system performance.
Detecting emulation through the Android API.
The Android API provides an abstract interface for application
programmers. Since many Android devices are smartphones, the
API provides a rich interface for telephony operations as well as
methods for interacting with other hardware and local storage.
Table 1 enumerates several API methods that return particular
values when used with an emulated device. Each of the API-value
pairs in Table 1 can be used to explicitly detect an emulated de-
vice or used in conjunction with other values in order to determine
a likelihood. For example, if the TelephonyManager.get-
DeviceId() API returns all 0’s, the instance in question is cer-
tainly an emulator because no physical device would yield this
value.
Similarly, emulator instances adopt a telephone number based
on the Android Debug Bridge (ADB) port in use by the emulator.
When an emulator instance starts, the emulator reserves a pair of
TCP ports starting with 5554/5555 (a second instance would ac-
quire 5556/5558) for debugging purposes. The adopted telephone
number is based on the reserved ports such that the initial emulator
instance adopts precisely 1-555-521-5554. Therefore, if the Tel-
ephonyManager.getLine1Number() API indicates that the
device phone number is in the form 155552155xx, then the device
is certainly an emulator. Such a number would never naturally be
used on a device because the 555 area code is reserved for directory
assistance [9]. The presence of the 555 area code may also be used
in other emulator detections such as the pre-conﬁgured number for
voicemail.
Other values in Table 1 are certainly used by the emulator but
may also be used by some real devices. Consider the Mobile
Country Code (MCC) and Mobile Network Code (MNC) values
obtained via the TelephonyManager.getNetworkOpera-
tor() method. The emulator always returns values associated
with T-Mobile USA. Since there are certainly real devices that use
the same codes, checks based on the MCC and MNC need to be
448API method
Build.ABI
Build.ABI2
Build.BOARD
Build.BRAND
Build.DEVICE
Build.FINGERPRINT
Build.HARDWARE
Build.HOST
Build.ID
Build.MANUFACTURER
Build.MODEL
Build.PRODUCT
Build.RADIO
Build.SERIAL
Build.TAGS
Build.USER
TelephonyManager.getDeviceId()
TelephonyManager.getLine1 Number()
TelephonyManager.getNetworkCountryIso()
TelephonyManager.getNetworkType()
TelephonyManager.getNetworkOperator().substring(0,3)
TelephonyManager.getNetworkOperator().substring(3)
TelephonyManager.getPhoneType()
TelephonyManager.getSimCountryIso()
TelephonyManager.getSimSerial Number()
TelephonyManager.getSubscriberId()
TelephonyManager.getVoiceMailNumber()
Value
armeabi
unknown
unknown
generic
generic
generic††
goldﬁsh
android-test††
FRF91
unknown
sdk
sdk
unknown
null
test-keys
android-build
All 0’s
155552155xx†
us
3
310
260
1
us
89014103211118510720
310260000000000‡‡
15552175049
meaning
is likely emulator
is likely emulator
is emulator
is emulator
is emulator
is emulator
is emulator
is likely emulator
is emulator
is emulator
is emulator
is emulator
is emulator
is emulator
is emulator
is emulator
is emulator
is emulator
possibly emulator
possibly emulator (EDGE)
is emulator or a USA device (MCC)‡
is emulator or a T-Mobile USA device (MNC)
possibly emulator (GSM)
possibly emulator
is emulator OR a 2.2-based device
is emulator
is emulator
Table 1: Listing of API methods that can be used for emulator detection. Some values clearly indicate that an emulator is in use,
others can be used to contribute to likelihood or in combination with other values for emulator detection. † xx indicates a small range
of ports for a particular emulator instance as obtained by the Android Debug Bridge (ADB). Emulator instances begin at 54 and will
always be an even number between 54 and 84 (inclusive). ‡ 310 is the MCC code for U.S. but may also be used in Guam. †† The value
is a preﬁx.‡‡ An emulator will be in the form MCC + MNC + 0’s, checking for the 0’s is likely sufﬁcient.
augmented with other data. If another check establishes that the
device is a Verizon device, but the MNC shows T-Mobile, this may
indicate a modiﬁed emulator that is returning spoofed values.
Table 1 also contains several values from the android.os-
.Build class which contains information about the current soft-
ware build. Retail devices will have system properties that detail
the actual production build. An emulator will have build properties
based on the SDK build process used to create the emulator bi-
nary, such as the Build.FINGERPRINT. The ﬁngerprints listed
in Table 2 clearly show a pattern followed by the SDK build pro-
cess. Even though the SDK documentation warns “Do not attempt
to parse this value,” testing for the presence of “generic,” “sdk,” or
“test-keys” yields perfect results for emulator detection when com-
pared to our sample of physical devices.
Experiments with physical devices led to some counter-intuitive
ﬁndings for some values. For example, the Build.ABI value on
the emulator is “armeabi” which is a plausible value for all devices
with an ARM processor (nearly all devices). However, the API re-
turned an empty string when used on a Motorola Droid. Similarly,
a Build.HOST value starting with “android-test” was also found
on the Droid. As shown in Table 3, the Build.HOST value is not
as useful for emulator detection as other Build values.
Detecting emulated networking.
The emulated networking environment is often quite different
than that found on physical devices. Each emulator instance is iso-
lated from the host PC’s network(s) via software. The network
address space is always 10.0.2/24. Furthermore, the last octet of
the virtual router, host loopback, up to four DNS resolvers, and the
Device
Emulator
Emulator
Emulator
Emulator
Emulator
Emulator
Emulator
Motorola Droid
HTC EVO 4G
Samsung Charge
Samsung Galaxy Tab7
Samsung Galaxy Nexus
Build.HOST
apa27.mtv.corp.google.com
android-test-15.mtv.corp.google.com
android-test-13.mtv.corp.google.com
android-test-25.mtv.corp.google.com
android-test-26.mtv.corp.google.com
vpbs30.mtv.corp.google.com
vpak21.mtv.corp.google.com
android-test-10.mtv.corp.google.com
AA137
SEI-26
SEP-40
vpak26.mtv.corp.google.com
Table 3: Build values collected from various instances.
emulator’s address are always known (1, 2, 3–6, and 15, respec-
tively). Unlike ADB, which reserves adjacent, incrementing TCP
ports, the network schema is the same for every emulator instance,
even if several instances are simultaneously running on one host.