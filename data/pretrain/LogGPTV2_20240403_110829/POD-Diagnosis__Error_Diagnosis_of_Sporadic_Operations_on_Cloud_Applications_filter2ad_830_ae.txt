different  types  of  error  responses  from  different  systems. 
Exception  handling  also  only  has  local  information  rather 
than  global  visibility  when  an  exception  is  caught.  That’s 
why  run  time  external  monitoring,  diagnosis  and  recovery 
are needed.  
resource  provisioning  mechanisms 
Most of the configuration management  (e.g. CFEngine) 
and 
(e.g. 
CloudFormation)  use  a  convergence-based  or  declarative 
approach  where  they  keep  retrying  and  waiting  or  exit  if 
something 
is  non 
prescriptive or behind a black box engine, there is often very 
little can be done on diagnosing intermediate errors.  
B.  Configuration Error Detection and Diagnosis 
the  execution  order 
is  wrong.  As 
cloud 
Confdiagnoser 
the  error  symptoms  and 
As  traditional  operations  are  often  about  changing 
configuration, there are a large number of configuration error 
detection and diagnosis tools and research.  
[12]  uses  static  analysis,  dynamic 
profiling  and  statistical  analysis  to  link  the  undesired 
behavior to specific configuration options. It could diagnose 
both  crashing  and  non-crashing  configuration  errors.  The 
technique requires instrumentation around the configuration 
options in source code and does not diagnose the root cause. 
ConfAid [13] uses information-flow tracking to analyze the 
dependencies  between 
the 
configuration  entries  to  identify  root  causes.  ConfAid  uses 
dynamic taint analysis to diagnose conﬁguration problems by 
monitoring causality within the program binary as it executes. 
The  approach  is  largely  for  diagnosing  a  single  program’s 
configuration  rather  than  configuration  errors  introduced 
during operation. CODE [14] is a tool automatically detects 
software configuration errors, which is based on identifying 
invariant configuration access rules that predict what access 
events  follow  what  contexts.  It  requires  no  source  code, 
application-specific  semantics,  or  heavyweight  program 
analysis.  Using  these  rules,  CODE  can  sift  through  a 
voluminous  number  of  events  and  detect  deviant  program 
execution.  Our  approach  is  complimentary  to  CODE  by 
detecting a wider source of errors and diagnosing them. 
C.  Log analysis tools 
There are a large number of log analysis tools that can be 
used  for  error  diagnosis  purposes.  First,  all  of  them  do  not 
conduct  online  diagnosis  through  automatically  performing 
diagnosis  tests.  Second,  there  is  no  process  context  used 
261261261
during  diagnosis.  The  logs  are  usually  organized  around 
sources and only provide manual drilling down. 
In  Nov.  2013,  AWS  announced  a  new  product  called 
CloudTrail14that logs all API calls. We evaluated the product, 
but  the  delay  (up  to  15  minutes)  between  a  call  and  its 
CloudTrail log appearing is not suitable for online diagnosis. 
SEC [15] is an event correlation tool for advanced event 
processing, which can be harnessed for event log monitoring, 
for  network  and  security  management,  for  fraud  detection, 
and  for  any  other  task,  which  involves  event  correlation. 
Event correlation is a procedure where a stream of events is 
processed,  in  order  to  detect  (and  act  on)  certain  event 
groups  that  occur  within  predefined  time  windows.  SEC 
reads  lines  from  files,  named  pipes,  or  standard  input, 
matches  the  lines  with  patterns  (like  regular  expressions  or 
Perl subroutines) for recognizing input events, and correlates 
events according to the rules in its configuration file(s). SEC 
can  produce  output  by  executing  external  programs 
(e.g., snmptrap or mail), by writing to files, by sending data 
to TCP and UDP based servers, by calling precompiled Perl 
subroutines,  etc.  LogMaster  [16]  is  an  event  correlation 
mining  system  and  an  event  prediction  system.  LogMaster 
parses  logs  into  event  sequences  where  each  event  is 
represented  as  an  informative  nine-tuple.  The  correlations 
between non-failure and failure events are very important in 
predicting failures. 
Source  code  is  the  schema  of  logs.  [17]  parses  console 
logs  by  combining  source  code  analyses.  Then  it  applies 
machine learning techniques to learn common patterns from 
a  large  amount  of  console  logs,  and  detect  abnormal  log 
patterns  that  violate  the  common  patterns.  Sherlog  [18]  is 
proposed  as  a  tool  to  analyze  source  code  by  leveraging 
information provided by run-times log to infer what must or 
may  have  happened  during  the  failed  production  run.  It 
requires neither re-execution of the program nor knowledge 
on  the  log's  semantics.  It  could  infer  both  control  and  data 
value  information  regarding  to  the  failed  execution.  It  uses 
runtime log to narrow down the possibilities in terms of both 
execution paths and states during the failed execution in the 
source code.  
D.  Intrusive Log improvement for error diagnosis 
While  there  are  a  number  of  “rules  of  thumb”  [19]  for 
designing better logging messages, these still do not capture 
the  specific  information.  LogEnhancer  [20]  automatically 
enhances  existing  logging  code  to  aid  future  post-failure 
debugging.  LogEnhancer  modifies  each  log  message  in  a 
given piece of software to collect additional causally related 
information to ease diagnosis in case of failures. It is the first 
attempt  to  systematically  and  automatically  enhance  log 
messages 
for 
diagnosis  in  case  of  failures.  [21]  proposes  an  approach 
based on software fault injection to assess the effectiveness 
of logs to keep track of software faults triggered in the field. 
It  provides  an  approach 
log 
effectiveness  in  a  quantitative  way.  This  work  motivates 
to  collect  causally-related 
the  built-in 
information 
to  assess 
14 Cloudtrail – http://aws.amazon.com/cloudtrail/  
262262262
their follow-up work [22], which leverage artifacts produced 
at  system  design  time  and  puts  forth  a  set  of  rules  to 
formalize  the  placement  of  the  logging  instructions  within 
the  software  fault  source  code  to  make  logs  effective  to 
analyze software failures.  
E.  Error Diagnosis during Normal Operation 
There are also statistics, machine learning and rule-based 
approaches  for  diagnosing  errors  during  normal  operation 
[23-27].  As  noted  earlier,  they  are  built  for  use  during 
“normal”  operations.  They  assume  a  system  has  normal 
operation  profiles  that  can  be  learned  from  historical  data 
and deviations from the profiles can help detect, localize and 
identify faults. 
Another  category  of  tools  are  distributed  tracing  tools 
such  as  Google’s  Drapper  [28],  Twitter’s  Zipkin  [29]  and 
Berkeley’s  X-Trace  [30].  They  are  usually  used  manually 
during  diagnosis.  There  is  also  no  notion  of  processes  and 
process contexts.  
VIII.  CONCLUSION AND FUTURE WORK 
Error diagnosis during sporadic operations is difficult, as 
the  systems  are  constantly  changing  and  there  are  no 
“normal”  profiles  to  help  to  detect  anomalies.  In  the  past, 
such sporadic operations were less frequent and done at off-
peak  times  or  during  scheduled  down  time.  But  nowadays, 
continuous  deployment  practices  are 
turning  sporadic 
operations into relatively high-frequency occurrences. In this 
paper,  we  propose  POD-Diagnosis  which  treats  a  sporadic 
operation  as  a  process,  detects  and  diagnoses  intermediate 
step  errors 
through  conformance  checking,  assertion 
evaluation,  and  on  demand  diagnosis  tests.  The  evaluation 
results  are  promising,  with  percentages  above  90%  for 
precision, recall, and diagnosis accuracy. We also discussed 
weaknesses  of  our  approach,  including  types  of  errors  that 
are challenging to detect with POD-Diagnosis.  
In order to simplify specifying boilerplate assertions, we 
are  designing  an  assertion  specification  language  at  the 
moment. We plan to automate  the generation of assertions. 
We  are  also  expanding  the  scope  into  more  types  of 
configurations and operation tasks inside a virtual machine. 
Finally, we are working towards releasing most of the tools 
and services described as open-source software.  
ACKNOWLEDGEMENTS 
We  would  like  to  thank  all  the  reviewers,  especially 
Matti Hiltunen, for guiding us to improve the paper. NICTA 
is  funded  by  the  Australian  Government  through  the 
Department of Communications and the Australian Research 
Council through the ICT Centre of Excellence Program.  
REFERENCES 
[1]  S. Kavulya, K. Joshi, F. Giandomenico, and P. Narasimhan, "Failure 
Diagnosis  of  Complex  Systems,"  Resilience  Assessment  and 
Evaluation of Computing Systems, K. Wolter, A. Avritzer, M. Vieira 
and A. van Moorsel, eds., pp. 239-261: Springer Berlin Heidelberg, 
2012. 
ACM Symposium on Operating Systems Principles (SOSP2009), Big 
Sky, MT, 2009. 
[18]  D.  Yuan,  H.  Mai,  W.  Xiong,  L.  Tan,  Y.  Zhou,  and  S.  Pasupathy, 
“SherLog:  Error  Diagnosis  by  Connecting  Clues  from  Run-time 
Logs,” in The 15th International Conference on Architectural Support 
for Programming Languages and Operating Systems (ASPLOS2010), 
Pittsburgh, Pennsylvania, US, 2010. 
[19]  S. Schmidt, "7 More Good Tips on Logging," 2009. 
[20]  D.  Yuan,  J.  Zheng,  S.  Park,  Y.  Zhou,  and  S.  Savage,  “Improving 
Software  Diagnosability  via  Log  Enhancement,”  in  The  16th 
International  Conference  on  Architectural  Support  for  Programming 
Languages and Operating Systems (ASPLOS2011), Newport Beach, 
California, USA, 2011. 
[21]  M. Cinque, D. Cotroneo, R. Natella, and A. Pecchia, “Assessing and 
improving  the  effectiveness  of  logs  for  the  analysis  of  software 
faults,”  in  The  40th  Annual  IEEE/IFIP  International  Conference  on 
Dependable  Systems  and  Networks  (DSN  2010),  Chicago,  IIIinois, 
USA, 2010. 
[22]  M.  Cinque,  D.  Cotroneo,  and  A.  Pecchia,  “Event  Logs  for  the 
Analysis  of  Software  Failures:  A  Rule-Based  Approach,”  IEEE 
Transactions on Software Engineering, vol. 39, no. 6, 2013. 
[23]  B.  Sharma,  P.  Jayachandran,  A.  Verma,  and  C.  R.  Das,  "CloudPD: 
Problem determination and diagnosis in shared dynamic clouds." pp. 
1-12. 
[24]  P.  Xinghao,  T.  Jiaqi,  K.  Soila,  G.  Rajeev,  and  N.  Priya,  “Ganesha: 
blackBox diagnosis of MapReduce systems,” SIGMETRICS Perform. 
Eval. Rev., vol. 37, no. 3, pp. 8-13, 2010. 
[25]  H. Nguyen, Z. Shen, Y. Tan, and X. Gu, "FChain: Toward Black-box 
Online Fault Localization for Cloud Systems ". 
[26]  T.  Jiaqi,  P.  Xinghao,  E.  Marinelli,  S.  Kavulya,  R.  Gandhi,  and  P. 
Narasimhan, "Kahuna: Problem diagnosis for Mapreduce-based cloud 
computing environments." pp. 112-119. 
[27]  P. K. Soila, D. Scott, J. Kaustubh, H. Matti, G. Rajeev, and N. Priya, 
“Draco: Statistical diagnosis of chronic problems in large distributed 
systems,”  in  Proceedings  of  the  2012  42nd  Annual  IEEE/IFIP 
International  Conference  on  Dependable  Systems  and  Networks 
(DSN), 2012. 
[28]  B.  H.  Sigelman,  and  Luiz  André  Barroso,  Dapper,  a  Large-Scale 
Systems 
"Distributed 
[29]  Twitter. 
Distributed Systems Tracing Infrastructure, Google, 2010. 
with 
Zipkin," 
https://blog.twitter.com/2012/distributed-systems-tracing-with-zipkin. 
[30]  R.  Fonseca,  G.  Porter,  R.  H.  Katz,  S.  Shenker,  and  I.  Stoica,  “X-
Trace:  A  Pervasive  Network  Tracing  Framework,”  in  The  4th 
USENIX  Symposium  on  Networked  Systems  Design  & 
Implementation (NSDI'07), Cambridge, MA, 2007 
Tracing 
[2]  X. Xu, I. Weber, L. Bass, L. Zhu, H. Wada, and F. Teng, “Detecting 
Cloud  Provisioning  Errors  Using  an  Annotated  Process  Model,”  in 
The  8th  Workshop  for  Next  Generation  Internet  Computing 
(MW4NG2013), Beijing, China, 2013. 
[3]  W.  v.  d.  Aalst,  Process  Mining:  Discovery,  Conformance  and 
Enhancement of Business Processes: Springer Verlag, 2011. 
[4]  A. Oliner, A. Ganapathi, and W. Xu, “Advances and challenges in log 
analysis,” Communications of the ACM, no. 55, pp. 2, 2012. 
[5]  Etsy, "Infrastructure upgrades with Chef," Code as Craft, 2013. 
[6]  T. Dumitra¸s, and P. Narasimhan, “Why Do Upgrades Fail and What 
Can  We  Do  about  It?  Toward  Dependable,  Online  Upgrades  in 
Enterprise  System,” 
in  ACM/IFIP/USENIX  10th  International 
Middleware Conference (MIDDLEWARE2009), Urbana Champaign, 
Illinois, USA, 2009. 
[7]  OMG, "Business Process Model and Notation V2.0," 2011. 
[8]  E.  Martin,  "Dealing  with  Eventual  Consistency  in  the  AWS  EC2 
API," Cloud Foundry Blog, 2013. 
[9]  AWS.  "Summary  of  the  December  24,  2012  Amazon  ELB  Service 
Region," 
Event 
US-East 
http://aws.amazon.com/message/680587/. 
the 
in 
[10]  AWS. 
"Error  Codes--Amazon  Elastic  Compute  Cloud," 
http://docs.aws.amazon.com/AWSEC2/latest/APIRefer
ence/api-error-codes.html. 
[11]  OpsCode. 
"About 
Exception 
and 
Report 
Handlers," 
http://docs.opscode.com/essentials_handlers.html. 
[12]  S.  Zhang,  and  M.  D.  Ernst,  “Automated  Diagnosis  of  Software 
Configuration Errors,” in 35th International Conference on Software 
Engineering (ICSE 2013), San Francisco, CA, USA, 2013. 
[13]  M.  Attariyan, 
and 
J.  Flinn, 
configuration 
troubleshooting with dynamic information flow analysis,” in The 9th 
USENIX  Conference  on  Operating  Systems  Design 
and 
Implementation (OSDI2010), Vancouver, BC, Canada, 2010. 
“Automating 
[14]  D.  Yuan,  Y.  Xie,  R.  Panigrahy,  J.  Yang,  C.  Verbowski,  and  A. 
Kumar, “Context-based Online Conﬁguration-Error Detection,” in the 
2011  USENIX  conference  on  USENIX  annual  technical  conference 
(USENIX ATC2011), Portland, OR, 2011. 
[15]  J.  P.  Rouillard,  “Real-time  log  file  analysis  using  the  Simple  Event 
Installation  System 
Correlator 
Administration Conference (LISA2004), Atlanta, GA, 2004. 
in  The  18th  Large 
(SEC),” 
[16]  X.  Fu,  R.  Ren,  J.  Zhan,  W.  Zhou,  Z.  Jia,  and  G.  Lu,  “LogMaster: 
Mining Event Correlations in Logs of Large-scale Cluster Systems,” 
in The 31st International Symposium on Reliable Distributed Systems 
(SRDS2012), Irvine, California, 2012. 
[17]  W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting 
Large-Scale System Problems by Mining Console Logs,” in The 22nd 
263263263