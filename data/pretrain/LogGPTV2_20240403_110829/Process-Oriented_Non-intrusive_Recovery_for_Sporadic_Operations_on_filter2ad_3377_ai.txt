2016). 
R8: Generalizability of Recovery 
Many existing cloud recovery mechanisms (J. Simmonds, et al., 2010; S. Nelson-Smith, 2011; J. 
Behl,  et  al.,  2012;  Z.  B.  Zheng,  et  al.,  2010)  are  either  operation/application  specific  or  for 
normal  activities  of  applications.  They  usually  depend  on  the  contextual  knowledge  of  the 
applications or operations. Applying them to different applications or operations will introduce 
additional effort. A requirement for our recovery service is that it can be applied to a different 
context with minimal effort (M. Fu, et al., 2016). 
3.6  Research Challenges 
For each of the afore-mentioned eight recovery requirements, making the recovery methodology 
able to fulfil them is not straightforward. Hence, the research challenges with our non-intrusive 
recovery  methodology  are  determined  to  be  in  line  with  the  eight  recovery  requirements 
proposed  by  us.  The  research  challenges  are:  1)  how  to  achieve  runtime  recovery;  2)  how  to 
make  recovery  satisfy  RTO  (Recovery Time  Objective);  3)  how to  reduce  negative impact  on 
cloud  system  incurred  by  recovery;  4)  how  to  reduce  monetary  cost  incurred  by  recovery;  5) 
how  to  recover  from  errors  without  known  causes;  6)  how  to  deal  with  false  positives  of  the 
error  detection  service;  7)  how  to  recover  for  recovery  itself;  8)  how  to  make  the  recovery 
generalizable to be applicable for different types of sporadic operations on cloud. Importantly, 
the proposed recovery methodology requires maintenance  of the various recovery patterns and 
recovery actions, as well as the generated system states and checkpoints. Hence, there are some 
additional challenges related to how to manage and maintain recovery patterns, recovery actions, 
and  system  resource  states  and  checkpoints.  All  of  the  challenges  are  addressed  in  POD-
Recovery, the non-intrusive recovery methodology proposed by us. 
51 
Chapter 4.  Overview of POD-Recovery 
POD-Recovery  is  a  non-intrusive  recovery  service,  embodying  the  concept  of  “Recovery  as  a 
Service” (RaaS) (T. Wood, et al., 2010; S. Subashini and V. Kavitha, 2011; B. R. Kandukuri, et 
al.,  2009).  POD-Recovery  does  not  change  any  of  the  source  code  of  the  operation  and  is 
thereby non-intrusive. POD-Recovery is based on the following assumptions: 1) the operation’s 
process  model  with  the  timestamp  of  each  step  can  be  mined  from  cloud  operational  logs;  2) 
cloud API call history logs with API call timestamps are available and accessible for mapping 
from  process  steps  to  relevant  cloud  APIs;  3)  cloud  logs  are  accessible  for  error  detection;  4) 
cloud APIs are accessible by external parties (e.g. cloud consumers or automation tools that can 
start and stop virtual machines, register them with load balancers, etc.);  5) cloud resources are 
accessible from  external  parties (M.  Fu,  et  al.,  2016).  Currently,  all  major  public  clouds  fulfil 
those  assumptions.  Specifically,  sporadic  operations  on  all  major  public  clouds  are  able  to 
generate timestamped cloud operational logs which can be analysed and mined to figure out the 
process models of those sporadic operations, and all major public clouds are able to generate the 
logs which track the cloud APIs call history for each sporadic operation, and hence the mapping 
between each operational process step and its corresponding cloud APIs can always be available 
for all the major public clouds. All public clouds allow users and external parties to get access 
to cloud logs for error detection, get access to cloud APIs for cloud manipulation and get access 
to cloud resources for cloud interaction. For private clouds, it is subject to the management tools 
and  their  configurations  whether  those  assumptions  are  met,  in  full  or  in  parts  (M.  Fu,  et  al., 
2016).  
POD-Recovery  requires  a  process  model  of  an  operation,  e.g.  the  process  model  of  rolling 
upgrade obtained by process mining (L. Bass, et al., 2015; W. V. D. Aalst, 2011). The overview 
of POD-Recovery is illustrated in Fig. 12 (M. Fu, et al., 2014; M. Fu, et al., 2015; M. Fu, et al., 
2016). Before an operation begins, we perform once-off off-line activities. First, we determine 
the  operation  resource  space  that  governs  the  expected  states  and  captured  states  during  the 
operation  being  performed.  The  inputs  are  the  mined  process  model  together  with  the 
timestamps of each step of the process model, cloud API call logs which contain the cloud API 
call history from past successful runs of the operation (e.g. AWS CloudTrail logs (CloudTrail, 
2016)),  and  the  API-Resource  mapping  table  which  specifies  the  cloud  resources  manipulated 
by each cloud API involved in the operation. Cloud API call logs list  the API  calls that took 
place, their timestamps, which API operation was called, parameters of relevant API calls, and 
the response information of each API call. The logs are automatically processed to identify the 
cloud resources being manipulated. Then using the information in cloud API call logs such as 
AWS CloudTrail logs (CloudTrail, 2016), we automatically generate a template for the expected 
52 
state  of  the  affected  resources  at  each  process  step.  We  do  this  by  correlating  API  calls  with 
steps in the process model using timestamps. The templates include variables, e.g. for resource 
IDs, whose values need to be obtained at operation runtime. Finally, the recovery points inside 
an  operational  process  are  determined.  The  failures  occurring  within  the  operation  step(s) 
between two adjacent recovery points are recoverable. At runtime, the external error detection 
service observes the cloud operation, e.g. through its logs, and invokes the recovery service if 
errors  are  detected.  The  recovery  service  then  obtains  the  current  state  of  the  relevant  cloud 
resources  by  using  the  cloud  resource  state  capturing  service.  This  current  state  of  these 
resources can be more recent than the erroneous state identified by the error detection service so 
it effectively serves as  a  double  check  to  reduce false  positives  from  error detection.  Both  the 
generated  expected  resource  state  templates  and  the  captured  resource  states  are  stored  as 
computer-recognizable  data  structures,  such  as  XML.  Error  recovery  will  be  triggered  after 
detecting  the  error  for  a  certain  step  section.  In  the  error  recovery  procedure,  there  are  eight 
recovery  patterns:  1)  Compensated  Undo  &  Redo;  2)  Compensated  Undo  &  Alternative;  3) 
Rewind  &  Replay;  4)  Rewind  &  Alternative;  5)  Reparation;  6)  Direct  Redo;  7)  Direct 
Alternative;  and  8)  Farther  Undo  &  Redo.  Specifically,  Compensated  Undo  means  relying  on 
the  expected  states  generated  to  go  back  to  the  previous  state,  Rewind  means  relying  on  the 
captured states (checkpoints) to go back to the previous state, Replay/Redo means re-executing 
the  same  steps,  Reparation  means  bringing  the  system  resources  to  a  desired  resource  state 
directly  from  the  erroneous  state  and  Alternative  means  executing  the  alternative  step  of  a 
certain step. Among the eight recovery patterns,  maybe not all of them are applicable. Hence, 
we  determine  the  applicable  recovery  patterns  by  performing  state  reachability  checking, 
idempotence checking and step alternative existence checking. Each applicable recovery pattern 
could contain several recovery actions. Hence, after filtering the applicable recovery patterns we 
automatically generate the recovery actions for each applicable recovery pattern by using an AI 
planning technique (J. Hoffmann, et al., 2012). Importantly, certain recovery actions within the 
applicable  recovery  patterns  might  not  be  acceptable  because  they  may  fail  in  satisfying 
recovery requirements defined by the business. We compute the values of the recovery metrics 
of each recovery action. The recovery metrics are Recovery Time, Recovery Cost and Recovery 
Impact.  Finally,  we  select  the  acceptable  recovery  action  which  satisfies  the  recovery 
requirements  specified  by  the  business  stakeholders  and  execute  it.  I  have  proposed  two 
selection  methods  for  selecting  the  acceptable  recovery  actions:  1)  Pareto  set  search  based 
selection (M. Fu, et al., 2015; M. Fu, et al., 2016) and 2) User constraints based selection (M. Fu, 
et  al.,  2015;  M.  Fu,  et  al.,  2016).  The  service-oriented  design  has  two  benefits  here:  1)  the 
recovery service can be implemented in any programming language; 2) the recovery service is 
more  generalizable and  easy  to  maintain  since it  is  independent of the error  detection  service. 
53 
Fig. 12.  Overview of POD-Recovery. 
54 
We use the rolling upgrade operation as the illustrating example to explain how POD-Recovery 
works  for  a  sporadic  operation  on  cloud.  As  mentioned  in  section  1.2.2,  the  rolling  upgrade 
operation has seven steps: 1) create new launch configuration; 2) update the auto scaling group; 
3)  set  user-specified  rolling  policy;  4)  deregister  instances  from  the  elastic  load  balancer;  5) 
terminate  the  instances;  6)  launch  new  cloud  instances;  7)  register  the  new  instances  with  the 
elastic load balancer. Steps 4 to 7 are repeated until all the old cloud instances are upgraded (M. 
Fu, et al., 2014). The operation logs and cloud API call history logs are available after running 
this  rolling  upgrade  operation  on  AWS  cloud  in  the  past.  The  process  model  for  the  rolling 
upgrade  operation  is  first  created  by  mining  the  operations  logs  provided  by  Asgard,  and  the 
resource space as well as expected resource state templates  for all the steps of rolling upgrade 
are generated by analysing the cloud API call history logs and correlating them with the process 
model  of  rolling  upgrade.  Next,  the  recovery  points  inside  the  rolling  upgrade  process  are 
determined and failure detection and recovery is performed after each  recovery point. The first 
recovery  point  is  determined  to  be  after  step  3  (set  user-specified  rolling  policy),  and  the 
remaining  four  recovery  points  are  determined  to  be  after  step  4,  step  5,  step  6  and  step  7, 
respectively.  Taking  the  second  recovery  point  as  an  example,  if  failures  occur  after  this 
recovery point, initially there are eight recovery patterns available, and the applicable recovery 
patterns  will  be  first  filtered  from  these  eight  recovery  patterns,  followed  by  selecting  the 
optimal  or  acceptable  recovery  pattern  from  the  applicable  ones.  After  the  recovery  for  the 
second  recovery  point  is  done,  the  operation  continues  its  execution  and  failure  detection  and 
recovery is performed for the remaining recovery points as well. 
Fig.  13  describes  the  detailed  generalized  recovery  workflow  (M.  Fu,  et  al.,  2016).  This 
recovery  workflow  is  applicable  for  all  the  different  operational  steps  within  a  sporadic 
operation  and  is  applicable  for  different  types  of  sporadic  operations  as  well.  Before  the 
operation  starts,  the  off-line  activities  (resource  space  generation  and  expected  resource  state 
templates generation) are performed. Say, an error is detected after a recovery point (e.g. Step 
Section 1). Then the current erroneous resource state is captured (e.g. Resource State 1’). The 
recovery  service  also  retrieves  the  expected  resource  state  template  after  the  current  recovery 
point and sets the variable values from  the operation logs (e.g. Expected State 1). Meanwhile, 
the  recovery  service  also  retrieves  the  expected  resource  state  template  before  the  current 
recovery point and sets the variable values from the operation logs (e.g. Expected State 0). The 
recovery service retrieves a previous consistent resource state (e.g. Resource State 0). For Step 
Section  1,  only  these  four  states  are  used  for  recovery  because  there  is  no  farther  expected 
resource state of this step section. For the step sections after Step Section 1 (e.g. Step Section 2), 
five  states  (including  the  farther  expected  resource  state  of  this  step  section)  are  used  for 
recovery. Compensated Undo & Redo as well as Compensated Undo & Alternative require the 
55 
previous consistent expected state and the current erroneous resource state; Rewind & Replay as 
well  as  Rewind  &  Alternative  require  the  previous  consistent  resource  state  and  the  current 
erroneous  resource  state;  Reparation  requires  the  expected  resource  state  and  the  erroneous 
resource  state;  Farther  Undo  &  Redo  requires  the  farther  expected  resource  state  and  the 
erroneous  resource  state  (M.  Fu,  et  al.,  2016).  The  remaining  two  recovery  patterns,  Direct 
Undo and Direct Alternative, do not rely on resource states. After recovery, the latest resource 
state (e.g. Resource State 1) will be captured again before continuing to the next step section, 
and this latest resource state serves as the next step section’s previous consistent resource state 
(M. Fu, et al., 2016). 
Fig. 13.  Generalized Recovery Workflow. 
Now,  we  explain  why  the  design  of  POD-Recovery  is  able  to  satisfy  all  the  eight  recovery 
requirements.  Again,  the  eight  recovery  requirements  are:  1)  Runtime  Recovery  (R1);  2) 
Recovery  Satisfying  RTO  (R2);  3)  Reducing  Negative  Impact  on  Cloud  System  (R3);  4) 
Reducing  Monetary  Cost  of  Recovery  (R4);  5)  Recovery  from  Errors  without  Known  Causes 
(R5); 6) Dealing with False Positives of Error Detection (R6); 7) Recovery for Recovery Itself 
(R7); 8) Generalizability of Recovery (R8). For R1, the error detection is performed at runtime 
56 
and recovery is triggered immediately after error detection, so the proposed recovery framework 
enables a runtime recovery. For R2, the proposed recovery framework will perform a selection 
optimization  on  the  recovery  action  candidates  and  select  the  recovery  action  which  has  a 
minimum  recovery  time,  so  the  proposed  recovery  framework  is  able  to  meet  RTO.  For  R3, 
likewise,  the  proposed  recovery  framework  will  perform  a  selection  optimization  on  the 
recovery  action  candidates  and  select  the  recovery  action  which  has  a  minimum  recovery 
negative  impact,  so  the  proposed  recovery  framework  is  able  to  reduce  bad  consequence  on 
cloud  system.  For  R4,  likewise,  the  proposed  recovery  framework  will  perform  a  selection 
optimization  on  the  recovery  action  candidates  and  select  the  recovery  action  which  has  a 
minimum  recovery  monetary  cost,  so  the  proposed  recovery  framework  is  able  to  reduce 
recovery’s monetary cost. For R5, the recovery framework depends on a global resource state 
space which includes all the error cause items in the cloud system, so the error causes will be 
catered for and recovered without knowing what they are. For R6, the double check mechanism 
in  the  proposed  recovery  framework  is  able  to  guarantee  that  no  recovery  will  be  triggered  if 
there is no error occurred or the error has been self-healed before recovery action is taken, so the 
recovery  framework  is  able  to  handle  false  positives from  the  error  detection  service.  For  R7, 
the exception handling mechanism within the selected recovery action is able to deal with errors 
occurring during the recovery, so the recovery framework is able to recover for recovery itself. 
For R8, the resource space determination, the expected resource states generation, resource state 
capturing,  applicable  recovery  patterns  filtering  and  state  transition  based  recovery  plan 
generation  are  all  designed  to  be  applicable  for  any  sporadic  operations  on  cloud,  so  the 
recovery framework enables generalizability. 
The  offline  and  online  components  in  POD-Recovery  can  be  further  categorized  into  the 
following three frameworks: 1) the recovery-oriented analysis framework which is responsible 
for  determining  the  recovery  points  inside  the  sporadic  operation;  2)  the  resource  state 
management framework which consists of four parts: operational resource space determination, 
resource  state  capturing,  expected  resource  state  templates  generation  and  populating  missing 
values  in  expected  resource  state  templates;  3)  the  recovery  actions  generation  and  selection 
framework  which  consists  of  the  following  seven  parts:  handling  false  positives  of  error 
detection, preparing the eight recovery patterns, applicable recovery patterns filtering, recovery 
actions  generation,  applicable  recovery  actions  evaluation  based  on  three  evaluation  metrics, 
selection  of  acceptable recovery  actions from  recovery  action  candidates  list  and  mapping  the 
selected  recovery  actions  into  executable  code.  We  expand  on  these  three  frameworks  in 
chapters 5-7. 
57 
Chapter 5.  Recovery-Oriented Analysis in POD-Recovery 
In this chapter, we discuss how we make recovery-oriented analysis for sporadic operations on 
cloud.  By  recovery-oriented  analysis,  we  mean  the  procedure  of  determining  recovery  points 
inside a sporadic operation (M. Fu, et al., 2013; M. Fu, et al., 2014). We define recovery points 
in a sporadic operation to be the positions in the operation where the presence of errors should 
be checked and, if needed, recovery should be triggered (M. Fu, et al., 2016). A recovery point 
thus also doubles as a consistent checkpoint: if an error is detected in the current recovery point, 
the  previous  recovery  point  is  the  last  consistent  checkpoint  (M.  Fu,  et  al.,  2016).  Recovery 
points  determination  is  based  on  the  process  model  of  the  sporadic  operation,  and  hence  the 
purpose of this chapter is to discuss why we model a sporadic operation as a process and how 
we leverage on the process model to divide it into several recoverable sections for performing 
operational  recovery  in  a  runtime  way.  As  such,  recovery  points  determination  addresses  the 
challenge of how to achieve a runtime recovery. 
First, we treat a sporadic operation as a process (section 5.1). Second, we define the criteria for 
determining recovery points (section 5.2). Third, we manually determine the recovery points for 
the sporadic operation based on the criteria (section 5.3).  
5.1  Operations as Processes 
Like WS-BPEL processes (L. Baresi and S. Guinea, 2005; E. Juhnke, et al., 2009), a sporadic 
operation on cloud manifests itself as a series of action steps executed by agents (e.g. automated 
scripts or tools or human) requiring various resources (computing power, readied environment, 
nodes,  etc.)  (X.  Xu,  et  al.,  2013;  M.  Fu,  et  al.,  2014).  Hence,  our  approach  first  models  and 
analyses a sporadic operation as a process (M. Fu, et al., 2013; M. Fu, et al., 2014; L. Bass, et al., 
2015).  For  example,  the  cloud  rolling  upgrade  operation  in  Asgard  (Asgard,  2013)  can  be 
modelled as an operational process that consists of a set of operational steps (M. Fu, et al., 2013; 
M.  Fu,  et  al.,  2014;  M.  Fu,  et  al.,  2015;  M.  Fu,  et  al.,  2016).  By  modelling  the  sporadic 
operation as a process, we are able to obtain the workflow, the purpose and the functionalities of 
the  sporadic  operation,  and  we  are  also  able  to  know  the  contextual  information  of  each 
operational  step  of  the  sporadic  operation,  i.e.  the  purpose  of  each  operational  step  and  the 
meaning of each operational step. 
5.2  Recovery Points Determination Criteria 
After we treat a sporadic operation as a process, we can determine its recovery points based on 
certain  criteria.  The  criteria  for  recovery  points  determination  contain  several  aspects:  1) 
Atomicity to support the imposition of all-or-nothing transactions on portions of the process; 2) 
58 
Idempotence  to  enable  the  same  or  parameterized  actions  to  be  re-executed;  3)  Recovery 
Actions Identifiable to allow proper recovery actions to be executed (M. Fu, et al., 2013; M. Fu, 
et al., 2014). 
1)  Ensuring  Atomicity:  Similar  to  the  atomicity  in  WS  transactions  (F.  Cabrera,  et  al.,  2001) 
and DB transactions (T. Haerder and A. Reuter, 1983), atomicity in our division criteria means 
to achieve all-or-nothing for a group of actions. For instance, if removing an old AMI from an 
ASG  (auto-scaling  group)  or  attaching  a  new  AMI  to  this  ASG  fails,  the  ASG  will  fail  to  be 
updated,  so  they  must  both  complete  successfully  and  should  be  put  into  an  atomic  group. 
Atomicity can help maintain the process in a consistent state when rolling back certain steps for 
the recovery. Dividing by Atomicity makes the process into several atomic action-groups, and 
this is the first step in our process division (M. Fu, et al., 2014).   
2)  Ensuring  Idempotence:  In  our  process  division  criteria,  Idempotence  means  to  enable  the 
same or parameterized actions to be re-executed without changing the result. After the operation 
process  is  divided  into  several  atomic  action-groups,  some  of  them  should  be  combined  as  a 
section  in order to  make  this section  idempotent.  If  a  section is idempotent,  it means  that  this 
section, no matter parameterized or non-parameterized, and no matter being executed how many 
times,  will  always  return  the  same  execution  result.  For  example,  when  using  Chef  Script  to 
remove an old AMI from the existing LC (launch configuration) and attach a new AMI to the 
existing  LC,  the  result  will  always  be  the  same  (new  AMI  attached  to  the  LC),  regardless  of 
execution  times.  Idempotent  sections,  in  some  cases,  can  be  recovered  by  re-executing  them. 
Since  Idempotence  can  ensure  that  the  execution  result  won’t  change,  Idempotence  should  be 
included in the criteria (M. Fu, et al., 2014). 
3)  Ensuring  Recovery  Actions  Identifiable:  Recovery  Actions  Identifiable  means  to  allow 
recovery  actions  to  be  taken  for  recovery.  When  dividing  the  operation  process  in  a 
recoverability-oriented  way,  the  key  point  is to  make  sure  each  section  can  be  recoverable  by 
certain recovery action(s). For example, after the step of updating existing ASG, if  the ASG is 
wrongly  reattached  with  old  LC  by  another  team,  we  can  recover  it  by  recalling  the  API 
function  of  “UpdateAutoScalingGroup”  (AWS,  2016).  Since  the  goal  is  to  facilitate 
recoverability, the aspect of  Recovery Actions Identifiable should be necessarily included into 
the recovery points determination criteria (M. Fu, et al., 2014). 
5.3  Recovery Points Determination 
We use the above-mentioned criteria to determine recovery points inside a sporadic operation. 
First of all, Atomicity is used to break the operation process into atomic action-groups, followed 
by  Idempotence  to  combine  certain  action-groups  into  idempotent  sections.  Next,  we  have  to 
59 
make  sure  that  for  each  section  certain  recovery  actions  can  be  identified  to  recover  from  the 
errors inside  (M.  Fu,  et  al.,  2014).  We  demonstrated how  to  utilize  these criteria  to  determine 
recovery  points  by  using  the  rolling  upgrade  operation  as  a  case  study,  and  presented  the 
demonstration and the case study in our previous work (M. Fu, et al., 2014). The approach for 
determining recovery points is feasible for different types of sporadic operations on cloud and is 
able  to  apply  to  multiple  cloud  platforms,  because  of  three  reasons:  1)  the  criteria  are 
determined based on understanding the features and characteristics of various types of sporadic 
operations  on  cloud  ;  2)  the  same  sporadic  operation  performed  on  different  cloud  platforms 
shares the same workflow and requires the same set of resources; 3) the correlation between the 
operational log lines and the process steps is always able to be determined for different types of 
cloud  platforms  since  the  timestamps  of  each  log  line  and  each  process  step  are  available  for 
different clouds.  One thing to note is that sometimes not all the  three criteria are required for 
recovery points determination and only a subset of the criteria are needed (M. Fu, et al., 2016). 
For  a  particular  sporadic  operation,  which  of  these  criteria  should  be  used  depends  on  the 
features and characteristics (e.g. how many operational steps, what the operational steps are, etc.) 
of  the  sporadic  operation  (M.  Fu,  et  al.,  2016).  The  recovery  points  determination  results  for 
each  of  the  sporadic  operations  evaluated  in  this  research  are  presented  in  the  chapter  of 
experimental evaluation (chapter 8). 
60 
Chapter 6.  Resource State Management in POD-Recovery  
In  POD-Recovery,  the  resource state  management  framework  consists  of  four components:  1) 
Operational  resource  space  determination;  2)  Resource  state  capturing;  3)  Expected  resource 