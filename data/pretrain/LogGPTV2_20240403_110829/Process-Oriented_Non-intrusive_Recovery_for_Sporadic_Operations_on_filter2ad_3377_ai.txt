### 2016). 
#### R8: Generalizability of Recovery
Many existing cloud recovery mechanisms (J. Simmonds, et al., 2010; S. Nelson-Smith, 2011; J. Behl, et al., 2012; Z. B. Zheng, et al., 2010) are either specific to certain operations or applications, or are designed for normal application activities. These mechanisms often rely on contextual knowledge of the applications or operations, making their application to different contexts challenging and requiring additional effort. A key requirement for our recovery service is that it can be easily adapted to different contexts with minimal effort (M. Fu, et al., 2016).

### 3.6 Research Challenges
Addressing the eight recovery requirements outlined above is not straightforward. The research challenges associated with our non-intrusive recovery methodology are aligned with these requirements:
1. Achieving runtime recovery.
2. Ensuring recovery meets the Recovery Time Objective (RTO).
3. Minimizing the negative impact on the cloud system during recovery.
4. Reducing the monetary cost incurred by recovery.
5. Recovering from errors with unknown causes.
6. Handling false positives from the error detection service.
7. Enabling recovery for the recovery process itself.
8. Making the recovery generalizable to various types of sporadic operations on the cloud.

Additionally, the proposed recovery methodology requires the maintenance of various recovery patterns, recovery actions, and the generated system states and checkpoints. This introduces further challenges related to managing and maintaining these elements. All these challenges are addressed in POD-Recovery, our non-intrusive recovery methodology.

### Chapter 4. Overview of POD-Recovery
POD-Recovery is a non-intrusive recovery service that embodies the concept of "Recovery as a Service" (RaaS) (T. Wood, et al., 2010; S. Subashini and V. Kavitha, 2011; B. R. Kandukuri, et al., 2009). It does not modify any source code of the operation, ensuring its non-intrusive nature. POD-Recovery is based on the following assumptions:
1. The operation's process model, including timestamps for each step, can be mined from cloud operational logs.
2. Cloud API call history logs with timestamps are available and accessible for mapping process steps to relevant cloud APIs.
3. Cloud logs are accessible for error detection.
4. Cloud APIs are accessible to external parties, such as cloud consumers or automation tools.
5. Cloud resources are accessible to external parties (M. Fu, et al., 2016).

Currently, all major public clouds meet these assumptions. Specifically, sporadic operations on major public clouds generate timestamped operational logs, which can be analyzed to derive process models. These logs also track API call history, enabling the mapping between operational steps and corresponding cloud APIs. Public clouds allow users and external parties to access logs for error detection, APIs for manipulation, and resources for interaction. For private clouds, the fulfillment of these assumptions depends on the management tools and their configurations (M. Fu, et al., 2016).

POD-Recovery requires a process model of the operation, such as the process model of a rolling upgrade obtained through process mining (L. Bass, et al., 2015; W. V. D. Aalst, 2011). The overview of POD-Recovery is illustrated in Fig. 12 (M. Fu, et al., 2014; M. Fu, et al., 2015; M. Fu, et al., 2016).

Before an operation begins, we perform one-time offline activities:
1. Determine the operation resource space, which defines the expected and captured states during the operation.
2. Inputs include the mined process model with timestamps, cloud API call logs from past successful runs, and an API-Resource mapping table.
3. Automatically generate templates for the expected state of affected resources at each process step using the information in the API call logs.
4. Identify recovery points within the operational process. Failures occurring between two adjacent recovery points are recoverable.

At runtime, an external error detection service monitors the cloud operation through logs and invokes the recovery service if errors are detected. The recovery service then captures the current state of relevant cloud resources, which serves as a double check to reduce false positives. Both the expected and captured resource states are stored as computer-recognizable data structures, such as XML.

Error recovery is triggered after detecting an error for a specific step section. There are eight recovery patterns:
1. Compensated Undo & Redo
2. Compensated Undo & Alternative
3. Rewind & Replay
4. Rewind & Alternative
5. Reparation
6. Direct Redo
7. Direct Alternative
8. Farther Undo & Redo

Each pattern has specific meanings:
- **Compensated Undo**: Reverts to the previous state using expected states.
- **Rewind**: Reverts to the previous state using captured states (checkpoints).
- **Replay/Redo**: Re-executes the same steps.
- **Reparation**: Brings the system to a desired state directly from the erroneous state.
- **Alternative**: Executes an alternative step.

Not all patterns may be applicable, so we filter them based on state reachability, idempotence, and step alternative existence. We then generate recovery actions for each applicable pattern using AI planning techniques (J. Hoffmann, et al., 2012). Some recovery actions may not be acceptable due to business-defined recovery requirements. We compute the values of recovery metrics (Recovery Time, Recovery Cost, and Recovery Impact) and select the acceptable recovery action that satisfies the business stakeholders' requirements. Two selection methods are proposed:
1. Pareto set search-based selection (M. Fu, et al., 2015; M. Fu, et al., 2016)
2. User constraints-based selection (M. Fu, et al., 2015; M. Fu, et al., 2016)

The service-oriented design offers two benefits:
1. The recovery service can be implemented in any programming language.
2. The recovery service is more generalizable and easier to maintain, as it is independent of the error detection service.

### Rolling Upgrade Example
We use the rolling upgrade operation as an example to illustrate how POD-Recovery works for a sporadic operation on the cloud. The rolling upgrade operation consists of seven steps:
1. Create a new launch configuration.
2. Update the auto-scaling group.
3. Set a user-specified rolling policy.
4. Deregister instances from the elastic load balancer.
5. Terminate the instances.
6. Launch new cloud instances.
7. Register the new instances with the elastic load balancer.

Steps 4 to 7 are repeated until all old instances are upgraded (M. Fu, et al., 2014). After running the rolling upgrade operation on AWS, the process model is created by mining the Asgard operation logs. The resource space and expected resource state templates for each step are generated by analyzing the cloud API call history logs and correlating them with the process model.

Recovery points are determined, and failure detection and recovery are performed after each point. The first recovery point is after step 3, and the remaining four points are after steps 4, 5, 6, and 7, respectively. If a failure occurs after a recovery point, the applicable recovery patterns are filtered, and the optimal or acceptable pattern is selected. After recovery, the operation continues, and failure detection and recovery are performed for the remaining points.

### Generalized Recovery Workflow
Fig. 13 describes the detailed generalized recovery workflow (M. Fu, et al., 2016). This workflow is applicable to different operational steps within a sporadic operation and to different types of sporadic operations. Before the operation starts, offline activities (resource space generation and expected resource state templates generation) are performed. If an error is detected after a recovery point, the current erroneous resource state is captured. The recovery service retrieves the expected resource state templates before and after the current recovery point and sets variable values from the operation logs. The recovery service also retrieves a previous consistent resource state. For the first step section, only four states are used for recovery. For subsequent step sections, five states are used. Different recovery patterns require different combinations of these states. After recovery, the latest resource state is captured before continuing to the next step section.

### Satisfying Recovery Requirements
POD-Recovery is designed to meet the eight recovery requirements:
1. **Runtime Recovery (R1)**: Error detection and recovery are performed at runtime.
2. **Recovery Satisfying RTO (R2)**: The framework selects the recovery action with the minimum recovery time.
3. **Reducing Negative Impact (R3)**: The framework selects the recovery action with the minimum negative impact.
4. **Reducing Monetary Cost (R4)**: The framework selects the recovery action with the minimum monetary cost.
5. **Recovering from Unknown Causes (R5)**: The framework uses a global resource state space to recover without knowing the error cause.
6. **Handling False Positives (R6)**: The double-check mechanism ensures no unnecessary recovery.
7. **Recovery for Recovery (R7)**: The exception handling mechanism deals with errors during recovery.
8. **Generalizability (R8)**: The resource space determination, expected state generation, and recovery plan generation are designed to be applicable to any sporadic operation.

### Offline and Online Components
POD-Recovery can be categorized into three frameworks:
1. **Recovery-Oriented Analysis Framework**: Determines recovery points inside the sporadic operation.
2. **Resource State Management Framework**: Consists of operational resource space determination, resource state capturing, expected resource state templates generation, and populating missing values.
3. **Recovery Actions Generation and Selection Framework**: Handles false positives, prepares recovery patterns, filters applicable patterns, generates recovery actions, evaluates actions, selects acceptable actions, and maps them into executable code. These frameworks are expanded upon in Chapters 5-7.

### Chapter 5. Recovery-Oriented Analysis in POD-Recovery
This chapter discusses the recovery-oriented analysis for sporadic operations on the cloud. By recovery-oriented analysis, we mean the procedure of determining recovery points within a sporadic operation (M. Fu, et al., 2013; M. Fu, et al., 2014). A recovery point is a position in the operation where errors should be checked, and if needed, recovery should be triggered. A recovery point also serves as a consistent checkpoint.

**Recovery Points Determination:**
1. **Operations as Processes**: Treat a sporadic operation as a process, similar to WS-BPEL processes (L. Baresi and S. Guinea, 2005; E. Juhnke, et al., 2009). Model the operation to understand its workflow, purpose, and functionalities.
2. **Recovery Points Determination Criteria**:
   - **Atomicity**: Ensure all-or-nothing transactions for portions of the process.
   - **Idempotence**: Allow re-execution of the same or parameterized actions without changing the result.
   - **Recovery Actions Identifiable**: Ensure that proper recovery actions can be executed.
3. **Recovery Points Determination**: Use the criteria to determine recovery points. First, break the operation into atomic action-groups, then combine certain groups into idempotent sections. Ensure that each section can be recovered by identifiable actions. The approach is feasible for different types of sporadic operations and multiple cloud platforms.

### Chapter 6. Resource State Management in POD-Recovery
In POD-Recovery, the resource state management framework consists of four components:
1. **Operational Resource Space Determination**: Define the expected and captured states during the operation.
2. **Resource State Capturing**: Capture the current state of resources.
3. **Expected Resource State Templates Generation**: Generate templates for the expected state of affected resources.
4. **Populating Missing Values in Expected Resource State Templates**: Set variable values from the operation logs.

These components ensure that the recovery service can effectively manage and maintain the necessary states and checkpoints for recovery.