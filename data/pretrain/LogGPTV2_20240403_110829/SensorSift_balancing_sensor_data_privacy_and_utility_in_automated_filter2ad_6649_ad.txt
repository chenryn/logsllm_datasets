negative correlations).
A regression analysis model attributes 63% of the PubLoss es-
timate to correlation and 37% to overlap. In the case of PrivLoss
the weights correspond to 67% and 33% for correlation and over-
lap respectively. Furthermore, using regression we ﬁnd that cor-
relation alone as a predictor leads to a SSE (sum of squared er-
ror) term which is 315% larger than if correlation and overlap are
used together. These ﬁndings suggest that correlation and overlap
should be considered together when analyzing sifting performance.
Extensions: Complex Policies. Although the bulk of our analy-
sis has focused on policies in which there is one public and one
private attribute, our algorithm can be augmented to support multi-
ple public and multiple private attributes. To illustrate the potential
156
for more complex policies, we modiﬁed the PPLS objective func-
tion to produce the largest average gap between multiple public and
multiple private attribute covariances relative to data features.
f ind maxw[avg(cov(Xw,Y +
−λ∗ avg(cov(Xw,Y
1
.
s.t.
w(cid:2)w = 1
),cov(Xw,Y +
), ...)2
−
−
2
),cov(Xw,Y
2
1
), ...)2]
Using this averaging method, we were able to ﬁnd high performing
masks for various policies which include several public and/or sev-
eral private attributes. An example of two such policies is provided
in Figure 3.
Complex policies can include arbitrary ratios of (public:private)
1:2, 1:3, 2:1, 2:2, 3:1 (i.e., public: ‘Male’ + ‘Smiling’, private:
‘White’ + ‘Youth’). The number of complex policy combinations
is very large, however in our tests using (35 complex policies) we
found that the same principles from Section 7 apply. Just as in
the case of simple policies correlations and overlap have a big im-
pact on PubLoss and PrivLoss. In general as polices grow to in-
clude many attributes the likelihood of signiﬁcant correlation/over-
lap grows thus increasing the chance of diminishing utility and pri-
vacy balance. A more detail analysis of complex policies is a deep
topic which is certainly an attractive target for future work.
Extensions: Streaming Content.
So far we have focused our
analysis on static sensor samples (i.e., still photos), however dy-
namic data (i.e. streaming video) is also of importance. To evalu-
ate the SensorSift scheme in a dynamic context we used the Talk-
ing Face dataset [3]. The data consists of 5000 frames taken from
a 29fps video of a person engaged in a natural conversation last-
ing roughly 200 seconds. Using the annotations provided from the
dataset we ﬁrst cropped the face region from each frame. Next we
extracted image features as described in Section 5. Subsequently
we used the Face.com [16] labeling tool to determine the frames
in which the individual was smiling.
As evaluation, we applied the sift for the policy Male (public)
Smiling (private) to concatenated sets of 10 sequential frames (iden-
tiﬁed as smiling) together prior to computing PubLoss and PrivLoss.
As an additional pre-processing step we made sure that the se-
quences of frames we used as our concatenated samples did not oc-
cur at the boundaries of smiling events). We ﬁnd that the PrivLoss
accuracy increases by only 2.3% while PubLoss accuracy decreased
by 4.5% (using 5 dimensional sifts and a λ = 5).
This is an encouraging result and suggests that the SensorSift
technique can be applied to dynamic sensor contexts, however, in
instances where samples are accumulated over longer time sequences
(i.e., days, months) the dynamics of privacy exposure are likely to
change and so will the optimal parameter settings for sift output
dimensionality and privacy emphasis (λ). This is certainly an im-
portant area for further research as dynamic sensing becomes more
ubiquitous (i.e. Microsoft Face Tracking Software Development
Kit in Kinect for Windows [2]).
Comparison to Related Work. The most similar publication to
our present effort is a recent article by Whitehill and Movellan [18]
which uses image ﬁlters (applied to a face dataset) to intentionally
decrease discriminability for one classiﬁcation task while preserv-
ing discriminiability for another (smiling and gender). This work
uses a ratio of discriminability metrics (based on Fisher’s Linear
Discriminant Analysis) to perform a type of linear feature selec-
tion. Perhaps the most signiﬁcant difference between [18] and Sen-
sorSift is that the authors evaluate the quality of their privacy ﬁlters
against human judgments whereas we target automated inferences.
To compare against [18] we used the methods and demo dataset
provided on their website. The dataset consists of 870 grayscale
images (16x16 pixel ‘face patches’).
It also provides labels for
smiling and gender thus enabling analysis of two policies (1) gen-
der (public) : smiling (private), and (2) smiling (private) : gender
(public).
For each policy we evaluated 3 different combinations of training
and testing data splits (using different 80% 20% splits of training
and testing respectively). For each combination we generated 100
discriminability ﬁlters using the provided algorithm (total of 300
ﬁlters for each policy) and subsequently used a linear SVM clas-
siﬁer to evaluate their quality. We found that even though these
ﬁlters were reported to prevent successful human judgement on the
private attribtue, even the best ﬁlter we found was not able to deter
machine inference.
In particular the lowest private attribute accuracy for the gender
( public ) smiling ( private ) policy was 81.21% (average 86.32%).
Conversely the lowest private attribtue accuracy for the smiling
(public) gender (private) policy was 77.65% (average 83.12%). The
public attribute accuracy decreased by 4% on average relative to
classiﬁcation performance on unﬁlted (raw) images.
8. RELATED WORK
Below we touch on the related literature in the broader context
of balancing utility and privacy and subsequently describe efforts
withing the more focused area of face-privacy on which we base
our experimental evaluation.
Utility Privacy Balance. There are several classes of approaches
which have been proposed for ﬁnding a utility and privacy balance
in database and/or information sharing contexts. Among these, the
developments in differential privacy and cryptographic techniques
are only remotely connected to our present discussion as they focus
157
on statistical databases and very limited homomorphic encryption
respectively [7]. More pertinent are the systems based approaches
which typically use proxies/brokers for uploading user generated
content prior to sharing with third-parties. These approaches use
access control lists, privacy rule recommendation, and trace audit
functions; while they help frame key design principles, they do not
provide quantitative obfuscation algorithms beyond degradation of
information resolution (typically for location data) [14].
Lastly, there are several papers which have looked at the question
of privacy and utility balance from a trust modeling and informa-
tion theoretic perspectives [5, 6]. While these are very valuable
problem characterizations which we use to motivate our formal
analysis, we go beyond their framing and develop an algorithmic
defense tool which we apply to a real world problem. Furthermore
we introduce an information processing scheme for embedding our
algorithm into a trusted platform for potential deployment in smart
sensor applications.
Previous Approaches to Face Privacy. Prior work on preserving
the privacy of face images and videos has been almost exclusively
focused on data transformations aimed at identity anonymization.
The methods range from selectively masking or blurring regions of
the face or the whole face [4], perturbing the face ROI (region of in-
terest) with noise through lossy encoding [13], and face averaging
schemes (k-Same, and its variants [8, 15]) aimed at providing k-
anonymity guarantees (each de-identiﬁed face relates ambiguously
to at least k other faces). Whereas these methods emphasize recog-
nition deterrence their methods of limiting information exposure
are unconstrained in what face attribute details they perturb. The
only notable exception is the multi-factor (ε,k)-map algorithm [8]
which demonstrates a selective ability to enhance the representa-
tions of facial expressions in k-anonymity de-identiﬁed faces, how-
ever this approach does not consider privacy granularity below the
level of identity protection.
9. DISCUSSION
Our approach aims to mitigate the emerging privacy threats posed
by automated reasoning applied to harvested digital traces of per-
sonal activity by using algorithmic defenses that enable selective
information exposure – private information should remain private,
while other non-private information can be harvested and used. We
believe that this is a promising approach towards offering quanti-
tative privacy assurances in the rapidly growing market of smart
sensing applications.
A critical strength of the SensorSift design is the built in sup-
port for innovation by future application developers. We provide
an algorithm for generating sifting transformations which can be
used by developers to unlock access to novel data features. As
long as the sifting transformation functions can be veriﬁed to yield
minimal sensitive information exposure our system will allow it to
operate over the sensor data. This ability to dynamically generate
and verify novel privacy respecting data access functions enables
ﬂexibility and provides a way to keep up with the rapidly evolving
needs of software providers.
Limitations. We stress that, as with many systems, privacy is
not binary. Indeed, it may be impossible to achieve absolute pri-
vacy in any useful sensor-based system. Our goal, therefore, is to
explore new directions for increasing privacy for sensor-based sys-
tems while ﬂexibly supporting the desired functionality.
An important point to consider is that multiple applications may
request different privacy views (i.e., sift functions) of the image
data. In the present work, we do not consider collusion between
applications – two applications may be able to combine their func-
tions to reconstruct information greater than that granted to each
application alone. We do note, however, that some simple measures
can be used to protect against collusion (e.g., apply SensorSift to
all the applications running on a system in unity rather than to each
application by itself, or only allow one application access to facial
attributes over some period of time).
A second potential weakness of our approach is that adversaries
may have additional knowledge sources at their disposal which can
reveal private information that SensorSift is unable to counteract.
Our goal is to explore how to protect against unauthorized privacy
disclosures from the sensed data itself, not to defend against auxil-
iary information sources. Indeed, auxiliary information can almost
always break any privacy or anonymity-preserving system. As an
extreme example, suppose the private attribute is race but that the
application asks the user to complete a biographical form – which
includes race – during the application installation process.
Third, our approach leverages classiﬁcation metrics to verify that
the data exposed to applications does not reveal signiﬁcant informa-
tion about private attributes; it is possible that future machine learn-
ing tools can signiﬁcantly outperform our benchmarks. To mitigate
this evolving algorithmic threat, our scheme uses an ensemble of
multiple machine classiﬁcation tools which span the space of state
of the art linear and non-linear methods. Further, the design is
meant to support plug in modules so that new classiﬁers can be
added on demand to enrich the privacy metrics.
10. CONCLUSION
Given the growing demand for interactive systems, the low cost
of computational resources, and the proliferation of sophisticated
sensors (in public/private locations and mobile devices) digital traces
of our identities and activity patterns are becoming increasingly ac-
cessible to third parties with analytics capabilities. Thus, although
sensor systems enhance the quality and availability of digital in-
formation which can aid technical innovation, they also give rise
to security risks and cause privacy tensions. To address these con-
cerns we proposed a theoretical framework for quantitative balance
between utility and privacy though policy based control of sensor
data exposure.
In our analysis we found promising results when we evaluated
the PPLS algorithm within the context of optical sensing and au-
tomated face understanding. However, the algorithm we introduce
is general, as it exploits the statistical properties of the data; and in
the future it would be exciting to evaluate SensorSift in other sensor
contexts.
Acknowledgements
We would like to thank the members of the UW SecurityLab and
Dr. Daniel Halperin for their insightful feedback during the writing
process. This work was supported in part by the Intel Science and
Technology Center for Pervasive Computing and NSF Grant CNS-
0846065.
References
[1] Inside
part
http://www.fastcompany.com/1838801/
exclusive-inside-google-x-project-glass-steve-lee.
project
google
glass
1,
2012.
[2] Kinect for windows sdk, 2012. http://www.microsoft.
com/en-us/kinectforwindows/develop/new.aspx.
[3] Talking face video, 2012. http://www-prima.inrialpes.
fr/FGnet/data/01-TalkingFace/talking_face.html.
158
[4] Mukhtaj S. Barhm, Nidal Qwasmi, Faisal Z. Qureshi, and
Khalil El-Khatib. Negotiating privacy preferences in video
surveillance systems.
In IEA/AIE, volume 6704 of Lecture
Notes in Computer Science, pages 511â ˘A¸S–521, 2011.
[5] Supriyo Chakraborty, Haksoo Choi, and Mani B. Srivastava.
Demystifying privacy in sensory data: A qoi based approach.
In 2011 IEEE International Conference on Pervasive Com-
puting and Communications Workshops (PERCOM Work-
shops), pages 38 –43, 2011.
[6] Supriyo Chakraborty, Haksoo Choi Zainul Charbiwala, Kas-
turi Rangan Raghavan, and Mani B. Srivastava. Balancing be-
havioral privacy and information utility in sensory data ﬂows.
In Preprint, 2012.
[7] Craig Gentry. Fully homomorphic encryption using ideal lat-
tices. In Proceedings of the 41st annual ACM symposium on
Theory of computing, STOC ’09, pages 169–178, 2009.
[8] R. Gross, L. Sweeney, F. de la Torre, and S. Baker. Semi-
supervised learning of multi-factor models for face de-
identiﬁcation. In CVPR, pages 1–8, 2008.
[9] David Kotz, Sasikanth Avancha, and Amit Baxi. A privacy
framework for mobile health and home-care systems.
[10] N. Kumar, P. N. Belhumeur, and S. K. Nayar. Facetracer: A
search engine for large collections of images with faces. In
ECCV, pages 340–353, 2008.
[11] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar.
Attribute and simile classiﬁers for face veriﬁcation. In ICCV,
2009.
[12] Shan Li and David Sarno. Advertisers start using facial
recognition to tailor pitches, 2011. http://www.latimes.
com/business/la-fi-facial-recognition-20110821,
0,7327487.story.
[13] Isabel Martnez-ponte, Xavier Desurmont, Jerome Meessen,
and Jean franois Delaigle. Robust human face hiding ensuring
privacy. In WIAMIS, 2005.
[14] Min Mun, Shuai Hao, Nilesh Mishra, Katie Shilton, Jeff
Burke, Deborah Estrin, Mark Hansen, and Ramesh Govin-
dan. Personal data vaults: a locus of control for personal data
streams. In Proceedings of the 6th International COnference,
Co-NEXT ’10, pages 17:1–17:12, 2010.
[15] Elaine M. Newton, Latanya Sweeney, and Bradley Malin.
IEEE
Preserving privacy by de-identifying face images.
Trans. Knowl. Data Eng, 17(2):232–243, 2005.
[16] Yaniv Taigman and Lior Wolf. Leveraging billions of faces to
overcome performance barriers in unconstrained face recog-
nition, August 2011.
[17] Cajo J. F. ter Braak and Sijmen de Jong. The objective func-
tion of partial least squares regression. Journal of Chemomet-
rics, 12(1):41–54, 1998.
[18] Jacob Whitehill and Javier Movellan. Discriminately decreas-
In CVPR,
ing discriminability with learned image ﬁlters.
2012.