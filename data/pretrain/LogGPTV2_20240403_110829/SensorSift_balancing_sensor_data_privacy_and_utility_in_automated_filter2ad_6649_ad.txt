### Negative Correlations
A regression analysis model attributes 63% of the PubLoss estimate to correlation and 37% to overlap. For PrivLoss, the weights are 67% for correlation and 33% for overlap. Additionally, using regression, we find that considering correlation alone as a predictor results in a Sum of Squared Error (SSE) term that is 315% larger than when both correlation and overlap are used together. These findings suggest that both correlation and overlap should be considered together when analyzing sifting performance.

### Extensions: Complex Policies
Although our primary analysis has focused on policies with one public and one private attribute, our algorithm can be extended to support multiple public and multiple private attributes. To illustrate this, we modified the PPLS objective function to maximize the average gap between the covariances of multiple public and multiple private attribute features relative to data features:

\[
\text{max}_w \left[ \text{avg} \left( \text{cov}(Xw, Y^+), \text{cov}(Xw, Y_1^+), \ldots \right)^2 - \lambda \cdot \text{avg} \left( \text{cov}(Xw, Y^-), \text{cov}(Xw, Y_1^-), \ldots \right)^2 \right]
\]

subject to:
\[
w^T w = 1
\]

Using this averaging method, we were able to find high-performing masks for various policies that include several public and/or private attributes. An example of two such policies is provided in Figure 3.

Complex policies can include arbitrary ratios of public to private attributes, such as 1:2, 1:3, 2:1, 2:2, 3:1 (e.g., public: 'Male' + 'Smiling', private: 'White' + 'Youth'). The number of complex policy combinations is large, but in our tests using 35 complex policies, we found that the same principles from Section 7 apply. As policies grow to include many attributes, the likelihood of significant correlation and overlap increases, thus impacting the balance between utility and privacy. A more detailed analysis of complex policies is a deep topic and an attractive target for future work.

### Extensions: Streaming Content
So far, our analysis has focused on static sensor samples (e.g., still photos). However, dynamic data (e.g., streaming video) is also important. To evaluate the SensorSift scheme in a dynamic context, we used the Talking Face dataset [3]. This dataset consists of 5000 frames taken from a 29fps video of a person engaged in a natural conversation lasting about 200 seconds. We first cropped the face region from each frame and extracted image features as described in Section 5. We then used the Face.com [16] labeling tool to determine the frames in which the individual was smiling.

For evaluation, we applied the sift for the policy 'Male' (public) and 'Smiling' (private) to concatenated sets of 10 sequential frames (identified as smiling) before computing PubLoss and PrivLoss. We ensured that the sequences of frames did not occur at the boundaries of smiling events. We found that PrivLoss accuracy increased by only 2.3%, while PubLoss accuracy decreased by 4.5% (using 5-dimensional sifts and a λ = 5).

This result is encouraging and suggests that the SensorSift technique can be applied to dynamic sensor contexts. However, in instances where samples are accumulated over longer time sequences (e.g., days, months), the dynamics of privacy exposure may change, and so will the optimal parameter settings for sift output dimensionality and privacy emphasis (λ). This is an important area for further research as dynamic sensing becomes more ubiquitous (e.g., Microsoft Face Tracking Software Development Kit in Kinect for Windows [2]).

### Comparison to Related Work
The most similar publication to our work is a recent article by Whitehill and Movellan [18], which uses image filters (applied to a face dataset) to intentionally decrease discriminability for one classification task while preserving it for another (smiling and gender). Their work uses a ratio of discriminability metrics based on Fisher’s Linear Discriminant Analysis for feature selection. A key difference is that [18] evaluates the quality of their privacy filters against human judgments, whereas we target automated inferences.

To compare against [18], we used the methods and demo dataset provided on their website. The dataset consists of 870 grayscale images (16x16 pixel 'face patches') and provides labels for smiling and gender, enabling analysis of two policies: (1) gender (public): smiling (private), and (2) smiling (private): gender (public).

For each policy, we evaluated three different combinations of training and testing data splits (using 80% for training and 20% for testing). For each combination, we generated 100 discriminability filters using the provided algorithm (totaling 300 filters for each policy) and subsequently used a linear SVM classifier to evaluate their quality. We found that even though these filters were reported to prevent successful human judgment on the private attribute, the best filter we found could not deter machine inference.

Specifically, the lowest private attribute accuracy for the gender (public) and smiling (private) policy was 81.21% (average 86.32%). Conversely, the lowest private attribute accuracy for the smiling (public) and gender (private) policy was 77.65% (average 83.12%). The public attribute accuracy decreased by 4% on average relative to classification performance on unfiltered (raw) images.

### Related Work
#### Utility-Privacy Balance
Several approaches have been proposed for balancing utility and privacy in database and information sharing contexts. Developments in differential privacy and cryptographic techniques, while valuable, focus on statistical databases and very limited homomorphic encryption, respectively [7]. More relevant are systems-based approaches that use proxies/brokers for uploading user-generated content before sharing with third parties. These approaches use access control lists, privacy rule recommendations, and trace audit functions, but they do not provide quantitative obfuscation algorithms beyond degrading information resolution, typically for location data [14].

Additionally, there are papers that address the balance from trust modeling and information-theoretic perspectives [5, 6]. While these characterizations are valuable, we go beyond their framing and develop an algorithmic defense tool for real-world applications. We also introduce an information processing scheme for embedding our algorithm into a trusted platform for potential deployment in smart sensor applications.

#### Previous Approaches to Face Privacy
Prior work on preserving the privacy of face images and videos has primarily focused on data transformations aimed at identity anonymization. Methods range from selectively masking or blurring regions of the face, perturbing the face ROI with noise through lossy encoding [13], and face averaging schemes (k-Same and its variants [8, 15]) aimed at providing k-anonymity guarantees. These methods emphasize recognition deterrence but are unconstrained in what face attribute details they perturb. The multi-factor (ε,k)-map algorithm [8] demonstrates selective enhancement of facial expressions in k-anonymity de-identified faces but does not consider privacy granularity below the level of identity protection.

### Discussion
Our approach aims to mitigate emerging privacy threats posed by automated reasoning applied to harvested digital traces of personal activity by using algorithmic defenses that enable selective information exposure. We believe this is a promising approach for offering quantitative privacy assurances in the rapidly growing market of smart sensing applications.

A critical strength of the SensorSift design is its built-in support for innovation by future application developers. We provide an algorithm for generating sifting transformations that can be used by developers to unlock access to novel data features. As long as the sifting transformation functions can be verified to yield minimal sensitive information exposure, our system will allow them to operate over the sensor data. This flexibility enables keeping up with the rapidly evolving needs of software providers.

#### Limitations
We stress that privacy is not binary. It may be impossible to achieve absolute privacy in any useful sensor-based system. Our goal is to explore new directions for increasing privacy while flexibly supporting desired functionality.

One important consideration is that multiple applications may request different privacy views (i.e., sift functions) of the image data. In the present work, we do not consider collusion between applications—two applications might combine their functions to reconstruct information greater than that granted to each application alone. Simple measures can protect against collusion, such as applying SensorSift to all applications running on a system in unity rather than individually, or allowing only one application access to facial attributes over some period.

Another potential weakness is that adversaries may have additional knowledge sources that can reveal private information that SensorSift cannot counteract. Our goal is to protect against unauthorized privacy disclosures from the sensed data itself, not auxiliary information sources. Auxiliary information can almost always break any privacy or anonymity-preserving system. For example, if the private attribute is race, and the application asks the user to complete a biographical form during installation, the form could include race.

Finally, our approach leverages classification metrics to verify that the data exposed to applications does not reveal significant information about private attributes. Future machine learning tools might outperform our benchmarks. To mitigate this, our scheme uses an ensemble of multiple machine classification tools spanning state-of-the-art linear and non-linear methods. The design supports plug-in modules to add new classifiers on demand, enriching the privacy metrics.

### Conclusion
Given the growing demand for interactive systems, the low cost of computational resources, and the proliferation of sophisticated sensors, digital traces of our identities and activity patterns are becoming increasingly accessible to third parties with analytics capabilities. Although sensor systems enhance the quality and availability of digital information, they also give rise to security risks and privacy tensions. To address these concerns, we proposed a theoretical framework for a quantitative balance between utility and privacy through policy-based control of sensor data exposure.

In our analysis, we found promising results when evaluating the PPLS algorithm within the context of optical sensing and automated face understanding. The algorithm we introduce is general, exploiting the statistical properties of the data, and it would be exciting to evaluate SensorSift in other sensor contexts in the future.

### Acknowledgements
We would like to thank the members of the UW SecurityLab and Dr. Daniel Halperin for their insightful feedback during the writing process. This work was supported in part by the Intel Science and Technology Center for Pervasive Computing and NSF Grant CNS-0846065.

### References
[1] Inside project google glass, 2012. http://www.fastcompany.com/1838801/exclusive-inside-google-x-project-glass-steve-lee.
[2] Kinect for windows sdk, 2012. http://www.microsoft.com/en-us/kinectforwindows/develop/new.aspx.
[3] Talking face video, 2012. http://www-prima.inrialpes.fr/FGnet/data/01-TalkingFace/talking_face.html.
[4] Mukhtaj S. Barhm, Nidal Qwasmi, Faisal Z. Qureshi, and Khalil El-Khatib. Negotiating privacy preferences in video surveillance systems. In IEA/AIE, volume 6704 of Lecture Notes in Computer Science, pages 511–521, 2011.
[5] Supriyo Chakraborty, Haksoo Choi, and Mani B. Srivastava. Demystifying privacy in sensory data: A QoI-based approach. In 2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops), pages 38–43, 2011.
[6] Supriyo Chakraborty, Haksoo Choi Zainul Charbiwala, Kasturi Rangan Raghavan, and Mani B. Srivastava. Balancing behavioral privacy and information utility in sensory data flows. In Preprint, 2012.
[7] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the 41st annual ACM symposium on Theory of computing, STOC '09, pages 169–178, 2009.
[8] R. Gross, L. Sweeney, F. de la Torre, and S. Baker. Semi-supervised learning of multi-factor models for face de-identification. In CVPR, pages 1–8, 2008.
[9] David Kotz, Sasikanth Avancha, and Amit Baxi. A privacy framework for mobile health and home-care systems.
[10] N. Kumar, P. N. Belhumeur, and S. K. Nayar. Facetracer: A search engine for large collections of images with faces. In ECCV, pages 340–353, 2008.
[11] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and simile classifiers for face verification. In ICCV, 2009.
[12] Shan Li and David Sarno. Advertisers start using facial recognition to tailor pitches, 2011. http://www.latimes.com/business/la-fi-facial-recognition-20110821, 0,7327487.story.
[13] Isabel Martínez-Ponte, Xavier Desurmont, Jérôme Meessen, and Jean-François Delaigle. Robust human face hiding ensuring privacy. In WIAMIS, 2005.
[14] Min Mun, Shuai Hao, Nilesh Mishra, Katie Shilton, Jeff Burke, Deborah Estrin, Mark Hansen, and Ramesh Govindan. Personal data vaults: a locus of control for personal data streams. In Proceedings of the 6th International COnference, Co-NEXT '10, pages 17:1–17:12, 2010.
[15] Elaine M. Newton, Latanya Sweeney, and Bradley Malin. Preserving privacy by de-identifying face images. IEEE Trans. Knowl. Data Eng, 17(2):232–243, 2005.
[16] Yaniv Taigman and Lior Wolf. Leveraging billions of faces to overcome performance barriers in unconstrained face recognition, August 2011.
[17] Cajo J. F. ter Braak and Sijmen de Jong. The objective function of partial least squares regression. Journal of Chemometrics, 12(1):41–54, 1998.
[18] Jacob Whitehill and Javier Movellan. Discriminately decreasing discriminability with learned image filters. In CVPR, 2012.