title:Hardware Performance Counters Can Detect Malware: Myth or Fact?
author:Boyou Zhou and
Anmol Gupta and
Rasoul Jahanshahi and
Manuel Egele and
Ajay Joshi
Hardware Performance Counters Can Detect Malware:
Myth or Fact?
Boyou Zhou, Anmol Gupta, Rasoul Jahanshahi, Manuel Egele, Ajay Joshi
{bobzhou,anmol.gupta1005,rasoulj,megele,joshi}@bu.edu
Eletrical and Computer Engineering Department, Boston University
ABSTRACT
The ever-increasing prevalence of malware has led to the explo-
rations of various detection mechanisms. Several recent works
propose to use Hardware Performance Counters (HPCs) values
with machine learning classiication models for malware detection.
HPCs are hardware units that record low-level micro-architectural
behavior, such as cache hits/misses, branch (mis)prediction, and
load/store operations. However, this information does not reliably
capture the nature of the application, i.e. whether it is benign or
malicious. In this paper, we claim and experimentally support that
using the micro-architectural level information obtained from HPCs
cannot distinguish between benignware and malware. We eval-
uate the idelity of malware detection using HPCs. We perform
quantitative analysis using Principal Component Analysis (PCA) to
systematically select micro-architectural events that have the most
predictive powers. We then run 1,924 programs, 962 benignware
and 962 malware, on our experimental setups. We achieve 83.39%,
84.84%, 83.59%, 75.01%, 78.75%, and 14.32% F1-score (a metric of
detection rates) of Decision Tree (DT), Random Forest (RF), K Near-
est Neighbors (KNN), Adaboost, Neural Net (NN), and Naive Bayes,
respectively. We cross-validate our models 1,000 times to show
the distributions of detection rates in various models. Our cross-
validation analysis shows that many of the experiments produce
low F1-scores. The F1-score of models in DT, RF, KNN, Adaboost,
NN, and Naive Bayes is 80.22%, 81.29%, 80.22%, 70.32%, 35.66%,
and 9.903%, respectively. To further highlight the incapability of
malware detection using HPCs, we show that one benignware
(Notepad++) infused with malware (ransomware) cannot be de-
tected by HPC-based malware detection.
KEYWORDS
Malware Detection, Hardware Performance Counters, Machine
Learning
ACM Reference Format:
Boyou Zhou, Anmol Gupta, Rasoul Jahanshahi, Manuel Egele, Ajay Joshi.
2018. Hardware Performance Counters Can Detect Malware: Myth or Fact?.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proit or commercial advantage and that copies bear this notice and the full citation
on the irst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciic permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’18, June 4ś8, 2018, Incheon, Republic of Korea
© 2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5576-6/18/06. . . $15.00
https://doi.org/10.1145/3196494.3196515
In ASIA CCS ’18: 2018 ACM Asia Conference on Computer and Communica-
tions Security, June 4ś8, 2018, Incheon, Republic of Korea. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3196494.3196515
1 INTRODUCTION
Distinguishing between malicious and benign software has re-
mained one of the biggest challenges facing computer security
over recent decades. As signature-based anti-virus scanners are
easily thwarted by polymorphic malware, most commercial and
academic anti-malware solutions rely on behavioral analysis. Be-
havioral analysis monitors programs as they execute, collects infor-
mation on the process, and, upon a violation of a behavioral proile,
classiies the program as malware. To this end, software-based
behavioral analysis can draw from a wealth of semantically rich
information sources, such as ile names, registry keys, or network
endpoints, which characterize the program’s behavior. As software-
level behavioral analysis performs malware detection at the cost of
performance overhead, recent research proposes to reduce this per-
formance overhead by leveraging Hardware Performance Counters
(HPCs) to classify programs as benignware or malware.
HPCs are hardware units that count the occurrences of micro-
architectural events such as instruction counts, hits/misses in vari-
ous cache levels and branch (mis)predictions during runtime. Mod-
ern processors can capture more than 100 micro-architectural events,
but a design-imposed strict limit of 4 (on Intel [1]) and 6 (on
AMD [2]) counter registers dictates that HPCs can only monitor a
small subset of these events at one time.
Under these constraints, previous works [3ś6] leverage the mea-
sured HPC values to classify an unknown program as either be-
nign or malicious. To this end, measured HPC values are sampled
at a ixed frequency and the resulting data is aggregated into a
time-series. Previous works record data of labeled programs in
time-series, and use the HPC values in time-series to train various
supervised machine learning models. The measured HPC values
yield classiiers that can subsequently distinguish unknown pro-
grams as either benign or malicious.
The underlying assumption for previous HPC-based malware
detectors is that malicious behavior afects measured HPC values
diferently than benign behavior. However, it is questionable, and in
fact counter-intuitive, why the semantically high-level distinction
between benign and malicious behavior would manifest itself in
the micro-architectural events that are measured by HPCs. As a
concrete example, consider that malware as well as benignware
make use of the cryptographic APIs. While ransomware might ma-
liciously encrypt the user data, the user might rely on encryption
to safeguard privacy and data conidentiality. In both cases, ran-
somware and benignware, the program performs cryptographic
operations. One cannot discriminate between malicious and benign
usage based on the measured HPC values. The semantic difer-
ence of whether the encryption was performed maliciously or not,
exclusively depends on who holds the decryption keys, i.e., the
attacker or the user. There is no indication that any HPC event
would correlate with the ownership of the keys.
Given the substantial semantic diference between the high-level
malicious behavior and the low-level micro-architectural events, it
is expected from previous works that assert the utility of HPCs for
malware detection to provide a rigorous analysis, interpretation,
and justiication of why the extracted features from measured HPC
values identify the maliciousness of programs. This includes, for
example, an analysis of the events found to be the most predictive of
malicious behavior and a discussion of why these features capture
behavioral information at all. Unfortunately, existing works elide
any such discussions, and instead commit the logical fallacy of łcum
hoc ergo propter hocž 1 Ð or concluding causation from correlation.
Moreover, the correlations and resulting detection capabilities re-
ported by previous works frequently result from small sample sets
and experimental setups that put the detection mechanism at an
unrealistic advantage.
To shine a light on the feasibility of using HPCs for detecting
malicious behavior, we survey the existing literature in this ield,
and identify common traits that exhibit impractical setups and mis-
interpretation of data analysis. Subsequently, we design, implement,
and evaluate an experimental setup that allows us to reproduce
previous works in this area, and compare these previous results
with results obtained under more realistic scenarios.
In this work, we build an experimental setup close to the user
environment, and evaluate idelity of machine learning models.
We run all experiments in a bare-metal environment instead of
relying on virtualization techniques. This choice is motivated by
two observations. First, our experiments indicate that measured
HPC values collected for the same program running inside a virtual
machine substantially difer from those collected on a bare metal
system (comparisons in ğ2). Second, regular users likely execute
programs directly on their systems outside of virtual machines.
Further contributing to the realism of our experiments is the se-
lection of training data for the machine learning models. Previous
works [3, 5, 6] test their machine learning models using measured
HPC values from the same programs used during training (In ğ4.3,
we refer to this approach as TTA1). In a real-world deployment,
this scenario would relect a situation where all programs (benign
and malicious) are known and labeled for training. In such situa-
tions, machine learning is unnecessary, as each program could be
perfectly identiied based on its hash. As Anti-Virus (AV) vendors
report thousands of new malware samples every day, this scenario
is highly unlikely to ever occur in reality. Thus, we test our mod-
els with measured HPC values from programs that have not been
observed during training. This relects a realistic scenario where,
during training machine learning models, malware samples from
the same category or family are available, but not the exact same
malware that a user may encounter.
We train 6 diferent machine learning classiiers and compare
the results obtained with both realistic and unrealistic approaches.
Unsurprisingly, we observe that classiiers trained in the realistic
1łwith this, therefore because of thisž
scenario perform worse than those trained in an unrealistic sce-
nario. To rigorously evaluate the performance of our classiiers,
we perform 1,000 iterations of 10-fold cross-validations and con-
sistently observe False Discovery Rate2 of larger than 20%. Such
high False Discovery Rates would disqualify HPC-based malware
detectors from real-world deployments, as it would lag 264 pro-
grams in a default Windows 7 installation as malicious. Finally, we
illustrate how fragile the resulting classiiers are by simply com-
posing a benign program (Notepad++) with malicious functionality
(ransomware). This straight-forward composition evades all our
classiiers, even when they are trained with the benign and mali-
cious components individually. In summary, this work makes the
following contributions:
• We identify the prevalent unrealistic assumptions and the
insuicient analysis used in prior works that leverage HPCs
for malware detection (ğ2).
• We perform thorough experiments with a program count
that exceeds prior works [3, 5ś8] by a factor of 2× ∼ 3×,
and the number of experiments in cross-validations that is 3
orders of magnitude more than previous works.
• We train and test dataset similar to what prior works have
done, as well as, in a realistic setting where testing programs
are not in the training programs. We compare the efects of
this choice on the quality of the machine learning models
(ğ 5).
• Finally, to facilitate reproducibility, and enable future re-
searchers to easily compare their experiments with ours,
we make all code, data, and results of our project publicly
available under an open-source license: https://github.com/
bu-icsg/Hardware_Performance_Counters_Can_Detect_Malware_
Myth_or_Fact
2 RELATED WORK AND MOTIVATION
Malware detection is the process of detecting malicious programs,
for example, viruses. Many previous works commonly utilize sub-
semantic features in malware detection [3, 5ś11]. Ozsoy et al. deined
the term sub-semantic features as łmicro-architectural information
about an executing program that does not require modeling or
detecting program semanticsž [9]. All these previous works have
several drawbacks to various extent. We categorize the drawbacks
that we observed into the following classes.
Dynamic Binary Instrumentation (DBI)
Virtual Machines (VMs)
I
II
III Division of Data By Traces (TTA1 in ğ 4.3)
IV No Cross-Validations or Insuicient Validations
V
Few Data Samples
Besides HPCs, sub-semantic features can be extracted with dy-
namic binary instrumentation (DBI) tools such as Intel’s Pin [4, 12],
QEMU [13], Valgrind [14], or DynamoRIO [15]. Khasawneh et al.
use Pin to monitor the instructions executed on virtual machines
in their experimental setup [9ś11]. Though DBI can extract sub-
semantic features that are not available from HPCs, DBI introduces
a substantial amount of performance overhead and is thus not
suited to run in an always-on, online protection setting, which is
2F+ /(F+ + T+ ), where F+ is number of benignware classiied as malware and T+ is
number of malware classiied as malware
e
c
i
o
h
C
l
o
o
T
l
a
t
n
e
m
i
r
e
p
x
E
s
p
u
t
e
S
e
c
i
o
h
C
t
n
e
v
E
n
o
i
s
i
v
i
D
a
t
a
D
n
o
i
t
a
d
i
l
a
V
s
s
o
r
C
e
n
i
h
c
a
M
g
n
i
n
r
a
e
L
s
l
e
d
o
M
)
3
4
.
ğ
n
i
1
A
T
T
(
s
e
c
a
r
T
y
B
d
e
d
i
v
i
D
a
t
a
D
:
I
I
I
k
c
a
b
w
a
r
D
•
•
•
⋄
•
⋄
⋄
⋄
•
n
o
i
s
i
v
i
D
a
t
a
D
%
0
2
−
0
2
−
0
6
:
V
I
k
c
a
b
w