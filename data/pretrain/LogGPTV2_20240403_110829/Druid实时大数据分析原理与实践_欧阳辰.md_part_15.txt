106
Druid数据复制可以分为两个方面。
从图5-7可以看到，这是两个分区中的第一个分区，存储在Test-Druid-Histrical-1这台机
·Druid系统内部数据复制：Druid存储的基本单位是Segment，我们可以通过Coordi-
.DeepStorage:
我们可以从CoordinatorConsole看到分区信息，如图5-7所示。
"type":"index_hadoop"
nator 设置Segment在Druid系统内部的副本因子，如图5-8所示。
会丢失。
201
201
dailyemonthlya
数据复制
16-08-19
6-07-
6-08-19
19
'type":"hadoop"
"partitionsSpec":{
80
15
00
0
15
"type":"hashed"
"numShards":2,
一般使用HDFS、S3等，这些系统本身就有副本复制能力，保证数据不
6dimensions
4.38kB
图5-7分区信息
72016081972390000
1metrics
Druid实时大数据分析原理与实践
---
## Page 131
分片、复制、服务发现以及无缝的数据结构调整。
运用起来比较麻烦。Tranquility对索引服务的API进行了封装，可以方便地创建任务、处理
5.5.3
一台10.24.199.19机器存储这个Segment。
第5章数据摄入
在上面内容中，我们讲述了如何通过索引服务摄取数据，然而索引服务的API太过底层，
从图 5-9可以看到，之前只有 Test-Druid-Histrical-1这台机器存储的 Segment，现在多了
设置后，
索引服务之Tranquility
2016-07-1904
2016-07-1908
2016-08-19 00
2016-08-19
2016-08-1915
earchlne
dalymonthly
659KB
，Coordinator会调度历史节点将副本加载到系统中，如图5-9所示。
15
08
Editrutesfor dianshang_orderr
4.38kB
6dimensions
图5-8
Foreve
图5-9
通过规则更改副本因子
+addarule
：20150419073u000900
加载副本
ResetCancelSave alnles
oefault,iel
1metrics
107
---
## Page 132
import java.util.Map;
import java.io.InputStream;
import org.joda.time,DateTime;
import com.metamx.tranquility.tranquilizer.MessageDroppedException;
import com.metamx.tranquility.config.PropertiesBasedConfig;
import com.metamx.tranquility.config.DataSourceConfig:
import com.metamx.common.logger.Logger;
import com.google.common.collect.ImmutableMap;
package com.metamx.tranquility.example;
然后根据example.json的配置启动任务，最后通过sender异步发送数据。
3.API
够的资源运行这么多任务。中间管理者至少能支持的任务数量是：2×分片数×复制副本数。
import scala.runtime.BoxedUnit;
import com.twitter.util.FutureEventlistener;
import com.metamx.tranquility.tranquilizer.Tranquilizer;
import com.metamx.tranquility.druid.DruidBeams;
import com.metamx.tranquility.config.TranquilityConfig;
新任务在旧任务结束之前就已经提交，所以要考虑中间管理者（MidleManager）是否有足
些任务有相同的分片号。
同的分片号（partitionNum）。同样，Tranquility也是通过多个任务实现复制的，不同的是，这
2.
的时间+数据加载到历史节点上的时间。
间=任务时间窗口+可容忍数据延迟时间+生成 Segment 的时间+数据存储到DeepStorage
把Segment存储到DeepStorage，然后等待Segment被加载到历史节点上。所以，任务生存时
去时，Druid就开始关闭这个任务。在关闭之前，需要合并生成 Segment并移交，移交是指
某一时间段的第一条数据时创建。但是当任务的时间段过去，并且可容忍的数据延迟窗口过
1.
一个实时摄取的任务。每一个任务都与特定的时间段和分区相关。任务会在Tranquility收到
108
分片与复制
Tranquility会周期性地将任务提交到索引服务。在通常情况下，由于时间窗口的存在，
任务创建
以下是Tranquility官方提供的样例代码，可以看到，其实读取了example.json配置文件，
Tranquility通过对一个特定的时间段创建多个任务来实现分片，这些任务都会有一个不
在通常情况下，
，Tranqulity会POST一个请求到索引服务，以便为每一个Segment 创建
Druid实时大数据分析原理与实践
---
## Page 133
public class JavaExample {
第5章
public static void main(String[] args)
private static final Logger log = new Logger(JavaExample.class);
try
sender.start();
final Tranquilizer wikipediaConfig=config.getDataSource
final InputStream configStream= JavaExample.class.getClassLoader().
// Read config from"example.json" on the classpath.
for（inti=0;i obj = ImmutableMap.of(
//Build asample event tosend;make sureweuseacurrentdate
("wikipedia");
buildTranquilizer（wikipediaConfig.tranquilizerBuilder());
wikipediaConfig)
new FutureEventListener()
"page"，
"timestamp",new DateTime().toString(),
"added"，i
public void onFailure(Throwable e)
@Override
public void onSuccess(Boxedunit value){
@Override
log.info("Sent message: %s", obj);
"foo"，
Object>> sender = DruidBeams.fromConfig(
109
---
## Page 134
druidService=DruidBeams
DruidConfig config = new DruidConfig();
List aggregators = Arrays.asList(aggregatorFactories);
AggregatorFactory[] aggregatorFactories = DataSourceConfiguration.getAggregators(command
List dimensions = DataSourceConfiguration.getDimensions(command);
下所示：
110
,.timestampSpec(timestampSpec)
.builder(timestamper)
当然，如果需要更灵活地控制启动任务的配置，可以通过更底层的API来实现，
location(
discoveryPath(discoveryPath)
curator（curator)
finally{
sender.stop(）;
sender.flush(）;
DruidLocation.create(
dataSource
firehosePattern,
indexService,
}else{
if（e instanceof MessageDroppedException）{
log.warn(e, "Dropped message: %s", obj);
log.error(e,
"Failed to send message: %s", obj);
Druid实时大数据分析原理与实践
如
---
## Page 135
aggregator低。Cardinalityaggregator的用法如下：
优化，所以并不能减少存储的容量，查询效率也会比在摄取阶段就进行优化的HyperUnique
nality aggregator基于HyperLogLog算法，但是Cardinality aggregator只是在查询阶段进行了
1.Cardinality aggregator
减少基数，而在某些业务场景下，我们可以使用一些优化算法。
所以需要对基数大的维度进行调优。尽量不用、少用基数大的维度，或者通过离散化的方式
5.5.4
调整配置变化等高阶需求。
druidService.apply(events);
List> events;
第5章数据摄入
"byRow": #(optional,defaults to false)
"name":""
"type":"cardinality",
对于计算一个维度基数这样的需求，
"fieldNames":[,，
Druid高效的一个重要原因是聚合，而对于基数十分大的维度，会严重降低聚合的效率，
可以看到，通过API可以指定维度、指标以及分片与复制的配置等，这样可以支持动态
).buildJavaService(）;
.tuning(ClusteredBeamTuning
.rollup(DruidRollup.create(DruidDimensions.specific(dimensions),aggregators,
高基数维度优化
QueryGranularity.MINUTE))
.build()
.replicants(Integer.parseInt(config.getProperty(dataSource +".replicants"))
.windowPeriod(new Period("PT10M"))
.segmentGranularity(Granularity.HOUR)
.builder()
，我们可以使用Cardinalityaggregator来满足。Cardi-
111
---
## Page 136
F
基数维度进行过滤、分组的操作，则完全可以在摄取阶段就进行优化，以获得更少的存储容
SELECT COUNT(*） FROM ( SELECT DIM1, DIM2, DIM3 FROM  GROUP BY DIM1, DIM2,
SELECT COUNT(DISTINCT(value)） FROM（
SELECT COUNT(DISTINCT(dimension))FROM
度，
量和更快的查询速度。仍使用之前的用户行为数据摄取案例，userid就是一个基数很大的维
112
，而通常我们需要统计的是UV这样的指标，那么完全可以通过HyperUniqueaggregator满
正如前文所述，Cardinalityaggregator并没有在摄取阶段进行优化。如果业务不需要对高
在摄取阶段，可以进行如下配置：
DIM3）
按行求基，相当于如下SQL:
SELECT dim_3 asvalueFROM
UNION
SELECT dim_2 as value FROM
UNION
SELECT dim_1 as value FROM 
"spec":
·多维度一
·单维度一
（2）Cardinality by row
在默认情况下，基数计算基于维度的值，
(1 ) Cardinality by value
"dataSchema":[
"granularitySpec":{
"intervals":[
一按值求基，相当于如下SQL：
一按值求基，相当于如下SQL：
"2016-08-28/2016-08-29"
，下面举例说明。
Druid实时大数据分析原理与实践
---
## Page 137
第
5章
数据摄入
"parser":{
"metricsSpec":
"type":"string"
"parseSpec":{
"type":
"segmentGranularity”:"HOUR"
"queryGranularity":“MINUTE""
"timestampSpec":
"dimensionsSpec":
"type":"hyperUnique”
"name":"unique_user_id"
"fieldName":"user_id"
"fieldName":"count",，
"format":"json"
"uniform"
"column":"timestamp"，
"dimensionExclusions":[],
"format":"auto"
"spatialDimensions":[]
"dimensions":[
"category"
"commodity"
"city",
"age",
"event_name"
人
113
---
## Page 138
{"timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":5,"age
{"timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":4,"age
{"timestamp":"2016-08-28T08:50:0.563Z","event_name":"browse_commodity","user_id":2,"age
{"timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":3,"age
{"timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":2,"age
{"timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":1,"age
114
":"90+","city":"Beijing","commodity”:"xxxxx","category”:"3c","count"”:1}
":"g0+","city”:"Beijing","commodity”:"xxxxx",”category”:"3c","count":1}
":"90+","city”:"Beijing","commodity”:"xxxxx","category":"3c","count":1}
":"90+",”city”:"Beijing","commodity”:”xxxx","”category”:"3c","count":1}
":"90+","city”:"Beijing","commodity”:"xxxx","category":"3c","count":2}
样例数据如下：
"type";"index"
"tuningConfig":{
"windowPeriod":"PT10m"
"type":"index",
"maxRowsInMemory":500000,
"intermediatePersistPeriod":"PT10m",
'rejectionPolicy":
"basePersistDirectory":"/rc/data/druid/realtime/basePersist",
"ioConfig":{
"type":"none"
"type": "index"
"firehose":{
"type":"local"
"filter":"*,json",
"baseDir":
"/rc/data/druid/tmp/test-data",
Druid实时大数据分析原理与实践
---
## Page 139
["timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":7,"age
{"timestamp":"2016-08-28T08:50:00.5637","event_name":"browse_commodity","user_id":8,"age
["timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":9,"age
["timestamp":"2016-08-28T08:50:00.563Z","event_name":"browse_commodity","user_id":6,"age
第5章数据摄入
4oxxxxg+06
unoobae.xxxxpoobug+06
":"90+","city”:"Beijing","commodity":"xxxxx","category":"3c","count":1}
得到结果如下：
"queryType":"timeseries"
"granularity":
"dataSource":"dianshang_order",
"aggregations":[
我们可以通入如下查询，查询UV：
intervals":[
"2016-08-27/2016-08-29"
"timestamp":"2016-08-28T00:00:00.000Z"
"result":{"unique_user_id":9.019833517963864},
"type":
"name":“unique_user_id",
"fieldName":"unique-user_id",