(§5.1.2) that does not rely on this assumption.
For a move without guarantees, the controller (1) calls getPer-
flow and delPerflow on srcInst, (2) calls putPerflow on
dstInst, and (3) updates the ﬂow table on sw to forward the affected
ﬂows to dstInst. To move multi-ﬂow state as well (or instead), the
analogous multi-ﬂow functions are also (instead) called. For the
rest of this section, we assume the scope is per-ﬂow, but our ideas
can easily be extended to multi-ﬂow state.
With the above sequence of steps, packets corresponding to the
state being moved may continue to arrive at srcInst from the start of
getPerflow until after the forwarding change at sw takes effect
and all packets in transit to srcInst have arrived and been read from
the NIC and operating system buffers. A simple approach of drop-
ping these packets when srcInst receives them [34] prevents srcInst
from establishing new state for the ﬂows or failing due to missing
state. But this is only acceptable in scenarios where an application
is willing to tolerate the effects of skipped processing: e.g., scan
167detection in the Bro IDS will still function if some TCP packets are
not processed, but it may take longer to detect scans. Alternatively,
an NF may be on the forwarding path between ﬂow endpoints (Fig-
ure 4(b)), e.g., a Squid caching proxy, in which case dropped TCP
packets will be retransmitted, although throughput will be reduced.
5.1.1 Loss-free Move
In some situations loss is problematic: e.g., the Bro IDS’s mal-
ware detection script will compute incorrect md5sums and fail to
detect malicious content if part of an HTTP reply is missing; we
quantify this in our technical report [23]. Thus, we need a move
operation that satisﬁes the following property:
Loss-free: All state updates resulting from packet process-
ing should be reﬂected at the destination instance, and all
packets the switch receives should be processed.
The ﬁrst half of this property is important for ensuring all infor-
mation pertaining to a ﬂow (or group of ﬂows) is available at the
instance where subsequent packet processing for the ﬂow(s) will
occur, and that information is not left, or discarded, at the original
instance. The latter half ensures an NF does not miss gathering
important information about a ﬂow.
In an attempt to be loss-free, Split/Merge halts, and buffers at
the controller, all trafﬁc arriving at sw while migrating per-ﬂow
state [34]. However, when trafﬁc is halted, packets may already be
in-transit to srcInst, or sitting in NIC or operating system queues at
srcInst. Split/Merge drops these packets when they (arrive and) are
dequeued at srcInst. This ensures that srcInst does not attempt to
update (or create new) per-ﬂow state after the transfer of state has
started, guaranteeing the ﬁrst half of our loss-free property. How-
ever, dropping packets at srcInst violates the latter half. While we
could modify Split/Merge to delay state transfer until packets have
drained from the network and local queues, it is impossible to know
how long to wait, and extra waiting increases the delay imposed on
packets buffered at the controller.
SDN consistency abstractions [27, 35] are also insufﬁcient for
guaranteeing loss-freedom. They can guarantee packets will be for-
warded to srcInst or dstInst, but they do not provide any guarantees
on what happens to the packets once they arrive at the NF instances.
If srcInst processes the packets after state transfer has started, then
the state installed at dstInst will not include some updates; if srcInst
drops the packets instead, then some state updates will never occur.
What then should we do to ensure loss-freedom in the face of
packets that are in-transit (or buffered) when the move operation
starts? In OpenNF, we leverage events raised by NFs. Speciﬁcally,
the controller calls enableEvents(ﬁlter,drop) on srcInst be-
fore calling getPerflow. This causes srcInst to raise an event
for each received packet matching ﬁlter. The events are buffered
at the controller until the putPerflow call on dstInst completes.
Then, the packet in each buffered event is sent to sw to be forwarded
to dstInst; any events arriving at the controller after the buffer has
been emptied are handled immediately in the same way. Lastly, the
ﬂow table on sw is updated to forward the affected ﬂows to dstInst.
Calling disableEvents(ﬁlter) on srcInst is unnecessary, be-
cause packets matching ﬁlter will eventually stop arriving at srcInst
and no more events will be generated. Nonetheless, to eliminate
the need for srcInst to check if it should raised events for incoming
packets, the controller can issue this call after several minutes—i.e.,
after all packets matching ﬁlter have likely arrived or timed out.
5.1.2 Order-preserving Move
In addition to loss, NFs can be negatively affected by re-ordering.
For example, the “weird activity” policy script included with the
Figure 5: Order-preserving problem in Split/Merge
Bro IDS will raise a false “SYN_inside_connection” alert if the IDS
receives and processes SYN and data packets in a different order
than they were actually exchanged by the connection endpoints.
Another example is a redundancy elimination decoder [16] where
an encoded packet arriving before the data packet w.r.t. which it
was encoded will be silently dropped; this can cause the decoder’s
data store to rapidly become out of synch with the encoders.
Thus, we need a move operation that satisﬁes the following:
Order-preserving: All packets should be processed in the
order they were forwarded to the NF instances by the switch.
This property applies within one direction of a ﬂow (e.g., process
SYN before ACK), across both directions of a ﬂow7 (e.g., pro-
cess SYN before SYN+ACK), and, for moves including multi-ﬂow
state, across ﬂows (e.g., process an FTP get command before the
SYN for the new transfer connection).
Unfortunately, neither Split/Merge nor the loss-free move de-
scribed above are order-preserving. The basic problem in both sys-
tems is a race between ﬂushing packets buffered at the controller
and changing the ﬂow table at sw to forward all packets to dstInst.
Figure 5 illustrates the problem in the context of Split/Merge. Even
if all buffered packets (pi and pi+1 ) are ﬂushed before the con-
troller requests a forwarding table update at sw, another packet
(pi+2 ) may arrive at sw and be forwarded to the controller before
sw applies the forwarding table update. Once the update is applied,
sw may start forwarding packets (pi+3 ) to dstInst, but the controller
may not have received the packet pi+2 from sw. Thus, the packet
pi+2 will be forwarded to dstInst after a later packet of the ﬂow
(pi+3 ) has already been forwarded to dstInst.
We use a clever combination of events and a two-phase forward-
ing state update to guarantee a loss-free and order-preserving move.
Figure 6 has psuedo-code for the steps.
We start with the steps used for a loss-free move, through call-
ing putPerflow on dstInst. After putPerflow completes we
extract the packet from each buffered event, mark it with a special
“do-not-buffer” ﬂag, and send it to sw to be forwarded to dstInst;
any events arriving at the controller after the buffer has been emp-
tied are handled immediately in the same way. Then, we call ena-
bleEvents(ﬁlter,buffer) on dstInst, so that any packets for-
warded directly to dstInst by sw will be buffered; note that the pack-
ets marked with “do-not-buffer” (discussed above) are not buffered.
Next, we perform the two phase forwarding state update. First,
we update the forwarding entry for ﬁlter on sw to forward match-
ing packets to both srcInst and the controller.8 The controller waits
7If packets in opposite directions do not traverse a common switch
before reaching the NF—e.g., a NAT is placed between two
switches—then we lack a vantage point to know the total order of
packets across directions, and we cannot guarantee such an order
unless it is enforced by a ﬂow’s end-points—e.g., a server will not
send SYN+ACK until the NAT forwards the SYN from a client.
8We use existing SDN consistency mechanisms [27, 35] to ensure
the update is atomic and no packets are lost.
168if shouldBufferEvents then
1 eventReceivedFromSrcInst (event)
2
3
4
5
else
eventQueue.enqueue (event.packet)
sw.forward (event.packet, dstInst)
if lastPacketFromSw== null then
6 packetReceivedFromSw (packet)
7
8
9
lastPacketFromSw ← packet
signal (GOT_FIRST_PKT_FROM_SW)
// wait @ 24
10 eventReceivedFromDstInst (event)
11
12
if event.packet == lastPacketFromSw then
signal (DST_PROCESSED_LAST_PKT) // wait @ 26
shouldBufferEvents ← true
srcInst.enableEvents (ﬁlter, DROP)
chunks ← srcInst.getPerﬂow (ﬁlter)
srcInst.delPerﬂow (chunks.keys)
dstInst.putPerﬂow (chunks)
foreach event in eventQueue do
13 moveLossfreeOrderpreserve (srcInst, dstInst, ﬁlter)
14
15
16
17
18
19
20
21
22
23
24
25
26
27
shouldBufferEvents ← false
dstInst.enableEvents (ﬁlter, BUFFER)
sw.install (ﬁlter, {srcInst, ctrl}, LOW_PRIORITY)
wait (GOT_FIRST_PKT_FROM_SW)
sw.install (ﬁlter, dstInst, HIGH_PRIORITY)
wait (DST_PROCESSED_LAST_PKT)
dstInst.disableEvents (ﬁlter)
sw.forward (event.packet, dstInst)
Figure 6: Pseudo-code for loss-free and order-preserving move
for at least one packet from sw, and always stores the most recent
packet it receives. Second, we install a higher priority forward-
ing entry for ﬁlter on sw to forward matching packets to dstInst.
Through this two phase update, the controller can become aware of
the last packet sent to srcInst.9
Finally, we need to ensure that dstInst processes all packets for-
warded to srcInst before processing any packets that sw directly
forwards to dstInst. We achieve this with the following sequence
of steps: (1) wait for an event from srcInst for the last packet sent
to srcInst—this is the packet we stored during the two phase for-
warding state update; (2) send the packet contained in the event
to sw to forward to dstInst; (3) wait for an event from dstInst for
the packet; and (4) call disableEvents(ﬁlter) on dstInst to re-
lease any packets that had already been sent to dstInst by sw and
were buffered at dstInst.
In our technical report [23], we formally prove that this sequence
of steps is loss-free and order-preserving.
The additional waiting required for order-preserving does come
at a performance cost (we quantify this in §8.1.1). Thus, we offer
applications three versions of move (loss-free and order-preserving,
loss-free only, and no guarantees) so they can select the most efﬁ-
cient version that satisﬁes their requirements.
5.1.3 Optimizations
Supporting the above guarantees may impose additional laten-
cies on packets arriving during the move operation. In particular,
when a move involves multiple ﬂows, we halt the processing of
those ﬂows’ packets from the time enableEvents is called until
after putPerflow completes.
One way to reduce these latencies (and reduce drops in the case
of a move without guarantees) is to reduce the total time taken to
complete the move operation. To achieve this, an application could
issue multiple pipelined moves that each cover a smaller portion of
the ﬂow space. However, this requires more forwarding rules in sw
and requires the application to know how ﬂows are divided among
the ﬂow space. Instead, we can leverage the fact that getPer-
flow and putPerflow operations can be, at least partially, ex-
ecuted in parallel. Rather than returning all requested states as a
single result, the srcInst can return each chunk of per-ﬂow state im-
mediately, and the controller can immediately call putPerflow
with just that chunk. The forwarding table update(s) at sw occurs
after the getPerflow and all putPerflow calls have returned.
The additional latency imposed on redirected packets can be fur-
ther reduced by following an early release and late locking strat-
egy. For late-locking, the controller calls getPerflow on srcInst
with a special ﬂag instructing srcInst to enable events for each ﬂow
just before the corresponding per-ﬂow state is prepared for export
(avoiding the need to call enableEvents for all ﬂows before-
hand). Also, once putPerflow for a speciﬁc chunk returns, the
controller can release any events pertaining to that chunk.10
The parallelizing optimization can be applied to any version of
move, and the early-release optimization can be applied to a move
of either per-ﬂow or multi-ﬂow state, but not a move involving both.
5.2 Copy and Share Operations
OpenNF’s copy and share operations address applications’
need for the same state to be readable and/or updateable at multiple
NF instances and, potentially, for updates made at one instance to
be reﬂected elsewhere. For example, in a failure recovery applica-
tion (§2) a backup NF instance needs to keep an updated copy of all
per-/multi-/all-ﬂows state. Similarly, a load balancing application
that distributes an end-host’s ﬂows among multiple IDS instances
needs updates to the host connection counter at one instance to be
reﬂected at the other instances to effectively detect port scans.
In particular, copy can be used when state consistency is not
required or eventual consistency is desired, while share can be
used when strong or strict consistency is desired. Note that eventual
consistency is akin to extending our loss-free property to multiple
copies of state, while strict consistency is akin to extending both our
loss-free and order-preserving properties to multiple NF instances.
5.2.1 Copy Operation
OpenNF’s copy operation clones state from one NF instance
(srcInst) to another (dstInst). Its syntax is:
copy(srcInst,dstInst,ﬁlter,scope)
The ﬁlter argument speciﬁes the set of ﬂows whose state to copy,
while the scope argument speciﬁes which class(es) of state (per-
ﬂow, multi-ﬂow, and/or all-ﬂows) to copy.
The copy operation is implemented using the get and put calls
from the southbound API (§4.2). No change in forwarding state oc-
curs as part of copy because state is not deleted from srcInst, allow-
ing srcInst to continue processing trafﬁc and updating its copy of
state. It is up to control applications to separately initiate a change
in forwarding state where the situation warrants (e.g., by directly
interacting with the SDN controller, or calling move for some other
class of state).
Eventual consistency can be achieved by occasionally re-copying
the same set of state. As described in §4.2, an NF will automatically
replace or combine the new and existing copies when putPer-
flow, putMultiflow, and putAllflows are called. Since
there are many possible ways to decide when state should be re-
copied—based on time, NF output, updates to NF state, or other
9The controller can check the counters on the ﬁrst ﬂow entry in sw
against the number of packets it has received from sw to ensure the
packet it currently has stored is in fact the last packet.
10Although state chunks get transferred and events get processed via
the controller in our current system, they can also happen peer to
peer.
169external factors—we leave it to applications to issue subsequent