### 4.3 Attacks

An attack is defined as a sequence of system calls that executes a malicious action against the operating system. However, these sequences are not unique. Attackers can generate an infinite number of obfuscated attack sequences by inserting extraneous, no-operation (nop) system calls into a known sequence and by substituting attack system calls with semantically equivalent ones. Manually specifying all possible attack sequences is impractical, as there may be unknown obfuscations. To address this, we specify the effects of attacks rather than the sequences themselves.

**Definition 5:** An attack effect \( E \) is a boolean formula over the set of OS state variables \( V \).

The formula \( E \) characterizes undesirable OS configurations indicative of a successful intrusion. It describes the attacker's intent and the impact of the attack on the OS. Any system call sequence \( A \) that transforms the OS from an initial, safe configuration satisfying \( I \) to a configuration satisfying \( E \) is considered an attack sequence. If \( A \) is allowed by the program model, then it is an undetected attack.

### 5 Automatic Attack Discovery

The goal of automatic attack discovery is to determine if any system call sequences accepted as valid by a program model will induce an attack configuration \( E \). Let \( E \) be an attack effect. The notation \( \Box \neg E \) expresses a safety property in linear-time temporal logic (LTL) that means "globally, \( E \) is never true." A program model \( M \) will detect any attack attempting to induce the effect \( E \) if and only if \( M \models \Box \neg E \). This means that within the executions allowed by \( M \), interpreted in the OS model \( \Omega \), the attack goal can never occur. The model checker attempts to prove this formula. If the proof succeeds, the attack goal cannot be reached given the system call sequences allowed by the program model. If the proof fails, the model checker has discovered a system call sequence that induces the attack goal.

We consider several examples:

**Example 2 (Expanded from Sect. 3 Example 1):** First, we find attacks that execute a root shell undetected by the four models in Figure 3.

If the attack succeeds, the executing image file is `/bin/sh` and the effective user ID is `0`:
\[ E : \text{image} = \text{"/bin/sh"} \land \text{euid} = 0 \]

This boolean expression captures the effect of the attack rather than any specific sequence of system calls. Running our tool for each of the four models shows that none detect the attack, as detailed in Section 3.3.

**Example 3:** Next, we try to find undetected attacks that write to the system’s password file.

If this attack succeeds, the file `/etc/passwd` will have been altered:
\[ E : \text{file["/etc/passwd"].written} = \text{true} \]

The tool automatically finds a successful attack against the Digraph and NFA models:
```plaintext
read(0);
setreuid(0, 0);
write(0);
stat(0, 0);
open("/etc/passwd", O_WRONLY | O_APPEND) = 3;
mmap(0, 0, 0, 0, 0, 0);
write(3);
```

The attack sequence first sets the effective user ID to root, allowing the process to open and modify the password file. The read, stat, mmap, and first write calls are nops irrelevant to the attack.

Conversely, the tool discovers that the Stide and PDA models will always detect any attack that tries to alter the password file. These models do not accept any system call sequence that grants write privileges to `/etc/passwd`.

**Example 4:** Finally, we try to find undetected attacks that add a new root-level account to the system and execute a user-level shell, enabling the attacker to subsequently switch to high privilege via the new account.

This combines elements of Examples 2 and 3:
\[ E : \text{image} = \text{"/bin/sh"} \land \text{file["/etc/passwd"].written} = \text{true} \]

The system finds an attack against the Digraph model:
```plaintext
read(0);
setreuid(0, 0);
write(0);
stat(0, 0);
open("/etc/passwd", O_WRONLY | O_APPEND) = 3;
mmap(0, 0, 0, 0, 0, 0);
write(3);
execve("/bin/sh");
```

The system proves that the Stide, NFA, and PDA models all detect this attack, regardless of any attempts to obfuscate the system call sequence. This is evident from the models: although they accept sequences that open and write to a file, they do not allow subsequent execution of a different program.

### 6 Implementation

Model checking either proves that an unsafe OS configuration cannot be reached in a program model or provides a counter-example system call trace that produces the unsafe configuration. As we are verifying transition systems that may be pushdown automata, we use pushdown model checkers [5, 17]. Moped [18] and Bebop [1] are interchangeable tools for analyzing pushdown systems. Our implementation uses Moped due to its public availability.

When a context-sensitive program model verifies a stream of system calls generated by an executing process, we call that model a pushdown automaton (PDA). The system calls are the input tape, and the PDA has final states corresponding to possible program termination points. When we analyze a model to verify its ability to detect attacks, we call the model a pushdown system (PDS).

**Definition 6:** A pushdown system (PDS) is a tuple \( Q = (S, \Sigma, \Gamma, \delta, s_0, Z_0) \), where each element is defined as in Definition 1.

A PDS is identical to a PDA except that it has no final states and no input tape. A PDS is a transition system used to analyze properties of sequences and is not a language acceptor. Moped verifies that no sequence of system calls in the PDS will produce an unsafe OS configuration.

The input to Moped includes a collection of variables and a PDS where each transition in \( \delta \) is tagged with a boolean formula. The formula expresses preconditions and postconditions over the variables. If no preconditions hold, Moped does not traverse the transition and does not alter the state variables. The Moped input language supports both boolean and integer variables, with integers represented internally as ordered lists of boolean bits.

We have developed a specification compiler that generates valid Moped input files from a PDA program model, the OS state variables, the initial OS configuration, the system call transformers, and the attack we wish to prove is detected (Figure 5). The compiler converts the PDA to a PDS by removing final state designations. It compiles each system call transformer into a boolean formula expected by Moped and annotates all system call transitions in the PDS with these formulas. For other transitions, such as push and pop, the compiler annotates them with a formula whose preconditions match any OS variable assignment and whose postconditions maintain that assignment. We add a new state \( A \) to the PDS and new transitions to \( A \) after each system call transition. The precondition on these new transitions is exactly the OS attack configuration \( E \) that we wish to prove cannot be reached in the model. We then invoke Moped to prove that state \( A \) cannot be reached or provide a counterexample trace of system calls reaching state \( A \).

### 7 Experiments

We used our implementation to find undetected attacks in program models from academic literature. Our automated approach can find mimicry and evasion attacks previously discovered manually [24, 22, 20, 21], allowing for better scaling of test cases compared to manual methods.

Previous work considered four test programs—wu-ftpd, restore, traceroute, and passwd—that had known vulnerabilities allowing attackers to execute a root shell. Forrest et al. [8] successfully detected known attack instances using the Stide model, a context-insensitive characterization of execution learned from system call traces. Wagner and Soto [24] and Tan et al. [22, 20, 21] demonstrated that attackers could modify their attacks to evade detection by the Stide model. In some cases, the undetected attacks were not semantically equivalent to the original root shell exploit but still adversely modified the system state to enable subsequent root access. For example, successful attack variants may:
- Write a new root-level account to the user accounts file `/etc/passwd`.
- Set `/etc/passwd` world-writable so that an ordinary user can add a new root account.
- Set `/etc/passwd` owned by the attacker so that the attacker can add a new root account.

We automatically found these undetected attacks. We analyzed the Stide model for each of the four programs with respect to each of the four attack goals. For wu-ftpd, we constructed the Stide model using the original Linux training data of Forrest et al. [7]. We were unable to obtain the wu-ftpd training data used by Wagner and Soto or the Stide models they constructed. Thus, we found attacks in the wu-ftpd model constructed from Forrest’s data that were reportedly not present in the model constructed from Wagner’s data. For the remaining three test programs, we constructed models from training data generated as described by Tan et al. [20]. Our specification compiler combined PDA representations of the Stide models with specifications of Linux system calls to produce pushdown systems amenable to model checking.

Table 1 lists the size of the PDA representation of the Stide model for each program. The OS state model included 119 bits of global state and 50 bits of temporary state for system call argument variables. This temporary state reduces Moped’s resource demands because it exists only briefly during the model checker’s execution.

Table 2 presents the ability of the Stide model to detect any attack designed to reach a particular attack goal, as determined by Moped. A “yes” indicates that the model will always prevent any attacker from reaching their goal, regardless of how they transform or alter their attack sequence. A “no” indicates the reverse: the model checker was able to find a system call sequence, with arguments, accepted by the model but that induces the unsafe operating system condition. Figure 6 shows the undetected attack against traceroute’s Stide model discovered by our system. We automatically found all attacks that researchers previously found manually, one additional attack due to differences between Forrest’s and Wagner’s training data for wu-ftpd, and an additional attack against restore not found by previous manual research.

Previous work missed this attack because manual inspection does not scale to many programs and attacks, making it difficult to show that an attack is not possible. Model checking can prove that a goal is unreachable regardless of the actual system calls used by the attacker. Table 3 lists the model checker’s running times in seconds for each model and attack goal. A trend is apparent: when the model checker finds an attack, the running times are very small. When no attack is found, the model checker executes for a longer period. This disparity reflects the behavior of the underlying model checking algorithms. A successful proof that the logical formula holds requires the model checker to follow all execution paths exhaustively, while finding a counter-example allows for early termination.

Automating the previously manual process of attack construction is a significant achievement. Attackers have significant freedom in program models that do not constrain system call arguments. For example, the sequence `open` followed by `write` without argument constraints can be misused to alter the system’s password file. Our automated system provides a means to understand exactly where a program model fails, as shown in Table 2. What is important is not just that the models fail to detect some attacks, but that we know exactly what types of attacks are missed.

### 8 Conclusions

Model-based intrusion detection systems are useful only when they actually detect or prevent attacks. Finding undetected attacks manually is difficult, error-prone, and unable to scale to large numbers of program models and attacks. We showed that formalizing the effects of attacks on the operating system provides a way to find undetected attacks automatically. A model checker attempts to prove that the attack effect will never hold in the program model. By finding counter-examples that cause the proof to fail, we find undetected attacks: system call sequences and arguments that are accepted as valid execution and induce the malicious attack effect on the operating system. This automation allows us to find undetected attacks against program models that were previously found only through manual inspection. The efficiency of the computation—about 2 seconds to find undetected attacks—indicates the potential for practical application.