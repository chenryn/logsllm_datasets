conﬁguration, and ∆ = {∆1, . . . , ∆n} is the collection of system call transformers.
4.3 Attacks
An attack is a sequence of system calls that executes some malicious action against
the operating system. However, these sequences are not unique. Attackers can pro-
duce an inﬁnite number of obfuscated attack sequences by inserting extraneous, nop
system calls into a known sequence and by changing attack system calls into other
semantically-equivalent calls. Manual speciﬁcation of actual attack sequences can be
incomplete, as there may be attack obfuscations not known to the individual specifying
the attacks. We circumvent this problem by specifying the effects of attacks rather than
the sequences themselves.
Deﬁnition 5. An attack effect E is a boolean formula over V .
The formula E characterizes bad operating system conﬁgurations indicative of a suc-
cessful intrusion. It describes the attacker’s intent and the effect of the attack upon the
OS. Any system call sequence A that takes the OS from an initial, safe conﬁguration
satisfying I to a conﬁguration satisfying E is then an attack sequence. If A is allowed
by the program model, than A is an undetected attack.
5 Automatic Attack Discovery
The role of automatic attack discovery is to determine if any system call sequences
accepted as valid execution by a program model will induce an attack conﬁguration E.
Let E be an attack effect. The notation (cid:1)¬E expresses a safety property in linear-
time temporal logic (LTL) that means “globally, E is never true”. A program model M
will detect any attack attempting to induce the effect E if and only if M (cid:2) (cid:1)¬E. That
is, within the executions allowed by M interpreted in the OS model Ω, the attack goal
can never occur. The model checker attempts to prove this formula true. If the proof
succeeds, then the attack goal could not be reached given the system call sequences
allowed by the program model. If the proof fails, then the model checker has discovered
a system call sequence that induces the attack goal.
We consider several examples:
Example 2 (Expanded from Sect. 3 Example 1). First, we ﬁnd attacks that execute a
root-shell undetected by the four models of Fig. 3.
If the attack succeeds, then the executing image ﬁle is /bin/sh and the effective
user ID is 0:
E : image = /bin/sh ∧ euid = 0
54
J.T. Gifﬁn, S. Jha, and B.P. Miller
This boolean expression expresses the effect of the attack rather than any particular
sequence of system calls that produces the effect. Running our tool for each of the four
models shows that none detect the attack, as shown in Sect. 3.3.
Example 3. Next, we try to ﬁnd undetected attacks that write to the system’s password
ﬁle.
If this attack succeeds, then the ﬁle /etc/passwd will have been altered:
E : f ile[/etc/passwd].written = true
The tool automatically ﬁnds a successful attack against the Digraph and NFA models:
read(0);
setreuid(0, 0);
write(0);
stat(0, 0);
open(“/etc/passwd”, O WRONLY | O APPEND) = 3;
mmap(0, 0, 0, 0, 0, 0);
write(3);
The attack sequence ﬁrst sets the effective user ID to root, which then allows the process
to open the password ﬁle and add a new user. The read, stat, mmap, and ﬁrst write
calls are all nops irrelevant to the attack.
Conversely, the tool discovers that the Stide and PDA models will always detect any
attack that tries to alter the password ﬁle. These models accept no system call sequence
that ever has write privilege to the ﬁle /etc/passwd.
Example 4. Finally, we try to ﬁnd undetected attacks that add a new root-level account
to the system and execute a user-level shell, with the expectation that the attacker can
subsequently switch to high privilege via the new account.
This combines elements of Examples 2 and 3:
E : image = /bin/sh ∧ f ile[/etc/passwd].written = true
The system ﬁnds an attack against the Digraph model:
read(0);
setreuid(0, 0)
write(0);
stat(0, 0);
open(“/etc/passwd”, O WRONLY | O APPEND) = 3;
mmap(0, 0, 0, 0, 0, 0);
write(3);
execve(“/bin/sh”);
The system proves that the Stide, NFA, and PDA models all detect this attack regardless
of any attempts to obfuscate a system call sequence. This is evident from the models: al-
though they accept sequences that open and write to a ﬁle, they do not allow subsequent
execution of a different program.
Automated Discovery of Mimicry Attacks
55
Compiler
Pushdown
System
Moped
Model
Checker
Attack Sequence
Fig. 5. Architecture
PDA Program Model
OS State Variables
Initial Configuration
Syscall Specifications
Attack Configuration
6 Implementation
Model checking either proves that an unsafe OS conﬁguration cannot be reached in
a program model or provides a counter-example system call trace that produces the
unsafe conﬁguration. As we are verifying transition systems that may be pushdown au-
tomata, we are limited in implementation options to pushdown model checkers [5, 17].
Moped [18] and Bebop [1] are interchangeable tools that analyze pushdown systems.
Our implementation uses Moped simply because of its public availability.
When a context-sensitive program model is used to verify a stream of system calls
generated by an executing process, we call that model a pushdown automaton (PDA).
The system calls are the input tape and the PDA has ﬁnal states that correspond to
possible program termination points. When we analyze a model to verify its ability to
detect attacks, we call the model a pushdown system.
Deﬁnition 6. A pushdown system (PDS) is a tuple Q = (cid:8)S, Σ, Γ, δ, s0, Z0(cid:9), where
each element of the tuple is deﬁned as in Deﬁnition 1.
The deﬁnition of a PDS is identical to that of a PDA, with the exception that the PDS
has no ﬁnal states and no input tape. A PDS is just a transition system used to analyze
properties of sequences and is not a language acceptor. Moped veriﬁes that no sequence
of system calls in the PDS will produce an unsafe operating system conﬁguration.
The input to Moped is a collection of variables and a PDS where each transition
in δ is tagged with a boolean formula. The formula expresses preconditions over the
variables that are required to hold before traversing the transition and postconditions
that hold after traversal. If no preconditions hold, then Moped will not traverse the
transition and will not alter the state variables. The Moped input language allows both
boolean and integer variables, although the integer variables are represented internally
as ordered lists of boolean bits.
We have written a speciﬁcation compiler that will produce valid Moped input ﬁles
from a PDA program model, the OS state variables, the initial OS conﬁguration, the
system call transformers, and the attack that we wish to prove is detected (Fig. 5). The
compiler converts the PDA to a PDS in a straightforward manner by simply removing
the designations for ﬁnal states. It compiles each system call transformer into a boolean
formula expected by Moped and annotates all system call transitions in the PDS with
these formulas. If the PDS contains other transitions, such as push and pop transitions
that do not correspond to system calls, the compiler annotates the transitions with a
formula whose preconditions match any OS variable assignment and whose postcon-
ditions simply maintain that assignment. We add one new state A to the PDS and new
J.T. Gifﬁn, S. Jha, and B.P. Miller
56
transitions to A after each system call transition. The precondition on these new transi-
tions is exactly the OS attack conﬁguration E that we wish to prove cannot be reached
in the model. We then invoke Moped so that it proves that state A cannot be reached or
provides a counterexample trace of system calls reaching state A.
7 Experiments
We used our implementation to ﬁnd undetected attacks in program models that have
appeared in academic literature. We show that our automated approach can ﬁnd the
mimicry and evasion attacks that previously were discovered manually [24, 22, 20, 21].
The automated techniques allow for better scaling of the number of test cases when
compared to manual approaches.
We can automatically ﬁnd mimicry and evasion attacks that previous research found
only with manual analysis. Previous work considered four test programs—wu-ftpd,
restore, traceroute, and passwd—that had known vulnerabilities allowing at-
tackers to execute a root shell. Forrest et al. [8] successfully detected known attack
instances using a model called Stide. The Stide model is a context-insensitive charac-
terization of execution learned from system call traces generated by a series of train-
ing runs. Wagner and Soto [24] and Tan et al. [22, 20, 21] demonstrated that attackers
could modify their attacks to evade detection by the Stide model. In some cases, the
undetected attacks were not semantically equivalent to the original root shell exploit,
although the attacks adversely modiﬁed system state so that the attacker can subse-
quently gain root access. For example, successful attack variants may:
– write a new root-level account to the user accounts ﬁle /etc/passwd;
– set /etc/passwd world-writable so that an ordinary user can add a new root
account; or
– set /etc/passwd owned by the attacker so that the attacker can add a new root
account.
We automatically found these undetected attacks. We used our infrastructure to ana-
lyze the Stide model for each of the four programs with respect to each of the four attack
goals. For wu-ftpd, we constructed the Stide model using the original Linux training
data of Forrest et al. [7]. We were unable to obtain either the wu-ftpd training data
used by Wagner and Soto or the Stide models that they constructed from that data. As a
result, we were able to ﬁnd attacks in the wu-ftpd model constructed from Forrest’s
data that were reportedly not present in the model constructed from Wagner’s data. For
the remaining three test programs, we constructed models from training data generated
in the manner described by Tan et al. [20]. Our speciﬁcation compiler combined PDA
representations of the Stide models with speciﬁcations of Linux system calls to produce
pushdown systems amenable to model checking.
Table 1 lists the size of the PDA representation of the Stide model for each program.
The OS state model included 119 bits of global state and 50 bits of temporary state
for system call argument variables. This temporary state reduces Moped’s resource de-
mands because it exists only brieﬂy during the model checker’s execution.
Table 2 presents the ability of the Stide model to detect any attack designed to reach
a particular attack goal, as determined by Moped. A “yes” indicates that the model
Automated Discovery of Mimicry Attacks
57
Table 1. Number of states and edges in the transition systems describing the Stide model for each
of the four test programs. The boolean OS state includes 119 bits for global state variables and
50 bits of temporary state for system call argument variables.
Edge count
State count
wu-ftpd restore traceroute passwd
1,058
766
2,085 1,206
1,477
892
623
459
Table 2. Evaluation of the Stide model’s ability to detect classes of attacks. A “yes” indicates
that the Stide model will always detect the attack because the model checker was unable to ﬁnd
an undetected attack. A “no” indicates that Stide is unable to protect the system from the attack
because the model checker discovered an undetected attack sequence. Writing to /etc/passwd
is normal behavior for passwd.
wu-ftpd restore traceroute passwd
No
Execute root shell
No
Write to /etc/passwd
Yes
Set /etc/passwd world-writable
Set /etc/passwd attacker-owned Yes
No
No
Yes
Yes
Yes
No
Yes
Yes
Yes
—
No
No
will always prevent any attacker from reaching their goal, regardless of how they try to
transform or alter their attack sequence of system calls. A “no” indicates the reverse:
the model checker was able to ﬁnd a system call sequence, with arguments, accepted by
the model but that induces the unsafe operating system condition. Figure 6 shows the
undetected attack against traceroute’s Stide model discovered by our system. We auto-
matically found all attacks that researchers previously found manually, one additional
attack due to differences between Forrest’s training data and Wagner’s training data for
wu-ftpd, and an additional attack against restore not found by previous manual
research.
Previous work missed this attack because manual inspection does not scale to many
programs and attacks, and hence the research did not attempt to compute results for all
attack goals in all programs. When using manual inspection, it is likewise difﬁcult to
show that an attack is not possible: has the analyst simply not considered an attack that
would be successful? Model checking can prove that a goal is unreachable regardless
of the actual system calls used by the attacker in their attempt to reach the goal. We
close; munmap; open(“/etc/passwd”, O RDWR | O APPEND) = 3;
fcntl64; fcntl64; fstat64; mmap2; read; close; munmap; write(3);
Fig. 6. Undetected attack against the Stide model of traceroute that adds a new root-level user to
/etc/passwd. The system calls producing the attack effect are shown in boldface. Although
our system discovers arguments for the nop system calls, we omit the arguments here for con-
ciseness. We do not discover the actual string value written to /etc/passwd; a suitable string
would be “attacker::0:0::/root:/bin/sh\n”.
58
J.T. Gifﬁn, S. Jha, and B.P. Miller
Table 3. Model checking running times, in seconds.
Execute rootshell
Write to /etc/passwd
Set /etc/passwd world-writable
Set /etc/passwd attacker-owned
wu-ftpd restore traceroute passwd
2.70
—
2.02
1.81
0.75
0.34
0.39
0.73
39.10 74.41
41.11 65.21
2.38
1.33
0.90
1.15
can show that the models of the ﬁrst three test programs detect all attacks that try to
set /etc/passwd world-writable or owned by the attacker—assertions that previous
manual efforts were unable to make. Although the proofs of detection hold with respect
to the OS abstraction and may not hold in the actual OS implementation, as described
in Sect. 2, the proofs do provide a strong indication that runtime attack detection in the
real system will be effective.
Table 3 lists the model checker’s running times in seconds for each model and attack
goal. When comparing the running times with Table 2, a loose trend becomes apparent.
In cases where the model checker found an attack, the running times are very small.
When no attack was found, the model checker executed for a comparatively longer
period of time. This disparity is to be expected and reﬂects the behavior of the un-
derlying model checking algorithms. When a model checker ﬁnds a counter-example
that disproves a logical formula—here an attack sequence that violates an LTL safety
property—the model checker can immediately terminate its execution and report the
counter-example. However, a successful proof that the logical formula holds requires
the model checker to follow exhaustively all execution paths and early termination is
not possible.
We believe that automating the previously manual process of attack construction is
a signiﬁcant achievement. We are not surprised at our ability to ﬁnd undetected attacks:
attackers have signiﬁcant freedom in program models that do not constrain system call
arguments. For example, the system call sequence open followed by write without
argument constraints can be misused by an attacker to alter the system’s password ﬁle.
Yet, this is a common sequence contained in nearly every non-trivial program, including
programs that execute with the root-level privilege required to alter the password ﬁle.
Our automated system provides us with the means to understand exactly where a
program model fails. From Table 2 we learn which classes of attack can be effectively
detected by a program model and which classes of attack require alternative protection
strategies. What is important is not simply that the models fail to detect some attacks,
but that we know exactly what type of attacks are missed.
8 Conclusions
Model-based intrusion detections systems are useful only when they actually detect or
prevent attacks. Finding undetected attacks manually is difﬁcult, error-prone, unable
to scale to large numbers of program models and attacks, and unable to prove that an
attack will always be detected. We showed here that formalizing the effects of attacks
Automated Discovery of Mimicry Attacks
59
upon the operating system provides the operational means to ﬁnd undetected attacks
automatically. A model checker attempts to prove that the attack effect will never hold
in the program model. By ﬁnding counter-examples that cause the proof to fail, we
ﬁnd undetected attacks: system call sequences and arguments that are accepted as valid
execution and induce the malicious attack effect upon the operating system. This au-
tomation let us ﬁnd undetected attacks against program models that previously were
found only with manual inspection of the models. The efﬁciency of the computation—
about 2 seconds computation to ﬁnd undetected attacks—provides an indication that