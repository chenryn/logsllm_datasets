### 深度强化学习中的动作分布

在深度强化学习中，常用的动作分布包括伯努利分布（Bernoulli Distribution）、类别分布（Categorical Distribution）和对角高斯分布（Diagonal Gaussian Distribution）。这些分布用于表示随机性策略中的动作选择。

- **伯努利分布**：适用于离散的二值动作空间。单变量 \( x \in \{0, 1\} \) 的伯努利分布定义为 \( P(x; \theta) = \theta^x (1 - \theta)^{1-x} \)。它可以扩展到多维情况，每个维度都是独立的二值变量。
- **类别分布**：适用于有限的离散动作空间。类别型策略将状态作为输入，并输出每个动作的概率。常用的激活函数是Softmax，以确保所有概率之和为1。实践中，动作通常编码为独热向量（One-Hot Vector），并通过逐元素乘积操作计算给定状态下的动作概率。
- **对角高斯分布**：适用于连续动作空间。对角高斯分布通过均值向量 \(\mu\) 和方差向量 \(\sigma\) 表示，协方差矩阵只有对角元非零。再参数化技巧可以用来从该分布中采样动作，同时保持梯度可微性。

### 类别型策略与耿贝尔-Softmax技巧

类别型策略使用Softmax函数来输出每个动作的概率。为了保持采样过程的可微性，可以使用耿贝尔-Softmax技巧：

\[ z_i = \text{one_hot} \left( \arg\max_j \left( \log \pi_j + g_j \right) \right) \]

其中 \( g_j \) 是从耿贝尔分布中采样的噪声项。耿贝尔-Softmax技巧通过引入温度参数 \(\tau\) 来近似上述过程：

\[ a_i = \frac{\exp \left( (\log \pi_i + g_i) / \tau \right)}{\sum_{j} \exp \left( (\log \pi_j + g_j) / \tau \right)} \]

### 对角高斯策略与再参数化技巧

对角高斯策略输出均值 \(\mu\) 和方差 \(\sigma\)，用于连续动作空间。再参数化技巧通过以下方式从高斯分布中采样动作：

\[ a = \mu + \sigma \odot z \]

其中 \( z \sim N(0, I) \)，\(\odot\) 表示逐元素乘积。

### 基于策略的优化方法

基于策略的优化方法直接优化智能体的策略，而不是估计或学习动作价值函数。常见的方法包括基于梯度的方法（如REINFORCE）和无梯度方法（如交叉熵方法）。

#### 策略梯度定理

策略梯度定理提供了计算策略梯度的一种方法：

\[ \nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(A_t | S_t) Q^{\pi_\theta}(S_t, A_t) \right] \]

其中 \( Q^{\pi_\theta}(S_t, A_t) \) 是状态-动作对的价值函数。

#### 随机性策略梯度

对于随机性策略，轨迹的概率可以表示为：

\[ \log p(\tau | \theta) = \log \rho_0(S_0) + \sum_{t=0}^{T} \left( \log p(S_{t+1} | S_t, A_t) + \log \pi_\theta(A_t | S_t) \right) \]

利用对数-导数技巧，可以得到轨迹概率的梯度：

\[ \nabla_\theta \log p(\tau | \theta) = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(A_t | S_t) \]

最终，策略梯度可以表示为：

\[ \nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} R_t \nabla_\theta \log \pi_\theta(A_t | S_t) \right] \]

这使得我们可以使用梯度上升法来优化策略。

通过这些方法，深度强化学习中的策略可以被有效地优化，从而提高智能体在复杂环境中的表现。