作A =µ (S )和A ∼π (·|S )。
t θ t t θ t
在深度强化学习领域，有一些常见的具体分布用来表示随机性策略中的动作分布：伯努利
分布（Bernoulli Distribution），类别分布（Categorical Distribution）和对角高斯分布（Diagonal
GaussianDistribution）。伯努利和类别分布可以用于离散动作空间，如二值的（Binary）或多类别
的（Multi-Category），而对角高斯分布可以用于连续动作空间。
一个以θ 为参数的单变量x ∈ {0,1}的伯努利分布为P(s;θ) = θx(1−θ)(1−x)。因而它可以
被用于表示二值化的动作，可以是单维，也可以是多维（对一个矢量中含多个变量的情况应用），
它可以用作二值化动作策略（Binary-ActionPolicy）。
类别型策略（Categorical Policy）使用类别分布作为它的输出，因而可以用于离散且有限的
动作空间，它将策略视为一个分类器（Classifier），以状态为条件（ConditionedonAState）而输
出在有限动作空间中每个动作的概率，比如π(a|s)=P[A =a|S =s]。所有概率和为1，因此，
t t
当将类别型策略参数化时，最后输出层（OutputLayer）常用Softmax激活函数。这里我们具体使
用 P[·|·] 矩阵表示有限动作空间的情况，来替代概率函数 p(·|·)。智能体可以根据类别分布采样
选择一个动作。实践中，这种情况下的动作通常可以编码为一个独热编码矢量（One-HotVector）
a =(0,0,··· ,1,··· ,0)，这个矢量跟动作空间有相同的维度，从而a ⊙p(·|s)给出p(a |s)，其中
i i i
⊙是逐个元素的乘积（Element-WiseProduct）算子，而p(·|s)是给定状态s时的矩阵中的一个矢量
（行或列，依状态动作顺序而定），而这通常也是归一化后类别型策略的输出层。耿贝尔-Softmax
89
第2章 强化学习入门
函数技巧（Gumbel-Softmax Trick）可以在实践中参数化类别型策略后用来保持类别分布采样过
程的可微性。在没有使用其他技巧的情况下，有采样过程或像argmax类操作的随机性节点往往
是不可微的（Non-Differentiable），从而在对参数化策略使用基于梯度的优化（在随后小节中介
绍）时可能是有问题的。
耿贝尔-Softmax函数技巧（Gumbel-SoftmaxTrick）：首先，耿贝尔-最大化技巧（Gumbel-Max
Trick）允许我们从类别分布π中采样
z =one_hot[argmax(z +logπ )] (2.95)
i i
i
其中“one_hot”是一个将标量转换成独热编码矢量的操作。然而，如上所述，argmax操作通常
是不可微的。因此，在耿贝尔-Softmax函数技巧中，一个Softmax操作被用来对耿贝尔-最大化技
巧中的argmax进行连续性近似：
exp((logπ +g )/τ)
a = P i i ,∀i=0,··· ,k (2.96)
i exp((logπ +g )/τ
j j j
其中k 是欲求变量a（强化学习策略的动作选择）的维度，而g 是采样自耿贝尔分布（Gumbel
i
Distribution）的耿贝尔（Gumbel）变量。耿贝尔(0,1)分布可以用逆变换（InverseTransform）采
样实现，通过采样均匀分布u∼Uniform(0,1)并计算g =−log(−log(u))得到。
对角高斯策略（Diagonal Gaussian Policy）输出一个对角高斯分布的均值和方差用于连续动
作空间。一个普通的多变量高斯分布包括一个均值矢量 µ 和一个协方差（Covariance）矩阵 Σ，
而对角高斯分布是其特殊情况，即协方差矩阵只有对角元非零，因此我们可以用一个矢量σ来表
示它。当使用对角高斯分布来表示概率性动作时，它移除了不同动作维度间的协相关性。一个策
略被参数化时，如下所示的再参数化（Reparametrization）技巧（与Kingmaetal.(2014)提出的变
分自动编码器中类似）可以被用来从均值和方差矢量表示的高斯分布中采样，同时保持操作的可
微性。
再参数化技巧：从对角高斯分布中采样动作a∼N(µ ,σ )，该分布的均值和方差矢量为µ
θ θ θ
和σ （参数化的），而这可以通过从正态分布中采样一个隐藏矢量z ∼N(0,I)来得到动作：
θ
a=µ +σ ⊙z (2.97)
θ θ
其中⊙是两个相同形状矢量的逐个元素乘积。
深度强化学习中的常用策略如图2.19所示，便于读者理解。
基于策略的优化（Policy-BasedOptimization）方法在强化学习情景下直接优化智能体的策略
而不估计或学习动作价值函数。采样得到的奖励值通常用于改进动作选择的优化过程，而优化过
程可以使用基于梯度或无梯度（Gradient-Free）的方法。其中，基于梯度的方法通常采用策略梯
90
2.7 策略优化
度（PolicyGradient），它在某种程度上代表了连续动作强化学习最受欢迎的一类算法，受益于对
高维情况的可扩展性。典型的基于梯度优化方法包括REINFORCE等。无梯度方法对策略搜索中
相对简单的情况通常有更快的学习过程，无须有复杂计算的求导过程。典型的无梯度类方法包括
交叉熵（Cross-Entropy，CE）方法等。
确定性策略
⼆值化动作策略
深度强化学习中的常⽤策略
类别型策略
随机性策略
⾼斯分布
其他：⾃回归策略等
图2.19 深度强化学习中的不同策略类型
回想我们在强化学习中智能体的目标是从期望或估计的角度去最大化从一个状态开始的累
计折扣奖励（CumulativeDiscountedReward），可以将其表示为
J(π)=E τ∼π[R(τ)] (2.98)
P
其中R(τ)= T γtR 是有限步（适用于多数情形）的折扣期望奖励，而τ 是采样的轨迹。
t=0 t
基于策略的优化方法将根据以上目标函数J(π)通过基于梯度的或无梯度的方法，来优化策
略π。我们将首先介绍基于梯度的方法，并给出一个REINFORCE法的例子，随后介绍无梯度的
算法和CE方法的例子。
基于梯度的优化
基于梯度的优化方法是使用在期望回报（总的奖励）上的梯度估计来进行梯度下降（或上
升），以改进策略，而这个期望回报是从采样轨迹中得到的。这里我们把关于策略参数的梯度叫
作策略梯度（PolicyGradient），具体表达式如下：
∆θ =α∇ J(π ) (2.99)
θ θ
其中θ表示策略参数，而α是学习率。基于策略参数的梯度计算方法叫作策略梯度法。文献(Sutton
etal.,2000)和文献(Silveretal.,2014)提出的策略梯度定理（PolicyGradientTheorem）及其证明
将在下面介绍。
注：式(2.99)中参数θ的表示方法实际上是不合适的，根据本书默认的格式，它应当是θ从
而表示矢量。然而，这里我们使用基本的 θ 格式作为一种可以在使用模型参数时替代的 θ 的方
式，而这种简单的写法也在文献中常见。一种考虑这种写法合理性的方式是：参数的梯度可以对
91
第2章 强化学习入门
每个参数分别得到，而每个参数均可单独表示为θ，只要方程对所有参数相同，它就可以用θ来
表示所有参数。本书的其余章节将遵循以上声明。
定理2.2 策略梯度定理
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θ(logπ θ(A t|S t))Qπθ(S t,A t)5 (2.100)
t=0
=E St∼ρπ,At∼πθ[∇ θ(logπ θ(A t|S t))Qπθ(S t,A t)] (2.101)
R P
其中第二项需定义折扣状态分布（Discounted State Distribution）ρπ(s′) := T γt−1 ρ (s)
S t=0 0
p(s′|s,t,π)ds，而p(s′|s,t,π)是在策略π下第t个时间步从s到s′的转移概率（TransitionProba-
bility），参见文献(Silveretal.,2014)。
策略梯度定理对随机性策略和确定性策略都适用。它起初由Sutton等人(Suttonetal.,2000)
为随机性策略而提出，后被Silver等人(Silveretal.,2014)扩展到确定性策略。对确定性的情况，
尽管确定性策略梯度（DeterministicPolicyGradient，DPG）定理（后续介绍）与上述策略梯度定
理看起来不同，实际上可以证明确定性策略梯度只是随机性策略梯度（StochasticPolicyGradient，
SPG）的一种特殊（极限）情况。若用一个确定性策略µ : S → A和一个方差参数σ 来参数化
θ
随机性策略π ，则有σ =0时随机性策略等价于确定性策略，即π ≡µ。
µθ,σ µθ,0
(1)随机性策略梯度
首先我们对随机性策略证明策略梯度定理，因而被称为随机性策略梯度方法。为了简便，在
本小节中，我们假设有限MDP下的片段式（Episodic）设定，每个轨迹长度固定为T +1。考虑
一个参数化的随机性策略π (a|s)，对以ρ (S )为初始状态分布的MDP过程，有轨迹的概率为
Q θ 0 0
p(τ|π)=ρ (S ) T p(S |S ,A )π(A |S )，因而可以得到基于参数化策略π 的轨迹概率的对
0 0 t=0 t+1 t t t t θ
数（Logarithm）为
XT
logp(τ|θ)=logρ (S )+ logp(S |S ,A )+logπ (A |S ) . (2.102)
0 0 t+1 t t θ t t
t=0
我们也需要对数-导数技巧（Log-DerivativeTrick）：∇ p(τ|θ)=p(τ|θ)∇ logp(τ|θ)得到轨迹概率
θ θ
对数（Log-Probability）的导数为
XT
∇ logp(τ|θ)=∇ logρ (S )+ ∇ logp(S |S ,A )+∇ logπ (A |S ) (2.103)
θ θ 0 0 θ t+1 t t θ θ t t
t=0
XT
= ∇ logπ (A |S ). (2.104)
θ θ t t
t=0
92
2.7 策略优化
其中包含ρ (S )和p(S |S ,A )的项被移除，因为它们不依赖于参数θ，尽管是未知的。
0 0 t+1 t t
回想之前介绍过，学习目标是最大化期望累计奖励（ExpectedCumulativeReward）：
2 3
  XT XT
J(π θ)=E τ∼πθ R(τ) =E τ∼πθ4 R t5 = E τ∼πθ[R t], (2.105)
t=0 t=0
P
其中τ =(S ,A ,R ,··· ,S ,A ,R ,S )且R(τ)= T R 。我们可以直接在策略参数θ上
0 0 0 T T T T+1 t=0 t
进行梯度上升来逐渐改进策略π 的表现。
θ
注意R 只依赖τ ，其中τ =(S ,A ,R ,··· ,S ,A ,R ,S )。
t t t 0 0 0 t t t t+1
Z
∇ θE τ∼πθ[R t]=∇ R tp(τ t|θ)dτ 展开期望 (2.106)
θ t
Z τt
= R ∇ p(τ |θ)dτ 对换梯度和积分 (2.107)
t θ t t
Zτt
= R p(τ |θ)∇ logp(τ |θ)dτ 对数-导数技巧 (2.108)
t t θ t t
τt
=E τ∼πθ R t∇ θlogp(τ t|θ) 回归期望形式 (2.109)
上面第三个等式是根据之前介绍的对数-导数技巧得到的。
将上面式子代入到J(π )，
θ
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 R t∇ θlogp(τ t|θ)5 .
t=0
现在我们需要计算∇ logp (τ )，其中p (τ )依赖于策略π 和模型p(R ,S |S ,A )的真
θ θ t θ t θ t t+1 t t
实值，而该模型对智能体是不可用的。幸运的是，为了使用策略梯度方法，我们只需要logp (τ )
θ t
的梯度而不是它本身的值，而这可以简单地用τ =τ 替换式(2.104)中的τ =τ 而得到下式：
t 0:t 0:T