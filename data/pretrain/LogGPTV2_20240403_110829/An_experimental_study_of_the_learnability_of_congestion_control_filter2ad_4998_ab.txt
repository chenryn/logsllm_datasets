rithms typically compute a congestion window or a paced trans-
mission rate using the stream of acknowledgments (ACKs) arriving
from the receiver. In response to congestion, inferred from packet
loss or, in some cases, rising delays, the sender reduces its window
or rate; conversely, when no congestion is perceived, the sender
increases its window or rate.
In this paper, we use the Remy [29] protocol-design tool to gen-
erate end-to-end Tao congestion-control schemes from ﬁrst prin-
ciples. The Remy work showed that such an approach can pro-
duce schemes whose performance is competitive with or outper-
forms human-generated schemes, including most varieties of TCP
congestion control, on intended target networks.
By contrast, this paper uses the Remy program as a tool for un-
derstanding the nature of the problem of protocol design without
being able to fully deﬁne the intended target networks. We use the
program’s output as a proxy for the “best possible” Tao congestion-
control protocol intended for a particular imperfect network model,
and then ask how that protocol performs on a different set of net-
works that varies from the model in some respect (topology, link
speed, behavior of contending endpoints, etc.).
For reference, we also compare with existing congestion-control
protocols in wide use, including Linux’s TCP Cubic [12], and the
less-aggressive NewReno algorithm [14]. End-to-end congestion
control may be assisted with explicit router participation; we also
measure Cubic in conjunction with sfqCoDel [2]. sfqCoDel runs
at the bottleneck gateways and uses the CoDel [21] queue man-
agement algorithm along with the stochastic fair queueing [20]
scheduling algorithm.
2.2 Learnability
TCP congestion control was not designed with an explicit objec-
tive function in mind. Kelly et al. present an interpretation of TCP
congestion-control variants in terms of the implicit goals they at-
tempt to optimize [17]. This line of work is known as Network Util-
ity Maximization (NUM); more recent work has modeled stochastic
NUM problems [31], where a stochastic process dictates how ﬂows
enter and leave the network over time.
We extend this problem by examining the difﬁculty of designing
a network protocol given an imperfect model of the network where
481it will be deployed, in order to understand the inherent difﬁculties
of the problem of congestion control.
Formally speaking, designing such a protocol is a problem in
sequential decision-making under uncertainty and can be modeled
as a decentralized partially observable Markov decision process [4]
(Dec-POMDP). In that context, the purpose of this paper is to ask:
how well can a protocol designer “learn” the optimal policy (proto-
col) for one Dec-POMDP on a given set of networks and success-
fully apply the learned policy to a different set of networks?
In doing so, we draw an explicit analogy to the concept of
“learnability” employed in machine learning [28, 25]. A canoni-
cal machine-learning task attempts to design a classiﬁer for a large
population of data points, supplied with only a smaller (and possi-
bly skewed) “training set” meant to teach the classiﬁer about the full
population of data. Subsequently, the performance of the resulting
classiﬁer is evaluated in how well it correctly classiﬁes points in a
“test set”, generally drawn from the actual population. Learnability
theory measures the difﬁculty of inferring an accurate classiﬁer for
the test set, given a training set.
Just as a classiﬁer-design procedure may minimize the error rate
or maximize the width of the margin [5] over the training set as a
proxy for maximizing predictive performance on unseen inputs, the
Remy tool uses an objective function in terms of throughput and
delay, averaged over the design model, as a proxy for performance
on as-yet-unseen networks.
In our work, we envision a protocol designer working to generate
a congestion-control protocol for a large set of real networks (e.g.,
the Internet), supplied with only an imperfect model of the range of
variation and behavior of those networks. The imperfect model is
the “training scenarios”—a model of the target networks used for
design purposes. The “testing scenarios” are drawn from the popu-
lation of actual target networks. In contrast with theoretical notions
of learnability that rigorously demonstrate the learnability of entire
families of functions [28], this study assesses learnability experi-
mentally: we measure the difﬁculty (in terms of lost performance)
of designing an adequate protocol for a network model, and then
deploying it on target networks that cannot be perfectly envisioned
at the time of design.
3. EXPERIMENTAL SETUP
We describe our experimental procedure below. First, we spec-
ify a set of training scenarios: a set of network conﬁgurations
(§3.1) that express the designer’s imperfect model of the network.
Next, we specify an objective function (§3.2). The protocol-design
process (§3.3) synthesizes a congestion-control protocol that max-
imizes the value of this function, averaged over the set of train-
ing scenarios. Finally (§3.6), we specify the testing scenarios of
network conﬁgurations, which may be similar or dissimilar to the
training scenarios.
We evaluate the synthesized congestion-control protocol on the
testing scenarios to assess the questions of this study—how easy
is it to “learn” a network protocol to achieve desired goals, given
an imperfect model of the networks where it will ultimately be de-
ployed?
3.1 Training scenarios
The training scenarios specify the set of network conﬁgurations
that the protocol-design process is given access to. Formally, a net-
work conﬁguration speciﬁes:
2. The locations of senders and receivers within the topology,
and the paths connecting the senders to the receivers.
3. A model of the workload generated by the application run-
ning at each endpoint. We use an on/off model for the
workload, where a sender turns “on” for a certain duration
drawn from an exponential distribution, then turns “off” for
an amount of time drawn from another exponential distribu-
tion before turning on again.
4. The buffer size and queueing discipline at each gateway. For
all training scenarios in this paper, we model a FIFO queue.
For testing scenarios, we model a FIFO queue except in the
case of Cubic-over-sfqCoDel, which runs sfqCoDel at the
gateway nodes.
3.2 Objective function
The objective function expresses the protocol designer’s ﬁgure of
merit for the goodness of a congestion-control protocol. Many such
metrics have been proposed, including alpha-fair throughput [26],
ﬂow completion time [8], throughput-over-delay [22], or measures
based on a subjective opinion score [13].
In this study, we speciﬁcally considered objective functions of
the form:
log (throughput)− δ log (delay)
(1)
Here, “throughput” is the average information transmission rate
of a sender-receiver pair, deﬁned as the total number of bytes suc-
cessfully delivered divided by the total time the sender was “on”
and hence had offered load. The “delay” is the average per-packet
delay of packets in the connection, including propagation delay and
queueing delay. The δ factor expresses a relative preference be-
tween high throughput and low delay.
The protocol-design process works to maximize the sum of the
objective function across all connections. The log in the objective
function expresses a preference for “proportionally fair” resource
allocation [17]—for example, it is worthwhile to cut one connec-
tion’s throughput in half, as long as this allows another connection’s
throughput to be more-than-doubled.
3.3 Protocol-design process
We outline the Remy protocol-design tool brieﬂy here, following
the treatment of [29]. Remy models a congestion-control protocol
as a set of match-action rules, mapping between the state main-
tained by the sender and an action to be executed. The “state”
tracks a small set of congestion signals, updated on every acknowl-
edgment from the receiver. The “action” speciﬁes changes in the
behavior of the congestion-control protocol.
To simplify learning, Remy assumes a piecewise-constant map-
ping, and searches for the mapping that maximizes the average
value of the objective function across the training scenarios. The
mapping is initialized to prescribe a default action for all memory
values. Remy then simulates the protocol on the training scenar-
ios and uses the simulation results—and the resulting value of the
objective function—to gradually reﬁne the mapping.
For the experiments in this paper, the sender tracks four conges-
tion signals:
1. rec_ewma: An exponentially-weighted moving average, or
EWMA, of the interarrival times between acks with a weight
of 1/8 for new samples.
1. The topology: the link speed and propagation delay of each
link in the network along with a graph representing the inter-
connections between nodes.
2. slow_rec_ewma: The same as rec_ewma, but with a
weight of 1/256 for new samples, producing an average taken
over a longer history.
4823. send_ewma: A moving average of the intersend time be-
tween sender timestamps echoed in the received ACKs, with
a weight of 1/8 for new samples.
4. rtt_ratio: The ratio of the most recent round-trip-time
measurement and the minimum RTT seen so far.
3.4 Value of the congestion signals
We performed a measurement study to evaluate the value of each
of these four signals on the ultimate performance of a congestion-
control protocol. We selectively “knocked out” each signal in turn
and designed a new congestion-control protocol from scratch (miss-
ing that signal), in order to observe the effect of losing the signal on
the protocol’s ultimate behavior.
In our measurements, we found that each of these congestion
signals independently brought value to a congestion-control proto-
col. No three-signal subset was as strong as using all four signals.
The most valuable signal—by which we mean the signal whose re-
moval caused the greatest harm to the ultimate performance—was
the rec_ewma. This suggests that these protocols may gain con-
siderable value from understanding the short-term packet-arrival
dynamics at the receiver.
3.5 The congestion response
The action uses a window-based congestion-control protocol that
caps the number of packets in ﬂight, with pacing to regulate the rate
at which an end host meters packets into the network. The action
for any value of the memory is a triplet that speciﬁes:
1. A multiplier m to the current value of the congestion window.
2. An increment b to the current value of the congestion win-
dow.
3. A lower bound τ on the pacing interval between outgoing
packet transmissions.
To emphasize that the resulting protocol is a brute-force approxi-
mation of the best algorithm for a given set of training scenarios and
objective function, we refer to such protocols as “tractable attempts
at optimal” congestion control, or Tao protocols.
3.6 Evaluation procedure
To measure the difﬁculty of learning a congestion-control pro-
tocol with an imperfect model of the eventual network, we choose
a testing scenario of network conﬁgurations and evaluate the Tao
protocol on it. All evaluations are performed in the ns-2 simulator.
Using a different simulator for training and testing helps build con-
ﬁdence that the congestion-control protocols learned are robust to
quirks in a simulator implementation.
We compare the performance of Tao protocols optimized for “ac-
curate” models of the network against Tao protocols optimized for
various kinds of imperfect models, in order to measure how faith-
fully protocol designers need to understand the network they are
designing for. In the interest of reproducibility, for each experiment
that we run, we tabulate the training and testing scenarios (for e.g.
Tables 3a and 3b).
For reference, we also compare the Tao protocols with two com-
mon schemes in wide use today:
1. TCP Cubic [12], the default congestion-control protocol in
Linux
2. Cubic over sfqCoDel [2], an active-queue-management and
scheduling algorithm that runs on bottleneck routers and as-
sists endpoints in achieving a fairer and more efﬁcient use of
network resources
3.7 Caveats and non-goals
We view this work as a ﬁrst step towards answering questions un-
der the broad umbrella of the “learnability” of congestion control.
The simulation experiments are deliberately simplistic, favoring
scenarios that explore foundational questions over simulations that
are representative of deployed networks. We use Remy-generated
protocols as a proxy for the optimal solutions, which means that
the results may change when better protocol-design tools are devel-
oped, or when these protocols are tested on real physical networks
outside of simulation. It is not our goal here to reverse-engineer
the per-node, microscopic behavior of Tao protocols—we are inter-
ested in using measurements of their varying performance to char-
acterize the learnability of the protocol-design problem itself.
4.
INVESTIGATING THE LEARNABILITY OF
CONGESTION CONTROL
4.1 Knowledge of link speed
We evaluated the difﬁculty of designing a congestion-control
protocol, subject to imperfect knowledge of the parameters of the
network.
Some congestion-control protocols have been designed for spe-
ciﬁc kinds of networks [3, 19] or require explicit knowledge of the
link speed a priori [16]. Others are intended as a “one size ﬁts all,”
including most variants of TCP.
We set out to answer a question posed in [30]: are “one size ﬁts
all” protocols inherently at a disadvantage, because of a tradeoff
between the “operating range” of a protocol and its performance?
To quantify this, we designed four Tao protocols for training
scenarios encompassing a thousand-fold variation in link speeds,
a hundred-fold variation, a ten-fold variation, and a two-fold vari-
ation. Each range was centered on the geometric mean of 1 and
1000 Mbps (32 Mbps), and each set of training scenarios sampled
100 link speeds logarithmically from the range. The training and
testing scenarios are shown in Table 2.
Tao
1000x
100x
10x
2x
Link speeds
1–1000 Mbps
3.2–320 Mbps
10–100 Mbps
22–44 Mbps
RTT
150 ms
150 ms
150 ms
150 ms
Number of senders
2
2
2
2
(a) Tao protocols designed for breadth in link speed
Link speeds
1–1000 Mbps
RTT
150 ms
Number of senders
2
(b) Testing scenarios to explore breadth in link speed
Table 2: Scenarios for “knowledge of link speed” experiment,
showing the effect of varying the intended link-speed operating
range. Each Tao was designed for a network with a single bot-
tleneck, and each sender had a mean “on” and “off” time of 1 s.
We tested these schemes in ns-2 by sweeping the link speed be-
tween 1 and 1000 Mbps, keeping the other details of the simulated
network identical to the training scenario. The results are shown
Figure 2.
We ﬁnd evidence of a weak tradeoff between operating range
and performance—optimizing for a smaller range did help mod-
estly over that range. However, the improvements in objective from
narrowing down the operating range were modest, and each Tao
483Tao
Tao-1–2
Tao-1–10
Tao-1–20
Tao-1–50
Tao-1–100
Link
speeds
15 Mbps
15 Mbps
15 Mbps