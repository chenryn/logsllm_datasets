### Unique Objects and Multi-Flow State Caching

Unique objects were cached before the copy operation. Ignoring the multi-flow state entirely causes the second instance to crash, as the objects currently being served to the second client are not available. Copying the multi-flow state for the second client's flows avoids the crash, but skipping other multi-flow states results in a 28% lower cache hit ratio at Squid2 compared to copying all multi-flow states (i.e., the entire cache). However, the latter requires a 14.2x larger state transfer. OpenNF’s APIs allow each application to make the appropriate trade-offs in such respects when selecting the granularity at which to invoke operations.

### 8.2 Southbound API

The time required to export and import state at Network Functions (NFs) directly impacts how quickly a move or copy operation completes and how much additional packet latency is incurred when sharing is used. We thus evaluate the efficiency of OpenNF’s southbound operations for several of the NFs we modified. We also examine how much code was added to the NFs to support these operations.

#### 8.2.1 API Call Processing

Figures 12(a) and 12(b) show the time required to complete a `getPerflow` and `putPerflow` operation, respectively, as a function of the number of flows whose state is exported/imported. We observe a linear increase in the execution time of `getPerflow` and `putPerflow` as the number of per-flow state chunks increases. The time required to (de)serialize each chunk of state and send it to (receive it from) the controller accounts for the majority of the execution time. Additionally, we observe that `putPerflow` completes at least 2x faster than `getPerflow`; this is due to deserialization being faster than serialization. Overall, the processing time is highest for Bro because of the size and complexity of the per-flow state. The results for multi-flow state are qualitatively similar; we exclude them for brevity. We are working on techniques to further improve the efficiency of southbound API calls.

We also evaluate how NF performance is impacted by the execution of southbound operations. In particular, we measure the average per-packet processing latency (including queuing time) during normal NF operation and when an NF is executing a `getPerflow` call. Among the NFs, the PRADS asset monitor has the largest relative increase—5.8% (0.120ms vs. 0.127ms), while the Bro IDS has the largest absolute increase—0.12ms (6.93ms vs. 7.06ms). In both cases, the impact is minimal, implying that southbound operations do not significantly degrade NF performance.

#### 8.2.2 NF Changes

To quantify the NF modifications required to support our southbound API, we counted the lines of code (LOC) that we added to each NF (Table 2). The counts do not include the shared library used with each NF for communication with the controller: ≈2.6K LOC. At most, there is a 9.8% increase in LOC, most of which is state serialization code that could be automatically generated [3]. Thus, the NF changes required to support OpenNF are minimal.

| NF | Total LOC Added | Serialization LOC Added | Increase in NF Code |
|----|-----------------|-------------------------|---------------------|
| Bro IDS | 3.3K | 1.0K | 4.0% |
| PRADS asset monitor | 7.8K | 1.0K | 9.8% |
| Squid caching proxy | 2.9K | 0.1K | 4.2% |
| iptables | 5.0K | 0.6K | n/a |

**Table 2: Additional NF code to implement OpenNF’s southbound API**

### 8.3 Controller Scalability

Since the controller executes all northbound operations (§5), its ability to scale is crucial. We thus measure the performance impact of conducting simultaneous operations across many pairs of NFs. To isolate the controller from the performance of individual NFs, we use “dummy” NFs that replay traces of past state in response to `getPerflow`, simply consume state for `putPerflow`, and infinitely generate events during the lifetime of the experiment. The traces we use are derived from actual state and events sent by the PRADS asset monitor while processing our cloud traffic trace. All state and messages are small (202 bytes and 128 bytes, respectively) for consistency, and to maximize the processing demand at the controller and minimize the impact due to network transfer.

Figure 13 shows the average time per loss-free move operation as a function of the number of simultaneous operations. The average time per operation increases linearly with both the number of simultaneous operations and the number of flows affected.

We profiled our controller using HPROF [8] and found that threads are busy reading from sockets most of the time. This bottleneck can be overcome by optimizing the size of state transfers using compression. We ran a simple experiment and observed that, for a move operation for 500 flows, state can be compressed by 38%, improving execution latency from 110ms to 70ms.

### 8.4 Prior NF Control Planes

Lastly, we compare the ability to satisfy the objectives of an elastic/load balanced network monitoring application using OpenNF versus existing approaches [5, 18, 22, 26, 32] (§2.2). We start with one Bro IDS instance (Bro1) and replay our data center traffic trace at a rate of 2500 packets/sec for 2 minutes. We then double the traffic rate, add a second Bro IDS instance (Bro2), and rebalance all HTTP flows to Bro2 (other flows remain at Bro1); 2 minutes later, we scale back down to one instance.

#### VM Replication

This approach takes a snapshot of the current state in an existing NF instance (Bro1) and copies it to a new instance (Bro2) as is. Since VM replication does not perform fine-grained state migration, we expect it to have unneeded states (§2.2) in all instances. We quantify unneeded state by comparing: a snapshot of a VM running the Bro IDS that has not yet received any traffic (base), a snapshot taken at the instant of scale up (full), and snapshots of VMs that have only received either HTTP or other traffic prior to scale up (HTTP and other). Base and full differed by 22MB. HTTP and other differed from base by 19MB and 4MB, respectively; these numbers indicate the overhead imposed by the unneeded state at the two Bro IDS instances. In contrast, the amount of state moved by OpenNF (i.e., per-flow and multi-flow state for all active HTTP flows) was 8.1MB. More crucial are the correctness implications of unneeded state: we found 3173 and 716 incorrect entries in `conn.log` at the two Bro IDS instances, arising because the migrated HTTP (other) flows terminate abruptly at Bro1 (Bro2).

#### Scaling Without Re-balancing Active Flows

Control planes that steer only new flows to new scaled-out NF instances leave existing flows to be handled by the same NF instance [22]. Thus, Bro1 continues to remain bottlenecked until some of the flows traversing it complete. Likewise, in the case of scale-in, NFs are unnecessarily "held up" as long as flows are active. We observe that ≈9% of the HTTP flows in our cloud trace were longer than 25 minutes; this requires us to wait for more than 25 minutes before we can safely terminate Bro2, otherwise, we may miss detecting some attacks.

### 9. Conclusion

Fully extracting the combined benefits of NFV and SDN requires a control plane to manage both network forwarding state and internal NF state. Without such joint control, applications will be forced to make trade-offs among key objectives. Providing such control is challenging because we must address race conditions and accommodate a variety of application objectives and NF types. We presented a novel control plane architecture called OpenNF that addresses these challenges through careful API design informed by the ways NFs internally manage state today, and clever techniques that ensure lock-step coordination of updates to NF and network state. A thorough evaluation of OpenNF shows that: its joint control is generally efficient even when applications have certain stringent requirements; OpenNF allows applications to make suitable choices in meeting their objectives; and NFs need modest changes and incur minimal overhead when supporting OpenNF primitives.

### 10. Acknowledgements

We would like to thank Vivek Pai (our shepherd), Katerina Argyraki, Tom Anderson, David Cheriton, Vimalkumar Jeyakumar, Arvind Krishnamurthy, Ratul Mahajan, Jennifer Rexford, and the anonymous reviewers for their insightful feedback. This work is supported in part by a Wisconsin Alumni Research Foundation (WARF) Accelerator Award and National Science Foundation grants CNS-1302041, CNS-1314363, and CNS-1040757. Aaron Gember-Jacobson is supported by an IBM PhD Fellowship.

### 11. References

[1] Balance. http://inlab.de/balance.html.
[2] Boost C++ libraries. http://boost.org.
[3] C++ Middleware Writer. http://webebenezer.net.
[4] Check Point Software: ClusterXL. http://checkpoint.com/products/clusterxl.
[5] CRIU: Checkpoint/Restore In Userspace. http://criu.org.
[6] Floodlight OpenFlow Controller. http://floodlight.openflowhub.org.
[7] HAProxy: The reliable, high-performance TCP/HTTP load balancer. http://haproxy.1wt.eu/.
[8] HPROF. http://docs.oracle.com/javase/7/docs/technotes/samples/hprof.html.
[9] iptables. http://netfilter.org/projects/iptables.
[10] libnetfilter_conntrack project. http://netfilter.org/projects/libnetfilter_conntrack.
[11] nDPI. http://ntop.org/products/ndpi.
[12] Network functions virtualisation: Introductory white paper. http://www.tid.es/es/Documents/NFV_White_PaperV2.pdf.
[13] Passive Real-time Asset Detection System. http://prads.projects.linpro.no.
[14] RiverBed Steelhead Load Balancing. http://riverbed.com/products-solutions/products/wan-optimization-steelhead/wan-optimization-management.
[15] Squid. http://squid-cache.org.
[16] A. Anand, V. Sekar, and A. Akella. SmartRE: An architecture for coordinated network-wide redundancy elimination. In SIGCOMM, 2009.
[17] B. Anwer, T. Benson, N. Feamster, D. Levin, and J. Rexford. A slick control plane for network middleboxes. In HotSDN, 2013.
[18] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warfield. Xen and the art of virtualization. In SOSP, 2003.
[19] T. Benson, A. Akella, and D. Maltz. Network Traffic Characteristics of Data Centers in the Wild. In IMC, 2010.
[20] S. K. Fayazbakhsh, L. Chaing, V. Sekar, M. Yu, and J. C. Mogul. Enforcing network-wide policies in the presence of dynamic middlebox actions using FlowTags. In NSDI, 2014.
[21] A. Gember, R. Grandl, A. Anand, T. Benson, and A. Akella. Stratos: Virtual Middleboxes as First-Class Entities. Technical Report TR1771, University of Wisconsin-Madison, 2012.
[22] A. Gember, A. Krishnamurthy, S. St. John, R. Grandl, X. Gao, A. Anand, T. Benson, A. Akella, and V. Sekar. Stratos: A network-aware orchestration layer for middleboxes in the cloud. Technical Report arXiv:1305.0209, 2013.
[23] A. Gember, R. Viswanathan, C. Prakash, R. Grandl, J. Khalid, S. Das, and A. Akella. OpenNF: Enabling innovation in network function control. Technical report, University of Wisconsin-Madison, 2014.
[24] K. He, L. Wang, A. Fisher, A. Gember, A. Akella, and T. Ristenpart. Next stop, the cloud: Understanding modern web service deployment in EC2 and Azure. In IMC, 2013.
[25] D. Joseph and I. Stoica. Modeling middleboxes. IEEE Network, 2008.
[26] D. A. Joseph, A. Tavakoli, and I. Stoica. A policy-aware switching layer for data centers. In SIGCOMM, 2008.
[27] R. Mahajan and R. Wattenhofer. On consistent updates in software-defined networks. In HotNets, 2013.
[28] J. Martins, M. Ahmed, C. Raiciu, V. Olteanu, M. Honda, R. Bifulco, and F. Huici. ClickOS and the art of network function virtualization. In NSDI, 2014.
[29] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, and J. Turner. OpenFlow: Enabling innovation in campus networks. ACM SIGCOMM CCR, 38(2), 2008.
[30] C. Nicutar, C. Paasch, M. Bagnulo, and C. Raiciu. Evolving the internet with connection acrobatics. In HotMiddlebox, 2013.
[31] V. Paxson. Bro: a system for detecting network intruders in real-time. In USENIX Security (SSYM), 1998.
[32] Z. A. Qazi, C.-C. Tu, L. Chiang, R. Miao, V. Sekar, and M. Yu. SIMPLE-fying middlebox policy enforcement using SDN. In SIGCOMM, 2013.
[33] S. Rajagopalan, D. Williams, and H. Jamjoom. Pico Replication: A high availability framework for middleboxes. In SoCC, 2013.
[34] S. Rajagopalan, D. Williams, H. Jamjoom, and A. Warfield. Split/Merge: System support for elastic execution in virtual middleboxes. In NSDI, 2013.
[35] M. Reitblatt, N. Foster, J. Rexford, C. Schlesinger, and D. Walker. Abstractions for network update. In SIGCOMM, 2012.
[36] M. Z. Shafiq, L. Ji, A. X. Liu, J. Pang, and J. Wang. A first look at cellular machine-to-machine traffic: Large-scale measurement and characterization. In SIGMETRICS, 2012.
[37] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratnasamy, and V. Sekar. Making middleboxes someone else’s problem: Network processing as a cloud service. In SIGCOMM, 2012.
[38] R. Wang, D. Butnariu, and J. Rexford. OpenFlow-based server load balancing gone wild. In Hot-ICE, 2011.