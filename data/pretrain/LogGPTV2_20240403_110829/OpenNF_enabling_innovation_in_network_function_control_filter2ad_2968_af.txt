unique objects were cached before the copy. Ignoring multi-ﬂow
state entirely causes the second instance to crash, as the objects
currently being served to the second client are not available. Copy-
ing multi-ﬂow state for the second client’s ﬂows avoids the crash,
but skipping the other multi-ﬂow state results in a 28% lower cache
hit ratio at Squid2 compared to copying all multi-ﬂow state (i.e,
the entire cache). However, the latter requires a 14.2x larger state
transfer. OpenNF’s APIs allows each application to make the ap-
propriate trade-offs in such respects when selecting the granularity
at which to invoke operations.
8.2 Southbound API
The time required to export and import state at NFs directly im-
pacts how quickly a move or copy operation completes and how
much additional packet latency is incurred when share is used.
We thus evaluate the efﬁciency of OpenNF’s southbound opera-
tions for several of the NFs we modiﬁed. We also examine how
much code was added to the NFs to support these operations.
8.2.1 API Call Processing
Figures 12(a) and 12(b) show the time required to complete a
getPerflow and putPerflow operation, respectively, as a func-
tion of the number of ﬂows whose state is exported/imported. We
observe a linear increase in the execution time of getPerflow
and putPerflow as the number of per-ﬂow state chunks increases.
The time required to (de)serialize each chunk of state and send it
to (receive it from) the controller accounts for the majority of the
execution time. Additionally, we observe that putPerflow com-
pletes at least 2x faster than getPerflow; this is due to deserial-
ization being faster than serialization. Overall, the processing time
is highest for Bro because of the size and complexity of the per-
ﬂow state. The results for multi-ﬂow state are qualitatively similar;
we exclude them for brevity. We are working on techniques for
further improving the efﬁciency of southbound API calls.
We also evaluate how NF performance is impacted by the ex-
ecution of southbound operations. In particular, we measure aver-
age per-packet processing latency (including queueing time) during
normal NF operation and when an NF is executing a getPerflow
call. Among the NFs, the PRADS asset monitor has the largest rel-
ative increase—5.8% (0.120ms vs. 0.127ms), while the Bro IDS
NF
Bro IDS
PRADS asset monitor
Squid caching proxy
iptables
LOC added for
Total
serialization LOC added
3.3K
1.0K
7.8K
1.0K
2.9K
0.1K
5.0K
0.6K
Increase in
NF code
4.0%
9.8%
4.2%
n/a
Table 2: Additional NF code to implement OpenNF’s southbound API
1000 flows
2000 flows
3000 flows
e
m
i
t
e
g
a
r
e
v
A
)
s
m
(
e
v
o
m
r
e
p
 1500
 1250
 1000
 750
 500
 250
 0
 0
 4
 8
 12  16  20
Number of simultaneous moves
Figure 13: Performance of concurrent loss-free move operations
has the largest absolute increase—0.12ms (6.93ms vs. 7.06ms). In
both cases, the impact is minimal, implying that southbound oper-
ations do not signiﬁcantly degrade NF performance.
8.2.2 NF Changes
To quantify the NF modiﬁcations required to support our south-
bound API, we counted the lines of code (LOC) that we added to
each NF (Table 2). The counts do not include the shared library
used with each NF for communication with the controller: ≈2.6K
LOC. At most, there is a 9.8% increase in LOC11, most of which is
state serialization code that could be automatically generated [3].
Thus, the NF changes required to support OpenNF are minimal.
8.3 Controller Scalability
Since the controller executes all northbound operations (§5), its
ability to scale is crucial. We thus measure the performance impact
of conducting simultaneous operations across many pairs of NFs.
To isolate the controller from the performance of individual NFs,
we use “dummy” NFs that replay traces of past state in response
to getPerflow, simply consume state for putPerflow, and
inﬁnitely generate events during the lifetime of the experiment.
The traces we use are derived from actual state and events sent
by PRADS asset monitor while processing our cloud trafﬁc trace.
All state and messages are small (202 bytes and 128 bytes, respec-
tively) for consistency, and to maximize the processing demand at
the controller and minimize the impact due to network transfer.
Figure 13 shows the average time per loss-free move operation
as a function of the number of simultaneous operations. The aver-
age time per operation increases linearly with both the number of
simultaneous operations and the number of ﬂows affected.
We proﬁled our controller using HPROF [8] and found that threads
are busy reading from sockets most of the time. This bottleneck
can be overcome by optimizing the size of state transfers using
compression. We ran a simple experiment and observed that, for
a move operation for 500 ﬂows, state can be compressed by 38%
improving execution latency from 110ms to 70ms.
8.4 Prior NF Control Planes
Lastly, we compare the ability to satisfy the objectives of an elas-
tic/load balanced network monitoring application using OpenNF
versus existing approaches [5, 18, 22, 26, 32] (§2.2). We start with
one Bro IDS instance (Bro1) and replay our data center trafﬁc trace
11We do not calculate an increase for iptables because we wrote
a user-level tool to export/import state rather than modifying the
Linux kernel.
173at a rate of 2500 packets/sec for 2 minutes. We then double the traf-
ﬁc rate, add a second Bro IDS instance (Bro2), and rebalance all
HTTP ﬂows to Bro2 (other ﬂows remain at Bro1); 2 minutes later
we scale back down to one instance.
VM Replication. This approach takes a snapshot of the current
state in an existing NF instance (Bro1) and copies it to a new in-
stance (Bro2) as is. Since, VM replication does not do ﬁne-grained
state migration, we expect it to have unneeded states (§2.2) in all
instances. We quantify unneeded state by comparing: a snapshot
of a VM running the Bro IDS that has not yet received any traf-
ﬁc (base), a snapshot taken at the instant of scale up (full), and
snapshots of VMs that have only received either HTTP or other
trafﬁc prior to scale up (HTTP and other). Base and full differed by
22MB. HTTP and other differed from base by 19MB and 4MB, re-
spectively; these numbers indicate the overhead imposed by the un-
needed state at the two Bro IDS instances. In contrast, the amount
of state moved by OpenNF (i.e., per-ﬂow and multi-ﬂow state for
all active HTTP ﬂows) was 8.1MB. More crucial are the correctness
implications of unneeded state: we found 3173 and 716 incorrect
entries in conn.log at the two Bro IDS instances, arising because the
migrated HTTP (other) ﬂows terminate abruptly at Bro1 (Bro2).
Scaling Without Re-balancing Active Flows. Control planes that
steer only new ﬂows to new scaled out NF instances leave existing
ﬂows to be handled by the same NF instance [22]. Thus, Bro1
continues to remain bottlenecked until some of the ﬂows traversing
it complete. Likewise, in the case of scale in, NFs are unnecessarily
“held up” as long as ﬂows are active. We observe that ≈9% of the
HTTP ﬂows in our cloud trace were longer than 25 minutes; this
requires us to wait for more than 25 minutes before we can safely
terminate Bro2, otherwise we may miss detecting some attacks.
9. CONCLUSION
Fully extracting the combined beneﬁts of NFV and SDN requires
a control plane to manage both network forwarding state and inter-
nal NF state. Without such joint control, applications will be forced
to make trade-offs among key objectives. Providing such control is
challenging because we must address race conditions and accom-
modate a variety of application objectives and NF types. We pre-
sented a novel control plane architecture called OpenNF that ad-
dresses these challenges through careful API design informed by
the ways NFs internally manage state today, and clever techniques
that ensure lock-step coordination of updates to NF and network
state. A thorough evaluation of OpenNF shows that: its joint con-
trol is generally efﬁcient even when applications have certain strin-
gent requirements; OpenNF allows applications to make suitable
choices in meeting their objectives; and NFs need modest changes
and incur minimal overhead when supporting OpenNF primitives.
10. ACKNOWLEDGEMENTS
We would like to thank Vivek Pai (our shepherd), Katerina Ar-
gyraki, Tom Anderson, David Cheriton, Vimalkumar Jeyakumar,
Arvind Krishnamurthy, Ratul Mahajan, Jennifer Rexford, and the
anonymous reviewers for their insightful feedback. This work is
supported in part by a Wisconsin Alumni Research Foundation
(WARF) Accelerator Award and National Science Foundation grants
CNS-1302041, CNS-1314363 and CNS-1040757. Aaron Gember-
Jacobson is supported by an IBM PhD Fellowship.
11. REFERENCES
[1] Balance. http://inlab.de/balance.html.
[2] Boost C++ libraries. http://boost.org.
[3] C++ Middleware Writer. http://webebenezer.net.
[4] Check Point Software: ClusterXL.
http://checkpoint.com/products/clusterxl.
[5] CRIU: Checkpoint/Restore In Userspace. http://criu.org.
[6] Floodlight OpenFlow Controller.
http://floodlight.openflowhub.org.
[7] HAProxy: The reliable, high performance TCP/HTTP load balancer.
http://haproxy.1wt.eu/.
[8] HPROF. http://docs.oracle.com/javase/7/docs/technotes/
samples/hprof.html.
[9] iptables. http://netfilter.org/projects/iptables.
[10] libnetﬁlter_conntrack project.
http://netfilter.org/projects/libnetfilter_conntrack.
[11] nDPI. http://ntop.org/products/ndpi.
[12] Network functions virtualisation: Introductory white paper.
http://www.tid.es/es/Documents/NFV_White_PaperV2.pdf.
[13] Passive Real-time Asset Detection System.
http://prads.projects.linpro.no.
[14] RiverBed Steelhead Load Balancing.
http://riverbed.com/products-solutions/products/wan-
optimization-steelhead/wan-optimization-management.
[15] Squid. http://squid-cache.org.
[16] A. Anand, V. Sekar, and A. Akella. SmartRE: An architecture for coordinated
network-wide redundancy elimination. In SIGCOMM, 2009.
[17] B. Anwer, T. Benson, N. Feamster, D. Levin, and J. Rexford. A slick control
plane for network middleboxes. In HotSDN, 2013.
[18] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer,
I. Pratt, and A. Warﬁeld. Xen and the art of virtualization. In SOSP, 2003.
[19] T. Benson, A. Akella, and D. Maltz. Network Trafﬁc Characteristics of Data
Centers in the Wild. In IMC, 2010.
[20] S. K. Fayazbakhsh, L. Chaing, V. Sekar, M. Yu, and J. C. Mogul. Enforcing
network-wide policies in the presence of dynamic middlebox actions using
FlowTags. In NSDI, 2014.
[21] A. Gember, R. Grandl, A. Anand, T. Benson, and A. Akella. Stratos: Virtual
Middleboxes as First-Class Entities. Technical Report TR1771, University of
Wisconsin-Madison, 2012.
[22] A. Gember, A. Krishnamurthy, S. St. John, R. Grandl, X. Gao, A. Anand,
T. Benson, A. Akella, and V. Sekar. Stratos: A network-aware orchestration
layer for middleboxes in the cloud. Technical Report arXiv:1305.0209, 2013.
[23] A. Gember, R. Viswanathan, C. Prakash, R. Grandl, J. Khalid, S. Das, and
A. Akella. OpenNF: Enabling innovation in network function control. Technical
report, University of Wisconsin-Madison, 2014.
[24] K. He, L. Wang, A. Fisher, A. Gember, A. Akella, and T. Ristenpart. Next stop,
the cloud: Understanding modern web service deployment in EC2 and Azure.
In IMC, 2013.
[25] D. Joseph and I. Stoica. Modeling middleboxes. IEEE Network, 2008.
[26] D. A. Joseph, A. Tavakoli, and I. Stoica. A policy-aware switching layer for
data centers. In SIGCOMM, 2008.
[27] R. Mahajan and R. Wattenhofer. On consistent updates in software deﬁned
networks. In HotNets, 2013.
[28] J. Martins, M. Ahmed, C. Raiciu, V. Olteanu, M. Honda, R. Bifulco, and
F. Huici. ClickOS and the art of network function virtualization. In NSDI, 2014.
[29] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner. OpenFlow: Enabling innovation in
campus networks. ACM SIGCOMM CCR, 38(2), 2008.
[30] C. Nicutar, C. Paasch, M. Bagnulo, and C. Raiciu. Evolving the internet with
connection acrobatics. In HotMiddlebox, 2013.
[31] V. Paxson. Bro: a system for detecting network intruders in real-time. In
USENIX Security (SSYM), 1998.
[32] Z. A. Qazi, C.-C. Tu, L. Chiang, R. Miao, V. Sekar, and M. Yu. SIMPLE-fying
middlebox policy enforcement using SDN. In SIGCOMM, 2013.
[33] S. Rajagopalan, D. Williams, and H. Jamjoom. Pico Replication: A high
availability framework for middleboxes. In SoCC, 2013.
[34] S. Rajagopalan, D. Williams, H. Jamjoom, and A. Warﬁeld. Split/Merge:
System support for elastic execution in virtual middleboxes. In NSDI, 2013.
[35] M. Reitblatt, N. Foster, J. Rexford, C. Schlesinger, and D. Walker. Abstractions
for network update. In SIGCOMM, 2012.
[36] M. Z. Shaﬁq, L. Ji, A. X. Liu, J. Pang, and J. Wang. A ﬁrst look at cellular
machine-to-machine trafﬁc: Large scale measurement and characterization. In
SIGMETRICS, 2012.
[37] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratnasamy, and V. Sekar.
Making middleboxes someone else’s problem: Network processing as a cloud
service. In SIGCOMM, 2012.
[38] R. Wang, D. Butnariu, and J. Rexford. OpenFlow-based server load balancing
gone wild. In Hot-ICE, 2011.
174