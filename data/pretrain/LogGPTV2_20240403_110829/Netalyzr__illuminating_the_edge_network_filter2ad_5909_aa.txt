title:Netalyzr: illuminating the edge network
author:Christian Kreibich and
Nicholas Weaver and
Boris Nechaev and
Vern Paxson
Netalyzr: Illuminating The Edge Network
Berkeley, CA, 94704, USA
ICSI
Christian Kreibich
1947 Center Street
PI:EMAIL
Boris Nechaev
PO Box 19800
HIIT & Aalto University
00076 Aalto, Finland
boris.nechaev@hiit.ﬁ
Nicholas Weaver
1947 Center Street
ICSI
Berkeley, CA, 94704, USA
PI:EMAIL
Vern Paxson
ICSI & UC Berkeley
1947 Center Street
Berkeley, CA, 94704, USA
PI:EMAIL
ABSTRACT
In this paper we present Netalyzr, a network measurement and de-
bugging service that evaluates the functionality provided by peo-
ple’s Internet connectivity. The design aims to prove both compre-
hensive in terms of the properties we measure and easy to employ
and understand for users with little technical background. We struc-
ture Netalyzr as a signed Java applet (which users access via their
Web browser) that communicates with a suite of measurement-
speciﬁc servers. Trafﬁc between the two then probes for a diverse
set of network properties, including outbound port ﬁltering, hid-
den in-network HTTP caches, DNS manipulations, NAT behavior,
path MTU issues, IPv6 support, and access-modem buffer capacity.
In addition to reporting results to the user, Netalyzr also forms the
foundation for an extensive measurement of edge-network prop-
erties. To this end, along with describing Netalyzr’s architecture
and system implementation, we present a detailed study of 130,000
measurement sessions that the service has recorded since we made
it publicly available in June 2009.
Categories and Subject Descriptors
C.4 [Performance of Systems]: MEASUREMENT TECH-
NIQUES
General Terms
Measurement, Performance, Reliability, Security
Keywords
Network troubleshooting, network performance, network measure-
ment, network neutrality
1.
INTRODUCTION
For most Internet users, their network experience—perceived
service availability, connectivity constraints, responsiveness, and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
reliability—is largely determined by the conﬁguration and man-
agement of their edge network, i.e., the speciﬁcs of what their Inter-
net Service Provider (ISP) gives them in terms of Internet access.
While conceptually we often think of users receiving a straight-
forward “bit pipe” service that transports trafﬁc transparently, in
reality a myriad of factors affect the fate of their trafﬁc.
It then comes as no surprise that this proliferation of complexity
constantly leads to troubleshooting headaches for novice users and
technical experts alike, leaving providers of web-based services un-
certain regarding what caliber of connectivity their clients possess.
Only a few tools exist to analyze even speciﬁc facets of these prob-
lems, and fewer still that people with limited technical understand-
ing of the Internet will ﬁnd usable. Similarly, the lack of such tools
has resulted in the literature containing few measurement studies
that characterize in a comprehensive fashion the prevalence and na-
ture of such problems in the Internet.
In this work we seek to close this gap. We present the design,
implementation, and evaluation of Netalyzr,1 a publicly available
service that lets any Internet user obtain a detailed analysis of the
operational envelope of their Internet connectivity, serving both as
a source of information for the curious as well as an extensive trou-
bleshooting diagnostic should users ﬁnd anything amiss with their
network experience. Netalyzr tests a wide array of properties of
users’ Internet access, starting at the network layer, including IP
address use and translation, IPv6 support, DNS resolver ﬁdelity and
security, TCP/UDP service reachability, proxying and ﬁrewalling,
antivirus intervention, content-based download restrictions, content
manipulation, HTTP caching prevalence and correctness, latencies,
and access-link buffering.
We believe the breadth and depth of analysis Netalyzr provides is
unique among tools available for such measurement. In addition, as
of this writing we have recorded 130,000 runs of the system from
99,000 different public IP addresses, allowing us both to construct a
large-scale picture of many facets of Internet edge behavior as well
as to track this behavior’s technological evolution over time. The
measurements have found a wide range of behavior, on occasion
even revealing trafﬁc manipulation unknown to the network oper-
ators themselves. More broadly, we ﬁnd chronic over-buffering
of links, a signiﬁcant inability to handle fragmentation, numerous
incorrectly operating HTTP caches, common NXDOMAIN wild-
carding, impediments to DNSSEC deployment, poor DNS perfor-
mance, and deliberate manipulation of DNS results.
1http://netalyzr.icsi.berkeley.edu
246We begin by presenting Netalyzr’s architecture and implementa-
tion (§ 2) and the speciﬁcs of the different types of measurements it
conducts (§ 3). We have been operating Netalyzr publicly and con-
tinuously since June 2009, and in § 4 report on the resulting data
collection, including ﬂash crowds, their resulting measurement bi-
ases, and our extensive calibration tests to assess the correct oper-
ation of Netalyzr’s test suite. In § 5 we present a detailed analysis
of the resulting dataset and some consequences of our ﬁndings. We
defer our main discussion of related work to § 6 in order to have
the context of the details of our measurement analysis to compare
against. § 7 discusses our plans for future tests and development.
Finally, we summarize in § 8.
2. SYSTEM DESIGN
When designing Netalyzr we had to strike a balance between a
tool with sufﬁcient ﬂexibility to conduct a wide range of measure-
ment tests, yet with a simple enough interface that unsophisticated
users would run it—giving us access to a much larger (and less bi-
ased towards “techies”) end-system population than possible if the
measurements required the user to install privileged software. To
this end, we decided to base our approach on using a Java applet
(≈ 5,000 lines of code) to drive the bulk of the test communication
with our servers (≈ 12,000 lines of code), since (i) Java applets
run automatically within most major web browsers, (ii) applets can
engage in raw TCP and UDP ﬂows to arbitrary ports (though not
with altered IP headers), and, if the user approves trusting the ap-
plet, contact hosts outside the same-origin policy, (iii) Java applets
come with intrinsic security guarantees for users (e.g., no host-level
ﬁle system access allowed by default runtime policies), (iv) Java’s
ﬁne-grained permissions model allows us to adapt gracefully if a
user declines to fully trust our applet, and (v) no alternative technol-
ogy matches this level of functionality, security, and convenience.
Figure 1 shows the conceptual Netalyzr architecture, whose com-
ponents we now discuss in turn.
Application Flow. Users initiate a test session by visiting the
Netalyzr website and clicking Start Analysis on the webpage with
the embedded Java test applet. Once loaded, the applet conducts
a large set of measurement probes, indicating test progress to the
user. When testing completes, the applet redirects to a summary
page that shows the results of the tests in detail and with explana-
tions (Figure 2). The users can later revisit a session’s results via a
permanent link associated with each session. We also save the ses-
sion state (and server-side packet traces) for subsequent analysis.
Front- and Back-end Hosts. The Netalyzr system involves
three distinct locations: (i) the user’s machine running the test ap-
plet in a browser, (ii) the front-end machine responsible for dis-
patching users and providing DNS service, and (iii) multiple back-
end machines that each hosts both a copy of the applet and a full
set of test servers. All back-end machines run identical conﬁg-
urations and Netalyzr conducts all tests in a given client’s ses-
sion using the same back-end machine. We use Amazon’s EC2
service (http://aws.amazon.com/ec2/) to facilitate scalabil-
ity, employing 20 back-end hosts during times of peak load. Given
a conservative, hard-wired maximum number of 12 parallel ses-
sions per minute, this allows Netalyzr to serve up to 240 sessions
per minute.2
Front-end Web Server. Running on the front-end machine,
this server provides the main website, including a landing/dispatch
page, documentation, FAQs, an example report, and access to re-
2We limited each node to conducting 12 sessions per minute to
prevent the UDP-based network bandwidth/buffer stress test from
interfering with other tests.
Figure 1: Netalyzr’s conceptual architecture. ❶ The user vis-
its the Netalyzr website. ❷ When starting the test, the front-
end redirects the session to a randomly selected back-end node.
❸ The browser downloads and executes the applet. ❹ The ap-
plet conducts test connections to various Netalyzr servers on
the back-end, as well as DNS requests which are eventually re-
ceived by the main Netalyzr DNS server on the front-end. ❺ We
store the test results and raw network trafﬁc for later analysis.
❻ Netalyzr presents a summary of the test results to the user.
ports from previous sessions. The front page also includes an applet
that ensures that the user has Java installed and then directs the user
to a randomly selected back-end server to load-balance the actual
testing process. Finally, the front page rate-limits visitors to a ﬁxed
number of measurements per minute per back-end server.
Back-end Web Servers. The back-end web servers host the ac-
tual measurement applet (so that its probe connections to the server
accord with the same-origin policy) and perform HTTP testing and
overall session management. When sending the measurement ap-
plet, the server includes a set of conﬁguration parameters, including
a globally unique session ID.
Measurement Applet. The Java applet implements 38 types of
tests, some with a number of subtests. We describe them in detail in
Section 3. The applet conducts the test cases sequentially, but also
employs multithreading to ensure that test sessions cannot stall the
entire process, and to speed up some parallelizable tasks. As tests
complete, the applet transmits detailed test results to the back-end
server; it also sends a continuously recorded client-side transcript
of the session. Note that we sign our applet with a certiﬁcate from
a trusted authority so that browsers indicate a valid signature.
DNS Servers.
On the front-end,
These run on the front-end as well
it acts
as
the back-end machines.
as the authoritative resolver
the two subdomains em-
ployed by Netalyzr, netalyzr.icsi.berkeley.edu and
netalyzr.icir.org. (In the following, we abbreviate these
to netalyzr.edu and netalyzr.org, respectively.) On the
back-ends, the server receives DNS test queries generated directly
from the applet rather than through the user’s DNS resolver li-
brary. The server interprets queries for speciﬁc names as com-
mands, generating replies that encode values in A and CNAME
records. For example, requesting has_edns.netalyzr.edu
will return an A record reﬂecting whether the query message in-
dicated EDNS support. The server also accepts names with arbi-
trary interior padding to act as a cache-busting nonce, ensuring that
queries reach our server.
Echo Servers. An array of simple TCP and UDP echo servers
allow us to test service-level reachability and content modiﬁca-
for
(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:8)(cid:9)(cid:4)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:10)(cid:11)(cid:12)(cid:13)(cid:5)(cid:14)(cid:15)(cid:16)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:17)(cid:2)(cid:18)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:19)(cid:15)(cid:20)(cid:21)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:10)(cid:4)(cid:13)(cid:21)(cid:6)(cid:12)(cid:22)(cid:5)(cid:23)(cid:6)(cid:21)(cid:14)(cid:13)(cid:24)(cid:5)(cid:14)(cid:25)(cid:26)(cid:12)(cid:15)(cid:27)(cid:24)(cid:5)(cid:14)(cid:25)(cid:10)(cid:28)(cid:29)(cid:30)(cid:31)(cid:32)(cid:33)(cid:34)(cid:35)(cid:36)(cid:28)(cid:37)(cid:28)(cid:28)(cid:28)(cid:29)(cid:28)(cid:30)(cid:28)(cid:31)(cid:28)(cid:32)(cid:28)(cid:33)(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:8)(cid:9)(cid:4)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:4)(cid:13)(cid:21)(cid:6)(cid:12)(cid:22)(cid:5)(cid:1)(cid:2)(cid:3)(cid:4)(cid:28)(cid:37)(cid:5)(cid:6)2473. MEASUREMENTS CONDUCTED
We now describe the types of measurements Netalyzr conducts
and the particular methodology used, beginning with layer-3 mea-
surements and then progressing to higher layers, and obtaining user
feedback.
3.1 Network-layer Information
Addressing. We obtain the client’s local IP address via the Java
API, and use a set of raw TCP connections and UDP ﬂows to our
echo servers to learn the client’s public address. From this set of
connections we can identify the presence of NAT, and if so how
it renumbers addresses and ports. If across multiple ﬂows we ob-
serve more than one public address, then we assess whether the
address ﬂipped from one to another—indicating the client changed
networks while the test was in progress—or alternates back and
forth. This latter implies either the use of load-balancing, or that
the NAT does not attempt to associate local systems with a sin-
gle consistent public address but simply assigns new ﬂows out of a
public address block as convenient. (Only 1% of sessions included
an address change from any source.)
IP Fragmentation. We test for proper support of IP fragmenta-
tion (and also for MTU measurement; see below) by sending UDP
payloads to our test servers. We ﬁrst check for the ability to send
and receive fragmented UDP datagrams. In the applet → server
direction, we send a 2 KB datagram which, if received, generates
a small conﬁrmation response. Due to the prevalence of Ethernet
framing, we would expect most clients to send this packet in frag-
ments, but it will always be fragmented by the time it reaches the
server. We likewise test the server → applet direction by our server
transmitting (in response to a small query from the client) a 2 KB
message to the client. This direction will deﬁnitely fragment, as the
back-end nodes have an interface MTU of 1500 bytes.
If either of the directional tests fails, the applet performs binary
search to ﬁnd the maximum packet size that it can successfully
send/receive unfragmented.
Path MTU. A related set of tests conducts path MTU probing.
The back-end server for this test supports two modes, one for each
direction. In the applet → server direction, the applet sends a large
UDP datagram, resulting in fragmentation. The server monitors ar-
riving packets and reports the IP datagram size of the entire origi-
nal message (if received unfragmented) or of the original message’s
initial resulting fragment. This represents a lower bound on MTU
in the applet → server direction, since the ﬁrst fragment’s size is
not necessarily the full path MTU. (Such “runts” occurred in only a
handful of sessions). Additionally, the applet tests for a path MTU
hole in the applet → server direction by sending a 1499 B packet
using the default system parameters.
In the server → applet direction, the applet conducts a binary
search beginning with a request for 1500 bytes. The server re-
sponds by sending datagrams of the requested size with DF set. In
each iteration one of three cases occurs. First, if the applet receives
the DF-enabled response, its size is no more than the path MTU.
Second, if the response exceeds the path MTU, the server processes
any resulting ICMP “fragmentation required” messages and sends
to the applet the attempted message size, the offending location’s
IP address, and the next-hop MTU conveyed in the ICMP message.
Finally, if no messages arrive at the client, the applet infers that the
ICMP “fragmentation required” message was not generated or did
not reach the server, and thus a path MTU problem exists.
Latency, Bandwidth, and Buffering. We measure packet deliv-
ery performance in terms of round-trip latencies, directional band-
width limits, and buffer sizing. With these, our primary goal is not
to measure capacity itself (which numerous test sites already ad-
Figure 2: A partial screen capture of Netalyzr’s results page as
seen by the user upon completion of all tests. The full report
is 4–10 times this size, depending on whether the user expands
the different sections.
tion of trafﬁc on various ports. The servers mostly run on well-
known ports but do not implement the associated application pro-
tocol. Rather, they use their own simple payload schema to convey
timing, sequencing, and the requester’s IP address and source port
back to the client. An additional server can direct a DNS request to
the user’s public address to check if the user’s NAT or gateway acts
as a proxy for external DNS requests.
Bandwidth Measurement Servers. To assess bandwidth, la-
tency, buffer sizing, and packet dynamics (loss, reordering, duplica-
tion), we employ dedicated UDP-based measurement servers. Like
the echo servers, these use a custom payload schema that includes
timing information, sequence numbers, instructions regarding fu-
ture sending, and aggregate counters.
Path MTU Measurement Server. To measure directional path
MTUs, we use a server that can capture and transmit raw packets,
giving us full access to and control over all packet headers.
Storage. To maintain a complete record of server-side session
activity, we record all relevant network trafﬁc on the front- and
back-end machines, except for the relatively high-volume band-
width tests. Since Java applets do not have the ability to record
packets, we cannot record such traces on the client side.
Session Management. The back-end web servers establish and
maintain session state as test sessions progress, identifying sessions
via RFC 4122 UUIDs. We serialize completed session state to
disk on the back-end hosts and periodically archive it on the front-
end where it can still be accessed by the web browser. Thus, the
URL summarizing the results can be subsequently refetched when
desired, which enables third-party debugging where an individual
runs Netalyzr but others can interpret the results.3
3The “League of Legends” online game community regularly uses
Netalyzr in this way, as part of their Internet connection trou-
bleshooting instructions.
248dress), but as a means to measure the sizing of bottleneck buffers,
which can signiﬁcantly affect user-perceived latency. We do so
by measuring the increase in latency between quiescence and that
experienced during the bandwidth test, which in most cases will
brieﬂy saturate the path capacity in one direction and thus ﬁll the
buffer at the bottleneck.
Netalyzr conducts these measurements in two basic ways. First,
early in the measurement process it starts sending in the back-
ground small packets at a rate of 5 Hz. We use this test to detect
transient outages, such as those due to a poor wireless signal.
Second, it conducts an explicit latency and bandwidth test. The
test begins with a 10 Hz train of 200 small UDP packets, for which