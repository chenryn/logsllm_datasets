decay means packets sent more recently have a higher
inﬂuence on the count while bursty trafﬁc does not sig-
niﬁcantly affect scheduling priorities.
Connection Output. A cell
that has been chosen
and written to a connection output buffer (Figure 1i)
causes an activation of the write event registered with
libevent for that connection. Once libevent determines
the TCP socket can be written, the write callback is asyn-
chronously executed (Figure 1j). Similar to connection
input, the relay checks both the global write bucket and
per-connection write bucket for tokens.
If the buckets
are not empty, the connection is eligible for writing (Fig-
ure 1k) and again is allowed to write the smaller of 16
KiB and 1
8 of the global token bucket size per connection
(Figure 1l). The data is written to the kernel-level TCP
buffer (Figure 1m) and sent to the next hop.
3 Throttling Client Connections
Client performance in Tor depends heavily on the traf-
ﬁc patterns of others in the system. A small number of
clients performing bulk transfers in Tor are the source
of a large fraction of total network trafﬁc [38]. The
overwhelming load these clients place on the network
increases congestion and creates additional bottlenecks,
Figure 2: Throttling occurs at the connection between the
client and guard to capture all streams to various destinations.
causing interactive applications, such as instant messag-
ing and remote SSH sessions, to lose responsiveness.
This section explores client throttling as a mechanism
to prevent bulk clients from overwhelming the network.
Although a relay may have enough bandwidth to han-
dle all trafﬁc locally, bulk clients that continue producing
additional trafﬁc cause bottlenecks at other low-capacity
relays. The faster a bulk downloader gets its data, the
faster it will pull more into the network. Throttling bulk
and other high-trafﬁc clients prevents them from pushing
or pulling too much data into the network too fast, reduc-
ing these bottlenecks and improving performance for the
majority of users. Therefore, interactive applications and
Tor in general will become much more usable, attracting
new users who improve client diversity and anonymity.
We emphasize that throttling algorithms are not a re-
placement for congestion control or scheduling algo-
rithms, although each approach may cooperate to achieve
a common goal. Scheduling algorithms are used to man-
age the utilization of bandwidth, throttling algorithms re-
duce the aggregate network load, and congestion con-
trol algorithms attempt to do both. The distinction be-
tween congestion control and throttling algorithms is
subtle but important: congestion control reduces circuit
load while attempting to maximize network utilization,
whereas throttling reduces network load in an attempt to
improve circuit performance by explicitly under-utilizing
connections to bulk clients using too many resources.
Each approach may independently affect performance,
and they may be combined to improve the network.
3.1 Static Throttling
Recently, Tor introduced the functionality to allow entry
guards to throttle connections to clients [17] (see Fig-
ure 2). This client-to-guard connection is targeted be-
cause all client trafﬁc (using this guard) will ﬂow over
this connection regardless of the number of streams or
the destination associated with each.2 The implemen-
tation uses a token bucket for each connection in addi-
tion to the global token bucket that already limits the to-
tal amount of bandwidth used by a relay. The size of
the per-connection token buckets can be speciﬁed us-
ing the PerConnBWBurst conﬁguration option, and
the bucket reﬁll rate can be speciﬁed by conﬁguring the
PerConnBWRate. The conﬁgured throttling rate en-
2This work does not consider modiﬁed Tor clients.
4
sures that all client-to-guard connections are throttled
to the speciﬁed long-term-average throughput while the
conﬁgured burst allows deviations from the throttling
rate to account for bursty trafﬁc. The conﬁguration op-
tions provide a static throttling mechanism: Tor will
throttle all connections using these values until directed
otherwise. Note that Tor does not enable or conﬁgure
static throttling by default.
While static throttling is simple, it has two main draw-
backs. First, static throttling requires constant monitor-
ing and measurements of the Tor network to determine
which conﬁgurations work well and which do not in or-
der to be effective. We have found that there are many
conﬁgurations of the algorithm that cause no change in
performance, and worse, there are conﬁgurations that
harm performance for interactive applications [33]. This
is the opposite of what throttling is attempting to achieve.
Second, it is not possible under the current algorithm
to auto-tune the throttling parameters for each Tor relay.
Conﬁgurations that appear to work well for the network
as a whole might not necessarily be tuned for a given
relay (we will show that this is indeed the case in Sec-
tion 4). Each relay has very different capabilities and
load patterns, and therefore may require different throt-
tling conﬁgurations to be most useful.
3.2 Adaptive Throttling
Given the drawbacks of static throttling, we now explore
and present three new algorithms that adaptively adjust
throttling parameters according to local relay informa-
tion. This section details our algorithms while Section 4
explores their effect on client performance and Section 5
analyzes throttling implications for anonymity.
There are two main issues to consider when design-
ing a client throttling algorithm: which connections to
throttle and at what rate to throttle them. The approach
discussed above in Section 3.1 throttles all client con-
nections at the statically speciﬁed rate. Each of our three
algorithms below answers these questions adaptively by
considering information local to each relay. Note that our
algorithms dynamically adjust the PerConnBWRate
while keeping a constant PerConnBWBurst.3
Bit-splitting. A simple approach to adaptive throttling
is to split a guard’s bandwidth equally among all active
client connections and throttle them all at this fair split
rate. The PerConnBWRate will therefore be adjusted
as new connections are created or old connections are
destroyed: more connections will result in lower rates.
No connection will be able to use more than its allot-
3Our experiments [33] indicate that a 2 MiB burst is ideal as it al-
lows directory requests to be downloaded unthrottled during bootstrap-
ping while also throttling bulk trafﬁc relatively quickly. The burst may
need to be increased if the directory information grows beyond 2 MiB.
Algorithm 1 Throttling clients by splitting bits.
1: B ← getRelayBandwidth()
2: L ← getConnectionList()
3: N ← L.length()
4: if N > 0 then
splitRate ← B
5:
N
for i ← 1 to N do
6:
7:
8:
9:
10:
11: end if
if L[i].isClientConnection() then
L[i].throttleRate ← splitRate
end if
end for
ted share of bandwidth unless it has unused tokens in its
bucket. Inspired by Quality of Service (QoS) work from
communication networks [11, 50, 60], bit-splitting will
prevent bulk clients from unfairly consuming bandwidth
and ensure that there is a minimum “reserved” bandwidth
for clients of all types.
Note that Internet Service Providers employ similar
techniques to throttle their customers, however, their
client base is much less dynamic than the connections an
entry guard handles. Therefore, our adaptive approach is
more suitable to Tor. We include this algorithm in our
analysis of throttling to determine what is possible with
such a simple approach.
Flagging Unfair Clients. The bit-splitting algorithm fo-
cuses on adjusting the throttle rate and applying this to
all client connections. Our next algorithm takes the op-
posite approach: conﬁgure a static throttling rate and ad-
just which connections get throttled. The intuition be-
hind this approach is that if we can properly identify the
connections that use too much bandwidth, we can throttle
them in order to maximize the beneﬁt we gain per throt-
tled connection. Therefore, our ﬂagging algorithm at-
tempts to classify and throttle bulk trafﬁc while it avoids
throttling web clients.
Since deep packet inspection is not desirable for pri-
vacy reasons, and is not possible on encrypted Tor trafﬁc,
we instead draw upon existing statistical ﬁngerprinting
classiﬁcation techniques [14, 29, 36] that classify trafﬁc
solely on its statistical properties. When designing the
ﬂagging algorithm, we recognize that Tor already con-
tains a statistical throughput measure for scheduling traf-
ﬁc on circuits using an exponentially-weighted moving
average (EWMA) of recently sent cells [52]. We can use
the same statistical measure on client connections to clas-
sify and throttle bulk trafﬁc.
The ﬂagging algorithm, shown in Algorithm 2, re-
quires that each guard keeps an EWMA of the number
of recently sent cells per client connection. The per-
connection cell EWMA is computed in much the same
way as the per-circuit cell EWMA: whenever the cir-
5
Algorithm 2 Throttling clients by ﬂagging bulk connections,
considering a moving average of throughput.
Require: f lagRate,P,H
1: B ← getRelayBandwidth()
2: L ← getConnectionList()
3: N ← L.length()
4: M ← getMetaEW MA()
5: if N > 0 then
6:
7: M ← M.increment(H,splitRate)
8:
9:
10:
11:
12:
13:
L[i]. f lag ← True
L[i].throttleRate ← f lagRate
L[i].EW MA  M then
L[i]. f lag ← False
L[i].throttleRate ← in f inity
14:
15:
16:
17:
18:
19: end if
end if
end if
end for
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end if
end if
end for
Algorithm 3 Throttling clients considering the loudest thresh-
old of connections.
Require: T ,R,F
1: L ← getClientConnectionList()
2: N ← L.length()
3: if N > 0 then
4:
5:
6:
selectIndex ← f loor(T · N)
L ← reverseSortEW MA(L)
thresholdRate ← L[selectIndex].
if thresholdRate < F then
getMeanT hroughput(R)
thresholdRate ← F
end if
for i ← 1 to N do
if i ≤ selectIndex then
L[i].throttleRate ← thresholdRate
L[i].throttleRate ← in f inity
else
cuit’s cell counter is incremented, so is the cell counter
of the connection to which that circuit belongs. Note
that clients can not affect others’ per-connection EWMA
since all of a client’s circuits are multiplexed over a
single throttled guard-to-client connection.4 The per-
connection EWMA is enabled and conﬁgured indepen-
dently of its circuit counterpart.
We rely on the observation that bulk connections will
have higher EWMA values than web connections since
bulk clients are steadily transferring data while web
clients “think” between each page download. Using this
to our advantage, we can ﬂag connections as containing
bulk trafﬁc as follows. Each relay keeps a single sepa-
rate meta-EWMA M of cells transferred. M is adjusted
by calculating the fair bandwidth split rate as in the bit-
splitting algorithm, and tracking its EWMA over time.
M does not correspond with any real trafﬁc, but rep-
resents the upper bound of a connection-level EWMA
if a connection were continuously sending only its fair
share of trafﬁc through the relay. Any connection whose
EWMA exceeds M is ﬂagged as containing bulk trafﬁc
and penalized by being throttled.
There are three main parameters for the algorithm. As
mentioned above, a per-connection half-life H allows
conﬁguration of the connection-level half-life indepen-
dent of that used for circuit scheduling. H affects how
4The same is not true for the unthrottled connections between relays
since each of them contain several circuits and each circuit may belong
to a different client (see Section 2).
6
long the algorithm remembers the amount of data a con-
nection has transferred, and has precisely the same mean-
ing as the circuit priority half-life [52]. Larger half-life
values increase the ability to differentiate bulk from web
connections while smaller half-life values make the algo-
rithm more immediately reactive to throttling bulk con-
nections. We would like to allow for a speciﬁcation of
the length of each penalty once a connection is ﬂagged
in order to recover and stop throttling connections that
may have been incorrectly ﬂagged. Therefore, we intro-
duce a penalty fraction parameter P that affects how long
each connection remains in a ﬂagged and throttled state.
If a connection’s cell count EWMA falls below P ·M,
its ﬂag is removed and the connection is no longer throt-
tled. Finally, the rate at which each ﬂagged connection is
throttled, i.e. the FlagRate, is statically deﬁned and is
not adjusted by the algorithm.
Note that the ﬂagging parameters need only be set
based on system-wide policy and generally do not re-
quire independent relay tuning, but provides the ﬂexi-
bility to allow individual relay operators to deviate from
system policy if they desire.
Throttling Using Thresholds. Recall the two main is-
sues a throttling algorithm must address: selecting which
connections to throttle and the rate at which to throttle
them. Our bit-splitting algorithm explored adaptively
adjusting the throttle rate and applying this to all con-
nections while our ﬂagging algorithm explored statically
conﬁguring a throttle rate and adaptively selecting the
throttled connections. We now describe our ﬁnal algo-
rithm which attempts to adaptively address both issues.
The threshold algorithm also makes use of a
connection-level cell EWMA, which is computed as de-
scribed above for the ﬂagging algorithm. However,
EWMA is used here to sort connections by the loudest