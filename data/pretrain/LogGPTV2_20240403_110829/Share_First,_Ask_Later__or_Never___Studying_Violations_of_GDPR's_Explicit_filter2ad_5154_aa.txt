title:Share First, Ask Later (or Never?) Studying Violations of GDPR's Explicit
Consent in Android Apps
author:Trung Tin Nguyen and
Michael Backes and
Ninja Marnau and
Ben Stock
Share First, Ask Later (or Never?) 
Studying Violations of GDPR’s Explicit 
Consent in Android Apps
Trung Tin Nguyen, CISPA Helmholtz Center for Information Security; 
Saarbrücken Graduate School of Computer Science, Saarland University; 
Michael Backes, Ninja Marnau, and Ben Stock, CISPA Helmholtz Center 
for Information Security
https://www.usenix.org/conference/usenixsecurity21/presentation/nguyen
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Share First, Ask Later (or Never?)
Studying Violations of GDPR’s Explicit Consent in Android Apps
Trung Tin Nguyen§*, Michael Backes§, Ninja Marnau§, and Ben Stock§
§ CISPA Helmholtz Center for Information Security
* Saarbrücken Graduate School of Computer Science, Saarland University
{tin.nguyen,backes,marnau,stock}@cispa.de
Abstract
Since the General Data Protection Regulation (GDPR) went
into effect in May 2018, online services are required to ob-
tain users’ explicit consent before sharing users’ personal
data with third parties that use the data for their own purposes.
While violations of this legal basis on the Web have been stud-
ied in-depth, the community lacks insight into such violations
in the mobile ecosystem.
We perform the ﬁrst large-scale measurement on Android
apps in the wild to understand the current state of the vio-
lation of GDPR’s explicit consent. Speciﬁcally, we build a
semi-automated pipeline to detect data sent out to the Internet
without prior consent and apply it to a set of 86,163 Android
apps. Based on the domains that receive data protected under
the GDPR without prior consent, we collaborate with a legal
scholar to assess if these contacted domains are third-party
data controllers. Doing so, we ﬁnd 24,838 apps send personal
data towards data controllers without the user’s explicit prior
consent. To understand the reasons behind this, we run a noti-
ﬁcation campaign to inform affected developers and gather
insights from their responses. We then conduct an in-depth
analysis of violating apps as well as the corresponding third
parties’ documentation and privacy policies. Based on the
responses and our analysis of available documentation, we de-
rive concrete recommendations for all involved entities in the
ecosystem to allow data subjects to exercise their fundamental
rights and freedoms.
1 Introduction
Increasing data collection and tracking consumers by today’s
online advertising industry is becoming a major problem for
individuals’ rights regarding their personal data (e.g., users
are secretly tracked and proﬁled [29, 43, 45]). To protect user
privacy, regulatory efforts around the globe such as the Gen-
eral Data Protection Regulation (GDPR) and the California
Consumer Privacy Act (CCPA) have been made in recent
years [14, 54] — which mandate online services to disclose
transparently how they handle personal data and grant users
crucial data protection rights.
In mobile apps, researchers have analyzed the app privacy
policies to identify legislation violations, i.e., determining
whether an app’s behavior is consistent with what is declared
in the privacy policy [4, 56, 58, 72, 73]. However, irrespec-
tive of a privacy policy, under the GDPR [54], to be legally
compliant, an app is required to obtain users’ explicit con-
sent before sharing personal data with third parties if such
parties use the data for their own purposes (e.g., personal-
ized advertising) [21]. The GDPR requires the consent to
be freely given, speciﬁc, informed, and unambiguous (Sec-
tion 2.2). That is, personal data transfer must only occur after
the user has actively agreed (e.g., by clicking accept), i.e.,
“consent” packaged in terms and conditions or privacy poli-
cies is not compliant [29].
While many researchers have worked to detect and analyze
consent notices (i.e., cookie banners) and their impact on the
Web advertising and tracking industry after the GDPR went
into effect [17, 37, 43, 57, 61, 62, 65], the community lacks
insight into such violations in the mobile ecosystem. Recently,
Weir et al. [69] surveyed app developers and observed that
most developers’ changes were cosmetic due to the GDPR
legislation (e.g., adding dialogues) — which raises a serious
question about whether these changes fulﬁll the legal condi-
tions for collecting valid consents. Figure 1 shows examples
of consent dialogues that mobile users in the European Union
observe on many apps they use today. Notably, neither (a) nor
(b) are valid consent dialogues required before data sharing
with third parties, and even dialogue (c) is meaningless if data
sharing occurs before the user has the ability to reject this.
To understand how prevalent violations of GDPR’s ex-
plicit consent requirement are in the wild, we conduct a study
with 86,613 Android apps available through the German Play
Store, allowing us to provide a comprehensive overview of
the current state of the violation of GDPR’s explicit consent
on mobile apps in the wild. Speciﬁcally, we ﬁrst build a semi-
automated and scalable pipeline to detect personal data sent
to the Internet by analyzing the network trafﬁc generated
USENIX Association
30th USENIX Security Symposium    3667
2 Research Questions and Legal Background
Our work focuses on the violation of GDPR’s explicit consent
requirement in the realm of Android apps available through
the European Play Store (i.e., from Germany). In the follow-
ing, we brieﬂy outline prior work in the area of GDPR and
related privacy legislation, as well as more general privacy
analyses for mobile apps. We subsequently present the legal
background on GDPR and present our research questions.
2.1 Context of our Work
In recent years, many researchers have started to study the
impact of GDPR on the online advertising and tracking in-
dustry and proposed different techniques to detect legislation
violations. A related line of work aims to study the consent
notices in the Web ecosystem, which are usually presented in
cookie banners. Researchers have shown that many websites
potentially violate the GDPR consent requirements, such not
allowing users to refuse data collection or installing track-
ing and proﬁling cookies before the user gives explicit con-
sent [17, 37, 43, 57, 61, 62, 65]. While many researchers
have worked to detect and analyze consent notices and their
impact on the Web advertising and tracking industry after
the GDPR, no study has measured the GDPR violations of
explicit consent on mobile apps. For mobile apps, researchers
mostly focused on analyzing the app privacy policies to iden-
tify legislation violations, i.e., determining whether an app’s
behavior is consistent with what is declared in the app privacy
policy [4, 56, 58, 72, 73].
Researchers have proposed different techniques to detect
privacy violations by mobile apps and identify third-party ad-
vertising and tracking services. Many techniques have relied
on the static program analysis of app binary code to detect
malicious behaviors and privacy leaks [7, 8, 45, 48] as well
as third-party library use [9, 40, 42]. While the static analysis
techniques are well known for producing high false positives
(e.g., do not produce actual measurements of privacy viola-
tions) [12, 39, 67], the dynamic analysis shows precisely how
the app and system behave during the test (i.e., by running
the app and auditing its runtime behavior) [10, 56, 70, 71].
However, an effective dynamic analysis requires building an
instrumentation framework for possible behaviors of interest,
which involves extensive engineering effort [53]. Another
line of work aims to inspect network communications to iden-
tify third-party advertising and tracking services and privacy
leaks [13, 25, 36, 52, 55] — which is closely related to our
work. However, while prior works primarily focused on data
protected by OS permissions (e.g., GPS data), we further de-
tect potential unique identiﬁers which could be used to track
an individual (Section 3.2.2). We believe our work is an im-
portant ﬁrst step in understanding the magnitude of violations
of GDPR consent requirements and potential causes, and can
spark further research into addressing these problems.
(a)
(b)
(c)
Figure 1: Example of consent dialogues in Android apps.
by apps without user explicit prior consent and apply this to
our dataset, which consists of both high-proﬁle and long-tail
apps. Based on the domains that receive data protected under
the GDPR without prior consent, we collaborate with a legal
scholar to assess the extent to which contacted domains are
third-party data controllers — which require explicit consent.
Doing so, we ﬁnd 24,838 apps sent personal data towards
advertisement providers that act as data controllers without
the user’s explicit prior consent. To inform developers about
these issues and understand the reasons behind them, we run
a notiﬁcation campaign to contact 11,914 affected developers
and gather insights from 448 responses to our notiﬁcations.
Inspired by the responses, we conduct an in-depth analysis
of available documentation and default data collection set-
tings of third-party SDKs. Based on the insights from both
developers and our own analysis, we ﬁnd that GDPR issues
are widespread, often misunderstood, and require effort from
advertisement providers, app stores, and developers alike to
mitigate the problems. In summary, our paper makes the fol-
lowing contributions:
• We build a semi-automated and scalable solution (which
is publicly available at [1]) to detect personal data sent
to the Internet by analyzing the network trafﬁc generated
by apps without user explicit prior consent (Section 3).
• We perform a large-scale measurement on the mobile
apps in the wild to understand the current state of the
violation of GDPR’s explicit consent (Section 4).
• We run a notiﬁcation campaign to inform affected devel-
opers and gather insights from their responses. We then
conduct an in-depth analysis of violating apps and the
corresponding third parties’ documentation (Section 5).
• We derive concrete recommendations to all concerned
parties and make an urgent call to help developers com-
ply with GDPR (Section 6).
3668    30th USENIX Security Symposium
USENIX Association
Research Questions Orthogonal to prior work, we aim to
understand how often GDPR’s explicit consent mandate is
violated in the mobile ecosystem, focusing on Android. To
that end, we derive a semi-automated system that allows us to
detect apps which sent out users’ personal data without prior
consent. By further analyzing the parties involved in receiving
such data, this allows us to determine which parties act as data
controllers, which require explicit consent, including speciﬁc
explanations of what the data is used for. Speciﬁcally, our
research aims at answering the following research questions:
• RQ1: How many apps send out personal data without
any prior consent? By developing a semi-automated
system to tackle this question, we analyze a dataset of
86,163 apps to detect to which hosts the apps send data
without any prior explicit consent from the user.
• RQ2: Of the apps which send out any data, how many
send it towards parties that act as data controllers under
the GDPR? By analyzing the legal documents provided
by third-party vendors, we determine which of them
unequivocally must be considered data controllers, al-
lowing us to reason about GDPR consent violations.
• RQ3: Are developers aware of the requirements of GDPR
and the issues that might arise from not following the out-
lined laws? To answer this, we notify affected develop-
ers, provide details on which parties their apps contacted
without prior consent, and survey the issues they face in
integrating third-party SDKs in a GDPR-compliant way.
2.2 Legal Background
In this work, the GDPR is used as the base for our legal analy-
sis. The GDPR governs all processing of personal data related
to individuals situated in the EU and EEA. Additionally, the
ePrivacy Directive applies to how third parties gather consent
to accessing information stored on the consumers’ device
(also known as “cookie law”), but this is outside our scope.
2.2.1 Deﬁnition of Personal Data
Under GDPR’s Article 4 [30], “personal data” (referred to
as “PD”) means any information relating to an identiﬁed or
identiﬁable natural person (“data subject”). This deﬁnition
includes unique identiﬁcation numbers, which may include
Advertising IDs, location data, and online identiﬁers (such as
IP addresses) — when they can be used to identify users over
a long period across different apps and services [4].
The deﬁnition of personal data under the GDPR is much
broader than personal identiﬁable data (PII) under US laws.
Instead of only including directly identifying data, GDPR also
considers personal data such data that can be used alone or
in combination to single out an individual in a data set. The
EU Court of Justice has already declared that even dynamic
IP addresses may be considered personal data in its Breyer v.
Germany ruling [2].
Android’s Advertising ID (AAID) is an interesting sub-
ject for the courts, which lacks a ruling as of yet. Google
describes the ID as “a unique, user-resettable ID for advertis-
ing, provided by Google Play services. [...]. It enables users
to reset their identiﬁer or opt-out of personalized ads” [34].
While Google itself remained vague on characterisation of
the AAID as personal data, the IAB Europe GDPR Imple-
mentation Working Group already established in their 2017
Working Paper on personal data that “Cookies and other de-
vice and online identiﬁers (IP addresses, IDFA, AAID, etc.)
are explicitly called out as examples of personal data under
the GDPR” [35]. In May 2020, NOYB – European Center for
Digital Rights [46], a European not-for-proﬁt privacy advo-
cacy group, lodged a formal complaint over the AAID with
Austria’s data protection authority. The complaint states that
although the AAID is personal data, Google does not adhere
to the requirements of valid consent. Android users have no
option to deactivate or delete the tracking ID, only to reset it
to a new one. Furthermore, even Google’s own brand Admob
explicitly lists the AAID as personal data in their documenta-
tion about the User Messaging Platform used to deliver their
ads [33]. Meanwhile, Apple has recently taken actions for
mandatory prior consent for sharing of Advertising Identi-
ﬁers for its iOS 14 update [6], clarifying that even dynamic
advertising identiﬁers are considered personal data.
2.2.2 Legal Basis for Processing of Personal Data
Under the GDPR, all processing of European residents’ per-
sonal data has to have a legal justiﬁcation. App developers
(ﬁrst parties) process user data in order to provide the app’s
functionalities and services. By deciding on the means and
purposes for processing the user’s personal data, they act as
data controllers, the legal role that is the responsible party
for data processing. Parties external to this app developer
(third parties) that also receive the user’s data could act in two
possible capacities. If they act purely on behalf of the ﬁrst
party with no data use for their own purposes and under the
complete control of the ﬁrst party (e.g., error logging), they
act as data processors. If they use the user’s data for their
own purposes and gains, i.e., in order to do market research,
create and monetize user proﬁles across customers or improve
their services, and are not controlled by the ﬁrst party, they
act as data controllers.
GDPR Article 6 [31] contains the six general justiﬁcations
for processing. Among others, the processing may be based on
consent, the fulﬁllment of a contract, compliance with a legal
obligation, or the data controller’s legitimate interests when
such interest outweighs the fundamental rights and freedoms
of the data subjects. In practice, most advertising companies
rely on consent or legitimate interests as the legal basis for
processing personal data for proﬁling and targeted advertising
USENIX Association
30th USENIX Security Symposium    3669
(i.e., since the legal ground necessary for the performance of
a contract does not apply in these circumstances [11, 29]).
However, a recent study from the Norwegian Consumer
Council [29] shows that data subjects do not have a clear
understanding of the amount of data sharing and the variety
of purposes their personal data is used for in targeted ads.
A large amount of personal data being sent to various third
parties, who all have their own purposes and policies for
data processing, are detrimental to the data subjects’ privacy.
Even if advertising is necessary to provide services free of
charge, these privacy violations are not strictly necessary to
provide digital ads. Consequently, it seems unlikely that these
companies’ legitimate interests may claim to outweigh the
fundamental rights and freedoms of the data subject. This
means that many of the ad tech companies would most likely
have to rely on consent as the legal basis for their processing
operations. In case the data transfer in question relies on user
consent, the GDPR requires the consent to be freely given,
speciﬁc, informed, and unambiguous. Further, the data subject
must have given consent through a statement or by a clear
afﬁrmative action prior to the data processing in question
(GDPR Art. 4(11) [30] and Art. 7 [32]).
Unambiguous consent under the GDPR must meet certain
conditions. The GDPR Art. 7(2) states that: “If the data sub-
ject’s consent is given in the context of a written declaration
which also concerns other matters, the request for consent
shall be presented in a manner which is clearly distinguish-
able from the other matters, in an intelligible and easily ac-
cessible form, using clear and plain language”. The user’s
consent has to be easily differentiated from other declarations
or even consent to other processing activities. The user has to
be speciﬁcally asked to consent to data sharing and processing
for advertising purposes and this consent must not be grouped
together with, e.g., consent to download the app or consent to
access certain APIs on the phone.
In order to be legally valid, consent with regard to the pro-
cessing of personal data has to be explicit. This means that the
controller should obtain verbal or written conﬁrmation about
the speciﬁc processing [Recital 32]. According to the Arti-
cle 29 Working Party, consent cannot be based on an opt-out
mechanism, as the failure to opt-out is not a clear afﬁrmative
action [49]. The user has to actively give their consent, i.e.,
by clicking “I agree” on a consent form. Merely continuing
to use an app or other passive behavior does not constitute
explicit consent. Lastly, the consent has to be obtained prior
to the data processing to be considered valid.
Our research focuses explicitly on these aspects of user
consent. In particular, with respect to the aforementioned
regulations, transmitting data to an advertisement company
without prior, explicit consent by the user for the purpose of
targeted advertisement is considered violating GDPR.
Data Type
AAID
BSSID
Email
GPS
IMEI
IMSI
MAC
PHONE
SIM_SERIAL
SERIAL
SSID
GSF ID
Description
Android Advertising ID
Router MAC addresses of nearby hotspots
Email address of phone owner
User location
Mobile phone equipment ID
SIM card ID
MAC address of WiFi interface
Mobile phone’s number
SIM card ID
Phone hardware ID (serial number)
Router SSIDs of nearby hotspots
Google Services Framework ID
Table 1: Overview of personal data tied to a phone.
3 Methodology
Our main goal is to have a mostly automated and scalable so-