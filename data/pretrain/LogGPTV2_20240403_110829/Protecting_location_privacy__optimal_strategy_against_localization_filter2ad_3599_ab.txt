2. is aware of the LPPM’s internal algorithm f (.).
Hence, the adversary implements the optimal attack h(.)
that estimates the true location of the user with the least
distortion as measured by dp(.).
3. GAME FORMULATION
The problem of ﬁnding an LPPM that oﬀers optimal lo-
cation privacy given the knowledge of the adversary is an
instance of a zero-sum Bayesian Stackelberg game.
In a
Stackelberg game the leader, in our case the user, plays ﬁrst
by choosing an LPPM and committing to it by running it on
her actual location. The follower, in our case the adversary,
plays next by estimating the user’s location, knowing the
LPPM that the user has committed to. It is a Bayesian game
because the adversary has incomplete information about the
user’s true location, and plays according to his hypothesis
about this location.
It is also an instance of a zero-sum
game, as the adversary’s gain (or loss) of utility is exactly
balanced by the losses (or gains) of the utility of the user:
the information gained (lost) by the adversary is the loca-
tion privacy lost (gained) by the user. We now proceed to
deﬁne the game adapted to our problem:
Step 0 Nature selects a location r ∈ R for the user to access
the LBS, according to a probability distribution ψ(.).
That is, location r is selected with probability ψ(r).
Step 1 Given r, the user runs the LPPM f (r′|r) to select
a pseudolocation r′ ∈ R′, subject to f (.) complying
with the service quality constraint (3).
Step 2 Having observed r′, the adversary selects an esti-
mated location ˆr ∼ h(ˆr|r′), ˆr ∈ R. The adversary
knows the probability distribution f (r′|r) used by the
LPPM; he also knows the user’s proﬁle ψ(.), but not
the true location r.
Final Step The adversary pays an amount dp(ˆr, r) to the
user. This amount represents the adversary’s error
(equivalently, the location privacy gained by the user).
The above description is common knowledge to both the
adversary and the user. They both aim to maximize their
payoﬀ, i.e.
the adversary tries to minimize the expected
amount that he will pay, while the user tries to maximize it.
4. SOLUTION
In this section, we describe a precise optimization problem
that formalizes the objectives of the user and of the adver-
sary. We construct two linear programs that, given ψ(.),
dp(.) and dq(.), we can compute the user’s optimal choice
of protection mechanism f (.), and the adversary’s optimal
choice of inference attack h(.).
4.1 Optimal Strategy for the User
The adversary observes the pseudolocation r′ output by
the LPPM, he knows the function f (r′|r) implemented by
the LPPM, and he also knows the user’s proﬁle ψ(.). Thus,
he can form the posterior distribution
Pr(r|r′) =
Pr(r, r′)
Pr(r′)
=
f (r′|r)ψ(r)
Pr f (r′|r)ψ(r)
(8)
on the true location r of the user, conditional on the obser-
vation r′. The adversary’s objective is then to choose ˆr to
minimize the user’s conditional expected privacy, where the
expectation is taken under Pr(r|r′). The user’s conditional
expected privacy for an arbitrary ˆr is
Pr(r|r′)dp(ˆr, r),
Xr
and for the minimizing ˆr it is
Pr(r|r′)dp(ˆr, r).
min
ˆr Xr
(9)
(10)
If there are multiple values of ˆr that satisfy (10), then the
adversary randomizes arbitrarily among them. The proba-
bility with which ˆr is chosen in this randomization is h(ˆr|r′).
Of course, h(ˆr|r′) will be positive only for minimizing values
of ˆr; for all other values h(ˆr|r′) will be zero. When random-
izing, (10) is rewritten as
Pr(r|r′)h(ˆr|r′)dp(ˆr, r).
(11)
Xr,ˆr
Note that if there is only one value of ˆr satisfying (10),
then this value is selected with probability 1 in the random-
ization, whereas all other values are selected with probability
0, so (11) reduces to (10). In this sense, (11) is a general-
ization of (10), but it should be noted that both expressions
compute the same conditional expected privacy.
We see that for a given r′, the user’s conditional privacy
is given by (10). The probability that r′ is output by the
LPPM is Pr(r′) =Pr f (r′|r)ψ(r). Hence, the user’s uncon-
ditional expected privacy (averaged over all r′) is
Pr(r′) min
Pr(r|r′)dp(ˆr, r)
ˆr Xr
Xr′
=Xr′
min
ˆr Xr
ψ(r)f (r′|r)dp(ˆr, r).
(12)
620To facilitate the computations, we deﬁne
xr′ , min
ˆr Xr
ψ(r)f (r′|r)dp(ˆr, r).
(13)
Incorporating xr′ into (12), we rewrite the unconditional
expected privacy of the user as
xr′ ,
Xr′
(14)
which the user aims to maximize by choosing the optimal
f (r′|r). The minimum operator makes the problem non-
linear, which is undesirable, but (13) can be transformed to
a series of linear constraints:
ψ(r)f (r′|r)dp(ˆr, r), ∀ˆr.
(15)
xr′ ≤Xr
It turns out that maximizing (14) under (13) is equivalent
to maximizing (14) under (15) [4, Ch. 7, p. 224].
We construct the linear program for the user from (14)
and (15). Note that variable xr′ is a decision variable in the
linear program, i.e. it is among the quantities chosen by the
solver. This might appear counterintuitive, as xr′ is deﬁned
in (13) as a function of f (.), rather than as an independent
variable that can be freely selected. But, because of the
transformation, it is always guaranteed that (13) will hold.
The linear program for the user is the following: Choose
f (r′|r), xr′ , ∀r, r′ in order to
Maximize Xr′
subject to
xr′
xr′ ≤Xr
Xr
ψ(r)Xr′
Xr′
f (r′|r) = 1, ∀r
f (r′|r) ≥ 0, ∀r, r′
ψ(r)f (r′|r)dp(ˆr, r), ∀ˆr, r′
f (r′|r)dq(r′, r) ≤ Qmax
loss
(18)
(16)
(17)
(19)
(20)
Inequalities (17) are the series of linear constraints (15),
one series for each value of r′; inequality (18) reﬂects the
service quality constraint; constraints (19) and (20) reﬂect
that f (r′|r) is a probability distribution function.
4.2 Optimal Strategy for the Adversary
The reasoning is similar for the formalization of the ad-
versary’s optimization problem. When the LPPM’s output
is pseudolocation r′, the adversary will solve (10) to ﬁnd an
estimate ˆr. More generally, the adversary will ﬁnd many
minimizing values of ˆr, and each of them will be selected
with some probability h(ˆr|r′). Given that the true location
is r and that the observed pseudolocation is r′, the condi-
tional expected user privacy is
h(ˆr|r′)dp(ˆr, r).
(21)
Xˆr
The user chooses r′ to maximize (21). So, given that the
true location is r, the conditional expected user privacy for
the maximizing r′ is
yr , max
r′ Xˆr
h(ˆr|r′)dp(ˆr, r).
(22)
Similarly as before, the maximization can be generalized
to a randomization among maximizing values of r′. The
probability with which r′ is chosen is f (r′|r).
The prior distribution ψ(r) contains the adversary’s knowl-
edge of r. Thus, the unconditional expected user privacy is
ψ(r) yr,
Xr
(23)
that the adversary aims to minimize by choosing h(ˆr|r′).
Similarly as before, (22) can be transformed to an equivalent
series of linear constraints:
h(ˆr|r′)dp(ˆr, r), ∀r′.
(24)
yr ≥Xˆr
We construct the linear program for the adversary (which
is the dual of the user’s linear program) from (23) and (24):
Choose h(ˆr|r′), yr, ∀r, r′, ˆr, and z ∈ [0, ∞) in order to
ψ(r) yr + zQmax
loss
(25)
Minimize Xr
subject to
h(ˆr|r′)dp(ˆr, r) + zdq(r′, r), ∀r, r′ (26)
yr ≥Xˆr
Xˆr
h(ˆr|r′) = 1, ∀r′
h(ˆr|r′) ≥ 0, ∀r′, ˆr
z ≥ 0
(27)
(28)
(29)
Note the role of variable z: In linear programming par-
lance, it is the shadow price of the service quality constraint.
Intuitively, z is the “exchange rate” between service quality
and privacy. Its value in the optimal solution indicates the
amount of privacy (in privacy units) that is lost (gained) if
the service quality threshold Qmax
loss increases (decreases) by
one unit of quality.
loss in Qmax
For example, if z > 0 in the optimal solution, then any
change ∆Qmax
loss will aﬀect the privacy achieved by
z∆Qmax
loss . In this case, constraint (18) is satisﬁed as a strict
equality. In contrast, if constraint (18) is satisﬁed as a strict
inequality, then, intuitively, the selection of f (r′|r) has not
been constrained by Qmax
loss . In this case, any (small) changes
loss will have no eﬀect on f (r′|r), nor on the privacy
in Qmax
achieved. So, z would be zero.
Note that both linear programs compute the uncondi-
tional expected privacy of the user (5), which we repeat here
for convenience.
P rivacy(ψ, f, h, dp) = Xˆr,r′,r
ψ(r)f (r′|r)h(ˆr|r′)dp(ˆr, r). (30)
Previous expressions can be derived from this one. For
instance, if there is a single best choice of a pseudolocation
r′ for each given location r, then f (r′|r) is always either 0
or 1, so (10) is obtained.
The optimal solution of each linear program results in the
same value for the privacy of the user. Hence, in princi-
ple, we only need to compute one of the two to quantify
maximum level of privacy of the user. We choose to present
both, because the user’s linear program incorporates the ser-
vice quality constraint in a more straightforward manner,
whereas the adversary’s linear program explicitly computes
the “exchange rate” between service quality and privacy.
621 
Figure 1: Spatial histogram showing the density of
users per region (in log scale) in Lausanne. The area
size is 15.32km × 7.58km, divided into 20 × 15 regions.
5. EVALUATION
The proposed optimization framework enables us to deter-
mine the most eﬀective location-privacy protection mecha-
nism (LPPM) against optimal inference attacks. The opti-
mal LPPM is designed under the constraint of guaranteeing
a minimum service quality such that the location-based ser-
vice remains useful for the user. In this section, we evalu-
ate the relation between location privacy and service qual-
ity for a few example location-based services (Recall that the
service-quality sensitivity of a LBS to location obfuscation is
encoded through the dissimilarity function dq(.)). Moreover,
we evaluate the performance of non-optimal LPPMs and
non-optimal inference attacks against the optimal strategies.
We use real location traces of people (in Lausanne, Switzer-
land) who use various means of transportation.1 We select
11 users at random, and we focus on their location traces
during the day (8am to 8pm), when it is more probable that
user use location-based services. The length of the consid-
ered traces is one month. The location area, within which
they move, is divided into 300 regions. Figure 1 shows the
density of users across all the regions. The grayness of the
cells shows the density of its corresponding region in log
scale. As many of the regions are abandoned (or very rarely
visited) by many individual users, we compute each user’s
proﬁle ψ(.) by considering only the 30 most popular regions
across the whole population. This prevents sparse user pro-
ﬁles. A user’s proﬁle is the normalized number of her visits
to each region.
Given distance functions dp(.) and dq(.) and service-quality
loss threshold Qmax
loss , we compute the optimal LPPM and its
corresponding optimal attack by solving (16) and (25) us-
ing Matlab’s linear programming solver. We then compare
the obtained optimal protection mechanism and the optimal
inference attack against obfuscation LPPMs and Bayesian
inference attacks, respectively.
Basic Obfuscation LPPM.
The basic obfuscation LPPM, with an obfuscation level
k = 1, 2, 3, . . ., is constructed in the following way: For each
location r, we ﬁnd its k − 1 closest locations (using the Eu-
clidean distance between the centers of the regions). The
1The traces are obtained from the Lausanne Data Collection
Campaign dataset, http://research.nokia.com/page/11367
probability distribution function f (.|r) will be the uniform
probability distribution on the set of the k − 1 selected lo-
cations together with the location r. That is, location r
is replaced by each of the k locations, as a pseudolocation,
with the same probability 1
k , and all the rest of locations
have probability 0. Thus, in practice, an actual location r
is hidden among its k − 1 nearest locations. We choose this
mechanism, as it has been very popular in the literature.
Given the user proﬁle ψ(.) and quality distance function
dq(.), we use (2) to compute the expected service-quality
loss Qloss(ψ, f, dq) for any LPPM obfuscation f (.), whether
it be optimal or not.
Bayesian Inference Attack on an LPPM.
We compare the eﬀectiveness of our optimal attack with
the Bayesian inference attack, which has been shown eﬀec-
tive before in [26]. In the Bayesian approach, for each pseu-
dolocation r′, the posterior probability distribution over the
locations is used to invert the noise added by the LPPM
and, thus, to estimate the actual location:
h(ˆr|r′) =
Pr(ˆr, r′)
Pr(r′)
=
f (r′|ˆr)ψ(ˆr)
Pr f (r′|r)ψ(r)
(31)
We use (5) to compute the expected location privacy of
a user who adopts a given (obfuscation or optimal) LPPM
f (.) against a (Bayesian or optimal) inference attack h(.).
The expected location privacy also depends on the distortion
function dp(.) that we choose to use.
Brieﬂy, if dp(.) is the Hamming distance, then the Bayesian
attack chooses the location with the highest posterior proba-
bility Pr(ˆr|r′). If dp(.) is the Euclidean distance, the Bayesian
attack chooses the conditional expected value E[ˆr|r′].
Optimal Inference Attack on an Arbitrary LPPM.
In order to make a fair comparison between the eﬀec-
tiveness of the optimal and obfuscation LPPM, we need to
run the same attack on both of them. The Bayesian in-
ference attack described by (31) can be performed against
both. However, we still need to design an optimal attack
against arbitrary LPPMs that have not been constructed in
our game-theoretic framework.
The optimal inference attack is the one that minimizes
the expected user privacy:
h(ˆr|r′) = arg min
h
P rivacy(ψ, f, h, dp).
(32)
Given the user proﬁle ψ(.), an LPPM f (.) and distortion
function dp(.), the following linear program ﬁnds the optimal
attack h(.). Note that, compared to (25), there is no service
quality constraint here, as the LPPM has been assumed to
be arbitrary.
ψ(r)f (r′|r)h(ˆr|r′)dp(ˆr, r)
(33)
h(ˆr|r′) = 1, ∀r′, and h(ˆr|r′) ≥ 0, ∀ˆr, r′ (34)
Minimize Xˆr,r′,r
subject toXˆr
Location-Privacy Protection Mechanism Output.
Consider a LBS user making use of our optimal LPPM on
her mobile device. The way her location appears in the eyes
of the adversary is shown in Figure 2. For the sake of com-
parison, Figure 2 also shows how a basic obfuscation LPPM
distributes the pseudolocations over space. In order to make
622User’s Profile
Optimal LPPM, over all locations
Obfuscation LPPM, over all locations
Optimal LPPM. Loc(13,7)
Obfuscation LPPM. Loc(13,7)
(1)
(2)
(3)
(4)
(5)
Figure 2: Input/Output of LPPM. Proﬁle of a user for whom the subsequent calculations are made (sub-ﬁgure
1). Distribution Pr(r′) of observed pseudolocations when using the optimal LPPM with Qmax
loss = 0.8690 (sub-
ﬁgure 2). Distribution Pr(r′) of observed pseudolocations when using obfuscation LPPM with Qloss(ψ, f, dq) =
0.8690 (sub-ﬁgure 3). Conditional distribution Pr(r′|r) when using the optimal LPPM on location r = (13, 7)
(sub-ﬁgure 4). Conditional distribution Pr(r′|r) when using obfuscation LPPM on location r = (13, 7) (sub-
ﬁgure 5). Column 1 is the leftmost column, and row 1 is the bottom row. (Euclidean dp, Hamming dq)
a fair comparison, we need to make sure that the cost of the
two LPPMs, in terms of service quality, is the same. To
do so, we compute the quality loss Qloss of the obfuscation
LPPM and assign this loss as the quality threshold Qmax
loss of
the optimal LPPM. Hence, the optimal LPPM cannot sac-
riﬁce the service quality more than the obfuscation LPPM
to gain higher location privacy.
Figures 2(2) and 2(3) show Pr(r′), the distribution of