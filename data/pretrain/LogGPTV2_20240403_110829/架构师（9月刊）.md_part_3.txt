在一个多客户端组群的环境下，客户端与服务器端的版本不匹配是常
见现象。早在 0.10.0 版本中，Kafka 已经加入了允许不同版本客户端与服
务器交互的功能，即高版本的 Kafka 客户端依然可以与低版本的服务器进
行数据传导，反之亦然。然而当低版本的消费者客户端和高版本的服务器
进行交互时，服务器有时需要将数据向下转换（format down-conversion）
成为低版本客户端可以认知的格式后才能发回给消费者。向下转换有两个
缺点：
1. 丢失了 Kafka 数据零拷贝（zero-copy）的性能优势；
2. 向下转换需要额外的大量内存，在极端情况下甚至会导致内存溢
出。
前者无法避免，但是后者依然可以改进：在即将发布的 2.0 版本中，
我们使用了一种新的基于分块（chunking）的向下转换算法，使得需要同
时占据的内存需求大幅缩减。这使得高低版本的客户端与服务器之间的交
互变得更加有效。
更多的可监控指标
对于企业级数据平台来说，另一个很重要的要求就是提供各种实时的
监控能力。在 LinkedIn 的时候，同事间流传着据传是我们公司传奇人物
18
专热题点 | |T Hopoict
David Henke 的一句话：what gets measured gets fixed，充分体现了监测的
重要性。
长期以来，Apache Kafka 社区不断地完善各种区块的各种指标，这
每一个新添加的指标背后都有一个我们曾经踩过的坑，一段在线调试和修
bug 的痛苦经历。
举一个具体的例子：Kafka 长期以来被诟病添加分区太慢，因此在 1.1.0
版本里面来自六个不同企业的贡献者共同完成了重新设计 Kafka 控制器
（Kafka Controller）这个规模巨大的 JIRA。在这个长达九个月的项目里，
被谈论很多的一点就是如何增添控制器操作的各种指标。在未来更多的新
功能和新属性，比如继续增强 Kafka 的伸缩性，包括多数据中心支持等等，
如何能够让用户继续便捷地实时监测这些新增功能的性能，及时发现可疑
问题，并且帮助缩短需要的在线调试时间，都将是讨论的重要一环，因为
这也是任何一个企业级流数据平台必须要注意到的。
在 2.0.0 版本中，我们进一步加强了 Kafka 的可监控性，包括添加了
很多系统静态属性以及动态健康指标，比如 KIP-223、KIP-237、KIP-272
等等。
KIP-223：加入消费者客户端的领先指标
在此前， Kafka 消费者客户端已经加入了对每一个消费分区的延迟
指标（lag metrics），定义为当前消费者在分区上的位置与分区末端（log-
end-offset）的距离。当此指标变大时，代表消费者逐渐跟不上发布的速度，
需要扩容。我们发现，当分区 renteion 时间很短而导致消费者跌出可消费
范围时（out-of-range），此指标不能完全针对潜在的危险为用户报警。
因此在即将发布的 2.0 版本中，我们加入了另一个“领先”指标（lead
metrics），定义为分区首端（log-start-offset）与消费者在分区上的位置距
离，当此指标趋近于零时，代表消费者有跌出可消费范围因而丢失数据的
危险。值得称赞的是，这个新特性是由国内的社区贡献者完成的。
19
InfoQ 架构师 2018 年 9 月
KIP-237：加入更多 Kafka 控制器的健康指标
在完成了针对增强 Kafka 控制器性能的全面重设计之后，我们认为现
在 Kafka 已经可以支持千台机器，百万分区级别的集群。在此之前，这样
规模集群的一个阻碍就是 Kafka 控制器本身处理各种 admin 请求，诸如
主本迁移、话题扩容、增加副本等等时的时间消耗。在完成了这个重设计
之后，我们也相应地加入了更多和新设计相关的健康指标，包括控制器内
部请求队列的长度，请求处理速率等等。
更全面的数据安全支持
最后，也是最近一段时间被讨论最多的，就是数据安全的重要性：在
云计算大行其道的今天，多租户方案已成为潮流。而许多数据保护条例的
实施，比如 GDPR，更是让“保证数据不泄漏”成为一个标准流数据平台的
基本要求。这项要求包括从写入，到处理，到导出的一些系列措施，包括
认证、访问控制及授权、端到端加密等等。
举个例子，如果一个 Kafka 集群可以被一个企业里面的多个部门所共
享，如何控制哪些部门可以读取或写入哪些主题、哪些部门可以创建或删
除哪些主题、谁可以看见别人创建的主题、以及与 Kafka 相关的其他服务
和客户端，比如应该如何保护 Zookeeper、谁可以直接读写、哪些 Kafka
Streams 或者 Kafka Connect 应用程序可以发起哪些管理请求等，都需要
提供足够的控制手段。
在 2.0.0 版本里面，我们对此提供了一系列的改进，比如更细粒度
的更细粒度的前缀通配符访问控制（KIP-290、KIP-227），支持 SASL/
OAUTHBEARER（KIP-255），将委托令牌扩展到管理客户端（KIP-249），
等等。
KIP-290、KIP-227：细粒度前缀通配符访问控制
在 2.0 以前，很多 Kafka 自身的访问控制（access control list）机制还
20
专热题点 | |T Hopoict
是粗粒度的。比如对“创建话题”这一访问方式的控制，只有“全集群”这一
种范围。也就是说，对于任何一个用户来说，我们只能或者给予其在一个
集群内创建任何话题的权限——比如说，这个 Kafka 集群的运维或者可靠
性工程团队（Devops or SRE），或者不给予任何话题的创建权限。但是
在一个多租户环境下，我们很可能会出现需要更细粒度的控制机制。比如
一个 Kafka Streams 客户端，除了读取给予的源话题之外，还需要实时创
建一下内部用于 data shuffling 和 change logging 的话题，但是给予其一个
“全集群”的话题创建权限又太危险。
因而在 2.0 版本中，我们进一步细粒度化了很多权限，比如 KIP-290
就加入了前缀通配符（prefix wildcard）的范围，而 KIP-227 就将这种范围
加入到了单个或多个话题创建的权限中。在这个机制下，用户可以被赋予
单独一个话题基于其话题名的创建权限（literal topic），也可以被赋予多
个话题基于其话题名前缀匹配的创建权限，比如可以创建所有名字开头为
“team1-”的话题，等等。
总结
这些总结出来的经验，很多都是在将 Apache Kafka 推广到互联网以
外的领域，比如银行金融，制造和零售业内的公司时发现的。我们观察到
在各种不同的领域里面，公司内部工程团队的技术倚重和深度、事务逻辑
的严谨性要求、对于实时性以及成本控制的权衡偏好都各有不同，因此用
户对于“流数据处理”这一名词所代表的需求都或多或少有自己独特的定义，
但是以上这些特征却是共同的。因此，我觉得作为一个逐步成为标准流数
据平台的开源项目，Kafka 还很“年轻”，还有很多值得我们去探索，未来
可期。
关于 2.0.0 版本，我们其实还有很多新的发布特性，比如新的 Kafka
Streams Scala API 帮助 Scala 用户更好的利用 Scala typing system 进行编程，
Kafka Connect REST extension plugin 对于认证和访问控制的支持等等。
关于更多细节，欢迎大家阅读相关公告：https://www.apache.org/dist/
21
InfoQ 架构师 2018 年 9 月
kafka/2.0.0/RELEASE_NOTES.html
作者介绍
王国璋
，Apache Kafka PMC，Kafka Streams 作者。分别于复旦大学
计算机系和美国康奈尔大学计算机系取得学士和博士学位，主要研究方向
为数据库管理和分布式数据系统。现就职于 Confluent，任流数据处理系
统架构师和技术负责人。此前曾就职于 LinkedIn 数据架构组任高级工程
师，主要负责实时数据处理平台，包括 Apache Kafka 和 Apache Samza 系
统的开发与维护。
22
推荐文章专 题| A |r Ttiocpleic
都去炒 AI 和大数据了，落地的事儿谁来做？
作者 杨雷
摘要
几乎每个企业都期望建立自己的完善的合体的数据体系，但成功的例
子并不多。本文希望用一些实践阐述以下几个观点：
• 数据产品应该朴实无华；
• 浮躁的认知会有大麻烦；
• 如何正确认识自己，如何敏捷。
前言
最近读到一篇文章"SQL足以解决你的问题，别动不动就是机器学习"，
23
InfoQ 架构师 2018 年 9 月
教我们落地之法，在这个浮躁的世界中，犹如一股清流，实在大快人心。
就像皇帝的新衣一样，终于有人说了出来。
有位做供应链数据分析的朋友很开心的说正在创业中，打算在供应链
金融方面有一番作为，用神经网络的方法做用户画像，然后进行市场精准
营销。作者工科数学博士一枚，每每看到有人探讨这么实际应用的东西，
都觉得汗颜（自己不懂）与欣慰（越来越多人参与）并存，以至于给我已
经是博导的师姐说，“好好鼓励你的弟子，数学系的春天来了！”
但是，要泼一下冷水，想必每个投身于大数据、人工智能的人士都碰
到过某个瓶颈阶段，就是想要更深入了解原理的时候，那些公式算法实在
是看不懂啊。每次我只能劝慰说，就当那是个黑盒，你只要知道输入输出，
就能得到想要的结果。难道我要告诉实情其实是，最快你得花费半年到一
年时间恶补数学知识，才能知道什么时候用模式识别，什么时候用小波分
形，什么时候那个东西是动态规划。
这篇文章，继续泼冷水，“如果所有人都去做人工智能了，落地的事
情谁来做？”，好比烧饭师傅都去研究自动炒菜机，在“懒人创造新的世界”
之前，世界上的人都已经饿死了。认清自己手头要做的事情，比展望未来
更关键，至少你能先存活下来。
为什么要做数据产品
不论是初创、上升期、转型还是平台期的企业，回答好自己是谁，为
谁服务，服务得如何，怎样更好的获利这几个问题，离不开数据。
从产品的角度看数据产品。
• Why？很明显，企业需要看数据，用户需要看数据。不管你是做
战略计划、公司愿景、企业架构、运维治理、扩张市场、客户流
失、目标营销，甚至做OKR、KSF、KPI、威士忌分析，或者告诉
你的老板或下属做得有多成功或，，，多失败，你需要数据，这
是你的价值。
• What？When？Who？这是你的内容（范围和服务），你的视野
24
推荐文章专 题| A |r Ttiocpleic
（过程和效率），你的上帝（细分）。
到底怎样做？一个笨手笨脚的人（Klutz）都告诉你可以这样做。
• KNOW 找准自己的定位，企业用户尚在起步阶段，是没有能力去
索取更多的数据的。此外，还需要精通业务流程，数据流离不开
业务流，不论你是To B还是To C，把握好用户痛点和需求，是首
要的。
• LIGHT 不用再介绍一次KISS原则了吧。保持轻装上阵吧，那样就
算死，也死得轻松。
• USE 动手吧，"想总是问题，做才是答案"。试错是如此的关键，
一个企业是否有这个价值观，甚至影响了是否最终的成败。后面
会提到完美主义者，是如何总是在关键时候触礁的。
• TELL 告诉你的用户或老板，这个产品现在该有多糟糕，虽然你
和它已经竭尽全力在工作。奇迹是，他们总会站在你这边。
• ZANY 莎士比亚造了这个词“滑稽”，是让我们轻松点，数据人已
经很累了。“数据科学家”，这个称号显然已经违背了这个原则，