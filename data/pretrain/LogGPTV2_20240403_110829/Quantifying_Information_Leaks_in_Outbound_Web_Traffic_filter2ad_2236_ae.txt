request  and  response  stream  at  1.7  Mbps.  The  bottleneck 
during  processing  was  CPU.  The  real  web  traffic  consisted 
138
of  3.04  GB  of  total  data,  15%  of  which  (475  MB)  was 
request data and 85% of which (2.58 GB) was response data. 
The engine processed the requests at an average rate of 0.25 
Mbps,  and  the  responses  at  an  average  rate  of  10.9  Mbps. 
This disparity in performance is due to the time required to 
compute  the  edit  distance  for  request  URLs.  Javascript 
execution was included under the response processing time. 
None of the scripts were given a time limit, and none of them 
entered infinite loops. 
and 
Analysis  performance  for  the  prototype  implementation 
would  need  improvement  for  use  in  an  intrusion  detection 
system  that  inspects  large  volumes  of  network  traffic.  One 
area for optimization is reducing the number of edit distance 
comparisons 
edit  distance 
computation by only considering multi-byte chunks. Another 
way to improve performance would be to employ a string co-
processor specially designed for edit distance computations. 
Exploring  CPU  performance  optimizations  and  maximizing 
the throughput of the unconstrained bandwidth measurement 
engine is future work. 
approximating 
the 
The memory footprint during analysis was quite large for 
the  prototype  implementation.  It  kept  all  of  the  observed 
links in memory and did not attempt to free older data that 
was  less  likely  to  improve  analysis  results.    Processing  20 
MB  of  web  browsing  traffic  from  one  user  during  a  single 
day  required  120  MB  of  RAM.  Although  this  would  be 
unreasonably large for an intrusion detection application, we 
believe that this number could be greatly reduced by simply 
discarding links from old pages. While analysis results may 
be a little bit worse, the number of links that are loaded from 
pages that have been open for hours is far smaller than links 
that  are 
loaded  from  recent  pages.  Another  possible 
optimization is sharing link information across users.  
8.  Entropy Mitigation Strategies 
The  evaluation  showed  that  a  significant  portion  of 
information  in  web  requests  must  be  counted  because  it 
originates from entropy on the client. If this entropy can be 
reduced  or  measured  at  a  trusted  source,  then  the  analysis 
engine  can  obtain  more  accurate  results.  This  section 
discusses  possible  strategies  for  reducing  inaccuracies  in 
unconstrained  bandwidth  measurements  due  to  entropy  on 
client computers. 
8.1.  System Information and Human Input 
The  current  leak  measurement  engine  cannot  see  actual 
system  information  or  human  input  to  a  client;  it  only 
witnesses  the  resulting  requests.  Due  to  the  complexity  of 
active  content  on  websites,  system  information  and  human 
input can sometimes lead to a chain of events that generates 
a  much 
the  original 
information. For example, clicking on a particular place on a 
web page may lead to an AJAX request that contains a few 
larger  output 
the  size  of 
than 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
hundred  bytes  of  XML.  Speculatively  firing  events  would 
help somewhat with determining expected requests, but such 
an approach would quickly lead to an exponential blow-up. 
A  better  solution  would  be  to  obtain  system  information 
(screen  resolution,  OS,  installed  plug-ins,  etc.)  and  human 
input hints from an agent running on the end host. This agent 
could be a browser plug-in that records and sends all of the 
system  information  and  human  input  events  to  the  analysis 
engine. Instead of having to speculate, the engine could then 
replay the exact sequence of inputs and verify that the output 
is the same. It could only count the size of the original input, 
rather  than  the larger resulting  output. It  is also  okay  if  the 
agent reports data incorrectly, because doing so would only 
increase the unconstrained bandwidth measurement and raise 
suspicion. 
unconstrained 
bandwidth  measurements 
Depending on the threat model, it may also be possible to 
reduce 
by 
discounting  human  input  entirely.  This  approach  may  be 
appropriate  if  the  user  is  trustworthy,  but  malware  is  a 
concern. A trusted device, similar to a hardware key-logger, 
could intercept mouse and keyboard events before they reach 
the computer, and then report them to the leak measurement 
engine. This would aid analysis in a similar manner as a hint 
from  a  browser  plug-in,  except  that  the  size  of  the  original 
human input could be discounted as well, assuming that the 
user is trusted. 
8.2.  Timing 
The  timing  of  each  request  has  the  potential  to  leak 
several bits of information to an observer stationed outside of 
the  network.  The  traditional  method  for  mitigating  timing 
channels  is  to  add entropy  to each request.  For  web traffic, 
this  can  be  achieved  by  adding  a  trusted  proxy  server 
between  the  client  and  the  web  server.  This  proxy  can  add 
jitter to each web request by delaying it a random amount of 
time. This could significantly increase the size of the timing 
interval, raising it from 0.06 seconds to 1 second (any more 
might  disrupt  usage).  Randomly  delaying  requests  up  to  1 
second  would  reduce  the  amount  of  timing  information  in 
each  request  by  5  bits,  which  can  add  up  to  a  significant 
savings for a large number of requests. 
Another option available to us that would not be feasible 
for  mitigating  a  traditional  IP  packet  timing  channel  is 
reducing  the  total  number  of  requests.  Every  time  a  client 
makes a request for a web page, a smart caching proxy could 
pre-fetch  all  of  the  mandatory  links.  Then,  when  the  client 
requests  a  resource  from  a  mandatory  link,  the  proxy  can 
return  the  result  without  any  information  leaving  the 
network, thus precluding leakage through those requests. 
In  addition  to  the  timing  of  requests  themselves,  some 
requests  include  an  explicit  time  value.  This  is  the  system 
time  at  which  a  script  executed  on  the  end  host.  Websites 
may include this time value to prevent caching, or to collect 
statistics about latency from their users. In any case, it differs 
slightly from the time that a request actually appears on the 
139
network,  has  a  high  precision,  and  can  therefore  leak 
information.  A  proxy 
timing 
information  of  this  form  by  discovering  it  with  the  edit 
distance algorithm and then overwriting it with the time that 
the proxy actually sends the request. 
server  can  eliminate 
8.3.  Random Number Generator 
Many websites have scripts that include random numbers 
in  link  URLs.  The  purpose  of  doing  this  is  to  prevent 
caching. At the same time, however, these requests leak data 
in their selection of random numbers. One way of reducing 
entropy  from  the  random  number  generator  (RNG)  is  to 
instead have a network service that handles random number 
generation. When an executing script makes a call to fetch a 
random  number,  the  Javascript  engine  could  request  a  new 
random  number  from  a  trusted  central  location  instead  of 
using  the  local  RNG.  This  would  move  random  numbers 
from  the  set  U  of  UI-layer  input  to  the  set  I  of  network 
inputs,  allowing  the  analysis  engine  to  discount  them  from 
the  information  measurement  in  outbound  web  requests 
(assuming they are not modified by malware). 
9.  Conclusions and Future Work 
This  paper  introduced  a  new  approach  for  quantifying 
information  leaks  in  web  traffic.  Instead  of  inspecting  a 
messageâ€™s  data,  the  goal  was  to  quantify  its  information 
content. The algorithms in this paper achieve precise results 
by discounting fields that are repeated or constrained by the 
protocol.  This  work  focuses  on  web  traffic,  but  similar 
principles can apply to other protocols. Our analysis engine 
processes  static  fields  in  HTTP,  HTML,  and  Javascript  to 
create  a  distribution  of  expected  request  content.  It  also 
executes  dynamic 
in  an  emulated  browser 
environment to obtain complex request values. 
scripts 
We  evaluated  our  analysis  techniques  on  controlled  test 
cases  and  on  real  web  traffic  from  10  users  over  a  30-day 
period. For the controlled tests, the measurement techniques 
yielded  byte  counts  that  ranged  from  0.32%-1.12%  of  the 
raw  message  size.  These  tests  highlighted  some  limitations 
of our approach, such as being unable to filter parts of URLs 
that contain random numbers to prevent caching. For the real 
web traffic evaluation, the precise unconstrained byte counts 
averaged 1.48% of  the  corresponding raw  values. This  was 
significantly  better  than  a  generic  compression  algorithm, 
which averaged 9.87% of the raw size for each request. 
In  the  future,  we  plan  to  implement  similar  leak 
measurement techniques for other protocols. E-mail (SMTP) 
will probably be the most challenging because a majority of 
its data is free-form information from the user. There is also 
a  lot  of  room  to  improve  the  dynamic  content  analysis 
techniques.  Obtaining  user  input  hints  from  clients  and 
executing plug-in objects can help extract additional request 
URLs.  Finally,  we  hope  to  optimize  and  integrate  the 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
techniques from this paper into a network intrusion detection 
system 
to  discover 
information leaks. 
that  uses  bandwidth 
thresholds 
Acknowledgements 
We  would  like  to  thank  friends  and  students  at  the 
University of Michigan who participated in this study. Also, 
thanks  to  Peter  Chen,  Myron  Gutmann,  Morley  Mao,  and 
Patrick  McDaniel  for  their  feedback  on  the  research.  The 
web  traffic  study  was  conducted  with  IRB  approval  under 
project HUM00024168 at the University of Michigan. 
References 
[1]  Adobe  Systems 
Incorporated.  Adobe  Flash  Player. 
http://www.macromedia.com/software/flash/about,   2008. 
[2]  R.  Anderson  and  F.  Petitcolas.  On 
the  Limits  of 
in 
Steganography. 
Communications, 16(4):474-481, 1998. 
IEEE  Journal  of  Selected  Areas 
[3]  K. Borders and A. Prakash. Web Tap: Detecting Covert Web 
Traffic.  In  Proc.  of  the  11th  ACM  Conference  on  Computer 
and Communications Security (CCS), 2004. 
[4]  K.  Borders  and  A.  Prakash.  Towards  Quantification  of 
Network-Based Information Leaks Via HTTP. In Proc. of the 
3rd USENIX Workshop on Hot Topics in Security, 2008. 
[5]  S. Brand. DoD 5200.28-STD Department of Defense Trusted 
(Orange  Book). 
Computer  System  Evaluation  Criteria 
National Computer Security Center, 1985. 
[6]   S.  Cabuk,  C.  Brodley,  and  C.  Shields.  IP  Covert  Timing 
Channels:  Design  and  Detection.  In  Proc.  of  the  11th  ACM 
Conference  on  Computer  and  Communications  Security 
(CCS), 2004. 
[7]  S.  Castro.  How 
to  Cook  a  Covert  Channel.  hakin9, 
http://www.gray-world.net/projects/ 
cooking_channels/hakin9_cooking_channels_en.pdf, 2006. 
[8]  J.  Gailly  and  M.  Adler.  The  gzip  Home  Page. 
http://www.gzip.org/, 2008. 
[9]  J.  Giles  and  B.  Hajek.  An  Information-Theoretic  and  Game-
Theoretic  Study  of  Timing  Channels.  IEEE  Transactions  on 
Information Theory, 48:2455â€“2477, 2003. 
[10] M. Handley, V. Paxson, and C. Kreibich. Network Intrusion 
Detection:  Evasion,  Traffic  Normalization,  and  End-to-End 
Protocol  Semantics.  In  Proc.  of  the  10th  USENIX  Security 
Symposium, 2001. 
[11] M.  Kang,  I.  Moskowitz,  and  D.  Lee.  A  Network  Version  of 
the Pump. In Proc. of the 1995 IEEE Symposium in Security 
and Privacy, 1995.  
[12] G.  Malan,  D.Watson,  F.  Jahanian,  and  P.  Howell.  Transport 
and  Application  Protocol  Scrubbing.  In  Proc.  of  the  IEEE 
INFOCOM 2000 Conference, 2000. 
[13] S. McCamant and M. Ernst. Quantitative Information Flow as 
Network  Flow  Capacity.  In  Proc.  of  the  ACM  SIGPLAN 
Conference  on  Programming  Language  Design  and 
Implementation (PLDI), 2008. 
[14] Mozilla. 
The 
Firefox 
Web 
Browser. 
http://www.mozilla.com/firefox/, 2008. 
[15] Mozilla. 
(Javscript-C) 
http://www.mozilla.org/js/spidermonkey/, 2008. 
SpiderMonkey 
Engine. 
[16] A. Myers, N. Nystrom, L. Zheng, and S. Zdancewic. Jif: Java 
information flow. http://www.cs.cornell.edu/jif, 2001. 
[17] R.  Richardson.  CSI  Computer  Crime  and  Security  Survey. 
v2.gocsi.com/pdf/CSISurvey2007.pdf, 
http://i.cmpnet.com/ 
2007. 
[18] RSA  Security,  Inc.  RSA  Data  Loss  Prevention  Suite.  RSA 
Brief, 
Solution 
http://www.rsa.com/products/EDS/sb/DLPST_SB_1207-
lowres.pdf, 2007. 
[19] N.  Schear,  C.  Kintana,  Q  Zhang,  and  A.  Vahdat.  Glavlit: 
Preventing  Exfiltration  at  Wire  Speed.  In  Proc.  of  the  5th 
Workshop on Hot Topics in Networks (HotNets), 2006. 
[20]  J. Seward. bzip2 and libbzip2, version 1.0.5 â€“ A Program and 
Compression. 
Library 
http://www.bzip.org/1.0.5/bzip2-manual-1.0.5.html, 2007. 
Data 
for 
[21] C. Shannon. Prediction and Entropy of Printed English. Bell 
System Technical Journal, 30:50â€“64, 1951. 
[22] S. Servetto and M. Vetterli. Communication Using Phantoms: 
Covert  Channels  in  the  Internet.  In  Proc.  of  the  IEEE 
International Symposium on Information Theory, 2001. 
[23] Sun Microsystems. Java. http://www.java.com, 2008. 
[24] VONTU. Data Loss Prevention, Confidential Data Protection 
â€“ Protect Your Data Anywhere. http://www.vontu.com, 2008. 
[25] R.  Wagner  and  M.  Fischer.  The  String-to-String  Correction 
Problem. Journal of the ACM, 21(1):168â€“173, 1974.  
[26] Websense, Inc. Web Security, Internet Filtering, and Internet 
http://www.websense.com/global/en/, 
Security  Software. 
2008. 
[27] A.  Yumerefendi,  B.  Mickle,  and  L.  Cox.  TightLip:  Keeping 
applications  from  spilling  the  beans.  In  Proc.  of  the  4th 
USENIX  Symposium  on  Networked  Systems  Design  and 
Implementation(NSDI), 2007. 
140
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply.