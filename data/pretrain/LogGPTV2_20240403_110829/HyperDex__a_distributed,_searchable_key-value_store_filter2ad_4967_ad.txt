value-dependent chains.
a client’s send buﬀer, even before it has been seen by any
server. Cassandra’s default behavior considers a put com-
plete when it is queued in the ﬁlesystem cache of just one
replica. Unlike these systems, the latency of a HyperDex
put operation includes the time taken to fully replicate the
object on f + 1 servers. Because MongoDB does not wait
for full fault tolerance, it is able to complete a majority of
operations in less than 1 ms; however, it exhibits a long-
tail (Figure 7) that adversely impacts average throughput.
Similarly, Cassandra completes most operations in less than
2 ms. Despite its stronger fault-tolerance guarantees, Hy-
perDex completes 99% of its operations in less than 2 ms.
6.2 Search vs. Scan
Unlike existing key-value stores, HyperDex is architected
from the ground-up to perform search operations eﬃciently.
Current applications that rely on existing key-value stores
emulate search functionality by embedding additional infor-
mation about other attributes in the key itself. For exam-
ple, applications typically group logically related objects by
using a shared preﬁx in the key of each object, and then
rely upon the key-value store to locate keys with the com-
mon preﬁx.
In Cassandra, this operation is eﬃcient be-
cause keys are stored in sorted order, and returning all log-
ically grouped keys is an eﬃcient linear scan. Fittingly, the
YCSB benchmark calls this a scan operation. HyperDex’s
search functionality is a strict superset of the scan opera-
tion. Rather than using a shared preﬁx to support scans,
HyperDex stores, for each object, the preﬁx and suﬃx of
the key as two secondary attributes. Scans are then imple-
Figure 8: SEARCH performance. Latency distribution
for 10,000 operations consisting of 95% range queries
and 5% inserts with keys selected from a Zipf distri-
bution. HyperDex is able to oﬀer signiﬁcantly lower
latency for non-primary key range queries than the
other systems are able to oﬀer for primary-key range
queries.
mented as a multi-attribute search that exactly matches a
provided preﬁx value and a provided range of suﬃx values.
Thus, all YCSB benchmarks involving a scan operation op-
erate on secondary attributes in HyperDex, but operate on
the key for other systems.
Despite operating on secondary attributes instead of the
key, HyperDex outperforms the other systems by an or-
der of magnitude for scan operations (Figure 8). Seventy
ﬁve percent of search operations complete in less than 2 ms,
and nearly all complete in less than 6 ms. Cassandra sorts
data according to the primary key and is therefore able to
retrieve matching items relatively quickly. Although one
could alter YCSB to use Cassandra’s secondary indexing
schemes instead of key-based operations, the result would
be strictly worse than what is reported for primary key op-
erations. MongoDB’s sharding maintains an index; conse-
quently, scan operations in MongoDB are relatively fast.
The search performance of HyperDex is not attributable
to our eﬃcient implementation as search is more than an
order of magnitude faster in HyperDex, which eclipses the
2-4× performance advantage observed for get/put through-
put. Hyperspace hashing in HyperDex ensures that search
results are located on a small number of servers; this en-
ables eﬀective pruning of the search space and allows each
search to complete by contacting exactly one host in our
experiments.
An additional beneﬁt of HyperDex’s aggressive search prun-
ing is the relatively low latency overhead associated with
search operations. Figure 9 shows the average latency of a
1000
100
)
s
m
(
y
c
n
e
t
a
L
10
1
0.1
1
Cassandra
MongoDB
HyperDex
10
50
Percent scan/search
Figure 9: The eﬀect of an increasing scan workload
on latency. HyperDex performs signiﬁcantly better
than the other systems even as the scan workload
begins to approach 50%.
y
c
n
e
t
a
L
)
s
m
(
t
u
p
h
g
u
o
r
h
T
)
s
/
s
p
o
d
n
a
s
u
o
h
t
(
7
6
5
4
3
2
1
0
14
12
10
8
6
4
2
0
HyperDex
4
2
8
Number of Subspaces
6
10
Figure 10: Latency and throughput for put oper-
ations as a function of non-key subspaces The er-
ror bars indicate standard deviation from 10 exper-
iments. Latency increases linearly in the length of
the chain, while throughput decreases proportion-
ally. In applications we have built with HyperDex,
all tables have three or fewer subspaces.
single scan operation as the total number of scan operations
performed increases. In this test, searches were constructed
by choosing the lower bound of the range uniformly at ran-
dom from the set of possible values, as opposed to workload
E which uses a Zipf distribution to select objects. Using a
uniform distribution ensures random access, and mitigates
the eﬀects of object caching. HyperDex consistently oﬀers
low latency for search-heavy workloads.
A critical parameter that aﬀects HyperDex’s search per-
formance is the number of subspaces in a HyperDex table.
Increasing the number of subspaces leads to additional op-
portunities for pruning the search space for search opera-
tions, but simultaneously requires longer value-dependent
chains that result in higher put latencies.
In Figure 10,
we explore the tradeoﬀ using between zero and ten addi-
tional subspaces beyond the mandatory key subspace. Note
that adding ten additional subspaces increases the value-
dependent chain to be at least 22 nodes long. As expected,
HyperDex’s put latency increases linearly with each addi-
tional subspace.
)
s
/
s
p
o
n
o
i
l
l
i
m
(
t
u
p
h
g
u
o
r
h
T
4
3
2
1
0
4 Clients
8 Clients
16 Clients
32 Clients
48 Clients
4
8
12
16
20
Nodes
24
28
32
Figure 11: HyperDex scales horizontally. As more
servers are added, aggregate throughput increases
linearly. Each point represents the average through-
put of the system in steady state over 30 second
windows. The error bars show the 5th and 95th per-
centiles.
6.3 Scalability
We have deployed HyperDex on the VICCI [42] testbed to
evaluate its performance in an environment representative of
the cloud. Each VICCI cluster has 70 Dell R410 PowerEdge
servers, each of which has 2 Intel Xeon X5650 CPUs, 48 GB
of RAM, three 1 TB hard drives, and two 1 Gbit ethernet
ports. Users are provided with an isolated virtual machine
for conducting experiments. Each virtual machine comes
preinstalled with Fedora 12 and runs the 2.6.32 Linux kernel.
We examined the performance of a HyperDex cluster as
the cluster increases in size. Increasing the number of servers
in the cluster provides HyperDex with additional resources
and leads to a proportional increase in throughput. In Fig-
ure 11, we explore the change in system throughput as re-
sources are added to the cluster. As expected, HyperDex
scales linearly as resources are added to the cluster. Each
point in the graph represents the average throughput ob-
served over a 30 second window and the error bars show
the 5th and 95th percentiles observed over any 1-second win-
dow. At its peak, HyperDex is able to average 3.2 million
operations per second.
The workload for Figure 11 is a 95% read, 5% write work-
load operating on 8 B keys and 64 B values. The measure-
ments reported are taken in steady state, with clients ran-
domly generating requests according to the workload. This
workload and measurement style reﬂects the workload likely
to be encountered in a web application. The reported mea-
surements exclude the warm-up time for the system.
In
all experiments, 15 seconds was suﬃcient to achieve steady
state. Clients operate in parallel, and are run on separate
machines from the servers in all but the largest conﬁgura-
tions. Clients issue requests in parallel, and each client main-
tains an average of 1,000 outstanding requests per server.
Increasing the number of clients does not signiﬁcantly im-
pact the achievable average throughput.
This experiment shows that a medium-sized HyperDex
cluster is able to achieve high throughput for realistically
sized deployments [3]. Additional resources allow the cluster
to provide proportionally better throughput.
7. RELATED WORK
Database system Storage systems that organize their data
in high-dimensional spaces were pioneered by the database
community more than thirty years ago [5,7,24,31,39,41,51].
These systems, collectively known as Multi-Dimensional Da-
tabases (MDB), leverage multi-dimensional data structures
to improve the performance of data warehousing and on-
line analytical processing applications. However, unlike hy-
perspaces in HyperDex, the data structures in MDBs are
designed for organizing data on a single machine and are
not directly applicable to large-scale distributed storage sys-
tems. Alternatively, more recent database systems [1, 17]
have begun exploring eﬃcient mechanisms for building and
maintaining large-scale, tree-based distributed indices.
In
contrast, the mapping HyperDex constructs is not an index.
Indices must be maintained and updated on object inser-
tion. Hyperspace hashing, on other hand, is a mapping that
does not change as objects are inserted and removed.
Peer-to-peer systems Past work in peer-to-peer systems
has explored multi-dimensional network overlays to facili-
tate decentralized data storage and retrieval. CAN [47] is a
distributed hash-table that, much like HyperDex, organizes
peers in a multi-dimensional space. However, CAN only pro-
vides key inserts and lookups; the purpose of CAN’s multi-
dimensional peer conﬁguration is to limit a CAN node’s
peer-set size to provide eﬃcient overlay routing. HyperDex
provides search, and does not do routing in the space.
MURK [21], SkipIndex [58], and SWAM-V [30] dynam-
ically partition the multi-dimensional space into kd-trees,
skip graphs, and Voronoi diagrams respectively to provide
multi-dimensional range lookups. Although conceptually
similar to HyperDex, providing coordination and manage-
ment of nodes for these dynamic space partitioning schemes
is signiﬁcantly more complex and error-prone than for Hy-
perDex’s static space partitioning and require additional op-
erational overhead. These systems also do not address sev-
eral critical and inherent problems associated with mapping
structured data into a multi-dimensional space and provid-
ing reliable data storage. Speciﬁcally, they are not eﬃcient
for high dimensional data due to the curse-of-dimensionality,
and they either lack data replication or provide only even-
tually consistent operations on replicated data. Addressing
these problems by augmenting dynamic space partitioning
schemes with subspaces and value-dependent chaining would
further increase the complexity and overhead of node coor-
dination and management. Mercury [8] builds on top of a
Chord [55] ring, and uses consistent hashing [29] on each
attribute as secondary indexes. Although Mercury’s imple-