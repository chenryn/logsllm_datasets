### Impact of Memory Deduplication on Migration

Memory deduplication, if enabled, will lose its benefits during migration. Consequently, if more aggressive memory deduplication is deployed to maintain memory savings, existing side channels that rely on memory deduplication (e.g., [41, 20]) may become even stronger. We acknowledge that this is a limitation of Nomad, and studying the implications of migration on side channels is an interesting direction for future work.

### Other Cloud Threats

Cross-VM side channels are not the only risks in cloud environments. Another significant risk is a compromised hypervisor. Constantly migrating VMs may increase the likelihood of a VM being placed on a machine with a compromised hypervisor. However, existing defenses for compromised hypervisors (e.g., [8, 39]) can be used in conjunction with Nomad. Addressing this threat is another interesting direction for future research.

## System Implementation

In this section, we describe our Nomad prototype, implemented using OpenStack [5], following the structure in Figure 2. We begin with a high-level overview of OpenStack before detailing how we modified it to implement Nomad.

### OpenStack Overview

OpenStack is a popular open-source cloud computing software platform used to deploy Infrastructure-as-a-Service (IaaS) cloud solutions [5]. At a high level, OpenStack controls Compute, Storage, and Network resources. The key component of interest to us is OpenStack Compute, known as Nova, which consists of a cloud controller representing a global state and other compute nodes (i.e., machines). Each compute node runs a hypervisor, in our case, KVM, which manages VMs and executes migrations via OpenStack API calls.

### Migration Choices

OpenStack supports different modes of VM migration [3]:
1. **Non-live migration**: This mode shuts down the instance for a period to migrate to another hypervisor.
2. **Live migration**: This mode has almost zero instance downtime.

To minimize the impact on client applications, we chose live migration. Within live migration, there are several implementation options [3]:
- **Shared storage-based live migration**
- **Block live migration**
- **Volume-backed live migration**

Generally, shared storage and block live migration perform better than volume-backed live migration because they avoid copying the entire VM image from the source to the destination. For implementation convenience in our testbed, we chose the shared storage live migration option.

### Migration Engine

The Nomad Migration Engine executes VM migrations as dictated by the Placement Algorithm. We implemented the engine in the Nova-Scheduler at the Controller node, which was a natural choice as Nomad requires a global view of all machines and VM states. We extended the code to include the Migration Engine as part of the controller services.

The high-level workflow is as follows:
1. When VMs are launched, the Migration Engine saves the VM ID and the client ID to its internal client-to-VM mapping.
2. At every epoch, the Migration Engine queries OpenStack’s database to get the VM-to-host mappings.
3. The Migration Engine offloads the job to the Placement Algorithm to compute the VM assignments.
4. Once the algorithm finishes computing the VM placements, the Migration Engine executes the migrations as dictated by the algorithm.

Our implementation consists of approximately 200 lines of Python code in the Nova-Scheduler and achieves our goal of minimal modification to the existing cloud platform.

### Placement Algorithm

Our implementation of the Placement Algorithm consists of 2,000 lines of custom C++ code. This module is invoked every epoch by an API call from the Migration Engine. Upon receiving the high-level inputs described in §4, the algorithm computes the VM assignments and stores co-residency history for subsequent epochs. All the optimizations described in §5 are implemented as part of the Placement Algorithm.

The modular design of Nomad, with a standardized interface between the placement algorithm and the Migration Engine, allows us to easily decouple the scheduling logic from the implementation of the Migration Engine. In our development experience, this "plug-and-play" capability proved quite useful.

## Evaluation

In this section, we address the following questions:
1. How does the information leakage resilience of Nomad’s algorithm compare with strawman solutions?
2. Does Nomad's algorithm scale to large deployments? What are the benefits of the optimizations from §5?
3. What is the impact of migrations on real applications in a realistic cloud workload?
4. How resilient is Nomad to smarter adversaries?

### Setup

For (3), we used a local OpenStack Icehouse deployment on our testbed equipped with:
- 2.50 GHz 64-bit Intel Xeon CPU L5420 processor with 8 cores
- 16 GB RAM
- 500 to 1000 GB disks
- Two network interfaces with 100 Mbps and 1 Gbps network speed
- Each machine runs KVM on Ubuntu 14.04 (Linux kernel v3.13.0)

For (1), (2), and (4), we evaluated Nomad Placement Algorithm and other placement strategies using synthetic workloads. The evaluation of Nomad placement algorithm was conducted using varying cluster sizes with 4 slots each. For our simulation workloads, the number of customers was the same as the cluster size, and the initial setup consisted of 2 VMs per client. Every epoch, 15% of new VMs would arrive, and 15% of existing VMs would depart, creating constant churn. The migration budget was set to 15% for testing our solution and an ILP solution. For end-to-end application performance with Nomad, we used an epoch duration (D) of 4 minutes for web-service and 1 minute for Hadoop MapReduce.

### Information Leakage

We compared the per-client leakage achieved by Nomad versus three strawman solutions:
1. **Integer Linear Programming (ILP)**
2. **Random Scheduler**
3. **Static Scheduler**

- **Random Scheduler**: Picks a VM at random and a random slot. If the slot is occupied, the chosen and occupying VMs are swapped until the migration budget is exhausted.
- **Static Scheduler**: Runs an initial randomized placement when VMs arrive but runs no migration.
- **ILP**: Uses CPLEX [4] to run the exact optimization algorithm.

We compared these approaches on a simulated cluster size of 20 with 4 slots per machine, 20 clients, an expected occupancy rate of 50%, and 15% arrival and 15% departure rates per epoch. We chose a small setup since the ILP does not scale to larger setups.

Figure 4 shows the CDF of inter-client InfoLeakage measured over a sliding window of 5 epochs for the 4 different InfoLeakage models. We make two key observations:
1. Naive random migration or static placement can result in substantially higher leakage.
2. The Nomad Placement Algorithm achieves close-to-optimal performance w.r.t. the ILP solution. There is also good fairness across different clients, as even the 95th percentile of the distribution is low.

### Scalability

#### Nomad vs. ILP

We chose a greedy heuristic due to the scaling limitations of the optimal ILP formulation. Table 1 compares the scaling properties of Nomad vs. ILP, showing that the ILP is fundamentally intractable even for a small cluster size of 40 machines, while Nomad is several orders of magnitude faster. This scalability benefit does not compromise optimality, as shown in Figure 4, where the optimality loss of Nomad is negligible.

#### Scaling to Large Deployments

Our target computation time for Nomad was roughly 1 minute. Figure 5 shows the scaling properties for different datacenter sizes, indicating that a reasonable cluster size is 1,500 machines across different models.

#### Effect of Design Choices

Nomad's scaling properties stem from three key design decisions:
1. **Pruning**
2. **Lazy evaluation**
3. **Incremental computation**

We evaluated the relative benefits of each of these ideas and confirmed that they do not sacrifice optimality. Figure 6a shows that these design choices result in negligible drop in optimality. Figure 6b shows the increased scalability with each idea enabled, demonstrating that each idea is critical for providing an order-of-magnitude reduction in compute time. The largest decrease comes from the use of incremental benefit computation.

### System and Application Performance

#### Migration Microbenchmark

Table 2 shows the total migration time for different images using shared-NFS storage live migration. The migration occurs via a 1 Gbps network, and we envision faster migration in faster networks.

| Image                  | Total Migration Time (s) |
|------------------------|--------------------------|
| Ubuntu-Cloud (512MB RAM, 1.5GB) | 0.89                     |
| Cirros (512MB RAM, 132MB)      | 0.97                     |
| Ubuntu (2048MB RAM, 7GB)       | 1.47                     |

#### End-to-End Experiment

We experimented with two representative workloads: web-server and MapReduce workloads.

##### Wikibench Evaluation

For the web-service application, we chose Wikibench, which uses a real application (Mediawiki) and a website populated with real data dumps [6]. We took the trace file from September 2007 and post-processed it such that the request rate is approximately 10 to 15 HTTP requests per second.

We conducted an experiment in our 20-node setup, launching 4 benign clients and 2 additional clients playing the role of adversaries. Each of the 4 clients had:
- 3 replicated Wikibench backends
- 1 proxy to load balance between 3 servers
- 1 worker instance sending HTTP GET requests using the Wikibench trace file

At each epoch, adversarial clients create 15% arrival and 15% departure churn. In our setup, each benign client requested that the client worker and a proxy be "non-movable." The experiment was conducted for 20 minutes (4 minutes per epoch, 5 as a ∆) with and without our system. Figure 7 shows the distribution of throughput (i.e., number of completed requests per 10-second bin) over the entire run for each client.

Figure 7 shows the distribution of throughput using a box-and-whiskers plot with the 25th, 50th, and 75th percentiles, and the minimum and 98th percentile value. We observe relative resilience of our system to migration, as the distribution is largely identical with and without Nomad. We only observe a decrease in throughput for the lower tail of the distribution, indicating fairness across each client.

##### Hadoop Evaluation

The second representative workload is Hadoop Terasort sorting 800 MB of data. The VM arrival and departure workloads are identical to those of Wikibench, except that the churn was introduced every minute and the epoch size was set to 1 minute. Each Hadoop client consists of 5 VMs (1 master VM and 4 slave VMs). Each client, via the client API, requests that the master node be "non-movable."

Figure 8 shows the distribution of the job completion time from 100 runs. We consider two types of initial placements: random and clustered. Clustered initial placement refers to the setup in which each client is clustered on 2 machines. For each client, we report three categories:
1. With Nomad - random initial placement
2. Without Nomad - random initial placement
3. Without Nomad - optimal (clustered) initial placement

The results show that Nomad does not impact the job performance. Both the Wikibench and Hadoop experiments demonstrate that:
1. Our system prototype can handle real workloads in an open system.
2. Cloud-representative applications are resilient to migration.

### Resilience to Advanced Adversaries

For brevity, we focus on the adversary that exploits the non-uniformity of the system.