migration, memory deduplication(e.g., [36]), if enabled, will
lose its beneﬁts. Therefore, if more aggressive memory dedu-
plication is deployed to preserve memory saving beneﬁts, exist-
ing side channels that rely on memory deduplication (e.g., [41,
20]) may become even stronger. We acknowledge that this is a
limitation of Nomad and studying the implication of migration
on side channels is an interesting direction for future work.
• Other cloud threats: Cross-VM side channels are not the only
risks in cloud environments. For instance, a separate risk in
cloud deployments is a compromised hypervisor. Constantly
migrating VMs may increase the risk of a VM being placed on
a machine with a compromised hypervisor. However, existing
defenses for compromised hypervisors could also be used in
conjunction with Nomad (i.e., [8, 39]). Addressing this threat
is an interesting direction for future work.
7 System Implementation
In this section, we describe our Nomad prototype (using OpenSta-
ck [5]) following the structure in Figure 2. We begin with a high-
level overview of OpenStack before describing how we modiﬁed
it to implement Nomad.
OpenStack overview: OpenStack is a popular open-source cloud
computing software platform that is used to deploy Infrastructure-
as-a-Service (IaaS) cloud solutions [5]. At a high level, Open-
Stack controls Compute, Storage, and Network resources. The
key component of interest to us is OpenStack Compute, known
as Nova, which consists of the cloud controller representing a global
state and other compute nodes (i.e., machines). Each compute node
runs a hypervisor, in our case, KVM, which is responsible for man-
aging VMs and executing the migration of VM invoked via the
OpenStack API calls.
Migration choices: OpenStack supports different modes of VM
migration [3]: 1) Non-live migration which shuts down instance for
a period of time to migrate to another hypervisor; and 2) Live mi-
gration which has almost zero instance downtime. To allow for
minimal impact on client application, the natural choice was to use
live migration. Within live migration, there are several implementa-
tion options [3]: shared storage-based live migration, block live mi-
gration, and volume-backed live migration. In general, shared stor-
age and block live migration have better performance that volume-
backed live migration as they avoid copying the entire VM image
from the source compute nodes to the destination. For implementa-
tion convenience in our testbed, we choose the shared storage live
migration option.
Migration Engine: Recall the role of the Nomad Migration En-
gine, which executes the migration of VMs as dictated by the Place-
ment Algorithm. We implement the engine in the Nova-Schedul-
er at the Controller node, which was a natural implementation
choice as Nomad requires having a global view of all the machines
and VM states. We extended the code to implement Migration En-
gine as part of the controller services.
The high-level workﬂow is as follows. First, when VMs are
launched, the Migration Engine saves the VM ID and the client
ID to its internal client-to-VM mapping. At every epoch, Migra-
tion Engine queries OpenStack’s database to get the VM-to-host
mappings. Then, Migration Engine ofﬂoads the job to the Place-
ment Algorithm to compute the VM assignments. Once the algo-
rithm ﬁnishes computing the VM placements, the Migration En-
gine executes migrations as dictated by the algorithm.
Recall that one of the goals of Nomad is the minimal modiﬁca-
tion to an existing cloud platform. Our implementation consists of
roughly 200 lines of Python code in the Nova-Scheduler code
and achieves our objectives of minimal modiﬁcation.
Placement Algorithm: Our implementation of the Placement Al-
gorithm consists of 2,000 lines of custom C++ code. This mod-
ule is invoked every epoch by an API call from the Migration En-
gine. Upon the call from the Migration Engine, which sends the
high-level inputs described in §4, the algorithm computes the VM
assignments and also internally stores co-residency history to be
used in subsequent epochs. All the optimizations described in §5
are implemented as part of the Placement Algorithm.
Finally, we note that the modular design of Nomad with a stan-
dardized interface between the placement algorithm and the Mi-
gration Engine allows us to easily decouple the scheduling logic
from the implementation of the Migration Engine. In our own de-
velopment experience, this “plug-and-play” capability proved quite
useful.
8 Evaluation
In this section, we address the following questions:
(1) How does the information leakage resilience of Nomad’s algo-
rithm compare with strawman solutions?
(2) Does Nomad algorithm scale to large deployments? What are
the beneﬁts of the optimizations from §5?
(3) What is the impact of migrations on real applications in a real-
istic cloud workload?
(4) How resilient is Nomad to smarter adversaries ?
Setup: For (3), we use a local OpenStack Icehouse deployment on
our testbed equipped with 2.50 GHz 64-bit Intel Xeon CPU L5420
processor with 8-cores, 16 GB RAM, 500 to 1000GB disks, and
two network interfaces with 100Mbps and 1Gbps network speed.
Each machine runs KVM on Ubuntu 14.04 (Linux kernel v3.13.0).
For (1), (2), and (4), we evaluated Nomad Placement Algorithm and
other placement strategies using synthetic workloads. The evalua-
tion of Nomad placement algorithm was conducted using varying
cluster sizes with 4 slots each. For our simulation workloads, the
number of customers was the same as the cluster size and the ini-
tial setup consisted of 2 VMs per clients.4 Every epoch a 15% of
new VMs would arrive and 15% of an existing VMs would depart,
creating constant churn every epoch. The migration budget was set
to 15% for testing our solution and an ILP solution. In testing the
end-to-end application performance with Nomad, we used epoch
duration (i.e., D) of 4 min for web-service and 1 min for Hadoop
MapReduce.5
8.1
We compare the per-client leakage achieved by Nomad vs. three
strawman solutions: (1) Integer Linear Programming, (2) Random
Scheduler, and (3) Static Scheduler. The random scheduler picks
a VM at random and a random slot. The VM picked at random is
inserted to the slot if the slot is empty. If the slot is occupied, the
4Note that the use of 4 slots per server and 2 VMs per client was
bottlenecked by the run-time of the ILP and is not fundamental
limitation of the algorithm.
5Different Ds are used because the job completion time for each
experiment (i.e., Wikibench and Hadoop) differs. Therefore, D
was scaled such that the number of epochs, hence the number of
migrations, in each experiment is roughly the same for both exper-
iments (i.e., D = 4 min for 20 min completion time and D = 1
min for 3–4 min completion time).
Information Leakage
(a) (cid:104)R, C(cid:105) - CDF
(b) (cid:104)NR, C(cid:105) - CDF
(c) (cid:104)R, NC(cid:105) - CDF
(d) (cid:104)NR, NC(cid:105) - CDF
Figure 4: CDF of client-to-client InfoLeakage for different VM placement strategies
ILP
Nomad scheduler
10
0.48s
0.00005s
Cluster Size
20
40
7.83
0.002s
>1 day
0.015s
50
>1 day
0.02s
Table 1: Scalability of ILP vs. Nomad
chosen and occupying VMs are swapped. This process of swapping
is run until all the migration budget is exhausted. The static sched-
uler runs an initial randomized placement when VMs arrive but
runs no migration. The ILP (Appendix A) runs the exact optimiza-
tion algorithm using CPLEX [4]. We compare these approaches on
a simulated cluster size of 20 with 4 slots per machine, 20 clients,
with an expected occupancy rate of 50% and 15% arrival and 15%
departure rates per epoch. We chose a small setup since the ILP
does not scale to larger setups.
Figure 4 shows the CDF of inter-client InfoLeakage measured
over a sliding window of 5 epochs for the 4 different InfoLeakage
models. We make two key observations. First, naive random mi-
gration or static placement can result in substantially higher leak-
age. Second, the Nomad Placement Algorithm achieves close-to-
optimal performance w.r.t. the ILP solution.6 Finally, there is one
subtle observation regarding fairness across different clients; i.e.,
do some clients incur more leakage or migration relative to oth-
ers? By intent, the ILP or the Nomad algorithm does not explicitly
take fairness into account. But we ﬁnd that both ILP and Nomad
achieve good fairness in practice as even the 95th percentile of the
distribution is low.
8.2 Scalability
Nomad vs. ILP: Recall that we chose a greedy heuristic because
of the scaling limitations of the optimal ILP formulation. First, we
compare the scaling properties of Nomad vs. ILP in Table 1. The
result shows that the ILP is fundamentally intractable even for a
small cluster size of 40 machines and that Nomad is several orders
of magnitude faster. Note that this scalability beneﬁt does not com-
promise optimality as we saw in Figure 4 where the optimality loss
of Nomad is negligible.
Scaling to large deployments: Our target computation time for
Nomad was roughly 1 min. Next, we analyze the scalability of
the scheduler to determine the dimensions of the cluster that can
meet this target. Figure 5 shows the scaling properties for different
datacenter sizes. Based on the result, we can determine that a rea-
sonable size of our cluster is 1,500 machines across the different
models.
6Note that both the ILP and Nomad Greedy optimize the total In-
foLeakage. Therefore, it is possible for Nomad Greedy to outper-
form the ILP solution w.r.t. the inter-client InfoLeakage.
Figure 5: Scalability for different InfoLeakage models
Effect of design choices: The scaling properties of Nomad stem
from three key design decisions in Appendix §5): (1) pruning, (2)
lazy evaluation, and (3) incremental computation. Next, we evalu-
ate the relative beneﬁts of each of these ideas and also conﬁrm that
these do not sacriﬁce optimality.
To tease apart the relative contributions, we evaluate the impact
of applying these ideas progressively as follows: (1) We begin with
a Baseline Greedy which is a naive implementation of Algorithm 1
that considers three types of moves: free-insert, pair-wise swaps,
and 3-way swaps;7 and (2) We prune the search space to only con-
sider free inserts and pair-wise swaps; (3) We enable lazy evalua-
tion to eliminate the need to re-compute the move table after every
state changes (every move); (4) Last, we enable the incremental
computation to efﬁciently compute the beneﬁt of each move. Note
that (4) also entails the use of “Soft-Max” of InfoLeakage calcula-
tions for non-(cid:104)R, C(cid:105) models.
Figure 6a shows that these design choices result in negligible
drop in optimality. We could only show the optimality results for
a 50-node cluster because the basic greedy takes several hours to
complete for larger sizes. Figure 6b also shows the increased scal-
ability with each idea enabled and shows that each idea is critical
to provide an order-of-magnitude reduction in compute time. We
also see that the largest decrease comes from the use of incremental
beneﬁt computation.
(a) Optimality loss
(b) Compute time
Figure 6: Impact of key design ideas in scaling the Nomad
Placement Algorithm and justifying that these do not sacriﬁce
optimality
7Even adding 3-way moves dramatically increases the run time and
thus we do not consider more complex moves.
 0.75 0.8 0.85 0.9 0.95 1 0 2 4 6 8 10CDFClient-to-Client InfoLeak (epoch)ILPNomad GreedyStatic - No MoveRandom - 20% budget 0.75 0.8 0.85 0.9 0.95 1 0 2 4 6 8 10CDFClient-to-Client InfoLeak (epoch)ILPNomad GreedyStatic - No MoveRandom - 20% budget 0.75 0.8 0.85 0.9 0.95 1 0 2 4 6 8 10CDFClient-to-Client InfoLeak (epoch)ILPNomad GreedyStatic - No MoveRandom - 20% budget 0.75 0.8 0.85 0.9 0.95 1 0 2 4 6 8 10CDFClient-to-Client InfoLeak (epoch)ILPNomad GreedyStatic - No MoveRandom - 20% budget 0.01 0.1 1 10 100 1000 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000Average runtime per epoch (s)Nomad greedy runtime for varying cluster sizes, 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1 0 1 2 3 4 5 6 7 8CDFClient-to-Client InfoLeakage (epoch)Baseline GreedyBaseline Greedy + PruningBaseline Greedy+Pruning+LazyEvalNomad Greedy 0.0001 0.001 0.01 0.1 1 10 100 1000 10000 0 200 400 600 800 1000 1200 1400 1600 1800 2000Avg runtime per epoch (s)Cluster sizeBaseline GreedyBaseline Greedy + PruningBaseline Greedy+Pruning+LazyEvalNomad Greedy8.3 System and Application Performance
Migration microbenchmark: First, we start with microbench-
marking the time it takes to migrate three instances using shared-
NFS storage live migration (Table 2) [3]. Note that the migration
occurs via 1Gbps network and we envision faster migration in faster
networks.
Ubuntu-Cloud
(512MB RAM, 1.5GB)
Cirros
(512MB RAM, 132MB)
Ubuntu
(2048MB RAM, 7 GB)
0.89
0.97
1.47
Total Migration
Time (s)
Table 2: Total migration time for different images (in s)
As an end-to-end experiment, we also experimented with an ap-
plication benchmark to see the performance impact at the applica-
tion level which combines all CPU, disk I/O and network I/O of
application. We chose two representative workloads: web-server
and MapReduce workloads.
Wikibench evaluation: For web-service application, we choose
Wikibench because it uses a real application (Mediawiki) and web-
site is populated with real data dumps [6]. We took the trace ﬁle
from Sept. 2007 and post-processed it such that the request rate is
approximately 10 to 15 HTTP requests per sec.
We conduct an experiment in our 20-node setup. Initial setup
includes launching 4 benign clients and 2 additional clients whom
for the purpose of illustration play a role of an adversary.8 Each of
4 client has the following setup: 1) 3 replicated Wikibench back-
ends 2) 1 proxy to load balance between 3 servers, and 3) a worker
instance sending HTTP GET requests using the Wikibench trace
ﬁle. At each epoch, adversarial clients create 15% arrival and 15%
departure churn. In our setup, each benign client requests that the
Figure 7: Distribution of throughput for Wikibench workload
(with and without Nomad)
client worker and a proxy to be “non-movable.” The experiment
was conducted for 20 min (4 min per epoch, 5 as a ∆) with and
without our system. We present the distribution of the throughput
(i.e., number of completed requests per 10s bin) over the entire run
for each client (Figure 7).
For each client running the Wikibench workload, Figure 7 shows
the distribution of throughput using a box-and-whiskers plot with
the 25th, 50th, and 75th percentiles, and the minimum and 98th
percentile value. We observe relative resilience of our system to
migration as the distribution is largely identical with and without
Nomad. We only observe the decrease in throughput for the lower
tail of the distribution. This plot also shows the fairness across
each client as no particular client is being penalized by incurring
high performance degradation.
Hadoop evaluation: The second representative workload is Hado-
op Terasort sorting 800MB data. The VM arrival and departure
workloads are identical with that of Wikibench except that the churn
was introduced every minute and the epoch size was set to 1 min.
Each Hadoop client consists of 5 VMs (i.e.,1 master VM and 4
slave VMs). Each client, via the client API, requests that the master
node to be “non-movable”. The results are shown in Figure 8. For
this experiment, we report the distribution of the job completion
time from 100 runs. We consider two types of initial placements
(i.e., random vs. clustered). Clustered initial placement refers to
the setup in which each client is clustered on 2 machines. Thus, for
each client, we report three categories: 1) with Nomad- random ini-
tial placement; 2) without Nomad- random initial placement; and 3)
without Nomad- optimal (i.e., clustered) initial placement. The re-
sults show that Nomad does not impact the job performance. Both
Wikibench and Hadoop experiments demonstrate that: (1) our sys-
tem prototype can handle real workloads in an open system; and
(2) cloud-representative applications are resilient to migration.
Figure 8: Distribution of job completion time for Hadoop
workload (with and without Nomad)
8.4 Resilience to advanced adversaries
For brevity, we only focus on the adversary that exploits the non-