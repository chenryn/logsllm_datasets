but towards a different goal: allow switches to handle state larger
than their on-device memory. It does not address fault tolerance or
multi-writer consistency.
Switch-based reliability protocols. Other recent work runs co-
ordination protocols between switches to build reliable storage [29,
37]. Our goal is conceptually different – to replicate state for in-
switch applications rather than provide a networked storage service
– but uses some similar mechanisms, like network sequencing [46].
9 Conclusions
While many recent efforts have demonstrated the potential benefits
of running datacenter functions on programmable switches, we ar-
gue that there is one critical missing piece in current designs, which
is fault tolerance. To address this issue, in this paper, we present
RedPlane, which provides a fault tolerant state store abstraction for
in-switch applications. We formally define a linearizability-based
correctness model for a replicated switch data plane state and build
a practical replication protocol based on it. Our evaluation with
various stateful applications on a real testbed shows that RedPlane
can support fault-tolerance with minimal performance and resource
overheads and enable end-to-end performance to quickly recover
from switch failures.
Ethics: This work does not raise any ethical issues.
Figure 14: End-to-end throughput changes during failover
and recovery with and without RedPlane.
Figure 15: Switch packet buffer occupancy due to request
buffering.
passes through a NAT running on the programmable switches. We
compare changes in TCP throughput when (1) there is no failure
(Baseline), (2) one switch fails without RedPlane (Failure), (3) one
switch fails while using RedPlane (Failure+RedPlane).
Fig. 14 shows the results. In a network without RedPlane, when
Switch-1 fails, packets are rerouted to another switch and dropped,
breaking the TCP connections. In contrast, RedPlane-enabled NAT
successfully maintains high throughput when the switch fails and
recovers after short disruptions (0.9 and 1.0 seconds). This recovery
time is affected both by the core switch’s failure detection/rerouting
time and RedPlane’s lease period (set to 1 second here). Control
plane and state store optimizations could further reduce this.
7.4 RedPlane Switch ASIC Resource Usage
Packet buffer usage. In this experiment, we evaluate the overhead
of our request buffering mechanism (§5.2). Since RedPlane buffers
a replication request until it receives a reply corresponding to the
request from the state store, it consumes some amount of the switch
packet buffer. Since there is no precise way of measuring the buffer
usage in real-time, we instead use the queue depth information
provided by the switch ASIC to estimate the upper bound of the
buffer occupancy.11 Specifically, we assume a write-centric applica-
tion where every incoming packet issues a request (i.e., the most
demanding scenario). And we let each request packet record its
queue depth information to a P4 register in the data plane and read
it from the control plane for every second and take the maximum
value. We generate packets from a traffic generation server while
varying the traffic rate and the request loss rate.12 Fig. 15 shows
the result. When there is no request loss, the buffer occupancy is
less than 1.5 KB even at 100 Gbps traffic rate. As we increase the
request loss rate, the buffer usage also grows; when the traffic rate
11It is a per-packet queue depth measured when a packet is dequeued from the buffer,
and the Tofino ASIC provides this information as an intrinsic metadata that can be
accessed at the egress pipeline.
12We emulate the request loss by dropping requests at a certain probability at the
switch.
234
051015202530354045505560Time(sec)050100Throughput(Gbps)Switch-1failedSwitch-1recoveredBaseline(nofailure)Failure+RedPlaneFailure20406080100Traﬃcrate(Gbps)0102030Buﬀeroccupancy(KB)0%loss1%loss2%lossRedPlane: Enabling Fault-Tolerant Stateful In-Switch Applications
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Acknowledgments
We would like to thank the anonymous SIGCOMM reviewers and
our shepherd, Mythili Vutukuru, for their insightful comments and
constructive feedback. This work was supported in part by the
CONIX Research Center, one of six centers in JUMP, a Semicon-
ductor Research Corporation (SRC) program sponsored by DARPA,
and by NSF award 1700521. Daehyeok Kim was also supported by
the Microsoft Research PhD Fellowship.
References
[1] 2009. 2009-M57-Patents packet trace. http://downloads.digitalcorpora.org/
corpora/scenarios/2009-m57-patents/net/.
[2] 2010. Data Set for IMC 2010 Data Center Measurement. http://pages.cs.wisc.edu/
~tbenson/IMC10_Data.html.
[3] 2011.
RDMA Aware Networks Programming User Manual.
//www.mellanox.com/related-docs/prod_software/RDMA_Aware_
Programming_user_manual.pdf.
[4] 2012. A signaling storm is gathering - Is your packet core ready? https://www.
nokia.com/blog/a-signaling-storm-is-gathering-is-your-packet-core-ready/.
[5] 2017. vEPC Acceleration Using Agilio SmartNICs. https://www.netronome.com/
https:
[6] 2018. Advanced Network Telemetry. https://www.barefootnetworks.com/use-
media/documents/SB_vEPC.pdf.
cases/ad-telemetry/.
[7] 2018.
Barefoot Networks Unveils Tofino 2,
the Next Generation
of
the World’s First Fully P4-Programmable Network Switch ASICs.
https://www.barefootnetworks.com/press-releases/barefoot-networks-unveils-
tofino-2-the-next-generation-of-the-worlds-first-fully-p4-programmable-
network-switch-asics/..
[8] 2018. Cavium Xpliant Ethernet Switches. https://www.cavium.com/xpliant-
ethernet-switch-product-family.html.
[9] 2018.
Offloading VNFs to programmable switches using P4.
https:
//wiki.onosproject.org/download/attachments/12420314/p4-vnf-offloading-
ons2018.pdf.
[10] 2019. Cisco Visual Networking Index. https://www.cisco.com/c/en/us/solutions/
collateral/service-provider/visual-networking-index-vni/white-paper-c11-
738429.html.
[11] 2019. P416 Language Specification. https://p4.org/p4-spec/docs/P4-16-v1.2.0.
[12] 2020. Barefoot P4 Studio. https://www.barefootnetworks.com/products/brief-p4-
html.
studio/.
[13] 2020. Cisco Silicon One Q200 and Q200L Processors Data Sheet. https://www.
cisco.com/c/en/us/solutions/collateral/silicon-one/datasheet-c78-744312.html.
https://www.intel.com/content/www/us/en/products/
Intel Tofino.
[14] 2020.
network-io/programmable-ethernet-switch/tofino-series/tofino.html.
[15] 2020. iperf(1) - Linux man page. https://linux.die.net/man/1/iperf.
[16] 2020. Redis. https://redis.io/.
[17] 2020. The Evolved Packet Core. https://www.3gpp.org/technologies/keywords-
acronyms/100-the-evolved-packet-core.
[18] 2020. The TLA+ Home Page. https://lamport.azurewebsites.net/tla/tla.html.
[19] 2020. Trident4 / BCM56880 Series. https://www.broadcom.com/products/
ethernet-connectivity/switching/strataxgs/bcm56880-series.
[20] 2021. RedPlane Public Repository. https://github.com/daehyeok-kim/redplane-
public.
[21] 3GPP. 2012. 3GPP TS 23.007: Restoration procedures.
[22] Alexey Andreyev. 2014.
Facebook data center network.
engineering/introducing-data-center-fabric-the-next-generation-facebook-
data-center-network/.
Introducing data center fabric, the next-generation
https://engineering.fb.com/production-
[23] Arista Networks. 2020. Arista 7170 Series. https://www.arista.com/en/products/
[24] Barefoot Networks. 2017. Tofino Switch Architecture Specification (accessible
7170-series.
under NDA).
[25] Burton H. Bloom. 1970. Space/Time Trade-offs in Hash Coding with Allowable
Errors. Commun. ACM 13, 7 (July 1970), 422–426.
[26] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin
Izzard, Fernando Mujica, and Mark Horowitz. 2013. Forwarding Metamorphosis:
Fast Programmable Match-action Processing in Hardware for SDN. In ACM
SIGCOMM (2013).
[27] David Clark. 1988. The design philosophy of the DARPA Internet protocols. In
Symposium proceedings on Communications architectures and protocols (1988).
106–114.
[28] Graham Cormode and Marios Hadjieleftheriou. 2008. Finding Frequent Items in
Data Streams. Proceedings of the VLDB Endowment 1, 2 (2008).
[29] H. T. Dang, P. Bressana, H. Wang, K. S. Lee, N. Zilberman, H. Weatherspoon, M.
Canini, F. Pedone, and R. Soulé. 2020. P4xos: Consensus as a Network Service.
IEEE/ACM Transactions on Networking (2020).
[30] Daniel E. Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov,
Eric Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and Jin-
nah Dylan Hosein. 2015. Maglev: A Fast and Reliable Software Network Load
Balancer. In USENIX NSDI (2015).
[31] Aaron Gember-Jacobson, Raajay Viswanathan, Chaithan Prakash, Robert Grandl,
Junaid Khalid, Sourav Das, and Aditya Akella. 2014. OpenNF: Enabling Innovation
in Network Function Control. In ACM SIGCOMM (2014).
[32] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. 2003. The Google File
System. In ACM SOSP (2003).
[33] Cary Gray and David Cheriton. 1989. Leases: An efficient fault-tolerant mech-
anism for distributed file cache consistency. ACM SIGOPS Operating Systems
Review 23, 5 (1989).
[34] Albert Greenberg, James R Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A Maltz, Parveen Patel, and Sudipta Sen-
gupta. 2009. VL2: a scalable and flexible data center network. In ACM SIGCOMM
(2009).
[35] Arpit Gupta, Rob Harrison, Marco Canini, Nick Feamster, Jennifer Rexford, and
Walter Willinger. 2018. Sonata: Query-driven Streaming Network Telemetry. In
ACM SIGCOMM (2018).
[36] Maurice P Herlihy and Jeannette M Wing. 1990. Linearizability: A correctness
condition for concurrent objects. ACM Transactions on Programming Languages
and Systems (TOPLAS) 12, 3 (1990).
[37] Xin Jin, Xiaozhou Li, Haoyu Zhang, Nate Foster, Jeongkeun Lee, Robert Soulé,
Changhoon Kim, and Ion Stoica. 2018. NetChain: Scale-Free Sub-RTT Coordina-
tion. In USENIX NSDI (2018).
[38] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster,
Changhoon Kim, and Ion Stoica. 2017. NetCache: Balancing Key-Value Stores
with Fast In-Network Caching. In ACM SOSP (2017).
[39] Naga Katta, Mukesh Hira, Changhoon Kim, Anirudh Sivaraman, and Jennifer
Rexford. 2016. HULA: Scalable Load Balancing Using Programmable Data Planes.
In ACM SOSR (2016).
[40] Naga Katta, Haoyu Zhang, Michael Freedman, and Jennifer Rexford. 2015. Ravana:
Controller fault-tolerance in software-defined networking. In ACM SOSR (2015).
[41] Eric Keller, Jennifer Rexford, and Jacobus E van der Merwe. 2010. Seamless BGP
Migration with Router Grafting.. In NSDI (2010).
[42] Daehyeok Kim, Zaoxing Liu, Yibo Zhu, Changhoon Kim, Jeongkeun Lee, Vyas
Sekar, and Srinivasan Seshan. 2020. TEA: Enabling State-Intensive Network
Functions on Programmable Switches. In ACM SIGCOMM (2020).
[43] Petr Lapukhov, Ariff Premji, and Jon Mitchell. 2016. Use of BGP for routing in
large-scale data centers. Internet Requests for Comments RFC Editor RFC 7938
(2016).
[44] Collin Lee, Seo Jin Park, Ankita Kejriwal, Satoshi Matsushita, and John K. Ouster-
hout. 2015. Implementing linearizability at large scale and low latency. In ACM
SOSP (2015).
[45] Jialin Li, Ellis Michael, and Dan RK Ports. 2017. Eris: Coordination-free consistent
transactions using in-network concurrency control. In ACM SOSP (2017).
[46] Jialin Li, Ellis Michael, Naveen Kr. Sharma, Adriana Szekeres, and Dan R. K.
Ports. 2016. Just Say No to Paxos Overhead: Replacing Consensus with Network
Ordering. In USENIX OSDI (2016).
[47] Jialin Li, Jacob Nelson, Ellis Michael, Xin Jin, and Dan RK Ports. 2020. Pegasus:
Tolerating Skewed Workloads in Distributed Storage with In-Network Coherence
Directories. In USENIX OSDI (2020).
[48] Hyeontaek Lim, Dongsu Han, David G Andersen, and Michael Kaminsky. 2014.
MICA: A holistic approach to fast in-memory key-value storage. In USENIX NSDI
(2014).
[49] Barbara Liskov and James Cowling. 2012. Viewstamped Replication Revisited.
Technical Report MIT-CSAIL-TR-2012-021. MIT Computer Science and Artificial
Intelligence Laboratory, Cambridge, MA, USA.
[50] Hongqiang Harry Liu, Yibo Zhu, Jitu Padhye, Jiaxin Cao, Sri Tallapragada, Nuno P
Lopes, Andrey Rybalchenko, Guohan Lu, and Lihua Yuan. 2017. Crystalnet:
Faithfully emulating large production networks. In ACM SOSP (2017).
[51] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas Anderson.
2013. F10: A fault-tolerant engineered network. In USENIX NSDI (2013).
[52] Zaoxing Liu, Zhihao Bai, Zhenming Liu, Xiaozhou Li, Changhoon Kim, Vladimir
Braverman, Xin Jin, and Ion Stoica. 2019. DistCache: Provable Load Balancing for
Large-Scale Storage Systems with Distributed Caching. In USENIX FAST (2019).
[53] Zaoxing Liu, Antonis Manousis, Gregory Vorsanger, Vyas Sekar, and Vladimir
Braverman. 2016. One Sketch to Rule Them All: Rethinking Network Flow
Monitoring with UnivMon. In ACM SIGCOMM (2016).
[54] Justin Meza, Tianyin Xu, Kaushik Veeraraghavan, and Onur Mutlu. 2018. A large
scale study of data center network reliability. In ACM IMC.
[55] Rui Miao, Hongyi Zeng, Changhoon Kim, Jeongkeun Lee, and Minlan Yu. 2017.
SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switch-
ing ASICs. In ACM SIGCOMM (2017).
235
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan
[56] Ali Mohammadkhan, KK Ramakrishnan, Ashok Sunder Rajan, and Christian
Maciocco. 2016. Considerations for re-designing the cellular infrastructure ex-
ploiting software-based networks. In IEEE ICNP (2016).
[57] Diego Ongaro and John Ousterhout. 2014.
Consensus Algorithm. In USENIX ATC (2014).