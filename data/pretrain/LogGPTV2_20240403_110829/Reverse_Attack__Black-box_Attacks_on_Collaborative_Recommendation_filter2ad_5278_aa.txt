title:Reverse Attack: Black-box Attacks on Collaborative Recommendation
author:Yihe Zhang and
Xu Yuan and
Jin Li and
Jiadong Lou and
Li Chen and
Nian-Feng Tzeng
Reverse Attack: Black-box Attacks on Collaborative
Recommendation
Yihe Zhang
Xu Yuan∗
PI:EMAIL
University of Louisiana at Lafayette
PI:EMAIL
University of Louisiana at Lafayette
Jin Li
PI:EMAIL
Guangzhou University
Guangzhou, Guangdong, China
Nian-Feng Tzeng
PI:EMAIL
Lafayette, LA, USA
Lafayette, LA, USA
Jiadong Lou
PI:EMAIL
University of Louisiana at Lafayette
Lafayette, LA, USA
Lafayette, LA, USA
Li Chen
PI:EMAIL
Lafayette, LA, USA
University of Louisiana at Lafayette
University of Louisiana at Lafayette
ABSTRACT
Collaborative filtering (CF) recommender systems have been exten-
sively developed and widely deployed in various social websites,
promoting products or services to the users of interest. Meanwhile,
work has been attempted at poisoning attacks to CF recommender
systems for distorting the recommend results to reap commercial
or personal gains stealthily. While existing poisoning attacks have
demonstrated their effectiveness with the offline social datasets,
they are impractical when applied to the real setting on online social
websites. This paper develops a novel and practical poisoning attack
solution toward the CF recommender systems without knowing
involved specific algorithms nor historical social data information
a priori. Instead of directly attacking the unknown recommender
systems, our solution performs certain operations on the social
websites to collect a set of sampling data for use in constructing a
surrogate model for deeply learning the inherent recommendation
patterns. This surrogate model can estimate the item proximities,
learned by the recommender systems. By attacking the surrogate
model, the corresponding solutions (for availability and target at-
tacks) can be directly migrated to attack the original recommender
systems. Extensive experiments validate the generated surrogate
model’s reproductive capability and demonstrate the effectiveness
of our attack upon various CF recommender algorithms.
CCS CONCEPTS
• Security and privacy → Web application security.
KEYWORDS
Recommender System; Poisoning Attack
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484805
ACM Reference Format:
Yihe Zhang, Xu Yuan, Jin Li, Jiadong Lou, Li Chen, and Nian-Feng Tzeng.
2021. Reverse Attack: Black-box Attacks on Collaborative Recommenda-
tion. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security (CCS ’21), November 15–19, 2021, Virtual Event,
Republic of Korea. ACM, New York, NY, USA, 18 pages. https://doi.org/10.
1145/3460120.3484805
1 INTRODUCTION
The web service providers, driven by profits, have devoted ef-
forts to promoting their products and enriching user experience,
through employing the recommender systems. The mainstream
of recommender systems is based on the collaborative filtering
(CF) methods, with a series of algorithms developed and some
of them widely deployed in the E-commerces [30, 42, 71], social
networks [56, 73], online websites [29, 67], and mobile applica-
tions [55, 85]. They aim to help the service providers promote prod-
ucts or services that can increase the visibility of their items (i.e.,
products and services) to the users of interest. Most social websites
reportedly are using the CF recommender algorithms, for exam-
ple, Amazon [33], Airbnb [43], NetEase Music [74], and eBay [88],
although the specific algorithms are not revealed. The essence of
a CF recommender system is to mine the intrinsic correlations of
user behaviors, item-item relationships, and user-item interactions,
to locate users/items in the preference of other users. In litera-
ture, the CF recommender algorithms can be categorized as being
item-based [8, 17, 54, 68], matrix-factorization-based [44, 77, 87],
neural network-based [6, 16, 37, 78, 86, 90], and graph structure-
based [5, 26]. Plentiful algorithms have been proposed, with some
of them already deployed into online websites for production use.
However, existing works have demonstrated that the CF recom-
mender systems are vulnerable to an attacker with the purpose of
distorting the recommender results for some specific profits. Data
poisoning attack is the most popular and effective technique that
has been applied for recommendation result falsification via inject-
ing fake users and operations to intentionally change the items or
users’ recommendation relationships so that the original recommen-
dation list is distorted. Various poisoning attack strategies have been
proposed to target item-based [9, 11, 19, 31, 32, 48, 58, 63, 70, 82],
matrix factorization-based [14, 23, 39, 51], neural network-based
[12, 20, 36, 53, 75, 80], graph-based recommender systems [24, 84],
respectively. However, all the aforementioned data poisoning at-
tack solutions fall into the white-box attack, where the specific
recommender algorithm and historical training data information
Session 1A: Cybercrime CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea51are all assumed to be exposed to the attacker. This is impractical
in real-world social websites, since the respective algorithm and
information typically are kept secret by service providers.
Although some approaches have been proposed in [15, 22, 72, 83]
without knowing the specific recommender algorithms, they still re-
quire certain historical data knowledge, e.g., the training set and the
frequency of items that were recommended within a certain period,
which are not always available in practice. In this paper, we aim to
develop a practical black-box attack strategy for effectively distort-
ing recommender systems embedded in social websites, without the
prior knowledge of either recommender algorithms or historical
data information. An attacker only needs to have public knowledge
obtained simply by performing the operations as normal users.
Our strategy is to construct a surrogate model that estimates
item proximities for the embedded recommender systems. Instead
of performing attack straight, we craft our attack strategy in the
surrogate model to optimize the attack profits, and then directly
migrate such a strategy to the target social website for distorting
its recommendations. Specifically, we first intentionally perform
some regular operations (e.g., viewing, clicking, rating) as normal
users on the social websites through user’s interface and gather the
recommended results as the sampling data. Two alternative meth-
ods, i.e., random walk collection and random injection collection,
are proposed, to suit different social websites. Since the sampling
data include affluent and implicit information of the embedded
recommender algorithms (no matter which specific algorithm is
using), we construct a surrogate model by deeply learning the item
proximities. Focusing on attacking the surrogate model, we craft
our solutions for both the availability attack and the target attack.
We define the objective functions of maximizing attack profits, i.e.,
demoting and promoting recommendation results by the largest
degree, under the availability attack and target attack, respectively.
By solving the formulated optimization problems, the number of op-
erations can be obtained, representing our attack solutions, which
can be applied to target social websites for an effective attack.
We conduct extensive experiments to verify the capability of our
constructed surrogate model in estimating item proximities on the
sampling data from several online websites and some real-world
datasets. Results demonstrate that our models efficiently calculate
the items’ distributional representations, highly correlated with
those obtained from the original recommender systems. To show
our attack performance, we conduct experiments solely on real-
world datasets due to ethical considerations. Our attack strategies
generated from the surrogate model are directly migrated to the
target recommender systems for attacking. Through experimenting
on 9 datasets, we obtain consistent results, exhibiting that both
availability attack and target attack work effectively on all 9 ex-
amined CF algorithms. In addition, our solutions outperform the
compared counterparts which conduct white-box attacks with both
the recommender algorithms and training sets known a priori.
2 BACKGROUND KNOWLEDGE
2.1 CF Recommender Systems
CF is a widely adopted method in many categories of recommender
systems and has been implemented in various social medias. It
makes the personalized recommendation of items or users to a
certain user by mining the latent intricate relationships of users
and items in the historical data for modeling the similarity of users’
interests or behaviors. Collectively, CF recommender systems can
be grouped into the following four categories.
1) Item-based CF calculates the similarity among users or items
using Pearson correlation, Vector cosine, Euclidean distance, and oth-
ers across the entire user-item relationship matrix, denoted by
MMMuv ∈ Rm×n, where the rows represent m users and the columns
represent n items, to identify the K most similar users or items for
recommendation. Each entry is a numerical value representing the
relationship between a user and an item. For example, in Movie-
Lens, each entry denotes a rating score (0.5 to 5) or no-relationship
(Ω) from a user to a movie. In Amazon, each entry represents the
visiting behavior (denoted as 1 or Ω) of a user to an item.
2) Matrix factorization-based CF further decomposes the user-item
interaction matrix MMMuv into the product of two lower dimensional
matrices, where the first one has a row for each user while the
second one has a column for each item. Such two lower dimensional
matrices can be considered as the distributional representation of
users or items, representing the latent factors of users or items
similarity. The product of two lower dimensional matrices will then
result in a full ranked matrix, which models all the users and items
relationships, assisting item recommendations.
3) Graph-based CF models the relationships of users or items as
the bipartite graph GGGb = {U,V, E} or co-visitation graph GGGc =
{V, E} where U represents users, V denotes items and E denotes
relationships of two vertices. Graph-based recommender systems
recommend products to users based on the geometry relationships
of users and items in the user-item bipartite graph. Different graph-
based techniques can be utilized to find users’ or items’ similarities
and relationships, which will be utilized to make recommendations.
4) Neural network-based CF encodes each user or item into a latent
vector space. At each hidden layer, new features can be extracted
from the previous vector space to a new vector space. At the output
layer, the similarity among the latent user vectors or item vectors
will be calculated. Such a method can make an in-depth calculation
of the inherent and sophisticated relationships of users to users,
items to items, or users to items so as to find the similarity patterns.
2.2 Related Works
Regarding item-based CF recommender systems, the early attack
solutions proposed in [11, 19] aimed to help the developer to build a
robust CF method when suffering from unfair ratings in online trad-
ing communities. Later, [63] showed that by injecting users with
biased ratings, an attacker can promote or demote the target items.
[48] demonstrated that some naive methods like injecting Random-
Bot and AverageBot could also promote or demote the target item
by assigning the maximum or minimum ratings to the randomly
selected items, respectively. Furthermore, some advanced methods,
i.e., Bandwagon attack[9], Probe attack[57], Consistency attack[10]
and Segment attack [31], were proposed. These attack methods,
although effective, are not efficient in poisoning recommender sys-
tems. To broaden the impact of poisoning attack, the authors in
[70, 82] proposed the power user/item attack model based on in-
degree centrality, via using power users to lunch an attack or solely
Session 1A: Cybercrime CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea52Work on attacking the matrix factorization-based CF abounds.
Specifically, [51] leveraged the first-order KKT conditions to derive
the fake ratings for the attacking strategy. [39] developed a bi-level
program to perform target attacks. [14] proposed a generative ad-
versarial network (GAN)-based attack model to generate ratings
for fake user profiles. [23] proposed to select the influential users
for attacking. However, each of these attacks targets only one spe-
cific algorithm while requiring full or partial historical user-item
interaction knowledge for designing the attack strategies.
In the line of graph-based CF system attacks, [84] proposed to
inject the fake co-visitations into a co-visitation graph. The opti-
mized attack strategy was derived by solving the constrained linear
optimization problems given a bounded number of fake visitations,
with full graph knowledge available. They then proposed a low
knowledge attack method, which is only feasible to the simplified
recommender algorithm with the linear model. [24] proposed an
attack solution that calculates the fake user rating scores by solv-
ing an optimization problem. However, it still requires full graph
topology knowledge, often unavailable in the real-world systems.
Recently, the successful applications of neural network tech-
niques to recommender systems attract adversary attacks. Such
attacks target the personality ranking [36], E-commerce websites
[20], visual-based recommender systems [75], and others. However,
all these attacks rely on the gradient-based methods, such as Fast
Gradient Sign Method (FGSM) [28] and Projected Gradient Descent
(PGD) [46], requiring to generate the adversarial perturbations
from the entire datasets, apparently inapplicable to real-world so-
cial platforms, since the whole datasets are unlikely to be available.
Similarly, GAN-based [27] attack models [12, 15, 80] also suffer
from the same shortcomings. Moreover, [53] proposed to generate
the applicable fake user profiles based on GAN, but it needs to use
the existing real user profile as a “template”, which is very hard to
acquire from recommender systems and also raises the privacy issue.
[41] and [89] proposed the poisoning attacks on neural-network
based recommender systems. However, they requires the whole
knowledge of the recommender system structure and of the dataset.
Again, the specific neural network algorithms in aforementioned
works must be known prior to performing all these attacks.
attack the power items. However, this line of attacks requires the
recommender algorithms or users’ historical knowledge.
All the aforementioned attack solutions belong to the white-box
attack, since the specific recommender algorithms are known a
priori. Meanwhile, some black-box attacks without knowing the
recommender algorithms, have been pursued. Specifically, [83]
discovered the vulnerability of the YouTube recommender system
and conducted a real-world pollution attack. However, this attack
solution only distorted the partial functionality of the recommender
systems. In [22], reinforcement learning was introduced to develop
black-box attack on a source domain, with the attack solution then
transferred to the target systems. Such a solution has the strict
requirement that the source domain and the target system should
have overlapped user profiles, but how to guarantee overlapped
user profiles in real-world attack remains questionable. [72] also
employed reinforcement learning to develop a black-box attack
solution, leveraging the binary tree structure to generate fake user
profiles. It requires knowledge on the recommended frequency of
some items within a certain period, which rarely holds in practice.
3 PROBLEM STATEMENT
This paper aims to design effective strategies to perform black-box
poisoning attacks on CF recommender systems that are embedded
in social websites, with the goal of twofold distortions: 1) demoting
recommended results and 2) promoting the target items, referring
to as availability attack and as target attack, respectively.
3.1 Threat Model
To perform the attacks, the amounts of adversarial behaviors (i.e.,
fake users and their operations) are constrained due to the resource
limitation and detection avoidance. An attacker will craft an ef-
fective attack strategy, subject to these constrained resources, for
optimizing the attack profits in target social websites. By injecting
the well-crafted fake users and operations, an attacker can achieve
its goal of maximally distorting the recommendation results. In
practice, social websites do not expose the specific recommender
algorithms currently being used for security reasons. Public in-
formation only indicates that Amazon [33], Airbnb [43], NetEase
Music [74], Spotify [74], eBay [88], and others currently employ the
CF methods to produce high quality recommendations [54]. Thus,
when performing practical attacks on a social website, an attacker
does not have the knowledge of its specific recommender algorithm
being used, other than that it falls in the category of CF. Also, the
historical data of the users and items relationships are unknown to
the attacker either. All knowledge obtainable by an attacker is from
the attacker’s normal interactions with the social websites. In our
attack, we treat both the recommender system and historical data
in a target social website as a black box. Nonetheless, an attacker
can access the social website through its user interface to view or
visit users and items, via regular operations provided to users. The
attacker then collects the public data and uses them to design its
attack strategy. Following the attack strategy, the attacker injects
fake users and operations allowed by the systems, such as rating,
clicking, or viewing actions, to perform effective attacks.
3.2 Sketch of Our Attack Strategy
Before presenting design details in the next two sections (Sections 4
and 5), we first give a sketch of our black-box attack strategy. Since
an attacker neither knows recommendation algorithms nor has the
prior knowledge of users or items relationships, the first step is to
interact with target social websites via the normal operations and
collect a set of recommended results to serve as the sampling data
for learning. To this end, a surrogate model is developed for deeply
learning the recommendation relationships from the sampled data,
aiming to estimate the item proximities and the implicit patterns.
It will assist in developing solutions offline for evaluation. After
that, a solution that is effective on the surrogate model, is directly
transferred to the target social websites to achieve the similar at-
tack goals. Notably, the recommended objects can be either users
or items corresponding to different social websites. For ease of
expression, we uniformly call them items in the following sections.
4 CONSTRUCTING SURROGATE MODEL
To construct the surrogate model, we first collect the sampling
data in online social websites for learning use. Two alternative
approaches, i.e., Random Walk Collection and Random Injection
Session 1A: Cybercrime CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea53(a) Amazon
(b) Netflix
(c) Yelp
(d) MoiveLens
Figure 1: Layout on different websites. Red and blue boxes denote the key item and the related recommendation, respectively.
(a) Type-I layout
(b) Type-II layout
Figure 2: Two common recommendation web page layouts.
Collection, are proposed for this purpose. Next, we design a train-
ing strategy for the surrogate model to efficiently learn from the
collected dataset for exploring item proximities.
4.1 Sampling Data Collection
Random Walk Collection. This approach is suitable for those
websites that recommend items based on items’ similarities, e.g.,
Amazon, Netflix, and Yelp, exampled as in Figures 1(a), 1(b), and
1(c). The recommendation web page layout suitable for apply-
ing this collection approach is illustrated in Figure 2(a), called
Type-I layout. There is one key item I and a recommendation
area which lists K similar items for recommendation, denoted as
F(I) = {I(1), I(2), . . . , I(K)}. Notably, rankings of these items are usu-
ally implicit to users but their proximities are disclosed. Our collec-
tion procedure takes into account such relative ranking information.
The indicator of recommendation area varies on different social
websites. For example, in Amazon, this section is named as “Cus-
tomers who viewed this item also viewed”. To start the Random Walk
Collection, we randomly select an item I1 (called the key item) for
the first sampling trail Cw(1). Assuming there are K similar recom-
mender items on the Type-I recommender area, we shall record all
, K)}, where
of them as Cw(1) = {(I1, I
1 to K are the listed rankings for them. Corresponding to each rec-
ommended item, a sampling score ck = e−λk is assigned based on
its ranking, where k is item’s ranking, and λ is a parameter to adjust
item ranking importance, ranging from 0.001 to 0.5. Hence, each
item can be sampled with the probability of p(I
. The
larger the λ, the higher the probability to sample a higher ranking
item. We iteratively execute operations above to select the next
nodes until the maximum walk length Z is reached. With further
trails sampled following the similar way, we eventually obtain a set
of sampled data, denoted as Cw = {Cw(1), Cw(2), . . . , Cw(Z)}.
Random Injection Collection. This approach is suitable for those
websites that recommend items based on users’ historical prefer-
ences, e.g., MovieLens as shown in Figure 1(d). We call this type
of website a Type-II layout, as sketched in Figure 2(b), where the
items recommended to users are not targeted to one particular
(2)
1 , 2), . . . ,(I1, I
) = ckK
(1)
1 , 1),(I1, I
(K)
1
(k)
1