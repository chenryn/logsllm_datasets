(6)
The second step is to initialize the context vector uw randomly.
Then, the importance of the word as the similarity of uit with a
word-level context vector uw is measured. This gives a normalized
importance weight vector α through a softmax function. The Con-
text vector uw is learned during the training process. α measures
the importance of ith word for malicious behavior.

exp(u⊤
ituw)
t exp(u⊤
ituw)
αit =
(7)
In the end the sentence can be represented as the weighted
hidden vector r:
r = HαT .
(8)
3 APPROACH
In this section, we describe our proposed method, Neurlux, a model
which treats the dynamic behavior classification problem in a way
similar to a document classification problem. The steps of this
approach are as follows:
• Data cleaning: We want to treat the reports as a document
classification task, so the first step is to clean the JSON for-
matted reports so that it is structured less like a JSON doc-
ument and more like sequences of words, which makes up
sequences of sentences. To do this, we first remove special
characters, such as the brackets which are part of the JSON
structure. Then the document is tokenized to extract words,
of which the top 10,000 most common words are converted
into numerical sequences.
• Data Formatting: The naïve method of converting words to
vectors assigns each word with a one-hot vector. This vector
would be all zeros except one unique index for each word.
This kind of word representation can lead to substantial data
sparsity and usually means that we may need more data
to train statistical models successfully. This can be fixed by
continuous vector space representation of words. To be more
specific, we want semantically similar words to be mapped
to nearby points, thus making the representation encode
useful information about the words’ actual meaning.
Therefore, we use trainable word embeddings in Neurlux,
which can have the property that similar words have similar
vectors as described in Section 2.1. This way, the model can
cluster words based on their usage patterns and they can
provide more meaningful inputs to the later layers in the
model.
• Model: We use the combination of CNN, BiLSTM networks,
and Attention networks to create a model that understands
the hidden lexical patterns in the malicious and benign re-
ports. This model is designed using concepts from document
classification. For example, an important idea is that not all
words in a sentence are equally important, so it uses the
attention mechanism to recognize and extract important
words [41]. Another aspect is that context is important for
Figure 1: Overview of Models
Figure 2: CNN-BiLSTM-Attention model for document classification
understanding the meaning of words, therefore we use BiL-
STM to give context to the model. This model is described
in detail in the next section ( 3.0.1).
3.0.1 CNN + BiLSTM + Attention. Inspired by the previous
document classification methods, we create the model illustrated
in Figure 2, which consists of a convolutional neural network layer
(CNN), and two pairs of bi-directional long short term memory
network (BiLSTM) and attention layers. Convolutional neural net-
works (CNN) extract local and deep features from the input text.
Then we obtain the high-level representation from the bidirectional
LSTM network by using the hidden units from both the forward
and backward LSTM. The CNN and LSTM combination is useful
to extract local higher level features, which the LSTM can then
find temporal relationships between [45]. The two pairs of BiLSTM
and Attention are inspired by hierarchical attention networks [41].
The input is a trainable word embedding of dimension 256 to al-
low the model to cluster similar words as it learns. Each of these
components is described thoroughly in the Section 2.
As described before, different parts of the report have different
importance for determining the overall malicious behavior of a
binary. For example, some parts of a registry key can be decisive,
while others are irrelevant. Suppose we have the sentence attention
score Ai for each sentence si ∈ x, and the word attention score ai, j,
for each word wi, j ∈ si; both scores are normalized which satisfy
the following equations,

Ai = 1
and
ai, j = 1.
(9)
i
j
The sentence attention measures which sentence is more im-
portant for the overall behavior while the word attention captures
behavior signals such as the behavior words in each sentence. There-
fore, the document representation r for document x is calculated as
Table 1: Overview of the models that we create and compare
against Neurlux.
Model
Counts Model
Individual Model
Ensemble Model
MalDy
Raw Model
Neurlux
Feature
Engineering Description
Yes
Yes
Yes
No
No
No
Counts of each feature
Document classification on
individual extracted features
Ensemble of individual features
State of the art model for
report classification from [11]
Model trained on raw bytes
Document classification on
whole report
follows,
r =
i
(cid:20)
Ai .

j
(cid:18)
(cid:19)(cid:21)
ai, j .hi, j
.
(10)
Finally, Neurlux outputs a classification decision as a score from
0-1 where 0 is benign, and 1 is malicious.
4 COMPARISON METHODS
In this section, we describe various models with which we compare.
We compare with a previous state of the art method and with a
couple approaches which involve feature engineering to check if
we can actually do better than feature engineering approaches.
An overview of the approaches we compare against are shown in
Table 1.
4.1 Comparison With a State-of-the-Art Model
We used the method described in MalDy [11], as a model for compar-
ison. Their approach is to preprocess sandbox reports with standard
Natural Language Processing (NLP) techniques and then create an
ensemble supervised machine learning (ML) model from a mul-
titude of different ML algorithms. They attempt to formalize the
behavioral reports in a way agnostic to the execution environment.
This is done on both Win32 and Android. They argue that the key
to their success is using their bag of words (BOW) model with Com-
mon N-Grams (CNG). CNG effectively computes the contiguous
sequences of n items where n is an adjustable hyper-parameter. In-
stead of using single words (1-grams), using n-grams aids in finding
distinct features. Once the reports are in a list of n-gram strings,
they carry out two different vectorization approaches: TF-IDF and
Feature Hashing. Feature Hashing creates fixed length feature vec-
tors from sparse input n-grams. A hash is taken of each n-gram, and
if the value is found within the table, it is incremented; otherwise,
a value of 1 is added to the table. This process creates probabilisti-
cally unique vectors, given that the hash bucket size is sufficiently
large. These vectors are subsequently fed into the ML models. We
implement and use their best performing model as a comparison. In
our evaluation (Section 6), this model reaches a plateau with 89.23%
accuracy and an F-1 score of 88.5%.
A weakness of the BOW approach used in MalDy is that it does
not take into account the context, just the frequencies with which
words appear [22].
4.2 Raw JSON Data
A more basic deep learning approach is to learn from raw bytes,
treating it as an image classification problem. Although the struc-
ture of the input data is defined, the placement of different string
objects within the file is not ordered. To best capture such high-
level location invariance, we choose to use a convolution network
architecture. Combining the convolutional activations with a global
max-pooling, followed by fully-connected layers allows this model
to produce its activation regardless of the location of the detected
features.
The Raw Model was inspired by an earlier approach on byte
classification [26]. First, we clean the document, removing special
characters. Then the bytes are extracted as integer values then
padded to fix length to form a vector x of d elements. This ensures
that regardless of the length of the input file, the input vector
provided to the network has a fixed dimensionality. Each byte xj
is then embedded as a vector zj = ϕ(xj) of eight elements (the
network learns a fixed mapping during training). This amounts
to encoding x as a matrix ZϵR[d×8]. Figure 1(a) shows an outline
of the model used for raw JSON data binary classification. Then,
it goes through the convolutional layers to eventually produce a
classification between 0 and 1.
4.3 Features for Engineering Approaches
For the feature engineering approach comparisons, we begin by
categorizing the six main categories of features available in the
reports. These features are described in more detail below.
• API Sequence Calls. The reports typically include all sys-
tem calls and their arguments stored in a representation
tailored explicitly to behavior-based analysis. Much of the
past work on behavior analysis has focused on using API
call sequences for malware classification [23, 32, 35].
• Mutexes. Mutexes control the simultaneous access of the
system resources. They are used by malware creators to
avoid infecting a system more than once, and coordinate
between processes [42].
• File System Changes. The interaction of a malware sam-
ple with the host file system might be a good indicator to
determine malicious behavior. We consider all the important
file operations such as create, read, write, modify, delete, etc.
• Registry Changes. The registry is a core part of Windows
and contains a plethora of raw data. Registry keys can reveal
much information about the system, but the true challenge
is in unraveling which modifications to the registry are mali-
cious and which are legitimate. The registry also represents
a fundamental tool to hook into the operating system to
gain persistence. Discovering what keys are queried, created,
deleted, and modified can shed light on many significant
characteristics of a sample.
• Loaded DLLs. The reports contain the shared library code
loaded by a program. Nearly every executable program im-
ports DLLs during execution. These DLLs can give insights
into the types of APIs used by the program.
Figure 4 is an example of a CuckooSandbox report for a malicious
sample that shows the various behavioral features cuckoo identifies.
We obtain 28 such different features from CuckooSandbox and 43
features from VendorSandbox. These features are mapped based on
semantic similarities and divided into 6 main behavioral groups as
described above. The following sections give an exhaustive descrip-
tion of the various feature engineering techniques used. They are
shown in Figure 1(a).
4.4 Feature Counts Model
In this section, we discuss an approach to use a neural network on
shallow numerical features. Numerical features here are simply the
counts of each event that was recorded, e.g., number of registry
reads, number of file writes, etc. The first step is to parse the reports
and extract all the available features. The number of features ex-
tracted differs due to the structural differences in dynamic analysis
reports collected from CuckooSandbox and VendorSandbox. Each
report lists features according to the parent process and child pro-
cesses (any process that was either spawned by or tampered with
by the primary process). Each process has its own set of individual
features. Since each executable can contain one or more processes,
the final representation of input features per sample will be:
S = processes × f eatures
which expands to
r eд_r ead
f ile_write
a1,2
a2,2
.
.
.
am,2
· · · mutex
· · ·
a1,n
· · ·
a2,n
. . .
· · ·
am,n
.
.
.
1
2
.
.
.
n
(11)
a1,1
a2,1
.
.
.
am,1
S =
(Columns are features, rows are processes)
The data representation is similar to that of a gray-scale image;
therefore, a 2D CNN can be used for training on this dataset. We
use an 8-layer deep CNN model inspired by Simonyan et al. [36].
The model consists of 8 convolution layers and 2 fully-connected
layers. Every convolutional filter has a kernel size of 3, 4, or 5 with
a stride of 1 and pooling region of 3x3 without overlap. A pooling
function is applied to each feature map to induce a fixed-length
vector. These fixed-length, top-level feature vectors generated from
filter maps are then fed through a softmax function to generate the
final classification. Figure 1 gives an overview of this model.
4.5 Text-Based Individual Feature Model
Each analysis report is a collection of statements, and each state-
ment is a sequence of words. We believe that these sequences can
give a more granular description of the actual events, compared to
the features count method discussed in the previous section. The
assumption for the text classification approach described in this sec-
tion is that the difference between malicious and benign behavior
of binaries could be translated into sequences present in the reports.
In other words, the sequence of actions better represents if a binary
is malicious or not than merely the number of actions. Additionally,
we were looking for a feature representation (sequences of words)
that uses an automatic feature extraction without the intervention
of a security expert.
The input generation process can be divided into four steps. This
process is performed iteratively for all six feature groups.
• Feature Selection: Different features are selected from the
feature pool based on the top six behavioral groups discussed
earlier in this section.
• Data cleaning: Similar to our method for Neurlux, we need
to remove special characters and perform tokenization to
extract numerical sequences.
• Data Formatting: As discussed previously, we want to have
a continuous vector space representation of words, with
semantically similar words mapped to a nearby point. So
once again we use a trainable word embedding.
• Model training: We use the combination of CNN, BiLSTM,
and Attention networks to create a model that understands
the hidden lexical patterns in the malicious and benign re-
ports. This model is described in detail in Section 3.0.1
4.6 Integrated Features using Ensemble
When the neural network, described in Section 3.0.1, is trained
on individual behavioral feature types (such as mutexes or api
calls), it exhibits a high variance depending on the feature. This
variance can be attributed to the importance and contribution of
each feature extracted from the reports. Therefore, in this section,
we describe a way to use ensemble learning to combine multiple