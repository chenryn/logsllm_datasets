### Optimized Text

#### 1. Executable Code Overhead
The overhead for the executable code is less than 0.5% on average, with a maximum of 1.1%. This minimal overhead is achieved by maintaining good cache locality, which is ensured by keeping the switch table close to its usage. Additionally, modern processors can prefetch instructions past direct jumps, reducing their performance impact. We have omitted this experiment from Figure 11 for clarity, as it showed such minimal overheads.

#### 2. Code-Pointer Hiding
We evaluated full code pointer protection, including the hiding of both function pointers and return addresses. Our findings indicate that code-pointer hiding results in an average performance slowdown of 4.1% across all benchmarks (see "Pointer Hiding" in Figure 11). This protection introduces two additional direct jumps for each method call and one direct jump when dereferencing function pointers. Using hardware performance counters, we observed that these jumps increase the instruction-cache pressure, leading to more instruction-cache misses.

We hypothesized that the majority of the overhead from code-pointer hiding is due to call trampolines, which are more common than function pointers. To verify this, we disabled return address hiding while keeping function pointer protection enabled. The results showed that function pointer protection alone incurred an average overhead of only 0.2% on SPEC, with no benchmark exceeding 2%. Therefore, most of the overhead for code-pointer hiding is caused by the frequent use of call trampolines, especially in benchmarks with many function calls, such as xalancbmk.

#### 3. Hypervisor Performance Impact
To understand the performance impact of the hypervisor layer, including the additional EPT translation overhead, we ran SPEC under our hypervisor without any execute-only page protections or code-pointer hiding enabled (see "Hypervisor" in Figure 11). The virtualization overhead was 1.1% on average. Since we allow the virtual processor full control of all hardware and registers, this overhead is primarily due to the extra memory translation overhead from translating guest physical addresses to host physical addresses through the EPT. Even with an identity mapping from guest physical to host physical addresses, the processor must still walk through the entire EPT for each new memory address translation. The higher overhead observed for the mcf benchmark, which has a larger and more stable memory footprint (845 MiB), supports this theory. This results in more swapping in and out of the cache, triggering more EPT address translations.

#### 4. Execute-Only Page Permissions
After compiling SPEC with separation of code and data and marking code pages as execute-only during linking, we ran the benchmarks under the hypervisor, enforcing execute-only page permissions (see "Hypervisor XO" in Figure 11). This configuration resulted in a performance slowdown of 2.5%, slightly higher than the overhead of the hypervisor itself. Much of this overhead difference is due to the separation of code and data, which slightly de-optimizes execution. The rest of the difference can be attributed to measurement variance, as the hypervisor should not add significant overhead when enforcing execute-only pages compared to legacy readable and executable pages. In either case, the processor must still translate all addresses through the EPT when the hypervisor is enabled.

#### 5. Full Readactor System
Enabling code-pointer hiding along with page protections provided by the hypervisor resulted in a slowdown of 5.8% (see "Hypervisor XO + Hiding" in Figure 11). This overhead is approximately the sum of the overheads of both components: the execute-only hypervisor enforcement and pointer hiding. This confirms our hypothesis that each component of the Readactor system is orthogonal with respect to performance.

With the addition of our fine-grained diversity scheme (function, register, and callee-saved register slot permutation), we now have all components of Readactor in place. For the final integration benchmark, we built and ran SPEC using three different random seeds to capture the effects of different code layouts. Overall, we observed that the full Readactor system incurred a geometric mean performance overhead of 6.4% (see "Full Readactor" in Figure 11). This demonstrates the overhead of applying our full protection scheme to a realistic worst-case scenario of CPU-intensive code, which bounds the overhead of our system in practice.

#### 6. Chromium Browser Performance
To test the performance impact of our protections on complex, real-world software, we compiled and tested the Chromium browser, which is the open-source variant of Google’s Chrome browser. Chromium is a highly complex application, consisting of over 16 million lines of code. We were able to easily apply all our protections to Chromium with minor changes.

Overall, we found that the perceived performance impact on web browsing with the protected Chromium, as measured by Chromium’s internal UI smoothness benchmark, was 4.0%, which is in line with the average slowdown we observed for SPEC. Since our protection system interferes with conventional stack walking, we had to disable debugging components of Chromium that use stack walking. We also disabled the optimized memory allocator (TCMalloc) used in Chromium, which uses stack tracing for detailed memory profiling. With these minor modifications, we could compile and test the current development version of Chromium with our LLVM-based Readactor compiler.

To understand the perceived performance impact during normal web browsing, we benchmarked page scrolling smoothness with Chromium’s internal performance testing framework. We ran the scrolling smoothness benchmark on the Top 25 sites selected by Google, including popular websites like Google search, GMail, YouTube, Facebook, CNN, and Yahoo. The benchmark quantifies perceived smoothness by computing the mean time to render each frame while automatically scrolling down the page. We report the average slowdown as time per frame averaged over three runs of the benchmark suite to account for random variance.

Overall, we found that the slowdown in rendering speed for our full Readactor system was about 4.0%, averaged over three different diversified builds of Chromium. This overhead is slightly lower than what we found for SPEC, which is natural considering that browser rendering is not as CPU-intensive as the SPEC benchmarks. However, browser smoothness and responsiveness are critical factors for daily web browsing, rather than raw computing performance.

We also evaluated the performance impact of our techniques on Chromium using the extensive Dromaeo benchmark suite to provide a worst-case estimate for browser performance. This suite, composed of 55 individual benchmarks, includes standard JavaScript benchmarks such as the Sunspider and V8 JavaScript benchmarks, as well as benchmarks that exercise DOM and CSS processing. Dromaeo is comprehensive and ideal for evaluating the overall impact of our protections on performance-critical browser components.

We found that execute-only code protection alone, without code-pointer hiding, introduced a 2.8% overall performance slowdown on Dromaeo. Combining the hypervisor execute-only code pages with code-pointer hiding resulted in a 12% performance slowdown. We attribute this higher overhead to increased instruction cache pressure caused by our call pointer protection. However, Dromaeo represents a worst-case performance test, and rendering smoothness on real websites is a far more important factor in browsing.

#### 7. V8 JavaScript JIT Performance
We evaluated the performance impact of our changes to the V8 JavaScript engine, as well as the overhead of running the JIT compiler under Readactor. To get a more accurate sample, we benchmarked the V8 engine alone, outside of the browser. Figure 12 shows the results of our evaluation. We see small to negligible overhead for most benchmarks, with the exception of two benchmarks that put significant pressure on the memory allocator: EarleyBoyer and Splay. Both benchmarks allocate large numbers of temporary objects and trigger frequent garbage collection cycles, which become much more expensive due to our repeated re-mapping of pages. For another benchmark—Richards—we observe a very small speedup of 1%, which we attribute to measurement noise. Overall, the performance penalty of our changes comes to 6.2% when running natively and 7.8% with Readactor enabled.

We also added our V8 JIT compiler patches to the full Chromium browser to evaluate their impact on the whole browsing experience. We observed a higher impact on scrolling smoothness, with an average time per frame slowdown of 13.8% versus 4.0% without the JIT compiler patches. This extra slowdown is due to the overhead of separating JIT-generated data from code to allow the JIT to map generated code pages as execute-only.

#### 8. Related Work
Most code-reuse exploit mitigation approaches are based on either program randomization or some form of integrity checking. In contrast, Readactor combines a probabilistic defense against ROP (code layout randomization) with integrity checks (execute-only page permissions) to comprehensively thwart code-reuse attacks. We discuss probabilistic and integrity-checking defenses separately. Table I summarizes the main differences between our approach and closely related work, namely Oxymoron [6], XnR [7], and HideM [27]. Readactor is the only defense that provides protection against all known variants of ROP attacks (traditional ROP, direct and indirect JIT-ROP), while performing efficiently and protecting JIT-compiled code.

**A. Code Randomization Defenses**
Cohen was the first to explore program protection using diversity [18]. Forrest et al. [24] later demonstrated stack-layout randomization against stack smashing. Address space layout permutation (ASLP) [37] randomizes the code layout at the function granularity; adversaries must therefore disclose more than one code pointer to bypass ASLP. Binary Stirring [65] permutes both functions and basic blocks within functions, and Instruction Layout Randomization (ILR) [32] randomizes the code layout at the instruction level. Larsen et al. [40] compare these and additional approaches to automatic software diversity.

Unfortunately, these defenses remain vulnerable to memory disclosure attacks. The appearance of JIT-ROP attacks convincingly demonstrated that probabilistic defenses cannot tacitly assume that attackers cannot leak code memory layout at runtime [59]. Blind ROP [9], another way to bypass fine-grained code randomization, further underscores the threat of memory disclosure.

In response to JIT-ROP, Backes and Nürnberger proposed Oxymoron [6], which randomizes the code layout at a granularity of 4KB memory pages. This preserves the ability to share code pages between multiple protected applications running on a single system. Oxymoron seeks to make code references in direct calls and jumps that span code page boundaries opaque to attackers. Internally, Oxymoron uses segmentation and redirects inter-page calls and jumps through a dedicated hidden table. This prevents direct memory disclosure, i.e., it prevents the recursive-disassembly step in the original JIT-ROP attack. Unfortunately, Oxymoron can be bypassed via indirect memory disclosure attacks, as described in Section III. In contrast to Readactor, Oxymoron does not protect JIT-compiled code. Readactor offers more comprehensive protection against both direct and indirect code disclosure and protects JIT-compiled code against direct disclosure.

Another defense against JIT-ROP, Execute-no-Read (XnR) by Backes et al. [7], is conceptually similar to Readactor as it is also based on execute-only pages. However, it only emulates execute-only pages in software by keeping a sliding window of n pages as both readable and executable, while all other pages are marked as non-present. XnR does not fully protect against direct memory disclosure because pages in execution are readable. Hence, the adversary can disclose function addresses (see Section III) and force XnR to map a target page as readable by calling the target function through an embedded script.