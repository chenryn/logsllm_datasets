U’s images, using each of the four feature extractors in Ta-
ble 1. On the tracker side, we perform transfer learning on the
same feature extractor (with cloaked images of U) to build a
model that recognizes U. Finally, we evaluate whether the
tracker model can correctly identify other clean images of U
it has not seen before.
Results show that cloaking offers perfect protection, i.e. U
is always misclassiﬁed as someone else, for all four feature
extractors and under the perturbation budget ρ = 0.007. To
explore the impact of ρ, Figure 4 plots protection success
rate vs. ρ when the tracker runs on the FaceScrub dataset.
Fawkes achieves 100% protection success rate when ρ >
0.005. Figure 5 shows original and cloaked images, demon-
strating that cloaking does not visually distort the original
image. Even when ρ = 0.007, the perturbation is barely de-
tectable by the naked eye on a full size, color image. For cali-
bration, note that prior work [28] claims much higher DSSIM
values (up to 0.2) are imperceptible to the human eye. Finally,
the average L2 norm of our cloaks is 5.44, which is smaller
than that of perturbations used in prior works [29, 59].
Feature Space Deviation. The goal of a cloak is to change
the image’s feature space representation in the tracker’s
model. To examine the effect of the cloak in the tracker
model, we visualize feature space representations of user im-
ages before and after cloaking, their chosen target images,
USENIX Association
29th USENIX Security Symposium    1595
i
2
n
o
s
n
e
m
D
i
Original Images
Other Images
Target Images
 0.5
 0.4
 0.3
 0.2
 0.1
 0
-0.1
-0.2
-0.3
i
2
n
o
s
n
e
m
D
i
Cloaked Images
Other Images
Target Images
 0.5
 0.4
 0.3
 0.2
 0.1
 0
-0.1
-0.2
-0.3
-0.4
-0.2
 0
 0.2
 0.4
-0.4
-0.2
 0
 0.2
 0.4
Dimension 1 
(a) Before Cloaking
Dimension 1 
(b) After Cloaking
e
t
a
R
s
s
e
c
c
u
S
n
o
i
t
c
e
t
o
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 0.002
 0.004
 0.006
 0.008
 0.01
DSSIM Perturbation Budget
Figure 3: 2-D PCA visualization of VGG2-Dense feature space representations of
user images (sampled from FaceScrub) before/after cloaking. Triangles are user’s
images, red crosses are target images, grey dots are images from another class.
Figure 4: Protection performance as
DSSIM perturbation budget increases.
(User/Tracker: Web-Incept)
and a randomly chosen class from the tracker’s dataset. We
use principal components analysis (PCA, a common dimen-
sionality reduction technique) to reduce the high dimensional
feature space to 2 dimensions. Figure 3 shows the PCA re-
sults for cloaked images from a PubFig class, using cloaks
constructed on the Web-Incept feature extractor. Figure 3(a)
shows the feature space positions of the original and target
images before cloaking, along with a randomly selected class.
Figure 3(b) shows the updated feature space after the original
images have been cloaked. It is clear that feature space repre-
sentations of the cloaked images are well-aligned with those
of the target images, validating our intuition for cloaking (an
abstract view in Figure 2).
Impact of Label Density. As discussed in §3, the number
of labels present in the tracker’s model impacts performance.
When the tracker targets fewer labels, the feature space is
“sparser,” and there is a greater chance the model continues to
associate the original feature space (along with the cloaked
feature space) with the user’s label. We empirically evalu-
ate the impact of fewer labels on cloaking success using the
PubFig and FaceScrub datasets (65 and 530 labels, respec-
tively). We randomly sample N labels (varying N from 2 to
10) to construct a model with fewer labels. Figure 6 shows
that for PubFig, cloaking success rate grows from 68% for
2 labels to > 99% for more than 6 labels, conﬁrming that a
higher label density improves cloaking effectiveness.
5.3 User/Tracker Using Different Feature Ex-
tractors
We now consider the scenario when the user and tracker
use different feature extractors to perform their tasks. While
the model transferability property suggests that there are sig-
niﬁcant similarities in their respective model feature spaces
(since both are trained to recognize faces), their differences
could still reduce the efﬁcacy of cloaking. Cloaks that shift
image features signiﬁcantly in one feature extractor may pro-
duce a much smaller shift in a different feature extractor.
To illustrate this, we empirically inspect the change in fea-
ture representation between two different feature extractors.
Original
Cloaked
Original
Cloaked
Original
Cloaked
Figure 5: Pairs of original and cloaked images (ρ = 0.007).
We take the cloaked images (optimized using VGG2-Dense),
original images, and target images from the PubFig dataset
and calculate their feature representations in a different fea-
ture extractor, Web-Incept. The result is visualized using
two dimensional PCA and shown in Figure 7. From the PCA
visualization, the reduction in cloak effectiveness is obvious.
In the tracker’s feature extractor, the cloak “moves” the origi-
nal image features only slightly towards the target image fea-
tures (compared to Figure 3(b)).
Robust Feature Extractors Boost Transferability. To ad-
dress the problem of cloak transferability, we draw on recent
work linking model robustness and transferability. Demontis
[14] argue that an input perturbation’s (in our case,
et al.
cloak’s) ability to transfer between models depends on the
“robustness” of the feature extractor used to create it. They
show that more “robust” models are less reactive to small
perturbations on inputs. Furthermore, they claim that pertur-
bations (or, again, cloaks) generated on more robust models
will take on “universal” characteristics that are able to effec-
tively fool other models.
Following this intuition, we propose to improve cloak
transferability by increasing the user feature extractor’s ro-
bustness. This is done by applying adversarial training [18,
30], which trains the model on perturbed data to make it
less sensitive to similar small perturbations on inputs. Specif-
ically, for each feature extractor, we generate adversarial ex-
amples using the PGD attack [25], a widely used method
1596    29th USENIX Security Symposium
USENIX Association
User’s Robust
Feature
Extractor
VGG2-Incept
VGG2-Dense
Web-Incept
Web-Dense
VGG2-Incept
Model Trainer’s Feature Extractor
VGG2-Dense
Web-Incept
Web-Dense
PubFig
100%
100%
100%
100%
FaceScrub
100%
100%
100%
100%
PubFig
100%
100%
100%
100%
FaceScrub
100%
100%
100%
100%
PubFig
95%
100%
100%
100%
FaceScrub
100%
100%
100%
97%
PubFig
100%
100%
99%
100%
FaceScrub
100%
100%
99%
96%
Table 3: Protection performance of cloaks generated on robust feature extractors.
for adversarial training. Following prior work [30], we run
the PGD4 algorithm for 100 steps using a step size of 0.01.
We train each feature extractor for an additional 10 epochs.
These updated feature extractors are then used to generate
user cloaks on the PubFig and FaceScrub datasets.
Results in Table 3 show that each robust feature extractor
produces cloaks that transfer almost perfectly to the tracker’s
models. Cloaks now have protection success rates > 95%
when the tracker uses a different feature extractor. We visu-
alize their feature representation using PCA in Figure 8 and
see that, indeed, cloaks generated on robust extractors trans-
fer better than cloaks computed on normal ones.
5.4 Tracker Models Trained from Scratch
Finally, we consider the scenario in which a powerful tracker
trains their model from scratch. We select the user U to be
a label inside the WebFace dataset. We generate cloaks on
user images using the robust VGG2-Incept feature extractor
from §5.3. The tracker then uses the WebFace dataset (but
U’s cloaked images) to train their model from scratch. Again
our cloas achieve a success rate of 100%. Other combina-
tions of labels and user-side feature generators all have 100%
protection success.
6 Image Cloaking in the Wild
Our results thus far have focused on limited conﬁgurations,
including publicly available datasets and known model ar-
chitectures. Now, we wish to understand the performance of
Fawkes on deployed facial recognition systems in the wild.
We evaluate the real-world effectiveness of image cloak-
ing by applying Fawkes to photos of one of the co-authors.
We then intentionally leak a portion of these cloaked photos
to public cloud-based services that perform facial recogni-
tion, including Microsoft Azure Face [3], Amazon Rekogni-
tion [2], and Face++ [4]. These are the global leaders in facial
recognition and their services are used by businesses, police,
private entities, and governments in the US and Asia.
4We found that robust models trained on CW attack samples [10] pro-
duce similar results
Face
Recognition
API
Microsoft Azure
Face API
Amazon Rekognition
Face Veriﬁcation
Face++
Face Search API
Protection Success Rate
Without
protection
Protected by
normal cloak
Protected by
robust cloak
0%
0%
0%
100%
34%
0%
100%
100%
100%
Table 4: Cloaking is highly effective against cloud-based face
recognition APIs (Microsoft, Amazon and Face++).
6.1 Experimental Setup
We manually collected 82 high-quality pictures of a co-
author that feature a wide range of lighting conditions, poses,
and facial expressions. We separate the images into two
subsets, one set of 50 images for “training” and one set of
32 images for “testing.” We generate both normal and ro-
bust cloaks for the “training” images using the setup dis-
cussed in Section 5 (using normal and robust versions of the
Web-Incept feature extractor). This allows us to compare
the relative effectiveness of normal and robust user feature
extractors in real life.
For each API service, we experiment with three scenarios:
• Unprotected: We upload original training images, and test
the model’s classiﬁcation accuracy on testing images.
• Normal Cloak: We upload training images protected by
a nonrobust cloak and then test the model’s classiﬁcation
accuracy on the testing images.
• Robust Cloak: We upload training images protected by
a robust cloak and test the model’s classiﬁcation accuracy
on the testing images.
For each scenario, we use the online service APIs to up-
load training images to the API database, and then query the
APIs using the uncloaked testing images. The reported pro-
tection success rate is the proportion of uncloaked test im-
ages that the API fails to correctly identify as our co-author.
6.2 Real World Protection Performance
Microsoft Azure Face API. Microsoft Azure Face API [3]
is part of Microsoft Cognitive Services, and is reportedly
used by many large corporations including Uber and Jet.com.
The API provides face recognition services. A client uploads
USENIX Association
29th USENIX Security Symposium    1597
e
t
a
R
s
s
e
c
c
u
S
n
o
i
t
c
e
t