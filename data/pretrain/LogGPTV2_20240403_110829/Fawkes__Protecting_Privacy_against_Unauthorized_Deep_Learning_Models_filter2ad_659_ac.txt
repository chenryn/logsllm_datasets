### Image Cloaking and Tracker Model Performance

#### 1. Methodology
We evaluate the effectiveness of cloaked images using four different feature extractors, as listed in Table 1. On the tracker side, we perform transfer learning on the same feature extractor (trained with cloaked images of user U) to build a model that can recognize U. We then assess whether the tracker model can accurately identify other clean images of U that it has not seen before.

#### 2. Results
Our results show that cloaking provides perfect protection: U is always misclassified as someone else for all four feature extractors under a perturbation budget \(\rho = 0.007\). To explore the impact of \(\rho\), Figure 4 plots the protection success rate against \(\rho\) when the tracker uses the FaceScrub dataset. Fawkes achieves a 100% protection success rate when \(\rho > 0.005\). Figure 5 illustrates original and cloaked images, demonstrating that cloaking does not visually distort the original image. Even at \(\rho = 0.007\), the perturbation is barely detectable by the naked eye on a full-size, color image. For calibration, prior work [28] claims that much higher DSSIM values (up to 0.2) are imperceptible to the human eye. The average L2 norm of our cloaks is 5.44, which is smaller than the perturbations used in previous studies [29, 59].

#### 3. Feature Space Deviation
The goal of cloaking is to alter the image's feature space representation in the tracker’s model. To examine this effect, we visualize the feature space representations of user images before and after cloaking, along with their chosen target images and a randomly selected class from the tracker’s dataset. We use principal component analysis (PCA) to reduce the high-dimensional feature space to two dimensions. Figure 3 shows the PCA results for cloaked images from a PubFig class, using cloaks constructed with the Web-Incept feature extractor. Figure 3(a) displays the feature space positions of the original and target images before cloaking, along with a randomly selected class. Figure 3(b) shows the updated feature space after the original images have been cloaked. It is evident that the feature space representations of the cloaked images align well with those of the target images, validating our cloaking intuition (as shown in Figure 2).

#### 4. Impact of Label Density
As discussed in §3, the number of labels in the tracker’s model affects performance. When the tracker targets fewer labels, the feature space is sparser, increasing the likelihood that the model continues to associate the original feature space (and the cloaked feature space) with the user’s label. We empirically evaluate the impact of fewer labels on cloaking success using the PubFig and FaceScrub datasets (65 and 530 labels, respectively). We randomly sample \(N\) labels (varying \(N\) from 2 to 10) to construct a model with fewer labels. Figure 6 shows that for PubFig, the cloaking success rate increases from 68% for 2 labels to over 99% for more than 6 labels, confirming that higher label density improves cloaking effectiveness.

#### 5. User/Tracker Using Different Feature Extractors
We now consider the scenario where the user and tracker use different feature extractors. While the model transferability property suggests significant similarities in their respective feature spaces (since both are trained to recognize faces), differences can still reduce the efficacy of cloaking. Cloaks that significantly shift image features in one feature extractor may produce a much smaller shift in a different feature extractor. To illustrate this, we empirically inspect the change in feature representation between two different feature extractors. We take the cloaked images (optimized using VGG2-Dense), original images, and target images from the PubFig dataset and calculate their feature representations in a different feature extractor, Web-Incept. The result, visualized using two-dimensional PCA in Figure 7, shows a clear reduction in cloak effectiveness. In the tracker’s feature extractor, the cloak only slightly shifts the original image features towards the target image features compared to Figure 3(b).

#### 6. Robust Feature Extractors Boost Transferability
To address the problem of cloak transferability, we draw on recent work linking model robustness and transferability. Demontis et al. [14] argue that an input perturbation’s (in our case, cloak’s) ability to transfer between models depends on the “robustness” of the feature extractor used to create it. More robust models are less reactive to small perturbations on inputs, and perturbations generated on more robust models take on “universal” characteristics that effectively fool other models. Following this intuition, we propose to improve cloak transferability by increasing the user feature extractor’s robustness. This is done by applying adversarial training [18, 30], which trains the model on perturbed data to make it less sensitive to similar small perturbations on inputs. Specifically, for each feature extractor, we generate adversarial examples using the PGD attack [25], a widely used method for adversarial training. We run the PGD algorithm for 100 steps using a step size of 0.01 and train each feature extractor for an additional 10 epochs. These updated feature extractors are then used to generate user cloaks on the PubFig and FaceScrub datasets. Results in Table 3 show that each robust feature extractor produces cloaks that transfer almost perfectly to the tracker’s models, with protection success rates > 95% when the tracker uses a different feature extractor. We visualize their feature representation using PCA in Figure 8, confirming that cloaks generated on robust extractors transfer better than those computed on normal ones.

#### 7. Tracker Models Trained from Scratch
Finally, we consider the scenario where a powerful tracker trains their model from scratch. We select user U as a label inside the WebFace dataset. We generate cloaks on user images using the robust VGG2-Incept feature extractor from §5.3. The tracker then uses the WebFace dataset (but with U’s cloaked images) to train their model from scratch. Our cloaks achieve a 100% success rate, and other combinations of labels and user-side feature generators also have 100% protection success.

### 6. Image Cloaking in the Wild

#### 6.1 Experimental Setup
To understand the real-world performance of Fawkes, we apply it to photos of one of the co-authors. We intentionally leak a portion of these cloaked photos to public cloud-based services that perform facial recognition, including Microsoft Azure Face [3], Amazon Rekognition [2], and Face++ [4]. These services are global leaders in facial recognition and are used by businesses, police, private entities, and governments in the US and Asia.

We manually collected 82 high-quality pictures of a co-author, featuring a wide range of lighting conditions, poses, and facial expressions. We separate the images into two subsets: 50 images for “training” and 32 images for “testing.” We generate both normal and robust cloaks for the “training” images using the setup discussed in Section 5 (using normal and robust versions of the Web-Incept feature extractor). This allows us to compare the relative effectiveness of normal and robust user feature extractors in real life.

For each API service, we experiment with three scenarios:
- **Unprotected**: Upload original training images and test the model’s classification accuracy on testing images.
- **Normal Cloak**: Upload training images protected by a non-robust cloak and test the model’s classification accuracy on the testing images.
- **Robust Cloak**: Upload training images protected by a robust cloak and test the model’s classification accuracy on the testing images.

For each scenario, we use the online service APIs to upload training images to the API database and then query the APIs using the uncloaked testing images. The reported protection success rate is the proportion of uncloaked test images that the API fails to correctly identify as our co-author.

#### 6.2 Real-World Protection Performance
**Microsoft Azure Face API**: Part of Microsoft Cognitive Services, the Microsoft Azure Face API [3] is reportedly used by many large corporations, including Uber and Jet.com. The API provides face recognition services. A client uploads training images to the API database and then queries the API using the uncloaked testing images. Table 4 summarizes the protection success rates for the three scenarios:

| Face Recognition API | Without Protection | Protected by Normal Cloak | Protected by Robust Cloak |
|----------------------|---------------------|----------------------------|---------------------------|
| Microsoft Azure Face API | 0% | 34% | 100% |
| Amazon Rekognition | 0% | 0% | 100% |
| Face++ | 0% | 0% | 100% |

These results demonstrate that Fawkes is highly effective against cloud-based face recognition APIs, especially when using robust cloaks.