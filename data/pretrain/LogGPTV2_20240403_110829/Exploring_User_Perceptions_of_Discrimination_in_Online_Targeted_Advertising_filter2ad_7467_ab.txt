### 2.6%
### 41.3%
### 31.0%
### 27.7%
### 20.9%
### 34.7%
### 26.0%
### 18.4%
### 47.6%
### 52.4%
### 67.0%
### 12.0%
### 5.0%
### 13.1%
### 2.9%
### 18.5%
### 40.0%
### 41.6%
### 23.7%
### 38.8%
### 23.5%
### 14.1%

**Figure 2: Problem severity, organized by targeting mechanism (Pilot 1).**

**Table 3: Respondent demographics, Pilot 1, compared to 2015 U.S. Census figures [41]**

**Figure 3: Problem severity, organized by human or algorithmic decision (Pilot 1).**

### Analysis of Problem Severity

In evaluating the severity of various scenarios, a notable pattern emerged: scenarios that targeted individuals based on behavior (e.g., browsing history) rather than explicit demographics were generally rated as less problematic (see Figure 2).

### Role of Decision Maker

We hypothesized that whether a human or an algorithm made the decision to target the advertisement would significantly influence respondents' perceptions of the scenario. However, the pilot study did not provide evidence supporting this hypothesis. Despite this, we decided to include this factor in subsequent studies to confirm (or disprove) its lack of importance (see Figure 3).

### 5. Main Study

Based on the results from Pilot 1, we designed our final survey. Below, we detail the content of this final survey and the results of our generation and validation of regression models for the analysis of this data.

#### 5.1 Final Survey Instrument

Our final survey contrasted demographic and behavioral explanations, as well as human and algorithmic decisions. Given the confusion about which entity in the complex advertising ecosystem makes decisions that can have discriminatory outcomes, and our interest in questions of responsibility, we included a factor locating the decision-making either at Systemy (the company placing the ad) or Bezo (the ad network). The local news site was excluded as a potential decision-maker because it did not yield particularly interesting results in Pilot 1.

**Figure 1: Problem severity, organized by target (Pilot 1).**

One key goal was to develop a smaller set of issues to focus on in follow-up studies.

### Targeting Issues

First, we considered the issue of who was targeted in the scenario, i.e., which group of people benefited from or was shortchanged by the discriminatory advertising. We found that scenarios targeting race were more likely to be considered problematic than those targeting age, political affiliation, and health condition (see Figure 1). Opinions about which groups are targeted touch on a range of cultural and sociological issues that are not unique to online targeted advertising. Therefore, we decided to limit future scenarios to targeting race, in the interest of provoking more dramatic reactions that might allow us to identify interesting explanation-based differences.

### Respondents' Reactions

Second, we considered respondents' reactions to the different targeting mechanisms. The most noticeable pattern was that scenarios targeting based on behavior (e.g., browsing history) rather than explicit demographics were generally rated as less problematic (see Figure 2).

### Hypothesis Testing

Third, we hypothesized that whether a human or an algorithm made the decision to target the advertisement would play an important role in respondents' perceptions of the scenario. We were surprised to find no evidence for this in the pilot, but we decided to include it in our subsequent studies to confirm (or disprove) its lack of importance (see Figure 3).

### 5.2 Pilot 2: Training Data Generation

Before running the final data collection with this survey, we conducted an additional pilot survey to generate training data for testing various potential regression models without worrying about erosion of statistical confidence due to multiple testing. This allowed us to narrow down the breadth of potential covariates to only the most relevant.

#### 5.2.1 Respondents

The goal of Pilot 2 was to create training data for selecting a final set of regression models to be confirmed with a larger data collection. We deployed a four- to five-minute survey to 191 respondents using Amazon's Mechanical Turk (MTurk) crowdsourcing service. MTurk has been shown to provide adequate data quality, though the sample tends to be younger and more educated than the general population [24, 26]. We required respondents to have an approval rate of at least 85% on the MTurk service and reside in the U.S., and compensated them $0.75 each. To avoid duplicate respondents, each participant's unique MTurk identification number was recorded, and duplicate IDs were prevented from completing the survey again. Detailed demographics can be found in Table 6.

**Table 6: Respondent demographics for Pilot 2, compared to figures from the 2015 U.S. Census [41].**

#### 5.2.2 Analysis and Results

Since the majority of our survey questions used Likert scales, we primarily analyzed our data using logistic regression, which measures how several different input factors correlate with a step increase in the output Likert variable being studied [23]. This allowed us to examine how both our experimental factors and demographic covariates correlated with respondents' reactions to the presented scenario.

For the degree of responsibility and problem questions, we generated an initial model including the experimental factors (the target, mechanism, decider, and entity variables from Table 4); participant demographic covariates including age, gender, ethnicity, and education level; and pairwise interactions between various factors. We then compared a variety of models using subsets of these covariates, looking for the best fit according to the lowest Akaike Information Criterion (AIC) [4]. (We included the experimental factors in every model we considered.)

For each question, multiple models had very close AIC values. From among those with near-minimal AIC for each of the five questions, we selected a final model that included the four experimental factors—target, mechanism, decider, and entity—along with the demographic covariates that appeared most relevant. No pairwise interactions were included in the final model. The final set of factors and covariates is summarized in Table 7.

**Table 7: Factors used in the regression models for problem, responsibility, ethics, and believability. The sample provider factor was used in the main study only, not in Pilot 2.**

#### 5.3 Final Survey Results

To validate the regression model developed during Pilot 2, we conducted a final, larger-scale data collection with our final survey instrument. To promote high data quality and broad generalizability, we deployed our survey with both MTurk and SSI. We again required Turkers to have an 85% approval rate and compensated them $0.75; we paid SSI $3.00 per completion. Respondents from both the first and second pilot studies were excluded from participation in this survey. To account for differences in the two samples, we added the sample provider as a covariate to our regression model (shown at the bottom of Table 7).

**Table 8: Summary of regression results.**

#### 5.3.1 Respondents

We collected responses from 535 MTurk respondents and 372 SSI respondents, for a total of 907. Demographics for the two samples are shown in Table 9, with U.S. Census data for comparison [41].

The 16 respondents who reported their race as "other" were excluded from the dataset, as the small sample frequently prevented the regression model from converging. All further results are therefore reported for the remaining 891 respondents, or for slightly fewer when respondents answered "don't know" to certain questions.

**Table 9: Demographics for the two samples, with U.S. Census data for comparison [41].**

#### 5.3.2 Model Validation

To verify that the set of factors and covariates we selected in Pilot 2 were also reasonable for our final data, we verified that the error rate when applying this regression to the final dataset was within the confidence interval of the error rate observed on our training data (i.e., the Pilot 2 data). Specifically, we bootstrapped [15] root mean square error (RMSE) [31] confidence intervals from the final dataset and compared them to the RMSE from the training data.

#### 5.3.3 Severity of Problem

The final regression model validated the selection of factors and covariates, confirming that the model was appropriate for the final dataset. The results are summarized in Table 8, showing the significant effects of various factors on the severity, responsibility, and ethical perceptions of the scenarios.

**USENIX Association**
**26th USENIX Security Symposium**