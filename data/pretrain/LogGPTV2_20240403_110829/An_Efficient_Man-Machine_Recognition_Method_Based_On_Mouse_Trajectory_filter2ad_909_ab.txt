tical due to time bias or sampling point collection bias, but evasion
attacks can be detected from the perspective of feature similarity.
Therefore, evasion detection of all sample sets can be a prerequisite
for model prediction.
4 HUMAN SLIDING BEHAVIOR
We processed the human mouse sliding data graphically and sam-
pled the human sliding sample data equidistantly. After drawing
the X-Y diagram of the moving path, we can see that the human
behavior is similar to the machine behavior in the sliding trajectory
by observing the coordinate diagram.
Machine behavior can be divided into simple machine behavior
and complex machine behavior. Simple machine behavior focuses
on the starting and ending positions, with little or no consideration
for sliding modes. Hopping movements with little or no lateral
deviation are common. Complex machine behavior mimics human
367ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Xiaofeng Lu, Zhenhan Feng, and Jupeng Xia
Figure 6: X-Y of human behavior.
acceleration, deceleration, and bias, but there are few reentry phe-
nomena. From T-X figure of machine behavior and human behavior,
we can more clearly observe the changes of mouse point displace-
ment under time conditions and derive velocity characteristics.
human.png
Figure 7: Time-X of human behavior.
By looking at the time series coordinate of Figure 7, we found
that there is a significant difference between human and machine
sliding behavior in time series. Machine behavior was more similar
to uniform motion, and the sample points were arranged densely.
Mouse sliding track is time series data, we use fuzzy entropy to
distinguish the fuzziness of series data, The formula for calculating
fuzzy entropy is[5]:
(1) Defines the distance dn
X n
i and X n
elements, Defines the similarity Dn
the fuzzy function µ(dn
ij between two n-dimensional vectors
j to be the largest difference between their corresponding
j with
ij of two vectors X n
i and X n
ij , m, r), that is:
ij = µ(dn
ij , m, r) = e
Dn
i j )m
(−dn
r
In the formula (1), µ(dn
(1)
ij , m, r) is an exponential function, where
m and r are the gradient and width of the exponential function
boundary, respectively.
(2) Define function
On(m, r) =
M−n
M−n
(2)
{
ij}
Dn
1
M − n
1
M − n − 1
i =1
(3) Fuzzy entropy is defined as
j=1, j(cid:44)i
FuzzyEn(n, m, r) = lim
M→∞[ln On(m, r) − ln On+1(m, r)]
(3)
Because the sampling intervals were different, the displacement
coordinates cannot be used as the basis for judging the fuzzy en-
tropy. We used the velocity values of each small segment of dis-
placement as the input sample series of the fuzzy entropy. Figure 8
shows the fuzzy entropy of 3000 sample data. Of the 3000 samples,
the first 2599 were human behavior, and the 2600 to 3000 were
machine behavior. Figure 8 shows that the fuzzy entropy value of
simple machine behavior is low, the speed variation value is regular,
Figure 8: Fuzzy entropy.
and can be easily distinguished from human behavior. Complex
machine behavior mimics human motion track acceleration and
deceleration, and the speed sequence changes greatly, so the fuzzy
entropy value remains stable at a high level. The uncertainty of
human sliding track speed is high, but there are multiple uniform
motion segments in the track when fast sliding occurs.
From the point of view of feature extraction, the calculation
of fuzzy entropy value is extremely time-consuming, and it takes
about 130 seconds to calculate 3000 samples, which does not meet
the requirements of fast validation.
5 A FAST HUMAN-MACHINE RECOGNITION
METHOD BASED ON MOUSE SLIDING
5.1 Feature Extraction
Mouse gliding behavior as a motion track has motion-related phys-
ical quantities, the mouse itself is considered as a particle, and the
sliding plane is used as a two-dimensional reference system to ob-
tain parameters related to displacement, speed and acceleration. In
human recognition verification, a reference coordinate system is es-
tablished with the upper left corner of the screen as the origin, and
track data is collected when the slider verification code is sliding
horizontally. Human behavior has longitudinal deviations during
lateral gliding, with significant acceleration and deceleration, sta-
tionary and retraction phenomena. Simple machine behavior jumps
significantly, including sampling points with large displacement
spans, and complex machine behavior mimics human gliding to
produce longitudinal deviation and static behavior. We calculate the
sliding characteristics from displacement, speed and acceleration
information, and extract additional feature information based on
other differences between human and machine tracks.
Defines the difference distance as the value of the last point in
the whole distance vector minus the value of the previous point.
Similarly, the differential velocity and differential acceleration can
be obtained. The time vector is the desensitization time stamp
collected when the coordinate point changes. We used different
datasets, and the sampling frequencies are different. So, the time
information is not used as a feature. However, the time difference
is used as the basis for classification.
368An Efficient Man-Machine Recognition Method Based On Mouse Trajectory Feature De-redundancy
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Define the horizontal coordinate vectors separately (cid:174)X = [x1, x2
, x3, . . . , xn], vertical coordinate vector (cid:174)Y = [y1, y2, y3, . . . , yn],
time vector (cid:174)T = [t1, t2, t3, . . . , tn]. Coordinate difference can be
(cid:174)∆X = [∆x1, ∆x2∆x3, . . . , ∆xn−1],
(cid:174)∆Y = [∆y1,
computed,
(cid:174)∆T = [∆t1, ∆t2
∆y2∆y3, . . . , ∆yn−1] , time difference vector
∆t3, . . . , ∆tn−1], velocity vector (cid:174)V = (cid:174)∆X(cid:174)∆T
= [v1, v2, v3, . . . , vn−2],
acceleration vector (cid:174)A = (cid:174)V(cid:174)∆T
= [a1, a2, a3, ..., an−2]. Velocity dif-
ference vector can be obtained by the same calculation (cid:174)∆V =
[∆v1, ∆v2∆v3, . . . , ∆vn−2] and acceleration difference vector (cid:174)∆A =
[∆a1, ∆a2∆a3, . . . , ∆an−2].
We divide the entire sliding track into three stages, the start
segment, the middle segment and the end segment. For each phys-
ical quantity, start, middle, and end points can be calculated, and
machine behavior and human behavior differ from each other in
local feature analysis. Meanwhile, maximum, minimum, mean and
standard deviation are used to describe the feature information of
the whole sliding process.
Figure 9: Example diagram of slider verification codes.
As shown in Figure 9, due to the large or small longitudinal
deviation when human operates mouse movement, Therefore, from
the perspective of two-dimensional space, the detection area is not
only the slider track area. The lateral and vertical features are useful
for human-machine classification, such as the lateral differential
velocity and the vertical differential velocity, so the lateral and
vertical sliding features in two-dimensional space are calculated
separately. Because the slider verification code itself is a lateral
directional sliding behavior and contains only minimal vertical
deviation. Therefore, from the point of view of feature distinction,
the vertical vector feature distinction is low, while the horizontal
vector feature distinction is high.
In addition to the above calculated features, we also extract some
other features, such as Euclidean distance, the sum of differential
distances, the distance between maximum transverse coordinate
and target point, the relationship between maximum transverse
coordinate and final point, which is used to judge the retrace of
sliding, and the longitudinal deviation which is used to judge the
stability of sliding.
After the above feature extraction methods, 119 feature vectors
are obtained.
5.2 Feature Selection
XGBoost calculates and selects which feature as the split point
based on the increase of the structure fraction. The importance of a
feature is the sum of the number of times it occurs in all trees. The
more occurrences of a feature used to construct a decision tree in
the model, the more important the feature will be. To avoid feature
redundancy and noise problems caused by too many features, 20
features with scores higher than 10 were selected as candidate
features after model fitting training.
In addition, the problem of collinearity exists even when features
with high scores. Feature collinearity and high correlation can bias
the trained model. The specific manifestations are as follows:
(1) The feature importance of strong features decreases. If there
are many groups of strong features with high correlation, the fea-
ture importance of strong features will be diluted and the validity
of features will be reduced.
(2) It affects the generalization performance of the model, and
there are a number of highly correlated features in the mouse sliding
behavior, If the vertical deviation is small, the displacement, velocity
and acceleration information will not change. It makes displacement
and velocity, which should be irrelevant, appear collinearity. One of
the reasons for the good generalization performance of XGBoost is
that the base models in different feature spaces and sample spaces
can be constructed by row-column sampling. If there is a large
amount of collinearity in the features, the types of subspaces that
can be used in the base model will be reduced, the integration
model will be weakened, and the diversity will be reduced, so the
generalization performance of the model will also be reduced.
(3) High-dimensional features consume too much time and mem-
ory, and produce noise (insignificant features) interference. Al-
though XGBoost is robust to noise in a large sample set, splitting
trees like XGBoost on irrelevant features will not properly assess
the validity of features.
n
(cid:113)n
i =1(Xi − X)2(cid:113)n
i =1(Xi − X)(Yi − Y)
i =1(Yi − Y)2
r =
(4)
There is a linear correlation between the characteristics of the
mouse slide track data, for example, because the lateral slide span
is large and there is only a slight deviation in the longitudinal slide,
there is a linear correlation between the lateral velocity or dis-
tance and the two points (coordinate points of the two-dimensional
plane).
Pearson correlation coefficient can effectively assess the correla-
tion between features, which is recorded as r. In this paper, pearson
correlation coefficient is used to reflect the linear correlation degree
of two features X and Y. The r value is between -1 and 1, and the
greater the absolute value, the stronger the correlation.
We compute Pearson coefficients between any two features to
filter out highly correlated features. Considering memory consump-
tion and time performance, the best classification results are ob-
tained by using the least features possible. In addition to the sample
number and label, seven features are obtained, two features are
obtained from the classification of attack models, so nine features
are obtained.
5.3 Human Recognition Model Based on
XGBoost
Random forest, gradient lifting trees, and convolution neural net-
works [9, 13, 19–21] have been used in human-machine identifica-
tion and behavioral validation based on machine learning methods.
369ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Xiaofeng Lu, Zhenhan Feng, and Jupeng Xia
Table 1: Features and Descriptions
Description
Distance between maximum x-coordinate and target point
Minimum x-coordinate
Maximum x-coordinate difference
Feature
x_max_target
data_x_min
delt_x_max
acc_speed_x_start X-coordinate initial acceleration
speed_x_end
data_y_start
speed_xy_start
delt_xy_start
delt_speed_t_start The difference in the degree of change in starting time
X-coordinate end speed
Y-coordinate start value
Slide start speed
Slide start difference
Random forest models tend to get stuck in over-fitting in noisy sam-
ple sets, gradient-lifting tree serial training data takes time on larger
datasets, and has lower classification performance than Extreme
Gradient Boost (XGBoost). Convolutional neural networks are used
to process graphical mouse sliding tracks and are not suitable for
recognition in directional sliding scenes.
In this paper, XGBoost model algorithm is used as classifier.
Compared with other machine learning algorithms, XGBoost not
only has a good classification effect, but also can effectively prevent
the occurrence of overfitting and improve the generalization ability
of the model.
XGBoost is a variant of the GBDT algorithm. It is a commonly
used supervised integrated learning algorithm with high flexibility,
scalability and the ability to build parallel models. It supports a
variety of weak learners such as gbtree, gblinear. XGBoost’s tree
model training is much better than linear models. Compared with
the GBDT algorithm, XGBoost not only uses the first derivative
information, but also uses the second-order Taylor expansion of
the cost function. To prevent overfitting, regular terms are added
to the target function of XGBoost. For example, formula (5):
res =
l(yi , ˆyi) +
Ω(fi)
(5)
t
i =1
n
i =1
t
n=1
XGBoost follows the forward iteration method, and the t th