title:When Does Machine Learning FAIL? Generalized Transferability for Evasion
and Poisoning Attacks
author:Octavian Suciu and
Radu Marginean and
Yigitcan Kaya and
Hal Daum&apos;e III and
Tudor Dumitras
When Does Machine Learning FAIL? Generalized 
Transferability for Evasion and Poisoning Attacks
Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III,  
and Tudor Dumitras, University of Maryland
https://www.usenix.org/conference/usenixsecurity18/presentation/suciu
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.When Does Machine Learning FAIL? Generalized Transferability for
Octavian Suciu
Evasion and Poisoning Attacks
Radu M˘arginean
Yi˘gitcan Kaya
Tudor Dumitras,
University of Maryland, College Park
Hal Daum´e III
Abstract
Recent results suggest that attacks against supervised
machine learning systems are quite effective, while de-
fenses are easily bypassed by new attacks. However,
the speciﬁcations for machine learning systems currently
lack precise adversary deﬁnitions, and the existing at-
tacks make diverse, potentially unrealistic assumptions
about the strength of the adversary who launches them.
We propose the FAIL attacker model, which describes
the adversary’s knowledge and control along four dimen-
sions. Our model allows us to consider a wide range of
weaker adversaries who have limited control and incom-
plete knowledge of the features, learning algorithms and
training instances utilized.
To evaluate the utility of the FAIL model, we consider
the problem of conducting targeted poisoning attacks in
a realistic setting: the crafted poison samples must have
clean labels, must be individually and collectively incon-
spicuous, and must exhibit a generalized form of trans-
ferability, deﬁned by the FAIL model. By taking these
constraints into account, we design StingRay, a targeted
poisoning attack that is practical against 4 machine learn-
ing applications, which use 3 different learning algo-
rithms, and can bypass 2 existing defenses. Conversely,
we show that a prior evasion attack is less effective under
generalized transferability. Such attack evaluations, un-
der the FAIL adversary model, may also suggest promis-
ing directions for future defenses.
1
Introduction
Machine learning (ML) systems are widely deployed
in safety-critical domains that carry incentives for po-
tential adversaries, such as ﬁnance [14], medicine [18],
the justice system [31], cybersecurity [1], or self-driving
cars [6]. An ML classiﬁer automatically learns classiﬁ-
cation models using labeled observations (samples) from
a training set, without requiring predetermined rules for
mapping inputs to labels. It can then apply these mod-
els to predict labels for new samples in a testing set. An
adversary knows some or all of the ML system’s param-
eters and uses this knowledge to craft training or testing
samples that manipulate the decisions of the ML system
according to the adversary’s goal—for example, to avoid
being sentenced by an ML-enhanced judge.
Recent work has focused primarily on evasion at-
tacks [4, 44, 17, 50, 35, 9], which can induce a targeted
misclassiﬁcation on a speciﬁc sample. As illustrated in
Figures 1a and 1b, these test time attacks work by mu-
tating the target sample to push it across the model’s de-
cision boundary, without altering the training process or
the decision boundary itself. They are not applicable in
situations where the adversary does not control the tar-
get sample—for example, when she aims to inﬂuence a
malware detector to block a benign app developed by a
competitor. Prior research has also shown the feasibility
of targeted poisoning attacks [34, 32]. As illustrated in
Figure 1c, these attacks usually blend crafted instances
into the training set to push the model’s boundary toward
the target. In consequence, they enable misclassiﬁcations
for instances that the adversary cannot modify.
These attacks appear to be very effective, and the
defenses proposed against them are often bypassed in
follow-on work [8]. However, to understand the actual
security threat introduced by them, we must model the
capabilities and limitations of realistic adversaries. Eval-
uating poisoning and evasion attacks under assumptions
that overestimate the capabilities of the adversary would
lead to an inaccurate picture of the security threat posed
to real-world applications. For example, test time attacks
often assume white-box access to the victim classiﬁer
[9]. As most security-critical ML systems use propri-
etary models [1], these attacks might not reﬂect actual
capabilities of a potential adversary. Black-box attacks
consider weaker adversaries, but they often make rigid
assumptions about the adversary’s knowledge when in-
vestigating the transferability of an attack. Transferabil-
USENIX Association
27th USENIX Security Symposium    1299
(a)
(b)
(c)
(d)
Figure 1: Targeted attacks against machine learning classiﬁers. (a) The pristine classiﬁer would correctly classify the target. (b) An evasion attack
would modify the target to cross the decision boundary. (c) Correctly labeled poisoning instances change the learned decision boundary. (d) At
testing time, the target is misclassiﬁed but other instances are correctly classiﬁed.
ity is a property of attack samples crafted locally, on
a surrogate model that reﬂects the adversary’s limited
knowledge, allowing them to remain successful against
the target model. Speciﬁcally, black-box attacks often
investigate transferability in the case where the local and
target models use different training algorithms [36]. In
contrast, ML systems used in the security industry often
resort to feature secrecy (rather than algorithmic secrecy)
to protect themselves against attacks, e.g. by incorporat-
ing undisclosed features for malware detection [10].
In this paper, we make a ﬁrst step towards modeling
realistic adversaries who aim to conduct attacks against
ML systems. To this end, we propose the FAIL model,
a general framework for the analysis of ML attacks in
settings with a variable amount of adversarial knowledge
and control over the victim, along four tunable dimen-
sions: Features, Algorithms, Instances, and Leverage.
By preventing any implicit assumptions about the adver-
sarial capabilities, the model is able to accurately high-
light the success rate of a wide range of attacks in realis-
tic scenarios and forms a common ground for modeling
adversaries. Furthermore, the FAIL framework general-
izes the transferability of attacks by providing a multidi-
mensional basis for surrogate models. This provides in-
sights into the constraints of realistic adversaries, which
could be explored in future research on defenses against
these attacks. For example, our evaluation suggests that
crafting transferable samples with an existing evasion at-
tack is more challenging than previously believed.
To evaluate the utility of the FAIL model, we con-
sider the problem of conducting targeted poisoning at-
tacks in a realistic setting. Speciﬁcally, we impose four
constraints on the adversary. First, the poison samples
must have clean labels, as the adversary can inject them
into the training set of the model under attack but can-
not determine how they are labeled. Second, the samples
must be individually inconspicuous, i.e. to be very sim-
ilar to the existing training instances in order to prevent
an easy detection, while collectively pushing the model’s
boundary toward a target instance. Third, the samples
myst be collectively inconspicuous by bounding the col-
lateral damage on the victim (Figure 1d). Finally, the
poison samples must exhibit a generalized form of trans-
ferability, as the adversary tests the samples on a surro-
gate model, trained with partial knowledge along multi-
ple dimensions, deﬁned by the FAIL model.
By taking into account the goals, capabilities, and lim-
itations of realistic adversaries, we also design StingRay,
a targeted poisoning attack that can be applied in a broad
range of settings 1. Moreover, the StingRay attack is
model agnostic: we describe concrete implementations
against 4 ML systems, which use 3 different classiﬁ-
cation algorithms (convolutional neural network, linear
SVM, and random forest). The instances crafted are able
to bypass three anti-poisoning defenses, including one
that we adapted to account for targeted attacks. By sub-
jecting StingRay to the FAIL analysis, we obtain insights
into the transferability of targeted poison samples, and
we highlight promising directions for investigating de-
fenses against this threat.
In summary, this paper makes three contributions:
• We introduce the FAIL model, a general frame-
work for modeling realistic adversaries and evalu-
ating their impact. The model generalizes the trans-
ferability of attacks against ML systems, across var-
ious levels of adversarial knowledge and control.
We show that a previous black-box evasion attack
is less effective under generalized transferability.
• We propose StingRay, a targeted poisoning at-
tack that overcomes the limitations of prior attacks.
StingRay is effective against 4 real-world classiﬁca-
tion tasks, even when launched by a range of weaker
adversaries within the FAIL model. The attack also
bypasses two existing anti-poisoning defenses.
• We systematically explore realistic adversarial sce-
narios and the effect of partial adversary knowledge
and control on the resilience of ML models against
a test-time attack and a training-time attack. Our
1Our
implementation code
could be
found at https://
github.com/sdsatumd
1300    27th USENIX Security Symposium
USENIX Association
Training InstancesPristine Decision BoundaryTargetTesting InstancesAdversarial ExamplePoisoning InstancesPoisoned Decision BoundaryTesting Instancesresults provide insights into the transferability of at-
tacks across the FAIL dimensions and highlight po-
tential directions for investigating defenses against
these attacks.
This paper is organized as follows. In Section 2 we
formalize the problem and our threat model. In Section 3
we introduce the FAIL attacker model. In Section 4 we
describe the StingRay attack and its implementation. We
present our experimental results in Section 5, review the
related work in Section 6, and discuss the implications in
Section 7.
2 Problem Statement
Lack of a unifying threat model to capture the dimen-
sions of adversarial knowledge caused existing work to
diverge in terms of adversary speciﬁcations. Prior work
deﬁned adversaries with inconsistent capabilities. For
example, in [36] a black-box adversary possesses knowl-
edge of the full feature representations, whereas its coun-
terpart in [50] only assumes access to the raw data (i.e.
before feature extraction).
Compared to existing white-box or black-box models,
in reality, things tend to be more nuanced. A commercial
ML-based malware detector [1] can rely on a publicly
known architecture with proprietary data collected from
end hosts, and a mixture of known features (e.g. system
calls of a binary), and undisclosed features (e.g. reputa-
tion scores of the binary). Existing adversary deﬁnitions
are too rigid and cannot account for realistic adversaries
against such applications. In this paper, we ask how can
we systematically model adversaries based on realistic
assumptions about their capabilities?
Some of the recent evasion attacks [28, 36] investigate
the transferability property of their solutions. Proven
transferability increases the strength of an attack as it
allows adversaries with limited knowledge or access to
the victim system to craft effective instances. Further-
more, transferability hinders defense strategies as it ren-
ders secrecy ineffective. However, existing work gener-
ally investigates transferability under single dimensions
(e.g.
limiting the adversarial knowledge about the vic-
tim algorithm). This weak notion of transferability lim-
its the understanding of actual attack capabilities on real
systems and fails to shed light on potential avenues for
defenses. This paper aims to provide a means to de-
ﬁne and evaluate a more general transferability, across a
wide range of adversary models. The generalized view of
threat models highlights limitations of existing training-
time attacks. Existing attacks [51, 29, 20] often assume
full control over the training process of victim classi-
ﬁers and have similar shortcomings to white-box attacks.
Those that do not assume full control generally omit im-
portant adversarial considerations.Targeted poisoning at-
tacks [34, 32, 11] require control of the labeling process.
However, an attacker is often unable to determine the la-
bels assigned to the poison samples in the training set
—consider a case where a malware creator may provide
a poison sample for the training set of an ML-based mal-
ware detector, but its malicious/benign label will be as-
signed by the engineers who train the detector. These
attacks risk being detected by existing defenses as they
might craft samples that stand out from the rest of the
training set. Moreover, they also risk causing collateral
damage to the classiﬁer; for example, in Figure 1c the at-
tack can trigger the misclassiﬁcation of additional sam-
ples from the target’s true class if the boundary is not
molded to include only the target. Such collateral dam-
age reduces the trust in the classiﬁer’s predictions, and
thus the potential impact of the attack. Therefore, we aim
to observe whether an attack could address these limita-
tions and discover how realistic is the targeted poisoning
threat?
Machine learning background.
For our purpose, a
classiﬁer (or hypothesis) is a function h ࢼ X → Y that
maps instances to labels to perform classiﬁcation. An
instance x ∈ X is an entity (e.g., a binary program ) that
must receive a label y ∈ Y ={y0,y1, ...,ym} (e.g., reﬂect-
instance as a vector x =(x1, . . . ,xn), where the features
binary). A function D(x,x′) represents the distance in
ing whether the binary is malicious ). We represent an
reﬂect attributes of the artifact (e.g. APIs invoked by the
the feature space between two instances x,x′ ∈ X. The
function h can be viewed as a separator between the mali-
cious and benign classes in the feature space X; the plane
of separation between classes is called decision bound-
ary. The training set S ⊂ X includes instances that have
known labels YS ⊂ Y . The labels for instances in S are
assigned using an oracle — for a malware classiﬁer, an
oracle could be an antivirus service such as VirusTotal,
whereas for an image classiﬁer it might be a human anno-
tator. The testing set T ⊂ X includes instances for which
the labels are unknown to the learning algorithm.
Threat model. We focus on targeted poisoning attacks
against machine learning classiﬁers. In this setting, we
refer to the victim classiﬁer as Alice, the owner of the
target instance as Bob, and the attacker as Mallory. Bob
and Mallory could also represent the same entity. Bob
possesses an instance t ∈ T with label yt, called the tar-
get, which will get classiﬁed by Alice. For example, Bob
develops a benign application, and he ensures it is not
ﬂagged by an oracle antivirus such as VirusTotal. Bob’s
expectation is that Alice would not ﬂag the instance ei-
ther. Indeed, the target would be correctly classiﬁed by
Alice after learning a hypothesis using a pristine training
set S∗ (i.e. h∗ = A(S∗),h∗(t) = yt). Mallory has partial
USENIX Association
27th USENIX Security Symposium    1301
knowledge of Alice’s classiﬁer and read-only access to
the target’s feature representation, but they do not con-