(cid:3)(cid:9)(cid:15)(cid:16)(cid:14)(cid:9)(cid:6)(cid:19)(cid:17)(cid:12)(cid:11)(cid:1)
(cid:3)(cid:9)(cid:15)(cid:16)(cid:14)(cid:9)(cid:6)(cid:19)(cid:17)(cid:12)(cid:11)(cid:1)
(cid:1)(cid:13)(cid:8)(cid:5)(cid:15)(cid:7)(cid:1)(cid:4)(cid:4)(cid:1)
(cid:1)(cid:13)(cid:8)(cid:5)(cid:15)(cid:7)(cid:1)(cid:4)(cid:4)(cid:1)
(cid:2)(cid:10)(cid:7)(cid:5)(cid:11)(cid:1)(cid:19)(cid:13)(cid:1)
(cid:2)(cid:10)(cid:7)(cid:5)(cid:11)(cid:1)(cid:19)(cid:13)
(cid:13)(cid:8)(cid:5)(cid:15)(cid:7)(cid:1)
(cid:13)(cid:8)(cid:5)(cid:15)(cid:7)(cid:1)
Figure 4: Multi-threaded Melbourne shufﬂe with constant
latency per access
Next array 
1,··· ,d(cid:4)
N) along with
mute the data which is uploaded by the user during the ini-
√
tialization phase. The enclave permutes N encrypted real
blocks (d(cid:4)
N dummy blocks and adds
them to the active array (as shown in lines 4-9 in Al-
gorithm 1). In each round, the enclave executes the Mel-
bourne shufﬂe algorithm with the active array as input
and the next array as output. It makes a two pass call to
the mel_shuffle function (lines 10 and 11). Internally,
the function performs the three phases of dist_phase1,
dist_phase2 and clean_up_phase (lines 2, 3, 4 in Algo-
√
N steps, where each step
rithm 2). Each phase performs
N blocks of the input array, re-arranges and re-
fetches
encrypts them and writes to the output array.
√
N)
threads and thus parallelize the execution of each phase (as
√
shown in Figure 4). Carefully selecting the hidden constants
in O(
N) allows us to securely distribute the work with-
out compromising on the security of the original algorithm
(see Lemma 5.1 in Section 5). Each thread re-encrypts and
re-arranges only a single block in every step of the phase
and writes them back in a deterministic manner. The oper-
ations on each block are independent of other blocks and
In PRO-ORAM, we distribute this computation over O(
√
Algorithm 3: Pseudocode for Read Algorithm
Input: di: block identiﬁer,
active_array: encrypted data blocks,
request: current request number
Output: d’: encrypted block
1 Lookup in the private stash;
2 if di in stash then
3
4
// access dummy value
addr ← π(N + request);
d’ ← active_array(addr) ;
// select value from stash
d’ ← stash(di);
addr ← π(di) ;
d’ ← active_array(addr) ;
Write d’ to the stash;
5
6 else
7
8
9
10 end
11 return d(cid:4);
have a constant depth. The threads use the private mem-
ory within the enclave as a stash to obliviously shufﬂe the
blocks. However, each thread reads and writes to its corre-
sponding memory location during the shufﬂing step. We ex-
ploit this property and parallelize the computation on each of
these blocks. In PRO-ORAM, we implement this approach using
√
multi-threading with SGX enclaves. The shuffle enclave
starts O(
N) threads in parallel to compute the re-encryption
and rearrangement of data blocks. This results in a constant
computation time per step. Thus, with parallelization imposed
√
in each step, the total computation time for shufﬂing N data
N). Hence, the amortized computation latency
blocks is O(
√
N requests is reduced to O(1). PRO-ORAM
per request over
N) threads.
distributes the work in each shufﬂe step over O(
After the shufﬂe is completed, the next_array is copied to
the active_array. The shuffle enclave sends the new keys
(Knew) and permutation value (π) to the access enclave using
a secure channel established initially. The latter enclave uses
these keys to access the correct requested blocks from the
active_array in the next round.
√
4.3 Access Enclave
√
Unlike the shuffle enclave, the access enclave begins exe-
N read requests
cution from round 1. Each round accepts
from the users. Before the start of each round, the access
enclave gets the permutation π and encryption key Knew from
the shuffle enclave. The active array corresponds to data
blocks shufﬂed and encrypted using the keys π and Knew. For
each request, the access enclave takes as input the block
identiﬁer di and the requesting user id uid. The enclave ﬁrst
conﬁrms that the requesting uid is a valid registered user and
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 203Algorithm 4: Pseudocode for each round of access
enclave
Input: di: request ﬁle identiﬁer ,
Pub_map: User id and public key mapping table ,
uid: requesting user id,
Knew: encryption key
π: permutation key
active_array: permuted array
√
Output: response_msg
N do
Pbuid ← Pub_map (uid);
d’ ← Read (di, active_array, request);
k’ ← GenSE;
d” ← EncSE(DecSE(d’, Knew), k’);
key_msg = EncPKE(Pbuid, k(cid:4)(cid:4));
response_msg = (d(cid:4)(cid:4), key_msg);
1 for request from 1 to
2
3
4
5
6
7
8 end
has a public key corresponding to the user. On conﬁrmation,
the enclave invokes the read function in Algorithm 3.
The algorithm to read a block is same as the main logic of
square-root ORAM. Algorithm 3 provides the pseudocode for
reading a data block in PRO-ORAM. Note that, we do not store
the private stash on the untrusted cloud storage as proposed in
the original square-root ORAM approach. Instead, the stash is
maintained in the private memory within the access enclave.
The stash is indexed using a hash table and hence can be
looked up in a constant time. The read algorithm checks
if the requested data block is present in the private stash.
If present, the enclave accesses a dummy block from the
untrusted storage. Else, it gets the address for the requested
block di using permutation π and fetches the real block from
the untrusted storage. The output of the read algorithm is an
encrypted block d(cid:4). The block is stored in the private stash.
After fetching the encrypted block d(cid:4), either from the pri-
vate stash or the active array, the access enclave decrypts
it using Knew. Algorithm 4 shows the pseudocode for this step.
It then selects a new key k(cid:4) and encrypts the block. The output
message includes this re-encrypted block and the encryption
of k(cid:4) under public key of the requesting user Pbuid. At the
end of each round i.e., after serving
N request, the access
enclave clears the private storage, permutation π and Knew.
Note that unlike the original square-root ORAM, there is no
N requests. The permuted next array from
shufﬂing after
the shuffle enclave replaces the active array.
Performance Analysis. In PRO-ORAM, the access enclave
sends only the requested block to the user. This results in
a communication overhead of O(1) with respect to the re-
quested block size. Further, the access enclave computes i.e.,
re-encrypts only a single block for each request. Thus, the
√
computation on the server for the access enclave is O(1).
The shuffle enclave computes a permuted array in O(
N)
√
√
√
√
N) blocks for each request. Note that
steps. It fetches O(
N)
the total computation performed at the server is still O(
√
for each request. However, in PRO-ORAM, we parallelize the
computation i.e, re-encryption on O(
N)
threads. This reduces the computation time required for each
step to only a single block. Thus, the overall computation
latency for the shuffle enclave is O(1) per request.
√
N) blocks in O(
4.4 Optimizations
We further propose optimizations to Melbourne shufﬂe algo-
rithm such that the performance can be improved by a constant
factor. Both these optimizations are possible by exploiting the
design structure of the algorithm.
Pipelining. We observe that in the existing algorithm (shown
in Algorithm 1) the three phases execute sequentially (see
Figure 4). Once the dist_phase1 function generates the
temp1 array, it waits until the remaining phases complete. On
the other hand, the dist_phase2 and the cleanup_phase
functions have to wait for the previous phase to complete
before starting their execution. To eliminate this waiting time,
we separate the execution of these phases into different en-
claves and execute them in a pipeline. Thus, instead of waiting
for the entire shufﬂe algorithm to complete, dist_phase1
enclave generates a new temp array in every round to be
used as input by the dist_phase2 enclave. Eventually, each
phase enclave outputs an array in every round to be used
by the next phase enclave, thereby pipelining the entire ex-
ecution. Note that this optimization is possible because the
input to the dist_phase1 does not depend on any other phase.
dist_phase1 enclave can continue to use the initial uploaded
data as input and generate different temp arrays based on a
new permutation value selected randomly in each round. This
allows us to continuously execute each of the phases in its
own enclave without becoming a bottleneck on any other
phase. Thus, the overall latency is reduced by a factor of 3.
This optimization increases the external storage requirement
by 2N to store the additional temp array.
√
Parallel Rounds using Multiple Enclaves. Another opti-
√
mization is to instantiate multiple (possibly O(
N)) enclaves
and execute each of the O(
N) rounds in parallel in these
√
enclaves. With this optimization, the latency for shufﬂing N
data blocks reduces from O(
N) to O(1). This observation
is also discussed by Ohrimenko et al. [33]. However, the main
√
drawback in implementing this optimization is the blow-up
√
N) enclaves
in the combined private storage. As each of O(
requires private memory of size O(
N), the combined pri-
vate memory is linear in the total data size O(N). Such a huge
requirement of private storage may not be feasible even on
very high-end servers. In our work, to use this optimization
without requiring linear private storage, we propose using
only a constant number of enclaves, thereby improving the
performance by a constant factor.
204          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX Association5 Security Analysis
The observable access patterns in PRO-ORAM include accesses
made both from access and shuffle enclave. We ﬁrst show
that the shuffle enclave executes an oblivious algorithm.
Lemma 5.1. Given N data blocks, Melbourne Shufﬂe is an
√
oblivious algorithm and generates a permuted array with very
high probability (1−negl(N)) in O(
√
N) steps, each exchang-
N) between a private memory of
ing a message size of O(
O(
√
N) and untrusted storage of size O(N).
This Lemma directly follows from Theorem 5.1 and 5.6
√
in [33]. In PRO-ORAM, the shuffle enclave executes the Mel-
bourne Shufﬂe algorithm using O(
N) memory within an
enclave. Thus, from Lemma 5.1, we get the corollary below,
√
Corollary 5.1. The shuffle enclave generates a permuted
array of O(N) data blocks in O(
N) steps and the access
patterns are oblivious to the server.
From Corollary 5.1, the access patterns of the shufﬂe en-
clave are oblivious and the output is indistinguishable from a
pseudo-random permutation (PRP) [33].
Further, the communication between access and shuffle
enclave happens over a secure channel. This preserves the
conﬁdentiality of the permutation and encryption keys that
shuffle enclave sends to the access enclave at the end
of each round. Thus, no information is leaked due to the
interaction between these enclaves in PRO-ORAM. Now, to
prove that PRO-ORAM guarantees obliviousness for read access
patterns, we ﬁrst show that a request to the access enclave is
indistinguishable from random for an adaptive adversary.
Aadt ,E
Let E = (Gen,Enc,Dec) be a IND-CPA secure encryp-
tion scheme where Gen generates a key which is used by
the Enc and Dec algorithms to perform encryption and de-
cryption respectively. Let λ be the security parameter used in
E . ExpPRO−ORAM
refers to the instantiation of the experiment
with PRO-ORAM, E algorithms and adaptive adversary Aadt.
This experiment captures our security deﬁnition for read-only
obliviousness. The experiment consists of:
• Aadt creates request r = (read,di) and sends it to a chal-
lenger C .
• The challenger selects b $←− {1,0}.
• If b = 1, then C outputs the address access patterns to
fetch di i.e., A(d1) ← access (di) and encrypted output
1 ← d(cid:4)
O(cid:4)
• If b = 0, then C outputs a random address access
$←− {0,1}λ
pattern i.e., A(d0) $←− {1,··· ,N +
N} and O(cid:4)
√
i
0
• Adversary Aadt has access to an oracle O PRO−ORAM that
issues q queries of type (read,d) both before and after
executing the challenge query r. The oracle outputs ad-
dress access patterns to fetch d i.e., A(d) ← access (d)
.
• Aadt outputs b
• The output of the experiment is 1 if b = b
The adversary Aadt wins if ExpPO
(cid:4) otherwise 0.
(cid:4) ∈ {1, 0}.
(cid:4)) =1 .
E (λ, b
Based on the experiment and its output, we deﬁne read-only
obliviousness as follows:
Deﬁnition 5.1. An algorithm satisﬁes read-only oblivious-
ness iff for all PPT adversaries A, there exists a negligible
function negl such that:
Pr[ExpPRO−ORAM
(λ,1) = 1]−Pr[ExpPRO−ORAM
Aadt ,E
Aadt ,E
(λ,0) = 1]≤ negl
(1)
Theorem 5.1. If shuffle enclave executes an oblivious al-
gorithm and E is a CPA-secure symmetric encryption then
PRO-ORAM guarantees read-only obliviousness as in Def. 5.1.
Proof. We present the proof in Appendix A
6
Implementation and Evaluation
Implementation. We have implemented our proposed
PRO-ORAM algorithm in C/C++ using Intel SGX Linux SDK
v1.8 [9]. For implementing symmetric and public key encryp-
tion schemes, we use AES with 128-bit keys and Elgamal
cryptosystem with 2048 bit key size respectively from the
OpenSSL library [39]. We use SHA256 as our hash function.
We implement the read logic of square-root ORAM and the
parallelized shufﬂe algorithm as explained in Section 4.2. We
use multi-threading with SGX enclaves to implement our par-
allel execution approach for each step. The prototype contains
total 4184 lines of code measured using CLOC tool [3].
Experimental Setup & Methodology. To evaluate
PRO-ORAM, we use SGX enclaves using the Intel SGX
simulator and perform experiments on a server running
Ubuntu 16.04 with Intel(R) Xeon(R) CPU E5-2640 v4
√
processors running at 2.4 GHz (40 cores) and 128 GB
of RAM. As PRO-ORAM’s design uses
N threads, our
experimental setup of 40 cores can execute a total of 80
threads using Intel’s support for Hyper-Threading, thereby
handling requests with block-size of 256 KB for around 1
GB of data. Operating with data of this size is not possible
with SGX in hardware mode available on laptops due to
their limited processing capacity (8 cores). However, for real
cloud deployments, the cost of a deca-core server is about
a thousand dollars [10]; so, the one-time cost of buying 40
cores worth of computation per GB seems reasonable. To
measure our results for gigabyte sized data, we chose to run
40 cores (80 threads) each with an SGX simulator.