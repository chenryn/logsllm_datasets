also outperforms our re-implementation of a prior
work in terms of precision-recall curve.
3) We demonstrate with case studies that ACOBE can be
applied in practice for realistic cyber threats, including
botnets and ransomware attacks.
II. RELATED WORK
Most anomaly detection models are zero-positive ma-
chine learning models that are trained by only normal
(i.e., negative) data. These models are then used in testing
whether an observation is normal or abnormal, assuming
unforeseen anomalies do not follow the learned patterns.
Kanaza et al. [3] integrated supports vector data description
and clustering algorithms, and Liu et al. [4] integrated K-
prototype clustering and k-NN classiﬁcation algorithms to
detect anomalous data points, assuming anomalies are rare or
accidental events. When prior domain knowledge is available
for linking causal or dependency relations among sub-
jects, objects and operations, graph-based anomaly detection
methods (such as Elicit [9], Log2Vec [16], Oprea et al. [17])
could be powerful. When little prior domain knowledge
is available, Principal Component Analysis (PCA) based
anomaly detection methods (for example, Hu et al. [8]
proposed an anomaly detection model for heterogeneous
logs using singular value decomposition) could be power-
ful. Contrary to zero-positive anomaly detection are semi-
supervised or online learning anomaly detection, in which
some anomalies will be available over time for training [18].
The autoencoder framework is a PCA approach that is
widely used in anomaly detection. A typical autoencoder-
based anomaly detection method learns how to reconstruct
normal data. It then detects anomalies by checking whether
the reconstruction error of an observation has exceeded
a threshold. To detect anomalies, Zong et al. proposed
deep autoencoding Gaussian mixture models [19] and Chiba
et al. proposed autoencoders with back propagation [20].
Sakurada and Yairi proposed autoencoders with nonlinear
dimensionality reduction [21]. Lu et al. proposed an autoen-
coder constrained by embedding manifold learning, MC-
AEN [22]. Nguyen et al. proposed a variational autoencoder
with gradient-based anomaly explanation, GEE [23]. Wang
et al. proposed a self-adversarial variational autoencoder
with Gaussian anomaly prior assumption, adVAE [24]. Alam
et al. proposed an ensemble of autoencoders accompanied
by K-mean clustering algorithm, AutoPerf [25]. Mirsky
et al. proposed an ensemble of lightweight autoencoders,
Kitsune [5]. Liu et al. proposed an ensemble of autoencoders
for multi-sourced heterogeneous logs [6], [7]. Chalapathy et
al. [26] and Zhou et al. [27] proposed robust autoencoders.
For other anomaly detection methods, detail surveys can be
found in [10]–[12], [28]–[31].
Each of the above autoencoder work focuses on either
the optimization of a particular learning algorithm without
cybersecurity context [19], [21], [22], [24]–[27], or the
optimization of a learning framework with very speciﬁc
cybersecurity context (i.e., network intrusion detection sys-
tem) [5], [20], [23]. However, though they provide fruitful
insights in their particular domains,
is hard to apply
their model or framework to the other anomaly detection
problems (including ours—detection of abnormal users in a
large-scale organization) due to the requirement difference
for the input data. For example, with statistical network-
trafﬁc features for individual transactions, it is difﬁcult to
it
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:27 UTC from IEEE Xplore.  Restrictions apply. 
251
discover a disgruntled insider who exﬁltrates proprietary
secrets piece-by-piece in a long run.
Regardless of model and framework design, any input
data that does not meet the requirement may fail the de-
tection methodology, and this is commonly known as the
data quality challenge [13]. While Sundararajan et al. [13]
discussed six data-quality requirements (i.e., completeness,
measurement accuracy, attribute accuracy, plausibility and
availability, origination, and logical consistency) for cyber-
security applications, important factors such as misconduct
timeliness and institutional environment are not considered.
Hence, in this paper, we utilize two additional signals—long-
term signals and group-correlation signals—to address the
anomaly detection problem challenges and limitations of the
existing work (e.g., high false-positive rate).
III. MOTIVATION
Anomaly detection upon user behaviors enables security
analysts to ﬁnd suspicious activities caused by cyber threats
including insider threats (Section V) and cyber attacks (Sec-
tion VI). Typical anomaly detection methods often suffer
from the overwhelming number of false positives due to
the sensitivity to normal behavioral deviation. Moreover,
such methods often only output anomaly labels (i.e., either
normal or abnormal). However, if a model provides only
anomaly labels, security analysts will be overwhelmed by
heavy workload of investigation. Hence, in practice, it is
more preferable to have an ordered list of users that need to
be investigated [32]. We thus deﬁne an anomaly detection
problem as follows: given a set of per-user activities,
provide an investigation list of usernames that need to
be orderly investigated. On the top of such a list are the
most abnormal users.
Autoencoders have been widely used in solving such an
anomaly detection problem, as it is capable of learning what
are normal in order to ﬁnd what are abnormal. Among
the aforementioned anomaly detection methods, we ﬁnd
Liu et al. [6] and Hu et al. [8]’s works are representative.
They are similar anomaly detection methods that reconstruct
single-day individual-user behaviors. However, single-day
user-behavior reconstruction is not an ideal solution for
identifying cyber threats. We argue that it is important to
examine long-term signals and group-correlation signals,
because of the following reasons.
First, certain cyber compromise scenarios do not cause
immediate behavioral deviation, but progressively cause
small long-lasting behavioral deviation in different behav-
ioral aspects across multiple days. Take Zeus botnet malware
as an example, once triggered, it modiﬁes registry values
(deviation in system conﬁguration). After a few days, it
communicates with the C&C server and acts maliciously
(deviation in network trafﬁc). Since these two behavioral
deviations occur on different days, only models that ex-
amine long-term behaviors can identify such an anomaly.
In contrast, single-day user-behavior reconstruction may fail
to identify the threats, and it may also wrongly give high
anomaly scores to busy days (e.g., working Mondays and
make-up days).
The granularity of feature measurements is also an impor-
tant factor in building proﬁles to accurately capture normal
user behavior. Concretely, users often tend to have more
human-initiated activities (e.g., sending emails, browsing
web pages, writing documents) during working hours, but
more computer-initiated activities (e.g., system updates, sys-
tem backups, network retries due to no password input)
during off hours. Therefore, our approach also captures
behaviors over multiple time-frames (e.g., working hours
and off hours) within each day of the measurement window.
Second, there often exists certain behavioral correlation
between an individual user and its group due to environ-
mental change or locale change. Take environmental change
as an example, when there is a new service or service outage,
one can expect correlated unrecognized trafﬁcs or correlated
retry trafﬁcs, respectively. Take locale change as another
example, one can expect more human-initiated activities
during working hours on working days, as opposed to during
off hours or holidays. Based on this observation, we make
the following hypothesis: the greater behavioral correlation a
user has with the group, the less likely the user is compro-
mised. A model without incorporating the group behavior
may not only be ineffective in identifying anomalies, but
also wrongly give high anomaly scores to individual users
in events of environmental or locale changes. Hence, we
propose to incorporate both individual-user behaviors and
group behaviors to better capture normal behavior and avoid
obvious false positives that may result from unusual yet
common activities of users.
IV. OUR METHODOLOGY
To address the challenges discussed in the previous sub-
section, we present ACOBE, anAnomaly detection method
based on COmpound BEhavior, where a compound behav-
ior encloses individual-user behaviors and group behaviors
across multiple time-frames and a time window in days.
Having compound behavior, we then apply anomaly detec-
tion implemented with deep fully-connected autoencoders.
Figure 1 illustrates the workﬂow: (1) ACOBE ﬁrst derives
compound behavioral deviation matrices from organiza-
tional audit logs, (2) for each user, ACOBE then calculates
anomaly scores in different behavioral aspects using an
ensemble of autoencoders, and (3) having anomaly scores,
ACOBE ﬁnally produces an ordered list of users that need
further investigation.
A. Compound Behavioral Deviation Matrix
A compound behavioral deviation matrix encloses devia-
tion of individual user behavior and group behavior across
multiple time-frames and multi-day time window. Figure 2
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:27 UTC from IEEE Xplore.  Restrictions apply. 
252
Logs
User 
Behavioral
Deviation
Group 
Behavioral
Deviation
Matrix
Matrix
Model
Matrix
Matrix
Model
Matrix
Matrix
Model
Score
Score
Score
score
score
Score
score
score
Score
score
score
Score
Anomaly 
Critic
Derive compound behavioral 
deviation matrices from logs
For each user, calculate anomaly 
scores in each behavioral aspects
Analyze scores and report 
a list of anomalous users
List of 
Anomalous 
Users
Figure 1. ACOBE Workﬂow
Timeframe (cid:2286)
:
:
Feature 1
Feature (cid:2280)
Feature (cid:2280):
Feature 1
:
Timeframe 1
User Behavior
during Working Hours
User Behavior
during Off Hours
Group Behavior
during Working Hours
Group Behavior
during Off Hours
……
……
Figure 2. Compound Behavioral Deviation Matrix
illustrates an example of a compound behavioral deviation
matrix with F features, D days, and T = 2 time-frames
(i.e., working hours 6am-6pm and off hours 6pm-6am). Each
feature in user behavior represents a normalized character-
istic of an aggregated behavior, including but not limited
to, the numbers of successful logons, ﬁle accesses, failure
HTTP queries (during the speciﬁc time-frame on speciﬁc day
indicated by columns). Since feature selection is domain-
speciﬁc, we leave the details of features in the evaluation
(Section V) and case-study sections (Section VI). Features in
group behavior are derived by averaging the corresponding
features of all users in the group. Note that, how these four
components are stacked together is not important (alternative
stackings are applicable), because matrices will be ﬂattened
before going through the anomaly detection models.
We derive our deviation measurement σf,t,d for feature f
in time-frame t on day d with the below equations. mf,t,d
denotes the numeric measurements (of feature f in time-
frame t on day d). (cid:3)hf,t,d denotes the vector of history
numeric measurements (in ω−1 days before day d, where ω
(cid:3)hf,t,d) denotes the standard
is the window size in days). std(
deviation of history measurements (std is set to  if it is
less than  to avoid divide-by-zero exception). δf,t,d denotes
the variance of numeric measurement and σf,t,d denotes the
ﬁnal behavioral deviation which is bounded by Δ. We bound
σf,t,d by a large Δ, as it is equivalently anomalous when
|δf,t,d| ≥Δ. For example, variances larger than Δ = 3
are equivalently very abnormal, assuming the numeric mea-
surements follow Gaussian distribution. Note that, in events
when users slowly shift their normal behavioral patterns over
time, their compound behavioral deviation matrices will not
show increasing deviation over time, as the history (cid:3)hf,t,d
(from which deviations are derived) slides through time and
will always cover the recent shift.
mf,t,d = numeric measurements of feature f
std(
(cid:2)
in timeframe t on day d
(cid:3)hf,t,d =[mf,t,i|i : d − ω + 1 ≤ i  Δ
if δf,t,d < −Δ
otherwise
⎧⎪⎨
⎪⎩Δ,
−Δ,
δf,t,d,
σf,t,d =
δf,t,d =
std (
(cid:3)hf,t,d) < 
(cid:3)hf,t,d), otherwise
Weights are applied to features, as different behaviors
have different importance in capturing user behavior; for
example, frequent and chaotic ﬁle-read activities are often
less critical than rarer ﬁle-write activities. Since the anomaly
scores are essentially the reconstruction errors of features,
applying weights to features can scale-down unimportant
features and thus the partial errors introduced by unimportant
features. Consequently, it makes ACOBE to focus only on
reconstructing important features while being more resilient
to noise introduced by unimportant features. To automat-
ically reﬂect
the relative importance without conducting
empirical studies upon individual users, Hu et al. [8] ap-
plied weights to features based on Term Frequency-Inverse
Document Frequency (TF-IDF) measurements, which was
originally designed for measuring the amount of information
a particular text subject provides based on text frequency.