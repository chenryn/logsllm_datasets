|                          |              | `SequenceFile` 、        |
|                          |              | `DataStream` 、          |
|                          |              | `CompressedStream` 。 1. |
|                          |              | `DataStream`             |
|                          |              | 不会压缩文               |
|                          |              | 件，不需要设置hdfs.codeC |
|                          |              | 2. `CompressedStream`    |
|                          |              | 必须设置hdfs.codeC参数   |
+--------------------------+--------------+--------------------------+
| hdfs.maxOpenFiles        | 5000         | 允许打开的最大           |
|                          |              | 文件数，如果超过这个数量 |
|                          |              | ，最先打开的文件会被关闭 |
+--------------------------+--------------+--------------------------+
| hdfs.minBlockReplicas    | \--          | 指定                     |
|                          |              | 每个HDFS块的最小副本数。 |
|                          |              | 如果未指定，则使用       |
|                          |              | classpath 中 Hadoop      |
|                          |              | 的默认配置。             |
+--------------------------+--------------+--------------------------+
| hdfs.writeFormat         | Writable     | 文件写入格式。可选值：   |
|                          |              | `Text` 、 `Writable`     |
|                          |              | 。在使用 Flume           |
|                          |              | 创建数据文件之前设置为   |
|                          |              | `Text`，否则 Apache      |
|                          |              | Impala（孵化）或 Apache  |
|                          |              | Hive 无法读取这些文件。  |
+--------------------------+--------------+--------------------------+
| hdfs.threadsPoolSize     | 10           | 每个HDFS                 |
|                          |              | Sink实例操作HDFS         |
|                          |              | IO时开                   |
|                          |              | 启的线程数（open、write  |
|                          |              | 等）                     |
+--------------------------+--------------+--------------------------+
| hdfs.rollTimerPoolSize   | 1            | 每个HDFS                 |
|                          |              | Sink实例                 |
|                          |              | 调度定时文件滚动的线程数 |
+--------------------------+--------------+--------------------------+
| hdfs.kerberosPrincipal   | \--          | 用于安全访问 HDFS 的     |
|                          |              | Kerberos 用户主体        |
+--------------------------+--------------+--------------------------+
| hdfs.kerberosKeytab      | \--          | 用于安全访问 HDFS 的     |
| hdfs.proxyUser           |              | Kerberos keytab 文件     |
|                          |              | 代理名                   |
+--------------------------+--------------+--------------------------+
| hdfs.round               | false        | 是否应将时间戳向下舍     |
|                          |              | 入（如果为true，则影响除 |
|                          |              | `%t`                     |
|                          |              | 之外                     |
|                          |              | 的所有基于时间的转义符） |
+--------------------------+--------------+--------------------------+
| hdfs.roundValue          | 1            | 向下舍入（               |
|                          |              | 小于当前时间）的这个值的 |
|                          |              | 最高倍（单位取决于下面的 |
|                          |              | *hdfs.roundUnit* ）      |
|                          |              | 例                       |
|                          |              | 子：假设当前时间戳是18:3 |
|                          |              | 2:01，\*hdfs.roundUnit\* |
|                          |              | = `minute`               |
|                          |              | 如果roundValue=          |
|                          |              | 5，则时间戳会取为：18:30 |
|                          |              | 如果roundValue=          |
|                          |              | 7，则时间戳会取为：18:28 |
|                          |              | 如果roundValue=1         |
|                          |              | 0，则时间戳会取为：18:30 |
+--------------------------+--------------+--------------------------+
| hdfs.roundUnit           | second       | 向下舍入的单位，可选值： |
|                          |              | `second` 、 `minute` 、  |
|                          |              | `hour`                   |
+--------------------------+--------------+--------------------------+
| hdfs.timeZone            | Local Time   | 解析                     |
|                          |              | 存储目录路径时候所使用的 |
|                          |              | 时区名，例如：America/Lo |
|                          |              | s_Angeles、Asia/Shanghai |
+--------------------------+--------------+--------------------------+
| hdfs.useLocalTimeStamp   | false        | 使                       |
|                          |              | 用日期时间转义符时是否使 |
|                          |              | 用本地时间戳（而不是使用 |
|                          |              | Event header             |
|                          |              | 中自带的时间戳）         |
+--------------------------+--------------+--------------------------+
| hdfs.closeTries          | 0            | 开                       |
|                          |              | 始尝试关闭文件时最大的重 |
|                          |              | 命名文件的尝试次数（因为 |
|                          |              | 打开的文件通常都有个.tmp |
|                          |              | 的后缀，写入结束关闭文件 |
|                          |              | 时要重命名把后缀去掉）。 |
|                          |              |                          |
|                          |              | 如果设置为1，Sink        |
|                          |              | 在重命名失败（可能是因为 |
|                          |              | NameNode 或 DataNode     |
|                          |              | 发生错误）后             |
|                          |              | 不会重试，这样就导致了这 |
|                          |              | 个文件会一直保持为打开状 |
|                          |              | 态，并且带着.tmp的后缀； |
|                          |              |                          |
|                          |              | 如果                     |
|                          |              | 设置为0，Sink会一直尝试  |
|                          |              | 重命名文件直到成功为止； |
|                          |              |                          |
|                          |              | 关闭文件操作失           |
|                          |              | 败时这个文件可能仍然是打 |
|                          |              | 开状态，这种情况数据还是 |
|                          |              | 完整的不会丢失，只有在F  |
|                          |              | lume重启后文件才会关闭。 |
+--------------------------+--------------+--------------------------+
| hdfs.retryInterval       | 180          | 连续尝试关               |
|                          |              | 闭文件的时间间隔（秒）。 |
|                          |              | 每次关闭操作都会调用多次 |
|                          |              | RPC 往返于 Namenode      |
|                          |              | ，                       |
|                          |              | 因此将此设置得太低会导致 |
|                          |              | Namenode                 |
|                          |              | 上产生大量负载。         |
|                          |              | 如果设置为0或            |
|                          |              | 更小，则如果第一次尝试失 |
|                          |              | 败，将不会再尝试关闭文件 |
|                          |              | ，并且可能导致文件保持打 |
|                          |              | 开状态或扩展名为".tmp"。 |
+--------------------------+--------------+--------------------------+
| serializer serializer.\* | TEXT         | Event                    |
|                          |              | 转为文件使用的           |
|                          |              | 序列化器。其他可选值有： |
|                          |              | `avro_event` 或其他      |
|                          |              | `EventSeri               |
|                          |              | alizer.Builderinterface` |
|                          |              | 接                       |
|                          |              | 口的实现类的全限定类名。 |
|                          |              | 根据上面 *serializer*    |
|                          |              | 配置的类型来根           |
|                          |              | 据需要添加序列化器的参数 |
+--------------------------+--------------+--------------------------+
废弃的一些参数：
  属性名             默认值   解释
  ------------------ -------- ---------------------------------------------------------------------------------------------------------------------
  hdfs.callTimeout   10000    允许HDFS操作文件的时间，比如：open、write、flush、close。如果HDFS操作超时次数增加，应该适当调高这个这个值。（毫秒）
配置范例：
``` properties
a1.channels = c1
a1.sinks = k1
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
```
上面的例子中时间戳会向前一个整10分钟取整。比如，一个 Event 的 header
中带的时间戳是11:54:34 AM, June 12, 2012，它会保存的 HDFS
路径就是/flume/events/2012-06-12/1150/00。
#### Hive Sink
此Sink将包含分隔文本或JSON数据的 Event 直接流式传输到 Hive表或分区上。
Event 使用 Hive事务进行写入， 一旦将一组 Event
提交给Hive，它们就会立即显示给Hive查询。
即将写入的目标分区既可以预先自己创建，也可以选择让 Flume
创建它们，如果没有的话。 写入的 Event 数据中的字段将映射到
Hive表中的相应列。
  属性                         默认值       解释
  ---------------------------- ------------ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **channel**                  \--          与 Sink 连接的 channel
  **type**                     \--          组件类型，这个是： `hive`
  **hive.metastore**           \--          Hive metastore URI (eg thrift://a.b.com:9083 )
  **hive.database**            \--          Hive 数据库名
  **hive.table**               \--          Hive表名
  hive.partition               \--          逗号分隔的要写入的分区信息。 比如hive表的分区是（continent: string, country :string, time : string）， 那么"Asia,India,2014-02-26-01-21"就表示数据会写入到continent=Asia,country=India,time=2014-02-26-01-21这个分区。
  hive.txnsPerBatchAsk         100          Hive从Flume等客户端接收数据流会使用多次事务来操作，而不是只开启一个事务。这个参数指定处理每次请求所开启的事务数量。来自同一个批次中所有事务中的数据最终都在一个文件中。 Flume会向每个事务中写入 *batchSize* 个 Event，这个参数和 *batchSize* 一起控制着每个文件的大小，请注意，Hive最终会将这些文件压缩成一个更大的文件。
  heartBeatInterval            240          发送到 Hive 的连续心跳检测间隔（秒），以防止未使用的事务过期。设置为0表示禁用心跳。
  autoCreatePartitions         true         Flume 会自动创建必要的 Hive分区以进行流式传输
  batchSize                    15000        写入一个 Hive事务中最大的 Event 数量
  maxOpenConnections           500          允许打开的最大连接数。如果超过此数量，则关闭最近最少使用的连接。
  callTimeout **serializer**   10000        Hive、HDFS I/O操作的超时时间（毫秒），比如：开启事务、写数据、提交事务、取消事务。 序列化器负责解析 Event 中的字段并把它们映射到 Hive表中的列，选择哪种序列化器取决于 Event 中的数据格式，支持的序列化器有：`DELIMITED` 和 `JSON`
  round                        false        是否启用时间戳舍入机制
  roundUnit                    minute       舍入值的单位，可选值：`second` 、 `minute` 、 `hour`
  roundValue                   1            舍入到小于当前时间的最高倍数（使用 *roundUnit* 配置的单位） 例子1：roundUnit=second，roundValue=10，则14:31:18这个时间戳会被舍入到14:31:10; 例子2：roundUnit=second，roundValue=30，则14:31:18这个时间戳会被舍入到14:31:00，14:31:42这个时间戳会被舍入到14:31:30;
  timeZone                     Local Time   应用于解析分区中转义序列的时区名称，比如：America/Los_Angeles、Asia/Shanghai、Asia/Tokyo等
  useLocalTimeStamp            false        替换转义序列时是否使用本地时间戳（否则使用Event header中的timestamp ）
下面介绍Hive Sink的两个序列化器：
**JSON** ：处理UTF8编码的 Json 格式（严格语法）Event，不需要配置。
JSON中的对象名称直接映射到Hive表中具有相同名称的列。 内部使用
`org.apache.hive.hcatalog.data.JsonSerDe` ，但独立于 Hive表的 `Serde` 。
此序列化程序需要安装 HCatalog。
**DELIMITED**: 处理简单的分隔文本 Event。 内部使用
LazySimpleSerde，但独立于 Hive表的 Serde。
  属性                        默认值   解释
  --------------------------- -------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  serializer.delimiter        ,        （类型：字符串）传入数据中的字段分隔符。 要使用特殊字符，请用双引号括起来，例如"\\t"
  **serializer.fieldnames**   \--      从输入字段到Hive表中的列的映射。 指定为Hive表列名称的逗号分隔列表（无空格），按顺序标识输入字段。 要跳过字段，请保留未指定的列名称。 例如， \'time,,ip,message\'表示输入映射到hive表中的 time，ip 和 message 列的第1，第3和第4个字段。
  serializer.serdeSeparator   Ctrl-A   （类型：字符）自定义底层序列化器的分隔符。如果 *serializer.fieldnames* 中的字段与 Hive表列的顺序相同，则 *serializer.delimiter* 与 *serializer.serdeSeparator* 相同， 并且 *serializer.fieldnames* 中的字段数小于或等于表的字段数量，可以提高效率，因为传入 Event 正文中的字段不需要重新排序以匹配 Hive表列的顺序。 对于\'\\t\'这样的特殊字符使用单引号，要确保输入字段不包含此字符。 注意：如果 *serializer.delimiter* 是单个字符，最好将本参数也设置为相同的字符。
以下是支持的转义符：
  转义符    解释
  --------- -------------------------------------------------------------------------------------------------------------------------------------
  %{host}   Event header中 key 为 host 的值。这个 host 可以是任意的 key，只要 header 中有就能读取，比如%{aabc}将读取 header 中 key 为 aabc 的值
  %t        毫秒值的时间戳（同 System.currentTimeMillis() 方法）
  %a        星期的缩写（Mon、Tue等）
  %A        星期的全拼（Monday、 Tuesday等）
  %b        月份的缩写（Jan、 Feb等）
  %B        月份的全拼（January、February等）
  %c        日期和时间（Thu Feb 14 23:05:25 2019）
  %d        月份中的天（00到31）
  %D        日期，与%m/%d/%y相同 ，例如：02/09/19
  %H        小时（00到23）
  %I        小时（01到12）
  %j        年中的天数（001到366）
  %k        小时（0到23），注意跟 `%H` 的区别
  %m        月份（01到12）
  %M        分钟（00到59）
  %p        am 或者 pm
  %s        unix时间戳，是秒值。比如：2019/4/1 15:12:47 的unix时间戳是：1554102767
  %S        秒（00到59）
  %y        一年中的最后两位数（00到99），比如1998年的%y就是98
  %Y        年（2010这种格式）
  %z        数字时区（比如：-0400）
::: note
::: title
Note
:::
对于所有与时间相关的转义字符，Event header
中必须存在带有"timestamp"键的属性（除非 *useLocalTimeStamp* 设置为
`true` ）。快速添加此时间戳的一种方法是使用
[时间戳添加拦截器](#时间戳添加拦截器) （ TimestampInterceptor）。
:::
假设Hive表如下：
``` none
create table weblogs ( id int , msg string )
    partitioned by (continent string, country string, time string)
    clustered by (id) into 5 buckets
    stored as orc;
```
配置范例：
``` properties
a1.channels = c1
a1.channels.c1.type = memory
a1.sinks = k1
a1.sinks.k1.type = hive
a1.sinks.k1.channel = c1
a1.sinks.k1.hive.metastore = thrift://127.0.0.1:9083
a1.sinks.k1.hive.database = logsdb