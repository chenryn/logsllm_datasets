analysis is the VersionGraph for the user-level versions, which
the forensic analyst can use to quickly assess the different behaviors
exhibited by the domain version they are evaluating.
Page: asean.orgParsedRequestHTMLParserRequestDOM InsertionCompiledScript:adv.jsDomain:jupyter.eï¬‚inwood.topDomain:jsdelivr.netResponseEndpoint:adv.jsResponseEndpoint: ï¬ngerprint.jsCompiledScript:ï¬ngerprint.jsPage: asean.orgParsedRequestHTMLParserRequestDOM InsertionCompiledDOM InsertionScript:adv.jsDomain:jupyter.eï¬‚inwood.topDomain:jsdelivr.netResponseEndpoint:adv.jsResponseEndpoint: ï¬ngerprint.jsCompiledScript:ï¬ngerprint.jsDOM InsertionScript:adFeedback.jsAttatchediframe:OverlayUser NavigationPage: malicious.comAttachediframe:account.google.comPage: asean.orgParsedRequestHTMLParserRequestDOM InsertionCompiledDOM InsertionScript:adv.jsDomain:jupyter.eï¬‚inwood.topDomain:jsdelivr.netResponseEndpoint:adv.jsResponseEndpoint: ï¬ngerprint.jsCompiledScript:ï¬ngerprint.jsDOM InsertionScript:adFeedback.jsAttatchediframe:Overlay(a)(b)(c)Session 3B: Malware CCS '20, November 9â€“13, 2020, Virtual Event, USA792Algorithm 1: Differential Analysis
Input: UserVersions The set of initial user-level versions.
Result: VersionGraph: The resulting Versioning Graph.
VersionGraph â† list();
while |UserVersions| > 0 do
// Initialize current based on pageCount.
current = max(UserVersions.pageCount)
// Insert current version into VersionGraph.
VersionGraph.insert(current);
foreach ğ‘¢ğ‘£ in UserVersions do
// Determine if current is a parent of ğ‘¢ğ‘£ .
isParentVersion = ğ‘¢ğ‘£ .deltaSet âˆ© current.deltaSet
if |isParentVersion| > 0 then
// Add current as a parent to version ğ‘¢ğ‘£ .
ğ‘¢ğ‘£.parents.append(current.versionId);
// Complete diff operation on version ğ‘¢ğ‘£ .
ğ‘¢ğ‘£.deltaSet = ğ‘¢ğ‘£.deltaSet - current.deltaSet;
foreach ğ‘šğ‘£ in UserVersions do
if ğ‘šğ‘£ == ğ‘¢ğ‘£ âˆ¨ ğ‘šğ‘£ == ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ then continue ;
else if ğ‘¢ğ‘£.deltaSet == ğ‘šğ‘£ .ğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ğ‘†ğ‘’ğ‘¡ then
// Complete Merge on ğ‘¢ğ‘£ and ğ‘šğ‘£ .
ğ‘¢ğ‘£.pageSet.append(ğ‘šğ‘£.pageSet);
ğ‘¢ğ‘£.userSet.append(ğ‘šğ‘£.userSet);
UserVersions.remove(ğ‘šğ‘£);
1 begin
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31 end
end
end
end
end
// Remove current version from UserVersions.
UserVersions.remove(current);
end
return VersionGraph
4 EVALUATION
Our evaluation addresses the following research questions:
of the forensic investigation?
â€¢ How effective is Mnemosyne at reducing the analysis scope
â€¢ How does the benign evolution of websites affect Mnemosyne
â€¢ What is the runtime performance overhead of Mnemosyneâ€™s
â€¢ What are the data storage requirements for Mnemosyne?
auditing daemon and analysis?
analysis?
4.1 Data Collection
The highly-targeted nature of watering hole attacks makes them
extremely difficult to detect in the wild, which results in difficulty of
collecting data on them. To overcome this, we developed a scalable
testbed that is capable of simulating sophisticated watering hole
attacks on a large organization. This testbed has the capability to
make arbitrary modifications to an otherwise benign website and
simulate visits to compromised websites. To simulate a compro-
mised website, we relied on Chromiumâ€™s DevToolâ€™s Fetch names-
pace, which can intercept and modify network requests made by
the browser. Our testbed supports directly modifying HTML pages
and scripts on-the-fly to support various modification techniques.
This allows the testbed to simulate malicious modifications being
made to the website. To simulate visits, we developed a driver based
on puppeteer [8]. During a visit, the driver navigates to up to 15
webpages on the site. However, only visiting webpages limits exe-
cution coverage because modern webpages are highly-dynamic and
event-driven. To address this, our driver simulates JS-based events
while visiting the page. Also, the driver automatically emulates
different browser/OS combinations, including mobile operating
systems (e.g., screen size and other system properties are adjusted
according to the emulated system). Finally, we designed our testbed
to be scalable by making each crawler a container-based applica-
tion, which allowed us to execute multiple crawlers in parallel. Each
container contains a headless Chromium browser and the driver
that simulates a visit to the compromised domain.
4.2 Datasets
Leveraging the watering hole testbed discussed in Â§4.1, we collected
datasets to develop 7 attack scenarios discussed in Â§4.2.1 and two
benign datasets discussed in Â§4.2.2.
4.2.1 Attack Scenarios. To extensively evaluate Mnemosyne, we
developed 7 attack scenarios inspired by watering hole attacks
that have been reported in the wild. A detailed description of the
attack scenarios is provided in Table 4, including the website that
we simulated being compromised and a reference to the real-world
attack that inspired this scenario. For each attack scenario, we
collected data for at least two weeks, and each scenario had three
phases. During the first phase, the website was benign, and no
malicious modifications were made. The purpose of this phase
was to collect the auditing information necessary to model the
benign behavior of the website. The second phase simulated a
reconnaissance phase, where the website was compromised. At this
point, the simulated attacks had not targeted any users. The final
stage was the targeting phase, where the attack actively sent the
malicious payload to victims. We provide statistics related to the
size of the attack graph for each scenario and important crawling
statistics in Table 5. On average, we simulated 1,844 visits during
the attack scenarios. Also, we found that, on average, 1,941 distinct
URLs related to the compromised domain were visited. Finally, the
average graph size for each attack scenario was 6.2M and 11.0M
nodes and edges respectively.
4.2.2 Benign Datasets. The benign datasets contain simulated vis-
its to a large set of benign websites. We collected two datasets,
which we will call â€œCategoriesâ€ and â€œAlexaâ€. The details of each
dataset is described below.
Categories. The categories dataset was developed by crawling 900
websites from February 6th, 2020 to August 18, 2020. Each website
crawled had an associated category tag, where the category tag
represents the website type (e.g., News, Sports, etc.). To categorize
the websites, we leveraged DMOZ; the most comprehensive, human-
edited directory of the Web [3]. The 900 websites were randomly
selected. The number of webpages in each category is provided in
Table 9 in the Appendix. After removing websites that returned an
error, we had a dataset of 830 valid websites that included 278,177
unique pages related to the websites.
Alexa. The Alexa dataset was developed by crawling the Alexa
1k from July 13th, 2020 to August 18, 2020. In total, we collected
120,245 unique pages related to these websites.
Session 3B: Malware CCS '20, November 9â€“13, 2020, Virtual Event, USA793Attack Scenario
Malicious OAuth
Access
Clickjacking
Malicious Software
Update
Credential Harvesting
Keylogging
Tabnabbing
Driveby
Website & Description
Reference
www.cfr.org â€” The adversary injected a malicious script into the homepage. If users were targeted, the original content
of the page was blurred, and a malicious overlay was injected into the DOM. If the client interacted with the overlay,
it redirected them to an attacker-controlled website hosting a malicious OAuth app that requested sensitive email
permissions.
www.acumen.org â€” The attack embedded a malicious iframe onto the page, which redirected users to a malicious
website, hosting a malicious OAuth app. The app requested sensitive Gmail permissions.
www.energy.gov â€” The adversary injected a malicious flash update onto the webpage. Victims of the attack were
tricked into downloading a trojanized version of Adobe Flash.
www.cipe.org â€” The adversary manipulated the original webpageâ€™s DOM to mimic a Google login page. Victims of the
attack would be tricked into leaking sensitive credentials.
www.xero.com â€” The adversary injected a keylogger into the webpage, which logged all keystrokes by targeted clients.
www.thebanker.com â€” The adversary used a tabnabbing attack to distract the user. Next, the attackers injected an
iframe, which mimicked the institutionâ€™s login page. The attack victims leaked their sensitive email credentials.
www.zingnews.vn â€” The adversary injected a malicious script into the homepage. If the users matched the client
profile, the malicious script injected a 1x1 iframe into the DOM, which navigated to a malicious website that exploited
CVE-2020-6405 to complete a drive-by download attack.
Table 4: Description of each attack scenario and the corresponding case in the wild.
[41]
[54]
[38]
[11]
[55]
[21]
[31]
Attack Scenario
Nodes/Edges
Visit
Sessions
Malicious OAuth Access
9.20M / 16.1M
Clickjacking
4.40M / 5.90M
Malicious Software Update 628K / 1.30M
770K / 1.60M
Credential Harvesting
20.9M / 33.8M
Keylogging
6.70M / 15.0M
Tabnabbing
952K / 3.10M
Driveby
Average
6.20M / 11.0M
5.37K
950
1.00K
927
1.91K
2.18K
580
1.84K
Pages
Visited
57.8K
7.66K
8.69K
8.40K
100K
83.2K
6.72K
39.0K
Distinct
URLs
4.96K
267
640
364
5.33K
741
1.29K
1.94K
Script
Instances
8.80M
3.99M
547K
710K
18.8M
6.20M
742K
5.7M
Network
Events
3.10M
658K
374K
449K
6.20M
4.30M
926K
2.31M
Table 5: Graph statistics for each attack scenario.
4.2.3 Data and Evaluation Limitations. There are some potential
limitations to pay attention to when relying on simulated attack
scenarios to complete an evaluation. First, when visiting each site in
the attack scenario, the navigation through different pages on this
website was randomized. In practice, website visitors will typically
follow specific and routine visiting patterns to complete specific
tasks. However, Mnemosyneâ€™s analysis does not rely on the vis-
iting pattern of the users, so this is not expected to represent a
significant issue in practice. Next, our testbed does not support
automatically logging into a webpage. Since portions of a website
may require authentication to view, some portions of a website may
be unavailable to our testbed. While this does limit the visibility of
the website in our experiments, it will not be a significant issue in
practice. This is because, in a real-world deployment, Mnemosyne
would have visibility to these portions of the website once the user
logged into the site, since Mnemosyne will record audit logs as
the user interacts with webpages through the browser. Finally, one
limitation of our testbed is content tailored to a specific user for be-
nign use-cases. While our testbed can simulate different users, this
simulation mainly alters the User-Agent string when visiting the
website. Unfortunately, for websites that distribute content based
on profiling the user or requiring the user to log in, our current
implementation of emulating different users will most likely lead
to the websites not serving "user-specific" content in a meaningful
way. However, we believe benign use-cases of user-specific content
will not have a large affect on Mnemosyneâ€™s analysis because, while
websites routinely serve user-specific content, this content will be
served off the same set of domains. Since Mnemosyne would iden-
tify these domains during its profiling phase, these user-specific
modifications would be filtered out of the analysis scope.
4.3 Attack Scenario Investigation
Forensic Analysis Scope Reduction. To measure the efficiency
4.3.1
gains that Mnemosyne provides, we completed an empirical evalu-
ation to quantify how much of the analysis space is reduced when
using Mnemosyne to complete the investigation.
Defining the Analysis Space. We define the analysis space as
the set of domains and scripts related to each attack scenario in
Table 4. We want to point out that the number of scripts reported
for each attack scenario is the number of unique script URLs, not
to be confused with the number of script instances. The choice to
focus on domains and scripts is based on a preliminary study we
conducted with 5 security-trained professionals. The purpose of
this study was to assess how different professionals approach foren-
sic investigation tasks. To this end, we assigned an investigation
task to each participant, provided them access to the browser logs,
and asked them to determine the window-of-compromise and the
attackâ€™s victims. Each participant was provided with access to a
graph database that contained the attack scenario logs and a graph-
ical interface2 for interacting and making queries to the database
to enable the investigation. After each participant completed the
task, we conducted an exit interview to discuss what strategies the
participants adopted to perform the investigation. We found that
most participants used a two-phased approach. First, they filtered
out well-known domains. Then, for the remaining domains, they
analyzed the scripts served by those domains. Because of this ap-
proach, we consider the number of domains and scripts involved
in the attack scenario to play a larger role in the analysis time
compared to other types of resources (e.g., images, CSS files, etc.).
2https://neo4j.com/developer/neo4j-browser
Session 3B: Malware CCS '20, November 9â€“13, 2020, Virtual Event, USA794Raw
# of Domains
59
18
23
20
11
64
571
109
Attack Scenario
Malicious OAuth Access
Clickjacking
Malicious Software Update
Credential Harvesting