o
j
d
e
t
e
p
m
o
c
f
o
r
e
b
m
u
N
50
40
30
20
10
0
s
e
p
i
r
t
s
d
e
d
o
c
n
e
f
o
r
e
b
m
u
N
96
72
48
24
0
RR
EAR
Testbed 
Simulation
0
50
100
150
200 0
Time (s)
50
100
150
200
Figure 12. Experiment B.1: Simulator validation.
0
1000
Time (s)
2000
3000
Figure 10. Experiment A.3: Impact of EAR on MapReduce performance.
We observe similar performance trends between EAR and RR.
PlacementManager
Write/Encoding Decision
RR or EAR 
TrafficManager
  1.Write Traffic   
  2.Encoding Traffic 
  3.Background Traffic
Topology
data placement decision
data traffic
Figure 11. Simulator overview.
CFS topology and manages both cross-rack and intra-rack
link resources. To complete a data transmission request,
the Topology module holds the corresponding resources for
some duration of the request subject to the speciﬁed link
bandwidth. We assume that a CFS topology only contains
nodes that store data (e.g., DataNodes of HDFS) and ex-
cludes the node that stores metadata (e.g., the NameNode
of HDFS), as the latter only involves a small amount of
data exchanges. The TrafﬁcManager module generates three
trafﬁc streams: write, encoding, and background trafﬁc, and
feeds the streams simultaneously to the Topology module.
We simulate the write, encoding, and background trafﬁc
streams as follows. We ﬁrst assign a node to perform each
of the requests. For each write request, it receives replica
placement decisions from the PlacementManager module
and performs replication based on either RR or EAR. For
each encoding request, it ﬁrst obtains the replica locations
of the data blocks for the stripe under RR or EAR from
the PlacementManager module. It then downloads the data
blocks and uploads the parity blocks. For each background
request, it transmits a certain amount of data to another node,
either in the same rack or a different rack.
Experiment B.1 (Simulator validation): We ﬁrst val-
idate that our simulator can accurately simulate the per-
formance of both RR and EAR. We simulate our testbed
Table I
EXPERIMENT B.1: VALIDATION OF WRITE RESPONSE TIMES.
Time (in seconds)
Without encoding
With encoding
testbed
1.43
2.45
RR
simulation
1.40
2.35
EAR
simulation
1.40
2.04
testbed
1.42
2.13
with the same topology (12 racks with one DataNode each)
and the same link bandwidth (1Gb/s Ethernet). Using the
simulator, we replay the write and encode streams using the
same setting as in Experiment A.2, such that we encode
96 stripes with (10, 8) erasure coding after we issue write
requests for 30s. We obtain the averaged simulation results
from 30 runs with different random seeds and compare with
the results of the testbed experiments. Figure 12 shows the
cumulative number of encoded stripes versus the elapsed
time from the start of the encoding operations for both
testbed experiments and simulations. We observe that the
simulator precisely captures the encoding performance under
both RR and EAR.
Table I also shows the averaged write response times
in both testbed experiments and discrete-event simulations
when the write requests are carried out with and without
encoding in the background. We ﬁnd that in all cases we
consider, the response time differences between the testbed
experiments and discrete-event simulations are less than
4.3%. Thus, our simulator also precisely captures write
performance.
Experiment B.2 (Impact of parameter choices): We
evaluate RR and EAR with our simulator in a large-scale
setting. We consider a CFS composed of R = 20 racks
with 20 nodes each, such that nodes in the same rack are
connected via a 1Gb/s top-of-rack switch, and all top-of-
rack switches are connected via a 1Gb/s network core. We
conﬁgure the CFS to store data with 3-way replication,
where the replicas are stored in two racks. The CFS then
encodes the data with (n, k) = (14, 10) erasure coding that
can tolerate 4-node or 4-rack failures, as in Facebook [21].
Although RR may require block relocation after encoding
to preserve availability, we do not consider this operation, so
156156
Encode
Write
Encode
Write
Encode
Write
t
p
h
t
d
e
z
i
l
r
o
m
r
o
N
2.0
1.8
1.6
1.4
1.2
1.0
t
p
h
t
d
e
z
i
l
r
o
m
r
o
N
3.0
2.5
2.0
1.5
1.0
6
8
10 12
k
6
8
10 12
2
4
6
8
2
4
6
8
n−k
(a) Varying k
(b) Varying n − k
0.2 0.5 0.8 1.0
0.2 0.5 0.8 1.0
Link bandwidth (Gb/s)
(c) Varying link bandwidth
Encode
Write
Encode
Write
t
p
h
t
d
e
z
i
l
r
o
m
r
o
N
2.5
2.0
1.5
1.0
t
p
h
t
d
e
z
i
l
r
o
m
r
o
N
2.2
2.0
1.8
1.6
1.4
1.2
1.0
Encode
Write
t
p
h
t
d
e
z
i
l
r
o
m
r
o
N
2.2
2.0
1.8
1.6
1.4
1.2
1.0
t
p
h
t
d
e
z
i
l
r
o
m
r
o
N
1.8
1.6
1.4
1.2
1.0
2
4
6
8
2
4
6
8
Number of replicas
1
2
3
4
1
2
3
4
Write request rate (requests/second)
1
2
4
1
2
4