Patch keywords (number) For each patch, we counted
the number of occurrences of each C/C++ keyword.
See Table 2 for a statistical analysis of the diﬀerent
distributions of each keyword.
3.2.4 Features Scoped by File
Future changes (number) If the commit at hand is not
the most current one, this is the number of times the
ﬁle will be changed by later commits. We only use
this feature for our historical analysis and not for the
classiﬁer, since this feature is naturally not available
for new commits.
430Keyword VCCs
mean mean
others
eﬀect
size
U
if
int
struct
return
static
void
unsigned
goto
sizeof
break
char
39.00
31.30
32.38
18.76
15.17
12.52
8.66
5.92
4.37
5.56
6.71
7.82
7.02
3.66
3.60
3.58
4.31
1.51
0.43
0.78
0.84
2.68
37013390*
39930128*
39729656*
41342834*
45382955*
63935365*
64440969*
64798818*
66764357*
74389604*
93400907*
70 %
68 %
68 %
67 %
64 %
49 %
48 %
48 %
46 %
40 %
25 %
Table 2: Statistical analysis of C/C++ keywords
sorted by eﬀect size [33], Mann–Whitney U test sig-
niﬁcant (*) if p < 0.000357.
3.3 Excluded Features
As can be seen in Table 1, the vast majority of the fea-
tures depend only on data gathered from the version control
system and not from additional information on GitHub or
any other platform. In fact, we left out some features that
were only available on some projects or for few commits since
the data was too sparse to reveal anything reliable. We will
brieﬂy discuss why we excluded some features which might
seem counter-intuitive.
One feature that would be promising but which we did
not include was issue tracker information. GitHub provides
an issue tracker and even links texts like “ﬁxes #123” in the
commit message to the corresponding issue. However, the
projects which use this feature tend to be smaller projects,
while the older and larger projects for which we have a rich
set of CVE data predominantly use an external issue tracker.
Thus, this feature is not useful for us at this time.
Another piece of information that is interesting – but un-
fortunately too sparse at the moment – is the content of the
discussion surrounding the inclusion of a change into the
main repository. For this information, features could be the
length of the discussion, the number of people involved, or
the mean experience (in terms of contributions) of the people
involved. Projects that use GitHub’s functionalities exten-
sively often do this through “pull requests”. A contributor
submits a commit to his own, unoﬃcial repository and sub-
sequently notiﬁes the maintainer of the oﬃcial repository to
pull in the changes he made. GitHub provides good support
for this work ﬂow, including the ability to make comments
on a pending pull request. Although this data could be use-
ful for the classiﬁcation of commits, at this point, too few
projects use this work ﬂow to be useful.
3.4 Statistical Analysis of Features
For each numerical feature, we wanted to assess its ﬁt-
ness with respect to distinguishing VCCs from unclassiﬁed
commits. We used the Mann–Whitney U test4 in order to
compare the distribution of a given feature within the set
of commits with vulnerabilities against the set of all unclas-
siﬁed commits. The null hypothesis states that the feature
is distributed independently from whether the commit con-
4The Mann–Whitney U test is used to test whether a value
is distributed diﬀerently between two populations.
tained a bug or not. If we can reject the null hypothesis,
the feature is distributed diﬀerently in each set and thus
is a promising candidate as input for the machine-learning
algorithms.
We used the Bonferroni correction to correct for multi-
ple testing for the 17 features we tested. Therefore, we test
against the stricter signiﬁcance level of 0.00059, which corre-
sponds to a non-corrected p ≤ 0.01 for each individual test.
The date and time features (project age and commit with
time zone) were converted to numerical features based on
seconds that have elapsed since January 1, 1970 UTC (Unix
epoch).
3.4.1 Features Scoped by Project
These features were attributed to the commit depending
on the project the commit was taken from. Since all commits
from a repository, whether containing vulnerabilities or not,
have the same features, these features are too broad to ac-
tually distinguish commits. However, they can be valuable
in combination with other features later on. For brevity, we
do not discuss the features on their own here, though the
table shows the signiﬁcance testing.
3.4.2 Patch Keyword Features
For each commit we counted the occurrences of each of the
following 28 C/C++ keywords: bool, char, const, extern,
false, float, for, if, int, long, namespace, new, opera-
tor, private, protected, sizeof, static, static, struct,
switch, template, throw, typedef, typename, union, un-
signed, virtual, and volatile. We then used the Mann–
Whitney U test to ﬁnd out whether the given keyword is
used more or less frequently in VCCs compared to unclassi-
ﬁed commits. Table 2 shows a subset of those keywords with
high signiﬁcance and high eﬀect. We say that an eﬀect is
signiﬁcant if p < 0.000357, corresponding to 0.01/28, again
accounting for a Bonferroni correction for multiple testing
for the 28 keywords.
The eﬀect size measures the percentage of pairs that sup-
port the hypothesis. For example, for the keyword if, the
vulnerable commits contain more ifs than the unclassiﬁed
commits in 70 % of the cases. As can be seen by looking at
the mean values for each distribution, if there is a statistical
eﬀect, the VCCs are more likely to contain those keywords
compared to unclassiﬁed commits.
3.4.3 Features Scoped by Commit or File
All remaining features except for the number of deleted
lines are distributed diﬀerently over VCC versus unclassiﬁed
commits, with p = 3.9 × 10−6 the number of hunks being the
least signiﬁcant result. We note that the fact that a feature
is distributed diﬀerently does not mean that this feature
can be used to distinguish between the two sets. However,
these results provide some hint as to why a machine-learning
approach that uses a combination of these features can be
successful.
The only feature where the diﬀerence was not signiﬁcant
was the number of deleted lines (p = 4.6 × 10−4), contrary
to the number of added lines (p = 3.9 × 10−37), for which
there is a signiﬁcant diﬀerence in the distribution. When
we manually looked at commits with known vulnerabilities
and compared them to unclassiﬁed commits, we saw that
the former often added a great deal of code, whereas the
number of deleted or edited lines were the same as for un-
431classiﬁed commits. This ﬁnding conﬁrms the intuition that
security bugs are not commonly introduced by code edits or
refactoring, but that new code is a more likely entry points
for vulnerabilities. To the best of our knowledge this fact
has not been used to ease the workload of code reviewers.
3.4.4 Text-Based Features
One of the central tenets of our work is that combining
code metrics with GitHub metadata can help with the de-
tection of VCCs. While both the code and the metadata
features detailed above are “hard” numerical features, there
are also a number “soft” features contained in GitHub that
can be helpful. These text-based features, like the commit
message, cannot be evaluated using statistical tests as above,
but will be integrated into the machine-learning algorithm
using a generalized bag-of-words model as we will discuss in
Section 4.1.
4. LEARNING-BASED DETECTION
The diﬀerent features presented in the previous sections
provide information for analyzing the search for suspicious
commits and the discovery of potential vulnerabilities. As
the large number of these features renders the manual con-
struction of detection rules diﬃcult, we apply techniques
from the area of machine-learning to automatically analyze
the commits and rank them so code-reviewers can prioritise
their work. The construction of a learning-based classiﬁer,
however, poses several challenges that need to be addressed
to make our approach useful in practice:
1. Generality: Our features comprise information that
range from numerical code metrics to structured meta-
data, such as words in commit messages and keywords
in code. Consequently, we strive for a classiﬁer that is
capable of jointly analyzing these heterogeneous fea-
tures and inferring a combined detection model.
2. Scalability: To analyze large code repositories with
thousands of source ﬁles and commits, we require a
very eﬃcient learning method which is able to operate
on the large amount of available features in reasonable
time.
3. Explainability: To help an analyst in practice, it is ben-
eﬁcial if the classiﬁer can give a human-comprehensible
explanation as to why the commit was ﬂagged, instead
of requiring an analyst to blindly trust a black-box de-
cision.
We address these challenges by combining two concepts
from the domains of machine-learning and information re-
trieval. In particular, we ﬁrst create a joint representation
for the heterogeneous features using a generalized bag-of-
words model and then apply a linear Support Vector Ma-
chine (SVM)—a learning method that can be extended to
provide explanations for its decisions and which is also eﬃ-
cient enough to cope with the large number of features which
need to be analysed.
4.1 Generalized Bag-of-Words Models
Bag-of-word models have been initially designed for anal-
ysis of text documents [30, 29]. In order to combine both
code metric based numerical features with GitHub meta-
data features, we generalize these models by considering a
generic set of tokens S for our analysis. This set can contain
textual words from commit messages as well as keywords,
identiﬁers and other tokens from the code of a commit. In
particular, we obtain these tokens by splitting the commit
message and its code using spaces and newlines. Further-
more, we ignore certain tokens, such as author names and
email addresses, since they might bias the generality of our
classiﬁer and could compromise privacy.
Formally, we deﬁne the mapping ϕ from a commit to a
vector space as
ϕ : X −→ R|S|
, ϕ : x (cid:55)−→(cid:0)b(x, s)(cid:1)
,
s∈S
where X is the set of all commits and x ∈ X an individual
commit to be embedded in the vector space. The auxiliary
function b(x, s) returns a binary ﬂag for the presence of a
token s in x and is given by
(cid:40)
b(x, s) =
1 if token s is contained in x
0 otherwise.
To also incorporate numerical features like the author con-
tribution into this model, we additionally convert all nu-
merical features into strings. This enables us to add all
arbitrary numbers to S and thereby treat both kinds of fea-
tures equally. However, when using a string representation
for numerical features we have to ensure that similar values
are still identiﬁed as being similar. This is obviously not
the case for a naive mapping, as “1.01” and “0.99” represent
totally diﬀerent strings.
We tackle this problem by mapping all numerical features
to a discrete grid of bins prior to the vector space embed-
ding. This quantization ensures that similar values fall into
the same bins. We choose diﬀerent bin sizes depending on
the type of the feature. If the numerical values are rather
evenly distributed, we apply a uniform grid, whereas for
features with skewed distribution we a apply a logarithmic
partitioning. For the latter, we apply the logarithmic func-
tion to its values and cut oﬀ all digits after the ﬁrst decimal
place.
To better understand this generalized bag-of-words model,
let us consider a ﬁctitious commit x, where a patch has been
written by a user who did not contribute to a project before.
The committed patch is written in C and contains a call
to an API function which is associated with a buﬀer write
operation. The corresponding vector representation of the
commit x looks as follows
···
···
1
0···
1
0···
ϕ(x) (cid:55)→
AUTHOR_CONTRIBUTION:0.0
AUTHOR_CONTRIBUTION:10.0
···
buf_write_func();
some_other_func();
···
The two tokens indicative of the commit are reﬂected by
non-zero dimensions, while all unrelated tokens are associ-
ated with zero dimensions. Note that the resulting vector
space is high-dimensional and may contain several thousands
of dimensions. For a concrete commit x, however, the vast
majority of these dimensions are zero and thus the vector
ϕ(x) can be stored in a sparse data structure. We make use
of the open-source tool Sally [27] for this purpose, which
implements diﬀerent strategies for extracting and storing
sparse feature vectors.
4324.2 Classiﬁcation and Explainability
While in principle a wide range of methods are available
for learning a classiﬁer for the detection of vulnerability
contributing commits, only few methods scale with larger
amount of data while also providing explanations for their
decisions. One technique satisfying both properties are lin-
ear Support Vector Machines (SVM). This variant of classic
SVMs does not apply the kernel trick for learning, but in-
stead directly operates in the input space. As a result, the
run-time complexity of a linear SVM scales linearly in the
number of vectors and features.
We implement our classiﬁer for commits using the open-