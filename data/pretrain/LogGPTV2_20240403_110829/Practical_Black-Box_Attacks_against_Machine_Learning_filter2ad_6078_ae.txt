DNN
87.44
96.78
95.68
LR
96.19
96.43
95.83
DNN
84.50
97.17
91.57
LR
88.94
92.05
97.72
Table 3: Misclassiﬁcation rates (%) of the Amazon
and Google oracles on adversarial samples produced with
DNN and LR substitutes after ρ = 3, 6 epochs. The 2nd
column is the number of queries during substitute training.
Last row uses a periodic step size and reservoir sampling.
.
515upload the CSV ﬁle with the MNIST training data to Google
Cloud Storage. We then train a model using the Prediction
API. The only property we can specify is the expected multi-
class nature of our model. We then evaluate the resulting
model on the MNIST test set. The API reports an accuracy
of 92% on this test set for the model trained.
Substitute Training: By augmenting an initial training
set of 100 test set samples, we train a DNN and LR substitute
for each of the two oracles. We measure success as the rate of
adversarial samples misclassiﬁed by the corresponding oracle,
among the 10, 000 produced from the test set using the fast
gradient sign method with parameter ε = 0.3. These rates,
computed after ρ ∈ {3, 6} dataset augmentation iterations,
are reported in Table 3. Results reported in the last row use
both a periodic step size and reservoir sampling (hence the
reduced number of queries made to train the substitute).
Experimental Results: With a 96.19% misclassiﬁcation
rate for a perturbation ε = 0.3 crafted using a LR substitute
trained with 800 oracle queries, the model hosted by Amazon
is easily misled. The model trained by Google is somewhat
more robust to adversarial samples, but is still vulnerable to
a large proportion of samples: 88.94% of adversarial samples
produced in the same conditions are misclassiﬁed. A careful
read of the documentation indicated that the model trained
by Amazon is a multinomial logistic regression.8 As pointed
out in [4], shallow models like logistic regression are unable
to cope with adversarial samples and learn robust classiﬁers.
This explains why the attack is very successful and the LR
substitute performs better than the DNN substitute. We
were however not able to ﬁnd the ML technique Google uses.
The last row of Table 3 shows how combining periodic step
sizes with reservoir sampling allow us to reduce querying
of both oracles during substitute training, while crafting
adversarial samples with higher transferability to the target
classiﬁer.
Indeed, querying is reduced by a factor larger
than 3 from 6, 400 to 2, 000 queries, while misclassiﬁcation
decreases only from 96.78% to 95.68% for the Amazon DNN
substitute. It is still larger than the rate of 87.44% achieved
after 800 queries by the substitute learned without the reﬁne-
ments. Similarly, the misclassiﬁcation rate of the Google LR
substitute is 97.72%—compared to 92.05% with the original
method after ρ = 6 epochs, conﬁrming the result.
8. DEFENSE STRATEGIES
The two types of defense strategies are: (1) reactive where
one seeks to detect adversarial examples, and (2) proactive
where one makes the model itself more robust. Our attack is
not more easily detectable than a classic adversarial example
attack. Indeed, oracle queries may be distributed among a
set of colluding users, and as such remain hard to detect. The
defender may increase the attacker’s cost by training models
with higher input dimensionality or modeling complexity,
as our experimental results indicate that these two factors
increase the number of queries required to train substitutes.
In the following, we thus only analyze our attack in the face
of defenses that seek to make the (oracle) model robust.
Many potential defense mechanisms fall into a category we
call gradient masking. These techniques construct a model
that does not have useful gradients, e.g., by using a nearest
neighbor classiﬁer instead of a DNN. Such methods make
8
docs.aws.amazon.com/machine-learning
Training ε Attack ε O→O
0.15
0.15
0.3
0.3
0.3
0.4
0.3
0.4
S → S
S → O
10.12% 94.91% 38.54%
43.29% 99.75% 71.25%
0.91% 93.55% 1.31%
29.56% 99.48% 10.30%
Table 4: Evaluation of adversarial training: the columns
indicate the input variation parameter used to inject adversar-
ial examples during training and to compute the attacks, the
attack success rate when examples crafted on the (O)racle are
deployed against the (O)racle, the attack success rate when
examples crafted on the (S)ubstitute are deployed against
the (S)ubstitute, and the attack success rate when examples
crafted on the (S)ubstitute are deployed against the (O)racle.
.
it diﬃcult to construct an adversarial example directly, due
to the absence of a gradient, but are often still vulnerable
to the adversarial examples that aﬀect a smooth version of
the same model. Previously, it has been shown that nearest
neighbor was vulnerable to attacks based on transferring
adversarial examples from smoothed nearest neighbors[4].
We show a more general ﬂaw in the category of gradient
masking. Even if the defender attempts to prevent attacks by
not publishing the directions in which the model is sensitive,
these directions can be discovered by other means, in which
case the same attack can still succeed. We show that the
black-box attack based on transfer from a substitute model
overcomes gradient masking defenses. No fully eﬀective
defense mechanism is known, but we study the two with the
greatest empirical success so far: adversarial training [4, 14],
and defensive distillation for DNNs [10].
Adversarial training: It was shown that injecting adver-
sarial examples throughout training increases the robustness
of signiﬁcantly descriptive models, such as DNNs [4, 14, 17].
We implemented an approximation of this defense using the
Google Prediction API. Since the API does not support the
generation of adversarial examples at every step of training,
as a correct implementation of adversarial training would
do, we instead inject a large amount of adversarial examples
infrequently. After training in this way, the model has a
misclassiﬁcation rate of 8.75% on the unperturbed test set,
but the adversarial misclassiﬁcation rate rises to 100% when
ρ = 6. To evaluate this defense strategy using a correct
implementation, we resort to training the oracle locally, us-
ing our own codebase that includes support for generating
adversarial examples at each step. After each training batch,
we compute and train on adversarial examples generated
with the fast gradient sign method before starting training
on the next batch of the original training data. Results are
given in Table 4. We observe that for ε = 0.15, the defense
can be evaded using the black-box attack with adversarial
examples crafted on the substitute and misclassiﬁed by the
oracle at rates up to 71.25%. However, for ε = 0.3, the
black-box attack is not eﬀective anymore. Therefore, making
a machine learning model robust to small and inﬁnitesimal
perturbations of its inputs is an example of gradient mask-
ing and can be evaded using our substitute-based black-box
approach. However, making the model robust to larger and
ﬁnite perturbations prevents the black-box attack. To con-
ﬁrm this hypothesis, we now show that defensive distillation,
which makes the model robust to inﬁnitesimal perturbations,
can be evaded by the black-box approach.
516said small perturbations. We conclude that defending against
ﬁnite perturbations is a more promising avenue for future
work than defending against inﬁnitesimal perturbations.
9. CONCLUSIONS
We introduced an attack, based on a novel substitute
training algorithm using synthetic data generation, to craft
adversarial examples misclassiﬁed by black-box DNNs. Our
work is a signiﬁcant step towards relaxing strong assump-
tions about adversarial capabilities made by previous attacks.
We assumed only that the adversary is capable of observing
labels assigned by the model to inputs of its choice. We vali-
dated our attack design by targeting a remote DNN served
by MetaMind, forcing it to misclassify 84.24% of our adver-
sarial samples. We also conducted an extensive calibration
of our algorithm and generalized it to other ML models by
instantiating it against classiﬁers hosted by Amazon and
Google, with success rates of 96.19% and 88.94%. Our attack
evades a category of defenses, which we call gradient mask-
ing, previously proposed to increase resilience to adversarial
examples. Finally, we provided an intuition for adversarial
sample transferability across DNNs in Appendix B.
10. REFERENCES
[1] Marco Barreno, et al. Can machine learning be secure? In
Proceedings of the 2006 ACM Symposium on Information,
Computer and Communications Security.
[2] Battista Biggio, et al. Evasion attacks against machine learning
at test time. In Machine Learning and Knowledge Discovery
in Databases, pages 387–402. Springer, 2013.
[3] Ian Goodfellow, et al. Deep learning. Book in preparation for
MIT Press (www.deeplearningbook.org), 2016.
[4] Ian J Goodfellow, et al. Explaining and harnessing adversarial
examples. In Proceedings of the International Conference on
Learning Representations, 2015.
[5] Ling Huang, et al. Adversarial machine learning. In
Proceedings of the 4th ACM workshop on Security and
artiﬁcial intelligence, pages 43–58, 2011.
[6] Alexey Kurakin, et al. Adversarial examples in the physical
world. arXiv preprint arXiv:1607.02533, 2016.
[7] Yann LeCun et al. The mnist database of handwritten digits,
1998.
[8] Erich L. Lehmann, et al. Testing Statistical Hypotheses.
Springer Texts in Statistics, August 2008.
[9] Nicolas Papernot, et al. The limitations of deep learning in
adversarial settings. In Proceedings of the 1st IEEE European
Symposium on Security and Privacy, 2016.
[10] Nicolas Papernot, et al. Distillation as a defense to adversarial
perturbations against deep neural networks. In Proceedings of
the 37th IEEE Symposium on Security and Privacy.
[11] Mahmood Sharif, et al. Accessorize to a crime: Real and
stealthy attacks on state-of-the-art face recognition. In
Proceedings of the 2016 ACM SIGSAC Conference on
Computer and Communications Security. ACM, 2016.
[12] Nedim Srndic, et al. Practical evasion of a learning-based
classiﬁer: A case study. In Proceeding of the 35th IEEE
Symposium on Security and Privacy.
[13] Johannes Stallkamp, et al. Man vs. computer: Benchmarking
machine learning algorithms for traﬃc sign recognition. Neural
networks, 32:323–332, 2012.
[14] Christian Szegedy, et al. Intriguing properties of neural
networks. In Proceedings of the International Conference on
Learning Representations, 2014.
[15] Florian Tram`er, et al. Stealing machine learning models via
prediction apis. In 25th USENIX Security Symposium, 2016.
[16] Jeﬀrey S Vitter. Random sampling with a reservoir. ACM
Transactions on Mathematical Software, 1985.
[17] D Warde-Farley, et al. Adversarial perturbations of deep neural
networks. Advanced Structured Prediction, 2016.
[18] Weilin Xu, et al. Automatically evading classiﬁers. In
Proceedings of the 2016 Network and Distributed Systems
Symposium.
Figure 12: Evaluation of defensive distillation: Per-
centage of adversarial examples crafted using the Goodfellow
algorithm at varying ε misclassiﬁed by the oracle. T is the
temperature of distillation [10]. Curves marked by (direct)
indicate baseline attacks computed on the oracle, all other
curves where computed using a substitute, as described in
Section 4. Despite distillation preventing the attack on the
oracle directly, using a substitute allows us to evade it.
Defensive distillation: Due to space constraints, we refer
readers to [10] for a detailed presentation of defensive distil-
lation, which is an alternative defense. Because the remotely
hosted APIs we study here do not implement defensive distil-
lation or provide primitives that could be used to implement
it, we are forced to evaluate this defense on a locally trained
oracle. Therefore, we train a distilled model as described
in [10] to act as our MNIST oracle.
We train several variants of the DNN architecture A at
diﬀerent distillation temperatures T = 5, 10, 100. For each
of them, we measure the success of the fast gradient sign
attack (i.e., the Goodfellow et al. algorithm) directly per-
formed on the distilled oracle—as a baseline corresponding
to a white-box attack—and using a substitute DNN trained
with synthetic data as described throughout the present pa-
per. The results are reported in Figure 12 for diﬀerent values
of the input variation parameter ε on the horizontal axis.
We ﬁnd that defensive distillation defends against the fast
gradient sign method when the attack is performed directly
on the distilled model, i.e. in white-box settings. However, in
black-box settings using the attack introduced in the present
paper, the fast gradient sign method is found to be successful
regardless of the distillation temperature used by the ora-
cle. We hypothesize that this is due to the way distillation
defends against the attack: it reduces the gradients in local
neighborhoods of training points. However, our substitute
model is not distilled, and as such possesses the gradients
required for the fast gradient sign method to be successful
when computing adversarial examples.
Defenses which make models robust in a small neighbor-
hood of the training manifold perform gradient masking: they
smooth the decision surface and reduce gradients used by ad-
versarial crafting in small neighborhoods. However, using a
substitute and our black-box approach evades these defenses,
as the substitute model is not trained to be robust to the
010203040506070809010000.20.40.60.81Adversarial examples misclassified by oracleGoodfellow attack input variation parameterno distillationT=5T=10T=100T=5 (direct)T=10 (direct)T=100 (direct)51711. ACKNOWLEDGMENTS
Nicolas Papernot is supported by a Google PhD Fellow-
ship in Security. Research was also supported in part by the
Army Research Laboratory, under Cooperative Agreement
Number W911NF-13-2-0045 (ARL Cyber Security CRA),
and the Army Research Oﬃce under grant W911NF-13-1-
0421. The views and conclusions contained in this document
are those of the authors and should not be interpreted as
representing the oﬃcial policies, either expressed or implied,
of the Army Research Laboratory or the U.S. Government.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for government purposes notwithstanding
any copyright notation hereon.
A. DNN architectures