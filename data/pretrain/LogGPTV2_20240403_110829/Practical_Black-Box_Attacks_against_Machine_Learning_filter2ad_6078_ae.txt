# Table 3: Misclassification Rates (%) of the Amazon and Google Oracles on Adversarial Samples

| Model | 3 Epochs | 6 Epochs | Queries |
|-------|----------|----------|---------|
| DNN   | 87.44    | 96.78    | 95.68   |
| LR    | 96.19    | 96.43    | 95.83   |
| DNN   | 84.50    | 97.17    | 91.57   |
| LR    | 88.94    | 92.05    | 97.72   |

**Note:** The last row uses a periodic step size and reservoir sampling, which reduces the number of queries.

---

### Experimental Setup

1. **Data Preparation:**
   - Upload the CSV file with the MNIST training data to Google Cloud Storage.
   - Train a model using the Prediction API, specifying only the expected multi-class nature of the model.
   - Evaluate the resulting model on the MNIST test set, achieving an accuracy of 92%.

2. **Substitute Training:**
   - Augment an initial training set of 100 test set samples to train DNN and LR substitutes for each oracle.
   - Measure success as the rate of adversarial samples misclassified by the corresponding oracle, among the 10,000 produced from the test set using the fast gradient sign method with parameter ε = 0.3.
   - Results are reported in Table 3 after ρ ∈ {3, 6} dataset augmentation iterations.

3. **Experimental Results:**
   - The Amazon model, identified as a multinomial logistic regression, is easily misled with a 96.19% misclassification rate for ε = 0.3.
   - The Google model is more robust but still vulnerable, with 88.94% of adversarial samples misclassified.
   - Combining periodic step sizes with reservoir sampling reduces querying while maintaining high transferability. For example, the Amazon DNN substitute's misclassification rate decreases from 96.78% to 95.68% with a reduction in queries from 6,400 to 2,000.

### Defense Strategies

1. **Reactive Defenses:**
   - Detect adversarial examples.
   - Oracle queries may be distributed among colluding users, making detection difficult.

2. **Proactive Defenses:**
   - Increase the model's robustness.
   - Techniques like gradient masking (e.g., using nearest neighbor classifiers) make it difficult to construct adversarial examples directly but are often still vulnerable to attacks on smoothed versions of the same model.

### Adversarial Training

- **Objective:** Inject adversarial examples throughout training to increase robustness.
- **Implementation:** Use the Google Prediction API to inject a large amount of adversarial examples infrequently.
- **Results:** The model has a misclassification rate of 8.75% on the unperturbed test set, but the adversarial misclassification rate rises to 100% when ρ = 6. A correct implementation using local training shows that for ε = 0.15, the defense can be evaded with up to 71.25% misclassification rates. For ε = 0.3, the black-box attack is not effective, indicating that robustness to larger perturbations is more promising.

### Defensive Distillation

- **Objective:** Make the model robust to infinitesimal perturbations.
- **Evaluation:** Train a distilled model at different temperatures T = 5, 10, 100. The fast gradient sign method is successful in black-box settings regardless of the distillation temperature, suggesting that defensive distillation does not prevent our black-box attack.

### Conclusions

- We introduced a novel substitute training algorithm to craft adversarial examples for black-box DNNs.
- Our attack is validated by targeting remote DNNs served by MetaMind, Amazon, and Google, achieving high success rates.
- The attack evades gradient masking defenses, highlighting the need for more robust defense mechanisms.

### References

- [1] Marco Barreno, et al. Can machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security.
- [2] Battista Biggio, et al. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases, pages 387–402. Springer, 2013.
- [3] Ian Goodfellow, et al. Deep learning. Book in preparation for MIT Press (www.deeplearningbook.org), 2016.
- [4] Ian J Goodfellow, et al. Explaining and harnessing adversarial examples. In Proceedings of the International Conference on Learning Representations, 2015.
- [5] Ling Huang, et al. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pages 43–58, 2011.
- [6] Alexey Kurakin, et al. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.
- [7] Yann LeCun et al. The mnist database of handwritten digits, 1998.
- [8] Erich L. Lehmann, et al. Testing Statistical Hypotheses. Springer Texts in Statistics, August 2008.
- [9] Nicolas Papernot, et al. The limitations of deep learning in adversarial settings. In Proceedings of the 1st IEEE European Symposium on Security and Privacy, 2016.
- [10] Nicolas Papernot, et al. Distillation as a defense to adversarial perturbations against deep neural networks. In Proceedings of the 37th IEEE Symposium on Security and Privacy.
- [11] Mahmood Sharif, et al. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016.
- [12] Nedim Srndic, et al. Practical evasion of a learning-based classifier: A case study. In Proceeding of the 35th IEEE Symposium on Security and Privacy.
- [13] Johannes Stallkamp, et al. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural networks, 32:323–332, 2012.
- [14] Christian Szegedy, et al. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations, 2014.
- [15] Florian Tram`er, et al. Stealing machine learning models via prediction APIs. In 25th USENIX Security Symposium, 2016.
- [16] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software, 1985.
- [17] D Warde-Farley, et al. Adversarial perturbations of deep neural networks. Advanced Structured Prediction, 2016.
- [18] Weilin Xu, et al. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium.