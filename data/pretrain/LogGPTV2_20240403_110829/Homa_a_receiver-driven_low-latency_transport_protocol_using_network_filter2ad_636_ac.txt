sender retains the most recent allocations for each receiver (a
few dozen bytes per receiver) and uses that information when
transmitting unscheduled packets. If the receiver’s incoming
traffic changes, it disseminates new priority allocations the next
time it communicates with each sender.
SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
Figure 4: Homa receivers allocate unscheduled priorities based
on traffic patterns. This figure shows the CDF of unscheduled
bytes across messages of different sizes for workload W2; 100%
on the y-axis corresponds to all network traffic, both scheduled
and unscheduled. About 80% of all bytes are unscheduled; Homa
allocates a corresponding fraction of priority levels (6 out of 8)
for unscheduled packets. The CDF is then used to determine the
range of message sizes for each priority level so that traffic is evenly
distributed among them. For example, P7 (the highest priority level)
will be used for unscheduled bytes for messages of length 1–280 bytes.
Figure 5: Preemption lag occurs if a higher priority message uses
the same priority level as an existing lower priority message. Packets
arrive at the top from the aggregation switch, pass through the TOR
priority queues, and are transmitted to the receiver at the bottom.
The notation “m1-S3” refers to a scheduled packet for message m1
with priority 3; “m2-U4” refers to an unscheduled packet for message
m2 with priority 4. RTTbytes corresponds to 4 packets. In (a) the first
unscheduled packet for m2 arrives at the TOR during an ongoing
transmission of scheduled packets for m1. Unscheduled packets have
higher priority than scheduled packets, so m1’s scheduled packets
will be buffered; (b) shows the state as the last unscheduled packet
for m2 is being sent to the receiver. If scheduled packets for m2 also
use priority level 3, they will be queued behind the buffered packets
for m1 as shown in (c). If the receiver assigns a higher priority level
for m2’s scheduled packets, it avoids preemption lag.
Homa allocates priorities for unscheduled packets so that
each priority level is used for about the same number of bytes.
Each receiver records statistics about the sizes of its incoming
messages and uses the message size distribution to compute pri-
ority levels as illustrated in Figure 4. The receiver first computes
the fraction of all incoming bytes that are unscheduled (about
80% in Figure 4). It allocates this fraction of the available pri-
orities (the highest ones) for unscheduled packets, and reserves
the remaining (lower) priority levels for scheduled packets. The
receiver then chooses cutoffs between the unscheduled prior-
ities so that each priority level is used for an equal number of
unscheduled bytes and shorter messages use higher priorities.
Figure 6: Bandwidth can be wasted if a receiver grants to only a single
sender at a time. In this example, S1 has messages ready to send to R1
and R2 while S2 also has a message for R1. If R1 grants to only one
message at a time, it will choose m1, which is shorter than m3. How-
ever, S1 will choose to transmit m2, since it is shorter than m1. As a
result, R1’s downlink will be idle even though it could be used for m3.
For scheduled packets, the receiver specifies a priority in
each GRANT packet, and the sender uses that priority for the
granted bytes. This allows the receiver to dynamically adjust the
priority allocation based on the precise set of messages being
received; this produces a better approximation to SRPT than
approaches such as PIAS, where priorities are set by senders
based on historical trends. The receiver uses a different priority
level for each message, with higher priorities used for messages
with fewer ungranted bytes. If there are more incoming mes-
sages than priority levels, only the highest priority messages
are granted, as described in §3.5. If there are fewer messages
than scheduled priority levels, then Homa uses the lowest of
the available priorities; this leaves higher priority levels free for
new higher priority messages. If Homa always used the highest
scheduled priorities, it would result in preemption lag: when
a new higher priority message arrived, its scheduled packets
would be delayed by 1 RTT because of buffered packets from
the previous high priority message (see Figure 5). Using the
lowest scheduled priorities eliminates preemption lag except
when all scheduled priorities are in use.
3.5 Overcommitment
One of the important design decisions for Homa is how many
incoming messages a receiver should allow at any given time.
A receiver can stop transmission of a message by withhold-
ing grants; once all of the previously-granted data arrives, the
sender will not transmit any more data for that message until the
receiver starts sending grants again. We use the term active to
describe the messages for which the receiver is willing to send
grants; the others are inactive.
One possible approach is to keep all incoming messages ac-
tive at all times. This is the approach used by TCP and most other
existing protocols. However, this approach results in high buffer
occupancy and round-robin scheduling between messages, both
of which contribute to high tail latency.
In our initial design for Homa, each receiver allowed only
one active message at a time, like pHost. If a receiver had mul-
tiple partially-received incoming messages, it sent grants only
to the highest priority of these; once it had granted all of the
bytes of the highest priority message, it began granting to the
next highest priority message, and so on. The reasoning for this
approach was to minimize buffer occupancy and to implement
run-to-completion rather than round-robin scheduling.
100806020400110105100001000100107106Message Size (Bytes)Cumulative % of All BytesP2P3P4P5P6P7P2P3P7P6P5P4Scheduled packetsuse P0 and P1UnscheduledBytes(a)P3P4m1-S3m1-S3m1-S3m1-S3P3P4m1-S3m1-S3m1-S3m2-S3P3P4m1-S3m2-U4m1-S3m2-U4m1-S3m2-S3(b)(c)m2-S3R1R2S1S2m1m3m2Homa: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
Our simulations showed that allowing only one active mes-
sage resulted in poor network utilization under high load. For
example, with workload W4 from Figure 1, Homa could not use
more than about 63% of the network bandwidth, regardless of of-
fered load. The network was underutilized because senders did
not always respond immediately to grants; this caused downlink
bandwidth to be wasted. Figure 6 illustrates how this can happen.
There is no way for a receiver to know whether a particular
sender will respond to grants, so the only way to keep the down-
link fully utilized is to overcommit: a receiver must grant to more
than one sender at a time, even though its downlink can only
support one of the transmissions at a time. With this approach, if
one sender does not respond, then the downlink can be used for
some other sender. If many senders respond at once, the priority
mechanism ensures that the shortest message is delivered first;
packets from the other messages will be buffered in the TOR.
We use the term degree of overcommitment to refer to the
maximum number of messages that may be active at once on
a given receiver. If there are more than this many messages
available, only the highest priority ones are active. A higher
degree of overcommitment reduces the likelihood of wasted
bandwidth, but it consumes more buffer space in the TOR (up
to RTTbytes for each active message) and it can result in more
round-robin scheduling between messages, which increases
average completion time.
Homa currently sets the degree of overcommitment to the
number of scheduled priority levels: a receiver will grant to
at most one message for each available priority level. This ap-
proach resulted in high network utilization in our simulations,
but there are other plausible approaches. For example, a receiver
might use a fixed degree of overcommitment, independent of
available priority levels (if necessary, several messages could
share the lowest priority level); or, it might adjust the degree of
overcommitment dynamically based on sender response rates.
We leave an exploration of these alternatives to future work.
Incast
The need for overcommitment provides another illustration
why it isn’t practical to completely eliminate buffering in a trans-
port protocol. Homa introduces just enough buffering to ensure
good link utilization; it then uses priorities to make sure that the
buffering doesn’t impact latency.
3.6
Homa solves the incast problem by taking advantage of the
fact that incast is usually self-inflicted: it occurs when a node
issues many concurrent RPCs to other nodes, all of which return
their results at the same time. Homa detects impending incasts
by counting each node’s outstanding RPCs. Once this number
exceeds a threshold, new RPCs are marked with a special flag
that causes the server to use a lower limit for unscheduled bytes
in the response message (a few hundred bytes). Small responses
will still get through quickly, but larger responses will be sched-
uled by the receiver; the overcommitment mechanism will limit
buffer usage. With this approach, a 1000-fold incast will con-
sume at most a few hundred thousand bytes of buffer space in
the TOR.
Incast can also occur in ways that are not predictable; for ex-
ample, several machines might simultaneously decide to issue
requests to a single server. However, it is unlikely that many
such requests will synchronize tightly enough to cause incast
problems. If this should occur, Homa’s efficient use of buffer
space still allows it to support hundreds of simultaneous arrivals
without packet loss (see Section 5.1).
Incast is largely a consequence of the high latency in cur-
rent datacenters. If each request results in a disk I/O that takes
10 ms, a client can issue 1000 or more requests before the first re-
sponse arrives, resulting in massive incast. In future low-latency
environments, incast will be less of an issue because requests
will complete before very many have been issued. For exam-
ple, in the RAMCloud main-memory storage system [24], the
end-to-end round-trip time for a read request is about 5µs. In a
multiread request, it takes the client 1–2µs to issue each request
for a different server; by the time it has issued 3–4 RPCs, re-
sponses from the first requests have begun to arrive. Thus there
are rarely more than a few outstanding requests.
3.7 Lost packets
We expect lost packets to be rare in Homa. There are two reasons
for packet loss: corruption in the network, and buffer overflow.
Corruption is extremely rare in modern datacenter networks, and
Homa reduces buffer usage enough to make buffer overflows
extremely uncommon as well. Since packets are almost never
lost, Homa optimizes lost-packet handling for efficiency in the
common case where packets are not lost, and for simplicity
when packets are lost.
In TCP, senders are responsible for detecting lost packets.
This approach requires acknowledgment packets, which add
overhead to the protocol (the simplest RPC requires two data
packets and two acknowledgments). In Homa, lost packets are
detected by receivers; as a result, Homa does not use any ex-
plicit acknowledgments. This eliminates half of the packets for
simple RPCs. Receivers use a simple timeout-based mechanism
to detect lost packets. If a long time period (a few milliseconds)
elapses without additional packets arriving for a message, the
receiver sends a RESEND packet that identifies the first range
of missing bytes; the sender will then retransmit those bytes.
If all of the initial packets of an RPC request are lost, the server
will not know about the message, so it won’t issue RESENDs.
However, the client will timeout on the response message, and
it will send a RESEND for the response (it does this even if
the request has not been fully transmitted). When the server
receives a RESEND for a response with an unknown RPCid,
it assumes that the request message must have been lost and it
sends a RESEND for the first RTTbytes of the request.
If a client receives no response to a RESEND (because of
server or network failures), it retries the RESEND several times
and eventually aborts the RPC, returning an error to higher level
software.
3.8 At-least-once semantics
RPC protocols have traditionally implemented at most once
semantics, where each RPC is executed exactly once in the
SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
normal case; in the event of an error, an RPC may be executed
either once or not at all. Homa allows RPCs to be executed more
than once: in the normal case, an RPC is executed one or more
times; after an error, it could have been executed any number
of times (including zero). There are two situations where Homa
re-executes RPCs. First, Homa doesn’t keep connection state, so
if a duplicate request packet arrives after the server has already
processed the original request and discarded its state, Homa will
re-execute the operation. Second, servers get no acknowledg-
ment that a response was received, so there is no obvious time
at which it is safe to discard the response. Since lost packets
are rare, servers take the simplest approach and discard all state
for an RPC as soon as they have transmitted the last response
packet. If a response packet is lost, the server may receive the
RESEND after it has deleted the RPC state. In this case, it will
behave as if it never received the request and issue a RESEND
for the request; this will result in re-execution of the RPC.
Homa allows re-executions because it simplifies the im-
plementation and allows servers to discard all state for inac-
tive clients (at-most-once semantics requires servers to retain
enough state for each client to detect duplicate requests). More-
over, duplicate suppression at the transport level is insufficient
for most datacenter applications. For example, consider a repli-
cated storage system: if a particular replica crashes while exe-
cuting a client’s request, the client will retry that request with a
different replica. However, it is possible that the original replica
completed the operation before it crashed. As a result, the crash
recovery mechanism may result in re-execution of a request,
even if the transport implements at-most-once semantics. Du-
plicates must be filtered at a level above the transport layer.
IMPLEMENTATION
Homa assumes that higher level software will either tolerate
redundant executions of RPCs or filter them out. The filtering
can be done either with application-specific mechanisms, or
with general-purpose mechanisms such as RIFL [19]. For ex-
ample, a TCP-like streaming mechanism can be implemented
as a very thin layer on top of Homa that discards duplicate data
and preserves order.
4
We implemented Homa as a new transport in the RAMCloud
main-memory storage system [24]. RAMCloud supports a vari-
ety of transports that use different networking technologies, and
it has a highly tuned software stack: the total software overhead
to send or receive an RPC is 1–2 µs in most transports. The
Homa transport is based on DPDK [9], which allows it to by-
pass the kernel and communicate directly with the NIC; Homa
detects incoming packets with polling rather than interrupts.
The Homa implementation contains a total of 3660 lines of C++
code, of which about half are comments.
The RAMCloud implementation of Homa includes all of the
features described in this paper except that it does not yet mea-
sure incoming message lengths on the fly (the priorities were
precomputed based on knowledge of the benchmark workload).