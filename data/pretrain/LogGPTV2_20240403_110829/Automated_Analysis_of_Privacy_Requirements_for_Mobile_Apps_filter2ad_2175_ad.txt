### Table II: Classifiers, Parameters, and Classification Results

| Metric | NPC | CC | SL | SID | SC | Base (n=40) |
| --- | --- | --- | --- | --- | --- | --- |
| F-1neg (n=40) | 0.67–0.93 | 0.73–0.96 | 0.73–0.96 | 0.80–0.98 | 0.62–0.89 | - |
| Precneg (n=40) | 0.79 | 0.71 | 0.77 | 0.83 | 0.71 | 0.94, 0.97, 0.79 |
| Recneg (n=40) | 0.92 | 0.71 | 0.71 | 0.95 | 0.63 | 0.91, 0.95, 0.93 |
| F-1pos (n=40) | 0.85 | 0.71 | 0.74 | 0.89 | 0.67 | 0.93, 0.96, 0.86 |
| F-1pos (n=40) | 0.93 | 0.78 | 0.87 | 0.86 | 0.92 | 0.55, -, 0.47 |
| Pos (n=9,050) | 46% | 36% | 46% | 34% | 56% | 10%, 12%, 6% |

**Table II: Classifiers, parameters, and classification results for the policy test set (n=40) and the occurrence of positive classifications (Pos) in a set of n=9,050 policies (full app/policy set). We obtained the best results by always setting the regularization constant to C = 1 and for NPC, CC, and SL, adjusting weights inversely proportional to class frequencies with scikit-learn’s class_weight (weight). Except for the SL practice, all classifiers' accuracies (Accpol) reached or exceeded the baseline (Base) of always selecting the most often occurring class in the training set. P recneg, Recneg, and F-1neg are the scores for the negative classes (e.g., data is not collected or shared) while F-1pos is the F-1 score for positive classes.**

### Feature Extraction for Location Sharing Practice

```python
def location_feature_extraction(policy):
    data_type_keywords = ['geo', 'gps']
    action_keywords = ['share', 'partner']
    relevant_sentences = ''
    feature_vector = []

    for sentence in policy:
        for keyword in data_type_keywords:
            if keyword in sentence:
                relevant_sentences += sentence
                words = tokenize(relevant_sentences)
                bigrams = ngrams(words, 2)
                for bigram in bigrams:
                    for keyword in action_keywords:
                        if keyword in bigram:
                            feature_vector.append(bigram)

    return feature_vector
```

**Listing 1: Pseudocode for the location sharing practice.**

### Impact of Annotation Reliability

The reliability of our annotations had a minimal effect on the classifier performance. For some practices, the classification accuracy slightly increased, while for others, it slightly decreased. This suggests that our annotations are sufficiently reliable to serve as ground truth for our classifiers. As noted in other works, low levels of agreement in policy annotations are common and do not necessarily reflect unreliability [56], [72]. In fact, an annotator's addition or omission of an annotation does not necessarily indicate disagreement with other annotations.

### Feature Selection

One of the most important tasks for correctly classifying data practices described in privacy policies is appropriate feature selection. Listing 1 shows a simplified example of our algorithm for the location sharing practice. Using information gain and tf-idf, we identified the most meaningful keywords for each practice and created sets of keywords. One set of keywords refers to the data type of the practices (e.g., for the location sharing practice: 'geo' and 'gps') and is used to extract all sentences from a policy that contain at least one of the keywords. On these extracted sentences, we use a second set of keywords that refers to the actions of a data practice (e.g., for the location sharing practice: 'share' and 'partner') to create unigram and bigram feature vectors [72]. For example, if the keyword "share" is encountered, the bigrams "not share" or "will share" would be extracted if the words before "share" are "not" and "will," respectively. The feature vectors created from bigrams (and unigrams) are then used to classify the practices. If no keywords are extracted, the classifier will select the negative class. We applied the Porter stemmer to all processed text.

### Cross-Validation and Tuning

For finding the most meaningful features and subsequent classifier tuning, we performed nested cross-validation with 75 policies separated into ten folds in the inner loop and 40 randomly selected policies as the held-out test set (policy test set). We used the inner cross-validation to select the optimal parameters during the classifier tuning phase and the held-out policy test set for the final measure of classification performance. We stratified the inner cross-validation to avoid misclassifications due to skewed classes. After evaluating the performance of our classifiers with the policy test set, we added the test data to the training data for the final classifiers to be used in our large-scale analysis.

### Classifier Performance

During the tuning phase, we prototyped various classifiers with scikit-learn [51], a Python library. Support vector machines and logistic regression had the best performance. We selected classification parameters individually for each data practice.

#### Policy Test Set Results

The classification results for our policy test set, shown in Table II, suggest that the ML analysis of privacy policies is generally feasible. For the negative classifications, our classifiers achieve F-1neg scores between 0.67 and 0.96. These scores are the most important measures for our task because the identification of a potential privacy requirement inconsistency demands that a practice occurring in an app is not covered by its policy (§ V-A). Consequently, it is less problematic that the sharing practices, which are skewed towards the negative classes, have relatively low F-1pos scores of 0.55 (SID) and 0.47 (SC) or could not be calculated (SL) due to a lack of true positives in the policy test set.

#### Full App/Policy Set Results

We applied our classifiers to the policies in the full app/policy set with n = 9,050 policies. We obtained this set by adjusting our full policy set (n = 9,295) to account for the fact that not every policy link might actually lead to a policy. For 40 randomly selected apps from our full policy set, we checked whether the policy link in fact led to a policy, which was the case for 97.5% (39/40) of links (with a CI of 0.87 to 1 at the 95% level). As the other 2.5%, that is, one link, led to some other page and would not contain any data practice descriptions, we randomly excluded from the full policy set 2.5% = 245 of policies without any data practice descriptions, leaving us with n = 9,295 − 245 = 9,050 policies in the full app/policy set. We emphasize that this technique does not allow us to determine whether the 245 documents actually did not contain a policy or had a policy that did not describe any practices. However, in any case, the adjustment increases the occurrence of positive data practice instances in the full app/policy set, keeping discrepancies between apps and policies at a conservative level, as some apps for which the analysis did not find any data practice descriptions are now excluded.

It appears that many privacy policies fail to satisfy privacy requirements. Most notably, per Table II, only 46% describe the notification process for policy changes, a mandatory requirement for apps under California and Delaware law. Similarly, only 36% of policies contain a statement on user access, edit, and deletion rights, which COPPA requires for children's apps, that is, apps intended for children or known to be used by children. For the sharing practices, we expected more policies to engage in the SID, SL, and SC practices. The respective 10%, 12%, and 6% are rather small percentages for a presumably widely occurring practice, especially given our focus on policies of free apps that often rely on targeted advertising.

### Runtime Performance and Failure Rate

The analysis of all practices for the policies in the full app/policy set required about half an hour in total, running ten threads in parallel on an Amazon Web Services (AWS) EC2 instance m4.4xlarge with 2.4 GHz Intel Xeon E5-2676 v3 (Haswell), 16 vCPU, and 64 GiB memory [2]. The feature extraction took up the majority of time, and the training and classification finished in about one minute. There was no failure in extracting policy features or analyzing policies.

### Mobile App Analysis

To compare our policy analysis results to what apps actually appear to do, we now discuss our app analysis approach. We begin with our system design (§ IV-A) and follow up with the system's analysis results (§ IV-B).

#### App Analysis System Design

Our app analysis system is based on Androguard [20], an open-source static analysis tool written in Python that provides extensible analytical functionality. Apart from the manual intervention in the construction and testing phase, our system's analysis is fully automated. Figure 5 shows a sketch of our system architecture. A brief example for sharing of device IDs will convey the basic program flow of our data-driven static analysis.

For each app, our system builds an API invocation map, which is utilized as a partial call graph (call graph creation). To illustrate, for sharing of device IDs, all calls to the `android.telephony.TelephonyManager.getDeviceId` API are included in the call graph because the caller can use it to request a device ID. All calls to this and other APIs for requesting a device ID are added to the graph and passed to the identification routine (call ID analysis), which checks the package names of the callers against the package names of third-party libraries to detect sharing of data. We focus on a set of ten popular libraries, which are listed in Table III. In order to make use of the `getDeviceId` API, a library needs the `READ_PHONE_STATE` permission. Only if the analysis detects that the library has the required permission, the app is classified as sharing device IDs with third parties. We identified relevant Android API calls for the types of information we are interested in and the permission each call requires by using PScout [6].

Our static analysis is informed by a manual evaluation of Android and third-party APIs. Because sharing of data most often occurs through third-party libraries [23], we can leverage the insight that the observation of data sharing for a given library allows extension of that result to all apps using that library.

#### Table III: Ad and Analytics Libraries

| 3rd Party Library |
| --- |
| Crashlytics/Fabric |
| Crittercism/Apteligent |
| Flurry Analytics |
| Google Analytics |
| Umeng |
| AdMob* |
| InMobi* |
| MoPub* |
| MillennialMedia* |
| StartApp* |

**Table III: Ad and analytics libraries.**

### App Analysis Results

| Practice | CID | CL | CC | SID | SL | SC | Base (n=40) |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Accapp (n=40) | 0.8 | 0.55 | 0.78 | 0.68 | 0.93 | 0.98 | 0.9 |
| 95% CI (n=40) | 0.76–0.97 | 0.64–0.91 | 0.91–1 | 0.83–0.99 | 0.91–1 | 0.91–1 | - |
| Precpos (n=40) | 0.89 | 0.73 | 1 | 1 | 1 | 1 | - |
| Recpos (n=40) | 1 | 1 | 1 | 0.93 | 1 | 1 | - |
| F-1pos (n=40) | 0.94 | 0.85 | 1 | 0.96 | 1 | 1 | - |
| F-1neg (n=40) | 0.67 | 0.71 | 1 | 0.93 | 1 | 1 | - |

**Table: App analysis results for the policy test set (n=40).**