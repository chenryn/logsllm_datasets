0.67–0.93
0.73–0.96
0.73–0.96
0.73–0.96
0.8–0.98
0.62–0.89
Precneg
(n=40)
0.79
0.71
0.77
0.83
0.71
0.94
0.97
0.79
Recneg
(n=40)
0.92
0.71
0.71
0.95
0.63
0.91
0.95
0.93
F-1neg
(n=40)
0.85
0.71
0.74
0.89
0.67
0.93
0.96
0.86
F-1pos
(n=40)
0.93
0.78
0.87
0.86
0.92
0.55
-
0.47
Pos
(n=9,050)
46%
36%
46%
34%
56%
10%
12%
6%
TABLE II: Classiﬁers, parameters, and classiﬁcation results for the policy test set (n=40) and the occurrence of positive
classiﬁcations (Pos) in a set of n=9,050 policies (full app/policy set). We obtained the best results by always setting the
regularization constant to C = 1 and for NPC, CC, and SL adjusting weights inversely proportional to class frequencies
with scikit-learn’s class_weight (weight). Except for the SL practice, all classiﬁers’ accuracies (Accpol) reached or exceeded
the baseline (Base) of always selecting the most often occurring class in the training set. P recneg, Recneg, and F-1neg are the
scores for the negative classes (e.g., data is not collected or shared) while F-1pos is the F-1 score for positive classes.
1 def location_feature_extraction(policy):
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
data_type_keywords = [’geo’, ’gps’]
action_keywords = [’share’, ’partner’]
relevant_sentences = ’’
feature_vector = ’’
for sentence in policy:
for keyword in data_type_keywords:
if (keyword in sentence):
relevant_sentences += sentence
words = tokenize(relevant_sentences)
bigrams = ngrams(words,2)
for bigram in bigrams:
for keyword in action_keywords:
if (keyword in bigram):
feature_vector += bigram, bigram[0],
bigram[1]
return feature_vector
Listing 1: Pseudocode for the location sharing practice.
set for our classiﬁers had only little noticeable effect. For
some practices the classiﬁcation accuracy slightly increased,
for others it slightly decreased. Thus, we believe that our
annotations are sufﬁciently reliable to serve as ground-truth
for our classiﬁers. As other works have already explored, low
levels of agreement in policy annotations are common and do
not necessarily reﬂect their unreliability [56], [72]. In fact,
different from our approach here, it could be argued that an
annotator’s addition or omission of an annotation is not a
disagreement with the others’ annotations to begin with.
important
tasks
for correctly classifying data practices described in privacy
policies is appropriate feature selection. Listing 1 shows a
simpliﬁed example of our algorithm for the location sharing
practice. Using information gain and tf-idf we identiﬁed the
most meaningful keywords for each practice and created sets
of keywords. One set of keywords refers to the data type of the
practices (e.g., for the location sharing practice geo and gps)
and is used to extract all sentences from a policy that contain at
least one of the keywords. On these extracted sentences we are
2) Feature Selection: One of the most
using a second set of keywords that refers to the actions of a
data practice (e.g., for the location sharing practice share and
partner) to create unigram and bigram feature vectors [72].
Thus, for example, if the keyword “share” is encountered, the
bigrams “not share” or “will share” would be extracted if the
words before “share” are “not” and “will,” respectively. The
feature vectors created from bigrams (and unigrams) are then
used to classify the practices. If no keywords are extracted, the
classiﬁer will select the negative class. We applied the Porter
stemmer to all processed text.
For ﬁnding the most meaningful features as well as for
the subsequent classiﬁer tuning we performed nested cross-
validation with 75 policies separated into ten folds in the inner
loop and 40 randomly selected policies as held out test set
(policy test set). We used the inner cross-validation to select
the optimal parameters during the classiﬁer tuning phase and
the held out policy test set for the ﬁnal measure of classiﬁcation
performance. We stratiﬁed the inner cross-validation to avoid
misclassiﬁcations due to skewed classes. After evaluating the
performance of our classiﬁers with the policy test set we added
the test data to the training data for the ﬁnal classiﬁers to be
used in our large-scale analysis.
3) Classiﬁcation: During the tuning phase we prototyped
various classiﬁers with scikit-learn [51], a Python library.
Support vector machines and logistic regression had the best
performance. We selected classiﬁcation parameters individu-
ally for each data practice.
Classiﬁer Performance for Policy Test Set. The classiﬁcation
results for our policy test set, shown in Table II, suggest that the
ML analysis of privacy policies is generally feasible. For the
negative classiﬁcations our classiﬁers achieve F-1neg scores
between 0.67 and 0.96. These scores are the most important
measures for our task because the identiﬁcation of a potential
privacy requirement
inconsistency demands that a practice
occurring in an app is not covered by its policy (§ V-A).
Consequently, it is less problematic that the sharing practices,
which are skewed towards the negative classes, have relatively
low F-1pos scores of 0.55 (SID) and 0.47 (SC) or could not
be calculated (SL) due to a lack of true positives in the policy
test set.
Classiﬁcation Results for Full App/Policy Set. We applied
our classiﬁers to the policies in the full app/policy set with
7
Fig. 5: (1) Our system ﬁrst crawls the US Google Play store for free apps. (2) Then, it performs for each app a static analysis.
Speciﬁcally, it applies permission extraction, call graph creation, and call ID analysis, the latter of which is based on Android
system and third party APIs. (3) Finally, results for the collection and sharing practices are generated and stored.
n = 9, 050 policies. We obtained this set by adjusting our
full policy set (n = 9, 295) to account for the fact
that
not every policy link might actually lead to a policy: for 40
randomly selected apps from our full policy set we checked
whether the policy link in fact lead to a policy, which was
the case for 97.5% (39/40) of links (with a CI of 0.87 to 1
at the 95% level). As the other 2.5%, that is, one link, lead
to some other page and would not contain any data practice
descriptions, we randomly excluded from the full policy set
2.5% = 245 of policies without any data practice descriptions
leaving us with n = 9, 295 − 245 = 9, 050 policies in the
full app/policy set. We emphasize that this technique does not
allow us to determine whether the 245 documents actually
did not contain a policy or had a policy that did not describe
any practices. However, in any case the adjustment increases
the occurrence of positive data practice instances in the full
app/policy set keeping discrepancies between apps and policies
at a conservative level as some apps for which the analysis did
not ﬁnd any data practice descriptions are now excluded.22
It appears that many privacy policies fail to satisfy privacy
requirements. Most notably, per Table II, only 46% describe
the notiﬁcation process for policy changes, a mandatory
requirement for apps under California and Delaware law.
Similarly, only 36% of policies contain a statement on user
access, edit, and deletion rights, which COPPA requires for
childrens’ apps, that is, apps intended for children or known
to be used by children. For the sharing practices we expected
more policies to engage in the SID, SL, and SC practices. The
respective 10%, 12%, and 6% are rather small percentages
for a presumably widely occurring practice, especially, given
our focus on policies of free apps that often rely on targeted
advertising.
Runtime Performance and Failure Rate. The analysis of
all practices for the policies in the full app/policy set required
about half an hour in total running ten threads in parallel on an
Amazon Web Services (AWS) EC2 instance m4.4xlarge with
2.4 GHz Intel Xeon E5-2676 v3 (Haswell), 16 vCPU, and 64
GiB memory [2]. The feature extraction took up the majority
of time and the training and classiﬁcation ﬁnished in about
one minute. There was no failure in extracting policy features
or analyzing policies.
22We also checked the random sample of 40 apps for policies dynamically loaded via
JavaScript because for such policies the feature extraction would fail. We had observed
such dynamic loading before. However, as neither of the policies in the sample was
loaded dynamically, we do not make an adjustment in this regard.
IV. MOBILE APP ANALYSIS
In order to compare our policy analysis results to what
apps actually appear to do we now discuss our app analysis
approach. We begin with our system design (§ IV-A) and
follow up with the system’s analysis results (§ IV-B).
A. App Analysis System Design
Our app analysis system is based on Androguard [20],
an open source static analysis tool written in Python that
provides extensible analytical functionality. Apart from the
manual intervention in the construction and testing phase, our
system’s analysis is fully automated. Figure 5 shows a sketch
of our system architecture. A brief example for sharing of
device IDs will convey the basic program ﬂow of our data-
driven static analysis.
For each app our system builds an API invocation map,
which is utilized as a partial call graph (call graph creation).
To illustrate,
for sharing of device IDs all calls to the
android.telephony.TelephonyManager.getDeviceId
API are included in the call graph because the caller can
use it to request a device ID. All calls to this and other
APIs for requesting a device ID are added to the graph and
passed to the identiﬁcation routine (call ID analysis), which
checks the package names of the callers against the package
names of third party libraries to detect sharing of data. We
focus on a set of ten popular libraries, which are listed in
Table III.23 In order to make use of the getDeviceId API a
library needs the READ_PHONE_STATE permission. Only if the
analysis detects that the library has the required permission
(permission extraction), the app is classiﬁed as sharing device
IDs with third parties.24 We identiﬁed relevant Android API
calls for the types of information we are interested in and the
permission each call requires by using PScout [6].
Our static analysis is informed by a manual evaluation
of Android and third party APIs. Because sharing of data
most often occurs through third party libraries [23], we can
leverage the insight that the observation of data sharing for
a given library allows extension of that result to all apps
23The limitation on a small set of libraries allows us to manually analyze the library
functions freeing us from using resource-intensive data ﬂow analysis techniques in our
app analysis. However, in principle, it is possible to include more libraries.
24Android’s permission model as of Android 6.0 does not distinguish between
permissions for an app and permissions for the app’s libraries, which, thus, can request
all permissions of the app.
8
3rd Party Library
Crashlytics/Fabric
Crittercism/Aptel.
Flurry Analytics
Google Analytics
Umeng
AdMob*
InMobi*
MoPub*
MillennialMedia*
StartApp*
TABLE III: Ad*
and analytics
li-
braries.
Pract
CID
CL
CC
SID
SL
SC
Base
(n=40)
Accapp
(n=40)
0.8
0.55
0.78
0.68
0.93
0.98
0.9
0.8
1
0.95
1
1
95% CI
(n=40)
0.76–0.97
0.64–0.91
0.91–1
0.83–0.99
0.91–1
0.91–1
Precpos
(n=40)
0.89
0.73
1
1
1
1
Recpos
(n=40)
1
1
1
0.93
1
1
F-1pos
(n=40)
0.94
0.85
1
0.96
1
1
F-1neg
(n=40)
0.67
0.71
1
0.93
1