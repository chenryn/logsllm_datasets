the best sample, based on
possible quality metrics. Second, we deliberately leave some
sentences non-watermarked. Preserving utility has a trade-
off relationship with veriﬁcation conﬁdence and bit accuracy,
which we discuss in Sections V-B4 and V-B5.
a) Best-of-many encoding: We here sample n sentences
for each input sentence using the Gumbel sampler in the
autoencoder network. We then use the trained language model
(AWD-LSTM) to compute the likelihood for each output
sample. Then, we pick the sample with the highest likelihood
(excluding samples with no changes to the input) and feed
it to the message decoder. An alternative quality metric is to
pick the sample with the lowest SBERT distance to the input
sentence, we found that these two metrics give comparable
results, however, using the language model gives slightly better
samples in terms of grammatical and syntactic correctness
(discussed in Section V-B6 and Appendix VIII-A).
Fig. 3: Different operating points from selective and best-of-
many sampling encoding.
(bit accuracy 97%), and selecting the best from 30 samples
(bit accuracy ∼85%). In the latter case, the output is moving
towards identical reconstruction. This analysis suggests that
higher-quality output sentences can be acquired by sampling
and that the language model metric also correlates with the
meteor and SBERT ones.
b) Selective encoding: Alternatively,
to provide fur-
ther ﬂexibility, we leave a percentage of sentences non-
watermarked to reduce the overall change to the output text.
The message decoder side does not need to know which
sentences were watermarked as it can attempt to decode the
message from all sentences in a document. The matching
accuracy of non-watermarked sentences approximates the ran-
dom chance while watermarked sentences will have a strong
matching (we use the 1-sample output in Table I). We can
then base the decision on the matching of the whole decoded
sequence of messages (i.e., using null-hypothesis testing as
we show in Section V-B4). We decide which sentences to
leave based on setting a threshold on the increase of the
language model loss compared to the original sentence. We
examine different thresholds that encode different quantiles of
the test set sentences (from 75% to 100%). We perform this
experiment by sampling only 1 sample from the model. We
show in Figure 3 the mean meteor and SBERT distance versus
bit accuracy at each quantile. Besides the ﬂexibility and utility
advantage, selective encoding hinders the adversary effort to
localize the watermark as not all sentences are watermarked.
4) Watermark veriﬁcation by sentence aggregation: The
previous strategies help to improve the output’s quality. How-
ever, they reduce the bit accuracy. Therefore, in this section,
we discuss the relationship between the veriﬁcation conﬁdence
and bit accuracy at different input lengths.
We show in Figure 3 different operating points based on
varying n from 1 to 40 samples. For each point, we show
the relationship between bit accuracy and text utility (demon-
strated by the averaged meteor score and SBERT distance). We
found that the meteor score increases and the SBERT distance
decreases with increasing the number of samples. Additionally,
we show in Figure 4 a histogram of the SBERT distances
and meteor scores for two sampling settings; only 1 sample
3Unless mentioned otherwise, all the following experiments are performed
on the ﬁne-tuned model, and AWT stands for the full model.
Model
AWT
− ﬁne-tuning
− discriminator
Bit accuracy
97.04%±0.16
95.13%±0.21
96.15%±0.22
Meteor
SBERT distance
0.962±0.0003
0.943±0.0005
0.938±0.0006
1.26±0.008
1.73±0.015
2.29±0.016
TABLE I: Model’s variants quantitative analysis. The ﬁrst row
is the full model, the second row is without ﬁne-tuning, the
third row is without ﬁne-tuning or a discriminator.
(a) SBERT distance
(b) Meteor
Fig. 4: Histograms of (a) SBERT distances (lower is better),
and (b) meteor scores (higher is better) for 2 sampling settings.
82.585.087.590.092.595.097.5Bit Accuracy (%)0.91.01.11.2SBERT distanceSBERT - SelectiveSBERT - Best sample0.96250.96500.96750.97000.97250.97500.9775Meteor scoreSelective and best-of-many encodingMeteor - SelectiveMeteor - Best sample024SBERT distance025050075010001 samplebest of 30 samples0.850.900.951.00Meteor01000200030001 samplebest of 30 samplesTo allow a large number of watermarks and support an
article-level watermarking, a longer watermark can be com-
posed of multipliers of 4 bits messages; each 4 bits are
embedded into one text segment. If the total text length is
longer than the watermark, the long watermark sequence can
be repeated partially or fully. The length of the unique long
watermark can be determined based on the expected minimum
text length. The decoded messages can be then veriﬁed against
the sequence. Thus, we accumulate observations from all
messages in the document to perform a null hypothesis test
based on the number of matching bits [84]. We assume that the
null hypothesis (H0) is getting this number of matching bits by
chance. Under the null hypothesis, the probability of matching
bits (random variable X) follows a binomial distribution; the
number of trials is the number of bits in the sequence (n), k
is the number of successes (matching bits), and each bit has
a 0.5 probability of success. We then compute the p-value of
the hypothesis test by computing the probability of getting k
or higher matching bits under the null hypothesis:
(cid:18)n
(cid:19)
0.5n
P r(X > k|H0) =
n(cid:88)
i
i=k
The watermark is veriﬁed if the p-value is smaller than
a threshold T ; meaning that
is not very likely to get
this sequence by chance. This allows a soft matching of the
decoded watermark instead of an exact one. We evaluate the
thresholds of 0.05 and 0.01 [84].
it
We empirically ﬁnd the percentage of instances where the
null hypothesis can be rejected (i.e., the watermark is correctly
veriﬁed), and its relationship with the text length (i.e., the
number of bits in the sequence). We perform this at different
operating points that vary in their bit accuracy. We demonstrate
this experiment in Figure 5; when increasing the text length,
we observe more correct observations, and thus, can reject
the null hypothesis. Therefore, the use of operating points
can be ﬂexibly determined by the expected text length; at
longer lengths, it is affordable to use an operating point with
lower bit accuracy (i.e., higher utility). We validate that the bit
accuracy is close to chance level (49.9%) when the input is
non-watermarked (real) text, which results, naturally, in high
p-values (and low false-positive rates).
5) Decoding by averaging: We here aim to improve the
bit accuracy of the best-of-many samples encoding strategy,
Fig. 5: Percentage of instances where the null hypothesis
(no watermarking) is rejected (for 0.05 and 0.01 p-value
thresholds) versus text and bit lengths (words/bits), done for
different operating points (i.e., bit accuracy), and real text.
Fig. 6: Bit accuracy for 4 sampling operating points when
averaging the posterior probabilities of multiple sentences
encoded with the same message.
this can be needed in applications where one is interested
in decoding the message itself, rather than watermarking by
concatenating segments from the whole document. We encode
multiple text segments/sentences with the same binary mes-
sage, decode each sentence independently, and then average
their posterior probabilities. We demonstrate in Figure 6 the
performance gain when averaging up to 4 sentences, compared
to using only 1 sentence. We perform this analysis for 4
different operating points that vary in the number of samples.
As can be observed, using only 2 sentences can increase the
bit accuracy for all operating points. Increasing the number of
sentences can still further improve the accuracy. This strategy
can be used by repeating the messages in the document with
an agreed-upon sequence.
6) Qualitative analysis: We qualitatively analyse the
model’s output. We ﬁrst compare different variants, we then
discuss the implications of the used metrics. Lastly, we visu-
alize and analyse the changes performed by the model.
a) Model’s variants: To examine the effect of the ad-
versarial training, we show in Table II examples of input and
output pairs of the model trained with text reconstruction only
(the third row in Table I). We observed that there are two main
problems with this model: ﬁrst, it performs systematic and
ﬁxed modiﬁcations that alter the text statistics, e.g., the word
“the” is often changed. Second, it encodes the message with
tokens that have low occurrences count in the natural text (pos-
sibly, since there are no other constraints on the naturalness,
the model exploits this shortcut as a trivial solution as these
rare tokens would be clearly distinctive of the message). These
two problems could make the watermark easily detectable by
adversaries (and thus removable). It also makes the output
less natural and reduces the semantic correctness (which is
indicated by the higher SBERT distance in Table I, supporting
Input
− discriminator output
He was appointed the commanding ofﬁcer.
one of the most fascinating characters in the
series
He was appointed Bunbury commanding
ofﬁcer.
one of Milton most fascinating characters in
Milton series
TABLE II: Examples of input and output pairs of the model
trained without adversarial training showing systematic ﬁxed
changes that insert less likely tokens.
480/24640/32800/401040/52Words/bits count0255075100H0 rejected instances (%)acc. = 91% - 0.05 p-valueacc. = 91% - 0.01 p-valueacc. = 87% - 0.05 p-valueacc. = 87% - 0.01 p-valueacc. = 85% - 0.05 p-valueacc. = 85% - 0.01 p-valueacc. = 82% - 0.05 p-valueacc. = 82% - 0.01 p-valuenon-WM - 0.05 p-value1.01.52.02.53.03.54.0Number of sentences82.585.087.590.092.595.097.5Bit accuracy (%)SBERT dist=1.00SBERT dist=0.94SBERT dist=0.91SBERT dist=0.88Input
AWT output
In 1951 , a small airstrip was built on the
ruins
In 1951 , a small airstrip was built at the
ruins
It is the opening track from their 1987 album It is the opening track of their 1987 album
the ancient city is built from limestone
He also performed as an actor and a singer
While  had retained some control of
the situation
It is bordered on the east side by identical
temples
a family that ’s half black , half white , half
American , half British
they called out to the other passengers , who
they thought were still alive .
, but the complex is broken up by the heat
of cooking
the ancient city is built with limestone
He had performed as an actor and a singer
While  also retained some control of
the situation
It is bordered at the east side by identical
temples
a family that was half black , half white ,
half American , half British
they called out to the other passengers , who
they thought , still alive .
, and the complex is broken up by the heat
of cooking
TABLE IV: Examples of input and output pairs using AWT
where the meaning and correctness are preserved.
removed a verb (“had”) with an adverb (“also”) while still
being grammatically correct and also semantically consistent.
b) Metrics Analysis: We use the SBERT distance as an
evaluation metric in addition to using the language model
likelihood as a sorting metric. Therefore, we validate them
by evaluating their recall of the best sample. On a subset of
100 input sentences, we use AWT to generate 10 samples for
each input sentence. We examine the possible sentences to
ﬁnd the best sample (in terms of both semantic similarity
and grammatical correctness). For 92 out of 100 sentences,
we found that the best sample is retrieved by either one or
both metrics. This suggests that these two evaluation methods
correlate with human annotation.
Since we use the language model
to sort samples, we
compare the best sample by the SBERT versus the best sample
by the language model. On a subset of 200 sentences: the
two metrics yielded the same sample in 44% of the cases,
while they yielded comparable samples in 25%. The SBERT
metric had a better sample in 9%, while the language model
had a better sample in 22%. This shows that
they have
comparable performance, however, the language model was
slightly better and more sensitive to grammar correctness,
see Appendix VIII-A for such cases and for more qualitative
analysis of the SBERT distance metric.
c) Visualizations and analysis: To further visualize the
types of changes performed by the model at scale, we analyzed
the count of transitions between words in the input to output
text, as shown in Figure 8. We performed this analysis on
the most commonly changed words (or changed to), shown
Input
AWT output
He is also present in the third original video
animation
resulting in a population decline as workers
left for other areas
government ofﬁcials had been suspected
who has been in ofﬁce since 2009
The M @-@ 82 designation was truncated
at this time
He is could present in the third original
video animation
resulting in a population decline an workers
left for other areas
government ofﬁcials at been suspected
who has were in ofﬁce since 2009
The M @-@ 82 designation was truncated
were this time
TABLE V: Examples of failure modes showing input and
output pairs with grammatical errors.
Fig. 7: Top words’ count in the model trained without adver-
sarial training compared to their counts in AWT output and the
original dataset.
the use of an additional metric besides the meteor). To validate
this observation, we show in Figure 7 the occurrences of the
top words in this model compared to their occurrences in the
AWT model and the original text. Unlike AWT, this model’s
variant pushes unlikely words to the top and decreases the
count of more likely words (e.g., “the”), introducing clear
artifacts. In contrast, AWT keeps the distribution of top words
similar and encodes the message with also likely words,
providing better concealing. The model without ﬁne-tuning
also keeps the top words’ counts similar (not shown in the
ﬁgure), but it still shows syntactic inconsistencies, e.g., using
the end-of-sentence token in the middle of the sentence. We
observed that ﬁne-tuning the model helps to reduce these
inconsistencies, examples are shown in Table III.
We also show in Table IV examples of input and output pairs
obtained using AWT and the best-of-many sampling strategy (n
= 20 samples). The hidden information in these examples was
encoded using common tokens (e.g., preposition, articles, or
auxiliary verbs), correct structure, and with a very comparable
meaning to the input sentence.
Even though ﬁne-tuning and sampling improve the quality
of the output
to a large extent, we still observed some
failure cases of incorrect replacements that cause grammatical
and syntactic mistakes. Examples of such cases are shown
in Table V. One common failure mode happens when the
type of the word changes. However, this cannot be entirely
generalized as a failure case, e.g., some examples in Table IV
Input
− ﬁne-tuning output
AWT output
the Business Corporation,
which was
formed by a
group of leaders from the
area.
The railroads provided a
means of transportation and
an inﬂux of industries
the measurements indicated
that a segment of M @-@
82 west of  had the
peak volume for the high-
way
the Business Corporation,
 was formed by a
group of leaders from the
area.
The railroads provided a
means of transportation and
 inﬂux of industries
the measurements indicated
that a segment of M @-
@ 82 west of ’s the
peak volume for the high-
way
the Business Corporation,
which was formed by a
group of leaders at the area.
The railroads provided a
means of transportation and
that inﬂux of industries
the measurements indicated
that a segment of M @-@
82 west of  were the
peak volume for the high-
way
TABLE III: Comparison between two variants of the model:
before and after ﬁne-tuning. The ﬁne-tuned model shows better
syntactic consistency.
,the.ofandintoa="wasThe@-@on'sasthat(for)withbyBunburyLucasFamerisatfromwereMaritimehisAshurbanipalhehadMiltonitanwhich0500010000150002000025000No adv. trainingAWT [best sample]Originalin Appendix VIII-C. Based on this analysis, we highlight the
following observations: 1) Words are not consistently replaced
since the diagonal line has a high count, meaning that in most