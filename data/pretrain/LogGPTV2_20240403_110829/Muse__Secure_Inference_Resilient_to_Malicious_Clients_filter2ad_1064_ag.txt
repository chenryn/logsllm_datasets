saadi. “A survey of deep neural network architectures
and their applications”. In: Neurocomputing (2017).
Q. Lou and L. Jiang. “SHE: A Fast and Accurate
Deep Neural Network for Encrypted Data”. ArXiV,
cs.CR 1906.00148.
S. Milli, L. Schmidt, A. D. Dragan, and M. Hardt.
“Model Reconstruction from Model Explanations”. In:
FAT* ’19.
P. Mishra, R. Lehmkuhl, A. Srinivasan, W. Zheng, and
R. A. Popa. “Delphi: A Cryptographic Inference Ser-
vice for Neural Networks”. In: USENIX Security ’20.
A. Moghimi, G.
Irazoqui, and T. Eisenbarth.
“CacheZoom: How SGX Ampliﬁes the Power of
Cache Attacks”. In: CHES ’17.
P. Mohassel and Y. Zhang. “SecureML: A System for
Scalable Privacy-Preserving Machine Learning”. In:
S&P ’17.
P. Mohassel and P. Rindal. “ABY3: A Mixed Protocol
Framework for Machine Learning”. In: CCS ’18.
M. O. Rabin. “How To Exchange Secrets with Oblivi-
ous Transfer”. Harvard University Technical Report
81 (TR-81).
D. Rathee, M. Rathee, N. Kumar, N. Chandran, D.
Gupta, A. Rastogi, and R. Sharma. “CrypTFlow2:
Practical 2-Party Secure Inference”. In: CCS ’20.
O. Regev. “On lattices, learning with errors, random
linear codes, and cryptography”. In: JACM (2009).
M. S. Riazi, C. Weinert, O. Tkachenko, E. M.
Songhori, T.
Schneider, and F. Koushanfar.
“Chameleon: A Hybrid Secure Computation Frame-
work for Machine Learning Applications”.
In:
AsiaCCS ’18.
M. S. Riazi, M. Samragh, H. Chen, K. Laine, K.
Lauter, and F. Koushanfar. “XONN: XNOR-based
[Moh+18]
[Rab81]
[Rat+20]
[Reg09]
[Ria+18]
[Ria+19]
[Rol+20]
[Rot+19]
[Rou+18]
[San+18]
[Sch+17]
[Sea]
[Top+18]
[Tra+16]
[Tra+19]
[Van+18]
[Wag+19]
[Wag+21]
In:
Oblivious Deep Neural Network Inference”.
USENIX Security ’19.
D. Rolnick and K. P. Körding. “Reverse-Engineering
Deep ReLU Networks”. In: ICML ’20.
D. Rotaru and T. Wood. “MArBled Circuits: Mixing
Arithmetic and Boolean Circuits with Active Secu-
rity”. In: INDOCRYPT ’19.
B. D. Rouhani, M. S. Riazi, and F. Koushanfar.
“DeepSecure: Scalable Provably-secure Deep Learn-
ing”. In: DAC ’18.
A. Sanyal, M. Kusner, A. Gascón, and V. Kanade.
“TAPAS: Tricks to Accelerate (encrypted) Prediction
As a Service”. In: ICML ’18.
M. Schwarz, S. Weiser, D. Gruss, C. Maurice, and S.
Mangard. “Malware Guard Extension: Using SGX to
Conceal Cache Attacks”. In: DIMVA ’17.
“Microsoft SEAL (release 3.3)”. https://github.
com / Microsoft / SEAL. Microsoft Research, Red-
mond, WA.
S. Tople, K. Grover, S. Shinde, R. Bhagwan, and R.
Ramjee. “Privado: Practical and Secure DNN Infer-
ence”. ArXiV, cs.CR 1810.00602.
F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T.
Ristenpart. “Stealing Machine Learning Models via
Prediction APIs”. In: USENIX Security ’16.
F. Tramer and D. Boneh. “Slalom: Fast, Veriﬁable
and Private Execution of Neural Networks in Trusted
Hardware”. In: ICLR ’19.
J. Van Bulck, M. Minkin, O. Weisse, D. Genkin, B.
Kasikci, F. Piessens, M. Silberstein, T. F. Wenisch,
Y. Yarom, and R. Strackx. “Foreshadow: Extracting
the Keys to the Intel SGX Kingdom with Transient
Out-of-Order Execution”. In: USENIX Security ’18.
S. Wagh, D. Gupta, and N. Chandran. “SecureNN: 3-
Party Secure Computation for Neural Network Train-
ing”. In: Proc. Priv. Enhancing Technol. (2019).
S. Wagh, S. Tople, F. Benhamouda, E. Kushilevitz, P.
Mittal, and T. Rabin. “Falcon: Honest-Majority Ma-
liciously Secure Framework for Private Deep Learn-
ing”. In: Proc. Priv. Enhancing Technol. (2021).
[Wan+17] W. Wang, G. Chen, X. Pan, Y. Zhang, X. Wang, V.
Bindschaedler, H. Tang, and C. A. Gunter. “Leaky
Cauldron on the Dark Land: Understanding Memory
Side-Channel Hazards in SGX”. In: CCS ’17.
A. C. Yao. “How to Generate and Exchange Secrets
(Extended Abstract)”. In: FOCS ’86.
R. Zhu, Y. Huang, J. Katz, and a. shelat. “The Cut-and-
Choose Game and Its Application to Cryptographic
Protocols”. In: USENIX Security ’16.
[Zhu+16]
[Yao86]
2216    30th USENIX Security Symposium
USENIX Association
Functionality FACG
1. On client input {ri}i∈[(cid:96)], server input {si,Mi,αi,βi}i∈[(cid:96)],
(cid:104)αi(Mi(ri) −
2. Secret
3. Output (Mi(ri) − si,(cid:104)αi(Mi(ri) − si)(cid:105)1,(cid:104)βi · ri(cid:105)1) to the
compute {Mi(ri)− si,αi(Mi(ri)− si),βi · ri}i∈[(cid:96)].
si)(cid:105)1,(cid:104)αi(Mi(ri)− si)(cid:105)2 and βi · ri to (cid:104)βi · ri(cid:105)1,(cid:104)βi · ri(cid:105)2.
client, and ((cid:104)αi(Mi(ri)− si)(cid:105)2,(cid:104)βi · ri(cid:105)2) to the server.
αi(Mi(ri) − si)
share
to
Figure 11: The ideal functionality for Authenticated Correlations
Generator
Functionality FCDS
1. The client and server input (Mi(ri) − si,(cid:104)αi(Mi(ri) −
si)(cid:105)1,ri+1,(cid:104)βi+1 · ri+1(cid:105)1)
and (αi,βi+1,(cid:104)αi(Mi(ri) −
i,k,1}k∈[|inpi|]) respectively
si)(cid:105)2,(cid:104)βi+1 · ri+1(cid:105)2,{labC
for some i ∈ [(cid:96)].
2. If αi(Mi(ri)−si) =(cid:104)αi(Mi(ri)−si)(cid:105)1 +(cid:104)αi(Mi(ri)−si)(cid:105)2
and βi+1 · ri+1 = (cid:104)βi+1 · ri+1(cid:105)1 +(cid:104)βi+1 · ri+1(cid:105)2, output the
i,k,1}k∈[|inpi|] corresponding to Mi(ri)−
labels {labC
si and ri+1 to the client. Otherwise, abort.
i,k,0, labC
i,k,0, labC
Figure 12: The ideal functionality for Conditional Disclosure of
Secrets
FInputAuth
• The client’s input is mc and the server’s input is ms and a
MAC key δ. The client receives [[mc]]1, [[ms]]1 and the server
receives [[mc]]2, [[ms]]2.
Figure 13: Description of FInputAuth.
Protocol ΠInputAuth
[[r]]
1. Both parties invoke FRand to receive |mc| random shares
2. r is privately opened to the client.
3. The client broadcasts ε = mc − r.
4. The server’s share is [[mc]]2 = (ε, [[r]]2) and the client’s share
is [[mc]]1 = (ε, [[r]]1).
5. The server chooses two masking vectors u,v and sends the
client [[ms]]1 = (ms−u,δ·ms−v). The server sets its share
[[ms]]2 = (u,v)
FTriple
• The client’s input is a1,b1 and the server’s input is a2,b2
and a MAC key δ. The client receives [[a1 + a2]]1, [[b1 +
b2]]1, [[(a1 + a2)· (b1 + b2)]]1 and the server receives [[a1 +
a2]]2, [[b1 + b2]]2, [[(a1 + a2)· (b1 + b2)]]2.
Figure 15: Description of FTriple.
FRand
• The client’s input is r1 and the server’s input is r2 and a
MAC key δ. The client receives [[r1 + r2]]1 and the server
receives [[r1 + r2]]2.
Figure 16: Description of FRand.
Protocol ΠTriple
i. The client and the server engage in a two-party computa-
tion protocol with security against malicious clients and
semi-honest servers to generate the public key, secret key
pair for HE. At the end of the protocol, the client learns
the public key pk and the secret key sk whereas the server
only learns pk.
ii. The client sends Enc(pk,a1), Enc(pk,b1) to the server
along with a zero-knowledge proof of well-formedness of
the two ciphertexts. The server veriﬁes this proof before
continuing.
iii. The server homomorphically computes Enc(pk,a1 +
a2), Enc(pk,b1 + b2) and Enc(pk, (a1 + a2)· (b1 + b2))
along with Enc(pk,δ(a1 + a2)), Enc(pk,δ(b1 + b2)) and
Enc(pk,δ· (a1 + a2)· (b1 + b2)).
iv. The server chooses six random masking elements
u1,v1,t1, u2,v2,t2 and computes Enc(pk,a1 + a2 − u1),
Enc(pk,b1 +b2−v1) and Enc(pk, (a1 +a2)· (b1 +b2)−
t1) along with Enc(pk,δ(a1 + a2)− u2), Enc(pk,δ(b1 +
b2) − v2) and Enc(pk,δ · (a1 + a2) · (b1 + b2) − t2). It
sends these six ciphertexts to the client.
v. The client decrypts the above ciphertexts and ob-
tains [[a1 + a2]]1 = (a1 + a2 − u1,δ(a1 + a2)− u2), [[b1 +
b2]]1 = (b1 + b2 − v1,δ(b1 + b2)− v2), [[(a1 + a2)· (b1 +
b2)]]1 = (a1 + a2) · (b1 + b2) − t1,δ · (a1 + a2) · (b1 +
b2)−t2). The server outputs [[a1 + a2]]2 = (u1,u2), [[b1 +
b2]]2 = (v1,v2), [[(a1 + a2)· (b1 + b2)]]2 = (t1,t2).
Figure 14: The protocol for Input Authentication.
Figure 17: The protocol for Triple Generation.
USENIX Association
30th USENIX Security Symposium    2217
A Pseudocode for our attacks from Section 2
RecoverNetwork:
1. First, recover the last layer:
(a) Denote by ˜M(cid:96) the recovered matrix for the last
(b) For each j ∈ [t]:
layer.
i. Set the initial input to the network to be zero,
ii. Follow the inference protocol to partially evalu-
i.e. x1 := 0.
ate the network up to the (cid:96)− 1-th layer:
x(cid:96)−1 := ReLU(M(cid:96)−1(. . . ReLU(M1x1))) = 0.
iii. Malleate the client’s share of x(cid:96)−1: (cid:104)x(cid:48)
(cid:96)−1(cid:105)C :=
(cid:104)x(cid:96)−1(cid:105)C + e j.
x(cid:96) := M(cid:96)x(cid:48)
˜M(cid:96).
(cid:96)−1 = M(cid:96)e j.
v. Set the j-th column of
iv. Complete the protocol with the server to obtain
˜M(cid:96) to be x(cid:96).
2. Then, recover all previous layers:
(c) Output
(d) For each i ∈ [(cid:96)− 1, . . . ,1]:
i. If the i-th layer is a fully-connected layer, set
Mi := RecoverFCLayer(Mi+1, . . . ,M(cid:96)).
ii. If the i-th layer is a convolutional layer, set Mi :=
RecoverConvLayer(Mi+1, . . . ,M(cid:96)).
(e) Output (M1, . . . ,M(cid:96)−1).
xi−1.
RecoverConvLayer(Mi+1, . . . ,M(cid:96)) :
1. Let the dimensions of the convolutional kernel Ki be ki × ki.
2. Sample a random matrix R having the same dimension as
3. Apply the im2col transformation to R to obtain R(cid:48).
4. Let S be the indices of the pivot columns of R(cid:48) when it is
in row-reduced echelon form. If |S| < ki × ki, resample R
and retry. Then S speciﬁes the indices of the independent
columns of R(cid:48).a
5. Follow the inference protocol to evaluate the network up
to the i− 1-th layer to obtain (a share of) the intermediate
state xi−1 := ReLU(Mi−1(. . . ReLU(M1x1))) = 0.
6. Malleate the client’s share of xi−1: (cid:104)x(cid:48)
i−1(cid:105)C := (cid:104)xi−1(cid:105)C + R.
7. Interact with the server to evaluate the i-th linear layer to
obtain a share of yi := Mix(cid:48)
8. Obtain the input for the next linear layer: (cid:104)xi(cid:105)C :=
MaskAndLinearizeReLU((cid:104)yi(cid:105)C,S).
The vector xi is now all-zero, except at locations in S, where
it equals the corresponding elements of yi.
9. Interact with the server to complete the evaluation of the
rest of the network, invoking LinearizeReLU to force inter-
vening ReLUs to behave linearly.
i−1.
10. Set K := [X1, . . .Xki×ki
], where each Xj is a formal variable.
11. Set X to be the all-zero matrix of dimension equal to R(cid:48),
except at locations in S, where it equals R(cid:48).
12. Compute X(cid:96) := M(cid:96) · M(cid:96)−1···Mi+1 · (KX).
13. Solve the linear system X(cid:96) = x(cid:96) to learn the values of the
formal variables Xj, and hence the kernel Ki.
aThe pivot columns are linearly independent by deﬁnition, and row
operations do not change linear dependence of columns.
MaskAndLinearizeReLU((cid:104)y(cid:105)C,S):
1. Malleate the client’s local share of y to obtain a share of the
malleated y(cid:48) as follows:
(a) For all l ∈ S, set (cid:104)y(cid:48)(cid:105)C[l] := (cid:104)y(cid:105)C[l] + c.
(b) For all l (cid:54)∈ S, set (cid:104)y(cid:48)(cid:105)C[l] := (cid:104)y(cid:105)C[l]− c.
2. Obtain (cid:104)x(cid:105)C := LinearizeReLU(y(cid:48)
3. Invert Step 1a by malleating (cid:104)x(cid:105)C: set (cid:104)x(cid:48)(cid:105)C[S] := (cid:104)x(cid:105)C[S]− c
4. Output (cid:104)x(cid:48)(cid:105)C.
)..
LinearizeReLU((cid:104)y(cid:105)C):
1. Malleate the client’s local share of y to obtain a share of the
2. Interact with the server to obtain (cid:104)x(cid:105)C, which is the client’s
3. Invert Step 1 by malleating (cid:104)x(cid:105)C: set (cid:104)x(cid:48)(cid:105)C := (cid:104)x(cid:105)C − c
4. Output (cid:104)x(cid:48)(cid:105)C.
malleated y(cid:48): set (cid:104)y(cid:48)(cid:105)C := (cid:104)y(cid:105)C + c.
share of x := ReLU(y(cid:48)
).
i to be a si ×ti matrix consisting of formal variables.
i := (cid:98)si/m(cid:99).
RecoverFCLayer(Mi+1, . . . ,M(cid:96)) :
1. Let the dimension of the i-th linear layer be si ×ti.
2. Set M(cid:48)
3. Let s(cid:48)
4. For each j ∈ [ti], and for each k ∈ [s(cid:48)
i]:
(a) Set the initial input to the network to be zero, i.e. x1 := 0.
(b) Follow the inference protocol to evaluate the network up to
the i− 1-th layer to obtain (a share of) the intermediate state
xi−1 := ReLU(Mi−1(. . . ReLU(M1x1))) = 0.
(c) Construct a query q j := e j.
(d) Malleate the client’s share of xi−1: (cid:104)x(cid:48)
i−1(cid:105)C := (cid:104)xi−1(cid:105)C + q j.
(e) Interact with the server to evaluate the i-th linear layer to
obtain a share of yi := Mix(cid:48)
(f) Set k(cid:48) := k· m, and S := {k(cid:48)
(g) Obtain the input for the next linear layer:
(cid:104)xi(cid:105)C := MaskAndLinearizeReLU(yi,S).
The vector xi is now all-zero, except at locations in S, where
it equals the corresponding elements of yi.
i−1.
, . . . ,k(cid:48)
+ m− 1}.
(h) Interact with the server to complete the evaluation of the rest
of the network, invoking LinearizeReLU to force intervening
ReLUs to behave linearly.
(i) Compute Xi as follows. First, compute M(cid:48)
i · q j, and then zero
(j) Construct the k-th linear system x(cid:96) = M(cid:96) · M(cid:96)−1···Mi+1 · Xi.
5. Solve all the linear systems to recover the matrix Mi.
out all locations that are not in S.
2218    30th USENIX Security Symposium
USENIX Association