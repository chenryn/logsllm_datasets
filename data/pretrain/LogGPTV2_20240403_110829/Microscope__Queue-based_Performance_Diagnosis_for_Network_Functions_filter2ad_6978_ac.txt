The direct cause of p’s long latency is the queue with pending
packets when p arrives at f . Thus we consider the queuing period
from when the first packet is enqueued up to the time at which
p arrives at the NF (Figure 5). By considering the entire queuing
period, Microscope learns the whole history of how the queue is
built up. Therefore, even if the culprits for the problem do not
overlap with the victim packets in time, Microscope can still detect
the cause of the problem.
Let T be the length of the queuing period at NF f . Let ni(T)
and np(T) be the number of packets arriving and getting processed
at the NF during time T . Figure 5 shows that the queue builds up
because the input rate is higher than the processing rate. This can
happen due to two reasons: (1) High input rate: the input rate is
higher than the peak processing rate; (2) Low processing rate: the
processing rate of the NF f is lower than its peak processing rate
(e.g., due to cache misses, CPU interrupt). We define ri as the peak
processing rate of an NF with the same hardware/software settings
in the NFV topology.3
By comparing ni(T) and np(T) with the expected number of
packets (ri · T ), we can quantify the two reasons. We use an input
f
workload score S
i to represent the number of extra input packets,
compared to the number of packets that can be processed at the
peak rate during a period of T .
(cid:26) ni(T) − ri · T
0
f
i
S
=
if ni(T) ⩾ ri · T
if ni(T)  0), which
we call culprit packets. Later we use pattern aggregation (§ 4.4) to
determine which flows lead to such slow processing.
p
4.4 Pattern Aggregation
Given many packet-level causal relations, our next step is to ag-
gregate them into causal relation patterns operators can act on.
For example, if we can narrow down that certain flow aggregates
always face problems at a particular NF, the vendor of this NF can
investigate the configuration and the processing of these flows.
Our pattern aggregation takes the packet-level causal relations
→: score>
TB TCTA TsourceTsourceTATBTCScore:TimePropagation delayPreSet(p)InterruptfCBAsourceVictim packet pOther packetsTexp=ni(T)/rif(Tsource-TB)(Texp-Tsource)S f←source=Texp-TsourceTexp-TC⋅ SifS f←A=Tsource-TBTexp-TC⋅ SifS f←B= 0S f←C= TB-TCTexp-TC⋅ Sif(TB-TC)p experiences problem at f: S fQueuing period at ff-local: Spf f-input (PreSet(p)): SifA’s impact on PreSet(p): S f←A B’s impact on PreSet(p): S f←B … S f =Spf +SifSif =S f←A +S f←B +…A-local: Spf←AA-input: Sif←AQueuing period at AS f←A=Spf←A +Sif←AB-local: Spf←BB-input: Sif←BQueuing period at BS f←B=Spf←B+Sif←BTimePropagation delayPreSet(p)VPNFWsourceVictim packet pPackets with port=80② SiVPN>0① p experiences long latency at VPN④ SpVPN← FW>0③ S VPN← FW>0Microscope: Queue-based Performance Diagnosis for Network Functions
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
as input, and generates aggregate patterns in the form of →: score. Here a flow aggregate is defined by the five-tuple
derived from source IP prefix, source port range, destination IP
prefix, destination port range, and protocol set; one can also extend
flow aggregates to cover other packet fields. An NF set includes not
only NFs but also input traffic sources.
Our pattern aggregation problem is similar to AutoFocus [25]
which automatically aggregates multi-dimensional flows into traf-
fic clusters that best represent the current traffic (i.e., hierarchical
heavy hitter). Our goal is to find pattern aggregates along many
dimensions which include culprit flow aggregates, culprit NF set,
victim flow aggregates, victim NF set, where the flow aggregates
further include five tuples or more and the NF set aggregates NF
instances of the same type. We determine significant pattern aggre-
gates which contributes to a large portion of the score (e.g., above
a threshold th), after excluding descendants of each of these aggre-
gates (similar to hierarchical heavy hitters [25]). Note that a higher
threshold th leads to fewer details in the report. In practice, opera-
tors can adjust the aggregation threshold th, and thereby trade-off
succinctness of the report with the amount of detail in it.
A key challenge is that we have far more dimensions than traffic
aggregates. To speed up the aggregation process, we leverage the
causal relation between culprit packets/NFs and victim packets/NFs.
Most of the time, in a significant pattern aggregate, its culprit
flow aggregate is also a significant flow aggregate if we just run
AutoFocus on the culprit flow fields. The same observation applies
to victim flow aggregates. This is because a victim packet is affected
by a limited set of packets and a culprit packet set affects a limited
number of victim packets.
Our pattern aggregation algorithm works in three steps: First,
we group packet-level causal relations by culprit packets and cul-
prit NFs. For each  we run AutoFocus
on  dimensions and generate a few in-
termediate pattern aggregates with aggregated victim packet/NF
fields. Next, we run AutoFocus again on the intermediate pattern
aggregates to generate the final significant pattern aggregates. Our
evaluation shows that this decoupling significantly reduces the
aggregation time without losing any significant patterns.
Some problems may be intermittent but happen repeatedly (for
example in Section 6.4, big-triggering flows arrive intermittently
to the Firewall, which cause significant performance problems).
Our aggregation algorithm can effectively find out these repeating
problems over the timeframe when the operator runs Microscope,
since they usually share some patterns.
5 IMPLEMENTATION
Microscope includes a data collector in the runtime and an offline
diagnosis module.
Runtime information collection. We implement the data col-
lector in the DPDK library with about 200 lines of code. Thus we
can support any NF using DPDK as the packet I/O library. DPDK
has a receive function and a transmission function, which handle
the input and output queues of an NF. We instrument these func-
tions to collect the required runtime data (see Table 1). It is feasible
396
Figure 9: Resolving IPID ambiguity using order of packets.
to extend our collector to other packet I/O libraries (based on VNF
vendor’s choice) like netmap [50] or VPP [11].
To minimize the performance impact of Microscope on the NF,
we keep the overhead on the critical path of the execution to a min-
imum. Instead of dumping to the hard disk directly, Microscope’s
collector writes the data to shared memory where it is picked up
by a standalone dumper for storing on the disk.
For each packet, we record the five-tuple and the IPID, so a
packet is uniquely identified across different NFs. Since DPDK
fetches packets in batches (the maximum batch size is typically 32
packets), we also record a per-batch timestamp as well as the size
of the batch. These data are sufficient for identifying the queuing
period, because if the batch size is smaller than the maximum size,
the queue must have been cleared, which is an indication of the
start of a new queuing period (we will discuss cases where the
queue is mostly non-empty in Section 7).
Directly collecting the data incurs a high overhead because we
need more than 15 bytes per packet. We compress the data down to
around two bytes per packet. The intuition is that the same packet
traverses multiple NFs, so we just need to keep the five tuples of
each packet at the end of the NF graph. For all other NFs, we only
need to record the IPID of the packet.
However, this leads to challenges in reconstructing the trace
of each packet (mapping the records of the same packet across
different NFs), because different packets may have the same IPID in
different NFs and we get confused about the traces of these packets.
We resolve this ambiguity by using three pieces of “side-channel”
information: the paths of packets, the timing of packets, and the
order of packets.
(1) The paths of packets. We reconstruct each packet trace from the
last NF where we record the five tuples backward to the source. This
means at each step, we only need to look into packet records at the
immediate upstream NFs, which reduces the chances of overlapping
with other packets with the same IPID. Note that this filter does not
work for NFs that assign path dynamically such as load balancers.
(2) The timing of packets. Since the delay from an upstream NF
to a downstream NF is bounded, we can just consider mapping
records on the two NFs that are within the maximum delay (i.e.,
queuing delay plus propagation delay). Since the propagation delay
is small and the maximum number of packets in a queue in DPDK
is 1024, out of 65,536 possible IPIDs, the chances that of two packets
colliding with the same IPID in the delay bound is small.
(3) The order of packets. The intuition is illustrated in Figure 9 where
the IPID 5 has ambiguity. However, if IPID 3 is unambiguous, we
know that the left IPID 5 in the downstream queue cannot come
from upstream 2 if packet ordering is to be preserved. This allows
us to resolve the ambiguity for IPID of 5.
Offline diagnosis. The offline diagnosis module includes 6000
lines of code implemented in C and C++. Operators define the