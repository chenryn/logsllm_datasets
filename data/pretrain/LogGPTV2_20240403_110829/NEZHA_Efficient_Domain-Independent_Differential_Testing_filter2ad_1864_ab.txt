across both programs is 1 + 2 = 3. One may drive the
input generation process favoring the mutation of inputs that
increase coverage (i.e., exercise previously unexplored edges).
Since v=7 increased the code coverage, it is added to the
corpus that will be used for the next generation: I1 = {7}.
In the following stage of the testing, we pick any remaining
inputs from the current corpus and pass them to programs
A and B. Selecting v=0 as the next input will also increase
coverage, since execution touches three previously-unseen
edges (A3, A2 and B1), and thus the ﬁle is picked for further
mutations: I1 = {7, 0}. At this stage, the only input of I0 that
has not been executed is v=1. This input’s execution does
not increase coverage, since both edges A1 and B1 have been
visited again, and thus v=1 is not added to I1 and will not be
considered for future mutations. However, we notice that v=1,
with a single increment mutation, could be transformed to an
input that would disclose the discrepancy between programs A
and B, had it not been discarded. This example demonstrates
that simply maximizing edge-coverage often misses interesting
inputs that may trigger semantic bugs. By contrast, had we
tracked the δ-diversity using path tuples across past iterations,
input v=1 would invoke the path tuple (cid:2){A1},{B1}(cid:3), which,
as a pair/combination, would have not been seen before. Thus,
using a path δ-diversity state, instead of code coverage, results
in v=1 been considered for further mutations. As seen in
Table I, the mutated input v=2 uncovers the semantic bug.
2) Scenario 2: Black-box Guidance: If program instrumen-
tation or binary rewriting are not feasible options, we may
still adapt the notion of program diversity to a black-box
setting. The key intuition is, again, to look for previously
unseen patterns across the observed outputs of the tested
programs. Depending on the context of the application being
tested, available outputs may vary greatly. For instance, a
malware detector may only provide one bit of information
based on whether some input ﬁle contains a malware or not,
whereas other applications may offer richer sets of outputs
such as graphical content, error or debug messages, values
returned to the executing shell, exceptions, etc. In the context
of differential testing, the outputs of a single application A
can be used as a reference against the outputs of all other
applications being tested. For example, if browsers A, B, and C
are differentially tested, one may use browser A as a reference
and then examine the contents of different portions of the
617
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
Generation Mutation Input
seed
seed
seed
1
-
-
-
increment
7
0
1
2
Execution Paths
A
B
Path Tuple
{A1}
{A3, A2}
{A1}
{A3, A4}
{B3, B2} P1 = (cid:2){A1}, {B3, B2}(cid:3)
P2 = (cid:2){A3, A2}, {B1}(cid:3)
{B1}
P3 = (cid:2){A1}, {B1}(cid:3)
{B1}
P4 = (cid:2){A3, A4}, {B1}(cid:3)
{B1}
δ-diversity State
{P1}
{P1, P2}
{P1, P2, P3}
{P1, P2, P3, P4}
Add to Corpus
Report Bug
Coverage δ-diversity Coverage δ-diversity



-







-




TABLE I: A semantic bug that is missed by differential testing using code coverage but can be detected by NEZHA’s path
δ-diversity (gray-box) during testing of the examples shown in Figure 1. NEZHA’s black-box δ-diversity input generation
scheme (not shown in this example) would also have found the semantic bug.
rendered Web pages with respect to A, using an arbitrary
number of values for the encoding (different values may
denote a mismatch in the CSS or HTML rendering etc.).
Regardless of the output formulation, however, for each
input used during testing, NEZHA may receive a corresponding
set of output values and then only select the inputs that result
in new output tuples for further mutations. In the context of the
example of Figure 1, let us assume that the outputs passed to
NEZHA are the values returned by routines checkVer_A and
checkVer_B. If inputs 0, 7, and 1 are passed to programs
A and B, NEZHA will update its internal state with all unique
output tuples seen so far: {(cid:2)−1,−1(cid:3),(cid:2)−2,−2(cid:3),(cid:2)−1,−2(cid:3)}.
Any new input which will result in a previously unseen tuple
will be considered for future mutations, otherwise it will
be discarded (e.g., with the aforementioned output tuple set,
input 2 resulting in tuple (cid:2)0,−2(cid:3) would be considered for
future mutations, but input 9 resulting in (cid:2)−1,−2(cid:3) would be
discarded).
III. METHODOLOGY
In each testing session, NEZHA observes the relative be-
havioral differences across all tested programs to maximize
the number of reported semantic bugs. To do so, NEZHA
uses Evolutionary Testing (ET) [53], inferring correlations
between the inputs passed to the tested applications and their
observed behavioral asymmetries, and, subsequently, reﬁnes
the input generation, favoring more promising inputs. Contrary
to existing differential testing schemes that drive their input
generation using monolithic metrics such as the code coverage
that is maximized across some or all of the tested programs,
NEZHA utilizes the novel concept of δ-diversity: metrics that
preserve the differential diversity (δ-diversity) of the tested
applications will perform better at ﬁnding semantic bugs than
metrics that overlook relative asymmetries in the applications’
execution. The motivation behind δ-diversity becomes clearer
if we examine the following example. Suppose we are per-
forming differential testing between applications A and B.
Now, suppose an input I1 results in a combined coverage
C across A and B, exercising 30% of the CFG edges in A
and 10% of the edges in B. A different input I2, that results
in the same overall coverage C, however exercising 10% of
the edges in A and 28% of the edges of B, would not be
explored further under monolithic schemes, despite the fact
that it exhibits much different behavior in each application
compared to input I1.
Algorithm 1 DiffTest: Report all discrepancies across appli-
cations A after n generations, starting from a corpus I
1: procedure DIFFTEST(I, A, n, GlobalState)
discrepancies = ∅ ;reported discrepancies
2:
while generation ≤ n do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
input = RANDOMCHOICE(I)
mut_input = MUTATE(input)
generation_paths = ∅
generation_outputs = ∅
for app ∈ A do
app_path, app_outputs = RUN(app, mut_input)
geneneration_paths ∪ = {app_path}
geneneration_outputs ∪ = {app_outputs}
end for
if NEWPATTERN(generation_paths,
14:
15:
16:
17:
18:
19:
20:
21:
22: end procedure
end while
return discrepancies
generation_outputs,
GlobalState) then
I ← I ∪ mut_input
end if
if ISDISCREPANCY(generation_outputs) then
discrepancies ∪ = mut_input
end if
generation = generation + 1
We present NEZHA’s core engine in Algorithm 1. In each
testing session, NEZHA examines if different inputs result in
previously unseen relative execution patterns across the tested
programs. NEZHA starts from a set of initial seed inputs I, and
performs testing on a set of programs A for a ﬁxed number of
generations (n). In each generation, NEZHA randomly selects
(line 4) and mutates (line 5) one input (individual) out of the
population I, and tests it against each of the programs in A.
The recorded execution paths and outputs for each application
are added to the sets of total paths and outputs observed during
the current generation (lines 8-12). Subsequently, if NEZHA
determines that a new execution pattern is observed during this
input execution, it adds the respective input to the input corpus,
which will be used to produce the upcoming generation (lines
13-14). Finally, if there is a discrepancy in the outputs of
the tested applications, NEZHA adds the respective input to
the set of total discrepancies found (lines 16-18). Whether a
discrepancy is observed in each generation depends on the
outputs of the tested programs: if at least one application
rejects an input and at least one other accepts it, a discrepancy
618
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
is logged.
A. Guidance Engines
In Algorithm 1, we demonstrated that NEZHA adds an input
to the active corpus only if that input exhibits a newly seen
pattern. In traditional evolutionary algorithms, the ﬁtness of
an individual for producing future generations is determined
by its ﬁtness score. In this section, we explain how δ-diversity
can be used in NEZHA’s guidance engines, both in a gray-box
and a black-box setting.
1) Gray-box guidance: The most prevalent guidance mech-
anism in gray-box testing frameworks is the code coverage
achieved by individual inputs across the sets of tested applica-
tions. Code coverage can be measured using function coverage
(i.e.,
the functions accessed in one execution run), basic
block coverage or edge coverage. However, as discussed in
Section II, this technique is not well suited for ﬁnding semantic
bugs. By contrast, NEZHA leverages relative asymmetries
of the executed program paths to introduce two novel δ-
diversity path selection guidance engines, suitable for efﬁcient
differential testing.
Suppose a program p is executing under an input i. We
call the sequence of edges accessed during this execution the
execution path of p under i, denoted by pathp,i. Tracking
all executed paths (i.e., all the sequences of edges accessed in
the CFG) is impractical for large-scale applications containing
multiple loops and complex function invocations. In order
to avoid this explosion in tracked states, NEZHA’s gray-box
guidance uses two different approximations of the execution
paths, one of coarse granularity and the other offering ﬁner
tracking of the relative execution paths.
Path δ-diversity (coarse): Given a set of programs P
that are executing under an input i, let P CP,i be the Path
Cardinality tuple (cid:2)|pathp1,i|,|pathp2,i|, ...,|pathp|P|,i|(cid:3). Each
P CP,i entry represents the total number of edges accessed
in each program pk ∈ P, for one single input i. Notice
that P CP,i differs from the total coverage achieved in the
execution of programs P under i, in the sense that P CP,i does
not maintain a global, monolithic score, but a per-application
count of the edges accessed, when each program is executing
under input i. Throughout an entire testing session, starting
from an initial input corpus I, the overall (coarse) path δ-
diversity achieved is the cardinality of the set containing all
the above tuples: P DCoarse = | (cid:2)
This representation expresses the maximum number of
unique path cardinality tuples for all programs in P that
have been seen throughout the session. However, we notice
that, although the above formulation offers a semantically
richer representation of the execution, compared to total edge
coverage, it constitutes a coarse approximation of the (real)
execution paths. A ﬁner-grained representation of the execu-
tion can be achieved if we take into account which edges,
speciﬁcally, have been accessed.
i∈I{P CP,i}|.
Path δ-diversity (ﬁne): Consider the path pathp,i, which
holds all edges accessed during an execution of each pro-
gram pk ∈ P under input i. Let path_setp,i be the set
input
consisting of all unique edges of pathp,i. Thus path_setp,i
contains no duplicate edges, but
instead holds only the
CFG edges of p that have been accessed at
least once
during the execution. Given a set of programs P,
the
i across P is the tuple
(ﬁne) path diversity of
P DP,i = (cid:2)path_setp1,i, path_setp2,i, ..., path_setp|P|,i(cid:3). Es-
sentially, P DP,i acts as a "ﬁngerprint" of the execution of
input i across all tested programs and encapsulates relative
differences in the execution paths across applications. For an
entire testing session, starting from an initial input corpus I,
the (ﬁne) path δ-diversity achieved is the cardinality of the set
containing all the above tuples: P DF ine = | (cid:2)
i∈I{P DP,i}|.
To demonstrate how the above metrics can lead to different
discrepancies, let us consider a differential testing session
involving two programs A and B. Let An, Bn denote edges
in the CFG of A and B, respectively, and let us assume that a
given test input causes the paths (cid:2)A1, A2, A1(cid:3) and (cid:2)B1(cid:3) to be
exercised in A and B respectively. At this point, P DCoarse =
{(cid:2)3, 1(cid:3)}, and P DF ine = {(cid:2){A1, A2},{B1}(cid:3)}. Suppose we
mutate the current input, and the second (mutated) input now
exercises paths (cid:2)A1, A2(cid:3) and (cid:2)B1(cid:3) across the two applications.
After the execution of this second input, P DF ine remains
unchanged, because the tuple (cid:2){A1, A2},{B1}(cid:3) is already in
the P DF ine set. Conversely, P DCoarse will be updated to
P DCoarse = {(cid:2)3, 1(cid:3),(cid:2)2, 1(cid:3)}. Therefore, the new input will be
considered for further mutation under a coarse path guidance,
since it increased the cardinality of the P DCoarse set, however
it will be rejected under ﬁne δ-diversity guidance. Finally, note
that if we use total edge coverage as our metric for input
selection, both the ﬁrst and second inputs result in the same
code coverage of 3 edges (two unique edges for A plus one
edge for B). Thus, under a coverage-guided engine, the second
input will be rejected as it does not increase code coverage,
despite the fact that it executes in a manner that has not been
previously observed across the two applications.
2) Black-box guidance: As mentioned in Section II-B2,
NEZHA’s input generation can be driven in a black-box manner
using any observable and countable program output, such
as error/debug messages, rendered or parsed outputs, return
values etc. For many applications, especially those implement-
ing particular protocols or RFCs, such outputs often uniquely
identify deterministic execution patterns. For example, when a
family of similar programs returns different error codes/mes-
sages, any change in one test program’s returned error relative
to the error codes returned by the other programs is highly
indicative of the relative behavioral differences between them.
Such output asymmetries can be used to guide NEZHA’s path
selection.
Output δ-diversity: Let p be a program which, given an
input i, produces an output op,i. We deﬁne the output diversity
of a family of programs P, executing with a single input
i, as the tuple ODP,i = (cid:2)op1,i, op2,i, ..., op|P|,i(cid:3). Across a
testing session that starts from an input corpus I, output δ-
diversity tracks the number of unique output tuples that are
observed throughout the execution of inputs i ∈ I across
all programs in P: | (cid:2)
i∈I{ODP,i}|. Input generation based
619
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
on output δ-diversity aims to drive the tested applications to
result in as many different output combinations across the
overall pool of programs, as possible. This metric requires
no knowledge about the internals of each application and is
completely black-box. As a result, it can even be applied on
applications running on a remote server or in cases were binary
rewriting or instrumentation is infeasible. We demonstrate in
Section V that this metric performs equally well as NEZHA’s
gray-box engines for programs that support ﬁne-grained error