## PostgreSQL 11 tpcc 测试(103万tpmC on ECS) - use sysbench-tpcc by Percona-Lab  
### 作者                                                           
digoal                                                           
### 日期                                                           
2018-09-13                                                         
### 标签                                                           
PostgreSQL , tpcc   
----                                                           
## 背景    
## 环境  
阿里云虚拟机  
```  
[root@pg11-test ~]# lscpu  
Architecture:          x86_64  
CPU op-mode(s):        32-bit, 64-bit  
Byte Order:            Little Endian  
CPU(s):                64  
On-line CPU(s) list:   0-63  
Thread(s) per core:    2  
Core(s) per socket:    32  
Socket(s):             1  
NUMA node(s):          1  
Vendor ID:             GenuineIntel  
CPU family:            6  
Model:                 85  
Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz  
Stepping:              4  
CPU MHz:               2500.008  
BogoMIPS:              5000.01  
Hypervisor vendor:     KVM  
Virtualization type:   full  
L1d cache:             32K  
L1i cache:             32K  
L2 cache:              1024K  
L3 cache:              33792K  
NUMA node0 CPU(s):     0-63  
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1  
```  
```  
[root@pg11-test ~]# free -g  
              total        used        free      shared  buff/cache   available  
Mem:            503          12         216          65         274         423  
Swap:             0           0           0  
```  
```  
uname -a  
Linux pg11-test 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  
```  
## 配置ECS SSD盘存储  
1、卷管理  
```  
dd if=/dev/zero of=/dev/vdb bs=1024 count=1024  
dd if=/dev/zero of=/dev/vdc bs=1024 count=1024  
dd if=/dev/zero of=/dev/vdd bs=1024 count=1024  
dd if=/dev/zero of=/dev/vde bs=1024 count=1024  
dd if=/dev/zero of=/dev/vdf bs=1024 count=1024  
dd if=/dev/zero of=/dev/vdg bs=1024 count=1024  
dd if=/dev/zero of=/dev/vdh bs=1024 count=1024  
dd if=/dev/zero of=/dev/vdi bs=1024 count=1024  
pvcreate /dev/vd[b-i]  
vgcreate -A y -s 128M vgdata01 /dev/vd[b-i]  
lvcreate -A y -i 8 -I 8 -L 4096GiB -n lv01 vgdata01  
lvcreate -A y -i 8 -I 8 -L 4096GiB -n lv02 vgdata01  
lvcreate -A y -i 8 -I 8 -L 4096GiB -n lv03 vgdata01  
```  
2、文件系统条带  
```  
mkfs.ext4 /dev/mapper/vgdata01-lv01 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=16 -b 4096 -T largefile -L lv01  
mkfs.ext4 /dev/mapper/vgdata01-lv02 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=16 -b 4096 -T largefile -L lv02  
mkfs.ext4 /dev/mapper/vgdata01-lv03 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=16 -b 4096 -T largefile -L lv03  
```  
3、mount  
```  
vi /etc/fstab   
LABEL=lv01 /data01 ext4 defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback 0 0  
LABEL=lv02 /data02 ext4 defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback 0 0  
LABEL=lv03 /data03 ext4 defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback 0 0  
mkdir /data01  
mkdir /data02  
mkdir /data03  
mount -a  
```  
## 配置ECS虚拟机OS参数  
1、内核参数  
```  
vi /etc/sysctl.conf  
# add by digoal.zhou      
fs.aio-max-nr = 1048576      
fs.file-max = 76724600      
# 可选：kernel.core_pattern = /data01/corefiles/core_%e_%u_%t_%s.%p               
# /data01/corefiles 事先建好，权限777，如果是软链接，对应的目录修改为777      
kernel.sem = 4096 2147483647 2147483646 512000          
# 信号量, ipcs -l 或 -u 查看，每16个进程一组，每组信号量需要17个信号量。      
kernel.shmall = 107374182            
# 所有共享内存段相加大小限制 (建议内存的80%)，单位为页。      
kernel.shmmax = 274877906944         
# 最大单个共享内存段大小 (建议为内存一半), >9.2的版本已大幅降低共享内存的使用，单位为字节。      
kernel.shmmni = 819200               
# 一共能生成多少共享内存段，每个PG数据库集群至少2个共享内存段      
net.core.netdev_max_backlog = 10000      
net.core.rmem_default = 262144             
# The default setting of the socket receive buffer in bytes.      
net.core.rmem_max = 4194304                
# The maximum receive socket buffer size in bytes      
net.core.wmem_default = 262144             
# The default setting (in bytes) of the socket send buffer.      
net.core.wmem_max = 4194304                
# The maximum send socket buffer size in bytes.      
net.core.somaxconn = 4096      
net.ipv4.tcp_max_syn_backlog = 4096      
net.ipv4.tcp_keepalive_intvl = 20      
net.ipv4.tcp_keepalive_probes = 3      
net.ipv4.tcp_keepalive_time = 60      
net.ipv4.tcp_mem = 8388608 12582912 16777216      
net.ipv4.tcp_fin_timeout = 5      
net.ipv4.tcp_synack_retries = 2      
net.ipv4.tcp_syncookies = 1          
# 开启SYN Cookies。当出现SYN等待队列溢出时，启用cookie来处理，可防范少量的SYN攻击      
net.ipv4.tcp_timestamps = 1          
# 减少time_wait      
net.ipv4.tcp_tw_recycle = 0          
# 如果=1则开启TCP连接中TIME-WAIT套接字的快速回收，但是NAT环境可能导致连接失败，建议服务端关闭它      
net.ipv4.tcp_tw_reuse = 1            
# 开启重用。允许将TIME-WAIT套接字重新用于新的TCP连接      
net.ipv4.tcp_max_tw_buckets = 262144      
net.ipv4.tcp_rmem = 8192 87380 16777216      
net.ipv4.tcp_wmem = 8192 65536 16777216      
net.nf_conntrack_max = 1200000      
net.netfilter.nf_conntrack_max = 1200000      
vm.dirty_background_bytes = 409600000             
#  系统脏页到达这个值，系统后台刷脏页调度进程 pdflush（或其他） 自动将(dirty_expire_centisecs/100）秒前的脏页刷到磁盘      
#  默认为10%，大内存机器建议调整为直接指定多少字节      
vm.dirty_expire_centisecs = 3000                   
#  比这个值老的脏页，将被刷到磁盘。3000表示30秒。      
vm.dirty_ratio = 95                                
#  如果系统进程刷脏页太慢，使得系统脏页超过内存 95 % 时，则用户进程如果有写磁盘的操作（如fsync, fdatasync等调用），则需要主动把系统脏页刷出。      
#  有效防止用户进程刷脏页，在单机多实例，并且使用CGROUP限制单实例IOPS的情况下非常有效。        
vm.dirty_writeback_centisecs = 100                  
#  pdflush（或其他）后台刷脏页进程的唤醒间隔， 100表示1秒。      
vm.swappiness = 0      
#  不使用交换分区      
vm.mmap_min_addr = 65536      
vm.overcommit_memory = 0           
#  在分配内存时，允许少量over malloc, 如果设置为 1, 则认为总是有足够的内存，内存较少的测试环境可以使用 1 .        
vm.overcommit_ratio = 90           
#  当overcommit_memory = 2 时，用于参与计算允许指派的内存大小。      
vm.swappiness = 0                  
#  关闭交换分区      
vm.zone_reclaim_mode = 0           
# 禁用 numa, 或者在vmlinux中禁止.       
net.ipv4.ip_local_port_range = 40000 65535          
# 本地自动分配的TCP, UDP端口号范围      
fs.nr_open=20480000      
# 单个进程允许打开的文件句柄上限      
# 以下参数请注意      
# vm.extra_free_kbytes = 4096000      
# vm.min_free_kbytes = 2097152    # vm.min_free_kbytes 建议每32G内存分配1G vm.min_free_kbytes  
# 如果是小内存机器，以上两个值不建议设置      
# vm.nr_hugepages = 66536          
#  建议shared buffer设置超过64GB时 使用大页，页大小 /proc/meminfo Hugepagesize      
# vm.lowmem_reserve_ratio = 1 1 1      
# 对于内存大于64G时，建议设置，否则建议默认值 256 256 32  
```  
2、资源限制  
```  
vi /etc/security/limits.conf  
# nofile超过1048576的话，一定要先将sysctl的fs.nr_open设置为更大的值，并生效后才能继续设置nofile.      
* soft    nofile  1024000      
* hard    nofile  1024000      