    ```
    基于文本的进度条将显示`alpine-overlay1`服务部署的进度:
    ```
    overall progress: 1 out of 1 tasks 
    1/1: running   [===========================================>]
    verify: Service converged 
    ```
8.  Repeat the same `docker service create` command, but now specify `alpine-overlay2` as the service name:
    ```
    Machine1 ~$ docker service create -t --replicas 1 --network overlaynet1 --name alpine-overlay2 alpine:latest
    ```
    基于文本的进度条将再次显示服务部署的进度:
    ```
    overall progress: 1 out of 1 tasks 
    1/1: running   [===========================================>]
    verify: Service converged
    ```
    注意
    关于在 Docker swarm 中创建服务的更多细节可以在*第 9 章，Docker Swarm* 中找到。由于本练习的范围是网络，我们现在将重点讨论网络组件。
9.  From the `Machine1` node, execute the `docker ps` command to see which service is running on this node:
    ```
    Machine1 ~$ docker ps 
    ```
    将显示正在运行的容器。Docker 将智能地在 Docker 集群中的节点之间扩展容器。在本例中，来自`alpine-overlay1`服务的容器落在`Machine1`上。根据 Docker 部署服务的方式，您的环境可能会有所不同:
    ```
    CONTAINER ID    IMAGE           COMMAND     CREATED
      STATUS              PORTS             NAMES
    4d0f5fa82add    alpine:latest   "/bin/sh"   59 seconds ago
      Up 57 seconds                         alpine-overlay1.1.
    r0tlm8w0dtdfbjaqyhobza94p
    ```
10.  Run the `docker inspect` command to view the verbose details of the running container:
    ```
    Machine1 ~$ docker inspect alpine-overlay1.1.r0tlm8w0dtdfbjaqyhobza94p
    ```
    将显示正在运行的容器实例的详细信息。以下输出已被截断以显示`docker inspect`输出的`NetworkSettings`部分:
    ![Figure 6.24: Inspecting the alpine-overlay1 container instance ](img/B15021_06_24.jpg)
    图 6.24:检查高山覆盖 1 容器实例
    请注意，该容器的 IP 地址在您在`Machine1`上指定的子网内。
11.  On the `Machine2` instance, execute the `docker network ls` command to view the Docker networks available on the host:
    ```
    Machine2 ~$ docker network ls
    ```
    所有可用 Docker 网络的列表将显示在 Docker 主机上:
    ```
    NETWORK ID       NAME              DRIVER     SCOPE
    8c7755be162f     bridge            bridge     local
    28055e8c63a0     docker_gwbridge   bridge     local
    c62fb7ac090f     host              host       local
    8hm1ouvt4z7t     ingress           overlay    swarm
    6182d77a8f62     none              null       local
    60wqq8ewt8zq     overlaynet1       overlay    swarm
    ```
    请注意在`Machine1`上定义的`overlaynet1`网络也可以在`Machine2`上获得。这是因为使用`overlay`驱动程序创建的网络对 Docker 集群中的所有主机都可用。这使得可以使用该网络部署容器，以便在集群中的所有主机上运行。
12.  Use the `docker ps` command to list the running containers on this Docker instance:
    ```
    Machine2 ~$ docker ps
    ```
    将显示所有运行容器的列表。在本例中，`alpine-overlay2`服务中的容器落在了`Machine2`集群节点上:
    ```
    CONTAINER ID   IMAGE           COMMAND      CREATED
      STATUS              PORTS               NAMES
    53747ca9af09   alpine:latest   "/bin/sh"    33 minutes ago
      Up 33 minutes                           alpine-overlay2.1.ui9vh6zn18i48sxjbr8k23t71
    ```
    注意
    在您的示例中，服务所在的节点可能与此处显示的不同。Docker 根据各种标准(如可用的 CPU 带宽、内存和对已部署容器的调度限制)来决定如何部署容器。
13.  Use `docker inspect` to investigate the network configuration of this container as well:
    ```
    Machine2 ~$ docker inspect alpine-overlay2.1.ui9vh6zn18i48sxjbr8k23t71
    ```
    将显示详细的容器配置。该输出已被截断，以 JSON 格式显示输出的`NetworkSettings`部分:
    ![Figure 6.25: docker inspect output of the alpine-overlay2 container instance ](img/B15021_06_25.jpg)
    图 6.25: docker 检查高山覆盖 2 容器实例的输出
    请注意，该容器在`overlaynet1` `overlay`网络中也有一个 IP 地址。
14.  Since both services are deployed within the same `overlay` network but exist in two separate hosts, you can see that Docker is using the `underlay` network to proxy the traffic for the `overlay` network. Check the network connectivity between the services by attempting a ping from one service to the other. It should be noted here that, similar to static containers deployed in the same network, services deployed on the same network can resolve each other by name using Docker DNS. Use the `docker exec` command on the `Machine2` host to access an `sh` shell inside the `alpine-overlay2` container:
    ```
    Machine2 ~$ docker exec -it alpine-overlay2.1.ui9vh6zn18i48sxjbr8k23t71 /bin/sh
    ```
    这将把你放到`alpine-overlay2`容器实例的根外壳中。使用`ping`命令启动与`alpine-overlay1`容器的网络通信:
    ```
    / # ping alpine-overlay1
    PING alpine-overlay1 (172.45.0.10): 56 data bytes
    64 bytes from 172.45.0.10: seq=0 ttl=64 time=0.314 ms
    64 bytes from 172.45.0.10: seq=1 ttl=64 time=0.274 ms
    64 bytes from 172.45.0.10: seq=2 ttl=64 time=0.138 ms
    ```
    请注意，即使这些容器部署在两个独立的主机上，这些容器也可以使用共享的`overlay`网络通过名称相互通信。
15.  From the `Machine1` box, you can attempt the same communication to the `alpine-overlay2` service container. Use the `docker exec` command to access an `sh` shell on the `Machine1` box:
    ```
    Machine1 ~$ docker exec -it alpine-overlay1.1.r0tlm8w0dtdfbjaqyhobza94p /bin/sh
    ```
    这应该会将您放入容器内的根外壳中。使用`ping`命令启动与`alpine-overlay2`容器实例的通信:
    ```
    / # ping alpine-overlay2
    PING alpine-overlay2 (172.45.0.13): 56 data bytes
    64 bytes from 172.45.0.13: seq=0 ttl=64 time=0.441 ms
    64 bytes from 172.45.0.13: seq=1 ttl=64 time=0.227 ms
    64 bytes from 172.45.0.13: seq=2 ttl=64 time=0.282 ms
    ```
    再次注意，通过使用 Docker DNS，可以使用`overlay`网络驱动程序在主机之间解析`alpine-overlay2`容器的 IP 地址。
16.  Use the `docker service rm` command to delete both services from the `Machine1` node:
    ```
    Machine1 ~$ docker service rm alpine-overlay1
    Machine1 ~$ docker service rm alpine-overlay2
    ```
    对于这些命令中的每一个，服务名称将短暂出现，表示命令执行成功。在这两个节点上，`docker ps`将显示当前没有容器正在运行。
17.  Delete the `overlaynet1` Docker network by using the `docker rm` command and specifying the name `overlaynet1`:
    ```
    Machine1 ~$ docker network rm overlaynet1
    ```
    `overlaynet1`网络将被删除。
在本练习中，我们研究了 Docker 集群中两台主机之间的 Docker `overlay`网络。`Overlay`联网在 Docker 容器集群中非常有益，因为它允许在集群中的节点之间水平扩展容器。从网络角度来看，这些容器可以通过主机的物理网络接口上代理的服务网格直接相互对话。这不仅减少了延迟，而且通过利用 Docker 的许多功能(如 DNS)简化了部署。
既然我们已经了解了所有原生的 Docker 网络类型以及它们如何工作的示例，我们可以看看最近越来越流行的 Docker 网络的另一个方面。正如我们所看到的，由于 Docker 网络非常模块化，Docker 支持一个插件系统，允许用户部署和管理定制的网络驱动程序。
在下一节中，我们将通过从 Docker Hub 安装第三方网络驱动程序来了解非本地 Docker 网络如何工作。
# 非本地 Docker 网络
在本章的最后一节，我们将讨论非本地 Docker 网络。除了可用的本机 Docker 网络驱动程序之外，Docker 还支持自定义网络驱动程序，这些驱动程序可以由用户编写，也可以通过 Docker Hub 从第三方下载。自定义第三方网络驱动程序在需要非常特殊的网络配置的情况下，或者在容器网络需要以某种方式运行的情况下非常有用。例如，一些网络驱动程序为用户提供了设置有关访问互联网资源的自定义策略的能力，或者为容器化应用之间的通信定义白名单的能力。从安全、策略和审计的角度来看，这可能很有帮助。
在下面的练习中，我们将下载并安装 Weave Net 驱动程序，并在 Docker 主机上创建一个网络。Weave Net 是一个高度受支持的第三方网络驱动程序，它提供了对容器网状网络的出色可见性，允许用户创建复杂的服务网状基础架构，可以跨越多云场景。我们将从 Docker Hub 安装 Weave Net 驱动程序，并在我们在前面的练习中定义的简单 Swarm 集群中配置一个基本网络。
## 练习 6.05:安装和配置编织网 Docker 网络驱动程序
在本练习中，您将下载并安装 Weave Net Docker 网络驱动程序，并将其部署在您在上一个练习中创建的 Docker 集群中。Weave Net 是最常见和最灵活的第三方 Docker 网络驱动程序之一。使用编织网，可以定义非常复杂的网络配置，以实现基础架构的最大灵活性:
1.  Install the Weave Net driver from Docker Hub using the `docker plugin install` command on the `Machine1` node:
    ```
    Machine1 ~$ docker plugin install store/weaveworks/net-plugin:2.5.2
    ```
    这将提示您在安装编织网的机器上授予编织网权限。授予请求的权限是安全的，因为 Weave Net 要求他们在主机操作系统上正确设置网络驱动程序:
    ```
    Plugin "store/weaveworks/net-plugin:2.5.2" is requesting 
    the following privileges:
     - network: [host]
     - mount: [/proc/]
     - mount: [/var/run/docker.sock]
     - mount: [/var/lib/]
     - mount: [/etc/]
     - mount: [/lib/modules/]
     - capabilities: [CAP_SYS_ADMIN CAP_NET_ADMIN CAP_SYS_MODULE]
    Do you grant the above permissions? [y/N]
    ```
    按下 *y* 键，回答提示。织网插件应该安装成功。
2.  On the `Machine2` node, run the same `docker plugin install` command. All nodes in the Docker swarm cluster should have the plugin installed since all nodes will be participating in the swarm mesh networking:
    ```
    Machine2 ~$ docker plugin install store/weaveworks/net-plugin:2.5.2
    ```
    将显示权限提示。当提示继续安装时，用 *y* 响应:
    ```
    Plugin "store/weaveworks/net-plugin:2.5.2" is requesting 
    the following privileges:
     - network: [host]
     - mount: [/proc/]
     - mount: [/var/run/docker.sock]
     - mount: [/var/lib/]
     - mount: [/etc/]
     - mount: [/lib/modules/]
     - capabilities: [CAP_SYS_ADMIN CAP_NET_ADMIN CAP_SYS_MODULE]
    Do you grant the above permissions? [y/N]
    ```
3.  Create a network using the `docker network create` command on the `Machine1` node. Specify the Weave Net driver as the primary driver and the network name as `weavenet1`. For the subnet and gateway parameters, use a unique subnet that has not yet been used in the previous exercises:
    ```
    Machine1 ~$  docker network create --driver=store/weaveworks/net-plugin:2.5.2 --subnet 10.1.1.0/24 --gateway 10.1.1.1 weavenet1
    ```
    这应该会在 Docker 集群中创建一个名为`weavenet1`的网络。
4.  List the available networks in the Docker swarm cluster using the `docker network ls` command:
    ```
    Machine1 ~$ docker network ls 
    ```
    列表中应显示`weavenet1`网络:
    ```
    NETWORK ID     NAME             DRIVER
      SCOPE