### Performance and Scalability
To demonstrate the scalability of our system, we performed privacy-preserving logistic regression on the Gisette dataset, which contains 5000 features and up to 1,000,000 samples, using a LAN network. The first protocol took 268.9 seconds, while the second protocol required 623.5 seconds. The trained model achieved an accuracy of 97.9% on the testing dataset.

To our knowledge, this is the first implementation of a scalable system for privacy-preserving logistic regression in this security model.

### Protocol Performance
The performance of our protocols is summarized in the following table:

| Protocol | Offline (s) | Online (s) |
|----------|-------------|------------|
| Protocol 1 | 290,000* | 4239.7 (RELU), 653.0 (Square) |
| Protocol 2 | 14951.2 | 10332.3 (RELU), 4260.3 (Square) |

*Note: The offline phase times are estimated due to the high computational cost.

### Experiments for Neural Networks
We also implemented our privacy-preserving protocol for training a neural network on the MNIST dataset. The neural network consists of two hidden layers, each with 128 neurons. We experimented with both the RELU and square activation functions in the hidden layers, and used our proposed alternative to the softmax function in the output layer. The neural network is fully connected, and the cost function is the cross-entropy function. The labels are represented as one-hot vectors with 10 elements, where the element indexed by the digit is set to 1, and the others are 0.

The system was run on a LAN network, and the performance is summarized in Table III. The batch size \( |B| \) was set to 128, and the training converged after 15 epochs.

| Activation Function | Offline (s) | Online (s) |
|---------------------|-------------|------------|
| RELU                | 290,000*    | 4239.7     |
| Square              | 320,000*    | 653.0      |

*Note: The offline phase times are estimated due to the high computational cost.

When using the RELU function, the online phase of our first protocol took 4239.7 seconds, with the offline phase using OT taking approximately 2.9×10^5 seconds. Using the square function, the online phase was significantly improved, requiring only 653.0 seconds. However, the offline phase time increased, indicating a trade-off between the two phases. Using client-aided multiplication triplets, the offline phase was reduced to about 1.5×10^4 seconds, with a slight overhead on the online phase.

### Training on WAN Setting
Due to the high number of interactions and communication requirements, training the neural network on a WAN setting is not yet practical. For one round of forward and backward propagation, the online phase takes 30.52 seconds using the RELU function, and the offline phase takes around 2200 seconds using the LHE-based approach. The total running time is linear in the number of rounds, which is around 7000 in this case.

### Model Accuracy
The model trained by our protocol achieved 93.4% accuracy using the RELU function and 93.1% accuracy using the square function. In practice, other types of neural networks, such as convolutional neural networks, may achieve better accuracy for image processing tasks. These networks use 2-D convolutions instead of fully connected layers, and while they can be supported, improving performance with techniques like Fast Fourier Transform within secure computation remains an open question. Experimenting with various MPC-friendly activation functions is another avenue for future research.

### Acknowledgements
We thank Jing Huang from Visa Research for helpful discussions on machine learning and Xiao Wang from the University of Maryland for his assistance with the EMP toolkit. This work was partially supported by NSF grants #5245250 and #5246010.

### References
[References listed here]

### Appendix A: The UC Framework
An execution in the UC framework involves a collection of (non-uniform) interactive Turing machines. In this work, we consider an admissible and semi-honest adversary \( A \). The parties exchange messages according to a protocol. Protocol inputs of uncorrupted parties are chosen by an environment machine. Uncorrupted parties also report their protocol outputs to the environment. At the end of the interaction, the environment outputs a single bit. The adversary can interact arbitrarily with the environment, acting as a dummy adversary that forwards all received protocol messages to the environment and acts in the protocol as instructed by the environment.

Security is defined by comparing a real and ideal interaction. Let \( \text{REAL}[Z, A, \pi, \lambda] \) denote the final (single-bit) output of the environment \( Z \) when interacting with adversary \( A \) and honest parties who execute protocol \( \pi \) on security parameter \( \lambda \).

In the ideal interaction, parties simply forward the inputs they receive to an uncorruptable functionality machine and forward the functionality’s response to the environment. Hence, the trusted functionality performs the entire computation on behalf of the parties. The target ideal functionality \( F_{\text{ml}} \) for protocols is described in Figure 3. Let \( \text{IDEAL}[Z, S, F_{\text{ml}}, \lambda] \) denote the output of the environment \( Z \) when interacting with adversary \( S \) and honest parties who run the dummy protocol in the presence of functionality \( F \) on security parameter \( \lambda \).

We say that a protocol \( \pi \) securely realizes a functionality \( F_{\text{ml}} \) if for every admissible adversary \( A \) attacking the real interaction, there exists an adversary \( S \) (called a simulator) attacking the ideal interaction, such that for all environments \( Z \), the following quantity is negligible (in \( \lambda \)):

\[
\left| \Pr[\text{IDEAL}[Z, S, F_{\text{ml}}, \lambda] = 1] - \Pr[\text{REAL}[Z, A, \pi, \lambda] = 1] \right|
\]

Intuitively, the simulator must achieve the same effect (on the environment) in the ideal interaction that the adversary achieves in the real interaction. Note that the environment’s view includes all of the messages that honest parties sent to the adversary as well as the outputs of the honest parties.

### Appendix B: Proof of Small Truncation Error
**Theorem.** In the field \( \mathbb{Z}_{2^l} \), let \( x \in [0, 2^{lx}] \cup [2^l - 2^{lx}, 2^l) \), where \( l > lx + 1 \) and given shares \( \langle x \rangle_0, \langle x \rangle_1 \) of \( x \), let \( \langle \lfloor x \rfloor \rangle_0 = \lfloor \langle x \rangle_0 \rfloor \) and \( \langle \lfloor x \rfloor \rangle_1 = 2^l - \lfloor 2^l - \langle x \rangle_1 \rfloor \). Then with probability \( 1 - 2^{lx + 1 - l} \), \( (\langle \lfloor x \rfloor \rangle_0, \langle \lfloor x \rfloor \rangle_1) \in \{\lfloor x \rfloor - 1, \lfloor x \rfloor, \lfloor x \rfloor + 1\} \), where \( \lfloor \cdot \rfloor \) denotes truncation by \( l_D \leq lx \) bits.

**Proof.** Let \( \langle x \rangle_0 = x + r \mod 2^l \), where \( r \) is uniformly random in \( \mathbb{Z}_{2^l} \), then \( \langle x \rangle_1 = 2^l - r \). We decompose \( r \) as \( r_1 \cdot 2^{l_D} + r_2 \), where \( 0 \leq r_2 < 2^{l_D} \) and \( 0 \leq r_1 < 2^{l - l_D} \). We prove that if \( 2^{lx} \leq r < 2^l - 2^{lx} \), then \( \text{Rec}(\langle \lfloor x \rfloor \rangle_0, \langle \lfloor x \rfloor \rangle_1) \in \{\lfloor x \rfloor - 1, \lfloor x \rfloor, \lfloor x \rfloor + 1\} \). Consider the following two cases.

**Case 1:** If \( 0 \leq x \leq 2^{lx} \), then \( 0 < x + r < 2^l \) and \( \langle x \rangle_0 = x + r \), without modulo. Let \( x = x_1 \cdot 2^{l_D} + x_2 \), where \( 0 \leq x_2 < 2^{l_D} \) and \( 0 \leq x_1 < 2^{lx - l_D} \). Then we have \( x + r = (x_1 + r_1) \cdot 2^{l_D} + (x_2 + r_2) = (x_1 + r_1 + c) \cdot 2^{l_D} + (x_2 + r_2 - c \cdot 2^{l_D}) \), where the carry bit \( c = 0 \) if \( x_2 + r_2 < 2^{l_D} \) and \( c = 1 \) otherwise. After the truncation, \( \langle \lfloor x \rfloor \rangle_0 = \lfloor x + r \rfloor = x_1 + r_1 + c \) and \( \langle \lfloor x \rfloor \rangle_1 = 2^l - r_1 \). Therefore, \( \text{Rec}(\langle \lfloor x \rfloor \rangle_0, \langle \lfloor x \rfloor \rangle_1) \in \{\lfloor x \rfloor - 1, \lfloor x \rfloor, \lfloor x \rfloor + 1\} \).