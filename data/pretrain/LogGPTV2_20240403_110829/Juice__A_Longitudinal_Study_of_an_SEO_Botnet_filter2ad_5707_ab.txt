fake anti-virus, malware, etc., all of which can earn money
for the botmaster. The SEO kit then returns this target URL
together with redirect JavaScript code as the HTTP response
to trigger the user’s browser to automatically visit the target.
The SEO kit also uses its cloaking mechanism to pro-
vide backdoor access to the compromised site for the bot-
master. To identify the botmaster, the SEO kit inspects the
User-Agent ﬁeld in the HTTP request headers, looking
for a speciﬁc, unique phrase as the sole means of authen-
tication. With this authentication token, the botmaster has
the ability to read ﬁles from the local hard disk of the site,
fetch URLs while using the compromised site as a proxy,
3It appears that the botmaster is only interested in poisoning Google’s
search results, as they solely target the Googlebot crawler—a trend also
observed in previous cloaking studies [19].
4Spinning is another black hat SEO technique that rephrases and rear-
ranges text to avoid duplicate content detection.
run scripts pulled from the C&C, etc., all controlled through
parameters to HTTP GET requests.
Finally, if the visitor does not match either the Googlebot
crawler, a user clicking on a search result, or the backdoor,
then the SEO kit returns the original page from the site be-
fore it was compromised. Thus, site owners who visit their
pages directly will be unaware of the compromise.
3.2.2 Directory Server
The directory server’s only role is to return the location of
the C&C server, either as a domain or IP address. Although
relatively simple in functionality, it is the ﬁrst point of con-
tact from the compromised Web sites in the botnet and per-
forms the important function of rendezvousing a compro-
mised site with the C&C server. As a result, the directory
server must be reachable and available and the SEO kit uses
a typical multi-step process to locate it. The SEO kit will
ﬁrst attempt to reach the directory server through a hard-
coded domain from the SEO kit, then a hard-coded IP ad-
dress, before ﬁnally resorting to a backup domain genera-
tion algorithm (DGA) calculated using a time-based func-
tion. The directory server appears to have received little
takedown pressure, though. We probed the potential backup
domains up to a year into the future and found that no
backup domains were registered, suggesting that this ﬁnal
fallback has not been necessary.
3.2.3 Command Server
The C&C server acts as a centralized content server where
the botmaster stores data that the compromised sites will
eventually pull down. The content is mostly transient in
nature, and includes the trending search terms to target with
SEO, the redirect URLs returned to users leading them to
scams, and even the driver component of the SEO kit. This
architecture allows the botmaster to make a single update
that eventually propagates to all active nodes of the botnet.
3.3 SEO Kit Evolution
Examining the SEO kit’s source revealed a variety of
comments in the code. These comments were primarily
written in Russian, suggesting the SEO campaign is imple-
mented and operated by Russian speakers. From the trans-
lated comments we saw hints of the existence of revious
versions of the SEO kit in the wild, such as:
/**
* v7.2 (14.09.11)
* - Automatic cleaning of other malware
*
* v7.1 (05.09.11)
* - Re-written for object oriented model
Date
Version
Capability
Aug 6 2010
page v1
Sep 22 2010
Oct 6 2010
index v1.1
page v2.1
Mar 29 2011
page v4
Jul 15 2011
Aug 18 2011
index v6
page v5
v7
Sep 14 2011
Sep 27 2011
Oct 28 2011
v7.2
vOEM
vMAC
Mar 06 2012
v8
Build SEO page using Bing search results.
User-Agent cloaking against Google, Yahoo, and Bing while ignoring “site:” queries.
Redirect trafﬁc from Google, Yahoo, Bing search using JS through gogojs.net.
Reverse DNS cloaking against Googlebot.
Use statistical model (# links, # images) to build SEO page.
Also redirect trafﬁc from Google Image Search.
Redirect trafﬁc with HTTP 30X and use cookie to redirect only once a day per visitor.
Modify .htaccess to rewrite URLs and use Google Suggest terms for cross linking.
Reverse DNS cloaking only against Googlebot.
Hotlink images from Bing Image Search to help build SEO page.
Proxy images instead of hotlinking.
index + page code branches merged.
Morph proxied images.
Redirect trafﬁc using JS.
Clean other malware.
OEM terms targeted.
Mac OS X OEM terms targeted for low frequency trafﬁc.
Redirect trafﬁc from any Google service due to referer policy change.
Only redirect Google Image Search trafﬁc.
Table 1: Timeline of SEO kit versions along with the capabilities added in each version. The SEO techniques
used are colored blue. The redirect mechanisms and policies for funneling trafﬁc are colored purple. The
various cloaking techniques and policies are colored green. And orange capabilities focus speciﬁcally on
Google Image Search poisoning. The remaining are purely informational.
These indications of previous versions of the SEO kit
motivated us to search for them using identifying substrings
unique to the SEO kit code, such as “GR HOST ID”. We
discovered that previous versions were posted on the Web
by site owners who were seeking assistance in deciphering
the injected code on their site. After verifying older versions
existed, we were able to download additional previous ver-
sions of the SEO kit from the C&C server by reverse engi-
neering the protocol for downloading the driver and fuzzing
likely inputs. In the end, we were able to download nearly
all major SEO kit revisions since August 2010.
As seen in the sample above, the comments from each
version of the SEO kit have a date and a short log message
about the update similar to a version control system. From
these comments, we reconstructed the developments in the
SEO kit and thus the evolution of the SEO botnet and the
botmaster’s SEO strategies over two years. Table 1 sum-
marizes our ﬁndings by presenting changes in capabilities
with the corresponding version and date. Below are some
highlights, many of which conﬁrmed our early theories.
Structure. The compromised sites were at one time di-
vided into indexers, which SEO-ed search engine visitors,
and doorways, which redirected users, each with different
cloaking mechanisms and policies. Starting August 2011,
however, the code was merged into a single SEO kit with a
uniﬁed cloaking mechanism and policy.
Cloaking.
Initially, the doorways and indexers used
User-Agent cloaking, where the server examines the
User-Agent ﬁeld in the HTTP request headers to iden-
tify user trafﬁc and avoid detection. Speciﬁcally, the door-
ways used the cloaking mechanism to identify visitors who
clicked through one of the three largest search engines:
Google, Yahoo, Bing. By late September 2010, however,
the indexers implemented the reverse DNS cloaking mech-
anism as described above. Similarly, by late March 2011
the doorways used the same cloaking mechanism and be-
gan targeting user trafﬁc from Google exclusively.
Redirection. The redirection mechanism, used to fun-
nel user trafﬁc to scams, also changes signiﬁcantly over
time. Originally, the doorways redirected user trafﬁc us-
ing JavaScript through an intermediary site, gogojs.net,
which we suspect served as a trafﬁc aggregation hop to col-
lect statistics. By October 2010, the doorway redirected
trafﬁc via the HTTP 30* status with a cookie to limit visi-
tors to one visit per day. Then in August 2011, the SEO kit
returns to using JavaScript to redirect user trafﬁc.
SEO. The SEO models and policies, used by the SEO
kit to manipulate search result ranking, also change heavily
over time. In the earliest version we have, the SEO page re-
turned to search engine crawlers was generated from Bing
search results. Then the SEO kit began using a statistical
model when building an SEO page, requiring that the SEO
page contents be composed of various percentages of text,
images, and links. In late March 2011, the SEO kit used
Google Suggest to target long-tail search terms. Then in late
September 2011 it began to poison search results for OEM
queries. And by late October 2011, the SEO kit started poi-
soning Mac OEM queries, also long-tail search terms.
Image Search. One of the surprising ﬁndings from the
SEO kit code is the amount of effort placed in poisoning
Google Image Search. The doorways ﬁrst started redirect-
ing user trafﬁc from Google Image Search in October 2010.
In July 2011, the indexers hotlinked images from Bing to
help build the SEO page and shortly thereafter the door-
ways began proxying images instead of hotlinking. By Au-
gust 2011, the SEO kit began morphing the images, such as
inverting them, to avoid duplicate detection. And currently,
since March 2012, the SEO kit only redirects trafﬁc from
Google Image Search.
4 Methodology
We use data from three crawlers to track the SEO bot-
net and monitor its impact: (1) a botnet crawler for tracking
compromised Web sites in the botnet and downloading SEO
data from the C&C server, (2) a search crawler that identi-
ﬁes poisoned search results in Google, enabling us to evalu-
ate the effectiveness of the botnet’s black hat SEO, and (3) a
redirection crawler that follows redirection chains from the
doorway pages linked from poisoned search results to the
ﬁnal landing pages of the scams the botmaster uses to mon-
etize user trafﬁc. Table 2 summarizes these data sets, and
the rest of this section describes each of these crawlers and
the information that they provide.
4.1 Odwalla Botnet Crawler
We implemented a botnet crawler called Odwalla to
track and monitor SEO botnets for this study. It consists
of a host crawler that tracks compromised Web sites and a
URL manager for tracking URL to site mappings.
Host Crawler. The host crawler tracks the compromised
Web sites that form the SEO botnet. Recall from Sec-
tion 3.2.1 that the SEO kit provides a backdoor on compro-
mised sites for the botmaster through the HTTP request’s
User-Agent ﬁeld. While this backdoor provides access
to many possible actions, the default response is a simple
diagnostic page with information about the compromised
Web site such as:
Version: v MAC
Cache ID: v7mac_cache
Host ID: example.com
1 (28.10.2011)
These ﬁelds show the basic conﬁguration of the SEO kit:
the version running on the compromised site, the version of
the cache it is running, and the compromised site’s host-
name. The diagnostic page also reports a variety of ad-
ditional information, such as the relative age of the SEO
kit (for caching purposes), various capabilities of the Web
host (e.g., whether certain graphics libraries are installed),
and information about the requestor and request URL (e.g.,
whether the visitor arrived via Google Search). While the
majority of this information allows the botmaster to debug
and manage the botnet, we use the diagnostic page to both
conﬁrm a site’s membership in the botnet and monitor the
status of the compromised site.
The host crawler maintains a set of potentially compro-
mised sites together with site metadata, such as the repre-
sentative probe URL for a site and the last time it conﬁrmed
the site as compromised. The probe URL is the URL that
the host crawler visits for each potentially compromised
site. Since a given site may have many URLs that link to
different pages, all managed by the same SEO kit, the host
crawler maintains one active probe URL per site to limit
crawl trafﬁc. As URLs expire, a URL manager (described
below) provides alternate probe URLs for a site. The host
crawler visits each probe URL twice, once to fetch the di-
agnostic page and once to fetch the SEO page—the page
returned to search engines—containing the cross links.
The last time the site was detected as compromised inﬂu-
ences the crawling rate. The host crawler visits all sites that
were either previously conﬁrmed as compromised, using
the diagnostic page mechanism described above, or newly
discovered from the cross links. It crawls these sites at a
four-hour interval. For the sites that were not conﬁrmed as
compromised, for example because it could not fetch the di-
agnostic page, the host crawler visits them using a two-day
interval as a second chance mechanism. If it does not detect
a site as compromised after eight days, it removes the site
from the crawling set. This policy ensures that we have near
real time monitoring of known compromised sites, while
limiting our crawling rate of sites where we are uncertain.
We used three methods to bootstrap the set of hosts for
Odwalla to track. First, in October 2011 and then again in
January 2012, we identiﬁed candidate sites using manual
queries in Google for literal combinations of search terms
targeted by the SEO botnet. Since the terms formed unusual
combinations, such as “herman cain” and “cantaloupe”,
typically only SEO pages on compromised sites contained
them. Second, since these pages contained cross links to
other compromised sites for manipulating search ranking
algorithms, we added the cross links as well. Interestingly,
these cross links were insufﬁcient for complete bootstrap-
ping. We found multiple strongly connected components in
the botnet topology, and starting at the wrong set of nodes
could potentially only visit a portion of the network. Fi-
nally, we modiﬁed the SEO kit to run our own custom bots
that inﬁltrated the botnet. These custom bots issued requests
to the C&C server to download targeted search terms and
links to other hosts in the botnet, providing the vast major-
ity of initial set of bots to track. Once bootstrapped, the host
Odwalla
Dagger
Trajectory
Time Range
Data Collected
Data Perspective
Contribution
October 1011 – June 2012
Diagnostic pages and cross links
from nodes of SEO campaign.
SEO Campaign botmaster.
Characterize support infrastructure
of SEO campaign.
April 2011 – August 2011
Cloaked search results in trending
searches over time.
Users of search engines.
Assess efﬁcacy of SEO campaign.
chains
April 2011 – August 2011
Redirect
from cloaked
search results in trending searches.
Users of search engines.
Analyze landing scams.
Table 2: The three data sets we use to track the SEO botnet and monitor its impact.
crawler used the cross links embedded in the SEO pages re-
turned by compromised sites to identify new bots to track.