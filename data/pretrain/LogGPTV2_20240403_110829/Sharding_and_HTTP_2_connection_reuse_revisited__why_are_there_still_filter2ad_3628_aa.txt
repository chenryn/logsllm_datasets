title:Sharding and HTTP/2 connection reuse revisited: why are there still
redundant connections?
author:Constantin Sander and
Leo Bl&quot;ocher and
Klaus Wehrle and
Jan R&quot;uth
Sharding and HTTP/2 Connection Reuse Revisited:
Why Are There Still Redundant Connections?
Constantin Sander, Leo Blöcher, Klaus Wehrle, Jan Rüth
Communication and Distributed Systems, RWTH Aachen University, Aachen, Germany
{sander,bloecher,wehrle,rueth}@comsys.rwth-aachen.de
ABSTRACT
HTTP/2 and HTTP/3 avoid concurrent connections but instead
multiplex requests over a single connection. Besides enabling new
features, this reduces overhead and enables fair bandwidth sharing.
Redundant connections should hence be a story of the past with
HTTP/2. However, they still exist, potentially hindering innovation
and performance. Thus, we measure their spread and analyze their
causes in this paper. We find that 36 % - 72 % of the 6.24 M HTTP
Archive and 78 % of the Alexa Top 100k websites cause Chromium-
based webbrowsers to open superfluous connections. We mainly
attribute these to domain sharding, despite HTTP/2 efforts to revert
it, and DNS load balancing, but also the Fetch Standard.
CCS CONCEPTS
• Networks → Network measurement; Application layer pro-
tocols; Network architectures.
KEYWORDS
HTTP/2, Connection Reuse, Domain Sharding, Fetch Standard
ACM Reference Format:
Constantin Sander, Leo Blöcher, Klaus Wehrle, Jan Rüth. 2021. Sharding
and HTTP/2 Connection Reuse Revisited: Why Are There Still Redundant
Connections?. In ACM Internet Measurement Conference (IMC ’21), November
2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3487552.3487832
1 INTRODUCTION
Internet standards of the past decade, such as HTTP/2 [2] and
HTTP/3 [3], have paved the Web’s way to use a single transport
connection. While HTTP/1.1 needs multiple concurrent connec-
tions to achieve parallelism, its successors can multiplex content
over a single connection. A single connection has many advan-
tages on paper, e.g., connection-establishment overheads such as
the 3-way-handshake + additional TLS handshakes or growing the
congestion window (slow start) diminish. Further, it decreases re-
source use as fewer connections have to be maintained, especially
at content-providers with many users. But focussing on a single
connection also offered further innovation potential within HTTP:
header compression and resource prioritization became viable, and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IMC ’21, November 2–4, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9129-0/21/11...$15.00
https://doi.org/10.1145/3487552.3487832
new features such as server push could be tested. Research found
that these new features can significantly improve web performance
when used correctly [25, 27, 28]. Here, multiple connections can
even degrade effectiveness as, for instance, prioritization does not
span across connections and priorities lose their meaning.
However, simply updating a webserver to use the most recent
standard does not revert previous performance tricks. E.g., domain
sharding, i.e., spreading content over various subdomains, was seen
as a valid way to gain more connections and parallelism in the
age of HTTP/1.1. The structures created by these practices are still
present [25], as even CDNs do not reduce redundancy if domain
sharding is structurally still enforced by spreading resources over
domains. A recognized problem, the HTTP/2 and HTTP/3 stan-
dards1 try to revert domain sharding with their Connection Reuse
mechanism when a subdomain resolves to an IP for which a connec-
tion is already established. Nonetheless, a 2016 study [13] showed
that browsers still tended to open multiple HTTP/2 connections to
the same domain. Roughly one-third of all flows were duplicates
although HTTP/2 should use a single connection [2, Section 9.1] –
the root causes remained vague.
In this work, we revisit the occurrence of redundant connections
w.r.t. HTTP/2 to discover if the problem persists and, more impor-
tantly, identify why Connection Reuse is ineffective and what can
be done for rectification. We do not solely focus on domain sharding
but generally look at connections that could have been avoided.
To this end, our study is two-fold: We analyze data provided in
the HTTP Archive [16] (visiting millions of websites per month
to collect web statistics) to get an idea of the scale of redundant
connections and further perform additional measurements on the
Alexa Top 100k to pinpoint and attribute redundant connections.
Specifically, our paper contributes and finds the following:
• We elaborate why Connection Reuse, a seemingly simple reversal
of domain sharding, can miss its target in practice.
• We present and apply a method to quantify the causes of redun-
dant connections of Chromium browsers.
• Redundancy still persists; 2.26 M to 4.49 M of the 6.24 M HTTP
Archive and 77.88 k of Alexa Top 100k websites are affected.
• DNS load-balancing is the leading cause, then privacy-driven
Web standards and domain sharding with separate certificates.
• We find that only few parties cause the majority of redundancy.
2 MULTIPLE CONNECTIONS AND HTTP
Modern web pages consist of a multitude of resources [11]. However,
HTTP/1.1 [8] uses a single TCP connection to send one resource
after another. I.e., a delayed resource, e.g., due to database accesses,
can delay subsequent resources hindering the rendering process.
1It is easier to include a workaround in a standard that is eventually implemented than
to get operators to change their practices.
292
IMC ’21, November 2–4, 2021, Virtual Event, USA
Sander et al.
2.1 HTTP/1 – Parallel Connections
Hence, browsers open six or more parallel TCP connections [10, 13,
27] to achieve more parallelism with HTTP/1.1. Website operators
employed domain sharding [10] to stretch these limits by spreading
resources over more domains causing additional connections, e.g.,
images can be moved to a subdomain (img.example.tld). How-
ever, each connection has its costs. For instance, all connections
have to be maintained on client- and server-side. While negligible
for clients, the overhead for servers maintaining several thousand
connections can easily build up. Similarly, latency penalties occur,
e.g., with TCP, 1 RTT is spent on connection establishment, increas-
ing to 2 or 3 RTTs when TLS is added. Additionally, congestion
control (CC) slow starts with every new connection, which adds
several RTTs of latency until the full throughput can be achieved.
2.2 HTTP/2 and HTTP/3 – One Connection
As latency is essential for web performance, HTTP/2 [2] and its
successor HTTP/3 [3] follow the goal to avoid new connections.
They use multiple streams that are multiplexed over a single con-
nection to allow for parallel transfers of resources. For this, HTTP/2
implements stream semantics on top of TCP, while HTTP/3 uses
the novel transport QUIC and its integrated streams. This enables
better efficiency on server-side, but also innovations such as header
compression and fine-grained scheduling of data – to prioritize
important resources over less critical ones – on protocol-level.
2.2.1 Effects of Redundancy. These features were developed as-
suming that the Web would use only a single connection. When
data remains split across connections, overheads are not saved, and
prioritization and header compression cannot boost performance.
In that regard, Bocchi et al. [4] find that fewer connections usu-
ally increase the QoE with HTTP/2 in real-world settings. Wang et
al. [26] find generally worse page load times (PLTs) with multiple
connections for HTTP/2’s predecessor SPDY. However, they find
improved PLTs for high packet loss. Similarly, Goel et al. [9] find
that multiple connections can worsen HTTP/2’s PLTs for many
small objects, but improve them for few, large objects – especially
when loss is high – which they ascribe to growing the cumula-
tive congestion window (ccwnd) faster. While not discussed by
the authors, this, however, allocates more than a fair bandwidth
share compared to a single connection. Manzoor et al. [14] also find
multiple connections to be beneficial for HTTP/2 under high loss.
They additionally attribute this to better exploitation of ECMP load-
balancing and TCP’s head-of-line (HOL) blocking during packet
loss, pausing all HTTP/2 streams. However, the authors also find
that Google QUIC (predecessor of IETF QUIC and HTTP/3) alle-
viates the HOL issues and surpasses HTTP/2 in many scenarios.
Marx et al. [15] also find HTTP/2 to benefit from multiple connec-
tions and higher ccwnds for large resources. Nevertheless, they see
that HTTP/2’s PLTs for many small resources worsen when using
multiple connections and that header compression is less effective
as the compression dictionary has to be bootstrapped again.
We argue that with QUIC and HTTP/3, allowing easily tunable
CC and removing suffering from HOL blocking, a single connection
might be the desired state in all scenarios to best exploit its features
and performance. Moreover, fewer connections mean fewer com-
peting CCs and potentially better fairness. Also, content-providers
probably benefit from fewer connections increasing the efficiency
of their servers due to reduced connection maintenance overhead.
2.2.2 Connection Reuse. Nevertheless, techniques such as domain
sharding intend to open multiple connections in all cases, and
Varvello et al. [25] find that websites switching to HTTP/2 do not
adapt but still place resources across domains.
HTTP/2 [2] hence specifies Connection Reuse to revert domain
sharding: Requests for domain D may be sent over an existing
connection A if D resolves to the same destination IP that A is using
(+ matching ports) and if A’s TLS certificate includes D (e.g., via
Subject Alternative Name, SAN). I.e., an image residing on the img.
subdomain-shard hosted on the same server as the root document
can reuse an existing connection. HTTP/3 inherits this mechanism.
However, it is unknown whether this mechanism is effective.
Varvello et al. [25] find fewer connections when switching to HTTP/2
but also that domain sharding is still used. They do not analyze
whether these domains were correctly reused or whether more
connections could have been avoided. On the other hand, Manzoor
et al. [13] find multiple HTTP/2 connections for the same domain
(in total 33%), i.e., connection reuse seems to be ineffective here.
Later, the authors note that a fixed Chromium bug introduced this
behavior [14]. However, we still see redundant connections for
which the precise reasons and the extent are unknown.
We thus recognize a need to again look at Connection Reuse, do-
main sharding and why redundant connections still exist in the real
world. Hence, we devise a methodology to analyze redundant con-
nections but we will first present different combinations of website
and network structure as well as browser behavior that we have
identified to lead to redundant connections.
3 CAUSES OF MULTIPLE CONNECTIONS
Connection Reuse [2] depends on two factors: the destination IP
and the domain. If an already opened connection uses the same
destination IP as a new request, the connection may be reused if,
additionally, its certificate includes the domain. Thus, a browser
opening connections for a given domain and IP can have different
reasons, which we visualize in Figure 1 and present in the following.
Unknown 3𝑟𝑑 Party Requests: If the IP differs and open connec-
tions do not include the new domain in their certificates, as for
third party resources, where the third party has not been contacted
before, a new connection has to be opened. We argue these cannot
be avoided in the HTTP context, but would require a redesign of a
website such that we ignore them in the following.
Different Certificates (CERT): Also, if the IP is the same, the do-
main might not be included in previous certificates. I.e., if operators
use domain sharding and different certificates for their domains,
HTTP/2 still opens new connections.
Different IPs (IP): Vice versa, the domain can be included in pre-
vious certificates, but the request’s destination IP differs. I.e., really
distributed resources, but also domain sharding with one certificate
and differing IPs in the DNS can still open new connections.
Fetch Standard (CRED): Even if both factors match, browsers can
refuse Connection Reuse when following the WHATWG Fetch Stan-
dard [24]. Depending, e.g., on a request’s tainting type (changes,
e.g., for cross-origin resource sharing / CORS requests such as
293
Sharding and HTTP/2 Connection Reuse Revisited:
Why Are There Still Redundant Connections?
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 1: Visualization of the four root causes that lead browsers to create a new HTTP/2 connection.
font downloads across domains) and credentials mode, the Fetch
Standard decides whether credentials (such as cookies) should be
included in the request. A browser then only reuses a connection if
its previous requests also included (or vice versa did not include)
credentials (cf. §4.6, §4.7, §2.5 of [24]). Otherwise, the existing con-
nection would be tainted with identifying information, or vice versa,
a new request would be tied to previously used credentials [22]. This
privacy-enhancing measure can, of course, lead to a new connec-
tion to the same IP and SAN-included domain, as is discussed by the
author [22]. While Chromium implements this mechanism [12], its
necessity is discussed [22] and, e.g., Firefox does not follow it [23].
In essence, the discussion in [22] revolves around the privacy effect
of opening a new connection to the same server as identifying in-
formation could also be injected into Ajax request URLs and servers
could also map user identities via, e.g., user IPs such that the actual
privacy improvements are little to non-existing.
Exception: Explicitly Excluded Domains. Additionally, Web
servers can announce no support for a domain via HTTP Status
421 [2] or HTTP ORIGIN Frames [18], disabling connection reuse.
In the following, we present our methodology that allows at-
tributing redundancies to these root causes.
4 METHODOLOGY
To analyze real-world websites for redundant connections w.r.t.
HTTP/2, we rely on Chromium browsers to record their connec-
tions when visiting these websites. We then analyze and classify
the connections accordingly.
4.1 Connection Analysis
For the analysis, we group connections w.r.t. HTTP/2 (in the fol-
lowing also HTTP/2 sessions) by their IP to find causes CERT and
CRED, and, for IP and CRED, group their initially used domain name
and certificate SANs of previous connections. Domains which web
servers explicitly exclude, e.g., via HTTP status 421, are ignored.
Moreover, we intercept the corner case of same-initial-domain re-
quests on different IPs. Otherwise, these would be classified as IP,
but only happen when CRED forbids reuse and multiple IPs are
announced via DNS. We hence mark these cases as CRED.
Inherently, connections can be redundant due to multiple causes.
For example, when we see four successively opened same-IP con-
nections, where #1 and #3 use certificate A and #2 and #4 use B,
we find three redundant connections in total but attribute them at
time of connection establishment to three times type CERT (#2 is
redundant to #1, #3 is redundant to #2, #4 is redundant to #1 and
#3) and two times type CRED (#3 is redundant to #1, #4 to #2).
4.2 Chromium-based Connection Data
To gather the actual session information, we base our analysis
on Chromium / Chrome browsers visiting websites. We focus on
Chrome as it makes up around 2/3 of the browser market share [21].
Additionally, more and more browsers build on Chromium. In total,
we use two different sources: We rely on the HTTP Archive’s [16]
desktop browser crawls from April 2021 (6.24 M websites) and also
use Chromium to visit the Alexa Top 100k of April 20th, 2021.
4.2.1 HTTP Archive. The HTTP Archive [16] crawls the top web-
sites of the Chrome User Experience Reports using Chrome, pro-
viding aggregated statistics and HAR files with detailed page-load
information. For every website, the landing page is loaded 3 times
and the HAR file for the median load time is saved. We parse these
HAR files to identify HTTP/2 requests on the same sessions (by
socket / connection ID) to reconstruct the HTTP/2 session lifecycle.
We ignore HTTP/3 / QUIC requests as these all have socket ID 0, i.e.,
we cannot distinguish between the connections. Moreover, HAR
files only give request-level information, i.e., we can determine the
start time of a connection by the first request but cannot determine
the end time precisely. Hence, we evaluate two cases: One (end-
less), where connections are kept open, and one (immediate), where
connections are closed after the last request. The latter is probably
atypical, as long flows are desired, but we still evaluate it to give a
lower bound, while the former likely overestimates flow durations
and redundancy counts.
4.2.2 Own Measurements. To gather more detailed information,
we complement the HTTP Archive data with additional measure-
ments. We leverage Browsertime [20] to visit the first 100k domains
of the Alexa Top 1M with Chromium 87.0.4280.88. As URLs, we
use the domains in the Alexa list (second level domains and deeper)
preceded with https:// and load every landing page once. We set
a page-load timeout of 300s, do not ignore certificate errors, and
disable QUIC to focus on HTTP/2 and avoid switching between
HTTP/3 and HTTP/2 after observing an alt-svc header. To achieve
reproducibility, we disable Chromium field trials, that would other-
wise randomly enable experimental features or parameterizations.
We then collect Chromium’s NetLog [19] files giving more details
on low-level connection events (e.g., start and end) and stitch these
events together to gather a precise view of the session lifecycle for
analyzing it as described before.
Ethics. Our Chromium measurements are conducted from within
our university’s network on a dedicated IP inside our measurement
subnet. Aiming to minimize impact, we set up a reverse DNS entry
for our IP, hinting at our research context, and provide a website
294
IMC ’21, November 2–4, 2021, Virtual Event, USA
Sander et al.
explaining our measurements and how to opt out. Abuse e-mails
are handled correspondingly.
4.3 Limitations
As is typical for measurements, our approach is limited: E.g., providers
blocking our measurements (we provide instructions on our web-
site) can skew results for our vantage point.
Further, we are limited by Chromium’s features: We filter HTTP
status 421 [2] (see Sec. 3) in our measurements to not wrongly clas-
sify these cases as unwanted redundancy but cannot consider ORI-
GIN Frames [18] as these are not implemented in Chromium [17].
I.e., if Web servers signal to reuse a connection for other domains
via this frame, Chromium and in turn our analysis do not react.
Also, we only review landing pages, which can show different
behavior than internal pages [1]. However, the HTTP Archive fo-
cuses solely on landing pages and we aim at a broader overview of
many different websites.
Additionally, cookie accept-banners are not clicked, such that
further requests/connections are potentially missed due to missing
consent. Furthermore, caching effects are ignored as the caches of
the browsers are reset after each visit. Also, as described before, we
ignore HTTP/3 requests in the HTTP Archive, as these requests lack
information to attribute them to individual connections; and in our
measurements to avoid potentially switching between HTTP/2 and
HTTP/3 in between. We would expect that the results are compara-
ble, as the IP and CRED case would occur equally and certificates
are probably also shared for use with HTTP/2 and HTTP/3.
Aside from that, our classification can misclassify redundant
connections: E.g., with newly opened connections, browsers can
switch IPs if multiple IPs are announced. Hence, cause CRED can
be misclassified as IP and CERT as a third party. To specifically
distinguish between CRED and the remaining cases, we conduct
one more measurement described later (cf. Section 5.3.3).
Lastly, we are limited by logging inconsistencies in the HTTP
Archive’s HAR files: We ignored 26.93 k requests with 0 as socket
ID, 1.30 k / 653 requests with missing / inconsistent IPs, 66.75 M
/ 273.49 k / 124.37 k requests with invalid HTTP request methods /
versions / statuses, and 14 requests with an incorrect page reference.
2.22 M requests did not provide SSL certificates, which we use for
SAN extraction, and 11.12 M / 172.73 M requests were HTTP/3 or
HTTP/1 requests. Further, we filtered 9 HAR files with invalid cer-
tificates and one without request IDs. In total, 69.12 M of 401.63 M
HTTP/2 requests and 5.33 M websites were affected by these in-
consistencies, but all the aforementioned information are used by
our analysis to distinguish between HTTP/2 requests and missing
or inconsistent entries can skew our results such that we conser-
vatively exclude these. I.e., our HTTP Archive results potentially
underestimate requests and open connections.
5 RESULTS
In the following, we present our results of the analysis of redundant
HTTP/2 connections. In total, we analyzed 6 242 688 websites of
the HTTP Archive (from April 2021), of which 5 883 212 open at
least one HTTP/2 connection. Moreover, we measured the Alexa
Top 100k websites from April 20th twice in May 2021. The first
time, we followed the Fetch Standard, while the second time, we
295
Figure 2: Distribution of connections per website
ignore its connection pool credentials flag. We found 18 282 / 18 309
sites to be unreachable for the first / second measurement run. We
review the intersection of websites for comparability, consisting of
81 553 sites that all opened at least one HTTP/2 session. For ease
of readability, we round percentages to integer numbers.
5.1 Websites with Redundant Connections
Depending on the assumed connection duration, we find between
4 493 097 (endless) to 2 263 751 (immediate) of the 5 883 212 HTTP/2
sites (76 % / 38 %) in the HTTP Archive to open at least one redun-
dant connection (also shown in Table 1). From the Alexa Top 100k
measurements, we find 77 878 HTTP/2 sites (95 %) opening redun-
dant connections. If we had assumed endless connection duration
(here we know the exact durations), these numbers do not change a
lot, and we get 77 898 sites with redundant connections. As such, we
find that connections are rather long-lived with a median lifetime
of 122.2s for those connections that close prior to our test ending
(3.5%). Given the small difference here, we continue by regarding
the endless connection case for the HTTP Archive data.
Figure 2 shows the distribution of sites in relation to their re-
dundant connections. For the HTTP Archive (×), around 50 % of all
sites open at least two redundant connections. For the Alexa Top
100k (+), around 50 % open at least six.
We explain the differences, i.e., the higher number of affected
websites and connections for our own measurements, by (1) the
different sets of websites, (2) our measurements including also
requests which we could not analyze in the HTTP Archive (cf.