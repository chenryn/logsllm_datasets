aggregate [41]. Other tools implement algorithms targeting speciﬁc
applications like: location data [35], genomic data [27, 47], mobility
data [37], and browser error reports [26].
Several tools have been proposed for providing formal veriﬁca-
tion of the differential privacy guarantee, using a wide variety of
veriﬁcation approaches: dynamic checking [24, 36], relational pro-
gram logic [2, 6] and relational reﬁnement type systems [9], linear
(dependent) type systems [29, 42], product programs [7], meth-
ods based on computing bisimulations families for probabilistic
automata [50, 51], and methods based on counting variants of satis-
ﬁability modulo theories [28]. None of these techniques can handle
advanced composition, interactive online algorithms and privacy
depending on accuracy. Barthe et al. [5] present a system for rea-
soning about computational differential privacy [39] a relaxation of
differential privacy where the adversary are computationally-bound.
Coupling is an established tool in probability theory, but it seems
less familiar to computer science. It was only quite recently that
couplings have been used in cryptography; according to Hoang and
Rogaway [31], who use couplings to reason about generalized Feis-
tel networks, Mironov [38] ﬁrst used this technique in his analysis
of RC4. There are seemingly few applications of coupling in formal
veriﬁcation, despite considerable research on probabilistic bisimu-
lation (ﬁrst introduced by Larsen and Skou [33]) and probabilistic
relational program logics (ﬁrst introduced by Barthe et al. [4]).
The connection between liftings and couplings was recently noted
by Barthe et al. [8] and explored for differential privacy by Barthe
et al. [10]. The latter uses a coupling argument to prove differen-
tially private the sparse vector algorithm that we also consider in
this work. The additional challenges that we face are: ﬁrst, the inte-
gration of advanced composition, providing a much better privacy
bound; second, the proof that sparse vector is differentially private
also in the interactive model, which requires additionally to have a
logic that permits to reason about the adversary. Moreover, Barthe
et al. [10] do not provide methods to prove privacy using accuracy.
In promising recent work, Zhang and Kifer [52] design a system
to automatically verify differentially privacy for examples where
the privacy proof uses tools beyond the standard composition the-
orem, including the Sparse Vector technique. Their proof strategy
is morally similar to couplings, but their work uses a combination
of product programs and lightweight reﬁnement types backed by
novel type-inference techniques, rather than a relational program
logic like we consider. Their system can also optimize the privacy
cost, something that we do not consider. While their work is highly
automated, their system is limited to pure, (, 0) differential privacy,
so it cannot verify the algorithms we consider, where privacy fol-
lows from accuracy or the advanced composition theorem. Their
techniques also seem limited to couplings from bijections; in partic-
ular, it is not clear how to prove privacy for examples that use more
advanced couplings like the optimal subset coupling.
10. CONCLUDING REMARKS
We have presented an extension of the logic apRHL [6] that
can express three classes of privacy proofs beyond current state-of-
the-art techniques for privacy veriﬁcation: privacy depending on
accuracy, privacy from advanced composition, and privacy for inter-
active algorithm. We have formalized a generalization of the adap-
tive Sparse Vector algorithm, known as Between Thresholds [13].
This and other possible generalizations of sparse vector could bring
interesting results in domains like geo-indistinguishability [1].
For the future, it would be interesting to explore generalizations
of differential privacy like the recent notion of concentrated dif-
ferential privacy [12, 18]. This generalization features a simple
composition principle that internalizes the advanced composition
principle of standard differential privacy. However, it is currently
unclear whether the deﬁnition of concentrated differential privacy,
which involves Rényi divergences, can be modeled using apRHL.
Additionally, there is still room for improving the expressivity of
apRHL for differential privacy. One interesting example combin-
ing accuracy and privacy is the large margin mechanism [15]. The
privacy proof for this algorithm requires careful reasoning about
the size of the support when applying pointwise equality, and so-
phisticated facts about the accuracy Sparse Vector. This example
seems beyond the reach of our techniques, but we believe it could
be handled by generalizing the existing rules.
Finally, it would be interesting to explore a tighter integration of
accuracy and privacy proofs. We currently use two systems, aHL
and apRHL, to verify privacy. This can lead to awkward proofs
since the two logics can only interact in speciﬁc places in the proof
(i.e., the up-to-bad rules). A combined version of the logics could
allow more natural proofs.
References
[1] M. E. Andrés, N. E. Bordenabe, K. Chatzikokolakis, and
C. Palamidessi. Geo-indistinguishability: differential privacy
for location-based systems. In ACM SIGSAC Conference on
Computer and Communications Security (CCS), Berlin,
Germany, pages 901–914, 2013.
[2] G. Barthe and F. Olmedo. Beyond differential privacy:
Composition theorems and relational logic for f-divergences
between probabilistic programs. In International Colloquium
on Automata, Languages and Programming (ICALP), Riga,
Latvia, volume 7966 of Lecture Notes in Computer Science,
pages 49–60. Springer, 2013.
[3] G. Barthe, B. Grégoire, J. Hsu, and P.-Y. Strub. Coupling
proofs are probabilistic product programs.
[4] G. Barthe, B. Grégoire, and S. Zanella-Béguelin. Formal
certiﬁcation of code-based cryptographic proofs. In ACM
SIGPLAN–SIGACT Symposium on Principles of
Programming Languages (POPL), Savannah, Georgia, pages
90–101, New York, 2009.
[5] G. Barthe, G. Danezis, B. Grégoire, C. Kunz, and S. Z.
Béguelin. Veriﬁed computational differential privacy with
applications to smart metering. In IEEE Computer Security
Foundations Symposium (CSF), New Orleans, Louisiana,
pages 287–301, 2013.
[6] G. Barthe, B. Köpf, F. Olmedo, and S. Zanella-Béguelin.
Probabilistic relational reasoning for differential privacy.
ACM Transactions on Programming Languages and Systems,
35(3):9, 2013.
[7] G. Barthe, M. Gaboardi, E. J. Gallego Arias, J. Hsu, C. Kunz,
and P.-Y. Strub. Proving differential privacy in Hoare logic. In
IEEE Computer Security Foundations Symposium (CSF),
Vienna, Austria, 2014.
[8] G. Barthe, T. Espitau, B. Grégoire, J. Hsu, L. Stefanesco, and
P.-Y. Strub. Relational reasoning via probabilistic coupling. In
International Conference on Logic for Programming,
Artiﬁcial Intelligence and Reasoning (LPAR), Suva, Fiji,
volume 9450, pages 387–401, 2015.
[9] G. Barthe, M. Gaboardi, E. J. Gallego Arias, J. Hsu, A. Roth,
and P.-Y. Strub. Higher-order approximate relational
reﬁnement types for mechanism design and differential
privacy. In ACM SIGPLAN–SIGACT Symposium on
Principles of Programming Languages (POPL), Mumbai,
India, 2015.
[10] G. Barthe, M. Gaboardi, B. Grégoire, J. Hsu, and P.-Y. Strub.
Proving differential privacy via probabilistic couplings. In
IEEE Symposium on Logic in Computer Science (LICS), New
York, New York, 2016.
[11] G. Barthe, M. Gaboardi, B. Grégoire, J. Hsu, and P.-Y. Strub.
A program logic for union bounds. In International
Colloquium on Automata, Languages and Programming
(ICALP), Rome, Italy, 2016.
[12] M. Bun and T. Steinke. Concentrated Differential Privacy:
Simpliﬁcations, Extensions, and Lower Bounds. May 2016.
[13] M. Bun, T. Steinke, and J. Ullman. Make Up Your Mind: The
Price of Online Queries in Differential Privacy. Apr. 2016.
[14] T.-H. H. Chan, E. Shi, and D. Song. Private and continual
release of statistics. ACM Transactions on Information and
System Security, 14(3):26, 2011.
[15] K. Chaudhuri, D. J. Hsu, and S. Song. The large margin
mechanism for differentially private maximization. In
Conference on Neural Information Processing Systems (NIPS),
Montréal, Québec, pages 1287–1295, 2014.
[16] C. Dwork and J. Lei. Differential privacy and robust statistics.
In ACM SIGACT Symposium on Theory of Computing
(STOC), Bethesda, Maryland, pages 371–380, 2009.
[17] C. Dwork and A. Roth. The algorithmic foundations of
differential privacy. Foundations and Trends in Theoretical
Computer Science, 9(3–4):211–407, 2014.
[18] C. Dwork and G. N. Rothblum. Concentrated Differential
Privacy. Mar. 2016.
[19] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating
noise to sensitivity in private data analysis. In IACR Theory of
Cryptography Conference (TCC), New York, New York, pages
265–284, 2006.
[20] C. Dwork, M. Naor, O. Reingold, G. N. Rothblum, and S. P.
Vadhan. On the complexity of differentially private data
release: efﬁcient algorithms and hardness results. In ACM
SIGACT Symposium on Theory of Computing (STOC),
Bethesda, Maryland, pages 381–390, 2009.
[21] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum.
Differential privacy under continual observation. In ACM
SIGACT Symposium on Theory of Computing (STOC),
Cambridge, Massachusetts, pages 715–724, 2010.
[22] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting and
differential privacy. In IEEE Symposium on Foundations of
Computer Science (FOCS), Las Vegas, Nevada, pages 51––60,
2010.
[23] C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and
A. Roth. The reusable holdout: Preserving validity in adaptive
data analysis. Science, 349(6248):636–638, 2015.
[24] H. Ebadi, D. Sands, and G. Schneider. Differential privacy:
Now it’s getting personal. In ACM SIGPLAN–SIGACT
Symposium on Principles of Programming Languages
(POPL), Mumbai, India, pages 69–81, 2015.
[25] F. Eigner and M. Maffei. Differential privacy by typing in
security protocols. In IEEE Computer Security Foundations
Symposium (CSF), New Orleans, Louisiana, pages 272–286,
2013.
[26] Ú. Erlingsson, V. Pihur, and A. Korolova. RAPPOR:
randomized aggregatable privacy-preserving ordinal response.
In ACM SIGSAC Conference on Computer and
Communications Security (CCS), Scottsdale, Arizona, pages
1054–1067, 2014.
[27] S. E. Fienberg, A. B. Slavkovic, and C. Uhler. Privacy
preserving GWAS data sharing. In IEEE International
Conference on Data Mining Workshops (ICDMW), Vancouver,
British Colombia, pages 628–635, 2011.
[28] M. Fredrikson and S. Jha. Satisﬁability modulo counting: a
new approach for analyzing privacy properties. In IEEE
Symposium on Logic in Computer Science (LICS), Vienna,
Austria, pages 42:1–42:10, 2014.
[41] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In ACM
SIGACT Symposium on Theory of Computing (STOC), San
Diego, California, 2007.
[29] M. Gaboardi, A. Haeberlen, J. Hsu, A. Narayan, and B. C.
Pierce. Linear dependent types for differential privacy. In
ACM SIGPLAN–SIGACT Symposium on Principles of
Programming Languages (POPL), Rome, Italy, pages
357–370, 2013.
[30] M. Hardt and G. N. Rothblum. A multiplicative weights
mechanism for privacy-preserving data analysis. In IEEE
Symposium on Foundations of Computer Science (FOCS), Las
Vegas, Nevada, pages 61–70, 2010.
[31] V. T. Hoang and P. Rogaway. On generalized Feistel networks.
In IACR International Cryptology Conference (CRYPTO),
Santa Barbara, California, volume 6223 of Lecture Notes in
Computer Science, pages 613–630. Springer, 2010.
[32] P. Kairouz, S. Oh, and P. Viswanath. The composition
theorem for differential privacy. 2015.
[33] K. G. Larsen and A. Skou. Bisimulation through probabilistic
testing. In ACM Symposium on Principles of Programming
Languages (POPL), Austin, Texas, pages 344–352, 1989.
[34] M. Lyu, D. Su, and N. Li. Understanding the sparse vector
technique for differential privacy. 2016.
[35] A. Machanavajjhala, D. Kifer, J. M. Abowd, J. Gehrke, and
L. Vilhuber. Privacy: Theory meets practice on the map. In
International Conference on Data Engineering (ICDE),
Cancún, México, pages 277–286, 2008.
[36] F. McSherry. Privacy integrated queries. In ACM SIGMOD
International Conference on Management of Data (SIGMOD),
Providence, Rhode Island, 2009.
[37] D. J. Mir, S. Isaacman, R. Cáceres, M. Martonosi, and R. N.
Wright. DP-WHERE: differentially private modeling of
human mobility. In IEEE International Conference on Big
Data (ICBD), Santa Clara, California, pages 580–588, 2013.
[38] I. Mironov. (Not so) random shufﬂes of RC4. In IACR
International Cryptology Conference (CRYPTO), Santa
Barbara, California, volume 2442 of Lecture Notes in
Computer Science, pages 304–319. Springer, 2002.
[39] I. Mironov, O. Pandey, O. Reingold, and S. P. Vadhan.
Computational differential privacy. In IACR International
Cryptology Conference (CRYPTO), Santa Barbara,
California, pages 126–142, 2009.
[40] P. Mohan, A. Thakurta, E. Shi, D. Song, and D. E. Culler.
GUPT: privacy preserving data analysis made easy. In ACM
SIGMOD International Conference on Management of Data
(SIGMOD), Scottsdale, Arizona, pages 349–360, 2012.
[42] J. Reed and B. C. Pierce. Distance makes the types grow
stronger: A calculus for differential privacy. In ACM
SIGPLAN International Conference on Functional
Programming (ICFP), Baltimore, Maryland, 2010.
[43] R. Rogers, A. Roth, J. Ullman, and S. Vadhan. Privacy
odometers and ﬁlters: Pay-as-you-go composition. 2016.
[44] A. Roth and T. Roughgarden. Interactive privacy via the
median mechanism. In ACM SIGACT Symposium on Theory
of Computing (STOC), Cambridge, Massachusetts, pages
765–774, 2010.
[45] I. Roy, S. T. V. Setty, A. Kilzer, V. Shmatikov, and E. Witchel.
Airavat: Security and privacy for MapReduce. In USENIX
Symposium on Networked Systems Design and
Implementation (NSDI), San Jose, California, pages 297–312,
2010.
[46] R. Shokri and V. Shmatikov. Privacy-preserving deep learning.
In ACM SIGSAC Conference on Computer and
Communications Security (CCS), Boulder, Colorado, pages
1310–1321, 2015.
[47] S. Simmons, C. Sahinalp, and B. Berger. Enabling
privacy-preserving GWAS in heterogenous human
populations. In RECOMB, 2016.
[48] A. Thakurta and A. Smith. Differentially private feature
selection via stability arguments, and the robustness of the
lasso. In Conference on Computational Learning Theory
(CoLT), Princeton, New Jersey, pages 819–850, 2013.
[49] H. Thorisson. Coupling, Stationarity, and Regeneration.
Springer, 2000.
[50] M. C. Tschantz, D. Kaynar, and A. Datta. Formal veriﬁcation
of differential privacy for interactive systems (extended
abstract). Electronic Notes in Theoretical Computer Science,
276(0):61–79, 2011.
[51] L. Xu, K. Chatzikokolakis, and H. Lin. Metrics for
differential privacy in concurrent systems. In IFIP
International Conference on Formal Techniques for
Distributed Objects, Components and Systems (FORTE),
Berlin, Germany, volume 8461 of Lecture Notes in Computer
Science, pages 199–215, June 2014.
[52] D. Zhang and D. Kifer. AutoPriv: Automating differential
privacy proofs. 2016.