17
18
19
20
21
22
23
24
25
26
27
28
29
30
31 }
int ret = 0;
aim_rxcallback_t userfunc;
guint16 tmp;
char *bn, *msg;
/* Read buddy name */
if ((tmp = byte_stream_get8(bs)))
bn = byte_stream_getstr(bs, tmp);
else
bn = NULL;
/* Read message (null terminated) */
if ((tmp = byte_stream_get16(bs)))
msg = byte_stream_getstr(bs, tmp);
else
msg = NULL;
/* Unknown */
tmp = byte_stream_get16(bs);
if ((userfunc =
aim_callhandler(od, snac->family, snac->subtype)))
ret = userfunc(od, conn, frame, bn, msg);
g_free(bn);
g_free(msg);
return ret;
static int
parseicon(OscarData *od,
FlapConnection *conn,
aim_module_t *mod,
FlapFrame *frame,
aim_modsnac_t *snac,
ByteStream *bs)
{
}
int ret = 0;
aim_rxcallback_t userfunc;
char *bn;
guint16 flags, iconlen;
guint8 iconcsumtype, iconcsumlen, *iconcsum, *icon;
bn = byte_stream_getstr(bs, byte_stream_get8(bs));
flags = byte_stream_get16(bs);
iconcsumtype = byte_stream_get8(bs);
iconcsumlen = byte_stream_get8(bs);
iconcsum = byte_stream_getraw(bs, iconcsumlen);
iconlen = byte_stream_get16(bs);
icon = byte_stream_getraw(bs, iconlen);
if ((userfunc =
aim_callhandler(od, snac->family, snac->subtype)))
ret = userfunc(od, conn, frame, bn, iconcsumtype,
iconcsum, iconcsumlen, icon, iconlen);
g_free(bn);
g_free(iconcsum);
g_free(icon);
return ret;
Figure 7: Original vulnerability (CVE-2011-4601) in receiveauthgrant (left), zero-day vulnerability in parseicon (right).
Sim. Function name
Sim. Function name
1.00 receiveauthgrant
1.00 receiveauthreply
1.00 parsepopup
1.00 parseicon
1.00 generror
0.99 incoming_.._buddylist
0.99 motd
0.99 receiveadded
0.99 mtn_receive
0.99 msgack
0.99 keyparse
0.99 hostversions
0.98 userlistchange
0.98 migrate
0.98 error
0.98 incomingim_ch4
0.98 parse_flap_ch4
0.98 infoupdate
0.98 parserights
0.98 incomingim
0.98 parseadd
0.97 userinfo
0.97 parsemod
0.97 parsedata
0.97 rights
0.97 rights
0.97 uploadack
0.96 incomingim_ch2_sendfile
0.96 rights
0.96 parseinfo_create
Table 3: Top 30 most similar functions to a known vulner-
ability in Pidgin.
code to a handful of functions. Again, the extraploation en-
ables us to identify previously unknown vulnerabilities by
inspecting only a small fraction of the code base.
4. LIMITATIONS
The discovery of vulnerable code in software is a hard
problem. Due to the fundamental inability of one program
to completely analyze another program’s code, a generic
technique for ﬁnding arbitrary vulnerabilities does not ex-
ist [10]. As a consequence, all practical approaches either
limit the search to speciﬁc types of vulnerabilities or, as
in the case of vulnerability extrapolation, only identify po-
tentially vulnerable code.
In the following we discuss the
limitations of our approach in more detail.
Our method builds on techniques from machine learning,
such as the embedding in a vector space and latent seman-
tic analysis. These techniques are eﬀective in identifying
potentially vulnerable code, yet they do not provide any
guarantees whether the identiﬁed code truly contains a vul-
nerability. This limitation is inherent to the application of
machine learning, which considers the statistics of the source
code rather than the true semantics. Due to Rice’s theorem,
however, a generic discovery of vulnerabilities is impossible
anyway and thus even the discovery of potential vulnerabil-
ities is beneﬁcial in practice.
A prerequisite for extrapolation is the existence of a start-
ing vulnerability. In cases where no known vulnerability is
available, our method cannot be applied. In practice such
cases are rare. For large software projects, it is not the
discovery of a single vulnerability that is challenging but
making sure that similar ﬂaws are not spread across the en-
tire code base. Extrapolation addresses exactly this setting.
Moreover, related techniques such as fuzz testing, taint anal-
ysis and symbolic execution can be easily coupled with vul-
nerability extrapolation and provide starting vulnerabilities
automatically.
Finally, the discovery of our method is limited to vulner-
abilities present in few functions of source code. Complex
ﬂaws that span several functions across a code base can be
diﬃcult to detect for our method. However, our case study
with FFmpeg shows that vulnerabilities distributed over two
functions can still be eﬀectively identiﬁed, as long as both
functions share some structural patterns with the original
vulnerability.
5. RELATED WORK
The identiﬁcation of vulnerabilities has been a vivid area
of security research. Various contrasting concepts have been
devised for ﬁnding and eliminating security ﬂaws in source
366
code. Our method is related to several of these approaches,
as we point out in this section.
5.1 Code Clone Detection
In the simplest case, functions containing similar vulnera-
bilities exist because code has been copied. The detection of
such copy-&-paste code clones has been an ongoing research
topic. In particular, Kontogiannis et al. [14] explore the use
of numerical features, such as the number of called func-
tions, to detect code clones, while Baxter et al. [2] suggest a
more ﬁne-grained method, which compares ASTs. Li et al.
present CP-Miner, a tool for code clone detection based on
frequent itemset mining [16]. They demonstrate the supe-
riority of their approach to the well-known tool CCFinder
developed by Kamiya et al. [13], a token-based detection
method. Maletic et al. [19] propose a method for code clone
detection, which compares functions in terms of comments
and identiﬁers. Similarly, Jang et al. have introduced a
method for ﬁnding unpatched copies of code using n-gram
analysis [11]. A thorough evaluation of existing methods is
provided by Bellon et al. [3].
Code clone detection shares some similarities with vul-
nerability extrapolation. However, corresponding methods
address a fundamentally diﬀerent problem and are specif-
ically tailored to ﬁnding copied code. As result, they can
only uncover vulnerabilities that have been introduced by
duplication of code.
5.2 Static Code Analysis
The idea of vulnerability extrapolation hinges on the ob-
servation that patterns of API usage are often indicative for
vulnerable code. This correspondence has been recognized
for a long time and is reﬂected in several static analysis tools,
such as Flawﬁnder [30], RATS [24] or ITS4 [28]. These tools
oﬀer databases of API symbols commonly found in conjunc-
tion with vulnerabilities and allow a code base to be auto-
matically scanned for their occurrences. The eﬀectiveness
of these tools critically depends on the quality and coverage
of the databases. Vulnerabilities related to internal APIs or
unknown patterns of API usage cannot be uncovered.
Engler et al.
[6] are among the ﬁrst to explore the link
between vulnerabilities and programming patterns. Their
method is capable of detecting vulnerabilities given a set of
manually deﬁned programming patterns. As an extension,
Li and Zhou [15] present an approach for mining similar pat-
terns automatically and detecting their violation in code. An
inherent problem of this approach is that frequent program-
ming mistakes will lead to the inference of valid patterns and
thus common ﬂaws cannot be detected. Williams et al. [31]
and Livshits et al. [17] address this problem by incorporating
software revision histories into the analysis. Our method is
related to these approaches. However, we focus on the anal-
ysis of code structure for ﬁnding vulnerabilities, rather than
modelling common programming templates.
5.3 Taint Analysis and Symbolic Execution
A more generic approach to vulnerability discovery builds
on taint analysis, where vulnerabilities are identiﬁed by a
source-sink system. If tainted data stemming from a source
propagates to a sink without undergoing validation, a poten-
tial vulnerability is detected. The success of this approach
has been demonstrated for several types of vulnerabilities,
including SQL injection and cross-site scripting [12, 18] as
well as integer-based vulnerabilities [29].
In most realiza-
tions, taint analysis is a dynamic process and thus limited
to discovery of vulnerabilities observable during execution
of a program.
The limitations of taint analysis have been addressed by
several authors using symbolic execution [e.g., 1, 4, 8, 22].
Instead of passively monitoring the ﬂow from the a source to
a sink, these approaches try to actively explore diﬀerent exe-
cution paths. Most notably is the work of Avgerinos et al. [1]
that introduces a framework for ﬁnding and even exploiting
security vulnerabilities. The power of symbolic execution,
however, comes at a prize: the analysis often suﬀers from a
vast space of possible execution paths. In practice, diﬀerent
assumptions and heuristics are necessary to trim down this
space to a tractable number of branches. As a consequence,
the application of symbolic execution for regular code au-
diting is still far from being practical [9].
5.4 Vulnerability Extrapolation
The concept of vulnerability extrapolation has been ﬁrst
introduced in [33]. In this work a method for vulnerability
extrapolation is proposed that analyzes the usage of function
and type names for ﬁnding vulnerabilities. However, neither
the syntax nor the structure of the code are considered and
thus the analysis is limited to ﬂaws reﬂected in particular
API symbols. Our method signiﬁcantly extends this work by
extracting and analyzing structural patterns in ASTs. As a
result, we are able to discover vulnerabilities that are related
to API usage as well as the structure of code. Moreover, we
demonstrate the eﬃcacy of our approach on a signiﬁcantly
larger set of code.
In comparison with related approaches, it is noteworthy
that vulnerability extrapolation does not aim at identifying
vulnerabilities automatically, but rendering manual audit-
ing of source code more eﬀective. The underlying rationale
is that manual auditing—though time-consuming—is still
superior to automatic methods and indispensable in prac-
tice. The assisted approach of vulnerability extrapolation
here better ﬁts the needs of security practitioners.
6. CONCLUSIONS
A key to strengthening the security of computer systems
is the rigorous elimination of vulnerabilities in the underly-
ing source code. To this end, we have introduced a method
for accelerating the process of manual code auditing by sug-
gesting potentially vulnerable functions to an analyst. Our
method extrapolates known vulnerabilities using structural
patterns of the code and enables eﬃciently ﬁnding similar
ﬂaws in large code bases. Empirically, we have demonstrated
this capability by identifying real zero-day vulnerabilities in
open-source projects, including Pidgin and FFmpeg.
The concept of vulnerability extrapolation is orthogonal
to other approaches for ﬁnding vulnerabilities and can be di-
rectly applied to complement current instruments for code
auditing. For example, if a novel vulnerability is identiﬁed
using fuzz testing or symbolic execution, it can be extrap-
olated to the entire code base, such that similar ﬂaws can
be immediately patched. This extrapolation raises the bar
for attackers, as they are required to continue searching for
novel vulnerabilities, once an existing ﬂaw has been suﬃ-
ciently extrapolated and related holes have been closed in
the source code.
367
Reporting of Vulnerabilities
The discovered vulnerabilities have been reported to the re-
spective developers before submission of this paper. The
ﬂaws should be ﬁxed in upcoming versions of the projects.
References
[1] T. Avgerinos, S. K. Cha, B. L. T. Hao, and D. Brum-
ley. AEG: Automatic Exploit Generation. In Proc. of
Network and Distributed System Security Symposium
(NDSS), 2011.
[2] I. D. Baxter, A. Yahin, L. Moura, M. S. Anna, and
L. Bier. Clone detection using abstract syntax trees.
In Proc. of the International Conference on Software
Maintenance (ICSM), 1998.
[3] S. Bellon, R. Koschke, I. C. Society, G. Antoniol,
J. Krinke, I. C. Society, and E. Merlo. Comparison and
evaluation of clone detection tools. IEEE Transactions
on Software Engineering, 33:577–591, 2007.
[4] M. Cova, V. Felmetsger, G. Banks, and G. Vigna. Static
detection of vulnerabilities in x86 executables. In Proc.
of Annual Computer Security Applications Conference
(ACSAC), pages 269–278, 2006.
[5] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman.
Indexing by latent semantic analysis.
Journal of the American Society for Information Sci-
ence, 41(6):391–407, 1990.
[6] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and
B. Chelf. Bugs as deviant behavior: A general approach
to inferring errors in systems code. In Proc. of ACM
Symposium on Operating Systems Principles (SOSP),
pages 57–72, 2001.
[7] N. Falliere, L. O. Murchu, , and E. Chien. W32.stuxnet
dossier. Symantec Corporation, 2011.
[8] P. Godefroid, M. Y. Levin, and D. Molnar. SAGE:
whitebox fuzzing for security testing. Communications
of the ACM, 55(3):40–44, 2012.
[9] S. Heelan. Vulnerability detection systems: Think cy-
borg, not robot. IEEE Security & Privacy, 9(3):74–77,
2011.
[10] J. Hopcroft and J. Motwani, R. Ullmann.
Introduc-
tion to Automata Theory, Languages, and Computa-
tion. Addison-Wesley, 2 edition, 2001.
[11] J. Jang, A. Agrawal, , and D. Brumley. ReDeBug: ﬁnd-
ing unpatched code clones in entire os distributions.
In Proc. of IEEE Symposium on Security and Privacy,
2012.
[12] N. Jovanovic, C. Kruegel, and E. Kirda. Pixy: A static
analysis tool for detecting web application vulnerabil-
ities.
In Proc. of IEEE Symposium on Security and
Privacy, pages 6–263, 2006.
[13] T. Kamiya, S. Kusumoto, and K. Inoue. CCFinder:
a multilinguistic token-based code clone detection sys-
tem for large scale source code. IEEE Transactions on
Software Engineering, pages 654–670, 2002.
[14] K. A. Kontogiannis, R. Demori, E. Merlo, M. Galler,
and M. Bernstein. Pattern matching for clone and con-
cept detection. Journal of Automated Software Engi-
neering, 3:108, 1996.
[15] Z. Li and Y. Zhou. PR-Miner: automatically extract-
ing implicit programming rules and detecting violations
in large software code. In Proc. of European Software
Engineering Conference (ESEC), pages 306–315, 2005.
[16] Z. Li, S. Lu, S. Myagmar, and Y. Zhou. Cp-miner:
Finding copy-paste and related bugs in large-scale soft-
ware code. IEEE Transactions on Software Engineer-
ing, 32:176–192, 2006.
[17] B. Livshits and T. Zimmermann. Dynamine: ﬁnding
common error patterns by mining software revision his-
tories. In Proc. of European Software Engineering Con-
ference (ESEC), pages 296–305, 2005.
[18] V. B. Livshits and M. S. Lam. Finding security vul-
nerabilities in java applications with static analysis. In
Proc. of USENIX Security Symposium, 2005.
[19] A. Marcus and J. I. Maletic. Identiﬁcation of high-level
concept clones in source code. In Proc. of International
Conference on Automated Software Engineering (ASE),
page 107, 2001.
[20] L. Moonen. Generating robust parsers using island
grammars. In Proc. of Working Conference on Reverse
Engineering (WCRE), pages 13–22, 2001.
[21] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Stani-
ford, and N. Weaver. Inside the Slammer worm. IEEE
Security and Privacy, 1(4):33–39, 2003.
[22] J. Newsome and D. Song. Dynamic taint analysis for
automatic detection, analysis, and signature generation
of exploits on commodity software. In Proc. of Network
and Distributed System Security Symposium (NDSS),
2005.
[23] T. Parr and R. Quong. ANTLR: A predicated-LL(k)
parser generator. Software Practice and Experience, 25:
789–810, 1995.
[24] rats. Rough auditing tool for security. Fortify Soft-
ware Inc., https://www.fortify.com/ssa-elements/
threat-intelligence/rats.html, visited April, 2012.
[25] G. Salton and M. J. McGill. Introduction to Modern
Information Retrieval. McGraw-Hill, 1986.
[26] C. Shannon and D. Moore. The spread of the Witty
worm. IEEE Security and Privacy, 2(4):46–50, 2004.
[27] M. Sutton, A. Greene, and P. Amini. Fuzzing: Brute
Force Vulnerability Discovery. Addison-Wesley Profes-
sional, 2007.
[28] J. Viega, J. Bloch, Y. Kohno, and G. McGraw. ITS4:
A static vulnerability scanner for C and C++ code. In
Proc. of Annual Computer Security Applications Con-
ference (ACSAC), pages 257–267, 2000.
[29] T. Wang, T. Wei, Z. Lin, and W. Zou. IntScope: Auto-
matically detecting integer overﬂow vulnerability in x86
binary using symbolic execution. In Proc. of Network
and Distributed System Security Symposium (NDSS),
2009.
[30] D. A. Wheeler. Flawﬁnder. http://www.dwheeler.
com/flawfinder/, visited April, 2012.
[31] C. C. Williams and J. K. Hollingsworth. Automatic
mining of source code repositories to improve bug ﬁnd-
ing techniques. IEEE Transactions on Software Engi-
neering, 31:466–480, 2005.
[32] Y. Xie and A. Aiken. Static detection of security vul-
nerabilities in scripting languages. In Proc. of USENIX
Security Symposium, 2006.
[33] F. Yamaguchi, F. Lindner, and K. Rieck. Vulnerabil-
ity extrapolation: Assisted discovery of vulnerabilities
using machine learning. In USENIX Workshop on Of-
fensive Technologies (WOOT), Aug. 2011.
368