osdmap e32: 6 osds: 6 up, 6 in
flags sortbitwise
pgmap v69: 64 pgs, 1 pools, 0 bytes data, 0 objects
203 MB used, 119 GB / 119 GB avail
39 active+clean
10 activating
10 peering
5 remapped+peering
\[root@node111 ceph-cluster\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
vdb 252:16 0 20G 0 disk
├─vdb1 252:17 0 10G 0 part
└─vdb2 252:18 0 10G 0 part
vdc 252:32 0 20G 0 disk
└─vdc1 252:33 0 20G 0 part /var/lib/ceph/osd/ceph-0
vdd 252:48 0 20G 0 disk
└─vdd1 252:49 0 20G 0 part /var/lib/ceph/osd/ceph-1
### 2）常见错误（非必须操作）
如果查看状态包含如下信息：
health: HEALTH_WARN
clock skew detected on node2, node3...
clock
skew表示时间不同步，解决办法：请先将所有主机的时间都使用NTP时间同步！！！
Ceph要求所有主机时差不能超过0.05s，否则就会提示WARN，如果使用NTP还不能精确同步时间，可以手动修改所有主机的ceph.conf，在\[MON\]下面添加如下一行：
mon clock drift allowed = 1
如果状态还是失败，可以尝试执行如下命令，重启ceph服务：
\[root@node1 \~\]# systemctl restart ceph\\\*.service ceph\\\*.target
Ceph集群创建后之后,可以在任意集群节点上执行ceph命令
至此：前面实验完成了ODS和Monitor
OSD ：存储设备,负责真正的存储空间, 软件包：ceph-osd
Monitor ：集群监控组件，利用Crush算法分配存储,要求至少要做三台
软件包：ceph-mon
接下来是客户端共享使用
# 3 案例3：创建Ceph块存储
3.1 问题
沿用练习一，使用Ceph集群的块存储功能，实现以下目标：
-   创建块存储镜像
-   客户端映射镜像
-   创建镜像快照
-   使用快照还原数据
-   使用快照克隆镜像
-   删除快照与镜像
创建共享盘(image镜像),镜像必须放在共享池,默认有个rbd共享池
共享池内:
Image镜像1(共享磁盘)\--/dev/存储设备
Image镜像2(共享磁盘)\--/dev/存储设备
Image镜像3(共享磁盘)\--/dev/存储设备
## 步骤一：创建镜像
### 查看存储池。
\[root@node1 \~\]# ceph osd lspools
0 rbd,
### 创建镜像、查看镜像
demo-image功能 \--image-feature 支持快照
\[root@node1 \~\]# rbd create demo-image \--image-feature layering
\--size 10G
\[root@node1 \~\]# rbd create rbd/image \--image-feature layering
\--size 10G
\[root@node1 \~\]# rbd list #查看已创建镜像文件名(node2,node3都能查看到)
demo-image #创建的两个镜像
Image
\[root@node1 \~\]# rbd info demo-image #查看demo-image镜像共享的相信信息
rbd image \'demo-image\':
size 10240 MB in 2560 objects
order 22 (4096 kB objects)
block_name_prefix: rbd_data.d3aa2ae8944a
format: 2
features: layering
## 步骤二：容量动态调整
### 1）缩小容量
\[root@node1 \~\]# rbd resize \--size 7G image \--allow-shrink
#\--allow-shrink 作用:防止缩小容量发生事故
\[root@node1 \~\]# rbd info image
rbd image \'image\':
size 7168 MB in 1792 objects
order 22 (4096 kB objects)
block_name_prefix: rbd_data.1032238e1f29
format: 2
features: layering
flags:
### 2）扩容容量
\[root@node1 \~\]# rbd resize \--size 15G image
\[root@node1 \~\]# rbd info image
rbd image \'image\':
size 15360 MB in 3840 objects
order 22 (4096 kB objects)
block_name_prefix: rbd_data.1032238e1f29
format: 2
features: layering
flags:
## 步骤三：通过KRBD访问
### 1）集群内将镜像映射为本地磁盘(本试验中未做)
\[root@node1 \~\]# rbd map demo-image #挂载使用
/dev/rbd0
\[root@node1 \~\]# lsblk
... ...
rbd0 251:0 0 10G 0 disk
\[root@node1 \~\]# mkfs.xfs /dev/rbd0
\[root@node1 \~\]# mount /dev/rbd0 /mnt
集群内有ceph.client.admin.keyring 文件.里面有用户名和密码,可直接挂载使用
\[root@node1 ceph-cluster\]# rbd unmap demo-image #卸载镜像
### 2）客户端通过KRBD访问
#客户端需要安装ceph-common软件包
#拷贝配置文件（否则不知道集群在哪）
#拷贝连接密钥（否则无连接权限）
\[root@client \~\]# yum -y install ceph-common
\[root@client \~\]# ls /etc/ceph #安装完之后有rbdmap 程序
Rbdmap
\[root@client \~\]# scp 192.168.4.11:/etc/ceph/ceph.conf /etc/ceph/
#拷贝配置文件（否则不知道集群在哪）
\[root@client \~\]# ls /etc/ceph
ceph.conf rbdmap
\[root@client \~\]# scp 192.168.4.11:/etc/ceph/ceph.client.admin.keyring
\\
/etc/ceph/
#拷贝连接密钥文件（否则无连接权限）
\[root@client \~\]# ls /etc/ceph/
ceph.client.admin.keyring ceph.conf rbdmap
\[root@client \~\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
\[root@client \~\]# rbd map image #添加镜像盘image,前面创建了两个
/dev/rbd0
\[root@client \~\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
rbd0 251:0 0 15G 0 disk #多了rbd0
\[root@client \~\]# rbd showmapped
id pool image snap device
0 rbd image - /dev/rbd0
### 3) 客户端格式化、挂载分区
\[root@client \~\]# mkfs.xfs /dev/rbd0
\[root@client \~\]# mount /dev/rbd0 /mnt/
\[root@client \~\]# echo \"test\" \> /mnt/test.txt
\[root@client \~\]# umount /mnt #卸载挂载
\[root@client \~\]# rbd unmap image #卸载镜像
ceph快照:基于COW(copy in write)\-\--写时复制
例如:
原始磁盘100G:
## 步骤四：创建镜像快照
防止误删数据，有3副本是无法解决人为修改数据，
### 1) 查看镜像快照
\[root@node1 \~\]# rbd snap ls image
### 2) 创建镜像快照
\[root@node1 \~\]# rbd snap create image \--snap image-snap1
\[root@node1 \~\]# rbd snap ls image
SNAPID NAME SIZE
4 image-snap1 15360 MB
#针对image镜像做的快照
### 3) 删除客户端写入的测试文件
\[root@client \~\]# rm -rf /mnt/test.txt
###  还原快照
Ceph不支持在线还原
\[root@node1 \~\]# rbd snap rollback image \--snap image-snap1
#客户端重新挂载分区
\[root@client \~\]# umount /mnt
\[root@client \~\]# mount /dev/rbd0 /mnt/
\[root@client \~\]# ls /mnt
### 5) 实验中遇到错误
\[root@client \~\]# mount /dev/rbd0 /mnt
mount: 文件系统类型错误、选项错误、/dev/rbd0 上有坏超级块、
缺少代码页或助手程序，或其他错误
有些情况下在 syslog 中可以找到一些有用信息- 请尝试
dmesg \| tail 这样的命令看看。
解决:在集群中重新执行快照还原
\[root@node1 \~\]# rbd snap rollback image \--snap image-snap1
\[root@client \~\]# mount /dev/rbd0 /mnt
## 步骤五：创建快照克隆
### 1）克隆快照
\[root@node111 \~\]# rbd snap ls image
SNAPID NAME SIZE
4 image-snap1 15360 MB
\[root@node1 \~\]#rbd snap protect image \--snap image-snap1
> #保护image-snap1镜像,防止误删,导致整个镜像损坏
\[root@node1 \~\]#rbd snap unprotect image \--snap image-snap1
> #取消保护image-snap1(本实验此时不操作)
\[root@node1 \~\]# rbd snap rm image \--snap image-snap1
> #前面做了保护,此时删除镜像会失败
\[root@node1 \~\]# rbd clone image \--snap image-snap1 image-clone
\--image-feature layering
//使用镜像image的快照image-snap1克隆一个新的镜像image-clone
### 查看克隆镜像与父镜像快照的关系
\[root@node111 \~\]# rbd list
demo-image
image
image-clone #多了image-clone镜像
\[root@node1 \~\]#rbd info image-clone
rbd image \'image-clone\':
size 15360 MB in 3840 objects
order 22 (4096 kB objects)
block_name_prefix: rbd_data.d3f53d1b58ba
format: 2
features: layering
flags:
parent: rbd/image@image-snap1 #这个镜像的父镜像是image@image-snap1
overlap: 15360 MB
#克隆镜像很多数据都来自于快照链
#如果希望克隆镜像可以独立工作，就需要将父快照中的数据，全部拷贝一份，但比较耗时！！！
\[root@node1 \~\]#rbd flatten image-clone
#将父镜像image@image-snap1的数据全部拷贝一份到image-clone上