3.1  TTA Bus Validation with SWIFI 
A  TTA  system  with  the  bus  interconnection  structure 
was  validated  using  software  implemented  fault  injection 
(SWIFI)  experiments.  The  hardware  setup  consists  of  4 
active  TTP  nodes  interconnected  by  redundant  busses. 
Fault injection experiments are performed in only one node 
(node 4). The single bit-flip fault model was used, as this 
model corresponds with transient physical faults, which are 
considered  to  be  the  most  common  physical  faults  in 
computer systems. Fault injector is implemented using the 
code insertion technique. As workload application we use 
a  distributed 
application 
developed  by  Volvo  Technological  Development.  For  a 
detailed  description  of  the  implementation  of  the  SWIFI 
fault injector the reader is referred to [2].  
simulation  brake-by-wire 
in 
SWIFI 
experiments  were 
the 
performed 
communication  controller  (registers,  CNI, 
instruction 
memory).  Every  bit  of  the  CNI,  TTP/C-C1  controller 
registers,  and  the  instruction  memory  was  target  of 
systematic  software-implemented  fault 
injection.  The 
TTP/C protocol code is stored in flash memory and upon 
restart  it  is  downloaded  into  the  RAM  denoted  as 
instruction  memory,  and  executed  from  there.  Emulating 
transient bit flip faults in the instruction memory changes 
the protocol code and therefore enables the fault injector to 
emulate transient bit flips in the TTP/C protocol code. 
During  the  execution  of  SWIFI  experiments  no  frame 
collisions on the bus were observed. A detailed analysis of 
some  experiments  has  shown  that  the  local  bus  guardian 
was able to prevent the faulty controller from transmitting 
outside  its  sending  window.  However,  we  have  observed 
several  cases  of  slightly-off-specification  (SOS)  failures 
with  respect  to  the  start  of  frame  transmission  [1].  SOS 
failures occur at the interface between the analog and the 
digital  world.  The  specification  for  a  node  requires  that 
every  correct  node  must  accept  input  signals  if  they  are 
within  a  specified  range  of  tolerance  with  respect  to 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 06:59:21 UTC from IEEE Xplore.  Restrictions apply. 
parameters  such  as  frequency  or  voltage  [11].  Each 
individual  node  will  have  a  slightly  different  range  of 
tolerance (e.g., determined by the actual speed of the local 
hardware  clock  or  local  power  supply).  If  an  erroneous 
node produces an output signal (in time or value) slightly 
outside  the  specified  range  of  tolerance,  some  nodes  will 
correctly  receive  this  signal,  while  others  might  reject  it. 
Such  a  scenario  will  result  in  an  inconsistent  state  of  the 
distributed  system.  Intermediate  voltages  or  weak  edges 
are causes of SOS failures in the value domain [14].  
SOS  failures  with  respect  to  the  start  of  frame 
transmission  occur  when  a  frame  is  received  within  the 
time  receive  window  of  some  nodes  and  slightly  outside 
the time receive window of other nodes. Upon occurrence 
of  an  SOS  failure  with  respect  to  the  start  of  frame 
transmission,  some  of  the  nodes  accept  the  frame  as  a 
correct  one  (first  clique  –  the  start  of  the  frame  was 
received  within  the  time  receive  window),  while  others 
reject it as an incorrect frame (second clique - the start of 
the  frame  was  received  slightly  outside  the  time  receive 
window).    Therefore,  SOS  failures  lead  to  inconsistent 
state  of  the  distributed  system,  and  consequently  to  the 
formation of cliques. The minority clique performs restart 
because  the  TTP/C  clique  avoidance  algorithm  [10] 
resolves  clique  formations.  In  this  way  the  nodes  in  the 
minority clique will perform restart and thus an SOS faulty 
node affects the operation of correct nodes.  Even though 
an error has propagated, the TTA system was still able to 
detect the (propagated) error, and the infected (yet correct) 
part  of  the  system  takes  recovery  actions  by  means  of 
restart.  
A high reproducibility of SWIFI experiments allowed a 
detailed  study  of  the  behavior  of  the  system  during  the 
execution of the experiments that lead to SOS failures with 
respect  to  the  start  of  frame  transmission.  SOS  failure 
scenarios  were  caused  by  single  bit  flips  (in  the  registers 
and  the  instruction  memory  of  the  TTP/C  controller)  [1]. 
Therefore,  the  probability  of  occurrence  of  SOS  failures 
cannot  be  considered  negligible.  No  SOS  failures  in  the 
value domain were observed, as faults injected by software 
are orthogonal to this dimension.  
Table 1 summarizes the results of software implemented 
fault  injection  experiments.  The  table  provides  the  total 
number  of  faults  (i.e.,  bit  flips)  injected  into  a  particular 
unit  of  the  TTP/C  communication  controller  and  the 
number of errors (i.e., node failures) that were triggered by 
these faults. Furthermore, the table indicates the number of 
inconsistent  perceptions,  i.e.,  Slightly-Off-Specification 
Target Unit 
Instruction Memory 
Register File 
HW Registers  
CNI
Total 
Number of 
experiments  
507744 
14192 
6074 
34112 
562122 
Errors 
triggered
47359
5081 
1926 
3015 
57381 
SOS 
6 
0 
8 
0 
14 
failures with respect to the start of the frame transmission 
that were caused by the fault injection experiments and the 
relative frequency of SOS failures with respect to the total 
number of errors triggered. 
3.2  TTA  Bus  Validation  with  Heavy-Ion  Fault 
Injection 
The  same  cluster  setup  that  was  exposed  to  SWIFI 
experiments  was  validated  by  heavy-ion  fault  injection 
experiments.  These  experiments  utilized  a  Californium-
252  source  that  irradiated  the  communication  controller 
(TTP/C-C1)  with  heavy  ions,  which  cause  bit  flips  in  the 
silicon.  The  lid  of  the  communication  controller  was 
removed to make the silicon accessible for the ions. For a 
more detailed description of the setup the reader is referred 
to [16]. The source was placed above the silicon, inside a 
vacuum  chamber  with  adjusted  pressure  to  reach  an 
appropriate fault injection rate. Faults were injected in one 
node, denoted as fault injection node.
During 
the 
of 
execution 
heavy-ion 
injection 
experiments,  several  cases  of  error  propagation  were 
observed.  After  a  detailed  analysis  of  the  results,  other 
additional  types  of  error  propagation  other  than  the  SOS 
with  respect  to  the  start  of  frame  transmission  were 
detected. Error propagation scenarios are classified as: 
•  SOS with respect to the start of frame transmission. 
•  Reintegration – appears when the fault injection node 
fails  because  of  the  injected  faults,  and  tries  to 
reintegrate.  Since  the  radiation  source  performs 
continuous irradiation, faults are injected also during 
the reintegration phase. The faulty node has a faulty 
state and reintegrates by sending a frame in a wrong 
slot, colliding with the frame sent from other node.  
•  Asymmetric  –  a  frame  from  faulty  node  is  accepted 
as correct by some nodes, and rejected as incorrect by 
other  nodes,  and  the  cause  of  it  was  not  an  SOS 
failure with respect to the start of frame transmission. 
An analysis of some experiments has shown that one 
possible  cause  of  such  a  scenario  could  be  an  SOS 
failure in the value domain (the power supply of the 
transmission  unit  of  the  faulty  sender  node  can  be 
affected such that the output signal can be “weak” – 
in the limits of threshold level).  
Babbling  idiot  –  faulty  node  performs  continuous 
transmission  in  the  bus  at  arbitrary  points  in  time, 
colliding  with  frames  sent  from  other nodes.  Such a 
failure  makes 
the 
communication  of  correct  nodes 
within the cluster. 
impossible 
SOS (%) 
• 
Table 1: SWIFI Experiment Results with Bus Topology.
0.01%
0 %
0.4%
0 %
0.02 %
In 
fault 
heavy-ion 
injection 
experiments,  unfortunately,  there  is  no 
possibility to determine the exact origin of 
the fault and the number of injected faults 
(it  is  not  possible  to  count  the  number  of 
ions  that  penetrate  in  silicon  and  cause 
single  upset  events).  Therefore,  we  count 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 06:59:21 UTC from IEEE Xplore.  Restrictions apply. 
one experiment from the point in time when all nodes are 
running synchronized until the point in time when the node 
under  test  detects  an  error  and  performs  restart  (or  other 
nodes  in  the  cluster  detect  an  error  –  error  propagation). 
Since  the  node  under  test  is  exposed  to  continuous 
irradiation, the probability of having multiple faults during 
the duration of one experiment is relatively high. Multiple 
faults  are  outside  the  scope  of  the  tolerated  faults  in  the 
TTP/C  fault  hypothesis  (with  the  bus  topology).  Table 2 
summarizes 
radiation 
experiments.   
the  heavy-ion 
the 
results  of 
Number of Exp. 
SOS
Reintegration
Asymmetric
Babbling Idiot
Total 
37,036
17  0,04%
49  0,13 %
11  0,028%
1  0,003%
78  0,21% 
Table 2: Heavy-Ion Experiment Results with 
Bus Topology. 
The  most  common  failure  scenarios  were  those  that 
came  in  an  already  degraded  system,  e.g.  when  the  node 
under test tried to reintegrate in the synchronized (running) 
cluster.  The  second  most  frequent  failures  were  SOS 
failures with respect to the start of frame transmission. In 
all  error  propagation  cases  observed  during  the  execution 
of  heavy-ion  experiments,  there  was  no  case  where  a 
failure  resulted  in  a  permanent  disturbance  of  the  cluster, 
even  though  there  were  cases  when  the  communication 
controller  was  permanently  damaged  and  has  to  be 
removed.  Upon  the  occurrence  of  error  propagation  the 
error detection mechanisms of the TTP/C have detected the 
error  and  the  system  takes  recovery  actions  by  means  of 
restart. 
4 
Fault Containment and Error 
Containment Regions 
In  any  fault-tolerant  architecture  it  is  important  to 
distinguish  clearly  between  fault  containment  and  error 
containment. Fault containment is concerned with limiting 
the immediate impact of a single fault to a defined region, 
while  error  containment  tries  to  avoid  the  propagation  of 
the  consequences  of  a  fault,  the  error  [11].  It  must  be 
avoided  that  an  error  in  one  fault-containment  region 
propagates  into  another  fault-containment  region  that  has 
not  been  directly  affected  by  the  original  fault.  A  fault-
containment  region  (FCR)  is  defined  as  the  set  of 
subsystems  that  share  one  or  more  common  resources.  A 
fault in any one of these shared resources may thus impact 
all subsystems of the FCR, i.e. the subsystems of an FCR 
cannot be considered to be independent of each other [7]. 
Subsystems  that  share  common  resources,  for  example 
computing  hardware,  power  supply,  timing  source,  clock 
synchronization  service  or  physical  space  form  a  fault-
containment region. 
The  analysis  of  the  main  trends  in  deep  submicron 
technology  shows  the  multi-bit  errors  are  more  likely  to 
occur, as both junction size and critical charge continue to 
decrease.  Smaller  features  and  modified  wire  cross-
sections,  as  well  as  higher  frequencies,  will  raise  the 
likelihood of timing violations [5]. Because of the further 
miniaturization  of  future  submicron  integrated  circuits, 
transient  and 
faults  will  happen  more 
frequently  and  transient  and  intermittent  failures  will  in 
general  not  be  contained  within  a  region  of  a  single  die. 
This  means  that  independent  fault  containment  regions 
must be implemented in separate silicon dies.  
intermittent 
In  a  TTA  system  fail  silent  violations  in  the  time 
domain can cause error propagation, therefore, in order to 
avoid  error  propagation  caused  by  message  transmission 
we  need  timing  error  detection  mechanisms  that  are  in 
different  FCRs  than  the  message  sender  [11].  Otherwise, 
the  error  detection  mechanism  may  be  impacted  by  the 
same fault that caused the timing failure. In the TTA with 
the  bus  interconnection  structure  and  the  TTP/C-C1 
version of the communication controller with the local bus 
guardian  there  is  no  clear  physical  separation  of  FCRs 
between  the  bus  guardian  and  the  TTP/C  protocol  unit 
(message  sender).  The  local  bus  guardian  and  the  TTP/C 
protocol unit share the same clock synchronization service, 
power supply and physical space (silicon die). Only a truly 
independent guardian allows the clear distinction between 
a  sending  FCR  and  the  FCR  performing  timing  failure 
detection.  This 
to  be 
implemented  in  a  separate  silicon  die,  and  needs  to 
maintain independent clock synchronization service, and to 
have  separate  oscillators  and  power  supply.  One  solution 
to implement a truly independent guardian is to use the star 
interconnection structure where two central hubs have the 
role of central guardians. In this manner a TTA system can 
handle  arbitrary  failures  in  of  the  components  without  an 
impact on the operation of a properly configured cluster. 
independent  guardian  needs 
5 
The Central Guardian Approach 
Figure 2 provides the (logical) top-level architecture of 
a  TTA  cluster  utilizing  a  star  topology  network.  The 
cluster  comprises  of  regular  nodes  (TTA  system  nodes) 
and two star couplers. The regular nodes are connected to 
each  of  the  replicated  channels  of  the  (star  topology) 
interconnection  network  via  bi-directional  links.  Two 