of attack edges, its AUC degraded signiﬁcantly as more attack
edges were added to each graph. Íntegro, however, maintained
its performance, with at most 0.07 decrease in AUC, even
when the number of attack edges was relatively large. Notice
that Íntegro performed nearly as good as SybilRank when a
random victim classiﬁer was used, but performed much better
when the RF classiﬁer was used instead. This shows the impact
of leveraging victim prediction on fake account detection.
D. Sensitivity to seed-targeting attacks
Sophisticated attackers might obtain a full or partial knowl-
edge of which accounts are trusted by the OSN operator. As
the total trust is initially distributed among these accounts, an
attacker can adversely improve the ranking of the fakes by
establishing attack edges directly with them. We next evaluate
both systems under two variants of this seed-targeting attack.
Attack scenarios. We focus on two main attack scenarios. In
the ﬁrst scenario, the attacker targets accounts that are k nodes
away from all trusted accounts. This means that the length of
the shortest path from any fake account to any trusted account
is exactly k+1, representing the distance between the seeds
and the fake region. For k=0, each trusted account is a victim
and located at a distance of 1. We refer to this scenario, which
assumes a resourceful attacker, as the distant-seed attack.
In the second scenario, attackers have only a partial knowl-
edge and target k trusted accounts picked at random. We refer
to this scenario as the random-seed attack.
Evaluation method. To evaluate the sensitivity of each system
to a seed-targeting attack, we used the ﬁrst Facebook graph
to simulate each attack scenario. We implemented this by
replacing the endpoint of each attack edge in the real region
with a real account picked at random from a set of candidates.
For the ﬁrst scenario, a candidate account is one that is k
nodes away from all trusted accounts. For the second scenario,
a candidate account is simply any trusted account. We ran
experiments for both systems using different values of k and
measured the corresponding AUC at the end of each run.
Results. In the ﬁrst attack scenario, both systems had a poor
ranking quality when the distance was small, as illustrated in
Fig. 6a. Because Íntegro assigns low weights to edges incident
to victim accounts, the trust that escapes to the fake region is
less likely to come back into the real region. This explains why
SybilRank had a slightly better AUC for distances less than 3.
However, once the distance was larger, Íntegro outperformed
SybilRank ,as expected from earlier results.
In the second attack scenario, the ranking quality of both
systems degraded, as the number of victimized trusted accounts
(a) Targeted-victim attack
(b) Random-victim attack
Fig. 5: The ranking quality of both systems in terms of its AUC under
each inﬁltration scenario (CI=95%). SybilRank and Íntegro resulted in
a similar performance when a random victim classiﬁer is used, which
represents a practical baseline for Íntegro. As the number of attack
edges increased, SybilRank’s AUC decreased signiﬁcantly close to
0.7, while Íntegro sustained its high performance with AUC > 0.9.
Facebook graph as the real region. We then generated a
synthetic fake region consisting of 3,068 fakes with 36,816
friendships using the small-world graph model [60]. We then
added 35,306 random attack edges between the two regions
(n=9,204 and m=110,266). As suggested in related work [34],
we used a relatively large number of fakes and attack edges
in order to stress-test both systems under evaluation. We refer
to the this scenario as the random-victim attack.
Propagation rates. For each inﬁltration scenario, we deployed
the previously trained victim classiﬁer in order to assign new
edge weights. As we injected fakes in the second scenario,
we generated their feature vectors by sampling each feature
distribution of fakes from the ﬁrst scenario.4 We also assigned
edge weights using another victim classiﬁer that simulates two
operational modes. In the ﬁrst mode, the classiﬁer outputs the
best possible victim predictions with an AUC≈1 and proba-
bilities greater than 0.95. In the second mode, the classiﬁer
outputs uniformly random predictions with an AUC≈0.5. We
used this classiﬁer to evaluate the theoretical best and practical
worst case performance of Íntegro.
Evaluation method. To evaluate each system’s ranking qual-
ity, we ran the system using both inﬁltration scenarios starting
with a single attack edge. We then added another attack
edge, according to its timestamp if available, and repeated the
experiment. We kept performing this process until there were
no more edges to add. At the end of each run, we measured
the resulting AUC of each system, as explained next.
Performance metric. For the resulting ranked list of accounts,
we performed ROC analysis by moving a pivot point along the
list, starting from the bottom. If an account is behind the pivot,
we marked it as fake; otherwise, we marked it as real. Given
the ground-truth, we measured the TPR and the FPR across
the whole list. Finally, we computed the corresponding AUC,
which in this case quantiﬁes the probability that a random real
account is ranked higher than a random fake account.
Seeds and iterations. In order to make the chance of guessing
seeds very small, we picked 100 trusted accounts that are non-
victim, real accounts. We used a total trust that is equal to n,
the number of nodes in the given graph. We also performed
4We excluded the “friends” feature, as it can be computed from the graph.
11
0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Mean(area(under(ROC(curve(Number(of(a9ack(edges(IntegroYBest(IntegroYRF(IntegroYRandom(SybilRank(0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Mean(area(under(ROC(curve(Number(of(a9ack(edges((thousands)(IntegroYBest(IntegroYRF(IntegroYRandom(SybilRank((a) Distant-seed attack
(b) Random-seed attack
(a) Users connectivity
(b) Friendship growth over time
Fig. 6: The sensitivity of both systems to each seed-targeting attack
(CI=95%). In distant-seed attack, an attacker befriends users that are
at a particular distance from all trusted accounts, which represents
a practical worst case scenario for both system. In the random-seed
attack, the attacker directly befriends a subset of the trusted accounts.
Overall, both systems are sensitive to seed-targeting attacks.
increased, where Íntegro consistently outperformed SybilRank,
as shown in Fig. 6b. Notice that by selecting a larger number
of trusted accounts, it becomes much harder for an attacker to
guess which account is trusted, while the gained beneﬁt per
victimized trusted account is further reduced.
E. Deployment at Tuenti
We deployed both systems on a snapshot of Tuenti’s daily
active users graph in February 6, 2014. The graph consisted
of several million nodes and tens of millions of edges. We
had to mask out the exact numbers due to a non-disclosure
agreement with Tuenti. After initial analysis of the graph, we
found that 96.6% of nodes and 94.2% of edges belonged to
one giant connected component (GCC). Therefore, we focused
our evaluation on this GCC.
Preprocessing. Using a uniform random sample of 10K users,
we found that new users have weak connectivity to others due
to the short time they have been on Tuenti, as shown in Fig. 7a.
If these users were included in our evaluation, they would end
up receiving low ranks, which would lead to false positives.
To overcome this issue, we estimated the period after which
users accumulate at least 10% of the average number of friends
in Tuenti. To achieve this, we used a uniformly random sample
of 10K real users who joined Tuenti over the last 77 months.
We divided the users in the sample into buckets representing
how long they have been active members. We then calculated
the average number of new friendships they made after every
other month. As illustrated in Fig. 7b, users accumulated 53%
of their friendships during the ﬁrst 12 months. In addition,
18.6% of friendships were made after one month since joining
the network. To this end, we decided to defer the consideration
of users who have joined in the last 30 days since Feb 6, 2014,
which represented only 1.3% of users in the GCC.
Community detection. We applied the Louvain method on the
preprocessed GCC. The method ﬁnished quickly after just 5
iterations with a high modularity score of 0.83, where a value
of 1 corresponds to a perfect partitioning. In total, we found 42
communities and the largest one consisted of 220,846 nodes.
In addition, 15 communities were relatively large containing
more than 50K nodes. Tuenti’s account analysts veriﬁed 0.05%
Fig. 7: Preprocessing. In (a), there is a positive correlation between
number of days since a user joined Tuenti and how well-connected
the user is in terms of number of friends (Pearson’s r = 0.36). In
fact, 93% of all new users who joined Tuenti in the last 30 days had
weak connectivity of 46 friends or less, much smaller than the average
of 254 friends. In (b), we found that most of the friendship growth
happens in the ﬁrst month since joining the network, where users on
average establish 18.6% of their friendships. We accordingly defer
the consideration of users who joined Tuenti in the last 30 days, as
they will likely be assigned low ranks.
of the nodes in each detected community, and designated them
as trusted accounts for both systems.
Performance metric. As the number of users in the processed
GCC is large, it was infeasible to manually inspect and label
each account. This means that we were unable to evaluate the
system using ROC analysis. Instead, we attempted to determine
the percentage of fake accounts at equally-sized intervals in
the ranked list. We accomplished this in collaboration with
Tuenti’s analysts by manually inspecting a user sample in each
interval in the list. This percentage is directly related to the
precision of fake account detection, which is a performance
metric typically used to measure the ratio of relevant items
over the top-k highest ranked items in terms of relevance [61].
Evaluation method. We utilized the previously trained victim
classiﬁer in order to weight a copy of the graph. We then ran
both systems on two versions of the graph (i.e., weighted and
unweighted) for (cid:100)log2(n)(cid:101) iterations, where n is number of
nodes in the graph. After that, we examined the ranked list of
each system by inspecting the ﬁrst lowest-ranked one million
users. We randomly selected 100 users out of each 20K user
interval for inspection in order to measure the percentage of
fakes in the interval, that is, the precision. We do not include
the complete range due to conﬁdentiality reasons.
Results. As shown in Fig. 8a, Íntegro resulted in 95% precision
in the lowest 20K ranking user accounts, as opposed to 43%
by SybilRank and 5% by Tuenti’s user-based abuse reporting
system. This percentage dropped dramatically as we went up
in the list, which means our ranking scheme placed most of
the fakes at the bottom of the ranked list, as shown in Fig. 8b.
Let us consider SybilRank’s ranking shown in Fig. 8a and
Fig. 8c. The precision, starting with 43% for the ﬁrst interval,
gradually decreased until rising again at the 10th interval. This
pattern repeated at the 32nd interval as well. We inspected the
fake accounts at these intervals and found that they belonged
to three different, large communities. In addition, these fakes
had a large number of friends, much larger than the average
of 254 friends. In particular, the fakes from the 32nd interval
12
0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 Mean(area(under(ROC(curve(Distance(from(the(fake(region(IntegroYRF(SybilRank(0 0.2 0.4 0.6 0.8 1 1 10 20 30 40 50 60 70 80 90 100 Mean(area(under(ROC(curve(Number(of(vicSmized(trusted(accounts(IntegroYBest(IntegroYRF(IntegroYRandom(SybilRank(0(200(400(600(800(1000(0 500 1000 1500 2000 2500 Number(of(friends(Days(since(joining(TuenS(0 2 4 6 8 10 12 14 16 18 20 1 2 3 4 5 6 7 8 9 10 11 12 PorSon(of(expected(friendships((%)(Months(since(joining(TuenS((a) Precision at lower intervals
(b) Precision over the whole list
(c) Precision at higher intervals
(d) Node degree distribution
Fig. 8: Deployment results at Tuenti. The overall ranking quality of both systems is summarized in (b). Ideally, all fake accounts should be in
the bottom of the ranked list. In (a) and (c), we observed that Íntegro consistently outperforms SybilRank in term of fake account detection
precision (i.e., the percentage of fakes in each sample). In particular, most of the fake accounts identiﬁed by Íntegro were located at signiﬁcantly
lower locations in the ranked list, unlike SybilRank. Upon further inspection of fakes at higher intervals, we found that they established a large
number of attack edges, as suggested by the degree distribution in (d).
onwards had more than 300 friends, with a maximum of up
to 539. Fig. 8d shows the degree distribution for both veriﬁed
fake and real accounts. This ﬁgure suggests that fakes tend to
create many attack edges with real accounts, which conﬁrms
earlier ﬁndings on other OSNs such as Facebook [7]. Also,
this behavior explains why Íntegro outperformed SybilRank in
user ranking quality; these high degree fakes received lower
ranks as most of their victims were identiﬁed by the classiﬁer.
SybilRank in retrospect. SybilRank was initially evaluated on
Tuenti, where it effectively detected a signiﬁcant percentage
of the fakes [13]. The original evaluation, however, pruned
excessive edges of nodes that had a degree greater than 800,
which include a non-disclosed number of fakes that highly
inﬁltrated Tuenti. Also, the original evaluation was performed
on the whole graph, which included many dormant accounts.
However, our evaluation was based on the daily active users
graph in order to focus on active fake accounts that could be
harmful. While this change limited the number of fakes that
existed in the graph, it has evidently revealed the ineffective-
ness of SybilRank under social inﬁltration. Additionally, the
original evaluation showed that 10–20% of fakes received high
ranks, a result we also attest, due to the fact that these fake
accounts had established many attack edges. On the other hand,
Íntegro has 0–2% of fakes at these high intervals, and so it
delivers an order of magnitude better precision than SybilRank.
VI.
IMPLEMENTATION AND SCALABILITY
We implemented Íntegro in Mahout5 and Giraph6, which
are widely used, open-source distributed machine learning and
graph processing platforms, respectively. We next describe the
scalability of Íntegro using a synthetic benchmark.
Benchmark. We deployed Íntegro an Amazon Elastic MapRe-
duce7 cluster. The cluster consisted of one m1.small instance
serving as a master node and 32 m2.4xlarge instances serving
as slave nodes. We employed the small-world graph model [60]
to generate 5 graphs with an exponentially increasing number
of nodes. For each one of these graphs, we used the Facebook
dataset to randomly generate all feature vectors with the same
5http://mahout.apache.org
6http://giraph.apache.org/
7http://aws.amazon.com/elasticmapreduce
distribution for each feature. We then ran Íntegro on each of
the generated graphs and measured its execution time.
Results. Íntegro achieved a nearly linear scalability with the
number of nodes in a graph, as illustrated in Fig. 9. Excluding
the time required to load the 160M node graph into memory,
20 minutes for a non-optimized data format, it takes less than
2 minutes to train an RF classiﬁer and compute vulnerability
scores for nodes, and less than 25 minutes to weight the graph,
rank nodes, and ﬁnally sort them. This makes Íntegro compu-
tationally practical even for large OSNs such as Facebook.
VII. DISCUSSION
As mentioned in Section IV-F, Íntegro’s security guarantee
is sensitive to the performance of the deployed victim classiﬁer,
which is formally captured by the volume vol(Ea) in the bound
O(vol(Ea) log n), and can be practically measured by its AUC.
Sensitivity to victim classiﬁcation. As illustrated in Fig. 5,
improving the AUC of the victim classiﬁer from random with
AUC ≈ 0.5, to actual with AUC= 0.7, and ﬁnally to best with
AUC ≈ 1 consistently improved the resulting ranking in terms
of its AUC. Therefore, a higher AUC in victim prediction leads
to a higher AUC in user ranking. This is the case because the
ROC curve of a victim classiﬁer monotonically increases, so a
higher AUC implies a higher true positive rate (TPR). In turn, a
higher TPR means more victims are correctly identiﬁed, and so
more attack edges are assigned lower weights, which evidently
leads to a higher AUC in user ranking.
Sensitivity to social inﬁltration. Regardless of the used victim
classiﬁer, the ranking quality decreases as the number of attack
edges increases, as illustrated in Fig. 5. This is the case because
even a small false negative rate (FNR) in victim classiﬁcation
means more attack edges indecent to misclassiﬁed victims are