(7 rows)  
postgres=# explain select count(*) from t2 where c4=1;  
                                      QUERY PLAN                                         
---------------------------------------------------------------------------------------  
 Finalize Aggregate  (cost=167761.52..167761.53 rows=1 width=8)  
   ->  Gather  (cost=167761.31..167761.52 rows=2 width=8)  
         Workers Planned: 2  
         ->  Partial Aggregate  (cost=166761.31..166761.32 rows=1 width=8)  
               ->  Parallel Seq Scan on t2  (cost=0.00..165720.33 rows=416389 width=0)  
                     Filter: (c4 = 1)  
(6 rows)  
postgres=# select count(*) from t1 where c4=1;  
  count    
---------  
 1000046  
(1 row)  
Time: 169.707 ms  
postgres=# select count(*) from t2 where c4=1;  
  count    
---------  
 1000046  
(1 row)  
Time: 509.801 ms  
postgres=# select * from t1 where id=100;  
 id  |               info               | c1  | c2 | c3 | c4 |             ts               
-----+----------------------------------+-----+----+----+----+----------------------------  
 100 | d32424fc616965907c473bc8e1a123ba | 457 | 63 | 85 |  5 | 2023-07-04 07:30:05.451909  
(1 row)  
Time: 8.929 ms  
postgres=# select * from t2 where id=100;  
 id  |               info               | c1  | c2 | c3 | c4 |             ts               
-----+----------------------------------+-----+----+----+----+----------------------------  
 100 | d32424fc616965907c473bc8e1a123ba | 457 | 63 | 85 |  5 | 2023-07-04 07:30:05.451909  
(1 row)  
Time: 4.282 ms  
Time: 4.282 ms  
postgres=# explain select * from t1 order by id limit 10;  
                                      QUERY PLAN                                         
---------------------------------------------------------------------------------------  
 Limit  (cost=0.43..0.76 rows=10 width=61)  
   ->  Index Scan using t1_id_idx on t1  (cost=0.43..329839.15 rows=10000000 width=61)  
(2 rows)  
Time: 3.760 ms  
postgres=# explain select * from t2 order by id limit 10;  
                                      QUERY PLAN                                         
---------------------------------------------------------------------------------------  
 Limit  (cost=0.43..0.81 rows=10 width=61)  
   ->  Index Scan using t2_id_idx on t2  (cost=0.43..373328.43 rows=10000000 width=61)  
(2 rows)  
Time: 4.343 ms  
postgres=# select * from t1 order by id limit 10;  
 id |               info               |  c1  | c2  | c3 | c4 |             ts               
----+----------------------------------+------+-----+----+----+----------------------------  
  1 | 45d36ba92109a1420b2ac2806d8e561d | 7083 |  87 | 27 |  7 | 2023-07-04 07:30:05.432647  
  2 | 6fe46ccb8cf8f13773774e6223026891 | 6753 |  98 | 79 |  5 | 2023-07-04 07:30:05.451664  
  3 | e439cbc684978e4ab62ba4cee591ec7b |  118 | 601 | 88 | 10 | 2023-07-04 07:30:05.451696  
  4 | 1817f420d924588a0d2bc343efe70ceb | 8137 | 313 | 40 |  4 | 2023-07-04 07:30:05.451701  
  5 | e605cd17b528650df09daa36273ca472 | 5316 | 643 | 39 |  6 | 2023-07-04 07:30:05.451703  
  6 | b11da20b18f3cc63ed2dfcfcc8454737 | 1632 | 806 | 59 |  9 | 2023-07-04 07:30:05.451705  
  7 | 38197cd4e42eb089f19066a0d968c9e7 | 6830 | 813 |  5 |  5 | 2023-07-04 07:30:05.451708  
  8 | 1d46375a93c0bed800bcbf62d3a70918 |  499 | 611 | 35 |  0 | 2023-07-04 07:30:05.45171  
  9 | 77247cfad6597dca0e89470e30060cb6 | 2947 |  34 | 78 |  9 | 2023-07-04 07:30:05.451712  
 10 | 5532018eb724d518ba73667e9980c5cf | 5341 | 407 | 97 |  0 | 2023-07-04 07:30:05.451714  
(10 rows)  
Time: 9.199 ms  
postgres=# select * from t2 order by id limit 10;  
 id |               info               |  c1  | c2  | c3 | c4 |             ts               
----+----------------------------------+------+-----+----+----+----------------------------  
  1 | 45d36ba92109a1420b2ac2806d8e561d | 7083 |  87 | 27 |  7 | 2023-07-04 07:30:05.432647  
  2 | 6fe46ccb8cf8f13773774e6223026891 | 6753 |  98 | 79 |  5 | 2023-07-04 07:30:05.451664  
  3 | e439cbc684978e4ab62ba4cee591ec7b |  118 | 601 | 88 | 10 | 2023-07-04 07:30:05.451696  
  4 | 1817f420d924588a0d2bc343efe70ceb | 8137 | 313 | 40 |  4 | 2023-07-04 07:30:05.451701  
  5 | e605cd17b528650df09daa36273ca472 | 5316 | 643 | 39 |  6 | 2023-07-04 07:30:05.451703  
  6 | b11da20b18f3cc63ed2dfcfcc8454737 | 1632 | 806 | 59 |  9 | 2023-07-04 07:30:05.451705  
  7 | 38197cd4e42eb089f19066a0d968c9e7 | 6830 | 813 |  5 |  5 | 2023-07-04 07:30:05.451708  
  8 | 1d46375a93c0bed800bcbf62d3a70918 |  499 | 611 | 35 |  0 | 2023-07-04 07:30:05.45171  
  9 | 77247cfad6597dca0e89470e30060cb6 | 2947 |  34 | 78 |  9 | 2023-07-04 07:30:05.451712  
 10 | 5532018eb724d518ba73667e9980c5cf | 5341 | 407 | 97 |  0 | 2023-07-04 07:30:05.451714  
(10 rows)  
Time: 3.574 ms  
```  
如果同一张表想使用行列混合存储, 可以使用分区表, 不同的分区采用行、列存储.  
```  
CREATE TABLE parent(ts timestamptz, i int, n numeric, s text)  
PARTITION BY RANGE (ts);  
-- columnar partition  
CREATE TABLE p0 PARTITION OF parent  
FOR VALUES FROM ('2020-01-01') TO ('2020-02-01')  
USING COLUMNAR;  
-- columnar partition  
CREATE TABLE p1 PARTITION OF parent  
FOR VALUES FROM ('2020-02-01') TO ('2020-03-01')  
USING COLUMNAR;  
-- row partition  
CREATE TABLE p2 PARTITION OF parent  
FOR VALUES FROM ('2020-03-01') TO ('2020-04-01');  
INSERT INTO parent VALUES ('2020-01-15', 10, 100, 'one thousand'); -- columnar  
INSERT INTO parent VALUES ('2020-02-15', 20, 200, 'two thousand'); -- columnar  
INSERT INTO parent VALUES ('2020-03-15', 30, 300, 'three thousand'); -- row  
```  
hydra的列存储表的update和delete是通过打标来实现的, 另外存储了一个打标的数据, 如果一行被删除了, 则标记为删除, 但是在列存储里面实际还存在.   
所以随着update越来越多, 可能膨胀, 膨胀后需要整理来提升列存储性能.  
测试使用aliyun oss做冷热分离存储, 遇到一些问题, 现象就是没有建立oss服务连接.  
先用duckdb生成一些parquet数据到oss.  
[《DuckDB DataLake 场景使用举例 - aliyun OSS对象存储parquet》](../202210/20221026_01.md)    
```    
AK ID: LTAI***
AK Secret: 8a8k***
Endpoint外网域名: oss-cn-shanghai.aliyuncs.com    
Bucket名称: jemu***
Object路径: digoal/   
```    
```  
IT-C02YW2EFLVDL:~ digoal$ ./duckdb/build/release/duckdb   
v0.7.1 b00b93f  
Enter ".help" for usage hints.  
Connected to a transient in-memory database.  
Use ".open FILENAME" to reopen on a persistent database.  
D INSTALL httpfs;    
D load httpfs;  
D set s3_access_key_id='LTAI***';             
D set s3_secret_access_key='.......................';   
D set s3_endpoint='s3.oss-cn-shanghai.aliyuncs.com';           
D create table a(id int, first_name text, last_name text);    
D insert into a select range, md5(random()::text), md5(random()::text) from range(1,100000);   
D copy a to 's3://jemuod20230704161428/digoal/a.parquet';      
100% ▕████████████████████████████████████████████████████████████▏   
D select count(*) from 's3://jemuod20230704161428/digoal/a.parquet';   
┌──────────────┐  
│ count_star() │  
│    int64     │  
├──────────────┤  
│        99999 │  
└──────────────┘  
```
然后使用hydra访问oss.  
```  
postgres=# create extension parquet_s3_fdw ;  
CREATE EXTENSION  
postgres=# CREATE SERVER parquet_s3_srv FOREIGN DATA WRAPPER parquet_s3_fdw OPTIONS (use_minio 'false', region 'oss-cn-shanghai.aliyuncs.com');        
postgres=# CREATE USER MAPPING FOR public SERVER parquet_s3_srv OPTIONS (user 'LTAI***', password '8a8k***');  
postgres=# CREATE FOREIGN TABLE userdata ( id int,  first_name   text, last_name    text)    
SERVER parquet_s3_srv OPTIONS ( filename 's3://jemuod20230704161428/digoal/a.parquet'  );    
-- CREATE FOREIGN TABLE userdata ( id int,  first_name   text, last_name    text)    
-- SERVER parquet_s3_srv OPTIONS ( dirname 's3://jemuod20230704161428/digoal/'  );    
postgres=# \set VERBOSITY verbose  
postgres=# select * from userdata;  
ERROR:  XX000: parquet_s3_fdw: failed to exctract row groups from Parquet file: failed to open Parquet file HeadObject failed  
LOCATION:  extract_rowgroups_list, parquet_impl.cpp:992  
postgres=# insert into userdata values (1,'test','test');  
ERROR:  XX000: parquet_s3_fdw: failed to open Parquet file HeadObject failed  
LOCATION:  parquetS3BeginForeignModify, parquet_impl.cpp:3783  
```
在这个过程中看不到与oss建立的连接, 正常情况下应该有连接:   
```
postgres=# select * from parquet_s3_fdw_get_connections();
 server_name | valid 
-------------+-------
(0 rows)
```
其他功能建议有兴趣的小伙伴参考文档.   
https://docs.hydra.so/concepts/using-hydra-columnar  
## 参考  
https://github.com/citusdata/citus  
https://github.com/hydradatabase/hydra  
https://github.com/pgspider/parquet_s3_fdw  
https://docs.hydra.so/concepts/using-hydra-columnar  
vector  
pg_ivm  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
