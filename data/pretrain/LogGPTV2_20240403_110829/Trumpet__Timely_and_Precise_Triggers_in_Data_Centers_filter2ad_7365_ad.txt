DoS attack, and show that removing some of our optimiza-
tions (discussed below) can result in unﬁnished sweeps. We
also demonstrate that, in many cases, shorter epoch durations
cannot be sustained in at least one current CPU.
Performance Optimizations. We lazily reset counters and
remove old ﬂows. In each ﬂow table entry, we store the epoch
number when the entry was reset. Then, when we update an
entry or read its data during trigger sweep, if the stored epoch
(a) Multiple sweeps
(b) Zoomed in View
Figure 4: Queue-adaptive sweep scheduling strategy is effective at
keeping queue sizes below 150 packets or 10 µs.
number does not match the current, we reset the statistics
(lines 15-16 in Algorithm 1).
Trumpet also incorporates several memory optimizations
for this phase. Trigger entries are stored contiguously in
memory to facilitate hardware cache prefetching. We store
the trigger repository in a huge page to reduce TLB misses
and store the list of ﬂow entries that match a trigger in a
chunked linked list [43] to improve access locality. Each
chunk contains 64 ﬂow entries, and these are allocated from
a pre-allocated huge page, again for TLB efﬁciency. For
triggers with many ﬂows, chunking reduces the cost of pointer
lookups comparing to a linked list of ﬂows at the cost of
slightly higher memory usage.
Our ﬁnal optimization involves efﬁciently gathering statis-
tics at the required ﬂow granularity. For example, to support
a trigger that reports if the volume of trafﬁc to a host (A) from
any source IP (at ﬂow-granularity of source IP) is larger than
a threshold, we must track the volume per source IP across
ﬂows. We can track this with a hash table per trigger, but this
adds memory and lookup overhead. Instead, we dynamically
instantiate a new instance of the trigger: when a packet from
source IP X matches the trigger with a ﬁlter dstIP=A, we
create a new trigger with ﬁlter srcIP=X and dstIP=A.
5.6 Degrading Gracefully Under DoS At-
tacks
It is important for Trumpet to be robust to DoS attacks that
aim at exhausting resources in the monitoring system and
either cause packet loss, or prevent trigger sweep completion
(which would prevent accurate detection of events). The
expensive steps in Trumpet are matching new ﬂows against
triggers in the match-and-scatter phase and updating a trigger
based on its ﬂow list in the gather-test-and-report phase.
We assume the attacker knows the design of the system. To
attack the system by triggering the largest possible number
of expensive match operations, the attacker should send one
minimal-sized packet per ﬂow. With this, sweeps might not
complete, so trigger reports might not be correctly reported.
To mitigate this, when we detect unﬁnished sweeps, we im-
pose a DoS threshold: matching is invoked only on ﬂows
whose size in bytes exceeds this threshold in the ﬁlter table,
and ﬂows will be removed from the ﬂow table if they send
fewer bytes than the threshold as new ﬂows collide with them
in the ﬂow table or triggers read them at sweep. The DoS
threshold can be predicted by proﬁling the cost of each packet
processing operation and each matching operation (Section
6). This design comes at the cost of not monitoring very
small ﬂows: we quantify this tradeoff in Section 7.
010203050050100150200Time (ms)# Packets  QlengthMicrostep00.51050100150Time (ms)# Packets  QlengthMicrostep3Mbps or not. In step 5, TPMs respond to the controller polls
after ﬁnishing phase 2 when they can steal cycles from packet
processing; TPM allows triggers to keep the history of a few
last epochs to answer polls. Finally in step 6, TEM evaluates
the event after receiving all poll responses. For this to work,
TEM relies on time synchronized measurement epochs; the
synchronization accuracy of PTP [29] should be sufﬁcient for
TEM.
Network wide queries. The approach of dividing a network-
wide threshold for an aggregate function can accommodate
many kinds of aggregate functions. For example, it is pos-
sible to design thresholds for triggers to bound the error on
any convex function on the average of quantities distributed
among hosts [50] or their standard deviation [18]. Beyond
these, Trumpet can also support other kinds of network-wide
queries. For example: a) Gather statistics from many events:
To ﬁnd if 50% of VMs (or a certain number of VMs) of a
tenant receive trafﬁc exceeding a threshold, we add events
for each VM in Trumpet, gather their trigger satisfaction ev-
ery epoch and report if more than 50% of the related events
happened. b) Drill down based on events: TEM can install
events conditionally, for example, install a heavy hitter detec-
tion event only when another event (e.g., loss) happens. c)
Estimating network-wide statistics: We can monitor standard
deviation of trafﬁc volume to a set of VMs with a bounded
error, ε by deﬁning two events over the standard deviation
(std  stdold + ε) and updating them
accordingly if one of them is satisﬁed. d) Relative predicates:
By feeding the estimate of average and standard deviation to
another event, we can detect outlier VMs that receive more
than k× standard deviation above the average.
40G and beyond. Although Trumpet needs only one core to
process 14.8Mpps small packets on 10G links, or full line rate
650 byte packets on 40G links, at 100G and/or with smaller
packet sizes, multiple cores might be necessary. To avoid
inter-core synchronization, TEM runs independent TPMs on
each core and treats them as independent entities. Some syn-
chronization overhead is encountered at the TEM, but that is
small as it is incurred only when one of triggers is satisﬁed.
Assuming all packets from a ﬂow are usually handled by the
same core, this ensures TPMs can keep the state of a ﬂow
correctly. Our network-wide use-case in Section 7.1 demon-
strates this capability. It is also possible to design, at each host
with multiple TPMs, a local-TEM which performs similar
aggregation at the host itself with minimal synchronization
overhead: this can reduce polling overhead and speed up
event detection at the TEM.
DoS resiliency. TEM allows operators to set a DoS thresh-
old such that ﬂows whose size are below that threshold are
not processed. This reduces matching overhead, allowing
Trumpet to degrade gracefully under DoS attacks. TEM cal-
culates the threshold based on a model that proﬁles ofﬂine,
using the set of deﬁned triggers (Eq. 1), the TPM processing
costs in the end-host system conﬁguration. Trumpet process-
ing costs include: a) packet processing and checking the ﬁlter
table, TP b) matching, TM c) updating the ﬂow table, TU and
d) sweeping, TS. The goal is to ﬁnd the maximum threshold
Figure 5: TEM interactions with TPMs
5.7 Summary
Trumpet balances several conﬂicting goals: event expres-
sivity, tight processing and delay budgets, and efﬁcient core
usage. It achieves this by carefully partitioning trigger pro-
cessing over two phases (match-and-scatter and gather-test-
and-report) to keep data access locality per packet processing,
keep per ﬂow statistics efﬁciently, and access trigger infor-
mation once per epoch. To make match-and-scatter phase
efﬁcient, Trumpet uses tuple search matching, NIC polling
(DPDK), batching packets, cache prefetching, huge pages
(fewer TLB misses) and caching last ﬂow entry. To minimize
processing delay during gather-test-and-report, we proposed
a queue-adaptive multi-step sweep. We optimized the sweep
to ﬁnish within the query time interval using lazy reset (bring-
ing less data into cache), accessing data in a linear fashion
(leveraging cache prefetching), checking the time less often,
chunking ﬂow entries (fewer pointer jumps) and using huge
pages (fewer TLB misses). While some of these are well-
known, others such as the adaptive sweep, our two-phase
partitioning, and our approach to DDoS resiliency are novel.
Moreover, the combination of these techniques is crucial to
achieving the conﬂicting goals. Finally, our experiences with
cache and TLB effects and data structure design can inform
future efforts in fast packet processing.
6. TRUMPET EVENT MANAGER
Trumpet Event Manager (TEM) translates network-wide
events to triggers at hosts, collects satisﬁed triggers and statis-
tics from hosts, and reports network-wide events. Figure 5
shows the detailed process using an example: Consider an
event expressed as a predicate over the total trafﬁc volume
received by a service on two hosts where the predicate is true
if that quantity exceeds a threshold of 10Mbps. In step 1,
TEM statically analyzes the event description to determine
the trigger predicate, ﬁnds the hosts to install the triggers on
based on the event ﬁlter deﬁned by the service IP addresses
and the mapping of IP addresses to hosts. Then, it divides
the threshold among the triggers and installs them in step 2.
For the example, we set the threshold for each host as half of
the event threshold (5Mbps). If neither of the host triggers
exceed 5Mbps, their sum cannot be larger than 10Mbps. In
step 3, TPMs at hosts measure trafﬁc and send trigger sat-
isfaction messages to TEM specifying the event and epoch
when evaluating triggers in the gather-test-and-report phase.
In step 4, upon receiving the ﬁrst satisfaction report for
the event, TEM polls the other hosts for their local value of
the quantity at that epoch. For our example, a TPM may
have sent a satisfaction with value 7Mbps, thus TEM asks
for the value at the other TPM to check if its value is above
TEM3: Satisfaction5: Poll replyTPMTPM1: Event definition6: EventMake eventUpdate event2: Install triggers4: PollFind hosts & translate to triggersvalue that keeps total Trumpet processing time, T , below 1s in
each second. We can ﬁnd T − TP by proﬁling the free time of
the CPU in an experiment that only forwards the trafﬁc with
smallest packets that do not pass DoS threshold. Matching
overhead per ﬂow (Match(#patterns)) is a linear function of #
ﬁlter patterns with coefﬁcients that can be calculated ofﬂine.
Similarly, we compute maximum time to update per ﬂow
statistics (Update) and the time to update a trigger based on a
ﬂow (Sweep) ofﬂine.6 The factor 1.5 in Eq. 4 is because the
sweep can run in multiple steps while new ﬂows may arrive.
As a result, triggers may keep ﬂow entries for both ﬂows that
came last epoch and current epoch. Thus, the sweep process
may process 50% more entries per trigger on average. We
evaluate the accuracy of our model in Section 7.2.
rategood
)
(1)
(2)
(3)
(4)
rategood
avg pkt per ﬂow
avg pkt per ﬂow
ratedos
threshold +
T = TP + TM + TU + TS
TM = Match(#patterns)× (
TU = U pdate× rategood
TS = 1.5× Sweep× max trigger per ﬂow×
TEM Scalability and TPM performance. As we show in
Section 7, TEM can process nearly 16M trigger satisfaction
reports per second per core. If necessary, we can scale TEM
even more by sharding events on different servers or by re-
ducing the frequency of polling if TEM divides the threshold
unequally based on the history of trigger satisfaction at each
host [10]. To avoid delaying packets at end-hosts, TPM uses
non-blocking sockets. To reduce poll response latency, TPM
batches socket reads and writes and poll processing. TEM
can, if necessary (left to future work), reduce TPM overhead
by slicing event ﬁlters to minimize the number of trigger
patterns for matching, and by time multiplexing triggers on
hosts by (un)installing them over time.
7. EVALUATION
In this section, we evaluate Trumpet’s expressivity and
performance using an implementation of Trumpet’s TPM and
TEM (10,000 lines of C code).7 Our implementation uses
DPDK 2.2 [15] to bypass the network stack. Our experiments
in this section are conducted on 3 Xeon E5-2650 v3 2.30GHz
with two 10-core CPUs 25MB L3 and 256KB L2 cache. Our
machine has an Intel 82599 10G NIC.
7.1 Expressivity
Trumpet is expressive enough to track and control ﬁne time-
scale ﬂow dynamics, correlate or drill-down on interfering
ﬂows, or detect service-scale anomalies. To demonstrate this,
we implemented in Trumpet the three use-cases discussed in
Section 2 whose event deﬁnition is presented in Section 3.
Identifying losses caused by trafﬁc bursts. Trumpet can
detect losses caused by trafﬁc bursts and, in a tight control
6The model assumes that the attacker is not aware of the installed triggers,
thus triggers only need to process “good” ﬂows. The model can be extended
to the case where the attacker knows the installed triggers; we have left this
to future work.
7Available at https://github.com/USC-NSL/Trumpet
(a) Original
(b) With Trumpet+Pacing
Figure 6: Losses caused by bursts
(a) Congestion diagnosis
(b) Network-wide scenario
Figure 7: Network-wide and congestion usecases
loop, activate pacing to smooth bursts; this can ameliorate
the overhead of pacing [1, 21]. Such an approach can be
used in data centers with bottlenecks in the wide-area: bursts
can be preferentially paced to reduce loss. As an aside, VM
stacks, which may often run different transport protocols,
can use Trumpet instead of implementing burst (and other
kinds of event) detection within their protocol stacks. In our
experiment, a user connects to a server in the cloud through
an edge router with shallow queues. The connection between
the Internet user and the edge router is 10Mbps and incurs
100ms delay. The link from the server to the edge router is
1 Gbps. Figure 6a shows that there are packet losses every
time there is a trafﬁc burst. Trumpet quickly detects the burst
and installs a trigger that informs the controller whenever
there are more than 8 lost packets within a burst. Figure 6b
shows that the trigger is matched after 3 seconds from the
starting point of the experiment when the bursts become large
enough. Based on the triggers, the controller quickly enables
TCP pacing to eliminate the losses and achieves much higher
throughput (from 1.94Mbps to 5.3Mbps).
Identifying the root cause of congestion. Despite advances
in transport protocols, transient packet losses can be triggered
during sudden onset of ﬂows. In data centers, these losses
can slow distributed computations and affect job completion
times. To diagnose this transient congestion, it is necessary
to identify competing ﬂows that might be root causes for the
congestion. Trumpet can be used for this task in two ways. In
the reactive drill-down approach, the operator deﬁnes a TCP
congestion detection event and, when it is triggered for many
ﬂows, programmatically installs another event to ﬁnd heavy
hitters at the common bottleneck of those ﬂows. This may
miss short ﬂows because of the delay in detecting congestion.
In the proactive approach, the operator installs both events
on all servers in advance and correlates their occurrence.
To demonstrate both approaches, we use three senders and