done on a commodity desktop PC running Windows XP
with dual 3.40 GHZ CPUs and 3.00 GB of RAM.
2.1 Color Features
Recall that an Asirra image is 250-by-250 pixels. We di-
vide the image into N vertical and N horizontal strips of
equal width. We thus obtain a division of the image into a
grid of N 2 cells. Each cell is a square of width and height
250/N (rounded to the nearest integer).
We also partition the color space. We use the HSV (hue,
saturation, value) model of color, since it is closer to hu-
man perception of color, and thus easier to interpret, than
the RGB (red, green, blue) model. We subdivide the hue
channel of the color spectrum into Ch bands of equal width,
the saturation channel into Cs bands of equal width and
the value channel into Cv bands of equal width. Altogether,
this gives us a partition of the color space into ChCsCv color
regions.
Informally, the feature vector associated with an image
indicates, for every cell in the image and every color re-
gion, whether there is at least one pixel in the cell which
belongs to the color region. Note that these features are
boolean. Unlike color histograms, they do not indicate how
many pixels in a given cell fall within a certain color region,
but only whether one or more pixel in the cell falls in the
color region. Our experiments show that these boolean fea-
tures yield more accurate classiﬁers than color histograms,
in addition to being more eﬃcient.
More precisely, the feature vector F(N, Ch, Cs, Cv) is a
boolean vector of length N 2ChCsCv. The boolean feature
associated with cell (x, y) ∈ [1, . . . , N ]× [1, . . . , N ] and color
region (h, s, v) ∈ [1, . . . , Ch] × [1, . . . , Cs] × [1, . . . , Cv] takes
the value 1 (or true) if there is one or more pixel in cell (x, y)
of a color that belongs to color region (h, s, v). Otherwise,
the feature takes the value 0 (false).
We trained SVM classiﬁers with these color features, and
measured their accuracy using 5-fold cross-validation, as ex-
plained above. Our results are given in Table 1 for vari-
ous values of the parameters N, Ch, Cs, Cv and for training
sets of various sizes. For example, the feature set F3 =
F(5, 10, 6, 6) consists of 9,000 color features obtained with
a division of images into 25 cells (N = 5) and a division
of the color space into 360 color regions (Ch = 10, Cs = 6
and Cv = 6). With 4,000 training images, an SVM classiﬁer
using the feature set F3 is on average 74.6% accurate (over
the 5 experiments of 5-fold cross-validation). With 8,000
training images, the accuracy of this classiﬁer increases to
75.7 %.
Combining color features computed on cells of various
sizes further improves the accuracy of our classiﬁer. We
experimented with the union of the three feature sets F1 =
F(1, 10, 10, 10), F2 = F(3, 10, 8, 8) and F3 = F(5, 10, 6, 6) of
Table 1. The total number of color features in F1 ∪ F2 ∪ F3
is 15,760. The accuracy of a classiﬁer using these features is
shown in Table 2. With 4,000 training images, the classiﬁer
is 76.3 % accurate. With 8,000 training images, it is 77.1 %
accurate.
Color features and classiﬁer accuracy. We observed
in our experiments that SVM classiﬁers trained on boolean
color features are more accurate than those trained on color
histograms. This ﬁnding runs counter to intuition: boolean
features, which record only the presence or absence in an
image of pixels belonging to a certain region of the color
spectrum, encode strictly less information than color his-
tograms, which also record the number of such pixels. We
advance two hypotheses for why boolean features outper-
form color histograms. The ﬁrst is that boolean color fea-
tures, unlike color histograms, are scale-independent: they
record the presence or the absence of, say, the green of a
cat’s eye or the pink of a dog’s tongue regardless of the size
of the eye or tongue in the picture. Our second hypothe-
sis is that the distribution of boolean features is much more
regular (only two values are possible) than the distribution
of real-valued color histograms (in which the range of pos-
sible values may cover several orders of magnitude). The
regularity of boolean features facilitates the tuning of SVM
parameters (notably γ), which results in superior accuracy.
Predictive power of individual color features. We
observe that individual boolean color features, in isolation,
fail to distinguish cats from dogs with any accuracy. The
graph in Figure 1 shows the 1,000 boolean color features
in F1 = F(1, 10, 10, 10), plotted according to the fraction
Figure 1: This graph shows the 1,000 boolean color
features in F1 = F(1, 10, 10, 10), plotted according to
the fraction of cats (horizontal axis) and the fraction
of dogs (vertical axis) for which the feature evaluates
to “True”. Note that features are clustered along the
diagonal.
of cats (horizontal axis) and the fraction of dogs (vertical
axis) for which the feature evaluates to “True”. The features
are clustered along the diagonal, which indicates that the
ability of any given color feature to distinguish between cats
and dogs is very weak. Nevertheless, an SVM classiﬁer can
harness the aggregate power of the color features to produce
more accurate predictions. The success of our classiﬁer does
not come from the careful selection of a few colors with high
predictive values, but rather from the combination of a large
number of weakly predictive features.
2.2 Texture Features
Approaches to texture recognition can be broadly divided
into two categories. The ﬁrst category takes a statistical
approach to texture computation. Texture is deﬁned via
quantitative measurements of intensity in diﬀerent regions
of the image (e.g. by convolution with a Gabor ﬁlter [11]).
The second category takes a structural approach, and de-
ﬁnes texture as a set of texture tiles (also called texels) in
repeated patterns. We experimented with both approaches,
and found that structural measurements of texture produced
more accurate classiﬁers.
We start with an informal presentation of our structural
approach to texture recognition. We extract small sub-
images (5-by-5 pixels) from training images of cats and dogs.
We call these sub-images texture tiles. Figure 2 shows exam-
ples of texture tiles extracted from Asirra images. We collect
a set T of texture tiles of size t = |T |, such that the distance
between any two tiles in T is above a certain threshold (we
deﬁne below our measure of distance between tiles). This
ensures that the tiles in T are suﬃciently diverse, and that
there are no duplicate tiles. The feature vector associated
with an image is the vector of distances between the image
and each texture tile in T (we deﬁne below our measure of
distance between an image and a tile). Finally, we train an
SVM classiﬁer with these feature vectors. More precisely,
we proceed as follows.
Figure 2: Example of texture tiles (5-by-5 pixels) ex-
tracted from images of cats and dogs. The classiﬁer
of section 2.2 relies on such texture tiles.
Selection of texture tiles. We select random images of
cats and dogs from the set of training images. We divide
each image into vertical and horizontal strips of equal width
(5 pixels). We thus obtain a division of each image into
(250/5)2 = 2500 feature tiles. Each feature tile is a square
of 5-by-5 pixels. Let us denote T0 this initial set of candidate
tiles. We deﬁne the distance between two tiles as the average
Euclidian distance between the pixels of the tiles in RGB
color space. From T0, we then compute a subset T of texture
tiles iteratively as follows. Initially, T is empty. We consider
in turn each tile T ∈ T0.
If there already exists a tile in
T whose distance to T is below a certain threshold δ, we
discard T . Otherwise, we add the tile T to T . We repeat
this computation for all candidate tiles in T0 until we obtain
a set T of size t. Note that the initial set T0 must be chosen
of suﬃciently large size to ensure the existence of a subset
T of size t.
Feature vector. The feature vector associated with an
image is the vector of distances between the image and each
of the t texture tiles in T . The distance between an image A
and a texture tile T ∈ T is deﬁned as follows. For 0 ≤ i, j ≤
(250 − 5), let us denote Ai,j the square sub-image of A, of
width 5 pixels and height 5 pixels, whose top left corner is
the pixel of A in row i and column j. We deﬁne the distance
d(Ai,j, T ) between a sub-image Ai,j and a texture tile T as
the maximum of the Euclidean distance between their pixels
in RGB space. The distance between A and T is deﬁned as
d(A, T ) = mini,j d(Ai,j, T ). Distances are normalized to the
range [0, 1].
Results. We trained an SVM classiﬁer with these tex-
ture features, and measured its accuracy using 5-fold cross-
validation. Our results are given in Table 3. The feature set
G1 consists of 1,000 features which record the distance of an
image to 1,000 texture tiles. The texture tiles are selected
such that the distance between any two of them is at least
40.0. With 4,000 training images, an SVM classiﬁer using
the feature set G1 is 74.5 % accurate. We deﬁne a feature
set G2 which consists of 5,000 features which record the dis-
tance of an image to 5,000 texture tiles similarly selected.
(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:3)(cid:5)(cid:2)(cid:2)(cid:3)(cid:6)(cid:7)(cid:8)(cid:9)(cid:2)(cid:3)(cid:10)(cid:2)(cid:3)(cid:11)(cid:2)(cid:3)(cid:1)(cid:2)(cid:3)(cid:2)(cid:3)(cid:10)(cid:2)(cid:3)(cid:11)(cid:2)(cid:3)(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:3)(cid:5)(cid:2)(cid:2)(cid:3)(cid:12)(cid:13)(cid:14)(cid:9)Texture features
# Images
Feature set
G1
G2
G2
# tiles
1,000
5,000
5,000
δ
40.0
40.0
40.0
Total
5,000
5,000
10,000
Training set
4,000
4,000
8,000
mean
74.5 %
78.0 %
80.4 %
Classiﬁer accuracy
stdev
2.0
1.9
0.9
Table 3: Accuracy of SVM classiﬁers trained on texture features extracted from Asirra images. The texture
features are described in section 2.2. The accuracy of the classiﬁer is the fraction of cat and dog images
classiﬁed correctly in the test set.
Features
# Images
(F1 ∪ F2 ∪ F3) + G2
(F1 ∪ F2 ∪ F3) + G2
Total
5,000
10,000
Training set
4,000
8,000
Classiﬁer accuracy
stdev
mean
80.3 %
82.7 %
1.4
0.5
Table 4: Accuracy of the combined outputs of the color classiﬁer of section 2.1 and the texture classiﬁer of
section 2.2. The color classiﬁer is given half the weight of the texture classiﬁer (see section 2.3).
Figure 3: Probability distribution of the combined
output of the color and texture classiﬁers of sec-
tion 2.3.
Figure 4: The cats and dogs in our sample that are
most cat-like and most dog-like, according to the
classiﬁer of section 2.3.
Using this larger feature set and 4,000 training images, the
accuracy of our SVM classiﬁer increases to 78.0%. With
8,000 training images, it is 80.4 % accurate.
2.3 Combination of Color and Texture
Features
The SVM classiﬁers of sections 2.1 and 2.2 produce for
each image a real-valued estimate of whether the image is
of a cat or of a dog. We mapped the “cat” class to the value
1.0 and the “dog” class to the value −1.0. Thus, an image
which produces a positive output is labelled “cat” and an
image which produces a negative output is labelled “dog”.
The output of diﬀerent SVM classiﬁers can be combined
simply by a weighted average of the estimates they produce.
We combined in this way an SVM classiﬁer which uses the
set of color features F1 ∪ F2 ∪ F3 (with weight 1/3) and a
second SVM classiﬁer which uses the set of texture features
G2 (with weight 2/3). The accuracy of this combination is
given in Table 4. With a training set of 8,000 images, we
obtain a classiﬁer which is 82.7 % accurate. The confusion
matrix of this classiﬁer is in Table 5.
Classiﬁed as cat Classiﬁed as dog
Cats
Dogs
4,271
997
729
4,003
Table 5: Confusion matrix for the combined color
and texture classiﬁer of section 2.3.
Figure 3 shows the probability distribution of the com-
bined outputs of the color and texture SVM classiﬁers, for
the “cat” and “dog” classes. Figure 4 shows the cats and
dogs in our sample of 13, 000 pets that are most cat-like and
most dog-like, according to the combined classiﬁer.
Accuracy versus completeness. We can achieve lower
error rates if we allow the classiﬁer to assign some images
a “don’t know” label. The quality of this 3-class classiﬁer is
measured by completeness (the fraction of images classiﬁed
as either “cat” or “dog”) and accuracy (the fraction of images
in the “cat” and “dog” classes which are accurately classiﬁed).
We turn our combined color and texture classiﬁer into a 3-
class classiﬁer as follows. The classiﬁer is parameterized by
(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:2)(cid:1)(cid:4)(cid:1)(cid:2)(cid:5)(cid:1)(cid:2)(cid:5)(cid:6)(cid:1)(cid:2)(cid:5)(cid:7)(cid:1)(cid:2)(cid:5)(cid:3)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:11)(cid:15)(cid:16)(cid:13)(cid:17)(cid:9)(cid:17)(cid:18)(cid:19)(cid:18)(cid:10)(cid:20)(cid:21)(cid:18)(cid:11)(cid:10)(cid:16)(cid:18)(cid:17)(cid:22)(cid:10)(cid:18)(cid:13)(cid:23)(cid:1)(cid:1)(cid:2)(cid:1)(cid:6)(cid:1)(cid:2)(cid:1)(cid:7)(cid:1)(cid:2)(cid:1)(cid:3)(cid:24)(cid:25)(cid:24)(cid:6)(cid:24)(cid:5)(cid:1)(cid:5)(cid:6)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:13)(cid:22)(cid:10)(cid:30)(cid:22)(cid:10)Most dog-likeMost cat-likeCatsDogsin section 2.3. Let δcat denote the probability distribution
of the output of Cl over images of cats, and δdog denote the
probability distribution of the output of Cl over images of
dogs. With this classiﬁer, the attacker can estimate
(cid:89)
Pr[C|A] =
(cid:89)
δcat(Cl(Ii))
δdog(Cl(Ii)).
i | ai=cat
i | ai=dog
Figure 5: Accuracy versus completeness of the com-
bined color and texture classiﬁer of section 2.3.
a real-valued parameter  ≥ 0.
Images which produce an
output smaller than − are labelled “dog”.
Images which
produce an output between − and  are labelled “don’t
know”. Finally, images which produce an output larger than
 are labelled “cat”.
Figure 5 shows a plot of the accuracy versus completeness
of this classiﬁer obtained for diﬀerent values of the param-
eter . The base accuracy of the combined color and tex-
ture classiﬁer, when classifying all images (completeness of
100 %), is 82.7 %. If the classiﬁer can ignore half the images
(completeness of 50 %), its accuracy rises to 94.8 %. For
20 % completeness, accuracy rises to 98.5 %.
3. ATTACKING ASIRRA
In this section, we describe the application of the machine
classiﬁers of section 2 to attacking Asirra. Recall that an