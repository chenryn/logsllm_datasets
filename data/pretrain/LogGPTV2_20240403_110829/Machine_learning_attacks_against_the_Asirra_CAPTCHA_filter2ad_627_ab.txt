### 2.1 Color Features

The Asirra image is 250-by-250 pixels. We divide the image into \( N \) vertical and \( N \) horizontal strips of equal width, resulting in a grid of \( N^2 \) cells. Each cell is a square with a width and height of \( \frac{250}{N} \) (rounded to the nearest integer).

We also partition the color space using the HSV (hue, saturation, value) model, which is closer to human color perception than the RGB (red, green, blue) model. The hue channel is subdivided into \( C_h \) bands, the saturation channel into \( C_s \) bands, and the value channel into \( C_v \) bands, resulting in a total of \( C_h \times C_s \times C_v \) color regions.

The feature vector associated with an image indicates, for each cell and each color region, whether there is at least one pixel in the cell that belongs to the color region. These features are boolean, meaning they do not indicate the number of pixels in a given cell that fall within a certain color region, but only whether one or more pixel in the cell falls in the color region. Our experiments show that these boolean features yield more accurate classifiers than color histograms and are more efficient.

Formally, the feature vector \( F(N, C_h, C_s, C_v) \) is a boolean vector of length \( N^2 \times C_h \times C_s \times C_v \). The boolean feature associated with cell \( (x, y) \in [1, \ldots, N] \times [1, \ldots, N] \) and color region \( (h, s, v) \in [1, \ldots, C_h] \times [1, \ldots, C_s] \times [1, \ldots, C_v] \) takes the value 1 (or true) if there is one or more pixel in cell \( (x, y) \) of a color that belongs to color region \( (h, s, v) \). Otherwise, the feature takes the value 0 (false).

We trained SVM classifiers with these color features and measured their accuracy using 5-fold cross-validation. Table 1 shows the results for various values of the parameters \( N, C_h, C_s, C_v \), and for training sets of different sizes. For example, the feature set \( F_3 = F(5, 10, 6, 6) \) consists of 9,000 color features obtained by dividing images into 25 cells (\( N = 5 \)) and the color space into 360 color regions (\( C_h = 10, C_s = 6, C_v = 6 \)). With 4,000 training images, an SVM classifier using the feature set \( F_3 \) is on average 74.6% accurate. With 8,000 training images, the accuracy increases to 75.7%.

Combining color features computed on cells of various sizes further improves the classifier's accuracy. We experimented with the union of the three feature sets \( F_1 = F(1, 10, 10, 10) \), \( F_2 = F(3, 10, 8, 8) \), and \( F_3 = F(5, 10, 6, 6) \). The total number of color features in \( F_1 \cup F_2 \cup F_3 \) is 15,760. The accuracy of a classifier using these features is shown in Table 2. With 4,000 training images, the classifier is 76.3% accurate. With 8,000 training images, it is 77.1% accurate.

#### Color Features and Classifier Accuracy
In our experiments, SVM classifiers trained on boolean color features were more accurate than those trained on color histograms. This finding is counterintuitive because boolean features, which record only the presence or absence of pixels belonging to a certain color region, encode less information than color histograms, which also record the number of such pixels. We propose two hypotheses for why boolean features outperform color histograms:

1. **Scale Independence**: Boolean color features are scale-independent; they record the presence or absence of, say, the green of a cat's eye or the pink of a dog's tongue, regardless of the size of the eye or tongue in the picture.
2. **Regular Distribution**: The distribution of boolean features is more regular (only two values are possible) compared to the distribution of real-valued color histograms, which can cover several orders of magnitude. The regularity of boolean features facilitates the tuning of SVM parameters, notably \( \gamma \), leading to superior accuracy.

#### Predictive Power of Individual Color Features
Individual boolean color features, in isolation, fail to distinguish cats from dogs with any accuracy. Figure 1 shows the 1,000 boolean color features in \( F_1 = F(1, 10, 10, 10) \), plotted according to the fraction of cats (horizontal axis) and the fraction of dogs (vertical axis) for which the feature evaluates to "True". The features are clustered along the diagonal, indicating that the ability of any given color feature to distinguish between cats and dogs is very weak. However, an SVM classifier can harness the aggregate power of the color features to produce more accurate predictions. The success of our classifier does not come from the careful selection of a few colors with high predictive values, but rather from the combination of a large number of weakly predictive features.

### 2.2 Texture Features

Approaches to texture recognition can be broadly divided into two categories: statistical and structural. The statistical approach defines texture via quantitative measurements of intensity in different regions of the image (e.g., by convolution with a Gabor filter). The structural approach defines texture as a set of texture tiles (also called texels) in repeated patterns. We experimented with both approaches and found that structural measurements of texture produced more accurate classifiers.

We start with an informal presentation of our structural approach to texture recognition. We extract small sub-images (5-by-5 pixels) from training images of cats and dogs, which we call texture tiles. Figure 2 shows examples of texture tiles extracted from Asirra images. We collect a set \( T \) of texture tiles of size \( t = |T| \), ensuring that the distance between any two tiles in \( T \) is above a certain threshold. This ensures that the tiles in \( T \) are sufficiently diverse and that there are no duplicate tiles. The feature vector associated with an image is the vector of distances between the image and each texture tile in \( T \). Finally, we train an SVM classifier with these feature vectors.

#### Selection of Texture Tiles
We select random images of cats and dogs from the set of training images and divide each image into vertical and horizontal strips of equal width (5 pixels). This results in a division of each image into \( (250/5)^2 = 2500 \) feature tiles, each a 5-by-5 pixel square. Let \( T_0 \) be this initial set of candidate tiles. We define the distance between two tiles as the average Euclidean distance between the pixels of the tiles in RGB color space. From \( T_0 \), we then compute a subset \( T \) of texture tiles iteratively. Initially, \( T \) is empty. We consider each tile \( T \in T_0 \) in turn. If there already exists a tile in \( T \) whose distance to \( T \) is below a certain threshold \( \delta \), we discard \( T \). Otherwise, we add the tile \( T \) to \( T \). We repeat this process until we obtain a set \( T \) of size \( t \).

#### Feature Vector
The feature vector associated with an image is the vector of distances between the image and each of the \( t \) texture tiles in \( T \). The distance between an image \( A \) and a texture tile \( T \in T \) is defined as follows. For \( 0 \leq i, j \leq (250 - 5) \), let \( A_{i,j} \) denote the 5-by-5 pixel sub-image of \( A \) whose top left corner is the pixel of \( A \) in row \( i \) and column \( j \). We define the distance \( d(A_{i,j}, T) \) between a sub-image \( A_{i,j} \) and a texture tile \( T \) as the maximum of the Euclidean distance between their pixels in RGB space. The distance between \( A \) and \( T \) is defined as \( d(A, T) = \min_{i,j} d(A_{i,j}, T) \). Distances are normalized to the range [0, 1].

#### Results
We trained an SVM classifier with these texture features and measured its accuracy using 5-fold cross-validation. Table 3 shows the results. The feature set \( G_1 \) consists of 1,000 features recording the distance of an image to 1,000 texture tiles, selected such that the distance between any two tiles is at least 40.0. With 4,000 training images, an SVM classifier using the feature set \( G_1 \) is 74.5% accurate. The feature set \( G_2 \) consists of 5,000 features recording the distance of an image to 5,000 similarly selected texture tiles. Using this larger feature set and 4,000 training images, the accuracy of our SVM classifier increases to 78.0%. With 8,000 training images, it is 80.4% accurate.

### 2.3 Combination of Color and Texture Features

The SVM classifiers of sections 2.1 and 2.2 produce for each image a real-valued estimate of whether the image is of a cat or a dog. We mapped the "cat" class to the value 1.0 and the "dog" class to the value -1.0. An image producing a positive output is labeled "cat," and an image producing a negative output is labeled "dog." The outputs of different SVM classifiers can be combined by a weighted average of the estimates they produce. We combined an SVM classifier using the set of color features \( F_1 \cup F_2 \cup F_3 \) (with weight 1/3) and a second SVM classifier using the set of texture features \( G_2 \) (with weight 2/3). Table 4 shows the accuracy of this combination. With a training set of 8,000 images, we obtain a classifier that is 82.7% accurate. The confusion matrix for this combined classifier is in Table 5.

Figure 3 shows the probability distribution of the combined outputs of the color and texture SVM classifiers for the "cat" and "dog" classes. Figure 4 shows the cats and dogs in our sample of 13,000 pets that are most cat-like and most dog-like, according to the combined classifier.

#### Accuracy versus Completeness
We can achieve lower error rates if we allow the classifier to assign some images a "don't know" label. The quality of this 3-class classifier is measured by completeness (the fraction of images classified as either "cat" or "dog") and accuracy (the fraction of images in the "cat" and "dog" classes that are accurately classified). We turn our combined color and texture classifier into a 3-class classifier by parameterizing it with a real-valued parameter \( \epsilon \geq 0 \). Images producing an output smaller than \(-\epsilon\) are labeled "dog," images producing an output between \(-\epsilon\) and \(\epsilon\) are labeled "don't know," and images producing an output larger than \(\epsilon\) are labeled "cat."

Figure 5 shows a plot of the accuracy versus completeness of this classifier for different values of the parameter \(\epsilon\). The base accuracy of the combined color and texture classifier, when classifying all images (completeness of 100%), is 82.7%. If the classifier can ignore half the images (completeness of 50%), its accuracy rises to 94.8%. For 20% completeness, accuracy rises to 98.5%.

### 3. Attacking Asirra

In this section, we describe the application of the machine classifiers of section 2 to attacking Asirra. Recall that an Asirra image is 250-by-250 pixels. We use the combined color and texture classifier to classify the images. The attacker can estimate the probability distribution of the output of the classifier over images of cats and dogs. With this classifier, the attacker can estimate the probability of a given image being a cat or a dog.