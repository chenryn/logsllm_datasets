link) is bad if its available bandwidth is less than a threshold
δ.
3.1 Solution and analysis for RT-Min
Deﬁnition 3. A bad link l is detectable if there is at
least one path pi,j traversing l such that (I) there is no other
bad link in pi,j OR (II) other bad links in pi,j have higher
available bandwidth than link l.
The above condition implies that for every bad link l, there
should be at least one path pi,j such that l is the bottleneck
link of that path.
Deﬁnition 4. A good link l is removable, if there is at
least one good path traversing l.
Lemma 1. Assume that there is no measurement error,
i.e. ˜mi,j = mi,j ,∀pi,j ∈ Π. In that ideal case, the available
bandwidth of every bad link can be determined if (I) all good
links are removable, AND (II) all bad links are detectable.
The proof is straightforward and it is omitted. If both con-
ditions of Lemma-1 hold, we can use the following method
(referred to as Min-Ideal) to estimate the available band-
width of the bottleneck link for every bad path. First,
“mark” all links in good paths as good; the rest of the links
are “unmarked”. Then, select the bad path pb with the high-
est available bandwidth among the remaining paths and as-
sign the available bandwidth of pb to all unmarked links
on pb (pb is then labeled as justiﬁed and all links in pb are
marked). We repeat the previous step until there is no un-
justiﬁed path in Π.
However, the two conditions of Lemma-1 (good links are
removable and bad links are detectable) may not be true
in practice.
In addition, the measurement process is al-
ways error-prone. Therefore, we need to consider an algo-
rithm that can infer bad links, even when the assumptions
of Lemma-1 are not met.
Based on the complexity analysis of the Boolean tomogra-
phy problem [8], it is easy to see that the RT-Min problem is
also NP-hard3. So, we need to approximate solution of RT-
Min with heuristics. Our solution to this problem, referred
3To prove this we have to consider the simpliﬁed case where
the available bandwidth of all bad links is the same. In that
case, the RT-Min problem is the same with the Boolean
tomography problem, which is NP-hard [8].
to as Min-Tomo, is based on the following two principles:
I) As in the case of Min-Ideal, the Min-Tomo algorithm runs
iteratively, and in each iteration it considers the bad path
pb that has the highest available bandwidth among all re-
maining paths. Paths that are α-similar with pb are then
grouped into the set Ω.
II) Since good links are not necessarily removable, it is pos-
sible that we consider a link as bad even though it is good,
and vice versa. To reduce this classiﬁcation error, Min-Tomo
greedily selects the link l that is traversed by the largest
number of bad paths in Ω and marks it as bad. The intu-
ition is that it is unlikely that a large number of bad paths
traverse a good link, without any good path traversing that
link.4 Additionally, this greedy heuristic aims to minimize
the number of links identiﬁed as bad.
The pseudo-code of Min-Tomo is presented in Algorithm 1.
We ﬁrst mark all links on every good path as good (line 3).
Then, we consider the path with the highest available band-
width among all bad paths. Let pb be such a path. Instead
of considering only path pb, we consider the set of paths
that are α-similar with pb (this set is referred to by Ω in
line 7). We then select link lm as the link that is shared
most between paths in Ω. In the case of a tie, we select the
link which is traversed by most unjustiﬁed paths (lines 17
and 18). We also check in line 11 that there is at least one
path that is α-similar with pb and traverses lm. Taking ¯r
as the average available bandwidth of paths going over lm
(line 19), the performance range of lm is determined in line
20 so that every value in the range ˜xlm is α-similar with ¯r.
We label all paths traversing lm as justiﬁed (line 21), if their
available bandwidth falls in the performance range ˜xlm . We
repeat this process until there is no unjustiﬁed path.
The estimated ranges produced by Min-Tomo satisfy the
consistency requirement for the following reason. Let h(lm)
be the highest available bandwidth path traversing lm. Sup-
pose there are several paths that are α-similar with h(lm)
and traverse lm. Let Φ(lm) be the set of these paths. In the
RT-Min problem, the range ˜xlm assigned to a link lm is con-
sistent with the measured performance of path pi,j ∈ Φ(lm)
if the measurement ˜mi,j belongs in the range ˜xlm . It can be
shown that this is true for every path in Φ(lm) if we select
the range ˜xlm as shown in line 20.
The run-time complexity of Min-Tomo is O(|Π|2+|Π||E|).
The running time for a network with about 1000 bad paths
and 4600 links is less than 5.5msec on a workstation with a
2.6GHz Intel i5 processor.
4. SUM METRIC FUNCTION
In this section we consider the following path metric func-
tion,
mi,j = X
∀l∈pi,j
xl
(5)
The corresponding range tomography problem is referred to
as RT-Sum. The Sum metric function can capture perfor-
mance metrics such as delay and jitter. Under the previous
spatial independence assumption, the path loss rate is given
by
mi,j = 1 − Y
∀l∈pi,j
(1 − xl)
(6)
4The analysis of three real topologies, described in Section 5,
conﬁrms this intuition.
388Algorithm 1 Min-Tomo
Require: Set of all links E
Require: Set of all paths Π
Require: The measured available bandwidth of every path
in Π, ˜mi,j,∀pi,j ∈ Π
1: Initialize R = Π {set of unjustiﬁed paths}
2: Initialize U = E {set of unmarked links}
3: Remove from R and U all good paths and all links on
good paths, respectively.
available bandwidth going over l}
4: h(l) = argmax∀pi,j∈R{ ˜mi,j}, ∀l ∈ U {path with highest
5: while R 6= ∅ and U 6= ∅ do
6:
pb = argmax∀pi,j∈R{ ˜mi,j} {path with highest avail-
able bandwidth in R}
Ω = {pi,j|pi,j ∈ R, pi,j k pb} {set of paths which are
α-similar with pb}
num(l) = |{pi,j|l ∈ pi,j, pi,j ∈ R}| {number of unjus-
tiﬁed paths going over l}
for each link l in U do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
C(l) = set of paths in Ω containing l
if h(l) k pb then
else
score(l) = |C(l)| {number of paths in C(l)}
score(l) = 0
18:
19:
end if
end for
Lm = argmaxl∈Uscore(l) {set of links with the max-
imum score}
lm = argmaxl∈Lmnum(l) {link with the maximum
score and maximum number of unjustiﬁed paths going
over it}
¯r = avg{ ˜mi,j|pi,j ∈ Ω, lm ∈ pi,j} {average available
bandwidth of paths in Ω going over lm }
˜xlm = [¯r( 1
1+α ), ¯r(1 + α)] {performance range of link
lm }
21: R = R − {pi,j|lm ∈ pi,j, pi,j ∈ R, ˜mi,j ∈ xlm} {Now,
some paths going over lm are justiﬁed}
22:
U = U − {lm}
23: end while
24: return ˜xl, ∀l ∈ E
20:
However, if the link loss rates are low and there are few
lossy links in each path, the two formulas give similar results
(e.g., in the case of three lossy links each with 1% loss rate,
the actual loss rate is about 2.97% even though the Sum
approximation gives 3%).
In the rest of this section, we
consider loss rate as the performance metric of interest, and
we refer to bad links as “lossy” (i.e., δ=0).
4.1 Solution and analysis for RT-Sum
We ﬁrst present a necessary condition for the identiﬁcation
of lossy links assuming error-free measurements.
Deﬁnition 5. A lossy link l is detectable-1 if there is at
least one path pi,j traversing l such that there is no other
lossy link in pi,j . A lossy link l is detectable-n (n > 1) if
it is either detectable-(n-1) OR there is at least one path
pi,j traversing l such that all other lossy links on pi,j are
detectable-(n-1).
Lemma 2. Assume there is no measurement error, i.e.,
˜mi,j = mi,j,∀pi,j ∈ Π. The loss rate of every lossy link can
be determined if (I) all good links are removable AND (II)
all lossy links are detectable-n (n ≥ 1).
Proof: Under the previous conditions, we can use the
following method (referred to as Sum-Ideal) to estimate the
loss rate of lossy links. Let U be the set of links identiﬁed as
lossy. Based on the recursive deﬁnition of detectable-n links,
there should exist at least one path that traverses only one
link in U . Suppose pi,j is such a path, traversing link l ∈ U .
Hence, the loss rate of l is the same as the loss rate of pi,j
(xl = mi,j). We remove l from U and then subtract the loss
rate of l from the loss rate of every path traversing l.
In
the next iteration, there should again exist at least one path
that traverses only one link of U ; otherwise, the remaining
links in U would not be detectable-n (n ≥ 1). We repeat
this process until the set U is empty. In each iteration, one
link is removed from U , and so this algorithm is guaranteed
to determine the loss rate of every bad link.
However, the two conditions of Lemma-2 may not be true
in practice. We conducted a simple experiment to evaluate
how often those two conditions are violated in three real
topologies (described in Section 5). We randomly select a
number of lossy links and assign a loss rate to them from
a uniform distribution. The percentage of lossy links that
are not detectable-n is quite low in all three topologies (less
than 2% on average when up to 10% of links in the ESNet
topology are lossy). But, the percentage of good links that
are not removable is higher (around 7% when up to 10%
of links in ESNet are lossy). In other words, it is possible
that all paths traversing a good link are bad. Therefore,
we need to consider an algorithm that can infer lossy links,
even when the conditions of Lemma-2 are not met.
The RT-Sum problem is also NP-hard. To prove this, it
is enough to consider the case where all lossy paths have
the same loss rate. The solution in that case is the same
with the solution of the Boolean tomography problem. We
approximate the solution of the RT-Sum problem with a
heuristic, referred to as Sum-Tomo, which is based on the
following three principles:
I) In each iteration, we only consider the set of paths that
are α-similar with the path that has the lowest loss rate.
This set is referred to as Ω.
II) Similar to Min-Tomo, we greedily choose the link l that
is traversed by the maximum number of lossy paths in Ω.
III) We subtract the assigned loss rate of a detected bad link
from all paths traversing that link.
The pseudo-code of Sum-Tomo is presented in Algorithm 2.
The input arguments are the same with Min-Tomo in Algo-
rithm 1 and they are omitted. After removing good paths
and their links (in line 3), we consider the path pb with the
lowest loss rate; let Ω be the set of paths that are α-similar
with pb (line 6 and 7). In line 14, we greedily choose link
lm as the link that is traversed by most paths in Ω (if there
are multiple such links, the link that is traversed by most
unjustiﬁed paths is chosen). We calculate ¯r as the average
loss rate of paths in Ω that traverse lm (line 15), and so the
performance range of lm (line 16) is such that every point
in ˜xlm is α-similar with ¯r. We then mark all paths (travers-
ing lm) as justiﬁed (line 17), if their loss rate is within the
performance range of lm.
In that case, we also subtract
the average loss rate ¯r from the loss rate of all paths that
traverse lm (line 18).
The run-time complexity of Sum-Tomo is also O(|Π|2 +
|Π||E|). The Sum-Tomo algorithm can only approximate
389Algorithm 2 Sum-Tomo
1: Initialize R = Π {set of unjustiﬁed paths}
2: Initialize U = E {set of unmarked links}
3: Remove from R and U all good paths and all links on
good paths, respectively.
5. EVALUATION
This section presents an evaluation study of diﬀerent to-
mography algorithms using simulations on real topologies.
For brevity, we only consider the RT-Sum problem and the
loss rate metric.
which has not been justiﬁed yet}
4: Initialize ri,j = ˜mi,j,∀pi,j ∈ R {the loss rate of pi,j
5: while R 6= ∅ and U 6= ∅ do
6:
pb = argmin∀pi,j ∈R{ri,j} {path with lowest loss rate
in R}
Ω = {pi,j|pi,j ∈ R, pi,j k pb} {set of paths which are
α-similar with pb, using ri,j instead of mi,j}
num(l) = |{pi,j|l ∈ pi,j, pi,j ∈ R}| {number of lossy
paths going over l}
for each link l in U do
7:
8:
C(l) = set of paths in Ω containing l
score(l) = |C(l)| {number of paths in C(l)}
9:
10:
11:
12:
13:
15:
14:
end for
Lm = argmaxl∈Uscore(l) {set of links with the max-
imum score}
lm = argmaxl∈Lmnum(l) {link with the maximum
score and maximum number of lossy paths going over
it}
¯r = avg{ri,j|lm ∈ pi,j , pi,j ∈ Ω} {average loss rate of
paths in Ω going over lm }
˜xlm = [¯r( 1
1+α ), ¯r(1 + α)] {performance range of link
lm }
17: R = R−{pi,j|pi,j ∈ R, ri,j ∈ ˜xlm} {Some paths going
over lm are now justiﬁed}
18:
ri,j = max{0, ri,j − ¯r}, ∀pi,j ∈ R, lm ∈ pi,j {Update
loss rate of paths going over lm}
U = U − {lm}
16:
19:
20: end while
21: return ˜xl, ∀l ∈ E
the optimal solution to the RT-Sum problem for two reasons.
First, as in the case of Min-Tomo, it may not return the
minimum number of bad links. Second, as opposed to Min-
Tomo, Sum-Tomo may violate the consistency constraint.
To understand how this can happen consider the following
example. Assume we have three links l1, l2 and l3, and three
lossy paths p1 =, p2 = and p3 =.
The measured path loss rates are 3%, 4% and 2%, respec-
tively. Suppose that α = 0.1. The Sum-Tomo algorithm
would only detect l1 and l2 as lossy links, with the same loss
rate range [1.8%-2.2%] for both links. Note that these ranges
are not consistent with the measurement for path p1. A con-
sistent solution in this example would be that all three links
are lossy and their loss rate ranges are xl1 = [2.7% − 3.3%],
xl2 = [0.9%−1.1%] and xl3 = [0.9%−1.1%]. Notice however
that this solution results in more lossy links that the solu-
tion provided by Sum-Tomo. In other words, there can be
a trade-oﬀ between minimizing the number of inferred lossy
links and satisfying the consistency requirement.
In fact,
in the RT-Sum problem, we can show that ﬁnding a solu-
tion that satisﬁes the consistency requirement is NP-hard5
(assuming that the performance ranges cannot contain zero
loss rate).
5.1 Simulation model
We have developed a packet-level event-driven simulator
to evaluate tomography algorithms under various network
conditions. In each simulation run, we ﬁrst set the number
of lossy links to c. We vary c to examine how diﬀerent to-
mography algorithms perform as the likelihood of lossy links
increases. We then select the lossy links in the underlying
network and assign an average loss rate to each of them. In
the following results, the link loss rates are drawn from a
Lognormal distribution with mean 0.04 and standard devia-
tion 0.16 (these parameters were estimated from a large set
of loss rate measurements in about 3600 Planetlab paths).
The loss rate of the remaining links is set to zero.
We consider two loss processes:
(I) A Bernoulli random process where each arriving packet
at a link is dropped with a given probability (equal to that
link’s loss rate).
(II) A Gilbert random process where the link’s state varies
between good and congested. In the good state, the link does
not drop packets. In the congested state, the link drops ar-
riving packets with a certain probability. The duration of the
good and congested states follows an exponential distribu-
tion (with average duration Tgood and Tcong, respectively).
The loss probability in the congested state is calculated so
that the long-term loss rate, across both good and congested
time periods, is equal to the assigned average loss rate for
that link.
The Gilbert process can better capture the bursty con-
gestion events that are commonly observed in practice, es-
pecially when Tcong is much shorter than Tgood. In our sim-
ulations, we have examined two cases for Tgood and Tcong:
1) they are both set to 10sec and, 2) Tgood is set to 100sec
and Tcong = 10sec. The results with the ﬁrst pair of values
are similar to the Bernoulli case, and so we only report the
results with the second pair of values.
To measure path loss rates, we simulate sending 4000
probing packets in each path at a rate of ten packets per
second (i.e., the measurement duration is 400 seconds). If
the measured path loss rate is less than 0.1%, the path is
good (i.e. δ = 0.1%). The actual loss rate of a lossy link l
is the ratio of probing packets (across all paths traversing l)
that have been dropped at l.
We consider three IP-layer network topologies. Two of
them (Internet2 and ESNet) have been provided to us by the
corresponding operators. The third was obtained running
Paris-traceroute [22] between 100 PlanetLab hosts. In more
detail, the three topologies are: