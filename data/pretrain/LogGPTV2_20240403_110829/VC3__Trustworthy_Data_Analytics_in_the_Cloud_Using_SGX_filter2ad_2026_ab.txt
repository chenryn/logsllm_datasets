for key establishment.
A public-key pair pk, sk is generated using an algorithm
PKGen(). We write PKEncpk{text} for the encryption of
text under pk. In every session, the user is identiﬁed and
authenticated by a public-key pku. We assume the public-
key encryption scheme to be at least IND-CPA [10]: without
the decryption key, and given the ciphertexts for any chosen
plaintexts, it is computationally hard to extract any information
from those ciphertexts. Our implementation uses an IND-
CCA2 [10] RSA encryption scheme.
We write ESigP [C]{text} for a quote from a QE with
identity P that jointly signs H(text) and the EDigest(C)
on behalf of an enclave with code identity C. We assume
that this quoting scheme is unforgeable under chosen message
attacks (UF-CMA). This assumption follows from collision-
resistance for H and EDigest and UF-CMA for the EPID
group signature scheme [15]. Furthermore, we assume that
Intel’s quoting protocol implemented by QEs is secure [3]:
only an enclave with code identity C may request a quote of
the form ESigP [C]{text}.
Authenticated Encryption
For bulk encryption, we rely on a scheme that provides
authenticated encryption with associated data (AEAD). We
write Enck(text, ad) for the encryption of text with associated
data ad, and Deck(cipher, ad) for the decryption of cipher
with associated data ad. The associated data is authenticated,
but not included in the ciphertext. When this data is commu-
nicated with the ciphertext, we use an abbreviation, writing
Enck[ad]{text} for ad | Enck(text, ad). (Conversely, any IV
or authentication tag used to implement AEAD is implicitly
included in the ciphertext.) We assume that our scheme is
both IND-CPA [11] (explained above) and INT-CTXT [11]:
without
the secret key, and given the ciphertexts for any
chosen plaintexts and associated data, it is hard to forge any
other pair of ciphertext and associated data accepted by Deck.
Our implementation uses AES-GCM [40], a high-performance
AEAD scheme.
III. ADVERSARY MODEL
We consider a powerful adversary who may control the entire
software stack in a cloud provider’s infrastructure, including
hypervisor and OS. The adversary may also record, replay,
and modify network packets. The adversary may also read or
modify data after it left the processor using probing, DMA, or
similar techniques. Our adversary may in particular access any
number of other jobs running on the cloud, thereby accounting
for coalitions of users and data center nodes. This captures
typical attacks on cloud data centers, e. g., an administrator
logging into a machine trying to read user data, or an attacker
exploiting a vulnerability in the kernel and trying to access
user data in memory, in the network, or on disk.
We assume that the adversary is unable to physically open
and manipulate at least those SGX-enabled processor pack-
ages that reside in the cloud provider’s data centers. Denial-
4040
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:24 UTC from IEEE Xplore.  Restrictions apply. 
Stack
e
v
a
l
c
n
E
Public Code E+
Private Code E-
Heap
Shared Memory
Framework F
...
Private Code E-
Public Code E+
readKV()/writeKV()
Framework F
CPU
M
Enclave 
Code
RAM
R
R
Input
M
M
Operating System
Fig. 3: High-level concept of a VC3 enhanced MapReduce job: code and data
are always kept encrypted when outside the processor chip.
Process Memory Layout
Dependencies
Fig. 2: Left: Memory layout of process containing SGX enclave and
framework code. Right: Dependencies between the involved components.
of-service, network trafﬁc-analysis, side-channels, and fault
injections are also outside the scope of this paper.
We consider the user’s implementation of the map and
reduce functions to be benign but not necessarily perfect: the
user’s code will never intentionally try to leak secrets from
enclaves or compromise the security of VC3 in any other way,
but it may contain unintended low-level defects.
IV. DESIGN OVERVIEW
Our goal is to maintain the conﬁdentiality and integrity of
distributed computations running on a network of hosts poten-
tially under the control of an adversary. This section outlines
our design to achieve this with good performance and keeping
large software components out of the TCB.
In VC3, users implement MapReduce jobs in the usual way:
they write, test, and debug map and reduce functions using
normal C++ development tools. Users may also statically link
libraries for particular data analytics domains (e. g., machine
learning) with their code; these libraries should contain pure
data-processing functions that do not depend on the operating
system (we provide mathematical and string libraries in our
prototype).
When their map and reduce functions are ready for produc-
tion, users compile and encrypt them, obtaining the private
enclave code E−. Then they bind the encrypted code together
with a small amount of generic public code E+ that imple-
ments our key exchange and job execution protocols (see §V).
Users then upload a binary containing the code to the cloud;
they also upload ﬁles containing encrypted data. In the cloud,
enclaves containing E− and E+ are initialized and launched
by public and untrusted framework code F on worker nodes.
Figure 2 depicts the memory layout of a process containing
the described components; it also shows their dependencies.
In VC3, a MapReduce job starts with a key exchange between
the user and the public code E+ running in the secure enclave
on each node. After a successful key exchange, E+ is ready
to decrypt the private code E− and to process encrypted data
following the distributed job execution protocol.
To keep the operating system out of VC3’s TCB, we kept
the interface between E+ and the outside world narrow.
Conceptually,
it has only two functions: readKV() and
writeKV(), for reading and writing key-value pairs from
and to Hadoop (akin to receiving and sending messages). Since
F and enclave share the virtual address space of a process,
data is passed from E+ inside the enclave to F outside the
enclave over a shared memory region outside the enclave.
Other than relying on this narrow interface, the code in the
enclave is self-sufﬁcient: it has no further dependencies on
the operating system. The enclave has its own stack which we
reserve on start-up (it includes a guard page to detect stack
out-of-memory conditions); it has its own heap, carved out
of the enclave memory region; and we guarantee that only
one thread at a time executes the user code (the enclave is
created with a single thread control structure, to ensure that
all execution of enclave code is single threaded); parallelism
is achieved in MapReduce jobs by running many instances of
the map and reduce functions in parallel in separate enclaves.
With this design, the operating system and the hypervisor can
still mount attacks such as not scheduling processes, dropping
or duplicating network packets, not performing disk I/O, and
corrupting data when it is out of the enclaves. While we cannot
guarantee progress if the operating system mounts denial of
service attacks, our job execution protocol guarantees that the
results are correct and complete if the distributed computation
terminates successfully.
Note that while in the cloud, both E− and the user’s data
are always kept encrypted, except when physically inside the
trusted processor chip on a mapper or reducer node, as shown
in Figure 3. Inside the processor chip, the user’s map and
reduce functions run on plaintext data at native speed. At
the same time, we allow Hadoop to manage the execution of
VC3 jobs. The framework code (F ) implements the Hadoop
streaming interface [5] to bind to unmodiﬁed Hadoop de-
ployments; VC3’s map and reduce nodes look like regular
worker nodes to Hadoop. Thus, Hadoop can use its normal
scheduling and fault-tolerance mechanisms to manage all data-
ﬂows, including performance mechanisms for load balancing
and straggler mitigation. But
the
operating system, and the hypervisor are kept out of the TCB.
the Hadoop framework,
4141
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:24 UTC from IEEE Xplore.  Restrictions apply. 
Some of these properties could also be achieved using
trusted hypervisors [17], [28], [36], [38], [58], but trusted
hypervisors are problematic in a cloud environment. They
force a potentially large privileged software component that is
under the control of the (possibly adversarial) cloud provider
into the TCB of every user. While users can use attestation to
authenticate a digest of such software, it is unclear how they
can establish trust in it, especially in light of periodic software
upgrades. While the software TCB of a VC3 application
may have as many or more lines of code as small special-
purpose hypervisors [38], [58], the former code is entirely
chosen and compiled by the user, whereas the latter are not.
It is also unclear how these special-purpose hypervisors could
be extended to coexist with the more functional hypervisors
used in cloud systems [8]. Finally, note that VC3’s hardware
TCB is only the SGX processor package; this is smaller than
traditional systems based on TXT [31] or a TPM (large parts
of the motherboard); this is important in a cloud setting where
the hardware is under the control of the cloud provider.
The ﬁnal aspect of the VC3 design is that users may enforce
region self-integrity invariants using our compiler. The region
integrity invariants act as an additional layer of protection that
allows the trusted code inside the enclave to continuously
monitor its internal state to prevent memory corruption and
disclosure of information due to low-level defects. Users who
want the additional security assurances may use our compiler,
but we emphasize that this optional; users may use other
mechanisms, including manual inspection, testing, and formal
veriﬁcation, to check that their code does not have defects.
V. JOB DEPLOYMENT
After preparing their code, users deploy it in the cloud. The
code is then loaded into enclaves in worker nodes, and it runs
our key exchange protocol to get cryptographic keys to decrypt
the map and reduce functions. After this, the worker nodes run
our job execution and veriﬁcation protocol. This section and
the next present our cryptographic protocols for the exchange
of keys and the actual MapReduce job execution, respectively.
Before describing these protocols in detail, we ﬁrst discuss the
concept of cloud attestation used in VC3.
A. Cloud Attestation
it does not guarantee that
As described above, in SGX, remote attestation for enclaves
is achieved via quotes issued by QEs. The default SGX QE
only certiﬁes that the code is running on some genuine SGX
processor, but
the processor is
actually located in the cloud provider’s data centers. This may
be exploited via a type of cuckoo attack [47]: an attacker could,
for example, buy any SGX processor and conduct a long term
physical attack on it to extract the processor’s master secret.
If no countermeasures were taken, she would then be in a
position to impersonate any processor in the provider’s data
centers. Note that our threat model excludes physical attacks
only on the processors inside the data centers.
To defend against such attacks, we use an additional Cloud
QE, created by the cloud provider whenever a new SGX-
enabled system is provisioned. The purpose of the Cloud QE
is to complement quotes by the SGX QE with quotes stating
that the enclave runs on hardware owned and certiﬁed by the
cloud provider, in a certain data center. At the same time, to
defend against possibly corrupted cloud providers, we only use
the Cloud QE in conjunction with the SGX QE. (Note that the
cloud provider cannot fake quotes from the SGX QE, since
our threat model excludes physical attacks on the processors
inside the data centers.) The procedure to provision Cloud
QEs is simple. Before a new machine enters operation in a
data center, a Cloud QE is created in it. This Cloud QE then
generates a public/private key pair, outputs the public key and
seals the private key which never leaves the Cloud QE.
two ﬁxed
the
assume
for
following, we
for SGX and
In
the
signing
identities
cloud, we write
ESigSGX [C]{text} and ESigCloud[C]{text} for quotes
by the main SGX QE and the Cloud QE, respectively,
and write ESigSGX,Cloud[C]{text} for their concatenation
ESigSGX [C]{text} | ESigCloud[C]{text}.
We foresee that cloud providers will create groups of proces-
sors based on geographical, jurisdictional, or other boundaries
that are of interest to the user, and will publish the appropriate
public keys to access these groups of processors.
B. Key Exchange
To execute the MapReduce job, enclaves ﬁrst need to get keys
to decrypt the code and the data, and to encrypt the results.
In this section we describe our protocol for this. Our key
exchange protocol is carefully designed such that it can be
implemented using a conventional MapReduce job that works
well with existing Hadoop installations. We ﬁrst describe
the protocol using generic messages, and then show how to
integrate it with Hadoop. We present a multi-user variant in
Appendix A and a lightweight variant in Appendix B.
Recall that the user is identiﬁed and authenticated by her key
pku for public-key encryption and each SGX processor runs
a pair of SGX and Cloud QEs. Before running the protocol
itself, the user negotiates with the cloud provider an allocation
of worker nodes for running a series of jobs.
Setting up a new job involves three messages between the
user and each node:
1) The user chooses a fresh job identiﬁer j and generates a
fresh symmetric key kcode to encrypt E−, then sends to
any node involved the code for its job enclave (Cj,u):
Cj,u = E+ | Enckcode
[]{E−} | j | pku.
2) Each node w starts an enclave with code identity Cj,u.
1 and
Within the enclave E+ derives a symmetric key kw
encrypts it under the user’s public key:
{kw}.
mw = PKEncpku
1This can be the enclave’s sealing key or a key generated using the random
output of the x86-64 instruction RDRAND.
4242
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:24 UTC from IEEE Xplore.  Restrictions apply. 
The enclave then requests quotes from the SGX and
Cloud QEs with text mw, thereby linking mw to its code
identity Cj,u (and thus also to the job-speciﬁc j and pku).
The message mw and the quotes are sent back to the user:
pw = mw | ESigSGX,Cloud[Cj,u]{mw}.