for differential privacy, it assumes that the tuples in the database
are independent. However, the authors have shown how to relax
this assumption and presented results for databases that can be
partitioned in disjoint sets of dependent tuples.
Fingerprinting and anti-collusion codes. Wagner [29] was
one of the first to propose to use fingerprints to personalize copies
of digital content. A fingerprint is a unique string of symbols linked
to an individual. It must be concealed in the digital content with a
watermarking method ensuring that it cannot be easily remove.
The problem is harder to solve when a group of individuals can
collude to forge a potential untraceable counterfeit based on their
personalized copies. These individuals can always compare their
copies (e.g., perform a bit by bit comparison between their copies).
Hence, the following assumption is generally made in the context
of fingerprinting codes:
Assumption 1. Marking assumption [4]. A set of colluders can
only identify the bits of their fingerprints (i.e., marks) that do not all
coincide at a given position.
ϵ ) log( 1
The challenge is to develop short fingerprinting or anti-collusion
codes that resist to a limited number of colluders. The first codes
resisting to any number of colluders c > 1 were proposed by Boneh
and Shaw [4]. Their length are equal to 32c4 log( n
ϵ ) bits, in
which n is the maximum number of buyers and ϵ is the probability
of errors (i.e., accusing an innocent buyer or being unable to identify
a malicious one). Based on the same marking assumption, Peikert,
Sheilat and Smith [17] proved that the length of anti-collusion
binary codewords must be in Ω(c2 log( 1
cϵ )), for any scheme.
Unfortunately, the first fingerprinting codes [4, 28] were useless
in practice even for relatively small values of c since their code
lengths were large. In 2003, Tardos [27] proposed a probabilistic
scheme whose codeword length matched almost the best lower
bounds. In a nutshell, to resist to collusions composed of c collud-
ers with a probability of error of ϵ, binary codes with length of
100c2 ln( 1
ϵ ) bits are sufficient. Tardos’ construction is very simple,
since all codewords are constructed independently and their bits
are selected randomly according to some predefined probability
functions.
Škorić, Katzenbeisser and Celik [23] reduced the length of Tardos
codes to 2π 2c2 ln( 1
ϵ ) by relying on all positions of the codeword
during the accusation process. In parallel, efficient solutions have
been presented for small collusions (i.e., c = 2 or 3). They usually
rely on error-correcting codes (e.g., dual Hamming codes [7]).
Database watermarking. Watermarking has been developed
to securely embed fingerprints in digital content. They may be
used for different purposes. The owner of the content may want
to prove his ownership and embed his own identity in the content.
Alternatively, the owner may one to prove that he has prepared a
personalized copy for a given buyer, which is the problem consid-
ered in this paper.
Agrawal and Kiernan [1] suggested to embed proofs of owner-
ship in the least significant bits of the numerical attributes. Unfor-
tunately, their approach does not consider the impact of altering
attributes on the utility of the resulting databases. To address this
problem, Sion, Atallah and Prabhakar [22] gave an algorithm to
find the best attributes to embed a fingerprint while still preserving
the utility of the resulting databases. Their method chooses the
attributes depending on the fingerprint bits themselves. Thus, this
approach is suitable for a proof of ownership but not for proofs of
possession since they could be easily remove due to this asymmetry.
Finally, Lafaye, Gross-Amblard, Constantin and Guerrouani [13]
addressed the shortcomings of the two previous solutions by propos-
ing an approach in which the marking attributes are computed once
for all. It is based on global and local constraints ensuring that the
impact on the utility of the resulting database is not significant.
Session 6: Privacy 1ASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea209Combination of sanitization and watermarking. Interac-
tions between the sanitization and the personalization processes
for databases have not been extensively studied. To the best of our
knowledge, only three solutions have been proposed. Unfortunately,
these solutions are all based on the k-anonymity model, which thus
brings all the drawbacks of the model in these solutions. In addition,
the three solutions have different objectives.
In the first solution, Bertino, Ooi, Yang and Deng [2] aimed to
embed a proof of ownership into a sanitized database. In the two
others [12, 20], the authors directly address the same problem as
in this paper, i.e., sanitizing and personalizing views of a relational
database. One of the shortcomings of these approaches is that the
anonymity model that has been adopted is not strong enough to
ensure a good privacy protection. Furthermore, they have neglected
the utility of the resulting sanitized databases. Finally, the authors
use the sanitization process to generate personalized views. Such
an approach can be quite prohibitive for large databases.
3 DEFINITIONS AND REQUIREMENTS
In this section, we give the main definitions and the high-level
description of the algorithms composing our solution and its re-
quirements in terms of privacy, utility and security.
3.1 Sanitization and personalization
The approach followed in this paper closely entwines the sanitiza-
tion and the personalization processes.
Definition 1. Database. A relational database DB is a set of
j-tuples {t1, . . . , t|DB|}. Each of these tuples is a set of j ordered
attributes, which means that a tuple ti is given by [Ai,1, . . . , Ai, j].
Each database is defined with respect to an underlying domain,
which characterizes the attributes of the tuples.
Definition 2. Domain. The underlying data domain D of a
database DB is the cross product of the attribute domains (i.e., D =
DA1 × · · · × DAj ). Once defined, DB can be seen as a subset of D.
The database represents the proprietary content to be protected.
In contrast, the underlying domain can usually be shared publicly
as it may simply represent the potential demographic attributes of
the entire population or a large subset of this population.
As mentioned in the introduction, there are two generic settings
to protect the privacy of a database. In the interactive setting, the
database owner restricts the access to the database through a finite
number of queries. The answers to these queries should be ran-
domized before being outputted. In contrast, in the non-interactive
one, the database owner has to sanitize the database before its
distribution. This sanitization process can be formalized as follows:
Definition 3. Database sanitization. Let DB be a database in
the domain D. The sanitization process S(DB, D) is a randomized
algorithm that produces an altered view V in a potentially altered
domain D′ in order to protect the personal information in DB.
The sanitization process aims at reaching an equilibrium be-
tween the privacy of the personal information and the utility of the
resulting view. Both concepts are defined later.
Once a sanitized view V of the database DB has been produced,
it can be personalized for any potential buyer upon request. Our
novel personalization process can now be described. It relies on (1)
binary fingerprinting codewords uniquely identifying individuals
and (2) a watermarking solution coherent with the selected database
representation. As mentioned in the previous section, numerous
binary fingerprinting schemes exist [4, 7, 27] but this process should
be independent to the code selected.
Definition 4. Binary fingerprints. Let W be a set of n binary
codewords generated by the selected fingerprinting scheme. Thus, any
buyer bi is assigned to a unique l-bit codewords from W.
Once the set of fingerprinting codewords has been generated,
they have to be concealed securely in the view of the database. The
chosen watermarking technique encodes a binary codeword by
the presence or the absence of selected tuples of the domain. This
choice is closely related to the (α,β)-sanitization algorithm adopted
in this paper. Hence, to personalize the view, a limited number
of false tuples will be added to this view. These tuples should be
indistinguishable from real tuples.
Definition 5. Watermarking mechanism. Let V be a view of
a database DB in the domain D to be personalized and let the
Pos = {t1, t2,· · · , tl} ⊂ D be a set of l randomly selected tuples in
D \ DB. Thus, the tuple ti is inserted in the view if and only if the
ith bit of the codeword to be inserted in the sanitized view is 1.
Obviously, marking positions should be kept secret as any indi-
vidual receiving a personalized copy of a database should not be
able to recover the fingerprints. Some restrictions on these positions
may have to be defined with respect to the sanitization process and
type of data considered.
Two different types of participants are defined hereafter.
• Merchant. Assume that a merchant M has a database DB
containing personal information. This merchant may want to
distribute/sell his database to third parties (e.g., for statistical
analysis). To protect the personal information and prevent its
illegal redistribution, he wants to sanitize (Definition 3) and
personalize (Definitions 4 and 5) it before its distribution.
• Buyers. A set of buyers B = {b1, . . . , bn}, each aim at obtain-
ing their view V of the database DB from the merchant.
Each view is sanitized and personalized for a given buyer.
Remark that the distributed views of the database are generated
by the merchant without the help of the buyers who would eventu-
ally acquire these views. Thus, a malicious merchant would be able
to forge a fake view accusing an innocent buyer. If we assume that
the merchant does not have any real advantage to do so, we may
consider henceforth that the merchant is trustworthy.
Another way of relaxing this assumption is to ask the buyer
to sign some information (e.g., a hash of the distributed view plus
some additional information) that can later be used by the merchant
to prove to a judge that a particular buyer has indeed acquire a
particular view. Of course, this approach would not be able to
prevent a malicious merchant that publicly leaks on purpose a view
associated to a particular buyer to accuse him wrongfully.
3.2 Main algorithms
Any solution to our problem should be composed of the following
algorithms: DBSetup, DBBuy, and DBAccuse. They are described
Session 6: Privacy 1ASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea210here in terms of their high-level functionalities before detailing
their instantiations in the next section.
DBSetup(DB, privpar, utilpar, secpar) is run by the merchant on
the database DB and the public parameters for the privacy,
utility and security requirements (e.g., the number of poten-
tial buyers n, an upper bound on the number of colluders c,
an upper bound on the a priori information of the adversary).
It outputs the secret information required by the other algo-
rithms: the sanitized view V, the codebook W composed
of n distinct fingerprints and the marking positions Pos.
DBBuy(i,V,W, Pos) is run by the merchant to generate a person-
alized view Vi with the fingerprint ωi ∈ W for a buyer
bi. In addition, the merchant stores this information in Rec.
Thus, Rec = Rec ∪ {(bi , ωi )}.
DBAccuse(V, Rec,V∗) takes V∗, which is an illegally released
view, and outputs the identity of a potential malicious buyer
bi∗ who may have forged it with probability 1 − ϵ.
Remark 1 (memory cost of the proposed approach). The merchant
simply has to kept only one copy of the sanitized view V and
the list of the generated fingerprints as well as the corresponding
identities of the buyers. This is relatively efficient even for very
large databases and a high number of buyers. Furthermore, in some
cases (e.g., using Tardos codes), the fingerprints can be generated on
demand as long as their parameters have been defined beforehand.
3.3 Privacy, utility and security requirements
In this section, we introduce the formal requirements to ensure the
privacy of the personal information in a sanitized view of a database
and its utility. These two concepts are actually closely related. These
definitions follow the work of Rastogi, Hong and Suciu [18, 19].
The security requirements to ensure the traceability of illegitimate
copies distributed by malicious buyers are also presented.
Privacy and utility. The privacy level of a sanitized database
depends on the a priori knowledge of an adversary on the tuples of
that database. This concept can be defined in the following manner.
Definition 6. Prior knowledge [18]. Let the probability function
F : 2D → [0,· · · , 1] be such that(cid:80)DB⊂D F (DB) = 1. The a priori
Prprio[t ∈ DB] =(cid:80)
knowledge of an adversary can be defined by the a priori probability
function Prprio : D → [0,· · · , 1] such that for any tuple t ∈ D,
t ∈DB∧DB⊂D F (DB).
In practice, the merchant does not know usually the a priori
knowledge of the adversary. The next step is to restrict the a priori
information of any adversary.
Assumption 2. Tuple independent adversary [18]. For a param-
eter d ∈ (0,· · · , 1), a d-tuple-independent adversary is such that, for
any t ∈ D, his a priori probability
• Prprio[t ∈ DB] is either smaller than d or equal to 1,
• Prprio[t ∈ DB] is independent of the tuple t.
This assumption can be relaxed. However, it is a sensitive issue
for most of the database-privacy models as it may allow inference
attacks in case of a strong dependence between tuples [15].
The privacy of a sanitized database is based also on the a posteri-
ori probability of an adversary once he has accessed to the sanitized
view. This can be defined as follows:
Definition 7. Posterior knowledge [18]. Let the probability
function Pr : 2D → [0,· · · , 1] be such that Pr[V|DB] is the
probability to obtain the view from the database. This function is
given by the sanitization process. The a posteriori knowledge of
an adversary can be defined by the a posteriori probability func-
: D → [0,· · · , 1] such that for any tuple t ∈ D,
tion Prpost
Prpost [t ∈ DB|V] =(cid:80)
t ∈DB∧DB⊂D Pr[V|DB] · F (DB).
The privacy of a sanitized database can now be formally stated
with respect of the knowledge of the adversaries.
Definition 8. Privacy [18]. A probabilistic sanitization algorithm
A gives (d, γ )-private views if, for any d-tuple-independent adversary,
any view V of the database DB, and any potential tuple t such that
Prprio[t ∈ DB] < d, the following inequalities hold
Prprio[t ∈ DB] ≤ Prpost [t ∈ DB|V] ≤ γ .
d
γ
As the sanitized view of a database should remain useful for a
potential buyer, the sanitization algorithm cannot perturb too many
tuples or aggregate a high number of attributes. To formalize this
concept, the typical analysis of the buyers have to be considered.
For instance, in the context of a statistical database, the class of
possible queries C can be such that:
• it contains all the possible queries on DB;
• it contains only queries on tuples in DB fulfilling a given
predicate and providing correctness guarantees (e.g., the
average of an attribute field is preserved by the sanitization);
• it contains only counting queries on the number tuples in
DB fulfilling a given predicate.
Obviously, unrestricted class of queries may leak too much pri-
vate information on some given tuples. Moreover, the second al-
ternative depends heavily on the expectations of buyers and the
capabilities of the merchant. To avoid these problems, Rastogi, Hong
and Suciu [18] considered only counting queries, which actually
are generic enough to answer most of the traditional analysis tasks.
Definition 9. Utility [18]. A randomized sanitization algorithm
S is called (ρ, ϵ)-useful if for any query Q in the counting query class
C on any instance DB, there is an estimator ˜Q be such that
(cid:102)
(cid:103)
(cid:112)|DB|
≤ ϵ
|Q (DB) − ˜Q (V )| ≥ ρ
Pr
In this context, the objective of an adversary A is to reconstruct
the original database DB from his view V. A result of Dinur and