### 3.2. Restart Groups, MTTF, and MTTR

In the context of the restart tree, subtrees are referred to as restart groups, similar to process groups in UNIX. This grouping indicates that the nodes within a group share commonality regarding their restart behavior. For example, in Figure 2, there are five restart groups: three trivial ones (RA, RB, RC) and two non-trivial ones (RBC, which includes B and C, and RABC, which encompasses RA, RBC, RB, and RC). When a restart group is triggered, all components attached to the leaves of the respective subtree are restarted. For instance, restarting the group rooted at RBC would restart components B and C. The entire system is always considered a restart group.

To align with industry standards, we will use Mean Time To Failure (MTTF) and Mean Time To Repair (MTTR) for individual components, restart groups, and the system as a whole. The time-to-recover for a component includes the time from failure detection to recovery, even though downtime starts at the moment of failure, not detection.

It is important to note that MTTF and MTTR provide a simplified representation of a system's or subsystem's failure behavior. Without detailed knowledge of the distribution of failure and recovery times, it is challenging to predict the probability of a subsystem failing during a specific time interval, which can be crucial since not all downtime is equally costly. The techniques described in this paper for constructing and evolving restartability trees assume that MTTF and MTTR represent the means of distributions with small coefficients of variation. Our experiments confirm this assumption for our system. We will use MTTFS and MTTRS to denote the MTTF and MTTR, respectively, of subsystem S. Specifically, the MTTF for a restart group G containing components \( c_0, c_1, \ldots, c_n \) is given by:
\[ \text{MTTF}_G = \min(\text{MTTF}_{c_i}) \]
and the corresponding MTTR is:
\[ \text{MTTR}_G = \max(\text{MTTR}_{c_i}) \]

### 3.3. The Recoverer and the Oracle

The restart tree is central to maintaining a recursively restartable system, working in conjunction with a recoverer, which performs the actual restarts. The recoverer does not decide which component needs to be restarted; this decision is made by the oracle, which represents the restart policy. Based on the information about which component has failed, the oracle instructs the recoverer on which node in the tree to restart.

If the restart resolves the issue, the system continues normal operation. If the failure persists or another failure occurs after the restart, the oracle moves up the tree and requests the restart of the node’s parent. This process can be repeated until the entire system is restarted. In our ground station, the recoverer and the oracle are collocated in the REC component, as shown in Figure 1.

A failure is said to be \(\alpha\)-curable if it is cured by a restart at node \(\alpha\) or any of its ancestors in the restart tree. A minimally \(\alpha\)-curable failure is one that is \(\alpha\)-curable, and \(\alpha\) is the lowest node in the tree for which a restart will cure the failure. Assuming that mean-time-to-repair is non-decreasing as we move up the tree, a minimal cure implies the failure is resolved with minimal downtime. For a given failure, \(\alpha\) may not be unique (e.g., if restarting the parent of \(\alpha\) is no more expensive than restarting \(\alpha\) itself). A perfect oracle should embody the minimal restart policy, i.e., for every minimally \(\alpha\)-curable failure, it recommends a restart of node \(\alpha\). Section 4.4 illustrates the consequences of an imperfect oracle.

### 4. Evolving the Restart Tree

Having introduced the concept of a restart tree, we show a simple restart tree for Mercury (tree I) on the left side of Figure 3, consisting of a single restart group. The only possible policy with this tree is to reboot all of Mercury when something goes wrong. The system's MTTF is at least as bad as the lowest MTTF of any component, and its MTTR is at least as bad as the highest MTTR of any component. Table 1 shows rough estimates of component failure rates, provided by the administrators who have operated the ground station for the past two years. Components interacting with hardware are particularly prone to failure due to their inability to handle a wide variety of corner cases.

| Component | MTTF |
|-----------|------|
| b	     | 1 month |
| fedc  | 5 hr   |
| e       | 10 min |
|        | 5 hr   |
| 	    | 5 hr   |

**Table 1. Observed per-component MTTFs**

In Mercury, each software component is isolated in its own Java virtual machine (JVM) process, so a failure in one component does not necessarily cause failures in others, and a restart of one component does not necessarily entail the restart of others. This suggests the potential to exploit partial restarts to lower MTTR. However, some failures do propagate across JVM boundaries, and restarting some components can cause other components to need a restart as well, leading to observed correlated failures. In the former case, a state dependency leads to a restart dependency; in the latter case, a functional dependency leads to a restart dependency. In the rest of this section, we describe how to modify the trivial restart tree to reduce the MTTR of the overall system, illustrating which tree modifications are most effective under specific conditions of correlated failures.

We describe three techniques: depth augmentation, which adds new nodes to the tree, and group consolidation and node promotion, which remove nodes from the tree. Since the focus of this work is on a recovery strategy designed for transient failures, we make the following simplifying assumption:

- **Assumption A**: All failures that occur are detectable by FD and curable through restart.

This assumption is consistent with the fail-silent and restart properties of our system’s components. Future work will address the detection of non-fail-silent failures and the diagnosis of whether failures are restart-curable.

Another assumption, **Assumption B**, arises when there is no functional redundancy in the system:

- **Assumption B**: A failure in any component will result in temporary unavailability of the entire system.

### 4.1. Simple Depth Augmentation

A failure in any component of tree I will result in a maximum-duration recovery. For example, 	 takes less than 6 seconds to restart, whereas fedc takes over 21 seconds. Whenever 	 fails, the entire system must be restarted, causing unnecessary downtime. We assume that components can restart concurrently without significantly affecting each other’s time-to-restart.

**Figure 3. Simple depth augmentation gives tree II.**

To measure the effect of this transformation on system recovery time, we cause the failure of each component (using a SIGKILL signal) and measure the system's recovery time. We log the time when the signal is sent and the time when the component logs a timestamped message indicating it is functionally ready. The difference between these times is the recovery time. Table 2 shows the results of 100 experiments for each failed component.

In the new restart tree II, each restart group (except the root) contains exactly one component. Due to Assumption B, the system’s MTTF remains unchanged, but its MTTR is lower because a failure in a component can potentially be cured by restarting a subset of the components, possibly only the failed component. Specifically, for a restart group G:
\[ \text{MTTR}_{\text{II}, G} = \sum_{i} f_{c_i} \cdot \text{MTTR}_{c_i} \]
where \( c_i \) is G’s i-th child, and \( f_{c_i} \) represents the probability that a manifested failure in G is minimally \( c_i \)-curable. As all observed failures in our ground station prototype were restart-curable, the sum of \( f_{c_i} \) in any G is 1. As long as our system contains some component \( c_k \) such that \( f_{c_k} > 0 \) and \( \text{MTTR}_{c_k} \neq \max(\text{MTTR}_{c_i}) \), the result will be that \( \text{MTTR}_{\text{II}} < \text{MTTR}_{\text{I}} \).

| Failed Node | MTTR_I (s) | MTTR_II (s) |
|-------------|------------|-------------|
| b	      | 24.75      | 5.73        |
| e        | 24.75      | 9.50        |
|         | 24.75      | 9.76        |
| 	    | 24.75      | 5.59        |
| fedc    | 24.75      | 20.93       |

**Table 2. Tree II recovery: time to detect failed component plus time to recover system (in seconds).**

Given that restart tree II now has more than one restart group, we assume the oracle is perfect, as described in section 3 (we will relax this assumption in section 4.4):

- **Assumption C**: The system’s oracle always recommends the minimal restart policy.

Another assumption in this transformation is that the restart groups are independently restartable:

- **Assumption D**: Restarting a group will not induce failures in any component of another restart group.

This assumption is crucial for recursive restartability, as it ensures strong fault-isolation boundaries around groups. In section 4.3, we describe how to transform the restart tree to preserve this property even when the design of our components imposes the relaxation of Assumption D.

### 4.2. Augmenting Depth of Tight Subtrees

An interesting observation is that components may be decomposable into sub-components with highly disparate MTTR and MTTF. In our system, the fedc component connects to the serial port at startup and negotiates communication parameters with the radio device. Thereafter, it translates commands received from other components to radio commands. Due to the hardware negotiation, it takes a long time to restart fedc, and due to instability in the command translator, it crashes often. Hence, fedc has high MTTR and low MTTF—a poor combination.

We therefore split fedc into the bc component, which maps a serial port to a TCP socket, and fed, the front-end driver-radio that connects to bc over TCP. bc is simple and very stable but takes a long time to recover (over 21 seconds); fed is buggy and unstable but recovers quickly (under 6 seconds). After restructuring the code and augmenting the restart tree (Figure 4), it becomes possible to restart the two components independently. We show the intermediate tree II', which is identical to tree II, except the fedc component is split.

**Figure 4. Subtree depth augmentation: tree III.**

The new tree III has no effect on the system’s MTTF, as the split did not affect the failure characteristics of what used to be fedc. All failures that were previously minimally curable by a restart of fedc are now minimally curable by a restart of bc, a restart of fed, or a restart of both. Since \( \text{MTTR}_{\text{fed}} \ll \text{MTTR}_{\text{bc}} \) and \( \text{MTTF}_{\text{fed}} \ll \text{MTTF}_{\text{bc}} \), most of the failures will be cured by quick fed restarts, and a few will require slow bc restarts, whereas previously they would have all required slow bc restarts.