# Bayesian Information Criterion (BIC) for Model Selection

The Bayesian Information Criterion (BIC) is defined as:
\[ \text{BIC}(M_i) = 2 \log L_C - (\ln(n + m))l, \]
where:
- \(\log L_C\) is the maximum likelihood of the complete data,
- \(n + m\) is the total number of observations,
- \(l\) is the number of parameters in the model.

## 5. Results

### 5.1 In-Sample Validation

We conducted experiments using dynamic logs obtained from our Android emulator. We compared the performance of MBSS with popular malware classifiers: SVM, kNN, and LDA. The experiments were divided into two categories: in-sample validation and out-of-sample validation.

#### In-Sample Validation
In-sample validation involves splitting a dataset into training and test sets. A classifier is trained on the training set and evaluated on the test set. Cross-validation is typically used to assess the classifier's performance. This setup provides an ideal classification scenario where the test data distribution matches the training data distribution.

For in-sample validation, we used a dataset of 55,994 Android APK dynamic logs, consisting of 24,217 benign and 31,777 malicious APKs. The in-sample malicious behaviors included stealing location, device ID, MAC information, dynamic code loading, and sending this information to external networks. The label distribution was 43% benign and 57% malicious, with a chance accuracy of 0.57.

We performed 10-fold cross-validation, reporting the mean accuracy, standard deviation of accuracies, and similar metrics for false positive rates. Table 1 summarizes the results:

| Classifier | Mean ACC | Sd ACC | Mean FP | Sd FP | DR for OOS1 | DR for OOS2 |
|------------|----------|--------|---------|-------|-------------|-------------|
| MBSS       | 97.6%    | 0.002  | 3%      | 0.004 | 55.3%       | 90.8%       |
| SVM (radial) | 98.8% | 0.002  | 1.8%    | 0.003 | 0.06%       | 68.4%       |
| SVM (linear) | 98.6% | 0.001  | 2%      | 0.003 | 35.4%       | 9.4%        |
| 3NN        | 97.9%    | 0.001  | 3%      | 0.004 | NA          | 0           |
| LDA        | 90.0%    | 0.003  | 6%      | 0.003 | 33.8%       | 0           |

Under this ideal classification scenario, SVM with radial and linear kernels demonstrated the best performance, with accuracies of 98.8% and 98.6%, respectively, and false positive rates of 2%. 3NN and MBSS had similar performance, with accuracies of 97.6% and 97.9%, and false positive rates of 3%. LDA showed the lowest performance, with an accuracy of 90% and a false positive rate of 6%.

#### Out-of-Sample Classification
Out-of-sample classification tests the classifier's ability to predict labels for incoming unlabeled data that may not follow the same distribution as the training data. This is a common challenge in practical applications. A robust and practical classifier should maintain reasonable performance in out-of-sample scenarios.

##### 5.2.1 OOS1: Out-of-Sample with Similar Malicious Behaviors
We applied the five classifiers to a dataset of 12,185 malicious APKs, which exhibited similar malicious behaviors to the training set, such as intercepting and sending messages without user consent. Principal Component Analysis (PCA) was used to visualize the similarity in data distribution.

Table 1 shows the detection rates for the five classifiers. Both SVM with a linear kernel and MBSS achieved a detection rate of 0.9, while 3NN had a detection rate of 0.68, and LDA had the lowest rate at 0.1. SVM with a radial kernel, which performed best in in-sample testing, showed a significant degradation in performance, with a detection rate near 0. The highly parametric nature of LDA contributed to its poor performance.

##### 5.2.2 OOS2: Out-of-Sample with Dissimilar Malicious Behaviors
In this experiment, we applied the classifiers to a dataset of 11,986 malicious APKs, whose behaviors primarily involved stealing private information and sending it to the Internet via a command and control (C&C) server, but did not include dynamic code loading. PCA revealed slight similarities in the data distribution.

Table 1 shows the detection performance of the five classifiers. All classifiers experienced performance degradation. SVM with a linear kernel degraded significantly, and 3NN failed due to too many ties. MBSS outperformed the other classifiers, maintaining a robust detection rate.

To further examine the out-of-sample classification performance, we varied the test size by applying the classifiers to randomly selected subsets of the test data (0.1%, 1%, 20%, 50%, 90%) with independent Monte Carlo replications (50, 30, 20, 10, 5, 1). MBSS consistently showed superior performance compared to the other classifiers.

## 6. Conclusion

In this study, we demonstrated the effectiveness of the model-based semi-supervised learning (MBSS) approach for detecting Android malware using dynamic behavior data. For in-sample testing, MBSS achieved competitive accuracy and false positive rates compared to popular malware classifiers. For out-of-sample testing, MBSS produced significantly higher detection rates. Our findings suggest that the framework of semi-supervised learning for dynamic analysis is valuable and practical for anti-malware research.

## References

[1] 2008. Android Debug Bridge. https://developer.android.com/studio/command-line/adb.html. (2008).

[2] 2012. Droidbox. https://github.com/pjlantz/droidbox. (2012).

[3] 2014. MindMac/AndroidEagleEye. https://github.com/MindMac/AndroidEagleEye. (2014).

[4] 2015. Droidmon. https://github.com/idanr1986/droidmon. (2015).

[5] 2016. CuckooDroid - Automated Android Malware Analysis. https://github.com/idanr1986/cuckoo-droid. (2016).

[6] 2016. rovo89/Xposed. https://github.com/rovo89/xposed. (2016).

[7] Chris Fraley, Adrian E. Raftery, Thomas Brendan Murphy, and Luca Scrucca. 2012. mclust Version 4 for R: Normal Mixture Modeling for Model-Based Clustering, Classification, and Density Estimation.

[8] Tony Jebara and Alex Pentland. 1998. Maximum conditional likelihood via bound maximization and the CEM algorithm. In Proceedings of the 11th International Conference on Neural Information Processing Systems. MIT Press, 494â€“500.

[9] R Core Team. 2016. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.

[10] Niamh Russell, Laura Cribbin, and Thomas Brendan Murphy. 2014. upclass: Updated Classification Methods using Unlabeled Data. R package version 2.0.