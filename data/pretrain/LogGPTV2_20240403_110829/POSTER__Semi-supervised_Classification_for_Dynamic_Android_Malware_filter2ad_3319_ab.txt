BIC(Mi) = 2 log LC − ln(n + m)l,
(3)
where log LC is the maximum likelihood of the complete data, n +m
is the number of obeservations, and l is the number of parameters.
5 RESULTS
We experiment using the dynamic logs obtained from our Android
emulator. We compare MBSS with some of the most popular mal-
ware classifiers: SVM, kNN and LDA. We conduct two categories
of experiments: in-sample validation and out-of-sample validation.
In-sample validation is the typical classification setting where a
dataset is split into training and test set. A classifier is trained on
the training set and expected to perform well when tested on the
test set. Cross validation is usually employed to assess the perfor-
mance of a classifier. In-sample classification provides the ideal
classification scenario, where test data distribution is the same as
the training data distribution.
Our next set of experiments focuses on out-of-sample testing. An
out-of-sample experiment uses the classifier trained and validated
on the in-sample data, and predicts the labels of incoming unlabeled
data. The vast majority of the unlabeled data are not guaranteed to
follow the same distribution as the in-sample training data. This
is a challenge for machine learning algorithms used for practical
applications. We consider a classifier robust and practical when it
can still achieve reasonably well performance for out-of-sample
classification. All the APKs for our experiments are retrieved from
VirusTotal, and the experiments are conducted in R [9], [10], [7].
5.1 In-sample validation
We first demonstrate that for in-sample classification, MBSS has
competitive performance when compared with SVM with radial
kernel, SVM with linear kernel, 3NN and LDA.
As in-sample dataset, we obtain 55994 Android APK dynamic
logs from our emulator with 24217 benign Android APKs and 31777
malicious APKs. The in-sample malicious behaviors include steal-
ing location, device ID, MAC information, dynamic code loading
behavior, and sending the information to outside network. The label
distribution is (43%, 57%) and the chance accuracy (classification
accuracy when randomly guessing) is 0.57.
We conduct 10-fold cross validation, report the accuracy mean by
averaging the accuracies and calculate the standard deviation of the
accuracies across all the 10 folds, and report the similar metrics for
false positive rates. As indicated in Table 1, all the classifiers have
PosterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2480Table 1: Classification performance comparison for all five classifiers. All classifiers have competitive classification perfor-
mance for in-sample testing. For OOS1, MBSS and SVM with linear kernel achieve the highest detection rate. For OOS2, MBSS
performs significantly better than all other classifiers. Due to too many ties for 3NN, we do not report its result here.
Mean ACC Sd ACC Mean FP
Classifier
MBSS
SVM (radial)
SVM (linear)
3NN
LDA
97.6%
98.8%
98.6%
97.9%
90.0%
0.002
0.002
0.001
0.001
0.003
3%
1.8%
2%
3%
6%
90.0%
Sd FP DR for OOS1 DR for OOS2
0.004
0.003
0.003
0.004
0.003
55.3%
0.06%
35.4%
NA
33.8%
90.8%
68.4%
9.4%
0
competitive performance. Under this ideal classification scenario,
SVM with radial and linear kernels demonstrate the best perfor-
mance with accuracies at 98.8% and 98.6% respectively and false
positive rates both at 2%, 3NN and MBSS have similar performance
of accuracy at 97.6% and 97.9% respectively and false positive rate
at 3%, while LDA shows lesser performance with accuracy at 90%
and false positive at 6%. The receiver operating curve (ROC) of
MBSS in the 10-th fold classification and the area under the curve
is 0.99.
5.2 Out-of-sample classification
Here we report the detection rate (DR), which is defined by the
number of correctly classified malware APKs divided by the total
number of out-of-sample test data. After validating that these classi-
fiers have high accuracy and low false positive in Section 5.1, we use
them to test on incoming samples. Under this practical and realistic
scenario, the test samples do not follow very similar distribution as
the training samples. In this case, the classifiers with high accuracy
degrade, sometimes even significantly, for out-of-sample testing.
5.2.1 OOS1: Out-of-sample with malicious similarity to in-sample
data. We first apply the five classifiers on a dataset of 12185 ma-
licious APKs and report the detection rate. These out-of-sample
APKs exhibit similar malicious behaviors of intercepting and send-
ing messages without the user’s consent as in the training set.
To visualize this similarity, we conduct principal component
analysis (PCA) and inspect the scatter plots of the first four principal
components (PC). (See poster for details)
The sixth column in Table 1 demonstrates the detection rates of
the five classifiers. Both SVM with linear kernel and MBSS have
detection rate of 0.9, while the performance of 3NN is 0.68 and
LDA has the lowest detection rate of 0.1. A dramatic performance
degradation is seen for SVM with radial kernel, which went from
the best in-sample performing classifier to the worst classifier with
detection rate near 0. The significant degradation of LDA is due to
its highly parametric nature. In this case, SVM with linear kernel
and MBSS maintain relatively stable detection rate.
apply the classifiers on the randomly selected {0.1%, 1%, 20%, 50%, 90%}
of the test data with independent Monte Carlo replications at
{50, 30, 20, 10, 5, 1} respectively. We also observe the superior per-
formance of MBSS at 0.9, compared with other classifiers.
Next, we examine the detection rate as we vary the test size. We
5.2.2 OOS2: Out-of-sample with dissimilar distribution to in-
sample data. Our next out-of-sample experiment applies the five
classifiers onto a dataset of 11986 malicious APKs, whose malicious
behaviors primary include stealing private information, sending it
to the Internet through commodity command and control (C&C)
server, but do not include dynamic code loading behavior. This
slight similarity in malicious behavior can been seen in the data
distribution as reflected by the principal components achieved from
principal component analysis.
The last right column in Table 1 demonstrates the detection
performance of the five classifiers. All classifiers degrade in perfor-
mance. However SVM with linear kernel degrades in performance
significantly. The result of 3NN is omitted, because it fails due to
too many ties. MBSS performs significantly better than the other
classifiers.
We vary the test size to examine the out-of-sample classification
performance. As we vary the test size, we apply the classifiers on
the randomly selected {0.1%, 1%, 20%, 50%, 90%} of the test data
with independent Monte Carlo replications at {50, 30, 20, 10, 5, 1}
respectively. We also observe the robust performance of MBSS
compared to the great degradation by other classifiers.
6 CONCLUSION
In this poster, we demonstrate the effectiveness of using model-
based semi-supervised learning (MBSS) approach on dynamic An-
droid behavior data for Android malware detection. We show that
for in-sample testing, MBSS has competitive accuracy and false
positive rate compared with the most popular malware classifiers.
For out-of-sample testing, MBSS produces significantly higher de-
tection rate compared with the other classifiers in consideration.
We are optimistic that the framework of semi-supervised learning
for dynamic analysis is valuable and practical for anti-malware
research.
REFERENCES
[1] 2008.
https://developer.android.com/studio/
Android Debug Bridge.
command-line/adb.html. (2008).
[2] 2012. Droidbox. https://github.com/pjlantz/droidbox. (2012).
[3] 2014.
MindMac/AndroidEagleEye.
https://github.com/MindMac/
AndroidEagleEye. (2014).
[4] 2015. Droidmon. https://github.com/idanr1986/droidmon. (2015).
[5] 2016. CuckooDroid - Automated Android Malware Analysis. https://github.com/
idanr1986/cuckoo-droid. (2016).
[6] 2016. rovo89/Xposed. https://github.com/rovo89/xposed. (2016).
[7] Chris Fraley, Adrian E. Raftery, Thomas Brendan Murphy, and Luca Scrucca.
2012. mclust Version 4 for R: Normal Mixture Modeling for Model-Based Clustering,
Classification, and Density Estimation.
[8] Tony Jebara and Alex Pentland. 1998. Maximum conditional likelihood via bound
maximization and the CEM algorithm. In Proceedings of the 11th International
Conference on Neural Information Processing Systems. MIT Press, 494–500.
[9] R Core Team. 2016. R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria.
[10] Niamh Russell, Laura Cribbin, and Thomas Brendan Murphy. 2014. upclass:
Updated Classification Methods using Unlabeled Data. R package version 2.0.
PosterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2481