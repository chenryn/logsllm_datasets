ziLzLk
X
k
zLk
Nsik
N
(15)
1An alternative to (13) would be to compute the Hamming
distance between the two assignment vectors. However, this
has the drawback of penalizing pairs with diﬀerently sized
role sets.
105In the second line we made use of the fact that a user is only
assigned to a single set of roles L.
Given the top-down objective function in this form, we
can directly compare it with the log-likelihood costs given
by (10).
5.3 Inference Algorithm
We use Deterministic Annealing (DA) [2, 18], an iterative
gradient-descent optimization method, to infer the model
parameters. In the following, we explain how we compute
the objective function derived above in such an iterative set-
ting. Afterwards, we brieﬂy describe this iterative optimiza-
tion scheme and explain how, in each DA-step, we update
the parameters to be optimized in our particular problem.
Computation of R(S)
i,L.
Given an iterative optimization scheme for minimizing an
objective function R(S), one faces a computational prob-
lem with the above quantities: compute the optimal as-
signments ziL from the Nsik, which are, in turn, computed
from the ziL themselves. To make this computation at
step t of our algorithm feasible, we use the expected as-
signments γ(t−1)
of the previous step instead
of the Boolean z(t)
z(t−1)
iL
iL
h
i
i
X
L
h
:= E
iL to approximate N (t)
N (t−1)
X
zLk
=
sik
i0
sik by its expectation:
wi0si γ(t−1)
i0L .
(16)
sik ≈ E
N (t)
i,L
This so-called mean-ﬁeld approximation [2] makes the com-
putation of R(S)(t)
feasible. Therewith, the costs of a user
belonging to a set of roles are
N (t−1)
sik
N
N (t−1)
sik
N
i,L ≈
R(S)(t)
X
X
h
i
h
i
(17)
E
E
.
−
k∈L
k /∈L
Deterministic Annealing.
Deterministic Annealing is a gradient-descent algorithm
for optimizing an objective function. At each step t of the
algorithm, it enables a smoothly varying trade-oﬀ between
the cost function to be optimized and the uniform distri-
P
bution controlled by the Lagrange parameter T . The cost
function R(·) of a problem determines the Gibbs distribution
{·} exp(−R(·)/T )
p(·) = 1/Z exp(−R(·)/T ), where Z =
is the normalizing constant and the sum is over all points
in the solution space. Minimizing the Lagrangian F =
−T log(Z) = E [R] − T H at a given T is equivalent to max-
imizing the entropy H (seeking a solution close to the uni-
form distribution) while minimizing the expected costs E [R]
(seeking a minimum cost solution). For historical reasons,
T is often called the (computational) temperature. Start-
ing the optimization at a high T and successively decreasing
it, smooths the costs landscape in the beginning and helps
the gradient-based optimization procedure to avoid getting
trapped in local minima.
We choose an initial temperature and a constant rate cool-
ing scheme (T (t) = α · T (t−1), with α  0.02average = 0.24 bits00.10.20.30.40.50.60.70.80.910100200300400500I(perm;JC) [bit]number of permissionsHistogram of Mutual Information between JC and permissions  p(perm)    0.02average = 0.14 bits00.10.20.30.40.50.60.70.80.910100200300400500I(perm;OU) [bit]number of permissionsHistogram of Mutual Information between OU and permissions  p(perm)    0.02average = 0.22 bits00.10.20.30.40.50.60.70.80.910100200300400500I(perm;OUJC) [bit]number of permissionsHistogram of Mutual Information between OUJC and permissions  p(perm)    0.02average = 0.23 bits30405060708090050100150200I(perm;JC)/h(perm)) = 1 − h(perm|JC) / h(perm) [%]number of permissionsDistribution over Fraction of Permission Entropy removed by JC  p(perm)   0.02average = 49.7%304050607080900200400600I(perm;OU)/h(perm)) = 1 − h(perm|OU) / h(perm) [%]number of permissionsDistribution over Fraction of Permission Entropy removed by OU  p(perm)   0.02average = 87.8%107The top histogram in Figure 1 illustrates the distribu-
tion of the permission entropy h(Xj) for the direct user-
permission assignment. Since the assignment of a permission
is either one or zero, the maximum entropy is one bit, which
corresponds to a permission that is possessed by exactly half
of the users. Permissions possessed by either very few, or
almost all, users have low entropy. For the enterprise under
consideration, all permissions with low entropy belong to
only a few users. To make this distinction clear, in all of the
histograms in Figures 1 and 2, we display counts of permis-
sions shared by less than 2% of the employees in white and
the counts of all other permissions in black. As can be seen
in the top histogram of Figure 1, most of the permissions
have low entropy, but a signiﬁcant number of permissions
have very high entropy. The lower three histograms show
(in this order) the distribution of the mutual information
between permissions and job-codes, organization unit, and
the combination of the two.
The results are surprising. Since the job-code provides an
abstract high-level job description, one might expect it to be
highly relevant. However, the results show that a user’s job-
code carries only little information about his permissions.
The reason is that, in this enterprise, job-codes are not re-
ally abstract task descriptions. Instead they express other
properties that are interesting for Human Relations, such as
the employee’s salary class, contract duration, seniority, etc.
In contrast, we found that the organization unit is much
more relevant for the user’s permissions. On average, the
organizational unit reduces the entropy by 0.22 bits. More-
over, the entropy of a large number of permissions is even
reduced by 0.9 bits (right peak in second lowest histogram of
Figure 1). Combining the two attributes yields only a slight
gain of 0.01 bits on average (see the lower two histograms
of Figure 1). Hence, we conclude that most of the informa-
tion gained by using job-codes is already contained in the
organizational units.
For the bottom three histograms, note that the bimodal
distribution of the permission-entropy histogram is preserved:
a high peak at very low entropy and a smaller peak at high
entropy. This leads us to a general problem in interpreting
mutual information. In many cases, the mutual information
I(Xj; S) is low simply because the entropy h(Xj) of the per-
mission j is low. This is the case for permissions that almost
all users have (for instance, reading email) or, as is usually
the case in this data, permissions that very few users have.
In Figure 1, we highlighted such permissions in white. This
illustrates that almost all permissions whose entropy is not
reduced by the knowledge of the given business information
have a very low entropy anyway.
To overcome this problem, we weight I(Xj; S) by 1/h(Xj)
and obtain the relative mutual information ρj(S) = 1 −
h(Xj|S)/h(Xj) (see Eq. 4) as a relevance measure that in-
dicates the fraction of entropy that is explained by S (see
Fig. 2). This relative representation better reveals the diﬀer-
ence in information content between organization units and
job-codes. Whereas, on average, job codes remove roughly
50% of the uncertainty, knowledge of the organization unit
removes 88%. Admittedly, there is no way to really deter-
mine if knowledge of S would decrease more of the permis-