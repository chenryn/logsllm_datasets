hosts [1] in February 2017, October 2017, and January
2018. Similar to [23, 27], this measurement cannot apply
to DKIM, because querying the DKIM record requires
knowing the selector information for every each domain.
The selector information is only available in the DKIM
signature in the email header, which is not a public in-
formation. We will measure the DKIM usage later in the
end-to-end measurement.
Recent Adoption Rates.
Table 1 shows the statis-
tics for the most recent January 2018 snapshot. SPF
and DMARC both have some increase in the adoption
rate but not very signiﬁcant. About 44.9% of the do-
mains have published a valid SPF record in 2018 (40%
in 2015 [27]), and 5.1% have a valid DMARC record
in 2018 (1.1% in 2015 [27]). The invalid records are
often caused by the domain administrators using the
wrong format for the SPF/DMARC record. Another
common error is to have multiple records for SPF (or
DMARC), which is equivalent to “no record” according
to RFC7489 [50]. Figure 2 shows the adoption rate for
all three snapshots. Again, the adoption rates have been
increasing at a slow speed.
Among the 1 million domains, 792,556 domains are
MX domains (i.e., mail exchanger domains that host
email services). The adoption rates among MX do-
mains are slightly higher (SPF 54.3%, DMARC 6.0%).
For non-MX domains, we argue that it is also impor-
tant to publish the SPF/DMARC record. For example,
office.com is not a MX domain, but it hosts the website
of Microsoft Ofﬁce. Attackers can spoof office.com to
phish Microsoft Ofﬁce users or even the employees.
Failing Policy.
SPF and DMARC both specify a
policy regarding what actions the receiver should take
after the authentication fails. Table 1 shows that only
a small portion of the domains speciﬁes a strict “reject”
policy: 12.5% of the domains set “hard fail” for SPF, and
1098    27th USENIX Security Symposium
USENIX Association
 0 0.1 0.2 0.3 0.4 0.5 0.602/1710/1701/1802/1710/1701/18RateSPF-RejectSPF-OtherDMARC-RejectDMARC-OtherDMARCSPFFigure 3: The adoption rate as a function of the domains’
Alexa rankings (January 2018).
0.6% set “reject” for DMARC. The rest of the domains
simply leave the decision to the email receiver. “Soft
fail”/“quarantine” means that the email receiver should
process the email with caution. “Neutral”/“none” means
that no policy is speciﬁed. SPF’s “pass” means that the
receiver should let the email go through.
If a domain
has both SPF and DMARC policies, DMARC overwrites
SPF as long as the DMARC policy is not “none”.
Domains that use DKIM also need to publish their
policies through DMARC. The fact that only 5.1% of the
domains have a valid DMARC record and 0.6% have a
“reject” policy indicates that most DKIM adopters also
did not specify a strict reject policy.
Popular Domains.
Not too surprisingly, popular do-
mains’ adoption rates are higher as shown in Figure 3.
We divide the top 1 million domains into log-scale sized
bins. For SPF, the top 1,000 domains have an adoption
rate of 73%. For DMARC, the adoption rate of top 1000
domains is 41%. This indicates that administrators of
popular domains are more motivated to prevent their do-
mains from being spoofed. Nevertheless, there is still a
large number of (popular) domains remain unprotected.
4 End-to-End Spooﬁng Experiments
Given the current adoption rate of SMTP extension pro-
tocols, it is still challenging for email providers to reli-
ably authenticate all incoming emails. When encounter-
ing questionable emails, we are curious about how email
providers make such decisions. In the following, we de-
scribe the details of our measurement methodology and
procedures.
4.1 Experiment Setup
We conduct end-to-end spooﬁng experiments on popu-
lar email providers that are used by billions of users. As
shown in Figure 4, for a given email provider (B.com),
we set up a user account under B.com as the email re-
ceiver (PI:EMAIL). Then we set up an experimental
Figure 4: End-to-end spooﬁng experiment setup. We use
our server E.com to send a forged email to the target
email service B.com by spooﬁng A.com.
server (E.com) to send forged emails to the receiver ac-
count. Our server runs a Postﬁx mail service [3] to di-
rectly interact with the target mail server using SMTP.
By controlling the input (the forged email) and observing
the output (the receiver account), we infer the decision-
making process inside of the target email service.
Selecting Target Email Providers.
This study fo-
cuses on popular and public email services with two con-
siderations. First, popular email services such as Ya-
hoo Mail and Gmail are used by more than one billion
users [46, 55]. Their security policies and design choices
are likely to impact more people. Second, to perform
end-to-end experiments, we need to collect data from the
receiver end. Public email services allow us to create an
account as the receiver. Our experiment methodology is
applicable to private email services but requires collabo-
rations from the internal users.
To obtain a list of popular public email services, we
refer to Adobe’s leaked user database (152 million email
addresses, 9.3 million unique email domains) [41]. We
ranked the email domains based on popularity, and man-
ually examined the top 200 domains (counting for 77.7%
of all email addresses). After merging domains from the
same service (e.g., hotmail.com and outlook.com)
and excluding services that don’t allow us to create
an account, we obtained a short list of 28 email do-
mains. To include the more recent public email ser-
vices, we searched on Google and added 6 more ser-
vices (yeah.net, protonmail.com, tutanota.com,
zoho.com, fastmail.com, and runbox.com). We no-
tice that Google’s Gmail and Inbox have very different
email interfaces and we treat them as two services.
In total, we have 35 popular email services which
cover 99.8 million email addresses (65.7%) in the Adobe
database. As an additional reference, we also analyze the
Myspace database (131.4 million email addresses) [54].
We ﬁnd that 101.8 million email addresses (77.5%) are
from the 35 email services, conﬁrming their popularity.
The list of the email providers is shown in Table 2
4.2 Experiment Parameters
To examine how different factors affect the outcome
of email spooﬁng, we apply different conﬁgurations to
the experiment. We primarily focus on parameters that
USENIX Association
27th USENIX Security Symposium    1099
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1(0-1k](1k-10k](10k-100k](100k-1m](0-1k](1k-10k](10k-100k](100k-1m]RateSPF-RejectSPF-OtherDMARC-RejectDMARC-OtherDMARCSPFHTTPIMAPPOPSMTPE.comExperiment Mail Server (Sender) B.comTarget Email ServerTarget Email ClientWithin Our Controltest@B.comMAIL FROM : PCPT TO : are likely to affect the spooﬁng outcome, including the
spoofed sender address, email content, sender IP, and the
receiver’s email client (user interface).
Spoofed Sender Address.
The sender address is
a critical part of the authentication. For example, if
the spoofed domain (A.com) has a valid SPF/DKIM/D-
MARC record, then the receiver (in theory) is able to
detect spooﬁng. We conﬁgure three proﬁles for the
spoofed sender domain: (1) None: no SPF/DKIM/D-
MARC record (e.g., thepiratebay.org); (2) Relaxed:
SPF/DKIM with a “none” policy (e.g., tumblr.com);
and (3) Strict: SPF/DKIM with a strict “reject” policy
(e.g., facebook.com). For each proﬁle, we randomly
pick 10 domains (30 domains in total) from Alexa top
5000 domains (the detailed list is in Appendix A).
Email Content.
Email content can affect how spam
ﬁlters handle the incoming emails [11]. Note that our
experiment is not to reverse-engineer exactly how spam
ﬁlters weight different keywords, which is an almost
inﬁnite searching space.
Instead, we focus on spoof-
ing (where the sender address is forged). We want to
minimize the impact of spam ﬁlters and examine how
the receivers’ decision is affected by the address forgery
(spooﬁng) alone.
To this end, we conﬁgure 5 different types of email
content for our study:
(1) a blank email, (2) a blank
email with a benign URL (http://google.com), (3)
a blank email with a benign attachment (an empty text
ﬁle). Then we have (4) a benign email with actual con-
tent. This email is a real-world legitimate email that in-
forms a colleague about the change of time for a meet-
ing. The reason for using “benign” content is to test how
much the “spooﬁng” factor alone contributes to the email
providers’ decisions. In addition, to test whether a phish-
ing email can penetrate the target service, we also include
(5) an email with phishing content. This phishing email
is a real-world sample from a phishing attack targeting
our institution recently. The email impersonates the tech-
nical support to notify the victim that her internal account
has been suspended and ask her to re-activate the account
using a URL (to an Amazon EC2 server).
Sender IP.
The IP address of the sender’s mail server
may also affect the spooﬁng success. We conﬁgure a
static IP address and a dynamic IP address. Typically,
mail servers need to be hosted on a static IP. In practice,
attackers may use dynamic IPs for the lower cost.
Email Client.
We examine how different email
clients warn users of forged emails. We consider 3 com-
mon email clients: (1) a web client, (2) a mobile app,
and (3) a third-party email client. All the 35 selected
services have a web interface, and 28 have a dedicated
mobile app. Third-party clients refer to the email ap-
plications (e.g., Microsoft Outlook and Apple Mail) that
allow users to check emails from any email providers.
5 Spooﬁng Experiment Results
In this section, we describe the results of our experi-
ments. First, to provide the context, we measure the au-
thentication protocols that the target email providers use
to detect forged emails. Then, we examine how email
providers handle forged emails and identify the key fac-
tors in the decision making. For emails that reached
the inbox, we examine whether and how email providers
warn users about their potential risks. Note that in this
section, the all experiment results reﬂect the state of the
target email services as of January 2018.
5.1 Authentication Mechanisms
To better interpret the results, we ﬁrst examine how the
35 email providers authenticate incoming emails. One
way of knowing their authentication protocols is to an-
alyze the email headers and look for SPF/DKIM/D-
MARC authentication results. However, not all the
email providers add the authentication results to the
header (e.g., qq.com) Instead, we follow a more reliable
method [27] by setting up an authoritative DNS server
for our own domain and sending an email from our do-
main. In the meantime, the authoritative DNS server will
wait and see whether the target email service will query
the SPF/DKIM/DMARC record. We set the TTL of the
SPF, DKIM and DMARC records as 1 (second) to force
the target email service always querying our authorita-
tive DNS server. The results are shown in Table 2 (left
4 columns). 35 email providers can be grouped into 3
categories based on their protocols:
• Full Authentication (16): Email services that per-
form all three authentication checks (SPF, DKIM and
DMARC). This category includes the most popular
email services such as Gmail, Hotmail and iCloud.
• SPF/DKIM but no DMARC (15): Email services
that check either SPF/DKIM, but do not check the
sender’s DMARC policy. These email services are
likely to make decisions on their own.
• No Authentication (4): Email services that do not
perform any of the three authentication protocols.
5.2 Decisions on Forged Emails
Next, we examine the decision-making process on forged
emails. For each of the 35 target email services, we test
all the possible combinations of the parameter settings
(30 spoofed addresses × 5 types of email content × 2 IP
1100    27th USENIX Security Symposium
USENIX Association
Email Content
Benign
Email
Provider
mail.ru
fastmail.com
163.com
126.com
gmail.com
gmail inbox
naver.com
yeah.net
tutanota.com
yahoo.com
inbox.lv
protonmail.com
seznam.cz
aol.com
icloud.com
hotmail.com
juno.com
sina.com
op.pl
sapo.pt
zoho.com
qq.com
mynet.com
gmx.com
mail.com
daum.net
runbox.com
interia.pl
o2.pl
wp.pl
sohu.com
t-online.de
excite.com
freemail.hu
rediffmail.com
Supported Protocols
SPF
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)




DKIM DMARC
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)





(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)



















Overall
Rate
n=1500
0.69
0.66
0.58
0.57
0.53
0.53
0.50