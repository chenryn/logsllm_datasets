Currently using Celery 4.1.0
## Steps to reproduce
Start a new project using RabbitMQ and register the following task:
    from django.core.cache import cache
    @shared_task(bind=True)
    def test_1(self):
        if not cache.add(self.request.id, 1):
            raise Exception('Duplicated task {}'.format(self.request.id))
Now start 2 workers. I used gevent with a concurrency of 25 for this test:
    celery worker -A my_proj -Q my_queue -P gevent -c 25
Open a python shell and fire a a bunch of tasks:
    from myproj.tasks import test_1
    for i in range(10000):
        test_1.apply_async()
Now quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still
processing the tasks, you should see the errors popping in the second worker:
    ERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)
    Traceback (most recent call last):
      File "/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py", line 374, in trace_task
        R = retval = fun(*args, **kwargs)
      File "/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py", line 629, in __protected_call__
        return self.run(*args, **kwargs)
      File "/code/scp/python/my_proj/tasks.py", line 33, in test_1
        raise Exception('Duplicated task {}'.format(self.request.id))
    Exception: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9
## Expected behavior
Since I am not using late acknowledgment and I am not killing the workers I
wasn't expecting the tasks to execute again.
## Actual behavior
The tasking are being executed twice, this is causing some problems in our
servers because we restart our works every 15 minutes or so in order to avoid
memory leaks.