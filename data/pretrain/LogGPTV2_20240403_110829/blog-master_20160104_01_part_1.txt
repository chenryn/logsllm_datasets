## zfs 快照增量大小 vs PostgreSQL产生的XLOG大小  
### 作者                                                                         
digoal                       
### 日期                         
2016-01-04                       
### 标签                       
PostgreSQL , zfs , xlog , 增量 , 对比           
----                      
## 背景                     
zfs快照增量 和 oracle的rman incremental backup极其类似。是其他不具备oracle rman 数据库级增量备份的数据库产品的福音，例如PostgreSQL(注意，使用pg_rman , pg_probackup都可以支持块级增量了)。  
下面我们来测试一下zfs快照增量的空间占用情况？  
理论上，快照增量的极限是当前zfs文件系统的大小，也就是在打完快照后，快照对应的ZFS上的每个块都被改变了。  
所以文件系统越大，同时更新面越广，快照就可能越大。  
这种情况什么时候会发生呢？  
比如插入数据库的每条记录，将来都会被变更一次，这样的应用场景，快照是会很大的。  
测试CASE， TPC-B。  
环境  
```  
PostgreSQL 9.5rc1  
$PGDATA目录  
zp1/data01            4.1T  1.1T  3.1T  25% /data01  
pg_xlog目录  
挂载在zfs以外的某个目录。  
```  
参数  
关闭full page write(因为$PGDATA所在的zfs是cow的，不需要开启FPW。)  
数据量, 70亿，包括索引超过1TB。  
```  
pgbench -i -s 70000  
```  
这样的测试数据量，覆盖的范围是1TB，而且测试包含了4条更新，1条插入，更新范围是1TB的范围，所以单个快照最大的新增的空间是可能达到1TB的大小的。  
## 测试1  
测试开始前，创建检查点，记录XLOG位置。  
```  
postgres=# checkpoint;  
CHECKPOINT  
postgres=# select pg_current_xlog_insert_location();  
 pg_current_xlog_insert_location   
---------------------------------  
 592/9D563C70  
(1 row)  
```  
创建快照  
```  
#zfs snapshot zp1/data01@2016010401  
```  
压测600秒  
```  
pgbench -M prepared -n -r -P 1 -c 48 -j 48 -T 600  
transaction type: TPC-B (sort of)  
scaling factor: 70000  
query mode: prepared  
number of clients: 48  
number of threads: 48  
duration: 600 s  
number of transactions actually processed: 2866811  
latency average: 10.044 ms  
latency stddev: 16.135 ms  
tps = 4777.542920 (including connections establishing)  
tps = 4777.781687 (excluding connections establishing)  
statement latencies in milliseconds:  
        0.004060        \set nbranches 1 * :scale  
        0.001105        \set ntellers 10 * :scale  
        0.000861        \set naccounts 100000 * :scale  
        0.001687        \setrandom aid 1 :naccounts  
        0.001017        \setrandom bid 1 :nbranches  
        0.000961        \setrandom tid 1 :ntellers  
        0.001000        \setrandom delta -5000 5000  
        0.052541        BEGIN;  
        9.318532        UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;  
        0.138619        SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
        0.169681        UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;  
        0.147535        UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;  
        0.115753        INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);  
        0.079338        END;  
```  
创建检查点，记录XLOG位置  
```  
postgres=# checkpoint;  
CHECKPOINT  
postgres=# select pg_current_xlog_insert_location();  
 pg_current_xlog_insert_location   
---------------------------------  
 592/EC6AEB38  
(1 row)  
```  
计算XLOG产生量  
```  
postgres=# select pg_size_pretty(pg_xlog_location_diff('592/EC6AEB38', '592/9D563C70'));  
 pg_size_pretty   
----------------  
 1265 MB  
(1 row)  
```  
创建快照  
```  
#zfs snapshot zp1/data01@2016010402  
```  
计算快照增量，约120GB，比XLOG大很多。后面会分析原因。  
```  
#zfs send -n -P -v -i zp1/data01@2016010401 zp1/data01@2016010402  
incremental     2016010401      zp1/data01@2016010402   124825285344  
size    124825285344  
```  
## 测试2  
记录XLOG位置  
```  
postgres=# select pg_current_xlog_insert_location();  
 pg_current_xlog_insert_location   
---------------------------------  
 592/ED000098  
(1 row)  
```  
压测600秒  
```  
pgbench -M prepared -n -r -P 1 -c 48 -j 48 -T 600  
transaction type: TPC-B (sort of)  
scaling factor: 70000  
query mode: prepared  
number of clients: 48  
number of threads: 48  
duration: 600 s  
number of transactions actually processed: 1837930  
latency average: 15.667 ms  
latency stddev: 34.785 ms  
tps = 3062.922412 (including connections establishing)  
tps = 3063.082480 (excluding connections establishing)  
statement latencies in milliseconds:  
        0.004004        \set nbranches 1 * :scale  
        0.001107        \set ntellers 10 * :scale  
        0.000849        \set naccounts 100000 * :scale  
        0.001673        \setrandom aid 1 :naccounts  
        0.000973        \setrandom bid 1 :nbranches  
        0.000928        \setrandom tid 1 :ntellers  
        0.000969        \setrandom delta -5000 5000  
        0.052813        BEGIN;  
        14.959468       UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;  
        0.131955        SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
        0.156640        UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;  
        0.140038        UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;  
        0.125661        INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);  
        0.079363        END;  
```  
创建检查点，记录XLOG位置  
```  
postgres=# checkpoint;  
CHECKPOINT  
postgres=# select pg_current_xlog_insert_location();  
 pg_current_xlog_insert_location   
---------------------------------  
 593/1FAEC728  
(1 row)  
```  
产生XLOG量  
```  
postgres=# select pg_size_pretty(pg_xlog_location_diff('593/1FAEC728', '592/ED000098'));  
 pg_size_pretty   
----------------  
 811 MB  
(1 row)  
```  
创建快照  
```  
#zfs snapshot zp1/data01@2016010403  
```  
计算快照增量，约80GB  
```  
#zfs send -n -P -v -i zp1/data01@2016010402 zp1/data01@2016010403  
incremental     2016010402      zp1/data01@2016010403   83633656888  
size    83633656888  
```  
递归增量，总共约200GB  
```  
#zfs send -n -P -v -I zp1/data01@2016010401 zp1/data01@2016010403  
incremental     2016010401      zp1/data01@2016010402   124825285344  
incremental     2016010402      zp1/data01@2016010403   83633656888  
size    208458942232  
```  
每个快照占用的空间  
```  
#zfs list -t snapshot  
NAME                    USED  AVAIL  REFER  MOUNTPOINT  
zp1/data01@2016010401   121G      -  1.01T  -  
zp1/data01@2016010402  20.9G      -  1.01T  -  
zp1/data01@2016010403   200K      -  1.01T  -  
#df -h  
Filesystem            Size  Used Avail Use% Mounted on  
zp1/data01            4.9T  1.1T  3.9T  21% /data01  
```  
删掉这三个快照，能回收140G空间.  
```  
#zfs destroy zp1/data01@2016010401  
#zfs destroy zp1/data01@2016010402  
#zfs destroy zp1/data01@2016010403  
#df -h  
Filesystem            Size  Used Avail Use% Mounted on  
zp1/data01            5.1T  1.1T  4.1T  20% /data01  
```  
由于ZFS的快照是COW的，我们前面的测试涉及的块变更范围是1TB，两次600秒的压测，数据块变更的范围是80GB和120GB。  
如果将活跃数据降低，快照也会变小，如下：  
把活跃数据降低到1GB，再次测试。  
## 测试3  
```  
pgbench -i -s 100  digoal  
#zfs snapshot zp1/data01@2016010403  
```  
这次测试，TPS达到了4.57w/s，但是影响的块在1GB的范围内快照却小了很多。  
```  
pgbench -M prepared -n -r -P 1 -c 48 -j 48 -T 600  digoal  
transaction type: TPC-B (sort of)  
scaling factor: 100  
query mode: prepared  
number of clients: 48  
number of threads: 48  
duration: 600 s  
number of transactions actually processed: 27425009  
latency average: 1.048 ms  
latency stddev: 0.530 ms  
tps = 45704.065177 (including connections establishing)  
tps = 45706.628863 (excluding connections establishing)  
statement latencies in milliseconds:  
        0.003538        \set nbranches 1 * :scale  