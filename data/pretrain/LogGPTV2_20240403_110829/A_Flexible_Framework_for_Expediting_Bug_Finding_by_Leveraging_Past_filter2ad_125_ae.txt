larger ratio in the violin plots means that our approach finds crashes quicker. Statistical significance via a pairwise two-tailed
test is presented in Tables 9-11 in Appendix A.2.
Fuzzing: The art and science of fuzzing has witnessed explosive
growth [3, 4, 6, 27, 41–43, 53, 60], driven in part by the booming
software security market. Many approaches [4, 43] try to explore
low-frequency paths in order to reach bugs hidden inside less ex-
plored paths. Vuzzer [43], for example, uses control and data-flow
features to prioritize deep and less frequently explored code paths.
It performs taint analysis to infer the data types at certain off-
sets in the input, and then uses that knowledge to mutate inputs.
CollAFL [16] prioritizes input selection based on more untouched
branches (to increase the coverage) as well as more memory ac-
cess operations (to find memory corruptions). Similarly, Angora [7]
seeks to increase branch coverage by solving path constraints using
context-sensitive branch count and byte-level taint tracking. To
distinguish the executions of the same branch in different contexts,
Angora appends context to the branch IDs to explore paths more
pervasively. By tracking which input bytes flow into each path
constraint, the approach mutates only these bytes instead of the
entire input.
Li et al. [25] applied a static approach on known vulnerable
programs to extract basic block information, comprising the number
of call instruction, operand types and string types. In some sense,
these attributes can be viewed as characterizing program behavior,
albeit not at the architectural level. Based on these attributes, a deep
learning model is built wherein scores are assigned to basic blocks
traversed by the program. By calculating scores for basic blocks, Li
et al. [25] infers the fitness of an input for mutation during fuzzing.
We infer similar control flow and data flow behavior information
using performance counter events but with low overhead.
Concurrent to our work, Österlund et al. [39] proposed a sanitizer-
based approach to direct fuzzing towards triggering sanitizer checks
to find bugs faster. At a high-level, they also prioritize paths by
steering the program towards locations that are more likely to
have bugs. Unlike ours, their approach is based on sanitizer checks,
which may miss certain bug types e.g., logical errors. Moreover, we
implement a feedback mechanism to actively guide fuzzing and
switch between multiple strategies at runtime.
A handful of works [44, 53, 57] have examined what scheduling
algorithms produce the best results for seed selection. Rebert et al.
[44] formalize the notion of ex post facto optimality seed selection,
and provide evidence-driven techniques for identifying the quality
of a seed selection strategy compared to an optimal solution. Unlike
our work, these proposals focus on ways to measure the optimal
case for bugs found with a particular subset of seeds or to find a
“good” set of seeds that can be reused from one application to an-
other. Overall, these works show that the choice of seed scheduling
algorithm can significantly impact the success of fuzzing campaigns.
P0P1P2P3P4P5P6P70204060Ratio of time-to-crashBase-vs-1-aP0P1P2P3P4P5P6P70102030405060Base-vs-2-aP0P1P2P3P4P5P6P70510152025Base-vs-2-bP0P1P2P3P4P5P6P7051015202530Base-vs-3-aP0P1P2P3P4P5P6P7010203040Base-vs-3-bP0P1P2P3P4P5P6P7010203040Ratio of time-to-crashBase-vs-1-aP0P1P2P3P4P5P6P701020304050Base-vs-2-aP0P1P2P3P4P5P6P70102030Base-vs-2-bP0P1P2P3P4P5P6P7010203040Base-vs-3-aP0P1P2P3P4P5P6P7010203040Base-vs-3-bP0P1P2P3P4P5P6P701020304050Ratio of time-to-crashBase-vs-1-aP0P1P2P3P4P5P6P70204060Base-vs-2-aP0P1P2P3P4P5P6P7010203040Base-vs-2-bP0P1P2P3P4P5P6P7051015202530Base-vs-3-aP0P1P2P3P4P5P6P70204060Base-vs-3-bA Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 5: Performance of OmniFuzz vs base measured by consistency of finding bugs (over 6 runs)
The strategies we apply to accept or reject inputs to be queued for
mutation are complementary to the scheduling algorithms.
Hardware assistance: Lastly, there is a growing body of research
on hardware-assisted fuzzing [10, 15, 47, 50]. These approaches use
the processor trace feature to gather information for gauging pro-
gram coverage. Although these approaches leverage processor trace
facilities to efficiently collect an execution trace, the underlying
coverage-guiding principle is similar to that of AFL.
8 CONCLUSION
We demonstrate inefficiencies in contemporary coverage-guided
fuzzers, due principally to their equal prioritization of all program
paths. To address the inefficiencies, we propose a framework called
OmniFuzz that incorporates on-the-fly crash deduplication as a
feedback mechanism to coax the fuzzer to change course when
no unique crashes are obtained for some time. A unique aspect
of OmniFuzz is its use of performance counter data to derive in-
formation that can be used as a coverage metric to guide input
selection. Armed with these capabilities, we show how one can
devise a multitude of strategies to guide a fuzzer toward finding
similar or different bugs from those discovered in the past. These
improvements can be integrated with most of contemporary fuzzers
as they are not tied to a particular architecture. Our experimental
results show that OmniFuzz can find more unique bugs, and can
also find bugs significantly faster than the base fuzzer it augments
(e.g., AFL, MOpt, Fairfuzz). Taken as a whole, our experiments aptly
demonstrate that our vulnerability-aware selection of coverage
metrics, coupled with our on-the-fly deduplication technique, of-
fers an expedient and comprehensive solution for improving the
performance of a base fuzzer.
ACKNOWLEDGMENTS
We thank Prof. Yousra Aafer and the anonymous reviewers for their
suggestions on how to improve the paper. We also thank Murray
Anderegg for his assistance with deploying the infrastructure used
in this study. This work was supported in part by the Department
of Defense (DoD) under award FA8750-17-C-0016 and the National
Science Foundation (NSF) under award CNS-1749895. Any opinions,
ACSAC2020,December7–11,2020,Austin,USASanjeevDas,KedrianJames,JanWerner,ManosAntonakakis,MichalisPolychronakis,andFabianMonroseTable5:PerformanceofOmniFuzzvsbasemeasuredbyconsistencyoffindingbugs(over6runs)ProgramsBugIDAFL1-a2-a2-b3-a3-bMOpt1-a2-a2-b3-a3-bFairfuzz1-a2-a2-b3-a3-blibarchive16○✓✓○✓6○○✓✓✓2○○○○○26○○✓○✓6✓✓○✓○1○○○○✓32○○○○○0○○○○○6○○○○○40✓✓✓✓✓0✓✓✓✓✓0✓✓✓✓✓50✓✓✓✓✓0○○○○○0✓✓✓✓✓libjpeg65○○○○○6✓✓✓✓✓5○○○○○75✓○○○○6✓○✓✓✓0✓○○○○83○✓✓○✓0✓✓✓✓✓0✓○○○○93○✓✓✓✓0✓✓✓✓✓0✓○○○○103○✓✓○✓0✓✓✓✓✓0✓○○○○112✓○○○○0✓✓✓✓✓0✓○○○○125✓○○✓○6✓○✓○✓0✓○○○○135○○○○✓5○○○○✓0✓○✓✓○140○○○○○3○○○○○0✓○✓○○libplist156✓✓✓✓✓6✓✓✓✓○6✓✓✓✓○166✓✓✓✓✓6✓✓✓✓○6✓✓✓✓○libpng173✓✓✓✓✓0✓✓✓✓✓0✓✓✓✓✓libxml2181○○○○○0✓✓○✓○3○✓✓○✓190✓✓✓○✓0✓✓✓✓✓0✓✓✓✓✓200○○○○✓0○○○○○5○○○○○214○○○○○0✓✓✓✓○4○○○○○pcre223○✓✓✓✓1○○○○○0✓✓✓✓✓230✓✓✓✓✓1○○○✓✓0✓✓✓✓✓tiff245○○○○✓2○✓○✓✓6✓○○○○256✓✓✓✓✓2○✓○✓✓6✓○○○○265○○✓○✓2○✓○✓✓6✓○○○○275○○○○✓2○✓○✓✓6✓○○○○285○○○○✓2○✓○○✓6✓○○○○yaml296✓✓✓✓✓6✓✓✓✓✓6✓✓✓✓✓Thenumberoftimesabugisfoundbyanapproachispresentedwithrespecttothebasefuzzerandisshownas✓todenotesameasthebasefuzzer,○morethanthebasefuzzer,and○fewerthanthebasefuzzer.Thatsaid,itispossiblethattechniquesfromtransferlearning[18]andfew-shotlearning[55]canbeusedtobuildmoresophisticatedmodelswhenasufficientnumberofqualityinputsarenotread-ilyavailable.Moreover,wearenotarguingthattheapproachwetakeusingmulti-layerperceptronstobuildourmodelsisthebestchoice.Asstatedearlier,wechosethatsolutionbecauseitofferedsignificantoperationalbenefits.Secondly,thoughwemakenoas-sumptionsabouttheinputschedulingalgorithmused,wedonotstudyhowtheguidancewegiveduringtheinputselectionprocesscouldimpactoptimality.Thetheoreticalframeworksandformal-izationsbyRebertetal.[44]andHayesetal.[21]couldhelpinthatregard.Lastly,themodelswebuildmaynotberobustagainstanti-fuzzingtechniques[19,22]thattrytoimpedeautomatedbugfinding.Ourdeduplicationtechniqueis,atpresent,notapplicabletofuzzingatthebinary-levelasourcurrentinstantiationmandatesthatwehaveaccesstosourcecodeofthetargetprogram,andthatitbecompiledwithdebuggingsymbolsandnooptimizations.Whilethisisnotanissueforopen-sourcesoftware,closed-sourceapplicationsarealsosubjecttofuzzing.7RelatedWorkDeduplication:Todate,bothdeduplicationandrootcauseanalysishavebeenactiveareasofresearch.Fromanindustrystandpoint,stackbacktracehashingandedgecoveragearethemostcommonapproachestodeduplication[13,23,28].However,theseapproachessufferfromeitherover-approximationorunder-approximation[23,28].Toaddressthoselimitations,severalaca-demicsolutionshavebeenproposed.Forexample,Linetal.[26]usedstaticanddynamicanalysisatthesourcecodeleveltodetectanddeterminetherootcauseofout-of-boundsvulnerabilities.Cuietal.[12]proposedtodeduplicatecrashesinproductionsystemsbyreconstructingdataflowfromacoredump,andperformingback-wardanalysisfromthecrashpoint.Crashesarededuplicatedbasedonthefirstfunctionfromwhichthebadvaluethatcausedthecrashwasderived.Xuetal.[58]proposedtoimproverootcauseanalysiswhenacoredumpcontainscorrupteddata(e.g.,duetomemorycorruptionvulnerabilities).Xuetal.[59]laterproposedanapproach(andextensions[36])thatusestheIntelprocessortracefeatureandacoredumptoperformofflinebinaryanalysisACSAC 2020, December 7–11, 2020, Austin, USA
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian Monrose
Table 6: Portfolio mode consistency (over 18 runs)
Table 7: List of new bugs discovered by our approach
Versions
CVEs/Bugs
Bug details
Programs
libjpeg-turbo
(cjpeg)
libarchive
(bsdtar)
2.0.4
3.4.0,
3.4.1dev
3.4.1dev
pcre
(pcre2test)
10.34-RC1,
10.33
CVE-2020-13790
Heap-based buffer-over-read in get_rgb_row()
CVE-2019-19221
Bug 1298
Bug-2479
Bug-2480
Bug-2481
Bug-2482
Bug-2483
Bug-2484
Out-of-bounds read in
archive_wstring_append_from_mbs()
Out-of-bounds write in
archive_string_append_from_wcs()
Heap overflow in GETCHARINC()
Heap overflow in GETCHARLEN()
Heap overflow in GETCHARLENTEST()
Heap overflow in GETCHARINCTEST()
Out-of-bounds read in internal_dfa_match()
Stack-overflow in internal_dfa_match()
findings, and conclusions expressed herein are those of the authors
and do not necessarily reflect the views of the DoD or NSF.
REFERENCES
[1] Tim Blazytko, Moritz Schlögel, Cornelius Aschermann, Ali Abbasi, Joel Frank,
Simon Wörner, and Thorsten Holz. 2020. AURORA: Statistical Crash Analysis for
Automated Root Cause Explanation. In USENIX Security Symposium. 235–252.
[2] Marcel Böhme, Valentin Manès, and Sang Kil Cha. 2020. Boosting fuzzer effi-
ciency: An information theoretic perspective. In Symposium on the Foundations
of Software Engineering (ESEC/FSE). 1–11.
[3] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoud-
hury. 2017. Directed Greybox Fuzzing. In ACM Conference on Computer and
Communications Security. 2329–2344.
[4] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
Based Greybox Fuzzing as Markov Chain. In ACM Conference on Computer and
Communications Security. 1032–1043.
[5] Augusto Born de Oliveira. 2015. Measuring and Predicting Computer Software
Performance: Tools and Approaches. http://hdl.handle.net/10012/9259
[6] Hongxu Chen, Yinxing Xue, Yuekang Li, Bihuan Chen, Xiaofei Xie, Xiuheng Wu,
and Yang Liu. 2018. Hawkeye: Towards a Desired Directed Grey-box Fuzzer. In
ACM Conference on Computer and Communications Security. 2095–2108.
[7] Peng Chen and Hao Chen. 2018. Angora: Efficient fuzzing by principled search.
In IEEE Symposium on Security & Privacy. 711–725.
[8] Yue Chen, Mustakimur Khandaker, and Zhi Wang. 2017. Pinpointing Vulner-
abilities. In ACM Asia Conference on Computer and Communications Security.
334–345.
[9] Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong Zhang,
Long Lu, et al. 2020. SAVIOR: Towards Bug-Driven Hybrid Testing. In IEEE
Symposium on Security & Privacy.
[10] Yaohui Chen, Dongliang Mu, Jun Xu, Zhichuang Sun, Wenbo Shen, Xinyu Xing,
Long Lu, and Bing Mao. 2019. PTrix: Efficient Hardware-assisted Fuzzing for
COTS Binary. In ACM Asia Conference on Computer and Communications Security.
ACM, 633–645.
[11] Weidong Cui, Xinyang Ge, Baris Kasikci, Ben Niu, Upamanyu Sharma, Ruoyu
Wang, and Insu Yun. 2018. REPT: Reverse Debugging of Failures in Deployed
Software. In USENIX Symposium on Operating Systems Design and Implementation.
17–32.
[12] Weidong Cui, Marcus Peinado, Sang Kil Cha, Yanick Fratantonio, and Vasileios P
Kemerlis. 2016. Retracer: Triaging Crashes by Reverse Execution From Partial
Memory Dumps. In IEEE/ACM International Conference on Software Engineering.
820–831.
[13] Yingnong Dang, Rongxin Wu, Hongyu Zhang, Dongmei Zhang, and Peter Nobel.
2012. Rebucket: A method for clustering duplicate crash reports based on call
stack similarity. In IEEE/ACM International Conference on Software Engineering.
1084–1093.
[14] Sanjeev Das, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian
Monrose. 2019. SoK: The Challenges, Pitfalls, and Perils of Using Hardware
Performance Counters for Security. In IEEE Symposium on Security & Privacy.
20–38.
[15] Leila Delshadtehrani, Sadullah Canakci, Boyou Zhou, Schuyler Eldridge, Ajay
Joshi, and Manuel Egele. 2020. PHMon: A Programmable Hardware Monitor and
Its Security Use Cases. In USENIX Security Symposium.
[16] Shuitao Gan, Chao Zhang, Xiaojun Qin, Xuwen Tu, Kang Li, Zhongyu Pei, and
Zuoning Chen. 2018. CollAFL: Path Sensitive Fuzzing. In IEEE Symposium on
Security & Privacy. 679–696.
[17] Patrice Godefroid. 2020. Fuzzing: Hack, Art, and Science. In Communications of
[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2019. Deep Learning. MIT
the ACM, Vol. 63. 70–76.
Press.
[19] Emre Güler, Cornelius Aschermann, Ali Abbasi, and Thorsten Holz. 2019. Anti-
Fuzz: Impeding Fuzzing Audits of Binary Executables. In USENIX Security Sym-
posium. 1931–1947.
[20] Mark Andrew Hall. 1999. Correlation-based Feature Selection for Machine
Learning. (1999).
[21] Liam Hayes, Hendra Gunadi, Adrian Herrera, Jonathon Milford, Shane Ma-
grath, Maggi Sebastian, Michael Norrish, and Antony L Hosking. 2019. Moon-
Light: Effective Fuzzing with Near-Optimal Corpus Distillation. arXiv preprint
arXiv:1905.13055 (2019).
[22] Jinho Jung, Hong Hu, David Solodukhin, Daniel Pagan, Kyu Hyung Lee, and
Taesoo Kim. 2019. Fuzzification: Anti-Fuzzing Techniques. In USENIX Security
Symposium. 1913–1930.
[23] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018.
Evaluating Fuzz Testing. In ACM Conference on Computer and Communications
Security. 2123–2138.
[24] Caroline Lemieux and Koushik Sen. 2018. FairFuzz: A Targeted Mutation Strat-
egy for Increasing Greybox Fuzz Testing Coverage. In ACM/IEEE International
Conference on Automated Software Engineering.
[25] Yuwei Li, Shouling Ji, Chenyang Lv, Yuan Chen, Jianhai Chen, Qinchen Gu, and
Chunming Wu. 2019. V-Fuzz: Vulnerability-Oriented Evolutionary Fuzzing. arXiv
preprint arXiv:1901.01142.
[26] Zhiqiang Lin, Xuxian Jiang, Dongyan Xu, Bing Mao, and Li Xie. 2007. AutoPaG:
Towards Automated Software Patch Generation with Source Code Root Cause
Identification and Repair. In ACM Asia Conference on Computer and Communica-
tions Security. 329–340.
[27] Chenyang Lyu, Shouling Ji, Chao Zhang, Yuwei Li, Wei-Han Lee, Yu Song, and
Raheem Beyah. 2019. MOPT: Optimized Mutation Scheduling for Fuzzers. In
USENIX Security Symposium. 1949–1966.
[28] Valentin Jean Marie Manès, HyungSeok Han, Choongwoo Han, Sang Kil Cha,
Manuel Egele, Edward J Schwartz, and Maverick Woo. 2019. The Art, Science, and
Engineering of Fuzzing: A Survey. IEEE Transactions on Software Engineering.
[29] Timothy Merrifield, Joseph Devietti, and Jakob Eriksson. 2015. High-Performance
Determinism with Total Store Order Consistency. In European Conference on
Computer Systems. 1–13.
[30] Barton P Miller, Louis Fredriksen, and Bryan So. 1990. An Empirical Study of the
Reliability of UNIX Utilities. In Communications of the ACM, Vol. 33. ACM New
York, NY, USA, 32–44.
[31] Charlie Miller. 2010. Babysitting an Army of Monkeys. In CanSecWest.