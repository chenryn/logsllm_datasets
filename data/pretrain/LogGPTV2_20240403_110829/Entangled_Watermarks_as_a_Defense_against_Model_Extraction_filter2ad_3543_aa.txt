title:Entangled Watermarks as a Defense against Model Extraction
author:Hengrui Jia and
Christopher A. Choquette-Choo and
Varun Chandrasekaran and
Nicolas Papernot
Entangled Watermarks as a Defense against 
Model Extraction
Hengrui Jia and Christopher A. Choquette-Choo, University of Toronto and 
Vector Institute; Varun Chandrasekaran, University of Wisconsin-Madison; 
Nicolas Papernot, University of Toronto and Vector Institute
https://www.usenix.org/conference/usenixsecurity21/presentation/jia
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Entangled Watermarks as a Defense against Model Extraction
Hengrui Jia†, Christopher A. Choquette-Choo†, Varun Chandrasekaran*, Nicolas Papernot†
†University of Toronto and Vector Institute, * University of Wisconsin-Madison
Abstract
Machine learning involves expensive data collection and
training procedures. Model owners may be concerned that
valuable intellectual property can be leaked if adversaries
mount model extraction attacks. As it is difﬁcult to defend
against model extraction without sacriﬁcing signiﬁcant predic-
tion accuracy, watermarking instead leverages unused model
capacity to have the model overﬁt to outlier input-output pairs.
Such pairs are watermarks, which are not sampled from the
task distribution and are only known to the defender. The de-
fender then demonstrates knowledge of the input-output pairs
to claim ownership of the model at inference. The effective-
ness of watermarks remains limited because they are distinct
from the task distribution and can thus be easily removed
through compression or other forms of knowledge transfer.
We introduce Entangled Watermarking Embeddings
(EWE). Our approach encourages the model to learn features
for classifying data that is sampled from the task distribution
and data that encodes watermarks. An adversary attempting
to remove watermarks that are entangled with legitimate data
is also forced to sacriﬁce performance on legitimate data.
Experiments on MNIST, Fashion-MNIST, CIFAR-10, and
Speech Commands validate that the defender can claim model
ownership with 95% conﬁdence with less than 100 queries
to the stolen copy, at a modest cost below 0.81 percentage
points on average in the defended model’s performance.
1 Introduction
Costs associated with machine learning (ML) are high. This
is true in particular when large training sets need to be col-
lected [16] or the parameters of complex models tuned [49].
Therefore, models being deployed for inference constitute
valuable intellectual property that need to be protected. A
good example of a pervasive deployment of ML is automatic
speech recognition [18], which forms the basis for personal
assistants in ecosystems created by Amazon, Apple, Google,
and Microsoft. However, deploying models to make predic-
tions creates an attack vector which adversaries can exploit to
mount model extraction attacks [3, 8, 35, 40, 41, 43, 51].
Techniques for model extraction typically require that the
adversary query a victim model with inputs of their choice—
analogous to chosen-plaintext attacks in cryptography. The
adversary uses the victim model to label a substitute dataset.
One form of extraction involves using the substitute dataset
to train a substitute model, which is a stolen copy of the
victim model [41,43]. Preventing model extraction is difﬁcult
without sacriﬁcing performance for legitimate users [2, 5,
29, 51]: queries made by attackers and benign users may be
sampled from the same task distribution.
One emerging defense proposal is to extend the concept
of watermarking [22] to ML [6]. The defender purposely
introduces outlier input-output pairs (x,y) only known to
them in the model’s training set—analogous to poisoning
or backdoor attacks [1]. To claim ownership of the model
f , the defender demonstrates that they can query the model
on these speciﬁc inputs ˜x and have knowledge of the (poten-
tially) surprising prediction f ( ˜x) = ˜y returned by the model.
Watermarking techniques exploit the large capacity in mod-
ern architectures [1] to learn watermarks without sacriﬁcing
performance when classifying data from the task distribution.
Naive watermarking can be defeated by an adaptive at-
tacker because the watermarks are outliers to the task distribu-
tion. As long as the adversary queries the watermarked model
only on inputs that are sampled from the task distribution, the
stolen model will only retain the victim model’s decision sur-
face relevant to the task distribution, and therefore ignore the
decision surface learned relevant to watermarking. In other
words, the reason why watermarking can be performed with
limited impact on the model’s accuracy is the reason why wa-
termarks can easily be removed by an adversary. Put another
way, watermarked models roughly split their parameter set
into two subsets, the ﬁrst encodes the task distribution while
the second overﬁts to the outliers (i.e., watermarks).
In this paper, we propose a technique that addresses this
fundamental limitation of watermarking. Entangled Water-
mark Embedding (EWE) encourages a model to extract fea-
USENIX Association
30th USENIX Security Symposium    1937
tures that are jointly useful to (a) learn how to classify data
from the task distribution and (b) predict the defender’s ex-
pected output on watermarks. Our key insight is to leverage
the soft nearest neighbor loss [12] to entangle representations
extracted from training data and watermarks. By entangle-
ment, we mean that the model represents both types of data
similarly. Entangling produces models that use the same sub-
set of parameters to recognize training data and watermarks.
Hence, it is difﬁcult for an adversary to extract the model
without its watermarks, even if the adversary queries models
with samples only from the task distribution to avoid trigger-
ing watermarks (e.g., the adversary avoids out-of-distribution
inputs like random queries). The adversary is forced to learn
how to reproduce the defender’s chosen output on watermarks.
An attempt to remove watermarks would also have to harm
the stolen substitute classiﬁer’s generalization performance
on the task distribution, which would defeat the purpose of
model extraction (i.e., steal a well-performing model).
We evaluate1 the approach on four vision datasets–
MNIST [28], Fashion MNIST [55], CIFAR-10, and CIFAR-
100 [26] as well as an audio dataset—Google Speech Com-
mand [54]. We demonstrate that our approach is able to wa-
termark models at moderate costs to utility—below 0.81 per-
centage points on average on the datasets considered. Unlike
prior approaches we compare against, our watermarked clas-
siﬁers are robust to model extraction attacks. Stolen copies
retain the defender’s expected output on > 38% (in average)
of entangled watermarks (see Table 1, where the baseline
achieves  1 and 0 otherwise.
Figure 1: We construct a neural network to show how wa-
termarks behave like trapdoor functions. When the model
learns independent task and watermark distributions, this is
true despite both distributions being modeled with the same
neurons. Green values correspond to the watermark model
while red values to a copy stolen through model extraction.
Inputs x1 and x2, are sampled from two independent uniform
distributions U(0,1). We watermark this model to output 1 if
x2 = −1 regardless of x1. One could model this function as a
feed-forward DNN shown in Figure 1. A sigmoid activation σ
is utilized as the ultimate layer to obtain the following model:
ˆy = σ(w1 · R(x1 + b1) + w2 · R(x2 + b2) + w3 · R(x2 + b3) + b4 − 1)
where R(x) = max(0,x) denotes a ReLU activation. We in-
stantiate this model with the following parameter values:
y = σ(1· R(x1) + 2· R(x2)− 1· R(x2 + 2) + 2− 1)
y = σ(0.96· R(x1) + 0.54· R(x2) + 0.54· R(x2)− 1)
We chose parameter values to illustrate the following set-
ting: (a) the model is accurate on both the task distribution and
watermark, and (b) the neuron used to encode the watermark
is also used by the task distribution. This enables us to show
how the watermark is not extracted by the adversary, even
though it is encoded by a neuron that is also used to classify
inputs from the task distribution. As the adversary attempts to
extract the model, they are unlikely to trigger the watermark
by setting x2 = −1 if they sample inputs from U(0,1) i.e.,
the task distribution. After training the substitute model with
inputs from the task distribution and labels (which are predic-
tions) obtained from the victim model, the decision function
learned by the adversary is:
This function can be written as y = σ(0.96x1 + 1.08x2 − 1)
since x1,x2 ∼ U(0,1). This is very similar to our objective
function, y = σ(x1 + x2 − 1), and has high utility for the ad-
versary. However, if the out-of-distribution (OOD) input x2 is
-1, the largest value of the function (obtained when x1 = 1) is