82.5 ± 0.2
82.4 ± 0.2
83.1 ± 0.2
82.9 ± 0.1
84.0 ± 1.3
84.8 ± 1.6
92.4 ± 0.2
92.9 ± 0.1
68.1 ± 4.1
67.2 ± 6.7
63.5 ± 4.6
85.9 ± 1.1
than the multi-bit mechanism. This result shows that node features
are also effective in addition to the graph structure, and we cannot
ignore their utility.
Analyzing the effect of KProp. In this experiment, we inves-
tigate whether the KProp layer can effectively gain performance
boost, for either node feature or labels. For this purpose, we varied
the KProp’s step parameters 𝐾𝑥 and 𝐾𝑦 both within {0, 2, 4, 8, 16},
and trained the LPGNN model under varying privacy budget, whose
result is depicted in Figure 4. In the top row of the figure, we
change 𝐾𝑥 ∈ {0, 2, 4, 8, 16} and 𝜖𝑥 ∈ {0.01, 0.1, 1}, while fixing 𝜖𝑦=1
and selecting the best values for 𝐾𝑦 based on the validation loss.
Conversely, in the bottom row, we vary 𝐾𝑦 ∈ {0, 2, 4, 8, 16} and
𝜖𝑦 ∈ {0.5, 1, 2}, and set the best 𝐾𝑥 at 𝜖𝑥 = 1.
We observe that in all cases, both the feature and the label KProp
layers are effective and can significantly boost the accuracy of the
LPGNN depending on the dataset and the value of the correspond-
ing privacy budget. Based on the results, the accuracy of LPGNN
rises to an extent by increasing the step parameters, which shows
that the model can benefit from larger population sizes to have
a better estimation for both graph convolution and labels. Fur-
thermore, we see that the maximum performance gain is different
across the datasets and privacy budgets. As the estimates become
more accurate due to an increase in the privacy budget, we see that
KProp becomes less effective, mainly due to over-smoothing. But at
lower privacy budgets, KProp usually achieves the highest relative
accuracy gain.
In the case of feature KProp, the performance is also correlated
to the average node degree. For instance, at 𝜖𝑥 = 0.01, on the social
network datasets with a higher average degree, the accuracy gain
is around 6% and 10% on Facebook and LastFM, respectively, while
Feature
Ones
Ohd
Rnd
Mbm (𝜖𝑥 = 0.01)
Mbm (𝜖𝑥 = 0.1)
Mbm (𝜖𝑥 = 1)
Cora
22.6 ± 5.0
44.4 ± 3.5
26.4 ± 3.0
63.0 ± 4.1
62.4 ± 2.0
69.3 ± 1.2
Pubmed
38.9 ± 0.4
52.5 ± 5.7
56.0 ± 1.3
78.9 ± 0.2
76.5 ± 0.4
74.9 ± 0.3
Facebook
29.0 ± 1.4
77.2 ± 0.3
35.2 ± 5.6
85.0 ± 0.4
85.1 ± 0.2
84.9 ± 0.2
LastFm
19.6 ± 1.8
66.4 ± 1.6
32.3 ± 6.3
76.9 ± 4.3
81.2 ± 1.3
82.1 ± 1.0
on lower-degree citation networks, it is over 20% on both Cora and
Pubmed, which suggests that lower-degree datasets can benefit
more from KProp. Furthermore, the optimal step parameter 𝐾𝑥 that
yields the best result also depends on the average degree of the
graph. For example, we see that the trend is more or less increasing
until the end for citation networks with a lower average degree.
In contrast, the accuracy begins to fall over higher-degree social
networks after 𝐾𝑥 = 4. This means that in lower-degree datasets,
KProp requires more steps to reach the sufficient number of nodes
for aggregation, while on higher-degree graphs, it can achieve this
number in fewer steps.
Regarding the label KProp, the performance growth depends
not only on the average degree, but also on the number of classes,
which can significantly affect the accuracy of randomized response.
For instance, despite its high average degree, KProp could increase
the accuracy on LastFM with 10 classes by over 20% at 𝜖𝑦 = 0.5,
while on the other high-degree dataset, Facebook, which has 4
classes, this number is at most 5%. Low-degree datasets still can
benefit much from label KProp, with both Cora and Pubmed achiev-
ing a maximum of 30% accuracy boost at 𝜖𝑦 = 1 and 𝜖𝑦 = 0.5,
respectively.
Investigating the Drop algorithm. In this final experiment,
we investigate how using the Drop algorithm can affect the per-
formance of LPGNN under different feature privacy budgets 𝜖𝑦
ranging with {0.5, 1.0, 2.0}, fixing 𝜖𝑥 = 1. We compare the result
of Drop with the classic cross-entropy, where we directly train the
GNN with noisy labels. We also compare with the forward correc-
tion method [39], described in Section 3. Note that since our method
does not rely on any clean validation data and is tailored for GNNs,
it is not directly comparable to other general methods for deep
learning with noisy labels that do not have these two characteris-
tics. Table 4 presents the accuracy of different learning algorithms
for the three label privacy budgets. It is evident that our Drop al-
gorithm substantially outperforms the other two methods and can
remarkably increase the final accuracy compared to the baselines,
especially at high-privacy regimes with severe label noise, and also
on datasets like LastFM with a high number of classes. Specifically,
at 𝜖𝑦 = 0.5, using Drop improves the accuracy of LPGNN by over
24%, 31%, 6%, and 25% on Cora, Pubmed, Facebook, and LastFM,
respectively, compared to the forward correction method. As 𝜖𝑦
increases to 2, the labels become less noisy, so the accuracy differ-
ence between Drop and the other baselines shrinks. Still, Drop can
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2139𝜖𝑥 = 0.01
𝜖𝑥 = 0.1
𝜖𝑥 = 1
𝜖𝑥 = 0.01
𝜖𝑥 = 0.1
𝜖𝑥 = 1
𝜖𝑥 = 0.01
𝜖𝑥 = 0.1
𝜖𝑥 = 1
𝜖𝑥 = 0.01
𝜖𝑥 = 0.1
𝜖𝑥 = 1
70
60
50
40
80
60
40
20
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
0
2
𝜖𝑦 = 0.5
8
4
𝐾𝑥
𝜖𝑦 = 1.0
16
𝜖𝑦 = 2.0
80
75
70
65
60
80
70
60
50
40
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
0
2
𝜖𝑦 = 0.5
8
4
𝐾𝑥
𝜖𝑦 = 1.0
16
𝜖𝑦 = 2.0
86
84
82
80
90
85
80
75
70
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
0
2
𝜖𝑦 = 0.5
8
4
𝐾𝑥
𝜖𝑦 = 1.0
16
𝜖𝑦 = 2.0
85
80
75
70
65
60
90
80
70
60
50
40
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
0
2
𝜖𝑦 = 0.5
8
4
𝐾𝑥
𝜖𝑦 = 1.0
16
𝜖𝑦 = 2.0
0
2
4
𝐾𝑦
8
16
(a) Cora
0
2
4
𝐾𝑦
8
16
(b) Pubmed
0
2
4
𝐾𝑦
8
16
(c) Facebook
0
2
4
𝐾𝑦
8
16
(d) LastFM
Figure 4: Effect of the KProp step parameter on the performance of LPGNN. The top row depicts the effect of feature KProp
with 𝜖𝑦 = 1. The bottom row shows the effect of label KProp with 𝜖𝑥 = 1. The y-axis is not set to zero to focus on the trends.
Table 4: Effect of Drop on the accuracy of LPGNN (𝜖𝑥 = 1)
Dataset
Cora
Pubmed
Facebook
LastFm
𝜖𝑦
0.5
1.0
2.0
0.5
1.0
2.0
0.5
1.0
2.0
0.5
1.0
2.0
Cross
Entropy
18.6 ± 1.3
25.5 ± 1.7
52.9 ± 2.1
37.1 ± 0.9
65.4 ± 0.6
80.5 ± 0.2
50.9 ± 4.2
55.2 ± 1.3
81.6 ± 1.2
21.1 ± 4.6