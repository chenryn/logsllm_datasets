dmax
˜dmax
G
A = (ai, j)
ai
Ri
f(cid:52)(G)
fk(cid:63)(G)
i-th user in V .
Maximum degree of G.
Upper-bound on dmax (used for projection).
Set of possible graphs on n users.
Adjacency matrix.
i-th row of A (i.e., neighbor list of vi).
Randomized algorithm on ai.
Number of triangles in G.
Number of k-stars in G.
3.4 Graph Statistics and Utility Metrics
Graph statistics. We consider a graph function that takes
a graph G ∈ G as input and outputs some graph statistics.
Speciﬁcally, let f(cid:52) : G → Z≥0 be a graph function that outputs
the number of triangles in G. For k ∈ N, let fk(cid:63) : G → Z≥0 be
a graph function that outputs the number of k-stars in G. For
example, if a graph G is as shown in Figure 1, then f(cid:52)(G) = 5,
f2(cid:63)(G) = 20, and f3(cid:63)(G) = 8. The clustering coefﬁcient can
also be calculated from f(cid:52)(G) and f2(cid:63)(G) as: 3 f(cid:52)(G)
f2(cid:63)(G) = 0.75.
Table 1 shows the basic notations used in this paper.
Utility metrics. We use the l2 loss (i.e., squared error) [32,
40, 57] and the relative error [12, 14, 61] as utility metrics.
Speciﬁcally, let ˆf (G) ∈ R be an estimate of graph statis-
tics f (G) ∈ R. Here f can be instantiated by f(cid:52) or fk(cid:63); i.e.,
ˆf(cid:52)(G) and ˆfk(cid:63)(G) are the estimates of f(cid:52)(G) and fk(cid:63)(G),
respectively. Let l2
2 be the l2 loss function, which maps the
estimate ˆf (G) and the true value f (G) to the l2 loss; i.e.,
2 ( ˆf (G), f (G)) = ( ˆf (G)− f (G))2. Note that when we use a
l2
randomized algorithm providing edge LDP (or edge central-
ized DP), ˆf (G) depends on the randomness in the algorithm.
In our theoretical analysis, we analyze the expectation of the
l2 loss over the randomness, as with [32, 40, 57].
When f (G) is large, the l2 loss can also be large. Thus in
our experiments, we also evaluate the relative error, along
with the l2 loss. The relative error is deﬁned as: | ˆf (G)− f (G)|
max{ f (G),η},
where η ∈ R≥0 is a very small positive value. Following the
convention [12, 14, 61], we set η = 0.001n for f(cid:52) and fk(cid:63).
4 Algorithms
In the local model, there are several ways to model how the
data collector interacts with the users [20,31,46]. The simplest
model would be to assume that the data collector sends a query
Ri to each user vi once, and then each user vi independently
sends an answer Ri(ai) to the data collector. In this model,
there is one-round interaction between each user and the data
USENIX Association
30th USENIX Security Symposium    987
collector. We call this the one-round LDP model. For example,
the RR for a neighbor list in Section 3.2 assumes this model.
However, in certain cases it may be possible for the data
collector to send a query to each user multiple times. This
could allow for more powerful queries that result in more
accurate subgraph counts [53] or more accurate synthetic
graphs [46]. We call this the multiple-rounds LDP model.
In Sections 4.1 and 4.2, we consider the problems of com-
puting fk(cid:63)(G) and f(cid:52)(G) for a graph G ∈ G in the one-round
LDP model. Our algorithms and bounds highlight limitations
of the one-round LDP model. Compared to the centralized
graph DP model, the one-round LDP model cannot compute
fk(cid:63)(G) as accurately. Furthermore, the algorithm for f(cid:52)(G)
does not perform well. In Section 4.3, we propose a more so-
phisticated algorithm for computing f(cid:52)(G) in the two-rounds
LDP model, and show that it provides much smaller expected
l2 loss than the algorithm in the one-round LDP model. In
Section 4.4, we show a general result about lower bounds on
the expected l2 loss of graph statistics in LDP. The proofs of
all statements in Section 4 are given in the full version [28].
4.1 One-Round Algorithms for k-Stars
Algorithm. We begin with the problem of computing fk(cid:63)(G)
in the one-round LDP model. For this model, we introduce a
simple algorithm using the Laplacian mechanism, and prove
that this algorithm can achieve order optimal expected l2 loss
among all one-round LDP algorithms.
Data: Graph G represented as neighbor lists a1, . . . ,an
∈ {0,1}n, privacy budget ε ∈ R≥0, ˜dmax ∈ Z≥0.
Result: Private estimate of fk(cid:63)(G).
1 ∆ ←(cid:0) ˜dmax
k−1
(cid:1);
2 for i = 1 to n do
3
ai ← GraphProjection(ai, ˜dmax);
/* di is a degree of user vi.
di ← ∑n
4
*/
(cid:1);
j=1 ai, j;
ri ←(cid:0)di
ˆri ← ri + Lap(cid:0) ∆
k
ε
release(ˆri);
(cid:1);
5
6
7
8 end
9 return ∑n
i=1 ˆri
Algorithm 1: LocalLapk(cid:63)
Algorithm 1 shows the one-round algorithm for k-stars.
It takes as input a graph G (represented as neighbor lists
a1, . . . ,an ∈ {0,1}n), the privacy budget ε, and a non-negative
integer ˜dmax ∈ Z≥0.
The parameter ˜dmax plays a role as an upper-bound on
the maximum degree dmax of G. Speciﬁcally, let di ∈ Z≥0
be the degree of user vi; i.e., the number of “1”s in her
neighbor list ai. In line 3, user vi uses a function (de-
noted by GraphProjection) that performs graph projec-
k−1
k−1
tion [16, 36, 48] for ai as follows. If di exceeds ˜dmax, it ran-
domly selects ˜dmax neighbors out of di neighbors; otherwise,
it uses ai as it is. This guarantees that each user’s degree never
exceeds ˜dmax; i.e., di ≤ ˜dmax after line 4.
After the graph projection, user vi counts the number of
k-stars ri ∈ Z≥0 of which she is a center (line 5), and adds the
Laplacian noise to ri (line 6). Here, since adding one edge
results in the increase of at most(cid:0) ˜dmax
of k-star counts for user vi is at most(cid:0) ˜dmax
(cid:1) k-stars, the sensitivity
(cid:1) (after graph pro-
ε ) to ri, where ∆ =(cid:0) ˜dmax
(cid:1)
jection). Therefore, we add Lap( ∆
k−1
and for b ∈ R≥0 Lap(b) is a random variable that represents
the Laplacian noise with mean 0 and scale b. The ﬁnal answer
of Algorithm 1 is simply the sum of all the noisy k-star counts.
We denote this algorithm by LocalLapk(cid:63).
The value of ˜dmax greatly affects the utility. If ˜dmax is too
large, a large amount of the Laplacian noise would be added.
If ˜dmax is too small, a great number of neighbors would be
reduced by graph projection. When we have some prior knowl-
edge about the maximum degree dmax, we can set ˜dmax to an
appropriate value. For example, the maximum number of con-
nections allowed on Facebook is 5000 [3]. In this case, we
can set ˜dmax = 5000, and then graph projection does nothing.
Given that the number of Facebook monthly active users is
over 2.7 billion [6], ˜dmax = 5000 is much smaller than n. For
another example, if we know that the degree is smaller than
1000 for most users, then we can set ˜dmax = 1000 and perform
graph projection for users whose degrees exceed ˜dmax.
In some applications, the data collector may not have such
prior knowledge about ˜dmax. In this case, we can privately
estimate dmax by allowing an additional round between each
user and the data collector, and use the private estimate of
dmax as ˜dmax. We describe how to privately estimate dmax with
edge LDP at the end of Section 4.1.
Theoretical properties. LocalLapk(cid:63) has the following guar-
antees:
.
max
ε2
(cid:17)
(cid:16) n ˜d2k−2
Theorem 1. LocalLapk(cid:63) provides ε-edge LDP.
Theorem 2. Let ˆfk(cid:63)(G,ε, ˜dmax) be the output of LocalLapk(cid:63).
Then, for all k ∈ N,ε ∈ R≥0, ˜dmax ∈ Z≥0, and G ∈ G such
˜dmax,
that the maximum degree dmax of G is at most
E[l2
2 ( ˆfk(cid:63)(G,ε, ˜dmax), fk(cid:63)(G))] = O
The factor of n in the expected l2 loss of LocalLapk(cid:63)
comes from the fact that we are adding the Laplacian noise
n times. In the centralized model, this factor of n is not
there, because the central data collector sees all k-stars; i.e.,
the data collector knows fk(cid:63)(G). The sensitivity of fk(cid:63) is
at most 2(cid:0) ˜dmax
ply adds the Laplacian noise Lap(2(cid:0) ˜dmax
outputs fk(cid:63)(G) + Lap(2(cid:0) ˜dmax
(cid:1) (after graph projection) under edge central-
(cid:1)/ε) to fk(cid:63)(G), and
(cid:1)/ε). We denote this algorithm
ized DP. Therefore, we can consider an algorithm that sim-
by CentralLapk(cid:63). Since the bias of the Laplacian noise is
k−1
k−1
k−1
988    30th USENIX Security Symposium
USENIX Association
(cid:16) ˜d2k−2
(cid:17)
0, CentralLapk(cid:63) attains the expected l2 loss (= variance) of
O
.
max
ε2
It seems impossible to avoid this factor of n in the one-
round LDP model, as the data collector will be dealing with n
independent answers to queries. Indeed, this is the case—we
prove that the expected l2 error of LocalLapk(cid:63) is order optimal
among all one-round LDP algorithms, and the one-round LDP
model cannot improve the factor of n.
ˆfk(cid:63)(G, ˜dmax,ε) be any one-round LDP
Corollary 1. Let
algorithm that computes fk(cid:63)(G) satisfying ε-edge LDP.
Then, for all k,n, ˜dmax ∈ N and ε ∈ R≥0 such that n
is even, there exists a set of graphs A ⊆ G on n
nodes such that the maximum degree of each G ∈ A is
2 ( ˆfk(cid:63)(G, ˜dmax,ε), fk(cid:63)(G))] ≥
at most ˜dmax, and 1|A| ∑G∈A E[l2
Ω
(cid:16) e2ε
(cid:17)
˜d2k−2
max n
.
(e2ε+1)2
This is a corollary of a more general result of Section 4.4.
Thus, any algorithm computing k-stars cannot avoid the fac-
tor of n in its l2
2 loss. k-stars is an example where the non-
interactive graph LDP model is strictly weaker than the cen-
tralized DP model.
Nevertheless, we note that LocalLapk(cid:63) can accurately cal-
culate fk(cid:63)(G) for a large number of users n. Speciﬁcally, the
relative error decreases with increase in n because LocalLapk(cid:63)
has a factor of n (not n2) in the expected l2 error, i.e.,
E[( ˆfk(cid:63)(G,ε, ˜dmax)− fk(cid:63)(G))2] = O(n) and fk(cid:63)(G)2 ≥ Ω(n2)
(when we ignore ˜dmax and ε). In our experiments, we show
that the relative error of LocalLapk(cid:63) is small when n is large.
Private calculation of dmax. By allowing an additional round