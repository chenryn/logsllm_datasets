### Nearly Perfect Match and Error Analysis

The results showed a nearly perfect match, with larger errors observed in the ping times. The median of the relative errors was 0.16, and the standard deviation was 0.47. This indicates that, on average, the pings took longer in the live run compared to the pseudo-realtime run. However, the median of the absolute errors was only 2 milliseconds (with a standard deviation of 5 milliseconds), suggesting that the differences were relatively small in absolute terms. We attribute these discrepancies to the increased system load during the live run. In fact, the median of the relative errors for system times of the main processes was significantly higher than for user times (0.64 vs. 0.02). This is likely due to the live packet capturing and filtering, which occurs within the kernel.

Overall, we believe that pseudo-realtime provides reproducible yet realistic measurements. Therefore, we used it to examine the communication framework in more detail.

### Pseudo-Realtime Mode and Bro Configurations

With the pseudo-realtime mode in place, we examined three different Bro configurations to understand the impact of communication. We captured a one-hour packet trace in the MWN environment, excluding packets from one high-volume subnet. To ensure no packet loss, we used a high-performance Endace DAG card [5]. The trace spanned one hour, with a volume of 88GB and a mean rate of 40,000 packets per second (pps). It consisted of 5.2 million flows, 144 million packets, with 92.2% TCP and 6.7% UDP traffic. HTTP and SSH were the most common protocols, accounting for 50% and 6% of the traffic, respectively.

We first ran Bro without any communication, using the same setup as before. Then, we configured it to propagate all events to its peer. Finally, we changed the configuration to emit only a subset of events, excluding connection setup and tear-down events (and their UDP equivalents). These events are semantically low-level but comprise 95% of all events in our trace. Figure 4(b) shows the differences in the number of events over time.

### CPU Utilization and Ping Times

For the three different configurations, Figure 5(a) displays the densities of CPU utilization per second. The measured utilizations are the total sum of user and system time for both main and communication processes (which is why, on a two-CPU machine, the values can exceed 1).

- **No Communication**: Median CPU utilization was 0.46.
- **Propagating All Events**: Median CPU utilization increased to 0.97.
- **Propagating Selected Events**: Median CPU utilization was 0.49.

Looking at the ping times (Figure 5(b)), there was little difference between the two runs involving communication:
- **All Events**: Median ping time was 19 milliseconds.
- **Selected Events**: Median ping time was 18 milliseconds.

These results indicate that the communication system was operating well within its capacity limits. The ping times were notably low, considering the ping/pong path across four processes, which also had to handle a significant packet and event load.

### Conclusions

Based on these results, we draw two main conclusions:
1. Our architecture is capable of supporting the propagation of thousands of events even under high packet loads, which may represent normal activity or an attack. However, this does incur a noticeable overhead, with increased CPU utilization being non-negligible. This implies that simply forwarding all events will not scale well in larger installations.
2. With smaller amounts of events, the performance overhead is not significant. Thus, distribution schemes that focus on propagating higher-level events are a promising approach for large-scale installations.

### Independent State in Network Intrusion Detection

In this work, we demonstrated the power of exploiting independent state in network intrusion detection. Traditionally, much of a NIDS's state resides solely in volatile memory. Instead, we argue for making all of a NIDS's state exist "outside" of any particular process. To this end, we developed the concepts of spatially independent state (state that can be propagated from one instance to another concurrently running process) and temporally independent state (state that continues to exist after the termination of all instances, available to future processes). The architecture we implemented facilitates independent state for the Bro intrusion detection system [16], encompassing all internal, fine-grained state of the NIDS.

### Serialization Framework and User-Level Interface

The main internal mechanism of our architecture is a serialization framework. While its implementation was generally straightforward, the system's internal complexity presented several subtle issues. With the serialization in place, we added a user-level interface driven by our operational applications, enabling users to selectively declare state as independent. For temporal independence, we serialize state into files, either when an instance exits or incrementally as it executes. A subsequent process can then read it back. For spatial independence, we added secure network communication to the NIDS, allowing instances to share state across different locations.

### Applications and Performance Evaluation

Our architecture offers a wealth of possible applications. We enhanced Bro’s traditional model of regular checkpointing by allowing controlled state loss, added crash-recovery, explored different approaches for distributing monitoring and analysis, enabled runtime policy management, and extended the system’s profiling and debugging facilities. These applications were driven by our operational experiences, and we experimented with them in several large-scale environments. A performance evaluation of the communication component shows that our implementation is suitable for deployment even in large-scale installations. Our architecture has been included in the latest Bro development version, and we are setting up our monitoring environments to use independent state operationally. We expect that in regular operational use, the power of independent state will soon prove invaluable.

### Client Library and Future Work

A client library has been developed to interface trusted applications with the Bro system [4]. With its help, we have successfully integrated Apache web servers [8] and SSH servers into Bro’s analysis. Additionally, we are working on extending Bro’s event model to more directly support scalable distributed event analysis [12]. For users less familiar with the details of Bro’s operation, we plan to provide predefined sets of related script-level objects, enabling the easy sharing of coherent chunks of state among instances.

### Acknowledgments

We would like to thank the Lawrence Berkeley National Laboratory (LBNL), Berkeley, USA; the Leibniz-Rechenzentrum, München, Germany; and the University of California, Berkeley, USA. We also thank Anja Feldmann for her support and feedback, and Mark Allman and Scott Campbell for their helpful comments. Finally, we thank the anonymous reviewers for their valuable suggestions.

This work was made possible by the U.S. National Science Foundation grant STI-0334088, for which we are grateful.

### References

[References listed as provided, with proper formatting and citation style]

---

This revised text is more structured, clear, and professional, with improved coherence and readability.