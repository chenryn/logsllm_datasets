the 
conventional  LRU  cache  replacement  policy,  different 
candidates  may  be  selected  for  replacement.  Thus  the 
private cache consistency will be violated. 
the  master-slave  pair.  According 
to 
A simple way to guarantee the master and the slave 
to make the same decision may be the replacement hints 
of the master core. However, because of the asynchronous 
execution  of  the  pair,  the  master’s  replacement  hint  is 
insufficient here for the slave to determine when to trigger 
the corresponding replacement action.  
the  master-slave  execution 
In  this  paper,  we  adopt  an  update-after-retirement-
assisted  LRU  (uar-LRU)  cache  replacement  policy.  The 
policy forces the master-slave private caches to make the 
identical replacement decision. As we know, the invariant 
within 
in-order 
instruction  retirement  sequences.  Therefore,  we  do  not 
refresh the  most-recently-used (MRU) timestamps of the 
accessed cache line at the time when the memory access 
operation  is  finished.  Instead,  we  delay  to  refresh  the 
timestamp until the memory instruction is retired. In this 
way we prevent the wrong path memory instructions from 
shuffling  the  MRU  sequences.  Since  the  private  caches 
maintain the same MRU sequences between the pair, the 
replacement  engine  will  select  the  identical  candidate  to 
evict.  Note  that  the  invariant  can  be  correctly  expressed 
by  the  retirement  order  of  the  memory  instructions,  and 
such  a  uar-LRU  policy  can  be  readily  transferred  to 
pseudo-LRU policy, which is widely used in real systems. 
the 
is 
C.  Transparent Input Coherence Strategy 
The  inherent  thread  level  parallelism  of  the  CMP 
systems  has  been  widely  exploited  by  programmers  to 
build  parallel  workloads.  However,  for  our  core-level 
fault-tolerant  systems,  since  parallel  workloads  share 
resources  among  the  threads,  an  external  master’s  write 
operation  may  violate  the  memory  consistency  of  the 
master-slave  pair.  Figure  5a)  shows  how  the  violation 
occurs.  As  the  asynchronous  execution  characteristic  of 
the  master-slave  pair,  an  external  write  operation  may 
cause the master and the slave to obtain different memory 
value. To avoid such consistency intervention, as  shown 
in  Figure  5b),  DCC  proposal  employs  a  master-slave 
memory  access  window  and  delays  the  external  write 
operation  [12].  However,  since  the  frequency  of  external 
write operations increases rapidly as the system scales, the 
maintenance overhead exacerbates dramatically, which has 
already been studied in [13].  
Our transparent binding scheme offers the chance to 
design a transparent input replication strategy, with which 
we  decouple  the  complexity  of  consistency  maintenance 
from the system scalability. Similar to the data obtaining 
strategy  based  on  the  consumer-consumer  data  access 
pattern,  we  call  for  the  master  cores  to  synchronize  and 
respond  for  external  exclusive  requests.  The  master  core 
also informs the slave of the invalidation hints. In order to 
guarantee  the  appropriate  invalidation  occasion  of  the 
slave core, we use the latest access sequence of the cache 
line as well as the original invalidation request to generate 
the  invalidation  message  for  the  slave  core.  The  access 
sequence of a cache line is used to uniquely identify when 
the  cache  line  should  be  invalidated.  When  a  memory 
instruction updates the MRU timestamps in the retirement 
stage, the latest access sequence of the cache line is also 
updated using the in-order sequence of the instruction. To 
implement this, we add a counter for each core to generate 
the in-order retirement sequence for the instructions. When  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
296TABLE I.  
Protocol Part 
MOESI_M 
MOESI_S 
MOESI_FT CACHE COHERENCE STATES 
State 
I, S, E, O, M 
SM, OM, MI, OI, II 
S_V, M_V, SM_V, OM_V 
PI, PE, PM 
PIE, PIM, 
PE_V, PM_V 
Description 
Stable state 
Transient State 
Victim State 
Stable State 
Transient State 
Victim State 
the  slave  receives  the  invalidation  message  from  its 
corresponding master, it checks the latest access sequences 
of the cache line. The invalidation is set once the memory 
instruction  with  the  same  latest  access  sequence  finishes 
the  data  access.  In  conclusion,  our  transparent  input 
coherence strategy removes the atomicity of the core pair’s 
data access behavior. The parallel execution of the external 
write  and  the  internal  slave’s  data  access  optimizes  the 
overall performance.  
Figure  6  shows  an  example of  our  transparent  input 
coherence  strategy.  Considering  the  master-slave  pair 
executes  with  a  long  slack  and  an  external  exclusive 
request  is  forwarded  to  invalidate  the  data  copy  in  the 
master  A.  Later,  A  receives  the  Inv  message,  invalidates 
the block and informs A’ with the Inv message as well as 
the  latest  access  sequence  of  the  block.  As  soon  as  the 
slave  A’  receives  Inv,  it  reserves  the  message  and  sends 
Ack  back  to  A.  After  the  slave’s  Ack  message  arrives,  A 
generates a global Ack message to the external master and 
the  write  operation  is  allowed  to  be  performed.  On  the 
other hand, the data block in the slave A’ is appropriately 
accessed  and  invalidated  with  the  help  of  the  master  A’s 
invalidation message. 
D.  MOESI_FT: A Fault-Tolerant Cache Coherence 
Protocol 
In  the  previous  three  subsections,  we  modified  the 
cache  coherence  protocol  to  achieve  the  objective  of 
master-slave  private  cache  consistency.  Here  we  briefly 
describe the cache coherence protocol called MOESI_FT, 
which  is  altered  from  a  conventional  directory-based 
protocol.  The  MOESI_FT  protocol  contains 
two 
components:  the  MOESI_M  part  is  related  to  the  master 
cores, which is responsible for the cache coherence of the 
system;  while  the  MOESI_S  part  helps  to  maintain  the 
private cache coherence of the slave cores. Table 1 shows 
the  states  of  the  MOESI_FT.  The  victim  state  represents 
that the data blocks are residing in the victim buffer. The 
transient state indicates that the cache block has taken an 
initial step to transit to a stable state; however, the cache 
block is waiting for data or acknowledge messages before 
making  the  transition.  Note  that  the  L1  cache  controller 
needs an extra bit  to  identify which  one of  the two parts 
should  be  used.  Once  the  coupling  relationship  of  the 
master-slave  pair  is  established,  the  bit  is  refreshed. 
Thereafter,  based  on  this  identification  bit,  the  cache 
controller chooses appropriate protocol components. 
Figure 7.   The master-slave binding information table 
IV.  TDB IMPLEMENTAL CONSIDERATION 
Previously we describe the key techniques to achieve 
the consistency objective. In this section we further discuss 
the implemental consideration of our TDB proposal. 
A.  Reliability VS Cost-Effective 
Reliability  and  cost-effective  are  two  architectural 
design considerations: the former one gives reliability the 
first order priority, while the latter one pays more attention 
to  the  execution  cost,  such  as  hardware  overhead,  power 
consumption or network traffic. As for our TDB proposal, 
it provides excellent flexibility for its fault-tolerant design 
at  the  cache  coherence  protocol  level.  We  argue  that  the 
workloads requiring high reliability need to be scheduled 
to  the  coupled  cores  and  the  workloads  requiring  cost-
effectiveness  need  to  be  scheduled  to  the  non-coupled 
cores.  In  this  way  we  can  switch  between  different 
execution priorities in a single system on demand, which 
in turn enhances the flexibility of our TDB proposal. 
B. 
 The Master-Slave Binding Information Table 
Dynamic binding scheme promises to provide better 
flexibility  as  the  binding  can  be  dynamically  established 
during  the  process  of  the  system.  Our  transparent  design 
further calls for the producer to provide data blocks for the 
master-slave  pair  when  it  receives  an  L1  cache  miss 
request  from  the  master  core.  Therefore,  the  producer 
should be aware of the binding information. In this paper, 
we  construct  a  simple  binding  information  table  for  the 
shared  L2  cache,  with  which  the  cache  controller  can 
easily  identify  the  redundant  core.  According  to  the 
consumer-consumer data access pattern diagram in Figure 
3, the request message firstly arrives at the shared cache. 
The lookup of the binding information table and the data 
access of the shared cache are then performed in parallel. 
If the shared cache acts as the producer, then it responds 
simultaneously to the consumer-consumer pair. Otherwise, 
the  request  is  packed  with  the  binding  information  and 
then is forwarded to another L1 cache which subsequently 
responds the data. Therefore, the binding information table 
stored in the shared cache can well support the consumer-
consumer data access pattern.  
Figure 7 shows the structure of the information table. 
The left part of the table entries preserves the indexes of 
master cores. For the sake of simplicity, in this paper we  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
297TABLE II.  
SYSTEM CONFIGURATION PARAMETERS. 
Processor Parameters 
Max Fetch/Retire width 
ROB/LSQ 
Memory Parameters 
Coherence Protocol 
Split L1 I/D Caches 
Shared L2 Cache 
Network Parameter 
Configuration 
4 Inst./cycle 
64/32 entries 
Non-inclusive MOESI_CMP_directory 
32KB, 2-way, 1-cycle latency 
2MB, 4-way, 16-cycle latency 
Topology 
Link Latency 
Routing Time 
Link Bandwidth 
Fault Tolerance Parameter 
State Compression Latency 
State Checkpoint Latency 
L1 Victim Size 
Checkpoint Interval 
2D-MESH 
4 cycles 
2 cycles 
1 flit/cycles 
35 cycles 
8 cycles 
32 entries 
10,000 cycles 
use the PID of the master core as the index. The right part 
of the table represents the matched slave cores. According 
to  the  consumer-consumer  data  access  pattern,  once  a 
cache miss request generated by a master core arrives, the 
shared  cache  controller  looks  up  the  table  and  identifies 
the master-slave pair. Considering a 2N-core CMP system, 
the number of the table entries is no more than N, and the 
whole  hardware  overhead  is  2N*log2N.  For  a  64-core 
CMP system, the area overhead of the table is about 384 
bits, which is negligible compared with the megabytes L2 
cache size. 
C.  Checkpoint and Fault Detection/Recovery 
As claimed before, the TDB execution model adopts 
the  dynamic  binding  scheme  to  offer  the  flexibility,  in 
which  the  master-slave  communication  is  accomplished 
via the system network. To amortize the network traffic of 
the master-slave data comparison, we implement a coarse-
grained  checkpoint  scheme.  Meanwhile,  we  utilize  the 
fingerprinting state compression method proposed in [17]. 
We  compress  the  results  of  the  store  instruction  when  it 
retires and compress the register states at checkpointing, as 
what is done in DCC [12]. The fault detection and the fault 
recovery  scheme  can  be  easily  implemented.  With  our 
fault-tolerant cache coherence protocol, it is convenient to 
integrate  the  reliability  and  the  cost-effectiveness  design 
philosophy  into  a  single  chip.  What’s  more,  when  a 
permanent  fault  is  detected  by  the  coupled  pair,  another 
core  can  be  involved  to  vote  for  the  defective  core. 
However,  how  to  tradeoff  between  reliability  and  cost-
effectiveness is beyond the scope of this paper and will be 
our future work. 
V. 
EXPERIMENTAL FRAMEWORK 
We  evaluate  our  TDB  proposal  with  a  full  system 
functional simulator Virtutech Simics [18] extended with 
Multifacet GEMS v2.1 [19]. GEMS provides two timing 
modules:  a  detailed  memory  simulation  module  called 
Ruby  and  an  out-of-order  processor  simulation  module 
called Opal. Garnet [31] is used to simulate the network 
in  the  CMP  system.  Table  1  lists  the  relevant  processor 
and fault-tolerant parameters used in our simulations.  
For  benchmarks,  we  select  some  applications  from 
the  SPLASH-2  benchmark  suite  [20]  including  barnes, 
con-ocean, radiostiy, fft, cholesky, lu and radix. In order 
to compare our TDB proposal with baseline fault-tolerant 
CMP  systems,  we  need  to  construct  the  baseline  fault-
tolerant systems. First, we construct four 2N-core systems 
(N=2, 4, 8, 16), afterwards, we bind the N phases of the 
parallel threads onto the former 0~N-1 cores and disable 
the  latter  cores  to  avoid  their  interventions  to  the 
execution.  The  2N-core  configuration  of  the  system 
simulates  the  baseline  Dual  Modular  Redundant  (DMR) 
system  where  the  redundant  cores  perform  the  fault-
tolerant  computing  without  disturbing  the  whole  master 
cores.  Therefore,  the  baseline  fault-tolerant  systems’ 
performance penalty on master-slave memory consistency 
is  zero.  Meanwhile,  for  our  fault-tolerant  CMP  systems, 
we simply couple the ith (i = 0, 1, …, N ) and the (N+i)th 
two  cores  to  form  a  master-slave  pair.  The  N-phase 
threads  are  also  replicated  and  bound  onto  the  cores  to 
form the proposed fault-tolerant CMP system. 
VI.  EXPERIMENTAL RESULTS 
In this section,  we firstly assess the performance of 
our  TDB  proposal.  Afterwards,  our  experimental  results 
are  compared  with  baseline  fault-tolerant  CMP  systems 
and the DCC architecture. 
A.  The Performance of TDB Proposal 
through 
recovery 
Core-level  fault-tolerant  design  schemes  implement 
fault  detection  and 
redundantly 
executing the original workload. In the ideal situation, the 
performance of the proposed fault-tolerant system should 
be the same as that of the baseline fault-tolerant systems. 
However,  as 
intervenes, 
performance overhead is inevitably induced. In our TDB 
proposal,  the  SoC  are  limited  to  the  private  caches  to 
reduce  the  complexity  of  the  master-slave  memory 
consistency  maintenance.  In  this  way  we  minimize  the 
performance overhead of the TDB execution model.  
redundant  execution 
the 
Figure 8 shows the normalized runtime of the TDB 
proposal.  On  average,  the  overall  runtime  of  our  TDB 
proposal  is  97.2%,  99.8%,  101.2%  and  105.4% over  the 
baseline fault-tolerant systems when considering 4, 8, 16 
and 32 cores respectively. For benchmarks fft, barnes and 
radix, it even has a shorter runtime in the 4-core CMP and 
the  8-core  CMP  systems  than  the  baseline  fault-tolerant 
systems.  This 
time  saved  by  our 
conservative ingress rule exceeds the delay induced by the 
redundant 
our 
conservative  ingress  rule  only  accepts  in  data  blocks 
accessed by memory instructions from correct paths. In  