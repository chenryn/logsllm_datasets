### 25th USENIX Security Symposium
**USENIX Association**

#### Feature Extraction and Reverse-Engineering

The process of feature extraction, denoted as \( \text{ex} \), increases the number of features in a model. If an adversary (A) reverse-engineers \( \text{ex} \), they can query the service on samples \( M \) in the input space, compute \( x = \text{ex}(M) \) locally, and extract the model \( f \) in the feature space using equation-solving techniques.

We applied this approach to models trained by Amazon. The results are summarized in Table 7. Initially, we trained a model with no categorical features and quantile binning disabled (a manually tunable parameter) over the Digits dataset. The attack is identical to the one described in Section 4.1.2: using 650 queries to Amazon, we extracted a model that achieved \( R_{\text{test}} = R_{\text{unif}} = 0 \).

Next, we considered models with feature extraction enabled. We assumed that A knows the input space \( M \) but not the training data distribution. For one-hot-encoding, knowledge of \( M \) suffices to apply the same encoding locally. However, for quantile binning, applying \( \text{ex} \) locally requires knowledge of the training data quantiles. To reverse-engineer the binning transformation, we used line-searches similar to those used for decision trees: for each numeric feature, we searched the feature’s range in the input space for thresholds (up to a granularity \( \epsilon \)) where \( f \)'s output changes, indicating that the value landed in an adjacent bin with a different learned regression coefficient.

Learning the bin boundaries is interesting in its own right, as it leaks information about the training data distribution. Once the bin boundaries are found, we can apply both one-hot-encoding and binning locally and extract \( f \) over its feature space. Since we are restricted to queries over \( M \), we cannot define an arbitrary system of equations over \( X \). Building a well-determined and consistent system can be difficult due to the sparsity of inputs generated by \( \text{ex} \). However, Amazon facilitates this process by setting all corresponding features in \( X \) to 0 if a feature is omitted from a query. For a linear model, we can trivially reconstruct the model by issuing queries with a single feature specified, obtaining equations with a single unknown in \( X \).

We trained models for the Circles, Iris, and Adult datasets with Amazon's default feature-extraction settings. Table 7 shows the results of our attacks for the reverse-engineering of \( \text{ex} \) and extraction of \( f \). For binary models (Circles and Adult), we used \( d + 1 \) queries to solve a linear equation system over \( X \). For models with \( c > 2 \) classes, we used \( c \cdot (d + 1) \) queries. In all cases, the extracted model matched \( f \) on 100% of tested inputs. To optimize query complexity, the queries used to find quantile bins were reused for equation-solving. As line searches require adaptive queries, we did not use batch predictions. Even for the Digits model, we resorted to real-time predictions due to the significant overhead of evaluating batches. For attacks requiring a large number of non-adaptive queries, batch predictions would likely be faster than real-time predictions.

#### Discussion

**Additional Feature Extractors**

In some ML services, users may enable further feature extractors. Common transformations include feature scaling or normalization. If A has access to training data statistics (as provided by BigML, for example), applying the transformation locally is trivial. More generally, for models with a linear input layer (e.g., logistic regressions, linear SVMs, MLPs), the scaling or normalization can be seen as being applied to the learned weights rather than the input features. Thus, the composition \( f \circ \text{ex} \) can be viewed as a model \( f' \) that operates over the 'unscaled' input space \( M \), and \( f' \) can be extracted directly using equation-solving.

Further extractors include text analysis (e.g., bag-of-words or n-gram models) and Cartesian products (grouping many features into one). While we have not analyzed these in this work, we believe they could also be easily reverse-engineered, especially given some training data statistics and the ability to make incomplete queries.

**Learning Unknown Model Classes or Hyper-parameters**

For our online attacks, we obtained information about the model class of \( f \), the enabled feature extraction \( \text{ex} \), and other hyper-parameters directly from the ML service or its documentation. Generally, if A does not have full certainty about certain model characteristics, they may narrow down a guess to a small range. Model hyper-parameters, such as the free parameter of an RBF kernel, are typically chosen through cross-validation over a default range of values. Given a set of attack strategies with varying assumptions, A can use a generic extract-and-test approach: each attack is applied in turn and evaluated by computing \( R_{\text{test}} \) or \( R_{\text{unif}} \) over a chosen set of points. The adversary succeeds if any strategy achieves a low error. Note that A needs to interact with the model \( f \) only once to obtain responses for a chosen set of extraction and test samples, which can be reused for each strategy.

Our attacks on Amazon's service followed this approach: we first formulated guesses for model characteristics left unspecified by the documentation (e.g., no mention of one-hot-encoding or how missing inputs are handled). We then evaluated our assumptions with successive extraction attempts. Our results indicate that Amazon uses softmax regression and does not create binary predictors for missing values. Interestingly, BigML takes the opposite approach (i.e., BigML uses OvR regression and adds predictors for missing values).

#### Extraction Given Class Labels Only

The successful attacks in Sections 4 and 5 highlight the danger of revealing confidence values. While current ML services are designed to reveal rich information, our attacks suggest that returning only labels might be safer. Here, we explore model extraction in a setting with no confidence scores. We primarily focus on settings where A can make direct queries to an API, i.e., queries for arbitrary inputs \( x \in X \). We briefly discuss indirect queries in the context of linear classifiers.

**The Lowd-Meek Attack**

We start with the prior work of Lowd and Meek [36]. They present an attack on any linear classifier, assuming black-box oracle access with membership queries that return just the predicted class label. A linear classifier is defined by a vector \( w \in \mathbb{R}^d \) and a constant \( \beta \in \mathbb{R} \), and classifies an instance \( x \) as positive if \( w \cdot x + \beta > 0 \) and negative otherwise. Examples of linear classifiers include SVMs with linear kernels and binary LRs. Their attack uses line searches to find points arbitrarily close to \( f \)'s decision boundary (points for which \( w \cdot x + \beta \approx 0 \)), and extracts \( w \) and \( \beta \) from these samples. This attack works only for linear binary models. We describe a straightforward extension to some non-linear models, such as polynomial kernel SVMs. Extracting a polynomial kernel SVM can be reduced to extracting a linear SVM in the transformed feature space. For any kernel \( K_{\text{poly}}(x, x') = (x^T \cdot x' + 1)^d \), we can derive a projection function \( \phi(\cdot) \) so that \( K_{\text{poly}}(x, x') = \phi(x)^T \cdot \phi(x') \). This transforms the kernel SVM into a linear one, as the decision boundary becomes \( w_F \cdot \phi(x) + \beta = 0 \) where \( w_F = \sum_{i=1}^t \alpha_i \phi(x_i) \). We can use the Lowd-Meek attack to extract \( w_F \) and \( \beta \) as long as \( \phi(x) \) and its inverse are feasible to compute; this is not the case for more common RBF kernels.

**The Retraining Approach**

In addition to evaluating the Lowd-Meek attack against ML APIs, we introduce several other approaches based on the broad strategy of retraining a model locally, given input-output examples. Informally, our hope is that by extracting a model that achieves low training error over the queried samples, we would effectively approximate the target model’s decision boundaries. We consider three retraining strategies:

1. **Retraining with Uniform Queries**: This baseline strategy consists of sampling \( m \) points \( x_i \in X \) uniformly at random, querying the oracle, and training a model \( \hat{f} \) on these samples.
2. **Line-Search Retraining**: This strategy issues \( m \) adaptive queries to the oracle using line search techniques to find samples close to the decision boundaries of \( f \). A model \( \hat{f} \) is then trained on the \( m \) queried samples.
3. **Adaptive Retraining**: This strategy applies techniques from active learning [18, 47]. For some number \( r \) of rounds and a query budget \( m \), it first queries the oracle on \( \frac{m}{r} \) uniform points and trains a model \( \hat{f} \). Over a total of \( r \) rounds, it then selects \( \frac{m}{r} \) new points along the decision boundary of \( \hat{f} \) (intuitively, these are points \( \hat{f} \) is least certain about) and sends those to the oracle before retraining \( \hat{f} \).

**Linear Binary Models**

We first explore how well the various approaches work in settings where the Lowd-Meek attack can be applied. We evaluate their attack and our three retraining strategies for logistic regression models trained over the binary datasets shown in Table 3. These models have \( d + 1 \) parameters, and we vary the query budget as \( \alpha \cdot (d + 1) \) for \( 0.5 \leq \alpha \leq 100 \). Figure 4 displays the average errors \( R_{\text{test}} \) and \( R_{\text{unif}} \) over all models as a function of \( \alpha \).

The retraining strategies that search for points near the decision boundary clearly perform better than simple uniform retraining. The adaptive strategy is the most efficient of our three strategies. For relatively low budgets, it even outperforms the Lowd-Meek attack. However, for budgets large enough to run line searches in each dimension, the Lowd-Meek attack is the most efficient. For the models we trained, about 2,050 queries on average, and 5,650 at most, are needed to run the Lowd-Meek attack effectively. This is 50 times more queries than what we needed for equation-solving attacks. With 827 queries on average, adaptive retraining yields a model \( \hat{f} \) that matches \( f \) on over 99% of tested inputs. Thus, even if an ML API only provides class labels, efficient extraction attacks on linear models remain possible.

We further consider a setting where feature extraction (specifically one-hot-encoding of categorical features) is applied by the ML service rather than by the user. A is then limited to indirect queries in input space. Lowd and Meek [36] note that their extraction attack does not work in this setting, as A cannot run line searches directly over \( X \). In contrast, for the linear models we trained, we observed no major difference in extraction accuracy for the adaptive-retraining strategy when limited to queries over \( M \). We leave an in-depth study of model extraction with indirect queries and class labels only for future work.

**Multiclass LR Models**

The Lowd-Meek attack is not applicable in multiclass (\( c > 2 \)) settings, even when the decision boundary is a combination of linear boundaries (as in multiclass regression) [39, 50]. We thus focus on evaluating the three retraining attacks we introduced for the type of ML models we expect to find in real-world applications. We focus on softmax models here, as softmax and one-vs-rest models have identical output behaviors when only class labels are provided: in both cases, the class label for an input \( x \) is given by \( \arg\max_i (w_i \cdot x + \beta_i) \). From an extractor’s perspective, it is thus irrelevant whether the target was trained using a softmax or OvR approach.

We evaluate our attacks on softmax models trained on the multiclass datasets shown in Table 3. We again vary the query budget as a factor \( \alpha \) of the number of model parameters, namely \( \alpha \cdot c \cdot (d + 1) \). Results are displayed in Figure 5. We observe that the adaptive strategy clearly performs best, and the line-search strategy does not improve over uniform retraining, possibly because the line-searches have to be split across multiple decision boundaries. We further note that all strategies achieve lower \( R_{\text{test}} \) than \( R_{\text{unif}} \). It thus appears that for the models we trained, points from the test set are on average 'far' from the decision boundaries of \( f \) (i.e., the trained models separate the different classes with large margins).

For all models, \( 100 \cdot c \cdot (d + 1) \) queries resulted in extraction accuracy above 99.9%. This represents 26,000 queries on average and 65,000 at the most (Digits dataset). Our equation-solving attacks achieved similar or better results with 100 times fewer queries. Yet, for scenarios with high monetary incentives (e.g., intrusion detector evasion), extraction attacks on MLR models may be attractive, even if APIs only provide class labels.

**Neural Networks**

We now turn to attacks on more complex deep neural networks. We expect these to be harder to retrain than multiclass regressions, as deep networks have more parameters.