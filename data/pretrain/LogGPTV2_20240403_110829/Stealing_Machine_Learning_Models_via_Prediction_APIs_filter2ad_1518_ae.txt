610  25th USENIX Security Symposium 
USENIX Association
i.e., ex increases the number of features. If A reverse-
engineers ex, she can query the service on samples M in
input space, compute x = ex(M) locally, and extract f in
feature-space using equation-solving.
We apply this approach to models trained by Amazon.
Our results are summarized in Table 7. We first train a
model with no categorical features, and quantile binning
disabled (this is a manually tunable parameter), over the
Digits data set. The attack is then identical to the one
considered in Section 4.1.2: using 650 queries to Ama-
zon, we extract a model that achieves Rtest = Runif = 0.
We now consider models with feature extraction en-
abled. We assume that A knows the input space M, but
not the training data distribution. For one-hot-encoding,
knowledge of M suffices to apply the same encoding lo-
cally. For quantile binning however, applying ex locally
requires knowledge of the training data quantiles. To
reverse-engineer the binning transformation, we use line-
searches similar to those we used for decision trees: For
each numeric feature, we search the feature’s range in in-
put space for thresholds (up to a granularity ε) where f ’s
output changes. This indicates our value landed in an ad-
jacent bin, with a different learned regression coefficient.
Note that learning the bin boundaries may be interesting
in its own right, as it leaks information about the train-
ing data distribution. Having found the bin boundaries,
we can apply both one-hot-encoding and binning locally,
and extract f over its feature space. As we are restricted
to queries over M, we cannot define an arbitrary sys-
tem of equations over X . Building a well-determined
and consistent system can be difficult, as the encoding ex
generates sparse inputs over X . However, Amazon facil-
itates this process with the way it handles queries with
missing features:
if a feature is omitted from a query,
all corresponding features in X are set to 0. For a lin-
ear model for instance, we can trivially re-construct the
model by issuing queries with a single feature specified,
such as to obtain equations with a single unknown in X .
We trained models for the Circles, Iris and Adult data
sets, with Amazon’s default feature-extraction settings.
Table 7 shows the results of our attacks, for the reverse-
engineering of ex and extraction of f . For binary models
(Circles and Adult), we use d +1 queries to solve a linear
equation-system over X . For models with c > 2 classes,
In all cases, the extracted
we use c · (d + 1) queries.
model matches f on 100% of tested inputs. To optimize
the query complexity, the queries we use to find quantile
bins are re-used for equation-solving. As line searches
require adaptive queries, we do not use batch predictions.
However, even for the Digits model, we resorted to using
real-time predictions, because of the service’s significant
overhead in evaluating batches. For attacks that require
a large number of non-adaptive queries, we expect batch
predictions to be faster than real-time predictions.
5.3 Discussion
Additional feature extractors.
In some ML services
we considered, users may enable further feature extrac-
tors. A common transformation is feature scaling or nor-
If A has access to training data statistics
malization.
(as provided by BigML for instance), applying the trans-
formation locally is trivial. More generally, for models
with a linear input layer (i.e., logistic regressions, linear
SVMs, MLPs) the scaling or normalization can be seen
as being applied to the learned weights, rather than the
input features. We can thus view the composition f ◦ ex
as a model f (cid:26) that operates over the ‘un-scaled’ input
space M and extract f (cid:26) directly using equation-solving.
Further extractors include text analysis (e.g., bag-of-
words or n-gram models) and Cartesian products (group-
ing many features into one). We have not analyzed these
in this work, but we believe that they could also be easily
reverse-engineered, especially given some training data
statistics and the ability to make incomplete queries.
Learning unknown model classes or hyper-parame-
ters. For our online attacks, we obtained information
about the model class of f , the enabled feature extrac-
tion ex, and other hyper-parameters, directly from the
ML service or its documentation. More generally, if A
does not have full certainty about certain model charac-
teristics, it may be able to narrow down a guess to a small
range. Model hyper-parameters for instance (such as the
free parameter of an RBF kernel) are typically chosen
through cross-validation over a default range of values.
Given a set of attack strategies with varying assump-
tions, A can use a generic extract-and-test approach:
each attack is applied in turn, and evaluated by comput-
ing Rtest or Runif over a chosen set of points. The adver-
sary succeeds if any of the strategies achieves a low error.
Note that A needs to interact with the model f only once,
to obtain responses for a chosen set of extraction samples
and test samples, that can be re-used for each strategy.
Our attacks on Amazon’s service followed this ap-
proach: We first formulated guesses for model charac-
teristics left unspecified by the documentation (e.g., we
found no mention of one-hot-encoding, or of how miss-
ing inputs are handled). We then evaluated our assump-
tions with successive extraction attempts. Our results in-
dicate that Amazon uses softmax regression and does not
create binary predictors for missing values. Interestingly,
BigML takes the ’opposite’ approach (i.e., BigML uses
OvR regression and adds predictors for missing values).
6 Extraction Given Class Labels Only
The successful attacks given in Sections 4 and 5 show
the danger of revealing confidence values. While current
USENIX Association  
25th USENIX Security Symposium  611
ML services have been designed to reveal rich informa-
tion, our attacks may suggest that returning only labels
would be safer. Here we explore model extraction in a
setting with no confidence scores. We will discuss fur-
ther countermeasures in Section 7. We primarily focus
on settings where A can make direct queries to an API,
i.e., queries for arbitrary inputs x ∈ X . We briefly discuss
indirect queries in the context of linear classifiers.
100
10−1
10−2
10−3
10−4
r
o
r
r
E
n
o
i
t
c
a
r
t
x
E
.
g
v
A
Rtest
Runif
Uniform
Line-Search
Adaptive
Lowd-Meek
The Lowd-Meek attack. We start with the prior work
of Lowd and Meek [36]. They present an attack on any
linear classifier, assuming black-box oracle access with
membership queries that return just the predicted class
label. A linear classifier is defined by a vector w ∈ Rd
and a constant β ∈ R, and classifies an instance x as pos-
itive if w· x + β > 0 and negative otherwise. SVMs with
linear kernels and binary LRs are examples of linear clas-
sifiers. Their attack uses line searches to find points ar-
bitrarily close to f ’s decision boundary (points for which
w· x + β ≈ 0), and extracts w and β from these samples.
This attack only works for linear binary models. We
describe a straightforward extension to some non-linear
models, such as polynomial kernel SVMs. Extracting a
polynomial kernel SVM can be reduced to extracting a
linear SVM in the transformed feature space.
Indeed,
for any kernel Kpoly(x,x(cid:26))=(xT · x(cid:26) + 1)d, we can derive
a projection function φ (·), so that Kpoly(x,x(cid:26))=φ (x)T ·
φ (x(cid:26)). This transforms the kernel SVM into a linear one,
since the decision boundary now becomes wF · φ (x) +
β = 0 where wF = ∑t
i=1 αiφ (xi). We can use the Lowd-
Meek attack to extract wF and β as long as φ (x) and its
inverse are feasible to compute; this is unfortunately not
the case for the more common RBF kernels.3
The retraining approach.
In addition to evaluating
the Lowd-Meek attack against ML APIs, we introduce
a number of other approaches based on the broad strat-
egy of re-training a model locally, given input-output
examples. Informally, our hope is that by extracting a
model that achieves low training error over the queried
samples, we would effectively approximate the target
model’s decision boundaries. We consider three re-
training strategies, described below. We apply these
to the model classes that we previously extracted using
equation-solving attacks, as well as to SVMs.4
(1) Retraining with uniform queries. This baseline
strategy simply consists in sampling m points xi ∈
X uniformly at random, querying the oracle, and
training a model ˆf on these samples.
3We did explore using approximations of φ, but found that the adap-
tive re-training techniques discussed in this section perform better.
4We do not expect retraining attacks to work well for decision trees,
because of the greedy approach taken by learning algorithms. We have
not evaluated extraction of trees, given class labels only, in this work.
0
0
50
25
75
Budget Factor α
100
0
50
25
75
Budget Factor α
100
Figure 4: Average error of extracted linear models. Results are for
different extraction strategies applied to models trained on all binary
data sets from Table 3. The left shows Rtest and the right shows Runif.
(2) Line-search retraining. This strategy can be seen
as a model-agnostic generalization of the Lowd-
Meek attack.
It issues m adaptive queries to the
oracle using line search techniques, to find samples
close to the decision boundaries of f . A model ˆf is
then trained on the m queried samples.
(3) Adaptive retraining. This strategy applies tech-
niques from active learning [18, 47]. For some
number r of rounds and a query budget m, it first
queries the oracle on m
r uniform points, and trains a
model ˆf . Over a total of r rounds, it then selects m
r
new points, along the decision boundary of ˆf (in-
tuitively, these are points ˆf is least certain about),
and sends those to the oracle before retraining ˆf .
6.1 Linear Binary Models
We first explore how well the various approaches work
in settings where the Lowd-Meek attack can be applied.
We evaluate their attack and our three retraining strate-
gies for logistic regression models trained over the binary
data sets shown in Table 3. These models have d + 1 pa-
rameters, and we vary the query budget as α · (d +1), for
0.5 ≤ α ≤ 100. Figure 4 displays the average errors Rtest
and Runif over all models, as a function of α.
The retraining strategies that search for points near
the decision boundary clearly perform better than simple
uniform retraining. The adaptive strategy is the most ef-
ficient of our three strategies. For relatively low budgets,
it even outperforms the Lowd-Meek attack. However, for
budgets large enough to run line searches in each dimen-
sion, the Lowd-Meek attack is clearly the most efficient.
For the models we trained, about 2,050 queries on av-
erage, and 5,650 at most, are needed to run the Lowd-
Meek attack effectively. This is 50× more queries than
what we needed for equation-solving attacks. With 827
queries on average, adaptive retraining yields a model ˆf
that matches f on over 99% of tested inputs. Thus, even
if an ML API only provides class labels, efficient extrac-
612  25th USENIX Security Symposium 
USENIX Association
tion attacks on linear models remain possible.
We further consider a setting where feature-extraction
(specifically one-hot-encoding of categorical features) is
applied by the ML service, rather than by the user. A is
then limited to indirect queries in input space. Lowd and
Meek [36] note that their extraction attack does not work
in this setting, as A can not run line searches directly over
X . In contrast, for the linear models we trained, we ob-
served no major difference in extraction accuracy for the
adaptive-retraining strategy, when limited to queries over
M. We leave an in-depth study of model extraction with
indirect queries, and class labels only, for future work.
6.2 Multiclass LR Models
The Lowd-Meek attack is not applicable in multiclass
(c > 2) settings, even when the decision boundary is a
combination of linear boundaries (as in multiclass re-
gression) [39, 50]. We thus focus on evaluating the three
retraining attacks we introduced, for the type of ML
models we expect to find in real-world applications.
We focus on softmax models here, as softmax and one-
vs-rest models have identical output behaviors when only
class labels are provided: in both cases, the class label
for an input x is given by argmaxi(wi · x + βi). From an
extractor’s perspective, it is thus irrelevant whether the
target was trained using a softmax or OvR approach.
We evaluate our attacks on softmax models trained on
the multiclass data sets shown in Table 3. We again vary
the query budget as a factor α of the number of model
parameters, namely α · c· (d + 1). Results are displayed
in Figure 5. We observe that the adaptive strategy clearly
performs best and that the line-search strategy does not
improve over uniform retraining, possibly because the
line-searches have to be split across multiple decision-
boundaries. We further note that all strategies achieve
lower Rtest than Runif. It thus appears that for the models
we trained, points from the test set are on average ‘far’
from the decision boundaries of f (i.e., the trained mod-
els separate the different classes with large margins).
For all models, 100· c· (d + 1) queries resulted in ex-
traction accuracy above 99.9%. This represents 26,000
queries on average, and 65,000 at the most (Digits data
set). Our equation-solving attacks achieved similar or
better results with 100× less queries. Yet, for scenar-
ios with high monetary incentives (e.g., intrusion detec-
tor evasion), extraction attacks on MLR models may be
attractive, even if APIs only provide class labels.
6.3 Neural Networks
We now turn to attacks on more complex deep neural
networks. We expect these to be harder to retrain than
multiclass regressions, as deep networks have more pa-
r
o
r
r
E
n
o
i
t
c
a
r
t
x
E
.
g
v
A
100
10−1
10−2
10−3
10−4
Rtest
Runif
Uniform
Line-Search
Adaptive
0
50
25
75