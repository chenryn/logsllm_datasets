may be some higher order eﬀects.
Duplicate segments at the TCP receiver normally trig-
ger DSACKs [9]. The DSACK statistics provide a measure
of network resources that were wasted by overly aggressive
retransmits of nearly all types, except segments with no pay-
2This statistic is particularly odd, because a timeout from
the open state implies that an entire window of data or
ACKs was lost in a single RTT. Although the high volume
of tiny transactions contribute to timeouts from open, they
can not explain numbers this large. Some other mechanism
must be present.
157load. In DC1, we see an average of 12% fast-recoveries caus-
ing DSACKS, implying at least a 3.8% spurious retransmis-
sion rate.
In DC2 the spurious retransmission rate is at
least 1.4%. Note that since some Web clients don’t gener-
ate DSACKS, these are lower bounds of the actual spurious
retransmissions.
Linux also detects lost retransmissions as described in sec-
tion 3.2. Since the retransmissions are delivered in a new
round trip, they provide some indication of how quickly con-
gestion subsides following the initial packet losses. With
lost retransmission detection, lost retransmissions cause ad-
ditional RTTs in recovery. Without it, they cause timeouts.
In DC1 and DC2, about 2% and 3% fast-retransmits are
lost.
The data shown here is for the Linux baseline, however
these metrics are important for comparing any recovery al-
gorithms, and are used elsewhere.
3. STATE-OF-THE-ART FAST RECOVERY
Our work is based on the Linux TCP implementation,
which includes several non-standard algorithms and exper-
imental RFCs. The goal of this section is two-fold: ﬁrst,
we highlight the diﬀerences between standard TCP recovery
algorithms [4, 8] and the design of widely deployed Linux re-
covery. Second, we discuss the drawbacks of each design,
speciﬁcally in the context of short Web transactions.
3.1 Fast recovery in RFC standards
Standard congestion control in RFC 5681 [4] requires that
TCP reduce the congestion window in response to losses.
Fast recovery, described in the same document, is the refer-
ence algorithm for making this adjustment. Its stated goal is
to recover TCP’s self clock by relying on returning dupacks
during recovery to clock more data into the network. De-
pending on the pattern of losses, we ﬁnd that the standard
can be either too aggressive or too conservative.
Algorithm 1: RFC 3517 fast recovery
On entering recovery:
// cwnd used during and after recovery.
cwnd = ssthresh = F lightSize/2
// Retransmit first missing segment.
f ast retransmit()
// Transmit more if cwnd allows.
T ransmit M AX(0, cwnd − pipe)
For every ACK during recovery:
update scoreboard() pipe = (RFC 3517 pipe algorithm)
T ransmit M AX(0, cwnd − pipe)
Algorithm 1 brieﬂy presents the standard RFC design for
recovery. A TCP sender enters fast recovery upon receiving
dupthresh number of dupacks (typically three). On entering
recovery, the sender performs the following: 1) sets cwnd
and ssthresh to half of the data outstanding in the network
and, 2) f ast retransmit the ﬁrst unacknowledged segment,
and further transmits more segments if allowed by cwnd.
On subsequent ACKs during recovery, the sender re-
computes the pipe variable using the procedure speciﬁed in
RFC 3517. pipe is an estimate of the number of total seg-
Features
Initial cwnd
Cong. control (NewReno)
SACK
D-SACK
Rate-Halving [17]
FACK [16]
Limited-transmit
Dynamic dupthresh
RTO
F-RTO
Cwnd undo (Eifel)
TCP segmentation oﬄoad
RFC Linux
3390
5681
2018
3708
3042
2988
5682
3522
Default
10
CUBIC
on
on
always on
on
always on
always on
min=200ms
on
always on
determined by NIC
p
+
+
+
+
+
+
+
p
+
p
+
+ indicates the feature is fully implemented.
p indicates a partially implemented feature.
Table 4: Loss recovery related features in Linux.
Non-standard features are those without RFC num-
bers.
ments in the network based on the SACK scoreboard. It is
only an estimate because it relies on heuristics that deter-
mine if a segment can be marked as lost. The algorithm then
transmits up to cwnd − pipe segments. Fast recovery ends
when all data that was outstanding before entering recovery
is cumulatively acknowledged or when a timeout occurs.
There are two main problems exhibited by the standard:
1. Half RT T silence: The algorithm waits for half of
the received ACKs to pass by before transmitting any-
thing after the ﬁrst fast rertansmit. This is because
cwnd is brought down to ssthresh in one step, so it
takes cwnd-ssthresh ACKs for pipe to go below cwnd,
creating a silent period for half of an RTT. This de-
sign wastes precious opportunities to transmit which
sometimes results in nothing being sent during recov-
ery. This in turn increases the chances of timeouts.
2. Aggressive and bursty retransmissions: The stan-
dard can transmit large bursts on a single received
ACK. This is because pipe − cwnd can be arbitrary
large under burst losses or inaccurate estimation of
losses. Furthermore, the more losses there are, the
larger the bursts transmitted by the standard.
Note that both problems occur in the context of heavy
losses, wherein greater than or equal to half of the cwnd is
lost. We will see in Section 5 that such heavy losses are
surprisingly common for both Web and YouTube.
3.2 Fast recovery in Linux
Linux implements a rich set of standard and non-standard
loss recovery algorithms [23]. Table 4 lists the main algo-
rithms related to loss recovery and their default settings.
Linux keeps a SACK scoreboard as described in
RFC 2018 [18] and computes pipe, the estimate of the num-
ber of segments in the network, per RFC 3517. Our detailed
analysis of TCP retransmissions is based on the following
four recovery states in the Linux TCP code base:
• Open: There is no missing data and the sender is re-
ceiving in-sequence ACKs.
• Disorder: The sender has received dupacks indicating
that there is data missing, but has not yet retransmit-
ted anything.
158• Recovery: There has been a fast retransmit.
4. PROPORTIONAL RATE REDUCTION
• Loss: There has been a timeout and resetting of cwnd
to one to recover TCP’s self clock.
There are three key diﬀerences between Linux and
RFC 3517.
Linux is more aggressive in marking segments lost because
it implements several algorithms from FACK [16] in addition
to the standard loss detection described in RFC 3517. FACK
was developed at an earlier time when network reordering
was relatively rare, so it makes some simplifying assump-
tions. Speciﬁcally, fast retransmit can be triggered immedi-
ately by the very ﬁrst SACK if it indicates that more than
dupthresh segments are missing (so called threshold retrans-
mission). Furthermore, once in fast recovery, all holes below
the highest SACK block are assumed to be lost and marked
for retransmission. Linux also includes a slightly later al-
gorithm for detecting lost retransmissions. If any new data
sent after a retransmission is later SACKed, the retransmis-
sion is deemed to have been lost [17]. If any reordering is
detected then FACK and some of the related algorithms are
disabled, the loss detection falls back to use conventional
dupthresh.
Linux implements the rate halving algorithm [17, 19] in
recovery. When cwnd is reduced, Linux sends data in re-
sponse to alternate ACKs during recovery, instead of wait-
ing for cwnd/2 dupacks to pass as speciﬁed in the standard.
A minor problem with rate halving is that it is based on
the original Reno TCP that always halved the cwnd during
fast recovery. Several modern congestion control algorithms,
such as CUBIC [10], reduce the window by less than 50% so
unconditionally halving the rate is no longer appropriate.
While in recovery, Linux prevents bursts by reducing cwnd
to pipe + 1 on every ACK that reduces pipe for some reason.
This implies that if there is insuﬃcient new data available
(e.g., because the application temporarily stalls), cwnd can
be become one by the end of recovery. If this happens, then
after recovery the sender will slow start from a very small
cwnd even though only one segment was lost and there was
no timeout!
The main drawbacks of the fast recovery in Linux are its
excessive window reductions and conservative retransmis-
sions, which occur for the following reasons:
1. Slow start af ter recovery : Even for a single loss
event, a connection carrying short Web responses can
complete recovery with a very small cwnd, such that
subsequent responses using the same connection will
slow start even when not otherwise required.
2. Conservative retransmissions : There are at least
two scenarios where retransmissions in Linux are
overly conservative.
In the presence of heavy losses,
when pipe falls below ssthresh, Linux (re)transmits at
most one packet per ACK during the rest of recovery.
As a result, recovery is either prolonged or it enters an
RTO.
A second scenario is that rate halving assumes every
ACK represents one data packet delivered. However,
lost ACKs will cause Linux to retransmit less than half
of the congestion window.
Proportional Rate Reduction is designed to overcome all
four problems mentioned in the previous section.
The PRR algorithm determines the number of segments to
be sent per ACK during recovery to balance two goals: 1) a
speedy and smooth recovery from losses, and 2) end recovery
at a congestion window close to ssthresh. The foundation of
the algorithm is Van Jacobson’s packet conservation princi-
ple: segments delivered to the receiver are used as the clock
to trigger sending additional segments into the network.
PRR has two main parts. The ﬁrst part, the propor-
tional part is active when the number of outstanding seg-
ments (pipe) is larger than ssthresh, which is typically true
early during the recovery and under light losses. It gradu-
ally reduces the congestion window clocked by the incom-
ing acknowledgments. The algorithm is patterned after rate
halving, but uses a fraction that is appropriate for the tar-
get window chosen by the congestion control algorithm. For
example, when operating with CUBIC congestion control,
the proportional part achieves the 30% window reduction
by spacing out seven new segments for every ten incoming
ACKs (more precisely, for ACKs reﬂecting 10 segments ar-
riving at the receiver).
If pipe becomes smaller than ssthresh (such as due to ex-
cess losses or application stalls during recovery), the second
part of the algorithm attempts to inhibit any further con-
gestion window reductions. Instead it performs slow start to
build the pipe back up to ssthresh subject to the availability
of new data to send.3
Note that both parts of the PRR algorithm are inde-
pendent of the congestion control algorithm (CUBIC, New
Reno, GAIMD [29] etc.) used to determine the new value of
ssthresh.
We also introduced a complete description of PRR into
the IETF as a possible future RFC [15].
4.1 Examples
We present TCP time-sequence graphs of packet traces to
illustrate and compare the three fast recovery algorithms.
The traces are produced by our Linux kernel implementa-
tion and an internally developed TCP testing tool. The
measurement setup is a simple network with a 100ms RTT
and 1.2Mbps link and a 1000 byte MSS. The server applica-
tion writes 20kB at time 0 and 10kB at time 500ms. TCP
use traditional Reno congestion control, which sets ssthresh
to half of the cwnd at the beginning of recovery.
The ﬁrst example is shown in Figure 2. Vertical dou-
ble ended arrows represent transmitted data segments that
carry a range of bytes at a particular time. Retransmit-
ted data is red. The green staircase represents advancing
snd.una carried by the ACKs returning to the sender, and
the vertical purple lines represent SACK blocks, indicating
data that has arrived at the receiver, but is above snd.una.
In this example, the ﬁrst 4 segments are dropped. Since
TCP uses FACK, in all three traces TCP enters fast recov-
ery after receiving ﬁrst dupack and sets ssthresh to 10 (half
of cwnd). In the PRR trace (top), the proportion part of
3We considered several alternative reduction bound algo-
rithms, as described in [15], and this variant provides the
best combination of features. This combined algorithm
should more properly be called Proportional Rate Reduc-
tion with Slow-Start Reduction Bound (PRR-SSRB), but
we shortened it to PRR.
159.
200 ms 
400 ms 
600 ms 
.
30000 
20000 
10000 
0 
.
100 ms 
200 ms 
300 ms 
400 ms 
500 ms 
Figure 3: PRR algorithm under heavy losses.
200 ms 
400 ms 
600 ms 
800 ms 
1 s 
.
30000 
20000 
.
10000 
0 
30000 
20000 
10000 
0 
30000 
20000 
10000 
0 
.
.
200 ms 
400 ms 
600 ms 
Figure 2: A comparison of PRR (top), Linux recov-
ery (middle), and RFC 3517 (bottom). Legend for the
plot: Original data transmissions (Black), retransmis-
sions (Red), snd.una (Green), duplicate ACKs with
SACK blocks (Purple).
PRR retransmits one segment every other ACK. At time
460ms, the sender completes the recovery and sets the cwnd
to ssthresh (10). When the application writes 10 segments
into the socket at time 500ms, the segments are sent in one
RTT. The Linux trace (middle) highlights the ﬁrst problem
in Section 3.2. The retransmission timings are similar to
the PRR trace because Linux does rate-halving. But the
key diﬀerence is that cwnd remains pipe + 1 when fast re-
covery completes, pipe is 1 right before recovery completes.
Therefore it takes 4 RTTs to deliver the next 10 segments
in slow-start. The RFC trace (bottom) highlights the ﬁrst
problem in Section 3.1: the 2nd retransmission happens af-
ter about half of the cwnd of ACKs are received resulting,
in the half RTT silence.
Figure 3 illustrates how PRR reacts to heavy losses. We
use the same setup but drop segments 1 to 4 and 11 to 16.
After ﬁrst cluster of drops and pipe is larger than ssthresh,
the proportional part of the algorithm is active transmit-
ting on alternate received ACKs. However, after the second
cluster of losses when pipe falls below ssthresh, PRR oper-
ates in slow start part and transmits two segments for every
ACK.
4.2 Pseudo code for PRR
Algorithm 2 shows how PRR determines the number of
bytes that should be sent in response to each ACK in recov-
ery. The algorithm relies on three new state variables:
Algorithm 2: Proportional Rate Reduction (PRR)
Initialization on entering recovery:
// Target cwnd after recovery.
ssthresh = CongCtrlAlg()
// Total bytes delivered during recovery.
prr delivered = 0
// Total bytes sent during recovery. prr out = 0
// FlightSize at the start of recovery.
RecoverF S = snd.nxt − snd.una
On every ACK during recovery compute:
// DeliveredData is number of new bytes that the
current acknowledgment indicates have been
delivered to the receiver.
DeliveredData = delta(snd.una) + delta(SACKd)
prr delivered+ = DeliveredData
pipe = (RFC 3517 pipe algorithm)
if pipe > ssthresh then
// Proportional Rate Reduction
sndcnt = CEIL(prr delivered ∗
ssthresh/RecoverF S) − prr out
else
// Slow start
ss limit =
M AX(prr delivered − prr out, DeliveredData) + 1
sndcnt = M IN (ssthresh − pipe, ss limit)
sndcnt = M AX(sndcnt, 0) // positive
cwnd = pipe + sndcnt
On any data transmission or retransmission:
prr out+ = data sent
At the end of recovery: