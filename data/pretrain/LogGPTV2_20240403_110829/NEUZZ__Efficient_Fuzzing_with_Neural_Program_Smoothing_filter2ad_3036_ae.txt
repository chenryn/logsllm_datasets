r
e
v
o
c
e
g
d
E
e
g
a
r
e
v
o
c
e
g
d
E
1500
1250
1000
750
500
250
0
0
2
4
6
8 10 12 14 16 18 20 22 24
Time (hour)
libjpeg
0
2
4
6
8 10 12 14 16 18 20 22 24
Time (hour)
mupdf
0
2
4
6
8 10 12 14 16 18 20 22 24
Time (hour)
libxml
e
g
a
r
e
v
o
c
e
g
d
E
3000
2500
2000
1500
1000
500
0
0
2
4
6
300
e
g
a
r
e
v
o
c
e
g
d
E
200
100
0
0
2
4
6
8 10 12 14 16 18 20 22 24
Time (hour)
zlib
0
2
4
6
8 10 12 14 16 18 20 22 24
Time (hour)
0
2
4
6
8 10 12 14 16 18 20 22 24
Time (hour)
objdump
size
8 10 12 14 16 18 20 22 24
Time (hour)
strip
Fig. 4: The edge coverage of different fuzzers running for 24 hours.
RQ3. Can NEUZZ perform better than existing RNN-based
fuzzers?
Existing recurrent neural network (RNN)-based fuzzers learn
mutation patterns from past fuzzing experience to guide future
mutations [72]. These models ﬁrst learn mutation patterns
(composed of critical bytes) from a large number of mutated
inputs generated by AFL. Next, they use the mutation patterns
to build a ﬁlter to AFL which only allows mutations on critical
bytes to pass, vetoing all other non-critical byte mutations. We
choose 4 programs studied by the previous work to evaluate the
performance of NEUZZ compared to the RNN-based fuzzer for
1 million mutations. We train two NN models with the same
training data, then let the two NN-based fuzzers run to generate
1 million mutations and compare the new code coverage
achieved by the two methods. We report both the achieved
edge coverage and training time, as shown in Table VII.
TABLE VII: NEUZZ vs. RNN fuzzer w.r.t. baseline AFL
Programs
readelf -a
libjpeg
libxml
mupdf
Edge Coverage
NEUZZ RNN AFL NEUZZ
Training Time (sec)
RNN AFL
1,800
89
256
260
215
21
38
70
213
28
19
32
108
56
95
62
2,224
1,028
2,642
848
NA
NA
NA
NA
For all the four programs, NEUZZ signiﬁcantly outperforms
the RNN-based fuzzer on 1M mutations. NEUZZ achieves
8.4×, 4.2×, 6.7×, and 3.7× more edge-coverage than the RNN-
based fuzzer across the four programs respectively. In addition,
the RNN-based fuzzer has, on average, 20× more training
overhead than NEUZZ, because RNN models are signiﬁcantly
more complicated than feed-forward network models.
An additional comparison of the RNN-based fuzzer with
AFL shows that the former achieves 2× more edge coverage
on average than AFL on libxml and mupdf using the 1-hour
corpus. We also observe that the RNN-based fuzzer vetoes
around 50% of the mutations generated by AFL. Thus, the
new edge coverage of 1M mutations from RNN-based fuzzer
can achieve the edge coverage of 2M mutations in vanilla AFL.
This explains why the RNN-based fuzzer uncovers around 2×
more new edges of AFL on some programs. If AFL gets stuck
after 2M mutations, the RNN-based fuzzer would also get stuck
after 1M ﬁltered mutations. The key advantage of NEUZZ over
the RNN-based fuzzer is that NEUZZ obtains critical locations
using neural-network-based gradient-guided search, while the
RNN fuzzer tries to model the task in an end-to-end manner.
Our model can distinguish different contributing factors of
critical bytes that the RNN model may miss as demonstrated
by our experimental results. For mutation generation, we
perform an exhaustive search for critical bytes determined
by corresponding contributing factors, while the RNN-based
fuzzer still relies on AFL’s uniform random mutations.
Result 3: NEUZZ, a fuzzer based on simple feed-
forward network, signiﬁcantly outperforms the RNN-based
fuzzers by achieving 3.7× to 8.4× more edge coverage
across different projects.
RQ4. How do different model choices affect NEUZZ’s
performance?
NEUZZ’s fuzzing performance heavily depends on the
accuracy of the trained NN. As described in Section V, we
empirically ﬁnd that an NN model with 1 hidden layer is
expressive enough to model complex branching behavior of
real-world programs. In this section, we conduct an ablation
study by exploring different model settings for a 1 hidden
layer architecture, i.e., a linear model, an NN model without
reﬁnement, and an NN model with incremental reﬁnement. We
evaluate the effect of these models on NEUZZ’s performance.
To compare the fuzzing performance, we generate 1M muta-
(cid:25)(cid:18)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VIII: Edge coverage comparison of 1M mutations
generated by NEUZZ using different machine learning models.
Programs Linear Model NN Model NN + Incremental
readelf -a
libjpeg
libxml
mupdf
1,723
63
117
93
1,800
89
256
260
2,020
159
297
329
tions for each version of NEUZZ on 4 programs. We implement
the linear model by removing the non-linear activation functions
used in the hidden layer and thus making the whole feed-
forward network completely linear. The NN model is trained
same seed corpus from AFL. Next, We generate 1M mutations
from the passive learning model and measure the edge coverage
achieved by these 1M mutations. Finally, we ﬁlter out the
mutated inputs that exercise unseen edges from the 1 million
mutations and add these selected inputs to original seed corpus
to incrementally retrain another NN model and use it to generate
further mutations. The results are summarized in Table VIII.
We can see that both NN models (with or without incremental
learning) outperform the linear models for all 4 tested programs.
This shows that the nonlinear NN models can approximate
program behaviors better than a simple linear model. We also
observe that incremental learning helps NNs to achieve signif-
icantly higher accuracy and therefore higher edge coverage.
Result 4: NN models outperform linear models and
incremental learning makes NNs even more accurate over
time.
VII. CASE STUDIES OF BUGS
In this section, we provide samples of and analyze three
different types of bugs discovered by NEUZZ: integer overﬂow,
out-of-memory, and crash-inducing bugs.
We note that a large number of program bugs result from
incorrect handling of extreme values of variables. As NEUZZ
can enumerate all critical bytes from 0x00 to 0xff (see
Algorithm 1 line 3), we manage to ﬁnd a large number of bugs
caused by mishandled extrema. For example, NEUZZ is able
to ﬁnd many out-of-memory bugs in libjpeg, objdump,
nm and strip by setting the input bytes that affect memory
allocation size to extremely large values.
strip’s integer overﬂow. NEUZZ found an integer overﬂow
bug that can induce an inﬁnite loop on strip. Listing 2
shows a function in the strip program that parses every
section in the program header table of an input ELF ﬁle and
assigns all sections to a new program header table in the
output ELF ﬁle. The integer overﬂow occurs at the if-condition
in line 11 of Listing 2 as NEUZZ sets segment_size to an
extremely large value. Consequently, the program gets stuck in
an inﬁnite loop. We found that this bug exists in both the latest
version of Binutils 2.30 and in older versions 2.26 and 2.29.
libjpeg’s out-of-memory. During the JPEG compression
process, the data of every color space is down-sampled by
the corresponding sampling factor in order to reduce ﬁle size.
According to the JPEG standard, the sampling factor must
be an integer between 1 and 4. This value is used during
the decompression process to determine how much memory
needs to be allocated as shown in Listing 4. NEUZZ sets a
large value which causes too much memory to be allocated
for image data, causing a out-of-memory error. Such errors
can potentially be exploited to launch denial of service attacks
on servers using libjpeg for displaying images.
(saddr >= baddr
&& saddr output_section;
if(IS_CONTAINED(output_section,
...
isec++;
sections[j] = NULL;
...
segment_size, base_addr))
{
}
}
Listing 2: strip integer overﬂow
...
return TRUE;
filedata->section_headers = NULL;
...
if(filedata->file_header.e_shnum == 0)
{
1 // binutils-2.30/binutils/readelf.c:5901
2 static bfd_boolean
3 process_section_headers(Filedata* filedata)
4 {
5
6
7
8
9
10
11
12 }
13 // binutils-2.30/binutils/readelf.c:654
14 static Elf_Internal_Shdr *
15 find_section(Filedata* filedata, char* name)
16 {
17
18
19
20 }
...
assert(filedata->section_headers != NULL);
...
}
Listing 3: readelf section header parsing bug
1 // libjpeg/jmemmgr.c:444
2 alloc_barray(j_common_ptr cinfo, ...)
3 {
4
5
6
7
...
alloc_large((size_t) rowsperchunk
...
while (currow < numrows) {
* (size_t) blocksperrow * SIZEOF(JBLOCK));
...
8
9
10 }
}
Listing 4: libjpeg out-of-memory bug
readelf’s crash. An ELF ﬁle consists of a ﬁle header,
(cid:25)(cid:18)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
program header, section header and section data. According
to the ELF speciﬁcation, the ELF header contains the ﬁeld
e_shnum located at the 60th byte for a 64-bit binary, which
speciﬁes the number of sections in the ELF ﬁle. NEUZZ sets
the number of sections of the input ﬁle to be 0. As shown
in Listing 3, if the number of sections is equal to 0, the
implementation returns a NULL pointer which is dereferenced
by subsequent code, triggering a crash.
VIII. RELATED WORK
Program smoothing. Parnas et al.
[67] observed that
discontinuities are one of the fundamental challenges behind
the development of secure and reliable software. Chaudhury et
al. [21], [18], [19] suggested the idea of program smoothing to
facilitate program analysis and presented a rigorous smoothing
algorithm using abstract interpretation and symbolic execution.
Unfortunately, such algorithms incur prohibitive performance
overhead, especially for large programs. By contrast, our
smoothing technique leverages the learning power of NNs to
achieve better scalability.
Learning-based fuzzing. Recently, there has been increasing
interest in using machine learning techniques for improving
fuzzers [37], [72], [84], [9], [81], [12], [64]. However, existing
learning-based fuzzers model fuzzing as an end-to-end ML