We can express fα,β as a linear combination of the bit d = (a ≤ b):
To implement this classiﬁer privately, we compute(cid:74)d(cid:75) by comparing a and b, keeping the result encrypted with QR,
fα,β(d) = d · β + (1 − d) · α = α + d · (β − α).
and then changing the encryption scheme (cf. Section 4.3) to Paillier.
Then, using Paillier’s homomorphism and knowledge of α and β, we can compute an encryption of fα,β(d):
(cid:74)fα,β(d)(cid:75) =(cid:74)α(cid:75) ·(cid:74)d(cid:75)β−α.
10.1.2 Viola and Jones face detection
The Viola and Jones face detection algorithm [VJ01] is a particular case of an AdaBoost classiﬁer. Denote by X an
image represented as an integer vector and x a particular detection window (a subset of X’s coefﬁcients). The strong
classiﬁer H for this particular detection window is
(cid:32) t(cid:88)
(cid:33)
H(x) = sign
αihi(x)
where the ht are weak classiﬁers of the form hi(x) = sign ((cid:104)x, yi(cid:105) − θi) .
i=1
17
Data set
Model size
Breast cancer (2)
Credit (3)
30
47
Computation
Client
46.4 ms
55.5 ms
Server
43.8 ms
43.8 ms
Time per protocol
Compare Dot product
194 ms
194 ms
9.67 ms
23.6 ms
Total
running time
204 ms
217 ms
Comm.
Interactions
35.84 kB
40.19 kB
7
7
(a) Linear Classiﬁer. Time per protocol includes communication.
Data set
Breast Cancer (1)
Nursery (5)
Audiology (4)
Specs.
F
C
2
9
9
5
24
70
Computation
Time per protocol
Client
150 ms
537 ms
1652 ms
Server
104 ms
368 ms
1664 ms
Prob. Comp.
82.9 ms
82.8 ms
431 ms
Argmax
396 ms
1332 ms
3379 ms
Total
running time
479 ms
1415 ms
3810 ms
Comm.
Interactions
72.47 kB
150.7 kB
1911 kB
14
42
166
(b) Naïve Bayes Classiﬁer. C is the number of classes and F is the number of features. The Prob. Comp. column corresponds to the
computation of the probabilities p(ci|x) (cf. Section 6). Time per protocol includes communication.
Data set
Nursery (5)
ECG (6)
Tree Specs.
N
4
6
D
4
4
Computation
Time per protocol
FHE
Client
1579 ms
2297 ms
Server
798 ms
1723 ms
Compare
446 ms
1410 ms
ES Change
1639 ms
7406 ms
Eval.
239 ms
899 ms
Decrypt
33.51 ms
35.1 ms
Comm.
Interactions
2639 kB
3555 kB
30
44
(c) Decision Tree Classiﬁer. ES change indicates the time to run the protocol for changing encryption schemes. N is the number of
nodes of the tree and D is its depth. Time per protocol includes communication.
Table 6: Classiﬁers evaluation.
In our setting, Alice owns the image and Bob the classiﬁer (e.g. the vectors {yi} and the scalars {θi} and {αi}).
Neither of them wants to disclose their input to the other party. Thanks to our building blocks, Alice can run Bob’s
classiﬁer on her image without her learning anything about the parameters and Bob learning any information about her
image.
The weak classiﬁers can be seen as multiplexers; with the above notation, we have ht(x) = f1,−1((cid:104)x, yt(cid:105) − θt).
Using the elements of Section 10.1.1, we can easily compute the encrypted evaluation of every one of these weak
classiﬁers under Paillier, and then, as described in Section 8, compute the encryption of H(x).
10.2 Performance evaluation setup
Our performance evaluations were run using two desktop computers each with identical conﬁguration: two Intel Core
i7 (64 bit) processors for a total 4 cores running at 2.66 GHz and 8 GB RAM. Since the machines were on the same
network, we inﬂated the roundtrip time for a packet to be 40 ms to mimic real network latency. We used 1024-bit
cryptographic keys, and chose the statistical security parameter λ to be 100. When using HELib, we use 80 bits of
security, which corresponds to a 1024-bit asymmetric key.
10.3 Building blocks performance
We examine performance in terms of computation time at the client and server, communication bandwidth, and also
number of interactions (round trips). We can see that all these protocols are efﬁcient, with a runtime on the order of
milliseconds.
10.3.1 Comparison protocols
Comparison with unencrypted input. Table 3 gives the running time of the comparison protocol with unencrypted
input for various input size.
18
Comparison with encrypted input. Table 4 presents the performance of the comparison with encrypted inputs
protocols.
10.3.2 argmax
Figure 5 presents the running times and the communication overhead of the argmax of encrypted data protocol (cf.
Section 4.2). The input integers were 64 bit integers.
Figure 5: Argmax of encrypted data protocol evaluation. The bars represent the execution of the protocol when the comparisons are
executed one after each other, linearly. The line represents the execution when comparisons are executed in parallel, tree-wise.
10.3.3 Consequences of the latency on performances
It is worth noticing that for most blocks, most of the running time is spend communicating: the network’s latency has a
huge inﬂuence on the performances of the protocols (running time almost linear in the latency for some protocols). To
improve the performances of a classiﬁer implemented with our blocks, we might want to run several instances of some
building blocks in parallel. This is actually what we did with the tree-based implementation of the argmax protocol,
greatly improving the performances of the protocol (cf. Figure 5).
10.4 Classiﬁer performance
Here we evaluate each of the classiﬁers described in Sections 5–7. The models are trained non-privately using
scikit-learn6. We used the following datasets from the UCI machine learning repository [BL13]:
1. the Wisconsin Diagnostic Breast Cancer data set,
2. the Wisconsin Breast Cancer (Original) data set, a simpliﬁed version of the previous dataset,
3. Credit Approval data set,
4. Audiology (Standardized) data set,
5. Nursery data set, and
6http://scikit-learn.org
19
 0 1000 2000 3000 4000 5000 6000 7000456789101112131415161718192025303550Time (ms)ElementsParty AParty BCommunicationTree6. ECG (electrocardiogram) classiﬁcation data from Barni et al. [BFK+09]
These data sets are scenarios when we want to ensure privacy of the server’s model and client’s input.
Based on the suitability of each classiﬁer, we used data sets 2 and 3 to test the hyperplane decision classiﬁer, sets 1,
4 and 5 for the Naïve Bayes classiﬁer, and sets 5 and 6 for the decision tree classiﬁer.
Table 6 shows the performance results. Our classiﬁers run in at most a few seconds, which we believe to be practical
for sensitive applications. Note that even if the datasets become very large, the size of the model stays the same – the
dataset size only affects the training phase which happens on unencrypted data before one uses our classiﬁers. Hence,
the cost of our classiﬁcation will be the same even for very large data sets.
For the decision tree classiﬁer, we compared our construction to Barni et al. [BFK+09] on the ECG dataset (by
turning their branching program into a decision tree). Their performance is 2609 ms7 for the client and 6260 ms for the
server with communication cost of 112.2KB. Even though their evaluation does not consider the communication delays,
we are still more than three times as fast for the server and faster for the client.
10.5 Comparison to generic two-party tools
A set of generic secure two- or multi-party computation tools have been developed, such as TASTY [HKS+10] and
Fairplay [MNPS04, BDNP08]. These support general functions, which include our classiﬁers.
However, they are prohibitively slow for our speciﬁc setting. Our efﬁciency comes from specializing to classiﬁcation
functionality. To demonstrate their performance, we attempted to evaluate the Naïve Bayes classiﬁer with these. We
used FairplayMP to generate the circuit for this classiﬁer and then TASTY to run the private computation on the circuit
thus obtained. We tried to run the smallest Naïve Bayes instance, the Nursery dataset from our evaluation, which has
only 3 possible values for each feature, but we ran out of memory during the circuit generation phase on a powerful
machine with 256GB of RAM.
Hence, we had to reduce the classiﬁcation problem to only 3 classes (versus 5). Then, the circuit generation took
more than 2 hours with FairplayMP, and the time to run the classiﬁcation with TASTY was 413196 msec (with no
network delay), which is ≈ 500 times slower than our performance (on the non-reduced classiﬁcation problem with 5
classes). Thus, our specialized protocols improve performance by orders of magnitude.
11 Conclusion
In this paper, we constructed three major privacy-preserving classiﬁers as well as provided a library of building blocks
that enables constructing other classiﬁers. We demonstrated the efﬁciency of our classiﬁers and library on real datasets.
Acknowledgment
We thank Thijs Veugen, Thomas Schneider, and the anonymous reviewers for their helpful comments.
7In Barni et al. [BFK+09], the evaluation was run over two 3GHz computers directly connected via Gigabit Ethernet. We scaled the given results
2.3 to get a better comparison basis.
by 3
20
References
[AB06]
[AB07]
[AD01]
Shai Avidan and Moshe Butman. Blind vision. In Computer Vision–ECCV 2006, pages 1–13. 2006.
Shai Avidan and Moshe Butman. Efﬁcient methods for privacy preserving face detection. In Advances in
Neural Information Processing Systems, page 57, 2007.
Mikhail J Atallah and Wenliang Du. Secure multi-party computational geometry. In Algorithms and Data
Structures, pages 165–179. 2001.
[BDMN05] Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the sulq framework.
In Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database