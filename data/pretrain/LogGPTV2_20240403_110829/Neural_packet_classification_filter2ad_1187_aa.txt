title:Neural packet classification
author:Eric Liang and
Hang Zhu and
Xin Jin and
Ion Stoica
Neural Packet Classification
Eric Liang1, Hang Zhu2, Xin Jin2, Ion Stoica1
1UC Berkeley, 2Johns Hopkins University
PI:EMAIL, PI:EMAIL, PI:EMAIL, PI:EMAIL
ABSTRACT
Packet classification is a fundamental problem in computer network-
ing. This problem exposes a hard tradeoff between the computation
and state complexity, which makes it particularly challenging. To
navigate this tradeoff, existing solutions rely on complex hand-
tuned heuristics, which are brittle and hard to optimize.
In this paper, we propose a deep reinforcement learning (RL) ap-
proach to solve the packet classification problem. There are several
characteristics that make this problem a good fit for Deep RL. First,
many existing solutions iteratively build a decision tree by splitting
nodes in the tree. Second, the effects of these actions (e.g., splitting
nodes) can only be evaluated once the entire tree is built. These
two characteristics are naturally captured by the ability of RL to
take actions that have sparse and delayed rewards. Third, it is com-
putationally efficient to generate data traces and evaluate decision
trees, which alleviate the notoriously high sample complexity prob-
lem of Deep RL algorithms. Our solution, NeuroCuts, uses succinct
representations to encode state and action space, and efficiently
explore candidate decision trees to optimize for a global objective.
It produces compact decision trees optimized for a specific set of
rules and a given performance metric, such as classification time,
memory footprint, or a combination of the two. Evaluation on Class-
Bench shows that NeuroCuts outperforms existing hand-crafted
algorithms in classification time by 18% at the median, and reduces
both classification time and memory footprint by up to 3×.
CCS CONCEPTS
• Networks → Packet classification; • Theory of computation
→ Reinforcement Learning.
KEYWORDS
Packet classification, Reinforcement learning
ACM Reference Format:
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. 2019. Neural Packet Classifica-
tion. In SIGCOMM ’19: 2019 Conference of the ACM Special Interest Group on
Data Communication, August 19–23, 2019, Beijing, China. ACM, New York,
NY, USA, 14 pages. https://doi.org/10.1145/3341302.3342221
1 INTRODUCTION
Packet classification is one of the fundamental problems in com-
puter networking. The goal of packet classification is to match a
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5956-6/19/08. . . $15.00
https://doi.org/10.1145/3341302.3342221
given packet to a rule from a set of rules, and to do so while opti-
mizing the classification time and/or memory footprint. Packet clas-
sification is a key building block for many network functionalities,
including firewalls, access control, traffic engineering, and network
measurements [13, 29, 55]. As such, packet classifiers are widely
deployed by enterprises, cloud providers, ISPs, and IXPs [1, 29, 48].
Existing solutions for packet classification can be divided into
two broad categories. Solutions in the first category are hardware-
based. They leverage Ternary Content-Addressable Memories (TCAMs)
to store all rules in an associative memory, and then match a
packet to all these rules in parallel [23]. As a result, TCAMs provide
constant classification time, but come with significant limitations.
TCAMs are inherently complex, and this complexity leads to high
cost and power consumption. This makes TCAM-based solutions
prohibitive for implementing large classifiers [55].
The solutions in the second category are software based. These
solutions build sophisticated in-memory data structures—typically
decision trees—to efficiently perform packet classification [29].
While these solutions are far more scalable than TCAM-based so-
lutions, they are slower, as the classification operation needs to
traverse the decision tree from the root to the matching leaf.
Building efficient decision trees is difficult. Over the past two
decades, researchers have proposed a large number of decision tree
based solutions for packet classification [13, 29, 40, 47, 55]. However,
despite the many years of research, these solutions have two major
limitations. First, they rely on hand-tuned heuristics to build the tree.
Examples include maximizing split entropy [13], balancing splits
with custom space measures [13], special handling for wildcard
rules [47], and so on. This makes them hard to understand and
optimize over different sets of rules. If a heuristic is too general, it
cannot take advantage of the characteristics of a particular set of
rules. If a heuristic is designed for a specific set of rules, it typically
does not achieve good results on another set of rules with different
characteristics.
Second, these heuristics do not explicitly optimize for a given ob-
jective (e.g., tree depth). They make decisions based on information
(e.g., the difference between the number of rules in the children, the
number of distinct ranges in each dimension) that is only loosely
related to the global objective. As such, their performance can be
far from optimal.
In this paper, we propose a learning approach to packet classifica-
tion. Our approach has the potential to address the limitations of the
existing hand-tuned heuristics. In particular, our approach learns
to optimize packet classification for a given set of rules and objec-
tive, can easily incorporate pre-engineered heuristics to leverage
their domain knowledge, and does so with little human involve-
ment. The recent successes of deep learning in solving notoriously
hard problems, such as image recognition [22] and language trans-
lation [51], have inspired many practitioners and researchers to
apply deep learning, in particular, and machine learning, in general,
256
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica
to systems and networking problems [4, 6, 16, 33, 34, 54, 62, 64, 65].
While in some of these cases there are legitimate concerns about
whether machine learning is the right solution for the problem
at hand, we believe that deep learning is a good fit for our prob-
lem. This is notable since, when an efficient formulation is found,
learning-based solutions have often outperformed hand-crafted
alternatives [21, 36, 44].
There are two general approaches to apply learning to packet
classification. The first is to replace the decision tree with a neural
network, which given a packet will output the rule matching that
packet. Unfortunately, while appealing, this end-to-end solution has
a major drawback: it does not guarantee the correct rule is always
matched. While this might be acceptable for some applications
such as traffic engineering, it is not acceptable for others, such as
access control. Another issue is that large rule sets will require corre-
spondingly large neural network models, which can be expensive to
evaluate without accelerators such as GPUs. The second approach,
and the one we take in this paper, is to use deep learning to build a
decision tree. Recent work has applied deep learning to optimize
decision trees for machine learning problems [20, 38, 59]. These so-
lutions, however, are designed for machine learning settings that are
different than packet classification, and aim to maximize accuracy.
In contrast, decision trees for packet classification provide perfect
accuracy by construction, and the goal is to minimize classification
time and memory footprint.
Our solution uses deep reinforcement learning (RL) to build effi-
cient decision trees. There are three characteristics that makes RL
a particularly good fit for packet classification. First, the natural
solution to build a decision tree is to start with one node and recur-
sively split (cut) it. Unfortunately, this kind of approach does not
have a greedy solution. When making a decision to cut a node, we
do not know whether that decision was a good one (i.e., whether
it leads to an efficient tree) before we finish building the actual
tree. RL naturally captures this characteristic as it does not assume
that the impact of a given decision on the performance objective is
known immediately. Second, unlike existing heuristics which take
actions that are only loosely related to the performance objective,
the explicit goal of an RL algorithm is to directly maximize the
performance objective. Third, unlike other RL domains such as as
robotics, for our problem it is possible to evaluate an RL model
quickly (i.e., a few seconds of CPU time). This alleviates one of the
main drawbacks of RL algorithms: the non-trivial learning time due
to the need to evaluate a large number of models to find a good
solution. By being able to evaluate each model quickly (and, as we
will see, in parallel) we significantly reduce the learning time.
To this end, we design NeuroCuts, a deep RL solution for packet
classification that learns to build efficient decision trees. There
are three technical challenges to formulate this problem as an RL
problem. First, the tree is growing during the execution of the
algorithm, as existing nodes are split. This makes it very difficult
to encode the decision tree, as RL algorithms require a fixed size
input. We address this problem by noting that the decision of how
to split a node in the tree depends only on the node itself; it does not
depend on the rest of the tree. As such, we do not need to encode the
entire tree; we only need to encode the current node. The second
challenge is in reducing the sparsity of rewards to accelerate the
learning process; here we exploit the branching structure of the
257
(cid:13)(cid:22)(cid:19)(cid:21)(cid:22)(cid:19)(cid:24)(cid:25) (cid:14)(cid:22)(cid:18) (cid:12)(cid:13)
(cid:7)
(cid:6)
(cid:5)
(cid:6)(cid:5)(cid:3)(cid:5)(cid:3)(cid:5)(cid:3)(cid:5)
(cid:1)
(cid:1)
(cid:11)(cid:23)(cid:24) (cid:12)(cid:13)
(cid:6)(cid:5)(cid:3)(cid:5)(cid:3)(cid:5)(cid:3)(cid:5)(cid:4)(cid:6)(cid:9) (cid:1)
(cid:1)
(cid:1)
(cid:14)(cid:22)(cid:18) (cid:13)(cid:21)(cid:22)(cid:24)
(cid:16)(cid:5)(cid:2) (cid:6)(cid:5)(cid:7)(cid:8)(cid:17)
(cid:1)
(cid:11)(cid:23)(cid:24) (cid:13)(cid:21)(cid:22)(cid:24)
(cid:1)
(cid:16)(cid:5)(cid:2) (cid:6)(cid:5)(cid:7)(cid:8)(cid:17)
(cid:1)
(cid:13)(cid:22)(cid:21)(cid:24)(cid:21)(cid:18)(cid:21)(cid:20)
(cid:1)
(cid:15)(cid:10)(cid:13)
(cid:1)
Figure 1: A packet classifier example. Real-world classifiers
can have 100K rules or more.
problem to provide denser feedback for tree size and depth. The
final challenge is that training for very large sets of rules can take
a long time. To address this, we leverage RLlib [30], a distributed
RL library.
In summary, we make the following contributions.
• We show that the packet classification problem is a good fit for
reinforcement learning (RL).
• We present NeuroCuts, a deep RL solution for packet classifica-
tion that learns to build efficient decision trees.
• We show that NeuroCuts outperforms state-of-the-art solutions,
improving classification time by 18% at the median and reducing
both time and memory usage by up to 3×.
The code for NeuroCuts is open source and is available at
https://github.com/neurocuts/neurocuts.
2 BACKGROUND
In this section, we provide background on the packet classification
problem, and summarize the key ideas behind the decision tree
based solutions to solve this problem.
2.1 Packet Classification
A packet classifier contains a list of rules. Each rule specifies a
pattern on multiple fields in the packet header. Typically, these
fields include source and destination IP addresses, source and des-
tination port numbers, and protocol type. The rule’s pattern spec-
ifies which packets match the rule. Matching conditions include
prefix based matching (e.g., for IP addresses), range based match-
ing (e.g., for port numbers), and exact matching (e.g., for protocol
type). A packet matches a rule if each field in the packet header
satisfies the matching condition of the corresponding field in the
rule, e.g., the packet’s source/destination IP address matches the
prefix of the source/destination address in the rule, the packet’s
source/destination port number is contained in the source/destination
range specified in the rule, and the packet’s protocol type matches
the rule’s protocol type.
Figure 1 shows a packet classifier with three rules. The first rule
matches all packets with source address 10.0.0.1 and the destination
addresses sharing prefix 10.0.0.0/16. Other fields are unspecified
(i.e., they are (cid:2)) meaning that the rule matches any value in these
fields. The second rule matches all TCP packets with source and
destination ports in the range [0, 1023], irrespective of IP addresses
(as they are (cid:2)). Finally, the third rule is a default rule that matches
all packets. This guarantees that any packet matches at least one
rule.
Since rules can overlap, it is possible for a packet to match multi-
ple rules. To resolve this ambiguity, each rule is assigned a priority.
A packet is then matched to the highest priority rule. For example,
packet (10.0.0.0, 10.0.0.1, 0, 0, 6) matches all the three rules of the
Neural Packet Classification
SIGCOMM ’19, August 19–23, 2019, Beijing, China
(cid:14)(cid:6)
(cid:14)(cid:7)
(cid:14)(cid:8)
(cid:14)(cid:6)(cid:4) (cid:14)(cid:7)(cid:4)(cid:1)(cid:14)(cid:8)(cid:4)(cid:1)
(cid:14)(cid:9)(cid:4)(cid:1)(cid:14)(cid:10)(cid:4) (cid:14)(cid:11)
(cid:14)(cid:11)
(cid:14)(cid:7)(cid:4) (cid:14)(cid:9)(cid:4)
(cid:14)(cid:10)
(cid:14)(cid:6)(cid:4) (cid:14)(cid:7)(cid:4)
(cid:14)(cid:10)
(cid:14)(cid:7)(cid:4) (cid:14)(cid:8)(cid:4)
(cid:14)(cid:10)
(cid:14)(cid:7)(cid:4) (cid:14)(cid:10)(cid:4)
(cid:14)(cid:11)
(cid:2)
(cid:14)(cid:9)
(cid:14)(cid:10)
(cid:1)
(cid:2)(cid:15)(cid:3)(cid:1)(cid:13)(cid:15)(cid:17)(cid:21)(cid:18)(cid:27)(cid:1)(cid:17)(cid:22)(cid:15)(cid:26)(cid:26)(cid:20)(cid:19)(cid:20)(cid:18)(cid:25)(cid:5)
(cid:14)(cid:7)
(cid:14)(cid:9)(cid:4)
(cid:14)(cid:10)
(cid:14)(cid:6)(cid:4)
(cid:14)(cid:7)
(cid:14)(cid:10)
(cid:14)(cid:7)(cid:4)
(cid:14)(cid:8)
(cid:14)(cid:10)
(cid:14)(cid:7)(cid:4)
(cid:14)(cid:11)
(cid:14)(cid:10)(cid:4)
(cid:14)(cid:11)
(cid:2)(cid:16)(cid:3)(cid:1)(cid:12)(cid:18)(cid:17)(cid:20)(cid:26)(cid:20)(cid:24)(cid:23)(cid:1)(cid:27)(cid:25)(cid:18)(cid:18)(cid:5)
Figure 2: Node cutting.
packet classifier in Figure 1. However, since the first rule has the
highest priority, we match the packet to the first rule only.