where Si, Wi, and Gi are as deﬁned above, and Ui is the
duration of time the replica spends waiting for the next lazy
update.
Ri = Si + Wi + Gi + Ui
(6)
For each request, we experimentally measure the values of
the above performance parameters as described in Section 5.4.
The client handlers record the most recent l measurements
of these parameters in separate sliding windows in an in-
formation repository. The size of the sliding window, l,
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply. 
is chosen so as to include a reasonable number of recently
measured values, while eliminating obsolete measurements.
(d), we ﬁrst compute the probability mass
To evaluate F I
Ri
functions (pmf ) of Si and Wi based on the relative fre-
quency of their values recorded in the sliding window. We
then use the pmf of Si, the pmf of Wi, and the most re-
cently recorded value of Gi to compute the pmf of the
response time Ri as a discrete convolution of Si, Wi, and
Gi. For Gi, unlike the other parameters, we use its most
recently recorded value instead of its history recorded over
a period of time, because the gateway delay does not ﬂuc-
tuate as much as the other parameters do. The pmf of Ri
can then be used to compute the value of the distribution
(d). We follow a similar procedure to compute
function F I
Ri
(d), although in this case we also record a performance
F D
Ri
history of Ui and include the pmf of Ui in the convolution.
5.3. State-Based Replica Selection Algorithm
Algorithm 1 outlines the selection algorithm that enables
a client gateway to select a set of replicas that can together
meet the client’s QoS speciﬁcation, based on the prediction
made by the probabilistic models described above. The al-
gorithm uses the model’s prediction to select no more than
the number of replicas necessary to meet the client’s re-
sponse time constraint with the probability the client has
requested. This algorithm is executed in a distributed man-
ner by a client gateway when the client associated with it
performs a read-only request on a server object.
The model used by the algorithm makes use of the per-
formance information broadcast by a replica to estimate the
replica’s ability to meet a client’s QoS speciﬁcation. Since
the information repositories of the different clients may con-
tain almost identical performance histories for the replicas,
this may cause the clients to select the same or common
replicas. Hence, Algorithm 1 has been designed to select
the replica subset in such a way that it alleviates the occur-
rence of such ‘hot-spots,’ to achieve a more balanced uti-
lization of all the available replicas. It does this by using
information that is speciﬁc to a client-replica pair, in addi-
tion to the replica-speciﬁc performance information, as we
now describe.
The algorithm receives as input the QoS speciﬁcation
of the client and the list of secondary and primary repli-
cas, along with relevant information about them. For each
replica i, the algorithm receives the values of its immedi-
ate and delayed response time distribution functions, which
(d). For a primary replica i,
are denoted by F I
Ri
(d) is not used. The algorithm also receives the elapsed
F D
Ri
response time, erti, which is the duration that has elapsed
since a reply was last received by the client from replica i.
The response time distributions, which are computed from
the performance history as explained in Section 5.2, are spe-
ciﬁc to the individual replica and are nearly identical in all
(d) and F D
Ri
the client information repositories. However, the ert infor-
mation is not the same in all the repositories, as it is speciﬁc
to each client-replica pair. In addition, the algorithm also re-
ceives the staleness factor for the secondary replicas, which
is computed using Equation 4.
(d), F D
Ri
(d), erti > , staleFactor
imum probability of meeting this deadline
Algorithm 1 State-Based Replica Selection Algorithm
Require: V = maxCDFReplica.immedCDF() then
includeCDF(maxCDFReplica,
maxCD-
(d), F D
Ri
(d))
Ri
vance(sortedList)
⇐
else
17: includeCDF(replica, immedCDF, delayedCDF)
18: begin
19: if replica ∈ PrimaryGroup then
20:
21: else
22:
primCDF ⇐ primCDF * (1 - immedCDF)
secImmedCDF ⇐ secImmedCDF * (1 - immedCDF); secDelayed-
CDF ⇐ secDelayedCDF * (1 - delayedCDF)
secCDF ⇐ secImmedCDF * staleFactor + secDelayedCDF * (1 -
staleFactor)
23:
24: end if
25: if 1 - (primCDF * secCDF) ≥ Pc(d) then
26:
27: else
28:
29: end if
30: end
return true {found an acceptable replica set}
return false {need more replicas}
Since replicas may crash, our goal is to choose a set of
replicas that can meet a client’s time constraint with the
probability the client has requested, even when one of the
replicas in the selected set crashes while servicing the re-
quest. To do this, we propose that if we can choose a set of
replicas that can satisfy the timing constraint with the spec-
iﬁed probability despite the failure of the selected member,
m, that has the highest probability of meeting the client’s
deadline, then such a set should be able to handle the failure
of any other member in the set. In [5] we have provided a
formal justiﬁcation for this proposal. We now describe the
steps of Algorithm 1, which makes use of this proposal to
select the replicas to service the client.
The algorithm ﬁrst sorts the replicas in decreasing order
of their elapsed response times, ert. This allows the clients
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply. 
Ri
(d) and F D
to favor the selection of replicas that it used least recently
and thereby obviate the hot-spot problem mentioned above.
Replicas that have the same value of ert are sorted in de-
creasing order of the values of their distribution functions.
The algorithm traverses the replica list in sorted order, in-
cluding each visited replica in the candidate set K, until
it includes enough replicas in K such that the terminating
condition PK(d) ≥ Pc(d) is satisﬁed. The function in-
(d), which
cludeCDF() uses the values of F I
Ri
it receives as inputs, to compute the value of PK(d) accord-
ing to Equation 1. The function then tests the terminating
condition in Line 25 and returns true if the condition is sat-
isﬁed, indicating that an appropriate replica subset has been
found. Notice that when evaluating PK(d), we exclude the
response time distribution of the selected member, max-
CDFReplica, that has the highest probability (among the
selected members) of responding by the requested deadline.
This exclusion in effect simulates the failure of the replica
with the highest probability of meeting the client’s dead-
line among the selected replicas, and therefore allows us to
choose a set K that can tolerate a single replica failure, as
proposed above. Finally, the selected set K is extended to
include the sequencer.
5.4. Online Performance Monitoring
We now explain some of the main implementation de-
tails of how the client and server gateway protocols interact
to measure and record the different performance parameters
that are used to compute the distribution function and stale-
ness factor. When a client makes a request to a service, the
client-side handler transparently intercepts the request and
records the interception time, t0. The handler makes use
of the performance history recorded in its local information
repository to select a set of replicas based on the client’s
QoS speciﬁcation, as explained in Section 5.3. The han-
dler then multicasts the request to the selected set of replicas
through the Maestro-Ensemble group communication layer.
Upon receiving the request from the client, the server-
side gateway handler delivers the read or update request to
the server application, after processing it according to the
sequential consistency gateway protocol described in Sec-
tion 4. We instrumented the gateway handler so that it can
record the service time, ts, and queuing time, tq, for the re-
quest. In addition, if a replica performs a deferred read, it
records the duration of time, tb, for which it buffered the
request until the next lazy update. When the server sends
its response to the client, the server handler intercepts the
response and piggybacks t1 = ts + tq + tb in the response
message. Each server handler also publishes the newly mea-
sured values of ts, tq, and tb to all of its clients whenever it
completes servicing a read request. All of this information,
published by the server replicas, is used by the client to up-
date its gateway information repository.
When the client handler receives a reply from a replica,
it records the time of reception, tp, in its information repos-
itory. This is used by the client to compute the elapsed re-
sponse time for the replica, when the client executes Algo-
rithm 1 to sort the replicas for its next read request. The
client uses the piggybacked information, t1, to record the
new value of the two-way gateway-to-gateway delay, tg, be-
tween the client and the replica. This delay, tg, is given by
tg = tp − tm − t1, where tm is the time at which the client
handler transmitted the request to the selected set of replicas
using Maestro-Ensemble.
If the reply is the ﬁrst one it has received for a request,
the client handler delivers the reply to the client. The timing
failure detector in the client handler computes the response
time, tr = tp − t0, to check whether a timing failure has
occurred. A timing failure occurs if tr > d, where d is the
response time requested by the client. The timing failure
detector maintains a counter that keeps track of the number
of times the client has failed to receive a timely response
from a service.
If the frequency of timely response from
the service is lower than the minimum probability of timely
response the client has requested in its QoS speciﬁcation,
the client handler notiﬁes the client by issuing a callback.
5.4.1. Measuring the Staleness. From Equation 4, we infer
that if a client gateway knows the arrival rate of the update
requests (λu) and the time elapsed since the last lazy up-
date (tl), then it can determine whether a secondary replica
has a state that can meet the staleness threshold speciﬁed
by the client. To measure the values of these parameters,
the server that is designated as the lazy publisher broadcasts
the following additional information when it publishes its
performance measurements to the clients: 1) ,
where nu is the number of update requests the lazy pub-
lisher has received from the clients in the duration tu, which
is the time elapsed since the publisher’s last performance
broadcast, and 2) , where nL is the number
of update requests the lazy publisher has received from the
clients in the duration tL, which is the time elapsed since
the lazy publisher propagated its last lazy update. The client
handlers record in their information repositories the most
recently published value of  and a history of
 over a sliding window. The arrival rate is com-
puted as λu =
u, where the sum is taken over the
sliding window. At the time of request transmission t, the
duration elapsed since the last lazy update is computed as
tl = (tL + tz) modulo TL, where TL is the periodicity with
which the lazy updates are propagated. tz is the duration
of time that has elapsed since the client received the most
recent performance broadcast from the lazy publisher, rela-
tive to t. Note that in order to collect any of the timing data
as explained above, it is not necessary to synchronize the
clocks across the machines, because we always measure the
two end-points of a timing interval on the same machine.
ni
u
(cid:1)
/ti
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply. 
)
s
c
e
s
o
r
c
m
i
(
d
a
e
h
r
e
v
O
m
h
l
t
i
r
o
g
A
n
o
i
t
c
e
e
S
l
1300
1200