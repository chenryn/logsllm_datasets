Both fragmentation and ambiguity can exist. When the default
exclusive-IP setting is applied, Data-link acts just like veth pairs
in Ubuntu and epair in BSD to provide the isolated network stack
where sockets are virtualized.
Ambiguity exists. System V IPC objects are naturally isolated and
two System V IPC objects can have the same ID/name.
The same provenance effect as that in Table 2 will occur for zones.
These do not affect dataﬂow in practice and thus do not directly
impact provenance.
can effectively track the origins of a container microservice
attack as well as assess the forensic impact of such attacks.
For example, was the effect of the attack limited to the speciﬁc
container or was it used as a stepping stone to other container
targets? To effectively answer such questions, we need to de-
mystify the boundary of containers in the provenance graph.
Initialization of containers is another frequent activity that
explodes provenance graphs and may be abstracted to sim-
plify analysis. Thus, if we can accurately identify subgraphs
corresponding to initialization of each containers, we can pro-
duce simpliﬁed provenance graphs, effectively reducing the
effort for forensic analysts by automatically annotating ab-
normal cross-container activity. Speciﬁcally, we investigate
the container initialization regulation of several representa-
tive container engines, including Docker, rkt and LXC, and
summarize the patterns observed in each of them.
4 System Design and Implementation
In this section, we provide a detailed description of the
CLARION prototype design and the implementation that ex-
tends the SPADE provenance tracking system with additional
container-speciﬁc extensions. Our design goal is to propose a
solution for addressing soundness and clarity challenges by
only using trusted information from the kernel while limiting
extra instrumentation.
We claim that our solution is complete in handling all
aliasing introduced by namespaces. First, we cover all system
calls that can be used to manipulate namespaces generally, i.e.,
clone, unshare and setns. We investigate their semantics
and provide associated provenance data models with con-
sideration to different argument combinations as shown in
Section 4.2. Second, we analyze all existing namespaces and
understand what information will be aliased in the low-level
audit and cause problems to provenance tracking as shown in
Section 3.1. Our solution is designed to address all introduced
problems below in Section 4.1.
Table 5: Namespace Provenance Mapping Strategies
Strategy
Namespace
Affected Provenance Data
PID
Mount
Network
IPC
Process IDs
File paths
Local IP addresses and ports
IPC Obejct IDs and names
Host-container mapping
Host-container mapping
Host-container mapping
Namespace labeling
4.1 Mapping Virtualized Namespaces
We summarize our virtualized namespace-mapping strategies
in Table 5. For the soundness challenge, we establish a host-
container mapping view on provenance graph artifacts that
are impacted by most Linux namespaces because we believe
this will provide the most clear view for users to understand
the provenance. However, for the IPC namespace, the host
view of an IPC object does not actually exist. Hence, we adopt
a namespace-labeling approach.
4.1.1 PID Namespace
We considered multiple options to tackle the PID host-
container mapping problem including: (i) directly using
PPID (parent PID) to connect processes; (ii) using times-
tamps to map cloned child processes to its parent; (iii) using
/proc/PID/status for mapping information; and (iv) using
kernel module injection to get the PID mapping from kernel
data structures.
We ultimately eliminated other options and chose to imple-
ment a kernel module for several reasons. We found that di-
rectly using PPID was infeasible because it sometimes points
to the parent of the the process creating it. For the timestamp
option, the granularity provided by audit record cannot guar-
antee that the order of process creation matches the order
corresponding system call events. We also decided against us-
ing /proc/PID/status information as the /proc ﬁlesystem
does not support asynchronous callbacks and the overhead of
polling is prohibitive.
We implement our PID namespace host-container map-
ping solution as a kernel module that intercepts process-
manipulation-related system calls, e.g., clone, fork, and
USENIX Association
30th USENIX Security Symposium    3995
Annotation
Table 6: Operator Annotation
Explanation
put((key,value), X)
put a pair (key,value) in a mapping X
get(key, X)
get the value from a mapping Y given the key
vfork. Once those system calls are invoked by a process,
we do not directly use the return value to determine the PID
of its child process because it can be virtualized. Instead, we
input this return value to a kernel helper function pid_nr()
in /include/linux/pid.h to generate the global PID. Ultimately,
we use the global PID to generate the sound provenance graph.
However, we still capture both the global PID and virtualized
PID for every process vertex such that a complete view can
be provided.
4.1.2 Mount Namespace
To obtain the host-container mapping for ﬁle paths virtual-
ized by containers, we leverage an empirically derived de-
sign principle about the mount namespace, that is consis-
tent across state-of-the-art container engines, to develop an
instrumentation-free solution.
This empirical design principle is that the newly created
mount namespace needs the init process, i.e., the process
with virtual PID 1, to provide a new ﬁlesystem view different
from that in the parent mount namespace. It is achieved by us-
ing root directory change system calls, i.e., pivot_root and
chroot, where new root directories are provided in their ar-
guments. Speciﬁcally, state-of-the-art container engines make
the init process move CWD to the root directory of a new
container by using chdir(container_root_path) and then
invoke a pivot_root(‘.’,‘.’) or a chroot(‘.’) to wrap
up the root directory change.
Therefore, if we monitor those root directory change system
calls, we can use the CWD record associated with the chdir
to ﬁnd the host path of the container root directory, and then
we attach this host path to every virtualized path as a preﬁx
to establish the host-container mapping on ﬁle paths. Given
the annotation in Table 6, the algorithm is described as four
steps.
Step 1. Handle chdir. (input: PID ‘p1’, CWD ‘cwd1’; op-
eration: put((p1,cwd1), LastCWD)). We do this to record the
last working directory for every process. With this informa-
tion we can know what is the last CWD of the ﬁrst process
inside a new container, which will be the preﬁx for every
virtualized path.
Step 2. Handle pivot_root or chroot. (input: PID ‘p1’;
operation: put((p1, get(p1, LastCWD)), Preﬁx)). When a root
directory changing system call occurs, we label the corre-
sponding process with the last CWD as the preﬁx.
Step 3. Handle virtualized PATH records, CWD records
and arguments related to ﬁle operation system calls with path
preﬁx. (input: PID ‘p1’, syscall ‘s1’, operation: if ‘s1’ is
‘open’,‘read’,‘write’ etc. Use get(p1, Preﬁx) to add a new
annotation ‘nsroot’ representing the host preﬁx in the cor-
responding artifacts). This helps propagate the preﬁx from
processes to ﬁle artifacts.
Step 4. Handle (clone, fork, vfork). (input: Parent PID
‘p1’, Child PID ‘p2’; operation: put((p2, get(p1, Preﬁx)), Pre-
ﬁx)). The preﬁx (root directory) information will be propa-
gated through process creation as kernel does.
We consider our mount namespace mapping solution to
be robust because it relies on a standardized implementation
technique for ﬁlesystem isolation and empirically validate its
adoption across representative container engines including
Docker, rkt and LXC.
For other cases where directories are shared between host
and container than chroot-like cases, we claim that our solu-
tion still works well. Taking bind mount as an example, the
key components in the bind mount provenance graph will be
one process vertex which executes a mount system call along
with two ﬁle artifacts representing the bound directories and
two ﬁle artifacts are connected by an edge representing that
mount system call. In this case, only the ﬁle path of the ﬁle
artifact inside the container will be affected and our solution
can still provide the host view of this ﬁle.
4.1.3 Network Namespace
For accurate provenance tracking of container network activ-
ity, CLARION needs to establish the host-container mapping
for virtualized local IP addresses and ports. To this end, we de-
sign a Netﬁlter-based solution for tracking the host-container
IP/port mapping and use the network namespace ID as a dis-
tinguisher. Netﬁlter is a Linux-kernel framework that provides
hooks to monitor every ingress and egress packet, including
packets from or to containers, on the host network stack [19].
The host network stack will do a source NAT for container
egress packets and a destination NAT for container ingress
packets before correctly forwarding those packets. Therefore,
by monitoring the IP/port NAT about container ingress/egress
packets on the host network stack, we can build the host-
container mapping of local IP addresses and ports for sockets
inside containers. We annotate each network socket artifact
with the corresponding network namespace identiﬁer, so sock-
ets from different containers can be reliably distinguished.
The CLARION prototype implementation for the net-
work namespace consists of two parts: network namespace
identiﬁcation and netﬁlter-based address mapping. For net-
work namespace identiﬁcation, we modify SPADE’s ker-
nel module to intercept network-related system calls and
put the network namespace identiﬁer of the calling process
on the generated network socket. For netﬁlter-based map-
ping, we register kernel modules at the beginning and the
end of netﬁlter hooks corresponding to NAT. Speciﬁcally,
POST_ROUTING and LOCAL_INPUT are used for source
NAT, while PRE_ROUTING and LOCAL_OUTPUT are used
for destination NAT. The former two hooks provide the map-
ping for egress connections from container and the latter two
3996    30th USENIX Security Symposium
USENIX Association
Figure 7: Handling the clone system call: a process vertex
representing the child will be created with the new namespace
label.
provide the mapping for ingress connections.
Whenever a new mapping is added, we will search for
the network device having the virtualized local IP address
in the new mapping, by iterating through network names-
paces using the function ip_dev_find(struct net *net,
__be32 addr). Through this, we ﬁnd the container related
to this virtualized local IP address and put the mapped global
local IP address/port on the socket artifact that has the virtual-
ized local IP address/port in the new mapping. As a special
case, a socket may listen on 0.0.0.0 (IN_ADDR_ANY),
i.e., it can accept connection on any local IP address. Hence,
when we match socket artifacts with the virtualized local IP
address/port in the container, we always treat 0.0.0.0 as a
matched local IP and only check the local port.
4.1.4 IPC Namespace
The issue in the IPC namespace is that two different IPC
objects from different IPC namespaces may have the same
ID/name. Unlike other namespaces, the host-container map-
ping strategy for disambiguation does not extend to IPC object
artifacts, because there is no corresponding host IPC object
for virtualized IPC objects. Our design involves adding an IPC
namespace ID to every IPC object artifact so that IPC objects
from different containers can be uniquely distinguished.
The implementation of the IPC namespace solution was
effected by adding IPC namespace IDs to IPC objects affected
by namespace virtualization. Those objects consist of the
POSIX message queue and all System V IPC objects, i.e.,
message queue, semaphore, and shared memory. We assign
and propagate IPC namespace ids by carefully interpreting
process management system calls, e.g., clone, and IPC object
management system calls, e.g., msgget and msgsnd.
4.2 Essential Container Semantic Patterns
To address the clarity challenge, we propose two essential
container semantics which can signiﬁcantly improve the qual-
ity of provenance graph. In addition, we design the semantic
patterns for summarizing them during provenance tracking.
4.2.1 Boundary of Containers
We begin by ﬁrst providing a practical deﬁnition for a con-
tainer at runtime. A container at runtime is a set of processes
that share the same PID namespace. Usually processes inside
a container can share multiple namespaces but, most critically,
they at least have to share the PID namespace. In fact, while
container runtimes often provide support for sharing other
Figure 8: Handling the unshare and setns system calls on
NEWPID: a process vertex representing the calling process
itself will be created with the new assigned pid_for_children
label.
Figure 9: Handling unshare and setns system calls with
other ﬂags: a process vertex representing the calling process
itself will be created with the new assigned namespace label.
namespaces, e.g., mount, IPC, and network, between contain-
ers, none of them allow for sharing the PID namespace.
Next, we deﬁne the relationship between an artifact, e.g.,
ﬁle and network, and a container. An artifact relates to a
container if and only if it can be accessed by a process in-
side that container. Here, "accessed" may refer to any type of
read-write operation. An artifact may relate to several con-
tainers and thus may be used to infer the relationship between
speciﬁc containers. An important challenge is labeling each
process with the correct namespace identiﬁer. We address
this by carefully designing a new provenance data model for
system calls related to namespace operations. There are three
essential system calls for tracking the boundary of containers,
i.e., clone, unshare and setns. Clone and unshare sys-
tem calls are used for creating new namespaces; thus, they
signal the process of creating a container boundary. Setns
is used for aggregating two namespace together or making
another process join a namespace.
We designed ﬁve different namespace labels (correspond-
ing to PID, mount, network, IPC, and pid_for_children) and
handle them when three essential namespace-related system
calls (i.e., clone, unshare, and setns) occur, as shown in
Figure 7, 8 and 9. All ﬁgures are illustrated in the OPM prove-
nance data model format. The red areas highlight the changes
between before and after. The implementation follows the
Linux Kernel semantics for each system call and each names-
pace. The special case here is that if CLONE_NEWPID ﬂag
is speciﬁed for unshare or setns process, this only affects
the child process generated by the calling process but does not
affect the calling process itself. By adding namespace labels
and handling namespace-related system calls, CLARION is
able to capture the namespace information for every single
process and leverage the PID namespace label to certify the
boundary of each container.
USENIX Association
30th USENIX Security Symposium    3997
ns_pid: A0ns_pid_for_children: A0ns_mnt: C0ns_net: D0ns_ipc: E0pid: F0cloneﬂag: CLONE_NEWPID |CLONE_NEWNS |CLONE_NEWNET |CLONE_NEWIPCns_pid: A1ns_pid_for_children: A1ns_mnt: C1ns_net: D1ns_ipc: E1pid: F1ns_pid: A0ns_pid_for_children: A0ns_mnt: C0ns_net: D0ns_ipc: E0pid: F0unshare | setnsﬂag: CLONE_NEWPIDns_pid: A0ns_pid_for_children: B0ns_mnt: C0ns_net: D0ns_ipc: E0pid: F0ns_pid: A0ns_pid_for_children: A0ns_mnt: C0ns_net: D0ns_ipc: E0pid: F0unshare | setnsﬂag: CLONE_NEWNS |CLONE_NEWNET |CLONE_NEWIPCns_pid: A0ns_pid_for_children: A0ns_mnt: C1ns_net: D1ns_ipc: E1pid: F0Figure 10: CVE 2019-5736: Provenance graph for 1st start without (top) and with (bottom) namespace/container awareness
4.2.2 Initialization of Containers
By analyzing several state-of-the-art container engines, we
ﬁnd that speciﬁc common pattern exist across containers that
may be leveraged to identify the initialization of containers.
In a nutshell, this pattern can be summarized as follows: start
with an unshare/clone with new namespace ﬂag speciﬁed,
and end with an execve so that a new application can be
launched inside the container. Slight differences exist across
different container engines as described in Section 4. Identi-