0
ls
7
5
4
0
0
0
0
0
less
2
3
1
0
0
0
0
0
cd
ls
less
emacs
gcc
gdb
mkdir
cp
emacs
0
0
0
0
0
0
0
0
gcc
0
0
0
0
0
0
0
0
gdb mkdir
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
cp
0
0
0
0
0
0
0
0
Fig. 4. Co-occurrence matrix of User1
lation between events, we construct a so-called co-occurrence matrix by counting the
occurrence of every event pair within a certain distance (scope size). Thus, the correla-
tions of both connected and non-connected events are captured for every event pair and
subsequently represented in the matrix.
We deﬁne MX as the co-occurrence matrix of a sequence X (= x1, x2, x3, . . . , xl) with
length l. We deﬁne the unique events appearing in the sequence as a set of observation
events, denoted as O (= o1, o2, o3, . . . , om). In the example dataset of Figure 2, O is cd
ls less emacs gcc gdb mkdir cp. The correlation between the ith and jth events
in MX, oi and o j, is computed by counting the number of occurrences of the event-
pair within a scope size of s. Here, we did not change the strength of the correlations
between events depending on their distance, but instead used a constant value 1 for
simplicity. Doing this for every event pair generates a matrix representing all of the
respective occurrences. Each element in the matrix represents the perceived strength of
correlation between two events. For example, as illustrated in Figure 3, the events ls
and less are correlated with a strength of three when s and l are deﬁned as 6 and 10,
respectively. Figure 4 shows the matrix generated from the sequence of User1.
2.2 Extracting the Principal Features
As explained earlier, to distinguish a malicious user from a normal user, it is nec-
essary to introduce a pattern classiﬁcation method. Measuring the distance between
co-occurrence matrices is considered the simplest pattern classiﬁcation method. A co-
occurrence matrix is highly dimensional, however, and to make an accurate comparison,
it is necessary to extract the matrix’s principal features.
The ECM method uses principal component analysis (PCA) to extract the principal
features, so-called feature vectors. PCA transforms a number of correlated variables
228
Mizuki Oka et al.
into a smaller number of uncorrelated variables called principal components. It can thus
reduce the dimensionality of the dataset while retaining most of the original variability
within the data. The process for obtaining a feature vector is divided into the following
ﬁve steps:
(Step 1) Take a domain dataset and convert its sequences to co-occurrence matrices:
As a ﬁrst step (Step 1 in in Figure 1), we take a set of sample sequences, which we call
a domain dataset and denote as D, and convert the sequences into corresponding co-
occurrence matrices, M1, M2, M3, ..., Mn, where n is the number of sample observation
sequences and M is an m × m matrix (m: number of observation events). In the current
example, the domain dataset consists of all the three users’ sequences (n = 3), and M is
an 8 × 8 matrix (m = 8).
(Step 2) Subtract the mean: We then take the set of co-occurrence matrices M1, M2,
M3, ..., Mn and compute its mean co-occurrence matrix Mmean (Step 2 in Figure 1). Here
we introduce two diﬀerent ways to compute Mmean. The ﬁrst way is to compute it nor-
mally:
Mmean =
1
n
n(cid:7)
k=1
Mk.
(1)
The second way is to compute Mmean by taking into account the fact that a co-
occurrence matrix can be sparse. Let mmean(i, j) be the ith-row jth-column element of
the mean co-occurrence matrix Mmean. We then compute mmean(i, j) by taking the sum
of all the values in m1(i, j), m2(i, j), m3(i, j), . . . , mn(i, j) and dividing by the number of
those values that are non-zero. In summary,
mmean(i, j) =
1
K(i, j)
n(cid:7)
k=1
mk(i, j),
(2)
where mk(i, j) is the ith-row jth-column element of the kth co-occurrence matrix, and
K(i, j) and δ[x] are deﬁned as
and
K(i, j) =
n(cid:7)
k=1
δ[mk(i, j)]
δ[x] =
1 if x is not equal to zero
0 otherwise
,
(3)
(4)
respectively. The mean co-occurrence matrix Mmean is then subtracted from each event
co-occurrence matrix,
Ak = Mk − Mmean
for k = 1, 2, 3, . . . , n,
(5)
where Ak is the kth co-occurrence matrix with the mean subtracted.
Anomaly Detection Using Layered Networks Based on Eigen Co-occurrence Matrix
229
(Step 3) Calculate the covariance matrix: We then construct the covariance matrix as
P =
n(cid:7)
k=1
ˆAk ˆAk
T ,
(6)
where ˆAk is created by taking each row in Ak and concatenating its elements into a single
vector (Step 3 in Figure 1). The dimension of ˆAk is 1 × m2. In the example dataset, the
dimension of ˆAk is 1 × 64.
The components of P, denoted by pi j, represent the correlations between two event
pairs qi and q j, such as the event pairs (ls less) and (ls cd) in the example dataset. An
event pair qi (= ox, oy) can be obtained by
x = γ[(i − 1)/m] + 1
y = i − γ[(i − 1)/m] × m,
(7)
where γ[z] is the integer part of the value. The variance of a component indicates the
spread of the component values around its mean value. If two components qi and q j
are uncorrelated, their variance is zero. By deﬁnition, the covariance matrix is always
symmetric.
(Step 4) Calculate the eigenvectors and eigenvalues of the covariance matrix: Since
the covariance matrix P is symmetric (its dimension is m2 × m2, or 64 × 64 in the
example dataset), we can calculate an orthogonal basis by ﬁnding its eigenvalues and
eigenvectors (Step 4 in Figure 1). The eigenvector with the highest eigenvalue is the
ﬁrst principal component (the most characteristic feature) since it implies the highest
variance, while the eigenvector with the second highest eigenvalue is the second prin-
cipal component (the second most characteristic feature), and so forth. By ranking the
eigenvectors in order of descending eigenvalues, namely (v1, v2, ..., vm2), we can create
an ordered orthogonal basis according to signiﬁcance. Since the eigenvectors belong to
the same vector space as the co-occurrence matrices, vi can be converted to an m × m
matrix (8 × 8 in the example dataset). We call such a matrix an Eigen co-occurrence
matrix and denote it as Vi.
Instead of using all the eigenvectors, we may represent a co-occurrence matrix by
choosing N of the m2 eigenvectors. This compresses the original co-occurrence matrix
and simpliﬁes its representation without losing much information. We deﬁne these N
eigenvectors as the co-occurrence matrix space. Obviously, the larger N is, the higher
the contribution rate of all the eigenvectors becomes. The contribution rate is deﬁned as
contribution rate =
where λi denotes the ith largest eigenvalue.
(cid:12)
(cid:12)
N
i=1
m2
i=1
,
λi
λi
(8)
(Step 5) Obtain a feature vector: We can obtain the feature vector of any co-occurrence
matrix, M, by projecting it onto the deﬁned co-occurrence matrix space (Step 5 in Figure
230
Mizuki Oka et al.
ls
cd
less
Layer 1
ls
cd
cp
Layer 2
Fig. 5. Positive layered network for User1
less
ls
cd
cp
Layer 1
Layer 2
Fig. 6. Combined positive layered network
of User1. The solid lines and dotted lines
correspond to layer 1 and 2, respectively
1). The feature vector FT = [ f1, f2, f3, ..., fN] of M is obtained by the dot product of
vectors vi and ˆA, where fi is deﬁned as
fi = vT
i
ˆA
for i = 1, 2, 3, . . . , N
(9)
The Components f1, f2, f3, ..., fN of F are the coordinates within the co-occurrence ma-
trix space. Each component represents the contribution of each respective Eigen co-
occurrence matrix. Any input sequence can be compressed from m2 to N while main-
taining a high level of variance.
2.3 Constructing a Layered Network
Once a feature vector F is obtained from a co-occurrence matrix, the ECM method
converts it to a so-called layered network (shown as construction of layered network in
Figure 1) . The ith layer of a network is constructed from the corresponding ith Eigen
co-occurrence matrix Vi multiplied by the ith coordinate fi of F. In other words, the ith
layer of the network represents the ith principal feature of the original co-occurrence
matrix.
The layered network can be obtained from equation (9). Recall that this equation
for obtaining a component fi (for i = 1, 2, 3, . . . , N) of a feature vector is
fi = vT
i
ˆA
for i = 1, 2, 3, . . . , N,
where ˆA is the vector representation of A = (M − Mmean). We can obtain an approxima-
tion to the original co-occurrence matrix M
with the mean Mmean subtracted from the
original co-occurrence matrix M by isolating ˆA from equation (9). In summary,
(cid:3)
(M − Mmean) (cid:4) N(cid:7)
fiVi = M
(cid:3),
i=1
(10)
where fiVi can be considered an adjacency matrix labeled by the set of observation
events O. The ith network layer can be constructed by connecting the elements in the
obtained matrix M
(cid:3)
.
Anomaly Detection Using Layered Networks Based on Eigen Co-occurrence Matrix
231
gdb
gcc
ls
emacs
Layer 1
gdb
gcc
ls
emacs
Layer 2
Fig. 7. Negative layered network for User1
gdb
gcc
ls
emacs
Layer 1
Layer 2
Fig. 8. Combined negative layered network
for User1. The solid and dotted lines corre-