I am currently running a Kubernetes (k8s) cluster on AWS, and I have successfully set up the environment using `kube-up.sh`. I can deploy and run a simple Rails application (including a controller and service). However, when I return to the cluster the next day, I encounter issues where the nodes are in a `NotReady` state, and the pods are stuck in a `Pending` state.

### Node Status
```bash
$ kubectl get nodes
NAME                                         LABELS                                                              STATUS
ip-172-20-0-196.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-196.us-west-2.compute.internal   NotReady
ip-172-20-0-197.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-197.us-west-2.compute.internal   NotReady
ip-172-20-0-198.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-198.us-west-2.compute.internal   NotReady
ip-172-20-0-199.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-199.us-west-2.compute.internal   NotReady
```

### Pod Status
```bash
$ kubectl get pods
NAME                           READY     STATUS    RESTARTS   AGE
chronic-controller-16uic       0/1       Pending   0          6h
chronic-controller-6x025       0/1       Pending   0          1h
chronic-controller-x7owf       0/1       Pending   0          6h
private-registry-pbvdi         0/1       Pending   0          6h
tripplanner-controller-3q6km   0/1       Pending   0          6h
tripplanner-controller-bhwwy   0/1       Pending   0          1h
tripplanner-controller-fdep8   0/1       Pending   0          6h
```

### Recent Events
```bash
$ kubectl get events
FIRSTSEEN                         LASTSEEN                          COUNT     NAME                                         KIND      SUBOBJECT   REASON             SOURCE                                                 MESSAGE
Sat, 12 Sep 2015 22:44:10 -0500   Tue, 15 Sep 2015 15:04:08 -0500   44        ip-172-20-0-199.us-west-2.compute.internal   Node                  NodeNotReady       {kubelet ip-172-20-0-199.us-west-2.compute.internal}   Node ip-172-20-0-199.us-west-2.compute.internal status is now: NodeNotReady
Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:41 -0500   7531      chronic-controller-ljzn6                     Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:41 -0500   7757      private-registry-ysjfj                       Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7284      tripplanner-controller-7b0yr                 Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7413      tripplanner-controller-s86wz                 Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7374      chronic-controller-f2nrx                     Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7447      chronic-controller-j4538                     Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
Mon, 14 Sep 2015 01:42:38 -0500   Tue, 15 Sep 2015 15:22:42 -0500   9303      tripplanner-controller-04ztj                 Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
```

### Additional Information
- I attempted to log into the master and one of the minions to inspect `/var/log/kubelet.log`, but the file does not exist on these hosts.
- SSH access to the AWS instances is inconsistent, although the EC2 instances report as being up and healthy.
- I have tried tearing down the entire cluster with `kube-down.sh` and rebuilding it, but the issue persists. The cluster works fine for a few hours and then fails again the next morning.

### Next Steps
I am unsure what additional steps to take or what else to investigate. I am open to suggestions and further troubleshooting. You can reach me at @barrettclark in the Slack channel.