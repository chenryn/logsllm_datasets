I'm running k8s on AWS. `kube-up.sh` works, and I can run a simple Rails app
(controller and service). Everything is great, but when I go back the next day
to access the app or load a new version the nodes have degraded and are all
`NotReady`
    $ kubectl get nodes
    NAME                                         LABELS                                                              STATUS
    ip-172-20-0-196.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-196.us-west-2.compute.internal   NotReady
    ip-172-20-0-197.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-197.us-west-2.compute.internal   NotReady
    ip-172-20-0-198.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-198.us-west-2.compute.internal   NotReady
    ip-172-20-0-199.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-199.us-west-2.compute.internal   NotReady
The pods are `Pending`
    $ kubectl get pods
    NAME                           READY     STATUS    RESTARTS   AGE
    chronic-controller-16uic       0/1       Pending   0          6h
    chronic-controller-6x025       0/1       Pending   0          1h
    chronic-controller-x7owf       0/1       Pending   0          6h
    private-registry-pbvdi         0/1       Pending   0          6h
    tripplanner-controller-3q6km   0/1       Pending   0          6h
    tripplanner-controller-bhwwy   0/1       Pending   0          1h
    tripplanner-controller-fdep8   0/1       Pending   0          6h
Here are the recent events:
    $ kubectl get events
    FIRSTSEEN                         LASTSEEN                          COUNT     NAME                                         KIND      SUBOBJECT   REASON             SOURCE                                                 MESSAGE
    Sat, 12 Sep 2015 22:44:10 -0500   Tue, 15 Sep 2015 15:04:08 -0500   44        ip-172-20-0-199.us-west-2.compute.internal   Node                  NodeNotReady       {kubelet ip-172-20-0-199.us-west-2.compute.internal}   Node ip-172-20-0-199.us-west-2.compute.internal status is now: NodeNotReady
    Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:41 -0500   7531      chronic-controller-ljzn6                     Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
    Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:41 -0500   7757      private-registry-ysjfj                       Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
    Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7284      tripplanner-controller-7b0yr                 Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
    Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7413      tripplanner-controller-s86wz                 Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
    Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7374      chronic-controller-f2nrx                     Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
    Mon, 14 Sep 2015 09:24:35 -0500   Tue, 15 Sep 2015 15:22:42 -0500   7447      chronic-controller-j4538                     Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
    Mon, 14 Sep 2015 01:42:38 -0500   Tue, 15 Sep 2015 15:22:42 -0500   9303      tripplanner-controller-04ztj                 Pod                   failedScheduling   {scheduler }                                           no nodes available to schedule pods
I logged into the master and one of the minions to look at
`/var/log/kubelet.log` but that file doesn't exist on those hosts. Trying to
ssh into one of the AWS instances is also spotty for me, but the EC2 instances
all claim to be up and healthy.
I have killed everything with `kube-down.sh` and built it all again.
Everything worked for the afternoon and then was down again the next morning.
I don't know what else to look at or do. I'm happy to look at things. I'm
@barrettclark in the slack channel.