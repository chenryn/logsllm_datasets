execute in guest-mode (via return from ihub()) before it ex-
ecutes any attacker code (recall §IV-B5). More speciﬁcally,
XMHF maps the LAPIC to a page in M during initialization.
Subsequently, assuming SAFEUPD holds, ensuring MED on
multicore systems reduces to verifying that: (i) ih npf()
disables guest interrupts and sets the guest trap-ﬂag on
access to the LAPIC memory-mapped I/O page, and (ii)
ih db() prevents a direct write to the LAPIC Interrupt
Control Register (ICR) upon detecting a startup command
and instead runs the target guest code on C in guest-mode.
We check this property by a combination of manual
audits and automatic veriﬁcation by model-checking the
validity of assertions inserted in ihub(), ih npf and ih db
(Figure 3). The assertions check that the appropriate core
is switched to guest-mode if the single-step intercept is
triggered. The manual audits comprise of 51 lines of C code
which correspond to the LAPIC page mapping setup and
handling within init(), ih npf and ih db.
2) DMA protection: The XMHF secure-loader is started
via a DRT operation which ensures that the secure-loader
memory is automatically DMA-protected by the hardware
(§IV-B2). The XMHF secure-loader activates DMA protec-
tion for the XMHF runtime before transferring control to the
XMHF core startup component which, during initialization,
re-activates DMA protection before transferring control to
the guest. The DMA protection activation is done by setting
the appropriate bits in the DMA protection hardware regis-
ters to enforce DMA protection. We verify this by model-
checking the validity of a properly inserted assertion in
XMHF, as shown in Figure 4. The inserted assertion checks
that the relevant (enable) bit is set in the DMA protection
hardware register value before writing to the register to
enable DMA protection.
G. Verifying SAFEUPD
Under the assumption that the XMHF core and the hypapp
only modify guest memory protections through the core
API function setprot, and that neither the XMHF core
438
//top-level intercept handler
//cpu = CPU where intercept triggered
//x = triggered intercept
void ihub(int cpu, int x){
//...main body of ihub
#ifdef VERIFY
assert( cpu_HPT_enabled );
assert( cpu_HPT_base == HPT_base );
assert( cpu_intercept_handler == ihub );
#endif
}
//nested page fault handler
void ih_npfe(){
#ifdef VERIFY
int pre_npfe = NPFELAPIC_TRIGGERED();
#endif
//...main body of ih_npfe
#ifdef VERIFY
assert (!pre_npfe || GUEST_TRAPPING(cpu));
#endif
}
//single-step exception handler
void ih_db(){
#ifdef VERIFY
int pre_dbe = LAPIC_ICRWRITE();
#endif
//...main body of ih_db
#ifdef VERIFY
assert (!pre_dbe || CORE_PROTECTED(cpu));
#endif
}
Outline of ihub(), the top-level intercept handler function,
Figure 3.
and ih npf and ih db, the nested page fault and single-step handlers.
NPFELAPIC_TRIGGERED() = true iff the npf intercept was trig-
gered in response to the guest accessing the LAPIC memory-mapped
I/O page. GUEST_TRAPPING(cpu) = true iff the CPU identiﬁed by
cpu has interrupts disabled and is set
to generate a db intercept.
LAPIC_ICRWRITE() = true iff a write was performed to the LAPIC
ICR. CORE_PROTECTED(cpu) = true iff the LAPIC ICR write was
disallowed upon detecting a startup command. cpu_HPT_enabled and
cpu_HPT_base are the MMU ﬁelds that enforce hardware page tables
(HPT). cpu_intercept_handler is the ﬁeld that the CPU transfers
control to when an intercept is triggered in guest-mode. HPT_base and
ihub are the XMHF initialized HPTs and the intercept handler hub respec-
tively. These macros and assertions allow a model checker to automatically
verify DRIVE MED in XMHF.
or the hypapp modify their own code regions and the entry
point to the hypervisor, veriﬁcation of SAFEUPD is reduced
to checking that when setprot completes, all hypervisor
memory addresses are still protected. We additionally know
that after XMHF’s init() function completes, the hypervisor
memory is protected (§V-E), so we further reduce the
veriﬁcation obligation to checking that setprot does not
alter the protection bits of hypervisor memory regions
Similar to before, we use CBMC to automatically verify
setprot does not alter the protection bit of the hypervisor
memory by inserting an assertion preceding every write op-
eration to the HPTs, checking that the address being written
to does not belong to the hypervisor. More concretely, the
hypervisor memory is maintained in a contiguous set of ad-
dresses beginning at HVLO and ending at HVHI. Therefore,
every statement that potentially modiﬁes the permission of
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
//activate DMA protection
...
#ifdef VERIFY
assert( controlreg_value & DMAP_ENABLE );
#endif
DMAPwrite(controlreg, controlreg_value);
...
Figure 4. Outline of DMA protection activation in XMHF. controlreg
and controlreg_value are the DMA protection hardware control
register and register value respectively. DMAP_ENABLE enables the DMA
protection and DMAPwrite writes a value to a given DMA protection
hardware register. The assertion allows a model checker to automatically
verify DMA protection activation and consequently DRIVE MED in XMHF.
//set permission of address a to p
void xmhf_memprot_setprot(int a,int p)
{
...
//the following assertion precedes every
//statement that sets permission of
//address a to p
#ifdef VERIFY
assert (a  HVHI);
#endif
...
}
Figure 5. Outline of the XMHF core API function setprot, which is
used to modify guest memory protections via the Hardware Page Tables
(HPT). The assertion allows a model-checker to verify DRIVE SAFEUPD.
a memory address a is preceded by an assertion that checks
that a  HVHI, as shown in Figure 5.
H. Discussion
1) Modular Development and Veriﬁcation: XMHF’s ver-
iﬁcation is intended to be used as part of the XMHF build
process automatically. Developers of hypapps are not re-
quired to deal with the veriﬁcation directly. This is similar to
other approaches such as the SDV tool [34] which packages
the device driver veriﬁer as part of the driver veriﬁer kit
for Windows. Developers, however, must adhere to the
prescribed XMHF core APIs when changing guest memory
protections or accessing the hardware TCB control structures
(e.g., performing chipset I/O or accessing hardware virtual
machine control structures). Developers must also ensure
that hypapp code does not perform writes to code or write
to arbitrary data that is not part of the hypapp.
Note that the assertions and veriﬁcation statements in-
serted in the XMHF code are for use by CBMC only. Once
CBMC reports a successful veriﬁcation, these statements and
assertions are proven unnecessary, and can therefore be
removed in the production version of XMHF, so that they
do not hinder performance.
2) Manual Audits: The manual audits described in the
previous sections include constructs that CBMC cannot ver-
ify, including loops that iterate over entire page tables (e.g.,
runtime paging, DMA table and HPTs), platform hardware
initialization and interaction (e.g., CPU, LAPIC, BIOS and
PCI) and concurrency (e.g., multicore initialization within
XMHF and multicore guest setup). These are veriﬁcation
challenges that continue to garner attention from the re-
search community. For example, a number of other tools
(see [35] for a list) are being developed for verifying
concurrent C programs. There are design-level veriﬁcation
techniques [36], [37] that could be employed to address the
scalability problem (e.g. loops) with current model-checkers,
for hypervisor designs. We plan to explore their applicability
in the future.
VI. EVALUATION
We present the TCB size of XMHF’s current implemen-
tation and describe our efforts in porting several recent
hypervisor-based research efforts as hypapps running on
XMHF. We then present the performance impact on a legacy
guest operating system running on XMHF and evaluate the
performance overhead that XMHF imposes on a hypapp.
We also compare XMHF’s performance with the popular
open-source Xen hypervisor. These results explain the basic
hardware virtualization overhead intrinsic to the design of
XMHF. Finally, we discuss our veriﬁcation results.
A. XMHF TCB and Case Studies with hypapps
XMHF’s TCB consists of the XMHF core, the hypapp
and supporting libraries used by the hypapp. The XMHF
supporting libraries (totaling around 8K lines of C code)
currently include a tiny C runtime library, a small library of
cryptographic functions, a library with optional utility func-
tions such as hardware page table abstractions and command
line parsing functions, and a small library to perform useful
TPM operations. From a hypapp’s perspective, the minimum
TCB exposed by XMHF comprises the XMHF core which
consists of 6018 SLoC.
We demonstrate the utility of XMHF as a common frame-
work for developing hypapps by porting several recent open-
source research efforts in the hypervisor space to XMHF. Fig-
ure 6 shows the SLoC metrics and platform support for each
hypapp before and after the port to XMHF. TrustVisor [7] and
Lockdown [2] are fully functional, and their code sizes are
precise. The development of HyperDbg [8], XTRec [6] and
SecVisor [10] is sufﬁciently advanced to enable estimation
of their ﬁnal sizes via manual inspection of their existing
sources and differentiation between the hypervisor core and
hypapp-speciﬁc logic. Figure 6 shows that the XMHF core
forms 48% of a hypapp’s TCB, on average. This supports
our hypothesis that
these hypervisors share a common
hypervisor core that is re-used or engineered from scratch
with every new application. Also, using XMHF endows the
hypapps with support for x86 muticore platforms from both
Intel and AMD for free.
B. Performance Measurements
We measure XMHF’s runtime performance using two
metrics: 1) guest overhead imposed solely by the framework
439
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
hypapp
SLoC
Arch. Support Multicore
Original
TrustVisor
Lockdown
XTRec*
SecVisor*
HyperDbg*
6481
~10000
2195
1760
18967
x86 AMD
x86 AMD
x86 AMD
x86 AMD
x86 Intel
Support
No
No
No
No
No
XMHF
core
SLoC
6018
6018
6018
6018
6018
hypapp
+ libs.
SLoC
9138
9391
3500*
2200*
17800*
On XMHF
% XMHF
core
Total
SLoC
Arch. Support
Multicore
Support
15156
15409
9500*
8200*
23800*
40%
40%
63%*
73%*
25%*
x86 AMD, Intel Yes
x86 AMD, Intel Yes
x86 AMD, Intel Yes
x86 AMD, Intel Yes
x86 AMD, Intel Yes
Figure 6. Porting status of several hypervisor-based open-source research efforts as XMHF hypapps. Note (*) the development of HyperDbg, XTRec and
SecVisor is sufﬁciently advanced to enable estimation of their ﬁnal sizes via manual inspection of their existing sources and differentiation between the
hypervisor core and hypapp-speciﬁc logic
(i.e., without any hypapp), and 2) base overhead imposed by
XMHF for a given hypapp.
Our platform is an HP Elitebook 8540p with a Quad-Core
Intel Core i7 running at 3 GHz, 4 GB RAM, 320GB SATA
HDD and an Intel e1000 ethernet controller, using Ubuntu
12.04 LTS as the guest OS running the Linux kernel v3.2.2.
For network benchmarks, we connect another machine via a
1 Gbps Ethernet crossover link and run the 8540p as a server.
We use XMHF with both 4K and 2MB hardware page table
(HPT) mappings for measurement purposes.
1) Guest Performance: With the rich single-guest execu-
tion model (§IV-A) all platform devices are directly accessed
and managed by the guest without any intervention (traps)
by XMHF. Further, the XMHF runtime gets control only when
a conﬁgured guest event is explicitly triggered (§IV-B1).
Thus, when a well-behaved legacy guest runs, the perfor-
mance overhead is exclusively the result of the hardware
virtualization mechanisms, particularly the Hardware Page
Tables (HPT) and the DMA protection.
We execute both compute-bound and I/O-bound appli-
cations with XMHF. For compute-bound applications, we