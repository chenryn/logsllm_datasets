federated learning, respectively.
A. Blockchain and InterPlanetary File System (IPFS)
The blockchain is a chain of blocks that contain the hash of
the previous block, transaction information, and a timestamp.
Blockchain originates from a bitcoin network as an append-
only, distributed and decentralized ledger to record peer-to-
peer transactions permanently and immutably. The IPFS is
a peer-to-peer distributed ﬁle system that enables distributed
computing devices to connect with the same ﬁle system. We
implement the off-chain storage by using IPFS, and store
hashes of data locations on the blockchain instead of actual
ﬁles. The hash can be used to locate the exact ﬁle across the
system.
B. Differential Privacy
Differential privacy (DP) guarantees the privacy and util-
ity of a dataset with rigorous theoretical foundation [9,10].
This paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
Intuitively, because the algorithm’s output
is perturbed by
the noise, the absence or presence of one user’s informa-
tion in the database will not affect it much. By using the
DP algorithm, analysts cannot derive conﬁdential information
when analyzing the algorithm’s outputs. DP has received much
interest in both the academia and the industry. For example,
Apple’s mobile operating system iOS utilizes DP algorithms
in [11]. Google’s Chrome browser implements a DP tool
called RAPPOR for collecting customers’ data [12]. A smaller
privacy parameter  denotes the stronger privacy protection,
but less utility of data as more randomness is added to the data.
In contrast, a higher privacy parameter  denotes the weaker
privacy protection but ensuring better utility of the data. The
formal deﬁnition for differential privacy is as follows :
Deﬁnition 1. (Differential Privacy.) A randomized mechanism
M provides (, δ)-differential privacy if any two neighbouring
datasets D and D(cid:48) (i.e., D and D(cid:48) differ in at most one
record), M guarantees that
where Pr[·] denotes the probability, and the probability space
is over the coin ﬂips of the randomized mechanism M. Y
iterates through all subsets of the output range of mechanism
M. When δ = 0, the mechanism M becomes -differentially
private.
Pr[M(D) ∈ Y ] ≤ ePr[M(D(cid:48)) ∈ Y ] + δ,
The Laplace mechanism of [9] can be used to ensure differ-
ential privacy by adding independent zero-mean Laplace noise
with scale λ to each dimension of the output. Speciﬁcally, λ
equals ∆/, where ∆ is the (cid:96)1-norm sensitivity of the query Q
and it measures the maximum change of the true query output
over neighboring databases.
The post-processing property [9] of differential privacy
means that a data analyst cannot compute the function of
the output of a differentially private algorithm and reduce
its privacy guarantee without obtaining additional knowledge
about the private data. Hence, in our design, although noise
is added to an intermediate layer of the neural network, the
post-processing property ensures that the ﬁnal trained model
also satisﬁes differential privacy.
C. Federated Learning
Traditional machine learning algorithms are conducted in a
centralized data center where data owners upload their data.
However, data are privacy sensitive, and data owners are
unwilling to share; thus, collecting data is a tough and verbose
task that hinges the progress of machine learning. To avoid
the data deﬁciency problem as well as maintain the machine
learning model’s accuracy and performance, a decentralized
approach of conducting machine learning, federated learning
(FL), is proposed [13]; that is, data are distributed and scat-
tered among different users, and no such a single node stores
the whole dataset. The workﬂow of FL is that each user trains a
local machine learning model using her dataset and uploads it
to the centralized model for summarizing and averaging. Then,
a global model is achieved in the centralized server. Thus,
FL prevents a single point of failure effectively. FL is similar
to the traditional distributed machine learning [14], but as-
sumptions on local datasets are different. More speciﬁcally, the
traditional distributed learning aims at optimizing the parallel
computing power, but data are IID among different parties,
while FL focuses on the heterogeneous local datasets, meaning
that training data can be distributed, non-IID, and unbalanced
among various participants. That is, each participant i trains
the same model with her local dataset, and the goal is to
obtain a global model with the minimized averaged sum of
loss functions among all participants.
III. RELATED WORK
Blockchain and federated learning (FL) techniques have
been widely used in training a neural network with distributed
data [15]–[24]. For example, Weng et al. [21] proposed a
system called DeepChain for the collaborative learning. But
they did not ofﬂoad the training task to the edge server, and
they did not propose to use differential privacy to protect
the privacy of model parameters. Awan et al. [20] proposed
a blockchain-based privacy-preserving FL framework, which
secured the model update using blockchain’s immutability
and decentralized trust properties. Li et al. [25] designed a
decentralized framework based on blockchain for crowdsourc-
ing tasks, which enabled them to do crowdsourcing tasks
without a centralized server. Lu et al. [16] proposed to leverage
blockchain, FL, and differential privacy for data sharing.
However,
they directly added differential privacy noise to
the original data instead of the gradients, which may affect
the accuracy seriously. Lyu et al. [22] made the ﬁrst-ever
investigation on the federated fairness in a blockchain-assisted
decentralized deep learning framework, and designed a local
credibility mutual evaluation mechanism to enforce fairness.
They also developed a scheme for encryption to ensure privacy
and accuracy.
Moreover, FL has attracted substantial attention re-
cently [26]–[30], and one of the most important issues in
FL is privacy protection, which is explored in [31]–[35].
Li et al. [31] considered the privacy issue during sharing model
updates in FL. They proposed to leverage the sketch algorithms
to build the sketching-based FL, which provides privacy
guarantees while maintaining the accuracy. Hao et al. [32]
proposed a privacy-enhanced FL scheme to solve the privacy
issue in FL. Their scheme helps to achieve efﬁcient and
privacy-preserving FL. Dolui et al. [33] applied FL paradigms
to recommender systems and matrix factorization, which guar-
antees the recommender systems’ functionality and privacy.
Nasr et al. [34] performed a comprehensive privacy analysis
with white-box inference attacks. Wang et al. [35] proposed a
framework incorporating generative adversarial network with a
multitask discriminator to solve the user-level privacy leakage
in FL against attacks from a malicious server.
Furthermore, there are many studies focusing on privacy-
preserving crowdsourcing [36,37], and leveraging fog com-
puting or edge computing to improve the performance as they
have gained popularity [38]–[43]. For example, Wu et al. [36]
proposed two generic models for quantifying mobile users’
privacy and data utility in crowdsourced location-based ser-
vices, respectively. He et al. [38] designed a privacy model for
the crowdsourced bus service, which takes advantage of the
3
This paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
Figure 1: An overview of our system.
computational power of the fog computing. However, their
models are applicable only to the traditional crowdsourcing
approach (i.e., customers transmit data to a centralized server)
without considering the FL crowdsourcing tasks which lever-
age locally trained models. Zhao et al. [40] presented with a
privacy-preserving mechanism to prevent the poisoning attack
to the mobile edge computing. However, users need to ofﬂoad
data to the MEC server in their system which may leak the
privacy, instead, we propose that users retain their data locally.
In addition, a few studies have combined deep learning or
FL with edge computing [44]–[47]. Lyu et al. [44] proposed
a privacy-preserving deep learning framework, where a two-
level protection mechanism, including Random Projection and
Differentially Private SGD, is leveraged to protect the data pri-
vacy. Jiang et al. [45] designed a collaborative training method
to protect features’ privacy. In detail, the feature extraction is
done locally in the devices such as smartphones while the
classiﬁcation is executed in the cloud service. However, they
did not use the FL to protect the privacy of training data, and
they did not propose a normalization technique to improve the
test accuracy. Wang et al. [46,47] proposed control algorithms
to solve the problem of low resources in IoT devices while
participating in the federated learning.
IV. SYSTEM DESIGN
In this section, we present the system which we design
for smart home appliance manufacturers who are interested in
building a machine learning model using data from customers’
home appliances to analyze customers’ habit and improve their
service and products.
4
A. System Overview
Figure 1 shows an overview of our system architecture. The
system consists of three primary components: manufacturers,
customers and blockchain. Speciﬁcally, manufacturers raise a
request for a crowdsourcing FL task. Then, customers who are
interested in the crowdsourcing FL tasks submit their trained
models to the blockchain. Finally, the blockchain serves as the
centralized server to gather customers’ models, and a selected
miner calculates and generates the global FL model for home
appliance manufacturers. In the following, we will introduce
each component in detail.
Manufacturers. Manufacturers raise a request to build a
machine learning model to predict customers’ consumption
behaviours and improve home appliances, which is a crowd-
sourcing FL task. Customers who have home appliances can
participate in the FL task. To facilitate the progress of FL, we
use the blockchain to store the initial model with randomly
selected parameters. Otherwise, manufacturers need to send
the model to everyone or save it in a third party’s cloud
storage. In addition, neither manufacturers nor customers can
deny recorded contributions or activities. Eventually, manufac-
turers will learn a machine learning model as more and more
customers participate in the crowdsourcing FL task.
Customers. Customers who have home appliances satisfy-
ing crowdsourcing requirements can apply for participating in
the FL task. However, since home appliances are equipped
with heterogeneous storage and computational powers, it is
difﬁcult to enable each IoT device to train the deep model.
To address this issue, we adopt the partitioned deep model
training approach [45,48]. Speciﬁcally, we use a mobile phone
to collect data from home appliances and extract features.
To preserve privacy, we add -DP noise to features. Then,
CompanyPreliminary Model(cid:172)Blockchain Local trained modelMobile Edge Computing Data to exchange Mobile IoT Devices Machine Learning Trained model with signature Signal towerThis paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
customers continue training fully connected layers in the MEC
server. To be speciﬁc, we clarify the customers’ responsibilities
in four detailed steps as follows.
Step 1: Customers download the initial model from the
blockchain. Customers who are willing to participate in the FL
task check and download the initial model which is uploaded
by the manufactures and available on the blockchain.
Step 2: Customers extract features on the mobile. The
mobile phone collects all participating home appliances’ data
periodically. Then, customers can start
training the model
using collected data. Since the MEC server is provided by a
third-party, it may leak information. Therefore, we divide the
local training process into two phrases: the mobile training
and the MEC server training. Because perturbing original
data directly may compromise the model’s accuracy, we treat
the convolutional neural network (CNN) layers as the feature
extractor to extract features from the original data in the
mobile. Then, we add -DP noise to features before ofﬂoading
them to the fully connected layers in the MEC.
Step 3: Customers train fully connected layers in the
mobile edge computing server. The mobile sends the privacy-
preserving features and original labels to the mobile edge
computing server, so that
the server helps train the fully
connected layers. The training loss is returned to the mobile
to update the front layers.
Step 4: Customers upload models to the blockchain. After
training the model, customers sign on hashes of models
with their private keys, and then they upload models to the
blockchain via smartphones. However, if miners determine
that the signature is invalid, the transaction fails because it is
possible that an adversary is attempting to attack the learning
process using faked data. After miners conﬁrm the transaction,
customers can use the transaction history as an invoice to
claim reward and reputation. Section IV-B shows the detail
of reputation calculation. By using the immutable property of
the blockchain, both manufacturers and customers cannot deny
transactions stored on the blockchain.
Blockchain. A consortium blockchain is used in our crowd-
sourcing system to store machine learning models perma-
nently. The consensus protocol
is the Algorand which is
based on proof of stake (PoS) and byzantine fault tolerance
(BFT) [49,50]. Algorand relies on BFT algorithms for com-
mitting transactions. The following steps are required to reach
the consensus: (1) Miners compete for the leader. The ratio
of a miner’s stake (i.e., coins) to all tokens determines the
probability for the miner to be selected. Subsequently, by
hashing the output of random function with the identities of
nodes which are speciﬁed by their stake, the order of the block
proposals is obtained. Thus, a miner with more stakes will gain
a higher chance to become a leader. (2) Committee members
verify the block generated by the selected leader. When more
than 2/3 of the committee members sign and agree on the
leader’s block, the new block gets admitted. (3) Committee
members execute the gossip protocol to broadcast the new
block to neighbours to arrive at a consensus in blockchain.
In our case, the workﬂow starts with a manufacturer up-
loading an initial model to the blockchain. Then, customers
can send requests to obtain that model. After training models
locally, customers upload their locally trained models to the
blockchain. Because of the limitation of the block size, we
propose to use IPFS as the off-chain storage. Then, customers
upload their models to the IPFS, and a hash will be sent to
the blockchain as a transaction. The hash can be used to
retrieve the actual data from IPFS. The leader and miners
are responsible for conﬁrming transactions and calculating
the averaged model parameters to obtain a global model.
Miners’ results are mainly used for verifying the leader’s
result. After all customers upload their trained models, the
miners download them and start calculating the averaged
model parameters. Then, one of miners is selected as the leader
to upload the global model to the blockchain. We will explain
the process in detail as follows:
x Miners verify the validity of the uploaded model. When
a customer uploads a model or the hash of the model to
the blockchain, a miner checks the digital signature of the
uploaded ﬁle. If the signature is valid, then the miner conﬁrms