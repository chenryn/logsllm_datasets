**Kubernetes version** (use `kubectl version`):  
Client Version: version.Info{Major:"1", Minor:"4+",
GitVersion:"v1.4.0-alpha.2.1807+f80530c7f77e2f",
GitCommit:"f80530c7f77e2fd272e6718b7d1754df0869d478", GitTreeState:"clean",
BuildDate:"2016-08-29T12:08:57Z", GoVersion:"go1.6.3", Compiler:"gc",
Platform:"linux/amd64"}  
Server Version: version.Info{Major:"1", Minor:"4+",
GitVersion:"v1.4.0-alpha.2.1807+f80530c7f77e2f",
GitCommit:"f80530c7f77e2fd272e6718b7d1754df0869d478", GitTreeState:"clean",
BuildDate:"2016-08-29T12:03:17Z", GoVersion:"go1.6.3", Compiler:"gc",
Platform:"linux/amd64"}
**Environment** :  
2 kubelet nodes + 1 master, deployed by local vagrant provider
**What happened** :  
While working on #31098 I added `--eviction-hard=memory.available<90%` to
evict pets from node-1. It did its job and they were evicted, but after that
pod was constantly re-scheduled onto same node.
**What you expected to happen** :  
Expected that eviction-hard will prevent pod from being scheduled onto the
same node, as long as evict condition is preserved.
**How to reproduce it** (as minimally and precisely as possible):
  1. Create petset (i was using cockroachdb example)
  2. Add `--eviction-hard=memory.available<90%` (or another condition which will evict pods from node)
I didn't try it with regular pods, so you may need to use my patch #31777