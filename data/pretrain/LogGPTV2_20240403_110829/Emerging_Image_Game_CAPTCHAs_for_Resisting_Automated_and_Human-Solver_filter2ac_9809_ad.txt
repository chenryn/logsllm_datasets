DCG over the streaming channel with respect to the measures of
solving time and error rate. The results are summarized in Table 5.
In the LSHL relay attack setting, only two participants could
complete one of the EI-DCG Easy variant, and only one participant
could complete one of the EI-DCG medium variant. The overall
error rate is therefore 0.99 on an average for the three categories
of EI-DCG. The average time taken by the participants to complete
the challenges was 42.68 seconds. Moreover, the participants per-
formed much higher number of drag-drops and attempts compared
to the participants in the usability study. Moreover, if we limit the
number of allowed drags/attempts to 21, the overall error rate for
the Easy EI-DCG becomes 1.00.
In the HSLL relay attack setting, the error rate decreased from
around 0.99 to 0.87 on average when compared to the LSHL relay
attack study. However, the error rate is still considerably high. The
18
Table 5: The solving time, error rate, number of drags, and number of attempts for the India-based and USA-based streaming-based relay attack studies on EI-DCG
EI-DCG
Challenge Type
Time (sec)
LSHL (human-solver in India)
# Attempts
# Drags
mean (std. dev.)
Easy
Medium
Difﬁcult
39.05 (5.47)
8.00 (4.24)
24.00 (1.41)
49.92
-
4.00
-
15.00
-
Error
Rate
0.98
0.99
1.00
HSLL (human-solver in USA)
Time (sec)
# Drags
# Attempts
mean (std. dev.)
18.73 (10.62)
22.13 (11.69)
24.05 (11.71)
4.35 (3.05)
4.17 (2.91)
4.53 (2.37)
4.71 (15.83)
1.63 (1.88)
1.66 (2.89)
Error
Rate
0.87
0.88
0.86
completion time and number of drags and attempts for the partici-
pants who successfully solved the challenges are comparable to the
ones in the usability study, except for the number of attempts in
the Easy variant. Limiting the number of allowed drags/attempts to
our threshold of 21 did not effect the over all error rate. Comparing
the successful completion time between each variant of EI-DCG in
usability and its corresponding streaming-based relay attack study,
using Mann-Whitney test, did not reveal statistically signiﬁcant dif-
ferences, however. Further investigation of the collected data shows
that only 9 out of the 48 participants (18.75%) were able to com-
plete any EI-DCG challenges. On an average each of them solved
around 10.33 out of the 15 challenges.
The above analysis suggests that EI-DCG is very hard to solve
via streaming-based relaying, especially when the human-solvers
are located far away and have slow Internet connections. The
chance of solving EI-DCG challenges seem to increase, but still
only marginally, when recruiting the solvers residing in near prox-
imity and having high-speed Internet connections. Even when the
solvers can solve the CAPTCHA challenges, we will next show
how they can be detected based on different game play patterns
compared to legitimate human users.
5.2.3 Attack Detection Technique
We design and implement a machine-learning based EI-DCG
Stream Relay attack detection mechanism based on the differences
between the participants’ performances in completing the chal-
lenges in the usability study and the human-solvers’ performance in
the HSLL relay attack study. We logged all the participants’ inter-
actions with the challenges, while they were solving the challenges.
We logged the interactions with the challenges regularly and when-
ever a mouse event is performed. Speciﬁcally, at each timestamp,
we log the mouse position, mouse status (up or down), and for each
object, we log its position and weather it is being dragged or not.
From each of the collected challenge log ﬁles, we extract a total of
sixteen different features, as described below:
outside of the moving objects.
• Number of Drags (ND): Total number of drags.
• Number of Attempts (NA): The number of mouse clicks
• Distance-based Features: (1) Distance Drag (DD): The to-
tal distance of the mouse movement while it is dragging an
object; (2) Distance Attempts (DA): Distance of the mouse
movement while the mouse status is down but not dragging
an object; and (3) Distance mouse Up (DU): Distance of the
mouse movement while the mouse status is up.
• Time-based Features: (1) Completion Time (T): The total
time taken by the participant to complete the challenge; (2)
Time drags (TD): The total time in which an object is being
dragged; (3) Time Attempts (TA): Time in which the mouse
is down and no object is being dragged, and (4) Time mouse
Up (TU): Time in which the mouse status is up.
• Speed-based Features (Distance/Time): Speed of Drags
(SD), Speed of Attempts (SA) and Speed of Mouse Up (SU).
• Acceleration-based features (Speed/Time): Acceleration of
Drags (AD), Acceleration of Attempts (AA) and Accelera-
tion of Mouse Up (AU).
• Max Attempt (MA): The maximum time taken in a single
attempt.
For each of the EI-DCG variants, we picked randomly a num-
ber of instances of the successfully completed challenges in the us-
ability study equal to the number of successfully completed chal-
lenges in the HSLL relay attack study. Then, we implemented a
Java program to check which subset of features along with which
classiﬁer provides the best results for each of the challenges cate-
gories (Easy, Medium, Difﬁcult). We experimented with different
classiﬁers: SVM (C-SVC and nu-SVN, linear, polynomial and ra-
dial kernels), Multilayer Perceptron, Naive Bayes, Random Forest,
Random Tree, Simple Logistic and Logistic.
As performance measures for our classiﬁcation task, we used
Precision, Recall and F-measure (F1 score), as shown in Equation 1
(TP: True Positive; FP: False Positive; FN: False Negative). Preci-
sion measures the security of the proposed system, i.e., the robust-
ness of the system in detecting the relay attack. Recall measures
the system usability as low recall leads to high rejection rate of le-
gitimate users’ actions. F-measure is the weighted average of both
of the precision and recall. We select the classiﬁer with maximum
recall, to reduce the rejection of honest users, and have acceptable
precision rate.
precision =
; recall =
T P
T P + F P
T P
T P + F N
;
F -measure = 2 ∗ precision ∗ recall
precision + recall
(1)
Table 6: Results of using the optimal feature subset for each EI-DCG game in the
classiﬁcation of legitimate user and HSLL streaming-based relay attacker
Challenge
Type
Easy
Classiﬁer
Features
Precision Recall F-measure
Multilayer
Perceptron
ND, T, TD, TA, SD, SU 0.78
0.90
0.84
Medium Random Tree ND, T, DD, DA, DU,
0.72
0.87
0.79
Difﬁcult Multilayer
Perceptron
TU, MA, AD, AU
T, TU, AD, AA, AU
0.66
0.91
0.76
The results of the best classiﬁer along the best features subset for
each of the EI-DCG difﬁculty level are shown in Table 6. The re-
sults show that a classiﬁer can be effectively built such that it rejects
around 11% of legitimate users and be able to detect around 65%
for HSLL relay attackers. As shown in Table 5, on average only
13% of HSLL relay attackers could solve the EI-DCG challenges,
utilizing our proposed detection method 65% of them could be de-
tected. Therefore, the detection rate for the HSLL relay attack is
about 95%. The LSHL relay attack is already prevented with at
least 98% probability (Table 5). This suggests that the streaming-
based relay attack in general has a very low chance of succeeding
against EI-DCG CAPTCHAs.
5.3 Discussion
The previous subsection shows that EI-DCGs is resilient to
streaming-based relay attacks. Inherently, this ability is due to the
fact that the user needs to see the challenge at about 40 fps to be
19
able to recognize the objects and solve the challenge successfully.
That requires high data connection speed with low latency between
the attacker and the human-solver.
As the game size is 340×400 and the required frame rate is 40
fps, using 2 bit per pixel will require the connection speed of 10.38
Mbit/s between the attacker and the solver, if no compression is
performed. However, RealVNC performs some compression on
the data sent between the client and the server to enhance the per-
formance. First, RealVNC sends only the modiﬁed pixels between
the current frame and the previous frame rather than sending the
whole frame. Second, the RealVNC client allows the user to ad-
just the quality of the images, varying from low quality (8 col-
ors) to best quality (all available colors). In order to evaluate the
minimum bandwidth required between the attacker and the human-
solver to transfer the game with 40 fps, we performed an exper-
iment in which we connected two laptops over a LAN, installed
RealVNC server on one of them and RealVNC client on the other.
We varied the RealVNC compression levels starting from the best
compression that achieves the best transmission quality. We used
Wireshark to capture the packets sent between the two laptops in
all of the cases, and then analyzed the average data rate used. The
average Mbit/s for the getting all colors with maximum compres-
sion is 4.70 Mbit/s, 3.15 Mbit/s for the default settings (256 colors)
and 2.22 Mbit/s for the best compression (8 colors). This shows
the minimum connection speed required between the attacker and
human-solver should be 2.22 Mbit/s to be able to solve the EI-DCG
at the human-solver’s end. Any slower connection would result in
a jittery rendering of the game, and therefore the human-solver will
not be able to solve the EI-DCG successfully. This justiﬁes the
results of the user studies presented in the previous subsection.
According to [3], the average connection speed is 2.0 Mbit/s in
India. Also, since the long distance between the attacker (USA)
and the human solver slows down the connection speed, almost all
the participants in India could not remain connected to our server
with high speed and they failed to successfully solve the challenges.
On the other hand, the average connection speed in USA is 11.1
Mbit/s. Also, the distance is much less between the attackers and
the human-solvers in the HSLL streaming-based relay attack study.
This explains why some of the human-solver were able to success-
fully solve some of the challenges.
We tested multiple screen-sharing applications,
such as
TeamViewer and anydesk. All of them require almost similar data
rates between the server and the client to transfer the challenges.
For example, TeamViewer requires 2.73 Mbit/s for gray-scale, and
anydesk requires 2.63 Mbit/sec. Therefore, we conclude that the
results of our relay attack studies are not limited to RealVNC.
A third relay attack scenario was suggested in [12], namely,
Small Game Relay, whereby the attacker reduces the game size be-
fore sending it to the human-solver to lower the data rate required
between the attacker and the solvers. However, this scenario can-
not be applied to EI-DCG as reducing the game size will make it
almost impossible for the human-solver to recognize the moving
objects (because of the emerging effect).
6. CONCLUSIONS
We proposed a new class of CAPTCHAs, EI-DCG, based on
the notion of emerging images and Dynamic Cognitive Games.
EI-DCG applies a series of countermeasures, such as pseudo 3D
rotation, hidden edge segments, segment split-erosion-rotation-
translation and tiled background, to resist automated object recog-
nition based on both single frames and frame superimposition.
Since a human can perceive objects in EI-based frames from mo-
tion, playing an EI-based video in a relatively slower frame rate de-
20
creases the human recognition rate. Based on this property, a faster
frame rate (i.e., 40 fps) was applied for the normal playing of an
EI-DCG challenge. The higher the network delay is, the lower the
frame rate of EI-DCG becomes on the human-solver’s side, pro-
viding less opportunity for the solver to complete the challenge
successfully. The experiments against both automated and relay
attacks indicated the robustness of EI-DCG.
Acknowledgments
The work has been partially supported by a grant from the National
Science Foundation (CNS-1255919) and an award from COM-
CAST.
7. REFERENCES
[1] Are you a human. http://areyouahuman.com/.
[2] Nucaptcha. http://www.nucaptcha.com/.
[3] Akamai. Akamai’s state of the internet: Q4 2014 report.
http://www.stateoftheinternet.com/resources-connectivity-2014-q4-
state-of-the-internet-report.html.
[4] H. S. Baird, A. L. Coates, and R. J. Fateman. Pessimalprint: a reverse
turing test. International Journal on Document Analysis and
Recognition, 2003.
[5] A. Bangor, P. Kortum, and J. Miller. Determining what individual sus
scores mean: Adding an adjective rating scale. Journal of usability
studies, 2009.
[6] J. Brooke. Sus-a quick and dirty usability scale. Usability evaluation
in industry, 1996.
[7] J. M. G. Hidalgo and G. Alvarez. Captchas: An artiﬁcial intelligence
application to web security. Advances in Computers, 2011.
[8] G. Kanizsa. Organization in vision: Essays on Gestalt perception.
Praeger New York, 1979.
[9] G. Keizer. Spammers’ bot cracks microsoft’s captcha. Computer
World, Available at: http://www.computerworld.com/article/
2536901/security0/spammers--bot-cracks-microsoft-s-captcha.html,
2008.
[10] K. A. Kluever and R. Zanibbi. Balancing usability and security in a
video captcha. In Symposium on Usable Privacy and Security, 2009.
[11] N. J. Mitra, H.-K. Chu, T.-Y. Lee, L. Wolf, H. Yeshurun, and
D. Cohen-Or. Emerging images. In Transactions on Graphics, 2009.
[12] M. Mohamed, S. Gao, N. Saxena, and C. Zhang. Dynamic cognitive
game captcha usability and detection of streaming-based farming. In
the Workshop on Usable Security (USEC), 2014.
[13] M. Mohamed, N. Sachdeva, M. Georgescu, S. Gao, N. Saxena,
C. Zhang, P. Kumaraguru, P. C. van Oorschot, and W.-B. Chen. A
three-way investigation of a game-captcha: automated attacks, relay
attacks and usability. In ACM symposium on Information, computer
and communications security, 2014.
[14] M. Motoyama, K. Levchenko, C. Kanich, D. McCoy, G. M. Voelker,
and S. Savage. Re: Captchas-understanding captcha-solving services
in an economic context. In USENIX Security Symposium, 2010.
[15] J. K. Tsotsos. On the relative complexity of active vs. passive visual
search. International journal of computer vision, 1992.
[16] L. Von Ahn, M. Blum, N. J. Hopper, and J. Langford. Captcha:
Using hard ai problems for security. In EUROCRYPT 2003.
[17] Y. Xu, G. Reynaga, S. Chiasson, J. Frahm, F. Monrose, and
P. Van Oorschot. Security analysis and related usability of
motion-based captchas: Decoding codewords in motion.
Transactions On Dependable And Secure Computing, 2013.
[18] Y. Xu, G. Reynaga, S. Chiasson, J.-M. Frahm, F. Monrose, and P. C.
van Oorschot. Security and usability challenges of moving-object
captchas: Decoding codewords in motion. In USENIX Security
Symposium, 2012.
[19] J. Yan and A. S. El Ahmad. Breaking visual captchas with naive
pattern recognition algorithms. In Computer Security Applications
Conference, 2007.
[20] J. Yan and A. S. El Ahmad. A low-cost attack on a microsoft captcha.
In Conference on Computer and communications security, 2008.
[21] B. B. Zhu, J. Yan, Q. Li, C. Yang, J. Liu, N. Xu, M. Yi, and K. Cai.
Attacks and design of image recognition captchas. In ACM
Conference on Computer and Communications Security (CCS), 2010.