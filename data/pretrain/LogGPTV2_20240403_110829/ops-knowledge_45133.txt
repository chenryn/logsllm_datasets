User: I have a Python generator which pulls in a pretty huge table from a data warehouse. After pulling in the data, I am processing the data using celery in a distributed manner. After testing I realized the the generator is the bottleneck. It can't produce enough tasks for celery workers to work on. This is when I have decided to optimize my python generator.
More details on the generator 
The generator hits the data warehouse with chunk queries and these query results are basically independent of each other and stateless. So I thought this is a good candidate for making it parallel using the multiprocessing module. I looked around how to parallelize generators fetch without much direction. 
So if my Python generator generates stateless chunks of data, this should be a good candidate for multiprocessing right? Are there any ways to parallelize python generators? Also are there any side effects which I should be aware of using parallelism in Python generators?
Assistant: I think you may be trying to solve this problem at the wrong level of abstraction. Python generators are inherently stateful, and thus you can't split a generator across processes without some form of synchronization, and that will kill any performance gains that you might achieve through parallelism. I would recommend instead creating separate generators for each process and having them start at some offset from each other.
For example if you have 4 processes, you basically have the first process handle the first chunk and then it process the 5th chunk followed by the 9th chunk and so on adding N where N is the number of processes that you've setup. This requires you to hand off a unique index to each of the processes at startup.