provide control over, and coordination of, trafﬁc forwarding. As
already discussed, forwarding changes alone are insufﬁcient to sat-
isfy multiple objectives without degrading NF accuracy.
VM [18] or process replication [5] only allows cloning of NF in-
stances in their entirety. The additional, unneeded state included in
a clone not only wastes memory, but more crucially can cause un-
desirable NF behavior: e.g., an IDS may generate false alerts (we
Figure 2: OpenNF architecture
quantify this in §8.4). Moreover, this approach prevents state from
multiple NF instances from being moved and merged, precluding,
e.g., fast elastic scale-down.3 Because of their intrinsic limitations,
combining existing control planes with techniques for VM migra-
tion/process replication does not address the above requirements.
Vendor-supplied controllers [4, 14] that move, copy, and share
NF state between multiple NF instances can leverage knowledge
about the internal workings of NFs. However, they cannot control
network state in a way that fully satisﬁes all goals—e.g., it is hard
to provide optimized load balancing across network links.
Split/Merge [34] and Pico Replication [33] are the only systems
that provide some control over both internal NF state and network
state. They provide a shared library that NFs use to create, access,
and modify internal state through pre-deﬁned APIs. In Split/Merge,
an orchestrator is responsible for coordinating load balancing by in-
voking a simple migrate (f) operation that reroutes ﬂow f and moves
corresponding NF state. In Pico Replication, modules are added to
an NF to manage the ﬂow of packets in and out of each instance
and to clone states at policy-deﬁned frequencies.
Unfortunately, the migrate operation can cause lost or re-ordered
NF state updates, since packets arriving at an NF instance after
migrate is initiated are dropped, and a race exists between applying
the network forwarding state update and resuming the ﬂow of trafﬁc
(which is halted when migrate starts). Furthermore, the orchestra-
tor and NF modules are targeted to speciﬁc problems, making them
ill-suited to support other complex control applications. Finally, the
API NFs must use to create and access states uses nondescript keys
for non-ﬂow-based state, making it difﬁcult to know the exact states
to move and copy when ﬂows are rerouted, and the API only allows
one state allocation per ﬂow, requiring some internal NF state and
packet processing logic to be signiﬁcantly restructured. We discuss
these issues in more detail later in the paper.
3. OpenNF OVERVIEW
OpenNF is a novel control plane architecture (Figure 2) that sat-
isﬁes the aforementioned requirements and challenges. In this sec-
tion, we outline our key ideas; §4 and §5 provide the details.
OpenNF allows control applications to closely manage the be-
havior and performance of NFs to satisfy high level objectives.
Based on NF output or external input, control applications: (1) de-
termine the precise sets of ﬂows that speciﬁc NF instances should
process, (2) direct the controller to provide the needed state at each
instance, including both ﬂow-speciﬁc state and state shared be-
tween ﬂows, and (3) ask the controller to provide certain guarantees
on state and state operations.
In turn, the OpenNF controller encapsulates the complexities
of distributed state control and, when requested, guarantees loss-
freedom, order-preservation, and consistency for state and state op-
erations. We design two novel schemes to overcome underlying
race conditions: (1) an event abstraction that the controller uses
3Basic scale-down can be supported by assigning new ﬂows to the
“combined” instance and waiting for ﬂows at the “old” instance to
terminate; but this can take a long time.
165Figure 3: NF state taxonomy, with state from the Squid caching proxy
as an example
to closely observe updates to state, or to prevent updates but know
what update was intended, and (2) a two phase forwarding state
update scheme. Using just the former, the controller can ensure
move operations are loss-free, and state copies are eventually con-
sistent. By carefully sequencing state updates or update preven-
tion (scheme 1) with the phases of scheme 2, the controller can
ensure move operations are loss-free and order-preserving; we pro-
vide a formal proof in our technical report [23]. Lastly, by buffer-
ing events corresponding to intended updates and handling them
one at a time in conjunction with piece-meal copying of state, the
controller can ensure state copies are strongly or strictly consistent.
OpenNF’s southbound API deﬁnes a standard NF interface for
a controller to request events or the export or import of internal
NF state. We leave it to the NFs to furnish all state matching a
ﬁlter speciﬁed in an export call, and to determine how to merge
existing state with state provided in an import call. This requires
modest additions to NFs and, crucially, does not restrict, or require
modiﬁcations to, the internal state data structures that NFs main-
tain. Furthermore, we use the well deﬁned notion of a ﬂow (e.g.,
TCP connection) as the basis for specifying which state to export
and import. This naturally aligns with the way NFs already create,
read, and update state.
4. SOUTHBOUND API
In this section, we describe the design of OpenNF’s southbound
API. To ensure a variety of NFs can be easily integrated into
OpenNF, we must address two challenges: (1) account for the di-
versity of NF state and (2) minimize NF modiﬁcations.
4.1 State Taxonomy
To address the ﬁrst challenge, we must identify commonalities in
how internal state is allocated and accessed across various NFs. To
this end, we examined several types of NFs from a variety of ven-
dors, including: NATs [9], IDSs [31], load balancers [1, 7], caching
proxies [15], WAN optimizers [16], and trafﬁc monitors [11, 13].
We observe that state created or updated by an NF while pro-
cessing trafﬁc applies to either an individual ﬂow (e.g., TCP con-
nection) or a collection of ﬂows. As shown in Figure 1, the Bro
IDS maintains connection and analyzer objects for each TCP/UD-
P/ICMP ﬂow and state for each host summarizing observations re-
lating to all ﬂows involving that host. Similarly, as shown in Fig-
ure 3, the Squid caching proxy maintains socket context, request
context, and reply context for each client connection and cache en-
tries for each requested web object. Most NFs also have state which
is updated for every packet or ﬂow the NF processes: e.g., statistics
about the number of packets/ﬂows the NF processed.4
Thus, as shown in Figure 3, we classify NF state based on scope,
or how many ﬂows an NF-created piece of state applies to—one
ﬂow (per-ﬂow), multiple ﬂows (multi-ﬂow), or all ﬂows (all-ﬂow).
In particular, per-ﬂow state refers to structures/objects that are read
or updated only when processing packets from the same ﬂow (e.g.,
4NFs also have conﬁguration state. It is read but never updated by
NFs, making it easy to handle; we ignore the details in this paper.
TCP connection), while multi-ﬂow state is read or updated when
processing packets from multiple, but not all, ﬂows.
Thinking about each piece of NF-created state in terms of its
association with ﬂows provides a natural way for reasoning about
how a control application should move/copy/share state. For exam-
ple, a control application that routes all ﬂows destined for a host H
to a speciﬁc NF instance can assume the instance will need all per-
ﬂow state for ﬂows destined for H and all multi-ﬂow state which
stores information related to one or more ﬂows destined for H.
This applies even in the case of seemingly non-ﬂow-based state:
e.g., the ﬁngerprint table in a redundancy eliminator is classiﬁed
as all-ﬂows state, and cache entries in a Squid caching proxy are
multi-ﬂow state that can be referenced by client IP (to refer to
cached objects actively being served), server IP, or URL.
Prior works on NF state management either draw no association
between state and ﬂows [25], or they do not distinguish between
multi-ﬂow and all-ﬂows state [34]. This makes it difﬁcult to know
the exact set of state to move, copy, or share when ﬂows are re-
routed. For example, in the Squid caching proxy, cached web ob-
jects (multi-ﬂow states) that are currently being sent to clients must
be copied to avoid disrupting these in-progress connections, while
other cached objects may or may not be copied depending on the
SLAs a control application needs to satisfy (e.g., high cache hit
ratio vs. fast scale out).5 We quantitatively show the beneﬁts of
granular, ﬂow-based control in §8.1.2.
We also discovered during our examination of NFs that they tend
to: (1) allocate state at many points during ﬂow processing—e.g.,
when the Bro IDS is monitoring for malware in HTTP sessions, it
allocates state when the connection starts, as protocols are identi-
ﬁed, and as HTTP reply data is received—and (2) organize/label
state in many different ways—e.g., the Squid caching proxy orga-
nizes some state based on a traditional 5-tuple and some state based
on a URL. Prior works [34] assume NFs allocate and organize state
in particular ways (e.g., allocate state once for each ﬂow), which
means NFs may need signiﬁcant changes to use these frameworks.
4.2 API to Export/Import State
We leverage our taxonomy to design a simple API for NFs to
export and import pieces of state; it requires minimal NF modiﬁca-
tions. In particular, we leverage the well deﬁned notion of a ﬂow
(e.g., TCP or UDP connection) and our deﬁnition of state scope to
allow a controller to specify exactly which state to export or import.
State gathering and merging is delegated to NFs which perform
these tasks within the context of their existing internal architecture.
For each scope we provide three simple functions: get, put, and
delete. More formally, the functions are:
multimap getPerflow(ﬁlter)
void putPerflow(multimap)
void delPerflow(list)
multimap getMultiflow(ﬁlter)
void putMultiflow(multimap)
void delMultiflow(list)
list getAllflows()
void putAllflows(list)
A ﬁlter is a dictionary specifying values for one or more stan-
dard packet header ﬁelds (e.g., source/destination IP, network pro-
tocol, source/destination ports), similar to match criteria in Open-
Flow [29].6 This deﬁnes the set of ﬂows whose state to get/put/del-
5NF-speciﬁc state sharing features, such as inter-cache protocols
in Squid, can also be leveraged, but they do not avoid the need for
per-ﬂow state, and some multi-ﬂow state, to be moved or copied.
6Some NFs may also support extended ﬁlters and ﬂowids that in-
clude header ﬁelds for other common protocols: e.g., the Squid
caching proxy may include the HTTP URL.
166ete. Header ﬁelds not speciﬁed are assumed to be wildcards. The
getAllflows and putAllflows functions do not contain a
ﬁlter because they refer to state that applies to all ﬂows. Similarly,
there is no delAllflows function because all-ﬂows state is al-
ways relevant regardless of the trafﬁc an NF is processing.
A chunk of state consists of one or more related internal NF
structures, or objects, associated with the same ﬂow (or set of ﬂows):
e.g., a chunk of per-ﬂow state for the Bro IDS contains a Conn ob-
ject and all per-ﬂow objects it references (Figure 1). A correspond-
ing ﬂowid is provided for each chunk of per-ﬂow and multi-ﬂow
state. The ﬂowid is a dictionary of header ﬁelds and values that
describe the exact ﬂow (e.g., TCP or UDP connection) or set of
ﬂows (e.g., host or subnet) to which the state pertains. For exam-
ple, a per-ﬂow chunk from the Bro IDS has a ﬂowid that includes
the source and destination IPs, ports, and transport protocol, while
a multi-ﬂow chunk containing a counter for an end-host has a ﬂowid
that only includes the host’s IP.
When getPerflow or getMultiflow is called, the NF is
responsible for identifying and providing all per-ﬂow or multi-ﬂow
state that pertains to ﬂows matching the ﬁlter. Crucially, only ﬁelds
relevant to the state are matched against the ﬁlter; other ﬁelds in
the ﬁlter are ignored: e.g., in the Bro IDS, only the IP ﬁelds in a
ﬁlter will be considered when determining which end-host connec-
tion counters to return. This API design avoids the need for a con-
trol application to be aware of the way an NF internally organizes
state. Additionally, by identifying and exporting state on-demand,
we avoid the need to change an NF’s architecture to conform to a
speciﬁc memory allocation strategy [34].
The NF is also responsible for replacing or combining existing
state for a given ﬂow (or set of ﬂows) with state provided in an invo-
cation of putPerflow (or putMultiflow). Common methods
of combining state include adding or averaging values (for coun-
ters), selecting the greatest or least value (for timestamps), and cal-
culating the union or intersection of sets (for lists of addresses or
ports). State merging must be implemented by individual NFs be-
cause the diversity of internal state structures makes it prohibitive
to provide a generic solution.
4.3 API to Observe/Prevent State Updates
The API described above does not interpose on internal state cre-
ations and accesses. However, there are times when we need to
prevent an NF instance from updating state—e.g., while state is be-
ing moved—or we want to know updates are happening—e.g., to
determine when to copy state.
OpenNF uses two mechanisms to prevent and observe updates:
(1) having NFs generate packet-received events for certain packets—
the controller tells the NF which subset of packets should trigger
events—and (2) controlling how NFs should act on the packets that
generate events—process, buffer, or drop them.
Speciﬁcally, we add the following functions to the API:
void enableEvents(ﬁlter,action)
void disableEvents(ﬁlter)
The ﬁlter deﬁnes the set of packets that should trigger events; it
has the same format as described in §4.2. The action may be
process, buffer, or drop; any buffered packets are released to
the NF for processing when events are disabled. The events them-
selves contain a copy of the triggering packet.
In the next section, we discuss how events are used to realize
important guarantees on state and state operations.
5. NORTHBOUND API
OpenNF’s northbound API allows control applications to ﬂex-
ibly move, copy, or share subsets of state between NF instances,
(a) Off-path NF
(b) On-path NF
Figure 4: Assumed topologies for move operation
and to request important guarantees, including loss-freedom, order-
preservation, and various forms of consistency. This API design ap-
propriately balances OpenNF’s generality and complexity: Not of-
fering some guarantees would reduce complexity but make OpenNF
insufﬁcient for use with many NFs—e.g., a redundancy elimina-
tor [16] will incorrectly reconstruct packets when re-ordering oc-
curs (§5.1.2). Similarly, always enforcing the strongest guarantees
would simplify the API but make OpenNF insufﬁcient for scenar-
ios with tight SLAs—e.g., a loss-free and order-preserving move is
unnecessary for a NAT, and the latency increase imposed by these
guarantees (§8.1) could cripple VoIP sessions.
The main challenge in supporting this API is designing suit-
able, low-overhead mechanisms to provide the necessary guaran-
tees. In this section, we show how we use events together with ﬁne-
grained control over network forwarding to overcome this chal-
lenge. We ﬁrst describe how we provide a loss-free and order-pre-
serving move operation (we provide a formal proof of these guar-
antees in our technical report [23]), and what optimizations we use
to improve efﬁciency. We then describe how OpenNF’s copy and
share operations provide eventual, strong, or strict consistency
for state required by multiple NF instances.
5.1 Move Operation
OpenNF’s move operation transfers both the state and input (i.e.,
trafﬁc) for a set of ﬂows from one NF instance (srcInst) to another
(dstInst). Its syntax is:
move(srcInst,dstInst,ﬁlter,scope,properties)
As in the southbound API, the set of ﬂows is deﬁned by ﬁlter; a
single ﬂow is the ﬁnest granularity at which a move can occur. The
scope argument speciﬁes which class(es) of state (per-ﬂow and/or
multi-ﬂow) to move, and the properties argument deﬁnes whether
the move should be loss-free (§5.1.1) and order-preserving (§5.1.2).
In what follows, sw denotes the last SDN switch through which
all packets matching ﬁlter will pass before diverging on their paths
to reach srcInst and dstInst (Figure 4). We assume the SDN con-
troller keeps track of sw. We also assume that loss and reordering
does not occur on the network path from sw to srcInst; our techni-
cal report [23] includes a stronger version of order-preserving move