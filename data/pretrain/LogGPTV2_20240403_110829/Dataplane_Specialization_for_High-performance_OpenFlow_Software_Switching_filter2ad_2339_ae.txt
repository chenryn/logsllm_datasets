In contrast,
ESWITCH’s compact custom datapaths deliver high switch
performance and stable and small working set size.
Latency. The mean time for a packet to traverse the dat-
apath is given in Fig. 16. For ESWITCH, we get about 0.1
µsec packet processing time independently of the active ﬂow
set, while latency for OVS varies between 0.2–13 µsec. This
shows that a compiled datapath yields smaller and predictable
latency compared to a ﬂow-caching-based switch.
Update processing. For the ﬁrst sight, one would consider
template-compilation prohibitive for update-intensive work-
loads, like cloud hypervisor switches. To show that this is
not the case, we measured the time it takes for OVS and
ESWITCH to set up the complete load-balancer use case at
different scales of pipeline complexity. The switches were
fed ﬁrst from a command line tool (ovs-ofctl, CLI) and
then using the Ryu OpenFlow controller (we got similar re-
sults with OpenDaylight). The results are in Fig. 17 (note the
log-log scale). Both switches scale linearly, but in general it
takes just one ﬁfth the time for ESWITCH to set up the use
case than for OVS, when using the CLI tool. With the con-
troller the two perform similarly. Overall, this indicates that
it is the OpenFlow controller, rather than ESWITCH itself,
that bottlenecks update rates, which justiﬁes our choice of
template-based compilation over slower methods that may
548
2M6M10M12M1101001K10K100Kpacket rate [pps]number of active flowsES(1)ES(10)ES(100)OVS(1)OVS(10)OVS(100)2M6M10M12M1101001K10K100K1Mpacket rate [pps]number of active flowsES (model-ub)ES (measured)ES (model-lb)OVS 0 0.2 0.4 0.6 0.8 11101001K10K100K1Mcache hit rate/packetnumber of active flowsmicroflowmegaflowvswitchd 0.01 0.1 1 101101001K10K100K1MLLC-load-misses/packetnumber of active flowsESOVS2005001K2K1101001K10K100K1MCPU cycles/packetnumber of active flowsES (model-ub)ESES (model-lb)OVS10-210-11101101001K10K100Ktotal update time [sec]number of web servicesES (CLI)ES (ctrl)OVS (CLI)OVS (ctrl)Pipeline stage CPU cycles Comment
PKT_IN
parser template 28
hash template 1 8 + L1
hash template 2 8 + Lx
LPM template
action templates 25
PKT_OUT
40
40
DPDK packet receive
IO
Parse header ﬁelds
Table 0 lookup
Per-CE table lookup
13+2∗Lx Routing table LPM
Action set processing
DPDK packet
transmit IO
Figure 18: Normed packet rate (rela-
tive to the unloaded case) in the gate-
way use case (1K active ﬂows), as the
update intensity grows to 100K/sec.
Figure 19: Packet rate as the number
of packet processing CPU cores grows;
L3 routing 100, 10K, resp. 500K active
ﬂows over 2K IP preﬁxes.
Figure 20: Performance model for the gate-
way use case: estimated number of CPU
cycles spent per pipeline stage (Lx denotes
access time for CPU cache level x).
potentially generate better code. Further, we observed that
ESWITCH packet processing is more robust in the face of
updates; ESWITCH churns out 95% of its nominal packet
rate when the last level IP routing table in the gateway use
case (Table 110) is updated 100 times per second and even at
100K update/sec intensity it maintains 80% of its unloaded
performance; contrarily, OVS throughput falls by more than
65% even for 100 updates/sec due to deteriorating ﬂow cache
hit rates (Fig. 18). For batched updates (20 ﬂow add and
delete operations periodically), we saw at most 3% change
in ESWITCH packet rate (most probably due to cold CPU
caches) while for OVS the throughput drop reached 23%.
This is because in ESWITCH datapath updates are, in con-
trast to OVS, per-table and usually non-destructive.
CPU scalability. Finally, we show that previous single-
core results extend nicely to multiple CPUs. Since the Intel
XL710 NIC in the SUT supports only about 23 Mpps packet
rate with 64-byte packets [53,54], ESWITCH proves too fast
for this experiment: it saturates the NIC even with just two
cores. To still make packet forwarding CPU-bounded rather
than IO-bounded, we had to downgrade to a slower 2.40GHz
Intel Atom platform. The evaluations were performed on the
L3 use case, with a subset of 2K routes obtained from a real
router (see Fig. 19). Both OVS and ESWITCH show strong
linear CPU scaling (apart from a small reproducible glitch
with 5 cores), but ESWITCH consistently outperforms OVS
roughly 5-fold and the gap increases with more ﬂows and
more CPU cores. Again, ESWITCH performance is robust
against the number of active ﬂows.
4.4 Performance Estimation
A crucial advantage of dataplane specialization is that the
compiled datapaths are simple enough to lend themselves
to coarse-grained performance models. After all, a com-
piled datapath is just a handful of templates linked into a bi-
nary and so we can deﬁne elementary performance “atoms”
to characterize each template and track down the template
generation process to combine these atoms into composite
datapath models. Such models can then yield coarse per-
formance estimates, providing operators with quick perfor-
mance promises, supporting network function placement, etc.
We demonstrate the derivation of such a model on the
549
gateway pipeline. Our measurements indicated that it is the
user-network direction, involving a costly longest preﬁx match
on a largish routing table, that dominates performance for
this use case. Fig. 20 gives a rundown on the templates used
for compiling this pipeline direction.
A quick analysis of the assembly code suggests that the
most expensive operations in the compiled datapath are the
memory fetches. We also observe that ESWITCH performs
very few last-level CPU cache misses (roughly one for every
10th packet, Fig. 15). Based on these considerations, we can
divide per-packet cost into two components, a ﬁx cost that is
invariant for each packet and a variable cost component that
changes with the distribution of accesses between the L1, L2
and L3 CPU caches and, correspondingly, with the working
set size (the amount of total data accessed by the datapath)
determined by the number of active ﬂows. The ﬁx cost can
come from assembly code analysis, CPU cycle-count mea-
surements, and basic common sense, while the variable cost
component is shaped by CPU memory access speed.
Static code analysis yields that a generic DPDK packet
IO takes about 40-50 CPU cycles (the NIC loads the packet
directly into the L3 cache, from where the ﬁrst 64 bytes con-
taining the header is fetched by the CPU in a single L3 load),
packet parsing takes 28 cycles, and applying actions another
25. Table 0 will compile into the hash template but the size is
small enough to warrant a safe L1 CPU cache access, taking
8 + L1 CPU cycles where Lx denotes the number of cycles
needed to access the cache at level x. The per-CE tables
again use the hash template but, being variable size, may
access the L2 or the L3 cache, which takes 8 + Lx cycles.
Finally, the LPM stage, using DPDK’s built-in DIR-24-8
data structure, runs in 13 + 2∗ Lx cycles, assuming that each
LPM search needs two memory accesses. Eventually, we get
166 + 3 ∗ Lx cycles per packet.
Of course, such models can never aim to be comprehen-
sive, as CPU pipeline semantics, branch prediction misses,
cache collisions, etc., greatly inﬂuence real performance.
That being the case, we can still use the model to obtain
simple best-case and worst-case throughput estimates. The
optimistic presumption that all cache accesses succeed from
the L1 cache would give 178 cycles/packet and 11.2 Mpps
packet rate. A slightly less optimistic assumption is that,
0.20.40.60.81.01101001K10K100Knormed packet rate [pps]number of updates per secondsOVSES2M6M10M14M12345packet rate [pps]number of CPU coresES (100 flow)ES (10K flow)ES (500K flow)OVS (100 flow)OVS (10K flow)OVS (500K flow)at roughly 1K active ﬂows, the working set size grows large
enough to shift memory accesses to the L2 CPU cache, which
gives 202 cycles/packet and 9.9 Mpps throughput. These op-
timistic assumptions suggest a rude upper bound on achiev-
able packet rate. Conversely, a pessimistic assumption will
constrain all memory accesses to the L3 cache, yielding 253
cycles/packet and 7.9 Mpps lower bound on the throughput.
When validated against real measurements, these bounds
turn out to provide surprisingly useful performance hints,
both in terms of packet rates (Fig. 13) and per-packet pro-
cessing cost (Fig. 16). Our experiments so far with similar
performance models have given promising results. For most
scenarios the models deliver close performance estimates for
at least a limited regime of the conﬁguration space, even on
workloads as complex as the gateway use case. We have
seen cases, however, when the predicted performance was
off by an order of magnitude; further research is therefore
needed to make our models reliable and to eliminate, or to at
least prognosticate, such pathologic cases.
5. RELATED WORK
Recent work takes a programming-language approach to
leverage compilers and runtime systems to optimize Open-
Flow performance [8–10]. These compilers, however, are
to provide high-level human-centric abstractions to network
conﬁguration, whereas ESWITCH shoots at a lower-level of
the OpenFlow food chain: compiling a pipeline speciﬁcation
to the bare metal. In this regard, ESWITCH is closer to pro-
posals like NOSIX [34], P4 [55], and RMT [56], aimed at
ﬁnding a better match between controller programs and the
underlying dataplane. The closest to our approach is perhaps
network stack specialization [57], but instead of concentrat-
ing on endhosts herein we focus on intermediate systems.
Of particular interest here is P4 [49, 55]. Both P4 and
ESWITCH are datapath compilers, emitting efﬁcient fast-paths
from an abstract, declarative description of a switch’s packet
processing functionality. P4, however, is much more generic
than ESWITCH as it offers a complete language to customize
switch behavior, all the way from header formats and sup-
ported actions to the semantics of ﬂow tables and the control
of ﬂow among them; ESWITCH is, in contrast, limited to
OpenFlow, with hard-coded header formats, actions, match-
ing semantics, etc. On the other hand, ESWITCH can poten-
tially generate a more efﬁcient fast-path than an equivalent
P4 program would do; in contrast to P4 that has only an
abstract dataplane description available at compile time but
not the ﬂow entries themselves, ESWITCH also knows the
content of the pipeline; correspondingly, P4 is constrained
to produce the datapath statically while ESWITCH can ap-
ply sophisticated run-time optimizations in full knowledge
of the pipeline, like upgrading small tables to the direct code
template, template specialization with full constant inlining
and direct jump pointers, etc. Of course, run-time optimiza-
tion does not come for free, as ESWITCH needs to partially
recompile the datapath from time to time but, as we have
shown experimentally, this is very cheap thanks to the efﬁ-
ciency of template-based code generation.
Flow table templates are common in hardware OpenFlow
switches, which usually include separate pipeline stages sup-
porting varying sorts of match semantics [34]; herein, we
simply adopted this design for softswitches. Template-based
program specialization, a technique to dynamically link pre-
compiled code fragments into machine code [58, 59], has
for a long time been the preferred choice for writing com-
pilers for embedded domain-speciﬁc languages, like SQL
[60] or LINQ [61], thanks to the fast compilation cycles.
Notably, it also allows to eliminate looping overhead and
fold constants into instructions. ESWITCH heavily builds
on these features. Recently runtime code generation has re-
ceived renewed interest, for just-in-time (JIT) compiling hot
code paths from intermediate representations into machine
code [62]. The analogous JIT compilation of “hot ﬂows” by
ESWITCH seems a particularly intriguing research direction.
Performance modeling and prediction for network func-
tions was initiated in [63] and has been shown to provide
useful characterizations for various real-life use cases [64,
65]. Nevertheless, as far as we know this is the ﬁrst time
that a, however simple, datapath performance model for a
complex OpenFlow pipeline appears in the literature. In the
future ESWITCH could be easily taught to derive such mod-
els automatically, by programmatically composing template
model “atoms” using the primitives introduced in Atomix
[66]. This would make it possible to not only produce ef-
ﬁcient specialized datapaths but also to deliver reliable per-
formance promises for these datapaths in real time.
6. CONCLUSIONS
OpenFlow software switches are indeed true masterpieces
of genericity, supporting a broad spectrum of packet for-
warding semantics with considerable efﬁciency. Unfortu-
nately, genericity comes at a high cost: despite increasingly
powerful hardware operators still often need to manually
tweak their pipelines to work around softswitch performance
regressions. In this paper, we argued that this should be the
other way around: instead of customizing ﬂow tables for the
underlying data plane it is rather the dataplane that should
be specialized with respect to the workload.
We introduced ESWITCH, a novel switch architecture cap-
italizing on the observation that OpenFlow pipelines are suf-
ﬁciently structured to admit efﬁcient machine code repre-
sentations constructed out of simple packet processing tem-
plates. The resultant specialized datapaths then were shown
to give major performance gains over ﬂow-caching-based al-
ternatives, with several times higher raw packet rates, much
smaller latency, and, perhaps most importantly, robust and
predictable throughput even on widely varying, or border-
line malicious, workloads. The proposed switch architec-
ture easily scales to hundreds of ﬂow tables and hundreds of
thousands of trafﬁc ﬂows, while supporting updating the fast
path at similar, or higher, intensity.
Acknowledgements
The authors would like to thank the support for the HSNLab
at BME-TMIT and the MTA-BME Future Internet Research
550
Group. G.R. was visiting TrafﬁcLab, Ericsson Research,
Hungary, while working on this paper. Corresponding au-
thor: Gábor Rétvári .
7. REFERENCES
[1] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner,
“OpenFlow: enabling innovation in campus networks,” ACM