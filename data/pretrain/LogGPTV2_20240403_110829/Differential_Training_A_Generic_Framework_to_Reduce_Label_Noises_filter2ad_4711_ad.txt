68.09%
8,135%
TABLE VIII: Noise Reduction on Drebin Dataset at Different Noise Ratios
Noise Ratio
# of Training Set
# of Wrongly-labeled Samples
# of TP Noise Detection Results
# of FP Noise Detection Results
% of Noise Reduced
# of Wrongly-labeled Samples Left
5%
103,210
5,160
4,658
788
75.00%
1,290
10%
103,210
10,321
9,121
605
82.51%
1,805
15%
103,210
15,482
12,700
878
76.36%
3,660
20%
103,210
20,400
17,870
2,232
76.66%
4,762
30%
103,210
30,936
26,120
1,875
78.37%
6,691
45%
103,210
46,445
44,804
20,247
52.87%
21,888
TABLE IX: Noise Reduction on DeepReﬁner Dataset at Different Noise Ratios
Noise Ratio
# of Training Set
# of Wrongly-labeled Samples
# of TP Noise Detection Results
# of FP Noise Detection Results
% of Noise Reduced
# of Wrongly-labeled Samples Left
5%
88,352
4,415
3,985
1,247
62.02%
1,677
10%
88,352
8,835
7,947
2,230
64.71%
3,118
15%
88,352
13,253
12,406
3,677
65.86%
4,524
20%
88,352
17,670
14,737
5,150
54.26%
8,083
30%
88,352
26,502
21,707
8,799
48.71%
13,594
45%
88,352
39,758
23,604
14,795
22.15%
30,949
labels. Though this percentage is lower than the other cases in
the experiments, it is still substantial in this extreme case.
Table VIII shows a similar trend for Differential Training’s
effectiveness on noise reduction working on Drebin dataset
at various noise ratios. The percentage of noisy labels being
reduced ﬂuctuates from 75.00% to 82.51% if the noise ratio
varies between 5% and 30%, and it decreases to 52.87% at
noise ratio 45%.
Table IX shows a wider ﬂuctuation margin of noise label
detection rate on DeepReﬁner dataset, which varies from
48.71% to 65.86% for the noise ratio range of 5% to 30%.
This wider ﬂuctuation margin is probably due to the relatively
simple feature set selected by DeepReﬁner as compared to
more comprehensive feature sets used by SDAC and Drebin.
Nonetheless, Differential Training can still detect nearly half
of wrong labels even if the noise ratio is as high as 30% in
the training set. While in the extremely noisy case for 45% of
noise ratio, Differential Training reduces 22% of wrong labels.
While this result is lower than the other cases as the noise ratio
is close to random labelling, the effectiveness of Differential
Training is still non-negligible in reducing the noise in the
10
training dataset.
Tables VII, VIII, and IX also show a trend between the
detection performance of Differential Training and the noise
ratio in the dataset. When the noise ratio ranges from 5%
to 30%, the detection accuracy does not change much; while
the ratio increases to 45%, a signiﬁcant decrease in detection
accuracy is observed.
Differential Training detects label noises as outliers. When
there are almost the same number of outliers as non-outliers,
it is difﬁcult for Differential Training to distinguish between
outliers and non-outliers, leading to a signiﬁcant drop in its
detection accuracy.
IX. COMPARISON AMONG DIFFERENTIAL TRAINING,
CO-TEACHING, AND DECOUPLING ON NOISE REDUCTION
In this section, we compare Differential Training with
two state-of-the-art robust learning algorithms, including Co-
Teaching [20] and Decoupling [27], both of which are designed
for training robust deep neural networks with noisy labels.
initialized independently at
Co-Teaching trains two neural networks simultaneously
on a noisy dataset, where the two models are of identical
architecture but
the beginning.
Given each mini-batch of the dataset, each network views
its small-loss data samples as potentially-clean samples, and
provides them to its peer network for updating the parameters
in the peer network. In Co-Teaching, the two networks have
different learning abilities; they can thus ﬁlter each other’s
different types of error that are introduced by noisy labels
in the learning process. After training, the two fully-trained
models cooperate to output a predicted label for each sample
in the testing phase, where the predicted label is determined by
an output weight that is the sum of the output weights of the
input sample from the two models. Co-Teaching can be used to
detect noisy labels. If the predicated label of an input sample
is different from the original label of the input sample, the
original label is considered as a noisy label, and thus ﬂipped.
The source code of this algorithm is publicly available and
provided in [1].
Decoupling also trains two neural networks simultaneously
on a noisy dataset, with these two networks being of the
same structure but initialized independently. During the model
training, each mini-batch of the dataset is fed to both models
simultaneously to generate the prediction results. If a sample in
the mini-batch is predicted with different labels from the two
models, it is regarded as meaningful for the model learning
and only the “meaningful” samples in the mini-batch are later
used in the backward propagation step to update the parameters
in both models. At the end of training, Decoupling randomly
chooses one of the two trained models as the produced classi-
ﬁer. Decoupling can be used to detect noisy labels. If an input
sample’s label that is predicted by the produced classiﬁer is
different from its original label, the original label is considered
as noisy, and thus ﬂipped for correction. Decoupling’s source
code is publicly available and provided in [2].
We apply both Differential Training, Co-Teaching, and
Decoupling to the noisy versions of all three datasets, including
SDAC dataset, Drebin dataset, and DeepReﬁner dataset, where
the noise ratio is set to 10% as in the default setting. We
TABLE X: Comparison between Differential Training (DT),
Co-Teaching (CT) and Decoupling (DC)
% of Wrongly Labels
Reduced by DT
% of Wrongly Labels
Reduced by CT
% of Wrongly Labels
Reduced by DC
# of TP/FP Noises
Detected by DT
# of TP/FP Noises
Detected by CT
# of TP/FP Noises
Detected by DC
Detection results (F-score)
on Noisy dataset
Detection results (F-score)
on dataset processed by DT
Detection results (F-score)
on dataset processed by CT
Detection results (F-score)
on dataset processed by DC
Runtime Performance
of DT (hour)
Runtime Performance
of CT (hour)
Runtime Performance
of DC (hour)
SDAC
Dataset
87.45%
76.49%
68.43%
Drebin
Dataset
82.51%
78.14%
65.37%
DeepReﬁner
Dataset
64.71%
21.72%
29.80%
5,246/343
9,121/605
7,497/2,230
5,131/841
8,997/933
3,311/1,392
4,305/463
8,107/1,360
4,392/1,606
89.04%
97.19%
96.01%
92.38%
55.34
4.08
2.24
73.20%
84.40%
77.36%
79.80%
62.52
20.09
21.71
91.37%
93.41%
93.33%
92.19%
102.12
23.93
14.15
compare their performances in terms of the percentage of
wrongly labels being detected/reduced in the noisy datasets.
For fair comparison, the neural networks used in Co-Teaching
or Decoupling are chosen to be the same as the ones used in
Differential Training, being two-layer MLP networks with 500
nodes in the ﬁrst layer and 1000 nodes in the second layer.
Table X compares Differential Training with Co-Teaching
and Decoupling in terms of noise detection result and runtime
performance. The runtime performance is evaluated on a single
desktop personal computer without GPU. The PC is equipped
with one Intel(R) i5-4590 3.3 GHz CPU and 12 GB physical
memory running on the Ubuntu 14.04 (LTS) operating system.
The table shows that while Differential Training takes
longer time than Co-teaching and Decoupling, it outperforms
these two approaches considerably in all three cases in terms
of noise detection accuracy. Speciﬁcally, Differential Training
produces the most True-Positive (TP) results and the least
False-Positive (FP) results in all
the cases except for the
FP result in the case of DeepReﬁner. The malware detection
results (F-score) with the datasets processed by Differential
Training are also better than those processed by the other two
noise detection approaches.
Different strategies are exploited for identifying or process-
ing noise samples. Differential Training identiﬁes a sample as
“noise” based on all of its loss values in the whole training
process, while Co-Teaching treats a sample to be potentially-
clean based on its individual loss value in each mini-batch, and
Decoupling identiﬁes a noise sample based on its prediction
result that is related to its loss value in the last epoch only. Our
comparison shows that the strategy exploited by Differential
Training is more reliable in detecting noisy labels than other
strategies exploited by Co-Teaching and Decoupling.
11
X. DISCUSSION
A. Limitation
Time cost of Differential Training. As shown in section IX,
Differential Training has a higher time cost than Co-teaching
and Decoupling. This is mainly because Differential Training
reduces the label noises in a gradual way in multiple iterations,
while in each iteration two separate classiﬁcation models (i.e.,
WS model and DS model) are trained. In comparison, both
Co-teaching and Decoupling rely on two classiﬁcation models
being trained in a single iteration.
Accuracy of noise ratio estimation. The accuracy of the
noise ratio estimation used in outlier detection may affect the
performance of Differential Training. If the estimated noise
ratio is signiﬁcantly different from the actual ratio, Differential
Training may produce more false positives and false negatives
in identifying noise labels. While the noise ratio estimation
algorithm we adopted enables Differential Training to outper-
form Co-Teaching and Decoupling in rigorous experiments,