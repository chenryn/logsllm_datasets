title:Keep net working - on a dependable and fast networking stack
author:Tom&apos;as Hrub&apos;y and
Dirk Vogt and
Herbert Bos and
Andrew S. Tanenbaum
Keep Net Working - On a Dependable and Fast Networking Stack
Tomas Hruby
The Network Institute, VU University Amsterdam
Dirk Vogt
{thruby,dvogt,herbertb,ast}@few.vu.nl
Herbert Bos
Andrew S. Tanenbaum
Abstract—For many years, multiserver1 operating systems
have been demonstrating, by their design, high dependability
and reliability. However, the design has inherent performance
implications which were not easy to overcome. Until now
the context switching and kernel involvement in the message
passing was the performance bottleneck for such systems to
get broader acceptance beyond niche domains. In contrast to
other areas of software development where ﬁtting the software
to the parallelism is difﬁcult, the new multicore hardware is a
great match for the multiserver systems. We can run individual
servers on different cores. This opens more room for further
decomposition of the existing servers and thus improving
dependability and live-updatability. We discuss in general the
implications for the multiserver systems design and cover in
detail the implementation and evaluation of a more dependable
networking stack. We split the single stack into multiple servers
which run on dedicated cores and communicate without kernel
involvement. We think that the performance problems that
have dogged multiserver operating systems since their inception
should be reconsidered: it is possible to make multiserver
systems fast on multicores.
Keywords-Operating systems; Reliability; Computer network
reliability; System performance
I. INTRODUCTION
Reliability has historically been at odds with speed—as
witnessed by several decades of criticism against multiserver
operating systems (“great for reliability, but too slow for
practical use”). In this paper, we show that new multicore
hardware and a new OS design may change this.
Reliability is crucial in many application domains, such as
hospitals, emergency switchboards, mission critical software,
trafﬁc signalling, and industrial control systems. Where
crashes in user PCs or consumer electronics typically mean
inconvenience (losing work, say, or the inability to play
your favorite game), the consequences of industrial control
systems falling over go beyond the loss of documents, or
the high-score on Angry Birds. Reliability in such systems
is taken very seriously.
By radically redesigning the OS, we obtain both the fault
isolation properties of multiserver systems, and competitive
performance. We present new principles of designing multi-
server systems and demonstrate their practicality in a new
network stack to show that our design is able to handle very
high request rates.
The network stack is particularly demanding, because it
is highly complex, performance critical, and host to several
catastrophic bugs, both in the past [14] and the present [4].
Mission-critical systems like industrial control systems often
cannot be taken ofﬂine to patch a bug in the software stack—
such as the recent vulnerability in the Windows TCP/IP
stack that generated a storm of publicity [4]. When uptime is
critical, we need to be able to patch even core components
like the network stack on the ﬂy. Likewise, when part of the
stack crashes, we should strive toward recovery with minimal
disturbance—ideally without losing connections or data.
In this paper, we focus on the network stack because it
is complex and performance critical, but we believe that the
changes we propose apply to other parts of the operating
system as well. Also, while we redesign the OS internals,
we do not change the look and feel of traditional operating
systems at all. Instead, we adhere to the tried and tested
POSIX interface.
Contributions: In this paper, we present a reliable and
well-performing multiserver system NewtOS2 where the
entire networking stack is split up and spread across cores to
yield high performance, fault isolation and live updatability
of most of the stack’s layers. We have modiﬁed Minix 3 [1]
and our work has been inspired by a variety of prior art,
such as Barrelﬁsh [5], fos [43], FlexSC [39], FBufs [12],
IsoStack [37], Sawmill Linux [16] and QNX [33]. However,
our design takes up an extreme point in the design space,
and splits up even subsystems (like the network stack) that
run as monolithic blobs on all these systems, into multiple
components.
The OS components in our design run on dedicated cores
and communicate through asynchronous high-speed channels,
typically without kernel involvement. By dedicating cores
and removing the kernel from the fast path, we ensure
caches are warm and eliminate context switching overhead.
Fast, asynchronous communication decouples processes on
separate cores, allowing them to run at maximum speed.
Moreover, we achieve this performance in spite of an
extreme multiserver architecture. By chopping up the net-
working stack into many more components than in any other
system we know, for better fault isolation, we introduce even
more interprocess communication (IPC) between the OS
1Operating systems implemented as a collection of userspace processes
(servers) running on top of a microkernel
2A newt is a salamander that, when injured, has the unique ability to
re-generate its limbs, eyes, spinal cord, intestines, jaws and even its heart.
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
components. As IPC overhead is already the single most im-
portant performance bottleneck on multiserver systems [26],
adding even more components would lead to unacceptable
slowdowns in existing OS designs. We show that a careful
redesign of the communication infrastructure allows us to
run at high speeds despite the increase in communication.
Breaking up functionality in isolated components directly
improves reliability. Making components smaller allows us to
better contain the effects of failures. Moreover, components
can often be restarted transparently, so that a bug in IP,
say, will not affect TCP. Our system recovers seamlessly
from crashes and hangs in drivers, network ﬁlters, and most
protocol handlers. Since the restarted component can easily
be a newer or patched version of the original code, the same
mechanism allows us to update on the ﬂy many core OS
components (like IP, UDP, drivers, packet ﬁlters, etc.).
The OS architecture and the current trend towards many-
core hardware together allow, for the ﬁrst time, an architecture
that has the reliability advantages of multiserver systems and
a performance approximating that of monolithic systems [7]
even though there are many optimizations left to exploit. The
price we pay is mainly measured in the loss of cores now
dedicated to the OS. However, in this paper we assume that
cores are no longer a scarce resource as high-end machines
already have dozens of them today and will likely have even
more in the future.
Outline: In Section II, we discuss the relation between
reliability, performance, multiservers and multicores. Next,
in Section III, we explain how a redesign of the OS greatly
improves performance problems without giving up reliability.
We present details of our framework in Section IV and
demonstrate the practicality of the design on the networking
stack in Section V. The design is evaluated in Section VI
We compare our design to related work in Section VII and
conclude in Section VIII.
II. RELIABILITY, PERFORMANCE AND MULTICORE
Since it is unlikely that software will ever be free of bugs
completely [19], it is crucial that reliable systems be able
to cope with them. Often it is enough to restart and the
bug disappears. For reliability, new multicore processors
are double-edged swords. On the one hand,
increasing
concurrency leads to new and complex bugs. On the other
hand, we show in this paper that the abundance of cores and a
carefully designed communication infrastructure allows us to
run OS components on dedicated cores—providing isolation
and fault-tolerance without the performance problems that
plagued similar systems in the past.
Current hardware trends suggest that the number of cores
will continue to rise [2], [3], [23], [29], [35], [36] and that the
cores will specialize [31], [39], [41], for example for running
system services, single threaded or multithreaded applications.
As a result, our view on processor cores is changing, much
like our view on memory has changed. There used to be a
time when a programmer would know and cherish every byte
in memory. Nowadays, main memory is usually no longer
scarce and programmers are not shy in wasting it if doing so
improves overall efﬁciency—there is plenty of memory. In the
same way, there will soon be plenty of cores. Some vendors
already sacriﬁce cores for better energy efﬁciency [2], [3].
The key assumption in this paper is that it is acceptable to
utilize extra cores to improve dependability and performance.
Unfortunately, increasing concurrency makes software
more complex and, as a result, more brittle [24]. The OS is no
exception [28], [32], [34]. Concurrency bugs lead to hangs,
assertion failures, or crashes and they are also particularly
painful, as they take considerably longer to ﬁnd and ﬁx than
other types of bugs [24].
Thus, we observe (a) an increase in concurrency (forced by
multicore hardware trends), (b) an increase in concurrency
bugs (often due to complexity and rare race conditions),
and (c) systems that crash or hang when any of the OS
components crashes or hangs. While it is hard to prevent (a)
and (b), we can build a more reliable OS that is capable of
recovering from crashing or hanging components, whether
they be caused by concurrency bugs or not.
Our design improves OS reliability both by structural
measures that prevent certain problems from occurring in
the ﬁrst place, and by fault recovery procedures that allow
the OS to detect and recover from problems. Structural
measures include fault isolation by running OS components
as unprivileged user processes, avoiding multithreading in
components, and asynchronous IPC. For fault recovery, we
provide a monitor that checks whether OS components are
still responsive and restarts them if they are not.
The research question addressed in this paper is whether we
can provide such additional reliability without jeopardizing
performance. In existing systems, the answer would be: “No”.
After all, the performance of multiserver degrades quickly
with the increase in IPC incurred by splitting up the OS in
small components.
This is true even for microkernels like L4 that have
optimized IPC greatly [26], and is the main reason for the
poor performance of multiserver systems like MINIX 3 [1].
However fast we make the mechanism, kernel-based IPC
always hurts performance: every trap to the kernel pollutes the
caches and TLB with kernel stuff, ﬂushes register windows,
and messes up the branch predictors.
In the next section, we discuss how we can reduce this
cost in a new reliable OS design for manycore processors.
III. RETHINKING THE INTERNALS
As a ﬁrst step, and prior to describing our design, we
identify the tenets that underlie the system. Speciﬁcally,
throughout this work we adhere to the following principles:
1) Avoid kernel involvement on the fast path. Every
trap in the kernel pollutes caches and branch predictors
and should be avoided when performance counts.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1. Conceptual overview of NewtOS. Each box represents a core.
Application (APP) cores are timeshared.
2) Do not share cores for OS components. There is no
shortage of cores and it is ﬁne to dedicate cores to
OS components, to keep the caches, TLBs and branch
prediction warm, and avoid context switching overhead.
3) Split OS functions in isolated components. Multiple,
isolated components are good for fault tolerance: when
a component crashes, we can often restart it. Moreover,
it allows us to update the OS on the ﬂy.
4) Minimize synchronous intra-OS communication.
Synchronous IPC introduces unnecessary waits. Asyn-
chronous communication avoids such bottlenecks. In
addition, asynchrony improves reliability, by preventing
clients from blocking on faulty servers [21].
We now motivate the most interesting principles in detail.
A. IPC: What’s the kernel got to do with it?
All functions in a multiserver system run in isolated servers.
A crash of one server does not take down the entire system,
but the isolation also means that there is no global view of
the system and servers rely on IPC to obtain information
and services from other components. A multiserver system
under heavy load easily generates hundreds of thousands of
messages per second. Considering such IPC rates, both the
direct and indirect cost of trapping to the kernel and context
switching are high.
To meet the required message-rate, we remove the kernel
from high-frequency IPC entirely and replace it with trusted
communication channels which allow fast asynchronous
communication. Apart from initially setting up the channels,
the kernel is not involved in IPC at all (Section IV).
As shown in Figure 1, every OS component in NewtOS
can run on a separate core for the best performance while the
remaining cores are for user applications. The OS components
themselves are single-threaded, asynchronous user processes
that communicate without kernel involvement. This means
no time sharing, no context switching, and competing for
the processor resources. Caching efﬁciency improves both
because the memory footprint of each component is smaller
than of a monolithic kernel, and because we avoid many
expensive ﬂushes. By dedicating cores to OS components, we
further reduce the role of the kernel because no scheduling
is required and dedicated cores handle interrupts. This leaves