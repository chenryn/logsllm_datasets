| Aug. 20th 2017 BY 霍明明
node
node
node
repication
scheduler
1
elet
kubectl
apiserver
node
container
xube
master
WebUI
fire wall
Internet
---
## Page 12
按照功能划分，k8s是由很多松耦合的功能组件构成:
● kube-controller-manager 
●kube-apiserver组件
·etcd 组件
这里以ReplicaSetController为例进行简单阐述。
源进行相应的处理，使得资源状态最终达到规定的状态。
list/watch机制）来注册和发现自己关心的资源状态变化。通过感知资源状态的变化，对这些资
现对各种资源的CRUD操作。这些控制器通过k8s资源注册与发现框架（其中最核心的就是
caSetController、DeploymentController、ConfigMapController等等。通过这些控制器实
kube-controller-manager是k8s多种资源控制器的集合。像ReplicationController、Repli-
接下来我们看看k8s中其它组件是如何利用k8s的资源注册与发现机制实现与k8s完美对接的。
的变化。
于资源发现而言就是想要了解集群中是否存在某资源，并且通过名字就可以查找和监听资源状态
现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接。而对
即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发
然而并没有。服务也可以看成是一种资源。服务发现解决的是分布式系统中最常见的问题之一
服务的注册与发现相信大家都很了解了，那资源的注册与发现和它有啥区别吗？
kube-proxy，还有我们自己实现的lb-controller和log-controller。
实现的。比如下面将要介绍的kube-controller-manager、kube-scheduler、kubelet、
框架是整个k8s架构设计的灵魂。所有k8s的组件，除了etcd外，都是围绕k8s的这一机制去扩展
大家不深入去看k8s各个组件工作机制和源码实现的话，不一定能够体会得到。资源注册与发现
第一点大家都能很轻松的理解，这么大的系统，肯定是要提供一个统一的数据查询接口；第二点
两大功能。一是对外提供查询资源对象的接口；二是提供k8s中资源注册与发现的框架。
从功能实现角度来看，kube-apiserver是对etcd的一个包装，通过对etcd的进一步包装，实现
的枢纽和消息总线。没有kube-apiserver整个k8s系统将会土崩瓦解，无法运转。
是k8s 的入口，也是整个k8s除了etcd外最最基础、最最核心的组件。
这个问题。
快速被集群中的所有机器发现是迫切要解决的问题，也是最最核心的问题。etcd很好的解决了
发现的支持。在云计算时代，如何让服务快速透明地接入到计算集群中，如何让共享配置信息
一个高可用强一致性的服务发现存储仓库，etcd本身非常好的提供了数据的持久化存储和服务
是k8s系统的资源(数据)存储的地方，是整个k8s的基石，也是k8s很多功能的设计之源。做为
基于etcd实现的资源的注册和发现框架是整个k8s生态的灵魂。
容器化－我所理解的 Kubernetes 架构
。它是整个k8s系统和生态
07
---
## Page 13
08我所理解的 Kubernetes 架构－容器化
●kube-proxy
●kubelet
● kube-scheduler
我们自实现的lb-controller、log-controller 
源注册与发现框架从apiserver中获取期望的状态，并调用对应的接口使其达到这个状态。
是service的控制器。它主要关心service、endpoint资源的状态变化，kube-proxy通过k8s资
会为这个LB提供一个IP，一般称为cluster IP。kube-proxy的作用主要是负责service的实现,
service是一组Pod的服务抽象，相当于一组Pod的LB，负责将请求分发给对应的Pod。service
是指的通过k8s资源注册与发现框架从apiserver中获取。
如何配置等等），并调用对应的容器平台接口达到这个状态。
地方获取节点上Pod/Container 的期望状态（运行什么容器、运行的副本数量、网络或者存储
命周期的管理，这个 agent 程序就是 kubelet。简单地说，kubelet 的主要功能就是定时从某个
k8s是一个分布式的集群管理系统，在每个节点（Node）上都要运行一个 agent 对容器进行生
最终返回可用的Node，并将Pod与之绑定。
是基于k8s的资源注册与发现框架来发现Pod创建事件的，然后将Pod入队列等待资源运算处理
然后去做各种资源是否满足需求的处理，但是实际上它并不是直接接收apiserver的请求，而
对OpenStack调度的理解来看，本以为对于kube-scheduler而言也是接收apiserver的请求
kube-scheduler 是k8s资源调度组件，负责Pod资源落到哪个或哪些Node的调度。按照之前
证存活Pod数不变。
的Pod因某种原因挂掉时，ReplicaSetController也会监听到，并重新创建1个新的Pod，保
rs对象，根据对象的资源描述（Pod、Pod副本数等）去创建相应副本数的Pod。当rs中 
ReplicaSet资源时，ReplicaSetController监听到该对象的创建事件，将通过apiserver获取
态变化。在启动时便会通过apiserver的list/watch机制注册并监听所关心的资源。当用户创建
ReplicaSetController是k8s中用来管Pod的控制器，所以主要关心rs本身和Pod资源的状
Recrietet
bect
List Wath 1
iscrete
anager
-------
List-Watch2
etcd
scheduler
上面说的定时从某个地方获取就
-------------7
List-Watch3
kubelet
---
## Page 14
些，有问题欢迎留言哦。
多细节，开发者和用户只关心实现资源状态变化后的处理逻辑就可以了。client-go使用介绍:
client-go，通过client-go可以很方便的连接到apiserver并监听资源状态的变化。向下屏蔽了很
通过上面的介绍，我相信你已经对k8s的整体架构有了一个更清晰的认识。今天内容就说这
●log-controller
●Ib-controller
为了k8s的开发者和用户更加方便扩展k8s的功能，
大提高了生产效率。
对Pod资源的自发现。动态地感知到Pod的增删，从而实现自动化地去配置logstash的功能，大
去处理的话，运维人员会疯掉的。为了解决这个问题，我们基于k8s资源注册与发现框架实现了
与配置是可以接受的，但是对于k8s中Pod这类生命周期短，频繁变更的资源而言，再通过人工
logstash的配置是人工参与处理的。对于虚拟机和物理机这种不会频繁变更的计算资源，人工参
于要收集日志的机器而言，会在上面部署一个loogstash的agent，负责读取日志并上传给Qbus 。
我司基于Kafka、logstash和Qconf实现的一套日志收集系统，已被公司绝大部分业务使用。对
对于Kubernetes中应用的日志，我们是通过公司现有的Qbus日志收集系统收集走的。Qbus是
功能。
vIP 下；对于删Pod，会将该Pod从LVS中的vIP下卸载，从而实现动态感知Pod变化并更新rs的
删事件，根据不同的事件做出不同的操作。对于增Pod，会将该Pod挂载到LVS 中与其关联的
Ib-controller。 Ib-controller 主要是利用k8s资源注册与发现框架，从apiserver中获取Pod增
的四层代理，为了实现Kubernetes 与我司LVS 的对接我们实现了自己的负载均衡控制器
截止到1.7.0版本，Kubernetes 还没有实现对四层代理的支持，而我司大部分使用的是基于LVS
General patern of a Kubernetes controller
00
Cont
oller
CRUD
，k8s提供了资源注册和发现的客户端库的
容器化－我所理解的Kubernetes架构09
海
1111
---
## Page 15
https://opsdev.cn/post/understanding_of_Kubernetes.html 
本文链接：
stash。
是我司基于logstash和kafka有一套完整的日志收集系统（Qbus），我们不想重复造轮子，因此就使用了log-
A：filebeat 是当前容器圈使用比较多的日志收集agent，功能和性能都非常棒。我们之所以选择ogstash 主要
Q：为什么不用filebeat，而是使用logstash?
新内部逻辑，配合lb-controller实现更平滑的升级。
造成到该POD实例的流量出现丢失的情况。
A：原始的k8s滚动更新时还是比较暴力的，在删除旧实例时不管当前POD实例的流量直接给删除，这样，会
Q：为什么要修改RollingUpdate逻辑？如何做到平滑升级的？
2． service 规模大了后维护成本和性能都会是个问题。
也就说源IP会丢失。
1．k8s的service是基于iptables的NAT实现，通过service最终请求到Pod内无法获取请求客户端的真实IP,
A：自己实现lb-controller主要解决几个问题：
Q：你们为什么要自实现lb-controller，Kubernetes 的 service 满足不了需求吗？
面对面：
为了能确保滚动更新时新启动的实例正常提供服务且挂载到LVS后再去摘掉旧的实例，就需要深入滚动更
10我所理解的 Kubernetes 架构－容器化
一扫查看文章详情
口
---
## Page 16
手工制作Docker镜像时，大概这几步：
Docker的镜像，已然成为容器镜像的事实标准，我们的容器服务也是基于Docker构建的。
纯手工捣鼓Docker镜像
术实现，Jenkins算是“引引擎”，本文简要介绍这其中的技术方案。
用户提供UI页面化的一整套的镜像定制、制作、管理、私有镜像仓库的服务，这套服务的背后技
背景
IHULK容器服务的镜像CI解决方案
巧妇难为无米之炊，玩容器，
I Aug. 8th 2017 BY 王浩宇
3、写一个Dockerfile。
ADD指令统一添加到镜像中。
2、可以创建一个子目录，存放要添加到镜像中的配置文件，并组织好目录层次，最后用
1、创建制作镜像的工作目录。
rootfs/
-usr
rootfs
local
-nginx
“镜像”就是下锅的米，我们私有云HuIk平台的容器服务，向
-nginx.conf
-include
-fastcgi.conf
mime.types
-xxx.conf
容器化－Hulk容器服务的镜像Cl解决方案
11
---
## Page 17
在后端的技术实现方面，我们采用下面的架构：
在面向用户的功能方面，要解决好下面几个主要问题：
针对上面提到的效率、使用门槛的问题，简要介绍下我们的解决方案。
UI页面化、自动化地生产Docker镜像
多以后，
12
这种纯手工的方式，很明显，由于自动化程度低，工作量较大，尤其是当镜像种类、版本较
3、触发build、push，以及镜像仓库的管理。
5、push到镜像仓库。
4、build镜像。
、镜像内容的管理，主要是一些配置文件，比如上面的rootfs目录；
# docker push r.your.domain/xxx/nginx-19-el6:01
# docker build -t r.your.domain/xxx/nginx-19-el6:01 .
CMD
ENTRYPoINT ["/usr/local/nginx/sbin/nginx"]
EXPOSE
HuIk容器服务的镜像CI解决方案－容器化
Dockerfile的定制、自动生成，比如定制RUN、EXPOSE、ENTRYPOINT、CMD;
，而且对于不了解Docker命令、Dockerfile语法的同学，使用门槛还是比较高哦。
cat dockerfile
80
/usr/local/nginx/conf/nginx.conf", "-g",
nginx-1.9.15-5.el6 && yum clean all
ARROR
GitLab
push
lind
镜像管理
build镜像
内容管理
---
## Page 18
下面是一个示例的Jenkinsfile:
成Dockerfile、build、push、清理。
Jenkinsfile里充分利用了plPeline的语法，把一系列步骤串起来：前期检查、创建tar文件、生
Jenkinsfile里定义逻辑“制作镜像”。
如果用户触发“制作镜像”，会触发一个Jenkins的job，该job从GitLab拉取后，根据一个
Jenkins实现自动化生产线
后台会把这些内容、信息，存储到GitLab。
端口等，比如：
UI页面上支持用户管理自己的配置文件（rootfs）、运行的命令（RUN）、入口程序、暴露的
镜像内容管理、Dockerfile定制生成
文件信息
na-formet
procs.eo
Locelhos
4096;
容器化－Huk容器服务的镜像Cl解决方案13
---
## Page 19
Docker pull拉取镜像、Docker run测试镜像。
制作好的镜像，存储于私有镜像仓库，用户在页面可以方便地管理，也可以在自己测试环境，
镜像仓库管理
14HuIk容器服务的镜像Cl解决方案－容器化
#!groovy
ilding and
ists
chive
post
and specifi
stage('Push') {
timeout(time: 30,unit:
rootfs'
list
steps