protocol, the interviewer focused primarily on the set of
questions given in Appendix B, with the option to ask follow-
ups or skip questions that were already answered [89]. Each
interview was divided along three lines of questioning: general
experience, task analysis, and skill development.
Prior to the main study, we conducted four pilot interviews
(two testers, two hackers) to pre-test the questions and ensure
2Interviews were conducted via video teleconference because it was
geographically infeasible to meet face-to-face.
376
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:31 UTC from IEEE Xplore.  Restrictions apply. 
validity. We iteratively updated our protocol following these
interviews, until we reached the ﬁnal protocol detailed below.
General experience. We began the interviews by asking
participants to expand on their screening-survey responses
regarding vulnerability discovery experience. Speciﬁcally, we
asked their motivation behind doing this type of work (e.g.
altruism, fun, curiosity, money) and why they focus (or do not
focus) on a speciﬁc type of vulnerability or software.
Task analysis. Next, we asked participants what steps they
take to ﬁnd vulnerabilities. Speciﬁcally, we focused on the
following sub-tasks of vulnerability discovery:
• Program selection. How do they decide which pieces of
software to investigate?
• Vulnerability search. What steps are taken to search for
vulnerabilities?
• Reporting. How do they report discovered vulnerabilities?
What information do they include in their reports?
To induce in-depth responses, we had participants perform a
hierarchical task analysis focused on these three sub-tasks.
Hierarchical
task analysis is a process of systematically
identifying a task’s goals and operations and decomposing
them into sub-goals and sub-operations [90]. Each operation is
deﬁned by its goal, the set of inputs which conditionally activate
it, a set of actions, and the feedback or output that determine
when the operation is complete and which follow-on operations
are required. Hierarchical task analysis was developed to
analyze complex, non-repetitive, cognitively loaded tasks to
identify errors or inefﬁciencies in the process. We adopted this
for our study, as it provides a useful framework for eliciting
details from experts who typically perform some parts of tasks
automatically and subconsciously [90].
For each sub-operation identiﬁed, we also asked participants
to discuss any speciﬁc tools they use, what skills are useful
to complete this step, how they learned and developed their
process for completing the necessary actions, and how the
steps they take differ across software and vulnerability types.
Skill development. Finally, we asked participants to describe
how they developed the skills necessary to ﬁnd vulnerabilities.
Here, we focused on their learning process and how they
interact with other members of their respective communities.
During the task analysis portion of the interview, we asked
participants to explain how they learned how to complete certain
tasks. In this segment, we broadened this line of questioning and
asked what development steps they recommend to newcomers
to the ﬁeld. This question was intended to elicit additional
learning sources that may have been missed previously and
highlight steps participants believe are the most important.
Finally, we asked each participant to describe their interac-
tions with other members of their local community and the
vulnerability discovery and software tester community at large.
Speciﬁcally, we discussed who they interact with, the forms
of their interaction (e.g., one-to-one, large groups), how these
interactions are carried out (e.g., conferences, online forums,
direct messaging), and what types of information are discussed.
C. Data analysis
The interviews were analyzed using iterative open coding [91,
pg. 101-122]. When all the interviews were completed, four
members of the research team transcribed 10 interviews. The
remaining 15 interviews were transcribed by an external
transcription service. The interviewer and another researcher
independently coded each interview, building the codebook
incrementally and re-coding previously coded interviews. This
process was repeated until all interviews were coded. The codes
of the two interviewers were then compared to determine inter-
coder reliability using the ReCal2 software package [92]. We
use Krippendorff’s Alpha (α) to measure inter-coder reliability
as it accounts for chance agreements [93].
The α after coding all the interviews was .68. Krippendorff
recommends using α values between .667 and .80 only in
studies “where tentative conclusions are still acceptable” [94];
and other work has suggested a higher minimum threshold of
.70 for exploratory studies [95]. To achieve more conclusive
results, we recoded the 16 of our 85 codes with an α less
than .70. For each code, the coders discussed a subset of the
disagreements, adjusted code deﬁnitions as necessary to clarify
inclusion/exclusion conditions, and re-coded all the interviews
with the updated codebook. After re-coding, the α for the study
was .85. Additionally, all individual codes’ αs were above .70.
Next, we grouped the identiﬁed codes into related categories.
In total,
there were six categories describing the partici-
pants’ discovery process (i.e., Information Gathering, Program
Understanding, Attack Surface, Exploration, Vulnerability
Recognition, and Reporting) and four categories regarding
factors that inﬂuenced participants’ implementation of this
process (i.e., Vulnerability Discovery Experience, Underlying
System Knowledge, Access to the Development Process, and
Motivation). We then performed an axial coding to ﬁnd
connections between categories and between codes within
categories [91, pg. 123-142]. Based on the categories and
connections between them, we derive a theory describing the
process practitioners use to ﬁnd vulnerabilities, the factors
that inﬂuence their implementation of this process, and how
testers and hackers differ with respect to their process and
implementation.
D. Limitations
Our study has several limitations common to exploratory
qualitative research. A lack of complete recall is especially
prominent in studies like ours, where participants are asked
to describe expert tasks [90]. We employ a hierarchical task
analysis in our interview protocol to improve the thoroughness
of information elicited. Participants may have also adjusted
their answers to portray themselves as more or less skilled,
if they were concerned with the interviewer’s perception of
them [96], [97]. Additionally, there could be selection bias
among the testers and hackers studied. Because we explicitly
stated the purpose of the study when recruiting, it is possible
that those with experience or an interest in security were more
likely to respond to our request. Also, since some hackers
tend to be more privacy sensitive, some may have decided not
377
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:31 UTC from IEEE Xplore.  Restrictions apply. 
to participate in order to protect their identity or intellectual
property. To partially mitigate these issues, we recruited through
a wide variety of sources and interviewed a diverse pool of
participants to increase the likelihood that relevant ideas would
be stated by at least one participant. Finally, for each ﬁnding,
we give the number of testers and hackers that expressed a
concept, to indicate prevalence. However, if a participant did
not mention a speciﬁc idea, that does not necessarily indicate
disagreement; they may have simply failed to state it. For
these reasons, we do not use statistical hypothesis tests for
comparison among participants. Our results do not necessarily
generalize beyond our sample; however, they suggest many
directions for future work and provide novel insights into the
human factors of software vulnerability discovery.
IV. PARTICIPANTS
We received 49 responses to our screening survey. We
selected 10 testers and 15 hackers. (Themes related to their
vulnerability discovery process converged more quickly with
testers than with hackers, so we required fewer interviews [79]).
Table I shows our participants’ demographics, including their
self-reported vulnerability-discovery skill level (on a scale
from 0-5, with 0 indicating no skill and 5 indicating an expert),
self-reported number of vulnerabilities they have discovered,
company size (only applicable for testers), and the method
used to recruit them.
Hacker demographics match prior surveys. Our study de-
mographics are relatively congruent with hacker demographics
reported in studies from the popular bug bounty services
HackerOne [13] and BugCrowd [98]. 90% of HackerOne’s
70,000 users were younger than 34; 60% of BugCrowd’s 38,000
users are 18-29 and 34% are 30-44 years old. Our hacker
population was 60% under 30 and 90% under 40 years old.
Regarding education, 84% of BugCrowd hackers have attended
college and 21% have a graduate degree; 93% of our hackers
have attended college and 33% have a graduate degree.
Testers are more diverse than hackers. In contrast to our
hacker population, none of our software testers were under 30
and only 60% were under 40 years old. All of our testers have
some college education, but only one has a graduate degree.
With respect to ethnicity and gender, the software tester group
was much more diverse, at 60% male and 60% Caucasian.
Hackers reported higher vulnerability discovery skills. As
expected, there is a contrast in vulnerability ﬁnding skills
between testers and hackers. The hacker population self-
reported an average skill level of 3.5, whereas software testers
self-reported an average skill of 2.5. This self-reported measure
cannot be used to directly compare participants’ abilities;
instead, it indicates their self-efﬁcacy, telling us that testers
tend to be less conﬁdent in their ability to ﬁnd security bugs.
Interestingly, despite the hacker population possessing more
vulnerability ﬁnding experience, more software testers self-
reported having discovered more than 500 vulnerabilities.
However, we note that the number of vulnerabilities is not
ID1,2
Vulns.
Fnd
Skill
1
2
3
5
4
3
4
0
0
3
4
4
5
4
4
3
3
5
4
2
4
1
4
4
2
Educ.
B.S
B.S.
B.S
SC
B.S.
B.S.
B.S.
Assoc.
B.S.
M.S.
B.S.
B.S.
M.S.
M.S.
B.S.
M.S.
M.S.
SC
H.S.
SC
B.S.
B.S.
B.S.
M.S.
B.S.
Gender:
Source4
Age:Race3
M:30-39:H
T1W
O
F:40-49:W
T2W
O
F:30-39:W
T3W
O
M:30-39:A
T4G
O
M:30-39:W
T5W
O
M:50-59:A
T6W
O
M:30-39:A
T7G
O
F:50-59:W
T8H
O
T9W
F:30-39:W
C
T10W M:40-49:W
O
M:18-29:W
H1H
O
M:30-39:W
H2H
O
M:30-39:W
H3G
O
F:18-29:W
H4H
O
M:18-29:W
H5M
O
H6G
M:18-29:H
O
H7W M:18-29:W
O
M:30-39:W
H8M
C
H9G
M:18-29:W
O
H10H M:18-29:W
O
H11W M:18-29:W
O
H12W M:40-49:B
O
H13W M:30-39:W
C
H14W M:30-39:W
P
H15W M:18-29:W
P
1 IDs are coded by population (T: Tester, H: Hacker) in date order
2 Software Specialization – W: Web, H: Host, M: Mobile, G: General
3 W: White, B: Black, A: Asian, H: Hispanic
4 Recruitment method – O: Related Organization, P: Public Bug Bounty
Data, C: Personal Contact
Org.
Sz
>20K
100
150