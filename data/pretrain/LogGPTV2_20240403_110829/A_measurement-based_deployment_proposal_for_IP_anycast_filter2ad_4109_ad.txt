The servers at Cornell and Cambridge have similar failure
mode behavior with a median failover time of ≈35 seconds
and a 95th percentile of ≈120 seconds. The peaks around 30
and 60 seconds in these curves can be attributed to the BGP
MinRouteAdverTimer which governs the time between up-
dates for the same preﬁx to the BGP same peer. The value
of this timer defaults to 30 seconds on many routers [25].
On the other hand, the servers at Berkeley, Pittsburgh and
Seattle have a median failover time ranging from 7 to 12
seconds with a 95th percentile of 14 to 18 seconds. The
trends in the recovery times are less obvious and we don’t
completely understand these results.
In case of the server at Cornell, there is no other anycast
server with WCG as an upstream provider. Consequently,
the failure of this server causes clients using the server to
be re-routed to servers with other upstream providers. This
also implies that the BGP updates resulting from the server
failure need to be propagated beyond WCG’s network. As
a result, the failover process involves a number of ASs, is
impacted by the various BGP timers and overall, is aﬀected
by delayed routing convergence resulting in the large failover
time. A similar explanation applies to the failure of the
server at Cambridge.
On the other hand, the other three servers at Berkeley,
Pittsburgh and Seattle oﬀer much faster failover. This is an
outcome of the fact that these servers have the same up-
stream provider (ATT). Thus, when one of the servers fails,
clients accessing that server are routed to the one of the
other two servers. To verify this, we determined the frac-
tion of clients that go to other operational servers when a
particular server fails – this is plotted in ﬁgure 9. As can
be seen, in case of a failure at Berkeley, Seattle and Pitts-
burgh, most of the clients are re-routed to one of the other
two operational servers from the same group. This implies
that when one of these servers fails, the BGP convergence
process is restricted mostly to ATT’s network resulting in
faster failover.
The distribution of clients when a server fails (ﬁgure 9)
can also be used to explain some of the trends in ﬁgure 8(a).
For example, almost all the clients accessing the Berkeley
server when it fails are routed to the server at Seattle. As
a result, the failover time for the Berkeley server is almost
the same for all clients accessing it. On the other hand, a
small fraction of clients accessing the Seattle server when
it fails are routed to the server at Cornell.7 This explains
the inﬂection point in the failover time for the Seattle server
showing that a small fraction of the clients take much more
time to failover than the rest.
These results suggest that in an IP Anycast deployment
with a number of anycast servers per upstream provider (as
proposed in the previous section), the failure of an any-
cast server would cause clients to be routed to one of the
other servers with the same upstream provider. Hence, the
proposed deployment model is conducive to fast failover
with a large majority of clients being rerouted to another
server within 20 seconds. Reports that many commercial
DNS-based anycast deployments aim for similar sub-minute
failover times (by using a TTL values between 10-20 seconds
[22,33]) lead us to conclude the failover rate of a planned IP
Anycast deployment should suﬃce for almost all anycast
applications.
On a broader note, our study shows that while results
from previous studies reporting slow BGP convergence [23]
do apply to IP Anycast deployments in general, an IP Any-
cast deployment can be planned so as to decouple anycast
failover from delayed routing convergence.
In eﬀect, this
addresses the long held belief that IP Anycast is bound to
provide very slow failover, for example, the possibility of
server failures causing outages of ﬁve or more minutes [42].
As a matter of fact, the clustered deployment model (as de-
scribed in section 3) used by most commercial IP Anycast
deployments was primarily motivated by the need to decou-
ple host failure from BGP events. While we agree that us-
ing clustered hosts at each anycast server site is a necessary
part of the IP Anycast deployment picture, an IP Anycast
deployment conforming to our deployment proposal ensures
that even when an entire server site fails, clients do not have
to wait an inordinate amount of time for failover.
7The failure of the Seattle server probably causes ATT to
modify the MEDs on the anycast preﬁx advertisements it
propagates to its peers, some of whom then choose to route
anycast packets to other servers that don’t have ATT as
an upstream provider. This explains why some clients are
routed to Cornell when the server at Seattle fails.
d
d
e
e
t
t
u
u
o
o
r
r
s
s
t
t
n
n
e
e
i
i
l
l
r
r
e
e
v
v
r
r
e
e
s
s
t
t
n
n
e
e
r
r
e
e
f
f
f
f
i
i
d
d
a
a
o
o
t
t
c
c
f
f
o
o
n
n
o
o
i
i
t
t
c
c
a
a
r
r
F
F
 1
 1
 0.9
 0.9
 0.8
 0.8
 0.7
 0.7
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
Cor.
Cor.
Pit.
Pit.
Sea.
Sea.
Ber.
Ber.
Cam.
Cam.
Cornell
Cornell
Pittsburgh
Pittsburgh
Seattle
Seattle
Berkeley
Berkeley
Cambridge
Cambridge
Failed Server
Failed Server
Figure 9: As a server fails, clients that were being
routed to the server are now routed to other op-
erational servers. The Y-axis shows the fraction of
clients that failover to each other operational server
when the particular server on the X-axis fails.
7. AFFINITY
The fact that IP Anycast is a network layer service implies
that two consecutive anycast packets from a single client
need not be routed to the same server. For example, in case
of the internal deployment, it is possible that a client whose
anycast packets were being routed to the server at Cornell
suddenly gets routed to the server at Cambridge. Such an
occurrence, hereon referred to as a ﬂap, would break any
higher-layer connections (for example, TCP connections)
that may exist between the client and the anycast service.
Hence, determining the aﬃnity oﬀered by IP Anycast is im-
portant for characterizing its the impact on stateful services
being run on top.
As mentioned earlier, past studies have painted a con-
tradictory picture of IP Anycast aﬃnity. A study using a
few (<200) PlanetLab nodes as vantage points [4] claimed
that ﬂaps are relatively rare; for example, they reported a
median inter-ﬂap duration when probing the F root-servers
to be more than 10 days. Similarly, operators of the any-
casted K root-server [12] found that their IP Anycast deploy-
ment oﬀers very good aﬃnity. On the other hand, a study
based on a few (<200) volunteer and PlanetLab nodes [8,31]
and anecdotal evidence from the anycasted J root-server [6]
support claims to the contrary. For example, Boothe et.
al. [8] found the median inter-ﬂap duration to be 1.4 hours
for PlanetLab nodes and 3 hours for their volunteer nodes.
As a matter of fact, IP Anycast aﬃnity and its suitability
for stateful services has been passionately debated on many
mailing lists; a summary of some these discussions can be
found at [43]. However, none of the aforementioned studies
have attempted to delve into the reasons behind the rout-
ing ﬂaps (few or many) observed by them. In this section,
we present a detailed analysis of the aﬃnity oﬀered by the
internal anycast deployment as determined through active
probing from our clients.
Methodology: The TXT-record based querying describ-
ed in section 6 allows us to determine the particular anycast
server that anycast packets from a given client are routed
to. Using this, we can periodically query a client in order
to capture the anycast ﬂaps experienced by it. For these
experiments, we randomly chose 5200 clients from our list of
clients and determined the number of ﬂaps they experience
by querying them at a rate of once per minute for a period
of 17 days.
F
F
D
D
C
C
 1
 1
 0.8
 0.8
 0.6
 0.6
 0.4
 0.4
 0.2
 0.2
 0
 0
0
0
r
r
e
e
v
v
r
r
e
e
S
S
t
t
s
s
a
a
c
c
y
y
n
n
A
A
Ber
Ber
Pitt
Pitt
Sea
Sea
Cam
Cam
Cor
Cor
1
1
10
10
# of Flaps
# of Flaps
100
100
1000
1000
 0
 0
 20
 20
 40
 40
 60
 60
 80
 80
 100
 100
 120
 120
Time (seconds)
Time (seconds)
Figure 10: Aﬃnity measurements for our anycast
deployment – the measurements involve 5277 name-
servers as vantage points and span a period of 17
days.
i
i
)
)
s
s
s
s
e
e
h
h
t
t
n
n
e
e
r
r
a
a
p
p
n
n
i
i
r
r
e
e
t
t
s
s
u
u
c
c
l
l
e
e
h
h
n
n
i
i
s
s
p
p
a
a