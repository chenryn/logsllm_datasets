### Server Failure Behavior and Failover Times

The servers at Cornell and Cambridge exhibit similar failure mode behavior, with a median failover time of approximately 35 seconds and a 95th percentile of around 120 seconds. The peaks observed at 30 and 60 seconds in the failover time curves can be attributed to the BGP MinRouteAdverTimer, which controls the interval between updates for the same prefix to the same BGP peer. This timer is set to a default value of 30 seconds on many routers [25].

In contrast, the servers at Berkeley, Pittsburgh, and Seattle have significantly faster failover times, with a median ranging from 7 to 12 seconds and a 95th percentile of 14 to 18 seconds. The trends in recovery times for these servers are less clear, and further investigation is needed to fully understand the results.

### Analysis of Failover Times

#### Cornell and Cambridge Servers
For the server at Cornell, there is no other anycast server with WCG as an upstream provider. Consequently, if this server fails, clients must be re-routed to servers with different upstream providers. This process involves propagating BGP updates beyond WCG’s network, affecting multiple ASs and being influenced by various BGP timers. The delayed routing convergence resulting from this complex process leads to a longer failover time. A similar explanation applies to the server at Cambridge.

#### Berkeley, Pittsburgh, and Seattle Servers
The servers at Berkeley, Pittsburgh, and Seattle, all of which share the same upstream provider (ATT), offer much faster failover times. When one of these servers fails, clients are quickly re-routed to one of the other two operational servers within the same group. To verify this, we measured the fraction of clients that switch to other operational servers during a failure, as shown in Figure 9. The results indicate that most clients are re-routed to one of the other two servers, confining the BGP convergence process primarily to ATT’s network, thus achieving faster failover.

### Client Distribution During Failures

The distribution of clients when a server fails, as depicted in Figure 9, also helps explain some of the trends observed in Figure 8(a). For example, almost all clients accessing the Berkeley server are re-routed to the Seattle server upon failure, leading to consistent failover times for all clients. Conversely, a small fraction of clients accessing the Seattle server are re-routed to the Cornell server, explaining the inflection point in the failover time for the Seattle server, where a small number of clients experience much longer failover times.

### Implications for IP Anycast Deployments

These findings suggest that in an IP Anycast deployment with multiple anycast servers per upstream provider, the failure of one server will typically cause clients to be re-routed to another server within the same upstream provider. This model supports fast failover, with the majority of clients being re-routed within 20 seconds. Commercial DNS-based anycast deployments often aim for sub-minute failover times by using TTL values between 10-20 seconds [22,33], indicating that the proposed deployment model should be sufficient for most anycast applications.

### Broader Implications

Our study shows that while previous research has reported slow BGP convergence [23], IP Anycast deployments can be designed to decouple anycast failover from delayed routing convergence. This addresses the long-held belief that IP Anycast inherently provides very slow failover, such as server failures causing outages of five or more minutes [42]. The clustered deployment model, commonly used in commercial IP Anycast deployments, was developed to separate host failures from BGP events. While clustering is necessary, our proposed deployment ensures that even in the event of an entire server site failure, clients do not experience excessively long failover times.

### Affinity in IP Anycast

IP Anycast, being a network layer service, does not guarantee that consecutive packets from a single client will be routed to the same server. For instance, a client's anycast packets might suddenly switch from the Cornell server to the Cambridge server, potentially disrupting higher-layer connections like TCP. Understanding the affinity provided by IP Anycast is crucial for assessing its impact on stateful services.

Previous studies have presented conflicting views on IP Anycast affinity. Some studies, using a limited number of PlanetLab nodes, reported rare flaps, with median inter-flap durations exceeding 10 days [4, 12]. Other studies, based on volunteer and PlanetLab nodes, found more frequent flaps, with median inter-flap durations of 1.4 to 3 hours [8, 31, 6]. These discrepancies have been extensively debated in mailing lists [43], but none of the studies have thoroughly investigated the causes of the observed flaps.

### Methodology for Measuring Affinity

To measure the affinity provided by our internal anycast deployment, we used the TXT-record based querying method described in Section 6. This method allows us to determine the specific anycast server to which a client's packets are routed. We periodically queried 5200 randomly selected clients at a rate of once per minute over 17 days to capture the flaps they experienced.

### Results of Affinity Measurements

Figure 10 presents the affinity measurements for our anycast deployment, involving 5277 name-servers as vantage points over a 17-day period. The data provide insights into the frequency and duration of flaps, helping to characterize the stability of the anycast service for stateful applications.