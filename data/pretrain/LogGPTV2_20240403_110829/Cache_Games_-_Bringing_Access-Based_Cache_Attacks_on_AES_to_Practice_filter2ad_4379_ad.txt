s
t4
⊕
k4
1
⊕
k3
3
10
⊕
k5
0
⊕
k4
2
⊕
≫
s
t5
⊕
k5
1
⊕
k4
3
20
⊕
k6
0
⊕
k5
2
⊕
≫
s
t6
⊕
k6
1
⊕
k5
3
40
⊕
k7
0
⊕
k6
2
⊕
≫
s
t7
⊕
k7
1
⊕
k6
3
80
⊕
k8
0
⊕
k7
2
⊕
≫
s
t8
⊕
k8
1
⊕
k7
3
1b
⊕
k9
0
⊕
k8
2
⊕
t9
⊕
k9
1
⊕
k8
3
k10
0
⊕
k9
2
⊕
k10
1
⊕
k9
3
k10
2
⊕
k10
3
Figure 2. Key schedule of the 128 bit variant of AES. By kn
any further semantics. The bottommost elements, i.e., k0
the key schedule are ﬁxed in order to ﬁnd a full key schedule with maximal score.
m we denote the mth column of the nth round key. The tn are temporary variables without
i , are passed as inputs to the nonlinearity at the top. During the key search, incrementally parts of
A. CFS – The Completely Fair Scheduler
Scheduling is a central concept in multitasking OS where
CPU time has to be multiplexed between processes, creating
the illusion of parallel execution. In this context there are
three different states a process can possibly be in (we do not
need to distinguish between processes and threads for now):
• A running process is currently assigned to a CPU and
uses that CPU to execute instructions.
• A ready process is able to run, but temporarily stopped.
• A blocked process is unable to run until some external
event happens.
The scheduler decides when to preempt a process (i.e., set
it from running to ready) and which process to activate
next when a CPU becomes idle (i.e., set it from ready to
running). This is a difﬁcult problem, because of the multiple,
conﬂicting goals of the scheduler:
• guaranteeing fairness according to a given policy,
• maximizing throughput of work that is performed by
processes (i.e., avoid wasting time on overhead like
context switching and scheduling decisions) and
• minimizing latency of responses to external events.
Starting from Linux kernel 2.6.23, all Linux systems are
equipped with the Completely Fair Scheduler (or CFS) [34],
whose general principle of operation we describe in the
following. Its central design idea is to asymptotically behave
like an ideal system where n processes are running truly
in parallel on n CPUs clocked at 1/nth of normal speed
each. To achieve this on a real system, the CFS introduces
a virtual runtime τi for every process i. In the ideal system,
all virtual runtimes would increase simultaneously and stay
equal when the processes were started at the same time and
never block. In a real system, this is clearly impossible, as
only the running process’s virtual runtime can increase at
a time. Therefore CFS keeps a timeline (an ordered queue)
of virtual runtimes for processes that are not blocked. Let
497
Figure 3.
Functioning of the Completely Fair Scheduler. Here, three
process are running concurrently. After process 1 was the assigned the CPU
for some time, process 2 is the next to be activated to keep the unfairness
among the different processes smaller than some threshold.
the difference between the rightmost and leftmost entries be
∆τ = τright − τleft. This difference of the virtual runtimes
of the most and least favorably scheduled processes can be
interpreted as unfairness, which stays always zero in an ideal
system. CFS lives up to its name by bounding this value
above by some ∆τmax. It always selects the leftmost process
to be activated next and preempts the running rightmost
process when further execution would result in ∆τ ≥ ∆τmax.
This logic is illustrated in Figure 3 where three processes
are running on a multitasking system. At the beginning,
process 1 is the next to be activated because it has the least
virtual runtime. By running process 1 for some time the
unfairness is allowed to increase up to ∆τmax. Then CFS
switches to process 2, which became the leftmost entry on
the timeline in the meantime. This procedure is repeated
inﬁnitely so that every process asymptotically receives its
fair share of 1/nth CPU computing power per time.
A very important question is how to compute the virtual
time of a processes which blocked at τblock when it
is
unblocked again. We denote this computed virtual runtime
by τunblock. Following a concept called sleeper fairness it is
desirable that the process is activated as soon as possible and
given enough time to react to the event it was waiting for
virtual runtimetimeprocess 1process 2process 3maximum“unfairness” reachedIn the time where no spy thread is active the kernel ﬁrst
activates the victim process (or some other process running
concurrently on the system). This process is allowed to run
until the timer unblocking thread i + 1 expires. Because
of the large number of threads and the order they run,
their virtual runtimes will only increase very slowly. Thus,
upon unblocking a spy thread is the leftmost element in
the timeline of CFS and the currently running process is
immediately preempted. This mechanism ensures that S
immediately regains control of the CPU after V ran.
Typically, twakeup − tsleep is set to about 1500 machine
cycles. Subtracting time spent executing kernel code and for
context switching, this leaves less than 200 cycles for the
CPU to start fetching instructions from V , decode and issue
them to the execution units and ﬁnally retire them to the
architecturally visible state, which is saved when the timer
interrupts. When V performs memory accesses which result
in cache misses, these few hundreds cycles are just enough
to let one memory access retire at a time, on average.
Because of different timers used within the system, accu-
rately setting tsleep and twakeup is a challenging issue. In a
ﬁrst step, we have to ﬁnd out the precise relation between
the time stamp counter (in machine cycles), and the wall
time of the OS (in nanoseconds as deﬁned by the POSIX
timer API). This can be achieved by repeatedly measuring
the CPU time using the rdtsc instruction and the OS time,
and interpolating among these values. This approximation
only has to be performed once for every hardware setting.
For our test environment, we got 0.6672366819 ns per CPU
cycle. When starting our spy process the offset of the time
stamp counter to the OS time is measured, which enables us
to convert time measured by rdtsc to OS time with very
high accuracy.
Since newer Linux versions change the CPU clock to save
power when the idle thread runs, a dummy process with
very low priority is launched to prevent the idle thread from
changing the linear relationship between OS time and time
stamp counter.
But even with exact computations of twakeup and tsleep there
are still other sources of inaccuracy. First, the time spent
in the OS kernel stays constant for many measurements,
but sometimes abruptly changes by hundreds of machine
cycles. This is dynamically compensated by a feedback
loop that adjusts twakeup − tsleep according to the rate of
observed memory accesses. Second, the clock and timer
devices do not actually operate with nanosecond accuracy
as suggested by their APIs. As a result the actual time
when the timer expires lies in an interval of about ±100
machine cycles around twakeup for our hardware setting. In
theory this could also be compensated with a more complex
computational model of the hardware. However, assuming a
linear relationship between the time stamp counter and OS
time is sufﬁcient for our purposes.
To hide the spy process from the user twakeup − tsleep is
Figure 4.
Sequence diagram of our denial of service attack on CFS.
Multiple threads run alternatingly and only leave very small periods of
time to the victim process.
with low latency. In CFS terms this means assigning it the
lowest possible virtual runtime while not violating CFS’s
invariants: to not exceed the maximum unfairness it must
hold that τunblock < τright − ∆τmax. Also, the virtual runtime
must not decrease by blocking and unblocking to prevent a
trivial subversion of CFS’s strategy. Therefore τblock needs to
be remembered and serves as another lower bound. Finally,
we get
τunblock = max(τblock, τright − ∆τmax).
By blocking for a sufﬁciently long time, a process can ensure
that it will be the leftmost entry on the timeline with τleft =
τright − ∆τmax and preempt the running process immediately.
B. A Denial of Service Attack on CFS
On a high level, our spy process S measures the memory
accesses of the victim process V as follows: It requests
most of the available CPU time, and only leaves very
small
intervals to V . By choosing the parameters of S
appropriately, V will only be able to advance by one memory
access on average before it is preempted again. Then, S
accesses each entry of the lookup table, and checks whether
a cache hit, or a cache miss occurs. Next V is again allowed
to run for “a few” CPU cycles, and V measures again, etc.
In this section, we describe how the sleeper fairness of
CFS can be exploited for the denial of service (DoS) attack
underlying our spy process. The procedure for measuring
cache accesses can be found in §IV-C.
When getting started, our spy process launches some hun-
dred identical threads, which initialize their virtual runtime
to be as low as possible by blocking for a sufﬁciently long
time. Then they perform the following steps in a round-robin
fashion, which are also illustrated in Figure 4:
• Upon getting activated, thread i ﬁrst measures which
memory accesses were performed by V since the
previous measurement.
• It then computes tsleep and twakeup, which denote the
points in time when thread i should block and thread
i + 1 should unblock. It programs a timer to unblock
thread i + 1 at twakeup.
• Finally, thread i enters a busy wait loop until tsleep is
reached, where it blocks to voluntarily release the CPU.
498
TimeThread iThread i+1Victim processOS kerneltwakeupmeasure accessesprogram timerbusy waitmeasure accessestsleep#define CACHELINESIZE 64
#define THRESHOLD 200
unsigned measureflush(const uint8_t *table,
size_t tablesize,
uint8_t *bitmap)
{
}
size_t i;
uint32_t t1, t2;
unsigned n_hits = 0;
for (i = 0; i < tablesize/CACHELINESIZE; i++)
{
__asm__ (
"xor %%eax, %%eax
"cpuid
"rdtsc
"mov %%eax, %%edi
"mov (%%esi), %%ebx
"xor %%eax, %%eax
"cpuid
"rdtsc
"clflush (%%esi)
: /* output operands */
"=a"(t2), "=D"(t1)
\n"
\n"
\n"
\n"
\n"
\n"
\n"
\n"
\n"
: /* input operands */
"S"(table + CACHELINESIZE * i)
: /* clobber description */
"ebx", "ecx", "edx", "cc"
);
if (t2 - t1 < THRESHOLD) {
n_hits++;
bitmap[i/8] |= 1 << (i%8);
} else {
bitmap[i/8] &= ˜(1 << (i%8));
}
}
return n_hits;
Listing 5. Complete C source code for checking which parts of a lookup
table table have been accessed by some process shortly before.
dynamically increased if no memory accesses are detected
for an empirically set number of measurements. This allows
the system to react to the actions of an interactive user with
sufﬁcient speed while no victim process is running.
Remark: Note that in spirit our DoS attack is similar
to that of Tsafrir et al. [35]. However, while their attack
is still suited for the current BSD family, it does not work
any more for the last versions of the Linux kernel. This is
because the logics of billing the CPU time of a process has
advanced to a much higher granularity (from ms to ns) and
no process can be activated without being billed by CFS any
more, which was a central corner stone of their attack.
C. Testing for Cache Accesses
In the foregoing we described how the fairness condition
of the CFS can be exploited to let the victim process advance
by only one table lookup on average. We next show how the
spy process can learn information about this lookup. That is,
we show how the spy process can ﬁnd the memory location
the victim process indexed into, up to cache line granularity.
An implementation of this procedure in C is given in
Listing 5, which we now discuss in detail. On a high level, it
measures the time needed for each memory access into the
lookup table and infers whether or not this data had already
been in the cache before.
We start by describing the inner block of the for loop.
The __asm__ keyword starts a block of inline assembly,