 1
 0.8
 0.6
 0.4
 0.22
F
D
C
 0
 0
 5
Unverified loss bursts
Verified loss bursts
 15
 10
 20
Duration (seconds)
 25
 30
 0
 0
 5
Loop-free routing failures
Forwarding loops
 15
 10
 20
Duration (seconds)
 25
 30
(a) Loss burst veriﬁed vs.unveriﬁed
as caused by routing failures
(b) Forwarding loops vs.
loop-free routing failures
Figure 5: Duration for loss bursts.
r
e
b
m
u
n
e
c
n
e
u
q
e
s
t
e
k
c
a
P
450
400
350
300
250
200
150
100
50
0
Time 04:00:01
Time 04:00:19
250
300
0
50
100
150
200
packet arrival order
6:
loss
Figure
by
“planet02.csc.ncsu.edu” during 04:00:00-04:00:30 on July
30, 2005.
experienced
bursts
Long
ures last longer than those unveriﬁed loss bursts. Figure 5(b) further
shows that loss bursts caused by forwarding loops last longer than
those caused by loop-free routing failures.
Internet
RR5
RR4
RR6
PlanetLab host
ISP3
4.3 How Routing Failures Occur
We use an example to illustrate the cause for packet loss during
failover events. The PlanetLab host planet02.csc.ncsu.edu experi-
ences packet loss during the routing failure occurred at 04:00:00
on July 30, 2005, where the host switches from the preferred path
via ISP 1 to the less preferred path via ISP 2 after the withdrawal
of the path via ISP 1. Figure 6 shows the UDP packets received
during the period from 04:00:00 to 04:00:30. The x-axis is the ar-
rival order of each packet, and y-axis is the sequence number of the
packet. We observe two major gaps (i.e., loss bursts) at 04:00:01
and 04:00:19. This can be explained by using the topology shown
in Figure 7. The probing host planet02.csc.ncsu.edu is a customer
of ISP 3 which peers with both ISP 1 and ISP 2. Before the
failover event, all routers except e2 have only one route via e1 to the
Beacon. During the failover-1 event, the route via ISP 1 is with-
drawn by the Beacon. Routers e1 and RR1 within ISP 1 will lose
the route and explore the alternate path from router e2. During the
path exploration, router RR1 cannot reach the Beacon indicated by
the “Destination Host Unreachable” ping replies at time 04:00:01.
After they obtain the path via peer ISP 2, data trafﬁc from the host
to the Beacon will be forwarded via the peer link between ISP 1
and ISP 2. However, router RR2 in ISP 1 cannot announce it
to router RR4 in ISP 3 because of the “no-valley” routing policy.
So router RR2 will send a withdrawal to router RR4. As a re-
sult, router RR4 loses its route to the Beacon and is triggered to
explore the alternate path. This is indicated by the “Destination
Host Unreachable” ping message at time 04:00:19. In summary,
the two long loss bursts shown in Figure 6 can be correlated with
two unreachable ICMP responses, which indicate that these long
loss bursts are caused by routing failures.
In the above example, during the round of path exploration, a
router losing its routing entry is affected by the delay in obtaining
the alternate route. If the router can obtain the alternate route with-
out delay, the routing failure is not visible as packet loss bursts in
the data plane. The latency in obtaining the alternate route is deter-
mined by the MRAI timer and the distance from the router that can
provide the alternate route.
Based on BGP updates collected from these 12 backbone routers
within ISP 2, we identify the MRAI timer applied by ISP 2. We
observe many instances where there is little time difference be-
tween two consecutive announcements for the same preﬁx but with
different BGP attributes. This observation implies that routers within
ISP 2 use a very small MRAI timer. This is veriﬁed by private
communication with network operators of ISP 2. The observa-
tion explains the fact that majority of large loss bursts do not occur
RR2
RR2
RR3
RR1
e2
RR3
RR1
ISP1
e1
ISP2
provider−to−customer
peer−to−peer
Beacon
Figure 7:
routers
“planet02.csc.ncsu.edu” to the Beacon.
Topology
of
on the path from
within ISP 2. On the other hand, most routers within ISP 1 are
Cisco routers, which have default 5 second MRAI timer. Thus,
we suspect ISP 1 does not use small MRAI timer so that routers
within that ISP can experience routing failures. Our analysis here
conﬁrms the importance of MRAI timer setting on routing dynam-
ics and subsequent impact on data plane performance.
4.4 Multiple Loss Bursts Caused by Failover
Events
As we have observed in the above example, a host can experi-
ence multiple loss bursts after the injection of a withdrawal mes-
sage. This can be explained by the widely used “no-valley” routing
policy. That is, when an AS obtains an alternate route from its peer,
it cannot transit the route to another peer. So it will send a with-
drawal message to the peer to invalidate the previous route. Thus,
the withdrawal message can trigger the second loss burst within the
peer.
In this section, we measure the number of loss bursts that a host
experiences for each failover event. Figure 8 shows that, in over
75% of the cases, a hosts experiences fewer than two loss bursts
as a result of a failover event, while a host can experience up to 6
loss bursts as a result of a single failover event. Figure 9 shows the
percentage of packet loss in the ﬁrst two loss bursts experienced
by PlanetLab hosts for each failover event. We observe that the
ﬁrst two bursts contribute to the majority of packet loss. In the rest
of the paper, we will focus on the ﬁrst two loss bursts for failover
events.
Among the ﬁrst two loss bursts during failover-1 events, we can
verify about 57% of the ﬁrst loss bursts as caused by routing fail-
ures, and about 40% of the second loss bursts as caused by routing
failures. The ﬁgures for the failover-2 events are 61% and 42%, re-
spectively. In general, we observe that the number of the ﬁrst loss
bursts that can be veriﬁed as due to routing failures is larger than
that of the second loss bursts.
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
1
2
3
4
5
6
Number of loss bursts
Figure 8: Multiple loss bursts may be experienced by end hosts.
s
s
o
l
t
e
k
c
a
p
f
o
e
g
a
t
n
e
c
r
e
P
 100
 80
 60
 40
 20
 0
 0
 5  10  15  20  25  30  35  40  45
Failover events
the second loss burst
the first loss burst
(a) Failover-1
s
s
o
l
t
e
k
c
a
p
f
o
e
g
a
t
n
e
c
r
e
P
 100
 80
 60
 40
 20
 0
 0
 5  10  15  20  25  30  35  40  45
Failover events
the second loss burst
the first loss burst
(b) Failover-2
Figure 9: Percentage of packet loss contributed by the ﬁrst two
loss bursts.
One interesting observation is that when the second routing fail-
ures are triggered due to the “no valley” routing policy, trafﬁc on
data plane may traverse a path which violates the “no valley” rout-
ing policy. For example, in the previous example shown in Figure 7,
we ﬁnd that between the ﬁrst large loss burst (at 04:00:01) and the
second large loss burst (at 04:00:19), about 250 UDP packets are re-
ceived by the Beacon. During this period of time, UDP packets tra-
verse a path across three tier-1 ASes, “ISP 3 ISP 1 ISP 2”, which
violates the “no-valley” routing policy. After the second loss burst,
the IP-level path will change to traverse two tier-1 ASes, “ISP 3
ISP 2”. The number of packets that traverse this “valley” path is
determined by the delay of sending the withdrawal, which could be
as long as 30 seconds (the default MRAI timer applied on eBGP
session) if the MRAI timer is applied to withdrawal messages.
4.5 Location of Routing Failures
After identifying routing failures causing packet loss during failover
events, we further investigate a number of factors (e.g., interior net-
work topologies, iBGP conﬁgurations, or MRAI timers) that can
cause routing failures by analyzing the location of routing failures.
From the source address of each ICMP message, we can identify
which router loses its route entry if the ICMP message is unreach-
able, or which router is trapped in a forwarding loop if the ICMP
message is an exceeded TTL or a forwarding loop is observed in
traceroute data. We then derive the locations where routing fail-
ures occur according to the DNS name of the corresponding IP ad-
dresses. During each failover event, among all the ﬁrst loss bursts
caused by routing failures, we measure how many of them occur
within ISP 1, ISP 2, or other ASes. As shown in Table 3, 92% of
the ﬁrst routing failures occur within ISP 1 during failover-1 event,
while the ﬁgure for ISP 2 during failover-2 events is 9%. During
failover-2 events, most of the ﬁrst loss bursts occur within ISP 2’s
neighbors and those neighbors are tier-1 ASes.
In addition, we
ﬁnd that about 55% and 96% of the second routing failures occur
within other tier-1 ASes during failover-1 and failover-2 events, re-
spectively. This means that routing failures occurring within ISP 1
or ISP 2 are propagated to their neighboring ASes.
In addition, we evaluate the occurrence of routing failures from
BGP updates, which are cascaded from ISP 1 or ISP 2 to other
Table 3: Location of the ﬁrst loss bursts caused by routing fail-
ures during failover events
Class
Failover-1
Failover-2
ISP 1
92%
0
ISP 2 Other tier-1
0
9%
5%
73%
Non tier-1
3%
18%
Table 4: Percentage of failover events involving routing failures
for three different hosts.
Class
Customer of either ISP 1 or ISP 2
Multihomed to ISP 1 and ISP 2
Customer of other ISPs
Failover
events
206
225
1054
Events causing
routing failure
111 (53%)
43 (19%)
463 (43%)
ASes. We ﬁrst examine routing failures from BGP updates col-
lected from 52 backbone routers within a tier-1 ISP. We observe
that 134 withdrawal messages come from 4 monitored routers. We
then use BGP updates from Oregon RouteView to examine routing
failures occurring at other ASes. We observe 210 withdrawal mes-
sages from 7 ASes, which do not include ISP 1 and ISP 2. Those
observations imply that routing failures during failover events in-
deed can be cascaded to other ASes.
We further classify PlanetLab hosts into three categories accord-
ing to their connection to ISP 1 and ISP 2: (1) single-homed to
either ISP 1 or ISP 2; (2) multi-homed to both ISP 1 and ISP 2;
and (3) customer of other ISPs. In our measurement, the number of
PlanetLab hosts in these three categories are 6, 6, and 25, respec-
tively. Table 4 shows the number of failover events in which there
is at least one loss burst caused by routing failures. We observe
that every category of hosts experience packet loss caused by rout-
ing failures, and, as expected, multi-homed hosts experience less
packet loss than the other two categories.
4.6 Methodology Evaluation
In this section, we evaluate our approach to correlating ICMP
messages with loss bursts. We identify packet loss caused by rout-
ing failures by correlating loss with ICMP unreachable messages.
The rationale is that if a destination network is unreachable from
a router according to its routing table, an ICMP unreachable des-
tination error message will be sent back to the source host. We
assess the number of ICMP messages in the absence of routing
change (i.e., at times other than the failover events). Recall that
ping packets are sent to the Beacon when there is no Beacon event
(10 minutes before and after 01:00, 03:00, 05:00, 09:00, 11:00,
13:00, 15:00, 17:00, 21:00, and 23:00). We observe a total of 3801
ICMP messages in our measurement during the period where there
is no faildown events (during which the Beacon is completely with-
drawn from both ISPs), only 0.6% of which are not caused by Bea-
con events. Thus, ICMP unreachable messages provide a good in-
dication for routing failures.
Another issue that might introduce bias to our measurement is
that some ISPs disable ICMP replies from their routers. We ex-
pect such policy to be typically uniformly applied to all the routers
within a given ISP. In our measurement, we observe that ICMP
messages come from 726 routers belonging to 68 ASes, and about
53% of those routers belong to 10 tier-1 ASes. In particular, 70
routers within ISP 1 (i.e., 52% of ISP 1’s routers visible in our
measurement) generate ICMP messages. The corresponding ﬁgure
for ISP 2 is 24 routers (95% of ISP 2’s routers observed). Given
such a good coverage of ASes responding with ICMP messages and
a high coverage from both ISP 1 and ISP 2, we conjecture that our
measurement is not signiﬁcantly biased by ICMP blocking.
 200
 150
 100
 50
s
t
s
r
u
b