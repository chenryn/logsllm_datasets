title:Voting: You Can't Have Privacy without Individual Verifiability
author:V&apos;eronique Cortier and
Joseph Lallemand
Voting: You Can’t Have Privacy without Individual
Verifiability
Véronique Cortier, Joseph Lallemand
To cite this version:
Véronique Cortier, Joseph Lallemand. Voting: You Can’t Have Privacy without Individual Verifiabil-
ity. [Research Report] CNRS, Inria, LORIA. 2018. hal-01858034
HAL Id: hal-01858034
https://hal.inria.fr/hal-01858034
Submitted on 18 Aug 2018
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Voting: You Can’t Have Privacy without Individual Verifiability
(Technical Report)
Véronique Cortier
CNRS, Loria
Nancy, France
PI:EMAIL
ABSTRACT
Electronic voting typically aims at two main security goals: vote
privacy and verifiability. These two goals are often seen as antago-
nistic and some national agencies even impose a hierarchy between
them: first privacy, and then verifiability as an additional feature.
Verifiability typically includes individual verifiability (a voter can
check that her ballot is counted); universal verifiability (anyone
can check that the result corresponds to the published ballots); and
eligibility verifiability (only legitimate voters may vote).
We show that actually, privacy implies individual verifiability. In
other words, systems without individual verifiability cannot achieve
privacy (under the same trust assumptions). To demonstrate the
generality of our result, we show this implication in two different
settings, namely cryptographic and symbolic models, for standard
notions of privacy and individual verifiability. Our findings also
highlight limitations in existing privacy definitions in cryptographic
settings.
1 INTRODUCTION
Electronic voting is often seen as a convenient way for running
elections as it allows voters to vote from any place. Moreover, it
eases the tally and it can therefore often be used for non trivial
counting procedures such as Single Transferable Vote or Condorcet.
Numerous voting systems have been proposed so far, like Helios [4],
Belenios [15], Civitas [14], Prêt-à-voter [30], or the protocols de-
ployed in Estonia [24] or in Australia [11] to cite a few. On the other
hand, many weaknesses or even attacks have been unveiled [31, 32],
from voting machines [23] to Internet voting [33].
In order to carefully analyse voting systems, security require-
ments have been defined. The two main security properties are:
• privacy: no one should know how I voted;
• verifiability is typically described through the three follow-
ing sub-properties.
– individual verifiability: a voter can check that her ballot is
counted;
– universal verifiability: anyone can check that the results
corresponds to the published ballots;
– eligibility verifiability: only legitimate voters may vote.
These two main properties seem antagonistic and an impossibility
result has even been established between verifiability and uncondi-
tional privacy [13], that is, a notion of privacy that is independent
of the power of the attacker.
The main contribution of this paper is to establish that, in fact,
(computational) privacy implies individual verifiability, that is, guar-
antees that all the honest votes will be counted. This result holds
for arbitrary primitives and voting protocols without anonymous
Joseph Lallemand
Inria, Loria
Nancy, France
PI:EMAIL
channels. To show that this implication is not due to a choice of a
very particular definition, we prove this implication in two very
distinct contexts, namely symbolic and cryptographic models. In
symbolic models, messages are represented by terms and the at-
tacker’s behaviour is typically axiomatised through a set of logical
formulas or rewrite rules. Cryptographic models are more precise.
They represent messages as bitstrings and consider attackers that
can be any probabilistic polynomial time Turing machines. Proofs
of security are made by reduction to well accepted security assump-
tions such as hardness of factorisation or discrete logarithm. In
both models, we consider a standard notion of privacy, already
used to analyse several protocols. In both cases, we establish that
privacy implies individual verifiability for a (standard) basic notion
of individual verifiability, namely that the result of the election
must contain the votes of all honest voters.
We now describe the main idea of the result. Actually, we show
the contrapositive implication: if there is an attack against individ-
ual verifiability, then there is an attack against privacy. To explain
the idea, let’s consider a very simple protocol, not at all verifiable.
In this simple protocol, voters simply encrypt their votes with the
public key of the election. The ballot box stores the ballots and, at
the end of the election, it provides the list of recorded ballots to the
talliers, who detain the private key, possibly split in shares. The
talliers compute and publish the result of the election. The ballot
box is not public and no proof of correct decryption is provided
so voters have no control over the correctness of the result. Such
a system is of course not satisfactory but it is often viewed as a
“basic” system that can be used in contexts where only privacy is a
concern. Indeed, it is typically believed that such a system guaran-
tees privacy provided that the attacker does not have access to the
private key of the election. In particular, the ballot box (that is, the
voting server) seems powerless. This is actually not the case. If the
ballot box aims at knowing how a particular voter, say Alice, voted,
he may simply keep Alice’s ballot in the list of recorded ballots and
then replace all the other ballots by encryptions of valid votes of
his choice, possibly following a plausible distribution, to make the
attack undetected. When the result of the election is published, the
ballot box will know all the votes but Alice’s vote, and will therefore
be able to deduce how Alice voted.
One may argue that such an attack is not realistic: the ballot box
needs to be able to change all ballots but one. Note however that
elections are often split in many small voting stations (sometimes as
small as 20 voters in total [18]). Therefore changing a few ballots can
be sufficient to learn how Alice voted. Maybe more importantly, this
attack highlights the fact that it is not possible to require privacy
without verifiability as sometimes specified by national agencies.
For example, in France, only privacy is required [1]. In Switzerland,
1
privacy is a pre-requisite and the level of verifiability depend on the
percentage of voters that can vote electronically [2]. Our findings
point out that if voters cannot trust some authorities w.r.t. the fact
that their vote will be counted they cannot trust the same authorities
w.r.t. their privacy, even for entities that do not have access to the
secret keys. Beyond the attack explained on a simple (and naive)
protocol, our proof that privacy implies individual verifiability
shows that as soon as a protocol is not verifiable, then the adversary
can take advantage of the fact that he may modify a vote without
being detected in order to break privacy. Individual verifiability is
only one part of verifiability. It does not account for universal nor
eligibility verifiability. So our result cannot be used to conclude that
a private voting scheme ensures all desirable verifiability properties.
Instead, it demonstrates that there is no hope to design a private
voting system if it does not include some degree of verifiability,
namely individual verifiability at least.
Our results also emphasise issues in existing privacy definitions.
Indeed, if privacy implies individual verifiability, how is it possible
to prove Helios [8] or Civitas [5] without even modelling the verifi-
cation aspects? How can a system that is not fully verifiable like
the Neuchâtel protocol be proved private [22]? As already pointed
out in [9], existing cryptographic definitions of privacy (see [7] for
a survey) implicitly assume an honest voting ballot box: honest bal-
lots are assumed to be properly stored and then tallied. Actually, we
notice that the same situation occurs in symbolic models. Although
the well adopted definition of privacy [21] does not specify how the
ballot box should be modelled, most symbolic proofs of privacy (see
e.g. [5, 18, 19, 21]) actually assume that the votes of honest voters
always reach the ballot box without being modified and that they
are properly tallied. The reason is that the authors were aware of
the fact that if the adversary may block all ballots but Alice’s ballot,
he can obviously break privacy. However, to avoid this apparently
systematic attack, they make a very strong assumption: the ballot
box needs to be honest. This means that previous cryptographic
and symbolic privacy analyses only hold assuming an honest ballot
box while the corresponding voting systems aim at privacy without
trusting the ballot box. This seriously weakens the security analysis
and attacks may be missed, like the attack of P. Roenne [29] on
Helios, for which there is no easy fix.
Why is it so hard to define vote privacy w.r.t. a dishonest ballot
box? Intuitively, vote privacy tries to capture the idea that, no
matter how voters vote, the attacker should not be able to see any
difference. The key issue is that the result of the election does leak
some information (typically the sum of the votes) and the adversary
may notice a difference based on this. This particularity makes vote
privacy differ from privacy in other contexts, where the adversary
really should learn no information. Therefore, most definitions
of vote privacy (roughly) say that, no matter how honest voters
voted, provided that the aggregation of the corresponding votes
remains the same, then the attacker should not see any difference.
However, as soon as the ballot box is dishonest, it may discard
some honest ballots and break privacy, as already discussed. The
first definition of privacy w.r.t. a dishonest ballot box [9] weakens
privacy by requiring that among the ballots that are ready to be
tallied, the (sub-)tally of the honest ones does not change. This
preliminary definition has two limitations. First, it assumes that the
tallied ballots are exactly the same as the cast ones, which is not
2
the case of all protocols (e.g. in ThreeBallots [28], only a part of the
ballot is published; in BeleniosRF [12], ballots are re-randomised).
Second, it does not model re-voting: the tally process cannot discard
ballots due to some revote policy.
We propose here another approach. Instead of changing the pri-
vacy definition, we now include a model of the verification process:
the ballots should be tallied only if the honest voters have success-
fully performed the tests specified by the protocol. We compare
our definition with [9] and an original definition of privacy [6] on
a selection of well-studied protocols, that have different levels of
verifiability (Helios, Civitas, Belenios, Neuchâtel, and our simple -
non verifiable - protocol). We show again that our notion of privacy,
w.r.t. a dishonest ballot box, implies individual verifiability. We do
not consider our new definition of privacy as final but it opens the
way to a better understanding of privacy in the context of fully
dishonest authorities.
Threat model. We show that privacy implies individual verifia-
bility, under the same trust assumptions, that is, trusting the same
group of authorities, channels, etc. In symbolic models, the pri-
vacy definition does not make prior assumptions on the threat
model. Instead, the encoding of the protocol defines which parties
are trusted. In particular, as already discussed, existing proofs of
privacy [5, 18, 19, 21] often implicitly assume that honest ballots
reach the ballot box without any modification. We show that when-
ever privacy holds then individual verifiability holds, for the same
encoding, hence the same assumptions. In contrast, most crypto-
graphic definitions of privacy implicitly assume an honest ballot
box. Therefore, we first show that privacy implies individual veri-
fiability, assuming an honest ballot box, considering the standard
definition of privacy by Benaloh [6]. Then we show that privacy
still implies individual verifiability, assuming a dishonest ballot box,
considering our novel definition of privacy, that explicitly models
the verification steps.
Related work. As already mentioned, [13] shows an impossibility
result between universal verifiability and unconditional privacy.
We show in contrast that the commonly used (computational) def-
initions of privacy actually imply verifiability. The discrepancy
between the two results comes from the fact that [13] considers
unconditional privacy while most protocols achieve only computa-
tional privacy, that is against a polynomially bounded adversary.
Interestingly, the impossibility result still holds between uncondi-
tional privacy and our notion of individual verifiability. [20] estab-
lishes a hierarchy between privacy, receipt-freeness, and coercion
resistance, while in a quantitative setting, [27] shows that this hi-
erarchy does not hold anymore. [16] recasts several definition of
verifiability in a common setting, providing a framework to com-
pare them. Besides [13], none of these approaches relates privacy
with verifiability. Many privacy definitions have been proposed as
surveyed in [7]. However, they all assume an honest ballot box. To
our knowledge, [9] is the only exception, as already discussed in
details. [18] shows how to break privacy by replaying a ballot. If an
attacker may replay Alice’s ballot and cast it in his own name (or
cast a related ballot), then he introduces a bias in the result, that
leaks some information on Alice’s vote. Note that this replay attack
does not break individual verifiability: honest votes are correctly
counted. We show here another breach for privacy: if an attacker
may remove some honest votes, then he breaks privacy as well.
Roadmap. We first prove that privacy implies individual verifia-
bility in symbolic models, in Section 3, and then in cryptographic
models, in Section 4. These two parts are rather independent. In Sec-
tion 6, we examine a selection of well-studied voting protocols and
compare the effect of different (cryptographic) notions of privacy
when the ballot box is dishonest.
2 PRELIMINARIES
Notations: The multiset of elements a, a, b, c is denoted {|a, a, b, c|}.
The union of two multisets S1 and S2 is denoted S1 ⊎ S2.
In both cryptographic and symbolic models, we assume a set V
of votes and a set R of possible results, equipped with an associative
and commutative operator ∗ (e.g. addition of vectors). A counting
function is a function ρ that associates a result r ∈ R to a multiset
of votes. We assume that counting functions have a partial tally
property: it is always possible to count the votes in two distinct
multisets and then combine the results.
∀V , V
′
ρ(V ⊎ V
′) = ρ(V) ∗ ρ(V
′)
A vote v is said to be neutral if ρ(v) is neutral w.r.t. ∗.