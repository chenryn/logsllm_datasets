title:An empirical study on the software integrity of virtual appliances:
are you really getting what you paid for?
author:Jun Ho Huh and
Mirko Montanari and
Derek Dagit and
Rakesh Bobba and
Dongwook Kim and
Yoonjoo Choi and
Roy H. Campbell
An Empirical Study on the Software Integrity of Virtual
Appliances: Are You Really Getting What You Paid For?
Jun Ho Huh
University of Illinois at
Urbana-Champaign, USA
PI:EMAIL
Rakesh B. Bobba
University of Illinois at
Urbana-Champaign, USA
PI:EMAIL
Mirko Montanari
University of Illinois at
Urbana-Champaign, USA
PI:EMAIL
Derek Dagit
University of Illinois at
Urbana-Champaign, USA
PI:EMAIL
Yoonjoo Choi
Dartmouth College, USA
PI:EMAIL
Dong Wook Kim
University of Illinois at
Urbana-Champaign, USA
PI:EMAIL
Roy Campbell
University of Illinois at
Urbana-Champaign, USA
PI:EMAIL
ABSTRACT
Virtual appliances (VAs) are ready-to-use virtual machine
images that are conﬁgured for speciﬁc purposes. For exam-
ple, a virtual machine image that contains all the software
necessary to develop and host a JSP-based website is typi-
cally available as a “Java Web Starter” VA. Currently there
are many VA repositories from which users can download
VAs and instantiate them on Infrastructure-as-a-Service (IaaS)
clouds, allowing them to quickly launch their services. This
marketplace, however, lacks adequate mechanisms that al-
low users to a priori assess whether a speciﬁc VA is really
conﬁgured with the software that it is expected to be con-
ﬁgured with. This paper evaluates the integrity of software
packages installed on real-world VAs, through the use of
a software whitelist-based framework, and ﬁnds that indeed
there is a lot of variance in the software integrity of packages
across VAs. Analysis of 151 Amazon VAs using this frame-
work shows that about 9% of real-world VAs have signiﬁcant
numbers of software packages that contain unknown ﬁles,
making them potentially untrusted. Virus scanners ﬂagged
just half of the VAs in that 9% as malicious, demonstrat-
ing that virus scanning alone is not suﬃcient to help users
select a trustable VA and that a priori software integrity
assessment has a role to play.
Categories and Subject Descriptors
D.4.6 [Software]: OPERATING SYSTEMS—Security and
Protection
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIA CCS’13, May 8–10, 2013, Hangzhou, China.
Copyright 2013 ACM 978-1-4503-1767-2/13/05 ...$15.00.
General Terms
Security, Experimentation, Measurement, Veriﬁcation
Keywords
IaaS, Virtual appliances, Whitelists, Software integrity
1.
INTRODUCTION
Cloud computing has become popular over the years as or-
ganizations have tried to reduce the complexities and costs
associated with deploying and maintaining an internal IT in-
frastructure. Infrastructure-as-a-Service (IaaS) cloud model
[24], where users can build and conﬁgure virtual comput-
ing infrastructure by renting computing platform resources
(e.g., [1, 6]) in the form of virtual machines, is a popular
cloud computing service model. On these rented virtual
machines, users have administrative privileges to install any
piece of software they want and can conﬁgure them to meet
their needs. Many services (e.g., [2, 10, 4, 5, 8]) that ei-
ther provide preconﬁgured virtual machine disk images or
enable sharing of such virtual machine disks have emerged
to support the IaaS paradigm.
Virtual machine (VM) disk images that are preconﬁgured
with the necessary software to support speciﬁc workﬂows of
functions are known as virtual appliances (VAs), and ser-
vices providing repositories of VAs are referred to as appli-
ance stores. A VM image with a Tomcat web server fully
conﬁgured to host Java web services and use the underlying
MySQL database is an example of a VA. In the rest of the
paper, VA and VM image will be used interchangeably. The
beneﬁts of this publisher-consumer model (see Figure 1) are
clear: consumers enjoy the convenience of downloading a VA
that suits their needs and launching a service quickly; and
publishers can get paid for providing the VA1. This market-
1Other models exist in which publishers provide the virtual
machine images to promote their software, or promote the
use of their IaaS infrastructure, or simply share their images
for the common good. All of these settings provide beneﬁts,
tangible or otherwise, to both consumers and publishers.
231place, however, lacks adequate mechanisms that allow users
to a priori assess whether a speciﬁc VA is correctly equipped
with the software packages that it claims to contain. This
paper sheds light on the integrity of software packages in
real-world VAs, and demonstrates a clear need for a mecha-
nism to assess their software integrity.
There are many well-known security challenges associated
with the cloud computing paradigm (e.g., [19, 11, 27, 14, 29,
28]), and the security implications of the publisher-consumer
model associated with VA stores (e.g., [32, 17]) are an im-
portant sub-class. A malicious publisher could install mal-
ware in a VA and attempt to read private information while
a consumer uses the VA. A careless publisher might unin-
tentionally publish a VA that is infected with a virus, is
conﬁgured insecurely (e.g., running an unpatched version of
an operating system, or with unintentional backdoors), or
contains sensitive private information. Further, an irrespon-
sible publisher could publish an incomplete VA that is miss-
ing integral software packages that it claims to have. Some
software packages might be partially installed (i.e., missing
critical ﬁles) or have ﬁles that are modiﬁed (i.e., diﬀerent
from the version provided by the software vendor). VAs
with such partially installed or modiﬁed packages will not
meet the expectations of VA consumers, who would expect
the VAs to be conﬁgured with all the packages claimed by
publishers, and that those packages are unmodiﬁed (unless
otherwise noted). Currently, checking the integrity and iden-
tify of the installed software packages is a time-consuming
process that must be performed manually by the consumer
after the instantiation of the VA. Such information, if avail-
able during the selection process, could provide early insight
into whether a VA is suitable and can be trusted. Some VAs
come with build-scripts and logs, but without any guaran-
tees that these logs are correct and accurate. The same is
true for VA publishers and store providers: they lack mech-
anisms to verify the integrity of VA software before allowing
them to be published, leading to security and quality assur-
ance problems in appliance stores.
This paper presents an empirical study on the integrity
of software packages in real-world VAs, assessed using a
whitelist-based framework, showing that there is a signiﬁ-
cant variance in the software integrity of software packages
across VAs. Our ﬁndings show that about 9% of the eval-
uated real-world VAs have signiﬁcant software integrity is-
sues and would probably not meet consumers’ expectations.
Surely, consumers should be informed about such VAs when
selecting a VA. We would like to point out that while a well-
conﬁgured VA with authentic software packages can still be
susceptible to software vulnerabilities that are present in le-
gitimate packages, the whitelisting approach can be used
to avoid VAs with partially installed or modiﬁed packages2
thus mitigating risk for consumers.
Whitelist-based approaches are hard to realize in general
computing environments, given the variety and vast amount
of existing software [20]. Our hypothesis is that in the ap-
pliance store model, a majority of consumers use fairly well-
known, popular pieces of software, and thus the whitelist
2Note that there is a legitimate need for modiﬁed or cus-
tomized software packages. Our focus is on VAs meant for
well established workﬂows with standard software stacks.
None of the VAs that we found to have “modiﬁed” software
in our analysis claimed to be custom or modiﬁed VAs in
their description.
Figure 1: A typical virtual appliance store
sizes will be more manageable. To test this hypothesis, we
constructed a whitelist for our sample VAs and indeed found
that the whitelist size growth decelerates.
Contributions: The key contributions of this paper are
(1) an empirical evaluation of the software integrity in real-
world VAs, showing that there is a signiﬁcant variation, (2)
demonstration of the usefulness of assessing the software in-
tegrity in VAs to help customers choose correctly conﬁgured
VAs, and (3) demonstration of the feasibility and scalability
of using whitelists for integrity assessment.
Organization: Section 2 covers related work. Section 3
provides an overview of the assessment framework used for
analyzing real-world VAs in Section 4. Section 5 discusses
the security implications. Our conclusions and future work
are in Section 6.
2. RELATED WORK
Over the last few years, researchers and industry practi-
tioners have been looking at challenges associated with cloud
computing, including security and reliability challenges [19,
11, 26, 15, 27, 32, 14, 28, 29, 34, 16, 18, 33]. Here, we
focus our discussion on past work [26, 32, 34, 17, 21] that
addresses security risks and challenges associated with VA
stores and with the management of VAs in general, which is
most relevant to our work.
Reimer et al.
[26] address the challenges posed by the
sprawl of virtual machine images that need to be mounted
for maintenance tasks. Speciﬁcally they propose Mirage Im-
age Format (MIF), which simpliﬁes the maintenance and ad-
ministration process. To our knowledge, Wei et al. [32] are
the ﬁrst work to identify the security challenges and risks as-
sociated with the VA store model and with managing VAs in
general. They proposed an image management framework
with access control for VAs, ﬁltering to remove unwanted
information (e.g., browsing history), provenance tracking
of images for accountability, and image maintenance (e.g.,
virus scanning), and argues for the beneﬁts of such a system.
In a later work [34], a scalable oﬄine patching tool called
N¨uwa, which leverages MIF to signiﬁcantly reduce the over-
head of patching, was proposed. Accountability through
provenance tracking, virus and malware scanning, and soft-
ware patching are good security hygiene practices and do
provide some assurances to the consumer regarding the se-
curity and trustworthiness of the VA. However, they alone
are not suﬃcient. Accountability is after the fact and may
come too late for some consumers, and the proportion of new
or near zero-day malware that is detected through scanning
is not very high [12].
In contrast, our approach provides
more explicit information regarding a VA’s software conﬁg-
232uration to the consumer and is complementary to the afore-
mentioned work.
Bugiel et al. [17] analyzed over a thousand Amazon Ma-
chine Images (VAs for the Amazon Elastic Compute Cloud)
and found sensitive information (e.g., keys and credentials)
and SSH backdoors inadvertently left in the VAs. To mit-
igate the risks to publishers and consumers they suggest
countermeasures that include running ﬁlters and scanners on
VAs and use of ratings based on certain veriﬁable properties
and reputation, but they do not elaborate on the properties
of interest or the rating system. There are in fact appliance
stores that provide ratings for VAs [8]. However, the pri-
mary basis for the ratings seems to be user opinion, which,
while quite useful, may not be objective, and may be based
on many factors, of which software integrity and security are
only a part. Furthermore, in ratings-based systems that are
based on reputation and user feedback, it takes time for VAs
to gather ratings and publishers to earn reputation, and it
will be easy for brand-name providers to build their reputa-
tion even though many of the smaller players may be pro-
viding VAs that are as good or better than their well-known
counterparts. In contrast, our software integrity assessment
approach provides a means to rate VAs before publishing
them, and it could be used either on its own or as part of
a larger ratings ecosystem that also considers reputation of
provider and user feedback.
Jayaram et al. [21] undertook an empirical study of sim-
ilarity between VAs to enable eﬃcient design of VA man-
agement systems. Their study considered block-level sim-
ilarity in VAs to enable eﬃcient design of de-duplication
schemes, reduce storage, and improve image distribution,
among other things. Our work focuses on use of similar-
ity in VMs at the software or ﬁle level to gauge the size of
whitelists that need to be maintained by a VA store provider.
Moreover, to the best of our knowledge, we are the ﬁrst
group to analyze comprehensively the software integrity of
VAs used in the real world.
Techniques based on ﬁle analysis similar to the one we
used have been used previously in the context of operating
systems. Seminal work from Kim et al. [23] and Vincenzetti
et al.
[31] introduced mechanisms for detecting changes in
critical ﬁles in the system that might indicate the presence
of rootkits or of other illicit modiﬁcations. These techniques
are based on comparing ﬁles in the disk with a previously-
generated hash of the ﬁle to detect changes. More recent
work [25] extended such approaches to perform the analysis
in a virtual machine environment, so that modiﬁcations to
the ﬁles cannot be hidden even when the kernel is compro-
mised. The goal of such techniques is to identify at runtime
malware or malicious modiﬁcations to critical ﬁles by de-
tecting changes from a trusted conﬁguration (i.e., the initial
conﬁguration of the system). While we use similar ﬁle in-
tegrity based techniques, our focus is on analyzing the soft-
ware integrity of a VA to assess the “initial” conﬁguration.
Further, our analysis discusses ratings for assessed software
in contrast to earlier eﬀorts that stop at detecting changes.
3. SOFTWARE INTEGRITY ASSESSMENT
FRAMEWORK
To study the integrity of software packages in real-world
VAs, we designed an integrity assessment framework based
on software whitelisting techniques. At the heart of the
Figure 2: Software integrity assessment framework
overview
framework is a Conﬁguration Resolver (described in Section
3.1) that uses a VM Conﬁguration Veriﬁcation Tool (VM-
CVT) to generate a “veriﬁcation report” on a given VA (see
Figure 2). Those tools use information provided by publish-
ers and by software producers to verify the integrity of the
software installed on the system, and to identify any modi-
ﬁed or missing ﬁles. Based on this information, we introduce
a simple rating system to score the installed software. The
veriﬁcation reports provide consumers with explicit informa-
tion about the conﬁguration of the system that they can use
to gauge its suitability without having to download and in-
stantiate the VA. Providers can use the veriﬁcation reports
to ensure that published VAs respect basic requirements by
specifying policies. For example, a cloud provider could cre-
ate a more trustable store by introducing a policy stating
that all of the installed software and updates need to be
fully veriﬁed. Such policies are discussed further in Section
5.3 as means to enhance VA security.
Our integrity validation is based on reference integrity
measurements (RIMs) [30] published by the vendors for their
software. The RIMs contain the signed hash values of all the
ﬁles in the software and metadata, including a description
of each of them. This is a common software practice, and
many software vendors already provide signed checksums
for ﬁles (e.g., rpm packages contain MD5 checksums for the
ﬁles), allowing users to verify the ﬁle integrity. In fact, the
U.S. National Institute of Standards and Technology (NIST)
maintains a software registry, called the National Software
Reference Library (NSRL) [7], that provides RIMs for soft-
ware. Bit9, a private security company, also maintains a
massive software registry [3].
The framework can take advantage of additional informa-
tion made available from publishers and from software ven-
dors. In particular, we take advantage of the fact that VAs
are often generated from trusted base images made avail-
able from the repository provider (e.g., Amazon Linux AMIs
[1]) to speed up the analysis. This technique also allows a
provider to whitelist implicitly proprietary software without
having to register with third party Resolver. Moreover, a
log describing the list of all software (and updates) installed
on a VA, as well as the version information at the time of
233VA publication, can be generated by the publishers and sub-
mitted with the VA to simplify the analysis. If publishers
submit fabricated logs (e.g., lying about the installed pack-
ages), their VAs will get low integrity scores as the installed
ﬁles will not match the RIMs and will appear unreliable.
Software vendors can provide additional metadata associ-
ated with the RIMs for identifying the ﬁle types (e.g., source