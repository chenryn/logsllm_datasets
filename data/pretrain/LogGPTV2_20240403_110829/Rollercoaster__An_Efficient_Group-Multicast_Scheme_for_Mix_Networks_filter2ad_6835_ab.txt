tal message size of |msg| bytes and the rates given in mes-
sages/s, the required bandwidth of a client can be estimated
as (λp + λd + λl) · |msg| bytes/s.
3.2 Multicast and Group Messaging
A multicast protocol allows a single message to be delivered
to all members of a group. Broadly speaking, there are two
approaches for implementing multicast: by sending each mes-
sage individually to each recipient over unicast, or by relying
on the underlying network to make copies of a message that
are delivered to multiple recipients. IP multicast [13] is an
example of the latter approach, which avoids having to send
the same message multiple times over the same link.
In this paper we are interested in group multicast, a type
of multicast protocol in which there is a pre-deﬁned, non-
hierarchical group of users U . At any time any member of
the group might send a message to all other group members.
We call the initial sender source s and all others the intended
recipients Urecv = U \ s.
4 Naïve Approaches to Multicast
In this section we discuss the reasons why message delays
occur in Loopix. We then explore two simple approaches to
implementing multicast on Loopix, and explain why they are
not suitable, before introducing Rollercoaster in Section 5.
We deﬁne the message latency dmsg of a single unicast
message msg from user to A to user B:
dmsg = Trecv,B − Tsend,A
(1)
where Tsend,A is the time at which user A’s application sends
msg, and Trecv,B is the time at which user B’s application re-
ceives the message.
In Loopix, message delays are the sum of delays at various
points in the network. First, any outbound message sent by
the user to the provider experiences a queuing delay dQ based
on the number of messages in the send queue. The delay
between two successive messages in the queue being sent,
dp, is exponentially distributed with a rate parameter λp (see
Table 1). Hence, a message’s time spent in the send queue,
dQ, is a random variable with a Gamma distribution Γ(n, 1
),
λp
where the shape parameter n denotes the number of messages
in the queue ahead of our message msg.
Secondly, the payload message is held up at the ingress
provider and each of the l mix nodes by an exponentially-
distributed delay dµ. Finally, the receiving user checks their
inbox in ﬁxed time intervals of ∆pull, leading to a delay dpull
that is uniformly distributed between 0 and ∆pull. Therefore
the message delay in a Loopix network with l layers can be
expressed as a sum of these components:
dmsg = dQ + dp + (l + 1) · dµ + dpull
(2)
The above equation ignores processing and network delays.
The Loopix paper demonstrates that these are negligible com-
pared to the delays imposed by sensible rate parameters.
For a Poisson distribution with parameter λ, the expected
) has the mean
. For the pull interval, the expected mean delay is ∆pull/2.
mean is 1/λ. The Gamma distribution Γ(n, 1
λp
n
λp
3436    30th USENIX Security Symposium
USENIX Association
Hence, the mean latency for Equation 2 is:
mean(dmsg) =
n + 1
λp
+
l + 1
λµ
+
∆pull
2
(3)
When a source s wants to send a payload to a group by multi-
cast, we deﬁne the multicast latency D to be the time from the
initial message sending until all of the recipients Urecv have
received the message:
D = max
u∈Urecv
(Trecv,u) − Tsend,s
(4)
4.1 Naïve Sequential Unicast
In the simplest implementation of multicast, the source user s
sends an individual unicast message to each of the recipients
u ∈ Urecv in turn. While the messages can travel through the
mix network in parallel, their emission rate is bounded by the
payload rate λp of the sender.
For a recipient group of size |Urecv| = m − 1, the last mes-
sage in the send queue will be behind n = m − 2 other mes-
sages. Further, the last message will incur the same network
delay and pull delay as all other unicast messages. The average
delay for the last message therefore describes the multicast
latency for when performing sequential unicast:
Dunicast =
m − 1
λp
+
l + 1
λµ
+
∆pull
2
= O(m)
(5)
The mean delay Dunicast therefore grows linearly with m.
As we show in Section 6, sequential unicast is too slow for
large groups with realistic choices of parameters (λp is typi-
cally set to less than one message per second).
Another problem with the sequential unicast approach is
that the effective rate at which a user can send messages to the
λp
m−1 , as all copies of the ﬁrst message need to be sent
group is
before the second multicast message can begin transmission.
One might argue that this problem can be addressed by
increasing the payload bandwidth by increasing the value for
λp. However, this would require similar adjustments to the
rates for drop and loop messages to preserve the network’s
anonymity properties. As these parameters are ﬁxed across
all users, this would lead to a proportional increase in overall
bandwidth used by the network. Moreover, the factor by which
we increase λp would be determined by the largest group
size we want to support. As a result, users participating in
smaller groups would face an unreasonable overhead. This
inefﬁciency particularly applies to users who mostly receive
and only rarely send messages.
a single message from the source and creates |Urecv| = m − 1
mix messages sent on to the other group members. A provider
node would not be suitable as a multiplication node as it
would learn about the group memberships of its users and
their group sizes.
When the multiplication node receives such a multicast
message, it inserts m − 1 messages into its input buffer, one
for each of the recipients, and processes them as usual. This
provides optimal group message latency of D = dmsg as there
is no rate limit on messages sent by a mix node, and hence no
queuing delay. However, this design has signiﬁcant ﬂaws.
First, a corrupt multiplication mix node can learn the exact
group size |U| = m, in contravention of our threat model. This
is undesirable as it may allow an attacker to make plausible
claims regarding the presence or absence of communication
within certain groups. Even without corrupting a node, an
adversary can observe the imbalance between incoming and
outgoing messages of a multiplication node.
The weakened anonymity properties could perhaps be miti-
gated with additional cover trafﬁc that incorporates the same
behaviour as the payload trafﬁc. In particular, the cover trafﬁc
must model all possible group sizes. Allowing a group size of
200 requires cover trafﬁc to multicast by factor 200 as well.
However, this would signiﬁcantly increase the network band-
width requirements in the following mix layers, increasing
the cost of operating the network.
Permitting message multiplication also opens up the risk
of denial of service attacks: a malicious user could use the
multicast feature to send large volumes of messages to an
individual provider, mix node, or user, while requiring com-
paratively little bandwidth themselves.
Finally, supporting group multicast in a mix node requires
the input message to contain m − 1 payloads and headers, one
for each outgoing message. As all outgoing messages must
travel independently of each others they must be encrypted
with different keys for their respective next hops. Otherwise,
all outgoing messages share the same encrypted payload. This
makes it trivial for an observer to identify the recipients of
this group message. The only solution is to either increase the
size of all messages in the system or enforce a very low limit
on maximum group size.
In summary, naïvely performing message multiplication on
mix nodes is not a viable option. However, a viable variant of
this approach is possible by ﬁxing the multiplication factor of
messages to be a small constant (e.g. p = 2). We discuss this
design in Section 5.4 where we present MultiSphinx.
5 Rollercoaster
4.2 Naïve Mix-Multicast
An alternative approach shifts the multicast distribution of
a message to mix nodes. In this scheme, the source chooses
one mix node as the multiplication node. This node receives
We propose Rollercoaster as an efﬁcient scheme for group
multicast in Loopix. Rollercoaster distributes the multicast
trafﬁc over multiple nodes, arranged in a distribution graph.
This not only spreads the message transmission load more
uniformly across the network, but it also improves the balance
USENIX Association
30th USENIX Security Symposium    3437
A
B
a
b
f
a
b
f
g
g
c
h
c
h
h
i
i
i
d
e
d
d
e
acting as "c"
5.1 Detailed Construction
The Rollercoaster scheme is built upon the concept of a sched-
ule. This schedule is derived deterministically from the source
s, the total set of recipients Urecv, and the maximum branching
factor k following Algorithm 1. First, a list U of all group
members is constructed with the initial source at the 0-th in-
dex. The group size |U| and branching factor k lead to a total
of ⌈logk+1 |U|⌉ levels. In the t-th level the ﬁrst (k + 1)t mem-
bers have already received the message. All of them send the
message to the next w recipients, increasing the next group of
senders to (k + 1)t+1. In the 0-th level only U[0] (the initial
sender) sends k messages to U[1] . . .U[k].
Algorithm 1 The basic Rollercoaster schedule algorithm for
a given initial source s, list of recipients Urecv, and branching
factor k. The schedule contains a list for every level with a
tuple (sender, recipient) for each message to be sent.
1: procedure GENSCHEDULE(s, Urecv, k)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Figure 2: Message distribution graph for a group of size m = 9
and branching factor k = 2. Graph A: Expected delivery from
source s = a. Graph B: The node c is ofﬂine and breaks
delivery to h and i. Using the fault-tolerant variant the node d
is assigned the role of c and delivers the payload to h and i.
of payload and cover trafﬁc. Rollercoaster is implemented as
a layer on top of Loopix, and it does not require any modi-
ﬁcations to the underlying Loopix protocol (we discuss an
optional protocol modiﬁcation in Section 5.4).
As we have seen with naïve sequential unicast, messages
slowly trickle from the source into the network as the source’s
message sending is limited by the payload rate λp. However,
users who have already received the message can help dis-
tribute it: after the source has sent the message to the ﬁrst
recipient, both of them can send it to the third and fourth
recipient concurrently. Subsequently, these four nodes then
can send the message to the next four recipients, and so on,
forming a distribution tree with the initial source at the root.
The distribution tree for a set of users U is structured in
levels such that each parent node has k children at each level,
until all recipients have been included in the tree. An example
with eight recipients is shown in Figure 2. With each level the
total number of users who have the message increases by a
factor of k + 1, which implies that the total number of levels
is logarithmic in the group size |U|.
In this section we ﬁrst detail the construction of Roller-
coaster in Section 5.1. As a second step, Section 5.2 adds
fault tolerance to ensure that the scheme also works when
nodes are ofﬂine. Asymptotic delay and trafﬁc properties are
analysed in Section 5.3. Section 5.4 develops the MultiSphinx
message format, which allows restricted multicast through
designated mix nodes. Further optimisations to the scheme
are brieﬂy discussed in Section 5.5.
U ← [s] +Urecv
L ← ⌈logk+1 |U|⌉
schedule ← [ ]
for t = 0 until L − 1 do
p ← (k + 1)t
w ← min(k · p, |U| − p)
R ← [ ]
for i = 0 until w − 1 do
⊲ number of levels
⊲ ﬁrst new recipient
k ⌋
idxsender ← ⌊ i
idxrecipient ← p + i
R[i] ← (U[idxsender],U[idxrecipient])
schedule[t] ← R
return schedule
In order to associate an incoming message with the correct
source node and group of all recipients, all Rollercoaster pay-
loads contain a 16 byte header as illustrated in Figure 3, in
addition to the Sphinx packet header used by Loopix. Each
group is identiﬁed by a 32-bit groupid shared by all group
members. The 32-bit nonce identiﬁes previously received
messages, which becomes relevant with fault-tolerance (Sec-
tion 5.2). The ﬁelds source, sender, and role refer to individual
group members and have a 10-bit size, allowing groups with
up to 1024 members. The source ﬁeld indicates the original
sender and is necessary to construct the distribution graph at
the recipient. The ﬁelds sender and role are used by the fault
tolerant variant in Section 5.2 for acknowledgement messages
and to route around nodes that are ofﬂine. Field lengths can
easily be increased or decreased as they do not have to be
globally the same across all Loopix clients. Finally, the header
contains a signature that is generated by the original source
and covers the payload as well as all static header ﬁelds. It
assures recipients that the message indeed originated from
a legitimate group member and that they are not tricked by
an adversary to start distributing a fake message to group
3438    30th USENIX Security Symposium
USENIX Association
0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
groupid
nonce
source
sender
role
Signature over {groupid, nonce, source, payload}
. . . payload . . .
Figure 3: Payload header for the Rollercoaster scheme con-
taining both the ﬁelds for the minimal scheme and the ﬁelds