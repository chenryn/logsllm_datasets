can possibly identify such non-ideal AD training conditions
by analyzing the entropy of a particular dataset (too high
or too low may indicate exceptional circumstances). We
leave this analysis for the future. Furthermore, although we
cannot predict the time of an attack in the training set, the
attack itself will manifest as a few packets that will not per-
sist throughout the dataset. Common attack packets tend to
cluster together and form a sparse representation over time.
For example, once a worm outbreak starts, it appears con-
centrated in a relatively short period of time, and eventu-
ally system defenders quarantine, patch, reboot, or ﬁlter the
infected hosts. As a result, the worm’s appearance in the
dataset decreases [16]. We expect these assumptions to hold
true over relatively long periods of time, and this expecta-
tion requires the use of large training datasets to properly
sanitize an AD model. In short, larger amounts of training
data can help produce better models — a supposition that
seems intuitively reasonable.
We must be cautious, however, as having a large train-
ing set increases the probability that an individual datum
appears normal (the datum appears more frequently in the
dataset; consequently, the probability of it appearing “nor-
mal” increases). Furthermore, having the AD system con-
sider greater amounts of training data increases the proba-
bility of malcode presence in the dataset. As a result, mal-
code data can poison the model, and its presence compli-
cates the task of classifying normal data. We next describe
how we use micro-models in an ensemble arrangement to
process large training data sets in a manner that resists the
effects of malcode content in that data.
84
2.2 Micro-models
Our method of sanitizing the training data for an AD sen-
sor employs the idea of “ensemble methods.” Dietterich [7]
deﬁnes an ensemble classiﬁer as “a set of classiﬁers whose
individual decisions are combined in some way (typically
by weighted or unweighted voting) to classify new exam-
ples.” Methods for creating ensembles include, among other
actions, techniques that manipulate the training examples.
Given our assumption about the span of attacks in our train-
ing set, it seems appropriate to use time-delimited slices of
the training data.
We employ the following strategy: consider a large train-
ing dataset T partitioned into a number of smaller disjoint
subsets (micro-datasets): T = {md1, md2, . . . , mdN}
where mdi is the micro-dataset starting at time (i − 1) ∗ g
and, g is the granularity for each micro-dataset. We deﬁne
the model function AD: M = AD(T ) where AD can be
any chosen anomaly detection algorithm, T is the training
dataset, and M denotes the model produced by AD.
In order to create the ensemble of classiﬁers, we use
each of the “epochs” mdi to compute a micro-model, Mi.
Mi = AD(mdi). We posit that each distinct attack will be
concentrated in (or around) time period tj affecting only a
small fraction of the micro-models: Mj may be poisoned,
having modeled the attack vector as normal data, but model
Mk computed for time period tk, k (cid:2)= j is likely to be un-
affected by the same attack. In order to maximize this like-
lihood, however, we need to identify the right level of time
granularity g. Naturally, epochs can range over the entire
set of training data. Our experiments, reported in Section 3,
analyze network packet traces captured over approximately
500 hours. We ﬁnd that a value of g from 3 to 5 hours was
sufﬁcient to generate well behaved micro-models.
2.3 Sanitized and Abnormal Models
After generating the micro-models, we compute a new
AD model using the set of previously computed micro-
models. In this second phase, we produce a sanitized nor-
mal model using either the training set used to produce the
micro-models or a second set of training data. Splitting the
training data set into two parts represents the worst case sce-
nario, because it assumes that we are not able to store the
large dataset necessary to build the micro-models. Hence,
the AD sensor is required to generate the micro-models on-
line using a fraction of the necessary space (the models are
far smaller than the raw trafﬁc). Then, we can sanitize the
training dataset by (online or ofﬂine) testing using all the
pre-computed micro-models Mi. Each test results in a new
labeled data set with every packet Pj labeled as normal or
abnormal:
Lj,i = T EST (Pj, Mi)
(1)
where the label, Lj,i, has a value of 0 if the model Mi deems
the packet Pj normal, or 1 if Mi deems it abnormal.
3 Evaluation of Sanitization
However, these labels are not yet generalized; they re-
main specialized to the micro-model used in each test. In or-
der to generalize the labels, we process each labeled dataset
through a voting scheme, which assigns a ﬁnal score to each
packet:
N(cid:1)
i=1
SCORE(Pj) =
1
W
wi · Lj,i
(2)
where wi is the weight assigned to model Mi and W =
(cid:2)N
i=1 wi. We have investigated two possible strategies:
simple voting, where all models are weighted identically,
and weighted voting, which assigns to each micro-model
Mi a weight wi equal to the number of packets used to train
it. The study of other weighting strategies can provide an
avenue for future research.
To understand the AD decision process, we consider the
case where a micro-model Mi includes attack-related con-
tent. When used for testing, the AD may label as normal
a packet containing that particular attack vector. Assum-
ing that only a minority of the micro-models will include
the same attack vector as Mi, we use the voting scheme to
split our data into two disjoint sets: one that contains only
majority-voted normal packets, Tsan from which we build
the sanitized model Msan, and the rest, used to generate a
model of abnormal data, Mabn.
Tsan =
(cid:3){Pj | SCORE(Pj) ≤ V }, Msan = AD(Tsan)
(cid:3){Pj | SCORE(Pj) > V }, Mabn = AD(Tabn)
Tabn =
where V is a voting threshold. In the case of unweighted
voting, V is the maximum percentage of abnormal labels
permitted such that a packet is labeled normal. Conse-
quently, it must be the case that 1 − V > Np, where Np
is the maximum percentage of models expected to be poi-
soned by any speciﬁc attack vector. We provide an analysis
of the impact of this threshold on both voting schemes in
Section 3.
After this two-phase training process, the AD sensor can
use the sanitized model for online testing. Note that we
have described a general approach to sanitization without
resorting to the speciﬁc details of the AD decision process;
it is enough that the AD sensor outputs a classiﬁcation for
each discrete piece of its input (e.g., a network packet or
message). Consequently, we believe that our approach can
help generate sanitized models for a wide range of anomaly
detection systems. In the remainder of this paper, we eval-
uate our approach on two anomaly sensors drawn from the
research literature; we are in the process of evaluating two
others (pH and libanomaly).
In this section, we quantify the increase in the detection
accuracy of any content-based AD system when we apply
training data sanitization. We treat the AD sensor as a black
box to avoid using optimizations that are speciﬁc to a par-
ticular AD system. In the following experiments, we use
two anomaly sensors: Anagram [29] and Payl [28,30]. Both
sensors are n-gram content-based anomaly detectors for net-
work packets. Although they both use an n-gram approach,
these sensors have very different learning algorithms. The
details of these algorithms are beyond the scope of this pa-
per. We refer the interested reader to the citations above.
We evaluate our approach using two different scenarios.
In the ﬁrst scenario, we measure the performance of the sen-
sor with and without sanitization. Additionally, we use the
sensor as a packet classiﬁer for incoming network trafﬁc:
we test each packet and consider the computational costs
involved in diverting each alert to a host-based shadow sen-
sor. Both the feasibility and scalability of this scenario de-
pend mainly on the amount of alerts generated by the AD
sensor, since all “suspect-data” (data that causes the sensor
to generate an alert) are signiﬁcantly delayed by the shadow
sensor.
Our experimental corpus consists of 500 hours of real
network trafﬁc, which contains approximately four million
content packets. We collected the trafﬁc from three differ-
ent hosts: www, www1, and lists. We partitioned this data
into three separate sets: two used for training and one used
for testing. We use the ﬁrst 300 hours of trafﬁc to build the
micro-models and the next 100 hours to generate the sani-
tized model.
The remaining 100 hours of data was used for testing. It
consists of approximately 775, 000 packets (with 99 attack
packets) for www1, 656, 000 packets (with 70 attack pack-
ets) for www, and 26, 000 packets (with 81 attack packets)
for lists. Given that www1 exhibits a larger volume of traf-
ﬁc, we chose to perform a more in-depth analysis on its
trafﬁc. In addition, we applied a cross-validation strategy:
we used the last 100 hours to generate the sanitized model
while testing on the other preceding 100-hour dataset.
Throughout the paper, we refer to detection and false
positive rates as rates determined for a speciﬁc class of at-
tacks that we observed in these data sets. We note that dis-
covering ground truth for any realistic data set is currently
infeasible. We are, in part, trying to address this chicken-
and-egg problem through this work.
3.1 Experimental Results
Initially, we measured the detection performance of both
Anagram and Payl when used as standalone AD sensors
without sanitizing the training data. Then, we repeated the
85
experiments with the same setup and network traces, but
we included the sanitization phase. Table 1 presents our
ﬁndings, which show that sanitization boosts the detection
capabilities of both sensors. The results summarize the av-
erage values of false positive (FP) and true positive (TP)
rates. Both voting methods perform well. We used a granu-
larity of three hours and a value of V which maximizes the
detection performance (in our case V ∈ [0.15, 0.45]).
The optimal operating point appears to be that which
maximizes the detection of the real alerts and has the lowest
FP rate. Section 3.2 studies this point in more detail. For
Anagram, when the sanitized and abnormal models were
created, given the nature of the sensor, the two models were
built to be disjoint (no abnormal feature would be allowed
inside the sanitized model). The trafﬁc contains instances
of phpBB forum attacks (mirela, cbac, nikon, criman) for
all three hosts that are analyzed.
Sensor
0
0
0
www1
www
lists
FP(%) TP(%) FP(%) TP(%) FP(%) TP(%)
0.07
A
A-S
0.04
A-SAN 0.10
P
0.84
P-SAN 6.64
0.01
0.29
0.34
6.02
10.43
0.04
0.05
0.10
64.14
2.40
18.51
100
64.19
86.54
17.14
100
40
61
20.20
100
0
76.76
FP: false positive rate; TP: true positive rate
0
Sensor www1 www
A
A-S
A-SAN
P
P-SAN
59.10
294.11
6.64
5.84
0
505
1000
11.56
0
lists
0
370.2
1000
1.00
36.05
signal-to-noise ratio (TP/FP); higher values mean
better results
Table 1. Impact of the sanitization phase on
the AD performance (A = Anagram; A - S = Ana-
gram + Snort; A - SAN = Anagram + sanitization; P = Payl;
P - SAN = Payl + sanitization )
Note that without sanitization, the normal models used
by Anagram would be poisoned with attacks and thus un-
able to detect new attack instances appearing in the test data.
Therefore, increasing AD sensor sensitivity (e.g. changing
its internal detection threshold) would only increase false
alerts without increasing the detection rate. When using
previously known malcode information (using Snort signa-
tures represented in an “abnormal model”), Anagram was
able to detect a portion of the attack packets. Of course, this
detection model is limited because it requires that a new 0-
day worm will not be sufﬁciently different from previous
worms that appear in the traces. To make matters worse,
86
such a detector would fail to detect even old threats that do
not have a Snort signature. On the other hand, if we enhance
Anagram’s training phase to include sanitization, we do not
have to rely on any other signature or content-based sensor
to detect malware.
Furthermore, the detection capability of a sensor is in-
herently dependent on the algorithm used to compute the
distance of a new worm from the normal model. For ex-
ample, although Payl is effective at capturing attacks that
display abnormal byte distributions, it is prone to miss well-
crafted attacks that resemble the byte distribution of the tar-
get site [9]. Our traces contain such attacks: we observe
this effect when we use the sanitized strategy on Payl, as
we can only get a maximum 86.54% attack detection rate.
In this case the sanitization phase is a necessary but not suf-
ﬁcient process for reducing false negatives: the actual algo-
rithm used by the sensor is also important in determining
the overall detection capabilities of the sensor.
Interestingly, the combination of Payl operating on the
lists data set without sanitization shows a high FP rate com-
pared to the same case where sanitization is used. After