I am using TF2.0 latest nightly build and I am trying to train LSTM model for
text classification on very large dataset of 16455928 sentences. For embedding
layer in the model, I have a vocab size of 366856 and I used 1000 as embedding
dimension value in it, on which the 2 GPUs(Tesla T4 from Google) ran out of
memory.  
Since I can not lower the size of vocabulary (maybe there is a way), so I used
lower value for embedding dimension (100) on which the model starts training.
Now my question is if there is a way I can use higher value of embedding
dimension?. Maybe by putting set of layers of my model on different GPUs, if
so then what is the way in TF2.0? Also, will using more number GPUs help?
Thank you!