SMTP (Simple Mail Transport Protocol) is the dominant protocol
today to exchange emails between mail servers across organiza-
tional boundaries. An enterprise typically filters incoming emails
over SMTP before they reach its internal email servers. Figure 1
shows the typical structure of an email per the SMTP specification,
which is composed of two parts: the header and the body (i.e., the
content). The header contains key information for email delivery,
such as the email addresses and names of the sender and recipients,
subjects, delivery dates, the communicating SMTP servers and their
corresponding timestamps. The email content contains everything
else in an email, including attachments. With the introduction of
MIME extensions, besides unstructured text, email contents nowa-
days could also include non-textual information such as images,
documents, and videos.
Since an email header contains vital information for mail deliv-
ery and needs to be updated when an email is delivered from one
email server to another, email headers typically are not encrypted
207RAID â€™21, October 6â€“8, 2021, San Sebastian, Spain
Mohamed Nabeel, Enes Altinisik, Haipei Sun, Issa Khalil, Hui (Wendy) Wang, and Ting Yu
Figure 3: The architecture of CADUE
Figure 2: The architecture of enterprise email systems
in todayâ€™s email encryption protocols and systems [17, 31, 37]. They
largely focus on encryption of email contents, which motivates this
work to investigate content-agnostic techniques to filter unwanted
E2EE emails. Sometimes, to further enhance privacy, some com-
mercial E2EE systems (e.g., ProtonMail and Tutanota) also support
encrypting or stripping some header fields (for example, subjects,
and the IPs and timestamps of the email servers involved in the
delivery of an email) while still compliant with SMTP. The tech-
nique proposed in this paper could be easily adapted to different
email encryption settings, as will be shown in our experiments
(section 6).
3.2 Enterprise Email Systems
An enterprise email system handles emails from the external Inter-
net as well as those inside the enterprise Intranet. All the emails
from the external Internet go through a firewall before reaching
the Intranet. Security applications are typically installed behind the
firewall, and filter out unwanted emails so that only benign emails
can reach the Intranet. The filtered emails are normally stored in
log files. Most enterprises archive the email logs for a considerable
period of time, ranging from six months to several years depending
on legal regulations and its security policy. Figure 2 illustrates the
architecture of a typical email system in an enterprise.
Typically, an enterprise has its own email domain name (e.g.
paypal.com), and its users have their unique email addresses under
the enterprise domain name (e.g. PI:EMAIL)1. An incoming
email is either external if the senderâ€™s domain is different from that
of the enterprise, or internal if its sender domain address is the
same as that of the enterprise. All incoming external emails have
to go through the firewall before they reach the Intranet, while
internal emails are transmitted within the Intranet. In this paper,
we assume that attackers are from outside the enterprise and focus
on filtering incoming external emails. Meanwhile, we will leverage
internal emails to design features about the communication pattern
among internal users to help identify unwanted external emails.
3.3 Threat Model
In this work, we focus on an enterprise unwanted email threat
model, where an adversary tries to trick enterprise users into per-
forming an action, such as opening an email attachment, clicking
a link, or leaking personal information, by sending one or more
unwanted emails to them. Therefore, the unwanted emails include
phishing, spear-phishing, spam, or other unsolicited emails deemed
1Though an enterprise may have multiple email domain names, for simplicity and
without loss of generality, in this work we assume each enterprise has only one email
domain name.
harmful by an enterprise. Our threat model assumes that there are
one or more attackers outside an enterprise who send unwanted
emails to the enterprise. During the period where the profiling
is performed, we assume that sender email addresses as well as
all internal emails are not spoofed. Thus, the enterprise features
learned from the historical data reflect benign behaviors. Mean-
while, we assume that attackers could send emails with spoofed
sender email addresses or sender IPs. In other words, an attacker
may send unwanted emails in the name of legitimate external users
who have interacted with the enterprise before. Our threat model
assumes that the attacker cannot craft malicious emails that mimic
the enterprise features of legitimate ones.
3.4 Overview of Our Approach
We design CADUE, a Content-Agnostic email Detection system
for Unwanted Emails in the enterprise setting. CADUE takes a
collection of emails ER, which consists of a set of labeled external
emails and a set of past internal emails of an enterprise as the input,
and derives a set of features from ER for a binary classifier H. For
any unlabeled external email e in the testing dataset ET , H labels e
as 1 if it is predicted as an unwanted email, or 0 otherwise. Our focus
is not to design a new classification model for detection. Instead,
our goal is to extract a set of content-agnostic features (cid:174)x from ER,
and build a state-of-the-art classifier H with existing classification
models (e.g., random forest) to label ET based on their features (cid:174)x.
Specifically, we categorize the extracted features (cid:174)x into two types:
(1) the non-enterprise features, which are extracted directly from the
headers of the emails in ER. These features only rely on an email
itself; and (2) the enterprise features, which capture the external
and internal communication patterns between employees of an
enterprise. Enterprise features depend on not only the email itself
but also historical external and internal emails.
components (illustrated in Figure 3):
To extract these features, CADUE builds the following three
â€¢ Header analysis: This component analyzes the header infor-
mation of the emails in ER, and outputs header features. We
use header features for our baseline classifier.
â€¢ Sender analysis: This component groups the external emails
in ER by their senders, and extracts the sender profiling fea-
tures from the groups. In other words, sender profiling fea-
tures capture the historical interaction of an external sender.
â€¢ Enterprise graph analysis: This component constructs enter-
prise communication graphs from the internal emails in ER
and uses these graphs to extract the enterprise graph features.
As discussed before, though email contents are not accessi-
ble, an emailâ€™s recipients could reveal the intention of the
email, and thus could be useful to distinguish benign emails
from unwanted one.
FirewallSecurityApplianceExchangeServerEnterprise IntranetExternalEmailsInternetInternalEmailsHeaderFeaturesSender Profiling Featuresð¸ð¸ð‘…ð‘…SenderAnalysisEnterprise Graph AnalysisHeaderAnalysisClassifierð¸ð¸ð‘‡ð‘‡Training DatasetTesting DatasetEmail ð‘’ð‘’ExternalEmailsInternalEmailsEmail HeaderInformationPrediction 0/1Enterprise GraphFeatures208CADUE: Content-Agnostic Detection of Unwanted Emails for Enterprise Security
RAID â€™21, October 6â€“8, 2021, San Sebastian, Spain
Some prior works [2, 28, 45, 49] have considered similar header
features. However, our enterprise features are novel. To our best
knowledge, only a few works in the literature focus on email filter-
ing in an enterprise setting [18, 22, 23]. Most of the features in [23]
and [22] rely on email contents (e.g., the URLs in the email). Their
content-agnostic features are mainly derived from the From field in
email headers. The problem setting in [18] is different from ours
in that it targets solely at one type of unwanted emails â€“ external
spoofed emails, while our technique covers a broad range of emails
deemed risky by enterprise policies. Furthermore, none of the fea-
tures in the above works capture the communication patterns and
relationships among internal users.
4 ENTERPRISE FEATURES
In this section, we discuss the details of enterprise features. Based
on the sources from which these features are derived, we categorize
them into two types:
â€¢ Sender profiling features that are extracted from the ex-
ternal emails. These features describe the patterns of how
external senders communicate with internal users by emails.
â€¢ Enterprise graph features that are extracted from the in-
ternal enterprise emails. These features capture the email
communication patterns of internal users.
4.1 Sender Profiling Features
Consider the training dataset ER, and a new given email e, we first
select three types of emails from ER:
â€¢ ES N DR âŠ† ER consists of all emails in ER that only have the
same sender as e;
â€¢ ES N DR+SU B âŠ† ER includes all emails in ER that have the
same sender and subject as e; and
â€¢ ES N DR+RCV R âŠ† ER contains all emails in ER that have the
same sender and recipient as e.
For each type of emails, we derive a set of features for the input
email e. In total, we have 18 features (summarized in Table 1). We
call these 18 features the sender profiling features.
Next, we discuss the details of these features as well as the
intuition behind them.
Features derived from ES N D R. Our first observation is that
unwanted emails, especially spam emails, often behave in a distinc-
tive pattern - they are often sent in a large number by the same
sender during a short time duration. We design three features (Fea-
tures 1 - 3 in Table 1) to catch this pattern. Feature 1 measures the
average number of emails per day sent by the sender, and feature 2
counts the number of broadcast emails in the training data. Intu-
itively, a sender who sends out a large number of broadcast emails
are more suspicious. Since the number of emails (for both Features
1 and 2) could be quite large for some senders, instead of returning
the actual count k, we take the logarithmic scale of k and return a
score sk defined as:
(1)
Feature 3 in Table 1 measures the average time interval between
the consecutive emails received within a time window of T days in
ES N DR. To measure the average time interval, first we calculate the
averaged daily time interval. Suppose there are n emails in ES N DR
sk = log(1 + k)
on the i-th day ei,1, ei,2, . . . , ei,n sorted by their timestamps. We
use the function t(e) to return the timestamp of an email e. Then
the averaged daily time interval at the i-th day di is calculated as:
j=2 t(ei, j) âˆ’ t(ei, jâˆ’1),
if n > 1
if n âˆˆ {0, 1}
(2)
(cid:26) 1
n
di =
nâˆ’1
86, 400,
(cid:40)T
T
In particular, when there is no email or only one email in a day, we
set the time interval di = 86, 400 (representing 86,400 seconds, i.e.,
24 hours). Based on the average daily time interval, we calculate the
average time interval sintv as the average of di in past T days (from
current date dc to dcâˆ’T ), where T is a user-specified parameter for
the time window size:
sintv =
i =1 dcâˆ’i Â·1(dcâˆ’i (cid:44)86400)
86400,
i =1 1(dcâˆ’i (cid:44)86400)
,
if âˆƒ1 â‰¤ i â‰¤ T , dcâˆ’i (cid:44) 86400
otherwise
(3)
To make the score representative, the value of T should not be too
small. In our experiments, considering the number of days we have
in the dataset, we use T = 14, i.e., a 2-week time window, as it leads
to the best performance.
Besides the features derived from the frequency of emails, we
also design a reputation-based feature, namely Feature 4 in Table 1,
to measure the reputation of the sender based on its email history.
This feature returns the number of unwanted emails in ES N DR sent
by the same sender of the input email e. Since the count can be
large, we take its logarithmic scale (Equation 1).
Next, we design four features (Features 5 - 8 in Table 1) that
measure the similarity between the input email and past benign
emails from the same sender. Intuitively, the higher the similarity,
the less likely the input email e be an unwanted one. Since email
contents are not available, we measure email similarity based on
four header fields:
H F = {user_agent, path, message_id, helo}
The user_agent field indicates the name and version of the senderâ€™s
email client. The path field contains the IP addresses of all hops
along the transmission path of the email. The message_id field
contains a hash string as a prefix (possibly with delimiters between
hash strings), an "@" character, and a domain address as the suf-
fix. The hash string and the domain address are generated by the
senderâ€™s email server. The helo field contains the domain address
of the senderâ€™s email server. For each header field f âˆˆ H F, we
derive a feature that measures the similarity between the input
email e and the emails in ES N DR on the field f by calculating the
similarity score ssim. Formally, let Eâˆ’
S N DR be the set of emails in
ES N DR that are labeled as benign (negative), and TÎ² be the col-
lection of unique values of the field f . For each value Î³ âˆˆ TÎ² , we
measure the similarity between the input email e and Î³ (at field
f ) as sim(Ë†Î³ , Î³), where Ë†Î³ is the value of field f in the input email e.
The function sim() can be defined as any string similarity metric,
e.g., Levenshtein distance and Jaccard similarity. We use Jaccard
similarity in our experiments. Finally, we compute the similarity
score ssim between the input email e and Eâˆ’
S N DR on the field f as:
if |TÎ² | > 0
if |TÎ² | = 0
(cid:26) maxÎ³ âˆˆTÎ² sim(Î³ , Ë†Î³)
ssim =
(4)
0
209RAID â€™21, October 6â€“8, 2021, San Sebastian, Spain
Mohamed Nabeel, Enes Altinisik, Haipei Sun, Issa Khalil, Hui (Wendy) Wang, and Ting Yu
Emails
ES N DR
ES N DR+SU B
ES N DR+RCV R
Feature
SENDER_NUM_EMAIL
SENDER_NUM_BC
SENDER_TIME_INTV
SENDER_PAST_DISTRUST
SENDER_SIM_UA
SENDER_SIM_PATH
SENDER_SIM_MSGID
SENDER_SIM_HELO
SENDER_SIM_FIELDS
SENDER_EMAIL_SUBNET_FREQUENCY
EMAIL_IS_SBCAST
No.
1
2
3
4
5
6
7
8
9
10
11
12 RECVER_NUM_EMAIL
13 RECVER_NUM_BC
14 RECVER_TIME_INTV
15 RECVER_SIM_UA
16 RECVER_SIM_PATH
17 RECVER_SIM_MSGID
18 RECVER_SIM_HELO
Domain Description
Average number of emails per day (at logarithmic scale).
Number of broadcast emails per day (at logarithmic scale).
Average time interval between consecutive emails (Equation 3).
Number of unsolicited emails in the training data (at logarithmic scale).
Similarity to historical value(s) of user_agent field.
Similarity to historical value(s) in path field.
Similarity to historical value(s) in msg_id field.
Similarity to historical value(s) in helo field.
Similarity to header fields of same sender emails.
Frequency of emails per sender /24 subnets.
R+
R+
R+
R+
[0, 1]
[0, 1]
[0, 1]
[0, 1]
[0, 1]
[0, 1]
{0, 1} Whether the email is a broadcasting email with one single recipient.
R+
R+
R+
[0, 1]
[0, 1]
[0, 1]
[0, 1]
Average number of emails per day (at logarithmic scale).
Number of broadcast emails per day (at logarithmic scale).