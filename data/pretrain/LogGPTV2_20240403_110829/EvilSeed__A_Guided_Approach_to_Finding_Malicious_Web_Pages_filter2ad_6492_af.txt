we have observed several cases that indicate otherwise. For
example, from January to November 2011 we have observed
a large-scale injection campaign on legitimate web sites that
was distributing malware: to this date, it can be still be found
querying Google and Bing for “calendar about pregnancy”.
This n-gram has been generated by our EVILSEED deployment
in January, when our crawler randomly visited a URL that
has been infected. Over the next ten months, both Bing and
Google have been slowly blacklisting more and more results
of this query, until the campaign was eventually shut down.
This incremental blacklisting is indicative that search engines
do not implement EVILSEED: otherwise, upon marking one
of the infected URLs as malicious, all the remaining URLs
would be quickly sent to in-depth analysis, and blacklisted.
Evasion. Cybercriminals could try to detect our visits. To
mitigate this, visits from crawler and gadgets come from a
“fresh” browser (no history and previous state). Cybercriminals
could still track our system’s IPs: using a large, dynamic pool of
IPs mitigates this problem. It is improbable that cybercriminals
specializing in SEO campaigns and injections will try to evade
EVILSEED by diminishing their footprint in search engines,
because this will reﬂect negatively in the trafﬁc on their sites,
and ultimately in their income.
Gadget selection. A ﬁnal question that we address is whether
there are particular limitations on the gadgets that can
be employed in our system. As we have seen, the only
requirements on gadgets are that they are capable of identifying
similarities among the pages of the evil seed, and of querying
search engines for pages with the same properties. We have
presented the implementation of a diverse set of gadgets, based
on the analysis of the textual content of web pages, on their
linking relationship, and on their behavior (cloaking). In our
experience, it was also easy to extend the existing system by
“plugging in” an additional gadget.
VI. Related Work
Finding malicious content on the web is a process that
requires two main components: a detection procedure (or
oracle), which, given a web page, decides whether or not it
is malicious, and a searching procedure, which locates web
pages to submit to the oracle.
Oracles. The problem of designing effective oracles for
the detection of malicious web pages (mostly pages that
perform drive-by download attacks [2], but also fake anti-
virus pages [4] and Flash-based malicious advertisements [33])
has received considerable attention. Two main approaches have
been proposed: (i) using high-interaction honeyclients to detect
unexpected changes in the underlying system (e.g., new ﬁles
or running processes) indicating a successful compromise [2],
[9], [10], [34], [35]; and (ii) using low-interaction honeyclients
or regular browsers instrumented with specialized,
light-
440
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:16 UTC from IEEE Xplore.  Restrictions apply. 
weight detectors [6]–[8], [36]–[41]. Additional efforts have
focused on preventing unconsented content execution, which
is the ultimate goal of drive-by download attacks [42], and
on identifying the common pieces of infrastructure (central
servers in malware distribution networks) employed by large
numbers of attacks [43]. In this work, we use Wepawet [6] (a
low-interaction honeyclient) and the Google Safe Browsing
blacklist [2] (a blacklist produced using high-interaction
honeyclients) as our oracle. We use these tools as black boxes,
and, therefore, it would be possible to use in EVILSEED any
other available tool that is able to provide an evaluation of the
malicious nature of a web page. Since honeyclients require
a signiﬁcant amount of resources to analyze a web page, a
preﬁltering step is often applied to a candidate page, so that
likely-benign pages are quickly discarded [2], [21], [22], [44].
Our work is orthogonal to preﬁlters: We are not concerned
with ﬁltering out from a pre-existing dataset “uninteresting
pages,” as preﬁlters do. Instead, we describe ways to effectively
build datasets that are more likely to contain “interesting” (i.e.,
malicious) web pages.
Searching for candidate pages. The second component of a
system that detects malicious web content is one that gathers
the web pages that are passed to the oracle. Previous work
has focused on web crawling, identifying common features
of malicious pages, and monitoring and learning from the
attackers’ behavior. Hereinafter, we discuss each of these areas.
Traditionally, researchers have relied on large web crawls to
collect web pages [2], [17]. These large crawls are effective,
because they provide a “complete” view of the web: Provos
et al. report a 5% detection rate, after a preﬁltering step in
which billions of pages are quickly discarded. The downside
is that these massive crawls require an extensive infrastructure,
which is available only to a few organizations.
Alternatively, web crawls can be guided to favor the visit
of parts of the web that, according to a number of heuristics,
are more likely to contain malicious content [21], [35]. These
smaller, targeted crawls are feasible also for smaller players,
since they do not require building and maintaining a large
infrastructure. However, they yield lower detection rates. For
example, Moshchuk et al. report a 0.4% detection rate [35].
Our approach hits a sweet spot between these two crawling
alternatives. In fact, EVILSEED does not require the extensive
infrastructure used for large crawls (the entire system runs
on one off-the-shelf server-class machine). Instead, it mines
malicious samples to generate search engine queries that point
to URLs with high toxicity, thereby leveraging the work that
search engines have already performed.
An alternative technique to searching for malicious content
is based on the observation that malicious resources are often
similar to each other, for example, because they are created
using the same attack toolkits. Previous efforts have focused
speciﬁcally on domain names, and have identiﬁed a number
of features that are characteristics of the malicious domains
controlled by cybercriminals. These characteristics can be
used to identify additional (previously uncategorized) domains
that are likely to be malicious [5], [45]–[47]. One important
difference to our approach is that we want to identify legitimate
but compromised pages, rather than domains that are directly
under control of the cybercriminals. In this context, the involved
domain names are not decided by the cybercriminals.
A ﬁnal approach to identifying potentially malicious sites
consists of replicating the techniques cybercriminals use to
search for vulnerable web sites. The insight, in this case, is that
vulnerable web sites that are searched for by cybercriminals
are likely to get compromised and become malicious [11],
[12], [14]. We approach the problem from a different angle.
Instead of identifying actual (manually-crafted) queries from
existing search engine logs, we analyze known, malicious
pages to automatically extract the searches that can be used
to discover both compromised landing pages and malware
distribution sites.
VII. Conclusions
As malicious activity on the web continues to increase, it is
critical to improve the defenses at our disposal. An important
component of our defense is the ability to identify as many
malicious web pages on the Internet as possible. This is a
daunting task that requires a substantial amount of resources.
In this paper, we propose a novel approach whose goal is to
improve the effectiveness of the search process for malicious
web pages. We leverage a seed of known, malicious web
pages and extract characterizing similarities that these pages
share. Then, we use the infrastructure of search engines and
the data that they have collected to quickly identify other
pages that show the same characteristics and, thus, are also
likely malicious. We have implemented this approach in a tool,
called EVILSEED, and validated it on large-scale datasets. Our
results show that EVILSEED can retrieve a set of candidate
web pages that contains a much higher percentage of malicious
web pages, when compared to random crawling (and even
to results returned for manually-crafted, malicious queries).
Therefore, by using EVILSEED, it is possible to improve the
effectiveness of the malicious page discovery process.
Acknowledgments
This research draws on data provided by the University
Research Program for Google Search, a service provided
by Google to promote a greater common understanding of
the web. This work was supported by the ONR under grant
N000140911042 and by the National Science Foundation
(NSF) under grants CNS-0845559 and CNS-0905537, and by
the European Commission through project 257007 (SysSec).
References
[1] N. Provos, D. McNamee, P. Mavrommatis, K. Wang, and
N. Modadugu, “The Ghost in the Browser: Analysis of Web-
based Malware,” in USENIX Workshop on Hot Topics in
Understanding Botnet, 2007.
[2] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose, “All
Your iFrames Point to Us,” in USENIX Security Symposium,
441
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:16 UTC from IEEE Xplore.  Restrictions apply. 
2008.
[3] M. Polychronakis, P. Mavrommatis, and N. Provos, “Ghost
Turns Zombie: Exploring the Life Cycle of Web-based Malware,”
in USENIX Workshop on Large-Scale Exploits and Emergent
Threats, 2008.
[4] M. A. Rajab, L. Ballard, P. Mavrommatis, N. Provos, and
X. Zhao, “The Nocebo Effect on the Web: An Analysis of Fake
Anti-Virus Distribution,” in USENIX Workshop on Large-Scale
Exploits and Emergent Threats, 2010.
[5] M. Cova, C. Leita, O. Thonnard, A. Keromytis, and M. Dacier,
“An Analysis of Rogue AV Campaigns,” in Symposium on
Recent Advances in Intrusion Detection (RAID), 2010.
[6] M. Cova, C. Kruegel, and G. Vigna, “Detection and Analysis
of Drive-by-Download Attacks and Malicious JavaScript Code,”
in International World Wide Web Conference (WWW), 2010.
[7] J. Nazario, “PhoneyC: A Virtual Client Honeypot,” in USENIX
Workshop on Large-Scale Exploits and Emergent Threats, 2009.
[8] K. Rieck, T. Krueger, and A. Dewald, “Cujo: Efﬁcient Detection
and Prevention of Drive-by-Download Attacks,” in Annual
Computer Security Applications Conference (ACSAC), 2010.
[9] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski,
S. Chen, and S. King, “Automated Web Patrol with Strider
HoneyMonkeys: Finding Web Sites that Exploit Browser
Vulnerabilities,” in Symposium on Network and Distributed
System Security (NDSS), 2006.
[10] C. Seifert and R. Steenson, “Capture-HPC,” http://goo.gl/vSdds.
[11] N. Provos, J. McClain, and K. Wang, “Search Worms,” in ACM
Workshop on Recurring Malcode (WORM), 2006.
[12] J. John, F. Yu, Y. Xie, M. Abadi, and A. Krishnamurthy,
“Searching the Searchers with SearchAudit,” in USENIX Security
Symposium, 2010.
[13] IBM, “Mid-Year Trend and Risk Report,” Tech. Rep., 2010.
[14] T. Moore and R. Clayton, “Evil Searching: Compromise and
Recompromise of Internet Hosts for Phishing,” in International
Conference on Financial Cryptography and Data Security, 2008.
[15] S. Small, J. Mason, F. Monrose, N. Provos, and A. Stubbleﬁeld,
“To Catch A Predator: A Natural Language Approach for
Eliciting Malicious Payloads,” in USENIX Security Symposium,
2008.
[16] Google, “Safe Browsing API,” http://goo.gl/vIYfk, 2011.
[17] J. Stokes, R. Andersen, C. Seifert, and K. Chellapilla, “WebCop:
Locating Neighborhoods of Malware on the Web,” in USENIX
Workshop on Large-Scale Exploits and Emergent Threats, 2010.
[18] J. Long, E. Skoudis, and A. v. Eijkelenborg, Google Hacking
for Penetration Testers, 2004.
[19] J. Long, “Google Hacking Database,” http://goo.gl/qmPA8.
[20] Cult of the Dead Cow, “Goolag Scanner,” http://goo.gl/lBK1o.
[21] D. Canali, M. Cova, C. Kruegel, and G. Vigna, “A fast
ﬁlter for the large-scale detection of malicious web pages,”
in International World Wide Web Conference (WWW), 2011.
[22] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “Zozzle:
Low-overhead Mostly Static JavaScript Malware Detection,”
Microsoft Research, Tech. Rep., 2010.
[23] Yahoo, “Yahoo Term Extraction API,” http://goo.gl/wGacG.
[24] R. Flores, “How Blackhat SEO Became Big,” Trend Micro,
Tech. Rep., 2010.
[25] F. Howard and O. Komili, “Poisoned search results: How hackers
have automated search engine poisoning attacks to distribute
malware.” SophosLabs, Tech. Rep., 2010.
[26] B. Googins, “Black Hat SEO Demystiﬁed: Abusing Google
Trends to Serve Malware,” 2010.
[27] B. Zdrnja, “Down the RogueAV and Blackhat SEO rabbit hole,”
http://goo.gl/5nDuK, 2010.
[28] D. Wang, S. Savage, and G. Voelker, “Cloak and dagger:
dynamics of web search cloaking,” in 18th ACM conference on
Computer and communications security. ACM, 2011.
[29] B. Wu and B. D. Davison, “Detecting semantic cloaking on
the web,” 2006.
[30] K. Chellapilla and D. M. Chickering, “Improving cloaking
detection using search query popularity and monetizability,”
in 2nd International Workshop on Adversarial Information
Retrieval on the Web (AIRWeb), 2006.
[31] B. Wu and B. D. Davison, “Cloaking and redirection: A
preliminary study,” in Workshop on Adversarial Information
Retrieval on the Web (AIRWeb), 2005.
[32] M. Cutts and J. Shellen, “Preventing comment spam,” http:
//goo.gl/rA9mZ, 2005.
[33] S. Ford, M. Cova, C. Kruegel, and G. Vigna, “Analyzing and
Detecting Malicious Flash Advertisements,” in Annual Computer
Security Applications Conference (ACSAC), 2009.
[34] A. Moshchuk, T. Bragin, D. Deville, S. Gribble, and H. Levy,
“SpyProxy: Execution-based Detection of Malicious Web
Content,” in USENIX Security Symposium, 2007.
[35] A. Moshchuk, T. Bragin, S. Gribble, and H. Levy, “A Crawler-
based Study of Spyware in the Web,” in Symposium on Network
and Distributed System Security (NDSS), 2006.
[36] B. Feinstein and D. Peck, “Caffeine Monkey: Automated
Collection, Detection and Analysis of Malicious JavaScript,” in
Black Hat Security Conference, 2007.
[37] P. Likarish, E. Jung, and I. Jo, “Obfuscated Malicious Javascript
Detection using Classiﬁcation Techniques,” in Conference on
Malicious and Unwanted Software (Malware), 2009.
[38] P. Ratanaworabhan, B. Livshits, and B. Zorn, “NOZZLE: A
Defense Against Heap-spraying Code Injection Attacks,” in
USENIX Security Symposium, 2009.
[39] M. Egele, P. Wurzinger, C. Kruegel, and E. Kirda, “Defend-
ing Browsers against Drive-by Downloads: Mitigating Heap-
spraying Code Injection Attacks,” in Conference on Detection of
Intrusions and Malware & Vulnerability Assessment (DIMVA),
2009.
[40] B. Hartstein, “Jsunpack: A Solution to Decode JavaScript
Exploits as they Rapidly Evolve,” in ShmooCon Conference,
2009.
[41] S. Chenette, “The Ultimate Deobfuscator,” in ToorCon, 2008.
[42] L. Lu, V. Yegneswaran, P. Porras, and W. Lee, “BLADE: An
Attack-Agnostic Approach for Preventing Drive-By Malware
Infections,” in ACM Conference on Computer and Communica-
tions Security (CCS), 2010.
[43] J. Zhang, C. Seifert, J. Stokes, and W. Lee, “ARROW:
Generating Signatures to Detect Drive-By Downloads,” in
International World Wide Web Conference (WWW), 2011.
[44] C. Seifert, P. Komisarczuk, and I. Welch, “Identiﬁcation of
Malicious Web Pages with Static Heuristics,” in Austalasian
Telecommunication Networks and Applications Conference,
2008.
[45] S. Yadav, A. K. Reddy, A. Reddy, and S. Ranjan, “Detect-
ing algorithmically generated malicious domain names,” in
Conference on Internet Measurement (IMC), 2010.
[46] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi, “EXPOSURE:
Finding Malicious Domains Using Passive DNS Analysis,” in
Symposium on Network and Distributed System Security (NDSS),
2011.
[47] M. Felegyhazi, C. Kreibich, and V. Paxson, “On the Potential
of Proactive Domain Blacklisting,” in USENIX Workshop on
Large-Scale Exploits and Emergent Threats, 2010.
442
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:16 UTC from IEEE Xplore.  Restrictions apply.