f i
2: Day entropy. PA Attackers are likely to write reviews within the same day,
because they may use automated posting process or want to ﬁnish the task as
quickly as possible. To measure the proportion of same-day reviews, we deﬁned
f i
2 using the information entropy: f i
j=1 P (tij) log P (tij), where
P (tij) is the frequency of same-day reviews: tij/sum and sum =
j=1 tij is the
sum of days reviewed by reviewer ri. If all the reviews are posted on the same
day, the entropy of the post time will be 0.
f i
3: Bi-gram matching. PA attackers often post similar reviews. Detecting
similar reviews is important due to the presence of made-up words that are
used to express strong feelings, such as “goooooood” and “coooooool”. Made-up
words cannot be reformed by existing spelling correction algorithms because they
are designed to correct misspelled words instead of intentionally created words.
To address this problem, we converted each word into a bi-gram and then used
bag of bi-gram to build a feature vector for each cij. Finally we calculated the
average of the cosine similarity score of each pair of reviews by the reviewer
ri. In other words, f i
i . Where cosim is cosine
similarity score. We set the threshold of cosine similarity as 0.9.
f i
4: Semantic similarity. Since reviewers may use diﬀerent words and expres-
sions to express the same feeling, we identify similar words and expressions using
the the Paragraph Vector (PV) algorithm [14], because it performs a semantic
analysis in discovering similar words and expressions. By applying the PV algo-
rithm realized in the Python library gensim [3] to 57, 868, 301 reviews in our
dataset, we get the predicted model after around 1 h. We deﬁned f i
4 as the aver-
age of the similarity scores predicted from the trained model for each pair of
reviews. f i
i , Where D is the distance of two dif-
ferent documents computed by PV algorithm. Table 2 presents some examples
of the similarity scores computed by the trained PV model. It is clear that
the model can infer the correlations between not only diﬀerent words with the
same purpose but also security-related similarity words without using the labeled
data. Note that although we used words to demonstrate the eﬀectiveness of the
approach, we actually apply the algorithm to the entire review texts.
f i
5: Sentiment analysis. PA attackers usually post positive reviews to promote
apps for monetary beneﬁt and/or luring victims to install malicious apps. Sen-
timent analysis classiﬁes the attitude of a text into three categories: negative,
neutral, positive. Using sentiment analysis, we could reveal potential PA attack-
ers if all the reviews are positive. We use TextBlob [7] to conduct the sentiment
analysis of all the reviews. The sentiment analysis in TextBlob was implemented
by a supervised learning naive Bayes classiﬁer that is trained on the labeled
movie reviews provided by NLTK. We deﬁne f i
5 as the average score for each
k=1 cosim(cij, cik)/m2
k=1 D(cij, cik)/m2
(cid:2)mi
j=1
4 =
(cid:2)mi
j=1
3 =
(cid:2)mi
(cid:2)mi
Characterizing Promotional Attacks in Mobile App Store
119
Table 2. Examples of similarity score
computed with the trained Paragraph
vector model.
Table 3. Example of score predicted
by sentiment analysis classiﬁer
word1 word2
similarity score
Sentence
The score of sentiment
analysis
adware malware
ads
spam
camera permission
hack
access
internet location
good
nice
0.88
0.64
0.74
0.71
0.62
0.60
That is my
opinion
0.0
Awesome game
0.3
Nice graphics
0.55
and I love it
Very bad game −0.65
−0.8
I hate all the
covers I’m here
to look for the
songs made by
the artist not
covers
pair of reviews predicted by the sentiment analysis classiﬁer. Table 3 shows an
example of the scores predicted by the sentiment analysis classiﬁer. If the score is
zero, it means the sentiment of the review is neutral. It shows that our classiﬁer
can correctly identify the sentiment of the reviews.
f i
6: The average length of the reviews. Fake reviews injected by promotional
attackers are likely to be short, because they may use an automated posting
process or want to get income as quickly as possible. Therefore, we deﬁned f i
6
as the average length of the reviews written by the reviewer ri.
f i
7: True Reputation Score. Users often rely on the average ratings of the
apps, computed by the app stores, in selecting the apps. Unfortunately, PA
attackers can easily manipulate the average ratings by giving high ratings to
their target apps. We deﬁned f i
7 as the average of the margin between the app’s
rating and the reviewer’s rating based on the true reputation score of each app
instead of the average rating. This score is calculated according to the TRUE-
REPUTATION algorithm [19], which takes into account the user conﬁdence in
terms of user activity, user objectivity, and user consistency. f i
7 is computed as:
(cid:2)mi
i=1(sij − uaj)/mi, where mi is the number of apps reviewed by reviewer
f i
7 =
ri. a is an app and ua is true reputation score for app a.
f i
8: Average ratings. Since PA attackers give high ratings to malicious apps
for attracting more downloads, we deﬁned f i
8 as the average ratings posted by
reviewer ri. f i
9 as the
coeﬃcient of variation of all the ratings posted by each reviewer to measure
their distribution. It is the ratio of the standard deviation to the mean: f i
9 =
}. If a
σ(Si)/
reviewer posts identical ratings, f i
f i
10: Average number of installs. Since the number of installs is an important
metric aﬀecting users’ selection of apps, we deﬁned f i
10 as the average number
of installs for reviewer ri. f i
(cid:2)mi
j=1 sij, where σ is standard deviation and Si = {si1, . . . , simi
9: Coeﬃcient of variation of ratings. We deﬁned f i
9 will be 0.
(cid:2)mi
10 =
j=1 nij/mi.
120
B. Sun et al.
12 can be referred to the equation deﬁned by f i
f i
11: Coeﬃcient of variation of the number of installs. To measure the
distribution of the number of installs, we deﬁne f i
11 as the coeﬃcient of variation
of the number of installs for reviewer ri. The computation of f i
11 can be referred
to the equation deﬁned by f i
10. If a reviewer posts reviews to apps with the same
number of installs, the coeﬃcient of variation will be 0.
f i
12: Developer Entropy. PA attackers are more likely to promote apps from the
same developer because the targeted malicious apps should be associated with
each other. Therefore, we deﬁned f i
12 as the entropy of developer for reviewer
ri. The computation of f i
2. If a
reviewer only posts reviews for apps from the same developer, his/her f12 will
be 0.
f i
13: Category Entropy. PA attackers tend to promote apps having a small
number of distinct categories, possibly due to the automated posting process.
Similar to f i
13 as the entropy of category for reviewer ri. The
computation of f i
2. If a
reviewer only posts reviews for apps having a small number of distinct categories,
his/her f13 will be 0.
f i
14: Length of reviewer name. Legitimate reviewers usually use their own
name as the reviewer name, whereas the reviewer names selected by PA attackers
are likely to be unusually short or long. Hence, we deﬁned f i
14 as the length of
the reviewer name.
f i
15: Number of digits and symbols in reviewer name. The reviewer names
of promotional attackers are often randomly generated, and therefore they are
likely to contain digits and symbols such as “!”, “*”, “@.”According to this
observation, we deﬁned f i
15 as the number of digits and symbols in the reviewer
names.
13 can also be referred to the equation deﬁned by f i
12, we deﬁned f i
3.3 Eﬀectiveness of Feature and Description of Detection Model
Eﬀectiveness of feature. To demonstrate how our features facilitate the detec-
tion, we compute the importance of our features. For the space limitation, we
present the top-3 features that had the largest contributions (f i
1: Day inter-
vals, f i
12: Developer Entropy). We extracted
these three features by using tree-based feature selection method [2], which uses
forests of trees to evaluate the importance of features.
10: Average number of installs, f i
Figure 5 shows the CDF of the day intervals of promotional attackers and
those of normal reviewers. We can see that promotional attackers usually have
shorter day intervals than normal reviewers. It is likely that promotional attack-
ers want to get revenue quickly or are required by their employers to do so.
Figure 6 shows the CDF of the number of installs of promotional attackers and
those of normal reviewers. We can ﬁgure out that promotional attackers tend to
promote apps whose number of installs is not very large due to the prohibition
of promotion activity by Google Play [1]. Figure 7 shows the CDF of the devel-
oper entropy of promotional attackers and those of normal reviewers. We can see
that promotional attackers tend to promote apps produced by the same devel-
oper. Because promotional attackers are probably hired by the same developer.
Characterizing Promotional Attacks in Mobile App Store
121
Fig. 5. f i
1: Day intervals. Fig. 6. f i
of installs.
10: Average number
Fig. 7. f i
Entropy
12: Developer
We note that these three features are informative for identifying promotional
attackers from normal reviewers. We also found that the features extracted from
metadata are more eﬀective than those from UGC in PA detection, because it is
not easy for attackers to manipulate the metadata such as developer and number
of installs.
Description of detection model. We build our detection model using the
library scikit-learn [6] because it is eﬃcient, and implement several supervised
learning algorithms, including support vector machine (SVM), k-nearest neigh-
bor (KNN), random forest, decision tree, and adaBoost. To determine the best
algorithm and parameters, we test the algorithms and parameters using our
labeled dataset. The detailed model selection process and its results are pre-
sented in Sect. 4. Finally, we use the best detection model to perform a large-scale
analysis of our real-world dataset.
4 Performance Evaluation
This section presents the evaluation result of PADetective. We ﬁrst introduce
how we prepare the labeled dataset (i.e., the ground truth), and then describe
the evaluation method and the result, respectively.
Training Dataset. We ﬁrst generate the training dataset with the ground truth.
Since legitimate reviewers may comment bad apps and/or post reviews to mali-
cious apps, we deﬁne a PA attacker as a reviewer who only posts reviews to mali-
cious apps and comments at least three malicious apps. We determine whether
an app was malicious by submitting the app to VirusTotal [8] and making the
decision based on the results from a set of antivirus systems. Note that we did
not verify all the apps in our dataset to generate the training dataset because
of the limitation of time and computer resources. We also note that VirusTotal
usually classiﬁes malicious apps into two categories: malware and adware. We
did not distinguish between these categories because PAs would likely be used
to promote both malware and adware apps. With this approach and additional
manual inspection, we identiﬁed 723 promotional attackers. Aside from this, we
randomly selected 1,000 legitimate users to create the training dataset. The rea-
son why we randomly sampled legitimate users was to achieve a good balance
between the two classes when we trained our classiﬁers.
122
B. Sun et al.
Evaluation Method. We randomly divided the labeled data into two sets.
Containing 70% of labeled data, the ﬁrst dataset is the training dataset used to
optimize each machine learning model and select the best model. For optimizing
the machine learning algorithms, we specify a set of carefully chosen values for
each parameter used in those algorithms (e.g., for random forest, we set parame-
ter “n estimators” to a set of values: 50, 100, 150, 200, 250). Then, we evaluate
the machine learning algorithms with diﬀerent parameters through 10-fold cross-
validation. Finally, we select the best result in consideration of accuracy, false
positive and false negative. Having 30% of labeled data, the second dataset is the
test dataset utilized to evaluate PADetective’s performance after the best model
is selected. To measure the accuracy of various supervised learning algorithms,