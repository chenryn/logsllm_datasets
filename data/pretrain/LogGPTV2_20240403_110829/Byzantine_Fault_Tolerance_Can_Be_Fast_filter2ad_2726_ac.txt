0 
20 
10 
number of clients 
30 
40 
i
Figure 6. Request Batching Optimization. 
Figure 6 compares throughput with and without request 
batching for read-write operation 0/0. The throughput with- 
out hatching  grows with the number of clients because the 
algorithm  can  process many  requests  in  parallel.  But  the 
replicas'  CPUs saturate  for  a small  number of  clients be- 
cause processing each  of these requests  requires a full in- 
stance  of  the protocol.  Our hatching  mechanism reduces 
both  CPU  and  network  overhead  under  load  without  in- 
creasing the latency to process requests in an unloaded sys- 
tem.  Previous state machine replication  systems that tole- 
rate Byzantine faults [ IO, 91 have used batching techniques 
that impact latency significantly. 
- 
; a 2000 
h 
~ 0 0 0 1 - -  
v 
x 
j 1000  ?' 
- 
?' 
9' 
~~~~ .d 
*' 
0 
s 
g1oOo  ; 
P * 1 - 8 -   - - 8 -  ..* 
-+ SRT 
-* NO-SRT 
0 
0 
2000  4000  6000  8OOO 
argument size (bytes) 
number of clients 
Figure 7. Separate Request Transmission. 
0 
0 
20 
40 
60 
Figure  7  compares  performance with  and  without  the 
separate request transmission  optimization. The first graph 
shows latency  for varying  argument sizes, and  the second 
shows throughput for read-write operation 4/0. We  labeled 
the version of BFT without the optimization BFT-NO-SRT. 
Separating  request  transmission  reduces  latency  by  up  to 
40% because the request is sent only once and the primary 
and  the  backups compute the  request's  digest  in  parallel. 
The  other  benefit  of  separate request  transmission  is  im- 
proved throughput for large requests because it enables more 
requests per batch. 
The  impact  of  the  tentative  execution  optimization  on 
throughput  is  insignificant.  The  optimization  reduces  la- 
tency  by  up to  27%  with  small  argument and result  sizes 
but its benefit decreases quickly when sizes increase. 
Piggybacking commits  has  a  negligible  impact  on  la- 
tency because the commit phase of the protocol is performed 
in the background (thanks to tentative execution of requests). 
It also has a  small  impact on throughput except when  the 
number of concurrent clients accessing the service is small. 
For  example,  it  improves the  throughput of  operation  0/0 
by  33%  with  5  clients  but  only  by  3%  with  200 clients. 
The benefit decreases because  hatching amortizes the cost 
of processing the commit messages over the batch size. This 
optimization is the only one that is not currently part of the 
BFT library; we only wrote code for the normal case. 
5. File System Benchmarks 
We  compared the  performance of  BFS  with  two other 
implementations  of  NFS:  NO-REP,  which  is  identical  to 
BFS  except that  it  is  not replicated,  and NFS-STD, which 
is the NFS V2 implementation in Linux with Ext2fs at the 
server. The first comparison allows us to evaluate the over- 
head of the  BFT library accurately  within  an  implementa- 
tion  of  a real  service.  The second comparison shows that 
BFS is practical:  it performs similarly to NFS-STD, which 
is used daily by many users but is not fault-tolerant. 
5.1. Experimental Setup 
The experiments to evaluate BFS used the setup descri- 
bed  in  Section 4.1.  They  ran  two  well-known  file  system 
benchmarks: Andrew [ 1 13 and PostMark [8]. There were no 
view changes or proactive recoveries in these experiments. 
The  Andrew  benchmark emulates  a  software  develop- 
ment  workload.  We  scaled  up the  benchmark by  creating 
n copies of  the source tree in  the first two phases and ope- 
rating  on  all  copies  in  the  remaining  phases  [4]. We  ran 
a version of Andrew with n equal to  100, Andrewl00, and 
another with n equal to 500, Andrew500. They generate ap- 
proximately  200 MB and  1 GB of data; Andrew100 fits in 
memory at both the client and the replicas but Andrew500 
does not. 
PostMark [8] models the load on Internet Service Provi- 
ders. We configured PostMark with an initial pool of 10000 
files  with  sizes  between  5 12  bytes  and  16  Kbytes.  The 
benchmark ran  100000 transactions. 
For all benchmarks and NFS implementations, the actual 
benchmark code ran at the client workstation using the stan- 
517 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:48 UTC from IEEE Xplore.  Restrictions apply. 
dard NFS client implementation in the Linux kernel with the 
same mount options.  The most  relevant  of these options: 
UDP transport, 3 KB buffers, write-back client caching, and 
attribute caching. BFS and NO-REP do not to maintain the 
time-last-accessed  attribute.  We report the mean of at least 
three  runs, of  each  benchmark  and  the  standard deviation 
was always below 2% of the reported value. 
5.2. Experiments 
Figure 8 presents results for Andrew 100 and Andrew500 
in a configuration with four replicas and one client machine. 
The comparison between BFS and NO-REP shows that the 
overhead of  Byzantine  fault  tolerance  is  low  for this  ser- 
vice - BFS takes only  14% more time to run Andrew100 
and 22% more time to run Andrew500.  This slowdown is 
smaller than the one measured with the micro-benchmarks 
because the client spends a significant fraction of the elapsed 
time  computing between operations, and operations at the 
server perform some computation. Additionally, there are a 
significant number of disk writes at the server in Andrew500. 
h 
8 300 
v .g 200 
aJ 
z 
- 
p 100 
0 )  
1500 
h 2000 
U 
E - loo0 
.- 
z 
4  500 
n 
0 
BFS  NO-REP  NFS-STD 
Andrew500 
n 
"  BFS  NO-REP  NFS-STD 
Andrew100 
Figure 8. Modified Andrew. 
The comparison with  NFS-STD shows that BFS can be 
used  in  practice  - it  takes only  15% longer to complete 
Andrew100 and 24% longer to complete AndrewSOO. The 
performance difference would  be  smaller if  Linux  imple- 
mented NFS correctly. For example, the results in [2] show 
that BFS is 2% faster than the NFS implementation in Di- 
gital  Unix, which  implements the correct semantics.  The 
implementation of NFS on Linux does not ensure stability 
of modified data and meta-data before replying to the client 
(as required by  the  NFS  protocol),  whereas BFS  ensures 
stability through replication. 
"  BFS  NO-REP  NFS-STD 
Figure 9. PostMark. 
The overhead of  Byzantine fault tolerance is  higher in 
PostMark: BFS's throughput is 47% lower than NO-REP'S. 
This is explained by  a reduction on the computation time 
at the client relative to Andrew.  What is interesting is that 
BFS's throughput is only  13% lower than NFS-STD's.  The 
higher  overhead  is  offset  by  an  increase  in  the  number of 
disk. accesses performed by NFS-STD in this workload. 
6. Conclusions 
Elyzantine-fault-tolerant replication  can be used to build 
highly-available  systems  that  can  tolerate  even  malicious 
behavior from fauity replicas. But previous work on Byzan- 
tine fault tolerance has failed to produce solutions that per- 
form  well.  This  paper  presented  a  detailed  performance 
evaluation  of the BFT library, a replication  toolkit  to build 
syst,ems that tolerate Byzantine faults.  Our results show that 
services  implemented  with  the  library  perform  well  even 
when compared with  unreplicated  implementations that are 
not  fault-tolerant. 
Acknowledgements 
We  thank the anonymous reviewers and Lorenzo Alvisi  for 
their comments on drafts of this paper. 
References 
[ I ]   J. Black et al.  UMAC: Fast and Secure Message Authentication.  In 
Advnnces  in Cryprolog?  - CRYPTO, 1999. 
[2]  M. Castro.  Practical  Byzantine  Fault  Tolerance.  Technical  Report 
TR-8 17, PhD thesis. MIT Lab. for Computer Science. 2001. 
[3]  M. Castro and  B. Liskov.  Practical  Byzantine  Fault Tolerance.  In 
USENIX Symposium on  Opercrting Sjsterns Design and Irnplenienm- 
rion,  1999. 
[4]  M. Castro and B. Liskov.  Proactive Recovery in a Byzantine-Fault- 
Tolerant System. In  USENIX Synposium on  Operating Sjstenis De- 
sign rind It~r~~lenierrtntion, 2000. 
[5] M. Castro,  R .  Rodrigues.  and  B. Liskov.  Using Abstraction  to Jm- 
prove Fault Tolerance.  Submitted for publication, 2001. 
[6]  M. Herlihy and J.  Wing.  Axioms  for Concurrent  Objects.  In ACM 
Synrposirrm  on Principles  (if Prognr~zriiirrg Lnngurrges, 1987. 
[7]  A. lyengar et al. Design and Implementation of a Secure Distributed 
Data Repository. In I N P  lriterntrtiorrul Infirmtition Secrrriv Confer- 
ence,  1998. 
[8]  I.  Katcher.  PostMark:  A  New  File  System Benhmark.  Technical 
Report TR-3022, Network Appliance,  1997. 
[9]  K. Kihlstrom, L. Moser, and P.  Melliar-Smith.  The SecureRing Pro- 
tocols for Securing Group Communication.  In Hrrwuii  Imernntionrrl 
Conference  on Sjstem Sciences,  1998. 
[IO]  D. Malkhi and M. Reiter. A high-throughput secure reliable multicast 
protocol. In Cornputer Security  Fourrdutiorrs Workshop.  1996. 
[ I  I]  I .  Ousterhout.  Why Aren't Operating  Systems Getting Faster as Fast 
:1s Hardware?  In  USENlX Summer Conference,  1990. 
[I21  !M. Reiter.  The Rampart toolkit for building  high-integrity  services. 
'Theory rind Pructice in Distributed Systetns  (LNCS 9381, 1995. 
[I31  IW. Reiter et al.  The R  Key  Management  Service.  In ACM  Confer- 
m c e  on Computer trnd Conrmrinictrtio~is Securiry.  1996. 
[ 141  1' 
Schneider.  Implementing  fault-tolerant  services  using  the  state 
machine approach: a tutorial. ACM Conrpuring Surveys, 22(4), 1990. 
[ 151  I,.  Zhou, F. Schneider, and R. Renesse.  COCA: A Secure Distributed 
On-line  Certification  Authority.  Technical  Report  TR  2000- 1828, 
C.S. Department, Cornell University, 2000. 
5 18 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:48 UTC from IEEE Xplore.  Restrictions apply.